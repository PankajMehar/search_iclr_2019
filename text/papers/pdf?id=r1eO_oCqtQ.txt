Under review as a conference paper at ICLR 2019
GAUSSIAN-GATED LSTM: IMPROVED CONVERGENCE
BY REDUCING STATE UPDATES
Anonymous authors Paper under double-blind review
ABSTRACT
Recurrent neural networks can be difficult to train on long sequence data due to the well-known vanishing gradient problem. Some architectures incorporate methods to reduce RNN state updates, therefore allowing the network to preserve memory over long temporal intervals. To address these problems of convergence, this paper proposes a timing-gated LSTM RNN model, called the Gaussian-gated LSTM (g-LSTM). The time gate controls when a neuron can be updated during training, enabling longer memory persistence and better error-gradient flow. This model captures long-temporal dependencies better than an LSTM and the time gate parameters can be learned even from non-optimal initialization values. Because the time gate limits the updates of the neuron state, the number of computes needed for the network update is also reduced. By adding a computational budget term to the training loss, we can obtain a network which further reduces the number of computes by at least 10×. Finally, by employing a temporal curriculum learning schedule for the g-LSTM, we can reduce the convergence time of the equivalent LSTM network on long sequences.
1 INTRODUCTION
Numerous methods and architectures have been proposed to mitigate the vanishing gradient problem in RNNs, with LSTMs (Hochreiter & Schmidhuber, 1997) as one of the first prominent solutions doing so by including gating structures in the computation. Although the LSTM has excelled at handling many tasks (Schmidhuber, 2015; Lipton, 2015), it still has trouble learning complex and long time dependencies (Neil et al., 2016; Chang et al., 2017; Trinh et al., 2018).
In the last few years, various methods which reduce the state updates of an RNN (LSTM) have been explored to better learn long time dependencies from data. Clockwork RNNs (Koutnik et al., 2014) group the hidden units of the RNN into "modules," where each module is executed at pre-specified time steps thereby skipping time steps which helps learn longer time dependencies. Recently, various other methods have been proposed which can be characterized by the use of additional "time gates," kn, that control the information flow from one time step to the next (Krueger et al., 2017; Campos et al., 2017). Phased LSTM (PLSTM) (Neil et al., 2016) learns a parameterized function, kn, from the the time input of the current state, and was proven to be succesful at learning over long sequences.
The PLSTM time gate, parameterized by period, phase, and ratio parameters for each hidden unit, is defined through a modulo function with an ill defined gradient. Furthermore, with periodic functions being hard to learn using gradient based methods (Shamir, 2016), and with kn being periodic, the PLSTM was unable to learn the time gate parameters and hence relied on careful initialization. In order to offset these difficulties, this work proposes a new LSTM variant called the gaussian-gated LSTM (g-LSTM). Similar to the PLSTM, it is an LSTM model with a parameterized kn but with only two parameters per hidden unit. Unlike the PLSTM which uses a periodic formulation for kn, the g-LSTM uses a Gaussian function.
The g-LSTM provides a number of advantages over the LSTM model:
· The g-LSTM network can process very long sequences by reducing the time over which the neurons can be updated. It converges faster than the LSTM, especially on sequences that are over 1000 steps.
1

Under review as a conference paper at ICLR 2019

· The "openness" of the neuron for an update can be adapted according to the task during training.
· By introducing a computational budget term into the loss function during training, the "openness" of the neuron can be optimized for a reduced computational budget. This reduction can be achieved with little or no degradation to the network performance.
· A "temporal curriculum" training schedule can be set up for the g-LSTM so that it helps to speed up the convergence of a normal LSTM RNN.
The paper is structured as follows: In section 2, we discuss briefly the related work. Then, in section 3, we present the formulation of the g-LSTM, the datasets used in this work and details about the experimental hyperparameters. In section 4, we present experiments substantiating the four specific claims made above regarding the advantages of the network. Finally in section 5, we conclude with a brief discussion of the results.

2 RELATED WORK
There have been a multitude of proposed methods to improve the training of RNNs, especially for long sequences. Apart from incorporating additional gating structures, for e.g., the LSTM and the GRU (Cho et al., 2014), more recently various techniques were proposed to further increase the capabilities of recurrent networks to learn on sequences of length over 1000. Proposed initialization techniques such as the orthogonal initialization of kernel matrices (Cooijmans et al., 2016), and diagonal recurrent kernel matrices (e.g. Li et al. (2018)) have demonstrated success. Trinh et al. (2018) proposes using truncated backpropagation with an additional auxiliary loss to reconstruct previous events.
Alternately, methods based on skipping state updates were proposed, first explored by Koutnik et al. (2014) in the Clockwork RNN (CW-RNN). This network skips state updates by allowing different neurons to be "activated" on different, modulated clock cycles, Phased LSTM (PLSTM) (Neil et al., 2016) uses a modulo-periodic timing gate to limit state updates. The effectiveness of skipping state updates in a random manner was shown with the Zoneout network (Krueger et al., 2017). Skip RNN (Campos et al., 2017) learns a state skipping scheme from the data to shorten the effective sequence length in the computation. LSTM-Jump (Yu et al., 2017) uses a reinforcement learning algorithm to learn when to skip state updates, showing a method to more quickly process (long) sequential data with an RNN while maintaining an accuracy comparable to a baseline LSTM.
By preserving memory information over large timescales it has been shown that these solutions enable more efficient learning at large time scales (Koutnik et al., 2014). It has been suggested but not yet demonstrated in the literature that the parameters of the CW-RNN clock cycle and PLSTM timing gate could be learned in training. Currently, the implementation of these networks requires a careful initialization of these parameters. With the Gaussian-gated LSTM (g-LSTM) in this work we present a timing gated RNN network that is demonstrated to efficiently solve long sequence tasks while having the capability of learning its timing gate parameters during training.

3 METHODS

3.1 G-LSTM

The g-LSTM is an LSTM model with an additional time gate (Fig. 1). This time gate is used to regulate the information flow in time. Equations 1 - 3 describe the update equations for the hidden and cell states of the LSTM. Equations 4 and 5 describe the gating mechanism of the time gate, kn.

in = (xnWxi + hn-1Whi + bi), fn = (xnWxf + hn-1Whf + bf )

(1)

c~n = fn cn-1 + in (xnWxg + hn-1Whg + bg)

(2)

on = (xnWxo + hn-1Who + bo), h~n = on tanh(c~n) cn = kn c~n + (1 - kn) cn-1

(3) (4)

hn = kn h~n + (1 - kn) hn-1

(5)

2

Under review as a conference paper at ICLR 2019

cn-1 x

+ x tanh

f i tanh o x hn-1

cn cn-1 x

+ x tanh

f i tanh o x hn hn-1

K cn tn K hn

xn
(a) LSTM

xn
(b) g-LSTM

Figure 1: Comparison of the LSTM and g-LSTM models. k is the computational block for the time gate in (b).

In a standard LSTM, the gating functions in, fn, on, represent the input, forget, and output gates respectively at sequence index n. cn is the cell activation vector, whereas xn and hn represent the input feature vector and the hidden output vector respectively. The cell state cn itself is updated with a fraction of the previous cell state that is controlled by fn, and a new input state created from the element-wise (Hadamard) product, denoted by , of in and the candidate cell activation as in Eq. 2.
In the g-LSTM, we further control the cell state and the output hidden state through the kn gate which is independent of the input data and hidden states, and is purely dependent on the time input corresponding to the sequence index n. The use of the Hadamard product ensures that each hidden unit is independently controlled by the corresponding time gate unit, thus enabling the different units in the layer to process the input at different time scales.
The time gate kn is defined based on a Gaussian function as:

kn = e-(tn-µ)2/2

(6)

where the mean parameter, µ, defines the time when the hidden unit is "open" and the standard
deviation, , controls the openness of the time gate for each unit around its corresponding µ. The time inputs t = {t1, t2, ..., tn, ..., tN } for the sequence x = {x1, x2, ..., xn, ..., xN } can correspond to the physical notion of time at the respective sequence input. In the absence of a standard notion of time, we use the sequence indices as the time input, i.e. t = {1, 2, ..., n, ..., N }. In this work, we
assume this notion of time by default.

hidden unit

3 2 1 00 2 4 6 8 10
time
Figure 2: Time gate functions for four different hidden units on a sequence length of 10. From top to bottom, kn is shown for (µ, ) parameter values of (2, 0.5), (5, 2), (5, 0.5), and (7, 1).
The "openness" of kn as defined by the different parameterizations of the Gaussian function can be seen in Fig. 2. Here, kn is plotted as a function of time (x-axis) with black values corresponding to a fully closed gate (value 0) and white values corresponding to a fully open gate (value 1). Note that lower values of  ensure that the unit is processed only if the time input is µ, while higher  values lead to the unit processed like in the standard LSTM, at all times.
3

Under review as a conference paper at ICLR 2019

3.2 BACK PROPAGATION FOR G-LSTM

An important characteristic of the g-LSTM is reduced gradient flow in back propagation training methods. By having the gating structure as in Eqs. 4 and 5 there are fewer gradient product terms, which reduces the likelihood of vanishing or exploding gradients.

In a gradient descent learning scheme for a given loss function, L, when training the recurrent

parameters, Wh (from Eqs. 1 - 3), the gradient as in Eq. 7 is used.

L = L hN Wh hN Wh

(7)

By the

chain

rule

hN Wh

expands

for

all time

steps

of

the

sequence,

n



{1, ..., N }.

Because

each

output

state

is

gated

by

the

time

gate,

kn,

the

gradient

terms

in

the

expansion

of

hN Wh

are

scaled

by

kn. When the time gate is open less often, i.e. with a small  value, then there are fewer influential

gradient terms. More details are given in Appendix A.

3.3 DATASETS
To demonstrate the advantages of the g-LSTM, the experiments described in the paper are carried out on the adding task and two standard long sequence datasets: the sequential MNIST and the sequential CIFAR-10 datasets.

Adding task: In order to test the long sequence learning capability of the g-LSTM, we use

the adding task (Hochreiter & Schmidhuber, 1997). In this task, the network is presented with

two sequences of length N , x = {x1, ..., xN }, xn  U (0, 1)) and m = {m1, ..., mN }, mn 

{0, 1},

N t=1

mn

=

2.

The

sequence

m

has

exactly

two

values

of

1

and

the

remaining

values

of

the

sequence are 0. The indices of the "1" values are chosen at random. For each pair in the sequence

(x, m), the associated label value, y, is the sum of the two values in x corresponding to the "1"

values of m. The objective of this task is to minimize the mean squared error between the predicted

sum from the network, y^, and the labeled sum, y. A new training set of 5000 sequence samples is

presented in every epoch during training in order to avoid overfitting. The test set consists of further

5000 samples. For N > 1000, it is known that LSTMs have difficulty learning the task and hence

we focus on values of N > 1000 in this work.

sMNIST: The sequential MNIST dataset is widely used to analyze the performance of a recurrent model. This dataset consists of 60,000 training samples and 10,000 test samples, each a single vector sequence of length 784 corresponding to the 28 × 28 pixel images in the MNIST dataset (LeCun et al., 1998). We also use permuted MNIST (pMNIST), a permuted variant of the sMNIST dataset where the sequences are processed with a fixed random permutation, making the task harder.

sCIFAR-10: The sequential CIFAR-10 dataset is another long sequence dataset based on CIFAR10 (Krizhevsky et al., 2009) with 10 classes. The 32 × 32 RGB pixel images are reshaped into sequences of length 1024 with 3 dimensional features corresponding the RGB channels at every
time step. Like in the sMNIST dataset, the dataset consists of 60,000 training samples and 10,000
test samples.

3.4 EXPERIMENTAL HYPERPARAMETERS
For the adding task, a mean squared error (MSE) loss was used with the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 10-3. The g-LSTM time gate parameters were trained using a learning rate of 100. For both sMNIST and sCIFAR-10 datasets, the cross-entropy loss function was used along with the RMSProp optimizer (Tieleman & Hinton, 2012) with a learning rate of 10-3. Decay parameters of 0.5 and 0.9 were used for sMNIST and sCIFAR-10, respectively.

4 RESULTS
This section presents the experimental results that demonstrate the advantages of g-LSTM. Section 4.1 presents results that demonstrate the faster convergence properties of the g-LSTM over the

4

Under review as a conference paper at ICLR 2019

LSTM. Section 4.2 shows the trainability of the time gate parameters of the g-LSTM even when the parameters are initialized in an arbitrary way. Section 4.3 presents a modified loss function used during training to reduce the number of computes for the network update and Section 4.4 presents a new "temporal curriculum" learning schedule that allows g-LSTMs to help LSTMs converge faster.

4.1 FAST CONVERGENCE PROPERTIES OF G-LSTM
First, we look at the convergence properties of the g-LSTM, on the long-sequence adding task, the sMNIST task and the sCIFAR-10 task. Table 1 details the architectures used for networks in the experiments in this section. Similar to the architecture from Trinh et al. (2018), the recurrent layer of the sCIFAR-10 network is followed by two 256 unit fully-connected (FC) layers, where DropConnect (Wan et al. (2013)) (p = 0.5) is applied to the second FC layer. Also the kernel matrices in the LSTM networks were initialized in an orthogonal manner as described in (Cooijmans et al., 2016), while the g-LSTM network kernels required no such specific initialization.

Dataset
Adding (N=1000) Adding (N=2000) sMNIST sCIFAR-10

# units
110 110 110 128

Initialization

µ

 U (300, 700)
 U (500, 1500)  U (1, 784)  U (1, 1024)

40 40 250 650

Performance

g-LSTM LSTM

3.8 · 10-5 1.3 · 10-3
1.3%
41.1%

1.4 · 10-3 1.6 · 10-1
1.8%
41.8%

Table 1: Network architectures and performance for the convergence experiments in subsection 4.1. The performance metric is the final mean squared error (MSE) loss for the adding task, and the label error rate for both sMNIST and sCIFAR-10.

The test performances of these networks during the course of the training on different datasets are shown in Figure 3, while the corresponding final performance metrics at the end of training are shown in Table 1. From Figure 3, it is evident that the test loss of the g-LSTM decreases faster in training than the LSTM across all datasets.
Table 2 compares the performance of various networks including the g-LSTM and the baseline LSTM on sMNIST and sCIFAR-10 (from Table 1). The results show that the g-LSTM consistently performs better than the LSTM and has a similar performance to other state-of-the-art networks.

Network
LSTM (this work) g-LSTM
r-LSTM (Trinh et al., 2018) Zoneout (Krueger et al., 2017) IndRNN (6 layers) (Li et al., 2018) BN-LSTM (Cooijmans et al., 2016) Skip LSTM (Campos et al., 2017)

sMNIST
1.8% 1.3% 1.6% 1.3% 1.0% 1.0% 2.7%

pMNIST
8.4% 7.5% 4.8% 6.9% 4.0% 4.6%
-

sCIFAR-10
41.8% 41.1% 27.8%
-

Table 2: Comparison of label error rates across different networks.

4.2 TRAINABILITY OF THE TIME GATE PARAMETERS OF G-LSTM
As discussed in section 2, one of the main advantages of the g-LSTM over the PLSTM is that the time gate parameters can be trainable. In order to demonstrate this, we look at the performance of the g-LSTM network with different time gate parameter initializations. We concern ourselves with sequences of length 1000 for the adding task in this section. The time gate parameters are initialized in a way to temporally constrain the network so that it can only process for a short period of time. For example, a network with time gate parameters initialized with µ = 500 and  = 40 as in Figure 4(a) can only process a short period of time around the middle of sequence. It follows
5

Under review as a conference paper at ICLR 2019

10-1

g-LSTM LSTM

10-1

g-LSTM LSTM

Test loss

Test loss

10-3

10-3

10-5 0

200 400 Epoch Number

600

(a) adding task (N = 1000)

10-5 0

100 200 300 400 Epoch Number

(b) adding task (N = 2000)

500

Test Label Error Rate

0.8 0.6 0.4 0.2 0.0
0

g-LSTM LSTM

50 100 150 Epoch Number

200

Test Label Error Rate

0.8 0.7 0.6 0.5 0.4
0

g-LSTM LSTM
20 40 60 80 100 Epoch Number

(c) sMNIST

(d) sCIFAR-10

Figure 3: Test loss and test label error rates across training epochs for LSTM (black) and g-LSTM (blue) networks on different tasks.

that the network would be unable to learn with these parameters because in the adding task the input data is distributed equally across the sequence length (T = 1000). Therefore, in order to learn the task from this initialization, the time gate parameters must learn a distribution such that the gates over all hidden units are open across the entirety of the sequence.
We observe that the time gate parameters do learn, as shown in Fig. 4 (b), and thereby enable the network to solve the task. Independent of various time gate initializations, the network reaches a MSE of around 3.9 × 10-5 at the end of 700 epochs; details of which could be found in Appendix B. The ability of the network to learn the time gate parameters necessary to cover the entire sequence is especially significant because it shows that even with this narrow time window initialization that requires learning of the time gates, the g-LSTM learns the task, whereas the PLSTM does not learn the task as well. An example of this is shown in Appendix C.

1.0 1.0 100 100
0.8 0.8 75 75
0.6 0.6
50 0.4 50 0.4
25 0.2 25 0.2

00 250 500 750 1000 0.0 00 250 500 750 1000 0.0 time time

(a) Pre-training

(b) Post-training

Figure 4: Time gate behavior pre and post training, demonstrating the ability of the network to learn from extreme initialization parameters.

6

hidden units hidden units

Under review as a conference paper at ICLR 2019

hidden units

4.3 REDUCTION IN COMPUTATION

Although the formulation of the g-LSTM appears to require more updates, it offers substantial speedup as a large proportion of the neurons can be skipped in a timestep at runtime. We can set a threshold on the time gate so that we skip all corresponding computations for time steps where kn is below this threshold. To further reduce the number of operations, it is preferred that the  of the kn for different neurons should be small but the network performance should not be significantly degraded. To achieve this goal, we included a "computational budget" loss term during the optimization of the gate parameters, µ and . The loss equation for updating the kn parameters is given by:
L = Ldata + Lbudget.
Similar to the Skip RNN network (Campos et al., 2017), a budget loss term which minimizes the average openness of the time gate over time is applied:

NJ

Lbudget = E[kn]  

kn(j)

n=1 j=1

for every neuron j of the g-LSTM. The study was carried out on sMNIST using a network with 110 units,  initialized to 50, µ initialized uniformly at random between 1 to 784, and a  value of 1. The network's performance of 97.8% accuracy was comparable to the network's performance of 98.7% when no additional budget constraint was imposed. The final  range for the budgeted g-LSTM is much smaller compared to that of the g-LSTM as shown in Fig. 5. There is only a small drop in accuracy for the budgeted g-LSTM versus the g-LSTM (see Table 2), even though there is a significant decrease in the average time gate openness across all hidden units.

hidden units

1.0 1.0 100 100
0.8 0.8 75 75
0.6 0.6
50 0.4 50 0.4
25 0.2 25 0.2

00 200 400 600 time
(a) g-LSTM

0.0 00

200 400 time

600

(b) budgeted g-LSTM

0.0

Figure 5: Time gate behavior of (a) g-LSTM and (b) budgeted g-LSTM for 110 units post training.

Test label error rate

0.03
0.02
0.01 0.00

100
LER OPS
10-1 0.02 0.04 0.06 0.08 0.10
Threshold value

Fraction of recurrent ops Test Label Error Rate

0.8 0.7 0.6 0.5 0.4
0

curriculum no-curriculum
20 40 60 80 100 Epoch Number

Figure 6: Reduction in computes as a func- Figure 7: Speed up in convergence of LSTM us-

tion of threshold for budgeted g-LSTM.

ing the temporal curriculum learning schedule.

In order to reduce the number of computes, we set a threshold, vT for kn so that the update steps are carried out only if kn > vT . By increasing vT , the number of computes decreases as shown in Fig. 6. In the case of vT = 0.01, only 8.2% of the time gates are open on average across all

7

Under review as a conference paper at ICLR 2019

hidden units and the 784 time steps. Furthermore, the accuracy dropped only slightly to 97.7% from 97.8%. For the remaining time and units, the LSTM update equations can be "skipped" over and the previous neuron state can be copied over as the current state.
We give a quantitative estimate for the number of operations (Ops) corresponding to the number of update equations for a g-LSTM. In the estimate, we count a multiply and an add operation as 1 Op and non-linear functions as 5 Ops. For an LSTM, the number of operations is given by
NLST M = T H(8D + 8H + 29)
where T is the number of time steps, H is the number of hidden units, and D is the dimension of the input data. For a g-LSTM, the number of operations is given by
Ng-LST M = NLST M + Ngate + Ngauss
where Ngate = 4 T H is the number of operations for computing the time gate following Eqs. 4 and 5, and Ngauss = 9 T H is the number of operations for computing the Gaussian function of the time gate. When vT = 0, the total number of operations for the g-LSTM network on the sMNIST dataset is around 80 MOps for N = 110 and T = 784, whereas this number is reduced to 7.6 MOps for the budgeted g-LSTM with vT = 0.01.

4.4 TEMPORAL CURRICULUM TRAINING SCHEDULE FOR LSTMS

We demonstrate that it is possible to train a LSTM network by using a "temporal curriculum" train-

ing schedule for the equivalent g-LSTM network. According to this schedule, the initial  values of

the g-LSTM network are increased continuously throughout the training period ending up with high

values by the end of training. With such high values, the time gates are essentially open, resulting

in an LSTM network. At every training epoch, the lowest % of the  values, ^ in the layer are

updated as,

^ - (1 + ) · ^

(8)

We analyze the impact of this training schedule for training an LSTM network on sCIFAR-10. The equivalent g-LSTM network with 110 units was considered with µ initialized uniformly at random between 1 and 1024 and  initialized to 50. An  value of 1/6 and  value of 15% was chosen. To ensure that the time gate is fully open by the end of training,  is set to 5000 across all units during the last 10 epochs of training. The learning rate of the time gate parameters is set to 0, i.e. µ and  are no longer updated. Figure 7 shows that the temporal curriculum training schedule allows for faster convergence of an LSTM network. The final weights of the trained g-LSTM network can then be copied over to a LSTM network for inference.

5 CONCLUSION
This work proposes a novel RNN variant with a time gate which is parameterized by the input in time. The model uses a time gate similar to the PLSTM, but unlike the periodic function for the PLSTM, the g-LSTM uses a Gaussian function which allows better training properties on long input sequences. The g-LSTM network shows faster convergence and produce higher accuracies than LSTM networks, as demonstrated for the adding task sequences which are longer than 1000 timesteps, and on the sMNIST and sCIFAR-10 datasets.
We further demonstrate that the time gate parameters of the g-LSTM (unlike those of the PLSTM) are learnable even when the time gates are initialized in an extreme manner for the adding task. The time gate of the g-LSTM can reduce the number of computes that is needed for the updates of the LSTM equations and with an additional loss term to reduce the compute budget, the  values of the time gate are reduced leading to a 10× decrease in number of actual computes and little loss in network accuracy, for the SMNIST dataset. We also show that our proposed temporal curriculum training schedule for the g-LSTM can help a corresponding LSTM network to converge on training for long sequences.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Victor Campos, Brendan Jou, Xavier Giro i Nieto, Jordi Torres, and Shih-Fu Chang. Skip RNN: learning to skip state updates in recurrent neural networks. CoRR, abs/1708.06834, 2017. URL http://arxiv.org/abs/1708.06834.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael J. Witbrock, Mark Hasegawa-Johnson, and Thomas S. Huang. Dilated recurrent neural networks. CoRR, abs/1710.02224, 2017. URL http://arxiv.org/abs/1710.02224.
Kyunghyun Cho, Bart van Merrie¨nboer, C¸ alar Gu¨lc¸ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN Encoder­Decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1724­1734, Doha, Qatar, October 2014. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ D14-1179.
Tim Cooijmans, Nicolas Ballas, Cesar Laurent, and Aaron C. Courville. Recurrent batch normalization. CoRR, abs/1603.09025, 2016. URL http://arxiv.org/abs/1603.09025.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735­ 1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx. doi.org/10.1162/neco.1997.9.8.1735.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Jan Koutnik, Klaus Greff, Faustino J. Gomez, and Ju¨rgen Schmidhuber. A clockwork RNN. CoRR, abs/1402.3511, 2014. URL http://arxiv.org/abs/1402.3511.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced Research). 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html.
David Krueger, Tegan Maharaj, Janos Kramar, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and Christopher Pal. Zoneout: Regularizing RNNs by randomly preserving hidden activations. 2017. URL https://openreview.net/ forum?id=rJqBEPcxe.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, Nov 1998. ISSN 0018-9219. doi: 10.1109/5.726791.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network (IndRNN): Building a longer and deeper RNN. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5457­5466, 2018.
Zachary Chase Lipton. A critical review of recurrent neural networks for sequence learning. CoRR, abs/1506.00019, 2015.
Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased LSTM: Accelerating recurrent network training for long or event-based sequences. In Advances in Neural Information Processing Systems, pp. 3882­3890, 2016.
Ju¨rgen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85­117, 2015. doi: 10.1016/j.neunet.2014.09.003. Published online 2014; based on TR arXiv:1404.7828 [cs.NE].
Ohad Shamir. Distribution-specific hardness of learning neural networks. CoRR, abs/1609.01037, 2016. URL http://arxiv.org/abs/1609.01037.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26­ 31, 2012.
9

Under review as a conference paper at ICLR 2019 Trieu H. Trinh, Andrew M. Dai, Thang Luong, and Quoc V. Le. Learning longer-term dependencies
in RNNs with auxiliary losses. CoRR, abs/1803.00144, 2018. URL http://arxiv.org/ abs/1803.00144. Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. Regularization of neural networks using dropconnect. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 1058­1066, Atlanta, Georgia, USA, 17­19 Jun 2013. PMLR. URL http://proceedings.mlr.press/v28/wan13.html. Adams Wei Yu, Hongrae Lee, and Quoc V. Le. Learning to skim text. CoRR, abs/1704.06877, 2017. URL http://arxiv.org/abs/1704.06877.
10

Under review as a conference paper at ICLR 2019

A BACK PROPAGATION IN GAUSSIAN-GATED RNN

For ease of illustration we analyze the gradient of a plain RNN with a Gaussian time gate (Eqs. 9 and 10).

hn = kn · h~n + (1 - kn) · hn-1

(9)

h~n = f (Wxxn + Whhn-1)

(10)

hN Wh

=

 h~ 0 Wh

N NN

(knWhfn + (1 - kn)) + (knfnhn-1)

(ksWhfs + (1 - ks))

n=1

n=1

s=n+1

(11)

where

 h~ 0 Wh

= 1, h0

= h~0

= 0.

From Eq. 11 we can deduce some information about the advantages of the Gaussian time gate in gradient flow for two simple cases of the function kn.
In Case 1 we choose a timing gate openness which corresponds to a very small  for the Gaussian gate, i.e. the gate is only open for 1 time step.

k5 = 1, kn = 0  n  {1, ..., N }\{5}

hN Wh

=

f5

+ f5f5h4

=

f5Wh (1 + f5h4)

In Case 2 we choose a timing gate openness which corresponds to a slightly larger  for the Gaussian gate, i.e. it is open for 5 time steps.
k2 = 1, k3 = 1, k4 = 1, k5 = 1, k6 = 1, kn = 0  n  {1, ..., N }\{2, 3, 4, 5, 6}

hN Wh

=

f2Whf3Whf4Whf5Whf6Wh (1 + f2h1

+ f3h2

+ f4h3

+ f5h4

+ f6h5)

These cases show that there are fewer terms in the gradient for a timing gate that is opened for only a small percentage of the sequence.

11

Under review as a conference paper at ICLR 2019

B COMPARING VARIOUS G-LSTM INITIALIZATIONS

Experiment ID A1 A2 A3

Initialization

µ
 U (300, 700)  U (0, 400)  U (600, 1000)

 1 40 40

Final MSE Loss
4.4 · 10-4 2.0 · 10-5 4.0 · 10-4

Table 3: Adding task (T=1000): 110 unit g-LSTM network initializations and performances.

hidden units

hidden units

1.0 100
0.8 75
0.6 50 0.4
25 0.2
00 250 500 750 1000 0.0 time
(a) Experiment A1: before training

hidden units

1.0 100
0.8 75
0.6 50 0.4
25 0.2
00 250 500 750 1000 0.0 time
(b) Experiment A1: after training

1.0 100
0.8 75
0.6 50 0.4
25 0.2
00 250 500 750 1000 0.0 time
(c) Experiment A2: before training

hidden units

1.0 100
0.8 75
0.6 50 0.4
25 0.2
00 250 500 750 1000 0.0 time
(d) Experiment A2: after training

hidden units

1.0 1.0 100 100
0.8 0.8 75 75
0.6 0.6
50 0.4 50 0.4
25 0.2 25 0.2

00 250 500 750 1000 0.0 time

00 250 500 750 1000 0.0 time

(e) Experiment A3: before training

(f) Experiment A3: after training

Figure 8: Additional experiments: timing gate openness for non-optimal initializations

hidden units

12

Under review as a conference paper at ICLR 2019

C COMPARING TIME GATE PARAMETER TRAINABILITY IN G-LSTM AND PLSTM

Network

Initialization

Final MSE Loss

g-LSTM

µ  U (300, 700),  = 40

7.7 · 10-5

PLSTM  = 1000, s  U (250, 650), r = 0.10 2.4 · 10-4

Table 4: Adding task (T=1000): Comparing similar initializations of 110 unit g-LSTM and PLSTM network, trained for 500 epochs.

hidden units

hidden units

1.0 1.0 100 100
0.8 0.8 75 75
0.6 0.6
50 0.4 50 0.4
25 0.2 25 0.2

00 250 500 750 1000 0.0 time

00 250 500 750 1000 0.0 time

(a) before training

(b) after training (500 epochs)

Figure 9: g-LSTM

hidden units

0
25
50
75
100 0 250 500 750 time (s)
(a) before training

hidden units

1.0 0
0.8 25
0.6 50 0.4
75 0.2
100 0.0 0 250 500 750
time (s)
(b) after training (500 epochs)
Figure 10: PLSTM

1.0 0.8 0.6 0.4 0.2 0.0

13

