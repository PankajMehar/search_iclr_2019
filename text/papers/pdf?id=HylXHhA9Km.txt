Under review as a conference paper at ICLR 2019
STATISTICAL CHARACTERIZATION OF DEEP NEURAL NETWORKS AND THEIR SENSITIVITY
Anonymous authors Paper under double-blind review
ABSTRACT
Despite their ubiquity, it remains an active area of research to fully understand deep neural networks (DNNs) and the reasons of their empirical success. We contribute to this effort by introducing a principled approach to statistically characterize DNNs and their sensitivity. By distinguishing between randomness from input data and from model parameters, we study how central and non-central moments of network activation and sensitivity evolve during propagation. Thereby, we provide novel statistical insights on the hypothesis space of input-output mappings encoded by different architectures. Our approach applies both to fully-connected and convolutional networks and incorporates most ingredients of modern DNNs: rectified linear unit (ReLU) activation, batch normalization, skip connections.1
1 INTRODUCTION
While the empirical success of deep neural networks is not disputed anymore, a full understanding of these models is not yet achieved (Zhang et al., 2016; Dinh et al., 2017; Neyshabur et al., 2017). As no exception to this rule, advances in the design of neural network architectures have more often come from the relentless race of practical applications rather than by principled approaches. Consequently many common practices and rules of thumb are still awaiting for theoretical validation. An important obstacle in the characterization of neural networks is the complex interplay of different sources of randomness. In that respect, despite winning successes both theoretically (Poole et al., 2016; Schoenholz et al., 2016; Yang & Schoenholz, 2017; Pennington et al., 2018) and empirically (Pennington et al., 2017; Xiao et al., 2018), the mean-field theory of neural networks fails to distinguish between the randomness from input data and model parameters. As a result, input data is only modeled in the rudimentary case of two correlated signals with Gaussian pre-activation distribution. In Balduzzi et al. (2017) input data is similarly modeled using two correlated signals with typical activation patterns. Another path of research considers input data as a 1-dimensional manifold of evolving length and curvature (Poole et al., 2016; Raghu et al., 2017). All cases are limited in their scope or simplifying assumptions. In this paper, we introduce a novel approach to statistically characterize deep neural networks and their sensitivity. Only mild assumptions are required and the usual simplifications of infinite width, gaussianity or typical activation patterns are not made. Both fully-connected and convolutional networks are encompassed and the commonly used techniques of batch normalization and skip connections are incorporated. The key of our methodology is to consider statistical moments with respect to input data as random variables which depend on model parameters. By studying how different architecture choices influence the evolution with depth of these moments, we provide statistical insights on the corresponding hypothesis spaces of input-output mappings. Our findings span the topics of pseudo-linearity, exploding sensitivity, exponential and power-law evolution with depth.
2 PROPAGATION
We start by formulating the propagation for neural networks with neither batch normalization nor skip connections, that we refer as vanilla networks. The formulation will be slightly adapted in section 6 with batch-normalized feedforward nets, and in section 7 with batch-normalized resnets.
1Code to reproduce all results will be made available upon publication.
1

Under review as a conference paper at ICLR 2019

Clean propagation. Suppose that we are given a random tensorial input x  Rn×···×n×N0 which
is spatially d-dimensional with spatial extent of n in each direction and N0 channels. We further suppose that this input is not trivially zero such that Ex,,c[x2 ,c] > 0,2 where  denotes the spatial position and c the channel. This input is fed into a d-dimensional convolutional neural network with periodic boundary conditions and constant spatial extent n.3 For each layer, we denote Nl the number of channels or width, Kl the convolutional spatial extent, l  RKl×···×Kl×Nl-1×Nl the weight tensors, bl  RNl the biases, and xl, yl  Rn×···×n×Nl the tensors of post-activations and pre-activations. We further denote  the activation function and we adopt the convention x0  x. The propagation at each layer l is given by xl = (l  xl-1 + l), where l  Rn×···×n×Nl is the tensor with repeated version of bl at each spatial position. From now on, we refer to the propagated tensor xl as the signal.

Noisy propagation. Next we suppose that the input signal x is corrupted by a small white noise tensor  Rn×···×n×N0 with independent and identically distributed components such that E [ i j] =
2ij ( 1), with ij the Kronecker delta. The noisy signal is propagated in the same neural
network and we keep track of the noise corruption with the tensor l  l(x + ) - l(x), where l is the input-output mapping xl = l(x). Again with the convention 0  , the simultaneous propagation of the clean signal xl and the noise l is given by

yl = l  xl-1 + l, xl = (yl),

(1)

l = l  l-1  (yl),

(2)

where denotes the element-wise tensor multiplication and Eq. (2) is obtained by taking the

derivative in Eq. (1). As shown by Eq. (2), for given x the mapping from to l is linear. The noise

l thus stays centered with respect to during propagation with , c: E

l ,c

= 0.

To get rid of the dependence on  , we introduce the random sensitivity tensor as the rescaling of the noise with unit initial variance: s0  s  /  and sl  l /  . Due to the linearity of Eq. (2), the sensitivity tensor sl is the result of the simultaneous propagation of xl and sl in Eq. (1) and (2). We also have Es [sisj] = ij and , c: Es sl ,c = 0. The sensitivity tensor encodes derivative information while avoiding the burden of increased dimensionality (see computation in Appendix
D.1 for an illustration). This will prove very useful.

Further scope. We restrict our analysis to the ReLU activation function: (yl) = max(yl, 0). This is partly due to lack of space and partly because ReLU networks are the most widely used in
practice. Note however that the formulation with convolutional neural networks does not exclude fully-connected networks, obtained simply as a subcase with n = 1.

3 INPUT DATA RANDOMNESS ­ MOMENTS, NORMALIZED SENSITIVITY
3.1 INPUT DATA RANDOMNESS
To understand the importance of the input data distributions Px(xl) and Px,s(sl), let us adopt a geometrical perspective on Eq. (1) and Eq. (2) in the context of classification. The neural network is fed a noisy point cloud with different classes spread throughout space. Its goal is layer after layer to modify this point cloud in Rn×···×n×Nl in order to better group points from the same class and separate points from different classes. This continues until the point cloud reaches the final linear separation. The evolution of the point cloud and its derivative across layers, Px(xl) and Px,s(sl), is of crucial importance since it characterizes the internal neural network machinery.
Note the distinction between Px(xl), Px,s(sl) on one side, and Px,,(xl), Px,s,,(sl) on the other side. As an illustration, consider a neural network which has shrunk its input x to a point mass distribution Px(xl-1) = pl-1 . For given l and l, the propagation of Eq. (1) maps this distribution to another point mass Px(xl) = pl . On the other side, Px,,(xl) has density spreading in the whole ambient space Rn×···×n×Nl and misses the distributional pathology.
2Whenever  and c are considered as random variables they are supposed uniformly sampled among all spatial positions {1, . . . , n}d and all channels {1, . . . , Nl}.
3The assumptions of periodic boundary conditions and constant spatial extent n are made for simplicity of the analysis. Possible relaxations are discussed in section C.2.

2

Under review as a conference paper at ICLR 2019

3.2 MOMENTS

In order to keep track of Px(xl), Px,s(sl), our next challenge is the tensorial structure of xl and sl. In a similar way as batch normalization, we consider feature maps at different spatial positions
as interwoven sub-signals of the tensorial meta-signal. Statistically, we work at the granularity
of sub-signals and we treat equally the randomness of the input tensors x, s and the spatial position . This brings the definition of the feature map vector and centered feature map vector of xl as

fm(xl, )  xl ,:,

^fm(xl, )  xl ,: - Ex, xl,: ,

where  is uniformly sampled in {1, . . . , n}d and the denotation fm(xl, ) reminds the randomness of both xl and . For any order p, the non-central moment and central moment of xl per-channel and averaged over channels are defined as

p,c(xl)  Ex, fm(xl, )pc , p(xl)  Ex,,c fm(xl, )pc ,

µp,c(xl)  Ex, ^fm(xl, )cp , µp(xl)  Ex,,c ^fm(xl, )pc .

The previous definitions are further extended to any random tensor.

3.3 NORMALIZED SENSITIVITY

Normalized sensitivity. Using the moments of xl and sl we finally define our key metric for the statistical characterization of neural networks that we refer as the normalized sensitivity l:

1/2

l 

µ2(sl) µ2(x0) µ2(xl)

,

(3)

where µ2(sl) measures sensitivity and µ2(xl), µ2(x0) measure signal informativeness. Again in the classification task where the goal is to set apart different signals, informativeness is measured by µ2(xl) since a constant shift applied to all signals is uninformative. This is summarized by the property of l to measure an expected sensitivity when neural network input and output signals are
rescaled to have unit variances (proof and visual illustration in Appendix D.2).

Normalized Sensitivity and Signal-to-Noise. Now let us push further the view of noisy propagation with the terminology of signal-to-noise ratio SN Rl and noise factor F l:

SN Rl 

slignal nl oise

2

=

µ2(xl) µ2( l)

,

Fl



SN R0 SN Rl

=

µ2(sl) µ2(x0) µ2(xl)

=

( l )2 ,

where we used the definitions of µ2(xl), µ2( l) and µ2( l) = 2µ2(sl) and µ2( 0) = 2. In logarithmic decibel scale, SN RdlB = SN Rd0B - 20 log10 l. The normalized sensitivity l then directly measures how the neural network degrades (l > 1) or enhances (l < 1) the input signal-to-noise ratio. The factor of noise equivalence means that neural networks with high l are essentially noise amplifiers.
Normalized Sensitivity and Generalization. The relevance of l is further supported by several studies relating sensitivity and generalization for fully connected networks (Sokolic et al., 2017; Novak et al., 2018; Philipp & Carbonell, 2018). Notably the coefficient defined in Philipp & Carbonell (2018) is equivalent to the normalized sensitivity l in the fully-connected case (details on equivalence and reasons for our change of terminology in Appendix D.1).

4 MODEL PARAMETERS RANDOMNESS
We now introduce model parameters as the second source of randomness. We only consider untrained networks at initialization. Due to the randomness of the initialization point and its independence from input data distribution, this can be seen as characterizing the hypothesis space of input-output mappings encoded by the architecture. We assume that initialization is standard: (i) weights and biases are initialized following He et al. (2015), (ii) when pre-activations are

3

Under review as a conference paper at ICLR 2019

batch-normalized, scale and shift batch normalization parameters are initialized with ones and zeros respectively.
Our methodology from now on is to consider all moment-related quantities such as µp(xl), µp(sl), p(xl), p(sl), l as random variables depending on (W 1, b1, . . . , W l, bl). We introduce the notation l = (1, 1, . . . , l, l) for the full set of parameters and the notation l = l|l-1 for the conditional set of parameters, when (l, l) are considered as random and (1, 1, l-1, l-1) as given. Furthermore we denote µl2(x) the geometric increments such that log µ2(xl) = log µ2(xl) - log µ2(xl-1).

Evolution with Depth. We are now able to write the evolution with depth of µ2(xl) as

log µ2(xl) - log µ2(x0) = +

k log Ek [µ2(xk)] + k Ek [log µ2(xk)] - log Ek [µ2(xk)]

k log µ2(xk) - Ek [log µ2(xk)],

(4)

where Eq. (4) is obtained using log µ2(xl) - log µ2(x0) = k log µ2(xk) and by decomposing each term in the sum with telescoping terms. Let us define rµ2(xk) such that µ2(xk) = rµ2(xk) Ek [µ2(xk)], and denote m, m, s the three different terms in Eq. (4):

m[µ2(xk)]  log Ek [µ2(xk)],

(5)

m[µ2(xk)]  Ek [log µ2(xk)] - log Ek [µ2(xk)] = Ek [log rµ2(xk)],

(6)

s[µ2(xk)]  log µ2(xk) - Ek [log µ2(xk)] = log rµ2(xk) - Ek [log rµ2(xk)], (7)

where Eq. (6) is obtained by entering log Ek [µ2(xk)] into the conditional expectation and the logarithm, and Eq. (7) is obtained using Ek [ Ek [log µ2(xk)]] = Ek [log µ2(xk)].

Discussion. First we note that m[µ2(xk)] and m[µ2(xk)] are random variables which depend on k-1 while s[µ2(xk)] is a random variable which depends on k. We also note that m[µ2(xk)] < 0 by log-concavity and that s[µ2(xk)] is centered: Ek [s[µ2(xk)]] = 0.

Under standard initialization, each channel provides an independent contribution to

µ2(xk) = Ex,,c ^fm(xk, )c2 . As a consequence, for large Nk the relative increment rµ2(xk)

has low expected deviation to 1, meaning with high probability that | log rµ2(xk)|

1,

|m[µ2(xk)]| 1, |s[µ2(xk)]| 1. In addition, s[µ2(xk)] is centered and non-correlated at different k so its sum scales as l, whereas the sums of m[µ2(xk)] and m[µ2(xk)] scale as l (see

Lemma 10 in Appendix E.1). The term s[µ2(xk)] is thus doubly negligible. In summary, the

evolution with depth is dominated by m[µ2(xk)] when this term is non-vanishing and by m[µ2(xk)]

otherwise. The exact same analysis can be applied to 2(xl) and to sensitivity moments 2(sl),

µ2(sl). It can also be applied to the ratio µ2(sl) / µ2(xl) and thus to l.

Further notation. From now on, the geometric increment of any quantity is denoted with . The

definitions of m, m and s in Eq. (5), (6) and (7) are extended to other central and non-central

moments of signal and sensitivity as well as l with m[l]

=

1 2

m[µ2(sl)]

-

m[µ2(xl)]

,

m[ l ]

=

1 2

m[µ2(sl)] - m[µ2(xl)]

, s[l] =

1 2

s[µ2(sl)] - s[µ2(xl)]

.

The approximation of dominating terms such as m[µ2(xk)] in Eq. (4) is denoted with . To make it precise, we write a b when a(1 + a) = b(1 + b) with |a| 1, |b| 1 with high probability. We write a b when a(1 + a)  b(1 + b) with |a| 1, |b| 1 with high probability. From

now on we further assume that the width is large. We stress that this assumption is milder than

mean-field since dominating terms such as m[µ2(xk)] in Eq. (4) remain random variables and since we do not require the signal xl to be Gaussian.

5 VANILLA NETWORKS
We are now fully equipped to statistically characterize neural network architectures. We start by analyzing vanilla networks corresponding to the equations of propagation introduced in section 2. Theorem 1. Moments of vanilla networks. (proof in Appendix E.2) Denote Al the event
2(xl) > 0 and Al the complementary event 2(xl) = 0 = Px,,c xl ,c = 0 = 1 . Then:
4

Under review as a conference paper at ICLR 2019

(i)

l k=1

1 - 2-Nk

 P[Al] 

l k=1

1 - 2-KkdNk-1Nk

(ii) There exist positive constants mmin, mmax, vmin, vmax > 0 and sequences of random variables

(ml), (ml), (sl), (sl) such that under Al, sl, sl are centered and 

log 2(xl) = -lml + lsl + log 2(x0), mmin  ml  mmax, vmin  Varl|Al [sl]  vmax,

log µ2(sl) = -lml + lsl,

mmin  ml  mmax, vmin  Varl|Al [sl]  vmax.

Discussion. (i) is related to the collapse of ReLU networks, which is studied in Lu et al. (2018). Any

vanilla ReLU network almost surely collapses to 0 but the number of required layer is exponential

in the width Nl. In practice, it is not a real problem.



(ii) implies that moments of xl and sl can be written 2(xl) = 2(x0) exp(-lml + lsl) and

µ2(sl) = exp(-lml + lsl). This behaviour comes from the particularity of He et al. (2015)

to keep stable El Ex,,c[(xl ,c)2] = El [2(xl)] and El Ex,,c[(sl ,c)2] = El [µ2(sl)] during

propagation, which results in vanishing m[2(xl)] and m[µ2(sl)]. The dominating terms in Eq. (4)

are then m[2(xk)] < 0 and m[µ2(sk)] < 0 (see Appendix E.3 for details). The small negative

drift and the increasing variance of log 2(xl) and log µ2(sl) is clear in Fig. 1d and Fig. 1e. It is

also clear that the distribution of log 2(xl) and log µ2(sl) is nearly Gaussian and therefore the

distribution of 2(xl) and µ2(sl) is nearly lognormal. Since a lognormal distribution exp(µ + X)

with X  N (0, 1) has expectation equal to exp(µ + 2/2), the increasing variance of log 2(xl)

and log µ2(sl) must be compensated by small negative drift in order for their exponential 2(xl)

and µ2(sl) to have stable expectations El [2(xl)] and El [µ2(sl)]. Note that the diffusion happens

in log-space since layer composition amounts to multiplicative random effect in real space.

As a consequence of (ii) and Chebyshev's inequality, conditionally on Al the variables 2(xl) and µ2(sl) still converge in probability to 0 (proof in Appendix E.4). He et al. (2015) thus manages to stabilize expectations with respect to all realizations l. However in practice we only see a single realization l and for large l this leads with high probability to vanishing network signal
and sensitivity (i.e. activations and gradients). Note that this is a finite-width effect and the terms m[2(xl)], m[µ2(sl)], s[2(xl)], s[µ2(sl)] also vanish in the limit of infinite width.

Theorem 2. Normalized Sensitivity increments of vanilla networks. (proof in Appendix F.1)

Under Al-1, the dominating term in the evolution of the normalized sensitivity is:

l

exp mvanilla l

=

1 - Ec,l|Al-1

1,c yl,+ 1,c yl,- µ2(xl-1)

-1/2
,

(8)

where yl,+ = max(yl, 0) and yl,- = max(-yl, 0).

Discussion. An immediate consequence of Theorem 2 is that l 1, meaning that normalized
sensitivity always increases with depth for ReLU vanilla networks. To further understand the behaviour of l we proceed by contradiction. Suppose that there is an event D with probability P[D] > 0 under which log l has a drift larger than the diffusion. Under D the ratio µ2(xl) / 2(xl) then converges in probability to 0 and the variance µ2(xl) becomes arbritrary smaller than the average magnitude 2(xl) = Ex,[||xl||22] / Nl (proof in Appendix F.2). All inputs x are then mapped to a very localized region and the distribution of xl resembles that of a single point. In
turn, this implies that with high probability a given pre-activation channel will have all its values
concentrated around either a single positive or a single negative value. This means that with high probability either y:l,,c+ = 0 for nearly all x, , or y:l,,c- = 0 for nearly all x, , and thus 1,c(yl,+)1,c(yl,-) / µ2(xl-1) 1 and l 1. This contradicts the presence of the drift larger than the diffusion in log l.
The previous argument shows that l 1 for large l and small diffusion, i.e. large width.
This is further supported by the results of our experiments in Fig. 1a and Fig. 1b. A direct consequence of l 1 is that the ratio 1,c(yl,+)1,c(yl,-) / µ2(xl-1) in Eq. (8) is constrained to small values, which leaves two possibilities for the signal distribution that we illustrate qualitatively:
(i) If max 1,c(yl,+), 1,c(yl,-) 2 / µ2(xl-1)  0, then 1,c(|y|)2 / µ2(xl-1)  0 and firstorder standardized moments become ill-defined.

5

Under review as a conference paper at ICLR 2019

(ii) If min 1,c(yl,+), 1,c(yl,-) 2 / µ2(xl-1)  0, then fm(yl, ) becomes concentrated on the semi-line generated by its average vector (1,c(yl))1cNl (proof in Appendix F.3). In this case, the same pattern of activation is seen with probability one with respect to input data and the neural network becomes linear.
The situation (ii) is clearly visible in Fig. 1c. Our analysis provides a novel insight in this previously observed phenomenon of coactivation (Balduzzi et al., 2017). Note that the distributional pathology is severe since a 1-dimensional distribution at layer l implies a 1-dimensional input when taking the perspective of layers l > l.

1.150 a
1.125 1.100 1.075 1.050 1.025 1.000 0
2c
1

l 30 b

20

10

25 50 75 100 125 150 175 200 0 0

(1,c(y200)) (1,c(y200))

d
2(x50) 2(x100) 2(x150) 2(x200)

l
25 50 75 100 125 150 175 200
e
µ2(s50) µ2(s100) µ2(s150) µ2(s200)

00 1 2

10-5

10-2

101

10-2

100

102

Figure 1: Illustration of the distributional pathologies of vanilla networks with Nl = 128 and L = 200 layers. (a) Geometric increments of the normalized sensitivity l with rapid evolution towards l 1. (b) The normalized sensitivity l has sub-exponential evolution since it is limited by neural network pseudo-linearity. (c) 2-dimensional cuts of preactivation feature maps fm(y200, ) using the direction of average vector (1,c(y200)) and a random orthogonal direction (1,c(y200)). (d-e)
Evolution of the distributions of µ2(xl) and µ2(sl) with depth showing clear lognormality.

6 BATCH-NORMALIZED FEEDFORWARD NETS

Next we incorporate batch normalization (Ioffe & Szegedy, 2015) which has the effect of subtracting 1,c(yl) and normalizing by µ2,c(yl)1/2 for each channel c in yl. The equations of propagation are given by

yl = l  xl-1 + l, zl = BN (yl),

xl = (zl),

(9)

tl = l  sl-1,

ul = BN (yl) tl, sl =  (zl) ul,

(10)

where BN denotes batch normalization and where we introduced the tensors zl, tl and ul. Note
that Eq. (9) and Eq. (10) explicitly formulate a finer-grained subdivision of three different steps between layers l - 1 and l in the simultaneous propagation of (xl, sl).

Theorem 3. Normalized Sensitivity increments of batch-normalized feedforward nets. (proof in Appendix G.1) The dominating term in the evolution of l can be decomposed as the sum of a term mBN [l] due to batch normalization and a term m[l] due to the nonlinearity :

exp mBN l

=

µ2(sl-1) µ2(xl-1)

-1/2
Ec,l

1/2

µ2,c(tl) µ2,c(yl)

,

(11)

-1/2

exp m l = 1 - 2Ec,l 1,c zl,+ 1,c zl,-

,

(12)

l exp mBN/F F l = exp mBN l + m l .

(13)

6

Under review as a conference paper at ICLR 2019

a
1.3 1.2 1.1 1.0 0
103 c
102
101 0

l -  l - BN

l - BN + 

1014 b
1011

108

105

102

25 50 75 100 125 150 175 200

0

reff (xl )

reff (sl )

103 d

102

101

100

25 50 75 100 125 150 175 200

0

l

25 50 75 100 125 150 175 200

1(|zl|)

µ4(zl)

25 50 75 100 125 150 175 200

Figure 2: Illustration of the distributional pathology of batch-normalized feedforward nets with Nl = 384 and L = 200 layers. (a) Geometric increments l and their decomposition as the product of two terms: a batch normalization term corresponding to the evolution from (xl-1, sl-1) to (zl, ul) and a nonlinearity term corresponding to the evolution from (zl, ul) to (xl, sl). (b) The normalized sensitivity l has exploding behaviour. (c) Effective ranks of signal and sensitivity
confirm that sensitivity is much better conditioned than signal. There is a clear inverse correlation between reff(xl) and the batch normalization term in l. (d) zl becomes ill-behaved as 1(|zl|) vanishes and µ4(zl) explodes. This explains the decay of the nonlinearity term in l.

Effect of batch normalization. The term of Eq. (11) corresponds to the evolution of l from (xl-1, sl-1) at layer l - 1 to (zl, ul) just after BN . To understand this term qualitatively, the pre-activation tensor yl can be seen as Nl random projections of xl-1, and batch normalization
can be seen as an alteration of the magnitude for each projection. Given that batch normalization
uses µ2,c(yl)1/2 as normalization factor, directions of high signal variance are dampened while directions of low signal variance are amplified. This preferential exploration of low signal directions naturally deteriorates the signal-to-noise ratio and amplifies l due to the factor of noise equivalence.

Now let us look directly at the quantity inside the expectation in Eq. (11). By spherical symmetry under standard initialization, geometric increments from xl-1 to yl for the signal and sl-1 to tl for the sensitivity have the same expectation Ec,l [µ2,c(tl)] / µ2,c(sl-1) = Ec,l [µ2,c(yl)] / µ2,c(xl-1). On the other hand, the fluctuation of these geometric increments depends on the fluctuation of
the signal and sensitivity in the Nl random projections, i.e. on whether directions of signal and sensitivity variances are rare in the ambient space. To measure this effect of conditioning, we adopt
the metric of effective rank reff from Vershynin (2012):

reff(xl)



Tr C fm(xl, ) , ||C fm(xl, ) ||

reff(sl)



Tr C fm(sl, ) , ||C fm(sl, ) ||

(14)

whith C fm(xl, ) , C (fm(sl, ) the covariance matrices of signal and sensitivity feature map

vectors and || · || the spectral norm. We further extend this definition to any random tensor. If we

assume that sl has very good conditioning with reff(sl-1) Nl-1, then µ2,c(tl) has small relative

deviation to its expectation Ec,l [µ2,c(tl)] and this term can be treated as a constant. In turn, this

implies by convexity of x  1/x that exp mBN l

1. The worse the conditioning of xl-1,

i.e. the smaller reff(xl-1), the larger the variance of µ2,c(yl) / Ec,l [µ2,c(yl)] at the denominator

and the impact of the convexity. Thus the smaller reff(xl-1) and the larger exp mBN l . This

argument is strictly valid for the first step of the propagation where the sensitivity has perfect

conditioning, which results in exp mBN 1  1 (proof in Appendix G.2). In Fig. 2c we confirm

experimentally that reff(sl) Nl reff(xl). Together with Fig. 2a we also confirm that reff(xl) is highly predictive of the batch normalization effect on l.

7

Under review as a conference paper at ICLR 2019

Effect of the nonlinearity . The term of Eq. (12) corresponds to the evolution of l from (zl, ul) after BN to (xl, sl) after . The quantity inside the expectation can also be expressed as 1,c zl,+ 1,c zl,- / µ2,c(zl) since µ2,c(zl) = 1 after batch normalization. We then find a very similar expression as Eq. (8) for vanilla networks. The difference is that first-order moments are now normalized using µ2,c(zl) instead of µ2(xl-1). This implies that each random projection in yl is given the same importance in the sense of similar contribution to l. On the con-
trary, Eq. (8) for vanilla networks gives more importance to random directions with high signal since 1,c yl,+ 1,c yl,- / µ2(xl-1) is small for low signal directions. Note that 1,c |zl|) = 1,c zl,+ + 1,c zl,- implies 2 1,c zl,+ 1,c zl,-  1,c |zl|)2 when taking the square. The
term m l is thus limited by Ec,l 1,c(|zl|)2 as shown by the joint examination of Fig. 2a and Fig. 2d.

7 BATCH-NORMALIZED RESNETS

We finish our exploration of DNN architectures with the incorporation of skip connections. We now suppose that the width is constant, i.e. Nl = N ,4 and following He et al. (2016) we adopt the
perspective of pre-activation units. The propagation inside residual units is given by

yl,h = l,h   BN yl,h-1 + l,h,

(15)

tl,h = l,h  tl,h-1 BN yl,h-1  BN yl,h-1 ,

(16)

where Eq. (15) and Eq. (16) hold for 1  h  H, with H the number of layers in each residual unit. Denoting (yl,h, tl,h) = l,h yl,h-1, tl,h-1 , the propagation between successive residual
units is given by

(yl, tl) = (yl-1, tl-1) + (yl,H , tl,H ) = (yl-1, tl-1) + l,H . . . l,1 yl-1, tl-1 .

(17)

For consistency reasons, we rename the inputs of the propagation as y0  y, t0  t. We further adopt the convention that y0,H  y0, t0,H  t0 such that Eq. (17) can be rewritten as

l

(yl, tl) = (yk,H , tk,H ).

(18)

k=0

Theorem 4. Normalized Sensitivity increments of batch-normalized resnets. (proof

in Appendix H.3) Suppose that for all depth l we can bound the effective ranks rmin

reff(yl), reff(yl,H ), reff(tl), reff(tl,H ), the second-order central moment µ2,min µ2(yl,H ) µ2,max

and the feedforward increments inside residual units min

  l,h

max. Denote

min = (min)2H µ2,min - µ2,max / µ2,max and max = (max)2H µ2,max - µ2,min / µ2,min,

and further consider min, max such that min < min / 2 and max > max / 2.5 Then:

l

N rmin :

1

+

min l+1

1/2

l

1

+

max l+1

1/2
,

(19)

l 1 :

1 2

min

log

l

log l

1 2

max

log

l,

(20)

l 1 :

lmin

l

lmax .

(21)

Discussion. The evolution in Eq. (19) remains exponential inside residual units since min and max have an exponential dependence in H. However it is slowed down by the factor 1 / (l + 1) between successive residual units. This comes from the dilution of the residual path (yl,H , tl,H ) in the skip connection path (yl-1, tl-1) with ratio of signal variances µ2(yl,H ) / µ2(yl-1) scaling as 1 / l. If we remove the dilution effect in Eq. (19) by replacing l + 1 by 1 and if we set µ2,min = µ2,max,
4Again this assumption is only made for simplicity of the analysis. In practice, it holds at least approximately since Nl is only modified by very few units.
5Note that Theorem 4 has very mild assumptions. The assumption on effective ranks is not required in Eq. (20) and Eq. (21). Furthermore the assumption on µ2(yl,H ) is very reasonable since batch normalization controls signal variance at the beginning of layer H.

8

Under review as a conference paper at ICLR 2019

then we recover the feedforward evolution with (min)H l (max)H . The dilution is clearly visible as a side effect of the layer aggregation in Eq. (18): each residual unit l adds a new term of increased sensitivity, but its relative contribution to the aggregation becomes smaller and smaller with l, so it gets harder and harder for the model to grow l.
Since l becomes closer and closer to 1, its fluctuation eventually becomes dominant relatively to its expected deviation to 1. This explains why Eq. (19) only holds for small l. It continues however to hold statistically so that the bounds on log l = k log k in Eq. (20) correspond to the integration of the bounds in Eq. (19). A direct consequence of the dilution is thus the power-law evolution of l in Eq. (21) instead of the exponential evolution for feedforward nets. Equivalently, when Eq. (21) is written as exp min log l l exp max log l , the evolution of l for resnets is the same as the evolution of log l for feedforward nets. In words, the evolution with depth of resnets is the logarithmic version of the evolution with depth of feedforward nets. Up to some factor, an evolution from 100, to 1 000, and to 10 000 layers for resnets is equivalent to an evolution from 20, to 30, and to 40 layers for feedforward nets. Despite differences in the underlying assumptions, our results are reminiscent of the results in Philipp et al. (2017) on the role of the dilution to alleviate the exploding gradient problem, as well as the results in Yang & Schoenholz (2017) on the power-law evolution of mean-field resnet gradients.
A shown in Fig. 3, the slow power-law evolution ensures that all statistical quantities remain wellbehaved. The exponent in the power-law fit in Fig. 3d is set to  =  / 2 = (a2vH - 1) / 2, with av the feedforward increment averaged over the whole evolution. This shows that the bound in Eq. (21) is tight.

a
1.3 1.2

l,h -  l,h - BN

l,h - BN +  30 b
20

l l

1.1 10

1.0 0
c
103
102
101
100 0

100 200
reff (xl )

300 400
reff (sl )

100 200 300 400

500 0
6d
5 4 3 2 1 500 0 0

100 200
1(|zl|)

300 400
µ4(zl)

500

100 200 300 400 500

Figure 3: Illustration of the well-behaved evolution of batch-normalized resnets with Nl = 384 and L = 500 residual units of H = 2 layers. (a) Decomposing l,h as the product of two terms: a batch normalization term and a nonlinearity term. (b) The normalized sensitivity l has power law evolu-
tion. (c) Effective ranks of signal and sensitivity indicate that many directions of signal variance are preserved. (d) Moments 1(|zl|) and µ4(zl) indicate that zl has nearly Gaussian distribution.

8 CONCLUSION
This paper introduced a novel approach for the statistical characterization of deep neural networks and their sensitivity. Only very mild assumptions were required and most ingredients of modern DNNs were incorporated. The main scope restriction comes from our focus on the rectifier activation function. We expect however qualitatively similar results to hold for other activation functions.
Below is a summary of our key results: ­ For vanilla networks, He et al. (2015) only stabilizes second-order moments of activation and sensitivity in expectation. Depth propagation still induces an additive random walk with small negative drift in log-space. This results in slowly vanishing activations and gradients and the
9

Under review as a conference paper at ICLR 2019
inevitable convergence to a distributional pathology where the neural network becomes linear and its signal shrunk to a single dimension. ­ For batch-normalized feedforward nets, the exponential growth of sensitivity has two origins: on the one hand batch normalization which upweights low-signal pre-activation directions, on the other hand the nonlinear activation function . ­ Finally for resnets the sensitivity only grows as a power-law. Equivalently the evolution with depth of resnets is the logarithmic version of the evolution with depth of feedforward nets. The underlying phenomenon is the dilution of the residual path in the skip connection path with ratio of signal variances decaying as 1 / k. This ingenious mechanism is responsible for breaking the circle of depth multiplicativity which causes distributional pathology for vanilla networks and batch-normalized feedforward nets.
We hope that our methodology will open new perspectives in the statistical understanding of deep neural network architectures. We believe that it could also provide novel insights regarding model trainability and generalization.
REFERENCES
David Balduzzi, Marcus Frean, Lennox Leary, J. P. Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2017.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. ArXiv e-prints, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1026­1034. IEEE Computer Society, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Computer Vision ­ ECCV 2016, pp. 630­645. Springer International Publishing, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML'15, pp. 448­456, 2015.
L. Lu, Y. Su, and G. E. Karniadakis. Collapse of Deep and Narrow Neural Nets. ArXiv e-prints, 2018.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring generalization in deep learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5947­5956. 2017.
R. Novak, Y. Bahri, D. A. Abolafia, J. Pennington, and J. Sohl-Dickstein. Sensitivity and Generalization in Neural Networks: an Empirical Study. In International Conference on Learning Representations, 2018.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. CoRR, abs/1711.04735, 2017.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The emergence of spectral universality in deep networks. In AISTATS, volume 84 of Proceedings of Machine Learning Research, pp. 1924­1932. PMLR, 2018.
G. Philipp and J. G. Carbonell. The Nonlinearity Coefficient - Predicting Overfitting in Deep Neural Networks. ArXiv e-prints, 2018.
George Philipp, Dawn Song, and Jaime G. Carbonell. Gradients explode - deep networks are shallow - resnet explained. ArXiv e-prints, 2017.
10

Under review as a conference paper at ICLR 2019
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3360­3368. Curran Associates, Inc., 2016.
Maithra Raghu, Ben Poole, Jon M. Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. In ICML, volume 70 of Proceedings of Machine Learning Research, pp. 2847­2854. PMLR, 2017.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. CoRR, abs/1611.01232, 2016.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust large margin deep neural networks. IEEE Trans. Signal Processing, 65(16):4265­4280, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed Sensing: Theory and Applications, pp. 210­268. Cambridge University Press, 2012.
L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks. ArXiv e-prints, 2018.
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 7103­7114. Curran Associates, Inc., 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ArXiv e-prints, 2016.
A DETAILS OF THE EXPERIMENTS IN FIG. 1, FIG. 2 AND FIG. 3
All three experiments of Fig. 1, Fig. 2 and Fig. 3 were made on CIFAR10 with a random initial convolution of stride 2 reducing the spatial dimension to n = 16. In each case we considered the convolutional extent Kl = 3 and periodic boundary conditions. A few experiments with approximately statistics-preserving boundary conditions such as symmetric mirroring indicated qualitatively equivalent behaviour. Fig. 1 was obtained by considering width Nl = 128 and total depth L = 200. For 10 000 random initializations we randomly sampled 1 024 images and computed the evolution with depth of 2(xl) and µ2(sl). The distributions of 2(xl) and µ2(sl) were estimated with the empirical distributions on the 10 000 computations, which are shown in Fig. 1d and Fig. 1e. As for Fig. 1c, it was obtained with 2-dimensional cuts of preactivation feature maps fm(y200, ) for 1 randomly sampled point every 5 computations. Each time the cut was performed using the direction of average vector (1,c(y200)) and a random orthogonal direction (1,c(y200)). Fig. 2 was obtained by considering 1 000 batches of 64 randomly sampled images. Due to less demanding computations, we increased the width to a more realistic value Nl = 384. For simplicity, we always considered batch normalization in train mode. Solid curves correspond to the expectation over the 1 000 batches and Fig. 2a, Fig. 2c, Fig. 2d additionally show 1 intervals. Finally Fig. 3 was obtained by considering again 1 000 batches of 64 randomly sampled images with Nl = 384 and L = 500 residual units of H = 2 layers. Geometric increments l,h were computed in the feedforward propagation from (yl,h-1, tl,h-1) to (yl,h, tl,h) at layer h = 2 in residual unit l. Solid curves correspond again to the expectation over the 1 000 batches and Fig. 3a, Fig. 3c, Fig. 3d additionally show 1 intervals. We plan to release Jupyter notebooks to enable replication of our results upon publication.
11

Under review as a conference paper at ICLR 2019

B COMPLEMENTARY DEFINITIONS AND NOTATIONS

Receptive field mapping. Here we temporarily need to handle the mechanics of convolution. Let us consider the convolution at layer l of an input v  Rn×···×n×Nl-1 from layer l - 1. The output feature map of the convolution (wl  v),: at position   {1, . . . , n}d is obtained by the application of the convolution kernel wl over a local input region of size (KldNl-1), with Kld the spatial extent and Nl-1 the extent in the channel dimension. The local input region is called the receptive field of (wl  v) at spatial position .
The receptive field mapping associates an input v from layer l - 1 to RF (v). RF (v) is the tensor of Rn×···×n×KldNl-1 such that RF (v),: is the reshaped vectorial form of the receptive field of (wl  v) at spatial position . We denote Rl = KldNl-1 the dimensionality of RF (v),: and Icl the set of indices in RF (v),: corresponding to elements in channel c in the input v. Strictly speaking, RF depend on l, but this is implied by the argument so we write RF for simplicity.

Receptive field vectors. The receptive field vector rf and centered receptive field vector ^rf associated with an input v from layer l are random vectors which depend on v,  such that

rf(v, )  RF (v),: and ^rf(v, )  rf(v, ) - Ev,[rf(v, )],
where we kept the same denotation for the variable in the expectation, as a slight abuse of notation. Again rf and ^rf are strictly speaking dependent on l, but this is implied by the argument.

Statistics-preserving property. RF is statistics-preserving with respect to v if for any channel c and any index ic  Icl, the random variables RF (v),ic and v,c which depend on v,  have the same distribution RF (v),ic v, v,c.

Equation of Propagation. Using the definition of RF , the affine transformation from the receptive field RF (xl-1),: to the feature map in the next layer yl ,: is written as

yl ,: = W lRF (xl-1),: + bl,

(22)

where W l  RNl×Rl is the suitably reshaped matricial form of l. To lighten notation, we write yl = W lRF (xl-1) + bl as a short for the affine transformation of Eq. (22) occuring at all spatial positions . We have the following equivalence between the notations with receptive field and with convolutions:
W lRF (xl-1) + bl = l  xl-1 + l.

For vanilla networks, the simultaneous propagation of xl and sl is then written as

yl = W lRF (xl-1) + bl, xl = (yl),

tl = W lRF (sl-1),

sl =  (yl) tl.

(23) (24)

For batch-normalized feedforward nets, the simultaneous propagation of xl and sl is written as

yl = W lRF (xl-1) + bl, zl = BN (yl),

xl = (zl),

tl = W lRF (sl-1),

ul = BN (yl) tl, sl =  (zl) ul.

Covariance and Gramian. The Gramian operator G and covariance operator C associated with a random vector v  RN are defined as
G[v]  Ev vvT and C[v]  Ev vvT - Ev v Ev v T .
Note that the gramian and covariance of feature map and receptive field vectors are related by G[^fm(v, )] = C[^fm(v, )] = C[fm(v, )] and G[^rf(v, )] = C[^rf(v, )] = C[rf(v, )].

Symmetric propagation for vanilla networks. We define additional tensors obtained by symmetric propagation at each layer l. In the case of vanilla networks they are given by:

12

Under review as a conference paper at ICLR 2019

y~l = -W lRF (xl-1) - bl, x~l = (y~l),

~tl = -W lRF (sl-1),

~sl =  (y~l) ~tl.

By spherical symmetry, tensor moments have the same distribution with respect to l for both propagations. Furthermore , c, xl ,c + x~l ,c = yl ,c and xl,c 2 + x~l,c 2 = yl ,c 2 since xl ,c x~l ,c = 0. We deduce

c : 2,c(xl) + 2,c(x~l) = 2,c(yl),

(25)

Now consider the second-order moments of the sensitivity tensor and suppose first that x, s, , c, l-1 are fixed. We have the following identity:

(sl,c)2 + (~sl,c)2 = (tl ,c)2 (y,c)2 + (~tl,c)2 (y~,c)2 = (tl,c)2[ (y,c)2 +  (y~,c)2].

There are two possible cases depending on yl ,c = Wcl,:RF (xl-1),: + bl:
­ If ||RF (xl-1),:||22 = 0, then under standard initialization, Pl yl ,c = 0 = 1, and thus Pl  (yl)2 +  (y~l)2 = 1 = 1 and Pl (sl ,c)2 + (~sl ,c)2 = (tl,c)2 = 1.
­ If ||RF (xl-1),:||22 = 0, then the element-wise tensor multiplication of Eq. (24) at layer l - 1 implies ||RF (sl-1),:||22 = 0, and thus tl,c = 0, sl,c = 0, ~sl,c = 0.

In all cases, Pl (sl,c)2 + (~sl ,c)2 = (tl ,c)2 = 1. For given c, we now consider x, s,  as variables again and we get by Fubini's theorem:

E E 1l x,s,

(sl,c)2+(~sl,c)2=(tl ,c)2

= 1,

E E 1l x,s,

(sl ,c)2+(~sl ,c)2=(tl ,c)2

= 0,

Pl Px,s, (sl ,c)2 + (~sl,c)2 = (tl,c)2 = 0 = 1.

(26) (27)

where Eq. (27) is obtained by contradiction, since Pl Px,s, (sl ,c)2 + (~sl ,c)2 = (tl,c)2 > 0 > 0 would imply that Eq. (26) does not hold. We can relate the moments of sl, ~sl and tl by

2,c(sl) + 2,c(~sl) - 2,c(tl) 2 = Ex,s, (sl,c)2 + (~sl,c)2 - (tl ,c)2 2 = Ex,s, (sl,c)2 + (~sl ,c)2 - (tl,c)2 1 (sl,c)2+(~sl ,c)2=(tl ,c)2

2

 2,c (sl)2 + (~sl)2 - (tl)2 Px,s, (sl ,c)2 + (~sl ,c)2 = (tl,c)2 ,

where we used Cauchy-Schwarz inequality and the implicit assumption that the moment quantity is well-defined. Combined with Eq. (27), we then get

Pl 2,c(sl) + 2,c(~sl) = 2,c(tl) = 1.

Since sl, ~sl and tl are centered, 2,c(sl) + 2,c(~sl) = µ2,c(sl) + µ2,c(~sl) and 2,c(tl) = µ2,c(tl).

So we finally get for the event intersection c : µ2,c(sl) + µ2,c(~sl) = µ2,c(tl) =

Nl c=1

µ2,c(sl) + µ2,c(~sl) = µ2,c(tl) :

Pl c : µ2,c(sl) + µ2,c(~sl) = µ2,c(tl) = 1.

(28)

Symmetric propagation for batch-normalized feedforward nets. For batch-normalized feedforward nets, the symmetric propagation at each layer l is given by

y~l = -W lRF (xl-1) - bl, z~l = BN (y~l),

x~l = (z~l),

~tl = -W lRF (sl-1),

u~l = BN (y~l) ~tl, ~sl =  (z~l) u~l.

(29) (30)

13

Under review as a conference paper at ICLR 2019

BN in Eq. (29) and Eq. (30) use the statistics of y~l, so that tensor moments have the same distribution with respect to l for both propagations. We then simply have

z~l = -zl, u~l = -ul.

(31)

The exact same analysis as before gives: c : 2,c(xl) + 2,c(x~l) = 2,c(zl),

(32)

Pl c : µ2,c(sl) + µ2,c(~sl) = µ2,c(ul) = 1.

(33)

C STATISTICS-PRESERVING PROPERTY
C.1 CASE OF PERIODIC BOUNDARY CONDITIONS AND CONSTANT SPATIAL EXTENT n
Lemma 1. If convolutions have periodic boundary conditions and the global spatial extent n is constant, then RF is statistics-preserving with respect to any input v.
Proof. Fix a channel c in v, an index ic  Icl, and consider the tensors v:,c and RF (v):,ic  Rn×···×n. The index ic corresponds to a given convolution kernel position   {1, . . . , Kl}d. Furthermore under periodic boundary conditions, this fixed kernel position implies that each position  in RF (v),ic originates from a different position  in the tensor v ,c. Therefore the index mapping f :    from {1, . . . , n}d to {1, . . . , n}d is bijective. We then have RF (v),ic = vf(),c  v,c when RF (v),ic and v,c are seen as random variables which depend on  and v is given. In turn, this implies that RF (v),ic v, v,c, when they are seen as random variables which depend on v, .

Proposition 2. If convolutions have periodic boundary conditions and the global spatial extent n is constant, then RF is statistics-preserving with respect to xl-1 and sl-1.

Proof. This follows immediately from Lemma 1.

Corollary 3. For any c and ic  Icl, we have rf(xl-1, )ic x, fm(xl-1, )c and rf(sl-1, )ic x,s, fm(sl-1, )c. Since the cardinality |Icl| = Kld is the same for all channels c, it follows that

2(xl-1)

=

1 Nl-1

Tr G[fm(xl-1, )]

=

1 Rl

Tr G[rf(xl-1, )],

µ2(xl-1)

=

1 Nl-1

Tr C[fm(xl-1, )]

=

1 Rl

Tr C[rf(xl-1, )].

C.2 RELAXING THE ASSUMPTIONS ON BOUNDARY CONDITIONS AND CONSTANT SPATIAL
EXTENT
In this section, we detail possible relaxations of the assumptions on boundary conditions and constant spatial extent n. The global spatial extent is denoted nl when it is not constant.
C.2.1 CASE OF STATIONARY INPUTS Periodic extension. The periodic extension v~ of a random tensor v  Rn×···×n×N is defined as
v~1+k1n,...,d+kdn,c = v1,...,d,c, with (k1, . . . , kd)  Zd, and where 1, . . . , d are the d components of the spatial position  = (1, . . . , d)  {1, . . . , n}d.

Stationarity. The distribution of a random vector v~ is defined as stationary if, for any k and any
configuration of spatial positions (1, . . . , k) and channels (c1, . . . , ck), the joint distribution of v~+1,c1 , . . . , v~+k,ck is the same for all .

14

Under review as a conference paper at ICLR 2019

Lemma 4. If convolutions have periodic boundary conditions and the inputs x and s have stationary periodic extensions x~ and ~s, then their periodic extension x~l and ~sl remain stationary
during propagation.

Proof. First note that convolutions with periodic boundary conditions followed by periodic extensions on v are equivalent to convolutions on v~. This is also the case for componentwise operations such as batch normalization, nonlinear activation and their derivatives. So we can restrict
our attention to periodic extensions.

Consider a given spatial shift  and define the translation operator T, such that T(u~) = v~ with  , c : v~ ,c = u~+ ,c. It is easy to see that T commutes with convolutions as well as
componentwise operations such as batch normalization, nonlinear activation and their derivatives. It follows that T commutes with the input-output mapping l defined as (x~l, ~sl) = l(x~, ~s), and
thus we have

T(x~l, ~sl) = l(T(x~, ~s)),

(34)

where we adopted the notation T(u~, v~) = T(u~), T(v~) . Now consider a given k and
configuration of spatial positions (1, . . . , k) and channels (c1, . . . , ck) in x~l and ~sl. Due to limited convolutional spatial extent and due to Eq. (34), there exist a function  and a configuration of spatial positions (1, . . . , k ) and channels (c1, . . . , ck ) in x~ and ~s, such that we can write

x~l 1,c1 , . . . , x~l k,ck

=

(x~1

,c1

,

.

.

.

,

x~ k

,ck

),

x~l +1,c1 , . . . , x~l+k,ck

=

(x~+1

,c1

,

.

.

.

,

x~+ k

,c
k

),

~sl 1,c1 , . . . , ~slk,ck

=

(x~1

,c1

,

~s1

,c1

,

.

.

.

,

x~ k

,c
k

, ~s k

,c
k

),

~sl+1,c1 , . . . , ~sl +k,ck

=

(x~+1

,c1

,

~s+1

,c1

,

.

.

.

,

x~+ k

,ck

, ~s+ k

,ck

).

(35) (36) (37) (38)

By stationarity of x~ and ~s, the terms on the right-hand sides of Eq. (35), Eq. (36) have the same
distribution, and the terms on the right-hand sides of Eq. (37), Eq. (38) have the same distribution.
It follows that the terms on the left-hand sides of both pairs of equations have the same distribution, meaning that x~l and ~sl are stationary.

Lemma 5. If convolutions have periodic boundary conditions, then RF is statistics-preserving with respect to any input v which has stationary periodic extension v~ for any global spatial extent nl.
Proof. If v~ has stationary distribution, it means in particular that for any channel c, the distribution of v,c is the same for all   {1, . . . , nl-1}d. Fix a channel c and an index ic  Icl corresponding to a given convolution kernel position   {1, . . . , Kl}d. A given position  in the receptive field tensor RF (v),ic then corresponds to a given position  in the original tensor, such that RF (v),ic = v ,c. Since the distribution of v ,c does not depend on  , it follows that RF (v),ic v, v ,c for given  and random  , and thus that RF (v),ic v, v,c for random .
Proposition 6. If convolutions have periodic boundary conditions and the input x has stationary periodic extension x~, then RF is statistics-preserving with respect to xl and sl, for any global spatial extent nl.
Proof. This follows from Lemmas 4 and 5, and from the fact that the input sensitivity tensor has stationary periodic extension ~s due to its definition as a white noise tensor with independent and identically distributed components.

C.2.2 CASE nl Kl Proposition 7. If the convolution stride is one in most layers (i.e. nl-1 = nl in most layers) and the global spatial extent is much larger than the convolutional spatial extent nl Kl in most layers, then RF is approximately statistics-preserving with respect to xl-1 and sl-1, for any boundary conditions.
15

Under review as a conference paper at ICLR 2019

Proof. Fix a layer l-1 such that nl-1 = nl and nl Kl. Denote RF (p) the receptive field mapping

at layer l associated with periodic boundary conditions. Since nl-1 = nl Kl the receptive fields

RF (xl-1),:, RF (sl-1),: and RF (p)(xl-1),:, RF (p)(sl-1),: do not intersect boundary regions

for most , implying that RF (xl-1),: = RF (p)(xl-1),:, RF (sl-1),: = RF (p)(sl-1),:

for most . If we denote with F the cumulative distribution functions of any random vari-

able, this implies for any index ic that Fx, RF (xl-1),ic

Fx, RF (p)(xl-1),ic and

Fx,s, RF (sl-1),ic

Fx,s, RF (p)(sl-1),ic .

Since RF (p) is statistics-preserving with respect to xl-1 and sl-1 by Lemma 1, it follows

that for any channel c and index ic  Icl, we have Fx, RF (p)(xl-1),ic = Fx, xl-,c1

and Fx,s, RF (p)(sl-1),ic = Fx,s, sl-,c1 . We then deduce that Fx, RF (xl-1),ic

Fx, xl-,c1 and Fx,s, RF (sl-1),ic

Fx,s, sl-,c1 , meaning that RF is approximately

statistics-preserving for xl-1 and sl-1.

D DEFINITION AND PROPERTIES OF THE NORMALIZED SENSITIVITY

D.1 EQUIVALENCE WITH PREVIOUS DEFINITIONS

In the fully-connected case n = 1, Philipp & Carbonell (2018) recently introduced the following coefficient:

Ex ||J l||F2 Ec Varx[xc0] Nl Ec Varx[xcl ]

1/2
.

(39)

Let us prove the equivalence between the definitions of Eq. (3) and Eq. (39). In the fully-

connected case, the spatial position  can be ignored and tensors and feature map vectors

coincide as xl = fm(xl), sl = fm(sl) = ^fm(sl). When x is fixed, the input-output Jacobian

Jl

=

xl x0

 RNl×N0 directly summarizes the propagation of the noise

l = J l , and thus the

sensitivity sl = J ls. Due to the white noise property Es [sisj] = ij,

Ex,s

(slc)2 = Ex ||J l||F2 ,
c

Ex,s,c (scl )2

=

1 Nl Ex

||J l||F2

.

Here we clearly see the advantage of the sensitivity tensor to encode information on the Jacobian while avoiding increased dimensionality. Going back to our calculation, the definitions
µ2(sl) = Ex,s,c ^fm(sl)2c = Ex,s,c fm(sl)2c = Ex,s,c (slc)2 , µ2(xl) = Ex,c ^fm(xl)c2 = Ec Varx[xlc] ,

finally give the equivalence between the two definitions:

l =

µ2(sl) µ2(x0) µ2(xl)

1/2
=

Ex ||J l||F2 Ec Varx x0c Nl Ec Varx xlc

1/2
.

Philipp & Carbonell (2018) chose the terminology of nonlinearity coefficient for this metric. While our analysis unveils a strong relationship between l and the nonlinearity , it also reveals a strong
relationship with batch normalization which is still a linear operation. So we chose instead the
terminology of normalized sensitivity.

D.2 PROPERTY OF NORMALIZED SENSITIVITY
Proposition 8. The sensitivity tensor and the tensor xxl ,c containing for given , c the derivatives of xl,c with respect to x are related by Es (sl ,c)2 = ||xxl,c||22.

16

Under review as a conference paper at ICLR 2019

Proof of Proposition 8. Due to the definition of

l as a small corruption to xl,

l ,c

can

be

written

as a function of xxl ,c and the input noise ,

l ,c

=

xxl ,c,

,

where . denotes the standard dot product in input space. It follows from the definition of sl = l /  that sl,c = xxl ,c, s . Due to the white noise property Es [sisj] = ij, we then get

Es (sl ,c)2 = ||xx,c||22.

Proposition 9. Define v the rescaling by a constant factor of x with unit variance
Ex,,c[^fm(v, )2c ] = 1, and vl the rescaling by a constant factor of xl with unit variance Ex,,c[^fm(vl, )c2] = 1. When vl is considered as a function of v, l measures an expected sensitivity of ^fm(vl, ) as l = Ev,,c ||v^fm(vl, )c||22 1/2.

Proof of Proposition 9. First we work with the non-normalized x and xl. We can express the second-order central moment of s as

µ2(sl) = Ex,,cEs fm(sl, )2c = Ex,,cEs (sl,c)2 = Ex,,c ||xxl ,c||22 = Ex,,c ||xfm(xl, )c||22 ,

(40) (41)

where Eq. (40) follows from sl being centered and Eq. (41) follows from Proposition 8. Now for given c and , the difference fm(xl, )c - ^fm(xl, )c is constant with respect to xl, and thus with respect to x. The derivatives of fm(xl, )c and ^fm(xl, )c with respect to x are then equal for all x,  and c. Together with the definition of µ2(xl), we deduce the following:

µ2(sl) = Ex,,c ||x^fm(xl, )c||22 , µ2(xl) = Ex,,c ^fm(xl, )2c ,

l =

1/2

µ2(sl) µ2(x0) µ2(xl)

=

Ex,,c ||x^fm(xl, )c||22 Ex,,c ^fm(x, )c2 Ex,,c ^fm(xl, )2c

1/2
.

(42)

Defining vl = xl / Ex,,c ^fm(xl, )2c 1/2, v = x / Ex,,c ^fm(x, )2c 1/2, we get x^fm(vl, ) =
x^fm(xl, ) / Ex,,c ^fm(xl, )c2 1/2 and v^fm(vl, ) = x^fm(vl, ) Ex,,c ^fm(x, )2c 1/2. It follows that

Ev,,c ^fm(vl, )c2 = 1,

Ev,,c ^fm(v, )2c Ev,,c ||v^fm(vl, )c||22

= 1,

=

Ex,,c

||x^fm(xl, )c||22 Ex,,c Ex,,c ^fm(xl, )2c

^fm(x, )2c

.

(43)

Finally combining Eq. (42) and Eq. (43), we get l = Ev,,c ||v^fm(vl, )c||22 1/2.

Illustration. Let us consider the case of fully-connected networks with L layers, and with 1dimensional input N0 = 1 and 1-dimensional output NL = 1. Denote xL = (x) the original input-output mapping and vL = ~ (v) the rescaled input-output mapping. Proposition 9 then simply becomes L = Ev ~ (v)2 1/2. In Fig. 4 we show the resut of the propagation vL = (v) of an input v having a mixture of Gaussians distribution in three different cases:
­ In Fig. 4a the input v is propagated through L = 1 layer with sigmoid activation. After propagation, the expected derivative is low since each mode of the Gaussian mixture appears in a relatively flat region of the sigmoid activation. This is clearly shown by the input-output data points and by the histogram of the outputs.

17

Under review as a conference paper at ICLR 2019

­ In Fig. 4b the input v is propagated through L = 1 layer with linear activation.
­ In Fig. 4c the input v is propagated through L = 25 randomly initialized layers, with ReLU activation in the L - 1 first layers and linear activation in the final layer. After propagation, the expected derivative is high due to the erratic behavior of the input-output mapping.

2.0  L = 0.52

1.5

1.0

0.5

0.0
-2 -1
Input

0

12
Output

2  L = 1.00
1
0
-1
-2 -2 -1
Input

0

12
Output

19 18

L = 3.56

17

16

15

14

13

12 -2

-1

Input

0

12
Output

-2 0

20

1

2 -2 0

2 -2 0

2 -2 0

2 12 14 16 18

(a) (b) (c)

Figure 4: Illustration of the normalized sensitivity for fully-connected networks of L layers, with
1-dimensional input N0 = 1 and 1-dimensional output NL = 1. The distribution of the input v is a mixture of two Gaussians. We show the result of the propagation in three different cases: (a) L = 1 layer with sigmoid activation, (b) L = 1 layer with linear activation, (c) L = 25 randomly initialized layers, with Nl = 100 channels and ReLU activation for 1  l < L, and linear activation in the final layer l = L. Top: full input-output mapping (blue curve) and randomly sampled input-output data
points (red circles). Bottom: histograms of inputs and outputs.

E MOMENTS OF VANILLA NETWORKS

E.1 LEMMA ON THE SUM OF INCREMENTS
Lemma 10. Let Xk be a sequence of random variables which depend on k, and denote Yk = Ek [Xk] and Zk = Xk - Ek [Xk]. If there exist constants mmin, mmax, vmin, vmax with k  l: mmin  Yk  mmax and vmin  Vark [Zk]  vmax, then:

(i) The increments Zk are centered and non-correlated:

k  l : Ek [Zk] = 0,

k = k  l : Emax(k,k ) [ZkZk ] = 0.

(ii) There exist random variables ml and sl such that sl is centered and l Xk = lml + lsl, mmin  ml  mmax, vmin  Varl [sl]  vmax.
k=1

Proof of (i). First we show that Zk is centered:
Ek [Zk] = Ek [Xk] - Ek [Xk] = 0, Ek [Zk] = Ek-1 [Ek [Zk]] = 0.

(44)

Now consider k < k , which implies k  k - 1 and thus that Zk is a random variable which only depends on k -1. Then we can write

18

Under review as a conference paper at ICLR 2019

Ek [ZkZk ] = Ek E-1 k [ZkZk ] = Ek -1 Zk Ek [Zk ] = 0,
where Eq. (45) follows from Eq. (44).

(45)

Proof of (ii). Denote Ml =

l k=1

Yk

and

Sl

=

l k=1

Zk .

We

then

have

El Sl = El Sl2 =

k El [Zk] = 0, k,k El ZkZk ,

Varl Sl =

k Ek Zk2 =

k Vark Zk ,

(46)

where Eq. (46) follows from (i). The hypothesis then gives lmmin  Ml  lmmax and

lvmin  Varl [Sl]  lvmax. If we define ml = Ml / l and sl = Sl / l, then sl is centered and the

telescoping sum

l k=1

Xk

=

l k=1

Yk

+

l k=1

Zk

can

be

written

as

required:

l Xk = Ml + Sl = lml + lsl, mmin  ml  mmax, vmin  Varl [sl]  vmax.

k=1

E.2 PROOF OF THEOREM 1

Theorem 1. Moments of vanilla networks. Denote Al the event 2(xl) > 0 and Al the complementary event 2(xl) = 0 = Px,,c xl,c = 0 = 1 . Then:

(i)

l k=1

1 - 2-Nk

 P[Al] 

l k=1

1 - 2-KkdNk-1Nk

(ii) There exist positive constants mmin, mmax, vmin, vmax > 0 and sequences of random variables

(ml),

(ml),

(sl),

(sl) such 

that

under

Al,

sl,

sl

are

centered

and

log 2(xl) = -lml + lsl + log 2(x0), mmin  ml  mmax, vmin  Varl|Al [sl]  vmax,

log µ2(sl) = -lml + lsl,

mmin  ml  mmax, vmin  Varl|Al [sl]  vmax.

Proof of (i). We use the definitions and notations from section B. We further denote (e1, . . . , eRl ) and (1, . . . , Rl ) respectively the orthogonal eigenvectors and eigenvalues of G[rf(xl-1, )], and W^ l = W l(e1, . . . , eRl ). We then get

c : 2,c(yl) = Ex, (yl ,c)2 = Ex, Wcl,:rf(xl-1, ) 2

= i W^ cl,i 2i.

(47)

Given the assumption of standard initialization, biases are initialized with zeros and weights are
initialized as W l l W^ l l N (0, 2 / RlI). Under Al-1, it follows from Corollary 3 that Tr G[rf(xl-1, )] = Rl 2(xl-1) > 0 and thus Tr G[rf(xl-1, )] = i i > 0. Combined with Eq. (47), we get

c : Pl|Al-1 2,c(yl) = 0 = 0.

(48)

Now we introduce the symmetric propagation from section B and we denote Al,c = 2,c(xl) = 0 , A~l,c = 2,c(x~l) = 0 and Al,c = 2,c(xl) = 0 , A~l,c = 2,c(x~l) = 0 the complementary events. It follows from Eq. (48) and Eq. (25) that Pl|Al-1 [Al,c  A~l,c] = 0. Furthermore by spher-
ical symmetry of the propagation, 2,c(xl) l 2,c(x~l) and thus Pl|Al-1 [Al,c] = Pl|Al-1 [A~l,c]. We

19

Under review as a conference paper at ICLR 2019

then get

c : Pl|Al-1 Al,c  A~l+1,c

c : Pl|Al-1 Al,c



1 2

.

= Pl|Al-1 Al,c

+ Pl|Al-1 A~l+1,c

 1,

Since Al = c Al,c and since the events Al,c are independent conditionally on l-1 and Al-1, we conclude that Pl|Al-1 [Al] = c Pl|Al-1 [Al,c]  2-Nl .

The other side of the inequality is easier. If c, ic: Wcl,ic  0, it follows that x, , c: yl ,c < 0, xl ,c = (yl ,c)+ = 0, and thus 2(xl) = 0. Therefore Pl|Al-1 [Al]  2 ,-KldNl-1Nl since
KldNl-1Nl = RlNl is the number of elements in W l. We finally get

1 - 2-Nl  Pl|Al-1 Al  1 - 2 .-KldNl-1Nl

(49)

Since P[A0] = 1, due to the assumption 2(x) = Ex,,c[x2,c] > 0, it follows that 1 - 2-N1  P[A1]  1 - 2-K1dN0N1 . This proves (i) for l = 1.
Now we proceed by induction and suppose that (i) is true for given l - 1. Using Eq. (49), we get

l
1 - 2-Nk
k=1

 Pl Al = Pl-1 Al-1 Pl|Al-1 Al

l


1 - 2-KkdNk-1Nk

k=1

,

meaning that (i) is true for l. (i) is thus true for all l.

Proof of (ii) for 2(xl). Again we denote (e1, . . . , eRl ) and (1, . . . , Rl ) the eigenvectors and eigenvalues of G[rf(xl-1, )], and W^ l = W l(e1, . . . , eRl ). We further define

ulc =

2,c (xl )
2,c (xl )+2,c (x~ l )
0

if 2,c(xl) + 2,c(x~l) > 0 otherwise.

Combining the definition of ucl with Eq. (25) and Eq. (47), we get

c : 2,c(xl) = ucl 2,c(yl),

c : 2,c(xl) = ucl

i W^ cl,i 2i = Rl 2(xl-1) ucl

i W^ cl,i 2^i,

(50)

where we defined ^i = i / j j and used j j = Tr G rf(xl-1, ) = Rl2(xl-1) due to Corollary 3. The symmetric propagation gives

c : 2,c(x~l) = Rl 2(xl-1) (1 - ucl )

i - W^ cl,i 2^i,

c : 2,c(xl) + 2,c(x~l) = Rl 2(xl-1)

i W^ cl,i 2^i.

(51)

By symmetry of the propagation 2,c(xl) l 2,c(x~l). Combined with Eq. (51) and the assumption of standard initialization, we deduce

2El|Al-1 2,c(xl)

= El|Al-1 2,c(xl) + 2,c(x~l)

= El|Al-1 Rl 2(xl-1)

i W^ cl,i 2^i

=

Rl

2(xl-1)

2 Rl

^i = 22(xl-1).
i

We then obtain c : El|Al-1 [2,c(xl)] = 2(xl-1), and thus

El|Al-1 [2(xl)] = 2(xl-1),

El|Al-1 [2(xl)] = 1.

(52)

20

Under review as a conference paper at ICLR 2019

Both log x and (log x)2 are integrable at zero since log x dx = x log x - x and (log x)2dx = x(log x)2 - 2x log x + 2x. Combined with Eq. (50), we deduce that log 2(xl)
has well-defined conditional expectation and variance under Al. Furthermore by Eq. (49), Al has probability exponentially low in Nl, which gives

|El|Al [2(xl+1)] - 1| 1,

El|Al [log 2(xl+1)] < 0,

(53)

where Eq. (53) is obtained by log-concavity. We now write the evolution of 2(xl) as
l
log 2(xl) - 2(x0) = log 2(xk).
k=1
Let us define Xk = log 2(xk), Yk = Ek|Al [log 2(xk)] and Zk = log 2(xk) - Ek|Al [log 2(xk)] and apply Lemma 10 for each l conditionally on Al. Suppose there exist mmin, mmax, vmin, vmax, such that for each l we have conditionally on Al that k  l, mmin  -Yk  mmax and vmin  Vark|Ak [Zk]  vmax which implies vmin  Vark|Al [Zk]  vmax. Then we have k  l, -mmax  Yk  -mmin and by Lemma 10 there exist sequences of random variables (ml) and (sl) such that l under Al, sl is centered and
 log 2(xl) - log 2(x0) = lml + lsl, -mmax  ml  -mmin, vmin  Varl|Al [sl]  vmax.

By simply changing the variable ml to -ml, we get 
log 2(xl) - log 2(x0) = -lml + lsl, mmin  ml  mmax,

vmin  Varl|Al [sl]  vmax.

To obtain the bounds mmin, mmax, vmin, vmax, we consider extreme cases for ulc and i(W^ cl,i)2^i in Eq. (50). Denoting 2(N ) the chi-square distribution with N degree of freedom, we ob-

tain minimum bounds by considering ucl l 1/2 and i(W^ cl,i)2^i l 2(Rl). This leads to

log 2(xl) l 2(NlRl). Denoting Bern(1/2) the Bernouilli distribution with p = 1/2, we obtain

maximum bounds by considering uc l Bern(1/2) and on Al has highly negligible impact in practice.

i(W^ cl,i)2^i l 2(1). The conditionality

Let us give an example in the fully-connected case with constant width Nl = 100. We then find numerically mmin 9.7×10-5 and vmin 2.0×10-4 as minimum bounds and mmax 2.5×10-2 and vmax 5.2×10-2 as maximum bounds. The length scale is experimentally close to Lmax = 1/mmax 40 for 2(xl).

Proof of (ii) for µ2(sl). Let us denote Bl = k  l, c : µ2,c(sk) + µ2,c(~sk) = µ2,c(tl) and

Bl = k  l, c : µ2,c(sk) + µ2,c(~sk) = µ2,c(tl) =

l k=1

c :

µ2,c(sk) + µ2,c(~sk) =

µ2,c(tk) the complementary event. Eq. (28) gives k: Pk c : µ2,c(sk) + µ2,c(~sk) =

µ2,c(tk) = 0. Since Bl is the union of probability zero events, it follows that Pl (Bl) = 0,

Pl (Bl) = 1, and thus Pl (Bl) = 0, Pl (Bl) = 1.

Since Bl has probability 1, the conditionality on Bl leaves moments of random variables unchanged. To see this, consider the moment of order p of a random variable x:

El|Al [xp] - El|Al,Bl [xp] 2 =

El|Al [xp]

-

Pl

1
|Al

(Bl

)

El

|Al

[1Bl

xp

]

2

= El|Al [1Bl xp]2

 El|Al [x2p] Pl|Al [Bl] = 0,

(54)

where Eq. (54) is obtained with Cauchy-Schwarz inequality and the implicit assumption that x
has well-defined moment of order 2p. It follows that all arguments used in the proof of (i) remain valid, in particular regarding the distribution of W l. Conditionality on Bl, the proof then proceeds identically by simply replacing 2(xl) by µ2(sl), yl by tl, G by C, and using the identity with µ2
instead of 2 in Corollary 3. In particular, we have

21

Under review as a conference paper at ICLR 2019

El|Al-1,Bl [µ2(sl)] = µ2(sl-1),

El|Al-1,Bl [µ2(sl)] = 1.

(55)

Furthermore under Al Bl, for the same positive constants mmin, mmax, vmin, vmax > 0 as previously defined, there exist sequences of random variables (ml) and (sl) such that l under Al  Bl, sl is centered and
 log µ2(sl) = -lml + lsl, mmin  ml  mmax, vmin  Varl|Al,Bl [sl]  vmax,

where we used the fact that µ2(s0) = 1 by the definition of s. Now we extend ml and sl to Bl by setting ml to any value between mmin and mmax and sl such that log µ2(sl) = -lml+ lsl. The reasoning of Eq. (54) then ensures that El|Al,Bl [sl] = El|Al [sl] and El|Al,Bl [(sl)2] = El|Al [(sl)2], and Varl|Al,Bl [sl] = Varl|Al [sl]. It means that l under Al, sl is centered and
 log µ2(sl) = -lml + lsl, mmin  ml  mmax, vmin  Varl|Al [sl]  vmax.

E.3 RELATION TO THE TERMS m, m, o DEFINED IN SECTION 4
Here we relate Theorem 1 to the terms m, m, s defined in section 4. By Eq. (52), |Ek|Ak [2(xk)] - 1| 1, and thus:
|m[2(xk)]| = | log Ek|Ak [2(xk)]| |Ek|Ak [2(xk)] - 1| 1.
As in the proof of Theorem 1, we denote Bl = k  l, c : µ2,c(sk) + µ2,c(~sk) = µ2,c(tl) and Bl the complementary event. Then conditionally on Ak and Bk:
| log Ek|Ak,Bk [2(xk)]| |Ek|Ak,Bk [2(xk)] - 1| 1.
The reasoning of Eq. (54) can be applied to 2(xk), which results in Ek|Ak [2(xk)] = Ek|Ak,Bk [2(xk)]. Therefore we also get
|m[µ2(sk)]| = | log Ek|Ak [µ2(sk)]| 1.
The terms m[2(xk)] and m[µ2(sk)] are thus vanishing and the evolution is dominated by the terms m[2(xk)] < 0, m[µ2(sk)] < 0. These terms correspond to Yk in the proof of Theorem 1 (ii).

E.4 CONVERGENCE IN PROBABILITY TO ZERO

Corollary 11. Conditionally on Al the variables 2(xl) and µ2(sl) still converge in probability to zero:

 : Pl|Al |2(xl)| >  0,

 : Pl|Al |µ2(sl)| >  0.

Proof. Consider a given and the evolution of 2(xl). From theorem 1 (ii), we can write under Al: log 2(xl) = -lml + lsl + log 2(x0), with sl centered and mmin  ml  mmax, vmin  Varl|Al [sl]  vmax. Therefore:
 Pl|Al |2(xl)| > = Pl|Al log 2(xl) > log = Pl|Al lsl > lml + log - log 2(x0)

 Pl|Al

sl > 1 l

lml + log

- log 2(x0)

 Pl|Al

|sl|

>

1 l

lmmin + log

- log 2(x0)

.

(56)

Chebyshev's inequality on the centered random variable sl then gives:

22

Under review as a conference paper at ICLR 2019

Pl|Al |2(xl)| >



lmmin + log

l - log 2(x0)

2

Varl|Al [sl]



lmmin + log

l - log 2(x0)

2

vmax



1 vmax l m2min

where  denotes the equivalence for large l. It follows that Pl|Al |2(xl)| >  0. The same analysis applied to µ2(sl) gives Pl|Al |µ2(sl)| >  0.

F NORMALIZED SENSITIVITY INCREMENTS OF VANILLA NETWORKS

F.1 PROOF OF THEOREM 2

Theorem 2. Normalized Sensitivity increments of vanilla networks. Under Al-1, the dominat-

ing term in the evolution of the normalized sensitivity is:

l

exp mvanilla l

=

1 - Ec,l|Al-1

1,c yl,+ 1,c yl,- µ2(xl-1)

-1/2
,

(57)

where yl,+ = max(yl, 0) and yl,- = max(-yl, 0).

Proof.

The

dominating

term

in

the

evolution

of

l

is

1 2

m[µ2(sl)] - m[µ2(xl)

.

The terms

m[µ2(sl)] and m[µ2(xl) are simply obtained by considering El [µ2(sl)] and El [µ2(xl)].

By Eq. (53) in the proof of Theorem 1: El|Al-1,Bl [µ2(sl)] = 1. By replicating the reasoning of Eq. (54), this further gives El|Al-1 [µ2(sl)] = 1, and thus m[µ2(sl)] = log El|Al-1 [µ2(sl)] = 0.

Next we turn to the term m[µ2(xl)]. Again we use the definitions and notations from section B. We further denote (e1, . . . , eRl ) and (1, . . . , Rl ) respectively the orthogonal eigenvectors and eigenvalues of C[rf(xl-1, )] and W^ l = W l(e1, . . . , eRl ). Using these notations, we get

c : µ2,c(yl) = Ex, ^fm(yl, )c2 = Ex, Wcl,:^rf(xl-1, ) 2

= i W^ cl,i 2i.

(58)

Then due to W l l W^ l l N (0, 2 / RlI);

El|Al-1 [µ2,c(yl)]

=

2 Rl

i

i

=

2 Rl

Tr C[rf(xl-1, )]

= 2µ2(xl-1).

(59)

where Eq. (59) follows from Corollary 3. Furthermore the symmetric propagation gives:

µ2,c(xl) + µ2,c(x~l) = Ex,[(yl,+,c)2] - Ex,[yl,+,c]2 + Ex,[(yl,-,c)2] - Ex,[yl,-,c]2. = 2,c(yl,+) - 1,c(yl,+)2 + 2,c(yl,-) - 1,c(yl,-)2
= 2,c(yl) - 1,c(yl,+)2 + 1,c(yl,-)2

(60)

We have 1,c(yl) = 1,c(yl,+) - 1,c(yl,-) and thus 1,c(yl)2 = 1,c(yl,+)2 + 1,c(yl,-)2 - 21,c(yl,+)1,c(yl,-). We can then rewrite Eq. (60) as

µ2,c(xl) + µ2,c(x~l) = 2,c(yl) - 1,c(yl)2 - 21,c(yl,+)1,c(yl,-) = µ2,c(yl) - 21,c(yl,+)1,c(yl,-)

(61)

23

Under review as a conference paper at ICLR 2019

Combining Eq. (59) and Eq. (61):

El|Al-1 [µ2,c(xl) + µ2,c(x~l)] = 2µ2(xl-1) - 2El|Al-1 [1,c(yl,+)1,c(yl,-)], 2El|Al-1 [µ2,c(xl)] = 2µ2(xl-1) - 2El|Al-1 [1,c(yl,+)1,c(yl,-)],

(62)

El|Al-1 [µ2,c(xl)] = µ2(xl-1)

1 - El|Al-1

1,c(yl,+)1,c(yl,-) µ2(xl-1)

.

where Eq. (62) is obtained by spherical symmetry of the propagation. We finally get

El|Al-1 [µ2(xl)] = El|Al-1 Ec[µ2,c(xl)] = Ec El|Al-1 [µ2,c(xl)]

= µ2(xl-1)

1 - Ec,l|Al-1

1,c(yl,+)1,c(yl,-) µ2(xl-1)

El|Al-1 [µ2(xl-1)] = 1 - Ec,l|Al-1

1,c(yl,+)1,c(yl,-) µ2(xl-1)

.

,

Combined with El|Al-1 [µ2(sl)] = 1, l exp mvanilla l =

El|Al-1 [µ2(sl)] El|Al-1 [µ2(xl)]

1/2

=

1 - Ec,l|Al-1

1,c yl,+ 1,c yl,- µ2(xl-1)

-1/2
.

F.2 IF l HAS DRIFT LARGER THAN DIFFUSION, THEN µ2(xl) / 2(xl) CONVERGES IN
PROBABILITY TO ZERO

We only need to slightly adapt the reasoning of section E.4. From theorem 1, we can write

under Al: log 2(xl) = -lml + lsl + log 2(x0), with sl centered and mmin  ml  mmax,

vmin  Varl|Al [sl]  vmax. We can also write log µ2(sl) = -lml + lsl, with sl centered and

mmin  ml  mmax and vmin  Varl|Al [sl]  vmax. We further suppose that there is an event D

with P(D) > 0 under which l has drift larger than diffusion. To make it precise, this means that

m

>

1 2

(mmax

-

mmin)

such

that

for

l

large

enough:

log l



m,

and

thus

c



R

such

that

for

l: log l  c + lm.

The ratio µ2(xl) / 2(xl) can be expressed as

µ2(xl) 2(xl)

=

µ2(xl) µ2(sl)µ2(x0)

µ2(sl)µ2(x0) 2(xl)

=

1 ( l )2

µ2

(sl)µ2(x0 2(xl)

)

,

which gives with logarithms,

log

µ2(xl)

-

log

2(xl)

=

-2

log

l

+ log 

µ2(sl)

-

log 2(xl)

+

log

µ2(x0)



-2c

-

2lm - 

lmmin

+

lsl + lmmax -

lsl - log 2(x0) + log µ2(x0)

 C - lM + lsl,

where we denoted C = -2c - log 2(x0) + log µ2(x0), M = 2m + mmin - mmax > 0 and sl = sl - sl. The variance of sl is bounded as

El|Al [s2l ] = Varl|Al [sl] + Varl|Al [sl] - 2El|Al [slsl]

 Varl|Al [sl] + Varl|Al [sl] + 2Varl|Al [sl]1/2 Varl|Al [sl]1/2  4vmax

Varl |Al ,D [sl ]

=

El |Al ,D [sl2 ]

=

1 P(D)

El

|Al

[1D

s2l

]



1 P(D)

4vmax.

24

Under review as a conference paper at ICLR 2019

Now for given :

Pl |Al ,D

µ2(xl) 2(xl)

>

= Pl|Al,D log µ2(xl) - log µ2(xl) > log

 Pl|Al,D

sl

>

1 l

lM + log

-C

Chebyshev's inequality on the centered random variable sl further gives

Pl |Al ,D

µ2(xl) 2(xl)

>



l lM + log

- C 2 Varl|Al,D[sl]



l lM + log

-C

2

1 P(D)

4vmax



1 l

4vmax M 2P(D)

,

proving that under D the ratio µ2(xl) / 2(xl) converges in probability to zero.

F.3 LIMITS OF SIGNAL DISTRIBUTION

Proposition 12. Suppose that

Pc,l min 1,c(yl,+), 1,c(yl,-) = 0  max 1,c(yl,+), 1,c(yl,-) > 0 = 1.

Then fm(yl, ) concentrates on the semi-line generated by its average vector (1,c(yl))1cNl .

Proof. Let us consider the feature map vectors fm(xl-1, ) and receptive field vectors rf(xl-1, ). Due to the statistics-preserving property of Corollary 3, for each channel c and index ic  Icl, xl-,c1 x, RF (xl-1),ic and thus Ex,[fm(xl-1, )c] = Ex,[rf(xl-1, )ic ].

Now let us reason by contradiction and suppose that there exists a direction e which is orthogonal to Ex,[rf(xl-1, )ic ] 1 icRl and with non-zero variance v > 0. Consider the weight matrix W l which projects on direction e for given channel c. For this direction, we have 1,c(yl) = 0, 2,c(yl) > 0. In turn, this implies 1,c(yl,+) = 1,c(yl,-) and 1,c(yl,+) + 1,c(yl,-) > 0, which further gives min 1,c(yl,+), 1,c(yl,-) > 0. By continuity, min 1,c(yl,+), 1,c(yl,-) > 0
in a small neighborhood for the sampling of the weights W l, which contradicts the hypothesis. It follows that rf(xl, ) concentrates on the direction generated by Ex,[rf(xl-1, )ic ] 1 icRl .

Now consider the weight matrix W l which projects on the direction generated

by Ex,[rf(xl-1, )ic ] 1 icRl for given channel c.

A similar argument gives

min 1,c(yl,+), 1,c(yl,-) = 0, and thus that fm(yl, )c either concentrates in R+ or

concentrates in R-. It follows that rf(xl-1, ) concentrates on the semi-line generated by

Ex,[rf(xl-1, )ic ] 1 icRl . Under standard initialization, the image fm(yl, ) of rf(xl-1, ) by the affine transform fm(yl, ) = W lrf(xl-1, ) + bl = W lrf(xl-1, ) thus concentrates on

the semi-line generated by its average vector (1,c(yl))1cNl .

G NORMALIZED SENSITIVITY INCREMENTS OF BATCH-NORMALIZED
FEEDFORWARD NETS
G.1 PROOF OF THEOREM 3 Theorem 3. Normalized Sensitivity increments of batch-normalized feedforward nets. The dominating term in the evolution of l can be decomposed as the sum of a term mBN [l] due to batch normalization and a term m[l] due to the nonlinearity :
25

Under review as a conference paper at ICLR 2019

exp mBN l exp m l

=

µ2(sl-1) µ2(xl-1)

-1/2
Ec,l

1/2

µ2,c(tl) µ2,c(yl)

,

= 1 - 2Ec,l 1,c zl,+ 1,c zl,-

-1/2
,

l exp mBN/F F l = exp mBN l + m l .

Proof. First let us decompose the dominating term as the product of two terms:

1/2 -1/2

exp mBN l

=

El [µ2(ul)] µ2(sl-1)

El [µ2(zl)] µ2(xl-1)

,

1/2 -1/2

exp m l

=

El [µ2(sl)] El [µ2(ul)]

El [µ2(xl)] El [µ2(zl)]

,

1/2 -1/2

exp mBN/F F l

=

El [µ2(sl)] µ2(sl-1)

El [µ2(xl)] µ2(xl-1)

= exp mBN l exp m l .

mBN l is a dominating term in the evolution of l from (xl-1, sl-1) to (zl, ul), while m l is a
dominating term in the evolution of l from (zl, ul) to (xl, sl). These terms can be seen as the contribution to mBN/F F l of respectively batch normalization and . Now let us explicitate both terms.

Term exp mBN l . First we note that batch normalization directly gives µ2(zl) = 1 and thus

El [µ2(zl)] = 1. Now let us explicitate El [µ2(ul)]:

c :

ul:,c =

tl:,c , µ2,c(yl)1/2

c :

µ2,c(ul) =

µ2,c(tl) µ2,c(yl)

,

El [µ2(ul)] = Ec,l [µ2,c(ul)] = Ec,l

µ2,c(tl) µ2,c(yl)

All together, we get

1/2 -1/2

exp mBN l

=

El [µ2(ul)] µ2(sl-1)

El [µ2(zl)] µ2(xl-1)

=

µ2(sl-1) -1/2

1/2
µ2,c(tl)

µ2(xl-1)

Ec,l µ2,c(yl)

Term exp m l . We consider the symmetric propagation for batch-normalized feedforward

nets and again we denote Bl = k  l, c : µ2,c(sk) + µ2,c(~sk) = µ2,c(ul) =

l k=1

c :

µ2,c(sk) + µ2,c(~sk) = µ2,c(uk) and Bl the complementary event. By Eq. (33): Pl (Bl) = 1,

Pl (Bl) = 0, and replicating the reasoning of Eq. (54) we deduce

El [µ2(sl)] + El [µ2(~sl)] = El|Bl [µ2(sl)] + El|Bl [µ2(~sl)] = El|Bl [µ2(ul)] = El [µ2(ul)],

2El [µ2(sl)] = El [µ2(ul)],

(63)

where Eq. (63) follows from spherical symmetry of the propagation. Now we turn to the symmetric propagation of the signal:

µ2,c(xl) + µ2,c(x~l) = Ex,[(zl,+,c)2] - Ex,[zl,+,c]2 + Ex,[(zl,-,c)2] - Ex,[zl,-,c]2. = 2,c(zl,+) - 1,c(zl,+)2 + 2,c(zl,-) - 1,c(zl,-)2

(64)

= 2,c(zl) - 1,c(zl,+)2 + 1,c(zl,-)2 ,

26

Under review as a conference paper at ICLR 2019

where Eq. (64) follows from Eq. (31). Due to the constraints imposed by batch normalization, 1,c(zl) = 0 and 2,c(zl) = 1, it follows that

µ2,c(xl) + µ2,c(x~l) = 1 - 1,c(zl,+)2 + 1,c(zl,-)2 .

(65)

1,c(zl) = 1,c(zl,+) - 1,c(zl,-) = 0, 1,c(zl,+) - 1,c(zl,-) 2 = 1,c(zl,+)2 + 1,c(zl,-)2 - 21,c(zl,+)1,c(zl,-) = 0.

(66)

Using Eq. (65), Eq. (66) and the symmetry of the propagation, µ2,c(xl) + µ2,c(x~l) = 1 - 21,c(zl,+)1,c(zl,-),

2El [µ2(xl)] = 1 - 2Ec,l 1,c zl,+ 1,c zl,- .

(67)

We finally combine Eq. (63) and Eq. (67):

1/2

exp m Sl

=

El [µ2(sl)] El [µ2(ul)]

El [µ2(xl)] El [µ2(zl)]

= 1 - 2Ec,l 1,c zl,+ 1,c zl,-

-1/2
,
-1/2
.

G.2 exp mBN 1 > 1 IN THE FIRST STEP OF THE PROPAGATION

Let us explicitate the second-order moment in channel c of t1:

µ2,c(t1) = Ex,s, ^fm(t1, )2c = Ex,s, fm(t1, )c2 = Ex,s, Wc1,:rf(s, ) 2

(68)

= i,j Wc1,iWc1,j Es,[rf(s, )i rf(s, )j ] = i Wc1,i 2 = ||Wc1,:||22.

(69)

where Eq. (68) follows from t1 being centered and Eq. (69) follows from the white noise property Es[sisj] = ij, which implies for any  that Es[rf(s, )irf(s, )j] = ij under periodic boundary conditions.

Now we turn to the second-order moment in channel c of y1. Denoting (e1, . . . , eR1 ) and

(1, . . . , R1 ) respectively the orthogonal eigenvectors and eigenvalues of C[rf(x, )], and

W^ 1 = W 1(e1, . . . , eR1 ), we get

µ2,c(y1) = Ex, ^fm(y1, )c2 = Ex, Wc1,:^rf(x, ) 2 =

i W^ c1,i 2i

= ||Wc1,:||22

i W~ c1,i 2i = µ2,c(t1)

i W~ c1,i 2i,

where we defined W~ 1 such that c: W~ c1,: = W^ c1,: / ||Wc1,:|| and we used Eq. (69). Under standard initialization, the distribution of W 1 is spherically symmetric, implying that for all c the distribution

of W~ c1,: is uniform on the sphere of RR1 . In turn, this implies

i : E1

W~ c1,i 2

=

1 ,

R1

c : E1

i

W~ c1,i 2i

=1 R1

i,
i

Ec,1

i

W~ c1,i 2i

=1 R1

i.
i

(70)

Finally we can write exp mBN 1 as

exp mBN 1

=

µ2(s0) µ2(x0)

-1/2
Ec,1

1/2
µ2,c(t1) µ2,c(y1)

=

1 R1

1/2
i Ec,1
i

1/2

1

i W~ c1,i 2i

,



1 R1

1/2
i
i

Ec,1

1/2

i W~ c1,i 2i -1

= 1.

(71) (72)

27

Under review as a conference paper at ICLR 2019

where Eq. (71) is obtained using µ2(s0)

=

1 and µ2(x0)

=

1 R1

Tr

C [rf (x,

)]

=

1 R1

i i by

Corollary 3 while Eq. (72) is obtained using the convexity of x  1/x and Eq. (70).

H NORMALIZED SENSITIVITY INCREMENTS OF BATCH-NORMALIZED
RESNETS

H.1 ADAPTATION OF THE PREVIOUS SETUP TO RESNETS

Before proceeding to the analysis, slight adaptations and forewords are necessary. Let us denote l,h = (1,1, 1,1, . . . , 1,H , 1,H , . . . , l,1, l,1, . . . , l,h, l,h) for the full set of parameters up to layer h in residual unit l and l,h = l,h|l,h-1 for the conditional set of parameters of layer h in residual unit l. We further denote l = l,H and l = l,H |l-1,H respectively the full and
conditional sets of parameters at the granularity of the residual unit.

We now clarify to what extent Theorem 3 on batch-normalized feedforward nets still apply. First

let us rewrite the propagation at layer 1  h  H inside residual unit l with the pre-activation

perspective:

zl,h = BN (yl,h-1),

xl,h = (zl,h),

yl,h = l,h  xl,h + l,h, (73)

ul,h = BN (yl,h-1) tl,h-1, sl,h =  (zl,h) ul,h, tl,h = l,h  sl,h,

(74)

In the pre-activation perspective, each layer starts with (yl,h-1, tl,h-1) after the convolution and

ends at (yl,h, tl,h) again after the convolution. The concrete effect is that in the first layer h = 1 of each residual unit l, batch normalization and  are completely deterministic conditionally on l-1. This occurs again for h  2 since batch normalization and  are random conditionally on l-1 but completely deterministic conditionally on l,h-1. At even larger granularity, due to the aggregation

(yl, tl) =

l k=0

(yk,H

,

tk,H

),

the

input

signal

(yl-1, tl-1)

of

each

residual

unit

becomes

more

and more correlated between successive l and less and less dependent on the parameters l-k of

individual previous units.

Since batch normalization and the nonlinearity  are the drivers of the evolution of l, this shift can be thought as attributing the parameters and thus the stochasticity of layer h to the layer h - 1. A possible strategy is thus to consider the evolution from (xl,h-1, sl,h-1) to (xl,h, sl,h) for layers 2  h  H. This strategy however does not work for the first layer h = 1, since the input signal (yl-1, tl-1) depends on the whole sequence of parameters l-1 and not only on l-1,H . Another strategy consists in treating all moment-related quantities as deterministic instead of random. Sym-
metric propagation does not occur strictly in this case, but it still occurs in a mean-field sense when
averaging over channel. So we expect Theorem 3 to remain valid.

H.2 LEMMA ON DOT-PRODUCT

Lemma 13. For any random tensor u of Rn×···×n×N :

El Ey,,c ^fm(u, )c^fm(yl,H , )c El Ey,,c ^fm(u, )c^fm(yl,H , )c 2
El Ey,t,,c ^fm(u, )c^fm(tl,H , )c El Ey,t,,c ^fm(u, )c^fm(tl,H , )c 2

= 0,



N

1 reff

(u)

µ2

(u)El

[µ2

(yl,H

)],

= 0,



N

1 reff

(u)

µ2

(u)El

[µ2

(tl,H

)].

Proof. By spherical symmetry, moments of ^fm(yl,H , )c and -^fm(yl,H , )c = ^fm(-yl,H , )c have the same distribution with respect to l. It follows that

El Ey,,c ^fm(u, )c^fm(yl,H , )c El Ey,,c ^fm(u, )c^fm(yl,H , )c

= El Ey,,c ^fm(u, )c - ^fm(yl,H , )c = 0.

,

Next we note that

28

Under review as a conference paper at ICLR 2019

Ey,,c ^fm(u, )c^fm(yl,H , )c

=1 N

Ey, ^fm(u, )c^fm(yl,H , )c ,

c

=

1 N Ey,

^fm(u, )^fm(yl,H , )

,

(75)

where · denotes the standard dot product in RN . Let us denote (e1, . . . , eN ) and (1, . . . , N ) respectively the orthogonal eigenvectors and eigenvalues of C[^fm(u, )]. We further denote ui the unit-variance components of ^fm(u, ) in the basis (e1, . . . , eN ), and yi the components of ^fm(yl,H , ) in the basis (e1, . . . , eN ). This gives

^fm(u, ) =
i

iuiei, Ey, ui = 0, Ey, ui2 = 1,
^fm(yl,H , ) = yiei.
i

Now we decompose each component yi of yl,H as:

j : i,j = Ey,[yiuj ], From this definition, we get

yi = i,j uj + zi,
j

j : Ey, ziuj = 0,

Ey, yiui = i,i,

µ2(yl,H )

=

1 N

Ey,

yl,H , yl,H

=1 N

Ey, yi2

i

Ey, yi2 =

i2,j + Ey, zi2 ,

j

=1 N

i2,j +

Ey, zi2 .

i,j i

(76)

The dot product can be computed in any orthogonal basis, so we use the basis (e1, . . . , eN ):

Ey, ^fm(u, )^fm(yl,H , ) =
i

iEy,[yiui] =
i

ii,i.

Spherical symmetry implies that moments of y1e1 + · · · + yiei + · · · + yN eN and y1e1 + · · · - yiei + · · · + yN eN have the same distribution with respect to l. It follows that

j = i : Ey, yiui Ey, yj uj l Ey, - yiui Ey, yj uj , j = i : i,ij,j l (-i,i)j,j , j = i : El i,ij,j = 0.

We deduce that

El Ey, ^fm(u, )^fm(yl,H , ) 2 =

iEl i2,i .

i

Spherical symmetry also implies that the distribution of i,j with respect to l is the same for all i. Denoting (j) such that i, j: j = El i2,j , we get combined with Eq. (76):

El µ2(yl,H )

1 N

j = i,
i,j i

El Ey, ^fm(u, )^fm(yl,H , ) 2 =

ii  max

i

i  maxEl µ2(yl,H ) .
i

29

Under review as a conference paper at ICLR 2019

Finally combining with Eq. (75):

El Ey,,c ^fm(u, )c^fm(yl,H , )c 2

1 = N 2 El

Ey,

^fm(u, )^fm(yl,H , )

2



1 N 2 maxEl

µ2(yl,H )



N

1 reff

(u)

µ2

(u)El

µ2(yl,H )

,

where we used maxreff(u) = i i = N µ2(u). The same analysis can be applied to ^fm(u, ) and ^fm(tl,H , ).

Corollary 14. Let us denote the dot products for k, l  0 as
Yk,l = Ey,,c ^fm(yk,H , )c^fm(yl,H , )c , Tk,l = Ey,,c ^fm(tk,H , )c^fm(tl,H , )c ,
which combined with Eq. (18) implies
l-1
Yk,l = Ey,,c ^fm(yl-1, )c^fm(yl,H , )c ,
k=0 l-1
Tk,l = Ey,,c ^fm(tl-1, )c^fm(tl,H , )c .
k=0

Then by spherical symmetry k, l, Yk,l and Tk,l are centered, and {k, l} = {k , l }: Emax(k,l,k ,l ) [Yk,lYk ,l ] = 0, Emax(k,l,k ,l ) [Tk,lTk ,l ] = 0. Furthermore:

k < l : El Yk2,l

 El-1

N

reff

1 (yk,H

)

µ2

(yk,H

)El

[µ2

(yl,H

)]

,

k < l : El Tk2,l

 El-1

N

reff

1 (tk,H

)

µ2

(tk,H

)El

[µ2

(tl,H

)]

,

l : El l : El

l-1 2
Yk,l
k=0
l-1 2
Tk,l
k=0

 El-1  El-1

N

reff

1 (yl-1

)

µ2

(yl-1

)El

[µ2

(yl,H

)]

,

N

1 reff(tl-1

)

µ2

(tl-1

)El

[µ2

(tl,H

)]

.

(77) (78) (79) (80)

H.3 PROOF OF THEOREM 4

Theorem 4. Normalized Sensitivity increments of batch-normalized resnets. Suppose that for

all depth l we can bound the effective ranks rmin reff(yl), reff(yl,H ), reff(tl), reff(tl,H ), the

second-order central moment µ2,min µ2(yl,H ) µ2,max and the feedforward increments inside

residual units min

  l,h

max. Denote min = (min)2H µ2,min - µ2,max / µ2,max and

max = (max)2H µ2,max - µ2,min / µ2,min, and further consider min, max such that min < min / 2

and max > max / 2. Then:

l

N rmin :

1

+

min l+1

1/2

l

1

+

max l+1

1/2
,

(81)

l 1 :

1 2

min

log

l

log l

1 2

max

log

l,

(82)

l 1 :

lmin

l

lmax .

(83)

30

Under review as a conference paper at ICLR 2019

Proof. First we introduce the additional constants min = (min)2H and max = (max)2H , so that we can write min = minµ2,min - µ2,max / µ2,max and 2,max = maxµ2,max - µ2,min / µ2,min.

We also remind that the symbols in Theorem 4 denote inequalities up to small non-dominating terms. We write a b when a(1 + a)  b(1 + b) with |a| 1, |b| 1 with high probability. We write a b when a(1 + a) = b(1 + b) with |a| 1, |b| 1 with high probability. Denoting  for the logical and, the following rules are easily verified:
(a b)  (a b)  (a b),
(a b) = (-a -b),
(a b) = (1/a 1/b),
(a b)  (c d) = (ac bd),
(a b)  (b c) = a c.

Finally (a b)  (c d) = (a + c b + d) under the condition that |a + c| |a| + |c| and
|b + d| |b| + |d| are very small probability events. We keep these rules in mind in the course of this proof.

Proof of Eq. (81). Adopting the same notations as Corollary 14 and using yl =

l k=0

yk,H

by

Eq. (18), we get

µ2(yl) = Ey,,c µ2(tl) = Ey,,c

k,k ^fm(yk,H , )c^fm(yk ,H , )c = k,k ^fm(tk,H , )c^fm(tk ,H , )c =

k,k Yk,k . k,k Tk,k .

Now using the hypotheses µ2,min Eq. (77) from Corollary 14,

µ2(yl,H )

µ2,max and rmin

reff(yl,H ), combined with

(l + 1)µ2,min +

Yk,k
k=k

µ2(yl) (l + 1)µ2,max +

Yk,k ,
k=k

El

2
k=k Yk,k

l(l

+

1)

N

1 rmin

µ22,max,

El

k=k Yk,k

(l

+

1)

1 N rmin

µ2,max,

(84)

where Eq. (84) is obtained using Cauchy-Schwarz inequality. It follows that for large width N 1,

with high probability then gives

k=k Yk,k

(l + 1)µ2,min and k=k Yk,k

(l + 1)µ2,max, which

(l + 1)µ2,min µ2(yl) (l + 1)µ2,max.

(85)

We can write (l)2 as

( l )2

=

µ2(y0)µ2(tl) µ2(yl)

=

µ2

(y0

)

µ2(tl-1) µ2(yl-1)

+ +

Tl,l Yl,l

+ +

2 2

l-1 k=0 l-1 k=0

Tk,l Yk,l

,

( l )2

= (l-1)2

µ2(yl-1)

+

µ2 (y0 ) ( l-1 )2

µ2(yl-1) +

Tl,l Yl,l

+ +

2 µ2(y0)
( l-1 )2

2

l-1 k=0

l-1 k=0
Yk,l

Tk,l

.

(86)

Let us denote k



l:

T~k,l

=

Tµ2 (y0 )
(l-1)2 k,l

and Yl

=

gives

l-1 k=0

Yk,l

and

T~l

=

(  l )2

=

( l )2 ( l-1 )2

=

µ2(yl-1) + T~l,l µ2(yl-1) + Yl,l

+ +

2T~l 2Yl

.

l-1 k=0

T~k,l.

Eq.

(86)

then

(87)

Furthermore we can bound T~l,l as

T~l,l

=

µ2(y0) ( l-1 )2

µ2(tl,H

)

=

µ2(y0) ( l-1 )2

(

l-1)2

(
h

l,h

)2

µ2(yl,H ) µ2(y0)

,

minµ2,min T~l,l maxµ2,max.

(88)

31

Under review as a conference paper at ICLR 2019

Combining Eq. (79) and Eq. (80) from Corollary 14 with Eq. (85), we get for the variance of the terms T~l and Yl:

El Yl2

N

1 rmin

lµ22,max,

(89)

µ2(y0) El (l-1)2

2
Tk,l
k<l

1

µ2(y0)µ2(tl-1)

µ2(y0)µ2(tl,H )

N rmin El-1

( l-1 )2

El

( l-1 )2

,

El T~l2

1 N rmin El-1

µ2(yl-1)El

T~l,l

max

N

1 rmin

lµ22,max

.

(90)

It follows that Yl 1 and T~l Eq. (87) and Eq. (88),

1 with high probability when l N rmin. Combined with

µ2(yl-1) + minµ2,min

µ2(yl-1) + µ2,max

1

+

minµ2,min - µ2,max (l + 1)µ2,max

(  l )2 (  l )2

µ2(yl-1) + µ2(yl-1)

maxµ2,max + µ2,min

,

1

+

maxµ2,max - µ2,min (l + 1)µ2,min

.

Finally for l

N rmin, we find that l is bounded as

1

+

min l+1

1/2

l

1

+

max l+1

1/2
.

Proof of Eq. (82). Using Eq. (89) and Eq. (90), we apply Cauchy-Schwarz inequality on Yl and T~l:

El Yl El T~l

1 N rmin

 lµ2,max,

 max

1 N rmin

 lµ2,max.

Combined with Eq. (85), we deduce that for large width Yl µ2(yl) and T~l µ2(yl) always
hold with high probability. Due to µ2,min Yl,l µ2,max and to Eq. (88), we also have with high probability that Yl,l µ2(yl) and T~l µ2(yl) when l 1. Now let us take the logarithm in
Eq. (87):

2 log l = log

1

+

1 µ2(yl-1)

T~l,l

+

µ2

2 (yl-1

)

T~l

- log

1

+

1 µ2(yl-1) Yl,l

+

2 µ2(yl-1) Yl

.

When l 1, all the terms added to 1 in the logarithm are 1 with high probability. Therefore for l > l0 1, we can write

2 log l

µ2

1 (yl-1)

T~l,l

+

2 µ2(yl-1

)

T~l

-

1 µ2(yl-1) Yl,l

-

2 µ2(yl-1) Yl.

l
2 log k
k=l0 +1

l1 k=l0+1 µ2(yk-1)

T~k,k - Yk,k

+

l k=l0 +1

2 µ2(yk-1)

T~k - Yk

.

Let us bound the first term:

l minµ2,min - µ2,max

k=l0 +1

kµ2,max

l1

min

dx l0 x

l min log l0

l1 k=l0+1 µ2(yk-1)
l1 k=l0+1 µ2(yk-1)
l1 k=l0+1 µ2(yk-1)

T~k,k - Yk,k T~k,k - Yk,k T~k,k - Yk,k

l maxµ2,max - µ2,min ,

k=l0 +1

kµ2,min

l1

max

dx, l0 x

l max log l0 .

(91)

32

Under review as a conference paper at ICLR 2019

Now we consider the second term. By spherical symmetry, Yk / µ2(yk-1) and Yk / µ2(yk -1) are non-correlated for k = k . So we get combined with Eq. (89) that for l > l0 1,

El

l1

2

k=l0+1 µ2(yk-1) Yk

l
= Ek
k=l0 +1

12 µ2(yk-1) Yk ,

µ22,max

lk

N rminµ22,min k=l0+1 k2

µ22,max N rminµ22,min

log

l l0

.

A similar calculation for T~k gives

El

l k=l0 +1

1 µ2(yk-1)

T~k

2

max

µ22,max N rminµ22,min

log

l l0

.

For large width and l 1, these terms are very small with high probability compared to the term of Eq. (91). It follows that for l > l0 1,

min log

l l0

l 2 log l0

1 2

min

log

l

log l

max log

l l0

,

1 2

max

log

l.

Proof of Eq. (83). As a consequence Eq. (82), for l 1 there exist ,  with || 1, | | 1 with high probability and

(1

+

)

1 2

min

log

l



log

l



(1

+



)

1 2

max

log

l,

exp

1 2

(1

+

)min

log

l

 l  exp

1 2

(1

+



)max

log

l

.

Now

consider

min,

max

such

that

min

<

1 2

min

and

max

>

1 2

max.

Then

1 2

(1

+

)min

log

l

=

1 2

(1

+



)max

log

l

=

1 2 min

+

1 2 min

-

min

log l + min log l,

1 2 max

+

1 2 max

- max

log l + max log l.

The terms

1 2

min

+

1 2

min



-

min

and

1 2

max

+

1 2

max



- max are respectively positive with high

probability and negative with high probability. Therefore with high probability, exp min log l 

l  exp max log l and thus for l 1:

exp min log l l exp max log l , lmin  l lmax .

33

