Under review as a conference paper at ICLR 2019
EFFICIENT LIFELONG LEARNING WITH A-GEM
Anonymous authors Paper under double-blind review
ABSTRACT
In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularizationbased methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.1
1 INTRODUCTION
Intelligent systems, whether they are natural or artificial, must be able to quickly adapt to changes in the environment and to quickly learn new skills by leveraging past experiences. While current learning algorithms can achieve excellent performance on a variety of tasks, they strongly rely on copious amounts of supervision in the form of labeled data.
The lifelong learning (LLL) setting attempts at addressing this shortcoming, bringing machine learning closer to human learning. In lifelong learning, the learner is presented with a stream of tasks whose relatedness is not known a priori. The learner has then the potential to learn more quickly a new task, if it can remember how to combine and re-use knowledge acquired while learning related tasks of the past. Of course, for this learning setting to be useful, the model needs to be constrained in terms of amount of compute and memory required. Usually this means that the learner should not be allowed to merely store all examples seen in the past (in which case this reduces the lifelong learning problem to a multitask problem) nor should the learner engage in computations that would not be feasible in real-time, as the goal is to quickly learn from a stream of data.
Unfortunately, the established training and evaluation protocol as well as current algorithms for lifelong learning do not satisfy all the above desiderata, namely learning from a stream of data using limited number of samples, limited memory and limited compute. In the most popular training paradigm, the learner does several passes over the data (Kirkpatrick et al., 2016; Aljundi et al., 2018; Rusu et al., 2016; Schwarz et al., 2018), while ideally the model should need only a handful of samples and these should be provided one-by-one in a single pass (Lopez-Paz & Ranzato, 2017). Moreover, when the learner has several hyper-parameters to tune, the current practice is to go over the sequence of tasks several times, each time with a different hyper-parameter value, again ignoring the requirement of learning from a stream of data and, strictly speaking, violating the assumption of the LLL scenario. While some algorithms may work well in a single-pass setting, they unfortunately require a lot of computation (Lopez-Paz & Ranzato, 2017) or their memory scales with the number of tasks (Rusu et al., 2016), which greatly impedes their actual deployment in practical applications.
1We will release the code upon publication.
1

Under review as a conference paper at ICLR 2019
In this work, we propose an evaluation methodology and an algorithm that better match our desiderata, namely learning efficiently from a stream of tasks. First, we propose a new learning paradigm, whereby the learner performs cross validation on a set of tasks which is disjoint from the set of tasks actually used for evaluation (Sec. 2). In this setting, the learner will have to learn and will be tested on an entirely new sequence of tasks and it will perform just a single pass over this data stream. Second, we build upon GEM (Lopez-Paz & Ranzato, 2017), an algorithm which leverages a small episodic memory to perform well in a single pass setting, and propose a small change to the loss function which not only improves GEM performance, but it also makes GEM orders of magnitude faster at training time; we dub this variant of GEM, A-GEM (Sec. 4). Third, we explore the use of compositional task descriptors in order to improve the few-shot learning performance within LLL showing that with this additional information the learner can pick up new skills more quickly (Sec. 5). Fourth, we introduce a new metric to measure the speed of learning, which is useful to quantify the ability of a learning algorithm to learn a new task (Sec. 3). And finally, using our new learning paradigm and metric, we demonstrate A-GEM on a variety of benchmarks and against several representative baselines (Sec. 6). Our experiments show that A-GEM has a better trade-off between average accuracy and computational/memory cost. Moreover, all algorithms improve their ability to quickly learn a new task when provided with compositional task descriptors, and they do so better and better as they progress through the learning experience.
2 LEARNING PROTOCOL
Currently, most works on lifelong learning (Kirkpatrick et al., 2016; Rusu et al., 2016; Shin et al., 2017; Nguyen et al., 2018) adopt a learning protocol which is directly borrowed from supervised learning. There are T tasks, and each task consists of a training, validation and test sets. During training the learner does as many passes over the data of each task as desired. Moreover, hyperparameters are tuned on the validation sets by sweeping over the whole sequence of tasks as many times as required by the cross-validation grid search. Finally, metrics of interest are reported on the test set of each task using the model selected by the previous cross-validation procedure.
Since the current protocol violates our stricter definition of LLL for which the learner can only make a single pass over the data, as we want to emphasize the importance of learning quickly from data, we now introduce a new learning protocol.
We consider two streams of tasks, described by the following ordered sequences of datasets DCV = {D1, · · · , DT CV } and DEV = {DT CV +1, · · · , DT }, where Dk = {(xik, tik, yik)ni=k1} is the dataset of the k-th task, T CV T (in all our experiments T CV = 3 while T  {10, 20}), and we assume that all datasets are drawn from the same distribution over tasks. To avoid cluttering of the notation, we let the context specify whether Dk refers to the training or test set of the k-th dataset.
DCV is the stream of datasets which will be used during cross-validation; DCV allows the learner to replay all samples multiple times for the purposes of model hyper-parameter selection. Instead, DEV is the actual dataset used for final training and evaluation on the test set; the learner will observe training examples from DEV once and only once, and all metrics will be reported on the test sets of DEV . Since regularization-based approaches are rather sensitive to the choice of the regularization hyper-parameter, we introduced the set DCV , as it seems reasonable in practical applications to have similar tasks that can be used for tuning the system. However, the actual training and testing are then performed on DEV using a single pass over the data. See Algorithm 1 for a summary of the training and evaluation protocol.
Each example in any of these dataset consists of a triplet defined by an input (xk  X ), task descriptor (tk  T , see Sec. 5 for examples) and a target vector (yk  yk), where yk is the set of labels specific to task k and yk  Y. While observing the data, the goal is to learn a predictor f : X × T  Y, parameterized by   RP (a neural network in our case), that can map any test pair (x, t) to a target y.
3 METRICS
Below we describe the metrics used to evaluate the LLL methods studied in this work. In addition to Average Accuracy (A) and Forgetting Measure (F ) (Chaudhry et al., 2018), we define a new measure, the Learning Curve Area (LCA), that captures how quickly a model learns.
2

Under review as a conference paper at ICLR 2019

Algorithm 1 Learning and Evaluation Protocols

1: for h in hyper-parameter list do 2: for k = 1 to T CV do

Cross-validation loop, executing multiple passes over DCV Learn over data stream DCV using h

3: for i = 1 to nk do 4: Update f using (xki , tik, yik) and hyper-parameter h 5: Update metrics on test set of DCV

Single pass over Dk

6: end for

7: end for

8: end for 9: Select best hyper-parameter setting, h, based on average accuracy of test set of DCV , see Eq. 1.

10: Reset f.

11: Reset all metrics. 12: for k = T CV + 1 to T do

Actual learning over datastream DEV

13: for i = 1 to nk do 14: Update f using (xik, tik, yik) and hyper-parameter h 15: Update metrics on test set of DEV

Single pass over Dk

16: end for

17: end for 18: Report metrics on test set of DEV .

The training dataset of each task, Dk, consists of a total Bk mini-batches. After each presentation
of a mini-batch of task k, we evaluate the performance of the learner on all the tasks using the
corresponding test sets. Let ak,i,j  [0, 1] be the accuracy evaluated on the test set of task j, after
the model has been trained with the i-th mini-batch of task k. Assuming the first learning task in the continuum is indexed by 1 (it will be T CV + 1 for DEV ) and the last one by T (it will be T CV for DCV ), we define the following metrics:

Average Accuracy (A  [0, 1]) Average accuracy after the model has been trained continually with all the mini-batches up till task k is defined as:

1k

Ak = k

ak,Bk ,j

j=1

(1)

In particular, AT is the average accuracy on all the tasks after the last task has been learned; this is the most commonly used metric used in LLL.

Forgetting Measure (F  [-1, 1]) (Chaudhry et al., 2018) Average forgetting after the model has been trained continually with all the mini-batches up till task k is defined as:

Fk

=

k

1 -1

k-1
fjk

j=1

(2)

where fjk is the forgetting on task `j' after the model is trained with all the mini-batches up till task k and computed as:

fjk

=

max
l{1,··· ,k-1}

al,Bl

,j

-

ak,Bk ,j

(3)

Measuring forgetting after all tasks have been learned is important for a two-fold reason. It quantifies the accuracy drop on past tasks, and it gives an indirect notion of how quickly a model may learn a new task, since a forgetful model will have little knowledge left to transfer, particularly so if the new task relates more closely to one of the very first tasks encountered during the learning experience.

Learning Curve Area (LCA  [0, 1]) Let us first define an average b-shot performance (where b is the mini-batch number) after the model has been trained for all the T tasks as:

1T

Zb = T

ak,b,k

k=1

(4)

3

Under review as a conference paper at ICLR 2019

LCA at  is the area of the convergence curve Zb as a function of b  [0, ]:

1

1

LCA =  + 1

0

Zbdb =  + 1 Zb
b=0

(5)

LCA has an intuitive interpretation. LCA0 is the average 0-shot performance, the same as forward transfer in Lopez-Paz & Ranzato (2017). LCA is the area under the Zb curve, which is high if the 0-shot performance is good and if the learner learns quickly. In particular, there could be two models with the same Z or AT , but very different LCA because one learns much faster than the other while they both eventually obtain the same final accuracy. This metric aims at discriminating between these two cases, and it makes sense for relatively small values of  since we are interested
in models that learn from few examples.

4 AVERAGED GRADIENT EPISODIC MEMORY (A-GEM)

In this section, we first review GEM (Lopez-Paz & Ranzato, 2017), which is the algorithm we build upon. Then, we introduce A-GEM and explain in Sec. 5 how it can be easily extended to use task descriptors in the form of class attributes for faster few-shot learning.

GEM avoids catastrophic forgetting by storing an episodic memory Mk for each task k. While

minimizing the loss on the current task t, GEM treats the losses on the episodic memories of tasks

k < t, given by (f, Mk)

=

1 |Mk |

(xi,k,yi)Mk (f(xi, k), yi), as inequality constraints,

avoiding their increase but allowing their decrease. This effectively permits GEM to do positive

backward transfer which other LLL methods do not support. Formally, at task t, GEM solves for the

following objective:

minimize (f, Dt) s.t. (f, Mk)  (ft-1, Mk) k < t

(6)

Where ft-1 is the network trained till task t - 1. To inspect the increase in loss, GEM computes the angle between the loss gradient vectors of previous tasks gk, and the proposed gradient update on the current task g. Whenever the angle is greater than 90 with any of the gk's, it projects the proposed gradient to the closest in L2 norm gradient g~ that keeps the angle within the bounds. Formally, the

optimization problem GEM solves is given by:

minimizeg~

1 2

||g

-

g~||22

s.t.

g~, gk  0

k < t

(7)

Eq.7 is a quadratic program (QP) in P -variables (the number of parameters in the network), which
for neural networks could be in millions. In order to solve this efficiently, GEM works in the dual space which results in a much smaller QP with only t - 1 variables:

minimizev

1 v GG v + g G v
2

s.t. v  0

(8)

where G = -(g1, · · · , gt-1)  R(t-1)×P is computed at each gradient step of training. Once the solution v to Eq. 8 is found, the projected gradient update can be computed as g~ = G v + g.

While GEM has proven very effective in a single epoch setting (Lopez-Paz & Ranzato, 2017), the performance gains come at a big computational burden at training time. At each training step, GEM computes the matrix G using all samples from the episodic memory, and it also needs to solve the QP of Eq. 8. Unfortunately, this inner loop optimization becomes prohibitive when the size of M and the number of tasks is large, see Tab. 6 in Appendix for an empirical analysis. To alleviate the computational burden of GEM, next we propose a much more efficient version of GEM, called Averaged GEM (A-GEM).

Whereas GEM ensures that at every training step the loss of each individual previous tasks does not increase, A-GEM guarantees that at every training step the average loss over the previous tasks does not increase. Formally, while learning task t, the objective of A-GEM is:

minimize (f, Dt) s.t. (f, M)  (ft-1, M) where M = k<tMk

(9)

The corresponding optimization problem reduces to:

minimizeg~

1 2

||g

-

g~||22

s.t.

g~ gref  0

(10)

4

Under review as a conference paper at ICLR 2019

where gref is a gradient computed using a batch randomly sampled from the episodic memory, (xref , yref )  M, of all the past tasks. In other words, A-GEM replaces the t - 1 constraints of GEM with a single constraint, where gref is the average of the gradients from the previous tasks
computed from a random subset of the episodic memory.

The constrained optimization problem of Eq. 10 can now be solved very quickly; when the gradient

g

violates

the

constraint,

it

is

projected

via:

g~

=

g

-

g gref gref gref

gref .

The

formal

proof

of

the

update

rule of A-GEM is given in Appendix C. This makes A-GEM not only memory efficient, as it does not

need to store the matrix G, but also orders of magnitude faster than GEM because 1) it is not required

to compute the matrix G but just the gradient of a random subset of memory examples, 2) it does

not need to solve any QP but just an inner product, and 3) it will incur in less violations particularly

when the number of tasks is large (see Tab. 6 and Fig. 6 in Appendix for empirical evidence). All

together these factors make A-GEM faster while not hampering its good performance in the single

pass setting.

Intuitively, the difference between GEM and A-GEM loss functions is that GEM has better guarantess in terms of worst-case forgetting of each individual task since (at least on the memory examples) it prohibits an increase of any task-specific loss, while A-GEM has better guaratees in terms of average accuracy since GEM may prevent a gradient step because of a task constraint violation although the overall average loss may actually decrease, see Appendix Sec. D.1 and D.2 for further analysis and empirical evidence. The pseudo-code of A-GEM is given in Appendix Alg. 2.

5 JOINT EMBEDDING MODEL USING COMPOSITIONAL TASK DESCRIPTORS

In this section we discuss how we can improve forward transfer for efficient LLL. In order to speed up learning of a new task, we consider the use of compositional task descriptors where components are shared across tasks and thus allow transfer. Examples of compositional task descriptors are, for instance, a natural language description of the task under consideration or a matrix specifying the attribute values of the objects to be recognized in the task. In our experiments, we use the latter since it is provided with popular benchmark datasets (Wah et al., 2011; Lampert et al., 2009). For instance, if the model has already learned and remembers about two independent properties (e.g., color of feathers and shape of beak), it can quickly recognize a new class provided a descriptor specifying the values of its attributes (yellow feathers and red beak), although this is an entirely unseen combination.
Borrowing ideas from literature in few-shot learning (Lampert et al., 2014; Zhang et al., 2018; Elhoseiny et al., 2017; Xian et al., 2018), we propose the use of a joint embedding model. Let xk  X be the input (e.g., an image), tk be the task descriptor in the form of a matrix of size Ck × A where Ck is the number of classes in the k-th task and A is the total number of attributes, the joint embedding model consists of a feature extraction module, (.), parameterized by  and mapping xk into a D-dimensioanal feature space, and a task embedding module, (.), parameterized by  and mapping tk into C vectors of D-dimensions, one for each class to predict. During training, the parameters  and  are learned by minimizing the cross-entropy loss:

1 k(, ) = N

N

- log(p(y^c|xki , tk; , ))

i=1

(11)

where c is the target class of the i-th example of task k (xki , tk, c). The distribution p(.) is defined as:

p(y^c|xk, tk; , ) =

exp([(xk) [(tk)]k]) j exp([(xk) [(ak)]j])

(12)

where [(ak)]i denotes the i-th column vector from matrix . Note that the architecture and loss functions are general, and apply not only to A-GEM but also to any other LLL model (e.g., regularization based approaches). See Sec. 6 for the actual choice of parameterization of these functions.

5

Under review as a conference paper at ICLR 2019
6 EXPERIMENTS
We consider four dataset streams, see Tab.1 in Appendix Sec. A for a summary of the statistics. Permuted MNIST (Kirkpatrick et al., 2016) is a variant of MNIST (LeCun, 1998) dataset of handwritten digits where each task has a certain random permutation of the input pixels which is applied to all the images of that task. Split CIFAR (Zenke et al., 2017) consists of splitting the original CIFAR-100 dataset (Krizhevsky & Hinton, 2009) into 20 disjoint subsets of classes. Similarly to Split CIFAR, also Split CUB and Split AWA are incremental versions of their original datasets, the fine-grained image classification dataset CUB (Wah et al., 2011) of 200 bird categories and the AWA dataset (Lampert et al., 2009) of 50 animal categories, respectively. While on Permuted MNIST all tasks share the same output space, on the other datasets the output space is task-specific. Finally, while on Permuted MNIST and Split CIFAR we provide integer task descriptors, on Split CUB and Split AWA we assemble together the attributes of the classes (specifying for instance the type of beak, the color of feathers, etc.) belonging to the current task to form a descriptor.
In terms of architectures, we use a fully-connected network with two hidden layers of 256 ReLU units each for Permuted MNIST, a reduced ResNet18 for Split CIFAR like in Lopez-Paz & Ranzato (2017), and a standard ResNet18 (He et al., 2016) for Split CUB and Split AWA. For a given dataset stream, all models use the same architecture, and all models are optimized via stochastic gradient descent with mini-batch size equal to 10. We refer to the joint-embedding model version of these models by appending the suffix `-JE' to the method name.
As described in Sec. 2 and outlined in Alg. 1, in order to cross validate we use the first 3 tasks, and then report metrics on the remaining 17 tasks (7 for Split AWA) after doing a single training pass over each task in sequence.
Lastly, we compared A-GEM against several baselines and state-of-the-art LLL approaches which we describe next. VAN is the basic supervised learning model without any regularization, with the parameters of a new task initialized from the parameters of the previous task. ICARL (Rebuffi et al., 2017) is a class-incremental learner that uses nearest-exemplar-based classifier and avoids catastrophic forgetting by regularizing over the feature representation of previous tasks using a knowledge distillation loss. EWC (Kirkpatrick et al., 2016), PI (Zenke et al., 2017), RWALK (Chaudhry et al., 2018) and MAS (Aljundi et al., 2018) are regularization-based approaches aiming at avoiding catastrophic forgetting by limiting learning of parameters critical to the performance of past tasks. Progressive Networks (PROG-NN) (Rusu et al., 2016) is a modular approach whereby a new "column" with lateral connections to previous hidden layers is added once a new task arrives. GEM (Lopez-Paz & Ranzato, 2017) described in Sec. 4 is another natural baseline of comparison since A-GEM builds upon it. The total amount of episodic memory used in ICARL, GEM and A-GEM is set to 5000, 1300, 1000, and 1000 for MNIST, CIFAR, CUB and AWA, respectively. These samples are chosen uniformly at random for each task. And finally, we consider a multi-task baseline, MULTI-TASK, trained on a single pass over shuffled data from all tasks, and thus violating the LLL assumption. It can be seen as an upper bound performance for average accuracy.
6.1 RESULTS
Fig. 1 and 2 show the overall results on all the datasets we considered (for brevity we show only representative methods, see detailed results in Appendix Tab. 4, 5 and 6). First, we observe that A-GEM achieves the best average accuracy on all datasets, except Permuted MNIST, where PROGNN works better. The reason is because on this dataset each task has a large number of training examples, which enables PROG-NN to learn its task specific parameters and to leverage its lateral connections. However, notice how PROG-NN has the worst memory cost by the end of training - as its number of parameters grows super-linearly with the number of tasks. Also, PROG-NN does not learn well on datasets where tasks have fewer training examples. Second, A-GEM and GEM perform comparably in terms of average accuracy, but A-GEM has much lower time (about 100 times faster) and memory cost (about 10 times lower), comparable to regularization based approaches like EWC. On AWA, A-GEM has slightly poor time performance than PROG-NN which we attribute to the inefficiency of using small networks in modern GPUs. However, notice when the number of tasks are increased (Split CUB in Fig. 2 and Fig. 7 in Appendix) PROG-NN not only gets slower but runs out of memory. Third, EWC and similar methods perform only slightly better than VAN on this single pass LLL setting. The analysis in Appendix Sec. G demonstrates that EWC requires several epochs
6

Under review as a conference paper at ICLR 2019

0.5 1.0 1.0

0.25

0.8 0.4

0.8 0.8

0.20

0.6 0.3

0.6 0.6

0.15

0.4 0.2

0.4 0.4

0.10

0.2 0.1 0.05 0.2 0.2

0.6 0.25 0.35 1.0

0.5 0.20 0.30 0.8

0.25

0.4 0.15

0.6

0.20

0.3

0.10 0.15

0.4

0.2 0.10
0.05 0.2 0.1 0.05

1.0

0.8

0.6 1.0 0.8

VAN

EWC 0.6
PROG-NN 0.4
GEM

0.4 0.2

A-GEM

0.0

01

0.2

0.0

AT ()

0.0

FT ()

0.00 LCA10() 0.0

Time()

0.0
Memory()

0.0

AT ()

0.00

FT ()

0.00 LCA10() 0.0

Time()

0.0
Memory()

(a) Permuted MNIST

(b) Split CIFAR

Figure 1: Performance of LLL models across different measures on Permuted MNIST and Split CIFAR. For Accuracy (AT ) and Learning Curve Measure (LCA10) the higher the number (indicated by ) the better is the model. For Forgetting (FT ), Time and Memory the lower the number (indicated by ) the better is the model. For Time and Memory, the method with the highest complexity is
taken as a reference (value of 1) and the other methods are reported relative to that method. A-
GEM provides the best trade-off across different measures and dimensions. The numbers on other
baselines are given in Tab. 4 and 6 in the Appendix, which are used to generate the plots.

0.16 0.5 0.14
0.4 0.12 0.10
0.3 0.08
0.2 0.06 0.04
0.1 0.02
0.0 AT () 0.00

1.0 1.0 0.35
0.30 0.8 0.8

0.5 0.4

0.25 0.6 0.6
0.20

0.3

0.15 0.4 0.4

0.2

0.10 0.2 0.2
0.05

0.1

FT ()

0.00 LCA10() 0.0

Time()

0.0
Memory()

0.0

(a) Split CUB

AT ()

0.10 0.08 0.06 0.04 0.02 0.00

1.0 0.30
0.8 0.25
0.20 0.6
0.15 0.4
0.10 0.2
0.05

1.0

0.8

0.6 0.4

1.0 0.8 0.6 0.4 0.2 0.0
0

1

0.2

FT ()

0.00 LCA10() 0.0

Time()

0.0
Memory()

(b) Split AWA

VAN VAN-JE EWC EWC-JE PROG-NN A-GEM A-GEM-JE

Figure 2: Performance of LLL models across different measures on Split CUB and Split AWA. On Split CUB (left), PROG-NN produces out of memory (OoM) errors due to its large size and hence corresponding values of AT , FT and LCA10 are missing. The memory and time complexity of joint embedding models is the same as that of corresponding standard models and hence omitted from the plots. The numbers on other baselines are given in Tab. 5 and 6 in the Appendix, which are used to generate the plot.

and over-parameterized architectures in order to work well. Fourth, PROG-NN has no forgetting by construction and A-GEM and GEM have the lowest forgetting among methods that use a fixed capacity architecture. Next, all methods perform similarly in terms of LCA, with PROG-NN being the worst because of its ever growing number of parameters and A-GEM slightly better than all the other approaches. And finally, the use of task descriptors decreases forgetting and improves average accuracy across the board as shown Fig.2, with A-GEM a bit better than all the other methods we tried. All joint-embedding models using task descriptors have better LCA performance, although this is the same across all methods including A-GEM. Overall, we conclude that A-GEM offers the best trade-off between average accuracy performance and efficiency in terms of sample, memory and computational cost.
Fig. 3 shows a more fine-grained analysis and comparison with more methods on Permuted MNIST and Split CIFAR. The average accuracy plots show how A-GEM and GEM greatly outperform other approaches, with the exception of PROG-NN on MNIST as discussed above. On different datasets, different methods are best in terms of LCA, although A-GEM is always top-performing. Fig. 4 shows in more detail the gain brought by task descriptors which greatly speed up learning in the few-shot regime. On these datasets, A-GEM performs the best or on par to the best.
Finally, in Fig. 5, we report the 0-shot performance of LLL methods on Split CUB and Split AWA datasets over time, showing a clear advantage of using compositional task descriptors with joint embedding models, which is slightly more significant for A-GEM. Interestingly, the zero-shot learning
7

Under review as a conference paper at ICLR 2019

Avg Accuracy

0.9 0.8 0.7 0.6 0.5
Tasks1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
0.30 0.25 0.20 0.15 0.10
0 1 2 3 B4atc5he6s 7 8 9 10
(a) Permuted MNIST

LCA

Avg Accuracy

0.7 0.6 0.5 0.4
Tasks1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
0.35 0.30 0.25 0.20
0 1 2 3 B4atc5he6s 7 8 9 10
(b) Split CIFAR

MAS VAN PROG-NN EWC A-GEM RWALK ICARL PI GEM MULTI-TASK

LCA

Figure 3: Top Row: Evolution of average accuracy (Ak) as new tasks are learned. Bottom Row: Evolution of LCA during the first ten mini-batches.

LCA

0.40 0.32
0.35 0.30
0.30 0.28

LCA

0.25 0.26

0.20 0.24

0.15
0.10
0 1 2 3 B4atc5he6s 7 8 9 10

0.22 0.20
0 1 2 3 B4atc5he6s 7 8 9 10

(a) Split CUB

(b) Split AWA

Figure 4: Evolution of LCA during the first ten mini-batches.

VAN EWC RWALK A-GEM VAN-JE EWC-JE RWALK-JE A-GEM-JE

performance of joint embedding models improves over time, indicating that these models get better at forward transfer or in other words become more efficient over time; see additional experiments in Appendix Fig. 7.

0.5 VAN (0.1)

VAN (0.19)

EWC (0.1)

0.325

EWC (0.19)

RWALK (0.1)

RWALK (0.19)

0.4

A-GEM (0.1) VAN-JE (0.28)

0.300

A-GEM (0.20) VAN-JE (0.28)

EWC-JE (0.29)

EWC-JE (0.28)

RWALK-JE (0.30) 0.275

RWALK-JE (0.28)

0.3 A-GEM-JE (0.32)

A-GEM-JE (0.29)

0.250

0.2 0.225
0.200 0.1
0.175

Tasks1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17

1 2 3 Tas4 ks 5 6 7

(a) Split CUB

(b) Split AWA

Figure 5: Evolution of zero-shot performance as the learner sees new tasks on Split CUB and Split AWA datasets. The average zero-shot performance across all tasks is shown between parenthesis after the name of each method.

8

Zero-shot Acc Zero-shot Acc

Under review as a conference paper at ICLR 2019
7 RELATED WORK
Continual (Ring, 1997) or Lifelong Learning (LLL) (Thrun, 1998) have been the subject of extensive study over the past two decades. One approach to LLL uses modular compositional models (Fernando et al., 2017; Aljundi et al., 2017; Rosenbaum et al., 2018; Chang et al., 2018; Xu & Zhu, 2018; Ferran Alet, 2018), which limit interference among tasks by using different subset of modules for each task. Unfortunately, these methods require searching over the space of architectures which is not sample efficient with current methods. Another approach is to regularize parameters important to solve past tasks (Kirkpatrick et al., 2016; Zenke et al., 2017; Chaudhry et al., 2018), which has been proven effective for over-parameterized models in the multiple epoch setting (see Appendix Sec. G), while we focus on learning from few examples using memory efficient models. Methods based on episodic memory (Rebuffi et al., 2017; Lopez-Paz & Ranzato, 2017) require a little bit more memory at training time but can work much better in the single pass setting we considered (Lopez-Paz & Ranzato, 2017). The use of task descriptors for LLL has already been advocated by Isele et al. (2016) but using a sparse coding framework which is not obviously applicable to deep nets in a computationally efficient way, and also by Lopez-Paz & Ranzato (2017) although they did not explore the use of compositional descriptors. More generally, tasks descriptors have been used in Reinforcement Learning with similar motivations by several others (Sutton et al., 2011; Schaul et al., 2015; Baroni et al., 2017), and it is also a key ingredient in all the zero/few-shot learning algorithms (Lampert et al., 2014; Xian et al., 2018; Elhoseiny et al., 2017; Wah et al., 2011; Lampert et al., 2009).
8 CONCLUSION
We studied the problem of efficient Lifelong Learning (LLL) in the case where the learner can only do a single pass over the input data stream. We found that our approach, A-GEM, has the best tradeoff between average accuracy by the end of the learning experience and computational/memory cost. Compared to the original GEM algorithm, A-GEM is about 100 times faster and has 10 times less memory requirements; compared to regularization based approaches, it achieves significantly higher average accuracy. We also demonstrated that by using compositional task descriptors all methods can improve their few-shot performance, with A-GEM often being the best. Our detailed experiments reported in Appendix F also show that there is still a substantial performance gap between LLL methods, including A-GEM, trained in a sequential learning setting and the same network trained in a non-sequential multi-task setting, despite seeing the same data samples. Moreover, while task descriptors do help in the few-shot learning regime, the LCA performance gap between different methods is very small; suggesting a poor ability of current methods to transfer knowledge even when forgetting has been eliminated. Addressing these two fundamental issues will be the focus of our future research.
9

Under review as a conference paper at ICLR 2019
REFERENCES
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In CVPR, pp. 7120­7129, 2017.
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018.
M. Baroni, A. Joulin, A. Jabri, G. Kruszewski, A. Lazaridou, K. Simonic, and T. Mikolov. CommAI: Evaluating the first steps towards a useful general AI. arXiv, 2017.
Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. Automatically composing representation transformations as a means for generalization. In ICML workshop Neural Abstract Machines and Program Induction v2, 2018. URL https://arxiv.org/pdf/1807. 04640.pdf.
Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, 2018.
Mohamed Elhoseiny, Ahmed Elgammal, and Babak Saleh. Write a classifier: Predicting visual classifiers from unstructured text. IEEE transactions on pattern analysis and machine intelligence, 39(12):2539­2553, 2017.
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.
Leslie P. Kaelbling Ferran Alet, Tomas Lozano-Perez. Modular meta-learning. In arXiv preprint arXiv:1806.10166v1, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
David Isele, Mohammad Rostami, and Eric Eaton. Using task features for zero-shot knowledge transfer in lifelong learning. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI'16, pp. 1620­1626. AAAI Press, 2016. ISBN 978-1-57735-7704. URL http://dl.acm.org/citation.cfm?id=3060832.3060847.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America (PNAS), 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/ kriz/cifar.html, 2009.
Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 951­958. IEEE, 2009.
Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for zero-shot visual object categorization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(3):453­465, 2014.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
David Lopez-Paz and Marc'Aurelio Ranzato. Gradient episodic memory for continuum learning. In NIPS, 2017.
Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. ICLR, 2018.
10

Under review as a conference paper at ICLR 2019
S-V. Rebuffi, A. Kolesnikov, and C. H. Lampert. iCaRL: Incremental classifier and representation learning. In CVPR, 2017.
Mark B Ring. Child: A first step towards continual learning. Machine Learning, 28(1):77­104, 1997.
Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=ry8dvM-R-.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. ICML, 2015.
Jonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress and compress: A scalable framework for continual learning. In International Conference in Machine Learning, 2018.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In NIPS, 2017.
R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. The 10th International Conference on Autonomous Agents and Multiagent Systems, 2011.
Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181­209. Springer, 1998. C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011
dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning-a com-
prehensive evaluation of the good, the bad and the ugly. IEEE transactions on pattern analysis and machine intelligence, 2018. Ju Xu and Zhanxing Zhu. Reinforced continual learning. In arXiv preprint arXiv:1805.12369v1, 2018. F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. Ji Zhang, Yannis Kalantidis, Marcus Rohrbach, Manohar Paluri, Ahmed Elgammal, and Mohamed Elhoseiny. Large-scale visual relationship understanding. arXiv preprint arXiv:1804.10660, 2018.
11

Under review as a conference paper at ICLR 2019

APPENDIX
In Sec. A we report the summary of datasets used for the experiments. Sec. B details our A-GEM algorithm and Sec. C provides the proof of update rule of A-GEM discussed in Sec. 4 of the main paper. In Sec. D, we analyze the differences between A-GEM and GEM, and describe another variation of GEM, dubbed Stochastic GEM (S-GEM). Sec. E visualizes the 0-shot performance evolution for different models as the learner sees new tasks. The detailed results of the experiments which were used to generate Fig 1 and 2 in the main paper are given in Sec. F. In Sec. G, we provide empirical evidence to the conjecture that regularization-based approaches like EWC require overparameterized architectures and multiple passes over data in order to perform well as discussed in the Sec. 6.1 of the main paper. Finally, in Sec. H, we provide the grid used for the cross-validation of different hyper-parameters and report the optimal values for different models.

A DATASET STATISTICS

Table 1: Dataset statistics. Note, AWA has nonuniform number of images per class, so the reported number of images per task are averaged across 5 runs.

num. of tasks input size num. of classes per task num. of training images per task num. of test images per task

Perm. MNIST
20 1×28×28
10 60000 10000

Split CIFAR
20 3×32×32
5 2500 500

Split CUB
20 3×224×224
10 300 290

Split AWA
10 3×224×224
5 3170 560

B A-GEM ALGORITHM

Algorithm 2 Training and evaluation of A-GEM on sequential data D = {D1, · · · , DT }

1: procedure TRAIN(f, Dtrain, Dtest) 2: M  {}

3: A  0  RT ×T

4: for t = {1, · · · , T } do

5: for (x, y)  Dttrain do 6: (xref , yref )  M
7: gref   (f(xref , t), yref )
8: g   (f(x, t), y)

9: if g gref  0 then 10: g~  g

11: else

12:

g~



g

-

g gref gref gref

gref

13: end if

14:    - g~

15: end for 16: M  UPDATEEPSMEM(M, Dttrain, T ) 17: At,:  EVAL(f, Dtest)

18: end for

19: return f, A

20: end procedure

1: procedure EVAL(f, Dtest) 2: a  0  RT

3: for t = {1, · · · , T } do

4: at  0 5: for (x, y)  Dttest do 6: at  at + ACCURACY(f(x, t), y)

7: end for

8:

a t

at len(Dttest )

9: end for

10: return a

11: end procedure

1: procedure UPDATEEPSMEM(M, Dt, T )

2:

s



|M| T

3: for i = {1, · · · , s} do

4: (x, y)  Dt 5: M  (x, y)

6: end for

7: return M

8: end procedure

C A-GEM UPDATE RULE

Here

we

provide

the

proof

of

the

update

rule

of

A-GEM,

g~

=

g

-

g gref gref gref

gref ,

stated

in

Sec.

4

of

the main paper.

12

Under review as a conference paper at ICLR 2019

Proof. The optimization objective of A-GEM as described in the Eq. 10 of the main paper, is:

minimizeg~

1 2

||g

-

g~||22

s.t. g~ gref  0

(13)

Replacing g~ with z and rewriting Eq. 13 yields:

1

minimizez

z z-g z 2

s.t. - z gref  0

(14)

Note that we discard the term g g from the objective and change the sign of the inequality constraint. The Lagrangian of the constrained optimization problem defined above can be written as:

1

L(z, ) = z 2

z-g

z - z

gref

(15)

Now, we pose the dual of Eq. 15 as: D() = min L(z, )
z

(16)

Lets find the value z that minimizes the L(z, ) by setting the derivatives of L(z, ) w.r.t. to z to zero:

zL(z, ) = 0 z = g + gref

(17)

The simplified dual after putting the value of z in Eq. 16 can be written as:

1

D ()

=

(g 2

g + 2g

gref + 2gref gref ) - g

g - 2g

gref - 2gref gref

= -1g 2

g - g

gref

-

1 2

2gref

gref

The solution  = max;>0 D() to the dual is given by: D() = 0
 = - g gref gref gref

By putting  in Eq. 17, we recover the A-GEM update rule:

z

=

g

-

g gref gref gref

gref

=

g~

D ANALYSIS OF GEM AND A-GEM
In this section, we empirically analyze the differences between A-GEM and GEM, and report experiments with another computationally efficient but worse performing version of GEM.
D.1 FREQUENCY OF CONSTRAINT VIOLATIONS
Fig. 6 shows the frequency of constraint violations (see Eq. 8 and 10) on Permuted MNIST and Split CIFAR datasets. Note that, the number of gradient updates (training steps) per task on MNIST and CIFAR are 5500 and 250, respectively. As the number of tasks increase, GEM violates the optimization constraints at almost each training step, whereas A-GEM plateaus to a much lower value. Therefore, the computational efficiency of A-GEM not only stems from the fact that it avoids solving a QP at each training step (which is much more expensive than a simple inner product) but also from the fewer number of constraint violations. From the figure, we can also infer that as the number of tasks grows the gap between GEM and A-GEM would grow further. Thus, the computational and memory overhead of GEM over A-GEM, see also Tab. 6, gets worse as the number of tasks increases.
13

Under review as a conference paper at ICLR 2019

Constraint Violations Constraint Violations

5000

GEM A-GEM

4000

3000

2000

1000

GEM 200 A-GEM 150 100 50

0
Tasks1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17

0
Tasks1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17

(a) MNIST

(b) CIFAR

Figure 6: Number of constraint violations in GEM and A-GEM on Permuted MNIST and Split

CIFAR as new tasks are learned.

D.2 AVERAGE ACCURACY AND WORST-CASE FORGETTING
In Tab. 2, we empirically demonstrate the different properties induced by the objective functions of GEM and A-GEM. GEM enjoys lower worst-case task forgetting while A-GEM enjoys better overall average accuracy. This is particularly true on the training examples stored in memory, as on the test set the result is confounded by the generalization error.

Table 2: Comparison of average accuracy (AT ) and worst-case forgetting (Fwst) on the Episodic Memory (M) and Test Set (DEV ).

Methods
GEM
A-GEM

MNIST M DEV
AT Fwst AT Fwst 99.5 0 88.6 0.10 99.3 0.008 89.2 0.12

CIFAR M DEV
AT Fwst AT Fwst 97.1 0.05 62.1 0.11 72.1 0.15 63.5 0.14

D.3 STOCHASTIC GEM (S-GEM)
In this section we report experiments with another variant of GEM, dubbed Stochastic GEM (SGEM). The main idea in S-GEM is to randomly sample one constraint, at each training step, from the possible t - 1 constraints of GEM. If that constraint is violated, the gradient is projected only taking into account that constraint. Formally, the optimization objective of S-GEM is given by:

minimizeg~

1 2

||g

-

g~||22

s.t. g~, gk  0 where k  {1, · · · , t - 1}

(18)

In other words, at each training step, S-GEM avoids the increase in loss of one of the previous tasks sampled randomly. In Tab. 3 we report the comparison of GEM, S-GEM and A-GEM on Permuted MNIST and Split CIFAR.
Although, S-GEM is closer in spirit to GEM, as it requires randomly sampling one of the GEM constraints to satisfy, compared to A-GEM, which defines the constraint as the average gradient of the previous tasks, it perform slightly worse than GEM, as can be seen from Tab. 3.
14

Under review as a conference paper at ICLR 2019

Zero-shot Acc

Table 3: Comparison of different variations of GEM on MNIST Permutations and Split CIFAR.

Methods
GEM
S-GEM A-GEM

Permuted MNIST

AT (%)

FT

88.6 0.06 88.2 0.08 89.2 0.06

Split CIFAR AT (%) FT
62.1 0.06 58.6 0.06 63.5 0.06

E ZERO-SHOT PERFORMANCE EVOLUTION OVER 100 TASKS

In Fig. 7, we report the 0-shot performance over a sequence of 100 tasks on AWA dataset. To construct a task, 5 classes are randomly sampled without replacement from the total of 50 classes. In this setting, classes may overlap among multiple tasks, but within each task they compete against different set of classes. Note that a same class will have a different task descriptor in different tasks. From the Fig. 7 it can be seen that as the number of tasks increase the amount of 0-shot transfer also increases. The increase is more pronounced in the joint-embedding models that utilize task descriptors efficiently. Among all the methods, A-GEM and A-GEM-JE perform the best. PROG-NN ran out of memory while running this experiment confirming again that it does not scale in bigger setups as discussed in Sec. 6.1 of the main paper.

0.8

VAN EWC

0.7

RWALK A-GEM

0.6 0.5

VAN-JE EWC-JE RWALK-JE

0.4 A-GEM-JE

0.3

0.2

Tasks0 20 40 60 80 100

Figure 7: AWA: Evolution of zero-shot performance as the learner sees new tasks.

F RESULT TABLES

In Tab. 4, 5 and 6 we report the detailed results which were used to generate Fig.1 and 2.

Table 4: Comparison with different baselines on Permuted MNIST and Split CIFAR. The value of  is assigned to a metric when the model fails to train with the cross-validated values of hyperparameters found on the subset of the tasks as discussed in Sec. 2 of the main paper. The results from this table are used to generate Fig 1 in Sec. 6.1 of the main paper.

Methods
VAN ICARL EWC PI MAS RWALK
PROG-NN
GEM
A-GEM (Ours)
MULTI-TASK

Permuted MNIST

AT (%)

FT

50.6 0.48 --
67.3 0.30  70.8 0.26 85 0.08 93.5 0 88.6 0.06 89.2 0.06

95.3 -

Split CIFAR AT (%) FT
45.1 0.23 48.5 0.11 43.7 0.25 50 0.14 44.8 0.23 47 0.22 59.8 0 62.1 0.06 63.5 0.06
68.3 -

15

Under review as a conference paper at ICLR 2019

Table 5: Average accuracy and forgetting of standard models (left) and joint embedding models (right) on Split CUB and AWA datasets. The value of  is assigned to a metric when the model fails to train with the cross-validated values of hyper-parameters found on the subset of the tasks as discussed in Sec. 2 of the main paper. The value of `OoM' is assigned to a metric when the model fails to fit in the memory. The results from this table are used to generate Fig 2 in Sec. 6.1 of the main paper.

Methods
VAN EWC PI MAS RWALK
PROG-NN A-GEM (Ours) MULTI-TASK

Split CUB

AT (%)

FT

36.1 / 53.2 38.0 / 50.8 40.3 / 50.5 38.3 /  36.4 / 48.2 OoM / OoM 44.1 / 55.8

0.16 / 0.09 0.13 / 0.09 0.12 / 0.07 0.10 /  0.15 / 0.12 OoM / OoM 0.07 / 0.06

49.7 / 61.0

-/-

Split AWA

AT (%)

FT

36.1 / 41.8 38.1 / 42.1 37.8 / 43.3 38.7 / 38.7 36.7 / 42.5
43.1 / 48.9 / 50

0.03 / 0.11 0.09 / 0.09 0.10 / 0.07 0.08 / 0.09 0.11 / 0.09
0/0.02 / 0.02

64.8 / 66.8

-/-

Table 6: Computational cost and memory complexity of different LLL approaches. The timing refers to training time on a GPU device. Memory cost is provided in terms of the total number of parameters P, the size of the minibatch B, the total size of the network hidden state H (assuming all methods use the same architecture), the size of the episodic memory M per task. The results from this table are used to generate Fig. 1 and 2 in Sec. 6.1 of the main paper.

Methods
VAN EWC PROGRESSIVE NETS GEM
A-GEM (Ours)

Training Time [s] MNIST CIFAR CUB AWA

186 403 510 3442 477

105 250 409 5238 449

54 163 72 256 839 951 -420 1105

Memory

Training

Testing

P + B*H 4*P + B*H 2*P*T + B*H*T P*T + (B+M)*H 2*P + (B+M)*H

P + B*H P + B*H 2*P*T + B*H*T P + B*H P + B*H

G ANALYSIS OF EWC
In this section we provide empirical evidence to the conjecture that regularization-based approaches like EWC need over-parameterized architectures and multiple passes over the samples of each task in order to perform well. The intuition as to why models need to be over-parameterized is because it is easier to avoid cross-task interference when the model has additional capacity. In the single-pass setting and when each task does not have very many training samples, regularization-based appraches also suffer because regularization parameters cannot be estimated well from a model that has not fully converged. Moreover, for tasks that do not have much data, rgularization-based approaches do not enable any kind of positive backward transfer (Lopez-Paz & Ranzato, 2017) which further hurts performance as the predictor cannot leverage knowledge acquired later to improve its prediction on past tasks. Finally, regularization-based approaches perform much better in the multi-epoch setting simply because in this setting the baseline un-regularized model performs much worse, as it overfits much more to the data of the current task, every time unlearning what it learned before.
We consider Permuted MNIST and Split CIFAR datasets as described in Sec. 6 of the main paper. For MNIST, the two architecture variants that we experiment with are; 1) two-layer fully-connected network with 256 units in each layer (denoted by -S suffix), and 2) two-layer fully-connected network with 2000 units in each layer (denoted by -B suffix).
For CIFAR, the two architecture variants are; 1) ResNet-18 with 3 times less feature maps in all the layers (denoted by -S suffix), and 2) Standard ResNet-18 (denoted by -B token).
We run the experiments on VAN and EWC with increasing the number of epochs from 1 to 10 for Permuted MNIST and from 1 to 30 for CIFAR. For instance, when epoch is set to 10, it means that the training samples of task t are presented 10 times before showing examples from task t + 1. In
16

Under review as a conference paper at ICLR 2019

Avg Accuracy

Avg Accuracy

Fig. 8 and 9 we plot the Average Accuracy (Eq. 1) and Forgetting (Eq. 2) on Permuted MNIST and Split CIFAR, respectively.
We observe that the average accuracy significantly improves with the number of epochs only when EWC is applied to the big network. In particular, in the single epoch setting, EWC peforms similarly to the baseline VAN on Split CIFAR which has fewer number of training examples per task.

0.8 0.7 0.6 0.5
1 3 Epochs

Forgetting

0.5 0.4 0.3 0.2 10 1

3 Epochs

VAN-S EWC-S VAN-B EWC-B 10

(a) Accuracy

(b) Forgetting

Figure 8: Permuted MNIST: Change in average accuracy and forgetting as the number of epochs

are increased. Tokens '-S' and '-B' denote smaller and bigger networks, respectively.

0.6 0.5 0.4 0.3 1 3

10 Epochs

Forgetting

0.4 0.3 0.2 0.1 30 1 3

10 Epochs

VAN-S EWC-S VAN-B EWC-B 30

(a) Accuracy

(b) Forgetting

Figure 9: Split CIFAR: Change in average accuracy and forgetting as the number of epochs are

increased. Tokens '-S' and '-B' denote smaller and bigger networks, respectively.

H HYPER-PARAMETER SELECTION
Below we report the hyper-parameters grid considered for different experiments. Note, as described in the Sec. 6 of the main paper, to satisfy the requirement that a learner does not see the data of a task more than once, first T CV tasks are used to cross-validate the hyper-parameters. In all the datasets, the value of T CV is set to `3'. The best setting for each experiment is reported in the parenthesis.
· MULTI-TASK ­ learning rate: [0.3, 0.1, 0.03 (MNIST perm, Split CIFAR, Split CUB, Split AWA), 0.01, 0.003, 0.001, 0.0003, 0.0001]
· MULTI-TASK-JE ­ learning rate: [0.3, 0.1, 0.03 (Split CUB, Split AWA), 0.01, 0.003, 0.001, 0.0003, 0.0001]
· VAN ­ learning rate: [0.3, 0.1, 0.03 (MNIST perm, Split CUB), 0.01 (Split CIFAR), 0.003, 0.001 (Split AWA), 0.0003, 0.0001]
· VAN-JE ­ learning rate: [0.3, 0.1, 0.03 (Split CUB), 0.01, 0.003 (Split AWA), 0.001, 0.0003, 0.0001]
· PROG-NN ­ learning rate: [0.3, 0.1 (MNIST perm, ), 0.03 (Split CIFAR, Split AWA), 0.01 (Split CUB), 0.003, 0.001, 0.0003, 0.0001]
17

Under review as a conference paper at ICLR 2019
· EWC ­ learning rate: [0.3, 0.1, 0.03 (MNIST perm, Split CIFAR, Split CUB), 0.01, 0.003 (Split AWA), 0.001, 0.0003, 0.0001] ­ regularization: [1 (Split CUB), 10 (MNIST perm, Split CIFAR), 100 (Split AWA), 1000, 10000]
· EWC-JE ­ learning rate: [0.3, 0.1, 0.03 (Split CUB), 0.01, 0.003 (Split AWA), 0.001, 0.0003, 0.0001] ­ regularization: [1, 10 (Split CUB), 100 (Split AWA), 1000, 10000]
· PI ­ learning rate: [0.3, 0.1 (MNIST perm), 0.03 (Split CUB), 0.01 (Split CIFAR), 0.003 (Split AWA), 0.001, 0.0003, 0.0001] ­ regularization: [0.001, 0.01, 0.1 (MNIST perm, Split CIFAR, Split CUB), 1 (Split AWA), 10]
· PI-JE ­ learning rate: [0.3, 0.1, 0.03 (Split CUB), 0.01, 0.003 (Split AWA), 0.001, 0.0003, 0.0001] ­ regularization: [0.001, 0.01, 0.1 (Split CUB), 1, 10 (Split AWA)]
· MAS ­ learning rate: [0.3, 0.1 (MNIST perm), 0.03 (Split CIFAR, Split CUB), 0.01, 0.003 (Split AWA), 0.001, 0.0003, 0.0001] ­ regularization: [0.01, 0.1 (MNIST perm, Split CIFAR, Split CUB), 1 (Split AWA), 10]
· MAS-JE ­ learning rate: [0.3, 0.1, 0.03 (Split CUB), 0.01, 0.003, 0.001 (Split AWA), 0.0003, 0.0001] ­ regularization: [0.01, 0.1 (Split CUB, Split AWA), 1, 10]
· RWALK ­ learning rate: [0.3, 0.1 (MNIST perm), 0.03 (Split CIFAR, Split CUB), 0.01, 0.003 (Split AWA), 0.001, 0.0003, 0.0001] ­ regularization: [0.1, 1 (MNIST perm, Split CIFAR, Split CUB), 10 (Split AWA), 100, 1000]
· RWALK-JE ­ learning rate: [0.3, 0.1, 0.03 (SPLIT CUB), 0.01, 0.003 (Split AWA), 0.001, 0.0003, 0.0001] ­ regularization: [0.1, 1 (Split CUB), 10 (Split AWA), 100, 1000]
· A-GEM ­ learning rate: [0.3, 0.1 (MNIST perm), 0.03 (Split CIFAR, Split CUB), 0.01 (Split AWA), 0.003, 0.001, 0.0003, 0.0001]
· A-GEM-JE ­ learning rate: [0.3, 0.1, 0.03 (SPLIT CUB), 0.01, 0.003 (Split AWA), 0.001, 0.0003, 0.0001]
18

