Under review as a conference paper at ICLR 2019
SAFE POLICY LEARNING FROM OBSERVATIONS
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we consider the problem of learning a policy by observing numerous non-expert agents. Our goal is to extract a policy that, with high-confidence, acts better than the agents' average performance. Such a setting is important for real-world problems where expert data is scarce but non-expert data can easily be obtained, e.g. by crowdsourcing. Our approach is to pose this problem as safe policy improvement in reinforcement learning. First, we evaluate an average behavior policy and approximate its value function. Then, we develop a stochastic policy improvement algorithm that safely improves the average behavior. The primary advantages of our approach, termed Rerouted Behavior Improvement (RBI), over other safe learning methods are its stability in the presence of value estimation errors and the elimination of a policy search process. We demonstrate these advantages in the Taxi grid-world domain and in four games from the Atari learning environment.
1 INTRODUCTION
Recent progress in Reinforcement Learning (RL) has shown a remarkable success in learning to play games such as Atari from raw sensory input (Mnih et al., 2015; Hessel et al., 2017). Still, tabula rasa, RL typically requires a significant amount of interaction with the environment in order to learn. In real-world environments, particularly when a risk factor is involved, an inept policy might be hazardous (Shalev-Shwartz et al., 2016). Thus, an appealing approach is to record a dataset of other agents in order to learn a safe initial policy which may later be improved via RL techniques (Taylor et al., 2011).
Learning from a dataset of experts has been extensively researched in the literature. These learning methods and algorithms are commonly referred to as Learning from Demonstrations (LfD) (Argall et al., 2009). In this paper, we consider a sibling, less explored problem of learning form a dataset of observations (LfO). We define LfO as a relaxation of LfD, where: (1) we do no assume a single policy generating the data and; (2) the policies are not assumed to be optimal, nor do they cover the entire state space. Practically, LfO is of interest since it is often easier to collect observations than expert demonstrations, for example, using crowdsourcing (Kurin et al., 2017). Technically, LfO is fundamentally different from LfD, where the typical task is to clone a single expert policy. The data under the LfO setting is expected to be more diverse than LfD data, which in general can be beneficial for learning. However, it also brings in the challenge of learning from multiple, possibly contradicting, policy trajectories.
In this work, we propose to solve the LfO problem with a three phases approach: (1) imitation (2) annotation and; (3) safe improvement. The imitation phase seeks to learn the average behavior in the dataset. In the annotation part we approximate the value function of the average behavior and in the final safe improvement step (section 5), we craft a novel algorithm that takes the learned average behavior and its approximated value function and yields an improved policy without generating new trajectories. The improvement step is designed to increase the policy performance with a high confidence in the presence of the value estimation errors that exists in a LfO setup.
Our three phases approach which we term Rerouted Behavior Improvement (RBI), provides a robust policy (without any interaction with the environment), that both eliminates the risks in random policy initialization and in addition, can boost the performance of a succeeding RL process. We demonstrate our algorithm both in a Taxi grid-world (Dietterich, 2000) as well as in the Atari domain (section 6). In the latter, we tackle the challenge of learning from non-experts human players
1

Under review as a conference paper at ICLR 2019

(Kurin et al., 2017). We show that our algorithm provides a robust policy, on par with deep RL policies, using only the demonstrations and without any additional interaction with the environment. As a baseline, we compare our approach to two state-of-the-art algorithms: (1) learning from demonstrations, DQfD (Hester et al., 2018) and; (2) robust policy improvement, PPO (Schulman et al., 2017).

2 RELATED WORK
Learning from demonstrations in the context of deep RL in the Atari environment have been studied in Cruz Jr et al. (2017), DQfD (Hester et al., 2018) and recently in Pohlen et al. (2018). However, all these methods focus on expert demonstrations and they benchmark their scores after additional trajectory collection with an iterative RL process. Therefore, essentially these methods can be categorized as a RL augmented with expert's supervised data. In contrast, we take a deeper look into the first part of best utilizing the observed data to provide the highest initial performance.
Previously, LfO has often been solved solely with the imitation step, e.g. in AlphaGo (Silver et al., 2016), where the system learned a policy which mimics the average behavior in a multiple policies dataset. While this provides sound empirical results, we found that one can do better by applying a safe improvement step to boost the policy performance. A greedy improvement method with respect to multiple policies has already been suggested in Barreto et al. (2017), yet we found that practically, estimating the value function of each different policy in the dataset is both computationally prohibitive and may also produce large errors since the generated data by each different policy is typically small. In section 4 we suggest a feasible alternative. Instead of learning the value of each different policy, estimate the value of the average behavior. While such estimation is not exact, we show both theoretically and experimentally that it provides a surrogate value function that may be used for policy improvement purposes.
There is also a significant research in the field of safe RL (Garcia & Ferna´ndez, 2015), yet, here there may be multiple accepted definitions of this term ranging from worst-case criterion (Tamar et al., 2013) to baseline benchmark approach (Ghavamzadeh et al., 2016). We continue the line of research of safe RL investigated by Kakade & Langford (2002); Pirotta et al. (2013); Thomas et al. (2015) but we focus on a dataset composed of unknown policies. Finally, there are two recent well-known works in the context of non-decreasing policy improvement (also can be categorized as safe improvement) TRPO and PPO (Schulman et al., 2015; 2017). We compare our work to these algorithms and show two important advantages: First, our approach can be applied without an additional Neural Network (NN) policy optimization step and second we provide theoretical and experimental arguments why both approaches may be deemed unsafe when applied in the context of a LfO setup.

3 PROBLEM FORMULATION

We are dealing with a Markov Decision Process (MDP) (Puterman, 2014) where an agent interacts

with an environment and tries to maximize a reward. A MDP is defined by the tuple (S, A, P, R),

where S is a set of states and A is a set of actions. P : S × A  S is the set of probabilities of

switching from a state s to s when executing action a, i.e. P (s |s, a) and R is a reward function

S × A  R which defines the reward r that the agent gets when applying action a in state s.

An agent acts according to a policy , and its goal is to find a policy that maximizes the expected

cumulative discounted reward, also known as the objective function J() = E

 k=0

k

rk

s0, 

where  < 1 is a discount factor, k is a time index and s0 is an initial state.

We assume that all policies belong to the Markovian randomized set MR s.t.   MR is a

probability distribution over A given a state s, i.e.  (a|s).1 For convenient and when appropriate,

we may simply write i to denote (ai|s) (omitting the state's dependency). In the paper we will

discuss two important distance measures between policies. The first is the Total Variation (TV)

(, 

)

=

1 2

ai |i - i| and the second is the KL divergence DKL(|| ) = -

ai

i

log

i i

.

1Note that humans' policies can generally be considered as part of the history randomized set HR where   HR is a probability function over A given the states and actions history. In the appendix we explain how
we circumvented this hurdle in the Atari dataset.

2

Under review as a conference paper at ICLR 2019

These measures are often used to constrain the updates of a learned policy in an iterative policy improvement RL algorithm (Schulman et al., 2015; 2017).

For a given policy, the state's value is the expected cumulative reward starting at this state, V (s) =

E

 k=0

k

rk

s, 

.

Similarly, the Q-value function Q(s, a) is the value of taking action a in state

s and then immediately following with policy . The advantage A(s, a) = Q(s, a) - V (s) is

the gain of taking action a in state s over the average value (note that aA (a|s)A(s, a) = 0).

We denote by P (s0 -k s|) the probability of switching from state s0 to state s in k steps with a

policy .

We define the LfO problem as learning a policy  solely by observing a finite set of trajectories of

other behavior policies without interacting with the environment. Formally, we are given a dataset
D of trajectories executed by N different players each with a presumably different policy. Players are denoted by pi, i = 1, ..., N , and their corresponding policies are i, with value and Q-value functions V i, Qi respectively. D is indexed as {xj}j|D=|0, where the cardinality of the dataset is denoted by |D| and each record is the tuple xj = (sj, aj, rj, tj, ij) s.t. tj is a termination signal and ij is the player's index. D may also be partitioned to D = {Di|i = 1, .., N }, representing the
different players' records.

The paper is accompanied with a running example, based on the Taxi
Rgrid-world domain (Dietterich, 2000), In the Taxi world, the driver's task

G

is to pickup and drop a passenger in predefined locations with minimal

number of steps (See figure 1). For this example, we synthetically generated policies of the form i = i(s)rand + (1 - i(s)), where  is the optimal policy and i(s) is a different mixing parameter for each

different policy. Generally we divide the state space into two complementary and equal-size sets Si  S¯i = S, |Si|= |S¯i|. Where i(s) = 0 uefoxsreamsdipflfeeS,reirnatannsdedolmeclity(iosp)nisc=kof0hS.a7ilf5toofofgrtehsneersataSt¯etied.sifItfnoertfehonermtnteySxptiessiesocfttiedoranmtsae,sdewtisen. wFthiolerl paper as a random selection (see also appendix).

YB
Figure 1: Taxi world

4 AVERAGE BEHAVIOR POLICY AND ITS VALUE FUNCTION

We begin with the first phase of the RBI approach which is learning a behavior policy from the dataset. Learning a behavior policy is a sensible approach both for generating a sound initial performance, as well as to avoid unknown states and actions. Yet, contrary to LfD where a single expert policy is observed, in multiple policies dataset the definition of a behavioral policy is ambiguous. To that end, we define the average behavior of the dataset as a natural generalization of the single policy imitation problem.

Definition 4.1 (Average Behavior). The average behavior of a dataset D is

 (a|s) =

jD 1s,a(j) jD 1s(j)

=

pi

jDi 1s,a(j) jDi 1s(j)

jDi jD

1s(j) 1s(j)

,

(1)

where 1s,a(j) = 1 if (sj, aj) = (s, a), and 0 otherwise. The first form in Eq. equation 4.1 is simply
the fraction of each action taken in each state in D, which for a single policy dataset is identical to behavioral cloning. Typically, when  is expressed with a NN (as in section 6) we apply a standard classification learning process with a Cross Entropy loss. Otherwise, for a tabular representation (as in the Taxi example) we directly enumerate to calculate this expression. The second form in Eq. equation 4.1 is a weighted sum over all players in the dataset, which may also be expressed with conditional probability terminology as

 (a|s) = i(a|s)P pi|s .
pi

(2)

Here P pi|s is the probability of visiting an ith player's record given that a uniform sample x  U (D) picked s.

3

Under review as a conference paper at ICLR 2019

While other definitions of average behavior are possible, the ease of learning such formulation with

a NN makes it a natural candidate for RBI. Yet, for the second phase of RBI, one must evaluate its Qvalue function, i.e. Q. It is not straightforward to learn the Q-value of the average behavior since

essentially, such policy was never executed in the data.2 However, we suggest that the following

function

QD(s, a) = Qi(s, a)P pi|s, a

(3)

pi

which we term the Q-value of the dataset may be served as a surrogate value function for policy improvement purposes. QD may be interpreted as the weighted average over the players' Q-values, where the weights P pi|s, a are the probability of visiting an ith player's record given that a
uniform sample x  U (D), picked s, a. In the following two propositions, we show that such
a function has two appealing characteristics. First, it may be evaluated with a L1-norm loss and Monte-Carlo (MC) learning (Sutton & Barto, 2017) from the dataset trajectories, without Off-Policy corrections and without the burden of evaluating each Qi independently. Secondly, it is a Q-value
function of a time dependent policy with a very similar structure to the average behavior policy.
Taken together, this features provide an efficient way to approximate the value function of .

Proposition 4.1 (Consistency of MC upper bound). For an approximation Q^D and a loss

LL1 (Q^D)

=

1 |D|

jD|QD(sj, aj) - Q^D(sj, aj)|, an upper bound for the loss when |D| 

is

LL1 (Q^D)  LMC (Q^D) = ExjU(D) Q^D(sj , aj ) - Rj ,

(4)

where Rj = k0 krj+k is the sampled Monte-Carlo return (Proof in the appendix).

The attractive implication of Proposition 4.1 is that we can learn QD: (1) without approximating

and

plugging

any

sort

of

Importance

Sampling

(IS)

factor

 i

as

executed

in

Off-Policy

learning

and; (2) from complete trajectories without bootstrapping with Temporal Difference (TD) learning.

Learning a value function Off-Policy is prone to a high variance (Munos et al., 2016), particularly

when combined with a function approximation such as a NN and/or bootstrapping (Sutton & Barto,

2017).

Proposition 4.2. Given a state-action pair s~, a~, then QD(s~, a~) is the Q-value of a time dependent policy s~D,a~(a|s, k), where k is a time index and s~, a~ is a fixed initial state-action pair,

s~D,a~(a|s, k) = i(a|s)P (pi|s~, a~ -k s).
pi

(5)

Here the conditional probability is over a uniform sample x  U (Ds~,a~,k) where Ds~,a~,k is a subset of D that contains all the entries in the dataset with distance k from an entry with a state-action pair
s~, a~ (Proof in the appendix).

Proposition 4.2 indicates that when P (pi|s~, a~ -k s) can be approximated as P pi|s at least for

a finite horizon 

1 1-

,

then

QD

Q. This happens when the distribution of players in states

near s~, a~ equals to the distribution of players in those states in the entire dataset. Practically, while

both policies are not equal, they have a relatively low TV distance and therefore their Q-values are

close. To further increase the robustness of our method we add an interesting consideration: our

improvement step will rely only on the action ranking of the Q-value in each state, i.e. the order of

{Q(s, ai similarity

)b}eatwieAen(sQeeDnaenxdt

section). Q .

This,

as

we

show

hereafter,

significantly

increases

the

effective

We demonstrates the action ranking similarity between Q and QD in the Taxi grid-world example. To that end, we generated trajectories with N players according to three selection types of Si: (1) row selection (2) column selection and; (3) random selection. Each selection type provides a dif-
ferent class of policies and therefore form a different dataset (see exact definitions in the appendix).
In the first experiment (Figure 2a), we plot the average TV distance (for various initial states s~), between  and s~D,a~, as a function of the time-step for N = 2. Generally it is low, but it may be questionable whether relying on the true value of QD for the improvement step will provide adequate

2One may argue that it can be learned Off-Policy, we discuss Off-Policy learning in section 6 and show that other Off-Policy approaches yielded significantly lower results.

4

Under review as a conference paper at ICLR 2019

results. However, when we consider only the action ranking similarity (evaluated with the Pearson's rank correlation), we find even more favorable pattern.
First, in Figure 2b we plot the average rank correlation between Q and QD (for  = 0.9) as a function of the number of different policies used to generate the dataset. It is evident that the rank correlation is very high and stable for any number of policies. In the second experiment, we generated N = 2 (Figure 2c) and N = 10 (Figure 2d) policies and examined the impact of different discount factors. Also here, for the majority of practical scenarios we observe sufficiently high rank correlation. Only for a very large discount factor (close to 1) the rank correlation reduces. This happens since the long horizon accumulates more error from the difference (in high k steps) in the TV distance. In conclusion, while Proposition 4.2 state bounds on the similarity between Q and QD, evaluating the Pearson's rank correlation confirms our statement that in practice the action ranking of QD is an acceptable surrogate for the action ranking of Q.

0.5

Average TV distance vs time step row selection

column selection

0.4 random selection

0.3

0.2

0.1

1.2

Rank

Correlation vs number of policies row selection

1.2

Rank Correlation vs discount (N=2) row selection

1.1

column selection random selection

1.1

column selection random selection

1.0 1.0

0.9 0.9

0.8 0.7 constant = 0.9

0.8 0.7

1.2

Rank Correlation vs discount (N=10) row selection

1.1

column selection random selection

1.0

0.9

0.8

0.7

0.0 0 10 (a)2k0time3s0tep 40 50 0.6 0 10 (b2)0# pol3ic0ies 40 50 0.6 0.2 0(.c4) disco0u.6nt 0.8 1.0 0.6 0.2 0(.d4) disc0o.u6nt 0.8 1.0

Figure 2: Taxi: comparison between Q and QD

5 SAFE POLICY IMPROVEMENT

Our next step, is to harness the approximated Q-function QD Q (which here will be termed Q),

in order to improve the average behavior , i.e. generate a policy  such that J() - J()  0.

However, one must recall that Q is learned from a fixed, sometimes even small dataset. Therefore,

in order to guarantee improvement, we analyze the statistics of the value function's error. This

leads to an interesting observation: the Q-value has a higher error for actions that were taken less

frequently, thus, to avoid improvement penalty, we must restrict the ratio of the change in probability

ci

=

i i

.

We will use this observation to craft our third phase of RBI, i.e.

the safe improvement

step, and show that other well-known monotonic improvement methods (such as PPO (Schulman

et al., 2017) and TRPO (Schulman et al., 2015)) overlooked this consideration and therefore their

improvement method may be unsafe for a LfO setup.

5.1 SOFT POLICY IMPROVEMENT
Before analyzing the error's statistics, we begin by considering a subset of policies in MR which are proven to improve  if our estimation of Q is exact. Out of this subset we will later pick our improved policy. Recall that the most naive and also common improvement method is taking a greedy step, i.e. deterministically acting with the highest Q-value action in each state. This is known by the policy improvement theorem (Sutton & Barto, 2017), to improve the policy performance. The policy improvement theorem may be generalized to include a larger family of soft steps. Lemma 5.1 (Soft Policy Improvement). Given a policy , with value and advantage V , A, a policy   MR improves , i.e. V   V  s, if it satisfies a (a|s)A(s, a)  0 s with at least one state with strict inequality. The term a (a|s)A(s, a) is called the improvement step.3
Essentially, every policy that increases the probability of taking positive advantage actions over the probability of taking negative advantage actions achieves improvement. We will use the next Corollary to prove that our specific improvement step guarantees a positive improvement step.
3We post a proof in the appendix for completeness, though it may have been proven elsewhere.

5

Under review as a conference paper at ICLR 2019

Corollary 5.1.1 (Rank-Based Policy Improvement). Let (Ai)|iA=|1 be an ordered list of the  advantages in a state s, s.t. Ai+1  Ai, and let ci = i/i. If for all states (ci)|iA=|1 is a monotonic non-decreasing sequence s.t. ci+1  ci, then  improves  (Proof in the appendix).

5.2 STANDARD ERROR OF THE VALUE ESTIMATION

To provide a statistical argument for the expected error of the Q-function, consider learning Q with a tabular representation. The Q-function is the expected value of the random variable z(s, a) =

k0 krk|s, a, . Therefore, the Standard Error (SE) of an approximation Q^(s, a) for the Q-

value with N MC trajectories is

(s,a) =

z(s,a) , Ns(a|s)

(6)

where Ns = jD 1s(j) is the number of visitations in state s, s.t. N = (a|s)Ns. Therefore,

(s,a)



1
(a|s)

and

specifically

for

low

frequency

actions

such

estimation

may

suffer

large

SE.4

5.3 POLICY IMPROVEMENT IN THE PRESENCE OF VALUE ESTIMATION ERRORS

We now turn to the crucial question of what happens when one applies an improvement step with respect to an inaccurate estimation of the Q-function, i.e. Q^.
Lemma 5.2 (Improvement Penalty). Let Q^ = V^  + A^ be an estimator of Q with an error (s, a) = (Q - Q^)(s, a) and let  be a policy that satisfies lemma 5.1 with respect to A^. Then the following holds



V (s) - V (s)  -  kP (s -k s |) (s , a) ((a|s ) - (a|s )) ,

s S k0

aA

(7)

where the difference, denoted E(s), is called the improvement penalty (proof in the appendix).

For simplicity we may write E(s) = - s ,a (s |s)(s , a)((a|s ) - (a|s )), where (s |s) is sometimes referred to as the undiscounted state distribution of policy  given an initial state s. Since (s , a) is a random variable, it is worth to consider the variance of E(s). Assuming that the errors
(s, a) are positively correlated (since neighboring state-action pairs share trajectories of rewards)
and under the error model introduced above, it follows that

E2 (s)



s

( (s
,a

|s))22(s

,a)((a|s

)-(a|s

))2

=

s

,a

( (s

|s))2z2(s Ns

,a)

((a|s ) - (a|s (a|s )

))2 .

Hence,

it

is

evident

that

the

improvement

penalty

can

be

extremely

large

when

the

term

|-|2 

is

unregulated. Moreover, a single mistake along the trajectory, caused by an unregulated element,

might wreck the performance of the entire policy. Therefore, it is essential to regulate each one of

these elements to minimize the potential improvement penalty.

5.4 THE REROUTE CONSTRAINT

In

order

to

regulate

the

ratio

|-|2 

,

we

suggest

limiting

the

improvement

step

to

a

subset

of

M R

based on the following constraint.

Definition 5.1 (Reroute Constraint). Given a policy , a policy  is a reroute(cmin, cmax) of , if

(a|s) = c(s, a)(a|s) where c(s, a)  [cmin, cmax]. Further, note that reroute is a subset of the TV

constraint

with



=

min(1

-

cmin,

max(

cmax -1 2

,

1-cmin 2

))

(proof

in

the

appendix).

Now, it is evident that with the reroute constraint, each element in the sum of (7) is regulated and proportional to (a|s)|1 - c(s, a)| where c(s, a)  [cmin, cmax]. Analyzing other well-known

4Note that even for deterministic environments, a stochastic policy inevitably provides z(s,a) > 0.

6

Under review as a conference paper at ICLR 2019

trust regions such as the TV constraint 



1 2

a|(a|s) - (a|s)|, the average KL-divergence

constraint D¯ KL(||) = -Es[

a

(a|s)

log

(a|s) (a|s)

],

used

in

the

TRPO

algorithm,

and

the

PPO

objective function (Schulman et al., 2017), surprisingly reveals that non of them properly controls

the improvement penalty (see an example and an analysis of the PPO objective in the appendix, we

also show in the appendix that the solution of the PPO objective is not unique).

5.5 MAXIMIZING THE IMPROVEMENT STEP UNDER THE REROUTE CONSTRAINT

We now consider the last part of our improvement approach, i.e. maximizing the objective func-
tion under the reroute constraint and whether such maximization provides a positive improvement
step. It is well-known that maximizing the objective function without generating new trajecto-
ries of  is a hard task since the distribution of states induced by the policy  is unknown. Previous works have suggested to maximize a surrogate off-policy objective function JOP () = Es[ a (a|s)A(s, a)]. These works have also suggested to solve the constrained maximization with a NN policy representation and a gradient ascent approach (Schulman et al., 2015). Here we suggest a refreshing alternative, instead of precalculating the policy  that maximizes JOP one may ad hoc compute the policy that maximizes the improvement step a (a|s)A(s, a) (which is the argument of the JOP objective) for each different state. Such an approach maximizes also the JOP objective since the improvement step is independent between states. For the reroute constraint,
this essentially sums up to solving the following simple linear program for each state

Maximize: (A)T  Subject to: cmin    cmax
And: i = 1.

(8)

Where ,  and A^ are vector representations of ((ai|s))i|A=|1, ((ai|s))|iA=|1 and (A(s, a))|iA=|1 respectively. We term the algorithm that solves this maximization problem as Max-Reroute (see Algorithm 1). In the appendix we also provide an analogous algorithm that maximizes the improvement step under the TV constraint (termed Max-TV). We will use Max-TV as a baseline for the performance of the reroute constraint. With an ad hoc maximization approach, we avoid the hassle of additional learning task after the policy imitation step, and in addition, our solution guarantees maximization without the common caveats in NN learning such as converging to local minima or overfitting etc.
Further analyzing Max-Reroute and Max-TV quickly reveals that they both rely only on the action ranking at each state (as stated in the previous section). This is also in contrast with the aforementioned methods (TRPO and PPO) where by their definition as policy gradient methods (Sutton et al., 2000), they optimize the policy according to the magnitude of the advantage function. Finally, notice that both Max-Reroute and Max-TV satisfy the conditions of Corollary 5.1.1, therefore they always provide a positive improvement step and hence for a perfect approximation of the value function they are guaranteed to improve the performance.

Algorithm 1: Max-Reroute
Data: s, , A, (cmin, cmax) Result: {(a|s), a  A} begin
A~ - A  - 1 - cmin (a|s) - cmin(a|s) a  A while  > 0 do
a = arg maxaA~ A(s, a) a = min{, (cmax - cmin)(a|s)} A~ - A~/a  -  - a (a|s) - (a|s) + a

7

Under review as a conference paper at ICLR 2019

Let us now return to our Taxi example and examine three dif-

ferent types of improvement steps with respect to a behavioral cloning baseline: (1) a greedy step5 (2) a TV constrained and;

0

(3) a reroute constrained steps. The dataset is generated by two 20

policies with row selection and a discount factor  = 0.9 (the evaluation method for  and QD is in the appendix). Note that in a tabular representation, QD is undefined for missing stateaction pairs. While in such parameterization we can avoid vis-

40 60

behavioral reroute(0.5, 1.5) tv(0.25) greedy

iting unknown s, a by simply setting them with a very low Q- 80 value, in a function approximation such as a NN, the value in

such unseen states is practically uncontrolled. To examine the effect of this matter, we consider two different evaluations for unseen s, a: in Figure 3a, we assign a random value in the

100

101 102 103
(a) Number of trajectories

range [Qmin, Qmax] to an unseen pair and in Figure 3b we assign the minimal value, i.e. Qmin for such pairs. Both figures show the evaluated performance with respect to the number of
trajectories in the dataset.

0 20

Examining the score with respect to the average behavior baseline, reveals that indeed TV is not a safe step when unknown s, a are assigned a random score. On the other hand, reroute

40 60

behavioral reroute(0.5, 1.5) tv(0.25) greedy

nicely demonstrates its safety property where it always pro- 80 vides better performance than the average behavior baseline.

The results also show that for any size of the dataset the greedy policy provides the poorest performance. We found out that this is since it easily converges to recurrent states and does not

100

101
(b)

Number

of

1tr0a2jectories

103

complete the task (note that this is true for both type of experiments). For small to medium dataset sizes, the reroute step outperforms the TV step but for large datasets, when evalu-

Figure 3: Taxi: Improvement steps comparison

ation errors diminish TV is better. In real-world MDP with

larger number of states, it is extremely difficult to sufficiently

sample the entire state-space, hence, we project that reroute should be better than TV even for large

datasets. In the next section, this premise is verified in the Atari domain.

6 LEARNING TO PLAY ATARI BY OBSERVING INEXPERT HUMAN PLAYERS
In the previous section, we analyzed the expected error which motivated the reroute constraint for a tabular representation. In this section, we experimentally show that the same ideas holds for Deep NN parametric form. We conducted an experiment with a crowdsourced data of 4 Atari games (Spaceinvaders, MsPacman, Qbert and Montezuma's Revenge) (Kurin et al., 2017). Each game had roughly 1000 recorded episodes. We employed two networks, one for policy cloning  and one for Q-value estimation with architecture inspired by the Dueling DQN (Mnih et al., 2015; Wang et al., 2015) and a few modifications (see appendix). We evaluated three types of local maximization steps: (1) Max-Reroute, with different parameters (cmin, cmax); (2) Max-TV; and (3) greedy.6 We implemented two baselines: (1) DQfD algorithm with hyperparameters as in Hester et al. (2018) and (2) a single PPO policy search step based on the learned behavior policy and the estimated advantage. The following discussion refers to the results presented in Figure 4.
Average behavior performance: When comparing the average behavior performance to the average score, we get very similar performance in SpaceInvaders, lower results in MsPacman and significantly higher results in Qbert and Revenge. Generally, we expect that in games where longer trajectories lead to significantly more reward, the average policy obtains above average reward, since the effect of good players in the data has a heavier weight on the results. We assume that the lower score in MsPacman is due to the more complex and less clear frame in this game (after decimation) with respect to the other games. We take the average behavior performance as a baseline for comparing the safety level of the subsequent improvement steps.
5An unconstrained step, equivalent to reroute parameters (cmin, cmax.) = (0, ) 6We do not report the greedy step results due to a poor performance with respect to the other baselines.

8

Under review as a conference paper at ICLR 2019

mspacman
humans behavioral (r0e.r5o,1u.t5e) (0r.e2r5o,1u.t7e5) re(r0o,2u)te
4.0K

(0t.2v5)

3.0K

2.0K

1.0K

0.0
qbert

25.0K 20.0K 15.0K 10.0K 5.0K
0.0
revenge
8.0K

6.0K

4.0K

2.0K

0.0
spaceinvaders

1.2K 1.0K 0.8K 0.5K 0.2K 0.0
humans behavioral (r0e.r5o,1u.t5e) (0r.e2r5o,1u.t7e5) re(r0o,2u)te

(0t.2v5)

(p0p.5o) (p0p.5o)

dqfd dqfd

(a) Box Plot of final policy performance

1.5K 1.2K 1.0K 0.8K 0.5K 0.2K 0.0
12.0K 10.0K 8.0K 6.0K 4.0K 2.0K
0.0
2.5K 2.0K 1.5K 1.0K 0.5K 0.0
0.6K
0.4K
0.2K
0.0 0

mspacman
behavioral reroute(0.5,1.5) tv(0.25) dqfd
qbert

revenge

spaceinvaders

0.5M Iteration 1M (b) Learning Curves

1.5M

Figure 4: Atari experiment results

Local maximization steps: We chose to evaluate Max-TV with  = 0.25 since it encapsulates the reroute(0.5, 1.5) region. It is clear that Max-TV is always dominated by Max-Reroute and it did not secure safe improvement in any of the games. A comparison of different reroute parameters reveals that there is an fundamental trade-off between safety and improvement. Smaller steps like reroute(0.5, 1.5) support higher safety at the expense of improvement potential. In all games reroute(0.5, 1.5) provided safe improvement, but in Qbert its results were inferior to reroute(0, 2). On the other hand, reroute(0, 2) reduced the Revenge score which indicates a too greedy step. Our results indicates that it is important to set cmin > 0 since avoiding so may discard some important actions altogether due to errors in the value evaluation, resulting in a poor performance.
Comparison with PPO: For the PPO baseline, we executed a policy search with the learned advantage A according to the PPO objective

J P P O() = Es

(a|s) min A(s, a) (a|s) , A(s, a) clip (a|s) , 1 - , 1 + 

(a|s)

(a|s)

aA

.

We chose  = 0.5, motivated by the similarity to the reroute(0.5, 1.5) region (see appendix). Contrary to the PPO paper, we plugged our advantage estimator and did not use the Generalized Advantage Estimation (GAE). While P P O(0.5) scored (see box plot in Figure 4a) slightly better than reroute(0.5, 1.5) in Qbert, in all other games it reduced the behavioral cloning score. The

9

Under review as a conference paper at ICLR 2019
overall results indicate the similarity between P P O(0.5) and reroute(0, 2), probably since negative advantages actions tend to settle at zero-probability to avoid negative penalty. This emphasizes the importance of the cmin parameter of reroute which is missing from PPO. Comparison with DQfD and Off-Policy greedy RL: DQfD scored below the average behavior in all games. The significantly low scores in MsPacman and Qbert raise the question whether DQfD and more generally, Off-Policy greedy RL can effectively learn from multiple non-exploratory fixed policies. The most conspicuous issue is the greedy policy improvement approach taken by DQfD: we have shown that an unconstrained greedy improvement step leads to a poor performance (recall the Taxi example). However, Off-Policy RL also suffers from the second RL ingredient, i.e. policy evaluation. Notice that contrary to conventional Off-Policy learning, the behavior policy is unknown and should be estimated from the data. In practice, a NN might provide inaccurate estimation of the probability function when classes are imbalanced (Guo et al., 2008). This might lead to significant errors in the evaluation step, which in turn might lead to a significant improvement penalty in an iterative RL process. As our results show, our proposed safe policy improvement scheme mitigates these issues, leading to significantly better results
7 CONCLUSIONS
In this paper, we studied both theoretically and experimentally the problem of LfO. We analyzed factors that impede classical methods, such as TRPO/PPO and Off-Policy greedy RL, and proposed a novel alternative, Rerouted Behavior Improvement (RBI), that incorporates behavioral cloning and a safe policy improvement step. RBI is designed to learn from multiple agents and to mitigate value evaluation errors. It does not use importance sampling corrections or bootstrapping to estimate values, hence it is less sensitive to deep network function approximation errors. In addition, it does not require a policy search process. Our experimental results in the Atari domain demonstrate the strength of RBI compared to current state-of-the-art algorithms. We project that these attributes of RBI would also benefit an iterative RL process. Therefore, in the future, we plan to study RBI as an online RL policy improvement method.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469­483, 2009.
Andre´ Barreto, Will Dabney, Re´mi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. In Advances in neural information processing systems, pp. 4055­4065, 2017.
Gabriel V Cruz Jr, Yunshu Du, and Matthew E Taylor. Pre-training neural networks with human demonstrations for deep reinforcement learning. arXiv preprint arXiv:1709.04083, 2017.
Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decomposition. Journal of Artificial Intelligence Research, 13:227­303, 2000.
Javier Garcia and Fernando Ferna´ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437­1480, 2015.
Mohammad Ghavamzadeh, Marek Petrik, and Yinlam Chow. Safe policy improvement by minimizing robust baseline regret. In Advances in Neural Information Processing Systems, pp. 2298­2306, 2016.
Xinjian Guo, Yilong Yin, Cailing Dong, Gongping Yang, and Guangtong Zhou. On the class imbalance problem. In Natural Computation, 2008. ICNC'08. Fourth International Conference on, volume 4, pp. 192­201. IEEE, 2008.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. 2018.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In ICML, volume 2, pp. 267­274, 2002.
Vitaly Kurin, Sebastian Nowozin, Katja Hofmann, Lucas Beyer, and Bastian Leibe. The atari grand challenge dataset. arXiv preprint arXiv:1705.10998, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Re´mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054­1062, 2016.
Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Adaptive step-size for policy gradient methods. In Advances in Neural Information Processing Systems, pp. 1394­1402, 2013.
Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, Gabriel Barth-Maron, Hado van Hasselt, John Quan, Mel Vecer´ik, et al. Observe and look further: Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
Jeff Racine. Consistent cross-validatory model-selection for dependent data: hv-block crossvalidation. Journal of econometrics, 99(1):39­61, 2000.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889­1897, 2015.
11

Under review as a conference paper at ICLR 2019
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484­489, 2016.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000.
RS Sutton and AG Barto. Reinforcement learning: An introduction, (complete draft), 2017. Aviv Tamar, Huan Xu, and Shie Mannor. Scaling up robust mdps by reinforcement learning. arXiv
preprint arXiv:1306.6189, 2013. Matthew E Taylor, Halit Bener Suay, and Sonia Chernova. Integrating reinforcement learning with
human demonstrations of varying ability. In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp. 617­624. International Foundation for Autonomous Agents and Multiagent Systems, 2011. Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy improvement. In International Conference on Machine Learning, pp. 2380­2388, 2015. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.
12

Under review as a conference paper at ICLR 2019

A OPERATOR NOTATION
In part of the appendix, we use vector and operator notation, where v and q represent the value and Q-value functions as vectors in V  R|S| and Q  R|S×A|. We denote the partial order x  y  x(s, a)  y(s, a) s, a  S × A, and the norm x = maxs,a|x|. Let us define two mappings between V and Q:
1. Q  V mapping : (q) (s) = aA (a|s)q(s, a) = v(s). This mapping is used to backup state-action-values to state value.
2. V  Q mapping P~: P~v (s, a) = s S P (s |s, a)v(s ). This mapping is used to backup state-action-values based on future values. Note that this mapping is not dependent on a specific policy.
Let us further define two probability operators:
1. V  V state to state transition probability Pv: (Pvv) (s) = aA (a|s) s S P (s |s, a)v(s ).
2. Q  Q state-action to state-action transition probability Pq: Pqq (s, a) = s S P (s |s, a) a A (a |s )q(s , a ).
Note that the probability operators are bounded linear transformations with spectral radios (Pv)  1 and (Pq)  1 (Puterman, 2014). Prominent operators in the MDP theory are the recursive value operator Tvv = r + Pvv and recursive Q-value operator Tqq = r + Pqq. Both have a unique solution for the singular value equations v = Tvv and q = Tqq. These solutions are v and q respectively (Sutton & Barto, 2017; Puterman, 2014). In addition, in the proofs we will use the following easy to prove properties:
1. Pq = P~ 2. Pv = P~ 3. x  y, x, y  V,  P~x  P~y
4. The probability of transforming from s to s in k steps is P (s -k s |) = (Pv)k(s, s ). 5. [(Pv)kx](s) = Esk|s [x(sk)]

B AVERAGE BEHAVIOR PROOFS

B.1 UPPER BOUND FOR MONTE-CARLO EVALUATION OF QD WITH L1 LOSS

Proof. We denote by LL1 (Q^D) the true L1 least absolute error loss of the value function estimator, and we show that LMC (Q^D)  LL1 (Q^D) for |D|  such that minimization of LMC leads to a bounded error of LL1 . Remember that the dataset D is a concatenation of the sets Di = {(s1, a1), (s2, a2), ...}, i = 1, .., N .

LL1 (Q^D)

=

1 |D|

QD(sj, aj) - Q^D(sj, aj) =

jD



1 |D|

 Qi(sj, aj)P pi|sj, aj - Q^D(sj, aj) +

jD< pi



1 + |D|

Q^D(sj, aj) - Qi(sj, aj)P pi|sj, aj 

jD>

pi

13

Under review as a conference paper at ICLR 2019

where < and > denote the complementary subspaces of S × A where Q^D underestimates and overestimates the target (respectively).



1 |D|

 Qi(sj, aj)P pi|sj, aj - Q^D(sj, aj) =

jD< pi

1 |D|

P pi|sj, aj Qi(sj, aj) - Q^D(sj, aj) 

jD< pi

1 |D|

P pi|sj, aj Qi(sj, aj) - Q^D(sj, aj) .

jD< pi

Joining the equivalent term for the > region, we obtain the inequality

LL1 (Q^D)



1 |D|

P pi|sj, aj Qi(sj, aj) - Q^D(sj, aj) =

jD pi



1 |D|

 1s,a(j)

unique(s,a)D jD

pi

jDi 1s,a(j) jD 1s,a(j)

Qi(sj, aj) - Q^D(sj, aj)

=



1 |D|

 1s,a(j) Qi(sj, aj) - Q^D(sj, aj) =

unique(s,a)D pi jDi

1 |D|

Qij (sj , aj ) - Q^D(sj , aj )

1 = |D|

E Rj |sj , aj , pij - Q^D(sj , aj )

jD

jD

Plugging in the Jensen inequality for all sj, aj, pij

E Rj |sj , aj , pij - Q^D(sj , aj )  E |Rj - Q^D(sj , aj )| sj , aj , pij

we obtain

LL1 (Q^D)



1 |D|

E |Rj - Q^D(sj , aj )| sj , aj , pij .

jD

In the limit when |D|  the sample average converges to the expectation, i.e.

1 |D|

E [·], therefore

LL1 (Q^D)



1 |D|

E |Rj - Q^D(sj , aj )| sj , aj , pij

jD

= E E |Rj - Q^D(sj , aj )| sj , aj , pij

jD [·] 

= E |Rj - Q^D(sj, aj)|

1 = |D|

|Rj - Q^D(sj, aj)|= LMC (Q^D).

jD

B.2 QD IS A VALUE OF A TIME DEPENDENT POLICY s~D,a~

Given a state-action pair s~, a~, then QD(s~, a~) is the Q-value of a time dependent policy s~D,a~(a|s, k), where k is a time index and s~, a~ is a fixed initial state-action pair,

s~D,a~(a|s, k) = i(a|s)P (pi|s~, a~ -k s).
pi

(9)

Here the conditional probability is over a uniform sample x  U (Ds~,a~,k) where Ds~,a~,k is a subset of D that contains all the entries in the dataset with distance k from an entry with a state-action pair
s~, a~.

14

Under review as a conference paper at ICLR 2019

Proof. For the proof, let us construct an extended MDP, with an additional initial state, termed s0 with N different actions. Following an action selection in state s0, an action a~ is executed in state s~ (in the original MDP) and then the original MDP continues as usual.
Let us define a History Randomized (HR) policy h  HR according to the following rule:

1. In state s0, select an action i with probability P (pi|s~, a~). 2. In all consecutive states act with the Markovian policy i.

Notice that it is clearly a HR policy since the second step is determined by the history (that is the first step). It is straightforward to calculate the value of the s0 state with the law of total probability s.t. V h (s0) =  Qi(s~, a~)P (pi|s~, a~) = QD(s~, a~).
Puterman (2014) proved (Theorems 5.5.1-3) that for every HR policy, there exists a time dependent Markovian policy with the same value, let it be denoted as s~D,a~. The Markovian policy satisfies
s~D,a~(a|s, k) = P (a|s, k, h),
that is the probability of executing action a in state s and time step k when following h. Again, with the law of total probability this may be written as

s~D,a~(a|s, k) = i(a|s)P (a(s0) = i|s0 -k s),
i
where a(s0) is the chosen action in state s0. Notice that s~, a~ deterministically follow state s0 so by applying Bayes' theorem, we get that

P (a(s0)

=

i|s0

-k

s)

=

P (a(s0)

=

i)P (s~, a~ -k s|a(s0) P (s~, a~ -k s)

=

i)

=

P (pi|s~, a~)P (s~, a~ -k s|i) .
pj P (pj|s~, a~)P (s~, a~ -k s|j)

Let us define by 1s~,a~,k,s an indicator function which equals 1 for the entries in the dataset which
are in state s in a distance k from an entry with state-action pair s~, a~. We may represent the above
expression with the indicator summation notation

1P (pi|s~, a~)P (s~, a~ -k s|i)
=
1pj P (pj|s~, a~)P (s~, a~ -k s|j)

pi

1jDi s~,a~ 1jD s~,a~

1jDi s~,a~,k,s 1jDi s~,a~

1jDi s~,a~ 1jD s~,a~

1jDi s~,a~,k,s 1jDi s~,a~

=

jDi

s~,a~,k,s
.

jD s~,a~,k,s

Finally, the last expression can be transformed back to conditional probability notation of the form

1jDi s~,a~,k,s 1jD s~,a~,k,s

= P (pi|s~, a~ -k s)

C SAFE POLICY IMPROVEMENT

C.1 REROUTE IS A SUBSET OF TV

The set of reroute policies with [cmin, cmax] is a subset of the set of -TV policies, where  =

min(1

-

cmin,

max(

cmax 2

-1

,

1-cmin 2

)).

Proof.

For a reroute

policy,

the TV

distance is

1 2

a (ai|s)|1 - ci|

a (ai|s) = . For the

second upper bound 1 - cmin, notice that a rerouted policy {i} may be written as i = cmini + i,

where i  0. Therefore, the TV distance is

1 TV =
2

|i

- i|=

1 2

|cmini

+

i

-

i|

1 2

((1 - cmin)i + i) ,

ii

i

15

Under review as a conference paper at ICLR 2019

but 1 =

i i =

i(cmini + i) = cmin + i i. Thus, i i = 1 - cmin and we may write

TV



1 2

((1

-

cmin)

+

(1

-

cmin))

=

1

-

cmin.

To show that the TV region is not in reroute, take any policy with a zero probability action and another action with a probability of (ai|s)  . The policy which switches probabilities between these two actions is in T V () but it is not in reroute for any finite cmax.

C.2 SOFT POLICY IMPROVEMENT
The soft policy improvement rule states that every policy  that satisfies
(a|s)A(s, a)  0 s  S~
a
improves the policy  s.t. V   V  s. To avoid stagnation we demand that the inequality be strict for at least a single state. In operator notation, the last equation can be written as
a  0

Proof. Plugging in a = q - v and using v = v, we get that v  q = (r + P~v) = r + Pvv.
Then, by applying (10) recursively we get

(10)

v  r + Pvv = r + Pv(r + Pvv)  r + Pvr + 2(Pv)2v  r+
Pvr + 2(Pv)2r + 3(Pv)3v  ...  k(Pv)kr = (I - Pv)-1r = v.
k0

(Schulman et al., 2015) have shown that 
J () - J () = ET   kA(sk, ak) .
k0
This equation can also be written as

J () - J () = kEsk|s0

(a|sk)A(sk, ak) =

k0

aA

= k P (s0 -k s|) (a|s)A(s, a) =

k0 sS

aA



 kP (s0 -k s|) (a|s)A(sk, a).

sS k0

aA

Recall the definition of the discounted distribution of states
(s) = kP (s0 -k s|),
k0

we conclude that

J() - J() = (s) (a|s)A(s, a)

sS

aA

16

Under review as a conference paper at ICLR 2019

C.3 RANK-BASED POLICY IMPROVEMENT

Proof. Denote by i the index of the advantage ordered list {Ai}. Since ai iAi = 0, we can write it as an equation of the positive and negative advantage components

N in
iAi = i(-Ai),

a=ip

a=0

where ip is the minimal index of positive advantages and in is the maximal index of negative advantages. Since all probability ratios are non-negative, it is sufficient to show that

N in
ciiAi  cii(-Ai ).

a=ip

a=0

But clearly

N N in in

ciiAi  cip

iAi  cin i(-Ai)  cii(-Ai).

a=ip

a=ip

a=0

a=0

C.4 POLICY IMPROVEMENT PENALTY

Let Q^ be an approximation of Q with an error (s, a) = (Q - Q^)(s, a) and let  be a policy that satisfies the soft policy improvement theorem with respect to Q^. Then the following holds:



V (s) - V (s)  -  kP (s -k s |) (s , a) ((a|s ) - (a|s )) .

s S k0

aA

(11)

The proof resembles the Generalized Policy improvement theorem (Barreto et al., 2017).

Proof. We will use the equivalent operator notation. Define the vector  = q - q^
Tqq^ = r + P~q^  r + P~q^ = r + P~q - P~ = Tqq - Pq = q - Pq = q^ + (1 - Pq)
.

Note that the inequality is valid, since q^  q^ (by the theorem assumptions), and if v  u

then P~v  P~u. Set y = (1 - Pq) and notice that Tq(q^ + y) = Tqq^ + Pqy. By induction,

we show that

n

(Tq)nq^  q^ + k(Pq)ky.

k=0

We showed it for n = 1, assume it holds for n, then for n + 1 we obtain

n
(Tq)n+1q^ = Tq(Tq)nq^  Tq q^ + k(Pq)ky =

k=0

n n n+1
= Tqq^ + k+1(Pq)k+1y  q^ + y + k+1(Pq)k+1 = q^ + k(Pq)ky.

k=0

k=0

k=0

17

Under review as a conference paper at ICLR 2019

Using the contraction properties of Tq, s.t. limk(Tq)kx = q, x  Q and plugging back (1 - Pq) = y, we obtain

q = nlim(Tq)n(q^)  q^ + k(Pq)k(1 - Pq)
k0
= q^ +  + k+1(Pq)k(Pq - Pq).
k0
Applying  we transform back into V space
v  q^ +  + k+1(Pq)k(Pq - Pq)
k0
 q^ +  + k+1(Pq)k(Pq - Pq)
k0
= v + ( - ) + k+1(Pq)k(Pq - Pq)
k0
.
Notice that (Pq)kPq = (P~)kP~ = (P~)k+1 = (Pv)k+1, and in the same manner, (Pq)kPq = (Pv)k+1. Therefore, we can write
v  v - k(Pv)k( - ),
k0
which may also be written as (11).

C.5 UNBOUNDED PROBABILITY RATIOS IN THE TV AND KL CONSTRAINTS

To verify that TV and KL do not regulate the probability ratios, let's consider a tiny example of maximizing the objective function JP P O for a single state MDP with two actions {a0, a1}. Assume a behavior policy  = {1, 0} and an estimated advantage A = {0, 1}. We search for a policy
 = {1 - , } that Maximize improvement step under the TV or KL constraints.

For a -TV constraint,

1 2

(1

-

(1

-

)

+

)

=





.

The improvement step in this case is

ai
(and

Ai i = . undefined).

Hence the Similarly

solution is  =  and the probability ratio (a1)/(a1) is unconstrained for a -KL constraint we get - log    and the improvement step is

identical. Hence  = e- and again, no constraint is posed on the probability ratio.

C.6 MAX-TV

Algorithm 2: Max-TV
Data: s, , AD,  Result: {(a|s), a  A}
begin (a|s) - (a|s), a  A a = arg maxaA~ AD(s, a)  = min{, 1 - (a|s)} (a|s) - (a|s) + 
A~ - A
while  > 0 do a = arg minaA~ AD(s, a) a = min{, (a|s)} A~ - A~/a  -  - a (a|s) - (a|s) - a

18

Under review as a conference paper at ICLR 2019

C.7 THE PPO OBJECTIVE FUNCTION

Recently, (Schulman et al., 2017) have suggested a new surrogate objective function, termed Proximal Policy Optimization (PPO), that heuristically should provide a reliable performance as in TRPO without the complexity of a TRPO implementation:

J P P O() = Es

(a|s) min

A

(s,

a)

(a|s) (a|s)

,

A

(s,

a)

clip

(a|s) (a|s)

,

1

-

,

1

+



aA

,

where  is a hyperparameter. PPO tries to ground the probability ratio by penalizing negative advantage actions with probability ratios above 1-. In addition, it clips the objective for probability ratios above 1 +  so there is no incentive to move the probability ratio outside the interval [1 - , 1 + ].
However, we show in the following that the solution of PPO is not unique and is dependent on the
initial conditions, parametric form and the specific optimization implementation. This was also ex-
perimentally found in (Henderson et al., 2017). The effect of all of these factors on the search result
is hard to design or predict. Moreover, some solutions may have unbounded probability ratios, in this sense, JP P O is not safe.

First, notice that PPO maximization can be achieved by ad hoc maximizing each state since for each

state the objective argument is independent and there are no additional constraints. Now, for state

s, let's divide A into two sets: the set of positive advantage actions, denoted A+, and the set of

negative

advantage

actions,

A-.

For

convenience,

denote

ci

=

 (ai |s)  (ai |s)

and

i

=

(ai|s).

Then,

we

can write the PPO objective of state s as

J P P O(, s) =

iAi min (ci, 1 + ) -

i(-Ai ) max (ci, 1 - ) .

ai A+

ai A-

Clearly maximization is possible (yet, still not unique) when setting all ci = 0 for ai  A-, namely, discarding negative advantage actions. This translates into a reroute maximization with parameters (cmin, cmax) = (0, 1 + )







arg max[JP P O(, s)] = arg max 
ci ci

iAi

min

(ci,

1

+

)

=

arg

max
ci



ciiAi 

ai A+

ai A+

for ci  1 + . The only difference is that the sum i cii = 1 -  may be less then 1. In this case, let us take the unsafe approach and dispense  to the highest ranked advantage. It is clear that partition of  is not unique, and even negative advantage actions may receive part of it as long as their total probability is less than (1-)i. We summarize this procedure in the following algorithm.

Algorithm 3: Ad hoc PPO Maximization
Data: s, , A,  Result: {(a|s), a  A} begin
A~ = {A~+, A~-}
A~ - A+  - 1 (a|s) - 0 a  A
while  > 0 and |A~|> 0 do a = arg maxaA~ A(s, a) a = min{, (1 + )(a|s)} A~ - A~/a  -  - a (a|s) - (a|s) + a
a = arg maxaA A(s, a) (a|s) - (a|s) + 

19

Under review as a conference paper at ICLR 2019

D TAXI EXPERIMENT: TECHNICAL DETAILS

In the paper, we presented three selection types of the subset of the optimal policy, i.e. Si: (1)

row selection (2) column selection and; (3) random selection. In the Taxi environment the states are

enumerated {sj}5j=001. For N players, the definitions for our the selection types are

Row Selection:

sj  Si

j

-

500 i

N

(mod500) < 250

Column Selection:

sj



Si

(j

+

i) (modN )

<

N 2

Random Selection: Randomly (and uniformly) choose 250 states out of the 500 state for each different Si.

For the improvement step experiment, we evaluated the behavior policy  with a tabular repre-

sentation, i.e., (a|s) =

Ns,a Ns

(see

the first

item

in Definition 4.1).

The dataset's value func-

tion QD is evaluated with Monte-Carlo (MC) returns ( = 0.9) and a tabular representation, i.e.

QD(s, a) =

,j Rs,a(j)
Ns,a

where

Rs,a(j)

is

a

single

instance

of

the

total

discounted

return

from

a

state-action pair s, a, up to the terminal state.

E ATARI EXPERIMENT: TECHNICAL DETAILS
E.1 DATASET PREPROCESSING
The dataset (Kurin et al., 2017) (Version 2) does not contain an end-of-life signal. Therefore, we tracked the changes of the life icons in order to reconstruct the end-of-life signal. The only problem with this approach was in Qbert where the last life period has no apparent icon but we could not match a blank screen since, during the episode, life icons are flashed on and off. Thus, the last endof-life signal in Qbert is missing. Further, the dataset contained some defective episodes in which the screen frames did not match the trajectories. We found them by comparing the last score in the trajectory file to the score that appears in the last or near last frame.
We also found some discrepancies between the Javatari simulator used to collect the dataset and the Stella simulator used by the OpenAI gym package where we evaluated the agents' scores:
· The Javatari reward signal has an offset of -2 frames. We corrected this shift in the preprocessing phase.
· The Javatari actions signal has an offset of  -2 frames (depending on the action type), where it is sometimes recorded non-deterministically s.t. the character executed an action in a frame before the action appeared in the trajectory file. We corrected this shift in the preprocessing phase.
· The Javatari simulator is not deterministic, while Stella can be defined as deterministic. This has the effect that icons and characters move in different mod4 order, which is crucial when learning with frame skipping of 4. Thus, we evaluated the best offset (in terms of score) for each game and sampled frames according to this offset.
· There is a minor difference in colors/hues and objects location between the two simulators.
E.2 LEARNING FROM HUMAN PLAYERS
The Atari dataset introduced a further challenge of learning from human players. Contrary to RL agents, while much of the developed theory in previous sections assumes observation of Markovian policies, humans do not play with MR policies. We found that two significant sources of NonMarkovian behavior are: (1) delayed response and (2) action repetition. The first happens due to an unavoidable delay between eye and hand movement. In simple words, humans respond to a past state. The second implies that actions depend on action history, which violates the Markovian assumption. Practically, when using a NN classifier, such Non-Markovian behavior hurts the classification since the network learns to predict past actions. We found that training the network to predict

20

Under review as a conference paper at ICLR 2019

an action in the near future (6 frames away, i.e. 0.1 seconds) can mitigate such a non-Markovian nature. This way there is less correlation between the present state and the executed action, and the human delayed response is mitigated.

E.3 NETWORK ARCHITECTURE

A finite dataset introduces overfitting issues. To avoid overfitting, DQN constantly updates a replay buffer with new samples. In a finite dataset this is not possible, but, contrary to many Deep Learning tasks, partitioning into training and validation sets is also problematic: random partitions introduce a high correlation between training and testing, and blocking partitioning (Racine, 2000) might miss capturing parts of the action-states space. Moreover, the ultimate learning goal, i.e. the playing score, is not necessarily captured via the loss function score. In LfO, evaluating the agent's score to avoid overfitting violates the assumption of learning only from the dataset. Fortunately, we found that Dropout (Srivastava et al., 2014), as a source of regularization, improves the network's resiliency to overfitting. In addition, it has the benefit of better generalization to new unseen states since the network learns to classify based on a partial set of features. We added two sources of dropout: (1) in the input layer (25%) and (2) on the latent features i.e. before the last layer (50%).

We also found that for a finite dataset with a constant statistics, Batch Normalization (BN) can increase the learning rate. Therefore, we used BN layer before the ReLu nonlinearities. Note that for an online RL algorithm, BN may sometimes impede learning because the statistics may change during the learning process.

To estimate the advantage for the PPO objective, we used a trick inspired by the Duelling DQN

architecture (Wang et al., 2015). We added a single additional output to the last layer of the Q network which represents the value V . The other |A| outputs represents the unnormalized advantages
{A~i}i and the Q-function outputs are therefore expressed as

Qi = V  + A~i - A~j j ,

(12)

j

where {j}j are the outputs of the policy network. Note that i Qi = V . The normalized advantage is therefore Ai = A~i - j A~jj.

Finally, we also shaped a reward for an end-of-life signal (for all our experiments including DQfD). Usually, DQN variants set an end-of-life signal only as a termination signal so that the agent learns that near end-of-life states have a 0 value and, as a consequence, states that dodge termination are preferred. Since RBI policy is stochastic and does not just choose the best one, it is helpful to differentiate between near zero reward and termination. Thus, we added a negative reward (-1) for a terminal state.

To summarize, we used two DQN style networks: one for  and the second for Q. We used the same network architecture as in (Mnih et al., 2015) except for the following add-ons:

· A batch normalization layer before nonlinearities. · A 25% Dropout in the input layer. · A 50% Dropout in the latent features layer. To compensate for the dropped features we
expanded the latent layer size to 1024, instead of 512 as in the original DQN architecture. · An additional output that represents the state's value.

The overall architecture is depicted in figure 5. For the behavioral cloning, we used the CrossEntropy Loss. For the value network, though the theoretical bound in the paper is calculated for the L1 loss, we found that slightly better results are obtained with the M SE loss. This may be due to better regression of outliers. All results reported in the paper used the MSE loss for value.

E.4 LEARNING PROCESS
A single iteration of our learning process is depicted with the following PyTorch style pseudo-code. Value and Behavioral networks are trained simultaneously on the same sample. The state is passed through the beta net once in training mode (with dropout) and once in evaluation mode (without dropout) for the advantage calculation.

21

Under review as a conference paper at ICLR 2019
beta net . train () value net . train () for sample in train loader :
s , a , r mc = sample beta = beta net (s) beta net . eval () beta eval = beta net (s) beta net . train () q , adv , v = value net ( s , b e t a e v al ) q a = q[a] l o s s v = MSELoss ( q a , r mc ) loss beta = CrossEntropyLoss ( beta , a) # e x e c u t e g r a d i e n t d e s c e n t w i t h Adam # optimization over l o s s b e t a and l o s s v
Figure 5: Network architecture
E.5 EVALUATION The execution of a RBI policy at evaluation time is depicted with the following pseudo-code. beta net . eval () value net . eval () t =0 score = 0 s = env . reset () while not t :
beta = beta net (s) q , adv , v = value net ( s , beta ) pi = max reroute ( beta , adv , c min , c max ) a = choice ( pi ) s , r , t = env . step ( a ) s c o r e += r
22

Under review as a conference paper at ICLR 2019

E.6 HYPERPARAMETERS TABLES Table 1: Policy and Q-value networks Hyperparameters

Name

Value

Last linear layer size Optimizer Learning Rate Dropout [first layer] Dropout [last layer] Minibatch Iterations Frame skip Reward clip

1024 Adam 0.00025 0.25
0.5 32 1562500 4 -1,1

Table 2: DQfD Hyperparameters

Name

Value

Optimizer Learning Rate n-steps Target update period laE Priority Replay exponent Priority Replay IS exponent Priority Replay constant Other parameters

Adam 0.0000625
10 16384
0.8 0.4 0.6 0.001 As in policy/value networks (except Dropout)

F FINAL POLICY PERFORMANCE TABLE

Table 3: Final scores table

Method

MsPacman Qbert Revenge SpaceInvaders

Humans Behavioral cloning Reroute-(0.5, 1.5) Reroute-(0.25, 1.75) Reroute-(0, 2) TV(0.25) PPO(0.5) DQfD

3024 1389 1452 1423 1386 1046 1333
83

3401 6921 12675 13931 14898 2503 12938 1404

946 1784 2425 2432 872 478 467 1315

634 636 625 592 560 505 523 402

23

