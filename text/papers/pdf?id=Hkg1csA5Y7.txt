Under review as a conference paper at ICLR 2019

A FAST QUASI-NEWTON-TYPE METHOD FOR LARGE-
SCALE STOCHASTIC OPTIMISATION
Anonymous authors Paper under double-blind review

ABSTRACT
During recent years there has been an increased interest in stochastic adaptations of limited memory quasi-Newton methods, which compared to pure gradientbased routines can improve the convergence by incorporating second order information. In this work we propose a direct least-squares approach conceptually similar to the limited memory quasi-Newton methods, but that computes the search direction in a slightly different way. This is achieved in a fast and numerically robust manner by maintaining a Cholesky factor of low dimension. This is combined with a stochastic line search relying upon fulfilment of the Wolfe condition in a backtracking manner, where the step length is adaptively modified with respect to the optimisation progress. We support our new algorithm by providing several theoretical results guaranteeing its performance. The performance is demonstrated on real-world benchmark problems which shows improved results in comparison with already established methods.

1 INTRODUCTION

In learning algorithms we often face the classical and hard problem of stochastic optimisation where we need to minimise some non-convex cost function f (x)

min f (x),
xRd

(1)

when we only have access to noisy evaluations of the function and its gradients. We take a particular

interest in situations where the number of data and/or the number of unknowns d is very large.

The importance of this problem has been increasing for quite some time now. The reason is simple: many important applied problems ask for its solution, including most of the supervised machine learning algorithms when applied to large-scale settings. There are two important situations where the non-convex stochastic optimisation problem arise. Firstly, for large-scale problems it is often prohibitive to evaluate the cost function and its gradient on the entire dataset. Instead, it is divided into several mini-batches via subsampling, making the problem stochastic. This situation arise in most applications of deep learning. Secondly, when randomised algorithms are used to approximately compute the cost function and its gradients the result is always stochastic.

Our contributions are: 1. A new stochastic line search algorithm allowing for adaptive step-lengths akin to what is done in the corresponding state-of-the-art deterministic optimisation algorithms. This is enabled via a stochastic formulation of the first Wolfe condition. 2. We provide a new and efficient way of incorporating second-order (curvature) information into the stochastic optimiser via a direct least-squares approach conceptually similar to the popular limited memory quasi-Newton methods. 3. To facilitate a fast and numerically robust implementation we have derived tailored updating of a small dimension Cholesky factor given the new measurement pair (with dimension equal to the memory length). 4. We support our new developments by establishing several theoretical properties of the resulting algorithm. The performance is also demonstrated on real-world benchmark problems which shows improved convergence properties over current state-of-the-art methods.

2 RELATED WORK
Due to its importance, the stochastic optimisation problem is rather well studied by now. The first stochastic optimisation algorithm was introduced by Robbins & Monro (1951). It makes use of first-

1

Under review as a conference paper at ICLR 2019

order information only, motivating the name stochastic gradient (SG), which is the contemporary term (Bottou et al., 2018) for these algorithms, originally referred to as stochastic approximation. Interestingly most SG algorithms are not descent methods since the stochastic nature of the update can easily produce a new iterate corresponding to an increase in the cost function. Instead, they are Markov chain methods in that their update rule defines a Markov chain.
The basic first-order SG algorithms have recently been significantly improved by the introduction of various noise reduction techniques, see e.g. (Johnson & Zhang, 2013; Schmidt et al., 2013; Konecný & Richtárik, 2017; Defazio et al., 2014).
The well-known drawback of all first-order methods is the lack of curvature information. Analogously to the deterministic setting, there is a lot to be gained in extracting and using second-order information that is maintained in the form of the Hessian matrix. The standard quasi-Newton method is the BFGS method, named after its inventors (Broyden, 1967; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970). In its basic form, this algorithm does not scale to the large-scale settings we are interested in. The idea of only making use of the most recent iterates and gradients in forming the inverse Hessian approximation was suggested by Nocedal (1980) and Liu & Nocedal (1989). The resulting L-BFGS method is computationally cheaper with a significantly reduced memory footprint. Due to its simplicity and good performance, this has become one of the most commonly used second-order methods for large-scale problems. Our developments makes use of the same trick underlying L-BFGS, but it is carefully tailored to the stochastic setting.
Over the past decade we have witnessed increasing capabilities of these so-called stochastic quasiNewton methods, the category to which our developments belong. The work by Schraudolph et al. (2007) developed modifications of BFGS and its limited memory version. There has also been a series of papers approximating the inverse Hessian with a diagonal matrix, see e.g. Bordes et al. (2009) and Duchi et al. (2011). The idea of exploiting regularisation together with BFGS was successfully introduced by Mokhtari & Ribeiro (2014). Later Mokhtari & Ribeiro (2015) also developed a stochastic L-BFGS algorithm without regularisation. The idea of replacing the stochastic gradient difference in the BFGS update with a subsampled Hessian-vector product was recently introduced by Byrd et al. (2016), and Wang et al. (2017) derived a damped L-BFGS method.
Over the past five years we have also seen quite a lot of fruitful activity in combining the stochastic quasi-Newton algorithms with various first-order noise reduction methods (Moritz et al., 2016; Gower et al., 2016). A thorough and forward-looking overview of SG and its use within a modern machine learning context is provided by Bottou et al. (2018). It also includes interesting accounts of possible improvements along the lines of first-order noise reduction and second-order methods.
It is interesting--and perhaps somewhat surprising--to note that it is only very recently that stochastic line search algorithms have started to become available. One nice example is the approach proposed by Mahsereci & Hennig (2017) which uses the framework of Gaussian processes and Bayesian optimisation. The step length is chosen that best satisfies a probabilistic measure combining reduction in the cost function with satisfaction of the Wolfe conditions. Conceptually more similar to our procedure is the line search proposed by Bollapragada et al. (2018), which is tailored for problems that are using sampled mini-batches, as is common practice within deep learning.

3 STOCHASTIC LINE SEARCH

In deterministic line search algorithms we first compute a search direction pk and then decide how far to move along that direction according to

xk+1 = xk + kpk,

(2)

where k > 0 is referred to as the step length. The search direction is of the form pk = -Hkgk,

(3)

where Hk denotes an approximation of the inverse Hessian matrix. The question of how far to move in this direction can be framed as the following scalar minimisation problem

min


f

(xk

+

pk ),

 > 0.

(4)

2

Under review as a conference paper at ICLR 2019

The most common way of dealing with this problem is to settle for a possibly sup-optimal solution that guarantees at least a sufficient decrease. Such at solution can be obtained by selecting a step length k that satisfies the following inequality

f (xk + pk)  f (xk) + ckgkTpk,

(5)

where c  (0, 1). The above condition is known as the first Wolfe condition (Wolfe, 1969; 1971) or the Armijo condition (Armijo, 1966).

While deterministic line search algorithms have been well established for a long time, their stochastic counterparts are still to a large extent missing. Consider the case when the measurements of the function and its gradient are given by

fk = f (xk) + ek, gk = gk + vk,

(6)

where gk f (x)|x=xk , and ek  R and vk  Rd×1 denote noise on the function and gradient evaluations, respectively. Furthermore we assume that

E [ek] = b, Cov[ek] = f2, E [vk]= 0, Cov[vk] = g2I.

(7a)

The challenge is that since fk and gk are random variables it is not obvious how to select a step length k that--in some sense--guarantees a descent direction.
We explore the idea of requiring equation 5 to be fulfilled in expectation when the exact quantities are replaced with their stochastic counterparts,

E f (xk + pk) - f (xk) - ckgkTpk  0,

(8)

where pk = -Hkgk. This is certainly one way in which we can reason about the Wolfe condition in the stochastic setting we are interested in. Although satisfaction of equation 8 does not leave any guarantees when considering a single measurement, it still serves as an important property that could be exploited to provide robustness for the entire optimisation procedure. To motivate our proposed algorithm we hence start by establishing the following results.

lemma 1 (Stochastic Wolfe condition 1) Assume that i) the gradient estimates are unbiased , ii)
the cost function estimates are possibly biased , and iii) a descent direction is ensured in expectation E pkTgk < 0. Then (for small :s)

E f (xk + pk)  E f (xk) + ckgkTpk ,

where

0

<

c

<

c¯ =

pTk gk

pTk gk

.

- g2 Tr (H)

(9)

Proof 1 See Appendix A.1.

lemma 2 There exists a step length k > 0 such that E f (xk + kpk) - f (xk) < 0.

Proof 2 See Appendix A.2
Relying upon these results we propose a line search based on the idea of repeatedly decreasing the step length k until the stochastic version of the first Wolfe condition is satisfied. Pseudo-code for this procedure is given in Algorithm 1. An input to this algorithm is the search direction pk, which can be computed using any preferred method. The step length is initially set to be the minimum of the "natural" step length 1 and the iteration dependent value /k. In this way the initial step length is kept at 1 until k > , a point after which it is decreased at the rate 1/k. Then we check whether the new point xk + pk satisfies the stochastic version of the first Wolfe condition. If this is not the case, we decrease k with the scale factor . This is repeated until the condition is met, unless we hit an upper bound max{0,  - k} on the number of backtracking iterations, where  > k is a positive integer. With this restriction the decrease of the step length is limited, and when k   we use the initial step length no matter if the Wolfe condition is satisfied or not. The purpose of  is to facilitate convergence by reducing the step length as the minima is approached. The motivation of  comes from arguing that the backtracking loop does not contribute as much when the step length is small, and hence it is reasonable to provide a limit on its reduction.

3

Under review as a conference paper at ICLR 2019

Algorithm 1 Stochastic backtracking line search
Require: Iteration index k, spatial point xk, search direction pk, scale factor   (0, 1), reduction limit   1, backtracking limit  > 0.
1: Set the initial step length k = min{1, /k} 2: Set i = 1 3: while f (xk + pk) > f (xk) + ckgkTpk and i  max{0,  - k} do 4: Reduce the step length k  k 5: Set i  i + 1 6: end while 7: Update xk+1 = xk + kpk

4 COMPUTING THE SEARCH DIRECTION

In this section we address the problem of computing a search direction based on having a limited memory available for storing previous gradients and associated iterates. The approach we adopt is similar to limited memory quasi-Newton methods, but here we employ a direct least-squares estimate of the inverse Hessian matrix rather than more well-known methods such as damped LBFGS and L-SR1. In contrast to traditional quasi-Newton methods, this approach does not strictly impose neither symmetry or fulfilment of the secant condition. It may appear peculiar to relax these requirements. However, in this setting is not obvious that enforced symmetry necessarily produces a better search direction. Furthermore, the secant condition relies upon the rather strong approximation that the Hessian matrix is constant between two subsequent iterations. Treating the condition less strictly might be helpful when that approximation is poor, perhaps especially in a stochastic environment. We construct a limited-memory inverse Hessian approximation in Section 4.1 and show how to update this representation in Section 4.2. In Section 4.3 we discuss a particular design choice associated with the update and Section 4.4 provides a means to ensure that a descent direction is calculated. The complete algorithm combining the procedure presented in this section with the line search routine in Algorithm 1 is given in Appendix C.

4.1 QUASI-NEWTON INVERSE HESSIAN APPROXIMATIONS

According to the secant condition (see e.g. Fletcher (1987)), the inverse Hessian matrix Hk should satisfy

Hkyk = sk,

(10)

where yk = gk - gk-1 and sk = xk - xk-1. Since there are generally more unknown values in Hk than can be determined from yk and sk alone, quasi-Newton methods update Hk from a previous estimate Hk-1 by solving regularised problems of the type

Hk = arg min
H

H - Hk-1

2 F,W

s.t. H = HT,

Hyk = sk,

(11)

where

X

2 F,W

=

XW

2 F

= trace(W TXTXW ) and the choice of weighting matrix W results

in different algorithms (see Hennig (2015) for an interesting perspective on this). Examples of the

most common quasi-Newton methods are given in Appendix B.

We employ a similar approach and determine Hk as the solution to the following regularised leastsquares problem

Hk = arg min
H

HYk - Sk

2 F

+



H - H¯k

2 F

,

(12)

where Yk and Sk hold a limited number of past yk's and sk's according to

Yk [yk-m+1, . . . , yk] , Sk [sk-m+1, . . . , sk] .

(13)

Here, m << d is the memory limit and yk = gk - gk-1 is an estimate of yk. The regulator matrix H¯k acts as a prior on H and can be modified at each iteration k. The parameter  > 0 is used to control the relative cost of the two terms in equation 12. It can be verified that the solution to the
above least-squares problem (12) is given by

Hk = H¯k + SkYkT I + YkYkT -1 ,

(14)

4

Under review as a conference paper at ICLR 2019

where I denotes the identity matrix. The above inverse Hessian estimate can be used to generate a search direction in the standard manner by scaling the negative gradient, that is

pk = -Hkgk.

(15)

However, for large-scale problems this is not practical since it involves the inverse of a large matrix. To ameliorate this difficulty, we adopt the standard approach by storing only a minimal (limited memory) representation of the inverse Hessian estimate Hk. To describe this, note that the dimensions of the matrices involved are

Hk  Rd×d,

Yk  Rd×m,

Sk  Rd×m.

(16)

We can employ the Sherman­Morrison­Woodbury formula to arrive at the following equivalent expression for Hk

Hk = H¯k + -1SkYkT I - Yk I + YkTYk -1 YkT .

(17)

Importantly, the matrix inverse I + YkTYk -1 is now by construction a positive definite matrix of
size m × m. Therefore, we construct and maintain a Cholesky factor of I + YkTYk since this leads to efficient solutions. In particular, if we express this matrix via a Cholesky decomposition

RkTRk = I + YkTYk,

(18)

where Rk  Rm×m is an upper triangular matrix, then the search direction pk = -Hkgk can be computed via

pk = -H¯kzk - -1Sk(YkTzk), zk = gk - Ykwk, wk = Rk-1 Rk-T YkTgk .

(19)

Note that the above expressions for pk involve first computing wk, which itself involves a computationally efficient forward-backward substitution (recalling that Rk is an m × m upper triangular matrix and the memory length m is typically 10­50). Furthermore, H¯k is typically diagonal so that H¯kzk is also efficient to compute. The remaining operations involve four matrix-vector products
and two vector additions. Therefore, for problems where d >> m then the matrix-vector products

will dominate the computational cost.

Constructing Rk can be achieved in several ways. The so-called normal-equation method constructs

the (upper triangular) part of I + YkTYk and then employs a Cholesky routine, which produces Rk

in or

O(n

m(m+1) 2

+ m3 /3)

operations.

Alternatively, 

we

Householder reflections to the matrix Mk = I

can compute Rk by YkT T . This costs

applying Givens O(2m2((n + m)

rotations - m/3))

operations, and is therefore more expensive, but typically offers better numerical accuracy (Golub

& Van Loan, 2012).

4.2 FAST AND ROBUST INCLUSION OF NEW MEASUREMENTS

In order to maximise the speed, we have developed a method for updating a Cholesky factor given the new measurement pair (sk+1, yk+1). Suppose we start with a Cholesky factor Rk at iteration k such that

RkTRk = I + YkTYk.

(20)

Assume, without loss of generality, that Yk and Sk are ordered in the following manner

Yk [Y1, yk-m+1, Y2] ,

Sk [S1, sk-m+1, S2] ,

(21)

where Y1, Y2, S1 and S2 are defined as

Y1 [yk-m+ +1, . . . , yk] , Y2 [yk-m+2, . . . , yk-m+ ] ,

(22a)

S1 [sk-m+ +1, . . . , sk] , S2 [sk-m+2, . . . , sk-m+ ] ,

(22b)

and is an appropriate integer so that Yk and Sk have m columns. The above ordering arises from "wrapping-around" the index when storing the measurements. We create the new Yk+1 and Sk+1 by replacing the oldest column entries, yk-m+1 and sk-m+1, with the latest measurements yk+1 and sk+1, respectively, so that

Yk+1 [Y1, yk+1, Y2] ,

Sk+1 [S1, sk+1, S2] .

(23)

5

Under review as a conference paper at ICLR 2019

The aim is to generate a new Cholesky factor Rk+1 such that

RkT+1Rk+1 = I + YkT+1Yk+1.

(24)

To this end, let the upper triangular matrix Rk be written conformally with the columns of Yk as

R1 r1 R2
Rk = · r2 r3 , · · R4

(25)

so that R1 and R2 have the same number of columns as Y1 and Y2, respectively. Furthermore, r1 is a column vector, r2 is a scalar and r3 is a row vector. Therefore,

RT1 R1 RkTRk =  ·
·

RT1 r1 r22 + r1Tr1
·

R1TR2



r1TR2 + r2r3



RT4 R4 + RT2 R2 + r3Tr3

I + Y1TY1 = ·
·

Y1Tyk-m+1  + ykT-m+1yk-m+1
·

Y1TY2  ykT-m+1Y2  . I + Y2TY2

(26)

By observing a common structure for the update I + YkT+1Yk+1 it is possible to write

I + Y1TY1

I + YkT+1Yk+1 = 

·

·

Y1Tyk+1  + ykT+1yk-m+1
·

Y1TY2  ykT+1Y2  I + Y2TY2

R1TR1 = ·
·

RT1 r4 r52 + r4Tr4
·

R1TR2



r4TR2 + r5r6

,

RT6 R6 + R2TR2 + r6Tr6

(27)

where r4, r5 and r6 are determined by r4 = R-1 T(Y1Tyk+1), r5 =  + ykT+1yk+1 - r4Tr4 1/2 ,
The final term R6 can be obtained by noticing that

1 r6 = r5

ykT+1Y2 - r4TR2 .

(28)

R6TR6 + RT2 R2 + r6Tr6 = R4TR4 + RT2 R2 + r3Tr3,

(29)

which implies

R6TR6 = R4TR4 - r6Tr6 + r3Tr3.

(30)

Therefore R6 can be obtained in a computationally very efficient manner by down-dating and updating the Cholesky factor R4 with the rank-1 matrices r6Tr6 and r3Tr3, respectively (see e.g. Section 12.5.3 in Golub & Van Loan (2012)).

4.3 SELECTING H¯k

There is no magic way of selecting the prior matrix H¯k. However, in practise it has proved very useful to employ a simple strategy of H¯k kI, where the positive scalar k > 0 is adaptively
chosen in each iteration. As a crude measure of progress we adopt the following rule

k =

k-1, k-1/,
k-1,

if k-1 = 1, if k-1 < 1/q, otherwise,

(31)

where   1 is a scale parameter, and q corresponds to the number of backtracking loops in the
line search; the values  = 1.3 and q = 3 were found to work well in practise. The intuition
behind equation 31 is that if no modification of the step length k is made, we can allow for a more "aggressive" regularisation. Note that a low k is favouring small elements in Hk. Since pk = -Hgk, this limits the magnitude of pk and the change xk+1 - xk is kept down. Hence, it is good practice to set the initial scaling 0 relatively small and then let it scale up as the optimisation progresses. Furthermore, we should point out that a diagonal H¯k comes with an efficiency benefit, since the product H¯kzk in equation 19 then is obtained as the element-wise product between two vectors.

6

Under review as a conference paper at ICLR 2019

4.4 ENSURING A DESCENT DIRECTION

In deterministic quasi-Newton methods, the search direction pk must be chosen to ensure a descent direction such that pkTgk < 0, since this guarantees reduction in the cost function for sufficiently small step lengths k. Since pk = -Hgk, we have that pkTgk = -gkTHkgk which is always negative if the approximation Hk of the inverse Hessian is positive definite. Otherwise, we can modify the search direction by subtracting a multiple of the gradient pk  pk - kgk. This is motivated by
noticing that

(pk - gk)Tgk = pkTgk - kgkTgk,

(32)

which always can be made negative by selecting k large enough, i.e. if

k

>

pkT gkT

gk gk

.

(33)

In the stochastic setting, the condition above does not strictly enforce a descent direction. Hence the search direction pk as determined by equation 15 is not a descent direction in general. However, ensuring that the condition is fulfilled in expectation is necessary since this is one of the assumptions made in lemma 1. Hence, we now establish the following result.

lemma 3 If then

k

>

pkTgk - g2 gkTgk +

Tr (H dg2

)

,

E (pk - kgk)Tgk < 0.

(34) (35)

Proof 3 See Appendix A.3.

In the practical setting we can not use equation 34 as it is, since we do not have access to gk and pk, nor the noise variance g2. Instead we suggest a pragmatic approach in which these quantities
are replaced by their estimates gk, pk and g2. The noise estimate g2 could either be regarded a design parameter or empirically calculated from one or more sets of repeatedly collected gradient measurements. Nevertheless, picking k sufficiently large ensures fulfilment of equation 32 and equation 35 simultaneously.

5 NUMERICAL EXPERIMENTS
Let us now put our new developments to the test on a suite of problems from different categories to exhibit different properties and challenges. In Section 5.1 we study a commonly used benchmark, namely the collection of logistic classification problems described by Chang & Lin (2011) in the form of their library for support vector machines (LIBSVM). In Section 5.2 we consider an optimisation problem arising from the use of deep learning to solve the classical machine learning benchmark MNIST1, where the task is to classify images of handwritten digits. Also, we test our method on training a neural network on the CIFAR-10 dataset (Krizhevsky, 2009).
In our experiments we compare against relevant state-of-the-art methods. All experiments were run on a MacBook Pro 2.8GHz laptop with 16GB of RAM using Matlab 2018b. All routines where programmed in C and compiled via Matlab's mex command and linked against Matlab's Level-1,2 BLAS libraries. More details about the experiments are available in Appendix D. The source code used to produce the results will be made freely available.
5.1 LOGISTIC LOSS AND A 2-NORM REGULARISER
The task here is to solve eight different empirical risk minimisation problems using a logistic loss function with an L2 regulariser (two are shown here and all eight are profiled in Appendix D).
1yann.lecun.com/exdb/mnist/

7

Under review as a conference paper at ICLR 2019

(a) covtype

(b) spam

(c) CIFAR

(d) MNIST

Figure 1: Performance on two classification tasks using a logistic loss with a two-norm regulariser ((a) and (b)), and two deep convolutional neural networks (CNNs) used for recognising images of handwritten digits from the MNIST data (c), and classification of the images in the CIFAR-10 data (d). Lighter shaded lines indicate individual runs, whereas the darker shaded line indicates the average.

The data is taken from Chang & Lin (2011). These problems are commonly used for profiling optimisation algorithms of the kind introduced in this paper, facilitating comparison with existing state-of-the-art algorithms. More specifically, we have used a similar set-up as Gower et al. (2016), which inspired this study. The chosen algorithm parameters for each case is detailed in Appendix D.
We compared our limited memory least-squares approach (denoted as LMLS) against two existing methods from the literature, namely, the limited memory stochastic BFGS method after Bollapragada et al. (2018) (denoted as LBFGS) and the stochastic variance reduced gradient descent (SVRG) by Johnson & Zhang (2013) (denoted SVRG). Figures 1a and 1b show the cost versus time for 50 Monte-Carlo experiments.
5.2 DEEP LEARNING
Deep convolutional neural networks (CNNs) with multiple layers of convolution, pooling and nonlinear activation functions are delivering state-of-the-art results on many tasks in computer vision. We are here borrowing the stochastic optimisation problems arising in using such a deep CNN to solve the MNIST and CIFAR-10 benchmarks. The particular CNN structure used for the MNIST example employs 5 × 5 convolution kernels, pooling layers and a fully connected layer at the end. We made use of the publicly available code provided by Zhang (2016), which contains all the implementation details. For the CIFAR-10 example, the network includes 13-layers with more than 150,000 weights, see Appendix D for details. The MATLAB toolbox MatConvNet (Vedaldi & Lenc, 2015) was used in the implementation. Figures 1c and 1d show the average cost versus time for 10 Monte-Carlo trials with four different algorithms: 1. the method developed here (LMLS), 2. a stochastic limited memory BFGS method after Bollapragada et al. (2018) (denoted LBFGS), 3. Adam developed by Kingma & Ba (2015), and 4. stochastic gradient (denoted SG). Note that all algorithms make use of the same gradient code.
6 CONCLUSION AND FUTURE WORK
In this work we have presented a least-squares based limited memory optimisation routine that benefits from second order information by approximating the inverse Hessian matrix. The procedure is conceptually similar to quasi-Newton methods, although we do not explicitly enforce symmetry or satisfaction of the secant condition. By regularising with respect to an inverse Hessian prior, we allow for an adaptive aggressiveness in the search direction update. We have shown that the computations can be made robust and efficient using tailored Cholesky decompositions, with a cost that scales linearly in the problem dimension. Our method is designed for stochastic problems through a line search that repeatedly decreases the step length so as to satisfy the first Wolfe condition in expectation. Theoretical results have been established that support the proposed procedure. The method shows improved convergence properties over existing algorithms when applied to benchmark problems of various size and complexity.
8

Under review as a conference paper at ICLR 2019
REFERENCES
L. Armijo. Minimization of functions having Lipschitz continuous first partial derivatives. Pacific Journal of Mathematics, 16(1):1­3, 1966.
R. Bollapragada, D. Mudigere, J. Nocedal, H.-J. M. Shi, and P. T. P. Tang. A progressive batching L-BFGS method for machine learning. In Proceedings of the 35th International Conference on Machine Learning (ICML), Stockholm, Sweden, 2018.
A. Bordes, L. Bottou, and P. Gallinari. SGD-QN: Careful quasi-Newton stochastic gradient descent. Journal of Machine Learning Research (JMLR), 10:1737­1754, 2009.
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223­311, 2018.
C. G. Broyden. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21:368­381, 1967.
C. G. Broyden. The convergence of a class of double-rank minimization algorithms. Journal of the Institute of Mathematics and Its Applications, 6(1):76­90, 1970.
R. H. Byrd, S. L. Hansen, J. Nocedal, and Y. Singer. A stochastic quasi-Newton method for largescale optimization. SIAM Journal on Optimization, 26(2):1008­1031, 2016.
C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1­27:27, 2011.
A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: a fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems (NIPS), Montréal, Canada, 2014.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research (JMLR), 12:2121­2159, 2011.
R. Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3):317­322, 1970.
R. Fletcher. Practical methods of optimization. John Wiley & Sons, Chichester, UK, second edition, 1987.
D. Goldfarb. A family of variable metric updates derived by variational means. Mathematics of Computation, 24(109):23­26, 1970.
G. H. Golub and C. F. Van Loan. Matrix Computations. John Hopkins University Press, Baltimore, fourth edition, 2012.
R. M. Gower, D. Goldfarb, and P. Richtarik. Stochastic block BFGS: squeezing more curvature out of data. In Proceedings of the 33rd International Conference on Machine Learning (ICML), New York, NY, USA, 2016.
P. Hennig. Probabilistic interpretation of linear solvers. SIAM Journal on Optimization, 25(1): 234­260, 2015.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems (NIPS), Lake Tahoe, NV, USA, 2013.
D. P. Kingma and J. Ba. Adam: a method for stochastic optimization. In Proceedings of the 3rd international conference on learning representations (ICLR), San Diego, CA, USA, 2015.
J. Konecný and P. Richtárik. Semi-stochastic gradient descent methods. Frontiers in Applied Mathematics and Statistics, 3(9), 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
9

Under review as a conference paper at ICLR 2019
D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(3):503­528, 1989.
M. Mahsereci and P. Hennig. Probabilistic line searches for stochastic optimization. Journal of Machine Learning Research (JMLR), 18(119):1­59, 2017.
A. Mokhtari and A. Ribeiro. RES: regularized stochastic BFGS algorithm. IEEE Transactions on Signal Processing, 62(23):6089­6104, 2014.
A. Mokhtari and A. Ribeiro. Global convergence of online limited memory BFGS. Journal of Machine Learning Research (JMLR), 16:3151­3181, 2015.
P. Moritz, R. Nishihara, and M. I. Jordan. A linearly-convergent stochastic L-BFGS algorithm. In The 19th International Conference on Artificial Intelligence and Statistics (AISTATS), Cadiz, Spain, 2016.
J. Nocedal. Updating quasi-Newton matrices with limited storage. Mathematics of Computation, 35 (151):773­782, 1980.
J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research. Springer, New York, USA, second edition, 2006.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22(3):400­407, 1951.
M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. Technical Report arXiv:1309.2388, arXiv preprint, 2013.
N. N. Schraudolph, J. Yu, and S. Günter. A stochastic quasi-Newton method for online convex optimization. In Proceedings of the 11th international conference on Artificial Intelligence and Statistics (AISTATS), 2007.
D. F. Shanno. Conditioning of quasi-Newton methods for function minimization. Mathematics of Computation, 24(111):647­656, 1970.
A. Vedaldi and K. Lenc. Matconvnet ­ convolutional neural networks for matlab. In Proceeding of the ACM Int. Conf. on Multimedia, 2015.
X. Wang, S. Ma, D. Goldfarb, and W. Liu. Stochastic quasi-Newton methods for nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927­956, 2017.
P. Wolfe. Convergence conditions for ascent methods. SIAM Review, 11(2):226­235, 1969. P. Wolfe. Convergence conditions for ascent methods II: some corrections. SIAM Review, 13(2):
185­188, 1971. Z. Zhang. Derivation of backpropagation in convolutional neural networks (CNN). github.com/
ZZUTK/An-Example-of-CNN-on-MNIST-dataset, 2016.
10

Under review as a conference paper at ICLR 2019

A PROOFS OF THE LEMMAS

A.1 THE FIRST STOCHASTIC WOLFE CONDITION

In the interest of a simple notation we note that we can drop the sub-index k from all variables since all reasoning in this proof is for iteration k. We also drop x from the arguments and write f rather than f (x), etc. However, we do make the randomness explicit, since this is crucial for arriving at the
correct answer. For example we write fz to indicate that the random variable z was used in computing the estimate of the function value at the iterate xk. This means that the noise contaminating the measurement corresponds to the realisation of z.

Consider the stochastic version of the Wolfe condition

Ez,z fz (x + pz) - fz(x) - cpTz gz  0,

(36)

where the expected value is w.r.t. the randomness used in computing the required estimators as
indicated by z and z . Using Taylor series we can for small step lengths  express fz (x + pz) according to

fz (x + pz)  fz + pzTgz .

(37)

Here it is crucial to note that the randomness in the above estimate stem from two different sources. The search direction pz enters in the argument of the Taylor expansion which is why it has to be computed before actually performing the Taylor expansion, implying that the randomness is due to z and not z for the search direction. Based on equation 36 we have the following expression for the stochastic Wolfe condition

Ez,z fz - fz - pTz gz + cpTz gz  0.

(38)

Let b denote a possible bias in the cost function estimator, then we have that Ez fz = f + b and Ez fz = f + b. Hence, the first two terms in equation 38 cancel and we have

c  Ez,z Ez,z

pTz gz [pTz gz]

=

Ez

pzT Ez [gz Ez [pTz gz]

]

=

pTg Ez [pTz gz] .

The denominator can be written as

(39)

Ez pzTgz = Ez -gzTHgz = - Ez Tr HgzgzT = - Tr H Ez gzgzT .

(40)

Using the definition of covariance we have that

Ez gzgzT = Ez [gz] Ez gzT + Covg = ggT + g2I,

(41)

where we in the last equality made use of the assumption that the gradients are unbiased and that Covg = gI. Summarising we have that

Ez pTz gz = - Tr HggT - g2 Tr (H) = pTg - g2 Tr (H) ,

(42)

and hence we get

c



pTg

pTg - g2 Tr (H) .

(43)

Recalling the assurance given in lemma 3 with the associated proof in Appendix A.3, we can always modify p to guarantee that this bound is positive.

A.2 REDUCTION IN COST FUNCTION In this section we show that there exists an  > 0 such that
Ez fz (x + p) < Ez fz(x) .

(44)

11

Under review as a conference paper at ICLR 2019

We do this by studying the difference

Ez fz (x + p) - Ez fz(x) = Ez,z fz (x + p) - fz(x) ,

(45)

in which the first term can be expressed using a Taylor series according to

fz (x + pz) = fz + pTz gz + O( pz 2).

(46)

For small values of  we can discard the term O( pz 2) from this expression. Hence we have that

Ez,z fz + pTz gz - fz =  Ez,z pzTgz =  Ez pTz Ez [gz ] = pTg.

(47)

Following from Section 4 we can ensure pTg to be negative, and hence equation 44 holds as long as  is chosen small enough for the Taylor expansion to be a valid approximation.

A.3 ENSURING A DESCENT DIRECTION

Note that

Ez (pz - gz)Tgz = Ez pzTgz -  Ez gzTgz = Ez -gzTHgz -  Ez gzTgz .

Using equation 42 we have

Ez -gzTHgz = - Tr HggT - g2 Tr (H) = pTg - g2 Tr (H) ,

and it directly follows from this that

Ez gzTgz = Ez -gzTIgz = gTg + dg2.

Hence

Ez (pz - gz)Tgz = pTg - g2 Tr (H) -  gTg + dg2 ,

and we see that

Ez

(pz - gz)Tgz

<

0





>

pT

g - g2 gTg +

Tr (H dg2

)

.

(48) (49) (50) (51) (52)

B QUASI-NEWTON METHODS

The historically most popular quasi-Newton method is given by the BFGS algorithm (Broyden, 1970; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970)

Hk+1 =

I

-

sk ykT ykTsk

Hk

I

-

yk skT ykTsk

+

sk skT ykTsk

.

(53)

A closely related version is obtained if the optimisation in equation 11 is done with respect to the Hessian matrix rather than to its inverse. This results in the so-called DFP formula

Hk+1

=

Hk

-

Hk yk ykT Hk ykT Hk yk

+

sk skT ykTsk

.

(54)

Both of these are rank 2 updates that ensure Hk to be positive definite, an attractive feature in the sense that it guarantees a descent direction. However, it may cause problems in regions where the true inverse Hessian is indefinite. Another well-known alternative is the symmetric rank 1 (SR1) method

Hk

=

Hk-1

+

(sk

-

Hk-1yk)(sk - Hk-1 (sk - Hk-1yk)Tyk

yk

)T

.

(55)

Apart from BFGS and DFP above, this is a rank 1 update that in general does not preserve positive definiteness. For this reason it has received a particular interest within the so-called trust-region framework, where its ability of producing indefinite inverse Hessian approximations is being regarded as a major strength (Nocedal & Wright, 2006).

12

Under review as a conference paper at ICLR 2019

A main drawback of quasi-Newton methods is that they scale poorly to large problems, since the number of elements in Hk equals the square of the input dimension d. This has resulted in developments of so-called limited memory algorithms, which rely upon the idea of forming the search direction pk directly without the need of storing the full matrix Hk. To that end a number m << d of past differences y and s are being stored in memory. A notable member of this family is the L-BFGS algorithm (Nocedal, 1980), which has been widely within large-scale optimisation. During recent years, not at least due to the growing number of deep learning applications, there has been an increasing interest in adapting deterministic limited memory methods to the stochastic setting (Wang et al., 2017; Moritz et al., 2016; Gower et al., 2016; Schraudolph et al., 2007; Bordes et al., 2009; Mokhtari & Ribeiro, 2015; Byrd et al., 2016; Bollapragada et al., 2018).

C RESULTING ALGORITHM
We summarise our ideas from Section 3 and 4 in Algorithm 2. The if-statement on line 5 is included to provide a safe-guard against numerical instability.

D EXPERIMENT DETAILS

Details for the datasets used in Section 5 are listed in Table 1, including the parameter choices we made in our algorithm. Here, b denotes the mini-batch size. The results of all logistic regression experiments are collected in Figure 2 (including those already shown in Figure 1), and the neural network results in Figures 1d and 1c are provided in a more readable size in Figure 3a and 3b. Detailed information of the network structure in the CIFAR experiment is provided in the printout shown in Figure 4.
The adaptive procedure of selecting the inverse Hessian prior H¯k did not have much impact in most of the logistic regression examples, and thus it was kept constant H¯k = 0I except for in the covtype and URL problems. In the CIFAR and MNIST problems, however, the procedure was found to be of significant importance.
Table 1: List of the datasets used in the experiments, where n denotes the size of the dataset (column 2) and d denotes the number of variables in the optimisation problem (column 3). The remaining columns list our design parameters, including the mini-batch size b.

Problem n

d

b m

  0

gisette 6 000

5 000

770 20 10-4 50 10 10

covtype 581 012

54

7 620 55 10-4

100 5 100

HIGGS

11 000 000 28

66 340 28 10-4

20 5 100

SUSY

3 548 466 18

5 000 18 10-4

50 10 100

epsilon 400 000

2 000

1 000 20 10-4

150 1 100

rcv1

20 242

47 236 710 10 10-4 50 10 600

URL

2 396 130 3 231 961 1548 5 10-4

200 50 100

spam

82 970

823 470

2048

2 10-4

150 10 200

MNIST

60 000

3898

1000

50 8 · 10-4 150 50 1

CIFAR

50 000

150 000 200

20 10-2

100 10 0.5

13

Under review as a conference paper at ICLR 2019

Algorithm 2 Stochastic quasi-Newton with line search
Require: An initial estimate x1, a maximum number of iterations kmax, memory limit m, regularisation parameter , scale factor   (0, 1), reduction limit   1, backtracking limit  > 0

1: Set k = 1 2: while k < kmax do 3: Obtain a measurement of the cost function and its gradient

4: Set H¯k = kI where

fk = f (xk) + ek, gk = gk + vk.

k =

k-1, k-1/,
k-1,

if k-1 = 1, if k-1 < 1/q, otherwise.

5:

if ykTsk >

sk

2 2

then

6: if k > m then

7: Form Yk and Sk by replacing the oldest vector-pairs in Yk-1 and Sk-1 with yk and sk 8: else

9: Form Yk and Sk by adding yk and sk to Yk-1 and Sk-1

10: end if

11: else

12: Set Yk = Yk-1 and Sk = Sk-1 13: end if

14: Select pk as

pk = -H¯kzk - -1Sk(YkTzk), zk = gk - Ykwk,
wk = Rk-1 Rk-T YkTgk ,

with details provided in Section 4.2. 15: Set pk  pk - kgk with k where

k

>

pkTgk - g2 Tr (H) . gkTgk + dg2

16: Set k = min{1, /k} 17: Set i = 1 18: while f (xk + pk) > f (xk) + ckgkTpk and i  max{0,  - k} do 19: Reduce the step length k  k 20: Set i  i + 1
21: end while
22: Update xk+1 = xk + kpk 23: Set k  k + 1
24: end while

14

Under review as a conference paper at ICLR 2019

(a) gisette

(b) covtype

(c) HIGGS

(d) SUSY

(e) epsilon

(f) RCV1

(g) URL

(h) spam

Figure 2: Performance on classification tasks using a logistic loss with a two-norm regulariser.

15

Under review as a conference paper at ICLR 2019
(a) MNIST
(b) CIFAR Figure 3: Solving the optimisation problem used in training a state-of-the-art deep convolutional neural network (CNN) used for recognising images in the (a) MNIST and (b) CIFAR-10 dataset. LMLS refers to limited memory least-squares approach developed in this paper, SG refers to basic stochastic gradient and Adam refers to Kingma & Ba (2015).
16

Under review as a conference paper at ICLR 2019
Figure 4: Detailed network strucure in the CIFAR-10 experiment. 17

