Under review as a conference paper at ICLR 2019
VARIATIONAL SPARSE CODING
Anonymous authors Paper under double-blind review
ABSTRACT
Variational auto-encoders (VAEs) offer a tractable approach when performing approximate inference in otherwise intractable generative models. However, standard VAEs often produce latent codes that are disperse and lack interpretability, thus making the resulting representations unsuitable for auxiliary tasks (e.g. classification) and human interpretation. We address these issues by merging ideas from variational auto-encoders and sparse coding, and propose to explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution. We derive the variational lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational efficient as in the standard VAE case. With the new approach, we are able to infer truly sparse representations with generally intractable non-linear probabilistic models. We show that these sparse representations are advantageous over standard VAE representations on two benchmark classification tasks (MNIST and Fashion-MNIST) by demonstrating improved classification accuracy and significantly increased robustness to the number of latent dimensions. Furthermore, we demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation.
1 INTRODUCTION
Variational auto-encoders (VAEs) offer an efficient way of performing approximate posterior inference with otherwise intractable generative models and yield probabilistic encoding functions that can map complicated high-dimensional data to lower dimensional representations (Kingma & Welling, 2013; Sønderby et al., 2016; Rasmus et al., 2015). Making such representations meaningful and efficient, however, is a particularly difficult task and currently a major challenge in representation learning (Hsu et al., 2017; Burgess et al., 2018; Kim & Mnih, 2018; Tomczak & Welling, 2017). Large latent spaces often give rise to many latent dimensions that do not carry any information, and obtaining codes that properly capture the complexity of the observed data is generally problematic (Tomczak & Welling, 2017; Higgins et al., 2016; Burgess et al., 2018).
In the case of linear mappings, sparse coding offers an elegant solution to the aforementioned problem; the representation space is induced to be sparse. In such a way, the encoding function is encouraged to use the minimum number of non-zero elements necessary to describe each observation and condense information in few active variables, different for each sample. In fact, due to their efficiency of representation, sparse codes have been used in many learning and recognition systems, as they provide easier interpretation (Lee et al., 2007; Bengio et al., 2013; Mairal et al., 2009; Arora et al., 2015) and increased efficiency in, for example, classification, clustering, and transmission tasks when used as learning inputs (Yang et al., 2011; Wright et al., 2009; Labusch et al., 2008; Yang et al., 2009).
In this work, we aim to extent the aforementioned capability of linear sparse coding to non-linear probabilistic generative models thus allowing efficient, informative and interpretable representations in the general case. To this end we formulate a new variation of the classical VAE in which we employ a sparsity inducing prior in the latent space based on the Spike and Slab distribution. We match this by a discrete mixture recognition function that can map observations to sparse latent vectors. Efficient inference, comparable in complexity to that of standard VAEs, is achieved by deriving a variational lower bound (VLB) for the new model which is optimized using standard gradient methods to recover the encoding and decoding functions. In our experiments, we consider two benchmark dataset (MNIST and Fashion-MNIST) and show how the resulting VSC is able to
1

Under review as a conference paper at ICLR 2019

recover sparse, informative and interpretable representations regardless of the predefined number of latent dimensions. The ability to adjust to data complexity allows to automatically discover the sources of variation in given observations, without the need to carefully adjust the architecture of a model to the given representation task. We demonstrate these properties by first performing classification experiments using latent vectors as inputs, where we demonstrate that VSC representations marginally outperform VAE ones and display greatly improved robustness over large variations in latent space dimensionality. Secondly we show that many sparse elements in retrieved codes control subjectively recognisable features in the generated observations.

2 BACKGROUND AND RELATED WORK

2.1 SPARSE CODING

Sparse coding aims to approximately represent input vectors xi with a weighted linear combination of few unknown basis vectors bj (Lee et al., 2007; Bengio et al., 2013; Li et al., 2004). The problem of determining the optimal basis and weights is generally formulated as the minimisation of an

objective function of the following form

arg min
B,Z

1 ||X - BZ||2 +  2

(zi),

i

(1)

where X  RM×N is the matrix of data, having as columns the input vectors xi  RM×1, B  RJ×M is the matrix having as columns the basis vectors bj  RM×1, Z  RJ×N is the sparse codes matrix, having as columns the sparse codes zi  RJ×1 corresponding to the inputs xi,  is a real
positive parameter and (zi) is a sparsity inducing function.

Sparse coding can be probabilistically interpreted as a generative model, where the observed vectors

xi are generated from the unobserved latent variables zi through the linear process xi = Bzi + , where is the observation noise and is drawn from an isotropic normal distribution with zero mean

(Lee et al., 2007; Bengio et al., 2013). The model can then be described with the following prior

and likelihood distributions

p(zi) = exp(-(zi)), p(xi|zi) = N (xi; Bzi, I2),

(2)

where  is a real positive parameter,  is the standard deviation of the observation noise and I is the

identity matrix. Performing maximum a posteriori (MAP) estimation with this model results in the minimisation shown in equation 1 with  = 2.

In contrast to the MAP formulation, we are interested in maximising the marginal likelihood p(x) =
p(xi) and being able to perform such optimisation for arbitrarily complicated likelihood functions p(x|z).

Previous work has demonstrated variational EM inference for such maximisation in the linear generative model case, with a particular choice of sparsity inducing prior (Titsias & La´zaro-Gredilla, 2011; Goodfellow et al., 2012). However, EM inference becomes intractable for more complicated non-linear posteriors and a large number of input vectors (Kingma & Welling, 2013), making such an approach unsuitable to scale to our desired model.

Conversely, some work has been done in generalising sparse coding to non-linear transformations, by defining sparsity on Riemannian manifolds (Ho et al., 2013; Cherian & Sra, 2017). These generalisations, however, perform MAP inference as they define a non-linear equivalent of the objective function in equation 1 and are limited to simple manifolds due to the need to compute the manifold's logarithmic map.

2.2 VARIATIONAL AUTO-ENCODERS
Variational auto-encoders (VAEs) are models for unsupervised efficient coding that aim to maximise the marginal likelihood p(x) = p(xi) with respect to some decoding parameters  of the likelihood function p(x|z) and encoding parameters  of a recognition model q(z|x) (Kingma & Welling (2013); Pu et al. (2016)).
The VAE model is as follows; an observed vector xi  RM×1 is assumed to be drawn from a likelihood function p(x|z). The likelihood function is commonly chosen to be either a Gaussian or

2

Under review as a conference paper at ICLR 2019

Variational Auto-Encoder q(z|x)

p(z)
Gaussian

zi p(x|z)

xi

Variational Sparse Coding q(z|x)

ps(z)
Spike and Slab

zi p(x|z)
Sparse Code

xi

Figure 1: Schematic representation of the variational sparse coding model (right) compared to a standard VAE (left). In both cases an observed variable xi is assumed to be generated from an unobserved variable zi. Variational sparse coding, however, models sparsity in the latent space with the Spike and Slab prior distribution. One example is shown for each prior with a sample from the
MNIST dataset.

a Bernoulli distribution, depending on the nature of the noise in the observations. The parameters of p(x|z) are the output of a neural network having as input a latent variable zi  RJ×1. The latent variable is assumed to be drawn from a prior p(z) that takes the form of a multivariate Gaussian with identity covariance N (z; 0, I).

The aim is then to maximise a joint posterior distribution of the form p(x) = p(xi|z)p(z)dz, which for an arbitrarily complicated conditional p(x|z) is intractable. To address this intractability,
VAEs introduce a recognition model q(z|x) and define a variational lower bound (VLB) to be
estimated in place of the true posterior.

The recognition function is a Normal distribution with diagonal covariance q(z|x) =
N (zi; µz,i, z2,i), where the moments of the distribution µz,i and z,i are the output of a neural network having as input a data point xi. The VLB can, due to Jensen's inequality, be formulated as

log p(xi) = log

p (xi |z )p(z )

q(z|xi) q(z|xi)

dz



L(,

;

xi),

(3)

L(, ; xi) = -DKL(q(z|xi)||p(z)) + Eq(z|xi) [log p(xi|z)] .

The VLB is composed of two terms; a prior term, which encourages minimisation of the KL divergence between the encoding distributions and the prior, and a reconstruction term, which maximises the expectation of the data likelihood under the recognition function. The VLB is then maximised with respect to the model's parameters  and . The prior term can be defined analytically, while the reconstruction term is optimised stochastically through a reparameterization trick (Kingma & Welling, 2013). Figure 1 (left) schematically depicts the model with an example of data and corresponding latent variable.

3 VARIATIONAL SPARSE CODING
We propose to use the framework of VAEs to perform approximate variational inference with neural network sparse coding architectures. With this approach, we aim to discover and discern the nonlinear features that constitute variability in data and represent them as few non-zero elements in sparse vectors.
We model sparsity in the latent space with a Spike and Slab probability density prior. The Spike and Slab PDF is a discrete mixture model which assigns point mass to null elements and therefore probabilistically models sparsity (Goodfellow et al., 2012; Titsias & La´zaro-Gredilla, 2011; Mitchell & Beauchamp, 1988). Because of this characteristic, this distribution has been used in various Bayesian sparse inference models (Seeger, 2008; Mohamed et al., 2011; Shelton et al., 2015; Herna´ndez-Lobato et al., 2013).
The Spike and Slab distribution is defined over two variables; a binary spike variable sj and a continuous slab variable zj (Mitchell & Beauchamp, 1988). The spike variable is either one or zero with defined probabilities  and (1 - ) respectively and the slab variable has a distribution which is either a Gaussian or a Delta function centered at zero, conditioned on whether the spike variable

3

Under review as a conference paper at ICLR 2019

is one or zero respectively. The prior probability density over the latent variable z we are interested

in is then

J

ps(z) = (N (zj; 0, 1) + (1 - )(zj)) ,

(4)

j=1

where (·) indicates the Dirac delta function centered at zero. This choice of prior leads to the
assumption that observed data is generated from sparse vectors in the latent space. The recognition function q(z|x) is chosen to be a discrete mixture model of the form

J

q(z|xi) =

i,j N (zi,j ; µz,i,j , z2,i,j ) + (1 - i,j )(zi,j ) ,

j=1

(5)

where the distribution parameters µz,i,j, z2,i,j and i,j are the outputs of a neural network having parameters  and input xi. A description of the recognition function neural network can be found in appendix A.2. Similarly to the standard Spike and Slab distribution of equation 4, the distribution
of equation 5 can be described with Spike variables, having probabilities of being one i,j, and Slab variables having Gaussian distributions N (zi,j; µz,i,j, z2,i,j). On one side, this choice of recognition function allows for the posterior to match the prior, while on the other, the freedom to control
the Gaussian moments and the Spike probabilities independently enables the model to encode infor-
mation in the latent space. Figure 1 (right) schematically depicts the model with an example of an
observation and corresponding latent sparse vector. A more detailed description of the model can be
found in appendix A.

As in the standard VAE setting, we aim to perform approximate variational inference by maximising
a lower bound. The VLB we aim to maximise during training is of the form detailed in equation 3,
with the Spike and Slab probability density function ps(z) of equation 4 as prior and the discrete mixture distribution of of equation 5 as recognition function q(z|xi). In the following subsections we derive the prior and reconstruction terms of the VSC lower bound under these conditions.

3.1 VLB PRIOR TERM

By substituting the prior of equation 4 and the recognition function of equation 5 into the negative KL divergence term in equation 3, we derive the VLB prior term of the VSC model

-DKL(q(z|xi)||ps(z)) = q(z|xi)(log ps(z) - log q(z|xi))dz

J
=
j

- i,j 2

1 + log(z2,i,j ) - µ2z,i,j - z2,i,j

(6)

+ (1 - i,j) log

1- 1 - i,j

+ i,j log

 i,j

.

A detailed derivation is provided in appendix B. This prior term naturally presents two components. The first is the negative KL divergence between the distributions of the Slab variables, multiplied by the probability of zi,j being non-zero i,j. This component gives a similar regularisation to that of the standard VAE and encourages the Gaussian components of the recognition function to match those of the prior, proportionally to the Spike probabilities i,j. The second term is the negative KL divergence between the distributions of the Spike variables. This term encourages the probabilities of the latent variables being non-zero i,j to match the prior Spike probability .

3.2 VLB RECONSTRUCTION TERM

Similarly to the standard VAE, the reconstruction term of the lower bound is estimated and maximised stochastically as follows

Eq(z|xi) [log p(xi|z)]

1 L

L

log p(xi|zi,l),

l

(7)

where the samples zi,l are drawn from the recognition function q(z|xi). As in the standard VAE, to make the reconstruction term differentiable with respect to the encoding parameters , we employ

4

Under review as a conference paper at ICLR 2019

a reparameterization trick to draw from q(z|xi). We make use of two auxiliary noise variables and , normally and uniformly distributed respectively. is used to draw from the Slab distributions,
resulting in a reparametrisation analogous to the standard VAE (Kingma & Welling, 2013).  is used
to parametrise draws of the Spike variables through a non-linear binary selection function T (yi,l). The draws zi,l are then computed as follows

zi,l = T (l - 1 + i) (µz,i + z,i l),

(8)

where indicates an element wise product. The function T (yi,l) is in principle a step function centered at zero, however, in order to maintain differentiability, we employ a scaled Sigmoid function T (y) = S(cy). A more detailed description of the reparametrisation of the Spike variable is reported in appendix C. In the limit c  , S(cy) tends to the true binary mapping. In practice, the value of c needs to be small enough to provide stability of the gradient ascent. In our implementation we employ a warm-up strategy to gradually increase the value of c during training.

3.3 THE VSC LOWER BOUND

Combining the prior and reconstruction terms from section 3.1 and 3.2, we obtain the estimation of the VSC lower bound

L(, ; xi)

J

i,j 2

1 + log(z2,i,j ) - µz2,i,j - z2,i,j

j

+ (1 - i,j) log

1- 1 - i,j

+ i,j log

 i,j

1 +
L

L
log p(xi|zi,l).
l

(9)

The final VLB is relatively simple and of easy interpretation; the prior term is composed of the negative Spike and weighted Slab KL divergences, while the reconstruction term is the expectation of the likelihood under the recognition function PDF, estimated stochastically. We also point out that for i,j =  = 1 we recover the lower bound of the standard VAE (Kingma & Welling, 2013) as expected from the definition of the model. To train the VSC model, we maximise the VLB in the form of equation 9 with respect to the encoding and decoding parameters  and  through gradient ascent.

4 EXPERIMENTS
We test the VSC model on two image datasets commonly used to benchmark learning performance; the hand written digits dataset MNIST (LeCun et al., 1998) and the more recent fashion items dataset fashion-MNIST (Xiao et al., 2017), both composed of 28×28 grey scale images of handwritten digits and pieces of clothing respectively. We also make use of the CelebA faces dataset (Liu et al., 2015) to illustrate more qualitative results. Details of these datasets are given in appendix D.2. Various examples of latent sparse codes and corresponding reconstructions are shown in appendix E.1, while measurements of the latent space sparsity are presented in appendix E.2.
In the following subsections we test the VSC model in different settings. First, we evaluate the VLB at varying prior sparsity and number of latent space dimensions. Secondly, to evaluate quantitatively representation efficiency in the latent space, we test classification using latent variables as inputs. Lastly, we qualitatively assess latent space interpretation by examining the effect of altering individual non-zero elements in the sparse codes. Details of the experimental conditions can be found in appendix D.
4.1 VLB EVALUATION
We evaluate the VLB at varying numbers of latent dimensions and different levels of prior sparsity . We first train a standard VAE at a varying number of latent dimensions imposing a limit of 20, 000 iterations. For each dimensionality, we find the best performing initial step size for the Adam optimiser (Kingma & Ba, 2014). We then use identical settings in each condition to test VSCs lower bound with different prior sparsity. Our evaluation performed on the test sets is shown in figure 2. Results for different iteration limits are included in appendix E.3.

5

Under review as a conference paper at ICLR 2019
VAE VSC  = 0.5 VSC  = 0.2

VAE VSC  = 0.5 VSC  = 0.2

Figure 2: Test set VLB for the VSC at varying number of latent dimensions. The standard VAE reaches high VLB for a correct choice of latent space dimensions, but drops rapidly for larger latent spaces. With increasing sparsity in the latent space, the VSC drops in performance at the optimal VAE dimensionality, but remains more stable with larger latent spaces.
The standard VAE achieves high VLB values provided that the size of its latent space is chosen correctly, but for spaces which are too large its performance rapidly drops, as encoding in many latent variables becomes increasingly difficult. Conversely, the VSC reaches a lower maximum VLB, but remain significantly more stable with more latent dimensions. With few latent dimensions available, the sparsity imposed by the prior, controlled by the parameter , is too restrictive to allow rich descriptions of the observations and matching of the prior simultaneously. In this limit the VLB of the VSC is comparable to that of a VAE, but slightly under-performs it. With more latent dimensions, only a subset of the available elements is used to encode each observation, making learning efficiency more stable as the latent space grows in size.
4.2 LEARNING IN THE LATENT SPACE
An important focus of this work is the ability of VSC to recover latent codes which carry a high level of information about the input. To test this aspect, we compare the representation efficiency of VAE and VSC by performing a standard classification experiment using the latent variables as input features. In order to encourage information rich codes in the VSC, we set the prior Spike probability  to a low value of 0.01. With this very sparse prior, the recognition function activates non-zero elements only when needed to reconstruct an observation, while the prior induces the remaining elements to be mostly null.
We train VAEs and VSCs at varying number of latent dimensions for 20, 000 iterations. In each case, we use 5, 000 encoded labelled examples from the training sets to train a simple one layer fully connected classifiers using the latent codes as inputs. Figure 3 shows the classification performance obtained on the test set.
VSC is able to reliably recover efficient codes without the need to specify an optimal latent space size and also marginally outperforms the best VAE. This is because the recognition function activates only the subset of variables it needs to describe each observation, regardless of the latent space dimensionality. In such a way, the sources of variations in the observed data are automatically dis-

VAE VSC  = 0.01

VAE VSC  = 0.01

Figure 3: Classification performance of VSC and standard VAE at varying number of latent space dimensions. The VAE reaches its peak performance for optimal choice of latent space dimensions, but yields inefficient codes if the latent space is too large. VSC recovers efficient codes for arbitrarily large latent spaces which outperform the VAE ones as classification inputs.
6

Under review as a conference paper at ICLR 2019

covered and encoded into few non-zero elements. The peak performance for the VSCs occurs at larger latent spaces than for the standard VAEs, indicating that there is a representation advantage in encoding to larger spaces with sparser solutions than into smaller dense codes. Additional evaluations of classification accuracy at varying sparsity and number of labelled examples are shown in appendix E.4.
4.3 INTERPRETATION OF THE SPARSE CODES
Lastly, we qualitatively examine the interpretation of the non-zero elements in the sparse codes recovered with the VSC model. To this end, we encode several examples from the test sets of the Fashion-MNIST and CelebA datasets with VSCs trained with prior spike probability  = 0.01. The Fashion-MNIST and CelebA examples were encoded in 200 and 800 latent dimensions respectively. We then show the effect of altering individual non-zero components on the reconstructed observations. Examples are shown in figure 4.

Collar

Sole

Fit

Chest

Tightness

Complexion

Beard

Legs Gap Hair Colour

Child/Adult

Head Shape

Pose

Figure 4: Effect on generation of altering single non-zero elements in VSC latent codes for FashionMNIST (Top) and CelebA (Bottom). The original latent codes are shown in blue and those altered in the latent space are shown in orange. Underneath each code the corresponding reconstruction is shown. The altered elements are highlighted with a coloured circle.
7

Under review as a conference paper at ICLR 2019
Sleeves

Buttons and Collar

Shirt T-shirt
Shirt T-shirt

Figure 5: Effect of altering individual non-zero components of the interpolation vector between the sparse codes of two objects. The original latent codes are shown in blue and those altered in the latent space are shown in orange. The altered elements are highlighted with a coloured circle.
We find that many of the non-zero elements in the latent codes control interpretable features of the generated observations, as shown in figure 4. We further note that these results are not obtained through interpolation of many labelled examples, but simply by altering individually some of the few components activated by the recognition function. In fact, for a particular observation, the recognition function defines a low dimensional sub-space by activating only few non-zero elements that control the features necessary to describe such observation. For different observations, the model can activate different subsets of non-zero elements, exploiting a larger space for the aggregate posterior. In such a way, a particular observation is described by a small subset of variables which are easier to manually explore, while the model can adjust its capacity to represent large and varied datasets.
It is also interesting to consider interpolation between different objects in the VSC latent space; as representations are sparse, so are interpolation vectors between them and we can examine their nonzero elements individually. We show an example considering the interpolation between one image of a shirt and one of a t-shirt in the Fashion-MNIST dataset. Figure 5 shows the effect of altering individually the two largest interpolation vector elements for each example.
The first and largest of the two non-zero elements considered controls the sleeves, which can be added to the t-shirt and subtracted from the shirt by altering this element alone. The second element similarly controls the collar and buttons. Much like the non-zero elements of single encoded examples, those of the interpolation vector between similar objects seem to offer good interpretation.
5 CONCLUSION AND FUTURE WORK
In this paper, we lay the general framework to induce sparsity in the latent space of VAEs, allowing approximate variational inference with arbitrarily complicated and probabilistic sparse coding models. We derived a lower bound which is of clear interpretation and efficient to estimate and optimise, as the VLB of a standard VAE. With the resulting encoders, we recovered efficient sparse codes, which proved to be optimal learning inputs in standard classification benchmarks and exhibit good interpretation in many of their non-zero components. We conclude that inducing sparsity in the latent space of generative models appears to be a promising route to obtaining useful codes, interpretable representations and controlled data synthesis, which are all outstanding challenges in VAEs and representation learning in general.
In future work, we aim to further study the properties of a sparse latent space with respect to its interpretation and features disentanglement capability. We expect VSC to be able to model huge ensembles of varied data by sparsely populating large latent spaces, hence isolating the features that govern variability among similar objects in widely diverse aggregates of data.
8

Under review as a conference paper at ICLR 2019
REFERENCES
S. Arora, R. Ge, T. Ma, and A. Moitra. Simple, efficient, and neural algorithms for sparse coding. Conference on Learning Theory, pp. 113­149, 2015.
Y Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1798­1828, 2013.
C. P. Burgess, I. Higgins, Arka Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Understanding disentangling in -VAE. arXiv preprint arXiv:1804.03599, 2018.
A. Cherian and S. Sra. Riemannian dictionary learning and sparse coding for positive definite matrices. IEEE Transactions on neural networks and learning systems, 28(12):2859­2871, 2017.
I. Goodfellow, A. Courville, and Y. Bengio. Large-scale feature learning with spike-and-slab sparse coding. arXiv preprint arXiv, pp. 1206.6407, 2012.
D. Herna´ndez-Lobato, J. M. Herna´ndez-Lobato, and P. Dupont. Generalized spike-and-slab priors for bayesian group feature selection using expectation propagation. The Journal of Machine Learning Research, 14(1):1891­1945, 2013.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.
J. Ho, Y. Xie, and B. Vemuri. On a nonlinear generalization of sparse coding and dictionary learning. International conference on machine learning, pp. 1480­1488, 2013.
W. Hsu, Y. Zhang, and J. Glass. Unsupervised learning of disentangled and interpretable representations from sequential data. In Advances in neural information processing systems, pp. 1878­1889, 2017.
H. Kim and A. Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018.
D. P.. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv, pp. 1312.6114, 2013.
K. Labusch, E. Barth, and T. Martinetz. Simple method for high-performance digit recognition based on sparse coding. IEEE Transactions on neural networks, pp. 1985­1989, 2008.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
H. Lee, A. Battle, R. Raina, and A. Y. Ng. Efficient sparse coding algorithms. Neural Computation, pp. 801­808, 2007.
Y. Li, A. Cichocki, and S. I. Amari. Analysis of sparse representation and blind source separation. Neural computation, 16(6):1193­1234, 2004.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse coding. Proceedings of the 26th annual international conference on machine learning, pp. 689­696, 2009.
T. J. Mitchell and J. J. Beauchamp. Bayesian variable selection in linear regression. Journal of the American Statistical Association, 83(404):1023­1032, 1988.
S. Mohamed, K. Heller, and Z. Ghahramani. Bayesian and l1 approaches to sparse unsupervised learning. arXiv preprint arXiv, pp. 1106.1157, 2011.
Y. Pu, Z. Gan, Henao R., X. Yuan, C. Li, A. Stevens, and L. Carin. Variational autoencoder for deep learning of images, labels and captions. Advances in Neural Information Processing Systems, pp. 2352­2360, 2016.
9

Under review as a conference paper at ICLR 2019
A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko. Semi-supervised learning with ladder networks. Advances in Neural Information Processing Systems, pp. 3546­3554, 2015.
M. W. Seeger. Bayesian inference and optimal design for the sparse linear model. Journal of Machine Learning Research, 9(Apr):759­813, 2008.
J. A. Shelton, A. S. Sheikh, J. Bornschein, P. Sterne, and J. Lu¨cke. Nonlinear spike-and-slab sparse coding for interpretable image encoding. PloS one, 10(5):e0124088, 2015.
C. K. Sønderby, T. Raiko, L. Maaløe, S. K. Sønderby, and O. Winther. How to train deep variational autoencoders and probabilistic ladder networks. arXiv preprint arXiv, pp. 1602.02282, 2016.
M. K. Titsias and M. La´zaro-Gredilla. Spike and slab variational inference for multi-task and multiple kernel learning. Advances in Neural Information Processing Systems, pp. 2339­2347, 2011.
J. M. Tomczak and M. Welling. VAE with a VampPrior. arXiv preprint arXiv:1705.07120, 2017. J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse
representation. IEEE Transactions on pattern analysis and machine intelligence, pp. 210­227, 2009. H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv, pp. 1708.07747, 2017. J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding for image classification. Computer Vision and Pattern Recognition, pp. 1794­1801, 2009. M. Yang, L. Zhang, J. Yang, and D. Zhang. Robust sparse coding for face recognition. Computer Vision and Pattern Recognition (CVPR), pp. 625­632, 2011.
10

Under review as a conference paper at ICLR 2019

A DETAILS OF THE VSC MODEL
We describe here the details of the VSC model and the architecture of the neural networks we employed as likelihood and recognition functions.

A.1 LIKELIHOOD FUNCTION
The likelihood function p(x|zi) is composed of a neural network which takes as input a latent variable zi  RJ×1 and outputs the mean µi  RM×1 and log variance log(i2)  RM×1. The log likelihood of a sample xi is then computed evaluating the log probability density assigned to xi by a Gaussian having mean µi and standard deviation i.
In our experiments we use a one hidden layer fully connected neural network for the VSCs trained with the MNIST and Fashion-MNIST datasets and a two hidden layers network for the VSCs trained with the CelebA dataset.

A.2 RECOGNITION FUNCTION
The recognition function p(z|xi) is composed of a neural network which takes as input an observation xi  RM×1 and outputs the mean µz,i  RJ×1, the log variance log(z2,i)  RJ×1 and the log Spike probabilities vector log(i)  RJ×1. The elements of i need to be constrained between 0 and 1, therefore, other than using log(i) as output, which ensures i > 0, we employ a ReLU non-linearity at this output of the neural network as follows

log(i) = -ReLU (-vout,i)

(10)

Where vout,i is output to the same standard neural network that outputs µz,i and log(z2,i). This ensures that i < 1. Samples in the latent space zi,l can then be drawn as detailed in equation 8. As for the likelihood function, we use a one hidden layer fully connected neural network for the
VSCs trained with the MNIST and Fashion-MNIST datasets and a two hidden layers network for
the VSCs trained with the CelebA dataset.

B DERIVATION OF VLB PRIOR TERM
We report here a detailed derivation of the VSC lower bound prior term shown in equation 6. As described in section 3, the lower bound we aim to maximise has the same form as the standard VAE one of equation 3, with the Spike and Slab probability density function ps(z) of equation 4 as prior and the discrete mixture distribution of of equation 5 as recognition function q(z|xi). The VSC lower bound prior term is therefore obtained by substituting these distribution in the negative KL divergence term of equation 3. By doing so, we obtain four cross entropy components in each latent
11

Under review as a conference paper at ICLR 2019

dimension
- DKL(q(z|xi)||ps(z)) = q(z|xi)(log ps(z) - log q(z|xi))dz
J
= i,j N (zi,j ; µz,i,j , z2,i,j ) log [N (zj ; 0, 1) + (1 - )(zj )] dzj
j
1 + (1 - i,j) (zi,j) log [N (zj; 0, 1) + (1 - )(zj)] dzj
2 (11) - i,j N (zi,j ; µz,i,j , z2,i,j ) log i,j N (zi,j ; µz,i,j , z2,i,j ) + (1 - i,j )(zi,j ) dzj
3 - (1 - i,j ) (zi,j ) log i,j N (zi,j ; µz,i,j , z2,i,j ) + (1 - i,j )(zi,j ) dzj .
4

1 and 3 are of a similar form; the cross entropy between a Gaussian and a discrete mixture
distributions. These components reduce to the corresponding Gaussian-Gaussian entropy terms, as
the point mass contributions vanish. In fact, for any finite density distributions f (zj) and g(zj), the point mass contribution to the cross entropy between f (zj) and a discrete mixture h(zj) = g(zj) + (1 - )(zj - c) is infinitesimal. The proof is as follows: The cross entropy between the functions f (zj) and h(zj) is

f (zj) log [g(zj) + (1 - )(zj - c)] dzj.

(12)

We can split this integral in two components over two different domains, the first in the region where zj = c and the second in the region where zj = c. By using a Dirac Delta function, the first component can be expressed as follows

f (zj) log [g(zj) + (1 - )(zj - c)] dzj =
zj =c

f (zj) log [g(zj)] dzj =
zj =c

1 - (zj - c) (0)

f (zj) log [g(zj)] ,

(13)

where from the first to the second line we can ignore the component containing (zj - c), as the domain does not include zj = c. We then use a coefficient which is zero at zj = c and one otherwise to write the integral over the whole domain of zj. Similarly, we can write the term in the domain zj = c as

f (zj) log [g(zj) + (1 - )(zj - c)] dzj =
zj =c

(zj - (0)

c) f (zj)

log

[g(zj )

+

(1

-

)(zj

-

c)]

dzj ,

(14)

Now combining the two terms we obtain

12

Under review as a conference paper at ICLR 2019

f (zj) log [g(zj) + (1 - )(zj - c)] dzj

=

1 - (zj - c) (0)

f (zj) log [g(zj)] +

+

(zj - (0)

c) f (zj) log

[g(zj )

+

(1

-

)(zj

-

c)]

dzj

Rearranging to gather the terms in (zj - c)/(0) we get

(15)

f (zj) log [g(zj)] dzj+

(zj - c) (0)

f (zj) log [g(zj) + (1 - )(zj - c)] - f (zj) log [g(zj)]

dzj

=

f (zj) log [g(zj)] dzj +

(zj - (0)

c)

f

(zj

)

log

g(zj) + (1 - )(zj - c) g(zj )

dzj .

(16)

Simplifying the argument of the second logarithm and solving the second integral we get

f (zj) log [g(zj) + (1 - )(zj - c)] dzj

f (c) 1 -  u

=

f (zj)

log

[g(zj )]

dzj

+

lim
u

u

log(1 +



), g(c)

(17)

where the second term tends to zero, leaving the cross entropy between f (zj) and g(zj). Applying this result to 1 and 3 we obtain the following

1 - 3 = i,j N (zi,j ; µz,i,j , z2,i,j ) log [N (zj ; 0, 1)]

- N (zi,j ; µz,i,j , z2,i,j ) log i,j N (zi,j ; µz,i,j , z2,i,j ) dzj

= i,j

N (zi,j ; µz,i,j , z2,i,j ) log

N (zj; 0, 1) i,j N (zi,j ; µz,i,j , z2,i,j )

dzj

= -i,j DKL(N (zi,j ; µz,i,j , z2,i,j ) || N (zj ; 0, 1)) + i,j log

 i,j

(18)

The KL divergence DKL N (zi,j; µz,i,j, z2,i,j) || N (zj; 0, 1) is analogous to that of the standard VAE and has a simple analytic form (Kingma & Welling, 2013):

DKL

N

zi,j ; µz,i,j , z2,i,j

|| N (zj; 0, 1)

= -1 2

1 + log

z2,i,j

- µ2z,i,j - z2,i,j

(19)

2 and 4 take the form of the cross entropy between a Dirac Delta function and a discrete mixture distribution. In this case, instead, the continuous density contributions vanish:

13

Under review as a conference paper at ICLR 2019

2 - 4 = (1 - i,j) (zi,j) log [N (zj; 0, 1) + (1 - )(zj)]

- log i,j N (zi,j ; µz,i,j , z2,i,j ) + (1 - i,j )(zi,j ) dzj

=

lim (1
u

-

i,j

)

log

N (0; 0, 1) + (1 - )u i,j N (0; µz,i,j , z2,i,j ) + (1 - i,j )u

= (1 - i,j) log

1- 1 - i,j

.

(20)

Substituting the results of equations 18, 19 and 20 into equation 11, we obtain the prior term of the VSC lower bound

J
-DKL(q(z|xi)||ps(z)) =
j
J
=
j

1-3+2-4

1 i,j 2

1 + log(z2,i,j ) - µz2,i,j - z2,i,j

Negative Slab KL Divergence

+ (1 - i,j) log

1- 1 - i,j

+ i,j log

 i,j

Negative Spike KL Divergence

.

(21)

This prior term presents two components. The first is the negative KL divergence between the distributions of the Slab variables, multiplied by the probability of zi,j being non-zero i,j. The second term is the negative KL divergence between the distributions of the Spike variables. We find of particular interest that by computing the KL divergence analytically we recover a linear combination of the Spike and Slab components divergences.

C SPIKE VARIABLE REPARAMETRISATION
We report here a more detailed description of the Spike variable reparametrisation. Our aim is to find a function f (l,j, i,j) such that a binary variable wi,l,j  p(wi,l,j) drawn from the discrete distribution p(wi,l,j = 1) = i,j , p(wi,l,j = 0) = (1 - i,j ) can be expressed as wi,l,j = f (l,j , l,j ), where l,j is some noise variable drawn from a distribution which does not depend on i,j.
The function of choice f (l,j, i,j) should ideally only take values 1 and 0, as these are the only values of wi,l,j permitted by p(wi,l,j). Furthermore, the probabilities of wi,l,j being 1 or 0 are linear in i,j, therefore the distribution of the noise variable i,j should have evenly distributed mass. The simplest function which satisfy these conditions and yields our reparametrisation is then a step function f (l,j, i,j) = T (l,j - p(wi,l,j = 0)) = T (l,j - 1 + i,j) where l,j is uniformly distributed and T (y) is the following step function

T (y) = 1, if y  0. 0, if y < 0.

(22)

An illustration of this reparametrisation is shown in figure 6.
As described in section 3.2, the function T (yi,l,j) is not differentiable, therefore we approximate it with a scaled Sigmoid S(cyi,l,j), where c is a real positive constant. In our implementation, we gradually increase c from 50 to 200 during training to achieve good approximations without making convergence unstable.

14

Under review as a conference paper at ICLR 2019

P(l,j)

P(w=1) = 1

P(w=0) = (1-i,j) P(w=1) = i,j (1-i,j)

Figure 6: Schematic representation of the reparametrisation of the Spike variable. The variable yi,l,j is drawn in the range covered by the grey square with probability proportional to its height. On the left, for a spike probability i,j = 1, the variable yi,l,j is drawn to always be greater than zero and the Spike variable wi,l,j is always one. On the right, for an arbitrary i,j, the probability density of yi,l,j is displaced to the left by 1 - i,j and yi,l,j has probability i,j of being  0, in which case wi,l,j is one, and probability 1 - i,j of being < 0, in which case wi,l,j is zero.
D DETAILS OF THE EXPERIMENTS
D.1 DETAILS OF THE AUTO-ENCODERS
In our experiments, we use VAEs and VSCs having one 400-dimensional hidden layer between the observations and latent variables, both for encoders q(z|x) and decoders p(x|z). The only exception is the VSC used to obtain the qualitative results with the CelebA dataset, which was composed of two hidden layers with 2, 000 dimensions between the observations and latent variables. We trained all auto-encoders with the ADAM optimiser, where the initial training rate was chosen according to best VLB performance of the standard VAE and kept the same for the corresponding VSC we compare to it. All training rates used were between 0.001 and 0.01.
D.2 DETAILS OF THE DATASETS
MNIST and Fashion-MNIST are composed of 28 × 28 grey-scale images of hand-written digits and pieces of clothing respectively. Both sets contain ten different classes, which is the categories in which we classify in section 4.2. CelebA is a dataset of 200, 000 examples of colour images of celebrity faces. We normalise the MNIST and Fashion-MNIST examples to have unitary norm before performing our experiments. For the CelebA examples, we use the centered version of the dataset, crop and downsample the images to obtain 32 × 32 RGB pictures, which we also normalise to have unitary norm before performing experiments. We divide the datasets in training and test sets. For the MNIST and Fashion-MNIST sets, we preserve the original division of 60, 000 training examples and 10, 000 test examples. For the CelebA dataset, we use a subset of 100, 000 examples as training set and one of 20, 000 as test set. For all results presented, the models were trained using the training sets and the results presented are obtained by encoding/decoding examples from the test sets, unless otherwise stated.
E ADDITIONAL EXPERIMENTAL RESULTS
E.1 EXAMPLES OF SPARSE CODES AND RECONSTRUCTIONS
Figure 7 shows some examples of latent codes and reconstruction recovered with the VSC model at different values of prior sparsity .
15

Under review as a conference paper at ICLR 2019

=1

 = 0.5

 = 0.2

Rec. Latent Code Original

=1

 = 0.5

 = 0.2

Rec. Latent Code Original

=1

 = 0.5

 = 0.2

Rec. Latent Code Original

Figure 7: Examples of sparse codes and reconstruction for the MNIST (top), Fashion-MNIST (middle) and CelebA (bottom) datasets.
E.2 LATENT SPACE SPARSITY
We measure the latent space posterior sparsity at varying prior sparsity . We encode both the MNIST and Fashion-MNIST datasets in 200-dimensional spaces with different values of the prior Spike probability . In each case, we measure the aggregate posterior sparsity. Results are shown in figure 8. For larger values of  the resulting codes retain approximately the sparsity induced by the prior as expected. At lower values of  the latent codes sparsity increasingly departs from the value induced by the prior. This is expected since below a certain sparsity value, the recognition function is induced to activate a certain number of latent dimensions in order to satisfy reconstruction.
E.3 ADDITIONAL VLB EVALUATION
We report on the VLB evaluation results. First, we show analogous results to those in figure 2 for different imposed iterations limits in figure 9
16

Under review as a conference paper at ICLR 2019

Measured Sparsity Perfect Prior Match

Measured Sparsity Perfect Prior Match

Figure 8: Measured sparsity at varying prior Spike probability .

VAE VSC  = 0.5 VSC  = 0.2

VAE VSC  = 0.5 VSC  = 0.2

VAE VSC  = 0.5 VSC  = 0.2

VAE VSC  = 0.5 VSC  = 0.2

VAE VSC  = 0.5 VSC  = 0.2

VAE VSC  = 0.5 VSC  = 0.2

Figure 9: Test sets VLB evaluation of VSC at varying number of latent space dimensions for different iterations limits. The out most right graphs correspond to those shown in figure 2.
Next, we show the behaviour of the lower bound at varying prior sparsity  for high dimensional latent spaces. We encode both the MNIST and Fashion-MNIST datasets in 200-dimensional spaces with different values of  and measure the training and test sets VLB in each case. The results are shown in figure 10.

Training Set Test Set

Training Set Test Set

Figure 10: Training and test sets VLB at varying prior Spike probability .
By making the Prior increasingly sparser (i.e.  going from 1 to 0) the VLB increases thanks to the smaller sub-spaces needed to represent each observation. At very low , the lower bound decreases again, as the number of dimensions activated by the recognition function in order to describe the observations is too high to match the prior.
17

Under review as a conference paper at ICLR 2019
E.4 CLASSIFICATION AT VARYING SPARSITY We show the classification performance at varying prior sparsity  for high dimensional latent spaces and various limits of available number of training examples. We encode both the MNIST and Fashion-MNIST datasets in 200-dimensional spaces with different values of  and measure the classification accuracy when classifying with a one layer network as described in 4.2. Figure 11 displays the results.

20,000 Labelled Examples 5,000 Labelled Examples 2,000 Labelled Examples

20,000 Labelled Examples 5,000 Labelled Examples 2,000 Labelled Examples

Figure 11: Classification performance at varying prior Spike probability . As the prior Spike probability is decreased, the recovered codes are increasingly more efficient.

18

