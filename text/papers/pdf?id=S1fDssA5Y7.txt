Under review as a conference paper at ICLR 2019

DISTRIBUTIONALLY ROBUST OPTIMIZATION LEADS TO BETTER GENERALIZATION: ON SGD AND BEYOND
Anonymous authors Paper under double-blind review

ABSTRACT
In this paper, we adopt distributionally robust optimization (DRO) in hope to achieve a better generalization in deep learning tasks. We establish the generalization guarantees and analyze the localized Rademacher complexity for DRO, and conduct experiments to show that DRO obtains a better performance. We reveal the profound connection between SGD and DRO, i.e., selecting a batch can be viewed as choosing a distribution over the training set. From this perspective, we prove that SGD is prone to escape from bad stationary points and small batch SGD outperforms large batch SGD. We give an upper bound for the robust loss when SGD converges and keeps stable. We propose a novel Weighted SGD (WSGD) algorithm framework, which assigns high-variance weights to the data of the current batch. We devise a practical implement of WSGD that can directly optimize the robust loss. We test our algorithm on CIFAR-10 and CIFAR-100, and WSGD achieves significant improvements over the conventional SGD.

1 INTRODUCTION

In recent years, supervised learning based on deep neural networks (DNNs) has achieved state-of-
the-art performances in various tasks of different domains, such as computer vision and natural
language processing (Goodfellow et al., 2016). However, theoretical explanations for the general-
ization ability of deep learning remain challenging. Roughly speaking, supervised learning aims to learn a model from the training data. Let  denote the parameter space of the model, and Z denote the sample space. Let (z, ) : Z ×   R+ denote the loss function related to the parameter  and the instance z. In the general setting of supervised learning, there is a fixed but unknown distribution over Z (denoted P ). We attempt to find a   , which gives a small generalization error. That is,
we address the following optimization problem:

min R() EP (z, ).


(1)

However, the real distribution P is unknown, so we consider an empirical distribution. Given

the training samples S = (z1, z2, · · · , zm), the empirical distribution is defined as Pm(z) =

1 m

m i=1

I[z=zi],

where

I

is

the

indicator

function.

The loss related to this distribution is called

the empirical loss, which is

R()

1m

EPm (z, ) = m

(zi, ),

i=1

(2)

and we minimize the empirical loss instead. This procedure is referred to as Empirical Risk Minimization (ERM).

The overwhelming capacity and the paucity of data make deep learning architectures susceptible to
over-fitting. This urges us to propose a better approach to the minimization problem (1). Suppose there is a distribution family M = {P :   } parametrized by , and the underlying distribution P = P0  M, where 0 is fixed but unknown. It is a common case in deep learning tasks that the structure of the data is complicated while the amount of data is small. Hence, when we use an estimator ^ to estimate the parameter 0, the variance of ^ would be quite large. Thus, rather than directly minimizing the true expected loss R() = EP0 (z, ), we minimize a distributionally

1

Under review as a conference paper at ICLR 2019

robust loss alternatively, which can ensure the true expected loss to be small with high probability. The distributionally robust loss is defined as:

R(, K) sup EP (z, ),
K

where K   is a neighbourhood of the estimator ^. This procedure is called Distributionally
Robust Optimization (DRO). Because we only have access to the given training dataset, we construct an unbiased1 estimator of the expected loss EP (z, ):

1 m

m P(zi) i=1 P0 (zi)

(zi, ).

This can be viewed as the expected loss with respect to a perturbed empirical generalized distribu-

tion2

P

=

1 m

m i=1

P (z ) P0 (z)

I[z=zi

]

,

so

the

DRO

problem

becomes

minimizing

the

empirical

distri-

butionally robust loss:

R(, K)

sup
K

EP

m
(z, ) = sup

P(zi)

K i=1 mP0 (zi)

(zi, ).

Keeping the motivations above in mind, we focus our attention on the robust loss, defined as the maximal possible weighted loss over a pre-specified weight set P for simplicity:

RS,P ()

m
sup pi (zi, ).
pP i=1

In deep learning, the high capacity of the model and the non-convexity of the objective function lead to an extremely complicated landscape and induce a lot of minima, and different minima have different generalization performances. In this case, an optimization algorithm is both an optimizer and a minimum selector, and the minimum preference of the optimization algorithm is crucial in deep learning. Stochastic Gradient Descent (SGD) and its variants are widely used to minimize the empirical loss (2). The simplest mini-batch SGD consists of iterative sampling of a batch of data and then updating the parameter:
1 t+1 = t - t |Bt| ziBt  (zi, t) ,

where Bt  S is the sampled mini-batch, t is the learning rate, and |Bt| denotes the batch size. We interpret SGD from another perspective: at each iteration, SGD randomly chooses an empirical distribution over the data and performs a gradient descent update w.r.t. the corresponding loss. This interpretation shows a profound connection between SGD and DRO. Motivated by the connection, we develop a brand new version of SGD that can achieve a better generalization performance. The main contribution of this paper is summarized as follows:

· We clarify the connection between mini-batch SGD and DRO. We show that SGD helps escape from bad stationary points and the robust loss of local minima where SGD converges is not large.
· We analyze the generalization property of robust loss and show the trade off: the robust loss generalizes well when P is relatively small, but a larger P is beneficial to reducing the localized Rademacher complexity.
· We propose a framework of improved SGD algorithm, which enables the algorithm to explore more empirical distributions over the data. We give a practical improved SGD algorithm that can directly optimize the robust loss.
1To make the fraction reasonable, we assume P0 (z) > 0 whenever P(z) > 0. 2Strictly speaking, this is not a distribution because P(Z) may not equals to 1, but its expectation is 1 (the expectation is taken over the sample S). Nonetheless, we can view P as a measure over Z.

2

Under review as a conference paper at ICLR 2019

2 RELATED WORK

SGD is widely used in large-scale machine learning problems. The convergence theory of SGD in convex cases is solid (Ghadimi & Lan, 2013; Shamir & Zhang, 2013). However, due to the nonconvexity of deep learning models, the behaviour of SGD is more complicated. Dauphin et al. (2014) argued that it is the prevalence of saddle points rather than local minima that causes the difficulty in high dimensional non-convex optimization. Ge et al. (2015) analyzed the property of perturbed GD (i.e., gradient descent with an isotropic noise injected). They showed that perturbed GD has the ability to escape from saddle points in a polynomial number of iterations, and then converges to a second-order stationary point.

It is also well known that adding perturbation and noise at different levels of the training processes helps generalization, such as dropout (Srivastava et al., 2014), and random noise injection to the input data (Chapelle et al., 2001). The good performance of SGD might benefit from the noise. Keskar et al. (2016) discovered that large-batch training will incur a degradation in the generalization performance of the model. They explained this phenomenon with the concept of sharp minima, and carried out numerical experiments to support the idea that SGD with large batch size tends to converge to sharp minima, which leads to a bad generalization ability. However, Dinh et al. (2017) provided theoretical results indicating that most existing definitions of sharpness and flatness of minima can not be applied to some commonly used non-negative homogeneous activation functions such as ReLU.

DRO was originally applied to decision making under uncertainty. Namkoong & Duchi (2017)

introduced DRO to machine learning, and viewed it as a variance-based regularization that allows

us

to

automatically

trade-off

between

bias

and

variance.

They

proved

an

O(

1 m

)

approximation

to

the

generalization

error

R(),

in

contrast

to

the

general

O(

1 m

)

approximation

of

the

empirical

risk.

Recently, DRO is used to achieve fairness without demographic information in fair machine learning

(Hashimoto et al., 2018).

In learning theory, Rademacher complexity is widely used to measure the complexity of a hypothesis set (Bartlett & Mendelson, 2002). Different from the general Rademacher complexity which considers the entire hypothesis set, localized Rademacher complexity only cares about the near optimal hypothesis (Bartlett et al., 2002). In fact, the hypothesis selected by a learning algorithm usually has better performance than the worst hypothesis. Thus, localized Rademacher complexity is usually much smaller than the general Rademacher complexity.

3 NOTATION AND PRELIMINARIES

In our work, we assume the loss function (z, ) is bounded between 0 and 1, and is continu-

ously differentiable. Let  denote the hyperplane {p :

m i=1

pi

=

1}

and

+

denote

the

simplex

{p :

m i=1

pi

=

1,

p



0}.

Denote p0

=

(

1 m

,

.

.

.

,

1 m

)

.

For p

=

(p1, p2, . . . , pm)

 , let

RS,p() denote

m i=1

pi

(zi, ).

Then the ERM loss can be written as RS,p0 (), and the robust loss

w.r.t. the weight set P can be viewed as:

RS,P () = sup RS,p().
pP

Let conv(P) denote the convex hull of P. Given a v  , let (v) be the set of all the vectors whose entries are permutations of v's. Let P(v) denote the convex hull of (v).

We

define

P (k)

as

the

set

of

all

the

vectors

that

have

k

1 k

's

and

(m

-

k)

0's:

P(k) =

p

=

1 v

:

v



{0, 1}m ,

k

m

vi = k

.

i=1

Clearly, P(k) has a natural connection to SGD with batch size k: for any batch sampled from the

dataset,

there

is

a

corresponding

p



P (k),

where

pi

=

1 k

if

the

i-th

instance

is

in

the

batch.

In the remainder of the paper, we assume that P is symmetric, i.e., if p  P, then (p)  P. This follows that p0  conv(P). And we assume that P is a compact set. We assume the entries of

3

Under review as a conference paper at ICLR 2019

p  P sums to 1, but we relax the restrict that each entry of p  P is non-negative, i.e., we assume P  .

To describe the properties of the set P, we use the notation RADp(P) to denote the minimum radius of the Lp ball centered at p0 that contains P, and we use radp(P) to denote the maximum radius of the Lp ball centered at p0 whose intersection with  is contained within P. We denote
P p = suppP p p, and P 0 as the maximum number of non-zero entries of p  P.
Definition 1. A function f :   R is L-Lipschitz, if:

|f (1) - f (2)|  L 1 - 2 , 1, 2  .

Definition 2.

A

function

f

:





R

is

µ-strongly

convex

if

f ()

-

µ 2



2 is convex.

Some basic properties of the robust loss RS,P () are summarized in Appendix A.

4 CONNECTION BETWEEN SGD AND DRO

SGD has long been regarded as an excellent optimizer. It is generally believed that the good performance of SGD is due to the noise introduced by stochastic gradient. But the existing results commonly view stochastic gradient as an oracle that provides an unbiased estimation of the true gradient, ignoring the fact that the noise is data-dependent. In this section, we directly analyze the good properties of mini-batch SGD from the distributionally robust perspective.

At each iteration, SGD calculates the gradient of RS,p(), where p is the weight vector correspond-
ing to the current batch, and performs the update    - RS,p(). This can be viewed as changing the distribution over the data. Thus SGD can improve the distributional robustness of the model. The following theorem shows that changing the distribution helps mini-batch SGD escape from some bad stationary points, which is characterized by the heterogeneity of gradients of each individual instance.
Theorem 1. Let S be a fixed sample of size m. For a fixed   , define a matrix G = ( (z1, ), · · · ,  (zm, )), whose i-th column is the gradient of the i-th loss function w.r.t. . For SGD with batch size k, denote p as the random weight vector uniformly sampled from P(k). Then the expected squared update amount can be calculated by:

E
p

RS,p()

2 2

=

RS,p0 ()

2 2

+

m-k k(m - 1)

tr(G G) - m

RS,p0 ()

2 2

.

When

tr(G m

G)

RS,p0 () 22, the gradient of the ERM loss is relatively small, but the average

squared length of the gradients g1, · · · , gm is large. This happens only if those gradients lie in some

opposite directions and cancel with each other. In this case, if we leave out the instance with the

largest gradient norm, then the average gradient of the rest of the sample would be almost as large as

the

1 m

of

the

largest

gradient

norm,

and

the

local

landscape

of

the

average

loss

function

would

shift

dramatically. This indicates the model pays too much attention to this specific instance and fails to

learn the intrinsic structure of the data, which can be viewed as a signal of over-fitting. We call this

a

bad

stationary

point.

So

the

scale

of

tr(G m

G)

and

RS,p0 ()

2 2

captures

the

over-fitting

tendency

of a model by measuring the heterogeneity of gradients of the individual instance.

Furthermore, we can know from Theorem 1 that for any fixed , E
p

RS,p0 ()

2 2

decreases mono-

tonically when k becomes larger, indicating the update amount of small batch SGD is larger than

that of large batch SGD in expectation. Therefore, small batch SGD has a stronger ability to capture

the non-uniformity of gradients and escape from bad stationary points.

Intuitively, if SGD converges and keeps stable when the distribution changes, the weighted average
loss RS,p will not be too large, otherwise SGD has the tendency to escape. To theoretically prove it, we need to focus our attention to some simple case. If SGD has escaped from bad stationary points and the deep learning model really captures some intrinsic structure of the data, then it is conceivable that a batch of the instances Bt is able to reflect some common features of the entire sample S, so the local landscape of the average loss function will not shift too much when we only consider a batch of the data. In this case, we have the following theorem.

4

Under review as a conference paper at ICLR 2019

Theorem 2. Assume the loss function is L-Lipschitz. Suppose the SGD algorithm with constant

learning rate  converges to and stays in B, where B = { :  - 0 2  B} is a ball that contains

a minimum of ERM loss RS,p0 . Suppose that p  P(k), B contains a local minimum of RS,p and

RS,p is µ-strongly convex on B. Further assume that the minimum value of the batch average loss

RS,p

in

B

is

not

too

large,

i.e.,

min
B

RS,p()



min
B

RS,p0 ()

+

.

Then

the

robust

loss

RS,P (k) ()

can be bounded by:

RS,P (k) ()



min
 B

RS,p0

(

)

+

2B2 µ2

+

2LB

+

,

  B.

5 THEORETICAL ANALYSIS

5.1 GENERALIZATION GUARANTEE OF DRO

In this subsection, we introduce the concept of Robust Rademacher Complexity and use it to derive the generalization guarantee for robust loss, which provides justifications for DRO. We bound the expectation of robust loss by empirical robust loss plus a complexity term with high probability. Then we utilize the notion of covering number to show that the generalization of robust loss is not too hard compared to that of ERM loss.
Definition 3 (Empirical Robust Rademacher Complexity). Let S = (z1, · · · , zm) be a fixed sample of size m. Then the empirical robust Rademacher complexity of the parameter space  and the weight set P w.r.t. the sample S is defined as:

m

RS

(,

P

)

=

E


sup sup ipi (zi, )
 pP i=1

,

where  = (1, . . . , m) is a random vector with independent entries uniformly from {-1, 1}. Theorem 3. For any  > 0, with probability at least 1 - , the following holds for all   :

ERS ,P ()  RS,P () + 2RS(, P) + 3m P 
S

12 log .
2m 

For a fixed sample S of size m, denote (S) = ( (z1, ), · · · , (zm, )) :    and
(S, P) = (mp1 (z1, ), · · · , mpm (zm, )) :   , p  P . For a norm · in Rm, denote the dual norm by · . For > 0, let N (A, · , ) denote the covering number of the set A  Rm, which is the minimum cardinality of the -net w.r.t. the norm · . Denote d(m) as an upper bound of   = (1, · · · , m) .
Theorem 4. For any > 0, the empirical Rademacher complexity and the empirical robust Rademacher complexity can be respectively bounded by:

RS() 

2 log N ((S),

·

,) +

d(m) ,

mm

and

RS(, P)  (1 + RAD2(P))

2 log N ((S, P),

·

,) +

d(m) .

mm

(3) (4)

Notice that (S, P) is generated from (S) by having all the points perturbed a little bit, so it is conceivable that these two sets are close, implying that their covering numbers are close. Specifically, if we define
d = sup inf x - y
x(S,P) y(S)
as the maximum distance of a point in (S, P) deviating from the set (S), then it follows that an -net of (S) is also an ( + d)-net of (S, P). To compare the right-hand sides of the covering

5

Under review as a conference paper at ICLR 2019

number guarantees (3) and (4), we change to + d in (4), and we have:

(1 + RAD2(P))

N ((S, P),

·,

+ d) ( +

+ d)d(m)

mm

 (1 + RAD2(P))

N ((S),

·

,) +

d(m) + dd(m) .

m mm

When we use the Lp norm, one can easily show that d  mRADp(P), and   can also be

bounded as







1
mq

=

p-1
mp,

so

d(m)

=

p-1
mp.

Then

the

extra

term

dd (m) m

can be bounded

by

m1-

1 p

RADp (P ).

Those bounds quantify how the size of P will influence the generalization ability, and give a sufficient condition on the size of P to ensure that the covering number bounds are close. Conceivably, when P is sufficiently small, the generalization of robust loss is not too difficult. It is worth noticing
that the inequality N ((S, P), · , + d)  N ((S), · , ) is quite loose when the set (S) is some sort of "solid," and this is often the case in deep neural networks, whose capacities are thought to be very high. In one word, it is completely feasible to consider DRO when using DNNs.

5.2 LOCALIZED RADEMACHER COMPLEXITY

Next, we analyze the localized Rademacher complexity based on robust loss. Localized Rademacher complexity measures the complexity of near optimal hypotheses, which is much smaller than the entire hypothesis space. Here we assume the hypothesis set is restricted to a L2 norm ball.3
DRO can be viewed as a variance-based regularization technique (Namkoong & Duchi, 2017). A low robust loss means a low variance of the losses ( (z1, ), · · · , (zm, )), since otherwise one can choose a p  P that puts more weight on the instances with a large loss, and obtains a high robust loss. A larger size of P means a more radical way to assign weights, indicating that DRO with large P renders a stricter regularization on the variance of the losses. Rademacher complexity measures the ability of a hypothesis set to fit high variance random noises, and naturally a low variance of the losses means a small Rademacher complexity. The following theorem reveals the connection between Rademacher complexity and DRO, and provides a model-free bound on the localized empirical Rademacher complexity.
Theorem 5. Assume the loss function is L-Lipschitz w.r.t.  for all z. Consider the following hypothesis set:
c =  : ESRS,P ()  c, ||||2 < r .
For any > 0, let N (c, ) denote the covering number of c. Then for all 0 <  < 1, with probability  1 - , the localized empirical Rademacher complexity can be bounded as:

RS (c )



1 m

1+ 

4

mrad (conv(P ))

c+

P 0 P L+ P 

m log N (c, ) 2

.

Conceivably, when m and c are fixed, increasing the size of P reduces the set c. This bound shows

that increasing the size of P also reduces the complexity of c under certain conditions. For exam-

ple, let P be the intersection of  and a L ball with radius  centered at p0. When  is relatively

small

compared

to

1 m

,

increasing



does

not

change

P

 too much, but

1 rad (conv(P ))

is inversely

proportional

to

.

Therefore,

increasing



can

reduce

the

scale

of

P rad (conv(P ))

dramatically.

6 WEIGHTED SGD

Motivated by the DRO and the previous theoretical analyses, we propose a new variant of SGD algorithms in Algorithm 1. We will see that this variant is simple and practical for applications.

3 When we train models with a constant weight decay wd, generally the L2 norm of final parameters will

not excess f (0)/wd, where f is the loss function. This is because when the algorithm minimizing L() =

f () + wd



2 2

outputs



,

we

generally

haveL(

)



L(0),

hence,

wd



2 2



f (

)

+

wd



2 2



f (0).

6

Under review as a conference paper at ICLR 2019

We have emphasized the significance of distributional robustness before. The critical factors that influence distributional robustness are the size of P and the value of the corresponding robust loss

RS,P (). In order to obtain a more distributionally robust model, a direct approach is to increase the

size

of

P.

In

the

conventional

SGD,

the

weight

of

the

data

in

the

current

batch

is

1 k

,

and

the

other

is

0. Due to the limited computational resources, we still only have access to one mini-batch at each

iteration, but we can assign high-variance weights to the instances of the mini-batch. This implies

that our algorithm enjoys the idea behind importance sampling. Therefore, our algorithm explores a

wider region of empirical distributions, yielding a more distributionally robust solution.

Algorithm 1 Weighted Stochastic Gradient Descent (WSGD)

Require: Initial parameter: 0, Weight Generator: G, Learning Rate: t, Total Iteration: T , Training Data: S = {z1, z2, . . . , zm}.

1: for t = 1 to T do

2: Select a mini-batch: Bt = {zj1 , zj2 , . . . , zj|Bt| }

3: Generate weight: (w1, w2, . . . , w|Bt|)  G

4:

Calculate normalization factor: W =

|Bt | i=1

wi

5:

Calculate

stochastic

gradient:

gt

=

1 W

|Bt| i=1

wi



(zji , t-1)

6: Update parameter: t = t-1 - tgt

7: end for

8: return T

We turn to the choices of the weight generator. In particular, we recommend two approaches:

· G1(q, r): wi = r if (zji , t-1) is among the q|Bt| smallest elements of the losses ( (zj1 , t-1), · · · , (zjBt , t-1)), otherwise wi = 1;
· G2(q, r): Randomly select q|Bt| of the indices {ji} and set their weights to r. Set the others to 1,

where q  [0, 1] is the proportion of the instances whose weight is set to r, and r is a pre-specified constant. Notice that r is not necessarily non-negative.

We have conducted experiments to show that Algorithm 1 with generator G1 achieves a better performance on classification tasks than the conventional SGD (see Section 7). We refer to Algorithm 1 with weight generator G1(q, r) as WSGD(q, r).
Different from the conventional SGD, WSGD(q, r) directly optimizes a robust loss. It is easy to prove that there exists a vector v(q, r)   such that the stochastic gradient in WSGD(q, r) is an
unbiased estimation of the gradient of RS,P(v(q,r))() at differentiable points (see Appendix C for details). Furthermore, the stochastic gradient in WSGD(q, r) is bounded by the Lipshitz constant of when r is non-negative. Taking advantages of previous work, we can give the convergence guarantee for WSGD(q, r) with a non-negative r in the strongly convex case:

Corollary 1 (Shamir & Zhang 2013). Suppose the loss function (z, ) is µ-strongly convex and

L-Lipshitz.

Consider

WSGD(q, r)

with

step

size

t

=

1 µt

and

r

is

non-negative.

Let

RS ,P (v(q,r))

be

the optimal value of RS,P(v(q,r))(). Then for all T > 1, it holds that:

E[RS,P(v(q,r))(T )

- RS,P(v(q,r))]



17L2(1 + log(T )) .
µT

It is worth pointing out that G2 usually yields a lower test loss in our trial. Strictly speaking, this means G2 has a stronger generalization ability. However, the loss we choose in classification tasks is different from the true loss (0-1 loss), thus the algorithm with a low test loss not always has a low
prediction error. We still recommend G2 because it may achieve a good performance on tasks which we can deal with the true loss directly.

7

Under review as a conference paper at ICLR 2019

7 EXPERIMENTS
In this section, we compare WSGD with the conventional stochastic gradient descent, which is regarded as the optimizer with the best generalization. We carry out the experiment on the CIFAR10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009), which both have 50k training data and 10k test data. On CIFAR-10, we apply our algorithm and SGD to two small networks and two large networks: ResNet-44, ResNet-56, VGG-16 and ResNet-34 (He et al., 2016; Simonyan & Zisserman, 2014). On CIFAR-100, we apply our algorithm and SGD to three large networks: VGG-16, ResNet34 and DenseNet-121 (Huang et al., 2017). We train the networks following the data augmentation in Lee et al. (2015): 4 pixels are padded on each side, and a 32 × 32 crop is randomly sampled from the padded image or its horizontal flip. We only do evaluation on the original 32 × 32 image while testing. In order to compare the generalization ability of optimization algorithms, we train the models for 600 epochs to ensure convergence. We set the initial learning rate to 0.1, and divide it by 10 at 300 and 450 epochs. The choice of hyper-parameters in WSGD(q, r) is listed in the experiment results. Empirically speaking, the hyper-parameters are mainly related to the dataset and network architecture, and a smaller r is suitable for larger networks.
We summarize the experiment results on CIFAR-10 and CIFAR-100 in Table 1. For each network and algorithm, we report the test accuracy, the L2 ball and L ball robust loss. Lp ball robust loss is the robust loss with P as the intersection of + and the Lp ball centered at p0 with radius radp(+). In other words, we consider the largest Lp ball in  centered p0 that has positive entries.
From table 1 we can see that WSGD outperforms SGD in all experiments. On CIFAR-10, WSGD achieves top-1 accuracy improvement by  5 except VGG-16. On CIFAR-100, WSGD achieves top-1 accuracy improvement by 5 to 15 . Moreover, the models trained by WSGD have a lower L2 ball and L ball robust loss than SGD. The good performance shows WSGD improves the distributional robustness of the models.

Table 1: Top 1 accuracy(%) and robust loss(×10-3) on CIFAR 10 and CIFAR 100

Network ResNet-44 ResNet-44 ResNet-56 ResNet-56 VGG-16 VGG-16 ResNet-34 ResNet-34 VGG-16 VGG-16 ResNet-34 ResNet-34 DenseNet-121 DenseNet-121

Params 0.66M 0.66M 0.86M 0.86M 14.74M 14.74M 21.30M 21.30M 14.78M 14.78M 21.35M 21.35M 7.13M 7.13M

Acc
93.60 94.24 93.86 94.33 94.05 94.30 95.13 95.65
72.13 73.60 77.89 78.43 79.31 80.35

L2-ball 0.5902 0.4892 0.3762 0.3070 0.9617 0.8807 0.7113 0.6422 1.535 1.209 1.473 0.9876 3.359 1.196

L-ball 1.116 0.9507 0.6745 0.5835 1.112 1.004 0.9379 0.8381 2.942 2.346 2.562 1.871 4.614 2.152

Method SGD WSGD(0.5,0.3) SGD WSGD(0.5,0.4) SGD WSGD(0.5,0.2) SGD WSGD(0.5,0) SGD WSGD(0.5,0) SGD WSGD(0.5,0) SGD WSGD(0.5,-0.1)

Dataset CIFAR-10 CIFAR-10 CIFAR-10 CIFAR-10 CIFAR-10 CIFAR-10 CIFAR-10 CIFAR-10 CIFAR-100 CIFAR-100 CIFAR-100 CIFAR-100 CIAFR-100 CIAFR-100

8 DISCUSSION AND CONCLUSION
In this paper we have theoretically analyzed the good property of DRO, and revealed the profound connection between SGD and DRO. Accordingly we have proposed a practical algorithm that can utilize the non-isotropic noise of the stochastic gradient. We have tested WSGD algorithm on CIFAR-10 and CIFAR-100, achieving significant improvements compared to SGD.
We hope this paper can inspire works that theoretically study the generalization ability of optimization algorithms. We think one should pay attention to the loss of each individual instance, rather than the average of the entire sample. We expect future works on SGD without simplification to isotropic noise, and algorithms that take advantage of the non-isotropic noise of the stochastic gradient.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized rademacher complexities. In International Conference on Computational Learning Theory, pp. 44­58. Springer, 2002.
Olivier Chapelle, Jason Weston, Le´on Bottou, and Vladimir Vapnik. Vicinal risk minimization. In Advances in neural information processing systems, pp. 416­422, 2001.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pp. 2933­2941, 2014.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933, 2017.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pp. 797­842, 2015.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341­2368, 2013.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.
Tatsunori B Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. arXiv preprint arXiv:1806.08010, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeplysupervised nets. In Artificial Intelligence and Statistics, pp. 562­570, 2015.
Pascal Massart. Some applications of concentration inequalities to statistics. In Annales-Faculte des Sciences Toulouse Mathematiques, volume 9, pp. 245­303. Universite´ Paul Sabatier, 2000.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148­ 188, 1989.
Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems, pp. 2971­2980, 2017.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International Conference on Machine Learning, pp. 71­79, 2013.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
9

Under review as a conference paper at ICLR 2019

A SOME PROPERTIES OF ROBUST LOSS

Lemma 1. Let 1, · · · , m be arbitrary real numbers. Let conv(P) denote the convex hull of P, then

mm

sup pi i = sup

pi i.

pP i=1

pconv(P) i=1

Lemma 2. Let 1, · · · , m be arbitrary positive numbers. Assume P   and is symmetric. Then the following holds for all p  P:

mm

| pi i|  sup pi i.
i=1 p P i=1

Lemma 3. Assume the loss function (z, ) is L-Lipschitz with respect to  for all z, then for any

fixed sample S of size m, RS,P () is P 0 P L-Lipschitz

Proof.

|RS,P () - RS,P ( )|

= | sup pi (zi, ) - sup

pP

pP

pi (zi,  )|

 max sup pi ( (zi, ) - (zi,  )) , sup pi ( (zi,  ) - (zi, )) .

pP

pP

Notice that | (zi, ) - (zi,  )|  L  -  for all zi, we have: | pi ( (zi, ) - (zi,  )) |

 |pi|L  - 

 P 0 P L  -  .

One can develop various bounds on the Lipschitz constant of RS,P ().

Lemma 4 (Convexity). Assume that all entries of p  P is non-negative. Futher assume that (z, )

is convex with respect to  for all z, then for a fix sample of size m, the robust loss RS,P () is convex.

m

And denote p = arg sup RS,p(), then for any gi   (zi, ), pi gi is in the subgradient of

pP

i=1

RS,P () at point .

Proof. For any p  P, RS,p() is a convex combination of convex functions, thus it is convex.
RS,P is supremum over a family of convex functions, thus it is also convex. To complete the proof, by the definition of subgradient, we have the following holds for all i = 1, · · · , m:

Then, So,

(zi,  )  (zi, ) + gi,  -  .

m

RS,p ( ) = pi (zi,  )

i=1

mm

 pi (zi, ) +

pi gi,  -  .

i=1 i=1

RS,P ( )  RS,p ( )

mm

 pi (zi, ) +

pi gi,  - 

i=1 i=1

m

= RS,P () +

pigi,  -  .

i=1

10

Under review as a conference paper at ICLR 2019

Lemma 5. Assume the loss function (z, ) is µ-strongly convex (convex), then for any P  +, RS,P is µ-strongly convex (convex).

Lemma 6. Assume the loss function (z, ) takes values in [0, 1], then the robust loss RS,P is

bounded by

1+ P 2

1

B PROOF OF THE THEOREMS

Theorem 1. Let S be a fixed sample of size m. For a fixed   , define a matrix G = ( (z1, ), · · · ,  (zm, )), whose i-th column is the gradient of the i-th loss function w.r.t. . For SGD with batch size k, denote p as the random weight vector uniformly sampled from P(k).
Then the expected squared update amount can be calculated by:

E
p

RS,p()

2 2

=

RS,p0 ()

2 2

+

m-k k(m - 1)

tr(G G) - m

RS,p0 ()

2 2

.

Proof.

E
p

RS,p()

2 2

=E
p

Gp

2 2

=E
p

G(p - p0)

2 2

+

Gp0

2 2

=

E(p
p

-

p0)

G

G(p - p0) +

Gp0

2 2

= Etr
p

(p - p0)

G

G(p - p0)

+

Gp0

2 2

= Etr
p

(p - p0)(p - p0)

G

G

+

Gp0

2 2

= tr

E(p - p0)(p - p0)
p

G

G

+

Gp0

2 2

E(p - p0)(p - p0)
p

= E pp
p

+ p0p0 - pp0 - p0p

= E pp
p

- p0p0

One can easily calculate that

E pp
p

1

mk

 k-1

=

   

m(m-1)k
...

k-1

m(m-1)k

k-1 m(m-1)k
1 mk
···

···
···
...
k-1 m(m-1)k

k-1 

m(m-1)k

k-1 

m(m-1)k
...

   

,

1

mk

when

p

is

sampled

from

P (k),

namely

k

entries

of

p

are

randomly

set

to

1 k

,

while

other

entries

are

0. And

1
m2

1 m2

···

1
m2

1

p0p0

=

   

m2
...

1 m2

··· ...

1

m2
...

   

,

1 m2

···

1 m2

1 m2

11

Under review as a conference paper at ICLR 2019

Then

tr E(p - p0)(p - p0) G G
p

=

1-1 mk m2

tr(G G) +

k-1 - 1 m(m - 1)k m2

=

1 mk

-

1 m2

tr(G

G) - 1

G G1 m

+

k-1 m(m - 1)k

-

1 m2

1 G G1 - tr(G G) m

m-k = k(m - 1)

tr(G m

G)

-

p0

G

Gp0

1 G G1 - tr(G G)

Theorem 2. Assume the loss function is L-Lipschitz. Suppose the SGD algorithm with constant

learning rate  converges to and stays in B, where B = { :  - 0 2  B} is a ball that contains

a minimum of ERM loss RS,p0 . Suppose that p  P(k), B contains a local minimum of RS,p and

RS,p is µ-strongly convex on B. Further assume that the minimum value of the batch average loss

RS,p

in

B

is

not

too

large,

i.e.,

min
B

RS,p()



min
B

RS,p0 ()

+

.

Then

the

robust

loss

RS,P (k) ()

can be bounded by:

2B2

RS,P (k) ()



min
 B

RS,p0 (

)+

µ2

+ 2LB + ,

  B.

Proof. We count the iteration after SGD converges to B and stays in B. We denote t the parameter at iteration t and p(t) the weight at iteration t.

Due to the µ-strongly convexity of RS,p(),p  P(k) we have:

1 2µ

RS,p(t) (t)

2 2



RS,p(t) (t)

-

min
 B

RS,p(t) (

)



RS,p(t) (t)

-

min
 B

RS,p0 (

)

-

.

Since

the

algorithm

stays

in

B,

the

norm

of

the

stochastic

gradient

can

be

bounded

by

2B 

.

Thus:

2B2

RS,p(t) (t)



min
 B

RS,p0 (

)+

µ2

+ .

Plus, the weighted average loss RS,p(t) is L-Lipschitz, so RS,p(t) () can be bounded by:

2B2

RS,p(t) () 

min
 B

RS,p0

(

)+

µ2

+ 2LB + ,



 B.

As SGD algorithm keeps stable in B, the above equality holds for all p(t)  P(k). Finally we have:

2B2

RS,P (k) ()



min
 B

RS,p0 (

)+

µ2

+ 2LB + ,



 B.

Definition 1 (Robust Rademacher Complexity). For any m  1, the robust Rademacher complexity of the parameter space  and the weight set P is defined as:

Rm(, P) = E RS(, P) .
S
Theorem 3. For any  > 0, with probability at least 1 - , the following holds for all   :

ERS ,P ()  RS,P () + 2RS(, P) + 3m P 
S

12 log .
2m 

12

Under review as a conference paper at ICLR 2019

Proof. For simplicity, let Rm,P () = E RS ,P () , where S is m examples i.i.d. drawn from the
S
underlying distribution. For a sample S = (z1, · · · , zm) of size m, denote

(S) = sup Rm,P () - RS,P () .


If we change zi to zi and get S = (z1, · · · , zi, · · · , zm), then

(S) - (S)  sup RS,P () - RS,P ()

 sup sup pi( (zi, ) - (zi, ))
 pP
 P .

Similarly we have |(S) - (S)|  P . Then by McDiarmid's inequality(McDiarmid, 1989),

for

any



>

0,

the

following

holds

with

probability

at

least

1

-

 2

:

12

(S)  E[(S)] + m P 
S

log . 2m 

Next we are to bound E[(S)]. By symmetrization, we have:
S

E[(S)] = E sup Rm,P () - RS,P ()
S S 

= E sup ERS ,P () - RS,P ()
S  S

 E sup RS ,P () - RS,P ()
S,S 

m

 E sup sup
S,S  pP

pi( (zi, ) - (zi, ))
i=0

m

= E sup sup
,S,S  pP

ipi( (zi, ) - (zi, ))
i=0

m

 E sup sup

ipi (zi, ) + E sup sup

,S,S  pP i=0

,S,S  pP

= 2Rm(, P).

m
-ipi (zi, )
i=0

This shows that

sup Rm,P - RS,P ()  2Rm(, P) + m P 


12 log
2m 

holds

with

probability

at

least

1

-

 2

.

To complete the proof, next we show that

Rm(, P)  RS(, P) + m P 

holds

with

probability

at

least

1

-

 2

.

12 log
2m 

If the sample S and S differs only at the j-th instance, i.e., zi = zi for all i = j, then

mm

RS (,

P)

-

RS (,

P)

=

E


sup


sup
pP

ipi (zi, )
i=0

- E sup sup
  pP

ipi (zi, )
i=0



E


sup


sup
pP

[j

pj

(

(zj, ) -

(zj, ))]

 P .

Similarly, we have |RS(, P) - RS(, P)|  P . Then the result holds immediately from McDiarmid's inequality.

13

Under review as a conference paper at ICLR 2019

Theorem 4. For any > 0, the empirical Rademacher complexity and the empirical robust Rademacher complexity can be respectively bounded by:

RS() 

2 log N ((S),

·

,) +

d(m) ,

mm

and

RS(, P)  (1 + RAD2(P))

2 log N ((S, P),

·

,) +

d(m) .

mm

Proof. First, we introduce a lemma.

Lemma 7 (Massart 2000). Let A be a finite set in Rm, with r = maxxA x 2, then the following

holds:

E


sup m ixi xA i=1 m

r 

2 log |A| .
m

Denote N as the -net of (S) where |N | = N ((S), · , ). Notice that (zi, )  [0, 1], then r  m. Applying Massart's lemma to N , we get:

E


sup m ixi xN i=1 m



2 log N ((S), · , ) .
m

For any x  (S), there exists a x  N , such that x - x  . Then

m ixi = m ixi + m i(xi - xi)

mm

m

i=1 i=1 i=1

 m ixi + d(m) . mm
i=1

Then

RS

()

=

E


E


E


sup m ixi x(S) i=1 m

sup m ixi + d(m)

x(S) i=1 m

m

sup m ixi + d(m)

x N i=1 m

m



2 log N ((S),

·

,) +

d(m) ,

mm

which completes the first part of the theorem. The second part is similar, but one should notice that in this case r = max x 2  m P 2 = m(1 + RAD2(P)).
x(S,P )

Theorem 5. Assume the loss function is L-Lipschitz w.r.t.  for all z. Consider the following hypothesis set:
c =  : ESRS,P ()  c, ||||2 < r .
For any > 0, let N (c, ) denote the covering number of c. Then for all 0 <  < 1, with probability  1 - , the localized empirical Rademacher complexity can be bounded as:

RS (c )



1 m

1+ 

4

mrad (conv(P ))

c+

P 0 P L+ P 

m log N (c, ) 2

.

14

Under review as a conference paper at ICLR 2019

Proof. Notice that (z, )  [0, 1] for all z and , then if we change a single sample zi of S = (z1, · · · , zi, · · · , zm) to form S = (z1, · · · , zi, · · · , zm), we have
RS,P () - RS ,P ()

= sup RS,p() - sup RS ,p()

pP

pP

 sup(RS,p() - RS ,p())
pP
= sup pi( (zi, ) - (zi, ))
pP
 ||P||.

By the same argument, we can get |RS,P () - RS ,P ()|  ||P||. By McDiarmid's inequality(McDiarmid, 1989), we have:

P RS,P ()  ERS,P () + t

 exp

-2t2 m||P ||2

.

Let N denotes the minimum -net of c, where |N | = N (c, ). Then we have P 0  N, RS,P (0)  c + t

 N (c, )P RS,P ()  c + t

 N (c, )P RS,P ()  ESRS,P () + t

 N (c, ) exp

-2t2 m||P ||2

.

In other words, with probability larger than 1 - N (c, ) exp

-2t2 m||P ||2

, we have

sup RS,P () < c + t.
N

By the definition of -net, for any   c, there exists a   N , such that || -  || < . Let LS denotes an upper bound of the Lipschitz constant of RS(), then we have

sup RS,P () < c + t + LS,
c

with probability larger than 1 - N (c, ) exp

-2t2 m||P ||2

.

(5)

Recall that assuming the loss function (z, ) is L-Lipschitz with respect to  for all z, then the Lipschitz constant of RS() can be bounded by P 0 P L.

For those samples S = (z1, · · · , zm) satisfying the universal bound (5), the empirical Rademacher complexity can be bounded as well. The idea of the proof involves the representation of  as a convex combination of several vectors in P.

RS(c) = E sup
 c

i (zi, ) m

= E


 · (S, )

sup

c

m

.

Recall that p0 =

1 ,··· , 1 mm

conv(P). We hope that

and by our assumption of the symmetry of P, we have p0 

 m

=

ap0

+

b(p1

-

p0),

15

Under review as a conference paper at ICLR 2019

where p1  conv(P). Summing up all the coordinates of both sides of the equation, we have:

i m

=

a1 · p0

+ b (1 · p1 - 1 · p0)

=

a.

Define ¯ = i , Then m

b(p1 - p0) =

1 - ¯ , · · · , m - ¯

mm

mm

.

(6)

To derive the optimal bound, take the L norm of both sides of (6), we get

|b| p1 - p0  = max

| 1 - ¯ |, · · · , | m - ¯ |

mm

mm



2 .

m

Notice that

p1 - p0

  rad(conv(P)), so |b| 

2 mrad(conv(P)) .

Summarizing our result, we have:



· m

(S, ) = [¯p0 + b(p1 - p0)] ·

(S, )

 |¯ - b||p0 · (S, )| + |b||p1 · (S, )|

 (|¯| + 2|b|)RS,P (c).

Then

RS

(c)



E


(|¯|

+

2|b|)

RS,P

(c)



E|¯|2


+

4 mrad (conv(P ))

= 1 1 + 

4

m mrad(conv(P))

RS,P (c) RS,P (c).

C THE DETAILS OF WSGD

We denote the vector v(q, r) = (v1, v2, . . . , vm):



vi

=

1 W

r Cmj -iCi|-B1t|-j-1 +

Cmj -iCi|-B1t|-j-1

q |Bt |>j 0

|Bt |>j q |Bt |

where W is the normalizer s.t.

m i=1

vi

=

1.

Then

the

stochastic

gradient

calculated

in

WSGD(q,

r)

is an unbiased estimation of the gradient of RS,P(v(q,r))() at differentiable point. Recall that (v)

is the set of all the vectors whose entries are permutations of v's and P(v) denote the convex hull

of (v).

16

