Under review as a conference paper at ICLR 2019
A SAMPLING WITH PROBABILITY MATCHING
Anonymous authors Paper under double-blind review
ABSTRACT
Probabilistic methods often need to draw samples from a nontrivial distribution. A sampling is a nice algorithm by building upon a top-down construction of a Gumbel process, where a large state space is divided into subsets and at each round A sampling selects a subset to process. However, the selection rule depends on a bound function, which can be intractable. Moreover, we show that such a selection criterion can be inefficient. This paper aims to improve A sampling by addressing these issues. To design a suitable selection rule, we apply Probability Matching, a widely used method for decision making, to A sampling. We provide insights into the relationship between A sampling and probability matching by analyzing a nontrivial special case in which the state space is partitioned into two subsets. We show that in this case probability matching is optimal within a constant gap. Furthermore, as directly applying probability matching to A sampling is time consuming, we design an approximate version based on Monte-Carlo estimators. We also present an efficient implementation by leveraging special properties of Gumbel distributions and well-designed balanced trees. Empirical results show that our method saves a significantly amount of computational resources on suboptimal regions compared with A sampling.
1 INTRODUCTION
Probabilistic methods provide an important family of tools in machine learning for modeling uncertainty of complex systems, performing probabilistic inference, revealing hidden factors (Ghahramani, 2015), and making decisions Kocsis & Szepesvári (2006). These methods usually involve a fundamental task of drawing samples from a nontrivial distribution.
There exists a lot of work approaching the sampling problems, including rejection sampling (Gilks & Wild, 1992), MCMC (Propp & Wilson, 1996), etc. Recently, sampling algorithms based on the Gumbel process have received increasing attentions (Malmberg, 2013; Hazan et al., 2013; Gane et al., 2014; Hazan & Jaakkola, 2012; Papandreou & Yuille, 2011; Tarlow et al., 2012; Kappes et al., 2015; Kim et al., 2016) since a Gumbel process can turn a sampling task to an optimization problem so that we can use optimization tools to finish the original sampling task. In this work, we focus on A sampling (Maddison et al., 2014) which is one of the most famous Gumbel process based sampling algorithm. The major advantage of A sampling is that it can be applied to large state spaces, e.g., a continuous sample space or a discrete space whose size is exponentially large. The reason is that A sampling divides the state space into disjoint subsets and takes each subset as a whole, so that it can avoid initializing a large number of states, which is often encountered by other Gumbel process based algorithms (Papandreou & Yuille, 2011). Furthermore, A sampling adaptively selects subsets to process and the performance of A sampling is highly dependent on the selection rule. However, how to select subsets to process is very challenging. In each round, A sampling processes the subset with maximum D(S) which is an upper bound of the maximum Gumbel value within a subset S (see Section 2 for more details of D(S)). But in general, it is difficult to compute D(S) since it is an instance of non-convex optimization. Another challenge is that even if we are able to compute D(S) efficiently, selecting a subset with the maximum D(S) may not be a good choice. This is because our target is to process subsets with larger Gumbel values, but D(S) only provides an upper bound. So it is possible that the Gumbel value of S is relatively small with high probability while D(S) is very large. In this case, A sampling will waste many computational resources on
1

Under review as a conference paper at ICLR 2019

suboptimal regions. We'll discuss more on how this inaccuracy of D(S) deteriorates the performance of A sampling by analyzing a counter example in Section 3.
To address the above challenges, we improve the subset selecting procedure of A sampling with probability matching (PM) which has been proven efficient in many settings of making decisions, including Bayesian bandits (Chapelle & Li, 2011), MDP (Osband & Van Roy, 2016), economic decisions (Vulkan, 2000), etc.
Contributions: Intuitively, PM randomly selects an option according to its probability of being the optimal, so that it won't select a suboptimal option for too many rounds. To provide more insights into the efficiency of applying PM to A sampling, we first analyze a simple but nontrivial special case in which the state space is partitioned into two subsets. As we'll present in Section 4.1, in this case, PM is optimal within a constant gap in terms of the stochastic regret (Guha & Munagala, 2014) which measures the number of selected rounds on suboptimal options. Furthermore, as directly applying PM to A sampling is time consuming, we design a novel approximate algorithm based on Monte-Carlo estimators. The approximate algorithm is computationally efficient since it utilizes special properties of Gumbel distributions and well-designed balanced trees. We empirically compare our method with popular baselines of A sampling and Metropolis-Hastings algorithm. Experiments show that our algorithm works well.

2 PRELIMINARIES

In this section, we present some preliminary knowledge of the Gumbel process and A sampling. Below, we first introduce basic definitions of probability distributions and Gumbel distributions.

Definition 1 (Probability distributions). In general, a distribution P on a state space  provided

its potential function, P

:

2



R, is a sigma-finite measure such that

P (S)

=

1 ZP

exp{P (S)},

where ZP = exp(P ()) is normalizing constant.

Definition 2 (Gumbel and Truncated Gumbel distributions (Malmberg, 2013)). Let c denote the
Euler constant. For convenience, define e(g) = exp(-g + ), F(g) = exp(- exp(-g + )) and f(g) = e(g)F(g). Then (1), G(): a Gumbel distribution with location  has PDF and CDF at state g: f+c(g), F+c(g). (2), T G(, b): a Truncated Gumbel distribution with location  and
truncated value b has PDF and CDF at state g < b: f+c(g)/F+c(b), F+c(g)/F+c(b).

2.1 GUMBEL PROCESS
Now we are ready to introduce the Gumbel process.
Definition 3 (Gumbel process (Malmberg, 2013)). Let P (S) be a sigma-finite measure on sample space , S   is a measurable subset. Let P (·) denote the potential function of P such that P (S) = log P (S) + log ZP . Then GP = {GP (S)|S  } is a Gumbel process induced from P , if:
· (marginal distributions) GP (S)  G(P (S)). · (independence of disjoint sets) GP (S)  GP (Sc). · (consistency constraints) for measurable S1, S2  , then GP (S1  S2) =
max(GP (S1), GP (S2)).
The Gumbel process is useful in sampling since arg maxx GP (x)  P (Malmberg, 2013). Therefore, we can draw a sample from P by constructing a Gumbel process for distribution P , and then finding the maximum one with some optimization techniques.
In the sequel, we will use P to denote the target distribution, and we call GP (S) the Gumbel value of subset S. According to (Malmberg, 2013), Defn. 3 is associated with a natural bottom-up construction: for any x  , we first perturb it with an independent Gumbel noise, i.e., g(x)  G(0). After that we simply set GP (x) = g(x) + P (dx) and compute GP (S) = maxxS GP (x) for all S   according to the consistency constraints. However, when  is infinite, such a bottom-up construction is infeasible.
Top-down construction: (Maddison et al., 2014) presents a top-down construction, which partitions the state space into regions and resolves the problem caused by infinite spaces by considering each region as a whole. Formally, the top-down procedure constructs a top-down tree, tree(P ), with each

2

Under review as a conference paper at ICLR 2019

node corresponding to a subset of . tree(P ) is rooted in . Let par(S) denote the parent of subset S. For each S  tree(P ), its children is a disjoint partition of S, that is, S :par(S )=SS = S.
The top-down construction computes Gumbel values for subsets in the order from the top to the bottom of tree(P ). Formally, according to the consistency constraints and marginal distributions, we compute GP (S)  T G(P (S), L(S)) where L(S) := GP (par(S)). In the algorithmic view of point, the top-down construction maintains a collection of subsets of . Initially, the collection contains only . At each round, the algorithm selects an element S from the collection and computes GP (S). After that it divides S into subsets S1, S2 and adds them into the collection.

2.2 A SAMPLING

Obviously, if P (S) is hard to compute, the top-down construction for P is computationally in-

tractable. (Maddison et al., 2014) solves this problem by utilizing the linearity of Gumbel distri-

bution. More specifically, given a distribution Q, if GQ(x) induces a Gumbel process of Q, then GP (x) := GQ(x) + P (x) - Q(x) induces a Gumbel process of distribution P . Based on this insight, (Maddison et al., 2014) proposes the A sampling, which relies on a tractable proposal

distribution Q. Furthermore, since GQ(S)  arg maxxS sampling executes the top-down construction for Q, and for

GQ(x) (Maddison et al., 2014), A each subset, A sampling computes

G~P (S) = GQ(S) + P (xQ(S)) - Q(xQ(S)) where xQ(S)  Q(·|S).1 Suppose at some time point that A sampling has processed n nodes in tree(Q), denoted by DoneQ(n). It can be shown

that there are n + 1 nodes in the to be processed collection, denoted by CollectQ(n). As introduced above, for each A  DoneQ(n), we have a pair (xQ(A), G~P (A)), and each S  CollectQ(n) is associated with a truncated Gumbel distribution T G(Q(S), L(S)).

The subset selection and termination in A sampling rely on a bound function B : 2  R such that B(S)  supxS P (x) - Q(x). Let D(S) := L(S) + B(S). If for some n, maxSDone(n) G~P (S)  maxSCollect(n) D(S), A terminates and outputs the element with maximum value among the processed nodes. At round n, A sampling selects the node S with maximum
value of D(S) from Collect(n).

3 CHALLENGES OF A SAMPLING
There are two challenges in A sampling. The first one is about the function D on which A sampling relies. Computing this function for every S can be intractable since it can be a non-convex optimization. If we simply remove the (possibly intractable) bound function or use a very loose bound, A sampling will degenerate to an algorithm which is not efficiency (Maddison et al., 2014). We name the degenerated algorithm as A sampling without a bound ( See Appendix D for details.).
The second challenge is that selecting the subset with maximum D(S) is not always a good choice. This is because D(S) is just an upper bound of GP (S) and it is possible that GP (S) is relatively small with high probability while D(S) is very large. We now present a simple counter example for A sampling to intuitively explain the reason. In this example,  = (-10.0, +10.0), the target is a mixture distribution: P (x)  (1.0 - 10-5)N (x; -5.0, 1.0) + 1[|x|  0.5  10-405]10400 and Q(x)  N (x; 5.0, 1.0). The log likelihoods of P and Q are shown in Fig. 1(a). We first empirically evaluate A sampling on this example. Fig. 1(b) shows the selected rounds on the optimal subsets and Fig. 1(c) shows the maximum Gumbel value found by A sampling. Results are averaged over 100 runs. We can see that A sampling has a poor performance on this example. In this case, D(S) is large if S covers points near x = 0. So A sampling will allocate lots of computational resources into such intervals, however, GP (S) being high for such S is with probability only about 0.00001.
4 A SAMPLING WITH PROBABILITY MATCHING
We now present how to use PM to improve A sampling by addressing the above challenges. We first present an intuitive example in Section 4.1. Then, we present a practical PM algorithm based on Monte-Carlo estimators of GP (S) in Section 4.2 and an efficient implementation with well-designed balanced trees in Section 4.3.
1In this paper, we use P (·|S) to denote the distribution P conditioned on state space S.

3

Under review as a conference paper at ICLR 2019

(a) Log-likelihood

(b) Selected rounds on intervals containing the optimal point

(c) Maximum Gumbel value

Figure 1: The counter example with (a) the log likelihood of P and Q; (b) the selected number on the intervals containing the optimal point; (c) the maximum Gumbel value found by A sampling.

4.1 PROBABILITY MATCHING AND AN EXAMPLE WITH TWO SUBSETS

In general, when making a choice among a set

of options, PM selects an option randomly ac-

cording to its probability of being the optimal.
More specifically, in our problem, the optimal
option is the subset with the maximum Gumbel
value. Formally, by definition, the maximum
Gumbel value within region S is a random variable GP (S) = maxxS T G(Q(dx), L(S)) + P (x) - Q(x). Suppose the state space  is partitioned into {S1, · · · , SK }. PM selects a subset according to the probability:

Algorithm 1 Probability matching with MonteCarlo estimators.
1: Input: the target distribution P , proposal distribution Q, state space , time horizon T .
2: Output: x: a sample from P . 3: maxgumbel = -, x = N one. 4: Collect = {}, L() = , S  , t = 1. 5: while t  T do 6: t = t + 1.

7: Select S according to Eq. (4).

p i = arg max GP (Sk) .
k[K ]

(1) 8: Split S into disjoint sets S1, S2. 9: L(S1) = L(S2) = GQ(S).

Intuitively, PM has an excellent performance 10: for S  S1, S2 do

since it allocates computational resources into 11: x(S)  Q(·|S).

the options which are likely to have large out- 12: comes. To provide more intuition into why PM 13: suits A sampling, we analyze a simple but non-

G(S)  T G(Q(S), L(S)). G~(S) = G(S) + P (x(S)) - Q(x(S)).

trivial case in which we divide  into two sets. 14:

if maxgumbel < G~(S) then

In order to get a clean theoretical result, we 15:

maxgumbel = G~m(S), x = x(S).

additionally assume A sampling does not fur- 16:

end if

ther split a subset after processing it. We fo- 17: end for

cus on the stochastic regret (Guha & Muna- 18: Compute Monte-Carlo estimators for S1, S2

gala, 2014) which is the expected number of

and update balanced trees.

selections on the suboptimal subset. Formally, 19: Collect.insert(S1), Collect.insert(S2).

suppose  is partitioned into S1, S2. Let i = 20: end while

arg maxi{1,2} GP (Si) which is a random variable. Consider an algorithm A which selects a subset

SiA,t at time step t. The stochastic regret of A at time T is: RA(T ) = E

T t=1

1[iA,t

=

i]

.

Intuitively, the smaller RA is, the better A is, since A won't waste many computational resources on the suboptimal subset. Moreover, we can prove that PM is optimal within a constant gap in terms of the stochastic regret:
Lemma 1. Let opt(T ) denote the algorithm which minimizes the stochastic regret. Then :

RP M (T )  2Ropt(T )(T ), T

where RP M (T ) is the stochastic regret of PM.

The proof of Lemma 1 is adapted from the proof in (Guha & Munagala, 2014) for Bayesian bandits with two arms, we defer the details in Appendix A.

4

Under review as a conference paper at ICLR 2019

4.2 PROBABILITY MATCHING WITH A MONTE-CARLO ESTIMATOR
Unfortunately, drawing samples from the probability in Eq. (1) is intractable when GP (S) is complex. So in this section, we present an efficient PM algorithm based on a Monte-Carlo estimator of GP (S).
Consider a random variable Y = P (x) - Q(x), x  Q(·|S) whose expectation is a constant plussing the KL-divergence between Q and P conditioned on the subset S. We can equally characterize GP (S) as

max T G(log(Q(S) · p(Y = y)), L(S)) + y.
y

(2)

We present the proof of Eq. (2) in Appendix B. Eq. (2) suggests that we can get a Monte-Carlo

estimator of GP (S) by estimating Y . More specifically, let Y1, · · variables and w1, · · · , wm be the corresponding weights such that

·

,

Ymm
i=1

be wi

a sequence of random = 1, wi > 0. Suppose

the random variable Ym : p(Ym = Yi) = wi is an unbiased estimator of Y , then we can estimate

GP (S) by:

G^P (S) = max T G(log(wiQ(S)), L(S)) + Yi = max T G(log(wiQ(S)) + Yi, L(S) + Yi) (3)

i[m]

i[m]

The second equality holds due to the linearity of the truncated Gumbel distribution (Maddison et al., 2014). According to Eq. (3), we can estimate GP (S) with existing Monte-Carlo estimators of Y , such as adaptive importance sampling (Gilks & Wild, 1992).
The corresponding PM with Monte-Carlo estimators is to draw samples from

p ^i = arg max G^P (Sj) .
j[n]

(4)

What remains is how to sample from the probability in Eq. (4) efficiently. The most popular execution of Eq. (4) is as in (Chapelle & Li, 2011): we draw yi  G^P (Si), and take ^i = arg maxi yi, then it can be shown that ^i is a sample from the probability in Eq. (4).
However, a direct implementation of the above ideas requires time complexity O(m) since we need to draw samples from m truncated Gumbel distributions, where m = i[n] mi is the number of particles in total and mi is the number of particles in Si. So our selection algorithm executing m rounds would require running time O(m2). It is relatively slow comparing with the O(m log m) time complexity for A sampling (Maddison et al., 2014).

4.3 AN EFFICIENT IMPLEMENTATION BY BALANCED TREES
We now present a novel algorithm that only requires O(log m) running time to sample from the distribution in Eq. (4). Our algorithm is based on the properties of the truncated Gumbel distribution and under the help of well-designed balanced trees.
We first decompose sampling from the distribution in Eq. (4) into two steps which can be done efficiently. The decomposition is an immediate inference of Eq. (4):

p ^i = arg max G^P (Sj) = p(x = max G^P (S))p(^i = arg max G^P (S)|x = max G^P (S))dx.

j[n]

x j[n]

j[n]

j[n]

Thus, sampling from the distribution in Eq. (4) equals to the following two sampling problems:

x  max xi, xi  G^P (Si), ^i  p(i = arg max xj  G^P (Si)|x = max xj)

i[n]

j[n]

Recall that G^P (S) is the maximum one among a set of truncated Gumbels. Thus, the above two sampling problems are essentially sampling the maxima and the argument of the maxima among a
set of truncated Gumbels. So our target can be converted into the following problem:

5

Under review as a conference paper at ICLR 2019

Problem 1. Given a set of truncated Gumbel variables {vi}mi=1 with parameters (ai, bi), i.e., vi  T G(ai, bi). We define two sampling problems:

v = maxi[m] vi

(5)

^i  p(i = arg maxj[m] vj |v = maxj[m] vj )

(6)

We use the inverse transform sampling (Devroye, 1986) to sample v in Eq. (5). In inverse transform sampling, for a random variable X with CDF UX (x), we first draw a sample s  unif orm(0, 1), and then compute x such that UX (x) = s, it can be shown that x  X. Thus, let U (g) denote the CDF of v, we only need an algorithm to compute g such that U (g) = s, s  (0, 1). We now show
how to compute such g efficiently with balanced trees.

For notational clarity, let Ua,b(g) denote the CDF of a truncated Gumbel distribution, T G(a, b).

According

to

Defn.

2,

we

have

Ua,b(g)

=

exp(- exp(- min(g,b)+a)) exp(- exp(-b+a))

.

Recall

v

=

maxi

vi,

then

v

has

CDF: U (g) =

i Uai,bi (g) =

i

exp(- exp(- min(g,bi)+ai exp(- exp(-bi+ai))

))

.

Take

logarithm

on

both

sides,

we

get

log U (g) = i[m] (- exp(- min(g, bi) + ai) + exp(-bi + ai)).

Without loss of generality, we sort bi's in a non-decreasing order, that is, bi  bi+1. Since U (g) is a monotonically increasing function, for g  (bi, bi+1], we have:

log U (g) = - exp(-g + aj) + exp(-bj + aj) = - exp(-g) exp(aj) + exp(-bj + aj)

j>i j>i

j>i j>i

Thus, given U (g) and suppose g  (bi, bi+1], we can compute g by:

g = - log j>i exp(aj - bj) - log U (g) j>i exp(aj )

(7)

Thus, when we get s  unif orm(0, 1), we need to find i such that U (bi)  s  U (bi+1), and then solve g according to Eq. (7) and inverse sampling . Both of above two steps can be done efficiently

via a balanced tree.

Suppose we have a balanced tree such that each node in the tree corresponds to an index i  [m],
and the key of the balanced tree is bi, that is, for all j in the right subtree of node i, we have bj  bi and for all j in the left subtree, we have bj  bi. Suppose that from the balanced tree, we can query in O(1) time at each node i for terms: (1) exp(-bi) j>i exp(aj); (2) exp(ai) j>i exp(-bj) and (3) j>i exp(aj - bj). We can query these terms efficiently in a balanced tree because they all are summations over an interval. And according to Defn. 2, we know that
log U (bi) = j>i (exp(-bj + ai) - exp(aj - bi)), we can check out whether log s < log U (bi) in O(1). Therefore, we can find the index i such that U (bi)  s  U (bi+1) in running time O(log m). After that, we can compute U (g) = s via Eq. (7) in running time O(1).

Now we turn to sample ^i in Eq. (6). Without loss of generality, suppose g  (bi, bi+1). Obviously, for
j < i, p(j = arg maxj vj |g = maxj vj ) = 0. For j  i, by Defn. 2 and with simple calculations, we have:

p

j = arg max vj |g
j

 dUaj,bj (g) dg

Uaj ,bj (g)

j =j,j >i

= exp(-g

+

aj

)

exp(- exp(-g exp(- exp(-bj

+ +

aj )) aj ))

j

=j,j

Uaj
>i

,bj

(g)

=

exp(aj) exp(-g)
j

Uaj
>i

,bj

(g)



exp(aj )

(8)

According to Eq. (8), we can sample ^i in O(log m) running time with a balanced tree from which we
can query j>i exp(aj) efficiently. Putting the previous results together, we get the algorithm as outlined in Alg. 1.

5 EXPERIMENTS
In this section, we present our empirical results. We first check the correctness on a simple toy experiment and the counter example in Section 3. After that, we evaluate the efficiency of Alg. 1 on two Bayesian posterior inference tasks, results show that our algorithm outperforms vanilla A sampling significantly.

6

Under review as a conference paper at ICLR 2019

(a) The toy experiment

(b) The counter example

Figure 2: Experiment results with (a) the toy experiment; (b) the counter example.

Figure 3: Experiment results on the clutter problem with 5 dimensions on the left, 15 dimensions in the middle and 20 dimensions on the right.
5.1 CORRECTNESS OF ALG. 1
We first verify the correctness of Alg. 1 on a greenhouse experiment. We consider sampling from a one-dimensional Gaussian mixture with potential function P (x) = - log(N (x; -2.0, 1.0) + 2N (x; 2.0, 1.0)), which is a multi-mode target distribution. We set Q = N (0, 2). We present our result in Fig. 2(a) which shows the ground truth and the empirical sample distributions of Alg. 1 and baselines. From Fig. 2(a), Alg. 1 has a similar performance to A sampling and outperforms the A sampling without a bound function.
5.2 THE COUNTER-EXAMPLE We empirically compare the performance of algorithms on the example in Section 3. The result is shown in Fig. 2(b). We can see that PM-A outperforms baselines significantly since our algorithm wastes less resources on suboptimal subsets. 5.3 BAYESIAN POSTERIOR INFERENCE In this section, we evaluate our algorithm on two Bayesian posterior tasks: the clutter problem and the Bayesian logistic regression. More specifically, we focus on sampling tasks of formulations P (x)  pxin(,xba)ontdh{inAy=i1}spina=(my1ipa|xlrie)ngowbahsneedrrevAaxtligio.sn1tsh.,eFanovdrarvpiaa(nbyilile|lxaw)Aeisatrsheaemglopikilnienglgiht,oowosedamefxuppnlelco,tiipot (nth·.)eWissteathnseidmaprprdiloysrtsodecitshQtarsi(btxiuc)tig:o=rnadpoiv(exnert) descent (SGD) algorithm to calculate the bound function, maxxS i log p(yi|x). For the MonteCarlo estimator in Alg. 1, we exploit the importance sampling over the trajectory of the same SGD algorithm as in the vanilla A sampling 2.
5.3.1 EVALUATION ON THE CLUTTER PROBLEM
We now evaluate Alg. 1 on the Clutter problem proposed by (Minka, 2001). Clutter problem aims to inference the mean of an isotropic Gaussian with some data points are outliers. Consider a mixture
2We first tune the parameters of SGD for vanilla A sampling, and then apply them to Alg. 1 without further tuning.
7

Under review as a conference paper at ICLR 2019

(a) The toy experiment

(b) The counter example

Figure 4: Experiments on logistic regression with (a) averaged Gumbel values; (b) averaged loglikelihoods.

distribution:p(y|x) = (1 - w)N (y; x, I) + wN (y; 0, 1I), p(x) = N (x; 0, 2I), where w is the
ratio of outliers which is a known parameter, and N (µ, ) represents Gaussian distribution. Our goal is to inference x given data {yi}ni=1. We do experiments on dimensions varying in 5, 15, 20, n = 20. We compare the Gumbel of these algorithms. We run 100 times and present the averaged results in Fig. 3. We can see that Alg. 1 outperforms A sampling constantly.

5.3.2 EVALUATION ON BAYESIAN LOGISTIC REGRESSION

Our last experiment is on Bayesian Logistic Regression. Given a dataset {xi}ni=1 associated with

label {yi}in=1 the Bayesian

where yi  {0, 1}. We logistic regression: p()

follow the setting in = Gamma(; a, b),

(Gershman et al., 2012) and define p(wk) = N (wk; 0, -1), p(yi =

1; xi, w) = sigmoid(wT xi). In this model, {w, } are the hidden variables, where w denotes the

regression coefficients and  is a precision parameter. We set a = b = 1. We do experiments on 13

binary classification datasets proposed by (Mika et al., 1999). The number of features of these data

sets are in range from 2 to 60, and the number of points ranges from 24 to 7400 (See Appendix C for

more statistics). We present our results in Fig. 4(a) where all results are averaged over 100 runs. Fig.

4(a) presents the summation of the maximum likelihood found by each algorithm on these datasets over time. From Fig. 4(a), we can see that PM-A outperforms all baselines.

Furthermore, we compare our algorithm with standard Matropolis-Hastings algorithm (MH) and adaptive inference with exploration (AIE) (Rainforth et al., 2018) which also attempts to bridge the gap between sampling problems and decision-making techniques. For MH, the initial points are sampled from the prior. To make the comparison fair, we also evaluate Alg. 1 and AIE with the prior as the Monte-Carlo estimator instead of gradient-based methods. We compare the likelihoods in Fig. 4(b). We can see that Alg. 1 outperforms AIE even if they use the same Monte-Carlo estimator. This is AIE attempts to use UCB-like algorithm to make decisions, but UCB works only for those models in which concentration bounds hold which is not always valid in sampling problems.

6 CONCLUSION AND FUTURE WORK
In this work, we focus on improving the subset selection procedure in A sampling with PM. We proved that in the special case of two subsets, PM is optimal within a constant gap in terms of the stochastic regret. Moreover, we proposed a practical algorithm based on Monte-Carlo estimators and well-designed balanced trees. Empirical results show that our methods saves a significantly amount of computational resources on suboptimal regions compared with A sampling.
There exists several challenges in future work. The first one is on the analysis of PM. Though we proved PM is efficient in the case of two subsets, it is very challenging to prove the efficiency in general. The second one is that the performance of Alg. 1 relies on the accuracy of the Monte-Carlo estimator. However, it is time-consuming to compute an accurate Monte-Carlo estimator. So it is important to balance the accuracy of the Monte-Carlo estimator and the performance of PM. We hope our work is a starting point to address these problems.
8

Under review as a conference paper at ICLR 2019
ACKNOWLEDGMENTS
REFERENCES
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pp. 2249­2257, 2011.
Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pp. 260­265. ACM, 1986.
Andreea Gane, Tamir Hazan, and Tommi Jaakkola. Learning with maximum a-posteriori perturbation models. In Artificial Intelligence and Statistics, pp. 247­256, 2014.
Samuel Gershman, Matt Hoffman, and David Blei. Nonparametric variational inference. arXiv preprint arXiv:1206.4665, 2012.
Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553): 452, 2015.
Walter R Gilks and Pascal Wild. Adaptive rejection sampling for gibbs sampling. Applied Statistics, pp. 337­348, 1992.
Sudipto Guha and Kamesh Munagala. Stochastic regret minimization via thompson sampling. In COLT, pp. 317­338, 2014.
Tamir Hazan and Tommi Jaakkola. On the partition function and random maximum a-posteriori perturbations. arXiv preprint arXiv:1206.6410, 2012.
Tamir Hazan, Subhransu Maji, and Tommi Jaakkola. On sampling from the gibbs distribution with random maximum a-posteriori perturbations. In Advances in Neural Information Processing Systems, pp. 1268­1276, 2013.
Jörg Hendrik Kappes, Paul Swoboda, Bogdan Savchynskyy, Tamir Hazan, and Christoph Schnörr. Probabilistic correlation clustering and image partitioning using perturbed multicuts. In International Conference on Scale Space and Variational Methods in Computer Vision, pp. 231­242. Springer, 2015.
Carolyn Kim, Ashish Sabharwal, and Stefano Ermon. Exact sampling with integer linear programs and random perturbations. In AAAI, pp. 3248­3254, 2016.
Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In ECML, volume 6, pp. 282­293. Springer, 2006.
Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sampling. In Advances in Neural Information Processing Systems, pp. 3086­3094, 2014.
Hannes Malmberg. Random Choice over a Continuous Set of Options. PhD thesis, Department of Mathematics, Stockholm University, 2013.
Sebastian Mika, Gunnar Ratsch, Jason Weston, Bernhard Scholkopf, and Klaus-Robert Mullers. Fisher discriminant analysis with kernels. In Neural Networks for Signal Processing IX, 1999. Proceedings of the 1999 IEEE Signal Processing Society Workshop., pp. 41­48. IEEE, 1999.
Thomas P Minka. Expectation propagation for approximate bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pp. 362­369. Morgan Kaufmann Publishers Inc., 2001.
Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning. arXiv preprint arXiv:1607.00215, 2016.
George Papandreou and Alan L Yuille. Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models. In Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 193­200. IEEE, 2011.
9

Under review as a conference paper at ICLR 2019 James Gary Propp and David Bruce Wilson. Exact sampling with coupled markov chains and
applications to statistical mechanics. Random structures and Algorithms, 9(1-2):223­252, 1996. Tom Rainforth, Yuan Zhou, Xiaoyu Lu, Yee Whye Teh, Frank Wood, Hongseok Yang, and Jan-
Willem van de Meent. Inference trees: Adaptive inference with exploration. arXiv preprint arXiv:1806.09550, 2018. Daniel Tarlow, Ryan Adams, and Richard Zemel. Randomized optimum models for structured prediction. In Artificial Intelligence and Statistics, pp. 1221­1229, 2012. Nir Vulkan. An economist's perspective on probability matching. Journal of economic surveys, 14 (1):101­118, 2000.
10

Under review as a conference paper at ICLR 2019

A PROBABILITY MATCHING FOR TWO SUBSETS
We present the proof of Lemma 1 in this section. This proof is adapted from the proof of Thompson sampling for Bayesian bandits with two arms in (Guha & Munagala, 2014).
We first make the simplified A sampling for two subsets clearer. Recall  is divided into two subsets S1, S2. For notational convenience, let kt,i denote the number of selected rounds of Si up to time step t. Since we do not split S1, S2, if we have processed Si for t rounds, we will have t observations {(xi,j , ui,j )}jt=1 such that xi,1  Q(·|Si), ui,1  G(Q(Si)) and xi,j  Q(·|Si\Xi,j-1), ui,j  T G(Q(Si\Xi,j-1), ui,j-1) for j > 1 where Xi,j = {xi,j }jj =1. Let Vi,t = {(xi,j , ui,j )}kj=t,i1 denote the observations from subset Si until time t and Vt = {V1,t, V2,t} denote the collection of observations from both subsets. Obviously, a subset selection algorithm A selects a subset according to previous observations, that is, we can represent the subset selected by A at time t, iA,t+1, as iA,t+1 := iA(Vt). Now we present A sampling for two subsets with A as the subset selection algorithm in Alg. 2.
Algorithm 2 A sampling for two subsets. 1: Input: the target distribution P , proposal distribution Q, state space  = S1  S2, a subset selection algorithm A, time horizon T . 2: Output: x: a sample from P . 3: maxgumbel = -, x = N one. 4: u1,0 = u2,0 = , t = 0. 5: while t  T do 6: t = t + 1. 7: i = iA(Vt-1). 8: x  Q(·|Si\Xi,kt-1,i ). 9: ui,t  T G(Q(Si\Xi,kt-1,i ), ui,t-1). 10: u = ui,t + P (x(S)) - Q(x(S)). 11: if maxgumbel < u then 12: maxgumbel = u , x = x . 13: end if 14: end while
Recall opt(T ) denote the the algorithm which minimizes the stochastic regret up to time horizon T . We now explicitly define the optimal stochastic regret Ropt(T )(T ) with time horizon T via dynamic programming. Let qi(V ) denote the probability of Si being the optimal subset given observations V . For convenience, let RA(T |V ) be the stochastic regret of algorithm A given observations V . Then:
Definition 4. We can define Ropt(T )(T |V ) as follows:
Ropt(1)(1|V ) = min (1 - qi(V )).
i{1,2}

T > 1, Ropt(T )(T |V ) = min 1 - qi(V ) +

Ropt(T -1)(T - 1|V  (x, u))p(x, u|V )

i{1,2}

xi ,ui

where (xi, ui) is the new observation from subset Si.

We now present the formal definition and some properties of qi(V ) which are useful in the sequel:
Corollary 1. Let V = {V1 := {(x1,j, u1,j)}kj=1 1, V2 := {(x2,j, u2,j)}jk=2 1} denote a set of observations. Let Si,\V = Si\  {xi,j}kj=i 1. Then let i = {1, 2}\i and m(x) = P (x) - Q(x), we have:

qi(V ) = p

max
xSi,\V

T

G(Q(dx), ui,ki ) + m(x)

>

max
xSi ,\V

T

G(Q(dx), ui

,ki

)

+

m(x)

Suppose we process Si for the ki + 1 time, and then receive observation (xi, ui), we have: qi(V )  Exi,ui [qi(V  (xi, ui))]

(9)

11

Under review as a conference paper at ICLR 2019

Proof. Let E denote the event of the (ki + 1)-th observation from subset i is with the maximum

Gumbel value within Si, have qi(V  (xi, ui)|E¯) =

that is, ui + m(xi) qi(V |E¯); otherwise,

 qi

maxSi,\(V xi) T G(Q(dx), will decrease, that is, qi(V 

ui) + m(x). Then (xi, ui)|E)  qi(V

we |E ).

Overall, we have

Exi,ui [qi(V  (xi, ui))] = p(E)qi(V  (xi, ui)|E) + p(E¯)qi(V  (xi, ui)|E¯  qi(V )

For convenience, let RA(T |V ) = i p(iA,1 = i)RA(i, T |V ) where RA(i, T |V ) denote the stochastic regret of A if we select subset i at the first round. The proof of Lemma 1 relies on the following lemma:

Lemma 2. Let P denote the target distribution and Q denote the proposal. If an algorithm A satisfies that for any observations V and time horizon T :

RA(T |V )  RA(i, T |V ) + c(1 - qi(V ))

(10)

where c is a constant. Then we have RA(T |V )  (c + 1)Ropt(T )(T |V ) for all T and V .

Proof. For T = 1, it is obvious that Eq. (10) holds. We use mathematical induction to prove the case of T > 1. According to definition and with straight-forward calculations, we have:

RA(T |V )  RA(i, T |V ) + c(1 - qi(V )),

i

= (c + 1)(1 - qi(V )) + E(x,u)RA(i, T - 1|V  (x, u)),

i

 (c + 1)(1 - qi(V ) + E(x,u)Ropt(T -1)(i, T - 1|V  (x, u))), i



(c +

1) min(1 - qi(V )
i

+ E(x,u)Ropt(T -1)(i, T

-

1|V



(x, u)))

= (c + 1)Ropt(T )(i, T |V )

According to Lemma 2, we can prove Lemma 1 by proving that probability matching satisfies precondition (10) for c = 1. The following Lemma provides an equivalent characterization of Eq. (10) in the context of probability matching. Lemma 3. For probability matching, we have:
RP M (T )  RP M (i, T ) + 1 - qi, i  {1, 2}  |RP M (1, T ) - RP M (2, T )|  1 (11)
Proof. Suppose RP M (T )  RP M (1, T ) + 1 - q1. By definition, we have RP M (T ) = q1RP M (1, T ) + q2RP M (2, T ). Observing q1 + q2 = 1, we have RP M (2, T ) - RP M (1, T )  1. Conversely, if RP M (1, T )  RP M (2, T )+1, we have RP M (T ) = q1RP M (1, T )+q2RP M (2, T )  RP M (2, T ) + 1 - q2. Swapping the roles of 1 and 2, we complete the proof.
Now, we can complete the proof of Lemma 1.
Proof. For T = 1, it is obvious that Lemma 3 is true. We use mathematical induction to prove Lemma 3 holds for T > 1. For convenience, let u1  T G(Q(S1), L(S1)), x1  Q(·|S1), u2  T G(Q(S2), L(S2)), x2  Q(·|S2). We have RP M (1, T |V ) = 1 - q1(V ) + Ex1,u1 [RP M (T - 1|V  (x1, u1))]
 1 - q1(V ) + Ex1,u1 [1 - q2(V  (x1, u1)) + RP M (2, T - 1|V  (x1, u1))] = 1 - q1(V ) + Ex1,u1 [2q1(V  (x1, u1)) + Ex2,u2 RP M (T - 2|V  {(x1, u1), (x2, u2)})]  1 + q1(V ) + Ex1,u1 [Ex2,u2 RP M (T - 2|V  {(x1, u1), (x2, u2)})] The first equation is according to the inductive hypothesis, the next equality follows q1(V ) + q2(V ) = 1 and the last inequality is due to Corollary 1. Similarly, we have: RP M (2, T |V ) = 1 - q2(V ) + Ex2,u2 [RP M (T - 1|V  (x2, u2))]
= q1(V ) + Ex2,u2 [q1(V  (x2, u2))RP M (1, T - 1|V  (x2, u2)) + q2(V  (x2, u2))RP M (2, T - 1|V  (x2, u2))]  q1(V ) + Ex2,u2 [RP M (1, T - 1|V  (x2, u2)) - q2(V  (x2, u2))] = q1(V ) + Ex2,u2 [Ex1,u1 RP M (T - 2|(x1, u1), (x2, u2))]
12

Under review as a conference paper at ICLR 2019

The first inequality follows the inductive hypothesis in Lemma 3 and q1(V ) + q2(V ) = 1. Above inequalities show that RP M (1, T ) - RP M (2, T )  1. Reversing the roles of S1 and S2, we can show that probability matching satisfies the condition in Lemma 3 which completes the proof.
B THE EQUIVALENT CHARACTERIZATION OF GP (S)
We now prove the equivalent characterization of GP (S) in Section 4.2. According to definitions, we have:

GP (S) = max T G(Q(dx), L(S)) + P (dx) - Q(x)
xS
= max max T G(Q(dx), L(S)) + y
y P (x)-Q(x)=y
= max (y + T G(log(Q(S)p(Y = y)), L(S)))
y
the first equation is by definition and the third equation follows the fact that maxxS T G(Q(dx), L) = T G(log( xS exp(Q(x))dx), L).

C STATISTICS OF DATASETS PROPOSED BY (MIKA ET AL., 1999)
We present statistics of datasets proposed by (Mika et al., 1999) in Table 1 where n denotes the number of data points and dim denotes dimension.

Banana B.Cancer Diabetes German Heart

Image Ringnorm

dim 2

9

8 20

5

18

20

n 5300

263

768 1000

215

2086

7400

F.Sonar Splice Thyroid Titanic Twonorm Waveform

dim 9

60

5

3 20

21

n 144

2991

215

24

7400

5000

Table 1: Some statistics of datasets proposed by (Mika et al., 1999).

D A SAMPLING WITHOUT THE BOUND FUNCTION
As presented in Section 3, when the bound B(S) is not available or very loose, the vanilla A sampling degenerates to Alg .3 which is not very efficient since it selects subsets simply according to L(S).
13

Under review as a conference paper at ICLR 2019
Algorithm 3 A sampling without a bound. 1: Input: the target distribution P , proposal distribution Q, state space , time horizon T . 2: Output: x: a sample from P . 3: maxgumbel = -, x = N one. 4: Collect = {}, L() = , S  , t = 1. 5: while t  T do 6: t = t + 1. 7: Select S  Collect with maximum L(). 8: Split S into disjoint sets S1, S2. 9: L(S1) = L(S2) = GQ(S). 10: for S  S1, S2 do 11: x(S)  Q(·|S). 12: G(S)  T G(Q(S), L(S)). 13: G~(S) = G(S) + P (x(S)) - Q(x(S)). 14: if maxgumbel < G~(S) then 15: maxgumbel = G~m(S), x = x(S). 16: end if 17: end for 18: Collect.insert(S1), Collect.insert(S2). 19: end while
14

