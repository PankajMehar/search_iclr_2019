Under review as a conference paper at ICLR 2019
LEARNING TO DESIGN RNA
Anonymous authors Paper under double-blind review
ABSTRACT
Designing RNA molecules has garnered recent interest in medicine, synthetic biology, biotechnology and bioinformatics since many functional RNA molecules were shown to be involved in regulatory processes for transcription, epigenetics and translation. Since an RNA's function depends on its structural properties, the RNA Design problem is to find an RNA molecule that folds into a specified secondary structure. Here, we propose a new algorithm for the RNA Design problem, dubbed LEARNA. LEARNA uses deep reinforcement learning to train a policy network to sequentially design an entire RNA sequence given a specified secondary target structure. By meta-learning across thousands of different RNA target structures, our extension Meta-LEARNA constructs an RNA design policy that can be applied out of the box to solve novel RNA target structures. Comprehensive empirical results on two widely-used RNA secondary structure design benchmarks, as well as a third one that we introduce, show that our approach achieves new state-of-theart performance on all benchmarks while also being up to 450× faster than any previous approach. We achieve this by a joint optimization of the policy network's architecture, the training hyperparameters, and the state space representation. In an ablation study, we analyze our method's different components.
1 INTRODUCTION
RNA is one of the major classes of information-carrying biopolymers in the cells of living organisms. Recent studies revealed a key role of functional non-protein-coding RNAs (ncRNAs) in regulatory processes and transcription control, which have also been connected to certain diseases like Parkinson's disease and Alzheimer's disease (ENCODE Project Consortium and others, 2004; Gstir et al., 2014; Kaushik et al., 2018). Functional ncRNAs are involved in the modulation of epigenetic marks, altering of messenger RNA (mRNA) stability, mRNA translation, alternative splicing, signal transduction and scaffolding of large macromolecular complexes (Vandivier et al., 2016). Therefore, engineering of ncRNA molecules is of growing importance with applications ranging from biotechnology and medicine to synthetic biology (Delebecque et al., 2011; 2012; Guo et al., 2010; Terns & Terns, 2014; Meyer et al., 2015). In fact, successful attempts to create functional RNA sequences in vitro and in vivo have been reported (Dotu et al., 2014; Wachsmuth et al., 2013). At its most basic form, RNA is a sequence of the four nucleotides Adenine (A), Guanine (G), Cytosine (C) and Uracile (U). This nucleotide sequence is called the RNA sequence, or primary structure. While the RNA sequence serves as the blueprint, the functional structure of the RNA molecule is determined by the folding translating the RNA sequence into its 3D tertiary structure. The intrinsic thermodynamic properties of the sequence dictate the resulting fold. The hydrogen bonds formed between two corresponding nucleotides constitute one of the driving forces in the thermodynamic model and influence the tertiary structure heavily. The structure that encompasses these hydrogen bonds is commonly referred to as the secondary structure of RNA. Many algorithms for RNA tertiary structure design directly work on RNA secondary structures (Kerpedjiev et al., 2015; Zhao et al., 2012; Reinharz et al., 2012). Therefore, fast and accurate algorithms for RNA secondary structure design could advance the current state of the art in RNA engineering. The problem of finding an RNA sequence that folds into a desired secondary structure is known as the RNA Design problem or RNA inverse folding (Hofacker et al., 1994). Most algorithms for RNA Design focus on search strategies that start with an initial nucleotide sequence and modify it to find a solution for the given secondary structure (Hofacker et al., 1994; Andronescu et al., 2004; Taneda,
1

Under review as a conference paper at ICLR 2019

. .
(

. .
)

GCGAUAGC

A G
C

U A
G

()

GC

(a) Target structure 

(b) RNA sequence   N 

(c) Folded structure F()

Figure 1: Illustration of the RNA Design problem using folding algorithm F. Given the desired RNA secondary structure and its dot-bracket notation (a), the task is to design an RNA sequence (b) that folds into the desired secondary structure (c).

2011; Esmaili-Taheri et al., 2014; Eastman et al., 2018). In contrast, in this paper we describe a novel deep reinforcement learning approach to this problem. Our contributions are as follows:
· We describe LEARNA, a deep reinforcement learning (RL) algorithm for RNA design. LEARNA trains a policy network that, given a target secondary structure, can be rolled out to sequentially predict the entire RNA sequence. After generating an RNA sequence, our approach folds this sequence, locally adapts it, and uses the distance of the resulting structure to the target structure as an error signal for the RL agent.
· We describe Meta-LEARNA, a version of LEARNA that learns a single policy across many RNA design problems directly applicable to new RNA design problems. Specifically, it learns a conditional generative model from which we can sample candidate RNA sequences for a given RNA target structure and solves many problems with the first sample.
· We introduce a new benchmark data set, which we used to develop and tune our algorithm. · We jointly optimize the architecture of the policy network together with training hyperpa-
rameters, and the state representation on our development benchmark set. · A comprehensive empirical analysis shows that our approach achieves new state-of-the-
art performance on the two most commonly used RNA design benchmark datasets: Rfam (following Yang et al. (2017)) and Eterna100 (following Anderson-Lee et al. (2016)), as well as on the test split of our new benchmark. Furthermore, Meta-LEARNA achieves the same results as the other algorithms up to 26×, 450× and 4× faster than any previous approach on the respective datasets.
2 BAC K G RO U N D A N D R E L AT E D WO R K
In this section, we give a brief overview of the RNA Design problem and current approaches, followed by a short summary of RL approaches for similar problems.
2.1 RNA DESIGN The RNA Design problem is the inverse of the RNA folding problem. Formally, an RNA folding algorithm F maps from an RNA sequence   N  = {A, G, C, U } to a representation of the RNA's secondary structure. The RNA design problem aims to find an inverse mapping of this folding process: Definition 1 (RNA Design). Given a folding algorithm, F and a target RNA secondary structure , the RNA design problem is to find a RNA sequence   N  = {A, G, C, U } that satisfies  = F().
In this paper, we employ the most common folding algorithm: the Zuker algorithm (Zuker & Stiegler, 1981; Zuker & Sankoff, 1984), which uses a thermodynamic model to minimize the free energy to find the most stable conformation of the RNA structure. We note, however, that our approach is not limited to it and would directly apply for any other RNA folding algorithm.
2

Under review as a conference paper at ICLR 2019

Figure 1 illustrates this problem: In the dot bracket notation of the target structure (a), nucleotides are replaced by a dot for unbound sites, and opening and closing brackets for nucleotides connected by a hydrogen bond. The primary structure (b) folds into the desired target according to F (c).

Most algorithms for RNA design employ a structural loss function L(, F()) to quantify the difference between the target structure  and the structure resulting from folding an RNA sequence . The minimizer of this loss corresponds to a solution to the RNA design problem:

  arg min L(, F ()).
N 

(1)

A common loss function, which we also employ in this work, is the Hamming distance between two structures.

We note that multiple RNA sequences may fold to the same secondary structure, such that the RNA design problem does not generally have a unique solution; one could distinguish between solution by preferring more stable folds, targeting a specific GC content, or satisfying other constraints; all of these could be incorporated into the loss function being optimized.

Most algorithms targeting the RNA Design problem are either local or global algorithms. Local approaches commonly operate on a single sequence and try to find a solution by changing a small number of nucleotides at a time with the loss function guiding the search. Global methods, on the other hand, either have a large number of candidates being manipulated, or model a global distribution from which samples are sampled. They differ mostly by the specif loss used, which can be tailored for specific domains.

Local Methods Many early approaches follow the seminal work by Hofacker et al. (1994), which introduced RNAInverse, the first algorithm for the RNA design problem. Among others RNA-SSD (Andronescu et al., 2004), INFO-RNA (Busch & Backofen, 2006), NUPACK (Dirks & Pierce, 2004; Zadeh et al., 2010), ERD (Esmaili-Taheri et al., 2014), and the approach by Eastman et al. (2018) follow the same principle on manipulating a single sequence.

Global Methods The algorithms in this category employ different methods to globally search the space of primary structures. The genetic approach MODENA (Taneda, 2011) operates on an ensemble of sequences by applying local mutations and global cross-over events. AntaRNA (Kleinkauf et al., 2015) uses ant-colony optimization to find the most promising RNA sequence, while MCTS-RNA (Yang et al., 2017) employs Monte-Carlo Tree Search to model a global distribution from which it draws samples. We refer the interested reader to Churkin et al. (2017) and the references within for a more detailed review.

2.2 REINFORCEMENT LEARNING
In recent years Reinforcement Learning, which considers the setting of an agent interacting with an environment, achieved state-of-the-art results in a variety of different tasks (Mnih et al., 2013; Silver et al., 2016). In the field of Neural Architecture Search, Reinforcement Learning recently has been successfully used to design architectures for Neural Networks by sequentially sampling from sets of building blocks (Zoph & Le, 2017; Zoph et al., 2018; Pham et al., 2018). The work by Bello et al. (2016) heavily influenced our work. In it, the authors apply RL to combinatorial problems, namely the Traveling Salesman Problem. The agent proposes complete solutions rather than manipulating an existing on, and is trained using an episodic reward, in this case the tour length. Inspired by this work, we propose to frame the RNA-Design problem as a Reinforcement Learning problem where each candidate solution is designed from scratch. In our approach, the agent predicts which nucleotide to place next into the sequence, learning to design RNA in end-to-end. This is stark contrast to the recent work by Eastman et al. (2018) carried out in parallel to and independently from ours. The authors used reinforcement learning (RL) to perform a local search starting from a randomly initialized sequence. The RL agent applies local modifications to design a solution that folds into the desired target structure. The current sequence constitutes the state and each action represents changing an unpaired nucleotide or a pair of nucleotides. After each action the current sequence is evaluated utilizing Zuker's algorithm. The agent only receives a nonzero

3

Under review as a conference paper at ICLR 2019

t at

Sequence

((. . . .))

0 GC G ( . . . . ) C

1 CG G C . . . . G C

2 GC G C G . . . G C

Figure 2: Illustration of the design of a candidate solution. Starting at time point 0 the candidate solution is sequentially built by placing nucleotides from the action sequence (at)t{0, ..., T -1}.

reward signal once it finds a correct sequence. The agent's policy is a convolutional neural network pre-trained on fixed length, randomly generated sequences. In the remainder of the paper, we refer to this approach as RL-LS, as the RL agent performs a local search.

3 LEARNING TO DESIGN RNA
In this section we describe our novel generative approach for the RNA Design problem based on reinforcement learning. We first formulate RNA Design as a decision process and then propose several strategies to yield agents that learn to design RNA in an end-to-end fashion.

3.1 A DECISION PROCESS MODELLING RNA DESIGN We propose to model the RNA Design problem with respect to a given target structure  as the decision process D := (S, A, R, P, ). In our experiments, we used the popular Zuker algorithm (Zuker & Stiegler, 1981; Zuker & Sankoff, 1984) for folding, but our approach would directly be applicable to any other folding algorithm. In this section we denote the mapping, given by the Zuker algorithm, from the space of candidate solutions N  = {A, G, C, U } to the set of dot bracket encoded RNA secondary structures , with fold(·).

Action space In each episode an agent has the task to design an RNA sequence that folds into a given   . To design a candidate solution   N , at each time step t the agent places nucleotides by choosing an action at from the action space A. For unpaired sites at corresponds to one of the four RNA nucleotides (G, C, A or U); for paired sites at corresponds to one of the Watson-Crick base pairs (GC, CG, AU, or UA). The action space at time step t then is

A := {0, 1, 2, 3} 

{A, G, C, U } {GC, CG, AU, UA}

for [t] = . for [t]  {(, )}

,

(2)

where [t] is the t-th character of the target structure . See Figure 2 for an illustration.

State space The agent chooses an action at based on the state st provided by the environment. We formulated states to provide local information to the agent. For this we set st to the (2 + 1)-gram centered around the t-th site of the target structure , where  is a hyperparameter we dub the state radius. To be able to construct this centered n-gram at all sites we introduced  padding characters at the start and the end of the target structure. Formally, the state space can then be written as

S := {0, 1, 2, 3}2+1  (B  {padding})2+1 ,

(3)

where B is the set of symbols in the dot-bracket notation: a dot, an opening and a closing bracket.

Transition Function Since at each time step t the state st is set to a fixed (2 + 1)-gram, the transition function P is deterministic and defined accordingly.

4

Under review as a conference paper at ICLR 2019

Reward At the terminal time step T the agent has assigned nucleotides to all sites and the environment generates the (only non-zero) reward rT :

rT = - dH(fold(), )  ||

,

(4)

where dH(·, ·) is the Hamming distance and  > 1 is a hyperparameter to shape the reward. If the distance between the fold of the proposed sequence and the target structure becomes small enough, i.e., dH(fold(), ) <  (where  is a hyperparameter), we employ a local improvement step before computing the reward. The local strategy tries all nucleotide combinations for the sites fold() and  differ. Since the described decision process has only one non-zero reward per episode, we set the discount  to 1.

3.2 OBTAINING POLICIES FOR RNA DESIGN We use deep reinforcement learning to optimize policy networks  with parameters , that define a posterior distribution on the action space A conditional on the state space S. Our policy networks consist of an embedding layer for the input state and a deep neural network; this neural network optionally contains convolutional, recurrent and fully-connected layers, and its precise architecture is jointly optimized together with the hyperparameters as described in Section 4. We propose several strategies to learn the parameters  of a given policy network as detailed below.

LEARNA The LEARNA strategy learns to design a sequence for the target structure  in an online fashion, from scratch. The parameters  are randomly initialized before the agent episodically interacts with the decision process D. For updating the parameters we use the policy gradient method PPO (Schulman et al., 2017), which was recently successfully applied to several other applications of machine learning (Heess et al., 2017; Bansal et al., 2018; Zoph et al., 2018).

Meta-LEARNA Meta-LEARNA uses a meta-learning approach (Lemke et al., 2015) that views

the RNA design of the target structures in the training set train as tasks and learns to transfer

knowledge across them. Each of the target structures i  train defines a different decision

ipsroficneisshs eDd, ith, eanpdarwame etrtaeirns

a 

single policy are fixed and

network on all of  can be applied

them also using PPO. Once the training to the decision process D by sampling

from the learned generative model.

Meta-LEARNA-Adapt Meta-LEARNA-Adapt combines the previous two strategies: First, we run Meta-LEARNA to train parameters  in an offline training phase on train. However, to work on target structure , the policy is not fixed but is only used to initialize LEARNA running on the decision process D.

4 JOINT ARCHITECTURE SEARCH AND HYPERPARAMETER O P T I M I Z AT I O N

It is not obvious a priori which architecture would work best for our policy network. To decide this automatically, we perform a search over various possible structures, including elements of convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We construct our architecture as follows: The input state is first processed by an optional embedding layer that converts the symbol-based representation into a numerical one for each site. Then, an optional CNN with at most two layers can be applied to the state, followed by an optional LSTM with at most two layers. As the final stage, we always add a shallow fully-connected network with one or two layers, which outputs the distribution over actions. This parameterization covers a broad range of possible architectures while keeping the dimensionality of the search space relatively modest. Optimizing the architecture alone does not suffice to find very well-performing agents, since the performance of neural networks is also very sensitive to their training hyperparameters governing optimization and regularization. As we employ PPO for training the policy, we optimize some of its hyperparameters, namely the learning rate, batch size, and strength of the entropy regularization. Two additional hyperparameter are the exponent  used for reward shaping and the dimensionality

5

Under review as a conference paper at ICLR 2019
of the embedding. Finally, we also optimize the state space radius  (see Section 3.1), since our ultimate goal is not to solve a specific MDP, but to use the best MDP for solving our problem. Overall, these design choices yield a 14-dimensional search space comprising mostly integer variables. The complete list of parameters, their types, ranges, and the priors we used over them can be found in Appendix A. We used almost identical search spaces for LEARNA and Meta-LEARNA, but adapted the ranges for the learning rate and the entropy regularization slightly based on preliminary experiments. We used the recently-proposed optimizer BOHB (Falkner et al., 2018) to find good configurations, since it supports optimization in mixed integer/continuous search spaces and can utilize parallel resources. Additionally, BOHB allows to exploit cheap approximations of expensive-to-evaluate objective functions to speed up the optimization; these so-called low-fidelity approximations can be achieved in numerous ways, e.g., limiting the training time, data, or number independent repetitions of the evaluations (Falkner et al., 2018). In our setting, we decided to limit the wall-clock time for training (Meta-LEARNA) or the evaluations (LEARNA); this means that instantiations of our 14 design dimensions are first evaluated for a rather short time, and only the most promising candidates advance to higher budgets (here: computational time). To properly optimize the listed design choices without overfitting on our target RNA benchmarks, we needed a designated training and validation dataset of RNA target structures. The training set constitutes the corpus of sequences train on which we train Meta-LEARNA, and the validation set is used to estimate the performance of a configuration on unseen RNA target structures. Note that LEARNA requires no training, so we evaluated configurations for it directly on the validation set. To ensure a large enough and interesting dataset, we downloaded all families of the Rfam database version 13.0 (Kalvari et al., 2017) and removed all secondary structure duplicates. We filtered the remaining dataset for the 5% and 95% quantiles regarding the structure length to remove outliers. We only kept the sequences that a single run of MCTS-RNA for 30 seconds could not solve. (We chose MCTS-RNA for filtering as it was the fastest and most accurate algorithm at the time.) We split the remaining secondary structures into a training set of 65 000, a validation set of 100, and a test set of 100 secondary structures. We refer to these datasets as Rfam-Learn-Train, Rfam-Learn-Validation and Rfam-Learn-Test, respectively, for the remainder of the paper. Despite the fact that RL is known to often yield noisy or unreliable outcomes in single runs (Henderson et al., 2017), we actively decided to only use a single training run and a single validation set for each configuration to keep the optimization manageable. To counteract the problems associated with single noisy observations, we studied three different loss functions:
· The Number of unsolved sequences. This is the quantity we ultimately try to optimize, but it is rather noisy.
· The sum of minimal distances. This is also minimal when all sequences are solved, but it prefers almost solving a structure over complete failure and does not punish exploration.
· The sum of mean distances. By using the mean, rather than the minimum distance of a run, this choice encourages fast convergence of the agent while penalizing exploration.
In preliminary experiments, we found that the sum of minimum distances was the best choice: the optimizer found good configurations consistently, and repeated evaluations yielded consistent losses. This is in contrast to optimizing the number of unsolved sequences directly, which found less stable configurations (we attribute this due to the intrinsic noise of this quantity; with more evaluations on the validation set, this could likely be overcome at the expense of additional computational costs). Finally, the sum of mean rewards was not a good proxy loss, yielding only a weak correlation to the number of solved sequences; we attribute this to the fact that exploration is helpful and should not be discouraged.
5 EXPERIMENTS
We evaluate our approaches against state-of-the-art methods and perform an ablation study to assess the importance of our method's components. We report results on two established benchmarks from the literature and on our own benchmark. Following standard evaluation protocols for these benchmarks, we performed multiple runs with a fixed time limit on each target structure, reporting
6

Under review as a conference paper at ICLR 2019

Table 1: Summary of the final results for RNAInverse, MCTS-RNA, RL-LS, antaRNA, LEARNA, and Meta-LEARNA regarding the total number of solved target structures on the three benchmarks in percent. A target structure counts as solved if any run on the specific dataset solved it.

METHOD MCTS-RNA A N TA R N A RL-LS RNAINVERSE LEARNA-10MIN LEARNA-30MIN M E TA - L E A R N A - A D A P T M E TA - L E A R N A

ETERNA100
57 58 59 60
63 64 65

R F A M - TA N E D A
79 66 62 59
79 83 79

RFAM-LEARN-TEST
97 100 62 95
95 97 98 100

the accumulated number of solved targets across all runs. Additionally, we provide means and confidence bounds for all experiments. The three benchmarks we used are as follows:
· The Rfam benchmark as described at Taneda (2011) encompasses 29 target structures from living organisms as listed in the Rfam database version 9.0 (Gardner et al., 2009) Evaluation is based on 50 runs with a timeout of 10 minutes. We denote the dataset described at Taneda (2011) with Rfam-Taneda.
· The Eterna100 benchmark (Anderson-Lee et al., 2016) is comprised of 100 secondary structures that span a wide range of lengths and difficulties. Measurements comprise of 5 runs with a timeout of 24 hours.
· The Rfam-Learn benchmark we propose here consists of 100 target structures with a minimum length of 50- and a maximum length of 450 nucleotides. Performance is measured over 5 runs with a time limit of 1 hour.
We used the implementation of the Zuker algorithm provided by ViennaRNA (Lorenz et al., 2011) versions 2.4.8 (MCTS-RNA, RL-LS and LEARNA), 2.1.9 (AntaRNA) and 2.4.9 (RNAInverse). Our implementation uses the Reinforcement Learning library tensorforce, version 0.3.3 (Schaarschmidt et al., 2017) working with TensorFlow version 1.4.0 (Abadi et al., 2015). All computations were done on Broadwell E5-2630v4 2.2 GHz CPUs with 5 GByte RAM. For the training phase of MetaLEARNA, we used all 20 cores of these machines, but at evaluation time, all methods were only allowed a single core (using core binding).
5 . 1 C O M PA R AT I V E S T U DY
On the Eterna100 benchmark all variants of LEARNA achieve new state-of-the-art results. While LEARNA succeeds on 63% of the target structures, Meta-LEARNA solves upto 65%. Additionally, Meta-LEARNA only needs about 90 seconds to be on par with the final performance of any other method and achieves new state-of-the-art results in less than 3 minutes. This performance is stable through all of the 5 runs performed. Furthermore, Meta-LEARNA also achieves new state-of-the-art performance in each single run (solving at least 64% of the target structures). Our two pre-trained approaches, Meta-LEARNA and Meta-LEARNA-Adapt, outperform all other methods in terms of speed and the number of solved target structures. However, it seems counter-intuitive that further adaptation decreases performance of Meta-LEARNA-Adapt compared to Meta-LEARNA. We suspect that the loss in performance of Meta-LEARNA-Adapt is caused by the overhead associated with the weight updates which results in fewer evaluations in the same time. For more details, we refer to Appendix E. The results on the Eterna100 benchmark are visualized in Figure 3. Concerning the Rfam-Taneda benchmark, Meta-LEARNA and LEARNA are on par with the current state-of-the-art results of MCTS-RNA, see Figure 3. Remarkably, Meta-LEARNA needs less than 10 seconds to achieve this performance. Meta-LEARNA-adapt achieves new state-of-the-art results after 1 minute (see Appendix E) solving 83% of the target structures. The results on our new Rfam-Learn benchmark are shown in Figure 8 in Appendix B. Only MetaLEARNA and antaRNA were able to solve all of the target structures in 1 hour; 5 minutes and 20

7

Under review as a conference paper at ICLR 2019

minutes respectively. Except for RL-LS, all algorithms could solve at least 95% of the target structures within the one hour timelimit. In general, our approach achieved the best performance on all of the three benchmarks while being much faster than all other algorithms we compared against. The final performance of all algorithm of our comparison for the three benchmarks is summarized in Table 1.

Solved Sequences [%]

100 100 80 80 60 60 40 40 20 20

Meta-LEARNA-Adapt Meta-LEARNA LEARNA-30min RNAInverse MCTS-RNA AntaRNA RL-LS

Solved Sequences [%]

0 100
100 80 60 40 20

101 102 Time [seconds]

0 103 100
100 80 60 40 20

101 102 103 Time [seconds]

104
Meta-LEARNA-Adapt Meta-LEARNA LEARNA-10min RNAInverse MCTS-RNA AntaRNA RL-LS

00 100 101 102 103 100 101 102 103

Time [seconds]

Time [seconds]

Figure 3: Comparison of all methods on the Eterna 100 (top) and the RFAM-Taneda benchmark (bottom). On the left we show the minumum over 5 runs, while the right panels show the mean and the confidence interval.

5 . 2 A B L AT I O N S T U DY To study the influence of the different components on the performance of our approach we performed an ablation study. Therefore we either excluded the adaptation option, the local improvement step or the restart option. For all experiments we observed that the local improvement step boosts performance for all variants of our approach. The restart option stabilizes the single runs (see Figure 12 in the Appendix C) because it is effectively averaging over more shorter runs. We also observed a performance boost directly after the restart. Figure 9 exemplarily shows the results of the ablation study for Meta-LEARNA-adapt. All results of our ablation study are illustrated in the Appendix C.
6 CONCLUSION
We proposed the deep reinforcement learning algorithm LEARNA for the RNA-Design problem to sequentially construct candidate solutions in an end-to-end fashion. By pre-training on a large corpus of biological sequences, a local improvement step to aid the agent, and extensive architecture and hyperparameter optimization, we arrived at Meta-LEARNA, a ready-to-use agent that achieves stateof-the-art results on the Eterna100 benchmark (Anderson-Lee et al., 2016), and our own new dataset. By continuing training, dubbed Meta-LEARNA-Adapt, we can also improve over all previous results on the Rfam benchmark proposed by Taneda (2011). Our ablation study shows the importance of all components, suggesting that RL with an additional local improvement step can solve the RNA Design problem efficiently. Code and data to reproduce our results will be made publicly available after acceptance of this paper.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.
Jeff Anderson-Lee, Eli Fisker, Vineet Kosaraju, Michelle Wu, Justin Kong, Jeehyung Lee, Minjae Lee, Mathew Zada, Adrien Treuille, and Rhiju Das. Principles for predicting RNA secondary structure design difficulty. Journal of molecular biology, 428(5):748­757, 2016.
Mirela Andronescu, Anthony P. Fejes, Frank Hutter, Holger H. Hoos, and Anne Condon. A New Algorithm for RNA Secondary Structure Design. Journal of Molecular Biology, 336(3): 607­624, 2004. ISSN 0022-2836. doi: https://doi.org/10.1016/j.jmb.2003.12.041. URL http: //www.sciencedirect.com/science/article/pii/S0022283603015596.
Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent complexity via multi-agent competition. In Proceedings of the International Conference on Learning Representations (ICLR'18), 2018. Published online: iclr.cc.
Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.
Anke Busch and Rolf Backofen. INFO-RNA--a fast approach to inverse RNA folding. Bioinformatics, 22(15):1823­1831, 2006. doi: 10.1093/bioinformatics/btl194. URL http: //dx.doi.org/10.1093/bioinformatics/btl194.
Alexander Churkin, Matan Drory Retwitzer, Vladimir Reinharz, Yann Ponty, Jérôme Waldispühl, and Danny Barash. Design of rnas: comparing programs for inverse rna folding. Briefings in bioinformatics, 19(2):350­358, 2017.
Camille J. Delebecque, Ariel B. Lindner, Pamela A. Silver, and Faisal A. Aldaye. Organization of Intracellular Reactions with Rationally Designed RNA Assemblies. Science, 333(6041):470­ 474, 2011. ISSN 0036-8075. doi: 10.1126/science.1206938. URL http://science. sciencemag.org/content/333/6041/470.
CJ Delebecque, PA Silver, and AB Lindner. Designing and using RNA scaffolds to assemble proteins in vivo. Nature Protocols, 7(10):1797­807, 2012. doi: 10.1038/nprot.2012.102.
Robert M. Dirks and Niles A. Pierce. An Algorithm for Computing Nucleic Acid Base-Pairing Probabilities Including Pseudoknots. Journal of Computational Chemistry, 25(10):295---1304, 2004. doi: 10.1002/jcc.20057.
Ivan Dotu, Juan Antonio Garcia-Martin, Betty L. Slinger, Vinodh Mechery, Michelle M. Meyer, and Peter Clote. Complete RNA inverse folding: computational design of functional hammerhead ribozymes. Nucleic Acids Research, 42(18):11752­11762, 2014. doi: 10.1093/nar/gku740. URL http://dx.doi.org/10.1093/nar/gku740.
Peter Eastman, Jade Shi, Bharath Ramsundar, and Vijay S Pande. Solving the RNA design problem with reinforcement learning. PLoS computational biology, 14(6):e1006176, 2018.
ENCODE Project Consortium and others. The ENCODE (encyclopedia of DNA elements) project. Science, 306(5696):636­640, 2004.
Ali Esmaili-Taheri, Mohammad Ganjtabesh, and Morteza Mohammad-Noori. Evolutionary solution for the RNA design problem. Bioinformatics, 30(9):1250­1258, 2014. doi: 10. 1093/bioinformatics/btu001. URL http://dx.doi.org/10.1093/bioinformatics/ btu001.
9

Under review as a conference paper at ICLR 2019
Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Robust and efficient hyperparameter optimization at scale. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018), pp. 1436­1445, July 2018.
Paul P. Gardner, Jennifer Daub, John G. Tate, Eric P. Nawrocki, Diana L. Kolbe, Stinus Lindgreen, Adam C. Wilkinson, Robert D. Finn, Sam Griffiths-Jones, Sean R. Eddy, and Alex Bateman. Rfam: updates to the RNA families database. Nucleic Acids Research, 37(suppl-1):D136­D140, 2009. doi: 10.1093/nar/gkn766. URL http://dx.doi.org/10.1093/nar/gkn766.
Ronald Gstir, Simon Schafferer, Marcel Scheideler, Matthias Misslinger, Matthias Griehl, Nina Daschil, Christian Humpel, Gerald J. Obermair, Claudia Schmuckermair, Joerg Striessnig, Bernhard E. Flucher, and Alexander Hüttenhofer. Generation of a neuro-specific microarray reveals novel differentially expressed noncoding RNAs in mouse models for neurodegenerative diseases. RNA, 20(12):1929­1943, 2014.
Peixuan Guo, Oana Coban, Nicholas M. Snead, Joe Trebley, Steve Hoeprich, Songchuan Guo, and Yi Shu. Engineering RNA for Targeted siRNA Delivery and Medical Application. Advanced Drug Delivery Reviews, 62(6):650 ­ 666, 2010. ISSN 0169-409X. doi: https://doi. org/10.1016/j.addr.2010.03.008. URL http://www.sciencedirect.com/science/ article/pii/S0169409X10000773. From Biology to Materials: Engineering DNA and RNA for Drug Delivery and Nanomedicine.
Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin A. Riedmiller, and David Silver. Emergence of locomotion behaviours in rich environments. CoRR, abs/1707.02286, 2017. URL http: //arxiv.org/abs/1707.02286.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.
Ivo Hofacker, Walter Fontana, Peter Stadler, Sebastian Bonhoeffer, Manfred Tacker, and Peter Schuster. Fast Folding and Comparison of RNA Secondary Structures. Monatshefte fuer Chemie/Chemical Monthly, 125:167­188, 02 1994.
Ioanna Kalvari, Joanna Argasinska, Natalia Quinones-Olvera, Eric P Nawrocki, Elena Rivas, Sean R Eddy, Alex Bateman, Robert D Finn, and Anton I Petrov. Rfam 13.0: shifting to a genome-centric resource for non-coding RNA families. Nucleic acids research, 46(D1):D335­D342, 2017.
Kriti Kaushik, Ambily Sivadas, Shamsudheen Karuthedath Vellarikkal, Ankit Verma, Rijith Jayarajan, Satyaprakash Pandey, Tavprithesh Sethi, Souvik Maiti, Vinod Scaria, and Sridhar Sivasubbu. RNA secondary structure profiling in zebrafish reveals unique regulatory features. BMC Genomics, 20:147, 2018.
P Kerpedjiev, C Höner zu Siederdissen, and IL Hofacker. Predicting RNA 3D structure using a coarse-grain helix-centered model. RNA, 21(6):1110­1121, 2015. doi: 10.1261/rna.047522.114.
Robert Kleinkauf, Torsten Houwaart, Rolf Backofen, and Martin Mann. antaRNA­Multi-objective inverse folding of pseudoknot RNA using ant-colony optimization. BMC bioinformatics, 16(1): 389, 2015.
Christiane Lemke, Marcin Budka, and Bogdan Gabrys. Metalearning: a survey of trends and technologies. 44(1):117­130, Jun 2015.
Ronny Lorenz, Stephan H Bernhart, Christian Hoener Zu Siederdissen, Hakim Tafer, Christoph Flamm, Peter F Stadler, and Ivo L Hofacker. Viennarna package 2.0. Algorithms for Molecular Biology, 6(1):26, 2011.
Sarai Meyer, James Chappell, Sitara Sankar, Rebecca Chew, and Julius B. Lucks. Improving fold activation of small transcription activating RNAs (STARs) with rational RNA engineering strategies. Biotechnology and Bioengineering, 113(1):216­225, 2015. doi: 10.1002/bit.25693. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/bit.25693.
10

Under review as a conference paper at ICLR 2019
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. CoRR, abs/1802.03268, 2018. URL http://arxiv.org/abs/ 1802.03268.
Vladimir Reinharz, François Major, and Jérôme Waldispühl. Towards 3d structure prediction of large rna molecules: an integer programming framework to insert local 3d motifs in rna secondary structure. Bioinformatics, 28(12):i207­i214, 2012. doi: 10.1093/bioinformatics/bts226. URL http://dx.doi.org/10.1093/bioinformatics/bts226.
Michael Schaarschmidt, Alexander Kuhnle, and Kai Fricke. Tensorforce: A tensorflow library for applied reinforcement learning. Web page, 2017. URL https://github.com/ reinforceio/tensorforce.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Akito Taneda. MODENA: a multi-objective RNA inverse folding. Advances and applications in bioinformatics and chemistry: AABC, 4:1, 2011.
Rebecca M. Terns and Michael P. Terns. CRISPR-based technologies: prokaryotic defense weapons repurposed. Trends in Genetics, 30(3):111­118, 2014. doi: https://doi.org/10.1016/j.tig.2014.01. 003.
Lee E. Vandivier, Stephen J. Anderson, Shawn W. Foley, and Brian D. Gregory. The conservation and function of RNA secondary structure in plants. Annual review of plant biology, 67:463­488, 2016.
Manja Wachsmuth, Sven Findeiß, Nadine Weissheimer, Peter F. Stadler, and Mario Mörl. De novo design of a synthetic riboswitch that regulates transcription termination. Nucleic Acids Research, 41(4):2541­2551, 2013. doi: 10.1093/nar/gks1330. URL http://dx.doi.org/10.1093/ nar/gks1330.
Xiufeng Yang, Kazuki Yoshizoe, Akito Taneda, and Koji Tsuda. RNA inverse folding using Monte Carlo tree search. BMC bioinformatics, 18(1):468, 2017.
Joseph N. Zadeh, Conrad D. Steenberg, Justin S. Bois, Brian R. Wolfe, Marshall B. Pierce, Asif R. Khan, Robert M. Dirks, and Niles A. Pierce. Nupack: Analysis and design of nucleic acid systems. Journal of Computational Chemistry, 32(1):170­173, 2010. doi: 10.1002/jcc.21596. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.21596.
Y Zhao, Y Huang, Z Gong, Y Wang, J Man, and Y Xiao. Automated and fast building of threedimensional RNA structures. Scientific Reports, 2(734), 2012. doi: 10.1038/srep00734.
B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR'17), 2017. Published online: iclr.cc.
B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning transferable architectures for scalable image recognition. In Proceedings of the International Conference on Computer Vision and Pattern Recognition (CVPR'18), 2018.
M Zuker and P. Stiegler. Optimal computer folding of large RNA sequences using thermodynamics and auxiliary information. Nucleic Acids Research, 9(1):133­148, 1981.
11

Under review as a conference paper at ICLR 2019 Michael Zuker and David Sankoff. RNA secondary structures and their prediction. Bulletin
of Mathematical Biology, 46(4):591 ­ 621, 1984. ISSN 0092-8240. doi: https://doi.org/ 10.1016/S0092-8240(84)80062-2. URL http://www.sciencedirect.com/science/ article/pii/S0092824084800622.
12

Under review as a conference paper at ICLR 2019

Table 2: Search space for the agent's architecture and the trainings hyperparameter used for training Meta-Learna

Parameter Name filter size in 1st conv layer filter size in 2nd conv layer # filters in 1st conv layer # filters in 2nd conv layer # fully connected layers # units in fully connected layer(s) # LSTM layers # units in every LSTM layer # state space radius embedding dimensionality batch size entropy regularization learning rate for PPO reward exponent

Type integer integer integer integer integer integer integer integer integer integer integer float float float

Range [0, 8] [0, 4] [1, 32] [1, 32] [1, 2] [8, 64] [0, 2] [1, 64] [0, 32] [0, 4] [32, 128] [5 · 10-5, 5 · 10-3] [1 · 10-6, 1 · 10-4] [1, 10]

Prior uniform uniform uniform uniform uniform log-uniform uniform log-uniform uniform uniform log-uniform log-uniform log-uniform uniform

Table 3: Search space for the agent's architecture and the trainings hyperparameter used for training Learna for both the 10 and 30 minutes budget

Parameter Name filter size in 1st conv layer filter size in 2nd conv layer # filter in 1st conv layer # filter in 2nd conv layer # fully connected layers # units in fully connected layer(s) # LSTM layers # units in every LSTM layer # state space radius embedding dimensionality batch size entropy regularization learning rate for PPO reward exponent

Type integer integer integer integer integer integer integer integer integer integer integer float float float

Range [0, 8] [0, 4] [1, 32] [1, 32] [1, 2] [8, 64] [0, 2] [1, 64] [1, 32] [0, 4] [32, 128] [1 · 10-5, 1 · 10-2] [1 · 10-5, 1 · 10-3] [1, 10]

Prior uniform uniform log-uniform log-uniform uniform log-uniform uniform log-uniform uniform uniform log-uniform log-uniform log-uniform uniform

A ARCHITECTURE AND HYPERPARAMETER SEARCH SPACE

13

Under review as a conference paper at ICLR 2019

Table 4: The selected configurations for each scenario and budget.

Parameter Name filter size in 1st conv layer filter size in 2nd conv layer # filters in 1st conv layer # filters in 2nd conv layer # fully connected layers # units in fully connected layer(s) # LSTM layers # units in every LSTM layer # state space radius embedding dimensionality batch size entropy regularization learning rate for PPO reward exponent

LEARNA-10min 5 3 8 1 1 52 2 4 16 0 32
4.44 · 10-4 5.49 · 10-4
5.72

LEARNA-30min 0 3 10 3 1 32 2 7 2 0 79
1.63 · 10-4 3.38 · 10-4
9.43

Meta-LEARNA 5 7 32 14 1 9 0 53 26 1 80
1.98 · 10-4 6.37 · 10-5
9.22

fraction of solved sequences on validation [%] loss

102

101

100

10 1

10

2
103

b=400.000000 b=3600.000000 b=1200.000000

104 wall clock time [s]

105

Figure 4: Observed validation loss during the BOHB run for Meta-LEARNA. The different budgets b correnspond to the training time on 20 CPUs in parallel. The results seem to suggest that one can achieve a very similar performance with only 20 minutes of training, which could imply that much longer training of the agent might be required for substantially better performance.

100

80

60

40

20

010 2 10 1 100

101

validation loss

102

Figure 5: Relationship between the obseverd loss and the number of solved sequences. The plot suggests that our loss metric correlates strongly with the number of successfully found primary sequences.

14

Under review as a conference paper at ICLR 2019

B SOLVED VS TIME
B.1 ETERNA

Solved Sequences [%]

100 80 60 40 20 0 100

100 80 60 40 20

Meta-LEARNA-Adapt Meta-LEARNA LEARNA-30min RNAInverse MCTS-RNA AntaRNA RL-LS

101 102 Time [seconds]

0 103 100 101 102 103 104
Time [seconds]

Figure 6: Comparison of all methods on Eterna.

B.2 RFAM TANEDA

Solved Sequences [%]

100 80 60 40 20 0 100

100 80 60 40 20

Meta-LEARNA-Adapt Meta-LEARNA LEARNA-10min RNAInverse MCTS-RNA AntaRNA RL-LS

101 102 Time [seconds]

0 103 100

101 102 Time [seconds]

103

Figure 7: Comparison of all methods on RFAM Taneda.

15

Under review as a conference paper at ICLR 2019

B.3 RFAM LEARN

Solved Sequences [%]

100 80 60 40 20 0 100

100 80 60 40 20

Meta-LEARNA-Adapt Meta-LEARNA LEARNA-30min RNAInverse MCTS-RNA AntaRNA RL-LS

101 102 103 Time [seconds]

104

0 100 101 102 103 104
Time [seconds]

Figure 8: Comparison of all methods on RFAM LEARNA.

C A B L AT I O N
C.1 META-LEARNA-ADAPT

Solved Sequences [%]

100 100 80 80 60 60

Meta-LEARNA-Adapt no adaptation no local improvement no restarts

40 40

20 20

0 100 101 102 103 104
Time [seconds]

0 100 101 102 103 104
Time [seconds]

Figure 9: Ablation study of Meta-LEARNA-Adapt on RFAM LEARNA.

C.2 META-LEARNA

Solved Sequences [%]

100 80 60 40 20 0 100

100 80

Meta-LEARNA no local improvement

60

40

20

101 102 103 Time [seconds]

104

0 100 101 102 103 104
Time [seconds]

Figure 10: Aplation study of Meta-LEARNA on RFAM LEARNA.

C.3 LEARNA-30MIN C.4 LEARNA-10MIN

16

Under review as a conference paper at ICLR 2019

Solved Sequences [%]

100 100 80 80 60 60

LEARNA-10min no local improvement no restarts

40 40

20 20

0 100 101 102 103 104
Time [seconds]

0 100 101 102 103 104
Time [seconds]

Figure 11: Aplation study of LEARNA-30min on RFAM LEARNA.

Solved Sequences [%]

100 100 80 80 60 60

LEARNA-10min no local improvement no restarts

40 40

20 20

0 100 101 102 103 104
Time [seconds]

0 100 101 102 103 104
Time [seconds]

Figure 12: Aplation study of LEARNA-10min on RFAM LEARNA.

17

Under review as a conference paper at ICLR 2019

D VA L I D AT I O N
D.1 META-LEARNA

Solved Sequences [%]

100 100 80 80 60 60 40 40

id = (54, 0, 2) id = (77, 0, 1) id = (108, 0, 1) id = (215, 0, 1) id = (129, 0, 0)

20 20

00 100 101 102 103 100 101 102 103

Time [seconds]

Time [seconds]

Figure 13: 5 independent runs of the best 5 configurations for Meta-LEARNA on the validation set to asses the robustness. Chosen configuration highlighted in bold.

D.2 LEARNA-30MIN

Solved Sequences [%]

100 100 80 80 60 60 40 40

id = (3, 0, 1) id = (20, 0, 15) id = (64, 0, 0) id = (117, 0, 3) id = (129, 0, 0)

20 20

0 100 101 102 103 104
Time [seconds]

0 100 101 102 103 104
Time [seconds]

Figure 14: 5 independent runs of the best 5 configurations for LEARNA with a 30 minute budget on the validation set to asses the robustness. Chosen configuration highlighted in bold.

D.3 LEARNA-10MIN

Solved Sequences [%]

100 100 80 80 60 60 40 40

id = (85, 0, 0) id = (97, 0, 4) id = (182, 0, 2) id = (222, 0, 3) id = (239, 0, 2)

20 20

00 100 101 102 103 100 101 102 103

Time [seconds]

Time [seconds]

Figure 15: 5 independent runs of the best 5 configurations for LEARNA with a 10 minute budget on the validation set to asses the robustness. Chosen configuration highlighted in bold.

18

Under review as a conference paper at ICLR 2019

Table 5: Comparison between RNAInverse, MCTS-RNA, RL-LS, antaRNA, LEARNA, and our random baseline regarding the total number of solved target structures of the Eterna benchmark in accordance to Anderson-Lee et al. (2016) in percent. 5 runs with a timelimit of 24 hours were performed for each of the 100 target structures. A target structure was counted as solved if any of the 5 runs solved it.

METHOD R A N D O M - N O - M U TAT I O N RANDOM MCTS-RNA A N TA R N A RL-LS RNAINVERSE LEARNA-30MIN M E TA - L E A R N A - A D A P T M E TA - L E A R N A

TOTAL (%)
30 39 57 58 59 60
63 64 65

2 RUNS
27 36 57 58 59 60
61 63 65

3 RUNS
26 35 56 58 58 59
60 62 64

4 RUNS
26 34 54 56 57 59
60 60 64

5 RUNS
26 32 51 56 55 58
58 60 64

Table 6: Comparison between MCTS-RNA, antaRNA, RL-LS, RNAInverse, LEARNA and our random baseline regarding the total number of solved target structures of the Rfam derived benchmark in accordance to Taneda (2011) in percent. 50 runs with a timelimit of 10 minutes were performed for each of the 29 target structures. A target structure was counted as solved if any of the 50 runs solved it.

METHOD R A N D O M - N O - M U TAT I O N RANDOM RNAINVERSE RL-LS A N TA R N A MCTS-RNA LEARNA-10MIN M E TA - L E A R N A - A D A P T M E TA - L E A R N A

TOTAL (%)
28 35 59 62 66 79
79 83 79

5 RUNS
21 28 55 62 66 76
76 83 79

10 RUNS
21 28 55 55 66 72
76 79 79

25 RUNS
21 24 52 52 66 72
66 79 79

50 RUNS
17 21 48 48 62 59
28 69 76

E SUMMARY TABLES

19

Under review as a conference paper at ICLR 2019

Table 7: Comparison between MCTS-RNA, antaRNA, RL-LS, RNAInverse, LEARNA and our random baseline regarding the total number of solved target structures of the Rfam derived benchmark in percent. 3 runs with a timelimit of 4 hours were performed for each of the 100 target structures. A target structure was counted as solved if any of the 3 runs solved it.

METHOD R A N D O M - N O - M U TAT I O N RANDOM RL-LS RNAINVERSE MCTS-RNA A N TA R N A LEARNA-10MIN LEARNA-30MIN M E TA - L E A R N A - A D A P T M E TA - L E A R N A

TOTAL (%)
17 31 62 95 97 100
95 97 98 100

2 RUNS
14 26 53 90 94 99
94 94 98 100

3 RUNS
13 25 45 87 91 99
92 93 98 100

4 RUNS
12 22 41 83 89 99
84 90 96 99

5 RUNS
10 21 37 78 82 99
74 82 92 98

Table 8: Comparison between RNAInverse, MCTS-RNA, RL-LS, antaRNA, LEARNA, and our random baseline regarding the total number of solved target structures of the Eterna benchmark in accordance to Anderson-Lee et al. (2016) in percent. 5 runs with a timelimit of 24 hours were performed for each of the 100 target structures. A target structure was counted as solved if any of the 5 runs solved it.

METHOD

TOTAL (%) 10S 1MIN 30MIN 1H 4H 12H

R A N D O M - N O - M U TAT I O N RANDOM MCTS-RNA A N TA R N A RL-LS RNAINVERSE

30 39 57 58 59 60

15 21 19 22 41 48 36 46 0 40 32 44

25 26 29 30 29 33 37 39 55 56 57 57 54 55 55 58 53 55 58 59 55 57 59 60

LEARNA-30MIN M E TA - L E A R N A - A D A P T M E TA - L E A R N A

63 64 65

0 41 14 56 45 59

58 58 60 63 59 59 63 64 61 64 64 65

Table 9: Comparison between MCTS-RNA, antaRNA, RL-LS, RNAInverse, LEARNA and our random baseline regarding the total number of solved target structures of the Rfam derived benchmark in accordance to Taneda (2011) in percent. 50 runs with a timelimit of 10 minutes were performed for each of the 29 target structures. A target structure was counted as solved if any of the 50 runs solved it.

METHOD

TOTAL (%) 10S 30S 1MIN 5MIN

R A N D O M - N O - M U TAT I O N RANDOM A N TA R N A RNAINVERSE RL-LS MCTS-RNA

28 35 66 59 62 79

17 21 28 28 52 62 55 55 0 48 72 76

24 31 66 55 52 76

28 31 66 55 55 79

LEARNA-10MIN M E TA - L E A R N A - A D A P T M E TA - L E A R N A

79 83 79

0 62 28 79 79 79

72 83 79

76 83 79

20

Under review as a conference paper at ICLR 2019

Table 10: Comparison between MCTS-RNA, antaRNA, RL-LS, RNAInverse, LEARNA and our random baseline regarding the total number of solved target structures of the Rfam derived benchmark in percent. 3 runs with a timelimit of 4 hours were performed for each of the 100 target structures. A target structure was counted as solved if any of the 3 runs solved it.

METHOD

TOTAL (%) 10S 30S 1MIN 5MIN 10MIN 20MIN 30MIN

R A N D O M - N O - M U TAT I O N RANDOM RL-LS RNAINVERSE MCTS-RNA A N TA R N A

17 31 62 95 97 100

34 11 14 0 14 39 53 40 55 36 58

6 16 21 66 68 73

10 23 38 83 86 97

12 25 45 89 92 99

13 14 30 30 50 56 92 93 93 94 99 100

LEARNA-10MIN LEARNA-30MIN M E TA - L E A R N A - A D A P T M E TA - L E A R N A

95 97 98 100

0 24 39

78

85

92

93

0 15 34

78

88

92

93

4 81 87

98

98

98

98

41 90 95 100 100 100 100

21

Under review as a conference paper at ICLR 2019

F DETAIL TABLES
Table 11: Results for 5 independent attempts on the first half of the 100 targets of the Rfam benchmark. The timeout for each attempt was set to 4 hours.

ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 T O TA L
S O LV E D

LEARNA-30MIN 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 2/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 1/5 5/5 5/5 5/5 5/5 5/5
243/250
50/50

M E TA - L E A R N A 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5
250/250
50/50

MCTS-RNA 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 3/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 1/5 1/5 5/5 5/5 5/5 5/5
232/250
49/50

RL-SL 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 2/5 5/5 1/5 3/5 5/5 5/5 5/5 5/5 5/5 4/5 3/5 5/5 1/5 5/5 5/5 5/5 5/5 5/5 3/5 5/5 5/5 4/5
205/250
45/50

RNAINVERSE 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5
244/250
49/50

A N TA R N A 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 3/5 1/5 5/5 5/5 5/5 5/5 5/5
244/250
50/50

22

Under review as a conference paper at ICLR 2019

Table 12: Results for 3 independent attempts on the second half of the 100 targets of the Rfam benchmark. The timeout for each attempt was set to 4 hours.

ID 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 T O TA L
S O LV E D

LEARNA 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 3/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 4/5 3/5 5/5 5/5 4/5 3/5 1/5 4/5 5/5 5/5 4/5 5/5 5/5 5/5 1/5 5/5 4/5 -
218/250
47/50

M E TA - L E A R N A 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 3/5 5/5 5/5 5/5
247/250
50/50

MCTS-RNA 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 4/5 1/5 5/5 5/5 5/5 2/5 2/5 5/5 5/5 4/5 5/5 5/5 5/5 2/5 5/5 5/5 3/5
226/250
49/50

RL-SL 1/5 5/5 1/5 2/5 3/5 1/5 1/5 1/5 1/5 1/5 1/5 2/5 1/5 1/5 4/5 2/5 2/5 -
30/250
17/50

RNAINVERSE 5/5 5/5 5/5 5/5 5/5 5/5 4/5 3/5 5/5 3/5 1/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 3/5 5/5 5/5 5/5 5/5 2/5 5/5 2/5 5/5 4/5 5/5 1/5 2/5 4/5 3/5 5/5 5/5 5/5 5/5 1/5 5/5 5/5 5/5 4/5 5/5 1/5 5/5 1/5 5/5 -
198/250
47/50

A N TA R N A 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5
250/250
50/50

23

Under review as a conference paper at ICLR 2019

Table 13: Results for 5 independent attempts on each of the 100 targets of the Eterna100 benchmark in accordance to (Anderson-Lee et al., 2016). The timeout for each attempt was set to 24 hours.

ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 T O TA L
S O LV E D

LEARNA-30MIN 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 -
228/250
46/50

M E TA - L E A R N A 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 -
245/250
49/50

MCTS-RNA 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 -
202/250
41/50

RL-SL 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 3/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 2/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 -
219/250
44/50

RNAINVERSE 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 2/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 -
216/250
44/50

A N TA R N A 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 -
218/250
44/44

24

Under review as a conference paper at ICLR 2019

Table 14: Results for 5 independent attempts on each of the 100 targets of the Eterna100 benchmark in accordance to (Anderson-Lee et al., 2016). The timeout for each attempt was set to 24 hours.

ID 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 T O TA L
S O LV E D

LEARNA-30MIN 5/5 5/5 5/5 5/5 5/5 5/5 5/5 1/5 4/5 5/5 5/5 2/5 5/5 5/5 5/5 5/5 1/5 -
73/250
17/50

M E TA - L E A R N A 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 2/5 5/5 5/5
5/5 -
5/5 -
5/5 -
77/250
16/50

MCTS-RNA 5/5 5/5 5/5 5/5 5/5 5/5 5/5 2/5 5/5 3/5 5/5 3/5 5/5 5/5 5/5 5/5 -
73/250
16/50

RL-SL 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 -
69/250
14/50

RNAINVERSE 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 5/5 -
80/250
16/50

A N TA R N A 5/5 5/5 5/5 5/5 5/5 3/5 5/5 5/5 4/5 5/5 5/5 5/5 5/5 5/5 -
67/250
14/50

25

Under review as a conference paper at ICLR 2019

Table 15: Results for 50 independent attempts on each of the 29 targets of the Rfam benchmark in accordance to (Taneda, 2011). The timeout for each attempt was set to 10 minutes.

ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 T O TA L
S O LV E D

LEARNA-10MIN 24/50 40/50 1/50 48/50 50/50 50/50 46/50 50/50 21/50 41/50 47/50 50/50 50/50 29/50 50/50 50/50 48/50 43/50 49/50 50/50 10/50 48/50 35/50
930/1450
23/29

M E TA - L E A R N A 50/50 50/50 46/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50
1146/1450
23/29

MCTS-RNA 32/50 28/50 4/50 50/50 50/50 50/50 50/50 50/50 44/50 50/50 50/50 50/50 50/50 47/50 50/50 50/50 50/50 50/50 50/50 50/50 6/50 50/50 50/50
1011/1450
23/29

RL-SL 7/50 5/50 50/50 50/50 50/50 48/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 19/50 50/50 50/50 50/50 -
779/1450
18/29

RNAINVERSE 20/50 50/50 50/50 50/50 50/50 50/50 3/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 46/50 50/50 50/50 -
769/1450
17/29

A N TA R N A 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 50/50 36/50
1036/1450
21/29

26

