Under review as a conference paper at ICLR 2019
AUTOMATA GUIDED SKILL COMPOSITION
Anonymous authors Paper under double-blind review
ABSTRACT
Skills learned through (deep) reinforcement learning often generalizes poorly across tasks and re-training is necessary when presented with a new task. We present a framework that combines techniques in formal methods with reinforcement learning (RL) that allows for convenient specification of complex temporal dependent tasks with logical expressions and construction of new skills from existing ones with no additional exploration. We provide theoretical results for our composition technique and evaluate on a simple grid world simulation as well as a robotic manipulation task.
1 INTRODUCTION
Policies learned using reinforcement learning aim to maximize the given reward function and is often difficult to transfer to other problem domains. Skill composition is the process of constructing new skills out of existing ones (policies) with little to no additional learning. In stochastic optimal control, this idea has been adopted by authors of (Todorov, 2009) and (Da Silva et al., 2009) to construct provably optimal control laws based on linearly solvable Markov decision processes. Temporal logic (TL) is a formal language commonly used in software and digital circuit verification (Baier & Katoen, 2008) as well as formal synthesis (Belta et al., 2017). It allows for convenient expression of complex behaviors and causal relationships. TL has been used by (Tabuada & Pappas, 2004), (Fainekos et al., 2006), (Fainekos et al., 2005) to synthesize provably correct control policies. Authors of (Aksaray et al., 2016) have also combined TL with Q-learning to learn satisfiable policies in discrete state and action spaces. In this work, we focus on skill composition with policies learned using automata guided reinforcement learning (Li et al., 2018). We adopt the syntactically co-safe truncated linear temporal logic (scTLTL) as the task specification language. Compared to most heuristic reward structures used in the RL literature, formal specification language has the advantage of semantic rigor and interpretability. In our framework, skill composition is accomplished by taking the product of finite state automata (FSA). Instead of interpolating/extrapolating among learned skills/latent features, our method is based on graph manipulation of the FSA. Therefore, the compositional outcome is much more transparent. At testing time, the behavior of the policy is strictly enforced by the FSA and therefore safety can be guaranteed if encoded in the specification. Compared with previous work on skill composition, we impose no constraints on the policy representation or the problem class. We validate our framework in simulation (discrete state and action spaces) and experimentally on a Baxter robot (continuous state and action spaces).
2 RELATED WORK
Recent efforts in skill composition have mainly adopted the approach of combining value functions learned using different rewards. Authors of (Peng et al., 2018) construct a composite policy by combining the value functions of individual policies using the Boltzmann distribution. With a similar goal, authors of (Zhu et al., 2017) achieves task space transfer using deep successor representations (Kulkarni et al., 2016). However, it is required that the reward function be represented as a linear combination of state-action features.
1

Under review as a conference paper at ICLR 2019

Authors of (Haarnoja et al., 2018) have showed that when using energy-based models (Haarnoja et al., 2017), an approximately optimal composite policy can result from taking the average of the Q-functions of existing policies. The resulting composite policy achieves the -AN D- task composition i.e. the composite policy maximizes the average reward of individual tasks.
Authors of (van Niekerk et al., 2018) have taken this idea a step further and showed that by combining individual Q-functions using the log-sum-exponential function, the -OR- task composition (the composite policy maximizes the (soft) maximum of the reward of constituent tasks) can be achieved optimally.
We build on the results of (van Niekerk et al., 2018) and show that incorporating temporal logic allows us to compose tasks of greater logical complexity with higher interpretability. Our composite policy is optimal in both -AN D- and -OR- task compositions.

3 PRELIMINARIES

3.1 ENTROPY-REGULARIZED REINFORCEMENT LEARNING AND Q-COMPOSITION
We start with the definition of a Markov Decision Process. Definition 1. An MDP is defined as a tuple M = S, A, p(·|·, ·), r(·, ·, ·) , where S  IRn is the state space ; A  IRm is the action space (S and A can also be discrete sets); p : S ×A×S  [0, 1] is the transition function with p(s |s, a) being the conditional probability density of taking action a  A at state s  S and ending up in state s  S; r : S × A × S  IR is the reward function with r(s, a, s ) being the reward obtained by executing action a at state s and transitioning to s .
In entropy-regularized reinforcement learning (Schulman et al., 2017), the goal is to maximize the following objective

T -1
J() = E[rt + H((·|st))],
t=0

(1)

where  : S × A  [0, 1] is a stochastic policy. E is the expectation following . H((·|st)) is the entropy of .  is the temperature parameter. In the limit   0, Equation (1) becomes the standard
RL objective. The soft Q-learning algorithm introduced by (Haarnoja et al., 2017) optimizes the
above objective and finds a policy represented by an energy-based model

 (at|st)  exp(-E(st, at))

(2)

where E(st, at) is an energy function that can be represented by a function approximator.

Let rtent = rt + H((·|st)), the state-action value function (Q-function) following  is defined

as Q(s, a) = E[

T -1 t=0

rtent

|s0

=

s, a0

=

a].

Suppose we have a set of n tasks indexed by

i, i  {0, ..., n}, each task is defined by an MDP Mi that differs only in their reward function

ri. Let Qi be the optimal entropy-regularized Q-function. Authors of (van Niekerk et al., 2018)

provide the following results

Theorem 1. Define vectors r = [r1, ..., rn], Q = [Q1 , ..., Qn ]. Given a set of nonnegative weights w with ||w|| = 1, the optimal Q-function for a new task defined by r =  log(|| exp(r/)||w) is given by

Q =  log(|| exp(Q /)||w), where || · ||w is the weighted 1-norm.

(3)

The authors proceed to provide the following corollary

Corollary 1. max Q  Q0 as   0, where Q0 is the optimal Q-function for the objective

J() =

T -1 t=0

E [rt ].

2

Under review as a conference paper at ICLR 2019

Corollary 1 states that in the low temperature limit, the maximum of the optimal entropy-regularized Q-functions approaches the standard optimal Q-function
3.2 SCTLTL AND FINITE STATE AUTOMATA
We consider tasks specified with syntactically co-safe Truncated Linear Temporal Logic (scTLTL) which is derived from truncated linear temporal logic(TLTL) (Li et al., 2018). The syntax of scTLTL is defined as

 := | f (s) < c | ¬ |    |  |  U  |  T  | 

(4)

where is the True Boolean constant. s  S is a MDP state in Definition 1. f (s) < c is a predicate over the MDP states where c  IR. ¬ (negation/not),  (conjunction/and) are Boolean connectives.  (eventually), U (until), T (then), (next), are temporal operators. (implication) and and  (disjunction/or) can be derived from the above operators.
We denote st  S to be the MDP state at time t, and st:t+k to be a sequence of states (state trajectory) from time t to t + k, i.e., st:t+k = stst+1...st+k. The Boolean semantics of scTLTL is defined as:

st:t+k |= f (s) < c st:t+k |= ¬ st:t+k |=    st:t+k |=    st:t+k |=    st:t+k |=  st:t+k |=  st:t+k |=  U 
st:t+k |=  T 

 f (st) < c,  ¬(st:t+k |= ),  (st:t+k |= )  (st:t+k |= ),  (st:t+k |= )  (st:t+k |= ),  (st:t+k |= )  (st:t+k |= ),  (st+1:t+k |= )  (k > 0),  t  [t, t + k) st :t+k |= ,  t  [t, t + k) s.t. st :t+k |= 
 (t  [t, t ) st :t |= ),  t  [t, t + k) s.t. st :t+k |= 
 (t  [t, t ) st :t |= ).

A trajectory s0:T is said to satisfy formula  if s0:T |= .

The quantitative semantics (also referred to as robustness) is defined recursively as

(st:t+k, ) (st:t+k, f (st) < c) (st:t+k, ¬) (st:t+k,   ) (st:t+k, 1  2) (st:t+k, 1  2) (st:t+k, ) (st:t+k, )
(st:t+k,  U )
(st:t+k,  T )

= max, = c - f (st), = - (st:t+k, ), = max(-(st:t+k, ), (st:t+k, )) = min((st:t+k, 1), (st:t+k, 2)), = max((st:t+k, 1), (st:t+k, 2)), = (st+1:t+k, ) (k > 0), = max ((st :t+k, )),
t [t,t+k)
= max (min((st :t+k, ),
t [t,t+k)
min (st :t , ))),
t [t,t )
= max (min((st :t+k, ),
t [t,t+k)
max (st :t , ))),
t [t,t )

3

Under review as a conference paper at ICLR 2019

where max represents the maximum robustness value. A robustness of greater than zero implies that st:t+k satisfies  and vice versa ((st:t+k, ) > 0  st:t+k |=  and (st:t+k, ) < 0  st:t+k |= ). The robustness is used as a measure of the level of satisfaction of a trajectory s0:T with respect to a scTLTL formula .
Definition 2. An FSA corresponding to a scTLTL formula . is defined as a tuple A = Q, , q,0, p(·|·), F , where Q is a set of automaton states;  is the input alphabet (a set of first order logic formula); q,0  Q is the initial state; p : Q × Q  [0, 1] is a conditional probability defined as

p(q,j |q,i) =

1 0

q,i,q,j is true otherwise.

or

p(q,j |q,i, s) =

1 0

(s, q,i,q,j ) > 0 otherwise.

(5)

F is a set of final automaton states.
Here q,i is the ith automaton state of A. q,i,q,j   is the predicate guarding the transition from q,i to q,j. Because q,i,q,j is a predicate without temporal operators, the robustness (st:t+k, q,i,q,j ) is only evaluated at st. Therefore, we use the shorthand (st, q,i,q,j ) = (st:t+k, q,i,q,j ). The translation from a TLTL formula to a FSA can be done automatically with available packages like Lomap (Vasile, 2017).

3.3 FSA AUGMENTED MDP
The FSA Augmented MDP is defined as follows Definition 3. (Li et al., 2018) An FSA augmented MDP corresponding to scTLTL formula  (constructed from FSA Q, , q,0, p(·|·), F and MDP S, A, p(·|·, ·), r(·, ·, ·) ) is defined as M = S~, A, p~(·|·, ·), r~(·, ·), F where S~  S × Q, p~(s~ |s~, a) is the probability of transitioning to s~ given s~ and a,

p~(s~ |s~, a) = p (s , q)|(s, q), a =

p(s |s, a) 0

p(q |q, s) = 1 otherwise.

(6)

p is defined in Equation equation 5. r~ : S~ × S~  IR is the FSA augmented reward function, defined by

r~(s~, s~ ) = (s , Dq ),

(7)

where Dq = qq q,q represents the disjunction of all predicates guarding the transitions

that originate from q (q is the set of automata states that are connected with q through outgoing

edges).

A policy  is said to satisfy  if

 = arg maxE [1((s0:T , ) > 0)].


(8)

where 1((s0:T , ) > 0) is an indicator function with value 1 if (s0:T , ) > 0 and 0 otherwise. As is mentioned in the original paper, there can be multiple policies that meet the requirement of

Equation (8), therefore, a discount factor is used to find a maximally satisfying policy - one that

leads to satisfaction in the least amount of time.

The FSA augmented MDP M establishes a connection between the TL specification and the standard reinforcement learning problem. A policy learned using M has implicit knowledge of the FSA through the automaton state q  Q. We will take advantage of this characteristic in our skill
composition framework.

4

Under review as a conference paper at ICLR 2019

Figure 1 : FSA for (a) 1 = a  b. (b) 2 = c. (c)  = 1  2.

4 PROBLEM FORMULATION
Problem 1. Given two scTLTL formula 1 and 2 and their optimal Q-functions Q1 and Q2 , obtain the optimal policy  that satisfies  = 1  2 and  that satisfies  = 1  2.
Here Q1 and Q2 can be the optimal Q-functions for the entropy-regularized MDP or the standard MDP. Problem 1 defines the problem of skill composition: given two policies each satisfying a scTLTL specification, construct the policy that satisfies the conjunction (-AN D-)/disjunction (-OR-) of the given specifications. Solving this problem is useful when we want to break a complex task into simple and manageable components, learn a policy that satisfies each component and "stitch" all the components together so that the original task is satisfied. It can also be the case that as the scope of the task grows with time, the original task specification is amended with new items. Instead of having to re-learn the task from scratch, we can learn only policies that satisfies the new items and combine them with the old policy.

5 AUTOMATA GUIDED SKILL COMPOSITION

In section, we provide a solution for Problem 1 by constructing the FSA of  from that of 1 and 2
and using  to synthesize the policy for the combined skill. We start with the following definition.
Definition 4. Given A1 = Q1 , 1 , q1,0, p1 , F1 and A2 = Q2 , 2 , q2,0, p2 , F2 corresponding to formulas 1 and 2, the FSA of  = 1 2 is the product automaton of A1 and A1 , i.e. A = 1  2 = A1 × A2 = Q ,  , q,0, p , F where Q  Q1 × Q2 is the set of product automaton states, q,0 = (q1,0, q2,0) is the product initial state, F  F1  F2 are the final accepting states. Following Definition 2, for states q = (q1 , q2 )  Q and q = (q1 , q2 )  Q , the transition probability p is defined as

p (q |q ) =

1 0

p1 (q1 |q1 )p2 (q2 |q2 ) = 1 otherwise.

(9)

Example 1. Figure 1 illustrates the FSA of A1 and A2 and their product automaton A . Here 1 = r  g which entails that both r and g needs to be true at least once (order does not matter),
and 2 = b. The resultant product corresponds to the formula  = r  g  b.

5

Under review as a conference paper at ICLR 2019

Figure 2 : Policies for (a) 1 = a  b. (b) 2 = c. (c)  = 1  2. The agent moves in a 8 × 10 gridworld with 3 labeled regions. The agent has actions [up, down, left, right, stay] where the directional actions are represented by arrows, stay is represented by the blue dot.

We provide the following theorem on automata guided skill composition
Theorem 2. Let Q = [Q11 , ..., Qnn ] be a vector with entries Qii being the optimal Q-function for the FSA augmented MDP Mi . The optimal Q-function for M where  = i i is Q = max(Q ).

Proof. For q = (q1 , q2 )  Q , let q , q1 and q2 denote the set of predicates guarding the edges originating from q , q1 and q2 respectively. Equation equation 9 entails that a transition at q in the product automaton A exists only if corresponding transitions at q1 , q2 exist in A1 and A2 respectively. Therefore, q ,q = q1 ,q1  q2 ,q2 , for q ,q  q , q1 ,q1  q1 , q2 ,q2  q2 (here qi is a state such that pi (qi |qi ) = 1). Therefore,
we have

Dq


=

(q1 ,q1  q2 ,q2 )

q1 ,q2

(10)

where q1 , q2 don't equal to q1 , q2 at the same time (to avoid self looping edges). Using the fact

that qi ,qi = ¬ qi =qi qi ,qi and repeatedly applying the distributive laws (1)(2) =





(1



2)

and

(



1)



(



2)

=





(1



2)

to

Dq


,

we

arrive

at

Dq


=

q1 ,q1 

q2 ,q2

=

Dq1
1



Dq2
2

.

q1 =q1

q2 =q2

(11)

6

Under review as a conference paper at ICLR 2019

Let r~ , r~1 , r~2 and s~ , s~1 , s~2 be the reward functions and states for FSA augmented MDP M , M1 , M2 respectively. s , s1 , s2 are the states for the corresponding MDPs. Plugging
Equation (11) into Equation (7) and using the robustness definition for disjunction results in

r~ (s~ ,

s~ )

=

(s ,

Dq


)

=

(s

,

Dq1
1



Dq2
2

)

= max

(s1

,

Dq1
1

),

(s2

,

Dq2
2

)

= max r~1 (s~1 , s~1 ), r~2 (s~2 , s~2 ) .

(12)

Looking at Theorem 1, the log-sum-exp of the composite reward r =  log(|| exp(r/)||w) is in fact an approximation of the maximum function. In the low temperature limit we have r  max(r) as
  0. Applying Corollary 1 results in Theorem 2.

Having obtained the optimal Q-function, a policy can be constructed by taking the greedy step with respective to the Q-function in the discrete action case. For the case of continuous action space where the policy is represented by a function approximator, the policy update procedure in actorcritic methods can be used to extract a policy from the Q-function.
In our framework, -AN D- and -OR- task compositions follow the same procedure (Theorem 2). The only difference is the termination condition. For -AN D- task, the final state F = Fi in Definition 4 needs to be reached (i.e. all the constituent FSAs are required to reach terminal state, as in state q,f in Figure 1 ). Whereas for the -OR- task, only F = Fi needs to be reached (one of states q,2, q,4, q,5, q,6, q,f in Figure 1 ).

6 CASE STUDIES

We evaluate the our composition method in two environments. The first is a simple 2D grid world environment that is used as for proof of concept and policy visualization. The second is a robot manipulation environment.

6.1 GRID WORLD
Consider an agent that navigates in a 8 × 10 grid world. Its MDP state space is S : X × Y where x, y  X, Y are its integer coordinates on the grid. The action space is A : [up, down, left, right, stay]. The transition is such that for each action command, the agent follows that command with probability 0.8 or chooses a random action with probability 0.2. We train the agent on two tasks, 1 = r  g and 2 = b (same as in Example 1). The regions are defined by the predicates r = (1 < x < 3)  (1 < y < 3) and g = (4 < x < 6)  (4 < y < 6). Because the coordinates are integers, a and b define a point goal rather than regions. 2 expresses a similar task for b = (1 < x < 3)  (6 < y < 8). Figure 1 shows the FSA for each task.
We apply standard tabular Q-learning Watkins (1989) on the FSA augmented MDP of this environment. For all experiments, we use a discount factor of 0.95, learning rate of 0.1, episode horizon of 200 steps, a random exploration policy and a total number of 2000 update steps which is enough to reach convergence (learning curve is not presented here as it is not the focus of this paper).
Figure 2 (a) and (b) show the learned optimal policies extracted by i (x, y, qi ) = arg amaxQi (x, y, qi , a). We plot i (x, y, qi ) for each qi and observe that each represents a sub-policy whose goal is given by Equation 7.
Figure 2 (c) shows the composed policy of  = 1  2 using Theorem 2. It can be observed that the composed policy is able to act optimally in terms maximizing the expected sum of discounted rewards given by Equation (12). Following the composed policy and transitioning the FSA in Figure 1 (b) will in fact satisfy  (-AN D-). As discussed in the previous section, if the -OR- task is desired, following the same composed policy and terminate at any of the states q,2, q,4, q,5, q,6, q,f will satisfy  = 1  2.

7

Under review as a conference paper at ICLR 2019
Figure 3 : The FSA for (a):traverse = (r  (g  b)) b interrupt = (hand in sight  h) U b. The subscripts 1 and 2 are dropped for clarity.
Figure 4 : (a): The upper figure shows our simulation environment in V-REP Rohmer et al. (2013) and the lower shows the corresponding experimental environment. (b) An execution trace of policy  where  = traverse  interrupt. 6.2 ROBOTIC MANIPULATION In this sub-section, we test our method on a more complex manipulation task. 6.2.1 EXPERIMENT SETUP Figure 4 (a) presents our experiment setup. Our policy controls the 7 degree-of-freedom joint velocities of the right arm of a Baxter robot. In front of the robot are three circular regions (red, green, blue plates) are it has to learn to traverse in user specified ways. The positions of the plates are tracked by motion capture systems and thus fully observable. In addition, we also track the position of one of the user's hands (by wearing a glove with trackers attached). Our MDP state space is 22 dimensional that includes 7 joint angles, xyz positions of the three regions (denoted by pr, pg, pb), the user's hand (ph) and the robot's end-effector (pee). We define the following predicates
1. i = ||pi - pee|| < , i  {r, g, b, h} where is a threshold which we set to be 5 centimeters. i constrains the relative distance between the robot's end-effector and the selected object. 8

Under review as a conference paper at ICLR 2019

2. hand in sight = (xmin < phx < xmax)  (ymin < phy < ymax)  (zmin < pzh < zmax). This predicate evaluates to true if the user's hand appears in the cubic region defined by [xmin, xmax, ymin, ymax, zmin, zmax]. In this experiment, we take this region to be 40 centimeters above the table (length and width the same as the table).
We test our method on the following composition task
1. traverse = (r  (g  b)) Description: traverse the three regions in the order of red, green, blue.
2. interrupt = (hand in sight  h) U b Description: before reaching the blue region, if the user's hand appears in sight, then eventually reach for the user's hand and the blue region, otherwise just reach for the blue region.
3.  = traverse  interrupt Description: conjunction of the first two tasks.
The FSAs for 1 and 2 are presented in Figure 3 . The FSA for  (14 nodes and 72 edges) is not presented here due to space constraints.

6.2.2 IMPLEMENTATION DETAILS
Our policy and Q-function are represented by a neural network (3 layers each with 100 ReLU units). For tasks traverse and interrupt, the input state space is 23 dimensional (22 continuous dimensional MDP state and 1 discrete dimension for the automaton state). For task , the state space is 24 dimensional (2 discrete dimensions for automaton states of traverse and interrupt). We use soft Q-learning (SQL) (Haarnoja et al., 2017) to learn the optimal policies and Q-functions for traverse and interrupt and Theorem 2 to compose the Q-functions. The policy for task  is extracted from the composite Q-function by running the same algorithm on already collected experience without updating the Q-function (similar procedure as (Haarnoja et al., 2018)). All of our training is performed in simulation and the policy is able to transfer to the real robot without further fine-tuning. After each episode, the joint angles, position of the plates as well as position of the hand (represented by the yellow sphere in Figure 4 (a)) are randomly reset (within certain boundaries) to ensure generalization across different task configurations. The robot is controlled at 20 Hz. Each episode is 100 time-steps (about 5 seconds).

6.2.3 RESULTS AND DISCUSSION

In Figure 5 we report the aver-

age return (sum of discounted re-

wards averaged over 5 episodes) as

a function of update steps for task

. Since skill composition does

not need any exploration, 5 eval-

uation episodes are collected after

each update of the policy to calcu-

late the average return. As compar-

ison, we also learn the same task

from scratch using SQL. We can see

that our composition method takes

much less update steps to reach a

satisfying policy. It is important to

note that the wall time for learning

a policy from scratch is significantly longer than that from composition.

Figure 5 : Episode return averaged over 5 runs.

For robotic tasks with relatively sim-

ple policy representations, learning

speed is dominated by the time to

collect experience. Since skill composition uses the already collected experience, obtaining a policy

can be much faster. Table 1 shows the average time to convergence (over 5 random seeds) for each

task.

9

Under review as a conference paper at ICLR 2019

Table 1: Results on Training and Evaluation Performance

traverse interrupt  (learned)  (composed)

Average training time (Hour) 2.3

Average task success rate

80%

2.5 75%

3.5 3%

0.1 70%

In Table 1 we also show the average task success rate evaluated on the robot over 20 trials. Task success is evaluated by calculating the robustness of the trajectories resulting from each policy. A robustness of greater than 0 evaluates to success and vice versa.  learned from scratch fails to complete the task even though a convergence is reached. This is likely due to the large FSA with complex per-step reward (Dq in Equation (7)) that results from  which makes learning difficult. Figure 4 (b) shows an evaluation run of the composed policy for task .
7 CONCLUSION
We provide a technique that takes advantage of the product of finite state automata to perform deterministic skill composition. Our method is able to synthesize optimal composite policies for -AN D- and -OR- tasks. We provide theoretical results on our method and show its effectiveness on a grid world simulation and a real world robotic task. For future work, we will adapt our method to the more general case of task-space transfer - given a library of optimal policies (Qfunctions) that each satisfies its own specification, construct a policy that satisfies a specification that's an arbitrary (temporal) logical combination of the constituent specifications.
REFERENCES
Derya Aksaray, Austin Jones, Zhaodan Kong, Mac Schwager, and Calin Belta. Q-learning for robust satisfaction of signal temporal logic specifications. In Decision and Control (CDC), 2016 IEEE 55th Conference on, pp. 6565­6570. IEEE, 2016.
Christel Baier and Joost-Pieter Katoen. Principles of model checking. MIT press, 2008.
Calin Belta, Boyan Yordanov, and Ebru Aydin Gol. Formal Methods for Discrete-Time Dynamical Systems. Springer, 2017.
Marco Da Silva, Fre´do Durand, and Jovan Popovic´. Linear bellman combination for control of character animation. Acm transactions on graphics (tog), 28(3):82, 2009.
Georgios E Fainekos, Hadas Kress-Gazit, and George J Pappas. Hybrid controllers for path planning: A temporal logic approach. In Decision and Control, 2005 and 2005 European Control Conference. CDC-ECC'05. 44th IEEE Conference on, pp. 4885­4890. IEEE, 2005.
Georgios E Fainekos, Savvas G Loizou, and George J Pappas. Translating temporal logic to controller specifications. In Decision and Control, 2006 45th IEEE Conference on, pp. 899­904. IEEE, 2006.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In ICML, 2017.
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey Levine. Composable deep reinforcement learning for robotic manipulation. arXiv preprint arXiv:1803.06773, 2018.
Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, and Joshua B. Tenenbaum. Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. preprint arXiv:1604.06057, 2016.
Xiao Li, Yao Ma, and Calin Belta. Automata guided reinforcement learning with demonstrations. arXiv preprint arXiv:1809.06305, 2018.
10

Under review as a conference paper at ICLR 2019
Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Exampleguided deep reinforcement learning of physics-based character skills. CoRR, abs/1804.02717, 2018.
Eric Rohmer, Surya P. N. Singh, and Marc Freese. V-rep: A versatile and scalable robot simulation framework. 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 1321­1326, 2013.
John Schulman, Pieter Abbeel, and Xi Chen. Equivalence between policy gradients and soft qlearning. CoRR, abs/1704.06440, 2017.
Paulo Tabuada and George J Pappas. Linear temporal logic control of linear systems. IEEE Transactions on Automatic Control, 2004.
Emanuel Todorov. Compositionality of optimal control laws. In Advances in Neural Information Processing Systems, pp. 1856­1864, 2009.
Benjamin van Niekerk, Steven James, Adam Christopher Earle, and Benjamin Rosman. Will it blend? composing value functions in reinforcement learning. CoRR, abs/1807.04439, 2018.
C Vasile. Github repository, 2017. Christopher John Cornish Hellaby Watkins. Learning From Delayed Rewards. PhD thesis, King's
College, Cambridge, England, 1989. Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi,
and Ali Farhadi. Visual semantic planning using deep successor representations. arXiv preprint ArXiv:1705.08080, pp. 1­13, 2017.
11

