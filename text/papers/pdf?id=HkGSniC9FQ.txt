Under review as a conference paper at ICLR 2019
AN ANALYSIS OF COMPOSITE NEURAL NETWORK PERFORMANCE FROM FUNCTION COMPOSITION PER-
SPECTIVE
Anonymous authors Paper under double-blind review
ABSTRACT
This work investigates the performance of a composite neural network, which is composed of pre-trained neural network models and non-instantiated neural network models, connected to form a rooted directed graph. A pre-trained neural network model is generally a well trained neural network model targeted for a specific function. The advantages of adopting such a pre-trained model in a composite neural network are two folds. One is to benefit from other's intelligence and diligence, and the other is saving the efforts in data preparation and resources and time in training. However, the overall performance of composite neural network is still not clear. In this work, we prove that a composite neural network, with high probability, performs better than any of its pre-trained components under certain assumptions. In addition, if an extra pre-trained component is added to a composite network, with high probability the overall performance will be improved. In the empirical evaluations, distinctively different applications support the above findings.
1 INTRODUCTION
Deep learning has been a great success in dealing with natural signals, e.g., images and voices, as well as artifact signals, e.g., nature language, while it is still in the early stage in handling sophisticated social and natural applications shaped by very diverse factors (e.g., stock market prediction), or resulted from complicated processes (e.g., pollution level prediction). One of distinctive features of the complicated applications is their applicable data sources are boundless. Consequently, their solutions need frequent revisions. Although neural networks can approximate arbitrary functions as close as possible (Hornik, 1991), the major reason for not existing such competent neural networks for those complicated applications is their problems are hardly fully understood and their applicable data sources cannot be identified all at once. By far the best practice is the developers pick a seemly neural network with available data to hope for the best. The apparent drawbacks, besides the performance, are the lack of flexibility in new data source emergence, better problem decomposition, and the opportunity of employing proven efforts from others. On the other hand, some adopts a composition of several neural network models, based on function composition using domain knowledge.
An emerging trend of deep learning solution development is to employ well crafted pre-trained neural networks (i.e., neural network models with instantiated weights), especially used as a component in a composited neural network model. Most popular pre-trained neural network models are well fine tuned with adequate training data, and made available to the public, either free or as a commercial product. During the training phase of composite neural network, the weights of pre-trained models are frozen to maintain its good quality and save the training time, while the weights of their outgoing edges are trainable. In some cases as in the transfer learning, the weights of pre-trained neural network are used as initial values in the training phase of composite neural network. It is intuitive that a composite neural network should perform better than any of its components. The ensemble learning (Freund & Schapire, 1997; Zhou, 2012) and the transfer learning (Galanti et al., 2016) have great success and are popular when pre-trained models are considered. However, the following example shows some aspects missed by these two methods, and requests for more complicated composite function.
1

Under review as a conference paper at ICLR 2019

Example 1. Assume there is a set of locations indexed as X = {(0, 0), (0, 1), (1, 0), (1, 0)}

with the corresponding values Y = (0, 1, 1, 0). Obviously, the observed function is the XOR

(Goodfellow et al., 2016). Now consider three models: f1(x1, x2) := x1, f2(x1, x2) := x2, and

f3(x1, x2) := x1x2. Their corresponding output vectors are (0, 0, 1, 1), (0, 1, 0, 1), (0, 0, 0, 1) with

bit-wise accuracy 50%, 50%, 25%, respectively. This means that the AdaBoosting algorithm will

exclude

f1

and

f2

in

the

ensemble

since

their

coefficients

are

1 2

ln

1-50% 50%

=

0.

On

the

other

hand,

in

the transfer learning, f3 is fine-tuned by applying the gradient descent method with respect to L2 loss

on wf3 = wx1x2 to transfer the source task distribution to that of the target task. The result comes to

w = 0, and f3 is excluded. Now consider g1(x1, x2) = 1f1 + 2f2 and apply the back-propagation

method with

respect

to

the L2 loss.

The

results are

1

=

2

=

1 3

,

with loss

4 3

.

If

further

define

g2(x1, x2) = w1g1 + w2f3, the back-propagation yields g2 = 3g1 - 2f3 = x1 + x2 - 2x1x2 with the

output (0, 1, 1, 0). The final g2 computes Y with loss 0. This example shows the power of composite

function.

Composite Neural Network. In the transfer learning, how to overcome the negative transfer (a phenomenon of a pre-trained model has negative impact on the target task) is an important issue (Seah et al., 2013). In the ensemble learning, it is well known that the adding more pre-trained models, it is not always true to have the better accuracy of the ensemble (Zhou et al., 2002). Furthermore, Opitz & Maclin (1999) pointed that the ensemble by boosting having less accuracy than a single pre-trained model often happens for neural networks. In the unsupervised learning context, some experimental research concludes that although layer-wise pre-training can be significantly helpful, on average it is slightly harmful (Goodfellow et al., 2016). These empirical evidences suggest that in spite of the success of the ensemble learning and the transfer learning, the conditions that composite neural network can perform better is unclear, especially in the deep neural networks training process. The topology of a composite neural network can be represented as a rooted directed graph. For instance, an ensemble learning can be represented as 1-level graph, while a composite neural network with several pre-trained models that each is designed to solve a certain problem corresponds to a more complicated graph. It is desired to discover a mathematical theory, in addition to employing domain knowledge, to construct a composite neural network with guaranteed overall performance. In this work, we investigate the mathematical theory to ensure the overall performance of a composite neural network is better than any a pre-trained component, regardless the way of composition, to allow deep learning application developer great freedom in constructing a high performance composite neural network.

Contributions. In this work, we proved that a composite neural network with high probability performs better than any of its pre-trained components under certain assumptions. In addition, if extra pre-trained component is added into a composite network, with high probability the overall performance will be improved. In the empirical evaluations, distinctively different applications support the above findings.

2 PRELIMINARIES

In this Section, we introduce some notations and definitions about composite neural network. Pa-

rameters N ,K, d, dj, dj1 , and dj2 are positive integers. Denote {1, ..., K} as [K] and [K]  {0} as [K]+. Let  : R  R be a differentiable activation function, such as the Logistic function (z) = 1/(1 + e-z) and the hyperbolic tangent (z) = (ez - e-z)/(ez + e-z). For simplicity of

notation, we sometimes abuse  as a vector value function. A typical one hidden layer neural network

can be formally presented as w1,1

d i=1

w0,ixi

+

w0,0

+ w1,0.We abbreviate it as f,W(x),

where W is the matrix defined byw1,1, w1,0, ..., w0,1, w0,0. Recursively applying this representation
can obtain the neural network with more hidden layers. If there is no ambiguity on the activation function, then it can be skipped as fW(x). Now assume a set of neural networks {fWj (xj)}Kj=1 is given, where Wj is the real number matrix defining the neural network fWj : Rdj1 ×dj2  Rdj , and xj  Rdj1 ×dj2 is the input matrix of the jth neural network. For different fWj , the corresponding
dj , dj1 and dj2 can be different. For each j  [K], let Dj = {(xj(i), yj(i))  R(dj1 ×dj2 )×dj }Ni=1
be a set of labeled data (for the jth neural network). For each i  [N ], let x(i) = (x(1i), . . . , x(Ki)), y(i) = (y1(i), . . . , yK(i)), and D = {(x(i), y(i))}Ni=1.

2

Under review as a conference paper at ICLR 2019

For a pre-trained model (component), we mean Wj is fixed after its training process, and then
we denote fWj as fj for simplicity. On the other hand, a component fWj is non-instantiated means Wj is still free. A deep feedforward neural network is a hierarchical acyclic graph, i.e.
a directed tree. In this viewpoint, a feedforward neural network can be presented as a series of function compositions. For given {fWj (xj)}jK=1, we assume j  Rdj , j  [K], which make the
product jfWj (xj) is well-defined. Denote f0 as the constant function 1, then the liner combination with a bias is defined as as (f1, ..., fK ) = j[K]+ jfj(xj). Hence, an L layers of neural
network can be denoted as (L)    · · ·  (0) (x). A composite neural network defined by
components fWj (xj) can be designed as an directed tree. For instance, a composite neural network 2 (1,0 + 1,1f4(x4) + 1,21(0,0 + 0,1f1(x1) + 0,2fW2 (x2) + 0,3f3(x3))) can be denoted as 2  1 (f4, 1  0(f1, fW2 , f3)), where f1 and f3 are pre-trained and fW2 is non-instantiated. Note that in this work Dj is the default training data of component fj of composite neural network,
but Dj can be different from the training data deciding the frozen weights in the pre-trained fj.

Let a, b be the standard inner product of a and b, and || · || be the corresponding norm. For a composite neural network, the training algorithm is the gradient descent back-propagation algorithm and the loss function is the L2-norm of the difference vector. In particular, for a composite neural network g the total loss on the data set D is

L x; g = g (x) - y, g (x) - y = ||g (x) - y||2

(1)

This in fact is

N i=1

g(x(i)) - y(i)

2
.

By

the

definition

of

g (·),

this

total

loss

in

fact

depends

on

the

given data x, the components defined by {j}Kj=1, the output activation , and the weight vector w.

Similarly, let L(fj(xj)) be the loss function of a single component fi. Our goal is to find a feasible  s.t. L (x; g) < minj[K] L(fj(xj)).

3 PROBLEM SETTINGS AND RESULTS OVERVIEW

The problems considered in this work are as follows:

P1. What are the conditions that the pre-trained components must satisfy so that them can strictly improve the accuracy of the whole composition?
P2. Will more pre-trained components improve the accuracy of the whole composition?

Let fj be the output vector of the jth pre-trained component, and BK be the set of unit vectors in RK .

A1. Linearly Independent components (LIC) Assumption: t  [K], {j }  R, s.t. ft = j[K]\{t} j fj .

A2. No Perfect component (NPC) Assumption:

minj[K]

i[N] fj (x(ji)) - y(i) > , where  > 0 is constant.

Our results are as follows:

Theorem 1. Assume the set of components {fj(xj)}Kj=1 satisfies LIC. Let g be (f1, ..., fK ). With

probability

at

least

1

-

K eN

,

there

is

a

vector





RK

\

BK

s.t.

L

(x; g)

<

minj[K]{L(fj (xj ))}.

Theorem 2. Assume the set of pre-trained components {fj(xj)}jK=1 satisfies both NPC and LIC,

and g

be   (f1, ..., fK ).

Then with probability at least 1 -

K eN

there exists w s.t.

Lw (x; g) <

minj[K] L(fj (xj )).

Theorem 3. Assume the set of components {fj(xj)}jK=1 satisfies LIC. Let gK-1 = (f1, ...fK-1)

and gK

=

(f1, ...fK ).

With probability at least 1 -

K eN

, there is a vector w



RK

\ BRK

s.t.

Lw (x; gK ) < Lw (x; gK-1).

Theorem 1, and 2 together answer Problem P1, and Theorem 3 answers Problem P2.

3

Under review as a conference paper at ICLR 2019

4 RELATED WORK
Our framework is related but not the same with the models such as transfer learning. (Erhan et al., 2010; Kandaswamy et al., 2014; Yao & Doretto, 2010) and ensemble leaning (Zhou, 2012).
Transfer Learning. Typically transfer learning deals with two data sets with different distributions, source and target domains. A neural network, such as an auto-encoder, is trained with source domain data and corresponding task, and then part of its weights are taken out and plugged into other neural network, which will be trained with target domain data and task. The transplanted weights can be kept fixed during the consequent steps or trainable for the fine-tune purpose (Erhan et al., 2010). For multi-source transfer, algorithms of boosting based are studied in the paper (Yao & Doretto, 2010). Kandaswamy et al. (Kandaswamy et al., 2014) proposed a method of cascading several pre-trained layers to improve the performance. Transfer learning is considered as a special case of composite neural network that the transfered knowledge can be viewed as a pre-trained component.
Ensemble (Bagging and Boosting). Since the Bagging needs to group data by sampling and the Boosting needs to tune the probability of data (Zhou et al., 2002), these frameworks are different from composite neural network. However, there are fine research results revealing many properties for accuracy improvement (Dzeroski & Z enko, 2004; Gashler et al., 2008; Zhou et al., 2002). For example, it is known that in the ensemble framework, low diversity between members can be harmful to the accuracy of their ensemble (Dzeroski & Z enko, 2004; Gashler et al., 2008). In this work, we consider neural network training, but not data processing.
Ensemble (Stacking). Among the ensemble methods, the stacking is closely related to our framework. The idea of stacked generalization (Wolpert, 1992), in Wolpert's terminology, is to combine two levels of generalizers. The original data are taken by several level 0 generalizers, then their outputs are concatenated as an input vector to the level 1 generalizer. According to the empirical study of Ting and Witten (Ting & Witten, 1999), the probability of the outputs of level 0, instead of their values, is critical to accuracy. Besides, multi-linear regression is the best level 1 generalizer, and non-negative weights restriction is necessary for regression problem while not for classification problem. In (Breiman, 1996), it restricts non-negative combination weights to prevent from poor generalization error and concludes the restriction of the sum of weights equals to 1 is not necessary (Breiman, 1996). In (Hashem, 1997), Hashem showed that linear dependence of components could be, but not always, harmful to ensemble accuracy, while in our work, it allows a mix of pre-defined and undefined components as well as negative weights to provide flexibility in solution design.
Recently Proposed Frameworks. In You et al. (2017), Shan You et al. proposed a student-teacher framework where the outputs of pre-trained teachers are averaged as the knowledge for the student network. A test time combination of multiple trained predictors was proposed by Kim, Tompkin, and Richardt In Kim et al. (2017), that the combination weights are decided during test time. In above frameworks, the usage of pre-trained neural networks generally improves the accuracy of their combination.

5 THEORETICAL ANALYSIS

This section provides analyses of the loss function of composite neural network with the introduction
of pre-trained components. For the complete proofs, please refer to Supplementary Material. Observe that for given pre-trained components {fj}Kj=1, a composite neural network can be defined recursively by postorder subtrees search. For instance, 2  1 (f4, 1  0(f1, f2, f3)) can be presented as 2  1(f4, g1), and g1 = 1  0(f1, f2, f3). Without loss of generality, we assume dj = d = 1
for all j  [K] in the following proofs. We denote fj the vector (fj(x(1)), · · · , fj(x(N))), as the sequence of fj during the training phase. Similarly, y := (y(1), · · · , y(N)). Let ej be an unit vector in the standard basis of RK for each j  [K], i.e. e1 = (1, 0, · · · , 0) and e2 = (0, 1, 0, · · · , 0), etc. Let BK be the set containing all these standard unit-length basis of RK .

Theorem 1. Assume the set of components {fj(xj)}jK=1 satisfies LIC. Let g be (f1, ..., fK ). With

probability

at

least

1

-

K eN

,

there

is

a

vector





RK

\

BK

s.t.

L

(x; g)

<

minj[K]{L(fj (xj ))}.

4

Under review as a conference paper at ICLR 2019

Proof. (Proof Sketch) The whole proof is split to Lemma 5.1,5.2,5.3. Note that g(·) is the linear combination of  and {fj(xj)}Kj=1. It is well known (Friedman et al., 2001) that to search the
minimizer  for L, i.e. to solve a least square error problem, is equivalent to find an inverse matrix defined by {fj(xj)}jK=1. Since {fj(xj)}Kj=1 satisfy LIC, the inverse matrix can be written down concretely, which proves the existence. Furthermore, if this solved minimizer  is not es for some s  [K] then the g has lower loss than fs. Lemma 5.3 argues that the probability of  = es is at most the probability of the event f - y, f = 0, where f is uniformly taken from the vector set of
the same length of f - y.

The statements of Lemmas needed by previous Theorem are as follows. Lemma 5.1. There exists   RK+1 s.t. L x; (0)(f1, ...fK )  minj[K]+ {L(fj(xj))}.

This Lemma deals with the existence of the solution of the inequality. But our goal is to find a solution such that the loss is strictly less than any pre-trained component.

Lemma 5.2. Denote IL the indicator variable for the event that at least one of ej  BRK is the

minimizer of L. Then Pr

IL = 1

<

K eN

,

i.e.

Pr

IL = 0



1

-

K eN

.

2
Lemma 5.3. Define F(y, L(f )) = f  RN : f - y = Lf for given y and f . Then we have

Prf F(y,L(f ))

f - y, f

=0

<

1 eN

.

The above Lemmas prove Theorem 1. The following corollary is the closed from of the optimal weights.

Corollary 5.1. The closed form of the minimizer is:

-1

[t]t[K]+ =

fs, ft

×

s,t[K ]+

fs, y

.

s[K ]+

In the following, we deal with   (f1, ..., fK ) and 1    (f1, ..., fK ).

Theorem 2. Assume the set of pre-trained components {fj(xj)}jK=1 satisfies both NPC and LIC,

and g be   (f1, ..., fK ).

Then with probability at least 1 -

K eN

there exists  s.t.

L (x; g)

<

minj[K] L(fj (xj )).

Proof. (Proof Sketch) The whole proof is split to Lemma 5.4,5.5, and 5.6. The idea is to find an interval in the domain of  such that the output can approximate linear function as well as possible. Then in this interval, the activation  can approximate any given pre-trained component. However, under the assumptions LIC and NPC the gradient of the loss L is not zero with high probability. Since the training is based on the gradient descent algorithm, this none-zero gradient leads the direction of updating process to obtain a lower loss.

Lemma 5.4. Let N, K and j  [K] be fixed. For small enough , there exists   ZF,1, and

0

<





R

s.t.

|



(0)(f1,

..., fK )

-

fj (x) 

|

<

.

Lemma 5.5. Assume NPC holds with  > 0. If  /3 satisfies |  (0)(f1, ..., fK )(x) - fj(x)| <


3N

for any j



[K]+, then L(

/3)

=

0.

Lemma 5.6. If  /3 makes L( /3) = 0, then there exist  s.t. L (x; g) < minj[K]+ L(fj (xj )).

Now we consider the difference of losses of    (f1, ..., fK ) and   (f1, ..., fK-1).

Theorem 3. Assume the set of components {fj(xj)}jK=1 satisfies LIC. Let gK-1 = (f1, ...fK-1)

and gK

=

(f1, ...fK ).

With probability at least 1 -

K eN

, there is a vector 



RK

\ BRK

s.t.

L (x; gK ) < L (x; gK-1).

5

Under review as a conference paper at ICLR 2019

Table 1: Validation Error of Image Classification

number of parameter Validation Error (%)

ResNet50
25636712 70.1184

SIFT
2884200 13.1252

composite
3000 71.5079

Proof. (Proof Sketch) The idea is directly solve the inequality for the case of K = 2, and then generalize the result to larger K.
The following provides a generalized error bound for a composite neural network. Theorem 4. Assume pre-trained components {fj}Kj=1 satisfy LIC and NPC. Let {GE(fj)}jK=1 be corresponding generalization errors of {fj}Kj=1, and (L)  (L)  · · ·  (1)  (0)(f1, ..., fK ) be the composite neural network. Denote the generalization error, E{L((L)  (L)  · · ·  (1)  (0)(f1, ..., fK ))}, of the composite neural network as E{L,f1,...,fK }. Then with high probability, there exist a setting of {(L), ..., (0)} such that E{L,f1,...,fK }  (L)(GE(f1), ...GE(fK )).
Proof. (Proof Sketch) We apply the idea similar to Kawaguchi (2016): the exception of non-liner activations is same with the exception of liner activations. Previous theorems provide that with high probability there exists the solution of (i), i  [L]+ s.t. each (i)+1(i) approximates a degree one polynomial A(i)+1(i),1 as well as possible. If the weights are obey the normal distribution, then E{L,f1,...,fK }  (L)(GE(f1), ...GE (fK )).
6 EMPIRICAL STUDIES
This section is to numerically verify the performance of composite network for two distinctively different applications, image classification and PM2.5 prediction. For image classification, we examined two pre-trained components, the ResNet50 (He et al., 2016) from Keras and the SIFT algorithm(Lowe, 1999) from OpenCV, running on the benchmark of ImageNet competition(Russakovsky et al., 2015). For PM2.5 prediction, we implemented several models running on the open data of local weather bureau and environment protection agency to predict the PM2.5 level in the future hours.
6.1 IMAGENET CLASSIFICATION
We chose Resnet50 as the pre-trained baseline model and the SIFT model as an auxiliary model to form a composite neural network to validate the proposed theory. The experiments are conducted on the 1000-class single-label classification task of the ImageNet dataset, which has been a well received benchmark for image classification applications. A reason to choose the SIFT (Scale-Invariant Feature Transform) algorithm is that its function is very different from ResNet and it is interesting to see if the performance of ResNet50 can be improved as predicted from our theory.
We trained the SIFT model using the images of ImageNet, and directed the output to a CNN to extract useful features before merging with ResNet50 output. In the composite model, the softmax functions of both ResNet50 and SIFT model are removed that the outputs of length 1000 of both models are merged before the final softmax stage. During the training process of composite network, the weights of ResNet50 and SIFT model are fixed, and only the connecting weights and bias are trained.
The ResNet50 was from He et al. that its Top-1 accuracy in our context was lower than reported in (He et al., 2016) since we did not do any fine tuning and data preprocessing. In the Figure 1, it shows the composite network has higher accuracy than ResNet50 during almost the complete testing run. Table 1 shows the same result that the composite network performs better too. The experiment results support the claims of this work that a composite network performs better than any of its components, and more components work better than less components.
6

Under review as a conference paper at ICLR 2019

Figure 1: Image Classification Validation Accuracy

6.2 PM2.5 PREDICTION

The PM2.5 prediction problem is to forecast the particle density of fine atmospheric matter with the diameter at most 2.5 µm (PM2.5) in the future hours, mainly, for the next 12, 24, 48, 72
hours. The datasets used are open data provided by two sources including Environmental Protection Administration (EPA)1 , and Center Weather Bureau (CWB)2. The EPA dataset contains 21 observed
features, including the speed and direction of wind, temperature, relative humidity, PM2.5 and PM10
density, etc., from 18 monitoring stations, with one record per hour. The CWB has seventy monitoring
stations, one record per 6 hours, containing 26 features, such as temperature, dew point, precipitation, wind speed and direction, etc. We partitioned the observed area into a grid of 1140 km2 with 1 km×1
km blocks and aligned the both dataset into one-hour period. We called the two datasets as air quality
and weather condition dataset.

We selected ConvLSTM (Convolution LSTM) and FNN (fully connected neural network) as the components used in this experiment. The reason to select ConvLSTM is that the dispersion of PM2.5 is both spatially and temporally dependent and ConvLSTM is considered capable of catching the dependency, and FNN is a fundamental neural network that acts as the auxiliary component in the experiment.

The prediction models were trained with the data of 2014 and 2015 years, then the 2016 data was used for testing. We considered two function compositions, the linear combination  and the Logistic function 1 (as Theorem 2), to combine the two components to examine the applicability of the proposed theorems.

We trained and tested both ConvLSTM and FNN using air quality dataset (Dataset A) and weather condition dataset (Dataset B) separately as the baselines (denoted as f1, f2, f3 and f4) and their training error and testing error in MSE are list in the first part of Table 2. Then we composited FNNs using Dataset A and Dataset B, each FNN can be pre-trained (denoted as x) or non-instantiated (denoted as o). In addition, we used both linear and Sigmoid activation functions. As a result, we had eight combinations, as list in the part two. We treated ConvLSTM in the same way and the outcomes were in the part 3. Finally, we composited using one FNN and one ConvLSTM that each was the best in their category, and the resulting composite network was a tree of depth 2. For instance, the candidate of ConvLSTM of part 4 for 12 hours prediction was the 4th row (i.e., (f3,f4)) of part 3. Their training and testing errors in MSE were listed in the part 4.

From the empirical study results, it shows mostly the proposed theorems are followed. While the

composite networks with all pre-trained components may not perform better than others in their

category, (which is not a surprise), what we expect to see is after adding a new component, the

composite network has improvement over the previous one. For example, the   (f3×, f4×) has

strictly better accuracy than both f3 and f4 for all future predictions. Another example is the

NEXT 48 hr,   (C×, F ×) F =   (f3, f4).

also

has

strictly

better

accuracy

than

both

C

=





(f3, f4)

and

1https://opendata.epa.gov.tw/Home 2http://opendata.cwb.gov.tw/index

7

Under review as a conference paper at ICLR 2019

Table 2: Training and Testing Errors of PM2.5 Prediction

Model

Next 12 hr

Next 24 hr

Next 48 hr

Next 72 hr

(input)

TarinError TestError TarinError TestError TarinError TestError TarinError TestError

f1: FNN-A f2: FNN-B f3: ConvLSTM-A f4: ConvLSTM-B
(f1×,f2×) (f1×,f2) ((ff11,,ff22×))   (f1×,f2×)   (f1×,f2)   (f1,f2×)   (f1,f2)
(f3×,f4×) (f3×,f4) (f3,f4×) (f3,f4)   (f3×,f4×)   (f3×,f4)   (f3,f4×)   (f3,f4)
(C×,F ×) (C×,F ) (C,F ×) (C,F )   (C×,F ×)   (C×,F )   (C,F ×)   (C,F )

100.1812 134.1137
54.2775 67.8625
99.7005 95.6804 95.8110 101.1584 102.7556 98.1241 94.9931 98.2596
27.1760 27.1519 27.4436 26.3063 28.3844 27.5981 26.4125 26.5131
26.6556 24.2349 22.7651 21.9103 26.5950 24.0223 22.4443 38.4899

92.8528 120.0019
88.8156 118.4351
F :90.0214 93.0173 93.1131 90.2671 90.6280 93.1098 91.4667 91.3646
85.8922 81.7688 78.5360 C:70.8670 84.9029 80.7848 78.3990 75.5778
70.1159 67.4414 75.9468 68.0660 69.1897 H:66.4733 83.5953 67.1819

134.7095 139.8016 57.5677 73.1519
130.7800 120.3185 121.9737 126.6807 133.1453 127.4999 124.5461 124.4182
49.1624 42.7932 44.3214 40.1879 43.2981 44.4197 42.3181 42.3912
34.6885 31.7202 24.2132 20.9072 34.7400 28.8401 22.5040 17.6041

118.6065 128.5960 111.9122 125.6062
115.9283 117.9781 117.7771 114.5264 117.7397 118.8107 117.7332 F :114.2274
108.3157 104.2375 107.0898 100.8474 109.3709 98.1051 103.3361 C:94.6242
92.2737 90.7795 96.3126 91.9323 92.4715 H:90.7257 96.4027 92.2343

141.6287 136.7693
74.8937 68.7069
139.9744 134.3893 134.0676 132.6726 135.9256 135.1553 131.2684 134.5078
37.2116 33.2831 31.4910 25.3312 31.2413 29.7649 30.4183 27.5812
29.6484 30.1328 24.4488 23.2868 29.9215 28.5033 28.4714 33.9710

136.8358 134.9001 129.7418 137.0789
F :132.4764 134.0270 135.2255 132.8069 133.2544 134.1469 135.1281 132.8316
123.2186 C:110.4213
129.1829 119.1634 123.0041 111.6793 128.4138 112.8075
107.6833 H:105.1804
114.1803 112.8605 108.7482 108.8896 112.0727 105.7977

148.1807 142.7637
77.7394 84.9656
144.6826 139.6226 136.2009 139.2339 145.1052 137.7562 140.1604 138.0456
60.6415 74.3055 68.4413 76.1782 63.7286 69.3182 64.4193 70.5132
52.0060 46.9994 49.0663 30.6875 52.3708 46.2711 35.3947 40.2934

143.9980 140.8650 132.8923 138.6642
F :137.8403 140.5209 144.0116 139.3322 139.2513 142.1778 144.5220 139.8351
131.0565 C:110.9952
139.5661 120.6814 130.4122 117.0719 136.6043 117.6480
H:110.1283 111.2227 117.2747 113.6968 111.5474 110.1613 114.5947 110.3585

Let f5 be non-instantiated CNN with future rain fall input.

  (H×,f5)

24.1487 65.5776 30.6243

  (H,f5)

58.0852 68.8111 22.8776

87.2777 90.2324

55.9261 39.0996

102.2878 112.1639

50.5158 36.0659

108.8087 109.8240

Part 1: pre-trained components fi, i  [4]. Part 2: composite f1 and f2 by linear (·) or logistic   (·); similar for Part 3-5 ×: un-trainable component, i.e. pre-trained. : trainable component (original weights was deleted).

The best model of Part 2 (/Part 3) was assigned as composite model F (/C) which will be used in Part 4.

The best model of Part 4 was assigned as composite model H which will be used in Part 5.

7 CONCLUSION
In this work, we investigated the composite neural network with pre-trained components problem and showed that the overall performance of a composite neural network is better than any of its components, and more components perform better than less components. In addition, the developed theory consider all differentiable activation functions.
While the proposed theory ensures the overall performance improvement, it is still not clear how to decompose a complicated problem into components and how to construct them into a composite neural network in order to have an acceptable performance. Another problem worth some thinking is when the performance improvement will diminish (by power law or exponentially decay) even adding more components. However, in the real world applications, the amount of data, data distribution and data quality will highly affect the performance.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Leo Breiman. Stacked regressions. Machine learning, 24(1):49­64, 1996.
Saso Dzeroski and Bernard Z enko. Is combining classifiers with stacking better than selecting the best one? Machine learning, 54(3):255­273, 2004.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625­660, 2010.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119­139, 1997.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics New York, 2001.
Tomer Galanti, Lior Wolf, and Tamir Hazan. A theoretical framework for deep transfer learning. Information and Inference: A Journal of the IMA, 5(2):159­209, 2016.
Mike Gashler, Christophe Giraud-Carrier, and Tony Martinez. Decision tree ensemble: Small heterogeneous is better than large homogeneous. In Machine Learning and Applications, 2008. ICMLA'08. Seventh International Conference on, pp. 900­905. IEEE, 2008.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT Press, 2016.
Sherif Hashem. Optimal linear combinations of neural networks. Neural networks, 10(4):599­614, 1997.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, second edition, 2012.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2): 251­257, 1991.
Kwang In Kim, James Tompkin, and Christian Richardt. Predictor combination at test time. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3553­3561, 2017.
Chetak Kandaswamy, Lu´is M Silva, Lu´is A Alexandre, Ricardo Sousa, Jorge M Santos, and Joaquim Marques de Sa´. Improving transfer learning accuracy by reusing stacked denoising autoencoders. In Systems, Man and Cybernetics (SMC), 2014 IEEE International Conference on, pp. 1380­1387. IEEE, 2014.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586­594, 2016.
Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for back-propagation. In Proceedings of the 1988 connectionist models summer school, pp. 21­28. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988.
David G Lowe. Object recognition from local scale-invariant features. In Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pp. 1150­1157. Ieee, 1999.
David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of artificial intelligence research, 11:169­198, 1999.
9

Under review as a conference paper at ICLR 2019
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211­252, 2015.
Chun-Wei Seah, Yew-Soon Ong, and Ivor W Tsang. Combating negative transfer from predictive distribution differences. IEEE transactions on cybernetics, 43(4):1153­1165, 2013.
Kai Ming Ting and Ian H Witten. Issues in stacked generalization. Journal of artificial intelligence research, 10:271­289, 1999.
David H Wolpert. Stacked generalization. Neural networks, 5(2):241­259, 1992. Yi Yao and Gianfranco Doretto. Boosting for transfer learning with multiple sources. In Computer
vision and pattern recognition (CVPR), 2010 IEEE conference on, pp. 1855­1862. IEEE, 2010. Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1285­1294. ACM, 2017. Zhi-Hua Zhou. Ensemble methods: foundations and algorithms. CRC press, 2012. Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: many could be better than all. Artificial intelligence, 137(1-2):239­263, 2002.
10

Under review as a conference paper at ICLR 2019

8 SUPPLEMENTARY MATERIAL

For self-contained, we list some common Taylor expansion in the following.

Logistic:

S(z) :=

1 1+e-z

=

1 2

+

1 4

z

-

1 48

z

3

+

1 480

z5

-

17 80640

z7

+

O(z9),

z

 R,

Hyperbolic Tan:

tanH(z) =

ez -e-z ez +e-z

=z-

1 3

z3

+

2 15

z5

+

O(z7),

|z|



 2

arcTan:

arctan(z)

=

z

-

1 3

z3

+

1 5

z5

+

+O(z7),

|z|



1.

Definition 1. Given an activation (z) and its Taylor expansion T(z), let A,D(z) be the truncated the monomials of degree at most D from T(z). We define A,D(z) as the D-degree Taylor approximation polynomial, and R,D+1(z) as the remainder part such that T(z) = A,D(z) + R,D+1(z).

For instance, if we set D = 3 then the Taylor expansion of Logistic function S(z) is separated as the

approximation

part

AS(z),3(z)

=

1 2

+

1 4

z

-

1 48

z3

and

the

remainder

part

RS(z),4(z)

=

1 480

z

5

+

O(z7

).

Proposition 8.1. (Error Bound of The Remainder)

Let S(z) be the Logistic function. Consider the approximation AS(z),D(z) and the remainder

RS(z),D+1(z) defined as above. For given



(0,

1 1000

)

and

D



N,

if

|z|

<

1/(D+2), then

|S(z) - AS(z),D(z)| = |RS(z),D+1(z)| < .

Proof. Note that if < 1 then for all D  N, 1/(D+1) < 1. If |z| < 1/3 and D = 1, then

|RS(z),D+1(z)| 

- 1 z3 + 1 z5 - 17 z7 + O(z9) 48 480 80640

1 <
24

<

.

The general case (D  2) can be proven by the same argument as above.

This Proposition means that for a suitable range of z, the Logistic function can be seen as a linear function with the error at most . Definition 2. For the Logistic activation (z) = S(z), > 0 and given polynomial degree D, we define ZD, = {z  R : |(z) - A,D(z)| < }. Furthermore, for given components {fj : j  [K]} = F , we consider the variable z = (f1, ..., fK ) and define
ZF,D, =   RK+1 : z = (f1, ..., fK ), |(z) - A,D(z)| < .
Observe that if the parameters , F ,and |F | = K are fixed, then ZF,D,  ZF,D+1,  RK+1.
8.1 FUNCTION COMPOSITION BY LINEAR COMBINATION
Recall that for a set of pre-trained components {fj(xj) : j  [K]}, (0) (f1, ...fK ) = j[K]+ 0,jfj, where f0 = 1. For simplicity, we consider (1)(z) = z. This means
(1)    (0) (f1, ...fK ) = 1,1 j[K]+ 0,j fj + 1,0. Theorem 1 is a consequence of the following lemmas:

Proof. (of Lemma 5.1)

For simplicity of notations, let g(x) = (0)(f1, ...fK ), hence g(x) = j[K]+ jfj(xj). Also

recall that L (x; g) =

N i=1

g(x(i)) - y(i)

2. To prove the existence of the minimizer, it is enough

to solve the equations of critical points, in the case of a quadratic object function. That is, to solve the

set of equations:

L (x; g) =

L L ,...,
0 K

T
= (0, . . . , 0)T ,

11

Under review as a conference paper at ICLR 2019

where for each s, t  [K]+,

L N =2
s i=1 

g(x(i)) - y(i)


N
· fs(x(i)) = 2 

 j fj (x(ji)) - y(i) · fs(x(i))

i=1 j[K]+



=2 

j fs, fj - fs, y  .

j [K ]+

Hence, to solve L (x; g) = 0 is equivalent to solve fs, ft s,t[K]+ × [t]t[K]+ =

fs, y , where
s[K ]+

fs, ft s,t[K]+ is a (K + 1) by (K + 1) matrix, [t]t[K]+ and

fs, y are both 1 by (K + 1).
s[K ]+

Note that linear independence of {fj}j[K]+ makes fs, ft
matrix (Horn & Johnson, 2012) , which means the inversion solved:

a positive-definite Gram

s,t[K ]+

-1

fs, ft

exists. Then  is

s,t[K ]+

-1

[t]t[K]+ =

fs, ft

×

s,t[K ]+

fs, y
s[K ]+

(2)

The above shows the existence of the critical points. On the other hand, since L (x; g) is the summation of square terms, i.e. paraboloid, the the critical points can only be the minimum.

The meaning of the gradient on a function surface is the direction that increases the function value
most efficiently. Hence, if the gradient is not the zero vector then the corresponding point can not be the minimizer of the function surface. Recall for any s  [k],

L t

t[K ]+

=es = 2

fs - y, ft

t[K]+ .

Before the proof of Lemma 5.2, we need the upper bound of the probability of some events. Note that y is defined according to the given training data, and for each j  [K]+ the length of fj - y,
i.e. f - y , is also given. The question is, for fixed y what is the probability of selected f is
perpendicular to f - y? A folklore approach is considering that {f = (f (x(1)), ..., f (x(N)))} obeys the normal distribution, and setting the mean of f (x(i)) as y(i) for each i  [N ]. In the following we propose another simple probability argument to obtain a loose upper bound.

Proof. (of Lemma 5.3) Observe that f - y, f = 0  (f - y)  f , which implies the angle

between

them,

(f

-y),f

,

is

in

the

interval

[

- 2

,

+ 2

] for small

 R+, as shown in the left part of

Figure 2. The red, orange, and blue vectors show three possibles of the pair of f and f - y. The

length of f - y is fixed since f and y are given, but the angle between f - y and y can decide whether

(f - y)  f . The gray circle collects all possible end-point of the vector f - y emission from the end-point of y. Although on the whole circle there are exactly two specific angles 3 can satisfy

(f - y)  f , we give a loose small interval with respect to . In particularly, we set 0 < < e-N .



Pr
f F(y,Lf )

(f-y),f = 2

 Pr
f F(y,Lf )

- 2



(f -y),f



+ 2

1 =  < eN .

Now we are ready to proof Lemma 5.2. 3That is, two points on the circumference, which is in fact measure zero on all possible angle [0, 2)
12

Under review as a conference paper at ICLR 2019

Figure 2: An illustration of Lemma 5.3 (the left) and Lemma 5.4 (the right)

Proof. (of Lemma 5.2) We denote A the event that at least one of ej  BRK is the minimizer of L() for convenience.

IL(w) = 1  the event A is true



L t

t[K ]+

=es = 2

fs - y, ft

t[K]+ = [0]K×1 for somes  [K]+

 f1 - y, f1 = 0  f1 - y, f2 = 0  · · ·  f1 - y, fK = 0

or · · · or fK - y, f1 = 0  fK - y, f2 = 0  · · ·  fK - y, fK = 0

2
Hence, for given y and L(fj) = fj - y ,   [K]+, we have

Pr IL(w) = 1 

Pr fj - y, f1 = 0  fj - y, f2 = 0  · · ·  fj - y, fK = 0

j [K ]+

K · Pr f1 - y, f1 = 0
K < eN , where the second inequality is based on the symmetry between fs and ft for any s, t  [K]+, and the last inequality is by Lemma 5.3.

Proof. (of Theorem 3) We start from a simple case: Claim:   R s.t.

(f1(xi) - yi)2 -

(f1(xi) + f2(xi) - yi)2 > 0.

i[N ]

i[N ]

Proof.

(f1(xi) - yi)2 -

(f1(xi) + f2(xi) - yi)2

i[N ]

i[N ]

= (f1(xi) - yi)2 - (f1(xi) + f2(xi) - yi)2
i[N ]
 



= -

f1(xi)2 2 + 2

(f2(xi)yi - f2(xi)f1(xi)) 

i[N ]

i[N ]

Observe that the above is a quadratic equation of  with negative leading coefficient. Hence, to obtain the maximum of the difference, we can set

=

i[N] (f2(xi)yi - f2(xi)f1(xi)) i[N] f1(xi)2

=

y - f1, f2 f2, f2

13

Under review as a conference paper at ICLR 2019

Note that if y - f1, f2 = 0 then the last pre-trained component is no need to be added. We aim

to calculate the probability of this case. Observe that y - f1, f2 = 0  (y - f1)  f2. This condition is different from previous Lemma. Here we have to find the upper bound of the probability

of (y - f1)  f2 for given f1 and y. As shown in the left part of Figure 2), the angle between f2

and y

must

be

in

a

specific

interval,

say

[

- 2

,

+ 2

] for small

 R+. In order to be concrete, we set

0 < < e-N .

Pr
f F(y,1)

(y - f1)  f2





<

1 eN .

The general case can be reduced to the above claim by considering gK-1 as f1 and kfK as f2.

Furthermore, since there there K possibles the be selected as the least pre-trained component, the

probability

is

upper

bounded

by

K eN

.

8.2 FUNCTION COMPOSITION BY NON-LINEAR ACTIVATION

Proof. (of Lemma 5.4) Although the lemma is an existence statement, we give a constructive proof

here. By setting D = 1 in Proposition 8.1, we know that for Logistic S(z) and 0 < < 1/1000, the

degree-one

Taylor

approximation

AS(z),1

=

1 2

+

1 4

z

with

the

remainder

|RS(z),2|

<

. Define M :=

10 · maxj[K]+,i[N]{|fj (xi)|}. Hence by setting z =

fj (xj ) M

,

we

have

S

fj (xj ) M

-

1 2

-

fj (xj ) 4M

<

.

This

means

that

for

the

givenj



[K ],

j

=

1 M

,

0

=

and

for

all

j

= j, j

= 0. Furthermore,

 = 4M .

This lemma implies that the S(z) can approximate linear function as possible in a non-zero length interval, hence if the scaling of  is allowed then Theorem 1 can be applied. Corollary 8.1. If the activation function is the Logistic, S(z), and {fj(xj)}Kj=1 satisfies LIC, then with high probability there is a vector  s.t. L x;   (0)(f1, ..., fK ) < minj[K]+ L(fj(xj)).



Proof. Set of above Lemma as 3N , then previous Lemma shows that there exists  which maps

{fj }

into

Z(0)

,1,

 3N

.

Since

the

output

of





(0)

is

a

linear

function

with

error

at

most


3N

,

we

have the same conclusion.

Proof.

Let g(x)

=   (0)(f1, ..., fK )(x) for short.

First observe that |g(x) - fj(x)|

<


3N



i



[N ],g(x(i))

-

fj (x(i))

>

- 3N

.

Then

g(x(i)) - y(i) =

(g(x(i)) - fj (x(i))) + (fj (x(i)) - y(i))

> N · -  +  = 2  > 0. 3N 3

i[N ]

i[N ]

On the other hand, it can be calculated that

L L

L L T

L (x; g) |= /3 =

, ,..., ,

w0 1

K b0

|=

,
 /3

where [a]T is the transpose of the matrix [a].

Also

note

that

|L
w0 = /3

=

2·

i[N] (g(x(i)) - y(i)) · S(z),

and

|L
b0 = /3

=

2·

i[N] (g(x(i)) - y(i)) · 1.

Since

i[N] (g(x(i)) - y(i)) > 0, we can conclude L (x; g) |= /3 = 0.

Proof. (of Lemma 5.6 ) By previous Lemma, it is valid to consider the best performance component, fj ; i.e. L(fj ) = minj[K]+ L(fj(xj)). Since L( /3) = 0, by definition of the gradient, moving along the direction of -L( /3)/ L( /3) with a step-size  > 0 must strictly decrease the value of L (x; g). W.L.O.G, we can assume this  is optimal; that is, if  > r > 0 then L( /3) > L( /3) - r · L( /3)/ L( /3) , while if r =  +  for some  > 0 then

14

Under review as a conference paper at ICLR 2019

L( /3)  L( /3) - r · L( /3)/ L( /3) . The issue is how to find a proper step-size r > 0? We consider the line search approach such that:





 r = arg min L 0 - r ·

L(0)  

rR 

L(0) 

This outputs F  then we can make sure that L 0 > L 0 - r · L(0)/ L(0) . Since the underlining 0 is  /3 that makes the loss is the same with the best one. Hence, we have (0 - r · L( /3)/ L( /3) ) to fit our goal (beating the best one).

Combining these lemmas can obtain the conclusion of Theorem 2. Corollary 8.2. The process of Lemma 5.5 converges.

Proof. (of 8.2) It is know that if a monotone decreasing sequence is bounded below, then this sequence convergences. ..... (revising) Repeating the process of Lemma 2, we can obtain a strictly decreasing sequence: L(0) > L(1) > L(2) > . . . . Note that i, (i)  0. This means the sequence is monotone decreasing and bounded below, so theoretically it converges by monotone convergence theorem of mathematical analysis. Algorithmically, the gradient descent based sequence finding process stops at some term with L( ) = 0, which is a (local) minimum or a saddle point.
Corollary 8.3. If the assumptions in Theorem 2 are satisfied, then with height probability there exists  s.t. L is a (local) minimum or a saddle point, while L (x; g) < minj[K]+ |L(fj(xj))| still holds.
Assume the pre-trained component set {fj(xj)}Kj=1 satisfies both NPC and LIC, then there exists  s.t. L (x; g) < minj[K]+ L(fj(xj)).
In the previous proof, the critical properties of activations are local linearity and differentiability. Hence, it is not hard to check that if we replace (·) in Eq. (1) with other common activations, the conclusion still holds. By local linearity we mean that on a non-zero length interval in its domain, the function can approximate the linear mapping as well as possible.
Corollary 8.4. Theorem 1 and 2 can apply on any activations with local linearity and differentiability.
Based on Corollary 5.1 and Lemma 5.6, it is natural to obtain the process of finding : by gradient descent or the closed form of Corollary 5.1. We can compute the optimal weights for the bottom Sigmiod block. On the other hand, after random initializing, the parameters of un-trained components in the Relu or Tanh blocks are assigned. This implies they can be treated as the all pre-trained case in Theorem 1 or 2. In fact, given the outputs from bottom level block then Corollary 5.1 provides weights improving the accuracy. Then it goes to the next up level block until the top, which is the forwarding steps of Back-propagation LeCun et al. (1988). Hence with initialization for un-trained component, Corollary 5.1 is essentially the same as Back-propagation.

8.3 A MIX OF PRE-TRAINED AND UN-TRAINED COMPONENTS
Now we first consider some of {fj (xj)}Kj=1 are pre-trained and some are un-trained, and then investigate the hierarchical combination of both kinds of components. In particularly, Eq. (1) can be re-written as g(x) = w0 ·  (1f1 + 2f2 ) + b0, where f1 is a pre-trained component and f2 is un-trained. Since 2 is not fixed, it can not be checked that LIC and NPC assumptions are satisfied. On the other hand, after initialization, f2 can be seen as a pre-trained component at any a snapshot during training phase.
Theorem 5. In the end of an weight updating iteration, if the components f1 and f2 satisfy LIC and NPC assumptions, then with high probability w updated in the next iteration can improve the loss.

15

Under review as a conference paper at ICLR 2019

Proof. Recall the training algorithm is the backpropagation algorithm. Also note that according to
Eq. (1), the order of updating is  first and then 2. We denote in the end of iteration i the value of  and 2 as (iter=i) and 2(iter=i), respectively. With randomized initialization, 2 is assigned as (2iter=0) before the execution of the iteration 1. Then in each iteration i  1, g(x) is a combination of fixed parameter components. Hence this can reduce to the all pre-trained cases, and can apply
Theorem 1 and 2.

Lemma 8.1. For a given data set X, let g := (g1, ..., gN ) and y = (y1, ..., yN ). If g, g - y = 0, then there exists   R s.t.

(g(xi) - yi)2 <

(g(xi) - yi)2

i[N ]

i[N ]

Proof. It is equivalent to show the inequality

(g(xi) - yi)2 -

(g(xi) - yi)2 < 0

i[N ]

i[N ]

has a real number solution.

(g(xi) - yi)2 - (g(xi) - yi)2
i[N ]

 





=  g(xi)2 2 + -2

g(xi)yi  + -

g(xi)2 + 2g(xi)yi

i[N ]

i[N ]

i[N ]

= g, g 2 + (-2 g, y )  + (- g, g + 2 g, y ) .

This is a quadratic inequality of , hence if (-2 g, y )2 - 4 ( g, g ) (- g, g + 2 g, y )  0,

then there exists at least one real solution.

Now we first consider some of {fj (xj)}jK=1 are pre-trained and some are un-trained, and then investigate the hierarchical combination of both kinds of components. In particularly, Eq. (1) can be
re-written as g(x) = w0 ·  (1f1 + 2f2 ) + b0, where f1 is a pre-trained component and f2 is un-trained. Since 2 is not fixed, it can not be checked that LIC and NPC assumptions are satisfied.
On the other hand, after initialization, f2 can be seen as a pre-trained component at any a snapshot during training phase.

8.4 GENERALIZATION ERROR ANALYSIS
Theorem 4. Assume pre-trained components {fj}Kj=1 satisfy LIC and NPC. Let {GE(fj)}jK=1 be corresponding generalization errors of {fj}Kj=1, and (L)  (L)  · · ·  (1)  (0)(f1, ..., fK ) be the composite neural network. Denote the generalization error, E{L((L)  (L)  · · ·  (1)  (0)(f1, ..., fK ))}, of the composite neural network as E{L,f1,...,fK }. Suppose the learned weights obey the normal distribution. Then with high probability, there exist a setting of {(L), ..., (0)} such that E{L,f1,...,fK }  (L)(GE(f1), ...GE (fK )).
Proof. (of Theorem 4) (Proof Sketch) We apply the idea similar to Kawaguchi (2016): the exception of non-liner activations is same with the exception of liner activations. Previous theorems provide that with high probability there exists the solution of (i), i  [L]+ s.t. each (i)+1(i) approximates a degree one polynomial A(i)+1(i),1 as well as possible. If the weights are obey the normal distribution, then E{L,f1,...,fK }  (L)(GE(f1), ...GE(fK )).

16

