Under review as a conference paper at ICLR 2019
COMPOSING COMPLEX SKILLS BY LEARNING TRANSITION POLICIES WITH PROXIMITY REWARD INDUCTION
Anonymous authors Paper under double-blind review
ABSTRACT
Intelligent creatures acquire complex skills by exploiting previously learned skills and learning to transition between them. To empower machines with this ability, we propose transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To effectively train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. The proposed method is evaluated on a diverse set of experiments for continuous control in both bipedal locomotion and robotic arm manipulation tasks in MuJoCo. We demonstrate that transition policies enable us to effectively learn complex tasks and the induced reward computed using the proximity predictor improves training efficiency. Videos of policies learned by our algorithm and baselines can be found at https://sites.google.com/view/transitions-iclr2019.
1 INTRODUCTION
While humans are capable of learning complex skills by reusing previously acquired skills, composing and mastering complex tasks is not as trivial as sequentially executing those previously learned skills. Instead, it requires a smooth transition between skills since the final pose of one skill may not be appropriate to initiate the following one. For example, while scoring in basketball with a quick shot after receiving a pass can be simply decomposed into catching and shooting, it is still difficult for beginners who have learned how to catch passes and statically shoot. To master this skill, players must practice adjusting their footwork and body into a comfortable shooting pose after catching a pass.
Similarly, can machines master new and complex tasks by reusing acquired skills and learning transitions between them? Learning to perform composite and long-term tasks from scratch requires extensive exploration and sophisticated reward design, which can introduce undesired behaviors (Riedmiller et al., 2018). Thus, instead of employing intricate reward functions and learning from scratch, modular methods sequentially execute acquired skills with a rule-based meta-policy, enabling machines to solve complicated tasks (Pastor et al., 2009; Mu¨lling et al., 2013; Andreas et al., 2017). These modular approaches assume that a task can be clearly decomposed into several subtasks which are smoothly connected to each other. In other words, an ending state of one subtask falls within the set of starting states, initiation set, of the next subtask (Sutton et al., 1999). However, this assumption does not hold in many continuous control problems where a given skill may be executed in starting states not considered during training or designing and thus, fail to achieve its goal.
To bridge the gap between skills, we propose a modular framework that executes skills sequentially and employs transition policies, which smoothly navigate from an ending state of a skill to suitable initial states of the following skill. However, learning a transition policy between skills without reward shaping is difficult as the only available learning signal is the sparse reward for the successful execution of the next skill. Sparse success/failure reward is challenging to learn from due to the temporal credit assignment problem (Sutton, 1984) and the lack of information from failing trajectories. To alleviate these problems, we propose an proximity predictor which outputs the proximity to the initiation set of the next skill and acts as a dense reward function for the transition policy.
1

FINAL
Under review as a conference paper at ICLR 2019

Observation
joint pos. vel. acc.
curb pos.

1

c <latexit sha1_base64="ykyXXryT0qS3g8DIJalovrnOKSA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkUI8FLx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RS2tnd294r7pYPDo+OT8ulZR8epYthmsYhVL6AaBZfYNtwI7CUKaRQI7AbTu4XffUKleSwfzCxBP6JjyUPOqLFSiw3LFbfqLkE2iZeTCuRoDstfg1HM0gilYYJq3ffcxPgZVYYzgfPSINWYUDalY+xbKmmE2s+Wh87JlVVGJIyVLWnIUv09kdFI61kU2M6Imole9xbif14/NeGtn3GZpAYlWy0KU0FMTBZfkxFXyIyYWUKZ4vZWwiZUUWZsNiUbgrf+8ibp3FQ9t+q1apVGLY+jCBdwCdfgQR0acA9NaAMDhGd4hTfn0Xlx3p2PVWvByWfO4Q+czx/CL4zZ</latexit>

Jumping

Meta-policy Walking

Crawling

2 Observation Transition policy

3 Observation Primitive policy

4
pc= success
or f ailure<latexitsha1_base64="QOOQ5ofVdAHhLfY+VT0BMXS8/4g=">AAAB8XicbZBNS8NAEIYn9avWr6pHL4tF8FQSKeix4MVjBfuBbQib7aZdutmE3YlQQv+FFw+KePXfePPfuG1z0NYXFh7emWFn3jCVwqDrfjuljc2t7Z3ybmVv/+DwqHp80jFJphlvs0QmuhdSw6VQvI0CJe+lmtM4lLwbTm7n9e4T10Yk6gGnKfdjOlIiEoyitR4HSLMgTwM2C6o1t+4uRNbBK6AGhVpB9WswTFgWc4VMUmP6npuin1ONgkk+qwwyw1PKJnTE+xYVjbnx88XGM3JhnSGJEm2fQrJwf0/kNDZmGoe2M6Y4Nqu1uflfrZ9hdOPnQqUZcsWWH0WZJJiQ+flkKDRnKKcWKNPC7krYmGrK0IZUsSF4qyevQ+eq7lm+b9SajSKOMpzBOVyCB9fQhDtoQRsYKHiGV3hzjPPivDsfy9aSU8ycwh85nz/ij5D/</latexit>

at, <latexit sha1_base64="0eQ/607gU1FaD6YVSJ6+yrxGHbk=">AAACAHicbZDLSsNAFIYnXmu9RV24cBMsggspiRR0WXDjsoK9QBvCyXTSDp1MwsyJWEI2voobF4q49THc+TZOLwtt/WHg4z/ncOb8YSq4Rtf9tlZW19Y3Nktb5e2d3b19++CwpZNMUdakiUhUJwTNBJesiRwF66SKQRwK1g5HN5N6+4EpzRN5j+OU+TEMJI84BTRWYB9DgBc9hCzIe8geMUcFUhdFYFfcqjuVswzeHCpkrkZgf/X6Cc1iJpEK0LrruSn6OSjkVLCi3Ms0S4GOYMC6BiXETPv59IDCOTNO34kSZZ5EZ+r+nsgh1noch6YzBhzqxdrE/K/WzTC69nMu0wyZpLNFUSYcTJxJGk6fK0ZRjA0AVdz81aFDUEDRZFY2IXiLJy9D67LqGb6rVeq1eRwlckJOyTnxyBWpk1vSIE1CSUGeySt5s56sF+vd+pi1rljzmSPyR9bnD8tRlyE=</latexit>

trans

trans
<latexit sha1_base64="C58Zhhu5B2jTGUetWFF7FAOl28Y=">AAAB/HicbZDLSsNAFIYnXmu9Rbt0M1gEVyURQZcFNy4r2As0IUymk3boZBJmTsQQ4qu4caGIWx/EnW/jtM1CW38Y+PjPOZwzf5gKrsFxvq219Y3Nre3aTn13b//g0D467ukkU5R1aSISNQiJZoJL1gUOgg1SxUgcCtYPpzezev+BKc0TeQ95yvyYjCWPOCVgrMBueECyoPCAPUIBikhdloHddFrOXHgV3AqaqFInsL+8UUKzmEmggmg9dJ0U/IIo4FSwsu5lmqWETsmYDQ1KEjPtF/PjS3xmnBGOEmWeBDx3f08UJNY6j0PTGROY6OXazPyvNswguvYLLtMMmKSLRVEmMCR4lgQeccUoiNwAoYqbWzGdEEUomLzqJgR3+cur0LtouYbvLpttp4qjhk7QKTpHLrpCbXSLOqiLKMrRM3pFb9aT9WK9Wx+L1jWrmmmgP7I+fwADYpWV</latexit>

=

termination

at,
<latexit sha1_base64="SXcjfjd44UHVDxLievnEKU+0ZeE=">AAAB9XicbZBNS8NAEIY39avWr6pHL4tF8CAlkYIeC148VrAf0MYw2W7bpZtN2J0oJfR/ePGgiFf/izf/jds2B219YeHhnRlm9g0TKQy67rdTWFvf2Nwqbpd2dvf2D8qHRy0Tp5rxJotlrDshGC6F4k0UKHkn0RyiUPJ2OL6Z1duPXBsRq3ucJNyPYKjEQDBAaz1AgBc9hDTIkoBNg3LFrbpz0VXwcqiQXI2g/NXrxyyNuEImwZiu5yboZ6BRMMmnpV5qeAJsDEPetagg4sbP5ldP6Zl1+nQQa/sU0rn7eyKDyJhJFNrOCHBklmsz879aN8XBtZ8JlaTIFVssGqSSYkxnEdC+0JyhnFgApoW9lbIRaGBogyrZELzlL69C67LqWb6rVeq1PI4iOSGn5Jx45IrUyS1pkCZhRJNn8krenCfnxXl3PhatBSefOSZ/5Hz+AJw1koc=</latexit>

pc

Figure 1: Our modular network augmented with transition policies. To perform a complex task, our model repeats the following steps: (1) The meta-policy chooses a primitive policy of index c; (2) The corresponding transition policy helps initiate the chosen primitive policy; (3) The primitive
policy executes the skill; and (4) A success or failure signal for the primitive skill is produced.

The main contributions of this paper include a novel modular framework that reuses skills by employing transition policies to connect skills and a joint training algorithm with the proximity predictor specifically designed for transition policies. This framework is suited for learning complex skills that require sequential execution of primitive skills, which are common in the real world. Our experiments on simulated environments demonstrate that employing transition policies addresses complex continuous control tasks which traditional policy gradient methods struggle at because transitions allow the agent to utilize a diverse set of known skills given only sparse rewards. We will release our environments and primitive skills as well as the code for further research.

2 RELATED WORK
Learning continuous control of diverse behaviors in locomotion (Merel et al., 2017; Heess et al., 2017; Peng et al., 2017) and robotic manipulation (Ghosh et al., 2018) is an active research area in reinforcement learning (RL). While some complex tasks can be solved through extensive reward engineering (Ng et al., 1999), undesired behaviors often emerge (Riedmiller et al., 2018) when tasks require several different primitive skills, and training from scratch is not computationally practical.
Real-world tasks often require diverse behaviors and longer temporal dependencies. In hierarchical reinforcement learning, the option critic framework (Sutton et al., 1999) learns meta actions (options), a series of primitive actions over a period of time, but assumes a global initiation set for all options. Typically, a hierarchical reinforcement learning framework consists of two components: a high-level meta-controller and low-level controllers. A meta-controller determines the order of subtasks to achieve the final goal and chooses corresponding low-level controllers that generate a sequence of primitive actions. Unsupervised approaches to discover meta actions have been proposed recently (Bacon et al., 2017; Daniel et al., 2016; Vezhnevets et al., 2017; Dilokthanakul et al., 2017; Frans et al., 2018; Co-Reyes et al., 2018; Levy et al., 2017). However, to deal with more complex tasks, additional supervision signals (Andreas et al., 2017; Tianmin Shu, 2018; Merel et al., 2017) or pre-defined low-level controllers (Kulkarni et al., 2016; Oh et al., 2017) are required.
To exploit pre-trained modules as low-level controllers, neural module networks (Andreas et al., 2016) have been proposed, which construct a new network dedicated to a given query using a collection of reusable modules. In the RL domain, a meta-controller is trained to follow instructions (Oh et al., 2017), demonstrations (Xu et al., 2017) and support multi-level hierarchies (Gudimella et al., 2017). In the robotics domain, Pastor et al. (2009); Kober et al. (2010); Mu¨lling et al. (2013) propose a modular approach that learns table tennis by selecting appropriate low-level controllers. On the other hand, Andreas et al. (2017); Frans et al. (2018) learn abstract skills while experiencing a distribution of tasks and then solve a new task with the learned primitive skills. However, these modular approaches result in undefined behavior when two skills are not smoothly connected. Our
2

 <latexit sha1_base64="Dk1kSmpn8/yAFc/1TZ8TK1W+hyU=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF48RzAOSJcxOZpMhM7PLTK8QQn7BiwdFvPpD3vwbZ5M9aGJBQ1HVTXdXlEph0fe/vdLG5tb2Tnm3srd/cHhUPT5p2yQzjLdYIhPTjajlUmjeQoGSd1PDqYok70STu9zvPHFjRaIfcZryUNGRFrFgFHOpjzQbVGt+3V+ArJOgIDUo0BxUv/rDhGWKa2SSWtsL/BTDGTUomOTzSj+zPKVsQke856imittwtrh1Ti6cMiRxYlxpJAv198SMKmunKnKdiuLYrnq5+J/XyzC+DWdCpxlyzZaL4kwSTEj+OBkKwxnKqSOUGeFuJWxMDWXo4qm4EILVl9dJ+6oe+PXg4brWuC7iKMMZnMMlBHADDbiHJrSAwRie4RXePOW9eO/ex7K15BUzp/AH3ucPHPeOOg==</latexit> "#

Under review as a conference paper at ICLR 2019

Algorithm 1 ROLLOUT

Algorithm 2 TRAIN

1: Input: meta policy meta, primitive poli- 1: Input: primitive polices {p1 , ..., pn }

cies {p1 , ..., pn }, {1 , ..., n }, and

transition proximity

policies predictors

2: Initialize success buffers {B1S, ..., BnS } with successful trajectories of primitive policies

{D1 , ..., Dn }

2: Initialize an episode and receive initial state s0 3: Initialize rollout buffers B1, ..., Bn

3: Initialize failure buffers {B1F, ..., BnF } 4: Randomly initialize parameters of transi-

4: repeat

5: Choose a primitive policy c  meta(st)

6: repeat

7: Run transition policy at, ttrans  c (st) 8: st+1 = ENV(st, at)

9: rt = Dc (st+1) - Dc (st)

10: Store (st, at, rt, ttrans, st+1) in Bc

11: until ttrans is Continue, t = t + 1

12: repeat

13: Run primitive policy at, tpc  pc (st)

14: 15:

unstitl+1tp=c isENCVon(stitn, aute), t = t + 1

16: Label a transition trajectory with t-1

tion policies {1, ..., n} and initiation predictors {1, ..., n}. 5: repeat
6: Collect trajectories using ROLLOUT
7: for i = 1 to n do 8: Add trajectories of i to BiS and BiF 9: Update Di to minimize Equation (1)
using samples from BiS and BiF 10: Update i to maximize Equation (2) 11: end for
12: until convergence

17: until episode termination

18: Return B1, . . . , Bn

proposed framework aims to bridge this gap by training model-free transition policies to navigate the agent from unseen states for following skills to suitable initial states.
Deep RL techniques for continuous control demand dense reward signals; otherwise, they suffer from long training time. Instead of manual reward shaping for denser reward, adversarial reinforcement learning (Ho & Ermon, 2016; Merel et al., 2017; Wang et al., 2017; Bahdanau et al., 2018) employs a discriminator which learns to judge the state or the policy, and the policy takes as rewards the output of the discriminator. While those methods assume ground truth trajectories or goal states are given, our method collects both success and failure trajectories online to train proximity predictors which provide rewards for transition policies.

3 APPROACH
In this paper, we address the problem of solving a complex task, which requires sequential composition of primitive skills {p1, p2, . . . , pn}, given only sparse and binary rewards (i.e. subtask completion reward). While our method is agnostic to the form of primitive policies (e.g. rule based, inverse kinematics, etc.), we consider the case of a pre-trained neural network in this paper. To learn a complex task, we propose a modular framework that is able to exploit the given primitive skills with smooth composition by employing transition policies.

3.1 PRELIMINARIES

We formulate our problem as a Markov decision process defined by a tuple {S, A, T , R, , } of

states, actions, transition probability, reward, initial state distribution, and discount factor. An action

distribution of an agent is represented as a policy (at|st), where st  S is the current state, at  A

is an action, and  are the parameters of the policy. An initial state s0 is randomly sampled from

, and then, an agent iteratively takes an action at sampled from a policy (at|st) and receives a

reward rt until the episode ends. The performance of the agent is evaluated based on a discounted

return R =

T t=0

trt,

where

T

is

the

episode

horizon.

3

Under review as a conference paper at ICLR 2019
FINAL

t <latexit sha1_base64="fInOqGTCCrWkRGJFOZWK1l6FLBY=">AAAB6HicbVBNS8NAEJ34WetX1aOXYBE8lUQEPRa9eGzBfkAbymY7adduNmF3IpTSX+DFgyJe/Une/Ddu2xy09cHA470ZZuaFqRSGPO/bWVvf2NzaLuwUd/f2Dw5LR8dNk2SaY4MnMtHtkBmUQmGDBElspxpZHEpshaO7md96Qm1Eoh5onGIQs4ESkeCMrFSnXqnsVbw53FXi56QMOWq90le3n/AsRkVcMmM6vpdSMGGaBJc4LXYzgynjIzbAjqWKxWiCyfzQqXtulb4bJdqWIneu/p6YsNiYcRzazpjR0Cx7M/E/r5NRdBNMhEozQsUXi6JMupS4s6/dvtDISY4tYVwLe6vLh0wzTjabog3BX355lTQvK75X8etX5eptHkcBTuEMLsCHa6jCPdSgARwQnuEV3pxH58V5dz4WrWtOPnMCf+B8/gDgKYz4</latexit>

transition 1

p1
<latexit sha1_base64="VNts+i3MDO/Tb2F25NGquZU27Os=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeCF48VTVtoQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSNkmmGfdZIhPdDanhUijuo0DJu6nmNA4l74ST27nfeeLaiEQ94jTlQUxHSkSCUbTSQzrwBtWaW3cXIOvEK0gNCrQG1a/+MGFZzBUySY3peW6KQU41Cib5rNLPDE8pm9AR71mqaMxNkC9OnZELqwxJlGhbCslC/T2R09iYaRzazpji2Kx6c/E/r5dhdBPkQqUZcsWWi6JMEkzI/G8yFJozlFNLKNPC3krYmGrK0KZTsSF4qy+vk/ZV3XPr3n2j1mwUcZThDM7hEjy4hibcQQt8YDCCZ3iFN0c6L86787FsLTnFzCn8gfP5A/s7jYo=</latexit>

transition 2

p2
<latexit sha1_base64="3ubTlAh8r00l6s79wAiqXMmnmBY=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKoR4LXjxWtB/QhrLZbtqlm03YnQgl9Cd48aCIV3+RN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpb2zu7e8X90sHh0fFJ+fSsY+JUM95msYx1L6CGS6F4GwVK3ks0p1EgeTeY3i787hPXRsTqEWcJ9yM6ViIUjKKVHpJhbViuuFV3CbJJvJxUIEdrWP4ajGKWRlwhk9SYvucm6GdUo2CSz0uD1PCEsikd876likbc+Nny1Dm5ssqIhLG2pZAs1d8TGY2MmUWB7YwoTsy6txD/8/ophjd+JlSSIldstShMJcGYLP4mI6E5QzmzhDIt7K2ETaimDG06JRuCt/7yJunUqp5b9e7rlWY9j6MIF3AJ1+BBA5pwBy1oA4MxPMMrvDnSeXHenY9Va8HJZ87hD5zPH/y/jYs=</latexit>

transition 3

p3
<latexit sha1_base64="V29pvUQG3MKQlVRztaqqIJOthf8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0m0UI8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD8ngelCuuFV3AbJOvJxUIEdzUP7qD2OWRlwhk9SYnucm6GdUo2CSz0r91PCEsgkd8Z6likbc+Nni1Bm5sMqQhLG2pZAs1N8TGY2MmUaB7Ywojs2qNxf/83ophjd+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmlfVT236t3XKo1aHkcRzuAcLsGDOjTgDprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifP/5DjYw=</latexit>

Jumping

Walking

Crawling

success failure

Success buffer Failure buffer

success reward
Proximity predictor
failure

Transition

at, <latexit sha1_base64="20fMljnCRHez2TyaCMr9gjM6pBY=">AAAB8HicbVDLSgNBEOz1GeMr6tHLYBA8SNiVgB4DXjxGMA9JljA7mU2GzMwuM71CCPkKLx4U8ernePNvnCR70MSChqKqm+6uKJXCou9/e2vrG5tb24Wd4u7e/sFh6ei4aZPMMN5giUxMO6KWS6F5AwVK3k4NpyqSvBWNbmd+64kbKxL9gOOUh4oOtIgFo+ikR9rDS9JFmvVKZb/iz0FWSZCTMuSo90pf3X7CMsU1Mkmt7QR+iuGEGhRM8mmxm1meUjaiA95xVFPFbTiZHzwl507pkzgxrjSSufp7YkKVtWMVuU5FcWiXvZn4n9fJML4JJ0KnGXLNFoviTBJMyOx70heGM5RjRygzwt1K2JAaytBlVHQhBMsvr5LmVSXwK8F9tVyr5nEU4BTO4AICuIYa3EEdGsBAwTO8wptnvBfv3ftYtK55+cwJ/IH3+QMlf4/s</latexit>



Figure 2: Training of transition policies and proximity predictors. After executing a primitive policy, a previously performed transition trajectory is labeled and added to a replay buffer based on the execution success. A proximity predictor is trained on states sampled from the two buffers to output 0 and 1 for failing and successful states. and serves as a reward function to encourage the transition policy to move toward good initial states to initiate the corresponding primitive policy.

3.2 MODULAR NETWORK
To learn a new task given learned primitive polices, we design a modular network that consists of the following components: a meta-policy meta(·|s), primitive policies {p1 (a|s), . . . , pn (a|s)}, and transition policies {1 (a|s), . . . , n (a|s)} as illustrated in Figure 1 and Algorithm 1.
The meta-policy chooses the next primitive policy to execute when the current primitive policy is terminated. The action space of the meta-policy is a set of primitive policy indexes {1, 2, . . . , n}. The observation of the meta-policy contains the low-level information of primitives and task specifications indicating high-level goals (e.g. moving direction and target object position). In this paper, we use a rule-based meta-policy and focus on transitioning between consecutive primitive policies.
Once a primitive policy pc is chosen to be executed, the agent generates an action at  A based on the current state st  S, which consists of joint configurations. Note that we did not differentiate state spaces for primitive polices because of the simplicity of notations (e.g. the observation of the jumping primitive contains a distance to a curb while that of the walking primitive only has joint pose and velocities). Every primitive policy is required to generate termination signals pc  {continue, success, fail} to indicate policy completion and whether the execution is successful or not. Note that the primitive policies can be any form, such as neural network and rule-based policy.
For smooth transitions between primitive policies, we add a transition policy c (a|s) before executing primitive policy pc, which guides an agent to pc's initiation set. The transition policy's observation and action space are the same as the primitive policy's. The transition policy also learns a termination signal trans which indicates transition termination to successfully initiate the following primitive policy.
3.3 TRAINING TRANSITION POLICIES
Transitioning between a pair of primitive policies (pi, pj) can be considered as moving from end states of the former primitive policy pi to the initiation set of the subsequent primitive policy pj. After the transition is completed, pj proceeds with the execution of the primitive skill. The transition policy for pj is shared across different preceding primitive policies where a successful transition is set by the success of the following primitive policy pj. For brevity of notation, we omit the primitive policy index j in the following equations where unambiguous.
Transition policies are difficult to train as it is hard to learn a good mapping between end states and viable initial states in a continuous space. Furthermore, by definition, the only available learning
4

p2
<latexit sha1_base64="DjS1oOAr1VR/VatzfCTqK/ymJgE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI9FLx4r2g9oQ9lsJ+3SzSbsboQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ7dzvPKHSPJaPZpqgH9GR5CFn1FjpIRnUBuWKW3UXIOvEy0kFcjQH5a/+MGZphNIwQbXueW5i/Iwqw5nAWamfakwom9AR9iyVNELtZ4tTZ+TCKkMSxsqWNGSh/p7IaKT1NApsZ0TNWK96c/E/r5ea8NrPuExSg5ItF4WpICYm87/JkCtkRkwtoUxxeythY6ooMzadkg3BW315nbRrVc+tevdXlcZNHkcRzuAcLsGDOjTgDprQAgYjeIZXeHOE8+K8Ox/L1oKTz5zCHzifPwEEjZk=</latexit>
p3
<latexit sha1_base64="o4nZ68qYcK0np11Ttx2Ns6alTxc=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0m0oMeiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilh6R/2S9X3Ko7B1klXk4qkKPRL3/1BjFLI66QSWpM13MT9DOqUTDJp6VeanhC2ZgOeddSRSNu/Gx+6pScWWVAwljbUkjm6u+JjEbGTKLAdkYUR2bZm4n/ed0Uw2s/EypJkSu2WBSmkmBMZn+TgdCcoZxYQpkW9lbCRlRThjadkg3BW355lbQuqp5b9e5rlfpNHkcRTuAUzsGDK6jDHTSgCQyG8Ayv8OZI58V5dz4WrQUnnzmGP3A+fwACiI2a</latexit>
123<latexitsha1_base64="41qpZeab6F5HP2mdzxATRxWyecw=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48VTFtoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSNkmmGfdZIhPdDanhUijuo0DJu6nmNA4l74STu7nfeeLaiEQ94jTlQUxHSkSCUbSS30/FwBtUa27dXYCsE68gNSjQGlS/+sOEZTFXyCQ1pue5KQY51SiY5LNKPzM8pWxCR7xnqaIxN0G+OHZGLqwyJFGibSkkC/X3RE5jY6ZxaDtjimOz6s3F/7xehtFNkAuVZsgVWy6KMkkwIfPPyVBozlBOLaFMC3srYWOqKUObT8WG4K2+vE7aV3XPrXsP17XmbRFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+dj2VpyiplT+APn8wd2p45x</latexit> <latexit sha1_base64="pjS++59c/GUE3UrgzDBxESRDgoc=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoMeiF48VTFtoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSNkmmGfdZIhPdDanhUijuo0DJu6nmNA4l74STu7nfeeLaiEQ94jTlQUxHSkSCUbSS30/FoDGo1ty6uwBZJ15BalCgNah+9YcJy2KukElqTM9zUwxyqlEwyWeVfmZ4StmEjnjPUkVjboJ8ceyMXFhlSKJE21JIFurviZzGxkzj0HbGFMdm1ZuL/3m9DKObIBcqzZArtlwUZZJgQuafk6HQnKGcWkKZFvZWwsZUU4Y2n4oNwVt9eZ20G3XPrXsPV7XmbRFHGc7gHC7Bg2towj20wAcGAp7hFd4c5bw4787HsrXkFDOn8AfO5w94K45y</latexit> <latexit sha1_base64="nfhcPKRuD+NrXbo6TLjg9XwrUVKhKAfJy2qoCddN3qapftqn9Z02ZayalvaFdAnhqm6aw/wQCDuQb2rOpvVxE8sRKQv+MVUSXAyxQcow=">AAAB724HXicbZVBDNS8gNMAxEFIJ3XUv1r1Lq98/6aVqqh1a6193rLNbBo8bJEFBiUcu0CFlVnoU0zmdU0bGIKnPF9QRLpFiwuL8YHxc3F4KLrZpmwCibkL25bZ0CbQoahOWI5ryeLR2ZSMbk5S3kWt/bq4pblZ0mhE0sNkx32YMmyNSn0ROQI4g2JlIh/9DZHDeQ1gd0ze4BPuF8CHa2j5GCihIE1fiVRC3/9+3klQzvNXr/o/c3+xjpN/t2zVzs0l1YJoBbHW6bD4xXw8HQ0MA+wP8xzNzHk6nkhbIvJYuhvWfSpfZcl5euEmlYALZSqQKuUWFjfBQPNed/tf9+9beOdKbk62WWdo1b1t39m7bY+/1v3z+NbugrflOfu+JcXgr2fdZBNyWtf9dzX7D3j+obwk/9+e+PNFgRmae9Sofn2bVjfilUkz0b9ZkgJmwjmIsMGMiDP+lE6zoWlqszER5UtjYCnOaqJm6GhuFF1pUSXULjUDl42pNBYVCJYVDkJ9cRwCSW4p83EGAnLSMTdgq1z8yqiPQLTNaxWFRfqGEHxbAKj7d6n3jtgfi0kz+Pinm7d8/73nY+Ozgt/JsG8i18TSLtXT1a/JTEUY/zLTlQRrT6TDMxFKGMnIOrM4EKYjWgj5M7qtQicUOPlSCOsIkK7HgMJEOGo6WoWi2vgEy5lvavkr5Z/aT3A+K3drLfw/qMfv3WgVu2mNIWlV/ntxyPDF3r52C7C9ygCYJErakxnKNCX6bjt+gIDFGDSqQUsSo1G70+BK+D719qaDWsDUv6J33S1xERiaB8W0hmedWhoLIYCEzuaFBSh5MuQUEcLiGUXad1pNw7MSnMaTp3F8uoPwKa7TogTrgKH9chLIKqiURwsOTbUZBXOJUKwGrJz7m93lcVGlY+g2sUpPlnbMFNhKhMRI7W+vURTwRI6tOXR1uYDIxtz9lSzBTGxiR16WGUNdkHuLeQI7TAnadzy0YNxsQ6b+5fvEknzozDcYCmNkogGvwDV3EN9IY4ieXu5kKoSYlbZjg9UbRvsk3Ohrs9Wv4ta6uijeCd/JzyGD93mZIhNsOa9Gk7S9zGQjaOO2dNbMTQW9DPd/isNMZL7UBzXUixogyW2vlbeNVtz1/W/PE8xLuluP/d6juV+fkEbS1NmMbaero6gNxHCm6uy6KgCLillyw0oMVnMoN1E4G5wLOS5utL02dRTmHZBaJFRXJmljNSCFEAIhm2mICI3Czb5ZRPwlxP/NwypYTYEoaASZFrBCblZcaEuY4ZqkAyMJLCNa1KgyFyPlMldQcWarEX8tsyZhrMb3YuCHSYeROtSGTCbqTKXGD2R8Mn92aVDnV7f740CoKNwd2wBvuVGv/tI3+mw1Mte5ZKYX4AWXa0S6vLgnqdMu8qpMeI7FbHWX/nEtc0E1eIo7wcrN3GmxAqmvcNHw2Dy9UKCOBEMcLpgANzhAI3BK8O4AZBVByXDCeNvBC9wEYfG1nQoA3hg5wHBjntd240osguwg3Aq8cpdPM5GO6AeDp69zL/7eDhOKx4F7dIKw+458uc05kzrxbnlfw8x4q73xpRI827oT+P4HRsy<WrP/vXlJak8KFAteDWVxaWOkOin48tjQ>RA+wfc=Oz<5wx/9l5a4rbte45xzvi<t/>latexit> at <latexit sha1_base64="cg9eTEqUtCZqkwCW5khYw4me7mE=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpgfax71a9mjcHWSV+QapQoNF3v3qDhGUxV8gkNabreykGOdUomOTTSi8zPKVsTIe8a6miMTdBPj91Ss6sMiBRom0pJHP190ROY2MmcWg7Y4ojs+zNxP+8bobRdZALlWbIFVssijJJMCGzv8lAaM5QTiyhTAt7K2EjqilDm07FhuAvv7xKWhc136v595fV+k0RRxlO4BTOwYcrqMMdNKAJDIbwDK/w5kjnxXl3PhatJaeYOYY/cD5/AE4yjcw=</latexit>

Under review as a conference paper at ICLR 2019

signal for the transition policies is the sparse reward for the completion of the next task. To alleviate

the sparsity of rewards and to maximize the objective of moving to viable initial states for the next

primitive, we propose an proximity predictor that learns and provides a dense reward of how close

transition states are to the initiation set of the corresponding primitives, illustrated in Figure 2. We

denote an proximity predictor as Dj which is parameterized by j. We define the proximity of a state as the future discounted proximity, v = step, where step is the number of steps required

to reach an initiation set of the following primitive policy. The proximity of transition states in the

initiation set is 1 because the initiation set leads to successful execution of the respective primitive

policy. We train the proximity predictor by minimizing the following objective:

LD(, BS, BF )

=

1 2 E(s,v)BS [(D(s)

-

v)2]

+

1 2

EsBF

[D

(s)2

],

(1)

where BS and BF are buffers that contain sets of states collected from success and failure trajecto-

ries, respectively. To estimate the proximity to an initiation set, BS contains not only the state that

directly lead to the success of the following primitive policy, but also the intermediate states of the

successful trajectories with its proximity. By minimizing this objective, given a state, the proximity

predictor is learned to predict 1 if the state is in the initiation set, a value that is between 0 and 1 if

the state leads the agent to end up with a desired initial states, and 0 when the state leads to a failure.

The goal of a transition policy can be formulated as seeking a state s that is likely to be considered
as a plausible initial state by the proximity predictor (i.e. D(s) is close to 1). To achieve this goal, the
transition policy learns to maximize proximity prediction at the end state of the transition trajectory D(sT ). In addition to providing reward at the end, we also use the increase of proximity to the initiation set D(st+1) - D(st) at every timestep as a reward, dubbed proximity reward, to create a denser reward. The transition policy is trained to maximize the expected discounted return:

T -1

Rtrans() = E T D(sT ) + t(D(st+1) - D(st)) .

(2)

t=0

In addition to predicting an action distribution, the transition policy also predicts a termination signal

  (s) that decides whether the current state s is a plausible initial state for the following skill.

However, in this scenario, ground truth states in BS and BF are not available. Hence, we collect
training data for an proximity predictor online by utilizing the trajectories obtained during execution
of the corresponding transition policy and primitive policy. Specifically, we label the states in a
transition trajectory as success or failure based on whether the following primitive is successfully executed or not, and add them into the corresponding buffers BS or BF , respectively. As stated
in Algorithm 2, we train transition policies and proximity predictors by alternating between an Adam (Kingma & Ba, 2014) gradient step on  to minimize Equation (1) with respect to D and a PPO (Schulman et al., 2017) step on  to maximize Equation (2) with respect to . We refer readers to the supplementary for further details on training.

Utilizing the learned reward for training transition policies is beneficial in several perspectives: (1) the proximity predictor provides how proximate the current state is to suitable initial states; (2) the dense rewards speed up transition policy training by differentiating failing states from states in a successful trajectory; and (3) the joint training mechanism prevents a transition policy from getting stuck in local optima. Whenever a transition policy gets into a local optimum (i.e. fails the following skill with a high proximity reward), the proximity predictor learns to lower the reward for the failing transition as those states are added to its failure buffer, escaping the local optimum.

4 EXPERIMENTS
We conducted experiments on two classes of tasks: locomotion and robotic manipulation. To illustrate the potential of the proposed framework, we designed a set of complex tasks that require agents to utilize a diverse library of primitive skills described in the following sections. All of our environments are simulated in the MuJoCo physics engine (Todorov et al., 2012).
4.1 BASELINES
We evaluate our method to answer how transition policies benefit complex task learning and the effectiveness of joint training of transition policies and proximity predictors. To investigate the

5

Under review as a conference paper at ICLR 2019

(a) Repetitive picking

(b) Repetitive catching

(c) Serve

(d) Patrol

(e) Hurdle

(f) Obstacle course

Figure 3: Tasks and success count curves of our model (blue), TRPO (purple), PPO (magenta), and transition policies trained on task reward (green) and sparse proximity reward (yellow). Our model achieves the best performance and convergence time. Different temporal scales are used for TRPO and PPO (bottom) and ours (top). TRPO and PPO are trained 5 times longer than ours.

FINALTable 1: Success count for robotic manipulation, comparing our method with baselines.

TRPO PPO Without transition Transition + task reward Transition + sparse proximity reward Transition + proximity reward (ours)

Reward dense dense sparse sparse sparse sparse

Repetitive picking 0.69 ± 0.46 0.95 ± 0.53 0.99 ± 0.08 0.99 ± 0.08 1.52 ± 1.12 4.84 ± 0.63

Repetitive catching 4.54 ± 1.21 4.26 ± 1.63 1.00 ± 0.00 4.87 ± 0.58 4.88 ± 0.59 4.97 ± 0.33

Serve 0.32 ± 0.47 0.00 ± 0.00 0.11 ± 0.32 0.05 ± 0.21 0.92 ± 0.27 0.92 ± 0.27

impact of the transition policy, we compare policies learned from dense rewards with our model that learns from sparse and binary rewards by exploiting primitive policies. Moreover, we conducted ablation studies to dissect each component in the training method of transition polices:
· Trust Region Policy Optimization (TRPO) with dense reward represents a state-of-the-art policy gradient method, which we use for the standard RL comparison.
· Proximal Policy Optimization (PPO) with dense reward is another state-of-the-art policy gradient method, which is more stable than TRPO with small batch sizes.
· Modular Net without transition policies sequentially executes primitive policies without transition policies and has no learnable components.
· Modular Net with transition policies trained on task rewards represents a modular network augmented with transition policies learned from the sparse and binary reward (subtask completion reward), whereas our model learns from the reward predictor.
· Modular Net with transition policies trained on sparse proximity rewards is a sparse variant of our model where the proximity reward is provided only at the end of the transition trajectory.
Initially, we tried comparing baseline methods with our method using only sparse and binary rewards. However, the baselines could not solve any of our environments due to the complex tasks and sparse reward of the environments. To get a more competitive comparison, we hand engineer dense rewards for baselines to boost their performance and show that transitions with sparse rewards can compete with and even outperform dense reward baselines. As the performance of policy gradient methods like TRPO and PPO varies significantly between runs, we perform each experiment with 3 random seeds and report mean and standard deviation in Figure 3.
4.2 ROBOTIC MANIPULATION
For robotic manipulation, we simulate the Kinova Jaco, a 9 DoF robotic arm with 3 fingers. The agent receives full state information, including the absolute location of external objects. The agent uses joint torque control to perform actions. The results are shown in Figure 3 and Table 1.

6

Under review as a conference paper at ICLR 2019

Table 2: Success count for locomotion, comparing our method with baselines. Hurdle's TRPO reward was extensively engineered but our method is comparable with sparse reward*.

TRPO PPO Without transition Transition + task reward Transition + sparse proximity reward Transition + proximity reward (ours)

Reward dense dense sparse sparse sparse sparse

Patrol 1.37 ± 0.52 1.53 ± 0.53 1.02 ± 0.14 1.69 ± 0.63 2.51 ± 1.26 3.33 ± 1.38

Hurdle 4.13 ± 1.54 2.87 ± 1.92 0.49 ± 0.75 1.73 ± 1.28 1.47 ± 1.53 3.14 ± 1.69*

Obstacle course 0.98 ± 1.09 0.85 ± 1.07 0.72 ± 0.72 1.08 ± 0.78 1.32 ± 0.99 1.90 ± 1.45

Pre-trained primitives. There are four pre-trained primitives available: Picking, Catching, Tossing, and Hitting. Picking requires the robotic arm to pick up a small block, which is randomly placed on the table. If the box is not picked after a certain amount of time, the agent fails. Catching learns to catch a block that is thrown towards the arm with random initial position and velocity. The agent fails if it does not catch and stably hold the box for a certain amount of time. Tossing requires the robot to pick up a box, toss it vertically in the air, and land the box at a specified position. Hitting requires the robot to hit a box dropped overhead at a target.
Repetitive picking. The Repetitive picking task requires the agent to complete the Picking task 5 times. After a successful pick, the box disappears and is placed randomly on the table again. Our model achieves the best performance and converges the fastest by learning from the proximity reward. With our dense proximity reward at every transition step, we alleviate credit assignment when compared to providing a proximity reward at the end of the trajectory or using sparse task reward. Conversely, TRPO with dense rewards takes significantly longer to learn and is unable to pick the second box as the ending pose after the first pick is too unstable to initialize the next picking.
Repetitive catching. Similar to Repetitive picking, the Repetitive catching task requires the agent to catch boxes consecutively up to 5 times. In this task, other than the modular network without a transition policy, all baselines are able to eventually learn while our model still learns the fastest. We believe this is because the Catching primitive policy has a larger initiation set and therefore, the sparse reward problem is alleviated.
Serve Inspired by tennis, Serve requires the robot to toss the ball and hit it at a target. Even with an extensively engineered reward, TRPO and PPO baselines fail to learn because Hit needs to learn to cover all terminal states of the Toss primitive. On the other hand, learning to recover from Toss's ending states to Hit's initiation set is easier for exploration, which reduces the complexity of the task. In this task, our method and the modular network with sparse proximity reward baseline are able to solve it. We believe this is because the Hit primitive has a large initiation set so the transition policy could explore without dense proximity reward.

4.3 LOCOMOTION
For locomotion, we simulate a 9 DoF planar (2D) bi-pedal robot. The observation of the agent includes joint position, rotation, and velocity. When the agent needs to interact with environment, we provide additional input such as distance to the curb and ceiling in front of the agent. The agent uses joint torque control to perform actions. The results are shown in Figure 3 and Table 2.
Pre-trained primitives. Forward and Backward require the agent to walk forward and backward with a certain velocity, respectively. Balancing enables the walker to be robust under the random external forces. Jumping lets the walker jump over a randomly located curb and land safely. Crawling requires the walker to crawl under a ceiling. In all the aforementioned tasks, the walker fails when the height of the walker is lower than a threshold.
Patrol (Forward and backward). The Patrol task involves walking forward and backward toward goal points on either side and balancing in between to smoothly change its direction. As illustrated in Figure 3, our method consistently outperforms TRPO and ablated baselines in stably walking forward and transitioning to walk backward. The agent trained with dense reward is not able to consistently switch directions, whereas our model can utilize previously learned primitives including Balancing to stabilize a reversal in velocity.
7

Under review as a conference paper at ICLR 2019

Figure 4:

(a) PATROL Patrol
<latexit sha1_base64="d1Oyunve2JK9EyNed/HVDZSnRoQ=">AAAB+XicbZBNS8NAEIYn9avWr6hHL8EieCqJCHosePFYwX5AG8pmu22XbnbD7qRYQv+JFw+KePWfePPfuGlz0NYXFh7emWFm3ygR3KDvfzuljc2t7Z3ybmVv/+DwyD0+aRmVasqaVAmlOxExTHDJmshRsE6iGYkjwdrR5C6vt6dMG67kI84SFsZkJPmQU4LW6rtuD9kTGpo1CGol5pW+W/Vr/kLeOgQFVKFQo+9+9QaKpjGTSAUxphv4CYYZ0cipYPNKLzUsIXRCRqxrUZKYmTBbXD73Lqwz8IZK2yfRW7i/JzISGzOLI9sZExyb1Vpu/lfrpji8DTMukxSZpMtFw1R4qLw8Bm/ANaMoZhYI1dze6tEx0YSiDSsPIVj98jq0rmqB5Yfrav26iKMMZ3AOlxDADdThHhrQBApTeIZXeHMy58V5dz6WrSWnmDmFP3I+fwC7dZOo</latexit>

(b)

MASNeIPrUvLeATION <latexit sha1_base64="dzB379CJXvTdqYnMhntrfjPmBZg=">AAAB+HicbZDLSsNAFIYn9VbrpVWXboJFcFUSKeiy4MZlRXuBNpTJ9KQdOpmEmZNiDX0SNy4UceujuPNtnLRZaOuBgY//P2fmzO/Hgmt0nG+rsLG5tb1T3C3t7R8clitHx20dJYpBi0UiUl2fahBcQgs5CujGCmjoC+j4k5vM70xBaR7JB5zF4IV0JHnAGUUjDSrlPsIjapbeg5rCvDSoVJ2asyh7HdwcqiSv5qDy1R9GLAlBIhNU657rxOilVCFnwlzYTzTElE3oCHoGJQ1Be+li8bl9bpShHUTKHIn2Qv09kdJQ61nom86Q4livepn4n9dLMLj2Ui7jBEGy5UNBImyM7CwFe8gVMBQzA5Qpbna12ZgqytBklYXgrn55HdqXNdfwXb3aqOdxFMkpOSMXxCVXpEFuSZO0CCMJeSav5M16sl6sd+tj2Vqw8pkT8qeszx/q2ZMx</latexit>

Average transition length and average proximity reward of transition trajectories over

training on Patrol (left) and Manipulation (right).

Hurdle (Walking forward and jumping). The Hurdle task requires the agent to walk forward and jump across curbs and requires a transition between walking and jumping as well as landing the jump to walking forward. As shown in Figure 3, our method outperforms the sparse reward baselines, but TRPO with dense reward can learn this task as well. We extensively design dense rewards for competitive baselines and for the Hurdle task, the dense reward consists of eight components, which collectively enable TRPO to learn the task. With an intricately designed dense reward, difficult tasks can be trained with RL baselines. However, our focus is to learn a complex task by reusing acquired skills, avoiding an extensive reward design. Our model with sparse reward and prior knowledge is still able to learn this complex task in comparison to other sparse reward baselines.
Obstacle Course (Walking forward, jumping, and crawling). Obstacle Course is the most difficult among the locomotion tasks, where the walker must walk forward, jump across curbs, and crawl underneath ceilings. It requires three different behaviors and transitions between two very different primitive skills: crawling and jumping. Since the task requires significantly different behaviors that are hard to transition between, TRPO fails to learn the task and only tries to crawl toward the curb without attempting to jump. In contrast, our method learns to transition between all pairs of primitive skills and succeeds in crossing multiple obstacles.
4.4 ABLATION STUDY
We conducted additional experiments to understand the contribution of transition policies, proximity predictors, and soft labeling for intermediate states of successful trajectories. Figure 3 shows the gain from each component. The modular network without transition policies tends to fail the execution of the second skill as sequential executions of primitives is not smooth, and the next primitive policy is not trained to cover ending states of the first primitive. Transition policies trained from task completion reward learn to connect consecutive primitives slower as sparse task reward is hard to learn from due to the credit assignment problem. On the other hand, our model alleviates credit assignment and learns quickly by giving predicted rewards for every transition state-action pair.
4.5 TRAINING OF TRANSITION AND PROXIMITY PREDICTOR
In Patrol task, transition lengths for Forward, Balance, Backward primitives are 5, 33, and 3, respectively. Figure 4 shows that at first the proximity rewards for transition policies increase directly with the transition lengths since transition policies are exploring unseen states with high proximity rewards. However, as failing initial states with high proximity are collected in the failure buffers, the proximity predictor learns to distinguish good and bad initial states.

5 CONCLUSION
In this work, we propose a modular framework with transition policies to empower reinforcement learning agents to learn complex tasks with sparse reward. Specifically, we formulate the problem as executing existing primitives while smoothly transitioning between primitives and propose a joint training method to train transition policies with proximity predictors. Our experimental results on robotic manipulation and locomotion tasks demonstrate the effectiveness of employing transition policies. The proposed framework solves complex tasks without reward shaping and outperforms baseline RL algorithms and other ablated baselines that utilize prior knowledge on many tasks.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 39­48, 2016.
Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, 2017.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pp. 1726­ 1734, 2017.
Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward Grefenstette. Learning to follow language instructions with adversarial reward induction, 2018.
John Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, and Sergey Levine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. In Proceedings of the 35th International Conference on Machine Learning, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018.
Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104(2-3):337­357, 2016.
Nat Dilokthanakul, Christos Kaplanis, Nick Pawlowski, and Murray Shanahan. Feature control as intrinsic motivation for hierarchical reinforcement learning. arXiv preprint arXiv:1705.06769, 2017.
Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=SyX0IeWAW.
Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine. Divide and conquer reinforcement learning. In International Conference on Learning Representations, 2018.
Aditya Gudimella, Ross Story, Matineh Shaker, Ruofan Kong, Matthew Brown, Victor Shnayder, and Marcos Campos. Deep reinforcement learning for dexterous manipulation with concept networks. CoRR, abs/1709.06977, 2017. URL http://arxiv.org/abs/1709.06977.
Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin A. Riedmiller, and David Silver. Emergence of locomotion behaviours in rich environments. CoRR, abs/1707.02286, 2017. URL http: //arxiv.org/abs/1707.02286.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565­4573, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Jens Kober, Katharina Mu¨lling, Oliver Kro¨mer, Christoph H Lampert, Bernhard Scho¨lkopf, and Jan Peters. Movement templates for learning of hitting and batting. In Robotics and Automation (ICRA), 2010 IEEE International Conference on, pp. 853­858. IEEE, 2010.
Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural Information Processing Systems, pp. 3675­3683, 2016.
Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical actor-critic. arXiv preprint arXiv:1712.00948, 2017.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In International Conference on Computer Vision, 2017.
9

Under review as a conference paper at ICLR 2019
Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. Learning human behaviors from motion capture by adversarial imitation. CoRR, abs/1707.02201, 2017. URL http://arxiv.org/abs/1707.02201.
Katharina Mu¨lling, Jens Kober, Oliver Kroemer, and Jan Peters. Learning to select and generalize striking movements in robot table tennis. The International Journal of Robotics Research, 32(3): 263­279, 2013.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pp. 278­287, 1999.
Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In International Conference on Machine Learning, pp. 2661­2670, 2017.
Peter Pastor, Heiko Hoffmann, Tamim Asfour, and Stefan Schaal. Learning and generalization of motor skills by learning from demonstration. In Robotics and Automation, 2009. ICRA'09. IEEE International Conference on, pp. 763­768. IEEE, 2009.
Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel Van De Panne. Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics (TOG), 36(4):41, 2017.
Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playingsolving sparse reward tasks from scratch. arXiv preprint arXiv:1802.10567, 2018.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889­1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181­ 211, 1999.
Richard Stuart Sutton. Temporal credit assignment in reinforcement learning. 1984.
Richard Socher Tianmin Shu, Caiming Xiong. Hierarchical and interpretable skill acquisition in multi-task reinforcement learning. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=SJJQVZW0b.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012.
Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161, 2017.
Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. In Advances in Neural Information Processing Systems, pp. 5326­5335, 2017.
Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Neural task programming: Learning to generalize across hierarchical tasks. arXiv preprint arXiv:1710.01813, 2017.
10

Under review as a conference paper at ICLR 2019
A TRANSITION ANALYSIS
As illustrated in Figure 5, transition policies are capable of moving toward states with higher rewards from proximity predictors. Specifically, the learned transition policy moves between the states of the primitives and higher proximity predictor reward is given to states that are close to the initiation set of the corresponding primitive policy.

Picking end
s0 Picking end s1

Picking start

High D(s)

Picking start t0 t1

Low D(s)

Forward Forward
s0
Balance

Balancing t0 s1

Balance Backward

High D(s)

t1

Low Backward D(s)

(a) (b)
Figure 5: Visualized transitioning trajectories. (a) shows the transitioning trajectories sampled from the manipulation task Repetitive Picking and (b) shows the ones from the locomotion task Patrol. TOP AND BOTTOM ROWS: rendered frames of transition policies executing transitions. MIDDLE: contains states from each labeled primitive distribution projected onto PCA space. The dots connected with lines are extracted from the same transitioning trajectory, where the marker color indicates the proximity prediction of the proximity predictor D(s). A higher D(s) value indicates proximity to suitable states to initialize the next primitive. On the left, two transitions shown between repetitive picks starts at s0 and s1 near the end of the picking skill and transition to t0 and t1, respectively, near the start of the picking skill. Similarly, on the right, the forward to balance transition moves between the forward and balance state distributions and the balance to backward transition moves from the balancing states close to the backward states.
B ACQUIRING PRIMITIVE POLICIES
The modular framework proposed in this paper allows a primitive policy to be any of a pre-trained neural network, inverse kinematics module, or hard-coded policy. In this paper, we use neural networks trained with TRPO (Schulman et al., 2015) on dedicated environments as primitive policies (see section D for details on environments and reward functions). A policy network consists of 2 layers of 32 hidden units with tanh nonlinearities and predicts the mean and standard deviation of a Gaussian distribution over an action space. We trained all primitive policies until the total return converged (up to 10,000 iterations).
A primitive skill consists of a primitive policy and a termination signal. Given a state, a primitive policy outputs the next action and the termination signal indicates whether the execution is done and if the skill was successfully performed (see section D).
11

Under review as a conference paper at ICLR 2019

C TRAINING DETAILS

C.1 IMPLEMENTATION DETAILS
For the TRPO and PPO implementation, we used OpenAI baselines1 with default hyperparameters including learning rate, KL penalty, and entropy coefficients unless specified below.

Hyperparameters Transition policy Proximity predictor Primitive policy

TRPO

PPO

Learning rate # Mini-batch
Batch size Learning rate decay

1e-4 150 64 no

1e-4

1e-3 (for critic) 1e-3 (for critic)

1e-4

150 32 150 150

64 64 64 64

no no no linear decay

Table 3: Hyperparameter values for transition policy, proximity predictor, and primitive policy as well as TRPO and PPO baselines. For all networks, we use the Adam optimizer with batch size of 64.

C.2 REPLAY BUFFERS
The success buffer BS contains states and their proximity to the corresponding initiation set in successful transitions. We used  = 0.97 for all our experiments. On the other hand, the failure buffer BF contains states in failure transitions. We implement buffers with a deque data structure. New items are added on one end and once a buffer is full, a corresponding number of items are discarded from the opposing end. For all experiments, we use buffers, BS and BF , with a capacity of one million states.
For efficient training of the proximity predictors, we collect successful trajectories of primitive skills which can be sampled during the training of primitive skills. We run 1000 episodes for each primitive and put the first 10%-20% in trajectories into the success buffer as an initiation set. While initiation sets can be discovered via random exploration, we found that this initialization of success buffers improves the efficiency of training by providing initial training data for the proximity predictors.
C.3 THE PROXIMITY PREDICTOR
The proximity predictor takes a state as input which includes joint state information as well as any task specification, such as ceiling and curb information. An proximity predictor consists of 2 fully connected layers of 96 hidden units with ReLU nonlinearities and predicts the proximity to the initiation set based on samples from the success and failure buffers. Iterations consist of 10 epochs over a batch size of 64 and use a learning rate of 10-4. The predictor optimizes the loss in Equation (1), similar to the LSGAN loss (Mao et al., 2017).
C.4 TRAINING TRANSITION POLICIES
A transition policy consists of 2 fully connected layers of 32 hidden units with tanh nonlinearities and predicts the mean and standard deviation of a Gaussian distribution over an action space. A 2-way softmax layer is followed by the last fully connected layer to predict whether to terminate the current transition or not. We train transition policy using PPO (Schulman et al., 2017). Every iteration consists of 5 epochs over batch size.
In this paper, we use neural networks trained with TRPO (Schulman et al., 2015) on dedicated environments as primitive policies (see section D for details on environments and reward functions). A policy network consists of 2 layers of 32 hidden units with tanh nonlinearities and predicts the mean and standard deviation of a Gaussian distribution over actions. For the TRPO and PPO implementation, we used OpenAI baselines2 with default hyperparameters. We trained all primitive policies until the total return converged (up to 10,000 iterations).
1https://github.com/openai/baselines 2https://github.com/openai/baselines
12

Under review as a conference paper at ICLR 2019
C.5 THE PROXIMITY REWARD
Transition policies receive rewards based on the outputs of proximity predictors. Before computing the reward at every time step, we clip the output of the proximity predictor D by clip(D(s), 0, 1) which indicates how close the state s is to the initiation set of the following primitive (higher values correspond to closer states). We define the proximity of a state to an initiation set as step, where step is the shortest number of timesteps required to get to a state in the initiation set. We use  = 0.95 for all experiments. To make the reward denser, for every timestep t, we provide the increase in proximity, D(st+1) - D(st), as a reward for transition policy.
C.6 SCALABILITY
Each sub-policy requires its corresponding transition policy, proximity predictor, and two buffers. Hence, both the time and memory complexities of our method are linearly dependent on the number of sub-policies. The memory overhead is affordable since a transition policy (2 layers of 32 hidden units), a discriminator (2 layers of 96 hidden units), and buffers (1M states) are small.
D TASK DESCRIPTIONS
For every task, we add a control penalty, -0.001  a 2, to regularize the magnitude of actions where a is a torque action performed by an agent. We note that all measures are in meters, and we omit the measures here for clarity of the presentation.
D.1 LOCOMOTION
A bi-pedal planar walker is used for simulating locomotion tasks. The observation consists of the position and velocity of the torso, joint angles, and angular velocities. The observation is represented as a 17-dim vector: (torso.y, torso.z, torso.angle, left thigh.angle, left leg.angle, left foot.angle, right thigh.angle, right leg.angle, right foot.angle, torso.x vel, torso.angular vel, left thigh.angular vel, left leg.angular vel, left foot.angular vel, right thigh.angular vel, right leg.angular vel, right foot.angular vel). The action space is torque control on the 6 joints.
D.1.1 REWARD DESIGN
Different locomotion tasks share many components of reward design, such as velocity, stability, and posture. We use the same form of reward functions, but with different hyperparameters for each task. The basic form of the reward function is as following:
R(s) =vel · abs(vx - vtarget) + alive - height · abs(1.1 - min(1.1, h))+ angle · cos(angle) - foot(vright foot + vleft foot),
where vx, vright foot, and vleft foot are forward velocity, right foot angular velocity, left foot angular velocity; and h and angle are the distance between the foot and torso and the angle of the torso, respectively. The foot velocities help the agent to move its feet naturally. h and angle are used to maintain height of the torso and encourage an upright pose. Forward: The Forward task requires the walker agent to walk forward for 20 meters. To make the agent robust, we apply a random force with arbitrary magnitude and direction to a randomly selected joint every 10 timesteps.
vel = 2, alive = 1, height = 2, angle = 0.1, foot = 0.01, , and vtarget = 3
Backward: Similar to Forward, the Backward task requires the walker to walk backward for 20 meters under random forces.
vel = 2, alive = 1, height = 2, angle = 0.1, foot = 0.01, , and vtarget = -3
13

Under review as a conference paper at ICLR 2019
Balancing: In the Balancing task, the agent learns to balance under strong random forces for 1000 timesteps. Similar to other tasks, the random forces are applied to a random joint every 10 timesteps, but with magnitude 5 times larger.
vel = 1, alive = 1, height = 0.5, angle = 0.1, foot = 0, , and vtarget = 0
Crawling: In the Crawling task, a ceiling of height 1.0 and length 16 is located in front of the agent, and the agent is required to crawl under the ceiling without touching it. If the agent touches the ceiling, we terminate the episode. The task can be completed when the agent passes a point 1.5 after the ceiling and gets 100 additional reward.
vel = 2, alive = 1, height = 0, angle = 0.1, foot = 0.01, , and vtarget = 3
Jumping: In the Jumping task, a curb of height 0.4 and length 0.2 is located in front of the walker agent. The observation contains a distance to the curb in addition to the 17-dimensional joint information, where the distance is clipped by 3. The x location of the curb is randomly chosen from [2.5, 5.5]. In addition to the reward function above, it also gets an additional 100 reward for passing the curb and 200 · vy when the agent passes the front, middle, and end slices of the curb, where vy is y-velocity. If the agent touches the curb, the agent gets -10 penalty and the episode is terminated.
vel = 2, alive = 1, height = 2, angle = 0.1, foot = 0.01, and vtarget = 3
Patrol: The Patrol task is repetitive running forward and backward between two goals at x = -2 and x = 2. Once the agent touches a goal, the target is changed to another goal and the sparse reward +1 is given. The dense reward alternates between the reward functions of Forward and Backward. The agent gets the reward of Forward when the agent is heading toward x = 2 and gets the reward of Backward, otherwise.
Hurdle: The Hurdle environment consists of 5 curbs positioned at x = {8, 18, 28, 38, 48} and requires repetitive walking and jumping behaviors. The position of each curb is randomized with a uniformly sampled value from [-0.5, 0.5]. The sparse reward +1 is given when the agent jumps over a curb (i.e. pass a point 1.5 after a curb). The dense reward for Hurdle has 8 reward components to guide the agent to learn the desired behavior. By extensively designing dense rewards, it is possible to solve complex tasks. In comparison, our proposed method learns from sparse reward by re-using prior knowledge and doesn't require reward shaping.
Obstacle Course: The Obstacle Course environment replaces two curbs in Hurdle with a ceiling of height 1.0 and length 3. The sparse reward +1 is given when the agent jumps over a curb or passes through a ceiling (i.e. pass a point 1.5 after a curb or a ceiling). The dense reward is alternating between Jumping before the curb and Crawling before the ceiling.
D.1.2 TERMINATION SIGNAL
Locomotion tasks except Crawling fail if h < 0.8 and Crawling fails if h < 0.3. Forward, Backward, and Balancing tasks are considered successful when the agent does not fail for 50 timesteps. The agent succeeds on Jumping and Crawling if the agent passes the obstacles by a distance of 1.5.
D.2 ROBOTIC MANIPULATION
In object manipulation tasks, a 9-DOF Jaco robotic arm is used as an agent and a cube with the side length 0.06 m is used as a target object. We followed the tasks and environment settings proposed in Ghosh et al. (2018). The observation consists of the position of the base of the Jaco arm, joint angles, and angular velocities. The observation is represented as a 31-dim vector: (link1.angle, link2.angle, link3.angle, link4.angle, link5.angle, link6.angle, finger1.angle, finger2.angle, finger3.angle, link1.angular vel, link2.angular vel, link3.angular vel, link4.angular vel, link5.angular vel, link6.angular vel, finger1.angular vel, finger2.angular vel, finger3.angular vel, box.x, box.y, box.z, box.x vel, box.y vel, box.z vel, box.quat i, box.quat j, box.quat k, box.quat w, box.angular vel 1, box.angular vel 2, box.angular vel 3).
The action space is a torque control on 9 joints.
14

Under review as a conference paper at ICLR 2019
D.2.1 REWARD DESIGN AND TERMINATION CONDITION Pick: In the pick task, the position of the box is randomly initialized within a square region of size 0.1 m × 0.1 m with a center (0.5, 0.2). There is an initial guide reward to guide the arm to the box. There is also an over reward to guide the hand directly over the box. When the arm is not picking up the box, there is a pick reward to incentivize the arm to pick the box up. There is an additional hold reward that makes the arm hold the box in place after picking up. Finally, there is a success reward given after the arm has held the box for 50 frames. The success reward is scaled with number of timesteps to encourage the arm to succeed as quickly as possible.
R(s) = guide·1Box not picked and Box on ground+pick·1Box in hand and not picked+hold·1Box picked and near hold point guide = 2, pick = 100, hold = 0.1
Catch: The position of the box is initialized at (0, 2.0, 1.5) and the directional force of size 110 is applied to throw the box toward the agent with randomness (0.1 m × 0.1 m).
R(s) = 1Box in air and Box within 6 of Jaco end-effector Toss: The box is randomly initialized on the ground at (0.4, 0.3, 0.05) within a 0.005 × 0.005 m square region. A guide reward is given to guide the arm to the top of the box. A pick reward is then given to lift the box up to a specified release height. A release reward is given if the box is no longer in the hand. A stable reward is given to minimize variation in the box's x and y direction. An up reward is given while the ball is traveling upwards in air, up until the box hits a specified z height. Finally, a success reward is given based on the landing position of the box and the specified landing position. Hit: The box is randomly initialized overhead the arm at (0.4, 0.3, 1.2) within a 0.005 × 0.005 m square region. The box falls and the arm is given a hit reward for hitting the box. Once the box has been hit, a target reward is given based on how close the box is to the target. Repetitive pick: The repetitive pick task has two reward variants. The sparse version gives a reward 1 for every successful pick. The dense reward version gives a guide reward to the box after each successful pick following the reward for the pick task. Repetitive catch: The repetitive catch task gives a reward 1 for every successful catch. For dense reward, it uses the same reward function with that of the catch task. Pick-Catch: The pick catch task gives a reward 1 for every successful pick or catch. In the dense reward setting, it provides the reward following the pick and catch according to the box position. Serve: The serve task gives a toss reward for a successful toss and a target reward for successfully hitting the target. The dense reward setting provides the toss and hit reward according to box position.
15

