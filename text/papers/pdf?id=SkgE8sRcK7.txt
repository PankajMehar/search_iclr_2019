Under review as a conference paper at ICLR 2019
SAMPLE EFFICIENT DEEP NEUROEVOLUTION IN LOW DIMENSIONAL LATENT SPACE
Anonymous authors Paper under double-blind review
ABSTRACT
Current deep neuroevolution models are usually trained in a large parameter search space for complex learning tasks, e.g. playing video games, which needs billions of samples and thousands of search steps to obtain significant performance. This raises a question of whether we can make use of sequential data generated during evolution, encode input samples, and evolve in low dimensional parameter space with latent state input in a fast and efficient manner. Here we give an affirmative answer: we train a VAE to encode input samples, then an RNN to model environment dynamics and handle temporal information, and last evolve our low dimensional policy network in latent space. We demonstrate that this approach is surprisingly efficient: our experiments on Atari games show that within 10M frames and 30 evolution steps of training, our algorithm could achieve competitive result compared with ES, A3C, and DQN which need billions of frames.
1 INTRODUCTION
Training an agent that can handle difficult tasks in complex environment is always challenging for artificial intelligence. Most works for solving such problems are using reinforcement learning algorithms, e.g. learning a value function (Watkins & Dayan, 1992), or a policy function (Kakade, 2002), or both (Sutton et al., 2000). Combined with deep neural networks, the so-called deep reinforcement learning achieved great success in playing Atari games (Mnih et al., 2015), operating robots (Yahya et al., 2017), and winning human experts in challenging competitions like Go and Dota (Silver et al., 2016; OpenAI, 2018). Recently, a portion of works are trying to combine deep neural networks with evolution strategies to solve similar problems. This approach, called deep neuroevolution, achieved competitive results compared to deep reinforcement learning on Atari Games and Mujoco tasks as well in much shorter training time due to its outstanding scalability and feasibility for parallelization (Salimans et al., 2017; Such et al., 2017).
However, large-scale deep neuroevolution is both data and computation inefficient: it consumes thousands of CPU cores and needs billions of frames and thousands of generations to obtain significant results. In this work, we introduce a novel way to improve its efficiency: we train a variational autoencoder (VAE) to encode samples, and a recurrent neural network to model environment dynamics and handle temporal information; afterward, we train a two layer policy network with latent state vectors using covariance matrix adaption evolution strategy (CMA-ES).
We evaluate our sample efficient deep neuroevolution algorithm, or in short SEDN, on 50 Atari Games from OpenAI Gym (Brockman et al., 2016), experiment shows that our approach surpassed DQN (Mnih et al., 2013), A3C (Mnih et al., 2016), and ES (Salimans et al., 2017) methods in several Atari games within much fewer state frames, shown in Table 1. Our key findings are listed below:
и Evolution goes faster with latent state input. SEDN takes less than 30 evolution steps with only 32 children to surpass ES (Salimans et al., 2017) in several games: the latter needs thousands of children and generations.
и Training is more data efficient. Compared to ES and asynchronous RL methods which needs billions of frames to train, SEDN takes less than 10 million frames to achieve targets.
и Training is more computation efficient. On a workstation of one Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz and one Titan X Pascal GPU, SEDN takes less than 4 hours to finish the evolution, compared to ES which needs a cluster to finish.
1

Under review as a conference paper at ICLR 2019

2 METHOD

Many real world decision making problems could be modeled as Markov Decision Process (MDP).
Since proposed by Bellman (1957), it has been applied in many disciplines, e.g. robotics, economics and game playing. A Markov decision process is defined by: a set of states S, a set of actions A, transition probabilities defining probability over next state given current state and action P : S О A О S  [0, 1], a initial state distribution d : S  [0, 1], and a reward function that maps states and actions into a scalar R : S О A  R. A finite time horizon episode  of T steps is a sequence of state-action-reward tuples: (s0, a0, r0), (s1, a1, r1), . . . , (st, at, rt), . . . (sT , , ) called trajectory, where st  S, at  A, rt = R(st, at), st+1  P (st, at), s0  d0(s) where sT is called terminal state.

For a finite time horizon MDP, reinforcement learning (RL) algorithms learn a parameterized policy

that defining probability of actions to be taken given current states  : S О A  [0, 1], and

maximize episodic reward:

T -1 i=0

ri,

or

formally:

T
maximize E R(st, at)
 i=0
where s0  d(s),at  (st), st+1  P (st, at)

(1) (2)

Above objective function takes the expectation over episodic rewards, which could be approximated by sampling N trajectories from policy  and environment dynamics P and d. We misuse notations such that r( ) donates episodic reward and p( ) represents trajectory probability, then we have below reformulation:

N

maximize L() = r(i)p(i)
 i=1

T -1

T -1

where p( ) = d(s0) (st, at)p(st, at, st+1), r( ) = ri

t=0 t=0

(3) (4)

Vanilla policy gradient (PG) methods optimize L() by taking its gradient:

 L



1 N

N

r(i) log p(i)

i=1

1N

Ti -1

= N

r(i)

 log (st, at)

i=1 t=0

(5) (6)

and updates policy  to maximize expected episodic reward. Variants of policy gradient methods achieved great success in recent years, e.g. TRPO (Schulman et al., 2015) and PPO Schulman et al.
(2017).

Evolution strategies (ES), instead, take another approach to solve above optimization problem without any differentiation: it approximates the gradient by sampling, shown in Equation 7.

 L



1 2 E^N (0,2)(L(

+ ^)^)

(7)

Vanilla ES solves MDP problems iteratively shown in Algorithm 1: firstly, it samples Gaussian noise from a normal distribution; secondly, it injects noise into current policy, and evaluates it in environment, obtaining episodic rewards; thirdly, it updates current policy parameters by the product of episode reward and noise injected. ES and PG are interpreted as two faces of Gaussian smoothing on policy: ES is on parameter space smoothing and PG is on action space smoothing (Salimans et al., 2017).

2

Under review as a conference paper at ICLR 2019

Algorithm 1 Evolution Strategies

Input: Learning rate , noise standard derivation , initial policy parameter 

1: for t = 0, 1, . . . do

2: Sample N parameter noise: ^i  N (0, 2I) for i = 1, . . . , N

3: Execute perturbated deterministic policies i : at = arg maxa i (st) to produce trajectory

i, where i =  + ^i for i = 1, . . . , N

4:

Update policy parameter    + /(N 2)

N i=1

r(i)^i

5: end for

ES enjoys several advantages over PG. Firstly, it is a black-box optimization method that does not need gradients. In problems where gradients are not available, or state encoding is not feasible, or gradient is blocked by some operations in neural networks, ES exhibits its wider range of applications over PG. Secondly, in low dimensional optimization problems like Mujoco tasks, ES has competitive performance with PG (Salimans et al., 2017), but is less sensitive to hyperparameters (Heidrich-Meisner & Igel, 2008). Thirdly, the exploration-exploitation trade-off needs to be balanced in RL here is integrated: policy parameter perturbation ensures different behaviors during evaluation, whereas RL has to use a stochastic policy to explore. Lastly, ES is highly scalable therefore can be trained faster by a group of workers in parallel.
However, in n-dimensional search space where n is large, the exploration directions O(2n) increases in orders with n, therefore random sampling for gradient approximation would be very inefficient. For video games like Atari, high dimensional pixel state space induces more parameters of policy to be optimized, hence less efficient for ES. Surprisingly, Salimans et al. (2017) shows that even natural evolution strategies (NES) can beat common RL algorithms in several Atari games. Among the class of evolution strategies, the covariance matrix adaption evolution strategy (Hansen & Ostermeier, 2001) is the most successful one in solving low dimensional optimization problems. Would covariance matrix adaption evolution strategy (CMA-ES) perform better or faster over NES for high dimensional tasks? To answer this question, one problem has to be solved, that the application of CMA-ES on high dimensional space is infeasible since it needs to compute a large covariance matrix of size O(n2). Recently advances in deep learning show promises to encode high dimensional spatial-temporal information into low dimensional latent vectors (Oh et al., 2015; Pathak et al., 2018; Ha & Schmidhuber, 2018) by modeling environment dynamics. In this case, we can convert high dimensional RL tasks into low dimensional tasks, where CMA-ES can take its advantage.

2.1 COVARIANCE MATRIX ADAPTION EVOLUTION STRATEGY

CMA-ES improves ES in two ways. Firstly, vanilla ES samples noise from a fixed Gaussian distribution N (0, 2I) of only one degree of freedom , the step size; whereas CMA-ES, samples noise from N (0, C) where covariance matrix C has n(n + 1)/2 degree of freedom. Secondly, CMA-ES
updates sampling parameters , C by rank based method, rather than updating policy parameter by
approximating its gradients.

We now explain rank-based ES. Rank-based ES is a selection-recombination process. Let f donates

the wх

objective function,

> 0,

х i=1

wх

=

 be 1, i

population size, = 1, . . . , х} be

х be parent population size, {wi|w1 > w2 > weight coefficient for recombination. Firstly,

иии > Rank-

based ES first samples  points from Gaussian distribution xi  m + Ni(0, C) for i = 1, . . . , .

Next, it evaluates sampled points on the objective function, and sort according to the cost such that

f (x1) matrix

< f (x2) m

< иии < f

х i=1

wixi,

(x). C

In

the following, it recombines to get new mean and covariance

х i=1

wi(xi

-

m)(xi

-

m)T .

This procedure repeats for G

generations.

Similar to gradient based optimization methods, CMA-ES further uses momentum, step size control

and path accumulation for more stable optimization. Let yi(g+1)  N (0, C(g)), i = 1, . . . ,  be sampled noise to be injected, xi = m + yi, suppose {yi|i = 1, . . . , х} are sorted such that f (x1) < f (x2) < и и и < f (x), updating of covariance matrix with momentum gives:

х

C(g+1) = (1 - cх)C(g) + cх

wiyi(g+1)yi(g+1)T

(8)

i=1

3

Under review as a conference paper at ICLR 2019

where g is the number of generation and cх is momentum constant. In favor to generate best point from the current generation, it further adds the best point's covariance term into Equation 8:

х

C(g+1) = (1 - cх - c1)C(g) + c1y1(g+1)y1(g+1)T + cх

wiyi(g+1)yi(g+1)T

i=1

In practice, exponential smoothing over parents' mean path, or evolution path, is used:

(9)

p(cg+1) = (1 - cc)p(cg) + cc(m(g+1) - m(g))/(g)

(10)

х

C(g+1) = (1 - cх - c1)C(g) + c1pc(g+1)pc(g+1)T + cх

wiyi(g+1)yi(g+1)T

i=1

(11)

Next, we explain how to control the step size . The step size is related to the evolution path: it should be decreased if the path is short and increased if the path is long. In contrast to evolution path pc, here we need to correct its directions by taking conjugate evolution path p:

p(g+1)

=

(1

-

c )p(g)

+

c

C(g),-

1 2

(m(g+1)

-

m(g))/(g)

(12)

To update step size, we compare conjugate evolution path with the expectation of length of vectors sampled from normal distribution N (0, I):

ln (g+1)

= ln (g) +

c ( d E

p N (0, 1)

- 1)

(13)

Hansen (2016) suggests default hyperparameters for CMA-ES, we summarize complete CMA-ES in Algorithm 2.

Algorithm 2 CMA-ES

Input: wi

m  Rn,   R+, such that хw = 1/

, х  N, х

х i=1

wi2



< , C = I, pc = 0, p 0.3, w1  и и и  wх >

= 0,

0

х i=1

wi

=

1

cc,  4/n, c  4/n, c1  2/n2, cх  хw/n2, c1 + cх  1, d  1 + хw/n

1: for each generation step do

2: Sampling: yi  Ni(0, C), xi = m + yi for i = 1, . . . , 

3: Evaluation: obtain f (xi), sort {xi|i = 1, . . . , } points such that f (x1)  и и и  f (x)

4: 5: 6:

Mean update: m  Evolution path update:

х i=1
pc

wixi, yw =  (1 - cc)pc

х i=1

wiyi

+ yw 1

-

(1

-

cc)2хw

Conjugate evolution path update: p  (1 - c)p + C-1yw 1 -

(1

-

c

)2

 хw

7:

Covariance matrix update: C  (1 - c1 - cх)C + c1pcpcT + cх

х i=1

wiyiyiT

8:

Step size update:







exp(

c d

(E

p N (0,1)

- 1))

9: end for

CMA-ES addresses typical problems in non-linear optimization problems like ill-conditioning and ruggedness. The covariance matrix approximates the inverse Hessian matrix, and the updates approximate natural gradient by adapting the search metric into a sphere, hence increasing performance by orders of magnitude. Rank based selection ensures its invariance against translation and rotation in search space. The step size control facilitates fast convergence. However, it is not applicable to deep neural networks used in DRL which has millions of parameters and induces O(n2) order of the size of the covariance matrix. This leads to an intuitive approach by encoding frames into small latent vectors to model the dynamics of the game environment, and evolve with a lower dimensional policy parameter space.

4

Under review as a conference paper at ICLR 2019

Input Image

Mean Vector
Sampled  Latent  Vector

Conv Encoder Network

Deconv Decoder Network

Standard Derivation
Vector

(a) Variational Autoencoder

Current Latent State
zt
Current Action Embeded
at

Predicted Latent Mean

Predicted Next Latent
State

RNN

Predicted Latent Std

zt +1

Previous RNN Hidden
State
ht-1
Current Latent State
zt

MLP Controller

(b) LSTM Forward Model Dynamics

(c) Policy Network

Action to take
at

Figure 1: Our SEDN model. VAE encodes state frame into latent vector; LSTM encodes current latent state and action, predicts next latent state; policy network takes current latent state and hidden state from previous timestep of RNN as input, predict action to execute

2.2 ENVIRONMENT DYNAMICS MODELING FOR EPISODIC DATA ENCODING
How to learn a model that can encode episode data  : s0, a0, s1, a1, . . . sT , aT ? We develop a method by modeling environment dynamics. Perhaps the earliest work on building predictive model for vision based RL tasks was introduced by Schmidhuber & Huber (1991), which proposed using neural networks to predict attention region given previous frames and actions. After the success of deep learning, Oh et al. (2015) proposed an Encoding-Transformation-Decoding framework for action conditional video prediction, and applied on Atari games. More recently, Pathak et al. (2018) learns forward and inverse model dynamics to imitate expert's behavior given only image sequences. Ha & Schmidhuber (2018) also learned forward model dynamics using random policy demonstrations, and train a controller totally inside the dream.
In this work, we take similar approach to encode episodic data. Specifically, we first train a Variational Autoencoder (VAE) that encode spatial information; then train an recurrent neural network (RNN) to encode temporal information by learning model dynamics. Let e : S  Rn, d : Rn  S donates our encoder and decoder network where n is our latent space dimension, f : Rn О A  Rn be the our RNN, and c : Rn О Rm  A be our policy network where m is the dimension of our RNN's hidden state. The encoder e encodes state frame st into a latent vector zt:

х(st), (st) = e(st) zt  N (х(xt), (xt))

(14) (15)

The decoder d decodes latent vector back to state frame s^t. VAE is trained to minimize the reconstruction loss between st, s^t, and KL-divergence between latent vector and N (0, I):
5

Under review as a conference paper at ICLR 2019

s^t = d(zt) Lvae = st - s^t 2 + KL[N (х(st), (st)) N (0, I)]

(16) (17)

LSTM f models the environment dynamics in latent state space, and encode temporal information:

х(s^t+1), (s^t+1) = f (zt, at) z^t+1  N (х(s^t+1), (s^t+1))

(18) (19)

LSTM is trained to minimize the KL-divergence between actual latent state from vae's encoder and predicted latent state:

х(st+1), (st+1) = e(st+1) Lrnn = KL[N (х(s^t+1), (s^t+1)) N (х(st+1), (st+1))]

(20) (21)

The policy network takes the latent vector of the current state, and previous timestep's hidden state of RNN, we use deterministic policy in this work:
at  c(zt, ht-1)

We illustrate our model in Figure 1.

3 EXPERIMENTS
Our complete training pipeline is summarized in Algorithm 3. We use OpenMPI's master-slave mode (Scarabello & Clipp, 2018) for parallel training of policy network during evolution steps, Hansen (2018) for CMA-ES optimizer, and Pytorch (Paszke et al., 2017) deep learning platform to train VAE and RNN. We train the three modules separately. Firstly, we collect episode data during training the policy network using CMA-ES. Secondly, we extract all video frames from collected data and train our variational autoencoder. Thirdly, we convert all state frames into latent state vectors using VAE trained. Lastly, we train our LSTM using the converted episode data. This training cycle iterates till the end.
During the training, we set our training data buffer to be a FIFO queue of size 1M : this will ensure the safety of workstation memory. The training process will stop when the maximum number of frames is reached or the maximum evolution step is reached, whichever is earlier. Our parallel training adopts a task-queue master-slave mode: the master node immediately distributes remaining tasks once there is any slave node available, and slave node requests new task right after it finishes a previous task. Unlike Salimans et al. (2017), our master-slave communication frequency is so low that it only takes a small percentage of time compared to episode data sampling steps.
We conduct experiments on 50 Atari 2600 games in OpenAI Gym (Brockman et al., 2016). We choose a larger encoder convolutional network compared to Vanilla DQN (Mnih et al., 2013) for better VAE performance: our experiment shows poor decoding performance if following Vanilla DQN's convolutional network. Our data pre-possessing is also different from Vanilla DQN: we resize image frames directly without frame stack, grayscale conversion or frame skip. We tried to stack latent state vector as input for policy network but found performance drop: the reason may be that increased dimension required for policy network slows down the evolution process for CMA-ES optimizer.
Our encoder network consists of 4 convolutional layers with 32, 64, 128, 256 channels followed by a flatten layer and a linear layer of 128 units. The convolutional layers use 4 О 4, 4 О 4, 3 О 3, 3 О 3 filter size and strides of 2, 2, 1, 1. Our decoder network reverses the operations except the last deconvolutional layer uses 6 О 6 filter size. Each convolutional and deconvolutional layer uses rectified activation, except that last convolutional layer uses tanh activation. Image frames are resized into (84, 84, 3) to feed into our network. We set our latent state dimension to be 128. We use one layer LSTM of 128 hidden size to model environment dynamics, therefore our input dimension of policy network is 256. Our policy network is a two-layer MLP of size 256 - 32 - na, where na is the action space dimension.

6

Under review as a conference paper at ICLR 2019
The training of VAE and RNN is triggered every 8 evolution step. Each child policy is evaluated for 16 episodes, and the average episode reward is returned for CMA-ES optimizer to update. To visualize the training process of policy network, we make a boxplot of average episode reward distribution of Frostbite, shown in Figure 2 The maximum reward performance among policy networks increases significantly right after the training of VAE and RNN triggered at 8th, 16th generations. This agrees with our assumption that VAE and RNN encode spatial-temporal information from past experience and provides good feature representation for policy to evolve. It is worth to notice that for some game, e.g. Private Eye, policy performance increases significantly even without updating VAE and RNN; this observation agrees with Blundell et al. (2016) where features are extracted from random matrix projection.
Figure 2: Boxplot of the training process of Frostbite. Maximum reward performance among policy networks increases significantly after training of VAE and RNN triggered at 8th, 16th generations.
Figure 3: Boxplot of the training process of Atlantis. Maximum reward performance among policy networks drops significantly after training of VAE and RNN triggered at 8th generations.
However, we also notice that for some games, policy networks performance drops significantly right after updating of VAE and RNN, e.g Atlantis shown in Figure 3. We find that the cause may be the poor encoding performance of VAE. Shown in Figure 4, we compare VAE's encoding-decoding on Frostbite and Atlantis. We find that image decoded from VAE will ignore tiny moving objects, leaving only a static background; this issue occurs especially in rare frames far from episode starts; similar phenomenon was found in games like Kangaroo, Gravitar and Breakout. Experiment result is summarized in Table 1. Our SEDN surpasses ES, DQN, and A3C in 7 of 50 Atari games, but with only 10M frames, whereas DQN using 200M frames, and ES and A3C using 1B frames. Due to the limitation of time and resources, we did not conduct hyperparameter tuning on SEDN. We believe better encoding techniques to address above issues will give a boost of performance on SEDN. We left this as an open direction in future works.
4 DISCUSSION
Whereas traditional reinforcement learning algorithms need to balance between exploration and exploitation (Sutton et al., 1998), and even add perturbation on parameters to ensure good exploration (Plappert et al., 2017); evolution strategy directly drives exploration by sampling from optimizer's distribution parameters (Salimans et al., 2017) during evolution: it integrates exploration and exploitation. This is what makes it so efficient especially in some low dimensional control tasks like CartPole. By encoding spatial-temporal information from high dimensional pixel space into low dimensional latent space, a challenging RL task could hence be converted into a simpler task where evolution strategies have advantages over deep reinforcement learning methods.
7

Under review as a conference paper at ICLR 2019
(a) Frostbite #1 (b) Frostbite #16 (c) Atlantis #1 (d) Atlantis #16 Figure 4: Comparison of VAE's encoding-decoding on Atlantis and Frostbite. Upper: Original image; Lower: decoded image from the latent vector. Notice for Frostbite, decoded image agrees with original image at the 16th frame; but for Atlantis decoder does not recover the original small objects at 16th frame
Similar to what Salimans et al. (2017) are facing: ES does not perform well on some easy games like Enduro and Breakout. One possible reason is that ES's optimized policy is close to where it is randomly initialized due to evolution path control under ES, whereas optimal policy for dense reward games like Breakout are far away from where it is randomly initialized. Another reason is that, long episodic time horizon games like Enduro are very inefficient for ES to solve: given limited number of frames, a longer episode will result in less number of episodes, hence less and slower updates for ES. In addition, episode rewards ignore temporal reward information over time steps. This issue is critical in games like Enduro. Enduro is a car racing game, in which agent obtain rewards after passing other cars and penalty if it is passed by other cars, final ranking will be accounted as the episodic reward after a long episode. A random policy will always obtain a zero episodic reward under this case, hence ES learns nothing from such games. These two issues limit the application of vanilla ES and its variants, remaining to be resolved in future works. A possible direction is a mixture of ES and RL, e.g. using ES as a warm start of RL policy.
5 CONCLUSION
Our work introduces a fast and efficient evolution algorithm that trains deep neural networks to play Atari games. Our experiment shows that encoding spatial-temporal information from high dimensional pixel space into low dimensional latent space makes ES fast, effective and efficient: SEDN outperforms DQN, A3C, and ES in 30 evolution steps with 10M frames experience in several Atari games. We conclude SEDN is a competitive approach to solving challenging RL tasks. In future works, we plan to address issues discussed in section 4 by applying state of the art computer vision techniques like self-attention maps for better information encoding; we would also like to apply information encoding with RL to understand feature importance for RL.
8

Under review as a conference paper at ICLR 2019

Table 1: Result is obtained using SEDN on Atari games with 16 re-runs with up to 30 random initial no-ops compared with DQN Mnih et al. (2013), A3C Mnih et al. (2016), and ES Salimans et al. (2017); DQN, A3C, ES data are obtained from Salimans et al. (2017). Enduro's data is missing due to its long episode problem discussed in Section 4

DQN A3C, FF ES, FF SEDN, Ours

Frames Time

200 M 1 B 7-10 day 4 day

1 B 10 M 4 hour 4 hour

Amidar Assault Asterix Asteroids Atlantis Bank Heist Battle Zone Beam Rider Berzerk Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar Ice Hockey Kangaroo Krull Montezumas Revenge Name This Game Phoenix Pit Fall Pong Private Eye Q*Bert River Raid Road Runner Robotank Seaquest Skiing Solaris Space Invaders Star Gunner Tennis Time Pilot Tutankham Up and Down Venture Video Pinball Wizard of Wor Yars Revenge Zaxxon

133.4 3332.3 124.5 697.1 76108 176.3 17560 8672.4
41.2 25.8 303.9 3773.1 3046 50992 12835.2 21.6 475.6 2.3 25.8 157.4 2731.8 216.5 3.8 2696 3864 50 5439.9
16.2 298.2 4589.8 4065.3 9264 58.5 2793.9
1449.7 34081 2.3 5640 32.4 3311.3 54 20228.1 246
831

283.9 3746.1 6723 3009.4 772392 946 11340 13235.9 1433.4 36.2 33.7 551.6 3306.5 4669 101624 84997.5 0.1 82.2 13.6 0.1 180.1 8442.8 269.5 4.7 106 8066.6 53 5614 28181.8 123 11.4 194.4 13752.3 10001.2 31769 2.3 2300.2 13700 1884.8 2214.7 64393 10.2 5825 26.1 54525.4 19 185852.6 5278 7270.8 2659
9

112 1673.9 1440 1562 1267410 225 16600 744 686 30 49.8 9.5 7783.9 3710 26430 1166.5 0.2 95 49 31 370 582 805 4.1 11200 8647.2 0 4503 4041 0 21 100 147.5 5009 16590 11.9 1390 15442.5 2090 678.5 1470 4.5 4970 130.3 67974 760 22834.8 3480 16401.7 6380

33.4 175.9 387.5 519.4 5237.5 13.1 3625 332.8 224.2 97.6 55.3 8.8 3586.9 3718.8 23662.5 1812.4 -8.8
-75.4 23.8 387.5 242.5 131.2 -2.9 262.5 1238.2 0 5403.1 1258.1 0 21 6674.6 160.9 531 26430.8 4 128.8 16948.4 2832.5 148.8 337.5 -2.1 1518.8 12.5 8031.8 12.5 23058.4 606.2 2953.7 262.5

Under review as a conference paper at ICLR 2019

Algorithm 3 Complete Training Pipeline

Input: VAE encoder enc : xt  zt, VAE decoder dec : zt  xt RNN f : (zt, at)  zt+1, Policy p : (zt, ht-1)  at and CMA-ES optimizer: g

1: for each iteration do

2: training data buffer X = {}

// Train policy network and collect data

3: for each evolution step do

4: sample  children {c1, . . . , c} from CMA-ES optimizer g

5: for child i from 1 to  do

6: contrust policy pi from ci

7: for each episode j from 1 to N do

8: sample episode data Yij = (s0, a0, . . . , sT , aT ) and episode reward Rij with VAE,

RNN and Policy:enc, f, pi 9: append episode data to training data X  X  {Yij}

10: end for

11:

Calculate average return for child i: R»i =

N j=1

Rij

/N

12: end for

13: Update CMA-ES optimizer g  g(c1, R»1, c2, R»2, . . . , c, R»)

14: end for

// Train VAE and extract frames

15: Training image frames I = {}

16: for each Y in X do

17: (s1, a1, . . . , sT , aT )  Y 18: I  I  {s1, s2, . . . , sT }
19: end for

20: Train and update VAE enc, dec with I 21: Extracted episode data S = {}

22: for each Y in X do

23: (s1, a1, . . . , sT , aT )  Y 24: S  S  {enc(s1), a1, . . . , enc(sT ), aT }

25: end for

// Train RNN

26: for each Y in S do

27: (enc(s1), a1, . . . , enc(sT ), aT )  Y 28: х1:T , 1:T = enc(s1:T ), z1:T  N (х1:T , 1:T )

29: х^2:T , ^2:T = f (z1:T -1, a1:T -1) 30: Calculate KL-divergence Loss L = KL[N (х(s^t+1), (s^t+1)) N (х(st+1), (st+1))]

31: Backward NLL loss and update RNN f

32: end for

33: end for

34: Construct final policy pf from CMA-ES optimizer's favorite solution

35: return dec, enc, f, pf

10

Under review as a conference paper at ICLR 2019
REFERENCES
Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, pp. 679Г684, 1957.
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
David Ha and Juеrgen Schmidhuber. World models. arxiv preprint. arXiv preprint arXiv:1803.10122, 2018.
Nikolaus Hansen. The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.
Nikolaus Hansen. pycma. https://github.com/CMA-ES/pycma, 2018.
Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary computation, 9(2):159Г195, 2001.
Verena Heidrich-Meisner and Christian Igel. Evolution strategies for direct policy search. In International Conference on Parallel Problem Solving from Nature, pp. 428Г437. Springer, 2008.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems, pp. 1531Г1538, 2002.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928Г1937, 2016.
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In Advances in neural information processing systems, pp. 2863Г2871, 2015.
OpenAI. Openai five, Jul 2018. URL https://blog.openai.com/openai-five/.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. In International Conference on Learning Representations, 2018.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Luca Scarabello and Landon T. Clipp. mpi-master-slave. https://github.com/luca-s/ mpi-master-slave, 2018.
11

Under review as a conference paper at ICLR 2019
Juergen Schmidhuber and Rudolf Huber. Learning to generate artificial fovea trajectories for target detection. International Journal of Neural Systems, 2(01n02):125Г134, 1991.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889Г1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057Г1063, 2000.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279Г292, 1992. Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen Chebotar, and Sergey Levine. Collective robot
reinforcement learning with distributed asynchronous guided policy search. In Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on, pp. 79Г86. IEEE, 2017.
12

