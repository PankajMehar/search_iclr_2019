Under review as a conference paper at ICLR 2019
ISONETRY: GEOMETRY OF CRITICAL INITIALIZATIONS
AND TRAINING
Anonymous authors Paper under double-blind review
ABSTRACT
Recent work on critical initializations of deep neural networks has shown that by constraining the spectrum of input-output Jacobians allows for fast training of very deep networks without skip connections. The current understanding of this class of initializations is limited with respect to classical notions from optimization. In particular, the connections between Jacobian eigenvalues and curvature of the parameter space are unknown. Similarly, there is no firm understanding of the effects of maintaining orthogonality during training. With this work we complement the existing understanding of critical initializations and show that the curvature is proportional to the maximum singular value of the Jacobian. Furthermore we show that optimization under orthogonality constraints ameliorates the dependence on choice of initial parameters, but is not strictly necessary.
1 INTRODUCTION
Deep neural networks have been proven extremely powerful, achieving empirical success in a vast array of problems, ranging from image recognition (He et al., 2015), amortized probabilistic inference (Ritchie et al., 2016) to inferring the dynamics of neural data (Pandarinath et al., 2018). The practical hindrance in their application often stems from the difficulty in training them, both due to the excessive computational cost of running many epochs of gradient descent but also due to the inherent instability in gradient computation. It is therefore of great practical and theoretical interest to devise effective gradient optimization techniques for deep neural networks. A new and promising approach exploits mean field assumptions and random matrix theory to devise initialization strategies that ensure the boundedness of the backpropagated gradients (Schoenholz et al., 2016; Pennington et al., 2017). In particular, Pennington et al. shows that for orthogonal networks with appropriately chosen parameters, the hidden layer input-output Jacobian matrix is nearly isometric, approximately preserving the 2 norm of the gradients. Despite achieving an impressive increase in training speed, the exact mechanism by which this initialization confers its advantage is not well understood. In particular, it invites questions from an optimization perspective on how the boundedness of the Jacobian matrix relates to notions such as gradient smoothness, or (negative) strong convexity that describe the optimization landscape.
Recently, there have been several orthogonal motivations ranging from optimization speed, robustness against adversarial examples, and improving quality of generated samples for orthogonal initialization procedures (Xie et al., 2017; Ozay & Okatani, 2016; Cisse et al., 2017; Odena et al., 2018). All of which raise the question about the relative importance of maintaining orthogonality during training as compared to critical initializations proposed in (Pennington et al., 2017). In this work we study simple feed-forward, fully connected networks. We make theoretical advances in understanding the connection between eigenvalues of the Jacobian and local measures of gradient smoothness. Subsequently we go onto analyze experimentally, through the lens of manifold optimization the importance of maintaining orthogonality or near orthogonality throughout training of deep neural networks with 200 layers. This allows us to show that nearly isometric networks optimize worse because their gradients rapidly become less smooth. In contrast, less isometric networks gradually degrade in smoothness, which allows them to be trained with a higher learning rate and converge faster. Moreover, we show that networks trained with orthogonality and near orthogonality constraints are considerably less sensitive to initialization and optimize rapidly regardless. Contrary to the recent conjecture by Santurkar et al. (2018), this effect is not attributable to an increase in gradient smoothness. Our work suggests that maintaining near orthogonality constraint throughout
1

Under review as a conference paper at ICLR 2019

training provides robust performance in practice compared to the orthogonal weight initialization scheme which requires a highly specific parameter tuning. This offers insight into the role of Weight Normalization (Salimans & Kingma, 2016), of which the near orthogonal constraint can be seen as a variant.

2 BACKGROUND

2.1 FORMAL DESCRIPTION OF THE NETWORK

Following (Pennington et al., 2017; 2018; Schoenholz et al., 2016), we consider a feed-forward,

fully connected neural network with L hidden layers. Each layer l  {1, . . . , L} is given as a

recursion of the form

xl = (hl), hl = Wlxl-1 + bl

(1)

where xl are the activations, hl are the pre-activations, Wl  RN×N are the weight matrices, bl are the bias vectors and (·) is the activation function. For consistency, the input is denoted as x0. The output layer of the network computes y^ = g-1(hg) where g is a link function and hg = WgxL+bg.
The hidden layer input-output Jacobian matrix JxxL0 is given by,

JxxL0

xL x0

=

L

DlWl

l=1

(2)

where Dl is a diagonal matrix with entries Dil,i =  (hil). As pointed out in (Pennington et al., 2017; Schoenholz et al., 2016), the conditioning of the Jacobian matrix affects the conditioning of the back-propagated gradients for all layers.

2.2 INITIALIZATION STRATEGY FOR MAXIMIZING SIGNAL PROPAGATION

Extending the classic result on the Gaussian process limit for wide layer width obtained by Neal

(1996), recent work (Matthews et al., 2018; Lee et al., 2017) has shown that for deep untrained net-

works

with

elements

of

their

weight

matrices

Wi,j

drawn

from

a

Gaussian

distribution

N (0,

W2 N

)

the empirical distribution of the pre-activations hl converges weakly to a Gaussian distribution

N (0, qlI) for each layer l in the limit of the width N  . Under this mean-field condition

the variance the of the distribution is defined recursively given the pre-activations in the preceding

layer:

ql = W2  ql-1h dµ(h) + b2

(3)

where

dµ(h)

denotes

the

standard

Gaussian

measure

dh 2

exp

(

-h2 2

).

The variance of the pre-

activations of the recursion defined

first layer q1 in equation 3

depends on the has a fixed point

q22n, orm

of

the

inputs

q1

=

W2 N

x0

2 2

+

b2 .

The

q = W2  qh dµ(h) + b2

(4)

which can be satisfied for all layers by appropriately choosing W, b and scaling the input x0 accordingly. In order to ensure that Jxx0L is well conditioned, Pennington et al. (2017) require that in addition to the variance of pre-activation being constant for all layers, two additional constraints are
met. Firstly, they require that the mean square singular value of DW for each layer have a certain
value in expectation.



=

1 NE

Tr

(DW)

DW

= W2

 (qh) 2 dµ(h)

(5)

Given that the mean squared singular value of the Jacobian matrix JxxL0 is ()L, they require that  = 1 which corresponds to a critical initialization where the gradients are asymptotically stable as L  . Secondly, they require that the maximal squared singular value s2max of the Jacobian JxxL0 be bounded. In (Pennington et al., 2017) it was shown that for weights with Gaussian distributed

2

Under review as a conference paper at ICLR 2019

elements, the maximal singular value increases linearly in depth even if the network is initialized with  = 1. Fortunately, for orthogonal weights, the maximal singular value smax is bounded even as L   (Pennington et al., 2018) and for piecewise-linear (.), it is analytically derivable and admits a solution in q for smax(Jxx0L ) = 1 for any choice of L. For arbitrary (.)'s, smax can be obtained numerically from the solution of a functional equation describing the density of singular values.
The described, theoretically derived intialization scheme has been tested and has shown a substantial speed up in training times on CIFAR-10. The impressive results invite further inquest into the connection between the class of critical initializations with bounded maximal singular values of the Jacobian matrix and optimization, as well as the specific conditions under which the Gaussian limit on pre-activations holds. Particularly, we show that for random neural networks the maximum singular value of the Jacobian matrix is related to the curvature of the parameter space as measured by the Fisher information matrix G¯ , which in turn affects the upper bound on the maximum stable learning rate (Bottou, 2010). Subsequently, using manifold optimization we analyze how enforcing orthogonality affects the convergence speed. Finally, we show that for random networks with orthogonal weights pre-activations do not necessarily converge in distribution to a Gaussian, raising questions about the conditions under which the mean field approximation holds.

3 RESULTS

3.1 CONNECTION BETWEEN smax(JxxL0 ) AND THE MAXIMUM EIGENVALUE max(G¯ )

Seen from a probabilistic perspective, the output of a neural network defines a conditional probability distribution p(y|x0), where  = {vec(W1), . . . , vec(WL), b1, . . . , bL} is the set of all hidden layer parameters (Botev et al., 2017; Amari, 2016). In this context, the Fisher information matrix

defined as,

G¯ Ex0,y  log p(y|x0) log p(y|x0)

(6)

can be expanding using the chain rule:

G¯ = Ex0,y JxL Wg hg log p(y|x0)hg log p(y|x0) WgJxL .

(7)

Each block of the Fisher information matrix with respect to parameters a, b   can further be

expressed as

G¯ a,b = Ex0 Jha JxhL Wg HgWgJhxL Jhb 

(8)

where the final layer Hessian Hg is defined as h2 g log p(y|x0). We can re-express the outer product of the score function hg log p(y|x0) as the second derivative of the log-likelihood, provided it is twice differentiable and it does not depend on y, which also allows us to drop y from the expecta-
tion. This condition naturally holds for all canonical link functions and matching generalized linear model loss functions. We define the matrix of partial derivatives of the -th layer pre-activations with respect to the layer specific parameters separately for W and b as:

Jah = x-1  I for a = vec(W), Jha = I for a = b

(9)

We can further simplify the expression for the blocks of the Fisher information matrix equation 8, using the fact that the pre-activations converge weakly to an isotropic Gaussian distribution in the limit of wide layers (Matthews et al., 2018; Lee et al., 2017). This justifies assuming independence of the pre-activations of all the layers. We can additionally assume for convenience that the input to the network x0 has been whitened before being scaled by q. The simplified expressions are as follows:

G¯ vec(W),vec(W ) = E x-1x-1  JhxL Wg HgWgJhxL

(10)

= q2I  E JhxL Wg HgWgJxhL

(11)

G¯ b,b = E JxhL Wg HgWgJxhL

(12)

3

Under review as a conference paper at ICLR 2019

G¯ vec(W),b = E x-1  I JxhL Wg HgWgJhxL = E x-1  I E JxhL Wg HgWgJhxL = 0
=0

(13) (14)

We then consider a block diagonal approximation to the Fisher information matrix. Under this msimaxpalificmaatixo(nG¯ tah,ae).mTaxhiemquumalietyigoenfvtahliuseapopfrothxeimtahteionFisishegriviennfobrymathtieongemneartarliixzaitsionmoafxt(hGe¯ )Gershgorin circle theorem to block-partitioned matrices, which is detailed in the Appendix 4.1. Following equation 10, each diagonal block a with respect to the weight matrices W is bounded by
s2max(JxhL )sm2 ax(Wg)smax(Hg) , while the diagonal blocks a with respect to the biases b are
bounded by q2s2max(JhxL )s2max(Wg)smax(Hg) using equation 12. These results can be further
simplified if we initialize Wg as a scaled semi-orthogonal matrix such that Wg Wg = W2 I and consider a neural network regression model with mean-square loss. Then the respective bounds become sm2 ax(JhxL )W2 and q2s2max(JxhL )W2 . Assuming smax(JhxL ) is a monotonically increasing function of L it is sufficient to ensure that smax(JhxL1 ) is small in order for max(G¯) to be small. Furthermore since q 1 and since smax(JxxL0 ) is at most W larger than smax(JhxL1 ), bounding the maximum value of the hidden layer input-out Jacobian bounds the maximum eigenvalue of the Fisher information matrix.

3.2 OPTIMIZATION OVER MANIFOLDS
Optimizing neural network weights subject to manifold constraints has recently attracted considerable interest, with several lines of research focusing on either the approximate preservation of gradient norms in recurrent neural networks (Arjovsky et al., 2015; Henaff et al., 2016; Vorontsov et al., 2017), increasing the robustness to adversarial examples (Cisse et al., 2017), or heuristically motivated improvements to prediction accuracy (Xie et al., 2017; Cho & Lee, 2017; Ozay & Okatani, 2016). In this work we probe how constraining the weights of each layer to be orthogonal or near orthogonal affects the spectrum of the hidden layer input-out Jacobian and of the Fisher information matrix. We briefly review notions from differential geometry and optimization over matrix manifolds (Edelman et al., 1998; Absil et al., 2007), in order to lay ground for a discussion of the specifics of the optimization techniques used in our experiments. Informed readers are encouraged to skip to Sec. 3.2.1. The potentially non-convex constraint set constitutes a Riemannian manifold, when it is locally isomorphic to Rn, differentiable and endowed with a suitable (Riemannian) metric, which allows us to measure distances in the tangent space and consequentially also define distances on the manifold. There is considerable freedom in choosing a Riemannian metric; here we consider the metric inherited from the Euclidean embedding space which is defined as W, W Tr(W W).

Stiefel Manifold Let St(p, n) (p  n) denote the set of all n × p orthonormal matrices St(p, n) {W  Rn×p : W W = Ip}
Notice that for p = n, the Stiefel manifold parametrizes the set of all orthogonal matrices.

(15)

Oblique Manifold Let Ob(p, n) denote the set of all n × p matrices with unit norm columns

Ob(p, n) {W  Rn×p : diag(W W) = 1}

(16)

where diag(M) denotes an operator that extracts the diagonal entries of M. This manifold is equivalent to the direct product p unit-norm spheres Sn-1 × Sn-1 × · · · × Sn-1. To optimize a cost function with respect to parameters lying in a non-Euclidean manifold we
must define a descent direction. This is done by defining a manifold equivalent of the directional
derivative. An intuitive approach replaces the movement along a vector t with movement along a geodesic curve (t), which lies in the manifold and connects two points W, W  M such that (0) = W, (1) = W . The derivative of f ((t)) with respect to t then defines a tangent vector for each t.

4

Under review as a conference paper at ICLR 2019

Tangent vector W is a tangent vector at W if W satisfies (0) = W and

W

df ((t)) dt t=0

 (0)f

(17)

The set of all tangents to M at W is referred to as the tangent space to M at W and is denoted by TWM. The geodesic importantly is then specified by a constant velocity curve  (t) = 0 with initial velocity W. To perform a gradient step, we must then move along W while respecting the man-
ifold constraint. This is achieved by applying the exponential map defined as ExpW(W) (1), which moves W to another point W along the geodesic. While certain manifolds, such as the
Oblique manifold, have efficient closed-form exponential maps, for general Riemannian manifolds,
the computation of the exponential map involves numerical solution to a non-linear ordinary dif-
ferential equation (Absil et al., 2007). An efficient alternative to numerical integration is given by
an orthogonal projection onto the manifold. This projection is formally referred to as a retraction RtW : TWM  M. Finally, gradient methods using Polyak (heavy ball) momentum (e.g. ADAM (Kingma & Ba,
2014)) require the iterative updating of terms which naturally lie in the tangent space. The parallel translation T() : T M T M  T M generalizes vector composition from Euclidean to non-Euclidean manifolds, by moving the tangent  along the geodesic with initial velocity   T and endpoint W , and then projecting the resulting vector onto the tangent space TW M. As with the exponential map, parallel transport T may require the solution of non-linear ordinary differen-
tial equation. To alleviate the computational burden, we consider vector transport as an effective,
projection-like solution to the parallel translation problem. We overload the notation and also denote it as T , highlighting the similar role that the two mappings share. Technically, the geodesics and consequentially the exponential map, retraction as well as transport T depend on the choice of the
Riemannian metric. Putting the equations together the updating scheme for Riemmanian stochastic
gradient descent on the manifold is

Wt+1 = Wt (-t gradf )

(18)

where  is either the exponential map Exp or the retraction Rt and gradf is the gradient of the function f (W) lying in the tangent space TWM. The updating scheme for optimizers using momentum, as mentioned before, additionally requires updating the momentum term using parallel
transport.

3.2.1 OPTIMIZING OVER THE OBLIQUE MANIFOLD
Cho & Lee (2017) proposed an updating scheme for optimizing neural networks where the weights of each layer are constrained to lie in the oblique manifold Ob(p, n). Using the fact that the manifold itself is a product of p unit-norm spherical manifolds, they derived an efficient, closed-form Riemannian gradient descent updating scheme. In particular the optimization simplifies to the optimization over Ob(1, n) for each column wi{1,...,p} of W.

Oblique gradient The gradient gradf of the cost function f with respect to the weights lying in Ob(1, n) is given as a projection of the Euclidean gradient Gradf onto the tangent at w

gradf = Gradf - (w Gradf )w

(19)

Oblique exponential map The exponential map Expw moving w to w along a geodesic with

initial velocity w

Expw = w cos( w ) +

w w

sin( w )

(20)

Oblique parallel translation The parallel translation T moves the tangent vector w along the geodesic with initial velocity w

Tw (w) = w -

w w

((1 - cos(

w

)) + w sin( w

))

w w

w

(21)

5

Under review as a conference paper at ICLR 2019

Cho & Lee (2017) derived a regularization term which penalizes the distance between the point in the manifold W and the closest orthogonal matrix with respect to the Frobenius norm.

(, W)

=

 2

W

W-I

2 F

(22)

3.2.2 OPTIMIZING OVER THE STIEFEL MANIFOLD
Optimization over Stiefel manifolds in the context of neural networks has been studied by (Harandi & Fernando, 2016; Wisdom et al., 2016; Vorontsov et al., 2017). However, (Wisdom et al., 2016; Vorontsov et al., 2017) proposed a different parametrization resulting from an alternative choice of the Riemannian metric. Here we propose the parametrization using the Euclidean metric, which results in a different definition of vector transport.

Stiefel gradient The gradient gradf of the cost function f with respect to the weights lying in St(p, n) is given as a projection of the Euclidean gradient Gradf onto the tangent at W (Edelman
et al., 1998; Absil et al., 2007)

gradf = (I - WW

)Gradf

+

1 2W

W

Gradf - Gradf

W

(23)

Stiefel retraction The retraction RtW(W) for the Stiefel manifold is given by the Q factor of the QR decomposition (Absil et al., 2007).

RtW(W) = qf(W + W)

(24)

Stiefel vector transport The vector transport T moves the tangent vector w along the geodesic with initial velocity w for W  St(p, n) endowed with the Euclidean metric.

Tw (w) = I - YY

W

+

1 2

Y

Y

W - WY

(25)

where Y RtW(W). It is easy to see that the transport T consists of a retraction of tangent W
followed by the orthogonal projection of W at RtW(W). The projection is the same as the one mapping P : Gradf  gradf in equation 23.

3.3 OPTIMIZING OVER NON-COMPACT MANIFOLDS
The critical weight initialization yielding a singular spectrum of the Jacobian tightly concentrating on 1 implies that a substantial fraction of the pre-activations lie in expectation in the linear regime of the squashing nonlinearity and as a consequence the network acts quasi-linearly. To relax this constraint during training we allow the scales of the manifold constrained weights to vary. We chose to represent the weights as a product of a scaling matrix and matrix belonging to the manifold. Then the optimization of each layer consists in the optimization of the two variables in the product. In this work we only consider isotropic scalings, but the method generalizes easily to the use of any invertible square matrix.

3.4 NUMERICAL EXPERIMENTS

To experimentally test the potential effect of maintaining orthogonality throughout training and com-

pare it to the unconstrained optimization (Pennington et al., 2017), we trained a 200 layer network

tanh network on CIFAR-10 and SVHN. We present the results for the former in the main text, while

the latter can be found in the Appendix 4.2 in the interest of space. Following (Pennington et al.,

2017) we set the width of each layer to be N = 400 and chose the W, b in way to ensure that both

crictiocnacleinntirtiaatleiszaotnio1nsbwutitsh2mqax

varies as

=

1 64

and

a function of q q  9 × 10-4,

(see Fig. 1). which differ

We both

considered two different in spread of the singular

values as well as in the resulting training speed and final test accuracy, as reported by (Pennington

et al., 2017). To test how enforcing strict orthogonality or near orthogonality affects convergence

speed, and the spectrum of hidden layer input-output Jacobians and the maximum eigenvalues of the

Fisher information matrix, we trained Stiefel and Oblique constrained networks and compared them

6

Under review as a conference paper at ICLR 2019

max( ¯G)

Distribution of max(G¯) vs smax(Jxh0L) as a function of q

103.5

0.5

Networks used in experiments

Networks not used in experiments

Isometric linear networks

0.4

102.25

0.3
q 0.2

101.0 100

100.5

101.0

smax(Jxh0L)

0.1 101.5

Figure 1: At initialization the maximum eigenvalue of the Fisher information matrix G¯ correlates highly with the maximum singular value of the Jacobian JxxL0 . While the dependence is not perfectly quadratic as predicted, the deviation can be understood in terms of the bound implied by the Gershgorin theorem for block matrices ­ the bound becomes tight in the limit of q  . The networks not used in the experiments were randomly initialized with  = 1 and q varying uniformly on a logarithmical grid between 9 × 10-4 and 0.5.

CIFAR-10 performance
Per sample cross-entropy

0.00225 0.00200 0.00175 0.00150 0.00125 0.00100 0.00075 0.00050
0

0.0020 0.0015 0.0010 0.0005

1 Epochs

5 200

60 55 50 45

Test accuracy [%]

40

35

30
25 50 75 100 125 150 175 200
Epochs

Euclidean q = 1/64 Euclidean q  9 × 10-4 Oblique q = 1/64 Oblique q  9 × 10-4 Stiefel q = 1/64 Stiefel q  9 × 10-4
25 50 75 100 125 150 175 200
Epochs

Figure 2: Manifold constrained networks are insensitive to the choice of q: Train loss and test accuracy for Euclidean, Stiefel and Oblique networks with two different values of q. The manifold constrained networks minimize the training loss at approximately the same rate, being faster than both Euclidean networks. Despite this, there is little difference between the test accuracy of the Stiefel and Oblique networks and the Euclidean networks initialized with q = 9 × 10-4. Notably, the latter attains a marginally higher test set accuracy towards the end of training.

to the unconstrained "Euclidean" network described in (Pennington et al., 2017). We used a Riemannian version of ADAM (Kingma & Ba, 2014). When performing gradient descent on non-Euclidean manifolds, we split the variables into three groups: (1) Euclidean variables (e.g. the weights of the classifier layer, biases), (2) non-negative scaling W both optimized using the regular version of ADAM, and (3) manifold variables optimized using Riemannian ADAM. The initial learning rates for all the the groups, as well as the the non-orthogonality penalty (see 22) for Oblique networks were chosen with Bayesian optimization, maximizing validation set accuracy after 50 epochs. All networks were trained with a minibatch size of 1000. We trained 5 networks of each kind, and collected eigenvalue and singular value statistics every 5 epochs, from the first to the fiftieth, and then after the hundredth and two hundredth epochs.
Based on the bound the maximum eigenvalue of the Fisher information matrix derived in Section 3.1, we predicted that at initialization max(G¯ ) should covary with m2 ax(Jxx0L ). Our prediction is vindicated in that we find a strong, significant correlation between the two, with a Pearson coefficient of  = 0.88. The numerical values are presented in Fig. 1. Additionally we see that both the maximum singular value and maximum eigenvalue increase monotonically as a function of q. Based on the previous work by Saxe et al. (2013) showing depth independent learning dynamics in linear orthogonal networks, we included 5 instantiations of this model in the comparison. The input to the linear network was normalized using the following scheme applied to critical, non-linear networks with q = 1/64. Surprisingly, the deep linear networks had a substantially larger max(G¯ ) than its non-linear counterparts initialized with identically scaled input (Fig. 1).
Having established a connection between q the maximum singular value of the hidden layer inputoutput Jacobian and the maximum eigenvalue of the Fisher information, we wish to investigate the effects of initialization on subsequent optimization. As reported by Pennington et al. (2017), learning
7

Under review as a conference paper at ICLR 2019

speed and generalization peak at intermediate values of q  10-0.5. This result is counterintuitive given that the maximum eigenvalue of the Fisher information matrix, much like that of the Hessian in convex optimization, upper bounds the maximal learning rate (Boyd & Vandenberghe, 2004; Bottou et al., 2016). To gain insight into the effects of choice q on convergence rate, we trained the Euclidean networks and estimated the local values of smax and max during optimization. At the same time we asked whether we can effectively control the two aforesaid quantities by constraining the weights of each layer to be orthogonal or near orthogonal. To this end we trained Stiefel and Oblique networks and recorded the same statistics as for before.
We present the results of training in Fig. 2, where it can be seen that Euclidean networks with q  9 × 10-4 perform worse with respect to training loss and test accuracy than those initialized with q = 1/64. On the other hand, manifold constrained networks are insensitive to the choice of q. Moreover, Stiefel and Oblique networks perform marginally worse on the test set compared to the Euclidean network with q  9 × 10-4, despite attaining a lower training loss. This latter fact indicates that manifold constrained networks the are perhaps prone to overfitting. We observe that reduced performance of Euclidean networks initialized with q  9 × 10-4 may partially be explained by their rapid increase in max(G¯ ) the initial 5 epochs of optimization (see Fig. 4). While all networks undergo this rapid increase, it is most pronounced for Euclidean networks with q  9 × 10-4. The increase max(G¯ ) correlates with the inflection point in the training loss curve that can be seen in the inset of Fig. 2. Interestingly, the manifold constrained networks optimize efficiently despite differences in max(G¯ ), showing that their performance cannot be attributed to increasing the gradient smoothness as postulated by (Santurkar et al., 2018). Finally, we observe that for manifold constrained networks the maximum singular value of the Jacobian remains predictive of the maximum eigenvalue of the Fisher information matrix with an average Pearson correlation of 0.93 (see Fig.3). For Euclidean networks the correlation is considerably lower, with  = 0.26 for q = 1/64 and 0.83 for q = 9 × 10-4.

smax and max

q  9 × 10-4 Training loss

0.00225 0.00200 0.00175 0.00150 0.00125 0.00100 0.00075 0.00050
0
0.00225 0.00200 0.00175 0.00150 0.00125 0.00100 0.00075 0.00050
0

Training loss vs smax(Jxh0L) and max(G¯)

Euclidean

104.0

103.5 0.00225

103.0 0.00200

102.5 0.00175

102.0 0.00150

101.5 0.00125

101.0 0.00100

100.5 0.00075

100 0.00050

25 50 75 100 125 150 175 200

0

Oblique

104.0

103.5 0.00225

103.0 0.00200

102.5 0.00175

102.0 0.00150

101.5 0.00125

101.0 0.00100

100.5 0.00075

100 0.00050

25 50 75 100 125 150 175 200

0

104.0

104.0

103.5 0.00225

0.00225
103.5

103.0 0.00200

0.00200
103.0

102.5 0.00175

0.00175
102.5

102.0 0.00150

0.00150
102.0

101.5 0.00125

0.00125
101.5

101.0 0.00100

0.00100
101.0

100.5 0.00075

0.00075
100.5

100 0.00050

0.00050
100

25 50 75 100 125 150 175 200

0 25 50 75 100 125 150 175 200

0

Epochs

Epochs

Stiefel

104.0

103.5

103.0

102.5

102.0

101.5

101.0

100.5

100 25 50 75 100 125 150 175 200

104.0 103.5 103.0 102.5 102.0 101.5 101.0 100.5 100 25 50 75 100 125 150 175 200
Epochs

smax and max

q = 1/64 Training loss

Fmiugumreei3g:enTvhaelume aoxfitmheuFmisshienrguinlaforrvmaalutieonofmtahteriJxacombaixa(nG¯s)mdauxr(iJnxxgL0t)raisinpinregd. iTchtievedaosfhethdecomloarxeid-

lines represent max captures particularly

(G¯ ), well

while the the initial

continuous colored lines increase in the max(G¯ )

represent smax during the first

(JxxL0 ). The few epochs.

correlation Moreover,

for Stiefel and Oblique networks the correlation is stronger.

3.4.1 NON-GAUSSIAN LIMIT OF PRE-ACTIVATION
The derivation of the spectra of hidden layer input-output Jacobians presented in (Pennington et al., 2017; 2018) crucially depends on the existence of the Gaussian limit of the distribution of preactivations, which in turn assumes that the elements of the weight matrices are sampled iid from some probability measure with finite first two moments. This condition is trivially violated for matrices sampled from the Haar (uniform) measure over the Stiefel manifold. We invoke Theorem 1 from Meckes (2012) which asserts that for a random semi-orthogonal projection of an arbitrary random vector to converge to a Gaussian distribution in the bounded Lipschitz distance, the pro-

8

Under review as a conference paper at ICLR 2019

max( ¯G)

104.0 103.5 103.0 102.5 102.0 101.5
0

Gradient smoothness max(G¯)

Oblique Oblique Euclidean Euclidean Stiefel Stiefel

q0 = 1/64 q0  9 × 10-4
q0 = 1/64 q0  9 × 10-4
q0 = 1/64 q0  9 × 10-4

25 50 75 100 125 150 175 200
Epochs

Figure 4: For manifold constrained net-
works, gradient smoothness is not predic-
tive of optimization rate. Euclidean networks with a low initial max(G¯) rapidly become less smooth, whereas Euclidean networks with a larger max(G¯) remain relatively smoother. Notably, the Euclidean network with q = 1/64 has almost an order of magnitude smaller max(G¯ ) than the Stiefel and Oblique networks, but reduces training
loss at a slower rate.

jection

must

go

from

d

input

dimensions

to

2 log (d) log (log (d))

dimensions.

We

also

show

empirically

that

in orthogonally initialized random networks, the pre-activations do not necessarily converge to a

multivariate Gaussian distribution (see Fig. 5). We pose as an open problem the conditions under

which the random matrix argument of Pennington et al. (2017) holds.

Theorem 1. (Meckes, 2012) Let X be a random vector in Rd with

E X 2 = 2d,

E

X

2 -2 - d

  L d,

sup E ( X)2  1

Sd-1

Let

XW

denote

the

projection

WX

for

W



St(k, d),

with

k

=



log (d) log (log (d))

and

0







2,

then

there is a c > 0 such that for

= 2 exp

-c

log

(log 

(d))

there exists a subset I  St(k, d) with

probability mass p(I) with respect to the Haar measure. Then p(I)  1 - C exp -c d 2 such that

for all W  I

sup |EX [f (XW)] - E [f (Z)]|  C
max ( f ,|f |L)1

Layer 1

Layer 100

Layer 199
Gaussian t Empirical distribution

Overlay
Pre-activation Gaussian samples

Most non-Gaussian projection of hl

Figure 5: Non-Gaussian projection of the pre-activations for random networks q = 1/64 and evaluated on CIFAR-10. The projection was obtained using the algorithm in (Blanchard et al., 2006). The rightmost panel shows an overlay of the pre-activations compared to the most nonGaussian projection of a set of 400 dimensional Gaussian random variables with variance matched to the real data.
4 DISCUSSION
In this work we have analyzed the geometry of critical initializations and during training with orthogonal and near orthogonal manifold constraints. In the process we derived a novel bound on the maximum eigenvalue of the Fisher information matrix and related it to the expected maximum eigenvalue of the hidden layer input-output Jacobian, thereby elucidating the empirical success of the initialization proposed by (Pennington et al., 2017). We then probed numerically the benefits of maintaining orthogonality and near orthogonality of weight matrices during training, while relating them to the gradient smoothness measured by the maximum eigenvalue of the Fisher information matrix. We observed several interesting phenomena, which include a rapid decrease of the gradient smoothness of the Euclidean networks that are smoother at initialization. This paradoxical result
9

Under review as a conference paper at ICLR 2019 partially explains the observations made in (Pennington et al., 2017) but further analysis needs to be done to understand it fully. Another conclusion is that the robustness to the choice of q and low computational overhead makes optimization on the Oblique manifold an appealing alternative to searching for the optimal q. This could be particularly advantageous in situations where the input data is non-stationary. Finally we pose an open question the conditions under which the mean field approximation to the pre-activations hold.
10

Under review as a conference paper at ICLR 2019
ACKNOWLEDGMENTS
REFERENCES
P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press, Princeton, N.J. ; Woodstock, December 2007. ISBN 978-0-691-13298-3.
Shun-ichi Amari. Information Geometry and Its Applications. Springer, Japan, 1st ed. 2016 edition edition, February 2016. ISBN 978-4-431-55977-1.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary Evolution Recurrent Neural Networks. arXiv:1511.06464 [cs, stat], November 2015. arXiv: 1511.06464.
Gilles Blanchard, Motoaki Kawanabe, Masashi Sugiyama, and Vladimir Spokoiny. In Search of Non-Gaussian Components of a High-Dimensional Distribution. Journal of Machine Learning Research, pp. 36, 2006.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton Optimisation for Deep Learning. In International Conference on Machine Learning, pp. 557­565, July 2017.
Le´on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010, pp. 177­186. Springer, 2010. 00829.
Le´on Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization Methods for Large-Scale Machine Learning. arXiv:1606.04838 [cs, math, stat], June 2016. arXiv: 1606.04838.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization, With Corrections 2008. Cambridge University Press, Cambridge, UK ; New York, 1 edition edition, March 2004. ISBN 978-0-52183378-3.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In Advances in Neural Information Processing Systems, pp. 5229­5239, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval Networks: Improving Robustness to Adversarial Examples. arXiv:1704.08847 [cs, stat], April 2017. arXiv: 1704.08847.
Alan Edelman, Toma´s A. Arias, and Steven T. Smith. The Geometry of Algorithms with Orthogonality Constraints. SIAM Journal on Matrix Analysis and Applications, 20(2):303­353, January 1998. ISSN 0895-4798, 1095-7162. doi: 10.1137/S0895479895290954.
Mehrtash Harandi and Basura Fernando. Generalized backpropagation, E´ tude De Cas: Orthogonality. arXiv:1611.05927 [cs], November 2016. 00004 arXiv: 1611.05927.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs], December 2015. 01528 arXiv: 1512.03385.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent Orthogonal Networks and Long-Memory Tasks. arXiv:1602.06662 [cs, stat], February 2016. arXiv: 1602.06662.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 [cs], December 2014. 01869 arXiv: 1412.6980.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep Neural Networks as Gaussian Processes. arXiv:1711.00165 [cs, stat], October 2017. arXiv: 1711.00165.
Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin Ghahramani. Gaussian Process Behaviour in Wide Deep Neural Networks. arXiv:1804.11271 [cs, stat], April 2018. arXiv: 1804.11271.
Elizabeth Meckes. Projections of Probability Distributions: A Measure-Theoretic Dvoretzky Theorem. In Geometric Aspects of Functional Analysis, Lecture Notes in Mathematics, pp. 317­ 326. Springer, Berlin, Heidelberg, 2012. ISBN 978-3-642-29848-6 978-3-642-29849-3. doi: 10.1007/978-3-642-29849-3 18.
11

Under review as a conference paper at ICLR 2019
Radford M. Neal. Bayesian Learning for Neural Networks, volume 118 of Lecture Notes in Statistics. Springer New York, New York, NY, 1996. ISBN 978-0-387-94724-2 978-1-4612-0745-0. doi: 10.1007/978-1-4612-0745-0.
Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown, Christopher Olah, Colin Raffel, and Ian Goodfellow. Is Generator Conditioning Causally Related to GAN Performance? arXiv:1802.08768 [cs, stat], February 2018. arXiv: 1802.08768.
Mete Ozay and Takayuki Okatani. Optimization on Submanifolds of Convolution Kernels in CNNs. arXiv preprint arXiv:1610.07008, 2016. 00003.
Chethan Pandarinath, Daniel J. O'Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D. Stavisky, Jonathan C. Kao, Eric M. Trautmann, Matthew T. Kaufman, Stephen I. Ryu, Leigh R. Hochberg, Jaimie M. Henderson, Krishna V. Shenoy, L. F. Abbott, and David Sussillo. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature Methods, September 2018. ISSN 1548-7091, 1548-7105. doi: 10.1038/s41592-018-0109-9.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. arXiv:1711.04735 [cs, stat], November 2017. arXiv: 1711.04735.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The Emergence of Spectral Universality in Deep Networks. arXiv:1802.09979 [cs, stat], February 2018. arXiv: 1802.09979.
Daniel Ritchie, Paul Horsfall, and Noah D. Goodman. Deep Amortized Inference for Probabilistic Programs. arXiv:1610.05735 [cs, stat], October 2016. arXiv: 1610.05735.
Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. arXiv:1602.07868 [cs], February 2016. 00003 arXiv: 1602.07868.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). arXiv:1805.11604 [cs, stat], May 2018. arXiv: 1805.11604.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120 [cond-mat, q-bio, stat], December 2013. arXiv: 1312.6120.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information Propagation. arXiv:1611.01232 [cs, stat], November 2016. 00003 arXiv: 1611.01232.
Christiane Tretter. Spectral Theory of Block Operator Matrices and Applications. IMPERIAL COLLEGE PRESS, October 2008. ISBN 978-1-86094-768-1 978-1-84816-112-2. doi: 10.1142/ p493.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. arXiv:1702.00071 [cs], January 2017. arXiv: 1702.00071.
Scott Wisdom, Thomas Powers, John R. Hershey, Jonathan Le Roux, and Les Atlas. FullCapacity Unitary Recurrent Neural Networks. arXiv:1611.00035 [cs, stat], October 2016. arXiv: 1611.00035.
Di Xie, Jiang Xiong, and Shiliang Pu. All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation. arXiv:1703.01827 [cs], March 2017. arXiv: 1703.01827.
12

Under review as a conference paper at ICLR 2019

APPENDIX

4.1 SKETCH OF THE BOUND USING THE BLOCK DIAGONAL GERSHGORIN CIRCLE THEOREM
Using remark 1.13.2 from Tretter (2008), we consider a a block partitioned matrix A  Rdn×dn with n d × d blocks Ai,j i, j  [1, . . . , n]. Then consider diagonal blocks Aii which we additionally require to hermitian. Then define sets for each ith block-diagonal element the set of Gershgorin disks i


d


n

i s(Aii) 

C k(Aii),

Ai,j 

k=1

j=1,j=i

(26)

where s(·) denotes the spectrum of a matrix and C(c, r) denotes a ball centered on c with radius r

C(c, r) { :  - c  r}

(27)

Therefore each i denotes a union of balls each centered on the eigenvalues of (Ai,i) with radius given by the spectral norm of the off-diagonal blocks. The collection of i's contains the spectrum of A. Let us assume for convenience that we are trying to predict continuous variables in

a regression context and with a mean-square error loss. This simplifies the analysis in Section 3.1,

by setting Hg = I. Then the block-diagonal elements of G¯ become JhiL JhjL . The off diagonal

elements have a spectral radius of smax(JhiL JhjL )  smax(JhiL )smax(JhjL ). As a consequence

the Gershgorin disks for ith block diagonal matrix is a set sum of the eigenvalues of JhiL JhiL and

the disks centered on each of them, all only about the maximum eigenvalue of

with radius at most G¯ , we consider the

smax(JhiL )smax(JhjL ). maximum eigenvalue of

Since we care each diagonal

block and the disk around it.


n

i = C smax(JhiL ),

smax(JhiL

j=1,j=i

 JhjL )

(28)


n



 C smax(JhiL ),

smax(JhiL )smax(JhjL )

j=1,j=i

(29)

n

=  :  - sm2 ax(JhiL ) 

smax(JhiL )smax(JhjL )

j=1,j=i

(30)

n

=  :  - sm2 ax(JhiL )  smax(JhiL )

smax(JhjL )

j=1,j=i

(31)

So the relative size of the disk compare to smax(JhiL ) becomes small whenever smax(JhiL ) >

n j=1,j=i

smax(JhjL

)

and

smax(JhiL

)





since

the

magnitude

of

s2max(JhiL

)

grows

quadratically.

4.2 SVHN FIGURES

13

Under review as a conference paper at ICLR 2019

Per sample cross-entropy
Test accuracy [%]

SVHN performance

0.0020 0.0015 0.0010 0.0005

0.0020 0.0015 0.0010 0.0005

1 Epochs

5 200

0 25 50 75 100 125 150 175 200
Epochs

90
80
70
60
50 Euclidean q = 1/64 Euclidean q  9 × 10-4
40 Oblique q = 1/64 Oblique q  9 × 10-4
30 Stiefel q = 1/64 20 Stiefel q  9 × 10-4
25 50 75 100 125 150 175 200
Epochs

max( ¯G)

104.0 103.5 103.0 102.5 102.0 101.5
0

Gradient smoothness (max(G¯))
Oblique q0 = 1/64 Oblique q0  9 × 10-4 Euclidean q0 = 1/64 Euclidean q0  9 × 10-4 Stiefel q0 = 1/64 Stiefel q0  9 × 10-4 25 50 75 100 125 150 175 200
Epochs

0.00225 0.00200 0.00175 0.00150 0.00125 0.00100 0.00075 0.00050
0
0.00225 0.00200 0.00175 0.00150 0.00125 0.00100 0.00075 0.00050
0

Training loss vs smax(Jxh0L) and max(G¯)

Euclidean

104.0

103.5 0.00225

103.0 0.00200

102.5 0.00175

102.0 0.00150

101.5 0.00125

101.0 0.00100

100.5 0.00075

100 0.00050

25 50 75 100 125 150 175 200

0

Oblique

104.0

103.5 0.00225

103.0 0.00200

102.5 0.00175

102.0 0.00150

101.5 0.00125

101.0 0.00100

100.5 0.00075

100 0.00050

25 50 75 100 125 150 175 200

0

104.0

104.0

103.5 0.00225

0.00225
103.5

103.0 0.00200

0.00200
103.0

102.5 0.00175

0.00175
102.5

102.0 0.00150

0.00150
102.0

101.5 0.00125

0.00125
101.5

101.0 0.00100

0.00100
101.0

100.5 0.00075

0.00075
100.5

100 0.00050

0.00050
100

25 50 75 100 125 150 175 200

0 25 50 75 100 125 150 175 200

0

Epochs

Epochs

Stiefel

104.0

103.5

103.0

102.5

102.0

101.5

101.0

100.5

100 25 50 75 100 125 150 175 200

104.0 103.5 103.0 102.5 102.0 101.5 101.0 100.5 100 25 50 75 100 125 150 175 200
Epochs

14

smax and max

q  9 × 10-4 Training loss

smax and max

q = 1/64 Training loss

