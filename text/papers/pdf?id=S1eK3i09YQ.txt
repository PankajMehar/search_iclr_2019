Under review as a conference paper at ICLR 2019

GRADIENT DESCENT PROVABLY OPTIMIZES OVERPARAMETERIZED NEURAL NETWORKS
Anonymous authors Paper under double-blind review

ABSTRACT
One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.
Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.

1 INTRODUCTION

Neural networks trained by first order methods have achieved a remarkable impact on many applications, but their theoretical properties are still mysteries. One of the empirical observation is even though the optimization objective function is non-convex and non-smooth, randomly initialized first order methods like stochastic gradient descent can still find a global minimum. Surprisingly, this property is not correlated with labels. In Zhang et al. (2016), authors replaced the true labels with randomly generated labels, but still found randomly initialized first order methods can always achieve zero training loss.

A widely believed explanation on why a neural network can fit all training labels is because the neural network is over-parameterized. For example, Wide ResNet (Zagoruyko & Komodakis) uses 100x parameters than the number of training data and thus there must exist one such neural network of this architecture that can fit all training data. However, the existence does not imply why the network found by a randomly initialized first order method can fit all the data. The objective function is neither smooth nor convex, which makes traditional analysis technique from convex optimization not useful in this setting. To our knowledge, only the convergence to a stationary point is known (Davis et al., 2018).

In this paper we demystify this surprising phenomenon on two-layer neural networks with rectified

linear unit (ReLU) activation. Formally, we consider a neural network of the following form.

f (W, a, x) = 1 m

m

ar 

wr x

r=1

(1)

where x  Rd is the input, wr  Rd is the weight vector of the first layer, ar  R is the output weight and  (·) is the ReLU activation function:  (z) = z if z  0 and  (z) = 0 if z < 0 .

We focus on the empirical risk minimization problem with a quadratic loss. Given a training data set {(xi, yi)}in=1, we want to minimize

min
WRd×m

L(W)

=

n i=1

1 2

(f (W, a, xi)

-

yi)2 .

(2)

1

Under review as a conference paper at ICLR 2019

To do this, we fix the second layer and apply gradient descent (GD) on the first layer weights matrix

W(k + 1) = W(k) -  L(W(k)) . W(k)

(3)

where  > 0 is the step size. Here the gradient formula for each weight vector is 1

L(W) wr

=

1 m

n
(f (W, a, xi)
i=1

-

yi )ar xi I

wr xi  0

(4)

Though this is only a shallow fully connected neural network, the objective function is still nonsmooth and non-convex due to the use of ReLU activation function. Even for this simple function, why randomly initialized first order method can achieve zero training error is not known. In fact, many previous work have tried to answer this question or similar ones. Attempts include landscape analysis (Soudry & Carmon, 2016), partial differential equations (Mei et al.), analysis of the dynamics of the algorithm (Li & Yuan, 2017), optimal transport theory (Chizat & Bach, 2018), to name a few. These results often rely strong assumptions on the labels and input distributions, or do not imply why randomly initialized first order method can achieve zero training loss. See Section 2 for detailed comparisons between our result and previous ones.
In this paper, we rigorously prove that as long as the data set is not degenerate and m is large enough, with properly randomly initialized a and W(0), GD achieves zero training loss at a linear convergence rate, i.e., it finds a solution W(K) with L(W(K))  in K = O(log (1/ )) iterations.2 Thus, our theoretical result not only shows the global convergence but also gives a quantitative convergence rate in terms of the desired accuracy.

Analysis Technique Overview Our proof relies on the following insights. First we directly analyze the dynamics of each individual prediction f (W, a, xi) for i = 1, . . . , n. This is different from many previous work (Du et al., 2017b; Li & Yuan, 2017) which tried to analyze the dynamics of the parameter (W ) we are optimizing. Note because the objective function is non-smooth and nonconvex, analysis of the parameter space dynamics is very difficult. In contrast, we find the dynamics of prediction space is governed by the spectral property of a Gram matrix (which can vary in each iteration, c.f. equation 5) and as long as this Gram matrix's least eigenvalue is lower bounded, gradient descent enjoys a linear rate. Furthermore, previous work has shown in the initialization phase this Gram matrix does has lower bounded least eigenvalue as long as the data is not degenerate (Xie et al., 2017). Thus the problem reduces to showing the Gram matrix at later iterations is close to that in the initialization phase. Our second observation is this Gram matrix is only related to the activation patterns (I wr xi  0 ) and we can use matrix perturbation analysis to show if most of the patterns do not change, then this Gram matrix is close to its initialization. Our third observation is we find over-parameterization, random initialization and the linear convergence jointly restrict every weight vector wr is close to its initialization. Then we can use this property to show most of the patterns do not change. Combining these insights we prove the first global quantitative convergence result of gradient descent on ReLU activated neural networks for the empirical risk minimization problem. Notably, our proof only uses linear algebra and standard probability bounds so we believe it can be easily generalized to analyze deep neural networks.

Notations We use bold-faced letters for vectors and matrices. We Let [n] = {1, 2, . . . , n}. Given a set S, we use unif {S} to denote the uniform distribution over S. Given an event E, we use I {A} to be the indicator on whether this event happens. We use N (0, I) to denote the standard Gaussian distribution. For a matrix A, we use Aij to denote its (i, j)-th entry. We use · 2 to denote the Euclidean norm of a vector, and use · F to denote the Frobenius norm of a matrix. If a matrix A is positive semi-definite, we use min(A) to denote its smallest eigenvalue. We use ·, · to denote the standard Euclidean inner product between two vectors. Lastly, let O(·) and  (·) denote standard
Big-O and Big-Omega notations, only hiding absolute constants.

1

Note

ReLU

is

not

continuously

differentiable.

One

can

view

L(W) wr

as

a

convenient

notation

for

the

right

hand side of equation 4 and this is indeed the update rule used in practice.

2Here we omit the polynomial dependency on n and other data dependent quantities.

2

Under review as a conference paper at ICLR 2019
2 COMPARISON WITH PREVIOUS RESULTS
In this section we survey an incomplete list of previous attempts in analyzing why first order methods can find a global minimum.
Landscape Analysis A popular way to analyze non-convex optimization problems is to identify whether the optimization landscape has some benign geometric properties. Recently, researchers found if the objective function is smooth and satisfies (1) all local minima are global and (2) for every saddle point, there exists a negative curvature, then the noise-injected (stochastic) gradient descent (Jin et al., 2017; Ge et al., 2015; Du et al., 2017a) can find a global minimum in polynomial time. This algorithmic finding encouraged researchers to study whether the deep neural networks also admit these properties.
For the objective function define in equation 2, some partial results were obtained. Soudry & Carmon (2016) showed if md  n, then at every differentiable local minimum, the training error is zero. However, since the objective is non-smooth, it is hard to show gradient descent actually convergences to a differentiable local minimum. Xie et al. (2017) studied the same problem and related the loss to the gradient norm through the least singular value of the "extended feature matrix" D at the stationary points. However, they did not prove the convergence rate of gradient norm. Interestingly, our analysis relies on the Gram matrix which is actually DD .
Landscape analyses of ReLU activated neural networks for other settings have also been studied in many previous work (Ge et al., 2017; Safran & Shamir, 2018; 2016; Zhou & Liang, 2017; Freeman & Bruna, 2016; Hardt & Ma, 2016). These works establish favorable landscape properties such as all local minimizers are global, but do not ensure that gradient descent converges to a global minimizer of the empirical risk. For other activation functions, some previous work show the landscape does have the desired geometric properties (Du & Lee, 2018; Soltanolkotabi et al., 2018; Nguyen & Hein, 2017; Kawaguchi, 2016; Haeffele & Vidal, 2015; Andoni et al., 2014; Venturi et al., 2018). However, it is unclear how to extend their analyses in our setting.
Analysis of Algorithm Dynamics Another way to prove convergence result is to directly analyze the dynamics of first order methods. Our paper also belongs to this category. Many previous work assumed (1) the input distribution is Gaussian and (2) the label is generated according to a planted neural network. Based on these two (unrealistic) conditions, it can be shown that randomly initialized (stochastic) gradient descent can learn a ReLU (Tian, 2017; Soltanolkotabi, 2017), a single convolutional filter (Brutzkus & Globerson, 2017), a convolutional neural network with one filter and one output layer (Du et al., 2018b) and residual network with small spectral norm weight matrix (Li & Yuan, 2017).3 Beyond Gaussian input distribution, Du et al. (2017b) showed for learning a convolutional filter, the Gaussian input distribution assumption can be relaxed but they still require the label is generated from an underlying true filter. Comparing with these work, our paper does not try to recover the underlying true neural network. Instead, we focus on providing theoretical justification on why randomly initialized gradient descent can achieve zero training loss, which is what we can observe and verify in practice.
The most related paper is by Li & Liang (2018) who observed that when training a two-layer full connected neural network, most of the patterns (I wr xi  0 ) do not change over iterations, which we also use to show the stability of the Gram matrix. They used this observation to obtain the convergence rate of GD on a two-layer over-parameterized neural network for the cross-entropy loss. They need number of hidden nodes m scales with poly(1/ ) where is the desired accuracy. Thus unless the number of hidden nodes m  , they result does not imply GD can achieve zero training loss. We improve by allowing the amount of over-parameterization to be independent of the desired accuracy and show GD can achieve zero training loss. Furthermore, our proof is much simpler and more transparent so we believe it can be easily generalized to analyze other neural network architectures.
Other Analysis Approaches Chizat & Bach (2018) used optimal transport theory to analyze continuous time gradient descent on over-parameterized models. However, their results on ReLU acti-
3Since these work assume the label is realizable, converging to global minimum is equivalent to recover the underlying model.
3

Under review as a conference paper at ICLR 2019

vated neural network is only at the formal level. Mei et al. showed the dynamics of SGD can be captured by a partial differential equation in the suitable scaling limit. They listed some specific examples on input distributions including mixture of Gaussians. However, it is still unclear whether this framework can explain why first order methods can minimize the empirical risk. Daniely (2017) built connection between neural networks with kernel methods and showed stochastic gradient descent can learn a function that is competitive with the best function in the conjugate kernel space of the network. Again this work does not imply why first order method can achieve zero training loss.

3 CONTINUOUS TIME ANALYSIS

In this section, we present our result for gradient flow, i.e., gradient descent with infinitesimal step
size. In the next section, we will modify the proof and give a quantitative bound for gradient descent with positive step size. Formally, we consider the ordinary differential equation4 defined by :

dwr(t) = - L(W(t)) dt wr(t)

for r  [m]. We denote ui(t) = f (W(t), a, xi) the prediction on input xi at time t and we let u(t) = (u1(t), . . . , un(t))  Rn be the prediction vector at time t. Our main result in this section is the following theorem.

Theorem 3.1 (Convergence Rate of Gradient Flow). Assume for 1 and |yi|  C for some constant C and the matrix H

all 

i Rn×n

[n], with

xHiij2

= =

EwN(0,I) xi xjI w xi  0, w xj  0 satisfies min(H) 0 > 0. Then if we initialize

wr  N (0, I), ar  unif [{-1, 1}] for r  [m] and set the number of hidden nodes m = 

n6 04

,

with high probability over random initialization we have

u(t) - y

2 2



exp(-0t)

u(0) - y

2 2

.

We first discuss our assumptions. We assume xi 2 = 1 and |yi|  C for simplicity. We can easily modify the proof by properly scaling the initialization. The key assumption is the least eigenvalue of the matrix H is strictly positive. Interestingly, various properties of this H matrix has been thoroughly studied in previous work (Xie et al., 2017; Tsuchida et al., 2017). In general, unless the data is degenerate, the smallest eigenvalue of H is strictly positive. We refer readers to Xie et al. (2017); Tsuchida et al. (2017) for a detailed characterization of this matrix.

The number of hidden nodes m required is 

n6 40

, which depends on the number of samples n

and 0. As will be apparent in the proof, over-parameterization, i.e., the fact m = poly(n, 1/0),

plays a crucial role in guaranteeing gradient descent to find the global minimum. We believe using

a more refined analysis, this dependency can be further improved. Lastly, note the convergence rate

is linear because

u(t) - y

2 2

decreases to 0 exponentially fast.

The specific rate also depends on

0 but independent of number of hidden nodes m.

3.1 PROOF OF THEOREM 3.1

Our first step is to calculate the dynamics of each prediction.

dm dt ui(t) =
r=1

f (W(t), a, xi) , dwr(t)

 wr (t)

dt

=

n
(yj
j=1

-

uj )

m r=1

f (W(t), a, xi) ,  wr (t)

f (W(t), a, xj)  wr (t)

n
(yj - uj)Hij(t)
j=1

where H(t) is an n × n matrix with (i, j)-th entry

1m Hij(t) = m xi xj I xi wr(t)  0, xj wr(t)  0 .
r=1

(5)

4Strictly speaking, this should be differential inclusion (Davis et al., 2018)

4

Under review as a conference paper at ICLR 2019

With this H matrix, we can write the dynamics of prediction in a compact way: d u(t) = H(t)(y - u(t)). dt

Note H(t) is a time-dependent symmetric matrix. We first analyze its property when t = 0. The following lemma shows if m is large then H(0) has lower bounded least eigenvalue with high probability. The proof is by standard concentration bound so we defer it to appendix.

Lemma 3.1. If m = 

n2 02

log2

n 

,

we

have

with

probability

at

least

1 - ,

min(H(0))



3 4

0

.

Our second step is to show H(t) is stable in terms of W. Formally, the following lemma shows if W(t) is close to the initialization W(0), H(t) is close H(0) and has a lower bounded least eigenvalue.

Lemma 3.2. Suppose for a given time t, for all r  [m],

wr(t) - wr(0)

2



c0 n2

R for some

small

positive

constant

c.

Then

we

have

with

high

probability

over

initialization,

min(H(t))

>

0 2

.

This lemma plays a crucial in our analysis so we give the proof below.

Proof of Lemma 3.2 We define the event

Air = w : w - wr(0)  R, I xi wr(0)  0 = I xi w  0 .

Note this event happens if and only if wr(0) x0 < R. Recall wr(0)  N (0, I). By anti-

concentration

inequality

of

Gaussian,

we

have

P (Air)

=

PzN(0,1) (|z|

<

R)



2R .
2

Therefore,

we can bound the entry-wise deviation on H(t) in expectation:

E [|Hij(t) - Hij(0)|]

=E

1m m xi xj

I wr(0) xi  0, wr(0) xj  0

- I wr(t) xi  0, wr(t) xj  0

r=1

1 m

m
E [I {Air
r=1



Aj r }]



4R . 2

Summing over (i, j), we have E

(n,n) (i,j)=(1,1)

|Hij (t)

-

Hij

(0)|

 4n2R . Thus by Markov in-
2

equality, with high probability, we have

(n,n) (i,j)=(1,1)

|Hij (t)

-

Hij (0)|



C n2 R

where

C

is

a

large

absolute constant. Next, we use matrix perturbation theory to bound the deviation from the initial-

ization

(n,n)

H(t) - H(0) 2  H(t) - H(0) F 

|Hij(t) - Hij(0)|  Cn2R.

(i,j)=(1,1)

Lastly, we lower bound the smallest eigenvalue by plugging in R

min(H(t))



min(H(0))

-

C n2 R



0 . 2

The next lemma shows two facts if the least eigenvalue of H is lower bounded. First, the loss
converges to 0 at a linear convergence rate. Second, wr(t) is close to the initialization for every r  [m]. This lemma clearly demonstrates the power of over-parameterization.

Lemma 3.3. Suppose for 0  s  t, min (H(s)) 

0 2

.

Then we have


y - u(t)

2 2



exp(-0t)

y - u(0)

2 2

and for any r  [m],

wr(t) - wr(0) 2  2

ny-u(0) 2 m0

R.

Proof

of

Lemma

3.3

Recall

we

can

write

the

dynamics

of

predictions

as

d dt

u(t)

=

H(y

-

u(t)).

We

can calculate the loss function dynamics

d dt

y - u(t)

2 2

= - 2 (y - u(t))

H(t) (y - u(t))

 - 0

y - u(t)

2 2

.

5

Under review as a conference paper at ICLR 2019

Thus

we

have

d dt

exp(0t)

y - u(t)

2 2

 0, exp(0t)

y - u(t)

2 2

is

a

decreasing

function

with

respect to t. Using this fact we can bound the loss

y - u(t)

2 2



exp(-0t)

y - u(0)

2 2

.

Therefore, u(t)  y exponentially fast. Now we bound the gradient. Recall for 0  s  t,

d dt wr(s)

2

n
= (yi
i=1
 1 n m

-

ui)

1 m

ar

xi

I

wr (t)

xi  0



|yi

-

ui(s)|



n m

y - u(s) 2

2


 n
m

exp(-0s/2)

y - u(0)

2.

i=1

Integrating the gradient, we can bound the distance from the initialization

wr(t) - wr(0) 2 

t 0



d ds wr(s)

ds 
2

2

n y - u(0) 2 . m0

The next lemma shows we show if R < R, the conditions in Lemma 3.2 and 3.3 hold for all t  0.

The proof is by contradiction and we defer it to appendix.

Lemma 3.4. If R

< R, we have for all t  0, min(H(t)) 

1 2

0,

for all r



[m],

wr(t) - wr(0) 2  R and

y - u(t)

2 2



exp(-0t)

y - u(0) 22.

Thus it is sufficient to show R < R which is equivalent to m = 

n5

y-u(0)

2 2

40

. We bound

nn

E

y - u(0)

2 2

=

(yi2 + yiE [f (W(0), a, xi)] + E f (W(0), a, xi)2 ) =

(yi2 + 1) = O(n).

i=1 i=1

Thus by Markov's inequality, we have with high probability

y - u(0)

2 2

= O(n).

Plugging in this

bound we prove the theorem.

4 DISCRETE TIME ANALYSIS

In this section, we show randomly initialized gradient descent with a constant positive step size converges to the global minimum at a linear rate. We first present our main theorem.

Theorem 4.1 (Convergence Rate of Gradient Descent). Assume for all i  [n], 1, |yi|  C for some constant C and the matrix H  Rn×n with

Hxiij 2

= =

EwN(0,I) xi xjI w xi  0, w xj  0 satisfies min(H) 0 > 0. If we initialize

wr  N (0, I), ar  unif [{-1, 1}] for r  [m] and the number of hidden nodes m = 

n6 40

and we set the step size  = O

0 n2

then with high probability over the random initialization we

have for k = 0, 1, 2, . . .

u(k) - y

2 2



1 - 0 2

k

u(0) - y

2 2

.

Theorem 4.1 shows even though the objective function is non-smooth and non-convex, gradient descent with a constant positive step size still enjoys a linear convergence rate. Our assumptions on the least eigenvalue and the number of hidden nodes are exactly the same as the theorem for gradient flow. Notably, our choice of step size is independent of number of hidden nodes m in contrast to the previous work (Li & Liang, 2018).

4.1 PROOF OF THEOREM 4.1
We prove Theorem 4.1 by induction. Our induction hypothesis is just the following convergence rate of empirical loss.

6

Under review as a conference paper at ICLR 2019

Condition 4.1. At the k-th iteration, we have

y - u(k)

2 2



(1

-

0 2

)k

y - u(k)

2 2

.

A directly corollary of this condition is the following bound of deviation from the initialization. The proof is similar to that of Lemma 3.3 so we defer it to appendix.

Corollary 4.1. If Condition 4.1 holds for k = 0, . . . , k, then we have for every r  [m]



4 wr(k + 1) - wr(0) 2 

n y - u(0) 2 m0

R.

(6)

Now we show Condition 4.1 for every k = 0, 1, . . .. For the base case k = 0, by definition Condition 4.1 holds. Suppose for k = 0, . . . , k, Condition 4.1 holds and we want to show Condition 4.1 holds for k = k + 1.

Our strategy is similar to the proof of Theorem 3.1. We define the event

Air = w : w - wr(0)  R, I xi wr(0)  0 = I xi w  0 .

where R =

c0 n2

for some small positive constant c.

Different from gradient flow, for gradient

descent we need a more refined analysis. We let Si = {r  [m] : I{Air} = 1} and Si = [m] \ Si.

The following Lemma bounds the sum of sizes of Si. The proof is similar to the analysis used in

Lemma 3.2. See Section A for the whole proof.

Lemma 4.1. With high probability over initialization, we have

n i=1

Si

 CmnR for some

positive constant C > 0.

Next, we calculate the difference of predictions between two consecutive iterations, analogue to

dui (t) dt

term

in

Section

3.

ui(k

+

1)

-

u(k)

=

1 m

m
ar



wr(k + 1)

xi

-

wr (k)

xi

r=1

= 1 m

m

ar



r=1

wr

(k)

-





L(W(k))  wr (k)

xi

-  wr(k) xi

.

Here we divide the right hand side into two parts. I1i accounts for terms that the pattern does not change and I2i accounts for terms that pattern may change.

I1i

1 m

ar 

rSi

wr

(k)

-



L(W(k))  wr (k)

xi -  wr(k) xi

I2i

1 m

ar 

rSi

wr

(k)

-



L(W(k))  wr (k)

xi -  wr(k) xi

We view I2i as a perturbation and bound its magnitude. Because ReLU is a 1-Lipschitz function and |ar| = 1, we have

I2i

  m

rSi

L(W(k))  wr (k)

xi

 Si m

max
r[m]

L(W(k))  wr (k)





|Si|

 n

u(k) - y

2.

2m

To analyze I1i, by Corollary 4.1, we know wr(k) - wr(0)  R and wr(k) - wr(0)  R for all r  [m]. Furthermore, because R < R, we know I wr(k + 1) xi  0 =
I wr(k) xi  0 . Thus we can find a more convenient expression of I1i for analysis

I1i

=

-

 m

n

xi xj (uj - yj )

I wr(k) xi  0, wr(k) xj  0

j=1

rSi

n

= -  (uj - yj)(Hij(k) - Hij(k))

j=1

7

Under review as a conference paper at ICLR 2019

where Hij(k)

=

1 m

m r=1

xi

xj I

wr (k)

xi  0, wr(k)

xj  0

is just the (i, j)-th

entry of a discrete version of Gram matrix defined in Section 3 and Hij(k) =

1 m

rSi xi xjI wr(k) xi  0, wr(k) xj  0 is a perturbation matrix. Let H(k) be the

n × n matrix with (i, j)-th entry being Hij (k). Using Lemma 4.1, with high probability we obtain

an upper bound of the operator norm

(n,n)

H(k)


2

Hij (k)  n

(i,j)=(1,1)

n i=1

Si

 n2mR  n2R.

mm

Similar to the classical analysis of gradient, we also need bound the quadratic term.

u(k + 1) - u(k)

2 2

 2

n

1 m

i=1

m r=1

L(W(k)) wr(k) 2

2
 2n2

y - u(k)

2 2

.

With these estimates at hand, we are ready to prove the induction hypothesis.

y - u(k + 1)

2 2

=

y - u(k) - (u(k + 1) - u(k))

2 2

=

y - u(k)

2 2

-

2

(y

-

u(k))

(u(k + 1) - u(k)) +

u(k + 1) - u(k)

2 2

=

y - u(k)

2 2

-

2

(y

-

u(k))

H(k) (y - u(k))

+ 2 (y - u(k)) H(k) (y - u(k)) - 2 (y - u(k)) I2

+

u(k + 1) - u(k)

2 2

(1 - 0 + 2n2R + 2n3/2R + 2n2)

y - u(k)

2 2

(1 - 0 ) 2

y - u(k)

2 2

.

The third equality we used the decomposition of u(k + 1) - u(k). The first inequality we used the

Lemma 3.2, the bound on the step size, the bound on I2, the bound on H(k) 2 and the bound

on

u(k + 1) - u(k)

2 2

.

The last inequality we used the bound of the step size and the bound of R.

Therefore Condition 4.1 holds for k = k + 1. Now by induction, we prove Theorem 4.1.

5 CONCLUSION AND DISCUSSION

In this paper we show with over-parameterization, gradient descent provable converges to the global minimum of the empirical loss at a linear convergence rate. The key proof idea is to show the over-parameterization makes Gram matrix remain positive definite for all iterations, which in turn guarantees the linear convergence. Here we list some future directions.

First, we believe the number of hidden nodes m required can be reduced. For example, previous

work

(Soudry

&

Carmon,

2016)

showed

m



n d

is enough

to

make

all

differentiable

local

minima

global. In our setting, using advanced tools from probability and matrix perturbation theory to

analyze H(t), we may able to tighten the bound.

Another direction is to prove the global convergence of gradient descent on multiple layer neural networks and convolutional neural networks. We believe our approach is still applicable because for a fixed a neural network architecture, when the number of hidden nodes is large and the initialization scheme is random Gaussian with proper scaling, the Gram matrix is also positive definite, which ensures the linear convergence of the empirical loss (at least at the begining). We believe combing our proof idea with the discovery of balancedness between layers in Du et al. (2018a) is a promising approach.

Lastly, in our paper we used the empirical loss as a potential function to measure the progress. If we use another potential function, we may able to prove the convergence rate of accelerated methods. This technique has been exploited in Wilson et al. (2016) for analyzing convex optimization. It would be interesting to bring their idea to analyze other first order methods.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural networks. In International Conference on Machine Learning, pp. 1908­1916, 2014.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with gaussian inputs. In International Conference on Machine Learning, pp. 605­614, 2017.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural Information Processing Systems, pp. 2422­2430, 2017.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient method converges on tame functions. arXiv preprint arXiv:1804.07795, 2018.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. Proceedings of the 35th International Conference on Machine Learning, pp. 1329­1338, 2018.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pp. 1067­1077, 2017a.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017b.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. Proceedings of the 35th International Conference on Machine Learning, pp. 1339­1348, 2018b.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. arXiv preprint arXiv:1611.01540, 2016.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pp. 797­842, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017.
Benjamin D Haeffele and Rene´ Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning, pp. 1724­1732, 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information Processing Systems, pp. 586­594, 2016.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
9

Under review as a conference paper at ICLR 2019
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural networks. Proceedings of the National Academy of Sciences.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In International Conference on Machine Learning, pp. 2603­2612, 2017.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In International Conference on Machine Learning, pp. 774­782, 2016.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural networks. In International Conference on Machine Learning, pp. 4433­4441, 2018.
Mahdi Soltanolkotabi. Learning ReLus via gradient descent. In Advances in Neural Information Processing Systems, pp. 2007­2017, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory, 2018.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis. In International Conference on Machine Learning, pp. 3404­3413, 2017.
Russell Tsuchida, Farbod Roosta-Khorasani, and Marcus Gallagher. Invariance of weight distributions in rectified mlps. arXiv preprint arXiv:1711.09090, 2017.
Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.
Ashia C Wilson, Benjamin Recht, and Michael I Jordan. A Lyapunov analysis of momentum methods in optimization. arXiv preprint arXiv:1611.02635, 2016.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Artificial Intelligence and Statistics, pp. 1216­1224, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. NIN, 8:35­67. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. Yi Zhou and Yingbin Liang. Critical points of neural networks: Analytical forms and landscape
properties. arXiv preprint arXiv:1710.11205, 2017.
10

Under review as a conference paper at ICLR 2019

A TECHNICAL PROOFS

Proof of Lemma 3.1. For note every fixed (i, j) pair, Hij(0) is an average of independent random variables. Therefore, by Hoeffding inequality, we have with probability 1 -  ,

Hij (0) - Hij



2 log(1/

) .

m

Setting  = n2 and applying union bound over (i, j) pairs, we have for every (i, j) pair with probability at least 1 - 

Hij (0) - Hij

 4 log(n/) . m

Thus we have

H(0) - H

2 2



H(0) - H

2 F



i,j

Hij (0)

-

Hij

2



16n2

log2(n/) .
m

Thus if m = 

n2 log2(n/) 02

we have the desired result.

Proof of Lemma 3.4. Suppose the conclusion does not hold at time t. If there exists r  [m],

wr(t) - wr(0)

 R or

y - u(t)

2 2

>

exp(-0t)

y - u(0)

2 2

,

then

by

Lemma

3.3

we

know

there

exists

s



t

such

that

min(H(s))

<

1 2

0.

By

Lemma

3.2

we

know

there

exists

t0 = inf

t > 0 : max
r[m]

wr(t) - wr(0)

2 2



R

.

Thus at t0, there exists r  [m],

wr(t0) - wr(0)

2 2

=

R.

Now by Lemma 3.2, we know H(t0)



1 2

0

for

t

 t0. However, by Lemma 3.3, we know

wr(t0) - wr(0) 2 < R

< R. Contradiction.

For

the

other

case,

at

time

t,

min(H(t))

<

1 2

0

we

know

we

know

there

exists

t0 = inf

t  0 : max
r[m]

wr(t) - wr(0)

2 2



R

.

The rest proof is the same as the previous case.

Proof of Corollary 4.1. We use the norm of gradient to bound this distance.

k L(W(k ))

wr(k + 1) - wr(0) 2 
k =0

wr(k )

2



k

 n y- u(k ) 2

m

k =0



k

 n(1 - 

 2

)k

/2

m

y - u(k ) 2

k =0





 n(1

-

0 2

)k

/2

m

y - u(k ) 2

k =0

4 =

n y - u(0) 2 .

m0

Proof of Lemma 4.1. For a fixed i  [n] and r  [m], by anti-concentration inequality, we know

P(Air )



2R .
2

Thus

we

can

bound

the

size

of

Si

in

expectation.

E

Si

=

m r=1

P(Air )



2mR . 2

(7)

11

Under review as a conference paper at ICLR 2019

Summing over i = 1, . . . , n, we have

n
E Si
i=1

 2mnR . 2

Thus by Markov's inequality, we have with high probability

n
Si  CmnR.
i=1

for some large positive constant C > 0.

(8)

12

