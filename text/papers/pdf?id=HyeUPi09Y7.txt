Under review as a conference paper at ICLR 2019
DEEP CONVOLUTIONAL GAUSSIAN PROCESS
Anonymous authors Paper under double-blind review
ABSTRACT
Gaussian Processes (GPs) are flexible priors over functions which are robust to over-fitting and able to provide well-calibrated predictive uncertainty. However, most of the existing GP models rely on local metrics like Euclidean distance to generalize. We introduce Deep Convolutional Gaussian Processes (DCGPs), a Bayesian nonparametric variant of the well-known Deep Convolutional Neural Networks (DCNNs), to make Gaussian Processes have more powerful non-local generalization abilities on images. The main contribution of our work is incorporating convolutional structures into Deep Gaussian Processes (DGPs). We achieve this by introducing patch-response functions in patch domain, which perform similar roles like convolutional filters in DCNNs. Inference in the patch domain is not tractable, so we introduce inter-domain approximation to conduct inference on DCGPs. Results on MNIST and CIFAR-10 outperform other existing GP models, which show the effectiveness of our model.
1 INTRODUCTION
Gaussian Processes (GPs) are rich distributions over functions, which can be used as flexible priors in Bayesian nonparametric models. GP models have several desirable properties in practice, for example, they are robust to over-fitting and have the ability to provide well-calibrated predictive uncertainty. These properties make GP models achieve state-of-the-art performance in a wide range of applications (Snoek et al., 2012; Briol et al., 2015). Generally, a GP prior is defined by its mean and covariance function (or kernel). The commonly used kernel functions often rely on rather rudimentary and local metrics for generalization, e.g. stationary kernels only depend on the Euclidean distance between data. However, as Bengio (2009) pointed out, deep structures allow for more advanced and non-local metrics for generalization, which makes deep architectures enormously successful in recent years. How to make GP priors have more non-local generalization ability is still an open research problem.
Researchers have tried two orthogonal ways to improve the non-local generalization ability of GPs. One way is the kernel learning. Kernels can be learned from the data to some extent (Williams & Rasmussen, 1996). But inference over a large and richly parameterized space of kernels is expensive. And approximate methods may be at risk of over-fitting. Wilson et al. (2016b;a) parameterize the kernel function with a neural network, but they need regularization techniques to keep away from over-fitting. Duvenaud et al. (2013) develop expressive compositional kernels with sum and product of simple kernels, but their optimization is computational expensive and needs to specify the basic kernel. Convolutional structures have been proved successful as a feature extractor and applied to several different tasks like image caption (Vinyals et al., 2014), style transfer (Gatys et al., 2015) and machine translation (Gehring et al., 2017). However, there is little work on incorporating convolutional structures in non-parametric methods like GPs. Recently, van der Wilk et al. (2017) proposed convolutional GPs that introduce convolutional structures into kernels, which show a significant improvement compared with simple basic kernels. But the kernels they proposed are not universal (Sriperumbudur et al., 2010), which means that these kernels cannot capture all of the interactions between pixels (e.g. the interactions between pixels in different patches). They only extract patch-level features, and determine which categories those patch-level features belong to. Thus those kernels may not work well in more complex tasks.
Another way to introduce non-local generalization ability is deepening the GPs, which is inspired by the success of deep neural networks. Damianou & Lawrence (2013) first introduced Deep Gaussian Processes (DGPs), which are a hierarchical composition of GPs that can overcome the limitations of
1

Under review as a conference paper at ICLR 2019

standard (single-layer) GPs while retaining the advantages. DGPs are richer models than standard GPs, just as deep neural networks are richer than generalized linear models. However, compared with highly parameterized deep neural networks, DGPs learn a representation hierarchy non-parametrically with very few hyper-parameters to optimize. Inference in DGPs is intractable due to the nonlinearity of each GP layer. Several approximation methods have been proposed like approximate expectation propagation (Bui et al., 2016) and doubly stochastic variational inference (Salimbeni & Deisenroth, 2017).
In this paper, we propose Deep Convolutional Gaussian Processes (DCGPs), which is a general framework that incorporates convolutional structure into DGPs, to improve the non-local generalization ability of GPs on images, which is crucial for practical GPs' applications on image analysis. DCGPs combine both the advantages of convolutional structure and deep architecture: we introduce the convolutional structure to obtain non-local generalization ability, while utilize the deep architecture to ensure model's universality. More specifically, we introduce a patch-response function in each DGP's layer which leads to a convolutional structure. Inferring a patch-response function defined in patch domain is not straightforward, so we introduce inter-domain approximations (Lázaro-Gredilla & Figueiras-Vidal, 2009) to perform inference on DCGPs. Results on MNIST and CIFAR-10 shows that our methods outperform other GP models, which proves its effectiveness.

2 BACKGROUND
GPs are Bayesian nonparametric models with analytically tractable posteriors and marginal likelihoods, which lead to several desirable properties. For example, the posteriors can provide wellcalibrated uncertainties, while the marginal likelihood can help select the kernel hyper-parameters. However, the computational complexity of inferring the posteriors with N observations is O(N 3), which makes GPs hard to scale up. Sparse approximations (Snelson & Ghahramani, 2005) were proposed to solve this problem by introducing a small set of pseudo-inputs called inducing points, which can reduce the computational complexity to O(N M 2), where M is the number of inducing points. In this paper, we will adopt the popular sparse variational framework (de G. Matthews et al., 2016), which can simultaneously reduce the computational complexity and approximate the variational posterior efficiently.
2.1 SPARSE GAUSSIAN PROCESSES WITH VARIATIONAL INFERENCE
We consider the task of inferring a stochastic function: f : RD  R, given a likelihood p(y|f ) and a set of N observations y = (y1, · · · , yN ) at designed locations X = (X1, · · · , Xn) . Several meaningful tasks can be formulated in this framework. For example, when we deal with image classification problems X can be view as images and y can be view as labels. We place a GP prior on f : f  GP m(·), k(·, ·) that models all function values as jointly Gaussian, with a mean function m : RD  R and a covariance function k : RD × RD  R. Following Quiñonero-Candela et al. (2007), we further define an additional set of M inducing locations Z = (Z1, · · · , ZM ), trying to represent the whole dataset with M N . For simplicity, we use the notation f = f (X) and u = f (Z) for the function values at the designed and inducing locations respectively1. We also define [m(X)]i = m(Xi) and [k(X, Z)ij] = k(Xi, Zj).
By the definition of a GP, the joint density p(f , u) is a Gaussian whose mean is given by the mean function evaluated at every input (X, Z) and the corresponding covariance is given by the covariance function evaluated at every pair of inputs. The joint density of y, f and u is

NN
p(y, f , u) = p(f , u; X, Z) p(yi|fi) = p(f |u; X, Z)p(u, Z) p(yi|fi).
i=1 i=1

(1)

1Throughout this paper we use the bold type letters (e.g. f ) denotes the variables, while the normal type letters (e.g. f ) denotes the functions

2

Under review as a conference paper at ICLR 2019

In equation 1 we factorized the joint GP prior p(f , u|X, Z) into the prior p(u) = N u|m(Z), k(Z, Z) and the conditional p(f |u; X, Z) = N (f |µ, ), where

µ =m(X) + k(Z, Z)-1k(Z, X)(u - m(Z)),  =k(X, X) - k(X, Z)k(Z, Z)-1k(Z, X).

(2)

Notice that given Z, the likelihood p(yi|fi) only requires the marginal fi, which is a univariate Gaussian only depending on the corresponding input Xi, according to equation 2 as

p(fi|u; X, Z) = N (fi|µi, ii).

(3)

Thus inference in the model equation 1 is possible in closed form when the likelihood p(y|f ) is Gaussian, but the computation scales cubically with N . In general we're more interested in large datasets with non-Gaussian likelihood, which is a setting more close to the reality. However, large datasets increase the computational complexity, and non-Gaussian likelihood make the inference intractable.

To overcome both these difficulties simultaneously, we introduce a variational posterior. More specifically, we employ a variational posterior q(f , u), and optimize the lower bound (ELBO) on the marginal log likelihood (evidence) as

log

p(y|f ,

u)



log

p(y|f ,

u)

-

KL(q(f ,

u)||p(f ,

u))

=

Eq(f ,u)

log

p(y, f , u) ,
q(f , u)

(4)

where KL(·) is a operator to calculate the Kullback­Leibler divergence. Following Hensman et al. (2013), we factorize the variational posterior q(f , u) as

q(f , u) = p(f |u; X, Z)q(u),

(5)

where q(u) = N (u|m, S). This factorization retains the GP conditional structure p(f |u; X, Z), while keeps both terms in the variational posterior are Gaussian, thus we can analytically marginalize u as

q(f |m, S; X, Z) = p(f |u; X, Z)q(u)du = N (f |µ~, ~ ),

(6)

where

µ~ =m(X) + k(Z, Z)-1k(Z, X)(m - m(Z)) ~ =k(X, X) - k(X, Z)k(Z, Z)-1(k(Z, Z) - S)k(Z, Z)-1k(Z, X),

(7)

which is also a GP whose marginals only depend on the corresponding inputs. As a result, we can have a simplified lower bound

n
L = Eq(fi|u;Xi,Z) log p(yi|fi) - KL[q(u)||p(u)],
i=1

(8)

where q(fi|u; Xi, Z) can be inferred with only Xi.
Notice that the first term of ELBO is a sum over the data, so we can get an unbiased estimator with mini-batch sub-sampling, which permits a large-scale inference. We refer to a GP with this method of inference as a sparse variational GP (SVGP). The variational parameters (Z, m, S) are found by maximizing the lower bound equation 8 together with learning model parameters (hyper-parameters of the kernel or likelihood).

Recall that we have only considered scalar output yi  R. In the case of D-dimensional outputs yi  RD, we define Y as the matrix with ith row containing the ith observation yi. Similarly, we can define F and U 2. We further assume that each output is an independent GP, thus the GP prior

can be written as

D d=1

p(Fd|Ud;

X,

Z)p(Ud;

Z).

For

simplicity,

we

abbreviate

it

as

p(F|U;

X,

Z)

in the remaining sections.

2Notice that fi or [f ]i is the ith observation of F, which is useful throughout the paper.

3

Under review as a conference paper at ICLR 2019

2.2 DEEP GAUSSIAN PROCESSES

A Deep Gaussian Process defines a functional prior recursively on vector-valued stochastic functions F 1, · · · , F L. The prior on each function F l is an independent GP in each dimension, with input
locations given by the noisy corruptions of the function values at the next layer: the output of the GPs at layer l is F l, and the corresponding input is F l-1. The noise between layers is assumed i.i.d.
Gaussian.

Most of the existing deep Gaussian process methods (Damianou & Lawrence, 2013) (Bui et al., 2016) explicitly parameterize the noisy corruption separately from the outputs of each GP, while Salimbeni & Deisenroth (2017) use a predefined fixed noisy corruptions. Similar to single-layer case, we have inducing locations Zl-1 at each layer and inducing function values Ul for each dimension.

A sparse instantiation of the DGP has the joint density

LN
p(Y, {Fl, Ul}Ll=1) = p(Fl|Ul; Fl-1, Zl-1)p(Ul; Zl-1) p(yi|fiL),
l=1 i=1

(9)

where we define F0 = X. The conditional p(Fl|Ul; Fl-1, Zl-1)p(Ul; Zl-1) is similar to the fullyconnected layers in neural networks, and FL can be considered as the outputs of neural networks. After we obtain the outputs of DGPs, the likelihood p(yi|fiL) can help make decision on y.

Inference in equation 9 is intractable, so we must conduct approximation methods. The original DGP presentation (Damianou & Lawrence, 2013) uses a variational posterior that maintains the exact model conditioned on Ul and admits a tractable lower bound on the log marginal likelihood if the kernel is of particular form. However, this method forces the inputs to each layer independent from the outputs of the previous layer, which loses all the correlation between layers thus it cannot express the complexity of the full model and is likely to underestimate the variance. Salimbeni & Deisenroth (2017) proposed doubly stochastic variational inference for DGPs, which retains the full conditional structure between layers. It's not analytically tractable, however, due to the sparse posterior within each layer, the lower bound can be sampled using univariate Gaussian.

Our proposed method is still a DGP. However most present works of DGPs do not consider the structure of the input. Viewed from neural nets perspective, Neal (2012) has proven that a two-layer infinite-width multi-layer perceptron (MLP) is equivalent to a single-layer GP, DGPs performs like MLPs as well (Cutajar et al., 2017). As DCNNs have been proved successful in a wide range of applications due to the non-local generalization ability of convolutional structures, it is natural to consider how to incorporate convolutional structures into DGPs and explore whether DCNNs perform like some kinds of GPs.

2.3 INTER-DOMAIN GAUSSIAN PROCESSES
Consider a GP f  GP m(·), k(·, ·) with inputs x  RD and some deterministic function g(X, Z) with Z  RH , we define the following transformation:

u(Z) = f (X)g(X, Z)dX
RD

(10)

Since u(Z) is obtained by a linear transformation of GP f (X), it is also a GP. This new GP may lie in a different domain of a possibly different dimension, thus called inter-domain GP. For example, if X locates in image domain and Z locates in patch domain, we can define a patch-extract function g(X, Z) to transform f in image domain to u in patch domain.

By the definition we can get

m(Z) = m(X)g(X, Z)dX
RD

k(X, Z) = k(X, X )g(X , Z)dX
RD

.

k(Z, Z ) =

k(X, X )g(X, Z)g(X , Z )dXdX

RD RD

(11)

4

Under review as a conference paper at ICLR 2019

It is natural to use inter-domain inducing variables if the above integrals are tractable, for example, when we deal with classification problem, if we let X be the original input, Z denote the patch-level feature and g(X, Z) be the patch-extractor, then we can construct convolutional GPs van der Wilk et al. (2017), which we will introduce in the next subsection.

2.4 CONVOLUTIONAL GAUSSIAN PROCESS

Convolutional GPs van der Wilk et al. (2017) utilize inter-domain approximation in the sense that patch-level feature space is different from the input space and convolutional structure can be incorporated into a patch-level feature space to build a convolutional GP.

More specifically, convolutional GPs construct a prior on functions on images X of size D = W × H

to real valued responses f : Rd  R with a patch-response function g : RE  R that maps the

image patches P to real-valued responses, where E = w × h denotes the patch size. A GP prior is

first placed on g:

g  GP(mg(·).kg(·, ·)).

(12)

Then convolutional GP prior can be derived by some proper deterministic function describes the relation between image patches and images.
van der Wilk et al. (2017) follows (Duvenaud et al., 2011) and directly sum over patch responses:

f (X) = g(Pij) = f  GP( mg(Pij),

kg(Pij , Pij ))

(13)

where {Pij} denotes the patches extracted from X.

This method constructs a valid GP prior, however, it does not permit interaction between pixels in different patches, which limits its expressiveness. Our proposed method will still use an inter-domain approximation in a single-layer GP as it is a natural choice of convolutional structure. However, we introduced a hierarchical structure to solve the universality problem. Instead of directly sum over the responses, we treat the responses as next convolutional GP layer's input. This way is similar to DCNNs, which are always build with first several convolutional layers, then several fully-connected layers that can introduce non-local generalization and remain universal simultaneously.

3 DEEP CONVOLUTIONAL GAUSSIAN PROCESSES

In this section, we introduce how to construct the DCGPs with patch-response functions.

We start with the convolutional GP prior, which is a stochastic function whose inputs are images X of size D = W × H × C. Assume we use patch size w × h with strides s to extract the patches and get a total of P = W × H patches with shape w × h × C, denoted as [Pij]W ×H , Pij  Rw×h×C . To incorporate convolutional structures into GPs, we first define a patch-response function on image
patches of size d = w × h × C: G : Rd  RC . This is same in spirit to the convolutional layers in convolutional neural networks. Then we can apply G to each of the extracted patches, and get an output image of size D = W × H × C . Thus we can define a convolutional GP Fconv : RW ×H×C  RW ×H ×C .

Assume Gi denotes the ith output dimension of G and Fwhc denotes the (w, h, c)'s element of Fconv. If each of the Gi is given a GP prior, a GP prior will also be induced on Fwhc:

Gi  GP(mi(·), ki(·, ·)), F (X) = [G(Pij)]W ×H =Fwhc(X)  GP mc(Pwh), kc(Pwh, Pwh) .

(14)

Notice that the prior of each layer in DGPs only calls for independent GP in each output dimension,
so we can employ different mean functions and kernels for every single dimension, thus Fconv denotes a valid GP prior. We can recursively define this functional prior on stochastic feature map Fc1onv, · · · , FLconv, (assume F0conv = X) and finally get a DCGP:

LN
p(Y, {Flconv}lL=1) = p(Flconv|Fcl-on1v) p(yi|[fcLonv]i).
l=1 i=1

(15)

5

Under review as a conference paper at ICLR 2019

extract patches

Figure 1: The method we use to construct convolutional GP prior. We first extract patches from the original feature map, then apply patch-response function G(·) on it. If a GP prior is placed on G,
then we can get a convolutional GP prior on feature map.

DCGPs allow for the interaction between pixels in different patches with their hierarchies, and generally deep convolutional structures have more powerful non-local generalization abilities. However, inferences in DGPs are not tractable as GP layers are highly nonlinear. The patch-response functions are defined in patches domain, not the images domain, which makes the inference even harder. Thus, we will introduce inducing patches doubly stochastic approximation in next section, which is a variational inference methods combining inter-domain inducing points and doubly stochastic variational inference to solve these problems.

4 INDUCING PATCHES DOUBLY STOCHASTIC APPROXIMATION

We begin with approximating the true posterior of a single convolutional GP layer using a small set of inducing points. We are more interested in the patch-response functions, so the main idea is to place these inducing points in the input space of patches, rather than images, which corresponds to use inter-domain inducing points.

To provide more detail, consider a single convolutional GP layer, we define a set of M inducing
locations Z = (Z1, · · · , ZM ), Zi  Rw×h×c with its inducing function values U = G(Z)  RC . We are interested in the conditional p(Fconv|U, X, Z). If a GP prior is placed on Gi, i = 1, · · · , C :
Gi  GP mi(·), ki(·, ·) , we can get the conditional p(Fwhc|U, X, Z) = N (Fwhc|µwhc, whc),

where

µwhc whc

=mc(Pwh =kc(Pwh,

) + kc(Z, Z)-1kc Pwh) - kc(Pwh,

(ZZ),kPc(wZh,)Z(U)-c 1-kcm(Zc(,ZP)w)h).

(16)

This process can be seen as an inter-domain GP in which the transformation function of Fwhc in equation 11 is defined as g(X, Z) = wh, where wh is the Kronecker delta.

Thus the deep convolutional GP prior can be factorized as:

LN
p(Y, {Flconv, Ul}Ll=1) = p(Fcl onv|Ul; Flc-on1v, Zl-1)p(Ul, Zl-1) p(yi|[fcLonv]i).
l=1 i=1

(17)

Following Salimbeni & Deisenroth (2017), we propose a doubly stochastic variational inference
method to infer the DCGP model. We employ a variational distribution q({Flconv, Ul}lL=1), and optimize the evidence lower bound:

log p(Y|{Flconv, Ul}lL=1)  log p(Y|{Flconv, Ul}Ll=1) - KL(q({Flconv, Ul}lL=1)||p({Flconv, Ul}Ll=1))

=Eq({Fcl onv ,Ul}lL=1)

log

p(Y, {Fcl onv, Ul}Ll=1) q({Fcl onv, Ul}lL=1)

.

(18)

6

Under review as a conference paper at ICLR 2019

Like equation 5, we choose
L
q({Fcl onv, Ul}lL=1) = p(Flconv|Ul; Fcl-on1v, Zl-1)q(Ul).
l=1
where q(Ul)  N (ml, Sl). This factorization leads to a simplified evidence lower bound
NL
L = Eq([fcLonv]i) log p(yi|[fcLonv]i) - KL(q(Ul)||p(Ul)).
i=1 i=1

(19) (20)

Notice that the sparse variational posterior marginals q([FcLonv]i) only depends on the corresponding inputs Xi. This seems non-trivial however each GP layer's sparse variational posterior marginals q([fclonv]i) only depends on q([fclo-n1v]i) as mentioned in equation 7.Thus we can easily draw sample [fcLonv]i  q([fcLonv]i) with re-parameterization trick (Rezende et al., 2014) (Kingma & Welling,
2014) and calculate the lower bound efficiently.

In prediction phase, we use the Gaussian mixture:

q([fcLonv ]i )



1 S

S

q([fcLonv]i|mL-1, SL-1; [fcLo-nv1]i(s), ZL-1),

s=1

(21)

where [fcLo-nv1](is), s = 1, · · · , S are S samples drawn at layer L - 1 with the re-parameterization trick.

5 EXPERIMENTS

In this section, we evaluate our models on two most commonly used benchmark classification datasets, MNIST and CIFAR-10, to show our model's effectiveness. The whole section will be organized as following: first we introduce the models we use in our experiments in details, then we show our model's results on MNIST and compare our method with the existing GP models, and finally show our results on CIFAR-10.

5.1 MODEL DETAILS
We follow the general GPs setting and use zero mean functions and RBF kernels. Though Duvenaud et al. (2014) have shown that zero mean function can perform pathologically in DGPs to some extent, we find that in our experiments zero mean function is enough to perform well.
All convolutional GP layers in our experiments use patch size 5 × 5 with stride 2 × 2. Each patches calculate 10 features, thus the output channels of each convolutional GP layers are 10. We construct two types of DCGPs, which use one or two convolutional GP layers as feature extractor, and followed by a normal GP layer to do the prediction, denoted as DCGP2, DCGP3 respectively. All our experiments use 100 (interdomain) inducing points each layer that are initialized randomly, which is different from existing methods initializing inducing features by clustering in the images space or patches space. As we're dealing with multi-class classification problems, we utilize the widely-used robust max multi-class likelihood (Hernández-lobato et al., Figure 2: Features learned in MNIST experiments. 2011). At test case, we use 20 samples to cal- As our inducing points are initialized with uniform culate the Gaussian mixture equation 21. We distribution U (0, 1), our models finally learn the report classification accuracy and negative log features of handwritten digits. predictive probability (nlpp) in the following subsections.

7

Under review as a conference paper at ICLR 2019

Table 1: Experiments results on MNIST

Model

M Accuracy

SVGP (Salimbeni & Deisenroth, 2017) DGP2 (Salimbeni & Deisenroth, 2017) DGP3 (Salimbeni & Deisenroth, 2017)
DCGP2 (ours) DCGP3 (ours) SVGP-conv (van der Wilk et al., 2017) SVGP-conv-rbf (van der Wilk et al., 2017)

100 100 100 100 100 750 750

97.48% 98.06% 98.11% 98.88% 98.98% 97.92% 98.83%

NLPP
/ / / 0.037 0.035 0.077 0.039

Table 2: Experiments results on CIFAR-10

Model

M Accuracy

SVGP (van der Wilk et al., 2017) SVGP-wconv (van der Wilk et al., 2017) SVGP-multi (van der Wilk et al., 2017) SVGP-wconv (van der Wilk et al., 2017) SVGP-multi (van der Wilk et al., 2017)
DCGP2 (ours) DCGP3 (ours)

1000 1000 1000 100 100 100 100

51.4% 62.8% 64.6% 48.1% 54.4% 61.5% 64.7%

NLPP
2.165 2.016 1.982 3.229 2.695 1.579 1.373

5.2 MNIST MULTI-CLASS CLASSIFICATION
We use full unprocessed data with the standard train/test split of 60K/10K, and results are shown in Table 1.
Salimbeni & Deisenroth (2017) presented that two and three layers DGPs (denoted as DGP2 and DGP 3) with 100 inducing points get the test accuracy of 98.06% and 98.11% respectively. Introducing convolutional structure into DGPs significantly improves the performance, as DGPs with one convolutional GP layer achieves the test accuracy of 98.88%, which outperforms DGPs with two normal GP layer. It is therefore meaningful trying to incorporate the convolutional structure into DGPs.
van der Wilk et al. (2017) presents the convolutional kernels' performance on MNIST. A sparse variational GP with vanilla convolutional kernel (SVGP-conv) and 750 inducing points get 97.92% test accuracy with nlpp 0.077. Our first convolutional GP layer performs the similar role with their kernel to some extent. However, they use an additive structure, which is too restrictive as we mentioned below, yielding a poor performance. They try to solve this problem by using a weighted additive structure and adding an RBF kernel to capture the interactions not modeled by the convolutional structure. The best result (SVGP-conv-rbf) they get is 98.83% accuracy with nlpp 0.039, which is still worse than our results. The DCGP with two convolutional GP layers achieves a test accuracy of 98.98% with nlpp 0.035 which is only a little better than one convolutional GP layer variant. This may indicate one convolutional GP layer is enough to capture MNIST's feature well.
We want to show that our convolutional GP layer really learn the patch-level features and perform like convolutional filters. The optimized inducing patches of DCGP2 are shown in Figure 2. Our inducing points are randomly initialized with uniform distribution U (0, 1), however the optimization results show textures with different angles and thicknesses. As a result, the patch-response function will extract features from patches similar to these textures, which works like the convolutional filters.
5.3 CIFAR-10 MULTI-CLASS CLASSIFICATION
Still, we use full unprocessed data with the standard train/test split of 50K/10K. The results are shown in Table 2.
van der Wilk et al. (2017) presented several GP models' results with different kernels and 1000 inducing points. Our results seem not competitive compared with the convolutional kernel, however we need to point out that all our model only use 100 inducing points. In this setting their best
8

Under review as a conference paper at ICLR 2019

performance is test accuracy 54.4% with nlpp 2.695, and our model still outperform their results, by achieving a test accuracy 64.7% with NLPP 1.373.
Another interesting observation is that the test accuracy is not directly related to test nlpp in CIFAR-10, as our model achieves a lower nlpp while have a lower test accuracy than their 1000 inducing points results. Maybe their model correctly classified the samples while have little confidence on it, thus lead to this observations.

6 DISCUSSION FOR COMPUTATION EFFICIENCY

Our experiments show that DCGPs can outperform existing GP models on images. However, it's

computational cost is large as well. The training curve is shown in Figure 3 & 4. DCGPs get

comparable performances within acceptable times. Notice that DCGPs' computation complexity is

O(

l i=1

Mi3

+

N

li=1(Pi2 + MiPi + Mi2)), where l is the number of convolutional GP layers, Mi

and Pi are the number of inducing points and patches at layer i and N is the batch size. Compared

with convolutional GPs computational complexity O(M 3 + N (P 2 + M P + M 2)), DCGPs compu-

tational complexity is much higher. However, training DCGPs doesn't spend much longer time than

convolutional GPs. This is because of the lower inducing points number (100 in DCGPs and 1000 in

convolutional GPs). In fact, the hierarchy structure improves DCGPs' expressiveness, thus DCGPs

can learn a better representation with few numbers of inducing points.

Figure 3: The training curve of MNIST dataset. DCGPs can get comparable performances with less inducing points in acceptable times compared with different variants of convolutional GPs.

Figure 4: The training curve of CIFAR-10 dataset. DCGPs can get comparable performances with less inducing points in acceptable times compared with different variants of convolutional GPs. Notice that DCGPs' nlpp are converged much faster than convolutional GPs, which means convolutional GPs obtain a higher test accuracy with little confidence on it.
7 CONCLUSION
We introduce deep convolutional Gaussian processes (DCGPs), a Bayesian non-parametric variant of the successful deep convolutional neural networks (DCNNs). Our main contribution is incorporating
9

Under review as a conference paper at ICLR 2019
convolutional structures into deep Gaussian processes (DGPs) and introducing an inter-domain approximation method to conduct inference on it. Experiments show that DCGPs can always outperform existing GP models on benchmark image classification datasets with smaller number of inducing points. Meanwhile, the visualization of the optimized inducing patches show that our proposed convolutional GP layer can really learn patch-level feature and work like convolutional filters.
REFERENCES
Yoshua Bengio. Learning deep architectures for ai. Foundations & Trends R in Machine Learning, 2 (1):1­127, 2009.
Françoisxavier Briol, Chris. J. Oates, Mark Girolami, Michael A. Osborne, and Dino Sejdinovic. Probabilistic integration: A role for statisticians in numerical analysis? Statistics, 2015.
Thang Bui, Daniel Hernandez-Lobato, Jose Hernandez-Lobato, Yingzhen Li, and Richard Turner. Deep gaussian processes for regression using approximate expectation propagation. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 1472­1481, New York, New York, USA, 20­22 Jun 2016. PMLR. URL http://proceedings.mlr. press/v48/bui16.html.
Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi, and Maurizio Filippone. Random feature expansions for deep Gaussian processes. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 884­893, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/cutajar17a.html.
Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artificial Intelligence and Statistics, pp. 207­215, 2013.
Alexander G. de G. Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani. On sparse variational methods and the kullback-leibler divergence between stochastic processes. In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pp. 231­239, Cadiz, Spain, 09­11 May 2016. PMLR. URL http://proceedings.mlr. press/v51/matthews16.html.
David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B Tenenbaum, and Zoubin Ghahramani. Structure discovery in nonparametric regression through compositional kernel search. In Proceedings of the 30th International Conference on International Conference on Machine Learning-Volume 28, pp. III­1166. JMLR. org, 2013.
David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding pathologies in very deep networks. In Samuel Kaski and Jukka Corander (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, volume 33 of Proceedings of Machine Learning Research, pp. 202­210, Reykjavik, Iceland, 22­25 Apr 2014. PMLR. URL http://proceedings.mlr.press/v33/duvenaud14.html.
David K Duvenaud, Hannes Nickisch, and Carl E Rasmussen. Additive gaussian processes. In Advances in neural information processing systems, pp. 226­234, 2011.
Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. Computer Science, 2015.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1243­1252, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/gehring17a.html.
10

Under review as a conference paper at ICLR 2019
James Hensman, Nicolo Fusi, and Neil D. Lawrence. Gaussian processes for big data. Computer Science, 2013.
Daniel Hernández-lobato, Jose M. Hernández-lobato, and Pierre Dupont. Robust multi-class gaussian process classification. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 24, pp. 280­288. Curran Associates, Inc., 2011. URL http://papers.nips.cc/paper/ 4241-robust-multi-class-gaussian-process-classification.pdf.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2): 251­257, 1991.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. In Conference proceedings: papers accepted to the International Conference on Learning Representations (ICLR) 2014, 2014.
Miguel Lázaro-Gredilla and Aníbal Figueiras-Vidal. Inter-domain gaussian processes for sparse inference using inducing features. In Advances in Neural Information Processing Systems, pp. 1087­1095, 2009.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. In Advances in Neural Information Processing Systems, pp. 6231­6239, 2017.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.
Joaquin Quiñonero-Candela, Carl Edward Ramussen, and Christopher K I Williams. Approximation methods for gaussian process regression. Mit Press, 14(2):333­350, 2007.
Carl Edward Rasmussen and Christopher KI Williams. Gaussian process for machine learning. MIT press, 2006.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp. 1278­1286, Bejing, China, 22­24 Jun 2014. PMLR. URL http://proceedings.mlr.press/v32/rezende14.html.
Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian processes. In Advances in Neural Information Processing Systems, pp. 4591­4602, 2017.
Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. In International Conference on Neural Information Processing Systems, pp. 1257­1264, 2005.
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning algorithms. In International Conference on Neural Information Processing Systems, pp. 2951­2959, 2012.
Bharath K. Sriperumbudur, Kenji Fukumizu, and Gert R. G. Lanckriet. Universality, characteristic kernels and rkhs embedding of measures. Journal of Machine Learning Research, 12(7):2389­2410, 2010.
Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional gaussian processes. In Advances in Neural Information Processing Systems, pp. 2845­2854, 2017.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. pp. 3156­3164, 2014.
Christopher K. I. Williams and Carl Edward Rasmussen. Gaussian processes for regression. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (eds.), Advances in Neural Information Processing Systems 8, pp. 514­520. MIT Press, 1996. URL http://papers.nips.cc/ paper/1048-gaussian-processes-for-regression.pdf.
11

Under review as a conference paper at ICLR 2019 Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic variational deep
kernel learning. In Advances in Neural Information Processing Systems, pp. 2586­2594, 2016a. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial Intelligence and Statistics, pp. 370­378, 2016b.
12

Under review as a conference paper at ICLR 2019

A UNIVERSALITY OF DEEP CONVOLUTIONAL GAUSSIAN PROCESSES

We show that DCGPs can be universal consistent under some assumptions with the following proposition.
Proposition 1. If all except the last layer's patch-response function is L-Lipschitz, the corresponding output dimension is not less than the input dimension and we place GP priors with universal kernel on patch-response functions, then DCGPs are universal consistent.

Proof. We need only to prove that DCGPs assign probability measure over C0(Rd), in other worlds, arbitrary continuous function in C0(RD) can be represent with DCGPs under arbitrary accuracy.
First notice that GPs with universal kernel are universal consistent (Rasmussen & Williams, 2006),
G : Rd  RC can represent arbitrary continuous function in C0(Rd) under arbitrary accuracy. We want to show that f  C0(RD),  , there exists a patch-response function sequence G1, G2, · · · , Gl, s.t. the corresponding DCGP fl  · · ·  f1 satisfies that

fl  · · ·  f1 - f L2(R) <

(22)

where fl  · · ·  f1(x) = fl(fl-1(· · · (f1(x)))).

As the last convolutional GP layer directly use the whole feature map as input, we have that fl  C0(Rdl ),  , there exists fl = Gl, s.t.

fl - fl <L2(Rdl )

(23)

where dl is the size of layer l's feature map.

Meanwhile, as other layers' patch-response function are L-Lipschitz with output dimension no less than input dimension, there exists {Gi}il-=11, such that all Gi, i = 1, · · · , l - 1 are invertible, and the corresponding fi have the inverse function fi-1. Then we consider that f  C0(Rd)

fl  fl-1  · · ·  f1 - f  fl - f  f1-1  · · ·  fl--11 f1-1  · · ·  fl--11

(24)

As f  f1-1  · · ·  fl--11  C0(Rdl ), we can find proper Gl s.t. fl - f  f1-1  · · ·  fl--11 < . Thus we only need to prove that f1-1  · · ·  fl--11 is bounded.

As

we

assume

{Gi}li-=11

is

L-Lipschitz

with

inverse

function,

{Gi-1}li-=11

is

1 L

-Lipschitz

and

bounded.

So

f1-1  · · ·  fl--11



,1
Ll-1

thus

DCGPs

are

universal

consistent.

Proposition 1 shows that DCGPs with enough channels are universal. van der Wilk et al. (2017) proposed to solve the universality problem by introducing another RBF component. However, we consider it's not a general way as the residual can be too complex that an extra RBF component cannot capture it well (or we can directly use an RBF kernel GP to model all the data). DCGPs solve the universality problem by introducing hierarchy structures, which have been proved universal under some kinds of assumptions (Hornik, 1991; Lu et al., 2017). In our case, we need that the output dimension of patch-response functions are no less than the input dimension to ensure the universality. However, in practice, there's no need to use such a large number of channels, as models in general are not so complex. Experiments show that even with small number of channel numbers our model can get comparable results with lower inducing points number compared with convolutional GPs.

13

