Under review as a conference paper at ICLR 2019

SUPERVISED POLICY UPDATE
Anonymous authors Paper under double-blind review

ABSTRACT
We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.

1 INTRODUCTION

The policy gradient problem in deep reinforcement learning can be informally defined as seeking a parameterized policy  that produces a high expected reward J(). The parameterized policy  is realized with a neural network, and stochastic gradient descent with backpropagation is used to optimize the parameters. An issue that plagues traditional policy gradient methods is poor sample efficiency (Kakade, 2003; Schulman et al., 2015a; Wang et al., 2016b; Wu et al., 2017; Schulman et al., 2017). In algorithms such as REINFORCE (Williams, 1992), new samples are needed for every small gradient step. In environments for which generating trajectories is expensive (such as robotic environments), sample efficiency is of central concern. The sample efficiency of an algorithm is defined to be the number of calls to the environment required to attain a specified performance level (Kakade, 2003).

Thus, given the current policy k and a fixed number of trajectories (samples) generated from k , the goal of the sample efficiency problem is to construct a new policy with the highest performance improvement possible. In order to obtain a high performance improvement with the limited number of samples, it is desirable to limit the search to policies that are close to the original policy k (Kakade, 2002; Schulman et al., 2015a; Wu et al., 2017; Achiam et al., 2017; Schulman et al., 2017; Tangkaratt et al., 2018). Intuitively, if the candidate new policy  is far from the original policy k , it may not perform better than the original policy because too much emphasis is being placed on the relatively small batch of new data generated by k , and not enough emphasis is being placed on the relatively large amount of data and effort previously used to construct k .
This guideline of limiting the search to nearby policies seems reasonable in principle, but requires a notion of closeness of two policies. One natural approach is to define a distance or divergence (, k ) between the current policy k and the candidate new policy , and then attempt to solve the constrained optimization problem:

maximize


J^( | k , new data)

(1)

subject to (, k )  

(2)

where J^( | k , new data) is an estimate of J(), the performance of policy , based on the previous policy k and the batch of fresh data generated by k . The objective (1) attempts to maximize the performance of the updated policy, and the constraint (2) ensures that the updated
policy is not too far from the policy k that was used to generate the data.  is a hyper-parameter. As

1

Under review as a conference paper at ICLR 2019

discussed in the related work section, several recent papers (Kakade, 2002; Schulman et al., 2015a; 2017; Tangkaratt et al., 2018) belong to the framework (1)-(2).
We propose a new methodology, called Supervised Policy Update (SPU), for this sample efficiency problem. The methodology is general in that it applies to both discrete and continuous action spaces, and can address a wide variety of constraint types for (2). Starting with data generated by the current policy, SPU optimizes over a proximal policy space to find an optimal non-parameterized policy. It then solves a supervised regression problem to convert the non-parameterized policy to a parameterized policy, from which it draws new samples. We develop a general methodology for finding an optimal policy in the non-parameterized policy space, and then illustrate the methodology for three different definitions of proximity. We also show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. While SPU is substantially simpler than NPG/TRPO in terms of mathematics and implementation, our extensive experiments show that SPU is more sample efficient than TRPO in Mujoco simulated robotic tasks and PPO in Atari video game tasks.

2 PRELIMINARIES

We consider a Markov Decision Process (MDP) with state space S, action space A, and reward function r(s, a), s  S, a  A. Let  = {(a|s) : s  S, a  A} denote a policy, let  be the set of
all policies, and let the expected discounted reward be:

J ()



E
 

tr(st, at)

t=0

(3)

where   (0, 1) is a discount factor and  = (s0, a0, s1, . . . ) is a sample trajectory. Let A(s, a) be the advantage function for policy  (Silver, 2015; Levine, 2017; Schulman et al., 2015b). Deep reinforcement learning considers a set of parameterized policies DL = {|  }  , where each parameterized policy is defined by a neural network called the policy network. In this paper, we
will consider optimizing over the parameterized policies in DL as well as over the non-parameterized policies in . For concreteness, we assume that the state and action spaces are finite. However, the
methodology also applies to continuous state and action spaces, as shown in the Appendix.

One popular approach to maximizing J() over DL is to apply stochastic gradient ascent. The gradient of J() evaluated at a specific  = k can be shown to be (Williams, 1992; Sutton et al., 2000; Schulman et al., 2015b):



J (k

)

=

E
 k

t log k (at|st)Ak (st, at) .
t=0

(4)

To obtain an estimate of the gradient, we can sample finite-length trajectories i, i = 1, . . . , N , from k , and approximate (4) as:

1 N T -1

J (k )  N

 log k (ait|sit)Ait

i=1 t=0

gk

(5)

where T is the length of the trajectory, and Ait is an approximation of tAk (sit, ait) obtained from

a critic network. Additionally, define d(s)

(1 - )

 t=0

 t P (st

=

s)

for

the

the

future

state

probability distribution for policy , and denote (·|s) for the probability distribution over the action

space A when in state s and using policy . Further denote DKL( k )[s] for the KL divergence from distribution k (·|s) to (·|s), and denote

D¯ KL(

k )

=

E
sdk

[DKL(

k )[s]]

(6)

for the "aggregated KL divergence".

2.1 SURROGATE OBJECTIVES FOR THE SAMPLE EFFICIENCY PROBLEM
For the sample efficiency problem, the objective J() is typically approximated using samples generated from k (Schulman et al., 2015a; Achiam et al., 2017; Schulman et al., 2017). One of two

2

Under review as a conference paper at ICLR 2019

different approaches is typically used to approximate J() - J(k ) using samples from k . The first approach is to make a first order approximation of J() in the vicinity of k (Kakade, 2002;
Peters & Schaal, 2008a;b; Schulman et al., 2015a):

J () - J (k )  ( - k)T J (k )  ( - k)T gk

(7)

where gk is the sample estimate (5). The second approach, which applies to all policies    and not just to policies   DL, is to approximate the state distribution d(s) with dk (s), giving the approximation (Achiam et al., 2017; Schulman et al., 2017; Achiam, 2017):

J () - J (k )  Lk ()

1

1-

Esdk

E
ak (·|s)

(a|s) Ak (s, a) k (a|s)

(8)

To estimate the expectation in (8), as in (5), we generate trajectories of finite length T from k , create estimates of the advantage values using a critic network, and then form a sample average.
There is a well-known bound for the approximation (8) (Kakade & Langford, 2002; Achiam et al.,
2017). Furthermore, the approximation Lk () matches J() - J(k ) to the first order with respect to the parameter  (Achiam et al., 2017).

3 RELATED WORK

Natural gradient (Amari, 1998) was applied to policy gradient first by Kakade (Kakade, 2002) and

then in Peters & Schaal (2008a;b); Achiam (2017); Schulman et al. (2015a). We refer to this body

of literature as NPG/TRPO. The goal of NPG/TRPO is to solve the sample efficiency problem

(1)-(2) with (, k ) = D¯KL( k ), i.e., use the aggregate KL-divergence for the policy

proximity constraint (2). NPG/TRPO addresses this problem in the parameter space   . First, it

approximates J() with the first-order approximation (7) and D¯KL( k ) using a similar second-

order Third,

method. Second, it uses samples from k using these estimates (which are functions of

to form estimates of these ), it solves for the optimal

two approximations. . The optimal  is

a function of gk and of hk, the sample average of the Hessian evaluated at k. TRPO also limits the magnitude of the update to ensure D¯KL( k )   (i.e., ensuring the sampled estimate of the aggregated KL constraint is met without the second-order approximation).

SPU takes a very different approach by first (i) posing and solving the optimization problem in the non-parameterized policy space, and then (ii) solving a supervised regression problem to find a parameterized policy that is near the optimal non-parameterized policy. A recent paper, Guided Actor Critic (GAC), independently proposed a similar decomposition (Tangkaratt et al., 2018). However, GAC is much more restricted in that it considers only one specific constraint criterion (aggregated reverse-KL divergence) and applies only to continuous action spaces. Furthermore, GAC incurs significantly higher computational complexity because at each iteration, GAC inverts a damped Hessian whose complexity scales cubically in the number of action dimensions. In our experiments, the GAC implementation as provided by the authors can be up to 400 times slower than SPU in Mujoco environments with high-dimensional action spaces such as "Humanoid-v2". We also note that GAC uses past trajectories to update the policy parameters while SPU only uses the trajectories generated by the most recent policy.

Clipped-PPO (Schulman et al., 2017) takes a very different approach to TRPO. In the process of going
from k to k+1 , PPO makes many gradient steps while only using the data from k . Without the clipping, PPO is simply the approximation (8). The additional clipping is analogous to the constraint
(2) in that it has the goal of keeping  close to k . Indeed, the clipping can be seen as an attempt at keeping (at|st) from becoming neither much larger than (1 + )k (at|st) nor much smaller than (1 - )k (at|st). Thus, although the clipped PPO objective does not squarely fit into the optimization framework (1)-(2), it is quite similar in spirit.

Actor-Critic using Kronecker-Factored Trust Region (ACKTR) (Wu et al., 2017) proposed using Kronecker-factored approximation curvature (K-FAC) to update both the policy gradient and critic terms, giving a more computationally efficient method of calculating the natural gradients. ACER (Wang et al., 2016a) exploits past episodes, linearizes the KL divergence constraint, and maintains an average policy network to enforce the KL divergence constraint. In future work, it would of interest to extend the SPU methodology to handle past episodes.

3

Under review as a conference paper at ICLR 2019

4 SPU FRAMEWORK

The SPU methodology has two steps. In the first step, for a given constraint criterion (, k )  , we find the optimal solution to the non-parameterized problem:

maximize


Lk ()

(9)

subject to (, k )  

(10)

Note that  is not restricted to the set of parameterized policies DL. Also note, as is common practice, we are using an approximation for the objective function. Specifically, we are using the
approximation (8). However, unlike PPO/TRPO, we are not approximating the constraint (2). We will show below the optimal solution  for the non-parameterized problem (9)-(10) can be determined nearly in closed form for many natural constraint criteria (, k )  .

In the second step of the SPU methodology, we attempt to find a policy  in the parameterized space DL that is close to the target policy . Specifically, to advance from k to k+1 we perform the following steps:

(i) We first sample N trajectories using policy k , giving sample data (si, ai, Ai), i = 1, .., m. Here Ai is again an estimate of the advantage value Ak (si, ai), which is obtained from an auxiliary critic network. (For notational simplicity, we henceforth index the samples with i rather than with (i, t) corresponding to the tth sample in the ith trajectory.)
(ii) For each si, we define the target distribution  to be the optimal solution to the constrained optimization problem (9)-(10) for a specific constraint .
(iii) We then fit the policy network  to the target distributions (·|si), i = 1, .., m. Specifically, to find k+1, we use the following loss function:

1m

L() = N

DKL(

)[si]

i=1

(11)

For this supervised learning problem, we do not initialize with random initial weights, but
instead with the weights for k . In minimizing the loss function L(), we can use mini-batch optimization techniques such as RMSprop or Adam. Overfitting to the fresh data (si, ai, Ai), i = 1, .., m can be controlled by dynamic early stopping; see Chapter 7 of (Goodfellow et al.,
2016). The resulting  becomes our k+1.

Thus SPU proceeds by solving a series of supervised learning problems, one for each policy step k.

5 SPU APPLIED TO SPECIFIC PROXIMITY CRITERIA

To illustrate the SPU methodology, for three different but natural types of proximity constraints, we solve the corresponding non-parameterized optimization problem and derive the resulting gradient for the SPU supervised learning problem.

5.1 FORWARD AGGREGATE AND DISAGGREGATE KL CONSTRAINTS

We first consider constraint criteria of the form:

maximize

subject to

dk (s) E [Ak (s, a)]
a(·|s) s
dk (s)DKL( k )[s]  
s
DKL( k )[s]  for all s

(12) (13) (14)

Note that this problem is equivalent to minimizing Lk () subject to the constraints (13) and (14). We refer to (13) as the "aggregated KL constraint" and to (14) as the "disaggregated KL constraint".
These two constraints taken together restrict  from deviating too much from k . Note that this problem (minus the disaggregated constraints) is analogous to the NPG/TRPO problem, which was

4

Under review as a conference paper at ICLR 2019

solved in the parameterized space with an additional approximation for the aggregated constraint. We shall refer to (12)-(14) as the forward-KL non-parameterized optimization problem.

For

each



>

0,

define:

(a|s)

=

k (a|s) eAk (s,a)/ Z(s)

where

Z(s)

is

defined

so

that

(·|s)

is

a valid distribution. Further, for each s, let s be the solution to DKL( k )[s] = . Also let

 = {s : DKL( k )[s]  }.

Theorem 1 The optimal solution to the problem (12)-(14) is given by:

~(a|s) =

(a|s) s (a|s)

s   s / 

(15)

where  is chosen so that s dk (s)DKL(~ k )[s] =  (Proof in subsection A.1).

Equation (15) provides the structure of the optimal non-parameterized policy. As part of the SPU framework, we then seek a parameterized policy  that is close to ~(a|s), that is, minimizes the
loss function (11). A straightforward calculation shows (Appendix B):

 DKL (

~)[si] = DKL(

k )[si]

-

1 ~si

E [
a (·|si)

log (a|si)Ak (si, a)]

(16)



where si =  for expectation in (16).

si   For this,

and we

~si use

= the

si for sample

si ai

/ 

. k

In practice, we will need to estimate (·|si) and approximate Ak (si, ai) as

the Ai

(obtained from the critic network), giving:

 DKL (

~)[si]  DKL(

k )[si]

-

1

si

  (ai |si ) k (ai|si)

Ai

(17)

In order to simplify the algorithm, we slightly modify the gradient update given by (17). We replace

the hyper-parameter  with the hyper-parameter  and tune  rather than . Further, we set si =  for all si in (17) but introduce per-state acceptance to account for the disaggregated constraints,
resulting in the following approximate gradient:

 DKL (

~)  1 m

m
[ DKL (

i=1

k )[si] -

1 

  (ai |si ) k (ai|si)

1Ai] DKL(

k )[si]

(18)

We use (18) as our gradient for supervised training of the policy network. The equation (18) has an

intuitive interpretation: the gradient represents a trade-off between the approximate performance of



(as

captured

by

1 

  (ai |si k (ai|si)

)

Ai

)

and

how

far



diverges

from

k

(as

captured

by

 DKL (

1

k )[si]). For the stopping criterion, we train until m i DKL( k )[si]  .

5.2 BACKWARD KL CONSTRAINT

In a similar manner, we can derive the structure of the optimal policy when using the reverse KL-divergence as the constraint. For simplicity, we provide the result for when there are only disaggregated constraints. We seek to find the non-parameterized optimal policy by solving:

maximize


dk (s) E [Ak (s, a)]
a(·|s) s
DKL( k )[s]  for all s

(19) (20)

Theorem 2 The optimal solution to the problem (19)-(20) is given by:

(a|s)

=

k (a|s)



(s)

(s) - Ak

(s,

a)

where (s) > 0 and  (s) > maxa Ak (s, a) (Proof in subsection A.2).

(21)

5

Under review as a conference paper at ICLR 2019

Note that the structure of the optimal policy with the backward KL constraint is quite different from that with the forward KL constraint. A straight forward calculation shows (Appendix B):

 DKL (

)[s] = DKL(

k

)[s]

-

E
ak

(a|s) log k (a|s)

1  (s) - Ak (s, a)
(22)

The equation (22) has an intuitive interpretation. It increases the probability of action a if Ak (s, a) >  (s) - 1 and decreases the probability of action a if Ak (s, a) <  (s) - 1. (22) also tries to keep  close to k by minimizing their KL divergence.
5.3 L CONSTRAINT

In this section we show how a PPO-like objective can be formulated in the context of SPU. Recall
from Section 3 that the the clipping in PPO can be seen as an attempt at keeping (ai|si) from becoming neither much larger than (1 + )k (ai|si) nor much smaller than (1 - )k (ai|si) for i = 1, . . . , m. In this subsection, we consider the constraint function

(,

k )

=

max
i=1,...,m

|

(ai

|si) - k (ai k (ai|si)

|si

)|

which leads us to the following optimization problem:

(23)

maximize
 (a1 |s1 ),..., (am |sm )
subject to

m i=1

Ak

(si,

ai)

(ai|si) k (ai |si )

(ai|si) - k (ai|si)  k (ai|si)
m (ai|si) - k (ai|si) i=1 k (ai|si)

i = 1, . . . , m
2


(24) (25) (26)

Note that here we are using a variation of the SPU methodology described in Section 4 since here we first create estimates of the expectations in the objective and constraints and then solve the optimization problem (rather than first solve the optimization problem and then take samples as done for Theorems 1 and 2). Note that we have also included an aggregated contraint (26) in addition to the PPO-like constraint (25), which further ensures that the updated policy is close to k .

Theorem 3 The optimal solution to the optimization problem (24-26) is given by:

(ai|si) =

k (ai|si) min{1 + Ai, 1 + } Ai  0 k (ai|si) max{1 + Ai, 1 - } Ai < 0

for some  > 0 where Ai Ak (si, ai) (Proof in subsection A.3).

(27)

To simplify the algorithm, we treat  as a hyper-parameter rather than . After solving for , we seek

a parameterized policy  that is close to  by minimizing their mean square error over sampled

states and actions, i.e. by updating  in the negative direction of  refer to this approach as SPU with the L constraint.

i((ai|si) - (ai|si))2. We

6 EXPERIMENTAL RESULTS
We provide extensive experimental results which demonstrate SPU outperforms recent state-of-the-art methods for environments with continuous or discrete action spaces. We also provide ablation studies to show the importance of the different algorithmic components, and a sensitivity analysis to show that SPU's performance is relatively insensitive to hyper-parameter choices. There are two definitions of superior sample complexity performance that we use to compare two different algorithms A and B: (i) A is better than B if A takes fewer interactions with the environment to achieve a pre-defined performance threshold (Kakade, 2003); (ii) A is better than B if the averaged final performance of A is higher than that of B given the same number of interactions with the environment (Schulman et al., 2017). Implementation details are provided in Appendix D.

6

Under review as a conference paper at ICLR 2019
6.1 RESULTS ON MUJOCO The Mujoco (Todorov et al., 2012) simulated robotics environments provided by OpenAI gym (Brockman et al., 2016) have become a popular benchmark for control problems with continuous action spaces. In terms of final performance averaged over all available ten Mujoco environments and ten different seeds, SPU with L constraint (Section 5.3) and SPU with forward KL constraints (Section 5.1) outperform TRPO by 6% and 27% respectively. Since the forward-KL approach is our best performing approach, we focus subsequent analysis on it and hereafter refer to it as SPU. Figure 1 illustrates the performance of SPU versus TRPO for each of the ten Mujoco environments. We observe that SPU clearly outperforms TRPO throughout training in the first 5 environments while being roughly the same as TRPO in the other 5. We also note that SPU significantly outperforms TRPO on the 3 most challenging Mujoco environments, which are shown in the top left corner in Figure 1. Algorithm 1 in the Appendix provides the complete description of the SPU algorithm.
Figure 1: Performance of SPU versus TRPO on 10 Mujoco environments in 1 million timesteps. The x-axis indicates timesteps. The y-axis indicates the average episode reward of the last 100 episodes. To ensure that SPU is not only better than TRPO in terms of performance gain early during training, we further retrain both policies for 3 million timesteps. Again here, SPU outperforms TRPO by 28%. Figure 3 in the Appendix illustrates the performance for each environment. 6.2 ABLATION STUDIES FOR MUJOCO The indicator variable in (18) enforces the disaggregated constraint. We refer to it as per-state acceptance. Removing this component is equivalent to removing the indicator variable. We refer to using i DKL( k )[si] to determine the number of training epochs as dynamic stopping. Without this component, the number of training epochs is treated as a hyper-parameter. We also tried removing DKL( k )[si] from the gradient update step in (18). Table 1 illustrates the contribution of the different components of SPU to the overall performance. The third row shows that the term DKL( k )[si] makes a crucially important contribution to SPU. Furthermore, per-state acceptance and dynamic stopping are both also important for obtaining high performance, with the former playing a more central role. When a component is removed, the hyper-parameters are retuned to ensure that the best possible performance is obtained with the alternative (simpler) algorithm.
7

Under review as a conference paper at ICLR 2019

Table 1: Ablation study for SPU

Approach
Original Algorithm No grad KL
No dynamic stopping No per-state acceptance

Percentage better than TRPO
27% 4% 24% 9%

Performance vs. original algorithm
0% - 85% - 11% - 67%

6.3 SENSITIVITY ANALYSIS ON MUJOCO
To demonstrate the practicality of SPU, we show that its high performance is insensitive to hyperparameter choice. One way to show this is as follows: for each SPU hyper-parameter, select a reasonably large interval, randomly sample the value of the hyper parameter from this interval, and then compare SPU (using the randomly chosen hyper-parameter values) with TRPO. We sampled 100 SPU hyper-parameter vectors (each vector including , , ), and for each one determined the relative performance with respect to TRPO. First, we found that for all 100 random hyper-parameter value samples, SPU performed better than TRPO. 75% and 50% of the samples outperformed TRPO by at least 18% and 21% respectively. The full CDF is given in Figure 4 in the Appendix. We can conclude that SPU's superior performance is largely insensitive to hyper-parameter values.
6.4 RESULTS ON ATARI
It has recently been observed that neural networks are not always needed to obtain high performance in many Mujoco environments (Rajeswaran et al., 2017; Mania et al., 2018). To conclusively evaluate SPU, we compare it against state-of-the-art method on the Arcade Learning Environments (Bellemare et al., 2012) exposed through OpenAI gym (Brockman et al., 2016). Using the same network architecture and hyper-parameters, we learn to play 60 Atari games from raw pixels and rewards. This is highly challenging because of the diversity in the games and the high dimensionality of the observations.
Here, we compare SPU against PPO (Schulman et al., 2017) since PPO achieves comparable performance with TRPO in our experiments on Mujoco with much faster run-time (Appendix F.3). We also argue that PPO is a high-performing algorithm since it has been used to learn dexterous movements for robot arms (Marcin Andrychowicz, 2018) and has beat top human players in DOTA (Michael Petrov, 2018). Averaged over 60 Atari environments and 20 seeds, SPU is 55% better than PPO in terms of averaged final performance. Figure 2 provides a high-level overview of the result, from which we can draw two conclusions: (i) In 36 environments, SPU and PPO perform roughly the same (within 20% of each other); SPU clearly outperforms PPO in 15 environments while PPO clearly outperforms SPU in 9; (ii) In those 15+9 environments, the extent to which SPU outperforms PPO is much larger than the extent to which PPO outperforms SPU. Figure 6, Figure 7 and Figure 8 in the Appendix illustrate the performance of SPU vs PPO throughout training. SPU's high performance in both the Mujoco and Atari domains demonstrates its high performance and generality.

Figure 2: High-level overview of results on Atari 8

Under review as a conference paper at ICLR 2019
REFERENCES
Joshua Achiam. Advanced policy gradient methods. http://rll.berkeley.edu/ deeprlcourse/f17docs/lecture_13_advanced_pg.pdf, 2017.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International Conference on Machine Learning, pp. 22­31, 2017.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251­276, 1998.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. CoRR, abs/1207.4708, 2012. URL http://arxiv.org/abs/1207.4708.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/ abs/1606.01540.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/ openai/baselines, 2017.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. CoRR, abs/1604.06778, 2016. URL http: //arxiv.org/abs/1604.06778.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.
Sham Kakade. On the sample complexity of reinforcement learning. PhD thesis, University of London London, England, 2003.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In ICML, volume 2, pp. 267­274, 2002.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems, pp. 1531­1538, 2002.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Sergey Levine. UC Berkeley CS294 deep reinforcement learning lecture notes. http://rail. eecs.berkeley.edu/deeprlcourse-fa17/index.html, 2017.
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055, 2018.
Maciek Chociej Rafal Jozefowicz Bob McGrew Jakub Pachocki Arthur Petron Matthias Plappert Glenn Powell Alex Ray Jonas Schneider Szymon Sidor Josh Tobin Peter Welinder Lilian Weng Wojciech Zaremba Marcin Andrychowicz, Bowen Baker. Learning dexterous in-hand manipulation. 2018. URL https://arxiv.org/abs/1808.00177.
Filip Wolski Henrique Ponde Susan Zhang Rafal Jozefowicz Jakub Pachocki Przemyslaw Debiak Jonathan Raiman Christy Dennison David Farhi Jie Tang Greg Brockman Szymon Sidor Alec Radford Larissa Schiavo Christopher Berner Diane Yoon Jonas Schneider Scott Gray Shariq Hashme Jack Clark Christopher Hesse John Schulman David Luan Quirin Fischer Paul Christiano Ilya Sutskever Eric Sigler Michael Petrov, Brooke Chan. Openai five. 2018. URL https: //blog.openai.com/openai-five/.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518, 2015. URL http://dx.doi.org/10.1038/nature14236.
9

Under review as a conference paper at ICLR 2019
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180­1190, 2008a. Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
networks, 21(4):682­697, 2008b. Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham Kakade. Towards generalization
and simplicity in continuous control. CoRR, abs/1703.02660, 2017. URL http://arxiv. org/abs/1703.02660. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889­1897, 2015a. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b. John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. CoRR, abs/1506.02438, 2015c. URL http://arxiv.org/abs/1506.02438. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. David Silver. UCL course on reinforcement learning lecture notes. http://www0.cs.ucl.ac. uk/staff/d.silver/web/Teaching.html, 2015. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000. Voot Tangkaratt, Abbas Abdolmaleki, and Masashi Sugiyama. Guide actor-critic for continuous control. In International Conference on Learning Representations, 2018. URL https:// openreview.net/forum?id=BJk59JZ0b. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. 2012. URL https://ieeexplore.ieee.org/abstract/document/6386109/ authors. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Rémi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. CoRR, abs/1611.01224, 2016a. URL http://arxiv.org/abs/1611.01224. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016b. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pp. 5­32. Springer, 1992. Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems, pp. 5285­5294, 2017.
10

Under review as a conference paper at ICLR 2019

Appendices

A PROOFS FOR NON-PARAMETERIZED OPTIMIZATION PROBLEMS

A.1 FORWARD KL AGGREGATED AND DISAGGREGATED CONSTRAINTS

We first show that (12)-(14) is a convex optimization. To this end, first note that the objective (12) is a linear function of the decision variables  = {(a|s) ´: s  S, a  A}. The LHS of (14) can be rewritten as: aA (a|s) log (a|s) - aA (a|s) log k (a|s). The second term is a linear function of . The first term is a convex function since the second derivative of each summand is always positive. The LHS of (14) is thus a convex function. By extension, the LHS of (13) is also a convex function since it is a nonnegative weighted sum of convex functions. The problem (12)-(14) is thus a convex optimization problem. According to Slater's constraint qualification, strong duality holds since k is a feasible solution to (12)-(14) where the inequality holds strictly.
We can therefore solve (12)-(14) by solving the related Lagrangian problem. For a fixed  consider:

maximize

subject to

dk (s){ E [Ak (s, a)] - DKL(
a(·|s) s
DKL( k )[s]  for all s

k )[s]}

The above problem decomposes into separate problems, one for each state s:

maximize
(·|s)
subject to

E [Ak (s, a)] - DKL(
a(·|s)
DKL( k )[s] 

k )[s]

Further consider the unconstrained problem (30) without the constraint (31):

(28) (29)
(30) (31)

maximize
(·|s)
subject to

K
(a|s) Ak (s, a) -  log
a=1 K
(a|s) = 1
a=1
(a|s)  0, a = 1, . . . , K

(a|s) k (a|s)

(32)
(33) (34)

A simple Lagrange-multiplier argument shows that the opimal solution to (32)-(34) is given by:

(a|s) = k (a|s) eAk (s,a)/ Z(s)

where Z(s) is defined so that (·|s) is a valid distribution. Now returning to the decomposed constrained problem (30)-(31), there are two cases to consider. The first case is when DKL( k )[s]  . In this case, the optimal solution to (30)-(31) is (a|s). The second case is when DKL( k )[s] > . In this case the optimal is (a|s) with  replaced with s, where s is the solution to DKL( k )[s] = . Thus, an optimal solution to (30)-(31) is given by:


 

k

(a|s)

eAk

(s,a)/

~(a|s)

=

 Z(s)

 

k

(a|s)

eAk

(s,a)/s

 Z(s)

s   s / 

(35)

where  = {s : DKL( k )[s]  }. To find the Lagrange multiplier , we can then do a line search to find the  that satisfies:

dk (s)DKL(~ k )[s] = 
s

(36)

11

Under review as a conference paper at ICLR 2019

A.2 BACKWARD KL CONSTRAINT

The problem (19)-(20) decomposes into separate problems, one for each state s  S:

maximize
(·|s)
subject to

E
ak (·|s)

(a|s) Ak (s, a) k (a|s)

E
ak (·|s)

log k (a|s) (a|s)



After some algebra, we see that above optimization problem is equivalent to:

(37) (38)

maximize
(·|s)
subject to

K
Ak (s, a)(a|s)
a=1 K
- k (a|s) log (a|s) 
a=1 K
(a|s) = 1
a=1
(a|s)  0, a = 1, . . . , K

(39) (40) (41) (42)

where = + entropy(k ). (39)-(42) is a convex optimization problem with Slater's condition holding. Strong duality thus holds for the problem (39)-(42). Applying standard Lagrange multiplier
arguments, it is easily seen that the solution to (39)-(42) is

(a|s)

=

k (a|s)



(s)

(s) - Ak

(s,

a)

where (s) and  (s) are constants chosen such that the disaggregegated KL constraint is binding and the sum of the probabilities equals 1. It is easily seen (s) > 0 and  (s) > maxa Ak (s, a)

A.3 L CONSTRAINT

The problem (24-26) is equivalent to:

maximize
 (a1 |s1 ),..., (am |sm )
subject to

m i=1

Ak

(si,

ai)

(ai|si) k (ai |si )

1 -  (ai|si)  1 + k (ai|si)
m (ai|si) - k (ai|si) i=1 k (ai|si)

i = 1, . . . , m
2


(43) (44) (45)

This problem is clearly convex. k (ai|si), i = 1, . . . , m is a feasible solution where the inequality constraint holds strictly. Strong duality thus holds according to Slater's constraint qualification. To
solve (43)-(45), we can therefore solve the related Lagrangian problem for fixed :

maximize
 (a1 |s1 ),..., (am |sm )
subject to

m i=1

Ak

(si

,

ai

)

(ai|si) k (ai |si )

-



(ai|si) - k (ai|si) k (ai|si)

1 -  (ai|si)  1 + k (ai|si)

i = 1, . . . , m

which is separable and decomposes into m separate problems, one for each si:

maximize
 (ai |si )
subject to

Ak

(si

,

ai

)

(ai|si) k (ai |si )

-



1 -  (ai|si)  1 + k (ai|si)

(ai|si) - k (ai|si) k (ai|si)

2

2

(46) (47)
(48) (49)

12

Under review as a conference paper at ICLR 2019

The solution to the unconstrained problem (48) without the constraint (49) is:

(ai|si) = k (ai|si)

1 + Ak (si, ai) 2

Now consider the constrained problem (48)-(49). ), the optimal solution is k (ai|si)(1 + ).

If Ak (si, Similarly,

ai If

)0 Ak

and (si,

(ai|si) ai) < 0

a>ndk((aaii||ssii))(1<+

k (ai|si)(1 - ), the optimal solution is k (ai|si)(1 - ). Rearranging the terms gives Theorem 3.

To obtain , we can perform a line search over  so that the constraint (45) is binding.

B DERIVATIONS THE GRADIENT OF LOSS FUNCTION FOR SPU

Let CE stands for CrossEntropy.

B.1 FORWARD-KL

CE(||~~s )[s] = - (a|s) log ~~s (a|s)
a

= - (a|s) log
a

k (a|s) eAk (s,a)/~s Z~ s (s)

= - (a|s) log
a

k (a|s) Z~ s (s)

Ak (s, a)

- (a|s)
a

~s

=-

a

(a|s) log k (a|s) +

a

1 (a|s) log Z~s (s) - ~s

a

k

(a|s)

 (a|s) k (a|s)

Ak

(s,

a)

1

=

CE(||k )[s]

+

log Z~s (s)

-

~s

E
ak

(.|s)

(a|s) Ak (s, a) k (a|s)



CE(||~~s )[s]

=

CE(||k )[s]

-

1

~s

E
ak

(.|s)

(a|s) Ak (s, a) k (a|s)

 DKL(

~~s )[s] = DKL(

k )[s]

-

1

~s

E
ak

(.|s)

(a|s) Ak (s, a) k (a|s)

B.2 REVERSE-KL

C E ( || )[s]

= - (a|s) log (a|s)
a

(s) = - (a|s) log k (a|s)  (s) - Ak (s, a)
a

= - (a|s) log k (a|s) - (a|s) log (s) + (a|s) log( (s) - Ak (s, a))
a aa

= CE(||k )[s] - (s) + Eak

(a|s) log( (s) - Ak (s, a)) k (a|s)

= CE(||k )[s] - (s) - Eak

 (a|s) k (a|s)

log



(s)

-

1 Ak

(s,

a)

 CE(||)[s] = CE(||k )[s] - Eak

  (a|s) k (a|s)

log



(s)

-

1 Ak

(s,

a)

 DKL(

)[s] = DKL(

k )[s] - Eak

  (a|s) k (a|s)

log



(s)

-

1 Ak

(s,

a)

13

Under review as a conference paper at ICLR 2019

C EXTENSION TO CONTINUOUS STATE AND ACTION SPACES

The methodology developed in the body of this paper also applies to continuous state and action spaces. In this section, we outline the modifications that are necessary for the continuous case.

We first modify the definition of d(s) becomes a density function over the state

by replacing P space. With this

(st

=

s)

with

d ds

modification, the

P(st  definition

s) of

so that D¯ KL(

d (s) k )

and the approximation (8) are unchanged. The SPU framework described in Section 4 is also

unchanged.

Consider now the non-parameterized optimization problem with aggregate and disaggregate constraints (12-14), but with continuous state and action space:

maximize


dk (s) E [Ak (s, a)]ds
a(·|s)

subject to

dk (s)DKL( k )[s]ds  

DKL( k )[s]  for all s

(50)
(51) (52)

Theorem 1 holds although its proof needs to be slightly modified as follows. It is straightforward to show that (50-52) remains a convex optimization problem. We can therefore solve (50-52) by solving the Lagrangian (28-29) with the sum replaced with an integral. This problem again decomposes with separate problems for each s  S giving exactly the same equations (30-31). The proof then proceeds as in the remainder of the proof of Theorem 1.
Theorem 2 and 3 are also unchanged for continuous action spaces. Their proofs require slight modifications, as in the proof of Theorem 1.

D IMPLEMENTATION DETAILS AND HYPERPARAMETERS
D.1 MUJOCO
As in (Schulman et al., 2017), for Mujoco environments, the policy is parameterized by a fullyconnected feed-forward neural network with two hidden layers, each with 64 units and tanh nonlinearities. The policy outputs the mean of a Gaussian distribution with state-independent variable standard deviations, following (Schulman et al., 2015a; Duan et al., 2016). The action dimensions are assumed to be independent. The probability of an action is given by the multivariate Gaussian probability distribution function. The baseline used in the advantage value calculation is parameterized by a similarly sized neural network, trained to minimize the MSE between the sampled states TD- returns and the their predicted values. For both the policy and baseline network, SPU and TRPO use the same architecture. To calculate the advantage values, we use Generalized Advantage Estimation (Schulman et al., 2015c). States are normalized by dividing the running mean and dividing by the running standard deviation before being fed to any neural networks. The advantage values are normalized by dividing the batch mean and dividing by the batch standard deviation before being used for policy update. The TRPO result is obtained by running the TRPO implementation provided by OpenAI (Dhariwal et al., 2017), commit 3cc7df060800a45890908045b79821a13c4babdb. At every iteration, SPU collects 2048 samples before updating the policy and the baseline network. For both networks, gradient descent is performed using Adam (Kingma & Ba, 2014) with step size 0.0003, minibatch size of 64. The step size is linearly annealed to 0 over the course of training.  and  for GAE (Schulman et al., 2015c) are set to 0.99 and 0.95 respectively. For SPU, , ,  and the maximum number of epochs per iteration are set to 1.2, 0.05, 1.3 and 30 respectively. Training is performed for 1 million timesteps for both SPU and PPO. In the sensitivity analysis, the ranges of values for the hyper-parameters , ,  and maximum number of epochs are [0.05, 0.07], [0.01, 0.07], [1.0, 1.2] and [5, 30] respectively.
14

Under review as a conference paper at ICLR 2019

D.2 ATARI
Unless otherwise mentioned, the hyper-parameter values are the same as in subsection D.1. The policy is parameterized by a convolutional neural network with the same architecture as described in Mnih et al. (2015). The output of the network is passed through a relu, linear and softmax layer in that order to give the action distribution. The output of the network is also passed through a different linear layer to give the baseline value. States are normalized by dividing by 255 before being fed into any network. The TRPO result is obtained by running the PPO implementation provided by OpenAI (Dhariwal et al., 2017), commit 3cc7df060800a45890908045b79821a13c4babdb. 8 different processes run in parallel to collect timesteps. At every iteration, each process collects 256 samples before updating the policy and the baseline network. Each process calculates its own update to the network's parameters and the updates are averaged over all processes before being used to update the network's parameters. Gradient descent is performed using Adam (Kingma & Ba, 2014) with step size 0.0001. In each process, random number generators are initialized with a different seed according to the formula process_seed = experiment_seed + 10000  process_rank. Training is performed for 10 million timesteps for both SPU and PPO. For SPU, , ,  and the maximum number of epochs per iteration are set to 0.02, /1.3, 1.1 and 9 respectively.

E ALGORITHMIC DESCRIPTION FOR SPU

Algorithm 1 Algorithmic description of forward-KL non-parameterized SPU

Require: A neural net  that parameterizes the policy. Require: A neural net V that approximates V  . Require: General hyperparameters: ,  (advantage estimation using GAE),  (learning rate), N

(number of trajectory per iteration), T (size of each trajectory), M (size of training minibatch).

Require: Algorithm-specific hyperparameters:  (aggregated KL constraint), (disaggregated con-

straint), ,  (max number of epoch).

1: for k = 1, 2, . . . do

2: under policy k , sample N trajectories, each of size T (sit, ait, rit, si(t+1)), i = 1, . . . , N, t = 1, . . . , T

3: Using any advantage value estimation scheme, estimate Ait, i = 1, . . . , N, t = 1, . . . , T

4:   k

5:   k

6: for  epochs do

7: Sample M samples from the N trajectories, giving {s1, a1, A1, . . . , sM , aM , AM }

8:

L()

=

1 M

(V targ(sm) - V(sm))2
m

9:    - L()

10:

L()

=

1 M

 DKL (

m

k )[sm]

-

1 

  (am |sm k (am|sm)

)

Am

1DKL( k )[sm]

11:    - L()

1

12:

if m

m DKL( k )[sm] >  then

13: Break out of for loop

14: k+1   15: k+1  

F EXPERIMENTAL RESULTS
F.1 RESULTS ON MUJOCO FOR 3 MILLION TIMESTEPS
TRPO and SPU were trained for 1 million timesteps to obtain the results in section 6. To ensure that SPU is not only better than TRPO in terms of performance gain early during training, we further retrain both policies for 3 million timesteps. Again here, SPU outperforms TRPO by 28%. Figure 3 illustrates the performance on each environment.
15

Under review as a conference paper at ICLR 2019
Figure 3: Performance of SPU versus TRPO on 10 Mujoco environments in 3 million timesteps. The x-axis indicates timesteps. The y-axis indicates the average episode reward of the last 100 episodes. F.2 SENSITIVITY ANALYSIS CDF FOR MUJOCO When values for SPU hyper-parameter are randomly sampled as is explained in subsection 6.3, the percentage improvement of SPU over TRPO becomes a random variable. Figure 4 illustrates the CDF of this random variable.
Figure 4: Sensitivity Analysis for SPU 16

Under review as a conference paper at ICLR 2019 F.3 PPO RESULT ON MUJOCO We provides results for PPO versus TRPO to demonstrates that both have similar performance in Mujoco. In each environments, the results are averaged over 10 seeds. In terms of averaged final performance, PPO outperforms TRPO by 9%.
Figure 5: Performance of PPO versus TRPO on 10 Mujoco environments in 1 million timesteps. The x-axis indicates timesteps. The y-axis indicates the average episode reward of the last 100 episodes. F.4 ATARI RESULTS Figure 6, Figure 7 and Figure 8 illustrate the performance of SPU vs PPO throughout training in 60 Atari games.
17

Under review as a conference paper at ICLR 2019
Figure 6: Atari results (Part 1) for SPU vs PPO. The x-axis indicates timesteps. The y-axis indicates the average episode reward of the last 100 episodes.
18

Under review as a conference paper at ICLR 2019
Figure 7: Atari results (Part 2) for SPU vs PPO. The x-axis indicates timesteps. The y-axis indicates the average episode reward of the last 100 episodes.
19

Under review as a conference paper at ICLR 2019
Figure 8: Atari results (Part 3) for SPU vs PPO. The x-axis indicates timesteps. The y-axis indicates the average episode reward of the last 100 episodes.
20

