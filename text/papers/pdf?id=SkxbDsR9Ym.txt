Under review as a conference paper at ICLR 2019
RELWALK ­ A LATENT VARIABLE MODEL APPROACH TO KNOWLEDGE GRAPH EMBEDDING
Anonymous authors Paper under double-blind review
ABSTRACT
Knowledge Graph Embedding (KGE) is the task of jointly learning entity and relation embeddings for a given knowledge graph. Existing methods for learning KGEs can be seen as a two-stage process where (a) a scoring function is defined that evaluates the strength of a relation that holds between two entities using the corresponding relation and entity embeddings, and (b) the entity and relation embeddings that optimise the scoring function are learnt. Unfortunately, prior proposals for the scoring functions in the first step have been heuristically motivated, and it is unclear as to how the scoring functions in KGEs relate to the generation process of the underlying knowledge graph. To address this issue, we propose a generative account of the KGE learning task. Specifically, given a knowledge graph represented by a set of relational triples (h, R, t), where the semantic relation R holds between the two entities h and t, we extend the random walk model (Arora et al., 2016a) of word embeddings to KGE. We derive a theoretical relationship between the joint probability p(h, R, t) and the embeddings of h, R and t. Moreover, we show that marginal loss minimisation, a popular objective used by much prior work in KGE, follows naturally from the log-likelihood ratio maximisation under the probabilities estimated from the KGEs according to our theoretical relationship. Our goal in this paper is not to propose a state-of-the-art KGE method, but to provide a theoretical understanding of the otherwise heuristically derived score functions. However, the KGEs learnt according to the theoretically-derived scoring function reports strong results on several standard benchmark datasets, providing empirical evidence to support the theory.
1 INTRODUCTION
Knowledge graphs such as Freebase (Bollacker et al., 2008) organise information in the form of graphs, where entities are represented by vertices in the graph and the relation between two entities is represented by the edge that connects the corresponding two vertices. By embedding entities and relations that exist in a knowledge graph in some (possibly lower-dimensional and latent) space we can infer previously unseen relations between entities, thereby expanding a given knowledge graph (Nickel et al., 2016; Yang et al., 2015; Lin et al., 2015; Nickel et al., 2011; Trouillon et al., 2016; Wang et al., 2017; Bordes et al., 2011).
Existing KGE methods can be seen as involving two main steps. First, given a knowledge graph represented by a set of relational triples (h, R, t), where a semantic relation R holds between a head entity h and a tail entity t, entities and relations are represented using some mathematical structures such as vectors, matrices or tensors. Second, a scoring function is proposed that evaluates the relational strength of a triple (h, R, t) and entity and relation embeddings that optimise the defined scoring function are learnt using some optimisation method. Table 1 shows some of the scoring functions proposed in prior work in KGE learning.
Despite the wide applications of entity and relation embeddings created via KGE methods, the existing scoring functions are motivated heuristically to capture some geometric requirements of the embedding space. For example, TransE (Bordes et al., 2011) assumes that entity and relation embeddings co-exist in the same (possibly lower dimensional) vector space and translating (shifting) the head entity embedding by the relation embedding must make it closer to the tail entity embedding, whereas ComplEx (Trouillon et al., 2016) models the asymmetry in relations using
1

Under review as a conference paper at ICLR 2019

Model
Unstructured (Bordes et al., 2011) Structured embeddings (Bordes et al., 2011) TransE (Bordes et al., 2011) DistMult (Yang et al., 2015) RESCAL (Nickel et al., 2011) ComplEx (Trouillon et al., 2016)

Score function f (h, R, t)
h-t 1/2
R1h - R2t 1,2 h+R-t
1/2
h, R, t
h Rt h, R, ¯t

Relation parameters
none R1, R2  Rd×d R  Rd R  Rd Rd×d R  Cd

Table 1: Score functions proposed in selected prior work on KGE. Entity embeddings h, t  Rd are vectors in all models, except in ComplEx where h, t  Cd. Here, x 1/2 denotes either 1 or 2 norm of the vector x. In ComplEx, x¯ is the elementwise complex conjugate, and ·, ·, · denotes the
component-wise multi-linear inner-product.

the component-wise multi-linear inner-product among entity and relation embeddings. Relational triples extracted from a given knowledge graph are used as positive training instances, whereas pseudo-negative (Bordes et al., 2011) instances are automatically generated by randomly corrupting positive instances. Finally, KGE are learnt such that the prediction loss computed over the positive and negative instances is minimised.
Despite the good empirical performances of the existing KGE methods, theoretical understanding of KGE methods is comparatively under developed. For example, it is not clear how the heuristically defined KGE objectives relate to the generative process of a knowledge graph. In this paper, we attempt to fill this void by providing a theoretical analysis of KGE. Specifically, in section 2, we propose a generative process where we explain the formation of a relation R between two entities h and t using the corresponding relation and entity embeddings. Following this generative story, we derive a relationship between the probability of R holding between h and t, p(h, t | R), and the embeddings of R, h and t. Interestingly, the derived relationship is not covered by any of the previously proposed heuristically-motivated scoring functions, providing the first-ever KGE method with a provable generative explanation.
Next, in section 3, we show that the margin loss, which has been popularly used as a training objective in prior work on KGE, naturally arises as the log-likelihood ratio computed from p(h, t | R). Based on this result, we derive a training objective that we subsequently optimise for learning KGEs that satisfy our theoretical relationship. Using standard benchmark datasets proposed in prior work on KGE learning, we evaluate the learnt KGEs on a link prediction task and a triple classification task. Experimental results show that the learnt KGEs accurately encode knowledge in a given knowledge graph, thereby providing empirical evidence to the theoretical analysis. We emphasise here that our goal is not to propose state-of-the-art KGE methods but, for the best of our knowledge, the first-ever generative account of KGE learning. We hope that our work will inspire the research community to provide systematic derivations of objective functions for learning KGE in the future, without having to rely on ad hoc heuristics.
2 RELATIONAL WALK
Let us consider a knowledge graph D where the knowledge is represented by relational triples (h, R, t)  D. Here, R is a relational predicate of two arguments, where h (head) and t (tail) entities respectively filling the first and second arguments. We assume relations to be asymmetric in general. In other words, if (h, R, t)  D then it does not necessarily follow that (t, R, h)  D. The goal of KGE is to learn embeddings (representations) for the relations and entities in the knowledge graph such that the entities that participate in similar relations are embedded closely to each other in the entity embedding space, while at the same time relations that hold between similar entities are embedded closely to each other in the relational embedding space. We call the learnt entity and relation embeddings collectively as KGEs. Following prior work on KGE (Bordes et al., 2011; Trouillon et al., 2016; Yang et al., 2015), we assume that entities and relations are embedded in the same vector space, allowing us to perform linear algebraic operations using the embeddings in the same vector space.
2

Under review as a conference paper at ICLR 2019

Let us consider a random walk characterised by a time-dependent knowledge vector ck, where k is the current time step. The knowledge vector represents the knowledge we have about a particular
group of entities and relations that express some facts about the world. For example, the knowledge
that we have about people that are employed by companies can be expressed using entities of classes such as people and organisation, using relations such as CEO-of, employed-at, works-for, etc. We
assume that entities h and t are represented by time-independent d-dimensional vectors, respectively h, t  Rd.

We assume the task of generating a relational triple (h, R, t) in a given knowledge graph to be a two-step process as described next. First, given the current knowledge vector at time k, c = ck and the relation R, we assume that the probability of an entity h satisfying the first argument of R to be given by (1).

p(h | R, c) = 1 exp Zc

h

R1c

.

(1)

Here, R1  Rd×d is a relation-specific orthogonal matrix that evaluates the appropriateness of h for the first argument of R. For example, if R is the CEO-of relation, we would require a person as

the first argument and a company as the second argument of R. However, note that the role of R1 extends beyond simply checking the types of the entities that can fill the first argument of a relation.

For our example above, not all people are CEOs and R1 evaluates the likelihood of a person to be selected as the first argument of the CEO-of relation. Zc is a normalisation coefficient such that
hV p(h | R, c) = 1, where the vocabulary V is the set of all entities in the knowledge graph.1

After generating h, the state of our random walker changes to c = ck+1, and we next generate the second argument of R with the probability given by (2).

1

p(t | R, c ) = Zc

exp

t

R2c

.

(2)

Here, R2  Rd×d is a relation-specific orthogonal matrix that evaluates the appropriateness of t as the second argument of R. Zc is a normalisation coefficient such that cV p(t | R, c) = 1. Following our previous example of the CEO-of relation, R2 evaluates the likelihood of an organisation to be a company with a CEO position. Importantly, R1 and R2 are representations of the relation R and independent of the entities. Therefore, we consider (R1 and R2) to collectively represent the embedding of R. Orthogonality of R1, R2 is a requirement for the mathematical proof and also act as
a regularisation constraint to prevent overfitting by restricting the relational embedding space. We

first perform our mathematical analysis for relational embeddings represented by orthogonal matrices

and discuss later how this requirement can be relaxed.

We assume a slow random walk where the knowledge vectors do not change significantly between consecutive time steps (ck  ck+1). More specifically, we assume that ck - ck+1  2 for some small 2 > 0. This is a realistic assumption for generating the two entity arguments in the same relational triple because, if the knowledge vectors were significantly different in the two generation
steps, then it is likely that the corresponding relations are also different, which would not be coherent
with the above-described generative process. Moreover, we assume that the knowledge vectors are distributed uniformly in the unit sphere and denote the distribution of knowledge vectors by C.

To learn KGEs, we must estimate the probability that h and t satisfy the relation R, p(h, t | R), which can be obtained by taking the expectation of p(h, t | R, c, c ) w.r.t. c, c  C given by (3).

p(h, t | R) = Ec,c [p(h, t | R, c, c )] = Ec,c [p(h | R, c)p(t | R, c )]

(3) (4)

= Ec,c

exp h R1c exp t R2c Zc Zc

.

(5)

Here, partition functions are given by Zc = hV cC exp h R1c and Zc = tV c C exp t R2c . (4) follows from our two-step generative process where the gener-
ation of h and t in each step is independent given the relation and the corresponding knowledge vectors.
1We can consider different vocabularies for the entities that can fill the first argument and second argument of relations in a knowledge graph. However, for simplicity, we use a common vocabulary here.

3

Under review as a conference paper at ICLR 2019

Computing the expectation in (5) is generally difficult because of the two partition functions Zc and Zc . However, Lemma 1 shows that the partition functions are narrowly distributed around a constant value for all c (or c ) values with high probability.

Lemma 1 (Concentration Lemma). If the entity embedding vectors satisfy the Bayesian prior v = sv^, where v^ is from the spherical Gaussian distribution, and s is a scalar random variable, which is always bounded by a constant , then the entire ensemble of entity embeddings satisfies that

for

z

=

 O(1/ n),

and



Pr [(1 -
cC

z)Z  Zc

= exp(-(log2 n)),

 (1 + where n

z)Z]  1 - ,  d is the number

of

words

and

Zc

is

(6) the

partition function for c given by cV exp h R1c .

proof: To prove the concentration lemma, we show that the mean Eh[Zc] of Zc is concentrated around a constant for all knowledge vectors c and its variance is bounded. Recall that

Zc = exp h R1c .

(7)

hV

If P is an orthogonal matrix and x is a vector, then

P

x

2 2

= (P

x)

(P

x) = x

PP

x=

x 22,

because P P = I. Therefore, from (7) and the orthogonality of the relational embeddings, we see

that R1c is a simple rotation of c and does not alter the length of c. We represent h = shh^, where sh = h and h^ is a unit vector (i.e. h^ 2 = 1) distributed on the spherical Gaussian with zero mean and covariance matrix s2Id, where s2 is the variance of the entity embeddings and Id  Rd×d is the
unit matrix. Moreover, let us assume that s is upper bounded by a constant  such that s  . From

the assumption of the knowledge vector c, it is on the unit sphere as well, which is then rotated by

R1.

We can write the partition function using the inner-product between two vectors h and R1c, Zc = hV exp h (R1c) . Arora et al. (2016a) showed that (Lemma 2.1 in their paper) the expectation
of a partition function of this form can be approximated as follows:

Ec[Zc] = nEc[exp h R1c ]

(8)

 nEc[1 + h R1c] = n.

(9)

where n = |V| is the number of entities in the vocabulary. (8) follows from the expectation of a sum

and the independence of h and R1 from c. The inequality of (9) is obtained by applying the Taylor expansion of the exponential series and the final equality is due to the symmetry of the spherical

Gaussian. From the law of total expectation, we can write

Ec[Zc] = nEc[exp h R1c ] = nEsh Ex|sh exp h R1c | sh .

(10)

where, x = h R1c. Note that conditioned on sh, h is a Gaussian random variable with variance 2 = s2h. Therefore, conditioned on sh, x is a random variable with variance 2 = h2. Using this distribution, we can evaluate Ex|sh exp h R1c as follows:

Ex|sh

exp

h R1c

| sh

=

 1 exp x 22

- x2 22

exp(x)dx

(11)

=  1 exp - (x - 2)2 + 2/2 dx

x 22

22

(12)

= exp(2/2).

(13)

Therefore, it follows that

Ec[Zc] = nEsh [exp(2/2)] = nEsh [exp(sh2 /2)] = n exp(s2/2),

(14)

where s is the variance of the 2 norms of the entity embeddings. Because the set of entities is given and fixed, both n and  are constants, proving that E[Zc] does not depend on c.

Next, we calculate the variance Vc[Zc] as follows:

Vc[Zc] = Vc[exp h R1c ]
h

 nEc exp h R1c

= nEsh Ex|sh exp 2h R1t | sh .

(15)

4

Under review as a conference paper at ICLR 2019

Because 2h R1t is a Gaussian random variable with variance 42 = 4sh2 from a similar calculation as in (11) we obtain,

Ex|sh exp 2h R1t | sh = exp(22).

(16)

By substituting (16) in (15) we have that

Vc[Zc]  nEsh exp 22 = nEsh exp(2s2)  n

(17)

for  = exp(82) a constant bounding s   as stated.

From above, we have bounded both the mean and variance of the partition function by constants that are independent of the knowledge vector. Note that neither exp h R1c nor exp t R2c are subGaussian nor sub-exponential. Therefore, standard concentration bounds derived for sub-Gaussian or sub-exponential random variables cannot be used in our analysis. However, the argument given in Appendix A.1 in Arora et al. (2016b) for a partition function with bounded mean and variance can be directly applied to Zc in our case, which completes the proof of the concentration lemma.

From the symmetry between h and t, Lemma 1 also applies for the partition function c V t R2c . Under the conditions required to satisfy Lemma 1, the following main theo-
rem of this paper holds:

Theorem 1. Suppose that the entity embeddings satisfy (1). Then, we have

log p(h, t | R) =

R1

h + R2

t

2
2 - 2 log Z ±

.

 2d

for = O(1/ n) + O(1/d), where

(18)

Z = Zc = Zc .

(19)

The complete proof of Theorem 1 is given in Appendix A. Below we briefly sketch the main steps.

Proof sketch: Let F be the event that both c and c are within (1 ± z)Z. Then, from Lemma 1 and the union bound, event F happens with probability at least 1 - 2 exp(-(log2 n)). The the R.H.S.
of (5) can be split into two parts T1 and T2 according to whether F happens or not.

p(h, t | R) = Ec,c

exp h R1c exp h R2c Zc Zc

1F

+ Ec,c

exp h R1c exp h R2c Zc Zc

1F¯

.

=T1

=T2

(20)

T1 can be approximated as given by (21).

T1

=

1 ± O( Z2

z) Ec,c

exp h R1c exp t R2c

On the other hand, T2 can be shown to be a constant, independent of d, given by (22).

|T2| = exp(-(log1.8 n))

(21) (22)

The vocabulary size n of real-world knowledge graphs is typically over 105, for which T2 becomes
negligibly small. Therefore, it suffices to consider only T1. Because of the slowness of the random walk we have c  c

Using the law of total expectation we can write T1 as follows:

T1

=

1

± O( Z2

z) Ec

exp

h

R1c

Ec |c

exp

t

R2c

=

1

± O( Z2

z) Ec

exp

h

R1c

A(c)

where A(c) := Ec |c exp t R2c . Doing some further evaluations we show that

A(c) = (1 ± 2) exp t R2c

Plugging (50) back in (23) provides the claim of the theorem.

(23) (24)

The relationship given by (18) indicates that head and tail entity embeddings are first transformed
respectively by R1 and R2 , and the squared 2 norm of this vector is proportional to the probability p(h, t | R).

5

Under review as a conference paper at ICLR 2019

3 LEARNING KNOWLEDGE GRAPH EMBEDDINGS

In this section, we derive a training objective from Theorem 1 that we can then optimise to learn KGE. The goal is to empiricially validate the theoretical result by evaluating the learnt KGEs. Knowledge graphs represent information about relations between two entities in the form of relational triples. The joint probability p(h, R, t) given by Theorem 1 is useful for determining whether a relation R exists between two given entities h and t. For example, if we know that with a high probability that R holds between h and t, then we can append (h, R, t) to the knowledge graph. The task of expanding knowledge graphs by predicting missing links between entities or relations is known as the link prediction problem (Trouillon et al., 2016). In particular, if we can automatically append such previously unknown knowledge to the knowledge graph, we can expand the knowledge graph and address the knowledge acquisition bottleneck.

To derive a criteria for determining whether a link must be predicted among entities and relations, let us consider a relational triple (h, R, t)  D that exists in a given knowledge graph D. We call such relational triples as positive triples because from the assumption it is known that R holds between h and t. On the other hand, consider a negative relational triple (h , R, t )  D formed by, for example, randomly perturbing a positive triple. A popular technique for generating such (pseudo) negative triples is to replace h or t with a randomly selected different instance of the same entity type. As an alternative for random perturbation, Cai and Wang (2018) proposed a method for generating negative instances using adversarial learning. Here, we are not concerned about the actual method used for generating the negative triples but assume a set of negative triples, D¯, generated using some method, to be given.
Given a positive triple (h, R, t)  D and a negative triple (h , R, t )  D¯, we would like to learn KGEs such that a higher probability is assigned to (h, R, t) than that assigned to (h , R, t ). We can formalise this requirement using the likelihood ratio given by (25).

p(h, R, t)   p(h , R, t )

(25)

Here,  > 1 is a threshold that determines how higher we would like to set the probabilities for the positive triples compares to that of the negative triples.

By taking the logarithm of both sides in (25) we obtain

log p(h, R, t) - log p(h , R, t )  log  log  + log p(h , R, t ) - log p(h, R, t)  0

(26)

If a positive triple (h, R, t) is correctly assigned a higher probability than a negative triple p(h , R, t ), then the left hand side of (26) will be positive, indicating that there is no loss incurred during this classification task. Therefore, we can re-write (26) to obtain the marginal loss Bordes et al. (2013; 2011), L(D, D¯), a popular choice as a learning objective in prior work in KGE, as shown in (27).

L(D, D¯) =

max (0, log  + log p(h , R, t ) - log p(h, R, t))

(h,R,t)D (h ,R,t )D¯

= max

0, 2d log  +

R1

h + R2

t

2 2

-

R1

h + R2

t

2 2

We can assume 2d log  to be the margin for the constraint violation.

(27)

Theorem 1 requires R1 and R2 to be orthogonal. To reflect this requirement, we add two 2

regularisation terms

R1

R1 - I

2 2

and

R2

R2 - I

2 2

respectively

with

regularisation

coefficients

1 and 2 to the objective function given by (27). In our experiments, we compute the gradients

(27) w.r.t. each of the parameters h, t, R1 and R2 and use stochastic gradient descent (SGD) for

optimisation. This approach can be easily extended to learn from multiple negative triples as shown

in Appendix B.

4 RELATED WORK
At a high-level of abstraction, KGE methods can be seen as differing in their design choices for the following two main problems: (a) how to represent entities and relations, and (b) how to model the

6

Under review as a conference paper at ICLR 2019
interaction between two entities and a relation that holds between them. Next, we briefly discuss prior proposals to those two problems (refer (Wang et al., 2017; Nickel et al., 2015; Nguyen, 2017) for an extended survey on KGE).
A popular choice for representing entities is to use vectors, whereas relations have been represented by vectors, matrices or tensors. For example, TransE (Bordes et al., 2011), TransH (Wang et al., 2014), TransD (Ji et al., 2015), TransR (Lin et al., 2015), lppTransD (Yoon et al., 2016), DistMult (Yang et al., 2015), HolE (Nickel et al., 2016) and ComplEx (Trouillon et al., 2016) represent relations by vectors, whereas Structured Embeddings (Bordes et al., 2011), TranSparse (Ji et al., 2016), STransE (Nguyen et al., 2016), RESCAL (Nickel et al., 2011) use matrices and Neural Tensor Network (NTN) (Socher et al., 2013) uses 3D tensors. ComplEx (Trouillon et al., 2016) introduced complex vectors for KGEs to capture the asymmetry in semantic relations. (Ding et al., 2018) obtained state-of-the-art performance for KGE by imposing non-negativity and entailment constraints to ComplEx.
Given entity and relation embeddings, a scoring function is defined that evaluates the strength of a relation R between two entities h and t in a triple (h, R, t). The scoring functions that encode various intuitions have been proposed such as the 1 or 2 norms of the vector formed by a translation of the head entity embedding by the relation embedding over the target embedding, or by first performing a projection from the entity embedding space to the relation embedding space (Yoon et al., 2016) As an alternative to using vector norms as scoring functions, DistMult and ComplEx use the component-wise multi-linear dot product.
Once a scoring function is defined, KGEs are learnt that assign high scores to relational triples in existing knowledge graphs (positive triples) over triples where the relation does not hold (negative triples) by minimising a loss function such as the logistic loss (RESCAL, DistMult, ComplEx) or marginal loss (TransE, TransH, TransD, TransD). Because knowledge graphs record only positive triples, a popular method to generate pseudo negative triples is to perturb a positive instance by replacing its head or tail entity by an entity selected uniformly at random from the vocabulary of the entities. However, uniformly sampled negative triples are likely too be obvious examples that do not provide much information to the learning process and can be detected by simply checking for the type of the entities in a triple. To overcome this issue, Cai and Wang (2018) proposed an adversarial learning approach where a generator assigns a probability to each relation triple and negative instances are sampled according to this probability distribution to train a discriminator that discriminates between positive and negative instances.
Our work extends the random walk analysis by Arora et al. (2016a) that derives a useful connection between the joint co-occurrence probability of two words and the 2 norm of the sum of the corresponding word embeddings. Specifically, they proposed a latent variable model where the words in a corpus are generated by a probabilistic model parametrised by a time-dependent discourse vector that performs a random walk. However, unlike in our work, they do not consider the relations between two co-occurring words in a corpus. Bollegala et al. (2018) extended the model proposed by Arora et al. (2016a) to capture co-occurrences involving more than two words. They defined the co-occurrence of k unique words in a given context as a k-way co-occurrence, where Arora et al. (2016a)'s result could be seen as a special case corersponding to k = 2. Moreover, Bollegala et al. (2018) showed that it is possible to learn word embeddings that capture some types of semantic relations such as antonymy and collocation using 3-way co-occurrences more accurately than using 2-way co-occurrences. However, their model does not explicitly consider the relations between words/entities and uses only a corpus for learning the word embeddings.
5 EMPIRICAL VALIDATION
To empirically evaluate the theoretical result stated in Theorem 1, we learn KGEs (denoted by RelWalk) by minimising the marginal loss objective derived in section 3. We use the FB15k, FB13 (subsets of Freebase) and WN18, WN11 (subsets of WordNet) datasets, which are standard benchmarks for KGE. We use the standard training, validation and test splits as detailed in Table 2. We generate negative triples by replacing a head or a tail entity in a positive triple by a randomly selected different entity and learn KGEs. We train the model until convergence or at most 1000 epochs over the training data where each epoch is divided into 100 mini-batches. The best model is selected by early stopping based on the performance of the learnt embeddings on the validation set (evaluated after each 20 epochs). We selected the initial learning rate () for SGD in {0.01, 0.001}, the regularisation
7

Under review as a conference paper at ICLR 2019

Dataset
FB15k FB13 WN18 WN11

Table 2: Statistics of the datasets Relations Entities Train Test

1,345 13 18 11

14,951 75,043 40,943 38,588

483,142 316,232 141,442 112,581

59,071 23,733 5,000 10,544

Validation
50,000 5,908 5,000 2,609

Table 3: Performance on link prediction.

Method

FB15k

WN18

MR Hits@10 MR Hits@10

SE 162 39.8 TransE 125 47.1 DistMult 97 82.4 RESCAL 683 44.1 ComplEx - 84.0 RelWalk 42 60.42

985 251 902 1163 172

80.5 89.2 93.6 52.8 94.7 93.6

Table 4: Accuracy on triple classification.

Method WN11 FB13

SE TransE TransR NTN RelWalk

53.0 75.9 85.9 70.4 72.2

75.2 81.5 82.5 87.1 86.6

coefficients (1, 2) for the orthogonality constraints of relation matrices in {0, 1, 10, 100}. The number of randomly generated negative triples nneg for each positive example is varied in {1, 10, 20} and 50-dimensional entity and relation embeddings are learnt. Optimal hyperparameter settings were: 1 = 2 = 10, nneg = 20 for all the datasets,  = 0.001 for FB15K and FB13,  = 0.01 for WN18 and WN11. RelWalk is implemented in the open-source toolkit OpenKE (Han et al., 2018).2
We conduct two evaluation tasks: link prediction (predict the missing head or tail entity in a given triple (h, R, ?) or (?, R, t)) (Bordes et al., 2011) and triple classification (predict whether a relation R holds between h and t in a given triple (h, R, t)) (Socher et al., 2013). We evaluate the performance in the link prediction task using mean rank (MR, the average of the rank assigned to the original head or tail entity in a corrupted triple) and Hits@10 (percentage of the test cases where the original head or tail entity was found among the top-10 ranked entities), whereas in the triple classification task we use accuracy (percentage of the correctly classified test triples). We only report scores under the filtered setting Bordes et al. (2013), which removes all triples appeared in training, validating and testing sets from candidate triples before obtaining the rank of the ground truth triple. In link prediction, we consider all entities that appear in the corresponding argument in the entire knowledge graph as candidates.
In Tables 3 and 4 we compare the KGEs learnt by RelWalk against prior work using the published results. Although RelWalk does not consistently outperform SoTA on all tasks, we see that in WN18 and FB13 it performs competitively against ComplEx and NTN, which report the SoTA on those datasets. In particular, the improvement against structured embeddings (SE) is consistent and interesting because the scoring function of SE (Table 1) closely resembles that of RelWalk as we can redefine R2 with the negative sign. However, SE learns KGEs that minimise the 1,2 norm whereas according to (18) we must maximise the probability for relational triples in a knowledge graph. Overall, the experimental results support our theoretical claim and emphasise the importance of theoretically motivating the scoring function design process.
6 CONCLUSION
We proposed a generative model of KGE and derived a theoretical relationship between the probability of a triple and entity, relation embeddings. We then proposed a learning objective based on the theoretical relationship we derived. To the best of our knowledge ours is the first generative KGE learning method with a theoretically motivated scoring function. Experimental results on a link prediction and a triple classification tasks show that accurate KGEs can be learnt using the proposed scoring function.
2To facilitate the double blind policy, the source code for RelWalk will be released upon paper acceptance
8

Under review as a conference paper at ICLR 2019
REFERENCES
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. Transactions of Association for Computational Linguistics, 4:385­399, 2016a.
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Rand-walk: A latent variable model approach to word embeddings. arXiv, 2016b.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proc. of SIGMOD, pages 1247 ­ 1250, 2008.
Danushka Bollegala, Yuichi Yoshida, and Ken-ichi Kawarabayashi. Using k-way Co-occurrences for Learning Word Embeddings. In Proc. of AAAI, 2018.
Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured embeddings of knowledge bases. In Proc. of AAAI, 2011.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Durán, Jason Weston, and Oksana Yakhenko. Translating embeddings for modeling multi-relational data. In Proc. of NIPS, 2013.
Liwei Cai and William Yang Wang. Kbgan: Adversarial learning for knowledge graph embeddings. In Proc. of NAACL, pages 1470­1480, 2018.
Boyang Ding, Quan Wang, Bin Wang, and Li Guo. Improving knowledge graph embedding using simple constraints. In Proc. of ACL, pages 110­121, 2018.
Xu Han, Shulin Cao, Lv Xin, Yankai Lin, Zhiyuan Liu, Maosong Sun, and Juanzi Li. Openke: An open toolkit for knowledge embedding. In Proc. of EMNLP, 2018.
Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. Knowledge graph embedding via dynamic mapping matrix. In Proc. of ACL, pages 687­696, 2015.
Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Knowledge graph completion with adaptive sparse transfer matrix. In Proc. of AAAI, pages 985­991, 2016.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In Proc. of AAAI, pages 2181­2187, 2015.
Dat Quoc Nguyen. An overview of embedding models of entities and relationships for knowledge base completion. 03 2017. URL https://arxiv.org/abs/1703.08098.
Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark Johnson. Stranse: a novel embedding model of entities and relationships in knowledge bases. In Proc. of NAACL-HLT, pages 460­466, 2016.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In Proc. of ICML, pages 809­816, 2011.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11­33, 2015.
Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of knowledge graphs. In Proc. of AAAI, 2016.
Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. Reasoning with neural tensor networks for knowledge base completion. In Proc. of NIPS, 2013.
Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In Proc. of ICML, 2016. URL http://arxiv.org/abs/ 1606.06357.
Q. Wang, Z. Mao, B. Wang, and L. Guo. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):2724­2743, Dec 2017. ISSN 1041-4347. doi: 10.1109/TKDE.2017.2754499.
9

Under review as a conference paper at ICLR 2019 Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In Proc. of AAAI, pages 1112 ­ 1119, 2014. Bishan Yang, Wen-tau Yih, Xiadong He, Jianfeng Gao, and Li Deng. Embedding entities and relations
for learning and inference in knowledge bases. In ICLR, 2015. Hee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, and Se-Young Park. A translation-based knowledge
graph embedding preserving logical property of relations. In Proc. of NAACL, pages 907­916, 2016.
10

Under review as a conference paper at ICLR 2019

APPENDIX

A PROOF OF THEOREM 1

Let us consider the probabilistic event that (1 - z)Z  Zc  (1 + z)Z to be Fc and (1 - z)Z  Zc  (1 + z)Z to be Fc . From Lemma 1 we have Prc[Fc]  1 - . Then from the union bound
we have,

Pr[F¯c  F¯c ]  Pr[F¯c] + Pr[F¯c ]

= 1 - Pr[Fc] + 1 - Pr[Fc ]

= 2.

(28)

Moreover, let F be the probabilistic event that both Fc and Fc being True. Then from Pr[F ] = 1 - Pr[F¯c  F¯c ] we have, Pr[F ]  1 - 2. We can decompose the expectation in the R.H.S. in (5) into two terms T1 and T2 depending on whether respectively F is True or False as follows:

p(h, t | r) = Ec,c

exp h R1c exp h R2c Zc Zc

1F

+ Ec,c

exp h R1c exp h R2c Zc Zc

1F¯

.

=T1

Here, 1F and 1F¯ are indicator functions given by:

1 if F is True, 1F = 0 otherwise,

and

1F¯ =

0 1

if F is True, otherwise.

=T2

(29) (30) (31)

Let us first show that T2 is negligibly small.

For two real integrable functions 1(x) and 2(x) in [a, b], the Cauchy-Schwarz's inequality states that

b

2b

b

1(x)2(x)dx  [1(x)]2dx [2(x)]2dx.

a aa

(32)

Applying (32) to T2 in (29) we have:

12 Ec,c ZcZc exp h R1c exp t R2c 1F¯



Ec,c

1 Zc2 exp

h

R1c 2 1F¯

Ec,c

1 Zc2 exp

t

R2c

2 1F¯

=

Ec

1 Zc2 exp

h

R1c 2 Ec |c [1F¯ ]

Ec

1 Zc2 exp

t

R2c

2
Ec|c

[1F¯ ]

(33)

Note that Zc  1 because Zc is the sum of positive numbers and if h R1c  0 for at least one of the h  V, then the total sum will be greater than 1. Therefore, by dropping Zc term from the
denominator we can further increase the first term in (33) as given by (34).

Ec

1 Zc2 exp

h

R1c 2 Ec |c [1F¯ ]

 Ec exp h R1c 2 Ec |c [1F¯ ]

(34)

Let us split the expectation on the R.H.S. of (34) into two cases depending on whether h R1c > 0 or otherwise, indicated respectively by 1(h R1c>0) and 1(h R1c0).
Ec exp h R1c 2 Ec |c [1F¯ ]
= Ec exp h R1c 2 1(h R1c>0)Ec |c [1F¯ ] + Ec exp h R1c 2 1(h R1c0)Ec |c [1F¯ ] (35)

11

Under review as a conference paper at ICLR 2019

The second term of (35) is upper bounded by Ec,c [1F¯ ]  exp -(log2 n)

(36)

The first term of (35) can be bounded as follows:

Ec

exp

h

2
R1c 1(h R1c>0)Ec |c [1F¯ ]

 Ec

exp(h

2
R1c) 1(h R1c>0)Ec |c [1F¯ ]

 Ec exp(h R1c)2Ec |c [1F¯ ]

(37)

where  

>

1.

Therefore, it is sufficient to bound Ec

exp(h

R1c)2Ec |c [1F¯ ]

when

h

=

( d).

Let us denote by z the random variable 2h R1c. Moreover, let r(z) = Ec |z[1F¯], which is a function of z between [0, 1]. We wish to upper bound Ec[exp(z)r(z)]. The worst-case r(z) can be quantified using a continuous version of Abel's inequality (proved as Lemma A.4 in Arora et al. (2016b)), we
can upper bound Ec [exp(z)r(z)] as follows:

Ec [exp(z)r(z)]  E exp(z)1[t,+](z)

(38)

where t satisfies that Ec[1[t,+](z)] = Pr[z  t] = Ec[r(z)]  exp(-(log2 n)). Here, 1[t,+](z)
is a function that takes the value 1 when z  t and zero elsewhere. Then, we claim Prc[z  t]  exp(-(log2 n)) implies that t  (log.9 n).

If c

was distributed

as

N

(0,

1 d

I),

this would

be

a

simple

tail

bound.

However,

as c

is

distributed

uniformly on the sphere, this requires special care, and the claim follows by applying the tail bound

for the spherical distribution given by Lemma A.1 in (Arora et al., 2016a) instead. Finally, applying

Corollary A.3 in (Arora et al., 2016a), we have:

E[exp(z)r(z)]  E[exp(z)1[t,+](z)] = exp(-(log1.8 n))

(39)

From a similar argument as above we can obtain the same bound for c as well. Therefore, T2 in (29) can be upper bounded as follows:

1 Ec,c ZcZc exp h R1c exp t R2c 1F¯

=

Ec

1 Zc2 exp

h

R1c 2 Ec |c [1F¯ ]

1/2
Ec

 exp(-(log1.8 n))

1 Zc2 exp

t

R2c

2
Ec|c

[1F¯ ]

1/2
(40)

Because n = |V|, the size of the entity vocabulary, is large (ca. n > 105) in most knowledge graphs, we can ignore the T2 term in (29). Combining this with (29) we obtain an upper bound for p(h, t | R) given by (41).

p(h, t | R)  (1 +

z

)2

1 Z2

Ec,c

= (1 +

z

)2

1 Z2

Ec,c

exp h R1c exp t R2c exp h R1c exp t R2c

1F + |D| exp(-(log1.8 n)) + 0 (41)

where |D| is the number of relational tuples (h, R, t) in the KB D and 0 = |D| exp(-(log1.8 n))  exp(-(log1.8 n)) by the fact that Z  exp(2)n = O(n), where  is the upper bound on h R1c
and t R2c , which is regarded as a constant.

On the other hand, we can lower bound p(h, t | R) as given by (42).

p(h, t | R)  (1 -

z

)2

1 Z2

Ec,c

 (1 -

z

)2

1 Z2

Ec,c

 (1 -

z

)2

1 Z2

Ec,c

exp h R1c exp t R2c exp h R1c exp t R2c exp h R1c exp t R2c

1F - |D| exp(-(log1.8 n)) - 0 (42)

12

Under review as a conference paper at ICLR 2019

Taking the logarithm of both sides, from (41) and (42), the multiplicative error translates to an additive error given by (43).

log p(h, t | R) = log Ec,c exp h R1c exp t R2c ± 0 - 2 log Z + 2 log(1 ± z)

= log Ec exp h R1c Ec |c exp t R2c ± 0 - 2 log Z + 2 log(1 ± z)

= log Ec exp h R1c A(c) ± 0 - 2 log Z + 2 log(1 ± z)

(43)

where A(c) := Ec |c exp t R2c .

We assumed that c and c are on the unit sphere and R1 and R2 to be orthogonal matrices. Therefore,

R1c and R2c are also on the unit sphere. Moreover, ifwe let the upper bound of the 2 norm of the

entity embeddings to be  d, then we have h   d and t   d. Therefore, we have



R1h, c - c  h c - c   d c - c

(44)

Then we can lower bound A(c) as follows:

A(c) = exp t R2c Ec |c exp t R2(c - c) 
 exp t R2c Ec |c exp  d c - c

 (1 + 2) exp t R2c
For some 2 > 0. The last inequality holds because 
Ec|c exp  d c - c = exp  d c - c p(c |c)dc
 = exp( d) exp( c - c )p(c |c)dc

1
=1+ 2

1

(45) (46)

To obtain a lower bound on A(c) from the first-order Taylor approximation of exp(x)  1 + x we

observe that 
Ec|c exp  d c - c

 + Ec|c exp - d c - c

 2.

(47)

Therefore, from our model assumptions we have 
Ec|c exp - d c - c

1- 2

(48)

Hence,

A(c) = exp t R2c Ec |c exp t R2(c - c) 
 exp t R2c Ec |c exp - d c - c  (1 - 2) exp t R2c

(49)

Therefore, from (46) and (49) we have

A(c) = (1 ± 2) exp t R2c

(50)

Plugging A(c) back in (43) we obtain

log p(h, t | R) = log Ec exp h R1c A(c) ± 0 - 2 log Z + 2 log(1 ± z)
= log Ec exp h R1c (1 ± 2) exp t R2c ± 0 - 2 log Z + 2 log(1 ± z) (51)

= log Ec exp h R1c exp t R2c ± 0 - 2 log Z + 2 log(1 ± z) + log(1 ± 2) = log Ec exp h R1c + t R2c ± 0 - 2 log Z + 2 log(1 ± z) + log(1 ± 2) = log Ec exp R1 h + R2 t c ± 0 - 2 log Z + 2 log(1 ± z) + log(1 ± 2)

13

Under review as a conference paper at ICLR 2019

Note that c has a uniform distribution over the unit sphere. In this case, from Lemma A.5 in (Arora et al., 2016b), (52) holds approximately.

Ec exp R1 h + R2 t c = (1 ± 3) exp

R1 h + R2 t 2 2d

(52)

where 3 = O~(1/d). Plugging (52) in (51) we have that

log p(h, t | R) =

R1

h + R2 2d

t

2 2

+ O(

z) + O(

2) + O(

3) + O(0) - 2 log Z

(53)

where 0 = 0 · Ec ignored. Note that 3

exp (R1 = O~(1/d)

h+ and

R2 z=

Ot~)(1c/n-)1by=aessxupm(-ptio(nlo. gT1h.8erne)f)o.reT, hweereofbotraei,nt0hactan

be

log p(h, t | R) =

R1

h + R2 2d

t

2 2

+ O(

z) + O(

2) + O~(1/d) - 2 log Z

(54)

B LEARNING WITH MULTIPLE NEGATIVE TRIPLES

In this section, we show how the margin loss-based learning objective derived in section 3 can be extended to learn from more than one negative triples per each positive triple. This formulation leads to rank-based loss objective used in prior work on KGE. Considering that negative triples are generated via random perturbation, it is important to consider multiple negative triples during training to better estimate the classification boundary.

Let us consider that we are given a positive triple, (h, R, t) and a set of K negative triples {(hk, R, tk)}Kk=1. We would like our model to assign a probability, p(h, t | R), to the positive triple that is higher than that assigned to any of the negative triples. This requirement can be written
as (55).

p(h, t|R)



max
k=1,...,K

p(hk, tk

|

R)

(55)

We could further require the ratio between the probability of the positive triple and maximum probability over all negative triples to be greater than a threshold   1 to make the requirement of
(55) to be tighter.

p(h, t | R)

max
k=1,...,K

p(hk, tk

|

R)





(56)

By taking the logarithm of (56) we obtain

log p(h, t | R) - log

max
k=1,...,K

p(hk, tk

|

R)

 log()

Therefore, we can define the margin loss for a misclassification as follows:

(57)

L

(h, R, t), {(hk, R, tk)}Kk=1

= max 0, log

max
k=1,...,K

p(hk, tk

|

R)

+ log() - log p(h, t | R)

(58)

However, from the monotonicity of the logarithm we have x1, x2 > 0, if log(x1)  log(x2) then x1  x2. Therefore, the logarithm of the maximum can be replaced by the maximum of the logarithms in (58) as shown in (59).

L

(h, R, t), {(hk, R, tk)}Kk=1

= max

0, max k=1,...,K

log

p(hk, tk | R)

+ log() - log p(h, t | R)

(59)

By substituting (18) for the probabilities in (59) we obtain the rank-based loss given by (60).

L

(h, R, t), {(hk, R, tk)}kK=1

= max

0,

2d

log()

+

max
k=1,...,K

R1

hk + R2

tk

2 2

-

R1

h + R2

t

2 2

(60)

In practice, we can use p(hk, tk | R) to select the negative triple with the highest probability for training with the positive triple.

14

