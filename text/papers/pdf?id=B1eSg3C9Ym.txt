Under review as a conference paper at ICLR 2019
MEAN-FIELD ANALYSIS OF BATCH NORMALIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Batch Normalization (BatchNorm) is an extremely useful component of modern neural network architectures, enabling optimization using higher learning rates and achieving faster convergence. In this paper, we use mean-field theory to analytically quantify the impact of BatchNorm on the geometry of the loss landscape for multi-layer networks consisting of fully-connected and convolutional layers. We show that it has a flattening effect on the loss landscape, as quantified by the maximum eigenvalue of the Fisher Information Matrix. These findings are then used to justify the use of larger learning rates for networks that use BatchNorm, and we provide quantitative characterization of the maximal allowable learning rate to ensure convergence. Experiments support our theoretically predicted maximum learning rate, and furthermore suggest that networks with smaller values of the BatchNorm parameter  achieve lower loss after the same number of epochs of training.
1 INTRODUCTION
Deep neural networks have achieved remarkable success in the past decade on tasks that were out of reach prior to the era of deep learning (Krizhevsky et al., 2012; He et al., 2016). Amongst the myriad reasons for these successes are powerful computational resources, large datasets, new optimization algorithms, and modern architecture designs (Russakovsky et al., 2015; Kingma & Ba, 2015). In many modern deep learning architectures, one key component is batch normalization (BatchNorm). BatchNorm is a module that can be introduced in layers of deep neural networks that normalizes hidden layer outputs to have a common first and second moment. Empirically, BatchNorm enables optimization using much larger learning rates, and achieves better convergence (Ioffe & Szegedy, 2015).
Despite significant practical success, a theoretical understanding of BatchNorm is still lacking. A widely held view is that BatchNorm improves training by "reducing of internal covariate shift" (ICF) (Ioffe & Szegedy, 2015). Internal covariate shift refers to the change in the input distribution of internal layers of the deep network due to changes of the weights. Recent results (Santurkar et al., 2018), however, cast doubt on the ICF expalanation, by demonstrating that noisy BatchNorm increases ICF yet still improves training as in regular BatchNorm. This raises the question of whether the utility of BatchNorm is indeed related to the reduction of ICF. Instead, it is argued by Santurkar et al. (2018) that BatchNorm actually improves the Lipschitzness of the loss and gradient.
Meanwhile, dynamical mean-field theory (Sompolinsky & Zippelius, 1982), a powerful theoretical technique, has recently been applied by Poole et al. (2016) to ensembles of multi-layer random neural networks. This theory studies networks with an i.i.d. Gaussian distribution of weights and biases. Most recent work focuses on the analysis of order parameter flows and their fixed points (Schoenholz et al., 2017; Xiao et al., 2018; Yang & Schoenholz, 2017), including their stability and decay rates. Importantly, Karakida et al. (2018) also successfully used mean-field analysis to estimate the spectral properties of the Fisher Information Matrix.
In this paper, we analytically quantify the impact of BatchNorm on the landscape of the loss function, by using mean-field theory to estimate the spectral properties of the Fisher Information Matrix (FIM) for typical batch-normalized neural networks. In particular, it is shown that BatchNorm reduces the maximal eigenvalue of the FIM provided that the normalization coefficient  is not too large. By drawing on results linking Fisher Information to the geometry of the loss function, we explain how BatchNorm neural networks can be trained with a larger learning rate without leading to parameter
1

Under review as a conference paper at ICLR 2019

explosion, and provide upper bounds on the learning rate in terms of the BatchNorm parameters. As an additional contribution motivated by our theoretical findings, we demonstrate an empirical correlation between the BatchNorm parameter  and test loss. In particular, networks with smaller  achieve lower loss after a fixed number of training epochs.

2 PRELIMINARIES

In our theoretical analysis, we employ the recent application of mean-field theory to neural networks which studies an ensemble of random neural networks with pre-defined i.i.d. Gaussian weights and biases. In this section, we provide background information and briefly recall the formalism of Karakida et al. (2018) which first computes spectral properties of the Fisher Information of a neural network and then relates it to the maximal stable learning rate.

2.1 FISHER INFORMATION MATRIX AND LEARNING DYNAMICS

Given a data distribution D over the set of instance-label pairs X × Y, a family of parametrized functions f : X  Y and a loss function l(f, y), our focus will be to ensure convergence of the following gradient descent with momentum update rule:

t+1 = t - L(t) + µ(t - t-1) , where L() is the unobserved population loss,

(1)

L() := E(x,y)D l(f(x), y) .

(2)

In practice, the parameters are determined by minimizing an empirical estimate of equation 2 using a stochastic generalization (SGD) of the update rule equation 1. We neglect this difference by always working in the asymptotic limit of large sample size and moreover assuming full-batch gradient updates.

Suppose the loss function can be expressed in terms of a parametric family of positive densities as l(f(x), y) =: - log p(x, y). This assumption holds true for a large class of losses including squared loss and cross-entropy loss. Let I denote the Fisher Information Matrix (FIM) associated with the parametric family induced by the loss,

I := E(x,y)P [ log p(x, y)   log p(x, y)] . Recall that under suitable regularity conditions the following identity holds:

(3)

I = -E(x,y)P

Hess log p(x, y)


.

The above right-hand side is closely related to the Hessian of the population loss,

(4)

Hess L() = -E(x,y)D Hess log p(x, y) ,


(5)

where we interchanged the Hessian with the expectation value. In fact, if we assume that the esti-
mation problem is well-specified so that there exist parameters  such that the data distribution is generated by P = D, then we obtain the following equality between the Hessian of the population loss and the FIM evaluated at the optimal parameters,

Hess L() = I .

(6)

If  is initialized in a sufficiently small neighborhood of , then by expanding the population loss L() to quadratic order about  one can show that a necessary condition for convergence is that the step size is bounded from above by (LeCun et al., 2012; Karakida et al., 2018)1,

2(1 + µ)

2(1 + µ)

 <  := max

Hess(L())

=, max(I )

(7)

where max(M ) denotes the largest eigenvalue of the matrix M . Rather than computing the optimal parameters  directly, we follow the strategy of Karakida et al. (2018) by estimating the following

1In the quadratic approximation to the loss, the optimal learning rate is in fact /2.

2

Under review as a conference paper at ICLR 2019

quantity and arguing that the distribution of the weights and biases is not significantly impacted by the training dynamics,

¯max := max E I


,

(8)

where E denotes the expectation value with respect to the weights and biases. This heuristic was shown to yield a remarkably accurate prediction of the maximal learning rate in (Karakida et al.,
2018).

In this paper we adopt the data modeling assumption that P = PX PY | X where PX is the marginal distribution of the covariates, which is indepdent of . Under this factorization assumption, the FIM simplifies to

I = E(x,y)P  log p(y | x)   log p(y | x) .

(9)

Focusing on the Gaussian conditional model p(y |x)



exp(

1 2

f(x) - y

22), the FIM further

simplifies to

I = ExD f(x)  f(x) .

(10)

The family of parametrized functions f : RN0  RNL is chosen to be the family of functions computed by a multi-layer neural network architecture with N0 input nodes, NL output nodes and L  1 layers. In this paper, we consider neural networks consisting of fully-connected (FC) and
convolutional (Conv) layers, with and without batch normalization. The pointwise activation is
denoted by , which is taken to be the rectified linear unit (ReLU) in this paper. Our analysis can be straightforwardly extended to other architectures and non-linearities. We use hl (x) to denote the output of layer l and the input to layer l + 1. Clearly we have h0(x) = x and hL(x) = f(x).

3 THEORY

In this section we focus on applying dynamical mean-field theory to study the effect of introducing batch normalization modules into a deep neural network by estimating the largest eigenvalue of the FIM. This estimate, in turn, provides an upper bound on the largest learning rate for which the learning dynamics is stable. This section is structured as follows: We first define various thermodynamic quantities (order parameters, 6 for fully-connected layers and 9 for convolutional layers) that satisfy recursion relations in the mean-field approximation. Then we present an estimate of ¯max in terms of these order parameters, generalizing a result of Karakida et al. (2018). Using this estimate, we study how ¯max and  are affected by BatchNorm and calculate their dependence on the BatchNorm coefficient . Detailed derivations of the order parameters, their recursions, and the associated eigenvalue bound are deferred to the Supplementary Material.

3.1 FULLY CONNECTED LAYERS

3.1.1 STANDARD CASE

A general fully connected layer with input activation hl(x) and output pre-activation zl+1(x) is described by the affine transformation,

zl+1(x) := W l+1hl(x) + bl+1 ,

(11)

where W l+1  RNl+1×Nl , bl+1  RNl+1 and Nl denotes the number of units in layer l. In the framework of mean-field theory, we will consider an ensemble of neural networks with Gaussian
random weights and biases distributed as follows,

[W l+1]ij  N (0, w2 /Nl) ,

bli+1  N (0, b2) .

(12)

In the case of a standard fully connected layer, the input activation satisfies the recursions hl(x) = (zl(x)), where  denotes the pointwise activation.

A batch-normalized fully connected layer, in contrast, satisfies the following recursion,

hl(x) := 

zl(x) - µl sl

l + l

,

(13)

3

Under review as a conference paper at ICLR 2019

where µl  RNl and (sl)2 := sl sl  RNl denote the mean and variance of the pre-activation layers with respect to the data distribution,

µl := E zl(x) ,
x
(sl)2 := E (zl(x) - µl)2 .
x

(14) (15)

The weights and biases are drawn from the same distributions as in the standard, no BatchNorm, case. In addition, we now have the BatchNorm parameters l+1, l+1  RNl+1 which are assumed
to be non-random for simplicity,

l[i] = l , l[i] = 0 .

(16)

3.1.2 ORDER PARAMETERS AND THEIR RECURSIONS

To investigate the spectral properties of the FIM, we define the following order parameters,

ql

:=

1

Nl

E
x,

zl(x) 2

,

qxl y

:=

1

Nl

E
x,y,

zl(x), zl(y)

q~l := E l(x) 2 ,
x,

,

q^l

:=

1

Nl

E
x,

hl(x) 2

,

q^xl y

:=

1

Nl

E
x,y,

hl(x), hl(y)

q~xl y

:=

E
x,y,

l(x), l(y)

,

,

(17) (18) (19)

where l(x)

:=

f zl

(x).

Here we assume that the

data x

are

drawn i.i.d.

from a distribution with

mean 0 and variance 1, and also that the last layer is linear for classification. We then have the base

cases: q^0 = 0, q^x0y = 1, q~L = q~xLy = 1. The order parameters in the absence of BatchNorm satisfy

the following recursions derived in Karakida et al. (2018),

ql = b2 + w2 q^l-1 ,
qxl y = b2 + w2 q^xl-y1 , q~l = w2 q~l+1 , 2

q^l = ql , 2

q^xl y

=

ql 2

1-

c2xy

+

cxy  2

+

cxy

sin-1 cxy

q~xl y

=

w2 q~xl+y1 2

 2

+

sin-1

cxy

,

,

(20) (21) (22)

where cxy := qxl y/ql. In the case of batch normalization we find the following recursions, which are derived in the Supplementary Material,

ql = b2 + w2 q^l-1 ,

qxl y = b2 + w2 q^xl-y1 ,

q~l

=

l2w2 2

q~l+1 ql

,

q^l = l2 , 2

q^xl y

=

l2 2

,

q~xl y

=

l2w2 4

q~xl+y1 ql

.

(23) (24) (25)

3.2 CONVOLUTIONAL LAYER

The results of the preceding section also apply to structured affine transformations including con-
volutional layers. Let Kl denote the set of allowable spatial locations of the the lth layer feature map and let Fl+1 index the sites of the convolutional kernel applied to that layer. Let Cl denote the
number of input channels. The output of a general convolutional layer is of the form,

zl+1(x) =

Wl+1hl+ (x) + bl+1 ,

Fl+1

(26)

where   Kl+1, Wl+1  RCl+1×Cl and bl+1  RCl+1 . The weights and biases are now distributed as

[Wl+1]ij  N (0, w2 /Nl) ,

bli+1  N (0, b2) .

(27)

4

Under review as a conference paper at ICLR 2019

where now Nl := Cl|Fl+1|. As in the fully connected case, we consider convolutional layers with both vanilla activation functions of the form hl(x) := (zl (x)) as well as batch normalized convolutional layers, for which the input activations satisfy the recursive identity,

hl (x) := 

zl (x) - µl sl

l + l

,

(28)

3.2.1 ORDER PARAMETERS AND THEIR RECURSIONS

Similar to the definitions for fully connected layer, we define the following set of order parameters:

ql

:=

1

Cl

E


E
x,

zl (x) 2

,

qxl y

:=

1

Cl

E


E
x,y,

zl (x), zl (y)

,

ql ,xy

:=

1

Cl

E
=

E
x,y,

zl (x), zl (y)

,

q^l

:=

1

Cl

E


E
x,

hl (x) 2

,

q^xl y

:=

1

Cl

E


E
x,y,

hl(x), hl (y)

,

q^l ,xy

:=

1

Cl

E
=

E
x,y,

hl(x), hl (y)

,

(29) (30) (31)

q~l := E E
 x,

l (x) 2

,

q~xl y

:=

E


E
x,y,

l (x), l (y)

,

(32)

q~l ,xy

:=

E
=

E
x,y,

l (x), l (y)

,

(33)

where now l := f/zl in analogy with the fully connected layer. The expectations over  and  are with respect to the uniform measure over the set of allowed indices. For a standard
convolutional layer without BatchNorm, the order parameters can be shown to satisfy the following
recursion relations:

ql = b2 + w2 q^l-1 , qxl y = b2 + w2 q^xl-y1 , ql ,xy = b2 + w2 q^l-1,xy ,
q~l = w2 q~l+1 , 2

q^l = ql , 2

q^xl y

=

ql 2

1

-

cx2 y

+

cxy  2

+

cxy

sin-1

cxy

q^l ,xy

=

ql 2

1-

c2

+

c  2

+ c

sin-1 c

q~xl y

=

w2 q~xl+y1 2

 2

+

sin-1

cxy

,

q~l ,xy

=

w2 q~l+1,xy 2

 2

+

sin-1

c

.

, ,

In the case of convolutional layers with BatchNorm, the following recursions hold:

(34) (35) (36) (37) (38)

ql = b2 + w2 q^l-1 ,

qxl y = b2 + w2 q^xl-y1 ,

ql ,xy = b2 + w2 q^l-1,xy ,

q~l

=

l2w2 2

q~l+1 ql

,

q~l ,xy

=

l2w2 4

q~l+1,xy ql

,

q^l = l2 , 2

q^xl y

=

l2 2

,

q^l ,xy

=

l2 2

,

q~xl y

=

l2w2 4

q~xl+y1 ql

,

(39) (40) (41) (42) (43)

where cxy := qxl y/ql and c := ql ,xy/ql. The derivations of the recursion relations for both vanilla and batch-normalized convolutional layers are deferred to the Supplementary Material.

5

Under review as a conference paper at ICLR 2019

3.3 EIGENVALUE BOUND AND THERMODYNAMIC VARIABLES

The order parameters derived in the previous section are useful because they allow us to gain information about the maximal eigenvalue ¯max of the FIM. We derived a generalization of (Karakida et al., 2018, Theorem 6) to allow for the inclusion of batch normalization and convolutional layers. In particular, we obtain a lower bound on the maximal eigenvalue ¯max in terms of the previously introduced order parameters which satisfy the stated recursion relations in the mean-field approximation.
Claim 3.1. If the layer dimension Nl of the fully connected layers and the number of channels Cl of the convolutional layers satisfy Nl 1 and Cl 1 for 0 < l < L and the number of samples n in the training dataset satisfies n 1, we have,

¯max 

fl ,

l[L]

(44)

where fl = Nl-1q^xl-y1q~xl y for fully connected layers and

fl = Nl-1 (|Kl| - 1)q~l ,xy + q~xl y (|Kl| - 1)q^l-1,xy + q^xl-y1 ,

(45)

for convolutional layers, where recall Nl-1 = Cl-1|Fl|. The index sets Fl and Kl are defined in section 3.2. The order parameters are defined in the previous subsection.

Now we are ready to calculate the lower bound on ¯max for a given model architecture by calculating the order parameters using their recursions. In the next section, we will focus on the numerical
analysis of these recursion relations as well as present experiments that support our calculation.

4 NUMERICAL ANALYSIS AND EXPERIMENTS
In order to understand the effect of BatchNorm on the loss landscape, we numerically compute ¯max as a function of the BatchNorm parameter , for both fully connected and convolutional architectures (Fig. 1) with and without BatchNorm. For  3 (typical for deep network initialization (Ioffe & Szegedy, 2015)) BatchNorm significantly reduces ¯max compared to the vanilla networks. As a direct consequence of this, the theory predicts that batch normalized networks can be trained using significantly higher learning rates than their vanilla counterparts.
We tested the above theoretical prediction by training the same architectures on MNIST and CIFAR10 datasets, for different values of  and , starting from randomly intialized networks with same variances employed in the mean-field theory calculations. As shown in Fig. 2, the (, log10 )-plane clearly partitions into distinct phases characterized by convergent and non-convergent optimization dynamics, and our theoretically predicted upper bound  closely agrees with the experimentally determined phase boundary.
In addition to the striking match between our theoretical prediction and the experimentally determined phase boundaries, the experimental results also suggest a tendency for smaller -initiations to produce lower values of test loss after a fixed number of epochs, i.e. faster convergence. We leave detailed investigation of this initialization scheme to future work. Also, dark strips can be observed in the heatmaps indicate the optimal learning rates for optimization, which is around /2 and consistent with LeCun et al. (2012) in the quadratic approximation to the loss.
The architectural design for our experiments is as follows. In the fully connected architecture, we choose L = 4 layers with Nl = 1000 hidden units per layer except the final (linear) layer which has NL = 10 outputs. Batch normalization is applied after each linear operation except for the final linear output layer. The convolutional network has a similar structure with L = 4 layers. The first three are convolutional layers with filter size 3, stride 2, and number of channels C1 = 30, C2 = 60, C3 = 90. The final layer is a fully connected output layer to perform classification. The other architectural/optimization hyperparameters were chosen to be w2 = 2, b2 = 0.5,  = 0 and µ = 0.9.
6

Under review as a conference paper at ICLR 2019

log10
log10

log10 max

(a) Fully Connected$

34050000

Vanilla BN

3000

2500

2000

1500

1000

500

0

0.1 0.9 1.7 2.5 3.3 4.1

max
log10

0(.5b) 0.0

Fully Connected
Vanilla BN

0.5

1.0

1.5

2.0

2.5

3.0

0.1 0.9 1.7 2.5 3.3 4.1

(c) Convolutional

3500 3000

Vanilla BN

2500

2000

1500

1000

500

0

0.1 0.9 1.7 2.5 3.3 4.1

0(.5d)

Convolutional
Vanilla

0.0 BN

0.5

1.0

1.5

2.0

2.5

3.0 0.1 0.9 1.7 2.5 3.3 4.1

Figure 1: The maximum eigenvalue ¯max and associated critical learning rate  for vanilla (blue) and BatchNorm networks (red) as a function of the BatchNorm parameter  for different choices of architecture (fully-connected and convolutional). (a, c) shows the flattening effect of BatchNorm on the loss function for a wide range of hyperparameters and (b, d) further show that for sufficiently small  BatchNorm enables optimization with much higher learning rate than vanilla networks.

(a) Fully Connected, MNIST 0
-1

80

(b) Fully Connected, CIFAR-10 0

60 50

60 -1

40

(c) Convolutional, CIFAR-10 0 -1

60 50 40

log10

-2 40 -2 30 -2 30

-3

20 -3

20 10

-3

20 10

0.1 1.1 2.1 3.1

0.1 1.1 2.1 3.1

0.1 1.1 2.1 3.1

Figure 2: Heatmaps showing test loss as a function of (log10 , ) after 5 epochs of training for different choices of dataset and architecture. Results were obtained by averaging 5 random restarts.
The white region indicates parameter explosion for at least one of the runs. The red line shows the
theoretical prediction for the maximal learning rate . The dark band on the heatmaps for CIFAR10 approximately tracks the optimal learning rate /2 in the quadratic approximation to the loss. Note the log scale for the learning rate, so the theory matches the experiments over three orders of
magnitude for .

5 CONCLUSION AND FUTURE WORK
In this paper, we studied the impact of BatchNorm on the loss surface of multi-layer neural networks and its implication for training dynamics. By developing recursion relations for the relevant order parameters, the maximum eigenvalue of the Fisher Information matrix ¯max can be estimated and related to the maximal learning rate. The theory correctly predicts that adding BatchNorm with small  allows the training algorithm to exploit much larger learning rates, which speeds up convergence. The experiments also suggest that using a smaller  results in a lower test loss for a fixed number of training epochs. This suggests that initialization with smaller  may help the optimization process in deep learning models, which will be interesting for future study.
The close agreement between theoretical predictions and the experimentally determined phase boundaries strongly supports the validity of our analysis, despite the non-rigorous nature of the derivations. Although similar approaches have been used in other work (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Xiao et al., 2018; Karakida et al., 2018), we hope that future work will place these results on a firmer mathematical footing. Furthermore, our BatchNorm analysis is not limited to the convolutional and fully-connected architectures we considered in this paper and can be extended to arbitrary feedforward architectures such as ResNets.
7

Under review as a conference paper at ICLR 2019
REFERENCES
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Ryo Karakida, Shotaro Akaho, and Shun ichi Amari. Universal statistics of fisher information in deep neural networks: Mean field approach. arXiv preprint arXiv:1806.01316, 2018.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. pp. 10971105, 2012.
Yann A LeCun, Le´on Bottou, Genevieve B Orr, and Klaus-Robert Mu¨ller. Efficient backprop. In Neural networks: Tricks of the trade, pp. 9­48. Springer, 2012.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. pp. 3360­3368, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. 2015.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? (no, it is not about internal covariate shift). arXiv preprint arXiv:1805.11604, 2018.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. In International Conference on Learning Representations (ICLR), 2017.
Haim Sompolinsky and Annette Zippelius. Relaxational dynamics of the edwards-anderson model and the mean-field theory of spin-glasses. Physical Review B, 25(11):6860, 1982.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. In International Conference on Machine Learning (ICML), 2018.
Greg Yang and Samuel S. Schoenholz. Mean field residual networks: On the edge of chaos. In Advances in Neural Information Processing Systems (NIPS), 2017.
8

Under review as a conference paper at ICLR 2019

6 SUPPLEMENTARY MATERIAL

This section provides non-rigorous derivations of the order parameters, their recursions and the associated eigenvalue bound. Despite the non-rigorous nature of these calculations, we remark that similar reasoning has been successfully used in number of related works on mean-field theory, demonstrating impressive agreement with experiment (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Xiao et al., 2018; Karakida et al., 2018).

6.1 RECURSIONS FOR FULLY CONNECTED LAYERS
Claim 6.1. The forward recursions for 0  l  L-1 are ql+1 = b2 +w2 q^l and qxl+y1 = b2 +w2 q^xl y where q^l = l2/2 and q^xl y = l2/(2) for l  [L - 1] whereas q^0 = 1 and q^x0y = 0.

Derivation. In general, we have

1

Nl+1

E


zl+1(x), zl+1(y)

=

b2

+

w2 Nl

hl(x), hl (y)

.

(46)

Thus,

setting

l

=

0

we

obtain

q1

=

w2

+ b2

if

we

assume

E
x

x

2

=

N0,

and

qx1y

=

b2

since

E x, y = E x, E y = 0.
x,y

Recall (for l > 0) zl+1(x) = W l+1l ul(x) l + bl+1 where ul(x) = [zl(x) - µl]/sl so

1

Nl+1

E


zl+1(x), zl+1(y)

=

b2

+

w2 Nl

l ul(x)

l , l ul(y)

l

.

(47)

Therefore, setting x = y and taking expectation values over x gives,

ql+1

:=

1

Nl+1

E
x,

zl+1(x)

2

,

=

b2

+

w2 Nl

E
x,

hl(x)

2

,

= b2 + w2 q^l ,

q^l

:=

1 Nl

E
x,

hl(x)

2

,

=

w2

E
x,

l2

l ul(x)[1]

,

(48)
(49) (50) (51) (52)

w2 Dz l2 (l z) ,

(53)

= l2 , 2

(54)

where we have approximated each component of the random vector ul(x) as a standard Gaussian.

Similarly, taking expectations over x, y gives

qxl+y1

=

b2

+

w2

E
x,y,

l

ul(x)[1] l

l

ul(y)[1] l

.

(55)

Consider the approximation in which the random pair (ul(x)[1], ul(y)[1]) is Gaussian distributed with zero mean and covariance,

l :=

lxx lxy

lxy lyy

:=

E ul(x)[1]2 E ul(x)[1]ul(y)[1]

E ul(x)[1]ul(y)[1] E ul(y)[1]2

.

(56)

Then the recursion becomes

qxl+y1 b2 + w2 Dz1 Dz2 l xl x z1l l lyy clxy z1 + 1 - (clxy)2 z2 l , (57)

9

Under review as a conference paper at ICLR 2019

where clxy := xl y/ xl xyl y. Observe that by independence of x and y,

E
x,y

ul(x)ul(y)

=E
x,y

zl(x)zl(y) + µl2 - µl zl(x) + zl(y) sl2

,

=

Exzl(x)Eyzl(y) + µ2l - µl s2l

Exzl(x) + Eyzl(y)

=0 .

,

Thus lxy = 0 and consequently,

qxl+y1 = b2 + w2 q^xl y ,

q^xl y

=

l2 2

.

(58) (59) (60)
(61) (62)

Claim 6.2.

The backward recursions for l



[1, L - 1] are q~l

=

l2w2 q~l+1 2 ql

and q~xl y

=

l2w2 q~xl+y1 4 ql

with base cases q~L = q~xLy = 1.

Derivation. For each layer l  [L] and for each unit i  [Nl] define l[i]  RNL by

l[i]

:=

hL zl[i]

.

(63)

In particular, since we assume linear output L[i] = ei and thus q~L = E L[·] 2 = 1 ,
x,

(64)

where 1  RNL is the vector of ones. For ease of presentation and without loss of generality we restrict to the first component q~L[1] = 1 and abuse notation by writing q~L = 1. For l < L we have
by the chain rule,

l[i] =



zl+1[k] zl[i]

l+1

[k]

,

k[Nl+1 ]

=

l[i] sl[i]

l

ul[i] l[i]

[W l+1]kil+1[k] .

k[Nl+1 ]

(65) (66)

Hence,

l(x)[i]l(y)

=

l[i]2 sl[i]2

l

ul(x)[i] l[i]

l

ul(y)[i] l[i]

[W l+1]ki[W l+1]k il+1[k]l+1[k ] .

k,k [Nl+1]

(67)

Following the usual assumption that the back-propagated gradient is independent of the forward

signal we obtain,

E l(x), l(y)


=

w2 Nl

E
 i[Nl ]

l[i]2 sl[i]2

l

ul(x)[i] l[i] l

ul(y)[i] l[i]

E l+1(x), l+1(y)

(68)

Setting x = y and taking expectation values over x we obtain,

q~l := E l(x) 2 ,
x,

q~l+1 ql

l2w2

Dz l(lz)2 ,

= l2w2 q~l+1 . 2 ql

(69) (70) (71)

10

Under review as a conference paper at ICLR 2019

Similarly, taking expectation values over x, y we obtain,

q~xl y

=

E
x,y,

l(x), l(y)

,

=

w2 l2

q~xl+y1 ql

Dz1Dz2l

=

w2 l2 4

q~xl+y1 ql

.

xl xz1 l l

yl y cxl y z1 +

1 - (clxy)2 z2 l

(72) , (73)
(74)

6.2 RECURSIONS FOR CONVOLUTIONAL LAYERS

Recall that the output of a general convolutional layer is of the form,

zl+1(x) =

Wl+1hl+ (x) + bl+1 ,

Fl+1

(75)

where   Kl+1. As a concrete example, consider CIFAR-10 input of dimension 32 × 32 × 3 which is mapped by a convolutional layer with 3 × 3 kernels, stride 2 and 20 output channels and no
padding. Then C0 = 3, C1 = 20, |K0| = 1024 and |F1| = 9 and |K1| = 225.

We begin by deriving some useful identities for convolutional layers, before specializing to the batch-normalized and vanilla networks. For each channel i  [Cl], we have,

E


zl+1(x)[i]zl+1(y)[i]

=

b2

+

E


[Wl+1 1]ij1 [Wl+2 1]ij2 hl+1 (x)[j1]hl+2 (y)[j2] ,

(1 ,j1 )Fl+1 ×[Cl ]

(2 ,j2 )Fl+1 ×[Cl ]

=

b2

+

w2 Nl

Fl+1

E


hl+(x), hl +(y)

.

(76)

Hence,

1

Cl+1

E


zl+1(x), zl+1(y)

where Nl := Cl|Fl+1|.

=

b2

+

w2 Nl

Fl+1

E


hl+(x), hl +(y)

,

(77)

Moreover, let us introduce the following shorthand,

E


hl(x)

2 :=

1 |Kl| Kl

hl(x) 2 ,

E


hl(x), hl(y)

1 :=
|Kl| Kl

hl(x), hl(y)

,

E
=

hl (x), hl (y)

1 :=
|Kl|(|Kl| - 1) ,Kl×Kl:=

hl (x), hl(y)

Then we can write the recursion relations as,

ql+1

:=

1

Cl+1

EE
 x,

zl+1(x) 2

,



=

E
x

b2

+

w2 Nl

Fl+1

EE


hl+ (x)

2


,

=

b2

+

w2 Cl

E
x,

E


hl(x) 2

,

= b2 + w2 q^l ,

q^l

:=

1

Cl

E
x,

E


hl(x)

2

.

(78) (79) (80)
(81) (82) (83) (84) (85)

11

Under review as a conference paper at ICLR 2019

Furthermore we have,

qxl+y1

=

1

Cl+1

E


E
x,y,

zl+1(x), zl+1(y)

,



=

E
x,y

b2

+

w2 Nl

Fl+1

EE


hl+(x), hl+(y) 

,

= b2 + w2 q^xl y ,

q^xl y

:=

1

Cl

E


E
x,y,

hl (x), hl (y)

.

Finally, we have,

(86)
(87) (88) (89)

ql+1,xy

:=

E
=

E
x,y,

zl+1(x), zl+1(y)

,



=

E
x,y

b2

+

w2 Nl

EE
 = Fl+1

hl+(x), hl+(y)



,

=E
x,y

b2

+

w2

Cl

E


E
=

hl(x), hl (y)

= b2 + w2 q^l ,xy ,

q^l ,xy

:=

1

Cl

E
=

E
x,y,

hl(x), hl (y)

,

,

where we used,

hl +(x), hl+(y) =
Fl+1 ,Kl×Kl:=






hl +(x),

hl +(y)

Fl+1

Kl

Kl



- hl+(x), hl+(y) 
Kl


= |Fl+1| 

hl (x),

hl (y) -

hl (x), hl(y) 

Kl

Kl

Kl

= |Fl+1|

hl (x), hl (y) .

, Kl ×Kl :=

(90) (91) (92) (93) (94)
(95) (96) (97) (98)

6.2.1 BATCH NORMALIZATION
Claim 6.3. The forward recursions for 0  l  L - 1 are ql+1 = b2 + w2 q^l, qxl+y1 = b2 + w2 q^xl y and ql+1,xy = b2 + w2 q^l ,xy where for each l  [L - 1] we have q^l = l2/2 and q^xl y = q^l ,xy = l2/(2).
12

Under review as a conference paper at ICLR 2019

Derivation. Recalling that hl (x) = l ul (x)

l

where ul(x)

:=

zl (x)-µl sl

and substituting

into equation 85, equation 89 and equation 94 we obtain,

q^l Dz l2(lz) , = l2 , 2
q^xl y Dz1 Dz2 l(lz1)l(1z2) , = l2 , 2

(99) (100) (101) (102)

q^,xy

Dz1 Dz2 l(lz1)l(1z2) ,

= l2 , 2

(103) (104)

Claim 6.4.

The backward recursions are q~l

=

, q~w2 l2q~l+1
2ql

l xy

=

w2 l2q~xl+y1 4ql

,

and

q~l ,xy

=

.w2 l2q~l+1,xy
4ql

In order to derive the backward recursion, define (for each l  [L], i  [Cl],   Kl)

l [i]

=

hL zl [i]

.

(105)

Then by the chain rule,

l [i]

=

( ,k)Kl+1 ×[Cl+1 ]

zl+1[k] zl [i]

l+1

[k]

.

(106)

Now,

Thus,

zl+1[k] =

[Wl+1]kj l ul+ [j] l[j] + bl+1[k] ,

( ,j)Fl+1×[Cl]

zl+1[k] zl [i]

=

(

[Wl+1]kj l
,j )Fl+1 ×[Cl ]

ul + [j] l[j]

l[j] sl + [j]

ij

,+

= [Wl+-1 ]kil

ul[i] l[i]

l[i] sl [i]

.

,

(107) (108) (109)

l [i]

=

l[i] sl [i]

l

ul[i] l[i]

[Wl+1]kil+-1 [k] .
( ,k)Fl+1 ×[Cl+1 ]

(110)

By the distributional assumption on the weights, for each (, )  Kl × Kl we have,

E


[Wl+1 1]k1i[Wl+2 1]k2il+-11 [k1]l+-12 [k2]

(1 ,k1 )Fl+1 ×[Cl+1 ]

=

w2 Nl

E
 Fl+1

l+-1(x), l+-1(y)

.

(2 ,k2 )Fl+1 ×[Cl+1 ]

(111)

Thus, under the usual independence assumptions,

E


l (x), l (y)

=

w2 Nl

E
 i[Cl ]

l[i]2 sl [i]sl [i]

l

ul(x)[i] l[i] l

ul (y)[i] l[i]

E


l+-1(x), l+-1(y)

Fl+1

(112)

.

13

Under review as a conference paper at ICLR 2019

Setting  = , x = y, averaging over  and taking the expectation value over x,

q~l := E E
 x,

l (x) 2

,

=

w2 Nl

l2 2ql

Cl

E
x,

E


Fl+1

l -(x) 2

,

=

w2 Nl

l2 2ql

Cl

|Fl+1|

E


E
x,

l (x) 2

,

=

w2 l2q~l+1 2ql

.

Similarly, setting  = , averaging over  and taking expectation values over x, y gives

q~xl y

:=

EE
 x,y,

l (x), l (y)

=

w2 l2q~l+1 4ql

.

,

(113) (114) (115) (116)
(117) (118)

Now,

l+-1(x), l+-1(y)
, Kl+1 ×Kl+1 := Fl+1




=
Fl+1


l+-1 (x),

l+-1 (y)

Kl+1

Kl+1

- l+-1(x), l+-1(y)  ,
Kl+1


= |Fl+1| 

l+1(x),

l+1(y) -

l+1(x), l+1(y)  ,

Kl+1

Kl

Kl+1

= |Fl+1|

l+1(x), l+1(y)

, Kl+1 ×Kl+1 :=

.

(119)
(120) (121) (122)

Let us further assume that

1

|Kl|(|Kl|

-

1)

, Kl ×Kl :=

E
x,y,

l+1(x), l+1(y)

1

=

|Kl+1|(|Kl+1| - 1)

E
x,y, , Kl+1 ×Kl+1 :=

l+1(x), l+1(y)

.

(123) (124)

Thus, taking expectation values over (x, y) and averaging over the allowable indices such that  =  we obtain,

E
=

E
x,y,

l (x), l (y)

=

Cl|Fl+1| Nl

w2 l2 4ql

E
=

E
x,y,

l+1(x), l+1(y)

.

(125)

It follows that

q~l ,xy

:=

E
=

E
x,y,

l (x), l (y)

=

w2 l2q~l+1,xy 4ql

.

,

(126) (127)

14

Under review as a conference paper at ICLR 2019

6.2.2 VANILLA CNN
Claim 6.5. The forward recursions are ql+1 = b2 + w2 q^l, qxl+y1 = b2 + w2 q^xl y and ql+1,xy = b2 + w2 q^l ,xy where

q^l = 1 ql , 2

q^xl y

=

ql 2

1

-

cx2 y

+

cxy  2

+

cxy

sin-1 (cxy )

,

q^l ,xy

=

ql 2

1 - c2,xy

+

c,xy  2

+ c,xy

sin-1 (c,xy )

,

(128) (129) (130)

and cxy := qxl y/ql, c,xy := ql ,xy/ql.

Derivation. Substituting into equation 85, equation 89 and equation 94 we obtain,

q^l :=

Dz

l2

 ( qlz)

,

ql =,
2

q^xl y =

 Dz1 Dz2 l ql z1l l

 ql

cxl y z1 +

1 - (clxy)2 z2 l

,

ql =
2

1

- c2xy

+

cxy  2

+ cxy

sin-1 (cxy )

,

q^l ,xy =

 Dz1 Dz2 l ql z1l l

 ql

cl ,xy z1 +

1 - (cl ,xy)2 z2 l

ql =
2

1

- c2,xy

+

c,xy  2

+ c,xy

sin-1 (c,xy )

.

,

(131) (132) (133) (134) (135) (136)

Claim 6.6. The backward recursions are q~l =

q~ =l
,xy

w2 q~l+1,xy 2

 2

+ sin-1(c,xy)

.

,w2 q~l+1
2

q~xl y

=

w2 q~xl+y1 2

 2

+ sin-1(cxy)

and

Derivation. In the backward direction, we have

l [i]

=

( ,k)Kl+1 ×[Cl+1 ]

zl+1[k] zl [i]

l+1[k]

,

= l zl [i]

[Wl+1]kil+-1 [k]

( ,k)Fl+1 ×[Cl+1 ]

Under the usual independence assumptions,

E


l (x), l (y)

=

w2

Nl

E


l

zl (x)

, l

zl (y)

l+-1(x), l+-1(y) .
Fl+1

15

(137) (138)
(139)

Under review as a conference paper at ICLR 2019

Thus,

q~l := E E
 x,

l (x)

2

,

= w2 q~l+1 2

q~xl y

:=

EE
 x,y,

l (x), l (y)

,

= w2 q~xl+y1 2

 2

+

sin-1

cxy

q~l ,xy

:=

E
=

E
x,y,

l (x), l (y)

,

= w2 q~l+1,xy 2

 2

+

sin-1 c,xy

.

(140) (141) (142) (143) (144)
(145)

6.3 DERIVATION OF CLAIM 3.1

Recall that Karakida et al. (2018) bounds the maximum eigenvalue of the Fisher information matrix as,

¯ max (I )

=

1 n2

h(x), h(y) ,

(x,y)

1

= n2

l h(x), l h(y)

(x,y) l[L]

,

=: fl.
l[L]

(146) (147) (148)

For fully connected structure at layer l, we have

1 fl = n2
(x,y)

h(x) , h(y) + h(x) , h(y)

bl bl

Wl Wl

+ h(x) , h(y) + h(x) , h(y) ,

l l

l l

1 n2
(x,y)

h(x) , h(y) Wl Wl

,

1 = n2

hl-1(x), hl-1(y) l(x), l(y) ,

(x,y)

  

1  n2

hl-1(x), hl-1(y)

1   n2

l(x), l(y)  ,

(x,y)

(x,y)

= Nl-1

n(n - n2

1) q^xl-y1

+

1 n

q^l-1

n(n - n2

1) q~xl y

+

1 q~l n

,

Nl-1q^xl-y1q~xl y .

(149) (150) (151) (152)
(153) (154) (155)

In the first approximation we use the fact that the terms with respect to b, ,  are Nl times smaller than the term with respect to W . The second approximation uses the assumption that forward and
backward order parameters are independent. The last approximation is for m 1.

16

Under review as a conference paper at ICLR 2019

For convolutional layer, we have

1 fl = n2
(x,y)

h(x) , h(y) + h(x) , h(y)

bl bl

Wl Wl

(156)

+ h(x) , h(y) + h(x) , h(y) ,

l l

l l

(157)

1 n2
(x,y)

h(x) , h(y) Wl Wl

,

(158)

1 = n2

l (x), l (y) hl-+1(x), hl-+1 (y) ,

(x,y) Fl , Kl

(159)

 



1 |Kl|2

1  n2

(x,y) ,

Kl

l (x), l

(y)

1   n2

(x,y) Fl ,

Kl

hl-+1 (x), hl-+1

(y)



,

(160)

= |Kl|(|Kl| - 1)q~l ,xy + |Kl|q~xl y

Cl-1|Fl||Kl|(|Kl| |Kl|2

-

1) q^l-1,xy

+

Cl-1|Kl||Fl |Kl|2

|

q^xl-y 1

,

(161)

= Nl-1 (|Kl| - 1)q~l ,xy + q~xl y (|Kl| - 1)q^l-1,xy + q^xl-y1 ,

(162)

where Nl-1 = Cl-1|Fl|. In the first approximation we use the fact that the terms with respect to b, ,  are cl times smaller than the term with respect to W . The second approximation uses the assumption that forward and backward order parameters are independent.

17

