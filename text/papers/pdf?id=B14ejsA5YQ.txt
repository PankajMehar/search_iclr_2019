Under review as a conference paper at ICLR 2019
NEURAL CAUSAL DISCOVERY WITH LEARNABLE INPUT NOISE
Anonymous authors Paper under double-blind review
ABSTRACT
Learning causal relations from observational time series with nonlinear interactions and complex causal structures is a key component of human intelligence, and has a wide range of applications. Although neural nets have demonstrated their effectiveness in a variety of fields, their application in learning causal relations has been scarce. This is due to both a lack of theoretical results connecting risk minimization and causality (enabling function approximators like neural nets to apply), and a lack of scalability in prior causal measures to allow for expressive function approximators like neural nets to apply. In this work, we propose a novel causal measure and algorithm using risk minimization to infer causal relations from time series. We prove that under certain conditions, the positiveness of our measure can deduce a stringent direct structural causality. We demonstrate the effectiveness and scalablility of our algorithms to learn nonlinear causal models in synthetic datasets as comparing to other methods, and its effectiveness in inferring causal relations in a video game environment and real-world heart-rate vs. breath-rate and rat brain EEG datasets.
1 INTRODUCTION
From an early age, humans have a remarkable ability to infer causal relations from pure observations (White & Milne (1997); Scholl & Tremoulet (2000); Buchsbaum et al. (2012)). By observing that a left dot moving towards a right dot and the right dot moves correspondingly (the launching effect, see Michotte (1963)), a human can quickly infer that the left dot causes the right dot to move (Scholl & Tremoulet (2000)). In fact, much of our way of thinking is via cause and effect. Causal analysis also permits counterfactual reasoning, answering what would have happened if the cause had happened differently. The learning of causality also constitutes much of the scientific endeavor, for example finding the cause of a certain cancer (Bosch et al. (2002)), or discovering gene regulatory networks (Lozano et al. (2009)). In addition, causality plays a key role in neuroscience (Neves et al. (2008); Seth et al. (2015)), economics (e.g. Granger (1969); Stock & Watson (1989)) and finance (Hiemstra & Jones (1994); Granger et al. (2000)).
The study of causality has a long history, yet the application of neural nets to learning causal relations has been scarce. There have been various works that propose methods to infer causal structures with limited model space (e.g. Granger (1969)) which may not be able to model complex nonlinear causal relations, or propose measures to quantify causal strength (e.g. Schreiber (2000); Janzing et al. (2013)) that may not scale to high-dimensional data. For these, the use of universal functional approximators like neural nets may be beneficial in inferring causality, which motivate us to propose causal measures that are not only theoretically founded, but also amenable to the learning of function approximators. On the other hand, in the deep learning community, the learning of causal models has not been prevalent, in part due to the lack of theoretical understanding between learning a prediction model and learning a causal model. This also motivates us to propose causal measures, obtained via learning a prediction model, that can deduce causality in a stringent sense.
The contributions of this work is as follows:
· We propose a novel measure to quantify causality from observational data, and an effective empirical algorithm to estimate it. It is based on risk minimization of a prediction model, allowing function approximators such as neural nets to learn complex causal relationships.
1

Under review as a conference paper at ICLR 2019
· We prove that the causal measure has the property that under certain conditions, a positive value implies direct structural causality, which is an extension to Pearl causality in our system.
· We demonstrate on nonlinear synthetic datasets that our method outperforms other causal measures by a large margin, and can scale to a larger number of time series. We also demonstrate that our models are effective on real-world datasets.
2 RELATED WORK
The study of causality has a long history, and has been approached from different perspectives. Pearl (e.g. Pearl (2002; 2009); Pearl et al. (2009)) defines causality in terms of intervention and structural dependence, under the structural equation models (SEM). Granger (Granger (1969); Granger & Newbold (1986)) defines causality via prediction: if the prediction of Y via a linear model can be improved by including the information of X, then X causes Y in the Granger sense. Since its proposal, Granger causality has been widely applied in economics (e.g. Joerding (1986)) and neuroscience (e.g. Deshpande et al. (2009); Seth et al. (2015)). To learn nonlinear causal relations, later works also extend Granger causality to kernel methods (Ancona et al. (2004); Marinazzo et al. (2008a;b); Sindhwani et al. (2012)). To clear up the relations between these two notions of causality, White et al. (2011) provide conditions under which Granger causality can deduce direct structural causality in a general settable system framework (White & Chalak (2009)), with direct structural causality as a natural extension of Pearl causality in settable systems. Our method also utilizes the high-level idea of inferring causal relations via prediction, and building on the work of White et al. (2011) we propose a novel measure that can theoretically deduce direct structural causality under certain conditions.
In addition, various measures have been proposed to quantify causality. Schreiber (2000) proposes transfer entropy as measure for causality. It measures the mutual information between the current Y and the past of X, conditioned on the past of Y, to quantify the directional information transfer. As noted in Marinazzo et al. (2008a), Granger causality as defined in Granger & Newbold (1986) implies nonzero transfer entropy. Janzing et al. (2013) analyze several causal measures, and conclude that they are unsatisfactory measures of causal strength. They then propose causal influence, defined via the KL-divergence between the original joint distribution and the distribution with a set of causal arrows broken. We note that both the calculation of transfer entropy and causal influence requires density estimation of the full joint distributions of the input and output, which could easily become difficult in high dimensions. This motivates us to propose new causal measures based on risk minimization, which is much easier with high input dimensions. Besides, various other works have also proposed methods to infer causal structure under some specific conditions, for example, causal additive noise models (Hoyer et al. (2009)), information-geometric causal inference (Daniusis et al. (2012); Janzing et al. (2012)), dynamic causal modeling (Friston et al. (2003)), etc.
There has also been works that approach causality from a machine learning perspective, or have architectures that put causality to mind. Lopez-Paz et al. (2015) study causal inference as a supervised learning problem, where the pairwise causal directions are given as labels for training. Louizos et al. (2017) utilize a variational autoencoder (VAE) structure to estimate the unknown latent space summarizing the confounders and the causal effect. Kipf et al. (2018) propose a neural relational inference architecture, which simultaneously infers the interactions and learning the dynamics from observational data. Designing causal inference into the model architecture can also be beneficial in certain applications. For example, Kansky et al. (2017) propose a Schema Network that allows regression planning from a goal through causal chains, and demonstrates zero-shot learning in a suite of variations of Atari Breakout games.
3 METHOD
3.1 PROBLEM DEFINITION
Although causal data can be inferred from observations at individual time points, we are primarily concerned with time series. Time series have extra structure that is particularly useful for causal inference: causes must precede their effects. Therefore, we consider N time series
2

Under review as a conference paper at ICLR 2019

x(1), x(2), ...x(N), where each time series x(i) = (x(1i), x(2i), ...xt(i), ...) and each xt(i)  RM is an M -dimensional vector. Denote Xt(-i)1 = (xt(-i)K , xt(-i)K+1, ...xt(-i)1) with maximum time horizon of K, and Xt-1 = {Xt(-i)1}, i = 1, 2, ...N . We also denote X(t^-j)1 = Xt-1\Xt(-j)1, i.e. Xt-1 excluding Xt(-j)1, to notationally differentiate with the variable of interest Xt(-j)1. We assume that the time series is generated by a canonical settable system (White et al. (2011)). We adopt the settable
system (White & Chalak (2009)) paradigm due to the following reasons: (1) as a natural extension to
SEMs, it facilitates optimization, equilibrium, and learning; (2) it can formally link Granger causal-
ity with Pearl causality; (3) it is general enough to encompass a large number of practical scenarios,
including time series. Intuitively, each variable in a settable system can either be determined by
direct setting (which formalizes interventions), or by a response function on the settings of other
variables. A canonical settable system is a settable system where each variable's setting equals its
response, allowing the system to evolve naturally without intervention, formalizing time series. In this paper, we also assume causal sufficiency (Peters et al. (2017)), i.e., each time series x(i) can only be structurally caused by the time series from x(1), x(2), ...x(N), and generated by stationary
response functions hi that are unknown to the learner:

x(t1) := h1(Xt-1, u1) xt(2) := h2(Xt-1, u2) ...  xt(N) := hN (Xt-1, uN )

(1)

for t = K + 1, K + 2, ... . Here ui  RM , i = 1, 2, ...N are noise variables that are mutually independent, and also independent of any Xt(-i)1, xt(i), i  {1, 2, ...N }. For any i, j  {1, 2, ...N }, we assume that the variables (X(t^-j)1, Xt(-j)1, xt(i)) have probability density function P (Xt(^-j)1, Xt(-j)1, x(ti)). We will leave time series with hidden variables (therefore confounding can occur) for future work. We note that even without considering hidden variables, Eq. 1 is very general and already encompasses a wide range of scenarios.
In order to state the goal of the learner who wants to learn causality from the observations x(1), x(2), ...x(N), we need to rigorously define causality. Here, we restate the definitions of direct structural causality (White et al. (2011)) and Granger causality (Granger & Newbold (1986)) using our notations of the system Eq. 1, the former being a natural extension to Pearl causality (Pearl (2009)) in the settable systems.
Direct structural causality (White et al. (2011)) We say Xt(-j)1, j = i does not directly structurally cause xt(i), if for all possible values of X(t^-j)1 and ul, l  1, 2, ...N , the function Xt(-j)1  hi(Xt-1, ui) is constant in Xt(-j)1. Otherwise, we say Xt(-j)1 directly structurally causes xt(i).
Granger (1969) defines causality in terms of conditional expectations. Later, Granger & Newbold (1986) define it using conditional distributions. We use the latter definition, since it is more general, and also facilitates connection with direct structural causality, and the algorithms proposed in this paper. In this work, we only consider causality between different time series, i.e. requiring j = i as default.
Granger causality (Granger & Newbold (1986)) We say Xt(-j)1, j = i does not Granger-cause xt(i), if P (x(ti)|Xt(-j)1, X(t^-j)1) = P (xt(i)|X(t^-j)1), i.e. the conditional distribution function of x(ti) given Xt(-j)1, Xt(^-j)1 is identical to the conditional distribution function of x(ti) given Xt(^-j)1. Otherwise, we say Xt(-j)1 Granger-causes x(ti).
The goal of the learner is, given only x(1), x(2), ...x(N), determine for any xt(i), whether Xt(-j)1 directly structurally causes x(ti) for each j = 1, 2, ...N .
3

Under review as a conference paper at ICLR 2019

3.2 GRANGER CAUSALITY IMPLIES DIRECT STRUCTURAL CAUSALITY IN EQ. (1)
For our system Eq. (1), applying the results by White et al. (2011), we have that Granger causality is a sufficient condition for direct structural causality. See Appendix A for the detailed theorem and proof.
For system Eq. 1, for any i, j  {1, 2, ...N }, i = j, if Xt(-j)1 Granger-causes x(ti), then Xt(-j)1 directly structurally causes xt(i).
The reason that here Granger causality can deduce direct structural causality is in part due to the fact that for system Eq. (1), conditional exogeneity (White et al. (2011)) is automatically satisfied by the assumptions of the system.
Note that the reverse of the statement is not true, i.e. Granger non-causality does not necessarily imply direct structural non-causality (White & Lu (2010) give several examples). They also note that these instances are exceptional, in that mild perturbations to their structures destroy the Granger non-causality.

3.3 PROPOSING OUR METHOD

Based on section 3.2, if we have an algorithm that can deduce Granger causality in system Eq. (1), then we can immediately deduce direct structural causality. In general, a direct test of Granger causality is difficult. In particular, when the number of time series is large or the dimension M of each time series is large, density or mutual information-based methods like transfer entropy (I(xt(i); Xt(-j)1|X(t^-j)1), see Schreiber (2000)), may not give a good estimate. Alternatively, various works have resorted to testing whether the prediction of xt(i) given Xt(-j)1, X(t^-j)1 is better than given only X(t^-j)1, under some limited functional space. For example, in his original work, Granger (1969) investigates causality with linear function predictors. Later works have extended it to kernel methods, e.g., Ancona et al. (2004); Marinazzo et al. (2008a;b); Sindhwani et al. (2012), which essentially estimate linear Granger causality on the feature space of the kernel.
To enable causal learning with potentially highly nonlinear response functions, it may be desirable to use universal function approximators (Hornik (1991)) such as neural nets. As the main contribution of this paper, we will provide a novel causal measure and corresponding algorithm to estimate it. The measure is based on risk minimization (thus allowing training with neural nets), and we rigorously prove that it can consistently infer direct structural causality via Granger causality.
Our algorithm is inspired by asking a counterfactual question during the learning of a prediction model. Specifically, it asks:
Question: How much noise can I add to Xt(-j)1, without making the best prediction of xt(i) worse?
To give a quantitative answer to this question, we define a learnable noise risk:

RX,x(i) [f, ] = EXt-1,xt(i),

x(ti) - f(X~ t(-)1) 2

N

+·

I (X~t(-j)1(j ); Xt(-j)1)

j=1

(2)

where X~ (t-)1 := Xt-1 + 

(or element-wise, X~t(-j)1(j) := Xt(-j)1 + j · j , j = 1, 2, ...N ) is

the noise-corrupted inputs with learnable noise amplitudes j  RKM , and j  N (0, I).  > 0

is a positive hyperparameter. Intuitively, the minimization of the second term I(X~t(-j)1(j); Xt(-j)1) requires the noise amplitude j to go up. The minimization of the first term requires the noise

amplitude j to go down, and the larger causal strength from Xt(-j)1 to xt(i), the larger this force.
The minimization of the two terms strikes a balance, at which point the I(X~t(-j)1(j); Xt(-j)1) measures how many bits of information does time series j need to provide to the learner, without making the

prediction worsened. Thus, we propose to use

Wji = I (X~t(-j)1(j); Xt(-j)1)

(3)

4

Under review as a conference paper at ICLR 2019

Algorithm 1 Neural Causal Discovery with Learnable Input Noise
Require xt(i), Xt-1, for i  {1, 2, ...N }, t  T = {K + 1, K + 2, ...}. Require 0: a small value for initialization of . Require : coefficient for the mutual information term. 1: for i in {1, 2, ...N } do:
2: Initialize function approximator f. 3: Initialize  = (1, 2, ...N ) = (01, 01, ...01), where each element 01 is a KM -
dimensional vector, same dimension as Xt(-j)1. 4: (f , )  Minimize(f,)R^X,x(i), [f, ] (Eq. 4) with e.g. gradient descent 5: Wji  I(X~t(-j)1(j); Xt(-j)1), for j = 1, 2, ...N . 6: end for
7: return W

as another measure for causality, where (f , ) = argmin(f,)RX,x(i) [f, ] 1. Assuming that f has enough capacity, we have that the measure Wji has the following property:
(1) Wji  0 (2) Suppose that Xt(-j)1 has finite variance. If Wji > 0, then Xt(-j)1 directly structurally causes
x(ti)
The proof of the properties is provided in Appendix B. Empirically, we minimize the following empirical risk.

R^ X,x(i) ,

[f, ]

=

1 |T|

x(ti) - f(X~ t(-)1)

2
+

N

I (X~t(-j)1(j ); Xt(-j)1)

tT

j=1

(4)

Due to the simple form of I(X~t(-j)1(j); Xt(-j)1), if we assume that Xt(-j)1 approximately obeys a diag-

onal Gaussian distribution, we can further simplify I(X~t(-j)1(j); Xt(-j)1)

1 2

KM l=1

log(1

+

1/j2,l),

where j,l

=

j,l std(Xt(-j )1,l )

is the relative noise scale w.r.t.

the standard deviation of each element, l

denoting the lth element. We will use this approximation in this paper. When the dimension of Xt(-j)1

is large, differentiable estimate of the mutual information, such as MINE (Belghazi et al. (2018)),

can be applied. We provide Algorithm 1 to empirically estimate Wji.

4 EXPERIMENTS
To demonstrate that our proposed method works, we test it on both synthetic and real datasets. We first use synthetic datasets, where we know the underlying causal structure and compare with previous causal measures. We then test whether our algorithm can infer causal structure from watching an agent playing video games. Finally, we apply our algorithm to real-world heart-rate vs. breath-rate and rat EEG datasets to test its effectiveness. The metrics we use are the standard metrics of area under the precision-recall curve (AUC-PR) (Davis & Goadrich (2006)), and area under the ROC curve (AUC-ROC).

4.1 SYNTHETIC EXPERIMENT WITH LOG-NORMAL CAUSAL STRENGTH
In this experiment, we evaluate our method together with other methods with a nonlinear synthetic dataset generated to have a known causal structure (hidden to the methods being compared). We study how they perform with varying number N of time series, with N up to 30. To generate the data, we let each xt(i) have dimension M = 1, and also set the maximum time horizon K = 3, so
1Note also that throughout this paper, when talking about causal matrices, the (j, i)th element always denotes the causal strength from j to i.

5

Under review as a conference paper at ICLR 2019

each Xt(-j)1 is a K × M = 3 × 1 matrix. We use the following realization of the response function hi in Eq. (1):


N

xt(i) = hi(Xt-1, ut) = H1 

Aji

j=1

H2(Bj

 Xt(-j)1)  + ut, i = 1, 2, ...N

(5)

where ut  N (0, I)  RM , denoting element-wise multiplication, and H1 and H2 are two nonlinear functions to make the response functions nonlinear. In this experiment, we use H1(x) = softplus(x) = log(1 + ex), and H2(x) = tanh(x), we also find similar performance with other choices of nonlinear functions. Bj is a K × M random matrix, whose element is sampled from U [-1, 1]. Aji is a K × M matrix, with 0.5 probability of being a zero matrix and 0.5 probability of being a nonzero random matrix, characterizing the underlying causal strength from j to i. Crucially,
to reflect that the causal strength may span different orders of magnitude, if Aji is sampled to be a
nonzero matrix, then the amplitude of each of its element is sampled from a log-normal distribution
with µ = 1,  = 0, their sign sampling from U {-1, 1}. Denote Aindi as the 0-1 indicator matrix of causality (Aindi,ji = 1 if |Aji| > 0; 0 otherwise). The goal of each algorithm being evaluated is to produce an N × N causal matrix A~, where each entry A~ji characterizes the causal strength from j to i. Then the flattened A~ is evaluated against the flattened Aindi (excluding diagonal elements of the matrices), producing the metrics of AUC-PR and AUC-ROC for each i.

In general, for a large N , the number of possible causal graphs grows double exponentially: there are 2N2 possible matrix of Aindi. To give an estimate, for N = 3, 4, 5, 8, 10, 20, 30, there are 512, 6.6 × 104, 3.3×107, 1.8×1019, 1.2×1030, 2.6×10120, 8.5×10270 number of possible graphs, respectively. Therefore, estimating the causal graph is in general a non-trivial task when N is large. We compare
our algorithm, with previous methods including transfer entropy (Schreiber (2000)), causal influence
(Janzing et al. (2013)), linear Granger causality (Granger (1969); Ding et al. (2006)), and a baseline of mutual information (which gives A~ji = A~ji). The implementation details for each method and each subsection are provided in Appendix C and Appendix D, respectively. Table 1 and 2 shows the average AUC-PR and AUC-ROC with each N , each with 4 random initializations of the true underlying causal matrix A and dataset.

method

N

Ours Transfer Entropy Mutual Information Linear Granger Causal Influence

3
0.9757 0.9401 0.9139 0.7262 0.7910

4
0.9820 0.9308 0.9318 0.9431 0.7068

5
0.9604 0.9059 0.8775 0.8054 0.5481

8
0.9424 0.7376 0.8292 0.8062 0.3693

10
0.9302 0.6764 0.8058 0.7566 0.4149

15
0.9266 0.6197 0.7687 0.6589 0.4798

20
0.8614 0.5723 0.7147 0.6643 0.4612

30
0.7740 0.4818 0.6980 0.5071 0.4254

Table 1: Average AUC-PR with larger N , with random sampling of Aindi. Bold font marks the top method for each N .

Method

N

Ours Transfer Entropy Mutual Information Linear Granger Causal Influence

3
0.9722 0.8854 0.8889 0.6806 0.7083

4
0.9580 0.8469 0.8817 0.8969 0.6334

5
0.9525 0.9035 0.8611 0.7787 0.6077

8
0.9406 0.7893 0.8414 0.8166 0.4155

10
0.9382 0.7731 0.8403 0.7881 0.4705

15
0.9251 0.6609 0.7711 0.6887 0.5707

20
0.8546 0.6006 0.7380 0.6777 0.5153

30
0.7810 0.5275 0.7156 0.5840 0.5218

Table 2: Average AUC-ROC with larger N , with random sampling of Aindi. Bold font marks the top method for each N .
We see that not only does our method outperform other methods by a large margin across all N s, it also shows good performance when N is as large as 30, demonstrating our method's capability and scalability to infer complex causal structures from interacting time series. For the Causal

6

Under review as a conference paper at ICLR 2019

Influence method, although it has very good mathematical properties, it may be impractical in practice, as is also shown in the table. This is due to that it is defined as the KL-divergence between (Xt-1, xt(-i)1) and (X(t~-j)1, xt(-i)1), each of which is an (N K + 1)M -dimensional vector, which can quickly go to high dimension, where density estimation required to calculate KL-divergence is in general data-hungry and difficult. In comparison, our method that estimates causal strength via minimizing prediction errors is comparatively easier in high dimensions (only have to predict a M dimensional vector conditioned on the inputs), which contributes to a better performance when N is large.
Since in practice, we do not know the underlying causal structure a priori, it presents a greater challenge to select the model capacity for f, as compared with supervised learning method where we can do cross-validation. To see how the capacity of the function approximator f influences our method, we vary the number of layers and the number of neurons in each layer at N = 10. Table 3 summarizes the result. We see that our method's performance here is hardly influenced by the model capacity, with only a slight degradation at very low capacity. This shows that our method is quite tolerant and stable with model capacity variations.

Neurons in hidden layers
(8) (8, 8) (16, 16) (8, 8, 8) (16, 16, 16) (8, 8, 8, 8) (16, 16, 16, 16)

AUC-PR
0.8969 0.9309 0.9404 0.9339 0.9284 0.9312 0.9199

AUC-ROC
0.9086 0.9388 0.9455 0.9410 0.9288 0.9350 0.9202

Table 3: Average AUC-PR and AUC-ROC for different network structures for N = 10 with our method. Here for example, (8, 8, 8) means that the f has 3 hidden layers, each with 8 neurons.

4.2 EXPERIMENTS WITH VIDEO GAMES
action paddle ball-x ball-y brick reward action paddle ball-x ball-y brick reward

5 4 3 2 1 0

Figure 1: Causal strength Wji inferred by our method with watching a trained CNN playing Breakout. The (j, i) element denotes the inferred causal strength from j to i.

To see how our method can infer causal relations in real videos games, and potentially helping
reinforcement learning (RL) or imitation learning, we apply our method to the causal inference
between the trajectories of different objects from a trained CNN RL-agent playing Atari Breakout
games (Bellemare et al. (2013), implementation details see Appendix E). Fig 1 shows the inferred Wji matrix, with the (j, i)th element denoting the inferred causal strength from j to i. We see that there is a prominent causal direction from the ball's y position to the reward, which correctly

7

Wji

Under review as a conference paper at ICLR 2019
3.0 Breath Heart 2.5 Heart Breadth
2.0
1.5
1.0
0.5
0.00 5 10K 15 20
Figure 2: Causal strength Wji inferred by our method with the heart-rate vs. breath-rate dataset, averaged over 50 initializations of f. The shaded areas in this figure and Fig. 3 are the 95% confidence interval.
summarizes that the ball's y position has a large influence on the reward. Additionally, the other discovered causal directions with causal strength greater than 1 include: brick  reward, ball-y  brick, reward  paddle, ball-x  action, ball-y  action. The former two correctly summarize causal chains from ball-y  brick  reward, the latter two show that the ball's x and y positions also have big causal influences on the trained agent's action: in order that the ball does not fall to the bottom, the agent has to position itself at the right position depending on the x and y positions of the ball.
4.3 HEART-RATE VS. BREATH-RATE AND RAT BRAIN EEG DATASETS
Now we test our algorithm with real-world datasets. As a common dataset studied in previous causal works, we use the time-series of the breathing rate and instantaneous heart rate of a sleeping patient suffering from sleep apnea (samples 2350-3550 of data set B from Santa Fe Institute time series contest held in 1991, available in PhysioNet). We apply our method to infer the causal strength between the breathing rate and heart rate, with different maximum time horizon K. The result is shown in Fig. 2. The causal strength from heart to breath is significantly higher than the reverse direction, consistent with the results from previous causal inference methods (Schreiber (2000); Ancona et al. (2004); Marinazzo et al. (2008a)). Notably, the causal strength remains at roughly the same level for different Ks, showing a merit of our method in estimating causal strength across different time-horizons, aided by the flexibility of f in extracting the right information to predict the future. The implementation details in this section is provided in Appendix F.
As a second real-world example, we apply our algorithm in estimating the causal strength of the EEG signals between the right and left cortical intracranial electrodes (Quiroga), also studied in Ancona et al. (2004); Quiroga et al. (2002); Marinazzo et al. (2008a). Figure 3 (left) shows the inferred causal strength Wji for the EEG signals of a normal rat. We see that there is only a slight asymmetry, with the right channel having a slightly stronger influence on the left channel than the reverse direction. Figure 3 (right) shows Wji for the EEG signals with unilateral lesion in the rostral pole of the reticular thalamic nucleus. We see that there is stronger causal influence from the left to the right channels, in consistent with previous analysis in Ancona et al. (2004); Quiroga et al. (2002); Marinazzo et al. (2008a). The above two applications demonstrate our method's capability in inferring the causal relations from noisy, real-world data.
5 CONCLUSION
In this paper, we have addressed causal inference by proposing a novel causal measure, defined via a novel learnable noise risk, that allows function approximators like neural nets to learn complex causal relations. We proved that under certain conditions, the positiveness of our measure implies
8

Under review as a conference paper at ICLR 2019

Wji Wji

3.5 Before Lesion

3.5 After Lesion

3.0 3.0

2.5 2.5

2.0 2.0

1.5 1.5

1.0

Left Right

1.0

Left Right

0.5

Right Left

0.5

Right Left

0.00 5 10K 15 20 0.00 5 10K 15 20

Figure 3: Causal strength inferred by our method with the EEG datasets, for different maximum time
horizon K, averaged over 50 initializations of f. (Left) Causal strength Wji for the EEG signal for a normal rat. (Right) Causal strength Wji of EEG signal from the same rat, after brain lesion.

direct structural causality, providing a solid theoretical bridge between risk minimization and causal inference. We demonstrated in synthetic nonlinear datasets that our method outperforms previous methods by a large margin in inferring causal relations with complex structures, can scale to a large number of time series, and is hardly influenced by the model capacity of the function approximator. Our method also correctly infers the causal arrows by watching a trained CNN playing Breakout, and give causal directions consistent with prior works in real-world heart-rate vs. breath-rate and rat EEG datasets. We believe our work can pave the way for future exciting advancements in enabling machine learning models to understand causality, a key component of human intelligence.
REFERENCES
Nicola Ancona, Daniele Marinazzo, and Sebastiano Stramaglia. Radial basis function approach to nonlinear granger causality of time series. Physical Review E, 70(5):056221, 2004.
Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, and Aaron Courville. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253­279, 2013.
Francesc X Bosch, Attila Lorincz, Nubia Muñoz, CJLM Meijer, and Keerti V Shah. The causal relation between human papillomavirus and cervical cancer. Journal of clinical pathology, 55(4): 244­265, 2002.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Daphna Buchsbaum, Sophie Bridgers, Deena Skolnick Weisberg, and Alison Gopnik. The power of possibility: Causal learning, counterfactual reasoning, and pretend play. Philosophical Transactions of the Royal Society of London B: Biological Sciences, 367(1599):2202­2212, 2012.
Povilas Daniusis, Dominik Janzing, Joris Mooij, Jakob Zscheischler, Bastian Steudel, Kun Zhang, and Bernhard Schölkopf. Inferring deterministic causal relations. arXiv preprint arXiv:1203.3475, 2012.
Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In Proceedings of the 23rd international conference on Machine learning, pp. 233­240. ACM, 2006.
9

Under review as a conference paper at ICLR 2019
Gopikrishna Deshpande, Stephan LaConte, George Andrew James, Scott Peltier, and Xiaoping Hu. Multivariate granger causality analysis of fmri data. Human brain mapping, 30(4):1361­1373, 2009.
Mingzhou Ding, Yonghong Chen, and Steven L Bressler. Granger causality: basic theory and application to neuroscience. Handbook of time series analysis: recent theoretical developments and applications, pp. 437­460, 2006.
Carlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efficient reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pp. 240­247. ACM, 2008.
Karl J Friston, Lee Harrison, and Will Penny. Dynamic causal modelling. Neuroimage, 19(4): 1273­1302, 2003.
Clive Granger and Paul Newbold. Forecasting Economic Time Series. Elsevier, 2 edition, 1986. URL https://EconPapers.repec.org/RePEc:eee:monogr:9780122951831.
Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: Journal of the Econometric Society, pp. 424­438, 1969.
Clive WJ Granger, Bwo-Nung Huangb, and Chin-Wei Yang. A bivariate causality between stock prices and exchange rates: evidence from recent asianfluâYE. The Quarterly Review of Economics and Finance, 40(3):337­354, 2000.
Craig Hiemstra and Jonathan D Jones. Testing for linear and nonlinear granger causality in the stock price-volume relation. The Journal of Finance, 49(5):1639­1664, 1994.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4 (2):251­257, 1991.
Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear causal discovery with additive noise models. In Advances in neural information processing systems, pp. 689­696, 2009.
Dominik Janzing, Joris Mooij, Kun Zhang, Jan Lemeire, Jakob Zscheischler, Povilas Daniusis, Bastian Steudel, and Bernhard Schölkopf. Information-geometric approach to inferring causal directions. Artificial Intelligence, 182:1­31, 2012.
Dominik Janzing, David Balduzzi, Moritz Grosse-Wentrup, Bernhard Schölkopf, et al. Quantifying causal influences. The Annals of Statistics, 41(5):2324­2358, 2013.
Wayne Joerding. Economic growth and defense spending: Granger causality. Journal of Development Economics, 21(1):35­40, 1986.
Ken Kansky, Tom Silver, David A Mély, Mohamed Eldawy, Miguel Lázaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-shot transfer with a generative causal model of intuitive physics. arXiv preprint arXiv:1706.04317, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. arXiv preprint arXiv:1802.04687, 2018.
Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In Advances in Neural Information Processing Systems, pp. 971­980, 2017.
Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004.
Joseph T Lizier, Mikhail Prokopenko, and Albert Y Zomaya. Local information transfer as a spatiotemporal filter for complex systems. Physical Review E, 77(2):026110, 2008.
10

Under review as a conference paper at ICLR 2019
David Lopez-Paz, Krikamol Muandet, Bernhard Schölkopf, and Iliya Tolstikhin. Towards a learning theory of cause-effect inference. In International Conference on Machine Learning, pp. 1452­ 1461, 2015.
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6446­ 6456. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7223-causal-effect-inference-with-deep-latent-variable-models. pdf.
Aurélie C Lozano, Naoki Abe, Yan Liu, and Saharon Rosset. Grouped graphical granger modeling for gene expression regulatory networks discovery. Bioinformatics, 25(12):i110­i118, 2009.
Daniele Marinazzo, Mario Pellicoro, and Sebastiano Stramaglia. Kernel method for nonlinear granger causality. Physical review letters, 100(14):144103, 2008a.
Daniele Marinazzo, Mario Pellicoro, and Sebastiano Stramaglia. Kernel-granger causality and the analysis of dynamical networks. Physical review E, 77(5):056215, 2008b.
A Michotte. The perception of causality. 1963.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
M Morf, A Vieira, T Kailath, et al. Covariance characterization by partial autocorrelation matrices. The Annals of Statistics, 6(3):643­648, 1978.
Guilherme Neves, Sam F Cooke, and Tim VP Bliss. Synaptic plasticity, memory and the hippocampus: a neural network approach to causality. Nature Reviews Neuroscience, 9(1):65, 2008.
A Papoulis. Probability, random variables and stochastic processes. 1985.
Judea Pearl. Causality: models, reasoning, and inference. IIE Transactions, 34(6):583­589, 2002.
Judea Pearl. Causality. Cambridge university press, 2009.
Judea Pearl et al. Causal inference in statistics: An overview. Statistics surveys, 3:96­146, 2009.
Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. MIT press, 2017.
PhysioNet. Physionet data bank. URL http://www.physionet.org/.
R Quian Quiroga, A Kraskov, T Kreuz, and Peter Grassberger. Performance of different synchronization measures in real data: a case study on electroencephalographic signals. Physical Review E, 65(4):041903, 2002.
Rodrigo Quian Quiroga. The dataset can be downloaded from. URL www.vis.caltech.edu/ ~rodri.
Brian J Scholl and Patrice D Tremoulet. Perceptual causality and animacy. Trends in cognitive sciences, 4(8):299­309, 2000.
Thomas Schreiber. Measuring information transfer. Physical review letters, 85(2):461, 2000.
Anil K Seth, Adam B Barrett, and Lionel Barnett. Granger causality analysis in neuroscience and neuroimaging. Journal of Neuroscience, 35(8):3293­3297, 2015.
Vikas Sindhwani, Minh Ha Quang, and Aurélie C Lozano. Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality. arXiv preprint arXiv:1210.4792, 2012.
11

Under review as a conference paper at ICLR 2019 James H Stock and Mark W Watson. Interpreting the evidence on money-income causality. Journal
of Econometrics, 40(1):161­181, 1989. Halbert White and Karim Chalak. Settable systems: an extension of pearl's causal model with
optimization, equilibrium, and learning. Journal of Machine Learning Research, 10(Aug):1759­ 1799, 2009. Halbert White and Xun Lu. Granger causality and dynamic structural systems. Journal of Financial Econometrics, 8(2):193­243, 2010. Halbert White, Karim Chalak, and Xun Lu. Linking granger causality and the pearl causal model with settable systems. In NIPS Mini-Symposium on Causality in Time Series, pp. 1­29, 2011. Peter A White and Alan Milne. Phenomenal causality: Impressions of pulling in the visual perception of objects in motion. The American journal of psychology, 110(4):573, 1997.
12

Under review as a conference paper at ICLR 2019

Appendix
A THEOREM 1 AND PROOF
Here, we formally propose a theorem corresponding to the statement in section 3.2, and provide proof afterwords.
Theorem 1. For system Eq. 1, for any i, j  {1, 2, ...N }, i = j, if Xt(-j)1 Granger-causes xt(i), then Xt(-j)1 directly structurally causes xt(i).
Proof. We base the proof on the Theorem 5.6 in White et al. (2011). Firstly, by definition, the system Eq. (1) belongs to the canonical settable system (Def. 3.3 in White et al. (2011)), on which their Theorem 5.6 is based. To prove that in our system Granger causality can deduce direct structural causality, we only have to prove that the assumption A.1 and assumption A.2 in White et al. (2011) are satisfied by our system. If we identify our xt(i) with their Y1,t, our Xt-1 with their Yt-1, our x(tj) with their Y2,t, our ui,t (our ui at time t) with their U1,t, our uj,t with their U2,t, their Zt = , Wt = , then our system Eq. (1) satisfies their Assumption A.1. Additionally, by definition, our ui  RM , i = 1, 2, ...N are random variables that are mutually independent, and also independent of any Xt(-i)1, x(ti), i  {1, 2, ...N }. Therefore, our system satisfies their strict exogeneity (Yt-1, Zt)  U1,t (in our representation (Xt-1, )  ui,t), which is a sufficient condition for Assumption A.2. Therefore, both their Assumption A.1 and Assumption A.2 are satisfied by our system Eq. (1). Applying their Theorem 5.6, we prove Theorem 1.

B PROOF OF THE PROPERTIES OF Wji
Firstly we state the assumption that will be used through the proof of the properties of Wji: Assumption 1. Assume that f  F has enough capacity so that it can approximate any
dx(ti)P (x(ti)|Xt-1)xt(i). Let j = i and assume that P (Xt(-j)1) has support with intrinsic dimension of KM .
Also we emphasize that in this paper, the expected risks (with symbol R) are w.r.t. the distributions, and the empirical risks (with symbol R^) are w.r.t. a dataset drawn from the distribution, with finite number of examples. The theorems in this paper are all proved w.r.t. distributions (assuming infinite number of examples). Sample complexity results will be left for future work.
The structure of this section is as follows. First in subsection B.1 we prove a lemma that will be crucial in proving the properties. Then in subsection B.2 and B.3, we prove the two properties one by one.

B.1 PROVING A LEMMA Here we prove Lemmas 1.1. Lemma 1.1. Suppose that Assumption 1 holds, we have
argminf RX,x(i) [f] = dxt(i)P (xt(i)|Xt-1)xt(i)

(6)

and

2

minf RX,x(i) [f] = EXt-1,xt(i)

x(ti) - dxt(i)P (x(ti)|Xt-1)xt(i)

(7)

In other words, for the MSE risk, its minimum is attained when f(Xt-1) is the expectation of P (x(ti)|Xt-1).

13

Under review as a conference paper at ICLR 2019

Proof. The proof of the lemma is adapted from Papoulis (1985). The risk

RX,x(i) [f ] = EXt-1,xt(i)

xt(i) - f(Xt-1) 2

= dXt-1dxt(i) · P (Xt-1, xt(i)) x(ti) - f(Xt-1) 2

= dXt-1P (Xt-1) dxt(i)P (x(ti)|Xt-1) x(ti) - f(Xt-1) 2

Note that here (xt(i) - f(Xt-1))2  xt(i) - f(Xt-1), xt(i) - f(Xt-1) is an inner product in RM .

For any Xt-1, treating f(Xt-1)  RM as a vector, let's calculate its value such that the integral

F (f(Xt-1)) :=

dxt(i)P (x(ti)|Xt-1)

xt(i) - f(Xt-1)

2
attains its minimum.

Let 
0 = f(Xt-1) F (f(Xt-1))

 =
 f (Xt-1 )

dx(ti)P (xt(i)|Xt-1) xt(i) - f(Xt-1) 2

= -2 dxt(i)P (xt(i)|Xt-1) xt(i) - f(Xt-1)

we have

dx(ti)P (xt(i)|Xt-1)xt(i) = dxt(i)P (x(ti)|Xt-1)f(Xt-1)

= f(Xt-1) dx(ti)P (xt(i)|Xt-1)

= f(Xt-1)

Therefore, for any Xt-1, f(Xt-1) = dx(ti)P (xt(i)|Xt-1)xt(i) is the only stationary point for F (f(Xt-1)).

Taking the second derivative, we have

2 (f(Xt-1))2 F (f(Xt-1)) = 2

dx(ti)P (xt(i)|Xt-1)I = 2I

where I is an M × M identity matrix, which is always positive definite.

Therefore, for any Xt-1, f(Xt-1) = F (f(Xt-1)) w.r.t. f(Xt-1).
Since RX,x(i) [f] =

dx(ti)P (x(ti)|Xt-1)x(ti) is the only global minimum of dXt-1P (Xt-1)F (f(Xt-1))

The minimum of the risk RX,x(i) [f] is attained iff F (f(Xt-1)) attains minimum at every Xt-1, i.e.,
f(Xt-1) = dx(ti)P (xt(i)|Xt-1)xt(i)
is true for any Xt-1. Given Assumption 1, we know that f  F has enough capacity such that it can approximate any dxt(i)P (x(ti)|Xt-1)x(ti). Therefore,
argminf RX,x(i) [f] = dx(ti)P (xt(i)|Xt-1)x(ti)

and

2

minf RX,x(i) [f] = EXt-1,x(ti)

x(ti) - dx(ti)P (x(ti)|Xt-1)xt(i)

14

Under review as a conference paper at ICLR 2019
B.2 PROOF OF THE PROPERTY (1) OF Wji We state the theorem: Theorem 2. (1) Wji  0; (2) Suppose that Assumption 1 holds, then Wji is reparameterizationinvariant, i.e. Wji is invariant under any invertible transformation g: Xt(-l)1  g(Xt(-l)1), l.

Proof. The proof follows directly that Wji is the mutual information between two continuous variables, which is always non-negative.
B.3 PROOF OF THE PROPERTY (2) OF Wji
Theorem 3. Suppose that Assumption 1 holds, and Xt(-j)1 has finite variance. If Wji > 0, then Xt(-j)1 directly structurally causes x(ti).

Proof. Assuming Assumption 1. Suppose the hypothesis that Xt(-j)1 does not directly structurally cause x(ti) is true, with Theorem 1, we have Xt(-j)1 does not Granger-cause x(ti). We only need to prove that this can deduce Wji = 0. With Theorem 2, we also know that Wji  0 always holds. Contrapositively, if Wji > 0, it directly follows that the hypothesis that Xt(-j)1 does not directly structurally cause xt(i) is not true, i.e. Xt(-j)1 directly structurally causes x(ti).
Now we prove that Xt(-j)1 does not Granger-cause xt(i) can deduce Wji = 0. We have

RX,x(i) [f , ]  EXt-1,xt(i),

xt(i) - f(X~ (t-)1) 2

N

+·

I (X~t(-k)1(k); Xt(-k)1)

k=1

 dXt(-j)1dX(t^-j)1dx(ti)d X d S P (Xt(-j)1, X(t^-j)1, xt(i))P ( X )P ( S ) xt(i) - f(Xt(-j)1 + X · X , X(t^-j)1 + S · S ) 2

N

+·

I (X~t(-k)1(k); Xt(-k)1)

k=1

N

=·

I(X~t(-k)1(k); Xt(-k)1) +

k=1

dXt(-j)1dX(t^-j)1d X d S P (Xt(-j)1, X(t^-j)1)P ( X )P ( S ) xt(i) - f(Xt(-j)1 + X · X , X(t^-j)1 + S · S ) 2

dxt(i)P (xt(i)|Xt(-j)1, Xt(^-j)1)·

15

Under review as a conference paper at ICLR 2019

If Xt(-j)1 does not Granger-cause xt(i), i.e. P (x(ti)|Xt(-j)1, Xt(^-j)1) = P (xt(i)|X(t^-j)1), substituting into the last formula, we have

RX,x(i) [f, ]

N

=·

I(X~t(-k)1(k); Xt(-k)1) +

k=1

dXt(-j)1dX(t^-j)1d X d S P (Xt(-j)1, X(t^-j)1)P ( X )P ( S ) x(ti) - f(Xt(-j)1 + X · X , Xt(^-j)1 + S · S ) 2

dxt(i)P (x(ti)|X(t^-j)1)·

 minf RX,x(i) [f, ]

N

=·

I(X~t(-k)1(k); Xt(-k)1) +

k=1

dXt(-j)1dX(t^-j)1d X d S P (Xt(-j)1, X(t^-j)1)P ( X )P ( S ) x(ti) - fS (X(t^-j)1 + S · S ) 2

dx(ti)P (x(ti)|Xt(^-j)1)·

N

=·

I(X~t(-k)1(k); Xt(-k)1) +

k=1

dXt(^-j)1d X d S P (X(t^-j)1)P ( X )P ( S ) xt(i) - fS (X(t^-j)1 + S · S ) 2

dxt(i)P (xt(i)|X(t^-j)1)·

Here fS is the f such that RX,x(i) [f, ] attains its minimum w.r.t. f. It

does not depend on Xt(-j)1 because given Xt(-j)1, X(t^-j)1, the P (xt(i)|X(t^-j)1) in the integral

dx(ti)P (xt(i)|X(t^-j)1)

xt(i) - f(Xt(-j)1 + X ·

X , X(t^-j)1 + S ·

2
S) (before the inequality) does

not condition on Xt(-j)1. Using similar derivation as in Lemma 1.1, the argmin of f in the integral does not depend on Xt(-j)1, and we denote this function as fS .

To attain the minimization of RX,x(i) [f, ] w.r.t. (f, X , S), we still need to minimize

minf RX,x(i) [f, ] w.r.t. (X , S). Note that its second term does not contain Xt(-j)1, and the

first term  ·

N k=1

I (X~ t(-k)1(k ) ;

Xt(-k)1)

contains

I(X~t(-j)1(j );

Xt(-j)1).

Then

min(f,X )RX,x(i) [f, ]

=·

I(X~t(-k)1(k); Xt(-k)1) +

k=j

dX(t^-j)1d X d S P (X(t^-j)1)P ( X )P ( S )

dx(ti)P (xt(i)|Xt(^-j)1)·

xt(i) - fS (Xt(^-j)1 + S · S ) 2

at which I(X~t(-j)1(j); Xt(-j)1) = 0. Since Xt(-j)1 has finite variance, this can be attained when j  +. Since min(f,)RX,x(i) [f, ] belongs to the manifold of min(f,X )RX,x(i) [f, ], we have
that when RX,x(i) [f, ] attains minimum w.r.t. (f, ), Wji = I(X~t(-j)1(j); Xt(-j)1) = 0. This completes the logic link of the proof.

C IMPLEMENTATION DETAILS FOR THE METHODS
Here we state the implementation details for our method, as well as other methods being compared. Throughout this paper, unless otherwise specified, we use the standard technique in Kraskov et al. (2004) to estimate the KL-divergence and mutual information, which is used in our implementations of Mutual information, Transfer Entropy, and Causal Influence.
C.1 OUR METHOD
Without stating otherwise, our method (Algorithm 1) as a default uses a three layer neural net, with two hidden layers having 8 neurons and SELU (Klambauer et al. (2017)) activation, and the last layer
16

Under review as a conference paper at ICLR 2019

having linear activation. Adam (Kingma & Ba (2014)) optimizer with learning rate = 10-4 is used

as default throughout this paper. We set 0 = 0.01 and  = 0.01. We use 10000 epochs, patience of

40 and inspect-interval of 20. It also has a 400 epoch warm up period where the mutual information

term is turned off, to allow f to find a good initial model as a start. We use the approximation

I (X~t(-j)1(j); Xt(-j)1)

1 2

KM l=1

log(1

+

1/2j,l)

in

the

risk

and

also

in

estimating

Wji,

as

discussed

in

the

main

text

in

3.3.

Here

j,l

=

j,l std(Xt(-j )1,l )

is

the

relative

noise

scale

w.r.t.

the

standard

deviation

of

each element, l denoting the lth element of the KM -dimensional vector. In this work, we fix j,l to

be the same for each j, and let j be a single parameter instead of a vector. This simplifies the risk

calculation, and also to a first order invariant to the reparameterizaiton of each time series Xt(-j)1.

C.2 TRANSFER ENTROPY
We use the definition of transfer entropy as defined in Schreiber (2000). In that work the transfer entropy is defined for 2 time series. To deal with multiple time series, we let Xt(-^j)1 also include other time series, similar to the extension of transfer entropy as in Lizier et al. (2008).

C.3 CAUSAL INFLUENCE
For causal influence, we use the same network architecture as in our method, to learn a prediction model. Then the KL divergence is estimated via the technique in Kraskov et al. (2004).

C.4 LINEAR GRANGER
We use the definition of linear Granger causality (Eq. (7) and (8) in Ding et al. (2006)) to calculate linear Granger causality. Specifically, we estimate the variance of the residue of a linear predictor of xt(-i)1 with and without Xt(-j)1 (also conditioned on X(t^-j)1), using Levinson-Whittle(-Wiggins) and Robinson algorithm (Morf et al. (1978)). Then the linear Granger causality equals the log of the ratio of the two variances.

D IMPLEMENTATION DETAILS FOR SYNTHETIC EXPERIMENTS
For all experiments in this section, each metrics is obtained by performing the experiments (including generation of the dataset and the training) 4 times with seed = 0, 30, 60, 90 and averaging the resulting metrics. For the ground-truth causal tensor A, each element Aji is a K × M matrix, with 0.5 probability to be an all-zero matrix, and 0.5 probability to be a nonzero matrix. If Aji is a nonzero matrix, its each element is sampled from a log-normal distribution with µ = 0 and  = 1. For B, each Bj is also a K × N matrix, with each element sampled from U [-1, 1]. We use H1(x) = softplus(x) = log(1 + ex), and H2(x) = tanh(x) in equation (5).

E DETAILS FOR THE VIDEO GAME DATASET
Here, we implement a custom Atari Breakout game in the OpenAI Gym (Brockman et al. (2016)) environment, mimicking the original game, where we can access the state of the ball, paddle and bricks, etc. This representation is also used in the OO-MDP (Diuk et al. (2008)) paradigm for a more efficient representation of the environment state. We use the DQN algorithm, we same CNN architecture as in Mnih et al. (2015) to train an RL agent. Then we let it play the game for 20000 steps, obtaining a dataset with time-length of 20000 steps (if the agent dies we restart the game) and 6 time series: action, paddle's x position, ball's x position, ball's y position, number of bricks and reward. We then feed the time series to our method, the same procedure as performed in the synthetic experiment, to let it produce an inferred causal matrix Wji, which is shown in Fig. 1. All the datasets used in this paper and code will be open-sourced upon publication of the paper.
17

Under review as a conference paper at ICLR 2019
F IMPLEMENTATION DETAILS FOR THE REAL-WORLD DATASET
For the two real-world datasets, we obtain the data with the same procedure as in Ancona et al. (2004). Then the data are fed into our algorithm to infer the causal strength Wji with the default settings as described in Appendix C and D.
18

