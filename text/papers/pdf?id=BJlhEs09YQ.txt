Under review as a conference paper at ICLR 2019
END-TO-END LEARNING OF A CONVOLUTIONAL NEURAL NETWORK VIA DEEP TENSOR DECOMPOSITION
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper we study the problem of learning the weights of a deep convolutional neural network. We consider a network where convolutions are carried out over nonoverlapping patches with a single kernel in each layer. We develop an algorithm for simultaneously learning all the kernels from the training data. Our approach dubbed Deep Tensor Decomposition (DeepTD) is based on a rank-1 tensor decomposition. We theoretically investigate DeepTD under a realizable model for the training data where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to planted convolutional kernels. We show that DeepTD is data-efficient and provably works as soon as the sample size exceeds the total number of convolutional weights in the network. Our numerical experiments demonstrate the effectiveness of DeepTD and verify our theoretical findings.
1 INTRODUCTION
Deep neural network (DNN) architectures have led to state of the art performance in many domains including image recognition, natural language processing, recommendation systems, and video analysis (He et al. (2016); Krizhevsky et al. (2012); Van den Oord et al. (2013); Collobert & Weston (2008)). Convolutional neural networks (CNNs) are a class of deep, feed-forward neural networks with a specialized DNN architecture. CNNs are responsible for some of the most significant performance gains of DNN architectures. In particular, CNN architectures have led to striking performance improvements for image/object recognition tasks. Convolutional neural networks, loosely inspired by the visual cortex of animals, construct increasingly higher level features (such as mouth and nose) from lower level features such as pixels. An added advantage of CNNs which makes them extremely attractive for large-scale applications is their remarkable efficiency which can be attributed to: (1) intelligent utilization of parameters via weight-sharing, (2) their convolutional nature which exploits the local spatial structure of images/videos effectively, and (3) highly efficient matrix/vector multiplication involved in CNNs compared to fully-connected neural network architectures.
Despite the wide empirical success of CNNs the reasons for the effectiveness of neural networks and CNNs in particular is still a mystery. Recently there has been a surge of interest in developing more rigorous foundations for neural networks (Soltanolkotabi et al. (2017); Zhong et al. (2017b); Alon Brutzkus & Globerson (2017); Soltanolkotabi (2017); Oymak (2018); Zhong et al. (2017a); Janzamin et al. (2015); Li & Yuan (2017); Mei et al. (2018)). Most of this existing literature however focus on learning shallow neural networks typically consisting of zero or one hidden layer. In practical applications, depth seems to play a crucial role in constructing progressively higher-level features from pixels. Indeed, state of the art Resnet models typically have hundreds of layers. Furthermore, recent results suggest that increasing depth may substantially boost the expressive power of neural networks (Raghu et al. (2016); Cohen et al. (2016)).
In this paper, we propose an algorithm for approximately learning an arbitrarily deep CNN model with rigorous guarantees. Our goal is to provide theoretical insights towards better understanding when training deep CNN architectures is computationally tractable and how much data is required for successful training. We focus on a realizable model where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to planted convolutional kernels. We use both labels and features in the training data to construct a tensor. Our first insight is that, in the limit of infinite data this tensor converges to a population tensor which is approximately rank one and
1

Under review as a conference paper at ICLR 2019

Figure 1: Depiction of the input-output relationship of a non-overlapping Convolutional Neural Network (CNN) model along with the various notations and symbols.

whose factors reveal the direction of the kernels. Our second insight is that even with finite data this empirical tensor is still approximately rank one. We show that the gap between the population and empirical tensors provably decreases with the increase in the size of the training data set and becomes negligible as soon as the size of the training data becomes proportional to the total numbers of the parameters in the planted CNN model. Combining these insights we provide a tensor decomposition algorithm to learn the kernels from training data. We show that our algorithm approximately learns the kernels (up to sign/scale ambiguities) as soon as the size of the training data is proportional to the total number of parameters of the planted CNN model. Our results can be viewed as a first step towards provable end-to-end learning of practical deep CNN models. Extending the connections between neural networks and tensors (Janzamin et al. (2015); Cohen et al. (2016); Zhong et al. (2017a)), we show how tensor decomposition can be utilized to approximately learn deep networks despite the presence of nonlinearities and growing depth. While our focus in this work is limited to tensors, we believe that our proposed algorithm may provide valuable insights for initializing local search methods (such as stochastic gradient descent) to enhance the quality and/or speed of CNN training.

2 PROBLEM FORMULATION AND MODELS

In this section we discuss the CNN model which is the focus of this paper. A fully connected artificial neural network is composed of computational units called neurons. The neurons are decomposed into layers consisting of one input layer, one output layer and a few hidden layers with the output of each layer is fed in (as input) to the next layer. In a CNN model the output of each layer is related to the input of the next layer by a convolution operation. In this paper we focus on a CNN model where the stride length is equal to the length of the kernel. This is sometimes referred to as a non-overlapping convolution operation formally defined below.

Definition 2.1 (Non-overlapping convolution) For two vectors k  Rd and h  Rp=dp¯ their non-

overlapping

convolution,

denoted

by

k  h

yields

a

vector

u



Rp¯=

p d

whose

entries

are

given

by

ui = k( ), h[i] where h[i] = h(i-1)d+1 h(i-1)d+2 . . . hid T .

In this paper it is often convenient to view convolutions as matrix/vector multiplications. This leads us to the definition of the kernel matrix below.

Definition 2.2 (Kernel matrix) Consider a kernel k  Rd and any vector h  Rp=p¯d. Corresponding tIKop¯tdhee=nonItop¯ens-tohkveTep.r¯lH×apep¯rpeiid,neAgntciotyBnvmodaleutrntiioxot.neWsktehneoKhter,otwnheaectkabesarssopecrdoiaodtnuectthabiskeedtwrenfieeenlnitmitohanetrktwixoKhma=trKicResph¯×A.pTadhnerdfiounBgedhaonaudst the paper we shall use K interchangeably with K to denote this kernel matrix with the dependence on the underlying kernel and its non-overlapping form implied.

With the definition of the non-overlapping convolution and the corresponding kernel matrix in hand
we are now ready to define the CNN model which is the focus of this paper. For ease of exposition,
the CNN input-output relationship along with the corresponding notation is depicted in Figure 1.  Depth and numbering of the layers. We consider a network of depth D where we number the input as layer 0 and the output as layer D and the hidden layers 1 to D - 1.  Layer dimensions and representations. We assume the input of the CNN, denoted by x  Rp, consists of p features and the output is a one dimensional label. We also assume the hidden layers (numbered by = 1, 2, . . . , D - 1) consists of p units with h¯ ( )  Rp and h( )  Rp denoting the input and output values of the units in the th hidden layer. For consistency of our notation we shall also define h(0) = x  Rp and note that the output of the CNN is h(D)  R. Also, p0 = p and pD = 1.

2

Under review as a conference paper at ICLR 2019

 Kernel dimensions and representation. For = 1, . . . D we assume the kernel relating the output

of layer ( - 1) to the input of layer is of dimension d and is denoted by k( )  Rd .

 Inter-layer relationship. We assume the inputs of layer (denoted by h¯ ( )  Rp ) are related to

the outputs of layer ( - 1) (denoted by h( -1)  Rp )-1 via a non-overlapping convolution

h¯ ( ) = k( )  h( -1) = K( )h( -1) for = 1, . . . , D.

In the latter equality we have used the representation of non-overlapping convolution as a matrix/vector

product involving the kernel matrix K( )  Rp ×p -1 associated with the kernel k( )  Rd per

Definition 2.2. We note  Activation functions

that the non-overlapping nature and intra-layer relationship.

of the convolution implies that p = p We assume the input of each hidden

-1 d . unit is

related to its output by applying an activation function   R  R. More precisely, h( ) =  (h¯ ( ))

where for a vector u  Rp,  (u)  Rp is a vector obtained by applying the activation function 

to each of the entries of u. We Throughout, we also assume all

allow for using distinct activation functions activations are 1-Lipschitz functions (i.e. 

{ (a)

}-D=1

at every (b)  a

layer. - b ).

 Final output. The input-output relation of our CNN model with an input x  Rp is given by

x  fCNN (x) = h(D), with hidden unit relations h( ) =  (h¯ ( )) and h¯ ( ) = K( )h( -1).

(2.1)

3 ALGORITHM: DEEP TENSOR DECOMPOSITION (DEEPTD)

This paper introduces an approach to approximating the convolutional kernels from training data based on tensor decompositions dubbed DeepTD, which consists of a carefully designed tensor decomposition. To connect these two problems, we begin by stating how we intend to construct the tensor from the training data. To this aim given any input data x  Rp, we form a D-way tensor X  RD=1 d as follows. First, we convert x into a matrix by placing every d1 consecutive entries of x as a row of a matrix of size p1 × d1. From this matrix we then create a 3-way tensor of size p2 × d2 × d1 by grouping d2 consecutive entries of each of the d1 columns and so on. We repeat this procedure D times to arrive at the D-way tensor X  RD=1 d . We define T  Rp  RD=1 d as the corresponding tensor operation that maps x  Rp to X  RD=1 d .

Given a set of training data consisting of n input/output pairs (xi, yi)  Rp × R we construct a tensor Tn by tensorizing the input vectors as discussed above and calculating a weighted combination of

these tensorized inputs. More precisely,

Tn

=

1 n

n
(yi
i=1

- yavg) Xi

where

yavg

=

1 n

n
yi
i=1

and

Xi = T (xi).

(3.1)

We then perform a rank-1 tensor decomposition on this tensor to approximate the convolutional

kernels. Specifically we solve

D
k^(1), . . . , k^(D) = arg max Tn,  v  subject to

v Rd

=1

v1

= ... =
2

vD

= 1.
2

(3.2)

In the above D=1 v denotes the tensor resulting from the outer product of v1, v2, . . . , vD. This tensor rank decomposition is also known as CANDECOMP/PARAFAC (CP) decomposition Bro

(1997) and can be solved efficiently using Alternating Least Squares and a variety of other algorithms

Anandkumar et al. (2014a); Ge et al. (2015); Anandkumar et al. (2014b). 1

At this useful.

point it is completely unclear why The main intuition is that as the

dthaetatesnestogrroTwn sor(nitsrank)-1thdeeecmompiproicsaitliotenncsaonr

yield anything Tn converges

close to a population tensor T whose rank-1 decomposition reveals useful information about the

kernels. Specifically, we will show that

D

nlimTn

=

T

= ExN (0,Ip)[fCNN(x)T

(x)]



  k(
=1

),

(3.3)

with  a scalar whose value shall be discussed later on. Here, x is a Gaussian random vector with

i.i.d. N (0, 1) entries and represents a typical input with fCNN(x) the corresponding output and T (x)

the tensorized input. We will also utilize a concentration argument to show that when the training

1We would like to note that while finding the best rank-1 TD (3.2) is NP-hard, our theoretical guarantees continue to hold when using an approximately optimal solution to (3.2). In fact, we can show that unfolding the tensor along the ith kernel into a di × ji dj matrix and using the top left singular vector yields a good approximation to k(i). However, in our numerical simulations we instead utilize popular software packages to solve (3.2).

3

Under review as a conference paper at ICLR 2019

data set originates from an i.i.d. distribution, for a sufficiently large training data n, Tn yields a good approximation of the population tensor T .

Another perhaps perplexing aspect of the construction of Tn in (3.1) is the subtraction by yavg in the

weights. The reason this may be a source of confusion is that based on the intuition above

E[T ] = E

1n n i=1 yiXi

=E

n

1 -

1

n i=1

(yi

-

yavg )

Xi

=

n

n -

1

E[Tn]



D
  k(
=1

),

so that the subtraction by the average seems completely redundant. The main purpose of this

subtraction is to ensure the weights yi - yavg are centered (have mean zero). This centering allows

for a much better concentration of the empirical tensor around its population counter part and is

crucial to the success of our approach. We would like to point out that such a centering procedure is

reminiscent of batch-normalization heuristics deployed when training deep neural networks.

Finally, we note that based on (3.3), the rank-1 tensor decomposition step can recover the convolutional kernels {k( )}D=1 up to sign and scaling ambiguities. Unfortunately, depending on the activation function, it may be impossible to overcome these ambiguities. For instance, if the activations are homogeneous (i.e.  (ax) = a (x)), then scaling up one layer and scaling down the other layer by the same amount does not change the overall function fCNN(). Similarly, if the activations are odd functions, negating two of the layers at the same time preserves the overall function. In
Appendix C, we discuss some heuristics and theoretical guarantees for overcoming these sign/scale
ambiguities.

4 MAIN RESULTS

In this section we introduce our theoretical results for DeepTD. We will discuss these results in three sections. In Section 4.1 we show that the empirical tensor concentrates around its population counterpart. Then in Section 4.2 we show that the population tensor is well-approximated by a rank-1 tensor whose factors reveal the convolutional kernels. Finally, in Section 4.3 we combine these results to show DeepTD can approximately learn the convolutional kernels up to sign/scale ambiguities.

4.1 CONCENTRATION OF THE EMPIRICAL TENSOR

Our first result shows that the empirical tensor concentrations around the population tensor. We

measure the quality of this concentration via the tensor spectral norm defined below.

Definition 4.1 Let RO be the set of rank-one tensors D=1 v satisfying vi 2  1 for all 1  i  D. The spectral norm of a tensor X  RD=1 d is given by the supremum T = supV RO V , T .

Theorem 4.2 Consider a CNN model x  fCNN (x) of the form (2.1) consisting of D  2 layers

rwthaienthdCocNomNnvvmoelcoutdotierolndaaisnltdkreiXbruntee=ldsTka(s(1xN)), k(t0h(2e,)Ic,po.).r.rwe, ikstph(oDtnh)deoinfcgolertnreegnstshposorndizd1ei,dndg2in,lp.a.ub.te,.ldsSDuyp.=pLofesCtexNthNe(Rdxpa)btageesaneeGt rcaaoutnessdsiisabtnys

of n training samples corresponding labels

where the feature vectors yi = fCNN (xi) generated

xi by

 Rp are the same

dCisNtrNibumtoeddeil.ia.dn.dNX(i0,=IpT)

with (xi)

the the

corresponding tensorized input. Suppose n  (D=1 d ) log D. Then the empirical tensor Tn and

population tensor T defined based on this dataset obey

with

Tn - T = probability at

1 n

n
(yi
i=1

least 1 -

- yavg)Xi - E[yX] 

5e- min

t2

 ,t n,n

, where

c c

D
=1
>0

k( ) is an

D=1 d log 2n absolute constant.

D

+

t

,

(4.1)

The theorem above shows that the empirical tensor approximates the population tensor with high

probability. This theorem also shows that the quality of this approximation is proportional to

D=1

k( )

.
2

This

is

natural

as

D=1

k( )

is an upper-bound on the Lipschitz constant of the
2

network and shows how much the CNN output fluctuates with changes in the input. The more

fluctuations, the less concentrated the empirical tensor is, translating into a worse approximation

guarantee. Furthermore, the quality of this approximation grows with the square root of the parameters

in the model (D=1 d ) and is inversely proportional to the square root of the number of samples (n) which are typical scalings in statistical learning. We would also like to note that as we will

see in the forthcoming sections, in many cases

T

is roughly on the order of D=1 k( )

so
2

that (4.1) guarantees that Tn - T

T  c

D=1 d n

log D .

Therefore, the relative error in the

4

Under review as a conference paper at ICLR 2019

approximation is less than as soon as the number of observations exceeds the number of parameters

in

the

model

by

a

logarithmic

factor

in

depth

i.e.

n



(D=1

d

)

log D
2

.

4.2 RANK ONE APPROXIMATION OF THE POPULATION TENSOR
Our second result shows that the population tensor can be approximated by a rank one tensor. To explain the structure of this rank one tensor and quantify the quality of this approximation we require a few definitions. The first quantity roughly captures the average amount by which the nonlinear activations amplify or attenuate the size of an input feature at the output.

Definition 4.3 (CNN gain) Let x  N (0, Ip) and define the hidden unit/output values of the CNN based on this random input per equations (2.1). We define the CNN gain as CNN = D=1 E[ (h¯ 1( ))]. In words, this is the product of expectations of the activations evaluated at the first entry of each layer. For non differentiable activations,  should be interpreted as the average of the left and right derivatives. For instance, when  (z) =ReLU(z) then  (0) = 1 2.

This quantity is the product of the average slopes of the activations evaluated along a path connecting

the first input feature to the first hidden units across the layers all the way to the output. We note that

this quantity is the same when calculated along any path connecting an input feature to the output

passing through the hidden units. Therefore, this quantity can be thought of as the average gain

(amplification or attenuation) of a given input feature due to the nonlinear activations in the network.

To gain some intuition consider a ReLU network which is mostly inactive. Then the network is dead

and CNN  regime and

0. On CNN

the = 1.

other extreme if We would like

all ReLU units are active the to point out that CNN can in

network operates in the linear many cases be bounded from

below by a constant. For instance, as proven in Appendix B, for ReLU activations as long as the

kernels obey

1T k( )  4 k( ) ,

(4.2)

thCeNnNCN0N.3u1nd4e.r

2
Another example is the softplus activation 
similar assumptions (Also in Appendix B).

(x) We

= log (1 note that

+ ex) for which we prove an assumption similar to

(4.2) is needed for the network to be active. This is because if the kernel sums are negative one can

show that with high probability, all the ReLUs after the first layer will be inactive and the network

will be dead. With this definition in hand, we are now ready to describe the form of the rank one

tensor that approximates the population tensor.

Definition 4.4 (Rank one CNN tensor) We define the rank one CNN tensor LCNN  RD=1 d as LCNN = CNN D=1 k( ). That is, the product of the kernels {k( )}D=1 scaled by the CNN gain CNN.
To quantify how well the rank one CNN tensor approximates the population tensor we need two definitions. The first definition concerns the activation functions.

Definition 4.5 (Activation smoothness) We assume the activations are differentiable everywhere and S-smooth (i.e.  (x) -  (y)  S x - y for all x, y  R) for some S  0.

The reason smoothness of the activations play a role in the quality of the rank one approximation is that smoother activations translate into smoother variations in the entries of the population tensor. Therefore, the population tensor can be better approximated by a low-rank tensor. The second definition captures how diffused the kernels are.

Definition 4.6 (Kernel diffuseness parameter) the kernel diffuseness parameter µ is defined as

µG=ivseunpk1ernDels {dk(

)}D=1 k( )

with dimensions k( ) .
2

{d

}D=1,

The less diffused (or more spiky) the kernels are, the more the population tensor fluctuates and thus

the quality of the approximation to a rank one tensor decreases. With these definitions in place, we

are now ready to state our theorem on approximating a population tensor with a rank one tensor.

Theorem 4.7 Consider the setup of Theorem 4.2. Also, assume the activations are S-smooth per

Definition 4.5 and the convolutional kernels are µ-diffused per Definition 4.6. Then, the population

tensor

T

= E[yX] can T - LCNN 

be T

approximated by the rank-1 tensor

- LCNN

F



 8µS



D

k(i)

LCNN  sup

=

CNN D=1 k( ) k(i) D

as follows .

i=1 2 i=1 2 min d

The theorem above states that the quality of the rank one approximation deteriorates with increase in the smoothness of the activations and the diffuseness of the convolutional kernels. As mentioned

5

Under review as a conference paper at ICLR 2019

earlier increase in these parameters leads to more fluctuations in the population tensor making it

less likely that it can be well approximated by a rank one tensor. We also note that LCNN =

CNN D=1 k( )

and therefore the relative error in this approximation is bounded by

2

T - LCNN



 8

µS

sup

k(i)

D .

LCNN

CNN

i=1

2 min d

We would like to note that for many activations the smoothness is bounded by a constant. For instance, for the softplus activation ((x) = log(1 + ex)) and one can show that S  1. As stated earlier,

under appropriate assumptions on the kernels and activations, the CNN gain CNN is also bounded

from below by a constant. Assume the convolutional kernels have unit norm and are sufficiently

diffused so that the diffuseness parameter is bounded by a constant. We can then conclude that

T -LCNN
LCNN

c CNN

D min d

.

This implies

that as

soon as the length of the convolutional patches scale

with the square of depth of the network by a constant factor the rank one approximation is sufficiently good. Our back-of-the-envelope calculations suggest that the correct scaling is linear in D versus the quadratic result we have established here. Improving our result to achieve the correct scaling is an interesting future research direction. Finally, we would like to note that while we have assumed differentiable and smooth activations we expect our results to apply to popular non-differentiable activations such as ReLU activations.

4.3 LEARNING THE CONVOLUTIONAL KERNELS
We demonstrated in the previous two sections that the empirical tensor concentrates around its population counter part and that the population tensor is well-approximated by a rank one tensor. We combine these two results along with a perturbation argument to provide guarantees for DeepTD.

Theorem 4.8 (Main theorem) Consider the setups of Theorems 4.2 and 4.7. Assume activations

are S-smooth per Definition 4.5 and the convolutional kernels are µ-diffused per Definition 4.6. The

DeepTD estimates of the convolutional kernels given by (3.2) using the empirical tensor Tn obeys

D=1 k( ), k^( ) D=1 k( ) 2

1

with probability at least

- 2 c CNN 
1 - 5e- min

D=1

t2

 ,t n,n

d log D n
, where c

+ >

t 0

+

 8µS



sup

k(i)

i=1

is an absolute constant.

2

D  , min d 

The above theorem is our main result on learning a non-overlapping CNN with a single kernel at each

layer. It demonstrates that estimates k^( ) obtained by DeepTD have significant inner product with

the ground truth kernels k( ) with high probability, using only few samples. Indeed, similar to the

discussion after Theorem 4.7 assuming the activations are sufficiently smooth and the convolutional

kernels are unit norm and sufficiently diffused, the theorem above can be simplified as follows

D=1 k( ), k^( ) D=1 k( ) 2

 1 - c 

D=1 d log D + n

D min d

 .

Thus the kernel estimates obtained via DeepTD are well aligned with the true kernels as soon as the

number of samples scales with the total number of parameters in the model and the length of the

convolutional kernels (i.e. the size of the batches) scales quadratically with the depth of the network.

5 NUMERICAL EXPERIMENTS

Our goal in this section is to numerically corroborate the theoretical predictions of Section 4. To this

aim we use a CNN model of the form (2.1) with D layers and ReLU activations and set the kernel

lengths to layer (i.e. (i.e. D(z

)b=eDam(lzla)exq=(u0za,l)zt)wo)i.ethWactehhceootenhxdecruecip.teeti.dodno4uo=rfe.tx.hp.ee=rliadms1te=enxtdsp.ienWriPmeyeuthnsoet nwthuheseiinrdegewnthteietyuTseaencstaiovRralyteiLolinUbrfaaorcryttihfvoeartlitaohsnet

tensor decomposition in DeepTD (Kossaifi et al. (2016)). Each curve in every figure is obtained by

averaging 100 independent realizations of the same CNN learning procedure. Similar to our theory, we use Gaussian data points x and ground truth labels y = fCNN(x).

We conduct two sets of experiments: The first set focuses on larger values of depth D and the second set focuses on larger values of width d. In all experiments kernels are generated with random Gaussian entries and are normalized to have unit Euclidean norm. For the ReLU activation if one of the kernels have all negative entries, the output is trivially zero and learning is not feasible. To

6

Under review as a conference paper at ICLR 2019

Correlation

1.00

0.95

0.90

0.85

0.80

0.75 N = 20

0.70 N = 50 0.65 N = 100

2

4

6
Layer

()8

10 12

Correlation

1.00

0.95

0.90

0.85

0.80

0.75 N = 20

0.70 N = 50 0.65 N = 100

1

2

3

4
Layer

(5)

6

7

8

(a) d = 2, D = 12.

(b) d = 3, D = 8.

Figure 2: Correlations (corr(k^( ), k( )) = k^( ), k( ) ) between the DeepTD estimate and the ground truth

kernels for different layers and over-sampling ratios N .

Layer 1
1.0

Layer 2
0.95

0.90 0.9 0.85

Correlation

Correlation

0.80 0.8 0.75

0.7 0.6
2

DeepTD, N = 10 NaiveTD, N = 10 DeepTD, N = 20 NaiveTD, N = 20

3

45
Kernel

67
length

(d)8

9 10

0.70 0.65 0.60 0.55 2

DeepTD, N = 10 NaiveTD, N = 10 DeepTD, N = 20 NaiveTD, N = 20

3

45
Kernel

67
length

(d)8

9 10

Figure 3: DeepTD estimate vs NaiveTD estimate when final activation is ReLU. Bias of NaiveTD results in significantly worse performance.

address this, we consider operational networks where at least 50% of the training labels are nonzero.

Here, the number 50% is arbitrarily chosen and we verified that similar results hold for other values.

Finally, to study the effect of finite samples, we let the sample size grow proportional to the total

degrees of freedom the experiments for

D=1 d . N  {10,

In particular, 20, 50, 100}.

we set While

an our

oversampling factor theory requires N 

N

=

n D=1

log D, in

d and carry out our experiments,

we typically observe that improvement is marginal after N = 50.

In Figure 2, we consider two networks with d = 2, D = 12 and d = 3, D = 8 configurations. We plot the absolute correlation between the ground truth and the estimates as a function of layer depth. For each hidden layer 1   D, our correlation measure (y-axis) is
corr(k^( ), k( )) = k^( ), k( ) . This number is between 0 and 1 as the kernels and their estimates both have unit norm. We observe that for both d = 2 and d = 3, DeepTD consistently achieves correlation values above 75% for N = 20. While our theory requires d to scale quadratically with depth i.e. d  D2, we find that even small d values work well in our experiments. The effect of sample size becomes evident by comparing N = 20 and N = 50 for the input and output layers ( = 1, = D). In this case N = 50 achieves perfect correlation. Interestingly, correlation values are smallest in the middle layers. In fact this even holds when N is large suggesting that the rank one approximation of the population tensor provides worst estimates for the middle layers.

In Figure 3, we use a ReLU activation in the final layer and assess the impact of the centering

procedure of the DeepTD algorithm which is a major theme throughout the paper. We define the

NaiveTD algorithm which solves (3.2) without centering in the empirical tensor i.e.

k^(1), . . . , k^(D)

=

arg max  1

v Rd

n

nD
yiXi,  v
i=1 =1



subject to

v1

= ... =
2

vD

= 1.
2

(5.1)

Since the activation of the final layer is ReLU, the output has a clear positive bias in expectation

which will help demonstrating the importance of centering. We find that for smaller oversampling factors of N = 10 or N = 20, DeepTD has a visibly better performance compared with NaiveTD.

The correlation difference is persistent among different layers (we plotted only Layers 1 and 2) and

appears to grow with increase in the kernel size d.

Finally, in Figure 4, we assess the impact of activation nonlinearity by comparing the ReLU and identity activations in the final layer. We plot the first and final layer correlations for this setup. While the correlation performances of the first layer are essentially identical, the ReLU activation (dashed lines) achieves significantly lower correlation at the final layer. This is not surprising as the final layer passes through an additional nonlinearity.

7

Under review as a conference paper at ICLR 2019

Correlation Correlation

0.995 0.990 0.985 0.980 0.975 0.970
2

Layer 1

Identity, N = 20 ReLU, N = 20 Identity, N = 50 ReLU, N = 50

3

45
Kernel

67
length

(d)8

9 10

Layer 4

0.99 0.98 0.97 0.96 0.95 0.94
2

Identity, N = 20 ReLU, N = 20 Identity, N = 50 ReLU, N = 50

3

45
Kernel

67
length

(d)8

9 10

Figure 4: Performance of DeepTD when the final activation is ReLU in lieu of the identity activation.

6 RELATED WORK
Our work is closely related to the recent line of papers on neural networks as well as tensor decompositions. We briefly discuss this related literature.
Neural networks: Learning neural networks is a nontrivial task involving non-linearities and nonconvexities. Consequently, existing theory works consider different algorithms, network structures and assumptions. A series of recent work focus on learning zero or one-hidden layer fully connected neural networks with random inputs and planted models (Goel et al. (2016); Mei et al. (2018); Zhong et al. (2017b); Soltanolkotabi (2017); Oymak (2018); Ge et al. (2017); Fu et al. (2018)). Other publications (Alon Brutzkus & Globerson (2017); Du et al. (2017b;a); Zhong et al. (2017a); Goel et al. (2018)) consider the problem of learning a CNN with 1-hidden layer. In particular, first three of these focus on learning non-overlapping CNNs as in this paper (albeit in the limit of infinite training data). The papers above either focus on characterizing the optimization landscape, or population landscape, or providing exact convergence guarantees for gradient descent. In comparison, in this paper we focus on approximate convergence guarantees using tensor decompositions for arbitrary deep networks. A few recent publications (Soltanolkotabi et al. (2017); Sagun et al. (2017); Soudry & Carmon (2016)) consider the training problem when the network is over-parametrized and study the over-fitting ability of such networks. Closer to our work, Malach & Shalev-Shwartz (2018) and Arora et al. (2014) consider provable algorithms for deep networks using layer-wise algorithms. In comparison to our work, Malach & Shalev-Shwartz (2018) applies to a very specific generative model that assumes a discrete data distribution and studies the population loss in lieu of the empirical loss. Arora et al. (2014) studies deep models but uses activation functions that are not commonly used and assumes random network weights. In comparison, we work with realistic activations and arbitrary network weights. Tensor decomposition: Tensors are powerful tools to model a wide variety of big-data problems (Sidiropoulos et al. (2017); Anandkumar et al. (2014a)). Recent years have witnessed a growing interest in tensor decomposition techniques to extract useful latent information from the data (Anandkumar et al. (2014a); Ge et al. (2015)). The connection between tensors and neural networks have been noticed by several papers (Mondelli & Montanari (2018); Janzamin et al. (2015); Cohen et al. (2016); Kossaifi et al. (2017); Cohen & Shashua (2016)). Cohen & Shashua (2016); Cohen et al. (2016) relate convolutional neural networks and tensor decompositions to provide insights on the expressivity of CNNs. Mondelli & Montanari (2018) connects the hardness of learning shallow networks to tensor decompositions. Closer to this paper, Janzamin et al. (2015) and Zhong et al. (2017b) apply tensor decomposition on one-hidden layer, fully connected networks to approximately learn the latent weight matrices.

7 CONCLUSION
In this paper we studied a multilayer CNN model with depth D. We assumed a non-overlapping structure where each layer has a single convolutional kernel and has stride length equal to the dimension of its kernel. We establish a connection between approximating the CNN kernels and higher order tensor decompositions. Based on this, we proposed an algorithm for simultaneously learning all kernels called the Deep Tensor Decomposition (DeepTD). This algorithm builds a D-way tensor based on the training data and applies a rank one tensor factorization algorithm to this tensor to simultaneously estimate all of the convolutional kernels. Assuming the input data is distributed i.i.d. according to a Gaussian model with corresponding output generated by a planted set of convolutional kernels, we prove DeepTD can approximately learn all kernels with a near minimal amount of training data. A variety of numerical experiments complement our theoretical findings.

8

Under review as a conference paper at ICLR 2019
REFERENCES
A. Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with Gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773­2832, 2014a.
Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal tensor decomposition via alternating rank-1 updates. arXiv preprint arXiv:1402.5180, 2014b.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pp. 584­592, 2014.
Rasmus Bro. Parafac. tutorial and applications. Chemometrics and intelligent laboratory systems, 38(2):149­171, 1997.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions. In International Conference on Machine Learning, pp. 955­963, 2016.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In Conference on Learning Theory, pp. 698­728, 2016.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pp. 160­167. ACM, 2008.
Sjoerd Dirksen. Tail bounds via generic chaining. arXiv preprint arXiv:1309.3522, 2013.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017b.
Haoyu Fu, Yuejie Chi, and Yingbin Liang. Local geometry of one-hidden-layer neural networks for logistic regression. arXiv preprint arXiv:1802.06463, 2018.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points ­ online stochastic gradient for tensor decomposition. In Conference on Learning Theory, pp. 797­842, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. arXiv preprint arXiv:1611.10258, 2016.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches. arXiv preprint arXiv:1802.02547, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Jean Kossaifi, Yannis Panagakis, and Maja Pantic. Tensorly: Tensor learning in python. arXiv preprint arXiv:1610.09555, 2016.
Jean Kossaifi, Zachary C Lipton, Aran Khanna, Tommaso Furlanello, and Anima Anandkumar. Tensor regression networks. arXiv preprint arXiv:1707.08308, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
Eran Malach and Shai Shalev-Shwartz. A provably correct algorithm for deep learning that actually works. arXiv preprint arXiv:1803.09522, 2018.
9

Under review as a conference paper at ICLR 2019
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural networks. arXiv preprint arXiv:1804.06561, 2018.
Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural networks and tensor decomposition. arXiv preprint arXiv:1802.07301, 2018.
Samet Oymak. Learning compact neural networks with regularization. arXiv preprint arXiv:1802.01223, 2018. Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power
of deep neural networks. arXiv preprint arXiv:1606.05336, 2016. Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of
over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017. Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalexakis, and Christos
Faloutsos. Tensor decomposition for signal processing and machine learning. IEEE Transactions on Signal Processing, 65(13):3551­3582, 2017. Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. arXiv preprint arXiv:1705.04591, 2017. Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017. Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016. Michel Talagrand. The generic chaining: upper and lower bounds of stochastic processes. Springer Science & Business Media, 2006. Michel Talagrand. Gaussian processes and the generic chaining. In Upper and Lower Bounds for Stochastic Processes, pp. 13­73. Springer, 2014. Ryota Tomioka and Taiji Suzuki. Spectral norm of random tensors. arXiv preprint arXiv:1407.1870, 2014. Aaron Van den Oord, Sander Dieleman, and Benjamin Schrauwen. Deep content-based music recommendation. In Advances in neural information processing systems, pp. 2643­2651, 2013. Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a. Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for onehidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b.
10

Under review as a conference paper at ICLR 2019

A PROOFS
In this section we will prove our main results. Throughout, for a random variable X, we use zm(X) to denote X - E[X]. Simply stated, zm(X) is the centered version of X. For a random vector/matrix/tensor X, zm(X) denotes the vector/matrix/tensor obtained by applying the zm() operation to each entry. For a tensor T we use T F to denote the square root of the sum of squares of the entries of the tensor. Stated differently, this is the Euclidean norm of a vector obtained by rearranging the entries of the tensor. Throughout we use c, c1, c2, and C to denote fixed numerical constants whose values may change from line to line. We begin with some useful definitions and lemmas.

A.1 USEFUL CONCENTRATION LEMMAS AND DEFINITIONS

In this section we gather some useful definitions and well-known lemmas that will be used frequently throughout our concentration arguments.

Definition A.1 (Orlicz norms) For a scalar random variable Orlicz-a norm is defined as

X

a

=

sup k-1

a(E[

X

k1
])

k

k1

Orlicz-a norm of a vector x  Rp is defined as

x a = supvBp

vT x

a

where

p
B

is the unit

2 ball. The

sub-exponential norm is the function  1 and the sub-gaussian norm the function  2 .

We now state a few well-known results that we will use throughout the proofs. This results are standard and are stated for the sake of completeness. The first lemma states that the product of sub-gaussian random variables are sub-exponential.

Lemma A.2 Let X, Y be subgaussian random variables. Then XY 1  X 2 Y 2 .
The next lemma connects Orlicz norms of sum of random variables to the sum of the Orlicz norm of each random variable.

Lemma A.3 Suppose X, Y are random variables with bounded  a norm. Then X + Y a  2 max{ X a , Y a }. In particular X - E X] a  2 X a .
The lemma below can be easily obtained by combining the previous two lemmas.

Lemma A.4 Let X, Y be subgaussian random variables. Then zm(XY ) 1  2 X 2 Y 2 . Finally, we need a few standard chaining definitions.

Definition A.5 (Admissible sequence Talagrand (2014)) Given a set T an admissible sequence is an increasing sequence {An}n=0 of partitions of T such that An  Nn where N0 = 1 and Nn = 22n for all n  1.
For the following discussion d(An(t)), will be the diameter of the set S  An that contains t, with respect to the d metric.

Definition A.6 (a functional Talagrand (2014)) Given a > 0, and a metric space (T, d) we define
a(T, d) = inf sup 2n ad(An(t)),
tT n0
where the infimum is taken over all admissible sequences.

The following lemma upper bounds  functional with covering numbers of T . The reader is referred to Section 1.2 of Talagrand (2006), Equation (2.3) of Dirksen (2013), and Lemma D.17 of Oymak (2018).

Lemma A.7 (Dudley's entropy integral) Let N () be the  covering number of the set T with respect to the

d metric. Then

(T, d)  C  log1  N ()d,

0

where C > 0 depends only on  > 0.

11

Under review as a conference paper at ICLR 2019

A.2 CONCENTRATION OF THE EMPIRICAL TENSOR (PROOF OF THEOREM 4.2)

To

prove

this

theorem,

first

note

that

given

labels

{yi

n
}i=1



y

and

their

empirical

average

yavg

=

n-1

n
i=1

yi,

we

have E[yavg] = E[y]. Hence y - yavg = zm(y - yavg) and we can rewrite the empirical tensor as follows

1n

Tn

= n

(yi
i=1

-

yavg)Xi

1n = n i=1 zm(yi - yavg)Xi

1n

1n

= n i=1 zm(yi)Xi - zm(yavg)

n

Xi
i=1

.

(A.1)

Recall that the population tensor is equal to T = E[yiXi]. Furthermore, E[E[yi]Xi] = E[yi] E[Xi] = 0. Thus

the

population tensor can alternatively

be

written

as

T

= E[zm(yi)Xi] =

1 n

n
i=1

E[zm(yi)Xi

].

Combining

the latter with (A.1) we conclude that

1n

1n

Tn

-T

=n

(zm(yi)Xi
i=1

- E[zm(yi)Xi]) - zm(yavg)

n

Xi
i=1

,

1n

1n

= n i=1 zm(zm(yi)Xi) - zm(yavg)

n

Xi
i=1

,

1n

1n

1n

= n i=1 zm(zm(yi)Xi) - n i=1 zm(yi) n i=1 Xi .

Now using the triangular inequality for tensor spectral norm we conclude that

1n

1n

1n

Tn - T  n i=1 zm(zm(yi)Xi) +

n i=1 zm(yi)

n

Xi
i=1

.

We now state two lemmas to bound each of these terms. The proofs of these lemmas are defered to Sections

A.2.1 and A.2.2.

Lemma A.8 For i = 1, 2, . . . , n let xi  Rp be i.i.d. random Gaussian vectors distributed as N (0, Ip). Also let

Xi  RD=1 d be the tensorized version of xi i.e. Xi = T (xi). Finally, assume f  Rp  R is an L Lipschitz

function. Furthermore, assume n 

D
 =1

d

log D and D  2. Then

P 

1n n i=1 zm(zm(f (xi))Xi)

c1L   
n

D
d

log

D

+

 t

 



e-

min

t2 ,tn

,

=1  

holds with c1 > 0 a fixed numerical constant.

Lemma A.9 Consider the setup of Lemma A.8. Then

P 

1n n i=1 zm(f (xi))

1n n i=1 Xi

holds with c2 > 0 a fixed numerical constant.

c2t1L 



n

 

D
d

log

D

+

 t2

 



2

e-t12 + e-t22

,

=1  

Combining

Lemma

A.8

with

f

=

fCNN(),

L

=

D
 =1



k( )

, and c1 = c 2 together with Lemma A.9 with
2

t1 = n, t2 = t, and c2 = c 2 concludes the proof of Theorem 4.2. All that remains is to prove Lemmas A.8 and

A.9 which are the subject of the next two sections.

A.2.1 PROOF OF LEMMA A.8

It
1
n

is more convenient to

n
i=1

zm(zm(f

(xi

))Xi

).

carryout the steps The lemma trivally

of the follows

proof

on

n
i=1

by a scaling by

zm(zm(f (xi))Xi) in liue a factor 1 n. We first write

of the

tensor spectral norm as a supremum

nn

zm(zm(f (xi))Xi) = sup

zm(zm(f (xi))Xi), T  .

(A.2)

Let

Yi

=

i=1
zm(f (xi))Xi.

Define

the

random

T RO i=1
process g(T ) =

n
i=1

zm(Yi

),

T

.

We

claim

that

g(T )

has

a

mixture of subgaussian and subexponential increments (see Definition A.1 for subgaussian and subexponential

random variables). Pick two tensors T , H  RD=1 d . Increments of g satisfy the linear relation

n

g(T ) - g(H) = zm(Yi), T - H .

i=1

By construction E[g(T ) - g(H)] = 0. We next claim that Yi is a sub-exponential vector. Consider a

tensor T with unit length T F = 1 i.e. the sum of squares of entries are equal to one. We have Yi, T  =

zm(f (xi)) Xi, T . f (Xi) is a Lipschitz function of a Gaussian random vector. Thus, by the concentration of

12

Under review as a conference paper at ICLR 2019

Lipschitz functions of Gaussians we have

t2

P( zm(f (Xi))  t)  2 exp(- 2L2 ).

(A.3)

This immediately implies that zm(f (Xi)) 2  cL for a fixed numerical constant c. Also note that Xi, T  

N (0, 1) hence Xi, T  2  c. These two identities combined with Lemma A.4 implies a bound on the

sub-exponential norm

zm(Yi, T ) 1  cL.

Next, we observe that g(T ) - g(H) is sum of n i.i.d. sub-exponentials each obeying zm(Yi), T - H 1 

cL T - H F . Applying a standard sub-exponential Bernstein inequality, we conclude that

t t2

P( g(T ) - g(H)  t)  2exp

-c  min

L

T

-H

, F nL2

T

-H

2 F

,

(A.4)

holds with  a fixed numerical constant. This tail bound implies that g is a mixed tail process that is studied

by Talagrand and others (Talagrand (2014); Dirksen (2013)). In particular, supremum of such processes are

characterized in terms of a linear combination of Talagrand's 1 and 2 functionals (see Definition A.6 as well

as Talagrand (2014; 2006) for an exposition). We pick the following distance metrics on tensors induced by the

Frobenius norm: d1(T , H) = L H - T F c and d2(T , H) = H - T F L n c. We can thus rewrite (A.4)

in the form

t t2

P

g(T ) - g(H)  t 

 2 exp

- min

d1(T , H) , d22(T , H)

,

which implies P g(T ) - g(H)  td2(T , H) + td1(T , H)  2 exp(-t). Observe that the radius of

RO with respect to  F norm is 1 hence radius with respect to d1, d2 metrics are L c, L n c respectively. Applying Theorem 3.5 of Dirksen (2013), we obtain

P sup

g(T )  C 2(RO, d2) + 1(RO, d1) + L

un c + uL c

 



e-u.

 T RO

 

Observe that we can use the change of variable t = L  max un, u to obtain

P sup

g(T )



C

(2(RO,

d2)

+

1(RO,

d1)

+

t)

 



exp

 T RO



- min

t2 t ,
L2n L

,

(A.5)

with some updated constant C > 0. To conclude, we need to bound the 2 and 1 terms. To achieve this we will

upper bound the  functional in terms of Dudley's entropy integral which is stated in Lemma A.7. First, let us

find the  covering number of RO. Pick 0 <   1 coverings C

of the unit

2

balls

d
B

.

These covers have

size

at most (1 + 2 )d . Consider the set of rank 1 tensors C = C1  . . .  CD with size (1 + 2 )D=1 d . For any

D
 =1

v



RO,

we

can

pick

D
 =1

u

 C satisfying

v

-u

2   for all 1   D. This implies

D

({v

D
} =1)

-

({u

D
} =1)

F



(v1, . . . , v , u +1, . . . , uD) - (v1, . . . , v -1, u , . . . , uD) F

(A.6)

=1

D
= v 2 . . . v -1 2 v - u 2 u +1 2 . . . uD 2  D.
=1
Denoting Frobenius norm covering number of RO by N (), this implies that, for 0 <   1, N ()  (1 + 2D )D=1 d .

(A.7)

Clearly, N () = 1 for   1 by picking the cover {0}. Consequently,

1D

1

1(RO,  F )  log N ()d  c d log D, 2(RO,  F ) 

log N ()d  c

0 =1

0

D
d log D.
=1
(A.8)

Thus the metrics d1, d2 metrics are  F norm scaled by a constant. Hence, their  functions are scaled versions of (A.8) given by

DD

1(RO, d1)  cL d log D, 2(RO, d2)  cL n d log D.

=1 =1

Now, observe that if n 

D
 =1

d

log D, we have 1(RO, d1)  cL

n

D
 =1

d

log D. Substituting these

in (A.5) and using n 

D
 =1

d

log D we find

P sup

 g(T )  C L

n

D
d

log

D

+

 t

 



e-

min

t2 L2 n

,

t L

.

 T RO

 =1

 

 Substituting t  L nt and recalling (A.2), concludes the proof.

13

Under review as a conference paper at ICLR 2019

A.2.2 PROOF OF LEMMA A.9

We first rewrite,

1n n i=1 zm(f (xi))

1n

n

Xi
i=1

1n

1n

= n i=1 zm(f (xi)) 

n

Xi
i=1

.

(A.9)

As discussed

sum

favg

=

1 n

in in(=A1.z3m), (fzm(x(if)()xoib)e)ys2the

cL for bound

c a fixed favg 2

numerical  cL n

constant. Since as well. Hence,

xi's

are

i.i.d

the

empirical

P

favg



c

t1L 

n

 2e-t12 .

(A.10)

Also

note

that

1n

n
i=1

Xi

is

a

tensor

with

standard

normal

entries,

applying

(Tomioka

&

Suzuki,

2014,

Theorem 1) we conclude that

1n

 n

i=1

Xi

  c 


D d log D + t2 
=1 

1n n i=1 Xi

 c

D
 =1

d



log D

+ t2 ,

(A.11)

n

holds with probability 1 - 2e-t22 . Combining (A.10) and (A.11) via the union bound together with (A.9) concludes the proof.

A.3 RANK ONE APPROXIMATION OF THE POPULATION TENSOR (PROOF OF THEOREM 4.7)

We begin the proof of this theorem by a few definitions regarding non-overlapping CNN models that simplify our exposition. For these definitions it is convenient to view non-overlapping CNNs as a tree with the root of the tree corresponding to the output of the CNN and the leaves corresponding to input features. In this visualization D - th layer of the tree corresponds to the th layer. Figure 5 depicts such a tree visualization along with the definitions discussed below.
Definition A.10 (Path vector) A vector i  RD+1 is a path vector if its zeroth coordinate satisfies 1  i0  p and for all D - 1  j  0, ij+1 obeys ij+1 = ij dj. This implies 1  ij  pj and iD = 1. We note that in the tree visualization a path vector corresponds to a path connecting a leaf (input feature) to the root of the tree
(output). We use I to denote the set of all path vectors and note that I = p. We also define i(i)  I be the
vector whose zeroth entry is i0 = i. Stated differently i(i) is the path connecting the input feature i to the output. Given a path i and a p dimensional vector v, we define vi = vi0 . A sample path vector is depicted in bold in Figure 5 which corresponds to i = (d1, 1, 1, 1).

Definition A.11 (Kernel and activation path gains) Consider a CNN model of the form (2.1) with input x

and inputs of hidden units associate two path gains:

given by a kernel

{h¯ ( path

)}D=1. To any gain denoted

path vector i connecting an input feature to the output we by ki and activation path gain denoted by i(x) defined

as

DD

ki =

km( o)d(i
=1

-1 ,d

)

and

i(x) =  (h¯ (i )),
=1

where mod(a, b) denotes the remainder of dividing integer a by b. In words the kernel path gain is multiplication

of the kernel weights along the path and the activation path gain is the multiplication of the derivatives of the

activations evaluated at the hidden units along the path. For the path depicted in Figure 5 in bold the kernel path

gain is equal ki = kd(11)k1(2)k1(3) and the activation path gain is equal to i(x) = 1 (h¯(11))2 (h¯(12))3(h¯(13)).

Definition A.12 (CNN offsprings) Consider a CNN model of the form (2.1) with input x and inputs of hidden units given by {h¯ ( )}D=1. We will associate a set set (i)  {1, . . . , p} to the ith hidden unit of layer defined as set (i) = {(i - 1)r + 1, (i - 1)r + 2, . . . , ir } where r = p p . By construction, this corresponds to the set of
entries of the input data x that h¯ (i )(x) is dependent on. In the tree analogy set (i) are the leaves of the tree connected to hidden unit i in the th layer i.e. the set of offsprings of this hidden node. We depict set2(p2) which
are the offsprings of the last hidden node in layer two in Figure 5.

We now will rewrite the population tensor in a form such that it is easier to see why it can be well approximated

by a rank one tensor. Note that since the tensorization operation is linear the population tensor is equal to

T = E[fCNN(x)X] = E[fCNN(x)T (x)] = T (E[fCNN(x)x]) .

(A.12)

Define the vector gCNN to be the population gradient vector i.e. gCNN = E[fCNN(x)] and note that Stein's

lemma combined with (A.12) implies that

T = T (E[fCNN(x)x]) = T (E[fCNN(x)]) = T gCNN .

(A.13)

Also note that Thus we have

fCNN(x) xi

=

ki(i)i(i)(x).

giCNN = E

fCNN(x) xi

= ki(i) E[i(i)(x)].

(A.14)

14

Under review as a conference paper at ICLR 2019

x = h(0)  Rp = Rp0 k(1)  Rd1 h(1)  Rp1

k(2)  Rd2

h(2)  Rp2

k(1)

k(3)  Rd3 y = fCNN(x) = h(D) = h(3

k(1) k(1)

k(2)

set2(p2)

Figure 5: Tree visualization of a non-overlapping CNN model. The path in bold corresponds to

the path vector i = (d1, 1, 1, 1) from Definition A.10. For this path the kernel path gain is equal

skeit=sektd2(11()pk21()2)(okf1(f3s)prainndgsthoef

activation path the last hidden

gain is equal node in layer

to i(x) = 1(h¯(11))2 (h¯1(2))3(h¯(13)). two) is outlined.

The

15

Under review as a conference paper at ICLR 2019

Define a vector k  Rp such that ki = ki(i). Since ki(i) consists of the product of the kernel values across the path i(i) it is easy to see that the tensor K = T (k) is equal to

D
K =  k( ).

(A.15)

Similarly define the vector v  Rp such that vi = E[i=(i1)(x)] and define the corresponding tensor V = T (v). Therefore, (A.14) can be rewritten in the vector form gCNN = k  v where a  b denotes entry-wise (Hadamard)

product between two vectors/matrices/tensors a and b of the same size. Thus using (A.13) the population tensor

can alternatively be written as

D
T = T gCNN = K  V =  k( )  V .

=1
Therefore, the population tensor T is the outer product of the convolutional kernels whose entries are masked

with another tensor V . If the entries of the tensor V were all the same the population tensor would be exactly

rank one with the factors revealing the convolutional kernel. One natural choice for approximating the population

tensor with a rank one matrix is to replace the masking tensor V with a scalar. That is, use the approximation

T



c

D
 =1

k(

).

Recall

that LCNN

=

CNN

D
 =1

k(

)

is

exactly such an approximation with c set to CNN

given

by

D
CNN = ExN (0,I)[ (h¯ i( ))]. =1
To characterize the quality of this approximation note that

(a)
T - LCNN  T - LCNN F ,

(b)
=

K  (V

- CNN)

F,

(c)
=

k  (v - CNN)

,
2

(d)



k

2

v - CNN

,


(e) D
=

k( )

=1

v - CNN
2

.


Here, (a) follows from the fact for a tensor, its spectral norm is smaller than its Frobenius norm, (b) from

the definitions of T and LCNN, (c) from the fact that K = T (k) and V = T (v), (d) from the fact that

a  v  a b , and (e) from the fact that the Euclidean norm of the kronecker product of of vectors 2 2
is equal to the product of the Euclidean norm of the indivisual vectors. As a result of the latter inequality to

prove Theorem 4.7 it suffices to show that

v - CNN

  8µS  sup

k(i)

 i=1


2

D. min d

(A.16)

In the next lemma we prove a stronger statement.

Lemma A.13 Assume the activations are S-smooth. Also consider a vector v  Rp with entries vi =

ExN (0,I)[i(i)(x)]

and

CNN

=

D
 =1

ExN (0,I)[ (h¯ i(

)
)]

we

have

vi - CNN

 i

 = 8S

D =1

km( o)d(i

-1 ,d

)

-1 i=1

k(i)

2.

Here, i is the vector path that starts at input feature i.

Before proving this lemma let us explain how (A.16) follows from this lemma. To show this we use the kernel

diffuseness assumption introduced in Definition 4.6. This definition implies that km( o)d(i -1,d ) 

k( )




µ k( ) . Thus we have

d2

 i = 8S

D
=1 km( o)d(i -1,d )

-1 i=1

k(i)

2,

 D1  8µS 
=1 d

k( )

-1 k(i)
2 i=1

2,

 D1 = 8µS 
=1 d

k(i)
i=1

2,

  8µDS



sup

i=1k(i)

2,

d

  8µS  sup

k(i) 

D.

i=1 2 min d

16

Under review as a conference paper at ICLR 2019

This completes the proof of Theorem 4.7. All that remains is to prove Lemma A.13 which is the subject of the next section.

A.3.1 PROOF OF LEMMA A.13

To bound the difference between vi and CNN consider the path i = i(i) and define the variables {ai}iD=0 as

iD

ai = ExN (0,I)[  (h¯ (i ))]

ExN (0,I)[ (h¯ i( ))].

=1 =i+1

Note that aD = vi and a0 = CNN. To bound the difference vi - CNN = aD - a0 we use a telescopic sum

D-1

aD - a0 

a +1 - a .

(A.17)

=0

We thus focus on bounding each of the summands a

- a -1 . Setting 

D
= i=

ExN (0,I)[i(h¯ i(ii))], this can

be written as

a

-

a

-1

=

ExN (0,I)[( (h¯ i(

)
)

-

E[ (h¯ i(

)
)])

-1  (h¯ (i ))] +1.

i=1

Using   1 (which follows from the assumption that the activations are 1-Lipschitz), it suffices to bound

ExN (0,I)[(

(h¯ (i

)
)

-

E[

(h¯ (i

))])

-1 ]

where

=

i (h¯ i(ii)).

i=1

To this aim we state two useful lemmas whose proofs are deferred to Sections A.3.1.1 and A.3.1.2.

(A.18)

Lemma A.14 Let X, Y, Z be random variables where X is independent of Z. Let f be an L-Lipschitz function.

Then

E[f (X + Y )Z] - E[f (X + Y )] E[Z]  L E[ zm(Y ) ( Z + E[Z] )].

(A.19)

Furthermore if Z  1, then

E[f (X + Y )Z] - E[f (X + Y )] E[Z]  2L E[ zm(Y ) ].

(A.20)

Lemma A.15 h¯ (i )(x) (and hi( )(x)) is a deterministic function of the entries of x indexed by set (i). In other words, there exists a function f such that h¯ i( )(x) = f (xset (i)).

With these lemmas in-hand we return to bounding (A.18). To this aim we decompose h¯ (i ) as follows

d

h¯ (i

)

=

ki(
i=1

) h(d

-1) (i -1)+i

=

km( o)d(i

-1 ,d

) h(i

-1) -1

+ r,

where the r term is the contribution of the entries of h( -1) other than i -1. By the non-overlapping assumption,

r

is

independent

of



-1

as

well

as

hi(

-1) -1

(see

Lemma

A.15).

In

particular,

hi(

-1) -1

and



-1

is

a

function

of

xset -1(i -1) whereas r is a function of the entries over the complement set (i ) - set -1(i -1). With these

observations, applying Lemma A.14 with f

=  , X

= r, Y

= km( o)d(i

-1 ,d

) hi(

-1) ,
-1

Z

=

-1

and using the fact

that  -1  1 which holds due to 1-Lipschitzness of  's, we conclude that

ExN (0,I)[( (h¯ i( )) - E[ (h¯ i( ))]) -1]  2S E zm(Y ) .

Here, S is the smoothness of  and Lipschitz constant of  . To conclude, we need to assess the E zm(Y )

term. Now note that starting from x, each entry of h( -1) is obtained by applying a sequence of inner

products

with

{k(i) }i=-11

and

activations



(),

which

implies

hi(

-1) -1

is

a

-1
i=1

k(i)

2 -Lipschitz function

of set -1(i -1). This implies Y is a Lipschitz function of a Gaussian vector with Lipschitz constant LY =

km( o)d(i

-1 ,d

)

-1
i=1

k(i)

2 . Hence, zm(Y ) obeys the tail bound

t2

P( zm(Y )  t)  2 exp(- 2LY2 ).

Using a standard integration by parts argument the latter implies that



E zm(Y )  2LY .

Thus,

a

- a -1



km( o)d(i -1,d )

-1 i=1

k(i)

2.

concluding the upper-bound on each summand of (A.17). Combining such upper bounds (A.17) implies

aD - a0

  8S

D =1

km( o)d(i

-1 ,d

)

-1 i=1

k(i)

This concludes the proof of Lemma A.13.

2 = i.

17

Under review as a conference paper at ICLR 2019

A.3.1.1 Proof of Lemma A.14 Using the independence of X, Z, we can write E[f (X + Y )Z] = E[f (X + E[Y ])Z] + E[(f (X + Y ) - f (X + E[Y ]))Z]
= E[f (X + E[Y ])] E[Z] + E[(f (X + Y ) - f (X + E[Y ]))Z]
= E[f (X + Y )] E[Z] + E[f (X + E[Y ]) - f (X + Y )] E[Z] + E[(f (X + Y ) - f (X + E[Y ]))Z]. This implies E[f (X +Y )Z]-E[f (X +Y )] E[Z] = E[f (X +E[Y ])-f (X +Y )] E[Z]+E[(f (X +Y )-f (X +E[Y ]))Z]. Now, using Lipschitzness of f , we deterministically have that f (X + E[Y ]) - f (X + Y )  L Y - E[Y ] = L zm(Y ) . Similarly, (f (X + Y ) - f (X + E[Y ]))Z  L zm(Y )Z . Taking absolute values we arrive at
E[f (X + E[Y ]) - f (X + Y )] E[Z] + E[(f (X + Y ) - f (X + E[Y ]))Z]  L E[ zm(Y ) ( Z + E[Z] )]. This immediately implies (A.19). If Z  1 almost surely, we have E[ zm(Y )Z ]  E[ zm(Y ) ] and E[Z]  1 which yields the 2L E[ zm(Y ) ] upper bound.

A.3.1.2 Proof of Lemma A.15 Informally, this lemma is obvious via the tree visualization. To formally

prove this lemma we use an induction argument. For h¯ (1) the result is trivial because h¯ (i1) = k(1), x(i) which

is a weighted sum of entries corresponding to set1(i). Suppose the claim holds for all layers less than or equal

to

- 1 and h¯ (i -1) = f -1(xset -1(i)). For layer

,

we

can

use

the

fact

that

set

(i)

=

d
j=1

set

-1((i - 1)d

+ j)

to conclude that

d

h¯ i( ) =

kj( ) -1(h¯ ((i--11))d +j )

j=1

d
= kj( ) -1(f -1(xset -1((i-1)d +j))) = f (xset (i)). j=1
The latter is clearly a deterministic function of xset (i). Also it is independent of entry i because it simply chunks the vector xset (i) into d sub-vectors and returns a sum of weighted functions of these sub-vectors. Here, the weights are the entries of k( ) and the functions are given by  -1(f -1()) (also note that the activation output is simply hi( ) =  (f (xset (i)))).

A.4 PROOFS FOR LEARNING THE CONVOLUTIONAL KERNELS (PROOF OF THEOREM 4.8)

The first part of the theorem follows trivially by combining Theorems 4.2 and 4.7. To translate a bound on the tensor spectral norm of Tn - LCNN to a bound on learning the kernels, requires a perturbation argument for tensor decompositions. This is the subject of the next lemma.

Lemma A.16

Let

L

=



D
 =1

v

be

a

rank

one

tensor

with

{vi

D
}i=1

vectors

of

unit

norm.

Also

assume

E

is

a

perturbation tensor obeying E  . Set

D

u1, u2, . . . , uD = arg max L,  u¯  subject to

u¯ 1 ,u¯ 2 ,...,u¯ D

=1

Then we have

u¯ 1

=
2

u¯ 2

= ... =
2

u¯ D

= 1.
2

(A.21)

D
ui vi  1 - 2 .

i=1

The proof of Theorem 4.8 is complete by applying Lemma A.16 above with v = k( ), u = k^( ),  = CNN and E = Tn - LCNN. All that remains is to prove Lemma A.16 which is the subject of the next section.

A.4.1 PROOF OF LEMMA A.16

To prove this lemma first note that for any two rank one tensors we have

DD

D

 u ,  v  = ui, vi .

=1 =1

i=1

Using

this

equality

together

with

the

fact

that

the

vectors

{u

D
} =1

are

a

maximizer

for

(A.21)

we

conclude

that

DD

L + E,  v   L + E,  u  ,

=1 =1

DD
 L,  u  + E,  u  ,
=1 =1

D
  ui, vi + .
i=1

(A.22)

18

Under review as a conference paper at ICLR 2019

Furthermore, note that

DD

L + E,  v  =  + E,  v    - .

=1
Combining (A.22) and (A.23) we conclude that

=1

D

 ui, vi +    - ,

concluding the proof.

i=1

(A.23)

B LOWER BOUNDS ON CNN GAIN

Lemma B.1 Consider a CNN model per Section 2 with fCNN() the corresponding CNN function per (2.1). We

have

the

following

lower

bound

on

the

nonlinearity

parameter

CNN

=

D
 =1

E[

(h¯ (1

)
)]

from

Definition

4.3.

· ReLU model: (x) = max(0, x) with the added assumption that the kernels have mean larger than

zero and are modestly diffused. Specifically, assume

Then

(1T kr)  4. kr 2

1

CNN



. 4

(B.1)

· softplus model: (x) = log (1 + ex) with the added assumption that the kernels have mean larger

than zero, are modestly diffused and have a sufficiently large Euclidean norm. Specifically, assume

Then

(1T kr)  10 and kr 2

k  1. 2

(B.2)

CNN  0.3.

Proof ReLU: In this case note that

D
CNN = E  h¯ (i )
=1

D

=

E
=1

1

h¯ (i )0

D
= P h¯ (i )  0

=1

Thus using t = E h¯ i( ) we arrive at

Ph¯ (i

)

<

0 



- E h¯ (i )
e 2 r=1 kr

2
2 2

.

(B.3)

Since the entries of h( ) and h¯ ( ) are i.i.d. we use H( ) and H¯ ( ) to denote the corresponding distributions. We

note that

E h¯ i( ) = E k( ), h( )[i ]

= 1T k( ) E[H( )]

= 1T k( ) E  H¯ ( )

 1T k( )  E[H¯ ( )]

= 1T k( )  E[H¯ ( )].

where in the last inequality we used the fact that 1T k( )  0 and applied Jenson's inequality for a convex . Applying this inequality recursively we arrive at

E

h¯ (i )


r=1

1T k(r)

1 E[(g)] = 
2 r=1

1T k(r)

.

Using the latter in (B.3) we arrive at

Ph¯ (i

)

<

0 

-

1 4

e

r=1

1T k(r) 2
k(r) 2 2

1T k(r) 2



Ph¯ (i

)



0 



1

-

-

1 4

e

r=1

.k(r) 2 2

(B.4)

19

Under review as a conference paper at ICLR 2019

Thus using the diffusion assumption (B.2) in the latter inequality we arrive at

D
CNN 

1

-

e-

1 4

2

=1

Thus using the fact that for 0  x1, x2, ..., xn  1 we have
nn

(1 - xi)  1 - xi,

we conclude that

i=1 i=1

CNN 1 -

D

e-

1 4



2

=1

1 - 1 -

D

e-

1 4

 2x

dx

0



e-

1 4

 2x

dx

0

=1

-

E1

(

1 4

)

log 2

=1

-

E1

(

1 4

)

log 16

1

.

4

In

the

above

E1(x)

=


1

e-tx t

dt

and

we

used

the

fact

that

E1

1 4

 1.016 and   4. Thus,

1

CNN



. 4

Softplus: For the softplus activation we have

D
CNN = E  h¯ (i )
=1

D

 

1

 

=

=1

E

   

1

+

e-h¯ i(

)

   

D

 

1

 

=

E

 

1-

=1

 



1

+

eh¯ (i

)



  

D

 

1

 

=

=1

1 

-

E





 

1

+

eh¯ (i

)

   

.

Thus using the fact that for 0  x1, x2, ..., xn  1 we have

nn

(1 - xi)  1 - xi,

we conclude that

i=1 i=1

D

 

1

 

CNN 1 -

=1

E

   

1

+

eh¯ (i

)

   

1 -

D

1 

E

 

=1

 

+ 1h¯ (i )<E[h¯ i( )] 2

h¯ (i )E[h¯ i( )] 2

1 + eh¯ (i )

     

D

1 -

E
=1

1

h¯ i( )<E[h¯ (i )] 2

-

D

1 

E

 

=1

 

h¯ i( )E[h¯ i( )] 2
1 + eh¯ (i )

     

D

=1 -

P

h¯ (i

)

<

E[h¯ (i

)
]

2

=1

-

D

1 

E

 

=1

 

h¯ i( )E[h¯ (i )] 2
1 + eh¯ i( )

     

D

1 -

P

h¯ (i

)

<

E[h¯ (i

)
]

2

=1

D1 -
=1 1 + eE h¯ (i )

.
2

(B.5)

We bound the expected value of the hidden unites similar to the argument for ReLU activations. The only

difference is that in the identity (B.4) we need to use the softplus activation in lieu of the ReLU activation for .

20

Under review as a conference paper at ICLR 2019

Therefore, (B.4) changes to

E h¯ (i ) 

1T k(r) E[(g)] = 0.806059

1T k(r) .

(B.6)

r=1 r=1

Similar to the ReLU argument, we note that h¯ (i ) is a Lipschitz function of a Gaussian random vector (g) with

Lipschitz constant equal to r=1

k(r)

. Using Lipschitz concentration of Gaussians we thus have
2

Ph¯ i( ) - E

h¯ (i )

<

-t



-
e

2

t2 r=1 k(r)



2.

(B.7)

Thus using t = E h¯ i( ) 2 we arrive at

Ph¯ (i

)

< E[h¯ i(

)]

2



-
e

E h¯ (i 8 r=1

) kr



2
2 2

.

(B.8)

Thus using the diffusion assumption (B.2) with   4 we have

D-1
P

h¯ (i

)

<

E[h¯ i(

)
]

2

 D-1 e-0.12

=1 =1

 D-1 e-0.12x dx
0
 + e-0.12x dx
0

E1(0.1) = log 2

Also using (B.6) and assuming k( )  1 we have

2

D-1

1

D-1

1

=1 1 + eE h¯ (i )


2

=1 1 + e0.4030295 r=1 1T k(r)

 eD-1 -0.4030295 r=1 1T k(r)

=1

 eD-1 -0.4030295 r=1 k(r) 2

=1

 D-1 e-0.4030295

=1

 D-1 e-0.4030295x
0
 + e-0.4030295x
0

E1(0.4030295) =
log  Plugging the latter two inequalities in (B.5), allows to conclude that for   10

CNN

=1

-

E1(0.1) log 2

-

E1(0.4030295) log 

0.3.

C CENTERED ERM AND RESOLVING SIGN & SCALING AMBIGUITIES

We

note

that

DeepTD

operates

by

accurately

approximating

the

rank

one

tensor

D
 =1

k(

)

from

data.

Therefore,

DeepTD can only recover the convolutional kernels up to Sign/Scale Ambiguities (SSA). In general, it may not

be possible to recover the ground truth kernels from the training data. For instance, when activations are ReLU,

the norms of the kernels cannot be estimated from data as multiplying a kernel and dividing another by the same

positive scalar leads to the same training data. However, we can try to learn a good approximation f^CNN() of the network fCNN() to minimize the risk E[(fCNN(x) - f^CNN(x))2].

To this aim, we introduce Centered Empirical Risk Minimization (CERM) which is a slight modification of

Empirical Risk Minimization (ERM). Let us first describe how finding a good f^CNN() can be formulated with

CERM.

Given

n

i.i.d.

data

points

{(xi

,

yi

n
)}i=1



(x, y),

and

a

function

class

F,

CERM

applies

ERM

after

21

Under review as a conference paper at ICLR 2019

centering

the

residuals.

Given

f



F,

define

the

average

residual

function

ravg(f )

=

1 n

n
i=1

yi

- f (xi).

We

define the Centered Empirical Risk Minimizer as

f^

=

min
f F

1 n

n
(yi
i=1

-

f (xi)

-

ravg(f

2
))

,

=

min
f F

1 n

n i=1

(yi

-

f (xi)

-

E[(yi

-

f

(xi

2
))])

-

1 n2

n2
yi - f (xi) - E[(yi - f (xi))] .
i=1

(C.1)

The remarkable benefit of CERM over ERM is the fact that, the learning rate doesn't suffer from the label or

function bias. This is in similar nature to the DeepTD algorithm that applies label centering. In the proofs (in

particular Section C.2, Theorem C.2) we provide a generalization bound on the CERM solution (C.1) in terms

of the Lipschitz covering number of the function space. While (C.1) can be used to learn all kernels, it does

not provide an efficient algorithm. Instead, we will use CERM to resolve SSA after estimating the kernels via

DeepTD. Interestingly, this approach only requires a few (O(D)) extra training samples. Inspired from CERM,

in Section C.1, we propose a greedy algorithm to address SSA. We will apply CERM to the following function

class with bounded kernels,

Fk^,B =

f  Rp  R

f is a CNN function of the form (2.1) with kernels {

k^(

)D
} =1

with



1
 BD

.

(C.2)

In words this is the function class of all CNN functions with kernels the same as those obtained by DeepTD up

to

sign/scale

ambiguities

{

D
} =1

where

the

maximum

scale

ambiguity

is

B.

Theorem C.1

Let

fCNN()

be

defined

via

(2.1)

with

convolutional

kernels

{k(

)

D
} =1

obeying

k( )

2  B1 D

for some B > 0 and consider the function class Fk^,B above with the same choice of B. Assume we have n i.i.d.

samples (xi, yi)  (x, y) where x  N (0, Ip) and y = fCNN(x). Suppose for some   B,

n  cB2D log

CDBp

max

11 ,
2

,

holds for fixed numerical constants c, C > 0. Then the solution f^ to the CERM problem (C.1) obeys

E f^(x) - fCNN(x) 2  min E[(f (x) - fCNN(x))2] + . f Fk^,B

(C.3)

on a new sample x  N (0, Ip) with probability at least 1 - e-n - 4n exp(-p) with  > 0 an absolute constant.

The above theorem states that CERM finds the sign/scale ambiguity that accurate estimates the labels on new data as long as the number of samples which are used in CERM exceeds the depth of the network by constant/log factors. In the next section we present a greedy heuristic for finding the CERM estimate.

C.1 GREEDY ALGORITHM FOR RESOLVING SIGN AND SCALE AMBIGUITIES

In order to resolve SSA, inspired from CERM, we propose Algorithm 1 which operates over the function class, Fk^ = f  Rp  R f is a CNN of the form (2.1) with kernels { k^( )}D=1 with   {1, -1},   0 . (C.4)

It first determines the signs  by locally optimizing the kernels and then finds a global scaling  > 0. In the first

phase,

the

algorithm

attempts

to

maximize

the

correlation

between

the

centered

labels

yc,i

=

yi

- n-1

n
i=1

yi

and

the

f^CNN()

predictions

given

by

y^c,i

=

y^i

-

n-1

n
i=1

y^i.

It

goes

over

all

kernels

one

by

one

and

it

flips

a

kernel

(k^( )  -k^( )) if flipping increases the correlation. This process goes on as long as there is an improvement.

Afterwards, we use a simple linear regression to get the best scaling  by minimizing the centered empirical loss ni=1(yc,i -y^c,i)2. While our approach is applicable to arbitrary activations, it is tailored towards homogeneous activations ((cx) = c(x)). The reason is that for homogeneous activations, function classes (C.2) and (C.4)

coincide and a single global scaling  is sufficient. Note that ReLU and the identity activation (i.e. no activation)

are both homegeneous, in fact they are elements of a larger homogeneous activation family named Leaky ReLU.

Leaky ReLU is parametrized by some scalar 0    1 and defined as follows

LReLU (x) =

x if x  0, x if x < 0.

C.2 GENERALIZATION BOUNDS FOR CERM

In this section we prove a generalization bound for Centered Empirical Risk Minimization (CERM) (C.1). The following theorem shows that using a finite sample size n, CERM is guaranteed to choose a function close to population's minimizer. For the sake of this section f L will be the Lipschitz constant of a function.

Theorem C.2

Let F be a class of functions f  Rp  R. Suppose supfF

f

L



R.

Let

{(xi

,

yi

n
)}i=1



(x, y) be i.i.d. data points where x  N (0, Ip) and y is so that y - E[y] is K subgaussian. Suppose F has



L ,

-covering

bound

obeying

log N



s log

C 

for

some

constants

s



1, C



R.

Given





K¯

=

K

+ R,

22

Under review as a conference paper at ICLR 2019

Algorithm 1 Greedily algorithm for resolving sign/scale ambiguities for Leaky ReLU activations.

1: procedure MAXCORR

2: Inputs: Data (yi, xi)in=1, estimates {k^( )}D=1. 3: max  Corr({k^( )}D=1, 0) , FLIPTRUE.

4: while FLIP do

5: FLIPFALSE.

6: for 1   D do

7:   Corr({k^(1), . . . , - k^( ), . . . , k^(D)}, 0) .

8: 9:

if



>mamx ax

then 

10: k^( )  -k^( )

11: FLIPTRUE

12:   Corr({k^( )}D=1, 1). 13: return kernels {k^( )}D=1, scaling .

Algorithm 2 Return the correlation between centered labels.

1: procedure CORR({k^( )}D=1, opt)

2: 3: 4:

yy^ic,ifCiny=Ni1N-y(c{n1,ik^y^(c,nii)=.}1Dy=i1;

xi). and

y^c,i



y^i

-

1 n

in=1

y^i.

5: return  if opt = 0,  (in=1 y^c2,i) if opt = 1.

suppose

n



O(max{-1, -2}K¯ 2s

log

C  pK¯ 

)

for

some

C

>

0.

Then

the

CERM

output

(C.1)

obeys

E[

f^(x) - y - E[(f^(x) - y)]

2
]



min

E[(f

(x)

-

y

-

E[(f

(x)

-

y)])2]

+

.

f F

with probability 1 - exp(-O(n)) - 4n exp(-p).

(C.5)

Proof Consider the centered empirical loss that can alternatively be written in the form

E(f )

=

1 n

n
zm(yi
i=1

-

f

(xi

2
))

-

1n

n2

(
i=1

zm(yi

-

f

(xi

2
)))

- E[zm(f (x) - y)2].

=

1 n

n
zm(zm(yi
i=1

-

f

(xi

2
)) )

-

1n

n2

(
i=1

zm(yi

-

f

(xi

2
)))

= T1 + T2

(C.6)

To prove the theorem, we will simply bound the supremum supfF E(f )  supfF T1 + T2 . Pick a 

covering F

of F

with

size s log

C 

where 

will be

determined

later in

this proof.

We first bound E(f )

for

all f from



F. Given a fixed f zm(yi - f (xi)) 2

, observe  O(K¯ )

that zm(zm(yi - f (xi = O(K + R). Applying

2
)) )

1



O(K¯

2
)

=

O(K

+ R)2

which follows

subexponential concentration, since T1 is sum of

i.i.d. subexponentials, we have

P( T1  )  exp(-O(n min{2 K¯ 2,  K¯ })).

(C.7)

Next, since

O(K¯ )

 n

zm(yi - f (xi)) 2  O(K¯ ), we can conclude that

1 n

n
i=1

zm(yi

-

f

(xi))

2





T2

1



O(K¯

2
)

n.

Using (C.7) for T1 and the subexponential tail bound for T2 holds

when





K¯ ,

and

assuming

the

number

of

samples

n

obeys

n



O(max{-1,

-2}K¯ 2s log

C 

),

then

for

all

cover elements

T1 + T2  2.

holds with probability at least 1 - exp(-O(n)). To conclude the proof, we need to move from the cover F

to F . Pick f  F and its  neighbor f  F. Utilizing the deterministic relation zm(X)  X + E[X] and

using the fact that f is in a  neighborhood of f , we arrive at the following bounds

zm(f (x) - f(x))  ( x 2 + E[ x 2 ]).

(C.8)

zm(f (x) + f(x) - 2y)  2R( x 2 + E[ x 2 ]) + 2K

Next O(K

observe that, with probability at least 1 - 4n exp(-p), p). Combining this with (C.8), we conclude that for all

all 1

xi, i

yi n

obey

zm(y) . xi 2 

O(p),

(C.9) zm(yi) 

zm(f

(xi)

-

yi)2

-

zm(f (xi )

-

yi

2
)

 O(K¯ p).

(C.10)

23

Under review as a conference paper at ICLR 2019

Expanding the square differences in the same way, an identical argument shows the following two deviation

bounds,

E[zm(f

(xi)

-

yi

2
)

-

zm(f (xi )

-

yi)2]

 O(K¯ p),

(C.11)

1 n2

nn

(

zm(yi

-

f

(xi

2
)))

-

(

zm(yi

-

f

(xi

2
)))

i=1 i=1

 O(K¯ p).

Combining these three inequalities ((C.10) and (C.11)) and substituting them in (C.6), we conclude that for all

neighbors f, f , E(f ) - E(f)  O(K¯ p).
Next we set  = c (pK¯ ) for a sufficiently small constant c > 0, to find that with probability at least 1-exp(-n),

supf F

E(f )





holds

as

long

as

the

number

of

samples

n

obeys

n



O(max{-1,

-2}K¯ 2s

log

CpK¯ c

).

We

define Lerm(f )

=

1 n

n
i=1

(yi

-

f

(xi)

-

ravg(f

2
))

and Lpop(f )

=

E[zm(f (x) - y)2].

We also

denote the

CERM minimizer ferm = arg minfF L(f ) and population minimizer fpop = minfF Lpop(f ). Inequality

(C.5) follows from the facts that we simultaneously have E(ferm)  O() and E(fpop)  O() which

implies that

Lpop(ferm)  Lerm(ferm) + O()  Lerm(fpop) + O()  Lpop(fpop) + O(), concluding the proof.

C.3 PROOF OF THEOREM C.1

In this section we will show how Theorem C.1 follows from Theorem C.2. To this aim we need to show that Fk^,B has a small Lipscshitz covering number. We construct the following cover F  for the set Fk^,B. Let B = B1 D. Pick a   B 2 cover C of the interval [-B, B] which has size 2B . Let Ci be identical copies

of C. We set

F  = {fCNN( k^( ))   C , 1   D}.

In words, we construct CNNs by picking numbers from with them. We now argue that F  provides a cover of

the F.

cartesian Given f

product C1 × . . . CD  F with scalings 

and scaling {k^( , there exists f

 )}DF=1

which uses scalings  such that  -   . Now, let f be the function with scalings i until i = and i for

i > . Note that f0 = f , fD = f . With this, we write

D

f - f  L 

fi+1 - fi L .

i=1

Observe that fi-1 be the function of

and fi layers

have i+1

equal to D.

layers except We have that

the ith layer. fi+1(x) - fi

Let (x)

g1 be the function of the first i-1 layers and g2 = g2((Ki(g1(x)))) - g2((Ki(g1(x))))

where Ki, Ki differ in the ith layer kernels of f and f  created from ik^(i) and ik^(i) respectively. Also,

observe that g1 is Bi-1 Lipschitz and g2(()) is BD-i Lipschitz functions. Hence,

g2((Ki(g1(x)))) - g2((Ki(g1(x))))  BD-i Ki(g1(x)) - Ki(g1(x))  BD-iBi-1  BD-1.

(C.12)

Summing over all i, this implies that f - f  L  DBD-1. Recalling F   (2B )D and setting  =  (DBD-1), the  covering number of Fk^,B is N  (2DBD )D = (2DB )D which implies

log

N

=

D

log(

2DB 

).

Now,

since

all

kernels

have

Euclidean

norm

bounded

by

B,

we

have

fCNN() L  B

and f L  B for all f  F . This also implies zm(fCNN(x)) 2 = O(B). Hence, we can apply Theorem

C.2 to conclude the proof of Theorem C.1.

24

