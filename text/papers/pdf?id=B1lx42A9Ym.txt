Under review as a conference paper at ICLR 2019
NEURAL RENDERING MODEL: JOINT GENERATION AND PREDICTION FOR SEMI-SUPERVISED LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Unsupervised and semi-supervised learning are important problems that are especially challenging with complex data like natural images. Progress on these problems would accelerate if we had access to appropriate generative models under which to pose the associated inference tasks. Inspired by the success of Convolutional Neural Networks (CNNs) for supervised prediction in images, we design the Neural Rendering Model (NRM), a new hierarchical probabilistic generative model whose inference calculations correspond to those in a CNN. The NRM introduces a small set of latent variables at each level of the model and enforces dependencies among all the latent variables via a conjugate prior distribution. The conjugate prior yields a new regularizer for learning based on the paths rendered in the generative model for training CNNs­the Rendering Path Normalization (RPN). We demonstrate that this regularizer improves generalization both in theory and in practice. Likelihood estimation in the NRM yields the new Max-Min cross entropy training loss, which suggests a new deep network architecture­the MaxMin network­which exceeds or matches the state-of-art for semi-supervised and supervised learning on SVHN, CIFAR10, and CIFAR100. Keywords: neural nets, generative models, semi-supervised learning, cross-entropy
1 INTRODUCTION
Unsupervised and semi-supervised learning have still lagged behind compared to performance leaps we have seen in supervised learning over the last five years. This is partly due to a lack of good generative models that can capture all latent variations in complex domains such as natural images and provide useful structures that help learning. When it comes to probabilistic generative models, it is hard to design good priors for the latent variables that drive the generation.
Instead, recent approaches avoid the explicit design of image priors. For instance, Generative Adversarial Networks (GANs) use implicit feedback from an additional discriminator that distinguishes real from fake images (Goodfellow et al., 2014). Using such feedback helps GAN to generate visually realistic images, but it is not clear if this is the most effective form of feedback for predictive tasks. Moreover, due to separation of generation and discrimination in GANs, there are typically more parameters to train, and this might make it harder to obtain gains for semi-supervised learning in the low (labeled) sample setting.
We propose an alternative approach to GANs by designing a class of probabilistic generative models, such that inference in those models also has good performance on predictive tasks. This approach is well-suited for semi-supervised learning since it eliminates the need for a separate prediction network. Specifically, we answer the following question: what generative processes output Convolutional Neural Networks (CNNs) when inference is carried out? This is natural to ask since CNNs are stateof-the-art (SOTA) predictive models for images, and intuitively, such powerful predictive models should capture some essence of image generation. However, standard CNNs are not directly reversible and likely do not have all the information for generation since they are trained for predictive tasks such as image classification. We can instead invert the irreversible operations in CNNs, e.g., the rectified linear units (ReLUs) and spatial pooling, by assigning auxiliary latent variables to account for uncertainty in the CNN's inversion process due to the information loss.
Contribution 1 ­ Neural Rendering Model: We develop the Neural Rendering Model (NRM) whose bottom-up inference corresponds to a CNN architecture of choice (see Figure 1a). The "reverse"
1

Under review as a conference paper at ICLR 2019

image

unpooled feature
map

pooled rectified feature feature
map map

CNN:Inference NRM:Generation Render Upsample, Choose
select render location ornot
renderedupsampled masked class image template template template

0.5dog 0.2cat 0.1horse ... 1.0dog

OneLayer Rendering
inNRM
...

object category

imageatlevel` <latexit sha1_base64="rCGBMzn4wBg09WD3n0kBj8YdxsQ=">AAAB63icdVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGCeUCyhNlJbzJkZnaZmRXCkl/w4kERr/6QN//G2SRCfBU0FFXddHdFqeDG+v6Ht7K6tr6xWdoqb+/s7u1XDg5bJsk0wyZLRKI7ETUouMKm5VZgJ9VIZSSwHY2vC799j9rwRN3ZSYqhpEPFY86oLaQeCtGvVIOaPwPxf5EvqwoLNPqV994gYZlEZZmgxnQDP7VhTrXlTOC03MsMppSN6RC7jioq0YT57NYpOXXKgMSJdqUsmanLEzmVxkxk5DoltSPz0yvEv7xuZuOrMOcqzSwqNl8UZ4LYhBSPkwHXyKyYOEKZ5u5WwkZUU2ZdPOXlEP4nrfNa4NeC24tqvbOIowTHcAJnEMAl1OEGGtAEBiN4gCd49qT36L14r/PWFW8xcwTf4L19AhWEjlI=</latexit>
Is_render=Yes Location=LowerRight
(`)
<latexit sha1_base64="4Bj31PbILTA1W2KIvnDbqaOUfGA=">AAAB83icdVDLSgNBEJyNrxhfUY9eFoMQL2FXBD0GPOgxgnlAdgm9k95kyMzsMjMrhCW/4cWDIl79GW/+jZOHEF8FDUVVN11UlHKmjed9OIWV1bX1jeJmaWt7Z3evvH/Q0kmmKDZpwhPViUAjZxKbhhmOnVQhiIhjOxpdTf32PSrNEnlnximGAgaSxYyCsVIQXIMQUA2Q89NeueLXvBlc7xf5sipkgUav/B70E5oJlIZy0Lrre6kJc1CGUY6TUpBpTIGOYIBdSyUI1GE+yzxxT6zSd+NE2ZHGnanLFzkIrccispsCzFD/9KbiX143M/FlmDOZZgYlnT+KM+6axJ0W4PaZQmr42BKgitmsLh2CAmpsTaXlEv4nrbOa79X82/NKvbOoo0iOyDGpEp9ckDq5IQ3SJJSk5IE8kWcncx6dF+d1vlpwFjeH5Buct080xJEy</latexit>

imageatlevel`<latexit sha1_base64="rCGBMzn4wBg09WD3n0kBj8YdxsQ=">AAAB63icdVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGCeUCyhNlJbzJkZnaZmRXCkl/w4kERr/6QN//G2SRCfBU0FFXddHdFqeDG+v6Ht7K6tr6xWdoqb+/s7u1XDg5bJsk0wyZLRKI7ETUouMKm5VZgJ9VIZSSwHY2vC799j9rwRN3ZSYqhpEPFY86oLaQeCtGvVIOaPwPxf5EvqwoLNPqV994gYZlEZZmgxnQDP7VhTrXlTOC03MsMppSN6RC7jioq0YT57NYpOXXKgMSJdqUsmanLEzmVxkxk5DoltSPz0yvEv7xuZuOrMOcqzSwqNl8UZ4LYhBSPkwHXyKyYOEKZ5u5WwkZUU2ZdPOXlEP4nrfNa4NeC24tqvbOIowTHcAJnEMAl1OEGGtAEBiN4gCd49qT36L14r/PWFW8xcwTf4L19AhWEjlI=</latexit>

1

y <latexit sha1_base64="zX/nehuC+fK5+AT4o3l1JMUrCQQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPBi8cW7Ae0oWy2k3btZhN2N0II/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0bu53n1BpHssHkyXoR3QsecgZNVZqZcNqza27C5B14hWkBgWaw+rXYBSzNEJpmKBa9z03MX5OleFM4KwySDUmlE3pGPuWShqh9vPFoTNyYZURCWNlSxqyUH9P5DTSOosC2xlRM9Gr3lz8z+unJrz1cy6T1KBky0VhKoiJyfxrMuIKmRGZJZQpbm8lbEIVZcZmU7EheKsvr5POVd1z617rutboFXGU4QzO4RI8uIEG3EMT2sAA4Rle4c15dF6cd+dj2VpyiplT+APn8wfuW40T</latexit>
... ...

...

image

x <latexit sha1_base64="TQGEygQLk4MpTONXYfpVxGVfRf8=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeCF48t2A9oQ9lsJ+3azSbsbsQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ7dzvPKLSPJb3ZpqgH9GR5CFn1Fip+TQoV9yquwBZJ15OKpCjMSh/9YcxSyOUhgmqdc9zE+NnVBnOBM5K/VRjQtmEjrBnqaQRaj9bHDojF1YZkjBWtqQhC/X3REYjradRYDsjasZ61ZuL/3m91IQ3fsZlkhqUbLkoTAUxMZl/TYZcITNiagllittbCRtTRZmx2ZRsCN7qy+ukfVX13KrXvK7Uu3kcRTiDc7gED2pQhztoQAsYIDzDK7w5D86L8+58LFsLTj5zCn/gfP4A7NeNEg==</latexit>

OneLayer Inference
inCNN

...

latent variables

featuremap
atlevel` <latexit sha1_base64="W6wQyAqS/avuc1urkGvgoX6NZTA=">AAAB63icbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGCeUCyhNlJbzJkZnaZmRXCkl/w4kERr/6QN//G2SSIJhY0FFXddHdFqeDG+v6Xt7a+sbm1Xdop7+7tHxxWjo5bJsk0wyZLRKI7ETUouMKm5VZgJ9VIZSSwHY1vC7/9iNrwRD3YSYqhpEPFY86oLaQeCtGvVP2aPwP5IcEyqcICjX7lszdIWCZRWSaoMd3AT22YU205Ezgt9zKDKWVjOsSuo4pKNGE+u3VKzp0yIHGiXSlLZurviZxKYyYycp2S2pFZ9grxP6+b2fgmzLlKM4uKzRfFmSA2IcXjZMA1MismjlCmubuVsBHVlFkXT9mFsPLyKmld1gK/FtxfVeudRRwlOIUzuIAArqEOd9CAJjAYwRO8wKsnvWfvzXuft655i5kT+APv4xsVco5S</latexit>

ReLU If render

MaxPool

Location W

(`)/= <latexit sha1_base64="UPXTkYqCXzy/MqdSgfFHsL9yZc8=">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGCeUCyhNnJJBkyOzPM9AphyUd48aCIV7/Hm3/j5IFoYkFDUdVNd1dipHAYhl/B2vrG5tZ2Yae4u7d/cFg6Om44nVnG60xLbVsJdVwKxesoUPKWsZymieTNZHQ79ZuP3Dqh1QOODY9TOlCiLxhFLzU7xmqDulsqh5VwBvJDomVShgVq3dJnp6dZlnKFTFLn2lFoMM6pRcEknxQ7meOGshEd8Laniqbcxfns3Ak590qP9LX1pZDM1N8TOU2dG6eJ70wpDt2yNxX/89oZ9m/iXCiTIVdsvqifSYKaTH8nPWE5Qzn2hDIr/K2EDamlDH1CRR/CysurpHFZicJKdH9VrrYWcRTgFM7gAiK4hircQQ3qwGAET/ACr4EJnoO34H3euhYsZk7gD4KPb6rnj9c=</latexit>

Convolution

<latexit sha1_base64="vo8q8OCg4qCn9Dq28xQwFaWIFVs=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoMQL2FXAnoRAl48RjAPTJYwO+lNhszOLjOzQljyF148KOLVv/Hm3zh5IJpY0FBUddPdFSSCa+O6X05ubX1jcyu/XdjZ3ds/KB4eNXWcKoYNFotYtQOqUXCJDcONwHaikEaBwFYwupn6rUdUmsfy3owT9CM6kDzkjBorPbTKXRTinFyTXrHkVtwZyA/xlkkJFqj3ip/dfszSCKVhgmrd8dzE+BlVhjOBk0I31ZhQNqID7FgqaYTaz2YXT8iZVfokjJUtachM/T2R0UjrcRTYzoiaoV72puJ/Xic14ZWfcZmkBiWbLwpTQUxMpu+TPlfIjBhbQpni9lbChlRRZmxIBRvCysurpHlR8dyKd1ct1dqLOPJwAqdQBg8uoQa3UIcGMJDwBC/w6mjn2Xlz3uetOWcxcwx/4Hx8A7rpj7M=</latexit>

<latexit sha1_base64="dhvBbn2P5pU+jxDOYi6LGmOTXur3LZeU46dUfSkGH42QSDHd5pwS3XQit43qEuZsdERa+clQzuZaAdYFPAqgGQauQaP3hmHAcQbFvEHhkBESkxpmKoMR84mXrFBORHygswYg=">AAAB/82XHicdbVZDBLNSwgMxFMI3X4Uvr+1VP6LKFUe8y21vt620V6qSUWd71hbLrdBYNuIB8gHktEiqWQpN1ous2WCEy2oX7IZKGzobjcMaSnBut4NQCdLpCLuw1HnYi1RUF5WZ0uwsWKAbcT9E/6gZCog+H1dUoOJM05DasJROJmWMSMzTS52JkTb1NS4pothbpmMSQahs5BW5ZmkPSDIMOk7y8RghGjI2SHDwhKEY1DUaFPH09+h/x/KFBG4Fjx0x2YQtIe05ERFKLf+tR3G3CPf6wo9fIz3chOv7+/ovW/39wGb3pMp09HzC04gwHJtd+UtQhb1XvDP4wRgEQc4D4LuE+hDOzHM3kCuPf6nuIc5h5S3vlRSpy35wcsOu4fkm4lY8AHSLKhzQu4mUYwAjIBNPecM9/jd+72e9cnbO1KAWtdXoWYlVr3lbN1qb2dr/WXe+v1rg2fS9Gjcu8fzJgXvNlGmf30cNZXactfNurbzr6j2eezkv/2ub9ofNFP6Xer/m3uo7Wd3Gl2Xtfk/vZQzbGl0MOpgWsnVtGkjqUCsyKoimh0SlWp2vzRlSUxC8W5pajETMnXsSRmb61JFIENTpx6FLwQXMDUzpB82ZSJCZF5guKeV1sWJ2gDWCIpDRUNOw48LgEc776gdmUaylzyQNSZzFYLaIEsJFkYfRJPybkg2BK4jW76k1uU9fgHT07oijHNNyY7Fu71G2K+nO83/gf7suUfTpacW2gLed20YXKsX4kTMgb0zUG9TQ3r62eknMgmjCMNSWyxrFs4lhYK4PWo8UcIMkmBtFGgkDUiC5KyhkWSG7JPencGOYEBs6OKrLoqzBvFynUaSRcH3r0zayve0ABH3dIJlrLWZ0rMAGjAWRS8K2Q3cIW+OVpEKs4xYRBDWHIZqCTEGZCJ9RNJeYacOTNCRa6ZsbDgkg+HCEQ/Glx7OS/zKSO/2nmT73VK7vD7o1FFvBbDe7U/y9MJmGKxhzNbYxFaD0YPC3d9hyXihf3+pExTmFAZVBLvZqUFgTmqcURrYAHL0FNvNGS/enfaKe3v+F+u3woB7ePr30gRz1gB9jFEuILUNiqSw8CLJlSaVUXiEMoAYoUNIzR2I7ORXYjGE60mrgY/XZUKGu7PD9+ZvNBMluJL5dM7oDRq5LwRa0wiMtcMRQsCxLoZpOzwpYwBzOZBuiN6mzuidlUkkEghBtm7qSAaW0YwaENjhpPDpN+oC5iAiAoMxAYdG89ZkhbDaKv1xKu32NpLr9CJYK4VauipSIJKYUmZiW9Bn4rb0n+5WyOsDLEtT+3PT/jwMmGEdnnR6Hz1NDbht5gmpNKl4Al7g0xXGKhZNQaYpHn2Oy2MWzj1PH/5GRpgAzLq0ZfBbi+7CWp8rx4dyloktRs3DiRU1BiNE6pwmklTMdL0DCZVBZEkoJDMX3S7ukUaay2ZrkA0HYJk6U7DN2MKMCDI1Iz0vP/0ZVb1vozya52PLw4n4+vatl5/6dXt9m+VYXdaSN12JivINMMThCY6hoOrvI7fL5OJhzrwRVYx+WPwTwlcYKFaUFakWThnSBaoMFmblZ5Jg94YP+KkOiJPcIVN4oG16CBwyvQjIaek8tN86bAInAZQpE3UJZ7PHC5TnOYPoRCSNbSagKyiGUI28RhLjc9CMDOZE7cJMYk2N7RIsHo1dqMwtBUb7u1crYN3kzrxweIiHiEMORGsiYtJgKA6NMIaKUnrN0LMOiBMyu9F+rMiX75lZoEVsVEZolZLIhl/N+E3v7J+CAa/80HCJTaDm19qZXu28Cf+B2LLqVfe/ghKrO/hjUXvhIpw43v+UB3/VhXsqaAQ9eesvhS1n6Yki5MnAjCH43A5igpRnA1j2r7CgRDs4QucE1ACaqRpYo7XB5ihPcHG6juA956p2L1DNOhgA4NOarXIUASG0+p8FBCg6DHMzqIxBd7iY8RBXEh4xFKeGVMGIroXN6Qoc43A7<B3+/T9pl2fJhCa7zet9Z8TZe+637xf9imjeK8edt+Xa>jikR18L5eObniNKv9+6vfzOw51a4TcSIsVF+7n8ZbJf3zlxwJN8TMPCRB3mNk+zphCME8=vw<fn/Q2Jl0vaubtl5eKSxoui=<t/>latexit>

>(`)

featuremap

atlevel` <latexit sha1_base64="W6wQyAqS/avuc1urkGvgoX6NZTA=">AAAB63icbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGCeUCyhNlJbzJkZnaZmRXCkl/w4kERr/6QN//G2SSIJhY0FFXddHdFqeDG+v6Xt7a+sbm1Xdop7+7tHxxWjo5bJsk0wyZLRKI7ETUouMKm5VZgJ9VIZSSwHY1vC7/9iNrwRD3YSYqhpEPFY86oLaQeCtGvVP2aPwP5IcEyqcICjX7lszdIWCZRWSaoMd3AT22YU205Ezgt9zKDKWVjOsSuo4pKNGE+u3VKzp0yIHGiXSlLZurviZxKYyYycp2S2pFZ9grxP6+b2fgmzLlKM4uKzRfFmSA2IcXjZMA1MismjlCmubuVsBHVlFkXT9mFsPLyKmld1gK/FtxfVeudRRwlOIUzuIAArqEOd9CAJjAYwRO8wKsnvWfvzXuft655i5kT+APv4xsVco5S</latexit>

-

1

Noise <latexit sha1_base64="43puJAqhD10g3YH6cy9acITUDNU=">AAAB83icbVBNS8NAEN3Ur1q/qh69BIvgqSQi6LHoxZNUsLXQhLLZTtqlm92wOxFL6N/w4kERr/4Zb/4bt20O2vpg4PHeDDPzolRwg5737ZRWVtfWN8qbla3tnd296v5B26hMM2gxJZTuRNSA4BJayFFAJ9VAk0jAQzS6nvoPj6ANV/IexymECR1IHnNG0UpBgPCE+a3iBia9as2rezO4y8QvSI0UaPaqX0FfsSwBiUxQY7q+l2KYU42cCZhUgsxAStmIDqBrqaQJmDCf3TxxT6zSd2OlbUl0Z+rviZwmxoyTyHYmFIdm0ZuK/3ndDOPLMOcyzRAkmy+KM+GicqcBuH2ugaEYW0KZ5vZWlw2ppgxtTBUbgr/48jJpn9V9r+7fndcaV0UcZXJEjskp8ckFaZAb0iQtwkhKnskreXMy58V5dz7mrSWnmDkkf+B8/gCZlZIJ</latexit>

(a) (b)

Figure 1: (a) The Neural Rendering Model (NRM) captures latent variations in the data and yields
CNNs as its inference. Costs for training CNNs are derived from likelihood estimations in NRM. (b)
Graphical model depiction of NRM. Latent variables in NRM depend on each other. Object category y decides the class template, and then new latent variables are incorporated at each layer to render intermediate images (red) with finer details. Finally, pixel noise is added to render the image x.

top-down process of image generation is through coarse-to-fine rendering, which progressively increases the resolution of the rendered image. This is intuitive since the reverse process of bottom-up inference reduces the resolution (and dimension) through operations such as spatial pooling. We also introduce structured stochasticity in the rendering process through a small set of discrete latent variables, which capture the uncertainty in reversing the CNN feed-forward process. The rendering in NRM follows a product of linear transformations, which can be considered as the transpose of the inference process in CNNs. In particular, the rendering weights in NRM are proportional to the transpose of the filters in CNNs. Furthermore, the bias terms in the ReLU units at each layer (after the convolution operator) make the latent variables in different network layers dependent (when the bias terms are non-zero). This design of image prior has an interesting interpretation from a predictive-coding perspective in neuroscience: the dependency between latent variables can be considered as a form of backward connections that captures prior knowledge from coarser levels in the NRM and helps adjust the estimation at the finer levels (Rao & Ballard, 1999; Friston, 2018).

NRM is a likelihood-based framework, where unsupervised learning can be derived by maximizing the expected complete-data log-likelihood of the model while supervised learning is done through optimizing the class-conditional log-likelihood. Semi-supervised learning unifies both log-likelihoods into an objective cost for learning from both labeled and unlabeled data. The NRM prior has the desirable property of being a conjugate prior, which makes learning in NRM computationally efficient.

Interestingly, we derive the popular cross-entropy loss used to train CNNs for supervised learning as a lower bound of the NRM's negative class-conditional log-likelihood. This new interpretation of cross-entropy allow us to develop better losses for training CNNs. An example is the Max-Min cross-entropy discussed in Contribution 2 and Section 3.

Contribution 2 ­ New regularization, loss function, architecture and generalization bounds: The joint nature of generation, inference, and learning in NRM allows us to develop new training procedures for semi-supervised and supervised learning, as well as new theoretical (statistical) guarantees for learning. In particular, for training, we derive a new form of regularization termed as Rendering Path Normalization (RPN) from the NRM's conjugate prior. A rendering path is a set of latent variable values in NRM. Unlike the path-wise regularizer in (Neyshabur et al., 2015), RPN uses information from a generative model to penalizes the number of the possible rendering paths, encourage the network to be compact in terms of representing the image. It also helps to enforce the dependency among different layers in NRM during training and improves classification performance.

We provide new theoretical bounds based on NRM. In particular, we prove that NRM is statistically consistent and derive the generalization bounds of NRM for (semi-)supervised learning tasks. Our generalization bound is proportional to the number of rendering paths that generate close-to-real images. This suggests that RPN regularization may help in generalization since RPN enforces the dependencies among latent variables in NRM and, therefore, reduces the number of such rendering configurations. We observe that RPN helps improve generalization in our experiments.

Max-Min cross-entropy and network: We propose the new Max-Min cross-entropy loss function for learning, based on negative class-conditional log-likelihood in NRM. It combines the traditional cross-entropy with another loss, which we term as Min cross-entropy. While the traditional (Max) cross-entropy maximizes the probability of the correct labels, the Min cross-entropy minimizes the

2

Under review as a conference paper at ICLR 2019

probability of the incorrect labels. We show that the Max-Min cross-entropy is also a lower bound to the negative conditional log-likehood of NRM, just like the cross-entropy loss. The Max-Min cross-entropy is realized through a new CNN architecture, namely the Max-Min network, which is a CNN with an additional branch sharing weights with the original CNN but containing minimum pooling (MinPool) operator and negative rectified linear units (NReLUs), i.e., min(·, 0) (see Figure 4). Although the Max-Min network is derived from NRM, it is a meta-architecture that can be applied independently on any CNN architecture. We show empirically that Max-Min networks and crossentropy help improve the SOTA on object classification for supervised and semi-supervised learning.

Contribution 3 ­ State-of-the-art empirical results for semi-supervised and supervised learning: We show strong results for semi-supervised learning over CIFAR10, CIFAR100 and SVHN benchmarks in comparison with SOTA methods that use and do not use consistency regularization. Consistency regularization, such as those used in Temporal Ensembling (Laine & Aila, 2017) and Mean Teacher (Tarvainen & Valpola, 2017), enforces the networks to learn representation invariant to realistic perturbations of the data. NRM alone outperforms most SOTA methods which do not use consistency regularization (Salimans et al., 2016; Dumoulin et al., 2017) in most settings. Max-Min cross-entropy then helps improves NRM's semi-supervised learning results significantly. When combining the NRM, Max-Min cross-entropy, and Mean Teacher, we achieve SOTA results or very close to those on CIFAR10, CIFAR100, and SVHN (see Table 2, 3, and 4). Interestingly, compared to the other competitors, our method is consistently good, achieving either best or second best results in all experiments. Furthermore, Max-Min cross-entropy also helps supervised learning. Using the Max-Min cross-entropy, we achieve SOTA result for supervised learning on CIFAR10 (2.30% test error). Similarly, Max-Min cross-entropy helps improve supervised training on ImageNet.

Despite good classification results, there is a caveat that NRM may not generate good looking images since that objective is not "baked" into its training. It is primarily aimed at improving semi-supervised and supervised learning through better regularization. Potentially, an adversarial loss can be added to NRM to improve visual characteristics of the image, but that is beyond the scope of this paper.

Related Work: In addition to GANs, other recently developed deep generative models include the variational autoencoders (VAE) (Kingma & Welling, 2013) and the deep generative networks (Kingma et al., 2014). Unlike these models, which replace complicated or intractable inference by CNNs, NRM derives CNNs as its inference. This advantage allows us to develop better learning algorithms for CNNs with guaranteed statistical guarantees, as being discussed in Section 2.3 and 2.4. Recent works including the Bidirectional GANs (Donahue et al., 2017) and the Adversarially Learned Inference model (Dumoulin et al., 2017) try to make the discriminator and generator in GANs reversible of each other, thereby providing an alternative way to invert CNNs. These approaches, nevertheless, still employ a separate network to bypass the irreversible operators in CNNs. NRM is also close in spirit to the Deep Rendering Model (DRM) (Patel et al., 2016) and the Multi-layered Convolutional-SparseCoding Model (ML-CSC) (Papyan et al., 2018) but markedly different. Compared to NRM, DRM and ML-CSC has several limitations. In particular, all the latent variables in DRM are assumed to be independent, which is rather unrealistic. This lack of dependency causes the missing of the bias terms in the ReLUs of the CNN derived from DRM. Furthermore, the cross-entropy loss used in training CNNs for supervised learning tasks is not captured naturally by DRM and ML-CSC. Due to these limitations, model consistency and generalization bounds are not be derived for DRM and ML-CSC.

Notation: To facilitate the presentation, the NRM's notations are deferred to Table 5 in Appendix A.

2 THE NEURAL RENDERING MODEL

2.1 GENERATIVE MODEL

The NRM attempts to invert the CNNs as its inference by employing the structure of its latent variables. The dependencies among latent variables in the model are implicitly captured by their conjugate prior distribution. More precisely, NRM can be defined as follows:

Definition 2.1. [Neural Rendering Model (NRM)] NRM is a deep generative model in which the

latent variables z( ) = {t( ), s( )}L=1 at different layers are dependent. Let x be the input image and y  {1, . . . , K} be the target variable, e.g. object category. Generation in NRM takes the form

z|y

Softmax

1 2

(y,

z)

; z|y  Cat(z|y)

(1)

h(y, z; 0) (z; 1)(z; 2) · · · (z; L)µ(y); x|z, y  N (h(0), 21D(0)),

(2)

3

Under review as a conference paper at ICLR 2019

render atthe next location

h(`)
<latexit sha1_base64="e/CdV7XsMf2Xh+89wrgoEpUAHLQ=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBahXkoigh4LXjxWsB/QhrLZTtqlm03Y3Qgl9Ed48aCIV3+PN/+N2zQHbX0w8Hhvhpl5QSK4Nq777ZQ2Nre2d8q7lb39g8Oj6vFJR8epYthmsYhVL6AaBZfYNtwI7CUKaRQI7AbTu4XffUKleSwfzSxBP6JjyUPOqLFSd1IfoBCXw2rNbbg5yDrxClKDAq1h9WswilkaoTRMUK37npsYP6PKcCZwXhmkGhPKpnSMfUsljVD7WX7unFxYZUTCWNmShuTq74mMRlrPosB2RtRM9Kq3EP/z+qkJb/2MyyQ1KNlyUZgKYmKy+J2MuEJmxMwSyhS3txI2oYoyYxOq2BC81ZfXSeeq4bkN7+G61uwVcZThDM6hDh7cQBPuoQVtYDCFZ3iFNydxXpx352PZWnKKmVP4A+fzB6AMjyg=</latexit>

p
<latexit sha1_base64="fhC54FVT5NGEMJ6yFbTk8xYGdwc=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPBi8cWTFtoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dkobm1vbO+Xdyt7+weFR9fikrZNMMfRZIhLVDalGwSX6hhuB3VQhjUOBnXByN/c7T6g0T+SDmaYYxHQkecQZNVZqpYNqza27C5B14hWkBgWag+pXf5iwLEZpmKBa9zw3NUFOleFM4KzSzzSmlE3oCHuWShqjDvLFoTNyYZUhiRJlSxqyUH9P5DTWehqHtjOmZqxXvbn4n9fLTHQb5FymmUHJlouiTBCTkPnXZMgVMiOmllCmuL2VsDFVlBmbTcWG4K2+vE7aV3XPrXut61qjW8RRhjM4h0vw4AYacA9N8IEBwjO8wpvz6Lw4787HsrXkFDOn8AfO5w/gt40K</latexit>

 <latexit sha1_base64="ZshQRDCewTBC9osbqPvttQWWNL4=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGCeUCyhNnJbDJmdmaZ6RXCkn/w4kERr/6PN//GSbIHTSxoKKq66e6KUiks+v63t7a+sbm1Xdop7+7tHxxWjo5bVmeG8SbTUptORC2XQvEmCpS8kxpOk0jydjS+nfntJ26s0OoBJykPEzpUIhaMopNaPRQJt/1K1a/5c5BVEhSkCgUa/cpXb6BZlnCFTFJru4GfYphTg4JJPi33MstTysZ0yLuOKuqWhPn82ik5d8qAxNq4Ukjm6u+JnCbWTpLIdSYUR3bZm4n/ed0M45swFyrNkCu2WBRnkqAms9fJQBjOUE4cocwIdythI2ooQxdQ2YUQLL+8SlqXtcCvBfdX1XqniKMEp3AGFxDANdThDhrQBAaP8Ayv8OZp78V79z4WrWteMXMCf+B9/gC8yY9K</latexit>

h(`)
<latexit sha1_base64="e/CdV7XsMf2Xh+89wrgoEpUAHLQ=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBahXkoigh4LXjxWsB/QhrLZTtqlm03Y3Qgl9Ed48aCIV3+PN/+N2zQHbX0w8Hhvhpl5QSK4Nq777ZQ2Nre2d8q7lb39g8Oj6vFJR8epYthmsYhVL6AaBZfYNtwI7CUKaRQI7AbTu4XffUKleSwfzSxBP6JjyUPOqLFSd1IfoBCXw2rNbbg5yDrxClKDAq1h9WswilkaoTRMUK37npsYP6PKcCZwXhmkGhPKpnSMfUsljVD7WX7unFxYZUTCWNmShuTq74mMRlrPosB2RtRM9Kq3EP/z+qkJb/2MyyQ1KNlyUZgKYmKy+J2MuEJmxMwSyhS3txI2oYoyYxOq2BC81ZfXSeeq4bkN7+G61uwVcZThDM6hDh7cQBPuoQVtYDCFZ3iFNydxXpx352PZWnKKmVP4A+fzB6AMjyg=</latexit>

p
<latexit sha1_base64="/ZAggRAtMCulDnXktOjUB3UP/A8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBZBEEoigh4LXjxWtB/QhrLZbtqlm03YnQgl9Cd48aCIV3+RN/+N2zQHbX0w8Hhvhpl5QSKFQdf9dkpr6xubW+Xtys7u3v5B9fCobeJUM95isYx1N6CGS6F4CwVK3k00p1EgeSeY3M79zhPXRsTqEacJ9yM6UiIUjKKVHpILb1CtuXU3B1klXkFqUKA5qH71hzFLI66QSWpMz3MT9DOqUTDJZ5V+anhC2YSOeM9SRSNu/Cw/dUbOrDIkYaxtKSS5+nsio5Ex0yiwnRHFsVn25uJ/Xi/F8MbPhEpS5IotFoWpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nYkPwll9eJe3LuufWvfurWqNbxFGGEziFc/DgGhpwB01oAYMRPMMrvDnSeXHenY9Fa8kpZo7hD5zPH7cLjXo=</latexit>

+1
 <latexit sha1_base64="ZshQRDCewTBC9osbqPvttQWWNL4=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGCeUCyhNnJbDJmdmaZ6RXCkn/w4kERr/6PN//GSbIHTSxoKKq66e6KUiks+v63t7a+sbm1Xdop7+7tHxxWjo5bVmeG8SbTUptORC2XQvEmCpS8kxpOk0jydjS+nfntJ26s0OoBJykPEzpUIhaMopNaPRQJt/1K1a/5c5BVEhSkCgUa/cpXb6BZlnCFTFJru4GfYphTg4JJPi33MstTysZ0yLuOKuqWhPn82ik5d8qAxNq4Ukjm6u+JnCbWTpLIdSYUR3bZm4n/ed0M45swFyrNkCu2WBRnkqAms9fJQBjOUE4cocwIdythI2ooQxdQ2YUQLL+8SlqXtcCvBfdX1XqniKMEp3AGFxDANdThDhrQBAaP8Ayv8OZp78V79z4WrWteMXMCf+B9/gC8yY9K</latexit>

zero-pad zero-pad

translate translate

... <latexit sha1_base64="ca4An8D0SY3sUfhv6vlZZoxXxlo=">AAAB8nicbVBNS8NAEJ34WetX1aOXYBE8lUQEPRa8eKxgP6AJZbPZtEs3u2F3UiihP8OLB0W8+mu8+W/ctjlo64OBx3szzMyLMsENet63s7G5tb2zW9mr7h8cHh3XTk47RuWasjZVQuleRAwTXLI2chSsl2lG0kiwbjS+n/vdCdOGK/mE04yFKRlKnnBK0Er9IEqKYBIrNLNBre41vAXcdeKXpA4lWoPaVxArmqdMIhXEmL7vZRgWRCOngs2qQW5YRuiYDFnfUklSZsJicfLMvbRK7CZK25LoLtTfEwVJjZmmke1MCY7MqjcX//P6OSZ3YcFlliOTdLkoyYWLyp3/78ZcM4piagmhmttbXToimlC0KVVtCP7qy+ukc93wvYb/eFNv9so4KnAOF3AFPtxCEx6gBW2goOAZXuHNQefFeXc+lq0bTjlzBn/gfP4AzZaRpg==</latexit>

h(`
<latexit sha1_base64="tf2d+fCOcTNB4hDa1H9joi9ZuXo=">AAAB8nicbVBNS8NAEJ34WetX1aOXxSLUgyURQY8FLx4r2A9IQ9lsN+3STTbsToQS+jO8eFDEq7/Gm//GbZuDtj4YeLw3w8y8MJXCoOt+O2vrG5tb26Wd8u7e/sFh5ei4bVSmGW8xJZXuhtRwKRLeQoGSd1PNaRxK3gnHdzO/88S1ESp5xEnKg5gOExEJRtFK/qjW41KSS+Jd9CtVt+7OQVaJV5AqFGj2K1+9gWJZzBNkkhrje26KQU41Cib5tNzLDE8pG9Mh9y1NaMxNkM9PnpJzqwxIpLStBMlc/T2R09iYSRzazpjiyCx7M/E/z88wug1ykaQZ8oQtFkWZJKjI7H8yEJozlBNLKNPC3krYiGrK0KZUtiF4yy+vkvZV3XPr3sN1tdEt4ijBKZxBDTy4gQbcQxNawEDBM7zCm4POi/PufCxa15xi5gT+wPn8AS2Kj+4=</latexit>

... <latexit sha1_base64="ca4An8D0SY3sUfhv6vlZZoxXxlo=">AAAB8nicbVBNS8NAEJ34WetX1aOXYBE8lUQEPRa8eKxgP6AJZbPZtEs3u2F3UiihP8OLB0W8+mu8+W/ctjlo64OBx3szzMyLMsENet63s7G5tb2zW9mr7h8cHh3XTk47RuWasjZVQuleRAwTXLI2chSsl2lG0kiwbjS+n/vdCdOGK/mE04yFKRlKnnBK0Er9IEqKYBIrNLNBre41vAXcdeKXpA4lWoPaVxArmqdMIhXEmL7vZRgWRCOngs2qQW5YRuiYDFnfUklSZsJicfLMvbRK7CZK25LoLtTfEwVJjZmmke1MCY7MqjcX//P6OSZ3YcFlliOTdLkoyYWLyp3/78ZcM4piagmhmttbXToimlC0KVVtCP7qy+ukc93wvYb/eFNv9so4KnAOF3AFPtxCEx6gBW2goOAZXuHNQefFeXc+lq0bTjlzBn/gfP4AzZaRpg==</latexit>

Rendering

x <latexit sha1_base64="TQGEygQLk4MpTONXYfpVxGVfRf8=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeCF48t2A9oQ9lsJ+3azSbsbsQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ7dzvPKLSPJb3ZpqgH9GR5CFn1Fip+TQoV9yquwBZJ15OKpCjMSh/9YcxSyOUhgmqdc9zE+NnVBnOBM5K/VRjQtmEjrBnqaQRaj9bHDojF1YZkjBWtqQhC/X3REYjradRYDsjasZ61ZuL/3m91IQ3fsZlkhqUbLkoTAUxMZl/TYZcITNiagllittbCRtTRZmx2ZRsCN7qy+ukfVX13KrXvK7Uu3kcRTiDc7gED2pQhztoQAsYIDzDK7w5D86L8+58LFsLTj5zCn/gfP4A7NeNEg==</latexit>

1)
Noise

Figure 3: Rendering from level to level - 1 in the NRM. At each pixel p in the intermediate image h( ), NRM renders iff the template selects latent variable s( , p) = 1. If rendering, the template ( , p) is multiplied by the pixel value h( , p). Then the matrix B( , p) zero-pads the result to the size of the intermediate image at level - 1. Next, the translation matrix T ( , p) locally translates the rendered image to location specified by the latent variable t( , p). The same rendering repeats at other pixels of h( ). NRM adds all rendered images to obtain the intermediate image h( - 1).

where (y, z)

L =1

b(t;

), s( )

h( ) and Softmax ()

exp()/

 exp().

The generation process in the NRM can be summarized in the

following steps (details are in Algorithm 1 in Appendix A):

1) Sample the latent variables z from a cat-
egorical distribution whose prior is z|y. 2) Render its coarsest image, the object

Layer 4 Layer 3 Layer 2 Layer 1 Layer 0 Original

template h(L) = µ(y) of class y. 3) Incor-

porate a set of of latent variations z( ) into

h(y; ) at each layer via a linear trans-

formation (z; ) to render the finer im-

age h(y, z; - 1). 4) Add Gaussian pixel noise into h(y, z; 0) to render the final image x. In Eqn.2, if (z; ) is a linear transformation, it will yield many parameters
to learn. As a result, we would like to in-
troduce new structures into NRM so that
CNNs can be derived as NRM's inference.
This way, NRM will be informed by the

Figure 2: Reconstructed images at each layer in a 5layer NRM trained on MNIST with 50K labeled data. Original images are in the rightmost column. Early layers in the rendering such as layer 4 and 3 capture high-level features of the image while later layers such as layer 2 and 1 capture finer-scale features. From layer 2, we begin to see the gist of the rendered digits.

prior knowledge of natural images captured by CNNs. In our attempt to invert CNNs, we constrain

the latent variables z( ) at layer in NRM to a set of template selecting latent variables s( ) and

local translation latent variables t( ). As been shown later in Section 2.2, during inference of NRM,

the ReLU non- linearity at layer "inverts" s( ) to find if particular features are in the image or not.

Similarly, the MaxPool operator "inverts" t( ) to locate where particular features, if exist, are in the

image. Both s( ) and t( ) are vectors indexed by p( )  P( )  {pixels in level }. The rendering

process from layer to layer - 1 is given by:

h( - 1) ( )h( ) =

s( , p)T (t; , p)B( , p)( , p)h( , p).

pP( )

(3)

The rendering process in Eqn.3 is illustrated in Figure 3. At each pixel p in the intermediate image h( ) at layer , NRM decides to use that pixel to render or not according to the value of the template selecting latent variable s(p; ) at this pixel location. If s(p; ) = 1, then NRM renders; otherwise, it does not. If rendering, the pixel value h( , p) is used to scale the local rendering template ( , p), which has the same number of feature maps as the next rendered image h( - 1), but is of smaller size, e.g., 3 × 3 or 5 × 5. Next, the padding matrix B( , p) pads the resultant patch to the size of the image h( - 1) with zeros, and the translation matrix T (t; , p) translates the result to a local location. NRM then keeps rendering at the next pixel location p + 1 following the same process. All rendered images are added to form the final rendered image h( - 1) at layer - 1. Note that we can constrain NRM by enforcing all pixels in the same feature maps of h( ) share the same rendering template. This constraint helps to yield convolutions
in CNNs during the inference of NRM, and the rendering template in NRM now corresponds to
the convolution filters in CNNs. Reconstructed images at each layer of a 5-layered NRM trained
on MNIST are visualized in Figure 2. The network is trained using the semi-supervised learning
framework discussed in Section 2.3.

4

Under review as a conference paper at ICLR 2019

While s( ) and t( ) can be independent, we further constrain the model by enforcing the dependency among s( ) and t( ) at different layers in NRM. This constraint is motivated from realistic rendering of natural objects: different parts of a natural object are dependent
on each other. NRM captures such dependency in natural objects by imposing more structures into the joint prior of latent variables at all layer in the model. The form of the join prior z|y in Eqn. 1 might look mysterious at first, but NRM parametrizes z|y in this particular way so that z|y is the conjugate prior of the model likelihood as being proven in Appendix C .11. The conjugate form of z|y allows efficient inference in the NRM. Parameters b(t; ) of the conjugate prior z|y will become the bias terms after the CNN's
convolutions as will be shown in Theorem 2.2. When training in an unsupervised setup, the conjugate prior results in the RPN regularization as shown in Theorem 2.3(b). This RPN regularization helps enforce the dependencies among latent variables and increases the likelihood of latent configuration presents in the data during training.

Table 1: Correspondence between NRM and CNN.

NRM (Generation)

CNN (Inference)

Rendering template ( )

Transpose of weights W ( )

Class templates µ(y)

Softmax weights

Parameters b(t; ) of Bias terms b( ) after

the conjugate prior y|x

each convolution

Intermediate image h( ) = h(y, z; ) of size D( ) in layer

Feature maps in layer in CNNs

Latent variables z( , p) = {s( , p), t( , p)}
at pixel p, layer

States of ReLUs & MaxPools

Max over template selecting variable s( , p)

ReLU

Max over local translation variable t( , p)

MaxPool

Zero-padding matrices B( , p) in layer

Downsampling in MaxPool

Conditional log-likelihood

Cross-entropy loss

Expected complete-data log-likelihood

Reconstruction loss

NRM with skip connections: We derive ResNet and DenseNet by adding skip connections into the rendering matrices ( ) of NRM (see Appendix B).

2.2 INFERENCE

We show that calculations in the bottom-up inference in NRM corresponds to a CNN (see Figure 1b)

and, therefore, is tractable and efficient. The impact of this correspondence goes beyond a reverse-

engineering effort. First, it provides probabilistic semantics for components in CNNs, justifying

their usage, and providing an opportunities to employ probabilistic inference methods in the context

of CNNs. Second, such a correspondence offers a flexible framework to design CNNs. Instead

of directly engineering CNNs for new tasks and datasets, we can modify NRM to incorporate our

knowledge of the tasks and datasets into the model and perform JMAP inference to achieve a new

CNN architecture. The following theorem establishes the NRM-CNN correspondence:

Theorem 2.2. The JMAP inference of latent variable z in NRM is the feedforward step in CNNs.

Particularly, we have:

max {p(z|x, y)}
z

=

max
z

1 2

{

h(y, z; 0), x

+

(y, z)}

+

const



1 2

µ(y), (L)

+ const

(4)

where (L) is computed recursively. In particular, (0) = x and

( ) = MaxPool(ReLu(Conv( ( ), ( - 1))) + b( )).

(5)

The equality holds in Eqn. 4 when the parameters  in NRM satisfy the non-negativity assumption that the intermediate rendered image h( )  0,  = 1, 2, . . . , L.

There are four key results in Theorem 2.2. First, ReLU non-linearities in CNNs find the optimal value for the template selecting latent variables s( ) at each layer in NRM, detecting if particular features exist in the image or not. Second, MaxPool operators in CNNs find the optimal value for the local translation latent variables t( ) at each layer in NRM, locating where features are rendered in the image. Third, bias terms after each convolution in CNNs are from the prior knowledge of
latent variables in the model. Those bias terms update the posterior estimation of latent variables
from data using the knowledge encoded in the prior distribution of those latent variables. Fourth, convolutions in CNNs result from reversing the local rendering operator using template ( ) in NRM. Instead of rendering as in NRM, convolutions in CNNs perform template matching. Particularly, the convolution weights W ( ) in CNNs are proportional to the transposes of the rendering templates ( ) in NRM. The proofs and derivations for leaky ReLU and batch normalization are in Appendix C.

5

Under review as a conference paper at ICLR 2019

2.3 LEARNING

Learning in NRM can be posed as likelihood estimation problems in which we find the optimal values for parameters in NRM to optimize the appropriate likelihood functions. The optimization can be done by gradient-based methods (Robbins & Monro, 1985). The following theorem derives the learning objectives for NRM, and more details and proofs is provided in Appendix B and C.

Theorem 2.3. For any n  1, let x1, . . . , xn be i.i.d. samples from the NRM. Assume that the final rendered template h(y, z; 0) is normalized such that its norm is constant. The following holds:

(a) Cross-entropy loss for training CNNs with labeled data

max
(zi )in=1 ,

1 n

n i=1

log p(yi|xi, zi; )



max


1 n

n i=1

log q(yi|xi)

=

- min
A

Hp,q (y|x)

(6)

where q(y|x) is the posterior estimated by CNN, and Hp,q(y|x) is the cross-entropy between q(y|x) and the true posterior p(y|x) given by the ground truth.

(b) Reconstruction loss with RPN for unsupervised training of CNNs with labeled and unlabeled data

min


1 n

n

E

[log

p(xi,

zi|yi)]

asymp

min


1 n

n

xi - h(yi, zi; 0) 2

2

+ RPN, when   0

i=1 i=1

(7)

where the latent variable zi is estimated by the CNN as described in Theorem 2.2, h(yi, zi; 0) is the

reconstructed image, and the RPN regularization is the negative log prior defined as follows:

RPN

=

-

1 n

n

log

p(zi|yi)

=

-

1 n

n

Softmax ((yi, zi)) .

(8)

i=1 i=1

Cross-Entropy Loss for Training CNNs with Labeled Data: Theorem 2.3a establishes the cross-

entropy loss in the context of CNNs as a lower bound for the NRM's conditional log-likelihood

Lsup

:=

1 n

n i=1

log

p(yi|xi,

zi;

).

In

constrast

to

other

derivations

of

cross-entropy

loss

via

logistic

regression, we derives the cross-entropy loss in conjunction with the architecture of CNNs since

the estimation of the optimal latent variables z is part of the optimization in equation equation 6.

In other word, Theorem 2.3(a) ties feature extraction and learning for classification in CNNs into

an end-to-end conditional likelihood estimation problem in NRM. This new interpretation of the

cross-entropy loss suggests an interesting direction in which better losses for training CNNs with

labeled data for supervised classification tasks can be derived from other lower bounds for Lsup. The

Max-Min cross-entropy in Section 3 is an example.

Reconstruction Loss with RPN Regularization: Theorem 2.3b suggests that NRM learns without

labels by maximizing its expected complete-data log-likelihood. One term in this objective function

is the reconstruction loss between Another term is the RPN from Eqn.

the input image xi 8. RPN encourages

and the

the (yi,

reconstructed zi) inferred in

template h(yi, zi; 0). the bottom-up E-step

to have higher prior among all possible values of (y, z) and, thanks to its structure, enforces the

dependencies among latent variables (s( ), t( )) at different layers in NRM.

Semi-Supervised Learning in NRM: Let x1, . . . , xn be i.i.d. samples from NRM and assume that the labels y1, . . . , yn1 are unknown for some 0  n1  n, NRM determines optimal parameters
employed for the semi-supervised classification task via the following model:

min


RC n n
i=1

xi - h(yi, zi; 0) 2

2

+ RPN

-

CE n - n1

n
log q(yi|xi)
i=n1 +1

,

(9)

where RC and CE are non-negative weights associated with the reconstruction loss with RPN regularization and the cross-entropy loss, respectively. Again, the optimal latent variables zi is estimated by CNN as in Theorem 2.2. For unlabeled data, yi is set to the label estimated by CNN.

2.4 GENERALIZATION BOUND FOR CLASSIFICATION

Our generalization bound for classification with NRM is proportional to the ratio of the number of
active rendering paths and the total number of rendering paths in the trained NRM. A rendering path
is a configuration of all latent variables in NRM, and active optimal rendering paths are those among optimal rendering paths (y, z) whose corresponding rendered image is sufficiently close to one of the data point from the input data distribution. Let LA and LD denote the population and empirical losses on the data population A and the training set D of NRM, respectively. Our key result is summarized
below. More details and proofs are deferred to Appendix B and C.

6

Under review as a conference paper at ICLR 2019

Table 2: Error rate percentage on CIFAR-10 over 3 runs.

1K labels 50K images

2K labels 50K images

4K labels 50K images

50K labels 50K images

Adversarial Learned Inference (Dumoulin et al., 2017) Improved GAN (Salimans et al., 2016) Ladder Network (Rasmus et al., 2015)  model (Laine & Aila, 2017) Temporal Ensembling (Laine & Aila, 2017) Mean Teacher (Tarvainen & Valpola, 2017) VAT+EntMin (Miyato et al., 2018) DRM (Patel et al., 2016; 2015)

19.98 ± 0.89 21.83 ± 2.01
27.36 ± 1.20
21.55 ± 1.48
27.67 ± 1.86

19.09 ± 0.44 19.61 ± 2.09
18.02 ± 0.60
15.73 ± 0.31
20.71 ± 0.30

17.99 ± 1.62 18.63 ± 2.32 20.40 ± 0.47 13.20 ± 0.27 12.16 ± 0.31 12.31 ± 0.28
10.55
15.36 ± 0.34

6.06 ± 0.11 5.60 ± 0.10 5.94 ± 0.15
5.75 ± 0.24

Supervised-only NRM without RPN NRM+RPN NRM+RPN+Max-Min NRM+RPN+Max-Min+Mean Teacher

46.43 ± 1.21 24.88 ± 0.76 24.48 ± 0.43 21.55 ± 0.46 19.79 ± 0.74

33.94 ± 0.73 18.97 ± 0.80 18.62 ± 0.70 16.24 ± 0.17 15.11 ± 0.51

20.66 ± 0.57 14.41 ± 0.19 14.18 ± 0.46 12.50 ± 0.35 11.81 ± 0.13

5.82 ± 0.15 5.57 ± 0.07 5.35 ± 0.08 4.85 ± 0.10 4.88 ± 0.09

Theorem 2.4. Under the margin-based loss, with high probability, the following result on the
generalization bound of the classification framework withoptimal solutions from Eqn. 9 holds: LD  LA +  n|L|/ n.
Here,  n  (0, 1) denotes the ratio of active optimal rendering paths among all the optimal rendering paths, |L| is the total number of rendering paths, and n is the number of training data samples. The dependence of generalization bound on the number of active rendering paths  n|L| helps to justify our modeling assumptions. In particular, NRM helps to reduce the number of active rendering paths
thanks to the dependencies among its latent variables, thereby tightening the generalization gap.
Nevertheless, there is a limitation regarding the current generalization bound. In particular, the bound involves the number of rendering paths |L|, which is usually large. This is mainly because our bound has not fully taken into account the structures of CNNs, which is the limitation shared among other
latest generalization bounds for CNN. It is interesting to explore if techniques in works by Bartlett et al. (2017) and Golowich et al. (2018) can be employed to improve the term |L| in our bound.

3 NEW MAX-MIN CROSS ENTROPY FROM THE NEURAL RENDERING MODEL

In this section, we explore a particular way to derive an alternative to cross-entropy inspired by
the results in Theorem 2.3a. In particular, denoting zmax arg maxz { h(y, z; 0), x + (y, z)} and zmin arg minz { h(y, z; 0), x + (y, z)}, the new cross-entropy HM&M , which is called the Max-Min cross-entropy, is the weighted average of the cross-entropy losses from zmax and zmin:

HM&M maxHp,q(y|x, zmax) + minHp,q(y|x, zmin) = maxHpm,aqx(y|x) + minHpm,iqn(y|x).

Here the Max cross-entropy Hpm,aqx and Min cross entropy Hpm,iqn maximizes the correct target posterior and incorrect target posterior, respectively.
Similar to the cross-entropy loss, the Max-Min
cross-entropy can also be shown to be the lower bound for the conditional log-likelihood Lsup of

InputImage

Max Xentropy
Max-Min Xentropy Min Xentropy

the NRM and has the same generalization bound derived in Section 2.4. The Max-Min networks in Figure 4 realize this new loss. These networks

Sharedweights
Figure 4: The Max-Min network

have two CNN-like branches that share weights. The max branch estimates zmax using ReLU and

Max-Pooling, and the min branch estimates zmin using the Negative ReLU, i.e., min(·, 0), and Min-

Pooling. The Max-Min networks can be interpreted as a form of knowledge distillation like the Born

Again networks (Furlanello et al., 2018) and the Mean Teacher networks. However, instead of a

student network learning from a teacher network, in Max-Min networks, two students networks, the

Max and the Min networks, cooperate and learn from each other during the training.

4 EXPERIMENTS

4.1 SEMI-SUPERVISED LEARNING
We show NRM armed Max-Min cross-entropy and Mean Teacher regularizer achieves SOTA on benchmark datasets. We discuss the experimental results for CIFAR10 and CIFAR100 here. The results for SVHN, as well as model losses, and training details, can be found in the Appendix A & D.
CIFAR-10: Table 2 shows comparable results of NRM to SOTA methods. NRM is also better than the best methods that do not use consistency regularization like GAN, Ladder network, and ALI

7

Under review as a conference paper at ICLR 2019

when using only Nl=2K and 4K labeled images. NRM outperform DRM in all settings. Also, among methods in our comparison, NRM achieves the best test accuracy when using all available labeled data (Nl=50K). We hypothesize that NRM has the advantage over consistency regularization methods like Temporal Ensembling and Mean Teacher when there are enough labeled data is because the consistency regularization in those methods tries to match the activations in the network, but does not take into account the available class labels. On the contrary, NRM employs the class labels, if they are available, in its reconstruction loss and RPN regularization as in Eqns. 7 and 8. In all settings, RPN regularizer improve NRM performance. Even though the improvement from RPN is small, it is consistent across the experiments. Furthermore, using Max-Min cross-entropy significantly reduces the test errors. When combining with Mean-Teacher, our Max-Min NRM improves upon Mean-Teacher and consistently achieves either SOTA results or second best results in all settings. This consistency in performance is only observed in our method and Mean-Teacher. Also, like with Mean-Teacher, NRM can potentially be combined with other consistency regularization methods, e.g., the Virtual Adversarial Training (VAT) (Miyato et al., 2018), to obtain better results.

CIFAR-100: Table 3 shows NRM's comparable results to the  model and the Temporal

Ensembling, as well as better results than DRM. Same as with CIFAR10, using the RPN

regularizer results in a slightly better test accuracy, and NRM achieves better results than

the  model and the Temporal Ensembling method when using all available labeled data.

Notice that combining with Mean-Teacher just slightly

Table 3: Error rate percentage on CIFAR-100 over 3 runs.

10K labels

50K labels

improves NRM's performance

50K images

50K images

when training with 10K labeled data. This is again because consistency regularization methods like Mean-Teacher do not add much advantage when there are enough labeled data. However, NRM+Max-Min still yields

 model (Laine & Aila, 2017) Temporal Ensembling (Laine & Aila, 2017) DRM (Patel et al., 2016; 2015)
Supervised-only NRM without RPN NRM+RPN NRM+RPN+Mean Teacher NRM+RPN+Max-Min

39.19 ± 0.36 38.65 ± 0.51 41.09 ± 0.31
44.56 ± 0.30 40.70 ± 1.13 39.85 ± 0.46 39.84 ± 0.32 37.75 ± 0.66

26.32 ± 0.04 26.30 ± 0.15 27.06 ± 0.19
26.42 ± 0.17 26.27 ± 0.09 25.84 ± 0.10 25.98 ± 0.35 24.38 ± 0.29

better test errors and achieves SOTA result in all settings.

4.2 SUPERVISED LEARNING WITH MAX-MIN CROSS-ENTROPY

The Max-Min cross-entropy can be applied not only to improve semi-supervised learning but also on deep models including CNNs to enhance their supervised learning performance. In our experiments, we indeed observe Max-Min cross-entropy reduces the test error for supervised object classification on CIFAR10. In particular, using the Max-Min cross-entropy loss on a 29-layer ResNet Xie et al. (2017) trained with the Shake-Shake regularization Gastaldi (2017) and Cutout data augmentation DeVries & Taylor (2017), we are able to achieve SOTA test error of 2.30% on CIFAR10, an improvement of 0.26% over the test error of the baseline architecture trained with the traditional cross-entropy loss. While 0.26% improvement seems small, it is a meaningful enhancement given that our baseline architecture (ResNeXt + Shake-Shake + Cutout) is the second best model for supervised learning on CIFAR10. Such small improvement over an already very accurate model is significant in applications in which high accuracy is demanded such as self-driving cars or medical diagnostics. Similarly, we observe Max-Min improves the top-5 test error of the Squeeze-and-Excitation ResNeXt-50 network Hu et al. (2018) on ImageNet by 0.17% compared to the baseline (7.04% vs. 7.21%). For a fair comparison, we re-train the baseline models and report the scores in the re-implementation.

5 CONCLUSIONS

We present the NRM, a general and an effective framework for semi-supervised learning that combines generation and prediction with an end-to-end optimization. Using NRM, we can explain operations used in CNNs and develop new features that help learning in CNNs. For example, we derive the new Max-Min cross-entropy loss for training CNNs, which outperforms the traditional cross-entropy. Despite promising results in this paper, NRM can still be improved. For instance, an adversarial loss like in GANs can be incorporated into the NRM so that the model can generate realistic images. Furthermore, more knowledge of image generation from graphics and physics can be integrated in NRM so that the model can employ more structure to help learning and generation.

8

Under review as a conference paper at ICLR 2019
REFERENCES
P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural networks. Advances in Neural Information Processing Systems (NIPS), 2017.
D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859­877, 2017.
T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.
L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Stochastic Modelling and Applied Probability, Springer, 1996.
J. Donahue, P. Kra¨henbu¨hl, and T. Darrell. Adversarial feature learning. In International Conference on Learning Representations, 2017.
R. Dudley. Central limit theorems for empirical measures. Annals of Probability, 6, 1978.
V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and A. Courville. Adversarially learned inference. In International Conference on Learning Representations, 2017.
K. Friston. Does predictive coding have a future? Nature Neuroscience, 21(8):1019­1021, 2018. doi: 10.1038/s41593-018-0200-7.
T. Furlanello, Z. C. Lipton, M. Tschannen, L. Itti, and A. Anandkumar. Born-again neural networks. Proceedings of the International Conference on Machine Learning (ICML), 2018.
X. Gastaldi. Shake-shake regularization of 3-branch residual networks. In International Conference on Learning Representations, 2017.
N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks. Proceedings of the Conference On Learning Theory (COLT), 2018.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pp. 2672­2680. 2014.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016.
J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
D. P. Kingma, S. Mohamed, D. Jimenez Rezende, and M. Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems 27, pp. 3581­3589. 2014.
V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics, 30, 2002.
A. Kumar, P. Sattigeri, and T. Fletcher. Semi-supervised learning with gans: Manifold invariance with improved inference. In Advances in Neural Information Processing Systems 30, pp. 5534­5544. 2017.
S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations, 2017.
9

Under review as a conference paper at ICLR 2019
I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2016.
T. Miyato, S.-i. Maeda, S. Ishii, and M. Koyama. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 2018.
B. Neyshabur, R. R. Salakhutdinov, and N. Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2422­2430, 2015.
T. Nguyen, W. Liu, E. Perez, R. G. Baraniuk, and A. B. Patel. Semi-supervised learning with the deep rendering mixture model. arXiv preprint arXiv:1612.01942, 2016.
V. Papyan, Y. Romano, J. Sulam, and M. Elad. Theoretical foundations of deep learning via sparse representations: A multilayer sparse model and its connection to convolutional neural networks. IEEE Signal Processing Magazine, 35(4):72­89, July 2018. ISSN 1053-5888. doi: 10.1109/MSP.2018.2820224.
A. B. Patel, T. Nguyen, and R. G. Baraniuk. A probabilistic theory of deep learning. arXiv preprint arXiv:1504.00641, 2015.
A. B. Patel, M. T. Nguyen, and R. Baraniuk. A probabilistic framework for deep learning. In Advances in Neural Information Processing Systems 29, pp. 2558­2566. 2016.
R. P. N. Rao and D. H. Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2:79 EP ­, 01 1999.
A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko. Semi-supervised learning with ladder networks. In Advances in Neural Information Processing Systems 28, pp. 3546­3554. 2015.
H. Robbins and S. Monro. A stochastic approximation method. In Herbert Robbins Selected Papers, pp. 102­109. Springer, 1985.
T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen, and X. Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems 29, pp. 2234­2242. 2016.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1­9, 2015.
A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems 30, pp. 1195­1204. 2017.
S. van de Geer. Empirical Processes in M-estimation. Cambridge University Press, 2000. R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv:1011.3027v7,
2011. S. Xie, R. Girshick, P. Dolla´r, Z. Tu, and K. He. Aggregated residual transformations for deep neural
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5987­5995. IEEE, 2017.
10

Under review as a conference paper at ICLR 2019

Supplementary Material
Appendix A
This appendix contains several key tables with simulation studies and notations as well as figures for the connection between NRM and CNN in the main text. Semi-Supervised Learning Results on SVHN
Table 4: Error rate percentage on SVHN in comparison with other state-of-the-art methods. All results are averaged over 2 runs (except for NRM+RPN when using all labels, 1 run)

ALI Dumoulin et al. (2017) Improved GAN (Salimans et al., 2016)
+ Jacob.-reg + Tangents (Kumar et al., 2017)  model (Laine & Aila, 2017) Temporal Ensembling (Laine & Aila, 2017) Mean Teacher (Tarvainen & Valpola, 2017) VAT+EntMin (Miyato et al., 2018) DRM (Nguyen et al., 2016)
Supervised-only NRM without RPN NRM+RPN NRM+RPN+Max-Min+MeanTeacher

250 labels 73257 images
9.69 ± 0.92 4.35 ± 0.50
27.77 ± 3.18 9.78 ± 0.24 9.28 ± 0.01 3.97 ± 0.21

500 labels 73257 images
18.44 ± 4.8 4.87 ± 1.60 6.83 ± 0.66 5.12 ± 0.13 4.18 ± 0.27
9.85
16.88 ± 1.30 7.42 ± 0.61 6.56 ± 0.88 3.84 ± 0.34

1000 labels 73257 images
7.42 ± 0.65 8.11 ± 1.3 4.39 ± 1.20 4.95 ± 0.26 4.42 ± 0.16 3.95 ± 0.19 3.86 6.78
12.32 ± 0.95 5.64 ± 0.13 5.47 ± 0.14 3.70 ± 0.04

73257 labels 73257 images
2.50 ± 0.07 2.74 ± 0.06 2.50 ± 0.05
2.75 ± 0.10 3.46 ± 0.04 3.57 2.87 ± 0.05

Rendering Process in NRM

Algorithm 1 Rendering Process in NRM
Input: Object category y. Output: Rendered image x given the object category y.
Parameters:  = {µ(y)}yK=1 , {( )}L=1 , {b( )}L=1 where µ(y) is the class template, ( ) is
the rendering template at layer , and b( ) is the parameters of the conjugate prior p(z|y, x) at layer , which turn out to be the bias terms in the ReLU after convolutions at each layer in CNNs.

1. Use Markov chain Monte Carlo method to sample the latent variables z( ) in NRM, =

1, 2, . . . , L from z|y = Softmax

1 2



(y,

z)

, where (y, z)

L =1

b(t;

), s(

)

h( ) .

2. Render h( ),

= 0, 1, . . . , L - 1 using the recursion h( - 1) =

pP( ) s( , p)T (t; , p)B( , p)( , p)h( , p) in Eqn. 3, in which h(L) = µ(y) and T (t; , p)

and B( , p) are the local translation matrix and the zero-padding matrix at pixel location p in layer

as described above.

3. Add Gaussian pixel noise N (0, 21D(0)) into h(0) to achieve the final rendered image x, where D(0) is the dimension, i.e. the number of pixels, of h(0), as well as of x.

11

Under review as a conference paper at ICLR 2019

Table 5: Table of notations for NRM

x y z( ) = {s( ), t( )} s( , p) t( , p) h(y, z; ) = h( ) h(y, z; 0) = h(0) ( )
µ(y) = h(y; L) = h(L)
( ) ( )
W ( ) =  ( , p) B( ) T( )
b(t; ) = b( )
y 2
(y, z) Softmax ()
RPN (y,z(L), . . . , z(1))

Variables

input image of size D(0) object category all latent variables of size D( ) in level switching latent variable at pixel location p in level local translation latent variable at pixel location p in level intermediate rendered image of size D( ) in level rendered image of size D(0) from NRM before adding noise corresponding feature maps in layer in CNNs.

Parameters

the template of class y, as well as the coarsest image of size D(L) determined by the category y at the top of NRM before adding any fine detail. µ(y) is learned from the data.
rendering matrix of size D( - 1) × D( ) at layer .
dictionary of D( ) rendering templates ( , p) of size F ( ) × 1 at layer . ( ) is learned from the data.
corresponding weights at the layer in CNNs set of zero-padding matrices B( , p)  RD( -1)×F ( )) at layer set of local translation matrices T ( , p)  RD( -1)×D( -1)) at layer . T ( , p) is chosen according to value of t( , p)
parameters of the conjugate prior p(z|x, y) at layer . This term is of size D( ) and becomes the bias term after convolutions in CNNs. It can be made independent of t, which is equivalent to using the same bias in each feature map in CNNs. Here, b(t; ) is learned from data.
probability of object category y.
pixel noise variance

Other Notations

L1 =1 2

b(t;

), s(

)

exp() .

exp( )

h( ) .



nn

-

1 n

log

p(zi|yi)

=

-

1 n

Softmax ((yi, zi)).

i=1 i=1

rendering configuration.

12

Under review as a conference paper at ICLR 2019

Appendix B

In this appendix, we give proofs for Theorem 2.3. We also give further connection of NRM to cross entropy as well as additional derivation of NRM to various models under both unsupervised and (semi)supervised setting of data being mentioned in the main text. We also formally present our results on consistency and generalization bounds for NRM in supervised and semi-supervised learning settings. In addition, we explain how to extend NRM to derive ResNet and DenseNet. For the simplicity of
the presentation, we denote  = {µ(y)}Ky=1 , {( )}L=1 , {y}Ky=1 , {b( )}L=1 to represent all the
parameters that we would like to estimate from NRM where L is the set of all possible values of latent (nuisance) variables z = (t( ), s( ))L=1. Additionally, for each (y, z)  J := {1, . . . , K} × L, we
denote y,z = µ(y), {( )}L=1 , {b( )} , i.e., the subset of parameters corresponding to specific
label y and latent variable z. Furthermore, to stress the dependence of (y, z) on , we define the following function

LL
 (y,z) := (y, z) = b ( ) (s( ) z( )) = b ( )M (s; )z( )
=1 =1
for each (y, z)  J where M (s; ) = diag(s( )) is a masking matrix associated with s( ). Throughout this supplement, we will use  (y,z) and (y, z) interchangeably as long as the context is clear. Furthermore, we assume that ( )   , which is a subset of RF ( )×D( ) for any 1   L, µ(y)  , which is a subset of RD( ), for 1  y  K, and b(t; )  ( ), which is a subset of RD( ) for all choices of t( ) and 1   L. Last but not least, we say that  satisfies the non-negativity assumption if the intermediate rendered images z( ) satisfy z( )  0 for all 1   L.

Notation: In this appendix, we use A to denote transpose of the matrix A.

.1 CONNECTION BETWEEN NRM AND CROSS ENTROPY

As being established in part (a) of Theorem 2.3, the cross entropy is the lower bound of maximizing the conditional log likelihood. In the following full theorem, we will show both the upper bound and the lower bound of maximizing the conditional log likelihood in terms of the cross entropy.

Theorem .1. Given any  > 0, we denote A = { : h(y, z; 0) = }. For any n  1 and  > 0, let x1, . . . , xn be i.i.d. samples from the NRM. Then, the following holds
(a) (Lower bound)

max
(zi )in=1 ,A

1 n

n i=1

log p(yi|xi, zi; )



max 1 n log Softmax A n i=1

max
zi

h (yi, zi; 0)xi + (yi, zi) 2

=

max
A

1 n

n i=1

log q(yi|xi)

=

- min
A

Hp,q (y|x)

+ byi

where by = log y for all 1  y  K, q(y|x) = Softmax

max
z

h (y, z; 0)x + (y, z)

/2 + by

for all (x, y), and Hp,q(y|x) is the cross-entropy between the estimated posterior q(y|x) and the true posterior given by the ground-true labels p(y|x).

(b) (Upper bound)

max
(zi )ni=1 ,A

1 n

n i=1

log p(yi|xi, zi; )



max
A

1 n

n i=1

log

q(yi

|xi

)

+

max
y

max
zi

h (y, zi; 0)xi + (y, zi) 2

-h

(y, zi; 0)xi + (y, zi) 2

+ log K

13

Under review as a conference paper at ICLR 2019

where zi = arg max p(yi|xi, zi; ) for 1  i  n.
zi
Remark .2. As being demonstrated in Theorem 2.2, max h (y, z; 0)x + (y, z) /2, approxi-
z
mately, has the form of a CNN. If we further have the non-negativity assumption with , then this is exact. Therefore, the cross entropy Hp,q obtained in Theorem .1 has a strong connection with CNN.
Remark .3. The gap between the upper bound and the lower bound of maximizing the conditional
log likelihood in terms of cross entropy function suggests how good the estimation in Theorem .1 is.
In particular, this gap is given by:

1n n

max
y

max
zi

h

(y, zi; 0)xi + (y, zi) 2

-h

(y, zi; 0)xi + (y, zi) 2

i=1

+ log K,

trade-off loss
where zi = arg max p(yi|xi, zi; ) for 1  i  n. As long as the number of labels is not too large
zi
and the trade-off loss is sufficiently small, the gap between the upper bound and the lower bound in
Theorem .1 is small.

.2 LEARNING IN THE NRM WITHOUT NOISE FOR UNSUPERVISED SETTING UNDER NON-NEGATIVITY ASSUMPTION

To ease the presentation of the inference with NRM without noise, we first assume that rendered images h(y, z; 0) satisfy the non-negativity assumption (Later in Section .3, we will discuss the relaxation of this assumption for the inference of the NRM). With this assumption, as being demonstrated

in Theorem 2.2, we have:

max h (y, z; 0)x + (y, z) = max h (y)(L)
y,z y

(10)

where we define

(L) = max  (z(L)) max  (z; L - 1) · · · max  (z; 1)x + b(1) . . . + b(L - 1)

z(L)

z(L-1)

z(1)

+b( ).

Now, we will provide careful derivation of part (b) of Theorem 2.3 in the main text. Remind that, for

the unsupervised setting, we have data x1, . . . , xn are i.i.d. samples from NRM. The complete-data log-likelihood of the NRM is given as follows:

n

Ey,z [log p(x, (y, z))] =

P (y, z|xi){log y,z + log N (xi|h(y, z; 0))}

i=1 (y,z)J

where we have

P (y, z|xi) =

y,zN (xi|h(y, z; 0)) y ,z N (xi|h(y , z ; 0))

(y ,z )J

=

y exp

-

xi - h(y, z; 0) 2 - 2(y, z) 22

y exp
(y ,z )J

-

xi - h(y , z ; 0) 2 - 2(y , z ) 22

.

At the zero-noise limit, i.e.,   0, it is clear that P (y, z|xi) = 1 as (y, z) = arg min xi -
(y ,z )J

h(y , z ; 0) 2 - 2(y , z ) and P (y, z|xi) = 0 otherwise. Therefore, we can asymptotically view

the complete log-likelihood of the NRM under the zero-noise limit as

n

ry,z

log

y,z

-

1 2

xi - h(y, z; 0)

2

i=1 (y,z)J

n
=

-

1 2

ry,z

xn - h(y, z; 0)

2+

n

ry,z log y,z

i=1 (y,z)J

i=1 (y,z)J

Reconstruction Loss

Path Normalization Regularizer

14

Under review as a conference paper at ICLR 2019

where



ry,z

 

1,

 0,

if (y, z) = arg min
(y ,z )J
otherwise

xi - h(y , z ; 0) 2 - 2(y , z )

With the above formulation, we have the following objective function

Un

=

min


1 n

n

ry,z

1 2

xi - h(y, z; 0)

2 - log y,z

i=1 (y,z)J

(11)

where  = {µ(y)}Ky=1 , {( )}L=1 , {y}yK=1 , {b( )}L=1 . We call the above objective function to be unsupervised NRM without noise.

Relaxation of unsupervised NRM without noise Unfortunately, the inference with unsupervised NRM without noise is intractable in practice due to two elements: the involvement of h(y , z ; 0) 2

to determine the value of ry,z and the summation

exp((y , z )+log y ) in the denominator

(y ,z )J

of y,z for all (y, z)  J . Therefore, we need to develop a tractable version of this objective function.

Theorem .4. (Relaxation of unsupervised NRM without noise) Assume that y   for all 1  y  K for some given   (0, 1/2). Denote

Vn

:=

min


1 n

n i=1

(y

,z

)J

1

(y

,z

)=arg max

h

(y ,z ;0)xi + (y ,z )

 

 (y,z)J



xi - h(y , z 2

; 0)

2

- log(y ,z

)

K

where py ,z = exp (y , z ) + log y

/

exp
y=1

max
zL

(y,

z

)

+

log

y

For any , we define

for all (y , z )  J .

(yi, zi) = arg min xi - h(y, z; 0) 2 - 2(y, z)
(y,z)J
and

(yi, zi) = arg max h (y, z; 0)xi + (y, z)
(y,z)J

as 1  i  n. Then, the following holds (a) Upper bound:

Un



min


1 n

n

i=1

xi - h(yi, zi; 0) 2

2

- log(yi,zi )

+

log yi - log yi

prior loss



Vn + log

1 

-

1

+ log |L|

+ log |L|

(b) Lower bound:

Un



min


1 n

n

i=1

xi - h(yi, zi; 0) 2

2

- log(yi,zi )

+

log yi - log yi

prior loss

+

1 2

h(yi, zi; 0) 2 - h(yi, zi; 0) 2

 Vn + log

 1-

+

min


1 n

n

1 2

i=1

norm loss
h(yi, zi; 0) 2 - h(yi, zi; 0) 2

15

Under review as a conference paper at ICLR 2019

Unlike Un, the inference with objective function of Vn is tractable. According to the upper bound and lower bound of Un in terms of Vn, we can use Vn as a tractable approximation of Un for the inference purpose with unsupervised setting of data when the noise is treated to be 0. Therefore, we
achieve the conclusion of part (b) of Theorem 2.3 in the main text. The algorithm for determined (local) minima of Vn is summarized in Algorithm 2.

.3 RELAXATION OF NON-NEGATIVITY ASSUMPTION WITH RENDERED IMAGES

It is clear that the inference with Vn relies on the non-negativity assumption such that equation equation 10 holds. Now, we will argue that when the non-negativity assumption with rendered images h(y, z; 0) does not hold, we can relax Vn to a more tractable version under that setting.
Theorem .5. (Relaxation of objective function Vn when non-negativity assumption does not hold) Assume that y   for all 1  y  K for some given   (0, 1/2). Denote

Wn

:=

min


1 n

n

K
1

i=1 y =1

y =arg max g(y,xi)
yJ

xi - g(y 2

, zi)

2

- log(y ,zi )

where

g(y, x) = h (y) MaxPool ReLu Conv ( ), · · · MaxPool ReLu Conv (1), I + b(1)

· · · + b( )

for all (x, y). Additionally, zi is the maximal value of z in the CNN structure of g(y, xi) for 1  i  n. For any , we define

(yi, zi) = arg max h (y, z; 0)xi + (y, z)
(y,z)J

and yi = arg max g(y, xi) as 1  i  n. Then, the following holds
y
(a) Upper bound:

Vn



min


1 n

n

i=1

xi - h(yi, zi; 0) 2

2

- log(yi,zi )

+

log yi - log yi

prior loss

+

1 2

h(yi, zi; 0) 2 - h(yi, zi; 0) 2



Wn + log

1 

-1

+

min


1 n

n

1 2

i=1

norm loss
h(yi, zi; 0) 2 - h(yi, zi; 0) 2

(b) Lower bound:

Vn



min


1 n

n

i=1

xi - h(yi, zi; 0) 2

2

- log(yi,zi )

+

log yi - log yi

prior loss

+

1 2

h(yi, zi; 0) 2 - h(yi, zi; 0) 2 + g(yi, xi) - h(yi, zi; 0) xi + (yi, zi)

 Wn + log

+

min


1 n

n

i=1

norm loss

 1-

+

min


1 n

n

1 2

i=1

g(yi, xi) - h(yi, zi; 0)

h(yi, zi; 0) 2 - xi + (yi, zi)

CNN loss
h(yi, zi; 0) 2

16

Under review as a conference paper at ICLR 2019

Algorithm 2 Relaxation of unsupervised NRM without noise

Input: Data xi, translation matrices T (t; ), zero padding matrices B( ), number of labels K, number of layers L.
Output: Parameters .

Initialize  = {µ(y)}yK=1 , {( )}L=1 , {y}yK=1 , {b( )}L=1 .

while  has not converged do

1. E-Step: Update labels (y, z) of each data

for i = 1 to n do

(yi, zi) = arg max h (y, z)xi + log(y,z) .
y,z

end for

2. M-Step: By using Stochastic Gradient Descent (SGD), update  that minimizes

n i=1

xi - h(yi, zi) 2

2

- log(yi,zi )

.

end while

The proof argument of the above theorem is similar to that of Theorem .4; therefore, it is omitted.
The upper bound and lower bound of Vn in terms of Wn in Theorem .5 implies that we can use Wn as a relaxation of Vn when the non-negativity assumption with rendered images h(y, z; 0) does not hold. The algorithm for achieving the (local) minima of Wn is similar to Algorithm 2.

.4 NRM WITH (SEMI)-SUPERVISED SETTING

In this section, we consider the application of NRM to the (semi)-supervised setting of the data. Under that setting, only a (full) portion of labels of data x1, . . . , xn is available. Without loss of generality, we also assume that the rendering path h(y, z; 0) satisfies the non-negativity assumption. For the case that h(y, z; 0) does not satisfy this assumption, we can argue in the same fashion as that of Theorem .5. Now, we assume that only the labels (yn1+1, . . . , yn) are unknown for some n1  0. When n1 = 0, we have the supervised setting of data while we have the semi-supervised setting of data when n - n1 is small. Our goal is to build a semi-supervised model based on NRM such that the clustering information from data x1, . . . , xn1 can be used efficiently to increase the accuracy of classifying the labels of data xn1+1, . . . , xn. For the sake of simple inference with that purpose, we only consider the setting of NRM when the noise goes to 0. Our idea of constructing
the semi-supervised model based on NRM is insprired by an approximation of the upper bound of
maximizing the conditional log likelihood of NRM in terms of the cross entropy and reconstruction
loss in part (b) of Theorem 2.3. In particular, we combine the tractable version of reconstruction loss
from the unsupervised setting in Theorem 2.3b and the cross entropy of approximate posterior in
Theorem 2.3a, which can be formulated as follows

min


RC n

n1
1
 i=1 (y ,z )J (y ,z )=arg max
 (y,z)J

h (y,z;0)xi+ (y,z )

 


xi - h(y , z 2

; 0)

2

- log(y

,z

)

n

+ 1





i=n1+1 z L

z =arg max  zL

h (yi,z;0)xi+ (yi,z )



xi - h(yi, z 2

; 0)

2

- log(yi,z

)

-

CE n - n1

n i=n1 +1

log

q (yi |xi )

where RC and CE are non-negative weights associated with reconstruction loss and cross entropy respectively. Additionally, the approximate posterior q(y|xi) is chosen as

q(y|xi) =

exp
K

max
zL

h

(y, z; 0)xi +  (y,z)

+ log y

exp
y =1

max
zL

h

(y , z; 0)xi +  (y ,z)

+ log y

.

17

Under review as a conference paper at ICLR 2019

Note that, since the labels (yn1+1, . . . , yn) are known, the reconstruction loss for clustering data xn1+1, . . . , xn in the above objective function indeeds incorporate these information to improve the accuracy of estimating the parameters. We call the above objective function to be (semi)-supervised
NRM without noise.

Boosting the accuracy of (semi)-supervised NRM without noise In practice, it may happen that the accuracy of classifying data by using the parameters from (semi)-supervised NRM without noise is not very high. To account for that problem, we consider the following general version of (semi)-supervised NRM without noise that includes the variational inference term and the moment matching term

min


RC n

n1
1
 i=1 (y ,z )J (y ,z )=arg max
 (y,z)J

h (y,z;0)xi+ (y,z )

 


xi - h(y , z 2

; 0)

2

- log(y

,z

)

n

+ 1





i=n1+1 z L

z =arg max  zL

h (yi,z;0)xi+ (yi,z )



xi - h(yi, z 2

; 0)

2

- log(yi,z

)

-

CE n - n1

n
log q(yi|xi) +
i=n1 +1

KL n

n i=1

K
q(y|xi) log
y=1

q (y |xi ) y

L
+MM DKL N (µh( ), h2( ))||N (µ( ), 2 ( )) .

(12)

=1

Here, KL and MM are non-negative weights associated with the variational inference loss and moment matching loss respectively. Additionally, µh( ), h2( ), µ( ), 2 ( ) in the moment matching loss are defined as follows:

µh(

)

=

1 n

n

h^( )i,

i=1

h2(

)

=

1 n

n
(h^( )i - µh( ))2

i=1

µ(

)

=

1 n

n

( )i,

i=1

2 (

)

=

1 n

n
(( )i - µ( ))2

i=1

(13)

where h^( )i is the estimated value of h( ) given the optimal latent variables s^( ) and t^( ) inferred from the image xi for 1  i  n. It is clear that when KL = MM = 0, we return to (semi)-supervised
NRM without noise. In Appendix C, we provide careful theoretical analyses regarding statistical

guarantees of model equation 12.

Now, we will provide heuristic explanations about the improvement in terms of performance of model equation 12 based on the variational inference term and the moment matching term.

Regarding the variational term: The DRMM inference algorithm developed thus far ignores uncertainty in the latent nuisance posterior p(y, z|x) due to the max-marginalization over (y, z) in the E-step bottom-up inference. We would like to properly account for this uncertainty for two main reasons: (i) our fundamental hypothesis is that the brain performs probabilistic inference and (ii) uncertainty accounting is very important for good generalization in the semi-supervised setting since we have very little labeled data.

One approach attempts to approximate the true class posterior p(y|x) for the DRMM. We employ
variational inference, a technique that enables the approximate inference of the latent posterior.
Mathematically, for the DRMM this means we would like to approximate the true class posterior p(y|x)  q(y|x), where the approximate posterior q is restricted to some tractable family of
distributions (e.g. Gaussian or Categorical). We strategically choose the tractable family to be q(y|x)  p(y|z^, x), where z^  argmax p(y, z|x). In other words, we choose q to be restricted
z
to the DRMM family of nuisance max-marginalized class posteriors. Note that this is indeed an
approximation, since the true DRMM class posterior has nuisances that are sum-marginalized out p(y|x) = p(y, z|x), whereas the approximating variational family has nuisances that are max-
z
marginalized out.

18

Under review as a conference paper at ICLR 2019

Given our choice of variational family q, we derive the variational term for the loss function, starting from the principled goal of minimizing the KL-distance DKL[q(y|x)||p(y|x)] between the true and approximate posteriors with respect to the parameters of q. As a result, such an optimized q will tilt towards better approximating p(y|x), which in turn means that it will account for some of the uncertainty in p(z|x). The variational terms in the loss are defined as Blei et al. (2017):

LV I  LRC + KLLKL  -Eq [ln p(x|y)] + KLDKL[q(y|x)||p(y)].

(14)

This term is quite similar to that used in variational autoencoders (VAE) Kingma & Welling (2013), except for two key differences: (i) here the latent variable y is discrete categorical rather than continuous Gaussian and (ii) we have employed a slight relaxation of the VAE by allowing for a penalty parameter KL = 1. The latter is motivated by recent experimental results showing that such freedom enables optimal disentangling of the true intrinsic latent variables from the data.

Regarding the moment matching term: Batch Normalization can potentially be derived by normalizing the intermediate rendered images h( ), = 1, 2, . . . , L in the NRM by subtracting their means and dividing by their standard deviations under the assumption that the means and standard derivations of h( ) are close to those of the activation ( ) in the CNNs. From this intuition, in Section .4 of Appendix A, we introduce the moment-matching loss to improve the performance of
the NRM/CNNs trained for semi-supervised learning tasks.

.5 STATISTICAL GUARANTEES FOR (SEMI)-SUPERVISED SETTING

For the sake of simplicity with proof argument, we only provide detail theoretical analysis for statistical guarantee with the setting of equation 12 when the moment matching term is skipped. In particular, we are interested in the following (semi)-supervised model

Yn

:=

min


RC n

n1
1
 i=1 (y ,z )J (y ,z )=arg max
 (y,z)J

h (y,z;0)xi+ (y,z )

 


×

xi - h(y , z ; 0) 2

2

- log(y ,z

)

n

+ 1





i=n1+1 z L

z =arg max  zL

h (yi,z;0)xi+ (yi,z )



xi - h(yi, z 2

; 0)

2

- log(yi,z

)

-

CE n - n1

n
log q(yi|xi) +
i=n1 +1

KL n

n i=1

K
q(y|xi) log
y=1

q (y |xi ) y

(15)

where the approximate posterior q(y|xi) is chosen as

q(y|xi) :=

exp
K

max
zL

h

(y, z; 0)xi +  (y,z)

+ log y

exp
y =1

max
zL

h

(y , z; 0)xi +  (y ,z)

+ log y

.

Here, RC, CE, and KL are non-negative weights associated with reconstruction loss, cross entropy, and variational inference respectively. As being indicated in the formulation of objection function Yn, the only difference between Yn and equation 12 is the weight MM regarding moment matching loss in equation 12 is set to be 0. To ease the presentation with theoretical analyses later, we call the objective function with Yn to be partially labeled latent dependence regularized cross entropy (partially labeled LDCE).

Consistency of partially labeled LDCE Firstly, we demonstrate that the objective function of partially labeled LDCE enjoys the consistency guarantee.
Theorem .6. (Consistency of objective function of partially labeled LDCE) Assume that n1 is a function of n such that n1/n   as n  . Furthermore, P( x  R) = 1 as x  P for some

19

Under review as a conference paper at ICLR 2019

given R > 0. We denote the population version of partially labeled LDCE as follows

Y := min RC 


1 



(y ,z )J (y ,z )=arg max h (y,z;0)x+ (y,z )

 (y,z)J



x - h(y , z ; 0) 2 2

- log(y ,z ) dP (x) + (1 - )

1 



z L z =arg max h (y,z;0)x+ (y,z )

 zL



x - h(y, z ; 0) 2 2

- log(y,z ) dQ(x, c)

- CE

log q(y|x)dQ(x, c) + KL

K
q(y|x) log
y=1

q (y|x) y

dP (x).

Then, we obtain that Yn  Y almost surely as n  .

The detail proof of Theorem .6 is deferred to Appendix C. Now, we denote  :=

{µ(y)}Ky=1 ,

( )

L =1

,

{y }Ky=1

,

L
b( )
=1

the optimal solutions of objective function equa-

tion 15. Note that, the existence of these optimal solutions is guaranteed due to the compactness

assumption of the parameter spaces  , , and l for 1   L. The optimal solutions {µ(y)}Ky=1

L

and ( ) lead to corresponding set of optimal rendered images Sn. Similar to the case of SPLD

=1

regularized K-means, our goal is to guarantee the consistency of Sn as well as {y}yK=1 ,

b( )

L
.
=1

In particular, we denote F0 the set of all optimal solutions 0 of population partially labeled LDCE

where 0 :=

µ0(y)

K y=1

,

0( )

L
,
=1

y0

K y=1

,

b0( )

L =1

. For each 0  F0, we define

S0 the set of optimal rendered images associated with 0. We denote G(F0) the corresponding set of

all optimal rendered images S0, optimal prior probabilities

y0

K , and optimal biases
y=1

L
b0( ) .
=1

Theorem .7. (Consistency of optimal rendering paths and optimal solutions of partially labeled

LDCE) Assume that P( x  R) = 1 as x  P for some given R > 0. Then, we obtain that

KL

inf (S0,{y0},{b0( )})G(F0)

H(Sn, S0) + |y - y0| +
y=1

=1

b( ) - b0( )

0

almost surely as n  .

The detail proof of Theorem .7 is postponed to Appendix C.

.6 GENERALIZATION BOUND FOR CLASSIFICATION FRAMEWORK WITH (SEMI)-SUPERVISED
SETTING

In this section, we provide a simple generalization bound for certain classification function with

the optimal solutions  =

{µ(y)}Ky=1 ,

( )

L =1

,

{y

}Ky=1

,

L
b( )
=1

of equation 12. In

particular, we denote the following function f : RD(0) × {1, . . . , K}  R as

f (x, y) = max
zL

h

(y, z; 0)x +  (y,z)

+ log y

for all (x, y)  RD(0) × {1, . . . , K} where

h(y, z; 0) := (z; 1) . . . (z; L)µ(y),

(z; ) :=

s( , p)T (t; , p)B( , p)( , p)

pP( )

for all (y, z) and 1   L. To achieve the generalization bound regarding that classification function, we rely on the study of generalization bound with margin loss. For the simplicity of

20

Under review as a conference paper at ICLR 2019

argument, we assume that the true labels of x1, . . . , xn are y1, . . . , yn while y1, . . . , yn1 are not available to train. The margin of a labeled example (x, y) based on f can be defined as

(f, x, y) = f (x, y) - max f (x, l).
l=y

Therefore, the classification function f misspecifies the labeled example (x, y) as long as (f, x, y)  0. The empirical margin error of f at margin coefficient   0 is

Rn, (f )

=

1 n

n

1{(f ,xi ,yi ) } .

i=1

It is clear that Rn,0(f ) is the empirical risk of 0-1 loss, i.e., we have

Rn,0(f )

=

1 n

n

1

i=1

arg max f (xi,y)=yi
1yK

.

Similar to the argument in the case of SPLD regularized K-means, the optimal solutions  of partially
labeled LDCE lead to a set of rendered images h(y, z; 0) for all (y, z)  J . However, only a small fraction of rendering paths are indeed active in the following sense. There exists a subset Ln of L such that |Ln|   n|L| where  n  (0, 1], which is independent of data (x1, y1), . . . , (xn, yn), and the following holds

max
zL

h

(y, z; 0)x +  (y,z)

+

log

y

=

max
zLn

h (y, z; 0)x +  (y,z)

+ log y

for all 1  y  K. The above equation implies that

Rn,(f ) = Rn, (fn )

for

all





0

and

n



1

where

fn (x,

y)

=

max
zLn

h (y, z; 0)x +  (y,z)

+ log y for all (x, y).

With that connection, we denote the expected margin error of classification function fn at margin

coefficient   0 is

R (f n ) = E1{(fn ,x,y)}.
The generalization bound that we establish in this section will base on the gap between the expected margin error R0(fn ) and its corresponding empirical version Rn,(fn ), which is also Rn,(f ).
Theorem .8. (Generalization bound for margin-based classification) Assume that P ( x  R) = 1 for some given R > 0 and x  P . Additionally, the parameter spaces  and  are chosen such that h(y, z; 0)  R for all (y, z)  J . For any  > 0, with probability at least 1 - , we have

R0(fn )



inf
(0,1]

Rn, (fn )

+

8K(2K - n

1)

2 n|L|(R2 + 1) + | log |

+

log log2(2-1) n

1/2
+

log(2-1) 2n

where  is the lower bound of prior probability y for all y.
Remark .9. The result of Theorem .8 gives a simple characterization for the generalization bound of classification setup from optimal solutions of partially labeled LDCE based on the number of active rendering paths, which is inherent to the structure of NRM. Such dependence of generalization bound on the number of active rendering configurations  n|L| is rather interesting and may provide a new perspective on understanding the generalization bound. Nevertheless, there are certain limitations regarding the current generalization gap: (1) the active ratio  n may change with the sample size unless we put certain constraints on the sparsity of switching variables a to reduce the number of active optimal rendering configurations; (2) the generalization bound is depth- dependent due to the involvement of the number of rendering configurations |L|. This is mainly because we have not fully taken into account all the structures of CNNs for the studying of generalization bound. Given some current progress on depth-independent generalization bound (Bartlett et al., 2017; Golowich et al., 2018), it is an interesting direction to explore whether the techniques in these work can be employed to improve |L| in the generalization bound in Theorem .8.

21

Under review as a conference paper at ICLR 2019

.7 NEURAL RENDERING MODEL IS THE UNIFYING FRAMEWORK FOR NETWORKS IN THE CONVNET FAMILY

The structure of the rendering matrices ( ) gives rise to MaxPooling, ReLU, and convolution operators in the CNNs. By modifying the structure of ( ), we can derive different types of networks in the convolutional neural network family. In this section, we define and explore several other interesting variants of NRM: the Residual NRM (ResNRM) and the Dense NRM (DenseNRM). Inference algorithms in these NRMs yield ResNet He et al. (2016) and DenseNet Huang et al. (2017), respectively. Proofs for these correspondence are given in Appendix C. Both ResNet and DenseNet are among state-of-the-art neural networks for object recognition and popularly used for other visual perceptual inference tasks. These two architectures employ skip connections (a.k.a., shortcuts) to create short paths from early layers to later layers. During training, the short paths help avoid the vanishing-gradient problem and allow the network to propagate and reuse features.

.7.1 RESIDUAL NEURAL RENDERING MODEL YIELDS RESNET

In a ResNet, layers learn residual functions with reference to the layer inputs. In particular, as illustrated in Fig. 5, layers in a ResNet are reformulated to represent the mapping F () + Wskip and the layers try to fit the residual mapping F () where  is the input feature. The term Wskip accounts for the skip connections/shortcuts He et al. (2016). In order to derive the ResNet, we rewrite the rendering matrix ( ) as the sum of a shortcut matrix skip( ) and a rendering matrix, both of which can be updated during the training. The shortcut matrices yields skip connections in He et al. (2016). Note that skip( ) depends on the template selecting latent variables s( ). In the rest of this section, for clarity, we will refer to ( ) and skip( ) as (t, s; ) and skip(s; ), respectively, to show their dependency on latent variables in NRM. We define the Residual Neural Rendering Model
as follows:
Definition .10. The Residual Neural Rendering Model (ResNRM) is the Neural Rendering Model whose rendering process from layer to layer - 1, for some  {1, 2, · · · , L}, has the residual form as follows:

h( - 1) := ((t, s; ) + skip(s; )) h( ),

(16)

where skip(t, s; ) is the shorcut matrices that results in skip connections in the corresponding ResNet. In particular, skip(t, s; ) has the following form:

skip(s; ) = ~ skip( )M (s; ),

(17)

where M (s; )  diag (s( ))  RD( )×D( ) is a diagonal matrix whose diagonal is the vector s( ). This matrix selects the templates for rendering. Furthermore, ~skip( ) is a rendering matrix that is
independent of latent variables t and s.

The following theorem show that similar to how CNNs can be derived from NRM, ResNet can be derived as a bottom-up inference in ResNRM.
Theorem .11. Inference in ResNRM yields skip connetions. In particular, if the rendering process at layer has the residual form as in Definition .10, the inference at this layer takes the following form:

( )  max
t( ),s( )

 (t, s; ) + skip(s; ) ( - 1) + b( )





= MaxPool ReLu Conv( 

( ), (

- 1)) + b( ) + ~ skip( )(

- 1) 



skip connection


=d MaxPool ReLu Conv(W ( ), ( 

- 1)) + b( ) + Wskip( )(

- 1) . 

skip connection

(18)

Here, when ( - 1) and ( ) have the same dimensions, ~skip( ) is chosen to be an constant identity
matrix in order to derive the parameter-free, identity shortcut among layers of the same size in the ResNet. When ( - 1) and ( ) have the different dimensions, ~skip( ) is chosen to be a learnable shortcut matrix which yields the projection shortcut Wskip( ) among layers of different sizes in

22

Under review as a conference paper at ICLR 2019

x

F(x)

weight layer
relu
weight layer

F(x)+x
relu

x
identity

Figure 5: ResNet building block as in He et al. (2016)

the ResNet. As mentioned above, identity shortcuts and projection shortcuts are two types of skip connections in the ResNet. The operator =d implies that discriminative relaxation is applied. In practice, the skip connections are usually across two layers or three layers. This is indeed a straightforward extension from the ResNRM. In particular, the ResNRM building block that corresponds to the ResNet building block in He et al. (2016) (see Fig. 5) takes the following form:
h( - 2) := ((t, s; - 1)(t, s; ) + skip(s; )) h( ).
In inference, this ResNRM building block yields the ResNet building block in Fig. 5:

( ) = ReLu Conv  ( ), ReLu Conv  ( - 1), ( - 2) + b( - 1) + b( ) + ~ skip( )( - 2)
=d ReLu Conv W ( ), ReLu Conv W ( - 1), ( - 2) + b( - 1) + b( ) + Wskip( )( - 2) .

(19)

.7.2 DENSE NEURAL RENDERING MODEL YIELDS DENSENET
In a DenseNet Huang et al. (2017), instead of combining features through summation, the skip connections concatenate features. In addition, within a building block, all layers are connected to each other (see Fig. 6). Similar to how ResNet can be derived from ResNRM, DenseNet can also be derived from a variant of NRM, which we call the Dense Neural Rendering Model (DenseNRM). In DenseNRM, the rendering matrix ( ) is concatenated by an identity matrix. This extra identity matrix, in inference, yields the skip connections that concatenate features at different layers in a DenseNet. We define DenseNRM as follows.
Definition .12. The Dense Neural Rendering Model (DenseNRM) is the Neural Rendering Model whose rendering process from layer to layer - 1, for some  {1, 2, · · · , L}, has the residual form as follows:

h( - 1) := (t, s; )h( ), 1D( )h( ) .

(20)

We again denote ( ) as (t, s; ) to show the dependency of ( ) on the latent variables t( ) and s( ). The following theorem establishes the connections between DenseNet and DenseNRM.
Theorem .13. Inference in DenseNRM yields DenseNet building blocks. In particular, if the rendering process at layer has the dense form as in Definition .12, the inference at this layer takes

23

Under review as a conference paper at ICLR 2019

the following form: ( )  = =d

max  (t, s; )( - 1) + b( )
t( ),s( )
( - 1)

MaxPool(ReLu(Conv( ( ), ( - 1)) + b( ))) ( - 1)

MaxPool(ReLu(Conv(W ( ), ( - 1)) + b( ))) ( - 1)

.

(21)

In Eqn. 21, we concatenate the output MaxPool ReLu(Conv(W ( ), ( - 1)) + b( ))) at layer with the input feature ( - 1) at layer - 1 to generate the input to the next layer ( ), just like in the DenseNet. Proofs for Theorem .11 and .13 can be found in Appendix B. The approach to proving
Theorem .11 can be used to prove the result in Eqn. 19.

x0

H1 x1

H2 x2

H3 x3

H4 x4

Figure 6: DenseNet building block as in Huang et al. (2017)

24

Under review as a conference paper at ICLR 2019

Appendix C

In this appendix, we provide the proofs for key results in the paper.

.8 PROOF FOR THEOREM 2.2: DERIVING CONVOLUTIONAL NEURAL NETWORKS FROM THE NEURAL RENDERING MODEL

To

ease

the

clarity

of

the

proof

presentation,

we

ignore

the

normalizing

factor

1 2

and

only

consider

the proof for two layers, i.e., L = 2. The argument for L  3 is similar and can be derived recursively

from

the

proof

for

L

=

2.

Similar

proof

holds

when

the

normalizing

factor

1 2

is

considered.

Now,

we obtain that

max h (y, z; 0)x + (y, z)
z

= max

h(1, p1)s(1, p1) (1, p1)B (1, p1)T (t; 1, p1)x

t(1),s(1)

t(2),s(2) p1P(1)

+ h(1, p1)s(1, p1)b(t; 1, p1) + µ(y; p2)s(2, p2)b(t; 2, p2)

p1 P (1)

p2 P (2)

= max

h(1, p1)s(1, p1)  (1, p1)B (1, p1)T (t; 1, p1)x + b(t; 1, p1)

t(1),s(1)

t(2),s(2) p1P(1)

+ µ(y; p2)s(2, p2)b(t; 2, p2)
p2 P (2)

(=a) max

h(1, p1) max s(1, p1)  (1, p1)B (1, p1)T (t; 1, p1)x + b(t; 1, p1)

t(2),s(2) p1 P (1)

t(1,p1 ),s(1,p1 )

+ µ(y; p2)s(2, p2)b(t; 2, p2) = A,
p2 P (2)
where equation in (a) is due to the non-negativity assumption that h(1, p1)  0, just as in max-sum and max-product message passing. We define (1, p1) as follows:
(1, p1) = max s(1, p1)  (1, p1)B (1, p1)T (t; 1, p1)x + b(t; 1, p1) ,
t(1,p1 ),s(1,p1 )
and let (1) = ((1, p1))p1P(1) be the vector of (1, p1). The following holds:

A = max h (1)(1) +

µ(y; p2)s(2, p2)b(t; 2, p2)

t(2),s(2)

p2 P (2)

(=b) max

µ(y; p2)s(2, p2) (2, p2)B (2, p2)T (t; 2, p2)(1)

t(2),s(2)

p2 P (2)

+ µ(y; p2)s(2, p2)b(t; 2, p2)
p2 P (2)

(=c) µ(y; p2) max s(2, p2)  (2, p2)B (2, p2)T (t; 2, p2)(1) + b(t; 2, p2)
t(2),s(2) p2 P (2)
 (2,p2 )
=µ (y)(2).

Here (2) = ((2, p2))p2P(2) is the vector of (2, p2), and in line (b) we substitute h(1) by:

h(1) =

s(2, p2)T (t; 2, p2)B(2, p2)(2, p2)µ(y; p2).

pP (2)

Notice that line (a) and (c) form a recursion. Therefore, we finish proving that the feedforward step in CNNs is the latent variable inference in NRM if (1) has the structure of the building block of

25

Under review as a conference paper at ICLR 2019 CNNs, i.e. MaxPool (ReLu (Conv())). Indeed,

(1) = ((1, p(1)))p(1)P(1)

= max s(1, p1)  (1, p1)B (1, p1)T (t; 1, p1)x + b(t; 1, p1)

t(1,p1 ),s(1,p1 )

p(1)P (1)

= max ReLu  (1, p1)B (1, p1)T (t; 1, p1)x + b(t; 1, p1) t(1,p1)=0,1,2,3 p(1)P(1) (22)

= MaxPool ReLu Conv  (1), x + b(t; 1)

=d MaxPool (ReLu (Conv (W (1), x) + b(t; 1))) ,

where W (1) =  (1) corresponds to the weights at layer 1 of the CNN, and =d implies that discriminative relaxation is applied. In Eqn. 22, since s(1, p1)  {0, 1}, max (s(1, p1) × .) is equivalent to
s(1,p1 )
max( . , 0) and, therefore, yields the ReLU function. Also, in order to compute max (), we take the
t(1,p1 )
brute force approach, computing the function inside the parentheses for all possible values of t(1, p1) and pick the maximum one. This procedure is equivalent to the MaxPool operator in CNNs. Here we can make the bias term b(t; 1) independent of t and b(t; 1, p1) are the same for all pixels p1 in the same feature map of h(1) as in CNNs. Similarly, (2) = MaxPool (ReLu (Conv (W (2), (1)) + b(2))). Thus, we obtain the conclusion of the theorem for L = 2.
.9 PROOF FOR THEOREM .11: DERIVING THE RESIDUAL NETWORKS FROM THE RESIDUAL NEURAL RENDERING MODEL
Similar to the proof in Section .8 above, when the rendering process at each layer in ResNRM is as in Eqn. 16, the activations ( ) is given by:

( ) = max s( , p)
t( ,p),s( ,p)

 ( , p)B ( , p)T (t; , p) + ~ skip( , p) ( - 1) + b(t; , p)

pP( )

= max ReLu  ( , p)B ( , p)T (t; , p)( - 1) + b(t; , p)

+ ~ skip( , p)( - 1)

t( ,p)=0,1,2,3

pP( )

= MaxPool ReLu Conv  ( , p), ( - 1) + b(t; ) + ~skip( )( - 1)





=d MaxPool ReLu Conv(W ( ), ( 

- 1)) + b(t;

) + Wskip( )(

- 1) . 

skip connection

(23)

Here we can again make the bias term b(t; ) independent of t and b(t; , p) are the same for all pixels p in the same feature map of h( ) as in CNNs. We obtain the conclusion of the Theorem .11.
26

Under review as a conference paper at ICLR 2019

.10 PROOF FOR THEOREM .13: DERIVING THE DENSELY CONNECTED NETWORKS FROM THE DENSE NEURAL RENDERING MODEL

Similar to the proof for Theorem .11 above, when the rendering process at each layer in ResNRM is as in Eqn. 20, the activations ( ) is given by:

 ( )  

max s( , p)  ( , p)B ( , p)T (t; , p)( - 1) + b(t; , p)
s( ,p),t( ,p)
( - 1)

 pP( ) 

= max ReLu  ( , p)B ( , p)T (t; , p)( - 1) + b(t; , p) t( ,p)=0,1,2,3 pP( ) ( - 1)

=

MaxPool(ReLu(Conv( ( , p), ( - 1)) + b(t; ))) ( - 1)

=d

MaxPool(ReLu(Conv(W ( ), ( - 1)) + b(t; ))) ( - 1)

.

(24)

Again we can make the bias term b(t; ) independent of t and b(t; , p) are the same for all pixels p in the same feature map of h( ) as in CNNs. We obtain the conclusion of the Theorem .13.

.11 PROVING THAT THE PARAMETRIZED JOINT PRIOR P(Y,Z) IS A CONJUGATE PRIOR

Again, for simplicity, we only consider the proof for two layers. The argument for L  3 is similar and can be derived recursively from the proof for L = 2. In the derivation below, h(2) is µ(y). As in Eqn.1 in the definition of the NRM, the joint prior of y and z is given by:

p(y, z)  exp = exp = exp = exp

1 2

b

(t; 2)(s(2)

h(2))

+

1 2

b

(t; 1)(s(1)

h(1)) + ln y

1 2

h

(2)(b(t; 2)

s(2))

+

1 2

h

(1)(b(t; 1)

s(1)) + ln y

1 2

h

(2)(b(t; 2)

s(2))

+

1 2

h

(2)

(2)(b(t; 1)

s(1)) + ln y

1 2

h

(2)



(2)(b(t; 1)

s(1)) + b(t; 2)

s(2) + ln y

(25)

Furthermore, as explained in Section .14 of Appendix D, due to the constant norm assumption with h(y, z; 0), the likelihood p(x|y, z) is estimated as follows:

p(x|y, z)  exp

1 2

h

(y, z; 0)x

= exp

1 2

h

(2)

(2)

(1)x

The posterior p(y, z|x) is given by:

p(y, z|x)  exp

1 2

h

(2)



(2)(b(t; 1)

s(1)) +  (2) (1)x + b(t; 2)

s(2) + ln y

= exp

1 2

h

(2)



(2)(b(t; 1)

s(1) +  (1)x) + b(t; 2)

s(2) + ln y

(26)

Comparing Eqn. 25 and Eqn. 26, we see that the prior and the posterior have the same functional form. This completes the proof.
.12 DERIVING OTHER COMPONENTS IN THE CONVOLUTIONAL NEURAL NETWORKS
.12.1 DERIVING THE LEAKY RECTIFIED LINEAR UNIT (LEAKY RELU)
The Leaky ReLU can be derived from the NRM in the same way as we derive the ReLU in Section .8, but instead of s( )  {0, 1}, we now let s( )  {, 1}, where  is a small positive constant. Then, in

27

Under review as a conference paper at ICLR 2019

Eqn. 22, max(s( , p) × .) is equivalent to [. < 0]( × .) + [. >= 0](.), which is the LeakyReLU
s( ,p)
function. Note that compared to Eqn. 22, here we replace the layer number (1) by ( ) since the same derivation can be applied at any layer.

.13 PROOFS FOR BATCH NORMALIZATION

In this section we will derive the batch normalization from a 2-layer DRM. The same derivation can be generalized to the case of K-layer DRM.
In order to derive the Batch Normalization, we normalize the intermediate rendering image h( ).

h(y, z; 0)

=

p1 P (1)

s(1,

p1)T

(t;

1,

p1)B(1,

p1)(1,

p1)

1 h(1,

p1)

(h(1,

p1)

-

Eh(1,

p1))

-

Eh(y,z;0)

=

p1 P (1)

s(1,

p1)T

(t;

1,

p1)B(1,

p1)(1,

p1)

1 h(1,

p1)

×

p2

P

(2)

s(1,

p2

)T

(t;

2,

p2

)B

(2,

p2

)(2,

p2

)

h

1 (y,

z

;

0)

h(y

,

z

;

0)

(1, p1) - Eh(1, p1)

- Eh(y,z;0)

(27)

During inference, we de-mean the input image and find z = arg max h
z

(y, z; 0)(x - Eh(y,z;0)). In

particular, the inference can be derived as follows:

max h
z

(y, z; 0)(x - Eh(y,z;0))

= max
z

p1

P

(1)

s(1,

p1

)T

(t;

1,

p1

)B

(1,

p1

)(1,

p1

)

h

1 (1,

p1

)

h(1, p1) - Eh(1, p1)

- Eh(y,z;0)(x - Eh(y,z;0))

(x - Eh(y,z;0))

const w.r.t y,z

a max

s(1, p1)

z

p1 P (1)

h(1, p1) - Eh(1, p1)
scalar

1 h(1,

p1)



(1, p1)B

(1, p1)T

(t; 1, p1) (x - Ei)

row vector

column vector

scalar

+ const := A + const.

dot product

Direct computation leads to

A = max

s(1, p1) h(1, p1) - Eh(1, p1)

z

p1 P (1)

(1, p1) 1 h(1, p1) (1, p1)
(1,p1) normalize

× ( (1, p1)B (1, p1)T (t; 1, p1)x - E[ (1, p1)B (1, p1)T (t; 1, p1)x])
de-mean

= max

max s(1, p1) h(1, p1) - Eh(1, p1)

z(2) p1P(1) s(1,p1),t(1,p1)

× BatchNorm( (1, p1)B (1, p1)T (t; 1, p1)x; (1, p1))

= max h(1) - Eh(1)
z(2)

MaxPool(ReLu(BatchNorm(Conv((1), x); (1), 0))) (28)

28

Under review as a conference paper at ICLR 2019

In line (a), we approximate Eh(y,z;0) by its empirical value Ei. The de-mean and normalize operators with the scale parameter (1) and the shift parameter (1) = 0 in the equations above already have the form of batch normalization. Note that when the model is trained with Stochastic Gradient
Descent (SGD), the scale and shift parameters at each layer also account for the error in evaluating statistics of the activations using the mini-batches. Thus, (1) is not 0 any more, but a parameter learned during the training. Also, in the equations above, (1) is the activations at layer 1 in CNNs and given by:

(1) = MaxPool(ReLu(Normalize(Demean(Conv((1), x))))),

(29)

where (1) is the standard deviation of the (1). Eqn.28 can be expressed in term of (1) as follows:

max h(1) - Eh(1) (1) + const
z(2)

= max h (1)((1) - E(1)) + (h (1)E(1) - Eh (1)(1)) + const
z(2)

= max
z(2) p2 P (2)

s(2, p2)µ(y, p2)

(2, p2) µ(y)(p2)

1 (2, p2)

(2,p2) normalize

×  (2, p2)B (2, p2)T (t; 2, p2)(1) - E[ (2, p2)B (2, p2)T (t; 2, p2)(1)]
de-mean
+ h(1, p1)E(1, p1) - Eh(1, p1)(1, p1) + const
 (2,p2 )

= max µ (y) MaxPool(ReLu(BatchNorm(Conv((2), (1)); (2), (2) (2)))) + const
z(2)
(30)

The batch normalization at this layer of the CNN has the scale parameter (2) and the shift parameter is the element-wise product of (2) and (2).

.14 PROOFS FOR CONNECTION BETWEEN NRM AND CROSS ENTROPY

PROOF OF THEOREM .1 (a) To ease the presentation of proof, we denote the following key

notation

A

:=

max
(zi )ni=1 ,A

1 n

n i=1

ln p(yi|xi, zi; ).

From the definition of A = { : h(y, z; 0) = }, we achieve the following equations

A

=

max
A

1 n

n i=1

max log p(yi|xi, zi; )
zi

=

1n

max
A

n

i=1

max log
zi

p(xi|yi, zi; )p(yi|zi; )
K
p(xi|y, zi; )p(y|zi; )

y=1

=

1n

max
A

n

i=1

max
zi

log p(xi|yi, zi; ) + log p(yi, zi|) - log

K
p(xi|y, zi; )p(y, zi|)
y=1

.

From the formulation of NRM, we have the following formulation of prior probabilities p(y, z|)

p(y, z|)

=

y

exp
exp
,z

(y, z) 2 (y , z 2

y ) y

.

29

Under review as a conference paper at ICLR 2019

for all (y, z). By means of the previous equations, we eventually obtain that

A

=

max
A

1 n

n i=1

max
zi

log p(xi|yi, zi; )

+

log


   

exp

(yi, zi) 2

exp
y,z

(y, z) 2

yi y


   

K
- log
y=1

p(xi|y,

zi;

)
y

exp
exp
,z

(y, zi) 2 (y , z 2

y ) y

=

max 1 n max A n i=1 zi

log p(xi|yi, zi; ) + log

exp

(yi, zi) 2

yi

- log

K

p(xi|y, zi; ) exp

(y, zi) 2

y

.

y=1

By defining i(y, zi) := log p(xi|y, zi; ) + log

exp

(y, zi) 2

y

for all 1  i  n, the above

equation can be rewritten as

A

=

max
A

1 n

n i=1

max
zi

log (exp(i(yi, zi))) - log

K
exp(i(y, zi))
y=1

=

max
A

1 n

n i=1

max
zi

log

(Softmax(i

(yi

,

zi

)))



max 1 n log A n i=1

Softmax

max i(yi, zi))
zi

= B.

By means of direct computation, the following equations hold

B

=

max
A

1 n

n i=1

log

Softmax

max
zi

log

p(xi|yi,

zi;

)

+

(yi,

zi)/2

+

log

yi

=

max 1 n log A n i=1

Softmax

max -
zi

xi - µyi,zi 22

2

+

(yi, zi) 2

+

log yi

(=i)

max 1 n log A n i=1

Softmax

max
zi

h (yi, zi; 0)xi + (yi, zi) 2

+ byi

=

- min
A

1 n

n i=1

- log

Softmax

max
zi

h (yi, zi; 0)xi + (yi, zi) 2

+ byi

=

- min
A

1 n

n i=1

- log q(yi|xi)

=

- min
A

Hp,q (y|x)

where equation (i) is due to the constant norm assumption with rendered images h(y, z; 0). Therefore, we achieve the conclusion of part (a) of the theorem.

30

Under review as a conference paper at ICLR 2019

(b) Regarding the upper bound, from the definition of zi, we obtain that

max log (Softmax(i(yi, zi))) - log Softmax max i(yi, zi))
zi zi

= log (Softmax(i(yi, zi))) - log Softmax max i(yi, zi))
zi

max exp(i(yi, zi))

 log zi

- log

K

exp(i(y, zi))

y=1

Softmax

max i(yi, zi))
zi

KK

= log

exp(max i(y, zi)) - log

exp(i(y, zi))

y=1

zi

y=1

 log K + max max i(y, zi) - max i(y, zi)

y zi

y

 log K + max max i(y, zi) - i(y, zi)
y zi
for any 1  i  n. As a consequence, we obtain the conclusion of part (b) of the theorem.

PROOF OF THEOREM .4 (a) From the definitions of (yi, zi) and (yi, zi), we obtain that

Un

=

min


1 n

n

1 2

xi - h(yi, zi; 0)

2 - log yi,zi

i=1

=

min


1n n

1 2

xi - h(yi, zi; 0)

2 - (yi, zi) - log yi

i=1

+ log

exp((y , z ) + log y )

(y ,z )J

By means of direct computation, the following inequality holds

A



min


1n n

1 2

xi - h(yi, zi; 0)

2 - (yi, zi) - log yi

i=1

=A

+ log

exp((y , z ) + log y ) .

(y ,z )J

It is clear that

exp((y , z ) + log y )  |L|

exp(max
z L

(y

,

z

)

+

log

y

).

(y ,z )J

y

Combining this inequality with the inequality of the term A in the above display, we have

Un



min


1n n

1 2

xi - h(yi, zi; 0)

2 - (yi, zi) - log yi

+ (log yi - log yi )

i=1

+ log

exp(max
z L

(y

,

z

)

+

log

y

)

+ log |L|

y

=

min


1 n

n

i=1

xi - h(yi, zi; 0) 2

2

- log(yi,zi )

+

log yi - log yi

+ log |L|



min


1 n

n

i=1

xi - h(yi, zi; 0) 2

2

- log(yi,zi )

+ log

1 

-

1

+ log |L|

=

Vn + log

1 

-

1

+ log |L|

31

Under review as a conference paper at ICLR 2019

where the final inequality is due to the fact that yi /yi  (1 - )/ for all 1  i  n. Therefore, we achieve the conclusion of part (a) of the theorem. (b) Similar to the proof argument with part (a), we have

Un

=

min


1n n

1 2

xi - h(yi, zi; 0)

2 - (yi, zi) - log yi

i=1

+ log

exp((y , z ) + log y )

(y ,z )J



min


1n n

1 2

xi

2 - h(yi, zi; 0)

xi - (yi, zi) +

h(yi, zi; 0) 2/2 - log yi

i=1

+ log

exp(max
z L

(y

,

z

)

+

log

y

)

= B.

y

Direct computation with B leads to

B

=

min


1 n

n

i=1

xi - h(yi, zi; 0) 2

2

- log(yi,zi )

+

log yi - log yi

+ h(yi, zi; 0) 2 - h(yi, zi; 0) 2



min


1 n

n

i=1

xi - h(yi, zi; 0) 2

2

- log(yi,zi )

+

min


1 n

n

log yi - log yi

i=1

+

min


1 n

n

i=1

h(yi, zi; 0) 2 - h(yi, zi; 0) 2



Vn + log

 1-

+

min


1 n

n

i=1

h(yi, zi; 0) 2 - h(yi, zi; 0) 2

where the final inequality is due to the fact that yi /yi  /(1 - ) for all 1  i  n. As a consequence, we achieve the conclusion of part (b) of the theorem.

32

Under review as a conference paper at ICLR 2019

.15 PROOFS FOR STATISTICAL GUARANTEE AND GENERALIZATION BOUND FOR (SEMI)-SUPERVISED LEARNING

PROOF OF THEOREM .6 The proof of this theorem relies on several results with uniform laws of large numbers. In particular, we will need to demonstrate the following results

sup 1 n1 max  n1 i=1 (y,z)J

h (y, z; 0)xi +  (y,z)

- max h (y, z; 0)x +  (y,z) dP (x)  0,
(y,z)J

(31)

1 n1

Ln1

=

sup


n1

i=1 (y

,z

)J

1

(y

,z

)=arg max

h

(y,z;0)xi+ (y,z )

 

 (y,z)J



h(y , z ; 0) 2

2

- log y

- 1


 

h(y

,z 2

; 0)

2

- log y

dP (x)  0,(32)

(y ,z )J (y ,z )=arg max h (y,z;0)x+ (y,z )

 (y,z)J



sup


1 n - n1

n
max
zL i=n1 +1

h

(yi, z; 0)xi +  (yi,z)

-

max
zL

h

(y, z; 0)x +  (y,z)

En(1)

= sup


1 n - n1

n i=n1+1 z

1

L z

=arg max

 zL

h (yi,z;0)xi+ (yi,z )

 


dQ(x, c)  0,

(33)

h(yi, z 2

; 0)

2

- log yi

- 1


 

h(y, z ; 0) 2

2

- log y

dQ(x, c)  0,

(34)

z L z =arg max h (y,z;0)x+ (y,z )

 gJ



En(2)

= sup


1 n - n1

n
log q(yi|xi) -
i=n1 +1

log q(y|x)dQ(x, c)  0,

En(3)

= sup


1 n

nK
q(y|xi) log
i=1 y=1

q (y |xi ) y

-

K
q(y|x)dQ(x, c) log
y=1

q (y |x) y

dP (x)  0,

(35) (36)

almost surely as n  . The proof for equation 33 is similar to that of equation 31; therefore, it is omitted.

Proof of equation 31: It is clear that

sup


1 n1

n1
max
(y,z)J i=1

h (y, z; 0)xi +  (y,z) - max h (y, z; 0)x +  (y,z) dP (x)
(y,z)J

 sup
|S ||J |

1 n1

n1
max s
sS i=1

[xi, 1] -

max s [x, 1]dP (x)
sS

where [x, 1]  RD(0)+1 denotes the vector forms by concatenating 1 to x  RD(0) and S in the above supremum is the set of finite elements in RD(0)+1. Therefore, to achieve the result of equation 31, it is sufficient to show that

sup
|S ||J |

1 n1

n1
max s
sS i=1

[xi, 1] -

max s [x, 1]dP (x)  0.
sS

(37)

33

Under review as a conference paper at ICLR 2019

To obtain the conclusion of equation 37, we utilize the classical result with bracketing entropy to

establish the uniform laws of large number (cf. Lemma 3.1 in van de Geer (2000)). In particular,

we

denote

G

to

be

the

family

of

function

on

RD(0)

with

the

form

fS

(x)

=

max s
sS

[x, 1] where

S  O|J |, which contains all sets that have at most |J | elements in RD(0)+1. Due to the assumption with distribution P , we can restrict O|J | to contain only set S with elements in B(R), which is a

closed ball of radius R on RD(0)+1. By means of Lemma 2.5 in (van de Geer, 2000), we can find

a finite set E such that each element in B(R) is within distance  to some element of E for all  > 0. We denote O|J |() to be the subset of O|J | such that it only contains sets with elements in
E for all  > 0. Now, for each set S = {s1, . . . , sk}  O|J |, we can choose corresponding set

S = {s1, . . . , sk}  O|J |() such that si - si   for all 1  i  k. Now, we denote

f S(x) = max s [x, 1] +  [x, 1] ,
sS
f (x) = max s [x, 1] -  [x, 1]
S sS

for

any

S



O|J |().

It

is

clear

that

f (x)
S



fS (x)



f S(x)

for

all

x



RD(0) .

Furthermore,

we

also have that

(f

S

(x)

-

f

(x))dP
S

(x)

=

2

[x, 1] dP (x)  2

x dP (x) + 1 .

For any > 0, by choosing  < 2( x dP (x) + 1) then we will have that (f S(x) - f S(x))dP (x) < . It implies that the -bracketing entropy of G is finite for the L1 norm with distribution P (for the definition of bracketing entropy, see Definition 2.2 in van de Geer (2000)). According to Lemma 3.1 in van de Geer (2000), it implies that G satisfies the uniform law of large
numbers, i.e., equation 37 holds.

Proof of equation 32: To achieve the conclusion of this claim, we will need to rely on the control of Rademacher complexity based on Vapnik-Chervonenkis (VC) dimension. In particular, we firstly demonstrate that

sup

1

n1
1

|S |=k n1 i=1

j=arg max[xi,1]
1l|S |

sl

-

1 dP (x)  0
j=arg max[x,1] sl
1l|S |

(38)

almost surely as n1   for each 1  j  k and k  1 where the supremum is taken with respect to S = {s1, . . . , sk}. For each j, we denote the Rademacher complexity as follows

Rn1

= E sup
|S |=k

1 n1

n1
i1
i=1

j=arg max[xi,1]
1l|S |

sl

where 1, . . . , n1 are i.i.d. Rademacher random variables, i.e., P(i = -1) = P(i = 1) = 1/2 for 1  i  n1. Then, for any n1  1 and   0, according to standard argument with Rademacher complexity (Vershynin, 2011),

sup

1

n1
1

|S |=k n1 i=1

j=arg max[xi,1]
1l|S |

sl

-

1 dP (x)  2Rn1 + 
j=arg max[x,1] sl
1l|S |

with probability at least 1 - 2 exp

-

n12 8

. According to Borel-Cantelli's lemma, to achieve

equation 38, it is sufficient to demonstrate that Rn1  0 as n1  .

To achieve that result, we will utilize the study of VC dimension with partitions (cf. Section 21.5 in Devroye et al. (1996)). In particular, for each set S = (s1, . . . , sk), it gives rise to the partition Ai = x  RD(0) : [x, 1] si  [x, 1] sl  l  {1, . . . , k} as 1  i  k. For

34

Under review as a conference paper at ICLR 2019

our purpose with equation 38, it is sufficient to consider Pn1 = Aj, Ai , which is a par-
i=j
tition of B(R), for each set S with k elements. We denote F to be the collection of all Pn for all S with k elements and B(Pn) the collection of all sets obtained from the unions of elements of Pn. For each data (x1, . . . , xn1 ), we let NF (x1, . . . , xn1 ) the number of different sets in {(x1, . . . , xn1 )  A : A  B(Pn1 ) for Pn1  F }. The shatter coefficient of F is defined as

n1 (F )

=

s(F ,

n1)

=

max
(x1,...,xn1 )

NF

(x1,

.

.

.

,

xn1 ).

According to Lemma 21.1 in maximal number of different

Devroye ways that

ent1alp.o(i1n9ts96c)a,nbne1p(aFrt)ition4ed nb1y(mFe)mwbheerrseof Fn1.(FN)owis,

the for

each element Pn1 = Aj, Ai of F , it is clear that the boundaries between Aj and Ai are
i=j i=j

subsets of hyperplanes. From the formulation of Aj, we have at most k - 1 boundaries between

Aj and Ai. From the classical result of Dudley (1978), each n1 points in B(R) can be splitted

i=j

by

a

hyperplane

in

at

most

nD(0) +1
1

different

ways

as

the

VC

dimension

of

the

hyperplane

is

at

most D(0) + 1.

As a consequence, we would have n1 (F )



n(D(0)
1

+1)(k-1)

,

which

leads

to

n1 (F )  4n(1D(0)+1)(k-1).

Going back to our evaluation with Rademacher complexity Rn1 , by means of Massart's lemma, we have that

Rn1

=

E

E sup
|S |=k

1 n1

n1
i1
i=1

j=arg max[xi,1]

sl

|x1, . . . , xn1

1l|S |

E

2 log 2NF (x1, . . . , xn1 )  2(log 8 + (D(0) + 1)(k - 1) log n1)  0 (39) n1 n1

as n1  . Therefore, equation 38 is proved.

Proof of equation 34: To achieve the conclusion of this claim, we firstly demonstrate that

sup
|S |=k

1 n - n1

n
1
i=n1 +1

j=arg max[xi,1] sl
1l|S |

1{yi=l}

-1

1{c=l}dQ(x, c)  0

j=arg max[x,1] sl

1l|S |

(40)

almost surely as n   for each 1  j  k and 1  l  K where k  1 and the supremum is taken with respect to S = {s1, . . . , sk}. The proof of the above result will rely on VC dimension with Voronoi partitions being established in equation 32. In particular, according to the standard argument
with Rademacher complexity, it is sufficient to demonstrate that

Rn

= E sup
|S |=k

1 n - n1

n
i1
i=n1 +1

j=arg max[xi,1] sl
1l|S |

1{yi=l}

 0.

35

Under review as a conference paper at ICLR 2019

By means of the inequality with Rademacher complexity in equation 39, we obtain that

Rn

=

n-n1
E
v=0 cAv

E sup
|S |=k

1 n - n1

n
i1
i=n1 +1

j=arg max[xi,1]
1l|S |

sl

1{yi=l} |c  Av

P(c  Av)

=

n-n1
E sup
v=0 |S |=k

1 n - n1

n1 +v
i1
i=n1 +1

j=arg max[xi,1]
1l|S |

sl

plv(1 - pl)n-n1-v

n - n1 v



n-n1

v

v=1 n - n1

2(log

8

+

(D(0)

+ v

1)(k

-

1)

log

v)

pvl (1

-

pl )n-n1 -v

n - n1 v



2 log 8 n-n1 n - n1 v=1

n

v -

n1

plv

(1

-

pl )n-n1 -v

n - n1 v

+

2(D(0) + 1)(k - 1) log(n - n1) n-n1

n - n1

v=1

(n

-

v n1)

log v log(n

-

n1

)

pvl

(1

-

pl

)n-n1

-v

n - n1 v



2 log 8 n - n1

+

2(D(0) + 1)(k - 1) log(n - n1) n - n1

n-n1
plv(1 - pl)n-n1-v

n - n1 v

v=1

=

2 log 8 n - n1

+

2(D(0) + 1)(k - 1) log(n - n1) n - n1

(1 - (1 - pl)n-n1 )  0

as n   where c = (cn-n1+1, . . . , cn) and Av is the set of c such that there are exactly v values of yi to be l for 0  v  n - n1. The final inequality is due to the fact that v/(n - 1)  1 and v log v/ {(n - 1) log(n - 1)}  1 for all 1  v  n - n1. Therefore, we achieve the conclusion of equation 40.
Now, coming back to equation 34, by means of triangle inequality, we achieve that

En(1)



K
sup
l=1 

1 n - n1

n i=n1+1 z

1

L z

=arg max

 zL

h (yi,z;0)xi+ (yi,z )

 1{yi =l}
 

h(yi, z 2

; 0)

2

- log yi

-

1 1{y=l}
 z L z =arg max h (y,z;0)x+ (y,z )

h(y, z ; 0) 2

2

- log y

dQ(x, y)

 gJ



K
 sup
l=1  z L

h(l, z ; 0) 2

2

- log l

n

1 - n1

n
1
 i=n1+1 z

=arg max

 zL

h (yi,z;0)xi+ (yi,z )

 1{yi =l}
 

- 1

1{y=l}dQ(x, y)  0



z =arg max h (y,z;0)x+ (y,z )

 gJ



where the last inequality is due to the results with uniform laws of large numbers from equation 40 and

the fact that

h(l, z ; 0) 2

2
- log l is bounded for all l and z

 L. Hence, we obtain the conclusion

of equation 34.

36

Under review as a conference paper at ICLR 2019

Proof of equation 35: For this claim, we have the following inequality

En(2)



sup {Sy },{y }

1 n - n1

n
log
i=n1 +1

exp

max s
sSyi

[xi, 1] + log yi

K
exp max s [xi, 1] + log l

=1 sSl

exp max s [x, 1] + log y

- log

sSy

K

dQ(x, y)

exp max s [x, 1] + log l
=1 sSl



K
sup =1 {Sy},{y}

1 n - n1

n
log
i=n1 +1

exp

max s
sSyi

[xi, 1] + log yi

K
exp max s [xi, 1] + log l

=1 sSl

1{yi=l}

-

log

exp

max s
sSy

[x, 1] + log y

K

K
1{c=l}dQ(x, y) = Fn,l

exp max s [x, 1] + log l
=1 sSl

=1

where Sy in the above supremum stands for the collection of sets S1, . . . , SK such that |Sy|  |L| and elements in Sy are in RD(0)+1. Therefore, to achieve the conclusion of equation 35, it is sufficient to demonstrate that Fn,l  0 almost surely as n   for each 1  l  K.
In fact, for each 1  l  K, we denote G to be the family of function on RD(0) × {1, . . . , K} of the form

exp f{Sy},{y}(x, y) = log K

max s
sSy

[x, 1] + log y

exp max s [x, 1] + log l

=1 sSl

1{y=l}

for all (x, y) where S1, . . . , SK  O|L|, which contains all sets that have at most |L| elements in
RD(0)+1, and {y} satisfy that K y = 1 and y   for all 1  y  K. Due to the assumption
y=1
with distribution P , we can restrict O|L| to contain only set S with elements in the ball B(R)
of radius R on RD(0)+1. By means of Lemma 2.5 in (van de Geer, 2000), we can find a finite set E such that each element in B(R) is within distance  to some element of E for all  > 0. Additionally, there exists a set () such that for each (1, . . . , K), we can find a corresponding element (1, . . . , K )  () such that (1, . . . , K ) - (1, . . . , K )  . We denote

F () = Sy , {y} : elements of Sy in E, and(1, . . . , K )  ()

for all  > 0.

For each element

Sy

K y=1

,

{y

}yK=1,

we can choose the corresponding element

Sy

K y=1

,

{y }Ky=1



F () such that Sy

=

sy1, . . . , syky , Sy =

sy1, . . . , syky

satisfy syj - syj   for all 1  y  K and 1  j  ky. Additionally,

37

Under review as a conference paper at ICLR 2019

(1, . . . , K ) - (1, . . . , K )  . With these notations, we define

f {Sy},{y}(x, y) = log f {Sy},{y}(x, y) = log

exp(max s [x, 1] + log y)
sSy

K

exp(max s [x, 1] + log  )

 =1

sS

exp(max s [x, 1] + log y)
sSy

K

exp(max s [x, 1] + log  )

 =1

sS

1{y=l} + 2 [x, 1] 1{y=l} - 2 [x, 1]

+ 2/, - 2/

for any Sy , {y}  F (). By means of Cauchy-Schwarz's inequality, we have

syi[x, 1]1{y=l} - (syi) [x, 1]1{y=l}  sci - syi [x, 1] 1{y=l}   [x, 1]

for all x and 1  i  k. Additionally, the following also holds

syi[x, 1]1{y=l} - (syi) [x, 1]1{y=l}  - [x, 1] . Furthermore, | log y - log y|  log(1 + /)  /. Hence, we obtain that

exp(max s
sSy

[x, 1] + log y)



exp(max s [x, 1] + log y +  [x, 1] + /)
sSy

KK

exp(max s

 =1

sS

[x, 1] + log  )

exp(max s [x, 1] + log  -  [x, 1] - /)

 =1

sS

exp(max s [x, 1] + log y)


K

sSy

exp(2 [x, 1] + 2/).

exp(max s [x, 1] + log  )

 =1

sS

Similarly, we also have

exp(max s
sSy

[x, 1] + log y)



exp(max s [x, 1] + log y)
sSy

exp(-2 [x, 1] - 2/).

KK

exp(max s

 =1

sS

[x, 1] + log  )

exp(max s [x, 1] + log  )

 =1

sS

As a consequence, we achieve that

f {Sy},{y}(x, y)  f{Sy},{y}(x, y)  f {Sy},{y}(x, y) for all (x, y). With the formulations of f {Sy},{y}(x, c) and f {Sy},{y}(x, y), we have

f {Sy},{y}(x, y) - f {Sy},{y}(x, y) dQ(x, y)

= 4 [x, 1] dQ(x, y) + 4/  4

x dQ(x, y) + 1 + 4/.

For any

> 0, by choosing  < 4 x dQ(x, y) + 4 + 4/ then we will have

(f {Sy},{y}(x, y) - f {Sy},{y}(x, y))dQ(x, y) < . It implies that the -bracketing entropy of G is finite for the L1 norm with distribution Q. Therefore, it implies that G satisfies the uniform law of large numbers, i.e., Fn,l  0 almost surely as n   for all 1  l  K. As a consequence,
the uniform law of large number result equation 35 holds.

Going back to the original problem, denote 0 =

µ0(y)

K y=1

,

L
0( ) ,
=1

y0

K,
y=1

L
b0( )
=1

the optimal solutions of population partially labeled LDCE (Note that, the existence of these optimal

38

Under review as a conference paper at ICLR 2019

solutions is guaranteed due to the compact assumptions with the parameter spaces  , , and  for all 1   L). Then, according to the formulation of partially labeled LDCE, we will have that

Yn

 min RC n

n1
1
 i=1 (y ,z )J (y ,z )=arg max

xi h0(y,z;0)+ (y0,z )

 

 (y,z)J



- log(p0(y , z ))

n
+ 1
 i=n1+1 z L z =arg max
 zL

xi h0(y,z;0)+ (y0,z )

 


xi - h0(y , z ; 0) 2 2
xi - h0(yi, z ; 0) 2 2

- log(p0(yi, z ))

-

CE n - n1

n
log q0 (yi|xi) +
i=n1 +1

KL n

n i=1

K
q0 (y|xi) log
y=1

q0 (y|xi) y

= Dn

for all n  1 where we have the following formulations

h0(y, z; 0) = 0(z; 1) . . . 0(z; L)µ0(y),

0(z; ) =

s( , p)T (t; , p)B( , p)~0( , p),

pP( )

K

p0(y , z ) = exp  (y0 ,z ) + log y0 /

exp

y=1

max
zL



(y0,z

)

+

log

y0

.

From the results with uniform laws of large numbers in equation 31, equation 32, equation 33, equation 34, equation 35, and equation 36, we obtain that

1 n1

n1 i=1 (y

,z

)J

1

(y

,z

)=arg max

 (y,z)J

xi h0(y,z;0)+ (y0,z )

 


xi - h0(y , z ; 0) 2

2

- log(p0(y , z ))

 1

 x - h0(y , z ; 0) 2 - log(p0(y , z )) dP (x),

 (y ,z )J (y ,z )=arg max x h0(y,z;0)+ (y0,z )

2

 (y,z)J



n

1 - n1

n i=n1+1 z

1

L z

=arg max

 zL

xi h0(y,z;0)+ (y0,z )

 


xi - h0(yi, z 2

; 0)

2

- log(p0(yi, z

))

 1

 x - h0(y, z ; 0) 2 - log(p0(y, z )) dQ(x, y),

 z L z =arg max x h0(y,z;0)+ (y0,z )

2

 zL



n

1 - n1

n i=n1 +1

log

q0 (yi|xi)



log q0 (y|x)dQ(x, y),

1n n

K
q0 (y|xi) log

i=1 y=1

q0 (y|xi) y



K
q0 (y|x) log
y=1

q0 (y|x) y

dP (x)

almost surely as n  . Combining with the fact that n1/n  , the above results lead to Dn  Y

almost surely as n



.

Therefore, we have

lim
n

Yn



Y

almost surely as n



.

On the other

hand, with the results of uniform laws of large numbers in equation 31, equation 32, equation 33,

39

Under review as a conference paper at ICLR 2019

equation 34, equation 35, and equation 36, we also have that

1 n1

n1

i=1 (y

,z

)J

1

(y

,z

)=arg max

 (y,z)J

xi h(y,z;0)+ (y,z )

 


xi - h(y , z ; 0) 2

2

- log(p(y , z

))

 1





(y ,z )J (y ,z )=arg max x h(y,z;0)+ (y,z )

 (y,z)J



n

1 - n1

n i=n1+1 z

1

L z

=arg max

 zL

xi h(y,z;0)+ (y,z )

 


x - h(y , z 2

; 0)

2

- log(p(y

,z

))

dP (x),

xi - h(yi, z 2

; 0)

2

- log(p(yi, z

))



1 

z L z =arg max x h0(y,z;0)+ (y,z )

x - h(y, z ; 0) 2

2

- log(p(y, z

))

dQ(x, y),

 zL



n

1 - n1

n i=n1 +1

log

q (yi |xi )



1n n

K
q(y|xi) log

i=1 y=1

q (y |xi ) y

log q(y|x)dQ(x, y),
K
 q(y|x) log
y=1

q (y |x) y

dP (x)

almost surely as n   where  =

{µ(y)}Ky=1 ,

( )

L =1

,

{y }Ky=1

,

b( )

L =1

is the optimal

solution of partially labeled LDCE and

h(y, z; 0) = (z; 1) . . . (z; L)µ(y),

(z; ) =

s( , p)T (t; , p)B( , p)~( , p),

pP( )

K

p(y , z ) = exp  (y ,z ) + log y /

exp

max
zL

 (y

,z

)

+

log

y

y=1

Hence, we eventually achieve that

.

Yn  RC 

1 



(y ,z )J (y ,z )=arg max x h(y,z;0)+ (y,z )

 (y,z)J



x - h(y , z ; 0) 2 2

- log(p(y , z )) dP (x) + (1 - )

1 



z L z =arg max x h0(y,z;0)+ (y,z )

 zL



x - h(y, z ; 0) 2 2

- log(p(y, z )) dQ(x, y) - CE log q(y|x)dQ(x, y)

+KL

K
q(y|x) log
y=1

q (y |x) y

dP (x)  Y

almost surely as n  . As a consequence, we achieve Yn  Y almost surely as n  . We reach the conclusion of the theorem.

PROOF OF THEOREM .7 The proof argument of this theorem is a direct application of the results with uniform laws of large numbers in the proof of Theorem .6. In fact, we define

F0( ) = (S, {y} , {b( )}) : S = h(y, z; 0) : (y, z)  J ,

K

and inf (S0,{y0},{b0( )})G(F0)

H(S, S0) + |y - y0| +

y=1

zL

b( ) - b0( )



40

Under review as a conference paper at ICLR 2019

for any > 0. Since the parameter spaces of  are compact sets, the set F0( ) is also a compact set for all > 0. Denote

g (S, {y} , {b( )})

= RC 

1 

(y ,z )J (y ,z )= h (y,z;0)x+ (y,z )


+(1 - )

1 



z L z =arg max h (y,z;0)x+ (y,z )

 zL



x - h(y , z 2

; 0)

2

- log(y

,z

)

dP (x)

x - h(y, z 2

; 0)

2

- log(y,z

)

dQ(x, y)

-CE

log q(y|x)dQ(x, y) + KL

K
q(y|x)dQ(x, c) log
y=1

q (y |x) y

dP (x)

for all (S, {y} , {b( )}). From the definition of F0( ), we have that

g (S, {y} , {b( )}) > g S0, y0 , b0( )

for all (S, {y} , {b( )})  F0( ) and S0, y0 , b0( )  G(F0). As F0( ) is a compact set, we further have that
inf g (S, {y} , {b( )}) > g S0, y0 , b0( )
(S,{y },{b( )})F0( )
for all S0, y0 , b0( )  G(F0) and > 0. Now, according to the uniform laws of large numbers established in the proof of Theorem .6, we have that Yn  g Sn, {y} , b( ) almost surely as n  . According to the result of

Theorem .6, it implies that g Sn, {y} , b( )  g S0, y0 , b0( ) almost surely for all

S0, y0 , b0( )  G(F0). Therefore, for each > 0 we can find sufficiently large N such that we have

K

inf (S0,{y0},{b0( )})G(F0)

H(Sn, S0) + |y - y0| +

y=1

zL

b( ) - b0( )

<

almost surely for all n  N . As a consequence, we achieve the conclusion of the theorem.

PROOF OF THEOREM .8 The proof of the theorem is an application of Theorem 11 for generalization bound with margin from Koltchinskii & Panchenko (2002) based on an evaluation of Rademacher complexity. In particular, we denote
Jn = hn (x, y) : RD(0) × {1, . . . , K}  R|

hn (x,

y)

=

max
zL( n)

h

(y, z; 0)x +  (y,z)

+ log y  (x, y) for some |L( n)|   n|L| .

Now, we denote Jn = {hn (., y) : y  {1, . . . , K} , hn  Jn}. For any  > 0, using the same argument as that of the proof of Theorem 11 in (Koltchinskii & Panchenko, 2002), with probability at least 1 - , we have

R0(fn )



inf
(0,1]

Rn,(fn )

+

8K (2K 

-

1)

n(Jn)

+

log log2(2-1) n

1/2
+

log(2-1) 2n

(41)

41

Under review as a conference paper at ICLR 2019

where n(Jn) is Rademacher complexity of Jn, which in our case is defined as

n(Jn) = E sup sup
 |L( n)| n|L|

1 n

n
i
i=1

max h (y, z; 0)xi +  (y,z)
gL( n)

+ log y

where 1, . . . , n are i.i.d. Rademacher random variables. Since  is the lower bound of y for all 1  y  K, we obtain that

n(J )



1n

E sup sup
 |L( n)| n|L|

n

i
i=1

max h (y, z; 0)xi +  (y,z)
gL( n)

+

E sup sup
 |L( n)| n|L|

1 n

n
i log y
i=1



E sup sup
 |L( n)| n|L|

1 n

n
i
i=1

max h (y, z; 0)xi +  (y,z)
gL( n)

+

|

log | n

Furthermore, we have the following inequalities

E sup sup
 |L( n)| n|L|

1 n

n
i
i=1

max h (y, z; 0)xi +  (y,z)
gL( n)

E
|S

sup
|cn |L|

1 n

n

i

max
sS

s

i=1

[xi, 1]

 2cn|L|E sup
sB(R)

1 n

n
is
i=1

[xi, 1]

 2cn|L|RE

1 n

n

i[xi, 1]

i=1

 2cn|L|(R2 + 1) n

where the final inequality is due to Cauchy-Schwartz's inequality. Combining the above results with equation 41, we achieve the conclusion of the theorem.

42

Under review as a conference paper at ICLR 2019

Appendix D
In this appendix, we provide detail descriptions for several simulation studies in the main text. .16 ARCHITECTURE OF THE NETWORK USED IN OUR SEMI-SUPERVISED EXPERIMENTS
Table 6: The network architecture used in all of semi-supervised experiments on CIFA10, CIFAR100 and SVHN.

NAME
input conv1a conv1b conv1c pool1 drop1 conv2a conv2b conv2c pool2 drop2 conv3a conv3b conv3c pool3 dense output

DESCRIPTION
32 × 32 RGB image 128 filters, 3 × 3, pad = 'same', LReLU ( = 0.1) 128 filters, 3 × 3, pad = 'same', LReLU ( = 0.1) 128 filters, 3 × 3, pad = 'same', LReLU ( = 0.1) Maxpool 2 × 2 pixels Dropout, p = 0.5 256 filters, 3 × 3, pad = 'same', LReLU ( = 0.1) 256 filters, 3 × 3, pad = 'same', LReLU ( = 0.1) 256 filters, 3 × 3, pad = 'same', LReLU ( = 0.1) Maxpool 2 × 2 pixels Dropout, p = 0.5 512 filters, 3 × 3, pad = 'valid', LReLU ( = 0.1) 256 filters, 1 × 1, LReLU ( = 0.1) 128 filters, 1 × 1, LReLU ( = 0.1) Global average pool (6 × 6  1×1 pixels) Fully connected 128  10
Softmax

.17 TRAINING DETAILS
.17.1 SEMI-SUPERVISED LEARNING EXPERIMENTS ON CIFAR10, CIFAR100, AND SVHN
The training losses are discussed in Section 2.3. In addition to the cross-entropy loss, the recontruction loss, and the RPN regularization, in order to further improve the performance of NRM, we introduce two new losses for training the model. Those two new losses are from our derivation of batch normalization using the NRM framework and from applying variational inference on the NRM. More details on these new training losses can be found in Appendix A.4. We compare NRM with state-ofthe-art methods on semi-supervised object classification tasks which use consistency regularization, such as the  model (Laine & Aila, 2017), the Temporal Ensembling (Laine & Aila, 2017), the Mean Teacher (Tarvainen & Valpola, 2017), the Virtual Adversarial Training (VAT), and the Ladder Network (Rasmus et al., 2015). We also compare NRM with methods that do not use consistency regularization including the improved GAN (Salimans et al., 2016) and the Adversarially Learned Inference (ALI) (Dumoulin et al., 2017).
All networks were trained using Adam with learning rate of 0.001 for the first 20 epochs. Adam momentum parameters were set to beta1 = 0.9 and beta2 = 0.999. Then we used SGD with decayed learning rate to train the networks for another 380 epochs. The starting learning rate for SGD is 0.15 and the end learning rate at epoch 400 is 0.0001. We use batch size 128. Let the weights for the cross-entropy loss, the reconsruction loss, the KL divergence loss, the moment matching loss, and the RPN regularization be CE, RC , KL, MM , and P N , respectively. In our training, CE = 1.0, RC = 0.5, KL = 0.5, MM = 0.5, and P N = 1.0. For Max-Min cross-entropy, we use max = min = 0.5.
.17.2 SUPERVISED LEARNING EXPERIMENTS WITH MAX-MIN CROSS ENTROPY
Training on CIFAR10 We use the 26 2 x 96d "Shake-Shake-Image" ResNet in Gastaldi (2017) with the Cutout data augmentation suggested in DeVries & Taylor (2017) as our baseline. We implement the Max-Min cross-entropy on top of this baseline and turn it into a Max-Min network. In addition to Cutout data augmentation, standard translation and flipping data augmentation is applied on the 32 x 32 x 3 input image. Training procedure are the same as in (Gastaldi, 2017). In particular, the models were trained for 1800 epochs. The learning rate is initialized at 0.2 and is annealed using a Cosine

43

Under review as a conference paper at ICLR 2019

function without restart (see (Loshchilov & Hutter, 2016)). We train our models on 1 GPU with a mini-batch size of 128.
Training on CIFAR10 We use the Squeeze-and-Excitation ResNeXt-50 as in Hu et al. (2018) as our baseline. As with CIFAR10, we implement the Max-Min cross-entropy for the baseline and turn it into a Max-Min network. During training, we follow standard practice and perform data augmentation with random-size cropping (Szegedy et al., 2015) to 224 x 224 x 3 pixels. We train the network with the Nesterov accelerated SGD for 125 epochs. The intial learning rate is 0.1 with momentum 0.9. We divide the learning rate by 10 at epoch 30, 60, 90, 95, 110, 120. Our network is trained on 8 GPUs with batch size of 32.
.17.3 SEMI-SUPERVISED TRAINING ON MNIST WITH 50K LABELED TO GET THE TRAINED MODEL FOR GENERATING RECONSTRUCTED IMAGE IN FIGURE 2
The architecture of the baseline CNN we use is given in the Table 7. We use the same training procedure as in Section .17.1
Table 7: The network architecture used in our MNIST semi-supervised training.

NAME
input conv1 pool1 conv2a conv2b pool2 conv3 pool3 dense output

DESCRIPTION
28 × 28 image 32 filters, 5 × 5, pad = 'full', ReLU Maxpool 2 × 2 pixels 64 filters, 3 × 3, pad = 'valid', ReLU 64 filters, 3 × 3, pad = 'full', ReLU Maxpool 2 × 2 pixels 128 filters, 3 × 3, pad = 'valid', ReLU Global average pool (6 × 6  1×1 pixels) Fully connected 128  10
Softmax

44

