Under review as a conference paper at ICLR 2019
IDENTIFYING GENERALIZATION PROPERTIES IN NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian. We connect model generalization with the local property of a solution under the PAC-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order "smoothness" terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly.
1 INTRODUCTION
Deep models have proven to work well in applications such as computer vision (Krizhevsky et al., 2012) (He et al., 2014) (Karpathy et al., 2014), speech recognition (Mohamed et al., 2012) (Hinton et al., 2012), and natural language processing (Socher et al., 2013) (Graves, 2013) (McCann et al., 2018). Many deep models have millions of parameters, which is more than the number of training samples, but the models still generalize well (Huang et al., 2017).
On the other hand, classical learning theory suggests the model generalization capability is closely related to the "complexity" of the hypothesis space. This seems to be a contradiction to the empirical observations that over-parameterized models generalize well on the test data. Indeed, even if the hypothesis space is complex, the final solution learned from a given training set may still be simple. This suggests the generalization capability of the model is also related to the property of the solution.
Keskar et al. (2016) and Chaudhari et al. (2016) empirically observe that the generalization ability of a model is related to the spectrum of the Hessian matrix 2L(w) evaluated at the solution, and large eigenvalues of the 2L(w) often leads to poor model generalization. Also, (Keskar et al., 2016), (Chaudhari et al., 2016) and (Novak et al., 2018b) introduce several different metrics to measure the "sharpness" of the solution, and demonstrate the connection between the sharpness metric and the generalization empirically. Dinh et al. (2017) later points out that most of the Hessian-based sharpness measures are problematic and cannot be applied directly to explain generalization. In particular, they show that the geometry of the parameters in RELU-MLP can be modified drastically by re-parameterization.
Another line of work originates from Bayesian analysis. Mackay (1995) first introduced Taylor expansion to approximate the (log) posterior, and considered the second-order term, characterized by the Hessian, as a way of evaluating the model simplicity, or "Occam factor". Recently Smith & Le (2018) use the Occam factor to penalize sharp minima, and determine the optimal batch size. Germain et al. (2016) connect the PAC-Bayes bound and the Bayesian marginal likelihood when the loss is (bounded) negative log-likelihood, which leads to an alternative perspective on Occam's razor. (Langford & Caruana, 2001), and more recently, (Harvey et al., 2017) (Neyshabur et al., 2017a) (Neyshabur et al., 2017b) use PAC-Bayes bound to analysis the generalization behavior of the deep models.
Since the PAC-Bayes bound holds uniformly for all "posteriors", it also holds for some particular "posterior", for example, the solution parameter perturbed with noise. This provides a natural way to incorporate the local property of the solution into the generalization analysis. In particular, Neyshabur et al. (2017a) suggests to use the difference between the perturbed loss and the empirical loss as the sharpness metric. Dziugaite & Roy (2017) tries to optimize the PAC-Bayes bound
1

Under review as a conference paper at ICLR 2019

(a) Loss landscape. The color on the loss surface shows the pacGen scores. The color on the bottom plane shows an approximated generalization bound.

(b) Sample distribution

(c) Predicted labels by the sharp minimum

(d) Predicted labels by the flat minimum

Figure 1: Loss Landscape and Predicted Labels of a 5-layer MLP with 2 parameters. The sharp
minimum, even though it approximates the true label better, has some complex structures in its predicted labels, while the flat minimum seems to produce a simpler classification boundary.1

instead for a better model generalization. Still some fundamental questions remain unanswered. In particular we are interested in the following question:
How is model generalization related to local "smoothness" of a solution?
In this paper we try to answer the question from the PAC-Bayes perspective. Under mild assumptions on the Hessian of the loss function, we prove the generalization error of the model is related to this Hessian, the Lipschitz constant of the Hessian, the scales of the parameters, as well as the number of training samples. The analysis also gives rise to a new metric for generalization. Based on this, we can approximately select an optimal perturbation level to aid generalization which interestingly turns out to be related to Hessian as well. Inspired by this observation, we propose a perturbation based algorithm that makes use of the estimation of the Hessian to improve model generalization.
2 PAC-BAYES AND MODEL GENERALIZATION
We consider the supervised learning in PAC-Bayes scenario (Mcallester, 2003) (McAllester, 1998) (McAllester, 1999) (Langford & Shawe-Taylor, 2002). Suppose we have a labeled data set S = {si = (xi, yi) | i  {1, . . . , n}, xi  Rd, yi  {0, 1}k}, where (xi, yi) are sampled i.i.d. from a distribution xi, yi  Ds. The PAC-Bayes paradigm assumes probability measures over the function class F : X  Y. In particular, it assumes a "posterior" distribution Df as well as a "prior" distribution f over the function class F. We are interested in minimizing the expected loss, in terms of both the random
1The variables from different layers are shared so that the model only has two free parameters w1 and w2. The bound in (a) is approximated with  = 39 using inequality (8)
2

Under review as a conference paper at ICLR 2019

draw of samples as well as the random draw of functions:

L(Df , Ds) = EfDf Ex,yDs l(f, x, y).
Correspondingly, the empirical loss in the PAC-Bayes paradigm is the expected loss over the draw of functions from the posterior:

L^(Df

,

S)

=

Ef Df

1 n

n

l(f, xi, yi)

i=1

(1)

PAC-Bayes theory suggests the gap between the expected loss and the empirical loss is bounded by a term that is related to the KL divergence between Df and f (McAllester, 1999) (Langford & Shawe-Taylor, 2002). In particular, if the function f is parameterized as f (w) with w  W, when Dw is perturbed around any w, we have the following PAC-Bayes bound (Seldin et al., 2012) (Seldin et al., 2011) (Neyshabur et al., 2017a) (Neyshabur et al., 2017b):
Theorem 1 (PAC-Bayes-Hoeffding Perturbation). Let l(f, x, y)  [0, 1], and  be any fixed distribution over the parameters W. For any  > 0 and  > 0, with probability at least 1 -  over the draw of n samples, for any w and any random perturbation u,

Eu[L(w + u)]



Eu[L^(w + u)] +

KL(w + u||) + log 

1 

+

 2n

(2)

One may further optimize  to get a bound that scales approximately as Eu[L(w + u)] Eu[L^(w +

u)] + 2

K L(w+u|| )+log

1 

2n

(Seldin et al., 2011).2

A nice property of the perturbation bound (2) is

it connects the generalization with the local properties around the solution w through some pertur-

bation u around w. In particular, suppose L^(w) is a local optimum, when the perturbation level of

u is small, Eu[L^(w + u)] tends to be small, but KL(w + u|) may be large since the posterior is too "focused" on a small neighboring area around w, and vice versa. As a consequence, we may

need to search for an "optimal" perturbation level for u so that the bound is minimized.

3 MAIN RESULT

While some researchers have already discovered empirically the generalization ability of the models
is related to the second order information around the local optima, to the best of our knowledge there is no work on how to connect the Hessian matrix 2L^(w) with the model generalization rigorously.
In this section we introduce the local smoothness assumption, as well as our main theorem.

It may be unrealistic to assume global smoothness properties for the deep models. Usually the assumptions only hold in a small local neighborhood N eigh(w) around a reference point w. In this draft we define the neighborhood set as
N eigh(w) = {w | |wi - wi|  i i}
where i  R+ is the "radius" of the i-th coordinate. In our draft we focus on a particular type of radius i(w) = |wi| + , but our argument holds for other types of radius, too. In order to get a control of the deviation of the optimal solution we need to assume in N eigh, (w), the empirical loss function L^ in (1) is Hessian Lipschitz, which is defined as:
Definition 1 (Hessian Lipschitz). A twice differentiable function f (·) is -Hessian Lipschitz if:

w1, w2, 2f (w1) - 2f (w2)   w1 - w2 ,

(3)

where · is the operator norm.
The Hessian Lipschitz condition has been used in the numeric optimization community to model the smoothness of the second-order gradients (Nesterov & Polyak, 2006) (Carmon et al., 2018) (Jin et al., 2018). In the rest of the draft we always assume the following:
2Since  cannot depend on the data, one has to build a grid and use the union bound.

3

Under review as a conference paper at ICLR 2019

Assumption 1. In N eigh(w) the empirical loss L^(w) defined in (1) is convex, and -Hessian Lipschitz.

For the uniform perturbation, the following theorem holds:
Theorem 2. Suppose the loss function l(f, x, y)  [0, 1], and model weights are bounded |wi| + i(w)  i i. With probability at least 1 -  over the draw of n samples, for any w  Rm such that assumption 1 holds

 Eu[L(w + u)]  L^(w) + O 

m+

i log

i i

+

log

1 





n

where ui  U (-i, i) are i.i.d. uniformly distributed random variables, and

i(w, , ) = min

mn(i2,i

L^(w

1 )/3 +

m1/2i(w

)/9)

,

i

(w)

(4)

Theorem 2 says if we choose the perturbation levels carefully, the expected loss of a uniformly perturbed model is controlled. The bound is related to the diagonal element of Hessian (logarithmic), the Lipschitz constant  of the Hessian (logarithmic), the neighborhood scales characterized by  (logarithmic), the number of parameters m, and the number of samples n. Also roughly the
perturbation level is inversely related to 2i,iL^, suggesting the model be perturbed more along the coordinates that are "flat".3
Similar argument can be made on the truncated Gaussian perturbation, which is presented in Appendix A. In the next section we walk through some intuitions of our arguments.

4 CONNECTING GENERALIZATION AND HESSIAN

Suppose the empirical loss function L^(w) satisfies the local Hessian Lipschitz condition, then by Lemma 1 in (Nesterov & Polyak, 2006), the perturbation of the function around a fixed point can be bounded by terms up to the third-order,

L^(w + u)  L^(w) + L^(w)T u + 1 uT 2L^(w)u + 2

1 
6

u

3

for

w + u  N eigh(w)

(5)

For perturbations with zero expectation, i.e., E[u] = 0, the linear term in (5), Eu[2L^(w)T u] = 0.

Because the perturbation ui for different parameters are independent, the second order term can also

be

simplified,

since

Eu

[

1 2

uT

2L^(w)u]

=

1 2

i 2i,iL^(w)E[ui2].

Considering (2),(5) and assumption 1, it is straight-forward to see the bound below holds with probability at least 1 - 

Eu[L(w

+

u)]



L^(w)

+

1 2

2i,iL^(w)E[ui2] +

 6 E[

u

3] +

KL(w + u||) + log 

1 

+

 2n

i

(6)

Suppose ui  U (-i, i), and i  i(w) i. That is, the "posterior" distribution of the model
parameters are uniform distribution, and the distribution supports vary for different parameters. We also assume the perturbed parameters are bounded, i.e., |wi| + i(w)  i i.4 If we choose the prior  to be ui  U (-i, i), and then KL(w + u||) = i log(i/i).

The third order term in (6) is bounded by

 6 E[

u

3] 

m1/2 6 E[

u

33] 

m1/2 6

i(w)E[ui2]

=

m1/2 18

i(w)i2,

ii

3Unfortunately the bound in theorem 2 does not explain the over-parameterization phenomenon since when m n the right hand side explodes.
4One may also assume the same  for all parameters for a simpler argument. The proof procedure goes
through in a similar way.

4

Under review as a conference paper at ICLR 2019

where we use the inequality

u

2



m1 6

u 3 and m is the number of parameters. Pluging in (6),

we get

Eu[L(w

+

u)]



L^(w)

+

1 6

i2,iL(w)i2

+

m1/2 18

i(w)i2 +

ii

i log

i i

+

log

1 

+



 2n

(7)

Solve for  that minimizes the right hand side, and we have the following lemma:

Lemma 3. Suppose the loss function l(f, x, y)  [0, 1], and model weights are bounded |wi| + i(w)  i i. Given any  > 0 and  > 0, with probability at least 1 -  over the draw of n samples, for any w  Rm such that assumption 1 holds,

m/2 + Eu[L(w + u)]  L^(w) +

i log

i i

+ log

1 

+



 2n

(8)

where ui  U (-i, i) are i.i.d. uniformly perturbed random variables, and

i(w, , ) = min

(2i,i

L(w)/3

1 + m1/2

i

(w)/9))

,

i(w

)

.

(9)

In our experiment, we simply treat  as a hyper-parameter. Other other hand, one may further build a weighted grid over  and optimize for the best  (Seldin et al., 2011). That leads to Theorem 2. Details of the proof are presented in the Appendix B and C.

5 ON THE RE-PARAMETERIZATION OF RELU-MLP
Dinh et al. (2017) points out the spectrum of 2L^ itself is not enough to determine the generalization power. In particular, for a multi-layer perceptron with RELU as the activation function, one may re-parameterize the model and scale the Hessian spectrum arbitrarily without affecting the model prediction and generalization when cross entropy (negative log likelihood) is used as the loss and w is the "true" parameter of the sample distribution.
In general our bound does not assume the loss to be the cross entropy. Also we do not assume the model is RELU-MLP. As a result we would not expect our bound stays exactly the same during the re-parameterization. On the other hand, the optimal perturbation levels in our bound scales inversely when the parameters scale, so the bound only changes approximately with a speed of logarithmic factor. According to Lemma (3), if we use the optimal  on the right hand side of the bound, 2L^(w), , and w are all behind the logarithmic function. As a consequence, for RELU-MLP, if we do the re-parameterization trick, the change of the bound is small.

In the next two sections we introduce some heuristic-based approximations enlightened by the bound, as well as some interesting empirical observations.

6 AN APPROXIMATE GENERALIZATION METRIC

Assuming L^(w) is locally convex around w, so that i2,iL^(w)  0 for all i. If we look at Lemma

3, for fixed m and n, the only relevant term is

i

log

i i

.

Replacing the optimal , and using

|wi| + i(w) to approximate i, we come up with PAC-Bayes based Generalization metric, called

pacGen,5

(L^, w) = log (|wi| + i(w)) max
i

i2,i L^ (w )

+

 (w) mi(w),

1 i(w)

.

ever5yEvpeonintths.ouWghhewn ea2is,isLu^m(wet)he+loc(awl c)onvmexiity(win

our )<

metric, in application 0 we simply treat it as

we 0.

may

calculate

the

metric

on

5

Under review as a conference paper at ICLR 2019

(a) Test Loss - Train Loss (MNIST)

(b)  (MNIST)

Figure 2: Generalization gap and  as a function of epochs on MNIST for different batch sizes. SGD is used as the optimizer, and the learning rate is set as 0.1 for all configurations. As the batch size grows, , (L^, w) gets larger. The trend is consistent with the true gap of losses.

(a) Test Loss - Train Loss (CIFAR-10)

(b)  (CIFAR-10)

Figure 3: Generalization gap and  as a function of epochs on CIFAR-10 for different batch sizes. SGD is used as the optimizer, and the learning rate is set as 0.01 for all configurations.

A self-explained toy example is displayed in Figure 1. To calculate the metric on real-world data we need to estimate the diagonal elements of the Hessian 2L^ as well as the Lipschitz constant  of the Hessian. For efficiency concern we follow Adam (Kingma & Ba, 2014) and approximate 2i,iL^ by (L^[i])2. Also we use the exponential smoothing technique with  = 0.999 as in (Kingma & Ba,
2014).

To estimate , we first estimate the Hessian of a randomly perturbed model 2L^(w + u), and then

approximate



by



=

maxi

.|i2 L(w+ui )-i2 L(w)|
|ui |

For

the

neighborhood

radius



we

use



=

0.1

and = 0.1 for all the experiments in this section.

We used the same model without dropout from the PyTorch example 6. Fixing the learning rate as
0.1, we vary the batch size for training. The gap between the test loss and the training loss, and the metric (L^, w) are plotted in Figure 2. We had the same observation as in (Keskar et al., 2016) that as the batch size grows, the gap between the test loss and the training loss tends to get larger. Our proposed metric (L^, w) also shows the exact same trend. Note we do not use LR annealing heuristics as in (Goyal et al., 2017) which enables large batch training.

Similarly we also carry out experiment by fixing the training batch size as 256, and varying the learning rate. Figure 4 shows generalization gap and (L^, w) as a function of epochs. It is observed that as the learning rate decreases, the gap between the test loss and the training loss increases. And the proposed metric (L^, w) shows similar trend compared to the actual generalization gap. Similar trends can be observed if we run the same model on CIFAR-10 (Krizhevsky et al.) as shown in
Figure 3 and Figure 5.

7 A PERTURBED OPTIMIZATION ALGORITHM
Adding noise to the model for better generalization has proven successful both empirically and theoretically (Zhu et al., 2018) (Hoffer et al., 2017) (Jastrze¸bski et al., 2017) (Dziugaite & Roy, 2017) (Novak et al., 2018a). Instead of only minimizing the empirical loss, (Langford & Caruana,
6https://github.com/pytorch/examples/tree/master/mnist

6

Under review as a conference paper at ICLR 2019

(a) Test Loss - Train Loss (MNIST)

(b)  (MNIST)

Figure 4: Generalization gap and  as a function of epochs on MNIST for different learning rates. SGD is used as the optimizer, and the batch size is set as 256 for all configurations. As the learning rate shrinks, (L^, w) gets larger. The trend is consistent with the true gap of losses.

(a) Test Loss - Train Loss (CIFAR-10)

(b)  (CIFAR-10)

Figure 5: Generalization gap and  as a function of epochs on CIFAR-10 for different learning rates. SGD is used as the optimizer, and the batch size is set as 256 for all configurations.

2001) and (Dziugaite & Roy, 2017) assume different perturbation levels on different parameters, and minimize the generalization bound led by PAC-Bayes for better model generalization. However how to integrate the smoothness property of the local optima is not clear.
The right hand side of (2) has Eu[L^(w + u)]. This suggests rather than minimizing the empirical loss L^(w), we should optimize the perturbed empirical loss Eu[L^(w + u)] instead for a better model generalization power.
We introduce a systematic way to perturb the model weights based on the PAC-Bayes bound. Again we use the same exponential smoothing technique as in Adam (Kingma & Ba, 2014) to estimate the Hessian 2L^. The details of the algorithm is presented in Algorithm 1, where we treat  as a hyper-parameter.
Even though in theoretical analysis Eu[L^ · u] = 0, in applications, L^ · u won't be zero especially when we only implement 1 trial of perturbation. On the other hand, if the gradient L^ is close to zero, then the first order term can be ignored. As a consequence, in Algorithm 1 we only perturb the parameters that have small gradients whose absolute value is below 2. For efficiency issues we used a per-parameter i capturing the variation of the diagonal element of Hessian. Also we decrease the perturbation level with a log factor as the epoch increases.
We compare the perturbed algorithm against the original optimization method on CIFAR-10, CIFAR-100 (Krizhevsky et al.), and Tiny ImageNet7. The results are shown in Figure 6. We use the Wide-ResNet (Zagoruyko & Komodakis, 2016) as the prediction model.8 The depth of the chosen model is 58, and the widen-factor is set as 3. The dropout layers are turned off. For CIFAR-10 and CIFAR-100, we use Adam with a learning rate of 10-4, and the batch size is 128. For the perturbation parameters we use  = 0.01,  = 10, and =1e-5. For Tiny ImageNet, we use SGD with learning rate 10-2, and the batch size is 156. For the perturbed SGD we set  = 100,  = 1,
7https://tiny-imagenet.herokuapp.com/ 8https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/ wide_resnet.py
7

Under review as a conference paper at ICLR 2019

Algorithm 1 Perturbed OPT

Require: ,  = 0.1, 1 = 0.999, 2 = 0.1, =1e-5. 1: Initialization: i  0 for all i. t  0, h0  0

2: for epoch in 1, . . . , N do

3: for minibatch in one epoch do

4: for all i do

5: if t > 0 then

6:

[i]



|ht [i]-ht-1 [i]| wt -wt-1

7:

[i]



 log(1+epoch)

|wt-1[i]|

+

8:

i  min

1 , [i]
log(1+epoch) (ht[i]+[i]·[i])

· 1|gt[i]|<2

9: ut[i]  U (-i, i)(sample a set of perturbations)
10: gt+1  wL^t(wt + ut) (get stochastic gradients w.r.t. perturbed loss) 11: ht+1  1ht + (1 - 1)gt2+1 (update second moment estimate) 12: wt+1  OPT(wt) (update w using off-the-shell algorithms) 13: t  t + 1

(a) CIFAR-10

(b) CIFAR-100

(c) Tiny ImageNet

Figure 6: Training and testing accuracy as a function of epochs on CIFAR-10, CIFAR-100 and Tiny ImageNet. For CIFAR, Adam is used as the optimizer, and the learning rate is set as 10-4. For the Tiny ImageNet, SGD is used as the optimizer, and the learning rate is set as 10-2.

and =1e-5. Also we use the validation set as the test set for the Tiny ImageNet. We observe the effect with perturbation appears similar to regularization. With the perturbation, the accuracy on the training set tends to decrease, but the test on the validation set increases.
8 CONCLUSION
We connect the smoothness of the solution with the model generalization in the PAC-Bayes framework. We prove that the generalization power of a model is related to the Hessian and the smoothness of the solution, the scales of the parameters, as well as the number of training samples. In particular, we prove that the best perturbation level scales roughly as the inverse of the square root of the Hessian, which mostly cancels out scaling effect in the re-parameterization suggested by (Dinh et al., 2017). To the best of our knowledge, this is the first work that integrate Hessian in the model generalization bound rigorously. It also roughly explains the effect of re-parameterization over the generalization. Based on our generalization bound, we propose a new metric to test the model generalization and a new perturbation algorithm that adjusts the perturbation levels according to the Hessian. Finally, we empirically demonstrate the effect of our algorithm is similar to a regularizer in its ability to attain better performance on unseen data.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex optimization. SIAM Journal on Optimization, 28(2):1751­1772, 2018. URL https://doi. org/10.1137/17M1114296.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. CoRR, abs/1611.01838, 2016. URL http://arxiv.org/ abs/1611.01838.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp Minima Can Generalize For Deep Nets. 2017. ISSN 1938-7228. URL http://arxiv.org/abs/1703.04933.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data. 2017. URL http://arxiv.org/abs/1703.11008.
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 1884­1892. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/ 6569-pac-bayesian-theory-meets-bayesian-inference.pdf.
Priya Goyal, Piotr Dolla´r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706. 02677.
Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL http://arxiv.org/abs/1308.0850.
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks. In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pp. 1064­1068, Amsterdam, Netherlands, 07­10 Jul 2017. PMLR. URL http://proceedings. mlr.press/v65/harvey17a.html.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729, 2014. URL http: //arxiv.org/abs/1406.4729.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition. Signal Processing Magazine, 2012.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. 2017. ISSN 10495258. URL http://arxiv.org/abs/1705.08741.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.
Stanislaw Jastrze¸bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three Factors Influencing Minima in SGD. pp. 1­21, 2017. URL http://arxiv.org/abs/1711.04623.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. In Se´bastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp. 1042­1085. PMLR, 06­09 Jul 2018.
9

Under review as a conference paper at ICLR 2019
Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li FeiFei. Large-scale video classification with convolutional neural networks. pp. 1725­1732, 2014. doi: 10.1109/CVPR.2014.223. URL https://doi.org/10.1109/CVPR.2014.223.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR, abs/1609.04836, 2016. URL http://arxiv.org/abs/1609.04836.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://dblp.uni-trier.de/db/journals/corr/ corr1412.html#KingmaB14.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/~kriz/cifar.html.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. pp. 1097­1105, 2012. URL http://dl.acm.org/citation. cfm?id=2999134.2999257.
John Langford and Rich Caruana. (Not) Bounding the True Error. Advances in Neural . . . , 2001. ISSN 10495258. URL http://machinelearning.wustl.edu/mlpapers/ paper{_}files/nips02-AA54.pdf.
John Langford and John Shawe-Taylor. Pac-bayes & margins. In Proceedings of the 15th International Conference on Neural Information Processing Systems, NIPS'02, pp. 439­446, Cambridge, MA, USA, 2002. MIT Press. URL http://dl.acm.org/citation.cfm?id= 2968618.2968674.
David J C Mackay. Probable networks and plausible predictions - a review of practical bayesian methods for supervised neural networks. Network: Computation in Neural Systems, 6(3):469­ 505, 1995.
David Mcallester. Simplified pac-bayesian margin bounds. In In COLT, pp. 203­215, 2003.
David A. McAllester. Some pac-bayesian theorems. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT' 98, pp. 230­234, New York, NY, USA, 1998. ACM. ISBN 1-58113-057-0. doi: 10.1145/279943.279989. URL http://doi.acm.org/ 10.1145/279943.279989.
David A. McAllester. Pac-bayesian model averaging. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT '99, pp. 164­170, New York, NY, USA, 1999. ACM. ISBN 1-58113-167-4. doi: 10.1145/307400.307435. URL http://doi.acm.org/ 10.1145/307400.307435.
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. 2018. URL https://arxiv.org/ abs/1806.08730. cite arxiv:1806.08730.
A. Mohamed, G. E. Dahl, and G. Hinton. Acoustic modeling using deep belief networks. Trans. Audio, Speech and Lang. Proc., 20(1):14­22, January 2012. ISSN 1558-7916. doi: 10.1109/ TASL.2011.2109382. URL https://doi.org/10.1109/TASL.2011.2109382.
Yurii Nesterov and B. T. Polyak. Cubic regularization of newton method and its global performance. Math. Program., 108(1):177­205, August 2006. ISSN 0025-5610. doi: 10.1007/ s10107-006-0706-8. URL https://doi.org/10.1007/s10107-006-0706-8.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring Generalization in Deep Learning. (Nips), 2017a. ISSN 10495258. URL http://arxiv.org/ abs/1706.08947.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks. (2017):1­9, 2017b. URL http: //arxiv.org/abs/1707.09564.
10

Under review as a conference paper at ICLR 2019
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and Generalization in Neural Networks: an Empirical Study. pp. 1­21, 2018a. URL http://arxiv.org/abs/1802.08760.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. In International Conference on Learning Representations, 2018b. URL https://openreview.net/forum?id= HJC2SzZCW.
Y. Seldin, F. Laviolette, and J. Shawe-Taylor. Pac-bayesian analysis of supervised, unsupervised, and reinforcement learning, 2012.
Yevgeny Seldin, Franc¸ois Laviolette, Nicolo` Cesa-Bianchi, John Shawe-Taylor, and Peter Auer. Pac-bayesian inequalities for martingales. CoRR, abs/1110.6886, 2011. URL http://arxiv. org/abs/1110.6886.
Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations, 2018. URL https:// openreview.net/forum?id=BJij4yg0Z.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. pp. 1631­1642, October 2013. URL http://www.aclweb.org/anthology/ D13-1170.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016. URL http://arxiv.org/abs/1605.07146.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent: Its behavior of escaping from minima and regularization effects zhanxing. pp. 1­15, 2018. URL http://arxiv.org/abs/1803.00195.
11

Under review as a conference paper at ICLR 2019

A TRUNCATED GAUSSIAN

Because the Gaussian distribution is not bounded but the inequality (5) requires bounded perturbation, we first truncate the distribution. The procedure of truncation is similar to the proof in (Neyshabur et al., 2017b) and (Mcallester, 2003).

Let ui  N (0, i2). Denote the truncated Gaussian as Ni (0, i2). If u~i  Ni (0, i2) then

1 Pi (u~) = Zi

p(ui) if |ui| < i(w) 0 o.w.

(10)

Now let's look at the event

E = {u | |ui| < i(w)  i}

(11)

If

i

i

<

, i(w)

2erf

-1

(

1 2m

)

by

union

bound

P(E)



1/2.

Here erf-1 is the inverse Gaussian error

function

defined

as

erf (x)

=

2 

x 0

e-t2 dt,

and

m

is

the

number

of

parameters.

Following

a

similar

procedure as in the proof of Lemma 1 in (Neyshabur et al., 2017b),

KL(w + u~||)  2(KL(w + u||) + 1)

(12)

Suppose the coefficients are bounded such that i wi2   , where  is a constant. Choose the prior  as N (0,  I), and we have

K L(w

+

u||)



1 (m

log



-

2

log

i2

-

m

+

1 

i2 + 1)

ii

(13)

Notice that after the truncation the variance only becomes smaller, so the bound of (6) for the trun-

cated Gaussian becomes

Eu[L(w

+

u~)]



L^(w)+

1 2

2i,iL(w)i2

+

m1/2 6

i(w)i2

ii

m log  - +

i

log

i2

-

m

+

1 

i

i2

+

1

+

2

log

1 

+



2 2n

(14)

Again when L^(w) is convex around w such that 2L^(w)  0, solve for the best i and we get the following lemma:

Lemma For any

4. Suppose the loss function l(f,  > 0 and , with probability at

x, y) least

 [0, 1], and model weights are bounded 1 -  over the draw of n samples, for any

wi w i2Rm.

such that assumption 1 holds,

Eu[L(w + u~)]  L^(w) + m log  -

i

log

i2

+

1

+

2

log

1 

+



2 2n

(15)

where u~i  Ni (0, i) are i.i.d. random variables distributed as truncated Gaussian,

i = min

 i2,i L^ (w )

+

1

m1/2 3

i

(w

)

+

1 

,

2erif(-w1()21m )

(16)

and i2 is the i-th diagonal element in .

Again we have an extra term , which may be further optimized over a grid to get a tighter bound. In our algorithm we treat  as a hyper-parameter instead.

B PROOF OF LEMMA 3

Proof. We rewrite the inequality (7) below

Eu[L(w

+

u)]



L^(w)

+

1 6

i2,iL(w)i2

+

m1/2 18

i(w)i2 +

ii

i log

i i

+

log

1 

+



 2n

(17)

12

Under review as a conference paper at ICLR 2019

The terms related to i on the right hand side of (17) are

1 6

2i,iL(w)i2

+

m1/2 18

i(w)i2

-

log i 

(18)

Since the assumption is 2i,iL^(w)  0 for all i, 2i,iL^(w) + m1/2i(w)/3 > 0. Solving for  that minimizes the right hand side of (17), and we have

i(w, , ) = min

1 (i2,iL^(w)/3 + m1/2i(w)/9) , i(w)

(19)

The

term

1 6

i i2,iL(w)i2

+

m1/2 18

i i(w)i2 on the right hand side of (7) is monotonically

increasing w.r.t. 2, so

1 6

2i,iL(w)i2

+

m1/2 18

i(w)i2

ii


i
m =
2

1 6

2i,iL(w)

+

m1/2 18

i(w)

1 (i2,iL^(w)/3 + m1/2i(w)/9)

(20)

Combine the inequality (20), and the equation (19) with (17), and we complete the proof.

C PROOF OF THEOREM 2

Proof. Combining (4) and (7), we get

Eu[L(w

+

u)]



L^(w)

+

1 2

m +
n

i log

i i

+ log

1 

+



 2n

The following proof is similar to the proof of Theorem 6 in (Seldin et al., 2011). Note the  in Lemma (3) cannot depend on the data. In order to optimize  we need to build a grid of the form

j = ej

1 2n log
j

for j  0.

For a given value of

i

log

i i

,

we

pick

j ,

such

that

1 j = log
2

i

log

i i

log

1 j

+1

where x is the largest integer value smaller than x. Set j = 2-(j+1), and take a weighted union bound over j-s with weights 2-(j+1), and we have with probability at least 1 - ,

Eu[L(w

+

u)]



L^(w)

+

1 2

m + (1 + 1/e)
n

i log

i i

+ log

1 

+

log 2 2

2 + log

2n

+ 1i log ii

log

1 

Simplify the right hand side and we complete the proof.

13

Under review as a conference paper at ICLR 2019

D PROOF OF LEMMA 4

Proof. We first rewrite the inequality (14) below:

Eu[L(w

+

u~)]



L^(w)+

1 2

i2,iL(w)i2

+

m1/2 6

i(w)i2

ii

m log  - +

i

log

i2

-

m

+

1 

i

i2

+

1

+

2

log

1 

+



2 2n

The terms related to i on the right hand side of (14) is

1 2

i2,i

L(w

)

+

m1/2 6

i(w

)

+

1 2 

i2

-

log i2 2

(21)

Take gradients w.r.t. i, when 2i L^  0, we get the optimal i,

i = min

 2i,i L^ (w )

+

1

m1/2 3

i

(w

)

+

1 

,

2erif(-w1()21m )

Note the first term in (21) is monotonously increasing w.r.t. i, so

1 2

i2,iL(w)

+

m1/2 6

i(w)

+

1 2 

i2



1 2

i2L(w)

+

m1/2 6

i(w)

+

1 2 

1 =
2

1

i2L^(w)

+

m1/2 3

i

(w

)

+

1 

(22)

Summing over m parameters and combine (14), we complete the proof.

E A LEMMA ABOUT EIGENVALUES OF HESSIAN AND GENERALIZATION

By extrema of the Rayleigh quotient, the quadratic term on the right hand side of inequality (5) is further bounded by

uT 2L^(w)u  max(2L^(w)) u 2.

(23)

This is consistent with the empirical observations of Keskar et al. (2016) that the generalization ability of the model is related to the eigenvalues of 2L^(w). The inequality (23) still holds even
if the perturbations ui and uj are correlated. We add another lemma about correlated perturbations below.

Lemma 5. Suppose the loss function l(f, x, y)  [0, 1]. Let  be any distribution on the parameters
that is independent from the data. Given  > 0  > 0, with probability at least 1 -  over the draw of n samples, for any local optimal w such that L^(w) = 0, L^(w) satisfies the local -Hessian Lipschitz condition in N eigh(w), and any random perturbation u, s.t., |ui|  i(w) i, we
have

Eu[L(w

+

u)]



L^(w)

+

1 2 max

2L^(w)

E[ui2] +

 6 E[

u

3]

i

+

K L(w

+

u||)

+

log

1 

+



.

 2n

(24)

14

Under review as a conference paper at ICLR 2019

Proof. The proof of the Lemma 5 is straight-forward. Since L^(w) = 0, the first order term is zero at the local optimal point even if E[u] = 0. By extrema of the Rayleigh quotient, the quadratic term on the right hand side of inequality (5) is further bounded by

uT 2L^(w)u  max 2L^(w) u 2.

(25)

Due to the linearity of the expected value,

E[uT 2L^(w)u]  max 2L^(w)

E[ui2],

i

which does not assume independence among the perturbations ui and uj for i = j.

(26)

15

