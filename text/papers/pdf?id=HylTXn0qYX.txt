Under review as a conference paper at ICLR 2019
EFFICIENTLY TESTING LOCAL OPTIMALITY AND ESCAPING SADDLES FOR RELU NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks. Our algorithm receives any parameter value and returns: local minimum, secondorder stationary point, or a strict descent direction. The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2M regions, which makes analysis difficult. By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M ) (in)equality tests, and one (or a few) nonconvex QP. For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity. In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast. In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints. Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases.
1 INTRODUCTION
Empirical success of deep neural networks has sparked great interest in the theory of deep models. From an optimization viewpoint, the biggest mystery is that deep neural networks are successfully trained by gradient-based algorithms despite their nonconvexity. On the other hand, it has been known that training neural networks to global optimality is NP-hard (Blum & Rivest, 1988). It is also known that even checking local optimality of nonconvex problems can be NP-hard (Murty & Kabadi, 1987). Bridging this gap between theory and practice is a very active area of research.
There have been many attempts to understand why optimization works well for neural networks, by studying the loss surface (Baldi & Hornik, 1989; Yu & Chen, 1995; Kawaguchi, 2016; Soudry & Carmon, 2016; Nguyen & Hein, 2017a;b; Safran & Shamir, 2017; Laurent & von Brecht, 2017; Yun et al., 2018a;b; Zhou & Liang, 2018; Wu et al., 2018; Shamir, 2018) and the role of (stochastic) gradient-based methods (Tian, 2017; Brutzkus & Globerson, 2017; Du et al., 2017). Besides nonconvexity, for ReLU networks significant additional challenges in the analysis arise due to nondifferentiability, and obtaining a precise understanding of the nondifferentiable points is still elusive.
Nondifferentiable points lie in a set of measure zero, so one may be tempted to overlook them as "non-generic." However, when studying critical points we cannot do so, as they are precisely such "non-generic" points. Laurent & von Brecht (2017) study one-hidden-layer ReLU networks with hinge loss and note that except for piecewise constant regions, local minima always occur on nonsmooth boundaries. Probably due to difficulty in analysis, there have not been other works that handle such nonsmooth points of losses and prove results that work for all points. Some theorems (Soudry & Carmon, 2016; Nguyen & Hein, 2017b) hold "almost surely"; some assume differentiability or make statements only for differentiable points (Nguyen & Hein, 2017a; Yun et al., 2018a); others analyze population risk, in which case the nondifferentiability disappears after taking expectation (Tian, 2017; Brutzkus & Globerson, 2017; Du et al., 2017; Safran & Shamir, 2017; Wu et al., 2018).
1.1 SUMMARY OF OUR RESULTS
In this paper, we take a step towards understanding nondifferentiable points of the empirical risk of one-hidden-layer ReLU(-like) networks. Specifically, we provide a theoretical algorithm that tests second-order stationarity (SOS) for any point of the loss surface. It takes an input point and returns:
1

Under review as a conference paper at ICLR 2019

(a) The point is a local minimum; or
(b) The point is a second-order stationary point (SOSP); or
(c) A descent direction in which the function value strictly decreases.
Therefore, we can test whether a given point is a SOSP. If not, the test extracts a guaranteed direction of descent that helps continue minimization; or even escape from saddle points! What makes it nontrivial is that unlike Hessian based methods for escaping saddles, we do not have differentiability.
The key computational challenge in constructing our algorithm for nondifferentiable points is posed by data points that lie on the "boundary" of a hidden neuron. Since each such data point bisects the parameter space into two halfspaces with different "slopes" of the loss surface, one runs into nondifferentiability. If there are M such boundary data points, then in the worst case the parameter space divides into 2M regions, so naively testing each region will be very inefficient. In our algorithm, we overcome this issue by a clever use of polyhedral geometry. Another challenge comes from the second-order test, which involves solving nonconvex QPs. Although QP is NP-hard in general (Pardalos & Vavasis, 1991), we prove that the QPs in our algorithm are still solved efficiently in most cases. We further describe the challenges and key ideas in Section 2.1.
Remarks. Many practitioners of deep learning rely on first-order methods, without good termination criteria for the optimization problem. Our algorithm proposes a tool for improvement: with a proper numerical implementation (although we leave numerical implementation to future work), it can test whether a given point is a SOSP, or extract a descent direction using second-order information. One can imagine running a first-order method until it "gets stuck," then using our algorithm to test SOS or escape from the saddle. This idea of mixing first and second-order methods has been explored in differentiable problems (Carmon et al., 2016; Reddi et al., 2017; Mokhtari et al., 2018).
 Notation. For a vector v, [v]i denotes its i-th component, and v H := vT Hv denotes a seminorm where H is a positive semidefinite matrix. Given a matrix A, we let [A]i,j, [A]i,·, and [A]·,j be A's (i, j)-th entry, the i-th row, and the j-th column, respectively.

2 PROBLEM SETTING AND KEY IDEAS

We consider a one-hidden-layer neural network with input dimension dx, hidden layer width dh, and output dimension dy. We are given m pairs of data points and labels (xi, yi)im=1, where xi  Rdx and yi  Rdy . Given an input vector x, the output of the network is defined as Y (x) := W2h(W1x+ b1) + b2, where W2  Rdy×dh , b2  Rdy , W1  Rdh×dx , and b1  Rdh are the network parameters. The activation function h is "ReLU-like," meaning h(t) := max{s+x, 0} + min{s-x, 0}, where s+ > 0, s-  0 and s+ = s-. Note that ReLU and Leaky-ReLU are members of this class. In
training neural networks, we are interested in minimizing the empirical risk

R((Wj , bj )j2=1) =

m
(Y (xi), yi) =
i=1

m
(W2h(W1xi + b1) + b2, yi),
i=1

over the parameters (Wj, bj)2j=1, where (w, y) : Rdy × Rdy  R is the loss function. We make the following assumptions on the loss function and the training dataset:

Assumption 1. The loss function (w, y) is twice continuously differentiable and convex in w.

Assumption 2. No dx + 1 data points lie on the same affine hyperplane.

Assumption 1 is satisfied by many standard loss functions such as squared error loss and crossentropy loss. Assumption 2 means, if dx = 2 for example, no three data points are on the same line. Since real-world datasets contain noise, this assumption is also quite mild.

2.1 CHALLENGES AND KEY IDEAS
In this section, we explain the difficulties at nondifferentiable points and ideas on overcoming them. Our algorithm is built from first principles, rather than advanced tools from nonsmooth analysis.

Bisection by boundary data points. Since the activation function h is nondifferentiable at 0, the
behavior of data points at the "boundary" is decisive. Consider a simple example dh = 1, so W1 is a row vector. If W1xi+b1 = 0, then the sign of (W1+1)xi+(b1+1) for any small perturbations 1 and 1 stays invariant. In contrast, when there is a point xi on the "boundary," i.e., W1xi + b1 = 0,

2

Under review as a conference paper at ICLR 2019

then the slope depends on the direction of perturbation, leading to nondifferentiability. We refer to such points as boundary data points. When 1xi + b1  0,
h((W1 + 1)xi + (b1 + 1)) = h(1xi + 1) = s+(1xi + 1) = h(W1xi + b1) + s+(1xi + 1),
and similarly, the slope is s- for 1xi + b1  0. This means that the "gradient" (as well as higher order derivatives) of R depends on direction of (1, 1).

Thus, every boundary data point xi defines a hyperplane through the origin, and bisects the parameter space into two halfspaces. The situation is even worse if we have M boundary data points: they lead to a worst case of 2M regions. Does it mean that we need to test all 2M regions separately? We
show that there is a way to remedy this problem, but before that, we first describe how to test local
minimality or stationarity for each region.

Second-order local optimality conditions. We can expand R((Wj + j, bj + j)2j=1) and obtain the following Taylor-like expansion for small enough perturbations (see Lemma 2 for details)

R(z + ) = R(z) + g(z, )T  +

1 2

T

H

(z

,

)

+

o(



2),

(1)

where z is a vectorized version of all parameters (Wj, bj)2j=1 and  is the corresponding vector of perturbations. Notice now that in (1), at nondifferentiable points the usual Taylor expansion does
not exist, but the corresponding "gradient" g(·) and "Hessian" H(·) now depend on the direction of perturbation . Also, the space of  is divided into O(2M ) regions, and g(z, ) and H(z, ) are piecewise-constant functions of , constant inside each region. One could view this problem as 2M
constrained optimization problems and try to solve for KKT conditions at z; however, we provide an approach that is developed from first principles and solves all 2M problems efficiently.

Given this expansion (1) and the observation that derivatives stay invariant with respect to scaling of , one can note that (a) g(z, )T   0 for all , and (b) T H(z, )  0 for all  such that g(z, )T  = 0 are necessary conditions for local optimality of z, thus z is a "SOSP" (see Definition 2.2). The conditions become sufficient if (b) is replaced with T H(z, ) > 0 for all  = 0 such that g(z, )T  = 0. In fact, this is a generalized version of second-order necessary (or sufficient) conditions, i.e., f = 0 and 2f 0 (or 2f 0), for differentiable f .

Efficiently testing SOSP for exponentially many regions. Motivated from the second-order expansion (1) and necessary/sufficient conditions, our algorithm consists of three steps:

(a) Testing first-order stationarity (in the Clarke sense, see Definition 2.1), (b) Testing g(z, )T   0 for all , (c) Testing T H(z, )  0 for { | g(z, )T  = 0}.

The tests are executed from Step (a) to (c). Whenever a test fails, we get a strict descent direction , and the algorithm returns  and terminates. Below, we briefly outline each step and discuss how we can efficiently perform the tests. We first check first-order stationarity because it makes Step (b) easier. Step (a) is done by solving one convex QP per each hidden node. For Step (b), we formulate linear programs (LPs) per each 2M region, so that checking whether all LPs have minimum cost of zero is equivalent to checking g(z, )T   0 for all . Here, the feasible sets of LPs are pointed polyhedral cones, whereby it suffices to check only the extreme rays of the cones. It turns out that there are only 2M extreme rays, each shared by 2M-1 cones, so testing g(z, )T   0 can be done with only O(M ) inequality/equality tests instead of solving exponentially many LPs. In Step (b), we also record the flat extreme rays, i.e., those with g(z, )T  = 0, for later use in Step (c).
In Step (c), we test if the second-order perturbation T H(·) can be negative, for directions where g(z, )T  = 0. Due to the constraint g(z, )T  = 0, the second-order test requires solving constrained nonconvex QPs. In case where there is no flat extreme ray, we need to solve only one equality constrained QP (ECQP). If there exist flat extreme rays, a few more inequality constrained QPs (ICQPs) are solved. Despite NP-hardness of general QPs (Pardalos & Vavasis, 1991), we prove that the specific form of QPs in our algorithm are still tractable in most cases. More specifically, we prove that projected gradient descent on ECQPs converges/diverges exponentially fast, and each step takes O(p2) time (p is the number of parameters). In case of ICQPs, it takes O(p3 + L32L) time to solve the QP, where L is the number of boundary data points that have flat extreme rays (L  M ). Here, we can see that if L is small enough, the ICQP can still be solved in polynomial time in p. At the end of the paper, we provide empirical evidences that the number of flat extreme rays is zero or very few, meaning that in most cases we can solve the QP efficiently.

3

Under review as a conference paper at ICLR 2019

2.2 PROBLEM-SPECIFIC NOTATION AND DEFINITION

In this section, we define a more precise notion of generalized stationary points and introduce some additional symbols that will be helpful in streamlining the description of our algorithm in Section 3. Since we are dealing with nondifferentiable points of nonconvex R, usual notions of (sub)gradients do not work anymore. Here, Clarke subdifferential is a useful generalization (Clarke et al., 2008):
Definition 2.1 (FOSP, Theorem 6.2.5 of Borwein & Lewis (2010)). Suppose that a function f (z) :   R is locally Lipschitz around the point z  , and differentiable in  \ W where W has Lebesgue measure zero. Then the Clarke differential of f at z is
zf (z) := cvxhull{limk f (zk) | zk  z, zk / W}. If 0  zf (z), we say z is a first-order stationary point (FOSP).
From the definition, we can note that Clarke subdifferential zR(z) is the convex hull of all the possible values of g(z, ) in (1). For parameters (Wj, bj)j2=1, let Wj f (z) and bj f (z) be the Clarke differential w.r.t. to Wj and bj, respectively. They are the projection of zf (z) onto the space of individual parameters. Whenever the point z is clear (e.g. our algorithm), we will omit (z) from f (z). Next, we define second-order stationary points for the empirical risk R. Notice that this generalizes the definition of SOSP for differentiable functions f : f = 0 and 2f 0. Definition 2.2 (SOSP). We call z is a second-order stationary point (SOSP) of R if (1) z is a FOSP, (2) g(z, )T   0 for all , and (3) T H(z, )  0 for all  such that g(z, )T  = 0.

Given an input data point x  Rdx , we define O(x) := h(W1x + b1) to be the output of hidden layer. We note that the notation O(·) is overloaded with the big-Oh notation, but their meaning will be clear from the context. Consider perturbing parameters (Wj, bj)2j=1 with (j, j)j2=1, then the perturbed output Y~ (x) of the network and the amount of perturbation dY (x) can be expressed as
dY (x) := Y~ (x) - Y (x) = 2O(x) + 2 + (W2 + 2)J(x)(1x + 1),
where J(x) can be thought informally as the "Jacobian" matrix of the hidden layer. The matrix J(x)  Rdh×dh is diagonal, and its k-th diagonal entry is given by

[J (x)]k,k :=

h ([W1x + b1]k) h ([1x + 1]k)

if [W1x + b1]k = 0 if [W1x + b1]k = 0,

where h is the derivative of h. We define h (0) := s+, which is okay because it is always multiplied with zero in our algorithm. For boundary data points, [J(x)]k,k depends on the direction of perturbations [1 1]k,·, as noted in Section 2.1. We additionally define dY1(x) and dY2(x) to separate the terms in dY (x) that are linear in perturbations versus quadratic in perturbations.

dY1(x) := 2O(x) + 2 + W2J (x)(1x + 1), dY2(x) := 2J (x)(1x + 1).

For simplicity of notation for the rest of the paper, we define for all i  [m] := {1, . . . , m},

x¯i := xTi 1 T  Rdx+1,  i := w (Y (xi), yi), 2 i := 2w (Y (xi), yi).
In our algorithm and its analysis, we need to give a special treatment to the boundary data points. To this end, for each node k  [dh] in the hidden layer, define boundary index set Bk as

Bk := {i  [m] | [W1x + b1]k = 0} .

The subspace spanned by vectors x¯i for in i  Bk plays an important role in our tests; so let us

define a symbol for it, as well as the cardinality of Bk and their sum:

Vk := span{x¯i | i  Bk}, Mk := |Bk|, M :=

dh
Mk .

k=1

For k  [dh], let vkT  R1×(dx+1) be the k-th row of [1 1], and uk  Rdy be the k-th column of 2. Next, we define the total number of parameters p, and vectorized perturbations   Rp:

p := dy + dydh + dh(dx + 1), T := 2T u1T · · · uTdh v1T · · · vdTh .

Also let z  Rp be vectorized parameters (Wj, bj)j2=1, packed in the same order as .

Define a matrix Ck := i/Bk h ([N (xi)]k) ix¯iT  Rdy×(dx+1). This quantity appears multiplie times and does not depend on the perturbation, so it is helpful to have a simple symbol for it.

We conclude this section by presenting one of the implications of Assumption 2 in the following lemma, which we will use later. The proof is simple, and is presented in Appendix B.1.
Lemma 1. If Assumption 2 holds, then Mk  dx and the vectors {x¯i}iBk are linearly independent.

4

Under review as a conference paper at ICLR 2019

Algorithm 1 SOSP-CHECK (Rough pseudocode)
Input: A tuple (Wj , bj )j2=1 of R(·). 1: Test if W2 R = {0dy×dh } and b2 R = {0dy }. 2: for k  [dh] do 3: if Mk > 0 then 4: Test if 0Tdx+1  [W1 b1]k,· R. 5: Test if gk(z, vk)T vk  0 for all vk via testing extreme rays v~k of polyhedral cones. 6: Store extreme rays v~k s.t. gk(z, v~k)T v~k = 0 for second-order test. 7: else 8: Test if [W1 Rb1]k,· = {0Tdx+1}. 9: end if 10: end for 11: For all 's s.t. g(z, )T  = 0, test if T H(z, )  0. 12: if  = 0 s.t. g(z, )T  = 0 and T H(z, ) = 0 then 13: return SOSP. 14: else 15: return Local Minimum. 16: end if

3 TEST ALGORITHM FOR SECOND-ORDER STATIONARITY

In this section, we present SOSP-CHECK in Algorithm 1, which takes an arbitrary tuple (Wj, bj)j2=1 of parameters as input and checks whether it is a SOSP. We first present a lemma that shows the ex-
plicit form of the perturbed empirical risk R(z +) and identify first and second-order perturbations.
The proof is deferred to Appendix B.2.

Lemma 2. For small enough perturbation ,

R(z + ) = R(z) + g(z, )T  +

1 2

T

H

(z

,

)

+

o(



2),

where g(z, ) and H(z, ) satisfy

g(z, )T  =


i

T i

dY1

(xi

)

=

i  iO(xi)T , 2 +

dh


i

i, 2

+

gk(z, vk)T vk,

k=1

T H(z, ) =


i

T i

dY2

(xi

)

+

1 2

i

dY1(xi)

2 2

,
i

and gk(z, vk)T := [W2]T·,k Ck + iBk h (x¯iT vk) ix¯Ti . Also, g(z, ) and H(z, ) are piecewise constant functions of , which are constant inside each polyhedral cone in space of .

Rough pseudocode of SOSP-CHECK is presented in Algorithm 1. As described in Section 2.1, the algorithm consists of three steps: (a) testing first-order stationarity (b) testing g(z, )T   0 for all , and (c) testing T H(z, )  0 for { | g(z, )T  = 0}. If the input point satisfies the second-
order sufficient conditions for local minimality, the algorithm decides it is a local minimum. If the
point only satisfies second-order necessary conditions, it returns SOSP. If a strict descent direction
 is found, the algorithm terminates immediately and returns . A brief description will follow, but
the full algorithm (Algorithm 2) and a full proof of correctness are deferred to Appendix A.

3.1 TESTING FIRST-ORDER STATIONARITY (LINES 1, 4, AND 8)

Line 1 of Algorithm 1 corresponds to testing if W2 R and b2 R are singletons with zero. If not, the opposite direction is a descent direction. More details are in Appendix A.1.1.

Test for W1 and b1 is more difficult because g(z, ) depends on 1 and 1 when there are boundary data points. For each k  [dh], Line 4 (if Mk > 0), and Line 8 (if Mk = 0) test if 0dTx+1 is in [W1 b1]k,· R. Note from Definition 2.1 and Lemma 2 that [W1 Rb1]k,· is the convex hull of all possible values of gk(z, vk)T . If Mk > 0, 0  [W1 Rb1]k,· can be tested by solving a convex QP:

minimize{si}iBk

[W2]T·,k(Ck +

iBk si

ix¯iT )

2 2

subject to

min{s-, s+}  si  max{s-, s+}, i  Bk.

(2)

If the solution {si }iBk does not achieve zero objective value, then we can directly return a descent direction. For details please refer to FO-SUBDIFF-ZERO-TEST (Algorithm 3) and Appendix A.1.2.

5

Under review as a conference paper at ICLR 2019

3.2 TESTING g(z, )T   0 FOR ALL  (LINES 5­6)

Linear program formulation. Lines 5­6 are about testing if gk(z, vk)T vk  0 for all directions of vk. If 0Tdx+1  [W1 b1]k,· R, with the solution {si } from QP (2) we can write gk(z, vk)T as

gk(z, vk)T = [W2]·T,k Ck + iBk h (x¯Ti vk) ix¯iT = [W2]T·,k

iBk h (x¯iT vk) - si  ix¯iT

Every i  Bk bisects Rdx+1 into two halfspaces, x¯Ti vk  0 and x¯Ti vk  0, in each of which h (x¯Ti vk) stays constant. Note that by Lemma 1, x¯i's for i  Bk are linearly independent. So, given Mk boundary data points, they divide the space Rdx+1 of vk into 2Mk polyhedral cones.

Since gk(z, vk)T is constant in each polyhedral cones, we can let i  {-1, +1} for all i  Bk, and define an LP for each {i}iBk  {-1, +1}Mk :

minimize
vk

[W2]·T,k

iBk (si - si ) ix¯iT vk

subject to vk  Vk, ix¯iT vk  0, i  Bk.

(3)

Solving these LPs and checking if the minimum value is 0 suffices to prove gk(z, vk)T vk  0 for all small enough perturbations. The constraint vk  Vk is there because any vk / Vk is also orthogonal to gk(z, vk). It is equivalent to dx + 1 - Mk linearly independent equality constraints. So, the feasible set of LP (3) has dx + 1 linearly independent constraints, which implies that the feasible set is a pointed polyhedral cone with vertex at origin. Since any point in a pointed polyhedral cone is a
conical combination (linear combination with nonnegative coefficients) of extreme rays of the cone,
checking nonnegativity of the objective function for all extreme rays suffices. We emphasize that
we do not solve the LPs (3) in our algorithm; we just check the extreme rays.

.

Computational efficiency. Extreme rays of a pointed polyhedral cone in Rdx+1 are computed from dx linearly independent active constraints. For each i  Bk, the extreme ray v^i,k  Vk  span{x¯j | j  Bk \ {i}} must be tested whether gk(z, v^i,k)T v^i,k  0, in both directions. Note that there are 2Mk extreme rays, and one extreme ray v^i,k is shared by 2Mk-1 polyhedral cones. Moreover, x¯Tj v^i,k = 0 for j  Bk \ {i}, which indicates that
gk(z, v^i,k)T v^i,k = (si,k - si)[W2]·T,k ix¯iT v^i,k, where i,k = sign(x¯iT v^i,k),
regardless of {j}jBk\{i}. Testing an extreme ray can be done with a single inequality test instead of 2Mk-1 separate tests for all cones! Thus, this extreme ray approach instead of solving individual LPs greatly reduces computation, from O(2Mk ) to O(Mk).

Testing extreme rays. For the details of testing all possible extreme rays, please refer to FO-INCREASING-TEST (Algorithm 4) and Appendix A.2. FO-INCREASING-TEST computes all possible extreme rays v~k and tests if they satisfy gk(z, v~k)T v~k  0. If the inequality is not satisfied by an extreme ray v~k, then this is a descent direction, so we return v~k. If the inequality holds with equality, it means this is a flat extreme ray, and it needs to be checked in second-order test, so we save this extreme ray for future use.
How many flat extreme rays (gk(z, v~k)T v~k = 0) are there? Presence of flat extreme rays introduce inequality constraints in the QP that we solve in the second-order test. It is ideal not to have them, because in this case there are only equality constraints, so the QP is easier to solve. Lemma A.1 in Appendix A.2 shows the conditions for having flat extreme rays; in short, there is a flat extreme ray if [W2]·T,k i = 0 or si = s+ or s-. For more details, please refer to Appendix A.2.

3.3 TESTING T H(z, )  0 FOR { | g(z, )T  = 0} (LINES 11­16)

The second-order test checks T H(z, )  0 for "flat" 's satisfying g(z, )T  = 0. This is
done with help of the function SO-TEST (Algorithm 5). Given its input {i,k}k[dh],iBk , it defines fixed "Jacobian" matrices Ji for all data points and equality/inequality constraints for boundary data
points, and solves the QP of the following form:

minimize subject to

i

T i

2

Ji(1

xi

+

1)

+

1 2

i

2O(xi) + 2 + W2Ji(1xi +1)

2 2

,
i

[W2]·T,kuk = [W1 b1]k,·vk, k  [dh],

x¯Ti vk = 0,

k  [dh], i  Bk s.t. i,k = 0,

i,kx¯Ti vk  0,

k  [dh], i  Bk s.t. i,k  {-1, +1}.

(4)

6

Under review as a conference paper at ICLR 2019

Constraints and number of QPs. There are dh equality constraints of the form [W2]·T,kuk = [[W1]k,· [b1]k] vk. These equality constraints are due to the nonnegative homogeneous property of activation h; i.e., scaling [W1]k,· and [b1]k by  > 0 and scaling [W2]·,k by 1/ yields exactly the same network. So, these equality constraints force  to be orthogonal to the loss-invariant di-
rections. This observation is stated more formally in Lemma A.2, which as a corollary shows that
any differentiable FOSP of R always has rank-deficient Hessian. The other constraints make sure that the union of feasible sets of QPs is exactly { | g(z, )T  = 0} (please see Lemma A.3 in
Appendix A.3 for details). It is also easy to check that these constraints are all linearly independent.

If there is no flat extreme ray, the algorithm solves just one QP with dh + M equality constraints. If there are flat extreme rays, the algorithm solves one QP with dh + M equality constraints, and 2K more QPs with dh + M - L equality constraints and L inequality constraints, where

dh dh
K := {i  Bk | [W2]·T,k i = 0} , L := |{i  Bk | v^i,k or -v^i,k is a flat ext. ray}| . (5)

k=1

k=1

Recall from Section 3.2 that i  Bk has a flat extreme ray if [W2]T·,k i = 0 or si = s+ or s-; thus, K  L  M . Please refer to Appendix A.3 for more details.

Efficiency of solving the QPs (4). Despite NP-hardness of general QPs, our specific form of QPs (4) can be solved quite efficiently, avoiding exponential complexity in p. After solving QP (4), there are three (disjoint) termination conditions:
(T1) T Q > 0 whenever   S,  = 0, or (T2) T Q  0 whenever   S, but  = 0,   S such that T Q = 0, or (T3)  such that   S and T Q < 0,

where S is the feasible set of QP. With the following two lemmas, we show that the termination

conditions can be efficiently tested for ECQPs and ICQPs. First, the ECQPs can be iteratively

solved with projected gradient descent, as stated in the next lemma.

Lemma 3. Consider the QP, where Q  Rp×p is symmetric and A  Rq×p has full row rank:

minimize

1 2

T

Q

subject to

A = 0q

Then, projected gradient descent (PGD) updates

(t+1) = (I - AT (AAT )-1A)(I - Q)(t)

with learning rate  < 1/max(Q) converges to a solution or diverges to infinity exponentially fast. Moreover, with random initialization, PGD correctly checks conditions (T1)­(T3) with probability 1.

The proof is an extension of unconstrained case (Lee et al., 2016), and is deferred to Appendix B.3. Note that it takes O(p2q) time to compute (I - AT (AAT )-1A)(I - Q) in the beginning, and each update takes O(p2) time. It is also surprising that the convergence rate does not depend on q.
In the presence of flat extreme rays, we have to solve QPs involving L inequality constraints. We prove that our ICQP can be solved in O(p3 + L32L) time, which implies that as long as the number of flat extreme rays is small, the problem can still be solved in polynomial time in p. Lemma 4. Consider the QP, where Q  Rp×p is symmetric, A  Rq×p and B  Rr×p have full row rank, and AT BT has rank q + r:
minimize T Q subject to A = 0q, B  0r.
Then, there exists a method that checks whether (T1)­(T3) in O(p3 + r32r) time.

In short, we transform  to define an equivalent problem, and use classical results in copositive
matrices (Martin & Jacobson, 1981; Seeger, 1999; Hiriart-Urruty & Seeger, 2010); the problem can be solved by computing the eigensystem of a (p-q -r)×(p-q -r) matrix, and testing copositivity of an r × r matrix. The proof is presented in Appendix B.4.

Concluding the test. During all calls to SO-TEST, whenever any QP terminated with (T3), then SOSP-CHECK immediately returns the direction and terminates. After solving all QPs, if any of SO-TEST calls finished with (T2), then we conclude SOSP-CHECK with "SOSP." If all QPs terminated with (T1), then we can return "Local Minimum."

7

Under review as a conference paper at ICLR 2019

Table 1: Summary of experimental results

(dx, dh, m)
(10, 1, 1000) (10, 1, 10000) (100, 1, 1000) (100, 1, 10000) (100, 10, 10000) (1000, 1, 10000) (1000, 10, 10000)

# Runs
40 40 40 40 40 40 40

Sum M (Avg.)
290 (7.25) 371 (9.275) 1,452 (36.3) 2,976 (74.4) 24,805 (620.125) 14,194 (354.85) 42,334 (1,058.35)

Sum L (Avg.)
0 (0) 1 (0.025) 0 (0) 2 (0.05) 4 (0.1) 0 (0) 37 (0.925)

Sum K (Avg.)
0 (0) 0 (0) 0 (0) 0 (0) 0 (0) 0 (0) 1 (0.025)

P{L > 0}
0 0.025 0 0.05 0.1 0 0.625

4 EXPERIMENTS

For experiments, we used artificial datasets sampled iid from standard normal distribution, and trained 1-hidden-layer ReLU networks with squared error loss. In practice, it is impossible to get to the exact nondifferentiable point, because they lie in a set of measure zero. To get close to those points, we ran Adam (Kingma & Ba, 2014) using full-batch (exact) gradient for 200,000 iterations and decaying step size (start with 10-3, 0.2× decay every 20,000 iterations). We observed that decaying step size had the effect of "descending deeper into the valley."

After running Adam, for each k  [dh], we counted the number of approximate boundary data points satisfying |[W1xi + b1]k| < 10-5, which gives an estimate of Mk. Moreover, for these

points, we solved the QP (2) using L-BFGS-B (Byrd et al., 1995), to check if the terminated points

are indeed (approximate) FOSPs. We could see that the optimal values of (2) are close to zero

( 10-6 typically, that ended up with

 0

10-3 for or 1. The

largest problems). After solving (2), we number of such si's is an estimate of L

counted the number of si's - K. We also counted the

number of approximate boundary data points satisfying |[W2]T·,k i| < 10-4, for an estimate of K.

We ran the above-mentioned experiments for different settings of (dx, dh, m), 40 times each. We fixed dy = 1 for simplicity. For large dh, the optimizer converged to near-zero minima, making  i uniformly small, so it was difficult to obtain accurate estimates of K and L. Thus, we had to
perform experiments in settings where the optimizer converged to minima that are far from zero.

Table 1 summarizes the results. Through 280 runs, we observed that there are surprisingly many boundary data points (M ) in general, but usually there are zero or very few (maximum was 3) flat extreme rays (L). This observation suggests two important messages: (1) many local minima are on nondifferentiable points, which is the reason why our analysis is meaningful; (2) luckily, L is usually very small, so we only need to solve ECQPs (L = 0) or ICQPs with very small number of inequality constraints, which are solved efficiently (Lemmas 3 and 4). We can observe that M , L, and K indeed increase as model dimensions and training set get larger, but the rate of increase is not as fast as dx, dh, and m.

For our experiments, we used artificial regression datasets instead of real ones. This is because popular real datasets are classification datasets, but cross-entropy loss gets arbitrarily small as parameters tend to infinity and hinge loss is not differentiable; the losses are not adequate for our purposes.

5 DISCUSSION AND FUTURE WORK
We provided an algorithm to test second-order stationarity and escape saddle points, for nondifferentiable points of empirical risk of shallow ReLU-like networks. Despite difficulty raised by boundary data points dividing the parameter space into 2M regions, we reduced the computation to dh convex QPs, O(M ) equality/inequality tests, and one (or a few more) QP. In benign cases, the last QP is equality constrained, which can be efficiently solved with projected gradient descent. In the worst case, the QP has a few (say L) inequality constraints, but it can be solved efficiently when L is small. We also provided empirical evidences that L is usually either zero or very small, suggesting that the test can be done efficiently in most cases. Extending this test to deeper neural networks is a possible future work. Also, numerical implementation of this algorithm and combining it with practical gradient-based methods will be of great interest.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53­58, 1989.
Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In Proceedings of the 1st International Conference on Neural Information Processing Systems, pp. 494­501. MIT Press, 1988.
Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and examples. Springer Science & Business Media, 2010.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In International Conference on Machine Learning, pp. 605­614, 2017.
Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):1190­1208, 1995.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex optimization. arXiv preprint arXiv:1611.00756, 2016.
Francis H Clarke, Yuri S Ledyaev, Ronald J Stern, and Peter R Wolenski. Nonsmooth analysis and control theory, volume 178. Springer Science & Business Media, 2008.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.
J-B Hiriart-Urruty and Alberto Seeger. A variational approach to copositive matrices. SIAM review, 52(4):593­629, 2010.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586­594, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Thomas Laurent and James von Brecht. The multilinear structure of relu networks. arXiv preprint arXiv:1712.10132, 2017.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pp. 1246­1257, 2016.
Duncan Henry Martin and David Harris Jacobson. Copositive matrices and definiteness of quadratic forms subject to homogeneous linear inequality constraints. Linear Algebra and its Applications, 35:227­258, 1981.
Aryan Mokhtari, Asuman Ozdaglar, and Ali Jadbabaie. Escaping saddle points in constrained optimization. arXiv preprint arXiv:1809.02162, 2018.
Katta G Murty and Santosh N Kabadi. Some np-complete problems in quadratic and nonlinear programming. Mathematical programming, 39(2):117­129, 1987.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 2603­2612, 2017a.
Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep cnns. arXiv preprint arXiv:1710.10928, 2017b.
Panos M Pardalos and Stephen A Vavasis. Quadratic programming with one negative eigenvalue is np-hard. Journal of Global Optimization, 1(1):15­22, 1991.
Sashank J Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdinov, and Alexander J Smola. A generic approach for escaping saddle points. arXiv preprint arXiv:1709.01434, 2017.
9

Under review as a conference paper at ICLR 2019
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv preprint arXiv:1712.08968, 2017.
Alberto Seeger. Eigenvalue analysis of equilibrium processes defined by linear complementarity conditions. Linear Algebra and its Applications, 292(1-3):1­14, 1999.
Ohad Shamir. Are resnets provably better than linear predictors? arXiv preprint arXiv:1804.06739, 2018.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. In International Conference on Machine Learning, pp. 3404­3413, 2017.
Chenwei Wu, Jiajun Luo, and Jason D Lee. No spurious local minima in a two hidden unit relu network. In International Conference on Learning Representations Workshop, 2018.
Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning. IEEE Transactions on Neural Networks, 6(5):1300­1303, 1995.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Spurious local minima in neural networks: a critical view. arXiv preprint arXiv:1802.03487, 2018a.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks. In International Conference on Learning Representations, 2018b.
Yi Zhou and Yingbin Liang. Critical points of neural networks: Analytical forms and landscape properties. In International Conference on Learning Representations, 2018.
10

Under review as a conference paper at ICLR 2019

Algorithm 2 SOSP-CHECK

Input: A tuple (Wj, bj)2j=1 of R(·).

1: if

m i=1



i

O(xi)T

1 = 0dy×(dh+1) then

2:

return [2

2]  -

m i=1



i

O(xi)T

3: end if

1 , 1  0dh×dx , 1  0dh .

4: for k  [dh] do

5: if Mk > 0 then 6: {si }iBk  FO-SUBDIFF-ZERO-TEST(k) 7: v~kT  [W2]·T,k(Ck + iBk si ix¯iT ). 8: if v~k = 0dx+1 then 9: return vk  -v~k, k  [dh] \ {k}, vk  0dx+1, 2  0dy×dh , 2  0dy .
10: end if 11: (decr, v~k, {Si,k}iBk )  FO-INCREASING-TEST(k, {si }iBk ). 12: if decr = True then

13: return vk  v~k, k  [dh] \ {k}, vk  0dx+1, 2  0dy×dh , 2  0dy . 14: end if

15: else if [W2]·T,kCk = 0dTx+1 then 16: return vk  -CkT [W2]·,k, k  [dh] \ {k}, vk  0dx+1, 2  0dy×dh , 2  0dy . 17: end if

18: end for

19: (decr, sosp, (j , j )j2=1)  SO-TEST({0}k[dh],iBk ).
20: if decr = True then return (j, j)2j=1. 21: end if

22: if M = 0 and {Si,k}k[dh],iBk = {{0}}k[dh],iBk then
23: for each element {i,k}k[dh],iBk  k[dh] iBk Si,k do 24: (decr, sospTemp, (j , j )2j=1)  SO-TEST({i,k}k[dh],iBk ).
25: if decr = True then return (j, j)2j=1. 26: end if

27: sosp  sosp  sospTemp

28: end for

29: end if

30: if sosp = True then return SOSP.

31: else return Local Minimum.

32: end if

Algorithm 3 FO-SUBDIFF-ZERO-TEST

Input: k  [dh] 1: Solve the following optimization problem and get optimal solution {si}iBk :

minimize{si}iBk

[W2]·T,k(Ck +

iBk si

ix¯iT )

2 2

subject to

min{s-, s+}  si  max{s-, s+}, i  Bk,

2: return {si }iBk .

(2)

A FULL ALGORITHMS AND PROOF OF CORRECTNESS

In this section, we present the detailed operation of SOSP-CHECK (Algorithm 2), and its helper functions FO-SUBDIFF-ZERO-TEST, FO-INCREASING-TEST, and SO-TEST (Algorithm 3­5).
In the subsequent subsections, we provide a more detailed proof of the correctness of Algorithm 2. Recall that, by Lemmas 1 and 2, Mk := |Bk|  dx and vectors {x¯i}iBk are linearly independent. Also, we can expand R(z + ) so that

R(z + ) = R(z) + g(z, )T  +

1 2

T

H

(z

,

)

+

o(



2),

11

Under review as a conference paper at ICLR 2019

Algorithm 4 FO-INCREASING-TEST

Input: k  [dh], {si }iBk 1: for all i  Bk do 2: Define Si,k  . 3: Get a vector v^i,k  Vk  span{x¯j | j  Bk \ {i}}. 4: for v~k  {v^i,k, -v^i,k} do
5: Define i,k  sign(x¯Ti v~k). 6: if (si,k - si )[W2]T·,k ix¯iT v~k < 0 then

7: 8:

elserieft(usrni,k(T-rusie),[v~Wk,2{]·T,k}iBix¯k )iT v~k = 0 then

9: Si,k  Si,k  {i,k}.

10: end if

11: end for

12: If Si,k = , Si,k  {0}.

13: end for

14: return (False, 0dx+1, {Si,k}iBk ).

Algorithm 5 SO-TEST
Input: {i,k}k[dh],iBk 1: For all i  [m], define diagonal matrices Ji  Rdh×dh such that for k  [dh],

h ([N (xi)]k)  [Ji]k,k  si,k
0

if i  [m] \ Bk if i  Bk and i,k  {-1, +1} if i  Bk and i,k = 0.

2: Solve the following QP. If there is no solution, get a descent direction (j, j)2j=1.

minimize

subject to



T i

2Ji

(1xi

+

1)

+

1 2

2O(xi)+2 +W2Ji(1xi +1)

2 2

,
i

ii

[W2]·T,kuk = [W1 b1]k,·vk, k  [dh],

x¯Ti vk = 0,

k  [dh], i  Bk s.t. i,k = 0,

i,kx¯Ti vk  0,

k  [dh], i  Bk s.t. i,k  {-1, +1}.

3: if There is no solution then return (True, False, (j , j)j2=1). 4: else if QP has nonzero minimizers then return (False, True, 0) 5: else return (False, False, 0) 6: end if

(4)

where g(z, ) and H(z, ) satisfy

g(z, )T  =


i

T i

dY1

(xi

)

=

 iO(xi)T , 2 +
i

T H(z, ) =


i

T i

dY2

(xi

)

+

1 2

i

dY1(xi)

2 2

,
i

and gk(z, vk)T := [W2]·T,k Ck + iBk h (x¯Ti vk) ix¯Ti .

dh
 i, 2 + gk(z, vk)T vk,
i k=1

A.1 TESTING FIRST-ORDER STATIONARITY (LINES 1­3, 6­10 AND 15­17)

A.1.1 TEST OF FIRST-ORDER STATIONARITY FOR W2 AND b2 (LINES 1­3)

Lines 1­3 of Algorithm 2 correspond to testing if W2 R = {0dy×dh } and b2 R = {0dy }. If they

are not all zero, the opposite direction is a descent direction, as Line 2 returns. To see why, suppose

m i=1



i

O(xi)T

1 = 0dy×(dh+1). Then choose perturbations

[2 2] = -

m


i=1

i

O(xi)T

1 , 1 = 0dh×dx , 1 = 0dh .

12

Under review as a conference paper at ICLR 2019

If we apply perturbation (j, j)2j=1 where  > 0, we can immediately check that dY1(xi) = 2O(xi) + 2 and dY2(xi) = 0. So,

g(z, )T  =

T H(z, )

=

1 2

m

i=1

T i

(2O(xi) + 2)

=

m
i
i=1

O(xi)T

dY1(xi)T 2 idY1(xi) = O(2)  0.
i

1 , [2 2]

= -O(),

and also that

m i=1

dY (xi)

2 2

=

O(2).

Then, by scaling  sufficiently small we can achieve

R(z + ) < R(z), which disproves that (Wj, bj)j2=1 is a local minimum.

A.1.2 TEST OF FIRST-ORDER STATIONARITY FOR W1 AND b1 (LINES 6­10 AND 15­17)

Test for W1 and b1 is more difficult because g(z, ) depends on 1 and 1 when there are boundary data points. Recall that vkT (k  [dh]) is the k-th row of [1 1]. Then note from Lemma 2 that

m

i=1

T i

(W2J (xi)(1xi

+ 1))

=

dh gk(z, vk)T vk,
k=1

where gk(z, vk)T := [W2]·T,k Ck + iBk h (x¯Ti vk) ix¯Ti . Thus we can separate k's and treat them individually.

Test for zero gradient. Recall the definition Mk := |Bk|. If Mk = 0, there is no boundary data point for k-th hidden node, so the Clarke subdifferential with respect to [W1 b1]k,·, is {CkT [W2]·,k}. Lines 15­17 handle this case; if the singleton element in the subdifferential is not zero, its opposite
direction is a descent direction, so return that direction, as in Line 16.

Test for zero in subdifferential. For the case Mk > 0, we saw that for boundary data points i  Bk, h ([1xi + 1]k) = h (x¯iT vk)  {s-, s+} depends on vk. Lines 6­10 test if 0dTx+1 is in the Clarke subdifferential of R with respect to [W1]k,· and [b1]k. Since the subdifferential is used many times, we give it a specific name Dk := [W1 b1]k,· R. By observing that Dk is the convex hull
of all possible values of gk(z, vk)T ,

Dk := [W2]·T,k Ck + iBk si ix¯iT | min{s-, s+}  si  max{s-, s+}, i  Bk .

Testing QP (2),

0Tdx and

+1  Dk is done returns {si}iBk .

by

FO-SUBDIFF-ZERO-TEST

in

Algorithm

3.

It solves a convex

If 0Tdx+1  Dk, {si }iBk will satisfy v~kT := [W2]·T,k(Ck + iBk si ix¯iT ) = 0dTx+1. Suppose 0dTx+1 / Dk. Then, v~k is the closest vector in Dk from the origin, so v~k, v > 0 for all vT  Dk. Choose perturbations

vk = -v~k, vk = 0dx+1 for all k  [dh] \ {k}, 2 = 0dy×dh , 2 = 0dy ,

and apply perturbation (j, j)2j=1 where  > 0. With this perturbation, we can check that

g(z, )T  =

m

i=1

T i

d

Y1(xi)

=

-[W2]T·,k

Ck +

h
iBk

(-x¯Ti v~k)

ix¯Ti

v~k ,

and since h (-x¯iT v~k)  {s-, s+} for i  Bk, we have

[W2]T·,k Ck + iBk h (-x¯iT v~k) ix¯iT  Dk,
and v~k, v > 0 for all vT  Dk shows that g(z, )T  is strictly negative with magnitude O(). It is easy to see that T H(z, ) = O(2), so by scaling  sufficiently small we can disprove local minimality of (Wj, bj)2j=1.

13

Under review as a conference paper at ICLR 2019

A.2 TESTING g(z, )T   0 FOR ALL  (LINES 11­14) Linear program formulation. Lines 11­14 are essentially about testing if gk(z, vk)T vk  0 for all directions vk. If 0dTx+1  Dk, with the solution {si }iBk from FO-SUBDIFF-ZERO-TEST we can write gk(z, vk)T as

gk(z, vk)T = [W2]·T,k Ck +

h (x¯iT vk) ix¯iT = [W2]T·,k

h (x¯iT vk) - si  ix¯iT .

iBk

iBk

For any i  Bk, h (x¯iT vk)  {s-, s+} changes whenever the sign of x¯Ti vk changes. Every i  Bk bisects Rdx+1 into two halfspaces, x¯iT vk  0 and x¯iT vk  0, in each of which h (x¯iT vk) stays constant. Note that by Lemma 1, x¯i's for i  Bk are linearly independent. So, given Mk linearly independent x¯i's, they divide the space Rdx+1 of vk into 2Mk polyhedral cones.

Since gk(z, vk)T is constant in each polyhedral cone, we can let i  {-1, +1} for all i  Bk, and define an LP for each {i}iBk  {-1, +1}Mk :

minimize
vk

[W2]·T,k

iBk (si - si) ix¯iT vk

subject to vk  Vk, ix¯Ti vk  0, i  Bk.

(3)

Solving these LPs and checking if the minimum value is 0 suffices to prove gk(z, vk)T vk  0 for all small enough perturbations. Recall that Vk := span{x¯i | i  Bk} and dim(Vk) = Mk. Note that any component of vk that is orthogonal to Vk is also orthogonal to gk(z, vk), so it does not affect the objective function of any LP (3). Thus, the constraint vk  Vk is added to the LP (3), which is equivalent to adding dx+1-Mk linearly independent equality constraints. The feasible set of LP (3) has dx + 1 linearly independent equality/inequality constraints, which implies that the feasible set is a pointed polyhedral cone with vertex at origin. Since any point in a pointed polyhedral cone is a
conical combination (linear combination with nonnegative coefficients) of extreme rays of the cone,
checking nonnegativity of the objective function for all extreme rays suffices. We emphasize that we
do not solve the LPs (3) in our algorithm; we just check the extreme rays.

Computational efficiency. Extreme rays of a pointed polyhedral cone in Rdx+1 are computed from dx linearly independent active constraints. Line 3 of Algorithm 4 is exactly computing such extreme rays: v^i,k  Vk  span{x¯j | j  Bk \ {i}} for each i  Bk, tested in both directions.
Note that there are 2Mk extreme rays, and one extreme ray v^i,k is shared by 2Mk-1 polyhedral cones. Moreover, x¯Tj v^i,k = 0 for j  Bk \ {i}, which indicates that
gk(z, v^i,k)T v^i,k = (si,k - si)[W2]T·,k ix¯iT v^i,k, where i,k = sign(x¯Ti v^i,k),
regardless of {j}jBk\{i}. This observation is used in Lines 6 and 8 of Algorithm 4. Testing gk(z, v~k)T v~k  0 for an extreme ray v~k can be done with a single inequality test instead of 2Mk-1 separate tests for all cones! Thus, this extreme ray approach instead of solving individual LPs greatly reduces computation, from O(2Mk ) to O(Mk).

Algorithm operation in detail. Testing all possible extreme rays is exactly what
FO-INCREASING-TEST in Algorithm 4 is doing. Output of FO-INCREASING-TEST is a tu-
ple of three items: a boolean, a (dx + 1)-dimensional vector, and a tuple of Mk sets. Whenever we have a descent direction, it returns True and the descent direction v~k. If there is no descent direction, it returns False and the sets {Si,k}iBk .
For both direction of extreme rays v~k = v^i,k and v~k = -v^i,k (Line 4), we check if gk(z, v~k)T v~k  0. Whenever it does not hold (Lines 6­7), v~k is a descent direction, so FO-INCREASING-TEST returns it with True. Line 13 of Algorithm 2 uses that v~k to return perturbations, so that scaling by small enough  > 0 will give us a point with R(z + ) < R(z). If equality holds (Lines 8­9), this means v~k is a direction of perturbation satisfying g(z, )T  = 0, so this direction needs to be checked if T H(z, )  0 too. In this case, we add the sign of boundary data point x¯i to Si,k for future use in the second-order test. The operation with Si,k will be explained in detail in Appendix A.3. After checking if gk(z, v~k)T v~k  0 holds for all extreme rays, FO-INCREASING-TEST returns False with {Si,k}iBk .

14

Under review as a conference paper at ICLR 2019

Counting flat extreme rays. How many of these extreme rays satisfy gk(z, v~k)T v~k = 0? Presence of such flat extreme rays introduce inequality constraints in the QP that we will solve in SO-TEST (Algorithm 5). It is ideal not to have flat extreme rays, because in this case there are only equality constraints, so the QP is easier to solve. The following lemma shows conditions for existence of flat extreme rays as well as output of Algorithm 4.
Lemma A.1. Suppose 0Tdx+1  Dk and all extreme rays v~k satisfy gk(z, v~k)T v~k  0. Consider all i  Bk, and its corresponding v^i,k  Vk  span{x¯j | j  Bk \ {i}}.
1. If [W2]T·,k i = 0, then both extreme rays v^i,k and -v^i,k are flat extreme rays, and Si,k = {-1, +1} at the end of Algorithm 4.
2. If [W2]T·,k i = 0 and si = s+ (or s-), one (and only) of v~k  {v^i,k, -v^i,k} that satisfies sign(x¯iT v~k) = +1 (or -1) is a flat extreme ray, and Si,k = {+1} (or {-1}) at the end of Algorithm 4.
3. If [W2]T·,k i = 0 and si = s±, both v^i,k and -v^i,k are not flat extreme rays, and Si,k = {0} at the end of Algorithm 4.
Proof First note that we already assumed that all extreme rays v~k satisfy gk(z, v~k)T v~k  0, so SOSP-CHECK will reach Line 14 at the end. Also note that x¯i's in i  Bk are linearly independent (by Lemma 1), so x¯iT v^i,k = 0.
If [W2]T·,k i = 0, then (si,k -si )[W2]·T,k ix¯iT v~k = 0 regardless of v~k, so both v^i,k and -v^i,k are flat extreme rays. If [W2]·T,k i = 0 and si = s+, v~k  {v^i,k, -v^i,k} that satisfies sign(x¯iT v~k) = +1 gives i,k = +1, so si,k = si. Thus, v~k is a flat extreme ray. The case with si = s- is proved similarly. If [W2]T·,k i = 0 and si = s+, none of (s± - si), [W2]·T,k i, and x¯Ti v^i,k are zero, so v^i,k and -v^i,k cannot be flat.

Let Bk(j)  Bk denote the set of indices i  Bk satisfying conditions in Lemma A.1.j (j = 1, 2, 3). Note that Bk(j)'s partition the set Bk. We denote the union of Bk(1) and Bk(2) by Bk(1,2), and similarly, Bk(2,3) := Bk(2)  Bk(3). We can see from the lemma that |Si,k| = 2 for i  Bk(1), and |Si,k| = 1 for i  Bk(2,3). Also, it follows from the definition of K and L (5) that

dh dh
K = |Bk(1)|, L = |Bk(1)| + |Bk(2)|.

k=1

k=1

Connection to KKT conditions. As a side remark, we provide connections of our tests to the wellknown KKT conditions. Note that the equality gk(z, vk)T = [W2]·T,k iBk (si - si ) ix¯iT for ix¯Ti vk  0, i  Bk corresponds to the KKT stationarity condition, where (si - si )[W2]·T,k i's correspond to the Lagrange multipliers for inequality constraints. Then, testing extreme rays are equivalent to testing dual feasibility of Lagrange multipliers, and having zero dual variables ([W2]·T,k i = 0 or si = s+ or s-, resulting in flat extreme rays) corresponds to having degeneracy in the complementary slackness condition.
As mentioned in Section 2.1, given that g(z, ) and H(z, ) are constant functions of  in each polyhedral cone, one can define inequality constrained optimization problems and try to solve for KKT conditions for z directly. However, this also requires solving 2M problems. The strength of our approach is that by solving the QPs (2), we can automatically compute the exact Lagrange multipliers for all 2M subproblems, and dual feasibility is also tested in O(M ) time.

A.3 TESTING T H(z, )  0 FOR { | g(z, )T  = 0} (LINES 19­32)
The second-order test checks T H(z, )  0 for "flat" 's satisfying g(z, )T  = 0. This is done with help of the function SO-TEST in Algorithm 5. Given its input {i,k}k[dh],iBk , it defines fixed "Jacobian" matrices Ji for all data points and equality/inequality constraints for boundary data points, and solves the QP (4).

15

Under review as a conference paper at ICLR 2019

Equality/inequality constraints. In the QP (4), there are dh equality constraints of the form [W2]·T,kuk = [[W1]k,· [b1]k] vk. These equality constraints are due to the nonnegative homogeneous property of activation function h: scaling [W1]k,· and [b1]k by  > 0 and scaling [W2]·,k by 1/ yields exactly the same network. This observation is stated more precisely in the following lemma.
Lemma A.2. Suppose z is a FOSP (differentiable or not) of R(·). Fix any k  [dh], and define perturbation  as
uk = -[W2]·,k, vk = [[W1]k,· [b1]k]T , uk = 0, vk = 0 for all k = k, 2 = 0.
Then, g(z, )T  = T H(z, ) = 0.

The proof of Lemma A.2 can be found in Appendix B.5. A corollary of this lemma is that any differentiable FOSP of R always has rank-deficient Hessian, and the multiplicity of zero eigenvalue is at least dh. Hence, these dh equality constraints on uk's and vk's force  to be orthogonal to the loss-invariant directions.

The equality constraints of the form x¯Ti vk = 0 are introduced when i,k = 0; this happens for boundary data points i  Bk(3). Therefore, there are M - L additional equality constraints. The inequality constraints come from i  Bk(1,2). So there are L inequality constraints. Now, the following lemma proves that feasible sets defined by these equality/inequality constraints added to (4) exactly correspond to the regions where gk(z, vk)T vk = 0. Recall from Lemma A.1 that
Si,k = {-1, +1} for i  Bk(1), Si,k = {-1} or {+1} for i  Bk(2), and Si,k = {0} for i  Bk(3).
Lemma A.3. Let {i,k}iBk(2) be the only element of iBk(2) Si,k. Then, in SO-TEST,

{i,k }iBk(1) 

i Si,k

vk | i  Bk(3), x¯Ti vk = 0, and i  Bk(1,2), i,kx¯Ti vk  0

= vk | i  Bk(3), x¯iT vk = 0, and i  Bk(2), i,kx¯Ti vk  0

= vk | gk(z, vk)T vk = 0 .

The proof of Lemma A.3 is in Appendix B.6.
In total, there are dh + M - L equality constraints and L inequality constraints in each nonconvex QP. It is also easy to check that these constraints are all linearly independent.

How many QPs do we solve? Note that in Line 19, we call SO-TEST with {i,k}k[dh],iBk = 0, which results in a QP (4) with dh + M equality constraints. This is done even when we have flat extreme rays, just to take a quick look if a descent direction can be obtained without having to deal
with inequality constraints.

If there exist flat extreme rays (Line 22), the algorithm calls SO-TEST for each element of k[dh] iBk Si,k. Recall that |Si,k| = 2 for i  Bk(1), so

k[dh ]

iBk Si,k = 2K .

In summary, if there is no flat extreme ray, the algorithm solves just one QP with dh + M equality constraints. If there are flat extreme rays, the algorithm solves one QP with dh + M equality constraints, and 2K QPs with dh + M - L equality constraints and L inequality constraints. This is also an improvement from the naive approach of solving 2M QPs.

Concluding the test. After solving the QP, SO-TEST returns result to SOSP-CHECK. The al-
gorithm returns two booleans and one perturbation tuple. The first is to indicate that there is no solution, i.e., there is a descent direction that leads to -. Whenever there was any descent direc-
tion then we immediately return the direction and terminate. The second boolean is to indicate that there are nonzero  that satisfies T H(z, ) = 0. After solving all QPs, if any of SO-TEST calls found out  = 0 such that g(z, )T  = 0 and T H(z, ) = 0, then we conclude SOSP-CHECK
with "SOSP." If all QPs terminated with unique minimum at zero, then we can conclude "Local
Minimum."

16

Under review as a conference paper at ICLR 2019

B PROOF OF LEMMAS

B.1 PROOF OF LEMMA 1

By definition, we have [W1]k,·xi + [b1]k = 0 for all i  Bk, meaning that they are all on the same hyperplane [W1]k,·x + [b1]k = 0. By the assumption, we cannot have more than dx points on the hyperplane.

Next, assume for the sake of contradiction that the Mk := |Bk| data points x¯i's are linearly dependent, i.e., there exists a1, . . . , aMk  R, not all zero, such that

Mk
ai

xi 1

Mk Mk
= 0 = a1 = - ai = ai(xi - x1) = 0,

i=1 i=2 i=2

where a2, . . . , aMk are not all zero. This implies that these Mk points xi's are on the same (Mk -2)dimensional affine space. To see why, consider for example the case Mk = 3: a2(x2 - x1) = -a3(x3 - x1), meaning that they have to be on the same line. By adding any dx + 1 - Mk additional xi's, we can see that dx + 1 points are on the same (dx - 1)-dimensional affine space, i.e., a hyperplane in Rdx . This contradicts Assumption 2.

B.2 PROOF OF LEMMA 2

From Assumption 1, (w, y) is twice differentiable and convex in w. By Taylor expansion of (·) at (Y (xi), yi),

m
R(z + ) = i=1 (Y (xi) + dY (xi), yi)

=

m i=1

(Y (xi), yi)

+

T i

dY

(xi)

+

1 2

d

Y

(xi

)T

2

idY (xi) + o(



2)

mm

m

= R(z) +



T i

d

Y1(xi)

+



T i

d

Y2(xi)

+

1 2

dY1(xi)

2 2

+ o(
i



2),

i=1 i=1

i=1

where the first-order term

m i=1



T i

d

Y1(xi)

=

m i=1



T i

(2O(xi

)

+

2

+

W2

J

(xi)(1xi

+

1

))

can be further expanded to show

m

i=1

T i

(2O(xi

)

+

2)

=

2,

 iO(xi)T + 2,
i

i ,
i

m i=1



T i

(W2J (xi)(1xi

+ 1))

=

tr

m i=1

J

(xi)W2T



ix¯Ti

1T 1T

dh
= [W2]T·,k
k=1

m
[J (xi)]k,k ix¯iT
i=1

dh
vk = [W2]·T,k Ck +
k=1

iBk h (x¯iT vk) ix¯iT

vk .

Also, note that in each of the 2M divided region (which is a polyhedral cone) of , J(xi) stays constant for all i  [m]; thus, g(z, ) and H(z, ) are piece-wise constant functions of . Specif-
ically, since the parameter space is partitioned into polyhedral cones, we have g(z, ) = g(z, )
and H(z, ) = H(z, ) for any  > 0.

B.3 PROOF OF LEMMA 3
Suppose that w1, w2, . . . , wq are orthonormal basis of row(A). Choose wq+1, . . . , wp so that w1, w2, . . . , wp form an orthonormal basis of Rp. Let W be an orthogonal matrix whose columns are w1, w2, . . . , wp, and W^ be an submatrix of W whose columns are wq+1, . . . , wp. With this definition, note that I - AT (AAT )-1A = W^ W^ T .
Suppose that we are given (t) satisfying A(t) = 0. Then we can write (t) = W^ µ(t), where µ(t)  Rp-q and [µ(t)]i = wiT+q(t). Define µ(t+1) likewise. Then, noting (t) = W^ W^ T (t) gives
W^ µ(t+1) = (t+1) = (t) - W^ W^ T QW^ µ(t) = W^ (I - W^ T QW^ )µ(t).

17

Under review as a conference paper at ICLR 2019

Define C := W^ T QW^  R(p-q)×(p-q), and then write its eigen-decomposition C = V SV T and denote its eigenvectors as 1, . . . , p-q and its corresponding eigenvalues 1, . . . , p-q. Then note

p-q

p-q

µ(t+1) = (I - C)µ(t) = (I - V SV T ) (iT µ(t))i = (1 - i)(iT µ(t))i

i=1 i=1

p-q

p-q

= (1 - i)2(iT µ(t-1))i = · · · = (1 - i)t+1(iT µ(0))i.

i=1 i=1

This proves that this iteration converges or diverges exponentially fast. Starting from the initial point (0) = W^ µ(0), the component of µ(0) that corresponds to negative eigenvalue blows up exponentially fast, those corresponding to positive eigenvalue shrinks to zero exponentially fast (if  < 1/max(C)), and those with zero eigenvalue will stay invariant. Therefore, if there exists i < 0, then (t) blows up to infinity quickly and finds an  such that T Q < 0 (T3). If all i  0, it converges exponentially fast to W^ i:i=0(iT µ(0))i (T2). If all i > 0, (t)  0 (T1).
It is left to prove that  < 1/max(Q) guarantees convergence, as stated. To this end, it suffices to show that max(Q)  max(C). Note that

C = W^ T QW^ = W^ T W W T QW W T W^ = [0

I] W T QW

0 I

.

Using the facts that max(Q) = max(W T QW ) and C is a principal submatrix of W T QW ,

xT W T QW x

xT W T QW x

max(Q) = max
x

xT x

 max
x:[x]1:q =0

xT x

= max(C).

Also, if we start at a random initial point (e.g., sample from a Gaussian in Rp and project to
row(A)), then with probability 1 we have iT µ(0) = 0 for all i  [p - q], so we will get the correct convergence/divergence result almost surely.

B.4 PROOF OF LEMMA 4

B.4.1 PRELIMINARIES

Before we prove the complexity lemma, we introduce the definitions of copositivity and Pareto spectrum, which are closely related concepts to our specific form of QP.
Definition B.1. Let Q  Rr×r be a symmetric matrix. We say that Q is copositive if T Q  0 for all   0. Moreover, strict copositivity means that T Q > 0 for all   0,  = 0.

Testing whether Q is not copositive known to be NP-complete (Murty & Kabadi, 1987); it is certainly a difficult problem. There is a method testing cositivity of Q in O(r32r) time which uses
its Pareto spectrum (Q). The following is the definition of Pareto spectrum, taken from Seeger
(1999); Hiriart-Urruty & Seeger (2010).

Definition B.2. Consider the problem

minimize T Q.
0,  2=1

KKT conditions for the above problem gives us a complementarity system

  0, Q -   0, T (Q - ) = 0,  2 = 1,

(6)

where   R is viewed as a Lagrange multiplier associated with  2 = 1. The number   R is called a Pareto eigenvalue of Q if (6) admits a solution . The set of all Pareto eigenvalues of Q, denoted as (Q), is called the Pareto spectrum of Q.

The next lemma reveals the relation of copositivity and Pareto spectrum:
Lemma B.1 (Theorem 4.3 of Hiriart-Urruty & Seeger (2010)). A symmetric matrix Q is copositive (or strictly copositive) if and only if all the Pareto eigenvalues of Q are nonnegative (or strictly positive).

18

Under review as a conference paper at ICLR 2019

Now, the following lemma tells us how to compute Pareto spectrum of Q.

Lemma B.2 (Theorem 4.1 of Seeger (1999)). Let Q be a matrix of order r. Consider a nonempty index set J  [r]. Given J, QJ refers to the principal submatrix of Q with the rows and columns
of Q indexed by J. Let 2[r] \  denote the set of all nonempty subsets of [r]. Then   (Q) if and
only if there exists an index set J  2[r] \  and a vector   R|J| such that

QJ  = ,   int(R+|J|),

[Q]i,j[]j  0 for all i / J.

jJ

In such a case, the vector   Rr by

[]j =

[]j 0

if j  J, if j / J

is a Pareto-eigenvector of Q associated to the Pareto eigenvalue .

These lemmas tell us that the Pareto spectrum of Q can be calculated by computing eigensystems of all 2r - 1 possible QJ , which takes O(r32r) time in total, and from this we can determine whether
a symmetric Q is copositive.

B.4.2 PROOF OF THE LEMMA

With the preliminary concepts presented, we now start proving our Lemma 4. We will first transform
 to eliminate the equality constraints and obtain an inequality constrained problem of the form minimizew:B¯w0 wT Rw. From there, we can use the theorems from Martin & Jacobson (1981), which tell us that by testing positive definiteness of a (p-q -r)×(p-q -r) matrix and copositivity
of a r × r matrix we can determine which of the three categories the QP falls into. Transforming  and testing positive definiteness take O(p3) time and testing copositivity takes O(r32r) time, so the test in total is done in O(p3 + r32r) time.

We now describe how to transform  and get an equivalent optimization problem of the form we want. We assume without loss of generality that A = [A1 A2] where A1  Rq×q is invertible. If not, we can permute components of . Then make a change of variables

 = TA

w¯ w

:=

A-1 1 0(p-q)×q

-A-1 1A2 Ip-q

w¯ w

, so that ATA

w¯ w

= [I

0]

w¯ w

= w¯.

Consequently, the constraint A = 0 becomes w¯ = 0. Now partition B = [B1 B2], where
B1  Rr×q. Also let R be the principal submatrix of TAT QTA composed with the last p - q rows and columns. It is easy to check that

minimize T Q

 minimizew wT Rw

subject to A = 0q, B  0r.

subject to (B2 - B1A-1 1A2)w  0r.

Let us quickly check if B2 - B1A-1 1A2 has full row rank. One can observe that

A1 B1

A2 B2

=

Iq B1A-1 1

0 Ir

A1 0

A2 B2 - B1A1-1A2

.

It follows from the assumption rank( AT BT ) = q + r that B¯ := B2 - B1A-1 1A2 has rank r, which means it has full row rank.

Before stating the results from Martin & Jacobson (1981), we will transform the problem a bit further. Again, assume without loss of generality that B¯ = B¯1 B¯2 where B¯1  Rr×r is invertible. Define another change of variables as the following:

w = TB :=

B¯1-1 0(p-q-r)×r

-B¯1-1B¯2 Ip-q-r

1 2

,

TBT RTB =:

R¯11 R¯1T2

R¯12 R¯22

=: R¯.

Consequently, we get

minimizew subject to

wT Rw  minimizew B¯w  0r. subject to

T R¯ = 1T R¯111 + 21T R¯122 + 2T R¯222 1  0r.

Given this transformation, we are ready to state the lemmas.

19

Under review as a conference paper at ICLR 2019

Lemma B.3 (Theorem 2.2 of Martin & Jacobson (1981)). If B¯ = B¯1 B¯2 , with B¯1 r × r invertible, then with R¯ij's given as above, wT Rw > 0 whenever B¯w  0, w = 0 if and only if

· R¯22 is positive definite, and
· R¯11 - R¯12R¯2-21R¯1T2 is strictly copositive. Lemma B.4 (Theorem 2.1 of Martin & Jacobson (1981)). If B¯ = B¯1 B¯2 , with B¯1 r × r invertible, then with R¯ij's given as above, wT Rw  0 whenever B¯w  0 if and only if

· R¯22 is positive semidefinite, null(R¯22)  null(R¯12), and

· R¯11 - R¯12R¯22R¯1T2 is copositive,

where R¯22 is a pseudoinverse of R¯22.

Using Lemmas B.3 and B.4, we now describe how to test our given QP and declare one of (T1), (T2), or (T3). First, we compute the eigensystem of R¯22 and see which of the following disjoint categories it belongs to:

(PD1) All eigenvalues 1, . . . , p-q-r of R¯22 satisfy i > 0.
(PD2) i, i  0, but i such that i = 0, and 2 s.t. R¯222 = 0, we have R¯122 = 0. (PD3) i, i  0, but i such that i = 0, and 2 s.t. R¯222 = 0 but R¯122 = 0. (PD4) i such that i < 0, i.e., 2 such that 2T R¯222 < 0.

If the test comes out (PD3) or (PD4), then we can immediately declare (T3) without having to look at

copositivity. This is because if we get (PD4), we can

In case of (PD3), one negative we can get T

can R¯

fix any  -.

1 satisfying 1T Notice that once

choose R¯122

1 =

= 0,

0 so and

that T R¯ by scaling

= 2T R¯222 < 2 to positive

0. or

we have these  satisfying T R¯ < 0, we can

recover  from  by backtracking the transformations.

Next, compute the Pareto spectrum of S := R¯11 - R¯12R¯22R¯1T2 and check which case S belongs to:

(CP1) S = R¯11 - R¯12R¯22R¯1T2 is strictly copositive. (CP2) S is copositive, but 1  0, 1 = 0 such that 1T S1 = 0. (CP3) 1  0 such that 1T S1 < 0.

Here, 1's are Pareto eigenvectors of S defined in Lemma B.2. If we have (CP3), we can declare (T3) because one can fix 2 = -R¯22R1T21 and get T R¯ = 1T S1 < 0. If the tests come out (PD1) and (CP1), by Lemma B.3 we have (T1). For the remaining cases, we conclude (T2).

B.5 PROOF OF LEMMA A.2

With the given ,

 0(k-1)×dx  1 =  [W1]k,·  , 1 =
0(dh -k)×dx

0k-1 [b1]k 0dh-k

, 2 = 0dy×(k-1)

-[W2]·,k

0dy×(dh-k) .

It is straightforward to check that for all i  [m],

0k-1

dY1(xi) = 2O(xi) + W2J (xi)(1xi + 1) = -[O(xi)]k[W2]·,k + W2 [O(xi)]k = 0.

0dh-k

From this, g(z, )T  =

i

T i

dY1

(xi)

=

0.

For

the

second

order

terms,

mm

T H(z, ) =



T i

dY2

(xi

)

+

1 2

dY1(xi)

2 2

i

=

i=1 i=1

m

i=1

T i

2J

(xi)(1

xi

+ 1)

=

m

i=1

T i

(-[O(xi)]k

[W2]·,k

)

=

-

m
i=1 [O(xi )]k 

T i

[W2 ]·,k .

From the fact that z is a FOSP of R, it follows that i  iO(xi)T = 0, so T H(z, ) = 0.

20

Under review as a conference paper at ICLR 2019

B.6 PROOF OF LEMMA A.3

The first equality is straightforward, because it follows from Si,k = {-1, +1} for all i  Bk(1) that taking union of {x¯iT vk  0} and {x¯Ti vk  0} will eliminate the inequality constraints for i  Bk(1).
For the next equality, we start by expressing U1 := vk | gk(z, vk)T vk = 0 as a linear combination of its linearly independent components. The set U1 can be expressed in the following form:

U1 = {v +

iv^i,k +

iv^i,k | v  Vk, i  Bk(1), i  R, and i  Bk(2), i  0},

iBk(1)

iBk(2)

where v^i,k  Vk  span{x¯j | j  Bk \ {i}} for all i  Bk(1,2). Additionally, for i  Bk(2), v^i,k is in the direction that satisfies i,k = sign(x¯Ti v^i,k). To see why U1 can be expressed in such a form, first note that at the moment SO-TEST is executed, it is already given that the point z is a FOSP. So, for any perturbation vk we have gk(z, vk)  Vk, and gk(z, vk)T v = 0 for any v  Vk. For the remaining components, please recall FO-INCREASING-TEST and Lemma A.1; v^i,k are flat extreme
rays, so they are the ones satisfying gk(z, vk)T vk = 0.

It remains to We show this

show that U2 := by proving U1 

{vk U2

| i  and U1c

Bk(U32)c,.

x¯iT

vk

=

0,

and i



Bk(2), i,kx¯Ti vk



0}

=

U1.

To show the first part, we start by noting that for any v  Vk, x¯iT v = 0 for i  Bk(2,3) because x¯i  Vk for these i's. Also, for all i  Bk(1), it follows from the definition of v^i,k that x¯Tj v^i,k = 0 for
all j  Bk(2,3). Similarly, for all i  Bk(2), x¯jT v^i,k = 0 for all j  Bk(2,3) \ {i}, and i,kx¯iT v^i,k > 0. Therefore, any vk  U1 must satisfy all constraints in U2, hence U1  U2.

For the next part, we prove that vk  U1c violates at least one constraint in U2. Observe that the whole vector space Rp can be expressed as

Rp = {v +

iBk(1) iv^i,k +

iBk(2) iv^i,k + w | w  Vk  span{v^i,k, i  Bk(1,2)}, v  Vk, i  Bk(1), i  R, and i  Bk(2), i  R}.

Therefore, any vk  U1c either has a nonzero component w in Vk  span{v^i,k, i  Bk(1,2)} or there exists i  B2(k) such that i < 0. By definition, v^i,k  span{x¯j | j  Bk(3)} for any i  Bk(1,2), which implies that Vk  span{v^i,k, i  Bk(1,2)} = span{x¯j | j  Bk(3)}. Thus, a nonzero component w  span{x¯j | j  Bk(3)} will violate some equality constraints in U2. Next, in case where i  B2(k) such that i < 0, this violates the inequality constraint corresponding to i.

21

