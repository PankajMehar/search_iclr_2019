Under review as a conference paper at ICLR 2019
PROVABLE GUARANTEES ON LEARNING HIERARCHICAL GENERATIVE MODELS WITH DEEP CNNS
Anonymous authors Paper under double-blind review
ABSTRACT
Learning deep networks is computationally hard in the general case. To show any positive theoretical results, one must make assumptions on the data distribution. Current theoretical works often make assumptions that are very far from describing real data, like sampling from Gaussian distribution or linear separability of the data. We describe an algorithm that learns convolutional neural network, assuming the data is sampled from a deep generative model that generates images level by level, where lower resolution images correspond to latent semantic classes. We analyze the convergence rate of our algorithm assuming the data is indeed generated according to this model (as well as additional assumptions). While we do not pretend to claim that the assumptions are realistic for natural images, we do believe that they capture some true properties of real data. Furthermore, we show that on CIFAR-10, the algorithm we analyze achieves results in the same ballpark with vanilla convolutional neural networks that are trained with SGD.
1 INTRODUCTION
The success of deep convolutional neural networks (CNN) has sparked many works trying to understand their behavior. As various theoretical studies have shown, learning deep networks is computationally hard in the general case, when no assumptions on the distribution of the data are taken (see for example Livni et al. (2014)). In practice, learning CNNs is done successfully using simple gradient-based optimization algorithms like SGD. Hence, to provide a theoretical analysis that will explain the practical success of deep learning, one must make assumptions on the distribution of the learned data. Currently, theoretical works in the literature of deep learning make rather strong assumptions, that clearly do not capture the properties of natural data. For example, many works assume that the examples are sampled from a Gaussian distribution, an assumption that is very far from describing distributions on natural images. Other works assume linear separability of the data, which clearly does not hold for any rich enough dataset.
In this work, we assume the data is generated from a deep generative model. According to this model, the examples are generated in a hierarchical manner: each example (image) is generated by first drawing a high-level semantic image, and iteratively refining the image, each time generating a lower-level image based on the higher-level semantics from the previous step. Similar models were suggested in other works as good descriptions of natural images encountered in real world data. While we do not claim that natural images actually come from such distribution, we believe that it captures some key properties of real world data. Importantly, the problem we study is not trivially learned by simple "shallow" learning algorithms.
Our work analyzes the training of a CNN on data from this generative distribution. In a shallow case, where the generative model has only two levels of hierarchy, we analyze the behavior of standard gradient-descent. In the deep case, where our model can have many levels of hierarchy, we analyze a layerwise optimization algorithm, proving its convergence under the assumed generative model (as well as additional, admittedly strong, assumptions). The algorithm we analyze is somewhat different than optimization algorithms that are commonly used in practice, and may seem "tailored" to solve the problem of learning our generative heirarchical model. We show that implementing this algorithm to learn real-world data (CIFAR-10 dataset) achieves performance that are in the same ballpark as a vanilla CNN trained with SGD-based optimizer. This result hints that our model and algorithm indeed capture properties of distributions and algorithms that are common in practice.
1

Under review as a conference paper at ICLR 2019
2 RELATED WORK
Any theoretical work that aims to give positive results on learning deep networks must make assumptions on the data distribution and on the learning algorithm. We can roughly divide such works into three categories: (1) works that study practical algorithms (SGD) solving "simple" problems that can be otherwise learned with "shallow" algorithms. (2) works that study problems with less restrictive assumptions, but using algorithms that are not applicable in practice. (3) works that study a generative model similar to ours, but either give no theoretical guarantees, or otherwise analyze an algorithms that are not applicable for learning CNNs on image data.
Trying to study a practically useful algorithm, Daniely (2017) proves that SGD learns a function that approximates the best function in the conjugate kernel space derived from the network architecture. Although this work provides guarantees for a wide range of deep architectures, there is no empirical evidence that the best function in the conjugate kernel space performs at the same ballpark as CNNs. The work of Andoni et al. (2014) shows guarantees on learning low-degree polynomials, which is learnable via SVM or direct feature mapping. Other works study shallow (one-hidden-layer) networks under some significant assumptions. The works of Gori & Tesi (1992); Brutzkus et al. (2017) study the convergence of SGD trained on linearly separable data, which could be learned with the Perceptron algorithm, and the works of Brutzkus & Globerson (2017); Tian (2017); Li & Yuan (2017); Zhong et al. (2017) assume that the data is generated from Gaussian distribution, an assumption that clearly does not hold in real-world data. The work of Du et al. (2017) extends the results in Brutzkus & Globerson (2017), showing recovery of convolutional kernels without assuming Gaussian distribution, but is still limited to the regime of shallow two-layer network.
Another line of work aims to analyze the learning of deep architectures, in cases that exceed the capacity of shallow learning. The works of Livni et al. (2014); Zhang et al. (2015; 2016a) show polynomial-time algorithms aimed at learning deep models, but that seem far from performing well in practice. The work of Zhang et al. (2016b) analyses a method of learning a model similar to CNN which can be applied to learn multi-layer networks, but the analysis is limited to shallow two-layer settings, when the formulated problem is convex.
Finally, there have been a few works suggesting distributional assumptions on the data that are similar in spirit to the generative model that we analyze in this paper. Again, these works can be largely categorized into two classes: works that provide algorithms that have theoretical guarantees but are not applicable for learning CNNs, and works that show practical results without theoretical guarantees. The work of Arora et al. (2014) shows a provably efficient algorithm for learning a deep representation, but this algorithm seems far from capturing the behavior of algorithms used in practice. Our approach can be seen as an extension of the work of Mossel (2016), who studies Hierarchal Generative Models, focusing on algorithms and models that are applicable to biological data. Mossel (2016) suggests that similar models may be used to define image refinement processes, and our work shows that this is indeed the case, while providing both theoretical proofs and empirical evidence to this claim. Finally, the works of Tang et al. (2012); Patel et al. (2016); Van den Oord & Schrauwen (2014) study generative models similar to ours, with promising empirical results when implementing EM inspired algorithms, but giving no theoretical foundations whatsoever.
3 DISTRIBUTIONAL ASSUMPTIONS
In this paper, we are concerned with the problem of learning binary classification of images. Assume we are given a sample S = {(X1, y1), . . . , (Xn, yn)} from some distribution D on Rm×m × Y, where Y = {±1} is our labels. We wish to learn a CNN model that will classify the images correctly. As noted, if we hope to give any guarantees on the success of learning such model, we must make strong assumptions on the distribution D. One naive assumption on D is that it is linearly separable: there exists W   Rm×m such that y W , X  1 for (X, y)  D, where we denote A, B = tr(A B). Clearly, such assumption does not hold for any rich enough distribution of natural images (as linear classifiers give poor results on natural image datasets).
To provide a more realistic assumption, we will instead assume that the images in D are generated from a latent distribution G0, such that G0 is linearly separable. Thus, sampling an image (X, y)  D is equivalent to sampling a latent representation (X(0), y)  G0, and then sampling (X, y)  GX(0) , where GX(0) is a distribution dependent on X(0). Note that in this case, the distribution
2

Under review as a conference paper at ICLR 2019

···

X (0) C05×4

X (1) C110×8

U{ ,

,

X R640×512
,...}

C0 = { , , , , , . . . }, C1 = { , , , . . . }

Figure 1: Generative model schematic description

D can be very complex, although the images sampled from it have a latent representation that is relatively simple. To give a concrete description of the latent representation, we will assume that G0 is a distribution over "semantic images": images where each "pixel" represents a semantic class. Formally, we will assume that X(0)  C0m0×m0 , where C0 is some finite set of "semantic classes".
Now, we want to describe the generative process of generating the image X from the semantic representation X(0). This process is done in a hierarchical manner: Given the high-level semantic representation X(0), where each "pixel" represents a semantic class (for example, background, sky, grass etc.), we generate a lower level image X(1), where each patch comes from a distribution depending on each "pixel" of the high-level representation, generating a larger semantic image (lower level semantic classes for natural images could be: edges, corners, texture etc.). We can repeat this process iteratively any number of times, each time creating a larger image of lower level semantic classes, thus generating a sequence of increasingly large semantic images X(0), X(1), . . . , X(d). Since D is a distribution over greyscale images, we will assume that the last iteration of this process samples patches over R, i.e X = X(d)  Rm×m. This model is described schematically in figure 1, with a formal description given in section 3.1. In section 3.2 we describe a synthetic example of digit images generated according to this model.

3.1 FORMAL DESCRIPTION

Here, we will give a detailed description of the generative process described above. To generate an
example, we start by sampling the label y  U (Y), where U (Y) is the uniform distribution over the
set of labels. Given y, we generate a small image with m0 × m0 pixels, where each pixel belongs to a set C0. Elements of C0 corresponds to semantic entities (e.g. "sky", "grass", etc.). The generated image, denoted X(0)  C0m0×m0 , is sampled according to some simple distribution Dy. Next, we generate a new image X(1)  C1m0s×m0s as follows. Pixel i in X(0) corresponds to some c  C0. For every such c, let Dc be the uniform distribution over a finite set of patches Sc(0)  C1s×s, where we refer to s as a "patch size". We assume that the sets {Sc(0)}cC0 are disjoint, and each one is of size k. So, pixel i in X(0) whose value is c  C0 generates a patch of size s in X(1) by sampling the patch according to Dc. This process continues, yielding images X(2), . . . , X(d) whose sizes are m0s2 × m0s2, . . . , m0sd × m0sd. Each pixel in level i comes from Ci, and each patch comes from Sc(i-1) for some c  Ci-1. We assume that Cd = R, so the final image is over the reals. The
observed example is the pair (X(d), y). We denote the distribution generating the image of level i
by Gi.

As noted, we assume that G0, the distribution that generates the high-level semantic images, is

linearly separable. As G0 is a distribution over semantic images, i.e over C0m×m × Y, we need to

define what "linear separability" means in this case. To do this, we assign to the i-th class in C0 the

unit and

vector ei  define linear

R|C0|. Then separability

we represent X  with respect to the

C0m×m with an equivalent tensor X tensor representation. Namely, there

 R|C0|×m×m, exists W such

that for (X, y)  G0 it holds that y X, W  1, where X, W := i,j,k Xi,j,k · Wi,j,k.

3

Under review as a conference paper at ICLR 2019

Figure 2: Left: Image generation process example. Right: Synthetic examples generated.

Notice that our semantic classes partition the patches in the image into disjoint sets. There are

various ways to partition the patches, some of which might in fact generate the same distribution.

For the analysis, we add an assumption that ensures that the semantic classes defined in the model are

different enough from each other. We identify each class with a matrix that captures the frequency

of its appearance in the image, with respect to the label of the image. For this, we define the "labeled

frequency matrix" of class c  Ci to be the matrix Fc  Rm0si×m0si defined as follows: for every c
we denote by Ic the operator that takes a matrix as its input and replaces every element of the input
by the boolean that indicates whether it equals to c. Then, we define: Fc := E(Z,y)Gi [yIc(Z)]. Notice that Fc is the "mean" image over the distribution for every given semantic class c. For

example, semantic classes that tend to appear in the upper-left corner of the image for positive

images will have positive values in the upper-left entries of Fc. We assume that the vectors {Fc}cCi

are linearly independent in pairs. For each c1, c2  Ci we denote the angle between Fc1 and Fc2

by: (Fc1 , Fc2 ) := arccos

Fc1 Fc2 Fc1 F Fc2 F

. Denote  := mini,c1,c2Ci (Fc1 , Fc2 ) and  :=

mini,cCi Fc F . From the linear independence assumption it follows that both  and  are strictly

positive. The runtime of the algorithms described in the next sections depend on 1/ and 1/.

3.2 SYNTHETIC DIGITS EXAMPLE
To demonstrate our generative model, we use a small synthetic example to generate images of digits. In this case, we use a three levels model, where semantic classes represent lines, corners etc. In the notations above, we use:
C0 = { , , , , , , , , } , C1 = { , , , , , } , C2 = R We define Deven, Dodd to be the uniform distributions over the even/odd digital representations:
Deven = U { , , , , }, Dodd = U { , , , , }

Now, in the second level of the generative model, each pixel in C0 can generate one of four possible manifestations. For example, for the pixel , we sample over: , , , . Similarly, in the final level we sample for each c  C1 from a distribution Dc supported over 4 elements. For example, for the pixel , we sample over: , , , .
Notice that though this example is extremely simplistic, it can generate 49 examples per digit in the first level, and 490 examples for each digit in the final layer, amounting to 9 · 490  1.38 · 1055 different examples. Figure 2 shows the process output.

4 TWO-LEVEL MODEL
As a warm-up, we first limit ourselves to observing distributions with only two levels in the hierarchy. Hence, the process of generating examples is simply: G0 (X(0), y) G1 (X(1), y). Furthermore, in this section we add an important assumption on the data distribution:

4

Under review as a conference paper at ICLR 2019

Patch Orthonormality: We assume that the set of patches that compose the images generated by G1 is orthonormal. Recall that we denoted Sc(0)  C0s×s the set of patches that are sampled for each pixel of class c. Our assumption means that P1, P2  {Sc(0)}cC0 we have P1, P2 = 1P1=P2 .
Notice that this assumption implies the following lemma:

LekmWma

1 ,

SVM finds

Given the where W
a classifier

patch orthonormality assumption, G1 is linearly separable with margin

is the linear separator of G0 (see section 3.1). with zero classification loss on this distribution

Consequently, running linear in O(k W 2) iterations.

While usually in classification problems our goal is to minimize the training loss, in this case we would like to find an algorithm that instead recovers the latent representation X(0) given the raw image X(1). The reasons for this requirement will become clear in the next section. We will prove that training a two layer linear CNN with gradient-descent (GD) on distribution G1 implicitly recovers the latent representation, even if G0 is not linearly separable. Note that this is a rather surprising result, as we are only observing (X(1), y), and have no access to the latent representation
X(0). The following section will give the details of this result: in section 4.1 we describe the details
of the GD algorithm, and in section 4.2 we describe the theoretical analysis of this algorithm.

4.1 ALGORITHM: GD ON TWO-LAYER CNN

Recall that for (X, y)  G1, X is an image of size sm0 × sm0, that is composed of s × s patches generated from a high-level semantic image of size m0 × m0. To simplify notation, we consider
X to be "reshaped" such that each column is a patch in the original image (the so called "im2col" operation), so X  Rs2×m02 . Given K  Rs2×n, W  Rm20×n, we define a convolutional subnet to be a function NK,W : Rs2×m20  R such that: NK,W (X) = W , K X .

This is equivalent to a convolution operation on an image, followed by a linear weighted sum: multiplying X by K is equivalent to performing a non-overlapping convolution operation with kernels k1, . . . , kn on the original image (where ki is the i-th vector of matrix K). Flattening the resulting matrix and multiplying by the weights in W yields the second linear layer.

The top linear layer of the network outputs a prediction for the label y  Y. The network is trained with respect to the loss LSK,W on a given set of examples S, defined as the expected value of some label dependent loss function y : R  R

LSK,W = E(X,y)S [ y(NK,W (X))]

For the analysis, we use the loss y(y^) = -yy^. This loss simplifies the analysis, and seems to capture a similar behavior to other loss types used in practice.

Although in practice we perform a variant of SGD on a sample of the data to train the network, we perform the analysis with respect to the population loss: LK,W = E(X,y)G [ y(NK,W (X))]. We denote Kt the weights of the first layer of the network in iteration t, and denote W0 the initial weights of the second layer. For simplicity of the analysis, we assume that only the first layer of the

network is trained, while the weights of the second layer are fixed. Thus, we perform the following

update

step

at

each

iteration

of

the

gradient

descent:

Kt

=

Kt-1

-



 K

LKt-1,W0 .

We initialize W0  N (0, 1) and each n-dimensional column of K0 is sampled from the uniform

distribution

on

the

sphere

of

radius

 2n

,

where



is

a

parameter

of

the

algorithm.

4.2 THEORETICAL ANALYSIS
In this section, we will consider a more general case, where we do not assume that G0 is linearly separable. As noted, our main claim in this section is that training the two-layer Conv net implicitly recovers the latent semantic representation of the image. Specifically, we show that gradient-descent finds an embedding of the observed patches into a space such that patches from the same semantic class are close to each other, while patches from different classes are far. Note that this property is indeed enough to recover the latent representation, as running a trivial clustering algorithm on the embedded patches would reconstruct the latent image. Recall that we do not have access to the

5

Under review as a conference paper at ICLR 2019

latent distribution, and thus cannot possibly learn such embedding directly. This surprising property of gradient descent is the key feature that allows our main algorithm (described in the next section) to learn the high-level semantics of the images. Our claim is given in the following theorem:

Theorem 1 Let ,  as described in section 3.1. Assume we train a two-layer network of size n >

2 0.23

log(

|C| 

)

with

respect

to the population loss on distribution G1, with learning rate

,

for T

>

(+2)k 

iterations,

for

some



>

0.

Assume

that

the

training

is

as

described

in

section

4.1,

where

the parameter  of the initialization is also described there. Then with probability of at least 1 - :

1. for each c  C0, for every x1, x2  Sc(0) we get KT · x1 - KT · x2 <  2. for c1, c2  C0, if c1 = c2, for every x1  Sc(10), x2  Sc(20), we get KT ·x1 -KT ·x2 > 

5 DEEP MODEL

Now, assume we are given data from a deep generative distribution as described in section 3 (with d  2), and our goal is to learn a classifier that predicts the label for each image. A reasonable approach, given the properties of the above distribution, would be to try to infer from the raw image the higher-level semantic representations. If given an example (X, y) we succeed to infer the sequence that generated it, X(0), X(1), . . . , X(d), we could then use SVM on the high-level representation, and learn to infer its label.
Given the results of the previous section, we can recover these semantic representations by training a two-layer Conv net and applying a trivial clustering on the resulting patches. Note that in general we do not assume that all the patches are orthogonal (such a property will make the whole model linearly separable). Thus, to apply the results of section 4.2, we would use the clustering algorithm both for standard clustering and also as an "orthogonalization" step, that will map different clusters to orthonormal vectors.
We next describe in details the full algorithm (section 5.1), and show that it finds a network that achieves zero loss in polynomial time (section 5.2).

5.1 ALGORITHM DESCRIPTION
The algorithm we suggest is built from three building-blocks composed together to construct the full algorithm: (1) clustering algorithm, (2) gradient-based optimization of two-layer Conv net and (3) a simple classification algorithm. In order to expose the latent representation of each layer in the generative model, we perform the following iteratively:
(1) Run a k-means algorithm on patches of size s × s from the input images defined by the previous step (or the original images in the first step), w.r.t. the cosine distance, to get ki cluster centers. (2) Run a convolution operation with the cluster centroids as kernels, followed by ReLU with a fixed bias and a pooling operation. This will result in mapping the patches in the input images to (approximately) orthogonal vectors in an intermediate space Rki . (3) Initialize a 1x1 convolution operation, that maps from ki channels into n channels, followed by a linear layer that predicts Y. We train this two-layer subnet using gradient-descent. As an immediate corollary of the analysis in the previous section, this step implicitly learns an embedding of the patches into a space where patches from the same semantic class are close to each other, while patches from different classes are far away. This lays the groundwork for the clustering step of the next iteration. (4) "Throw" the last linear layer, thus leaving a trained block of Conv(s×s)-ReLU-Pool-Conv(1×1) which finds a "good" embedding of the patches of the input image, and repeat the process again, where the output of this block is the input to step 1.
Finally, after we perform this process for d times, we get a network of depth d composed from Conv(s × s)-ReLU-Pool-Conv(1 × 1) blocks. Then, we feed the output of this (already trained) network to an SVM algorithm that learns a linear classifier, training it to infer the label y from the semantic representation that the convolution network outputs. We now describe the building blocks for the algorithm, followed by the definition of the complete algorithm.

6

Under review as a conference paper at ICLR 2019

Clustering. The first block of the algorithm is the clustering step. We denote KMEANS(S, k) to be
the output of the k-means++ algorithm (Arthur & Vassilvitskii (2007)) running on sample S to find k clusters, w.r.t the cosine similarity. We assume the algorithm returns a mapping S : Rs  Rk, such that if xi, xj  S are in the same cluster then S(xi) = S(xj), and otherwise S(xi)  S(xj).

For the consistency with common CNN architecture, we can use the centroids of each cluster as kernels for a convolution operation. Combining this with ReLU with a fixed bias and a pooling operation gives an operation that maps each patch to a single vector, where vectors of different patches are approximately orthogonal.

Two-Layer Network Algorithm We denote TLGD(S, T, , n, ) the GD algorithm, training a twolayer Conv net of width n, on sample S, with learning rate  for T interations. This algorithm returns the weights of the first layer KT . The details of this algorithm are described in section 4.1.

Classification Algorithm Finally, the last building block of the algorithm is a classification stage, that is used on top of the deep convolution architecture learned in the previous steps. As we show, at this stage the examples are linearly separated, so we can use the SVM algorithm to find large margin linear separator. We denote SVM(S) the output of running SVM on sample S.

Complete Algorithm Utilizing the building blocks described previously, our algorithm learns a

deep CNN layer after layer. This network is used to infer the label for each image. The algorithm

is described formally in algorithm 1. In the description, we use the notation   A to denote the

operation vectors in

oKf1ma1p.pFlyoirnmgaallym: apA::=K0m[0(A:,i·mK01m...1(i+on1)·amt0e)n]isor

A,

replacing

patches

of

size

m0

by

Algorithm 1 Deep Layerwise Clustering
input: numbers , T, n, , k1, . . . , kd sample S = {(X1, y1), . . . (XN , yN )}  Rm0sd×m0sd × Y
hd  id for i = d . . . 1 do
set Si  {(hi(X1), y1), . . . , (hi(XN ), yN )} // construct sample using the current network set Pi  {patches of size s × s from Si} // generate patches from the current sample set i  KMEANS(Pi, ki) // cluster the sampled patches set S^i  {(i  hi(X1), y1), . . . , (i  hi(XN ), yN )} // map patches to orthogonal vectors using i if i > 1 then
set Ki-1  TLGD(S^i, T, , n, ) // train a two-layer net to find a "good" embedding for the patches
set hi-1  Ki-1(i  hi) // add the current block to the network end if
end for set h  SVM(S^0) return h  h1

5.2 THEORETICAL ANALYSIS

In this section we show that, algorithm 1 learns in polynomial time (with high probability) a network model that correctly classifies the examples according to their labels.

Recall that we have shown that training a two-layer network implicitly learns an embedding of the patches into a space where patches from the same semantic class are close to each other, while patches from different classes are far. Using this, we show that performing clustering + two-layer network training iteratively, layer by layer, leads to revealing the underlying model, and hence the algorithm's convergence. We claim that algorithm 1 successfully learns a model that correctly classifies the examples sampled from the observed distribution Gd. This is stated in the following theorem:

Theorem 2 Fix  > 0, and let 

=

 2d

.

Denote C

:=

maxi<d |Ci|

the

maximal number


of semantic classes in any level i.

Choose  =

1 s

,

n

>

2 0.23

log(

C 

),

T

>

(C/

 +2)k 

,

k1 = |C0|, . . . , kd-1 = |Cd-2|, kd = k|Cd-1|. Then w.p  1 - , running algorithm 1 with parame-

ters , , T, n, , k1, . . . , kd on distribution Gd returns hypothesis h s.t P(X,y)Gd (h(X) = y) = 0, and runs in time O d(C2k2 + T ) + k W 2 .

7

Under review as a conference paper at ICLR 2019
6 EXPERIMENTS
A common criticism of theoretical results on deep learning is that they fail to account for the empirical success of deep networks. Indeed, negative results show that learning deep networks is computationally hard, while in practice efficient algorithms like SGD achieve remarkably good performance. Positive results, on the other hand, often make very strong assumptions that clearly do not hold in practice, like assuming the inputs are sampled from Gaussian distribution, or that they are linearly separable. Other positive results make less restrictive assumptions, but analyze algorithms that are very far from algorithms that are used in practice. At first glance, our work may seem to suffer from the same common drawbacks of positive theoretical results: we describe a distribution that is admittedly synthetic, and for deep models we analyze an algorithm that seems "tailored" to learn this distribution. To show that our model and algorithm do capture properties of distributions and algorithms used in practice, we implemented our algorithm to learn a CNN on the CIFAR-10 dataset, comparing it to a vanilla CNN trained with a common SGD-based optimization algorithm.
As our aim is to show that our algorithm achieves comparable result to a vanilla SGD-based optimization, and not to achieve state-of-the-art results on CIFAR-10, we do not use any of the common "tricks" that are widely used when training deep networks (such as data augmentation, dropout, batch normalization, scheduled learning rate, averaging of weights across iterations etc.). We implemented our algorithm by repeating the following steps twice: (1) Sample N patches of size 3x3 uniformly from the dataset. (2) For some , run the K-means algorithm to find cluster centers c1 . . . c . (3) At this step, we need to associate each cluster with a vector in R , such that the image of this mapping is a set of orthonormal vectors, and then map every patch in every image to the vector corresponding to the cluster it belongs to. We do so by performing Conv3x3 operation with the kernels c1 . . . c , and then perform ReLU operation with a fixed bias b. This roughly maps each patch to the vector ei, where i is the cluster the patch belongs to. (4) While our analysis corresponds to performing the convolution from the previous step with a stride of 3, to make the architecture closer to the commonly used CNNs (specifically the one suggested in the Tensorflow implementation Google-Brain (2016)), we used a stride of 1 followed by a 2x2 max-pooling. (5) Randomly initialize a two layered linear network, where the first layer is Conv1x1 with output channels, and the second layer is a fully-connected Affine layer that outputs 10 channels to predict the 10 classes of CIFAR-10. (6) Train the two-layers with Adam optimization (Kingma & Ba (2014)) on the cross-entropy loss, and remove the top layer. The output of this block is the output of these steps.
Repeating the above steps twice yields a network with two blocks of Conv3x3-ReLU-Pool-Conv1x1. We feed the output of these steps to a final classifier that is trained with Adam on the cross entropy loss for 100k iterations, to output the final classification of this model. We test two choices for this classifier: a linear classifier and a three-layers fully-connected neural network. Note that in both cases, the output of our algorithm is a vanilla CNN. The only difference is in the training algorithm. To calibrate the various parameters that define the model, we first perform random parameter search, where we use 10k examples from the train set as validation set (and the rest 40k as train set). After we found the optimal parameters for all the setups we compare, we then train the model again with the calibrated parameters on all the train data, and plot the accuracy on the test data every 10k iterations. The parameters found in the parameter search are listed in appendix C.
We compared our algorithm to several alternatives. First, the standard CNN configuration in the Tensorflow implementation with two variants: CNN+(FC+ReLU)3 is the Tensorflow architecture and CNN+Linear is the Tensorflow architecture where the last three fully connected layers were replaced by a single fully connected layer. The goal of this comparison is to show that the performance of our algorithm is in the same ballpark as that of vanilla CNNs. Second, we use the same two architectures mentioned before, but while using random weights for the CNN and training only the FC layers. Some previous analyses of the success of CNN claimed that the power of the algorithm comes from the random initialization (see Daniely (2017)), and only the training of the last layer matters. As is clearly seen, random weights are far from the performance of vanilla CNNs. Our last experiment aims at showing the power of the two layer training in our algorithm (step 6). To do so, we compare our algorithm to a variant of it, in which step 6 is replaced by random projections (based on Johnson-Lindenstrauss lemma). We denote this variant by Clustering+JL. As can be seen, this variant gives drastically inferior results, showing that the training step of Conv1x1 is crucial, and finds a "good" embedding for the process that follows, as is suggested by our theoretical analysis. A summary of all the results is given in figure 3.
8

Under review as a conference paper at ICLR 2019

Classifier CNN CNN(Random) Clustering+JL Ours

Accuracy(FC) 0.759 0.645 0.586 0.734

Accuracy(Linear) 0.735 0.616 0.588 0.689

Figure 3: Results of various configurations on the CIFAR-10 dataset

REFERENCES
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural networks. In International Conference on Machine Learning, pp. 1908­1916, 2014.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pp. 584­592, 2014.
David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pp. 1027­1035. Society for Industrial and Applied Mathematics, 2007.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.
Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Information Processing Systems, pp. 2419­2427, 2017.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017.
G. Google-Brain. Tensorflow. https://www.tensorflow.org/tutorials/deep_cnn, 2016.
Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(1):76­86, 1992.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pp. 855­863, 2014.
Elchanan Mossel. Deep learning and hierarchal generative models. arXiv preprint arXiv:1612.09057, 2016.
Rafail Ostrovsky, Yuval Rabani, Leonard J Schulman, and Chaitanya Swamy. The effectiveness of lloyd-type methods for the k-means problem. In Foundations of Computer Science, 2006. FOCS'06. 47th Annual IEEE Symposium on, pp. 165­176. IEEE, 2006.
Ankit B Patel, Minh Tan Nguyen, and Richard Baraniuk. A probabilistic framework for deep learning. In Advances in Neural Information Processing Systems, pp. 2558­2566, 2016.
Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey Hinton. Deep mixtures of factor analysers. arXiv preprint arXiv:1206.4635, 2012.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.

9

Under review as a conference paper at ICLR 2019 Aaron Van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep
gaussian mixture models. In Advances in Neural Information Processing Systems, pp. 3518­3526, 2014. Yuchen Zhang, Jason D Lee, Martin J Wainwright, and Michael I Jordan. Learning halfspaces and neural networks with random initialization. arXiv preprint arXiv:1511.07948, 2015. Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly learnable in polynomial time. In International Conference on Machine Learning, pp. 993­1001, 2016a. Yuchen Zhang, Percy Liang, and Martin J Wainwright. Convexified convolutional neural networks. arXiv preprint arXiv:1609.01000, 2016b. Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
10

Under review as a conference paper at ICLR 2019

A PROOF OF LEMMA 1 AND THEOREM 1

Denote m = m0s the size of the images sampled from G1 (these images are in Rm×m). As in section 4, we will consider the "reshaped" version of images sampled from G1, so we will have X  Rs2×m02 . Equivalently, patches of the sampled images, that belong to sets {Sc(0)}cC, will be vectors in Rs2 . Finally, we denote (Z, y)  G0 the latent images in the generative process, where Z  C0m0×m0 . Here we will instead consider these images as vectors in C0m20 . Similarly, the labeled frequency matrix of class c is Fc  Rm0×m0 , and we will consider it to be a vector Fc  Rm20 . In the proof we use the notations Fc to denote the negative labeled frequency matrix:
Fc := E(Z,y)Gi [-yIc(Z)]

We will start by proving Lemma 1:

Proof of Lemma 1: For simplicity, we can assume C0 = [C], for some natural C. Therefore, the linear separator of G0 is a matrix W   RC×m02 . For (Z, y)  G0 define W , Z =
c[C] Wc, Ic(Z) , where Ic is the operator that replaces each entry of Z with the boolean 1
if it equals c, or 0 otherwise. Notice that the definition of linear separability in section 3 is equivalent to y W , Z  1 for every (Z, y)  G0.

Denote P 1, . . . , P C  Rs2 such that P i = P Si(0) P . From the orthonormality of the patches, for every P  Si(0) we have P · P j = 1i=j. Now, denote T : Rs2×m20  R such that:

T (X) =

Wj, P j X

j[C]

For (X, y)  G1, denote Z the latent representation of X, then by the definition of the generative

distribution it is easy to verify that Ij(Z) = P j X, and therefore:

T (X) =

Wj, Ij (Z)

j [C ]

Since T is a linear function such that

T

2 F

=k

W

2 F

,

and

using

standard

results

on

the

SVM

algorithm, the conclusion of the lemma follows.

In the rest of this section, we will prove Theorem 2. We use the notations wi(t), ki(t) to denote the i-th columns of Wt, Kt respectively.

For some class c  C0 and for some patch x  Sc(0), denote fx a function that takes a matrix X and returns a vector fx (X) such that the i'th element of fx (X) is the 1 if the i'th column of X, denoted xi, equals to x and 0 otherwise. That is,

 1x1=x 

fx

(X )

:=

 

...

 

1xm=x

Notice that from the orthonormality of the observed columns of X it follows that: fx (X) = X x .

We begin with proving the following technical lemma.

Lemma 2 For each c  C0 and for each x  Sc(0) we have: 1
E(X,y)G1 [-yfx (X)] = k Fc

Proof Denote DZ the distribution of (X(1), y)  G1 conditioned on X(0) = Z (the distribution of the images generated from the latent image Z). Observe that

1

E(X,y)G1 [-yfx (X)] = 2

-yEZDy [EXDZ [fx (X)]]

y=±1

11

Under review as a conference paper at ICLR 2019

Recall that for each c  C0, we have |Sc(0)| = k. Therefore, for each j  [m20] we have:

11

2 -yEzDy [EXDZ [fx (X)j ]] = 2 -yEZDy EXDZ 1xj=x

y=±1

y=±1

1

= 2

-yEZDy [PXDZ (xj = x )]

y=±1

11

= 2

-yEZDy k 1zj =c

y=±1

11

1

=· k2

-yEZDy [Ic(Z)j ] = k [Fc]j

y=±1

The next lemma reveals a surprising connection between the gradient and the vectors Fc.

Lemma 3 for every c  C0 and for every x  Sc(0):

x

 ki

LK,W

=

1 k wi

· Fc

Proof For a fixed X and W , denote y^(K) = NK,W (X). Note that:

 ki y^(K) = Xwi

So for x  Sc(0) we have:

x

·  y^ = (x ) ki

Xwi = wi · fx (X)

Combining the above with the definition of the loss function, y(y^) = -yy^, and with Lemma 2 we get:

 x ki LKt,W0 = E(X,y)G1
= E(X,y)G1

 x ki y(y^)

-y x

 y^

ki

= E(X,y)G1 -y wi(0) · fx (X)

= wi(0) · E(X,y)G1 [-yfx (X)]

=

1 k

wi(0)

·

Fc

As an immediate corollary we obtain that a gradient step does not change the projection of the kernel on two vectors that correspond to the same class (both are in the same Sc(0)).

Corollary 1 For every t  0, i  [n], for every semantic class c  C and for every x1, x2  Sc(0) it holds that: |ki(t+1) · x1 - ki(t+1) · x2| = |ki(t) · x1 - ki(t) · x2|.

Proof From Lemma 3 we can conclude that for a given c  C, for every x1, x2  Sc(0) we get:

x1

 ki

LKt

,W0

=

x2

 ki

LKt

,W0

12

Under review as a conference paper at ICLR 2019

From the gradient descent update rule:

ki(t+1)

=

ki(t)

-

 
ki

LKt ,W0

And therefore:

|ki(t+1)

·

x1

-

ki(t+1)

·

x2|

=

|(kt(i)

-



 ki

LKt,W0 )

·

x1

-

(ki(t)

-



 ki

LKt,W0 )

·

x2|

=

|ki(t)

·

x1

-

ki(t)

·

x2

-

(

 ki

LKt,W0 x1

-



 ki

LKt,W0 x2)|

= |ki(t) · x1 - ki(t) · x2|

Next we turn to show that a gradient step improves the separation of vectors coming from different semantic classes.
Lemma 4 Fix c1, c2  C0. Recall that we denote (Fc1 , Fc2 ) to be the angle between the vectors Fc1 , Fc2 . Then, with probability (Fc1 , Fc2 )/ on the initialization of wi(0) we get:
sign(wi(0) · Fc1 ) = sign(wi(0) · Fc2 )
Proof Observe the projection of wi(0) on the plane spanned by Fc1 , Fc2 . Then, the result is immediate from the symmetry of the initialization of wi(0).

Lemma 5

Fix c1

=

c2



C0.

Then,

with

probability

of

at

least

0.23

(Fc1 ,Fc2 

)

we get for every

x1  Sc(10), x2  Sc(20):

|ki(T )

·

x1

-

ki(T )

·

x2|

>

1 T
k

Fc1

+ Fc2 2

- 2

Proof Notice that since wi(0)  N (0, 1), we get that wi(0) · Fcj  N (0, Fcj 2) for j  {1, 2}.

Therefore,

the

probability

that

wi(0) · Fcj

deviates

by

at

most

1 2

-std

from

the

mean

is

erf( 1 ).
22

Thus,

we get that:

P (|wi(0) · Fcj | 

Fcj

) = erf( 1 ) 22

And using the union bound:

P (|wi(0) · Fc1 | 

Fc1

 |wi(0) · Fc2 | 

Fc2

)  2erf( 1 ) < 0.77 22

Thus,

using

Lemma

4,

we

get

that

the

following

holds

with

probability

of

at

least

0.23

(Fc1 ,Fc2 

)

:

· |wi(0) · Fc1 | > Fc1 · |wi(0) · Fc2 | > Fc2 · sign(wi(0) · Fc1 ) = sign(wi(0) · Fc2 )

Assume w.l.o.g that wi(0) · Fc1 < 0 < wi(0) · Fc2 , then using Lemma 3 we get:

T
ki(T )x1 = ki(0)x1 - 

 k(i) x1LKt,W0

t=1

T
= ki(0) - 

1 k

wi(0)

·

Fc1

t=1

=

ki(0)

-

1 k

T

wi(0)

· Fc1

>

1 T
k

Fc1 2

-

13

Under review as a conference paper at ICLR 2019

In a similar fashion we can get:

ki(T )x2

<

-1 k

T

Fc2 2

+

And thus the conclusion follows:

ki(T )x1 - ki(T )x2

>

1 T
k

Fc1

+ 2

Fc2

- 2

Finally, we are ready to prove the main theorem.
Proof of Theorem 1. We show two things:

1. Fix c  C0. By the initialization, we get that for every x1, x2  Sc(0) and for every i  [n]:

|x1

·

ki(0)

- x2

· ki(0)|

<

 n

Using Corollary 1, we get that:

|x1

· ki(T )

- x2

· ki(T )|

<

 n

And thus:

KT x1 - KT x2 < 

2.

Let c1

=

c2



C0.

Assume T

>

(+2)k 

.

For i



[n], from Lemma 5 we get that with

probability

of

at

least

0.23

(Fc1 ,Fc2 

)

>

0.23

 

for

every

x1



Sc(10), x2



Sc(20):

|ki(T )x1

- ki(T )x2|

>

1 T
k

Fc1

+ Fc2 2

-

2

>

1 T 

-

2

>



k

For a given c1 = c2  C0, denote the event:

Ac1,c2 = {i  [n] : |ki(T ) · x1 - ki(T ) · x2| < , x1  Sc(10), x2  Sc(20)} Then, from what we have showed, it holds that:

P

(Ac1,c2 )

<

(1

-

 0.23


)n



exp(-0.23n  

)

Using the union bound, we get that:

P

(c1

=

c2



C0

s.t

Ac1,c2 )

<

exp(-0.23n  

)|C0|2

Choosing

n

>

2 0.23

log(

|C| 

)

we

get

P (c1

=

c2



C0

s.t

Ac1,c2 )

<

.

Now,

if

for

every

c1 = c2  C0 the event Ac1,c2 doesn't hold, then clearly for every x1  Sc(10), x2  Sc(20)

we would get KT · x1 - KT · x2 > , and this is what we wanted to show.

B PROOF OF THEOREM 2

Our algorithm uses the k-means++ algorithm, which is a variant of Lloyd's algorithm where the

initial cluster centers are chosen with probability proportional to their distance to the closest cluster

that was already chosen. The algorithm first chooses a cluster center c1 uniformly on all examples.

Any new cluster is chosen in the following way: assume we already chose cluster centers c1, . . . , cn,

denote D(x) := minj[n] x - cj , the minimal distance from example x  S to the closest cluster

center. Then, the algorithm chooses an example x  S with probability

.D(x)2
x S D(x )2

After the

cluster centers are chosen, we run the standard Lloyd's algorithm for k-means.

The following lemma shows that k-means++ finds an optimal solution with high probability on highly-clustered data:

14

Under review as a conference paper at ICLR 2019

Lemma 6 Fix  > 0. Let S be a finite set that is "highly-clustered": S is partitioned such that S = j[C]Bj, where the partition satisfies:

1. For every x, y  Bj it holds that x - y < 1. 
2. For every x  Bi, y  Bj such that i = j it holds that x - y > C/ .
3. All Bj-s are sets of fixed size: i,j|Bi| = |Bj|.

Assume we run the k-means++ algorithm on set S to find C clusters. Then with probability at least 1 - , the algorithm returns an "optimal clustering": the centers returned by the algorithm x^1, . . . , x^C satisfy that for every j  [C] there exist i  [C] with x^i  Bj.

Proof We will prove by induction that at the i-th step of choosing the centers, with probability

at

least

1-

i C

,

the

chosen

centers

x^1, . . . , x^i

satisfy

that

there

are

no

i1

=

i2



[i]

such

that

x^i1 , x^i2  Bj :

· for i = 1 this is immediate.
· assume we chose x^1, . . . , x^i that satisfy the above condition, and w.l.o.g we can assume that for j  i we have x^j  Bj. Then, for every x  Bj with j  i we have D(x) < 1, and for every x  Bj with j > i we have D(x) > C/ . Therefore, the probability of choosing x  Bj with j  i in the next step is at most:
i C-1  
i + (C - i)C2/ C2/ C

Now, using the union bound we get the required.

Taking i = C proves the initialized center are already optimal. Clearly, any step of Lloyd's

algorithm will maintain this property. In fact, the algorithm will converge after one step, returning

x^1, . . . , x^C

with

x^j

=

1 C

xBj x.

Before we complete the proof of the theorem, we remind a few notations that were used in the algorithm's description. We use i to denote the clustering of patches learned in the i-th iteration of the algorithm, and Ki the weights of the kernels learned BEFORE the i-th step (thus, the patches mapped by Ki are the input to the clustering algorithm that outputs i). Note that we do not learn a mapping in the last step, as from Lemma 1 as the distribution G1 is linearly separable, so we could simply use SVM on the output of the network at that stage. Finally, we use the notations   A to indicate that we operate  on every patch of the tensor A. When we use operations on distributions, for example h  G or   G, we refer to the new distribution generated by applying these operation to every examples sampled from G. The essence of the proof is the following lemma:
Lemma 7 Let G := Gd be the distribution over pairs (X, y), where X is the observed image over the reals, and recall that for i < d, the distribution Gi is over pairs (X(i), y) where X(i) is in a space of latent semantic images over Ci. For every i  [d], with probability at least 1 - 2(d - i) , there exists an orthonormal patch mapping i : Cis×s  Rki such that i  (hi  G) = i  Gi, where i and hi are as defined in algorithm 1.
The lemma tells us that the neural network at step i of the algorithm reveals (in some sense) the latent semantic structure.
We will again use a "reshaped" notation: in every level i (the i-th step of the induction), we treat patches P  Cis×s as vectors P  Cis2 . For a sub-image in the next level, denoted Z  Cis+2×1 s2 , we will denote Zj  Cis+2 1 to be the j-th patch of size s × s in the sub-image (generated by the class at the j-th coordinate of higher level patch P  Cis2 ).
We will prove by induction the following claim:
Lemma 8 Assume that for some i < d the condition of the lemma holds for i + 1, namely that there exists an orthonormal patch mapping i+1 : Cis+×1s  Rki+1 such that i+1  (hi+1  G) = i+1  Gi+1. Then, with probability at least 1 - 2 this condition holds for i.

15

Under review as a conference paper at ICLR 2019

Proof Let i+1 be the mapping satisfying the condition of the claim for i + 1. Notice that the data that is fed to the two-layer training step comes from the distribution i+1  Gi+1, and satisfies the
conditions for the analysis of Theorem 1.

First, we will show that the set of patches Si, the set of all patches in the distribution hi  G is "highly-clustered" with probability at least 1 -  .

For every c  Ci, denote Bc = {Ki i+1(P ) : P  Sc(i)}. Recall that from our assumption,

hi  G = Ki (i+1  Gi+1). Therefore, we have that Si = cCi Bc. Now, from Theorem 1,

since we have T

>

k(C/  +2) 

and 

=

1 s

,

with

probability

at

least

1

-



, conditions 1 and 2 in

the "highly-clustered" definition hold. Since we assume that Sc(i) are of the same size, condition 3

follows.

From the above, using Lemma 6, with probability at least 1 - 2 the set Si is "highly-clustered" and k-means++ returns an optimal clustering. We will now limit ourselves to the event that both of these hold.
Now, define the map i : Cis×s  Rk in the following way: first, for every patch P  Cis2 we take an arbitrary manifestation of the patch P in the next level, denoted Z  Cis+2×1 s2 . In other words, Z could be any s2 × s2 sub-image in the next level that could be generated from the patch P . Now, take i(P ) := i(Ki · (i+1  Z)). Then, the following holds:

1. i(P ) does not depend on the choice of Z: if Z, Z are two different manifestations of P , then, from the definition of the generative model, for every j  [s] it holds that
Zj, Zj  SPj . Thus from what we have shown:

Ki · i+1(Zj ) - Ki · i+1(Zj )

1 <=
s

Therefore:

Ki · i+1  Z - Ki · i+1  Z

s2
2=
j=1
<1

Ki · i+1(Zj ) - Ki · i+1(Zj ) 2

and since we get an optimal clustering, we have: i(Ki · (i+1  Z)) = i(Ki · (i+1  Z ))

2. for every two patches P = P we get (P )  (P ): let Z, Z be the manifestations of p, p respectively. Since P = P there exists j  [s2] such that Pj = Pj . From the

generative

model

it

follows

that

Zj



SP(ij)

and

Zj



SP(ij),

and 

therefore from 

the

behavior

of the algorithm: Ki · i+1(Zj) - Ki · i+1(Zj) > C/  > ki/  . Therefore, we

get that Ki · i+1  Z - Ki · i+1  Z > ki/ , and thus from the clustering optimality

we get i(P )  i(P ).

Now, recall that in the algorithm definition: hi = Ki · (i+1  hi+1). Using the assumption for i + 1, we get:
hi  G = Ki(i+1  Gi+1)
and from the definition of i and what we have shown we get:
i  (hi  G) = i  Gi

Proof [of Lemma 7] We prove by induction: · Note that an immediate property of the k-means++ algorithm is that it doesn't choose the same example twice. Therefore, if we run k-means++ on sample S to find |S| clusters, we
16

Under review as a conference paper at ICLR 2019

get a single cluster center for each element of S. From the definition of our model, there are |Cd-1|k patches in the observed image, and since we run k-means++ to find kd = |Cd-1|k clusters, we get a cluster center for each patch. Hence, k-means++ returns an orthogonal patch mapping d, and since hd = id we get the required.
· Assume the claim holds for i + 1, then with probability at least 1 - 2(d - i - 1) the condition holds for i + 1. In such event, from Lemma 8 with probability at least 1 - 2 , it holds for i. Therefore, with probability (1 - 2(d - i - 1) )(1 - 2 )  1 - 2(d - i) it holds for i.

Proof [ of Theorem 2] From Lemma 7, after performing d iterations of the algorithm, we observe

that with probability at least 1 - 2d mapping. Therefore, using Lemma

1=, it1i-s lindeiasrtlryibsuetipoanrab1lewG1it,hwmhearregin1kisWan

orthonormal patch 2. Therefore, the

SVM algorithm finds an optimal separator in O(k W 2). To count the overall amount of iterations,

notice that the when running k-means++ on sample S to find C clusters, the initialization step at most

|S|C iterations and the algorithm itself converges after one iteration, so in our case its runtime is

bounded by C2k2. The runtime of the T LGD algorithm is T . We perform d iterations of clustering and the T LGD algorithm, and finally run the SVM algorithm that converges in O(k W 2). So the runtime of the whole algorithm is O d(C2k2 + T ) + k W 2 .

17

Under review as a conference paper at ICLR 2019

C PARAMETERS FOR EXPERIMENTS

figure 4 below lists the parameters that were learned in the random parameter search for the different
configurations of the algorithm, as described in 3. The table lists the parameters used in each layer: 1, 2 are the number of clusters for the first and second layer, and 1, 2 are the output channels of the Conv1x1 operation for each layer. These parameters could be used to reproduce the results of
our experiments.

Classifier Ours+FC

N k1 k1 k2 k2 b Accuracy 47509 1377 155 3534 216 -0.14 0.734

Ours+Linear

32124 1384 97 2576 211 -0.63 0.689

Clustering+JL+FC

12369 39 39 184 184 0.84 0.586

Clustering+JL+Linear 57893 5004 345 6813 407 -0.01 0.588

Figure 4: Parameters used in our experiment .

18

