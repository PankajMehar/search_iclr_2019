Under review as a conference paper at ICLR 2019
DATA POISONING ATTACK AGAINST NODE EMBED-
DING METHODS
Anonymous authors Paper under double-blind review
ABSTRACT
Unsupervised node embedding methods (e.g., DeepWalk, LINE, and node2vec) are attracting growing interests due to their simplicity and effectiveness. However, although these methods have been proved effective in a variety of applications, none of the existing work has analyzed the robustness of these methods. This could be very risky if these methods are attacked by an adversarial party. In this paper, we take the task of link prediction as an example, which is one of the most fundamental problems for graph analysis, and introduce a data positioning attack to node embedding methods. We give a complete characterization of attacker's utilities and present efficient solutions to adversarial attacks for two popular node embedding methods: DeepWalk and LINE. We evaluate our proposed attack model on multiple real-world graphs. Experimental results show that our proposed model can significantly affect the results of link prediction by slightly changing the graph structures (e.g., adding or removing a few edges). We also show that our proposed model is very general and can be transferable across different embedding methods.
1 INTRODUCTION
Node representations, which represent each node with a low-dimensional vector, have been proved effective in a variety of applications such as node classification (Perozzi et al., 2014), link prediction (Grover & Leskovec, 2016), and visualization (Tang et al., 2016). Some popular node embedding methods include DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015), and node2vec (Grover & Leskovec, 2016). These methods learn the node embeddings by preserving graph structures, which do not depend on specific tasks. As a result, the learned node embeddings are very general and can be potentially useful to multiple downstream tasks.
However, although these methods are very effective and have been used for a variety of tasks, none of the existing work has studied the robustness of these methods. As a result, these methods are susceptible to a risk of being maliciously attacked. Take the task of link prediction in a social network (e.g., Twitter) as an example, which is one of the most important applications of node embedding methods. A malicious party may create malicious users in a social network and attack the graph structures (e.g., adding and removing edges) so that the effectiveness of node embedding methods is maximally degraded. For example, the attacker may slightly change the graph structures (e.g., following more users) so that the probability of a specific user to be recommended/linked can be significantly increased or decreased. Such a kind of attack is known as data poisoning. In this paper we are interested in the robustness of the node embedding methods w.r.t. data poisoning and their vulnerability to the adversarial attack in the worst case.
We are inspired by existing literature on adversarial attacking, which has been extensively studied for different machine learning systems. Specifically, recently it has been shown that deep neural networks are very sensitive to adversarial attacks, which can significantly change the prediction results by slightly perturbing the input data. However, most of existing work on adversarial attacking focus on image and text data, which are independently distributed while this work focus on graph data. There are some very recent work which studied adversarial attacking for graph data (Dai et al., 2018; Zugner et al., 2018). However, these work mainly studied graph neural networks, which are supervised methods, and the gradients for changing the output label can be leveraged. Therefore, in
1

Under review as a conference paper at ICLR 2019
this paper we are looking for an approach that is able to attack the unsupervised node embedding methods for graphs.
In this paper, we introduce a systematic approach to adversarial attacks against unsupervised node embedding methods. We assume that the attacker can poison the graph structures by either removing or adding edges. Two types of adversarial goals are studied including integrity attack, which aims to attack the probabilities of specific links, and availability attack, which aims to increase overall prediction errors. We proposed a unified optimization framework based on projected gradient descent to optimally attack both goals. To summarize, we make the follow contributions:
· We formulate the problem of attacking unsupervised node embeddings for the task of link prediction and introduce a complete characterization of attacker utilities.
· We propose an efficient algorithm based on projected gradient descent to attack unsupervised node embedding algorithms, specifically DeepWalk and LINE, based on the first order Karush Kuhn Tucker (KKT) conditions.
· We conduct extensive experiments on real-world graphs to show the efficacy of our proposed attack model on the task of link prediction. Experimental results show that our proposed attack approach significantly outperforms the baseline approach by randomly adding or deleting edges. Moreover, results show that our proposed attacking model is transferable across different node embedding methods.
2 RELATED WORK
Adversarial attacks against image classification has been extensively studied in recent years(Szegedy et al., 2013; Goodfellow et al., 2014; Moosavi-Dezfooli et al., 2016; Carlini & Wagner, 2017; Xiao et al., 2018). However, adversarial attacks against graph has rarely been investigated before. Existing work (Dai et al., 2018; Zugner et al., 2018) on adversarial attacks on graph are limited to graph neural networks (), a supervised learning method. Our work, instead, is the first to show the vulnerabilities of unsupervised methods on graph. Here we briefly summarize previous work on graph embedding methods and then we will give an overview of adversarial attacks on graph data. Last, we will show the related work on the connection to matrix factorization. Unsupervised Learning on Graph Previous work on knowledge mining in graph has mainly focused on embedding methods, where the goal is to learn a latent embedding for each node in the graph. DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015) and Node2vec (Grover & Leskovec, 2016) are the three most representative unsupervised methods on graph. Adversarial Attack on Graph There are a few work on adversarial attack on graph before. Test time attack on graph convolutional network has been investigated. (Dai et al., 2018). Also, poisoning attack against graph is also studied. (Zugner et al., 2018) However, they only consider the attack against graph convolutional network. Matrix Factorization Skip-gram model from the NLP community has been shown to be doing implicit matrix factorization (Levy & Goldberg, 2014). Recently, based on the previous work, it has been shown that most of the popular unsupervised methods for gragh is doing implicit matrix factorization. (Qiu et al., 2017). Moreover, poisoning attack has been demonstrated for matrix factorization problem. (Li et al., 2016)
3 PRELIMINARIES
We first introduce the graph embedding problem and link prediction problem. Then we will give an overview of the two existing algorithms from computing the embedding. Given a graph G = (V, E), where V is the node set and |E| is the edge set, the goal of graph embedding methods is to learn a mapping from V to Rd which maps each node in the graph to a d-dimensional vector. We use A be denote the adjacency matrix of the graph G. For each node i, we use i to denote node i's neighbor. We let X be the learnt node embedding matrix where Xi is the embedding of node i.
In link prediction, the goal is to predict the missing edges or the edges that are most likely to emerge in the future. Formally, given a set of node pair T  V × V , the task is to predict a score for each node pair. In this paper, we compute the score of each edge from the cosine similarity matrix XXT .
2

Under review as a conference paper at ICLR 2019

Now we briefly review two popular graph embedding methods: DeepWalk (Perozzi et al., 2014) and LINE (Tang et al., 2015). DeepWalk extends the idea of Word2vec (Leskovec & Mcauley, 2012b) to graph, where they view each node as a word and use the generated random walks on graph as sentences. Then they use Word2vec to get the node embeddings. LINE learns the node embeddings by keeping both first-order proximity (LINE1st) and second-order proximity (LINE2nd) for sampled node pairs. For DeepWalk and LINE2nd, there is a context embedding matrix computed together with the node embedding matrix. We use Y to denote the context embedding matrix.

Previous work (Qiu et al., 2017) has shown that DeepWalk and LINE2nd is implicitly doing matrix factorization.

· DeepWalk is solving the following matrix factorization problem:

log vol(G)

1

T
(D-1A)r D-1

- log b = XY T

T

i=1

· LINE2nd is solving the following matrix factorization problem:

log vol(G)D-1AD-1 - log b = XY T

(1) (2)

where vol(G) = Aij is the volume of graph G, D is the diagonal matrix where each element represents the degree of the corresponding node, T is the context window size and b is the number
of negative samples. We use Z to denote the matrix that DeepWalk and LINE2nd is factorizing. We denote  = {(i, j) : Zij = 0} as the observable elements in Z when solving matrix factorization and i = {(i, j) : Zij = 0} as the nonzero elements in row i of Z. With these notations defined,
we now give a unified formulation for DeepWalk and LINE2nd:

min
X,Y

R(Z - XY T )

2 F

(3)

where [R(A)]ij is Aij if (i, j)   and 0 otherwise,

A

2 F

denotes the squared Frobenious norm

of matrix A.

4 PROBLEM DEFINITION
In this section we introduce the attack model, including attacker's action, attacker's utilities and the constraints on the attacker. We assume that the attacker can manipulate the poisoned graph G by adding or deleting edges. In this paper, we consider these two type of manipulation: adding edges and deleting edges respectively. We use Gadv to denote the poisoned graph.
We characterize two kinds of adversarial goals:
Integrity attack: The attacker's goal is either to increase or decrease the probability (similarity score) of the target node pair connected by an edge. For example, in social network, the attacker may be interested in increasing (or decreasing) the probability that a friendship occurs between two people. Also, in recommendataion system, an attacker associated with the producer of a product may be interested to increase the probability of recommending the users with that specific product. Specifically, the attacker aims to change the probability of the edge connected with a pair of nodes whose embeding is learnt from the poisoned graph Gadv
For integrity attack, we consider two kinds of constraints on the attacker: 1.Direct Attack: the attacker can only manipulate edges adjacent to the target node pair; 2. Indirect Attack: the attacker can only manipulate edges without connecting to the target node pair.
Availability attack The adversarial goals of availability attack is to reduce the performance over a test set consisting of a set of node pairs T  V × V . (Here T consists of both positive examples Tpos and negative examples Tneg). In this paper, we choose average precision score (AP score) to evaluate the attack performance. Specifically, we consider the attacker whose goal is to make the AP score over T lower by perturbing the clean graph.

5 ATTACKING UNSUPERVISED GRAPH EMBEDDING
In this section we show our algorithm for computing the adversarial strategy. Given that DeepWalk and LINE is implicitly doing matrix factorization, we can directly derive the back-propogated gra-

3

Under review as a conference paper at ICLR 2019

dient based on the first order KKT condition. Our algorithm has two steps: 1. Projected Gradient
Descent (PGD) step: gradient descent on the weighted adjacency matrix. 2. Projection step: projection of weighted adjacency matrix onto {0, 1}|V |×|V |. We first describe the projected gradient descent step and then we describe the projection method we use to choose which edge to add or
delete.

5.1 PROJECTED GRADIENT DESCENT (PGD)

Based on the matrix factorization formulation above, we describe the algorithm we use to generate the adversarial graph. The core part of our method is projected gradient descent (PGD) step. In this step, the adjacency matrix is continous since we view the graph as a weighted graph, which allows us to use gradient descent.

First we describe the loss function we use. We use L(X) to denote the loss function. For integrity attack, L is ±[XXT ]ij where (i, j) is the target node pair. (Here the + or - sign depends on whether
the attacker wants to increase or decrease the score of the target edge.) For availability attack, the loss function is (i,j)Tpos [XXT ]ij - (i,j)Tneg [XXT ]ij . The update of the weighted adjacency
matrix A in iteration t is as follows:

At+1 = ProjA(At - st · AL)

(4)

Here Proj is the projection function which projects the matrix to [0, 1] space and st is the step size in iteration t. The non-trivial part is to compute AL. We note that

AL = X L · AX

(5)

The computation of X L is trivial. Now to compute AX, using the chain rule, we have: AX = Z X · AZ. First we show how to compute Z X.
For the matrix factorization problem defined in Eq. 3, using the KKT condition, we have:

(Zij - XiYjT )Yj = 0
ji

(6)

Xi Zij

= ( Yj
j i

YjT )-1Yj

(7)

Next we show how to compute AZ for DeepWalk and LINE separately. In the derivation of AZ, we view vol(G) and D as constant.
DeepWalk We show how to compute AZ for DeepWalk. For DeepWalk, From Eq. 1:

Z = log vol(G)

1

T
(D-1A)r D-1

- log b

T

i=1

(8)

Now Note have

ttohPactPo(mrP,prwu)ete=canAcZorkm,=l1pePut tPke-1=PAZDP.-r1-Ak.,

then we only need to derive Then since computing AP

P P r for and P Z

each r  [1, is easy, once

T ]. we

LINE We show the derivation of AZ for LINE2nd:

Z = log(vol(G)D-1AD-1) - log b

(9)

since Zij = log(vol(G)d-i 1Aijd-j 1) - log b where di = Dii. We have:

Zij Aij

=

1 Aij

(10)

Once we have Z L and LZ, we can compute AX.

4

Under review as a conference paper at ICLR 2019
5.2 PROJECTION
In the Projected Gradient Descent step, we compute a weighted adjacency matrix Aopt. We use Aorg to denote the adjacency matrix of the clean graph. Therefore we need to project it back to {0, 1}|V |×|V |. Now we describe the projection method we use, which is straightforward. First, we show our projection method for an attacker that can add edges. To add edges, the attacker needs to choose some cells in S = {(i, j) | Aiojrg = 0} and turn it into 1. Our projection strategy is that the attacker chooses the cells (i, j) in S where Aoijpt are closest to 1 as the candidate set of edges to add. For deleting edges, it works in a similar way. The only difference is that we start from the cells that are originally 1 in Aorg and choose the cells that are closest to 0 in Aopt as the candidate set of edges to delete.
Now we briefly discuss when to use the projection step. A natural choice is to project once after the projected gradient descent step. Another choice is to incorporate the projection step into the gradient descent computation where we project every k iterations. The second projection strategy induces less loss in the projection step and is more accurate for computation but can take more iterations to converge than the first projection strategy. In our experiments, we choose the first projection strategy for its ease of computation.
6 EXPERIMENTS
In this section, we show the results of poisoning attack against DeepWalk and LINE on real-world graph datasets. We first show the results for integrity attack in section 6.1. We then show the results for availability attack in section 6.2. We evaluate our attack method on three real-world graph datasets: 1). Facebook (Leskovec & Mcauley, 2012a): a social networks with 4039 nodes and 88234 edges. 2). Cora (Sen et al., 2008): a citation network with 2708 nodes and 2708 edges. 3). Citeseer (Giles et al., 1998): a citation network with 2110 nodes and 7336 edges.
Baselines. We comapare with two baselines: (1) random attack: randomly adding ore removing a few edges; (2) personalized PageRank (Bahmani et al., 2010): this baseline is only used for integrity attack. Given a target edge (A, B), we use personalized PageRank to calculate the importance of the nodes. Given a list of nodes ranked by their importance, e.g., (x1, x2, x3, x4, ...), we select the edges which connect the top ranked nodes to A or B, i.e., (A, x1), (B, x1), (A, x2), (B, x2), .... In our experiments, we choose 128 as the latent embedding dimension. We use the default parameter settings for DeepWalk and LINE. we generate the test set consisted with positive examples by removing 15% of the edges from the graph to and negative examples by sampling an equal number of node pairs from the graph which has no edge connecting them. For both our attack and random attack, we guarantee that the attacker can't directly modify any node pairs in the target set.
6.1 INTEGRITY ATTACK
For each attack scenario, we choose 32 different target node pairs and use our algorithm to generate the adversarial graph. For decreasing the score of the target edge, the target node pair is randomly sampled from the positive examples in the test set. For increasing the score of the target edge, the target node pair is randomly sampled from the negative examples in the test set. We report the average score increase and compare it with the random attack baseline. We consider two kinds of attacker's actions: adding or deleting edges and two constraints : direct attack and indirect attack.
Adding edges We consider the adversary which can only add edges. Figure 1 shows the results under direct attack setting. In figure 1a, 1c, 1e, 1g, when the adversarial goal is to increase the score of the target node pair, we find that our attack method outperforms the random attack baseline. Note that we also plot the baseline of personalized pagerank. We can see this second baseline we propose is a very strong baseline, with attack performance the same level as our proposed method. To further understanding it, we analyze the edges we add in this attack scenario. We find the following pattern: if the target node pair is (i, j), then our attack tends to add edges from node i to the neighbors of node j and also from node j to the neighbors of node i. This is intuitive because connecting to other node's neighbors can increase the similarity score of two nodes. In figure 1d and 1h, when the
5

Under review as a conference paper at ICLR 2019

Score increase

Facebook
0.8 attack
0.6 pagerank random
0.4
0.2
0.0
-0.2
-0.4 2468
#Added edges

Score increase

0.2 0.0 -0.2 -0.4 -0.6
0

Facebook
attack random
10 20 30 40 50
#Added edges

(a) DeepWalk-Add-Up
Cora
attack 1.00 pagerank 0.75 random
0.50
0.25
0.00
-0.25 2468
#Added edges

(b) DeepWalk-Add-Down

Cora
0.6 attack 0.4 random

Score increase

0.2

0.0

-0.2

-0.4

-0.6 0

10 20 30 40 50
#Added edges

(e) DeepWalk-Add-Up (f) DeepWalk-Add-Down

Score increase

Score increase

Facebook
0.4 attack
0.3 pagerank random
0.2
0.1
0.0
-0.1
-0.2 2468
#Added edges
(c) LINE-Add-Up
0.8 Cora
attack 0.6 pagerank
random 0.4
0.2
0.0
-0.2
-0.4 2468
#Added edges
(g) LINE-Add-Up

Score increase

Score increase

Facebook
0.2 attack
0.1 random

0.0

-0.1

-0.2

-0.3

-0.4

5 10 15

#Added edges

(d) LINE-Add-Down
0.2 Cora
attack 0.1 random

0.0

-0.1

-0.2

-0.3

5 10 15

#Added edges

(h) LINE-Add-Down

Score increase

Figure 1: Result for direct integrity attack on two datasets. The first line contains the results for Facebook dataset. The second line contains the results for Cora dataset. The format "Method -- Type --Direction " is used to label the each sub-caption. "Method" refers to embedding method while "Type" refers to adding or deleting edges to poison the graph. "Direction" refers to increasing or decreasing the similarity score of the target node pair. This notation is also used in Figure 2, 3, 4.

adversarial goal is to decrease the score of target node pair, our method still outperforms the random attack baseline for attacking LINE.
Figure 2 show our result of indirect attack. We can see that for DeepWalk, even if the attacker can't modify the edges adjacent to the target node pair, it can still manipulate the score of the target edge with a few edges added. We also analyze the edges our algorithm chooses when the adversarial goal is to increase the score of the target node pair (i, j), (the case in figure 2a,2c) we find that our attack tends to add edges between the neighbors of node i and the neighbors of node j. It also follows the intuition that connecting the neighbor of two nodes can increase the similarity of two nodes.

Score increase Score increase Score increase Score increase

Facebook
0.6 attack random
0.4
0.2
0.0
-0.2 2468
#Added edges
(a) DeepWalk-Add-Up

0.10 Facebook
attack
random 0.05

0.00

-0.05

-0.10

5 10 15

#Added edges

(b) DeepWalk-Add-Down

Cora
1.0 attack
0.8 random
0.6
0.4
0.2
0.0 2468
#Added edges
(c) DeepWalk-Add-Up

Cora
0.2 attack
random 0.1

0.0

-0.1

-0.2

5 10 15

#Added edges

(d) DeepWalk-Add-Down

Figure 2: Result for indirect integrity attack against DeepWalk on two datasets where the action of attacker is adding edges.

Deleting edges Now we consider the adversary which can delete existing edges. Figure 4 summarizes our result for direct attack. We can see that our attack method works well for attacking DeepWalk, with average score increase higher than the random attack baseline. The large variance may be because that different edges have different sensitivity to deleting edges. Also, we can see that LINE is more robust to deleting edges. Figure 3 summarizes our results for indirect attack. Still, on average, our attack is able to outperform the random attack baseline on average.

6.2 AVAILABILITY ATTACK
In this part, we show the results for availability attack. We report our results on two dataset: Cora and Citeseer. For both datasets, we choose the test set (15% split) to be the attack set. Figure 5

6

Under review as a conference paper at ICLR 2019

Score increase Score increase Score increase Score increase

0.4 Facebook
attack 0.3 random

0.2

0.1

0.0

-0.1

-0.2

5 10 15

#Deleted edges

(a) DeepWalk-Del-Up

0.10 Facebook
attack
random 0.05

0.00

-0.05

-0.10 0

10 20 30 40
#Added edges

50

(b) DeepWalk-Del-Down

0.20 0.15 0.10 0.05 0.00 -0.05 -0.10

Cora
attack random
2468
#Deleted edges

(c) DeepWalk-Del-Up

0.2 Cora
attack 0.1 random

0.0

-0.1

-0.2

-0.3

-0.4

5 10 15

#Deleted edges

(d) DeepWalk-Del-Down

Figure 3: Result for indirect integrity attack against DeepWalk on Facebook dataset where the action of the attacker is deleting edges.

Score increase

Facebook
0.4 attack
0.3 random

0.2

0.1

0.0

-0.1

-0.2

5 10 15

#Deleted edges

(a) DeepWalk-Del-Up
0.4 Cora
attack 0.3 random
0.2
0.1
0.0
-0.1
-0.2 2468
#Deleted edges

(e) DeepWalk-Del-Up

Score increase

Score increase

Facebook
0.10 attack
random 0.05

0.00

-0.05

-0.10

5 10 15

#Deleted edges

(b) DeepWalk-Del-Down
Cora
0.2 attack random
0.0

-0.2

-0.4

-0.6

5 10 15

#Deleted edges

(f) DeepWalk-Del-Down

Score increase

Score increase

Facebook
0.2 attack
random 0.1

0.0

-0.1

-0.2

5 10 15

#Deleted edges

(c) LINE-Del-Up
0.2 Cora
attack random 0.1

0.0

-0.1

-0.2 2468
#Deleted edges
(g) LINE-Del-Up

Score increase

Score increase

Facebook
0.2 attack
random 0.1

0.0

-0.1

-0.2 0

5 10 15 20 25
#Deleted edges

(d) LINE-Del-Down
0.3 Cora
attack 0.2 random

0.1

0.0

-0.1

-0.2

-0.3

5 10 15

#Deleted edges

(h) LINE-Del-Down

Figure 4: Result for direct integrity attack against two methods on two datasets.

Score increase

AP score

Cora
1.0
0.9
0.8
0.7
0.6 attack
0.5 random baseline
0.4 100 200 300
# Added edges
(a) DeepWalk-Add
Citeseer
1.0
0.8
0.6
0.4
attack 0.2 random
baseline 0.0
100 200 300
# Added edges
(e) DeepWalk-Add

AP score

AP score

Cora
1.0
0.9
0.8
0.7
0.6 attack
0.5 random baseline
0.4 100 200
# Added edges
(b) LINE-Add
Citeseer
1.0
0.8
0.6
0.4
attack 0.2 random
baseline 0.0
100 200
# Added edges
(f) LINE-Add

300 300

AP score

AP score

Cora
1.0
0.9
0.8
0.7 attack random baseline
0.6 100 200 300
# Deleted edges
(c) Deepwalk-Del
Citeseer
1.0
0.9
0.8
0.7 attack random baseline
0.6 100 200 300
# Deleted edges
(g) DeepWalk-Del

AP score

AP score

Cora
0.90
0.85
0.80
0.75
0.70 attack
0.65 random baseline
0.60 100 200
# Deleted edges
(d) Line-Del
Citeseer
0.90
0.85
0.80
0.75
0.70 attack
0.65 random baseline
0.60 100 200
# Deleted edges
(h) LINE-Del

300 300

AP score

Figure 5: Result for availability attack on two datasets. The first line contains the results for Cora dataset and the second line contains the results for Citeseer dataset. Here the notation "DeepWalkAdd" means that the attacker is attacking DeepWalk by adding edges. The format "Method -- Type " here is used to label the each sub-caption. "Method" refers to embedding method while "Type" refers to adding or deleting edges to poison the graph. This notation is also used in Figure 6.

7

Under review as a conference paper at ICLR 2019

summarizes our result. We can see that our attack outperforms the random attack baseline for all cases. LINE is most robust to our attack for an adversary that deletes edges. Also, we notice the that adding edges is more powerful than deleting edges in our attack. Specifically, when the number of edges allowed to add is large, our attack can make the AP score over the test set to be lower than 0.5.

6.3 TRANSFERABILITY

In this part, we show that our attack can be transferred across different embedding methods. We

choose another three embedding methods to test the transferability of our approach: 1. Variational

Graph Autoencoder(GAE) (Kipf & Welling, 2016); 2. Spectral Clustering (Tang & Liu, 2011);

3. Node2Vec (Grover & Leskovec, 2016). For GAE, we use the default setting as in the original

paper. For Node2Vec, we first tune the parameters p, q on a validation set and use the best p, q for

Node2Vec.

Cora
1.0

Cora
1.0

Cora
1.0

Cora
1.0

AP score

0.8 0.9 0.9 0.9

AP score

AP score

AP score

0.6 0.8

0.8 0.8

0.4 GAE
SC 0.2 LINE

GAE 0.7 SC
DeepWalk

0.7 GAE
SC 0.6 LINE

GAE 0.7 SC
DeepWalk

Node2vec

Node2vec

Node2vec

Node2vec

0.0 0.6 0.5 0.6

100 200 300

100 200 300

100 200 300

100 200 300

# Added edges

# Added edges

# Deleted edges

# Deleted edges

AP score

(a) DeepWalk-Add
1.0 Citeseer
0.8
0.6
0.4 GAE SC
0.2 LINE Node2vec
0.0 100 200 300
# Added edges

AP score

(b) LINE-Add
1.0 Citeseer
0.9
0.8
0.7
0.6 GAE SC
0.5 DeepWalk Node2vec
0.4 100 200
# Added edges

300

AP score

(c) DeepWalk-Del
1.0 Citeseer
0.9
0.8
0.7
0.6 GAE SC
0.5 LINE Node2vec
0.4 100 200 300
# Deleted edges

AP score

(d) LINE-Del
Citeseer
1.00
0.95
0.90
0.85
0.80 GAE SC
0.75 DeepWalk Node2vec
0.70 100 200
# Deleted edges

300

(e) DeepWalk-Add

(f) LINE-Add

(g) DeepWalk-Del

(h) LINE-Del

Figure 6: Result for transferability test of our attack methods on two datasets. For each embedding method, the solid line is the AP score measured on the clean graph and the dotted line shows the AP score measured on the poisoned graph.

Figure 6 shows our result for transferability of attack. Specifically, our attack on DeepWalk transfers well to the other four methods (including LINE2nd). Also, we note that Node2Vec is the most robust to our transferability attack. Specifically, in Figure 6b, 6c, 6f, 6g the poisoned graph sometimes can increase the performance of Node2Vec.

7 CONCLUSION
In this paper, we investigate data poisoning attack against unsupervised node embedding methods and take the task of link prediction as an example. We study two types of data poisoning attacks including integrity attack and availability attack. We propose a unified optimization framework to optimally attack the node embedding methods for both types of attacks. Experimental results on several real-world graphs show that our proposed approach can effectively attack the results of link prediction by adding or removing a few edges. Results also show that the adversarial examples discovered by our proposed approach are transferable across different node embedding methods. In the future, we plan to study how to design effective defense strategies for node embedding methods.

REFERENCES
Bahman Bahmani, Abdur Chowdhury, and Ashish Goel. Fast incremental and personalized pagerank. Proceedings of the VLDB Endowment, 4(3):173­184, 2010.

8

Under review as a conference paper at ICLR 2019
Matthew J Beal, Zoubin Ghahramani, and Carl E Rasmussen. The infinite hidden markov model. In Advances in neural information processing systems, pp. 577­584, 2002.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pp. 39­57, 2017. doi: 10.1109/SP.2017.49. URL https://doi.org/10.1109/SP.2017. 49.
Kumar Chellapilla and Alexey Maykov. A taxonomy of javascript redirection spam. In Proceedings of the 3rd international workshop on Adversarial information retrieval on the web, pp. 81­88. ACM, 2007.
Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1115­1124, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. Citeseer: an automatic citation indexing system. In INTERNATIONAL CONFERENCE ON DIGITAL LIBRARIES, pp. 89­98. ACM Press, 1998.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. NIPS Workshop on Bayesian Deep Learning, 2016.
Jure Leskovec and Julian J. Mcauley. Learning to discover social circles in ego networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 539­547. Curran Associates, Inc., 2012a.
Jure Leskovec and Julian J. Mcauley. Learning to discover social circles in ego networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 539­547. Curran Associates, Inc., 2012b.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2177­2185. Curran Associates, Inc., 2014.
Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based collaborative filtering. In Advances in Neural Information Processing Systems 29, pp. 1885­1893. Curran Associates, Inc., 2016.
Christian Ludl, Sean McAllister, Engin Kirda, and Christopher Kruegel. On the effectiveness of techniques to detect phishing sites. In International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment, pp. 20­39. Springer, 2007.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2574­2582, 2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14, pp. 701­710, 2014. ISBN 978-1-4503-2956-9.
Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. CoRR, abs/1710.02971, 2017. URL http://arxiv.org/abs/1710.02971.
9

Under review as a conference paper at ICLR 2019
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. Collective classification in network data. AI Magazine, 29(3):93­106, 2008. URL http://www.cs.iit.edu/~ml/pdfs/sen-aimag08.pdf.
Sushant Sinha, Michael Bailey, and Farnam Jahanian. Improving spam blacklisting through dynamic thresholding and speculative aggregation. In NDSS, 2010.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In WWW. ACM, 2015.
Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Visualizing large-scale and highdimensional data. In Proceedings of the 25th International Conference on World Wide Web, pp. 287­297. International World Wide Web Conferences Steering Committee, 2016.
Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 990­998. ACM, 2008.
Lei Tang and Huan Liu. Leveraging social media networks for classification. Data Min. Knowl. Discov., 23(3):447­478, November 2011. ISSN 1384-5810.
Yee W Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Sharing clusters among related groups: Hierarchical dirichlet processes. In Advances in neural information processing systems, pp. 1385­1392, 2005.
Kurt Thomas, Chris Grier, Justin Ma, Vern Paxson, and Dawn Song. Design and evaluation of a real-time url spam filtering service. In Security and Privacy (SP), 2011 IEEE Symposium on, pp. 447­462. IEEE, 2011.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed adversarial examples. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HyydRMZC-.
Sandeep Yadav, Ashwath Kumar Krishna Reddy, AL Reddy, and Supranamaya Ranjan. Detecting algorithmically generated malicious domain names. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pp. 48­61. ACM, 2010.
Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate example weighting. In Data Mining, 2003. ICDM 2003. Third IEEE International Conference on, pp. 435­442. IEEE, 2003.
Daniel Zugner, Amir Akbarnejad, and Stephan Gunnemann. Adversarial attacks on neural networks for graph data. In KDD, 2018.
10

Under review as a conference paper at ICLR 2019

Appendices
A IMPLEMENTATION DETAIL
In this part, we discuss the initialization of weighted adjacency matrix in the projected gradient descent step. From the formulation in section 5.1, if we initialize all cells which are initially 0 to 0. Then there won't be back-propagated gradient on these cells. (This is because  won't contain these cells.) To handle this issue, we initialize these cells with a small value, for example 0.001, which allows the gradient on these cells to be efficiently computed.
B CASE STUDY: ATTACK DEEPWALK ON DBLP DATASET
We conduct a case study on a real-world citation network constructed from DBLP (Tang et al., 2008). Specifically, here we construct a citation network in two different research communities to understand the interactions between two research communities. We select two research communities including: machine learning and data mining (ML&DM), and security (Security). For each research community, we select some prestigious conferences in the field. For ML&DM, we select the following conferences including ICML, ICLR, NIPS, KDD, WWW, ICWD, ICDM while for Security we select IEEE S&P, CCS, Usenix and NDSS. We only keep some well cited papers, which eventually yield a citation network with 1,865 nodes in total.
We first take a look at the scenario which aims to increase the similarities of two papers, which d
Assume the attacker wants the link prediction algorithm to give higher score to a non-existing link, so that it will be likely to predict one paper cites the other while it actually does not. We refer to this reference relationship edge as victim edge, and the two end papers as victim papers. The attacker would like to achieve his goal by adding edges to the network, e.g. adding additional references to existing papers. We examine the attacking procedure manually. One example is shown in Figure 7.

B A

B

A e

f

gd

c

h

(a) Original Network

(b) Adversarial Goal

(c) Attack Example

Figure 7: Case study visualization. Blue nodes denote papers from Security field and green nodes denote papers from ML&DM field.

Starting from the two victim nodes, the visualization is a sub-graph that contains all of their 3-hop neighbours and the corresponding edges between these nodes. In the figure, blue nodes denote papers from Security field(published in a security-focused conference) and green nodes represent ML&DM papers. The arrows point from later papers to the ones that they reference to.
There are two connected components in Figure 7a. The connected component on the right hand side consists of only ML&DM papers(only green nodes) while the other one is a mixture of both Security and ML&DM papers(both blue and green nodes) and most papers in it are Security papers. The attacker chooses a non-existing edge between paper A(Beal et al., 2002) and paper B(Ludl et al., 2007) to attack. The edge is highlighted in orange in Figure 7b. The prediction score for the victim edge before attack is 0.3281 and the AUC score is 0.9526. Paper A, 'The Infinite Hidden Markov Model', is published on NIPS2001 and discusses an extension of hidden Markov models. Paper B, 'On the Effectiveness of Techniques to Detect Phishing Sites', on the other hand, analyzes two popular anti-phishing methods. The two victim papers are from two different domains which do not

11

Under review as a conference paper at ICLR 2019
have much overlap, and they are from two different connected components in the graph. The victim edge connects the two parts. For each attack, the attacker is allowed to add 5 different edges. One example of the attack is illustrated in Figure 7c. The red edges are adversarial links added by the attacker. The attacker selects one paper c(Thomas et al., 2011) that references to victim paper A and one paper d(Teh et al., 2005) that references to victim paper B, and then links the two together. It also links paper d with 4 other papers(Chellapilla & Maykov, 2007)(Sinha et al., 2010)(Yadav et al., 2010)(Zadrozny et al., 2003) that are referenced by paper c. The adversarially added edges are all related to paper c and paper d. This shows the attacker treats them as `important' papers. In reality, One coauthor of paper c, Vern Paxson, and one coauthor of paper d, Michael I. Jordan, are two of the 36 `important' selected authors in our study. Note that each of the 5 adversarial edges is connecting the two isolated connected components. In other words, one end paper is from the Security cluster and the other is from the ML&DM cluster, just like the victim papers. We can interpret this attack as trying to add links that are similar to the victim edge. After conducting the attack, the link prediction score rises to 0.5072 and the AUC score drops down to 0.715976.
12

