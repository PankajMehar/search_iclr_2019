Under review as a conference paper at ICLR 2019
COUNT-BASED EXPLORATION WITH THE SUCCESSOR REPRESENTATION
Anonymous authors Paper under double-blind review
ABSTRACT
The problem of exploration in reinforcement learning is well-understood in the tabular case and many sample-efficient algorithms are known. Nevertheless, it is often unclear how the algorithms in the tabular setting can be extended to tasks with large state-spaces where generalization is required. Recent promising developments generally depend on problem-specific density models or handcrafted features. In this paper we introduce a simple approach for exploration that allows us to develop theoretically justified algorithms in the tabular case but that also give us intuitions for new algorithms applicable to settings where function approximation is required. Our approach and its underlying theory is based on the substochastic successor representation, a concept we develop here. While the traditional successor representation is a representation that defines state generalization by the similarity of successor states, the substochastic successor representation is also able to implicitly count the number of times each state (or feature) has been observed. This extension connects two until now disjoint areas of research. We show in traditional tabular domains (RiverSwim and SixArms) that our algorithm empirically performs as well as other sample-efficient algorithms. We then describe a deep reinforcement learning algorithm inspired by these ideas and show that it matches the performance of recent pseudo-count-based methods in hard exploration Atari 2600 games.
1 INTRODUCTION
Reinforcement learning (RL) tackles sequential decision making problems by formulating them as tasks where an agent must learn how to act optimally through trial and error interactions with the environment. The goal in these problems is to maximize the sum of the numerical reward signal observed at each time step. Because the actions taken by the agent influence not just the immediate reward but also the states and associated rewards in the future, sequential decision making problems require agents to deal with the trade-off between immediate and delayed rewards. Here we focus on the problem of exploration in RL, which aims to reduce the number of samples (i.e., interactions) an agent needs in order to learn to perform well in these tasks when the environment is initially unknown.
The sample efficiency of RL algorithms is largely dependent on how agents select exploratory actions. In order to learn the proper balance between immediate and delayed rewards agents need to navigate through the state space to learn about the outcome of different transitions. The number of samples an agent requires is related to how quickly it is able to explore the state-space. Surprisingly, the most common approach is to select exploratory actions uniformly at random, even in high-profile success stories of RL (e.g., Tesauro, 1995; Mnih et al., 2015). Nevertheless, random exploration often fails in environments with sparse rewards, that is, environments where the agent observes a reward signal of value zero for the majority of states.1
In model-based approaches agents explicitly learn a model of the dynamics of the environment which they use to plan future actions. In this setting the problem of exploration is well understood. When all states can be enumerated and uniquely identified (tabular case), we have algorithms with proven sample complexity bounds on the maximum number of suboptimal actions an agent selects before
1When we refer to environments with sparse rewards we do so for brevity and ease of presentation. Actually, any sequential decision making problem has dense rewards. In the RL formulation a reward signal is observed at every time step. By environments with sparse rewards we mean environments where the vast majority of transitions lead to reward signals with the same value.
1

Under review as a conference paper at ICLR 2019

converging to an -optimal policy (e.g., Brafman & Tennenholtz, 2002; Kearns & Singh, 2002; Strehl & Littman, 2008). However, these approaches are not easily extended to large environments where it is intractable to enumerate all of the states. When using function approximation, the concept of state visitation is not helpful and learning useful models is by itself quite challenging.
Due to the difficulties in learning good models in large domains, model-free methods are much more popular. Instead of building an explicit model of the environment, they estimate state values directly from transition samples (state, action, reward, next state). Unfortunately, this approach makes systematic exploration much more challenging. Nevertheless, because model-free methods make up the majority of approaches scalable to large domains, practitioners often ignore the exploration challenges these methods pose and accept the high sample complexity of random exploration. Reward bonuses that promote exploration are one alternative to random walks (e.g., Bellemare et al., 2016; Martin et al., 2017), but none such proposed solutions are widely adopted in the field.
In this paper we introduce an algorithm for exploration based on the successor representation (SR). The SR, originally introduced by Dayan (1993), is a representation that generalizes between states using the similarity between their successors, i.e., the similarity between the states that follow the current state given the environment's dynamics and the agent's policy. The SR is defined for any problem, it can be learned through temporal-difference learning (Sutton, 1988) and, as we discuss below, it can also be seen as implicitly estimating the transition dynamics of the environment. Our approach is inspired by the substochastic successor representation (SSR), a concept we introduce here. The SSR is defined so that it implicitly counts state visitation, allowing us to use it to encourage exploration. This idea connects representation learning and exploration, two otherwise disjoint areas of research. The SSR allows us to derive an exploration bonus that when applied to model-based RL generates algorithms that perform as well as theoretically sample-efficient algorithms. Importantly, the intuition developed with the SSR assists us in the design of a model-free deep RL algorithm that achieves performance similar to pseudo-count-based methods in hard exploration Atari 2600 games (Bellemare et al., 2016; Ostrovski et al., 2017).

2 PRELIMINARIES

We consider an agent interacting with its environment in a sequential manner. Starting from a state S0  S, at each step the agent takes an action At  A, to which the environment responds with a state St+1  S according to a transition probability function p(s |s, a) = Pr(St+1 = s |St = s, At = a), astnadtewsituhnadererwaacrtidosnigan,athl aRtti+s,1r(sR, a, )w=h. eEre[Rr(ts|S, at )=insd,icAatte=s tahe]. expected reward for a transition from

The value of a state s when following a policy , v(s), is defined to be the expected sum of

discounted rewards from that state: v(s) =. E

T k=t+1

 k-t-1 Rk

St

=

s

,

with



being

the

discount factor. When the transition probability function p and the reward function r are known, we

can compute v(s) recursively by solving the system of equations below (Bellman, 1957):

v(s) =

(a|s) r(s, a) + 
a

s p(s |s, a)v(s ) .

This equation can also be written in matrix form with v, r  R|S| and P  R|S|×|S|: v = r + Pv = (I - P)-1r,

(1)

where P is the state to state transition probability function induced by , that is, P(s, s ) = a (a|s)p(s |s, a).

Traditional model-based algorithms for RL work by learning estimates of the matrix P and of the vector r and using them to estimate v, for example by solving Equation 1. We use P^ and ^r to denote empirical estimates of P and r. Formally,

P^ (s

|s)

=

n(s, s n(s)

),

^r(s)

=

C(s, s n(s)

),

(2)

where ^r(i) denotes the i-th entry in the vector ^r, n(s, s ) is the number of times the transition s  s was observed, n(s) = s S n(s, s ), and C(s, s ) is the sum of the rewards associated with the n(s, s ) transitions (we drop the action in the discussion to simplify notation).

2

Under review as a conference paper at ICLR 2019

Alternatively, in model-free RL, instead of estimating P and r we estimate v(s) directly from samples. We often use temporal-difference (TD) learning (Sutton, 1988) to update our estimates of

v(s), v^(·), online:

v^(St)  v^(St) +  Rt+1 + v^(St+1) - v^(St) ,

(3)

where  is the step-size parameter. Generalization is required in problems with large state spaces,

where it is unfeasible to learn an individual value for each state. We do so by parametrizing v^(s) with

a set of weights . We write, given the weights , v^(s; )  v(s) and q^(s, a; )  q(s, a), where q(s, a) = r(s, a) +  s p(s |s, a)v(s ). Model-free methods have performed well in problems with large state spaces, mainly due to the use of neural networks as function approximators (e.g.,

Mnih et al., 2015).

Our algorithm is based on the successor representation (SR; Dayan, 1993). The successor representa-

tion, with respect to a policy , , is defined as



(s, s ) = E,p

tI{St = s } S0 = s ,

t=0

where we assume the sum is convergent with I denoting the indicator function. Dayan (1993) has

shown that this expectation can be estimated from samples through TD learning. It also corresponds

to the Neumann series of P :



 = t(P)t = (I - P)-1.

(4)

t=0

Notice that the SR is part of the solution when computing a value function: v = r (Equation 1). We use ^  to denote the SR computed through P^, the approximation of P.

The definition of the SR can also be extended to features. Successor features generalize the SR to the

function approximation setting (Barreto et al., 2017). We use the definition for the uncontrolled case

in this paper. Importantly, the successor features can also be learned with TD learning.

Definition 2.1 (Successor Features). For a given 0   < 1, policy , and for a feature representation (s)  Rd, the successor features for a state s are:



(s) = E,p

t(St) S0 = s .

t=0

Alternatively, in matrix form,  =

 t=0

 t (P )t 

=

(I

-

 P )-1 .

Notice

that

this

definition

reduces to the SR in the tabular case, where  = I.

3 THE SUBSTOCHASTIC SUCCESSOR REPRESENTATION

In this section we introduce the concept of the substochastic successor representation (SSR). The SSR
is derived from an empirical transition matrix similar to Equation 2, but where each state incorporates a small (1/(n(s) + 1)) probability of terminating at that state, rather than transiting to a next state. As we will show, we can recover the visit counts n(s) through algebraic manipulation on the SSR.

While computing the SSR is usually impractical, we use it as inspiration in the design of a new

deep reinforcement learning algorithm for exploration (Section 4). In a nutshell, we view the SSR

as approximating the process of learning the SR from an uninformative initialization (i.e., the zero

vector), and using a stochastic update rule. While this approximation is relatively coarse, we believe

it gives qualitative justification to our use of the learned SR to guide exploration. To further this claim,

we demonstrate that using the SSR in synthetic, tabular settings yields comparable performance to

that of theoretically-derived exploration algorithms.

Definition 3.1 (Substochastic Successor Representation). Let P~ denote the substochastic matrix

induced

by

the

environment's

dynamics

and

by

the

policy



such

that

P~ (s

|s)

=

n(s,s ) n(s)+1

.

For

a

given

0   < 1, the substochastic successor representation, ~ , is defined as:


~  = tP~t = (I - P~)-1.

t=0

3

Under review as a conference paper at ICLR 2019

The theorem below formalizes the idea that the 1 norm of the SSR implicitly counts state visitation.
Theorem 1. Let n(s) denote the number of times state s has been visited and let (s) = (1 + ) - ||~ (s)||1, where ~  is the substochastic SR as in Definition 3.1. For a given 0   < 1,

 n(s) + 1

-

2 1-



(s)



 n(s) + 1

Proof of Theorem 1. Let P^ be the empirical transition matrix. We first rewrite P~ in terms of P^:

P~(s, s

)

=

n(s, s ) n(s) + 1

=

n(s) n(s, s ) n(s) + 1 n(s)

=

n(s) n(s) +

1

P^

(s,

s

)

=

1

-

1 n(s) +

1

P^(s, s ).

The expression above can also be written in matrix form: P~ = (I - N )P^, where N  R|S|×|S| denotes the diagonal matrix of augmented inverse counts. Expanding ~  we have:

~  = (P~)t = I + P~ + (P~)t = I + P~ + 2P~2~ .
t=0 t=2

The top eigenvector of a stochastic matrix is the all-ones vector, e (Meyn & Tweedie, 2012), and it corresponds to the eigenvalue 1. Using this fact and the definition of P~ with respect to P^ we have:

(I + P~)e + 2P~2~ e = I + (I - N )P^ e + 2P~2~ e = (I + )e - N e + 2P~2~ e.

(5)

We can now bound the term 2P~2~ e using the fact that e is also the top eigenvector of the

successor

representation

and

has

eigenvalue

1 1-

(Machado

et

al.,

2018b):

0



2P~2~ e



1

2 -



e.

Plugging (5) into the definition of  we have (notice that (s)e = ||(s)||1): (s) = (1 + )e - (1 + )e + N e - 2P~2~ e = N e - 2P~2~ e  N e.

When we also use the other bound on the quadratic term we conclude that, for any state s,

 n(s) + 1

-

2 1-

 (s)



 n(s) +

1

.

In other words, the SSR, obtained after a slight change to the SR, can be used to recover state visitation counts. The intuition behind this result is that the phantom transition, represented by the +1 in the denominator of the SSR, serves as a proxy for the uncertainty about that state by underestimating the SR. This is due to the fact that s P~(s, s ) gets closer to 1 each time state s is visited.
This result can now be used to convert the SSR into a reward function in the tabular case. We do so by using the SSR to define an exploration bonus, rint, such that the reward being maximized by the agent becomes r(s, a) + rint(s), where  is a scaling parameter. Since we want to incentivize agents to visit the least visited states as quickly as possible, we can trivially define rint = -||~ (s)||1, where we penalize the agent by visiting the states that lead to commonly visited states. Notice that the shift (1 + ) in (s) has no effect as an exploration bonus because it is the same across all states.
4

Under review as a conference paper at ICLR 2019

Table 1: Comparison between our algorithm, termed ESSR, and R-MAX, E3, and MBIE. The numbers reported for R-MAX, E3, and MBIE are an estimate obtained from the histograms presented by Strehl & Littman (2008). The performance of our algorithm is the average over 100 runs. A 95%
confidence interval is reported between parentheses.

RIVERSWIM SIXARMS

E3 3,000,000 1,800,000

R-MAX 3,000,000 2,800,000

MBIE 3,250,000 9,250,000

ESSR 3,088,924 (± 57,584) 7,327,222 (± 1,189,460)

EVALUATING -||~ (s)||1 AS AN EXPLORATION BONUS
We evaluated the effectiveness of the proposed exploration bonus in a standard model-based algorithm. In our implementation the agent updates its transition probability model and reward model through Equation 2 and its SSR estimate as in Definition 3.1 (the pseudo-code of this algorithm is available in the Appendix), which is then used for the exploration bonus rint. We used the domains RiverSwim and SixArms (Strehl & Littman, 2008) to assess the performance of this algorithm.2 These are traditional domains in the PAC-MDP literature (Kakade, 2003) and are often used to evaluate provably sampleefficient algorithms. Details about these environments are also available in the Appendix. We used the same protocol used by Strehl & Littman (2008). Our results are available in Table 1. It is interesting to see that our algorithm performs as well as R-MAX (Brafman & Tennenholtz, 2002) and E3 (Kearns & Singh, 2002) on RiverSwim and it clearly outperforms these algorithms on SixArms.

4 COUNTING FEATURE ACTIVATIONS WITH THE SR
In large environments, where enumerating all states is not an option, directly using the SSR as described in the previous section is not viable. Learning the SSR becomes even more challenging when the representation, (·), is also being learned and so is non-stationary. In this section we design an algorithm for the function approximation setting inspired by the results from the previous section.
Since explicitly estimating the transition probability function is not an option, we learn the SR directly using TD learning. In order to capture the SSR we rely on TD's tendency to underestimate values when the estimates are pessimistically initialized, just as the SSR underestimates the true successor representation; with larger underestimates for states (and similarly features) that are rarely observed. This is mainly due to the fact that when the SR is being learned with TD learning, because a reward of 1 is observed at each time step, there is no variance in the target and the predictions slowly approach the true value of the SR. When pessimistically initialized, the predictions approach the target from below. In this sense, what defines how far a prediction is from its final target is indeed how many times it has been updated in a given state. Finally, recent work (Kulkarni et al., 2016; Machado et al., 2018b) have shown successor features can be learned jointly with the feature representation itself. These ideas are combined together to create our algorithm.
The neural network we used to learn the agent's value function while also learning the feature representation and the successor representation is depicted in Figure 1. The layers used to compute the state-action value function, q^(St, ·), are structured as in DQN (Mnih et al., 2015), but with different numbers of parameters (i..e, filter sizes, stride, and number of nodes). This was done to match Oh et al.'s (2015) architecture, which is known to succeed in the auxiliary task we define below. From here on, we will call the part of our architecture that predicts q^(St, ·) DQNe. It is trained to minimize
LTD = E (1 -  )(s, a) +  MC(s, a) 2 ,
2Our algorithm maximizes the discounted return ( = 0.95). We used policy iteration where the policy evaluation step is terminated when the estimates of the value function change by less than 0.01. In RiverSwim  was set to 100 and in SixArms  was set to 1000. These values were obtained after evaluating the algorithm for   {1, 10, 100, 200, 1000, 2000}. The code used to generate these results is available at: https: //github.com/ommitted/blind.
5

Under review as a conference paper at ICLR 2019

Conv 64,6x6 pad 0,0 stride 2

Conv 64,6x6 pad 2,2 stride 2

Conv 64,6x6 pad 2,2 stride 2

fc

ReLU

ReLU

ReLU

ReLU

4 x 84 x 84

64 x 20 x 20 64 x10 x 10 64 x 40 x 40

1024

fc ReLU
2048 fc
18

action fc
2048

Deconv

Deconv

Deconv

64,6x6

64,6x6

1,6x6

pad 2,2

pad 2,2

pad 0,0

stride 2

stride 2

stride 2

fc fc

ReLU

2048

1024

6400

64 x 20 x 20 64 x 40 x 40

84 x 84

q^(St, ·)

fc fc

ReLU ReLU

2048

1024

(St)

Figure 1: Neural network architecture used by our algorithm when learning to play Atari 2600 games.

where (s, a) and MC(s, a) are defined as

(s,

a)

=

Rt

+

rint(s;

-)

+



max
a

q(s

,

a

;

-)

-

q(s,

a;

),



MC(s, a) = t r(St, At) + rint(St; -) - q(s, a; ).

t=0

This loss is known as the mixed Monte-Carlo return (MMC) and it has been used in the past by the
algorithms that achieved succesful exploration in deep reinforcement learning (Bellemare et al., 2016; Ostrovski et al., 2017). The distinction between  and - is standard in the field, with - denoting
the parameters of the target network, which is updated less often for stability purposes (Mnih et al., 2015). As before, we use rint to denote the exploration bonus obtained from the successor features of the internal representation, (·), which will be defined below. Moreover, to ensure all features are in the same range, we normalize the feature vector so that ||(·)||2 = 1. In Figure 1 we highlight the layer in which we normalize its output with the symbol  . Notice that the features are always
non-negative due to the use of ReLU gates.

The successor features are computed by the two bottom layers of the network, which minimize the loss

LSR = E,p (St; -) + (St+1; -) - (St; ) 2 .

Zero is a fixed point for the SR. This is particularly concerning in settings with sparse rewards. The
agent might learn to set (·) = 0 to achieve zero loss. We address this problem by not propagating LSR to (·) (this is depicted in Figure 1 as an open circle stopping the gradient), and by creating an auxiliary task (Jaderberg et al., 2017) to encourage a representation to be learned before a non-zero
reward is observed. As Machado et al. (2018b), we use the auxiliary task of predicting the next
observation, learned through the architecture proposed by Oh et al. (2015), which is depicted as the top layers in Figure 1. The loss we minimize for this last part of the network is LRecons = S^t+1 - St+1 2.

The overall loss minimized by the network is L = wTDLTD + wSRLSR + w LRecons Recons.
The last step in describing our algorithm is to define rint(St; -), the intrinsic reward we use to encourage exploration. We choose the exploration bonus to be the inverse of the 2-norm of the vector of successor features of the current state, that is,

rint(St; -)

=

1 ||(St; -)||2

,

where (St; -) denotes the successor features of state St parametrized by -. The exploration bonus comes from the same intuition presented in the previous section, but instead of penalizing the agent with the norm of the SR we make rint(St; -) into a bonus (we observed in preliminary experiments not discussed here that DQN performs better when dealing with positive rewards).
Moreover, instead of using the 1-norm we use the 2-norm of the SR since our features have unit length in 2 (whereas the successor probabilities in the tabular-case have unit length in 1).

Finally, we initialize our network the same way Oh et al. (2015) does. We use Xavier initializa-
tion (Glorot & Bengio, 2010) in all layers except the fully connected layers around the element-wise multiplication denoted by , which are initialized uniformly with values between -0.1 and 0.1.

6

Under review as a conference paper at ICLR 2019

Table 2: Performance of the proposed algorithm, DQNeMMC+SR, compared to various agents on the "hard exploration" subset of Atari 2600 games. The DQN results reported are from Machado et al. (2018a) while the DQNMMC+CTS and DQNMMC+PixelCNN results were extracted from the learning curves available in Ostrovski et al.'s (2017) work. DQNMe MC denotes another baseline used in the comparison. When available, standard deviations are reported between parentheses. See text for details.

FREEWAY GRAVITAR MONT. REV. PRIVATE EYE SOLARIS VENTURE

DQN 32.4 (0.3) 118.5 (22.0) 0.0 (0.0) 1447.4 (2,567.9) 783.4 (55.3)
4.4 (5.4)

DQNMMC +CTS 29.2 199.8 2941.9 32.8 1147.1 0.0

DQNMMC +PIXELCNN 29.4 275.4 1671.7 14386.0 2279.4 856.2

DQNeMMC 29.5 (0.1) 1078.3 (254.1) 0.0 (0.0) 113.4 (42.3) 2244.6 (378.8) 1220.1 (51.0)

DQNMe MC+SR 29.5 (0.1) 430.3 (109.4) 1778.6 (903.6) 99.1 (1.8) 2155.7 (398.3) 1241.8 (236.0)

5 EMPIRICAL EVALUATION OF EXPLORATION IN DEEP RL
We evaluated our algorithm on the Arcade Learning Environment (Bellemare et al., 2013). Following Bellemare et al.'s (2016) taxonomy, we evaluated our algorithm in the Atari 2600 games with sparse rewards that pose hard exploration problems. They are: FREEWAY, GRAVITAR, MONTEZUMA'S REVENGE, PRIVATE EYE, SOLARIS, and VENTURE.3
We followed the evaluation protocol proposed by Machado et al. (2018a). We used MONTEZUMA'S REVENGE to tune our parameters (training set). The reported results are the average over 10 seeds after 100 million frames. We evaluated our agents in the stochastic setting (sticky actions,  = 0.25) using a frame skip of 5 with the full action set (|A| = 18). The agent learns from raw pixels, that is, it uses the game screen as input.
Our results were obtained with the algorithm described in Section 4. We set  = 0.025 after a rough sweep over values in the game MONTEZUMA'S REVENGE. We annealed in DQN's -greedy exploration over the first million steps, starting at 1.0 and stopping at 0.1 as done by Bellemare et al. (2016). We trained the network with RMSprop with a step-size of 0.00025, an value of 0.01, and a decay of 0.95, which are the standard parameters for training DQN (Mnih et al., 2015). The discount factor, , is set to 0.99 and wTD = 1, wSR = 1000, wRecons = 0.001. The weights wTD, wSR, and wRecons were set so that the loss functions would be roughly the same scale. All other parameters are the same as those used by Mnih et al. (2015).
Table 2 summarizes the results after 100 million frames. The performance of other algorithms is also provided for reference. Notice we are reporting learning performance for all algorithms instead of the maximum scores achieved by the algorithm. We use the superscript MMC to distinguish between the algorithms that use MMC from those that do not. When comparing our algorithm, DQNeMMC+SR, to DQN we can see how much our approach improves over the most traditional baseline. By comparing our algorithm's performance to DQNMMC+CTS (Bellemare et al., 2016) and DQNMMC+PixelCNN (Ostrovski et al., 2017) we compare our algorithm to established baselines for exploration. As highlighted in Section 4, the parameters of the network we used are different from those used in the traditional DQN network, so we also compared the performance of our algorithm to the performance of the same network our algorithm uses but without the additional modules (next state prediction and successor representation) by setting wSR = wRecons = 0 and without the intrinsic reward bonus by setting  = 0.0. The column labeled DQNeMMC contains the results for this baseline. This comparison allows us to explicitly quantify the improvement provided by the proposed exploration bonus. The learning curves of these algorithms, their performance after different amounts of experience, and additional results analyzing, for example, the impact of the introduced auxiliary task, are available in the Appendix.
We can clearly see that our algorithm achieves scores much higher than those achieved by DQN, which struggles in games that pose hard exploration problems. Moreover, by comparing DQNMe MC+SR to DQNMe MC we can see that the provided exploration bonus has a big impact in the game MONTEZUMA'S REVENGE, which is probably known as the hardest game among those we used in our evaluation. Interestingly, the change in architecture and the use of MMC leads to a big improvement in games such as GRAVITAR and VENTURE, which we cannot fully explain. However, notice that the change
3The code used to generate these results is available at: https://github.com/ommitted/blind.
7

Under review as a conference paper at ICLR 2019
in architecture does not have any effect in MONTEZUMA'S REVENGE. The proposed exploration bonus seems to be essential in this game. Finally, we also compared our algorithm to DQNMMC+CTS and DQNMMC+PixelCNN. We can observe that, on average, it performs as well as these algorithms, but instead of requiring a density model it requires the SR, which is already defined for every problem since it is a component of the value function estimates, as discussed in Section 2.
6 RELATED WORK
There are multiple algorithms in the tabular, model-based case with guarantees about their performance (e.g., Brafman & Tennenholtz, 2002; Kearns & Singh, 2002; Strehl & Littman, 2008; Osband et al., 2016). RiverSwim and SixArms are domains traditionally used when evaluating these algorithms. In this paper we have given evidence that our algorithm performs as well as some of these algorithms with theoretical guarantees. Among these algorithms, R-MAX seems the closest approach to ours. As with R-MAX, the algorithm we presented in Section 3 augments the state-space with an imaginary state and encourages the agent to visit that state, implicitly reducing the algorithm's uncertainty in the state-space. However, R-MAX deletes the transition to this imaginary state once a state has been visited a given number of times. Ours lets the probability of visiting this imaginary state vanish with additional visitations. Moreover, notice that it is not clear how to apply these traditional algorithms such as R-MAX and E3 to large domains where function approximation is required.
Conversely, there are not many model-free approaches with proven sample-complexity bounds (e.g., Strehl et al., 2006), but there are multiple model-free algorithms for exploration that actually work in large domains (e.g., Stadie et al., 2015; Bellemare et al., 2016; Ostrovski et al., 2017; Plappert et al., 2018). Among these algorithms, the use of pseudo-counts through density models is the closest to ours (Bellemare et al., 2016; Ostrovski et al., 2017). Inspired by those papers we used the mixed Monte-Carlo return as a target in the update rule. In Section 5 we have shown that our algorithm performs generally as well as these approaches without requiring a density model. Importantly, Martin et al. (2017) had already shown that counting activations of fixed, handcrafted features in Atari 2600 games leads to good exploration behavior. Nevertheless, by using the SSR we are not only counting learned features but we are also implicitly capturing the induced transition dynamics.
Finally, the SR has already been used in the context of exploration. However, it was used to help the agent learn how to act in a higher level of abstraction in order to navigate through the state space faster (Machado et al., 2017; 2018b). Such an approach has led to promising results in the tabular case but only anecdotal evidence about its scalability has been provided when the idea was applied to large domains such as Atari 2600 games. Importantly, the work developed by Machado et al. (2018b), Kulkarni et al. (2016) and Oh et al. (2015) are the main motivation for the neural network architecture presented here. Oh et al. (2015) have shown how one can predict the next screen given the current observation and action (our auxiliary task), while Machado et al. (2018b) and Kulkarni et al. (2016) have proposed different architectures for learning the successor representation from raw pixels.
7 CONCLUSION
RL algorithms tend to have high sample complexity, which often prevents them from being used in the real-world. Poor exploration strategies is one of the main reasons for this high sample-complexity. Despite all of its shortcomings, uniform random exploration is, to date, the most commonly used approach for exploration. This is mainly due to the fact that most approaches for tackling the exploration problem still rely on domain-specific knowledge (e.g., density models, handcrafted features), or on having an agent learn a perfect model of the environment. In this paper we introduced a general method for exploration in RL that implicitly counts state (or feature) visitation in order to guide the exploration process. It is compatible to representation learning and the idea can also be adapted to be applied to large domains.
This result opens up multiple possibilities for future work. Based on the results presented in Section 3, for example, we conjecture that the substochastic successor representation can be actually used to generate algorithms with PAC-MDP bounds. Investigating to what extent different auxiliary tasks impact the algorithm's performance, and whether simpler tasks such as predicting feature activations or parts of the input (Jaderberg et al., 2017) are effective is also worth studying. Finally, it might be interesting to further investigate the connection between representation learning and exploration, since it is also known that better representations can lead to faster exploration (Jiang et al., 2017).
8

Under review as a conference paper at ICLR 2019
REFERENCES
Andre´ Barreto, Will Dabney, Re´mi Munos, Jonathan Hunt, Tom Schaul, David Silver, and Hado van Hasselt. Successor Features for Transfer in Reinforcement Learning. In Advances in Neural Information Processing Systems (NIPS), pp. 4058­4068, 2017.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research, 47:253­279, 2013.
Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Re´mi Munos. Unifying Count-Based Exploration and Intrinsic Motivation. In Advances in Neural Information Processing Systems (NIPS), pp. 1471­1479, 2016.
Richard E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.
Ronen I. Brafman and Moshe Tennenholtz. R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning. Journal of Machine Learning Research, 3:213­231, 2002.
Peter Dayan. Improving Generalization for Temporal Difference Learning: The Successor Representation. Neural Computation, 5(4):613­624, 1993.
Xavier Glorot and Yoshua Bengio. Understanding the Difficulty of Training Deep Feedforward Neural Networks. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 249­256, 2010.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement Learning with Unsupervised Auxiliary Tasks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual Decision Processes with Low Bellman Rank are PAC-Learnable. In Proceedings of the International Conference on Machine Learning (ICML), pp. 1704­1713, 2017.
Sham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, Gatsby Computational Neuroscience Unit, University College London, 2003.
Michael J. Kearns and Satinder P. Singh. Near-Optimal Reinforcement Learning in Polynomial Time. Machine Learning, 49(2-3):209­232, 2002.
Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J. Gershman. Deep Successor Reinforcement Learning. CoRR, abs/1606.02396, 2016.
Marlos C. Machado, Marc G. Bellemare, and Michael Bowling. A Laplacian Framework for Option Discovery in Reinforcement Learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 2295­2304, 2017.
Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. Journal of Artificial Intelligence Research, 61:523­562, 2018a.
Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray Campbell. Eigenoption Discovery through the Deep Successor Representation. In Proceedings of the International Conference on Learning Representations (ICLR), 2018b.
Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-Based Exploration in Feature Space for Reinforcement Learning. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 2471­2478, 2017.
Sean P Meyn and Richard L Tweedie. Markov Chains and Stochastic Stability. 2012.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level Control through Deep Reinforcement Learning. Nature, 518:529­533, 2015.
9

Under review as a conference paper at ICLR 2019
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L. Lewis, and Satinder P. Singh. ActionConditional Video Prediction using Deep Networks in Atari Games. In Advances in Neural Information Processing Systems (NIPS), pp. 2863­2871, 2015.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and Exploration via Randomized Value Functions. In Proceedings of the International Conference on Machine Learning (ICML), pp. 2377­2386, 2016.
Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, and Re´mi Munos. Count-Based Exploration with Neural Density Models. In Proceedings of the International Conference on Machine Learning (ICML), pp. 2721­2730, 2017.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter Space Noise for Exploration. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.
Bradly C. Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing Exploration in Reinforcement Learning With Deep Predictive Models. CoRR, abs/1507.00814, 2015.
Alexander L. Strehl and Michael L. Littman. An Analysis of Model-based Interval Estimation for Markov Decision Processes. Journal of Computer and System Sciences, 74(8):1309­1331, 2008.
Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC Model-Free Reinforcement Learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 881­888, 2006.
Richard S. Sutton. Learning to Predict by the Methods of Temporal Differences. Machine Learning, 3:9­44, 1988.
Gerald Tesauro. Temporal Difference Learning and TD-Gammon. Communications of the ACM, 38 (3):58­68, 1995.
10

Under review as a conference paper at ICLR 2019

Supplemental Material Count-Based Exploration with the Successor Representation
This supplementary material contains details omitted from the main text due to space constraints. The list of contents is below:
· Pseudo-code of the model-based algorithm discussed in Section 3; · Description of RiverSwim and SixArms, the tabular domains we used in our evaluation; · Learning curves of DQNe and DQNMe MC+SR and their performance after different amounts
of experience in the Atari 2600 games used for evaluation; · Results of additional experiments designed to evaluate the role of the auxiliary task in the
results reported in the paper for ESSR.

EXPLORATION THROUGH THE SUBSTOCHASTIC SUCCESSOR REPRESENTATION
In the main paper we described our algorithm as a standard model-based algorithm where the agent updates its transition probability model and reward model through Equation 2 and its SSR estimate as in Definition 3.1. The pseudo-code with details about the implementation is presented in Algorithm 1.

Algorithm 1 Exploration through the Substochastic Successor Representation (ESSR)

n(s, s )  0

s, s  S

t(s, a, s )  1 s, s  S, a  A

r^(s, a)  0

s  S, a  A

P^(s, a)  1/|S| s  S, a  A

P~(s, s )  0

s, s  S

  random over A

while episode is not over do Observe s  S, take action a  A selected according to (s), and observe a reward R and a next state s  S

n(s, s )  n(s, s ) + 1

t(s, a, s )  t(s, a, s ) + 1

n(s)  x ,b t(s, b, x )

n(s, a)  x t(s, a, x )

r^(s, a, s

)



(t(s,a,s )-2)×r^(s,a,s t(s,a,s )-1

)+R

for each state x  S do

P^(s, a, x

)



t(s,a,x ) n(s,a)

P~(s, x

)



n(s,x ) n(s)+1

end for ~  (I - P~)-1

rint  -~ e   POLICYITERATION(P^, r^ + rint)

end while

11

Under review as a conference paper at ICLR 2019

DESCRIPTION OF RIVERSWIM AND SIXARMS
The two domains we used as testbed to evaluate the proposed model-based algorithm with the exploration bonus generated by the substochastic successor representation are shown in Figure 2. These domains are the same used by Strehl & Littman (2008). For SixArms, the agent starts in state 0. For RiverSwim, the agent starts in either state 1 or 2 with equal probability.

1, 0.7, 0

1, 0.6, 0

1, 0.6, 0

1, 0.6, 0

1, 0.6, 0

1, 0.3, 10000

0, 1, 5

1, 0.3, 0

1, 0.3, 0

1, 0.3, 0

1, 0.3, 0

1, 0.3, 0

0 123 4 5

0, 1, 0 1, 0.1, 0

0, 1, 0 1, 0.1, 0

0, 1, 0 1, 0.1, 0

0, 1, 0 1, 0.1, 0

0, 1, 0 1, 0.1, 0

3, 1, 800

4 0-42-,51, ,10, 00, 1, 0 1

0-3, 1, 50 5, 1, 50

3, 0.05, 0

4, 1, 0

4, 1, 1660

5, 1, 0 0-3, 1, 0
5 4, 0.03, 0

0 1, 0.15, 0 2

1, 1, 133

0, 12, -05, 1, 0

0-4, 1, 0

5, 1, 6000

6

5, 0.01, 0

2, 0.1, 0

03--15,,

1, 1,

00

3

2, 1, 300

(a) RiverSwim

(b) SixArms

Figure 2: Domains used as testbed in the tabular case. The tuples in each transition should be read as action id, probability, reward . See text for details.

EVALUATION THE IMPACT OF THE AUXILIARY TASK IN ESSR
The algorithm we introduced in the paper, ESSR, relies on a network that estimates the state-action value function, the successor representation, and the next observation to be seen given the agent's current observation and action. While the results depicted in Table 2 allow us to clearly see the benefit of using an exploration bonus derived from the successor representation, they do not inform us about the impact of the auxiliary task in the results. The experiments in this section aim at addressing this issue. We focus on Montezumas Revenge because it is the game where the problem of exploration is maximized, with most algorithms not being able to do anything without an exploration bonus.
The first question we asked was whether the auxiliary task was necessary in our algorithm. We evaluated this by dropping the reconstruction module from the network to test whether the initial random noise generated by the successor representation is enough to drive representation learning. It is not. When dropping the auxiliary task, the average performance of this baseline over 4 seeds in MONTEZUMA'S REVENGE after 100 million frames was 100.0 points (2 = 200.0; min: 0.0, max: 400.0). As comparison, our algorithm obtains 1778.6 points (2 = 903.6, min: 400.0, max: 2500.0). These results suggest that auxiliary tasks seem to be necessary for our method to perform well.
We also evaluated whether the auxiliary task was sufficient to generate the results we observed. To do so we dropped the SR module and set  = 0.0 to evaluate whether our exploration bonus was actually improving the agent's performance or whether the auxiliary task was doing it. The exploration bonus seems to be essential in our algorithm. When dropping the exploration bonus and the successor representation module, the average performance of this baseline over 4 seeds in MONTEZUMA'S REVENGE after 100 million frames was 398.5 points (2 = 230.1; min: 0.0, max: 400.0). Again, clearly, the auxiliary task is not a sufficient condition for the performance we report.
The reported results use the same parameters as those reported in the main paper. Learning curves for each individual run are depicted in Figure 3.

Score Score

Montezuma's Revenge 1750 DQNeMMC + SR without aux. task
DQNeMMC + SR 1500
1250
1000
750
500
250
0
00M 10M 20M 30M N40uMm.5F0Mram60eMs 70M 80M 90M 100M

Montezuma's Revenge 1750 DQNeMMC with aux. task
DQNeMMC + SR 1500
1250
1000
750
500
250
0
00M 10M 20M 30M N40uMm.5F0Mram60eMs 70M 80M 90M 100M

Figure 3: Evaluation of the sufficiency and necessity of the auxiliary task in DQNMe MC+SR. The learning curves are smoothed with a running average computed using a window of size 100.

12

Under review as a conference paper at ICLR 2019

ADDITIONAL RESULTS FOR DQNeMMC +SR AND DQNeMMC IN THE ATARI 2600 GAMES
As recommended by Machado et al. (2018a), we report the performance of DQNMe MC+SR and DQNMe MC after different amounts of experience (10, 50, and 100 million frames) in Tables 3 and 4. Finally, Figure 4 depicts the learning curves obtained with the evaluated algorithms in each game. Lighter lines represent individual runs while the solid lines encode the average over the multiple runs.

Game
FREEWAY GRAVITAR MONT. REVENGE PRIVATE EYE SOLARIS VENTURE

10M frames

24.9 244.1
2.6 99.2 1547.5 26.2

(0.5) (23.8) (7.2) (1.2) (410.9) (22.1)

50M frames

29.5 326.4 563.8 98.5 2036.3 942.0

(0.1) (53.0) (465.7) (3.3) (339.0) (423.8)

100M frames

29.5 430.3 1778.6 99.1 2155.7 1241.8

(0.1) (109.4) (903.6) (1.8) (398.3) (236.0)

Table 3: Results obtained with DQNMe MC+SR after different amounts of experience.

Game
FREEWAY GRAVITAR MONT. REVENGE PRIVATE EYE SOLARIS VENTURE

10M frames
25.7 (1.5) 229.9 (31.3)
0.0 (0.0) 216.7 (219.5) 2230.0 (322.3) 63.8 (31.3)

50M frames
29.6 (0.1) 559.3 (75.9)
0.0 (0.0) 109.1 (44.1) 2181.5 (292.9) 794.1 (151.9)

100M frames
29.5 (0.1) 1078.3 (254.1)
0.0 (0.0) 113.4 (42.3) 2244.6 (378.8) 1220.1 (51.0)

Table 4: Results obtained with DQNMe MC after different amounts of experience.

Score Score Score Score Score Score

Freeway
30

25

20

15

10

5 DQNeMMC + SR 0 DQNeMMC
00M 10M 20M 30M N40uMm.5F0Mram60eMs 70M 80M 90M 100M
Private Eye

3000

DQNeMMC + SR DQNeMMC

2500

2000

1500

1000

500

0
00M 10M 20M 30M N40uMm.5F0Mram60eMs 70M 80M 90M 100M

Gravitar

1750

DQNeMMC + SR DQNeMMC

1500

1250

1000

750

500

250

00M 10M 20M 30M N40uMm.5F0Mram60eMs 70M 80M 90M 100M
Solaris

3000
2500
2000
1500
1000 DQNeMMC + SR DQNeMMC
00M 10M 20M 30M N40uMm.5F0Mram60eMs 70M 80M 90M 100M

Montezuma's Revenge
2500 DQNeMMC + SR DQNeMMC
2000

1500

1000

500

0
00M 10M 20M 30M N40uMm.5F0Mram60eMs 70M 80M 90M 100M
Venture

1400

DQNeMMC + SR DQNeMMC

1200

1000

800

600

400

200

0
00M 10M 20M 30M N40uMm.5F0Mram60eMs 70M 80M 90M 100M

Figure 4: DQNMe MC+SR and DQNMe MC learning curves in the Atari 2600 games used as testbed. The curves are smoothed with a running average computed using a window of size 100.

13

