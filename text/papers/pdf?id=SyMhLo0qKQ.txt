Under review as a conference paper at ICLR 2019
DISTRIBUTION-INTERPOLATION TRADE OFF IN GENERATIVE MODELS
Anonymous authors Paper under double-blind review
ABSTRACT
We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors ­ regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space. We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions.
1 INTRODUCTION
Generative latent variable models have grown to be a very popular research topic, with Variational Auto-Encoders (VAEs) (Kingma & Welling, 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) gaining a lot of interest in the last few years. VAEs use a stochastic encoder network to embed input data in a typically lower dimensional space, using a conditional probability distribution p(z|x) over possible latent space codes z  RD. A stochastic decoder network is then used to reconstruct the original sample. GANs, on the other hand, use a generator network that creates data samples from noise z  p(z), where p(z) is a fixed prior distribution, and train a discriminator network jointly to distinguish between real and generated data.
Both of these model families require a probability distribution to be defined on the latent space. The most popular variants are the multidimensional normal distribution and the uniform distribution on the zero-centred hypercube. Given a trained model, studying the structure of the latent space is a common way to measure generator capabilities.
1.1 MOTIVATION BEHIND THE LINEAR INTERPOLATION
There are various methods used to analyse the latent space. Locally, one can sample and decode points in close neighbourhood of a given latent vector to investigate a small region in the space. On the other hand, global methods are designed to capture long-distance relationships between points in the space, e.g. latent arithmetics, latent directions analysis, and interpolations (see e.g. Mikolov et al. (2013); Kilcher et al. (2017); Radford et al. (2015); White (2016); Agustsson et al. (2017)).
The main advantage of using interpolations is the interpretability that comes with dealing with onedimensional curves, instead of high-dimensional Euclidean space. For example, if the model has managed to find a meaningful representation, one would expect the latent space to be organised in a way that reflects the internal structure of the training dataset. In that case, decoding an interpolation will show a gradual transformation of one endpoint into the other. Contrarily, if the model memorises the data, the latent space might consist of regions corresponding to particular training examples, divided by boundaries with unnatural, abrupt changes in generated data (Arvanitidis et al., 2017).
What distinguishes interpolations from other low-dimensional methods is the shortest path property. In absence of any additional knowledge about the latent space, it feels natural to use the Euclidean
1

Under review as a conference paper at ICLR 2019
metric. In that case, the shortest path between two points is defined as a segment. This is the linear interpolation, formally defined as f L(x1, x2, ) = (1 - )x1 + x2, for   [0, 1], where x1, x2 are the endpoints.
While traversing the latent space along the shortest path between two points, a well-trained model should transform the samples in a sensible way. For example, if the modelled data has a natural hierarchy, we would expect the interpolation to reflect it, i.e. an image of a truck should not arise on a path between images of a cat and a dog. Also, if the data can be described with a set of features, then an interpolation should maintain any features shared by the endpoints along the path. For example, consider a dataset of images of human faces, with features such as wearing sunglasses, having a long beard, etc.
It is worth noting that there has been an amount of work done on equipping the latent space with a stochastic Riemannian metric (Arvanitidis et al., 2017) that additionally depends on the generator function. The role of the shortest paths is fulfilled by the geodesics, and the metric is defined precisely to enforce some of the properties mentioned above. This approach is somewhat complementary to the one we are concerned with ­ instead of analysing the latent space using simple tools, we would need to find a more sophisticated metric that describes the latent space comprehensively, and then analyse the metric itself.
1.2 THE DISTRIBUTION MISMATCH
While considered useful, the linear interpolation used in conjunction with the most popular latent distributions results in a distribution mismatch (also defined in Agustsson et al. (2017); Kilcher et al. (2017)). That is, if we fix the  coefficient and interpolate linearly between two endpoints sampled from the latent space distribution, the probability distribution of the resulting vectors will differ significantly from the latent distribution. This can be partially explained by the well-known fact that in high dimensions the norms of vectors drawn from the latent distribution are concentrated around a certain value. As a consequence, the midpoints of sampled pairs of latent vectors will have, on average, significantly smaller norm. Thus, the linear interpolation oversamples regions in close proximity of the origin of the latent space. A thorough analysis of this phenomenon will be conducted in section 2.1.
Such behaviour raises questions about the applicability of the linear interpolation to study the latent space. Indeed, changing the latent distribution after the model was trained may have unexpected consequences. In Kilcher et al. (2017), experiments conducted using a DCGAN model (Radford et al., 2015) on the celebA dataset (Liu et al., 2015) showed flawed data generation near the latent space origin. Other works concerning the traversal of latent space do not mention this effect, e.g. Agustsson et al. (2017). We recreated this experiment, and concluded that it might be caused by stopping the training process too early (see Appendix C figure 7 for a visualisation). This may explain the apparent disagreement in the literature. Nevertheless, with either a midpoint decoding to a median face, or a non-sensible sample, the interpolation is not informative ­ we would like to see smooth change of features, and not a transition through the same, homogeneous region.
The solution is, either, to change the latent distribution so that the linear interpolation will not cause a distribution mismatch, or redefine the shortest path property. A simple well-known compromise is to use spherical interpolations (Shoemake, 1985; White, 2016). As the latent distribution is concentrated around a sphere, replacing segments with arcs causes relatively small distribution mismatch (see section 3.2). Nonetheless, reducing the consequences of the distribution mismatch is still a popular research topic (Agustsson et al., 2017; Kilcher et al., 2017; Arvanitidis et al., 2017).
1.3 MAIN CONTRIBUTIONS
In section 2.1 we show that if the linear interpolation does not change the latent probability distribution, then it must be trivial or "pathological". Then, in section 2.2, we give an example of such an invariant distribution, namely the Cauchy distribution, thus proving its existence. We also discuss the negative consequences of choosing a heavy-tailed probability distribution as the latent prior.
In section 3 we relax the shortest path property of interpolations, and investigate non-linear interpolations that do not cause the latent distribution mismatch. We describe a general framework for creating such interpolations, and give two concrete examples in sections 3.4 and 3.5.
2

Under review as a conference paper at ICLR 2019

The experiments conducted using the DCGAN model on the CelebA dataset are presented solely to illustrate the problem, not to study the DCGAN itself, theoretically or empirically.

2 LATENT DISTRIBUTIONS
In this section we will tackle the problem of distribution mismatch by selecting a proper latent distribution. Let us assume that we want to train a generative model which has a D-dimensional latent space and a fixed latent probability distribution, defined by random variable Z. We denote by X  X that the random variable X has distribution X . Xn X represents the fact that the sequence of random variables {Xn}nN converges weakly to a random variable with distribution X as n tends to infinity. By Xn Xn we mean that limn supxR |CDFXn (x) - CDFXn (x)| = 0, where CDFX denotes the cumulative distribution function of X. The index n will usually be omitted for readability. In other words, by X X we mean, informally, that X has distribution similar to X .

2.1 LINEAR INTERPOLATION INVARIANCE PROPERTY

Property 2.1 (Linear Interpolation Invariance). If Z defines a distribution on the D-dimensional latent space, Z(1) and Z(2) are independent and distributed identically to Z, and for every   [0, 1] the random variable f L(Z(1), Z(2), ) := (1 - )Z(1) + Z(2) is distributed identically to Z, then
we will say that Z has the linear interpolation invariance property, or that the linear interpolation
does not change the distribution of Z.

The most commonly used latent probability distributions Z are products of D independent random
variables. That is, Z = (Z1, Z2, . . . , ZD), where Z1, Z2, . . . , ZD are the independent marginals distributed identically to Z. If the norms of Z concentrate around a certain value, then the latent
distribution resembles sampling from a zero-centred sphere and the linear interpolation oversamples
regions in the proximity of the origin of the latent space. As a consequence, Z does not have
the linear interpolation invariance property. The following observation will shed light upon this problem. Let N (µ, 2) denote the normal distribution with mean µ and variance 2.

Observation 2.1. Z N Dµ,

Let us assume that Z2

2 4µ

as D  . If µ =

has finite mean µ and finite variance 0, then Z = 0 almost everywhere.

2.

If

µ

>

0,

then

The proof of this and all further observations is presented in the appendix B.

For example, if Z  N (0, 1), then Z is distributed according to the D-dimensional normal dis-

tribution with mean 0 and identity covariance matrix I. Z2 has moments µ = 1, 2 = 2, thus

Z

N

D,

1 2

1.

The second example is Z

 U(-1, 1), where U(a, b) is the uniform distri-

bution on the interval [a, b], and Z is distributed uniformly on the hypercube [-1, 1]D. In that case,

Z2

has

moments

µ

=

1 3

,

2

=

4 45

,

thus

Z

N

D 3

,

1 15

.

It is worth noting that the variance of the approximated probability distribution of Z , the thickness of the sphere, does not change as D tends to infinity ­ only the radius of the sphere is affected. On the other hand, if the latent distribution is normalised (divided by the expected value of Z ), then the distribution concentrates around the unit sphere (not necessarily uniformly), and we observe the so-called soap bubble phenomenon (Ferenc, 2017).

One might think that the factorisation of the latent probability distribution is the main reason

why the linear interpolation changes the distribution. Unfortunately, this is not the case. Let

Z :=

1 2

(Z(1)

+

Z(2)),

where

Z(1), Z(2)

are

two

independent

samples

from

Z.

Therefore,

Z

is

the

distribution of the middle points of a linear interpolation between two vectors drawn independently

from Z.

Observation 2.2. If Z has a finite mean, and Z is distributed identically to Z, then Z must be concentrated at a single point.

1If Z  N (0, 1), then Z is distributed according to the chi distribution, equal to the square root of the chi-squared distribution.

3

Under review as a conference paper at ICLR 2019

There have been attempts to find Z, with finite mean, such that Z is at least similar to Z. Kilcher et al. (2017) managed to reduce the distribution mismatch by defining the latent distribution as

V  U (SD-1),

r



1 (

,

),

 > 0,

 Z = rV,

2

where

U

(S

D-1

)

is

the

uniform

distribution

on

the

unit

sphere,

and

(

1 2

,

)

is

the

gamma

distribution.

We extend this idea by using a distribution that has no finite mean, namely the Cauchy distribution.

2.2 THE CAUCHY DISTRIBUTION
The standard Cauchy distribution is denoted by C(0, 1), and its density function is defined as 1/ (1 + x2) . The most important property of the Cauchy distribution is the fact that if C(1), . . . , C(n) are independent samples from the standard Cauchy distribution, and 1, . . . , n  [0, 1] with 1 + . . . + n = 1, then 1C(1) + . . . + nC(n) is also distributed according to the standard Cauchy distribution. In case of n = 2 it means that the Cauchy distribution satisfies the distribution matching property. On the other hand, as a consequence of observation 2.2, the Cauchy distribution cannot have finite mean. In fact, all of its moments of order greater than or equal to one are undefined. See Siegrist (2017) for further details.
There are two ways of using the Cauchy distribution in high dimensional spaces while retaining the distribution matching property. The multidimensional Cauchy distribution is defined as a product of independent standard Cauchy distributions. Then, the linear interpolation invariance property can be simply proved by applying the above formulas coordinate-wise. In case of vectors drawn from the multidimensional Cauchy distribution we may expect that some of the coordinates will be sufficiently larger, by absolute value, than the others (Hansen et al., 2006), thus making the latent distribution similar to coordinate-wise sampling.
In contrast, the multivariate Cauchy distribution comes with the isotropy property at the cost of the canonical directions becoming statistically dependent. There are multiple ways of defining it, and further analysis is out of the scope of this paper. We tested both variants as latent distributions with similar results. From now on, we shall concentrate on the non-isotropic Cauchy distribution.
The Cauchy distribution is a member of the family of stable distributions, and has been previously used to model heavy-tailed data (Nolan, 2018). However, according to our best knowledge, the Cauchy distribution has never been used as the latent distribution in generative models. Figure 1 shows a set of samples from the DCGAN model trained on the CelebA dataset using the Cauchy distribution and the distribution from Kilcher et al. (2017). Figure 2 presents decoded linear interpolations between random latent vectors.

Figure 1: Comparison of samples from DCGAN trained on the Cauchy distribution (left) and one trained on the distribution proposed by Kilcher et al. (2017) (right).

Figure 2: Comparison of linear interpolations from DCGAN trained on the Cauchy distribution (left) and one trained on the distribution proposed by Kilcher et al. (2017) (right).
It should be noted that if D is large enough, the distribution of the norms of vectors sampled from the D-dimensional Cauchy distribution has a low density near zero ­ similarly to the normal and
4

Under review as a conference paper at ICLR 2019
uniform distributions ­ but linear interpolations do not oversample this part of the latent space, due to the heavy-tailed nature of the Cauchy distribution. Comparison of the distributions of norms is given in Figure 3.
Figure 3: Distributions of Euclidean norms of latent vectors, for different probability distributions, with increasing latent space dimension.
The distribution-interpolation trade off states that if the probability distribution has the linear interpolation invariance property, then it must be trivial or heavy-tailed. In case of the Cauchy distribution we observed issues with generating images if the norm of the sampled latent vector was relatively large (the probability distribution of the norms is also heavy-tailed). Some of those faulty examples are presented in the appendix C. This is consistent with the known fact, that artificial networks perform poorly if their inputs are not normalised (see e.g. Glorot & Bengio (2010)). A probability distribution having the linear interpolation invariance property cannot be normalised using linear transformations. For example, the batch normalisation technique (Ioffe & Szegedy, 2015) would be highly ineffective, as the mean of a batch of samples is, in fact, a single sample from the distribution. On the other hand, using a non-linear normalisation (e.g., clipping the norm of the latent vectors in subsequent layers), is mostly equivalent to changing the latent probability distribution and making the interpolation non-linear. This idea will be explored in the next section.
3 INTERPOLATIONS
In this section we review the most popular variants of interpolations, with an emphasis on the distribution mismatch analysis. We also present two new examples of interpolations stemming from a general scheme, that perform well with the popular latent priors. An interpolation on the latent space RD is formally defined as a function
f : RD × RD × [0, 1] (x1, x2, )  x  RD. For brevity, we will represent f (x1, x2, ) by fx1,x2 (). Property 3.1 (Distribution Matching Property). If Z defines a distribution on the D-dimensional latent space, Z(1) and Z(2) are independent and distributed identically to Z, and for every   [0, 1] the random variable fZ(1),Z(2) () is distributed identically to Z, then we will say that the interpolation f has the distribution matching property in conjunction with Z, or that the interpolation f does not change the distribution of Z.
3.1 LINEAR INTERPOLATION The linear interpolation is defined as fxL1,x2 () = (1 - )x1 + x2. This interpolation does not satisfy the distribution matching property for the most commonly used probability distributions, as they have a finite mean. A notable exception is the Cauchy distribution. This was discussed in details in the previous section.
5

Under review as a conference paper at ICLR 2019

3.2 SPHERICAL LINEAR INTERPOLATION As in Shoemake (1985); White (2016), the spherical linear interpolation is defined as

fxS1L,x2 ()

=

sin [(1 - )] sin  x1

+

sin[] sin  x2,

where  is the angle between vectors x1 and x2. Note that this interpolation is undefined for parallel endpoint vectors, and the definition cannot be extended without losing the continuity. Also, if vectors x1 and x2 have the same length R, then the interpolation corresponds to a geodesic on the sphere of radius R. In this regard, it might be said that the spherical linear interpolation is defined as the shortest path on the sphere. The most important fact is that this interpolation can have the distribution matching property.
Observation 3.1. If Z has uniform distribution on the zero-centred sphere of radius R > 0, then f SL does not change the distribution of Z.

3.3 NORMALISED INTERPOLATION Introduced in Agustsson et al. (2017), the normalised2 interpolation is defined as

fxN1,x2 ()

=

(1 - )x1 + x2 (1 - )2 + 2

.

Observation 3.2. If Z  N (0, I), then f N does not change the distribution of Z.

If vectors x1 and x2 are orthogonal and have equal length, then the curve defined by this interpolation is equal to the one of the spherical linear interpolation. On the other hand, the normalised
interpolation behaves poorly if x1 is close to x2. In the extreme case of x1 = x2 the interpolation is not constant with respect to , which violates any sensible definition of the shortest path.

3.4 CAUCHY-LINEAR INTERPOLATION
Here we present a general way of designing interpolations that have the distribution matching property in conjunction with a given probability distribution Z. This method requires some additional assumptions about Z, but it works well with the most popular latent distributions.
Let L be the D-dimensional latent space, Z define the probability distribution on the latent space, C be distributed according to the D-dimensional Cauchy distribution on L, K be a subset of L such that Z is concentrated on this set, and g : L  K be a bijection such that g(C) is distributed identically to Z on K. Then for x1, x2  K we define the Cauchy-linear interpolation as
fxC1L,x2 () = g (1 - )g-1(x1) + g-1(x2) . In other words, for endpoints x1, x2  Z:
1. Transform x1 and x2 using g-1. This step changes the latent distribution to the D-dimensional Cauchy distribution.
2. Linearly interpolate between the transformations to get x = (1 - )g-1(x1) + g-1(x2) for all   [0, 1]. The transformed latent distribution remains unchanged.
3. Transform x back to the original space using g. We end up with the original latent distribution.
Observation 3.3. With the above assumptions about g the Cauchy-linear interpolation does not change the distribution of Z.
Finding an appropriate function g might seem hard, but in practice it usually is fairly straightforward. For example, if Z is distributed identically to the product of D independent one-dimensional distributions Z, then we can define g as CDFC-1  CDFZ applied to every coordinate.
2Originally referred to as distribution matched.

6

Under review as a conference paper at ICLR 2019

3.5 SPHERICAL CAUCHY-LINEAR INTERPOLATION
We might want the interpolation to have some other desired properties. For example, to behave exactly as the spherical linear interpolation if only the endpoints have equal norm. For that purpose, we need to make additional assumptions. Let Z be isotropic, C be distributed according to the onedimensional Cauchy distribution, and g : R  (0, +) be a bijection such that g(C) is distributed identically as Z on (0, +). Then we can modify the spherical linear interpolation formula to define what we call the spherical Cauchy-linear interpolation

fxS1C,xL2 () =

sin [(1 - )]

x1

sin[] +

x2

sin  x1 sin  x2

g (1 - )g-1( x1 ) + g-1( x2 ) ,

where  is the angle between vectors x1 and x2. In other words:

1. Interpolate the directions of latent vectors using the spherical linear interpolation. 2. Interpolate the norms using the Cauchy-linear interpolation.

Observation 3.4. With the above assumptions about g, the spherical Cauchy-linear interpolation does not change the distribution of Z if the Z distribution is isotropic.

The simplest candidate for the g function is CDFC-1  CDF Z , but we usually need to know more about Z to check if the assumptions hold. For example, let Z be a D-dimensional normal distribution with zero mean and identity covariance matrix. Then Z  D2 and

CDFD2 (x)

=

CDFD2 (x2)

=

1 (D/2)



D x2 ,
22

, for every x  0,

where  denotes the gamma function, and  is the lower incomplete gamma function. Thus we set g(x) = CDFC-1  CDFD2 (x2), with g-1(x) = CDF-2D1  CDFC (x).

(a) Uniform dist.

(b) Cauchy dist.

(c) Normal dist.

(d) Normal dist.

Figure 4: Comparison of the Cauchy-linear (a, b, and c) and the Spherical Cauchy-linear (d) interpolations on a 2D plane for data pairs sampled from different distributions. The Cauchy-linear interpolation in conjunction with the Cauchy distribution naturally results in segments.

Figure 4 shows comparison of the Cauchy-linear and the spherical Cauchy-linear interpolations on a two-dimensional plane for pairs of vectors sampled from different probability distributions. It illustrates how these interpolations manage to keep the distributions unchanged. Figure 5 is a comparison between all the aforementioned interpolations in conjunction with the normal distribution for a set pair of endpoints. We also compare the data samples generated by the DCGAN model trained on the CelebA dataset; the results are shown in figure 6.

7

Under review as a conference paper at ICLR 2019
Figure 5: Comparison of all considered interpolation methods for two endpoints sampled from a standard normal distribution on a 2D plane.
Figure 6: Images generated using a DCGAN model trained on the standard Normal distribution. Comparison of the five interpolations.
4 SUMMARY
We investigated the properties of multidimensional probability distributions in the context of generative models. We found out that there is a certain trade-off: it is impossible to define a latent probability distribution with a finite mean and the linear interpolation invariance property. The D-dimensional Cauchy distribution serves as an example of a latent probability distribution that remains unchanged by linear interpolation, at the cost of poor model performance, due to the heavytailed nature. Instead of using the Cauchy distribution as the latent distribution, we propose to use it to define nonlinear interpolations that have the distribution matching property. The assumption of the shortest path being a straight line must be relaxed, but our scheme is general enough to provide a way of incorporating other desirable properties. The theory of generative models states that one should use the same latent probability distribution during both training and inference. However, in practice, we have observed that the DCGAN is capable of generating sensible images from seemingly out-of-distribution regions, e.g. the emergence of the median face mentioned in the introduction. In our opinion, this is a promising direction for future research.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Eirikur Agustsson, Alexander Sage, Radu Timofte, and Luc Van Gool. Optimal transport maps for distribution preserving operations on latent spaces of generative models. arXiv:1711.01970, 2017.
Georgios Arvanitidis, Lars Kai Hansen, and Søren Hauberg. Latent space oddity: on the curvature of deep generative models. arXiv:1710.11379, 2017.
Huszár Ferenc. Gaussian distributions are soap bubbles. http://www.inference.vc/ high-dimensional-gaussian-distributions-are-soap-bubble/, 2017. Accessed: 2018-09-12.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS'10). Society for Artificial Intelligence and Statistics, 2010.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Nikolaus Hansen, Fabian Gemperle, Anne Auger, and Petros Koumoutsakos. When do heavy-tail distributions help? In Parallel Problem Solving from Nature-PPSN IX, pp. 62­71. Springer, 2006.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015.
Yannic Kilcher, Aurelien Lucchi, and Thomas Hofmann. Semantic interpolation in implicit models. arXiv:1710.11381, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2013. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 3730­3738, 2015. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv:1301.3781, 2013. John Nolan. Stable Distributions - Models for Heavy Tailed Data. Birkhauser, Boston, 2018.
In progress, Chapter 1 online at http://fs2.american.edu/jpnolan/www/stable/ stable.html. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv:1511.06434, 2015. Ken Shoemake. Animating rotation with quaternion curves. In ACM SIGGRAPH computer graphics, volume 19, pp. 245­254. ACM, 1985. Kyle Siegrist. Random. http://www.randomservices.org/random/special/ Cauchy.html, 2017. Accessed: 2018-09-12. Tom White. Sampling generative networks: Notes on a few effective techniques. arXiv:1609.04468, 2016.
9

Under review as a conference paper at ICLR 2019

A EXPERIMENTAL SETUP
All experiments were conducted using a DCGAN model (Radford et al., 2015), in which the generator network consisted of a linear layer with 8192 neurons, followed by four convolution transposition layers, each using 5 × 5 filters and strides of 2, with number of filters in order of layers: 256, 128, 64, 3. Except for the output layer, where tanh activation function was used, all previous layers used ReLU. Discriminator's architecture mirrored the one from the generator, with a single exception of using leaky ReLU instead of vanilla ReLU function for all except the last layer. No batch normalisation was used in both networks. Adam optimiser with learning rate of 2e-4 and momentum set to 0.5 was used. Batch size 64 was used throughout all experiments. If not explicitly stated otherwise, latent space dimension was set to 100. For the CelebA dataset we resized the input images to 64 × 64.

B PROOFS

Observation 2.1. Z N Dµ,

Let us assume that Z2

2 4µ

as D  . If µ =

has finite mean µ and finite variance 0, then Z = 0 almost everywhere.

2.

If

µ

>

0,

then

Proof. Recall that Z, Z1, . . . , ZD are independent and identically distributed. Therefore

Z2, Z

Z12, 2=

.Z. 1.2,+ZD2. .

are also . + ZD2 .

independent

and

identically

distributed.

Z = (Z1, . . . , ZD) and

Z2  0, therefore µ  0. If µ = 0, then Z2 = 0 almost everywhere, Z2 = 0 almost everywhere,

Z = 0 almost everywhere, and finally Z = 0 almost everywhere. From now on we will assume

that µ > 0.

 Using the central limit theorem we know that D

Z12 +...+ZD2 D

-µ

converges in distribution to

N (0, 2) with D  . The convergence of cumulative distribution functions is uniform, because

the limit is continuous everywhere

  >0D>0D>D,DNxR : P r D

Z12 + . . . + ZD2 - µ D

x

- CDFN (0,2)(x) < .

D > 0, thus

 Pr D

Z12 + . . . + ZD2 - µ D

x

 = P r Z12 + . . . + ZD2  Dµ + x D


= CDF Z 2 Dµ + x D .

Additionally,

 CDFN (0,2)(x) = CDFN (Dµ,D2) Dµ + x D ,

and now we have 
 >0D>0D>D,DNxR : CDF Z 2 Dµ + x D - CDFN (Dµ,D2) Dµ + x D

<.

Finally, the function



R x  Dµ + x D  R



is a bijection (again, because D > 0), so we may substitute Dµ + x D with x and the innermost

statement will hold for every x  R

 >0D>0D>D,DNxR : CDF Z 2 (x) - CDFN (Dµ,D2)(x) < .

(1)

Before taking square root of the normal distribution we must deal with negative values. Let N+(,  ) be defined by its cumulative distribution function:
0 if x < 0 , CDFN+(,)(x) = CDFN (,)(x) if x  0 .
The idea is to take all negative values of N (,  ) and concentrate them at zero.

10

Under review as a conference paper at ICLR 2019

Now we can modify (1)

 >0D>0D>D,DNxR : CDF Z 2 (x) - CDFN+(Dµ,D2)(x) < ,

(2)

for x  0 we simply use (1), for x < 0 the inequality simplifies to |0 - 0| < .

Since Z 2 and N+(Dµ, D2) are non-negative, we are allowed to take the square root of these random variables. The square root is a strictly increasing function, thus for x  0 we have

CDFN+(Dµ,D2)(x2) = CDFN+(Dµ,D2)(x) and CDF Z 2 (x2) = CDF Z (x) ,

therefore we can approximate the variable Z  >0D>0D>D,DNxR : CDF Z (x) - CDFN+(Dµ,D2)(x) < ,

(3)

for x  0 we substitute x2 for x in (2), for x < 0 the inequality simplifies, again, to |0 - 0| < .

This paragraph is a summary of the second part of the proof. To calculate N+(Dµ, D2) we

observe that, with constant

in(2formDaµll)y-, 1i.n

proximity of Additionally,

Dµ the N (Dµ,

square root behaves approximately like scaling D2) has width proportional to D, which is

infinitesimally smaller than Dµ, so we expect the result to be

N+(Dµ, D2) N

2 Dµ, .
4µ

Let us define

b=

CDFN-(10,2/(4µ))(1 - ) 0

if if



(0,

1 2

)

,



1 2

.

Here b

is defined so that the probability of x drawn from N

 Dµ,

2 4µ

being at least b

far from

the mean is equal to 2 . Also, note that b does not depend on D. For now we will assume that Dµ - b > 0 ­ this is always true for sufficiently large D, as µ > 0

 >0D>0D>D,DN : Dµ - b > 0 .

(4)

Now let us assume that we have a fixed > 0. For x  [-b , b ] we write the following inequalities

Dµ + 2x Dµ  Dµ + x 2  Dµ + 2x Dµ + b2 ,

which are equivalent to 0  x2  b2, thus true.

Every cumulative distribution function is weakly increasing, therefore

CDFN (Dµ,D2) Dµ + 2x Dµ  CDFN (Dµ,D2)

Dµ + x 2 

 CDFN (Dµ,D2) Dµ + 2x Dµ + b2 .

Because we assumed that

 Dµ + x

2

>

0

for

x



[-b

,b

],

we

can

replace

N (Dµ, D2)

with

N+(Dµ, D2)

CDFN (Dµ,D2) Dµ + 2x Dµ  CDFN+(Dµ,D2)

Dµ + x 2 

 CDFN (Dµ,D2) Dµ + 2x Dµ + b2 .

We transform the outer distributions using basic properties of the normal distribution. We also take square root of the middle distribution and obtain

C

DFN

 ( Dµ,2

/(4µ))

Dµ + x  CDFN+(Dµ,D2) Dµ + x 



C

DFN

 ( Dµ,2

/(4µ))

Dµ + x + b2/ 2 Dµ . (5)

11

Under review as a conference paper at ICLR 2019

b2/(2Dµ)



0

with

D





and

C

DFN

 ( Dµ,2

/(4µ))

is

continuous,

thus

we

have

uniform

convergence

 >0D>0D>D,DNxR :

CDF 
N Dµ,2/(4µ)

Dµ + x

-

C

DFN

 ( Dµ,2/(4µ))

Dµ + x + b2/ 2 Dµ

<.

Using (5) we get

 >0D>0D>D,DNx[-b ,b ] : Dµ - b > 0 =

CDF 
N Dµ,2/(4µ)

Dµ + x - CDFN+(Dµ,D2)

Dµ + x < .

(6)

Now we will extend this result to all x  R. For > 0 we have

CDF 
N Dµ,2/(4µ)

Dµ - b  ,

CDF 
N Dµ,2/(4µ)

Dµ + b  1 - .

(7) (8)

Substituting -b and b for x in (6), and using (7) and (8) respectively, we obtain

 >0D>0D>D,DN : CDFN+(Dµ,D2)  >0D>0D>D,DN : CDFN+(Dµ,D2)

Dµ - b < 2 , Dµ + b > 1 - 2 .

(9) (10)

Cumulative distribution functions are increasing functions with values in [0, 1], thus combining (7) and (9)

 >0x<-b

: 0  CDF
N

 Dµ,2 /(4µ)

Dµ + x  ,

 >0D>0D>D,DNx<-b : 0  CDFN+(Dµ,D2) Dµ + x < 2 ,

 >0D>0D>D,DNx<-b :

CDF 
N Dµ,2/(4µ)

Dµ + x - CDFN+(Dµ,D2) Dµ + x

<2 .

(11)

Analogically, using (8) and (10)

 >0x>b

:

1



C

DFN

 ( Dµ,2

/(4µ))

(

Dµ + x)  1 - ,

 >0D>0D>D,DNx>b : 1  CDFN+(Dµ,D2)( Dµ + x) > 1 - 2 ,

 >0D>0D>D,DNx>b :

CDF 
N Dµ,2/(4µ)

Dµ + x - CDFN+(Dµ,D2) Dµ + x < 2 .

(12)

Thus,

 >0D>0D>D,DNxR : CDF 
N Dµ,2/(4µ)

Dµ - b > 0 = Dµ + x - CDFN+(Dµ,D2)

Dµ + x < 2 ,

(13)

because for any > 0 we may define D := max{D1, D2, D3}, where D1, D2, D3 are taken from (6), (11) and (12).

To simplify,

 >0D>0D>D,DNxR :

CDF
N

 Dµ,2 /(4µ)

(x) - CDFN+(Dµ,D2)(x)

<2

,

(14)

12

Under review as a conference paper at ICLR 2019

because for any > 0 we may define D := max{D1, D2}, where D1, D2 are taken from (4) and (13), making the antecedent true. We also replaced Dµ + x with x, since now the statement holds for all x  R.
Finally, we combine (3) and (14) using the triangle inequality

 >0D>0D>D,DNxR :

CDF Z

(x) - CDF
N

 Dµ,2 /(4µ)

(x)

<3 ,

(15)

because for any > 0 we may define D := max{D1, D2}, where D1, D2 are taken from (3) and (14), and since it is true for any positive , we replace 3 with

 >0D>0D>D,DNxR :

CDF

Z

(x) - CDF
N

 (x)
Dµ,2 /(4µ)

<

,

because for any > 0 we may define D := D1, where D1 is taken from (15), substituting 3 for .

Observation 2.2. If Z has a finite mean, and Z is distributed identically to Z, then Z must be concentrated at a single point.

Proof. Let Z, Z(1), Z(2), Z(3), . . . be an infinite sequence of independent and identically distributed

random variables.

Using induction on n we can show that

1 2n

Z(1) + . . . + Z(2n)

is distributed

identically to Z. Indeed, for n = 1 this is one of the theorem's assumptions. To prove the inductive

step let us define

1 A := 2n

Z(1) + . . . + Z(2n)

,

1 B := 2n

Z(2n+1) + . . . + Z(2n+1)

.

A and B are independent ­ they are defined as functions of independent variables ­ and, by the

inductive hypothesis, distributed identically to Z. Finally, it is sufficient to observe that

1 2n+1

Z(1) + . . . + Z(2n+1)

A+B =.
2

Z has finite mean ­ let us denote it by µ. Let also N+ be the set of strictly positive natural numbers.

By

the

law

of

large

numbers

the

sequence

{

1 n

(Z(1)

+

...

+

Z(n))}nN+

converges

in

probability

to

µ.

The

same

is

true

for

any

infinite

subsequence,

in

particular

for

{

1 2n

(Z(1)

+

.

.

.

+

Z(2n))}nN+ ,

but we have shown that all elements of this subsequence are distributed identically to Z, thus Z must

be concentrated at µ.

Observation 3.1. If Z has uniform distribution on the zero-centred sphere of radius R > 0, then f SL does not change the distribution of Z.

Proof. Let Z, Z(1), Z(2) be independent and identically distributed. Let   [0, 1] be a fixed real

number.

The

random

variable

f SL
Z(1)

,Z(2)

()

is

defined

almost

everywhere

(with

the

exception

of

parallel samples from Z(1), Z(2)) and is also concentrated on the zero-centred sphere of radius R

(because if x1 = x2 , then fxS1L,x2 () = x1 = x2 ).

Let iso be any linear isometry of the latent space. iso(x) = x , thus iso is also an isometry of the zero-centred sphere of radius R. Additionally, we have

iso fxS1L,x2 ()

= iso

sin [(1 - )] sin[] sin  x1 + sin  x2

sin [(1 - )]

sin[]

= sin  iso(x1) + sin  iso(x2)

= fiSsLo(x1),iso(x2)()

and the last equality holds because the isometry does not change the angle  between x1 and x2.

13

Under review as a conference paper at ICLR 2019

Thus, iso

f SL
Z(1)

,Z(2)

()

=

fiSsLo(Z(1) ),iso(Z(2) ) (),

and

this

is

distributed

identically

to

f SL
Z(1)

,Z(2)

(),

because Z(1), Z(2), both uniform distributions, are invariant to iso.

In

that

case,

f SL
Z(1) ,Z(2)

()

is

concentrated

on

the

zero-centred

sphere

of

radius

R

and

invariant

to

all

linear isometries of the latent space. The only distribution having these properties is the uniform

distribution on the sphere.

Observation 3.2. If Z  N (0, I), then f N does not change the distribution of Z.

Proof. Let Z, Z(1), Z(2) be independent and identically distributed. Let   [0, 1] be a fixed real
number. The random variables Z(1) and Z(2) are both distributed according to N (0, I). Using the definition of f N and elementary properties of the normal distribution we conclude

fN
Z(1)

,Z(2)

()

=

(1 - )Z(1) + Z(2) (1 - )2 + 2

 N(

(1 - )0 + 0 (1 - )2 + 2

,

(1 - )2I (1 - )2

+ +

2I 2

)

=

N

(0,

I).

Observation 3.3. With the above assumptions about g the Cauchy-linear interpolation does not change the distribution of Z.

Proof. Let Z, Z(1), Z(2) be independent and identically distributed. Let   [0, 1] be a fixed real number. First observe that g-1(Z(1)) and g-1(Z(2)) are independent (because Z(1), Z(2) are independent) and distributed identically to C (property of g). Likewise, (1 - )g-1(Z(1)) + g-1(Z(2))  C (property of the Cauchy distribution). Therefore, g((1 - )g-1(Z(1)) + g-1(Z(1)))  Z (property of g).
Observation 3.4. With the above assumptions about g, the spherical Cauchy-linear interpolation does not change the distribution of Z if the Z distribution is isotropic.

Proof. Let Z, Z(1), Z(2) be independent and identically distributed. Let   [0, 1] be a fixed real number. The following statements are straightforward consequences of Z(1), Z(2) being isotropic (and also independent).

1. The random variables

Z(1) Z(1)

,

Z(2) Z(2)

,

Z(1) ,

Z(2)

are independent,

2. Z(1) and Z(2) are both distributed identically to Z ,

Z(1)

Z(2)

3. Z(1) and Z(2) are both distributed uniformly on the sphere of radius 1.

The next two statements are consequences of Observations 3.1 and 3.3 respectively.

4. The random variable

f SCL
Z(1) ,Z(2)

()

f SCL
Z(1) ,Z(2)

()

= sin [(1 - )] sin 

tributed uniformly on the unit sphere.

Z(1) Z(1)

sin[] Z(2)

+ sin 

Z(2)

is dis-

5. The random variable

f SCL
Z(1) ,Z(2)

()

= g (1 - )g-1( Z(1) ) + g-1( Z(2) )

is dis-

tributed identically to Z .

f SCL
Z(1) ,Z(2)

()

f SCL
Z(1) ,Z(2)

()

and

f SCL
Z(1) ,Z(2)

()

random variables ( is a function of

are independent, because they are functions of independent

Z(1) Z(1)

and

Z(2) Z(2)

),

therefore

f SCL
Z(1) ,Z(2)

()

is

isotropic.

Using

14

Under review as a conference paper at ICLR 2019

the statement 5. and the fact that two isotropic probability distributions are equal if and only if

the

distributions

of

their

euclidean

norms

are

equal

we

conclude

that

f SCL
Z(1) ,Z(2)

()

is

distributed

identically to Z.

C THE CAUCHY DISTRIBUTION ­ SAMPLES AND INTERPOLATIONS

Figure 7: Emergence of sensible samples decoded near the origin of the latent space throughout the training process. Demonstrated using interpolations between opposite vectors sampled from the latent space.
Figure 8: Linear interpolations between hand-picked points from tails of the Cauchy distribution. 15

Under review as a conference paper at ICLR 2019
Figure 9: Generated images from samples from the Cauchy distribution, with occasional "failed" images from tails of the distribution.
Figure 10: Generated images from samples from the Cauchy distribution, with varying dimension of the latent space.
16

Under review as a conference paper at ICLR 2019
Figure 11: Linear interpolations between random latent vectors. The model was trained using the Cauchy distribution.
Figure 12: Linear interpolations between opposite latent vectors. The model was trained using the Cauchy distribution.
17

Under review as a conference paper at ICLR 2019
D MORE CAUCHY-LINEAR AND SPHERICAL CAUCHY-LINEAR
INTERPOLATIONS
Figure 13: Cauchy-linear interpolations between opposite latent vectors. The model was trained using the normal distribution.
Figure 14: Cauchy-linear interpolations between random latent vectors. The model was trained using the normal distribution.
18

Under review as a conference paper at ICLR 2019
Figure 15: Spherical Cauchy-linear interpolations between random latent vectors. The model was trained using the normal distribution.
19

