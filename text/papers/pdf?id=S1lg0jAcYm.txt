Under review as a conference paper at ICLR 2019

ARM: AUGMENT-REINFORCE-MERGE GRADIENT FOR STOCHASTIC BINARY NETWORKS
Anonymous authors Paper under double-blind review

ABSTRACT
To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased and has low variance. Exploiting data augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to antithetic sampling in an augmented space. Experimental results show the ARM estimator provides state-ofthe-art performance in auto-encoding variational Bayes and maximum likelihood inference, for discrete latent variable models with one or multiple stochastic binary layers. Python code is available at https://github.com/ABC-anonymous-1.

1 INTRODUCTION

Given a function f (z) of a random variable z = (z1, . . . , zV )T , which follows a distribution q(z) parameterized by , there has been significant recent interest in estimating  to maximize (or minimize) the expectation of f (z) with respect to z  q(z), expressed as

E() = f (z)q(z)dz = Ezq(z)[f (z)].

(1)

In particular, maximizing the marginal likelihood of a hierarchal Bayesian model (Bishop, 1995)
and maximizing the evidence lower bound (ELBO) for variational inference (Jordan et al., 1999;
Blei et al., 2017), two fundamental problems in statistical inference, both boil down to maximizing an expectation as in (1). To maximize (1), if zf (z) is tractable to compute and z  q(z) can be generated via reparameterization as z = T( ),  p( ), where are random noises and T(·) denotes a deterministic transform parameterized by , then one may apply the reparameterization
trick (Kingma & Welling, 2013; Rezende et al., 2014) to compute the gradient as

E() = E p( )[f (T( ))] = E p( )[f (T( ))].

(2)

Unfortunately, this trick is often not applicable to discrete random variables, which are widely used to construct discrete latent variable models such as the sigmoid belief net (Neal, 1992; Saul et al., 1996).

To maximize (1) for discrete z, using the score function  log q(z) = q(z)/q(z), one may compute E() via REINFORCE (Williams, 1992) as

1 E() = Ezq(z)[f (z) log q(z)]  K

K k=1

f

(z(k))

log

q(z(k)),

where z(k) iid q(z) are independent, and identically distributed (iid). This unbiased estimator is also known as (a.k.a.) the score-function (Fu, 2006) or likelihood-ratio estimator (Glynn, 1990). While it is unbiased and only requires drawing iid random samples from q(z) and computing  log q(z(k)), its high Monte-Carlo-integration variance often limits its use in practice. Note that if f (z) depends on , then we assume it is always true that Ezq(z)[f (z)] = 0. For example, in variational inference, we need to maximize the ELBO as Ezq(z)[f (z)], where f (z) = log[p(x | z)p(z)/q(z)]. In this case, although f (z) depends on , since Ezq(z)[ log q(z)] =
q(z)dz =  q(z)dz = 0, we have Ezq(z)[f (z)] = 0.
To address the high-variance issue, one may introduce appropriate control variates (a.k.a. baselines) to reduce the variance of REINFORCE (Paisley et al., 2012; Ranganath et al., 2014; Mnih & Gregor,

1

Under review as a conference paper at ICLR 2019

2014; Gu et al., 2016; Mnih & Rezende, 2016; Ruiz et al., 2016; Kucukelbir et al., 2017; Naesseth et al., 2017). Alternatively, one may first relax the discrete random variables with continuous ones and then apply the reparameterization trick to estimate the gradients, which reduces the variance of Monte Carlo integration at the expense of introducing bias (Maddison et al., 2017; Jang et al., 2017). Combining both REINFORCE and the continuous relaxation of discrete random variables, Tucker et al. (2017) and Grathwohl et al. (2018) both aim to produce a low-variance and unbiased gradient estimator by introducing a continuous relaxation based control variate, whose parameter, however, needs to be estimated at each mini-batch by minimizing the sample variance of the estimator with stochastic gradient descent (SGD), increasing not only the computational complexity, but also the risk of overfitting the training data. Another interesting variance-control idea applicable to discrete latent variables is using local expectation gradients, which estimates the gradients based on REINFORCE, by performing Monte Carlo integration using a single global sample together with exact integration of the local variable for each latent dimension (Titsias & La´zaro-Gredilla, 2015).
Distinct from the usual idea of introducing control variates to reduce the estimation variance of REINFORCE, we propose the augment-REINFORCE-merge (ARM) estimator, a novel unbiased and low-variance gradient estimator for binary latent variables. We show by rewriting the expectation with respect to Bernoulli random variables as one with respect to augmented exponential random variables and then expressing the gradient as an expectation via REINFORCE, with the assistance of appropriate reparameterization, one can derive the ARM estimator in the augmented space by using either the strategy of sharing common random numbers between two expectations or the strategy of applying antithetic sampling. Both strategies, as detailedly discussed in Owen (2013), can both be used to explain why the ARM estimator is unbiased and could lead to significant variance reduction.
Our experimental results on both auto-encoding variational Bayes and maximum likelihood inference for discrete latent variable models, with one or multiple discrete stochastic layers, show that the ARM estimator converges fast, has low computational complexity, and provides state-of-the-art out-of-sample prediction performance, suggesting the effectiveness of using the ARM estimator for gradient backpropagation through stochastic binary layers.

2 ARM: AUGMENT-REINFORCE-MERGE ESTIMATOR

In this section, we first present the key theorem of the paper, and then provide its derivation. With this theorem, we summarize ARM gradient ascent for multivariate binary latent variables in Algorithm 1, as shown in the Appendix. Let us denote () = e/(1 + e) as the sigmoid function and 1[·] as an indicator function that equals to one if the argument is true and zero otherwise.
Theorem 1 (ARM ). For a vector of V binary random variables z = (z1, . . . , zV )T , the gradient of

E() = Ez

V v=1

Bernoulli(zv

;(v

))

[f

(z

)]

(3)

with respect to  = (1, . . . , V )T , the logits of the Bernoulli probability parameters, can be expressed as

E() = Eu

V v=1

Uniform(uv

;0,1)

f (1[u>(-)]) - f (1[u<()]) (u - 1/2) ,

where 1[u>(-)] := 1[u1>(-1)], . . . , 1[uV >(-V )] T .

(4)

For simplicity, we will first present the ARM estimator for a univariate binary latent variable (i.e.,
V = 1), and then generalize it to a multivariate one (i.e., V > 1). In the univariate case, we need to evaluate the gradient of E() = EzBernoulli(())[f (z)] with respect to . Let us denote t  Exp() as an exponential distribution, whose probability density function is defined as p(t | ) = e-t, where  > 0 and t > 0. The mean and variance are E[t] = -1 and var[t] = -2, respectively. The exponential random variable t  Exp() can be reparameterized as t = /,  Exp(1). It is well known, e.g., in Ross (2006), that if t1  Exp(1) and t2  Exp(2) are two independent exponential random variables, then the probability that t1 is smaller than t2 can be expressed as P (t1 < t2) = 1/(1 + 2); moreover, since t1 =d 1/1 and t2 =d 2/2, where 1, 2 iid Exp(1)
and the symbol "=d " denotes "equal in distribution," we have

P (t1 < t2) = P ( 1/1 < 2/2) = P ( 1 < 21/2) = 1/(1 + 2).

(5)

2

Under review as a conference paper at ICLR 2019

2.1 AUGMENTATION OF A BERNOULLI RANDOM VARIABLE AND REPARAMETERIZATION

From (5) it becomes clear that the Bernoulli random variable z  Bernoulli(()) can be reparameterized by racing two augmented exponential random variables as

z = 1[ 1< 2e], 1  Exp(1), 2  Exp(1).

(6)

Consequently, the expectation with respect to the Bernoulli random variable can be reparameterized

as one with respect to two augmented exponential random variables as

E () = EzBernoulli(())[f (z)] = E 1, 2iidExp(1)[f (1[ 1e-< 2])].

(7)

2.2 REINFORCE ESTIMATOR IN THE AUGMENTED SPACE

Since the indicator function 1[ 1e-< 2] is not differentiable, the reparameterization trick in (2) is not directly applicable to computing the gradient of (7). Fortunately, as t1 = 1e-, 1  Exp(1) is equal in distribution to t1  Exp(e), the expectation in (7) can be further reparameterized as

E () = E 1, 2iidExp(1)[f (1[ 1e-< 2])] = Et1Exp(e), 2Exp(1)[f (1[t1< 2])],

(8)

and hence, via REINFORCE and then another reparameterization, we can express the gradient as

E () = Et1Exp(e), 2Exp(1)[f (1[t1< 2]) log Exp(t1; e)] = Et1Exp(e), 2Exp(1)[f (1[t1< 2])(1 - t1e)] = E 1, 2iidExp(1)[f (1[ 1e-< 2])(1 - 1)].

(9)

Similarly, we have E () = E 1, 2iidExp(1)[f (1[ 1< 2e])] = E 1Exp(1), t2Exp(-e)[f (1[ 1<t2])], and hence can also express the gradient as

E () = E 1Exp(1), t2Exp(e-)[f (1[ 1<t2]) log Exp(t2; e-)] = -E 1Exp(1), t2Exp(e-)[f (1[ 1<t2])(1 - t2e-)] = -E 1, 2iidExp(1)[f (1[ 1e-< 2])(1 - 2)].

(10)

2.3 MERGE OF REINFORCE GRADIENTS

A key observation of the paper is that by swapping the indices of the two iid standard exponential random variables in (10) , the gradient E() can be equivalently expressed as

E () = -E 1, 2iidExp(1)[f (1[ 2e-< 1])(1 - 1)].

(11)

As the term inside the expectation in (9) and that in (11) could be highly positively correlated, we are motivated to merge (9) and (11) by sharing the same set of standard exponential random variables for Monte Carlo integration, which provides a new opportunity to well control the estimation variance (Owen, 2013). More specifically, simply taking the average of (9) and (11) leads to

E () = E 1, 2iidExp(1) f (1[ 1e-< 2]) - f (1[ 2e-< 1]) (1/2 - 1/2) .

(12)

Note one may also take a weighted average of (9) and (11), and optimize the combination weight to potentially further reduce the variance of the estimator. We leave that for future study.

Note that letting 1, 2 iid Exp(1) is the same in distribution as letting 1 = u, 2 = (1 - u), where u  Uniform(0, 1),  Gamma(2, 1),

(13)

which can be proved using Exp(1) =d Gamma(1, 1), (u, 1 - u)T =d Dirichlet (12), where u  Uniform(0, 1)}, and Lemma IV.3 of Zhou & Carin (2012). Thus, (12) can be reparameterized as

E () = EuUniform(0,1), Gamma(2,1) f (1[u>(-)]) - f (1[u<()]) ( u/2 - 1/2) ,

Applying Rao Blackwellization (Casella & Robert, 1996), we can further express the gradient as

E () = EuUniform(0,1) f (1[u>(-)]) - f (1[u<()]) (u - 1/2) ,

(14)

which will be referred to as the Augment-REINFORCE-merge (ARM) estimator.

3

Under review as a conference paper at ICLR 2019

2.4 RELATIONSHIP TO ANTITHETIC SAMPLING

While we use augmentation, REINFORCE, and merge steps to obtain (12) and another reparameterization and marginalizing step to obtain the ARM gradient in (14), we find it can also be obtained by performing augmentation, REINFORCE, reparameterization, and antithetic sampling steps. More specifically, using the equivalence between 1, 2 iid Exp(1) and (13), we can reparameterize (9) as

E () = EuUniform(0,1), Gamma(2,1)[f (1[u<()])(1 - u)]

= EuUniform(0,1)[f (1[u<()])(1 - 2u)]. Further using antithetic sampling (Owen, 2013) with u~ = 1 - u, we have

(15)

E () = EuUniform(0,1)[f (1[u<()])(1/2 - u)] + Eu~Uniform(0,1)[f (1[u~<()])(1/2 - u~)] (16)

= EuUniform(0,1)[f (1[u<()])(1/2 - u) + f (1[u~<()])(1/2 - u~)]

(17)

which becomes the same as the ARM estimator in (14).

2.5 MULTIVARIATE GENERALIZATION

The ARM estimator for univariate binary, however, is of little use in practice, as one may first
analytically solve the expectation and then compute its gradient. Below we show how to generalize the univariate ARM estimator to a multivariate one. Let us denote (·)\v as a vector whose vth element is removed. For the expectation in (3), applying the univariate ARM estimator in (14), we have

v E () = Ez\v =v Bernoulli(z ;( )){v EzvBernoulli((v)[f (z)]}

= Ez\v =v Bernoulli(z ;( )) EuvUniform(0,1) (uv - 1/2)

× f (z\v, zv = 1[uv>(-v)]) - f (z\v, zv = 1[uv<(v)]) .

(18)

Since z\v  =v Bernoulli(z ; ( )) can be equivalently generated as z\v = 1[u\v<(\v)] or as z\v = 1[u\v>(-\v)], where u\v  =v Uniform(u ; 0, 1), exchanging the order of the two expectations in (18) and applying reparameterization, we have

v E () = EuvUniform(0,1) (uv - 1/2) Ez\v =v Bernoulli(z ;( ))

f (z\v, zv = 1[uv>(-v)]) - f (z\v, zv = 1[uv<(v)])

= Eu

V v=1

Uniform(uv ;0,1)

(uv - 1/2) f (1[u>(-)]) - f (1[u<()])

,

which concludes the proof for (4) shown in Theorem 1.

(19)

Alternatively, instead of generalizing the univariate ARM gradient as in (18) and (19), we can first do a multivariate generalization of the univariate Augment-REINFORCE gradient in (15) as

v E () = Ez\v =v Bernoulli(z ;( )){v EzvBernoulli((v)[f (z)]}

= Ez\v =v Bernoulli(z ;( )) EuvUniform(0,1) (1 - 2uv )f (z\v , zv = 1[uv<(v)])

= Eu

V v=1

Uniform(uv

;0,1)

(1 - 2uv)f (1[u<()])

,

and then add an antithetic sampling step to arrive at (4).

(20)

2.6 EFFECTIVENESS OF ARM IN VARIANCE REDUCTION

Let us denote gv(u) = f (1[u<()])(1 - 2uv) and u~ = 1 - u. With u(k) iid

V v=1

Uniform(0,

1),

we define the ARM estimate of v E() with K Monte Carlo samples, denoted as gARM,v, and the

augment-REINFORCE (AR) estimate with 2K Monte Carlo samples, denoted as gAR,v, using

gARM,v

=

1 2K

K k=1

(gv

(u(k)

)

+

gv(u~ (k))),

gAR,v

=

1 2K

2K k=1

gv

(u(k)

).

Similar to the analysis in Owen (2013), the amount of variance reduction brought by the ARM

estimator can be reflected by the ratio of the variance of gARM,v to that of gAR,v as

var[gARM,v ] var[gAR,v ]

=

var[gv (u)]

- cov(-gv(u), gv(u~)) var[gv (u)]

=

1-

v ,

v = Corr(-gv(u), gv(1 - u)).

Note -gv(u) = f (1[u<()])(2uv -1), gv(1-u) = f (1[u>(-)])(2uv -1), and P (1[uv<(v)] =
1[uv>(-v)]) = (|v|) - (-|v|), thus a strong positive correlation (i.e., v  1) and hence noticeable variance reduction is likely especially if v moves far away from zero during training.

4

Under review as a conference paper at ICLR 2019

3 BACKPROPAGATION THROUGH DISCRETE STOCHASTIC LAYERS

A latent variable model with multiple stochastic hidden layers can be constructed as

x  p0 (x | b1), b1  p1 (b1 | b2), . . . , bt  pt (bt | bt+1), . . . , bT  pT (bT ), (21)

whose joint likelihood given the distribution parameters 0:T = {0, . . . , T } is expressed as

p(x, b1:T | 0:T ) = p0 (x | b1)

T -1
t=1 pt (bt | bt+1) pT (bT ).

(22)

In comparison to deterministic feedforward neural networks, stochastic ones can represent complex

distributions and show natural resistance to overfitting (Neal, 1992; Saul et al., 1996; Tang &

Salakhutdinov, 2013; Raiko et al., 2014; Gu et al., 2016; Tang & Salakhutdinov, 2013). However, the

training, especially if there are stochastic discrete layers, is often much more challenging. Below we

show for both auto-encoding variational Bayes and maximum likelihood inference, how to apply the

ARM estimator for gradient backpropagation in stochastic binary networks.

3.1 ARM VARIATIONAL AUTO-ENCODER

For auto-encoding variational Bayes inference (Kingma & Welling, 2013; Rezende et al., 2014), we

construct a variational distribution as

qw1:T (b1:T | x) = qw1 (b1 | x)

T -1
t=1 qwt+1 (bt+1 | bt) ,

(23)

with which the ELBO can be expressed as

E (w1:T ) = Eb1:T qw1:T (b1:T | x) [f (b1:T )] , where f (b1:T ) = log p0 (x | b1) + log p1:T (b1:T ) - log qw1:T (b1:T | x).

(24)

Proposition 2 (ARM backpropagation). For a stochastic binary network with T binary stochastic

hidden layers, constructing a variational auto-encoder (VAE) defined with b0 = x and

qwt (bt | bt-1) = Bernoulli(bt; (Twt (bt-1))) for t = 1, . . . , T , the gradient of the ELBO with respect to wt can be expressed as

(25)

wt E (w1:T ) = Eq(b1:t-1) [EutUniform(0,1)[f(ut, Twt (bt-1), b1:t-1)(ut - 1/2)]wt Twt (bt-1)] ,

where

f(ut,

Twt (bt-1),

b1:t-1)

=

Ebt+1:T q(bt+1:T

|

bt ),

[f (bbt=1[ut>(-Twt (bt-1))]

1:T

)]

- E [f (b )]bt+1:T q(bt+1:T | bt), bt=1[ut<(Twt (bt-1))]

1:T

(26)

The gradient presented in (26) can be estimated with a single Monte Carlo sample as

f^(ut, Twt (bt-1), b1:t-1) =

0, f (b1:t-1, b(t:1T) ) - f (b1:t-1, b(t:2T) ),

if bt(1) = bt(2) , otherwise

(27)

where b(t1) = 1[ut>(-Twt (bt-1))], b(t+1)1:T  q(bt+1:T | b(t1)), bt(2) = 1[ut<(Twt (bt-1))], and bt(+2)1:T  q(bt+1:T | bt(2)). The proof of Proposition 2 is provided in the Appendix.

3.2 ARM MAXIMUM LIKELIHOOD INFERENCE

For maximum likelihood inference, the log marginal likelihood can be expressed as

log p0:T (x) = log Eb1:T p1:T (b1:T )[p0 (x | b1)]  E (1:T ) = Eb1:T p1:T (b1:T )[log p0 (x | b1)].
Generalizing Proposition 2 leads to the following proposition.

(28)

Proposition 3. For a stochastic binary network defined as

pt (bt | bt+1) = Bernoulli(bt; (Tt (bt+1))),

(29)

the gradient of the lower bound in (28) with respect to t can be expressed as

t E (1:T ) = Ep(bt+1:T ) [EutUniform(0,1)[f(ut, Tt (bt+1), bt+1:T )(ut - 1/2)]t Tt (bt+1)] ,

where f(ut, Tt (bt+1), bt+1:T ) = Eb1:t-1p(b1:t-1 | bt), [logbt=1[ut>(-Tt (bt+1))]) p0 (x | b1)]

- Eb1:t-1p(b1:t-1 | bt), [logbt=1[ut<(Tt (bt+1))]) p0 (x | b1)].

5

Under review as a conference paper at ICLR 2019

Gradient

True_grad
0.004 0.002 0.000 0.002 0.004
True_prob
1.0 0.8 0.6 0.4

REINFORCE_grad
0.2 0.1 0.0 0.1 0.2
REINFORCE_prob
1.0 0.8 0.6 0.4

0.0100 0.0075 0.0050 0.0025 0.0000 0.0025 0.0050 0.0075 0.0100
1.0
0.8
0.6
0.4

ARM_grad ARM_prob

0.49 0.2496

0.499

0.501 0.51

0.2498

0.2500

Loss

0.2502

0.2504

0.49 0.2506
0.499 0.501
0.51 0.2508

ARM True REINFORCE RELAX REBAR

Probability

0.2
0.0 0 500 1000 1500 2000 2500 3000
Iteration

0.2

0.0 0

500 1000 1500 2000 2500 3000
Iteration

0.2
0.0 0 500 1000 1500 2000 2500 3000
Iteration

0.2510 0 500 1000 1500 2000 2500 3000
Iteration

Figure 1: Left: Trace plots of the true/estimated gradients and estimated Bernoulli probability parameters for p0  {0.49, 0.499, 0.501, 0.51}; Right: Trace plots of the loss functions for p0 = 0.499.

4 EXPERIMENTAL RESULTS
To illustrate the working mechanism of the ARM estimator, related to Tucker et al. (2017) and Grathwohl et al. (2018), we consider learning  to maximize E() = EzBernoulli(())[(z - p0)2], where p0  {0.49, 0.499, 0.501, 0.51}, or equivalently, minimize the loss as -E(). The optimal solution is () = 1(p0 < 0.5). The closer p0 is to 0.5, the more challenging the optimization becomes. We compare the ARM estimator to the true gradient as g = (1 - 2p0)()(1 - ()) and three previously proposed unbiased estimators, including REINFORCE, REBAR (Tucker et al., 2017), and RELAX (Grathwohl et al., 2018). With a single random sample u  Uniform(0, 1) for Monte Carlo integration, the ARM gradient can be expressed as
g,ARM = (1[u>(-)] - p0)2 - (1[u<()] - p0)2 (u - 1/2),
while the REINFORCE gradient can be expressed as
g,REINFORCE = (1[u<()] - p0)2(1[u<()] - ()).
See Tucker et al. (2017) and Grathwohl et al. (2018) for the details about REBAR and RELAX, respectively, which both introduce stochastically estimated control variates to improve REINFORCE.
As shown in Figure 1 (a), the REINFORCE gradients have large variances. Consequently, a REINFORCE based gradient ascent algorithm may diverge. For example, when p0 = 0.501, the optimal value for the Bernoulli probability () is 0, but the algorithm infers it to be close to 1 at the end of 3000 iterations of a random trial. By contrast, the univariate ARM estimator well approximates the time-varying true gradients by adjusting the frequencies, amplitudes, and signs of its gradient estimates, with larger and more frequent spikes for larger true gradients. As shown in Figure 1 (b), using the ARM estimator is indistinguishable from using the true gradient for updating  to minimize the loss -EzBernoulli(())[(z - 0.499)2], significantly outperforming not only REINFORCE, which has a large variance, but also both REBAR and RELAX, which improve on REINFORCE by introducing carefully constructed control variates that are stochastically updated for variance reduction. We further plot in Figure 3 of the Appendix the gradient estimated with multiple Monte Carlo samples against the true gradient at each iteration, showing the ARM estimator has significant lower variance than REINFORCE given the same number of Monte Carlo samples.
4.1 DISCRETE VARIATIONAL AUTO-ENCODERS
To optimize a variational auto-encoder (VAE) for a discrete latent variable model, existing solutions often rely on biased but low-variance stochastic gradient estimators (Bengio et al., 2013; Jang et al., 2017), unbiased but high-variance ones (Mnih & Gregor, 2014), or unbiased REINFORCE combined with computationally expensive control variates, whose parameters are estimated by minimizing the sample variance of the estimator with SGD (Tucker et al., 2017; Grathwohl et al., 2018). Comparing to previously proposed methods for discrete latent variables, the ARM estimator exhibits low variance and is unbiased, computationally efficient, and simple to implement.
For discrete VAEs, we compare ARM with a variety of representative stochastic gradient estimators for discrete latent variables, including Wake-Sleep (Hinton et al., 1995), NVIL (Mnih & Gregor,

6

Under review as a conference paper at ICLR 2019

Table 1: The constructions of three differently structured discrete variational auto-encoders. The following symbols "", "]", )", and " " represent deterministic linear transform, leaky rectified linear units (LeakyReLU) (Maas et al., 2013) nonlinear activation, sigmoid nonlinear activation, and random sampling respectively, in the encoder (a.k.a. recognition network); their reversed versions are used in the decoder (a.k.a. generator).

Encoder Decoder

Nonlinear
784200]200]200) 200 784 (784[200[200200

Linear
784200) 200 784 (784200

Linear two layers
784200) 200200) 200 784 (784200 (200200

-ELBO

170 160 150 140 130 120 110 100
0

MNIST_static
Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM)
200000 400000 600000 800000 10000001200000
Steps

(a) Nonlinear

MNIST_static

Training(REBAR)

150 Validation(REBAR) Training(RELAX)

140

Validation(RELAX) Training(ARM)

Validation(ARM) 130

120

110

0 2500 5000 7500 1000012500150001750020000
Time(seconds)

(d) Nonlinear

-ELBO

-ELBO

MNIST_static

140 Training(REBAR)

Validation(REBAR)

135

Training(RELAX) Validation(RELAX)

Training(ARM) 130 Validation(ARM)

125

120

115 0

200000 400000 600000 800000 1000000
Steps

(b) Linear

155 150 145 140 135 130 125 120 115
0

MNIST_static
Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM)
2000 4000 6000 8000 10000 12000 14000 16000
Time(seconds)

(e) Linear

-ELBO

-ELBO

MNIST_static

150

Training(REBAR) Validation(REBAR)

Training(RELAX)

140 Validation(RELAX)

Training(ARM)

130 Validation(ARM)

120

110

0
150 140 130

200000 400000 600000 800000 1000000
Steps
(c) Linear two layers
MNIST_static
Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM)

120

110

0 5000 10000 15000 20000 25000 30000 35000
Time(seconds) (f) Linear two layers

-ELBO

Figure 2: Test negative ELBOs on MNIST-static with respect to training iterations, shown in the top row, and wall clock times on Tesla-K40 GPU, shown in the bottom row, for three differently structured Bernoulli VAEs.

2014), LeGrad (Titsias & La´zaro-Gredilla, 2015), MuProp (Gu et al., 2016), Concrete (GumbelSoftmax) (Jang et al., 2017; Maddison et al., 2017), REBAR (Grathwohl et al., 2018), and RELAX (Tucker et al., 2017). Following the settings in Tucker et al. (2017) and Grathwohl et al. (2018), for the encoder defined in (22) and decoder defined in (23), we consider three different network architectures, as summarized in Table 1, including "Nonlinear" that has one stochastic but two LeakyReLU (Maas et al., 2013) deterministic hidden layers, "Linear" that has one stochastic hidden layer, and "Linear two layers" that has two stochastic hidden layers. We consider a widely used binarization (Salakhutdinov & Murray, 2008; Larochelle & Murray, 2011), referred to as MNIST-static and available at http://www.dmi.usherb.ca/larocheh/mlpython/ modules/datasets/binarized mnist.html, making our numerical results directly comparable to those reported in the literature. In addition to MNIST-static, we also consider MNIST-threshold (van den Oord et al., 2017), which binarizes MNIST by thresholding each pixel value at 0.5, and the binarized OMNIGLOT dataset.
We train discrete VAEs with 200 conditionally iid Bernoulli random variables as the hidden units of each stochastic binary layer. We maximize a single-Monte-Carlo-sample ELBO using Adam (Kingma & Ba, 2014), with the learning rate selected from {5, 1, 0.5} × 10-4 by the validation set. We set the batch size as 50 for MNIST and 25 for OMNIGLOT. For each dataset, using its default training/validation/testing partition, we train all methods on the training set, calculate the validation log-likelihood for every epoch, and report the test negative log-likelihood when the validation negative log-likelihood reaches its minimum within a predefined maximum number of iterations.
We summarize the test negative log-likelihoods in Table 2 for MNIST-static. We also summarize the test negative ELBOs in Table 4 of the Appendix, and provide related trace plots of the training and validation negative ELBOs on MNIST-static in Figure 2, and these on MNIST-threshold and OMNIGLOT in Figures 5 and 6 of the Appendix, respectively. For these trace plots, for a fair comparison of convergence speed between different algorithms, we use publicly available code from
7

Under review as a conference paper at ICLR 2019

Table 2: Test negative log-likelihoods of discrete VAEs trained with a variety of stochastic gradient estimators on MNIST-static and OMNIGLOT, where , , ,  represent the results reported in Mnih & Gregor (2014), Tucker et al. (2017), Gu et al. (2016), and Grathwohl et al. (2018), respectively. The results for LeGrad (Titsias & La´zaro-Gredilla, 2015) are obtained by running the code provided by the authors. We report the results of ARM using the sample mean and standard deviation over five independent trials with random initializations.

(a) MNIST

Linear

Nonlinear

Algorithm

- log p(x)

Algorithm - log p(x)

REINFORCE Wake-Sleep
NVIL 
LeGrad MuProp
Concrete
REBAR RELAX
ARM

= 164.0 = 120.8 = 113.1  117.5
 113.0 = 107.3 = 107.7
 113.6 = 107.2 ± 0.1

REINFORCE Wake-Sleep
NVIL  LeGrad
MuProp Concrete REBAR RELAX
ARM

= 114.6 -
= 102.2 -
= 99.1 = 99.6 = 101.4  119.2 = 98.4 ± 0.3

(b) OMNIGLOT

Two layers

Algorithm

- log p(x)

REINFORCE Wake-Sleep
NVIL
LeGrad MuProp
Concrete
REBAR RELAX
ARM

= 159.2 = 107.7 = 99.8
 100.4 = 95.6 = 95.4  100.9 = 96.7 ± 0.3

Linear

Algorithm - log p(x)

NVIL MuProp Concrete REBAR RELAX
ARM

= 117.6 = 117.6 = 117.7 = 117.7
 122.1 = 115.8 ± 0.2

Nonlinear

Algorithm - log p(x)

NVIL MuProp Concrete REBAR RELAX
ARM

= 116.6 = 117.5 = 116.7 = 118.0
 128.2 = 117.6 ± 0.4

Two layers

Algorithm - log p(x)

NVIL MuProp Concrete REBAR RELAX
ARM

= 111.4 = 111.2 = 111.3 = 110.8
 115.4 = 109.8 ± 0.3

the authors and setting the learning rate of ARM the same as that selected by REBAR/RELAX in Grathwohl et al. (2018).
These results show that ARM provides state-of-the-art performance in delivering not only fast convergence, but also low negative log-likelihoods and negative ELBOs on both the validation and test sets, with low computational cost, for all three different network architectures. In comparison to the vanilla REINFORCE on MNIST-static, as shown in Table 2 (a), ARM achieves significantly lower test log-likelihoods, which can be explained by having much lower variance in its gradient estimation, while only costing 20% to 30% more computation time to finish the same number of iterations.
The trace plots in Figures 2, 5, and 6 show that ARM achieves its objective better or on a par with the state-of-the-art methods in all three different network architectures. In particular, the performance of ARM on MNIST-threshold is significantly better, suggesting ARM is more robust, better resists overfitting, and has better generalization ability. On OMNIGLOT, with "Nonlinear" network architecture, both REBAR and RELAX exhibit severe overfitting, which could be caused by their training procedure that updates the parameters of the control variates, which are designed to minimize the true variance of the gradient estimator, by minimizing the sample variance of the gradient estimator using SGD. For less overfitting linear and two-stochastic-layer networks, ARM overall performs better than both REBAR and RELAX and converges significantly faster (about 6-8 times faster) in terms of computation time.
4.2 MAXIMUM LIKELIHOOD INFERENCE FOR A STOCHASTIC BINARY NETWORK
Denoting xl, xu  R394 as the lower and upper halves of an MNIST digit, respectively, we consider a standard benchmark task of estimating the conditional distribution p0:2 (xl | xu) (Raiko et al., 2014; Bengio et al., 2013; Gu et al., 2016; Jang et al., 2017; Tucker et al., 2017), using a stochastic binary network with two stochastic binary hidden layers, expressed as
xl  Bernoulli((T0 (b1))), b1  Bernoulli((T1 (b2))), b2  Bernoulli((T2 (xu))). (30)
8

Under review as a conference paper at ICLR 2019

Table 3: For the MNIST conditional distribution estimation benchmark task, comparison of the test negative log-likelihood between ARM and various gradient estimators in Jang et al. (2017) is reported here.

Gradient estimator ARM ST DARN Annealed ST ST Gumbel-S. SF MuProp

- log p(xl | xu) 57.9 ± 0.1 58.9 59.7

58.7

59.3 72.0 58.9

We set the network structure as 392-200-200-392 which means both b1 and b2 are 200 dimensional

binary vectors and the transformation T are linear so the results are directly comparable with those in

Jang

et

al.

(2017).

We

approximate

log

p0:2 (xl

|

xu)

with

log

1 K

K k=1

Bernoulli(xl

;

(T0

(b(1k)

))),

where b1(k)  Bernoulli((T1 (b(2k)))), b(2k)  Bernoulli((T2 (xu))). We perform training with

K = 1, which can also be considered as optimizing on a single-Monte-Carlo-sample estimate of

the lower bound of the log likelihood shown in (28). We use Adam (Kingma & Ba, 2014), with the learning rate set as 10-4, mini-batch size as 100, and number of epochs for training as 2000. Given

the inferred point estimate of 0:2 after training, we evaluate the accuracy of conditional density estimation by estimating the negative log-likelihood as - log p0:2 (xl | xu), averaging over the test set using K = 1000. We show example results of predicting the activation probabilities of the pixels

of xl given xu in Figure 4 of the Appendix.

As shown in Table 3, optimizing a stochastic binary network with the ARM estimator, which is unbiased and computationally efficient, achieves the lowest test negative log-likelihood, outperforming previously proposed biased stochastic gradient estimators on similarly structured stochastic networks, including DARN (Gregor et al., 2013), straight through (ST) (Bengio et al., 2013), slope-annealed ST (Chung et al., 2016), and ST Gumbel-softmax (Jang et al., 2017), and unbiased ones, including score-function (SF) and MuProp (Gu et al., 2016).

5 CONCLUSIONS
To train a discrete latent variable model with one or multiple stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator to provide unbiased and low-variance gradient estimates of the parameters of Bernoulli distributions. With a single Monte Carlo sample, the estimated gradient is the product of uniform random noises and the difference of a function of two vectors of correlated binary latent variables. Without relying on learning control variates for variance reduction, it maintains efficient computation and avoids increasing the risk of overfitting. Applying the ARM gradient leads to not only fast convergence, but also low test negative log-likelihoods (and low test negative evidence lower bounds for variational inference), on both auto-encoding variational Bayes and maximum likelihood inference for stochastic binary feedforward neural networks. Some natural extensions of the proposed ARM estimator include generalizing it to multivariate categorical latent variables, combining it with a control-variate or local-expectation based variance reduction method, and applying it to reinforcement learning whose action space is discrete.

REFERENCES
Yoshua Bengio, Nicholas Le´onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Christopher M Bishop. Neural Networks for Pattern Recognition. Oxford university press, 1995.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859­877, 2017.
George Casella and Christian P Robert. Rao-blackwellisation of sampling schemes. Biometrika, 83 (1):81­94, 1996.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. arXiv preprint arXiv:1609.01704, 2016.
Michael C Fu. Gradient estimation. Handbooks in operations research and management science, 13: 575­616, 2006.

9

Under review as a conference paper at ICLR 2019
Peter W Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):75­84, 1990.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the Void: Optimizing control variates for black-box gradient estimation. In ICLR, 2018.
Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks. arXiv preprint arXiv:1310.8499, 2013.
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. MuProp: Unbiased backpropagation for stochastic neural networks. In ICLR, 2016.
Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The" wake-sleep" algorithm for unsupervised neural networks. Science, 268(5214):1158­1161, 1995.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-softmax. In ICLR, 2017.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183­233, 1999.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.
Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14):1­45, 2017.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29­37, 2011.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In ICML, 2013.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2017.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In ICML, pp. 1791­1799, 2014.
Andriy Mnih and Danilo J Rezende. Variational inference for monte carlo objectives. arXiv preprint arXiv:1602.06725, 2016.
Christian Naesseth, Francisco Ruiz, Scott Linderman, and David Blei. Reparameterization gradients through acceptance-rejection sampling algorithms. In AISTATS, pp. 489­498, 2017.
R. M. Neal. Connectionist learning of belief networks. Artificial Intelligence, pp. 71­113, 1992.
Art B. Owen. Monte Carlo Theory, Methods and Examples, chapter 8 Variance Reduction. 2013.
John Paisley, David M Blei, and Michael I Jordan. Variational Bayesian inference with stochastic search. In ICML, pp. 1363­1370, 2012.
Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning binary stochastic feedforward neural networks. arXiv preprint arXiv:1406.2989, 2014.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In AISTATS, pp. 814­822, 2014.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, pp. 1278­1286, 2014.
Sheldon M. Ross. Introduction to Probability Models. Academic Press, 10th edition, 2006.
10

Under review as a conference paper at ICLR 2019
Francisco J. R. Ruiz, Michalis K. Titsias, and David M. Blei. The generalized reparameterization gradient. In NIPS, pp. 460­468, 2016.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In ICML, pp. 872­879, 2008.
Lawrence K Saul, Tommi Jaakkola, and Michael I Jordan. Mean field theory for sigmoid belief networks. Journal of Artificial Intelligence Research, 4:61­76, 1996.
Yichuan Tang and Ruslan R Salakhutdinov. Learning stochastic feedforward neural networks. In NIPS, pp. 530­538, 2013.
Michalis K Titsias and Miguel La´zaro-Gredilla. Local expectation gradients for black box variational inference. In NIPS, pp. 2638­2646. MIT Press, 2015.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models. In NIPS, pp. 2624­2633, 2017.
Aaron van den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems, pp. 6306­6315, 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pp. 5­32. Springer, 1992.
Mingyuan Zhou and Lawrence Carin. Negative binomial process count and mixture modeling. arXiv preprint arXiv:1209.3442v1, 2012.
11

Under review as a conference paper at ICLR 2019

Appendix

A THE ARM ALGORITHM

We summarize the algorithm to compute ARM gradient for binary latent variables. Here we show the gradient with respect to the logits associated with the probability of Bernoulli random variables. If the logits are further generated by deterministic transform such as neural networks, the gradient with respect to the transform parameters can be directly computed by the chain rule. For stochastic transforms, the implementation of ARM gradient is discussed in Section 3.

Algorithm 1: ARM gradient for V -dimensional binary latent vector

input : Bernoulli distribution {qv (zv)}v=1:V with probability {(v)}v=1:V , target f (z);

z = (z1, · · · , zV ),  = (1, · · · , V )

output : and  that maximize E(, ) = Ez

V v=1

qv

(zv

)

[f

(z

;



)]

Initialize ,  randomly;

while not converged do

Sample zv  Bernoulli((v)) for v = 1, · · · , V ; sample uv  Uniform(0, 1) for v = 1, · · · , V , u = (u1, · · · , uV ) ; g = f (z; ) ;

f(u, ) = f 1[u>(-)] - f 1[u<()] ;

g

=

f(u, )(u

-

1 2

)

 =  + tg,  =  + tg with step-size t, t

end

B MULTILAYER DISCRETE STOCHASTIC NETWORK

We present the derivation of ARM gradient in multiple stochastic layers discussed in main text Proposition2.

Proof of proposition 2. First, to compute the gradient with respect to w1, since E (w1:T ) = Eq(b1)Eq(b2:T | b1)[f (b1:T )]
we have

where

w1 E (w1:T ) = Eu1Uniform(0,1)[f(u1, Tw1 (x))(u1 - 1/2)]w1 Tw1 (x),

f(u1,

Tw1 (x))

=

Eb2:T q(b2:T

|

b1 ),

[f (bb1=1[u1>(-Tw1 (x))])

1:T

)]

- E [f (b )]b2:T q(b2:T | b1), b1=1[u1<(Tw1 (x))])

1:T

(31) (32)
(33)

Second, to compute the gradient with respect to wt, where 2  t  T - 1, since E (w1:T ) = E Eq(b1:t-1) q(bt | Ebt-1) q(bt+1:T | bt)[f (b1:T )]
we have

(34)

wt E (w1:T ) = Eq(b1:t-1) EutUniform(0,1)[f(ut, Twt (bt-1), b1:t-1)(ut - 1/2)]wt Twt (bt-1) , (35)

where

f(ut,

Twt (bt-1),

b1:t-1)

=

Ebt+1:T

q(bt+1:T

| bt),

[f (bbt=1[ut>(-Twt (bt-1))])

1:T

)]

- E [f (b )]bt+1:T q(bt+1:T | bt), bt=1[ut<(Twt (bt-1))])

1:T

(36)

Finally, to compute the gradient with respect to wT , we have
wT E (w1:T ) = Eq(b1:T -1) EuT Uniform(0,1)[f(uT , TwT (bT -1), b1:T -1)(uT - 1/2)]wT TwT (bT -1) , (37)

f(uT , TwT (bT -1), b1:T -1) = f (b1:T -1, bT = 1[uT >(-TwT )(bT -1))] - f (b1:T -1, bT = 1[uT <(TwT )(bT -1))]

(38)

12

Under review as a conference paper at ICLR 2019

C MULTIPLE SAMPLES GRADIENT IN TOY EXAMPLE

The

variance

of

Monte-Carlo

estimation

with

sample

n

decreases

at

rate

1 n

.

Comparing

K

=

5000

for REINFORCE and K = 10 for ARM in Figure 3 indicates in this example ARM gradient can

reduce variance to around 1/500 of the REINFORCE one.

Gradient

0.10
0.05
0.00
0.05
0.10
0.15
0.006 0.004 0.002 0.000 0.002 0.004 0.006
0

REINFORCE(K=10) ARM(K=10)

0.04 0.03 0.02 0.01 0.00 0.01 0.02 0.03

0.004

0.002

0.000

0.002

0.004

500 1000 1500 2000 2500 3000
Iteration

0

REINFORCE(K=100) ARM(K=100)

0.008 0.006 0.004 0.002 0.000 0.002 0.004 0.006 0.008

0.004

0.002

0.000

0.002

0.004

500 1000 1500 2000 2500 3000
Iteration

0

REINFORCE(K=5000) ARM(K=5000)
500 1000 1500 2000 2500 3000
Iteration

0.49 0.499 0.501 0.51 True
0.49 0.499 0.501 0.51 True

Gradient

Figure 3: Estimation of the true gradient at each iteration using K > 1 Monte Carlo samples, using REINFORCE, shown in the top row, or ARM, shown in the bottom row. The ARM estimator exhibits significant lower variance given the same number of Monte Carlo samples.

D IMAGE COMPLETION AND VAE
The image completions in Figure 4 are obtained by random propagating the same upper half image through SBN for two times. The probability of the lower half digit pixels are ploted as the completion and suitble variations can be captured by the latent variables.

Figure 4: Randomly selected example results of predicting the lower half of a MNIST digit given its upper half, using a binary stochastic network, which has two binary linear stochastic hidden layers and is trained by ARM maximum likelihood inference. Red squares mark out notable variations between two random draws.
13

Under review as a conference paper at ICLR 2019

Bernoulli

Nonlinear Linear
Two layers

MNIST-threshold MNIST-static OMNIGLOT
MNIST-threshold MNIST-static OMNIGLOT
MNIST-threshold MNIST-static OMNIGLOT

ARM
101.3 109.9 129.5 110.3 116.2 124.2 98.2 105.8 118.3

RELAX
110.9 112.1 128.2 122.1 116.7 124.4 114.0 105.6 119.1

REBAR
111.6 111.8 128.3 123.2 117.9 124.9 113.7 105.5 118.8

ST Gumbel-Softmax
112.5 -
140.7 129.2
129.8
-

Table 4: Test negative ELBOs of discrete VAEs trained with four different stochastic gradient estimators. MNIST-threshold is the binarized MNIST thresholded at 0.5 and MNIST-static is the static binarized MNIST.

-ELBO

140 130 120 110 100
0
140 130 120 110 100
0

MNIST

Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM) Training(Gumbel) Validation(Gumbel)

-ELBO

150 140 130

MNIST

Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM) Training(Gumbel) Validation(Gumbel)

-ELBO

140 130 120

MNIST

Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM)

120

100000 200000 300000 400000 500000 600000 Steps
(a) Nonlinear

MNIST

Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM) Training(Gumbel) Validation(Gumbel)

-ELBO

0
150 140 130

110

100000 200000 300000 400000 500000 600000
Steps (b) Linear

100 0

MNIST

Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM) Training(Gumbel) Validation(Gumbel)

-ELBO

160 150 140 130 120

100000 200000 300000 400000 500000 600000
Steps (c) Linear two layers

MNIST

Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM)

120

2000 4000 6000 8000 10000 12000 14000
Time(seconds)

110 0

2000 4000 6000 8000 10000
Time(seconds)

(d) Nonlinear

(e) Linear

110 100
0

5000 10000 15000 20000
Time(seconds) (f) Linear two layers

-ELBO

Figure 5: Test negative ELBOs on MNIST-threshold with respect to training iterations, shown in the top row, and wall clock times on Tesla-K40 GPU, shown in the bottom row, for three differently structured Bernoulli VAEs.

-ELBO

Omniglot

Omniglot

Omniglot

160

Training(REBAR) Validation(REBAR) Training(RELAX)

140

Training(REBAR) Validation(REBAR) Training(RELAX)

140

Training(REBAR) Validation(REBAR) Training(RELAX)

Validation(RELAX)

Validation(RELAX)

135

Validation(RELAX)

150

Training(ARM) Validation(ARM)

135

Training(ARM) Validation(ARM)

130

Training(ARM) Validation(ARM)

Training(Gumbel)

Training(Gumbel)

-ELBO

-ELBO

140

Validation(Gumbel)

130

Validation(Gumbel)

125

130 125

120

0 100000 200000 300000 400000 500000 600000 Steps

115

0 200000 400000 600000 800000 1000000

0 200000 400000 600000 800000 1000000

Steps Steps

170 160 150 140 130
0

(a) Nonlinear

Omniglot

Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM) Training(Gumbel) Validation(Gumbel)

2000 4000 6000 Time(seconds)

8000

-ELBO

(b) Linear

150 145 140 135 130 125 120 0

Omniglot

Training(REBAR) Validation(REBAR) Training(RELAX) Validation(RELAX) Training(ARM) Validation(ARM) Training(Gumbel) Validation(Gumbel)

2500 5000 7500 10000 12500 15000 17500 Time(seconds)

-ELBO

(c) Linear two layers

Omniglot

Training(REBAR)

150

Validation(REBAR) Training(RELAX)

Validation(RELAX)

140 Training(ARM) Validation(ARM)

130

120

0 5000 10000 15000 20000 25000 30000 35000
Time(seconds)

(d) Nonlinear

(e) Linear

(f) Linear two layers

-ELBO

Figure 6: Test negative ELBOs on OMNIGLOT with respect to training iterations, shown in the top row, and wall clock times on Tesla-K40 GPU, shown in the bottom row, for three differently structured Bernoulli VAEs.

14

