Under review as a conference paper at ICLR 2019

EMI: EXPLORATION WITH MUTUAL INFORMATION MAXIMIZING STATE AND ACTION EMBEDDINGS

Anonymous authors Paper under double-blind review

ABSTRACT

Policy optimization struggles when the reward feedback signal is very sparse and essentially becomes a random search algorithm until the agent accidentally stumbles upon a rewarding or the goal state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or more ad-hoc measures of surprise. We propose EMI, which is an exploration method that constructs embedding representation of states and actions that does not rely on generative decoding of the full observation but extracts predictive signals that can be used to guide exploration based on forward prediction in the representation space. Our experiments show the state of the art performance on challenging locomotion task with continuous control and on image-based exploration tasks with discrete actions on Atari.

1 INTRODUCTION
The central task in reinforcement learning is to learn policies that would maximize the total reward received from interacting with the unknown environment. Although recent methods have demonstrated to solve a range of complex tasks (Mnih et al., 2015; Schulman et al., 2015; 2017), the success of these methods, however, hinges on whether the agent constantly receives the intermediate reward feedback or not. In case of challenging environments with sparse reward signals, these methods struggle to obtain meaningful policies unless the agent luckily stumbles into the rewarding or predefined goal states.
To this end, prior works on exploration generally utilize some kind of intrinsic motivation mechanism to provide a measure of surprise. These measures can be based on density estimation via generative models (Bellemare et al., 2016; Fu et al., 2017; Oh et al., 2015), predictive forward models (Stadie et al., 2015; Houthooft et al., 2016), or more ad-hoc measures that aim to approximate surprise (Pathak et al., 2017). Methods based on predictive forward models and generative models must model the distribution over state observations, which can make them difficult to scale to complex, high-dimensional observation spaces, while models that eschew direct forward predictive or density estimation rely on heuristic measures of surprise that may not transfer effectively to a wide range of tasks.
Our aim in this work is to devise a method for exploration that does not require a direct generation of high-dimensional state observations, while still retaining the benefits of being able to measure surprise based on the forward prediction. If exploration is performed by seeking out states that maximize surprise, the problem, in essence, is in measuring surprise, which requires a representation where functionally similar states are close together, and functionally distinct states are far apart.
In this paper, we propose to learn compact representations for both the states () and actions () simultaneously satisfying the following criteria: First, given the representations of state and the corresponding next state, the uncertainty of the representation of the corresponding action should be minimal. Second, given the representations of the state and the corresponding action, the uncertainty of the representation of the corresponding next state should also be minimal. Third, the action embedding representation () should seamlessly support both the continuous and discrete actions. Finally, we impose the linear dynamics model in the representation space which can also explain the rare irreducible error under the dynamics model. Given the representation, we guide the exploration by measuring surprise based on forward prediction and relative increase in diversity in the embedding representation space. Figure 1 illustrates an example of our learned state and action embedding representations and the linearity of sample transitions in the representation space in Montezuma's Revenge.

1

Under review as a conference paper at ICLR 2019

We present two main technical contributions that make this into a practical exploration method. First, we describe how compact state and action representations can be constructed via Donsker & Varadhan (1983) estimation of mutual information without relying on generative decoding of full observations. Second, we show that imposing linear topology on the learned embedding representation space (such that the transitions are linear), thereby offloading most of the modeling burden onto the embedding function itself, provides an essential informative measure of surprise when visiting novel states.

DOWN DOWN
Figure 1: Visualization of linear sample transitions in our learned embedding space.

For the experiments, we show that we can use our representations on a range of complex image-based tasks and robotic locomotion tasks with continuous actions. We report state of the art results compared to recent intrinsic motivation based exploration methods (Fu et al., 2017; Pathak et al., 2017) on several challenging Atari tasks and robotic locomotion tasks with sparse rewards.

2 RELATED WORKS
Our work is related to the following strands of active research:
Unsupervised representation learning via mutual information estimation Recent literature on unsupervised representation learning generally focus on extracting latent representation maximizing approximate lower bound on the mutual information between the code and the data. In the context of generative adversarial networks (Goodfellow et al., 2014), Chen et al. (2016); Belghazi et al. (2018) aims at maximizing the approximation of mutual information between the latent code and the raw data. Belghazi et al. (2018) estimates the mutual information with neural network via Donsker & Varadhan (1983) estimation to learn better generative model. Hjelm et al. (2018) builds on the idea and trains a decoder-free encoding representation maximizing the mutual information between the input image and the representation. Furthermore, the method uses Nowozin et al. (2016) estimation of Jensen-Shannon divergence rather than the KL divergence to estimate the mutual information for better numerical stability. Oord et al. (2018) estimates mutual information via autoregressive model and makes predictions on local patches in an image.
Exploration with intrinsic motivation Prior works on exploration mostly employ intrinsic motivation to estimate the measure of novelty or surprisal to guide the exploration. Bellemare et al. (2016) utilize density estimation via CTS (Bellemare et al., 2014) generative model and derive pseudocounts as the intrinsic motivation. Fu et al. (2017) avoids building explicit density models by training K-exemplar models that distinguish a state from all other observed states. Some methods train predictive forward models (Stadie et al., 2015; Houthooft et al., 2016; Oh et al., 2015) and estimate the prediction error as the intrinsic motivation. Oh et al. (2015) employs generative decoding of the full observation via recursive autoencoders and thus can be challenging to scale for high dimensional observations. VIME (Houthooft et al. (2016)) approximates the environment dynamics, uses the information gain of the learned dynamics model as intrinsic rewards, and showed encouraging results on robotic locomotion problems. However, the method needs to update the dynamics model per each observation and is unlikely to be scalable for complex tasks with high dimensional states such as Atari games.
Other approaches utilize more ad-hoc measures (Pathak et al., 2017; Tang et al., 2017) that aim to approximate surprise. ICM (Pathak et al. (2017)) transforms the high dimensional states to feature space and imposes cross entropy and euclidean loss so the action and the feature of the next state are predictable. However, ICM does not utilize the mutual information like VIME to directly measure the uncertainty and is limited to discrete actions. Our method (EMI) is also reminiscent of (Kohonen & Somervuo, 1998) in a sense that we seek to construct a decoder-free latent space from the high dimensional observation data with a topology in the latent space. In contrast to the prior works on exploration, we seek to construct the representation under linear topology and does not require

2

Under review as a conference paper at ICLR 2019

decoding the full observation but seek to encode the essential predictive signal that can be used for guiding the exploration.

3 PRELIMINARIES
We consider a Markov decision process defined by the tuple (S, A, P, r, ), where S is the set of states, A is the set of actions, P : S × A × S  R+ is the environment transition distribution, r : S  R is the reward function, and   (0, 1) is the discount factor. Let  denote a stochastic policy over actions given states. Denote P0 : S  R+ as the distribution of initial state s0. The discounted sum of expected rewards under the policy  is defined by

() = E

tr(st) ,

t=0

(1)

where  = (s0, a0, . . . , aT -1, sT ) denotes the trajectory, s0  P0(s0), at  (at | st), and st+1  P (st+1 | st, at). The objective in policy based reinforcement learning is to search over the space of parameterized policies (i.e. neural network) (a | s) in order to maximize ().

Also, denote PSAS as ing from s0  P0(s0)

the joint probability distribution of singleton experience and following the policy . Furthermore, define PA =

tuples
S ×S

(s, a, s dPS AS

) startas the

marginal distribution of actions, PSS = A dPSAS as the marginal distribution of states and the

corresponding next states, PS = S×A dPSAS as the marginal distribution of the next states, and

PSA = S dPSAS as the marginal distribution of states and the actions following the policy .

4 METHODS
Our goal is to construct the embedding representation of the observation and action (discrete or continuous) for complex dynamical systems that does not rely on generative decoding of the full observation, but still provides a useful predictive signal that can be used for exploration. This requires a representation where functionally similar states are close together, and functionally distinct states are far apart. We approach this objective from maximizing mutual information under several criteria.

4.1 MUTUAL INFORMATION MAXIMIZING STATE AND ACTION EMBEDDING
REPRESENTATIONS
We first introduce the embedding function of states  : S  Rd and actions  : A  Rd with parameters  and  (i.e. neural networks) respectively. We seek to learn the embedding function of states () and actions () satisfying the following two criteria:

1. Given the embedding representation of states and the actions [(s); (a)], the uncertainty of the embedding representation of the corresponding next states (s ) should be minimal and vice versa.
2. Given the embedding representation of states and the corresponding next states [(s); (s )], the uncertainty of the embedding representation of the corresponding actions (a) should also be minimal and vice versa.

Intuitively, the first criterion translates to maximizing the mutual information between

[(s); (a)], and (s ) which we define as IS(, ) in Equation (2). And the second crite-
rion translates to maximizing the mutual information between [(s); (s )] and (a) defined as IA(, ) in Equation (3).

maximize
,

IS(, )

:=

I([(s); (a)]; (s

))

=

DKL (PSAS

PSA  PS )

(2)

maximize
,

IA(, )

:=

I([(s); (s

)]; (a))

=

DKL (PSAS

PSS  PA)

(3)

Mutual information is not bounded from above and maximizing mutual information is notoriously difficult to compute in high dimensional settings. Motivated by Hjelm et al. (2018); Belghazi et al. (2018), we compute Donsker & Varadhan (1983) lower bound of mutual information. Concretely, Donsker-Varadhan representation is a tight estimator for the mutual information of two random variables X and Z, derived as in Equation (4).

3

Under review as a conference paper at ICLR 2019

"#&' (/*

("#&' (/* )

(" )

concat

FC FC fc fc

04 (("), ("), ("#&' (/* ))
-

sp
+

IS(JSD)

" CONV

CONV CONV CONV

CONV

"#

" %"&' (/*

fc FC

fc
FC
fc ("#)

concat
CONCAT concat

(") concat

concat
(%"&' (/* )

fc fc FC FC

sp
04 (("), ("), ("#))

01 (("), ("), ("#))
-

sp
+

IA(JSD)

sp
01 (" , (%"&' (/* ), ("#))

Figure 2: Computational architecture for estimating IS(JSD) and IA(JSD) for image-based observations.

I(X; Z) = DKL(PXZ PX  PZ )  sup EPXZ T(x, z) - log EPXPZ exp T(x, z),

where T : X × Z  R is a differentiable transform with parameter .

(4)

Theorem 1. I(JSD)(X; Z)  sup EPXZ [-sp (-T(x, z))] - EPXPZ [sp (T(x, z))] + log(4)

Proof.

I(JSD)(X; Z) = DJSD(PXZ PX  PZ )  sup EPXZ [S(x, z)] - EPXPZ [JSD (S(x, z))]

= sup EPXZ [-sp (-T(x, z))] - EPXPZ [sp (T(x, z))] + log(4),


(5)

where the inequality in the second line holds from the definition of f -divergence (Nowozin et al.,

2016). In conjugate

the third line, we substituted S of Jensen-Shannon divergence,

(x, z) = JSD(t)

log(2) - log(1 + exp(-T = - log(2 - exp(t)).

(x,

z)))

and

Fenchel

Furthermore, for better numerical stability, we approximate KL-divergence with Jensen-Shannon divergence (JSD) (Hjelm et al., 2018) which is bounded both from below and above by 0 and log(2). From Theorem 1, we have

maximize
,

IS(JSD)(,

)



maximize
,

sup
S S

EPSAS

[-sp (-TS ((s), (a), (s )))]

(6)

- EPSAPS sp TS ((s), (a), (s~ )) + log 4,

maximize
,

IA(JSD)(,

)



maximize
,

sup
A A

EPSAS

[-sp (-TA ((s), (a), (s )))]

(7)

- EPSS PA [sp (TA ((s), (a~), (s )))] + log 4,

where sp(z) = log(1 + exp z). The expectations in Equation (6) and Equation (7) are approximated using the empirical samples trajectories  . Note, the samples s~  PS and a~  PA from the marginals are obtained by dropping (s, a) and (s, s ) in samples (s, a, s~ ) and (s, a~, s ) from PSAS . Figure 2 illustrates the computational architecture for estimating the lower bounds on IS and IA.

4

Under review as a conference paper at ICLR 2019

4.2 EMBEDDING LINEAR DYNAMICS MODEL UNDER SPARSE NOISE
Since the embedding representation space is learned, it is natural to impose a topology on it (Kohonen, 1983). In EMI, we impose a simple and convenient topology where transitions are linear since this spares us from having to also represent a complex dynamical model. This allows us to offload most of the modeling burden onto the embedding function itself, which in turn provides us with a useful and informative measure of surprise when visiting novel states. Once the embedding representations are learned, this linear dynamics model allows us to measure surprise in terms of the residual error under the model or measure diversity in terms of the similarity in the embedding space. Section 5 discusses the intrinsic reward computation procedure in more detail.

Concretely, we seek to learn the representation of states (s) and the actions (a) such that the representation of the corresponding next state (s ) follow linear dynamics i.e. (s ) = (s) + (a). Intuitively, we would like the nonlinear aspects of the dynamics to be offloaded to the neural networks (·), (·) so that in the Rd embedding space, the dynamics become linear. Regardless of the expressivity of the neural networks, however, there always exists irreducible error under the linear dynamic model. For example, the state transition which leads the agent from one room to another in Atari environments (i.e. Venture, Montezuma's revenge, etc.) or the transition leading the agent in the same position under certain actions (i.e. Agent bumping into a wall when navigating a maze environment) would be extremely challenging to explain under the linear dynamics model.

To this end, we introduce the error model S : S × A  Rd, which is another neural network taking the state and action as input, estimating the irreducible error under the linear model. Motivated by the work of Cande`s et al. (2011), we seek to minimize for the sparsity of the term so that the error term contributes only on rare unexplainable occasions. Equation (8) shows the embedding learning problem under linear dynamics with sparse errors.

minimize S 2,0 ,, error sparsity
subject to  =  +  + S ,

(8)

embedding linear dynamics

where we used the matrix notation for compactness. , , S denotes the matrices of respective embedding representations stacked columns wise. Relaxing the 0 norm with 1 norm, Equation (9) shows our final learning objective.

minimize ,,

inf
S S

EPSAS

sp

-TS ((s), (a), (s ))

+ EPSAPS sp TS ((s),  (a), (s~ ))

+

inf
A A

EPSAS

sp

-TA ((s), (a), (s ))

+ EPSS PA sp TA ((s),  (a~), (s ))

+ residual  - ( +  + S ) 2,1 + sparsity S 2,1,

(9)

residual, sparsity are hyper-parameters which control the relative contributions of the linear dynamics error and the sparsity. In practice, we found the optimization process to be more stable when
we further regularize the distribution of action embedding representation to follow a predefined
prior distribution. Concretely, we regularize the action embedding distribution to follow a standard normal distribution via DKL(P N (0, I)) in similar spirit to VAEs Kingma & Welling (2013). Intuitively, this has the effect of grounding the distribution of action embedding representation (and
consequently state embedding representation) across different iterations of the learning process.

5 INTRINSIC REWARD AUGMENTATION

We consider two different formulations of computing the intrinsic reward. First, we consider a relative difference in the novelty of state representations based on the distance in the embedding representation space similar to Oh et al. (2015) as shown in Equation (10). The relative difference makes sure the intrinsic reward diminishes to zero (Ng et al., 1999) once the agent has sufficiently explored the state space. Also, we consider a formulation based on the prediction error under the linear dynamics model as shown in Equation (11). This formulation incorporates the sparse error term and makes sure we differentiate the irreducible error that does not contribute as the novelty.

rd(st, at, st) = g(st) - g(st), where

1 g(s) =
n

n
exp

-

(s) - (si) 22

2

i=1

(10)

5

Under review as a conference paper at ICLR 2019

re(st, at, st) = (st) + (at) + S(st, at) - (st) 2

(11)

Note the relative diversity term should be computed after the representations are updated based on the samples from the latest trajectories while the prediction error term should be computed before the update. Algorithm 1 shows the complete learning procedure in detail.

Algorithm 1 Exploration with mutual information state and action embeddings (EMI)

initialize , , , A, S

for i = 1, . . . , MAXITER do

Collect samples {(st, at, st)}tn=1 with policy  Compute residual error intrinsic rewards {re(st,

at,

st)}tn=1

following

Equation

(11)

for j = 1, . . . , OPTITER do

for k = 1, . Sample a

..,

n m

do

minibatch {(stl

,

atl

,

stl

)}ml=1

m

m

Compute TA (stl ), (atl ), (stl )

2 and
l=1

TA

(stl ), 

a~tl+

m 2

, (stl )

2
l=1

to derive the lower bound on IA(JSD)(, ) in Equation (7) m

m

Compute TS (stl ), (atl ), (stl )

2
and
l=1

TS

(stl ), (atl ), 

s~tl+

m 2

2
l=1

to derive the lower bound on IS(JSD)(, ) in Equation (6)

Update , , , A, S using the Adam (Kingma & Ba, 2015) update rule to minimize

Equation (9)

end for

end for Compute diversity intrinsic rewards {rd(st, at, st)}nt=1 following Equation (10) Augment the intrinsic rewards and update the policy network  using any RL method

end for

6 EXPERIMENTS
We compare the experimental performance of EMI to recent prior works on both of the lowdimensional locomotion tasks with continuous control from rllab benchmark (Duan et al., 2016) and the complex vision-based tasks with discrete control from the Arcade Learning Environment (Bellemare et al., 2013). For the locomotion tasks, we chose SwimmerGather and SparseHalfCheetah environments for direct comparison against the prior work of Fu et al. (2017). SwimmerGather is a hierarchical task where a two-link robot needs to reach green pellets giving positive reward instead of red pellets giving negative reward. SparseHalfCheetah is a challenging locomotion task where a cheetah-like robot does not receive any reward until it moves 5 units in one direction.
For vision-based tasks, we selected Freeway, Frostbite, Venture, Montezuma's Revenge, Gravitar, and Solaris for comparison with recent prior works (Pathak et al., 2017; Fu et al., 2017). These six Atari environments feature very sparse reward feedback and often contain many moving distractor objects which can be challenging for the methods that rely on explicit decoding of the full observations (Oh et al., 2015).
6.1 IMPLEMENTATION DETAILS
We use TRPO (Schulman et al., 2015) for policy optimization because of its capability to support both the discrete and continuous actions and its robustness with respect to the hyperparameters. In the locomotion experiments, we use a 2-layer fully connected neural network as the policy network. In the Atari experiments, we use a 2-layer convolutional neural network followed by a single layer fully connected neural network. We convert the 84 x 84 input RGB frames to grayscale images and resize them to 52 x 52 images following the practice in Tang et al. (2017). The embedding dimensionality is set to d = 2 in all of the environments except for Gravitar and Solaris where we set d = 8 due to their complex environment dynamics. We use Adam (Kingma & Ba, 2015) optimizer to train embedding networks. Please refer to Appendix A.1 for more details.
6.2 LOCOMOTION TASKS WITH CONTINUOUS CONTROL
We compare EMI with TRPO (Schulman et al., 2015) and EX2 (Fu et al., 2017) on two challenging locomotion environments: SwimmerGather and SparseHalfCheetah. Note that as ICM (Pathak et al.,

6

Under review as a conference paper at ICLR 2019

12

34

5

1 2
3 4

5

(a) Example paths in SparseHalfCheetah

8 17 6
23

9

10 11

(b) Our state embeddings for SparseHalfCheetah
3 7
6 214 58

54
(c) Example paths in Montezuma's Revenge

9 11 10
(d) Our state embeddings for Montezuma's Revenge

1 11
2 10 39 468 57

11 10 9

56 4 3 2 1
7
8

(e) Example paths in Frostbite

(f) Our state embeddings for Frostbite

Figure 3: Example sample paths in our learned embedding representations. Note the embedding dimensionality d is 2, and thus we did not use any dimensionality reduction techniques.

2017) does not support continuous control, we omit the comparison with ICM for this experiment. Figure 4 shows that EMI significantly outperforms the baseline methods by a large margin on both tasks. Figure 3b visualizes the scatter plot of the learned state embeddings and an example trajectory for the SparseHalfCheetah experiment. The figure shows that the learned representation successfully preserves the similarity in observation space.
6.3 VISION-BASED TASKS WITH DISCRETE CONTROL
For vision-based exploration tasks, we compare EMI with TRPO (Schulman et al., 2015), EX2 (Fu et al., 2017), and ICM (Pathak et al., 2017). Our results in Figure 5 show that EMI achieves the state of the art performance on Freeway, Frostbite, Venture, and Montezuma's Revenge in comparison to the baseline exploration methods. Figures 3c to 3f illustrate our learned state embeddings . Since our embedding dimensionality is set to d = 2, we directly visualize the scatter plot of the embedding representation in 2D. Figure 3d shows that the embedding space naturally separates state samples into two clusters each of which corresponds to different rooms in Montezuma's revenge. Figure 3f shows smooth sample transitions along the embedding space in Frostbite where functionally similar states are close together and distinct states are far apart. For information about how our error term S(s, a) works in those vision-based tasks, please refer to Appendix A.2.

7

Under review as a conference paper at ICLR 2019

TRPO TRPO + EX2 TRPO + EMI (ours) 0.6

300 250

200 0.4
150

0.2 100 50

0.0 0

50 0 250 500 750 1000 1250 1500 1750 2000 0
(a) SwimmerGather

200 400 600 800
(b) SparseHalfCheetah

1000

Figure 4: Performance of EMI on continuous Locomotion tasks with sparse rewards compared to baseline methods (TRPO, EX2). The solid line is the mean reward of 5 different seeds at each iteration and the shaded area represents one standard deviation from the mean.

12000 40 1000
10000
800 30 8000
6000 600 20 4000 400

10 2000 200

TRPO TRPO + EX2

0

0

0

TRPO + ICM TRPO + EMI (ours)

2000

200

0

100 200 300 400 500 0

100 200 300 400 500 0

100 200 300 400 500

(a) Freeway

(b) Frostbite

(c) Venture

6000 400
600 5000

500

300 4000

400 3000 200

300 2000 100

200 1000

0

0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500

(d) Gravitar

(e) Solaris

(f) Montezuma's Revenge

Figure 5: Performance of EMI on Atari environments with sparse rewards compared to the baseline methods (TRPO, EX2, ICM). EMI in (a), (b), (d), (e) uses relative diversity intrinsic rewards. Prediction error intrinsic rewards are used in (c), (f). The solid line is the mean reward of 5 different seeds at each iteration and the shaded area represents one standard deviation from the mean.

7 CONCLUSION
We presented EMI, a practical exploration method that does not rely on direct generation of high dimensional observations while extracting the predictive signal that can be used for exploration within a compact representation space. Our results on challenging robotic locomotion tasks with continuous actions and high dimensional image-based games with sparse rewards show that our approach transfers to a wide range of tasks and shows state of the art results significantly outperforming recent prior works on exploration. As future work, we would like to explore utilizing the learned linear dynamic model for optimal planning in the embedding representation space. In particular, we would like to investigate how an optimal trajectory from a state to a given goal in the embedding space under the linear representation topology translates to the optimal trajectory in the observation space under complex dynamical systems.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, and Aaron Courville. Mutual information neural estimation. In International Conference on Machine Learning, volume 2018, 2018.
Marc Bellemare, Joel Veness, and Erik Talvitie. Skip context tree switching. In International Conference on Machine Learning, pp. 1458­1466, 2014.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pp. 1471­1479, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253­279, 2013.
Emmanuel J Cande`s, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal of the ACM (JACM), 58(3):11, 2011.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems, pp. 2172­2180, 2016.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on Pure and Applied Mathematics, 36(2):183­ 212, 1983.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pp. 1329­1338, 2016.
Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2577­2587, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pp. 1109­1117, 2016.
Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Teuvo Kohonen. Representation of information in spatial maps which are produced by selforganization. In Synergetics of the Brain, pp. 264­273. Springer, 1983.
Teuvo Kohonen and Panu Somervuo. Self-organizing maps of symbol strings. Neurocomputing, 21 (1-3):19­30, 1998.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
9

Under review as a conference paper at ICLR 2019
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pp. 278­287, 1999.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In Advances in neural information processing systems, pp. 2863­2871, 2015.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, volume 2017, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, volume 2015, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2753­2762, 2017.
10

Under review as a conference paper at ICLR 2019

A APPENDIX
A.1 EXPERIMENT HYPERPARAMETERS
In all experiments, we use Adam optimizer with a learning rate of 0.001 and a minibatch size of 512 for 3 epochs to optimize embedding networks. In each iteration, we utilized collected TRPO batch at each iteration to train embedding networks except for SparseHalfCheetah which uses FIFO replay buffer of size 250000. The embedding dimensionality is set to d = 2 in all of the environments except for Gravitar and Solaris where we set d = 8. Relative diversity term is used as an intrinsic reward with the weight of 0.1, except for Venture and Montezuma's Revenge where the intrinsic reward is set as a prediction error term with the weight of 0.001. For the convenience of experiments, we set residual = 1 and tune the coefficient term info that is multiplied to the information terms in Equation (9). The following tables give the detailed information of the remaining hyperparameters.

Environments TRPO step size TRPO batch size Policy network Baseline network  network  network Information network
Error network
Max path length Discount factor info unitKL sparsity

SwimmerGather

SparseHalfCheetah

0.01

50k 5k

A 2-layer FC with (64, 32) hidden units (tanh)

A 32 hidden units FC (ReLU)

Linear baseline

Same structure as policy network

A 64 hidden units FC (ReLU)

A 2-layer FC with (64, 64) hidden units (ReLU)

State input passes the same network structure as policy network.

Concat layer concatenates state output and action.

A 256 units FC (ReLU)

500

0.995

0.05

0.1

10000

Table 1: Hyperparameters for MuJoCo experiments.

Environments TRPO step size TRPO batch size Policy network
Baseline network  network  network Information network
Error network
Max path length Discount factor info unitKL sparsity

Freeway, Frostbite, Venture, Montezuma's Revenge, Gravitar, Solaris
0.01 100k 2 convolutional layers (16 8x8 filters of stride 4, 32 4x4 filters of stride 2), followed by a 256 hidden units FC (ReLU) Same structure as policy network Same structure as policy network A 64 hidden units FC (ReLU) A 2-layer FC with (64, 64) hidden units (ReLU) State input passes the same network structure as policy network. Concat layer concatenates state output and action. A 256 units FC (ReLU) 4500 0.995 0.1 0.5 100

Table 2: Hyperparameters for Atari experiments.

11

Under review as a conference paper at ICLR 2019
A.2 EXPERIMENTAL EVALUATION OF THE ERROR MODEL
LEFT

UP

(a) st and st are from different rooms with distant background
images.

(b) The agent is already off the platform in st.

(c) The agent climbs up the ladder as expected.

Figure 6: Example transitions that entail large or small instances of the error term S(s, a), in Montezuma's Revenge.

In order to understand how the error term S(s, a) in EMI works in practice, we visualize three representative transition samples in Figure 6.
In the case of Figure 6a, due to the discrepancy between the two different background images, (st)-(st) 2 usually becomes large which makes the error term larger, too. The norm of the error term for this specific sample was 0.0296 and the resulting residual error was 2.69. Figure 6b describes the case where the action chosen by the policy has no effect on st i.e. P (st|st, at) = P (st|st). Linear models without any noise terms can easily fail in such events. Thus, the error term in our model gets bigger to mitigate the modeling error. S(st, at) 2 for this example transition was 0.0309 and its corresponding residual error was 3.78.
On the other hand, Figure 6c represents cases that the chosen action works in the environment as intended. Although this statement alone does not guarantee the effectiveness of the linear model, the error terms are likely to be small for most of the samples. The norm of the error term for the actual sample was 0.0082 and its residual error was 0.78.

12

