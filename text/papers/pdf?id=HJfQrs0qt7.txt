Under review as a conference paper at ICLR 2019

OL

D

Anonymous authors Paper under double-blind review

DN

N

A
While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures. We show that given proper initialization, learning expounds parallel independent modes and that certain regions of parameter space might lead to failed training. We also demonstrate that input norm and features' frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks. We provide a comparison between the dynamics of learning with cross-entropy and hinge losses, which could prove useful to understand recent progress in the training of generative adversarial networks. Finally, we identify a phenomenon that we baptize gradient starvation where the most frequent features in a dataset prevent the learning of other less frequent but equally informative features.
1I
Due to extremely complex interactions between millions of parameters, nonlinear activation functions and optimization techniques, the dynamics of learning observed in deep neural networks remain much of a mystery to this day. What principles govern the evolution of the neural network weights? Why does the training error evolve as it does? How do data and optimization techniques like stochastic gradient descent interact? Where does the implicit regularization of deep neural networks trained with stochastic gradient descent come from? Shedding some light on those questions would make training neural networks more understandable, and potentially pave the way to better techniques.
It is commonly accepted that learning is composed of alternating phases: plateaus where the error remains fairly constant and periods of fast improvement where a lot of progress is made over the course of few epochs (Saxe et al., 2013a). Theoretic explanations of that phenomenon exist in the case of regression on linear neural networks (Saxe, 2015) but extensions to the nonlinear case (Heskes & Kappen, 1993; Raghu et al., 2017; Arora et al., 2018) fail to provide analytical solutions.
It has been observed in countless experiments that deep networks present strong generalization abilities. Those abilities are however difficult to ground in solid theoretical foundations. The fact that deep network have millions of parameters ­ a number sometimes orders of magnitude larger than the dataset size ­ contradicts the expectations set by classic statistical learning theory on the necessity of regularizers (Vapnik, 1998; Poggio et al., 2004). This observation drove Zhang et al. (2016) to suggest the existence of an implicit regularization happening during the training of deep neural networks. Advani & Saxe (2017) show that the dynamics of gradient descent can protect against overfitting in large networks. Kleinberg et al. (2018) also offer some explanations of the phenomenon but understanding its roots remains an open problem.
In this work, we study the learning dynamics of a deep nonlinear neural network trained on a standard classification task using two different losses: the cross-entropy and the hinge loss. We mainly focus on binary classification, some of the results and properties can however be extended to the multi-class case. The questions we address in Sections 3, 4 and 5 respectively can be summarized as follows:
How does the confidence of a classifier evolve throughout learning?
1

Under review as a conference paper at ICLR 2019

How does the loss used during training impact its dynamics? Which properties of the features present in a dataset impact learning, and how?
Independent mode learning We show that, similarly to the case of linear networks and under certain initial conditions, learning happens independently between different classes, i.e. classes induce a partition of the network activations, corresponding to orthogonal modes of the data.
Learning dynamics We prove that in accordance to experimental findings, the hidden activations and the classification error of the network show a sigmoidal shape with slow learning at the beginning followed by fast saturation of the curve. We also characterize a region in the initialization space where learning is frozen or eventually dies out.
Hinge loss We study how using the hinge loss impacts learning and quantitatively compare it to the classic cross-entropy loss. We show that the hinge loss allows one to solve a classification task much faster, by providing strong gradients no matter how close to convergence the neural network is.
Gradient starvation Finally, we identify a phenomenon that we call gradient starvation where the most frequent features present in the dataset starve the learning of other very informative but less frequent features. Gradient starvation occurs naturally when training a neural network with gradient descent and might be part of the explanation as to why neural networks generalize so well. They intrinsically implement a variant of Occam's razor (Ariew, 1976): the simplest explanation is the one they converge to first.

2S

We are interested in a simple binary classification task, solved by training a deep neural net-

work with gradient descent. This simple setup encompasses for instance the training of gener-

ative adversarial networks discriminators. Some of our results extend to multi-class classifica-
tion, but, for the sake of conciseness, that case is only treated in the Appendix. We let D = {(xi, li)}1in  Rd × {1, 2} denote our dataset of vectors and labels. The classifier we consider
is a simple neural network with one hidden layer of h neurons and a ReLU non-linearity (see Fig. 1).

The output of the network is passed through a function denoted o which is either the sigmoid  in the binary

cross-entropy case or the identity in the hinge loss case.

The full function can be written as

Pt(x) := o(ut(x)) := o(ZtT (Wtx)+),

where Wt is an h × d matrix and Zt a vector of length h. In the C-class case, Zt is instead a C × h matrix and o the softmax function. We will see below an extension
to deeper networks. Wt and Zt are the parameters of the neural network. The subscript + (resp. t) denotes

the positive part of a real number (resp. the state of the element at time step t of training). The superscript T

Figure 1: Network Architecture

stands for the transpose operation. In the case of cross-entropy, Pt(x) represents the probability that

x belongs to class 1, for the hinge loss, Pt(x) is trained to reach {1, -1} for classes 1 and 2. For a

given class k, we let Dk denote the set of vectors belonging to it. We make the assumption

(H1) For any x, x  Dk, xT x > 0. For any x  D1 and x  D2, xT x  0.

It implies linear separability of the data, an assumption often necessary in theoretical studies (Soudry et al., 2017; Liao & Couillet, 2018; Nacson et al., 2018; Xu et al., 2018) and the positioning of the origin between the two sets. It is a very strong assumption which admittedly bypasses a large part of the deep learning dynamics. Nevertheless, it allows the discovery of interesting properties and helps develop intuitions about behaviors observed in more general settings.

3L

-

In this section, we focus on the case of the binary cross-entropy loss
LBCE(Wt, Zt; x) = 1- xD1 log((ZtT (Wtx)+)) - 1xD2 log(1 - (ZtT (Wtx)+)),

2

Under review as a conference paper at ICLR 2019

and train our network using stochastic gradient descent to minimize LBCE.

3.1 I
Our first lemma states that over the course of training and under suitable initialization, the active neurons of the hidden layer remain the same for each datapoint, and the coordinates of Zt remain of the same sign. To prove it, we let wti denote the i-th row of Wt and make the additional assumptions: there exists a partition {I1, I2} of {1, . . . , h} such that with k  {1, 2}
(H2) For any i  Ik, x  Dk and x / Dk, w0i x > 0 and w0i x  0. (H3) The i-th coordinate of Z0 is positive if i  I1, negative otherwise.
Assumption (H2) states that at the beginning of training, data points from different classes do not activate the same neurons. It is an analogue to the orthogonal initialization used in Saxe et al. (2013b). In Appendix A.6, we show that relaxing it hints towards an extended period of slow learning in the early stages of training. (H3) is introduced for Lemma 3.1 and Theorem 3.2 but will be relaxed later. Lemma 3.1. For any k  {1, 2}, x  Dk and t  0, the only non-negative elements of Wtx are the ones with an index i  Ik. The signs of the coordinates of Zt remain the same throughout training.
This lemma proves that updates to the parameters of our network are fully decoupled from one class to the other. An update for a data point in Dk will only influence the corresponding active rows and elements of Wt and Zt. This "independent mode learning" is an equivalent of the results by Saxe et al. (2013b) in a non-linear network trained on the cross entropy loss. The proof of the lemma and its extension to N - 1 hidden layers and multi-class classification can be found in Appendix A.

3.2 L

We are now interested in the actual dynamics of learning, and move from discrete updates to continuous ones by considering an infinitesimal learning rate  (Heskes & Kappen, 1993). Lemma 3.1 can easily be extended to this setting. For simplicity we assume h = 2, but similar results hold for arbitrary h (see Appendix A.4). For the moment, we maintain the assumptions (H1-3).
Theorem 3.2. Assuming that each class k contains the same vector repeated |Dk| times, then the output of the classifier on that class converges at a speed proportional to both |Dk|/|D| and to the norm of the vector it contains. The shape of the learning dynamics is shown on Fig. 2.

Proof. To simplify the notations, we arbitrarily assume that I1 = {1} and we write wt and zt the row and element modified by an update made using x  D1 (the case of D2 can be treated
symmetrically). By the independence proven above, we know that wt and zt are only affected by updates from D1. This greatly simplifies our evolution equations to: wt = f (x)zt xT and zt = f (x) wtx where f (x) is the gradient of the loss with respect to the pre-sigmoid output of the network ut(x): f (x) = 1{k=1} - (ut(x)) and the prime indicates a time derivative. We let
yt = wtx, which gives

yt

=

xT x zt 1 + eytzt

,

zt

=

yt 1 + eytzt

.

(1)

Writing x 2 = xT x, we see that the quantity yt2 - x 2zt2 is an invariant of the problem, so its solutions live on hyperbolas of equation y2 - x 2z2 = ±c with c := |y02 - x 2z02|.
We only treat the case of a degenerate hyperbola c = 0 i.e. y02 = x 2z02, and refer the interested reader to Appendix A.3 for the full derivation. In the case c = 0, we have t, yt = x zt (those quantities are both positive as x  D1 and (H2-3)). u(t) := ztwtx = ztyt thus follows the equation u (t) = 2 x u(t)(-u(t)). One can see a clear equivalence between our evolution equation and
the one obtained in Saxe et al. (2013b). Its analytical solution is (see Appendix A.3)

u(t) = (log +Ei)<-1>(2 x t + log(u0) + Ei(u0)) ,

(2)

where Ei is the exponential integral (Wiki., 2018) and <-1> denotes the inverse function.

We let u¯ denote that function for x = 1. For x  D2, the degeneracy assumption becomes y0 = - x z0. It can be shown similarly that v(t) := ztyt verifies the equation v (t) = 2 x v(t)(v(t))

3

Under review as a conference paper at ICLR 2019

y

c2
c3 c1

1
z0

Ti6m0e

yt (c=0.0 ||x||=0.5)

yt (c=0.0 ||x||=1.0)

yt (c=0.1 ||x||=1.0)

PPtt

(c=0.0 (c=0.0

||x||=0.5) ||x||=1.0)

Pt (c=0.1 ||x||=1.0)

120

Figure 2: Left. Phase diagram representing the dynamics of learning for the couple (zt, yt) depending on its initialization. yt is the value for the class considered, in which all examples have lined up. Each couple lives on a hyperbola. The slope of the linear curves is equal ± x (set to 0.7 in this diagram). The green region represents the initializations of (y, z) where the classification task will
be solved by the network. In the red region, learning does not start (the neuron is inactive at the beginning of training) or collapses as the neuron dies off when y reaches 0. The ci points show the three cases from Section 3.3. Right. yt and Pt(x  D1) = (ztyt) for different values of c and x .

with a negative initial condition (H2-3). In other words, u and v follow symmetric trajectories on the positive/negative real line. Below, u¯ (resp. v¯) denote those two trajectories for x = 1 and initial conditions u0 > 0 (resp. v0 < 0).
Let us now write p = |D1|/|D| the fraction of points in the dataset belonging to D1. Because we sample randomly from the dataset, this amounts to sampling p (resp. 1 - p) points from D1 (resp. D2) for each time unit during training, i.e. to rescaling the time axis by p for D1 and 1 - p for D2. Formally, this allows us to quantify the performance of the network at any time t

Pt(x  D1) = (u¯( x pt)) Pt(x  D2) = (-v¯( x (1 - p)t))

if x  D1 if x  D2

(3)

In particular, the convergence of u(t) to + can be bounded using our results: convergence happens at a rate slower than log(t) (Appendix A.3), a fact proved on its own by Soudry et al. (2017).

Interpretation Fig. 2 Right. shows the learning dynamics for different values of x and c. One

common characteristic between all the curves is their sigmoidal shape. Learning is slow at first, then

accelerates before saturating. This is aligned with empirical results from the literature. We also see on e.g. the blue and yellow curves that a larger x (or similarly a larger p) converges much faster. The effect of c on the dynamics can mostly been seen at the beginning of training (for instance

on the green and yellow curves). It fades as convergence happens, corresponding to points of the hyperbolas getting closer to the asymptote y = x z, see Fig. 2 Left. and below for more details.

We can characterize the convergence speeds more quantitatively with the

Corollary 3.3. Let  be the required accuracy on the classification task (i.e. Pt(x  D1)  1 - 

for x  D1). each classifier

Under verify

certain assumptions,

t2
t1

x1 x2

p 1-p

where

the xk

times t1 and is the norm

t2 of

required to vector xk

reach that from class

accuracy k.

for

The proof can be found in Appendix A.5. More frequent classes and larger inputs will be classified at a given level of confidence faster. The class frequency observation is fairly straightforward as updates on a more frequent class occur at a higher rate. As far as input sizes are considered, this can be seen as an analogous to the results from Saxe et al. (2013b) stating that input-output correlations drive the speed of learning. Because a sigmoid is applied on the network output, its (pre-sigmoid) targets are sent to ±. A larger input is more correlated with its target and converges faster.
On the assumptions The assumption that each class only contains one vector allows us to obtain the first closed-form solutions of the learning dynamics for the binary cross-entropy. It can be relaxed to classes containing orthogonal datapoints (see Appendix A.8) which still remains restrictive. A

4

Under review as a conference paper at ICLR 2019

possible interpretation is the following: if one were to consider a deep neural network that has learnt two discriminative features for the two classes, applying classic SGD on those features would result in a learning rate proportional to the prominence of those two features in the original dataset, and to learning curves of that exact shape. It is worth noting that such shapes are regularly observed by ML practitioners (Saxe et al., 2013b), our results reveal insights - otherwise unobtainable - into them.

3.3 P

In this section, we build the phase diagram of Fig. 2 Left.. The notations follow Theorem 3.2, in particular yt = wtx. So far, we have considered points in the top-right quadrant. In that region, the couple (zt, yt) lives on a hyperbola of equation y2 - x 2z2 = ±c where c  0. The sign in the equation is defined by the position of (z0, y0) relative to the function y = x z (positive if above, negative otherwise). If (z0, y0) is originally on that line, it will remain there throughout training.

We now explore the rest of the parameter space by relaxing some of our assumptions. We still consider a point x  D1, the diagram for D2 can be obtained by mirroring the z axis.
On Assumption (H2). Let us first consider the simple case of w0x = y0 < 0. The neuron is initially inactive because of the ReLU. No updates will ever be made to wt during training. This corresponds to the bottom half of the phase diagram, the parameters are frozen (also see Advani & Saxe (2017)).
On Assumption (H3). We now assume that z0  0. In that case, a simple extension of Lemma 3.1 shows that learning still happens independently on each row of wt. The outcome from Theorem 3.2 is still valid: the couple (zt, yt) lives on a hyperbola. It is however not guaranteed anymore that yt shall remain positive throughout training. There are three possible situations (numbered 1 to 3), each represented by the corresponding point on the diagram.
1) If y0 = - x z0, then the points (zt, yt) are stuck in the top-left quadrant and converge to zero. The equation verified by the logit u(t) is u (t) = -2 x u(t)(-u(t)) (see Appendix A.7).
2) If y0 > - x z0, the points (zt, yt) move on the hyperbola towards the top-right quadrant, at which point zt becomes positive and yt starts increasing again.
3) If y0 < - x z0, the points (zt, yt) move on the hyperbola towards the bottom-left quadrant, at which point yt becomes negative. When that happens, the neuron dies out, learning stops.

Only in the second case will the classifier end up solving the task: random initialization functions in certain parts of the (yt, zt) space. Those findings are summarized in the phase diagram Fig. 2 Left. The red region represents the initialization where the network will not be able to solve the task.
3.4 D

5

uutt

N=4 N=4

||x||=0.1 ||x||=0.5

ut N=8 ||x||=0.1

Pt N=4 ||x||=0.1

PPtt

N=4 N=8

||x||=0.5 ||x||=0.1

3

In a similar fashion to the above, we can extend our analysis to a network with N - 1 hidden layers of
the same shape. Under some stringent assumptions
on the initialization of the network, the classes are
learnt independently from one another. We can also
show that the ordinary differential equation verified by the logit u(t) of our network is

1
Time0 2 4 6 8 10
Figure 3: Logit u(t) and confidence Pt for different number of layers and values of x .

N x 2/N u2-2/N (t)

u (t) =

1 + eu(t)

.

The full analysis can be found in Appendix B. Solutions of Eq. 4 are plotted in Fig. 3.

(4)

Here too, a sigmoidal shape appears during the learning process. The effect of x reduces as N grows due to the power 2/N , however, larger values still converge faster (e.g. the blue and yellow curves). Additionally, as noted in Saxe et al. (2013b) for linear networks: the deeper the network, the faster the learning. This fact is studied in more details in Arora et al. (2018) where depth is shown
to accelerate convergence in some cases.

5

Under review as a conference paper at ICLR 2019

Time to convergence
Confidence

1.0 80 0.8

0.6

60 0.4

BCE p=0.8

BCE p=0.2

40 0.2

Hinge p=0.8 Hinge p=0.2

20 0.0 0 10 20 Ti3m0 e 40 50 60

BCE Hinge

Confidence00.5 0.6 0.7 0.8 0.9 1.0

Figure 4: Left. The three figures on the left are the result of training a generative adversarial network
on 8 Gaussians (see Appendix E for details on the experiment). The samples from the hinge loss are incomparably better. Right. Comparison between hinge loss and binary cross-entropy: training time required to reach a confidence  on the classification problem. Subplot: Solutions of Eqs. 3 and 5.

4O

Recent results in the field of generative adversarial networks have resurrected the hinge loss (Miyato et al., 2018). While its exact impact on performance is unclear, we run a small experiment to show its ability to generate better samples than the customary cross-entropy (see Fig. 4 and Appendix E). In order to perhaps uncover reasons behind its efficiency, we extend our results to the hinge loss:

LH (Wt, Zt; x) = max(0, 1 - ZtT (Wtx)+ · 1( xD1 - 1xD2 )) .

The hinge loss is non differentiable, but one can simply consider that learning stops as soon as the output of the network reaches 1 (resp. -1) for class D1 (resp. D2). Under the same assumptions than
in Theorem 3.2 (some of which can be relaxed, see Appendix C), we have the following result:

Theorem 4.1. For x  D1 (for D2 it is simply the opposite), the output u(t) of the network verifies

u(t) = min(1, u0 e2p x t) ,

c

u(t) = min(1, 2x

sinh(0 + 2p x t)) ,

(5)

where

0

=

cosh-1( y02+

x c

)2 z02

and

the

left

and

right

equations

correspond

to

c

=

0

and

c

=

0.

Proof. Simple computations show that the dynamics of the system are governed by yt = xT x zt and zt = yt. Following the method from Section 3, we see that u (t) = 2 x u(t) in the case where c = 0, leading to to the result. When c = 0, a classic hyperbolic change of variables allows to find
the solution. Its full derivation can be found in Appendix C.

The learning curves are plotted in Fig. 4 Right. We notice a hard sigmoidal shape corresponding to learning stopping when ut reaches 1. The comparison to binary cross-entropy (all other parameters kept equal) suggests a much faster convergence. With  the required confidence for our classifier, the time t required to reach  can easily be computed. We plot it for both losses in Fig. 4 Right which
confirms visually that the hinge loss converges much faster. We also notice the expected divergence of t for the binary cross entropy as  reaches 1 (training never converges in that case). We refer the
interested reader to Appendix C for a more general treatment of the Hinge loss, which fully relaxes
the assumption on the number of points in the classes.

5G

S

In this section, we attempt to quantify the impact of feature frequency inside a given class. We keep our simplified framework and consider that the input x to our network is composed of two underlying features x1  Rd1 and x2  Rd2 with d = d1 + d2. We let (x1, x2)  Rd denote the concatenation of the vectors x1 and x2. We assume that all the points in class D1 contain the feature x1 but only a fraction  of them contains the feature x2. This is equivalent to making continuous gradient updates using the vector (x1, x2) with a rate  and the vector (x1, 0) with a rate 1 - . We also assume that those features are fully informative for D1 ­ i.e. are absent from class D2. A network trained using
gradient descent on the dataset we just described has the following property

6

Under review as a conference paper at ICLR 2019

Even though the feature represented by x2 is fully informative of the class, the network will not classify a sample containing only x2 with high confidence.

It is the result of a phenomenon we coin gradient starvation where the most frequent features starve the gradient for the least frequent ones, resulting in a slower learning of those:

Theorem 5.1. Let  be our confidence requirement on class D1 i.e. training stops as soon as x  D1, Pt(x  D1)  1 - , and let t denote that instant. Then, under some mild assumptions,

1

Pt ((0, x2)  D1) 

1

+

e-

log(

1- 

)

.

(6)

Proof. From Lemma 3.1, assumptions (H2-3) are sufficient to guarantee independent mode learning

as well as positiveness of zt. We decompose wt = (tx1, tx2)+(x1, x2) where xT1 x1 = x2T x2 =

0, and assume that 0  0/ > 0 (in App. D, we relax some of those assumptions and prove an

equivalent

result).

The

evolution

equation

for

wt

is

wt

=

zt x 1+ezt wt x

with

x

=

(x1, x2)

(resp.

(x1, 0))

at an  (resp. 1 - ) rate. Projecting on x1 and x2 gives

t

=

 zt 1 + ezt(t+t)

+

(1

-

)

1

zt + eztt

,

t

=

 1

+

zt ezt (t +t )

.

(7)

From zt > 0, we see that t is an increasing function of time, which guarantees t > 0, and

t



t

+ (1 - )

zt

1 + ezt(t+t)

=

(1 +

1-  )t

=

t 

.

This proves that t



0,

t



t/.

We now consider t

such that zt t

=

log(

1- 

).

It is the

smallest t such that Pt((x1, x2)  D1)  Pt((x1, 0)  D1) = 1 - , its existence is guaranteed by

zt and t being increasing (we also assume that t = 0 does not verify those (in)equalities). We get

11

1

Pt ((0, x2)  D1) = 1 + e-zt t



1 + e-zt t

=

1

+

e-

log(

1- 

)

.

As can be seen in Eq. 7, the presence of t ­ which detects feature x1 ­ in the denominator of t greatly reduces its value, thus preventing the network from learning x2 properly.

Conf. on the rare feature

1.0
alpha = 0.5 alpha = 0.2 0.9 alpha = 0.1 alpha = 0.01
0.8
0.7

Table 1: Gradient starvation 
 0.5 0.2 0.1 0.01
99% 91% 71% 61% 51% 99.99% 99% 86% 72% 52%

0.6
0.5
0.5 0C.6onf. o0.n7 the 0d.8atase0t.9 1.0
Figure 5: Upper bound on Pt ((0, x2)  D1) as a function of 1 -  for different values of .

Table 2: Accuracy on the cats and dogs dataset (Real means the untouched test set)

Training Testing Testing (Real)

100% 100%

43.2%

In Fig. 5 Left., we plot for different values of  the confidence of the network when classifying x2 as a function of its confidence on x1 (see App. D. for more details). The gap between the two is very significant: with e.g.  = 0.1 and  = 10-4 (Table 1), Pt ((0, x2)  D1)  72%! Even though x2 is exclusively present in D1, and is thus extremely informative, the network is unable to classify it.
Experiment To validate those findings empirically, we design an artificial experiment based on the cats and dogs dataset (Kaggle, 2018). We create a very strong, perfectly discriminative feature by making the dog pictures brighter, and the cat pictures darker. We then train a standard deep neural network to classify the modified images and measure its performance on the untouched testing set.

7

Under review as a conference paper at ICLR 2019
Results The results can be seen in Table 2. The network perfectly learns to classify both the train and test modified set, but utterly fails on the real test data. This proves that the handcrafted light feature was learnt by the network, and is used exclusively to classify images. All the features allowing to recognize a cat from a dog are still present in the data, but the low level features (e.g. the presence of whiskers, how edges combine to form the shape of the animals and so on) are far less frequent than the light intensity, and thus were not learnt. The most frequent feature starved all the others.
6R
The learning dynamics of neural networks have been explored for decades. Baldi & Hornik (1989) studied the energy landscape of linear networks and the fixed point structure of gradient descent learning in that context. Heskes & Kappen (1993) developed a theory encompassing stochastic gradient descent and parameter dynamics and wrote down their evolution equations in on-line learning. However, those equations are heavily nonlinear and do not have closed-form solutions in the general case. Saxe et al. (2013b) study the case of deep linear networks trained with regression. They prove the existence of nonlinear learning phenomena similar to those seen in simulations of nonlinear networks and provide exact solutions to the dynamics of learning in the linear case. Some of our results are an extension of theirs to nonlinear networks. Choromanska et al. (2014); Raghu et al. (2017); Saxe (2015); Yosinski et al. (2014) also focus on neural network dynamics, while Nacson et al. (2018); Xu et al. (2018); Soudry et al. (2017) study the convergence rate of learning on separable data. Arora et al. (2018) prove that overparameterization can lead to faster optimization.
Recent work in the domain of generative adversarial networks (Goodfellow et al., 2014) has shown the resurgence of the hinge loss (Rosasco et al., 2004). In particular, part of the success encountered by Miyato et al. (2018) is due to their use of that specific loss function. Their main contribution however is a spectral normalization technique that produces state-of-the-art results on image generation. Their paper is part of a larger trend focusing on the spectra of neural network weight matrices and their evolution during learning (Vorontsov et al., 2017; Odena et al., 2018; Pennington et al., 2017). It, nevertheless, remains a poorly understood subject.
Zhang et al. (2016) performed some experiments proving that deep neural networks expound a socalled implicit regularization. Even though they have the ability to entirely memorize the dataset, they still converge to solutions that generalize well. A variety of explanations for that phenomenon have been advanced: correlation between flatness of minima and generalization (Hochreiter & Schmidhuber, 1997), natural convergence of stochastic gradient descent towards such minima (Kleinberg et al., 2018), built-in hierarchical representations (LeCun et al., 2015), gradient descent naturally protecting against overfitting (Advani & Saxe, 2017), and structure of deep networks biasing learning towards simpler functions (Neyshabur et al., 2014; Perez et al., 2018). Our results from Section 5 suggest that gradient descent indeed has a beneficial effect, but can also hurt in some situations.
7D
In order to obtain closed form solutions for the learning dynamics, we made the extremely simplifying assumption that each class only contains one point. We leave overcoming that limitation to future work. In the spirit of the proof in Section 5 where we considered two datapoints, we might be able to obtain upper and lower bounds on the learning dynamics.
Our comparison between the cross-entropy and the hinge losses reveals fundamental differences. It is noteworthy that the hinge loss is an important ingredient of the recently introduced spectral normalization (Miyato et al., 2018). The fast convergence of networks trained with the hinge loss might in part explain its beneficial impact. A deeper analysis of the connections between the two would lead to a better understanding of the performance of the algorithm. In this paper, we introduce the concept of gradient starvation and suggest that it might be a plausible explanation for the generalization abilities of deep neural networks. By focusing most of the learning on the frequent features of the dataset, it makes the network ignore the idiosyncrasies of individual datapoints. That rather desirable property has a downside however: very informative but rare features will not be learnt during training. This strongly limits the ability of the network to transfer to different data distributions where e.g. the rare feature exists on its own.
8

Under review as a conference paper at ICLR 2019
R
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural networks. CoRR, abs/1710.03667, 2017.
Roger Ariew. Ockham's Razor: A Historical and Philosophical Analysis of Ockham's Principle of Parsimony. PhD thesis, University of Illinois at Urbana-Champaign, 1976.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. CoRR, abs/1802.06509, 2018. URL http://arxiv.org/ abs/1802.06509.
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2:53­58, 1989.
Anna Choromanska, Mikael Henaff, Michaël Mathieu, Gérard Ben Arous, and Yann LeCun. The loss surface of multilayer networks. CoRR, abs/1412.0233, 2014. URL http://arxiv.org/ abs/1412.0233.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014. URL http://papers.nips.cc/paper/ 5423-generative-adversarial-nets.pdf.
Tom M. Heskes and Bert Kappen. On-line learning processes in artificial neural networks, 1993.
S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1­42, 1997.
Kaggle. Dogs vs. cats, 2018. URL https://www.kaggle.com/c/dogs-vs-cats.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL http: //arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.
Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape local minima? CoRR, abs/1802.06175, 2018. URL http://dblp.uni-trier.de/db/journals/ corr/corr1802.html#abs-1802-06175.
Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nature, 521(7553):436­ 444, 2015. URL http://dblp.uni-trier.de/db/journals/nature/nature521.html# LeCunBH15.
Zhenyu Liao and Romain Couillet. The dynamics of learning: A random matrix approach. In Jennifer G. Dy and Andreas Krause (eds.), ICML, volume 80 of JMLR Workshop and Conference Proceedings, pp. 3078­3087. JMLR.org, 2018. URL http://dblp.uni-trier.de/db/conf/ icml/icml2018.html#LiaoC18a.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral Normalization for Generative Adversarial Networks, 2018. arXiv:1802.05957v1.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. CoRR, abs/1803.01905, 2018. URL http://dblp. uni-trier.de/db/journals/corr/corr1803.html#abs-1803-01905.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. CoRR, abs/1412.6614, 2014. URL http://dblp.uni-trier.de/db/journals/corr/corr1412.html#NeyshaburTS14.
Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown, Christopher Olah, Colin Raffel, and Ian J. Goodfellow. Is generator conditioning causally related to gan performance? CoRR, abs/1802.08768, 2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1802. html#abs-1802-08768.
9

Under review as a conference paper at ICLR 2019
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), NIPS, pp. 4788­4798, 2017. URL http://dblp.uni-trier.de/db/conf/nips/nips2017. html#PenningtonSG17.
Guillermo Valle Perez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. CoRR, abs/1805.08522, 2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-08522.
Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi. General conditions for predictivity in learning theory. Nature, 428(6981):419­422, 2004.
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep understanding and improvement. arXiv preprint arXiv:1706.05806, 2017.
Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri. Are loss functions all the same?. Neural Computation, 16(5):1063­107, 2004. URL http: //dblp.uni-trier.de/db/journals/neco/neco16.html#RosascoVCPV04.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Learning hierarchical categories in deep neural networks. In Markus Knauff, Michael Pauen, Natalie Sebanz, and Ipke Wachsmuth (eds.), CogSci. cognitivesciencesociety.org, 2013a. ISBN 978-0-9768318-9-1. URL http:// dblp.uni-trier.de/db/conf/cogsci/cogsci2013.html#SaxeMG13.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. cite arxiv:1312.6120, 2013b. URL http: //arxiv.org/abs/1312.6120.
Andrew Michael Saxe. Deep Linear Neural Networks: A Theory of Learning in the Brain and Mind. PhD thesis, Stanford University, 2015.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. CoRR, abs/1710.10345, 2017. URL http://dblp.uni-trier.de/db/journals/corr/ corr1710.html#abs-1710-10345.
Vladimir Naumovich Vapnik. Statistical Learning Theory. Wiley-Interscience, September 1998.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. CoRR, abs/1702.00071, 2017. URL http: //arxiv.org/abs/1702.00071.
Wiki. Exponential integral, 2018. URL https://en.wikipedia.org/wiki/Exponential_ integral.
Tengyu Xu, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Convergence of sgd in learning relu models with separable data. CoRR, abs/1806.04339, 2018. URL http://dblp.uni-trier.de/db/ journals/corr/corr1806.html#abs-1806-04339.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320­3328, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http: //dblp.uni-trier.de/db/journals/corr/corr1611.html#ZhangBHRV16.
10

Under review as a conference paper at ICLR 2019

A A.

A.1 P

L 3.1

In this section, we prove the following lemma:
Lemma 3.1 For any k  {1, 2}, x  Dk and t  0, the only non-negative elements of Wtx are the ones with an index i  Ik. The signs of the coordinates of Zt remain the same throughout training.

Proof. We prove this with a simple induction. The claim is true at t = 0 from assumptions (H1-3).
Let us assume that at time set t, the different parts of the lemma are true. The SGD updates to the weights stemming from a single observation x  Dk (the extension to a mini-batch is straightforward) with a learning rate  are:

Zt(x) = f (x) (Wtx)+

Wt(x) = f (x) (ZtT  ek) xT ,

(8)

where  denotes the element-wise product of two vectors and ek  Rh is the binary vector with ones on the indices from Ik. f (x) is the gradient of the loss with respect to the pre-sigmoid output of the network Ft(x): f (x) = 1{k=1} - (Ft(x)). The updates to Zt are positive on its indices belonging to I1, and negative otherwise, proving the second claim of the lemma. Moving to Wt, only its rows and hidden neurons with indices i  Ik are modified. For an element x  D, Wt+1x = Wtx + f (x)( iIk zi) x where zi is the i-th element of Zt. By induction,
iIk zi has the same sign as f (x) (positive on D1, negative on D2). From (H1), x is positive (resp. negative) if x is in Dk (resp. otherwise). The update keeps the k-th neuron active (resp.
inactive).

A.2 E

L 3.1

-

Let us consider a simple C-classification task. D = {(xi, yi)}1in  Rd × C is our dataset of vectors/labels where C denotes both the set of possible labels and its cardinal depending on the context. The classifier we are training is a simple neural network with one hidden layer of C neurons, a ReLU non-linearity and a softmax . The full function is written as

Pt(x) := (Ft(x)) := (Zt(Wtx)+) ,

(9)

where Wt (resp. Zt) is a C ×d (resp. C ×C) weight matrix and  the softmax function. The subscript + denotes the positive part of a real number. Pt(x) is a C-vector representing the probability that x belongs to each class. The subscript t denotes the state of the element at time step t of training.

For a given class k, we let Dk be the set of vectors belonging to class k. We make the following assumptions, with k = k  C two arbitrary classes

(H4) For any x, x  Dk, xT x > 0. For any x  Dk and x  Dk , xT x  0. (H5) For any x  Dk and x  D \ Dk, w0kx > 0 and w0kx  0, where w0k is the k-th row of W0.

(H6) Z0 is initialized randomly to positive numbers on the diagonal, and non-positive elsewhere.

Those assumptions are straightforward extensions of the ones from the main text. We train the classifier using stochastic gradient descent on the cross-entropy loss of our problem. Ft+1 is the state of the neural network after one SGD update to Ft. Our first lemma states that over the course
of training, the active neurons of the hidden layer remain the same for each element of the dataset, and the elements of Zt remain of the same sign.
Lemma A.1. For any k  C, x  Dk and t  0, the only non-negative element of Wtx is its k-th element and all diagonal (resp. non-diagonal) elements of Zt are positive (resp. negative).

Proof. We prove this with a simple induction. The claim is true at t = 0 from assumptions (H4-6).
Let us assume that at time set t, the different parts of the lemma are true. The SGD updates to the weight matrices stemming from a single observation x  Dk (the extension to a mini-batch is straightforward) and a learning rate  are:

Zt = yL (Wtx)+T

Wt = (ZtT yL  ek )xT ,

(10)

11

Under review as a conference paper at ICLR 2019

where  denotes the element-wise product of two vectors and ek the k basis vector of RC . yL

is the gradient of log Ft(x)k (the cross entropy loss for a sample from class k) with respect to

the output of Zt. One can show that (yL)k = 1 -

eFt (x)k j eFt(x)j

and (yL)i = -

foreFt (x)i
j eFt(x)j

i = k i.e. yL = ek - Yt(x). The update to Zt is non-negative on the diagonal, and non-positive

elsewhere, which proves the second claim of the lemma. As far as Wt is concerned, only its k-

th row is modified. For an element x  D, Wt+1x = Wtx + KxT x ek where K is the dot

product between the k-th column of Zt and yL. By assumptions on the data, xT x is positive

(resp. negative) if x is in Dk (resp. otherwise), so the update keeps the k-th neuron active (resp.

inactive).

The proof can be extended to any number of hidden layers h  C by modifying assumptions (H5-6) using a partition {I1, . . . , IC } of {1, . . . , h} similar to the binary classification case. Each set Ii in the partition describes the neurons active at the initialization of the network for an element x  Di. The modified assumptions are:
(H5') For any i  Ik, x  Dk and x  D \ Dk, w0i x > 0 and w0i x  0.
(H6') For any i  Ik, j / Ik, (Z0)ki > 0 and (Z0)kj  0.
The lemma then translates to those inequalities remaining true at any time t  0.

A.3 P

T 3.2

In this section we develop the proof of Theorem 3.2 of the main text.

Proof. We consider an update made using x  D1. Writing yt = wtx, our system follows the system of ordinary differential equations

yt

=

xT 1+

x zt eyt zt

,

zt

=

yt 1 + eytzt

.

(11)

Writing xT x = x 2, we see that ytyt = x 2ztzt. The quantity yt2 - x 2zt2 is thus an invariant of the problem. With c := |y02 - x 2z02|, the solutions of Eq. 11 live on hyperbolas of equation

 y2 - x 2z2 = c  
y2 - x 2z2 = -c

 

y2 -

x 2z2 =

0

if y02 - x 2z02 > 0, if y02 - x 2z02 < 0, if y02 - x 2z02 = 0.

(12)

We start by treating the case of a degenerate hyperbola c = 0. We have t, yt = x zt (those quantities are both positive as x  D1 and (H2-3)). We let u(t) := ztwtx = ztyt and see by
combining the two equations in Eq. 11 that

2 x u(t) u (t) = 1 + eu(t) .

(13)

For any uf  u0, let t = u<-1>(uf ) (u is a bijection from R+  [u0, +[ since its derivative is

strictly positive). We have

1 t=
2x

uf 1 + ey

1

dy =

u0 y

2x

(log(

uf u0

)

+

Ei(uf

)

-

E

i(u0))

,

(14)

where Ei(x) = -

 -x

e-u u

du

is

the

exponential

integral.

In the end, with the superscript <-1>

denoting the inverse function

uf = u(t) = (log +Ei)<-1>(2 x t + log(u0) + Ei(u0)) .

We let u¯ denote the function u(t) above for x = 1, the solution for x = 1 can easily be inferred by a rescaling of t in u¯. For x  D2, the system of ODEs is

yt

=

-

1

x +

2 zt e-yt zt

,

zt

=

- 1

+

yt e-yt zt

,

(15)

12

Under review as a conference paper at ICLR 2019

the degeneracy assumption becomes y0 = - x z0 (we know from (H2) that y0 > 0 and from (H3) that z0 < 0). It can be shown similarly that v(t) := ztyt verifies the equation v (t) = 2 x v(t)(v(t)) with a negative initial condition. In other words, u and v follow symmetric trajectories on the positive/negative real line. Below, u¯ and v¯ denote those two trajectories for x = 1 and initial conditions u0 > 0 and v0 < 0.
Let us now write p = |D1|/|D| the fraction of points in the dataset belonging to D1. Because we sample randomly from the dataset, this amounts to sampling p (resp. 1 - p) points from D1 (resp. D2) for each time unit during training, ie to rescaling the time axis by p for D1 and 1 - p for D2. Formally, this allows us to quantify the performance of the network at any time t

Pt(x  D1) = (u¯( x pt)) Pt(x  D2) = (-v¯( x (1 - p)t))

if x  D1, if x  D2,

(16)

which concludes the proof for c = 0.

From Eq. 14, we also see that

1 t=
2x

uf 1 + ey

1

dy 

u0 y

2x

uf y
e 2 dy =
u0

1 x

uf
(e 2

-

e

u0 2

)

,

where we used the inequality y



y
0, e 2

>

y.

It eventually gives us

u(t)  2 log(

x

t

+

e

u0 2

)

,

(17) (18)

a result in line with convergence rates obtained in Soudry et al. (2017).

Let us now study the non-degenerate case. We apply the classic change of coordinates



yt =

c cosh( ), 2



yt =

c sinh( ), 2



c

zt =

x

sinh( ) 2



c

zt =

x

cosh( ) 2

if y02 > x 2z02, if y02 < x 2z02.

Since yt2 +

x

2zt2

= c cosh() and ytzt =

c 2x

sinh(), we see that

(ytzt)

=

yt2 + x 2zt2 1 + eytzt

=

c 2x

c cosh() cosh() =
1 + ec sinh()/2 x

,

(19) (20)
(21)

where the first equality used the system of equations Eq. 11. This gives us the dynamics of  as

2x



= 1 + ec sinh()/2 x

,

with an initial condition 0

=

cosh-1( y02+

x c

2z02 ).

For any f



0, we see that t

=

<-1>(f )

verifies

f 1 + ec sinh()/2 x

t=
0

2 x d .

There is no closed-form solution for that integral (that we know of). It can however be computed numerically. On Fig. 2 Right of the main text, we plot the curves for yt and (ztyt) for different values of c and x . We obtain a sigmoidal shape similar to previously made empirical observations. We notice in particular that for larger values of x the function converges faster.

A.4 E

We now extend the result from Theorem 3.2 to the case with h hidden neurons. Let us still consider
an update made on x  D1. We know from assumptions and by Lemma 3.1 that the only active neurons in the network are indexed by I1. The network weights follow the evolution equations:

(wti)

=

xT zti 1 + e ,jI1 ztj wtj x

(zti) = e

wtix .jI1 ztj wtj x

13

Under review as a conference paper at ICLR 2019

We similarly define yti = wtix which brings

(yti)

=

xT x zti 1 + e jI1 ztj ytj

,

(zti) = e

yti .jI1 ztj ytj

The couple (yti, zti) follows the same hyperbolic invariance, defined by a constant ci. In the case where i  I1, ci = 0, we see that uti := ztiyti verifies

(uit)

=

2 x uit 1 + e jI1 ujt

,

and a simple summation on i shows that ut := iI1 uti follows Eq. 13. The dynamics of the logit in this case are identical to the single active neuron case, the only difference is potentially its initial
value.

A.5 P

C

3.3

Corollary 3.3 Let  be the required accuracy on the classification task (i.e. Pt(x  D1)  1 - 

fcolarsxsifieDr v1e)r.ifUynttd12ercerxxta12in1a-pspsuwmhpetrieonsx,ktheistitmheesnto1rmandoftv2ercetqourirexdk

to reach that from class

accuracy k.

for

each

Proof. We consider the case c = 0. The classification error drops at a rate proportional to x1 p for D1 and to x2 (1 - p) for D2. More precisely, let us assume that u0  |v0| and look for a classification confidence of 1 - . We write uf = <-1>(1 - ) = log(1/ - 1), tv = u¯<-1>(|v0|)

and

t

=

u¯<-1>(uf )

=

1 log(1/ (log(
2 u0

- 1) )

+ Ei(log(1/

- 1)) -

Ei(u0)) ,

t represents the time taken to reach confidence  with an initialization u0, and tv the time to reach v0 starting in u0. We see that

P (x  D1)  1 -   t  t1 = P (x  D2)  1 -   t  t2 =

t
x1 p t - tv
x2 (1 - p)

if x  D1, if x  D2 .

The ratio between the initializations u0 and

vc0onavreercgleonscee(it.iem. etsvreisadsms att12ll)=andxx12the1-pcpo(n1fid-enttcve).reOqnueiresemeesntthlaatrigfeth(ie.ew. etighist

large), the ratio is approximately

x1 x2

p
1-p .

A.6 R

(H2)

Let us first recall that the assumption states:
(H2) For any i  Ik, x  Dk and x / Dk, w0i x > 0 and w0i x  0.
We now assume that h = 2 and study the evolution of the first row of matrix Wt, written wt in the following. We relax assumption (H2) by assuming that there is a point x2 in D2 such that w0x > 0. And we consider updates to wt coming from sampling equally x1 from D1 and x2. The evolution equations can be written as

wt

=

xT1 zt 1 + eztwtx1

-

xT2 zt 1 + e-ztwtx2

,

zt

=

wt x1 1 + eztwtx1

-

wt x2 1 + e-ztwtx2

.

In order to make the analysis simpler, we assume that xT1 x2 = 0. We decompose wt = tx1 + tx2 + x and get

t

=

zt 1 + eztt

,

t

=

- 1

+

zt e-zt t

,

zt

=

t 1 + eztt

-

t 1 + e-ztt

.

14

Under review as a conference paper at ICLR 2019

From our relaxation of assumption (H2), we know that 0 > 0. We also have 0 > 0 and z0 > 0.
This means that t is increasing, and that t is decreasing until it reaches 0, at which point learning
in that direction stops. A competition appears between t and t to determine the sign of zt. More importantly, tzt which measures the confidence the network has in x1 belonging to class 1 evolves

according to

(tzt)

=

zt2 + t2 1 + eztt

-

1

tt + e-ztt

.

The negative term slows down the learning of D1 until t reaches 0. When that happens, we obtain

the regime described in the main text. This observation might be a clue as to why in more complex

networks with random initialization, the slow phase of learning can last for a long time: the network

needs to build its independent modes of learning.

A.7 T -

When the initial conditions of the network verify y0 = - x z0, then at all time t, yt = - x zt. Plugging that equality in Eq. 11 results in

-2 x u(t) u (t) = 1 + eu(t)

where again u(t) = ztyt. This means that the logit is negative and increasing. Let u0 < uf < 0, we see that the time at which u reaches uf verifies

1 uf 1 + ey

1 -u0 1 + e-y

t=- 2 x u0

dy = y 2 x -uf

y dy  log(-u0) - log(-uf ) .

(22)

t diverges to + as uf tends to 0 from below. The logit converges to 0 without ever reaching it.

A.8 R

One of the major assumptions made in the main text is the fact that each class contains a single element. In this section, we slightly relax it to the case where the points ({xi}1im  Rd) in a class are all orthogonal to one another1 (while still verifying Assumption (H1)). In that case, each presentation of a training vector will only affect wt in the direction of that specific vector. Let yti denote wtxi, the unnormalized component of wt along xi. We consider a batch update on the
weights of the neural network. In that case:

(yti)

=

xi 2 zt 1 + eztyti

,

(zt)

=

m
i=1 1

yti + eztyti

.

Assuming that the vectors all have the same norm (denoted x below) and that the y0i are all equal, then that equality remains true at all time (they follow the same update equation). We let yt denote

that value:

(yt)

=

x 2 zt 1 + eztyt

,

(zt)

=

myt 1 + eztyt

.

A new invariant appears in those equations: c := |my02 - x 2z02|. In the case c = 0 (the other case can be treated as above), we obtain the following evolution equation for the logit of any point in the

class:



2 m x u(t)

u (t) =

1 + eu(t)

. 

We end up with a similar equation than before except for the m factor, which boosts the convergence

speed. However, one should not forget that we are now training on a full batch (i.e. on m points)

during each unit of time. Performing the same numberof updates for the single point class would generate a m factor in the convergence speed of u (one m factor for each y and z functions). The

slower convergence for the more general case can be explained by the fact that each point is making an update on wt in its own direction. That direction being orthogonal to all others points makes it

useless for their classification.

1This implies in particular m  d

15

Under review as a conference paper at ICLR 2019

A B.

In this section we study the case of a deeper network with N -1 hidden layers. Similarly to above, we can prove the existence of independent modes of learning. To that end, neurons need to be activated in a disjoint manner from one class to the other. Due to the growing complexity of the interactions between the parameters of the network, this requires very strong assumptions on the initialization of the network and on the shape of the network. Assuming that the network is written as

Pt(x) := (ZtT (ZtN-2 · · · (Zt1(Wtx)+)+ . . .)+) ,

with Wt an h × d matrix and for all 1  i  N - 2, Zti an h × h matrix. We maintain assumption (H2) from the main text, and extend (H3) to all Zti by assuming that they are diagonal, and that the j-th element of their diagonal is positive if j  I1, negative otherwise. To simplify notations, we go back to assuming h = 2 and take an update on x  D1. Only the first elements of every matrix are modified, we write them zti and keep the notations zt, wt and yt. We can then write the evolution
equations:

zt

=

ztN-2 · · · zt1yt 1 + eu(t)

,

(zti)

=

zt ztN -2

· · · zti+1zti-1 · · · yt 1 + eu(t)

,

yt =

x 2ztztN-2 · · · zt1

1 + eu(t)

,

with u(t) = zt zti yt.

Assuming that z0 = z01 = . . . = z0N-1 =

y0 x

,

we

see

that

those

equals

remain true throughout training. This gives us

zt

=

(zt)N-1 x 1 + eu(t)

,

u(t) = (zt)N x ,

u (t) = N (zt)N-1zt x .

Combining those equations gives us the ODE verified by the logit of our system

N x 2/N u2-2/N (t)

u (t) =

1 + eu(t)

.

The solution of that equation for N = 4 and N = 8 can be found on Fig. 3 of the main text.

(23)

A C.

C.1 P T 4.1

In this section we prove Theorem 4.1 from the main text on the dynamics of learning in the case of the Hinge loss:

LH (Wt, Zt; x) = max(0, 1 - ZtT (Wtx)+ · 1( xD1 - 1xD2 )) .

Let us consider updates made after observing a point x  D1, the converse can be treated similarly

with a simple change of sign. The system of ordinary differential equations verified by the parameters

of our network is:

yt = x 2 zt ,

zt = yt .

(24)

The same relation between yt and zt appears in those equations than in the cross-entropy case.

Defining c similarly, we have u (t) = 2 x u(t) in the case where c = 0, leading to u(t) = u0e2 x t.

When c = 0, the same change of variables can be applied and leads to

yt

=

 c

cosh(

0

+

2

x

t) ,



zt =

c x

sinh( 0 + 2

x

t) ,

c ut = 2 x sinh(0 +2 x t) , (25)

with 0

=

cosh-1( y02+

x c

2z02 ).

Those equations are only valid until ut reaches 12.

At that point

learning stops, the network has converged. If the initialization is such that that condition is already

verified, then the weights will not change as they already solve the task. The learning curves along with the initialization diagram can be found in Fig. 6. We notice a hard sigmoidal shape, corresponding to learning stopping when ut reaches 1.

2We are considering class {1} here, but the equivalent can be proven for class {-1}.

16

Under review as a conference paper at ICLR 2019

C.2 G
Let us now consider the general case of a class containing an arbitrary number of points C1 = {xi}1im  Rd. We consider the case of updates done in full batches (standard gradient descent in other words). In that case, we see that the network obeys the following dynamics:

m
wt = zt xiT ,
i=1

m
zt = wt xTi .
i=1

m

Letting X = xi denote the sum of all the datapoints in that class, we see that those dynamics boil

i=1

down to our previous treatment for a single point. The same cases appear, depending on the value

of c := |(w0X)2 - X

see that:

wt

=

yt

XT X2

2z02|. + w0

We explicitly where yt =

trecactotshhe(c20>+0

case. Following the methods above, we X t) and w0T is the component of w0

orthogonal to X (and thus unchanged during training). With zt and ut defined as above (with X

instead of x), an arbitrary example x is then classified as

P (x



C1)

=

ztyt

XT x X2

+

ztw0x

=

ut

XT x X2

+

ztw0x .

A D.

In this section, we prove a relaxed version of Theorem 5.1 from the main text:

Theorem D.2. Let  be our confidence requirement on class D1 i.e. the training stops as soon as

x



D1,

Pt(x



D1)



1 - .

Let

t

denote

that

instant

i.e.

zt t

=

log(

1- 

).

If

0

<

0,

the

inequality (9) from the main text is valid. Otherwise, with w0 = (0x1, 0x2) + (x1 , x2 ), we have

1

Pt ((0, x2)



D1)



1

+

e-

log(

1- 

)-zt

(0

-0

)

.

Proof. We start with the 0 < 0 case. If t is negative, the result from the main text clearly holds. Otherwise, there exists t~ < t such that t~ = 0 (t is increasing). The proof of Theorem 5.1 from the main text can then directly be applied to [t~, t]. If 0 > 0, the inequality on t and t holds and gives t  0 + (t - 0). Plugging it into Pt ((0, x2)  D1) concludes our proof.

In the main text, we assume that 0 - 0 < 0 and obtain a bound on the confidence which is independent from 0 and 0. Using that bound allows to obtain Fig. 7, but is partly unfair as the
initialization of the network is already favoring the strong feature.

However, we note that under small random initialization zt and t are of the same order of mag-

nitude

and

(0

-

0)

is

very

small

compared

to

log(

1- 

).

The

additional

term

in

the

denominator

thus has a limited effect on the exponential, gradient starvation is still happening (a fact confirmed

by the experiment on the cats and dogs dataset). In the main text, Fig. 5 plots the upper bound for a fair initialization 0 = 0 = 0.1 (in that case, we need to assume that zt = t ).

17

Under review as a conference paper at ICLR 2019

1.4

1.2

1.0

0.8

0.6

yt c=0.01 ||x||=0.7 yt c=0.20 ||x||=0.7

yt c=0.01 ||x||=1.5

0.4 yt c=0.20 ||x||=1.5

ut c=0.01 ||x||=0.7

0.2

ut c=0.20 ||x||=0.7 ut c=0.01 ||x||=1.5

ut c=0.20 ||x||=1.5

0.0 0 1 2 3 4 5 6

Time

Figure 6: Solution of Eq.25.

Confidence on the rare feature

1.0 alpha = 0.5

alpha = 0.2

0.9

alpha = 0.1 alpha = 0.01

0.8

0.7

0.6

0.5
0.5 Co0.n6 fidenc0.7e on th0.8e data0s.9et 1.0
Figure 7: Upper bound on Pt ((0, x2)  D1) as a function of 1 -  for different values of .

A E.

E.1 M

G

In this experiment, the data is constructed using eight independent Gaussian distributions around a unit circle. The variance of each Gaussian is chosen such that all eight modes of the data are separated by regions of low data probability, but still contain a reasonable amount of variance. This simple experiment resembles multi-modal datasets. Although this task might seem simple, in practice many generative adversarial networks fail to capture all the modes. This problem is generally known as mode collapse.
As shown in the main text, using the hinge loss instead of the common binary cross-entropy loss alleviates the problem significantly. The architectures used for the generator and discriminator both consist of four hidden layers where each layer has 256 hidden units. As a common choice, a ReLU is used as the non-linearity function for hidden units. The length of the noise input vector is 128. The Adam optimizer (Kingma & Ba, 2014) was applied during training with  = 10-4, 1 = 0.5 and 2 = 0.9. The PyTorch framework (Paszke et al., 2017) was used to conduct the experiment.

E.2 D

.C

For the purpose of highlighting the fact that the most frequent feature starved all the others, we conducted an experiment on a classification task. We modified the cats and dogs dataset (Kaggle, 2018) by setting the cats images to be lighter than the dogs images. To do so, each pixel in a cat image is scaled to be between 0 and 127 while each pixel in a dog image is scaled to be between 128 and 255. The dataset consists of 12500 images of each class. The classifier has an architecture similar to VGG16 (Simonyan & Zisserman, 2014). In order to isolate the effect of the induced bias, no regularization was applied. The Adam optimizer was applied here as well during training with  = 10-4, 1 = 0.9 and 2 = 0.99. The PyTorch framework was used to conduct the experiment.

18

