Under review as a conference paper at ICLR 2019
IMPROVING ON-POLICY LEARNING WITH STATISTICAL REWARD ACCUMULATION
Anonymous authors Paper under double-blind review
ABSTRACT
Deep reinforcement learning has obtained significant breakthroughs in recent years. Most methods in deep-RL achieve good results via the maximization of the reward signal provided by the environment, typically in the form of discounted cumulative returns. Such reward signals represent the immediate feedback of a particular action performed by an agent. However, tasks with sparse reward signals are still challenging to on-policy methods. In this paper, we introduce an effective characterization of past reward statistics (which can be seen as long-term feedback signals) to supplement this immediate reward feedback. In particular, value functions are learned with multi-critics supervision, enabling complex value functions to be more easily approximated in on-policy learning, even when the reward signals are sparse. We also introduce a novel exploration mechanism called "hot-wiring" that can give a boost to seemingly trapped agents. We demonstrate the effectiveness of our advantage actor multi-critic (A2MC) method across the discrete domains in Atari games as well as continuous domains in the MuJoCo environments. A video demo is provided at https://youtu.be/zBmpf3Yz8tc and source codes will be made available upon paper acceptance.
1 INTRODUCTION
Advances in deep learning have mobilized the research community to adopt deep reinforcement learning (RL) agents for challenging control problems, typically in complex environments with raw sensory state-spaces. Breakthroughs by Mnih et al. (2015) show RL-agents can reach abovehuman performance in Atari 2600 games, and AlphaGo Zero Silver et al. (2017) becomes the world champions on the game of Go. Still, training RL agents is non-trivial. Off-policy methods typically require days of training to obtain competitive performance, while on-policy methods could be trapped in local minima. Recent techniques featuring on-policy learning Mnih et al. (2016); Schulman et al. (2017); Wu et al. (2017) have shown promising results in stabilizing the learning processes, enabling an agent to solve a variety of tasks in much less time. In particular, the state-of-the-art on-policy ACKTR agent by Wu et al. (2017) shows improved sample efficiency with the help of Kronecker-factored (K-Fac) approximate curvature for natural gradient updates, resulting in stable and effective model updates towards a more promising direction.
However, tasks with sparse rewards remain challenging to on-policy methods. An agent could take massive amount of exploration before reaching non-zero rewards; and as the agent learns on-policy, the sparseness of reward feedback (receiving all-zero rewards from most actions performed by the agent) could be malicious and render an agent to falsely predict all states in an environment towards a value of zero. As there does not exist a universal criterion for measuring "task sparseness", we show an ad-hoc metric in Figure 1 in an attempt to provide intuition. For example, we observe that the ACKTR agent is unable to receive sufficient non-zero immediate rewards that can provide instructive agent updates in Atari games "Freeway" and "Enduro", resulting in failures when solving these two games. Moreover, if ACKTR gets drawn to and trapped in unfavorable states (as in games like Boxing and WizardOfWor), it could again take long hours of exploration before the agent can get out of the local minima. Such evidence shows that on-policy agent could indeed suffer from the insufficiencies of guidance provided by the exclusive immediate reward signals from the environment.
In this paper, we introduce an effective auxiliary reward signal in tasks with sparse rewards to remedy the deficiencies of learning purely from standard immediate reward feedbacks. As on-policy
1

Under review as a conference paper at ICLR 2019

6.4%

38.3%

55.3%

Figure 1: Performance of A2MC on Atari games trained with 15 million timesteps. Our method

has a winning rate of 55.3% among all the Atari games tested, as compared to the ACKTR. Our

A2MC learns quickly in some of the hardest games for on-policy methods, such as "Boxing",

"Enduro", "Freeway", "Robotank" and "WizardOfWor". The sparseness of a game is defined as

the sparseness of average rewards x obtained by ACKTR within the first n = 106 timesteps by

(x) =

 n-

x1 x2

 /( n - 1). A higher value of sparseness indicates sparser rewards. A relative

performance margin (in terms of final reward) larger than 10% is deemed as winning / losing. The

shaded region denotes the standard deviation over 2 random seeds.

agents may take many explorations before reaching non-zero immediate rewards, we argue that we can leverage past reward statistics to provide more instructive feedbacks to agents in the same environment. To this end, we propose to characterize the past reward statistics in order to gauge the "long-term" performance of an agent (detailed in Section 4). After performing an action, an agent will receive a long-term reward signal describing its past performance upon this step, as well as the conventional immediate reward from the environment. To effectively characterize the long-term performance of the agent, we take into considerations both the crude amount of rewards and the volatility of rewards received in the past, where highly volatile distributions of long-term rewards are explicitly penalized. This enables complex value functions to be more easily approximated in multi-critics supervision. We find in practice that by explicitly penalizing highly volatile long-term rewards while maximizing the expectation of short-term rewards, the learning process and the overall performance are improved regarding both sample efficiency and final rewards. We further propose a "hot-wiring" exploration mechanism that can boost seemingly trapped agent in the earlier stage of learning. By leveraging the characterization of long/short-term reward statistics, our proposed advantage actor multi-critic model (A2MC) shows significantly improved performance on the Atari 2600 games and the MuJoCo tasks as compared to the state-of-the-art on-policy methods.
2 RELATED WORK
The family of off-policy methods Wang et al. (2015) may be less prone to failure in tasks with sparse rewards at the cost of large amount of explorations before performing agent updates. To tackle the challenge in tasks with rarely observed rewards, pseudo-rewards maximization is adopted in earlier works Konidaris & Barto (2009); Silver & Ciosek (2012). Auxiliary vision tasks (e.g., learning pixel changes or network features) are adopted in the off-policy UNREAL agent Jaderberg et al. (2016) in order to facilitate learning better feature representations, particularly for sparse reward environments. Another direction of effort aims to design a better reward function for improving sample efficiency via experience replay. Andrychowicz et al. (2017) enhances off-policy learning by re-producing informative reward in hindsight for sequences of actions that do not lead to success previously. The HRA approach Van Seijen et al. (2017) exploits domain knowledge to define a set of environment-specific rewards based on reward categories. In contrast to heuristically defining
2

Under review as a conference paper at ICLR 2019

vision-related auxiliary tasks, our proposed on-policy A2MC agent learns from the characterization of intrinsic past reward statistics obtainable from any environment; and different from the hybrid architecture tailored specific to Ms. Pacman, our A2MC agent can generalize well to a variety of tasks without the need to engineer a decomposition of problem-specific environment rewards.
The multi-agent approaches Lanctot et al. (2017); Lowe et al. (2017); Jin et al. (2018) present another promising direction for learning. They propose to train multiple agents in parallel when solving a task, and to combine multiple action-value functions with a centralized action-value function. The multi-critics supervision in our proposed A2MC model can be seen as a form of joint-task or multi-task learning Teh et al. (2017) for both long-term and short-term rewards.
Our empirical results based on learning the characterization of long/short-term reward statistics also echo the effectiveness of a recently proposed off-policy reinforcement learning framework Bellemare et al. (2017) that features a distributional variant of Q-learning, wherein the value functions are learned to match the distribution of standard immediate returns. Also, Wang et al. (2016) shows that applying experience replay to on-policy methods can further enhance learning stability. Schulman et al. (2016) proposes a variant of advantage function that provides both low-variance and low-bias gradient estimates. These works are orthogonal to our approach can potentially be combined with the proposed characterization of past reward statistics to further enhance learning performance.

3 PRELIMINARY

Consider the standard reinforcement learning setting where an agent interacts with an environment

over a number of discrete time step. At each time step t, the agent receives an environment state

st, then executes an action at based on policy t. The environment produces reward rt and next

state st+1, according to which the agent gets feedback of its immediate action and will decide its

next action at+1. The process ¡S, A, R, S¿, typically considered as a Markov Decision Process,

continues until a terminal state sT upon which the environment resets itself and produces a new

episode. Under conventional settings, the return is calculated as the discounted summation of rewards

rt accumulated from time step t onwards Rt =

 k=0

 k rt+k .

The

goal

of

the

agent

is

to

maximize

the expected return from each state st while following policy . Each policy  has a corresponding

action-value function defined as Q(s, a) = E[Rt|st = s, at = a; ]. Similarly, each state s  S

under policy  has a value function defined as: V (s) = E[Rt|st = s]. In value-based approaches

(e.g., Q-learning Mnih et al. (2015)), function approximator Q(s, a; ) can be used to approximate

the optimal action value function Q(s, a). This is conventionally learned by iteratively minimizing

the below loss function:

L() = E[(yttarget - Q(st, at; ))2],

(1)

where yttarget = rt +  maxa Q(st+1, a ; ) and st+1 is the next state following state st.

In policy-based approaches (e.g., policy gradient methods), the optimal policy (a|s) is approxi-
mated using the approximator (a|s; ). The policy approximator is then learned by gradient ascent
on E[Rt]   log (at|st; )Rt. The REINFORCE method Williams (1992) further incorporates a baseline b(st) to reduce the variance of the gradient estimator: E[Rt]REINF ORCE   log (at|st; )(Rt - b(st))

In actor-critic based approaches, the variance reduction further evolves into the advantage function A(st, at) = Q(st, at) - V (st) in Mnih et al. (2016), where the action value Q(st, at) is approximated by Rt and b(st) is replaced by V (st), deriving the advantage actor-critic architecture where actor-head (·|s) and the critic-head V (s) share low-level features. The gradient update rule w.r.t.
the action-head is  log (at|st; )(Rt - V (st; )). The gradient update w.r.t. the critic-head is: (Rt - V (st; ))2, where Rt = rt + V (st+1).

4 CHARACTERIZATION OF PAST REWARD STATISTICS
The conventional reward rt received from the environment at time step t after an action at is performed represents the immediate reward regarding this particular action. This "immediacy" could be interpreted as a short-term horizon of how the agent is doing, i.e., evaluating the agent via judging its actions by immediate rewards. We argue that the deficiencies of learning solely from immediate rewards mainly come from this limitation that the agent is learning from one single type of exclusive

3

Under review as a conference paper at ICLR 2019

3 2 1 0
12
RH=1.06
0.2

Reward Sequence r1
past rewards immediate reward rt = 0.1
3Proce4ssed5 Seq6uenc7e 8 9

3 2 1 0
12
RH=8.01
0.2

Reward Sequence r2
past rewards immediate reward rt = 1.0
3Proce4ssed5 Seq6uenc7e 8 9

3 2 1 0
12
RH=8.01
0.2

Reward Sequence r3
past rewards immediate reward rt = 1.0
3Proce4ssed5 Seq6uenc7e 8 9

0.0 0.0 0.0

post-processed sequence R

post-processed sequence R

post-processed sequence R

zero-variability reference Rzero

zero-variability reference Rzero

zero-variability reference Rzero

0.2 0

1

V2olat3ility

4Stati5stics6 of

7 R

8

9

0.2 0

1

V2olat3ility 4Stati5stics6of

7 R

8

9

0.2 0

1

V2olat3ility 4Stati5stics6of

7 R

8

9

5

( R) = 0.62

rvwr =

H(1

[

]( R)
max

) = 0.65

5

( R) = 0.62

rvwr =

H(1

[

]( R)
max

)

=

4.88

5

( R) = 0.16

rvwr =

H(1

[

]( R)
max

) = 7.80

0 2.0 1.5 1.0 0.5 0.0 0.5 1.0 0 2.0 1.5 1.0 0.5 0.0 0.5 1.0 0 2.0 1.5 1.0 0.5 0.0 0.5 1.0
(a) (b) (c)

Figure 2: Illustration of the proposed variability-weighted reward (VWR). The first row shows the
raw reward sequence (blue) while the second row presents the post-processed sequence R (green) and the zero-variability reference Rzero (orange), and RH is calculated as a reflection of how high the immediate reward is. The third row shows the volatility statistics of R, representing how varied past rewards were. We curated 3 hypothetical reward sequences ­ (a) highly varied sequence with low immediate reward, resulting in the lowest VWR; (b) highly varied sequence with high immediate reward, leading to a relatively high VWR; (c) stable sequence with high immediate reward, achieving the best VWR. More examples can be found in the Appendix A.

short-term feedback. As the goal of providing reward feedback to an agent is to inform the agent of its performance, we seek to find an auxiliary performance metric that can measure whether the agent is performing consistently well. Inspired by the formulation of Sharpe Ratio Sharpe (1994) in evaluating the long-term performance of fund performance and trading strategies, an effective characterization of historical reward statistics should take into account at least two factors, namely 1) how high the immediate reward is and 2) how varied past rewards were.

4.1 VARIABILITY-WEIGHTED REWARD

To this end, we follow insight behind Dowd (2000) and define a variability-weighted characterization
of rewards in the past. This is illustrated in Figure 2. In particular, we consider a historical sequence of T rewards upon timestep t (looking backward T - 1 timesteps): r = [rt-(T -1)..., rt-2, rt-1, rt]. In order to evaluate how high and varied the reward sequence is, a few steps of pre-processing G is
applied, denoted as R = G(r). In particular, we first derive the reward change at each timestep by extracting the first-order difference with dn = rn - rn-1:

d = [dt-(T -1), dt-(T -2), . . . , dt] = [rt-(T -1), rt-(T -2) - rt-(T -1), . . . , rt - rt-1]. Then we re-order the sequence by flipping 1 with fn = dt+1-n:

(2)

f = [f1, f2, . . . , fT ] = [dt, dt-1, . . . , dt-(T -1)].

(3)

Next we append f0 = 1 to the head of sequence f and take the normalized cumulative sum to obtain post-processed reward sequence R as:

R

=

[R0, R1, . . . , RT ]

=

T

1 +

1 [f0, f0

+

f1, . . . ,

T

fi].

i=0

(4)

Under such processing, R is a reward sequence with RT

- R0

=

T

1 +1

rt

,

and

Rn

-

Rn-1

=

T

1 +1

(rt+1-n

-

rt-n).

Therefore, the difference between RT

and R0

represents the immediate

1By flipping, we further encourage recent stable rewards and penalize the volatility of recent past rewards. A concrete example is given in the Appendix A.

4

Under review as a conference paper at ICLR 2019

reward and the whole sequence R reflects the volatility of past rewards. In Figure 2, three examples of processed sequence are presented in the second row with the corresponding raw rewards shown in the first row. We account for how high the immediate reward is by defining the average log total return as:

RH

=

100

×

1
(e T

ln

RT R0

- 1).

(5)

To account for how varied past rewards were, we first define a smooth zero-variability reference as:

Rzero

=

[R0zero, Rz1ero, . . . , RzTero]

=

R0[e0×R, e1×R, . . . , eT R] with

R

=

1 T

ln

RT R0

, representing

a smooth monotonic reference sequence from R0 to RT . Then we define the reward differential R

as

the

differential

reward

versus

its

zero-variability

reference

as

R(n)

=

,Rn -Rznero
Rnzero

whose

statistics

are sketched in the third row of Figure 2. With maximally allowed volatility as max, the variability

weights can be defined as:

 = 1 - [ (R) ] , max

(6)

where (·) is the standard deviation and  controls the rate to penalize highly volatile reward distribution. Finally we can derive the variability-weighted past reward indicator rvwr for the

characterization of past reward statistics:

rvwr =

RH

(1

-

[

 (R ) max

]

)

if (R) < max, RT > 0

0 otherwise

(7)

Example computed values of rvwr for the characterization of different reward statistics are shown in Figure 2.

4.2 MULTI-CRITIC ARCHITECTURE

A higher value of rvwr indicates better agent performance as the result of the historical sequence of actions. The same set of optimization procedures for conventional value function (i.e., via maximization of immediate reward signal r) update can be applied accordingly. The actual returns computed from both the "long-term" and "short-term" rewards are discounted by the same factor . In particular, for N -step look-ahead approaches, we have:

N -1

Rtshort-term =

nrt+n + N V (st+N ),

n=0

(8)

N -1

Rtlong-term =

nrtv+wnr + N V vwr(st+N ).

n=0

(9)

Similar to the standard state value function V (s), we further define V vwr(s) as the value function w.r.t the variability-weighted reward rvwr. These value functions form multiple critics judging a
given state s. The gradients w.r.t. the critics then become:

short-term [(Rtshort-term - V (st; short-term))2] + long-term [(Rtlong-term - V vwr(st; long-term))2]. (10)
We show the effectiveness of the proposed characterization of past reward statistics in advantage actor-critic frameworks. The two different value functions can share the same low-level feature representation, enabling a single agent to learn multiple critics as parameterized by j, j  {short-term, long-term}.

5 HOT-WIRE -EXPLORATION
Being handed a game-stick, a human most likely would try out all the available buttons on it to see which particular button entails whatever actions on the game screen, hence receiving useful feedbacks. Inspired by this, we propose to hot-wire the agent to perform an identical sequence of

5

Under review as a conference paper at ICLR 2019

Figure 3: Performance of A2MC on Atari games. "Hot-Wiring" exploration makes the agent easier to figure out how to play challenging games like "Robotank" and "WizardOfWor", and for most games, it provides a better initial state for the agent to start off at a game and hence to obtain better final results. The number in figure legend shows the average reward among the last 100 episodes and the percentage shows the performance margin as compared to ACKTR. The shaded region denotes the standard deviation over 2 random seeds.

randomly chosen actions in the N-step look-ahead during the initial stage (randomly pressing down a game-stick button for a while):

at+k =

a random action identical for all k (at+k|st+k) for k = 0, 1, 2, ..., N - 1

with prob with prob 1 -

(11)

We show that by enabling the "hot-wiring" mechanism2, a seemingly trapped agent can be boosted to learn to quickly solve problems where rewards can only be triggered by particular action sequences, as shown in games like "Robotank" and "WizardOfWor" in Figure 3.

The full algorithm, advantage actor multi-critic learning (A2MC), is shown in Algorithm 1 in the Appendix E.

6 EXPERIMENTS

We use the same network architecture and natural gradient optimization method as in the ACKTR

model Wu et al. (2017). We set max = 1.0,  = 2.0 and T = 20 in the computation of variability-

weighted reward. For hot-wiring exploration, we choose

=

0.20

and

initial

stage

to

be

first

1 40

of the total training period for all experiments. Other hyperparameters such as learning rate and

gradient clipping remain the same as in the ACKTR model Wu et al. (2017). We first present results

of evaluating the proposed A2MC model in two standard benchmarks, the discrete Atari experiments

and the continuous MuJoCo domain. Then we show further ablation studies on the robustness of the

hyper-parameters involved as well as evaluating the extensibility of the proposed long/short-term

reward characterizations to other on-policy methods.

6.1 ATARI 2600 GAMES
We follow standard evaluation protocol to evaluate A2MC in a variety of Atari game environments (starting with 30 no-op actions). We train our models for 15 million timesteps for each game environment and score each game based on the average episode rewards obtained among the last 100
2hot-wire is triggered only when the agent is unable to receive meaningful rewards in an initial learning stage. The legend "vwr + hotWire" in Fig. 3 indicates that the mechanism is "enabled" but not "enforced".

6

Under review as a conference paper at ICLR 2019

Figure 4: Performance on the MuJoCo benchmark. A2MC is also competitive on MuJoCo continuous domain when compared to ACKTR. The shaded region denotes the standard deviation over 3 random seeds.

episodes as in Wu et al. (2017). The learning results on 12 Atari games are shown in Figure 3 where we also included an ablation experiment of A2MC without hot-wiring. We observe that on average A2MC improves upon ACKTR in terms of final performance under the same training budget. Our A2MC is able to consistently improve agent performance based on the proposed characterization of reward statistics, hence the agent is able to get out of local minima in less time (higher sample efficiency) compared to ACKTR. The complete learning results on all games are attached in the Appendix B.
We further expand the training budget and continue learning the games until 50 million timesteps as in Wu et al. (2017). As shown in Table 1, our A2MC model can solve games like "Boxing", "Freeway" and "Enduro" that are challenging for the baseline ACKTR model. For a full picture of model performance in Atari games, A2MC has a human-level performance rate of 74.5% (38 out of 51 games) in the Atari benchmarks, compared to 63.6% reached by ACKTR. Individual game scores for all the Atari games are reported in the Appendix B.

Table 1: Comparison of average episode rewards at the end of 50 million timesteps in Atari experiments. The reward scores and the first episodes reaching human-level performance Mnih et al. (2015) are reported as in Wu et al. (2017). A2MC is able to solve games that are challenging to ACKTR and also retain comparable performance in the rest of games.

ACKTR

A2MC

Domain

Human Level Rewards Episode

Rewards Episode

Asteroids Beamrider Boxing Breakout Double Dunk Enduro Freeway Pong Q-bert Robotank Seaquest Space Invaders Wizard of Wor

47388.7 5775.0 12.1 31.8 -16.4 860.5 29.6 9.3
13455.0 11.9
20182.0 1652.0 4756.5

34171.0 13581.4
1.5 735.7
-0.5 0.0 0.0 20.9 21500.3 16.5 1776.0 19723.0 702

N/A 3279 N/A 4097
742 N/A N/A 904 6422
N/A 14696 N/A

830232.5 13564.3 99.1 411.4 21.3 3492.2 32.7 19.4 25229.0 25.7 1798.6 11774.0 7471.0

11314 3012 158 3664 544
730 1058 804 7259 4158 N/A 11064 8119

6.2 CONTINUOUS CONTROL
For the evaluations on continuous control tasks simulated in MuJoCo environment, we first follow Wu et al. (2017) and tune a different set of hyper-parameters from Atari experiments. Specifically, all MuJoCo experiments are trained with a larger batch size of 2500. The results of eight MuJoCo environments trained for 1 million timesteps are shown in Figure 4. We observe that A2MC also

7

Under review as a conference paper at ICLR 2019

Figure 5: "Stress testing" ablation study on the MuJoCo continuous benchmark using hyperparameters tuned in Atari discrete control. Although this set of hyperparameters is suboptimal for the MuJoCo continuous control tasks, A2MC still obtain reasonable performance in the long run and it is less prone to overfitting.
performs well in all MuJoCo continuous control tasks. In particular, A2MC has obtained significant improvement as compared to ACKTR on the tasks of HalfCheetah, Swimmer and Walker2d (see Table 2).
To test the robustness of A2MC, we perform another set of evaluations on MuJoCo tasks by keeping an identical set of hyper-parameters used in the Atari experiments. Figure 5 shows this ablation result. We observe that even under sub-optimal hyper-parameters, our A2MC model can still learn to solve the MuJoCo control tasks in the long run. Moreover, it is less prone to overfitting when compared to ACKTR under such "stress testing". Additional hyper-parameter studies are shown in Appendix C.
We also evaluate a multi-critics variant of the proximal policy optimization (PPO) model on the MuJoCo tasks with our proposed long/short-term rewards. In particular, we observe that our proposed variability-weighted reward generalizes well with the vanilla PPO, and our multi-critics PPO variant (MC-PPO) shows more favorable performance, as shown in Table 2. Specifically, MC-PPO shows the best performance on Hopper and Walker-2d among all models under the 1-million timesteps training budget. Both of our multi-critics variants (A2MC and MC-PPO) have won 6 out of the 8 MuJoCo tasks with relative performance margins (percentages in parentheses) larger than 25% (see Table 2).

Table 2: Average episode rewards obtained among the last 10 episodes upon 1 million timesteps of training in MuJoCo experiments.

GAMES

ACKTR Our A2MC

PPO Our MC-PPO

Ant 1671.6

HalfCheetah

1676.2

Hopper

2259.1

InvertedDoublePendulum 6295.4

InvertedPendulum

1000.0

Reacher

-4.2

Swimmer

43.2

Walker2d

1090.8

2216.1 (32.5%) 2696.6 (60.8%) 2835.7 (25.5%) 7872.6 (25.0%) 957.2 (-4.2%)
-3.9 (0.4%) 187.4 (333.7%) 2405.9 (120.5%)

411.4 (± 107.7) 618.9 (50.4%) 1433.7 (± 83.9) 2473.4 (72.5%) 2055.8 (± 274.6) 3131.3 (52.3%) 4454.1 (± 1098.1) 7648.7 (71.7%) 839.7 (± 127.1) 777.4 (-7.4%) -5.47 (± 0.3) -10.3 (-8.5%)
79.1 (± 31.2) 102.9 (30.2%) 2300.8 (± 397.6) 3718.1 (61.6%)

Win -- Fair -- Lose N/A 6 -- 2 -- 0

N/A 6 -- 2 -- 0

7 CONCLUSION
In this work, we introduce an effective auxiliary reward signal to remedy the deficiencies of learning solely from the standard environment rewards. Our proposed characterization of past reward statistics results in improved learning and higher sample efficiencies for on-policy methods, especially in challenging tasks with sparse rewards. Experiments on both discrete tasks in Atari environment and MuJoCo continuous control tasks validate the effectiveness of utilizing the proposed long/short-term reward statistics for on-policy methods using multi-critic architectures. This suggests that expanding the form of reward feedbacks in reinforcement learning is a promising research direction.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048­5058, 2017.
Marc G Bellemare, Will Dabney, and Re´mi Munos. A distributional perspective on reinforcement learning. arXiv preprint arXiv:1707.06887, 2017.
Kevin Dowd. Adjusting for risk:: An improved sharpe ratio. International review of economics & finance, 9(3):209­222, 2000.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.
Junqi Jin, Chengru Song, Han Li, Kun Gai, Jun Wang, and Weinan Zhang. Real-time bidding with multi-agent reinforcement learning in display advertising. arXiv preprint arXiv:1802.09756, 2018.
George Konidaris and Andrew G Barto. Skill discovery in continuous reinforcement learning domains using skill chaining. In Advances in neural information processing systems, pp. 1015­1023, 2009.
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Julien Perolat, David Silver, Thore Graepel, et al. A unified game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4193­4206, 2017.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6382­6393, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928­1937, 2016.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. ICLR, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
William F Sharpe. The sharpe ratio. Journal of portfolio management, 21(1):49­58, 1994.
David Silver and Kamil Ciosek. Compositional planning using optimal option models. arXiv preprint arXiv:1206.6473, 2012.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.
Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4499­4509, 2017.
Harm Van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey Tsang. Hybrid reward architecture for reinforcement learning. In NIPS, pp. 5392­5402, 2017.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.
9

Under review as a conference paper at ICLR 2019 Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pp. 5­32. Springer, 1992. Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems, pp. 5285­5294, 2017.
10

Under review as a conference paper at ICLR 2019

APPENDIX

A EFFECTS OF FLIPPING

While introducing the variability-weighted reward, a flipping operation is conducted in the preprocessing of the reward sequence as formulated in Eq. (3). In Figure 6 and 7, we construct 4 reward sequences to show that the flipping operation can further penalize the oscillation in the recent past rewards while encourage recent stable rewards. (a1, a2, b1, b2) share the same value of immediate reward at t = 9 and thus the RH of all reward sequences are the same. Therefore,

Reward SReeqwueanrdceSer1quence r1 Reward SReeqwueanrdceSre2quence r2 Reward SReeqwueanrdceSre3quence r3 Reward SReqeuweanrdceSre4quence r4

2 2past rewardspast rewards 2 2past rewardspast rewards 2 2past rewards past rewards 2 2past rewards past rewards immediate reiwmamrdedrtia=te1.r0eward rt = 1.0 immediate reiwmamrdedrtia=te1.r0eward rt = 1.0 immediate reiwmamrderdti=at1e.r0eward rt = 1.0 immediate rewimamrderdti=at1e.0reward rt = 1.0

11

11

11

11

00

00

00

00

11

11

11

11

1 2 3 41 52 63 74 85 96 7 8 1 9 2 3 41 52 63 74 85 96 7 8 1 9 2 3 41 52 63 74 85 96 7 8 1 9 2 3 41 52 63 74 85 96 7 8 9
0.4 RHP=r8o.c001e.4ssReHdP=rw8o.c0/o1esFsliepdpiwn/go Flip0.p4inRgHP=r8o.c001e.4ssReHdP=rw8o./c0o1esFsliepdpiwng/o Flip0.p4inRgH=P8ro.00c1.e4ssReHd=P8rwo.0/c1eFslispepdinwg/ Flip0p.4inRgH=P8ro.00c1.e4ssReHd=P8rwo.0/c1eFlsispepdinwg/ Flipping

0.2 0.2

0.2 0.2

0.2 0.2

0.2 0.2

0.0 0.0

0.0 0.0

0.0 0.0

0.0 0.0

post-processepdoste-qpuroecnecseseRd sequence R post-processepdoste-qpuroecnecseseRd sequence R post-processepdoste-qpuroecnecseseRd sequence R post-processepdoset-qpuroecnecsesRed sequence R 0.2 ze0r.2o-variabilitzyeroe-fvearerinacbeiliRtyzerroefere0n.c2e Rzero ze0r.o2-variabilitzyeroe-fvearerinacbeiliRtyzerroefere0n.2ce Rzero ze0r.o2-variabilitzyereof-evraerniacbeiliRtyzerroefere0n.2ce Rzero ze0r.o2-variabilitzyereof-evraerniacbeilRitzyerroeference Rzero

0 1 2 30 41 52 63 74 85 96 7 80 91 2 30 41 52 63 74 85 96 7 80 91 2 30 41 52 63 74 85 96 7 80 91 2 30 41 52 63 74 85 96 7 8 9

Volatility VSotalatitsiltiitcysSotfatiRstics of VR olatility VSotalatitsiltiitcysSotfatiRstics of VRolatility VSotalatitsiltitcysSotfatiRstics of VRolatility VStoalatitsitlitcysSotfatiRstics of R

6

( R) = rvwr =

0H6.(710rv(w[rR=(m)aR=x) ]0H).(71=04.[0(m6aRx) ]

)

=

4.606rv(wrR=) =

0H6.(612rv(w[rR=(m)aR=x) ]0H).(61=24.[9(m7aRx) ]

)

=

4.697rv(wrR=) =

0H6.(816rv(w[rR(=m)aR=x) ]0H).(81=62.[0(m9aRx) ]

)

=

2.609rv(wrR=) =

0H6.(511rv(w[rR(=m)aR=x) ]0H).(5=115.[9(6maRx) ]

)

=

5.96

44

44

44

44

22

22

22

22

0

1 0 01

10

01 1 0 0 1 10

01 1 0 0 1 10

01 1 0 0 1 10

1

(a1) (a1)

(a2) (a2)

(b1) (b1)

(b2) (b2)

Figure 6: Calculation without flipping.

Figure 7: Calculation with flipping.

the variability-weighted reward only depends on the volatility statistics of R, i.e., how varied past rewards were.
Without flipping. In Figure 6, sequence (a1) and (a2) are mirror symmetrical to the y-axis, and the only difference between them is that the recent past rewards (t = 5, 6, 7, 8) of (a2) are more stable than (a1). Intuitively, we want to encourage stable past rewards like (a2) while penalizing oscillation in (a1). As presented in the third row of Figure 6, the rvwr difference of (a1) and (a2) is less than 1 without flipping in the pre-processing.
With flipping. In Figure 7, (b1, b2) exactly have the same reward sequence as (a1, a2), respectively. However, flipping is performed as a step of pre-processing, largely increasing the rvwr gap (from less than 1 to nearly 4) between the two constructed sequences. Comparing (b1, b2) with (a1, a2) ,
the post-processed sequences R (shown in green) become centrosymmetric to those without flipping. Specifically, the recent reward drops at t = 6, 7, 8 are reflected as high values at the beginning of
R as shown in (b1), while oscillations long ago are transformed into high values at the end of R as presented in (b2). When compared to the zero-variability reference (shown in orange), which is designed as an exponential function, the flipping leads to a higher variability for the former sequence while a lower variability for the latter one, enlarging the rvwr gap between those two sequences.

B COMPLETE RESULTS IN ATARI 2600 GAMES
We show the learning curves for 15 million timesteps on all Atari games in Figure 9 and in Table 3 we show the complete results of training til 50 million timesteps. report the mean episode reward as in Wu et al. (2017). Entries with  indicates approximated value as retrieved from learning figures published by Wu et al. (2017). Results from other models are taken from Wu et al. (2017) and Mnih

11

Under review as a conference paper at ICLR 2019

et al. (2015). We show that A2MC has reached a human-level performance rate of 74.5% (38 out of 51 games) as compared to 63.6% reached by ACKTR. The relative performance margin of A2MC as compared to ACKTR is also shown.

C HYPER-PARAMETER STUDIES

The proposed variability-weighted reward mechanism considers the volatility of rewards by keeping a T -step history of agent's performance. The hyper-parameter T = 20 is empirically chosen to be the same as the look-ahead parameter N in standard on-policy methods, so as to keep the same period (T = N = 20) in "T-step history" and "N-step forward". And max = 1 is chosen as the average of the observed volatility based on statistics in the T history rewards of the ACKTR models. As parameter choices could be vital, we perform an additional ablation study shown below. The result shows that the performance of A2MC is robust across different parameters of choice and is not too sensitive to changes on either of the hyper-params.

Games

ACKTR

A2MC w/

T=20 max=1

T=10 max=1

T=10 max=2

T=40 max=1

T=40 max=2

Boxing Jamesbond Wizard of Wor

1.23 409.50 744.50

99.19 94.76 98.51 99.18 98.07 453.50 438.50 470.00 442.25 457.75 5448.00 5601.00 5363.50 2528.50 3287.50

D EXTENSION TO MULTI-CRITIC PPO (MC-PPO)
The learning results of the proposed MC-PPO model on the MuJoCo tasks are shown in Figure 8. MC-PPO shows the best performance on Hopper and Walker-2d among all models under the 1-million timesteps training budget. Both of our multi-critics variants (A2MC and MC-PPO) have won 6 out of the 8 MuJoCo tasks with relative performance margins (percentages in parentheses) larger than 25%.

Figure 8: Performance on the MuJoCo continuous control benchmarks using PPO-based methods. Our proposed long/short-term reward characterization can be extended to the PPO method, i.e., the proposed multi-critic variant of PPO (MC-PPO). The shaded region denotes the standard deviation over 3 random seeds.
E ALGORITHM
The learning algorithm of A2MC is shown in Algorithm 1.
12

Under review as a conference paper at ICLR 2019

Table 3: Raw scores across all games, starting with 30 no-op actions. Scores are reported by averaging

the last 500 episodes upon 50 million timesteps of training as in Wu et al. (2017). A relative margin

comparing A2MC to ACKTR is shown. Scores from other models are taken from Wu et al. (2017)

and Mnih et al. (2015).

GAME

Human DQN DDQN Prior. Duel ACKTR Our A2MC (Margin)

Alien

7127.7 1620 3747.7

3941 3197.1 2986.3 -6.6%

Amidar

1719.5

978 1793.3 2296.8 1059.4 2040.1 92.6%

Assault

742.0 4280.4 5393.2 11477 10777.7 9892.4 -8.2%

Asterix

8503.3 4359 17356.5 375080 31583.0 32671.0 3.4%

Asteroids

47388.7 1364.5 734.7 1192.7 34171.6 828931.6 2325.8%

Atlantis

29028.1 279987 106056 395762 3433182.0 2886274.0 -15.9%

Bankheist

753.1

455 1030.6 1503.1 1289.7 1290.6 0.1%

Battlezone

37187.5 29900 31700 35520 8910.0 10570.0 18.6%

Beamrider

16926.5 8627.5 13772.8 30276.5 13581.4 13715.6 1.0%

Berzerk

2630.4 585.6 1225.4

3409 927.2

974.0 5.0%

Bowling

160.7 50.4 68.1

46.7 24.3

31.6 30.0%

Boxing

12.1 88 91.6 98.9

1.5 93.5 6344.8%

Breakout

30.5 385.5 418.5

366 735.7

420.6 -42.8%

Centipede

12017.0 4657.7 5409.4 7687.5 7125.3 12096.5 69.8%

Choppercommand 9882.0 N/A N/A

N/A 8000 12149.0 42.5%

Crazyclimber 35829.4 110763 117282 162224 150444.0 152439.0 1.3%

Demonattack

1971.0 12149.4 58044.2 72878.6 274176.7 361807.1 32.0%

Doubledunk

-16.4 -6.6 -5.5

-12.5

-0.5

20.6 3907.5%

Enduro

860.5

729 1211.8 2306.4

0.0 3550.6 %

Fishingderby

-38.7 -4.9 15.5

41.3 33.7

38.4 13.9%

Freeway

29.6 30.8 33.3

33 0.0 32.7 %

Frostbite

4335.0 N/A N/A

N/A 280

293.7 5.1%

Gopher

2412.5 8777.4 14840.8 104368.2 47730.8 86101.4 80.4%

Gravitar

2672.0 N/A N/A

N/A 300

995.0 -2.9%

Icehockey

0.9 -1.9 -2.7

-0.4 -4.2

-2.1 16.3%

Jamesbond

302.8 768.5 1358

812 490.0

545.0 11.2%

Kangaroo

3035.0 7259 12992

1792 3150.0 11269.0 257.7%

Krull

2665.5 8422.3 7920.5 10374.4 9686.9 10245.4 5.8%

Kungfumaster 22736.3 26059 29710 48375 34954.0 39773.0 13.8%

Mspacman

15693.0 N/A N/A

N/A 3500 5006.1 34.5%

Namethisgame 4076.0 N/A N/A

N/A 12500 12569.9 0.6%

Phoenix

7242.6 8485.2 12252.5 70324.3 133433.7 221288.3 65.8%

Pitfall

6463.7 -286.1 -29.9

0 -1.1

-2.5 -0.3%

Pong

14.6 20.9 21 20.9 20.9 19.7 -5.9%

Privateeye

69571.0 N/A N/A

N/A 560

507.0 -9.5%

Qbert

13455.0 13117.3 15088.5 18760.3 23151.5 24075.8 4.0%

Riverraid

17118.0 7377.6 14884.5 20607.6 17762.8 18671.9 5.1%

Roadrunner

7845.0 39544 44127 62151 53446.0 50071.0 -6.3%

Robotank

11.9 63.9 65.1

27.5 16.5

26.5 60.5%

Seaquest

42054.7 5860.6 16452.7

931.6 1776.0 1805.6 1.7%

Solaris

12326.7 3482.8 3067.8

133.4 2368.6 2277.2 -3.9%

Spaceinvaders

1668.7 1692.3 2525.5 15311.5 19723.0 13544.2 -31.3%

Stargunner

10250.0 54282 60142 125117 82920.0 89616.0 8.1%

Tennis

-8.9 N/A N/A

N/A -12

-4.7 20.4%

Timepilot

5229.2 4870 8339

7553 22286.0 21992.0 -1.3%

Tutankham

167.6 68.1 218.4

245.9 314.3

193.7 -38.4%

Upndown

11693.2 9989.9 22972.2 33879.1 436665.8 563659.3 29.1%

Videopinball

17667.9 196760.4 309941.9 479197 100496.0 127452.4 26.8%

Wizardofwor

4756.5 2704 7492

12352

702.0 7864.0 1020.2%

YarsRevenge

54576.9 18098.9 11712.6 69618.1 125169.0 143141.5 14.4%

Zaxxon

9173.3 5363 10163 13886 17448.0 19365.0 11.0%

Human-level (Win / Total)

N/A

21 / 44 31 / 44 (47.7%) (70.4 %)

34 / 44 28 / 44 (77.3 %) (63.6 %)

38 / 51 (74.5 %)

13

Under review as a conference paper at ICLR 2019
Algorithm 1 Advantage Actor Multi-Critic Learning (A2MC) 1: Initialize parameters: a, vj , j  {short-term, long-term} 2: Initialize look-ahead steps: N , step counter: T = 0, maximum step: Tmax 3: Initialize hot-wire probability: 4: Initialize environment: Env 5: Initialize reward history: r 6: repeat 7: Reset gradients: d  0 and dvj  0, j  {short-term, long-term} 8: Get state: st  Env 9: f lag = 1, arand is uniformly sampled in action space with probability , otherwise f lag = 0 10: for t = 0 : N - 1 do 11: Perform at according to policy (at|st; a) if not f lag else at = arand 12: Received reward rt and new state st+1, append rt to r 13: Calculate rtvwr from r based on Eq. (2-7) 14: T  T + 1 15: end for 16: Rshort-term = V (sN ; vshort-term) 17: Rlong-term = V (sN ; vlong-term) 18: for i = N - 1 to 0 step -1 do 19: Rshort-term  ri + Rshort-term 20: Rlong-term  rivwr + Rlong-term 21: Advantange gradients wrt a : da  da + a log (ai|si; a) j(Rj - V (si; vj )) 22: for j  {short-term, long-term} do 23: Accumulate gradients wrt vj : dvj  dvj + (Rj - V (si; vj ))2/vj 24: end for 25: end for 26: until T  Tmax
14

Under review as a conference paper at ICLR 2019
Figure 9: Performance of A2MC on Atari games. The number in figure legend shows the average reward among the last 100 episodes upon 15 million timesteps and the percentage shows the performance margin as compared to ACKTR. The shaded region denotes the standard deviation over 2 random seeds.
15

