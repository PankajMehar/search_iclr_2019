Under review as a conference paper at ICLR 2019
REMEMBER AND FORGET FOR EXPERIENCE REPLAY
Anonymous authors Paper under double-blind review
ABSTRACT
Experience replay (ER) is crucial for attaining high data-efficiency in off-policy reinforcement learning (RL). ER entails the recall of experiences obtained in past iterations to compute gradient estimates for the current policy. However, the accuracy of such updates may deteriorate when the policy diverges from past behaviors, possibly undermining the effectiveness of ER. Previous off-policy RL algorithms mitigated this issue by tuning their hyper-parameters in order to abate policy changes. We propose a method for ER that relies on systematically Remembering and Forgetting past behaviors (ReF-ER). ReF-ER forgets experiences that would be too unlikely with the current policy and constrains policy changes within a trust region of the behaviors in the replay memory. We couple ReF-ER with Q-learning, deterministic policy gradient and off-policy gradient methods and we show that ReF-ER reliably improves the performance of continuous-action off-policy RL. We complement ReF-ER with a novel off-policy actor-critic algorithm (RACER) for continuous-action control. RACER employs a computationally efficient closedform approximation of the action values and is shown to be highly competitive with state-of-the-art algorithms on benchmark problems, while being robust to large hyper-parameter variations.
1 INTRODUCTION
Deep Reinforcement Learning (RL) has an ever increasing number of success stories ranging from realistic simulated environments (Schulman et al., 2015; Mnih et al., 2016), robotics (Levine et al., 2016) and games (Mnih et al., 2015; Silver et al., 2016). Experience Replay (ER) (Lin, 1992) enhances deep RL algorithms by using information collected in past policy (µ) iterations to compute updates for the current policy (). ER has become one of the mainstay techniques to improve the sample-efficiency of off-policy RL. Sampling from a Replay Memory (RM) stabilizes stochastic gradient descent (SGD) by disrupting temporal correlations and extracts information from useful experiences over multiple updates (Schaul et al., 2015b). However, when  is parameterized by a deep Neural Network (NN), SGD updates may result in significant changes to the policy, thereby shifting the distribution of states observed from the environment. In this case sampling the RM for further updates may lead to incorrect gradient estimates, therefore RL methods must account for and limit the dissimilarity between  and past behaviors in the RM. Previous works employed trust region methods to bound policy updates (Schulman et al., 2015; Wang et al., 2017). Despite several successes, deep RL algorithms are known to suffer from instabilities and exhibit high-variance of outcomes (Islam et al., 2017; Henderson et al., 2017), especially continuous-action methods employing the stochastic (Sutton et al., 2000) or deterministic (Silver et al., 2014) Policy Gradients (PG or DPG).
In this work we redesign ER in order to control the distance between the behaviors µ used to compute the update and the current policy . More specifically, we classify experiences either as "near-policy" or "far-policy", depending on the ratio  of probabilities of selecting the associated action with  and that with µ. The weight  appears in many estimators that are used with ER such as the off-policy policy gradients (off-PG) (Degris et al., 2012) and the off-policy return-based evaluation algorithm Retrace (Munos et al., 2016). From this classification, we introduce measures to limit the fraction of "far-policy" samples in the RM, as well as computing gradient estimates only from "near-policy" experiences. Furthermore, these hyper-parameters can be gradually annealed during training to obtain increasingly accurate updates from nearly on-policy experiences. Remember and Forget Experience Replay (ReF-ER) is a simple algorithm that can be applied to virtually any off-policy RL algorithm with parameterized policies. We show that ReF-ER allows better stability and performance than
1

Under review as a conference paper at ICLR 2019

conventional ER in all three main classes of continuous-actions off-policy RL algorithms: methods based on the DPG (ie. DDPG (Lillicrap et al., 2016)), methods based on Q-learning (ie. NAF (Gu et al., 2016)), and with off-PG (Degris et al., 2012; Wang et al., 2017).
In recent years, there has been a growing interest in coupling RL with high-fidelity physics simulations (Reddy et al., 2016; Novati et al., 2017; Colabrese et al., 2017; Verma et al., 2018). The computational cost of these simulations calls for data-efficient RL methods that are reliable and do not require problem-specific tweaks to the hyper-parameters. Moreover, while on-policy training of simple policy architectures has been shown to be sufficient in some benchmark environments (Rajeswaran et al., 2017), agents aiming to solve complex problems with highly non-linear dynamics might require deep or recurrent models that can be trained more efficiently with off-policy methods. We address these issues by introducing a simple and computationally efficient off-policy actor-critic architecture (RACER) for continuous-action control problems. We systematically analyze a wide range of hyper-parameters on the OpenAI Gym (Brockman et al., 2016) robotic tasks, and show that RACER combined with ReF-ER reliably obtains results that are competitive with the state-of-the-art.

2 PRELIMINARIES

Consider the sequential decision process of an agent aiming to optimize its interaction with the environment. At each step t, the agent observes its state st  RdS , performs an action by sampling a policy at  µ(a|st)  RdA , and transitions to a new state st+1  D(s|at, st) with reward rt+1  R.
These interactions {st, at, rt, µt} are stored in a RM, which constitutes the data used by off-policy RL to train the parametric policy w(a|s). The importance weight t = w(at|st)/µt is the ratio between the probability of selecting at with the current w and with the behavior µt, which gradually becomes dissimilar from w as the latter is trained. The on-policy state-action value Q(s, a) measures the
expected returns from (s, a) following the policy w:

Q(s, a) = E
stD, atw


trt+1 s0 = s, a0 = a
t=0

(1)

Here  is a discount factor. The value of state s is the on-policy expectation: V (s) = Ea [Q(s, a)] and the action advantage is A(s, a) = Q(s, a)-V (s), such that Ea [A(s, a)] :=0. We consider algorithms that train parametric approximators Qw from off-policy data. The Q-learning target is q^t = rt+1 +  Ea  Qw(st+1, a ). The Retrace algorithm (Munos
et al., 2016) includes all the rewards obtained by the behavior µt in the value estimation:

Q^rtet = rt+1 + V w(st+1) +  min{1, t+1} Q^tre+t 1 - Qw(st+1, at+1)

(2)

The off-PG (Degris et al., 2012) can be used to update w by ER: g^toff-PG(w) = tA^t w log w(at|st). Here, A^t is an estimator for the on-policy advantage, such as A^rtet := Q^tret - V w(st).

3 REMEMBER AND FORGET EXPERIENCE REPLAY

In off-policy RL it is common to maximize on-policy returns averaged over the distribution of states in a RM (Degris et al., 2012). However, as w gradually shifts away from previous behaviors µt, the distribution of states in the RM is increasingly dissimilar from the on-policy distribution, and trying to increase an off-policy performance metric may not improve on-policy outcomes. This issue may be compounded with algorithm-specific concerns. For example, in ACER (Wang et al., 2017) the dissimilarity between µt and w may cause vanishing or diverging importance weights t, thereby increasing the variance of the off-PG and deteriorating the convergence speed of Retrace by inducing "trace-cutting" (Munos et al., 2016). As a remedy, ACER tunes the learning rate and uses a target network (Mnih et al., 2015), updated as a delayed copy of the policy network, to constrain policy updates. Target networks are also employed in DDPG (Lillicrap et al., 2016) in order to slow down the feedback loop between value-network and policy-network optimizations. This feedback loop causes overestimated action values that can only be corrected by acquiring new on-policy samples. When using non-linear networks there is no guarantee that small weight changes correspond to small changes in the output and recent works (Henderson et al., 2017) have shown the often opaque variability of outcomes of continuous-action RL algorithms depending on hyper-parameters.

2

Under review as a conference paper at ICLR 2019

Here we propose a set of three simple rules, collectively referred to as Remember and Forget ER (ReF-ER), to directly control the degree of "off-policyness" of the samples in the RM:

1. Updates are computed from mini-batches drawn uniformly from the RM. We compute the importance weight t of each sample and classify it either as "near-policy" if 1/cmax<t<cmax with cmax>1 , or "far-policy" otherwise. We will repeatedly use estimates of the number of far-policy samples, therefore we store for each element of the RM its most recent t.
2. When acquiring samples from the environment, older episodes with the highest fraction of far-policy samples are removed until the number nobs of samples in the RM is at most N .
3. Policy updates are penalized in order to attract the current policy w towards µt:

g^tReF-ER(w) =

 g^t(w) -(1-)DKL [µt w(·|st)] -(1-)DKL [µt w(·|st)]

if 1/cmax<t<cmax otherwise

(3)

Here DKL is the Kullback­Leibler divergence and the coefficient   [0, 1] is updated after each gradient step such that a fixed fraction D  (0, 1) of the RM are far-policy samples:



(1 - ) (1 - ) + ,

if nfar/nobs > D otherwise

(4)

where  is the learning rate and nfar is the number of far-policy samples. Note that iteratively updating  with Eq. 4 has fixed points in 0 for nfar/nobs>D and in 1 otherwise.

For cmax1 and D0, ReF-ER becomes asymptotically equivalent to computing gradient estimates from on-policy experiences. ReF-ER reduces the sensitivity on the network architecture and training hyper-parameters, because the rate at which the policy is allowed to change is directly linked to its similarity with the behaviors in the training set.

4 RACER: REGULARIZED ACTOR-CRITIC WITH EXPERIENCE REPLAY
Here we introduce RACER: an actor-critic architecture for continuous-action problems, such as the MuJoCo (Todorov et al., 2012) control tasks of OpenAI Gym (Brockman et al., 2016) and the DeepMind Control Suite (Tassa et al., 2018). The network architecture consist of a single Multilayer Perceptron (MLP) with weights w which receives the state s and outputs as shown in figure 1b. From the final layer, the first 2dA outputs are taken as the mean m and diagonal covariance  of the Gaussian policy w(a|s). One output is taken as the estimated state value V w(s), and the remaining outputs are the parameters of a closed-form approximator for the action advantage Aw. RACER can be extended for problems where the state is received as a visual feed (by inserting convolutional layers), or where the state is only partially observable by the agent (by substituting the MLP with a recurrent model). However, these extensions are outside the scope of this paper.
The parameters w are updated from mini-batches of off-policy time steps with ReF-ER. A separate gradient is defined for each component of the actor-critic framework, as sketched in figure 1b. The

CPU i st+1 rt+1

Learner process Spawn independent tasks

Environment Data acquisition Gradient estimation

at ~ w

Gradients summation Weight update

(a)

CPU j (a|s) g A(a,s) gA V(s) gV
KV

Reduction

Neural Network (w) Input state s
(b)

Figure 1: (a) Diagram of asynchronous ER-based RL algorithms. (b) Neural network architecture employed by RACER. Blue arrows connect each output with the elements of the actor-critic framework for which it is used. Red arrows represent the flow of the error signals.

3

Under review as a conference paper at ICLR 2019

policy statistics m(s) and (s) are updated with the off-PG (Degris et al., 2012):

g^t(w) = tA^rtet{m, } log w(at | st)

(5)

We use Retrace (Eq. 2) to estimate the on-policy advantage. From the "variance truncation and bias
correction (TBC) method (Wang et al., 2017) and Retrace, we obtain an estimator for the on-policy state value V^ttbc = V w(st)+ min{1, t}(Q^tret-Qw(st, at)), used as a target for V w:

^gVt (w) = V^ tbc(s)-V w(st) = min {1, t} Q^tret-Qw(st, at)

(6)

Rather than having a separate MLP with inputs (s, a) to parameterize Qw (as in ACER or DDPG),
whose expected value under the policy would be computationally demanding to compute, we employ a closed-form equation for Aw inspired by NAF (Gu et al., 2016). The network outputs the coefficients of a concave function f w(s, a) which is chosen such that its maximum coincides with the mean of the policy m(s), and such that it is possible to derive analytical expectations for a  w. In App. B we explore multiple choices of f w(s, a). For all results in the main text of the manuscript we used:

f w(s, a) = K(s) exp

-

1 2

a+

L-+1(s)

a+

-

1 2 a-

L--1(s)

a-

(7)

Here a-= min [a-m(s), 0] and a+= max [a-m(s), 0] (both element-wise operations). This pa-
rameterization requires one MLP output for K(s) and dA outputs for each diagonal matrix L+ and
L-. In total, given a state s, the MLP computes m, , V , K, L+ and L-. From Eq.7, for any action a the advantage is uniquely defined: Aw(s, a):= f w(s, a)-Ea  [f w(s, a )]. Like the exact on-policy advantage A, Aw has by design expectation zero under the policy. The parameterization coefficients K and L are updated to minimize the L2 error from A^tret:

g^tA(w) = t A^tret-Aw(st, at) {K, L}Aw(st, at)

(8)

Here, t reduces the weight of estimation errors for unlikely actions, where Aw is expected to be less accurate. To ensure that , K, L+ and L- are positive definite, the respective outputs are mapped onto R+ by a Softplus rectifier. The analytical derivation of Ea  [f w(s, a )] can be found in App. A. Collectively, g^t(w), ^gtV (w), and g^tA(w) form a vector g^tAC(w) with the same size as the MLP output. g^tAC(w) is weighted with the ReF-ER penalization (Eq. 3) and then back-propagated to the MLP.
In order to estimate Q^rtet for a sampled time step t, Retrace (Eq. 2) requires V w and Qw for all future steps in sample t's episode. These are naturally computed when training from batches of episodes (as in ACER) rather than time steps. However, the information contained in consecutive steps is highly correlated, worsening the quality of the gradient estimate, and episodes can be composed of thousands of time step, increasing the computational cost. In order to efficiently train from uncorrelated time steps, RACER stores for each sample in the RM the most recently computed estimations of V w(st), Aw(st, at), t and Qtret. When a batch of time steps is sampled, the stored Qrtet are used to compute the gradient update. At the same time, the stored V w(st), Aw(st, at) and t are updated with the current NN outputs and used to correct Qret for all prior time-steps in the respective episodes with Eq. 2. A description of all remaining implementation details is provided in App. D.

5 RELATED WORK
The rules that determine which samples are kept in the RM and how they are used for training can be designed to address several objectives. Prioritized Experience Replay (Schaul et al., 2015b) (PER) improves the performance of DQN (Mnih et al., 2015) (on discrete-action problems) by biasing sampling in favor of observations associated with greater temporal-difference (TD) errors. TD errors may signal rare events that would convey useful information to the learner, especially if the rewards are sparse. We will compare PER to ER and ReF-ER on continuous-action problems. ER can alternatively be used to train off-policy learners on auxiliary tasks (Schaul et al., 2015a; Jaderberg et al., 2017; Andrychowicz et al., 2017) to shape the network features or to guide exploration. Moreover, ER can be designed to maximize the diversity of the RM (de Bruin et al., 2015).
ReF-ER is inspired by the techniques developed for on-policy RL to bound policy updates of PPO (Schulman et al., 2017). Rule 1 of ReF-ER is similar to the clipped objective function of PPO

4

Under review as a conference paper at ICLR 2019

(gradients are zero if  lies outside of some range). However, Rule 1 is not affected by the sign of the advantage estimate and clips both policy and value gradients. Like Rule 3, one variant of PPO penalizes DKL(µt||w) (also Schulman et al. (2015) and Wang et al. (2017) employ trust-region schemes in the on- and off-policy setting respectively). While PPO picks one of the two techniques, in ReF-ER Rules 1 and 3 complement each other and can be applied to most off-policy RL methods with parametric policies. In the ER setting, samples remain in the RM over many iterations. If only Rule 1 is included, any PG could push the sample's  outside of the clipping range without means to recover, leading to zero-valued gradients. Including only Rule 3 would not reduce the number of hyper-parameters (a target DKL would be needed), and would not prevent unbounded .
The first method to combine ER and PG (Sutton et al., 2000) was the Off-Policy Actor Critic (Degris et al., 2012). Further contributions were introduced with ACER (Wang et al., 2017), such as extension to NN and estimation of on-policy returns with Retrace (Munos et al., 2016). In the continuous-action domain, the proposed RACER offers several advantages over ACER: (1) reduced complexity by employing a single NN and closed-form Aw (continuous-ACER uses 9 NN evaluations per gradient). (2) relies on ReF-ER rather than constraining policy updates around a target network. (3) samples uncorrelated time steps rather than episodes (which may consist of thousands of steps). Because of points (1) and (3) RACER is two orders of magnitude faster than ACER.

6 RESULTS
In this section we couple ReF-ER, conventional ER and PER with one method from each of the three main classes of continuous-action RL algorithms: DDPG (based on DPG), NAF (based on Q-learning), and RACER (off-PG). We measure the performance of each combination of algorithms by plotting the mean cumulative reward R = t rt. Each plot tracks the average R among all episodes entering the RM within intervals of 2·105 time steps and averaged again among five differently seeded training trials. Contours of the 20th to 80th percentiles of R obtained with DDPG and RACER can be found in the Appendix. The code to reproduce all present results is available on GitHub.1

6.1 RESULTS FOR DDPG

DDPG (Lillicrap et al., 2016) trains two networks by ER. The value network outputs Qw(s, a) and is trained to minimize the L2 distance from the Q-learning target (Sec. 2). The policy network outputs a deterministic policy mw (st+1) and is trained with the DPG (Silver et al., 2014):

g^tDPG(w ) = w mw (s) aQw(st, a)|a=mw (st)

(9)

Noise is added to the deterministic policy w =mw +N (0, 2I) for exploration. We consider two variants of DDPG: one with mw bounded to the unit box [-1, 1]dA (as in the original) and one without bounds (referred to as u-DDPG, implementation details can be found in App. D). Bounding the policy may lead to lower returns in the OpenAI MuJoCo benchmarks, which are defined for unbounded actions. We note that, without measures to constrain policy updates or without careful tuning of the hyper-parameters (we found the critic's weight decay and temporally-correlated action noise to be necessary), u-DDPG is unstable. The returns for u-DDPG can fall to large negative values, especially in tasks that include a control-cost penalty in the reward such as Ant. This behavior is explained by the value network not having learned local maxima with respect to the action (Silver et al., 2014).

By replacing ER with ReF-ER we can stabilize u-DDPG and greatly improve its performance,
especially for tasks with complex dynamics such as Humanoid or Ant. The hyper-parameter cmax determines how much the policy is allowed to change from the behaviors in the RM. By annealing
cmax we can allow faster improvements at the beginning of training, when an inaccurate policy gradient might be sufficient to estimate a good direction for the update. Conversely, during the later
stages of training, precise updates can be computed from almost on-policy samples. We anneal cmax and the learning rate according to:

cmax(k) = 1 + C/(1 + 5e-7 · k), (k) = /(1 + 5e-7 · k)

(10)

1 The repository is hidden to maintain anonymity during the review process.

5

Under review as a conference paper at ICLR 2019

cumulative reward

105 HumanoidStandup-v2 2.2
2 1.8 1.6

4000 3000 2000

1.4 1000

1.2
8000 7000 6000 5000 4000 3000

HalfCheetah-v2 2468
106 time steps

3000 2500 2000 1500 1000
500 10

Humanoid-v2 Hopper-v2
2468 106 time steps

3000 2500 2000 1500 1000
500 0
350
300
250
200
150
10 100

Ant-v2 Swimmer-v2 2468 106 time steps

3000 2500 2000 1500 1000
500 0
-8 -10 -12 -14 -16 -18 10 -20

Walker2d-v2

Reacher-v2

2468 106 time steps

10

cumulative reward

Figure 2: Cumulative rewards on OpenAI MuJoCo tasks for DDPG (green line), DDPG with rankbased PER (gray line), u-DDPG with regular ER (blue), and ReF-ER with C = 8 (purple), C = 4 (red), C = 2 (orange). Implementation details in App. D.

< DKL( || ) >

Humanoid-v2 106

Ant-v2

Walker2d-v2

HalfCheetah-v2

Swimmer-v2

104

102

100

10-2 2 4 6 8 10 2 4 6 8 10 2 4 6 8 10 2 4 6 8 10 2 4 6 8 10

106 time steps

106 time steps

106 time steps

106 time steps

106 time steps

Figure 3: Average Kullback-Leibler divergence between the policy w = mw +N (0, 2I) and the
behaviors µt in the RM during training for DDPG (green line), u-DDPG with regular ER (blue), and ReF-ER with C = 8 (purple), C = 4 (red), C = 2 (orange).

Here k is the gradient step index,  is the initial learning rate (10-4 and 10-5 for the value and

policy networks respectively, as when using regular ER). We found that annealing  worsened the

instability of u-DDPG with regular ER. Lower values of C reduce the speed of policy improvements,

but after 107 time steps C = 2 achieves the best performance in most tasks. In Fig. 3 we report for

a subset of problems the average DKL(µt||w). DKL decreases for lower values of C and further

decreases during training due to the annealing process. With regular ER, even after lowering  by one

order of magnitude from that of the original paper, the distance between  and µ may span the entire

action space. In fact, in most tasks shown in Fig. 3 the DKL(µt||w) of DDPG is of similar order of

magnitude

as

its

maximum

dA 2

(2/)2

(for

example,

since



=

0.2,

the

maximum

DKL

is

850

for

Humanoid and 300 for Walker2d and it oscillates during training around 100 and 50 respectively).

ReF-ER maintains a RM of mostly near-policy samples, providing the value-network with multiple examples of trajectories that are possible with the current policy. This focuses the predictive capabilities of the value-network, enabling it to extrapolate the effect of a marginal change of action on the expected returns, and therefore increasing the accuracy of the DPG. Any misstep of the DPG is weighted with a penalization term that attracts the policy towards past behaviors. This allows time for the learner to gather experiences with the new policy, improve the value-network, and correct the misstep. This reasoning is almost diametrically opposed to that behind PER. In PER observations that are associated with larger TD errors are sampled more frequently. In the continuous-action setting, however, TD errors may be result from actions that are farther from the current policy. Therefore, precisely estimating their value might not help the value network in yielding an accurate estimate of the DPG. We obtained better results with the rank-based variant of PER, which has similar performance to that of DDPG with ER. The main benefits over ReF-ER arise in tasks that require more exploration, such as Swimmer and HumanoidStandup.

6

Under review as a conference paper at ICLR 2019

cumulative reward

105 HumanoidStandup-v2 1.4 1.3 1.2 1.1
1 0.9 0.8

1600 1400 1200 1000
800 600 400

6000 5000 4000

HalfCheetah-v2

2000 1500

3000

1000

2000 1000
0

500 0

-1000

2468 106 time steps

10 -500

Humanoid-v2 Hopper-v2

2500 2000 1500 1000
500 0
350

300

250

200

2468 106 time steps

10 150

Ant-v2

1500

1000

500

0

Swimmer-v2 2468
106 time steps

-500 -8
-10 -12 -14 -16 -18 10 -20

Walker2d-v2

Reacher-v2

2468 106 time steps

10

cumulative reward

Figure 4: Cumulative rewards on the OpenAI MuJoCo tasks for NAF (blue line), NAF with rank-based PER (gray line), NAF with ReF-ER and C = 8 (purple), C = 4 (red), C = 2 (orange).

6.2 RESULTS FOR NAF

Normalized Advantage Functions (NAF) is the state-of-the-art of Q-learning based algorithms for continuous-action problems. It employs a quadratic-form approximation of the advantage Aw,
analogous to the one employed by RACER:

ANw AF(s, a) = - [a - mw(s)] LwQ(s) LQw (s) [a - mw(s)]

(11)

NAF trains Aw with the Q-learning target (Sec. 2) and uses the location mw of its maximum as the mean of the policy, with added Gaussian noise for exploration. When the actual A is not well
approximated by a quadratic (e.g. when the return landscape is multi-modal), NAF may fail to choose good actions. In contrast, RACER updates w with the off-PG, which is independent of the error in approximating A(st, at) (but depends on the error at t+1). Figure 4 shows how NAF is affected by the choice of ER algorithm. While Q-learning based methods are thought to be less
sensitive than PG-based methods to the dissimilarity between policy and stored behaviors owing to the bootstrapped Q-learning target, NAF benefits from REF-ER. This is because A is likely to be
approximated well by a quadratic in a small neighborhood near its local maxima. ReF-ER constrains
learning from actions within this neighborhood and prevents large TD errors from disrupting the locally-accurate approximation of Aw. This intuition is supported by observing that rank-based PER
(the better performing variant of PER also in this case), often worsens the performance of NAF. PER
aims at biasing sampling in favor of larger TD errors, which are more likely to be farther from m(s) (note that fNwAF is unbounded), and their accurate prediction might not help the learner in fine-tuning the policy by improving a local approximation of the advantage.

6.3 RESULTS FOR RACER
In this section we present state-of-the-art results in the OpenAI MuJoCo tasks obtained by coupling RACER with ReF-ER. We omit results from coupling RACER with ER or PER as it yields large negative R values that would not fit the graphs. In fact, without introducing measures to constrain policy updates, the unbounded importance weights t cause Eq. 5 to diverge, disrupting any prior learning progress. We compare RACER with PPO, an algorithm that owing to its simplicity and good performance on MuJoCo tasks is often used as baseline.
The two hyper-parameters that most strongly affect the performance of RACER are the RM size N and cmax, annealed with Eq. 10 (for a discussion of the other parameters see App. B). RACER uses the Retrace estimator Qret which is expected to converge to the on-policy Q (Munos et al., 2016). Because w is gradually changing during training, it is crucial to maximize the convergence speed of Retrace to obtain accurate estimates of the off-PG. Large values of cmax increase the variance of the policy gradient and increase the amount of "trace-cutting" as the importance weights are allowed to diverge from 1. The cumulative rewards reported in Fig. 5 show that "stable" tasks, where the

7

Under review as a conference paper at ICLR 2019

cumulative reward

6000 5000 4000 3000 2000

2

6000

5000

4000

3000

2000

1000

2

N:2 20 C:2

Humanoid-v2 46 Ant-v2 46 106 time steps
N:2 20 C:4

6000 5000 4000

Walker2d-v2

300 200

3000

100

2000 2 4 6 8 10

2

10000
8 10 8000 6000

HalfCheetah-v2

4000 3500 3000 2500

4000 105

2468 HumanoidStandup-v2

2000 10 1500

2

2

1.5

8 10 N:2 20 C:8

1 2
N:2 19 C:2

-5

-10

-15

46 106 time steps
N:2 19 C:4

8 10 -20

2

N:2 19 C:8

N:2 18 C:2

Swimmer-v2

46 Hopper-v2

8

10

46 Reacher-v2

8

10

46 106 time steps
N:2 18 C:4

8 10 N:2 18 C:8

cumulative reward

Figure 5: Average cumulative rewards on MuJoCo OpenAI Gym tasks obtained with PPO (dashed black lines) and with RACER by independently varying the two main hyper-parameters of ReF-ER: the RM size N and C (colored lines).

agent's success is less predicated on avoiding mistakes that would cause it to trip, are more tolerant to high values of cmax (e.g. HalfCheetah). In most other tasks, especially those that require precise controls (e.g. Walker), the best results are obtained for values of cmax that strike a balance between strict penalty terms, which slow down policy improvements, and high-variance gradient estimates. The RM size N has a parallel effect. A small RM may not contain enough diversity of samples for the learner to accurately estimate the gradients. Conversely, a large RM is composed of episodes obtained with increasingly older versions of w. In this case, the penalty terms required to preserve a sufficient fraction of near-policy samples may prevent the policy from improving. A discussion of all the secondary hyper-parameters and the choice of function parameterizing Aw can be found in App. B. For most combinations of hyper-parameters and tasks presented in this section, RACER outperforms the best result from DDPG (Sec. 6.1), PPO, and is competitive with the best results found in the published literature, which to our knowledge were achieved by the on-policy algorithms TRPO (Schulman et al., 2015) and Policy Search with Natural Gradient (Rajeswaran et al., 2017).
7 CONCLUSION
We present a novel Experience Replay algorithm (ReF-ER) that is shown to enhance the stability and performance of off-policy RL. ReF-ER regulates the pace at which the policy w is allowed to deviate from previous behaviors µ, and how past experiences are used to update w. The relevance of past behaviors to the current policy is quantified by the importance weight =(a|s)/µ(a|s). We characterize samples as either "near-policy" or "far-policy" by the deviation of  from unity. The proposed ReF-ER algorithm consists of: 1) Computing gradient estimates only from near-policy experiences. 2) Forgetting far-policy samples when new observations are obtained from the environment. 3) "Remembering" past policies through a penalty term that reduces the Kullback­Leibler divergence between w and µ. This allows time for the learner to gather experiences with the new policy, improve the value estimators, and increase the accuracy of the next steps. We show that ReF-ER can be applied to most off-policy RL algorithm with parametric policies by coupling it with off-policy PG, deterministic PG and Q-learning methods for continuous-action problems. The present results provide evidence that most off-policy RL methods may benefit from being trained with a replay memory composed of near-policy experiences. Finally, ReF-ER is combined with a novel offpolicy actor critic algorithm (RACER) with simplified closed-form parameterization of the on-policy advantage approximator. RACER emphasizes a simple architecture, computational efficiency and minimal hyper-parameters. The application of ReF-ER and RACER to OpenAI Gym benchmarks produces state-of-the-art results while being robust to large variations in their hyper-parameters.
8

Under review as a conference paper at ICLR 2019
REFERENCES
M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048­5058, 2017.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
S. Colabrese, K. Gustavsson, A. Celani, and L. Biferale. Flow navigation by smart microswimmers via reinforcement learning. Physical Review Letters, 118(15):158004, 2017.
T. de Bruin, J. Kober, K. Tuyls, and R. Babuska. The importance of experience replay database composition in deep reinforcement learning. In Deep Reinforcement Learning Workshop, NIPS, 2015.
T. Degris, M. White, and R. S. Sutton. Off-policy actor-critic. arXiv preprint arXiv:1205.4839, 2012.
Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pp. 1329­1338, 2016.
S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep q-learning with model-based acceleration. In International Conference on Machine Learning, pp. 2829­2838, 2016.
P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.
R. Islam, P. Henderson, M. Gomrokchi, and D. Precup. Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint arXiv:1708.04133, 2017.
M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In International Conference on Learning Representations (ICLR), 2017.
S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334­1373, 2016.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations (ICLR), 2016.
L. H. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3/4):69­97, 1992.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, j. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928­1937, 2016.
R. Munos, T. Stepleton, A. Harutyunyan, and M. Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054­1062, 2016.
G. Novati, S. Verma, D. Alexeev, D. Rossinelli, W. M. van Rees, and P. Koumoutsakos. Synchronisation through learning for two self-propelled swimmers. Bioinspiration & Biomimetics, 12(3): 036001, 2017.
A. Rajeswaran, K. Lowrey, E. V. Todorov, and S. M. Kakade. Towards generalization and simplicity in continuous control. In Advances in Neural Information Processing Systems, pp. 6553­6564, 2017.
9

Under review as a conference paper at ICLR 2019

G. Reddy, A. Celani, T. J. Sejnowski, and M. Vergassola. Learning to soar in turbulent environments. Proceedings of the National Academy of Sciences, pp. 201606075, 2016.
T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In International Conference on Machine Learning, pp. 1312­1320, 2015a.
T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. In International Conference on Learning Representations (ICLR), 2015b.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1889­ 1897, 2015.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In ICML, 2014.
D. Silver, A Huang, C. J Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484­489, 2016.
R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000.
Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­5033. IEEE, 2012.
S. Verma, G. Novati, and P. Koumoutsakos. Efficient collective swimming by harnessing vortices through deep reinforcement learning. Proceedings of the National Academy of Sciences, 2018.
Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. K., and N. de Freitas. Sample efficient actorcritic with experience replay. In International Conference on Learning Representations (ICLR), 2017.

A EXPECTATIONS OF THE PARAMETERIZED ADVANTAGE FUNCTIONS

We considered three parameterizations for the action advantage:

fQw(s, a) = fSwG(s, a) = fDwG(s, a) =

-

1 2

[a

-

m(s)]

LQ(s)LQ(s) [a - m(s)]

K(s) exp

-

1 2

[a

-

m(s)]

L-SG1(s) [a - m(s)]

K(s) exp

-

1 2

a+

L+-1(s)

a+

-

1 2

a-

L--1(s)

a-

(12) (13) (14)

We recall a-= min [a-m(s), 0] and a+= max [a-m(s), 0]. The first parameterization fQw is identical to the one employed by NAF (Gu et al., 2016), and requires training (dA2 + dA)/2 coefficients of the lower triangular matrix LQ. Its quadratic complexity makes the choice of fQw unfavorable for high-dimensional action spaces (e.g. it requires 153 parameters for the 17-dimensional Humanoid tasks of OpenAI Gym, against the 35 of fDwG). In order to preserve bijection between LQ and LQLQ, the diagonal terms are mapped to R+ with a Softplus function. The expectation under a Gaussian
policy can be computed as (Petersen et al., 2008):

Ea  fQw(s, a ) = Tr LQ(s)LQ(s)(s)

(15)

Here Tr denotes the trace of a matrix. The second parameterization requires training the 1 + dA coefficients for K(s) and the diagonal

10

Under review as a conference paper at ICLR 2019

matrix L(s). The expectation can be easily derived from the properties of products of Gaussian

densities:

Ea  [fSwG(s, a )] = K(s)

|L(s)| |L(s) + (s)|

(16)

Here | · | denotes a determinant. To derive the expectation of the third parameterization, which was used for most results in the paper, we recall that the expectation of a product of independent variables is the product of the expectations. For one component i of the action vector:

E e =a 

-

1 2

u+,i

L+-,1i (s)

u+,i

-

1 2

u-,i

L--,1i (s)

u-,i

+L+,i (s)
L+,i (s)+i (s)
2

L-,i (s) L-,i (s)+i (s)

(17)

Here we exploited the symmetry of the Gaussian policy around the mean. Since , L+, and L- are all diagonal, we can compute the expectation:

dA
Ea  [fDwG(s, a )] = K(s)
i=1

+L+,i (s)
L+,i (s)+i (s)
2

Finally, we note that all these parameterizations are differentiable.

L-,i (s) L-,i (s)+i (s)

(18)

B SENSITIVITY TO HYPER-PARAMETERS

Figure 6 shows the robustness of RACER to various hyper-parameters and to the choice of the advan-
tage parameterizations introduced in App. A. Moreover, we can bypass the advantage approximator (i.e. Aw:=0) and rely exclusively on the state value approximator. We recall the on-policy value
estimator obtained with the "variance truncation and bias correction trick" (Wang et al., 2017) (Sec. 4):

V^ttbc = V w(st) + min{1, t}(Q^rtet - Qw(st, at))

(19)

From Eq. 2 and 19 we obtain Q^ret(st, at) = rt+1 + V^ tbc(st+1). By neglecting Aw, from this relation

and Eq. 19, we obtain a recursive Retrace-based estimator for the on-policy state value that depends

on V w alone:

V^ttbc = V w(st) + min {1, (st, at)} rt+1 + V^ tbc(st+1) - V w(st)

(20)

This target is equivalent to the recently proposed V-trace estimator (Espeholt et al., 2018) when all importance weights are clipped at 1, which was empirically found by the authors to be the best-performing solution. As expected, lacking a model for Aw(s, a), this architecture (denoted as V-RACER in the first column of results of Fig. 6) yielded worse performance than the original RACER. However the difference is often minor and the increased simplicity of this architecture might justify its adoption for some problems.
The tolerance D for far-policy samples in the RM has a similar effect as cmax: low values tend to delay learning while high values reduce the fraction of the RM that is used to compute updates and may decrease the accuracy of gradient estimates. Increasing the number F of time steps per gradient step could either cause a rightward shift of the expected returns because the learner computes fewer updates for the same budget of observations, or it could aid the learner by providing more on-policy samples. The actual outcomes depend on the problem: tasks with simpler dynamics (e.g. HalfCheetah) can be learned more quickly by performing more gradient steps, while problems with more complex dynamics (e.g. Ant, Humanoid) benefit from more on-policy samples. The batch-size B and learning rate  have a minor effect on performance.

C STATE, ACTION AND REWARD PREPROCESSING
Several authors have employed state (Henderson et al., 2017) and reward (Duan et al., 2016) (Gu et al., 2017) rescaling to improve the learning results. This technique has not systematically been studied but reflects a deeper challenge of RL. The success of any RL algorithm is intrinsically linked to the design of the control problem. For example, the stability of DDPG is affected by the L2 weight decay

11

Under review as a conference paper at ICLR 2019

Humanoid-v2

7000 6000 5000
6500 6000 5500 5000

Ant-v2

HalfCheetah-v2 Walker2d-v2

5500 5000 4500

11000 10000 9000 8000

2468 106 time steps fDG(s,a) fSG(s,a) fQ(s,a) V-Racer

10 2 4 6 8 106 time steps D=0.05
D=0.10 D=0.15 D=0.20

10 2 4 6 8 106 time steps
B= 64 B=128 B=256 B=512

10 2 4 6 8 106 time steps F=0.2 F=0.5 F=1.0 F=2.0 F=4.0

10 2 4 6 8 106 time steps
=0.00003 =0.0001 =0.0003

10

Figure 6: Mean cumulative rewards with RACER on OpenAI Gym tasks exploring the effect of the choice of advantage parameterization, ReF-ER tolerance factor D, mini-batch size B, number of time steps per gradient step F , and learning rate . Dashed-black lines refer to the baseline parameters. The y-axes are magnified around the final returns to clarify often minor differences in performance.

of the value-network. Depending on the numerical values of the distribution of rewards provided by the environment and the choice of weight decay coefficient, the L2 penalization can be either negligible or dominate the Bellman error. Similarly, the distribution of values describing the state variables can increase the challenge of learning by gradient descent.

We propose partially addressing these issues by rescaling both rewards and state vectors depending

on the the experiences contained in the RM. At the beginning of training we prepare the RM by

collecting Nstart observations and then we compute:

µs =

s1 nobs
nobs t=0 t

(21)

s =

1 nobs

nobs t=0

(st

-

µs)2

(22)

Throughout training, µs and s are used to standardize all state vectors s^t = (st -µs)/(s + ) before feeding them to the network approximators. Moreover, every 1000 steps, chosen as the smallest
power of ten that did not affect the run time, we loop over the nobs observations stored in the RM to compute:

r 

1 nobs

nobs
(rt+1)2
t=0

(23)

This value is used to scale the rewards r^t = rt/(r + ) used by the Q-learning target of DDPG and the Retrace algorithm for RACER. We use = 10-7 to ensure numerical stability.

The actions sampled by the learner may need to be rescaled or bounded to some interval de-
pending on the environment. For the OpenAI Gym tasks this amounts to a linear scaling a = a (upper_value - lower_value)/2, where the values specified by the Gym library are ±0.4 for the Humanoid tasks, ±8 for the Pendulum tasks, and ±1 for all others. The tasks of DeepMind Control Suite are defined with all actions bounded do the interval [-1, 1]dA . In this case
we can learn Gaussian policies, as if the action space were unbounded, and then map the actions sent
to the environment with an hyperbolic tangent: at = tanh at. This approach, however, might prevent efficiently learning policies that behave like bang-bang controllers. Close to the bounds of the control
space, actions that should be similar may be mapped to distant positions in the unbounded action
space. Therefore, learning bang-bang controls is more likely to be hindered by the DKL penalties of

12

Under review as a conference paper at ICLR 2019

Table 1: RACER architecture's baseline hyper-parameters.

Short-hand
C N Nstart D B F  

Description
Annealing parameter of cmax. Size of the Replay Memory. Number of samples gathered before training starts. Fraction of far-policy samples allowed in the RM. Mini-batch size. Ratio between observed time steps and gradient steps. Learning rate. Discount factor.

Baseline value
4 219 218 0.1 256 1 10-4 0.995

ReF-ER. This issue may explain some poor results on the DeepMind Control Suite shown in Fig. 9 and 10. These results show that successfully tackling control problems with bounded action spaces may require policies parameterized as Beta distribution (Chou et al., 2017).
D IMPLEMENTATION AND NETWORK ARCHITECTURE DETAILS
We implemented all presented learning algorithms within smarties,2 our open source C++ RL framework, and optimized for high CPU-level efficiency through fine-grained multi-threading, strict control of cache-locality, and computation-communication overlap. On every step, we asynchronously obtain on-policy data by sampling the environment with , which advances the index t of observed time steps, and we compute updates by sampling from the Replay Memory (RM), which advances the index k of gradient steps (Fig. 1a). During training, the ratio of time and update steps is usually equal to a constant: t/k = F . This parameter affects the data efficiency of the algorithm; by lowering F each sample is used more times to improve the policy before being replaced by newer samples. Upon completion of all tasks, we apply the gradient update and proceed to the next step. The pseudo-codes in App. E neglect parallelization details as they do not affect execution.
In order to evaluate all algorithms on equal footing, we use the same baseline network architecture for RACER, DDPG and NAF, consisting of an MLP with two hidden layers of 128 units each. For the sake of computational efficiency, we employedSoftsign activation functions. The weights of the hidden layers are initialized according to U -6/ fi + fo, 6/ fi + fo , where fi and fo are respectively the layer's fan-in and fan-out (Glorot & Bengio, 2010). The weights of the linear output layer are initialized from the distribution U -0.1/ fi, 0.1/ fi , such that the MLP has near-zero outputs at the beginning of training. When sampling the components of the action vectors, the policies are treated as truncated normal distributions with symmetric bounds at three standard deviations from the mean. Finally, we optimize the network weights with the Adam algorithm (Kingma & Ba, 2015).
RACER We note that the values of the diagonal covariance matrix are shared among all states and initialized to  = 0.2I. The remaining hyper-parameters of RACER are listed in table 1.
DDPG In its original formulation, DDPG transforms the policy-network's output onto the bounded interval [-1, 1]dA with an hyperbolic tangent function. We found that bounding the action space may limit the performance of DDPG with the OpenAI MuJoCo tasks. In order to stabilize the unboundedaction version of DDPG (u-DDPG) we set the learning rate for the policy-network to 1 · 10-5 and that of the value-network to 1 · 10-4 with L2 weight decay coefficient of 1 · 10-4. These changes lead to better performance also with the bounded-action DDPG and therefore were used for all numerical experiments. The RM is set to contain N =219 observations and we follow Henderson et al. (2017) for the remaining hyper-parameters: mini-batches of B = 128 samples, =0.995, soft target network update coefficient 0.01. We note that while DDPG is the only algorithm employing two networks, choosing half the batch-size as RACER and NAF makes the compute cost roughly equal among the three methods. Finally, when using ReF-ER we add exploratory Gaussian noise to the deterministic policy: w =mw +N (0, 2I) with =0.2. When performing regular ER or PER we sample the exploratory noise from an Ornstein­Uhlenbeck process with =0.2 and =0.15.
2 The repository is hidden to maintain anonymity during the review process.
13

Under review as a conference paper at ICLR 2019

NAF We use the same baseline MLP architecture and learning rate  = 10-4, batch-size B = 256, discount  = 0.995, RM size N = 219, and soft target network update coefficient 0.01. Gaussian noise is added to the deterministic policy w =mw +N (0, 2I) with =0.2.
E PSEUDO-CODES
Algorithm 1 Serial description of the master algorithm.
t = 0, k = 0, Initialize an empty RM, network weights w, and Adam algorithm's (Kingma & Ba, 2015) moments estimates. while nobs < Nstart do
Advance the environment according algorithm 2. end while Compute the initial statistics used to standardize the state vectors (App. C). Compute the initial statistics used to rescale the rewards (App. C). while t < Tmax do
while t < F · k do Advance the environment according algorithm 2. tt+1
end while Compute a gradient estimate (for RACER with algorithm 3). Perform the gradient step with the Adam algorithm. If applicable, update the ReF-ER penalization coefficient . if modulo(k, 1000) is 0 then
Update the statistics used to rescale the rewards (App. C). end if k  k+1 end while

Algorithm 2 Environment sampling
Observe st and rt. if st concludes an episode then
Store data for t into the RM: {st, rt, V w(st)} Compute and store Qret for all steps of the episode else Sample the current policy at  w(a|st) = µt Store data for t into the RM: {st, rt, at, µt, V w(st), Aw(st, at)} Advance the environment by performing at end if

Remarks on algorithm 2: 1) The reward for an episode's initial state, before having performed any action, is zero by definition. 2) The value V w(st) for the last state of an episode is computed if the
episode has been truncated due the task's time limits or is set to zero if st is a terminal state. 3) Each time step we use the learner's updated policy network and we store µt = {m(st), (st)}.

Remarks on algorithm 3: 1) In order to compute the gradients g^tAiC (w), we rely on advantage estimates

hQatrveitinthgattowceorme pcoumtepthueteqduwanhteintiessubAswe,qVuewn,tatnimd e

steps in ti's episode were previously drawn by ER. Not for all following steps comes with clear computational

efficiency benefits, at the risk of employing an incorrect estimate for Qrteit. In practice, we find that the Retrace values incur only minor changes between updates (even when large RM sizes decrease the

frequency of updates to the Retrace estimator) and that relying on previous estimates has no evident

effect on performance. This could be attributed to the gradual policy changes enforced by ReF-ER.

2) With a little abuse of the notation, with  (or µ) we denote the statistics (mean, covariance) of the

multivariate normal policy, with (a|s) we denote the probability of performing action a given state

s, and with (·|s) we denote the probability density function over actions given state s.

14

Under review as a conference paper at ICLR 2019

Algorithm 3 RACER's gradient update

Sample mini-batch of B time steps from RM.

for i = 0 to B do

Fetch all relevant information: sti Call the approximator to compute

,awt,iV, Qw(rtesitt,ia)n, Ad wµ(tsi t=i ,

{mti ati )

,

ti

}.

Update Qret for all prior steps in ti's episode with V w(sti ), Aw(sti , ati )

Update the importance weight ti = w(ati |sti )/µti (ati |sti )

if 1/cmax < ti < cmax then

Compute g^tAiC (w) according to Sec. 4

else

g^tAiC (w) = 0 end if

ReF-ER penalization: g^tRieF-ER(w) = g^tAiC (w) - (1-)DKL[µti (·|sti )||w(·|sti )]

end for

Accumulate the gradients

T i=0

g^tRieF-ER(w)

cumulative reward

cumulative reward

7000 6000 5000 4000 3000
11000 10000 9000 8000 7000 6000
9500 9400 9300 9200 9100 9000

Humanoid-v2 HalfCheetah-v2 InvertedDoublePendulum-v2

6000 5000 4000 3000 2000
4500 4000 3500 3000 2500 2000 1500
-2

-4

-6

2468 106 time steps

10 -8

Ant-v2
Hopper-v2
Reacher-v2
2468 106 time steps

6000 5000 4000

Walker2d-v2

3000 350

Swimmer-v2

300

250

200
x105 2 1.8 1.6 1.4 1.2 10 1

HumanoidStandup-v2
2468 106 time steps

10

cumulative reward

Figure 7: Additional result. 20th and 80th percentiles of cumulative rewards for trajectories of OpenAI Gym tasks ended within intervals of 2 · 105 time steps across 5 independent training runs using the
baseline RACER hyper-parameters (Table 1).

15

Under review as a conference paper at ICLR 2019

cumulative reward

cumulative reward

5000 4000 3000 2000
8000 7000 6000 5000 5000 4000 3000 2000 1000
0

Humanoid-v2

3000

2000

HalfCheetah-v2
InvertedDoublePendulum-v2
2468 106 time steps

1000
3500 3000 2500 2000 1500 1000
-8 -10 -12 -14 10

Ant-v2
Hopper-v2
Reacher-v2
2468 106 time steps

4000

Walker2d-v2

3000

2000

1000
350 300 250 200 150
x105 1.6 1.5

Swimmer-v2 HumanoidStandup-v2

1.4

1.3 10

2468 106 time steps

10

cumulative reward

Figure 8: Additional result. 20th and 80th percentiles of cumulative rewards for trajectories of OpenAI Gym tasks ended within intervals of 2 · 105 time steps across 5 independent training runs for u-DDPG
with ReF-ER, C = 4, and all other hyper-parameters set according to App. D.

SUPPLEMENTARY REFERENCES
[S1] Po-Wei Chou, Daniel Maturana, and Sebastian Scherer. Improving stochastic policy gradients in continuous control with deep reinforcement learning using the beta distribution. In International Conference on Machine Learning, pp. 834­843, 2017.
[S2] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.
[S3] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­256, 2010.
[S4] S. Gu, T. Lillicrap, Z. Ghahramani, R. E Turner, and S. Levine. Q-prop: Sample-efficient policy gradient with an off-policy critic. In International Conference on Learning Representations (ICLR), 2017.
[S5] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.
[S6] K. B. Petersen, M. S. Pedersen, et al. The matrix cookbook. Technical University of Denmark, 7(15): 510, 2008.

16

Under review as a conference paper at ICLR 2019

cumulative reward cumulative reward cumulative reward cumulative reward cumulative reward cumulative reward

1000 800 600 400 200

acrobot swingup

cartpole swingup

ball in cup catch

cartpole balance

1000 800 600 400 200
1000 800 600 400 200

cheetah run fish swim

finger spin hopper hop

finger turn easy

finger turn hard

hopper stand

humanoid run

1000 800 600 400 200

humanoid walk

humanoid stand

manipulator bring ball

pendulum swingup

1000 800 600 400 200
1000 800 600 400 200

point mass easy
swimmer swimmer6
2468 106 time steps

reacher easy
walker run
10 2 4 6 8 106 time steps

reacher hard
walker walk
10 2 4 6 8 106 time steps

swimmer swimmer15

walker stand

10 2 4 6 8 106 time steps

10

Figure 9: 20th and 80th percentiles of cumulative rewards for trajectories of DeepMind Control Suite tasks ended within intervals of 2 · 105 time steps across 5 independent training runs using the baseline
RACER hyper-parameters (Table 1).

17

Under review as a conference paper at ICLR 2019

cumulative reward cumulative reward cumulative reward cumulative reward cumulative reward cumulative reward

1000 800 600 400 200

acrobot swingup

cartpole swingup

ball in cup catch

cartpole balance

1000 800 600 400 200

cheetah run

finger spin

finger turn easy

finger turn hard

1000 800 600 400 200

fish swim

hopper hop

hopper stand

humanoid run

1000 800 600 400 200

humanoid walk

humanoid stand

manipulator bring ball

pendulum swingup

1000 800 600 400 200
1000 800 600 400 200

point mass easy swimmer swimmer6

reacher easy walker run

reacher hard

swimmer swimmer15

walker walk

walker stand

2468 106 time steps

10 2 4 6 8 106 time steps

10 2 4 6 8 106 time steps

10 2 4 6 8 106 time steps

10

Figure 10: 20th and 80th percentiles of cumulative rewards for trajectories of DeepMind Control Suite tasks ended within intervals of 2 · 105 time steps across 5 independent training runs for u-DDPG with
ReF-ER, C = 4, and all other hyper-parameters set according to App. D.

18

