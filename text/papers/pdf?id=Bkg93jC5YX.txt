Under review as a conference paper at ICLR 2019
BLISS IN NON-ISOMETRIC EMBEDDING SPACES
Anonymous authors Paper under double-blind review
ABSTRACT
Recent work on bilingual lexicon induction (BLI) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. We then propose Bilingual Lexicon Induction with Semi-Supervision (BLISS) -- a novel semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method improves over strong baselines for 11 of 14 language pairs on the MUSE dataset, particularly for languages whose embedding spaces do not appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.
1 INTRODUCTION
Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017b; Artetxe et al., 2017a; Lample et al., 2018b), finds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao & Guo, 2014), document classification (Klementiev et al., 2012), and machine translation (Irvine & Callison-Burch, 2013; Qi et al., 2018).
Most work on BLI uses methods that learn a mapping between two word embedding spaces (Ruder, 2017), which makes it possible to leverage pre-trained embeddings learned on large monolingual corpora. A commonly used method for BLI, which also empirically works well, involves learning an orthogonal mapping between the two embedding spaces (Mikolov et al. (2013a), Xing et al. (2015), Artetxe et al. (2016), Smith et al. (2017)). However, learning an orthogonal mapping inherently assumes that the embedding spaces for the two languages are isometric (subsequently referred to as the orthogonality assumption). This is a particularly strong assumption that may not necessarily hold true, and consequently we can expect methods relying on this assumption to provide sub-optimal results. In this work, we examine this assumption, identify where it breaks down, and propose a method to alleviate this problem.
We first formally describe this orthogonality assumption, and then present a theoretically motivated approach based on the Gromov-Hausdroff (GH) distance to check the validity and extent of the orthogonality assumption (§2). We show that the constraint indeed does not hold, particularly for etymologically distant language pairs.
Motivated by the above observation, we propose a framework for Bilingual Lexicon Induction with Semi-Supervision (BLISS) (§3.2) that alleviates the aforementioned issues. Moreover, besides addressing the limitations of the orthogonality assumption, the semi-supervised framework also addresses the shortcomings of purely supervised and purely unsupervised methods for BLI (§3.1). Our framework jointly optimizes for supervised embedding alignment, unsupervised distribution matching, and a weak orthogonality constraint in the form of a back-translation loss. Our results show that the different losses work in tandem to learn a better mapping than any one could on its own (§4.3). We also show that the proposed framework improves performance over strong baselines on two datasets, particularly for the case of embedding spaces where the orthogonal assumption is not valid.
1

Under review as a conference paper at ICLR 2019

Our analysis (§4.3) demonstrates that adding supervision to the learning objective, even in the form of a small seed dictionary, significantly improves the stability of the learning procedure. In particular, for cases where either the embedding spaces are far apart according to GH distance or the quality of the original embeddings is poor, our framework converges where the unsupervised baselines fail to. We also show that for the same amount of available supervised data, leveraging unsupervised learning allows us to obtain superior performance over baseline supervised and unsupervised methods using a comparable amount of data. All our code can be found at www.toaddhereifaccepted.com.

2 ISOMETRY OF EMBEDDING SPACES

Both supervised and unsupervised BLI often rely on the assumption that the word embedding spaces are isometric to each other. Thus, they learn an orthogonal mapping matrix to map one space to another. For example, for the case of supervised Bilingual Lexicon Induction, Xing et al. (2015) learn an orthogonal mapping matrix to minimize the distance between the projected source and the target embeddings; while in the unsupervised case, Lample et al. (2018b) propose learning a matrix near the manifold of orthogonal matrices to match the distributions of the projected source and target word embeddings.

We hypothesize that this assumption might not always hold, in particular for the cases when the language pairs in consideration are etymologically distant -- Zhang et al. (2017a) and Søgaard et al. (2018) provide evidence of this by observing a higher Earth Mover's distance and eigenvector similarity metric respectively between etymologically distant languages. In order to test this hypothesis, we propose a novel way of a-priori analyzing the validity of the orthogonality assumption using the Gromov Hausdorff (also referred as GH) distance.

In order to analyze the validity of the orthogonality assumption, we quantitatively check how well
the metric spaces of the word vectors of two languages can be aligned under an isometric transform using the GH distance.1

The Hausdorff distance between two metric spaces is a measure of the worst case or the diametric
distance between the spaces. Intuitively, it measures the distance between the nearest neighbours that are the farthest apart. Concretely, given two metric spaces X , and Y with a distance function

d(., .), the Hausdorff distance is defined as:

H(X , Y) = max{ sup inf d(x, y), sup inf d(x, y) }.

xX yY

yY xX

(1)

The Gromov-Hausdorff distance minimizes the Hausdorff distance over all isometric transforms between X and Y, thereby providing a quantitative estimate of the isometry of two spaces

H(X , Y) = inf H(f (X ), g(Y)),
f,g

(2)

where f, g belong to set of isometric transforms.

Computing the Gromov-Hausdorff distance involves solving hard combinatorial problems, but can be tractably approximated using the Bottleneck distance (Chazal et al., 2009). In order to compute the Bottleneck distance between two metric spaces, we compute the first order Vietoris-Rips complex (first order for computational efficiency) at t for both spaces: a graph containing an edge between two points iff they lie within a Euclidean distance t from each other in the metric space. As t is varied, the Vietoris-Rips complex goes from the individual points (at t = 0) to a single cluster (at t = ). As t increases, clusters are formed (birth) and eventually merge together (death). The persistence diagram is a 2D plot of the (tbirth, tdeath) of each cluster, where tbirth and tdeath are the values of t at which the cluster was born and died respectively. Given two persistence diagrams f, g, let  be a bijective map from the points of f to the points of g. The bottleneck distance (B) is then defined as:

B(f, g) = inf


sup ||u - (u)||
uf

(3)

1Note that since we mean center the embeddings, the orthogonal transforms are equivalent to isometric transforms.

2

Under review as a conference paper at ICLR 2019

Source  Target aunt  
uruguay   regiments   comedian  

Incorrect Predicted  (Grandmother)  (Argentina)  (Cavalry)
 (Actor)

Table 1: Words for which semi-supervised method predicts correctly, but unsupervised method doesn't. The unsupervised method is able to guess the general family but fails to pinpoint exact match

Figure 1: Language Pairs and their GH distance

Chazal et al. (2009) showed that the Gromov-Hausdorff distance can be lower bounded by the Bottleneck Distance between the Persistence Diagrams of the Vietoris-Rips Filtration of the two spaces.
We compute this lower bound for the top frequency words of different language pairs. As can be observed from Figure 1, the GH distances are higher for distant language pairs.
3 SEMI-SUPERVISED FRAMEWORK
In this section, we motivate and define our semi-supervised framework for BLI. First we describe issues with purely supervised and unsupervised methods, and then lay the framework for tackling them along with orthogonality constraints.
3.1 DRAWBACKS OF PURELY SUPERVISED AND UNSUPERVISED METHODS
Purely supervised methods for aligning word vectors do not utilize the rich information present in the topology of the word vectors. Purely unsupervised methods, on the other hand, can suffer from poor performance if the distribution of the embedding spaces of the two languages are very different from each other. Moreover, unsupervised methods can successfully align clusters of words together, but miss out on fine grained alignment within the clusters.
We explicitly show the aforementioned problem of purely unsupervised methods with the help of the toy dataset shown in 2a, and 2b. In this dataset, due to the density difference between the two large blue clusters, unsupervised matching is consistently able to align them properly, but has trouble aligning the smaller embedded green and red sub-clusters. The correct transformation of the source space is a clockwise 90 rotation followed by reflection along the x-axis. Unsupervised matching converges to this correct transformation only half of the time; in rest of the cases, it ignores the alignment of the sub-clusters and converges to a 90 counter-clockwise transformation as shown in 2c.
We also find evidence of this problem in the real datasets used in our experiments as shown in Table 1. It can be seen that the unsupervised method aligns clusters of similar words, but is poor at the fine grained alignment. We hypothesize that this problem can be resolved by giving it a some supervision in the form of matching anchor points inside these sub-clusters, which correctly aligns them. Analogously, for the task of BLI, generating a small supervised seed lexicon is generally feasible for most language pairs, through either bilingual speakers, existing dictionary resources, or Wikipedia language links. This can provide the requisite supervision.
3.2 A SEMI-SUPERVISED FRAMEWORK
In order to alleviate the problems with the orthogonality constraints, the purely unsupervised and supervised approaches, we propose a semi-supervised framework, described below. Let X = {x1 . . . xn} and Y = {y1 . . . ym}, xi, yi  Rd be two sets of word embeddings from the source and target language respectively and let S = {(x1s, y1s) . . . (xks , yks)} denote the bilingual aligned word embeddings. Define W to be the mapping matrix.
3

Under review as a conference paper at ICLR 2019

(a) Source distribution

(b) Target distribution (c) Misaligned source distribution

Figure 2: A toy dataset demonstrating the shortcomings of unsupervised distribution matching. Fig. a) and b) show two different distributions (source and target respectively) over six classes. Classes 1 and 2; classes 3 and 4; classes 5 and 6 were respectively drawn from a uniform distribution over a sphere, rectangle and triangle respectively. Fig. c) shows the misprojected source distribution obtained from unsupervised distribution matching which fails to align with the target distribution of Fig. b).

For learning W , we leverage unsupervised distribution matching, aligning known word pairs and a data-driven weak orthogonality constraint.
Unsupervised Distribution Matching: Given all word embeddings X and Y, the unsupervised loss LW |D aims to match the distribution of both embedding spaces. In particular, for our formulation, we use an adversarial distribution matching objective, similar to the work of Lample et al. (2018b). Specifically, a mapping matrix W from the source to the target is learned to fool a discriminator D, which is trained to distinguish between the mapped source embeddings W X = {W x1 . . . W xn} and Y. The corresponding objectives are defined as:

11

LD|W

=

- n

log(1 - D(W xi)) - m

log D(xi)

xi X

xi Y

1

LW |D

=

- n

log D(W xi)

xi X

(4) (5)

Aligning Known Word Pairs: Given aligned bilingual word embeddings S, we aim to minimize a similarity function (fs) which maximizes the similarity between the corresponding matched pairs of words. Specifically, the loss is defined as:

LW |S

=

-1 |S |

fs(W xis, yis)

(xsi ,yis)S

(6)

Weak Orthogonality Constraint: Given an embedding space X , we define a consistency loss that maximizes the similarity fa between x and W T W x, x  X . This cyclic consistency loss LW|O
encourages orthogonality of the W matrix based on the joint optimization:

1 LW |O = - |X |

fa(xi, W T W xi)

xi X

(7)

The above loss term allows the model adjust the trade-off between orthogonality for accuracy, based on the joint optimization. This is particularly helpful for cases where the orthogonality constraint is violated, where the embedding spaces are not isometric, specifically for etymologically distant language pairs, as we show in (4.3).

4

Under review as a conference paper at ICLR 2019

Metric

en-fr ru-uk en-es es-fr en-uk en-ru en-sv en-el en-hi

GH Distance 0.17 0.18 0.20 0.24 0.34 0.44 0.46 0.47 0.50 ||I - W T W ||F 0.01 0.025 0.03 0.02 59.8 54.3 71.6 72.6 106.3

Table 2: Bottleneck distance and difference from Orthogonality for language pairs

en-ko
0.92 *

The final loss function for the mapping matrix is:

L = LW |D + LW |S + LW |O

(8)

LW |D enables the model to leverage the distributional information available from the two embedding spaces, thereby using all available monolingual data. On the other hand, LW |S allows for the correct alignment of labeled pairs when available in the form of a small seed dictionary. Finally, LW |O encourages orthogonality. One can think of LW |O and LW |S as working against each other when the spaces are not isometric. Jointly optimizing both helps the model to strike a bal-
ance between them in a data driven manner, encouraging orthogonality but still allowing for flexible
mapping.

3.3 ITERATIVE PROCRUSTES REFINEMENT AND HUBNESS REMOVAL
A common method of improving BLI is iteratively expanding the dictionary and learning the matrix as a post-processing step (Artetxe et al., 2017a; Lample et al., 2018b). Given a learnt mapping matrix, Procrustes Refinement first finds the pair of points in the two languages that are very closely matched by the mapping matrix and constructs a bilingual dictionary from these pairs. These pair of points are found by considering source words and their nearest neighbors (NN) The mapping matrix is then refined by setting it to be the Procrustes solution of the dictionary obtained. Iterative Procrustes Refinement (also referred as Iterative Dictionary Expansion) applies the above step iteratively until some metric (CSLS in case of Lample et al. (2018b)) converges.
However, learning an orthogonal linear map in such a way leads to some words (known as hubs) to become nearest neighbors of a majority of other words (Radovanovic´ et al., 2010; Dinu & Baroni, 2014). In order to estimate the hubness of a point, Radovanovic´ et al. (2010) first computed the distribution Nx(k), the counts of all points y such that x  k - N N (y), normalized over all k. The skewness of this distribution was defined as the hubness of the point, with positive skew representing hubs and negative skew representing isolated points. An approximation to this would be Nx(1), i.e the number of points that x is the 1-NN of.
We use a simple hubness filtering mechanism to filter out words in the target domain that are hubs, i.e., words in the target domain which have more than a threshold number of neighbors in the source domain are not considered in the iterative dictionary expansion. Empirically, this leads to a small boost in performance.

4 EXPERIMENTS AND RESULTS
In this section, we first measure the GH distances between embedding spaces of various language pairs and show that it correlates well with an empirical measure of the degree of orthogonality of a learned mapping matrix. Next, we analyze the performance of BLISS, our proposed semi-supervised approach, on two datasets, and compare its performance with several baseline methods. Finally, we highlight a few cases where BLISS does particularly well.
4.1 GH DISTANCE
To evaluate the lower bound on the GH distance between the two embedding spaces, we select the 5000 most frequent words of the source and target language and compute the Vietoris-Rips complex of the zeroth order (which considers only pairwise edges for identifying clusters). These embeddings are mean centered, unit normed and the Euclidean Distance is used as the distance metric.

5

Under review as a conference paper at ICLR 2019

Model
MUSE (U) MUSE (S) MUSE (R)
MUSE (HR) BLISS (5000) BLISS (all)

en-es
81.7 81.4 81.9
82.3 82.9 82.3

es-en
83.3 82.9 83.5
83.3 83.7 84.3

en-fr
82.3 81.1 82.1
82.5 83.3 83.3

fr-en
82.1 82.4 82.4
83.2 82.9 83.9

en-de
74.0 73.5 74.3
75.7 75.8 75.7

de-en
72.2 72.4 72.7
72.8 72.5 73.8

en-ru
44.0 51.7 51.7
52.8 52.1 55.7

ru-en
59.1 63.7 63.7
64.1 63.8 63.7

en-zh
32.5 42.7 42.7
42.7 39.8 41.1

zh-en
31.4 36.7 36.7
36.7 38.8 41.4

Table 3: Performance comparison of BLISS against various baseline models on the MUSE dataset.

Models

en-it all Num.

en-de all Num.

Pairs

GH MUSE(R) BLISS Post Refine

Vecmap Vecmap++
MUSE (U)
BLISS

39.7 37.3 45.3 -
45.8 45.9 44.3

40.9 39.6 44.1 -
0 48.3 47.2

en-uk en-ru en-el en-ja

0.34 0.44 0.47
*

36.9 51.7 44.7 12.7

38.9 55.7 46.9 20.6

36.2 52.3 44.6 12.5

Table 4: Results supervision from numbers (Num.).

on Vecmap Dataset with either all data (all) or just

Table 5: Results on language pairs with high GH distance. * denotes failed to converge. Post Refine refers to the accuracies obtained after Iterative Procrustes Refinement

Table 2 summarizes the obtained results for different language pairs. We find that etymologically close languages such as en-fr and ru-uk have a very low bottleneck distance and can possibly be aligned well using orthogonal transforms. In contrast, we find that etymologically distant language pairs such as en-ru and en-hi cannot be aligned well using orthogonal transforms.
To further corroborate this, we run Lample et al. (2018b)'s model (without dictionary expansion), and measure ||I - W T W ||F of the resulting mapping matrix obtained. Table 2 row 2 further corroborates this, with a Pearson correlation between the two measures of 0.953. 2 Consequently the GH distance allows us to a priori estimate the degree of isometry between two embedding spaces. In other words, the GH distance now allows us to efficiently compute how isometric two embedding spaces are without having to explicitly learn a mapping matrix or train a model.
4.2 PERFORMANCE OF BLISS ON BENCHMARK TASKS
We compare the performance of BLISS against the following models:
Vecmap models (Artetxe et al., 2017a) and (Artetxe et al., 2018) proposed two models, Vecmap and Vecmap++ respectively, and analyzed their performance on the dataset released by (Dinu & Baroni, 2014).
MUSE models (Lample et al., 2018b) propose two models: MUSE(U) and MUSE(S) for unsupervised and supervised BLI respectively. To benchmark these models, they use fastText embeddings Bojanowski et al. (2016) trained on Wikipedia along with bilingual dictionaries they release. They propose using the CSLS metric for iterative refinement in the unsupervised case. We find that applying this iterative refinement in the case of MUSE(S) yields a consistent performance boost, making for a stronger baseline, which we refer MUSE(R). We also evaluate MUSE(R) with our aforementioned hubness removal technique, referred to as MUSE(HR). For completeness, we also evaluate and report the performance of MUSE(U) on the (Dinu & Baroni, 2014) dataset.
We show our results against the Vecmap models in Table 4 -- as can be seen, we obtain significant improvements. Further, we show our results against the MUSE models in Table 3; BLISS outperforms baseline models on 8 out of 10 language pairs.
All the hyperparameters for the experiments can be found in the Appendix (§7.4)
2en-ko failed to converge
6

Under review as a conference paper at ICLR 2019

Accuracy CSLS
Accuracy CSLS
Accuracy CSLS

70 60 50 40 30 20 10 0
0

CSLS: Unsup en-de
5 10 15 Acc: Unsup Epochs

CSLS: Semi
0.175 0.150 0.125 0.100 0.075 0.050 0.025 20 25 Acc: Semi

50 40 30 20 10 0
0

CSLS: Unsup en-ru
5 10 15 Acc: Unsup Epochs

CSLS: Semi
0.125 0.100 0.075 0.050 0.025 0.000 20 25 Acc: Semi

35 30 25 20 15 10 5
0

CSLS: Unsup en-zh
5 10 15 Acc: Unsup Epochs

CSLS: Semi
0.03 0.02 0.01 0.00 0.01 0.02 0.03 0.04 20 25 Acc: Semi

en-de

en-ru Figure 3: Training Stability of different language pairs

en-zh

4.3 BENEFITS OF BLISS

Languages with high GH distance BLISS particularly shines when the two embedding spaces are significantly different and the orthogonality constraint is violated. Table 5 shows that BLISS achieves significant performance gains for language pairs with high GH distance. Contrary to popular methods that rely on Iterative Procrustes refinement (where Vecmap's dictionary expansion is done as a post-refinement step), we observe from Table 5 that using this step can actually hurt the performance (column 5), particularly for language pairs with high GH distance.

Performance with varying amount of supervision Table 6 shows the performance of BLISS as a function of the number of data points provided for supervision. As can be observed, for the case with a few number of data points provided as supervision (50), the model does particularly well; outperforming the baselines significantly. Moreover, note that the difference is more prominent for enzh, for which, we know from the GH distance that the spaces are not isometric. In this case the baseline models completely fail to train for 50 points, whereas BLISS performs reasonably well.

src-tgt en-de de-en en-zh

Model
MUSE (S) MUSE (R)
BLISS
MUSE (S) MUSE (R)
BLISS
MUSE (S) MUSE (R)
BLISS

Num Sup Points 50 500 5000 all
0.1 8.27 70.3 74.5 31.9 73.1 75.2 75.7 65.3 74.8 75.9 75.7
0.0 10.3 69.0 72.5 72.7 72.7 72.4 72.8 73.1 72.9 72.5 73.8
0.0 6.9 39.2 42.7 0.3 34.5 39.2 42.7 33.3 34.9 40.6 41.1

Stability of Training We also observe that providing even a little bit of supervision helps stabilize the training process, when

MUSE (S) 0.1 8.3 zh-en MUSE (R) 0.3 32.2
BLISS 32.4 32.9

33.7 36.3 39.4

36.7 36.7 41.4

compared to purely unsupervised distribution matching.

Table 6: Performance with varying Data

We measure the stability during training using both the ground truth accuracy and the CSLS metric

(which Lample et al. (2018b) showed to be correlated with the ground truth accuracy). As can be

seen from Figure 3, BLISS is significantly more stable, converging to better accuracy and CSLS

values.

When the word vectors are not rich enough (word2vec (Mikolov et al., 2013b) instead of fastText), the unsupervised method can completely fail to train. This can be observed for the case of en-de in table 4. BLISS does not face this problem: adding supervision, even in the form of 25 mapped words for the case of en-de, helps it to achieve reasonable performance.

5 RELATED WORK
Mikolov et al. (2013a) first used anchor points to align two embedding spaces, leveraging the fact that these spaces exhibit similar structure across languages. Since then, several approaches have been proposed for learning bilingual dictionaries (Faruqui & Dyer, 2014; Zou et al., 2013; Xing et al., 2015). Xing et al. (2015) showed that adding an orthogonal constraint significantly improves performance, and admits a closed form solution. This was further corroborated by the work of Smith

7

Under review as a conference paper at ICLR 2019
et al. (2017), who showed that in orthogonality was necessary for self-consistency. Artetxe et al. (2016) showed the equivalence between the different methods, and their subsequent work (Artetxe et al., 2018) analyzed different techniques proposed in various works (like embedding centering, whitening etc.), and showed that leveraging a combination different methods showed significant performance gains.
However, the validity of this orthogonality assumption has of late come into question: Zhang et al. (2017a) found that the Wasserstein distance between distant language pairs was considerably higher , while Søgaard et al. (2018) explored the orthogonality assumption using eigenvector similarity. We find our weak orthogonality constraint (along the lines of (Zhang et al., 2017b)) when used in our semi-supervised framework to be more robust to this.
There has also recently been an increasing focus on generating these bilingual mappings without an aligned bilingual dictionary, i.e., in an unsupervised manner. Zhang et al. (2017b) and Lample et al. (2018b) both use adversarial training for aligning two monolingual embedding spaces without any seed lexicon, while Zhang et al. (2017a) used a Wasserstein GAN to achieve this adversarial alignment, and use an earth-mover based fine-tuning approach; while Grave et al. (2018) formulate this as a joint estimation of an orthogonal matrix and a permutation matrix. However, we show that adding a little supervision, which is usually easy to obtain, improves performance.
Artetxe et al. (2017a) and Søgaard et al. (2018) motivate the utility of using both the supervised seed dictionaries and, to some extent, the structure of the monolingual embedding spaces. They use iterative Procrustes refinement starting with a small seed dictionary to learn a mapping; but doing may lead to sub-optimal performance for distant language pairs. However, these methods are close to our methods in spirit, and consequently form the baselines for our experiments.
Another avenue of research has been to try and modify the underlying embedding generation algorithms. Cao et al. (2016) modify the CBOW algorithm Mikolov et al. (2013b) by augmenting the CBOW loss to match the first and second order moments from the source and target latent spaces, thereby ensuring the source and target embedding spaces follow the same distribution. Luong et al. (2015), in their work, use the aligned words to jointly learn the embedding spaces of both the source and target language, by trying to predict the context of a word in the other language, given an alignment. An issue with the proposed method is that it requires the retraining of embeddings, and cannot leverage a rich collection of precomputed vectors (like ones provided by Word2Vec (Mikolov et al., 2013b), Glove (Pennington et al., 2014) and FastText (Bojanowski et al., 2016)).
6 CONCLUSIONS AND FUTURE WORK
In this work, we analyze the validity of the orthogonality assumption and show that it breaks for distant language pairs. We motivate the task of semi-supervised BLI by showing the shortcomings of purely supervised and unsupervised approaches. We finally propose a semi-supervised framework which combines the advantages of supervised and unsupervised approaches and uses a joint optimization loss to enforce a weak and flexible orthogonality constraint. We show that our framework obtains gains over several baseline models for numerous language pairs. On analyzing the model errors, we find that a large fraction of them arise due to polysemy and antonymy (An interested reader can find the details in Appendix (§7.2).
An interesting line of future work would be to extend the method proposed here to account for polysemy in translation, possibly by leveraging the work of Upadhyay et al. (2017), which uses multilingual context for sense disambiguation. Another confounding factor is synonyms and antonyms, which appear in similar contexts, and often incorrectly get translated to each other: leveraging the work done by Mrksic´ et al. (2016) and Faruqui et al. (2014) might be an interesting way to mitigate this problem. Along a different possible future line of work, leveraging the auto-encoder formulation used here in an unsupervised context, and drawing further inspiration from recent work in the unsupervised machine translation domain, such as Artetxe et al. (2017b) and Lample et al. (2018a), an extremely interesting line of work would be to have the auto-encoder output into a common latent embedding space coupled with cross-lingual and cyclic consistency to do unsupervised and semi-supervised multi-lingual word-level translation.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2289­2294, 2016.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 451­462, Vancouver, Canada, July 2017a. Association for Computational Linguistics. URL http://aclweb.org/ anthology/P17-1042.
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine translation. 2017b.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), 2018.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. 2016.
Hailong Cao, Tiejun Zhao, Shu Zhang, and Yao Meng. A distribution-based model to learn bilingual word embeddings. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pp. 1818­1827, 2016.
Fre´de´ric Chazal, David Cohen-Steiner, Leonidas J Guibas, Facundo Me´moli, and Steve Y Oudot. Gromov-hausdorff stable signatures for shapes using persistence. In Computer Graphics Forum, volume 28, pp. 1393­1403. Wiley Online Library, 2009.
Georgiana Dinu and Marco Baroni. Improving zero-shot learning by mitigating the hubness problem. volume abs/1412.6568, 2014. URL http://arxiv.org/abs/1412.6568.
Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pp. 462­471, 2014.
Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris Dyer, Eduard Hovy, and Noah A Smith. Retrofitting word vectors to semantic lexicons. arXiv preprint arXiv:1411.4166, 2014.
Edouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised alignment of embeddings with wasserstein procrustes. arXiv preprint arXiv:1805.11222, 2018.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL-08: HLT, pp. 771­779, Columbus, Ohio, June 2008. Association for Computational Linguistics. URL http://www.aclweb.org/ anthology/P/P08/P08-1088.
Ann Irvine and Chris Callison-Burch. Combining bilingual and comparable corpora for low resource machine translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pp. 262­270, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W13-2233.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. Inducing crosslingual distributed representations of words. pp. 1459­1474, 2012.
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato. Unsupervised machine translation using monolingual corpora only. In International Conference on Learning Representations, 2018a. URL https://openreview.net/forum?id=rkYTTf-AZ.
Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herve´ Je´gou. Word translation without parallel data. In International Conference on Learning Representations, 2018b. URL https://openreview.net/forum?id=H196sainb.
9

Under review as a conference paper at ICLR 2019

Thang Luong, Hieu Pham, and Christopher D Manning. Bilingual word representations with monolingual quality in mind. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pp. 151­159, 2015.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for machine translation. 2013a.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Dis-

tributed representations of words and phrases and their compositionality.

In

C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger

(eds.), Advances in Neural Information Processing Systems 26, pp. 3111­3119.

Curran Associates, Inc., 2013b.

URL http://papers.nips.cc/paper/

5021-distributed-representations-of-words-and-phrases-and-their-compositionality.

pdf.

Nikola Mrksic´, Diarmuid O Se´aghdha, Blaise Thomson, Milica Gasic´, Lina Rojas-Barahona, PeiHao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. Counter-fitting word vectors to linguistic constraints. arXiv preprint arXiv:1603.00892, 2016.

Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532­1543, 2014.

Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. When and why are pre-trained word embeddings useful for neural machine translation? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 529­535, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. URL http://www. aclweb.org/anthology/N18-2084.

Milos Radovanovic´, Alexandros Nanopoulos, and Mirjana Ivanovic´. Hubs in space: Popular nearest neighbors in high-dimensional data. volume 11, pp. 2487­2531, 2010.

Sebastian Ruder. A survey of cross-lingual embedding models. CoRR, abs/1706.04902, 2017. URL http://arxiv.org/abs/1706.04902.

Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. 2017.

Anders Søgaard, Sebastian Ruder, and Ivan Vulic´. On the limitations of unsupervised bilingual dictionary induction. arXiv preprint arXiv:1805.03620, 2018.

Shyam Upadhyay, Kai-Wei Chang, Matt Taddy, Adam Kalai, and James Zou. Beyond bilingual: Multi-sense word embeddings using multilingual context. arXiv preprint arXiv:1706.08160, 2017.

Min Xiao and Yuhong Guo. Distributed word representation learning for cross-lingual dependency parsing. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pp. 119­129, 2014.

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. Normalized word embedding and orthogonal transform for bilingual word translation. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1006­1011, 2015.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Earth mover's distance minimization for unsupervised bilingual lexicon induction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1934­1945, 2017a.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Adversarial training for unsupervised bilingual lexicon induction. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1959­1970, 2017b.

10

Under review as a conference paper at ICLR 2019
Yuan Zhang, David Gaddy, Regina Barzilay, and Tommi Jaakkola. Ten pairs to tag-multilingual pos tagging via coarse mapping between embeddings. Association for Computational Linguistics, 2016.
Will Y Zou, Richard Socher, Daniel Cer, and Christopher D Manning. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1393­1398, 2013.
7 APPENDIX
7.1 TOY DATASET EXPERIMENT

(a) Actual distributions

(b) Misaligned source and actual target distribution

Unsupervised distribution matching solution remains invariant to the permutations of the words within a language. One can easily construct a toy dataset on which this scenario can arise. 4a shows a source and target distribution. Although, in the real dataset each point carries a different label, we only consider 2 labels for the sake of simplicity. The correct transformation for matching distribution and labels is an anticlockwise rotation on the source. Since the GAN does not see the labels, it just matches the distributions by half of the times either choosing the correct anticlockwise rotation or the incorrect clockwise rotation on source 4b. This problem can be solved by giving some labeled data correspondence and adding a supervised loss term.

7.2 ANALYZING MODEL ERRORS

Figure 4: Fraction of errors coming from polysemy in the source/target side and antonymy, for the language pairs en-zh, en-it, en-es and en-fr
11

Under review as a conference paper at ICLR 2019

We characterize the mistakes made by the model, and find that most fall into the following 4 categories:
Polysemy on the target side: These are the cases in which the predicted words and the gold translation are synonyms/hypernyms/hyponyms of each other.
Polysemy on the source side: These are the cases in which the gold translations and the predicted words are different senses of the source word.
Antonyms: The distribution of the context of antonyms is often very similar. Unsurprisingly the word vectors of antonyms are quite similar. This leads to cases where the predicted words and gold labels are antonyms of each other.
Words that occur in common contexts: Words that occur in numerous contexts often have poor word embeddings, since a single embedding can't capture polysemy. Consequently, multiple such word embeddings that are frequent and have poor representations often get incorrectly translated to each other. Some examples include proper nouns and numbers
We quantitatively estimate the fraction of errors due to these reasons using WordNet synsets. Given 2 synsets, WordNet provides a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1. A score of 1 represents identity i.e. comparing a sense with itself will return 1. We approximate the fraction of target polysemy errors by finding those cases for which the aforementioned similarity scores between the synsets of the predicted words and the gold translations  0.1. Similarly we approximate the fraction of source polysemy errors by finding those cases for which the similarity scores between the synsets of the source word and the predicted word  0.1. Fig 4 shows these estimations for different language pairs. See Table 5a in (§7.2) for examples sampled from each of these error types.

Type of Error Target Polysemy Target Polysemy Source Polysemy
Source Polysemy Antonyms Antonyms Antonyms Common Words Common Words

Source Shadows Quest Worn
Bitter Unofficial Mature Afraid Everybody Fourteen

Gold  Quest use´  Ufficiale Mature Paura Jeder Vierzehn

Predicted  Avventura ve^tement 
Funzionario Jeune Contento Spaß Dreirzehn

Comments synonyms synonyms Gold: used, Predicted: cloth
Gold: bitter (taste), predicted: bitter (feeling) funzionario: official Jeune: young Gold: fear, Predicted: happy Gold: Everybody, Predicted: Fun Numbers translated incorrectly

(a) Sampled Errors

7.3  ORTHOGONALITY PROJECTION VS. AUTOENCODING LOSS
Lample et al. (2018b) constraint the mapping matrix to be close to the manifold of orthogonal matrices by applying the following projection step after every update.
W  (1 + )W - (W W T )W In our experiments we found out that the final accuracy is highly sensitive to the value of the hyperparameter . Our approach on the other hand uses an autoencoding loss which allows the model to flexibly adjusts the degree of orthogonality in a data driven manner and works consistently well for one choice of the scaling of the autoencoding loss.
7.4 HYPER-PARAMETERS
The following are the hyper parameters used in the experiments. The values separated by / are the different values tried in the parameter search.
· Number of words per language considered for GAN training: top 75000 · Discriminator Parameters:
12

Under review as a conference paper at ICLR 2019

Lang

Ortho

1e-2

 1e-3

1e-4

Auto

en-de 19.9 74.8 67.4 73.7 74.3 en-ru 102.5 40.8 30.7 36.7 46.1 en-zh 171.1 0 23.8 32.1 33.3

Table 7: Unsupervised accuracies for different values of  (MUSE) and our autoencoding loss.

­ embedding dim: 300 ­ hidden dim: 2048 ­ dropout prob: 0.1 (Only on the input layer) ­ label smoothing: 0.1 · Generator Parameters
· Initialization: Identity / Random Orthogonal · Mean Center: True · GAN Training Parameters
­ batch size: 32 ­ Optimizer: SGD ­ Supervised loss optimizer: SGD / Adam ­ lr: 0.1 (with a schedule of 0.98 decay per round, and halved if CSLS metric does not
improve over two rounds). ­ Hubness Threshold: 20 · fs = fa = cosine

13

