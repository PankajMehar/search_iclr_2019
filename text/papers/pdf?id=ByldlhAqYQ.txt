Under review as a conference paper at ICLR 2019
TRANSFER LEARNING FOR SEQUENCES VIA LEARNING TO COLLOCATE
Anonymous authors Paper under double-blind review
ABSTRACT
Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain. Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer. RNN uses a chain of repeating cells to model the sequence data. However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling. Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain. In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer. ART is in a recurrent manner that different cells share the same parameters. Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain. This strategy enables ART to capture the word collocation across domains in a more flexible way. We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis). ART outperforms the state-of-the-arts over all experiments.
1 INTRODUCTION
Most previous NLP studies focus on open domain tasks. But due to the variety and ambiguity of natural language (Glorot et al., 2011; Song et al., 2011), models for one domain usually incur more errors when adapting to another domain. This is even worse for neural networks since embeddingbased neural network models usually suffer from overfitting (Peng et al., 2015). While existing NLP models are usually trained by the benchmark datasets of the open domain, they suffer from severe performance degeneration when adapting to specific domains. This motivates us to train specific models for specific domains.
The key issue of training a specific domain is the insufficiency of labeled data. Transfer learning is one promising way to solve the insufficiency (Jiang & Zhai, 2007). Existing studies (Daume´ III, 2009; Jiang & Zhai, 2007) have shown that (1) NLP models in different domains still share many common features (e.g. common vocabularies, similar word semantics, similar sentence syntaxes). (2) The corpus of the open domain is usually much richer than that of a specific domain.
Most previous transfer learning approaches (Li et al., 2018; Ganin et al., 2016) only transfer information across the whole layers. This causes the information loss from cells in the source domain. We highlight the effectiveness of precisely capturing and transferring information of each cell from the source domain in two cases. First, in seq2seq (e.g. machine translation) or sequence labeling tasks (e.g. POS tagging), all cells directly affect the results. So layer-wise information transfer is unfeasible for these tasks. Second, even for the sentence classification, cells in the source domain provide more fine-grained information. Such information help the model understand the target domain with the insufficiently trained parameters. For example, in figure 1, parameters for "hate" are insufficiently trained. The model transfers the state of "hate" from the source domain to understand it better.
Besides transferring the corresponding position's information, the transfer learning algorithm needs to solve the cross domain long-term dependency. A word pair that has a strong correlation can have a long gap between them. Being in the insufficiently trained target domain, a word will need
1

Under review as a conference paper at ICLR 2019
Target: Sometimes I really hate RIBs.
Source: Sometimes I really hate RIBs.
Figure 1: Motivation of ART. The orange words "sometimes" and "hate" are with insufficiently trained parameters. The red line indicates the information transfer from the corresponding position. The blue line indicates the information transfer from a collocated word.
to benefit from its collocated words from the source domain to represent its precise meaning. For example, in figure 1, "hate" is modified by the adverb "sometimes", which implies the act of hating is not serious. But the "sometimes" in the target domain is trained insufficiently. We need to transfer the semantics of "sometimes" in the source domain to understand the implication. Thus we need to carefully align word collocations between the source domain and the target domain to represent the long-term dependency.
In this paper, we proposed ART (aligned recurrent transfer), a novel transfer learning mechanism, to transfer cell-level information by learning to collocate cross domain words. ART allows the cell-level information transfer by directly extending each RNN cell. It discriminates information transfer from the corresponding position and all positions with collocated words.
Cell-Level Recurrent Transfer ART extends each recurrent cell by taking the states from the source domain as an extra input. In traditional layer-wise transfer learning, the states of the intermediate cells are discarded. In ART, each cell benefits from the transferred information. Thus ART represents more fine-grained information transfer.
Learn to Collocate and Transfer ART discriminates between information of the corresponding position and that of all positions with collocated words. Transferring information from the corresponding position brings the semantics of the source domain for each word. We also learn to collocate with a set of positions from the source domain to represent the cross domain long-term dependency. By using the attention mechanism (Bahdanau et al., 2015), we compute the correlation for each word pair. We sum up the information from all cells in the source domain according to their correlations. Each RNN cell has a position, which indicates its order in the sequence. Each cell in the target domain attentively selects a set of positions in the source domain. ART collocates words of the positions and transfers their information. By aligning all collocated words, ART represents the cross domain long-term dependency.
We summarize the advantages of our proposed approach, ART, as follows:
· Cell-Level vs. Layer-Wise ART transfers cell-level information in a recurrent fashion. Compared with layer-wise transfer mechanisms, more fine-grained information is transferred. And ART can adapt to the seq2seq or sequence labeling tasks.
· Learn to Collocate vs. Corresponding Transfer Besides transferring information from the corresponding position, ART learns to collocate and transfer information from all potential words in the source domain. So the cross domain long-term dependency can be represented by the word collocations.
· Generality ART can be applied to different types of tasks, including seq2seq, sequence labeling, and sentence classification. In the experimental session, we verified that ART performs best in both sequence labeling (POS Tagging, named entity recognition), and sentence classification (sentiment analysis).
2 ALIGNED RECURRENT TRANSFER
In this section, we elaborate the general architecture of ART. We will show that, ART precisely learns to collocate words from the source domain and to transfer their cell-level information .
Figure 2 shows the architecture of ART. The yellow box contains the neural network for the source domain, which is a classical RNN. The green box contains the neural network for the target domain. Si and Ti are cells for the source domain and target domain, respectively. Ti takes xi as the input,
2

Under review as a conference paper at ICLR 2019

which is usually a word embedding. The two neural network overlap. The source domain's neural network transfers information through the overlapping modules. We will describe the details of the architecture below.

Source Domain

Target Domain

hs1
S1 . .

Corresponding

hsi hsn-1
Si . . Sn

hTi-1

hT i-1

hTi

+

Ti

+ Collocated

xTi

Input

Figure 2: ART architecture.

RNN for the Source Domain The neural network of the source domain consists of a standard RNN. Each cell Si captures information from the previous time step hSi-1, computes and passes the information hiS to the next time step. More formally, we define the performance of Si as:

hSi = RN N (hSi-1, xiS ; S )

(1)

where S is the parameter (recurrent weight matrix).

Information Transfer for the Target Domain Each RNN cell in the target domain leverages the
transferred information from the source domain. Different from the source domain, the i-th hidden state in the target domain hTi is computed by:

hiT = RN N (hTi-1, xTi ; T )

(2)

where hTi-1 contains the information passed from the previous time step in the target domain (hTi-1), and the transferred information from the source domain (i). We compute it by:

where f is the parameter for f .

hTi-1 = f (hTi-1, i|f )

(3)

Note that the source domain and the target domain use the same RNN function with different parameters (S and T ). Intuitively, we always want to transfer the common information across domains. And we think it's easier to represent common and shareable information with an identical network structure.

Learn to Collocate and Transfer We compute i by aligning its collocations in the source domain. We consider two kinds of alignments: (1) The alignment from the corresponding position. This always makes sense since the corresponding position has the information of the source domain. (2) The alignments from all collocated words of the source domain. This alignment is used to represent the long-term dependency across domains. We use a "concentrate gate" ui to control the ratio between the corresponding position and collocated positions. We compute i by:

i = (1 - ui)  i + ui  hiS

(4)

where

ui = (WuhiS + Cui)

(5)

i denotes the transferred information from collocated words.  denotes the element-wise multiplica-

tion. Wu and Cu are parameter matrices.

In order to compute i, we use attention (Bahdanau et al., 2015) to collocate with a set of positions in the source domain. We denote the strength of the collocation intensity to position j in the source
domain as ij. We merge all information of the source domain by a weighted sum according to the collocation intensity. More specifically, we define i as:

3

Under review as a conference paper at ICLR 2019

n
i = ij hjS
j=1

(6)

where

ij =

exp(a(hTi-1, hSj ))

n j

=1

exp(a(hiT-1

,

hjS

))

(7)

a(hiT , hjS) denotes the collocation intensity between the i-th cell in the source domain and the j-th cell in the target domain. The model needs to be evaluated O(n2) times for each sentence pair. By

following Bahdanau et al. (2015), we use a single-layer perception:

a(hTi , hjS) = va tanh(WahTi + UahjS)

(8)

where Wa and Ua are the parameter matrices. Since UahjS does not depend on i, we can pre-compute it to reduce the computation cost.

Update New State To compute f , we use an update gate zi to determine how much of the source
domain's information i should be transferred. i is computed by merging the original input xi, the previous cell's hidden state hTi-1 and the transferred information i. We use a reset gate ri to determine how much of hTi-1 should be reset to zero for i. More specifically, we define f as:

f (hiT , i) = (1 - zi)  hTi-1 + zi  i

(9)

where

i = tanh(Wxi + U[ri  hTi-1] + Ci)

zi =(Wzxi + UzhiT-1 + Czi)

ri =(Wrxi + UrhTi-1 + Cri)

Here these W, U, C are parameter matrices.

(10)

Model Training: The training process contains two stages. (1) The training for the source domain is considered as a pre-training process. It is trained by the data of the source domain. (2) With the initial parameter from (1), we train the parameters of the target domain with the data of the target domain.

3 ART OVER LSTM

In this section, we illustrate how we deploy ART over LSTM. LSTM specifically addresses the issue of learning long-term dependency in RNN. Instead of using one hidden state for the sequential memory, each LSTM cell has two hidden states for the long-term and short-term memory. So the ART adaptation needs to align and transfer information for both the long-term and short-term memory.

The source domain of ART over LSTM uses a standard LSTM layer. The computation of the t-th cell is precisely specified as follows:

ciS  tanh

oiSttS

  

=

 

ftS

  

 

TAS,b

xtS hSt-1

(11)

ctS = ctS  itS + ctS-1  ftS
htS = oSt  tanh(cSt ) Here hSt and ctS denote the short-term memory and long-term memory, respectively.

(12) (13)

In the target domain, we separately align and transfer the short-term and long-term memory from the source domain. More formally, we compute the t-the cell in the target domain by:

cTi  tanh

oitTTt

  

=

 

ftT

  

 

TAT,b

xTt f (hTi-1, hi|fh)

(14)

4

Under review as a conference paper at ICLR 2019

ctT = ctT  iTt + f (ciT-1, ci|fc)  ftT

(15)

htT = otT  tanh(cTt )

(16)

where f (hiT-1, hi|fh) and f (cTi-1, ci|fc) are computed by Eq (6) with parameters fh and fc, respectively. hi and ci are the transferred information from the short-term memory (hS1 · · · hSn in Eq. (12)) and the long-term memory (c1S · · · cSn in Eq. (13)), respectively.

Bidirectional Network We use the bidirectional architecture to reach all words' information for each cell. The backward neural network accepts the xis in reverse order. We compute the final output of the ART over LSTM by concatenating the states from the forward neural network and the backward neural network for each cell.

4 EXPERIMENTS
We evaluate our proposed approach over sentence classification (sentiment analysis) and sequence labeling task (POS tagging and NER).
4.1 SETUP
All the experiments run over a computer with Intel Core i7 4.0GHz CPU, 32GB RAM, and a GeForce GTX 1080 Ti GPU.
Network Structure: We use a very simple network structure. The neural network consists of an embedding layer, an ART layer as described in section 3, and a task-specific output layer for the prediction. We will elaborate the output layer and the loss function in each of the tasks below.
Competitor Models We compare ART with the following ablations.
· LSTM (no transfer learning): It uses a standard LSTM without transfer learning. It is only trained by the data of the target domain.
· Layer-Wise Transfer (LWT) (no cell-level information transfer): It consists of a layerwise transfer learning neural network. More specifically, only the last cell of the RNN layer transfers information. This cell works as in ART. LWT only works for sentence classification. We use LWT to verify the effectiveness of the cell-level information transfer.
· Corresponding Cell Transfer (CCT) (no collocation information transfer): It only transfers information from the corresponding position of each cell. We use CCT to verify the effectiveness of collocating and transferring from the source domain.
We also compare ART with state-of-the-art transfer learning algorithms. For sequence labeling, we compare with hierarchical recurrent networks (HRN) (Yang et al., 2017). For sentence classification, we compare with DANN (Ganin et al., 2016), DAmSDA (Ganin et al., 2016), and AMN (Li et al., 2017).
4.2 SENTENCE CLASSIFICATION: SENTIMENT ANALYSIS
Datasets: We use the Amazon review dataset (Blitzer et al., 2007), which has been widely used for cross-domain sentence classification. It contains reviews for four domains: books, dvd, electronics, kitchen. Each review is either positive or negative. We list the detailed statistics of the dataset in Table 1. We use the training data and development data from both domains for training and validating. And we use the testing data of the target domain for testing.
Model Details: To adapt ART to sentence classification, we use a max pooling layer to merge the states of different words. Then we use a perception and a sigmoid function to score the probability of the given sentence being positive. We use binary cross entropy as the loss function.
Results: We report the classification accuracy of different models in Table 2. The no-transfer LSTM only performs accuracy of 76.3% on average. ART outperforms it by 9.5%. ART also outperforms its ablations and other competitors. This overall verifies the effectiveness of ART.

5

Under review as a conference paper at ICLR 2019

Table 1: Statistics of the Amazon review dataset. %Neg. denotes the ration of the negative samples. Avg. L denotes the average length of each review. Vocab. denotes the vocabulary size.

Train Dev. Test % Neg. Avg. L Vocab.

Books

1400 200 400 50% 159 62k

Electronics 1398 200 400 50% 101 30k

DVD

1400 200 400 50% 173 69k

Kitchen 1400 200 400 50% 89 28k

Table 2: Classification accuracy on the Amazon review dataset.

Source

Target

Books

DVD

Books Electronics

Books

Kitchen

DVD

Books

DVD Electronics

DVD

Kitchen

Electronics Books

Electronics DVD

Electronics Kitchen

Kitchen

Books

Kitchen

DVD

Kitchen Electronics

Average

LSTM
0.695 0.733 0.798 0.745 0.733 0.798 0.745 0.695 0.798 0.745 0.695 0.733 0.763

CCT
0.730 0.768 0.818 0.800 0.775 0.815 0.773 0.768 0.823 0.803 0.750 0.810 0.803

LWT
0.784 0.763 0.790 0.778 0.785 0.785 0.735 0.723 0.793 0.755 0.748 0.805 0.774

DANN
0.725 0.690 0.770 0.745 0.745 0.780 0.655 0.720 0.823 0.645 0.715 0.810 0.735

DAmSDA
0.755 0.760 0.760 0.775 0.800 0.775 0.725 0.695 0.838 0.755 0.775 0.870 0.774

AMN
0.818 0.820 0.810 0.825 0.810 0.830 0.785 0.780 0.893 0.798 0.805 0.833 0.817

ART
0.870 0.848 0.863 0.855 0.845 0.853 0.868 0.855 0.890 0.845 0.858 0.853 0.858

Effectiveness of Cell-Level Transfer LWT only transfers layer-wise information and performs with 77.4% on average. But ART and CCT transfer more fine-grained cell-level information. CCT outperforms LWT by 2.9%. ART outperforms LWT by 8.4%. This verifies the effectiveness of the cell-level transfer.
Effectiveness of Collocation and Transfer CCT only transfers the corresponding position's information from the source domain. It achieves accuracy of 80.3% on average. ART outperforms CCT by 5.5% on average. ART provides a more flexible way to transfer a set of positions in the source domain and represent the long-term dependency. This verifies the effectiveness of ART in representing long-term dependency by learning to collocate and transfer.
4.3 SEQUENCE LABELING
We evaluate the effectiveness of ART w.r.t. sequence labeling. We use two typical tasks: POS tagging and NER (named entity recognition). The goal of POS tagging is to assign part-of-speech tags to each word of the given sentence. And the goal of NER is to extract and classify the named entity in the given sentence.
Model Settings: POS tagging and NER are multi-class labeling tasks. To adapt ART to them, we use a softmax layer after the ART layer to compute the tag distribution of each word. We predict the tag with maximized probability for each word. We use categorical cross entropy as the loss function.
Dataset: We use the dataset settings as in Yang et al. (2017). For POS Tagging, we use Penn Treebank (PTB) POS tagging, and a Twitter corpus (Ritter et al., 2011) as different domains. For NER, we use CoNLL 2003 (Tjong Kim Sang & De Meulder, 2003) and Twitter (Ritter et al., 2011) as different domains. Their statistics are shown in Table 3. By following Ritter et al. (2011), we use 10% training samples of Twitter (Twitter/0.1), 1% training samples of Twitter (Twitter/0.01), and 1% training samples of CoNLL (CoNLL/0.01) as the training data for the target domains to simulate a low-resource setting.
Table 4 shows the per-word accuracy (for POS tagging) and f1 (for NER) of different models. From the table, we see that the performances of all tasks are improved using transfer learning. For example,
6

Under review as a conference paper at ICLR 2019

Table 3: Dataset statistics.

Benchmark

Task # Training Tokens # Dev Tokens # Test Tokens

PTB Twitter

POS Tagging POS Tagging

912,344 12,196

131,768 1,362

129,654 1,627

CoNLL 2003 NER

Twitter

NER

204,567 36,936

51,578 4,612

46,666 4,921

Table 4: Performance over POS tagging and NER.

Task Source Target HRN LSTM CCT ART

POS Tagging PTB Twitter/0.1 0.837 0.798 0.852 0.859 POS Tagging PTB Twitter/0.01 0.647 0.573 0.653 0.658

NER NER

CoNLL Twitter/0.1 0.432 0.210 0.434 0.450 Twitter CoNLL/0.01 - 0.576 0.675 0.707

when transferring from PTB to Twitter/0.1, ART outperforms HRN by 2.2%. ART performs the best among all competitors. This verifies the effectiveness of ART w.r.t. sequence labeling. Similar to the results of sentiment analysis, the results also verifies the effectiveness of cell-level transfer and the effectiveness of collocation.

4.4 VISUALIZATION OF THE ALIGNED TRANSFER
ART aligns and transfers information from different positions in the source domain. Intuitively, we use the alignment and attention matrices to represent cross domain word collocations. So positions with stronger correlations will be highlighted during the information transfer. We visualize the attention matrix for sentiment analysis to verify this. We show the attention matrices for the short-term memory h and for the long-term memory c in figure 3.

Attention Matrix for the Long-Term Memory
very pleased w/item
. compact
. easy
to move from place
to place

0.6 0.5 0.4 0.3 0.2 0.1

Attention Matrix for the Short-Term Memory
very pleased w/item
. compact
. easy
to move from place
to place

0.35 0.30 0.25 0.20 0.15 0.10 0.05

plwe/iavtseeermyd comppfmllerpoaaaoavccttsceym..ootee plwe/iavtseeermyd comppfmllerpoaaaoavccttsceym..ootee

Figure 3: Attention matrix visualization. The x-axis and the y-axis denote positions in the target domain and source domain, respectively. Figure (a) shows the attention matrix for the long-term memory in the forward neural network. Figure (b) shows the attention matrix for the short-term memory in the forward neural network.
Effectiveness of the Cell-Level Transfer From figure 3, words with stronger emotions have more attentions. For example, the word "pleased" for the long-term memory and "easy to move" for the short-term memory have strong attention for almost all words, which sits well with the intuition of cell-level transfer. The target domain surely want to accept more information from the meaningful words in the source domain, not from the whole sentence. Notice that c and h have different attentions. Thus the two attention matrices represent discriminative features.
7

Under review as a conference paper at ICLR 2019
Effectiveness in Representing the Long-Term Dependency We found that the attention reflects the long-term dependency. For example, in figure 3 (b), although all words in the target domain are affected by the word "easy", the word "very" has highest attention. This makes sense because "very" is actually the adverb for "easy", although they are not adjacent. ART highlights cross domain word correlations and therefore gives a more precise understanding of each word.
5 RELATED WORK
Transfer learning is an important task for machine learning (Pan & Yang, 2010), and has drawn a lot of research interests in the NLP field. There are some previous works also working on transfer learning for NLP applications with deep neural networks. In Glorot et al. (2011); Zhou et al. (2016), the authors transfer the data from source domain to target domain with a deep neural network (DNN). The DNN consists of one unified encoder and two decoders of each domain. Thus if we use decoder of the target domain for source domain data, we are expected to obtain data adapted to target domain. The model has three drawbacks. First, it only uses bag-of-words of the input sentence, and is unaware of the phrase/sentence level knowledge transfer. Second, the labels of inputs are totally omitted during the transfer, i.e., the transfer learning framework cannot understand the labels. Third, it cannot be used for sequence-to-sequence labeling. Collobert & Weston (2008) uses a multi-task learning framework for domain adaptation. This work solves the domain adaptation problem from the data side. They alternatively choose a data sample from either domain to learn the parameters. The transfer learning in this approach treats the neural network as a black box. Thus it's unaware of the multi-layer representation of DNN.
Before the DNN, studies of transfer learning in NLP applications focus on instance transferring (Jiang & Zhai, 2007), feature transferring (Zhou et al., 2016), and parameter transferring (Collobert & Weston, 2008). In Jiang & Zhai (2007), the authors solve the instance weighting problem from the distributional view. They assume the target domain and the source domain have similar distributions. They implement several adaptation heuristics based on that assumption. Chiticariu et al. (2010) proposed a rule based approach for domain adaptation in NER. They design a high-level language called NERL. NERL can be used for domain-specific NER annotator.
We decide to use DNN for the transfer learning, due to its impressive performance in recent NLP applications. (Collobert et al., 2011) uses a convolution neural network for sentence modeling. The work performs well in many NLP tasks, including POS tagging, chunking, named entity recognition and semantic role labeling. Variances of DNN are also used in different applications and exhibit great values. For example, Liu et al. (2016) uses coupled-LSTM for matching question against answer. Luong et al. (2013) uses recursive neural networks for word representation learning. All these advantages inspire us to use DNN for transfer learning in NLP applications.
6 CONCLUSION
In this paper, we study the problem of transfer learning for sequences. We proposed the ART model to collocate and transfer cell-level information. ART has three advantages: (1) it transfers more fine-grained cell-level information, and thus can be adapted to seq2seq or sequence labeling tasks; (2) it aligns and transfers a set of collocated words in the source sentence to represent the cross domain long-term dependency; (3) it is general and can be applied to different tasks. We conducted extensive experiments over sentence classification (sentiment analysis) and sequence labeling (POS tagging and NER). ART outperforms all competitors over all tasks.
REFERENCES
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. 2015.
John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th annual meeting of the association of computational linguistics, pp. 440­447, 2007.
8

Under review as a conference paper at ICLR 2019
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan. Domain adaptation of rule-based annotators for named-entity recognition tasks. In EMNLP, pp. 1002­1012, 2010.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, pp. 160­167, 2008.
Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12 (Aug):2493­2537, 2011.
Hal Daume´ III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096­2030, 2016.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML, pp. 513­520, 2011.
Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In ACL, volume 7, pp. 264­271, 2007.
Zheng Li, Yu Zhang, Ying Wei, Yuxiang Wu, and Qiang Yang. End-to-end adversarial memory network for cross-domain sentiment classification. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2017), 2017.
Zheng Li, Ying Wei, Yu Zhang, and Qiang Yang. Hierarchical attention transfer network for crossdomain sentiment classification. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI 2018, 2018.
Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Modelling interaction of sentence pair with coupledlstms. CoRR, abs/1605.05573, 2016.
Thang Luong, Richard Socher, and Christopher D Manning. Better word representations with recursive neural networks for morphology. In CoNLL, pp. 104­113, 2013.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. TKDE, 22(10):1345­1359, 2010.
Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, and Zhi Jin. A comparative study on regularization strategies for embedding-based neural networks. In EMNLP, 2015.
Alan Ritter, Sam Clark, Oren Etzioni, et al. Named entity recognition in tweets: an experimental study. In Proceedings of the conference on empirical methods in natural language processing, pp. 1524­1534, 2011.
Yangqiu Song, Haixun Wang, Zhongyuan Wang, Hongsong Li, and Weizhu Chen. Short text conceptualization using a probabilistic knowledgebase. In IJCAI, pp. 2330­2336, 2011.
Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4, pp. 142­147, 2003.
Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen. Transfer learning for sequence tagging with hierarchical recurrent networks. In ICLR, 2017.
Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang, and Tingting He. Bi-transferring deep neural networks for domain adaptation. In ACL, 2016.
9

