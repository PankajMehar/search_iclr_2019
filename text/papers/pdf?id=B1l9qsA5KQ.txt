Under review as a conference paper at ICLR 2019
MENTAL FATIGUE MONITORING USING BRAIN DYNAMICS PREFERENCES
Anonymous authors Paper under double-blind review
ABSTRACT
Driver's cognitive state of mental fatigue significantly affects driving performance and more importantly public safety. Previous studies leverage the response time (RT) as the metric for mental fatigue and aim at estimating the exact value of RT using electroencephalogram (EEG) signals within a regression model. However, due to the easily corrupted EEG signals and also non-smooth RTs during data collection, regular regression methods generally suffer from poor generalization performance. Considering that human response time is the reflection of brain dynamics preference rather than a single value, a novel model called Brain Dynamics ranking (BDrank) has been proposed. BDrank could learn from brain dynamics preferences using EEG data robustly and preserve the ordering corresponding to RTs. BDrank model is based on the regularized alternative ordinal classification comparing to regular regression based practices. Furthermore, a transition matrix is introduced to characterize the reliability of each channel used in EEG data, which helps in learning brain dynamics preferences only from informative EEG channels. In order to handle large-scale EEG signals and obtain higher generalization, an online-generalized Expectation Maximum (OnlineGEM) algorithm also has been proposed to update BDrank in an online fashion. Comprehensive empirical analysis on EEG signals from 44 participants shows that BDrank together with OnlineGEM achieves substantial improvements in reliability while simultaneously detecting possible less informative and noisy EEG channels.
1 INTRODUCTION
As reported by sleep health report (Adams et al., 2017), mental fatigue is a major cause in 33% - 45% of all road accidents. In general, mental fatigue (Boksem & Tops, 2008) refers to the inability to maintain optimal cognitive performance in continuous task of the high demand of cognitive activity. Such inability in the context of driver could lead to accidents with severe consequences. Individuals may find themselves in a mental fatigue state because of lack of sleep, continuous driving for longtime, midnight driving, monotonous driving, and driving during the influence of sleeping drugs or sleep disorders (Ji et al., 2004; Ting et al., 2008).
In response to these critical issues, several methods (Jap et al., 2009; Wascher et al., 2014; Cook et al., 2007; Lal et al., 2003; de Naurois et al., 2017) have been proposed to estimate and predict the mental fatigue based on EEG and RT (Fig. 1a). Some of these methods, however, performed considerably well for some participants but failed for others due to lack of robustness. There are several challenges behind such instability and one of such problem is how to use RT effectively. RT is the most resourceful piece of information to predict mental fatigue. However, it is easily affected by the instrumental error, mind wandering or any other task non-related factor. A previous study (Wei et al., 2015) tried to overcome this problem by providing different techniques to adjust RT, they also tried removing outliers nevertheless, failed to make it work for all participants. The regression assumption of this method between EEG signals and RT is not correct. Human's RT is usually the result of preference (Izuma & Adolphs, 2013) in brain dynamics during the task, than just a single value. These preferences of human can be affected by different cognition (Mo®ckel et al., 2015) like mind wandering (Lin et al., 2016), and/or lower level of attention (Chuang et al., 2018). Therefore, the relationship between EEG and RT including the extreme/abnormal RT should be taken care in the way that reflect human brain dynamics preferences by the developed technique itself.
1

Under review as a conference paper at ICLR 2019

EEG Signals

Trial

Trial

Response Time

RT

RT

(a) Regression

Brain Dynamics Preferences



Trial

Trial

,

Pairwise RT Comparison

RT > RT

(b) BDrank

Figure 1: (a) Regression model with EEG signals. (b) BDrank model with brain dynamics preferences.
Another important problem is related to different brain regions, which are normally responsible for different functionalities. There was an attempt to choose different brain regions (Wascher et al., 2014) for a method during mental fatigue prediction but these regions of the brain are not necessary same for all participants (Gramann et al., 2006). For example, existing work (Wascher et al., 2014) used frontal theta to represent a different level of mental fatigue for all participants. In such case learning model's reliability would inevitably degrade because of possibly noisy channels chosen, on different brain regions, by the method. Some previous work (de Naurois et al., 2017), attempted to solve this issue using artificial neural network models but still failed to provide convincing results. Again these mentioned work reflect developed methods should be based on brain dynamics preferences rather than fix model or regions of brain.
To overcome the above-mentioned problems, a new approach has been proposed. We called it BDrank such that brain dynamics ranking. This approach not only learns from brain dynamics preferences for mental fatigue but also other cognitive states (Lal et al., 2003), while effectively preserving the exact ordering of RT (Fig. 1b). This approach surprisingly improved over defects of previous models and their performance due to noisy and extreme RT. Furthermore, BDrank model also proposes to use transition matrix to evaluate the high confidence BDrank (HC-BDrank) sources among different EEG channels, which contributes highly toward task performance. In order to handle large-scale EEG signals and obtain higher generalization, an online-generalized Expectation Maximum (OnlineGEM) (Roche, 2011) algorithm also has been proposed. Comprehensive empirical experiments on EEG signals from 44 participants show that BDrank model together with OnlineGEM algorithm delivers substantial improvements and robust performance while simultaneously providing the information about noisy or less informative EEG channels.

2 MATERIALS AND METHODS:

Huang et al.

Information transfer during drowsy driving

a) Experiment Paradigm: This

paper utilized the 33-channels EEG

data recorded in the previous study

(Huang et al., 2015) from 44 adult

participants while performing long

sustained attention task. The ex-

periment has been conducted us-

ing a virtual-reality (VR) dynamic

driving simulator (Fig. 2D-E). The

task involves driving on the four-

lane highway while lane-departure

events were randomly induced de-

viation toward the side of the

road from the original position.

Each participant was instructed to FIGURE 1 | Sustained-attention driving task. (A) Event-related lane-deviation paradigm. (B≠C) EEG and behavior were recorded simultaneously. (D) The
quickly respond to steer back toexperimental paradigm was implemented in a VR-based driving simulator, where a 3-D surrounding view simulated a monotonous highway scene and a vehicle was original position. A complete trial Figure 2: Sustained-attention driving taskmounted on a six degree-of-freedom dynamic platform. (E) The car speed was fixed at 100 km/h and the subject was asked to keep the car cruising on the central of
the lane.
in this study (Fig. 2A), includes 10s baseline, deviation onset, response onset, and response offset

(Fig. 2B-C). The next trial occurs within an interval of 5-10s after finishing the current trial. Each

participant completed TpcartnorcibeaselsesaspwpXroiaxtnihmdianYte.d1W.b5ye haas.sstuaFmtiooenrtahreaytaMceaahcrhktorovifaPtrlhoecise,esstphoroefcfieEnssieEtes G aSsscihTginEnd(aYlelrset{alXx.,)n2,,0ii0s}7cNn;oV=mi1cpeunftteredoetmasal(.,SN2c0h1rd1ei)ibfe-r, 2000; Hlav·Ëkov·ferent channels were recorodrerdedd. Tshiums, uwletacnaneoruecsolnystruacnt dthethsetatce osrpraecespofonthdeing response time RTi was also

collected afterwards. wake him up.

IfparocpesasrXticbyipaadnetlayfeelmlbaedsdleedepvecdtour roifndgimtehnseionexdpweirthiment, there was

past values. Representing the two time series as X = xt and

TE(Y  X) =

Y = yt, the delay embedded vector is defined as xtd =

no feedback
p(xt+u, xtd, ytm) log

top(xt+u xdt , ytm
p(xt+u xtd )

)

,

(3)

xt, xt- , xt-2 , ∑ ∑ ∑ , xt-(d-1) ; similar representation can also

2be made for ytd. The dimension of the embedding space is d,
and the delay is  . Under the assumption that the system X can

where ytm = yt, yt-, ∑ ∑ ∑ , yt-(m-1) indicating that the process X depends on m states of Y. The Equation (3) can be rewritten in

be approximated by a stationary Markov process of order d, the terms of differential entropy as the following

transition probabilities that describe the system are given by:

p(x xd ).

TE(Y  X) = H(xdt , ymt ) - H(xt+u, xdt , ytm) (1)

Under review as a conference paper at ICLR 2019

The response time is an intuitive indicator used to assess human mental fatigue. Therefore, the common practice for mental fatigue monitoring is to find a robust mapping for humans' response time (RT) to an emergent situation using the EEG signals recorded beforehand. The natural way to forecast the response time with the EEG signals is to formulate it as a regression task (Fig. 1a), namely finding a nonlinear mapping (e.g. neural network, SVR) from the EEG signals x to the corresponding RT. However, due to the easily corrupted properties of the EEG signals and the existence of extreme values in response times during data collection, focusing on predicting the exact value of a noisy and non-smooth measurement (the response time) is easier to create a near-perfectly fitted model with poor generalization performance (See Table 1 and Fig. 3 for more details). This creates a dilemma: it requires a powerful learning model to predict response time with the complex EEG signals (indeed, it is exactly our target) but it is not so significant to excessively approximate the exact value of response time, especially the extreme values.

Here comes the problem, how to find an efficient way to learn from the noisy response time while the exact value is not so significant? Actually, the RTs are defined in the totally ordered space R. The totally ordered space owns its structure meanings, which are preserved by the pairwise comparisons between the RTs. The pairwise comparisons indeed preserve the whole relative structure information between the response times while ignoring their absolute numerical information. Therefore, predicting the orderings of the pairwise comparisons can be accepted as a regularized alternative of previous regression model. Further, it makes it more flexible to consider more powerful learning unit in the proposed model.

Therefore, there is no longer requirement to estimate the RT with a regression model. Instead, the proposed approach will transform it into an ordinal classification problem, and focus on correctly preserving the whole orderings between the pairwise RT comparisons (Fig. 1b). First, the preference propositions1 could be constructed as follows,

D1 = {m}mM=1 1 = {RTm,1 > RTm,2}mM=1 1,

D2 = {m }Mm2=1 = {RTm ,1  RTm ,2}Mm2=1.

(1)

where M1 and M2 denote the number of type-1 and type-2 preference propositions. D1 denotes the type-1 preference propositions that the orderings between the RTs are significant; D2 denotes the type-2 preference propositions that the RTs in each comparison are comparable. Then, the brain
dynamics preference was constructed for each proposition with the corresponding pairwise EEG
signals recorded from each channel. For brevity of notations, new index (the notation in Eq.1)
is adopted in the following, instead of the original index used in sustained-attention driving task
(Fig. 1). Namely, the m-th proposition m  D1 D2 denotes the pairwise comparison RTm,1 > RTm,2 or RTm,1  RTm,2. And the pairwise brain dynamics preference (xn1,m, xn2,m) denotes the features recorded within the n-th channel for each preference proposition m m = 1, 2, ∑ ∑ ∑ , M1 + M2.

In this paper, the 10s baseline (Fig. 2B) as the feature vector has been adopted, which is assumed to be long enough to detect any significant changes in brain activity (Zhang, 2000). This followed by exploring the relation between the 10s baseline x ( Rk) and the preference proposition m under the following four assumptions: (1) different participants are independent during the data collection process; (2) Different EEG sensors used for recording are recorded independently from scalp without influencing other sensors (Homan et al., 1987; Teplan et al., 2002); (3) Different trials are conducted independently during the data collection process; (4) The collected response time are slightly corrupted by inherent (basically irremovable) sources of noise, but the ranking relationships are preserved to some extents.

b) Brain Dynamics Preferences: Again, instead of modelling the dependence between the RT and the 10s baseline as a regression problem, we aim to predict the orderings of the pairwise comparisons using the brain dynamics preferences, namely f (x1, x2)  . Further, a preference proposition  has three states: 1, 0, -1, denoting win (RT1 > RT2), tie (RT1  RT2) and loss (RT1 < RT2), respectively.
However, classical classification models, e.g. logistic ordinal regression (Harrell, 2001), are infeasible for our problem, due to the lack of a normalized probability definition for three states. Since two types of preference propositions D1 and D2 are considered in our problem, we tailor-define a normalized probability definition, namely first normalizing the probabilities of the states (1, -1)
1We used the term "preference" intentionally to show that brain dynamics keep changing w.r.t. human behaviours and it happens because the human brain prefers one decision over others. Therefore, we prefer to call it"preference" than "classification".

3

Under review as a conference paper at ICLR 2019

(exclusively to D1) to 1, then generalize the probability definition to state 0. To be specific, it can be mathematically formulated as follows,

 

(wT

x)[1

-



(wT

x)]

P(|w, x1, x2) = 

(wT x)

 (-wT x)[1 - (wT x)]

 =1  =0  = -1

(2)

where  (z) = 1/(1 + e-z) is the sigmoid function and  (-z) = 1 -  (z). The symbol x denotes the subtraction (x1 - x2) between the brain dynamics preferences (x1, x2). Following Weng & Lin
(2011), the probability of a tie is modelled as the geometric mean between a win and a loss, namely (wT x) =  (wT x)[1 -  (wT x)].

c) Preference State Transition: Furthermore, considering the different functions of different regions in the human brain, the relative contributions of different channels to human response time may vary a lot. For example, the information conveyed by positive channels is positive related to the RT, while negative channels may convey the information which is negative related to the RT. There are also some noisy (non-relevant) channels which are independent to the learning task. Therefore, if we directly model the EEG preferences recorded in each channel without making any distinctions about the channel state (i.e. positive, noisy and negative), the model's reliability would inevitably degrade. Note that a channel is called as "noise" if the current algorithms could not extract useful brain information with EEG signals from this channel (Alharbi, 2018; Lin et al., 2018).
In the following, a transition matrix n is introduced to characterize the reliability of each channel n w.r.t the corresponding task. Let  denote the preference proposition and (n) denote the prediction from the n-th channel. They are defined on the finite state space S = {1, 0, -1}. Then we have

 n

0 (1 - n)

n = P(|(n)) =  0 1 0  ,

(1 - n) 0 n

(3)

where Pi, j(|(n)) = P( = S j|(n) = Si). Note that we do not consider the transition between the type1 and type-2 preference propositions (i.e. P( = {1, -1}|(n) = 0) = 0 and P( = 0|(n) = {1, -1}) = 0), since the equal cases are hard to measure when do prediction. A promising approach to generalize the transition matrix n (Eq.3) is to introduce the concept of the confidence region to measure the equal cases (Pregibon et al., 1981).

Remark: The parameter n in the transition matrix n actually indicates the reliability of the n-th channel n = 1, 2, ∑ ∑ ∑ , N. It additionally helps to divide the channels into three states: (1) positive channels with n close to 1, the ranking model (Eq.2) can extract enough information from the n-th channel, and exactly predict the state of the preference proposition. (2) Noisy channels with n approximating to 0.5, the ranking model can not extract any useful information from the n-th channel. (3) Negative channels with n close to 0, the ranking model can extract enough information from the n-th channel, but the prediction states are exactly opposite to the proposition states. The identified positive and negative channels are all considered as informative EEG channels, which helps in learning reliable models for the corresponding task.
With the introduced transition matrix n, the marginal likelihood function for each proposition  can be further represented as P() = E(n) P(|(n))P((n)) . Specifically,

 [n

(wT

x)

+

(1

-

n)

(-wT

x)][1

-



(wT

xn,m)]

P(|w, n, x1, x2) =

(wT xn,m)

[(1 - n) (wT x) + n (-wT x)][1 - (wT xn,m)]

 =1  =0  = -1

(4)

For sake of simplicity, the subscripts (n, m) are omitted. Let D = D1 D2 denotes all the preference propositions and X represents the recorded EEG signals from N different channels. We further
extend our robust ordinal classification (Eq.4) for BDrank into the Bayesian version. A Gaussian prior is introduced for w (i.e., w  N(µ, )). Since the transition matrix n only depends on the parameter n, we only focus on estimate the parameter n n = 1, 2, ∑ ∑ ∑ , N in the following. Let  denote {n}nN=1, and we introduce a Beta prior for each n (i.e.,   B(,  ) = nN=1 B(n, n)). Then,

4

Under review as a conference paper at ICLR 2019

our BDrank can be formulated into a maximum a posteriori (MAP) estimate as follows,

arg max P0()P0(w)P(D|, w, X) = P0()P0(w)P(D1|, w, X)P(D2|, w, X)
 ,w

N M1

M2

= P0()P0(w)   P(m = 1|n, w, xn,m)  P(m = 0|n, w, xn,m )

n=1 m=1

m =1

(5)

N M1

M2

= B(|,  )N(w|µ, )   [n (wT x) + (1 - n) (-wT x)][1 - (wT xn,m)]  (wT xn,m ) .

n=1 m=1

m =1

Note that due to the symmetry of the state probability (Eq.2) and transition matrix (Eq.3) w.r.t. state 1 and -1, the resultant marginal likelihood (Eq.4) remains symmetry w.r.t. state 1 and -1. For simplicity, D1 is constructed using the preference propositions with state  = 1 only. M1 and M2 denote the number of type-1 and type-2 preference propositions, namely |D1| = M1 and |D2| = M2. The variable n iterates over the channels. m and m iterate over two types of preference propositions,
respectively. Now our aim is to estimate the model parameters (w and ) by maximizing Eq.5. In
principle, any solution strategies for MAP can be considered to solve this problem. See Section 3
for optimization details.

d) Reliability analysis and channel state estimation Note that our BDrank (Eq.5) indeed trains
a mixture of two complementary classifiers, which share the same parameter w. It is different from
classical mixture models, since it clusters at the channel level instead of the sample level. In particu-
lar, in terms of the positive channels with n close to 1, BDrank relies on the first classifier to update the shared parameter w. In terms of the negative channels with n close to 0, Eq.5 automatically switches to the opposite classifier which can extract correct information from the negative channels
and update the shared parameter w accordingly. Further, BDrank is robust to the noisy channels with n approximating to 0.5, because Eq.5 gives up extracting information from the noisy channels by assigning a constant likelihood (i.e. 0.5) to each brain dynamics preference. Note that the estimated n can be leveraged as the indicator to detect noisy channels with n  0.5, n = 1, 2, ∑ ∑ ∑ , N. See Fig. 4 in the experimental section for more details.

3 OPTIMIZATION METHODS

In this section, we describe a generalized Expectation-Maximization (EM) algorithm (Dempster
et al., 1977) to solve the proposed BDrank (Eq.5). Since the feasible region of n is restricted to [0, 1], the gradient-based optimization methods would make our solution inaccurate and inefficient.
The EM algorithm is an efficient iterative procedure to compute the MAP problem in presence of latent variable (m(n) in Eq.5). EM avoids calculating the derivative to the expectation of the latent variable directly, and resorts to a surrogate lower bound to optimize. Therefore, EM, a silver bullet
for MAP with latent variable, can significantly simplify the optimization over parameter n for Eq.5.

3.1 GENERALIZED EM FOR BDRANK

For each type-1 proposition m, we introduce an auxiliary variable m(n)( {1, 0}) for each channel,

representing

the

consistency

between

the

preference

proposition

m

and

the

prediction

(n)
m

given

by the n-th channel.

Specifically,

(n)
m

=

1

denotes

the

prediction

(n)
m

given by the first classifier

is

consistent

with

the

preference

proposition

m,

and

(n)
m

=

0

denotes

the

prediction

(n)
m

estimated

by the second classifier is consistent with the preference proposition m. We can therefore find an

equivalent formulation of our BDrank model involving the auxiliary variable  = {m(n)}mM=1 1.

P(D, , , w|X) = P0()P0(w)P(D1, |, w, X)P(D2|w, , X)

  N
= P0()P0(w)

M1

P(m

=

1,

(n)
m

|n,

w, xn,m)

M2

P(m

= 0|n, w, xn,m )

n=1 m=1

m =1

(6)

N
  = P0()P0(w)

M1

n (wT xn,m)

(n)
m

(1 - n) (-wT xn,m)

1-m(n) [1 - (wT xn,m)]

M2

(wT xn,m )

.

n=1 m=1

m =1

5

Under review as a conference paper at ICLR 2019

Now, we can deal with the joint distribution directly, which leads to significant simplifications for optimization. The complete log likelihood can be written as

N M1

N M2

log P(D, , , w|X) = log P0() + log P0(w) +   log[1 - (wT xn,m)] +   log (wT xn,m ) (7)

n=1 m=1

n=1 m =1

 N
+

M1

(n)
m

log

n



(wT

xn,m

)

+

(1

-

(n)
m

)

log(1

-

n

)

(-wT

xn,m

)

.

n=1 m=1

Expectation Step In the expectation step, we first calculate the expected value of the auxiliary

variable

(n)
m

w.r.t its posterior distribution

P(m(n)|, w, m = 1, xn,m) n = 1, 2, ∑ ∑ ∑ , N, m = 1, 2, ∑ ∑ ∑ , M1:

EP(m(n) | ,w,m =1,xn,m ) [m(n) ]

=

P(m = P(m

1,

(n)
m

|n,

w,

xn,m)

= 1|n, w, xn,m)

=

1+

1
(1-n) (-wT xn,m)[1-(wT xn,m)] n (wT xn,m)[1-(wT xn,m)]

 (m(n) ),

(8)

where



(m(n))

denotes

the

degree

of

the

consistency

between

the

prediction

(n)
m

and

the

preference

proposition m. Then, the expected value of the complete-data log likelihood function w.r.t the

posterior expectation of the auxiliary variable  = {m(n)}mM=1 1 can be represented as follows:

  L

(w,

)

=

N
[(n
n=1

-

1)

log

n

+

(n

-

1)

log(1

-

n)]

-

1 2

(w

-

µ )T

-1(w

-

µ)

+

N n=1

M1 m=1

log[1

-

 (wT

xn,m)]

   N
+

M2 N
log (wT xn,m ) +

M1

(m(n)) log n + (1 - (m(n))) log(1 - n)

n=1 m =1

n=1 m=1

(9)

 N
+

M1

(m(n)) log  (wT xn,m) + (1 - (m(n))) log  (-wT xn,m) + constant.

n=1 m=1

Maximization Step In the maximization step, we maximize the objective function Eq.9 w.r.t the
model parameters  and w, respectively. In terms of , we set the gradient of Eq.9 w.r.t n to zero and obtain the following estimate for n:

nnew

=

Mm=1 1 (m(n)) + n - 1 , M1 + n + n - 2

n = 1, 2, ∑ ∑ ∑ , N.

(10)

In terms of w, due to the complexity of the sigmoid function, we do not have a closed form solution
for w and we need to use gradient-based optimization methods. In the following, we adopt the LBFGS to optimize w, the objective function L (w) consists of the second, the third, the fourth and the sixth parts of Eq.9 and the gradient function g(w) can be represented as follows,

   g(w)

=

--1

(w

-

µ

)

+

N n=1

M1 m=1

1- 2(1

2 (wT - (wT

xn,m)

1 xn,m

)

)

xn,m

+

N n=1

M1
(
m=1

(m(n)

)

-



(wT

xn,m

))xn,m

 +

1 2

N n=1

M2
(1
m =1

-

2

(wT

xn,m

))xn,m

.

(11)

wnew can be solved with L-BFGS using L (w) and g(w). The EM algorithm then iterates the E-step

and M-step until convergence is achieved.

3.2 ONLINE GEM FOR BDRANK
The generalized EM approach introduced in Section 3.1 is inefficient for large-scale datasets, because we need to iteratively calculate the gradient with respect to parameters  and w over all samples during each maximization step. Motivated from the stochastic approximation literature, we introduce an online-generalized Expectation Maximization (OnlineGEM) approach, which resorts to stochastic mini-batch optimisation to learn the parameters. To be specific, OnlineGEM approximates the updated  and w in batch EM with a single sample or mini-batch samples. Since a mini-batch samples cannot be a perfect approximation for the whole dataset, we interpolate between the new and former estimations with a decreasing step-size k2, as in Liang & Klein (2009).
2k = (k + 2)-0 , where k is the number of iterations and 0.5 < 0 < 1. The smaller the 0, the larger the update k, and the more quickly we forget (decay) our old parameters. This can lead to swift progress but also generates instability.

6

Under review as a conference paper at ICLR 2019

Before the k-th iteration, we randomly downsample a mini-batch Dk from the preference propositions D. The number of two types preference propositions in Dk are M1 and M2, which are much smaller than the corresponding total size M1 and M2, respectively.

The expectation step remains similar. The difference is that we only need to calculate the posterior

expectation

of

the

auxiliary

variable

(n)
m

over

the

mini-batch

D

k.

In the maximization step, we maximize the objective function, calculated on the mini-batch Dk,
with regard to model parameters  and w. In terms of parameter n, since its marginal distribution belongs to exponential family, we could perform the stochastic update in the space of sufficient statistics (Cappe¥ & Moulines, 2009). n denotes the noisy estimate of the sufficient statistic for n.

n

=

M1 M1

mD k

 (m(n) ),

(12a)

(k)
n

=

(1

-

k )n(k-1)

+

k n ,

(12b)

nnew

=

(k)
n

+

M1 + n

n - 1 + n -

2

,

n = 1, 2, ∑ ∑ ∑ , N.

(12c)

In terms of parameter w, the above practice is infeasible due to its non-exponential marginal distribution. Inspired by the stochastic gradient EM algorithms in Cappe¥ & Moulines (2009), we perform
the stochastic update in the original space. First, a local optima regression weight w(k) can be estimated via iterative optimization over the mini-batch Dk, using L-BFGS algorithm. Then we interpolate between the local optima and former estimations to form a global approximation.

w(k) = L-BFGS(L (w), g(w), Dk)

(13a)

wnew = (1 - k)wold + kw(k).

(13b)

The convergence issues of the proposed online generalized EM algorithm are the analogues of the discussion given by Cappe¥ & Moulines (2009) for their stochastic gradient EM Algorithms. The existence of such links is hardly surprising. In view of the discussions in Section 3 of Cappe¥ &
Moulines (2009), the online update rule (Eq.13b) could also be seen as a stochastic gradient descent formula, namely wnew = wold + k(w(k) - wold) .

4 EMPIRICAL ANALYSIS

In this section, we demonstrate the reliability of the proposed BDrank (Eq.5) with EEG signals from forty four participants.

Data Preprocessing: EEG preferences for each participant has been generated as follows: (1) the
trials of each participant were randomly divides into two parts: 50% for training and 50% for
test; (2) EEG preferences were constructed according to the pairwise comparisons between the RTs. To be specific, the type-1 preference propositions D1 were constructed with RT comparisons (RTm,1, RTm,2), which satisfies RTm,2 < min(RTm,2 + 1, 2 ◊ RTm,2) < RTm,1; the type-2 preference propositions D2 were constructed with RT comparisons (RTm ,1, RTm ,2), which satisfies RTm ,2 < RTm ,1 < min(RTm ,2 + 3, 4 ◊ RTm ,2). It is notable that 1 > 3 > 0 and 2 > 4 > 1 control the difference in the RT comparisons simultaneously, we empirically set 1 = 1; 2 = 1.5; 3 = 0.8; 4 = 1.3 for all participants in our experiment setting. Considering the time delay among the channels in the
time domain, Fourier transform (Welch, 1967) has been applied to EEG signals to transform time-
series into frequency domain. Further, to avoid overhead computation, EEG power within 0-30Hz
has been selected, which is considered to be the most relevant to the RTs (Huang et al., 2015).

Baselines and Metrics: We compared BDrank with one regression model Support Vector Regres-

sion (SVR) (Smola & Scho®lkopf, 2004) and one ordinal classification model Logistics Ordinal Re-

gression (LOR) (Harrell, 2001). First, we aggregate the predictions from different channels using

a

simple

voting

scheme,

namely

^m = sign

nN=1

(n)
m

1(n

> ) - 1(n

< 1-)

.

(n)
m

denotes the

predicted state (1 means win and -1 means loss) for (RTm1, RTm2) by the n-th channel, using the brain

dynamics preference (xn1,m, xn2,m). ^m is the final estimated order for (RTm1, RTm2) by aggregating the

predictions

(n)
m

over all channels.

1() is an indicator that returns one if the argument is valid and

returns zero otherwise.

7

Under review as a conference paper at ICLR 2019

Then, we introduce two metrics to measure the performance of BDrank model from different per-

spectives. First, we adapted the Wilcoxon-Mann-Whitney statistics (Yan et al., 2003) to evaluate the

accuracy

(in

%,

higher

is

better)

over

all

pairs,

namely

1 M1

mM=1 1 1(m

=

^m).

Further,

we

investigate

the reliability of BDrank in terms of preserving the global ordering w.r.t RTs. Note that a totally

ordered set could be equally represented by a fully directed graph, where the fully directed graph

can be further encoded by its degree sequence. Therefore, we first collected the indegree sequences3

(Becirovic, 2017) of the constructed directed graph using the predicted RTs and then measured the

discrepancy between the predicted indegree sequences and ground truth using the root-mean-squared

errors (smaller is better). See Supplementary for the detailed description.

Note that we only trust the predictions from informative channels with reliability n >  or n < 1 - .  is set to 0.85 for all participants in our experiment. In terms of SVR and LOR, consider the scale difference between the EEG signals between different channels, we train a SVR/LOR model
for each channel and aggregate the results from different channels to calculate the final predictions
using the majority voting scheme. Since there is no mechanism for SVR and LOR to evaluate the
channel state, we trust all the channels by default. Further, we only calculate the two metrics on the type-1 preference propositions D1, since the state of the type-2 preference propositions D2 is hard to evaluate when do prediction.

Parameter Initialization: In terms of the weight w, we randomly initialized w in [-1, 1]. In terms
of the channel reliability n, we assumed all channels were relevant to the task beforehand, and randomly initialized the channel reliability n, n = 1, 2, ∑ ∑ ∑ , N in [0.5, 1]. The L-BFGS implementation was downloaded from Granzow, where we used the default parameters. In terms of the hyperparameters (µ, ) for w, we adopted the standard Gaussian distribution, namely w  N(0, 1). In terms of the hyperparameters (n, n), as we intended to eliminate the effects of noisy channels, we adopted a strong non-informative prior for n, namely n = n = 20, n = 1, 2, ∑ ∑ ∑ , N, according to Bishop (2006). In terms of the maximum iteration number, we set MaxIter = 30 in our experiment to ensure
the algorithm converged for each participant. In terms of the minibatch size R, we set the sample
ratio to 10% and 5% for type-1 and type-2 preference propositions, respectively. The parameters for
LOR were optimized with L-BFGS with the default setting for all participants. In terms of SVR, we
resorted to 5-fold cross validation to find the best parameters for each participant.

4.1 EMPIRICAL RESULTS OF BDRANK ON BRAIN DYNAMICS PREFERENCES

The accuracies of LOR and BDrank for training and test EEG preferences are presented in Table 1. For SVR, we followed standard procedure and optimization for each participant to predict RTs with training and test EEG signals. We adopted the Wilcoxon-Mann-Whitney statistics to measure the accuracy of the predicted RTs w.r.t. ground truth (shown in Table 1).

Table 1: Training and test accuracy (in %). Higher is better, the best is marked in gray

Participant SVR
Train LOR BDrank SVR
Test LOR BDrank

P1 98.16 91.66 98.96 68.56 71.22 77.04

P2 98.45 94.73 97.13 76.81 69.12 77.93

P3 100 77.24 95.26 68.49 53.12 84.05

P4 99.40 88.65 99.08 61.20 61.63 71.93

P5 100 81.31 94.92 76.04 64.63 81.21

P6 100 97.10 99.85 78.04 68.46 68.06

P7 100 96.37 97.45 69.59 70.99 73.45

P8 100 85.42 97.40 66.90 54.97 79.64

P9 100 84.35 97.33 67.01 67.18 72.97

P10 100 73.67 93.60 79.23 59.18 82.09

P11 100 82.42 99.43 73.34 52.96 68.92

P12 98.31 89.97 97.99 67.58 69.87 76.24

P13 98.73 89.79 98.48 69.45 70.50 72.95

P14 99.82 85.19 96.47 69.47 61.18 67.87

P15 99.69 91.83 98.60 85.38 76.27 87.64

P16 100 99.01 99.88 73.73 69.52 73.74

P17 100 88.38 98.95 75.38 50.35 63.08

P18 100 86.06 100.00 55.60 60.87 65.53

P19 100 91.90 99.68 66.55 82.73 83.85

P20 98.78 91.65 99.03 71.87 60.67 74.62

P21 99.64 88.66 96.23 75.03 65.10 74.38

Participant SVR
Train LOR BDrank SVR
Test LOR BDrank

P23 100 91.63 97.13 69.70 69.80 71.40

P24 100 98.08 99.79 72.84 74.24 82.06

P25 97.13 88.86 94.00 67.25 72.02 74.06

P26 100 92.10 99.19 80.89 63.80 72.61

P27 99.47 90.97 95.90 72.71 73.62 79.12

P28 100 84.00 92.29 66.17 66.46 68.56

P29 99.88 86.91 98.59 77.24 63.05 81.69

P30 99.98 92.00 96.60 76.59 77.12 79.99

P31 100 87.36 94.64 81.11 73.09 75.57

P32 100 90.84 92.61 79.54 72.15 77.67

P33 100 80.20 85.70 80.64 73.20 79.34

P34 100 86.10 92.32 73.83 69.21 79.30

P35 99.93 91.40 99.09 78.39 69.53 80.11

P36 99.97 89.03 96.37 70.99 71.31 73.97

P37 99.95 93.47 98.64 75.34 70.01 76.14

P38 100 93.52 99.21 61.9 84.42 86.93

P39 98.21 90.86 97.89 86.92 69.85 87.23

P40 99.09 92.19 95.94 76.51 65.87 77.47

P41 100 93.42 96.74 69.89 70.02 70.40

P42 100 82.37 93.41 68.21 77.20 77.38

P43 100 81.87 96.20 65.09 84.89 88.47

P22 100 84.83 85.95 68.22 69.98 77.28
P44 99.59 81.80 76.04 63.06 70.41 74.16

From Table 1, we observe that: (1) SVR achieves the highest accuracies (100% accuracy) on the training EEG data for almost all participants. However, test accuracies for most participants are inferior to other models, 21 participants for LOR and 35 participants for BDrank out of 44 participants. This observation is consistent with our previous discussion that regression-based models are easily overfitting to the training datasets, especially when extreme values (RTs in our problem) exist. (2) BDrank shows significant improvements over SVR and LOR in terms of test accuracies.

3We only consider the indegree sequence because the indegree and outdegree of a vertex can be uniquely
determined when the overall degree of each vertex is fixed. The indegree of vertex vt can be calculated as deg-(vt ) = mN1(vt )[1(^m = 1) + 0.5 ◊ 1(^m = 0)] + mN2(vt )[1(^m = -1) + 0.5 ◊ 1(^m = 0)], where N1(vt ), N2(vt ) denote the index set of the pairwise comparisons with the RT of trial t (vertex vt ) appearing in the first and second position, respectively.

8

Under review as a conference paper at ICLR 2019

In particular, the number of participants with test accuracy above 75% are 17 and 25 for SVR and BDrank, respectively. This observation is consistent with our statement that classification-based models, served as a regularized alternative for the regression-based models, can effectively circumvent the overfitting caused by non-smooth RTs and preserve the ordering corresponding to RTs. (3) However, simple classification-based extensions (e.g. LOR) do not enjoy the benefits, which only achieves comparable test accuracies with SVR (outperforming SVR on only 21/44 participants). It is because they are not robust to the noisy data, while it is a common phenomenon when dealing with EEG signals. (4) Note that, compared to the fine-tuned SVR for each participant, fixed parameters were adopted for all participants in the experiment of LOR and BDrank. It further verifies the superior robustness of ordinal classification models compared to regression-based models.

To further investigate the reliability of BDrank in terms of preserving the global ordering corresponding to RTs, we first collected the indegree sequences of the constructed directed graph using the predicted RTs and then measured the indegree discrepancy between the calculated indegree sequences and the ground truth using the root-mean-squared error (RMSE) (shown in Table. 2).

Table 2: Training and test RMSE (in #). Smaller is better, the best is marked in gray

Participant SVR
Train LOR BDrank SVR
Test LOR BDrank

P1 1.52 4.06 0.92 14.30 13.74 10.94

P2 2.51 4.69 3.78 19.52 23.08 19.45

P3 0.00 16.60 3.16 22.95 31.36 10.68

P4 1.23 9.54 0.74 30.11 27.96 19.37

P5 0.00 29.77 7.52 39.45 53.02 25.12

P6 0.00 3.31 0.11 16.80 22.61 10.60

P7 0.00 5.30 4.67 29.69 33.03 28.59

P8 0.00 13.79 1.82 27.01 34.91 12.31

P9 0.00 28.86 3.85 53.23 46.77 32.92

P10 0.00 42.19 11.38 38.90 62.14 27.63

P11 0.00 16.04 0.65 23.56 35.40 16.13

P12 2.01 6.38 1.73 20.32 20.02 14.82

P13 1.26 5.53 0.91 13.71 13.71 12.67

P14 0.39 11.66 3.18 21.55 26.61 21.75

P15 0.64 8.42 1.71 14.42 23.03 11.54

P16 0.00 1.25 0.39 26.66 30.98 26.32

P17 0.00 8.12 0.52 16.40 28.80 12.95

P18 0.00 4.90 0.00 16.24 15.43 3.35

P19 0.00 8.63 2.91 28.40 24.52 19.38

P20 3.84 15.02 2.15 47.88 62.99 32.09

P21 1.56 20.88 7.51 39.85 56.10 40.92

Participant SVR
Train LOR BDrank SVR
Test LOR BDrank

P23 0.00 16.60 5.05 42.89 46.80 44.58

P24 0.00 3.31 0.45 25.60 22.47 14.89

P25 6.08 15.98 7.70 40.55 36.34 32.64

P26 0.00 19.20 1.94 41.12 64.61 32.46

P27 2.22 17.59 9.65 44.93 41.59 37.91

P28 0.00 27.85 13.36 49.09 48.48 39.16

P29 0.98 22.97 2.79 39.85 31.45 24.13

P30 0.22 14.68 6.21 36.77 40.34 29.46

P31 0.00 28.61 10.07 39.03 53.32 38.94

P32 0.00 17.74 16.75 37.63 48.30 40.12

P33 0.00 36.52 28.45 35.72 47.65 39.23

P34 0.00 23.72 13.62 39.49 35.17 34.7

P35 0.26 7.71 1.02 16.90 22.46 15.26

P36 0.19 11.56 4.07 24.19 26.83 24.18

P37 0.26 7.46 1.20 21.14 25.55 18.03

P38 0.00 6.22 1.18 15.44 13.59 8.8

P39 2.10 6.05 1.81 9.15 18.60 9.13

P40 1.38 5.88 3.39 21.64 22.05 14.26

P41 0.00 11.70 6.58 46.41 47.83 42.25

P42 0.00 22.95 22.79 35.09 26.00 23.86

P43 0.00 15.26 12.28 26.92 20.95 9.09

P22 0.00 31.94 34.12 60.94 59.79 52.11
P44 0.67 11.39 15.43 20.37 18.10 16.40

From Table. 2, we observe that: (1) On the training datasets, SVR could exactly recover the indegree sequence (RMSE equalling to 0), which means SVR preserves the global ordering corresponding to RTs; while LOR and BDrank could not fully fit the training datasets. (2) Note that, on the test datasets, BDrank shows surprisingly good generalization performance and maximum preserves the global ordering corresponding to RTs.

To further demonstrate the superiority of our BDrank, we calculated the indegree sequences using the predicted RTs for participants P19, P38, P42, P43 with the most representative performance (Fig. 3). Similar observations can be found for other participants.

Indergee

200 Ground Truth SVR Train Bdrank Train
150
100 SVR: 0
BDrank: 2.91 50

Indergee

Ground Truth 150 SVR Train
Bdrank Train
100
SVR: 0 50 BDrank: 1.18

Indergee

Ground Truth 250 SVR Train
Bdrank Train 200
150
100 SVR: 0 BDrank: 22.79
50

Indergee

150 Ground Truth SVR Train Bdrank Train
100
SVR: 0 50 BDrank: 12.28

0 0 50 100 150 200
Trial Index (P19)

0 0 50 100 150
Trial Index (P38)

0 0 100 200
Trial Index (P42)

0 0 50 100 150
Trial Index (P43)

Indegree

200 Ground Truth SVR Test BDrank Test
150
100 SVR: 28.4
BDrank: 19.38 50
0 0 50 100 150 200 Trial Index (P19)

Indegree

Ground Truth 150 SVR Test
BDrank Test
100
SVR: 15.44 50 BDrank: 8.8
0 0 50 100 150 Trial Index (P38)

Indegree

Ground Truth 250 SVR Test
BDrank Test 200
150
100 SVR: 35.09 BDrank: 23.86
50
0 0 100 200 Trial Index (P42)

Indegree

150 Ground Truth SVR Test BDrank Test
100
SVR: 26.92 50 BDrank: 9.09
0 0 50 100 150 Trial Index (P43)

Figure 3: Indegree sequence for BDrank and SVR (closer is better). The root-mean-squared error (RMSE) was also measured between the estimated indegree sequences and the ground truth.
From Fig. 3, we observe that: (1) On the training datasets, the indegree sequences predicted by SVR completely overlaps with the ground truth; while the indegree sequence predicted by BDrank fluctuates locally but retains the overall trend. (2) On the test datasets, the indegree sequences predicted by BDrank still closely aligns with the ground truth with slight fluctuates; while the indegree sequences predicted by SVR fluctuates significantly and fails to maintain the trend with the ground truth, e.g. P19, P43. (3) It is interesting to note that the indegree sequences predicted by SVR usually fluctuates heavily for low indegree trials (denoting small RTs) and high indegree trials (denoting large

9

Under review as a conference paper at ICLR 2019

RTs). It means that SVR over-estimates the RTs with small values and under-estimates the RTs with large values. It is consistent with our claim that regression-based model is not suitable for the tasks with non-smooth response variable.

4.2 NOISY CHANNEL DETECTION

We also investigated the reliability of our BDrank from the perspective of noisy channel detection.
According to our analysis, the parameter n in the transition matrix n actually indicates the channel reliability. Hereafter, we leverage n as the channel reliability indicator to detect noisy channels. Fig. 4 lists the noisy channels (marked in red) detected with 0.15  n  0.85 , n = 1, 2, ∑ ∑ ∑ , N.
C35

C30

C25

C20 Positive Noisy
C15 Negative

C10

C5

C0 P0

P9 P18 P27 P36 P45

Figure 4: Reliability of different channels for forty four participants estimated by BDrank. Each column denotes the states of 33 channels for each participant. The channels with estimated reliability 0.15  n  0.85 are marked in red.
Fig. 4 shows that: (1) the noisy channels is universally exist among the EEG signals. There are total 42 among 44 participants with at least one noisy channels detected. For example, the 33-th channel is recognized as the noisy channel by BDrank for almost all participants. It is reasonable since the 33-th channel is the non-EEG channel, which is generally acknowledged as the non-relevant channel to any tasks; (2) For each participant, most channels are reliable, which ensures we can always find enough support to training our BDrank; and (3) The detected noisy channels varies from participant to participant, and do not possess the transitivity property between participants. Because the noise can arise due to (i) intrinsic non-EEG channel, e.g. the 33-th channel; (ii) channels for lateral mastoid references, e.g. the 23-th and 29-th channel (Chatrian et al., 1985); and (iii) improper experimentation or artifacts (Lin et al., 2018).

5 CONCLUSION
This work proposes a BDrank model to assess the state of mental fatigue. The efficacy of BDrank model was demonstrated using EEG data collected in sustained driving task from 44 participants. This model has been further combined with an online-generalized Expectation Maximization (OnlineGEM) algorithm to provide a continuous update in the model. BDrank model utilized a unique methodology with a regularized alternative, i.e. ordinal classification, to circumvent overfitting to the extreme values of RTs. It has been demonstrated that the overall performance of BDrank can be significantly improved with the introduction of the transition matrix, which enables the technique to evaluate the reliability of informative EEG channels while detecting noisy EEG channels. Empirical results show that BDrank combined with the OnlineGEM algorithm delivers significant improvement over SVR and LOR in terms of global ranking preservation.
In this work, the cooperation mechanism among channels is simplified as a weighted voting system, while different trials are viewed independently. We intend to further formulate it with more complex mechanisms, such as Markov decision process (MDP), to conduct learning and decision making simultaneously. Some previous (Chen et al., 2016; 2015) studied the decision making process among crowd (noisy) workers, which is promising to our setting to investigate the cooperation mechanism among noisy channels. Efforts are underway to apply this approaches in future work.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Robert J Adams, Sarah L Appleton, Anne W Taylor, Tiffany K Gill, Carol Lang, R Douglas McEvoy, and Nick A Antic. Sleep health of australian adults in 2016: results of the 2016 sleep health foundation national survey. Sleep Health: Journal of the National Sleep Foundation, 3(1):35≠42, 2017.
Nader Alharbi. A novel approach for noise removal and distinction of eeg recordings. Biomedical signal processing and control, 39:23≠33, 2018.
Ema Becirovic. On social choice in social networks, 2017.
Christopher M Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
Maarten AS Boksem and Mattie Tops. Mental fatigue: costs and benefits. Brain research reviews, 59(1):125≠139, 2008.
Olivier Cappe¥ and Eric Moulines. On-line expectation≠maximization algorithm for latent data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593≠613, 2009.
GE Chatrian, E Lettich, and PL Nelson. Ten percent electrode system for topographic studies of spontaneous and evoked eeg activities. American Journal of EEG technology, 25(2):83≠92, 1985.
Xi Chen, Qihang Lin, and Dengyong Zhou. Statistical decision making for optimal budget allocation in crowd labeling. Journal of Machine Learning Research, 16(1):1≠46, 2015.
Xi Chen, Kevin Jiao, and Qihang Lin. Bayesian decision process for cost-efficient dynamic ranking via crowdsourcing. Journal of Machine Learning Research, 17(217):1≠40, 2016.
Chun-Hsiang Chuang, Zehong Cao, Jung-Tai King, Bing-Syun Wu, Yu-Kai Wang, and Chin-Teng Lin. Brain electrodynamic and hemodynamic signatures against fatigue during driving. Frontiers in neuroscience, 12:181, 2018.
Dane B Cook, Patrick J OConnor, Gudrun Lange, and Jason Steffener. Functional neuroimaging correlates of mental fatigue induced by cognition among chronic fatigue syndrome patients and controls. Neuroimage, 36(1):108≠122, 2007.
Charlotte Jacobe¥ de Naurois, Christophe Bourdin, Anca Stratulat, Emmanuelle Diaz, and Jean-Louis Vercher. Detection and prediction of driver drowsiness using artificial neural network models. Accident Analysis & Prevention, 2017.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pp. 1≠38, 1977.
Klaus Gramann, HJ Mu®ller, B Scho®nebeck, and G Debus. The neural basis of ego-and allocentric reference frames in spatial navigation: Evidence from spatio-temporal coupled current density reconstruction. Brain research, 1118(1):116≠129, 2006.
Brian Granzow. A matlab implementation of L-BFGS-B. https://github.com/bgranzow/ L-BFGS-B. Accessed July 4, 2017.
Frank E Harrell. Ordinal logistic regression. In Regression modeling strategies, pp. 331≠343. Springer, 2001.
Richard W Homan, John Herman, and Phillip Purdy. Cerebral location of international 10≠20 system electrode placement. Electroencephalography and clinical neurophysiology, 66(4):376≠382, 1987.
Chih-Sheng Huang, Nikhil R Pal, Chun-Hsiang Chuang, and Chin-Teng Lin. Identifying changes in eeg information transfer during drowsy driving by transfer entropy. Frontiers in human neuroscience, 9:570, 2015.
11

Under review as a conference paper at ICLR 2019
Keise Izuma and Ralph Adolphs. Social manipulation of preference in the human brain. Neuron, 78 (3):563≠573, 2013.
Budi Thomas Jap, Sara Lal, Peter Fischer, and Evangelos Bekiaris. Using eeg spectral components to assess algorithms for detecting fatigue. Expert Systems with Applications, 36(2):2352≠2359, 2009.
Qiang Ji, Zhiwei Zhu, and Peilin Lan. Real-time nonintrusive monitoring and prediction of driver fatigue. IEEE transactions on vehicular technology, 53(4):1052≠1068, 2004.
Saroj KL Lal, Ashley Craig, Peter Boord, Les Kirkup, and Hung Nguyen. Development of an algorithm for an eeg-based driver fatigue countermeasure. Journal of safety Research, 34(3): 321≠328, 2003.
Percy Liang and Dan Klein. Online em for unsupervised models. In Proceedings of human language technologies: The 2009 annual conference of the North American chapter of the association for computational linguistics, pp. 611≠619. Association for Computational Linguistics, 2009.
Chin-Teng Lin, Chun-Hsiang Chuang, Scott Kerick, Tim Mullen, Tzyy-Ping Jung, Li-Wei Ko, ShiAn Chen, Jung-Tai King, and Kaleb McDowell. Mind-wandering tends to occur under low perceptual demands during driving. Scientific reports, 6:21353, 2016.
Chin-Teng Lin, Chih-Sheng Huang, Wen-Yu Yang, Avinash Kumar Singh, Chun-Hsiang Chuang, and Yu-Kai Wang. Real-time eeg signal enhancement using canonical correlation analysis and gaussian mixture clustering. Journal of Healthcare Engineering, 2018.
Tina Mo®ckel, Christian Beste, and Edmund Wascher. The effects of time on task in response selection-an erp study of mental fatigue. Scientific reports, 5:10113, 2015.
Daryl Pregibon et al. Logistic regression diagnostics. The Annals of Statistics, 9(4):705≠724, 1981.
Alexis Roche. Em algorithm and variants: An informal tutorial. arXiv preprint arXiv:1105.1476, 2011.
Alex J Smola and Bernhard Scho®lkopf. A tutorial on support vector regression. Statistics and computing, 14(3):199≠222, 2004.
Michal Teplan et al. Fundamentals of eeg measurement. Measurement science review, 2(2):1≠11, 2002.
Ping-Huang Ting, Jiun-Ren Hwang, Ji-Liang Doong, and Ming-Chang Jeng. Driver fatigue and highway driving: A simulator study. Physiology & behavior, 94(3):448≠453, 2008.
Edmund Wascher, Bjo®rn Rasch, Jessica Sa®nger, Sven Hoffmann, Daniel Schneider, Gerhard Rinkenauer, Herbert Heuer, and Ingmar Gutberlet. Frontal theta activity reflects distinct aspects of mental fatigue. Biological psychology, 96:57≠65, 2014.
Chun-Shu Wei, Yuan-Pin Lin, Yu-Te Wang, Tzyy-Ping Jung, Nima Bigdely-Shamlo, and ChinTeng Lin. Selective transfer learning for eeg-based drowsiness detection. In Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on, pp. 3229≠3232. IEEE, 2015.
Peter Welch. The use of fast fourier transform for the estimation of power spectra: a method based on time averaging over short, modified periodograms. IEEE Transactions on audio and electroacoustics, 15(2):70≠73, 1967.
Ruby C Weng and Chih-Jen Lin. A bayesian approximation method for online ranking. Journal of Machine Learning Research, 12(Jan):267≠300, 2011.
Lian Yan, Robert H Dodier, Michael Mozer, and Richard H Wolniewicz. Optimizing classifier performance via an approximation to the wilcoxon-mann-whitney statistic. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 848≠855, 2003.
Guoqiang Peter Zhang. Neural networks for classification: a survey. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 30(4):451≠462, 2000.
12

Under review as a conference paper at ICLR 2019

SUPPLEMENTARY:

The discrepancy between the predicted indegree sequence and the ground truth:

Assume

there

exists

a

ranking

list

S

:

a

>

b

>

c

>

d,

we

can

constructed

6

(=

4◊3 2

)

preference

propositions {i}i6=1 with the pairwise comparisons. Further, these could be equally represented by

the fully directed graph with four vertices as Fig.5a. Then, the indegree of each vertex vi can be

calculated following deg-(vi) = mN1(vi) 1(m = 1) + mN2(vi) 1(m = -1), where N1(vi), N2(vi) denote

the index set of the pairwise comparisons with the vertex i appearing in the first and second position,

respectively. Therefore, the indegree sequence (in ascending order) of Fig.5a can be represented as

X = [0, 1, 2, 3] for the corresponding vertex sequence dcba. Note that we only consider the indegree

sequence because the indegree and outdegree of a vertex can be uniquely determined when the

overall degree of each vertex is fixed.

Figure 5: Directed graph representation of the ranking list S : a > b > c > d. Left Panel: the ground truth, where {i}i6=1 is the constructed preference proposition. Right Panel: the prediction, where {^i}i6=1 is the predicted order for each pairwise comparison. Note that we use the edges marked in red (e.g. {^1, ^3, ^6}) to denote the wrong predictions.
As for each prediction model, we first calculate the final estimation {^i}6i=1 for each pairwise comparisons using a simple voting scheme, detailed in Section 4. The corresponding fully directed graph is shown in Fig.5b. We calculate the indegree of each vertex vi following deg-(vi) = mN1(vi)[1(^m = 1) + 0.5 ◊ 1(^m = 0)] + mN2(vi)[1(^m = -1) + 0.5 ◊ 1(^m = 0)]. Note that we consider the tie case (e.g.  = 0) here since the number of votes for state 1 and -1 may be comparable. Next, the predicted indegree sequence for the vertex sequence dcba is X^ = [1, 0.5, 2.5, 2]. Therefore the discrepancy between the predicted indegree sequences and ground truth using the root-mean-squared errors (RMSD) can be calculated as follows,
RMSD(X ) = Ti=1(Xi - X^i)2 = 0.7906. T
Then, we can use this value to investigate the reliability of the proposed method in terms of preserving the global ordering.

13

