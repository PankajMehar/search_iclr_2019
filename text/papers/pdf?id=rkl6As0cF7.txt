Under review as a conference paper at ICLR 2019
PROBABILISTIC RECURSIVE REASONING FOR MUTLI-AGENT REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs. In this paper, we introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning (RL). Our hypothesis is that it is beneficial for each agent to consider how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the best response and then improve their own policy. We develop decentralizedtraining-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to offer a new idea of embedding opponent modeling into the multi-agent RL context.
1 INTRODUCTION
In the long journey of creating artificial intelligent (AI) that mimics human intelligent, a hallmark of an AI agent is its capabilities of understanding and interacting with other agents (Lake et al., 2017). At the cognitive level, the real-world intelligent entities (e.g. rats, humans) are born to be able to reason about various properties of interests of others (Tolman, 1948; Pfeiffer & Foster, 2013). Those interests usually indicates unobservable mental state including desires, beliefs, and intentions (Premack & Woodruff, 1978; Gopnik & Wellman, 1992). In everyday life, people use this inborn ability to reason about others' behaviors (Gordon, 1986), plan effective interactions (Gallese & Goldman, 1998), or match with the folk psychology (Dennett, 1991). It is known that people can use this reasoning ability recursively; that is, they engage in considering what others believe about their own beliefs. A number of human social behaviors have been profiled by the recursion reasoning ability (Pynadath & Marsella, 2005). Behavioral game theorist and experimental psychologist believe that reasoning recursively is a tool of human cognition that is equipped with evolutionary advantage (Camerer et al., 2004; 2015; Goodie et al., 2012; Robalino & Robson, 2012).
Constructing the models of other agents, also known as opponent modeling, has a rich history in the multi-agent learning (Shoham et al., 2007; Albrecht & Stone, 2018). Even though equipped with modern machine learning methods that could enrich the representation of the opponent's behaviors (He et al., 2016; Foerster et al., 2018; Yang et al., 2018), those algorithms tend to only work either under limited types of scenarios (e.g. cooperative games, mean-field games), pre-defined opponent strategies (e.g. Tit-fot-Tat in iterated Prisoner's Dilemma), or in cases where opponents are assumed to constantly return to the same strategy (Da Silva et al., 2006). Recently, a promising methodology from game theory ­ recursive reasoning ­ has become popular in modeling the opponents (Gmytrasiewicz & Durfee, 2000; Camerer et al., 2004; Gmytrasiewicz & Doshi, 2005; De Weerd et al., 2013b). Similar to the way of thinking of humans, recursive reasoning refers to the belief reasoning process where each agent considers the reasoning process of other agents, based on which it expects to make better decisions. Importantly, recursive reasoning allows an opponent to reason about the modeling agent rather than being a fixed type; the process can therefore be nested in a form as "I believe
1

Under review as a conference paper at ICLR 2019
that you believe that I believe ... ". Despite some initial trails (Gmytrasiewicz & Doshi, 2005; Von Der Osten et al., 2017), there has been little work that tries to adopt this idea into the mutli-agent deep reinforcement learning (DRL) setting. One main reason is that computing the optimal policy is prohibitively expensive (Doshi & Gmytrasiewicz, 2006; Seuken & Zilberstein, 2008).
In this paper, we introduce a probabilistic recursive reasoning (PR2) framework for multi-agent DRL tasks. Unlike previous opponent models, each agent is to consider how the opponents would react to its potential behaviors before it tries to find the best response for its own decision making. By employing variational Bayes methods to model the uncertainty of opponents' conditional policies, we develop decentralized-training-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, and prove their convergence in the self-play scenario. Our methods are tested on the matrix game and the differential game. The games come with a non-trivial equilibrium where conventional gradient-based methods find challenging. We compare against multiple strong baselines. The results justify the unique value provided by agent's recursive reasoning capability throughout the learning. We expect our work to offer a new angel on incorporating conditional opponent modeling into the multi-agent DRL context.
2 RELATED WORK
Game theorists take initiatives in modeling the recursive reasoning procedures (Harsanyi, 1962; 1967). Since then, alternative approaches, including logics-based models (Bolander & Andersen, 2011; Muise et al., 2015) or graphical models (Doshi et al., 2009; Gal & Pfeffer, 2003; 2008), have been adopted. Recently, the idea of Theory of Mind (ToM) (Goldman et al., 2012) from cognitive science becomes popular. An example of ToM is the "Recursive Modeling Method" (RMM) (Gmytrasiewicz et al., 1991; Gmytrasiewicz & Durfee, 1995; 2000), which incorporates the agent's uncertainty about opponent's exact model, payoff, and recursion depth. However, these methods follow the decisiontheoretic approaches, and are studied in the limited context of one-shot games. The environment is relatively simple and the opponents are not RL agents.
The Interactive POMDP (I-POMDP) (Gmytrasiewicz & Doshi, 2005) implements the idea of ToM to tackle the multi-agent reinforcement learning problems. It extends the partially observed MDP (Sondik, 1971) by introducing an extra space of models of other agents into the MDP; as such, an agent can build belief models about how it believes other agents know and believe. Despite the added flexibility, I-POMDP has limitations in its solvability (Seuken & Zilberstein, 2008). Solving I-POMDP with N models considered in each of recursive level with K maximum level equals to solving O(NK ) PODMPs. Such inherent complexity requires high precision on the approximation solution methods, including particle filtering (Doshi & Gmytrasiewicz, 2009), value iteration (Doshi & Perez, 2008), or policy iteration (Sonu & Doshi, 2015). Out work is different from I-POMDP in that we do not adjust the MDP; instead, we provide a probabilistic framework to implement the recursive reason in the MDP. We approximate the opponent's conditional policy through variational Bayes methods. The induced PR2-Q and PR2-AC algorithms are model-freee and can practically be used as the replacement to other multi-agent RL algorithms such as MADDPG (Lowe et al., 2017).
Our work can also be tied into the study of opponent modeling (OM) Albrecht & Stone (2018). OM is all about shaping the anticipated movements of the other agents. Traditional OM can be regarded as level-0 recursive reasoning in that OM methods model how the opponent behaves based on the history, but not how the opponent would behave based on what I would behave. In general, OM methods have two major limitations. One is that OM tends to work with a pre-defined target of opponents; for example, fictitious play (Brown, 1951) and joint-action learners (Claus & Boutilier, 1998) require opponents play stationary strategies, Nash-Q (Hu & Wellman, 2003) require all agents play towards the Nash Equilibrium, so do Correlated Q-learning (Greenwald et al., 2003), Minimax-Q (Littman, 1994), and Friend-or-foe Q (Littman, 2001). These algorithms become invalid if the opponents change their types of policy. The other major limitation is that OM algorithms require to know the exact (Nash) equilibrium policy of the opponent during training. Typical examples include the series of WoLF models (Bowling, 2005; Bowling & Veloso, 2001a; 2002) or the Nash-Q learning (Hu & Wellman, 2003), both of which require the Nash Equilibrium at each stage game to update the Q-function. By contrast, our proposed methods, PR2-Q & PR2-AC, do not need to pre-define the type of the opponents, thus are robust to opponents that change their behaviors. Neither do our methods require to know the equilibrium beforehand.
2

Under review as a conference paper at ICLR 2019

i(ai|s)
<latexit sha1_base64="9bKfh/RHlTgZEBHV+2CkNURaKVUdWf54C3S7/DdSNsOfTbh/JYsZc0=">AAAB+3icbVC7TsMwFL30hpWr1cJoerolJYwHFsFoik1USqWSa5WqEwwBcFgZKJMBsDYAiW0iTYf6UkNhulSK4OT6m7ZvWVHcSLeybQHUVQVh8v8iLssCADCEDKsE/yoA+gwb8fT4cPT4dbQoCZoWOI9dK29rOoj3rPnulXvan+P6H+3BOFnmStOjNvO8Wt1yuVrYWa+9s/Ybm34CpWtu4l3vbZ2O79/t6YP+7fVMNByqWqS4WixRJhLDRZFJQxhC7PZK8rbo8GVi5nUAnzQa1lmEaxza022ok4lkxKaQH58PaTdjsvf+9X+CZrz++O4w9FKUKxhSaJKxqOzy2cxLq9BUIf8gFsWCAxgjRBGrsAj209se1AuSVpR/sezRu50R9YlWVsZzN3ZJf13JVna16dYtFldcpe+rZMOgHZGiaJVuODyfmplQeiI7UIm7wPg7Gqg0D0bMe/SeB1oROoIJwrCEFVSPmndeCsJVtJNd1gqYRu2nlhNWGC3pG1OY0MU2jKTvUKTZ4RTGHJuMGxCHhtxKQuo5SQXKHzV21HnNp09aPpYQMhnGoRhXlSlgNIJBIormvhI7aeSTdHXCgfG1CyzkwOzlZWSqEA9vUpQkMtesR56np4Rn9S8eXL9//ejOS6iSQ54iIuYvkJS0FJOWTNFw0UikNjnlSDIQcKqCRQjElAMmeBKBdoFw8ZSgYngmEk0plMwbEkcZlzgciYsgk2IcSZ0Vy0MiCOat7oyQl1ndAJXv+7zxMuOWnmcX311q6m050rNv2M645fingUAzMFJO1ADIjFyqE6KgALTpfxDQHhaBY6hQeAIU0Rgn8eAIjUP3K87ApVevrVHmfa9rYWOz/FWasx2PxKd0xI/ZrgvD6H/MIMfHWJkW8T/+dgoG=V=i</g=la=t<e/lxait>exit>
Mutual Effects


<latexit sha1_base64="IgZ2i+vH9rQ818CU2l8AybgRecLXHUPkDnkA6XL2g09ErZRuhdpMAI=">AAAB/XicbVC7TsMwFL0pr1Je4bGxWLRIZaBKWGCsBANjkehDakNPxluXKEe51r16XjEiRi270SECqVoUePJErXWLBAhwgAxiMJpX/sYOPNE3vucNGkMHOK0BHKzkp6q3h6td0czr6398y9ffFyYjMzp6URd25nG8v8rKtL7SwS8usLar/qWkVXywt9rs6bGx5ubtW7/9bi72TewkN0VFJiZSLS0QTOiolI4eyJFZsa+PVFepVQzM0QLeupamamUtN5bWLsaCkQ4O9fUDl6bt/+vsBPL84idjf+v8pqF1QKxsSENtzdq6FUUFMy9vxEHPc3FBACkxajwB2NklLhXd3+i6tD1UYidnalTdHesrrNGxZGZWyft1QR3R2VSVu6ntbpRFqp+TgJkZQ0IF/vEiznkZiExizWgFDqCrW2lw+CdoXdke3SSPkTiA8piNSOFUiaFqJ7hTwqrx19lXaIsdNWSHOscpljpgoudRTRNseEYFTkyqHJuo0j7MkahQ9Ao2ndbUUeWIlF2Dq/rRwgd0Gu36W6MHDgkoi3SaQE0hpEkl6TuQm+NMFvIdXKnjRUoLpfDpTUIaZhYbD9yZSD8rNAxdHq/83p9quIJ/D3i6n8tRlAIdkn40XsVpSEQ6nUGNgqBwypPSGOhI0COFQKIR12GOgSEBes1oHxhSmYnAmiImb0MkwVkcQzGcWimsGgATWS0MyG0E4CaMx5g/QenZDEn0zvi/yquXNUE34Fqvr3GlLNx1rah9ZxHi9HgQK7hmyCMMr+gHwjEAlUZX4RDipqFKUAlxcCDe/pAAMr4B/6BemP4VAVkevr1rUfvr12MXR3qzNW3qbeOjdfOWfugD26sw/uM/HNYwHu1U8aA1AN=H=<lf/gla=t<e/xliat>exit>

i(a i|s)

i(ai|s)
<latexit sha1_base64="i8K65gwHRV70wYFLiTdNHAkAhaIRFJlL8Yq3dmbpjOTRD0pkjaisk=">AAAB9XicbVAD9LTwgJBEJO3zD1Lif8hQvC1PNXJqmZIC5ChZ4gQI+bt5set9CEiTiRBwh4I+YTy+CUjOgBBIXHuvLHHWmZzgwYO273uZsm7dmnUQI5yDf+8wesNdAYrW/+6LnN/f/G4BaX5FQ8sJCJWTOvKlLwXd3k65el45KQEscyG1ZNcdq717v7Ze2VT1Wb1jXc12jtM7/LObVh3Zd37Zd3vfdv2PDH4RuzFRWUdJ8wtEqEydqo0gFkkrFvqVBDrKCiZmn4AJnJVaDMTMxewCN2oRL4VFMxAVHoAEaqwSesYDqX84l/8f1fg2BNKV8mk1ljxembyWYHCx/9SAMh78FkoCexckBorBsFSru9c9BrdKmePJVsXYBscff2fY9kFz4mhv9k48us7lJt+XcrKOsgkVyaJ8Ol5cEmSXZC2KFj43Di1wC/VdTgSuR6rJR1YI1UhpGHGjvFpd8qEdCzYtm+5C6lbWhGHhF+OEyp4nAVuq2omDjmbMySRjgVLPakcAd6wSgx5SOqqWS/oXyRY+9RkSfdTqW8WfWAk1QqCplsdECYsbPmKl6ju+RkJFqvEd6aeTG6GGLAk9dTkAbYKbjPGeSEypNqxP8X/8vzYnq4JCna9d/M8TlXIjkp4Mj7VhSQMUxsaMIkwn4cS0hIKNU0IsFAMTDCZiYiRxkCP5CXJjJBZgooxZtmA9SFp4ZIvQZVWJQsvYuoGVkBbAobheVNMsavG4rC5L2m+vRdExVqz5q9y6XdPLVX6rm3dXZHrH58k4DQTRNOk4oQBhIOeXoAEIgNeXbqEEAOZbDSqCAgCV4aBlCeg4cB1le54dFc61c5dF+6dcjd0Z+dpzj1sprlrijz+GAePnO84wAd+cEazZxG9gx<p/ZlNaB<t/exliat>exit>


<latexit sha1_base64="gI2Zi+vH9r8Q18CUl2A8bygRceLXUHPknDkAX62L0g9ErZRuhdMpIA=">AAAB/XicbVC7TsMwFL0pr1Je4bGxWLRIZaBKWGCsBANjkehDakPNlxuXEKe5r161XjEiiR72S0ECqVoUPeEJXrWLABhwAgxiJMXp/sYOPEN3vcuGNkMOH0KHBKzkpq63ht60dczr639y89ffFyjYMzp6RUd25n8Gv8KrtLS7Sw8usLarq/WkVXywt9rs6bGx5utb7W9/bi27Tewk0NVFJiZSSLQ0TOiolIe4JyFZsaP+VFepVQzM0QeLupmamatUN5bWLsCaQk4O9fDU6lbt/+vsBP8L4idjf+8vqp1FQKxsSEtNzdq6UFUFyMv9xEHP3cFBACkxajBw2NlkLhXd3+i6tD1UYidnalTdHesrrNGxGZZWfyt1RQR3V2VSu6ntpbFRqpT+JgkZQ0IFv/iEnzZkiExiWzFgqDrCW2lw+CodXdek3SSPTkiAp8iNOSUFaiFqJ7Thwqxr19XlaIsdWNHSsOpcljgpuodRTRsNEeYFkTqyJHouj0M7akQhA92ondbUeUIWFlD2q/Rrwg0dGu636WMHgDko3iaSEQ0hEplk6TQu+mNFMvIXdKnRjoULpDfTpUIZahYbDy9ZS8DNrAxHd/q389puqJI/D3i6n8tlRAIkdn4X0sVpSQEn6UGgNqBwypPGSOh0ICOFQKIR1G2gOSBEes1oHxhSYmAnimIm0bMkwVkQcGzWcmisGgTAWSM0Gy0E4CaM5x/geQZnDEn0zv/iyqXuUNE34Fvq3rlGLN1xrah9xZHiH9Qg7KmhCyMMr+gHwjEAlUZ4XRDpiqFKUAlxcCDe/pAAMr4/B6BemP4VAkVver1rUfv1r2MXR3qzN3WbqeOjdfOfWguD26sw/Mu/HYNHw1uU8Aa1AN==Hl</flga=t</elxiat>exit>

i(a i|s)

1 2 4
3


<latexit sha1_base64="Mb3/5DOzXVmpDUMH5miuITey/0x/0/k2rOYVFUrvjGa0FhSnaP6sA=">AAACAnicbZDLSgMxFIbP1eFKu31ttN1uFpXK34AiSZrYhUEApHaLjZBtztdoFsqnTAhLlsoxXK9sBQdDvqxWTZJNJpMpQG8zxOTkhIciQkjIZlRLGz4c+8CVXpcuuXFCjDiE1rqU/dhwz5r9cuYxnXchb5CaeWwiD+kEf4/Pz/P/IOSTTmn/9H32POmOltHOaN8cWb63PuwzCsL4ti0Lyv7FJmcVL/Oar2ta+bs2xamuv2bdVs7dUTyRKURlkhFtaAGJ4iEXIg2kf2zK5wWolZDxNGBtaGq5pYp5bTctuSSx4pDtDjn0tOOW3735dL4xcZ+Tv/3ZV5KPxpWUKRR0ouwNcGxjm9UHoLchjHVgjgWAMCNIZK1GkaXtur27LXldckTOto+9vkJ0yh4I0pr4OdLDs2oe1YLwEMsBNOj27rbqB2aWWfknj6mASrNF5gsjuHBNQoQvkxg5C1OX4v2AoVN7cK2fvkS1oSdkQZhKOQOlRpepqw4TrqFyT9TdFEWvLNtpCKVfhqjRUjdjNR8NMqb1Ek0DRgi9ToxP6u7DASIpsVEVeImhq10R5wR6WodiFGD6aB0MgapkYuMYCIjITcL2J1N90BSqpKQv6ydVGSoHWCo816CQ6H3yHTaGWtaLbdiU9PP9e5UnUPQzHPa5y17YK6ROJPxdSoKFsjW0JpoShDEZhPSxEdQokkHGgfmBqhM0nmgJD5piOMUDmaED4hmwg/oIrlkI5EqE+tMI9tELmDtERZEJJvUw8Zi1eYEedh+3ZrlpW1XaiWqdl7lrVybnr5lF63m7hcfRRIlhjHCws6gHAe7iA6cPRQXQ2DuhDoAMp4wNBIRPAWoIAz/oFAHKeb9IZaTXe9WLKO9eWrxB/fTr13ofKoVYtz+85zCZkn75kI+d+fFwDPWY5SZwc/0O<h/Zliaet<e/lxait>exit>

i(a

i|s, ai)

Angle

of

Agent

i: <latexit sha1_base64="j+3L9thu6cwTpljahA3+u36rtmTGozxHWTWQs9467UGpbo8ZLzC0U=">AAAB6nicbVZAC97SgwNBEFIJb2LPXeozF3x+RFSrW1t0GFhUPwBEKq7tzBZroxD50KgBLhWSwVjEm8wgsHkJSEf5Yi2dneE82mSGvzbM14ujMd074NCIWR3PI6CIjNYhUZKitsvLH4wiOZ/W+ztNmfB+sQKnlTX0IwwTf8xHhj4v+hPpl95zmQSHKNFOQkdAifu9djegot+bOm1yuvbrOa8+sXbdm0t77m+t/wePFbRO+7fti5+k4beJCUwMru9N5UiMsaYyxx1WN6MSCGqSGV6FCN4gCwkVuKs3GkW04E0pN1hEOgFeNSAeoYEN3oMzLB9z9hSRPXvRPsKLTSqEPaJcYPJZ9ypMig6HU9iGIeU5jKCFKVnH1qFjqirOvishRXL3nUJLqR7ALFbktTnkXWk4Xqwk5KlCM85+KHhy/1P3hwzFGgLI2i6l6Q8StbWpsMxSzy3OMTU9hDgOmqdUTcDtzJZE6+VN+naVnBnhOCB2YIS7Oye7MV9RSjQRtSNmuA9/rGBxlx6UdoxIcIWtGZV9INRwlxj2bRUk+tg0W6SRu+gJr+j6EQbhGUTK/Pd3ARdk0YUjxrY2bdVRmYC4vs/jaevb0pU6MwZ2us/YE/2ypWtJk1ISuSX2XfsBSZlmkmhqBMU5bPn+ZRTomAdCpciYoZjLxZamQ3pSkW5Q9lmbECx0AlRJTlhijtatZdCkget3BTRW3Z1mx51nb8SnvbIa3p5iLb8K+y699D/SubzM2sju6WXMvIzZ3iAtOWlb+mCBBmHBRzpDwCBZ0y1oBBAxYMdRQPgMVMuroQvgDn0SYe9XOHAJeXnuYD9VlEa8cH6zJZM03b7ehZDq5zUrPzHr4zbnRjCPU7g=I<+f/lgaBtYRex+QiItg>==</latexit>

(ai, a
<latexit sha1_base64="LKlz3w7Bo7v9QlpahyhnLkLGSxTh4cYWmInSpf+r579Rrc8GojXMI=">AAACLnicbVDLSgMxFLM3Uj2V/62qvquqnkQs3TwbVIaUKooWmGbXGcj6SL4KLggKrkisFsYqB0K/nQarXcfuSdjNGA1NDmMJ5kMOShUycShrYl7LB3eL5jr8V+dh0CIUBaGi3IWfoz/bpDVdFMpHBXqgwccDhChnH3OP5O5ueSdYeIBLN+JfGMaddZt+c+ksdTJGzx8wYunJLqS9enlCz3OMrzcq2/vEJpxGcfenOlrEyr1kRQsRVCaqd0SRwSYHUVWseoqGAaCoZxy6xGutuaBaHY5sbLUFEaMSQo0uCBxw02+vAByGdp2+CO/fXcUjOGlYuiYKy8P0TSs9OhIztQgOgv4DYhp5nyPiCGsgZKj7defJPn+2RUT9di4BJVetiw8gA2wAydr25+JCTN97Gt71Q+OdfjMnFC1NgU0grpJab2rLFdxOV6ul5vb3dVFdf2dmRZWcopqXf9UcEPNDSG3Ghj/v/dWIw3ulXt7DyLqdm4grP8Jldz4hnTKUdkBXKTYVLXvgO5AJI7cfntS4BoPzQfkUTBMSNOSWGrUyocOQ9KL0tbnBpKRumYhZoNNbKRzcCop0YVjvY+CAMn4miJqVI2DLA/0GhCNoKSqydNMpWOeKO68mJeX5nRe9tsmgXa0VTroxkQl4M1qT9+2OJZBFcAo/NTQ6wRY8aERwy1LANw1psXsM9k5LTY1/bM9+9rxgdfoi/f1a0SchNsjuNGJPNMQeJzJykZ5hMscVc0ua4ChHMH3BeTGESekD5LRoojPHjaQ4EYNiaGTJnmCrV5LjF07QVQL8RJ7puaGJcCaKmczGDZPBVfkuvCq9R/v+VkvHb+vRksqX+DqF5Vytem/0IKji9zX2aQYbIugpohWBIFxV0qjICrAoH2A1lCVDRDARzFi0EjOxl7CR4hK3Xqt4zhHqhxfn6z9nl360XmpEQ2z31YXnTREmG+Z/2xZC9hhfk9Fw3Stj28oAZGxg=m=pq<g/l==a<t/elxiat>exit>

i|s)

i(ai|s)

i(a

i|s, ai)

Opponent Effect

i(ai|s, a
<latexit sha1_base64="LtJxsXQL0vo7j19reMJ8AmJb0EjKSFByBeeDC0SB/vo3YDEf5gF8k=">AAACAXicbVCDL7TSgsMxwFL31ThXW7coW+rwRtI0LIEYbloGJFQqigKBRVlwxog0JjuCJR7gpwYiWc0EQ+fUolBu1KLJ4sz2q0twVcUxemJSbADJepCGCmceHhNvV1+gLYGQhSIiJVu/v2QDtj3b/o3D1apODtwBttyJPPZsDecwnXznO3vk7tHzuj8ixD5wOpl7HTefjfbVmmptFfhWcWFxla5zpbq3hkaVW9lfvWfN2Nzybyttr3ee2aKCkmNokJaoQJ0WSEP8UJiQN2DfaywvKomZ4aIB2VzNTNSOcnjtUmhNSJHcHeihcz21rv3LvBRLesZb+X76655lKULxSBJQx3oeh0cjRxd9UoBI87EgvCxmgMBYG2sjk9jer3y9bkrhvx3wGtNmyFnLpKEWnhf83gPz6ho2j9U0YS9lYRdxtS27zy40JS7cVEmQdDANscESzcenkZUcDh7wR/7T9lsAfq3HX5TsEkrp1AYK3JTHTFhAWqhuSMY6csK9fZVS0nLDUUij3nEyGwa1lbIq5yJomj+VMkadsaHtICTJOoAPQKdHoV01HCnpBAZI6rMaMHyRWqilDFj4BJ0aImpiYvM80UmqJio/jJN1BIqpcvKjyUcKSHfCdMg1ZDYDj1zTUGsW95DdY/VM9/NerJDv7o4n9N1WIPtm4n7kcRTJQaqJYPYBUQ0HlHGDO/kkLjxROFzCpfEWSURoy0oyHyxQmClimWg8TmNwr4UQgyMs8c1dRE+ml9hBiKJogk1R3odeRuOVC5O07jy3ytuLKkm7VdvllXybnL5tcFs678hjfiILlsjwJwGFDUPwdIiUzHqIrMhEwV1BKmEW4BBgBg7pUhGgVcA7jhzPMXMqryXvqFlx3P612ovP1aWbnr2DyMmWV+3e4syA+cwvOzB/2IHsP1l+QtOcH=S<5/lhna<t/exliat>exit>

i)

Angle of Opponent:

(ai, a
<latexit sha1_base64="6oQ0s/NHsz0+t8rdchm4RrTopoMtZcGrvm75l5Nx7Wz9/yq1vyiEwc=">AAACMHicbZDBNS8sNAEFIYUn9fblu/+/RqhV6269LGLWwaFCgBpS2bJEGi1x04LKCrnpqsUYsCKvoQ0pbZOV1smONm3bkHpJh5tk2wNM01FqKJ7/CUlO5e/8VClF60Uo6VCDEIqW57/CTSZduFDXWyi8MsvHLMz5P3DLL3fMzuCQSRLK4ONlqH7ac7V46oys0NTNT00xzOOTzc/e/MUFFu5bmcFWlxa5XZtidssfrW5m10qkmkirtIAGaElVUzKqI6ywA1U5EzSyxmNDcO00NYpJ9eJpYhBAgFgnFl40FEN3yYOcc5Xv97wqlwUpbjmEMRXz0+wpBvQYvY0Ire2jMELeGcQorFtGrRqI53i9UVvdlUPSt+gCYN2tt+oTebhBZnre2bDz4+b33da6xiR/6inCohQaYpVxkRr0fE20tmJ3bzOARn8hmXyLHd9noZMeG+D9ZOP8bmuBy7NSyLW6b1KYr1WeLJcrmjbgMDqkPCb/ncGGQ5llRSQtUfRyWGOV/ewBR1w0BiUokj+G+m1vCJ0Q4amjFWh6gyrTUazu0u6Bq5iRjWlgkdlqrAy1Un0gvQUTILDffqRpoZygr8gSYLI16qxr8ubYHTwRwE0G481bspsH4tCDfbINc2ikLTNRUK6p57PyIscNyGiaJQTafORIzKHMpjtEO5DF31gVa+2WMm0/HT+x0Vb5qarDHI/z7F9j6caZsLJq9RGpsPbjxJoDjDUslpWsANucBpY4Seq7IkTFSSKi/+HmcAlzIIxaJgKZRvP2WuLQSBKmwl7E/Sm4mgzHLJFVgJT3j7b18myTwRo/nBlQ+rc/52zXaneO+9FWjPLkf+LinOOXbqRlJ6itEpMCNO8QhNrFhsA+g6Yb4nMaME6eaVOiECYCTHtqAzGeFOkP7hvC1qVP7h1zYnX1lwYn+pP13Wk52lXNMYObKuaBMfZeXl1bh9hA05zYPOLqS2JYK=q<r/Yla=<t/elxaite>xit>

i|s)

i(a i|s)i(ai|s, a i)

Best Response

Agent Execution

Considering Impact on Opponent

Figure 1: Diagram of our probabilistic recursive reasoning framework. PR2 decouples the connections
between agents by Eq. 3. 1 : agent i takes the best response after considering all the potential consequences of opponents' actions given its own action ai. 2 : how agent i behaves in the environment serves as the prior for the opponents to learn how their actions would affect ai. 3 :
similar to 1 , opponents take the best response to agent i. 4 : similar to 2 , opponents' actions are the prior knowledge to agent i on estimating how ai will affect the opponents. Looping from step 1
to 4 forms recursive reasoning.

Despite the recent success of applying deep RL algorithms on the discrete (Mnih et al., 2015) and continuous (Lillicrap et al., 2015) control problems in the single-agent case, it is still challenging to transfer these methods into the multi-agent RL context. The reason is because learning independently while ignoring the others in the environment will simply break the theoretical guarantee of convergence (Tuyls & Weiss, 2012). A modern framework is to maintain a centralized critic (i.e. Q-network) during training, e.g. MADDPG (Lowe et al., 2017) and multi-agent soft Q-learning (Wei et al., 2018); however, they require strong assumptions that the parameters of the policy networks are fully observable (so does LOLA by Foerster et al. (2018)), letting alone the centralized Q-network potentially prohibits the algorithms from scaling up. By contrast, our approach is capable of employing decentralized training with no need to maintain a central critic; neither does it need to know the parameters of the opponents' policies.

3 PRELIMINARIES

For an n-agent stochastic game (Shapley, 1953), we define a tuple (S, A1, . . . , An, r1, . . . , rn, p, ), where S denotes the state space, p is the distribution of the initial state,  is the discount factor for future rewards, Ai and ri = ri(s, ai, a-i) are the action space and the reward function for agent i  {1, . . . , n} respectively. Agent i chooses its action ai  Ai according to the policy i i (ai|s) parameterized by i conditioning on some given state s  S. Let us define the joint policy as the collection of all agents' policies  with  representing the joint parameter. It is convenient to interpret the joint policy from the perspective of agent i such that  = (i i (ai|s), --ii (a-i|s)), where a-i = (aj )j=i, -i = ( j )j=i, and --ii (a-i|s) is a compact representation of the joint policy of all complementary agents of i. At each stage of the game, actions are taken simultaneously. Each
agent is presumed to pursue the maximal cumulative reward (Sutton et al., 1998), expressed as



max i( ) = E

tri(st, ati, at-i) ,

t =1

(1)

with (ati, at-i) we can define

tshaemsptaletef-raocmtio(nQi i,-fu-n-icit)i.oCn obryreQspio(nsdt,inagti,lya,t-fio)r=theEgamle=w0 itlhr

(infinite) time horizon, i(st+l, ati+l, at-+il) .

3.1 NON-CORRELATED FACTORIZATION ON THE JOINT POLICY

In the multi-agent learning tasks, each agent can only control its own action; however, the resulting

reward

value

depends

on

other

agents'

actions.

In

other

words,

the

Q-function

of

each

agent,

Q

i 

,

is

subject to the joint policy  consisting of all agents' policies. In the previous studies, one common

approach is to decouple the joint policy assuming conditional independence of actions from different

agents (Albrecht & Stone, 2018):

 (ai, a-i|s) = i i (ai|s)--ii (a-i|s).

(2)

The study regarding the topic of "centralized training with decentralized execution" in the deep RL domain, including MADDPG (Lowe et al., 2017), COMA (Foerster et al., 2017), MF-AC (Yang et al., 2018), Multi-Agent Soft-Q (Wei et al., 2018), and LOLA (Foerster et al., 2018), can be classified

3

Under review as a conference paper at ICLR 2019

into this category (see more clarifications in Appendix B). Although the non-correlated factorization of the joint policy simplifies the algorithm, this simplication is typically invalid by ignoring the agents' connections, e.g. impacts of one agent's action on other agents, and the subsequent reactions from other agents. One might argue that during training, the joint Q-function should potentially guide each agent to learn to consider and act for the mutual interests of all the agents; nonetheless, a counter-example is that the non-correlated policy could not even solve the simplest two-player zero-sum differential game where two agents act in x and y with the reward functions defined by (xy, -x y): following by Eq. 2, both agents are reinforced to trace a cyclic trajectory that never converge to the equilibrium (Mescheder et al., 2017).
It is worth clarifying that the idea of non-correlated policy is still markedly different from the independent learning (IL). IL is a naive method that completely ignore other agents' behaviors. The objective of agent i is simplified to i(i ), depending only on i's own policy i compared to Eq. 1. As Lowe et al. (2017) has pointed out, in IL, the probability of taking a gradient step in the correct direction decreases exponentially with the increasing number of agents, letting alone the major issue of the non-stationary environment due to the independence assumption (Tuyls & Weiss, 2012).

4 MULTI-AGENT PROBABILISTIC RECURSIVE REASONING

In the previous section, we have shown the weakness of the learning algorithms that build on the noncorrelated factorization on the joint policy. Here we introduce the probabilistic recursive reasoning approach that aims to capture how the opponents believe about what the agent believes. Under such setting, we devise a new multi-agent policy gradient theorem. We start from assuming the true opponent conditional policy --ii is given, and then move onward to the practical case where it is approximated through variational inference.

4.1 PROBABILISTIC RECURSIVE REASONING

The issue on the non-correlated factorization is that it fails to help each agent to consider the

consequence of its action on others, which could lead to the ill-posed behaviors in the multi-agent

learning tasks. On the contrary, people explicitly attribute contents such as beliefs, desires, and

intentions to others in daily life. It is known that human beings are capable of using this ability

recursively to make decisions. Inspired by this, here we integrate the concept of recursive reasoning

into the joint policy modeling, and propose the new probabilistic recursive reasoning (PR2) framework.

Specifically, we employ the nested process of belief reasoning where each agent simulates the

reasoning process of other agents, thinking about how its action would affect others, and then make

actions based on such predictions. The process can be nested in a form as "I believe [that you believe

(that I believe)]". Here we start from considering the level-1 recursion, as psychologist have found

that humans tend to reason on average at one or two level of recursion (Camerer et al., 2004), and

levels higher than two do not provide significant benefits (De Weerd et al., 2013a;b; de Weerd et al.,

2017). Based on this, we re-formulate the joint policy by

 (ai, a-i|s) = i i (ai|s)--ii (a-i|s, ai) = --ii (a-i|s)i i (ai|s, a-i) .

(3)

Agent i's perspective

The opponents' perspective

Similar way of decomposition can also be found in dual learning (Xia et al., 2017) on symmetrical tasks such as machine translation. From the perspective of agent i, the first equality in Eq. 3 indicates that the joint policy can be essentially decomposed into two parts. The conditional part --ii (a-i|s, ai) represents what actions would be taken by the opponents given the fact that the opponents know the current state of environment and agent i's action; this is based on what agent i believes other opponents might think about itself. Note that the way of thinking developed by agent i regarding how others would consider of itself is also shaped by opponents' original policy --ii (a-i|s), as this is also how the opponents actually act in the environment. Taking into account different potential actions that agent i thinks the opponents would take, agent i uses the marginal policy i i (ai|s) to find the best response. To this end, a level-1 recursive procedure is established: ai  a-i  ai. The same inference logic can be applied to the opponents from their perspectives, as shown in the second equality of Eq. 3.

Albeit instructive, Eq. 3 may not be practical due to the requirement on the full knowledge regarding the actual conditional policy --ii (a-i|s, ai). A natural solution is that one approximates the actual

4

Under review as a conference paper at ICLR 2019

Decentralized Training with Probabilistic Reasoning

Decentralized Execution
i

-i

-i ai

s a-i i

Qi Q-i

Figure 2: Diagram of multi-agent probabilistic recursive reasoning learning algorithms. It conducts decentralized training with decentralized execution. The light grey areas on two sides of middle indicate decentralized execution for each agent. White areas give the decentrilized learning procedures. All agents share the interaction experiences in the environment represented by dark area in the middle.
policy via a best-fit model from a family of distributions. We denote this family as --i i (a-i|s, ai) with learnable parameter -i. PR2 is probabilistic as it considers the uncertainty of modeling --ii (a-i|s, ai). The reasoning structure is now established as shown in Fig. 1. With the recursive joint policy defined in Eq. 3, the n-agent learning task can therefore be formulated as

arg max i i i (ai|s)--i i (a-i|s, ai) ,
 i, -i

(4)

arg max -i --ii (a-i|s)ii (ai|s, a-i) .
-i,i

(5)

With the new learning protocol defined in Eq. 4 and 5, each agent now learns its own policy as
well as the approximated conditional policy of other agents given its own actions. In such a way, both the agent and the opponents can keep track of the joint policy by i i (ai|s)--i i (a-i|s, ai)   (ai, a-i|s)  --ii (a-i|s)ii (ai|s, a-i). Once converged, the resulting approximates satistfies:  (ai, a-i|s) = i i (ai|s)--i i (a-i|s, ai) = --ii (a-i|s)ii (ai|s, a-i), according to Eq. 3.

4.2 PROBABILISTIC RECURSIVE REASONING POLICY GRADIENT
Given the true opponent policy --ii and that each agent tries to maximize its cumulative return in the stochastic game with the objective defined in Eq. 1, we estiablish the policy gradient theorem by accounting for the PR2 joint policy decomposition in Eq. 3. Proposition 1. In a stochastic game, under the recursive reasoning framework defined by Eq. 3, the update for the multi-agent recursive reasoning policy gradient method can be derived as follows:

i i = Esp,ai i i log i i (ai|s)

--ii (a-i|s, ai)Qi(s, ai, a-i) da-i .

a-i

Proof. See Appendix B.2.

(6)

Proposition 1 states that each agent should improve its policy toward the direction of the best response after it takes into account all kinds of possibilities of how other agents would react if that action is taken. The term of --ii (a-i|s, ai) can be regarded as the posterior estimation of agent i's belief about how the opponents would respond to his action ai, given opponents' true policy --ii (a-i|s) serving as the prior. Note that compared to the direction of policy update in the conventional multi-agent policy gradient theorem (Wei et al., 2018), a-i --ii (a-i|s)Qi(s, ai, a-i) da-i, the direction of the gradient update in PR2 is guided by the term a-i --ii (a-i|s, ai)Qi(s, ai, a-i) da-i.
In practice, agent i might not have access to the opponents' actual policy parameters -i, it is often needed to approximate --ii (a-i|s, ai) by --i i (a-i|s, ai), thereby we propose Proposition 2.
Proposition 2 raises an important point: the difference between decentralized training (algorithms that do not require the opponents' policies) with centralized learning (algorithms that require the opponents' policies) can in fact be quantified by a term of importance weights, similar to the connection between on-policy and off-policy methods. If we find a best-fit approximation such that --i i (a-i|s, ai)  --ii (a-i|s, ai), then Eq.7 collapses into Eq. 6.

5

Under review as a conference paper at ICLR 2019

Proposition 2. In a stochastic game, under the recursive reasoning framework defined by Eq. 3, with the opponent policy approximated by --i i (a-i|s, ai), the update for the multi-agent recursive reasoning policy gradient method can be formulated as follows:

i i =Esp,ai i i log i i (ai |s) · Ea-i --i i

--i i --i i

(a-i |s, (a-i |s,

ai) ai)

Q

i

(s,

ai,

a-i

)

.

(7)

Proof. Substituting the approximated model --i i (a-i|s, ai) for the true policy --i i in Eq. 6.
Based on Proposition 2, we could provide mutli-agent PR2 learning algorithm. As illustrated in Fig. 2, it is a decentralized-training-with-decentralized-execution algorithm. In this setting, agents share the experiences in the environment including state and historical joint actions, while each agent receive its rewards privately. Our method does not require the knowledge of other agents' policy parameters. We list the pseudo codes of PR2-AC and PR2-Q in Appendix A. Finally, one important piece missing is how to find a best-fit approximation of --i i (a-i|s, ai).

4.3 VARIATIONAL INFERENCE ON OPPONENT CONDITIONAL POLICY

We adopt an optimization-based approximation to infer the unobservable --i i (a-i|s, ai) via variational inference (Jordan et al., 1999). We first define the trajectory  up to time t including the experiences of t consecutive time stages, i.e.  = [(s1, a1i , a1-i), . . . , (st, ati, at-i)]. In the probabilistic reinforcement learning (Levine, 2018), the probability of  being generated can be derived as

T
p() = p(s1) p(st+1|st, ati, at-i) exp
t =1

T
ri(st, at, at-i) dt .
t =1

(8)

Assuming the dynamics is fixed (i.e. the agent can not influence the environment transition probability), our goal is then to find the best approximation of i i (ati|st )--i i (at-i|st, ati) such that the induced trajectory distribution p^() can match with the true trajectory probability p():

T
p^() = p(s1) p(st+1|st, ati, at-i)i i (ati |st )--ii (at-i|st, ati ).
t =1

(9)

In other words, we can optimize the opponents' policy --i i via minimizing the K L-divergence between p^() and p(), i.e.

DKL(p^() p()) = -Ep^()[log p() - log p^()]

t =T

=-

Ep^() ri st, ati, at-i + H i i ati |st --i i a-i|st, ati

t =1

. (10)

Minimizing the K L-divergence is equivalent to maximizing the reward; however, besides the reward term, the objective introduces an additional term of the conditional entropy on the joint policy H i i ati|st --i i a-i|st, ati , that potentially promotes the explorations for both the agent i's best response and the opponents' conditional policy. Note that the entropy here is conditioning not only on the state of environment but also on agent i's action. Minimizing Eq. 10 gives us:

Theorem 1. The optimal Q-function for agent i that satisfies minimizing Eq. 10 is formulated as:

Qi (s, ai) = log

exp(Qi (s, ai, a-i)) da-i .

a-i

And the corresponding optimal opponent conditional policy reads:

--i i (a-i|s, ai) = exp(Qi (s, ai, a-i) - Qi (s, ai))

Proof. See Appendix C.

(11) (12)

6

Under review as a conference paper at ICLR 2019

Policy of Agent 2 Iterations
Policy of Agent 2 Iterations Policy Policy

1.0 0.8 400 0.6 300 0.4 200 0.2 100 0.00.0 0.2 0.4 0.6 0.8 1.0 0
Policy of Agent 1

1.0 0.8 400 0.6 300 0.4 200 0.2 100 0.00.0 0.2 0.4 0.6 0.8 1.0 0
Policy of Agent 1

0.70 Agent Policy
0.65 Policy of Agent 1 0.60 Policy of Agent 2
0.55
0.50
0.45
0.40
0.35 0 100 200 300 400 500 Iteration

0.5

0.4

0.3

0.2

0.1

Opponent Policy Opponent Policy Estimated by Agent 1

0.0 Opponent Policy Estimated by Agent 2

0 100 200 300 400

Iteration

500

(a) IGA dynamics. (b) PR2-Q dynamics. (c) PR2-Q Agent Policies. (d) PR2-Q Opponent Policies Figure 3: Learning paths on the iterated matrix game. a: IGA. b-d: PR2-Q.

Theorem 1 states that the learning of --i i (a-i|s, ai) can be further converted to minimizing the K L-divergence between the estimated policy --i i and the advantage function:
DKL --i i (a-i|s, ai) exp(Qi(s, ai, a-i) - Qi(s, ai)) . We can obtain a solution to Eq. 12 by main-
taining two Q-functions, and then iteratively update them. We prove the convergence in the symmetric game under self-play. This leads to a fixed-point iteration that resembles value iteration.
Theorem 2. In a symmetric game with only one equilibrium, and the equilibrium meets one of the conditions: 1) the global optimum, i.e. E Qit (s)  E Qit (s) ; 2) a saddle point, i.e. E Qit (s)  Ei E-i Qit (s) or E Qit (s)  Ei E-i Qit (s) ; where Q and  are the equilibrium value function and policy, respectively. The PR2 soft value iteration operator defined by:

TQi(s, ai, a-i) ri(s, ai, a-i) + Es ,ai ps,i is a contraction mapping.

log exp(Qi(s , ai , a-i )) da-i
a-i

,

Proof. See Appendix D.

(13)

4.4 SAMPLING IN CONTINUOUS ACTION SPACE
In dealing with the continuous action space, getting the actions from the opponent policy is challenging, as --i i (a-i|s, ai)  exp(Qi(s, ai, a-i) - Qi(s, ai)). In this work, we follow Haarnoja et al. (2017) to adopt the amortized Stein Variational Gradient Descent (SVGD) (Liu & Wang, 2016; Wang & Liu, 2016) in sampling from the soft Q-function. Compared to MCMC, Amortized SVGD is a computationally-efficient way to estimate --i i (a-i|s, ai). Thanks to SVGD, agent i is able to reason about potential consequences of opponent bavhaviors a-i --ii (a-i|s, ai)Qi(s, ai, a-i) da-i, and finally find the corresponding best response.

4.5 ALTERNATIVE APPROACH
In learning the opponent conditional policy --i i (a-i|s, ai), one could also conduct variational inference directly on minimizing DKL(--i i (a-i|s, ai)||--ii (a-i|s, ai)). In such a way, it is equivalent to maximizing the evidence of lower bound, that is
L(i, -i, ai) = -DK L (--i i (a-i|s, ai)||--i i (a-i|s)) + Ea-i --i i (a-i |s,ai )[log i i (ai|s, a-i)].
However we believe this is not feasible. The main reason is that we have no information on either --ii (a-i|s) or i i (ai|s, a-i); therefore, we have to construct two additional models to learn from experiences in an supervised way. Despite the added complexity, this approach introduces another two origins of approximation errors.

5 EXPERIMENTS
We evaluate the performance of our algorithm on the iterated matrix games, and differential games. Those games can by design have a non-trivial equilibrium that requires certain levels of intelligent reasonings between agents. We compared our algorithm with a series of baselines. In the matrix game, we compare against IGA (Infinitesimal Gradient Ascent) (Singh et al., 2000).
In the differential games, the baselines from multi-agent learning algorithms are MASQL (MultiAgent Soft-Q) (Wei et al., 2018) and MADDPG (Lowe et al., 2017). We also including independent

7

Action of Agent 2 -18.0 -26.0 -14-.60.0

Under review as a conference p-1a0.0per a-1t8.-02I2.0CLR 2019 2.0 -2.0 6.0 5 -30.0 -26.0 -22.0
-18.0
0 -10.0 -14.0
-2.0
5

-30.0 -22.0

-6.0
1010 5 0 Action of Agent 1

5

(a) The learning path of PR2-AC vs.

(b) The learning curves.

PR2-AC.

Figure 4: Max of Two Quadratic Game.

learning algorithms implemented through DDPG (Lillicrap et al., 2015). To compare against traditional method of opponent modeling, we include one baseline that is also based on DDPG but with one additional opponent modeling unit that is trained in an online and supervised way to learn the most recent opponent policy, which is then fed into the critic. Similar approach has been implemented by Rabinowitz et al. (2018) in realizing machine theory of mind.

For the experiment settings, all the policies and Q-functions are parameterized by the MLP with 2 hidden layers and 100 units each with the ReLU activation. The sampling network  for the --i i in SGVD follows the standard normal distribution. In the iterated matrix game, we trained all the methods including the baselines for 500 iterations. In the differential game, we trained the agents for 350 iterations with 25 steps per iteration. For the actor-critic methods, we set the exploration noise to 0.1 in first 1000 steps, and the annealing parameters for PR2-AC and MASQL are set to 0.5 to
balance between the exploration and acting as the best response.

5.1 ITERATED MATRIX GAME

In the matrix game, the payoffs are defined by: R1 =

03 12

for agent 1, R2 =

32 01

for

agent 2. For this game, the only Nash Equilibrium stays at (0.5, 0.5). This game has been extensively

investigated in the studies on multi-agent learning (Bowling & Veloso, 2001a;b). One reason is that

in solving the Nash Equilibrium for this game, simply taking simultaneous gradient steps on both

agent's value funcctions will present the rotational behaviors on the gradient vector field; this leads to

an endlessly iterative change of behaviors. Without considering the consequence of one agent's action

on the other agent beforehand, it is challenging for both players to find the equilibrium. Interestingly,

similar issue has also been reported in training the GANs (Goodfellow et al., 2014). Mescheder et al.

(2017) has pointed out that the reason that game has a strong rotation gradient vector field is due to

the imaginary part in the eigenvalue of IGA learning matrix.

The results are shown in Fig.3. As expected, IGA fails to converge to the equilibrium but rotate around the equilibrium point. On the contrary, our method can find precisely the central equilibrium with a fully distributed fashion (see Fig. 3b). The convergence can also be confirmed by the agents' policies in Fig. 3c, and the opponent's policy that is maintained by each agent in Fig. 3d.

5.2 DIFFERENTIAL GAME

We adopt the same differential game, the Max of Two Quadratic Game, as Panait et al. (2006); Wei et al. (2018). The agents have continuous action space of [-10, 10]. Each agent's reward depends on the joint action following the equations: r1 a1, a2 = r2 a1, a2 = max ( f1, f2) , where:

f1 = 0.8 × -

a1 + 5

2
-

3

a2 + 5 3

2

, f2 = 1.0 ×

-

a1 - 5

2
-

1

a2 - 5 2 + 10
1

The task formulation is relatively simple, but it poses a great challenge to general gradient-based
algorithms because gradient tends to points to the sub-optimal solution. The reward surface is shown in Fig. 4a; there is a local maximum 0 at (-5, -5) and a global maximum 10 at (5, 5), with a deep valley staying in the middle. If the agents' policies are initialized to (0, 0) (the red starred point) that
lies within the basin of the left local maximum, the gradient based methods would tend to fail to find

8

-18.0 -26.0
-6.0 -14.0 -18.0 -26.0
-6.0 -14.0 -18.0 -26.0
-6.0 -14.0 -18.0 -26.0
-6.0 -14.0

-30.0 -22.0
-30.0 -22.0
-30.0 -22.0
-30.0 -22.0

Under review as a conference-10.0 -18.-022.0 paper at ICLR 2019-10.0 -18.-022.0

2.0 -2.0

2.0 -2.0

6.0 6.0

5

-30.0 -26.0

-22.0

5

-30.0 -26.0

-22.0

5

-18.0
0 -10.0 -14.0
-2.0

-18.0
0 -10.0 -14.0
-2.0

0

555

-30.0 -26.0 -22.0
-18.0 -10.0 -14.0
-2.0

-10.0 -18.-022.0 2.0 -2.0 6.0
5
0
5

-30.0 -26.0 -22.0
-18.0 -10.0 -14.0
-2.0

-10.0 -18.-022.0 2.0 -2.0 6.0

-6.0
10 10 5 0 -150.0

(a) DDPG / DDPG.

2.0 6.0

5

-30.0 -26.0

-22.0

10 10
-18.-022.0

-6.0

5

0 -150.0

(b)-2.0 DDPG- / DDPG-OM62..00.

-6.0

10 10
-18.-022.0

5

0

-150.0

-2.0

(c)

MA-

/

MADDPG.

2.0 6.0

5

-30.0 -26.0

-22.0

5

-30.0 -26.0

-22.0

-6.0

10 10
-18.-022.0

5

0

-150.0

-2.0 (d) MASQL / MASQL.62..00

5

-30.0 -26.0

-22.0

-18.-022.0 -2.0

-18.0 -26.0
-6.0 -14.0 -18.0 -26.0
-6.0 -14.0 -18.0 -26.0
-6.0 -14.0 -18.0 -26.0
-6.0 -14.0

-18.0
0 -10.0 -14.0
-2.0

-18.0
0 -10.0 -14.0
-2.0

-18.0
0 -10.0 -14.0
-2.0

-18.0
0 -10.0 -14.0
-2.0

-30.0 -22.0
-30.0 -22.0
-30.0 -22.0
-30.0 -22.0

5555

10 10

-6.0
5

0

5 10 10

-6.0
5

0

5 10 10

-6.0
5

0

5 10 10

-6.0
5

0

5

(e) PR2-AC / DDPG. (f) PR2-AC / DDPG-OM. (g) PR2-AC / MADDPG. (h) PR2-AC / MASQL.

Figure 5: The learning path of Agent 1 (x-axis) vs. Agent 2 (y-axis).

the global maximum equilibrium point due to the valley blocking the upper right area. The pathology of a suboptimal Nash Equilibrium in the joint space of actions being preferred over an optimal Nash Equilibrium is also called relative over-generalization (Wei & Luke, 2016).

We present the results in Fig.4b. PR2-AC shows superior performance that manages to converge to the global equilibrium, while all the other baselines fall into the local basin on the left, except that the MASQL has small chance to find the optimal point. On top of the convergence result, it is worth noting that as the temperature annealing is required for energy-based RL methods, the learning outcomes of PR2-AC and MASQL are extremely sensitive to the way of annealing, i.e. when and how to anneal the temperature to a small value during training is non-trivial. However, our method does not need to tune the the annealing parameter at all because the each agent is acting the best response to the approximated conditional policy, which has considered all potential consequences of the opponent's response if this action was taken.

Interestingly, by comparing the learning path in Fig. 4a against Fig. 5(a-d) where the scattered dots are the exploration trails at the beginning, we can tell that if the PR2-AC model finds the peak point in joint action space, the agents can quickly go through the shortcut out of the local basin in a clever way, while other algorithms cannot. This further justifies the effectiveness and benefits of conducting recursive reasoning with opponents. DDPG in Fig. 5a and MASQL in Fig.5d even miss the local equilibrium; we believe this is because of the inborn defect from the independent learning methods, and the sensitivity to the annealing process respectively.

Apart from testing in the self-play setting, we also test the scenario when the opponent type is different. We pair PR2-AC with all four baseline algorithms in Fig. 5(e-h). Similar result can be found, that is, algorithm that has the function of taking into account the opponents (i.e. DDPG_OM & MADDPG) can converge to the local equilibrium even though not global, while DDPG and MASQL completely fails due to the same reasons as in self-plays. Finally, we want to highlight the difference between PR2 methods and traditional OM, that is, PR2-Q/PR2-AC agent models how the opponents would believe about what it would behave, and then finds the best response to that belief, whereas OM agent tends to only model how the opponents behave based on the history. Such difference this seems to be a decisive factor in overcoming the rotational dynamics or the relative over-generalization issue.

6 CONCLUSION
Inspired by the recursive reasoning capability of human intelligent, in this paper, we introduce a probabilistic recursive reasoning framework for multi-agent RL that follows "I believe that you believe that I believe". We adopt variational Bayes approaches to approximating the opponents' conditional policy, to which each agent then finds the best response to improve their own policy. The training and execution is full decentralized and the resulting algorithms, PR2-Q and PR2-AC, converge in self-play. Our results on both the matrix game and the differential game justify the advantages of learning to reason about the opponents in a recursive manner. In the future, we plan to investigate other approximation methods for the PR2 framework.

9

Under review as a conference paper at ICLR 2019
REFERENCES
Stefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artificial Intelligence, 258:66­95, 2018.
Dipyaman Banerjee and Sandip Sen. Reaching pareto-optimality in prisoner's dilemma using conditional joint action learning. Autonomous Agents and Multi-Agent Systems, 15(1):91­108, 2007.
Thomas Bolander and Mikkel Birkegaard Andersen. Epistemic planning for single-and multi-agent systems. Journal of Applied Non-Classical Logics, 21(1):9­34, 2011.
Michael Bowling. Convergence and no-regret in multiagent learning. In Advances in neural information processing systems, pp. 209­216, 2005.
Michael Bowling and Manuela Veloso. Convergence of gradient dynamics with a variable learning rate. In ICML, pp. 27­34, 2001a.
Michael Bowling and Manuela Veloso. Rational and convergent learning in stochastic games. In International joint conference on artificial intelligence, volume 17, pp. 1021­1026. Lawrence Erlbaum Associates Ltd, 2001b.
Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate. Artificial Intelligence, 136(2):215­250, 2002.
George W Brown. Iterative solution of games by fictitious play. Activity analysis of production and allocation, 13(1):374­376, 1951.
Colin F Camerer, Teck-Hua Ho, and Juin-Kuan Chong. A cognitive hierarchy model of games. The Quarterly Journal of Economics, 119(3):861­898, 2004.
Colin F Camerer, Teck-Hua Ho, and Juin Kuan Chong. A psychological approach to strategic thinking in games. Current Opinion in Behavioral Sciences, 3:157­162, 2015.
Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998:746­752, 1998.
Bruno C Da Silva, Eduardo W Basso, Ana LC Bazzan, and Paulo M Engel. Dealing with nonstationary environments using context detection. In Proceedings of the 23rd international conference on Machine learning, pp. 217­224. ACM, 2006.
Harmen De Weerd, Rineke Verbrugge, and Bart Verheij. Higher-order theory of mind in negotiations under incomplete information. In International Conference on Principles and Practice of MultiAgent Systems, pp. 101­116. Springer, 2013a.
Harmen De Weerd, Rineke Verbrugge, and Bart Verheij. How much does it help to know what she knows you know? an agent-based simulation study. Artificial Intelligence, 199:67­92, 2013b.
Harmen de Weerd, Rineke Verbrugge, and Bart Verheij. Negotiating with other minds: the role of recursive theory of mind in negotiation with incomplete information. Autonomous Agents and Multi-Agent Systems, 31(2):250­287, 2017.
Daniel C Dennett. Two contrasts: folk craft versus folk science, and belief versus opinion. The future of folk psychology: Intentionality and cognitive science, pp. 135­148, 1991.
Prashant Doshi and Piotr J Gmytrasiewicz. On the difficulty of achieving equilibrium in interactive pomdps. In PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, volume 21, pp. 1131. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
Prashant Doshi and Piotr J Gmytrasiewicz. Monte carlo sampling methods for approximating interactive pomdps. Journal of Artificial Intelligence Research, 34:297­337, 2009.
Prashant Doshi and Dennis Perez. Generalized point based value iteration for interactive pomdps. In AAAI, pp. 63­68, 2008.
10

Under review as a conference paper at ICLR 2019
Prashant Doshi, Yifeng Zeng, and Qiongyu Chen. Graphical models for interactive pomdps: representations and solutions. Autonomous Agents and Multi-Agent Systems, 18(3):376, 2009.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1705.08926, 2017.
Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 122­130. International Foundation for Autonomous Agents and Multiagent Systems, 2018.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, pp. 202­211. AUAI Press, 2016.
Ya'akov Gal and Avi Pfeffer. A language for modeling agents' decision making processes in games. In Proceedings of the second international joint conference on Autonomous agents and multiagent systems, pp. 265­272. ACM, 2003.
Ya'akov Gal and Avi Pfeffer. Networks of influence diagrams: a formalism for representing agents' beliefs and decision-making processes. Journal of Artificial Intelligence Research, 33:109­147, 2008.
Vittorio Gallese and Alvin Goldman. Mirror neurons and the simulation theory of mind-reading. Trends in cognitive sciences, 2(12):493­501, 1998.
Piotr J Gmytrasiewicz and Prashant Doshi. A framework for sequential planning in multi-agent settings. Journal of Artificial Intelligence Research, 24:49­79, 2005.
Piotr J Gmytrasiewicz and Edmund H Durfee. A rigorous, operational formalization of recursive modeling. In ICMAS, pp. 125­132, 1995.
Piotr J Gmytrasiewicz and Edmund H Durfee. Rational coordination in multi-agent environments. Autonomous Agents and Multi-Agent Systems, 3(4):319­350, 2000.
Piotr J Gmytrasiewicz, Edmund H Durfee, and David K Wehe. A decision-theoretic approach to coordinating multi-agent interactions. In IJCAI, volume 91, pp. 63­68, 1991.
Alvin I Goldman et al. Theory of mind. The Oxford handbook of philosophy of cognitive science, pp. 402­424, 2012.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Adam S Goodie, Prashant Doshi, and Diana L Young. Levels of theory-of-mind reasoning in competitive games. Journal of Behavioral Decision Making, 25(1):95­108, 2012.
Alison Gopnik and Henry M Wellman. Why the child's theory of mind really is a theory. Mind & Language, 7(1-2):145­171, 1992.
Robert M Gordon. Folk psychology as simulation. Mind & Language, 1(2):158­171, 1986.
Amy Greenwald, Keith Hall, and Roberto Serrano. Correlated q-learning. In ICML, volume 3, pp. 242­249, 2003.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
John C Harsanyi. Bargaining in ignorance of the opponent's utility function. Journal of Conflict Resolution, 6(1):29­38, 1962.
John C Harsanyi. Games with incomplete information played by bayesian players, i­iii part i. the basic model. Management science, 14(3):159­182, 1967.
11

Under review as a conference paper at ICLR 2019
He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daumé III. Opponent modeling in deep reinforcement learning. In International Conference on Machine Learning, pp. 1804­1813, 2016.
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of machine learning research, 4(Nov):1039­1069, 2003.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183­233, 1999.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40, 2017.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine Learning Proceedings 1994, pp. 157­163. Elsevier, 1994.
Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pp. 322­328, 2001.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2378­2386, 2016.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6379­6390, 2017.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in Neural Information Processing Systems, pp. 1825­1835, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Christian J Muise, Vaishak Belle, Paolo Felli, Sheila A McIlraith, Tim Miller, Adrian R Pearce, and Liz Sonenberg. Planning over multi-agent epistemic states: A classical planning approach. In AAAI, pp. 3327­3334, 2015.
Liviu Panait, Sean Luke, and R Paul Wiegand. Biasing coevolutionary search for optimal multiagent behaviors. IEEE Transactions on Evolutionary Computation, 10(6):629­645, 2006.
Brad E Pfeiffer and David J Foster. Hippocampal place-cell sequences depict future paths to remembered goals. Nature, 497(7447):74, 2013.
David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515­526, 1978.
David V Pynadath and Stacy C Marsella. Psychsim: Modeling theory of mind with decision-theoretic agents. In IJCAI, volume 5, pp. 1181­1186, 2005.
Neil C Rabinowitz, Frank Perbet, H Francis Song, Chiyuan Zhang, SM Eslami, and Matthew Botvinick. Machine theory of mind. arXiv preprint arXiv:1802.07740, 2018.
Nikolaus Robalino and Arthur Robson. The economic approach to 'theory of mind'. Phil. Trans. R. Soc. B, 367(1599):2224­2233, 2012.
Sven Seuken and Shlomo Zilberstein. Formal models and algorithms for decentralized decision making under uncertainty. Autonomous Agents and Multi-Agent Systems, 17(2):190­250, 2008.
12

Under review as a conference paper at ICLR 2019
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10): 1095­1100, 1953.
Yoav Shoham, Rob Powers, Trond Grenager, et al. If multi-agent learning is the answer, what is the question? Artificial Intelligence, 171(7):365­377, 2007.
Satinder Singh, Michael Kearns, and Yishay Mansour. Nash convergence of gradient dynamics in general-sum games. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pp. 541­548. Morgan Kaufmann Publishers Inc., 2000.
Edward Jay Sondik. The optimal control of partially observable markov processes. Technical report, STANFORD UNIV CALIF STANFORD ELECTRONICS LABS, 1971.
Ekhlas Sonu and Prashant Doshi. Scalable solutions of interactive pomdps using generalized and bounded policy iteration. Autonomous Agents and Multi-Agent Systems, 29(3):455­494, 2015.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000. Edward C Tolman. Cognitive maps in rats and men. Psychological review, 55(4):189, 1948. Karl Tuyls and Gerhard Weiss. Multiagent learning: Basics, challenges, and prospects. Ai Magazine, 33(3):41, 2012. Friedrich Burkhard Von Der Osten, Michael Kirley, and Tim Miller. The minds of many: opponent modelling in a stochastic game. In Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI), AAAI Press, pp. 3845­3851, 2017. Dilin Wang and Qiang Liu. Learning to draw samples: With application to amortized mle for generative adversarial learning. arXiv preprint arXiv:1611.01722, 2016. Ermo Wei and Sean Luke. Lenient learning in independent-learner stochastic cooperative games. The Journal of Machine Learning Research, 17(1):2914­2955, 2016. Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. AAAI, 2018. Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. Dual supervised learning. arXiv preprint arXiv:1707.00415, 2017. Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multiagent reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5571­5580, Stockholmsmassan, Stockholm Sweden, 10­15 Jul 2018. PMLR.
13

Under review as a conference paper at ICLR 2019

APPENDIX
A DECENTRALIZED MULTI-AGENT PROBABILISTIC RECURSIVE REASONING ALGORITHMS Algorithm 1 gives the step by step learning procedures for PR2-AC algorithm.

Algorithm 1: Multi-Agent Probabilistic Recursive Reasoning Actor Critic (PR2-AC).

Result: Policy:i, Opponent Recursive Reasoning: -i(a-i|s, ai).

1 Initialize parameters i, -i, i arbitrarily for each agent i, random process N for action exploration.

2 Assign target parameters of joint action Q-function: i  i, and target policy parameter: i  i

3 Di  empty replay buffer for each agent.

4 for each episode do

5 Initialize random process N for action exploration.

6 for each step t do

7 Given the current s, for each agent i, select action ai = µii (s) + Nt ; 8 Take the joint action (ai, a-i) and observe own reward ri and new state s ;

9 Add the tuple (s, ai, a-i, ri, s ) in corresponding replay buffer Di;

10 s  s ;

11 for each agent i do

12

Sample

a

random

mini

batch

{(s,

aij,

a-j i,

rji,

s

j

)}

N j=0

from

Di;

13 Get aij = µii for each state sj ; 14 Sample {ak-, ij }kM=0  --i i (·|sj, aij ) for each aij and sj ;

15

Set

yij

=

r

i j

+



1 M

M k=0

Qiµi

(s

,

ai

,

ak-, ij

);

16

Update the critic by minimizing the loss L(i)

=

1 N

N j=0

yj - Qiµi (sj, aij, a-j i)

2
;

17 Update the actor using the sampled policy gradient:

i i



1 N

N

 i

µi

(s j )

ai

1 M

M
Qiµi (sj, aij, ak-, ij );

j=0 k=0

18 Compute --i i using empirical estimation; 19 Compute empirical gradient ^ -i J-i ; 20 Update -i according to ^ -i J-i ; 21 end 22 Update target network parameters for each agent i:
23
i  i + (1 - )i ;

i  i + (1 - )i ;

24 end 25 end

The Algorithm 2 shows the variant of Decentralized Multi-Agent Probabilistic Recursive Reasoning. We can simply approximate the -i(a-i|s, ai) by counting:-i(a-i|s, ai) = C(ai, a-i, s)/C(ai, s) in tabular if the state and action spaces are small, where C is the counting function. It this case, an agent only needs to learn a joint action Q-function, and if the game is also static game, it would be same as Conditional Joint Action Learning (CJAL) (Banerjee & Sen, 2007).
B MULTI-AGENT POLICY GRADIENT
B.1 MULTI-AGENT NON-CORRELATED POLICY GRADIENT Since  ai, a-i|s = i i ai --ii a-i|,ai = --ii a-i|s i i ai|s, a-i ,  ai, a-i|s can be factorized as i i (ai|s)--ii (a-i|s) if ai and a-i are non-correlated. We follow the policy gradient
14

Under review as a conference paper at ICLR 2019

Algorithm 2: Multi-Agent Probabilistic Recursive Reasoning Q-Learning (PR2-Q).
Result: Policy: i, Opponent Recursive Reasoning: -i(a-i|s, ai). 1 Initialize Qi(s, ai, a-i) arbitrarily, set  as the learning rate,  as discount factor; 2 while not converge do 3 Given the current s, calculate the opponent best response -i(a-i|s, ai) according to:

4
-i(a-i|s, ai) = exp(Qi(s, ai, a-i) - Qi(s, ai))
5 Select and sample action ai based on the Recursive Reasoning -i(a-i|s, ai);

6

softmax( -i(a-i|s, ai)Qi(s, ai, a-i))
a-i
7 Observing joint-action (ai, a-i), reward ri, and next state s ;

8
Qi(s, ai, a-i)  (1 - )Qi(s, ai, a-i) + (ri + V i(s ))

Qi(s, ai)  (1 - )Qi(s, ai) + (ri + Vi(s ))

where,

9 end

Vi(s) = max

-i(a-i|s, ai)Qi(s, ai, a-i)

ai a-i

formulation (Sutton et al., 2000; Wei et al., 2018) using Leibniz integral rule and Fubini's theorem which gives Multi-Agent Non-correlated Policy Gradient:

i =

(ai, a-i|s)Qi(s, ai, a-i) da-i dai ds

s ai a-i

= i(ai|s)-i(a-i|s)Qi(s, ai, a-i) da-i dai ds
s ai a-i

=

i (ai |s)

-i(a-i|s)Qi(s, ai, a-i) da-i dai ds.

s ai

a-i

(14)

Suppose the i(ai) is parameterized by i, and we apply the gradient over the i:

i i =

i i i (ai|s)

-i(a-i|s)Qi(s, ai, a-i) da-i dai ds

s ai

a-i

=Esp,ai i [i log i(ai|s)

-i(a-i|s)Qi(s, ai, a-i) da-i].

a-i

(15)

In practice, off-policy is more efficient. Therefore, in MADDPG (Lowe et al., 2017) and COMA (Foerster et al., 2017), they introduce replay buffer in a centralized deterministic actor-critic method for off-policy training. And they do not apply the optimal action in policy gradient, and sample batches to the centralized critic which gives joint-action Q-values.

i i = Es,ai,a-i D [i µii (ai|s)ai Qi(s, ai, a-i)|ai =µi (s)].

(16)

B.2 MULTI-AGENT RECURSIVE REASONING POLICY GRADIENT
Proposition 1. In a stochastic game, under the recursive reasoning framework defined by Eq. 3, the update rule for the multi-agent recursive reasoning policy gradient method can be devised as follows:

i i = Esp,ai i i log i i (ai|s)

--ii (a-i|s, ai)Qi(s, ai, a-i) da-i .

a-i

Proof: As following.

(17)

15

Under review as a conference paper at ICLR 2019

If we apply the chain rule to factorize the joint policy to:  (ai, a-i|s) = i i (ai|s)--ii (a-i|s, ai). Then, we can have multi-agent recursive reasoning objective function:

i =

(ai, a-i|s)Qi(ai, a-i) da-i dai ds

s ai a-i

(18)

=

i (ai |s)

-i(a-i|s, ai)Qi(s, ai, a-i) da-i dai ds.

s ai

a-i

Compare to Eq. 14, a-i in Eq. 18 is additionally conditional on ai where ai is the prior. That is we introduce agent i'a action ai into other agents -i's policy, obtaining -i(a-i|s, ai). It adds the

consideration that agent i can have influence on other agents. We now compute the policy gradient

analytically. Following the single agent Policy Gradient Theorem with Leibniz integral rule and

Fubini's theorem, we get the multi-Agent Recursive Reasoning Policy Gradient:

i i =Esp,ai i [i log i(ai|s)

-i(a-i|s, ai)Qi(s, ai, a-i) da-i].

a-i

(19)

However, in common case, the agent cannot get access to other agents' policies. We need to keep an approximation of other agents. We let --i i (a-i|s, ai) denotes the parameterized opponent conditional policy of agent i to approximate other agents policies, i.e, -i(a-i|s, ai). Then we have Decentralized Multi-Agent Recursive Reasoning Policy Gradient comes as:

i i Esp,ai i [i log i i (ai|s) a-i --i i (a-i|s, ai)Qi(s, ai, a-i) da-i] =Esp,ai i [i log i i (ai|s)Qi--i i (s, ai)].

(20)

In

Eq.

20,

the

gradient

for

agent

i

is

scaled

by

Qi
--i i

(s,

ai)

=

a-i --i i (a-i|s, ai)Qi(s, ai, a-i) da-i.

Then, the trajectories generated by updated policy would help to train --i i (a-i|s, ai) and

Qi(s, ai, a-i). These steps form a EM style learning procedures: fix --i i and Qi(s, ai, a-i), improve

i i (ai|s); then --i i and Qi(s, ai, a-i) would benefit from the trajectories broght by i i (ai|s). Fur-

thermore, because we do not need opponents' actual private policies, Decentralized Multi-Agent

Recursive Reasoning Policy Gradient decouple from other agents' on-policies or target policies. This

means the training can be done in an off-policy fashion by sampling batches from memory buffer D with the help of the on-policy learned --i i (a-i|s, ai) from Qi(s, ai, a-i).

C OPPONENT CONDITIONAL POLICY INFERENCE VIA OPTIMAL TRAJECTORY

Theorem 1. The optimal Q-function for agent i that satisfies minimizing Eq. 10 is formulated as:

Qi (s, ai) = log

exp(Qi (s, ai, a-i)) da-i .

a-i

And the corresponding optimal opponent conditional policy reads:

--i i (a-i|s, ai) = exp(Qi (s, ai, a-i) - Qi (s, ai))

Proof. As following.

(21) (22)

Follow the proof in Levine (2018); Haarnoja et al. (2017), we give the overall distribution at first:

TT
p() = [p(s1) p(st+1|st, ati, at-i)] exp( ri(st, at, at-i) dt).
t=1 t=1

(23)

We can simplify in the case of deterministic dynamics, and we can adopt an optimization-based

approximate inference approach to this problem, in which case the goal is to fit an approximation (ati, at-i|st )  i(ati|st )-i(at-i|st, ati) such that the trajectory distribution:

T
p^() = p(s1) p(st+1|st, ati, at-i)i i (ati |st )--ii (at-i|st, ati ).
t =1

(24)

16

Under review as a conference paper at ICLR 2019

In the case of exact inference, as derived in the previous section, the match is exact, which means that DKL(p^() p()) = 0, we can therefore view the inference process as minimizing the K L-divergence:

DKL(p^() p()) = -Ep^()[log p() - log p^()]. Negating both sides and substituting, we get:

(25)

T

-DKL(p^() p()) = Ep^()[log p(s1) +

(log p(st+1|st, at, at-i) + ri(st, ati, at-i)) dt

t =1

T

- log p(s1) -

(log p(st+1|st, ati, at-i) + log (ati, at-i|st )) dt]

t =1

T

= Ep^()[

ri(st, ati, at-i) - log (ati, at-i|st ) dt]

t =1

T
= t=1 E(st,ati,at-i )p^(st,ati,at-i ))[ri (st, ati, at-i ) - log (ati, at-i |st )] dt

T
= t=1 E(st,ati,at-i )p^(st,ati,a-t i ))[r i (st, ati, at-i )]

+ Est,ati p^(st )[H(-i(at-i|st, ati ))] + Est p^(st )[H(i(ati |st ))] dt,

(26)

where H is entropy term. In the recursive case, we note that we can rewrite the objective and have:

Qi(s, ai) = log

exp(Qi(s, ai, a-i)) da-i .

a-i

(27)

which corresponds to a standard bellman backup with a soft maximization for the value function. choosing optimal opponent recursive reasoning policy:

-i(a-i|s, ai) = exp(Qi(s, ai, a-i) - Qi(s, ai)).

(28)

Then we can have the objective function:

Ji(-i) =

T
t=1 E(st,ati,at-i )p^(st,ati,at-i )[r i (st, ati, at-i )
+ H(--i i (at-i|st, ati )) + H(i i (ati |st ))] dt.

Then the gradient is then given by:

(29)

TT

-i Ji(-i) =

t=1 E [(st,ati,a-t i )p(st,ati,at-i ) -i log --i i (at-i |st, ati )(

ri(st , ati , at-i) dt]
t =t

+ -i

T

t =1

E [H((st,ati,a-t i )p(st,ati,a-t i )

-i -i

(at-i |st ,

ati ))

+

H(i i

(ati|st ))]

dt .

(30)

The gradient of the entropy terms is given by:

-i H(--i i ) = - E [E [log (st,ati )p(st,ati,at-i ) a-t i --i i (a-t i |st,ati ) --i i (at-i |st, ati )]]
= -E [(st,ati,at-i )p(st,ati,a-t i )  log --i i (at-i |st, ati )(1 + log --i i (at-i |st, ati )]. (31)
We can do the same for -i H(i i ), and substitute these back:

T
-i Ji (-i ) = t=1 E [(st,ati,a-t i )p(st,ati,a-t i ) -i log --i i (at-i |st, ati )
T
( ri(st , ati , at-i) - log --i i (at-i|st, ati ) - log i i (ati |st ) - 1) dt] dt.
t =t

(32)

17

Under review as a conference paper at ICLR 2019

The -1 comes from the derivative of the entropy terms, and replacing -1 with a state and self-action dependent baseline b(st , ati ) we can obtain the approximated gradient for :
T
-i Ji (-i ) = t=1 E [(st,ati,at-i )p(st,ati,a-t i )  log --i i (at-i |st, ati )
T
( ri(st , ati , at-i) - log --i i (at-i|st , ati ) - log i i (ati |st ) - 1 ) dt] dt
t =t baseline ignore
T
 t=1 E [(st,ati,a-t i )p(st,ati,at-i ) -i log --i i (at-i |st, ati ) (ri(st, ati, at-i) - log i i (ati |st ) - log --i i (at-i|st, ati )

Qti (st, ati )-Vti (st ) Qti (st, ati, a-t i )-Qti (st, ati ) T
+ ri(st , ati , at-i) - log --i i (at-i|st , ati ) - log i i (ati |st )) dt ] dt
t =t+1

Qti (st+1, ati+1, at-+i1)
T
= t=1 E(st,ati,at-i )p(st,ati,a-t i )[ log --i i (at-i |st, ati ) (ri(st , ati , at-i) + Qit (st+1, ati+1, at-+i1) - Qit (st, ati, at-i) + Vti(st ))] dt

(33)

ignore
T
= t=1 E(st,ati,at-i )p(st,ati,a-t i )[(-i Qit (st, ati, at-i ) - -i Qit (st, ati )) (ri(st , ati , at-i) + Qit (st+1, ati+1, at-+i1) - Qit (st, ati, at-i) + Vti(st ))] dt

ignore
T
= t=1 E(st,ati,at-i )p(st,ati,at-i )[(-i Qit (st, ati, at-i ) - -i Qit (st, ati )) (Q^it (st, ati, at-i) - Qit (st, ati, at-i))] dt,
where Q^it (st, ati, at-i) is is an empirical estimate of the Q-value of the policy.

D SOFT BELLMAN EQUATION AND SOFT VALUE ITERATION
Theorem 2. In a symmetric game with only one equilibrium, and the equilibrium meets one of the conditions: 1) the global optimum, i.e. E Qit (s)  E Qit (s) ; 2) a saddle point, i.e. E Qit (s)  Ei E-i Qit (s) or E Qit (s)  Ei E-i Qit (s) ; where Q and  are the equilibrium value function and policy, respectively. The PR2 soft value iteration operator defined by:

TQi(s, ai, a-i) ri(s, ai, a-i) + Es ,ai ps,i log

exp(Qi(s , ai , a-i )) da-i ,

a-i

is a contraction mapping.

Proof. As following:

(34)

Based on Eq. 11 & 12 in Theorem 1, we can have the PR2 soft value iteration rules shown as:

Qi (s, ai, a-i) = ri(s, ai, a-i) + Es ps H(i(ai|s)-i(a-i|s, ai)) + Ea-i -i(·|s ,ai )[Qi (s , ai , a-i )]

= ri(s, ai, a-i) + Es ps Qi (s , ai ) .

(35)

Correspondingly, we define the soft value iteration operator T:

TQi(s, ai, a-i) ri(s, ai, a-i) + Es ,ai ps,i log

exp(Qi(s , ai , a-i )) da-i . (36)

a-i

18

Under review as a conference paper at ICLR 2019

In a symmetric game with either one global equilibrium or saddle equilibrium, it has been shown by Yang et al. (2018) (see condition 1&2 in Theorem 1) that the payoff at the equilibrium point is unique. This validates applying the similar idea in proving the contraction mapping of soft-value iteration operator in the single agent case (see Lemma 1 in Fox et al. (2016)). We include it here to stay self-contained.

We first define a norm on Q-values as Qi1 - Qi2 Suppose  = Qi1 - Q2i , then

maxs,ai,a-i |Q1i (s, ai, a-i) - Q2i (s, ai, a-i)|.

log exp(Q1i (s , ai , a-i )) da-i  log exp(Q2i (s , ai , a-i ) + ) da-i

a-i

a-i

= log

exp() exp(Qi2(s , ai , a-i )) da-i

a-i

=  + log

exp(Qi2(s , ai , a-i )) da-i

a-i

(37)

Similarly, log a-i exp(Q1i (s , ai , a-i )) da-i  - + log a-i exp(Q2i (s , ai , a-i )) da-i . Therefore TQi1 - TQi2   =  Q1i - Qi2 .

19

