Under review as a conference paper at ICLR 2019

q-NEURONS: NEURON ACTIVATIONS BASED ON STOCHASTIC JACKSON'S DERIVATIVE OPERATORS
Anonymous authors Paper under double-blind review

ABSTRACT
We propose a new generic type of stochastic neurons, called q-neurons, that considers activation functions based on Jackson's q-derivatives, with stochastic parameters q. Our generalization of neural network architectures with q-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions.

1 INTRODUCTION

The vanilla method to train a Deep Neural Network (DNN) is to use the Stochastic Gradient Descent (SGD) method (a first-order local optimization technique). The gradient of the DNN loss function, represented as a directed computational graph, is calculated using the efficient backpropagation algorithm relying on the chain rule of derivatives (a particular case of automatic differentiation).

The ordinary derivative calculus can be encompassed into a more general q-calculus Jackson (1909); Kac & Cheung (2001) by defining the Jackson's q-derivative (and gradient) as follows:

f (x) - f (qx) Dqf (x) := (1 - q)x , q = 1, x = 0.

(1)

The q-calculus generalizes the ordinary Leibniz gradient (obtained as a limit case when q  1 or when x  0) but does not enjoy a generic chain rule property. It can further be extended to the (p, q)-derivative Sadjang (2013); Khan et al. (2018) defined as follows:

f (px) - f (qx) Dp,qf (x) := (p - q)x , p = q, x = 0.

(2)

which encompasses the q-gradient as D1,qf (x) = Dq,1f (x) = Dqf (x).

The two main advantages of q-calculus are

1. To bypass the calculations of limits, and 2. To consider q as a stochastic parameter.

We refer to the textbook Kac & Cheung (2001) for an in-depth explanation of q-calculus. Appendix A recalls the basic rules and properties of the generic (p, q)-calculus Khan et al. (2018) that further generalizes the q-calculus.
To the best of our knowledge, the q-derivative operators have seldom been considered in the machine learning community Xu & Nielsen (2018). We refer to Gouve^a et al. (2016) for some encouraging preliminary experimental optimization results on global optimization tasks.
In this paper, we introduce a meta-family of neuron activation functions based on standard activation functions (e.g., sigmoid, softplus, ReLU, ELU). We refer to them as q-activations. The q-activation is a stochastic activation function built on top of any given activation function f . q-Activation is very easy to implement based on state-of-the-art Auto-Differentiation (AD) frameworks while consistently producing better performance. Based on our experiments, one should almost always use q-activation instead of its deterministic counterpart. In the remainder, we define q-neurons as stochastic neurons equipped with q-activations.
Our main contributions are summarized as follows:

1

Under review as a conference paper at ICLR 2019

· The generic q-activation and an analysis of its basic properties.
· An empirical study that demonstrates that the q-activation can reduce both training and testing errors.
· A novel connection and sound application of (stochastic) q-calculus in machine learning.

2 NEURONS WITH q-ACTIVATION FUNCTIONS

Given any activation function f : R  R, we construct its corresponding "quantum" version, also called q-activation function, as

f (x) - f (qx) gq(x) := 1 - q = (Dqf (x)) (x),

(3)

where q is a real-valued random variable. To see the relationship between gq(x) and f (x), let us observe that we have the following asymptotic properties:
Proposition 1. Assume f (x) is smooth and the expectation of q is E(q) = 1. Then x, we have

lim gq(x) = f (x)x.
Var(q)0

lim
Var(q)0

gq

(x)

=

f

(x)

+

f

(x)x,

where E(·) denotes the expectation, and Var(·) denotes the variance.

Proof.

f (x) - f (qx)

f (x) - f (qx)

lim gq(x) = lim

Var(q)0

Var(q)0

x - qx

x = lim
qxx

x - qx

x = f (x)x.

f (x) - qf (qx)

f (x) - qf (x) + qf (x) - qf (qx)

lim
Var(q)0

gq (x)

=

lim
Var(q)0

1-q

= lim
Var(q)0

1-q

qf (x) - qf (qx)

f (x) - f (qx)

= f (x) + lim
Var(q)0

1-q

= f (x) + lim
q1

x - qx

qx = f (x) + f (x)x.

Notice that as Var(q)  0, the limit of gq(x) is not f (x) but f (x)x. Thus informally speaking, the gradient of gq(x) carries second-order information of f (x). We further have the following property:
Proposition 2. We have:

Dp (gq (x))

=

1 1-

p Dqf (x)

-

p 1 - p Dp,pqf (x).

(4)

Proof.

Dp (gq (x))

=

gq(x) - (1 -

gq (px) p)x

=

(Dqf (x)) x - (Dqf (px)) px (1 - p)x

1p = 1 - p Dqf (x) - 1 - p Dqf (px).

Since Dqf (px)

=

f (px)-f (pqx) px-pqx

=

f (px)-f (pqx) (p-pq)x

=

Dp,pqf (x),

eq. (4) is

straightforward.

(5)

By proposition 2, the p-derivative of the q-activation gq(x) agrees with the original activation function f .

See table 1 for a list of activation functions with their corresponding functions f (x)x, where

sigm(x) = 1/(1 + exp(-x)) is the sigmoid function, softplus(x) = log(1 + exp(x)) is the softplus

function,

relu(x) =

x if x  0 0 otherwise

2

Under review as a conference paper at ICLR 2019

Table 1: Common activation functions f (x) with their corresponding limit cases limVar(q)0 gq(x) = f (x)x.

f (x)

sigm(x)

tanh(x) relu(x) softplus(x)

f (x)x sigm(x)(1 - sigm(x))x sech2(x)x relu(x) sigm(x)x

elu(x) x x0  exp(x)x x < 0

0 0

1 1

1-

1+

q

1-

1+

q

Figure 1: The probability density function of stochastic variable q used when calculating qderivatives.

is the Rectified Linear Unit (ReLU) Maas et al. (2013), and

elu(x) =

x if x  0 (exp(x) - 1) otherwise

denotes the Exponential Linear Unit (ELU) Clevert et al. (2016). A common choice for the random variable q that is used in our experiments is

q = 1 + (2[  0] - 1) (| | + ) ,

(6)

where  N (0, 1) follows the standard Gaussian distribution, [·] denotes the Iverson bracket (meaning 1 if the proposition is satisfied, and 0 otherwise),  > 0 is a scale parameter of q, and  = 10-3 is the smallest absolute value of q so as to avoid division by zero. See fig. 1 for the density function plots of q defined on (-, -]  [, ]

To implement q-neurons, one only need to tune the hyper-parameter . It can either be fixed to a small value, e.g. 0.02 or 0.05 during learning, or be annealed from an initial value 0. Such an annealing scheme can be set to



=

1+

0 (T

- 1) ,

(7)

where T = 1, 2, · · · is the index of the current epoch, and  is a decaying rate parameter. This parameter  can be empirically fixed based on the total number of epochs: For example, in our experiments, we train 100 epochs and apply  = 0.5, so that in the final epochs  is a small value (around 0.020). We will investigate both of those two cases in our experiments.
Let us stress out that deep learning architectures based on stochastic q-neurons are scalable and easy to implement. There is no additional free parameter imposed. The computational overhead of gq(x) as compared to f (x) involves sampling one Gaussian random variable, and then calling f (x) two times and computing gq(x) according to eq. (3). In our Python implementation, the core implementation of q-neuron is only in three lines of codes (see A.3).

Alternative approaches to inject stochasticity into neural network training include dropout Srivastava et al. (2014), gradient noise Neelakantan et al. (2016), etc. Both q-neuron and dropout modify the forward pass of the neural network. In the experimental section, we will investigate the effect of q-neurons with or without dropout. q-neuron contracts from the broad array of heristic-based DNN ingredients in that it is based on q-calculus, which combines stochasticity and some second-order information in an easy-to-compute way.

3

Under review as a conference paper at ICLR 2019

 = 0.1

 = 0.2

 = 0.8

-2 2 -2 2 -2 2 -2 2

-2 2 -2 2 -2 2 -2 2

2

qtanh

2 -2

-3 x

3 -3

x

3 -3

x

3

 = 0.2

 = 0.5

 = 0.8

qrelu

2 -2

-3 x

3 -3

 = 0.05

x
 = 0.2

3 -3

x
 = 0.8

3

2 -2 qelu

-3 x

3 -3

x

3 -3

 = 0.1

 = 0.5

x
=1

3

qsoftplus

-2

-3 x

3 -3

x

3 -3

x

3

Figure 2: The density function of q-neurons with q sampled according to eq. (6) for different values of . The activation is roughly a deterministic function f (x)x for small  as shown in table 1. The activation is random for large . Darker color indicates higher probability density.

3 EXPERIMENTS
We carried experiments on classifying MNIST digits1 and CIFAR10 images2 using Convolutional Neural Networks (CNNs) and Multi-Layer Perceptrons (MLPs). Our purpose is not to beat state-ofthe-art records but to investigate the effect of applying q-neuron and its hyper-parameter sensitivity.
1http://yann.lecun.com/exdb/mnist/ 2https://www.cs.toronto.edu/~kriz/cifar.html
4

Under review as a conference paper at ICLR 2019
The MNIST-CNN architecture is given as follows: 2D convolution with 3×3 kernel and 32 features; (q-)activation; batch normalization; 2D convolution with 3×3 kernel and 32 features; (q-)activation; 2 × 2 max-pooling; batch normalization; 2D convolution with 3 × 3 kernel and 64 features; (q)activation; batch normalization; 2D convolution with 3 × 3 kernel and 64 features; (q-)activation; 2 × 2 max-pooling; flatten into 1D vector; batch normalization; dense layer of output size 512; (q)activation; batch normalization; (optional) dropout layer with drop probability 0.2; dense layer of output size 10; soft-max activation.
The MNIST-MLP architecture is: dense layer of output size 256; (q-) activation; batch normalization; (optional) dropout layer with drop probability 0.2; dense layer of output size 256; (q-) activation; batch normalization; (optional) dropout layer with drop probability 0.2; dense layer of output size 10; soft-max activation.
The CIFAR-CNN architecture is: 2D convolution with 3 × 3 kernel and 32 features; (q-)activation; 2D convolution with 3 × 3 kernel and 32 features; (q-)activation; 2 × 2 max-pooling; (optional) dropout layer with drop probability 0.2; 2D convolution with 3 × 3 kernel and 64 features; (q)activation; 2D convolution with 3 × 3 kernel and 64 features; (q-)activation; 2 × 2 max-pooling; (optional) dropout layer with drop probability 0.2; flattern into 1D vector; dense layer of output size 512; (q-) activation; (optional) dropout layer with drop probability 0.1; dense layer of output size 10; soft-max activation.
We use the cross-entropy as the loss function. The model is trained for 100 epochs based on a stochastic gradient descent optimizer with a mini-batch size of 64 (MNIST) or 32 (CIFAR) and a learning rate of 0.05 (MNIST) or 0.01 (CIFAR) without momentum. The learning rate is multiplied by (1 - 10-6) after each mini-batch update. We compare tanh, relu, elu, softplus activations with their q-counterparts. We either fix 0 = 0.02 or 0.1, or anneal from 0  {1, 5, 9} with  = 0.5. The learning curves are shown in figs. 3 to 5, where the training curves show the sample-average crossentropy values evaluated on the training set after each epoch, and the testing curves are classification accuracy. In all figures, each training or testing curve is an average over 10 independent runs.
For q-activation, c means the  parameter is fixed; a means the  is annealed based on eq. (7). For example, "c0.02" means  = 0.02 throughout the training process, while "a1" means that  is annealed from 0 = 1.
We see that in almost all cases, q-activation can consistently improve learning, in the sense that both training and testing errors are reduced. This implies that q-neurons can get to a better local optimum as compared to the corresponding deterministic neurons. The exception worth noting is q-relu, which cannot improve over relu activation. This is because gq(x) is very similar to the original f (x) for (piece-wisely) linear functions. By proposition 1, f (x) = 0 implies that the gradient of gq(x) and f (x) are similar for small Var(q). One is advised to use q-neurons only with curved activation functions such as elu, tanh, etc.
We also observe that the benefits of q-neurons are not sensitive to hyper-parameter selection. In almost all cases, q-neuron with  simply fixed to 0.02/0.1 can bring better generalization performance, while an annealing scheme can further improve the score. Setting  too large may lead to under-fit. One can benefit from q-neurons either with or without dropout.
On the MNIST dataset, the best performance with error rate 0.35% (99.65% accuracy) is achieved by the CNN architecture with q-elu and q-tanh. On the CIFAR10 dataset, the best performance of the CNN with accuracy 82.9% is achieved by q-elu.
4 CONCLUSION
We proposed the stochastic q-neurons based on converting activation functions into corresponding stochastic q-activation functions using Jackson's q-calculus. We found experimentally that qneurons can consistently (although slightly) improve the generalization performance, and can goes deeper on the error surface.
5

Under review as a conference paper at ICLR 2019

test error (percentage)

train loss

test error (percentage)

train loss

0.06 0.05 0.04 0.03 0.02 0.01
0
0.06 0.05 0.04 0.03 0.02 0.01
0
0.05
0.04
0.03
0.02
0.01 0
0.07 0.06 0.05 0.04 0.03 0.02 0.01
0

tanh
0.4

tanh (99.57%) qtanh (c0.02) (99.63%) qtanh (c0.1) (99.61%) qtanh (a1.0) (99.62%) qtanh (a5.0) (99.65%) qtanh (a9.0) (99.63%)

0.5 0.6 0.7

0.8

test error (percentage) train loss

0.07 0.06 0.05 0.04 0.03 0.02

0.9 20 40 60 80 100
erpeolcuh 0.4

0

0.07

0.5 0.06

relu (99.56%) qrelu (c0.02) (99.57%) qrelu (c0.1) (99.57%) qrelu (a1.0) (99.58%) qrelu (a5.0) (99.57%) qrelu (a9.0) (99.59%)

0.6 0.7 0.8

test error (percentage) train loss

0.05 0.04 0.03

0.9 0.02

1 20 40 60 80 100
eeplouch

0

0.4 0.05 0.45

elu (99.59%) qelu (c0.02) (99.61%) qelu (c0.1) (99.60%) qelu (a1.0) (99.62%) qelu (a5.0) (99.62%) qelu (a9.0) (99.62%)

0.5 0.55 0.6 0.65

0.7

test error (percentage) train loss

0.04 0.03 0.02

0.75 0.01

20 40 60 80 100
seopfotpchlus

0 0.08

0.5 0.07

softplus (99.47%) qsoftplus (c0.02) (99.55%) qsoftplus (c0.1) (99.56%) qsoftplus (a1.0) (99.57%) qsoftplus (a5.0) (99.57%) qsoftplus (a9.0) (99.56%)

0.6 0.7 0.8

0.9

test error (percentage) train loss

0.06 0.05 0.04 0.03 0.02

1 20 40 epoch 60 80 100

0.01 0

tanh_dropout

0.4

tanh (99.59%) qtanh (c0.02) (99.62%) qtanh (c0.1) (99.62%) qtanh (a1.0) (99.62%) qtanh (a5.0) (99.63%) qtanh (a9.0) (99.63%)

0.5 0.6 0.7

0.8
20 40 60 80 100
relue_pdorcohpout
0.4

0.5

relu (99.60%) qrelu (c0.02) (99.61%) qrelu (c0.1) (99.60%) qrelu (a1.0) (99.61%) qrelu (a5.0) (99.61%) qrelu (a9.0) (99.62%)

0.6 0.7

0.8

0.9 20 40 60 80 100
elu_edproocphout
0.35

0.4

elu (99.62%) qelu (c0.02) (99.64%) qelu (c0.1) (99.65%) qelu (a1.0) (99.63%) qelu (a5.0) (99.65%) qelu (a9.0) (99.64%)

0.45 0.5 0.55

0.6

0.65

20 40 60 80
softpleupso_cdhropout

100 0.4

0.5

softplus (99.51%) qsoftplus (c0.02) (99.59%) qsoftplus (c0.1) (99.59%) qsoftplus (a1.0) (99.60%) qsoftplus (a5.0) (99.59%) qsoftplus (a9.0) (99.61%)

0.6 0.7 0.8

0.9

1

20 40 epoch 60 80 100

test error (percentage)

train loss

test error (percentage)

train loss

Figure 3: Training loss (descending curves) and testing accuracy (ascending curves) of a CNN on the MNIST dataset, using different activation functions (from top to bottom), with (left) or without (right) dropout.

6

Under review as a conference paper at ICLR 2019

test error (percentage)

train loss

test error (percentage)

train loss

tanh tanh_dropout

0.225 1 0.30

1

0.200 0.175 1.5 0.25

1.5

test error (percentage) train loss

0.150 0.125 0.100

tanh (99.08%) qtanh (c0.02) (99.10%) qtanh (c0.1) (99.09%) qtanh (a1.0) (99.13%) qtanh (a5.0) (99.14%) qtanh (a9.0) (99.15%)

2 2.5

0.20 0.15

tanh (98.86%) qtanh (c0.02) (99.08%) qtanh (c0.1) (99.08%) qtanh (a1.0) (99.10%) qtanh (a5.0) (99.11%) qtanh (a9.0) (99.10%)

2 2.5

0.075 3 0.10

3

0.050 3.5 0 20 40 epoch 60 80 100
relu

3.5 0 20 40 60 80 100
relue_pdorcohpout

0.225 1 0.30

1

0.200 0.175 1.5 0.25

1.5

test error (percentage) train loss

0.150 0.125

relu (99.14%) qrelu (c0.02) (99.15%) qrelu (c0.1) (99.13%) qrelu (a1.0) (99.14%) qrelu (a5.0) (99.16%) qrelu (a9.0) (99.15%)

2 2.5

0.20

relu (99.14%) qrelu (c0.02) (99.15%) qrelu (c0.1) (99.13%) qrelu (a1.0) (99.15%) qrelu (a5.0) (99.13%) qrelu (a9.0) (99.13%)

2 2.5

0.100 0.15 0.075 3

3

0.050

0.10 3.5

3.5

0 20 40 60 80 100 epoch
elu

0 20 40 60 80 100
elu_edproocphout

1 0.30 0.200

1

0.175 1.5 0.25

1.5

test error (percentage) train loss

0.150 0.125 0.100

elu (99.04%) qelu (c0.02) (99.15%) qelu (c0.1) (99.15%) qelu (a1.0) (99.15%) qelu (a5.0) (99.17%) qelu (a9.0) (99.17%)

2 2.5

0.20 0.15

elu (98.89%) qelu (c0.02) (99.17%) qelu (c0.1) (99.18%) qelu (a1.0) (99.17%) qelu (a5.0) (99.17%) qelu (a9.0) (99.16%)

2 2.5

3

0.075 3 0.10 0.050

3.5

0 20 40 60 80 100 3.5

0 20 40 60 80 100

0.250

epoch
softplus

1 0.40

softpluesp_odchropout

1

0.225 1.5 0.35 0.200

1.5 2

test error (percentage) train loss

0.175 0.150 0.125 0.100

softplus (98.85%) qsoftplus (c0.02) (99.07%) qsoftplus (c0.1) (99.08%) qsoftplus (a1.0) (99.09%) qsoftplus (a5.0) (99.10%) qsoftplus (a9.0) (99.12%)

2 2.5 3

0.30 0.25 0.20

softplus (98.52%) qsoftplus (c0.02) (99.09%) qsoftplus (c0.1) (99.09%) qsoftplus (a1.0) (99.10%) qsoftplus (a5.0) (99.10%) qsoftplus (a9.0) (99.07%)

2.5 3 3.5

0.15 4

0.075 3.5

0.050 0.10 4.5

0 20 40 epoch 60 80 100

0 20 40 epoch 60 80 100

test error (percentage)

train loss

test error (percentage)

train loss

Figure 4: Training loss (descending curves) and testing accuracy (ascending curves) of a MLP on MNIST.

7

Under review as a conference paper at ICLR 2019

test error (percentage)

train loss

test error (percentage)

train loss

1.2 1.1 1.0 0.9 0.8 0.7 0.6 0.5
0 1.4
1.2
1.0
0.8
0.6
0
1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4
0
1.6 1.4 1.2 1.0 0.8 0.6
0

tanh
20 22.5 1.2

25

tanh (78.76%) qtanh (c0.02) (80.24%) qtanh (c0.1) (80.19%) qtanh (a1.0) (80.37%) qtanh (a5.0) (80.63%) qtanh (a9.0) (80.35%)

27.5 30 32.5

35

test error (percentage) train loss

1.1 1.0 0.9 0.8

37.5 0.7

40

20 40 60 80 100
eproeclhu
20

0

1.4

25

relu (80.43%) qrelu (c0.02) (80.36%) qrelu (c0.1) (80.44%) qrelu (a1.0) (80.37%) qrelu (a5.0) (80.27%) qrelu (a9.0) (80.44%)

30 35 40

test error (percentage) train loss

1.2 1.0 0.8

45

0.6

20

40 60
eepluoch

80

100 17.5

0 1.2

20

22.5

elu (81.57%) qelu (c0.02) (81.93%) qelu (c0.1) (81.96%) qelu (a1.0) (82.03%) qelu (a5.0) (82.28%) qelu (a9.0) (82.48%)

25 27.5 30

32.5

test error (percentage) train loss

1.1 1.0 0.9 0.8 0.7

35 0.6

20

40

60

80

37.5 100

0.5 0

seopfotcphlus

2.0

25 1.8

30

softplus (72.02%) qsoftplus (c0.02) (78.22%) qsoftplus (c0.1) (78.40%) qsoftplus (a1.0) (78.07%) qsoftplus (a5.0) (77.94%) qsoftplus (a9.0) (78.17%)

35 40 45

test error (percentage) train loss

1.6 1.4 1.2 1.0

50 0.8

55 0.6

20 40 epoch 60 80 100

0

tanh_dropout

22.5

25

tanh (77.77%) qtanh (c0.02) (79.67%) qtanh (c0.1) (79.62%) qtanh (a1.0) (79.80%) qtanh (a5.0) (79.58%) qtanh (a9.0) (79.31%)

27.5 30 32.5 35

37.5

40

20 40 60 80 100
relue_pdocrohpout
20

25

relu (80.94%) qrelu (c0.02) (80.76%) qrelu (c0.1) (80.80%) qrelu (a1.0) (80.73%) qrelu (a5.0) (80.65%) qrelu (a9.0) (80.49%)

30 35 40

45

20 40 60
elu_deproocphout

80 100 50

17.5

20

22.5

elu (81.91%) qelu (c0.02) (82.76%) qelu (c0.1) (82.84%) qelu (a1.0) (82.94%) qelu (a5.0) (82.86%) qelu (a9.0) (82.77%)

25 27.5 30 32.5

35

20 40 60 80
softpleupso_cdhropout

37.5 100
20

30

softplus (70.64%) qsoftplus (c0.02) (80.08%) qsoftplus (c0.1) (80.14%) qsoftplus (a1.0) (80.09%) qsoftplus (a5.0) (79.78%) qsoftplus (a9.0) (79.76%)

40 50

60

70 20 40 epoch 60 80 100

test error (percentage)

train loss

test error (percentage)

train loss

Figure 5: Training loss (descending curves) and testing accuracy (ascending curves) of a CNN on CIFAR10.

8

Under review as a conference paper at ICLR 2019

REFERENCES
Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with Bregman divergences. Journal of machine learning research, 6(Oct):1705­1749, 2005.
Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (ELUs). In ICLR, 2016. arXiv 1511.07289.
Thomas Ernst. A comprehensive treatment of q-calculus. Springer Science & Business Media, 2012.
E´ rica JC Gouve^a, Rommel G Regis, Aline C Soterroni, Marluce C Scarabello, and Fernando M Ramos. Global optimization using q-gradients. European Journal of Operational Research, 251 (3):727­738, 2016.
Frank Hilton Jackson. On q-functions and a certain difference operator. Earth and Environmental Science Transactions of The Royal Society of Edinburgh, 46(2):253­281, 1909.
Victor Kac and Pokman Cheung. Quantum Calculus. Universitext. Springer New York, 2001. ISBN 9780387953410.
Shujaat Khan, Alishba Sadiq, Imran Naseem, Roberto Togneri, and Mohammed Bennamoun. Enhanced q-least mean square. 2018. arXiv:1801.00410 [math.OC].
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier non-linearities improve neural network acoustic models. In ICML, 2013.
Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. In ICLR, 2016.
P Njionou Sadjang. On the fundamental theorem of (p, q)-calculus and some (p, q)-Taylor formulas. arXiv preprint arXiv:1309.3934, 2013.
Aline C Soterroni, Roberto L Galski, and Fernando M Ramos. The q-gradient method for continuous global optimization. In AIP Conference Proceedings, volume 1558, pp. 2389­2393. AIP, 2013.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of machine learning research, 15:1929­1958, 2014.
Zhen Xu and Frank Nielsen. Beyond ordinary stochastic gradient descent, March 2018. preprint INF517.

A BRIEF OVERVIEW OF THE (p, q)-DIFFERENTIAL CALCULUS

For p, q  R, p = q, define the (p, q)-differential:

dp,qf (x) := f (qx) - f (px).

In particular, dp,qx = (q - p)x.

The (p, q)-derivative is then obtained as:

Dp,qf (x)

:=

dp,qf (x) dp,q x

=

f (qx) - f (px) (q - p)x .

We have Dp,qf (x) = Dq,pf (x).

Consider a real-valued scalar function f (x). The differential operator D consists in taking the

derivative:

Df (x)

=

d dx

=

f

(x).

The (p, q)-differential operator Dp,q for two distinct scalars p and q is defined by taking the following finite difference ratio:

Dp,qf (x) :=

f

(px)-f (qx) (p-q)x

,

x = 0,

f (0),

x = 0.

(8)

9

Under review as a conference paper at ICLR 2019

We have Dp,qf (x) = Dq,pf (x).

The (p, q)-derivative is an extension of Jackson's q-derivative Jackson (1909); Kac & Cheung (2001); Ernst (2012); Sadjang (2013) historically introduced in 1909. Notice that this finite difference differential operator that does not require to compute limits (a useful property for derivativefree optimization), and moreover can be applied even to nondifferentiable or discontinuous functions.

An important property of the (p, q)-derivative is that it generalizes the ordinary derivative:

Lemma 3. For a twice continuously differentiable function f , we have limpq Dp,qf (x) =

1 q

Df

(qx)

=

1 q

f

(qx)

and

limx0 Dp,qf (x)

=

Df (0).

Proof. Let us write the first-order Taylor expansion of f with exact Lagrange remainder for a twice continuously differentiable function f :

f (qx)

=

f (px)

+

(qx

-

px)f

(px)

+

1 (qx

-

px)2f

(),

2

for   (min{px, qx}, max{px, qx}).

It follows that

f (px) - f (qx) f (qx) - f (px)

Dp,qf (x) =

x(p - q) = x(q - p) ,

=

f

(px)

+

1 x(q

-

p)f

().

2

(9) (10)

Thus,

whenever

p

=

q

we

have Dp,qf (x)

=

f

(px)

=

1 p

Df

(px),

and

whenever x

=

0,

we

have

Dp,qf (0) = f (0). In particular, when p = 1, we have Dqf (x) = f (x) when q = 1 or when

x = 0.

Let us denote Dq the q-differential operator Dq := D1,q = Dq,1.

The following (p, q)-Leibniz rules hold:

Since BF (qx : px) := f (qx) - f (px) - (qx - px)f (px) =

1 2

(qx

-

px)2f

(), we can further

express the (p, q)-differential operator using Bregman divergences Banerjee et al. (2005) as follows:

Corollary 4. We have:

Dp,qf (x)

=

f (px)

-

f (qx)

=

f

(px)

+

BF (qx

:

px) ,

x(p - q)

x(p - q)

=

f (qx)

-

f (px)

=

f

(qx)

+

BF (px

:

qx) .

x(q - p)

x(q - p)

A.1 LEIBNIZ (p, q)-RULES OF DIFFERENTIATION

· Sum rule (linear operator):

Dp,q(f (x) + g(x)) = Dp,qf (x) + Dp,qg(x)

· Product rule:

Dp,q(f (x)g(x)) = f (px) + Dp,qg(x) + g(qx)Dp,qf (x), = f (qx) + Dp,qg(x) + g(px)Dp,qf (x).

· Ratio rule:

Dp,q(f (x)/g(x))

=

g(qx)Dp,qf (x) - f (qx)Dp,qg(x) , g(px)g(qx)

= g(px)Dp,qf (x) - f (px)Dp,qg(x) . g(px)g(qx)

General Leibniz rule for (p, q)-calculus.

10

Under review as a conference paper at ICLR 2019

A.2 THE (p, q)-GRADIENT OPERATOR

For a multivariate function F (x) = F (x1, . . . , xd) with x = (x1, . . . , xd), let us define the firstorder partial derivative and i  [d] and pi = qi,

Dp,q,xi F (x) := =

F (x1,...,pixi,...,xn)-F (x1,...,qixi,...,xn) (pi -qi )xi
F (x)
xi

xi = 0, xi = 0

F (x+(pi-1)ei)-F (x+(qi-1)ei) (pi -qi )xi
F (x)
xi

xi = 0, , xi = 0

where ei is a one-hot vector with the i-th coordinate at one, and all other coordinates at zero.

The generalization of the (p, q)-gradient Soterroni et al. (2013) follows by taking d-dimensional vectors for p = (p1, . . . , pd) and q = (q1, . . . , qd):

 Dp1,q1,x1 F (x) 

p,q F

(x)

:=

 

...

 

Dpd,qd,xd F (x)

The (p, q)-gradient is a linear operator: p,q(aF (x) + bG(x)) = ap,qF (x) + bp,qG(x) for any constants a and b. When p, q  1, p,q  : That is, the (p, q)-gradient operator extends the
ordinary gradient operator.

A.3 PSEUDO-CODES
We can easily implement q-neurons based on the following reference code, which is based on a given activation function activate. Note, q has the same shape as x. One can fix eps= 10-3 and only has to tune the hyper-parameter lambda.
def qactivate( x, lambda, eps ): q = random_normal( shape=shape(x) ) q = ( 2*( q>=0 )-1 ) * ( lambda * abs(q) + eps ) return ( activate( x * (1+q) ) - activate( x ) ) / q

11

