Under review as a conference paper at ICLR 2019
RETHINKING LEARNING RATE SCHEDULES
FOR STOCHASTIC OPTIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
There is a stark disparity between the learning rate schedules used in the practice of large scale machine learning and what are considered admissible learning rate schedules prescribed in the theory of stochastic approximation. Recent results, such as in the 'super-convergence' methods which use oscillating learning rates, serve to emphasize this point even more. One plausible explanation is that non-convex neural network training procedures are better suited to the use of fundamentally different learning rate schedules, such as the "cut the learning rate every constant number of epochs" method (which more closely resembles an exponentially decaying learning rate schedule); note that this widely used schedule is in stark contrast to the polynomial decay schemes prescribed in the stochastic approximation literature, which are indeed shown to be (worst case) optimal for classes of convex optimization problems. The main contribution of this work shows that the picture is far more nuanced, where we do not even need to move to non-convex optimization to show other learning rate schemes can be far more effective. In fact, even for the simple case of stochastic linear regression with a fixed time horizon, the rate achieved by any polynomial decay scheme is suboptimal compared to the statistical minimax rate (by a factor of condition number); in contrast the "cut the learning rate every constant number of epochs" provides an exponential improvement (depending only logarithmically on the condition number) compared to any polynomial decay scheme. Finally, it is important to ask if our theoretical insights are somehow fundamentally tied to quadratic loss minimization (where we have circumvented minimax lower bounds for more general convex optimization problems)? Here, we conjecture that recent results which make the gradient norm small at a near optimal rate, for both convex and non-convex optimization, may also provide more insights into learning rate schedules used in practice.
1 INTRODUCTION
The recent advances in machine learning and deep learning rely almost exclusively on stochastic optimization methods, primarily SGD and its variants. Here, these large scale stochastic optimization methods are manually (and often painstakingly) tuned to the problem at hand (often with parallelized hyper-parameter searches), where there is, as of yet, no class of "universal methods" which uniformly work well on a wide range of problems with little to no hyper-parameter tuning. This is in stark contrast to non-stochastic numerical optimization methods, where it is not an overstatement to argue that the l-BFGS and non-linear conjugate gradient methods (with no hyper-parameter tuning whatsoever) have provided nearly unbeatable procedures (for a number of decades) on nearly every unconstrained convex and non-convex problem. In the land of stochastic optimization, there are two dominant (and somewhat compatible approaches): those methods which often manually tune learning rate schedules to achieve the best performance (Krizhevsky et al., 2012; Sutskever et al., 2013; Kingma & Ba, 2014; Kidambi et al., 2018) and those methods which rely on various forms of approximate preconditioning (Duchi et al., 2011; Tieleman & Hinton, 2012; Kingma & Ba, 2014). This works examines the former class of methods, where we seek a more refined understanding of the issues of learning rate scheduling, through both theoretical analysis and empirical studies.
Learning rate schedules for SGD is a rather enigmatic topic since there is a stark disparity between what is considered admissible in theory and what is employed in practice to achieve the best re-
1

Under review as a conference paper at ICLR 2019

sults. Let us elaborate on this distinction more clearly. In theory, a vast majority of works starting

with Robbins & Monro (1951); Polyak & Juditsky (1992) consider learning rates that have the form

of t

=

a b+t

for some a, b



0 and 1/2

<





1 ­ we call these polynomial decay schemes.

The key property enjoyed by these polynomial decay schemes is that they are not summable but are

square summable. A number of works obtain bounds on the asymptotic convergence rates of such

schemes. Note that the focus of these works is to design learning rate schemes that work well for

all large values of t. In contrast, practitioners are interested in achieving the best performance given

a computational budget or equivalently a fixed time horizon T e.g., 100 passes on training dataset

with a batch size of 128.

The corresponding practically best performing learning rate scheme is often one where the step

size is cut by a constant factor once every few epochs, or, equivalently, when no progress is made

on a validation set (Krizhevsky et al., 2012; He et al., 2016b) (often called a dev set based decay

scheme). Such schemes are widely popular to the extent that they are available as schemes in deep

learning libraries such as PyTorch 1 and several such useful tools of the trade are taught on popular

deep learning courses 2. Furthermore, what is (often) puzzling (from a theory perspective) is the

emphasis that is laid on "babysitting" the learning rates 3 to achieve the best performance. Why

do practitioners use constant and cut learning rate schemes while most of the theory work routinely

works with polynomial decaying schemes? Of course, implicit to this question is the view that both

of these schemes are not equivalent. Indeed if both of these were equivalent, one could parameterize

the learning rate as

a b+t

and do hyperparameter search over a, b and .

In practice, this simply

does not give results comparable to the constant and cut schemes.4 One potential explanation for

this could be that, in the context of neural network training, local minima found by constant and

cut schemes are of much better quality than those found by polynomial decay schemes, while for

convex problems, polynomial decay schemes are indeed optimal.

The primary contribution of this work is to show that this is simply not the case. We concretely show how minimax optimal theoretical learning rates (i.e. polynomial decay schemes for wide classes of convex optimization problems) may be misleading (and sub-optimal for locally quadratic problems), and the story in practice is more nuanced. There important issues at play with regards to this suboptimality. First, even for the simple case of stochastic linear regression, with a fixed time horizon, the rate achieved by any polynomial decay scheme (i.e., any choice of a, b and ) is suboptimal compared to the statistical minimax rate (i.e., information theoretically best possible rate achievable by any algorithm) by a factor of condition number  (see Section 3 for definitions), while there exist constant and cut schemes that are suboptimal only by a factor of log .

Second, this work shows that a factor of  suboptimality is unavoidable if we wish to bound the error of each iterate of SGD. In other words, we show that the convergence rate of lim sup of the error, as t  , has to be necessarily suboptimal by a factor of ~ () compared to the statistical minimax rate, for any learning rate sequence (polynomial or not). In fact, at least ~ 1/ fraction of the iterates
have this suboptimality. With this result, things become quite clear ­ all the works in stochastic approximation try to bound the error of each iterate of SGD asymptotically (or lim sup of the error in other words). Since this necessarily has to be suboptimal by a factor of ~ () compared to the
statistical minimax rates, the suboptimality of polynomial decay rates is not an issue. However, with
a fixed time horizon, there exist learning rate schemes with much better convergence rates, while
polynomial decay schemes fail to get better rates in this simpler setting (of known time horizon).

Thirdly, the work shows that, for stochastic linear regression, if we consider lim inf (rather than lim sup) of the error, it is possible to design schemes that are suboptimal by only a factor of log  compared to the minimax rates. Variants of the constant and cut schemes achieve this guarantee.

In summary, the contributions of this paper are showing how widely used pratical learning rate schedules are, in fact, highly effective even in the convex case. In particular, our theory and empirical results demonstrate this showing that:

1https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.
ReduceLROnPlateau 2http://cs231n.github.io/ 3http://cs231n.github.io/neural-networks-3/ 4In fact, this work shows an instance where there is a significant (provable) difference between the perfor-
mance of these two schemes.

2

Under review as a conference paper at ICLR 2019
· For a fixed time horizon, constant and cut schemes are provably, significantly better than polynomial decay schemes.
· There is a fundamental difference between fixed time horizon and infinite time horizon. · The above difference can be mitigated by considering lim inf of error instead of lim sup. · In addition to our theoretical contributions, we empirically verify the above claims for
neural network training on cifar-10.
Extending results on the performance of constant and cut schemes to more general convex optimization problems, beyond stochastic linear regression, is an important future direction. However, the fact that the suboptimality of polynomial decay schemes even for the simple case of stochastic linear regression, has not been realized after decades of research on stochastic approximation is striking.
In summary, the results of this paper show that, even for stochastic linear regression, the popular in practice, constant and cut learning rate schedules are provably better than polynomial decay schemes popular in theory and that there is a need to rethink learning rate schemes and convergence guarantees for stochastic approximation. Our results also suggest that current approaches to hyperparameter tuning of learning rate schedules might not be right headed and further suggest potential ways of improving them.
Paper organization: The paper is organized as follows. We review related work in Section 2. Section 3 describes the notation and problem setup. Section 4 presents our results on the suboptimality of both polynomial decay schemes and constant and cut schemes. Section 5 presents results on infinite horizon setting. Section 6 presents experimental results and Section 7 concludes the paper.
2 RELATED WORK
We will split related work into two parts, one based on theory and the other based on practice.
Related efforts in theory: SGD and the problem of stochastic approximation was introduced in the seminal work of Robbins & Monro (1951); this work also elaborates on stepsize schemes that are satisfied by asymptotically convergent stochastic gradient methods: we refer to these schemes as "convergent" stepsize sequences. The (asymptotic) statistical optimality of iterate averaged SGD with larger stepsize schemes of O(1/n) with   (0.5, 1) was proven in the seminal works of Ruppert (1988); Polyak & Juditsky (1992). The notions of convergent learning rate schemes in stochastic approximation literature has been studied in great detail (Ljung et al., 1992; Kushner & Yin, 2003; Bharath & Borkar, 1999; Lai, 2003). Nearly all of the aforementioned works rely on function value sub-optimality to measure convergence and rely on the notion of asymptotic convergence (i.e. in the limit of the number of updates of SGD tending to infinity) to derive related "convergent stepsize schedules". Along this line of thought, there are several efforts that prove (minimax) optimality of the aforementioned rates (in a worst case sense and not per problem sense) e.g., Nemirovsky & Yudin (1983); Raginsky & Rakhlin (2011); Agarwal et al. (2012).
An alternative viewpoint is to consider gradient norm as a means to measure the progress of an algorithm. Along this line of thought are several works including the stochastic process viewpoint considered by Polyak & Juditsky (1992) and more recently, the work of Nesterov (2012) (working with deterministic (exact) gradients). The work of Allen-Zhu (2018) considers questions relating to making the gradient norm small when working with stochastic gradients, and provides an improved rate. We return to this criterion in Section 7.
In terms of oracle models, note that both this paper, as well as other results (Lacoste-Julien et al., 2012; Rakhlin et al., 2012; Bubeck, 2014), work in an oracle model that assumes bounded variance of stochastic gradients or similar assumptions. There is an alternative oracle model for analyzing SGD as followed in papers includingBach & Moulines (2013); Bach (2014); Jain et al. (2017) which is arguably more reflective of SGD's behavior in practice. For more details, refer to Jain et al. (2017). It is an important direction to prove the results of this paper working in the alternative practically more applicable oracle model.
Efforts in practice: As highlighted in the introduction, practical efforts in stochastic optimization have diverged from the classical theory of stochastic approximation, with several deep learning
3

Under review as a conference paper at ICLR 2019

libraries like pytorch 5 providing unconventional alternatives such as cosine/sawtooth/dev set decay schemes, or even exponentially decaying learning rate schemes. In fact, a natural scheme used in training convolutional neural networks for vision is where the learning rate is cut by a constant factor after a certain number of epochs. Such schemes are essentially discretized variants of exponentially decaying learning rate schedules. We note that there are other learning rate schedules that have been recently proposed such as sgd with warm restarts (Loshchilov & Hutter, 2016), oscillating learning rates (Smith & Topin, 2017) etc., that are unconventional and have attracted a fair bit of attention. Furthermore, exponential learning rates appear to be considered in more recent NLP papers (see for e.g., Krishnamurthy et al. (2017)) 6.

3 PROBLEM SETUP

Notation: We represent scalars with normal font a, b, L etc., vectors with boldface lowercase characters a, b etc. and matrices with boldface uppercase characters A, B etc. We represent positive semidefinite (PSD) ordering between two matrices using . The symbol represents that the direction of inequality holds for some universal constant.

Our theoretical results focus on the following additive noise stochastic linear regression problem.

We

present

the

setup

and

associated notation min f (w) where

fin(wth)isd=seef c1tiwon.

We wish Hw - w

to solve: b

wRd

2

for some positive definite matrix H and vector b.7 We denote the smallest and largest eigenvalues

of H by µ

>

0 and L

>

0.



d=ef

L µ

denotes

the

condition

number

of

H.

We have access to

a stochastic gradient oracle which gives us f (w) = f (w) + e, where e is a random vector

satisfying8 E [e] = 0 and E ee = 2H.

Given an initial point w0 and step size sequence t, the SGD algorithm proceeds with the update wt = wt-1 - tf (wt-1) = wt-1 - t (f (wt-1) + et) ,
where et are independent for various t and satisfy the above mean and variance conditions.

Let w d=ef arg minwRd f (w). The suboptimality of a point w is given by f (w) - f (w). It is well known that given t accesses to the stochastic gradient oracle above, any algorithm that uses

these stochastic gradients and outputs wt

has suboptimality that is lower bounded by

2 t

d

.

More

concretely (Van der Vaart, 2000), we have that

lim
t

E [f (wt)] - f (w) 2d/t



1.

Moreover

there

exist

schemes

that

achieve

this

rate

of

(1 + o(1))

2d t

e.g.,

constant

step

size

SGD

with averaging (Polyak & Juditsky, 1992). This rate of 2d/t is called the statistical minimax rate.

4 COMPARISON BETWEEN POLYNOMIAL DECAY SCHEMES VS CONSTANT
AND CUT SCHEMES

In this section, we will show that polynomial decay schemes are suboptimal compared to the statistical minimax rate by at least a factor of  while constant and cut schemes are suboptimal by at most a factor of log .

5see https://pytorch.org/docs/stable/optim.html for a complete list of alternatives

6Refer to their JSON file https://github.com/allenai/allennlp/blob/master/

training_config/wikitables_parser.jsonnet

7Any

linear

least

squares

1 2n

n i=1

xi w - yi

2 can be written as above with H d=ef

1 n

i xixi and

b d=ef

1 n

i yixi.

8While this might seem very special, this is indeed a fairly natural scenario. For instance, in stochastic linear

regression with independent additive noise, i.e., yt = xt w + t where t is a random variable independent of

xt and E [ t] = 0 and E

2 t

= 2, the noise in the gradient has this property. On the other hand, the results in

this paper can also be generalized to the setting where E ee = V for some arbitrary matrix V. However,

error covariance of 2H significantly simplifies exposition.

4

Under review as a conference paper at ICLR 2019

4.1 SUBOPTIMALITY OF POLYNOMIAL DECAY SCHEMES

Our first result shows that there exist problem instances where all polynomial decay schemes i.e.,

those

of

the

form

a b+t

,

for

any

choice

of

a, b

and



are

suboptimal

by

at

least

a

factor

of

()

compared to the statistical minimax rate.

Theorem 1. There exists a problem instance such that the initial function value f (w0)  2d, and for any fixed time T satisfying T  2, for all a, b  0 and 0.5    1, and for the learning rate

scheme

t

=

a b+t

,

we

have

E [f (wT )]

-

f (w)



 32

·

2 T

d

.

4.2 SUBOPTIMALITY OF CONSTANT AND CUT SCHEME

Our next result shows that there exist constant and cut schemes that achieve statistical minimax rate upto a multiplicative factor of only log  log2 T .

Theorem 2. For any problem and fixed time horizon T >  log(), there exists a constant and cut

learning

rate

scheme

that

achieves

E [f (wT )]

-

f (w)



f (w0)-f (w) T3

+

2 log 

·

log2

T

·

2 T

d

.

We will now consider an exponential decay scheme (in contrast to polynomial ones from Section 4.1) which is a smoother version of constant and cut scheme. We show that the same result above for constant and cut scheme can also be extended to the exponential decay scheme.

Theorem 3. For any problem and fixed horizon T , there exist constants a and b such that learning

rate scheme of t

=

b · exp (-at) achieves E [f (wT )] - f (w)



f (w0)-f (w) T 2-(1/100)

+

log



·

log

T

·

2d T

.

The above results show that constant and cut as well as exponential decay schemes, that depend on

the time horizon, are much better than polynomial decay schemes. Between these, exponential decay

schemes are smoother versions of constant and cut schemes, and so one would hope that they might

have better performance than constant and cut schemes ­ we do see a log T difference in our bounds.

One

unsatisfying

aspect

of

the

above

results

is

that

the

rate

behaves

as

log T T

,

which

is

asymptotically

worse

than

the

statistical

rate

of

1 T

.

It

turns

out

that it

is

indeed

possible

to

improve the

rate

to

1 T

using a more sophisticated scheme. The main idea is to use constant and polynomial schemes in

the beginning and then switch to constant and cut (or exponential decay) scheme later. To the best

of our knowledge, these kind of schemes have never been considered in the stochastic optimization

literature before. Using this learning rate sequence successively for increasing time horizons would

lead to oscillating learning rates. We leave a complete analysis of oscillating learning rates (for

moving time horizon) to future work.

Theorem 4. Fix   2. For any problem and fixed time horizon T / log T > 5, there exists a

learning

rate

scheme

that

achieves

E [f (wT )]

-

f (w)



f (w0)-f (w) T3

+

50 log2



·

2 T

d

.

5 INFINITE HORIZON SETTING

In this section we show a fundamental limitation of the SGD algorithm. First we will prove that the
SGD algorithm, for any learning rate sequence, needs to query a point with suboptimality more than (/ log ) · 2d/T for infinitely many time steps T .

Theorem 5. There exists a universal constant C > 0 such that for any SGD algorithm with t 

1/2

for

all

t9,

we

have

lim supT 

E[f (wT )]-f (w) (2d/T )



C

 log(+1)

.

Next we will show that in some sense the "fraction" of query points that has value more than  2/T

is at least (1/ ) when  is smaller than the threshold in Theorem 5.

Theorem 6.

There exists universal constants C1, C2

> 0 such that for any 



 CC1 log(+1)

where

C is the constant in Theorem 5, for any SGD algorithm and any number of iteration T > 0 there

exists a T



T

such

that

for

any

T~



[T

, (1 + 1/C2 )T

]

we

have

E[f (wT~ )]-f (w) (2d/T~)



.

Finally, we now show that there are constant and cut or exponentially decaying schemes that achieve the statistical minimax rate up to a factor of log  log2 T in the lim inf sense.
9Learning rate more than 2/ will make the algorithm diverge.

5

Under review as a conference paper at ICLR 2019

Model optimized for 50 optimized for 100 optimized for 200

err @50 0.00018 0.00018 0.00275

err @100 9.6 × 10-5 9.6 × 10-5
0.0007

err @200 9.6 × 10-5 9.6 × 10-5 2.5 × 10-5

Table 1: Does the learning rate sequence optimized for a given end time generalize to other end times? Observe how a model optimized to perform for a specific horizon Figure 1: Plot of final error versus condi- behaves sub-optimally for other time horizons. tion number  for decay schemes 1,2,3 for the 2-d regression problem.

Theorem 7. There exists an absolute constant C and a constant and cut learning rate scheme that

obtains

lim infT 

E[f (wT )]-f (w) (2d log2 T /T )



C log .

Similar results can be obtained for the exponential decay scheme of Theorems 3 and 4 with moving time horizon. However the resultant learning rates might have oscillatory behavior. This might partly explain the benefits of oscillating learning rates observed in practice (Smith & Topin, 2017).

6 EXPERIMENTAL RESULTS
We present experimental validation of our claims through controlled synthetic experiments on a twodimensional quadratic objective and on a real world non-convex optimization problem of training a residual network on the cifar-10 dataset, to illustrate the shortcomings of the traditional stochastic approximation perspective (and the advantages of non-convergent exponentially decaying and oscillating learning rate schemes) for a realistic problem encountered in practice. Complete details of experimental setup are given in Appendix D.

6.1 SYNTHETIC EXPERIMENTS: TWO-DIMENSIONAL QUADRATIC OBJECTIVE

We consider the problem of optimizing a two-dimensional quadratic objective, similar in spirit

as what is considered in the theoretical results of this paper. In particular, for a two-dimensional

quadratic, we have two eigenvalues, one of magnitude  and the other being 1. We vary our condition number   {50, 100, 200} and use a total of 200 iterations for optimization. The results ex-

pressed in this section are obtained by averaging over two random seeds. The learning rate schemes

we search over are:

t

=

0 1+b·

t

(1)

t

=

1

0 +b t

(2) t = 0 · exp (-b · t). (3)

For the schemes detailed above, there are two parameters that need to be searched over: (i) the

starting learning rate 0 and, (ii) the decay factor b. We perform a grid search over both these parameters and choose ones that yield the best possible final error at a given end time (i.e. 200).

We also make sure to extend the grid should a best performing grid search parameter fall at the edge

of the grid so that all presented results lie in the interior of our final grid searched parameters.

We will present results for the following experiments: (i) behavior of the error of the final iterate of the SGD method with the three learning rate schemes (1),(2), and (3) as we vary the condition number, and (ii) how the exponentially decaying learning rate scheme (3) optimized for a shorter time horizon behaves for a longer horizon.

For the variation of the final iterate's excess risk when considered with respect to the condition number (Figure 1), we note that polynomially decaying schemes have excess risk that scales linearly with condition number, corroborating Theorem 1. In contrast, exponentially decaying learning rate scheme admits excess risk that nearly appears to be a constant and corroborates Theorem 3. Finally, we note that the learning rate schedule that offers the best possible error in 50 or 100 steps does not offer the best error at 200 steps (Table 1).

6

Under review as a conference paper at ICLR 2019

Figure 2: Plot of the training function value (left) and test 0/1- error (right) comparing the three
decay schemes (two polynomial) 1, 2, (and one exponential) 3 scheme on the classification problem of cifar-10 with a 44-layer residual net with pre-activation blocks.

6.2 NON-CONVEX OPTIMIZATION: TRAINING A RESIDUAL NET ON CIFAR-10
We consider here the task of training a 44-layer deep residual network (He et al., 2016b) with pre-activation blocks (He et al., 2016a) (dubbed preresnet-44) for classifying images in the cifar10 dataset. The code for implementing the network employed in this paper can be found here10. For all the experiments, we use the Nesterov's Accelerated gradient method (Nesterov, 1983) implemented in pytorch 11 with a momentum set to 0.9 and batchsize set to 128, total number of training epochs set to 100, 2 regularization set to 0.0005.
Our experiments are based on grid searching for the best learning rate decay scheme on four parametric family of learning rate schemes described above 1,2,3; all gridsearches are performed on a separate validation set (obtained by setting aside one-tenth of the training dataset = 5000 images) and with models trained on the remaining 45000 images. For presenting the final numbers in the plots/tables, we employ the best hyperparameters from the validation stage and train it on the entire 50, 000 images and average results run with 10 different random seeds. The parameters for gridsearches and related details are presented in Appendix D. Furthermore, just as with the synthetic experiments, we always extend the grid so that the best performing grid search parameter lies in the interior of our grid search.
Comparison between different schemes: Figure 2 and Table 2 present a comparison of the performance of the three schemes (1)-(3). They clearly demonstrate that the best exponential scheme outperforms the best polynomial schemes.
Hyperparameter selection using truncated runs: Figure 3 and Tables 3 and 4 present a comparison of the performance of three exponential decay schemes each of which has the best performance at 33, 66 and 100 epochs respectively. The key point to note is that best performing hyperparameters at 33 and 66 epochs are not the best performing at 100 epochs (which is made stark from the perspective of the validation error). This demonstrates that selecting hyper parameters using truncated runs, which has been proposed in some recent efforts such as hyperband (Li et al., 2017), might necessitate rethinking.

Decay Scheme
O(1/t) (equation 1) O(1/ t) (equation 2) exp(-t) (equation 3)

Train Function Value
0.0713 ± 0.015 0.1119 ± 0.036 0.0053 ± 0.0015

Test 0/1 error
10.20 ± 0.7% 11.6 ± 0.67% 7.58 ± 0.21%

Table 2: Comparing Train Softmax Function Value and Test 0/1 Error of various learning rate decay schemes for the classification task on cifar-10 using a 44-layer residual net with pre-activations.

10https://github.com/D-X-Y/ResNeXt-DenseNet 11https://github.com/pytorch
7

Under review as a conference paper at ICLR 2019

Figure 3: Plot of the training function value (left) and test 0/1- error (right) comparing the exponential decay scheme (equation 3), with parameters optimized for 33, 66 and 100 epochs on the classification problem of cifar-10 with a 44-layer residual net with pre-activation blocks.

Decay Scheme
exp(-t) [optimized for 33 epochs] (eqn 3) exp(-t) [optimized for 66 epochs] (eqn 3) exp(-t) [optimized for 100 epochs] (eqn 3)

Train FVal @33
0.098 ± 0.006 0.107 ± 0.012
0.3 ± 0.06

Train FVal @66
0.0086 ± 0.002 0.0088 ± 0.0014
0.071 ± 0.017

Train FVal @100
0.0062 ± 0.0015 0.0061 ± 0.0011 0.0053 ± 0.0016

Table 3: Comparing Train Softmax Function Value of various learning rate decay schemes for the classification task on cifar-10 using a 44-layer residual net with pre-activations.

Decay Scheme
exp(-t) [optimized for 33 epochs] (eqn 3) exp(-t) [optimized for 66 epochs] (eqn 3) exp(-t) [optimized for 100 epochs] (eqn 3)

Test 0/1 @33
10.36 ± 0.235% 10.51 ± 0.45% 14.42 ± 1.47%

Test 0/1 @66
8.6 ± 0.26% 8.51 ± 0.13% 9.8 ± 0.66%

Test 0/1 @100
8.57 ± 0.25% 8.46 ± 0.19% 7.58 ± 0.21%

Table 4: Comparing Test 0/1 error of various learning rate decay schemes for the classification task on cifar-10 using a 44-layer residual net with pre-activations.

7 CONCLUSIONS AND DISCUSSION
The main contribution of this work shows that the picture of learning rate scheduling is far more nuanced than suggested by prior theoretical results, where we do not even need to move to nonconvex optimization to show other learning rate schemes can be far more effective than the standard polynomially decaying rates considered in theory.
Is quadratic loss minimization special? One may ask if there is something particularly special about why the minimax rates are different for quadratic loss minimization as opposed to more general convex (and non-convex) optimization problems? Ideally, we would hope that our theoretical insights (and improvements) can be formally established in more general cases. Here, an alternative viewpoint is to consider gradient norm as a means to measure the progress of an algorithm. The recent work of Allen-Zhu (2018) shows marked improvements for making the gradient norm small (when working with stochastic gradients) for both convex and non-convex, in comparison to prior results. In particular, for the strongly convex case, Allen-Zhu (2018) provides results which have only a logarithmic dependency on , an exponential improvement over what is implied by standard analyses for the gradient norm (Lacoste-Julien et al., 2012; Rakhlin et al., 2012; Bubeck, 2014); Allen-Zhu (2018) also provides improvements for the smooth and non-convex cases. Thus, for the case of making the gradient norm small, there does not appear to be a notable discrepancy between the minimax rate of quadratic loss minimization in comparison to more general strongly convex (or smooth) convex optimization problems. Interestingly, the algorithm of Allen-Zhu (2018) provides a recursive regularization procedure that obtains an SGD procedure, where the doubling regularization can be viewed as being analogous to an exponentially decaying learning rate schedule. Further work in this direction may be promising in providing improved algorithms.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright. Informationtheoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 2012.
Zeyuan Allen-Zhu. How to make the gradients small stochastically. CoRR, abs/1801.02982, 2018.
Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression. Journal of Machine Learning Research (JMLR), volume 15, 2014.
Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n). In NIPS 26, 2013.
B. Bharath and V. S. Borkar. Stochastic approximation algorithms: overview and recent trends. Sa¯dhana¯, 1999.
Se´bastien Bubeck. Theory of convex optimization for machine learning. CoRR, abs/1405.4980, 2014.
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121­2159, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV (4), Lecture Notes in Computer Science, pp. 630­645. Springer, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770­778, 2016b.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of existing momentum schemes for stochastic optimization. CoRR, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.
Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP, pp. 1516­1526, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
Harold J. Kushner and George Yin. Stochastic approximation and recursive algorithms and applications. Springer-Verlag, 2003.
Simon Lacoste-Julien, Mark W. Schmidt, and Francis R. Bach. A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method. CoRR, 2012. URL http://arxiv.org/abs/1212.2002.
Tze Leung Lai. Stochastic approximation: invited paper, 2003.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765­6816, 2017.
Lennart Ljung, Georg Pflug, and Harro Walk. Stochastic Approximation and Optimization of Random Systems. Birkhauser Verlag, Basel, Switzerland, Switzerland, 1992. ISBN 3-7643-2733-2.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with restarts. "CoRR", "abs/1608.03983", 2016.
Arkadi S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efficiency in Optimization. John Wiley, 1983.
9

Under review as a conference paper at ICLR 2019
Yurii Nesterov. How to make the gradients small, 2012. Yurii E. Nesterov. A method for unconstrained convex minimization problem with the rate of con-
vergence O(1/k2). Doklady AN SSSR, 269, 1983. Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.
SIAM Journal on Control and Optimization, volume 30, 1992. Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics in
convex programming. IEEE Transactions on Information Theory, 2011. Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In ICML, 2012. Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, vol. 22, 1951. David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Tech. Report,
ORIE, Cornell University, 1988. Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of residual networks
using large learning rates. arXiv preprint arXiv:1708.07120, 2017. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp. 1139­1147, 2013. Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012. Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
10

Under review as a conference paper at ICLR 2019

A PROOFS OF RESULTS IN SECTION 4.1

 

 Proof of Theorem 1. The problem instance is simple. Let H = 



... 

1

, where the first 



...

d 2

diagonal

entries

are

equal

to



and

the

remaining

d 2

diagonal

entries

are

equal

to

1

and

all

the

off

diagonal entries are equal to zero. Let us denote by vt(i) d=ef E wt(i) - (w)(i) 2 the variance in

the ith direction at time step t. Let the initialization be such that v0(i) = 2/ for i = 1, 2, ..., d/2 and v0(i) = 2 for i = d/2 + 1, ..., d. This means that the variances for all directions with eigenvalue  remain equal as t progresses and similarly for all directions with eigenvalue 1. We have

vT(1) d=ef E

wT(1) - (w)(1) 2

TT
= (1 - j )2 v0(1) + 2

j2

T

(1 - i)2 and

j=1

j=1 i=j+1

vT(d) d=ef E

wT(d) - (w)(d) 2

TT
= (1 - j )2 v0(d) + 2 j2

T

(1 - i)2 .

j=1

j=1 i=j+1

We consider a recursion for vt(i) with eigenvalue i (1 or ). By the design of the algorithm, we know
vt(+i)1 = (1 - ti)2vt(i) + i2t2.

Let

s(, )

=

 2  2 1-(1-)2

be

the

solution

to

the

stationary

point

equation

x

=

(1 - )2

+ 22.

Intuitively if we keep using the same learning rate , then vt(i) is going to converge to s(, i). Also note that s(, )  2/2 when  1.

We first prove the following claim showing that eventually the variance in direction i is going to be at least s(T , i).

Claim 1. Suppose s(t, i)  v0(i), then vt(i)  s(t, i).

Proof. We can rewrite the recursion as
vt(+i)1 - s(t, i) = (1 - ti)2(vt(i) - s(t, i)).
In this form, it is easy to see that the iteration is a contraction towards s(t, i). Further, vt(+i)1 - s(t, i) and vt(i) - s(t, i) have the same sign. In particular, let t0 be the first time such that s(t, i)  v0(i) (note that t is monotone and so is s(t, i)), it is easy to see that vt(i)  v0(i) when t  t0. Therefore we know vt(0i)  s(t0 , i), by the recursion this implies vt(0i)+1  s(t0 , i)  s(t0+1, i). The claim then follows from a simple induction.

If s(T , i)  v0(i) for i = 1 or i = d then the error is at least 2d/2  2d/T and we are done. Therefore we must have s(T , )  v0(1) = 2/, and by Claim 1 we know vT(1)  s(T , )  2T /2. The function value is at least

E [f (wT )]



d 2

·

vT(1)



d2T 4

.

To make sure E [f (wT )]



d 2 T 32

we must have T



1 8T

.

Next we will show that when this

happens, vT(d) must be large so the function value is still large.

We will consider two cases, in the first case, b  T .

Since

1 8T

 T

=

a b+T 



a 2b

,

we

have

a b



1 4T

.

Therefore vT(d)



(1 -

a b

)2T

v0(d)



2/2, so the function value is at least E [f (wt)]



d 2

vT(d)



d2 4



d2 T

,

and

we

are

done.

11

Under review as a conference paper at ICLR 2019

In the second case, b < T .

Since

1 8T

 T

=

a b+T 



a 2T 

,

we

have

a



0.25T

-1.

The sum of

learning rates satisfy

T

i 

T

a i 

T

0.25i-1  0.25 log T.

i=1 i=1

i=1

Here the second inequality uses the fact that T -1i-  i-1 when i  T . Similarly, we also know



T i=1

i2



< 1/4, we

T i=1

0.25i-2



2/24.

Using

the

get vT(d)  exp(-2

T i=1

i

-

4

apTi=pr1oxi2im)va0(tdi)on(1-2/5)2T,

exp(-2 - 42) for so the function value

is at least E [f (wt)] 

d 2

vT(d)



d2 20 T



d2 32T

.

This concludes the second case and proves the

theorem.

B PROOFS OF RESULTS IN SECTION 4.2

Proof of Theorem 2. The learning rate scheme is as follows. Divide the total time horizon into

log() equal sized phases. In the

th

phase,

the

learning

rate

to

be

used

is

log 2

T ·log ·µ·T



.

Note

that

the

learning rate in the first phase depends on strong convexity and that in the last phase depends on

smoothness (since the last phase has = log ). Recall the variance in the kth coordinate can be

upper bounded by

vT(k) d=ef E

wT(k) - (w)(1) 2

T


1 - j (k) 2 v0(1) + (k)2 T j2 T

2
1 - i(k)

j=1

j=1 i=j+1


T




TT

 exp -2 j (k) v0(1) + (k)2 j2 exp -2

i(k) .

j=1

j=1

i=j+1

We will show that for every k, we have

vT(k)



log  · log T (k)T

· 2,

which directly implies the theorem. Now choose any k. Let  denote the number satisfying 2  ·µ  (k) < 2 +1 · µ. Note that  depends on k but we suppressed the dependence for notational simplicity.


T




TT

vT(k)  exp -2 j (k) v0(k) + (k)2 j2 exp -2

i(k)

j=1

j=1

i=j+1

 exp

-2 log

T log T



·

(k)

·

T log



·

2

1 µ

v0(k)

+

(k)2

·

T log 

·

 -1
2

=1

+ (k)2



2


·

1

-

exp

1 -  (k)

log  T +·
log 
= +1

log  log T 2 µT

2



v0(k) T3

+ 2 ·

log2  log2 T µT 3

+

2 + 2 ·

log  log2 T T

log 

1 2µ

= +1



v0(k) T3

+

2

·

log  log2 T

T

(k)

+

2

1 µ



v0(k) T3

+

2 log  · log2 T (k)T

· 2.

This finishes the proof.

12

Under review as a conference paper at ICLR 2019

Proof of Theorem 3. The learning rate scheme we consider is t = 0 · ct-1 with 0 = log T /(µTe), Te = T / log  and c = (1 - 1/Te). Further, just as in previous lemmas, we consider a specific eigen direction (k) and write out the progress made along this direction by iteration say, T^  T :

T^

T^  T^



err(T^k) = (1 - t(k))2err0(k) + ((k))22

2 · 

(1 - t(k))2

t=1

 =1

t= +1

  exp -2(k)0

T^



ct-1err0(k)

+

((k))2202 c2

T^

 c2 exp -2(k)0

T^

 ct-1

t=1

 =1

t= +1

= exp = exp

- 2(k)0 1-c

·

(1

-

cT^ )

err0(k)

+

((k))2202 c2

T^
c2 exp

- 2(k)0 1-c

·

(c

-

cT^ )

 =1

2(k)0 · cT^ 1-c

 · exp

- 2(k)0 1-c

err(0k)

+

((k))2202 c2

T^

c2 exp

 =1

- 2(k)0 · (c ) 1-c

 

Represent c = x and 2(k)0/(1 - c) = . Substituting necessary values of the quantities, we have  = 2 log T · (k)/µ  2. Now, the second term is upper bounded using the corresponding
integral, which is the following:

cT^
x2 exp (-x) 

cT^ x2 exp (-x)dx  1 ·

x=c

1



22 1 +  + 2

exp (-).

Substituting this in the previous bound, we have:

  2

err(T^k)  exp -(1 - cT^)

· err(0k)

+

((k))2202 c2

·

2 1+




 exp

-(1 - cT^)

·

err0(k)

+

16

((k))2202 

 exp

-(1 - cT^)

·

err(0k)

+

16

((k))22 log T µ2Te2 · 2((k)/µ)

= exp

-(1 - cT^)

·

err0(k)

+

(k)2 log 8 µTe2

T

Now, setting T^ = Te log(C(k)/µ), and using 1 - a  exp (-a), with C > 1 being some (large) universal constant, we have:

err(T^k)  exp

-2((k)/µ)

log

T

·

(1

-

µ C (k)

)

·

err(0k)

+

(k)2 log 8 µTe2

T



1 T 2-(1/C)

·

err(0k)

+

(k)2 log 8 µTe2

T

(4)

Now, in order to argue the progress of the algorithm from T^ + 1 to T , we can literally view the

algorithm as starting with the iterate obtained from running for the first T^ steps (thus satisfying the

excess risk guarantee in equation 4) and then adding in variance by running for the duration of time

between T^ + 1 to T . For this part, we basically upper bound this behavior by first assuming that

there is no contraction of the bias and then consider the variance introduced by running the algorithm

from T^ + 1 to T . This can be written as:

T

T -T^

T -T^

errT(k) 

(1 - t(k))2errT(^k) + ((k))22

2+T^

(1 - t+T^(k))2

t=T^+1

 =1

t= +1

T -T^

T -T^

 err(T^k) + ((k))22

2+T^

(1 - t+T^(k))2

 =1

t= +1

13

Under review as a conference paper at ICLR 2019

Now, to consider bounding the variance of the process with decreasing sequence of learning rates, we will instead work with a constant learning rate and understand its variance:

T -T^

((k))22

2(1 - (k))2(T -T^-) 

22((k))2

(2 - (k))(k)

 =1

 2(k).

What this implies in particular is that the variance is a monotonic function of the learning rate and thus the overall variance can be bounded using the variance of the process run with a learning rate of T^.

T -T^

T -T^

T -T^

((k))22

2+T^

(1 - t+T^(k))2  ((k))22

T2^(1 - T^(k))2  T^2(k)

 =1

t= +1

t= +1

 log T · µ · 2(k) µTe C(k)
 2 log T log  T

Plugging this into equation 4 and summing over all directions, we have the desired result.

Proof of Theorem 4. The learning rate scheme is as follows.

We first break T into three equal sized parts. Let A = T /3 and B = 2T /3. In the first T /3 steps,

we use a constant learning rate of 1/L. In the second T /3 steps, we use a polynomial decay learning

rate

A+t

=

1 µ(+t/2)

.

In

the third

T /3

steps,

we break the

steps into log2() equal sized

phases.

In the

th

phase,

the

learning

rate

to

be

used

is

5 log2  2 ·µ·T

.

Note

that

the

learning

rate

in

the

first

phase

depends on strong convexity and that in the last phase depends on smoothness (since the last phase

has = log ).

Recall the variance in the kth coordinate can be upper bounded by

vT(k) d=ef E

wT(k) - (w)(1) 2

T


1 - j (k) 2 v0(1) + (k)2 T j2 T

2
1 - i(k)

j=1

j=1 i=j+1


T




TT

 exp -2 j (k) v0(1) + (k)2 j2 exp -2

i(k) .

j=1

j=1

i=j+1

We will show that for every k, we have

vT(k)



v0(k) T3

+

50 log2  (k)T

· 2.,

which directly implies the theorem.

(5)

We will consider the first T /3 steps. The guarantee that we will prove for these iterations is: for any

t



A,

vt(k)



(1

-

(k)/L)2tv0(k)

+

2 L

.

This can be proved easily by induction. Clearly this is true when t = 0. Suppose it is true for t - 1, let's consider step t. By recursion of vt(k) we know

vt(k) = (1 - (k)/L)2vt(-k)1 + (k)2/L2



(1

-

(k)/L)2tv0(k)

+

2 L

(1 - (k)/L)2 + (k)/L



(1

-

(k)/L)2tv0(k)

+

2 .
L

Here the second step uses induction hypothesis and the third step uses the fact that (1 - x)2 + x  1

when x  [0, 1]. In particular, since (1-(k)/L)2T/3  (1-1/)2T/3  (1-1/)3 log T = 1/T 3,

we

know

at

the

end

of

the

first

phase,

vA(k)



v0(k)/T 3

+

2 L

.

14

Under review as a conference paper at ICLR 2019

In the second T /3 steps, the guarantee would be: for any t  T /3, vA(k+) t  v0(k)/T 3 + 2A+t2.
We will again prove this by induction. The base case (t = 0) follows immediately from the guarantee for the first part. Suppose this is true for A + t - 1, let us consider A + t, again by recursion we know

vA(k+) t = (1 - (k)A+t-1)2vA(k+) t-1 + (k)2A2 +t-1

 v0(k)/T 3 + 2A+t-12

(1

-

(k)A+t-1)2

+

1 2

(k)

A+t-1



v0(k)/T 3

+

2A+t-12(1

-

1 2 µA+t-1)



v0(k)/T 3

+

2A+t2.

Here

the

last

line

uses

the

fact

that

2A+t-1(1

-

1 2

µA+t-1)



2A+t2,

which

is

easy

to

verify

by

our

choice

of

.

Therefore,

at

the

end

of

the

second

part,

we

have

vB(k)



v0(k)/T

3

+

22 µ(+T

/6)

.

Finally

we

will

analyze

the

third

part.

Let

T^

=

T /3 log2

,

we

will

consider

the

variance

v(k)
B+

T^

at

the end of each phase. We will make the following claim by induction:

Claim 2. Suppose 2 · µ  (k), then

v(k)
B+

T^

 vB(k) exp(-3

) + 2T^2(k)2.

Proof. We will prove this by induction. When = 0, clearly we have vB(k)  vB(k) so the claim is true. Suppose the claim is true for - 1, we will consider what happens after the algorithm uses  for T^ steps. By the recursion of the variance we have

v(Tk^)  v((k-) 1)T^ · exp(-2 · (k)T^) + T^2(k)2.

Since 2 · µ  (k), we know exp(-2 · (k)T^)  exp(-3). Therefore by induction hypothesis we have

v(k)
B+

T^



vB(k) exp(-3

)

+

exp(-3)

·

2T^

2 -1

(k)

+ T^2(k)



vB(k) exp(-3

)

+ 2T^2(k).

This finishes the induction.

By Claim 2, Let  denote the number satisfying 2  · µ  (k) < 2 +1 · µ, by this choice we know

µ/(k)



1 2

exp(-3

) we have

vT(k)



v(k)
B+

 T^



vB(k) exp(-3



)

+

2T^

2


(k)



2



v0(k) T3

+

242 (k)T

+

50 log2  3(k)T

· 2.



v0(k) T3

+

50 log2  (k)T

·

2.

Therefore, the function value is bounded by E [f (wT )] = 2d.

d i=1

(k)vT(k)



f (w0) T3

+

50 log2  T

·

C PROOFS OF RESULTS IN SECTION 5
All of our counter-examples in this section are going to be the same simple function. Let H be a diagonal matrix with d/2 eigenvalues equal to  and the other d/2 eigenvalues equal to 1. Intuitively, we will show that in order to have a small error in the first eigendirection (with eigenvalue ), one need to set a small learning rate t which would be too small to achieve a small error in the second

15

Under review as a conference paper at ICLR 2019

eigendirection (with eigenvalue 1). As a useful tool, we will decompose the variance in the two directions corresponding to  eigenvalue and 1 eigenvalue respectively as follows:

vT(1) d=ef E

wT(1) - (w)(1) 2

TT
= (1 - j )2 v0(1) + 2

j2

T

(1 - i)2

j=1

j=1 i=j+1





T TT

 exp -2 j v0(1) + 2 j2 exp -2

i and

j=1

j=1

i=j+1

(6)

vT(2) d=ef E

wT(2) - (w)(2) 2

TT
= (1 - j )2 v0(2) + 2

j2

T

(1 - i)2

j=1

j=1 i=j+1


T


TT

 exp -2 j v0(2) + 2 j2 exp -2

i .

j=1

j=1

i=j+1

(7)

Proof of Theorem 5. Fix  = /C log( + 1) where C is a universal constant that we choose later. We need to exhibit that the lim sup is larger than  . For simplicity we will also round  up to the nearest integer.

Let

T

be

a

given

number.

Our

goal

is

to

exhibit

a

T~

>

T

such

that

flin(wT~ )
(2/T~)



.

Given

the

step

size

sequence t, consider the sequence of numbers T0 = T, T1, · · · , T such that Ti is the first number

that

1 Ti

3

 

t



. 

t=Ti-1 +1

Note that such a number always exists because all the step sizes are at most 2/. We will also let

i be Ti - Ti-1. Firstly, from (6) and (7), we see that t t = . Otherwise, the bias will never

decay

to zero.

If f (wTi-1+i )

>

 2d Ti-1 +i

for

some i

=

1, · · ·

, ,

we are

done.

If not,

we obtain

the following relations:

2 1

1

 2

T20+t

t=1



exp(3) 

·E

 T0  (exp(3) - 1) 1.

wT(10)+1 - (w)(1) 2



exp(3)flin(wT0+1 )



exp(3) 2 T0 + 1

Here the second inequality is based on (6). We will use C1 to denote exp(3). Similarly, we have

2 2



2
2 T21+t
t=1



C1 E

wT(11)+2 - (w)(1) 2

 T1  (C1 - 1) 2



T0



(C1 - C1

1)2 2.

Repeating this argument, we can show that



C1flin(wT1+2 )



C1 2 T1 + 2

T

=

T0



(C1 - 1)i (C1 )i-1

i

and

Ti



(C1 - 1)j-i (C1 )j-i-1

j

We will use i = 1 in particular, which specializes to

 i < j.

T1



(C1 - 1)j-1 (C1 )j-2

j

 j  2.

Using the above inequality, we can lower bound the sum of j as


j
j=2


 T1 ·
j=2

(C1 )j-2 (C1 - 1)j-1

 T1 ·

1 C1


·
j=2

1 1+
C1

j-2



T1

·

1 C1

· exp (/(C1 )) .

(8)

16

Under review as a conference paper at ICLR 2019

This means that

E [f (wTi )]



d 2

·

E

wT(2i) - (w)(2) 2

1
 exp(-6)2d · T2 +i

i=1

 exp(-6)2d  exp(-6)2d  exp (/(C1 ) - 3) ·

1 T1

C1

where we used (8) in the last step. Rearranging, we obtain

E [f (wT )] (2d/T)



exp (/(C1 ) C1

-

3) .

2d

 j=2

j

,

If

we

choose

a

large

enough

C

(e.g.,

3C1),

the

right

hand

side

is

at

least

exp((C/C1) log(+1)-3) 



.

To prove Theorem 6, we rely on the following key lemma, which says if a query point wT is bad (in the sense that it has expected value more than 10 2d/T ), then it takes at least (T / ) steps to bring the error back down.

Lemma 8.

There exists universal constants C1, C2

>

0 such that for any 



 CC1 log(+1)

where C

is the constant in Theorem 5, suppose at step T , the query point wT satisfies f (wT )  C1 2d/T ,

then

for

all

T~



[T, (1 +

1 C2



)T

]

we

have

E [f (wT~)]



 2d/T



 2d/T~.

Proof of Lemma 8. Since f (wT )  C1 2d/T and f (wT )

d 2



wT(1) - (w)(1)

2
+

wT(2) - (w)(2) 2

,

we

know

either

wT(1) - (w)(1) 2

= 

C1 2/2T or wT(2) - (w)(2) 2  C1 2/2T . Either way, we have a coordinate i with

eigenvalue i ( or 1) such that

wT(i) - (w)(i)

2
 C1 2/(2T i).

Similar as before, choose  to be the first point such that

T +1 + T +2 + · · · + T +  [1/i, 3/i].

First, by (6) or (7), we know for any T  T~  T + , E wT(~i) - (w)(i) 2 
exp(-6)C1 2/(2iT ) just by the first term. When we choose C1 to be large enough the contribution to function value by this direction alone is larger than  2/T . Therefore every query in [T, T + ] is still bad.

We will consider two cases based on the value of S2 :=

T + T~=T +1

T2~ .

If S2  C2 /(i2T ) (where C2 is a large enough universal constant chosen later), then by CauchySchwartz we know

T +

S2 ·   (

T~)2  1/i2.

T~=T +1

Therefore   T /C2 , and we are done.

If S2 > C2 /(2i T ), by Equation (6) and (7) we know



E

wT(i+)  - (w)(i) 2

T +

T +

 2

T2~ exp -2i

j 

T~=T +1

j=T~+1

T +

 exp(-6)2

T2~  exp(-6) · C2 2/(2i T ).

T~=T +1

17

Under review as a conference paper at ICLR 2019

Here the first inequality just uses the second term in Equation (6) or (7), the second inequality is

because

T + j=T~+1

j



T + j=T +1

j



3/i

and

the

last

inequality

is

just

based

on

the

value

of

S2.

In this case as we can see as long as C2 is large enough, T +  is also a point with E [f (wT +)] 

iE wT(i+)  - (w)(i) 2  C1 2/(T + ), so we can repeat the argument there. Eventually

we either stop because we hit case 1: S2  C2 /i2T or the case 2 S2 > C2 /i2T happened more than T /C2 times. In either case we know for any T~  [T, (1 + 1/C2)T ] E [f (wT~)]   2/T   2/T~ as the lemma claimed.

Theorem 6 is an immediate corollary of Theorem 5 and Lemma 8.

Proof of Theorem 7. This result follows by running the constant and cut scheme for a fixed time horizon T and then increasing the time horizon to  · T . The learning rate of the initial phase for the new T =  · T is 1/µT = 1/µ · T = 1/LT which is the final learning rate for time horizon T .
Theorem 2 will then directly imply the current theorem.

D DETAILS OF EXPERIMENTAL SETUP
D.1 SYNTHETIC 2-D QUADRATIC EXPERIMENTS
As mentioned in the main paper, we consider three condition numbers namely   {50, 100, 200}. We run all experiments for a total of 200 iterations. The two eigenvalues of the Hessian are  and 1 respectively, and noise level is 2 = 1 and we average our results with two random seeds. All our grid search results are conducted on a 10 × 10 grid of learning rates × decay factor and whenever a best run lands at the edge of the grid, the grid is extended so that we have the best run in the interior of the gridsearch.
For the O(1/t) learning rate, we search for decay parameter over 10-points logarithmically spaced between {1/(100), 3000/}. The starting learning rate is searched over 10 points logarithmically spaced between {1/(20), 1000/}.
For the O(1/ (t)) learning rate, the decay parameter is searched over 10 logarithmically spaced points between {100/, 200000.0/}. The starting learning rate is searched between {0.01, 2}.
For the exponential learning rate schemes, the decay parameter is searched between {exp (-2/N ), exp (-106/N )}. The learning rate is searched between {1/5000, 1/10}.
D.2 NON-CONVEX EXPERIMENTS ON CIFAR-10 DATASET WITH A 44-LAYER RESIDUAL NET
As mentioned in the main paper, for all the experiments, we use the Nesterov's Accelerated gradient method (Nesterov, 1983) implemented in pytorch 12 with a momentum set to 0.9 and batchsize set to 128, total number of training epochs set to 100, 2 regularization set to 0.0005.
With regards to learning rates, we consider 10-values geometrically spaced as {1, 0.6, · · · , 0.01}. To set the decay factor for any of the schemes such as 1,2, and 3, we use the following rule. Suppose we have a desired learning rate that we wish to use towards the end of the optimization (say, something that is 100 times lower than the starting learning rate, which is a reasonable estimate of what is typically employed in practice), this can be used to obtain a decay factor for the corresponding decay scheme. In our case, we found it advantageous to use an additively spaced grid for the learning rate t, i.e., one which is searched over a range {0.0001, 0.0002, · · · , 0.0009, 0.001, · · · , 0.009} at the 80th epoch, and cap off the minimum possible learning rate to be used to be 0.0001 to ensure that there is progress made by the optimization routine. For any of the experiments that yield the best performing gridsearch parameter that falls at the edge of the grid, we extend the grid to ensure that the finally chosen hyperparameter lies in the interior of the grid. All our gridsearches are run such that we separate a tenth of the training dataset as a validation set and train on the remaining 9/10th dataset. Once the best grid search parameter is chosen, we train on the entire training dataset and
12https://github.com/pytorch

18

Under review as a conference paper at ICLR 2019 evaluate on the test dataset and present the result of the final model (instead of choosing the best possible model found during the course of optimization).
19

