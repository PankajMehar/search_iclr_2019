Under review as a conference paper at ICLR 2019
SCALABLE UNBALANCED OPTIMAL TRANSPORT USING GENERATIVE ADVERSARIAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures. In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generativeadversarial framework. We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner, and propose a new algorithm based on stochastic alternating gradient updates, similar in practice to GANs. We also provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018), and perform numerical experiments demonstrating how this methodology could be applied to population modeling.
1 INTRODUCTION
We consider the problem of unbalanced optimal transport: given two measures, find a cost-optimal way to transform one measure to the other using a combination of mass variation and transport. Such problems arise, for example, when modeling the transformation of a source population into a target population (Figure 1a). In this setting, one needs to model mass transport to account for the features that are evolving, as well as local mass variations to allow sub-populations to become more or less prominent in the target population (Schiebinger et al., 2017).
(a) (b) Figure 1: (a) Illustration of the problem of modeling the transformation of a source population µ to a target population . In this example, one sub-population is growing more rapidly than the others. (b) Schematic of our formulation of unbalanced optimal transport. The objective is to learn a (probabilistic) transport map T (for transporting mass) and scaling factor  (for mass variation) to push the source µ to the target .
1

Under review as a conference paper at ICLR 2019
Classical optimal transport (OT) considers the problem of pushing a source to a target distribution in a way that is optimal with respect to some transport cost without allowing for mass variations. Modern approaches are based on the Kantorovich formulation (Kantorovich, 1942), which seeks the optimal probabilistic coupling between measures and can be solved using linear programming methods for discrete measures. Cuturi (2013) showed that regularizing the objective using an entropy term allows the dual problem to be solved more efficiently using the Sinkhorn algorithm. In the continuous setting, several stochastic methods based on this dual objective have been proposed (Genevay et al., 2016; Seguy et al., 2017). Optimal transport maps have seen applications to many areas, such as computer graphics (Ferradans et al., 2014; Solomon et al., 2015) and domain adaptation (Courty et al., 2014; 2017). In many applications where a transport cost is not available, transport maps can also be learned using generative models such as generative adversarial networks (GANs) (Goodfellow et al., 2014), which push a source distribution to a target distribution by training against an adversary. Numerous transport problems in image translation (Mirza & Osindero, 2014; Zhu et al., 2017; Yi et al., 2017), natural language translation (He et al., 2016), domain adaptation (Bousmalis et al., 2017) and biological data integration (Amodio & Krishnaswamy, 2018) have been tackled using variants of GANs, with strategies such as conditioning or cycle-consistency employed to enforce correspondence between original and transported samples. However, all these methods conserve mass between the source and target and therefore cannot handle mass variation. This limits applications to large-scale problems in population modeling, particularly in computational biology (Schiebinger et al., 2017).
Several formulations have been proposed for extending the theory of OT to the setting where the measures can have unbalanced masses (Chizat et al., 2015; 2018; Kondratyev et al., 2016; Liero et al., 2018; Frogner et al., 2015). In terms of numerical methods, a class of scaling algorithms (Chizat et al., 2016) that generalize the Sinkhorn algorithm for balanced OT have been developed for approximating the solution to optimal entropy-transport problems; this formulation of unbalanced OT by Liero et al. (2018) corresponds to the Kantorovich OT problem in which the hard marginal constraints are relaxed using divergences to allow for mass variation. In practice, these algorithms have been used to approximate unbalanced transport plans between discrete measures for applications such as computer graphics (Chizat et al., 2016), tumor growth modeling (Chizat & Di Marino, 2017) and computational biology (Schiebinger et al., 2017). However, optimal entropy-transport allows mass variation without explicitly modeling it, and there are currently no methods that can perform large-scale unbalanced transport between high-dimensional continuous measures.
Contributions. Inspired by the recent successes of GANs for high-dimensional transport problems, we present a novel framework for unbalanced optimal transport that directly models mass variation in addition to transport. Concretely, our contributions are the following:
· We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a measure µ to a measure  in a cost-optimal manner (Figure 1b) and show how our formulation relates to the formulation by Liero et al. (2018), which has desirable theoretical properties.
· We propose scalable methodology for simultaneously learning the transport map and scaling factor. Our derivation is based on a variational inequality for divergences, which results in a stochastic alternating gradient descent method similar in practice to GANs (Goodfellow et al., 2014)
· We demonstrate in practice how our methodology can be applied towards population modeling using the MNIST and USPS handwritten digits datasets and the CelebA dataset.
2 PRELIMINARIES AND RELATED WORK
Notation. Let X , Y  Rn be topological spaces and let B denote the Borel -algebra. Let M+1 (X ), M+(X ) denote respectively the space of probability measures and finite non-negative measures over X . For a joint measure   M+(X × Y), we let #X  and #Y  denote its marginals with respect to X and Y respectively.
2

Under review as a conference paper at ICLR 2019

Optimal transport (OT) addresses the problem of transporting between measures in a cost-optimal manner.
The original Monge formulation of OT searches over deterministic transport maps (Monge, 1781), while
modern approaches are based on the Kantorovich relaxation (Kantorovich, 1942), which searches over probabilistic transport plans. Specifically, given µ  M+1 (X ),   M+1 (Y) and a cost function c : X × Y  R+, Kantorovich OT seeks a joint measure   M+1 (X × Y) subject to #X  = µ and #Y  =  minimizing

W (µ, ) := inf c(x, y) d(x, y),


(1)

For discrete measures µ, , (1) can be solved as a linear program. Cuturi (2013) showed that introducing entropic regularization results in a simpler dual optimization problem that can be solved using the Sinkhorn algorithm. More recently, Genevay et al. (2016) and Seguy et al. (2017) proposed stochastic algorithms based on entropic regularization that extend to continuous measures. Arjovsky et al. (2017) proposed a stochastic algorithm based on a simpler dual formulation for computing the 1-Wasserstein distance.

Unbalanced OT. Several formulations that extend classical OT to handle mass variation have been proposed
(Chizat et al., 2015; 2018; Kondratyev et al., 2016), the one most related to our work being optimal-entropy
transport (Liero et al., 2018). This formulation is obtained by relaxing the marginal constraints of (1) using divergences as follows: Given two positive measures µ  M+(X ) and   M+(Y) and a cost function c : X × Y  R+, optimal entropy-transport finds a measure   M+(X × Y) that minimizes

Wub(µ, ) := inf


c(x, y) d(x, y) + D1 (#X |µ) + D2 (#Y |),

(2)

where D1 and D2 are -divergences induced by 1, 2. The -divergence between non-negative finite measures P, Q over T  Rd induced by a lower semi-continuous, convex entropy functions  : R  R  {}

is defined as

D(P |Q) := P (T ) + 
T

dP dQ

dQ

(3)

where



:=

limr

(r) r

and

dP dQ

Q

+

P



is

the

Lebesgue

decomposition

of

P

with

respect

to

Q.

Note

that

mass variation is allowed since the marginals of  are not constrained to be µ and . In terms of numerical

methods, while practical algorithms for the continuous setting are completely missing, the state-of-the-art in

the discrete setting is a class of iterative scaling algorithms (Chizat et al., 2016) that generalizes the Sinkhorn

algorithm for computing regularized OT plans (Cuturi, 2013). The key idea is to introduce an entropic

regularization term into the objective so that the dual problem can be solved using Dykstra's algorithm (Chizat

et al., 2016). In practice, this algorithm can be used to approximate unbalanced OT plans between discrete

measures or low-dimensional continuous measures that can be discretized.

3 PROPOSED FORMULATION AND METHOD
We propose an alternative formulation of unbalanced OT, in which the objective is to learn a (probabilistic) transport map T and scaling factor  to push µ to  in a cost-optimal manner (Figure 1b). Let c1 : X ×Y  R+ denote the transport cost and c2 : R+  R+ denote the cost of scaling mass. Let (Z, B(Z), ) be a probability space1. Formally, we seek measurable functions T : X × Z  Y and  : X  R+ minimizing

L(µ, ) := inf
T ,

c1(x, T (x, z))d(z)(x)dµ(x) + c2((x))dµ(x),

1If the probability space only contains one event, then the problem simplifies to a deterministic map T

3

(4)

Under review as a conference paper at ICLR 2019

Figure 2: Illustration of unbalanced OT on synthetic dataset.

subject to

(B) = T,(B) := (Tx-1(B))dµ, B  B(Y ),

(5)

where Tx(z) : z  T (x, z). Concretely, the first and second terms of (4) penalize the cost of mass transport and variation respectively, and the constraint (5) ensures that (T, ) pushes µ to  exactly.
Intuitively, (4) describes a spectrum of unbalanced transport problems that interpolate between pure transport problems and pure mass variation problems. As an example, consider the unbalanced transport scenarios illustrated in Figure 2. The source and target measures are illustrated in Columns 1 and 2 respectively. When the cost of mass transport, c1, is low compared to the cost of mass variation, c2, the solution of (4) corresponds to a pure transport scheme, i.e. the transport map T translates the source distribution upward to the target distribution, and the scaling factor  is fixed to 1 and has no effect (Column 3). As the cost of mass transport is increased relative to the cost of mass variation, it becomes more costly to transport mass and less costly to change the amount of mass. Consequently, the solution of (4) tends more and more towards transporting only a small region of mass from the source to the target and growing this mass ( > 1), while mass that is farther from the target is allowed to shrink ( < 1) (Columns 4-6).
Relaxation. The form of (4) suggests that stochastic gradient methods could be used for optimization by sampling from µ × , but it would be challenging to satisfy the constraint (5) during the updates. In what follows, we consider a relaxation of (4) using a divergence penalty D in place of the hard constraint (5):

L(µ, ) := inf
T ,

c1(x, T (x, z))d(z)(x)dµ(x) + c2((x))dµ(x) + D(T,|).

(6)

The divergence can be rewritten using the following variational inequality: Lemma 3.1. For non-negative finite measures P, Q over T  Rd, it holds that

D(P |Q)  sup f dP - (f )dQ
f F

(7)

where F is a subset of measurable functions {f : T  (-, ]}. Equality holds if and only if f  F such

that

(i)

the

restriction

of

f

to

the

support

of

Q

belongs

to

the

subdifferential

of

(

dP dQ

),

i.e.

the

Radon-Nikodym

derivative of P with respect to Q, and (ii) f =  over the support of P .

Substituting (7) into (6), we obtain

L(µ, ) = inf sup
T, f F

c1(x, T (x, z))d(z)(x)dµ(x) + c2((x))dµ(x)

+ f (T (x, z))d(z)(x)dµ(x) - (f (y))d(y)

(8)

4

Under review as a conference paper at ICLR 2019

where F is a function class satisfying the equality conditions of Lemma 3.1. The objective in (8) can now be optimized using alternating gradient updates after parameterizing T , , and f with neural networks, similar in practice to GANs as shown in Algorithm 1. We assume that one has access to samples from µ, , and in the setting where µ,  are not normalized, then samples to the normalized measures µ~, ~ as well as the normalization constants cµ, c. These are reasonable assumptions for practical applications: for example, in a biological assay, one might collect cµ cells from time point 1 and c cells from time point 2. In this case, the samples are the measurements of each cell and the normalization constants are cµ, c.

Algorithm 1 Generative adversarial method for unbalanced OT
Input: Initial parameters , , ; step size ; normalized measures µ~, ~, constants cµ, c Output: Updated parameters , ,  while (, , ) not converged do
Sample x1, · · · , xn from µ~, y1, · · · , yn from ~, z1, · · · , zn from 

1n

(, , ) := n

[cµc1(xi, T(xi, zi))(xi) + cµc2((xi))

i=1

+ cµ(xi)f(T(xi, zi)) - c (f(yi))]

Update  by gradient descent on - (, , ) Update ,  by gradient descent (, , ) end while

(9)

Theoretical properties. While our objective in (6) is partly motivated by the generative-adversarial framework, we now show that it has a sound theoretical basis. For all statements, we assume (Z, B(Z), ) is an atomless probability space. Our first result points to the close relation between (6) and the optimal entropy-transport problem (2) proposed by Liero et al. (2018).
Lemma 3.2. Define

W~ c,1,2 (µ,

)

:=

inf
G

cd + D1 (#X |µ) + D2 (#Y |)

(10)

where G = {  M+(X × Y)|(X × Y) = 0}, and X = X \supp(µ). If c2 is convex and lower semi-continuous, then L(µ, ) = W~ c1,c2,(µ, ).
One implication of this result is that for certain cost functions and divergences, L defines a proper metric between positive measures µ and . For instance, taking c2,  to be entropy functions corresponding to the KL-divergence and c1 = log cos2+(d(x, y)), then L(µ, ) corresponds to the Hellinger-Kantorovich (Liero et al., 2018) or the Wasserstein-Fisher-Rao (Chizat et al., 2018) metric between positive measures µ and .
Next, we give sufficient conditions under which minimizers of (6) exist and are unique. For a given µ, , define the product measure generated by any (T, ) by

(C) :=

1C(x, T (x, z))d(z)(x)dµ(x), C  B(X ) × B(Y).

Proposition 3.3. Suppose L(µ, ) <  and c2 is convex and lower semi-continuous. If (i) c1 has compact sublevel sets in X × Y and c2 +  > 0, or (ii) c2 =  =  then a minimizer of L(µ, ) exists. Moreover, if c2,  are strictly convex,  =  and c1 satisfies Corollary 3.6 (Liero et al., 2018), then the product measure  generated by any minimizer of L(µ, ) is unique.

5

Under review as a conference paper at ICLR 2019
Under these assumptions, we can formalize the notion that for a sufficiently large and appropriate choice of divergence penalty, the solution to (6) approaches the solution of the original problem (4). Then we have the following convergence result:
Theorem 3.4. Suppose c1, c2,  satisfy the existence assumptions of Proposition 3.3. Furthermore, let  be uniquely minimized at (1) = 0. Then for a sequence 0 < 1 < · · · < k < · · · diverging to  indexed by k, limk Lk(µ, ) = L(µ, ). Additionally, let k be the product measure generated by a minimizer of Lk(µ, ). If L(µ, ) < , then up to extraction of a subsequence, k converges weakly to , the product measure generated by a minimizer of L(µ, ).
We emphasize that for this convergence result to hold,  must be uniquely minimized at (1) = 0. In practice, this means that many -divergences that can be used for matching probability measures, including the original GAN objective corresponding to (s) = s log s - (s + 1) log(s + 1) (Nowozin et al., 2016), cannot be used for unbalanced transport. Table 1 in the Appendix provides some examples of common -divergences that satisfy Theorem 3.4, along with their corresponding entropy functions and convex conjugates.
By a similar argument as for Theorem 3.4, it can be shown that the solution to (6) converges to the solution for optimal transport (1) if we increase the cost of mass variation to  along with the divergence penalty. Therefore, balanced OT is an extreme case of our formulation of unbalanced OT.
Relation to other GAN variants. Since the original GAN paper by Goodfellow et al. (2014), numerous variants of GANs have been proposed for learning transport maps between distributions (Zhu et al., 2017; Yi et al., 2017; Amodio & Krishnaswamy, 2018). These methods generally assume that the source and target measures are normalized probability measures and cannot handle mass variation. Algorithm 1 proposes a generalization of this strategy that can be used for unbalanced transport, based on the idea of learning both a transport map and a scaling factor. Our derivation of Algorithm 1 uses the variational inequality for divergences presented in Lemma 3.1, which enables us to minimize many divergences using alternating gradient updates similar to GAN training. A similar inequality with stronger assumptions proposed by Nguyen et al. (2008) was used by Nowozin et al. (2016) to propose a large class of generative models that generalize GANs. An important distinction of our work is that we consider divergence minimization between measures that are not probability measures, which requires additional assumptions on the choice of divergence (i.e. Theorem 3.4).
The problem of learning a scaling factor (or weighting factor) that "balances" measures µ and  also arises in causal inference. Generally, µ is the distribution of covariates from a control population and  is the distribution from a treated population. The goal is to scale the importance of different members from the control population based on how likely they are to be present in the treated population, in order to eliminate selection biases in the inference of treatment effects. Kallus (2018) proposed a generative-adversarial method for learning the scaling factor, but they do not consider transport.
Relation to Unbalanced OT. Lemma 3.2 points to the close relation between our problem formulation (6) and the optimal entropy-transport problem (2) by Liero et al. (2018). In practice, the formulations result in quite different numerical methods. The strategy proposed by Chizat et al. (2016) learns a joint measure whose marginals are approximately µ and  (thus modeling mass variation implicitly), requires entropic regularization, and currently can only be used for discrete measures. On the other hand, our method learns a transport map and scaling factor for pushing µ to , does not require entropic regularization, and uses the generative-adversarial framework, which is scalable and can model high-dimensional measures with intricate geometry. For completeness, we present in Section A of the Appendix a new stochastic method for approximating the solution to the optimal-entropy problem based on the same entropy-regularized dual objective by Chizat et al. (2016), that can handle unbalanced transport between continuous measures. This method generalizes the approach of Seguy et al. (2017) for balanced OT and overcomes the scalability limitations of Chizat et al. (2016), but still does not directly model mass variation. In the following numerical experiments, we show the advantage of being able to model mass variation directly using Algorithm 1.
6

Under review as a conference paper at ICLR 2019

(a) (b)

(c) (d)

Figure 3: Learning weights on MNIST data

4 NUMERICAL EXPERIMENTS
In this section, we illustrate in practice how Algorithm 1 performs unbalanced OT, with applications geared towards population modeling.
MNIST-to-MNIST. We first apply Algorithm 1 to perform unbalanced optimal transport between two modified MNIST datasets. The source dataset consists of regular MNIST digits with the class distribution shown in column 1 of Figure 3a. The target dataset consists of either regular (for the experiment in Figure 3b) or dimmed (for the experiment in Figure 3c) MNIST digits with the class distribution shown in column 2 of Figure 3a. The class imbalance between the source and target datasets imitates a scenerio in which certain classes (digits 0-3) become more popular and others (6-9) become less popular in the target population, while the change in brightness is meant to reflect population drift.
We evaluated Algorithm 1 on the problem of transporting the source distribution to the target distribution, enforcing a high cost of transport (w.r.t. Euclidean distance). In both cases, we found that the scaling factor over each of the digit classes roughly reflects its ratio of imbalance between the source and target distributions (Figure 3b-c). These experiments validate that the scaling factor learned by Algorithm 1 reflects the class imbalances and can be used to model the growth and decline of different classes in a population. Figure 3d is a schematic illustrating the reweighting that occurs during unbalanced transport.
MNIST-to-USPS. Next, we apply unbalanced OT from the MNIST dataset to the USPS datasets. As before, these two datasets are meant to imitate a population sampled at two different time points, this time with a large degree of evolution. We use Algorithm 1 to model the evolution of the MNIST distribution to the USPS distribution, taking as transport cost the Euclidean distance between the original and transported images.
A summary of the transport is visualized in Figure 4a. Each arrow originates from a real MNIST image and points towards the predicted appearance of this image in the USPS dataset. The size of the image reflects the scaling factor of the original MNIST image, i.e. whether it is increasing or decreasing in prominence in the USPS dataset compared to the MNIST dataset according to the unbalanced transport model. Even though the Euclidean distance is not an ideal measure of correspondence between MNIST and USPS digits, many MNIST digits were able to preserve their likeness during the transport (Figure 4b). We analyzed which MNIST digits were considered as increasing or decreasing in prominence by the model. The MNIST digits with higher scaling factors were generally brighter (Figure 4c) and covered a larger area of pixels (Figure 4d) compared to the MNIST digits with lower scaling factors. These results are consistent with the observation that the target USPS digits are generally brighter and contain more pixels.
CelebA-Young-to-CelebA-Aged. We applied Algorithm 1 on the CelebA dataset to performed unbalanced transport from the population of young faces to the population of aged faces. This synthetic problem most

7

Under review as a conference paper at ICLR 2019

(a) (b)

(c)

(d)

Figure 4: Unbalanced Optimal Transport from MNIST to USPS digits

(a) (b) (c)
Figure 5: Unbalanced Optimal Transport from Young to Aged CelebA Faces
closely imitates a real application of interest, which is modeling the transformation of a population based on samples taken from two timepoints. Since the Euclidean distance between two faces is a poor measure of semantic similarity, we first train a variational autoencoder (VAE) (Kingma & Welling, 2013) on the CelebA dataset and encode all samples into the latent space. We then apply Algorithm 1 to perform unbalanced optimal transport from the encoded young to the encoded aged faces, taking the transport cost to be the Euclidean distance in the latent space. The use of VAE latent encodings as a measure of semantic similarity between the decoded images was also considered by Engel et al. (2017).
A summary of the transport is visualized in Figure 5a. Each arrow originates from a real face from the young population and points towards the predicted appearance of this face in the aged population. Generally, the transported faces retain the most salient features of the original faces (Figure 5b), although there are exceptions (e.g. gender swaps) which reflects that some features are not prominent components of the VAE encodings. Interestingly, the young faces with higher scaling factors were significantly enriched for males compared to young faces with lower scaling factors (Figure 5c, top, p = 0). In other words, our model predicts growth in the prominence of male faces compared to female faces as the CelebA population evolves from young to aged. After observing this phenomenon, we confirmed based on checking the ground truth labels that there was indeed a strong gender imbalance between the young and aged populations: while the young population is predominantly female, the aged population is predominantly male (Figure 5c, bottom).
8

Under review as a conference paper at ICLR 2019
REFERENCES
Matthew Amodio and Smita Krishnaswamy. Magan: Aligning biological manifolds. arXiv preprint arXiv:1803.00385, 2018.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214­223, 2017.
Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth and sparse optimal transport. arXiv preprint arXiv:1710.06276, 2017.
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 7, 2017.
Le´na¨ic Chizat and Simone Di Marino. A tumor growth model of Hele-Shaw type as a gradient flow. arXiv preprint arXiv:1712.06124, 2017.
Lenaic Chizat, Gabriel Peyre´, Bernhard Schmitzer, and Franc¸ois-Xavier Vialard. Unbalanced optimal transport: geometry and Kantorovich formulation. arXiv preprint arXiv:1508.05216, 2015.
Lenaic Chizat, Gabriel Peyre´, Bernhard Schmitzer, and Franc¸ois-Xavier Vialard. Scaling algorithms for unbalanced transport problems. arXiv preprint arXiv:1607.05816, 2016.
Lenaic Chizat, Gabriel Peyre´, Bernhard Schmitzer, and Franc¸ois-Xavier Vialard. An interpolating distance between optimal transport and Fisher­Rao metrics. Foundations of Computational Mathematics, 18(1): 1­44, 2018.
Nicolas Courty, Re´mi Flamary, and Devis Tuia. Domain adaptation with regularized optimal transport. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 274­289. Springer, 2014.
Nicolas Courty, Re´mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853­1865, 2017.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems, pp. 2292­2300, 2013.
Jesse Engel, Matthew Hoffman, and Adam Roberts. Latent constraints: Learning to generate conditionally from unconditional generative models. arXiv preprint arXiv:1711.05772, 2017.
Sira Ferradans, Nicolas Papadakis, Gabriel Peyre´, and Jean-Franc¸ois Aujol. Regularized discrete optimal transport. SIAM Journal on Imaging Sciences, 7(3):1853­1882, 2014.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learning with a Wasserstein loss. In Advances in Neural Information Processing Systems, pp. 2053­2061, 2015.
Aude Genevay, Marco Cuturi, Gabriel Peyre´, and Francis Bach. Stochastic optimization for large-scale optimal transport. In Advances in Neural Information Processing Systems, pp. 3440­3448, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672­2680, 2014.
Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. Dual learning for machine translation. In Advances in Neural Information Processing Systems, pp. 820­828, 2016.
9

Nathan Kallus. Deepmatch: Balancing deep covariate representations for causal inference using adversarial training. arXiv preprint arXiv:1802.05664, 2018.
Leonid V Kantorovich. On the translocation of masses. In Dokl. Akad. Nauk. USSR (NS), volume 37, pp. 199­201, 1942.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Stanislav Kondratyev, Le´onard Monsaingeon, Dmitry Vorotnikov, et al. A new optimal transport distance on the space of finite Radon measures. Advances in Differential Equations, 21(11/12):1117­1164, 2016.
Matthias Liero, Alexander Mielke, and Giuseppe Savare´. Optimal entropy-transport problems and a new Hellinger­Kantorovich distance between positive measures. Inventiones Mathematicae, 211(3):969­1117, 2018.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.
Gaspard Monge. Me´moire sur la the´orie des de´blais et des remblais. Histoire de l'Acade´mie Royale des Sciences de Paris, 1781.
XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization. In Advances in Neural Information Processing Systems, pp. 1089­1096, 2008.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Siyan Liu, Stacie Lin, Peter Berube, Lia Lee, et al. Reconstruction of developmental landscapes by optimal-transport analysis of single-cell gene expression sheds light on cellular reprogramming. BioRxiv, pp. 191056, 2017.
Vivien Seguy, Bharath Bhushan Damodaran, Re´mi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel. Large-scale optimal transport and mapping estimation. arXiv preprint arXiv:1711.02283, 2017.
Justin Solomon, Fernando De Goes, Gabriel Peyre´, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric domains. ACM Transactions on Graphics (TOG), 34(4):66, 2015.
Zili Yi, Hao (Richard) Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for image-to-image translation. In ICCV, pp. 2868­2876, 2017.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint, 2017.

Under review as a conference paper at ICLR 2019

APPENDIX

A DUAL STOCHASTIC METHOD

In this section, we present a stochastic method for unbalanced OT based on the regularized dual formulation of (Chizat et al., 2015), which can be considered a natural generalization of Seguy et al. (2017). The dual formulation of (2) is given by

sup - 1(-u)dµ - 2(-v)d
u,v

subject to u  v  c, where the supremum is taken over functions u : X  [-1, ] and v : Y  [-2, ]. This is a constrained optimization problem that is challenging to solve. A standard technique for making the dual problem unconstrained is to add a strongly convex regularization term to the primal objective
(Blondel et al., 2017), such as an entropic regularization term (Cuturi, 2013):

Re() = DKL (|µ  )
where > 0. Concretely, this term has a "smoothing" effect on the transport plan, in the sense that it encourages plans with high entropy. By the Fenchel-Rockafellar theorem, the dual of the regularized problem is,

Wub(µ, ) := sup - 1(-u)dµ - 2(-v)d -
u,v

e(u+v-c)/ d(µ  ),

(11)

where the supremum is relationship between the

taken over functions u primal optimizer  and

:X dual

optim[-ize1r(u, , v]a)nids

v:Y  given by:

[-2, ],

and

the

d = e(u+v-c)/ d(µ  ).

(12)

Next, we rewrite (11) in terms of expectations. We assume that one has access to samples from µ, , and in the setting where µ,  are not normalized, also samples to the normalized measures µ~, ~ as well as the normalization constants. Based on these assumptions, we have
Wub(µ, ) = sup -cµExµ~1(-u(x)) - c Ey~2(-v(y)) - cµc Exµ~,y~e(u(x)+v(y)-c(x,y))/ . (13)
u,v
If 1, 2 are differentiable, we can parameterize u, v with neural networks u, v and optimize ,  using stochastic gradient descent. This is described in Algorithm 2. Note that this algorithm is a generalization of the algorithm of (Seguy et al., 2017) from classical OT to unbalanced OT. Indeed, taking 1, 2 to be equality constraints, (11) becomes

sup u dµ + v d -
u,v

e(u+v-c)/ d(µ  ),

which is the dual of the entropy-regularized classical OT problem.

11

Under review as a conference paper at ICLR 2019

Algorithm 2 SGD for Unbalanced OT
Input: Initial parameters , ; step size ; regularization parameter ; constants cµ, c and normalized measures µ~, ~ Output: Updated parameters ,  while (, ) not converged do
Sample (x1, y1), · · · , (xn, yn) from µ~  ~

1 (, ) :=
n

n
[cµ1(-u(xi)) + c 2(-v(yi)) +

cµc e(u(xi)+v(yi)-c(xi,yi))/ ]

i=1

Update ,  by gradient descent on (, ) end while

The dual solution (u, v) learned from Algorithm 2 can be used to reconstruct the primal solution  based on the relation in (12). Concretely,  is a transport map that indicates the amount of mass transported between every pair of points in X and Y. Note that the marginals of  with respect to X and Y are not necessarily µ and , which is where mass variation is implicitly built into the problem. Given , it is possible to also learn an "averaged" deterministic mapping from X to Y. A standard approach is to take the barycentric projection T : X  Y, defined as,

T

(x)

=

min
zY

Ey(·|x)d(z,

y),

with respect to some distance d : Y × Y  R+. Seguy et al. (2017) proposed a stochastic algorithm for learning such a map from the dual solution, which we reproduce in Algorithm 3.

Algorithm 3 Learning Barycentric Projection
Input: Learned functions u, v; initial T; distance function d Output: Updated T while T not converged do
Sample (x1, y1), · · · , (xn, yn) from µ~  ~

1 () :=
n

n

d(xi, T(yi))e(u(xi)+v(yi)-c(xi,yi))/

i=1

Update T by gradient descent on () end while

B PROOFS FROM SECTION 3

Proof of Lemma 3.1.

D(P |Q) = P(T ) + 
T

dP dQ

dQ

= P(T ) +

sup{ dP - ()}dQ T R dQ

(by defintion of convex conjugate)

12

Under review as a conference paper at ICLR 2019

= P(T ) +

sup { dP - ()}dQ T (-,] dQ

(by Lemma B.1)

=

T

sup {dP
(-, ]

+



dP dQ
dQ

-

()dQ}

= sup f dP - (f )dQ
fF T

The

optimal

f

over

the

support

of

Q

is

obtained

when

dP dQ

belongs

to

the

subdifferential

of

(f ),

or

equivalently

by

duality,

when

f

belongs

to

the

subdifferential

of

(

dP dQ

).

It

is

straightforward

to

see

that

the

optimal f over the support of P is equal to .

Lemma B.1.

If 

> 

:= lims

(s) s

,

then

()

=

.

Proof.

() = sup s - (s)
sR



lim

s( -

(s) )

s

s

=  if  > 

Proof of Lemma 3.2. First we show L(µ, )  W~ c1,c2,(µ, ). If L(µ, ) = , this is trivial, so assume L(µ, ) < . Let (T, ) be any solution and define Tx : z  T (x, z). Let y|x  M+1 (Y) denote the
pushforward measure of  under Tx for all x  X , and define X  M+(X ) as

X (A) := 1A(x)(x)dµ(x)
X
for all A  B(X ), i.e. so  is the Radon-Nikodym derivative of X with respect to µ. Let  be the product measure of y|x and X , defined as

(C) :=

1C (y, x)dy|x(y)dX (x)

for all C  B(X ) × B(Y). Finally, by these definitions, note that #X  = X and

#Y (B) = y|x(B)X (dx) = (Tx-1(B))dX (x) = T,(B)

13

Under review as a conference paper at ICLR 2019

for all B  B(Y). It follows that

( c1(x, Tx(z))d(z))(x)dµ(x) + c2((x))dµ(x) + D(T,|)
XZ

= ( c1(x, y)dy|x(y))dX +
XY
= ( c1(x, y)dy|x(y))dX +
XY

c2((x))dµ(x) + D(T,|)

c2(

dX dµ

)dµ(x)

+

D(#Y |)

= c1d + Dc2 (#X |µ) + D(#Y |)

 W~ c,1,2 (µ, )

Since this inequality holds for any (T, ), taking the infimum over the left side yields L(µ, )  W~ c,1,2 (µ, ). To show L(µ, )  W~ c,1,2 (µ, ), assume W~ c,1,2 (µ, ) <  and let  be a solution. Let X = #X , Y = #Y  denote its marginals. Then by the disintegration theorem, there exist probability measures {y|x}xX in M+1 (Y ) such that

(C) :=

1C (y, x)dy|x(y)dX (x)

for all C  B(X ) × B(Y). Since X , Y are in Rd, this family of measures is X -a.e. unique. Since (Z, B(Z), ) is an atomless probability space, there exists measurable functions {Tx : Z  Y}xX such that
y|x is the pushforward measure of  under Tx. Note that

Y (B) = y|x(B)X (dx) = (Tx-1(B))dX (x) = T,(B)

for all B  B(Y). It follows that

c1d + Dc2 (#X |µ) + D(#Y |)

= c1(x, y)dy|x(y) dX + Dc2 (X |µ) + D(Y |)
XY

=
X
=
X

c(x, Tx(z))d(z)
Z
c(x, Tx(z))d(z)
Z

(x)dµ(x) + Dc2 (X |µ) + D(Y |)

(x)dµ(x) +

c2(

dX dµ

)dµ(x)

+

D (T , | )

= c(x, Tx(z))d(z) (x)dµ(x) +
XZ
 L(µ, ),

c2((x))dµ(x) + D(T,|)

where



is

taken

to

be

the

Radon-Nikodym

derivative

dX dµ

.

Since

the

inequality

holds

for

any

,

this

implies

W~ c1,c2,(µ, )  L(µ, ).

14

Under review as a conference paper at ICLR 2019

Proof of Proposition 3.3. Note that W~ c1,c2,(µ, ) is equivalent to Wc1,c2,(µ, ) when X is restricted to the support of µ. If c1, c2,  satisfy (i) or (ii), they also satisfy (i) and (ii) when X is restricted to the support of µ. By Theorem 3.3 of (Liero et al., 2018), W~ c1,c2,(µ, ) has a minimizer. It follows from the construction of the proof of Lemma 3.2 that a minimizer of L(µ, ) also exists. For uniqueness, if  = , then it follows from Lemma 3.5 (Liero et al., 2018) and the fact that minimizers are restricted to G that the marginals #X , #Y  are uniquely determined for any solution  of W~ c1,c2,(µ, ). The uniqueness of  then follows from the
proof of Corollary 3.6 (Liero et al., 2018). It follows from the construction of the proof of Lemma 3.2 that the
product measure generated by minimizers of L(µ, ) is unique.

Proof of Theorem 3.4. Since k(s) converges pointwise to the equality constraint =(s), which is 0 for
s = 1 and  otherwise, by Lemma 3.9 (Liero et al., 2018), we have that lim infk W~ c1,c2,k(µ, )  W~ c1,c2,= (µ, ). Additionally, W~ c1,c2,k(µ, )  W~ c1,c2,= (µ, ) for any value of k since for any minimizer  of W~ c1,c2,= (µ, ), it holds that #Y  = , so

W~ c1,c2,= (µ, ) = c1d + Dc2 (#X |µ) + Dk2 (#Y |)  W~ c1,c2,k(µ, ),

for all k. Therefore, limk W~ c1,c2,k(µ, ) = W~ c1,c2,= (µ, ), and by Lemma 3.2, the first part of the proposition holds.

For the second part, by hypothesis, we have that W~ c1,c2,= (µ, ) = C < . It follows that W~ c1,c2,k(µ, )  C for all k. By Proposition 2.10 (Liero et al., 2018), the sequence of minimizers k is bounded. If the

assumptions of Proposition 3.3 are satisfied, then the sequence k is equally tight, under assumptions (ii)

by Proposition 2.10 (Liero et al., 2018) again, and under assumptions (i), by the Markov inequality: for any

 > 0,

k({(x, y)



X

×

Y|c1(x, y)

>

})



1 

c1dk



C .


Since k are bounded and equally tight, by an extension of Prokhorov's theorem (Theorem 2.2 of (Liero et al., 2018)), there exists a subsequence of k that is weakly convergent to some ¯. By lower semicontinuity, we
have

c1d¯ + Dc2 (#X ¯|µ) + lim sup Dk2 (#Y k|)  W~ c1,c2,= (µ, ) = C
k
Since Dk(#Y k|) = kD(#Y k|)  0 and k  , for the left side to be finite, D(#Y k|) must converge to 0, so D(#Y ¯|) = 0 by lower semicontinuity. Therefore, ¯ is a minimizer of W~ c1,c2,= (µ, ). By construction of the proof of Lemma 3.2, k is equivalent to the product measure induced by minimizers of Lk(µ, ), so the second part of the proposition holds.
C PRACTICAL CONSIDERATIONS FOR NUMERICAL EXPERIMENTS
Choice of cost functions. Proposition 3.3 gives sufficient conditions on c1, c2 for the problem to be well-posed. In practice, it is often convenient the cost of transport, c1, to be some measurement of correspondence between X and Y. For example, we can take c1(x, y) to be the Euclidean distance between x and y after mapping them to some common feature space. For the cost of mass adjustment, c2, it is generally sensible to choose some convex function that vanishes at 1 (i.e. no mass adjustment) and such that limx0 c2(x) = limx c2(x) =

15

Under review as a conference paper at ICLR 2019

Name Kullback-Leibler (KL)
Pearson 2
Hellinger
Jensen-Shannon

(s) s log s - s + 1

(s - 1)2

 (s

-

1)2

s

log

s

-

(s

+

1)

log

s+1 2

D(P |Q)

log

dP dQ

dP

-

dP +

dQ

(

dP dQ

-

1)2dQ

(

dP dQ

-

1)2dQ

1 2

DKL

(P

|

P

+Q 2

)

+

1 2

DK

L(Q|

P

+Q 2

)

(s)

es

s2 4

+s

s 1-s

- log(2 - es)

  
1
log 2

Table 1: Table of some common -divergences, associated entropy functions , and convex conjugates  for Algorithm 1, partly adapted from (Nowozin et al., 2016).

 to prevent  from becoming too small or too large. Any of the entropy functions shown in Table 1 are reasonable choices. In practice, we also found that employing a Lipschitz penalty on  stabilizes training.
Choice of -divergence and f . Based on Theorem 3.4,  must be uniquely minimized at (1) = 0. In Nowozin et al. (2016), it was shown that any -divergence could be used to train generative models, i.e. to match a generated distribution P to a true data distribution Q. This is due to Jensen's inequality: for any convex lower semi-continous entropy function , D(P |Q) is uniquely minimized when P = Q, where P, Q are probability measures. However, this additional constraint is required in our case since P, Q are not probability measures, as illustrated by the following example.
Example C.1. In the original GAN paper, the discrminative objective,

sup log f (x)dP (x) - log(1 - f (x))dQ(x),
f
corresponds to D(P |Q) with (s) = s log s - (s + 1) log(s + 1). If P, Q are probability measures, this divergence is equivalent to the Jensen-Shannon divergence and is minimized when P = Q. If P, Q are unnormalized positive measures, the divergence is minimized when P =  and Q = 0.

Table 1 provides some common -divergences that can be used for unbalanced transport.

According to Lemma 3.1, f should belong to a class of functions that maps from Y to (-, ]. In practice, this can be enforced by parameterizing f using a neural network with a final layer that maps to the correct
range. In practice, we also found that employing a Lipschitz penalty on f stabilizes training.

Improved training dynamics. Recall that the objective function for our alternating gradient updates was

1n

(, , ) := n

[cµc1(xi, T(xi, zi))(xi) + cµc2((xi))

i=1

+ cµ(xi)f(T(xi, zi)) - c (f(yi))]

Early in training,  can become very small for some xi as none of the transported samples resemble samples from the target distribution. As a result, T improves very slowly for some inputs xi. One way to address this without changing the fixed point is to update , f using the above objective and update T using the following objective.

1n

(, , ) := n

cµ[c1(xi, T(xi, zi)) + f(T(xi, zi))]

i=1

Note that we have omitted terms in the original objective that do not include T, and which therefore do not affect the gradient update. For the terms that remain, the difference is that we are rescaling the contribution of

each sample xi by 1/(xi). As long as (xi) > 0, this has the effect of rescaling the gradient update of the loss function with respect to each T(xi, zi) without changing the direction of the update.

16

