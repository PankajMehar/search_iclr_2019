Under review as a conference paper at ICLR 2019
BAYESIAN DEEP LEARNING VIA STOCHASTIC GRADIENT MCMC WITH A STOCHASTIC APPROXIMATION ADAPTATION
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables. Inspired by dropout, a popular tool for regularization and model ensemble, we assign sparse priors to the weights in deep neural networks (DNN) in order to achieve automatic "dropout" and avoid over-fitting. By alternatively sampling from posterior distribution through stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and optimizing latent variables via stochastic approximation (SA), the trajectory of the target weights is proved to converge to the true posterior distribution conditioned on optimal latent variables. This ensures a stronger regularization on the over-fitted parameter space and more accurate uncertainty quantification on the decisive variables. Simulations from large-p-small-n regressions showcase the robustness of this method when applied to models with latent variables. Additionally, its application on convolutional neural networks (CNN) leads to state-of-the-art performance on MNIST and Fashion MNIST datasets and improved resistance to adversarial attacks.
1 INTRODUCTION
Bayesian deep learning, which evolved from Bayesian neural networks (Neal, 1996; Denker and leCun, 1990), provides an alternative to point estimation due to its close connection to both Bayesian probability theory and cutting-edge deep learning models. It has been shown of the merit to quantify uncertainty (Gal and Ghahramani, 2016b), which not only increases the predictive power of DNN, but also further provides a more robust estimation to enhance AI safety. Particularly, Gal and Ghahramani (2016a;b) described dropout (Srivastava et al., 2014) as a variational Bayesian approximation. Through enabling dropout in the testing period, the randomly dropped neurons generate some amount of uncertainty with almost no added cost. However, the dropout Bayesian approximation is variational inference (VI) based thus it is vulnerable to underestimating uncertainty.
MCMC, known for its asymptotically accurate posterior inference, has not been fully investigated in DNN due to its unscalability in dealing with big data and large models. Stochastic gradient Langevin dynamics (SGLD) (Welling and Teh, 2011), the first SG-MCMC algorithm, tackled this issue by adding noise to a standard stochastic gradient optimization, smoothing the transition between optimization and sampling. Considering the pathological curvature that causes the SGLD methods inefficient in DNN models, Li et al. (2016) proposed combining adaptive preconditioners with SGLD (pSGLD) to adapt to the local geometry and obtained state-of-the-art performance on MNIST dataset. To avoid SGLD's random-walk behavior, Chen et al. (2014) proposed using stochastic gradient Hamiltonian Monte Carlo (SGHMC), a second-order Langevin dynamics with a large friction term, which was shown to have lower autocorrelation time and faster convergence (Chen et al., 2015). Saatci and Wilson (2017) used SGHMC with GANs (Goodfellow et al., 2014a) to achieve a fully probabilistic inference and showed the Bayesian GAN model with only 100 labeled images was able to achieve 99.3% testing accuracy in MNIST dataset. Raginsky et al. (2017); Zhang et al. (2017); Xu et al. (2018) provided theoretical interpretations of SGLD from the perspective of non-convex optimization, echoing the empirical fact that SGLD with model averaging works well in practice.
When the number of predictors exceeds the number of observations, applying the spike-and-slab priors is particularly powerful and efficient to avoid over-fitting by assigning less probability mass on
1

Under review as a conference paper at ICLR 2019

the over-fitted parameter space (Song and Liang, 2017). However, the inclusion of latent variables from the introduction of the spike-and-slab priors on Bayesian DNN is not handled appropriately by the existing methods. It is useful to devise a class of SG-MCMC algorithms that can explore the Bayesian posterior as well as optimize the hidden structures efficiently.
In this paper, we propose a robust Bayesian learning algorithm: SG-MCMC with a stochastic approximation adaptation (SG-MCMC-SA) to infer complex posteriors with latent variables and apply this method to supervised deep learning. This algorithm has four main contributions: 1. It enables SG-MCMC method to efficiently sample from complex DNN posteriors with latent variables and is proved to converge; 2. By automatically searching the over-fitted parameter space to add more penalties, the proposed method quantifies the posterior distributions of the decisive variables more accurately; 3. The proposed algorithm is robust to various hyperparameters; 4. This method demonstrates more resistance to over-fitting in simulations, leads to state-of-the-art performance and shows more robustness over the traditional SG-MCMC methods on the real data.

2 STOCHASTIC GRADIENT MCMC

We denote the decaying learning rate at time k by (k), the entire data by D = {di}Ni=1, where di = (xi, yi), the log of posterior by L(),  as the gradient of any function in terms of . The minibatch of data B is of size n with indices S = {s1, s2, ..., sn}, where si  {1, 2, ..., N }. Stochastic gradient L~() from a mini-batch of data B randomly sampled from D is used to approximate the
true gradient L():

L~() =  log P() + N n

 log P(di|).

iS

The stochastic gradient Langevin dynamics (no momentum) is formed as follows:

(k+1) = (k) + (k)GL~((k)) + N (0, 2 (k)G -1),

(1)

where  > 0 denotes the temperature, G is a positive definite matrix to precondition the dynamics. It has been shown that SGLD asymptotically converges to a stationary distribution (|D)  eL() (Teh et al., 2015; Zhang et al., 2017). As  increases, the algorithm tends towards optimization with underestimated uncertainty. Another variant of SG-MCMC, SGHMC (Chen et al., 2014; Ma et al., 2015), proposes to use second-order Langevin dynamics to generate samples:

 (k+1) = (k) + (k)M -1r(k)   r(k+1) = r(k) + (k)L~((k)) - (k)CM -1r + N (0, (k)(2C - (k)B^(k)) -1)

(2)

where r is the momentum item, M is the mass, B^ is an estimate of the error variance from the stochastic gradient, C is a user-specified friction term to counteracts the noisy gradient.

3 STOCHASTIC GRADIENT MCMC WITH A STOCHASTIC APPROXIMATION ADAPTATION
Dropout has been proven successful, as it alleviates over-fitting and provides an efficient way of making bagging practical for ensembles of countless sub-networks. Dropout can be interpreted as assigning the Gaussian mixture priors on the neurons (Gal and Ghahramani, 2016c). To mimic Dropout in our Bayesian CNN models, we assign the spike-and-slab priors on the most fat-tailed weights in FC1 (Fig. 1). From the Bayesian perspective, the proposed robust algorithm distinguishes itself from the dropout approach in treating the priors: our algorithm keeps updating the priors during posterior inference, rather than fix it. The inclusion of scaled mixture priors in deep learning models were also studied in Blundell et al. (2015); Li et al. (2016) with encouraging results. However, to the best of our knowledge, none of the existing SG-MCMC methods could deal with complex posterior with latent variables. Intuitively, the Bayesian formulation with model averaging and the spike-andslab priors is expected to obtain better predictive performance through averages from a "selective" group of "good" submodels, rather than averaging exponentially many posterior probabilities roughly. For the weight priors of the rest layers (dimension u), we just assume they follow the standard Gaussian distribution, while the biases follow improper uniform priors.

2

Under review as a conference paper at ICLR 2019

Convolution

Pooling

Convolution

Pooling

Flattening

FC1

FC2

Figure 1: Model Structure

3.1 CONJUGATE SPIKE-AND-SLAB PRIOR FORMULATION FOR DNN

Similarly to the hierarchical prior in the EM approach to variable selection (EMVS) (Rorková and George, 2014), we assume the weights   Rp in FC1 follow the spike-and-slab mixture prior

(|2, ) = Np(0, V, ),

(3)

where   {0, 1}p,   R, V, = 2 diag(a1, . . . , ap) with aj = (1 - j)v0 + jv1 for each j and 0 < v0 < v1. By introducing the latent variable j = 0 or 1, the mixture prior is represented as

j|j  (1 - j)N (0, 2v0) + jN (0, 2v1).

(4)

The interpretation is: if j = 0, then j is close to 0; if j = 1, the effect of j on the model is intuitively large. The likelihood of this model given a mini-batch of data {(xi, yi)}iS is



 

(22)-n/2 exp





-

1 22



(B|, 2) =

iS (yi - (xi; ))2

     iS

exp{yi (xi; )}

K t=1

exp{t(xi

;

)}

(regression) (classification),

(5)

where (xi; ) can be a mapping for logistic regression or linear regression, or a mapping based on a series of nonlinearities and affine transformations in the deep neural network. In the classification formulation, yi  {1, . . . , K} is the response value of the i-th example.

In addition, the variance 2 follows an inverse gamma prior

(2|) = IG(/2, /2).

(6)

The i.i.d. Bernoulli prior is used since there is no structural information in the same layer.

(|) = ||(1 - )p-||, where ()  a-1(1 - )b-1 and   R.

(7)

Finally, our posterior density follows

(,

2,

,

|B)



(B|,

2)

N n

(|2,

)(2|)(|)().

(8)

The EMVS approach is efficient in identifying potential sparse high posterior probability submodels on high-dimensional regression (Rorková and George, 2014) and classification problem (McDermott et al., 2016). These characteristics are helpful for large neural network computation, thus we refer the stochastic version of the EMVS algorithm as Expectation Stochastic-Maximization (ESM).

3.2 EXPECTATION STOCHASTIC-MAXIMIZATION

Due to the existence of latent variables, optimizing (, 2, |B) directly is difficult. We instead iteratively optimize the "complete-data" posterior log (, 2, , |B), where the latent indicator 
is treated as "missing data".

More precisely, the ESM algorithm is implemented by iteratively increasing the objective function

Q(, , |(k), (k), (k)) = E|· log (, , , |B)|(k), (k), (k), B ,

(9)

3

Under review as a conference paper at ICLR 2019

where E|· denotes the conditional expectation E|(k),(k),(k),B(·). Given ((k), (k), (k)) at the k-th iteration, we first compute the expectation of Q, then alter (, , ) to optimize it.

For the conjugate spike-slab hierarchical prior formulation, the objective function Q is of the form

Q(, , |(k), (k), (k)) = C + Q1(, |(k), (k), (k)) + Q2(|(k), (k), (k)), (10)

where

Q1(, |(k), (k), (k))

=

N n

log (B|, 2)

-

p j=1

j2E|·

1 v0(1 - j) + v1j 22

-

p+u j=p+1
2

j2

-

p

+

 2

+

2

log(2)

-

 22

(11)

and

p
Q2(|(k), (k), (k)) = log
j=1

 1-

E|·j + (a - 1) log() + (p + b - 1) log(1 - ). (12)

3.2.1 THE E-STEP

The physical meaning of E|·j in Q2 is the probability j, where   Rp, of j having a large effect on the model. Formally, we have

j

=

E|·j

=

P(j

=

1|(k), (k), (k))

=

aj , aj + bj

(13)

where aj = (j(k)|j = 1)P(j = 1|(k)) and bj = (j(k)|j = 0)P(j = 0|(k)). The choice of Bernoulli prior enables us to use P(j = 1|(k)) = (k).

The other conditional expectation comes from a weighted average j, where   Rp.

j = E|·

1 v0(1 - j) + v1j

= E|·(1 - j ) + E|·j = 1 - j + j . v0 v1 v0 v1

(14)

3.2.2 THE STOCHASTIC M-STEP

Since there is no closed-form optimal solution for  here, to optimize Q1 with respect to , we use Adam (Kingma and Ba, 2014), a popular algorithm with adaptive learning rates, to train the model.
In order to optimize Q1 with respect to , by denoting diag{i}pi=1 as V, following the formulation in McDermott et al. (2016) and Rorková and George (2014) we have:


      (k+1) =
     

N n

iS yi - (xi; (k+1)) 2 + ||V 1/2(k+1)||2 + 

N +p+

2
V 1/2(k+1) + 

p++2

(regression),
(classification). (15)

To optimize Q2, a closed-form solution can be derived from Eq.(12) and Eq.(13).

(k+1) = arg max Q2(|(k), (k), (k)) =
R

p j=1

j

+

a

-

1

.

a+b+p-2

(16)

3.3 SG-MCMC-SA

Extension of EMVS to DNN is nontrivial, since the EM algorithm is plagued by the local trap problems in high-dimensional nonlinear models (Wang and Zhang, 2006). In practice the improper

4

Under review as a conference paper at ICLR 2019

Algorithm 1 SGLD-SA with spike and slab priors
Inputs: { (k)}kT=1, {(k)}Tk=1,  , a, b, , , v0, v1 Outputs: {(k)}Tk=1 Initialize: (1), (1), (1), (1) and (1) from scratch for k  1 : T do
Sample mini-batch B(k) Sampling (k+1)  (k) + (k)Q(, , |(k), (k), (k)) + N (0, 2 (k) -1)
Optimization Compute (k+1) in Eq.(13), perform SA: (k+1)  (1 - (k+1))(k) + (k+1)(k+1) Compute (k+1) in Eq.(14), perform SA: (k+1)  (1 - (k+1))(k) + (k+1)(k+1) Compute (k+1) in Eq.(15), perform SA: (k+1)  (1 - (k+1))(k) + (k+1)(k+1) Compute (k+1) in Eq.(16), perform SA: (k+1)  (1 - (k+1))(k) + (k+1)(k+1) end for

initialization and the noise from approximation strongly bias the updates of latent variables, which causes them to rapidly converge to arbitrarily poor local optima. To alleviate the local trap problem in Monte Carlo computation as in Liang et al. (2007), we propose to apply the stochastic approximation in SG-MCMC:

(k+1) = (k) + (k)L~((k), (k)) + N (0, 2 (k) -1), (k+1) = (1 - (k+1))(k) + (k+1)g(k) ((k+1)),

(17)

where g(·) is the mapping detailed in Eq.(13), Eq.(14), Eq.(15) and Eq.(16) to derive the optimal  based on the current . A practical step size (k) can be set as a polynomial decay A(k + Z)-

with   (0.5, 1], and small positive constant A (large Z) helps reduce the risk of local trap. The

interpretation of this algorithm is that we sample (k+1) from L~((k), (k)) and adaptively optimize

(k+1) from the mapping g(k) in terms of (k+1). We expect to obtain an augmented sequence as follows:

(1), (1); (2), (2); (3), (3); . . .

(18)

Under certain assumptions, (k) will eventually converge to the optimal  and (k) will converge to samples from eL(,). We provide the theorem of the (local) L2 convergence rate of SGLD-SA here and present the details in Appendix B.
Theorem 1 (L2 convergence rate). For any   (0, 1] and any compact subset   R2p+2, under assumptions in Appendix B.1, the algorithm satisfies: there exists a constant  such that

E (k) -  2I(k  t())  (k),

where t() = inf{k : (k) / } and (k) = A(k + Z)-  O(k-). Corollary 1. For any   (0, 1] and any compact subset   R2p+2, under assumptions in Appendix B.2, (1), (2), ... in (18) converges to samples from the equilibrium distribution eL(,) as  0.

The key to proving the convergence is to use the generalized stochastic approximation (Benveniste

et al., 1990). The latent variable estimators will be consistent from the convexity assumption and

the fact of

 i=1

(i)

=

.

Furthermore,

the

mappings

of

Eq.(13),

Eq.(14),

Eq.(15)

and

Eq.(16)

all

satisfy the assumptions on g in a compact subset  (Appendix B), which enable us to apply theorem

1 to SGLD-SA. Because SGHMC proposed by Chen et al. (2014) is essentially a second-order

Langevin dynamics and yields a stationary distribution given an accurate estimation of the error

variance from the stochastic gradient, SGLD-SA can be naturally extended to SGHMC-SA without

losing the convergence property.

Corollary 2. For any   (0, 1] and any compact subset   R2p+2, under assumptions in Appendix
B.2, the subsequence (1), (2), ... from (1), (1), (1), (1), (1); (2), (2), (2), (2), (2); ... converges to samples from the equilibrium distribution (, , , , |D) as  0.

5

Under review as a conference paper at ICLR 2019

Table 1: Predictive errors in linear regression based on a test set considering different v0 and 

MAE / MSE
SGLD-SA EMVS SGLD

v0=10-3, =0.4
1.32 / 2.85 1.43 / 3.19 4.98 / 42.6

v0=10-3, =0.5
1.34 / 2.90 3.04 / 13.6 4.98 / 42.6

v0=10-2, =0.4
1.34 / 2.92 3.40 / 18.8 4.98 / 42.6

v0=10-2, =0.5
1.37 / 2.93 1.33 / 2.95 4.98 / 42.6

3.4 POSTERIOR APPROXIMATION

The posterior average given decreasing learning rates can be approximated through the weighted

sample average E[()] =

T k=1

(k)  ((k) )

T (k)

(Welling

and

Teh,

2011)

to

avoid

over-emphasizing

the

k=1

tail end of the sequence and reduce the variance of the estimator. Teh et al. (2015); Chen et al. (2015)

showed a theoretical optimal learning rate (k)  k-1/3 for SGLD and k-1/5 for SGHMC to achieve

faster convergence for posterior average, which are used in Sec. 4.1 and Sec. 4.2 respectively.

4 EXPERIMENTS

4.1 SIMULATION OF LARGE-P-SMALL-N REGRESSION

SGLD-SA can be applied to the (logistic) linear regression cases, as long as u = 0 in Eq.(11).

We conduct the linear regression experiments with a dataset containing n = 100 observations
and p = 1000 predictors. Np(0, ) is chosen to simulate the predictor values X (training set) where  = ()ip,j=1 with i,j = 0.6|i-j|. Response values y are generated from X + , where  = (1, 2, 3, 0, 0, ..., 0) and   Nn(0, 3In). To make the simulation in Rorková and George (2014) more challenging, we assume 1  N (3, c2), 2  N (2, c2), 3  N (1, c2), c = 0.2.

We introduce some hyperparameters, but most of them are uninformative, e.g.   {10-3, 1, 103}

makes little difference in the test set performance. Sensitivity analysis shows that three hyperparame-

ters are important: v0, a and , which are used to identify and regularize the over-fitted space. We fix

 = 1,  = 1,  = 1, v1 = 100,  = 0.5, b = p and set a = 1. The learning rates for SGLD-SA and

SGLD are set to

(k)

=

0.001

×

k

-

1 3

,

and

the

step

size

(k)

=

0.1

×

(k

+

1000)-

3 4

.

We

vary

v0

and

 to show the robustness of SGLD-SA with respect to different initializations.

To implement SGLD-SA, we perform stochastic gradient optimization by randomly selecting 50 observations and calculating the corresponding gradient in each iteration. We simulate 500, 000 samples from the posterior distribution and at the same time keep optimizing the latent variables. EMVS is implemented with  directly optimized each time. We also simulate a group of the test set with 1000 observations (display 50 in Fig. 2(e)) in the same way as generating the training set to evaluate the generalizability of our algorithm. Tab.1 shows that EMVS frequently fails given bad initializations, while SGLD-SA is fairly robust to the hyperparameters. In addition, from Fig. 2(a), Fig. 2(b) and Fig.2(c), we can see SGLD-SA is the only algorithm among the three that quantifies the uncertainties of 1, 2 and 3 reasonably and always gives more accurate posterior average (Fig.2(f)); by contrast, as shown in Fig.2(d), 2(e) and Tab.1, SGLD fails in regularizing the over-fitted space, yielding over-fitting in the training set and poor generalization in the test set.

For the simulation of SGLD-SA in logistic regression to demonstrate the advantage of SGLD-SA over SGLD and ESM, we leave the results in Appendix C.

4.2 CLASSIFICATION PROBLEM
We implement all the algorithms in Pytorch (Paszke et al., 2017) and run the experiments on GeForce GTX 1080 GPUs. The first DNN we use is a standard 2-Conv-2-FC CNN model (Fig.1) of 670K parameters (see details in Appendix D.1). The first set of experiments is to compare methods on the same model without using other complication, such as data augmentation (DA) or batch normalization (BN) (Ioffe and Szegedy, 2015). We refer to the general CNN without dropout as Vanilla, with 50% dropout rate applied to the green neurons (Fig.1) as Dropout. Vanilla and Dropout models are trained with Adam (Kingma and Ba, 2014) with Pytorch default parameters (with learning rate 0.001). We

6

Under review as a conference paper at ICLR 2019

SGLD-SA SGLD True value

SGLD-SA SGLD True value

SGLD-SA SGLD True value

-1 0 1 2 3 4

(a) Posterior estimation of 1.

q SGLD-SA

q q

q SGLD q EMVS

q qqq qqqqqqqqqqqq qqqqqqqqqqqqqqqq q

True value qq
qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqqq

qq

qqqqq q

qq

(d) Performance on training set.

-2 -1

0

1

2

3

(b) Posterior estimation of 2.

q SGLD-SA

q

q SGLD q

q q qq

q qqq q

q EMVS

q

qq

True valueq q qq

q q qqq

q q
q

q

qqqqq

q q

qqq q qq
q

q q
q

q q

q q

qqq q

qqqq

q q

qqqqqqq

q

q

qq q

qq q qq

qq qq

qqq q

q q

qq

q qqqqq

q q q

q

q

q q qq qqq q

q q

q

q q

q

q q qqq

q q

q

qqq

q

qq qqqq

q qq q q

q
q qq

q

q

q

q

qq

q

q

(e) Performance on test set.

-1 0 1 2
(c) Posterior estimation of 3.

0.0 0.5 1.0 1.5 2.0 2.5 3.0

q SGLD-SA q SGLD q EMVS

q

q

^ q

q

q

q

qq qq

q

0qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq.0

0.5

1.0

1.5

q
q 2.0

2.5

3.0

(f) Posterior means vs true values.

Figure 2: Linear regression simulation when v0 = 10-3 and  = 0.5

use SGHMC as a benchmark method as it is also sampling-based and has a close relationship with

the popular momentum based optimization approaches in DNN. SGHMC-SA differs from SGHMC

in that SGHMC-SA applies the spike-and-slab priors to the FC1 layer while SGHMC just uses the standard normal priors. The hyperparameters v1 = 1, v0 = 1 × 10-3 and  = 0.1 in SGHMC-SA

are used to regularize the over-fitted space, and a, b are set to p to obtain a moderate "sparsity" to

resemble

dropout,

the

step

size

is

(k)

=

0.1

×

(k

+

1000)-

3 4

.

We

use

training

batch

size

1000

and

a thinning factor 500 to avoid a cumbersome system, and the posterior average is applied to each

Bayesian model. Temperatures are tuned to achieve better results (Appendix D.2).

The four CNN models are tested on the classical MNIST and the newly introduced Fashion MNIST (FMNIST) (Xiao et al., 2017) dataset. Performance of these models is shown in Tab.2. Compared with SGHMC, our SGHMC-SA outperforms SGHMC on both datasets. We notice the posterior averages from SGHMC-SA and SGHMC obtain much better performance than Vanilla and Dropout. Without using either DA or BN, SGHMC-SA achieves 99.60% which even outperforms some state-of-the-art models, such as stochastic pooling (99.53%) (Zeiler and Fergus, 2013), Maxout Network (99.55%) (Goodfellow et al., 2013) and pSGLD (99.55%) (Li et al., 2016) . In F-MNIST, SGHMC-SA obtains 93.01% accuracy, outperforming all other competing models.

To further test the maximum performance of SGHMC-SA, we apply DA and BN to the following experiments (see details in Appendix D.3) and refer the datasets with DA as aMNIST and aFMNIST. All the experiments are conducted using a 2-Conv-BN-3-FC CNN of 490K parameters. Using this model, we obtain 99.75% on aMNIST (300 epochs) and 94.38% on aFMNIST (1000 epochs). The results are noticeable because posterior model averaging is essentially conducted on a single Bayesian neural network. We also conduct the experiments based on the ensemble of five networks and refer them as aMNIST-5 and aFMNIST-5 in Tab. 2. We achieve 99.79% on aMNIST-5 using 5 small Bayesian neural networks each with 2 thinning samples (4 thinning samples in aFMNIST-5), which is comparable with the state-of-the-art performance (Wan et al., 2013).

4.3 DEFENSES AGAINST ADVERSARIAL ATTACKS Continuing with the setup in Sec. 4.2, the third set of experiments focus on evaluating model robustness. We expect less robust models perform considerably well on a certain dataset due to
7

Under review as a conference paper at ICLR 2019

Table 2: Classification accuracy on MNIST and Fashion MNIST using small networks

DATASET
VANILLA DROPOUT
SGHMC SGHMC-SA

MNIST
99.31 99.38
99.55 99.60

aMNIST
99.54 99.56
99.71 99.75

aMNIST-5
99.75 99.74
99.77 99.79

FMNIST
92.73 92.81
92.93 93.01

aFMNIST
93.14 93.35
94.29 94.38

aFMNIST-5
94.48 94.53
94.64 94.78

100% 100%

SGHMC-SA

80%

80%

SGHMC Dropout

Vanilla

60% 60%

40%

SGHMC-SA

20%

SGHMC Dropout

Vanilla

0%0.0 0.1 0.2 0.3 0.4 0.5

(a)  = ..., 0.3, ....

(b) MNIST

40%

20%

(c)  = ..., 0.3, ....

0%0.0 0.1 0.2 0.3 0.4 0.5
(d) F-MNIST

Figure 3: Adversarial test accuracies based on adversarial images of different levels

over-tuning; however, as the degree of adversarial attacks increases, the performance decreases sharply. In contrast, more robust models should be less affected by these adversarial attacks.

We apply the Fast Gradient Sign method (Goodfellow et al., 2014b) to generate the adversarial examples with one single gradient step as in Papernot et al. (2016)'s study:

xadv



x

-



·

sign{x

max log
y

P(y

|x)},

where  ranges from 0.1, 0.2, . . . , 0.5 to control the different levels of adversarial attacks.

Similar to the setup in the adversarial experiments by Li and Gal (2017), we normalize the adversarial images by clipping to the range [0, 1]. As shown in Fig. 3(b) and Fig.3(d), there is no significant difference among all the four models in the early phase. As the degree of adversarial attacks arises, the images become vaguer as shown in Fig.3(a) and Fig.3(c). In this scenario the performance of Vanilla decreases rapidly, reflecting its poor defense against adversarial attacks, while Dropout performs better than Vanilla. But Dropout is still significantly worse than the sampling based methods SGHMC-SA and SGHMC. The advantage of SGHMC-SA over SGHMC becomes more significant when  > 0.25.

In the case of  = 0.5 in MNIST where the images are hardly recognizable, both Vanilla and Dropout models fail to identify the right images and their predictions are as worse as random guesses. However, SGHMC-SA model achieves roughly 11% higher than these two models and 1% higher than SGHMC, which demonstrates the strong robustness of our proposed SGHMC-SA. Overall, SGHMC-SA always yields the most robust performance.

5 CONCLUSION AND FUTURE RESEARCH
We propose a mixed sampling-optimization method called SG-MCMC-SA to efficiently sample from complex DNN posteriors with latent variables and prove its convergence. By adaptively searching and penalizing the over-fitted parameter space, the proposed method improves the generalizability of deep neural networks. This method is less affected by the hyperparameters, achieves higher prediction accuracy over the traditional SG-MCMC methods in both simulated examples and real applications and shows more robustness towards adversarial attacks.
Interesting future directions include applying SG-MCMC-SA towards popular large deep learning models such as the residual network (He et al., 2015) on CIFAR-10 and CIFAR-100, combining active learning and uncertainty quantification to learn from datasets of smaller size and proving posterior consistency and the consistency of variable selection under various shrinkage priors concretely.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Albert Benveniste, Michael Métivier, and Pierre Priouret. Adaptive Algorithms and Stochastic Approximations. Berlin: Springer, 1990.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. In Proc. of the International Conference on Machine Learning (ICML), pages 1613­1622, 2015.
Changyou Chen, Nan Ding, and Lawrence Carin. On the convergence of stochastic gradient MCMC algorithms with high-order integrators. In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS), pages 2278­2286, 2015.
Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In Proc. of the International Conference on Machine Learning (ICML), 2014.
John S. Denker and Yann leCun. Transforming neural-net output levels to probability distributions. In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS), pages 853­859, 1990.
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with Bernoulli approximate variational inference. In ICLR workshop, 2016a.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In Proc. of the International Conference on Machine Learning (ICML), 2016b.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS), page 1019­1027, December 2016c.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS), pages 2672­2680, 2014a.
Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. In Proc. of the International Conference on Machine Learning (ICML), pages III­1319­ III­1327, 2013.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. ArXiv e-prints, December 2014b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. ArXiv e-prints, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proc. of the International Conference on Machine Learning (ICML), pages 448­456, 2015.
K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In Proc. of the International Conference on Computer Vision (ICCV), pages 2146­2153, September 2009.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. of the International Conference on Learning Representation (ICLR), 2014.
Chunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned stochastic gradient Langevin dynamics for deep neural networks. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, Proc. of the National Conference on Artificial Intelligence (AAAI), pages 1788­1794, 2016.
Yingzhen Li and Yarin Gal. Dropout inference in Bayesian neural networks with alpha-divergences. In Proc. of the International Conference on Machine Learning (ICML), 2017.
9

Under review as a conference paper at ICLR 2019
Faming Liang. Trajectory averaging for stochastic approximation MCMC algorithms. The Annals of Statistics, 38:2823­2856, 2010.
Faming Liang, Chuanhai Liu, and Raymond J. Carroll. Stochastic approximation in Monte Carlo computation. Journal of the American Statistical Association, 102:305­320, 2007.
Yi-An Ma, Tianqi Chen, and Emily B. Fox. A complete recipe for stochastic gradient mcmc. In NIPS Autodiff Workshop, 2015.
Patrick McDermott, John Snyder, and Rebecca Willison. Methods for Bayesian variable selection with binary response data using the em algorithm. ArXiv e-prints, May 2016.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, 1996.
Nicolas Papernot, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Fartash Faghri, Alexander Matyasko, Karen Hambardzumyan, Yi-Lin Juang, Alexey Kurakin, Ryan Sheatsley, Abhibhav Garg, and Yen-Chen Lin. cleverhans v2.0.0: an adversarial machine learning library. ArXiv e-prints, October 2016.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS Autodiff Workshop, 2017.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis. ArXiv e-prints, June 2017.
Veronika Rorková and Edward I. George. Emvs: The EM approach to Bayesian variable selection. Journal of the American Statistical Association, 109(506):828­846, 2014.
Yunus Saatci and Andrew G Wilson. Bayesian gan. In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS), pages 3622­3631, 2017.
Issei Sato and Hiroshi Nakagawa. Approximation analysis of stochastic gradient Langevin dynamics by using fokker-planck equation and ito process. In Proc. of the International Conference on Machine Learning (ICML), pages 982­990, 2014.
Qifan Song and Faming Liang. Nearly optimal Bayesian shrinkage for high dimensional regression. ArXiv e-prints, December 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929­1958, 2014.
Yee Whye Teh, Alexandre Thiéry, and Sebastian Vollmer. Consistency and fluctuations for stochastic gradient Langevin dynamics. ArXiv e-prints, September 2015.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proc. of the International Conference on Machine Learning (ICML), 2013.
Yi Wang and Nevin L. Zhang. Severity of local maxima for the em algorithm: Experiences with hierarchical latent class models. In the Third European Workshop on Probabilistic Graphical Models, 2006.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proc. of the International Conference on Machine Learning (ICML), pages 681­688, 2011.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. ArXiv e-prints, August 2017.
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of Langevin dynamics based algorithms for nonconvex optimization. ArXiv e-prints, February 2018.
Matthew Zeiler and Robert Fergus. Stochastic pooling for regularization of deep convolutional neural networks. In Proc. of the International Conference on Learning Representation (ICLR), 2013.
Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient Langevin dynamics. In Proc. of Conference on Learning Theory (COLT), pages 1980­2022, 2017.
10

Under review as a conference paper at ICLR 2019

A REVIEW

A.1 FOKKER-PLANCK EQUATION

The Fokker-Planck equation (FPE) can be formulated from the time evolution of the conditional distribution for a stationary random process. Denoting the probability density function of the random process at time t by q(t, ), where  is the parameter, the stochastic dynamics is given by

tq(t, ) = ((-L())q(t, )) + 2q(t, ).

(19)

Let q() = limt q(t, ). If limt tq(t, ) = 0, then

lim
t

 ( (-L( ))q(t,

))

+

2 q(t,

)

=

0

  ( (-L()q())) + 2q() = 0

  ((-L())q() + q()) = 0   (q() ((-L() +  log q())) = 0.

(20)

Therefore, (-L()) +  log(q()) = 0, which implies q()  eL(). In other words, limt q(t, )  eL(), i.e. q(t, ) gradually converges to the Bayesian posterior eL().

A.2 STOCHASTIC APPROXIMATION
The stochastic approximation algorithm introduced by Robbins and Monro (1951) is intent on solving the integration equation
h() = H(, )f()d = 0,
where   ,   B and f() is a density function depending on . The stochastic approximation algorithm is an iterative recursive algorithm consisting of two steps:
(1) Generate (k+1)  f(k) (), where k indicates the iteration. (2) Set (k+1) = (k) + (k+1)H((k), (k+1)), where (k+1) > 0 is called the gain factor.

B CONVERGENCE ANALYSIS

B.1 CONVERGENCE OF HIDDEN VARIABLES

The stochastic gradient Langevin Dynamics with a stochastic approximation adaptation (SGLD-SA) is a mixed half-optimization-half-sampling algorithm to handle complex Bayesian posterior with latent variables, e.g. the conjugate spike-slab hierarchical prior formulation. Each iteration of the algorithm consists of the following steps:

(1) Sample (k+1) using SGLD based on the current (k), i.e. (k+1) = (k) +  L~((k), (k)) + N (0, 2  -1); 
(2) Optimize (k+1) from the following recursion

(21)

(k+1) = (k) + (k+1) g(k) ((k+1)) - (k)
= (1 - (k+1))(k) + (k+1)g(k) ((k+1)), where g(k) (·) is some mapping to derive the optimal  based on the current .

(22)

Remark: Define H((k), (k+1)) = g(k) ((k+1)) - (k). In this formulation, our target is to find  such that h() = 0, where h() := H(, )f()d. When  = , E[H((k), (k+1)) - h((k))|Fk] = 0, this algorithm falls to the category of generalized stochastic approximation.

11

Under review as a conference paper at ICLR 2019

GENERAL ASSUMPTIONS

To provide the local L2 upper bound for SGLD-SA, we first lay out the following assumptions:

Assumption 1 (Step size and Convexity). {(k)}kN is a positive decreasing sequence of real numbers such that



(k)  0,

(k) = +.

(23)

k=1

There exist constant  > 0 and  such that for all   

 - , h()  -  -  2,

(24)

with additionally

(k)

(k+1) - (k)

lim inf 2

+

k

(k+1)

(k+1)2

> 0.

Then for any   (0, 1] and suitable A and B, a practical (k) can be set as

(k) = A(k + B)-

(25) (26)

Assumption 2 (Existence of Markov transition kernel). For any   , in the mini-batch sampling, there exists a family of noisy Kolmogorov operators {} approximating the operator (infinitesimal) of the Ito diffusion, such that every Kolmogorov operator  corresponds to a single stationary distribution f, and for any Borel subset A of , we have
P[(k+1)  A|Fk] = (k) ((k), A).
Assumption 3 (Compactness). For any compact subset  of , we only consider    such that

  C0()

(27)

Note that the compactness assumption of the latent variable  is not essential, the assumption that the variable is in the compact domain is not only reasonable, but also simplifies our proof.

In addition, there exists constants C1() and C2() so that

E[ H(, (k)) 2|Fk]  C1()(1 + (k) 2)  C2()(1 + (k) -  +  2)  C2()(1 + (k) -  2)

(28)

Assumption 4 (Solution of Poisson equation). For all   , there exists a function µ on  that solves the Poisson equation µ() - µ() = H(, ) - h(), which follows that

H ((k), (k+1)) = h((k)) + µ(k) ((k+1)) - (k) µ(k) ((k+1)).

(29)

For any compact subset  of , there exist constants C3(, ) and C4(, ) such that for all ,   ,

(1) µ  C3(), (2) µ -  µ  C4()  -  ,

(30)

Remark: For notation simplicity, we write C1() as C1, C2() as C2, . . . in the following context.

Lemma 1 is a restatement of Lemma 25 (page 447) from Benveniste et al. (1990).

Lemma 1. Suppose k0 is an integer which satisfies with

inf
kk0

(k+1) - (k) (k)(k+1)

+ 2

- (k+1)C2

>

0.

12

Under review as a conference paper at ICLR 2019

Then for any k > k0, the sequence {kK }k=k0,...,K defined below is increasing and bounded by 2(K)

 2(k)  Kk =
 2(K)

K-1 j=k

(1

-

2j+1

+

j2+1C2)

if k < K, if k = K.

(31)

Lemma 2 is an extension of Lemma 23 (page 245) from Benveniste et al. (1990)

Lemma 2. There exist 0 and k0 such that for all   0 and k  k0, the sequence u(k) = (k) satisfies

u(k+1)  (1 - 2(k+1) + (k+1)2C2)u(k) + (k+1)2C2 + (k+1)C1.

(32)

Proof. Replace u(k) = (k) in (32), we have (k+1)  (1 - 2(k+1) + (k+1)2C2)(k) + (k+1)2C2 + (k+1)C1.

(33)

From (25), we can denote a positive constant + as limk inf 2(k+1)(k) + (k+1) - (k). Then (33) can be simplified as

(+

-

(k+1)2(k)C2)



(k+1)

2
C2

+

(k+1) C 1 .

(34)

There exist 0 and k0 such that for all  > 0 and k > k0, (34) holds. Note that in practical case when C1 is small, finding a suitable 0 will not be a problem.

Theorem 1 (L2 convergence rate). Suppose that Assumptions 1-4 hold, for any compact subset   , the algorithm satisfies: there exists a constant  such that
E (k) -  2I(k  t())  (k),

where t() = inf{k : (k) / }.

Proof. Denote T (k) = (k) - , with the help of (22) and Poisson equation (29), we deduce that T (k+1) 2 = T (k) 2 + (k+1)2 H ((k), (k+1)) 2 + 2(k+1) T (k), H ((k), (k+1)) = T (k) 2 + (k+1)2 H ((k), (k+1)) 2 + 2(k+1) T (k), h((k)) + 2(k+1) T (k), µ(k) ((k+1)) - (k) µ(k) ((k+1)) = T (k) 2 + D1 + D2 + D3.

First of all, according to (28) and (24), we have (k+1)2 H((k), (k+1)) 2  (k+1)2C2(1 + T (k) 2), 2(k+1) T (k), h((k))  -2(k+1) T (k) 2,

(D1) (D2)

Conduct the decomposition of D3 similar to Theorem 24 (p.g. 246) from Benveniste et al. (1990) and Lemma A.5 (Liang, 2010).
µ(k) ((k+1)) - (k) µ(k) ((k+1)) = µ(k) ((k+1)) - (k) µ(k) ((k)) + (k) µ(k) ((k)) - (k-1) µ(k-1) ((k)) + (k-1) µ(k-1) ((k)) - (k) µ(k) ((k+1)) , = D3-1 + D3-2 + D3-3.
13

Under review as a conference paper at ICLR 2019

(i) µ(k) ((k+1)) - (k) µ(k) ((k)) forms a martingale difference sequence since E µ(k) ((k+1)) - (k) µ(k) ((k))|Fk = 0.

(D3-1)

(ii) From (30) and (27) respectively, we deduce that (k) µ(k) ((k)) - (k-1) µ(k-1) ((k))  C4 (k) - (k-1)  2C4C0

Thus there exists C4 = 2C4C0 such that 2(k+1) T (k), (k) µ(k) ((k)) - (k-1) µ(k-1) ((k))  (k+1)C4.
(iii) D3-3 can be further decomposed to D3-3a and D3-3b T (k), (k-1) µ(k-1) ((k)) - (k) µ(k) ((k+1))
= T (k), (k-1) µ(k-1) ((k)) - T (k+1), (k) µ(k) ((k+1)) + T (k+1), (k) µ(k) ((k+1)) - Tk, (k) µ(k) ((k+1))
= (z(k) - z(k+1)) + T (k+1) - T (k), (k) µ(k) ((k+1)) = D3-3a + D3-3b,

(D3-2)

where z(k) = T (k), (k-1) µ(k-1) ((k)) with a constant C3 = 4C0C3 which satisfies that

2(k+1) T (k+1) - T (k), (k) µ(k) ((k+1)) = 2(k+1) (k+1) - (k), (k) µ(k) ((k+1))  2(k+1) (k+1) - (k) · (k) µ(k) ((k+1))  4(k+1)C0C3 = C3(k+1)

(D3-3b)

Finally, add all the items D1, D2 and D3 together, for some C1 = C3 + C4, we have E T (k+1) 2  (1 - 2(k+1) + (k+1)2C2)E Tk 2 + (k+1)2C2 + (k+1)C1 + 2(k+1)E[z(k) - z(k+1)].

Moreover, from (30) and (27), there exists a constant C5 such that E[|z(k)|]  C5.

(35)

Lemma 3 is a simple extension of Lemma 26 (page 248) from Benveniste et al. (1990).

Lemma 3. Let {u(k)}kk0 as a sequence of real numbers such that for all k  k0, some suitable constants C1 and C2

u(k+1)  u(k) 1 - 2(k+1) + (k+1)2C2 + (k+1)2C2 + (k+1)C1,

(36)

and assume there exists such k0 that E T (k0) 2  u(k0).

Then for all k > k0, we have

k

E T (k) 2  u(k) +

jk(z(j-1) - z(j)).

j =k0 +1

14

(37)

Under review as a conference paper at ICLR 2019

Proof of Theorem 1 (Continued). From Lemma 2, we can choose 0 and k0 which satisfy the conditions (36) and (37)
E[ T (k0) 2]  u(k0) = 0(k0).

From Lemma 3, it follows that for all k > k0


k



E Tk 2  u(k) + E 

kj z(j-1) - z(j)  .

j =k0 +1

(38)

From (35) and the increasing property of jk in Lemma 1, we have


k
E  jk
j =k0 +1

z(j-1) - z(j)


k-1



 = E

(kj+1 - kj )z(j) - 2(k)z(k) + kk0+1z(k0) 

j =k0 +1

k-1
 (kj+1 - jk)C5 + E[|2(k)z(k)|] + kkC5
j =k0 +1

 (kk - kk0 )C5 + kkC5 + kkC5  3kkC5 = 6C5(k).

(39)

Therefore, given the sequence u(k) = 0(k) that satisfies conditions (36), (37) and Lemma 3, for any k > k0, from (38) and (39), we have
E[ T (k) 2]  u(k) + 3C5kk = (0 + 6C5) (k) = (k),
where  = 0 + 6C5.

B.2 CONVERGENCE OF SAMPLES

In addition to the previous assumptions, we make one more assumption on the stochastic gradients to guarantee that the samples converge to the posterior conditioned on the optimal latent variables:
Assumption 5 (Gradient Unbiasedness and Smoothness). For all   B and   , the mini-batch of data B, the stochastic noise , which comes from ~ L(, ) - L(, ), is a white noise and independent with each other. In addition, there exists a constant l  2 such that the following conditions hold:

EB[] = 0 and EB[l] < .

(40)

For all ,   , there exists a constant M > 0 such that the gradient is M-smooth: L(, ) - L(,  )  M  - 

(41)

Corollary 1. For all   (0, 1], under assumptions 1-5, the subsequence (1), (2), ... in (1), (1); (2), (2); . . . converges to samples from the equilibrium distribution eL(,) as  0.
Proof. The proof framework follows from section 4 of Sato and Nakagawa (2014). In the context of stochastic noise (k), we ignore the subscript of and only consider the case of  = 1. Since (k) converges to  in SGLD-SA and the gradient is M-smooth (41), we transform the stochastic gradient from L~((k), (k)) to L((k), ) + (k) + O( ), therefore Eq.(21) can be written as
 (k+1) = (k) + L((k), ) + (k) + O( ) + 2 (k), where (k)  N (0, I). (42)
15

Under review as a conference paper at ICLR 2019

Using Eq.(40), the characteristic function of (k) + O( +1) is

E exp is( (k) + O( +1)

=E

1 l!

l
is( (k) + O( +1)

= 1 + O( +1).

l=0

(43)

 Then the characteristic function of 2 (k) is exp(- s2).

Rewrite (k+1) as (k+ ), the characteristicfunction of (t+ ) is the characteristic function of (k) + L((k), ) + (k) + O( ) + 2 (k), which is

t+ (s) = exp(is + is L(, ) - s2) 1 + O( +1) q(k, )d = exp(is + is L(, ) - s2)q(k, )d + O( +1).
With the fact exp(x) = 1 + x + O(x2), we can get

(k+ )(s) - (k)(s) = exp(is) exp(is L(, ) - s2) - 1 q(k, )d + O( +1) = exp(is) is L(, ) - s2 + O( 2) q(k, )d + O( +1) = exp(is) is L(, ) - s2 q(k, )d + O( +1).

Therefore,

(k+ )(s) - (k)(s) = - is exp(is)(-L(, ))q(k, )d + (-is)2 exp(is)q(k, )d + O( ).

For any integrable function f , set F as the Fourier transform defined by F[f (x)](s) = 1 f (x) exp(isx)dx. 2
The inverse Fourier transform of F [f (x)] and the l-th order derivatives of f (l)(x) is F -1[f (x)](s) = f (x) = 1 F [f (x)](s) exp(-isx)dx, 2 F (f (l))(s) = (-is)l(F (f ))(s).
Combine Eq.(46), Eq.(47) and Eq.(48), we arrive at the following simplified equation: (t+ )(s) - (k)(s) 2
= - isF (-L(, ))q(k, ) + (-is)2F q(k, ) + O( ) =F  ((-L(, ))q(k, )) + F 2 q(k, ) + O( ).
16

(44)
(45) (46) (47) (48) (49)

Under review as a conference paper at ICLR 2019

Table 3: Predictive errors in logistic regression based on a test set considering different v0 and 

MAE / MSE
SGLD-SA SGLD ESM

v0=10-2, =0.6
0.182 / 0.0949 0.311 / 0.2786 0.240 / 0.0982

v0=10-1, =0.6
0.195 / 0.1039 0.304 / 0.2645 0.454 / 0.2080

v0=10-2, =2
0.146 / 0.1049 0.333 / 0.2977 0.182 / 0.0882

v0=10-3, =2
0.165 / 0.0890 0.331 / 0.3037 0.172 / 0.1102

 Since F -1[(k)(s)] = 2q(k, ) and   (0, 1],

q(k + , ) - q(k, ) lim
0

= lim F -1 (k+ )(s) - (k)(s) 0 2

=

lim
0



(

(-L(,



))q(k,

))

+

2

q(k,

)

+

O(

)

=((-L(, ))q(k, )) + 2 q(k, ).

(50)

Finally, we have proved that the embedded subsequence (1), (2), . . . from the sequence (1), (1); (2), (2); . . . also follows the Fokker Planck equation when  0 and   .

C SIMULATION OF LOGISTIC REGRESSION

Now we conduct the experiments on binary logistic regression. The setup is similar as before, except

n is set to 500, i,j = 0.3|i-j| and   N (0, I/2). We set the learning rates in SGLD-SA and SGLD

to

0.01

×

k-

1 3

and

step

size

(k)

to

0.1

×

(k

+

1000)-

3 4

.

The

binary

response

values

are

simulated

from Bernoulli(p) where p = 1/(1 + e-X-). Fig.4(a), Fig.4(b) and Fig.4(c) demonstrate the

posterior distribution of SGLD-SA is significantly better than that of SGLD. As shown in Fig.4(f),

SGLD-SA is the best method to regulate the over-fitting space and provides the most reasonable

posterior mean. Table.3 illustrates the predictive power of SGLD-SA is overall better than the other

methods and robust to different initializations. Fig.4(d) and Fig.4(e) show that the over-fitting problem

of SGLD when p > n in logistic regression and the algorithm fails to regulate the over-fitting space;

We observe SGLD-SA is able to resist over-fitting and always yields reproducible results.

D EXPERIMENTAL SETUP
D.1 NETWORK ARCHITECTURE
The first DNN we use is a standard 2-Conv-2-FC CNN: it has two convolutional layers with a 2 × 2 max pooling after each layer and two fully-connected layers. The filter size in the convolutional layers is 5 × 5 and the feature maps are set to be 32 and 64, respectively (Jarrett et al., 2009). The fully-connected layers (FC) have 200 hidden nodes and 10 outputs. We use the rectified linear unit (ReLU) as activation function between layers and employ a cross-entropy loss.
The second DNN is a 2-Conv-BN-3-FC CNN: it has two convolutional layers with a 2 × 2 max pooling after each layer and three fully-connected layers with batch normalization applied to the first FC layer. The filter size in the convolutional layers is 4 × 4 and the feature maps are both set to 64. We use 256 × 64 × 10 fully-connected layers.
D.2 TEMPERATURE STRATEGY
In practice, we observe a suitable temperature setting is helpful to improve the classification accuracy. For example, by setting  = 100 in the second DNN (see Appendix D.1) we obtain 99.70% on aMNIST. To account for the scale difference of weights in different layers, we apply different temperatures to different layers based on different standard deviations of the gradients in each layer and obtain the results in Tab. 2.

17

Under review as a conference paper at ICLR 2019

SGLD-SA SGLD True value

SGLD-SA SGLD True value

SGLD-SA SGLD True value

123456
(a) Posterior estimation of 1.

SGLD-SAqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qq

SGLD

q

ESM

q

q

True value q q q q

q

q

q q

q qq

q

qq

qqqq

q

q q

q

q

q qq q q
q

qqqqq

q

q

q qq

q

q qqq

q q qqq

q q

q qqq
q

q q
qqq

q q
q

q

qqq qq

q qq

q qqqqq q

q q

qq

q qq

q

q
q q qq

q qq

q q
qq

qq

qq q

q qq qq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

0 20 40

60

80 100

(d) Performance on training set.

012345

(b) Posterior estimation of 2.

SGLD-SAqqq q

q

q
q

qq

qq

q

q
q

qq

q
q

q

qq
q

q

q
q

qq

qq

q

qq

q

qq

qq

SGLD

q

q q

ESM

q q
q

True value

q qq

q q
qq

q

qq

q

q

qq

q q

q

q q

q

qq q

qq

q

q
qq

q
q

qqq

qqq

q
qq

qqq

qq

q

q

qq

q

q
qqq

(e) Performance on test set.

01234

0123

(c) Posterior estimation of 3.

q SGLD-SA

qq

q SGLD

q ESM

qq q

^
q qq

q
q qq q q0qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq.0 0.5 1.0 1.5 2.0 2.5 3.0
(f) Posterior means vs true values.

Figure 4: Logistic regression simulation when v0 = 0.1 and  = 0.6

D.3 DATA AUGMENTATION
The MNIST dataset is augmented by (1) randomCrop: randomly crop each image with size 28 and padding 4, (2) random rotation: randomly rotate each image by a degree in [-15, +15], (3) normalization: normalize each image with empirical mean 0.1307 and standard deviation 0.3081.
The FMNIST dataset is augmented by (1) randomCrop: same as MNIST, (2) randomHorizontalFlip: randomly flip each image horizontally, (3) normalization: same as MNIST.

18

