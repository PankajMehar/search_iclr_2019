Under review as a conference paper at ICLR 2019
NOISY INFORMATION BOTTLENECKS FOR GENERALIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
We propose Noisy Information Bottlenecks (NIB) to limit mutual information between learned parameters and the data through noise. We show why this benefits generalization and allows mitigation of model overfitting both for supervised and unsupervised learning, even for arbitrarily complex architectures. We reinterpret methods including the Variational Autoencoder, -VAE, network weight uncertainty and a variant of dropout combined with weight decay as special cases of our approach, explaining and quantifying regularizing properties and vulnerabilities within information theory.
1 INTRODUCTION
Bayesian inference is at the core of many directions of machine learning research in recent years. Applications range from a principled treatment of uncertainty in neural networks (Gal, 2016) over latent variable models for high-dimensional data (Kingma & Welling, 2013; Rezende & Mohamed, 2015) to reinterpretations of popular stochastic regularizers (Kingma et al., 2015; Gal, 2016) that inspired more flexible extensions (Louizos & Welling, 2017; Molchanov et al., 2017). In all but the simplest models, exact inference is intractable, so approximations are necessary. Among those, variational inference (Wainwright et al., 2008) has been particularly popular, as it is amenable to gradient-based stochastic optimization (Kingma & Welling, 2013; Titsias & La´zaro-Gredilla, 2014; Ranganath et al., 2014) and thus highly scalable. Typically, researchers seek to develop more flexible approximate posterior distributions (Rezende & Mohamed, 2015; Kingma et al., 2016; Salimans et al., 2015; Ranganath et al., 2016; Husza´r, 2017; Chen et al., 2018; Vertes & Sahani, 2018; Burda et al., 2015; Cremer et al., 2017) in the hope of more faithfully representing the true posterior. However, recent works have suggested (Trippe & Turner, 2018; Braithwaite & Kleijn, 2018; Shu et al., 2018) that restricting the family of variational approximations can, in fact, have a positive regularizing effect, leading to better generalization. In this work, we seek to provide an information theoretic explanation for the observed behaviour of different families variational approximations. By reinterpreting Gaussian mean-field inference as maximum a-posteriori in a noisy model, we can quantify bounds on the mutual information between the data and the parameters variables. We explore the potential of this approach for regularization. Unlike methods relying on information-based objectives (Tishby et al., 2000; Shamir et al., 2010; Agakov & Barber, 2004; Chen et al., 2016; Phuong et al., 2018), it is compatible with standard variational inference.
2 NOISY INFORMATION BOTTLENECKS
This section characterizes model overfitting within information theory and points out a mitigation strategy that limits the amount of information extracted by the inferred model about the data. We will show this to be implicitly deployed by a common class of inference techniques in section 3 in order to explain observed regularizing effects.
2.1 AN INFORMATION-THEORETIC CHARACTERIZATION OF MODEL OVERFITTING
Intuitively, model overfitting can be understood as memorization or learning too much information about the training data. More formally, if D denote the random variable for all our data and  all
1

Under review as a conference paper at ICLR 2019

learned parameters and latent variables in a given probabilistic model, we can quantify the amount of information gained about the data D given a particular parameter value  in

I(D,  = ) := H(D) - H(D| = )

(1)

where H(D| = ) := -Ep(D|=)log p(D| = ) denotes the entropy of a variable D conditioned on a particular value .

In the context of variational inference, model overfitting can now be characterized by the symptom

that the information

Eq()I(D,  = )

(2)

a sampled  from the approximately inferred parameter distribution q() contains about the data D

on average is too large.

This can be motivated through the extreme case of maximum-likelihood learning in the limit of unrestrictedly expressive models for supervised tasks. Here, q is a point-mass on the parameter values that maximize the likelihood of the data. Under the assumption of identically distributed (iid) data, the model will then store all information about the data up to the identity of the datapoint, reflected in the entropy of the empirical distribution, as shown in Appendix A. Because the identity of the datapoint can never be learned by the model, this is the maximum amount of information that can possibly be extracted from the data. This will result in perfect overfitting up to the sample identity, thereby impeding generalization for smaller training datasets.

In unsupervised learning, information that can be learned by the model about the data will not be learned by the latent, a phenomenon known as the information preference problem (Chen et al., 2016; Alemi et al., 2016; Zhao et al., 2017; Phuong et al., 2018). Again, this can be characterized by the quantity from Equation 2 being too high.

2.2 WHY BOUNDING MUTUAL INFORMATION?

This observation motivates placing a limit on the expected amount of extracted information given by Equation 2. We observe that the expectation of I(D,  = ) under the prior p() is just the mutual information I(D, )
Ep()I(D,  = ) = H(D) - Ep()H(D| = ) = H(D) - H(D|) = I(D, ) (3)

This implies that if we could limit I(D, ) to be not greater than some capacity C, we would

obtain the following guarantee: If our dataset d was sampled from the marginal d  p(D) = dp()p(D|), then a sample from the exact posterior   p(|D = d) will, on average, contain

no more information about the data D than our capacity C due to

Edp(D)Ep(|D=d)I(D,  = ) = Ep()I(D,  = ) = I(D, )  C

(4)

This would limit the quantity given in Equation 2 in expectation under the assumptions that the model captures the nature of the generating process in p(D) and that our posterior estimate q() is close to the true posterior p(|D). These are common assumptions necessary to justify any variational Bayesian approach.

2.3 NOISY INFORMATION BOTTLENECKS

How can we limit I(D, )? One way to achieve this is to make the data dependent only on some

transformed version ~ of the learned variables  (Figure 1a), with a prior p() and corruption process

p(~|) that results in a certain finite I(~, ). The data processing inequality (Cover & Thomas, 2012)

then ensures

I(D, )  I(~, )

(5)

for any model architecture p(D|~). C = I(~, ) now acts as a capacity, bounding the expected amount of information that is extracted about the data. In the next section, we will show that such a bound is implicit in an important class of variational inference techniques. As we will see, the corruption process will be realized through injected noise, and we therefore term this architecture noisy information bottleneck (NIB). Quantifying the corresponding capacity C will help to explain observed regularizing effects in both supervised and unsupervised learning (as shown in Figure 1b and 1c) from an information-theoretic perspective.

2

Under review as a conference paper at ICLR 2019

 ~

 ~

 ~ D

X (n)

Y (n)

n  {1, . . . , N }

Y (n)

Y~ (n)

X (n)

n  {1, . . . , N }

(a) (b)

(c)

Figure 1: A noisy information bottleneck (a) limits the mutual information I(D, ) between the data D and the learned variables  by making the data only depend on a corrupted version ~ of the latent, with a limited I(~, ). It can be applied to both (b) supervised with inputs X(n) and labels Y (n) and
(c) generative models with latents Y (n) and datapoints X(n).

3 REGULARIZATION THROUGH CONSTRAINED INFERENCE

Most variational approximation schemes such as variational autoencoders (Kingma et al., 2015; Rezende & Mohamed, 2015) and network weight uncertainty (Blundell et al., 2015) use simple Gaussian mean field inference for tractable training using the reparameterization trick. In this section, we quantify information-theoretic capacity constraints implicit in Gaussian mean field inference that helps to understand observed regularizing effects. We will first illustrate the bound for a fixed-scale distribution and then discuss the more general case where the variance is learned as well.

3.1 FIXED-SCALE GAUSSIAN MEAN FIELD INFERENCE

The variational free energy objective can be expressed as

F (q) = Eq() log p(D|) - DKL (q()||p())

(6)

We consider the case of Gaussian mean field inference with fixed variance 2 on parameters with component-wise independent priors N 0, p2 . We can write the inferred q() in its reparameterized form  = µ +  , with µ being the inferred mean and i being noise sampled from N (0, 1), as
shown in Figure 2a. We can then write the objective as

F (µ) = Ep( ) log p(D| = µ +  ) -

i

µi2 2p2

+ const.

(7)

We now define a model p with injected noise that results in the same training algorithm when a MAP
objective is applied. The underlying idea is to reinterpret the means µ from the inference distribution as the parameters of our new model, as shown in Figure 2b. We define the prior µi  N 0, p2 and then define ~i = µi +  i, where noise i is sampled from the unit Gaussian q ( i) = p ( i) = p( i) = N (0, 1). We leave the rest of the model p (D|~) = p(D|) unchanged. In terms of
gradients, MAP can be understood as a variational approach employing a deterministic approximate posterior q (µ) with point-mass at a particular µ, as outlined in Appendix B. The MAP objective of our newly defined model can therefore be written in the variational form1

F (q ) = Eq (µ)q ( ) log p (D|µ, ) + log p (µi )

(8)

i

Using above definitions, and Equation 7, this implies equivalence F (µ) = F (µ) of both objec-

tives, resulting in training algorithms that are exactly equivalent.

For µ,

the

new

model,

we

get

H (~)

=

K 2

log 2e

inducing a noisy information bottleneck of

2 + p2 capacity

and H(~|µ

=

µ)

=

K 2

log 2e2

for any

I(~, µ)

=

H (~)

-

Eµ p(µ) H (~|µ

=

µ)

=

K 2

log

1

+

p2 2

(9)

1Due to the inferred noise q ( ), this is not pure MAP, but rather a lower bound objective similar to the free energy. We still call it MAP for compactness.

3

Under review as a conference paper at ICLR 2019

Inference model q 2 i

Generative model p

Inference model q Generative model p 2 i

µi i i D p2
i  {1, . . . , K}

µi µi

~i

p2

D

i  {1, . . . , K}

(a) (b)

Figure 2: Gaussian mean field inference of fixed scale on model parameters with a Gaussian prior (a) can be reinterpreted as MAP inference on a model with injected noise (b): The mean of the original inference model correspond to the parameters of the new generative model. i denotes the model parameter index.

in the sense of Equation 5, where K denotes the number of parameters. This quantity is known as

the capacity of channels with Gaussian noise in signal processing (Cover & Thomas, 2012). Simply

adjusting

the

signal-to-noise

ratio

p2 2

allows

to

create

an

information

bottleneck

of

any

desired

ca-

pacity. This suggests the usefulness of applying fixed-scale Gaussian mean field inference to model

parameters for regularization.

There are relations to existing approaches: When applied to training neural networks, MAP training on this model can be interpreted as applying weight decay combined with a additive noise N (0, 2) on all network weights. Molchanov et al. (2017) shows that this results in multiplicative noise on the unit activations. Wang & Manning (2013) reports that empirical results do not change significantly when dependencies between the different elements of the layer output, which is then equivalent to scaled Gaussian dropout (Kingma et al., 2015).

3.2 FLEXIBLE-SCALE GAUSSIAN MEAN FIELD INFERENCE

The variance in Gaussian mean field inference is typically learned for each parameter (Kingma et al., 2015; Rezende & Mohamed, 2015; Blundell et al., 2015). We can obtain a capacity constraint for this case by regarding both the inferred mean µ and variance 2 as the new latent. For simplicity and different from the fixed-scale case, we assume a prior variance of p2 = 1.
The resulting capacity is not in closed analytic form, but numeric results are shown in Figure 3. We obtain a capacity of 0.45 bits per latent component. The derivation can be generalized to approaches where the KL term from the objective is scaled by some factor  > 0 as done by -VAE (Higgins et al., 2017) in the context of amortized inference on latents. When observe that higher  corresponds to smaller capacity, which is given by the mutual information I(~i, (µi, i2)) between our new latent (µi, i2) and ~i.

I( i, ( i, i2)) in bits

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0
10 2 10 1 100 101 102
Figure 3: Relationship between  and capacity I(~i, (µi, i2)) of each latent component in flexible-scale Gaussian mean field inference with complexity term scaled by  > 0. Values are given in Table 1 in the appendix.

Curiously, this results in priors µi  N

0,

1 

and i2  

 2

+ 1,

 2

.

4

Under review as a conference paper at ICLR 2019

The derivation is similar to the one for fixed-scale case from the previous section and given in a slightly different form in Appendix C.
3.3 SHOULD VARIATIONAL INFERENCE BE FLEXIBLE OR REGULARIZING?
As mentioned in the introduction, more flexible approximate inference is the goal of many recent approaches. However, in previous subsections we showed that training with Gaussian mean field inference can be reinterpreted as MAP learning on a noise-injected model that has a certain informationtheoretic capacity that is useful to mitigate overfitting. Therefore the goals of good inference and regularization are in conflict.
The reinterpretation necessary to retrieve the capacity also suggests a natural resolution: Instead of using MAP on the noise-injected model, we can deploy arbitrarily flexible variational inference techniques. This separates out the concern of regularization into the model and allows to combine powerful inference with good regularization. The focus of this piece of work, however, is to demonstrate that limiting the mutual information between the model parameters and the data leads to better generalization, hence we leave exploring the use of powerful inference techniques in noisy models as future research.

4 EXPERIMENTS

In VAEs, Gaussian mean field inference on the latents leads to a restricted latent capacity, but leaves the capacity of the model unbounded. This leaves VAEs vulnerable to model overfitting, as shown in subsection 2.1, and setting  as done in (Higgins et al., 2017) is not sufficient to control complexity. Relatedly, the bound given through Equation 9 was also noted by Braithwaite & Kleijn (2018), but was again applied only to the latents and not the model parameters.
We therefore propose to apply Gaussian mean field inference of fixed scale to the model parameters in order to mitigate model overfitting both for supervised and unsupervised tasks. This is further motivated by the regularizing effect empirically observed when using Gaussian mean field inference over point estimates reported by Blundell et al. (2015). Fixing the variance 2 allows to easily control the capacity of the bottleneck, as described in section subsection 3.1.

loglikelihood

0 Test Train
1

2

3

4

5

6

3

4 mo5del capac6ity per d7imension8in bits 9

10

Figure 4: Classifying CIFAR10 with varying model capacities. Large capacities lead to overfitting while small capacities drown the signal in noise. Each configuration has been evaluated 5 times; mean and standard deviation are displayed.

In this section we validate this idea. In order to clearly show overfitting, we train large architectures and a small number of training samples for most experiments. We explore the effect noisy information bottlenecks implicit in fixed scale Gaussian mean field inference for varying model capacities, architectures and priors as well as training set sizes.

4.1 SUPERVISED LEARNING
We apply our approach to classification on the CIFAR10 dataset, where we train on a subset of the first 5000 samples. We use 6 3x3 convolutional layers with 128 channels each followed by a relu activation function, every second of which implements striding 2 to reduce the input dimensionality. Finally, the last layer is a linear projection which parameterizes a categorical distribution. The capacity of each parameters in this network is set to specific values given by Equation 9.
Figure 4 shows that decreasing the model capacity per channel (by increasing the noise) reduces the training log-likelihood and increases the test loglikelihood until both of them meet at an optimal

5

Under review as a conference paper at ICLR 2019

test elbo test elbo

Standard VAE
200

400

600

800

1000

1200

model prior 0.5

1400

1.0 2.0

4.0

01

234 capacity per dimension in bits

5

6

(a) The test ELBO is not invariant when varying the prior on the model parameters. Neverthless, the first increasing and then decreasing trend when changing the capacity remains.

500

1000

1500

2000

2500

3000

3500

NIB unit prior VAE Improper prior VAE

10 123456 log noise scale on model parameters

(b) Using an improper prior, similar to just using Gaussian Dropout on the weights, leads to accelerated decreasing generalization for smaller noise scales.

Figure 5: MNIST test reconstruction with a VAE training on 200 samples for various priors and capacities.

capacity. As noted before, without this prior the capacity would be infinite. We will analyze the phenomenon in the unsupervised case in the next subsection. It is also observable that very small capacities lead to a signal that is too noisy and good predictions are no longer possible. All these observations are in accordance with our predictions.
4.2 UNSUPERVISED LEARNING
We now evaluate the regularizing effect of fixed-scale Gaussian mean field inference in an unsupervised setting. Therefore, we use a VAE (Kingma & Welling, 2013) with 2 latent dimensions and a 3-layer neural network parameterizing the conditional factorized Gaussian distribution. As usual, it is trained using the free energy objective. Again, we use a small training set of 200 examples for most experiments.
In our first experiment we analyze generalization by inspecting the test ELBO when varying the model capacity which can be seen in Figure 5a. Similar to the supervised case we can observe that there is a certain model capacity range that explains the data very well while less or more capacity results in noise drowning and overfitting respectively. In the same figure we also investigated whether the information-theoretic model capacity can predict generalization independently of the specific prior distribution. From Figure 5a we can conclude that while model capacity seems to influence generalization very strongly, it seems like model choice such as the prior on the parameters also affects generalization slightly. It also implies that weight decay (Krogh & Hertz, 1992) of fixed scale without parameters noise is not sufficient to regularize arbitrarily large networks. Nevertheless, the shapes of all ELBO-capacity dependencies are similar. In Figure 5b we investigated the extreme case of dropping the prior entirely and switching to ML learning instead by using an improper uniform prior. This approach is very similar to Gaussian dropout, as described in subsection 3.1. Dropping the prior sets the bottleneck capacity to infinity and should lead to worse generalization. Comparing the test ELBO to NIB in Figure 5b confirms this result for larger capacities. For larger noise scales, generalization is still working well, a result that is not explained in our information-theoretic framework, but plausible due to the deployed limited architecture.
Figure 6a shows how our approach affects the test ELBO for varying amounts of training data. Models with very small capacity extract less information from the data into the model, thus yielding a good test ELBO somewhat independent of the dataset size. This is visible as a graph that ascends very little with more training data (e.g. total model capacity 330 kbits). Note that we here report the capacity of the entire model, which is the sum of the capacities for each parameter. In order to improve the test ELBO, more information from the data has to be extracted into the model. But clearly, this leads to non-generalizing information being extracted when the dataset is small, leading to overfitting, and generalizing information being extracted for larger datasets. This is visible as a
6

Under review as a conference paper at ICLR 2019

test elbo test elbo

100

150

200

250

300

Total model capacity 19 kbit Total model capacity 330 kbit

350

Total model capacity 396 kbit Total model capacity 495 kbit

400

Total model capacity 594 kbit Total model capacity 891 kbit

450

Total model capacity 1189 kbit Total model capacity 2378 kbit

Total model capacity unrestricted

500 103

104

dataset size

(a) Varying the number of samples. Depending on the size of the dataset higher capacities of the model are required to fit all the datapoints.

200

400

600

800 2 layers

3 layers

1000

4 layers 5 layers

6 layers

0.0 250.0K 500.0K 750.0K 1.0M 1.2M 1.5M 1.8M 2.0M total capacity in bits

(b) Varying architecture. Overfitting is not getting worse for more layers if capacity is low enough. More layers do overfit only for higher capacities.

Figure 6: MNIST test reconstruction with a VAE training on varying dataset sizes, architectures, and model capacities.

Figure 7: Test reconstruction means for binarized fashion MNIST trained on 200 samples with per-parameter capacities 5, 2 and 1 bits (from top) compared to the true data (bottom).
strongly ascending test ELBO with larger dataset sizes and bad generalization for small datasets. We can therefore conclude that the information bottleneck needs to be chosen based on the amounts of data that is available. This is expected as we want to extract more information into the model the more information is available.
Furthermore, we inspected how the size of the model (here in terms of number of layers) affects generalization in Figure 6b. The generalization does not decrease for the same total capacity but larger networks deteriorate the performance for larger total capacities. This indicates that the total capacity is less important than the individual capacity (i.e. noise) per parameter. Nevertheless, larger networks are more prone to overfitting for very large model capacities. This makes sense as their functional form is less constrained, an aspect that is not captured by our framework.
Finally, we plot test reconstruction means for the binarized fashion MNIST dataset under the same setup for various capacities in Figure 7. In accordance with previous experiments, we observe that if the capacity is chosen too small, the model is not learning anything useful, while too large capacities result in overconfidence, which can be seen through most means being close to either 0 or 1. An intermediate capacity, on the other hand, makes sensible predictions (given that it was trained only on 200 samples) with sensible uncertainty, visible through gray pixels that correspond to high entropy.
5 RELATED WORK
We have shown that NIB is a practical approach to regularize supervised and unsupervised models, and that in contrast to existing approaches, it successfully regularizes models with a fixed parameter setting, largely independent of the depth of a network. Unlike existing regularization techniques, our
7

Under review as a conference paper at ICLR 2019

approach features a capacity that can be naturally interpreted as a limit on the amount of information extracted about the given data by the inferred model.

The Information Bottleneck principle by Tishby et al. (2000); Shamir et al. (2010) aims to find a representation Z of some input X that is most useful to predict an output Y . For this purpose, the objective is to maximize the amount of information I(Y, Z) the representation contains about the output under a bounded amount of information I(X, Z) about the input:

max I(Y, Z)
I (X,Z )<C

(10)

They describe a training procedure using the softly constrained objective

min LIB = min I(X, Z) - I(Y, Z)

(11)

where  > 0 controls the trade-off.

Alemi et al. (2016) suggests a variational approximation for this objective. For the task of reconstruction, where labels Y are identical to inputs X, this results exactly in the -VAE objective (Achille & Soatto, 2017; Alemi et al., 2018). This is in accordance with our result from subsection 3.2 that there is a maximum capacity per latent dimension in the reinterpreted version of -VAE that get smaller for greater .

Reconstruction implicit in amortized inference for unsupervised learning is only one of many possible objectives for extracting representations, and it can be viewed a proxy for a not-yet-known future task. In this case, applying NIB to the latents is a way to enforce the hard constraint I(X, Z) < C of objective Equation 10 directly through the structure of the model, removing the need to augment the objective with a soft constraint, as in Equation 11.

For the supervised case where the task is already known, as in Tishby et al. (2000), we could achieve a similar hard constraint by constructing a noisy information bottleneck layer anywhere in the network based on the channel capacity theorem (Cover & Thomas, 2012) by limiting input variance (e. g. through a tanh activation) and injecting noise as in NIB. This allows extracting representations useful for a given output task with standard variational training, while naturally constraining mutual information with the input.

The Information Bottleneck principle is concerned with the information contained in the latent representation. NIB limits mutual information with all inferred variables, namely latents and model parameters in the case of unsupervised learning. This paper focuses on characterizing and mitigating the vulnerability of existing learning algorithms due to unconstrained mutual information of the data with the model parameters.

6 CONCLUSION
We have quantified the regularizing effects observed for Gaussian mean field approaches from an information-theoretic perspective. We have explored the usefulness of the implicit noisy information bottleneck for the purpose of generalization. Our approach features a capacity that can be naturally interpreted as a limit on the amount of information extracted about the given data by the inferred model. We validated its practicality for both supervised and unsupervised learning. We have shown that the approach allows to improve generalization even for arbitrarily large networks.
While this work demonstrates the capability of NIB to mitigate model overfitting when applied to model parameters, inspecting the effect of a limited latent capacity is left for future work. We expect our approach to be compatible with powerful inference techniques while keeping up regularization guarantees. This is still to be confirmed experimentally.

REFERENCES
A. Achille and S. Soatto. Emergence of invariance and disentangling in deep representations. arXiv preprint arXiv:1706.01350, 2017.
F. Agakov and D. Barber. The IM algorithm: A variational approach to information maximization. In Advances in Neural Information Processing Systems, pp. 201­208, 2004.

8

Under review as a conference paper at ICLR 2019
A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken ELBO. In International Conference on Machine Learning, pp. 159­168, 2018.
A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. In International Conference on Learning Representations, 2016.
C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks. In International Conference on Machine Learning, 2015.
D. Braithwaite and W. B. Kleijn. Bounded information rate variational autoencoders. arXiv preprint arXiv:1807.07306, 2018.
Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.
T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations. arXiv preprint arXiv:1806.07366, 2018.
X. Chen, D. P. Kingma, T. Salimans, Y. Duan, P. Dhariwal, J. Schulman, I. Sutskever, and P. Abbeel. Variational lossy autoencoder. In International Conference on Learning Representations, 2016.
T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
C. Cremer, Q. Morris, and D. Duvenaud. Reinterpreting importance-weighted autoencoders. In International Conference on Learning Representations Workshop, 2017.
Y. Gal. Uncertainty in Deep Learning. PhD thesis, 2016.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. -VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017.
F. Husza´r. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235, 2017.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.
D. P. Kingma, T. Salimans, and M. Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pp. 2575­2583, 2015.
D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pp. 4743­4751, 2016.
A. Krogh and J. A. Hertz. A simple weight decay can improve generalization. In Advances in Neural Information Processing Systems, pp. 950­957, 1992.
C. Louizos and M. Welling. Multiplicative normalizing flows for variational Bayesian neural networks. In International Conference on Machine Learning, pp. 2218­2227, 2017.
D. Molchanov, A. Ashukha, and D. Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pp. 2498­2507, 2017.
M. Phuong, M. Welling, N. Kushman, R. Tomioka, and S. Nowozin. The mutual autoencoder: Controlling information in latent code representations, 2018.
R. Ranganath, S. Gerrish, and D. Blei. Black box variational inference. In Artificial Intelligence and Statistics, pp. 814­822, 2014.
R. Ranganath, D. Tran, J. Altosaar, and D. Blei. Operator variational inference. In Advances in Neural Information Processing Systems, pp. 496­504, 2016.
D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning, pp. 1530­1538, 2015.
9

Under review as a conference paper at ICLR 2019
T. Salimans, D. Kingma, and M. Welling. Markov chain Monte Carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, pp. 1218­1226, 2015.
O. Shamir, S. Sabato, and N. Tishby. Learning and generalization with the information bottleneck. Theoretical Computer Science, pp. 2696­2711, 2010.
R. Shu, H. H. Bui, S. Zhao, M. J. Kochenderfer, and S. Ermon. Amortized inference regularization. arXiv preprint arXiv:1805.08913, 2018.
N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000.
M. Titsias and M. La´zaro-Gredilla. Doubly stochastic variational Bayes for non-conjugate inference. In International Conference on Machine Learning, pp. 1971­1979, 2014.
B. Trippe and R. Turner. Overpruning in variational Bayesian neural networks. arXiv preprint arXiv:1801.06230, 2018.
E. Vertes and M. Sahani. Flexible and accurate inference and learning for deep generative models. arXiv preprint arXiv:1805.11051, 2018.
M. J. Wainwright, M. I. Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, pp. 1­305, 2008.
S. Wang and C. Manning. Fast dropout training. In International Conference on Machine Learning, pp. 118­126, 2013.
S. Zhao, J. Song, and S. Ermon. InfoVAE: Information maximizing variational autoencoders. arXiv preprint arXiv:1706.02262, 2017.
10

Under review as a conference paper at ICLR 2019

A MAXIMUM-LIKELIHOOD ON UNRESTRICTED MODELS

We consider the extreme case of maximum-likelihood learning on N identically distributed (iid) data in the limit of unrestrictedly expressive models. For simplicity, we assume discrete data to avoid an undefined empirical differential entropy. A similar argument could be made for continuous data. We can then write

I(D,  = ) = N I(X,  = )

(12)

where p(X|) is the modeled distribution p(X(n)|)of any data sample X(n). This definition is valid because the distribution is independent of the data sample index n due to the iid assumption. Let p(X^ ) denote the empirical distribution. The maximum-likelihood objective can then be written
as

ML = arg min DKL p(X^ )||p(X| = )


(13)

In the unrestricted limit (e.g. arbitrarily sized network architecture), there exists a  so that the distribution p(X|) is equal to p(X^ ). Because DKL p(X^ )||p(X| = ) is then 0 and therefore
minimal, this is actually ML. From p(X| = ML) = p(X^ ) follows that the entropy conditioned on ML becomes the empirical entropy

H(X| = ML) = H(X^ )

(14)

Thus,

I(X,  = ML) = H(X) - H(X| = ML) = H(X) - H(X^ )

(15)

This implies that all information about the data is learned by the model up to the identity of the training sample, which remains as uncertainty in the prediction.

B MAP AND MAXIMUM-LIKELIHOOD AS VARIATIONAL INFERENCE

Variational inference constrained to deterministic approximate posterior distributions with (continuous) probability mass function

1 for Z = Z q(Z) =
0 else

(16)

(= point-mass at Z) can be interpreted as MAP:

arg min F (q)
Z
= arg min DKL (q(Z)||p(Z|X))
Z
= arg min Eq (log q(Z) - log p(Z|X))
Z
= arg min log q(Z) - log p(Z|X)
Z
= arg min - log p(Z|X)
Z
= arg max log p(Z|X)
Z
=ZMAP

(17) (18) (19) (20) (21) (22) (23)

DKL (q(Z)||p(Z|X)) is not finite, but its arg max and gradient are still well-defined. The same
result can be obtained more formally by regarding MAP as the limit of Gaussian mean field inference with diminishing variance 2  0.

11

Under review as a conference paper at ICLR 2019

Inference model q

Generative model p Inference model q

Generative model p

i2 i

i2 i2

i



X µi Yi Yi X X µi µi Yi X

i  {1, . . . , D}

i  {1, . . . , D}

(a) (b)

Figure 8: Gaussian mean field inference on a latent model (a) with the (-)VAE objective can be reinterpreted as MAP inference on mean and variance latents (b): The output mean and variance of (-)VAE's inference model correspond to the latents of the new generative model. This is a per-datapoint-view with data indices (n) and model parameter dependencies omitted.

For improper uniform priors p(Z) = const. we recover Maximum-Likelihood:
arg max log p(Z|X)
Z
= arg max log p(Z) + log p(X|Z)
Z
= arg max log p(X|Z)
Z
=ZML

(24) (25) (26) (27)

C LATENT CAPACITY IN -VAE-LIKE GAUSSIAN MEAN FIELD INFERENCE

The purpose of this section is to derive a capacity for flexible-scale Gaussian mean field inference, similar to subsection 3.1. We generalize this to the case when the KL term from the objective is scaled by a factor  > 0, as done by (Higgins et al., 2017). To illustrate the same capacity bound can be obtained for amortized inference, we derive this capacity for components Yi(n) of the latents in a -VAE, as this is where Gaussian mean field inference is deployed in this case. This is only done fur the purpose of familarity, the same bound would hold for when this inference approach would be applied to model parameters.
We assume Gaussian mean field inference on a latent with a unit Gaussian prior per dimension, as shown in Figure 8a. This analysis includes the standard VAE for  = 1 as a special case. The -VAE objective is (per datapoint, to be summed over all indices (n) which are omitted for readability):

F (q, ) = Eq(Y |X) log p(X|Y, ) -  DKL (q(Yi)||p(Yi))

(28)

i

We can write Y in its reparameterized form Y = µ +  , with µ and  being the mean and variance yielded deterministically by the inference network and i being noise sampled from N (0, 1). We can then write the objective as

F (µ, , ) = Ep(

) log p(X|Y

= µ + 

, ) +

 2

log i2 - i2 - µ2i - 1

i

(29)

As in subsection 3.1, we now define a new model featuring NIB that can be trained with a standard

variational objective (without need for a factor on the complexity term) so that it results in the same

training algorithm (data indices (n) are again omitted). The key idea is to reinterpret the means µi and variances i2 from the inference distribution as the latents of our new model, as shown in Figure 8b. We keep the deterministic part of the inference network as our new deterministic approximate posterior q (µi, i2|X) with point-mass at (µi , i2) per datapoint and dimension i,

resembling a MAP approach. We define the priors µi  N

0,

1 

and i2  

 2

+

1,

 2

and

then define Yi = µi + i i, where noise i is sampled from the same fixed q ( i) = p ( i) = p( i) =

12

Under review as a conference paper at ICLR 2019

 0.01 0.1 1 (VAE) 10 100

I(Yi, (µi, i2)) 0.68 bits 0.65 bits 0.45 bits 0.12 bits 0.014 bits

Table 1: Numeric results for latent capacities in reinterpreted -VAE given by Equations 31, 33 and 35, plotted in Figure 3.

N (0, 1). We leave the rest of the model and the inference network unchanged at p (X|Y, ) = p(X|Y, ) and q (µ, 2|X) = q(µ, 2|X), and as before, we perform ML learning on the model
parameters .

The MAP objective of our newly defined model then is (again per datapoint)

F (q , ) = Eq (µ,2|X)q ( ) log p (X|µ, , , ) + log p (µi ) + log p (i2)
i

(30)

Equation 29 now implies which is equivalent F (µ, , ) = F (µ, , ) + const., resulting in identical gradients. Because i is sampled from N (0, 1) in both our method and -VAE, the training algorithms are exactly equivalent.

We can now analyze the properties of this reinterpretation of -VAE. In particular, we are interested in

I(Yi, (µi, i2)) =H(Yi) - Ep(µi,i2)H(Yi|(µi, i2) = (µi, i2))

=H (Yi )

-

Ep(i2 )

1 2

log

2ei2

(31) (32)

where


H(Yi) = - dY p(Yi) log p(Yi)
0

(33)

Yi|µi, i2  N µi, i2 with µi  N

0,

1 

implies Yi|i2  N

0, i2

+

1 

. Therefore,



p(Yi) =

di2p(i2)p(Yi|i2)

0

=


di2 0

1
 2



 2

i2

e-i2

2

2

i2

+

1 

e ( )-

1 2

-1

2

i2

+

1 

Yi2

(34) (35)

Figure 3 shows numerical results of Equation 31 for various values of . Interestingly, the setting  > 1 that is suggested by Higgins et al. (2017) for obtaining disentangled representations corresponds to lower latent capacity.

13

