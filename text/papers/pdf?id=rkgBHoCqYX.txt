Under review as a conference paper at ICLR 2019

A KERNEL RANDOM MATRIX-BASED APPROACH FOR SPARSE PCA
Anonymous authors Paper under double-blind review

ABSTRACT
In this paper, we present a random matrix approach to recover sparse principal components from n p-dimensional vectors. Specifically, considering the large dimensional setting where n, p   with p/n  c  (0, ) and under Gaussian vector observations, we study kernel random matrices of the type f (C^ ), where f is a three-times continuously differentiable function applied entry-wise to the sample covariance matrix C^ of the data. Then, assuming that the principal components are sparse, we show that taking f in such a way that f (0) = f (0) = 0 allows for powerful recovery of the principal components, thereby generalizing previous ideas involving more specific f functions such as the soft-thresholding function.

1 INTRODUCTION

Principal component analysis (PCA) is extensively used in data analysis and machine learning applications. It is a dimension reduction technique that aims to project a given dataset onto principal subspaces spanned by the leading eigenvectors of the sample covariance matrix (Wold et al., 1987), which represent the principal modes of variance. Basically, the statistical interpretation of PCA lies in the fact that most of the variance in the data is captured by these modes. Consequently, PCA reduces the dimension of the feature space while keeping most of the information in the data. It is well-known (Anderson, 1963) that PCA performs efficiently in the traditional data setting where the number of features is small and the number of samples is large.

Consider a data matrix Y  Rp×n consisting of n centered samples, each sample having p features. The standard PCA method requires the computation of the sample covariance matrix C^ = Y Y /n

and estimates the first principal components u1, u2, . . . (i.e., the successive dominant eigenvectors of C = E[Y Y /n]) by the ordered eigenvectors u^1, u^2, . . . of C^ . (Johnstone & Lu, 2009) demonstrated that, in the high dimensional regime where n, p   with p/n  c > 0, the principal component

u^1 estimated by standard PCA is inconsistent. Essentially, if p/n 0 then u^1 - u1 2 0 in the high-dimensional asymptotic regime. This phenomenon is well investigated within the field

of random matrix theory for covariance positive semi-definite matrix and X is a

models of the p × n matrix

wfoirtmh rCa^n=domn1 i.1pi./d2.XenXtries1p./O2,nwehoefrethe pmiasina

results from random matrix theory concerns the so-called spiked models, where p is a low-rank

(tlophiBermietrah<tiruiokanrgnbfeodnatcnota|iamulto^.hln,ietm2uooe0ifas|0tttthr5>ihime)exa0aticrdnt.uoeedTdmenh(tpPmpiitrsrayiuiunpnnmlchc,iitaiap2ypsta0rae(li0lBxtc7r,ceo)aonnmnnmaasopmyipttocaieonohblnyen-lyGenpnteehtpu^xoeuihr=ngiiuobesm(Iisiitpne.e&egd+n.,osNatun^apainhdhkiud=aaaiasks1reuaddtttirPirautaCi0ni,cus)At2;ieit0dioio1swnrn1(ie;attpchhlCehmeknaeotnpofilsoiyttxthmaesmeiudenrurnewehcolehainyttnh:)aadtaarlt,.ssee,yisnpf2pmt/0ieonp0cintt9ot>;wtoiFciptcéah,,rlilanicnyfl,.

& Péché, 2007; Knowles & Yin, 2013).

The inconsistency of standard PCA in high dimensions motivated the idea to look for more structural information on the principal components. In particular, considering that the principal components are sparse in an appropriate basis (e.g., in the wavelet domain), a large body of works have emerged and proposed improved PCA approaches that account for sparsity. One of the most consistent sparse PCA methods in the literature is the covariance thresholding (CT) algorithm (Krauthgamer et al., 2015). Based on the intuition that the small entries of the empirical covariance matrix C^ induce noise in its principal components, this method consists in applying the popular soft-thresholding function

1

Under review as a conference paper at ICLR 2019

(with threshold  > 0); soft(· ;  ) : t  sign(t) · (|t| -  )+, entry-wise to the empirical covariance matrix C^ and performing PCA on the resulting matrix. (Deshpande & Montanari, 2014; 2016) have theoretically demonstrated that the covariance thresholding algorithm recovers the sought-for principal components with high probability under controlled growth rates between p, n and the sparsity level. In this paper, we show that the soft-thresholding method in fact falls within a broader class of kernel-based1 PCA algorithms that are particularly suited to sparse PCA recovery. This method consists in considering the matrix f (C^ ) instead of C^ where f is a function applied entry-wise. By imposing some constraints on f , most importantly that f (0) = f (0) = 0, sparse PCA can be performed with provably high accuracy for sufficiently large n, p.

The rest of the paper is organized as follows. In Section 2, we present the related work on sparse PCA. We recall some necessary concentration of measure tools and notions about sparse matrices in Section 3. Our main theoretical results are then provided in Section 4. Section 5 discusses the practical aspects and provides experimental results. Section 6 concludes the article.

Notation: In following, the notation [n] denotes the set {1, . . . , n}, a denotes the integer part

of a. Given a vector x  Rn, the 2-norm of x matrix M , Mij or [M ]ij denote the entry of the

is denoted matrix M

as x at line

2
i

= and

coin=lu1mxn2i

. j

Given an n × n . [M ]·,j denotes

the j-th column vector of M and [M ]i,· its i-th line vector. The Frobenius norm M F of the

matrix M is defined as

M

2 F

=

n i,j=1

Mi2j ,

and

the

operator

norm

M

op of M is defined as

M op = max x =1 M x . Finally, denotes the Hadamard product, with [M N ]ij = MijNij.

2 RELATED WORK
The problem of sparse PCA has been tackled with a large range of techniques. Mainly, three classes of approaches emerge in the literature. Most popular techniques are optimization-based algorithms (d'Aspremont et al., 2005; Moghaddam et al., 2006; Zass & Shashua, 2007; Zou et al., 2006; Wright et al., 2009), where the idea is to see the problem of sparse PCA through an optimization perspective, and to propose methods to solve the latter by either considering a different formulation ­ e.g. semi-definite programming (SDP) or convex relaxations ­ or adding penalties to the original optimization problem such as a LASSO regularization. The second class of approaches covers matrix decomposition-based techniques (Asteris et al., 2014; Papailiopoulos et al., 2013; Shen & Huang, 2008), where sparse principal components are extracted through solving a low rank matrix approximation problem based on Singular Value Decomposition. Finally, most consistent sparse PCA methods adopt thresholding-based approaches: initial heuristics used factor rotation techniques and thresholding of eigenvectors to obtain sparsity (Cadima & Jolliffe, 1995). Based on the well-known power method, (Yuan & Zhang, 2013) introduced an efficient sparse PCA approximation to obtain the exact level of required sparsity, by truncating to zero the principal components iteratively except for their largest entries. A step further, under a spiked covariance model (see Section 4.1), (Ma et al., 2013) proposed a very efficient iterative thresholding approach for estimating principal subspaces stihnpaatthr,seweshsppeanirksteeh;esienstptpianarrgsti.itcySuillmeavri,leatlhrsley,aausths(uomrsninp)gr,eaasessntiantnegddlaeer-mdsppSiikDreiPcmaaloprdpeersolu,al(tcKshsrcuaaugntghnegosattimrnegecrothevtaetar lfc.o,or2ns0si1=s5te)Onpt(rlyovtneh)de, recovery is possible by a simple covariance thresholding algorithm. More recently, (Deshpande & Montanari, 2016) analyzed and theoretically proved, under a spiked model, that indeed the covariance thresholding algorithm (Krauthgamer et al., 2015) succeeds with high probability under controlled growth rates between p, n and s. In this work, while restricting ourselves to a setting where p and n grow at a controlled joint rate, we provide an elementary argument, based on a matrix-wise Taylor expansion controlled through a concentration of measure approach, that generalizes the CT method to a large family of kernel-based methods, by means of a kernel random matrix approach (El Karoui, 2010b;a). Concretely, we study kernel random matrices of the form f (Y Y /n) where Y = p1/2X and X is a random matrix with N (0, 1) i.i.d. entries. (El Karoui, 2010b) studied kernel matrices of the form f (Y Y /n) (i.e., the socalled inner-product kernel matrices), which is equivalent to the case p = Ip when considering the
1We use the kernel-based terminology to highlight that our work falls within the framework of kernel random matrices and should not be confused with the standard kernel PCA.

2

Under review as a conference paper at ICLR 2019

form f (Y Y /n). In particular, we elaborate from El Karoui's study by Taylor expanding f (Y Y /n) in the vicinity of p entry-wise and controlling the resulting matrices via concentration arguments.

3 PRELIMINARIES

Before introducing our model setting we recall some definitions and notions of the concentration of measure theory (Ledoux, 2005) that are at the heart of our main results. Furthermore, we recall a definition, introduced by (El Karoui, 2008), of sparse matrices in the large-dimensional context that will also be exploited in this paper.

3.1 CONCENTRATION OF MEASURE RESULTS

We start by a definition of the notion of concentration for a real random variable.
Definition 1 (Concentration of a Random Variable). Given a function  : R+  R+, a random variable Z is said to be -concentrated (around its mean) and we write Z  , if for all t > 0, P {|Z - EZ|  t}  (t). In particular, Z is said to be normally (resp., exponentially) concentrated when (t) = Ce-c t2 (resp., (t) = Ce-c t) and we write Z  CN (c ·) (resp., Z  CE(c ·)), where C, c > 0 are some absolute constants.

In particular, -concentration remains stable by application of Lipschitz functions: Proposition 1 (Concentration of Lipschitz Functions). Given a -Lipschitz function f : R  R and a concentrated random variable Z  , we have f (Z)   (·/).

As a consequence, linear combinations of -concentrated random variables remain concentrated. However, products of -concentrated random variables are more technical to handle, but we still have the following proposition in the case of normally concentrated random variables and which will be essential in this article. Proposition 2 (Square of Normally Concentrated Random Variables). Given Z  CN (c ·), the random variable Z2 is exp-normally concentrated, precisely

Z2  KC E

c 2

·

+ KC N

c 16 E[Z]2

·

,

(1)

where KC > 0 is a constant depending only on C.

The extension of the notion of concentration to random vectors Z  Rp demands that Rp  R Lipschitz functions are concentrated random variables.
Definition 2 (Concentration of a Random Vector). Given a function  : R+  R+ and a normal space (E, . ), a random vector Z  E is said to be -concentrated if for any 1-Lipschitz function f : E  R, the random variable f (Z) is -concentrated. We note again Z  .

In particular, we have the concentration of Gaussian random vectors in the sense of Definition 2 in the following proposition (Tao, 2012, Theorem 2.1.12).
Proposition 3 (Normal Concentration of Gaussian Random Vectors). A Gaussian vector Z  Rp, with independent and identically distributed N (0, 1) entries, is normally concentrated independently on the dimension p. Furthermore, Z  2N (·/2).
Remark 1. According to Definition 2, given a Lipschitz application F : Rp  Rq for q  N, Proposition 3 provides the normal concentration of all the random vectors F (Z).

3.2 -SPARSE MATRICES

When considering a large-dimensional random matrix setting, the notion of sparsity for such matrices is particularly attached to the choice of the matrix norm.2 (El Karoui, 2008) introduced a definition (-sparsity) for sparsity of matrices that is compatible with spectral analysis, and specifically adapted to the operator norm. The -sparsity definition requires some notions from graph theory that we

2Considering the identity matrix (which is a sparse matrix),

Ip op = 1 while

Ip

F

=

 p



.

3

Under review as a conference paper at ICLR 2019

present in the following: to each p × p symmetric matrix M , we define its corresponding adjacency

matrix as A(M to be closed on

)= this

g{r1aMphiji=f 0it}sipt,ja=rt1s,awndhifichnicshoerrseastptohnedssatmoea

graph vertex

Gp with p vertices. A walk is said and the number of edges traversed

by a walk defines the length of this walk. Denote Cp(k) the set of closed walks of length k on the graph Gp.

Definition 3 (-sparse matrices (El Karoui, 2008, Definition 1)). A sequence of covariance matrices

{p}p=1 is said to be -sparse if the sequence of their associated graphs {Gp}p=1 satisfies, for all

k  2N

|Cp(k)|  Ck p(k-1)+1,

(2)

where   [0, 1], Ck > 0 independent of p and |S| denotes the cardinality of the set S.

The -sparsity is both useful and convenient to out study for the following reasons: 1) it is adapted to

the analysis of the operator norm of large sparse matrices (as we give concentration results on the

operator norm); 2) it is also more general than other sparsity notions such as in (Bickel & Levina,

2008). In the latter, the authors developed a natural permutation-invariant notion of sparsity which is

more specific than Definition 3 as pointed out in the introduction of their article. Furthermore, note

that both sparsity notions (Definition 3 and the one in (Bickel & Levina, 2008)) provide equivalent

bounds for  2.4 in (Bickel

< &

L21evainnda,w2h0e0n8)c);onthsiisdeisripnrgectihseellyartghee

dimensional p  setting considered

n in

setting (see Corollary 2

subsection introduced

subsequently (cf. µ > 0).

Remark 2. As Definition 3 is based on a graph defined by its corresponding adjacency matrix, we

have the following property: given an -sparse matrix M and a function f such that f (0) = 0

and f (x) =x=0 0, the matrix f (M ), resulting from the application of f entry-wise to M , remains -sparse; this is simply a consequence of A(M ) = A(f (M )).

4 MAIN RESULTS
In this section, we first present the setting of the article. Then, we provide an asymptotic equivalent to the matrix f (C^ ). Finally, we treat as a special case the application of our result to the context of sparse PCA.

4.1 GENERAL SETTING AND MAIN RESULTS

Consider a data matrix Y  Rp×n defined as Y  1p/2X = (Ip + P )1/2 X,

(3)

where X  Rp×n is a random matrix with i.i.d. N (0, 1) entries, P =

k i=1

i

uiui

and

U = [u1, . . . , uk]  Rp×k is isometric. Here, k refers to the number of principal components

(or eigenvectors) u1, . . . , uk  Rp to be evaluated, with 1 > . . . > k > 0 the corresponding

eigenvalues respectively. We define the quantity3 p  maxi [p1/2]·,i .

Assumptions: There exists B > 0 independent of p, n such that maxij |[p]ij| < B. Besides, there

exists

> 0 such that p  B

n

1 4

-

for all p, n and for some absolute constant B

> 0.

Under these assumptions, our main technical result is as follows: Theorem 1 (Asymptotic Equivalent). For f a three-times continuously differentiable function, define the matrices F and F~ respectively by

F f

1 YY
n ij

p

, F~  f (p) +

2

f (k)(p) k!

i,j=1

k=1

1p/2

1 XX
n

- Ip

1p/2

k
.

Then for  > 0 and for an absolute constant C > 0, we have with probability at least 1 - 

||F

-

F~||op



C

p6 n3/2

p 

.

(4)

3The role of p is to ensure the concentration of the quadratic form in equation 5 introduced subsequently. When p is a sparse matrix, p plays the same role as the maximum spike strength in the bounds given in (Deshpande & Montanari, 2016) for the CT method.

4

Under review as a conference paper at ICLR 2019

For a general smooth function f , the kernel random matrix f (C^ ) is particularly difficult to analyze through the usual tools of random matrix theory, such as the moment or Stieltjes transform-based methods (Tao, 2012). Rather than directly analyzing such a kernel random matrix, Theorem 1 gives an asymptotic equivalent to it, in operator norm, that has mainly two properties. First, the approximation matrix F~ contains "simple" objects that have already been analyzed in random matrix theory ­ in particular, the term (XX /n - Ip) in the expression of F~. Second, the approximation in operator norm implies (by Weyl's inequality (Eisenstat & Ipsen, 1998, Theorem 4.1)) that, when F -F~ op  0, F and F~ have the same eigenvalues and same "isolated" eigenvectors asymptotically (see Corollary 2 subsequently).

Sketch of Proof of Theorem 1. The main idea of the proof relies on the following intuition: for large n, the entries of XX /n - Ip and its successive Hadamard products tend to zero at controllable rate. The concentration of measure framework then allows for the control of non-linear functions of the entries of XX /n - Ip. Of utmost importance to this end is the following lemma.

Lemma 1 (A Concentration Result). For all i, j 

[p1/2]i,·

1 n

X

X

- Ip

[1p/2]·,j satisfies

gij(X)  KE

c1 n p2

·

+ KN

[p], the bilinear form gij(X) 

c2 n p4

·

,

(5)

for some absolute constants c1, c2, K > 0.

Proof. Denoting by vi the i-th column vector of the matrix 1p/2, we have by the polarization identity,

for all M

Hermitian, vi M vj

=

1 4

[(vi

+

vj

)

M (vi + vj) - (vi - vj)

M (vi - vj)]. It thus suffices

to prove the result for the quadratic form g(X) = v

1 n

X

X

- Ip

v where v  Rp. Noticing that

v XX

v=

v X 2 and E

1 n

v

XX

v

= v v, we need to prove the concentration of the random

variable v X 2. In fact, since v X is a Gaussian vector, by Proposition 3, v X

 2N

· 2v2

by Remark 1 and by Definition 2 since M  v M and u  u are respectively v -Lipschitz and

1-Lipschitz functions. We get the final result by Proposition 2.

A Taylor expansion of F around f (p) then leads to controlling the operator norm of f (3)(n) [p1/2(XX /n - Ip)1p/2] 3 for n a matrix with entries in the set [[Y Y /n]ij, [p]ij] (or [[p]ij, [Y Y /n]ij]). This follows precisely from exploiting Lemma 1 twice, to control the fluctuations of the entries of both n (by the conditions on maxij |[p]ij| and p) and [1p/2(XX /n - Ip)p1/2] 3, with the bound provided in the theorem statement, thereby completing the proof.

A detailed proof of Theorem 1 is provided in Section A.2 of the Appendix. From now on, to simplify our arguments, we make the following assumptions: Assumptions: As n  ,

A1 p/n  c  (0, ),

A2 lim supn maxi i < ; specifically lim supn p < .

Under this setting, we have the following important corollary to Theorem 1.

Corollary 1. Define the matrices F and F~ as in Theorem 1 and Assumptions A1 and A2 hold. Then,

for  > 0

F

=

F~

+

O

(n-

1 2

),

(6)

where the notation X = Om(n-) stands for the fact that P

X

op



C

n-

-

1 2m

  for

some absolute constant C > 0 and non-negative integer m.

As a consequence of Corollary 1, we have, by the sin() theorem of (Davis & Kahan, 1970), the corollary below concerning the eigenvectors of the matrices F and F~.

Corollary 2. Let v1, . . . , vk and v~1, . . . , v~k denote respectively the k principal eigenvectors of F and F~. Denote by i = i - i+1 for i  [k - 1]. Then for  > 0, we have

max
i[k]

min
s{+1,-1}

i2

vi - sv~i 2 = O(n-1).

(7)

5

Under review as a conference paper at ICLR 2019

4.2 SPECIAL CASE: SPARSE PCA

To get an insight on our coming results, consider the scenario where U contains finitely many

non-zero entries. In this case, the perturbation matrix P in Eq. equation 12 contains finitely many

non-zero entries (say s) on each line and a simple enumeration shows that |Cp(k)|  p sk-1, thus P

is 0-sparse in the sense of Definition 3. Similarly, Ip is 0-sparse and by the additive stability4 of the

-sparsity notion, p remains that the population covariance

0-sparse. matrix p

More generally, if we assume that is -sparse, we have the following

siet teoxfisctsonsequ[0e,n12c)ess.uBchy

Corollary 1, choosing f in such a way that f (0) = f (0) = 0 ensures that the terms f (p) . . .

and f (p) . . . vanish in the expression of F~. Indeed, for k  {1, 2}

(i) Only finitely entries of f (k)(p) do not vanish, precisely by Remark 2, since A(f (k)(p)) = A(p),5 the matrix f (k)(p) is also (almost) -sparse.

(ii) The matrix F (k) =

p1/2

1 n

X

X

- Ip

p1/2

k has entries of order O(n-k/2). As a

result,6 we have for  > 0 and for all m > 0, maxi,j |Fi(jk)| = Om

n-

k 2

+

1 m

.

Since in addition the operator norm of p1/2(XX /n - Ip)p1/2 is typically of order O(1) (see e.g., (Bai & Silverstein, 1998)), it is then easily seen that, for each k  1, the operator norm of the Hadamard product f (k)(p) F (k) vanishes (see Lemma 5 in Appendix A). In particular, note that the non-zero entries of p are controlled through the maximum entry of F (k) which is vanishing asymptotically, as mentioned in item (ii) above. On the opposite f (p) does not vanish since it has entries bounded away from zero (as long of course as f = 0). We precisely have the following result.

Theorem 2.

Let µ

> 0 and suppose p

is a

1 2+µ

-sparse

matrix.

For f

a three-times continuously

differentiable function and for  > 0, we have for all



(0,

µ 2(3+2µ)

)

F = f (p) + O1/

n ( )-µ 2(2+µ)

+

2-

1 2+µ

s.t. f (0) = f (0) = 0.

(8)

Proof. See Section A.3 in Appendix A.

Remark 3. Theorem 2 gives a general result concerning the estimation of -sparse covariance

matrices (more precisely, element-wise functionals of sparse covariance matrices). In particular, the

spiked model in Eq. equation 12 with U sparse corresponds to the particular case when µ  ; in

this case, for  > 0 and for all

 (0, 1/4), F = f (p) + O1/

(n-

1 2

+2

).

One may then perform a PCA on F for some function f with f (0) = f (0) = 0 (we denote by f 1,2(0) = 0 these two conditions in the following). But, while p = Ip +P is a low rank perturbation of the identity (therefore having only k eigenvalues strictly greater than 1), f (p) is likely more complex and not a mere low rank deformation of the identity. Now, if p has all its non-zero entries greater than a certain threshold  , an appropriate choice for f that avoids the deformation of Ip + P is such that f 1,2(0) = 0 and f (t) = t for all |t| >  .

Such a convenient choice is

f (t) = t(1 - e-at2 ),

(9)

for some a > 0. This function notably satisfies

2 0

f (t) = 1 + e-at2 (2at2 - 1)  f (0) = 0, f (t) = -2ate-at2 2at2 - 3  f (0) = 0.

-2 -2

0

t f (t) f (t) f (t)
2

The figure above depicts the function f along with its derivatives for a = 1. Note that a compromise in the choice of a must be made that both maintains a close approximation of the identity by f on

4See Fact .1 in (El Karoui, 2008).

5Given M 6See proof

 Rp×p, its corresponding adjacency matrix is defined of Lemma 5 in Appendix A for a proof of this result.

as

A(M

)

=

1{ Mij =0}ip,j=1.

6

Under review as a conference paper at ICLR 2019

Density

1
0.5
00 1
0.5

Spectr. of C^
gap
1234
Spectr. of f (C^)

00 1 2 3 4 Eigenvalues

|u1 ^u1|

1 0.5 PCA

f -PCA (ours)

0 Phase Trans. (1=5) 135
p/n

7

9

Density

Figure 1: (Left) Spectrum of C^ (up) and f (C^ ) (bottom) for p = 2048 and n = 7500. Limiting Marcenko-Pastur density (Marcenko & Pastur, 1967) in blue versus spectrum of p in black, with 1 = 2; estimated largest eigenvalue in red. (Right) Alignment between estimated PC and GT (the "Three Peak" example of (Johnstone & Lu, 2009) in the "Symmlet 8" wavelet basis), in terms of p/n. We considered 1 = 5 and thus one observes a phase transition for standard PCA at p/n = 5. Curves obtained from 500 realizations of X.

a large range and rather small values of f in the vicinity of zero. Interestingly, it can be verified

that the extrema of f are independent of a but are found at ±

sharper f in the vicinity of zero. Similarly, the extrema of

precisely given by the four values 2

.3a(3

±

6)e-

1 2

 (3± 6)



 a

3 2a
f

and are

thus smaller found at ±

values of a create

, and
3± 6 2a

  1/ a

Thus smaller a induce larger maxima

for f but no sharper slope.

5 EXPERIMENTS

In this section, we provide some experiments in the context of sparse PCA, where we consider the

spiked model presented in Section 4.1. Precise setting given in caption of Figure 1. The spectrum

of the sample covariance matrix (in gray) is quite different from that of p. One instead observes a "bulk" of eigenvalues spread in the vicinity of 1. Furthermore, one observes a gap between the

true spike and the estimated spike (in red) through the sample covariance matrix. This phenomenon

is well-understood in random matrix theory. In particular, the extreme eigenvalue in our setting

converges almost surely to the quantity (1 + 1)

1+

c 1

, where we recall that c = limn p/n.

However, thanks to sparsity, the spectrum of F = f (C^ ) closely matches that of p, as suggested by Theorem 2. In particular, the extreme eigenvalue, which corresponds to the principal component, is consistently estimated. Figure 1 (right) depicts the alignment between the estimated principal component and ground truth, by standard PCA (in blue) and our method (in black), in terms of p/n. Our method retrieves the principal component even when the spike is not visible in the spectrum of C^ ; namely beyond the phase transition p/n  1. In fact, the standard PCA result is too noisy compared with the one when considering f (C^ ), as depicted in Figure 2. Further detailed examples are provided in Section A.4 of Appendix A, that confirm the consistency of the proposed method.

In terms of complexity, as our method consists in computing the sparse eigenvectors of a p × p

matrix which can be done by power method, the complexity of estimating the principal component

is about O(ps) where s is the sparsity level. And regarding the performance w.r.t. state-of-the-

art methods, Figure 3 depicts the performances of standard PCA, different state-of-the-art sparse

PCA methods and our method, in terms of total projections score (left) and total projections error

(right), for different values of the amplitudes i's. We refer, in this figure, to standard PCA as PCA, TpowPCA for the method in (Yuan & Zhang, 2013), ITSPCA for the method in (Ma et al., 2013),

CT refers to the method in (Deshpande & Montanari, 2016) and finally we refer to our method as

f -PCA.

The total

projections

score

S

and

error

E

are

given

respectively

by

S

=

1 k

k i=1

(ui

u^i

)2

and E = UU - U^ U^ F , where U = [u1, . . . , uk] are the ground truth principal components and

U^ = [u^1, . . . , u^k] are the estimated ones.

7

Under review as a conference paper at ICLR 2019

n = 128

n = 256

n = 1024

0 1,000 2,000 0 1,000 2,000 0 1,000 2,000

f -PCA (ours) PCA

0 1,000 2,000 0 1,000 2,000 0 1,000 2,000

Figure 2: Principal component recovery (in orange) by standard PCA (up) and our method (down) for the "Three Peak" example of (Johnstone & Lu, 2009). The signal is sparse in the "Symmlet 8" wavelet basis. We use p = 2048, 1 = 5 for the strength of the spike and different values of n.

As suggested theoretically and verified experimentally, our proposed method strongly attenuates the "noise component" of the sample covariance matrix and thus consistently estimates the principal components. In particular, in term of total projections score, PCA is the most inconsistent. In general, ITSPCA, CT and our method give equivalent results. The same holds when considering the total projections error as a metric, except that TpowPCA performs inconsistently, compared to PCA, for small values of amplitudes due to the initialization step from the PCA eigenvectors.

The mostly used concurrent methods to PCA in a sparse context are iterative truncated power methods

(such as the TPower (Yuan & Zhang, 2013) algorithm or the ITSPCA (Ma et al., 2013) approach).

These algorithms, despite great observed performances, as compared to standard PCA, suffer from

two limitations. First, they are usually initialized from the PCA eigenvectors themselves and may

not converge to good estimates. For weak signals, PCA is so impacted by noise that the mentioned

initialization limitation may lead to non convergent or dramatically erroneous outcomes of the

method. The proposed approach deals precisely with this limitation by strongly attenuating the "noise

component" of the sample covariance matrix. In particular, our approach gives equivalent results to

the CT method while generalizing it to the class of smooth functions f such that f (0) = f (0) = 0,

in the considered regime. The second limitation concerns the choice of the hyper-parameters; in fact,

TPower and ITSPCA need to set up an arbitrary deterministic threshold value that maintains at each

iteration step only most powerful components. The proposed method as well as CT need also to set

up a "soft" parameter (a and  respectively). But, on the basis of (Cheng & Singer, 2013; Kammoun

& Couillet, 2017), we believe non-trivial setting where i =

tOha(t1/ourpp)r(eisnenwthiincvhecstaisgeattihoendcoamnibneanetxetiegnednemdotdoetshescaasleymatpatostiimcaillalyr

rate with residual noise); this setting may likely allow to exhibit and estimate optimal hyper-parameter

choices. Notably, this setting has already been used in (Tiomoko Ali et al., 2018) in a different

context, for hyper-parameters estimation.

Score Error

1 0.95
0.9 0.85
20 40 60 80 100 Mean of the amplitudes i's

100
10-1
10-2 20 40 60 80 100 Mean of the amplitudes i's

PCA TpowPCA ITSPCA
CT f -PCA (ours)

Figure 3: Performances of standard PCA, different state-of-the-art sparse PCA methods and our method in term of total projections score (left) and total projections error (right) for different values of the amplitudes i. The PCs ui, for i  [4] are the "Three Peak", "Piece Poly", "Step New" and "Sing" signals of (Johnstone & Lu, 2009). We use p = 2048 and n = 1024. The soft-parameters a and  (respectively for our method and CT) are selected by cross-validation using a validation set of size n. The selected parameters are a = 20 and  = 0.1.

8

Under review as a conference paper at ICLR 2019
6 CONCLUSION
In this paper, we tackled the problem of sparse PCA through a random matrix perspective thereby generalizing recent ideas to a broader kernel-based method. Our analysis of this problem has yielded insights into how the principal components can be consistently estimated. Namely, given a spiked covariance model C^ and a smooth function f , we gave in this paper sufficient conditions on f to consistently estimate the principal components through the matrix f (C^ ). Our methodology can be generalized to other sparse covariance matrix-based contexts, in the same vein as the works in (Bickel & Levina, 2008; El Karoui, 2008).
REFERENCES
Theodore Wilbur Anderson. Asymptotic theory for principal component analysis. The Annals of Mathematical Statistics, 34(1):122­148, 1963.
Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis. Nonnegative sparse pca with provable guarantees. In International Conference on Machine Learning, pp. 1728­1736, 2014.
Zhi-Dong Bai and Jack W Silverstein. No eigenvalues outside the support of the limiting spectral distribution of large-dimensional sample covariance matrices. Annals of probability, pp. 316­345, 1998.
Jinho Baik, Gérard Ben Arous, Sandrine Péché, et al. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. The Annals of Probability, 33(5):1643­1697, 2005.
Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices. Advances in Mathematics, 227(1):494­521, 2011.
Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of Statistics, pp. 2577­2604, 2008.
Jorge Cadima and Ian T Jolliffe. Loading and correlations in the interpretation of principle compenents. Journal of Applied Statistics, 22(2):203­214, 1995.
Mireille Capitaine, Catherine Donati-Martin, and Delphine Féral. The largest eigenvalues of finite rank deformation of large wigner matrices: convergence and nonuniversality of the fluctuations. The Annals of Probability, pp. 1­47, 2009.
Xiuyuan Cheng and Amit Singer. The spectrum of random inner-product kernel matrices. Random Matrices: Theory and Applications, 2(04):1350010, 2013.
Alexandre d'Aspremont, Laurent E Ghaoui, Michael I Jordan, and Gert R Lanckriet. A direct formulation for sparse pca using semidefinite programming. In Advances in neural information processing systems, pp. 41­48, 2005.
Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1­46, 1970.
Yash Deshpande and Andrea Montanari. Sparse pca via covariance thresholding. In Advances in Neural Information Processing Systems, pp. 334­342, 2014.
Yash Deshpande and Andrea Montanari. Sparse pca via covariance thresholding. J. Mach. Learn. Res., 17(1):4913­4953, January 2016. ISSN 1532-4435.
Stanley C Eisenstat and Ilse CF Ipsen. Three absolute perturbation bounds for matrix eigenvalues imply relative bounds. SIAM Journal on Matrix Analysis and Applications, 20(1):149­158, 1998.
Noureddine El Karoui. Operator norm consistent estimation of large-dimensional sparse covariance matrices. The Annals of Statistics, pp. 2717­2756, 2008.
Noureddine El Karoui. On information plus noise kernel random matrices. The Annals of Statistics, 38(5):3191­3216, 2010a.
9

Under review as a conference paper at ICLR 2019
Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1): 1­50, 2010b.
Delphine Féral and Sandrine Péché. The largest eigenvalue of rank one deformation of large wigner matrices. Communications in mathematical physics, 272(1):185­228, 2007.
Iain M Johnstone and Arthur Yu Lu. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association, 104(486):682­693, 2009.
Abla J Kammoun and Romain Couillet. Subspace kernel clustering of large dimensional data. (submitted to) Annals of Applied Probability, 2017.
Antti Knowles and Jun Yin. The isotropic semicircle law and deformation of wigner matrices. Communications on Pure and Applied Mathematics, 66(11):1663­1749, 2013.
Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik. Do semidefinite relaxations solve sparse pca up to the information limit? Ann. Statist., 43(3):1300­1322, 06 2015.
Michel Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical Soc., 2005.
Zongming Ma et al. Sparse principal component analysis and iterative thresholding. The Annals of Statistics, 41(2):772­801, 2013.
Vladimir A Marcenko and Leonid Andreevich Pastur. Distribution of eigenvalues for some sets of random matrices. Mathematics of the USSR-Sbornik, 1(4):457, 1967.
Baback Moghaddam, Yair Weiss, and Shai Avidan. Spectral bounds for sparse pca: Exact and greedy algorithms. In Advances in neural information processing systems, pp. 915­922, 2006.
Dimitris Papailiopoulos, Alexandros Dimakis, and Stavros Korokythakis. Sparse pca through lowrank approximations. In International Conference on Machine Learning, pp. 747­755, 2013.
Debashis Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance model. Statistica Sinica, 17:1617­1642, 2007.
Haipeng Shen and Jianhua Z Huang. Sparse principal component analysis via regularized low rank matrix approximation. Journal of multivariate analysis, 99(6):1015­1034, 2008.
Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Society Providence, RI, 2012.
Hafis Tiomoko Ali, Abla Kammoun, and Romain Couillet. Random matrix asymptotics of inner product spectral clustering. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.
Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3):37­52, 1987.
John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization. In Advances in neural information processing systems, pp. 2080­2088, 2009.
Xiao-Tong Yuan and Tong Zhang. Truncated power method for sparse eigenvalue problems. Journal of Machine Learning Research, 14(Apr):899­925, 2013.
Ron Zass and Amnon Shashua. Nonnegative sparse pca. In Advances in Neural Information Processing Systems, pp. 1561­1568, 2007.
Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics, 15(2):265­286, 2006.
10

Under review as a conference paper at ICLR 2019

A PROOFS AND FURTHER EXPERIMENTS
In this appendix we provide the proofs of the different results presented in the paper and some additional experiments that validate our findings. It includes the following items: (i) the proof of Theorem 1 (Section A.2); (ii) The proof of Theorem 2 concerning the analysis of the sparse case (Section A.3); and finally (iii) further experiments which confirm the consistency of our method and the necessity of the conditions f (0) = f (0) = 0 on the kernel function f , using the signals of Johnstone et al. (Johnstone & Lu, 2009) (Section A.4). For convenience, we make the present appendix self-contained by recalling the preliminaries and the results presented in the main paper.

A.1 PRELIMINARIES

Proposition 2 (Square of Normally Concentrated Random Variables). Given Z  CN (c ·), the random variable Z2 is exp-normally concentrated, precisely

Z2  KC E

c 2

·

+ KC N

c 16 E[Z]2

·

,

(10)

where KC > 0 is a constant depending only on C.
Definition 2 (Concentration of a Random Vector). Given a function  : R+  R+ and a normal space (E, . ), a random vector Z  E is said to be -concentrated if for any 1-Lipschitz function f : E  R, the random variable f (Z) is -concentrated. We note again Z  .
Proposition 3 (Normal Concentration of Gaussian Random Vectors (Tao, 2012, Theorem 2.1.12)). A Gaussian vector Z  Rp, with independent and identically distributed N (0, 1) entries, is normally concentrated independently on the dimension p. Furthermore, Z  2N (·/2).
Remark 1. According to Definition 2, given a Lipschitz application F : Rp  Rq for q  N, Theorem 3 provides the normal concentration of all the random vectors F (Z).
Definition 3 (-sparse matrices (El Karoui, 2008, Definition 1)). A sequence of covariance matrices {p}p=1 is said to be -sparse if the sequence of their associated graphs7 {Gp}p=1 satisfies, for all k  2N

|Cp(k)|  Ck p(k-1)+1,

(11)

where   [0, 1], Ck > 0 independent of p and |S| denotes the cardinality of the set S.
Remark 2. As Definition 3 is based on a graph defined by its corresponding adjacency matrix, we have the following property: given an -sparse matrix M and a function f such that f (0) = 0 and f (x) =x=0 0, the matrix f (M ), resulting from the application of f entry-wise to M , remains -sparse; this is simply a consequence of A(M ) = A(f (M )).1

A.2 PROOF OF THEOREM 1

Setting: Consider a data matrix Y  Rp×n defined as

Y  p1/2X = (Ip + P )1/2 X,

(12)

where X  is isometric,

Rp×n with k refers to

i.i.d. N (0, 1) entries, P the number of principal

c=ompoki=ne1ntisuaiundi

,

U 1

= >

[u1, . . . , uk]  Rp×k . . . > k > 0. Let

p  maxi [1p/2]·,i .

Assumptions: There exists B > 0 independent of p, n such that maxij |[p]ij| < B. Besides, there

exists

> 0 such that p  B

n

1 4

-

for all p, n and for some absolute constant B

> 0.

Under these assumptions, we have

7Defined through the corresponding adjacency matrix to p; given an p × p real symmetric matrix M , its
corresponding adjacency matrix is defined as A(M ) = 1{ Mij=0}pi,j=1.

11

Under review as a conference paper at ICLR 2019

Theorem 1. For f a three-times continuously differentiable function, define the matrices F and F~ respectively by

F f 1YY n

=f

1 YY
n ij

p i,j=1

F~  f (p) +

2

f (k)(p) k!

k=1

1p/2

1 XX
n

- Ip

1p/2

k
.

Then for  > 0 and for an absolute constant C > 0, we have with probability at least 1 - 

||F

-

F~||op



C

p6 n3/2

p 

.

(13)

Proof. Before starting the proof, we need to introduce the following key lemmas:

Lemma 1 (A Concentration Result). For all i, j  [p], the bilinear form gij(X) 

[p1/2]i,·

1 n

X

X

- Ip

[1p/2]·,j satisfies

gij(X)  KE

c1 n p2

·

+ KN

c2 n p4

·

,

(14)

for some absolute constants c1, c2, K > 0.

Proof. Denoting by vi the i-th column vector of the matrix p1/2, we have by the polarization identity,

for all M

Hermitian, vi M vj

=

1 4

[(vi

+

vj

)

M (vi + vj) - (vi - vj)

M (vi - vj)]. It thus suffices

to prove the result for the quadratic form g(X) = v

1 n

X

X

- Ip

v where v  Rp. Noticing that

v XX

v=

v X 2 and E

1 n

v

XX

v

= v v, we need to prove the concentration of the random

variable v X 2. In fact, since v X is a Gaussian vector, by Proposition 3, v X

 2N

· 2v2

by Remark 1 and by Definition 2 since u  u and M  v M are respectively 1-Lipschitz and

v -Lipschitz functions. We get the final result by Proposition 2.

Lemma 2 k  N and

(A for

Moment Result). For gij(X)  [p1/2]i,· some absolute constant Ck > 0,
E|gij (X)|2k  Ck

1 n

X

p4k nk

.

X

- Ip

[1p/2]·,j, we have, for all (15)

Proof. Given a random variable Z, we have



m > 0, E|Z|m =

m tm-1P{|Z|  t}dt,

0

whenever the right hand side is finite. Applying this identity to the random variable gij(X) with m = 2k and exploiting the concentration property in Lemma 1 yields the result.

The proof starts by a Taylor expansion of Fij in the vicinity of [p]ij, i.e.,

Fij =

2

f

(k)(ij k!

)

Fi(jk)

+

f

(3)(inj 6

)

Fi(j3)

k=0

where ij = [p]ij, inj  [Y Y /n]ij, ij ,8 and F (k) is the matrix with entries

Fi(jk)  [1p/2(n-1XX - Ip)p1/2]kij = gij (X)k. We have by Lemma 1 that [Y Y /n]ij concentrates around ij, so that inj is bounded by ij + , for all  > 0, with high probability9 (note that the condition maxij |ij| < B ensures that ij is bounded

8The notation a, b stands for the interval [a, b] if a < b or [b, a] otherwise. 9For a given asymptotic variable n, we say that an event En occurs with high probability when it exist a function (n) quasi-exponentially decreasing in n such that P{En}  1 - (n).

12

Under review as a conference paper at ICLR 2019

and the condition on p ensures the quasi-exponential concentration of [Y Y /n]ij around ij; see considered Assumptions above), formally

P

|inj |  ij + 



P {|gij(X)|



}



K

-
e

n p2

min(c1

,

c2 2 p2

)

 Ke-K

n

1 2

+2

min(c1 ,K

c2

2

n-

1 2

+2

)



pn



0,

where K > 0. And since f (3) is continuous, we deduce that f (3)(inj) is in particular bounded by

A  max |f (3)(x)|,
x[ij -,ij +]
with probability 1 - pn. Knowing that the operator norm is bounded by the Frobenius norm, we look for a control of the Frobenius norm of the tailing term. We have

By Lemma 2, for all k  N

f (3)(n)

F (3)

2 F

 A2

F (3)

2 F

.

E

F (k)

2 F

=

p

E |gij (X)|2k



Ck

p2p4k nk

,

i,j=1

for some absolute constant Ck > 0. Thus, by Markov's inequality, we have for all  > 0

(16)

P

F (k)

F



p p2k nk
2

Ck 

 .

Recalling Eq. equation 16, we have with probability at least 1 - 

A.3 PROOF OF THEOREM 2

f (3)(n)

F (3)

F



C

p n3
2

p6

.

Assumptions: As n  , A1 p/n  c  (0, ). A2 lim supn maxi i < ; specifically lim supn p < .

With these assumptions, we have the following corollary to Theorem 1.

Corollary 1. Define the matrices F and F~ as in Theorem 1 and let Assumptions A1 and A2 hold.

Then, for  > 0

F

=

F~

+

O

(n-

1 2

),

(17)

where the notation X = Om(n-) stands for the fact that P

X

op



C

n-

-

1 2m

  for

some absolute constant C > 0 and non-negative integer m.

Theorem 2.

Let µ

> 0 and suppose p

is a

1 2+µ

-sparse

matrix.

For f

a three-times continuously

differentiable function and for  > 0, we have for all



(0,

µ 2(3+2µ)

)

F = f (p) + O1/

n ( )-µ 2(2+µ)

+

2-

1 2+µ

s.t. f (0) = f (0) = 0.

(18)

Proof. The proof needs the introduction of the following two lemmas, that can be found in (El Karoui, 2008, Lemma A.1 and A.2) and which are a consequence of the -sparsity notion10

Lemma 3. Given an -sparse p×p real symmetric matrix M and calling m = maxij |Mij|, we have,

for all k  2N

M op  trace(M k)1/k = O(m p(1-1/k)+1/k).

(19)

Lemma 4. Given two real symmetric matrices M and N with |Mij|  Nij. Then, we have M op  N op.

10Through the identity trace(M k)  maxij |Mij |k · |Cp(k)|.

13

Under review as a conference paper at ICLR 2019

First, we show that when p is -sparse, the Hadamard product f (k)(p) F (k) is of vanishing operator norm for k  1, precisely

Lemma 5.

Let µ > 0, suppose p is a

1 2+µ

-sparse

matrix.

For

f

a real and differentiable function,

k  {1, 2} such that f (k)(0) = 0 and for  > 0, we have for all



(0,

k(2+µ)-2 2(3+2µ)

)

f (k)(p) F (k) op = O1/

n ( ) .2-k(2+µ) 2(2+µ)

+

2-

1 2+µ

Proof. We start by proving that the matrix F (k) has entries of order O(n-k/2). In fact, we have by Lemma 2, for all m  N

E|Fi(jk)|2m = E|gij (X)|2km = O(n-km),

thus applying Markov's inequality to the random variable |Fi(jk)|2m yields to the following tail control.

P{|Fi(jk)|  t} 

E|Fi(jk)|2m t2m

 C n-km t-2m,

where C is an absolute constant. Recalling Assumption A1 and by the union bound, we have

p

P{max
ij

|Fi(jk)|



t}



P{|Fi(jk)|  t}  p2 P{|Fi(jk)|  t}  C n2-km t-2m,

i,j=1

which implies for  > 0 and for all m > 0

max
ij

|Fi(jk)|

=

Om

n-

k 2

+

1 m

(20)

Besides, let M be the matrix defined as M  maxij |Fi(jk)| · f (k)(p), we have

thus, one has by Lemma 4

|[f (k)(p) F (k)]ij |  Mij ,

f (k)(p)

F (k) op 

M

op

=

max
ij

|Fi(jk)

|

·

f (k)(p) op.

In

particular,

since

f (k)(p)

is

1 2+µ

-sparse

(by

Remark

2),

we

have

by

Lemma

3

and

by

equation

20,

for some  > 0

f (k)(p)

F = O n ,(k)
op

2m 

1 2+µ

(1-

1 2m

)+

1 2m

-

k 2

+

1 2m

choosing

=

1 2m

<

k(2+µ)-2 2(3+2µ)

yields the final result.

When considering f such that f (0) = f (0) = 0, the result holds by Corollary 1 and Lemma 5. In fact, the dominant order corresponds to k = 1 in Lemma 5. Which completes the proof.

14

PC3 PC2 PC1

Under review as a conference paper at ICLR 2019 A.4 FURTHER EXPERIMENTS A.4.1 HIGHER RANK CASE In this section, we provide further experiments by considering a rank three case and by using the "Three Peak", "Piece Poly" and "Step New" signals of Johnstone et al. (Johnstone & Lu, 2009), in the "Symmlet 8" wavelet basis, as principal components. We compare the estimated PCs by our method with the kernel function in equation 9 to the estimated ones through standard PCA and the CT method (Deshpande & Montanari, 2016). As shown in Figure 4, the proposed method retrieves consistently the principal components compared to a standard PCA. In particular, we obtain results that are similar to the ones obtained by the CT method while generalizing it to the class of smooth functions with f (0) = f (0) = 0.
PCA CT f -PCA (ours)
0 1,000 2,000 0 1,000 2,000 0 1,000 2,000
0 1,000 2,000 0 1,000 2,000 0 1,000 2,000
0 1,000 2,000 0 1,000 2,000 0 1,000 2,000 Figure 4: Multiple principal components (k = 3) recovery (in orange) with standard PCA (left), the CT method (middle) and our method (right) where the PCs are considered to be the "Three Peak", "Piece Poly" and "Step New" signals of (Johnstone & Lu, 2009), in the "Symmlet 8" wavelet basis. We use p = 2048, n = 1024 and the spikes strengths are set respectively as 1 = 100, 2 = 75 and 3 = 50. In particular, we note the similarity between the results obtained by our method and CT.
15

Under review as a conference paper at ICLR 2019

A.4.2 OTHER CHOICES OF THE KERNEL FUNCTION f In this section, we consider functions of the form f (t) = t3 + t2 + t where , ,   R are some parameters to fix in order to allow or not the conditions f (0) = f (0) = 0. In particular, we set different parameters choices for ,  and  in order to validate these conditions. Figure 5 depicts different PC recovery using the f -PCA method with the considered class of functions. As we can observe from this figure, the "cleanest" signal recovery is obtained when  = 0,  = 0,  = 0 (i.e., when f (0) = f (0) = 0) thereby validating our theoretical conditions on the kernel function f for a consistent sparse PCA recovery. Note that these conditions are necessary but not sufficient in the sense that f has to be linear for large values of t (In particular, this is the case for the function f given by equation 9). In fact, the outcome provided by f -PCA for f (t) = t3 with  = 0 is not optimal as the obtained signal is deformed (due to the unverified linearity condition), compared to the GT one.

 = 0,  = 0,  = 0

 = 0,  = 0,  = 0

 = 0,  = 0,  = 0

0 1,000 2,000  = 0,  = 0,  = 0

0 1,000 2,000  = 0,  = 0,  = 0

0 1,000 2,000  = 0,  = 0,  = 0

0 1,000 2,000 0 1,000 2,000 0 1,000 2,000 Figure 5: PC recovery (in orange) by f -PCA with the function f (t) = t3 + t2 + t for different values of the parameters (, , )  R3. We consider the "Three Peak" example of (Johnstone & Lu, 2009) which is sparse in the "Symmlet 8" wavelet basis. We use p = 2048, n = 256 and 1 = 5. In particular, we notice that the "cleanest" signal is obtained when  = 0,  = 0,  = 0 which validate our theoretical conditions f (0) = f (0) = 0.

16

