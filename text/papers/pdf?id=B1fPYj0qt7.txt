Under review as a conference paper at ICLR 2019

RIEMANNIAN STOCHASTIC GRADIENT DESCENT FOR TENSOR-TRAIN RECURRENT NEURAL NETWORKS
Anonymous authors Paper under double-blind review

ABSTRACT
The Tensor-Train factorization (TTF) is an efficient way to compress large weight matrices of fully-connected layers and recurrent layers in recurrent neural networks (RNNs). However, high Tensor-Train ranks for all the core tensors of parameters need to be element-wise fixed, which results in an unnecessary redundancy of model parameters. This work applies Riemannian stochastic gradient descent (RSGD) to train core tensors of parameters in the Riemannian Manifold before finding vectors of lower Tensor-Train ranks for parameters. The paper first presents the RSGD algorithm with a convergence analysis and then tests it on more advanced Tensor-Train RNNs such as bi-directional GRU/LSTM and Encoder-Decoder RNNs with a Tensor-Train attention model. The experiments on digit recognition and machine translation tasks suggest the effectiveness of the RSGD algorithm for Tensor-Train RNNs.

1 INTRODUCTION

Recurrent Neural Networks (RNNs) are typically composed of large weight matrices of fullyconnected and recurrent layers, thus massive training data as well as exhaustive computational resources are required. The Tensor-Train factorization (TTF) aims to reduce the redundancy of RNN parameters by reshaping large weight matrices into high-dimensional tensors before factorizing them in a Tensor-Train format Oseledets (2011). The notation of Tensor-Train usually suggests that TTF is applied for the tensor representation of model parameters. Tensor-Train was initially applied to fully-connected layers Novikov et al. (2015), and it has been recently generalized to recurrent layers in RNNs such as LSTM and GRU Yu et al. (2017). Compared with other tensor decomposition techniques like the CANDECOMP/PARAFAC decomposition Kolda & Bader (2009) and Tucker decomposition Kim & Choi (2007), Tensor-Train can be easily scaled to arbitrarily high dimensions and have the advantage of computational tractability to significantly large weight matrices.
Given a vector of Tensor-Train ranks r = (r1, r2, · · ·, rd+1), TTF decomposes a d-dimensional tensor W  R(m1·n1)×(m2·n2)×···×(md·nd) into a multiplication of core tensors according to (1), where the k-th core tensor C[k]  R ,rk×mk×nk×rk+1 and any index pair (ik, jk) satisfies 1  ik  mk, 1  jk  nk. Additionally, the ranks r1 and rd+1 are fixed to 1.
W((i1, j1), (i2, j2), ..., (id, jd)) = C[1](r1, i1, j1, r2)C[2](r2, i2, j2, r3)···C[d](rd, id, jd, rd+1) (1)

Thus, when the TTF technique is applied to the fully-connected (FC) layer with feed-forward weight matrix W , a tensor W is firstly converted from W and is then decomposed to a multiplication of the core tensors as shown in (2), where C[t] is the t-th core tensor, X denotes a tensor of input, B refers to a tensor of bias, the tensor of outputs Y^  Rn1×n2×···×nd , and  is a sigmoid function. For clarity, the notation T T L(W , X) is used to simplify the representation of a Tensor-Train fully-connected
layer, which is shown in (3).

m1 md d

Y^(j1, j2, ..., jd) = ( · · ·

C[t](rt, it, jt, rt+1)X(i1, i2, ..., id) + B(j1, j2, ..., jd)) (2)

i1=1 id=1 t=1

Y^ = (T T L(W , X)).

(3)

1

Under review as a conference paper at ICLR 2019

Likely, we use (4) to represent an RNN with a feed-forward weight matrix W and a recurrent weight
matrix U . In (4), Xt is an input matrix at time t, ht-1 and ht separately denote the hidden vectors of time t - 1 and t, and  refers to the sigmoid function.

ht = (T T L(W , Xt) + T T L(U , ht-1)).

(4)

The largest benefit from Tensor-Train models is the capability to reduce the model parameters tremendously. For example, a Tensor-Train FC layer needs i mi · ni · ri · ri+1 parameters in total. In comparison, the total number of parameters of an FC layer is about O( i mi · i ni), which is much larger than the associated Tensor-Train FC one.
The Tensor-Train models are found widespread. For example, Yang et al. (2017) set up Tensor-Train Recurrent Neural Networks for video classification, and Yu et al. (2017) applied the Tensor-Train as an End-to-End dynamic model for multi-variate forecasting environmental data. However, those applications focused on simple deep learning architectures, and a vector of Tensor-Train ranks r is element-wise fixed. When more complex deep models with numerous Tensor-Train layers are involved, it is not easy to find appropriate Tensor-Train ranks for each core tensor. However, when applying a vector of shared and high Tensor-Train ranks to all parameters of Tensor-Train layers, each of them has a vector of lower Tensor-Train ranks, which results in an additional decrease in the number of parameters.
This work applies Riemannian Stochastic Gradient Descent (RSGD) to the RNN Tensor-Train layers. Unlike Stochastic Gradient Descent (SGD), which conducts the update of parameters in the Euclidean space, RSGD finds optimal core tensors of parameters associated with a vector of lower Tensor-Train ranks in Riemannian Manifold, thereby decreasing the number of total parameters of Tensor-Train RNNs.
Our work is inspired by Bonnabel (2013) and Lubich et al. (2015), which are two excellent introductions to the theory of Stochastic Gradient Descent on Riemannian Manifolds. Additionally, Zhang et al. (2016) was a recent work that proposed a fast stochastic optimization on Riemannian Manifolds. Besides, Absil et al. (2009) is a useful reference of Riemannian optimization.

Contributions. We summarize the key contributions of this paper as follows:

· This work applies RSGD to iteratively find vectors of lower Tensor-Train ranks with the update of parameters in the training process. The RSGD algorithm and the related theoretical analysis are also presented.
· We design Bi-directional Tensor-Train GRU/LSTM Chung et al. (2014), and EncoderDecoder Tensor-Train RNNs with a Tensor-Train Attention mechanism Luong et al. (2015). We apply the RSGD algorithm to the Tensor-Train RNN models on the digit recognition and machine translation tasks.

To the best of our knowledge, this is the first work that applies RSGD to train Tensor-Train RNNs to find the optimal core tensors of parameters with vectors of lower Tensor-Train ranks. Moreover, this is the first work that builds Tensor-Train RNNs with complex architectures for natural language processing (NLP) tasks.

2 RIEMANNIAN STOCHASTIC GRADIENT DESCENT

The optimization problem of Tensor-Train RNNs can be formulated as a Riemannian optimization problem as shown in (5), where {X, Y } is a data sequence with length T , W represents a tensor of parameters which lies in a d-dimensional Riemannian Manifold (M, µ) with a Riemannian measure µ , and the Tensor-Train ranks for core tensors of W must be element-wise no higher than the vector r = (r1, r2, ..., rd+1) as shown in (5).

min f (W; X, Y )
WM
s.t., tt rank(W)  r.

(5)

Besides, the Riemannian measure µ induces an inner product structure in each tangent space TxM associated with a tensor x  M . Specifically, u, v  TxM , the inner product < u, v >= µx(u, v).

2

Under review as a conference paper at ICLR 2019
Algorithm 1 Riemannian Stochastic Gradient Descent 1. Given the labeled input data (X, Y ) with sequence length T , and the learning rate . 2. W(0)  the randomly initialized core tensors {C[1], C[2], ..., C[d]}. 3. For t = 1, 2, ..., T : 4. For i = 1, 2, ..., d: 5. Choose a gradient gC[i] = C[i] f (W(t); X, Y ) in the tangent space TC[i] M . 6. A^ i = C[i] - gC[i] . 7. C^ [i]  ExpC[i] (A^ i). 8. (C^ [i], r^i)  rounding(C^ [i]). 9. W(t)  {C^ [1], C^ [2], · · ·C^ [d]}. 10. r  (r^1, r^2, ...r^d). 11. Reshape the core tensors {C[1], C[2], ..., C[d]} based on the updated r. 12. Return W(T +1) = {C[1], C[2], · · ·, C[d]}.
Similarly, µ induces the norm of u  TxM as ||u|| = µx(u, u)  0. In addition, the µ induced inner product and the norm preserve the basic properties like definiteness, homogeneity and triangle inequality.
Algorithm 1 presents the RSGD Algorithm. The algorithm mainly consists of two main procedures: one is the update of parameters in the tangent space and conducting an exponential mapping, and the second one is the rounding to lower Tensor-Train ranks. As illustrated in Figure 1, step 5 firstly obtains a gradient gC[i] on a tangent space TCi M at the core tensor C[i] in Riemannian Manifold (M, µ), and step 6 conducts a gradient descent on the tangent space to generate a new tensor A^ i. Step 7 projects A^ i back to C^ [i] in Riemannian Manifold (M, µ) by an exponential mapping. Finally, as shown in Figure 2, the rounding function in step 8 transforms C^ [i] in the submanifold Sr to the core tensor C^ [i] with a vector of lower Tensor-Train ranks ^r in a new submanifold S^r. Note that the vectors of Tensor-Train ranks r and ^r span two submanifolds Sr  M and S^r  M respectively. After that, the next iteration of the the parameter update is conducted in S^r.

Figure 1: An exponential mapping.

Figure 2: The rounding procedure.

The exponential mapping in Algorithm 1 is formulated in (6). Unfortunately, it is not easy to solve the problem because we have to deal with the calculus of variations, or we have to know the Christoffel symbols Lubich et al. (2015). Therefore, a fast and straightforward retraction method is applied as a first-order approximation to the exponential mapping as shown in Algorithm 2.

min ||W - Y||F
YSrM d
s.t., tt rank(Y) = r.

(6)

3

Under review as a conference paper at ICLR 2019

Algorithm 2 The Retraction Algorithm
1. Given the core tensors {C[1], C[2], · · ·, C[d]} and the Tensor-Train rank ^r = {r1, r2, · · ·, rd+1}. 2. For i = 1 to d: 3. Ai  reshape C[i] by the shape of (ri · ni · mi) × ri+1. 4. For i = 1 to d: 5. A^i, i  QR decomposition(Ai). 6. C[i]  reshape A^i by (ri, ni, mi, ri+1). 7. Ai+1  multiply(i, Ai+1), if i < d. 8. Return {C[1], C[2], · · ·, C[d]}.

Algorithm 3 The Rounding Algorithm

1. Given the core tensors {C[1], C[2], · · ·, C[d]} and a constant maximum rank rmax. 2. Initialize r^ = {1, 1, ..., 1}.

3. For i = 1 to d: 4. Ai  reshape C[i] by the shape of (ri · ni · mi) × ri+1.

5. For i = 1 to d:

6. 7.

rcoolwnnuumm(C(C[i[]i)])==nri i· ·mrii+·1r^.i+1.

r^i+1

8. r^i+1 = min(rmax, col num(C[i]), row num(C[i])).

9. (Ui, Si, Vi) = SVD(Ai).
10. U^i = Ui[:, 1 : r^i+1], S^i = Si[1 : r^i+1, 1 : r^i+1], V^i = Vi[1 : r^i+1, :].
11. A^i = U^iS^iV^i. 12. C[i]  reshape A^i as R .r^i×ni×mi×r^i+1
13. Return the updated core tensors {C[1], C[2], · · ·, C[d]}.

Algorithm 2 presents the retraction algorithm. The main idea of the retraction algorithm is to orthogonalize the core tensors {C[1], C[2], · · ·, C[d]} in a left-to-right order by the QR decomposition.
The rounding algorithm is shown in Algorithm 3. Similar to the retraction algorithm, a left-to-right tensor matricization is firstly initialized. Then, the Tensor-Train rank is updated before conducting an SVD computation. The returned core tensors {C[1], C[2], · · ·, C[d]} are based on the updated vector of lower Tensor-Train ranks. Furthermore, the rounding procedure has the property presented in Proposition 1.
Proposition 1. The rounding procedure of Algorithm 3 does not change values of the objective function f for the Tensor-Train RNNs. That is, for a tensor x  Sr, the rounding tensor x^  S^r, we have f (^x) = f (x).

Proof. Given the weight matrix W with core tensors {C[1], C[2], · · ·, C[d]}, for an input tensor X  M , we obtain (7) and (8) according to (1).

m1 md d

Y(j1, j2, ..., jd) = · · ·

C[t](rt, it, jt, rt+1)X(i1, i2, ..., id) + B(j1, j2, ..., jd)

i1=1 id=1 t=1

m1 md d

= ···

C[t](r^t, it, jt, r^t+1)X(i1, i2, ..., id) + B(j1, j2, ..., jd),

i1=1 id=1 t=1

(7) (8)

which suggests that a vector of Tensor-Train ranks determines a submanifold for generating core tensors of the tensor W, but the values of the objective functions are invariant to the change of the vector of Tensor-Train ranks, obtaining Proposition 1.

4

Under review as a conference paper at ICLR 2019

3 CONVERGENCE ANALYSIS

This section analyzes the convergence of the RSGD algorithm. The necessary definitions and theorems are firstly introduced, and the analysis is then provided. Since the objective functions of Tensor-Train RNNs are always geodesically non-convex Zhang et al. (2016), we only consider the convergence of RSGD for non-convex cases.
Definition 2. A different function f : M  R is geodesically L-smooth if its first-order gradient is geodesically L-Lipchitz continuous. Specifically, x, y  M we have 9.

f

(y)



f

(x)+

<

gx,

Expx-1(y)

>

+

L 2

||Exp-x 1(y)||2,

(9)

where gx is the sub-gradient of f (x) at x in the tangent space TxM , and Expx-1(y) is the inverse exponential mapping which projects the curve line in M d from x to y back to the gradient x in the tangent space TxM .
From the RSGD algorithm (Algorithm 1), it is not hard to find the sub-gradient gx = f (x) and Expx-1(y) = -xf (x), and thus Theorem 3 can be derived. Theorem 3. For a differentiable and geodesically L-smooth function f , the Riemannian stochastic gradient descent algorithm ensures (10).

min E[||f (xt)||]
t



1 T

(f (x0)

-

f (x))

+

L 2k2, 2

(10)

where T refers the total iterations, x0 and x denote the initial and the optimal points respectively,  is the learning rate, and ||Expx-1(y)||2 is bounded by 2k2.

Proof. Assume x and x0 separately refer to the optimal and initial points. For all xt and xt+1 at two consecutive times, we can derive (11) based on Definition 2.

E[f (xt+1)]



E[f (xt)]

+

E[<

f (xt),

Exp-xt1(xt+1)

>]

+

L 2

E

[||E

xpx-t1

(xt+1

)||2

].

(11)

By applying Expx-t1(xt+1) = -f (xt)) and E[||Expx-t1(xt+1)||2]  2k2, we obtain (12), where x^t+1 is the rounding tensor of x^t+1. The Proposition 1 ensures that f (xt+1) = f (x^t+1).

E[||f (xt)||]



1  E[f (xt)

-

f (xt+1)]

+

L 2k2 2

=

1  E[f (xt)

-

f (x^t+1)]

+

L 2k2. 2

(12)

By summing the two sides of equation 12 from 0 to T - 1, we derive (13).

min
t

E[||f

(xt

)||]



1 T

T -1
E[||f (xt)||]



1 T

(f (x0) - f (xT )) +

L 2k2. 2

t=0

(13)

Since f (xt)  f (xT ), 0  t  T - 1, we have 14.

min
t

E

[||f

(xt

)||]



1 T

(f (x0)

-

f (xT ))

+

L 2k2 2



1 T

(f (x0)

-

f (x))

+

L 2k2. 2

After rounding x to x^ , we finally obtain the result 15 of Theorem 3.

(14)

min
t

E[||f

(xt

)||]



1 T

(f (x0)

-

f (x^))

+

L 2k2. 2

(15)

Furthermore, Theorem 3 suggests that the number of iterations T satisfies (16) before reaching the

convergence.

T



f (x0) - f (x^)

(min
t

E

[||f

(xt)||]

-

L 2

2

k2)

.

(16)

5

Under review as a conference paper at ICLR 2019

4 ADVANCED TENSOR-TRAIN RNNS

This section introduces the Tensor-Train RNNs with the advanced architectures used in this work. The new Tensor-Train architecture is based on the Bi-directional Tensor-Train GSU/LSTM. The other one is the Tensor-Train Encoder-Decoder RNNs with the Tensor-Train Attention mechanism.

4.1 BI-DIRECTIONAL TENSOR-TRAIN GRU/LSTM

Firstly, we introduce the Bi-directional Tensor-Train GRU/LSTM, which involves twice more pa-

rameters than the Tensor-Train GRU/LSTM. Equations from (17) to mechanisms of the Bi-directional Tensor-Train GRU, where the pairs

((-r2t4,)r-pt-)reasne-dnt(-zthte,

z-fut )ncdteinoontael

forward and backward update gate operations, respectively, an-d th-e pair (dt, dt) represents the

forward-backward reset gate operations. In addition, the pair (ht, ht) is the memory cell holding

-informat-ion of last times. oh is the output of the Bi-directional Tensor-Train GRU which combines

ht and ht by concatenating them before feeding through a Tensor-Train linear layer.

-rt

=

(T

T

L(W-r ,

- Xt)

+

T

T

L(-U r,

-- ht-1))

(17)

r-t

=

(T

T

L(W-r ,

- Xt)

+

T

T

L(U-r ,

-- ht-1))

-zt

=

(T

T

L(-Wz

,

- Xt)

+

T

T

L(-U z,

-- ht-1))

z-t

=

(T

T

L(W-z

,

- Xt)

+

T

T

L(U-z ,

-- ht-1))

(18) (19) (20)

- dt

=

tanh(T

T

L(-Wd,

- Xt)

+

T

T

L(-U d,

(-rt



-- ht-1)))

- dt

=

tanh(T

T

L(W-d,

- Xt)

+

T

T

L(U-d,

(r-t



-- ht-1)))

- ht

=

(1

-

-zt )



-- ht-1

+

-zt



- dt ,

- ht

=

(1

-

z-t )



-- ht-1

+

z-t



- dt

(21) (22) (23)

- -

oh = (T T L(Ah, [ht; ht])).

(24)

In addition, we also design a Bi-directional Tensor-Train LSTM which involves more operational gates than the Bi-directional Tensor-Train GRU.

4.2 THE TENSOR-TRAIN ENCODER-DECODER RNNS WITH ATTENTION MODELS

The Encoder-Decoder RNNs are commonly used in sequence-to-sequence deep learning applications. Moreover, the attention mechanism significantly improves the performance of EncoderDecoder RNNs Vaswani et al. (2017).

The Bi-directional Tensor-Train RNN like GRU or LSTM is used to construct the Encoder-Decoder architecture. Moreover, we set up the Tensor-Train Attention model in addition to the Tensor-Train Encoder-Decoder RNNs. Thus, the entire model is built on Tensor-Train layers.

To build a Tensor-Train Attention model, it is necessary to add the Tensor-Train layer to generate an
Attention vector as shown in (25), where ct is a context vector (26) with attention weights ts (27), at is the output of the Attention model at time t, h¯ s denotes a vector built from the outputs of the forward and backward stages, and ht refers to the output of the hidden layers at time t.

at = tanh(T T L(Wc, [ct; ht]))

(25)

ct = tsh¯ s

(26)

ts =

s

exp(score(ht, h¯ s))

S s

=1

exp(score(ht,

h¯ s

))

.

(27)

5 APPLICATIONS

This section first introduces the implementation of the Tensor-Tensor RNNs. Then, we present two applications where the RSGD algorithms were tested. One application is the digit recognition task on the sequential MNIST dataset; the other is the task of machine translation on the Multi30K dataset Elliott et al. (2016).

6

Under review as a conference paper at ICLR 2019
5.1 IMPLEMENTATIONS We employed PyTorch to implement our Tensor-Train RNNs. The data structures of our implementations were partly built on the free Tensor-Train toolkit implemented by the tool Tensorflow Novikov et al. (2018). However, we employed the tool PyTorch to take the advantage of dynamic graph generation, which is much more useful for NLP tasks.
5.2 DIGIT RECOGNITION ON THE MNIST DATASET The first application is the digit recognition task on the MNIST dataset. The dataset consists of 60000 data with 28  28 pixels for each digital image. Instead of vectorizing the image pixels into a long vector as an input for a static deep neural network, image pixels are taken as data sequence where the time step is set to 28, and the input dimension is set to 28. In our experiments, the training and testing sets were separately composed of 50000 and 10000 data. 2000 data were selected from the training set for building a validation set, and they were not included in the training set. As for the experimental setup, we applied both Bi-directional Tensor-Train GRU and Bi-directional Tensor-Train LSTM to the task. The dimension of the Manifold (M, µ) was set to d = 3, and the vector of Tensor-Train ranks was initialized with high and shared values r = (1, 10, 10, 1) for the Tensor-Train layers. The weight matrix of the input-hidden Tensor-Train layer was converted to the tensor with the shape (2 × 7 × 2) by (6 × 6 × 6), and the weight matrix of the hidden-hidden Tensor-Train layer was converted to the tensor with the shape (6 × 6 × 6) by (6 × 6 × 6). The RSGD algorithm with a learning rate 0.01 was applied to both Bi-directional Tensor-Train GRU/LSTM. The results are shown in Figure 3, where we compared the Bi-directional Tensor-Train GRU/LSTM with the traditional Bi-directional GRU/LSTM regarding recognition error rates and number of parameters. Inspecting the recognition error rate, the Bi-directional Tensor-Train GRU obtains a result that is close to that of the traditional Bi-directional GRU/LSTM, and the performance of the Bidirectional Tensor-Train LSTM becomes a bit worse. Regarding comparison with the number of parameters, both Bi-directional Tensor-Train GRU/LSTM can significantly reduce the number of parameters by taking only 1% parameters of the Bi-directional GRU/LSTM at final. Notably, the RSGD algorithm further reduces the number of parameters of Bi-directional GRU/LSTM by lowering the Tensor-Train ranks.
Figure 3: Experimental results on the sequential MNIST dataset (`TT' denotes Tensor-Train, `params.' means the number of parameters, `init.' means the initial, and `final' refers to the final number).
7

Under review as a conference paper at ICLR 2019

Table 1: Statistics of Model Parameters

Models

Parameters of hidden layers

Bi-directional Encoder-Decoder

462144

Tensor-Train Bi-directional Encoder-Decoder (initial) 15154

Tensor-Train Bi-directional Encoder-Decoder (final) 11088

5.3 MACHINE TRANSLATION ON THE MULTI30K DATASET
The next application is a machine translation task from Dutch to English on the Multi30K dataset. In the dataset, there are separately 29000, 1014, and 1000 sentence pairs for training data, validation data, and test data, respectively.
The Bi-directional Tensor-Train GRU was used to build the Encoder-Decoder architecture, and a Tensor-Train Attention model was added to the architecture. The RSGD algorithm with a learning rate 0.01 was applied to the Tensor-Train Encoder-Decoder RNNs with the Tensor-Train Attention model. An initial vector of Tensor-Train ranks was set as r = (1, 6, 6, 6, 1), and the weight matrix of hidden layers was converted to the tensor with the shape of (4 × 4 × 4 × 4) by (4 × 4 × 4 × 4). Besides, the baseline was based on the traditional Bi-directional GRU and LSTM-based EncoderDecoder architecture with the Attention model, where the Stochastic Gradient Descent algorithm with learning rate 0.001 was used to update parameters.
The experimental results are shown in Figure 4, and the statistics of parameters of hidden layers are shown in Table 1. The results in Figure 4 suggest that the Tensor-Train Encoder-Decoder RNN performs closer or even better than the Encoder-Decoder one, although the convergence speed of the Tensor-Train model is relatively slower in the first several iterations. On the other hand, Table 1 shows that RSGD leads to a further decrease in the number of parameters.

Figure 4: Experimental results of Machine Translation on the Multi30K dataset (PPL refers to the logarithm of loss values).
6 CONCLUSIONS
This paper presents the RSGD algorithm for training Tensor-Train RNNs including the related properties, implementations, and convergence analysis. Our experiments on digit recognition and machine translation tasks suggest that RSGD can work effectively on the Tensor-Train RNNs regarding performance and model complexity, although the convergence speed is relatively slower in the beginning stages. Our future work will consider two directions: one is to apply the RSGD algorithm to more Tensor-Train models and test it on larger datasets of other fields; and the second one is to generalize Riemannian optimization to the variants of the SGD algorithms and study how to speed up the convergence rate.
8

Under review as a conference paper at ICLR 2019
REFERENCES
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2009.
Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Trans. Automat. Contr., 58(9):2217­2229, 2013.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Desmond Elliott, Stella Frank, Khalil Sima'an, and Lucia Specia. Multi30k: Multilingual englishgerman image descriptions. arXiv preprint arXiv:1605.00459, 2016.
Yong-Deok Kim and Seungjin Choi. Nonnegative tucker decomposition. In IEEE Conference on Computer Vision and Pattern Recognition., pp. 1­8. IEEE, 2007.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3): 455­500, 2009.
Christian Lubich, Ivan V Oseledets, and Bart Vandereycken. Time integration of tensor trains. SIAM Journal on Numerical Analysis, 53(2):917­941, 2015.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. In Advances in Neural Information Processing Systems, pp. 442­450, 2015.
Alexander Novikov, Pavel Izmailov, Valentin Khrulkov, Michael Figurnov, and Ivan Oseledets. Tensor train decomposition on tensorflow. arXiv preprint arXiv:1801.01928, 2018.
Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295­ 2317, 2011.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998­6008, 2017.
Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for video classification. arXiv preprint arXiv:1707.01786, 2017.
Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue. Long-term forecasting using tensortrain rnns. arXiv preprint arXiv:1711.00073, 2017.
Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian SVRG: Fast stochastic optimization on riemannian manifolds. In Advances in Neural Information Processing Systems, pp. 4592­4600, 2016.
9

