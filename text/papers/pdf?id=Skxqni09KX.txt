Under review as a conference paper at ICLR 2019
ONLINE BELLMAN RESIDUE MINIMIZATION VIA SADDLE POINT OPTIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
We study the problem of Bellman residual minimization with nonlinear function approximation in general. Based on a nonconvex saddle point formulation of Bellman residual minimization via Fenchel duality, we propose an online first-order algorithm with two-timescale learning rates. Using tools from stochastic approximation, we establish the convergence of our problem by approximating the dynamics of the iterates using two ordinary differential equations. Moreover, as a byproduct, we establish a finite-time convergence result under the assumption that the dual problem can be solved up to some error. Finally, numerical experiments are provided to back up our theory.
1 INTRODUCTION
Reinforcement learning (RL) (Sutton & Barto, 1998) studies the problem of sequential decision making under uncertainty. In these problems, an agent aims to make optimal decisions by interacting with the environment, which is modeled as a Markov Decision Process (MDP). Thanks to the recent advancement of deep learning, reinforcement learning has demonstrated extraordinary empirical success in solving complicated decision making problems, such as the game of Go (Silver et al., 2016; 2017), navigation (Banino et al., 2018), and dialogue systems (Li et al., 2016).
However, when nonlinear function approximation such as neural networks are utilized, theoretical analysis of RL algorithms becomes intractable as it involves solving a highly nonconvex statistical optimization problems. Whereas in the tabular case or in the case with linear function approximation, using tools for convex optimization and linear regression, the statistical and computational properties of the reinforcement learning algorithms are well-understood under these settings. In consequence, although RL algorithms with nonlinear function approximation great empirical success, their theoretical understanding lags behind, which makes it difficult to design RL methods in a principled fashion.
Moreover, from a statistical perspective, with nonlinear function approximation, RL methods such as fitted value iteration (Munos & Szepesva´ri, 2008), fitted Q-iteration (Antos et al., 2008a), and Bellman residual minimization Antos et al. (2008b) can be cast as nonlinear regression problems. Using nonparametric regression tools, statistical properties of batch RL methods with nonlinear function approximation are established (Farahmand et al., 2016). However, when it comes to computational properties, due to the fundamental hardness of nonconvex optimization, theoretical understanding of the convergence of RL methods remains less explored, which is contrast to the case with linear function approximation, where the convergence of online algorithms based on temporal-difference (TD) learning are well studied.
In this work, we we make the first attempt to study an online algorithm for Bellman residual minimization, with nonlinear function approximation. In the batch form, Bellman residual minimization is formulated as a bilevel optimization, which cannot be solved with computational efficiency. To tackle this problem, we formulate the Bellman residual itself as the optimal value of another maximization problem. In this way, Bellman residual minimization becomes a saddle point problem,
1

Under review as a conference paper at ICLR 2019
where the value function is the primal variable, and the dual variable tracks the TD-error of the primal variable. By also parametrizing the dual variable using a parametrized function class, we propose a primal-dual subgradient method which is an online first-order method for Bellman residue minimization.
Furthermore, since the saddle-point problem is not convex-concave, the order between the inner maximization and outer minimization problems plays a significant role. Similar to the batch algorithm, ideally we would fix the primal variable and solve the inner maximization problem to its global optima, and then updated the primal variable. However, in the online setting, this approach is not tractable. To achieve computational efficiency, we apply the two-timescale updates to the primal and dual variables. Specifically, we update the primal and dual variables using two sets of learning rates, where the learning rate of the dual variable is much larger than that of the primal variable. Using stochastic approximation (Borkar, 2008; Kushner & Yin, 2003), two-timescale updating rules ensures that we could safely fix the primal variable when studying the convergence of the dual variable. In this case, the dual variable converges to a local maximum of the inner maximization problem. Moreover, the dynamics of the iterates are characterized by two ordinary differential equations (ODE) running at different timescales.
Our contributions are three-fold. First, we formulate the problem of Bellman residual minimization as a nonconvex saddle point problem, for which we propose an online first-order algorithm using two-timescale learning rates. Second, using stochastic approximation, we show that the online algorithm converges almost surely to the asymptotically equilibria of an ODE. Third, assuming the existence of an optimization oracle which solves the dual problem up to some error, we show that the stochastic gradient method in the primal step converges to a stationary point of the squared Bellman residual up to some fundamental error.
Related Work. The statistical properties of Bellman residual minimization is studied in Antos et al. (2008b); Maillard et al. (2010); Farahmand et al. (2016) for policy evaluation, where the problem is solved using least-squares regression under the batch setting. Moreover, in these work, Bellman residue minimization is an intermediate step of least-squares policy iteration Lagoudakis & Parr (2003). These work are not comparable to our work since our study an online algorithm for Bellman residue minimization and its convergence.
In addition, our work is related to the line of research on the online algorithms for policy evaluation with function approximation. Most existing work focus on linear function approximation. Specifically, Tsitsiklis & Van Roy (1997) study the convergence of the on-policy TD() algorithm based on temporal-difference (TD) error. To handle off-policy sampling, Maei et al. (2010); Sutton et al. (2009; 2016); Yu (2015); Hallak & Mannor (2017) propose various TD-learning methods with convergence guarantees. Utilizing two-timescale stochastic approximation in Borkar (2008) and strong convexity, they establish global convergence results for the proposed methods. The finite-sample analysis of these methods are recently established in Dalal et al. (2017b;a). More related works are Liu et al. (2015); Du et al. (2017), which formulate minization of the mean-squared projected Bellman error as a saddle point problem using Fenchel duality. However, since they consider linear function approximation, the corresponding saddle point problem is convex-concave, whereas our objective is nonconvex. When it comes to nonlinear function approximation, to the best of our knowledge, the only convergent algorithm is the nonlinear-GTD algorithm proposed in Bhatnagar et al. (2009). Their algorithm depends on the Hessian of the value function, and thus might be costly in practice.
Moreover, it is worth noting that Dai et al. (2017b) apply the same saddle point formulation to soft Qlearning. However, they consider a batch algorithm with the assumption that the inner maximization can be solved to the global optima. Due to nonconvexity, this assumption could stringent.
Furthermore, Chen & Wang (2016); Wang (2017) propose primal-duality of reinforcement learning based on the Lagrangian duality of linear programming for MDP (Puterman, 2014). These work establish convergence results in the tabular case. Applying neural networks to the same duality
2

Under review as a conference paper at ICLR 2019

formulation, Dai et al. (2017a); Cho & Wang (2017) propose variants of the actor-critic algorithms (Konda & Tsitsiklis, 2000), which does not have convergence guarantees.
Notation. We use the following notations throughout this paper. For any vector x  Rn, we use x 2 and x  to denote the Euclidean norm and the -norm of x, respectively. For a finite set M, we use |M| to denote its cardinality. We denote by P(M) the set of all probability measures on M and we write B(M) for the set of all bounded functions defined on M. For a function f  B(M), we define the -norm of f as f  = supxM |f (x)|. Moreover, for any probability measure µ  P(M), we write f µ for the 2-norm with respect to µ, i.e., f µ = [ M |f (x)|2dµ(x)]1/2. Finally, we use [n] to denote the set of integers {1, · · · , n}.

2 VALUE FUNCTION ESTIMATION IN RL

In this section, we introduce some background on reinforcement learning that will be used in the presentation our main results.

In reinforcement learning, the environment is often modeled as a Markov decision process (MDP)
denoted by a tuple (S, A, P, r, ), where S is the set of states, A is the set of all possible actions,
P : S × A  P(S) is the Markov transition kernel, r : S × A  R is the reward function, and   (0, 1) is the discount factor. More specifically, an agent interacts with the MDP sequentially in
the following way. At the t-th step for any t  0, suppose the MDP is at state st  S and the agent selects an action at  A; then, the agent observes reward r(st, at) and the MDP evolves to the next state st+1  P (· | st, at). Here P (· | s, a) is the probability distribution of the next state when taking action a at state s. The discounted cumulative reward is defined as R = t0 t · r(st, at).

In addition, a policy  : S  P(A) specifies a rule of taking actions. Specifically, (a|s) is the
probability of selecting action a at state s under policy . We note that  induces a Markov chain
on S × A with transition probability p(s , a | s, a) = (a | s ) · P (s | s, a). We define the (action) value function of policy  as Q(s, a) = E(R | s0 = s, a0 = a, ), which is the expected value
of the discounted cumulative reward when the agent takes action a at state s, and follows policy  afterwards. Moreover, we define the optimal value function Q : S × A  R by letting Q(s, a) = sup Q(s, a) for all (s, a)  S × A, where the supremum is taken over all possible policies. By definition, Q(s, a) is the largest reward obtainable by the agent when starting from (s, a). It is well-known that Q and Q are the unique fixed points of the Bellman evaluation operator T  and the Bellman optimality operator T , respectively. Specifically, Bellman operators T  : B(S ×A)  B(S × A) and T  : B(S × A)  B(S × A) are defined respectively by

(T Q)(s, a) =r(s, a) +  · E Q(st+1, at+1) st = s, at = a, at+1  (· | st+1) , (T Q)(s, a) =r(s, a) +  · E max Q(st+1, a) st = s, at = a ,
aA

(2.1) (2.2)

where st+1  P (· | st, at). The problems of estimating Q and Q are usually referred to as policy evaluation and optimal control, respectively. Both of these problems lie at the core of reinforce-
ment learning. Specifically, policy evaluation is the pivotal step of dynamic programming methods. Moreover, estimating Q by applying the Bellman optimality operator in (2.2) repeatedly gives the
classical Q-learning algorithm (Watkins & Dayan, 1992), based on which a number of new algo-
rithms are developed.

In this work, we propose a saddle framework for stochastic fixed-point problems in general.

3 A SADDLE POINT FORMULATION OF BELLMAN RESIDUE MINIMIZATION
In this section, we formulate the problem of estimating the value functions introduced in §2 as saddle point optimization problems. Before going into the details, we first introduce a standard assumption on the MDP. Assumption 3.1 (MDP Regularity). The MDP (S, A, P, r, ) satisfies the following conditions.

3

Under review as a conference paper at ICLR 2019

(i). The action space A is a finite set, and there exists a constant Rmax > 0 such that |r(s, a)|  Rmax for any (s, a)  S × A.
(ii). For policy evaluation problem, we assume that the Markov chain on S × A induced by policy  has a stationary distribution d  P(S × A). For estimating Q, we consider the off-policy setting where we collect data using a behavioral policy b : S  A. Moreover, we assume that b induces a stationary distribution db over S × A. Moreover, we assume that it is possible to draw i.i.d. samples from d and db .

The first condition in Assumption 3.1 ensures that both Q and Q are bounded by Rmax/(1 - ). The stationary distributions d and db in the second condition is are the natural measures to evaluate the error of estimating Q and Q. This condition holds if the Markov chains induced by  and b
are irreducible and aperiodic. Moreover, when these two Markov chains possess the rapid mixing
property, the observations are nearly independent, which justifies the assumption of i.i.d. sampling from d and db .

In the following, we first focus on estimating Q; the results for Q can be similarly obtained by replacing T  by T  in (2.1). To simplify the notation, we denote db by . When the capacity of S is large, to estimate Q efficiently, we estimate Q using a parametrized function class F =
{Q : S × A  R,   Rd}, where  is the parameter. Then the problem is reduced to finding a parameter   Rd such that Q is close to Q.

Since Q is unknown, it is impossible to minimize the mean-squared error

Q - Q

2 

.

Since Q

is the unique fixed point of T , a direct idea is to minimize the mean-squared Bellman residual

minimize J() =
Rd

Q - T Q

2 

=

E(s,a)

[Q(s, a) - (T Q)(s, a)]2

.

(3.1)

via (stochastic) subgradient descent. By definition, the subgradient of J() is

J () = E(s,a) [Q(s, a) - (T Q)(s, a)] · (Q)(s, a) - [(T Q)](s, a) , (3.2)

where (T Q)(s, a) is the subgradient of T Q(s, a) with respect to , which is given by

(T Q)(s, a) =  · E ·Q(st+1, a ) st = s, at = a ,

(3.3)

where a = argmaxbA Q(st+1, b). Although combining (3.2) and (3.3) yields the closed form of J(), there exists a fundamental challenge when applying gradient-based methods to (3.1). Specifically, notice that both (T Q)(s, a) and (T Q)(s, a) involves conditional expectation given (s, a)  S × A, and these two terms are multiplied are together in J() in (3.2). Thus, to construct an unbiased estimator of J(), given an observation (s, a)  , we need to draw two independent samples from P (· | s, a) to ensure that the estimators of (T Q)(s, a) and (T Q)(s, a)
constructed respectively using these two samples are conditionally independent given (s, a). Such
an issue is called the "double sampling" problem in reinforcement learning literature (Baird et al.,
1995).

To resolve this issue, inspired by the saddle point formulation of soft Q-learning in Dai et al. (2017b), we formulate the objective function J() as

J() = maximize E -1/2 · [µ(s, a)]2 + [Q(s, a) - (T Q)(s, a)] · µ(s, a) ,
µB(S ×A)

(3.4)

where the maximization is taken over all functions µ : S × A  R. In particular, for any fixed , the solution of the optimization problem in (3.4) is (s, a) = Q(s, a)-(T Q)(s, a), which is known
as the temporal-difference (TD) error in the literature. Moreover, we parametrize µ in (3.4) using function class G = {µ : S × A  R,   Rp}. Then combining (3.1) and (3.4), we formulate
Bellman residual minimization as a stochastic saddle point problem

min max
Rd Rp

L(, ) = Es,a,s

-1/2 · [µ(s, a)]2+

Q (s,

a)

-

r(s,

a)

-



·

max
a A

Q (s

,

a

)

· µ(s, a)

,

(3.5)

4

Under review as a conference paper at ICLR 2019

where (s, a)   and s  P (· | s, a) is the next state given (s, a). Here  and  can be viewed as the primal and dual variables respectively. The gradients of L(, ) with respect to  and  are given by

L(, ) = Es,a,s µ(s, a) · Q(s, a) -  · Q(s , a ) ,

(3.6)

L(, ) = Es,a,s µ(s, a) · Q(s, a) - R(s, a) -  · max Q(s , b) - µ(s, a) , (3.7)
bA

where a in (3.6) satisfies that a = argmaxbA Q(s , b). From (3.6) and (3.7), replacing L(, ) and L(, ) by the stochastic gradients based on one observation (s, a, s ), we estab-
lish an stochasitc subgradient algorithm, whose details are given in Algorithm 1. Since the saddle point problem (3.5) is nonconvex, in the algorithm we project the iterates onto compact sets   Rd and   Rp respectively. Moreover, notice that ideally we would like to solve the inner maximiza-
tion problem in (3.5) for fixed . To achieve such a goal in an online fashion, we perform the primal
and dual steps in different paces. Specifically, the learning rates t and t satisfy t/t   as t
goes to infinity. Using results from stochastic approximation (Borkar, 2008; Kushner & Yin, 2003),
such a condition on the learning rates ensures that the dual iterates {t}t0 asymptotically track the sequence {argmaxRp L(t, )}t0, thus justifying our algorithm.

Algorithm 1 A Primal-Dual Algorithm for Q-Learning
Input: Initial parameter estimates 0  Rd and 0  Rp, primal and dual learning rates {t, t}t0. for t = 0, 1, 2, . . . until convergence do
Sample (st, at)  , observe reward r(st, at) and the next state st. Let at = argmaxaA Qt (st, a). Update the parameters by

t+1   t + t · µt (st, at) · Qt (st, at) - r(st, at) -  · Qt (st, at) - µt (st, at) , (3.8)

t+1   t - t · µt (st, at) · Qt (st, at) -  · Qt (st, at) .

(3.9)

end for

Furthermore, for policy evaluation, we replace T  by T  in (3.4) to obtain

min
Rd

max
Rp

Es,a,s

,a

-1/2 · [µ(s, a)]2 +

Q(s, a) - r(s, a) -  · Q(s , a )

· µ(s, a)

,

(3.10)

where (s, a)  d, s  P (· | s, a), and a  (· | s ). Here d is the stationary distribution on S × A induced by policy . Similarly, by computing the gradient of the objective in (3.10), we obtain a stochastic gradient algorithm for policy evaluation.

Finally, it is worth noting that, various saddle point formulations of Bellman residual minimization are proposed to avoid the issue of double sampling (Antos et al., 2008b; Farahmand et al., 2016; Dai et al., 2017b). Our formulation is the same as the one for soft Q-learning in Dai et al. (2017b), and is equivalent to that in Antos et al. (2008b); Farahmand et al. (2016) for policy evaluation. All of these work are batch algorithms, with the assumption that the global optima of the inner maximization can be reached. Whereas we propose an online first-order algorithm with nonlinear function approximation. Moreover, our analysis utilize tools from stochastic approximation of iterative algorithms (Borkar, 2008; Kushner & Yin, 2003), which is significantly different from their theory. .

4 THEORETICAL RESULTS
In this section, we lay out the theoretical results. For ease of presentation, we focus on the estimation of Q while our theory can be easily adapted to the policy evaluation problem. We first state the assumption on function class F = {Q :   Rd}.

5

Under review as a conference paper at ICLR 2019

Assumption 4.1. Here we assume that, for any (s, a)  S × A, Q(s, a) is a differentiable function of  such that |Q(s, a)|  Qmax Q(s, a) 2  Gmax, and that Q(s, a) is Lipschitz continuous in , where Qmax  Rmax/(1 - ) and Gmax > 0 are two constants.

Here our assumption on F allows nonlinear function approximation of the value function in general. Moreover, we only consider bounded value functions since Q is bounded by Rmax/(1 - ). Moreover, we assume that Q(s, a) is bounded and Lipschitz continuous for regularity. This assumption can be readily satisfied if  is restricted to a compact subset of Rd.
In addition, we assume that G = {µ :   Rp} is a class of linear functions as follows.
Assumption 4.2. We assume that µ(s, a) =  (s, a) for any   Rp, where  : S × A  Rp is a feature mapping such that (s, a) is uniformly bounded for any s  S, a  A. Furthermore, we assume that there exists a constant min > 0 such that E(s,a)[(s, a)(s, a) ] min · Ip.

Here we assume the dual function is linear for the purpose of theoretical analysis. In this case, the

inner maximization problem max L(, ) has a unique solution

() =

E(s,a)[(s, a)(s, a)

]

-1
E(s,a)

(s, a) · [Q(s, a) - (T Q)(s, a)]

(4.1)

for any   Rd. Thus, µ() is the minimizer of Q - T Q - µ() 2, i.e., µ() is the best approximation of the TD-error using function class G. It is possible to extend our result to nonlinear

µ following the analysis in Heusel et al. (2017) under stronger assumptions. In addition, we note that most online TD-learning algorithms uses linear function approximation. Moreover, it is shown

in Tsitsiklis & Van Roy (1997) that TD-learning with nonlinear function approximation may fail to

converge. To the best of our knowledge, for nonlinear function approximation, the nonlinear GTD

algorithm in Bhatnagar et al. (2009) is the only convergent online algorithm. However, their focus solely on policy evaluation, and their approach depends on the Hessian 2V. As a comparison, our method also consider nonlinear function approximation and can be applied to both policy evaluation

and optimal control.

Now we are ready to present the convergence result for Algorithm 1.

Theorem 4.3. We assume the learning rates {t, t}t0 in (3.8) and (3.9) satisfy

t = t = ,

t2 + t2 < ,

lim
t

t/t

=

0.

t0 t0

t0

(4.2)

In addition, let   Rd and   Rp be the Euclidean balls with radius R and R respectively. For function L(, ) defined in (3.5), we define

K =    : L(, )|=() = 0     : L(, )|=() =  for some   0 . (4.3)

Then under Assumptions 3.1, 4.1, and 4.2, the iterates {(t, t)}t0 created by Algorithm 1 converges almost surely to the set {[, ()],   K}.

In addition to the Robbins-Monro condition, (Robbins et al., 1951), the learning rates in (4.2) also satisfies limt t/t = 0. Intuitively, this means that the sequence {t}t1 tracks {(t)}t1 asymptotically. In other words, (4.2) enables our online algorithm to approximately solve the inner
maximization problem max L(, ) with  fixed. Using two-timescale stochastic approximation (Borkar, 2008; Kushner & Yin, 2003), when studying the convergence {t}t0, we could replace t in (3.9) by (t). In this case, using ODE approximation, {t}t0 converges almost surely to K in (4.3), which is the asymptotically stable equilibria of the projected ODE  = L(, )|=()+, where (t) is the correction term caused by projection onto .
Furthermore, even when  is sufficiently large such that {(t, t)}t0 converges to [(), ] with L[, ()] = 0, due to the error of function approximation,  is not a stationary point of J(·). Specifically, let  = Q - T Q be the TD-error. Then, by (3.6) we have
J () = E(s,a)[ (s, a) ·  (s, a)] = E(s,a)  (s, a) · [ (s, a) - µ()(s, a)] . (4.4)

6

Under review as a conference paper at ICLR 2019

Since µ() is the best approximator of  within G,  is in a neighborhood of a stationary point of J(·) if the function approximation error is small.

To see error incurred in estimating the TD-error reflects the fundamental limit of our method, we also provide a finite-time analysis provided that there exists an optimization oracle which approximately solves the inner maximization problem in (3.1).

Assumption 4.4 (Optimization Oracle). We assume that there exists an optimization oracle that

returns a bounded function µ  B(S × A) when queried by any   Rd. Moreover, we assume

that |µ(s, a)|  2Qmax for any (s, a)  S × A, and there exists a constant > 0 such that

µ - 

2 



.

Here we assume that the estimator µ for  is bounded by 2Qmax since  is bounded by
2Rmax/(1 - ) under Assumption 3.1. Moreover, in light of Algorithm 1,  can be viewed as
the estimation error of the TD-error using {µ,   }, i.e., sup inf  - µ . Now we update {t}t0 by

t+1  t - t · µt (st, at) · Qt (st, at) -  · Qt (st, at) ,

(4.5)

where st is the next state given (st, at)  , and at = argmaxaA Qt (st, a). The following theorem shows that any limit point of {t}t0 is in the vicinity of a stationary point of J(·) with
error proportional to .

Theorem 4.5. Assume that the learning rates {t}t0 in (4.5) satisfy t0 t =  and
t0 t2 < . Under Assumptions 3.1, 4.1, and 4.4, with probability one, we have lim supt0 J (t) 2  8Gmax · .

To understand this theorem, first notice that the update direction in (4.5) is a noisy version of
E(s,a)[µt (s, a) · t (s, a)], which is a biased estimate of J (t). Moreover, under Assumptions 4.1 and 4.4, such a bias can be bounded by 8Gmax · . Due to this bias in gradient estimation, {t}t1 can only enter a neighborhood of a stationary point. In addition, for Algorithm 1 with G being the class of linear functions, the optimization oracle outputs µ() =  () for any   . Applying Cauchy-Schwarz inequality to (4.4), we have J () 2  2Gmax ·  - µ() .
Thus, Theorems 4.3 and 4.5 yield consistent results, which implies that the error incurred by esti-
mating the TD-error using function class G leads to unavoidable error of our approach.

5 EXPERIMENTS
To justify our proposed method for estimating value function, we compare it with the classical deep Q-network (DQN) on several control tasks from the OpenAI Gym (Brockman et al., 2016). For a fair comparison, we use the codes from OpenAI Baselines (Dhariwal et al., 2017) for DQN, and use the same parametrization for both our method and DQN. The parameters are fine-tuned for each task to achieve the state-of-art performance. We run the algorithms with 5 random seeds and report the average rewards with 50% confidence intervals. Figure 1 (a)-(c) illustrates the empirical comparison results for the environments CartPole-v0, MountainCar-v0 and Pendulum-v0. We can see that in all these tasks, our method achieves the equivalent performance to DQN. The experiment settings are as follows.
· CartPole-v0. For both methods, we set the learning rate for Q-network, i.e. t, as 10-3 with batch size 32. The Q-network is a one-layer network with 30 hidden nodes. The µ-network's learning rate and structure are the same as the Q-network.
· Pendulum-v0. For both methods, we set t as 10-4 with batch size 1000. The Q-network is a two-layer network with 40 hidden nodes for each layer. The learning rate and structure of the µ-network are the same as Q-network.
· MountainCar-v0. For both methods, we set t as 10-4 with batch size 1000. The Qnetwork is a two-layer network with 20 hidden nodes for each layer. The learning rate for the µ-network, i.e. t, is also 10-4 with the same structure as Q-network.

7

Under review as a conference paper at ICLR 2019

As is shown in Theorem 4.3, we require limt t/t = 0 to guarantee the solution of inner maximization problem max L(, ) asymptotically. To justify this theoretical result, we test our method on CartPole-v0 with different setting of t and t. The results are illustrated in Figure 1-(d). We set t/t as 100, 1, and 0.01, respectively. As the learning rate ratio t/t deceases, the reward increases faster and become more stable around the solution. From Case I, when t/t is too big, our method cannot guarantee the solution of control task, which is in accordance with Theorem 4.3.

6 CONCLUSION
In this work, we propose an online first-order algorithm for the problem of Bellman residual minimization with nonlinear function approximation in general. Our algorithm is motivated by a duality formulation. Moreover, we establish the convergence of our problem by via ODE approximation, utilizing tools from stochastic approximation. In addition, we also establish a finite-time convergence result under the assumption of a computational oracle. Finally, numerical experiments are provided to back up our theory.

Reward

CartPole-v0
200 DQN
Our Method
150
100
50
0 50000 100000 150000 Frame
(a) CartPole-v0 MountainCar-v0
DQN Our method
120
140
160
180
200 0 50000 100000 150000 200000 250000 300000 Frame
(a) MountainCar-v0

Reward

Reward

0 250 500 750 1000 1250 1500 1750
0
200
150

Pendulum-v0
DQN Our method

50000 100000 Frame
(b) Pendulum-v0 CartPole-v0
Case I Case II Case III

150000

100

50

0 25000 50000 75000 100000 125000 Frame
(b) CartPole-v0 with various t/t

Reward

Figure 1: In (a)-(c) we compare our method and DQN on three classical control tasks. Each plot
shows the average reward during training across 5 random runs, with 50% confidence interval. As
shown in these plots, our method achieves the equivalent performance to the classical DQN. In
addition, in (d) we show the performances of our method for CartPole-v0 under different learning rates. For Case I, we set t = 10-2, t = 1e - 4 with t/t = 100. For Case II, we set t = 10-4, t = 10-4 with t/t = 1. For Case III, we set t = 10-4, t = 10-2 with t/t = 0.01. The figure shows that t/t  0 is critical for our method to work, which agrees with the theory.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Andra´s Antos, Csaba Szepesva´ri, and Re´mi Munos. Fitted Q-iteration in continuous action-space MDPs. In Advances in neural information processing systems, pp. 9­16, 2008a.
Andra´s Antos, Csaba Szepesva´ri, and Re´mi Munos. Learning near-optimal policies with bellmanresidual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89­129, 2008b.
Leemon Baird et al. Residual algorithms: Reinforcement learning with function approximation. In International Conference on Machine Learning, pp. 30­37, 1995.
Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based navigation using grid-like representations in artificial agents. Nature, pp. 1, 2018.
Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba Szepesva´ri. Convergent temporal-difference learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems, pp. 1204­1212, 2009.
Vivek S Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University Press, 2008.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity of reinforcement learning. arXiv preprint arXiv:1612.02516, 2016.
Woon Sang Cho and Mengdi Wang. Deep primal-dual reinforcement learning: Accelerating actorcritic using bellman duality. arXiv preprint arXiv:1712.02467, 2017.
Bo Dai, Albert Shaw, Niao He, Lihong Li, and Le Song. Boosting the actor with dual critic. arXiv preprint arXiv:1712.10282, 2017a.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Jianshu Chen, and Le Song. Smoothed dual embedding control. arXiv preprint arXiv:1712.10285, 2017b.
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Concentration bounds for two timescale stochastic approximation with applications to reinforcement learning. arXiv preprint arXiv:1703.05376, 2017a.
Gal Dalal, Bala´zs Szo¨re´nyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis for td (0) with linear function approximation. arXiv preprint arXiv:1704.01161, 2017b.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/ openai/baselines, 2017.
Joseph L Doob. Stochastic processes, volume 7. Wiley New York, 1953.
Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction methods for policy evaluation. In International Conference on Machine Learning, pp. 1049­1058, 2017.
Amir M Farahmand, Mohammad Ghavamzadeh, Shie Mannor, and Csaba Szepesva´ri. Regularized policy iteration. In Advances in Neural Information Processing Systems, pp. 441­448, 2009.
Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesva´ri, and Shie Mannor. Regularized policy iteration with nonparametric function spaces. The Journal of Machine Learning Research, 17(1):4809­4874, 2016.
9

Under review as a conference paper at ICLR 2019
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning, pp. 1352­1361, 2017.
Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. arXiv preprint arXiv:1702.07121, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6629­6640, 2017.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information Processing Systems, pp. 1008­1014, 2000.
Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and Applications. Springer, New York, NY, 2003.
Harold Joseph Kushner and Dean S Clark. Stochastic Approximation Methods for Constrained and Unconstrained Systems. Springer Science & Business Media, 1978.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning research, 4(Dec):1107­1149, 2003.
Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample analysis of proximal gradient td algorithms. In Conference on Uncertainty in Artificial Intelligence, pp. 504­513. AUAI Press, 2015.
Hamid Reza Maei, Csaba Szepesva´ri, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy learning control with function approximation. In nternational Conference on International Conference on Machine Learning, pp. 719­726, 2010.
Odalric-Ambrym Maillard, Re´mi Munos, Alessandro Lazaric, and Mohammad Ghavamzadeh. Finite-sample analysis of bellman residual minimization. In Proceedings of 2nd Asian Conference on Machine Learning, pp. 299­314, 2010.
Michel Metivier and Pierre Priouret. Applications of a Kushner and Clark lemma to general classes of stochastic algorithms. IEEE Transactions on Information Theory, 30(2):140­151, 1984.
Re´mi Munos and Csaba Szepesva´ri. Finite-time bounds for fitted value iteration. Journal of Machine Learning Research, 9(May):815­857, 2008.
Jacques Neveu. Discrete-parameter martingales. Elsevier, 1975.
HL Prasad, LA Prashanth, and Shalabh Bhatnagar. Actor-critic algorithms for learning Nash equilibria in n-player general-sum games. arXiv preprint arXiv:1401.2086, 2014.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 2014.
Herbert Robbins, Sutton Monro, et al. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400­407, 1951.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
10

Under review as a conference paper at ICLR 2019
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550(7676):354­359, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. Cambridge: MIT press, 1998.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesva´ri, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In International Conference on Machine Learning, pp. 993­1000. ACM, 2009.
Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1): 2603­2631, 2016.
John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation. In Advances in Neural Information Processing Systems, pp. 1075­1081, 1997.
Mengdi Wang. Primal-dual pi-learning: Sample complexity and sublinear run time for ergodic markov decision problems. arXiv preprint arXiv:1710.06100, 2017.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992. Huizhen Yu. On convergence of emphatic temporal-difference learning. In Conference on Learning
Theory, pp. 1724­1751, 2015.
11

Under review as a conference paper at ICLR 2019

A ALGORITHMS FOR POLICY EVALUATION AND SOFT Q-LEARNING

For policy evaluation, we replace T  in (3.4) by the Bellman evaluation operator T , which yields a saddle point problem

min max
Rd Rp

L(, ) = Es,a,s ,a

-1/2 · [µ(s, a)]2 + Q(s, a) - r(s, a) -  · Q(s , a ) · µ(s, a) ,

(A.1)

where (s, a)  , s  P (· | s, a) is the next state given (s, a), and a  (· | s ). The gradients of L(, ) with respect to  and  are given by

L(, ) = Es,a,s ,a µ(s, a) · Q(s, a) -  · Q(s , a ) , L(, ) = Es,a,s ,a µ(s, a) · Q(s, a) - R(s, a) -  · Q(s , a ) - µ(s, a) .

Therefore, replacing the primal and dual gradients by their unbiased estimates, we obtain Algorithm 2.

Algorithm 2 A Primal-Dual Algorithm for Policy Evaluation Input: Initial parameter estimates 0  Rd and 0  Rp, primal and dual stepsizes {t, t}t0. for t = 0, 1, 2, . . . until convergence do Sample (st, at)  d, observe r(st, at) and the next state st. Sample action a  (· | s ). Update the parameters by
t+1   t + t · µt (st, at) · Qt (st, at) - r(st, at) -  · Qt (st, a ) - µt (st, at) , t+1   t - t · µt (st, at) · Qt (st, at) -  · Qt (st, a ) .
end for

Furthermore, soft Q-learning is proposed Haarnoja et al. (2017) based on the maximum entropy principle. This problem aims to find the fixed point of the soft Bellman operator T defined by

(T Q)(s, a) = r(s, a) +  · E  · log

exp[Q(st+1, a)/ ] st = s, at = a ,

aA

(A.2)

where  > 0 is the temperature parameter. By definition, T can be seen as a smooth approximation
of the Bellman optimality operator, where the max function in (2.2) is replaced by the softmax function, where parameter  controls the approximation error. It is known that T also admits a unique fixed point, denoted by Q : S × A  R. To estimate Q with function approximation, similar to (3.5) and (A.1), we consider the saddle point problem minRd maxRp L (, ), where L (, ) is given by

L (, ) = Es,a,s -1/2 · [µ(s, a)]2 + Q(s, a) - r(s, a) -  ·  · log

exp[Q(s , a)/ ]

aA

where (s, a)   and s  P (· | s, a). Here  is the stationary distribution on S × A induced by the

behavioral policy b. By direct computation, the gradients of L (, ) with respect to  and  are given by

· µ(s, a) ,

L (, ) = Es,a,s ,a µ(s, a) · Q(s, a) -  · (s , a) · Q(s , a) ,
aA

L (, ) = Es,a,s ,a µ(s, a) · Q(s, a) - R(s, a) -  ·  · log

exp[Q(s , a)/ ] - µ(s, a)

aA

where we define  : S × A  [0, 1] by

(s, a) =

exp[Q(s, a)/ ] , a A exp[Q(s, a)/ ]

(s, a)  S × A,   Rd.

Thus, we obtain a stochastic gradient algorithm for estimating Q , which is stated in Algorithm 3.

,

12

Under review as a conference paper at ICLR 2019

Algorithm 3 A Primal-Dual Algorithm for Soft Q-Learning
Input: Initial parameter estimates 0  Rd and 0  Rp, primal and dual stepsizes {t, t}t0. for t = 0, 1, 2, . . . until convergence do
Sample (st, at)  , observe reward r(st, at) and the next state st. Sample action at = b with proabrbility t (st, b) for any b  A. Compute the TD error t = Qt (st, at) - r(st, at) -  ·  · log{ aA exp[Qt (st, a)/ ]}. Update the parameters by

end for

t+1   t + t · µt (st, at) · t - µt (st, at) , t+1   t - t · µt (st, at) · Qt (st, at) -  · Qt (st, at) .

B PROOFS OF THE MAIN RESULTS

B.1 PROOF OF THEOREM 4.3

Proof. The proof of this theorem consists of two steps, where we consider the faster and slower timescales seperately.

Step 1. Faster timescale. We first consider the convergence of {t, t}t1 in the faster timescale. Using ODE approximation, we will show that the sequence of dual variables {t}t0 generated from (3.8) tracks the solution of the inner maximization problem, i.e., argmax L(t, ). To begin with, for notational simplicity, we define t = (st, at),
t = Qt (st, at) - r(st, at) -  · Qt (st, at), At = Qt (st, at) -  · Qt (st, at), (B.1)
for all t  0, where at = argmaxaA Qt (st, a). Then the updating rules in Algorithms 1 reduce to

t+1   t + t · (t - t t) · t ,

t+1   t - t · t t · At ,

(B.2)

where {t, t}t0 are the learning rates, and  is the projection operator. Moreover, we define Ft = ({s , a , r(s , a ), s }t) as the -algebra generated by the history until time t. Thus, t and t are Ft-1-measurable for any t  1. Furthermore, for any   Rd and   Rp, we define

h(, ) = E(s,a) (s, a) · [Q(s, a) - (T Q)(s, a) - (s, a) ] , g(, ) = E(s,a) (s, a)  · Q(s, a) - [(T Q)](s, a) .

(B.3) (B.4)

Then by definition, we have E[t-t t)·t | Ft-1] = h(t, t) and E[t t·At | Ft-1] = g(t, t) for any t  1, which implies that {t}t1 and {t}t1 defined by

t = (t - t t) · t - h(t, t), t = t t · At - g(t, t)

(B.5)

are two martingale difference sequence with respect to filtration {Ft}t1. Moreover, we define C() as the outer normal cone of  at   , and define C() for  similarly.

Thus, (B.2) can be written as

t+1 t+1

= ×

t t

=

t t

- t ·

- t ·

t/t · At · t t tt t - t · t

t/t · At · t t tt t - t · t

+

t t

,

(B.6)

where × is the projection onto product set  ×  = {(u, v) : u  , v  }, and t and t in (B.5) are the correction terms induced by projection. Thus, by definition, we have t  -C(t+1) and t  -C(t+1), respectively.

Furthermore, under Assumption 4.1, both Q(s, a), Q(s, a), and (s, a) are bounded, which

implies that there exist a constant C

>

0 such that E[

t

2 2

+

t

2 2

|

Ft-1

]



C

for

all

t



1.

13

Under review as a conference paper at ICLR 2019

Moreover, let MT =

T t=1

t

·

(t

, t

)

 Rd+p for any T  1. Since

t1 t2 < ,

{MT }T 1 is a square-integrable martingale sequence. Moreover, by the Martingale convergence

theorem (Proposition VII-2-3(c) on page 149 of Neveu (1975)), {MT }T 1 converges almost surely.

Thus, for any > 0, by Doob's martingale inequality Doob (1953), we have

P sup MN - MT 
N T

 supNT E[

MN - MT

2 2

]



2C 2

2

tT 2

t2

,

which converges to zero as T goes to infinity. Furthermore, recall that, under the two-timescale assumption of the learning rates, we have t/t  0 as t goes to infinity, which implies that limt t/t · At · t t = 0.

Now we apply the Kushner-Clark lemma (Kushner & Clark, 1978, see also Theorem D.2 in §D for
details) to sequence {(t , t ) }t1, which implies that the asymptotic behavior of {(t, t)}t1 is characterized by the projected ODE

 = 0 + ,  = h(, ) + ,

(B.7)

where h is defined in (B.3), and  and  satisfy (t)  -C((t)) and (t)  -C((t)) for any t  0. Specifically, sequence {(t , t ) }t1 converges almost surely to the set of asymptotically stable equilibria of the projected ODE in (B.7), which is given by

(, ) :   , h(, )  C() .

(B.8)

Recall that  is the Euclidean ball in Rp with radius R. Thus, the boundary of , , is { :  2 = R}. For any   , the outer normal cone is C() = { ·  :   0}.
In the sequel, we show that, for any (, ) in the equilibria in (B.8),  is in the interior of , i.e.,  2 < R. This implies that {(t, t)}t1 converges almost surely to {(, ) :   , h(, ) = 0}.

Then, by definition, we have h[, ()] = 0 for any   Rd, which implies that () is the unique
maximizer of maxRp L(, ) since L(, ) is a strictly concave quadratic function of  and h(, ) = L(, ). Moreover, since (s, a) 2 is bounded for any (s, a)  S × A and the eigenvalues of E(s,a)[(s, a)(s, a) ] are at least min > 0, by norm inequality, we have

() 2  2Qmax/min · sup (s, a) 2.
(s,a)

(B.9)

Thus, if R is larger than the right-hand side of (B.9), () is in the interior of  for any   .

Now we assume to the contrary that   . By (B.8), there exists  > 0 such that

 ·  = h(, ) = E(s,a)[(s, a)(s, a) ] · [() - ],

(B.10)

where the second equality follows from (B.3) and (4.1). For the right-hand side of (B.10), we have

[() - ]

· E(s,a)[(s, a)(s, a)

] · [() - ]  cmin ·

 - ()

2 2

> 0.

(B.11)

However, since () is in the interior of , we have

·

() - , 

=·



2·

() 2 -  ·



2 2

<

0.

(B.12)

Thus, the contradiction between (B.11) and (B.12) implies that our assumption that    is not true, i.e.,  is in the interior of . In this case, C() is an empty set. Thus the asymptotically
stable equilibria of the ODE in (B.7) are given by

(, ) :   , h(, ) = 0 = [, ()] :    .

(B.13)

This implies that, in the faster timescale, we could fix the primal parameter at   , and the dual variable converges to (), which is the unique solution of the dual optimization problem max L(, ). In other words, using two timescale updates, we essentially solve the dual problem for each .

14

Under review as a conference paper at ICLR 2019

Step 2. Slower timescale. Note that (B.13) cannot characterize the asymptotic behavior of the
primal variable. Now we proceed to establish a finer characterization of the asymptotic behavior of {t, t}t1 by looking into the slower timescale. For ease of presentation, let Et = ( ,   t) be the -field generated by { ,   t}. In addition, we define

t(1) = -t t · At + E[t t · At | Et] t(2) = -E[t t · At | Et] + g[t, (t)], (B.14)

where function g is defined in (B.4), At and t are defined in (B.1). It holds that {t(1)}t1 is a martingale difference sequence. Note that by the definition of g, we have E[t (t) · At | Et] =
g[t, (t)]. Using the notation in (B.14), the primal update in (B.2) can be written as

t+1 =  t - t · g[t, (t)] + t · t(1) + t · t(2) .

(B.15)

As shown in the first step of the proof, (t) - t converges to zero as t goes to infinity. Moreover, under Assumption 4.1, both Q(s, a) and (s, a) are bounded. By Cauchy-Schwarz inequality,

t(2) 2 = E{t [(t) - t] · At | Et} 2

 sup (s, a) 2 · sup Q(s, a) 2 · E[ (t) - t 2 | Et],

(s,a)

(s,a)

which converges to zero almost surely. Besides, the boundedness of Q(s, a) and (s, a) also

implies that there exist a constant C > 0 such that E[

t(1)

2 2

|

Et]



C

for

all

t



1.

Furthermore, we define WT =

T t=1

t

·

t(1)



Rd for any T



1.

Since

t1 t2 < ,

{WT }T 1 is a square-integrable martingale sequence, which converges almost surely by the Mar-

tingale convergence theorem (Neveu, 1975). Moreover, Doob's martingale inequality (Doob, 1953)

implies that

lim P sup WN - WT 
T  N T

 lim supNT E[
T 

WN
2

- WT

22]  lim 2C2
T 

tT 2

t2

= 0.

To apply the Kushner-Clark lemma, we additionally need to verify that function g¯() = g[, ()] is a continuous. To see this, note that () defined in (4.1) is a continuous function, and g(, ) in (B.4) is continuous in both  and .

Finally, applying by the Kushner-Clark lemma (Kushner & Clark, 1978) to sequence {t}t1, it holds that {t}t1 converges almost surely to the set of asymptotically stable equilibria of the ODE

 = g¯() + , (t)  -C((t)),

(B.16)

where C() is the outer normal cone of . Since  is a Euclidean ball with radius R, if  is on
the boundary of , i.e.,  2 = R, we have C() = { · ,   0}. Thus, for any asymptotically stable equilibrium  of (B.16), if  is in the interior of , i.e.,  < R, we have g¯() = 0. Additionally, if  2 = Rmax, we have g¯()  C(), which implies that there exists  > 0 such that g¯() =  · . Therefore, we conclude the proof of Theorem 4.3.

B.2 PROOF OF THEOREM 4.5

Proof. In the sequel, to simplify the notation, we use C to denote absolute constant, whose value might change from line to line. In addition, for any   Rd, we define

J () = Es,a,s µ(s, a) · Q(s, a) -  · 1 a = argmax Q(s , b) · Q(s , a )

aA

bA

= E(s,a) µ(s, a) · (s, a) ,

(B.17)

where  = Q - T Q is the TD-error, and µ is the output of the optimization oracle for query .

15

Under review as a conference paper at ICLR 2019

Note that t in (4.5) is updated in the direction of an unbiased estimate of J(t). To simplify the notation, we let

t = µt (st, at) · Qt (st, at) - Qt (st, at) - J (t),

(B.18)

where J(t) is defined in (B.17), at = argmaxaA Qt (st, a). Then the update rule in (4.5) can
be written as t+1 = t - t · [J (t) + t]. By Assumptions 4.1 and 4.2, {t}t0 is a sequence of bounded and centered random vectors. Moreover, since both µ and Q are bounded by Qmax and Gmax on S × A, respectively, t defined in (B.18) is a bounded random variable satisfying t 2  4Qmax ·Gmax. Let Ft be the -field generated by {j, j  t}. Then we have E(t | Ft) = 0 and E( t 2 | Ft)  C for some constant C > 0.

Moreover, note that the gradient of J () can be written as J () = E(s,a)[(s, a)·(s, a)]. By the definition of the TD-error, under Assumption 4.1,  and  are bounded by 2Qmax and 2Gmax on S × A respectively. Moreover, since  is Lipschitz in , there exists a constant L > 0 such that J() is L-Lipschitz. Thus, we have

J()  J(t) -

J (t), t+1 - t

+ L/2 ·

t+1 - t

2 2

 J (t ) - t · J (t), J (t) + t + t2 · L/2 · J (t) + t 22.

(B.19)

Taking conditional expectation on both sides of (B.19) given Ft, since t is centered with finite variance, we obtain that

E[J (t+1) | Gt]  J (t) - t ·

J (t), J (t)

- t2 · L/2 ·

J (t)

2 2

-

C

·

t2,

(B.20)

where C > 0 is an absolute constant. Note that J() is a biased estimate of J(). Under Assumptions 4.1 and 4.4, the bias can be bounded via Cauchy-Schwarz inequality:

J () - J () 2 = E(s,a) (s, a) · [(s, a) - µ(s, a)] 2  E(s,a) (s, a) 2 ·  - µ   2Gmax · ,

(B.21)

where  is the error of the optimization oracle in Assumption 4.4. We denote J(t) - J(t) by t hereafter to simplify the notation. Moreover, by Cauchy-Schwarz inequality, we have

J (t), J (t)

=

J (t), J (t) - t



J (t)

2 2

-

J (t) 2 ·

t 2.

(B.22)

Similarly, for

J (t)

2 2

,

we

obtain

that

J (t)

2 2



J (t)

2 2

+

t

2 2

+

2

·

J (t) 2 ·

t 2.

Combining (B.20), (B.21), (B.22), and (B.23), we have

(B.23)

E[J (t+1) | Ft]

 J (t) + C · t2 - t · (1 - t · L/2) ·

J (t)

2 2

+ t · (1 + t · L) · J (t) 2 · t 2

 J (t) + C · t2 - t · (1 - t · L/2) ·

J (t)

2 2

+ t · 2(1 + t · L) · Gmax ·  · J (t) 2.

(B.24)

Since t0 t2 < , t converges to zero as t goes to infinity. When t is sufficiently large such that 4t · L < 1, (B.24) implies that

E[J (t+1) | Ft]  J (t) - t/2 ·

J (t)

2 2

+

4t

·

Gmax

·



·

J (t)

2 + C · t2

= J (t) - t/2 · J (t) 2 · J (t) 2 - 8Gmax ·  + C · t2. (B.25)

Furthermore, since J() is a nonnegative function and t1 t2 < , by (B.25) we have

t · J (t) 2 · J (t) 2 - 8Gmax ·  < .
t0

(B.26)

16

Under review as a conference paper at ICLR 2019

In the sequel, we show by contradiction that every limit point  of {t}t1 satisfies that J () 2  8Gmax · . Let  > 0 be an arbitrary number. We first show that {t : J (t) 2 <
8Gmax ·  + } is an infinite set. Suppose this is false, then there exists an integer t0 such that
J (t) 2  8Gmax ·  +  for all t  t0. Since t0 t = , we have

t · J (t) 2 · J (t) 2 - 8Gmax ·   t ·  · (8Gmax ·  + ) = ,

tt0

tt0

which contradicts (B.26). Thus, there are infinite t's satisfying J(t) 2  8Gmax + . Since  can be arbitrarily small, this implies

lim inf J (t) 2  8Gmax · .
t0

(B.27)

Let b = 8Gmax · . It remains to show that lim supt0 J (t) 2  b, which, combined with (B.27), establishes the theorem.

Suppose this argument does not hold, then for any  > 0, there exists  > 0 such that J(t) 2  b + 2 for infinitely many t. Moreover, for this particular , J(t) 2  b +  also holds for infinitely many t. We define index sets N1 and N2 by

N1 = {t : J (t) 2  b + 2}, N2 = {t : J (t) 2  b + },

which are two disjoint and closed sets by the continuity of J() 2. Moreover, we define

D(N1, N2) = inf inf  -  2,
N1  N2

(B.28)

which is a positive number since N1 and N2 are disjoint. In addition, since both N1 and N2 are infinite sets, there exists an index set I  N such that the subsequence {t}tI of {t}t0 crosses N1 and N2 infinitely often. That is, there exists two sequences {si}i1 and {ti}i1  N such that {t}tI = i1{si , si+1 , . . . , ti-1}. Furthermore, we have {si }i1  N1, {ti }i1  N2, and

b +   J ( ) 2  b + 2 for all  i1{si + 1, . . . , ti - 1}.

Hence, by triangle inequality, we have

 ti-1  

t+1 - t 2 =

 +1 -  2 

ti - si 2  D(N1, N2) = , (B.29)

tI i=1 =si i=1 i=1

where D(N1, N2) > 0 is defined in (B.28). Moreover, by (B.26), we have

 > t · J (t) 2 · J (t) 2 - b  t · (b + ) · ,
tI tI
which further implies that tI t < . However, by the update rule in (4.5), under Assumption 4.1, it holds that t+1 - t 2  t · 2Qmax · Gmax, where we use the boundedness of µ(s, a) and Q(s, a). Thus, it holds that

t+1 - t 2  2Qmax · Gmax t < ,
tI tI
which contradicts (B.29). This contradiction implies that lim supt0 J(t) 2  b. Therefore, we conclude the proof of Theorem 4.5.

C STATISTICAL ERROR
In this section, using tools from nonparametric regression, we establish the statistical error of our saddle point formulation in (3.5). To this end, we assume that the state space S is a closed compact

17

Under review as a conference paper at ICLR 2019

subset of the Euclidean space Rm and let H be the Sobolev space Wk(Rm) restricted on S. In addition, we define

HA = Q  B(S × A) : Q(·, a)  H, a  A ,

1/2

Q H=

Q(·, a)

2 Wk

aA

for any Q  HA, where · Wk is the Sobolev norm.

We consider the Bellman residual minimization problem with function class HA based on n i.i.d.
observations {(si, ai, si)}, where (si, ai)   and si is the next state. Replacing the loss function in (3.5) by its sample-based counterpart, we define

n

Ln(Q, µ) =
i=1

-1/2 · [µ(si, ai)]2 +

Q(si,

ai)

-

r(si,

ai)

-



·

max
ai A

Q(si,

ai)

· µ(si, ai)

(C.1)

=1· n 2
i=1

Q(si, ai)

-

r(si, ai)

-



·

max
ai A

Q(si

,

ai

)

2

-

Q(si, ai) - µ(si, ai) - r(si, ai) - 

·

max
ai A

Q(si,

ai)

2

.

Hence, if we denote Q - µ in (C.1) by h  HA, Ln(Q, µ) becomes

Ln(Q, h)

=

1 2

·

n

i=1

Q(si, ai) - r(si, ai) - 

·

max
ai A

Q(si

,

ai

)

2

-

h(si, ai)

-

r(si, ai)

-



·

max
ai A

Q(si

,

ai

)

2

.

(C.2)

Therefore, the sample-based sample-based functional optimization problem

min max Ln(Q, h) - µ · h H + Q · Q H
QHA hHA

(C.3)

reduces to the batch Bellman Residual Minimization algorithm studied in Antos et al. (2008b);

Farahmand et al. (2009; 2016). Here µ and Q in (C.3) are two positive regularization parameters, and we adopt regularization to avoid overfitting. Moreover, since µ = Q - h, the optimization

problem in (C.3) is equivalent to

min max Ln(Q, h) - µ · Q - µ H + Q · Q H.
QHA µHA

(C.4)

Let (Q, µ) be the solution of (C.4). Using the theoretical result in Farahmand et al. (2009; 2016), we obtain the statistical rate of Q, which is stated in the following theorem. Theorem C.1. Besides Assumption 3.1, we further make the following assumptions.

(a). The Sobolev space W k(Rm) satisfies that 2k > m.
(b). We assume that any Q  HA satisfies that |Q(s, a)|  Qmax with Qmax  Rmax/(1 - ). Moreover, HA contains the optimal value function Q.

(c). For any Q  HA, there exists positive constants K1 and K2 such that T Q H  K1 + K2 · Q H.

Moreover, let  = m/(2k). We set the regularization parameters in (C.4) to be

µ = Q =

n·

Q

2 H

-1/(1+).

Let (Q, µ) be the solution of (C.4). Then for any   (0, 1), we have

Q - Q

2 



n-1/(1+) (1 - )2

·

C1(Q, K1, K2) · log(1/) + C2(Q, K1, K2)

with probability at least 1 - , where we define C1(Q, K1, K2) = C · (K1 + K22) ·

Q

2/(1+) H

and C2(Q, K1, K2) = C · K12 · (1 + Q H-2/(1+)) for some absolute constant C > 0.

18

Under review as a conference paper at ICLR 2019

Proof. The proof follows from the results in Farahmand et al. (2009; 2016). Note that the functional optimization problems in (C.4) and (C.3) are equivalent. Applying Theorem 11 in Farahmand et al. (2016) to the problem in (C.3), we obtain that

Q - TQ

2 



n-1/(1+)

·

C1(Q, K1, K2) · log(1/) + C2(Q, K1, K2)

(C.5)

with probability at least 1 - . In addition, since Q is the unique fixed point of T , by triangle inequality, we have

Q - Q   Q - T Q  + T Q - T Q   Q - T Q  +  · Q - Q ,

where the last inequality holds since T  is -contractive. Thus it holds that Q - Q   (1 - )-1 · Q - T Q . Therefore, by (C.5), we obtain that

Q - Q

2 



(1

-

)-2

·

Q - T Q

2 

 n-1/(1+) · (1 - )-2 · C1(Q, K1, K2) · log(1/) + C2(Q, K1, K2) ,

which concludes the proof of Theorem C.1.

D KUSHNER-CLARK LEMMA

We state here the well-known Kushner-Clark Lemma (Kushner & Clark, 1978; Metivier & Priouret, 1984; Prasad et al., 2014) in the sequel.
Let  be an operator that projects a vector onto a compact set X  RN . Define a vector (·) as

[x + h(x)] - x

[h(x)] = lim

,

0<0



for any x  X and with h : X  RN continuous. Consider the following recursion in N dimensions

xt+1 =  xt + t[h(xt) + t + t] .

(D.1)

The ODE associated with (D.1) is given by

x = [h(x)].

(D.2)

Assumption D.1. We make the following assumptions:

· h(·) is a continuous RN -valued function.

· The sequence {t}, t  0 is a bounded random sequence with t  0 almost surely as t  .

· The stepsizes t, t  0 satisfy t  0 as t   and t t = .

· The sequence t, t  0 satisfies for any > 0

n

lim P
t

sup
nt

  
 =t 2

= 0.

Then the Kushner-Clark Lemma says the following.
Theorem D.2. Under Assumption D.1, suppose that the ODE (D.2) has a compact set K as its set of asymptotically stable equilibria. Then xt in (D.1) converges almost surely to K as t  .

19

