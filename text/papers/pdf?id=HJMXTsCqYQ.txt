Under review as a conference paper at ICLR 2019
CONSTRAINED BAYESIAN OPTIMIZATION FOR AUTOMATIC CHEMICAL DESIGN
Anonymous authors Paper under double-blind review
ABSTRACT
Automatic Chemical Design provides a framework for generating novel molecules with optimized molecular properties. The current model suffers from the pathology that it tends to produce invalid molecular structures. By reformulating the search procedure as a constrained Bayesian optimization problem, we showcase improvements in both the validity and quality of the generated molecules. We demonstrate that the model consistently produces novel molecules ranking above the 90th percentile of the distribution over training set scores across a range of objective functions. Importantly, our method suffers no degradation in the complexity or the diversity of the generated molecules.
1 INTRODUCTION
There are two fundamental ways in which Machine Learning can be leveraged in chemical design:
1. To evaluate a molecule for a given application. 2. To find a promising molecule for a given application.
There has been much progress in the first use-case through the development of quantitative structure activity relationship (QSAR) models using deep learning (Ma et al., 2015). These models have achieved state-of-the-art results in predicting properties of known molecules.
The second use-case, finding new molecules that are useful for a given application, is arguably more important however. One existing approach for finding molecules that maximise an applicationspecific metric involves searching a large library of compounds, either physically or virtually PyzerKnapp et al. (2015). This has the disadvantage that the search is not open-ended; if the molecule is not in the library you specify, the search won't find it.
A second method involves the use of genetic algorithms. In this approach, a known molecule acts as a seed and a local search is performed over a discrete space of molecules. Although these methods have enjoyed success in producing biologically active compounds, an approach featuring a search over an open-ended, continuous space would be beneficial. The use of geometrical cues such as gradients to guide the search in continuous space could accelerate both drug (Pyzer-Knapp et al., 2015; Go´mez-Bombarelli et al., 2016a) and material (Hachmann et al., 2011; 2014) discovery by functioning as a high-throughput virtual screen of unpromising candidates.
Recently, Go´mez-Bombarelli et al. (Go´mez-Bombarelli et al., 2016b) presented Automatic Chemical Design, a VAE architecture capable of encoding continuous representations of molecules. In continuous latent space, gradient-based optimization is leveraged to find molecules that maximize a design metric.
Although a strong proof of concept, Automatic Chemical Design possesses a deficiency in so far as it fails to generate a high proportion of valid molecular structures. Go´mez-Bombarelli et al. (2016b) hypothesise that molecules selected by Bayesian Optimization lie in "dead regions" of the latent space far away from any data that the VAE has seen in training, yielding invalid structures when decoded. Although there have been many attempts to address the issue of generating valid molecules, they all rely on model assumptions that can negatively impact either the complexity (Jaques et al., 2017; Janz et al., 2017; Kusner et al., 2017) or the diversity (Jin et al., 2018) of the generated molecules.
1

Under review as a conference paper at ICLR 2019
Figure 1: The SMILES and one-hot encoding for benzene. For simplicity only the characters present in Benzene are shown in the one-hot encoding. In reality there would be a column for each character in the SMILES alphabet.
Figure 2: The SMILES Variational Autoencoder with the learned constraint function illustrated by a circular feasible region in the latent space.
The principle contribution of this paper will be to present an approach based on constrained Bayesian optimization that generates a high proportion of valid sequences without relying on model assumptions that affect the complexity or the diversity of the generated molecules.
2 METHODS
2.1 SMILES REPRESENTATION SMILES strings Weininger (1988) are a means of representing molecules as a character sequence. This text-based format facilitates the use of tools from natural language processing for applications such as organic chemistry reaction prediction Schwaller et al. (2017); Jin et al. (2017). For use with the variational autoencoder, the SMILES strings in turn are converted into one-hot vectors indicating the presence or absence of a particular character within a sequence as illustrated in Figure 1. 2.2 VARIATIONAL AUTOENCODER Variational autoencoders Kingma & Welling (2013); Kingma et al. (2014) allow us to map molecules x to and from continuous values z in a latent space. The encoding z is interpreted as a latent variable in a probabilistic generative model over which there is a prior distribution p(z). The probabilistic decoder is defined by the likelihood function p(x|z). The posterior distribution p(z|x) is interpreted as the probabilistic encoder. The parameters of the likelihood p(x|z) as well as the parameters of the approximate posterior distribution q(z|x) are learned by maximizing the evidence lower bound (ELBO)
L(, ; x) = Eq(z|x)[log p(x, z) - log q(z|x)]. Variational autoencoders have been coupled with recurrent neural networks by Bowman et al. (2015) to encode sentences into a continuous latent space. This approach is followed for the SMILES format both by Go´mez-Bombarelli et al. (2016b) and here. The SMILES variational autoencoder, together with our constraint function, is shown in Figure 2.
2

Under review as a conference paper at ICLR 2019
2.3 OBJECTIVE FUNCTIONS FOR BAYESIAN OPTIMIZATION OF MOLECULES
Bayesian Optimization is performed in the latent space of the variational autoencoder in order to find molecules that score highly under a specified objective function. We assess molecular quality on the following objectives:
JcloogmPp(m) = logP(m) - SA(m) - ring-penalty(m),
JcQoEmDp(m) = QED(m) - SA(m) - ring-penalty(m),
J QED(m) = QED(m).
m denotes a molecule, logP(m) is the water-octanol partition coefficient, QED(m) is the quantitative estimate of drug-likeness (Bickerton et al., 2012) and SA(m) is the synthetic accessibility (Ertl & Schuffenhauer, 2009). The ring penalty term is as featured in (Go´mez-Bombarelli et al., 2016b). The "comp" subscript is designed to indicate that the objective function is a composite of standalone metrics. It is important to note, that the first objective, a common metric of comparison in this area Go´mezBombarelli et al. (2016b); Kusner et al. (2017); Jin et al. (2018), is mis-specified. From a chemical standpoint it is undesirable to maximize the logP score as is being done here. Rather it is preferable to optimize logP to be in a range that is in accordance with the Lipinski Rule of Five Lipinski et al. (1997). We use the penalized logP objective here because regardless of its relevance for chemistry, it serves as a point of comparison against other methods.
2.4 CONSTRAINED BAYESIAN OPTIMIZATION OF MOLECULES
We now describe our extension to the Bayesian Optimization procedure followed by Go´mezBombarelli et al. (2016b). Expressed formally, the constrained optimization problem is
max f (m) s.t. Pr(C(m))  1 - 
m
where f (m) is a black-box objective function, Pr(C(m)) denotes the probability that a boolean constraint C(m) is satisfied and 1 -  is some user-specified minimum confidence that the constraint is satisfied (Gelbart et al., 2014). The constraint is that a latent point must decode successfully a large fraction of the times decoding is attempted. The black-box objective function is noisy because a single latent point may decode to multiple molecules when the model makes a mistake, obtaining different values under the objective. In practice, f (m) is one of the objectives described in section 2.3
2.5 EXPECTED IMPROVEMENT WITH CONSTRAINTS (EIC)
EIC may be thought of as expected improvement (EI),
EI(m) = Ef(m) max(0,  - f (m)) ,
that offers improvement only when a set of constraints are satisfied (Schonlau et al., 1998):
EIC(m) = EI(m) Pr(C(m)).
The incumbent solution  in EI(m), may be set in an analogous way to vanilla expected improvement (Gelbart, 2015) as either:
1. The best observation in which all constraints are observed to be satisfied. 2. The minimum of the posterior mean such that all constraints are satisfied.
3

Under review as a conference paper at ICLR 2019

The latter approach is adopted for the experiments performed in this paper. If at the stage in the Bayesian optimization procedure where a feasible point has yet to be located, the form of acquisition function used is that defined by (Gelbart, 2015)

EIC(m) =

Pr(C(m)EI(m), if m, Pr(C(m))  1 - 

Pr(C(m)),

otherwise

with the intuition being that if the probabilistic constraint is violated everywhere, the acquisition function selects the point having the highest probability of lying within the feasible region. The algorithm ignores the objective until it has located the feasible region. It would also have been possible to adopt the methodologies of Rainforth et al. (2016); Mueller et al. (2017) under the assumption that the constraint is cheap to evaluate.

3 RELATED WORK
Go´mez-Bombarelli et al. (2016b); Segler et al. (2017) constructed character-level generative models of SMILES strings using recurrent decoders. These models both suffered from the problem that they produced a high proportion of invalid SMILES strings. Blaschke et al. (2017) compared a range of autoencoder architectures in terms of their ability to produce valid molecules. Kusner et al. (2017); Dai et al. (2018) addressed the issue of generating valid molecules explicitly by imposing syntactic and semantic constraints using context free and attribute grammars while Janz et al. (2017); Guimaraes et al. (2017) leveraged additional training signal to enforce molecular validity.
A potential drawback of these methods is that the constraints may favour the generation of simple molecules. Simonovsky & Komodakis (2018); Li et al. (2018) build generative models of molecular graphs but do not impose any constraints for molecular validity. Jin et al. (2018) build a generative model of molecular graphs with constraints in order to achieve 100% validity. One possible disadvantage of this approach is that each molecule is assumed to be constructed from a fixed vocabulary of subgraphs and so it will be impossible to generate a molecule comprised of subgraphs outside this vocabulary. De Cao & Kipf (2018) use a graph-based generative model which enforces a quality constraint in the latent space. Direct comparison with the aforementioned methods is complicated due to the orthogonality of the assumptions made for each model. As such, we compare out method against Go´mez-Bombarelli et al. (2016b) only.
Our principle contribution is in showing that valid molecules can be generated without encoding assumptions into the design model that limit the complexity and diversity of the generated molecules.

4 EXPERIMENT I: DRUG DESIGN
In this section we conduct an empirical test of the hypothesis from (Go´mez-Bombarelli et al., 2016b) that the decoder's lack of efficiency is due to data point collection in "dead regions" of the latent space far from the data on which the VAE was trained. We use this information to construct a binary classification Bayesian Neural Network (BNN) to serve as a constraint function that outputs the probability of a latent point being valid. Secondly, we compare the performance of our constrained Bayesian optimization implementation against the original model (baseline) in terms of the numbers of valid and drug-like molecules generated. Thirdly, we compare the quality of the molecules produced by constrained Bayesian optimization with those of the baseline model.
4.1 IMPLEMENTATION
The implementation details of the encoder-decoder network as well as the sparse GP for modelling the objective remain unchanged from (Go´mez-Bombarelli et al., 2016b). For the constrained Bayesian optimization algorithm, the BNN is constructed with 2 hidden layers each 100 units wide with ReLU activation functions and a logistic output. Minibatch size is set to 1000 and the network is trained for 5 epochs with a learning rate of 0.0005. 20 iterations of parallel Bayesian optimization are performed using the Kriging-Believer algorithm in all cases. Data is collected in batch sizes of 50. The same training set as (Go´mez-Bombarelli et al., 2016b) is used, namely 249, 456 drug-like molecules drawn at random from the ZINC database (Irwin et al., 2012).

4

Under review as a conference paper at ICLR 2019

(a) % Valid Molecules

(b) % Methane Molecules

(c) % Drug-like Molecules

Figure 3: Experiments on 5 disjoint sets comprising 50 latent points each. Very small (VS) Noise are training data latent points with approximately 1% noise added to their values, Small (S) Noise have 10% noise added to their values and Big (B) Noise have 50% noise added to their values. All latent points underwent 500 decode attempts and the results are averaged over the 50 points in each set. The percentage of decodings to: a) valid molecules b) methane molecules. c) drug-like molecules.

4.2 DIAGNOSTIC EXPERIMENTS AND LABELLING CRITERIA
These experiments were designed to test the hypothesis that points collected by Bayesian optimization lie far away from the training data in latent space. In doing so, they also serve as labelling criteria for the data collected to train the BNN acting as the constraint function. The resulting observations are summarized in Figure 3.
There is a noticeable decrease in the percentage of valid molecules decoded as one moves further away from the training data in latent space. Points collected by Bayesian optimization do the worst in terms of the percentage of valid decodings. This would suggest that these points lie farthest from the training data. The decoder over-generates methane molecules when far away from the data. One hypothesis for why this is the case is that methane is represented as 'C' in the SMILES syntax and is by far the most common character. Hence far away from the training data, combinations such as 'C' followed by a stop character may have high probability under the distribution over sequences learned by the decoder.
Given that methane has far too low a molecular weight to be a suitable drug candidate, a third plot in Figure 3(c), shows the percentage of decoded molecules such that the molecules are both valid and have a tangible molecular weight. The definition of a tangible molecular weight was interpreted somewhat arbitrarily as a SMILES length of 5 or greater. Henceforth, molecules that are both valid and have a SMILES length greater than 5 will be loosely referred to as drug-like. This is not to imply that a molecule comprising five SMILES characters is likely to be drug-like, but rather this SMILES length serves the purpose of determining whether the decoder has been successful or not.
As a result of these diagnostic experiments, it was decided that the criteria for labelling latent points to initialize the binary classification neural network for the constraint would be the following: if the latent point decodes into drug-like molecules in more than 20% of decode attempts, it should be classified as drug-like and non drug-like otherwise.
4.3 MOLECULAR VALIDITY
The BNN for the constraint was initialized with 117, 440 positive class points and 117, 440 negative class points. The positive points were obtained by running the training data through the decoder assigning them positive labels if they satisfied the criteria outlined in the previous section. The negative class points were collected by decoding points sampled uniformly at random across the 56 latent dimensions of the design space. Each latent point undergoes 100 decode attempts and the most probable SMILES string is retained. JcloogmPp(m) is the choice of objective function. The relative performance of constrained Bayesian optimization and unconstrained Bayesian optimization (baseline) (Go´mez-Bombarelli et al., 2016b) is compared in 4(a).
The results show that greater than 80% of the latent points decoded by constrained Bayesian optimization produce drug-like molecules compared to less than 5% for unconstrained Bayesian optimization. One must account however, for the fact that the constrained approach may be decoding
5

Under review as a conference paper at ICLR 2019

(a) Drug-like Molecules

(b) New Drug-like Molecules

Figure 4: a) The percentage of latent points decoded to drug-like molecules. The results are from 20 iterations of Bayesian optimization with batches of 50 data points collected at each iteration (1000 latent points decoded in total). The standard error is given for 5 separate train/test set splits of 90/10.

(a) Composite LogP

(b) Composite QED

(c) QED
Figure 5: The best scores for new molecules generated from the baseline model (blue) and the model with constrained Bayesian optimization (red). The vertical lines show the best scores averaged over 5 separate train/test splits of 90/10. For reference, the histograms are presented against the backdrop of the top 10% of the training data in the case of Composite LogP and QED, and the top 20% of the training data in the case of Composite QED.
multiple instances of the same novel molecules. Constrained and unconstrained Bayesian optimization are compared on the metric of the percentage of unique novel molecules produced in 4(b). One may observe that constrained Bayesian optimization outperforms unconstrained Bayesian optimization in terms of the generation of unique molecules, but not by a large margin. A manual inspection of the SMILES strings collected by the unconstrained optimization approach showed that there were many strings with lengths marginally larger than the cutoff point, which is suggestive of partially decoded molecules. As such, a fairer metric for comparison should be the quality of the new molecules produced as judged by the scores from the black-box objective function. This is examined next.
6

Under review as a conference paper at ICLR 2019

Table 1: Percentile of the averaged new molecule score relative to the training data. The results of 5 separate train/test set splits of 90/10 are provided.

OBJECTIVE
LOGP COMPOSITE QED COMPOSITE QED

BASELINE
36± 14 14± 3 11± 2

CONSTRAINED
92± 4 72± 10 79± 4

Figure 6: The best scores for novel molecules generated by the constrained Bayesian optimization model optimizing for PCE. The results are averaged over 3 separate runs with train/test splits of 90/10.
4.4 MOLECULAR QUALITY
The results of Figure 5 indicate that constrained Bayesian optimization is able to generate higher quality molecules relative to unconstrained Bayesian optimization across the three drug-likeness metrics introduced in section 2.3. Over the 5 independent runs, the constrained optimization procedure in every run produced new drug-like molecules ranked in the 100th percentile of the distribution over training set scores for the JcloogmPp(m) objective and over the 90th percentile for the remaining objectives. Table 1 gives the percentile that the averaged score of the new molecules found by each process occupies in the distribution over training set scores.
5 EXPERIMENT II: MATERIAL DESIGN
In order to show that the constrained Bayesian optimization approach is extensible beyond the realm of drug design, we trained the model on data from the Harvard Clean Energy Project (Hachmann et al., 2011; 2014) to generate molecules optimized for power conversion efficiency (PCE).
5.1 IMPLEMENTATION
A neural network was trained to predict the PCE of 200, 000 molecules drawn at random from the Harvard Clean Energy Project dataset using 512-bit Morgan circular fingerprints Rogers & Hahn (2010) as input features with bond radius of 2 using RDKit. If unmentioned the details of the implementation remain the same as section 4.
5.2 RESULTS
The results are given in Figure 6. The averaged score of the new molecules generated lies above the 90th percentile in the distribution over training set scores. Given that the objective function in this instance was learned using a neural network, advances in predicting chemical properties from data Duvenaud et al. (2015); Ramsundar et al. (2015) are liable to yield concomitant improvements in the optimized molecules generated through this approach.
7

Under review as a conference paper at ICLR 2019
6 CONCLUSION AND FUTURE WORK
6.1 CONTRIBUTIONS The reformulation of the search procedure in the Automatic Chemical Design model as a constrained Bayesian optimization problem has led to concrete improvements on two fronts:
1. Validity - The number of valid molecules produced by the constrained optimization procedure offers a marked improvement over the original model. Notably, by applying constraints solely to the latent space of the variational autoencoder, the method does not require model assumptions that compromise either the complexity or the diversity of the generated molecules.
2. Quality - For five independent train/test splits, the scores of the best molecules generated by the constrained optimization procedure consistently ranked above the 90th percentile of the distribution over training set scores for all objectives considered. The ability to find new molecules that are competitive with the very best of a training set of already drug-like molecules is a powerful demonstration of the model's capabilities. As a further point, the generality of the approach should be emphasised. This approach is liable to work for a large range of objectives encoding countless desirable molecular properties.
6.2 FUTURE WORK Future work could investigate whether performance gains can be achieved through the implementation of a more accurate constraint model. Recent work by Blaschke et al. (2017); Polykovskiy et al. (2018); Tabor et al. (2018); Aumentado-Armstrong (2018); Sanchez-Lengeling & Aspuru-Guzik (2018) has featured a more targeted search for novel compounds. This represents a move towards more industrially-relevant objective functions for Bayesian Optimization which should ultimately replace the chemically mis-specified objectives, such as the penalized logP score, identified here.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Tristan Aumentado-Armstrong. Latent molecular optimization for targeted therapeutic design. arXiv preprint arXiv:1809.02032, 2018.
R. G. Bickerton, G. V. Paolini, J. Besnard, S. Muresan, and A. L. Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90­98, 2012.
Thomas Blaschke, Marcus Olivecrona, Ola Engkvist, Ju¨rgen Bajorath, and Hongming Chen. Application of generative autoencoder in de novo molecular design. Molecular informatics, 2017.
S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
T. D. Bui, J. Yan, and R. E. Turner. A unifying framework for sparse gaussian process approximation using power expectation propagation. arXiv preprint arXiv:1605.07066, 2016.
Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoencoder for structured data. arXiv preprint arXiv:1802.08786, 2018.
Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973, 2018.
D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Go´mez-Bombarelli, T. Hirzel, A. AspuruGuzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In Proceedings of the 28th International Conference on Neural Information Processing Systems Volume 2, NIPS'15, pp. 2224­2232, Cambridge, MA, USA, 2015. MIT Press.
P. Ertl and A. Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1(1):8, 2009.
M. A. Gelbart. Constrained Bayesian Optimization and Applications. PhD thesis, Harvard University, 2015.
M. A. Gelbart, J. Snoek, and R. P. Adams. Bayesian optimization with unknown constraints. arXiv preprint arXiv:1403.5607, 2014.
David Ginsbourger, Rodolphe Le Riche, and Laurent Carraro. Kriging is well-suited to parallelize optimization. In Computational intelligence in expensive optimization problems, pp. 131­162. Springer, 2010.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249­256, Chia Laguna Resort, Sardinia, Italy, 2010. PMLR.
R. Go´mez-Bombarelli, J. Aguilera-Iparraguirre, T. D. Hirzel, D. Duvenaud, D. Maclaurin, M. A. Blood-Forsythe, H. S. Chae, M. Einzinger, D-G. Ha, T. Wu, and G. Markopoulos. Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach. Nature materials, 15(10):1120­1127, 2016a.
R. Go´mez-Bombarelli, D. Duvenaud, J. M. Herna´ndez-Lobato, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. arXiv preprint arXiv:1610.02415, 2016b.
Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Pedro Luis Cunha Farias, and Ala´n AspuruGuzik. Objective-reinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint arXiv:1705.10843, 2017.
J. Hachmann, R. Olivares-Amaya, S. Atahan-Evrenk, C. Amador-Bedolla, R. S. Snchez-Carrera., A. Gold-Parker, L. Vogt, A. M. Brockway, and A. Aspuru-Guzik. The harvard clean energy project: Large-scale computational screening and design of organic photovoltaics on the world community grid. The Journal of Physical Chemistry Letters, 2(17):2241­2251, 2011. doi: 10. 1021/jz200866s.
9

Under review as a conference paper at ICLR 2019
J. Hachmann, R. Olivares-Amaya, A. Jinich, A. L. Appleton, M. A. Blood-Forsythe, L. R. Seress, C. Roman-Salgado, K. Trepte, S. Atahan-Evrenk, S. Er, S. Shrestha, R. Mondal, A. Sokolov, Z. Bao, and A. Aspuru-Guzik. Lead candidates for high-performance organic photovoltaics from high-throughput quantum chemistry - the harvard clean energy project. Energy Environ. Sci., 7: 698­704, 2014.
J. M. Herna´ndez-Lobato, Y. Li, M. Rowland, T. Bui, D. Herna´ndez-Lobato, and R. E. Turner. Blackbox alpha divergence minimization. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 1511­1520, New York, New York, USA, 20­22 Jun 2016. PMLR.
J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G. Coleman. Zinc: a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52(7):1757­1768, 2012.
David Janz, Jos van der Westhuizen, Brooks Paige, Matt J Kusner, and Jose Miguel HernandezLabato. Learning a generative model for validity in complex discrete structures. arXiv preprint arXiv:1712.01664, 2017.
N. Jaques, S. Gu, D. Bahdanau, J. M. Herna´ndez-Lobato, R. E. Turner, and D. Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with KL-control. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1645­1654, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR.
Wengong Jin, Connor Coley, Regina Barzilay, and Tommi Jaakkola. Predicting organic reaction outcomes with weisfeiler-lehman network. In Advances in Neural Information Processing Systems, pp. 2604­2613, 2017.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. arXiv preprint arXiv:1802.04364, 2018.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581­3589, 2014.
Matt J Kusner, Brooks Paige, and Jose´ Miguel Herna´ndez-Lobato. Grammar variational autoencoder. In International Conference on Machine Learning, pp. 1945­1954, 2017.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative models of graphs. arXiv preprint arXiv:1803.03324, 2018.
C. A. Lipinski, F. Lombardo, B. W. Dominy, and P. J. Feeney. Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. Advanced drug delivery reviews, 23(1-3):3­25, 1997.
D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1):503­528, 1989.
Junshui Ma, Robert P Sheridan, Andy Liaw, George E Dahl, and Vladimir Svetnik. Deep neural nets as a method for quantitative structure­activity relationships. Journal of chemical information and modeling, 55(2):263­274, 2015.
Jonas Mueller, David Gifford, and Tommi Jaakkola. Sequence to better sequence: continuous revision of combinatorial structures. In International Conference on Machine Learning, pp. 2536­ 2544, 2017.
10

Under review as a conference paper at ICLR 2019
Daniil Polykovskiy, Alexander Zhebrak, Dmitry Vetrov, Yan Ivanenkov, Vladimir Aladinskiy, Marine Bozdaganyan, Polina Mamoshina, Alex Aliper, Alex Zhavoronkov, and Artur Kadurin. Entangled conditional adversarial autoencoder for de-novo drug discovery. Molecular pharmaceutics, 2018.
E. O. Pyzer-Knapp, C. Suh, R. Go´mez-Bombarelli, J. Aguilera-Iparraguirre, and A. Aspuru-Guzik. What is high-throughput virtual screening? a perspective from organic materials discovery. Annual Review of Materials Research, 45:195­216, 2015.
Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A Osborne, and Frank Wood. Bayesian optimization for probabilistic programs. In Advances in Neural Information Processing Systems, pp. 280­288, 2016.
B. Ramsundar, S. Kearnes, P. Riley, D. Webster, D. Konerding, and V. Pande. Massively Multitask Networks for Drug Discovery. ArXiv e-prints, February 2015.
RDKit. Rdkit: Open-source cheminformatics. URL http://www.rdkit.org. D. Rogers and M. Hahn. Extended-connectivity fingerprints. Journal of chemical information and
modeling, 50(5):742­754, 2010. Benjamin Sanchez-Lengeling and Ala´n Aspuru-Guzik. Inverse molecular design using machine
learning: Generative models for matter engineering. Science, 361(6400):360­365, 2018. M. Schonlau, W. J. Welch, and D. R. Jones. Global versus local search in constrained optimization
of computer models. Lecture Notes-Monograph Series, pp. 11­25, 1998. Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, and Teodoro Laino. " found in
translation": Predicting outcome of complex organic chemistry reactions using neural sequenceto-sequence models. arXiv preprint arXiv:1711.04810, 2017. Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS Central Science, 2017. Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational autoencoders. arXiv preprint arXiv:1802.03480, 2018. Daniel P Tabor, Lo¨ic M Roch, Semion K Saikin, Christoph Kreisbeck, Dennis Sheberla, Joseph H Montoya, Shyam Dwaraknath, Muratahan Aykol, Carlos Ortiz, Hermann Tribukait, et al. Accelerating the discovery of materials for clean energy in the era of smart automation. Nat. Rev. Mater., 3:5­20, 2018. Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, 2016. David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31­36, 1988.
11

Under review as a conference paper at ICLR 2019

(a) Minima Locations

(b) Disk Constraint

Figure 7: Constrained Bayesian optimization of the 2D Branin-Hoo Function.

A TOY EXPERIMENT: THE BRANIN-HOO FUNCTION
The Branin-Hoo function will act as a toy problem on which to test the functionality of the algorithmic implementation for constrained Bayesian optimization. The particular variant of the Branin-Hoo optimization of interest here is the constrained formulation of the problem as featured in Gelbart et al. (2014). This Branin-Hoo function has three global minima at the coordinates (-, 12.275), (, 2.275) and (9.42478, 2.475). In order to formulate the problem as a constrained optimization problem, a disk constraint on the region of feasible solutions is introduced. In contrast to the formulation of the problem in Gelbart et al. (2014), the disk constraint is coupled in this scenario in the sense that the objective and the constraint will be evaluated jointly at each iteration of Bayesian optimization. In addition, the observations of the black-box objective function will be assumed to be noise-free. The minima of the Branin-Hoo function as well as the disk constraint are illustrated in Figure 7.
The disk constraint eliminates the upper-left and lower-right solutions, leaving a unique global minimum at (, 2.275). Given that our implementation of constrained Bayesian optimization relies on the use of a sparse GP as the underlying statistical model of the black-box objective and as such is designed for scale as opposed to performance, the results will not be compared directly against those of Gelbart et al. (2014) who use a full GP to model the objective. It will be sufficient to compare the performance of the algorithm against random sampling. Both the sequential Bayesian optimization algorithm and the parallel implementation using the Kriging-Believer algorithm Ginsbourger et al. (2010) are tested.

A.1 IMPLEMENTATION

A Sparse GP featuring the FITC approximation, based on the implementation of Bui et al. (2016) is used to model the black-box objective function. The kernel choice is exponentiated quadratic with ARD lengthscales. The number of inducing points M was chosen to be 20 in the case of sequential Bayesian optimization, and 5 in the case of parallel Bayesian optimization using the Kriging-Believer algorithm. The sparse GP is trained for 400 epochs using Adam Kingma & Ba (2014) with the default parameters and a learning rate of 0.005. The minibatch size is chosen to be 5. The extent of jitter is chosen to be 0.00001. A Bayesian Neural Network (BNN), adapted from the MNIST digit classification network of Herna´ndez-Lobato et al. (2016) is trained using black-box alpha divergence minimization to model the constraint.

The network has a single hidden layer with 50 hidden units, Gaussian activation functions and lo-

gistic output units. The mean parameters of q, the approximation to the true posterior, are initialized

by

sampling from

a zero-mean

Gaussian with

variance

2 din +dout

according to

the

method

of

Glorot

& Bengio (2010), where din is the dimension of the previous layer in the network and dout is the

dimension of the next layer in the network. The value of  is taken to be 0.5, minibatch sizes are

taken to be 10 and 50 Monte Carlo samples are used to approximate the expectations with respect to

q in each minibatch. The BNN adapted from Herna´ndez-Lobato et al. (2016) was implemented in

the Theano library Theano Development Team (2016). The LBFGs method Liu & Nocedal (1989)

was used to optimize the EIC acquisition function in all experiments.

12

Under review as a conference paper at ICLR 2019

(a) Collected Data Points

(b) Objective Predictive Mean

(c) Pr(C(x)  0)
Figure 8: a) Data points collected over 40 iterations of sequential Bayesian optimization. b) Contour plot of the predictive mean of the sparse GP used to model the objective function. Lighter colours indicate lower values of the objective. c) The contour learned by the BNN giving the probability of constraint satisfaction.

Figure 9: Performance of Parallel Bayesian Optimization with EIC against Random Sampling.
A.2 RESULTS
The results of the sequential constrained Bayesian optimization algorithm with EIC are shown in Figure 8. The algorithm was initialized with 50 labeled data points drawn uniformly at random from the grid depicted. 40 iterations of Bayesian optimization were carried out. The figures show that the algorithm is correctly managing to collect data in the vicinity of the single feasible minimum. Figure 9 compares the performance of parallel Bayesian optimization using the Kriging-Believer algorithm against the results of random sampling. Both algorithms were initialized using 10 data points drawn uniformly at random from the grid on which the Branin-Hoo function is defined and were run for 10 iterations of Bayesian optimization. At each iteration a batch of 5 data points was collected for evaluation. After 10 iterations, the minimum feasible value of the objective function was 0.42 for parallel Bayesian optimization with EIC using the Kriging-Believer algorithm and 2.63 for random sampling. The true minimum feasible value is 0.40.
13

Under review as a conference paper at ICLR 2019 It is very likely that the parallel constrained Bayesian optimization algorithm experienced a serendipitous initialisation on the single run shown. A.3 DISCUSSION The Branin-Hoo experiment is designed to yield some visual intuition for the constrained Bayesian Optimization implementation in two dimensions before moving to higher dimensional molecular space. The results demonstrate that the implementation of constrained Bayesian optimization is behaving as expected in so far as the constraint in the problem is recognized and the search procedure outperforms random sampling. It could be worth performing some investigation into how much worse the sparse GP performs relative to the full GP in the constrained setting. Another aspect that could be explored is the impact of the initialization.
14

