Under review as a conference paper at ICLR 2019
ON THE CONVERGENCE OF A CLASS OF ADAM-TYPE ALGORITHMS FOR NON-CONVEX OPTIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the "Adam-type", includes the popular algorithms such as Adam (Kingma & Ba, 2014), AMSGrad (Reddi et al., 2018) , AdaGrad (Duchi et al., 2011). Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving non-convex problems remains an open question.
In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order O(log T / T ) for non-convex stochastic optimization. Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum). We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge. Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily monitor the progress of algorithms and determine their convergence behavior.
1 INTRODUCTION
First-order optimization has witnessed tremendous progress in the last decade, especially to solve machine learning problems (Bottou et al., 2018). Almost every first-order method obeys the following generic form (Boyd & Vandenberghe, 2004), xt+1 = xt - tt, where xt denotes the solution updated at the tth iteration for t = 1, 2, . . . , T , T is the number of iterations, t is a certain (approximate) descent direction, and t > 0 is some learning rate. The most well-known first-order algorithms are gradient descent (GD) for deterministic optimization (Nesterov, 2013; Cartis et al., 2010) and stochastic gradient descent (SGD) for stochastic optimization (Zinkevich, 2003; Ghadimi & Lan, 2013), where the former determines t using the full (batch) gradient of an objective function, and the latter uses a simpler but more computationally-efficient stochastic (unbiased) gradient estimate.
Recent works have proposed a variety of accelerated versions of GD and SGD (Nesterov, 2013). These achievements fall into three categories: a) momentum methods (Nesterov, 1983; Polyak, 1964; Ghadimi et al., 2015) which carefully design the descent direction t; b) adaptive learning rate methods (Becker et al., 1988; Duchi et al., 2011; Zeiler, 2012; Dauphin et al., 2015) which determine good learning rates t, and c) adaptive gradient methods that enjoy dual advantages of a) and b). In particular, Adam (Kingma & Ba, 2014), belonging to the third type of methods, has become extremely popular to solve deep learning problems, e.g., to train deep neural networks. Despite its superior performance in practice, theoretical investigation of Adam-like methods for non-convex optimization is still missing.
Very recently, the work (Reddi et al., 2018) pointed out the convergence issues of Adam even in the convex setting, and proposed AMSGrad, a corrected version of Adam. Although AMSGrad has made a positive step towards understanding the theoretical behavior of adaptive gradient methods, the convergence analysis of (Reddi et al., 2018) was still very restrictive because it only works for convex problems, despite the fact that the most successful applications are for non-convex problems. Apparently, there still exists a large gap between theory and practice. To the best of our knowledge, the question that whether adaptive gradient methods such as Adam, AMSGrad, AdaGrad converge for non-convex problems is still open in theory.
1

Under review as a conference paper at ICLR 2019

After the non-convergence issue of Adam has been raised in (Reddi et al., 2018), there have been a few recent works on proposing new variants of Adam-type algorithms. In the convex setting, reference (Huang et al., 2018) proposed to stabilize the coordinate-wise weighting factor to ensure convergence. Reference (Chen & Gu, 2018) developed an algorithm that changes the coordinatewise weighting factor to achieve better generalization performance. Concurrent with this work, several works are trying to understand performance of Adam in non-convex optimization problems. Reference (Basu et al., 2018) provided convergence rate of original Adam and RMSprop under full-batch (deterministic) setting, and (Ward et al., 2018) proved convergence rate of a modified version of AdaGrad where coordinate-wise weighting is removed. Furthermore, the work (Zhou et al., 2018) provided convergence results for AMSGrad that exhibit a tight dependency on problem dimension compared to (Reddi et al., 2018). The works (Zou & Shen, 2018) and (Li & Orabona, 2018) proved that both AdaGrad and its variant (AdaFom) converge to a stationary point with a high probability. The aforementioned works are independent of ours. In particular, our analysis is not only more comprehensive (it covers the analysis of a large family of algorithms in a single framework), but more importantly, it provides insights on how oscillation of stepsizes can affect the convergence rate.

Contributions Our work aims to build the theory to understand the behavior for a generic class of adaptive gradient methods for non-convex optimization. In particular, we provide mild sufficient conditions that guarantee the convergence for the Adam-type methods. We summarize our contribution as follows.
· (Generality) We consider a class of generalized Adam, referred to as the "Adam-type", and we show for the first time that under suitable conditions about the stepsizes and algorithm parameters, this class of algorithms all converge to first-order stationary solutions of the non-convex problem, with O(log T / T ) convergence rate. This class includes the recently proposed AMSGrad (Reddi et al., 2018), AdaGrad (Duchi et al., 2011), and stochastic heavy-ball methods as well as two new algorithms explained below.

· (AdaFom) Adam adds momentum to both the first and the second moment estimate, but this leads to possible divergence (Reddi et al., 2018). We show that the divergence issue can actually be fixed by a simple variant which adds momentum to only the first moment estimate while using the same second moment estimate as that of AdaGrad, which we call AdaFom (AdaGrad with First Order Moment).
· (Constant Momemtum) Our convergence analysis is applicable to the constant momentum parameter setting for AMSGrad and AdaFom. The divergence example of Adam given in (Reddi et al., 2018) is for constant momentum parameter, but the convergence analysis of AMSGrad in (Reddi et al., 2018) is for diminishing momentum parameter. This discrepancy leads to a question whether the convergence of AMSGrad is due to the algorithm form or due to the momentum parameter choice ­ we show that the constant-momentum version of AMSGrad indeed converges, thus excluding the latter possibility.

· (Practicality) The sufficient conditions we derive are simple and easy to check in practice. They can be used to either certify the convergence of a given algorithm for a class of problem instances, or to track the progress and behavior of a particular realization of an algorithm.

· (Tightness and Insight) We show the conditions are essential and "tight", in the sense that violating them can make an algorithm diverge. Importantly, our conditions provide insights on how oscillation of a so-called "effective stepsize" (that we define later) can affect the convergence rate of the class of algorithms. We also provide interpretations of the convergence conditions to illustrate why under some circumstances, certain Adam-type algorithms can outperform SGD.

Notations We use z x y is element-wise

= x/y to denote element-wise division if product, x2 is element-wise square if x is a

x and yare vector, x

both vectors of is element-wise

size d; square

root if x is a vector, (x)j denotes jth coordinate of x, x is x 2 if not otherwise specified. We use [N ] to denote the set {1, · · · , N }, and use O(·), o(·), (·), (·) as standard asymptotic notations.

2 PRELIMINARIES AND ADAM-TYPE ALGORITHMS

Stochastic optimization is a popular framework for analyzing algorithms in machine learning due to the popularity of mini-batch gradient evaluation. We consider the following generic problem where we are minimizing a function f , expressed in the expectation form as follows

min f (x) = E[f (x; )],
xRd

(1)

2

Under review as a conference paper at ICLR 2019

where  is a certain random variable representing randomly selected data sample or random noise.

In a generic first-order optimization algorithm, at a given time t we have access to an unbiased noisy gradient gt of f (x), evaluated at the current iterate xt. The noisy gradient is assumed to be bounded and the noise on the gradient at different time t is assumed to be independent. An important assumption that we will make throughout this paper is that the function f (x) is continuously differentiable and has Lipschitz continuous gradient, but could otherwise be a non-convex function. The non-convex assumption represents a major departure from the convexity that has been assumed in recent papers for analyzing Adam-type methods, such as (Kingma & Ba, 2014) and (Reddi et al., 2018).

Our work focuses on the generic form of exponentially weighted stochastic gradient descent method presented in Algorithm 1, for which we name as generalized Adam due to its resemblance to the original Adam algorithm and many of its variants.

Algorithm 1. Generalized Adam
S0. Initialize m0 = 0 and x1 For t = 1, · · · , T , do
S1. mt = 1,tmt-1 + (1 - 1,t)gt S2. vt = ht(g1, g2, ..., gt) S3. xt+1 = xt - tmt/ vt End
In Algorithm 1, t is the step size at time t, 1,t > 0 is a sequence of problem parameters, mt  Rd denotes some (exponentially weighted) gradient estimate, and vt = ht(g1, g2, ..., gt)  Rd takes all the past gradients as input and returns a vector of dimension d, which is later used to inversely weight the gradient estimate mt. And note that mt/ vt  Rd represents element-wise division. Throughout the paper, we will refer to the vector t/ vt as the effective stepsize.

We highlight that Algorithm 1 includes many well-known algorithms as special cases. We summarize some popular variants of the generalized Adam algorithm in Table 1.

Table 1: Variants of generalized Adam

1,t vt

1,t = 0

1,t  1,t-1

1,t

---
t

b



0

1,t = 1

vt = 1

SGD

N/A

Heavy-ball method

vt

=

1 t

t i=1

gi2

vt = 2vt-1 + (1 - 2)gt2,

vt = max(vt-1, vt)

AdaGrad AMSGrad

AdaFom AMSGrad

AdaFom AMSGrad

vt = 2vt-1 + (1 - 2)gt2 RMSProp

N/A

Adam

 N/A stands for an informal algorithm that was not defined in literature.

We present some interesting findings for the algorithms presented in Table 1. · Adam is often regarded as a "momentum version" of AdaGrad, but it is different from AdaFom which is also a momentum version of AdaGrad 1. The difference lies in the form of
vt. Intuitively, Adam adds momentum to both the first and second order moment estimate, while in AdaFom we only add momentum to the first moment estimate and use the same
second moment estimate as AdaGrad. These two methods are related in the following way: if we let 2 = 1 - 1/t in the expression of v^t in Adam, we obtain AdaFom. We can view AdaFom as a variant of Adam with an increasing sequence of 2, or view Adam as a variant of AdaFom with exponentially decaying weights of gt2. However, this small change has large impact on the convergence: we prove that AdaFom can always converge under standard
assumptions (see Corollary 3.2) , while Adam is shown to possibly diverge (Reddi et al.,
2018).

· The convergence of AMSGrad using a fast diminishing 1,t such that 1,t 

1,t-1, 1,t

---
t

b, b

=

0

in

convex

optimization

was

studied

in

(Reddi

et

al.,

2018).

1AdaGrad with first order momentum is also studied in (Zou & Shen, 2018) which appeared online after our first version

3

Under review as a conference paper at ICLR 2019

However, the convergence of the version with constant 1 or strictly positive b and the version for non-convex setting are unexplored before our work (an independent work (Zhou et al., 2018) also proved convergence of AMSGrad with constant 1).

It is also worth mentioning that Algorithm 1 can be applied to solve the popular "finite-sum" problems

whose objective is a sum of n individual cost functions. That is,

min
xRd

n i=1

fi(x)

:=

f

(x),

(2)

where each fi : Rd  R is a smooth and possibly non-convex function. If at each time instance the

index i is chosen uniformly randomly, then Algorithm 1 still applies, with gt = fi(xt). It can also

be extended

to a

mini-batch

case

with

gt

=

1 b

iIt fi(xt), where It denotes the minibatch of

size b at time t. It is easy to show that gt is an unbiased estimator for f (x).

In the remainder of this paper, we will analyze Algorithm 1 and provide sufficient conditions under which the algorithm converges to first-order stationary solutions with sublinear rate. We will also discuss how our results can be applied to special cases of generalized Adam.

3 CONVERGENCE ANALYSIS FOR GENERALIZED ADAM

The main technical challenge in analyzing the non-convex version of Adam-type algorithms is that the actually used update directions could no longer be unbiased estimates of the true gradients. Furthermore, an additional difficulty is introduced by the involved form of the adaptive learning rate. Therefore the biased gradients have to be carefully analyzed together with the use of the inverse of exponential moving average while adjusting the learning rate. The existing convex analysis (Reddi et al., 2018) does not apply to the non-convex scenario we study for at least two reasons: first, non-convex optimization requires a different convergence criterion, given by stationarity rather than the global optimality; second, we consider constant momentum controlling parameter.

In the following, we formalize the assumptions required in our convergence analysis.

Assumptions

A1: f is differentiable and has L-Lipschitz gradient, i.e. x, y, f (x) - f (y)  L x - y . It is also lower bounded, i.e. f (x) > - where x is an optimal solution.

A2: At time t, the algorithm can access a bounded noisy gradient and the true gradient is bounded, i.e. f (xt)  H, gt  H, t > 1.
A3: The noisy gradient is unbiased and the noise is independent, i.e. gt = f (xt) + t, E[t] = 0 and i is independent of j if i = j.

Reference (Reddi et al., 2018) uses a similar (but slightly different) assumption as A2, i.e., the

bounded elements of the gradient gt   a for some finite a. The bounded norm of f (xt) in A2 is equivalent to Lipschitz continuity of f (when f is differentiable) which is a commonly used

condition in convergence analysis. This assumption is often satisfied in practice, for example it holds

for the finite sum problem (2) when each fi has bounded gradient, and gt = fi(xt) where i is sampled randomly. A3 is also standard in stochastic optimization for analyzing convergence.
 Our main result shows that if the coordinate-wise weighting term vt in Algorithm 1 is properly chosen, we can ensure the global convergence as well as the sublinear convergence rate of the

algorithm (to a first-order stationary solution). First, we characterize how the effective stepsize

parameters t and vt affect convergence of Adam-type algorithms.

Theorem 3.1. Suppose that Assumptions A1-A3 are satisfied, 1 is chosen such that 1  1,t, 1,t  [0, 1) is non-increasing, and for some constant G > 0, tmt/ vt  G,  t. Then Algorithm 1 yields

T

E t f (xt), f (xt)/ vt

(3)

t=1

T

E C1

tgt/

t=1

2T
vt + C2
t=2

t - vt

t-1 vt-1

T -1
+ C3
1 t=2

2
t - t-1 vt vt-1

+ C4

where C1, C2, C3 are constants independent of d and T , C4 is a constant independent of T , the expectation is taken with respect to all the randomness corresponding to {gt}.

4

Under review as a conference paper at ICLR 2019

 Further, let t := minj[d] min{gi}ti=1 t/( vt)j denote the minimum possible value of effective stepsize at time t over all possible coordinate and past gradients {gi}ti=1. Then the convergence rate of Algorithm 1 is given by

min E
t[T ]

f (xt) 2 = O

s1(T ) s2(T )

,

(4)

where s1(T ) is defined through the upper bound of RHS of (3), namely, O(s1(T )), and

T t=1

t

=

(s2(T )).

Proof: See Appendix 6.2.



In Theorem 3.1, tmt/ vt  G is a mild condition. Roughly speaking, it implies that the change

of xt at each each iteration should be finite. As will be evident later, with gt  H, the condition

tmt/ vt  G is satisfied for both AdaGrad and AMSGrad. Besides, instead of bounding the

minimum norm of f in (4), we can also apply a probabilistic output (e.g., select an output xR with

probability p(R = t) =

t
T t=1

t

)

to

bound

E[

f (xR)

2] (Ghadimi & Lan, 2013; Lei et al., 2017).

We will provide a detailed explanation of Theorem 3.1 in Section 3.1.

Theorem 3.1 implies a sufficient condition that guarantees convergence of the Adam-type methods:
s1(T ) grows slower than s2(T ). We will show in Section 3.2 that the rate s1(T ) can be dominated by different terms in different cases, i.e. the non-constant quantities Term A and B below

T
E tgt/
t=1

2T
vt +
t=2

t - t-1

T -1
+

t

-

t-1

2
= O(s1(T )),

vt

vt-1 1 t=2

vt

vt-1

(5)

Term A

Term B

where the growth of third term at LHS of (5) can be directly related to growth of Term B via the relationship between 1 and 2 norm or upper boundedness of (t/ vt)j.

3.1 EXPLANATION OF CONVERGENCE CONDITIONS

From (4) in Theorem 3.1, it is evident that s1(T ) = o(s2(T )) can ensure proper convergence of the algorithm. This requirement has some important implications, which we discuss below.

· E

([ThTte=1Boutngdts/fovrts12(]T=)

and o(

s2(T )) First,

T t=1

t).

This

the is a

requirement that s1(T ) = o(s2(T )) common condition generalized from

implies that SGD. Term

A in (5) is a generalization of the term

T t=1

t2

for

SGD

(where

{t}

is

the

stepsize

sequence

for

SGD), and it quantifies possible increase in the objective function brought by higher order curvature.

The term

T t=1

t

is

the

lower

bound

on

the

summation

of

effective

stepsizes,

which

reduces

to

T t=1

t

when

Algorithm

1

is

simplified

to

SGD.

· (Oscillation of Effective Stepsizes) Term B in (5) characterizes the oscillation of effective stepsizes

t/ vt. In our analysis such an oscillation termupper bounds the expected possible ascent in

objective induced by skewed update direction gt/ vt ("skewed" in the sense that E[gt/ vt] is

not parallel with f (xt)), therefore it cannot be too large. Bounding this term is critical, and to

demonstrate this fact, in Section 3.2.2 we show that large oscillation can result in non-convergence of

Adam for even simple unconstrained non-convex problems.

· (Advantage of Adaptive Gradient). One possible benefit of adaptive gradient methods can be

seen from Term A. When this term dominates the convergence speed in Theorem 3.1, it is possible

that proper design of vt can help reduce this quantity compared with SGD (An example is provided in

Appendix 6.1.1 to further illustrate this fact.) in certain cases. Intuitively, adaptive gradient methods

like AMSGrad can provide a flexible choice of stepsizes, since vt can have a normalization effect

to reduce oscillation and overshoot introduced by large stepsizes. At the same time, flexibility of

stepsizes makes the hyperparameter tuning of an algorithm easier in practice.

3.2 TIGHTNESS OF THE RATE BOUND (4)
In the next, we show our bound (4) is tight in the sense that there exist problems satisfying Assumption 1 such that certain algorithms belonging to the class of Algorithm 1 can diverge due to the high growth rate of Term A or Term B.

5

Under review as a conference paper at ICLR 2019

3.2.1 NON-CONVERGENCE OF SGD AND ADAM DUE TO EFFECT OF TERM A

We demonstrate the importance of Term A in this subsection. Consider a simple one-dimensional optimization problem minx f (x), with f (x) = 100x2 if |x| <= b, and f (x) = 200b|x| - 100b2 if
|x| > b, where b = 10. In Figure 1, we show the growth rate of different terms given in Theorem

3.1, where 0 0, t = 0.01 for t  1, and 1,t = 0, 2,t = 0.9 for both Adam and AMSGrad.

We observe is because

that
T t=1

bothtgSt/GDvat n2d

Adam are not converging to grows with the same rate as

a stationary solution (x = 0), which accumulation of effective stepsizes as

shown in the figure. Actually, SGD only converges when t < 0.01 and our theory provides an

perspective of why SGD diverges when t  0.01. In the example, Adam is also not converging to

0 due to Term A. From our observation, Adam oscillates for any constant stepsize within [10-4, 0.1]

for this problem and Term A always ends up growing as fast as accumulation of effective stepsizes,

which implies Adam only converges with diminishing stepsizes even in non-stochastic optimization.

In contrast to SGD and Adam, AMSGrad converges in this case since both Term A and Term B

grow slower than accumulation of effective stepsizes. For AMSGrad and Adam, vt has a strong normalization effect and it allows the algorithm to use a larger range of t. The practical benefit of this flexible choice of stepsizes is easier hyperparameter tuning, which is consistent with the impression

of practitioners about the original Adam. We present more experimental results in Appendix 6.1.1

accompanied with more detailed discussions.

Figure 1: A toy example to illustrate effect of Term A on Adam, AMSGrad, and SGD.

3.2.2 NON-CONVERGENCE OF ADAM DUE TO EFFECT OF TERM B

Next, we use an example to demonstrate the importance of the Term B for the convergence of Adam-type algorithms.

Consider optimization problem minx f (x) =

11 i=1

fi(x)

where

fi(x) =

I[i = 1]5.5x2 + I[i = 1](-0.5x2), I[i = 1](11|x| - 5.5) + I[i = 1](-|x| + 0.5),

if |x|  1 if |x| > 1

(6)

and I[1 = 1] = 1, I[1 = 1] = 0. It is easy to verify that the only point with f (x) = 0 is x = 0.
The problem satisfies the assumptions in Theorem 3.1 as the stochastic gradient gt = fi(xt) is sampled uniformly for i  [11]. We now use the AMSGrad and Adam to optimize x, and the

6

Under review as a conference paper at ICLR 2019

results
T t=1
where

are given in Figure

t/ vt - t-1/

we recall that

T t=1

v2tt,-/w1hv1etrieniswTaeenrmsuepBtpegrtrbo=wous1nw,dioth1f,tth=etT=s0a1m,aetnirdnatTe2ha,etso=reTtm0=.113..1tW./AesvoatbsfroeesrrvuAeltd,tahwmaet,

obtain O(s1(T )/s2(T )) = o(1) in (4), implying the non-convergence of Adam. Our theoretical

analysis matches the empirical results in Figure 2. In contrast, AMSGrad converges in Figure 2

because of its smaller oscillation that the importance of the quantity

in

eTtf=f1ectivte/stevpts-izest,-a1s/socviat-te1d

with Term B. We finally remark 1 is also noticed by (Huang et al.,

2018). However, they did not analyze its effect on convergence, and their theory is only for convex

optimization.

Figure 2: A toy example to illustrate effect of Term B on Adam and AMSGrad.
3.3 CONVERGENCE OF AMSGRAD AND ADAFOM Theorem 3.1 provides a general approach for the design of the weighting sequence {vt} and the convergence analysis of Adam-type algorithms. For example, SGD specified by Table 1 with stepsizes t = 1/ t yields O(log T / T ) convergence speed by Theorem 3.1. Moreover, the explanation on the non-convergence of Adam in (Reddi et al., 2018) is consistent with our analysis in Section 3.2. That is, Term B in (5) can grow as fast as s2(T ) so that s1(T )/s2(T ) becomes a constant. Further, we notice that Term A in (5) can also make Adam diverge which is unnoticed before. Aside from checking convergence of an algorithm, Theorem 3.1 can also provide convergence rates of AdaGrad and AMSGrad, which will be given as corollaries later. Our proposed convergence rate of AMSGrad matches the result in (Reddi et al., 2018) for stochastic convex optimization. However, the analysis of AMSGrad in (Reddi et al., 2018) is constrained to diminishing momentum controlling parameter 1,t. Instead, our analysis is applicable to the more popular constant momentum parameter, leading to a more general non-increasing parameter setting. In Corollary 3.1 and Corollary 3.2, we derive the convergence rates of AMSGrad (Algorithm 3 in Appendix 6.2.3) and AdaFom (Algorithm 4 in Appendix 6.2.4), respectively. Note that AdaFom is more general than AdaGrad since when 1,t = 0, AdaFom becomes AdaGrad.
7

Under review as a conference paper at ICLR 2019

Corollary 3.1. For AMSGrad (Algorithm 3 in Appendix 6.2.3) with 1,t  1  [0, 1) and 1,t is non-increasing, t = 1/ t, we have for any T ,

min E f (xt) 2  1 (Q1 + Q2 log T )

t[T ]

T

(7)

where Q1 and Q2 are two constants independent of T .

Proof: See Appendix 6.2.3.

Corollary 3.2. For AdaFom (Algorithm 4 in Appendix 6.2.4) with 1,t  1  [0, 1) and 1,t is non-increasing, t = 1/ t, we have for any T ,

min E
t[T ]

f (xt) 2



1 T

(Q1

+ Q2 log T )

(8)

where Q1 and Q2 are two constants independent of T .

Proof: See Appendix 6.2.4.

4 EMPIRICAL PERFORMANCE OF ADAM-TYPE ALGORITHMS ON MNIST
Here, we empirically compare the popular Adam-type algorithms including AMSGrad, Adam, AdaGrad to verify their performance. We consider a convolutional neural network (CNN), which includes 3 convolutional layers and 2 fully-connected layers. In convolutional layers, we adopt filters of sizes 6 × 6 × 1 (with stride 1), 5 × 5 × 6 (with stride 2), and 6 × 6 × 12 (with stride 2), respectively. In both AMSGrad2 and Adam, we set 1 = 0.9 and 2 = 0.99, and choose 50 as the mini-batch size. The stepsize is choose to be t = 0.0001 + 0.003e-t/2000.
Figure 3 shows the training loss and the classification accuracy versus the number of iterations. As we can see, AMSGrad performs quite similarly to Adam which confirms the result in (Reddi et al., 2018). The performance of AdaGrad is a little worse than other algorithms under the parameter setting in the experiment which might be a result of significantly different choice of vt and/or lack of momentum.

(a) Training loss versus iterations

(b) Testing accuracy versus iterations

Figure 3: Comparison of AMSGrad, Adam and AdaGrad under MNIST in training loss and testing accuracy.

5 CONCLUSION AND DISCUSSION
We provided some mild conditions to ensure convergence of a class of Adam-type algorithms, which includes Adam, AMSGrad, AdaGrad, AdaFom, SGD, SGD with momentum as special cases. Apart from providing general convergence guarantees for algorithms, our conditions can also be checked in practice to monitor empirical convergence. To the best of our knowledge, the convergence of Adam-type algorithm for non-convex problems was unknown before. We also provide insights on how oscillation of effective stepsizes can affect convergence rate for the class of algorithms which could be beneficial for the design of future algorithms. This paper focuses on unconstrained non-convex optimization problems, and one future direction is to study a more general setting of constrained non-convex optimization.
2We customized our algorithms based on the open source code https://github.com/taki0112/ AMSGrad-Tensorflow.

8

Under review as a conference paper at ICLR 2019
REFERENCES
A. Basu, S. De, A. Mukherjee, and E. Ullah. Convergence guarantees for rmsprop and adam in non-convex optimization and their comparison to nesterov acceleration on autoencoders. arXiv preprint arXiv:1807.06766, 2018.
S. Becker, Y. Le Cun, et al. Improving the convergence of back-propagation learning with second order methods. In Proceedings of the 1988 connectionist models summer school, pp. 29­37. San Matteo, CA: Morgan Kaufmann, 1988.
J. Bernstein, Y. Wang, K. Azizzadenesheli, and A. Anandkumar. signsgd: compressed optimisation for nonconvex problems. arXiv preprint arXiv:1802.04434, 2018.
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223­311, 2018.
S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
C. Cartis, N. I. Gould, and P. L. Toint. On the complexity of steepest descent, newton's and regularized newton's methods for nonconvex unconstrained optimization problems. Siam journal on optimization, 20(6): 2833­2852, 2010.
J. Chen and Q. Gu. Closing the generalization gap of adaptive gradient methods in training deep neural networks. arXiv preprint arXiv:1806.06763, 2018.
Y. Dauphin, H. de Vries, and Y. Bengio. Equilibrated adaptive learning rates for non-convex optimization. In Advances in neural information processing systems, pp. 1504­1512, 2015.
T. Dozat. Incorporating nesterov momentum into adam. 2016.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
E. Ghadimi, H. R. Feyzmahdavian, and M. Johansson. Global convergence of the heavy-ball method for convex optimization. In 2015 European Control Conference (ECC), pp. 310­315. IEEE, 2015.
S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341­2368, 2013.
S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, 156(1-2):59­99, 2016.
H. Huang, C. Wang, and B. Dong. Nostalgic adam: Weighing more of the past gradients when designing the adaptive learning rate. arXiv preprint arXiv:1805.07557, 2018.
C. Jin, P. Netrapalli, and M. I. Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pp. 315­323, 2013.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
L. Lei, C. Ju, J. Chen, and M. I. Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pp. 2345­2355, 2017.
X. Li and F. Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. arXiv preprint arXiv:1805.08114, 2018.
A. Nemirovskii, D. B. Yudin, and E. R. Dawson. Problem complexity and method efficiency in optimization. 1983.
Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o (1/k^ 2). In Doklady AN USSR, volume 269, pp. 543­547, 1983.
Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.
P. Ochs, T. Brox, and T. Pock. ipiasco: Inertial proximal algorithm for strongly convex optimization. Journal of Mathematical Imaging and Vision, 53(2):171­181, 2015.
9

Under review as a conference paper at ICLR 2019
P. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964.
S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pp. 314­323, 2016.
S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26­31, 2012.
R. Ward, X. Wu, and L. Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
T. Yang, Q. Lin, and Z. Li. Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016.
M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. On the convergence of adaptive gradient methods for nonconvex
optimization. arXiv preprint arXiv:1808.05671, 2018. M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the
20th International Conference on Machine Learning (ICML-03), pp. 928­936, 2003. F. Zou and L. Shen. On the convergence of adagrad with momentum for training deep neural networks. arXiv
preprint arXiv:1808.03408, 2018.
10

Under review as a conference paper at ICLR 2019
6 APPENDIX
6.1 RELATED WORK
Momentum methods take into account the history of first-order information (Nesterov, 2013; 1983; Nemirovskii et al., 1983; Ghadimi & Lan, 2016; Polyak, 1964; Ghadimi et al., 2015; Ochs et al., 2015; Yang et al., 2016; Johnson & Zhang, 2013; Reddi et al., 2016; Lei et al., 2017). A well-known method, called Nesterov's accelerated gradient (NAG) originally designed for convex deterministic optimization (Nesterov, 2013; 1983; Nemirovskii et al., 1983), constructs the descent direction t using the difference between the current iterate and the previous iterate. A recent work (Ghadimi & Lan, 2016) studied a generalization of NAG for non-convex stochastic programming. Similar in spirit to NAG, heavy-ball (HB) methods (Polyak, 1964; Ghadimi et al., 2015; Ochs et al., 2015; Yang et al., 2016) form the descent direction vector through a decaying sum of the previous gradient information. In addition to NAG and HB methods, stochastic variance reduced gradient (SVRG) methods integrate SGD with GD to acquire a hybrid descent direction of reduced variance (Johnson & Zhang, 2013; Reddi et al., 2016; Lei et al., 2017). Recently, certain accelerated version of perturbed gradient descent (PAGD) algorithm is also proposed in (Jin et al., 2017), which shows the fastest convergence rate among all Hessian free algorithms.
Adaptive learning rate methods accelerate ordinary SGD by using knowledge of the past gradients or second-order information into the current learning rate t (Becker et al., 1988; Duchi et al., 2011; Zeiler, 2012; Dauphin et al., 2015). In (Becker et al., 1988), the diagonal elements of the Hessian matrix were used to penalize a constant learning rate. However, acquiring the second-order information is computationally prohibitive. More recently, an adaptive subgradient method (i.e., AdaGrad) penalized the current gradient by dividing the square root of averaging of the squared gradient coordinates in earlier iterations (Duchi et al., 2011). Although AdaGrad works well when gradients are sparse, its convergence is only analyzed in the convex world. Other adaptive learning rate methods include Adadelta (Zeiler, 2012) and ESGD (Dauphin et al., 2015), which lacked theoretical investigation although some convergence improvement was shown in practice.
Adaptive gradient methods update the descent direction and the learning rate simultaneously using knowledge in the past, and thus enjoy dual advantages of momentum and adaptive learning rate methods. Algorithms of this family include RMSProp (Tieleman & Hinton, 2012), Nadam (Dozat, 2016), and Adam (Kingma & Ba, 2014). Among these, Adam has become the most widely-used method to train deep neural networks (DNNs). Specifically, Adam adopts exponential moving averages (with decaying/forgetting factors) of the past gradients to update the descent direction. It also uses inverse of exponential moving average of squared past gradients to adjust the learning rate. The work (Kingma & Ba, 2014) showed Adam converges with at most O(1/ T ) rate for convex problems. However, the recent work (Reddi et al., 2018) pointed out the convergence issues of Adam even in the convex setting, and proposed a modified version of Adam (i.e., AMSGrad), which utilizes a non-increasing quadratic normalization and avoids the pitfalls of Adam. Although AMSGrad has made a significant progress toward understanding the theoretical behavior of adaptive gradient methods, the convergence analysis of (Reddi et al., 2018) only works for convex problems.
6.1.1 ADVANTAGES AND DISADVANTAGES OF ADAPTIVE GRADIENT METHOD
In this section, we provide some additional experiments to demonstrate how specific Adam-type algorithms can perform better than SGD and how SGD can out perform Adam-type algorithms in different situations.
One possible benefit of adaptive gradient methods is the "sparse noise reduction" effect pointed out in Bernstein et al. (2018). Below we illustrate another possible practical advantage of adaptive gradient methods when applied to solve non-convex problems, which we refer to as flexibility of stepsizes.
To highlight ideas, let us take AMSGrad as an example, and compare it with SGD. First, in nonconvex problems there can be multiple valleys with different curvatures. When using fixed stepsizes (or even a slowly diminishing stepsize), SGD can only converge to local optima in valleys with small curvature while AMSGrad and some other adaptive gradient algorithms can potentially converge to optima in valleys with relative high curvature (this may not be beneficial if one don't want to converge to a sharp local minimum). Second, the flexible choice of stepsizes implies less hyperparameter tuning and this coincides with the popular impression about original Adam.
11

Under review as a conference paper at ICLR 2019

We empirically demonstrate the flexible stepsizes property of AMSGrad using a deterministic

quadratic problem. Consider a toy optimization problem minx f (x), f (x) = 100x2, the gradient is

given by 200x. For SGD (which reduces to gradient descent in this case) to converge, we must have

t < 0.01; for AMSGrad, vt has a strong normalization effect and it allows the algorithm to use

larger t's. We show the growth rate of different terms given in Theorem 3.1 for different stepsizes in Figure A1 to Figure A4 (where we choose 1,t = 0, 2,t = 0.9 for both Adam and AMSGrad). In

Figure A1, t = 0.1 and SGD diverges due to large t, AMSGrad converges in this case, Adam is

oscillating between two non-zero points. In Figure A2, stepsizes t is set to 0.01, SGD and Adam

are oscillating, AMSGrad converges to 0. For Figure A3, SGD converges to 0 and AMSGrad is

converging slower than SGD due to its smaller effective stepsizes, Adam is oscillating. One may

wonder how diminishing stepsizes affects performance of the algorithms, this is shown in Figure A4 where t = 0.1/ t, we can see SGD is diverging until stepsizes is small, AMSGrad is converging

all the time, Adam appears to get stuck but it is actually converging very slowly due to diminishing

stepsizes. This example shows AMSGrad can converge with a larger range of stepsizes compared

with SGD.

From the figures, we can see that the term

T t=1

 tgt/ vt 2 is the key quantity that limits the

convergence speed of algorithms in this case. In Figure A1, Figure A2, and early stage of Figure

A4, the quantity is obviously a good sign of convergence speed. In Figure A3, since the difference

of quantity between AMSGrad and SGD is compensated by the larger effective stepsizes of SGD

and some problem independent constant, SGD converges faster. In fact, Figure A3 provides a case where AMSGrad does not perform well. Note that the normalization factor vt can be understood as

imitating the largest Lipschitz constant along the way of optimization, so generally speaking dividing

by this number makes the algorithm converge easier. However when the Lipschitz constant becomes smaller locally around a local optimal point, the stepsizes choice of AMSGrad dictates that vt

does not change, resulting a small effective stepsizes. This could be mitigated by AdaGrad and its

momentum variants which allows vt to decrease when gt keeps decreasing.

(a)

T t=1

 t/ vt

versus

T

(b)

T t=1

 tgt/ vt

2 versus T

(c)

T t=1

 t/ vt - t-1/

vt-1 1 ver-

sus T

(d) f (xT ) 2 versus T

Figure A1: Comparison of algorithms with t = 0.1, we defined 0 = 0

12

Under review as a conference paper at ICLR 2019

(a)

T t=1

 t/ vt

versus

T

(b)

T t=1

 tgt/ vt

2 versus T

(c)

T t=1

 t/ vt - t-1/

vt-1 1 ver-

sus T

(d) f (xT ) 2 versus T

Figure A2: Comparison of algorithms with t = 0.01, we defined 0 = 0

(a)

T t=1

 t/ vt

versus

T

(b)

T t=1

 tgt/ vt

2 versus T

(c)

T t=1

 t/ vt - t-1/

vt-1 1 ver-

sus T

(d) f (xT ) 2 versus T

Figure A3: Comparison of algorithms with t = 0.001, we defined 0 = 0

13

Under review as a conference paper at ICLR 2019

(a)

T t=1

 t/ vt

versus

T

(b)

T t=1

 tgt/ vt

2 versus T

(c)

T t=1

 t/ vt - t-1/

vt-1 1 ver-

sus T

(d) f (xT ) 2 versus T

 Figure A4: Comparison of algorithms with t = 0.1/ t, we defined 0 = 0

6.2 CONVERGENCE PROOF FOR GENERALIZED ADAM (ALGORITHM 1)
In this section, we present the convergence proof of Algorithm 1. We will first give several lemmas prior to proving Theorem 3.1.

6.2.1 PROOF OF AUXILIARY LEMMAS Lemma 6.1. Let x0 x1 in Algorithm 1, consider the sequence

zt

=

xt

+

1

1,t - 1,t

(xt

- xt-1),

t



1.

Then the following holds true

zt+1 - zt = -

1,t+1 - 1,t 1 - 1,t+1 1 - 1,t

tmt/

vt

- 1,t

t - t-1

1 - 1,t

vt

vt-1

mt-1 - tgt/ vt,  t > 1

and

z2 - z1 = -

1,2 - 1,1 1 - 1,2 1 - 1,1

1m1/

v1 - 1g1/

v1.

14

(9)

Under review as a conference paper at ICLR 2019

Proof. [Proof of Lemma 6.1] By the update rules S1-S3 in Algorithm 1, we have when t > 1,

xt+1 - xt = -tmt/ vt S=1 - t(1,tmt-1 + (1 - 1,t)gt)/ vt

S=31,t

t t-1

vt-1 vt

(xt - xt-1) - t(1 - 1,t)gt/ vt

= 1,t(xt - xt-1) + 1,t

t vt-1 - 1 t-1 vt

(xt - xt-1) - t(1 - 1,t)gt/

vt

S=31,t(xt - xt-1) - 1,t

t - t-1 vt vt-1

mt-1 - t(1 - 1,t)gt/ vt.

Since xt+1 - xt = (1 - 1,t)xt+1 + 1,t(xt+1 - xt) - (1 - 1,t)xt, based on (10) we have (1 - 1,t)xt+1 + 1,t(xt+1 - xt)

(10)

=(1 - 1,t)xt + 1,t(xt - xt-1) - 1,t

t - t-1 vt vt-1

mt-1 - t(1 - 1,t)gt/ vt.

Divide both sides by 1 - 1,t, we have

xt+1

+

1

1,t - 1,t

(xt+1

-

xt)

=xt

+

1

1,t - 1,t

(xt

-

xt-1)

-

1

1,t - 1,t

t - t-1 vt vt-1

Define the sequence Then (11) can be written as

zt

=

xt

+

1

1,t - 1,t

(xt

-

xt-1).

mt-1 - tgt/ vt.

(11)

zt+1 = zt +

1,t+1 - 1,t 1 - 1,t+1 1 - 1,t

(xt+1 - xt)

- 1,t

t - t-1

1 - 1,t

vt

vt-1

mt-1 - tgt/ vt

= zt -

1,t+1 - 1,t 1 - 1,t+1 1 - 1,t

tmt

vt

- 1,t

t - t-1

1 - 1,t

vt

vt-1

mt-1 - tgt/



where the second equality is due to xt+1 - xt = -tmt/ vt.

vt,

t > 1,

For t = 1, we have z1 = x1 (due to x1 = x0), and

z2

-

z1

=x2

+

1

1,2 - 1,2

(x2

-

x1)

-

x1

=x2 +

1,2 - 1,1 1 - 1,2 1 - 1,1

(x2

-

x1)

+

1

1,1 - 1,1

(x2

-

x1)

-

x1

=

1,2 - 1,1 1 - 1,2 1 - 1,1

(-1m1/

v1) +

1,1 + 1 1 - 1,1

(x2 - x1)

=

1,2 - 1,1 1 - 1,2 1 - 1,1

(-1m1/

v1)

+

1

1 - 1,1

(-1(1

-

1,1)g1/

=-

1,2 - 1,1 1 - 1,2 1 - 1,1

(a1m1/

v1) - 1g1/

v1,

v^1)

15

Under review as a conference paper at ICLR 2019

where the forth equality holds due to (S1) and (S3) of Algorithm 1.

The proof is now complete.

Q.E.D.

Without loss of generality, we initialize Algorithm 1 as below to simplify our analysis in what follows,

1 - 0 v1 v0

m0 = 0.

Lemma 6.2. Suppose that the conditions in Theorem 3.1 hold, then

where

6
E [f (zt+1) - f (z1)]  Ti,
i=1

T1 = -E

t i=1

f

(zi),

1

1,i - 1,i

i - i-1 vi vi-1

mi-1

t

T2 = -E

i f (zi), gi/ vi ,

i=1

t

T3 = -E

f (zi),

i=1

1,i+1 - 1,i 1 - 1,i+1 1 - 1,i

imi/

vi

t3

T4 = E

L 2

i=1

1,t+1 - 1,t 1 - 1,t+1 1 - 1,t

2 tmt/ vt ,



t3

T5 = E 

L 2

i=1

1,i 1 - 1,i

t - i-1 vi vi-1

2 mi-1  ,

, ,

t3

2

T6 = E

L 2

igi/

vi

.

i=1

Proof. [Proof of Lemma 6.2] By the Lipschitz smoothness of f , we obtain

f (zt+1)  f (zt) + f (zt), dt

L +
2

dt 2 ,

where dt = zt+1 - zt, and Lemma 6.1 together with (12) yield

dt = -

1,t+1 - 1,t 1 - 1,t+1 1 - 1,t

tmt/

vt

- 1

t - t-1

1 - 1 vt

vt-1

mt-1 - tgt/ vt, t  1.

(12)
(13)
(14) (15) (16) (17) (18) (19)
(20)
(21)

16

Under review as a conference paper at ICLR 2019

Based on (20) and (21), we then have

t

E[f (zt+1) - f (z1)] =E

f (zi+1) - f (zi)

i=1

E

t

f (zi), di

L +
2

di

2

i=1

=-E

t i=1

f (zi),

1

1,i - 1,i

i - i-1 vi vi-1

mi-1

t

- E i f (zi), gi/ vi

i=1

t
- E f (zi),
i=1

1,i+1 - 1,i 1 - 1,i+1 1 - 1,i

imi/

vi

+E

t

L 2

di 2

= T1 + T2 + T3 + +E

t

L 2

di 2

i=1 i=1

,

(22)

where {Ti} have been defined in (14)-(19). Further, using inequality a + b + c 2  3 a 2 + 3 b 2 + 3 c 2 and (22), we have

t
E di 2  T4 + T5 + T6.
i=1
Substituting the above inequality into (22), we then obtain (13).

Q.E.D.

The next series of lemmas separately bound the terms on RHS of (13).

Lemma 6.3. Suppose that the conditions in Theorem 3.1 hold, T1 in (14) can be bounded as

T1 = -E

t i=1

f

(zi),

1

1,t - 1,t



H 2

1

1 - 1

E

t

i=2

d j=1

i - vi

i - i-1 vi vi-1

i-1 vi-1



j

mi-1

Proof. [Proof of Lemma 6.3] Since gt  H, by the update rule of mt, we have mt  H, this can be proved by induction as below.

Recall that mt = 1,tmt-1 + (1 - 1,t)gt, suppose mt-1  H, we have mt  (1,t + (1 - 1,t)) max( gt , mt-1 ) = max( gt , mt-1 )  H,
then since m0 = 0, we have m0  H which completes the induction.

(23)

Given mt  H, we further have

T1 = - E

t i=2

f

(zi),

1

1,t - 1,t

i - i-1 vi vi-1


t
E  f (zi)
i=1

mi-1

1

d
-1

1 - 1,t

j=1

mi-1

i - vi

i-1 vi-1



j



H 2

1

1 - 1

E

t

i=1

d j=1

i - vi

i-1 vi-1



j

where the first equality holds due to (12), and the last inequality is due to 1  1,i.

The proof is now complete.

Q.E.D.

17

Under review as a conference paper at ICLR 2019

Lemma 6.4. Suppose the conditions in Theorem 3.1 hold. For T3 in (16), we have

t

T3 = -E

f (zi),

i=1

1,i+1 - 1,i 1 - 1,i+1 1 - 1,i

imi/

vi

 1 - 1,t+1 1 - 1 1 - 1,t+1

H2 + G2

Proof. [Proof of Lemma 6.4]

T3 E

t 1,i+1 - 1,i i=1 1 - 1,i+1 1 - 1,i

1 2

f (zi) 2 + imi/ vi 2

t
E

1,i+1 - 1,i 1 H2 + G2

i=1 1 - 1,i+1 1 - 1,i 2

t
=
i=1

1,i - 1,i+1 1 - 1,i 1 - 1,i+1

1 H2 + G2 2

 1 - 1,t+1 1 - 1 1 - 1,t+1

H2 + G2

where the first inequality is due to a, b



1 2

(

a

2+

b 2), the second inequality is using due

to upper bound on f (xt)  H and imi/ vi  G given by the assumptions in Theorem

3.1, the third equality is because 1,t  1 and 1,t is non-increasing, the last inequality is due to

telescope sum.

This completes the proof.

Lemma 6.5. Suppose the assumptions in Theorem 3.1 hold. For T4 in (17), we have

2t 3L T4 = E
i=1

1,t+1 - 1,t 1 - 1,t+1 1 - 1,t

2 tmt/ vt



1 - 1,t+1 1 - 1 1 - 1,t+1

2
G2

Q.E.D.

Proof. [Proof of Lemma 6.5] The proof is similar to the previous lemma.

2t 3L T4 =E
i=1

1,t+1 - 1,t 1 - 1,t+1 1 - 1,t

2

 tmt/ vt

2

t
E
i=1

1,t - 1,t+1

2
G2

1 - 1,t 1 - 1,t+1

 1 - 1,t+1

t

1,t - 1,t+1

G2

1 - 1 1 - 1,t+1 i=1 1 - 1,t 1 - 1,t+1



1 - 1,t+1

2
G2

1 - 1 where the first inequality is due to

1 - 1,t+1 tmt/ vt

 G by our assumptions, the second inequality is

due to non-decreasing property of 1,t and 1  1,t, the last inequality is due to telescoping sum.

This completes the proof.

Lemma 6.6. Suppose the assumptions in Theorem 3.1 hold. For T5 in (18), we have



2t 3L T5 =E 
i=1

1,i 1 - 1,i

t - i-1 vi vi-1

2 mi-1 

Q.E.D.



1 1 - 1


2 td
H2E 
i=2 j=1

t - i-1 vi vi-1

2 
j

18

Under review as a conference paper at ICLR 2019

Proof. [Proof of Lemma 6.6]



2 3L T5

E



t

i=2

1 1 - 1


2d

j=1

t - vi

i-1 vi-1

2  (mi-1)2j 
j



1 1 - 1


2 td
H2E 
i=2 j=1

t - i-1 vi vi-1

2 
j

where the fist inequality is due to 1  1,t and (12), the second inequality is due to mi < H.

This completes the proof.

Q.E.D.

Lemma 6.7. Suppose the assumptions in Theorem 3.1 hold. For T2 in (15), we have

t

T2 = - E

i f (zi), gi/ vi

i=1



t

1 2 igi/

i=2

vi 2 + L2

1 2 1 - 1

1 2 t-1

1 - 1

E igi/
i=1

+ L2H2

12 1 - 1

1 1 - 1



4d
E

t-1

i

-

j=1 i=2 vi

2 i-1
 vi-1 j

vi 2


td
+ 2H2E 
i=2 j=1

i - i-1 vi vi-1



j


dt

+ 2H2E  (1/ v1)j - E

i f (xi), f (xt)/ vi .

j=1

i=1

(24)

Proof. [Proof of Lemma 6.7] Recall from the definition (9), we have

zi

-

xi

= 1

1,i - 1,i

(xi

-

xi-1)

=

- 1

1,i - 1,i

i-1mi-1/

vi-1

(25)

Further we have z1 = x1 by definition of z1. We have

T2 = - E =-E

t
i f (zi), gi/
i=1 t
i f (xi), gi/
i=1

vi vi

t
- E i f (zi) - f (xi), gi/ vi
i=1

.

(26)

The second term of (26) can be bounded as

t

- E i f (zi) - f (xi), gi/ vi

i=1

E

t

1 2

f (zi) - f (xi)

2+ 1 2

igi/

i=2

 L2 2

T7

+

1 E
2

t

igi/ vi 2 ,

i=2

vi 2

(27)

where the first inequality is because

a, b



1 2

a 2 + b 2 and the fact that z1 = x1, the second

inequality is because

f (zi) - f (xi)  L zi - xi

=L

1

1,t - 1,t

i-1mi-1/

vi-1 ,

19

Under review as a conference paper at ICLR 2019

and T7 is defined as

T7 = E

t i=2

1

1,i - 1,i

i-1mi-1/

2
vi-1 .

(28)

We next bound the T7 in (28), by update rule mi = 1,imi-1 + (1 - 1,igi), we have mi =

ki =1[(

i l=k+1

1,l)(1

-

1,k )gk ].

Based

on

that,

we

obtain

T7 

1 1 - 1


2 td
E
i=2 j=1

i-1mi-1 vi-1

2 
j





=

1 1 - 1

2 t d i-1 i-1 E  
i=2 j=1 k=1

i-1 l=k+1

1,l

2 (1 - 1,k)gk





vi-1



j





2

1 1 - 1

2 t d i-1 k E  
i=2 j=1 k=1

i-1 l=k+1

1,l 

2 (1 - 1,k)gk
 

vk 

j

T8





+2

1 1 - 1

2 t d i-1
E  
i=2 j=1 k=1

i-1
1,l
l=k+1

(1 - 1,k)(gk)j

i-1 - k

vi-1

vk

2
 

j

T9
(29)

where the first inequality is due to 1,t  1, the second equality is by substituting expression of mt, the last inequality is because (a + b)2  2( a 2 + b 2), and we have introduced T8 and T9 for ease
of notation.

In (29), we first bound T8 as below



t d i-1 i-1
T8 =E 
i=2 j=1 k=1 p=1

k gk vk

j

i-1
1,k
l=k+1

(1 - 1,k)

pgp vp

j

i-1
1,p
q=p+1


(i) t
E 

d i-1 i-1
1i-1-k

i=2 j=1 k=1 p=1



1i-1-p

1 2

k gk vk

2
+
j

pgp vp

2 
j


t
(=ii)E 

d i-1
1i-1-k

i=2 j=1 k=1

k gk vk

2 i-1 j p=1

1i-1-p

 



(iii)


1

1 -

1

E



t i=2

d j=1

i-1 k=1

1i-1-k



k gk

2


vk j



(=iv)

1

1 -

1

E

t-1

k=1

d j=1

t i=k+1

1i-1-k



k gk

2


vk j





1 1 - 1

2 t-1 d
E
k=1 j=1

k gk

2
=

vk j

1 1 - 1

2
E

t-1
igi/
i=1

vi 2

 (1 - 1,p)
(30)

where (i) is due to ab

<

1 2

(a2

+

b2)

and

follows

from

1,t



1

and 1,t



[0, 1), (ii) is due

to symmetry of p and k in the summation, (iii) is because of

i-1 p=1

1i-1-p



1 1-1

,

(iv)

is

exchanging order of summation, and the second-last inequality is due to the similar reason as (iii).

20

Under review as a conference paper at ICLR 2019

For the T9 in (29), we have

 
t d i-1

T9

=E

 



i=2 j=1 k=1

i-1
1,k
l=k+1

(1 - 1,k)(gk)j

i-1 - k

vi-1

vk

2
 

j

 
t d i-1

H2E  



i=2 j=1 k=1

i-1
1,k
l=k+1

2

i-1 - k

 

vi-1

vk j 


t-1 d
H2E 
i=1 j=1

i
1i-k
k=1

i - k vi vk j

2 





t-1 d

i

i

H2E  

 1i-k

i=1 j=1 k=1

l=k+1

l - vl

2

l-1





vl-1 j 

(31)

where the first inequality holds due to 1,k < 1 and |(gk)j|  H, the second inequality holds due to 1,k  1, and the last inequality applied the triangle inequality. For RHS of (31), using Lemma 6.8

(that will be proved later) with ai =

i - i-1 , we further have vi vi-1 j





t-1 d

i

i

T9

H 2 E

 

 1i-k

i=1 j=1 k=1

l=k+1

l - vl

2

l-1





vl-1 j 

H 2

12 1 - 1

1 1 - 1



2d
E

t-1

l

-

j=1 i=2 vl

2 l-1
 vl-1 j

(32)

Based on (27), (29), (30) and (32), we can then bound the second term of (26) as

t
- E i f (zi) - f (xi), gi/ vi

i=1

L2

1 2 1 - 1

1 2 t-1

1 - 1

E igi/
i=1

vi 2

+ L2H2

12 1 - 1

1 1 - 1



4d
E

t-1

l

-

j=1 i=2 vl

2 l-1
 vl-1 j

1 +E
2

t

igi/ vi 2 .

i=2

(33)

Let us turn to the first term in (26). Reparameterize gt as gt = f (xt) + t with E[t] = 0, we have

t
E i f (xi), gi/ vi
i=1 t
=E i f (xi), (f (xi) + i)/ vi
i=1 tt
=E i f (xi), f (xi)/ vi + E i f (xi), i/ vi
i=1 i=1

.

(34)

21

Under review as a conference paper at ICLR 2019

It can be seen that the first term in RHS of (34) is the desired descent quantity, the second term is a bias term to be bounded. For the second term in RHS of (34), we have

t
E i f (xi), i/ vi
i=1 t
=E f (xi), i (i/ vi - i-1/
i=2
+ E 1 f (x1), 1/ v1

vi-1)

t
E f (xi), i
i=2

(i/ vi - i-1/ vi-1)

t
+ E i-1 f (xi), i
i=2

(1/ vi-1)


d
- 2H2E  (1/ v1)j
j=1

(35)

where the last equation is because given xi, vi-1, E i (1/ vi-1)|xi, vi-1 = 0 and i  2H due to gi  H and f (xi)  H based on Assumptions A2 and A3. Further, we have

t

E f (xi), t (i/ vi - i-1/ vi-1)

i=2


td

=E 

(f (xi))j (t)j (i/( vi)j - i-1/(

i=2 j=1

 vi-1)j )


td

-E

|(f (xi))j| |(t)j| (i/(

i=2 j=1

vi)j - i-1/(

 vi-1)j ) 


td

 - 2H2E 

(i/(

i=2 j=1

vi)j - i-1/(

 vi-1)j ) 

(36)

Substituting (35) and (36) into (34), we then bound the first term of (26) as

t

- E i f (xi), gi/ vi

i=1


td

2H2E 

(i/( vi)j - i-1/(

i=2 j=1

t
- E i f (xi), f (xi)/ vi
i=1


d
vi-1)j )  + 2H2E  (1/
j=1

 v1)j 

(37)

We finally apply (37) and (33) to obtain (24). The proof is now complete.

Lemma 6.8. For ai  0,   [0, 1), and bi =

i k=1

i-k

i l=k+1

al,

we

have

Q.E.D.

t
b2i 
i=1

12 1-

 1-

2

t

ai2

i=2

22

Under review as a conference paper at ICLR 2019

Proof. [Proof of Lemma 6.8] The result is proved by following

tt
bi2 =

i i2

i-k

al

i=1

i=1 k=1

l=k+1

t (i)
=

i l-1

2t

i-kal =

i l-1 2

i-l+1al

 l-1-k

i=1 l=2 k=1

i=1 l=2

k=1

(ii)


1 2t

1-

i2
i-l+1al =

1 2t 1-

ii
i-l+1ali-m+1am

i=1 l=2

i=1 l=2 m=2

(iii)


1 2t

1-

i

i

i-l+1i-m+1 1 2

a2l + am2

i=1 l=2 m=2

(=iv) 1 2 t 1-

i

i (v)
i-l+1i-m+1a2l 

1 2 t 1- 1-

t
i-l+1al2

i=1 l=2 m=2

l=2 i=l



12 1-

 1-

2t
a2l

l=2

where (i) is by changing order of summation, (ii) is due to

l-1 k=1

 l-1-k



1 1-

,

(iii) is by the

fact

that

ab



1 2

(a2

+

b2),

(iv)

is

due

to

symmetry

of

al

and

am

in

the

summation,

(v)

is

because

i m=2

 i-m+1



 1-

and

the

last

inequality

is

for

similar

reason.

This completes the proof.

Q.E.D.

6.2.2 PROOF OF THEOREM 3.1
Proof. [Proof of Theorem 3.1] We combine Lemma 6.2, Lemma 6.3, Lemma 6.4, Lemma 6.5, Lemma 6.6, and Lemma 6.7 to bound the overall expected descent of the objective. First, from Lemma 6.2, we have

6

E [f (zt+1) - f (z1)]  Ti

i=1

=-E

t i=1

f (zi),

1,i 1 - 1,i

i - i-1 vi vi-1

mi-1

t
- E i f (zi), gi/ vi
i=1

t
- E f (zi),
i=1

1,i+1 - 1,i 1 - 1,i+1 1 - 1,i

imi/

vi

t3 +E L
2
i=1

1,t+1 - 1,t 1 - 1,t+1 1 - 1,t

2 tmt/ vt



t3 +E L
2
i=1

1,i 1 - 1,i

t - i-1 vi vi-1

2 mi-1  + E

t3

L 2

igi/

i=1

2
vi

(38)

23

Under review as a conference paper at ICLR 2019
Then from above inequality and Lemma 6.3, Lemma 6.4, Lemma 6.5, Lemma 6.6, Lemma 6.7, we get

E [f (zt+1) - f (z1)]



H 2

1

1 - 1

E

t

i=2

d j=1

i - i-1 vi vi-1



j

+ 1 - 1,t+1 1 - 1 1 - 1,t+1

H2 + G2 +

1 - 1,t+1

2
G2

1 - 1 1 - 1,t+1

+

1 1 - 1


2 td
H2E 
i=2 j=1

i - i-1 vi vi-1

2 t 3

+E

L 2

igi/

j i=1

2
vi

+E

t1 2 igi/
i=2

vi 2

+

L2

1

1 - 1

1 1 - 1


2 t-1 d
E
k=1 j=1



k gk

2


vk j

+ L2H2

12 1 - 1

1 1 - 1


4 d t-1
E
j=1 i=2

i - i-1 vi vi-1

2


td

 + 2H2E 

j i=2 j=1


dt

+ 2H2E  (1/ v1)j - E

i f (xi), f (xi)/ vi

j=1

i=1

i - i-1 vi vi-1



j

By merging similar terms in above inequality, we further have

E [f (zt+1) - f (z1)]





H

2

1

1 - 1

+ 2H2

td
E
i=2 j=1

i - i-1 vi vi-1



j



+

1 + L2

12 1 - 1

1 2 1 - 1

H2

1 1 - 1

2 d t-1
E
j=1 i=2

i - i-1 vi vi-1

+

3 L
2

+

1 2

+

L2 1

1 - 1

12

t

2

1 - 1

E igi/ vi
i=1

+ 1 - 1,t+1 1 - 1 1 - 1,t+1

H2 + G2 +

1 - 1,t+1

2
G2

1 - 1 1 - 1,t+1

dt

+ 2H2E

(1/ v1)j - E

i f (xi), f (xi)/ vi

j=1

i=1

2 
j

(39)

24

Under review as a conference paper at ICLR 2019

Rearranging (39), we have

t

E i f (xi), f (xi)/ vi

i=1





H2 1

1 - 1

+

2H 2

td
E
i=2 j=1

i - i-1 vi vi-1


+
j

+

1 + L2

12 1 - 1

1 1 - 1

2
H2

1 1 - 1


2 d t-1
E
j=1 i=2

i - i-1 vi vi-1

2 
j

+

3 L

+

1

+

L2

1

22

1 - 1

12 1 - 1

t
E igi/
i=1

2
vi

+

1 - 1,t+1 1 - 1 1 - 1,t+1

H2 + G2 +

1 - 1,t+1 1 - 1 1 - 1,t+1


d
+ 2H2E  (1/ v1)j + E [f (z1) - f (zt+1)]

j=1

2
G2


t

E C1

tgt/

i=1

2t
vt + C2
i=2

i - vi

i-1 vi-1

t-1
+ C3
1 i=2

2

i - i-1 vi vi-1

 + C4

where

C1

3 L
2

+

1 2

+

L2

1

1 - 1

12 1 - 1

C2

H2 1 + 2H2 1 - 1

C3

1 + L2

1 1 - 1

2

1 1 - 1

H2

1 1 - 1

2

C4

1 1 - 1

H2 + G2 +

1

2
G2

1 - 1

+ 2H2E 1/ v1 1 + E [f (z1) - f (z)]

and z is an optimal of f , i.e. z  arg minz f (z). 
Using the fact that (i/ vi)j  i, j by definition, inequality (4) directly follows. This completes the proof.

Q.E.D.

6.2.3 PROOF OF COROLLARY 3.1
Proof. [Proof of Corollary 3.1]
Algorithm 3. AMSGrad (S0). Define m0 = 0, v0 = 0, v0 = 0; For t = 1, · · · , T , do
(S1). mt = 1,tmt-1 + (1 - 1,t)gt (S2). vt = 2vt-1 + (1 - 2)gt2 (S3). vt = max{vt-1, vt} (S4). xt+1 = xt - tmt/ vt End

25

Under review as a conference paper at ICLR 2019

We first bound non-constant terms in RHS of (3), which is given by


T

E C1

tgt/

t=1

2T
vt + C2
t=2

t - vt

t-1 vt-1

T -1
+ C3
1 t=2

2

t - t-1 vt vt-1

 + C4.

 For the term with C1, assume minj[d] ( v1)j  c > 0 (this is natural since if it is 0, division by 0 error will happen), we have

T2
E tgt/ vt

t=1

E

T
tgt/c 2
t=1

=E

T t=1

1 t

gt

/c

2

H2/c2 T 1  H2/c2(1 + log T ) t
t=1

=E

T t=1

1 ct

2

gt 2

where the first inequality is due to (vt)j  (vt-1)j, and the last inequality is due to 1 + log T .

T t=1

1/t



For the term with C2, we have

T
E
t=2

d
=E 
j=1

t - vt

t-1 vt-1 1


dT
=E
j=1 t=2

t-1 - t ( vt-1)j ( vt)j





1 - T

d
E

1

  d/c

( v1)j ( vT )j

(
j=1

v1)j

 

(40)

where the first equality is due to(vt)j  (vt-1)j and t  t-1, and the second equality is due to telescope sum.

For the term with C3, we have


T -1
E
t=2

t - vt

t-1 vt-1

2 

E 1 T -1 t -

c
t=2

vt

d/c2

 where the first inequality is due to |(t/ vt - t-1/

t-1 vt-1 1
vt-1)j |  1/c.

Then we have for AMSGRAD,


T

E C1

tgt/

t=1

2T
vt + C2
t=2

t - t-1 vt vt-1

C1H2/c2(1 + log T ) + C2d/c + C3(d/c)2 + C4

T -1
+ C3
1 t=2

t - vt

t-1 vt-1

2  + C4
(41)

Now we lower bound the effective stepsizes, since vt is exponential moving average of gt2 and gt  H, we have (vt)j  H2, we have

/(

vt)j



1 Ht

26

Under review as a conference paper at ICLR 2019

And thus
T
E i f (xt), f (xt)/ vt
t=1

E

T 1 t=1 H t

f (xt)

2





T min E

H t[T ]

f (xt) 2 (42)

Then by (3), (41) and (42), we have

1 T min E
H t[T ]

f (xt) 2  C1H2/c2(1 + log T ) + C2d/c + C3d/c2 + C4

which is equivalent to

min E f (xt) 2
t[T ]

 H C1H2/c2(1 + log T ) + C2d/c + C3d/c2 + C4 T

= 1 T

(Q1

+

Q2 log T )

 One more thing is to verify the assumption tmt/ vt  G in Theorem 3.1, since t+1/( vt+1)j  t/( vt)j and 1/( v1)j  1/c in the algorithm, we have tmt/ vt 

mt /c  H/c.

This completes the proof.

Q.E.D.

6.2.4 PROOF OF COROLLARY 3.2 Proof. [Proof of Corollary 3.2]

Algorithm 4. AdaFom
(S0). Define m0 = 0, v0 = 0; For t = 1, · · · , T , do
(S1). mt = 1,tmt-1 + (1 - 1,t)gt (S2). vt = (1 - 1/t)vt-1 + (1/t)gt2 (S3). xt+1 = xt - tmt/ vt End

The proof is similar to proof for Corollary 3.1, first let's bound RHS of (3) which is


T

E C1

tgt/

t=1

2T
vt + C2
t=2

t - vt

t-1 vt-1

T -1
+ C3
1 t=2

2

t - t-1 vt vt-1

 + C4

We tain

recall from t/ vt =

Table 1/

1 that

t i=1

gi2

in .

AdaGrad, v We assume

t

=

1 t

minj[d]

t i=1

gt2.

|(g1 )j |

Thus, c>

when t = 0, which is

 1/ t, we obequivalent to

minj[d]( v1)j  c > 0 (a requirement of the AdaGrad). For C1 term we have



T 2T

E

tgt/ vt

=E 

t=1 t=1

gt

t i=1

gi2

2 
dT
=E 
j=1 t=1

 (gt)2j ti=1(gi)j2 


dT

E 

1 - log((g1)2j ) + log (gt)2j   d(1 - log(c2) + 2 log H + log T )

j=1

t=1

where the third inequality used Lemma 6.9 and the last inequality used gt  H and minj[d] |(g1)j |  c > 0.

27

Under review as a conference paper at ICLR 2019

For C2 term we have E T t - t=2 vt

t-1 vt-1 1


dT

=E



j=1 t=2


d
=E  
j=1

1- (g1)j2

 1   d/c iT=1(gi)j2

1- it=-11(gi)j2


1 
ti=1(gi)j2

For C3 term we have


T -1
E
t=2

t - vt

t-1 vt-1

2 

E 1 T -1 t - t-1

c
t=2

vt

vt-1 1

d/c2

 where the first inequality is due to |(t/ vt - t-1/ vt-1)j|  1/c.

 Now we lower bound the effective stepsizes t/( vt)j,

t = 1  1 ,

( vt)j

ti=1(gi)2j H t



where we recall that t = 1/ t and gt  H. Following the same argument in the proof of

Corollary 3.1 and the previously derived upper bounds, we have



T min E
H t[T ]

f (xt) 2  C1d(1 - log(c2) + 2 log H + log T ) + C2d/c + C3d/c2 + C4

which yields

min E f (xt) 2
t[T ]

 H C1d(1 - log(c2) + 2 log H + log T ) + C2d/c + C3d/c2 + C4 T

= 1 T

(Q1 + Q2 log T )



The last thing is to verify the assumption tmt/ vt  G in Theorem3.1, since t+1/( vt+1)j 

t/( vt)j and 1/( v1)j  1/c in the algorithm, we have tmt/ vt  mt /c  H/c.

This completes the proof.

Lemma 6.9. For at  0 and

t i=1

ai

=

0,

we

have

Q.E.D.

T t=1

at

t i=1

ai



1 - log a1

T
+ log ai.
i=1

Proof. [Proof of Lemma 6.9] We will prove it by induction. Suppose

we have
T
t=1

at

t i=1

ai

=

T -1 t=1

at

t i=1

ai



1 - log a1

T -1
+ log ai,
i=1

aT
T i=1

ai

+

T -1 t=1

at

t i=1

ai



aT

T i=1

ai

+

1 - log a1

+

T -1
log ai.
i=1

28

Under review as a conference paper at ICLR 2019

Applying the definition of concavity to log(x), with f (z) log(z), we have f (z)  f (z0) +

f (z0)(z - z0), then substitute z = x - b, z0 = x, we have f (x - b)  f (x) + f (x)(-b) which is

equivalent to log(x)  log(x - b) + b/x for b < x, using x =

T i=1

ai,

b

=

aT

,

we

have

T T -1
log ai  log ai +
i=1 i=1

aT

T i=1

ai

and then

T t=1

at

t i=1

ai



aT

T i=1

ai

+ 1 - log a1

T -1
+ log ai
i=1



1 - log a1

T
+ log ai.
i=1

Now it remains to check first iteration. We have

a1 a1

=

1



1 - log(a1) + log(a1)

=

1

This completes the proof.

Q.E.D.

29

