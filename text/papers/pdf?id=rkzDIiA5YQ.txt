Under review as a conference paper at ICLR 2019
ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. We evaluate the method empirically using the Amazon Elastic Compute Cloud (EC2) and observe a 30 - 50% improvement in convergence speed.
1 INTRODUCTION
The advent of massive data sets has resulted in demand for solutions to optimization problems that are too large for a single processor to solve in a reasonable time. This has led to a renaissance in the study of parallel and distributed computing paradigms. Numerous recent advances in this field can be categorized into two approaches; synchronous Dekel et al. (2012); Duchi et al. (2012); Tsianos & Rabbat (2016); Zinkevich et al. (2010) and asynchronous Recht et al. (2011); Liu et al. (2015). This paper focuses on the synchronous approach. One can characterize synchronization methods in terms of the topology of the computing system, either master-worker or fully distributed. In a master-worker topology, workers update their estimates of the optimization variables locally, followed by a fusion step at the master yielding a synchronized estimate. In a fully distributed setting, nodes are sparsely connected and there is no obvious master node. Nodes synchronize their estimates via local communications. In both topologies, synchronization is a key step, and many approaches realize significant acceleration in convergence under idealized operating assumptions.
Maintaining synchronization in practical computing systems can, however, introduce significant delay. One cause is slow processing nodes, known as stragglers Dean et al. (2012); Yu et al. (2017); Tandon et al. (2017); Lee et al. (2018); Pan et al. (2017); S. Dutta & Nagpurkar (2018). A classical requirement in parallel computing is that all nodes process an equal amount of data per computational epoch prior to the initiation of the synchronization mechanism. In networks in which the processing speed and computational load of nodes vary greatly between nodes and over time, the straggling nodes will determine the processing time, often at a great expense to overall system efficiency. Such straggler nodes are a significant issue in cloud-based computing systems. Thus, an important challenge is the design of parallel optimization techniques that are robust to stragglers.
To meet this challenge, we propose an approach that we term Anytime MiniBatch (AMB). We consider a fully distributed topology1 and consider the problem of online stochastic convex optimization via dual averaging Nesterov (2009); Xiao (2010). Rather than fixing the minibatch size, we fix the
1The master-worker or "star" topology is a special network structure for which our resuls simplify substantially.
1

Under review as a conference paper at ICLR 2019

computation time (T ) in each epoch, forcing each node to "turn in" its work after the specified fixed time has expired. This prevents a single straggler (or stragglers) from holding up the entire network, while allowing nodes to benefit from the partial work carried out by the slower nodes. On the other hand, fixing the computation time means that each node process a different amount of data in each epoch. Our method adapts to this variability. After computation, all workers get fixed communication time (Tc) to share their gradient information via averaging consensus on their dual variables, accounting for the variable number of data samples processed at each node. Thus, the epoch time of AMB is fixed to T + Tc in the presence of stragglers and network delays.
 We analyze the convergence of AMB, showing that the online regret achieves the optimum of O( m¯ ) performance, where m¯ is the expected sum number of samples processed across all nodes. We further show an upper bound that, in terms of the expected wall time needed to attain a specified regret, AMB is O( n - 1) faster than methods that use a fixed minibatch size under the assumption that the computation time follows an arbitrary distribution where n is the number of nodes. Finally, we provide numerical simulations performed on Amazon Elastic Compute Cloud (EC2) that show our method is 30 - 50% faster than fixed minibatch method.

2 SYSTEM MODEL AND ALGORITHM

In this section we outline our computation and optimization model and step through the three phases of the AMB algorithm. We defer discussion of detailed mathematical assumptions and analytical results to Sec. 3.

We suppose a computing system that consists of n compute nodes. Each node corresponds to a vertex
in a connected and undirected graph G(V, E) that represents the inter-node communication structure. The vertex set V satisfies |V | = n and the edge set E tells us which nodes can communicate directly. Let Ni = {j  V : (i, j)  E} denote the neighborhood of node i.

The collaborative objective of the nodes is to find the parameter vector w  W  Rd that solves

w = arg min F (w) where F (w) := Ex[f (w, x)].
wW

(1)

The expectation Ex[·] is computed with respect to an unknown probability distribution Q over a set X  Rd. Because the distribution is unknown, the nodes must approximate the solution in (1) using data points drawn in an independent and identically distributed (i.i.d.) manner from Q.

AMB uses dual averaging Nesterov (2009); Dekel et al. (2012) as its optimization workhorse and
averaging consensus Nokleby & Bajwa (2017); Tsianos & Rabbat (2016) to facilitate collaboration
among nodes. It proceeds in epochs consisting of three phases: compute, in which nodes compute
local minibatches; consensus, in which nodes average their dual variables together; and update, in
which nodes take a dual averaging step with respect to the consensus-averaged dual variables. We let t index each epoch, and each node i has a primal variable wi(t)  Rd and dual variable zi(t)  Rd. At the start of the first epoch, t = 1, we initialize all primal variables to the same value w(1) as

wi(1)

=

w(1)

=

arg

min
wW

h(w),

(2)

and all dual variables to zero, i.e., zi(1) = 0  Rd. In here, h : W  R is a 1-strongly convex function.

Compute Phase: All workers are given T fixed time to compute their local minibatches. During

each epoch, each node is able to compute bi(t) gradients of f (w, x), evaluated at wi(t) where the data samples xi(t, s) are drawn i.i.d. from Q. At the end of epoch t, each node i computes its local

minibatch gradient:

1 bi(t)

gi(t) =

bi(t)

w f
s=1

wi(t), xi(t, s)

.

(3)

As we fix the compute time, the local minibatch size bi(t) is a random variable. Let b(t) :=

n i=1

bi

(t)

be the global minibatch size aggregated over all nodes. This contrasts with traditional approaches

in which the minibatch is fixed. In Sec. 3 we provide a convergence analysis that accounts for the

variability in the amount of work completed by each node. In Sec. 4, we presents a wall time analysis

based on random local minibatch sizes.

2

Under review as a conference paper at ICLR 2019

Consensus Phase: Between computational epochs each node is given a fixed amount of time, Tc, to communicate with neighboring nodes. The objective of this phase is for each node to get (an
approximation of) the following quantity:

1n

1n

1 n bi(t)

b(t) bi(t)[zi(t) + gi(t)] = b(t) bi(t)zi(t) + b(t)

wf wi(t), xi(t, s)

i=1 i=1 i=1 s=1

:= z¯(t) + g(t).

(4)

The first term, z¯(t), is the weighted average of the previous dual variables. The second, g(t), is the average of all gradients computed in epoch t.

The nodes compute this quantity approximately via several synchronous rounds of average consensus.
Each node waits until it hears from all neighbors before starting a consensus round. As we have fixed communication time Tc, the number of consensus rounds ri(t) varies across workers and epochs due to random network delays. Let P be a positive semi-definite, doubly-stochastic matrix (i.e., all entries of P are non-negative and all row- and column-sums are one) that is consistent with the graph G (i.e., Pi,j > 0 only if i = j or if (i, j)  E). At the start of the consensus phase, each node i shares its message mi(0) = nbi(t)[zi(t) + gi(t)] with its neighboring nodes. Specifically, in consensus iteration k  [ri(t)] node i computes its update as2

nn

m(ik) =

Pi,j m(jk-1) =

(Pi,j )kmj(0).

j=1

j=1

As long as G is connected and the second-largest eigenvalue of P is strictly less than unity, the
iterations are guaranteed to converge to the true average. For finite ri(t), each node will have an error in its approximation. Instead of (4), at the end of the rounds of consensus, node i will have

zi(t + 1) = z¯(t) + g(t) + i(t),

(5)

where i(t) is the error. We use D(ri(t)) {yj}jV , i to denote the distributed averaging affected by ri(t) rounds of consensus. Thus,

zi(t

+

1)

=

1 D(ri(t)) b(t)

nbj(t) zj(t) + gj(t)

,i
jV

=

1 b(t)

mi(ri

(t))

.

(6)

We note solution,

tnhoartmthaeliuzepddabtyedb(dtu)a=l variina=b1lebiz(it()t.

+

1)

is

a

normalized

version

of

the

distributed

average

Update Phase: After distributed averaging of dual variables, each node updates its primal variable as

wi(t

+

1)

=

arg

min
wW

w, zi(t + 1) + (t + 1)h(w) ;

(7)

where ·, · denotes the standard inner product. As will be discussed further in our analysis, in this
paper we assume h : W  R to be a 1-strongly convex function and (t) to be a sequence of positive non-decreasing parameters, i.e., (t)  (t + 1). We also work in Euclidean space where h(w) = w 2 is a typical choice.

3 ANALYSIS
In this section we analyze the performance of AMB in terms of expected regret. As the performance is sensitive to the specific distribution of the processing times of the computing platform used, we first present a generic analysis in terms of the number of epochs processed and the size of the minibatches processed by each node in each epoch. Then in Sec. 4, in order to illustrate the advantages of AMB, we assert a probabilistic model on the processing time and analyze the performance in terms of the elapsed "wall time" .
2The notation [r] stands for [r]  {1, . . . r}, which is used throughout the paper.

3

Under review as a conference paper at ICLR 2019

3.1 PRELIMINARIES

We assume that the feasible space W  Rd of the primal optimization variable w is a closed and bounded convex set where D = maxw,uW w - u . Let · denote the 2 norm. We assume the objective function f (w, x) is convex and differentiable in w  W for all x  X. We further assume
that f (w, x) is Lipschitz continuous with constant L, i.e.

|f (w, x) - f (w~, x)|  L w - w~ ,  x  X, and  w, w~  W.

(8)

Let f (w, x) be the gradient of f (w, x) with respect to w. We assume the gradient of f (w, x) is Lipschitz continuous with constant K, i.e.,

f (w, x) - f (w~, x)  K w - w~ ,  x  X, and  w, w~  W.

(9)

As mentioned in Sec. 2,

F (w) = E[f (w, x)],

(10)

where the expectation is taken with respect to the (unknown) data distribution Q, and thus F (w) = E[f (w, x)]. We also assume that there exists a constant  that bounds the second moment of the norm of the gradient so that

E[ f (w, x) - F (w) 2]  2,  x  X, and  w  W.

(11)

Let the global minimum be denoted w := arg minwW F (w).

3.2 SAMPLE PATH ANALYSIS

First we bound the consensus errors. Let z(t) be the exact dual variable without any consensus errors at each node

z(t) = z¯(t - 1) + g(t - 1).

(12)

The following Lemma bounds the consensus errors, which is obtained using (Tsianos & Rabbat, 2016, Theorem 2)

Lemma 1 Let zi(r)(t) be the output after r rounds consensus. Let 2(P ) be the second eigenvalue of the matrix P and let  0, then

zi(r)(t) - z(t)  , i  [n], t  [ ],

(13)

if the number of consensus rounds satisfies



ri(t) 

log (2 n(1 + 2L/ )) 1 - 2(P )

, i  [n], t  [ ].

(14)

We characterize the regret after  epochs, averaging over the data distribution but keeping a

fixed "sample path" of per-node minibatch sizes bi(t). We observe that due to the time spent in

communicating with other nodes via consensus, each node has computation cycles that could have

been used to compute more gradients had the consensus phase been shorter (or nonexistent). To

model this, let ai(t) denote the number of additional gradients that node i could have computed had

there been no consensus phase. This undone work does not impact the system performance, but does

enter into our characterization of the regret. Let ci(t) = bi(t) + ai(t) be the total number of gradients

that node i had the potential to compute during the t-th epoch. Therefore, the total potential data

samples processed in the t-th epoch is c(t) =

n i=1

ci(t).

After



epochs

the

total

number

of

data

points that could have been processed by all nodes in the absence of communication delays is


m = c(t).

(15)

t=1

An important quantity is the ratio of total potential computations in each epoch to that actually completed. Define the maximum such minibatch "skewness" as

c(t + 1)

 = max

.

t[ -1] b(t)

(16)

4

Under review as a conference paper at ICLR 2019

It turns out that it is important to compute this skewness across epochs (i.e., c(t + 1) versus b(t)) in order to bound the regret via a telescoping sum. [Details can be found in the supplementary material.]

In practice, ai(t) and bi(t) (and therefore ci(t)) depend on latent effects, e.g., how many other virtual machines are co-hosted on node i, and therefore we model them as random variables. We bound
the expected regret for a fixed sample path of ai(t) and bi(t). The sample paths of importance are ctot( ) = {ci(t)}iV,t[] and btot( ) = {bi(t)}iV,t[], where we introduce ctot and btot for notational compactness.

Define the average regret after  epochs as


 n ci(t)



R( ) = E R|btot( ), ctot( ) = E 

f wi(t), xi(t, s) - F (w)  ,

t=1 i=1 s=1

(17)

where the expectation is taken with respect the the i.i.d. sampling from the distribution Q. Then, we have the following bound on R( ).

Theorem 2 Suppose a sample path with m total potential data samples seen after  epochs, cf. (15),

minibatch skewness parameter , cf. (16), and let cmax = maxt[] c(t), cavg = (1/ )

 t=1

c(t)

and  = max{t,t }{1,-1} |c(t)-c(t )| be the maximum, average, and variation across c(t). Further,

suppose the averaging consensus has additive accuracy , cf. Lemma 1. Then, the expected regret is

R( )



cmax[F (w(1)) - F (w) + ( )h(w)] +

3K 2

2cmaxµ3/2 4

+

 1+
2

F (w) + h(w)

K +  1/2c-av1g/2



2 + 2KD + 2 + 2L cmax + 2KD 

 m.

(18)

Theorem 2 is proved in App. A of the supplementary material.

We now make a few comments about this result. First, recall that the expectation is taken with respect to the data distribution, but holds for any sample path of minibatch sizes. Further, the regret bound depends only on the summary statistics cmax, cavg, , and . These parameters capture the distribution of the processing speed at each node. Further, the impact of consensus error, which depends on the communication speed relative to the processing speed of each node, is summarized in the assumption of uniform accuracy on the distributed averaging mechanism. Thus, Theorem 2 is a sample path result that depends only coarsely on the distribution of the speed of data processing.

Next, observe that the dominant term is the final one, which scales in the aggregate number of samples m. The first term is approximately constant, only scaling with the monotonically increasing  and cmax parameters. The terms containing characterizes the effect of imperfect consensus, which can be reduced by increasing the number of rounds of consensus. The effect of variability across c(t) is reflected in the terms containing the cmax, cavg and  parameters. If perfect consensus were achieved ( = 0) then all components of the final term that scales in m would disappear except for the term that contains the minibatch skewness parameter . It is through this term that the amount of useful computation performed in each epoch (bi(t)  ci(t)) enters the result.

In the special case of constant minibatch size cmax = cavg and  = 0, we have the following corollary.

Corollary 3 If c(t) = c for all t  [ ] and the consensus error  1/c, then the expected regret is

R( ) = O(c + m).

(19)

Furthermore,

if

c

=

m

for

a

constant





(0,

1/2],

then

R( )

=

 O( m).

3.3 EXPECTED REGRET ANALYSIS
We can translate Theorem 2 and Cor. 3 to a regret bound averaged over the sample path. Since the summary statistics cmax, cavg, , and  are sufficient to bound the regret, we assert a joint distribution p over these terms rather than over the sample path btot( ), ctot( ). For the following result, we need only specify several moments of the distribution. In Sec. 4 we will take the further step of choosing a specific distribution p.

5

Under review as a conference paper at ICLR 2019

Theorem 4 Let c¯ = Ep[c(t)] so that m¯ =  c¯ is the expected total work that can be completed in  epochs. Also, let 1/^b = Ep[1/b(t)]. If averaging consensus has additive accuracy , then the expected regret is bounded by

Ep[R( )]  c¯ F (w(1)) - F (w) + ¯( )h(w)

3K2 2c¯5/2 ++
4

c¯2 2KD + 2^b + 2L c¯

 m¯ .

Theorem 4 is proved in App. E of the supplementary material. Note that this expected regret is over both the i.i.d. choice of data samples and the i.i.d. choice of (b(t), c(t)) pairs.

Corollary 5 If  1/c¯, the expected regret is 
Ep[R( )]  O(c¯ + m¯ ).
 Further, if c¯ = m¯  for a constant   (0, 1/2), then E[R]  O( m¯ ).

(20)

Remark 1 Note that by letting = 0, we can immediately find the results for master-worker setup.

4 WALL TIME ANALYSIS

In the preceding section we studied regret as a function of the number of epochs. The advantages of AMB is the reduction of wall time. That is, AMB can get to same convergence in less time than fixed minibatch approaches. Thus, in this section, we caracterize the wall time performance of AMB.
In AMB, each epoch corresponds to a fixed compute time T . As we have already commented, this contrasts with fixed minibatch approaches where they have variable computing times. We refer "Fixed MiniBatch" methods as FMB. To gain insight into the advantages of AMB, we develop an understanding of the regret per unit time.
We consider an FMB method in which each node computes computes b/n gradients, where b is the size of the global minibatch in each epoch. Let Ti(t) denote the amount of time taken by node i to compute b/n gradients for FMB method. We make the following assumptions:
Assumption 1 The time Ti(t) can follow any arbitrary distribution, identical across node index i and epoch index t. The mean and the variance of Ti(t) are µ and 2.
Assumption 2 If node i takes Ti(t) seconds to compute b/n gradients in the t-th epoch, then it will take nTi(t)/b seconds to compute one gradient.
Lemma 6 Let Assumptions 1 and 2 hold. Let the FMB scheme have a minibatch size of b. Let ¯b be the expected minibatch size of AMB. Then, if we fix the computation time of an epoch in AMB to T = (1 + n/b)µ, we have ¯b  b.

Lemma 6 is proved in App. F and it shows that the expected minibatch size of AMB is at least as big as FMB if we fix T = (1 + n/b)µ. Thus, we get same (or better) expected regret bound. Next, we show that AMB achieve this in less time.

Theorem 7 Let Assumptions 1 and 2 hold. Let T = (1 + n/b)µ and denote minibatch size of FMB as

b. Further, let SA and SF be the total compute time across  epochs of AMB and FMB, respectively, then

SF 

1

+



 n

-

1

µ

SA.

(21)

The proof is given in App. F. better) bound on the expected

Lemma 6 regret that

and Theorem 7 show is given in Theorem 4

that our but is at

method most 1

a+ttaµinsnth-e

same (or 1 faster

than traditional FMB methods. It was shown in Bertsimas et al. (2006), the bound (21) is tight and

achievable.

There are no analytical distribution that exactly match with finishing time distribution. Recent papers on stragglers Lee et al. (2018); S. Dutta & Nagpurkar (2018) use the shifted exponential distribution

6

Under review as a conference paper at ICLR 2019

to model Ti(t). The choice of shifted exponential distribution is motivated by the fact that it strikes a good balance between analytical tractability and practical behavior. Based on the assumption of shifted exponential distribution, we show that AMB is O(log(n)) faster than FMB. This result is
proved in App. G.

5 PREVIOUS WORK
This work contributes to the ever-growing body of literature on distributed learning and optimization, which goes back at least as far as Tsitsiklis et al. (1986), in which distributed first-order methods were considered. Recent seminal works include Nedic & Ozdaglar (2009), which considers distributed optimization in sensor and robotic networks, and Dekel et al. (2012), which considers stochastic learning and prediction in large, distributed data networks. A large body of work elaborates on these ideas, considering differences in topology, communications models, data models, etc. Duchi et al. (2012); Tsianos et al. (2012); Shi et al. (2015); Xi & Khan (2017).
The two recent works most similar to ours are Tsianos & Rabbat (2016) and Nokleby & Bajwa (2017), which consider distributed stochastic convex optimization over networks with communications constraints. However, both of these works suppose that worker nodes are homogeneous in terms of processing power, and do not account for the straggler effect examined herein.
The recent work Pan et al. (2017); Tandon et al. (2017); S. Dutta & Nagpurkar (2018) proposed synchronous fixed minibatch methods to mitigate stragglers for master-worker setup. These methods either ignore stragglers or use redundancy to accelerate convergence in the presence of stragglers.

6 NUMERICAL EVALUATION

To evaluate the performance of AMB and compare it with that of FMB, we ran several experiments on Amazon EC2 for both schemes to solve two different classes of machine learning tasks: linear regression and logistic regression using both synthetic and real datasets. In this section we present error vs. wall time performance using two experiments. Additional simulations are given in App. H

6.1 DATASETS

In our experiments, we solved two problems using two datasets: synthetic and real. Linear regression problem were solved using synthetic data. The element of global minimum parameter, w  Rd,
is generated from the multivariate normal distribution N (0, I). The workers observe a sequence of pairs (xi(s), yi(s)) where s is the time index, data xi(s)  Rd are i.i.d. N (0, I), and the labels yi(s)  R such that yi(s) = xi(s)T w + i(s). The additive noise sequence i(s)  R is assumed to be i.i.d N (0, 10-3). The aim of all nodes is to collaboratively learn the true parameter w. In our experiments, we generated data of dimension d = 10d.

For the logistic regression problem, we used the MNIST handwritten images of numbers from 0 to 9. Each image is of size 28 × 28 pixels which can be represented as a 784-dimensional vector. We used MNIST training dataset that consists of 60,000 data points.The cost function we used is the cross-entropy function J, defined as

J(y) = - 1[y = i]P(y = i|x)

(22)

i

where x is the observed data point sampled randomly from the dataset, y is the true label of
x. 1[.] is the indicator function and P(y = i|x) is the predicted probability that y = i given
the observed data point x which can be calculated using the softmax function. In other words, P(y = i|x) = ewix/ j ewjx. The aim of the system is to collaboratively learn the parameter w  Rc×d, where c = 10 classes and d = 785 the dimension (including the bias term) that minimizes
the cost function while streaming the inputs x online.

6.2 EXPERIMENT ON EC2
We tested the performance of AMB and FMB schemes using fully distributed setup. We used a network consisting of n = 10 nodes, in which the underlying network topology is given in Figure 2

7

Under review as a conference paper at ICLR 2019

Error cost

100
FMB AMB

FMB AMB

10-1

100

10-2
0 100 200 300 400 500 Time (sec)
(a) Linear reg. - distributed.

0 50 100 150 200 250 300 350 time (s)
(b) Logistic reg. - distributed.

Figure 1: AMB vs. FMB performance comparison on EC2.

of App. H.1. In all our experiments, we used t2.micro instances and ami-6b211202, a publicly available Amazon Machine Image (AMI), to launch the instances. Communication between nodes were handled through Message Passing Interface (MPI).
To ensure a fair comparison between the two schemes, we ran both algorithms repeatedly and for a long time and averaged the performance over the same duration. We also observed that the processors finish tasks much faster during the first hour or two before slowing significantly. After that initial period, workers enter a steady state in which they keep their processor speed relatively constant except for occasional bursts. We discarded the transient behaviour and considered the performance during the steady-state.
6.2.1 LINEAR REGRESSION
We ran both AMB and FMB in a fully distributed setting to solve the linear regression problem. In FMB, each worker computed b = 6000 gradients. The average compute time during the steady-state phase was found to be 14.5 sec. Therefore, in AMB case, the compute time for each worker was set to be T = 14.5 sec. and we set Tc = 4.5 sec. Workers are allowed r = 5 average rounds of consensus to average their calculated gradients.
Figure 1(a) plots the error vs. wall time, which includes both computation and communication times. One can notice AMB clearly outperforms FMB. In fact, the total amount of time spent by FMB to finish all the epochs is larger than that spent by AMB by almost 25% as shown in Figure 1(a). We notice, both scheme has the same average inter-node communication times. Therefore, when ignoring inter-node communication times, this ratio increases to almost 30%.
6.2.2 LOGISTIC REGRESSION
In here we perform logistic regression using n = 10 distributed nodes. The network topology is as same as above. The per-node fixed minibatch in FMB is b/n = 800 while the fixed compute time in AMB is T = 12 sec. and the communication time Tc = 3 sec. As in the linear regression experiment above, the workers on average go through r = 5 round of consensus.
Figures 1(b) shows the achieved cost vs. wall clock time. We observe AMB outperforms FMB by achieving the same error rate earlier. In fact, Figure 1(b) demonstrates that AMB achieves up to 50% convergence speedup compared to FMB.
7 CONCLUSION
We proposed a distributed optimization method called Anytime MiniBatch. A key property of our scheme is that we fix the computation time of each distributed node instead of minibatch size. Therefore, the finishing time of all nodes are deterministic and does not depend on the slowest processing node. We proved the convergence rate of our scheme in terms of the expected regret bound. We performed numerical experiments using Amazon EC2 and showed our scheme offers significant improvements over fixed minibatch schemes.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Barry C. Arnold and Richard A. Groeneveld. Bounds on expectations of linear systematic statistics based on dependent samples. Ann. Statist., 7(1):220­223, 01 1979. doi: 10.1214/aos/1176344567.
Dimitris Bertsimas, Karthik Natarajan, and Chung-Piaw Teo. Tight bounds on expected order statistics. Probab. Eng. Inf. Sci., 20(4):667­686, October 2006. ISSN 0269-9648.
Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems, NIPS'12, pp. 1223­1231, USA, 2012. Curran Associates Inc.
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13(Jan):165­202, 2012.
John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3): 592­606, 2012.
K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran. Speeding up distributed machine learning using codes. IEEE Trans. on Inf. Theory, 64(3):1514­1529, March 2018. ISSN 0018-9448. doi: 10.1109/TIT.2017.2736066.
Ji Liu, Stephen J. Wright, Christopher Ré, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. J. Mach. Learn. Res., 16(1):285­322, January 2015. ISSN 1532-4435.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48­61, 2009.
Yurii Nesterov. Primal-dual subgradient methods for convex problems. Math. Program., 120(1): 221­259, April 2009. ISSN 0025-5610.
M. Nokleby and W. U. Bajwa. Distributed mirror descent for stochastic learning over rate-limited networks. In 2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), pp. 1­5, Dec 2017. doi: 10.1109/CAMSAP.2017.8313171.
Xinghao Pan, Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Józefowicz. Revisiting distributed synchronous sgd. In Int. Conf. on Learning Representations Workshop Track'17, 2017.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems 24, pp. 693­701. 2011.
S. Ghosh P. Dube S. Dutta, G. Joshi and P. Nagpurkar. Slow and stale gradients can win the race: Error-runtime trade-offs in distributed SGD. In in Proc. of the Int. Conf. on Artificial Intelligence and Statistics (AISTATS), 2018.
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944­966, 2015.
R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis. Gradient Coding: Avoiding Stragglers in Distributed Learning. In in Proc. of the Int. Conf. on Int. Conf. on Machine Learning (ICML), 2017.
Konstantinos I Tsianos and Michael G Rabbat. Efficient distributed online prediction and stochastic optimization with approximate distributed averaging. IEEE Transactions on Signal and Information Processing over Networks, 2(4):489­506, 2016.
Konstantinos I Tsianos, Sean Lawlor, and Michael G Rabbat. Push-sum distributed dual averaging for convex optimization. In Decision and Control (CDC), 2012 IEEE 51st Annual Conference on, pp. 5453­5458. IEEE, 2012.
9

Under review as a conference paper at ICLR 2019 John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic
and stochastic gradient optimization algorithms. IEEE Transactions on Automatic Control, 31(9): 803­812, 1986. Chenguang Xi and Usman A Khan. Dextra: A fast algorithm for optimization over directed graphs. IEEE Transactions on Automatic Control, 62(10):4980­4993, 2017. Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11(Oct):2543­2596, 2010. Q. Yu, M. Maddah-Ali, and S. Avestimehr. Polynomial codes: An optimal design for high-dimensional coded matrix multiplication. In Advances Neural Inf. Proc. Systems, 2017. Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J. Smola. Parallelized stochastic gradient descent. In Advances in Neural Information Processing Systems 23, pp. 2595­2603. Curran Associates, Inc., 2010.
10

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 2

In this section, we prove Theorem 2. There are three factors impacting the convergence of our scheme; first is that gradient is calculated with respect to f (w, x) rather than directly computing the exact gradient wF (w), the second factor is the errors due to limited consensus rounds, and the last factor is that we have variable sized minibatch size over epochs. We bound these errors to find the expected regret bound with respect to a sample path.
Let w(t) be the primal variable computed using the exact dual z(t), cf. 12:

w(t) = arg min { w, z(t) + (t)h(w)}
wW

From (Tsianos & Rabbat, 2016, Lemma 2), we have

wi(t) - w(t)

1 (t)

zi(t) - z(t) ,

i  [n]

(23) (24)

. (t)

(25)

Recall that zi(t) is the dual variable after r rounds of consensus. The last step is due to Lemma 1. Let X(t) be the total set of samples processed by the end of t-th epoch:

X(t) := {xi(t , s) : i  [n], t  [t], s  [c(t)]} .

(26)

Let E[·] denote the expectation over the data set X( ) where we recall  is the number of epochs. Note that conditioned on X(t - 1) the wi(t) and xi(t, s) are independent according to equation 7. Thus,

E [f (wi(t), xi(t, s))] = E Exi(t,s) [f (wi(t), xi(t, s)) |X(t - 1)] = E [F (wi(t))] .
where equation 28 is due to equation 10. From equation 17 we have

(27) (28)

 n ci(t)

E R|btot( ), ctot( ) =

E f wi(t), xi(t, s) - F (w)

t=1 i=1 s=1

 n ci(t)
= E [F (wi(t)) - F (w)] .

t=1 i=1 s=1

(29) (30)

Now, we add and subtract F (w(t)) from equation 17 to get

 n ci(t)

E[R|btot( ), ctot( )] =

E [F (w(t)) - F (w) + F (wi(t)) - F (w(t))]

(31)

t=1 i=1 s=1

 n

= c(t)E[F (w(t)) - F (w)] +

ci(t)E[F (wi(t)) - F (w(t))]

t=1 t=1 i=1
(32)

 n

 c(t)E[F (w(t)) - F (w)] +

ci(t)LE[ wi(t) - w(t) ] (33)

t=1 t=1 i=1


 c(t)E[F (w(t)) - F (w)] +

n ci(t)L (t)

t=1 t=1 i=1


= c(t)E[F (w(t)) - F (w)] + L

 c(t) .
(t)

t=1 t=1

(34) (35)

Note that equation 33 and equation 34 are due to equation 8 and equation 25. Now, we bound the first term in the following Lemma, which is proved in App. B.

11

Under review as a conference paper at ICLR 2019

Lemma 8 Let (t) = K + (t) where (t) =

t µ

.

Then


c(t)E [F (w(t)) - F (w)]  c(1)[F (w(1)) - F (w)] + (c( ) - c(t))F (w)

t=1 t=2

+ c( )( )h(w) + -1 KD (t)

-1 c(t + 1)2 +
4b(t)(t)

K2 +
4

2 -1 c(t + 1) (t)(t)2

+ E[]

t=1 t=1

t=1

(36)

where

 -1
E[]  (c(t) - c(t + 1))

(t - 1)F (w)

+ (t)h(w) + 2KD

 µt

.

t=1

(37)

In equation 36, the first term is a constant, which depends on the initialization. The fourth and the sixth terms are due to consensus errors and the fifth term is due to noisy gradient calculation. The second and the last term E[] are due to variable minibatch sizes.

Now, the total regret can be obtained by using Lemma 8 in equation 35


E[R|btot( ), ctot( )]  c(1)[F (w(1)) - F (w)] + (c( ) - c(t))F (w) + c( )( )h(w)

t=2

-1 KD

-1 c(t + 1)2 K2 2 -1 c(t + 1)

 c(t)

+ ++

+

(t) 4b(t)(t) 4

(t)(t)2 + E[] + L

. (38) (t)

t=1 t=1

t=1

t=1

Define



=

maxt{1, -1}

c(t+1) b(t)

,

cmax

=

maxt[] c(t),

and



=

max{t,t

}{1, -1} |c(t)

-

c(t

)|.

Then


E[R|btot( ), ctot( )]  cmax[F (w(1)) - F (w) + ( )h(w)] + F (w)

t=2

-1 KD +
(t)

2 -1 1 +
4 (t)

+

K2

2cmax 4

 -1 1 (t)(t)2

+ E[] + L

cmax



1 (t)

t=1 t=1

t=1

t=1

(39)

In App. C, we bound

 -1 t=1

1 (t)

and

 -1 t=1

1 (t)(t)2

terms.

Using them, we have

E[R|btot( ), ctot( )]



cmax[F (w(1)) - F (w) + ( )h(w)] +  F (w) + 2KD

 µ

+

22µ 4

+

3K 2

2cmaxµ3/2 4

+ 2L

 cmax µ

+ E[]

(40)

E[R|btot( ), ctot( )]  cmax[F (w(1)) - F (w) + ( )h(w)] +  F (w)

+ 3K2 2cmaxµ3/2 + 4

2 2KD + 2 + 2L cmax

 µ + E[]

(41)

Now we bound E[]. Using  = max{t,t }{1,-1} |c(t) - c(t )| in equation 37, we can write

 -1
E[]  

(t - 1)F (w)

+ (t)h(w)

+ 2KD

 µt

t=1

 -1

 -1

  F (w) (t - 1) + h(w)

K+

t=1 t=1

t  -1



+ 2KD µt

µ

t=1

   2 F (w) + h(w) K +

  + 2KD µ 

2µ

   F (w) + h(w) K +



 + 2KD µ 

2µ

(42) (43) (44) (45)

12

Under review as a conference paper at ICLR 2019

By substituting equation 45 in equation 41

E[R|btot( ), ctot( )]  cmax[F (w(1)) - F (w) + ( )h(w)] +  F (w)

+ 3K2 2cmaxµ3/2 + 4

2 2KD + 2 + 2L cmax

 µ

+   F (w) + h(w) K +



 + 2KD µ  (46)

2µ

By rearranging terms

E[R|btot( ), ctot( )]



cmax[F (w(1)) - F (w) + ( )h(w)] +

3K 2

2cmaxµ3/2 4

+

 1+

F (w) + h(w)

K+





2µ

2  + 2KD + 2 + 2L cmax + 2KD  µ

Let µ = cavg = (1/ )

 t=1

c(t),

then

from

equation

15

µ

=

m

and

we

substitute

(47)

E[R|btot( ), ctot( )]



cmax[F (w(1)) - F (w) + ( )h(w)] +

3K 2

2cmaxµ3/2 4

+

 1+
2

F (w) + h(w)

K +  1/2c-av1g/2



2  + 2KD + 2 + 2L cmax + 2KD  m.

(48)

This completes the proof of Theorem 2.

B PROOF OF LEMMA 8

Note that g(t) is calculated with respect to wi(t) by different nodes in equation 3. Let g¯(t) be the minibatch calculated with respect to w(t) (given in equation 23) by all the nodes.

1 n bi(t)

g¯(t) = b(t)

wf (w(t), xi(t, s)).

i=1 s=1

(49)

Note that there are two types of errors in computing gradients. The first is common in any gradient based methods. That is, the gradient is calculated with respect to the function f (w, x), which is based on the data x instead of being a direct evaluation of wF (w). We denote this error as q(t):

q(t) = g¯(t) - wF (w(t)).

(50)

The second error results from the fact that we use g(t) instead of g¯(t). We denote this error as r(t):

r(t) = g(t) - g¯(t).

(51)

Lemma 9 The following four relations hold E[ q(t), w - w(t) ] = 0,

E[

r(t), w

- w(t)

]

=

KD (t)

,

E[

q(t)

2] =

2 ,
b(t)

13

(52) (53) (54)

Under review as a conference paper at ICLR 2019

E[

r(t)

2] =

K2 2 (t)2 .

(55)

The proof of Lemma 9 is given in App. D. Let lt(w) be the first order approximation of F (w) at w(t):

lt(w) = F (w(t)) + wF (w(t)), w - w(t) .

(56)

Let ~lt(w) be an approximation of lt(w) by replacing wF (w(t)) with g(t)

~lt(w) = F (w(t)) + g(t), w - w(t)

(57)

= F (w(t)) + wF (w(t)), w - w(t) + q(t), w - w(t) + r(t), w - w(t) (58)

= lt(w) + q(t), w - w(t) + r(t), w - w(t) .

(59)

Note that equation 58 follows since g(t) = q(t) + r(t) + wF (w(t)). By using the smoothness of F (w), we can write

F

(w(t

+

1))



lt(w(t

+

1))

+

K 2

w(t + 1) - w(t)

2

(60)

= ~lt(w(t + 1)) -

q(t), w - w(t)

-

r(t), w - w(t)

K +
2

w(t + 1) - w(t)

2

(61)

= ~lt(w) + q(t)

w - w(t) + r(t)

w - w(t)

K +

w(t + 1) - w(t) 2.

(62)

2

The last step is due to the Cauchy-Schwarz inequality. Let (t) = (t) - K. We add and subtract (t) w(t + 1) - w(t) 2/2 to find

F (w(t + 1))  ~lt(w(t + 1)) + q(t)

w - w(t) - (t) w(t + 1) - w(t) 2 + r(t) 4

w - w(t)

- (t) w(t + 1) - w(t) 2 + K + (t) w(t + 1) - w(t) 2. (63) 42

Note that

q(t) w - w(t) - (t) w(t + 1) - w(t) 2 4

q(t) 2 =-
4(t)

q(t) a(t)

2 q(t) 2

- w(t + 1) - w(t)  .

4(t)

4

4(t)

(64)

Similarly, we have that

r(t)

w - w(t) - (t) w(t + 1) - w(t) 2 

r(t) 2 .

4 4(t)

(65)

Using equation 64, equation 65, and (t) = K + (t) in equation 62 we have

F (w(t

+

1))



~lt(w(t

+

1))

+

(t) 2

w(t + 1) - w(t) 2 +

q(t) 2 +
4(t)

r(t) 2 4(t)

(66)

The following Lemma gives a relation between w(t) and ~lt(w(t))

Lemma 10 The optimization stated in equation 23 is equivalent to

t-1

w(t) = arg min
wW

~lt (w) + (t)h(w)

t =1

By using the result (Dekel et al., 2012, Lemma 8), we have

.

(67)

(t) 2

w(t + 1) - w(t)

t-1
2  ~lt (w(t + 1)) + ((t))h(w(t + 1))

t =1

t-1
- ~lt (w(t)) + (K + (t))h(w(t)).

t =1

(68)

14

Under review as a conference paper at ICLR 2019

Use equation 66 in equation 68 and substituting in (t) = K + (t) we get

t-1 t-1
F (w(t + 1))  lt(w(t + 1)) + ~lt (w(t + 1)) - ~lt (w(t)) + (K + (t))h(w(t + 1))

t =1

t =1

- (K + (t))h(w(t)) + q(t) 2 + r(t) 2 4(t)

(69)

t t-1
 ~lt (w(t + 1)) - ~lt (w(t)) + (K + (t + 1))h(w(t + 1))

t =1

t =1

q(t) 2 + r(t) 2

- (K + (t))h(w(t)) +

,

4(t)

(70)

where equation 70 is due to the fact that (t + 1)  (t). Now, we use (t) = K + (t), multiply by c(t + 1) and rewrite

t t-1

c(t+1)F (w(t+1))  c~(t+1)lt (w(t+1))- c~(t)lt (w(t))+c(t+1)(t+1)h(w(t+1))

t =1

t =1

q(t) 2 + r(t) 2 - c(t)(t)h(w(t)) + c(t + 1)
4(t)

t-1

- (c(t + 1) - c(t))

~lt (w(t)) + (t)h(w(t)) . (71)

t =1

Summing from t = 1 to  - 1 we get



 -1

 -1

c(t)F (w(t))  c( )~lt(w( )) + c( )( )h(w( )) + c(t + 1)

q(t) 2 + r(t) 2 4(t)

t=2 t=1

t=1

 -1

t-1

+ (c(t) - c(t + 1))

~lt (w(t)) + (t)h(w(t)) . (72)

t=1 t =1

Let  be the last two terms, i.e.,

 -1

t-1

 = (c(t) - c(t + 1))

~lt (w(t)) + (t)h(w(t)) .

t=1 t =1

(73)

Then, using Lemma 10



 -1

 -1

c(t)F (w(t))  c( )~lt(w) + c( )( )h(w) + c(t + 1)

q(t) 2 + r(t) 2 + .
4(t)

t=2 t=1

t=1

By substituting in equation 57 we continue



 -1

 -1

c(t)F (w(t))  c( )lt(w) + q(t), w - w(t) + r(t), w - w(t)

t=2 t=1 t=1

 -1
+ c( )( )h(w) + c(t + 1)

q(t) 2 +

r(t) 2 +

4(t)

t=1

 -1
 ( - 1)c( )F (w) + q(t), w - w(t) + r(t), w - w(t)

t=1

 -1
+ c( )( )h(w) + c(t + 1)

q(t) 2 +

r(t) 2 +

4(t)

t=1

(74) (75)

15

Under review as a conference paper at ICLR 2019

where equation 75 is due to convexity of F (w), i.e.,

 -1 t=1

lt(w)



(

-

1)F (w).

Adding

and

subtracting terms we find that


c(t)[F (w(t)) - F (w)]  c(1)[F (w(1)) - F (w)] + (c( ) - c(t))F (w)

t=1 t=2  -1
+ q(t), w - w(t) + r(t), w - w(t)

t=1

 -1
+ c( )( )h(w) + c(t + 1)

q(t) 2 +

r(t) 2 +

4(t)

t=1

Taking the expectation with respect to X( - 1)


E c(t)[F (w(t)) - F (w)]  c(1)[F (w(1)) - F (w)] + (c( ) - c(t))F (w)

t=1 t=2

 -1
+ c( )( )h(w) + E[ q(t), w - w(t) ] + E[ r(t), w - w(t) ]

t=1

+ -1 c(t + 1) E[

q(t)

2] + [ 4(t)

r(t)

2]

+ E[]

(76)

t=1

We use the bounds in Lemma 9 to get


E c(t)[F (w(t)) - F (w)]  c(1)[F (w(1)) - F (w)] + (c( ) - c(t))F (w)

t=1 t=2

+ c( )( )h(w) + -1 KD

-1 c(t + 1) +

(t) 4(t)

2 K2 2 b(t) + (t)2

+ E[].

(77)

t=1 t=1

We rewrite by rearranging terms


E c(t)[F (w(t)) - F (w)]  c(1)[F (w(1)) - F (w)] + (c( ) - c(t))F (w)

t=1 t=2

+ c( )( )h(w) + -1 KD (t)

-1 c(t + 1)2 +
4b(t)(t)

K2 +
4

2 -1 c(t + 1) (t)(t)2

+ E[]

(78)

t=1 t=1

t=1

Now we bound E[]. From equation 73 we find

 -1

t-1

 = (c(t) - c(t + 1))

~lt (w(t)) + (t)h(w(t))

(79)

t=1 t =1

 -1

t-1

 (c(t) - c(t + 1))

~lt (w) + (t)h(w)

(80)

t=1 t =1

 -1

t-1

= (c(t) - c(t + 1))

(lt (w) + q(t ), w - w(t ) + r(t ), w - w(t ) ) + (t)h(w)

t=1 t =1

(81)

 -1

t-1

 (c(t) - c(t + 1)) (t - 1)F (w) + (t)h(w) + q(t ), w - w(t ) + r(t ), w - w(t )

t=1 t =1
(82)

,

16

Under review as a conference paper at ICLR 2019

where equation 80 is due to Lemma 10, equation 81 is simple substitution of equation 57, and the last step is due to convexity of F (w). Now, we take the expectation over data samples X( - 1)

E[]

 -1

t-1

 (c(t) - c(t + 1)) (t - 1)F (w) + (t)h(w) + E[ q(t ), w - w(t ) ] + E[ r(t ), w - w(t ) ]

t=1 t =1
(83)

 -1
 (c(t) - c(t + 1))

(t - 1)F (w) + (t)h(w) + t-1 KD

(t )

t=1 t =1

 -1
 (c(t) - c(t + 1))

(t - 1)F (w) + (t)h(w) + 2KD

 µt

(84) (85)

t=1

where Lemma 9 is used in equation 84 and the last step is due to equation 86. This completes the proof of Lemma 8.

C PROOF OF BOUNDS USED IN APP. E

We know (t) = K + (t). Let (t) =

t µ

.

Then,

we

have

Similarly,

 -1

1



 2 µ

.

(t)

t=1

 -1 1

 -1

1

(t)(t)2 = (t)(K + (t)2)

t=1 t=1

 -1 1  (t)3
t=1

 -1

= µ3/2

t-3/2

t=1
 µ3/2 1 +  3µ3/2.


t-3/2dt
1

(86)
(87) (88) (89) (90) (91)

D PROOF OF LEMMA 9
Note that the expectation with respect to xs(t) E[f (w(t), xs(t))] = E[F (w(t))].
Also we use the fact that gradient and expectation operators commutes E[ f (w(t)), xs(t), w(t) ] = E[ F (w(t)), w(t) ].
17

(92) (93)

Under review as a conference paper at ICLR 2019

Bounding E[ q(t), w - w(t) ] and E[ q(t) 2] follows the same approach as in (Dekel et al., 2012, Appendix A.1) or Tsianos & Rabbat (2016). Now, we find E[ r(t), w - w(t) ]

E[ r(t), w - w(t) ]

(94)



= E

1 b(t)

n bi(t)

1

wf (wi(t), xi(t, s)) - b(t)

n bi(t)
wf (w(t), xi(t, s)), w - w(t)



i=1 s=1

i=1 s=1

(95)



= E

1 b(t)

n bi(t)

1

wF (wi(t)) - b(t)

n bi(t)
wF (w(t)), w - w(t)



i=1 s=1

i=1 s=1

(96)

1 =
b(t)

n

bi(t)E [ wF (wi(t)) - wF (w(t)), w - w(t) ]

i=1

1 b(t)

n

bi(t)E [ wF (wi(t)) - wF (w(t))

w - w(t) ]

i=1

1 b(t)

n

bi(t)E [K wi(t) - w(t) D]

i=1

(97) (98) (99)

where equation 98 is due to the Cauchy-Schwarz inequality and equation 99 due to equation 9 and D = maxw,uW w - u . Using equation 25

E[

r(t), w

-

w(t)

]



1 b(t)

n

bi (t)K D (t)

i=1

KD =.
(t)

(100) (101)

Now we find E[ r(t) 2].

 2

E[ r(t) 2] = E  

1 b(t)

n

bi (t)
wf (wi(t), xi(t, s)) - wf (w(t), xi(t, s))

 

i=1 s=1

(102)

 



E


 

1 b(t)

n

bi (t)

2

wf (wi(t), xi(t, s)) - wf (w(t), xi(t, s))



 

i=1 s=1

(103)

 



E


 

1 b(t)

n

bi (t)
K

2

wi(t) - w(t)



 

i=1 s=1

(104)

 2

 1

n bi(t) K 

b(t) (t)

i=1 s=1

K2 2 = (t)2 .

(105) (106)

E PROOF OF THEOREM 4
By definition

n
c(t) = ci(t)
i=1

(107)

18

Under review as a conference paper at ICLR 2019

where ci(t) is the total number of gradients computed at the node i in the t-th epoch. We assume ci(t) is independent across network and is independent and identically distributed according to some processing time distribution p across epochs. Let c¯ = Ep[c(t)] and let 1/^b = Ep[1/b(t)]. From
Lemma 8 we have that


c(t)E [F (w(t)) - F (w)]  c(1)[F (w(1)) - F (w)] + (c( ) - c(t))F (w)

t=1 t=2

+ c( )( )h(w) + -1 KD (t)
t=1

-1 c(t + 1)2 K2 2 -1 c(t + 1)

++ 4b(t)(t) 4

(t)(t)2 + E[].

t=1 t=1

(108)

Let (t) = t/c¯. Now take expectation over the c(t) to get

Ep



c(t)E [F (w(t)) - F (w)]

 c¯[F (w(1)) - F (w)] + c¯( )h(w) + -1 KD (t)

t=1 t=1

-1 c(t + 1) 2 K2 2 + Ep 4(t) b(t) + (t)2
t=1

+ Ep[E[]] (109)

= c¯[F (w(1)) - F (w)] + c¯( )h(w( )) + -1 KD (t)
t=1

-1 c¯

2 K2 2

+ 4(t)
t=1

^b + (t)2 .

(110)

The last step is due to the fact that c(t + 1) and b(t) are independent since these are in two different epochs. Further Ep[E[|c(t)]] = 0. After further simplification through the use of Appendix C, we get

Ep



c(t)E [F (w(t)) - F (w)]



c¯[F (w(1))

- F (w) + ¯( )h(w( ))]

+ 2KD

 c¯

t=1 

c¯2 c¯ 3K2 2c¯5/2

+ 2^b +

. 4

(111)

Taking the expectation over c(t) in equation 35, we have

Ep[E[R|btot( ), ctot( )]]  Ep


c(t)E [F (w(t)) - F (w)] + L

 c¯ (t)



t=1 t=1
c¯[F (w(1)) - F (w) + ¯( )h(w( ))] + 2KD

 c¯

 c¯2 c¯ 3K2 2c¯5/2



+ 2^b + 4 + 2L c¯ c¯ .

By definition

(112) (113)


m = c(t).

(114)

t=1
Then m¯ = Ep = c¯ . By substituting m¯ and rearranging we find that

Ep[E[R|btot( ),

ctot( )]]



c¯[F (w(1))

-

F (w)

+

¯( )h(w( ))]

+

3K2 2c¯5/2 4

c¯2  + 2KD + 2^b + 2L c¯ m¯

(115)

19

Under review as a conference paper at ICLR 2019

F PROOF OF THEOREM 7

Proof: Consider an FMB method in which each node computes b/n gradients per epoch, with Ti(t) denoting the time taken to complete the job.

Also consider AMB with a fixed epoch duration of T . The number of gradient computations completed by the i-th node in the t-th epoch is

bi(t) =

bT nTi(t)

 bT - 1. nTi(t)

(116)

Therefore, the minibatch size b(t) computed in AMB in the t-th epoch is

b(t) =

n
bi(t) 
i=1

n i=1

bT nTi(t)

-n=

bT n

n i=1

1 Ti(t)

- n.

(117)

Taking the expectation over the distribution of Ti(t) in (117), and applying Jensen's inequality, we find that

Ep[b(t)]



bT n

n
Ep

i=1

1 Ti(t)

- n  bT n

1 - n = bT µ-1 - n.

n i=1 Ep[Ti(t)]

where Ep[Ti(t)] = mu. Fixing the computing time to T = (1 + n/b)µ we find that Ep[b(t)]  b, i.e., the expected minibatch of AMB is at least as large as the minibatch size b used in the FMB.

The expected computing time for  epochs in our approach is

SA =  T =  (1 + n/b)µ.

(118)

In contrast, in the FMB approach the finishing time of the tth epoch is maxi[n] Ti(t). Using the

result of Arnold & Groeneveld (1979); Bertsimas et al. (2006) we find that



Ep[max Ti(t)]  µ +  n - 1,
i[n]

(119)

where  is the standard deviation of Ti(t). Thus  epochs takes expected time 
SF =  Ep[max Ti(t)]   µ +  n - 1
i[n]

(120)

Taking the ratio of the two finishing times we find that



SF  µ + 

n-1 =

1

+



 n

-

1

(1 + n/b)-1.

SA (1 + n/b)µ

µ

(121)

For parallelization to be meaningful, the minibatch size should be much larger than number of nodes and hence b n. This means (1 + n/b)  1 for any system of interest. Thus,

SF 

1

+



 n

-

1

µ

SA,

(122)

This completes the proof of Theorem 7.

G SHIFTED EXPONENTIAL DISTRIBUTION

The shifted exponential distribution is given by

pTi(t)(z) =  exp (-(z - )) , z  

(123)

where   0 and   0. The shifted exponential distribution models a minimum time () to complete a job, and a memoryless balance of processing time thereafter. The  parameter dictates the average processing speed, with larger  indicating faster processing. The expected finishing time is Ep[Ti(t)] = -1 + . Therefore,

SA =  T =  (1 + n/b)(-1 + ).

(124)

20

Under review as a conference paper at ICLR 2019

912

0 67

3

85

4

Figure 2: A cluster of 10 nodes.

By using order statistics, we can find Ep[max Ti(t)] = -1 log(n) + ,
i[n]

(125)

and thus  epochs takes expected time SF =  -1 log(n) + 

(126)

Taking the ratio of the two finishing times we find that

E[S] ST

=

-1 log(n) +  (1 + n/b)(-1 + ) .

(127)

For parallelization to be meaningful we must have much more data than nodes and hence b n. This means that the first factor in the denominator will be approximately equal to one for any system of interest. Therefore, in the large n regime,

log(n)

lim
n

SF

=

1+

 ST ,

(128)

which is order-log(n) since the product  is fixed.

H FURTHER NUMERICAL RESULTS
In this section we present further numerical results.
H.1 DISTRIBUTED NETWORK GRAPH
Figure 2 shows the distributed network graph used in the numerical simulation. This is generated randomly.
H.2 MASTER-WORKER SETUP
We ran the logistic regression experiments under master-worker setup. In the master-worker setup, we used 20 nodes including the master node. In the FMB case, each worker calculated b/n = 210 gradients. The average compute time was found to be 3 seconds and therefore, the compute time for AMB scheme was set at T = 3 sec. while the communication time Tc = 1 sec. Figures 3 shows the achieved cost vs. wall clock time for the master-worker setup
H.3 SHIFTED EXPONENTIAL MODEL SIMULATION
In this model, we use Ti(t) to denote the time taken by worker i to calculate 600 gradients in the t-th epoch. We assume Ti(t) follows a shifted exponential distribution whose probability density function is given by pTi(t)(z) = e-(z-). Conditioned on Ti(t), the worker is assumed to make a linear progress through the dataset; i.e., it takes kTi(t)/600 to calculate k gradients (note that k  600 is allowed). In our simulations, we choose  = 2/3 and  = 1 without loss of generality. Per our model, in the AMB scheme, node i computes bi(t) = 600T /Ti(t) gradients in epoch t where T is the

21

Under review as a conference paper at ICLR 2019

Error cost
Error Error

100.35 100.33 100.31 100.29 100.27 100.25
0

FMB AMB
20 40 60 80 100 120 140 time (s)

Figure 3: Logistic reg. - master worker.

100 100

FMB

FMB (r=5)

AMB

AMB (r=5)

FMB (r = )

10-1 AMB (r = )

10-1

10-2

10-2 10-3

10-3 0

20 40 60 80 100 120 140 Time (sec)

(a) AMB vs. FMB error rate vs. finishing time.

10-4 0

5 10 15 20 Epoch

(b) Error vs. epoch.

100
FMB (r=5) FMB (r=) AMB (r=5) AMB (r=) 10-1

25

10-2

10-3
0 20 40 60 80 100 120 Time (sec)
(c) Error vs. finishing time.
Figure 4: AMB vs. FMB performance comparison.

fixed compute time to calculate the gradients. To ensure a fair comparison between FMB and AMB, T is chosen according to Theorem 7 so that E[b(t)]  b where b(t) = i bi(t) and b is the fixed minibatch size used by FMB. Based on our parameter choices, T = (1 + n/b) (-1 + ) = 2.5.
Figure 4(a) plots the average error rate versus computation time for both FMB and AMB for 20 randomly generated sample paths where we use r = 5 rounds of consensus. The figure shows that for all sample paths, AMB outperforms FMB. The figure also shows that there is not much variance in each scheme's performance across all sample paths. Therefore, for all our subsequent analysis, we randomly choose a sample path and plot the results.
Figure 4(b) compares the per-epoch average error rate of both schemes for varying number of consensus rounds: r = 5 and r =  (perfect consensus). Although both schemes exhibit similar performance when plotted versus epoch, the superiority of AMB over FMB becomes apparent in Figure 4(c) when performance is measured versus time. In particular, AMB is 2.24 times faster than FMB at error rate of 10-3.

22

