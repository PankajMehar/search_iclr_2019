Under review as a conference paper at ICLR 2019
MINIMUM DIVERGENCE VS. MAXIMUM MARGIN: AN EMPIRICAL COMPARISON ON SEQ2SEQ MODELS
Anonymous authors Paper under double-blind review
ABSTRACT
Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper for the first time puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that training criteria based on the idea of minimum divergence can usually work better than maximum margin methods, on both the tasks of machine translation and sentence summarization.
1 INTRODUCTION
Sequence to sequence (seq2seq) models (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) are a powerful end-to-end solution to sequence prediction since they allow a mapping between two sequences of different lengths. However, Maximum Likelihood Estimation (MLE) which is used as a standard training criterion in seq2seq has a series of drawbacks:
· Training and evaluation mismatch: during training, we maximize the log likelihood, while during inference, the model is evaluated by a different metric such as BLEU or ROUGE;
· MLE fails to assign proper scores to different model outputs, which means that all incorrect outputs are treated equally.
Similar to the SEARN (Daume´ et al., 2009) and the DAGGER (Ross et al., 2011) in traditional models, many reinforcement learning (RL) techniques have been extended to seq2eq models recently to solve the mismatch between training and inference. The key idea is to replace MLE with a criterion that gives task-specific scores to different model outputs. Recent approaches to taking the evaluation metric into training criteria generally fall into one of the following two categories:
1. minimum divergence: Minimum divergence methods minimize some divergence functions between the model output distribution and the probability distribution given by the ground truth, and by optimizing the divergence between the two distributions, the model output distribution will ideally be equal to the ground truth distribution.
2. maximum margin: During inference, in structured prediction, the goal is to simply ensure good structures have the highest predicted score. That is, margin based learning differs from minimum divergence learning in the aspect that it only requires that the predicted score of the correct output should be higher than that of the others by a margin.
The main contribution of this paper can be summarized as: based on the analysis of existing work, we introduce a novel training criterion for seq2seq learning and show its effectiveness.
1

Under review as a conference paper at ICLR 2019
Based on our experimental results on the tasks of machine translation and sentence summarization, we conclude that some models that take the evaluation metric into consideration can improve over a strong MLE baseline by a large margin, and minimum divergence methods usually get better results than maximum margin based approaches.
2 RELATED WORK
Seq2seq models were proposed and improved by Kalchbrenner & Blunsom (2013) and Sutskever et al. (2014). With the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), seq2seq models achieve better results than traditional models in many fields including machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text summarization (Chopra et al., 2016).
Before seq2seq models, previous work on optimizing task specific evaluation metric scores genrally includes SEARN (Daume´ et al., 2009), DAGGER (Ross et al., 2011), Minimum Error Rate Training (MERT) (Och, 2003), and softmax-margin criterion (Gimpel & Smith, 2010).
Seq2seq model uses MLE as the training objective, which also has the problem of training and evaluation mismatch. Ranzato et al. (2016) took evaluation metric into training of seq2seq models and proposed the mixed incremental cross entropy REINFORCE (MIXER) training strategy, which is similar to the idea of Minimum Risk Training (MRT) (Smith & Eisner, 2006; Li & Eisner, 2009; Ayana et al., 2016; Shen et al., 2016). MIXER used decoder hidden states to predict the bias term to reduce the variance, while MRT renormalized the predicted probabilities.
Inspired by MIXER, Bahdanau et al. (2017) extended the actor critic algorithm (Konda & Tsitsiklis, 2000; Sutton et al., 2000) to the seq2seq framework. Sokolov et al. (2016) and Kreutzer et al. (2017) proposed the pairwise preference learning framework, which enables the model to learn from the difference between a pair of samples generated by the model. Another popular model in the form of divergence minimization is the smoothed version of MERT algorithm proposed by Och (2003). The smoothed MERT algorithm also minimizes the expected loss over samples from model outputs and is similar to MRT essentially. Norouzi et al. (2016) proposed the Reward Augmented Maximum Likelihood (RAML) which also optimizes the divergence by data augmentation, and RAML was further combined with maximum likelihood via the -divergence recently (Koyamada et al., 2017).
In contrast to the above divergence minimization based approaches, Wiseman & Rush (2016) used a margin based training objective and their search optimization process requires performing parameter updates or inference each time a new word is generated. One main drawback of maximum margin is that given a sample subset of an arbitrary size, only two samples of the subset can contribute to the gradient. The softmax-margin criterion proposed by Gimpel & Smith (2010) can allievate this problem, in which all samples are used in loss and gradient computation. However, Edunov et al. (2018) shows that the softmax-margin criterion still cannot reach the performance of risk minimization. As the latest work, Edunov et al. (2018) is very similar to ours. However, our interests are in finding better training criteria on a consistant setting, while they mainly focused on absolute performance improvement, using techniques like combination with token level loss, online vs. offline candidate generation, etc.
3 SEQUENCE TO SEQUENCE MODEL
This paper considers the seq2seq model from Bahdanau et al. (2015) and Luong et al. (2015), which consists of an RNN encoder and an RNN decoder. Due to the gradient vanishing problem, the vanilla RNN cell is replaced with a Gated Recurrent Unit (GRU) (Cho et al., 2014) or Long ShortTerm Memory (LSTM) (Hochreiter & Schmidhuber, 1997). A bidirectional RNN is used as encoder, with one RNN reading the source sequence from beginning to end, and the other RNN reading in reversed order. The bidirectional RNN encoder encodes the input sequence and gets a sequence of hidden states by concatenating the hidden states of each word in the two directions, which will be used as attention information in the decoder RNN.
The decoder RNN is initialized by passing the two final hidden states of the encoder to a feed forward network. The mixed information used as input in the first step m0 is initialized by a zero vector. In decoding step i, using the input feeding approach described in Luong et al. (2015), the decoder RNN reads the embedding of the word wi and the mixed information mi-1, to generate the hidden
2

Under review as a conference paper at ICLR 2019

state of current step hi:

hi = f1(concat(wi , mi-1), hi-1),

where f1 is the RNN cell. The decoder then generates the attention context ci from encoder using hi. Since there are many different attention mechanisms, we omit the details here. After getting ci, the model mixes ci and hi together by a feedforward network f2 to generate mi, which is used as the mixed information in the next step:

mi = f2(ci, hi)

Finally, mi is passed to a feedforward network to generate oi, and the softmax layer outputs the probability for each word at step i. Suppose the target word in step i is v , the probability of the
target word in step i is given by

p(yi = v | x, y1:i-1) =

exp(oiv ) . v exp(oiv)

For the sequence y = y1, y2, ..., yl , the probability p(y | x) is p(y | x) = p(y1 | x) p(y2 | x, y1)...p(yl | x, y1:l-1).
Suppose y is the correct output, MLE actually maximizes the log likelihood of p(y | x):
l
LMLE = - log p(y | x) = - log p(yi | x, y< i)
i=1

For a detail explanation, please refer to Luong et al. (2015). Now we describe the algorithm to construct a subset S from all possible output sequences which will be used in Section 4 & 5. In seq2seq model, the n-best list can only be approximated by performing beam search, as used in Wiseman & Rush (2016), or Monte Carlo (MC) sampling, as used in Ayana et al. (2016) and Shen et al. (2016). While using beam search may produce more accurate results, the time required for generating a candidate set of the same size as MC sampling using beam search is too long. In this paper, we construct the approximated n-best list S using MC sampling proposed in Shen et al. (2016) , and for completeness, we describe the sampling approach in Appendix 8.1 in details. Please note that the sampling algorithm does not guarantee the size of the sample subset, since it is inefficient to force the model to generate an exact number of distinct samples using GPU.

4 MINIMUM DIVERGENCE

In this section, we consider training criteria in the form of the following equation:
L = D( p1 || p2 ),
where p1 and p2 are two probability distributions given by the model or the ground truth, and D is a divergence function, for example, the Kullback Leibler (KL) divergence. The baseline training criterion MLE can also be seen as minimum divergence training, where the cross entropy between the model output distribution and the onehot encoded ground truth is minimized. Now we will discuss training criteria in which the ground truth distribution contains information of evaluation metric scores.

4.1 REWARD AUGMENTED MAXIMUM LIKELIHOOD

Suppose that {x, y} is a given input-output pair, the model learns parameters  and gives predition p(y | x; ) on the output space Y, the exponentiated payoff distribution (Norouzi et al., 2016) q(y | y) is defined as follows:

q(y | y) =

1 Z (y  )

exp[ r(y, y)/ ],

(1)

where Z(y) = yY exp[ r(y, y)/ ] is the normalization factor.  is a hyperparameter that controls the smoothness of the distribution, and r(y, y) is the score of prediction y given by the evaluation metric, e.g. BLEU (Papineni et al., 2002), or ROUGE (Lin, 2004)

3

Under review as a conference paper at ICLR 2019

The training objective of Reward augmented Maximum Likelihood (RAML) (Norouzi et al., 2016)

is defined as

LRAML = Eq(y | y)[ log p(y | x; ) ],

(2)

in which, the gradient is:

Eq(y | y)[  log p(y | x; ) ] =

q(y | y)  log p(y | x; )

yY

(3)

Equation (3) requires computing q(y | y) for all y and performing gradient desent on the whole Y. However, due to the huge search space of Y and the high computational cost of r(y, y), Equation (3) cannot be directly computed in an efficient way. Norouzi et al. (2016) proposed using stratified sampling. One can sample from negative edit distance or Hamming distance and then do importance reweighting to reweight the samples with task specific scores. After sampling from q(y | y), the model is trained to maximize log p(y | x; ).

Another choice is to construct a sample subset S from the model outputs using MC sampling, and use Equation (1) to compute the probability q (y | y) in the sample subset. The training criterion is then changed to
L RAML = q (y | y) log p(y | x; ).
yS

4.2 MINIMUM RISK TRAINING

We first introduce the REINFORCE algorithm (Williams, 1992), which is a famous policy gradient method in RL, and then compare it with the Minimum Risk Training (MRT) (Smith & Eisner, 2006; Li & Eisner, 2009; Ayana et al., 2016; Shen et al., 2016). In REINFORCE, given a state x, an action y under state x, and model parameters , the policy gradient g is

g = (r(y) - B(x)) log p(y | x; ),

(4)

where r(y) is the reward of action y, and B(x) is the baseline or control variate that is used to reduce the variance (without introducing bias).

In MRT, the training objective is the expectation of evaluation metric scores taken with respect to the model output distribution:

LMRT = -Ep(y | x;)[ r(y, y) ].

(5)

Also due to the huge search space, Equation (5) can only be estimated by sampling from the model outputs. Suppose S is a sample subset from Y, p(y | x; ) is replaced by p (y | x; ; ):

p (y | x; ; ) =

p(y | x; ) yS p(y | x; ) ,

(6)

where  is a hyperparameter that controls the smoothness of p (y | x; ; ), we will later show that the  here is essentially the same as  in RAML.

The gradient of LMRT can be derived as: -  Ep (y | x;,)[ r(y, y) ] = -  Ep (y | x;,)[  log p(y | x; )(r(y, y) - B) ],

with

B = Ep (y | x;,)[ r(y , y) ].

(7)

If we regard the whole seq2seq framework as a one-step Markov Decision Process (MDP), in which the input sequence x is the state, the output sequence y is the action, and the evaluation metric score r is the reward, both REINFORCE and MRT actually optimize the same training criterion. The difference between them is that REINFORCE requires only single sample and is unbiased, while MRT has multiple samples and the probability renormalization in Equation (6) introduces bias. The expected loss objective in Kreutzer et al. (2017) is also similar (and closer) to REINFORCE which uses one sample to estimate the gradient.

4

Under review as a conference paper at ICLR 2019

4.3 COMPARISON OF RAML & MRT

The KL divergence is defined as:

DKL(p1(x)||p2(x)) = Ep1(x)[log p1(x)] - Ep1(x)[log p2(x)]. Starting from RAML, in terms of KL divergence, Equation (2) can be rewritten as:

(8)

LRAML = DKL(q(y | y) || p(y | x; )) - q(y | y) log q(y | y)
y

The gradient of the second term with respect to model parameters  is zero, thus the second term can be regarded as a constant. The training criterion of RAML is essentially minimizing the KL divergence from q(y | y) to p(y | x; ).

Meanwhile, the training criterion of MRT in Equation (5) can be rewritten as:
LMRT = -  Ep(y | x;)[ r(y, y)/ ] = -  Ep(y | x;)[ log q(y | y) ] -  Ep(y | x;)[ log Z(y) ] = -  Ep(y | x;)[ log q(y | y) ] -  log Z(y),
in which, the second term is again a constant as it has no gradient with respect to . Thus we have:

LMRT =  DKL(p(y | x; )||q(y | y)) -  p(y | x; ) log p(y | x; ) + Const, (9)
y

in which, the second term is the negative Shannon Entropy of p(y | x; ). Increasing the entropy can prevent the model from generating too deterministic results, which is called the maximum entropy regularizer (Williams & Peng, 1991). Equation (9) actually shows that MRT minimizes the KL divergence from p(y | x; ) to q(y | y).

By comparing the above two methods, we find that both RAML and MRT are minimizing the KL divergence between the model output distribution and the exponentiated payoff distribution, but with different directions of DKL. Now we will discuss the effect of hyperparameter  in RAML and  in MRT.

KL divergence has a property that it is minimized if and only if the two probability distributions are equal. Ideally, when the KL divergence in RAML is minimized, we have:
p(y | x; ) = q(y | y) log p(y | x; ) = r(y, y)/ + Const
r(y, y) =  log p(y | x; ) + Const.

And for MRT, we similarly have: r(y, y) =  log p(y | x; ) + Const.

(10)

It turns out that the hyperparameter  in RAML and  in MRT have the same effect.

4.4 HELLINGER DISTANCE MINIMIZATION

Now we introduce another popular distance metric, the Hellinger distance (Hellinger, 1909), which also belongs to the family of Csisza´r f-divergence (Csisza´r, 1963) and has been used in graphical models (Beykikhoshk et al., 2015) and clustering algorithms (Abdel-Azim, 2016; Ji et al., 2018). Given two probability distributions p1 and p2, let f and g denote the probability density functions of p1 and p2. The square of the Hellinger distance H(p1, p2) is defined as:

H2(p1, p2)

=

1 2

( f (x) -

g(x))2 dx.

The training criterion using squared Hellinger distance is:

LHellinger = ( p (y | x; ) - q(y | y) )2,
yS
with p (y | x; ) defined in Equation (6) and q(y | y) defined in Equation (1).

5

Under review as a conference paper at ICLR 2019

5 MAXIMUM MARGIN

Let  denote the loss given by the evaluation metric:  = r(y, y) - r(y, y),

and let F be the predicted score of the model:

F =  log p(y | x; ).

Different from minimizing the expected risk, another choice of training criterion is to optimize the evaluation score of the sequence with the highest predicted score:

arg min (arg max F (y, ), y).
y

(11)

For each input sequence x, we first do inference to find the output y that has the highest predicted score, then we train the model to maximize the score of y given by evaluation metric.

Equation (11) includes an arg max and the dependency of r on y is complex, and also, the gradient of such complex function is usually not defined, thus optimizing it directly on a deep neural network is not computationally tractable. This problem has been widely discussed in the training process of Structural Support Vector Machine (SSVM) (Tsochantaridis et al., 2004; Joachims, 2006; Joachims et al., 2009). One solution is to find some surrogates that are relatively easier to optimize. Tsochantaridis et al. (2004) proposed using margin and slack rescaling instead.

Margin Rescaling is based on the following inequality: (y, y)  max((y, y) + F (y)) - F (y)
y

(12)

Similarly, Slack Rescaling is: (y, y)  (y, y) max(1 + F (y) - F (y))
y

(13)

As Equation (13) is a tighter bound on (y, y), slack rescaling generally gives better results than margin rescaling. Both Equation (12) and Equation (13) can be optimized by subgradient descent. Let y denote the sample that gives the max value. The subgradient for margin rescaling is:
 log p(y) -  log(p(y)),
and similarly, the subgradient for slack rescaling is:
(y, y)( log p(y) -  log p(y)).
Since doing loss augmented inference on such a huge search space is again not realistic, approximate inference is still needed. Equation (12) also has the name of structured hinge loss when used in SSVM. One may have found that Equation (13) is the same as the training criterion in Wiseman & Rush (2016). Besides, to make a fair comparison with other training criteria and make it applicable on large datasets, we replace the beam search with sample subset.

6 EXPERIMENTS
We evaluate the training criteria on two tasks in total. The first task is machine translation on a small dataset to explore the hyperparameter impact. The second task is sentence summarization using large datasets.
For all the experiments, we use LSTM as RNN cell. The attention mechanism used in our experiments is global attention with input feeding (Luong et al., 2015). All the predicted sequences for evaluation are generated using greedy search, since greedy search can be easily batched and is much faster than beam search.

6

Under review as a conference paper at ICLR 2019

6.1 MACHINE TRANSLATION
Due to space limit, experimental results on hyperparameter effects and training details are provided in Appendix 8.2.1.
Data We use the IWSLT 2014 German-English translation dataset, with the same splits as Ranzato et al. (2016) and Wiseman & Rush (2016), which contains about 153K training sentence pairs, 7K validation sentence pairs and 7K test sentence pairs. Sentences in the dataset are first tokenized and then converted into lowercase. During training, the maximum length for both inputs and outputs is restricted to 50. Words that appear less than three times are replaced with a UNK token.
Models The encoder is a single-layer bidirectional LSTM with 256 hidden units for either direction, and the decoder LSTM also has 256 hidden units. The size of word embedding for both encoder and decoder is 256. We use a dropout rate (Srivastava et al., 2014) of 0.2 to avoid overfitting. The gradient is clipped when its norm exceeds 1.0 to prevent gradient explosion and stabilize learning (Pascanu et al., 2013). The batch size is set to 32 and the training set is shuffled at each new epoch. All models are trained with the Adam optimizer (Kingma & Ba, 2015). The sentence level BLEU score used as reward or loss during training is smoothed by adding 1 to both numerator and denominator, as suggested by Chen & Cherry (2014).
Results For the two max margin methods,  is set to 1.0×10-3. We use the sample subset version of RAML, and  for RAML is also set to 1.0 × 10-3. For Hellinger loss,  = 5.0 × 10-4 and  = 0.5. The final results on the test set are described in Table 1. Max margin methods here improve the MLE baseline less significant than Wiseman & Rush (2016) did, since the search optimization algorithm is not used in our experiments. The training criterion of squared Hellinger distance gives the highest BLEU score on the test set.

Table 1: BLEU scores on IWSLT German-English translation evaluated on the test set.

Criterion BLEU Improvement

MLE 26.03

0.00

margin 26.22

+0.19

slack 26.32

+0.29

RAML 26.34

+0.31

MRT 27.35

+1.32

Hellinger 27.75

+1.72

6.2 SENTENCE SUMMARIZATION
The task of sentence summarization is: given a long sentence or a passage, the model will generate a short sentence that summarizes the long text. For evaluation, following Rush et al. (2015), we use the full length F1 variant of ROUGE score (Lin, 2004) as metric. Training details are described in Appendix 8.2.2.
Data We use the Gigaword corpus with the same preprocessing steps as in Rush et al. (2015). In the Gigaword dataset, the first sentence of each article is regarded as the source sequence, and the title of the article as the target sequence. The training set consists of 3.7M sequence pairs, and the vocabulary size for both article and title is 50K, with the out-of-vocabulary words replaced by a unified UNK token. The whole dataset is then converted into lowercase without tokenization. During training, we use the first 2K sequences of the dev corpus as validation set, and the size of the test set is also 2K. The maximum length for both input and output sequence is restricted to 50.
Models The encoder is a single layer bidirectional LSTM with 1000 hidden units for either direction. The single layer decoder LSTM also has 1000 dimmensions. The word embedding size for both encoder and decoder is 500. Dropout is not used in the sentence summarization task, and the gradient is clipped when its norm exceeds 1.0. The batch size is set to 128. The training set is shuffled at each new epoch, and we prefetch 20 batches and sort them by the length of target sequence to form new batches to speed up training. We use ROUGE-2 as the reward or loss function.
7

Under review as a conference paper at ICLR 2019

Results Table 2 shows the results on the validation set and the test set. We report ROUGE-1, ROUGE-2, ROUGE-L and ROUGE-S4 score. MRT outperforms the MLE baseline by a large margin, but is still slightly lower than the newly introduced Hellinger loss.

Table 2: Results on Gigaword sentence summarization task

ROUGE-1 Criterion
valid test

ROUGE-2 valid test

ROUGE-L valid test

ROUGE-S4 valid test

MLE margin slack RAML MRT Hellinger

48.41 48.43 48.40 48.43 50.41 51.02

34.20 34.20 34.20 34.20 35.63 35.84

24.14 24.16 24.12 24.16 26.42 26.88

15.22 15.24 15.25 15.24 16.96 17.03

45.08 45.09 45.08 45.10 47.05 47.71

31.84 31.85 31.83 31.84 33.24 33.47

22.62 22.65 22.63 22.66 24.47 25.12

14.00 14.00 14.00 14.00 15.31 15.54

7 DISCUSSION

For training criteria in the form of divergence minimization, according to our experimental results, MRT outperforms RAML. As mentioned before, MRT and RAML minimize the KL divergence in two different directions. By the definition of KL divergence in Equation (8), it is only minimized when p1 = p2. Optimizing the KL divergence in either of the two directions both guarantee consistency, i.e., the training procedure can ultimately recover the true probability distribution. For more explanation on the properties of KL divergence, we refer the readers to Husza´r (2015).

For a model that minimizes DKL(p1||p2), the model will sample from p1 and maximize log p2, which means that if x has a high probability in p1, it will also have a high probability in p2, but if has a low probability in p1, p2(x) may still be high.
Thus in RAML which samples from exponentiated payoff distribution q and maximizes log p, sequences that have a high evaluation score will also have a high predicted probability. However, samples that have a low q, e.g., a low BLEU or ROUGE, may still have a high p.

In contrast, MRT will first sample from model outputs and ensure that a high BLEU or ROUGE score will be assigned to sequences that have a high probability in model output distribution p, at the cost that sequences that have low probabilities in p may still have high evaluation scores. This can explain the difference between RAML and MRT.

The

squared

Hellinger

distance

can

be

regarded

as

the

squared

error

between

p1 2

and

q

1 2

,

which

is

es-

sentially a regression loss, while the KL divergence usually works better in classification problems.

By optimizing the KL divergence, the model is trained to find the candidate that has the highest

probability and pays less attention to samples that have low probabilities, while by optimizing the

squared Hellinger distance, the model learns to predict the evaluation metric score for every sample

in the candidate set and pays equal attention to all of them. Since seq2seq models generate a pre-

dicted sequence word by word, the target sequence which has the highest evaluation metric score

(sample from q and maximize log p) or the highest predicted probability (sample from p and max-

imize log q) may not be reached by approximate inference algorithms like greedy search or beam

search. In this case, a model trained by optimizing the KL divergence may not work as expected,

however, a model trained by optimizing the squared Hellinger distance can still make predictions

normally since it pays equal attention to samples with low probabilities during training.

For maximum margin methods, slack rescaling is a tighter bound and the experimental result is slightly higher than margin rescaling on our small scale experiments. However, slack rescaling still cannot achieve the same level of performance of MRT and Hellinger distance. The reason may be that the upper bound is still not tight enough. In the training process of maximum margin methods, only the target sequence and a negative sample selected from the candidate set are used to update model parameters, which may also explain their performance.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Gamil Abdel-Azim. New hierarchical clustering algorithm for protein sequences based on hellinger distance. Appl Math, 10(4):1541­9, 2016.
Ayana, Shiqi Shen, Zhiyuan Liu, and Maosong Sun. Neural headline generation with minimum risk training. arXiv:1604.01904, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In ICLR, 2017.
Adham Beykikhoshk, Ognjen Arandjelovic, Dinh Phung, and Svetha Venkatesh. Discovering topic structures of a temporally evolving document corpus. arXiv preprint arXiv:1512.08008, 2015.
Boxing Chen and Colin Cherry. A systematic comparison of smoothing techniques for sentencelevel bleu. In ACL, 2014.
Kyunghyun Cho, Bart van Merrie¨nboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder­decoder approaches. Syntax, Semantics and Structure in Statistical Translation, pp. 103, 2014.
Sumit Chopra, Michael Auli, and Alexander M Rush. Abstractive sentence summarization with attentive recurrent neural networks. In NAACL, 2016.
Imre Csisza´r. Eine informationstheoretische ungleichung und ihre anwendung auf den beweis der ergodizitat von markoffschen ketten. Magyar. Tud. Akad. Mat. Kutato´ Int. Ko¨zl, 8:85­108, 1963.
Hal Daume´, John Langford, and Daniel Marcu. Search-based structured prediction. Machine learning, 2009.
Sergey Edunov, Myle Ott, Michael Auli, David Grangier, et al. Classical structured prediction losses for sequence to sequence learning. In NAACL-HLT, 2018.
Kevin Gimpel and Noah A Smith. Softmax-margin training for structured log-linear models. 2010.
Ernst Hellinger. Neue begru¨ndung der theorie quadratischer formen von unendlichvielen vera¨nderlichen. Journal fu¨r die reine und angewandte Mathematik, 136:210­271, 1909.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 1997.
F. Husza´r. How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary? ArXiv e-prints, 2015.
Wenying Ji, Simaan M AbouRizk, Osmar R Za¨iane, and Yitong Li. Complexity analysis approach for prefabricated construction products using uncertain data clustering. Journal of Construction Engineering and Management, 144(8):04018063, 2018.
Thorsten Joachims. Training linear svms in linear time. In KDD, 2006.
Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. Cutting-plane training of structural svms. Machine Learning, 2009.
Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In EMNLP, 2013.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In NIPS, 2000.
Sotetsu Koyamada, Yuta Kikuchi, Atsunori Kanemura, Shin-ichi Maeda, and Shin Ishii. Neural sequence model training via -divergence minimization. In ICML Workshop, 2017.
Julia Kreutzer, Artem Sokolov, and Stefan Riezler. Bandit structured prediction for neural sequenceto-sequence learning. In ACL, 2017.
9

Under review as a conference paper at ICLR 2019
Zhifei Li and Jason Eisner. First-and second-order expectation semirings with applications to minimum-risk training on translation forests. In EMNLP, 2009.
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In ACL Workshop, 2004. Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In EMNLP, 2015. Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu,
and Dale Schuurmans. Reward augmented maximum likelihood for neural structured prediction. In NIPS, 2016. Franz Josef Och. Minimum error rate training in statistical machine translation. In ACL, 2003. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In ICML, 2013. Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In ICLR, 2016. Ste´phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011. Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In EMNLP, 2015. Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum risk training for neural machine translation. In ACL, 2016. David A Smith and Jason Eisner. Minimum risk annealing for training log-linear models. In ACL, 2006. Artem Sokolov, Julia Kreutzer, Stefan Riezler, and Christopher Lo. Stochastic structured prediction under bandit feedback. In NIPS, 2016. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 2014. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, 2014. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, 2000. Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. Support vector machine learning for interdependent and structured output spaces. In ICML, 2004. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992. Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 1991. Sam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search optimization. In EMNLP, 2016.
10

Under review as a conference paper at ICLR 2019

8 APPENDIX
8.1 THE SAMPLING ALGORITHM
For completeness, we describe the MC sampling algorithm proposed by Shen et al. (2016) and Ayana et al. (2016) here.
Algorithm 1 The sampling approach to constructing the approximated n-best list Input:
source sequence x, model parameters , target sequence y sample size m, maximum length l Output: A set of samples S and corresponding probability P 1: S = {y}; P = {p(y)}; 2: for i in 1, ..., m - 1 do 3: sample a sequence word by word given x and , stop sampling when length is l or meeting
end-of-sequence symbol, and finally store the result and probability in y and q respectively
4: if y in S then continue
5: end if 6: S = S  {y} 7: P = P  {q} 8: end for 9: return S, P

8.2 EXPERIMENTS
8.2.1 MACHINE TRANSLATION
The MLE baseline is trained with a learning rate of 3.0 × 10-4. The model is trained for 20 epochs in total, and is evaluated every 1000 steps on the validation set.
 for RAML For RAML, since we use BLEU instead of Hamming distance as evaluation metric, we first show the BLEU scores on the validation set for hyperparameter  in Table 3. RAML-I refers to importance reweighting and RAML-S refers to sample subset RAML, as mentioned in Section 4.1. Experimental results show that the BLEU score is not very sensitive to hyperparameter  although  varies from 1.0 × 10-7 to 1.0 × 10-1, and setting  larger will decrease the BLEU score compared with the MLE baseline. We also find that RAML-S tends to give slightly better results than RAML-I. The learning rate for RAML is 1.0 × 10-6.

 1.0 × 10-7 1.0 × 10-5 1.0 × 10-3 1.0 × 10-1

RAML-I 29.25 29.27 29.26 29.28

RAML-S 29.28 29.30 29.32 29.32

Table 3: Tokenized BLEU scores on IWSLT 2014 German-English translation evaluated on validation set using RAML criterion.

 for MRT For MRT, since Shen et al. (2016) did several experiments to study the effect of hyperparameters, the original hyperparameters are strictly followed in our experiments. Hyperparameter  is set to 5.0 × 10-3 and the sample size is 100. The best BLEU score of MRT on the validation set is 30.55. The learning rate is set to 1.0 × 10-5.
 and  for Hellinger loss As shown in Table 4,  = 5.0 × 10-4,  = 0.5 produces the best result. The highest BLEU score on the validation set is 31.13, which is 0.58 higher than MRT. The learning rate used in Hellinger loss is also 1.0 × 10-5.

11

Under review as a conference paper at ICLR 2019


 5.0 × 10-3 5.0 × 10-4 5.0 × 10-5

0.1
29.33 29.91 30.75

0.5
29.71 31.13 30.83

1.0
29.22 31.01 31.00

Table 4: Tokenized BLEU scores on IWSLT 2014 German-English translation evaluated on the validation set using squared Hellinger distance with different  and  as training criterion.

Sample size for Hellinger loss For squared Hellinger loss, the effect of sample size is shown in Table 5. A larger sample size generally gives a higher BLEU score, however, due to the limit of GPU memory, we cannot set sample size to a larger value.

sample size BLEU

20 29.51

50 30.30

100 31.13

Table 5: Tokenized BLEU scores on IWSLT 2014 German-English translation evaluated on the validation set using squared Hellinger Distance with different sample sizes.

 for max margin For the two max margin training criteria, the learning rate is 1.0 × 10-7. Table 6 shows the effect of  : according to the results on the validation set,  is not very sensitive to the BLEU scores. As shown in Table 6, BLEU scores given by slack rescaling are usually 0.1 higher than for margin rescaling, and we use  = 1.0 × 10-3 for both of the two models.

 1.0 × 10-1 1.0 × 10-3 1.0 × 10-5

Slack 29.16 29.33 29.35

Margin 29.23 29.25 29.16

Table 6: Tokenized BLEU scores on IWSLT 2014 German-English translation evaluated on the validation set using max margin criteria.

8.2.2 SENTENCE SUMMARIZATION
Details We use ROUGE-2 as reward or loss function. For the MLE baseline, we train the model for 7 epochs in total. For the first 5 epochs, we use Adam with a learning rate of 3.0 × 10-4, we then use SGD with a learning rate of 0.1, and decay the learning rate by a factor of 0.5 at the beginning of epoch 7.
For the other training criteria, we use the hyperparameters with the best performance described in Section 6.1.

12

