Under review as a conference paper at ICLR 2019
AN ANALYTIC THEORY OF GENERALIZATION DYNAMICS AND TRANSFER LEARNING IN DEEP LINEAR NET-
WORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.
1 INTRODUCTION
Many deep learning practitioners closely monitor both training and test errors, hoping to achieve both a small training error and a small generalization error, or gap between testing and training errors. Training is usually stopped early, before overfitting sets in and increases the test error. This procedure often results in large networks that generalize well on structured tasks, raising an important generalization puzzle (Zhang et al., 2016): many existing theories that upper bound generalization error (Bartlett & Mendelson, 2002; Neyshabur et al., 2015; Dziugaite & Roy, 2017; Golowich et al., 2017; Neyshabur et al., 2017; Bartlett et al., 2017; Arora et al., 2018, e.g) in terms of various measures of network complexity yield very loose bounds. Therefore they cannot explain the impressive generalization capabilities of deep nets.
In the absence of any such tight and computable theory of deep network generalization error, we develop an analytic theory of generalization error for deep linear networks. Such networks exhibit highly nonlinear learning dynamics (Saxe et al., 2013a;b) including many prominent phenomena like learning plateaus, saddle points, and sudden drops in training error. Moreover, theory developed for the learning dynamics of deep linear networks directly inspired better initialization schemes for nonlinear networks (Schoenholz et al., 2016; Pennington et al., 2017; 2018). Here we show that deep linear networks also provide a good theoretical model for generalization dynamics. In particular we develop an analytic theory for both the training and test error of a deep linear network as a function of training time, number of training examples, network architecture, initialization, and task structure and SNR. Our theory matches simulations and reveals that deep networks with small weight initialization learn the most important aspects of a task first. Thus the optimal test error at the early stopping time depends largely on task structure and SNR, and not on network architecture, as long as the architecture is expressive enough to attain small training error. Thus our exact analysis
1

Under review as a conference paper at ICLR 2019

of generalization dynamics reveals the important lesson that any theory that seeks to upper bound generalization error based only on network architecture, and not on task structure, is likely to yield exceedingly loose upper bounds. Intriguingly our theory also reveals a non-gradient-descent learning algorithm that proveably out-performs neural network training through gradient descent.
We also apply our theory to multi-task learning, which enables knowledge transfer from one task to another, thereby further lowering generalization error (Dong et al., 2015; Rusu et al., 2015; Luong et al., 2016, e.g.). Moreover, knowledge transfer across tasks may be key to human generalization capabilities (Hansen et al., 2017; Lampinen et al., 2017). We provide an analytic theory for how much knowledge is transferred between pairs of tasks, and we find that it displays a sensitive but computable dependence on the relationship between pairs of tasks, in particular, their SNRs and feature space alignments.
We note that a related prior work (Advani & Saxe, 2017) studied generalization in shallow and deep linear networks, but that work was limited to networks with a single output, thereby precluding the possibility of addressing the issue of transfer learning. Moreover, analyzing networks with a single output also precludes the possibility of addressing interesting tasks that require higher dimensional outputs, for example in language (Dong et al., 2015, e.g.), generative models (Goodfellow et al., 2014, e.g), and reinforcement learning (Mnih et al., 2015; Silver et al., 2016, e.g).

2 THEORETICAL FRAMEWORK

We work in a student-teacher scenario in which we consider an ensemble of low rank, noisy teacher networks that generate training data for a potentially more complex student network, and define the training and test errors whose dynamics we wish to understand.

2.1 AN ENSEMBLE OF LOW-RANK NOISY TEACHERS

We first consider an ensemble of 3-layer linear teacher networks with N i units in layer i, and weight matrices W21  RN2×N1 and W32  RN3×N2 between the input to hidden, and hidden to output
layers, respectively. The teacher network thus computes the composite map y = Wx, where W  W32W21. Of critical importance is the singular value decomposition (SVD) of W:

N2
W = U S VT = suvT ,

(1)

=1

Where U  RN3×N2 and V  RN1×N2 are both matrices with orthonormal columns and S is an N2 × N2 diagonal matrix. We construct a random teacher by picking U and V to be random matrices with orthonormal columns and choosing O(1) values for the diagonal elements of S. We work in
the limit N1, N3   with an O(1) aspect ratio A = N3/N1  (0, 1] so that the teacher has fewer outputs than inputs. Also, we hold N 2  O(1), so the teacher has a low, finite rank, and we study
generalization performance as a function of the N 2 teacher singular values.

We further assume the teacher generates noisy outputs from a set of N 1 orthonormal inputs: y^µ = Wx^µ + zµ for µ = 1, . . . , N1.

(2)

This training set yields important second-order training statistics that will guide student learning:

N1
11  x^µx^µT = I,

N1
31  y^µx^µT = W + ZX^ T .

(3)

µ=1

µ=1

Here the input covariance 11 is assumed to be white (a common pre-processing step), the input-
output covariance 31 is simplified using (2), and Z  RN3×N1 is the noise matrix, whose µ'th column is zµ. Its matrix elements ziµ are drawn iid. from a Gaussian with zero mean and variance z2/N 1. The noise scaling is chosen so the singular values of the teacher W and the noise Z are both O(1), leading to non-trivial generalization effects. As generalization performance will depend on the ratio of teacher singular values to the noise variance parameter z2, we simply set z = 1 in the following. Thus we can think of teacher singular values as signal to noise ratios (SNRs).

2

Under review as a conference paper at ICLR 2019

Finally, we note that while we focus for ease of exposition in the main paper on the case of one hidden layer networks and a full orthonormal basis of P = N1 training inputs in the main paper, neither of these assumptions are essential to our theory. Indeed in Section 3.4 and App. A we extend our theory to networks of arbitrary depth, and in App. G we extend our theory to the case of white inputs with P = N 1, obtaining a good match between theory and experiment in both cases.

2.2 STUDENT TRAINING AND TEST ERROR

Now consider a student network with Ni units in each layer. We assume the first and last layers match the teacher (i.e. N1 = N1 and N3 = N3) but N2  N2, allowing the student to have more hidden
units than the teacher. We also consider deeper students (see below and App. A). Now consider any student whose input-output map is given by y = W32W21  Wx. Its training error on the teacher
dataset in (2) and its test error over a distribution of new inputs are given by

train 

N1 µ=1

||Wx^µ

-

y^µ||22

,

N1 µ=1

||y^µ||22

test 

||Wx - y||22 ||y||22

,

(4)

respectively. Here x^µ and y^µ are the noisy training set inputs and outputs in (2), whereas x denotes a random test input drawn from zero mean Gaussian with identity covariance, yµ = Wxµ is noise free teacher output, and · denotes an average w.r.t the distribution of the test input x. Due to the
orthonormality of the training and isotropy of the test inputs, both train and test can be expressed as

Tr WT W - 2Tr WT 31 + Tr 31 T 31

Tr WT W - 2Tr WT W + Tr WT W

train =

Tr 31 T 31

, test =

Tr WT W

.

(5)

Both train and test can be further expressed in terms of the student, training data and teacher SVDs, which we denote by W = USVT , 31 = U^ S^V^ T , and W = U S VT respectively. Specifically,

 -1 
N 3 N2

N3

N2 N 3

train =  s^2  s2 + s^2 - 2

ss^

=1

=1

=1

=1 =1

u · u^

 -1 
N 2 N2

N2

N2 N 2

test =  s2   s2 + s2 - 2

ss

=1

=1

=1

=1 =1

u · u

 v · v^  ,
 v · v  .

(6) (7)

Thus as the student learns, its training and test error dynamics depends on the alignment of the
time-evolving student singular modes {s, u, v} with the fixed training data {s^, u^, v^} and teacher {s, u, v} singular modes respectively.

3 SINGLE TASK GENERALIZATION DYNAMICS: THEORY AND EXPERIMENT

Here we derive and numerically test analytic formulas for both the training and test errors of a student network as it learns from training data generated from a teacher network. We explore the dependence of these quantitites on the student network size, student initialization, teacher SNR, and training time.

3.1 STUDENT TRAINING DYNAMICS AND TRAINING-ALIGNED (TA) NETWORKS

We assume the student weights undergo batch gradient descent with learning rate  on the training error µ ||y^µ-W32W21x^µ||22, which for small  is well approximated by the differential equations:

 d W21 = W32T 31 - W32W2131 , dt

 d W32 = 31 - W32W2131 W21T , dt (8)

(where   1/), which must be solved from an initial set of student weights at time t = 0 (Saxe

et al., 2013a). We consider two classes of student initializations. The first initialization corresponds

to a random student where the weights W21 and W32 are chosen such that the composite map

W = W32W21 has an SVD W = UVT, where U and V are random singular vector matrices

3

Under review as a conference paper at ICLR 2019

Figure 1: Learning dynamics as a function of singular dimension strength. (a) shows how modes of different singular value are learned, (b) shows that there is a wave of learning that picks up singular dimensions with smaller and smaller singular values as t  .

and all student singular values are . As such a random student learns, the composite map undergoes

a time dependent evolution W(t) = U(t)S(t)V(t)T =

N2 =1

s(t)u(t)v(t)T

.

For

white

inputs,

as t  , W  31, and so the time-dependent student singular modes {s(t), u(t), v(t)}

converge to the training data singular modes {s^, u^, v^}. However, the explicit dynamics of the

student singular modes can be difficult to obtain analytically from random initial conditions.

Thus we also consider a special class of training aligned (TA) initial conditions in which W21 and W32 are chosen such that the composite map W = W32W21 has an SVD W = U^ V^ T. That is,
the TA network (henceforth referred to simply as the TA) has the same singular vectors as the training data covariance 31, but has all singular values equal to . As shown in (Saxe et al., 2013a), as the
TA learns according to (8), the singular vectors of its composite map W remain unchanged, while the singular values evolve as s(t) = s(t, s^), where the learning curve function s(t, s^) as well as its
functional inverse t(s, s^) is given by

s^e2s^t/

s(t, s^) =

,

e2s^t/ - 1 + s^/

 s^/ - 1

t(s, s^)

=

2s^

ln

s^/s

-

. 1

(9)

Here the function s(t, s^) describes analytically how each training set singular value s^ drives the

dynamics of the corresponding TA singular value s, and for notational simplicity, we have suppressed

the dependence of s(t, s^) on  and the initial condition . As shown in Fig. 1A, for each s^, s(t, s^)

is

a

sigmoidal

learning

curve

that

undergoes

a

sharp

transition

around

time

t/

=

1 2s^

ln (s^/

- 1),

at which it rises from its small initial value of at t = 0 to its asymptotic value of s^ as t/  .

Alternatively, we can plot s(t, s^)/s^ as a function of s^ for different training times t/ , as in Fig. 1B.

This shows that TA learning corresponds to a singular mode detection wave which progressively

sweeps from large to small singular values. At any given training time t, training data modes with

singular values s^ > t/ have been learned, while those with singular values s^ < t/ have not.

While the TA is more sophisticated than the random student, since it already knows the singular vectors of the training data before learning, we will see that the analytic solution for the TA learning dynamics provides a good approximation to the student learning dynamics, not only for the training error, as shown in (Saxe et al., 2013a), but also for the generalization error as shown below.

The results in this section assume a single hidden layer, but Saxe et al. (2013a) derived t(s, s^) for networks of arbitrary depth and we apply our theory to some deeper networks. The general differential equation and derivations for deeper networks can be found in Appendix A.

3.2 HOW THE TEACHER IS BURIED IN THE TRAINING DATA: A RANDOM MATRIX ANALYSIS

In the previous section, we reviewed an exact analytic solution for the composite map of a TA network, namely that its singular modes are related to those of the training data through the relation

s(t) = s(t, s^), u(t) = u^, v(t) = v^.

(10)

However, computation of the generalization error through (5) then requires understanding how the teacher singular modes of W are buried within the noisy training data singular modes of 31 through the relation (3). Since the input matrix X^ is orthonormal, 31 is simply a perturbation of the low
rank teacher W by a high dimensional noise matrix Z. The relation between the singular modes of a
low rank matrix and its noise perturbed version has been studied extensively in Benaych-Georges & Nadakuditi (2012), in the high dimensional limit we are working in, namely N1, N3   with the aspect ratio A = N3/N1  (0, 1], and N 2  O(1).

4

Under review as a conference paper at ICLR 2019

Figure 2: The teacher's signal through the noise. Theoretical vs. empirical (a) histogram of singular values of noisy teacher s^. (b) s^ as a function of s. (c) alignment of noisy teacher and noiseless teacher singular vectors as a function of s. (N1 = N3 = 100.)

In this limit, the top N 2 singular values and vectors of 31 converge to s^(s), where the transfer function from a teacher singular value s to a training data singular value s^ is given by the function

s^(s) = (s)-1 (1 + s2)(A + s2) if s > A1/4

1+ A

otherwise.

(11)

The associated top N 2 singular vectors of 31 can also acquire a nontrivial overlap with the N 2 modes of the teacher through the relation |u^ · u| |v^ · v| = O(s), where the singular vector overlap function is given by



 O(s) =

1-

A(1+s2 ) s2 (A+s2 )

1/2

1

-

(A+s2 ) s2 (1+s2 )

1/2

if s > A1/4

(12)

0 otherwise

The rest of the N3 - N2 singular vectors of 31 are orthogonal to the top N 2 ones, and their singular

values

are

distributed

according P (s^) =

to 4thAe-t(hs^e2A-Ms^(1a+rcAh)e)2nkos-^Pas[t1ur-(MPA) ,d1is+tribuAtio] n:

(13)

0 otherwise.

Overall, these equations describe a singular vector phase transition in the training data, as illustrated in Fig. 2BC. For example in the case of no teacher, the training data is simply noise and the singular values of 31 are distributed as an MP sea spread between 1 ± A. When one adds a teacher, how each teacher singular mode is imprinted on the training data depends crucially on the teacher singular value s, and the nature of this imprinting undergoes a phase transition at s = A1/4. For s  A1/4, the teacher mode SNR is too low and this mode is not imprinted in the noisy training data; the associated training data singular value s^ remains at the edge of the MP sea at 1 + A, and the overlap O(s) between training and teacher singular vectors remains zero.

However, when s > A1/4, this teacher mode is imprinted in the training data; there is an associated training data singular value s^ that pops out of the MP sea (Fig. 2AB). However, the training data singular value emerges at a position s^ > s that is inflated by the noise, though the inflation effect decreases at larger s, with the ratio s^/s approaching the unity line as s becomes large (Fig. 2B). Similarly, the corresponding training data singular vectors acquire a non-trivial overlap with the teacher singular vectors when s > A1/4, and the alignment approaches unity as s increases (Fig. 2C).

3.3 PUTTING IT ALL TOGETHER: AN ANALYTIC THEORY OF GENERALIZATION DYNAMICS

Based on an analytic understanding of how the singular mode structure {s, u, v} of the teacher

W is imprinted in the modes {s^, u^, v^} of the training data covariance 31 through (11), (12) and

(13), and in turn how this training data singular structure drives the time evolving singular modes of a TA network {s(t), u^, v^} of through (9), we can now derive analytic expressions for train and test
in (6) and (7), for a TA network. We will also show that these learning curves closely approximate those of a random student with time-evolving singular vectors {u(t), v(t)}, and match on several

key aspects. First, inserting the TA dynamics in (10) into train in (6), we obtain

 -1



N3 N2

train(t) =  s^2 (N3 -N2) s^2 Rout +(N2 -N 2) (s(s^, t) - s^)2 Rin + [s(t) - s^]2

=1

=1

(14)

5

Under review as a conference paper at ICLR 2019

Figure 3: Match between theory and experiment for rank 1 (row 1, a-d) and rank 3 (row 2, e-h) teachers with single-hidden-layer students: (a-b, e-f) log train and test error, respectively, showing very close match between theory and experiment for TA, and close match for the random student. (c,g) comparing TA and randomly initialized students minimum generalization errors, showing almost perfect match. (d,h) comparing TA and randomly initialized students optimal stopping times, showing small lag due to alignment. (N1 = 100, N2 = 50, N3 = 50.)

Here, s(t) = s(s^, t) as defined in (9) are the TA singular values, and s^ = s^(s) as defined in (11) are the training data singular values associated with the teacher singular values s. Also · R denotes an average with respect to the MP distribution in (13) over a region R. Two distinct regions
contribute to training error. First Rin contains those top N2 - N 2 training data singular values
that do not correspond to the N 2 singular values of the teacher but will be learned by a rank N2 student. Second, Rout corresponds to the remaining N3 - N2 lowest training data singularvalues that cannot be learned by a rank N2 student. In terms of the MP distribution, Rout = [1 - A, f ] and Rin = [f, 1 + A], where f is the point at which the MP density has 1 - N2/N3 of itsmass to the left and N2/N3 of its mass to the right. In the simple case of a full rank student, f = 1 - A, and one need only integrate over Rin which is the entire range. Equation (14) for train makes it manifest
that it will go to zero for a full rank student as its singular values approach those of the training data.

Of course the test error can behave very differently. Inserting the TA training dynamics in (10) into test in (7), and using (11), (12) and (13) to relate training data to the teacher, we find

 -1



N2 N2

test(t) =  s2 (N2 -N 2) s(s^, t)2 Rin +

(s(t) - s)2 + 2s(t)s(1 - O(s)) 

=1

=1

(15)

Together (14) and (15) constitute a complete theory of generalization dynamics in terms of the

structure of the data distribution (i.e. the teacher rank N 2, teacher SNRs {s}, and the teacher aspect

ratio A = N 3/N 1), the architectural complexity of the student (i.e. its rank N2, its number of layers Nl, and the norm of its initialization), and the training time t. They yield considerable insight into the dynamics of good generalization early in learning and overfitting later, as we show below.

3.4 NUMERICAL TESTS OF THE THEORY OF NEURAL NETWORK GENERALIZATION DYNAMICS
Fig. 3 demonstrates an excellent match between the theory and simulations for the TA, and a close match for random students, for single-hidden-layer students and various teacher ranks N 2. Intuitively, as time t proceeds, learning corresponds to singular mode detection wave sweeping from large to small training data singular values (i.e. the wave in Fig. 1B sweeps across the training data spectrum in Fig 2A). Initially, strong singular values associated with large SNR teacher modes are learned and both train and test drop. Fig. 3A-D are for a rank 1 teacher, and so in Fig 3AB we see a single sharp drop early on, if the teacher SNR is sufficiently high. By contrast, with a rank 3 teacher in Fig. 3E-H, there are several early drops as the three modes are picked up. However, as time progresses,

6

Under review as a conference paper at ICLR 2019
Figure 4: Our theory applies to deeper networks: match between theory and simulation for rank 1 (row 1, a-d) and rank 3 (row 2, e-h) teachers with nl = 5 students: (a-b, e-f) log train and test error, respectively, showing very close match between theory and experiment for TA. (c,g) comparing TA and randomly initialized students minimum generalization errors, showing almost perfect match. (d,h) comparing TA and randomly initialized students optimal stopping times, showing large lag due to slower alignment in deeper networks. (N1 = 100, N2 = 50, N3 = 50.)
the singular mode detection wave penetrates the MP sea, and the student picks up noise structure in the data, so train drops but test rises, indicating the onset of overfitting. The main difference between the random student and TA learning curves is that the random student learning is slightly delayed relative to the TA, especially late in training. This is understandable because the TA already knows the singular vectors of the training data, while the random student must learn them. Nevertheless, two of the most important aspects of learning, namely the optimal early stopping time togpratdient  argminttest(t) and the minimal test error achieved at this time ogrpatdient  minttest(t), match well between TA and random student, as shown in Fig. 3CD. At low teacher SNRs, the student takes a little longer to learn than the TA, but their optimal test errors match. Our theory can also be easily extended to describe the learning dynamics of student networks with multiple hidden layers. As noted above, Saxe et al. (2013a) derived t(s, s^) for networks of arbitrary depth, so we only need to adjust this factor in our formulas, see App. A for details. In Fig. 4 we show that again there is an excellent match between TA networks and theory for student networks with Nl = 5 layers (i.e. 3 hidden layers). Randomly-initialized networks show a much longer alignment lag for deeper networks (see App. B for details), but the qualitative shape of the curves is similar. We also show demonstrate extensions of our theory to different numbers of training examples (App. G).
3.5 QUALITATIVE COMPARISON TO NONLINEAR NETWORKS Many of the phenomena we observe in our linear networks are qualitatively replicated in nonlinear networks (Fig. 5), suggesting that our theory may help guide understanding of the nonlinear case. In particular, features such as stage-like initial learning, followed by a plateau if SNR is sufficiently high, and finally followed by overfitting are replicated. However, there are some discrepancies, in particular nonlinear networks (especially deeper ones) begin overfitting earlier than linear networks. This is not entirely surprising, because a mode in a non-linear network can be co-opted by an orthogonal mode, while in a linear network it cannot. Thus in a non-linear network the noise dimensions are able to "stow away" on the strong signal dimensions as soon as the network has learned them.
3.6 RANDOMIZED DATA VS. REAL DATA: A LEARNING TIME PUZZLE
An intriguing observation that resurrected the generalization puzzle in deep learning was the observation by Zhang et al. (2016) that deep networks can memorize data with the labels randomly permuted. However, as Arpit et al. (2017) pointed out, the learning dynamics of training error for randomized labels can be slower than than for structured data. This phenomenon also arises in
7

Under review as a conference paper at ICLR 2019

N 2 = 1, Nl = 3

N 2 = 3, Nl = 3

N 2 = 1, Nl = 5

N 2 = 3, Nl = 5

Figure 5: Train (first row, A-D) and test (second row, E-H) error for nonlinear networks (leaky relu at all hidden layers) with one hidden layer (first two columns) or three hidden layers (last two columns) trained on the tasks above, with a rank 1 teacher (first and third columns) or a rank 3 teacher (second and fourth columns). Note that many of the qualitative phenomena observed in linear networks, such as stage-like improvement in the errors, followed by a plateau, followed by overfitting, also appear in nonlinear networks. Compare the first column to Fig. 3AB, the second column to Fig. 3EF, the third to Fig. 4AB, and the fourth to Fig. 4EF. (N1 = 100, N2 = 50, N3 = 50.)

Figure 6: Learning randomized data: Comparing (a) singular value distributions and (b) learning curves for data with a signal vs. random data that preserves basic statistics (mean, variance). Randomizing the data dilutes the signal singular values, spreading their variance out over many modes, hence randomly labelled data is learned more slowly. (N1 = 100, N2 = 50, N3 = 50.)

deep linear networks, and our theory yields an analytic explanation for why. We randomize data by

choosing orthonormal inputs x^µ as in the structured case, but we choose the outputs y^µ to be i.i.d.

Gaussian with zero mean and the same diagonal variance as the structured training data generated by

the teacher. For structured data generated by a low rank teacher with singular values s, the diagonal

output

variance

is

given

by

r2

=

1 N3

N2 i=

s¯2

+

1 N1

z2,

where

z

is

the

noise

variance,

as

before.

Since there is no relation between inputand output, 31 is now distributed as a MP distribution

whose support is [(r(1 - A), r(1 + A)]. Thus randomization essentially destroys the outlier

signal singular values in 31 reflecting the teacher, and distributes them across all randomized data

modes, yielding this stretched MP distribution (compare 6A top and bottom). However, even on this

stretched MP distribution, the right edge will be much smaller than the signal singular values, since

the signal variance will be diluted by spreading it out over many more modes in the randomized data.

Thus the randomized data will lead to slower initial training error drops relative to the structured

data (Fig. 6B) since the singular mode detection wave encounters the first signal singular values in

structured data earlier than it encounters the edge of the stretched MP sea in randomized data.

8

Under review as a conference paper at ICLR 2019

Task 1 outputs
···

Relative signal strength?

Task 2 outputs
···

Multi-task outputs
··· ···

IID Gaussian noise (fixed across training)

outputs

targets

···

···

+=

···

···
N1 inputs Task 1 teacher

Aligned?

···
N1 inputs Task 2 teacher

···
N1 inputs Multi-task teacher

inputs Training

···
Student

Figure 7: Transfer setting­ If two different tasks are combined, how well students of the combined teacher perform on each task depends on the alignment and SNRs of the teachers.

 Figure 8: Transfer benefit T AB(sA, sB, q) plotted at different values of sA. (a) sA = 0.84 = 4 A.

Although this task is impossible to learn on its own, with support from another aligned task, especially

one with high SNR, learning can occur. (b) sA = 3. Tasks with modest signals will face interference

from poorly aligned tasks, but benefits from well aligned tasks. These effects are amplified by

SNR. (c) sA = 100. Tasks with very strong signals will show little effect from other tasks (note

y-axis scales), but any impact will be negative unless the tasks are very well aligned. (N1 = 100

N

A 2

=

N

B 2

=

1,

N2

=

50,

N3

=

50.)

3.7 OUT-PERFORMING OPTIMAL EARLY STOPPING THROUGH A NON-GRADIENT ALGORITHM

For the case of a rank 1 teacher, it is straightforward to derive a good analytic approximation to the important quantities gorpatdient and togrpatdient. We assume the teacher SNR is beyond the phase transition point so its unique singular value s1 > A1/4, yielding a separation between the training data singular value s^1 in (11) and the edge of the MP sea. In this scenario, optimal early stopping will occur at a time before the detection wave in Fig. 1B penetrates the MP sea, so to minimize test error, we can
neglect the first term in (15). Then optimizing the second term yields the optimal student singular value s1 = s1O(s1). Inserting this value into (15) yields gopratdient = 1 - O(s1)2, and inserting it into (9) yields togpratdient. Thus the optimal generalization error with a rank 1 teacher is very simply related to the alignment of the top training data singular vectors with the teacher singular vectors, and it
decreases as this alignment increases. In App. E, we show the match between the error achieved by
gradient descent on the network and this estimated optimal stopping error for a rank 1 teacher.

With higher rank teachers, gopratdient and togrpatdient must negotiate a more complex trade-off between teacher modes with different SNRs. For example, as the singular mode detection wave passes the top

training data singular value, s1(t)  s^1 which is greater than the optimal s1 = s1O(s1) for mode 1. Thus as learning progresses, the student overfits on the first mode but learns lower modes. However,

this neural generalization dynamics suggests a superior non-gradient training algorithm that simply

optimally sets each s to sO(s) in (15), yielding an optimal generalization error:

 -1 



N2 N2

nopont -gradient =  s2  s2(1 - O(s)2) .

(16)

=1

=1

Standard gradient descent learning cannot achieve this low generalization error because it cannot independently adjust all student singular values. A simple algorithm that achieves onpont -gradient is as

9

Under review as a conference paper at ICLR 2019

follows. From the training data covariance 31, extract the top singular values s^ that pop-out of the MP sea, use the functional inverse of (11) to compute sa(s^), use (12) to compute the optimal s, and then construct a matrix W with the same top singular vectors as 31, but with the outlier singular
values shrunk from s^ to s and the rest set to zero. This non-gradient singular value shrinkage algorithm provably outperforms neural network training with nopont -gradient  gopratdient.

4 A THEORY FOR THE TRANSFER OF KNOWLEDGE ACROSS MULTIPLE TASKS

Consider

two

tasks

A

and

B,

described

by

N3

by

N1

teacher

maps

WA

and

WB ,

of

ranks

N

A 2

and

N

B 2

,

respectively.

Now

two

student

networks

can

learn

from

the

two

teacher

networks

separately,

each achieving optimal early stopping test errors oApt and Bopt. Alternatively, one could construct a

composite teacher (and student) that concatenates the hidden and output units, but shares the same

input units (Fig. 7). The composite student and teacher each have two heads, one for each task, with

N 3 neurons per head. Optimal early stopping on each head of the student yields test errors oApt B and oBpt A. We define the transfer benefit that task B confers on task A to be T AB  oApt - Aopt B. A postive (negative) transfer benefit implies learning tasks A and B simultaneously yields a lower

(higher) optimal test error on task A compared to just learning task A alone.

A foundational question is how the transfer benefit T AB depends on the two tasks defined by

the teachers WA and WB. To answer this, consider the SVDs of each teacher alone: WA = UASAVAT and WB = UBSBVBT . From the above, we know that Aopt depends on WA only through SA. In App. D we show that the transfer benefit depends on both WA and WB only through

SA,

SB ,

and

the

N

A 2

by

N

B 2

similarity

matrix

Q

=

VAT

VB .

If

we

think

of

the

columns

of

each

V

as spanning a low dimensional feature space in N 1 dimensional input space that is important for each

task, then Q reflects the input feature subspace similarity matrix. Interestingly, the transfer benefit

is independent of output singular vectors UA and UB. What matters for knowledge transfer in this

setting are the relevant input features, not how you must respond to them.

We describe the transfer benefit for the simple case of two rank one teachers. Then SA, SB, and Q are simply scalars sA, sB and q, and we explore the function T AB(sA, sB, q) in Fig. 5ABC, which reveals several interesting features. First, knowledge can be transferred from a high SNR task to a low SNR task (Fig. 5A) and the degree of transfer increases with task alignment q. This can make it possible to capture signals from task A which would otherwise sink into the MP sea by learning jointly with a related task, even if the tasks are only weakly aligned (Fig. 5A). However, if task A already has a high SNR, task B must be very well aligned to it for transfer to be beneficial ­ otherwise there will be interference. The degree of alignment required increases as the task A SNR increases, but the quantity of benefit or interference decreases correspondingly (Fig. 5BC). In Appendices D and F we both explain why our theory predicts interference between tasks that are not well aligned, and we demonstrate these phenomena are qualitatively recapitulated in nonlinear networks.

5 DISCUSSION
In summary, our analytic theory of generalization dynamics in deep linear networks reveals that many puzzling aspects of generalization in deep learning already arise in the simple linear setting, where the puzzles can be understood analytically. In particular, deep linear networks learn more important structure in data first, leading to generalization errors that depend on task structure but not network size. Our theory explains why deep linear networks learn randomized data more slowly than structured data, and provides a non-gradient based learning method that out-performs gradient descent learning. Finally, we provide an analytic theory of how knowledge is transferred from one task to another, demonstrating that the degree of alignment of input features important for each task, but not how one must respond to these features, is critical for facilitating knowledge transfer. Among other things, we hope our work will motivate and enable: (1) the search for tighter upper bounds on generalization error that take into account task structure; (2) the design of non gradient based training algorithms that out-perform gradient based learning in general settings; and (3) the theory driven design of auxiliary tasks that maximize knowledge transfer.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv, pp. 1­32, 2017.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint, pp. 1­39, 2018. URL http://arxiv. org/abs/1802.05296.
Devansh Arpit, Stanislaw Jastrze¸bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A Closer Look at Memorization in Deep Networks. arXiv preprint, 2017. ISSN 1938-7228. URL http://arxiv.org/abs/1706.05394.
Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. arXiv preprint, pp. 1­24, 2017. URL http://arxiv.org/abs/1706.08498.
Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian Complexities : Risk Bounds and Structural Results. Journal of Machine Learning Research, 3:463­482, 2002.
Florent Benaych-Georges and Raj Rao Nadakuditi. The singular values and vectors of low rank perturbations of large rectangular random matrices. Journal of Multivariate Analysis, 111:120­135, 2012. ISSN 0047259X. doi: 10.1016/j.jmva.2012.04.019.
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. Multi-Task Learning for Multiple Language Translation. Acl, pp. 1723­1732, 2015.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data. arXiv preprint, 2017. URL http://arxiv.org/abs/1703.11008.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-Independent Sample Complexity of Neural Networks. arXiv preprint, (1):1­26, 2017. URL http://arxiv.org/abs/1712. 06541.
I.J. Goodfellow, J Pouget-Abadie, and Mehdi Mirza. Generative Adversarial Networks. arXiv preprint, pp. 1­9, 2014. ISSN 10495258. doi: 10.1001/jamainternmed.2016.8245.
Steven S. Hansen, Andrew Lampinen, Gaurav Suri, and James L. McClelland. Building on prior knowledge without building it in. Behavioral and Brain Sciences, 40, 2017.
Andrew Lampinen, Shaw Hsu, and James L Mcclelland. Analogies Emerge from Learning Dyamics in Neural Networks. Proceedings of the 39th Annual Conference of the Cognitive Science Society, pp. 2512­2517, 2017.
Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. Iclr, pp. 1­9, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei a Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015. ISSN 0028-0836. doi: 10.1038/nature14236.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. Conference on Learning Theory (COLT), pp. 1376­1401, 2015. ISSN 15337928. URL http://arxiv.org/abs/1503.00036.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks. arXiv preprint, (2017):1­9, 2017. URL http://arxiv.org/abs/1707.09564.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. Advances in Neural Information Processing Systems 30, (Nips):1­11, 2017. URL http://arxiv.org/abs/1711.04735.
11

Under review as a conference paper at ICLR 2019

Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The Emergence of Spectral Universality in Deep Networks. In AISTATS 2018, 2018. URL http://arxiv.org/abs/1802. 09979.
Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy Distillation. arXiv, pp. 1­12, 2015. ISSN 0028-0836. doi: 10.1038/nature14236.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. Advances in Neural Information Processing Systems, pp. 1­9, 2013a.
Andrew M Saxe, James L Mcclelland, and Surya Ganguli. Learning hierarchical category structure in deep neural networks. Proceedings of the 35th annual meeting of the Cognitive Science Society, pp. 1271­1276, 2013b.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information Propagation. In International Conference on Learning Representations (ICLR),, number 2016, pp. 1­18, 2016. URL http://arxiv.org/abs/1611.01232.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, and Koray Kavukcuoglu. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7585):484­489, 2016. ISSN 0028-0836. doi: 10.1038/nature16961. URL http://dx.doi.org/10.1038/nature16961.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint, 2016. ISSN 10414347. doi: 10.1109/TKDE.2015.2507132. URL http://arxiv.org/abs/1611.03530.

A LEARNING DYNAMICS FOR DEEPER NETWORKS

In the main text, we described the dynamics of how a single-hidden-layer network converges toward the training data singular modes {s^, u^, v^}, which were originally derived in Saxe et al. (2013a).
There it was also proven that for a network with Nl layers (i.e. Nl - 2 hidden layers), the strength of
the mode obeys the differential equation:



d dt

u

=

(Nl

-

1)u2-2/(Nl-1)(s

-

u)

This equation is separable and can be integrated for any integer number of layers. In particular, we

consider the case of 5 layers (3 hidden), in which case:

 t(s, s^) =
2

tanh-1 s^3/2

u s^

s
- 1 s^ u

This expression cannot be analytically inverted to find s(t, s^), so we numerically invert it where necessary.

B ALIGNMENT LAG IN RANDOMLY INITIALIZED NETWORKS
As noted in the main text, the randomly-initialized networks behave quite similarly to the TA networks, except that the randomly-initialized networks show a lag due to the time it takes for the network's modes to align with the data modes. In fig. 9 we explore this lag by plotting the alignment of the modes and the increase in the singular value for several randomly initialized networks.
Notice that stronger modes align more quickly. Furthermore, the mode alignment is relatively independent ­ whether the teacher is rank 1 or rank 3, the alignment of the modes is similar for the mode of singular value 2. Most importantly, note how the deeper networks show substantially slower mode alignment, with alignment not completed until around when the singular value increases. This explains why deeper networks show a larger lag between randomly-initialized and TA networks ­ the alignment process is much slower for deeper networks.

12

Under review as a conference paper at ICLR 2019
Figure 9: Alignment of randomly-initialized network modes to data modes and growth of singular values, plotted for 1 hidden layer (first two rows, a-d) and 3 hidden layers (last two rows, e-h), and for a rank 1 teacher (first and third rows, a & e), or a rank 3 teacher (second and fourth rows, b-d & f-h). The columns are the different modes, with respective singular values of 6, 4, and 2. z was set to 1. The deeper networks show substantially slower mode alignment, with alignment not completed until around when the singular value increases.
13

Under review as a conference paper at ICLR 2019

C TRAIN AND TEST ERRORS AFTER A PROJECTION

In the case of transfer learning, or more generally when we want to evaluate a network's loss on a subset of its outputs, we need to use a slight generalization of the train and test error formulas given in the main text. Suppose we are interested in the train and test errors after applying a projection operator P:

train 

N1 µ=1

||PWx^µ

-

Py^µ||22

,

N1 µ=1

||Py^µ||22

test 

respectively. As in the main text, we can rexpress these as:

N1 µ=1

||PWxµ

-

Pyµ||22

,

N1 µ=1

||Pyµ

||22

(17)

Tr WT PT PW - 2Tr WT PT P31 + Tr 31T PT P31

train =

Tr 31T PT P31

,

(18)

Tr WT PT PW - 2Tr WT PT PW + Tr WT PT PW

test =

Tr WT PT PW

.

Using the cyclic property of the trace, we can modify these to get:

(19)

Tr PWWT PT - 2Tr P31WT PT + Tr P3131T PT

train =

Tr P3131T PT

,

(20)

Tr PWWT PT - 2Tr PWWT PT + Tr PWWT PT

test =

Tr PWWT PT

.

(21)

As before, we express these in terms of the student, training data and teacher SVDs, W = USVT ,

31 = U^ S^V^ T , and W = U S VT respectively. Specifically,

train = test =

N3 =1

s^2 ||Pu^||22

-1

N2 =1

s2 ||Pu||22

-1

N2 =1

s2 ||Pu||22+

N3 =1

s^2 ||Pu^||22-2

N2 =1

( )( )N3
=1

s s^

Pu ·Pu^

v ·v^

,

(22)

N2 =1

s2 ||Pu ||22 +

N2 =1

s2 ||Pu||22-2

N2 =1

( )( )N2
=1

s s

Pu ·Pu

v ·v

.

(23)

D TRANSFER LEARNING DERIVATIONS & DETAILS

Thm 1 (Transfer theorem) The transfer benefit T AB:

· Is unaffected by the UA and UB.

·

Is

completely

determined

by

only

z2,

SA,

SB ,

and

the

N

A 2

by

N

B 2

similarity matrix Q

=

VAT VB.

Proof: We define

N

A 2

N

B 2

UAB = N 3 UA

0

N 3 0 UB

N

A 2

N

B 2

SAB =

N

A 2

SA

0

N

B 2

0

SB

VAB = N 1

N

A 2

VA

N

B 2

VB

WA+B =

UA 0 0 UB

SA 0 0 SB

 VAT   VBT 

(24)

14

Under review as a conference paper at ICLR 2019

Because of the 0 blocks in UAB, the vectors in blocks corresponding to task A and task B are completely orthogonal, so UAB remains orthonormal. Thus the relationship between the UA and UA is irrelevant to the transfer. (In our simulations we use arbitrary orthonormal matrices for UA and UB.) Therefore the transfer effects will be entirely driven by the relationship between the matrices VA and VB and the singular values.

We

define

N

A 2

by

N

B 2

similarity matrix Q

=

VAT VB.

If we think of the columns of each V

as spanning a low dimensional feature space in N 1 dimensional input space that is important for

each task, then Q reflects the input feature subspace similarity matrix. We can now calculate the

singular values of WA+B. First, note that the input singular modes of WA+B are eigenvectors of

WA+BT WA+B, and the associated singular values are square roots of the eigenvalues of WA+B.

Now

WA+B T WA+B = VAB SAB UAB T UAB SAB VAB T = VAB SAB 2VAB T

Now if c is an eigenvector of this matrix:

VAB SAB 2VAB T c = c

This implies that VAB T VAB SAB 2VAB T c = VAB T c
Hence eigenvalues of VABSAB2VABT are also eigenvalues of VABT VABSAB2, with the mapping between the eigenvectors given by VAB. Furthermore, this mapping must be a bijection for eigenvectors with non-zero eigenvalues, since the matrices have the same rank (the rank of VAB). To see this, note that SAB2 is full rank. From this, it is clear that
rankVABT VABSAB2 = rankVABT VAB = rankVAB. Furthermore, SAB2 is positive definite, so
rankVABSAB2VABT = rankVAB.

Now that we know the eigenvectors of these matrices are in bijection, note that:

VAB T VAB SAB 2

 =

VAT VB T

 

VA

VB

 SA2 
0



0 SB 2

=

I QT

Q I

 SA2 
0


0 SB2 

Because the output modes don't matter (as noted above), the alignment between the eigenvectors of VABSAB2VABT and VA, weighted by their respective eigenvalues, gives the transfer benefit. In
other words:

For any given tasks, the transfer benefit can be calculated using our theory. However, in certain

special cases, we can give exact answers. For example, in the rank one case with equal singular

values between the tasks (sA = sB = s), the matrix

I

Q  SA2

 0

QT I

 0

SB2 

reduces to

1 q

q 1

s2



with eigenvalues s 1 ± q and eigenvectors

11 1 -1

Corresponding to the shared structure between the tasks and the differences between them. We note that the sign of the alignment q is irrelevant as a special case of the fact (noted above) that any orthogonal transformation on the output modes does not affect transfer.

15

Under review as a conference paper at ICLR 2019
D.1 MISALIGNMENT AND INTERFERENCE Why is there interference between tasks which are not well aligned? In the rank one case, we are effectively changing the (input) singular dimensions of YA from VA to VAB. The two singular modes of VAB correspond to the shared structure between the tasks (weighted by the relative signal strengths), and the differences between them, respectively. Although we may be improving our estimates of the shared mode if q > 0 (by increasing its singular value relative to sA), we are actually decreasing its alignment with VA unless q = 1. This misalignment is captured by the second mode of VAB, but the increase in the singular value of the first mode must come at the cost of a decrease in the singular value of the second mode. See Fig. 10 for a conceptual illustration of this. This means that the multi-task setting allows the distinctions between the tasks to sink towards the sea of noise, while pulling out the common structure. In other words, transferring knowledge from one task always comes at the cost of ignoring differences between the tasks. Furthermore, incorporating a task B allows its noise to seep into the task A signal. Together, these two effects help to explain why transfer can be sometimes beneficial but sometimes detrimental.
Figure 10: Conceptual cartoon of how T AB, the transfer benefit (or cost) arises from alignment between the task's input modes.
E NON-GRADIENT TRAINING ALGORITHM
In Fig. 11 we show the match between the error achieved by training the student by gradient descent and the optimal stopping error predicted by the non-gradient shrinkage algorithm in the case of a rank-1 teacher.
Figure 11: Match between optimal stopping error prediction from non-gradient training algorithm and empirical optimal stopping error for a rank-1 teacher.
F TRANSFER RESULTS GENERALIZE TO NON-LINEAR NETWORKS
Since most deep learning practitioners do not train linear networks, it is important that our theoretical insights generalize beyond this simple case. In this section we show that the transfer patterns qualitatively generalize to non-linear networks. Here, we show results from teacher networks with N 1 = 100 N 3 = 50, N 2 = 4 (thus the task is higher rank) and leaky relu non-linearities at the hidden and output layers. We train a student with
16

Under review as a conference paper at ICLR 2019
leaky relu units and N2 = N3 to solve this task. Results qualitatively look quite similar to those in Fig 5. of the main text for rank one linear teachers, see below. Thus our insights into transfer may help to understand multi-task benefits in more complicated architectures.

Figure 12: Transfer benefit T AB(sA, sB, q) for non-linear teachers and students, plotted at different values of sA. (a) sA = 0.84 = 4 A. With support from another aligned task, especially one with moderately higher SNR, performance on a low SNR task will improve. (b) sA = 3. Tasks with modest signals will face interference from poorly aligned tasks, but benefits from well aligned tasks.
These effects are amplified by SNR. (c) sA = 100. Tasks with very strong signals will show little effect from other tasks (note y-axis scale), but any impact will be negative unless the tasks are very
well aligned.

G VARYING THE NUMBER OF TRAINING EXAMPLES

In the main text, we focused on the test error dynamics in the case in which the number of examples
equalled the number of inputs. Here we show how the formula for test error curves is modified as the number of training examples P is varied. For simplicity, when P = N1, we focus on the case of a full rank student with aspect ratio A = 1 (so that N1 = N2 = N3). The more general case of lower rank students with non-unity aspect ratios can be easily found from this case, but with some
additional bookkeeping.

As before, we assume the teacher generates noisy outputs from a set of P inputs:

y^µ = Wx^µ + zµ for µ = 1, . . . , P.

(25)

This training set yields important second-order training statistics that will guide student learning:

11  X^ X^ T 31  Y^ X^ T = WX^ X^ T + ZX^ T .

(26)

Here X^ , Y^ , and Z are each N 1 by P , N 3 by P , and N 3 by P matrices respectively, whose µ'th columns are x^µ, y^µ, and ^zµ, respectively. 11 is an N 1 by N 1 input correlation matrix, and 31 is an N 3 by N 1 the input-output correlation matrix. We choose the matrix elements ziµ of the noise matrix Z to be drawn iid from a Gaussian with zero mean and variance z2/N 1. The noise scaling is chosen so the singular values of the teacher W and the noise Z are both O(1), leading to non-trivial
generalization effects. Furthermore, we chose training inputs to be close to unit-norm, and make the input covariance matrix 11 as white as possible (whitening is a common pre-processing step for inputs). When P > N 1, this can be done by choosing the rows of X^ to be orthonormal and
then scaling up by P/N 1, so the columns are approximately unit norm. Then 11 = P/N 1I
is proportional to the identity. On the otherhand, if P < N 1, we choose the columns of X^ to be orthonormal, so that 11 = P||, where P|| is a projection operator onto the P dimensional column space of X^ spanned by the input examples. Both these choices are intended to approximate the situation in which the columns of X^ are chosen to be iid unit-norm vectors. Finally, as generalization performance will depend on the ratio of teacher singular values to the noise variance parameter z2, we simply set z = 1 as in the main text. Thus, given the unit-norm inputs, we can think of the teacher singular values as signal to noise ratios (SNRs). We now examine how the dynamics of the
test error evolves as we vary the number of training examples P . We split our analyses into two distinct regimes: (1) the oversampled regime in which the data density D  P/N1 > 1, and (2) the undersampled regime in which D < 1.

17

Under review as a conference paper at ICLR 2019
Figure 13: The effects of varying the number of training examples P . (a) Test error for a student learning from a rank-1 teacher with an SNR of 3, with different numbers of inputs. (b,c) Minimum generalization error plotted against P/N1 and SNR · P/N1, respectively, at different SNRs. When P  N1, the minimum generalization error is simply determined by SNR P/N1, so all curves converge to a single asymptotic line in (d) as P increases. When P < N1, however, the curves for different SNRs separate because the projection and noise effects depend on initial SNR. (e) Optimal stopping error for gaussian vs. orthogonal inputs, showing a strong correlation. Thus our use of orthogonal inputs in the theory also yields insight into the more general case of approximately unit norm Gaussian inputs. (For all panels N1 = N2 = N3 = 100, N 2 = 1.)
G.1 THE OVERSAMPLED REGIME The oversampled regime (D > 1) is relatively simple. First 11 is scaled up by a factor of D. And in the input-output covariance matrix, 31 = WX^ X^ T + ZX^ T , the signal component, WX^ X^ T is scaled up by a factor of D while the noise component ZX^ T has the same singular value spectrum as the D = 1 case, up to anoverall scaling by D (since the rows of X^ are orthogonal and all its singular values are equal to D). This leads to an increase in the effective SNR by a factor of D. Thus overall, the test error curves for the case of D > 1 can be simply obtained from the theory of the test error curves for D =1 through two modifications: (1) a boost in the SNR for the D = 1 case by a multiplicative factor of D, and (2) and an overall speed up in the learning time by a multiplicative factor of D.
G.2 THE UNDERSAMPLED REGIME For the undersampled regime (D < 1), we must account for the fact that the P training inputs do not span the full N1 dimensional space of all inputs. Thus the projection operator P|| onto the P dimensional column space of X^ plays a crucial role. Indeed the input-correlation 11 = P||. And 31 = WP|| + ZX^ T . This implies that the learning dynamics only transforms the composite student map W from the P dimensional subspace spanned by the inputs to the N3 dimensional output space. In contrast, the student map from the N1 - P dimensional subspace orthogonal to the image of P|| remains frozen. Tracing through the equations of the main paper and accounting for the projection operator P||, we find the effective aspect ratio for this undersampled learning problem (when N3 = N2 = N1) is no longer A = N3/N1 but rather D = P/N1. Furthermore, in the limit N 3, N 1   while N 2 remains O(1), the singular values of the signal component WP|| of 31 are attenuated by a factor of D, making the associated singular vectors more susceptible to noise. Again tracing through the equations of the main paper, with all of these modifications, we find the
18

Under review as a conference paper at ICLR 2019

final formula for test error curves in the undersampled measurement regime:

(N3 - P ) 2 + (P -N 2) s(s^, t)2 + test(t) =

N2 =1

(s (t)

-

s )2

+

2s (t)s (1

-

 O( Ds))

N2 =1

s2

(27)

This equation has several modifications compared to the case P = N1 in (15). First the term in the numerator involving N3 - P reflects generalization error due to the N3 - P dimensional frozen subspace, and the initial weight variance 2 contributes to this generalization error. The second term in the numerator involves all the P - N 2 training modes which cannot be correlated with the teacher, and the average · is over a Marcenko-Pasteur distribution of singular values (see (13)) except with the aspect ratio A replaced by D. The third term accounts for learned correlations between the
student and teacher. It involves the transformation from teacher singular values s to training data singular values s^ through the formula (11) except with the aspect ratio replacement A  D, and the effective teacher singular value attenuation s  Ds. Similarly, the computation of the singular vector overlap is done through (12) also with the replacements A  D and s  Ds.

G.3 COMPARISON OF THEORY AND EXPERIMENT FOR UNDER AND OVER SAMPLED
MEASUREMENT REGIMES
In Fig. 13, we show an excellent match between our theory and empirical simulations for varying values of P , both in the oversampled and undersampled measurement regimes. There are a number of interesting features to note. First, although the minimum generalization error improves monotonically with P , the asymptotic (t  ) generalization error does not, because of a frozen subspace (Advani & Saxe, 2017) of the modes that are not overfit when P < N1, because the training data rank is  P . Second, when P  N1, the minimum generalization error is simply determined by SNR P/N1, so all curves converge to a single asymptotic line as P increases. When P < N1, however, the curves for different SNRs separate because the projection and noise effects depend on initial SNR. Finally, in Fig. 13D we show that approximately unit norm i.i.d. gaussian inputs yield similar results to the orthogonalized data matrices we employed in the theory, although the gaussian inputs do result in slightly higher optimal stopping error.

H LESS THAN FULL RANK STUDENTS
Although we generally assumed students were full rank in the main text to simplify the calculations, our theory remains exact for TA networks of any rank. Furthermore, as shown in Fig. 14, the TA and random networks again show very similar optimal stopping generalization error, but with the optimal stopping time of the random networks lagging behind that of the TA networks. Furthermore, this lag increases as the rank of the random network decreases (because a low rank network will have less initial projection onto the random modes, there is is more alignment to be done). However, reducing the student rank does not change the optimal stopping error (as long as it is still greater than the teacher rank).
19

Under review as a conference paper at ICLR 2019
(a) Best generalization error is quite similar(b) Optimal stopping time is quite similar between aligned and between aligned and random initializations random initializations
(c) Optimal generalization error vs. optimal(d) Optimal generalization error vs. optimal stopping time for stopping time for randomly initialized networksinitially aligned networks Figure 14: Empirical verification that the simplifying assumptions of our theory are approximately valid in the regime we are considering at different student ranks. Initializations with random initial weights (random init.) and initializations with initial weight aligned to the noisy data SVD (aligned init.) are compared across varying student ranks. (a) The minimum generalization errors are almost identical between the different initializations and different student ranks. (b) The optimal stopping time in the randomly initialized networks consistently lags behind the aligned networks, because it takes time for the alignment to occur. This lag increases as the students rank decreases. (c) Randomly initialized networks of varying ranks obey qualitatively similar trends of increase in optimal stopping error and optimal stopping time as SNR decreases. (d) The theory predicts the aligned networks trends of increase in optimal stopping error and optimal stopping time with decreasing SNR almost perfectly. (All plots are made with a rank 1 teacher and N1 = N3 = 100)
20

