Under review as a conference paper at ICLR 2019
THREE MECHANISMS OF WEIGHT DECAY REGULARIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of L2 regularization. Literal weight decay has been shown to outperform L2 regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and KFAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) regularizing approximated input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks.
1 INTRODUCTION
Weight decay is one of the standard tricks in the neural network toolbox; there are some classic theoretical results about generalization (Bos & Chug, 1996; Bös, 1996; Krogh & Hertz, 1992), and is used in a large fraction of neural networks today (Huang et al., 2017; He et al., 2016). In the gradient descent setting, weight decay can be derived from the gradient of the L2 norm of the weights; hence, weight decay has traditionally been interpreted as a form of L2 regularization. However, several recent findings cast doubt on this interpretation:
· Weight decay has sometimes been observed to improve training accuracy, not just generalization performance (e.g. Krizhevsky et al. (2012)).
· Loshchilov & Hutter (2017) found that when using Adam as the optimizer, literally applying weight decay (i.e. scaling the weights by a factor less than 1 in each iteration) was a far more effective regularizer than adding an L2 regularizer to the training objective.
· Weight decay is widely used in networks with batch normalization (BN) (Ioffe & Szegedy, 2015). In principle, weight decay regularization should have no effect in this case, since one can scale the weights by a small factor without changing the network's predictions. Hence, it does not meaningfully constrain the network's capacity.
The effect of weight decay remains poorly understood, and we lack clear guidelines for which tasks and architectures it is likely to help or hurt. A better understanding of the role of weight decay would help us design more efficient and robust neural network architectures. In order to better understand the effect of weight decay, we experimented with both weight decay and L2 regularization applied to image classifiers using three different optimization algorithms: SGD, Adam, and Kronecker-Factored Approximate Curvature (KFAC) (Martens & Grosse, 2015). (In this paper, weight decay refers to rescaling the weights by a fixed factor less than 1 after each weight update.) Consistent with the observations of Loshchilov & Hutter (2017), we found that weight decay consistently outperformed L2 regularization in cases where they differ. Weight decay gave an especially strong performance boost to the KFAC optimizer, and closed most of the generalization gaps between first- and second-order optimizers, as well as between small and large batches.
1

Under review as a conference paper at ICLR 2019

KFAC-G

CIFAR10-ResNet32

KFAC-F

ADAM

SGD 0.84

0.86 0.88
accuracy

0.90

L2 Baseline CIFAR10-ResNet32 (BN+Aug)

Weight Decay CIFAR100-VGG16 (BN+Aug)

CIFAR100-ResNet32 (BN+Aug)

0.92 0.93 0.94 0.95 0.60
accuracy

0.65 0.70
accuracy

0.75 0.72

0.74 0.76
accuracy

0.78

Figure 1: Comparison of test accuracy of the networks trained with different optimizers on both CIFAR10 and CIFAR100 datasets. We compare Weight Decay regularization to L2 regularization and the Baseline (w/o regularization). To be noted, BN+Aug denotes with BN and data augmentation during training. KFAC-G and KFAC-F denote using Gauss-Newton and Fisher as the precondition matrix respectively. The results suggest that weight decay leads to improved performance over different optimizers and settings.

We then investigated the reasons for weight decay's performance boost. Surprisingly, we identified three distinct mechanisms by which weight decay has a regularizing effect, depending on the particular algorithm and architecture:

1. In our experiments with first-order optimization methods (SGD and Adam), we found that combined with BN, weight decay reduced the scale of the weights, hence increasing the effective learning rate, which increases the stochastic regularization effect (an effect previously observed by Srivastava et al. (2014); Neelakantan et al. (2015); Keskar et al. (2016)). As evidence, almost all of the generalization effect of weight decay was due to applying BN on layers (for which L2 regularization is meaningless). Furthermore, when we computed the effective learning rate for the network with weight decay, and applied the same effective learning rate to a network without weight decay, this captured the full regularization effect.
2. We show that when KFAC is applied to a linear network using the Gauss-Newton metric (KFAC-G), weight decay is equivalent to regularizing the squared Frobenius norm of the input-output Jacobian (which was shown by Novak et al. (2018) to improve generalization). Empirically, we found that the Gauss-Newton norm (which KFAC with weight decay is implicitly regularizing) is highly correlated with the Jacobian norm of real image classification networks, and that KFAC with weight decay significantly reduces the Jacobian norm.
3. Because the idealized, undamped version of KFAC is invariant to affine reparameterizations, the implicit learning rate effect described above should not apply. However, in practice the approximate curvature matrix is damped by adding a multiple of the identity matrix, and this damping is not scale-invariant. We show that without weight decay, the weights grow large, causing the effective damping term to increase, and may dominate the curvature term. Weight decay kept the effective damping term small, enabling KFAC to retain its second-order properties, and correspondingly improved generalization.

Hence, we have identified three distinct mechanisms by which weight decay improves generalization, depending on the optimization algorithm and network architecture. Our results underscore the subtlety and complexity of neural network training: the final performance numbers obscure a variety of complex interactions between phenomena. While more analysis and experimentation is needed to understand how broadly each of our three mechanisms applies (and to find additional mechanisms!), our work provides a starting point for understanding practical regularization effects in neural network training.

2 PRELIMINARIES
Natural gradient. For loss function L(), its gradient L is the direction of change to  that causes L() to increase most rapidly, assuming that  resides in Euclidean space. The natural gradient generalizes the gradient to allow  to lie on a Riemannian manifold with metric tensor C(). For C to describe a Riemannian manifold, it must vary smoothly from point to point and must be positive definite for all , in which case || · ||C() is a valid norm. Often, C() is taken to be Fisher
2

Under review as a conference paper at ICLR 2019

information matrix F for parameter distribution (Amari, 1998), where F is defined as1: F = E  log p(y|x, ) log p(y|x, )

(1)

In optimization, natural gradient can be derived by adapting the steepest descent formulation to use

an alternative definition of (local) distance (Martens, 2014). For Fisher matrix, the particular distance

function turns out to be

DKL(p

p+ )



1 
2

F

(2)

The distance function­KL divergence­is defined on the predictive distribution p(y|x, ), and therefore

is invariant to the reparameterization of the neural network.

Gauss-Newton metric. Another sensible distance function is the L2 distance on the output of the neural network, i.e. ||f+ - f||22. This is equivalent to using Gauss-Newton metric for the Riemannian manifold, which is defined as:

G = E J J

(3)

where J is the Jacobian of f(x) w.r.t the parameters . It is easy to see that ||f+ - f||22   G locally. Heskes (2000) showed that the Fisher information matrix and the Gauss-Newton
matrix are equivalent in the case of squared error loss. Later, Martens (2014) extended the equivalence
to more general settings and showed that the Fisher information matrix is equivalent to generalized Gauss-Newton matrix when model prediction p(y|x, ) corresponds to exponential family model
with natural parameters given by f(x), where the generalized Gauss-Newton matrix is given by

G = E J H J where H is the Hessian of (y, z) w.r.t z, evaluated at z = f(x).

(4)

KFAC. Kronecker-factored approximate curvature (KFAC) Martens & Grosse (2015) uses a Kronecker-factored approximation to the curvature matrix to perform efficient approximate natural gradient updates. As shown in Luk & Grosse (2018), KFAC can be applied to general pullback metric. For convenience, we introduce KFAC here using the Fisher metric.

Considering l-th layer in the neural network whose input activations are al  Rn1 , weight Wl  Rn1×n2 , and output sl  Rn2 , we have sl = Wl al. Therefore, weight gradient is Wl L = al(sl L) . With this gradient formula, KFAC decouples this layer's fisher matrix Fl using mild approximations,

Fl = E vec{Wl L}vec{Wl L} = E {sl L}{sl L}  alal  E {sl L}{sl L}  E alal = Sl  Al

(5)

Where Al = E aa and Sl = E {sL}{sL} . Further, assuming between-layer independence, the whole fisher matrix F can be approximated as block diagonal consisting of layerwise
fisher matrices Fl. Decoupling Fl into Al and Sl not only avoids the memory issue saving Fl, but also provides efficient natural gradient computation.

Fl-1vec{Wl L} = S-l 1  Al-1vec{Wl L} = vec[A-l 1Wl LSl-1]

(6)

As shown by equation 6, computing natural gradient using KFAC only consists of matrix transformations comparable to size of Wl, making it very efficient.

3 THE EFFECTIVENESS OF WEIGHT DECAY

Our goal is to understand weight decay regularization in the context of training deep neural networks. Towards this, we first analyze the relationship between L2 regularization and weight decay in different optimizers, which has long been confused as being the same in the deep learning community.

The classical weight decay is defined by the following update rule: t+1  (1 - )t - L(t)

(7)

1The underlying distribution for the expectation in equation 1 has been left ambiguous. Throughout the experiments, we sample the targets from the model's predictions, as done in Martens & Grosse (2015)

3

Under review as a conference paper at ICLR 2019

Table 1: Classification results on CIFAR-10 and CIFAR-100. B denotes BN while D denotes data augmentation including horizontal flip and random crop. WD denotes weight decay regularization.

Dataset

Network B D

SGD WD

ADAM WD

KFAC-F WD

KFAC-G WD

CIFAR-10 VGG16
CIFAR-10 ResNet32 CIFAR-100 VGG16 CIFAR-100 ResNet32

83.20 86.99 91.71 85.47 86.13 92.95 68.42 73.61

84.87 88.85 93.39 86.63 90.65 95.14 73.31 77.73

83.16 88.45 92.89 84.43 89.46 93.63 69.88 73.60

84.12 88.72 93.62 87.54 90.61 94.66 74.22 77.40

85.58 87.97 93.12 86.82 89.78 93.80 71.05 74.49

89.60 89.02 93.90 90.22 91.24 95.35 73.36 78.01

83.85 88.17 93.19 85.24 89.94 93.44 67.46 73.70

89.81 89.77 93.80 90.64 90.91 95.04 73.57 78.02

In the case of gradient descent update, weight decay is equivalent to L2 regularization, which adds an

extra

L2

norm

 2

||||22

to

the

loss

L().

However,

it

is

not

true

for

the

preconditioned

gradient

update

with the curvature matrix C, e.g, natural gradient update. The preconditioned gradient update with

L2 regularization is given by

t+1  (I - C-1)t - C-1L(t)

(8)

whereas natural gradient with weight decay is slightly different

t+1  (1 - )t - C-1L(t)

(9)

From equation 8 and equation 9, we can see that the main difference between L2 regularization and weight decay is whether the preconditioner C is applied to t. Though the difference may

appear subtle, we surprisingly find that it makes a noticeable difference in terms of generalization

performance. We demonstrate the regularization effect of L2 and weight decay by conducting

extensive experiments on image classification.

Initial Experiments. Our experiments were carried out on two different datasets: CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) with varied batch sizes. We test VGG16 (Simonyan & Zisserman, 2014) on CIFAR-10 and ResNet32 (He et al., 2016) on both CIFAR-10 and CIFAR-100. Moreover, we widen all convolutional layers in ResNet32 by a factor of 4 according to Zagoruyko & Komodakis (2016). To gain more insights, we also test ResNet32 on CIFAR-10 without both BN and data augmentation for the small batch setting (with a batch size of 128). In practice, we compute natural gradient using KFAC with different curvature matrix (see Appendix F for pseudo code of KFAC). For simplicity, we fix the  and  in BN throughout all the experiments2.
The results for small batch are summarized in Figure 1. Additionally, We plot the curves of test accuracies after tuning hyperparameters on held-out validation set. The final test accuracies are reported in Table 1. We also reported the final test accuracies of larger batch sizes in Table 3. Based on the initial results, we make the following observations regarding weight decay:
1. In all conducted experiments, weight decay regularization consistently improved the generalization performance and it is equally effective for different optimizers.
2. Interestingly, weight decay closed most of the generalization gaps between first- and secondorder optimizers, as well as between small and large batches.
3. Finally, we notice that weight decay gave an especially strong performance boost to KFAC optimizer when BN is disabled.
In the following section, we seek to explain these phenomena. With further testing, we find that weight decay can work in unexpected ways, especially in the presence of BN.

4 THREE MECHANISMS OF WEIGHT DECAY REGULARIZATION
4.1 MECHANISM I: HIGHER EFFECTIVE LEARNING RATE
Commonly, weight decay­when perceived to be similar to L2 regularization­is thought to improve generalization by regularizing the weights of neural networks to be small in magnitude. However,
2In practice, we found that fix these two parameters does not significantly impact results on image classification tasks.

4

Under review as a conference paper at ICLR 2019

0.8 SGD

ADAM

Acc (ResNet32) Acc (VGG16)

0.6

0.4

0.8

baseline

0.6 wd-conv wd-all

wd-fc

0.4 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200

epoch

epoch

Figure 2: Test accuracy as a function of training epoch for SGD and Adam on CIFAR-100 with ResNet32 model and different weight decay regularization schemes. baseline is the model without weight decay; wd-conv is the model with weight decay applied to all convolutional layers; wd-all is the model with weight decay applied to all layers; wd-fc is the model with weight decay applied to last layer (fc).

since the introduction of BN (Ioffe & Szegedy, 2015), it is unclear why weight decay is still effective due to the fact that networks with BN is invariant to the scaling of weights. van Laarhoven (2017) made an interesting argument that weight decay has no regularization effect when combined with normalization, and instead has an influence on the effective learning rate.

In this section, we confirm this argument and show that in first-order optimization methods with BN networks, the role of weight decay is simply picking a higher learning rate, which happens to improve generalization. For simplicity, we first focus on Stochastic Gradient Descent (SGD).

To be precise, weight decay has no regularization effect on the layers followed by BN, but it is not true for the last layer where we don't use BN in practice. As pointed out by van Laarhoven (2017), weight decay doesn't limit the capacity of neural networks for those layers with BN, since it gives the same function by scaling down the weights

BN(x; l) = BN(x; l)

(10)

where l is the weight of l-th layer. However, the scale of l does influence the updates that are performed in first-order methods:

BN(x; l) = 1 BN(x; l)

 (l)

  (l)

(11)

Due to its invariance to the scaling of the weights, the key feature of the weights becomes its direction. As shown in Hoffer et al. (2018), the weight direction ^l = l/||l||2 is updated according to

^lt+1  ^lt - ||lt||2-2(I - ^lt^lt )l L(^t) + O(2)

(12)

Therefore, the effective learning rate is approximately proportional to /||l||22. Which means that by decreasing the scale of the weights, weight decay regularization is able to increase the effective
learning rate. This is contrary to our intuition that weight decay results in a simple function.

10-3

effective lr

10-4

10-5

10-6 10-7 0

w/o weight decay w weight decay
25 50 75 100 125 150 175 200
epoch

Figure 3: Effective learning rate of the first layer of ResNet32 on CIFAR-100.

We show empirically that weight decay only improves generalization by controlling the norm, and therefore the effective learning rate. Without weight decay, the norm is unbounded, leading to decreasing effective learning rate. Therefore, we conjecture that higher effective learning rate will result in better generalization performance.
In order to verify the above argument, we apply weight decay to different parts of the networks, e.g., the layers with BN, the last layer and all layers and compare them to the baseline in terms of test accuracy as shown in Figure 2.

As expected, if we only apply weight decay to layers with BN, we still observe comparable gain over the baseline as wd-conv and wd-all are nearly identical. On the other hand, if we only apply weight decay to the last layer, the improvement compared to the baseline is marginal and can be ignored, suggesting higher effective learning rate will improve the generalization performance.

5

Under review as a conference paper at ICLR 2019

To further verify the "effective learning rate" argument, we normalize the weights of the layers with BN at the end of each epoch according to norm statistics from a training with weight decay. Ideally, we are supposed to normalize the weights each iteration. Nevertheless, we found that amortizing the normalization to each epoch doesn't degrade the performance much. As shown in Figure 4, normalizing the layers with BN (wn-conv) causes much of the gap diminished and we can close the gap by further adding weight decay to the last layer (wd-fc+wn-conv).

baseline

wn-conv

wd-all

wd-fc+wn-conv

0.8

0.7

accuracy

0.6 0.77

0.76
0.5 0.75

0.4

0.74 160 170 180 190

0.3 0 25 50 75 100 125 150 175 200
epoch
Figure 4: The curves of testing accuracies of ResNet32 on CIFAR-100. To be noted, we use wd and wn to denote weight decay and weight normalization respectively.

4.2 MECHANISM II: APPROXIMATE JACOBIAN REGULARIZATION

Since we observed in Section 3 that weight decay regularization is more effective in KFAC when BN is disabled, we now discuss the effect of weight decay in this case by focusing on plain neural networks without BN.

While weight decay is well-understood in SGD to be equivalent to L2 regularizer, its effects in natural gradient have not been explored. A natural question would be what's the underlying objective we are optimizing with the combination of natural gradient and weight decay? Below, we answer this question formally:

Proposition 1 (For Natural Gradient, Weight Decay  Curvature Norm Regularization). In the case

of natural gradient descent and base learning rate , optimizing the loss function L with weight

decay



is

equivalent

to

optimizing

L()

+

 2

||||2C

without

weight

decay.

With weight decay regularization, it is equivalent to adding curvature norm to the original loss function. We note that this interpretation holds only approximately3, because we don't differentiate
through C, which changes as a function of . When the curvature matrix is taken to be Gauss-Newton matrix, we get Gauss-Newton norm which is defined as ||||2G =  G(). We find that GaussNewton norm on the parameters is equivalent to L2 norm in the output space under some assumptions,
which we summarize in the following theorem.

Theorem 1 (Gauss-Newton Norm & KFAC Gauss-Newton Norm). For a feed-forward neural network of depth L with rectified linear activation and no bias, we observe:

||||G2 = (L + 1)2E [ f(x), f(x) ]

(13)

Furthermore, if we assume the network is linear, we have KFAC Gauss-Newton norm as follows

||||2GKFAC = (L + 1)E [ f(x), f(x) ]

(14)

According to Theorem 2, Gauss-Newton norm can be further rewritten as (L + 1)2E x Jx Jxx , therefore is equivalent to the squared Frobenius norm of input-output Jacobian norm under the assumptions.
Remark 1 (Approximate Jacobian norm). In the case that the distribution of x is spherical Gaussian with unit variance and f is a linear function of x (no nonlinear activation functions), ||||2G = (L + 1)2||Jx||F2rob. Furthermore, if G is approximated by KFAC, ||||G2 KFAC = (L + 1)||Jx||F2rob.
The intuition behind Remark 1 is that when the network is a linear function, the input-output Jacobian is independent of the input x and we can take it as constant. In that case, if the distribution of x is spherical Gaussian, E x Jx Jxx = Trace(Jx Jx) = ||Jx||2Frob. Moreover, the Frobenius norm of input-output Jacobian has been observed to be consistently coupled with generalization performance (Novak et al., 2018).
However, the connection between Gauss-Newton norm and Jacobian norm is not true for general deep non-linear networks. To test whether they are still correlated in practical neural networks, we train feed-forward networks with different optimizers on both MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009). For MNIST, we use simple fully-connected networks with different depth and width. For CIFAR-10, we adopt VGG family (From VGG11 to VGG19). We highlight that the generalization gap is measured by the gap between training loss and testing loss.
3We show in Appendix E that this interpretation holds exactly in the case of Gauss-Newton norm.

6

Under review as a conference paper at ICLR 2019

Gauss-Newton norm

MNIST
107

107

106 106

105 105

104 104

0.05 0.10 0.15 0.20 0.25 0.30
Generalization gap

101

MNIST
102 103 104
Jacobian norm

CIFAR-10
107 106 105 104

107 106 105 104

105 0.5 1.0 1.5 2.0 2.5 3.0
Generalization gap

CIFAR-10

101 102 103
Jacobian norm

104

Figure 5: Correlation between Gauss-Newton norm and Jacobian norm in practical deep neural networks. Each point corresponds to a network trained to 100% training accuracy.

Our main result is presented in Figure 5. The left two figures show that Gauss-Newton norm correlates well with generalization performance and Jacobian norm, justifying its use as regularization term. On right side, we report similar results on CIFAR-10 with VGG networks. We conclude that in the settings without BN, weight decay improves generalization of natural KFAC-G by approximately regularizing the Jacobian norm.

Table 2: Squared Frobenius norm of input-output Jacobian matrix. WD denotes weight decay.

Optimizer
SGD KFAC-G

VGG16 WD
564 142 498 51.44

ResNet32 WD
2765 1074 2115 64.16

We further compare the Jacobian norm of networks trained with and without weight decay after convergence. As shown in Table 2, weight decay, combined with KFAC-G, is able to reduce the Jacobian norm by order of magnitude. By contrast, the reduction in the case of SGD is much smaller.

Analogously to the case of Gauss-Newton matrix, we can obtain Fisher-Rao norm (Liang et al., 2017) by taking the curvature matrix as Fisher matrix, which is defined as |||F2 =  F. It has been shown that Fisher-Rao norm serves as an umbrella for the previously considered norm-based capacity
measures (Neyshabur et al., 2015) and is a good complexity measure of neural networks. Therefore,
weight decay in both KFAC-F and KFAC-G serves as a complexity regularizer, which can be used to
account for the significant improvements on the generalization performance.

4.3 MECHANISM III: SMALLER EFFECTIVE DAMPING PARAMETER

As we show in Section 4.2, weight decay approximately regularizes Jacobian norm of the networks. However, this argument may not hold once BN is applied. Similar to the first mechanism, we expect the effect of weight decay on intermediate layers is completely canceled out by BN, so weight decay should only affect the last layer of the network. Particularly, for first-order methods, the scaling of the weights does change the updates that are performed by adjusting the effective learning rate. However, KFAC update, preconditioned by approximated curvature matrix, is invariant to affine reparameterization (Luk & Grosse, 2018) and therefore not affected by the scaling of the weights (van Laarhoven, 2017). The key observation here is that, when with BN the curvature matrix, e.g., Fisher matrix or Gauss-Newton matrix, has the following property

C(l)

=

C(||l||2^l)

=

1 ||l||22

C(^l

)

which

compensates

the

term

of

 ||l ||22

in

effective

learning

rate

after

inversion.

(15)

In this sense, weight decay regularization has no impact on the layers with BN. Thus, we may expect that the reason weight decay does not significantly improve generalization when BN is applied (see CIFAR-10 results in Table 1) is because the Jacobian regularization is only implicitly applied to the last layer.

This raises the question of whether it is still necessary to reguarlize the layers with BN in KFAC. To test this, we repeat the experiments in Figure 2 but with KFAC instead. The results are summarized in Figure 6. Contrary to our expectations, applying weight decay to layers with BN can still improve the performance significantly in KFAC-F. Overall, KFAC-F performs very similarly to first-order methods while the gain of weight decay in KFAC-G is mostly from regularizing the last layer.

To understand this phenomenon, we notice that previous analysis of natural gradient does not take into account the damping term, which is commonly used in second-order optimization to dampen the updates by adding a multiple of the identity matrix to the curvature before inversion. Analogous to

7

Under review as a conference paper at ICLR 2019

0.8 KFAC-F

KFAC-G

Acc (ResNet32) Acc (VGG16)

0.6

0.4

0.8

baseline 0.6 wd-conv

wd-all

0.4 wd-fc

0 20 40 60 80 100 0 20 40 60 80 100

epoch

epoch

Figure 6: Test accuracy as a function of training epoch for KFAC on CIFAR-100 with ResNet32 model and different weight decay regularization schemes. baseline is the model without weight decay regularization; wd-conv is the model with weight decay applied to all convolutional layers; wd-all is the model with weight decay applied to all layers; wd-fc is the model with weight decay applied to last layer (fc).

equation 12, we can derive the update of weight direction by adding the fixed damping term to the curvature matrix before computing the inverse (see Appendix D for proof)

^lt+1  ^lt - (I - ^lt^lt )(C(^lt) + ||lt||22I)-1l L(^t) + O(2)

(16)

From equation 16, we can see that natural gradient updates are not exactly invariant to the scaling of the 106 weights. Particularly, KFAC-F and KFAC-G only dif- 105

Trace norm

fer in the choice of curvature matrix. We therefore 104

compare the trends of the scale of the Fisher matrix 103

Fisher Matrix Gauss-Newton Matrix

and Gauss-Newton matrix by monitoring the trace 102

norm of the first layer (see Figure 7). Surprisingly, the

scale of Fisher matrix F(^l) decreases as the training 101

progresses while the scale of Gauss-Newton matrix is

0

20

40 60
epoch

80 100

quite stable.

Figure 7: Trace norm of Fisher matrix and GaussIn classification task with cross-entropy loss, the Newton matrix of the first layer (Normalized) Fisher matrix is equivalent to generalized Gauss- of ResNet32. The model is trained on CIFARnewton matrix (Martens, 2014). Therefore, the only 10 with KFAC-F and BN is used. Note that the

difference between Gauss-Newton matrix and the curves are smoothened for visual clarity.

Fisher matrix is the Hessian of loss H (see Section 2),

which is given by diag (p) - pp . It is easy to see that the Hessian of cross-entropy loss would goes

to zero matrix as the prediction p collapses to one class, which is often the case in neural network

training. That explains the different trends of the scale of the Fisher and the Gauss-Newton well.

We notice that the equation 16 becomes exactly equation 12 as the Fisher matrix becomes smaller and smaller and therefore the damping term dominates, which explains why KFAC-F behaves like first-order methods. Although the scale of Gauss-Newton matrix is quite stable, the damping term will increase as the weight norm grows. That's why regularizing convolutional layers does make a small difference. Therefore, it is still necessary to apply weight decay regularization to control "effective damping parameter", which is proportional to ||l||22I.
To summarize, weight decay helps control a small effective damping parameter in KFAC with BN enabled, enabling KFAC to retain its second-order properties, and correspondingly improves generalization.

5 CONCLUSION
Weight decay has been exclusively used in deep learning community, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of L2 regularization. Through extensive systematic experiments, we show weight decay consistently outperforms L2 regularization in cases where they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and KFAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay improves generalization. Our results provide insight into how to improve the regularization of neural networks.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251­276, 1998.
Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using kroneckerfactored approximations. 2016.
Siegfried Bös. Optimal weight decay in a perceptron. In International Conference on Artificial Neural Networks, pp. 551­556. Springer, 1996.
Siegfried Bos and E Chug. Using weight decay to optimize the generalization ability of a perceptron. In Neural Networks, 1996., IEEE International Conference on, volume 1, pp. 241­246. IEEE, 1996.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Tom Heskes. On "natural" learning and pruning in multilayered perceptrons. Neural Computation, 12(4):881­901, 2000.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, pp. 1731­1741, 2017.
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate normalization schemes in deep networks. arXiv preprint arXiv:1803.01814, 2018.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in neural information processing systems, pp. 950­957, 1992.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. arXiv preprint arXiv:1711.01530, 2017.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 2017.
Kevin Luk and Roger Grosse. A coordinate-free construction of scalable natural gradient. arXiv preprint arXiv:1808.10340, 2018.
9

Under review as a conference paper at ICLR 2019

James Martens. New insights and perspectives on the natural gradient method. arXiv preprint arXiv:1412.1193, 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408­2417, 2015.
Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pp. 1376­1401, 2015.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha SohlDickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.

A EXPERIMENTS DETAILS
Throughout the paper, we perform experiments on image classification with three different datasets, MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). For MNIST, we use simple fully-connected networks with different depth and width. For CIFAR-10 and CIFAR100, we use VGG16 (Simonyan & Zisserman, 2014) and ResNet32 (He et al., 2016). To make the network more flexible, we widen all convolutional layers in ResNet32 by a factor of 4, according to Zagoruyko & Komodakis (2016).
We investigate three different optimization methods, including Stochastic Gradient Descent (SGD), Adam (Kingma & Ba, 2014) and KFAC (Martens & Grosse, 2015). In KFAC, two different curvature matrices are studied, including Fisher matrix and Gauss-Newton matrix.
In default, batch size 128 is used unless stated otherwise. In SGD and Adam, we train the networks with a budge of 200 epochs and decay the learning rate by a factor of 10 every 60 epochs for batch sizes of 128 and 640, and every 80 epochs for the batch size of 2K. Whereas we train the networks only with 100 epochs and decay the learning rate every 40 epochs in KFAC. Additionally, the curvature matrix is updated by running average with re-estimation every 10 iterations and the inverse operator is amortized to 100 iterations. For KFAC, we use fixed damping term 1e-3 unless state otherwise. For each algorithm, best hyperparameters (learning rate and regularization factor) were selected using grid search on held-out 5k validation set. For the large batch setting, we adopt the same strategies in Hoffer et al. (2017) for adjusting the search range of hyperparameters. Then, we retrain the model with both training data and validation data.

B GRADIENT STRUCTURE IN NEURAL NETWORKS

Lemma 1 (Gradinet structure). For a feed-forward rectified neural network without bias, one has the

following property:

f(x) = xf(x) x = Jxx

11 = L + 1 f(x)  = L + 1 J

(17)

10

Under review as a conference paper at ICLR 2019

The key observation of Lemma 1 is that rectified neural networks are piecewise linear up to the output f(x). And ReLU activation function satisfies the property (z) =  (z)z.

Proof. For convenience, we introduce some notations here. Let zL+1 denotes output logits f(x), zl the output l-th layer. Similarly, we define al = (zl) and a0 = x. By definition, it is easy to see that

zl+1

=

Wlal

=

zl+1 al

al

=

zl+1 al

al zl

zl

=

zl+1 zl

zl

By induction, we conclude that f(x) = xf(x) x.

On the other side, we have

zl+1 = Wlal =

i,j

zl+1 Wli,j

Wli,j

According

to

equation

B,

zL+1

=

z ,zL+1
zl+1 l+1

therefore

we

get

zL+1 =

i,j

zL+1 Wli,j

Wli,j

Summing over all the layers, we conclude the following equation eventually:

(L + 1)f(x) =
l

i,j

zL+1 Wli,j

Wli,j

=

 f (x)



C PROOF OF THEMREM 2

Theorem 2 (Gauss-Newton Norm & KFAC Gauss-Newton Norm). For a feed-forward neural network of depth L with rectified linear activation and no bias, we observe:

||||2G = (L + 1)2E [ f(x), f(x) ]

(18)

Furthermore, if we assume the network is linear, we have KFAC Gauss-Newton norm as follows

||||2GKFAC = (L + 1)E [ f(x), f(x) ]

(19)

Proof. We first prove the equaility ||||G2 = (L + 1)2E [ f(x), f(x) ]. Using the definition of the Gauss-Newton norm, we have
||||2G = E  J J
= E [ J, J ]
By Lemma 1, J = (L + 1)f(x) = (L + 1)Jxx
Combining above equalities, we arrive at the conclusion.

For second part ||||G2 KFAC = (L + 1)E [ f(x), f(x) ], we note that kronecker-product is exact under the condition that the network is linear, which means GKFAC is the diagonal block version of Gauss-Newton matrix G. Therefore

||||G2 KFAC =

E l Jl Jl l

l

According to Lemma 1, we have E l Jl Jl l = E [ f(x), f(x) ], therefore we conclude that ||||G2 KFAC = (L + 1)E [ f(x), f(x) ]

11

Under review as a conference paper at ICLR 2019

D DERIVATION OF EQUATION 16

Claim. During training, the weight direction ^lt = lt/||lt||2 is updated according to ^t+1  ^t - (I - ^t^t )(C(^t) + ||t||22I)-1L(^t) + O(2)

Proof. Natural gradient update is given by t+1  t - (C(t) + I)-1L(t)
Denote t = ||t||2. Then we have 2t+1 = t2 - 2t2^t (C(^t) + t2I)-1L(^t) + 22t ||(C(^t) + t2I)-1L(^t)||22
and therefore

t+1 = t 1 - 2^t (C(^t) + t2I)-1L(^t) + 2||(C(^t) + t2I)-1L(^t)||22 = t(1 - ^t (C(^t) + t2I)-1L(^t)) + O(2)

Additionally, we can rewrite the natural gradient update as follows

t+1^t+1 = t^t - t(C(^t) + t2I)-1L(^t)

And therefore,

^t+1

=

t t+1

^t - (C(^t) + t2I)-1L(^t)

= 1 + ^t (C(^t) + t2I)-1L(^t) ^t - (C(^t) + t2I)-1L(^t) + O(2)

= ^t - (I - ^t^t )(C(^t) + ||t||22I)-1L(^t) + O(2)

E THE GRADIENT OF GAUSS-NEWTON NORM

For Gauss-Newton norm ||||G2 = (L + 1)2Ex [ f(x), f(x) ], its gradient has the following form:

||||2G 

= 2(L + 1)2E

J f(x)

(20)

According

to

Lemma

1,

we

have

f (x)

=

1 L+1

J

,

therefore

we

can

rewrite

equation

20

||||2G 

= 2(L + 1)E

J J

= 2(L + 1)G

(21)

Surprisingly, the resulting gradient has the same form as the case where we take Gauss-Newton matrix as a constant of  up to a constant (L + 1).

F PSEUDO CODE OF KFAC WITH WEIGHT DECAY REGULARIZATION

G ADDITIONAL RESULTS
G.1 LARGE-BATCH TRAINING
It has been shown that KFAC scales very favorably to larger mini-batches compared to SGD, enjoying a nearly linear relationship between mini-batch size and per-iteration progress for medium-to-large sized mini-batches (Martens & Grosse, 2015; Ba et al., 2016). However, Keskar et al. (2016) showed that large-batch methods converge to sharp minima and generalize worse. In this subsection, we
12

Under review as a conference paper at ICLR 2019

Algorithm 1 K-FAC with L2 regularization and K-FAC with weight decay. Subscript l denotes layers, wl = vec(Wl). We assume zero momentum for simplicity.
Require: : stepsize Require: : weight decay Require: stats and inverse update intervals Tstats and Tinv
k  0 and initialize {Wl}lL=1, {Sl}Ll=1, {Al}lL=1 while stopping criterion not met do
k  k+1 if k  0 (mod Tstats) then
Update the factors {Sl}Ll=1, {Al}lL=-01 with moving average end if if k  0 (mod Tinv) then
Calculate the inverses {[Sl]-1}Ll=1, {[Al]-1}lL=-01 end if Vl = Wl log p(y|x, w)+ · Wl Wl  Wl - [Al]-1Vl[Sl]-1+ · Wl end while

measure the generalization performance of KFAC with large batch training and analyze the effect of weight decay.
In Table 3, we compare KFAC with SGD using different batch sizes. In particular, we interpolate between small-batch (BS128) and large-batch (BS2000). We can see that in accordance with previous works (Keskar et al., 2016; Hoffer et al., 2017) the move from a small-batch to a large-batch indeed incurs a substantial generalization gap. However, adding weight decay regularization to KFAC almost close the gap on CIFAR-10 and cause much of the gap diminish on CIFAR-100. Surprisingly, the generalization gap of SGD also disappears with well-tuned weight decay regularization. Besides, we observe that the training loss cannot decrease to zero if weight decay is not used, indicating weight decay may also speed up the training.

Table 3: Classification results with different batch sizes. WD denotes weight decay regularization. We tune weight decay factor and learning rate using held-out validation set.

Dataset CIFAR10 CIFAR10 CIFAR100

Network VGG16 ResNet32 ResNet32

Method
SGD KFAC-F KFAC-G
SGD KFAC-F KFAC-G
SGD KFAC-F KFAC-G

BS128 WD
91.71 93.39 93.12 93.90 93.41 93.67
92.95 95.14 93.80 95.35 93.44 95.04
74.03 77.73 75.42 78.01 73.70 78.02

BS640 WD
90.46 93.09 92.93 93.55 92.98 93.74
91.68 94.45 92.30 94.79 91.80 94.73
71.74 76.67 73.54 77.34 71.13 77.40

BS2000 WD
88.50 92.24 92.17 93.31 90.78 93.46
89.70 94.68 91.15 94.43 90.02 94.85
65.38 76.87 71.64 77.13 65.41 76.93

G.2 THE CURVES OF TEST ACCURACIES
G.3 OPTIMIZATION PERFORMANCE OF DIFFERENT OPTIMIZERS
While this paper mostly focus on generalization, we also report the convergence speed of different optimizers in deep neural networks; we report both per-epoch performance and wall-clock time performance. We consider the task of image classification on CIFAR-10 (Krizhevsky & Hinton, 2009) dataset. The models we use consist of VGG16 (Simonyan & Zisserman, 2014) and ResNet32 (He et al., 2016). We compare our KFAC-G, KFAC-F with SGD, Adam (Kingma & Ba, 2014).
13

Under review as a conference paper at ICLR 2019

CIFAR10-ResNet32

accuracy

0.90 0.85 0.80 0.75

SGD

Adam

KFAC-F

KFAC-G

CIFAR10-ResNet32+

accuracy

CIFAR100-VGG16+

accuracy

0.9

0.8

0.7 0.8

0.7

0.6

0.5

0.4 0.8

0.7

0.6

0.5

0.4 0

25 50 75 100 125 150 175 200 0
epoch

25 50 75 100 125 150 175 200 0
epoch

20 40 60 80 100 0
epoch

baseline L2 wd
20 40 60 80 100
epoch

CIFAR100-ResNet32+

accuracy

Figure 8: Test accuracy as a function of training epoch. We plot baseline vs L2 regularization vs weight decay regularization on CIFAR-10 and CIFAR-100 datasets. The '+' denotes with BN and data augmentation. Note that training accuracies of all the models are 100% in the end of the training. We smooth all the curves for visual clarity.

We experiment with constant learning for KFAC-G and KFAC-F. For SGD and Adam, we set batch size as 128. For KFAC, we use batch size of 640, as suggested by Martens & Grosse (2015).
In Figure 9, we report the training curves of different algorithms. Figure 9a show that KFAC-G yields better optimization than other baselines in training loss per epoch. We highlight that the training loss decreases to 1e-4 within 10 epochs with KFAC-G. Although KFAC based algorithms take more time for each epoch, Figure 9b still shows wall-clock time improvements over the baselines.
In Figure 9c and 9d, we report similar results on the ResNet32. Note that we make the network wider with a widening factor of 4 according to Zagoruyko & Komodakis (2016). KFAC-G outperforms both KFAC-F and other baselines in term of optimization per epoch, and compute time.

14

Under review as a conference paper at ICLR 2019

training loss

training loss

100 10-1 10-2 10-3 10-4 10-5 10-6
0

20 40 60 80 epoch

(a) Training loss (VGG16)

100 10-1 10-2 10-3 10-4 10-5 10-6 10-7
0

20 40 60 80 epoch

(c) Training loss (ResNet32)

100 100

training loss

training loss

100 10-1 10-2 10-3 10-4 10-5 10-6
0

SGD SGD/BN Adam/BN KFAC-G KFAC-F
500 1000 1500 2000 2500 3000 3500 compute time (in second)

(b) Wall-Clock Time (VGG16)

100 10-1 10-2 10-3 10-4 10-5 10-6 10-7
0

1000 2000 3000 4000 5000 6000 7000 compute time (in second)

(d) Wall-Clock Time (ResNet32)

Figure 9: CIFAR-10 image classification task.

15

