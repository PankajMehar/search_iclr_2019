Under review as a conference paper at ICLR 2019
EFFICIENT EXPLORATION THROUGH BAYESIAN DEEP Q-NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We propose Bayesian Deep Q-Networks (BDQN), a principled and a practical Deep Reinforcement Learning (DRL) algorithm for Markov decision processes (MDP). It combines Thompson sampling with deep-Q networks (DQN). Thompson sampling ensures more efficient exploration-exploitation tradeoff in high dimensions. It is typically carried out through posterior sampling over the model parameters, which makes it computationally expensive. To overcome this limitation, we directly incorporate uncertainty over the value (Q) function. Further, we only introduce randomness in the last layer (i.e. the output layer) of the DQN and use independent Gaussian priors on the weights. This allows us to efficiently carry out Thompson sampling through Gaussian sampling and Bayesian Linear Regression (BLR), which has fast closed-form updates. The rest of the layers of the Q network are trained through back propagation, as in a standard DQN. We apply our method to a wide range of Atari games in Arcade Learning Environments and compare BDQN to a powerful baseline: the double deep Q-network (DDQN). Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster: in less than 5M±1M samples for almost half of the games to reach DDQN scores while a typical run of DDQN is 50-200M. We also establish theoretical guarantees for the special case when the feature representation is fixed and not learnt. We show that the Bayesian regret is bounded by O(d N ), after N time steps for a d-dimensional feature map, and this bound is shown to be tight up-to logarithmic factors. To the best of our knowledge, this is the first Bayesian theoretical guarantee for Markov Decision Processes (MDP) beyond the tabula rasa setting.
1 INTRODUCTION
A central challenge in reinforcement learning (RL) is to design efficient exploration-exploitation tradeoff that also scales to high-dimensional state and action spaces. Recently deep RL has shown good promise in being able scale to high-dimensional (continuous) spaces. These successes are mainly demonstrated in simulated domains where exploration is considered to be inexpensive and simple exploration strategies are deployed, e.g. -greedy which uniformly explores over all the non-greedy strategies with 1 -  probability. Such exploration strategies inherently inefficient for complex high-dimensional environments. On the other hand, more sophisticated strategies have mostly been limited to low dimensional MDPs. For example, optimism-under-uncertainty (OFU) is only practical when the domain is small enough to be represented with lookup tables for the Q-values (Jaksch et al., 2010; Brafman & Tennenholtz, 2003).
An alternative to optimism-under-uncertainty is Thompson Sampling (TS), a general sampling and randomizathttps://www.overleaf.com/1332425641pdxdghynmdhyion approach (in both frequentist and Bayesian settings) (Thompson, 1933). Under the Bayesian framework, Thompson sampling maintains a prior distribution over the environmental models (i.e. model of reward and dynamics), and updating the posterior based on observations collected. An action is chosen from the posterior distribution of the belief such that it maximizes the expected return. Thompson sampling has been observed to work significantly better than optimistic approaches in many low dimensional settings such as contextual bandits Chapelle & Li (2011), small MDPs Osband et al. (2013) and also has strong theoretical bounds. (Russo & Van Roy, 2014; Agrawal & Goyal, 2012; Osband et al., 2013; Abbasi-Yadkori & Szepesva´ri, 2015).
1

Under review as a conference paper at ICLR 2019
In the MDP setting, (model-based) Thompson Sampling involves sampling the parameters of reward and dynamics model, performing MDP planning using the sampled model and then computing the policy Strens (2000); Osband et al. (2013). However, the computational costs of posterior sampling and planning scales as cubic in the problem dimension, which makes it intractable for large MDPs. Therefore, some form of function approximation is required to scale Thompson Sampling to high dimensions, and this can be either of the model, the Q-value function, or the policy. To address this, Osband et al. (2014) introduced randomized least-squares value iteration (RLSVI) which combines linear value function approximation with Bayesian regression to directly sample the value-function weights from a distribution. The authors prove a regret bound for this approach in tabular MDPs. This has been extended to continuous spaces by Osband et al. (2016), where deep networks are used to approximate the Q function. Through a bootstrapped-ensemble approach, several deep-Q network (DQN) models are trained in parallel to approximate the posterior distribution. Other works use the posterior distribution over the parameters of each node in the network and employ variational approximation (Lipton et al., 2016b) or use noisy networks (Fortunato et al., 2017). These approaches significantly increase the computation cost compared to the standard DQN. For instance, the. bootstrapped-ensemble incurs a computation overhead that is linear in the number of bootstrap models. Moreover, despite principled design of the methods in the above works, they only produced modest gains over DQN in empirical studies.
Contribution 1 ­ Design of BDQN: We introduce Bayesian Deep Q-Network (BDQN), a Thompsonsampling algorithm for deep RL. It is a simple approach that extends randomized least-squares value iteration (Osband et al., 2014) to deep networks. We introduce stocasticity only in the last layer of the Q-network using independent Gaussian priors on the weights. This allows us to do efficient (approximate1) Thompson sampling using Bayesian linear regression (BLR), which has fast closedform updates, and sampling from the resulting posterior distribution. The rest of the Q-network is trained through usual back propagation.
Contribution 2 ­ Strong empirical results for BDQN: We test BDQN on a wide range of Atari games (Bellemare et al., 2013; Machado et al., 2017), and compare our results to a powerful baseline: Double DQN (DDQN) (Van Hasselt et al., 2016) 3 a bias-reduced extension of DQN. BDQN and DDQN use the same network architecture, and follow the same target objective, and differ only in the way they select actions: DDQN uses -greedy exploration while BDQN performs (approximate) Thompson sampling.
We find that BDQN is able to reach much higher cumulative rewards, and also at a higher speed, compared to DDQN on all the tested games. We also found that BDQN can be trained with much higher learning rates compared to DDQN. This is intuitive since BDQN has better exploration strategy. We found that the cumulative reward (score) for BDQN at the end of training improves by a median of 300% with a maximum of 80k% in these games. Also, BDQN has 300% ± 40% (i.e. mean and standard deviation) improvement over these games w.r.t. area under the performance measure. This can be considered as a surrogate for sample complexity and regret. Indeed, no single measure of performance provides a complete picture of an algorithm, and we present detailed experiments in Section 4
In terms of computational cost, BDQN is only slightly more expensive compared to DQN and DDQN. This is the cost of inverting a 512 × 512 matrix every 100,000 time steps, which is negligible. On the other hand, more sophisticated Bayesian RL techniques are significantly more expensive and have not lead to large gains over DQN and DDQN.
Contribution 3 ­ Tight Bayesian regret bounds for continuous MDPs: We establish theoretical guarantees for the special case when the feature representation is fixed (i.e. all layers except the last), and not learnt. In other words, we consider episodic MDPs with continuous space of states and actions such that the Q-function is a linear function of a given d-dimensional feature map. We show that the Bayesian regret has an upper bound of O(d N ) after N time steps, which is shown to be tight up-to logarithmic factors. Since linear bandits are a special case of episodic continuous MDPs (with horizon length 1), it implies that our Bayesian regret bound is tight in the dimension d and in the number of episodes (up to logarithmic factors). This bound matches the Bayesian
1Since BDQN approximate the posterior, sampling from the induced confidence interval is an approximation to Thompson sample. In the remaining, we state Thompson Sampling and its approximation as Thomson samples unless we specify.
2

Under review as a conference paper at ICLR 2019

regret bound of linear bandits (Russo & Van Roy, 2014) as well as the frequentist regret bound for linear bandits (Abbasi-Yadkori et al., 2011) and for adaptive control of linear quadratic (LQ) framework (Abbasi-Yadkori & Szepesva´ri, 2011). To the best of our knowledge, this is the first Bayesian theoretical guarantee for continuous MDPs beyond the tabular setting.
Thus, our proposed approach has several desirable features ­ faster learning and better sample complexity due to targeted exploration, negligible computational overhead due to simplicity, significant improvement in experiments, and tight theoretical bounds.

2 THOMPSON SAMPLING VS -GREEDY AND BOLTZMANN EXPLORATION

In a value approximation RL algorithm, there are different ways to manage the explorationexploitation trade-off. DQN uses a naive -greedy for exploration, where with  probability it chooses a random action and with 1 -  probability it chooses the greedy action based on the estimated Q function. Note that there are only point estimates of the Q function in DQN. In contrast, our proposed Bayesian approach BDQN maintains uncertainties over the estimated Q function, and employs it to carry out Thompson Sampling based exploration. Here, we demonstrate the fundamental benefits of Thompson Sampling over -greedy and Boltzmann exploration strategies using simple examples. In Table. 1, we list the three strategies and their properties.

-greedy is the simplest exploration strategy since it is uniformly random over all the non-greedy actions. Boltzmann exploration is an intermediate strategy since it uses the estimated Q function to sample the actions. However, it does not maintain uncertainties over the Q function estimation. In contrast, Thompson sampling also incorporates uncertainties over Q estimation and utilizes most information for exploration strategy.

Consider the example in Figure 1(a) with current estimates and uncertainties of the Q function over different actions. -greedy is wasteful since it assigns uniform probability to explore over 5 and 6, which are obviously sub-optimal when the uncertainty estimates are available. In this setting, a possible remedy is Boltzmann exploration since it assigns lower probability to actions 5 and 6 but randomizes with almost the same probabilities over the remaining actions.

Table 1: Characteristics of Thompson Sampling, -greedy, and Boltzmann exploration what information they use for exploration

Strategy

Greedy-Action Estimated Q-values Estimated uncertainties

-greedy





Boltzmann exploration







Thompson Sampling







However, Boltzmann exploration is sub-optimal in settings where there is high variances. For example if the current Q estimate is according to Figure 1(b), then Boltzmann exploration assigns almost equal probability to actions 5 and 6, even though action 6 has much higher uncertainty and needs to be explored more. Thus, both -greedy and Boltzmann exploration strategies are sub-optimal since they do not maintain an uncertainty estimate over the Q function estimation. In contrast, Thompson sampling uses both estimated Q function and its uncertainty estimates to carry out the most efficient exploration.

3 BAYESIAN DEEP Q-NETWORKS

Consider an MDP M as a tuple X , A, P, P0, R,  , with state space X , action space A, the transition kernel P , accompanied with reward function of R, and discount factor 0   < 1. In value based
model free RL, the core of most prominent approaches to tackle the learning of the Q function follows
minimizing the Bellman residual (Lagoudakis & Parr, 2003; Antos et al., 2008). of the Q-function

L(Q) = E (Q(x, a) - r - Q(x , a ))2

(1)

Where the tuple (x, a, r, x ) consists of consecutive samples under behavioral policy . Mnih et al. (2015) carries the same idea, and propose DQN where the Q-function is parameterized by a deep

3

Under review as a conference paper at ICLR 2019

Estimated Q and Uncertainties Estimated Q and Uncertainties

Greedy action

No need to try them often!

Greedy action

try it!

Estimated Q Target Q Uncertainty
Actions

Estimated Q Target Q Uncertainty
(a) Actions

(b)

Figure 1: Thompson Sampling vs -greedy and Boltzmann exploration. (a) -greedy is wasteful since it assigns uniform probability to explore over 5 and 6, which are obviously sub-optimal when the uncertainty estimates are available. Boltzmann exploration randomizes over actions even if the optimal action is identifies. (b) Boltzmann exploration does not incorporate uncertainties over the estimated action-values and chooses actions 5 and 6 with similar probabilities while action 6 is significantly more uncertain. Thomson Sampling is a simple remedy to all these issues.

Algorithm 1 BDQN

1: Initialize parameter sets , target, W , W target, Cov, counter = 0

2: for episode = 1 to inf do

3: for t = to the end of episode do 4: if count mod T sample = 0 then sample W  N (W target, Cov) end if

5: Execute at = arg maxa W (xt) a , observe reward rt, successor State xt+1

6: Store transition (xt, at, rt, xt+1) in the replay buffer

7: Sample a random minibatch of transitions (x , a , r , x+1) from replay buffer

r for terminal x+1

8: y  r + W target target (x +1)

for non-terminal x+1

arg maxa [W  (x+1)]a

9:    -  · (y - W (x ) a )2 10: if count mod T target = 0 then set target   end if

11: if count mod T Bayes target = 0 then Update W target and Cov end if

12: count = count + 1

network. To improve the quality of Q estimate, they use back propagation on loss L(Q) using the
TD update (Sutton & Barto, 1998). In order to reduce the bias of the estimator, they utilize a target network Qtarget and target value y = r + Qtarget(x , a^) where a^ = arg maxa Q(x , a ) with a new loss L(Q, Qtarget).

L(Q, Qtarget) = E (Q(x, a) - y)2

(2)

This regression problem is pursued to minimize the estimated loss L(Q, Qtarget), which minimize the error between the Q and the target y. A DQN agent, once in a while updates the Qtarget network by setting it to Q network, follow the regression with the new target value and provides a biased estimator of the target. To mitigate the bias in this estimator, DDQN proposes to instead a^ = arg maxa Qtarget(x , a ). We deploy this approach for the remaining of this paper. In this work, instead of regression, we minimize L(Q, Qtarget) via Bayesian Linear regression, which is also inspired by our theoretical work. While deploying BLR, we also train the feature representation in order to approximately develop a linear features for Q
We utilize the DQN architecture, remove its last layer, and directly build a BLR (Rasmussen & Williams, 2006) on the output of the deep network parametrized by , (x)  Rd, known as feature representation layer. We use BLR to efficiently approximate the distribution over the Q-values where the uncertainty over the values is captured. A common assumption in DNN is that the feature representation is suitable for linear classification or regression (same assumption in DDQN), therefore,

4

Under review as a conference paper at ICLR 2019

building a linear model on the features is a suitable choice. The Q-functions are approximated as a linear transformation of the deep neural network features, i.e. for a given pair of state-action, Q(x, a) = (x) wa, where wa  Rd, a  A, x  X .
Consequently, as mentioned in the previous section, the target value is generated using a target model. The target model follows the same structure as the Q model, and contains target (·)  Rd denotes the feature representation of target network, and wtargeta^, a^  A denotes the target weight. For a given tuple of experience (x, a, r, x )
Q(x, a) = (x) wa  y := r + target (x )wtargeta^, where a^ = arg maxatarget watarget.

Therefore, by deploying BLR on the features, we approximate
the posterior distribution of model parameter for each wa, as well as the posterior distribution of the Q-function using the
corresponding target values. As in BLR methods, for each weight vector we maintain a Gaussian prior N (0, 2I) while the target value y  wa (x) + and  N (0, 2) is an i.i.d. noise.

wa* wa

w¯ a

Given a experience replay buffer D = {x , a , y }D=1, we construct |A| (number of actions) disjoint datasets for each action, Da, a set of experiences under action a. We are interested in approximating the posterior distribution of each wa, correspondingly the Q(x, a). For each action a, we construct

Figure 2: BDQN deploys Thompson Sampling to, sample wa a  A around the empirical mean wa with wa the underlying parameter

a matrix a  Rd×|Da|, a concatenation of feature column of interest.

vectors {(xi)}|iD=a1|, and ya  R|Da|, a concatenation of target values in set Da. Therefore the

posterior distribution of wa Fig. 2 is approximated as follows:

wa  N

wa

:=

1 2

aa

ya

,

a

, a =

1 2

a

a

1 + 2 I

-1

(3)

A sample of Q(x, a) is wa (x) where wa is a draw as in Eq. 3 and Fig. 2. Every T sample times step, we draw wa, a  A follow aTS and train the feature network under loss (y - wa (x ))2 with x , a , y samples from replay buffer i.e.

aTS = arg maxawa (x),

   -  · (y - W (x ) a )2

(4)

We update the target network every T target steps and set target to . With the period of T Bayes target,
we update the approximated posterior distribution using a larger minibatch of data, drawn from replay buffer, and set the watarget, a  A to the mean of the posterior A.6. Algorithm 1 gives the full description of BDQN.

4 EXPERIMENTS
We apply BDQN on a variety of Atari games using the Arcade Learning Environment (Bellemare et al., 2013) through OpenAI Gym2 (Brockman et al., 2016). As a baseline, we run the DDQN algorithm and evaluate BDQN on the measures of sample complexity and score. All the implementations are coded in MXNet framework (Chen et al., 2015). The details on architecture, Apx. A.1, learning rate Apx. A.4, computation A.5. In Apx. A.2 and A.3 we describe how we spend less than two days on a single game for the hyper parameter choices which is another evidence of significance of BDQN.
Baselines: We implemented DDQN and BDQN exactly the same way as described in Van Hasselt et al. (2016). We also aimed to implement a couple of other deep RL methods that employ strategic exploration. Unfortunately we encountered several implementation challenges where neither code nor the implementation details was publicly available. Despite the motivation of this work on sample
2Each input frame is a pixel-max of the two consecutive frames. We detailed the environment setting in the implementation code

5

Under review as a conference paper at ICLR 2019

Table 2: Comparison of scores and sample complexities (scores in the first two columns are average
of 100 consecutive episodes). Sample complexity, SC: the number of samples the BDQN requires to beat the human score (Mnih et al., 2015)(" - " means BDQN could not beat human score). SC+: the number of samples the BDQN requires to beat the score of DDQN+. The scores of DDQN+ as well
as bootstrap DQN (Osband et al., 2016), CTS, Pixel, Reactor (Ostrovski et al., 2017) are borrowed
from the original papers. For NoisyNet (Fortunato et al., 2017), the scores of NoisyDQN are reported.

Game

BDQN DDQN DDQN+3 Bootstrap4 NoisyNet5 CTS Pixel Reactor Human SC SC+ Step

Amidar Alien Assault Asteroids Asterix BeamRider BattleZone Atlantis DemonAttack Centipede BankHeist CrazyClimber ChopCmd6 Enduro Pong

5.52k 3k
8.84k 14.1k 58.4k 8.7k 65.2k 3.24M 11.1k 7.3k 0.72k 124k
72.5k 1.12k
21

0.99k 2.9k 2.23k 0.56k 11k 4.2k 23.2k 39.7k 3.8k 6.4k 0.34k 84k
0.5k 0.38k 18.82

0.7k 2.9k 5.02k 0.93k 15.15k 7.6k 24.7k 64.76k 9.7k 4.1k 0.72k 102k
4.6k 0.32k
21

1.27k 2.44k 8.05k 1.03k 19.7k 23.4k 36.7k 99.4k 82.6k 4.55k 1.21k 138k
4.1k 1.59k 20.9

1.5k 2.9k 3.1k 2.1k 11.0 14.7k 11.9k 7.9k 26.7k 3.35k 0.64k 121k
5.3k 0.91k
21

1.03k 1.9k 2.88k 3.95k 9.55k 7.0k 7.97k 1.8M 39.3k 5.4k 1.3k 112.9k
5.1k 0.69k 20.8

0.62k 1.7k 1.25k 0.9k 1.4k 3k 10k 40k 1.3k 1.8k 0.42k 75k
2.5k 0.19k
17

1.18k 3.5k 3.5k 1.75k 6.2k 3.8k 45k 9.5M 7k 3.5k 1.1k 119k
4.8k 2.49k
20

1.7k 6.9k 1.5k 13.1k 8.5k 5.8k 38k 29k 3.4k 12k 0.72k 35.4k
9.9k 0.31k
9.3

22.9M -
1.6M 58.2M 3.6M 4.0M 25.1M 3.3M 2.0M
2.1M 0.12M
4.4M 0.82M 1.2M

4.4M 36.27M 24.3M 9.7M 5.7M 8.1M 14.9M
5.1M 19.9M 4.2M 10.1M 2.1M
2.2M 0.8M 2.4M

100M 100M 100M 100M 100M 70M 50M 40M 40M 40M 40M 40M
40M 30M 5M

complexity, since we do not have access to the performance plots of these methods, the least is to report their final scores. To try to illustrate the performance of our approach we instead, extracted the best reported scores from a number of state-of-the-art deep RL methods and include them in Table 2, which is the only way to bring a comparison. These scores are reported score of the trained model on 200M steps, mainly in the evaluation phase where exploration is almost zero. Note that this is not a perfect comparison, as there are additional details that are not included in the papers, i.e. it is hard to just compare the reported results (an issue that has been discussed extensively recently, e.g. (Henderson et al., 2017)).7. Moreover, when the regret analysis of an algorithm is considered, no evaluation phase required, and the reported results of BDQN are those while exploration. It is worth noting that, the scores during evaluation are much higher than those during the course of learning and exploration. Apx. A.8
Results: The results are provided in Fig. 3. We observe that BDQN significantly improve the sample complexity of DDQN and reaches the highest score of DDQN in much fewer number of samples than DDQN needs. We expected BDQN, due to it better exploration-exploitation strategy, to improve the regret and enhance the sample complexity, but we also observed a significantly improvement in scores. It worth noting that since BDQN is design to minimize the regret, and the study in Fig. 3 also for sample complexity analysis, either of the reported BDQN DDQN scores are while exploration. For example, DDQN gives score of 18.82 during the learning phase, but setting  to zero, it mostly gives the score of 21. We report the number of samples (sample complexity (SC)) it take from BDQN to reach human scores, SC, and DDQN+ scores, SC+ in Table. 2 and Table. 3.
For the game Atlantis, DDQN+ gives score of 64.67k during the evaluation phase, while BDQN reaches score of 3.24M after 20M samples. As it is been shown in Fig. 3, BDQN saturates for Atlantis after 20M samples. We realized that BDQN reaches the internal OpenAIGym limit of max episode, where relaxing it improves score after 15M steps to 62M , Apx. A.7. We observe that BDQN immediately learns significantly better policies due to its targeted exploration in a much shorter period of time. Since BDQN on game Atlantis promise a big jump around time step 20M , we ran it five more times in order to make sure it was not just a coincidence Apx. A.7 Fig. 6. For the game Pong, we ran the experiment for a longer period but just plotted the beginning of it in order to observe the difference. Due to cost of deep RL methods, for some games, we run the experiment until a plateau is reached.
7To further reproducibility, we released our codes and trained models. Since DRL experiments are expensive, we also have released the recorded arrays of returns. We also implemented bootstrapped DQNOsband et al. (2016) and released the code but we were not able to reproduce their results beyond the performance of random policy
6

Under review as a conference paper at ICLR 2019

Figure 3: The efficient and targeted exploration of BDQN
5 BAYESIAN REGRET BOUND

In this section we provide the analysis of Bayesian regret bound when the feature representation is
given and fixed. Consider a finite horizon stochastic MDP M as a tuple X , A, P, P0, R, H , with state space X , action space A, the transition kernel P , accompanied with reward function of R, and
H as the length of episodes. The agent objective is to optimize the overall expected cumulative reward. For the feature representation (·, ·) := X × A  Rd, we say Q-function is a linear transformation of this feature representation if there exists a problem dependent vector   Rd such that Q (x, a) := (x, a) , x, a  X × A. Here  : X  A is the optimal policy of MDP and also satisfies (x) := arg maxaA Q (x, a). Respectively, let V denote the value of the  under the model with . The following is the generative model of the environment;

R := R = (x{1:H}, a{1:H}) + (x{1:H}, a{1:H})

(5)

Where  is upper triangular matrix all ones 6. The vector R  RH is the random vector of rewards in an episode while R  RH , the corresponding per step return. For a sequence of states and actions, x{1:H}, a{1:H}, the matrix (x{1:H}, a{1:H})  RH×d is row-wise concatenation of the corresponding features. Our algorithm maintains a prior over the vector  while updating the
posterior by observing data. At the beginning of an episode t, a sample of t is drawn from the posterior and the following decisions in the same episode are optimal with respected to t, i.e. t and a := arg maxaA  (x, a)t, x  X . Similar to the linear bandit problems, consider the following generic assumptions,

· The noise vector  is sub-Gaussian vector with parameter . Look at Asm. 1
· The random vectors h and variable h h  [1, . . . , H] satisfies the Markov property.
·  2  L and trace (·)(·)  L, a.s.
Theorem 1 (Bayesian Regret). For an episodic MDP with episode length H, feature mapping (x, a)  Rd, the PSRL on , after T episodes, guarantees;

T

BayesRegT : =

E V - Vt

t

with probability at least 1 - . Proof: in Theorem. B

O d

HT log(T /)

Proof Sketch: In order to prove this regret bound, we show that over the course of exploration and exploitation, the estimated parameter vector t concentrates to to the true  with a fast rate of O (1/t) Lemma. 3. Since the sampled parameter t is also close to t, overall per step regret shrinks as rate of O (1/t). Moreover, each episode has length H, the regret after T episodes should not grow

7

Under review as a conference paper at ICLR 2019
faster than O (T H), i.e. O (N ) where is the total number of interactions with the environment. This theorem directly extendable to 0    1. This bounded shows the complexity of this problem grows not faster than linear in the dimension, and HT . This bound is similar to those in linear bandits (Abbasi-Yadkori et al., 2011; Russo & Van Roy, 2014) and linear quadratic control (Abbasi-Yadkori & Szepesva´ri, 2011), i.e. O(d T ). On lower bound;since for H = 1, this problem reduces to linear bandit and for linear bandit the lower bound is (d T ) therefore, our bound is order optimal in d and T .
6 RELATED WORK
The complexity of the exploration-exploitation trade-off has been deeply investigated in RL literature for both continuous and discrete MDPs (Kearns & Singh, 2002; Brafman & Tennenholtz, 2003; Asmuth et al., 2009; Kakade et al., 2003; Ortner & Ryabko, 2012). Jaksch et al. (2010) investigates the regret analysis of MDPs with finite state and action where Optimism in Face of Uncertainty (OFU) principle is deployed to guarantee a regret upper bound, while Ortner & Ryabko (2012) relaxes it to continues state space and propose sublinear regret bound. Azizzadenesheli et al. (2016a) deploys OFU and propose a regret upper bound for Partially Observable MDPs (POMDPs) using spectral methods (Anandkumar et al., 2014). Furthermore, Barto´k et al. (2014) tackles a general case of partial monitoring games and provides minimax regret guarantee. For linear quadratic models, (Abbasi-Yadkori & Szepesva´ri, 2011), OFU is deployed to provide optimal regret bound.
In multi-arm bandit, there are compelling empirical pieces of evidence that Thompson Sampling sometimes provides better results than optimism-under-uncertainty approaches (Chapelle & Li, 2011), while also the state of the art performance bounds are preserved (Russo & Van Roy, 2014; Agrawal & Goyal, 2012). A natural adaptation of this algorithm to RL, posterior sampling RL (PSRL) Strens (2000) also shown to have good frequentist and Bayesian performance guarantees (Osband et al., 2013; Abbasi-Yadkori & Szepesva´ri, 2015).
Even though the theoretical RL addresses the exploration and exploitation trade-offs, these problems are still prominent in empirical reinforcement learning research (Mnih et al., 2015; Abel et al., 2016; Azizzadenesheli et al., 2016b). On the empirical side, the recent success in the video games has sparked a flurry of research interest. Following the success of Deep RL on Atari games (Mnih et al., 2015) and the board game Go (Silver et al., 2017), many researchers have begun exploring practical applications of deep reinforcement learning (DRL). Some investigated applications include, robotics (Levine et al., 2016), self-driving cars (Shalev-Shwartz et al., 2016), and safety (Lipton et al., 2016a). Inevitably for PSRL, the act of posterior sampling for policy or value is computationally intractable with large systems, so PSRL can not be easily leveraged to high dimensional problems (Ghavamzadeh et al., 2015; Engel et al., 2003). To remedy these failings Osband et al. (2017) consider the use of randomized value functions. For finite state-action space MDP, Osband et al. (2014) propose posterior sampling directly on the space of Q-functions and provide a strong Bayesian regret bound guarantee. To approximate the posterior, they use BLR on one-hot encoding of state-action and improve the computation complexity of PSRL. The approach deployed in BDQN is strongly related and similar to this work, and is a generalization to continues state-action space MDPs.
To combat these shortcomings, Osband et al. (2016) suggests a bootstrapped-ensemble approach that trains several models in parallel to approximate the posterior distribution. Other works suggest using a variational approximation to the Q-networks (Lipton et al., 2016b) or a concurrent work on noisy network (Fortunato et al., 2017). However, most of these approaches significantly increase the computational cost of DQN and neither approach produced much beyond modest gains on Atari games. Interestingly, Bayesian approach as a technique for learning a neural network has been deployed for object recognition and image caption generation where its significant advantage has been verified Snoek et al. (2015).
Concurrently, Levine et al. (2017) proposes least squares temporal difference which learns a linear model on the feature representation in order to estimate the Q-function while -greedy exploration is employed and improvement on 5 tested Atari games is provided. Out of these 5 games, one is common with our set of 15 games which BDQN outperform it by factor of 360% (w.r.t. the score reported in their paper). As motivated by the theoretical understanding, our empirical study shows that performing Bayesian regression instead, and sampling from the result, can yield a substantial
8

Under review as a conference paper at ICLR 2019
benefit, indicating that it is not just the higher data efficiency at the last layer, but that leveraging an explicit uncertainty representation over the value function is of substantial benefit. Drop-out, as another randomized exploration method is proposed by Gal & Ghahramani (2016) but Osband et al. (2016) investigates the sufficiency of the estimated uncertainty and hardness in driving a suitable exploitation out of it. As stated before, in spite of the novelties proposed by the methods, mentioned in this section, neither of them, including TSbased approaches, produced much beyond modest gains on Atari games while BDQN provides significant improvements in terms of both sample complexity and final performance.
7 CONCLUSION
In this work we proposed BDQN, a practical Thompson sampling based RL algorithm which provides targeted exploration in a computationally efficient manner. It involved making simple modifications to the DDQN architecture by replacing the last layer with a Bayesian linear regression. Under the Gaussian prior, we obtained fast closed-form updates for the posterior distribution. We demonstrated significantly faster training and much better performance in many games compared to the reported results of a wide number of state-of-the-art baselines. Due to the computational limitations we did not try the algorithm on all games and it remains an interesting issue to further explore its performance, and combine it with other advances in deep RL. We also established theoretical guarantees for an episodic MDP with continuous state and action spaces in the special case where the feature representation is fixed, BDQNis given the feature representation. We derived an order-optimal Bayesian regret bound of O(d N ) after N time steps. In future, we plan to extend the analysis to the more general Frequentist setting. The current theoretical guarantees are mainly developed for the class of linear functions. For general class of functions, optimism in the face of uncertainty is deployed to guarantee a tight PAC bound (Jiang et al., 2016) but the proposed algorithm requires solving a NP-hard problem at each iteration. We aim to further provide a tight theoretical bound through Thomson sampling while also preserving the computational feasibility.
9

Under review as a conference paper at ICLR 2019
REFERENCES
Yasin Abbasi-Yadkori and Csaba Szepesva´ri. Regret bounds for the adaptive control of linear quadratic systems. In COLT 2011 - The 24th Annual Conference on Learning Theory, June 9-11, 2011, Budapest, Hungary, 2011.
Yasin Abbasi-Yadkori and Csaba Szepesva´ri. Bayesian optimal control of smoothly parameterized systems. In UAI, pp. 1­11, 2015.
Yasin Abbasi-Yadkori, Da´vid Pa´l, and Csaba Szepesva´ri. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24 - NIPS, pp. 2312­2320, 2011.
David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, and Robert E Schapire. Exploratory gradient boosting for reinforcement learning in complex domains. arXiv, 2016.
Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In COLT, 2012.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773­2832, 2014.
Andra´s Antos, Csaba Szepesva´ri, and Re´mi Munos. Learning near-optimal policies with bellmanresidual minimization based fitted policy iteration and a single sample path. Machine Learning, 2008.
John Asmuth, Lihong Li, Michael L Littman, Ali Nouri, and David Wingate. A bayesian sampling approach to exploration in reinforcement learning. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, 2009.
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning of pomdps using spectral methods. In Proceedings of the 29th Annual Conference on Learning Theory (COLT), 2016a.
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning in rich-observation mdps using spectral methods. arXiv preprint arXiv:1611.03907, 2016b.
Ga´bor Barto´k, Dean P Foster, Da´vid Pa´l, Alexander Rakhlin, and Csaba Szepesva´ri. Partial monitoringclassification, regret bounds, and algorithms. Mathematics of Operations Research, 2014.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 2013.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. The Journal of Machine Learning Research, 3:213­231, 2003.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pp. 2249­2257, 2011.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv, 2015.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 208­214, 2011.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. 2008.
10

Under review as a conference paper at ICLR 2019
Victor H de la Pena, Michael J Klass, and Tze Leung Lai. Self-normalized processes: exponential inequalities, moment bounds and iterated logarithm laws. Annals of probability, pp. 1902­1933, 2004.
Yaakov Engel, Shie Mannor, and Ron Meir. Bayes meets bellman: The gaussian process approach to temporal difference learning. In Proceedings of the 20th International Conference on Machine Learning (ICML), 2003.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv, 2017.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In ICML, 2016.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian reinforcement learning: A survey. Foundations and Trends R in Machine Learning, 2015.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv, 2017.
Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian random vectors. Electronic Communications in Probability, 17, 2012.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 2010.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. arXiv, 2016.
Sham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 306­312, 2003.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2-3):209­232, 2002.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning research, 4(Dec):1107­1149, 2003.
Nir Levine, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. Shallow updates for deep reinforcement learning. arXiv, 2017.
Sergey Levine et al. End-to-end training of deep visuomotor policies. JMLR, 2016.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pp. 661­670. ACM, 2010.
Zachary C Lipton, Kamyar Azizzadenesheli, Abhishek Kumar, Lihong Li, Jianfeng Gao, and Li Deng. Combating reinforcement learning's sisyphean curse with intrinsic fear. arXiv preprint arXiv:1611.01211, 2016a.
Zachary C Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, and Li Deng. Efficient exploration for dialogue policy learning with bbq networks & replay buffer spiking. arXiv preprint arXiv:1608.05081, 2016b.
Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. arXiv preprint arXiv:1709.06009, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 2015.
11

Under review as a conference paper at ICLR 2019
Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1763­1771, 2012.
Ian Osband, Dan Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. In Advances in Neural Information Processing Systems, 2013.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. arXiv, 2014.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In Advances in Neural Information Processing Systems, 2016.
Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized value functions. arXiv, 2017.
Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Re´mi Munos. Count-based exploration with neural density models. arXiv, 2017.
Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning, volume 1. MIT press Cambridge, 2006.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395­411, 2010.
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221­1243, 2014.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. arXiv, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 2017.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep neural networks. In ICML, 2015.
Malcolm Strens. A bayesian framework for reinforcement learning. In ICML, 2000. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press
Cambridge, 1998. William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 1933. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In AAAI, 2016.
12

Under review as a conference paper at ICLR 2019

A APPENDIX
In the main text, for simplicity, we use the terms "BLR" for i.i.d. samples and BLR for non i.i.d. samples exchangeable, even though, technically, the do not have equal meaning. In RL, the data is not i.i.d, and we extend the BLR to non i.i.d. setting by deploying additional Martingale type argument and handles the data with temporal dependency.

Table 3: 1st column: score ratio of BDQN to DDQN run for same number of time steps. 2nd column: score ratio of BDQN to DDQN+. 3rd column: score ratio of BDQN to human scores reported at
Mnih et al. (2015). 4th column: Area under the performance plot ration (AuPPr) of BDQN to DDQN.
AuPPr is the integral of area under the performance plot ration. For Pong, since the scores start form -21, we shift it up by 21. 5th column: Sample complexity, SC: the number of samples the BDQN requires to beat the human score (Mnih et al., 2015)(" - " means BDQN could not beat human score). 6th column: SC+: the number of samples the BDQN requires to beat the score of DDQN+. We run
both BDQN and DDQN for the same number of times steps, stated in the last column.

Game

BDQN DDQN

Amidar Alien Assault Asteroids Asterix BeamRider BattleZone Atlantis DemonAttack Centipede BankHeist CrazyClimber ChopperCommand Enduro Pong

558% 103% 396% 2517% 531% 207% 281% 80604% 292% 114% 211% 148% 14500% 295% 112%

BDQN DDQN+
788% 103% 176% 1516% 385% 114% 253% 49413% 114% 178% 100% 122% 1576% 350% 100%

BDQN
HUMAN
325% 43% 589% 108% 687% 150% 172% 11172% 326% 61% 100% 350% 732% 361% 226%

AuPPr SC

C+ Steps

280% 110% 290% 680% 590% 210% 180% 380% 310% 105% 250% 150% 270% 300% 130%

22.9M -
1.6M 58.2M 3.6M 4.0M 25.1M 3.3M 2.0M
2.1M 0.12M 4.4M 0.82M 1.2M

4.4M 36.27M 24.3M
9.7M 5.7M 8.1M 14.9M 5.1M 19.9M 4.2M 10.1M 2.1M 2.2M 0.8M 2.4M

100M 100M 100M 100M 100M 70M 50M 40M 40M 40M 40M 40M 40M 30M
5M

A.1 NETWORK ARCHITECTURE:
The input to the network part of BDQN is 4 × 84 × 84 tensor with a rescaled and averaged over channels of the last four observations. The first convolution layer has 32 filters of size 8 with a stride of 4. The second convolution layer has 64 filters of size 4 with stride 2. The last convolution layer has 64 filters of size 3 followed by a fully connected layer with size 512. We add a BLR layer on top of this.
A.2 CHOICE OF HYPER-PARAMETERS:
For BDQN, we set the values of W target to the mean of the posterior distribution over the weights of BLR with covariances Cov and draw W from this posterior. For the fixed W and W target, we randomly initialize the parameters of network part of BDQN, , and train it using RMSProp, with learning rate of 0.0025, and a momentum of 0.95, inspired by (Mnih et al., 2015) where the discount factor is  = 0.99, the number of steps between target updates T target = 10k steps, and weights W are re-sampled from their posterior distribution every T sample steps. We update the network part of BDQN every 4 steps by uniformly at random sampling a mini-batch of size 32 samples from the replay buffer. We update the posterior distribution of the weight set W every T Bayes target using mini-batch of size B (if the size of replay buffer is less than B at the current step, we choose the minimum of these two ), with entries sampled uniformly form replay buffer. The experience replay contains the 1M most recent transitions. Further hyper-parameters are equivalent to ones in DQN setting.
13

Under review as a conference paper at ICLR 2019

Furthermore, for the BLR part of BDQN, we have noise variance  , variance of prior over weights , sample size B, posterior update period T Bayes target, and the posterior sampling period T sample. To optimize for this set of hyper-parameters we set up a very simple, fast, cheap, and non-exhaustive hyper-parameter tuning procedure using a pre-trained DQN model for the game of Assault. The simplicity and cheapness of our hyper parameter tuning proves the robustness and superiority of BDQN where the exhaustive hyper-parameter search is likely to provide even better performance. The details of hyper parameters tuning is provided in bellow.

A.3 HYPER-PARAMETERS TUNING:

For the BLR, we have noise variance  , variance of prior over weights , sample size B, posterior update period T Bayes target, and the posterior sampling period T sample. To optimize for this set of
hyper-parameters we set up a very simple, fast, and cheap hyper-parameter tuning procedure which
proves the robustness of BDQN. To find the first three, we set up a simple hyper-parameter search.
We used a pretrained DQN model for the game of Assault, and removed the last fully connected layer
in order to have access to its already trained feature representation. Then we tried combination of B = {T target, 10 · T target},  = {1, 0.1, 0.001}, and  = {1, 10} and test for 1000 episode of the game. We set these parameters to their best B = 10 · T target,  = 0.001,  = 1.

The above hyper-parameter tuning is cheap and fast since it requires only a few times the B number of

forwarding passes. For the remaining parameter, we ran BDQN ( with weights randomly initialized) on the same game, Assault, for 5M time steps, with a set of T Bayes target = {T target, 10 · T target}

and

T sample

=

{ ,T target
10

}T target
100

where

BDQN

performed

better

with

choice

of

T Bayes

target

=

10 · T target. For both choices of T sample, it performed almost equal where we choose the higher

one. We started off with the learning rate of 0.0025 and did not tune for that. Thanks to the efficient

Thompson sampling exploration and closed form BLR, BDQN can learn a better policy in an even

shorter period of time. In contrast, it is well known for DQN based methods that changing the learning

rate causes a major degradation in the performance, 4. The proposed hyper-parameter search is very

simple where the exhaustive hyper-parameter search is likely to provide even better performance.

A.4 LEARNING RATE:
It is well known that DQN and DDQN are sensitive to the learning rate and change of learning rate can degrade the performance to even worse than random policy. We tried the same learning rate as BDQN, 0.0025, for DDQN and observed that its performance drops. Fig. 4 shows that the DDQN with higher learning rates learns as good as BDQN at the very beginning but it can not maintain the rate of improvement and degrade even worse than the original DDQN.

Figure 4: Effect of learning rate on DDQN 14

Under review as a conference paper at ICLR 2019
A.5 COMPUTATIONAL AND SAMPLE COST COMPARISON:
For a given period of game time, the number of the backward pass in both BDQN and DQN are the same where for BDQN it is cheaper since it has one layer (the last layer) less than DQN. In the sense of fairness in sample usage, for example in duration of 10 · T Bayes target = 100k, all the layers of both BDQN and DQN, except the last layer, sees the same number of samples, but the last layer of BDQN sees 16 times fewer samples compared to the last layer of DQN. The last layer of DQN for a duration of 100k, observes 25k = 100k/4 (4 is back prob period) mini batches of size 32, which is 16 · 100k, where the last layer of BDQN just observes samples size of B = 100k. As it is mentioned in Alg. 1, to update the posterior distribution, BDQN draws B samples from the replay buffer and needs to compute the feature vector of them. Therefore, during the duration of 100k decision making steps, for the learning procedure, DDQN does 32  25k of forward passes and 32  25k of backward passes, while BDQN does same number of backward passes (cheaper since there is no backward pass for the final layer) and 36  25k of forward passes. One can easily relax it by parallelizing this step along the main body of BDQN or deploying on-line posterior update methods.
A.6 THOMPSON SAMPLING FREQUENCY:
The choice of Thompson sampling update frequency can be crucial from domain to domain. Theoretically, we show that for episodic learning, the choice of sampling at the beginning of each episode, or a bouned number of episodes is desired. If one chooses T sample too short, then computed gradient for backpropagation of the feature representation is not going to be useful since the gradient get noisier and the loss function is changing too frequently. On the other hand, the network tries to find a feature representation which is suitable for a wide range of different weights of the last layer, results in improper waste of model capacity. If the Thompson sampling update frequency is too low, then it is far from being Thompson sampling and losses randomized exploration property. We are interested in a choice of T sample which is in the order of upper bound on the average length of each episode of the Atari games. The current choice of T sample is suitable for a variety of Atari games since the length of each episode is in range of O(T sample) and is infrequent enough to make the feature representation robust to big changes.
For the RL problems with shorter horizon we suggest to introduce two more parameters, T~sample and W~ where T~sample, the period that of W~ is sampled our of posterior, is much smaller than T sample and W~ is being used just for making Thompson sampling actions while W is used for backpropagation of feature representation. For game Assault, we tried using T~sample and W~ but did not observe much a difference, and set them to T sample and W . But for RL setting with a shorter horizon, we suggest using them.
A.7 FURTHER INVESTIGATION IN ATLANTIS:
After removing the maximum episode length limit for the game Atlantis, BDQN gets the score of 62M. This episode is long enough to fill half of the replay buffer and make the model perfect for the later part of the game but losing the crafted skill for the beginning of the game. We observe in Fig. 5 that after losing the game in a long episode, the agent forgets a bit of its skill and loses few games but wraps up immediately and gets to score of 30M . To overcome this issue, one can expand the replay buffer size, stochastically store samples in the reply buffer where the later samples get stored with lowers chance, or train new models for the later parts of the episode. There are many possible cures for this interesting observation and while we are comparing against DDQN, we do not want to advance BDQN structure-wise.
A.8 FURTHER DISCUSSION ON REPRODUCIBILITY
In Table. 2, we provide the scores of bootstrap DQN (Osband et al., 2016) and NoisyNet8Fortunato et al. (2017) along with BDQN. These score are directly copied from their original papers and we did not make any change to them. We also desired to report the scores of count-based method (Ostrovski et al., 2017), but unfortunately there is no table of score in that paper in order to provide them here.
8This work does not have scores of Noisy-net with DDQN objective function but it has Noisy-net with DQN objective which are the scores reported in Table. 2
15

Under review as a conference paper at ICLR 2019

Figure 5: BDQN on Atlantis after removing the limit on max of episode length hits the score of 62M in 16M samples.

Figure 6: A couple of more runs of BDQN where the jump around 15M constantly happens
In order to make it easier for the readers to compare against the results in Ostrovski et al. (2017), we visually approximated their plotted curves for CT S, P ixel, P ixel, and Reactor, and added them to the Table. 2. We added these numbers just for the convenience of the readers. Surly we do not argue any scientific meaning for them and leave it to the readers to interpret them.
Table. 2 shows a significant improvement of BDQN over these baselines by looking at Table. 2. Despite the simplicity and negligible computation overhead of BDQN over DDQN, we can not scientifically claim that BDQN outperforms these baselines by just looking at the scores in Table.2 because we are not aware of their detailed implementation as well as environment detail. For example, in this work, we directly implemented DDQN by following the implementation details mentioned in the original DDQN paper and the scores of our DDQN implementation during the evaluation time almost matches the scores of DDQN reported in the original paper. But the reported scores of implemented DDQN in Osband et al. (2016) are much different from the reported score in the original DDQN paper.

A discussion on safety In BDQN, as mentioned in Eq. 3, the prior and likelihood are conjugate

of each others. Therefore, we have a closed form posterior distribution of the discounted return,

N t=0

trt|x0

=

x,

a0

=

a,

Da,

is

approximated

as

N

1 2



(x)

aaya, (x)

a  (x)

One can use this distribution and come up with a criterion for safe RL by just looking at the low and high probability events under different actions.

16

Under review as a conference paper at ICLR 2019

B BAYESIAN REGRET, PROOF OF THEOREM 1
Let's consider the following linear model over the Q functions and returns for episodic MDPs with episode length of H. (We consider the discount factor to be any number 0    1)

 1  2 . . . H-1   0 1  . . . H-2 



R = (x{1:H}, a{1:H}) + (x{1:H}, a{1:H}) ,

where

,



:=

 

0

0

1 ...

  

...

...

...

...

...

  

...

  

0 ... ... ... 1

(6) Where the Q function is a linear function of (x, a)  Rd, i.e. Q(x, a) := (x, a)  (for the

theoretical analysis, we use  instead of  to avoid any possible confusion). In order to study the

most general setting, we also represent actions through the same feature representation. The vector

R  RH is the random vector of rewards in a episode. The matrix (x{1:H}, a{1:H})  RH×d is

a row-wised concatenation of sequence of state-action features, (x, a) under a given policy, also

any instance of the feature representing matrix, trace (x{1:H}, a{1:H})(x{1:H}, a{1:H}) 

L , for anyx{1:H}, a{1:H}. The vector (x{1:H}, a{1:H})  RH is a state-action dependent noise

vector where, for any h , the noises after time step h , h=ih , are independent from the noises

before time step h , h=i<h condition on (xh, ah), the state action pair at time step h. Moreover, since the underlying model of environment is assumed to be an MDP, the noise at time step h, h models the induces noise by stochasticity of transition distribution to state xh, i.e. f (xh|xh-1, ah-1), and the stochasticity of reward at time step h, i.e. r(xh, ah). In the Bayesian approach, we consider

a Gaussian prior over the parameter vector   N (0, I), then exploit the samples to compute the

posterior. After computing the posterior at time t we sample a vector t, then follow the policy induced by t, i.e. t. We then collect the resulting trajectory to update the posterior and the policy for next episode.

In the following, we show that, given the true underlying model has parameter , after T episodes of following TS, the estimated ^bt concentrates around its true value . The main body of the following analyses on concentration of measure is a matrix extension of contextual linear bandit analyses Abbasi-Yadkori et al. (2011); Chu et al. (2011); Li et al. (2010); Rusmevichientong & Tsitsiklis (2010); Dani et al. (2008) and self normalized processes(de la Pena et al., 2004). Our regret analysis extends prior analysis in frequentist and Bayesian regret literature Osband et al. (2013); Jaksch et al. (2010).

In order to provide more intuition on the model, we can exploit the property of the matrix  where

 1 - 0 . . . 0 

0 1 - . . . 0



 -1 =  0 0

1

...

...

 



 

...

...

... ...

...

 

0 ... ... ... 1

and rewrite Eq. 6 as follows:

Rt = -1t + t We denote  := -1 . For the sake of analysis, we use the following formulation;

(7)

Rt := Rt = t + t , where t := t

(8)

Assumption 1 (Sub-Gaussian random vector (Hsu et al., 2012)). The noise model at time t, conditioned on the filtration at time t-1, Ft-1 is sub-Gaussian random vector, i.e. there exists a parameter   0 such that   Rd

E exp  t Ft-1  exp  22/2

17

Under review as a conference paper at ICLR 2019

Asm. 1 also implies that E t Ft-1 = 0 which means that after drawing the matrix t, the
generative model for t is means zero. A imilar assumption on the noise model is assumed in prior analyses of linear bandit (Abbasi-Yadkori et al., 2011).
Lemma 1. Let   Rd be be an arbitrary vector and for any t  0 define

Mt := exp

t



i i - 1 2

i

2 2

i

Then, for a stopping time under the filtration {Ft}t=0, M  1.

Proof. Lemma. 1 We first show that {Mt}t=0 is a supermartingale sequence. Let

Di = exp



i i - 1 2

i

2 2

Therefore, we can rewrite E [Mt] as follows: E [Mt|Ft-1] = E D1 . . . Dt-1Dt|Ft-1 = D1 . . . Dt-1E [Dt|Ft-1]  Mt-1

The last inequality follows since E [Dt|Ft-1]  1 due to Assumption 1, time step E [M1]  1, then E [Mt]  1. For a stopping time  , Define a

therefore since for the first

variable

M

 t

=

Mmin{t, }

and since E [M]

=

E

lim

inf

t

M

 t

 lim inft E

M

 t

 1, therefore the Lemma. 1

holds.

Let's defined the following quantities for self-normalized processes;

t
St := i i,
i

t
t := i i,
i=0

t = t + 

where  is a ridge regularization matrix and usually is equal to I.

Lemma 2. [Extension to Self-normalized bound in Abbasi-Yadkori et al. (2011)] For a stopping time  and filtration {Ft}t=0, with probability at least 1 - 

S

- 1



22

log( det (t)1/2 

det() )

Proof. Lemma. 2 Given the definition of the parameters of self-normalized process, we can rewrite Mt as follows;

Mt = exp

 St - 1 2



t

Consider  as a Gaussian random vector, denoted as f ( = ), with covariance matrix of -1. Define Mt := E Mt|F , which also results in E [Mt] = E E Mt|  1. Therefore

Mt = exp
Rd
= exp
Rd

 St - 1 2



t

f ()d

1 2

 - -t 1St/

2 t

+

1 2

St/

2 t-1

f ()d

det ()

1

= (2)d exp 2 St/ -t 1

exp
Rd

1 2

 - t-1St/

2 t

+

1 2



2 

d

Since t and  are positive semi definite and positive definite respectively, we have

 - -t 1St/

2 t

+



2 

=

=

- -

 + t-1  + -t 1

St/

2 +t

+

St/

2 +t

+

t-1St/

2 t

-

St/

2 (+t )-1

St/

-2
t-1

St/

2 (+t )-1

18

Under review as a conference paper at ICLR 2019

Therefore,

Mt = =

det ()

1

(2)d exp 2 St/ (+t)-1

exp
Rd

det () 1/2 exp
det ( + t)

1 2

St/

2 (+t )-1

1 2

-

 + -t 1

St/

2 +t

d

Since E [M ]  1 we have

P

S

2 (+ )-1



2 log

det ( +  )1/2  det ()1/2

E 

exp

1 2

S /

2 (+ )-1

det(+ ) 1/2  det()



Where the Markov inequality is deployed for the final step. The stopping is considered to be the time step as the first time in the sequence when the concentration in the Lemma. 2 does not hold.

Regret Decomposition Let's restate the definition of the expected regret and expected Bayesian regret.

T

RegT :=

V - Vt

t

where t is the deployed policy at episode t induced by t, t(x, a) = a = arg maxa t(x, a ) and V is the value of policy  on model derived by 9. The RHS for model based counterpart is VM  - VMt  . When there is a prior over the model choice  the expected Bayesian Regret might be target of study.

BayesRegT : = E [RegT ] := E

T
V (x0t ) - Vt (xt0)
t

T
= EHt E V (xt0) - Vt (x0t ) Ht
t

T
= EHt E V (x0t ) - Vtt (xt0) Ht
t

+ EHt E Vtt (xt0) - Vt (xt0) Ht

Here Ht is a multivariate random sequence which indicates history at the beginning of episode t,

ianncdludinagrexet0q, uaanldlydtisistritbhuetemdocdoenl dpiatiroanmeedteorncHhots, etnhethterormughEThomVpso-n

Sampling. Vtt Ht

Since the t = 0 and as a

consequence, the first term in the RHS of the last equality is zero. (This term of optimism analysis,

when t is the optimistic choice is zero.)

For a given state xt0, the value of the optimal policy t of model t under the very same model t is equal to

Vt (xt0, t) = (xt0, t(x0t )) t = E rt (x0t , t(xt0)) + Tt (x |x0t , t(xt0))Vt (x , t)
x
and for  under model  is equal to
V (x0t , ) = (x0t , (xt0))  = E r (x0t , (x0t )) + T (x |x0t , (xt0))V (x , )
x
9In general, the analysis also hold for general class of policies (stochastic) as long as the mapping from  to policy given and fixed for oracle and the RL algorithm
19

Under review as a conference paper at ICLR 2019

while (x0t , t(x0t )) at state x0t under the

t is model

value of t under t, (x0t , (xt0))

model t is

t the

avtaslutaeteofxf0tolwlohwicinhgisal(sxot0t)hfeoropotniemsatlepvaalnude

then follow t for the remaining part of horizon.

Furthermore, the value of t under model  is as follows:

V (x0t , t) = E r (x0t , t(x0t )) + T (x |x0t , t(x0t ))V (x , t)
x

Therefore, we can write the upper bound on the BayesRegT ;

Since

T

BayesRegT 

EHt E (xt0, t(xt0)) t - Vt (xt0) Ht

t

(xt0, t(x0t ))  = E r (x0t , t(xt0)) + T (x |xt0, t(xt0))V (x , )
x
the RHS term can be decomposed as follows; (x0t , t(x0t )) t - Vt (xt0) = (x0t , t(x0t )) t - (x0t , t(x0t ))  + (xt0, t(x0t ))  - Vt (x0t ) = (xt0, t(x0t )) (t - ) + (xt0, t(x0t ))  - Vt (xt0)




= (x0t , t(x0t )) (t - ) +

T

(Xt1|xt0,

t(x0t ))

V

(Xt1,

)

-

V

(Xt1,

t

) 

Xt1

(a)

where (a) can be decomposed the same way as V (x0t , ) - V (x0t , t). The remaining is to show each of these parts are bounded.

Lemma 3 (Confidence ellipsoid for problem in Eq. 6). For a matrix problem in Eq. 6, given a sequence of {i, Ri}ti=1, under the Asm. 1 and the following estimator for t
t -1 t

t := with probability at least 1 - 

i i + I
i

i Ri
i

t -  t   2 log where  2  L and trace(·)(·)  L.

1 + tL2/ 

+ 1/2L

Proof of Lemma 3. Given the estimator t we have the following:

t -1 t

t =

i i + I

i (i + i)

ii

t -1 t

= i i + I

i i

ii

t -1 t

+ i i + I

i i + I 

ii

t -1

-

i i + I



i

20

Under review as a conference paper at ICLR 2019

therefore, for any vector   Rd

 t -   =  - 

t -1 t

i i + I

i i

ii

t -1

i i + I



i

As a results, applying Cauchy-Schwarz inequality and inequalities



t-1



1 (t )



2



1 



2 we get

t

| t -  | =  -t 1

i i

-   t-1

t-1

 -t 1

i

  -t 1

t
i i -t 1 + 1/2  2
i

where applying self normalized Lemma 2, with probability at least 1 - 

| t -  |   -t 1

2 log

det (t)1/2  det ()1/2

+ 1/2L

hold for any . By plugging in  = t (t - ) we get the following;

t -  t  t() := 

2 log

det(t)1/2 det(I)1/2 

+ 1/2L

The det(t) can be written as det(t) =

d j

j

therefore,

trace(t)

=

d j

j .

We

also

know

that;

d
( j )1/d 
j

d j

j

d

A matrix extension to Lemma 10 and 11 in Abbasi-Yadkori et al. (2011) resutls in det(t) 

trace(t ) d

d
while we have trace(t) = trace(I) +

t i

i

i



d

+

tL,

therefore

the

main

statement of Lemma 3 goes through.

For the regret, condition on Ht, then drawn t following t

(x0t , t(xt0)) t - Vt (x0t )
= 1 t (t - ) = 1

tt-1/21t /2

(t

-

)



 H

t

-t 1

t - 

t

Therefore, by running the TS, we get a distribution over the whole history and for a given history we

have;

T

BayesRegT  HE

t -t 1 t -  t

t

T

 HE

t -t 1 t -  t + t t-1  -  t

t

  2 HE

T
t() t -t 1
t

Since we know assumed the maximum cumulative return of each episode is as most 1, we have;

 BayesRegT  2 HE

T min{t() t t-1 , 1}  2 HE
t

T
t() min{ t -t 1 , 1}
t

21

Under review as a conference paper at ICLR 2019

Moreover, at time T , we can use Jensen's inequality, exploit the fact that t() is an increasing function of t and have

 T BayesRegT  2 HE  T T ()2 min{ t t-1 , 1}
t



 2 H



2 log

1 + tL2/ 

+ 1/2L

E

T
T min{ t -t 1 , 1} (9)

t

Using the fact that for any scalar  such that 0    1, then   2 log(1 + ), we can rewrite the latter part of Eq. 9

TT
min{ t ,-t 1 1}  2 log 1 + t -t 1
tt
In order to bound the RHS of the this inequality, we use generalized matrix determinant lemma.

det (T ) = det T -1 + T T = det IH + T -t 1T det T -1
T
= det IH + t-t 1t det ()
t

From linear algebra, and given a time step t, we know the characteristic polynomial of the matrix tt-1t is CPt() = det(IH -tt-1t ). If the set {1, 2, . . . , d} represent the eigenvalues of tt-1t where i  j if i  j, one can rewrite CPt() as follows;

CP () = ( - 1)( - 2) · · · ( - H )

by adding the matrix IH to t-t 1t where i  )j, we get CPt () = det(IH - IH - tt-1t ), therefore;

CPt () = ( - 1 - 1)( - 1 - 2) · · · ( - 1 - H )

and the eigenvalues of IH + tt-1t are shifted eigenvalues of t-t 1t by +1, i.e. {1 +

1, 2 + 1, . . . , d + 1}. Since  t-t 1t  = t-1/2 

-t 1/2  all the eigenvalues

{1, 2, . . . , d} are non-negative. As a results,

As a consequence;

det IH + t-t 1t  1 + t t-1

T
det (T )  det ()
t

1 + t t-1

Therefore,

T
2 log 1 + t t-1  2 (log (det (T )) - log (det ()))  2d log( + T L2/d)
t

Substituting the RHS of Eq. 10 into Eq. 9, we get

(10)

 BayesRegT  2 H



2d log

1 + T L2/ 

+ 1/2L

2T d log( + T L2/d)

which is the the statement in the Theorem.

22

