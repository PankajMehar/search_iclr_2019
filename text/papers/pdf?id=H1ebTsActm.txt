Under review as a conference paper at ICLR 2019
ADAPTIVITY OF DEEP RELU NETWORK FOR LEARNING IN BESOV AND MIXED SMOOTH BESOV SPACES:
OPTIMAL RATE AND CURSE OF DIMENSIONALITY
Anonymous authors Paper under double-blind review
ABSTRACT
Deep learning has shown high performances in various types of tasks from visual recognition to natural language processing, which indicates superior flexibility and adaptivity of deep learning. To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness. The Besov space is a considerably general function space including the Ho¨lder space and Sobolev space, and especially can capture spatial inhomogeneity of smoothness. Through the analysis in the Besov space, it is shown that deep learning can achieve the minimax optimal rate and outperform any nonadaptive (linear) estimator such as kernel ridge regression, which shows that deep learning has higher adaptivity to the spatial inhomogeneity of the target function than other estimators such as linear ones. In addition to this, it is shown that deep learning can avoid the curse of dimensionality if the target function is in a mixed smooth Besov space. We also show that the dependency of the convergence rate on the dimensionality is tight due to its minimax optimality. These results support high adaptivity of deep learning and its superior ability as a feature extractor.
1 INTRODUCTION
Deep learning has shown great success in several applications such as computer vision and natural language processing. As its application range is getting wider, theoretical analysis to reveal the reason why deep learning works so well is also gathering much attention. To understand deep learning theoretically, several studies have been developed from several aspects such as approximation theory and statistical learning theory. A remarkable property of neural network is that it has universal approximation capability even if there is only one hidden layer (Cybenko, 1989; Hornik, 1991; Sonoda & Murata, 2015). Thanks to this property, deep and shallow neural networks can approximate any function with any precision (of course, the meaning of the terminology "any" must be rigorously defined like "any function in L1(R)"). A natural question coming next to the universal approximation capability is its expressive power. It is shown that the expressive power of deep neural network grows exponentially against the number of layers (Montufar et al., 2014; Bianchini & Scarselli, 2014; Cohen et al., 2016; Cohen & Shashua, 2016; Poole et al., 2016) where the "expressive power" is defined by several ways.
The expressive power of neural network can be analyzed more precisely by specifying the target function's property such as smoothness. Barron (1993; 1994) developed an approximation theory for functions having limited "capacity" that is measured by integrability of their Fourier transform. An interesting point of the analysis is that the approximation error is not affected by the dimensionality of the input. This observation matches the experimental observations that deep learning is quite effective also in high dimensional situations. Another typical approach is to analyze function spaces with smoothness conditions such as the Ho¨lder space. In particular, deep neural network with the ReLU activation (Nair & Hinton, 2010; Glorot et al., 2011) has been extensively studied recently from the view point of its expressive power and its generalization error. For example, Yarotsky (2016) derived the approximation error of the deep network with the ReLU activation for functions in the Ho¨lder space. Schmidt-Hieber (2017) evaluated the estimation error of regularized least squared estimator performed by deep ReLU network based on this approximation error analysis
1

Under review as a conference paper at ICLR 2019

Table 1: Comparison between the performances achieved by deep learning and linear methods. Here, N is the number of parameters to approximate a function in a Besov space (Bps,q([0, 1]d)), and n is the sample size. The approximation error is measured by Lr-norm. The O~ symbol hides the
poly-log order.

Model Approximation error rate
Estimation error rate

Deep learning

O~(N

-

s d

)

O~(n-

2s 2s+d

)

Linear method

O~ N -

s d

+(

1 p

-

1 r

)+

 n-

2s-(2/(p1)-1) 2s+1-(2/(p1)-1)

in a nonparametric regression setting. Petersen & Voigtlaender (2017) generalized the analysis by Yarotsky (2016) to the class of piece-wise smooth functions. Imaizumi & Fukumizu (2018) utilized this analysis to derive the estimation error to estimate the piece-wise smooth function and concluded that deep leaning can outperform linear estimators in that setting. Although these error analyses are standard from a nonparametric statistics view point and the derived rates are known to be (near) minimax optimal, the analysis is rather limited because the analyses are given mainly based on the Ho¨lder space. However, there are several other function spaces such as the Sobolev space and the space of finite total variations. A comprehensive analysis to deal with such function classes from a unified view point is required.
In this paper, we give generalization error bounds of deep ReLU networks for a Besov space and its variant with mixed smoothness, which includes the Ho¨lder space, the Sobolev space, and the function class with total variation as special cases. By doing so, (i) we show that deep learning achieves the minimax optimal rate on the Besov space and notably it outperforms any linear estimator such as the kernel ridge regression, and (ii) we show that deep learning can avoid the curse of dimensionality on the mixed smooth Besov space and achieves the minimax optimal rate. As related work, Mhaskar & Micchelli (1992); Mhaskar (1993); Chui et al. (1994); Mhaskar (1996); Pinkus (1999) also developed an approximation error analysis which essentially leads to analyses for Besov spaces. However, the ReLU activation is basically excluded and comprehensive analyses for the Besov space have not been given. Consequently, it has not been clear whether ReLU neural networks can outperform another representative methods such as kernel methods. As a summary, the contribution of this paper is listed as follows:
(i) To investigate adaptivity of deep learning, we give an explicit form of approximation and estimation error bounds for deep learning with the ReLU activation where the target functions are in the Besov spaces (Bps,q) for s > 0 and 0 < p, q   with s > d(1/p - 1/r)+ where Lr-norm is used for error evaluation. In particular, deep learning outperforms any linear estimator such as kernel ridge regression if the target function has highly spatial inhomogeneity of its smoothness. See Table 1 for the overview.
(ii) To investigate the effect of dimensionality, we analyze approximation and estimation problems in so-called the mixed smooth Besov space by ReLU neural network. It is shown that deep learning with the ReLU activation can avoid the curse of dimensionality and achieve the near minimax optimal rate. The theory is developed on the basis of the sparse grid technique (Smolyak, 1963). See Table 2 for the overview.

2 SET UP OF FUNCTION SPACES

In this section, we define the function classes for which we develop error bounds. In particular, we define the Besov space and its variant with mixed smoothness. The typical settings in statistical learning theory is to estimate a function with a smoothness condition. There are several ways to characterize "smoothness." Here, we summarize the definitions of representative functional spaces that are appropriate to define the smoothness assumption.

Let   Rd be a domain of the functions. Throughout this paper, we employ  = [0, 1]d. For

a function f :   R, let f p := f Lp() := (  |f |pdx)1/p for 0 < p < . For p =

, we define f  := f L() := supx |f (x)|. For   Rd, let || =

d j=1

|j

|.

Let

2

Under review as a conference paper at ICLR 2019

Table 2: Summary of relation between related existing work and our work for a mixed smooth Besov
space. N is the number of parameters in the deep neural network, n is the sample size.  represents
the smoothness parameter, and d represents the dimensionality of the input. The approximation accuracy is measured by L2-norm and estimation accuracy is measures by the square of L2-norm.
See Theorem 3 for the definition of u.

Function class Ho¨lder

Barron class

Approximation Author
Approx. error Estimation Author Estimation error

Yarotsky (2016), Liang

& Srikant (2016)

O~(N

-

 d

)

Schmidt-Hieber (2017)

O~(n-

2 2+d

)

Barron (1993) O~(N -1/2)

Barron (1993)

O~(n-

1 2

)

m-Sobolev (0 <   2)

m-Besov (0 < )

Montanelli & This work

Du (2017)

O~(N -)

O~(N -)

--- This work

---

O~(n-

2 2+1

×

2(d-1)(u+)
log(n) 1+2 )

C0() be the set of continuous functions equipped with L-norm: C0() := {f :   R |

f is continuous and

f



<

}

1.

For





N+d ,

we

denote

by

Df (x)

=

(x)||f
1 x1...d xd

2.

Definition 1 (Ho¨lder space (C())). Let  > 0 with   N be the smoothness parameter. For an

m times differentiable function f : Rd  R, let the norm of the Ho¨lder space C() be f C :=

max||m

Df



+

max||=m supx,y

|



f (x)-f (y)| |x-y|-m

,

where

m

=

 . Then, (-)Ho¨lder

space C() is defined as C() = {f | f C < }.

The parameter  > 0 controls the "smoothness" of the function. Along with the Ho¨lder space, the

Sobolev space is also important.

Definition 2 (Sobolev space (Wpk())). Sobolev space (Wpk()) with a regularity parameter k 

N and a parameter 1  p   is a set of functions such that the Sobolev norm f Wpk :=

(

||k

Df

p p

)

1 p

is

finite.

There are some ways to define a Sobolev space with fractional order, one of which will be defined

by using the notion of interpolation space (Adams & Fournier, 2003), but we don't pursue this

direction here. Finally, we introduce Besov space which further generalizes the definition of the

Sobolev space. To define the Besov space, we introduce the modulus of smoothness.

Definition 3. For a function f  Lp() for some p  (0, ], the r-th modulus of smoothness of f

is defined by

wr,p(f, t) = sup rh(f ) p,
h 2t

where hr (f )(x) =

r j=0

r j

(-1)r-jf (x + jh)

0

(x  , x + rh  ), (otherwise).

Based on the modulus of smoothness, the Besov space is defined as in the following definition.

Definition 4 (Besov space (Bp,q())). For 0 < p, q  ,  > 0, r :=  + 1, let the seminorm

| · |Bp,q be

|f |Bp,q :=

 0

(t-wr,p

(f,

t))q

dt t

1 q

supt>0 t-wr,p(f, t)

(q < ), (q = ).

The norm of the Besov space Bp,q() can be defined by f Bp,q := f p + |f |Bp,q , and we have Bp,q() = {f  Lp() | f Bp,q < }.

1Since  = [0, 1]d in our setting, the boundedness automatically follows from the continuity.
2 We let N+ := {0, 1, 2, 3, . . . }, N+d := {(z1, . . . , zd) | zi  N+}, R+ := {x  0 | x  R}, and R++ := {x > 0 | x  R}.

3

Under review as a conference paper at ICLR 2019

Note that p, q < 1 is also allowed. In that setting, the Besov space is no longer a Banach space but a quasi-Banach space. The Besov space plays an important role in several fields such as nonparametric statistical inference (Gine´ & Nickl, 2015) and approximation theory (Temlyakov, 1993a). These spaces are closely related to each other as follows (Triebel, 1983):

· For m  N, Bpm,1()  Wpm()  Bpm,(), and B2m,2() = W2m(). · For 0 < s <  and s  N, Cs() = Bs ,().

· For 0 < s, p, q, r   with s >  := d(1/p - 1/r)+, it holds that Bps,q()  Brs,-q (). In particular, under the same condition, from the definition of · Bps,q , it holds that

Bps,q()  Lr().

(1)

· For 0 < s, p, q  , if s > d/p, then Bps,q()  C0().

(2)

Hence, if the smoothness parameter satisfies s > d/p, then it is continuously embedded in the set of the continuous functions. However, if s < d/p, then the elements in the space are no longer continuous. Moreover, it is known that B11,1([0, 1]) is included in the space of bounded total variation (Peetre & Dept, 1976). Hence, the Besov space also allows spatially inhomogeneous smoothness with spikes and jumps; which makes difference between linear estimators and deep learning (see Sec. 4.1).
It is known that the minimax rate to estimate f o is lower bounded by n-2s/(2s+d), (Gine´ & Nickl, 2015). We see that the curse of dimensionality is unavoidable as long as we consider the Besov space. This is an undesirable property because we easily encounter high dimensional data in several machine learning problems. Hence, we need another condition to derive approximation and estimation error bounds that are not heavily affected by the dimensionality. To do so, we introduce the notion of mixed smoothness.

The Besov space with mixed smoothness is defined as follows (Schmeisser, 1987; Sickel & Ullrich, 2009). To define the space, we define the coordinate difference operator as

rh,i(f )(x) = hr (f (x1, . . . , xi-1, ·, xi+1, . . . , xd))(xi) for f : Rd  R, h  R+, i  [d], and r  1. Accordingly, the mixed differential operator for e  {1, . . . , d} and h  Rd is defined as

hr,e(f ) =

ie hr,ii (f ), rh,(f ) = f.

Then, the mixed modulus of smoothness is defined as

wre,p(f, t) := sup|hi|ti,ie rh,e(f ) p

for t  R+d and 0 < p  . Letting 0 < p, q  ,   R+d + and ri := i + 1, the semi-norm | · |MBp,,qe based on the mixed smoothness is defined by



|f |MBp,,qe

:=

 [( ie ti-i )wre,p(f, t)]q supt( ie ti-i )wre,p(f, t)

dt ie ti

1/q

(0 < q < ), (q = ).

By summing up the semi-norm over the choice of e, the (quasi-)norm of the mixed smooth Besov space (abbreviated to m-Besov space) is defined by

f MBp,q := f p +

|f |MBp,,qe ,

e{1,...,d}

and thus MBp,q() := {f  Lp() | f MBp,q < } where 0 < p, q  1 and   Rd++. In this paper, we assume that 1 = · · · = d. With a slight abuse of notation, we also use the notation
M Bp,q for  > 0 to indicate M Bp(,q,...,).

4

Under review as a conference paper at ICLR 2019

It is known that, when p = q, the m-Besov space is characterized as a tensor product space of Bps,q([0, 1]) (Sickel & Ullrich, 2009). The m-Besov space includes several important models considered in the literature of statistical learning, e.g., the additive model (Meier et al., 2009) and the tensor model (Signoretto et al., 2010). It is known that an appropriate estimator in these models can avoid curse of dimensionality (Meier et al., 2009; Raskutti et al., 2012b; Kanagawa et al., 2016; Suzuki et al., 2016). What we will show in this paper supports that this fact is also applied to deep learning from a unifying viewpoint.
The difference between the (normal) Besov space and the m-Besov space can be informally explained as follows. For regularity condition i  2 (i = 1, 2), the m-Besov space consists of functions for which the following derivatives are "bounded":
f f 2f 2f 2f 3f 3f 4f x1 , x2 , x21 , x22 , x1x2 , x1x22 , x21x2 , x12x22 .
That is, the "max" of the orders of derivatives over coordinates needs to be bounded by 2. On the other hand, the Besov space only ensures the boundedness of the following derivatives:
f f 2f 2f 2f x1 , x2 , x12 , x22 , x1x2 ,
where the "sum" of the orders needs to be bounded by 2. This difference directly affects the rate of convergence of approximation accuracy. Further details about this space and related topics can be found in a comprehensive survey (Du~ng et al., 2016).

Relation to Barron class. Barron (1991; 1993; 1994) showed that, if the Fourier transform of

a function f satisfies some integrability condition, then we may avoid curse of dimensionality for

estimating neural networks with sigmoidal activation functions. The integrability condition is given

by Cd  |f^()|d < , where f^ is the Fourier transform of a function f . We call the class of functions satisfying this condition Barron class. A similar function class is analyzed by Klusowski

& Barron (2016) too. We cannot compare directly the m-Besov space and Barron class, but they are

closely related. Indeed, if p = q = 2 and s = 1 = · · · = d, then m-Besov space MB2s,2() is equivalent to the tensor product of Sobolev space Sickel & Ullrich (2011) which consists of functions

f :   R satisfying Cd

d i=1

(1

+

|i

|2

)s

|f^()|2

d

<

.

Therefore,

our

analysis

gives

a

(similar

but) different characterization of conditions to avoid curse of dimensionality.

3 APPROXIMATION ERROR ANALYSIS
In this section, we evaluate how well the functions in the Besov and m-Besov spaces can be approximated by neural networks with the ReLU activation. Let us denote the ReLU activation by (x) = max{x, 0} (x  R), and for a vector x, (x) is operated in an element-wise manner. Define the neural network with height L, width W , sparsity constraint S and norm constraint B as
(L, W, S, B) := {(W (L)(·) + b(L))  · · ·  (W (1)x + b(1))
L
| W ( )  RW ×W , b( )  RW , ( W ( ) 0 + b( ) 0)  S, max W ( )   b( )   B},
=1
where · 0 is the 0-norm of the matrix (the number of non-zero elements of the matrix) and ·  is the -norm of the matrix (maximum of the absolute values of the elements). We want to evaluate how large L, W, S, B should be to approximate f o  MBp,q() by an element f  (L, W, S, B) with precision > 0 measured by Lr-norm: minf f - f o r  .
3.1 APPROXIMATION ERROR ANALYSIS FOR BESOV SPACES
Here, we show how the neural network can approximate a function in the Besov space which is useful to derive the generalization error of deep learning. Although its derivation is rather standard as considered in Chui et al. (1994); Bo¨lcskei et al. (2017), it should be worth noting that the bound derived here cannot be attained any non-adaptive method and the generalization error based on the analysis is also unattainable by any linear estimators including the kernel ridge regression. That

5

Under review as a conference paper at ICLR 2019

explains the high adaptivity of deep neural network and how it outperforms usual linear methods such as kernel methods.

To show the approximation accuracy, a key step is to show that the ReLU neural network can approximate the cardinal B-spline with high accuracy. Let N (x) = 1 (x  [0, 1]), 0 (otherwise), then the cardinal B-spline of order m is defined by taking m + 1-times convolution of N :

Nm(x) = (N  N  · · ·  N )(x),
m + 1 times

where f  g(x) := f (x - t)g(t)dt. It is known that Nm is a piece-wise polynomial of order m.

For k = (k1, . . . , kd)  Nd and j = (j1, . . . , jd)  Nd, let Mkd,j(x) =

d i=1

Nm(2ki xi

-

ji).

Even

for k  N, we also use the same notation to express Mkd,j(x) =

d i=1

Nm (2k xi

-

ji).

Here, k

controls the spatial "resolution" and j specifies the location on which the basis is put. Basically, we

approximate a function f in a Besov space by a super-position of Mkm,j(x), which is closely related

to wavelet analysis (Mallat, 1999).

Mhaskar & Micchelli (1992); Chui et al. (1994) have shown the approximation ability of neural network for a function with bounded modulus of smoothness. However, the activation function dealt with by the analysis does not include ReLU but it deals with a class of activation functions satisfying the following conditions,

lim (x)/xk  1, lim (x)/xk = 0, K > 1 s.t. |(x)|  K(1 + |x|)k (x  R), (3a)

x

x-

for k = 2 which excludes ReLU. Mhaskar (1993) analyzed deep neural network under the same
setting but it restricts the smoothness parameter to s = k + 1. Mhaskar (1996) considered the Sobolev space Wpm with an infinitely many differentiable "bump" function which also excludes ReLU. However, approximating the cardinal B-spline by ReLU can be attained by appropriately
using the technique developed by Yarotsky (2016) as in the following lemma.

Lemma 1 (Approximation of cardinal B-spline basis by the ReLU activation). There exists a constant c(d,m) depending only on d and m such that, for all > 0, there exists a neural network M 

(L0, W0, S0, B0) with L0 := 3+2 log2

+ 53dm
c(d,m)

log2(d  m) , W0 := 6dm(m+2)+2d,

S0 := L0W02 and B0 := 2(m + 1)m that satisfies

M0d,0 - M L(Rd)  ,

and M (x) = 0 for all x  [0, m + 1]d.

The proof is in Appendix A. Based on this lemma, we can translate several B-spline approximation results into those of deep neural network approximation. In particular, combining this lemma and the B-spline interpolant representations of functions in Besov spaces (DeVore & Popov, 1988; DeVore et al., 1993; Du~ng, 2011b), we obtain the optimal approximation error bound for deep neural networks. Here, let U (H) be the unit ball of a quasi-Banach space H, and for a set F of functions, define the worst case approximation error as

Rr(F , H) := sup inf
f oU (H) f F

fo - f

Lr ([0,1]d ) .

Proposition 1 (Approximation ability for Besov space). Suppose that 0 < p, q, r   and 0 < s <  satisfy the following condition:

s > d(1/p - 1/r)+.

(4)

Assume that m  N satisfies 0 < s < min(m, m - 1 + 1/p). Let  = (s - )/(2). For sufficiently large N  N and = N -s/d-(-1+d-1)(d/p-s)+ log(N )-1, let

L = 3 + 2 log2

3dm c(d,m)

S = (L - 1)W02N + N,

+ 5 log2(d  m) ,

W = N W0, B = O(N (-1+d-1)(1(d/p-s)+)),

then it holds that

Rr((L, W, S, B), Bps,q([0, 1]d)) N -s/d.

6

Under review as a conference paper at ICLR 2019

Remark 1. By Eq. (1), the condition (4) indicates that f o  Bps,q satisfies f o  Lr(). If we set p = q =  and r = , then Bps,q() = Cs() which yields the result by Yarotsky (2016) as a
special case.

The proof is in Appendix B. An interesting point is that the statement is valid even for p = r. In particular, the theorem also supports non-continuous regime (s < d/p) in which L-convergence does no longer hold but instead Lr-convergence is guaranteed under the condition s > d(1/p - 1/r)+. In that sense, the convergence of the approximation error is guaranteed in considerably general settings. Pinkus (1999) gave an explicit form of convergence when 1  p = r for the activation functions satisfying Eq. (3) which does not cover ReLU and an important setting p = r. Petrushev (1998) considered p = r = 2 and activation function with Eq. (3) where s is an integer and s  k + 1 + (d - 1)/2. Chui et al. (1994) and Bo¨lcskei et al. (2017) dealt with the smooth sigmoidal activation satisfying the condition (3) with k  2 or a "smoothed version" of the ReLU
activation which excludes ReLU; but Bo¨lcskei et al. (2017) presented a general strategy for neuralnet approximation by using the notion of best M -term approximation. Mhaskar & Micchelli (1992) gives an approximation bound using the modulus of smoothness, but the smoothness s and the order of sigmoidal function k in (3) is tightly connected and f o is assumed to be continuous which excludes the situation s < d/p. On the other hand, the above proposition does not require such a
tight connection and it explicitly gives the approximation bound for Besov spaces. Williamson &
Bartlett (1992) derived a spline approximation error bound for an element in a Besov space when d = 1, but the derived bound is only O(N -s+(1/p-1/r)+ ) which is the one of non-adaptive methods
described below, and approximation by a ReLU activation network is not discussed. We may also
use the analysis of Cohen et al. (2001) which is based on compactly supported wavelet bases, but
the cardinal B-spline is easy to handle through quasi-interpolant representation as performed in the
proof of Proposition 1.

It should be noted that the presented approximation accuracy bound is not trivial because it can not be achieved by a non-adaptive method. Actually, the best N -term approximation error (Kolmorogov width) of the Besov space is lower bounded as

inf sup inf
SN Bps,q f U (Bps,q ) fSN

f - f Lr()

N -s/d+(1/p-1/r)+ 
N -s/d+1/p-1/2
N -s/d

(1 < p < r  2, s > d(1/p - 1/r)), (1 < p < 2 < r  , s > d/p), (2  p < r  , s > d/2),
(5)

if 1 < p < r  , 1  q <  and 1 < s, where SN is any N -dimensional subspace of Bps,q (Romanyuk, 2009; Myronyuk, 2016; Vyba´ral, 2008). That is, any linear/non-linear approximator with fixed N -bases does not achieve the approximation error N -/d in some parameter settings
such as 0 < p < 2 < r. On the other hand, adaptive methods including deep learning can improve the error rate up to N -/d which is rate optimal (Du~ng, 2011b). The difference is significant when
p < r. This implies that deep neural network possesses high adaptivity to find which part of the
function should be intensively approximated. In other words, deep neural network can properly
extracts the feature of the input (which corresponds to construct an appropriate set of bases) to
approximate the target function in the most efficient way.

3.2 APPROXIMATION ERROR ANALYSIS FOR M-BESOV SPACE

Here, we deal with m-Besov spaces instead of the ordinary Besov space. The next theorem gives the

approximation error bound to approximate functions in the m-Besov spaces by deep neural network

models. Here, define Dk,d :=

1

+

d-1 k

k

1

+

k d-1

d-1
. Then, we have the following theorem.

Theorem 1 (Approximation ability for m-Besov space). Suppose that 0 < p, q, r   and s < 

satisfies s > (1/p - 1/r)+. Assume that m  N satisfies 0 < s < min(m, m - 1 + 1/p). Let

 = (1/p - 1/r)+ and  = (s - )/(2). For any K  1, let K =

K(1 +

2 -

)

.

Then,

for

N = (2 + (1 - 2- )-1)2K DK,d, if we set

L = 3 + 2 log2

3dm c(d,m)

+

5

+

(s

+

(

1 p

-

s)+

+

1)K 

+

log([e(m

+

1)]d(1

+

K  ))

W = N W0, S = (L - 1)N W02 + N, B = O(N (-1+1)(1(1/p-s)+)),

log2(d  m) ,

7

Under review as a conference paper at ICLR 2019

then it holds that (i) For p  r,
(ii) For p < r,

Rr((L, W, S, B), M Bps,q([0, 1]d)) Rr((L, W, S, B), M Bps,q([0, 1]d))

2-KsDK(1,/dmin(r,1)-1/q)+ ,

2-KsDK(1,/dr-1/q)+ 2-KsDK(1,-d1/q)+

(r < ), (r = ).

(6a) (6b)

The proof is given in Appendix C. Now, the number S of non-zero parameters for a given K is evaluated as S = (N ) 2K DK,d in this theorem. It holds that N 2K K(d-1), which implies 2-K N -1 logd-1(N ) if N d (see also the discussion right after Theorem 5 in Appendix
D.1 for more details of calculation). Therefore, when r q, the approximation error is given as O(N -s logs(d-1)(N )) in which the effect of dimensionality d is much milder than that of Proposi-
tion 1. This means that the curse of dimensionality is much eased in the mixed smooth space.

The obtained bound is far from obvious. Actually, it is better than any linear approximation methods as follows. Let the linear M -width introduced by Tikhomirov (1960) be N (M Bps,q, Lr) := infLN supfU(MBps,q) f - LN (f ) r, where the infimum is taken over all linear oprators LN with rank N from M Bps,q to Lr. The linear N -width of the m-Besov space has been extensively studies as in the following proposition (see Lemma 5.1 of Du~ng (2011a), and Romanyuk (2001)).
Proposition 2. Let 1  p, r  , 0 < q   and s > (1/p - 1/r)+. Then we have the following asymptotic order of the linear width for the asymptotics N d: (a) For p  r,

N (M Bps,q, Lr)



 (N

-1

logd-1

(N

))s



(N -1 logd-1(N ))s(logd-1(N ))1/r-1/q



 (N

-1

logd-1(N

))s(logd-1(N

))(1/2-1/q)+

 (q



2



r



p

<

),



(q  1, p = r = ),

(1 < p = r  2, q  r),

(1 < p = r  2, q > r),

(2  q, 1 < r < 2  p < ),

(b) For 1 < p < r < ,

M (M Bps,q, Lr)

(N -1 logd-1(N ))s+1/r-1/p

(2  p, 2  q  r),

(N -1 logd-1(N ))s+1/r-1/p(logd-1(N ))(1/r-1/q)+ (r  2).

Therefore, the approximation error given in Theorem 1 achieves the optimal linear width ((N -1 logd-1(N ))s) for several parameter settings of p, q, s. In particular, when p < r, the bound in Theorem 1 is better than that of Proposition 2. This is because to prove Theorem 1, we used an adaptive recovery technique instead of a linear recovery method. This implies that, by constructing a deep neural network accurately, we achieve the same approximation accuracy as the adaptive one which is better than that of linear approximation.

4 ESTIMATION ERROR ANALYSIS

In this section, we connect the approximation theory to generalization error analysis (estimation er-

ror analysis). For the statistical analysis, we assume the following nonparametric regression model:

yi = f o(xi) + i (i = 1, . . . , n),

where xi  PX with density 0  p(x) < R on [0, 1]d, and i  N (0, 2). The data Dn = (xi, yi)ni=1 is independently identically distributed. We want to estimate f o from the data. Here, we consider a regularized learning procedure:

n

f = argmin

(yi - f¯(xi))2

f¯:f (L,W,S,B) i=1

where f¯ is the clipping of f defined by f¯ = min{max{f, -F }, F } for F > 0 which is realized by

ReLU units. Since the sparsity level is controlled by S and the parameter is bounded by B, this esti-

mator can be regarded as a regularized estimator. In practice, it is hard to exactly compute f . Thus, we approximately solve the problem by applying sparse regularization such as L1-regularization and optimal parameter search through Bayesian optimization. The generalization error that we present

here is an "ideal" bound which is valid if the optimal solution f is computable.

8

Under review as a conference paper at ICLR 2019

4.1 ESTIMATION ERROR IN BESOV SPACES

In this subsection, we provide the estimation error rate of deep learning to estimate functions in Besov spaces.
Theorem 2. Suppose that 0 < p, q   and s > d(1/p - 1/2)+. If f o  Bps,q()  L() and. f o Bps,q  1 and f o   F for F  1, then letting (W, L, S, B) be as in Proposition 1 with
d
N n 2s+d , we obtain

EDn [

fo - f

2 L2

(PX

)

]

n-

2s 2s+d

log(n)2,

where EDn [·] indicates the expectation w.r.t. the training data Dn.

The proof is given in Appendix E. The condition f o   F is required to connect the empirical

L2-norm

1 n

in=1(f (xi) - f o(xi))2 to the population L2-norm

f - fo

2 L2

(PX

)

.

It is known that

the

convergence

rate

n-

2s 2s+d

is

mini-max

optimal

(Donoho

et

al.,

1998;

Gine´

&

Nickl,

2015).

Thus,

it cannot be improved by any estimator. Therefore, deep learning can achieve the minimax optimal

rate up to log(n)2-order. The term log(n)2 could be improved to log(n) by using the construction

of Petersen & Voigtlaender (2017). However, we don't pursue this direction for simplicity.

Here an important remark is that this minimax optimal rate cannot be achieved by any linear es-

timator. We call an estimator linear when the estimator depends on (yi)in=1 linearly (it can be non-linearly dependent on (xi)in=1). Several classical methods such as the kernel ridge regression, the Nadaraya-Watson estimator and the sieve estimator are included in the class of linear estimators

(e.g., kernel ridge regression is given as f (x) = kx,X (kXX + I)-1Y ). The following proposition

given by Donoho et al. (1998); Zhang et al. (2002) states that the minimax rate of linear estimators

is

lower

bounded

by

n-

2s-2(1/p-1/2)+ 2s+1-2(1/p-1/2)+

which

is

larger

than

the

minimax

rate

n-

2s 2s+2

if p < 2.

Proposition 3 (Donoho et al. (1998); Zhang et al. (2002)). Suppose that d = 1 and the input distribution PX is the uniform distribution on [0, 1]. Assume that s > 1/p, 1  p, q   or s = p = q = 1. Then,

inf sup EDn [
f : linear f oU (Bps,q )

fo - f

2 L2

(PX

)

]

n-

2s-v 2s+1-v

where v = 2/(p  2) - 1 and f runs over all linear estimators, that is, f depends on (yi)in=1 linearly.
When p < 2, the smoothness of the Besov space is somewhat inhomogeneous, that is, a function in the Besov space contains spiky/jump parts and smooth parts (remember that when s = p = q = 1 for d = 1, the Besov space is included in the set of functions with bounded total variation). Here, the setting p < 2 is the regime where there appears difference between non-adaptive methods and deep learning in terms of approximation accuracy (see Eq. (5)). On the other hand, the linear estimator captures only global properties of the function and cannot capture variability of local shapes of the function. Hence, the linear estimator cannot achieve the minimax optimal rate if the function has spatially inhomogeneous smoothness. However, deep learning possesses adaptivity to the spatial inhomogeneity.
Imaizumi & Fukumizu (2018) has pointed out that such a discrepancy between deep learning and linear estimator appears when the target function is non-smooth. Interestingly, the parameter setting s > 1/p assumed in Proposition 3 ensures smoothness (see Eq. (2)). This means that nonsmoothness is not necessarily required to characterize the superiority of deep learning, but nonconvexity of the set of target functions is essentially important. In fact, the gap is coming from the property that the quadratic hull of the model U (Bps,q) is strictly larger than the original set (Donoho et al., 1998).

4.2 ESTIMATION ERROR IN MIXED SMOOTH BESOV SPACES
Here, we provide the estimation error rate of deep learning to estimate functions in mixed smooth Besov spaces.

9

Under review as a conference paper at ICLR 2019

Theorem 3. Suppose that 0 < p, q   and s > (1/p - 1/2)+. Let u = (1 - 1/q)+ for p  2 and u = (1/2 - 1/q)+ for p < 2. If f o  MBps,q()  L() and f o MBps,q  1 and f o   F for F  1, then letting (W, L, S, B) be as in Theorem 1, we obtain

EDn [

fo - f

2 L2

(PX

)

]

n-

2s 2s+1

2(d-1)(u+s)
log(n) 1+2s

log(n)2.

Under the same assumption, if s > u log2(e) is additionally satisfied, we also have

EDn [

fo - f

2 L2

(PX

)

]

n log(n) .-

2s-2u log2(e) 2s+1+(1-2u) log2

(e)

2

The proof is given in Appendix E. The risk bound (Theorem 3) indicates that the curse of dimen-

sionality can be eased by assuming the mixed smoothness compared with the ordinary Besov space

(n-

2s 2s+d

).

We

show

that

this

is

almost

minimax

optimal

in

Theorem

4

below.

In

the

first

bound,

the

dimensionality d comes in the exponent of poly log(n) term. If u = 0, then the effect of d can be

further eased. Actually, in this situation (u = 0), the second bound can be rewritten as

n-

2s 2s+1+log2

(e)

log(n)2,

where the effect of the dimensionality d completely disappears from the exponent. This explains partially why deep learning performs well for high dimensional data. Montanelli & Du (2017) has analyzed the mixed smooth Ho¨lder space with s < 2. However, our analysis is applicable to the m-Besov space which is more general than the mixed smooth Ho¨lder space and the covered range of s, p, q is much larger.

Here, we again remark the adaptivity of deep learning. Remind that this rate cannot be achieved by the linear estimator for p < 2 when d = 1 by Proposition 3. This explains the adaptivity ability of deep learning to the spatial inhomogeneity of the smoothness.

Minimax optimal rate for estimating a function in the m-Besov space Here, we show the minimax optimality of the obtained bound as follows.

Theorem 4. Assume that 0 < p, q   and s > (1/p - 1/2)+ and PX is the uniform distribution over [0, 1]d. Regarding d as a constant, the minimax learning rate in the asymptotics of n   is

lower bounded as follows: There exists a constant C1 such that

inf sup EDn [
f f oU (M Bps,q )

f - fo

]  C n log(n)2
L2(PX )

1

-

2s 2s+1

2(d-1)(s+1/2-1/q)+ 2s+1

(7)

where "inf" is taken over all measurable functions of the observations (xi, yi)in=1 and the expectation is taken for the sample distribution.

The proof is given in Appendix F. Because of this theorem, our bound given in Theorem 3 achieves the minimax optimal rate in the regime of p < 2 and 1/2 - 1/q > 0 up to log(n)2 order. Even
outside of this parameter setting, the discrepancy between our upper bound and the minimax lower
bound is just a poly-log oder. See also Neumann (2000) for some other related spaces and specific
examples such as p = q = 2.

5 CONCLUSION
This paper investigated the learning ability of deep ReLU neural network when the target function is in a Besov space or a mixed smooth Besov space. Based on the analysis for the Besov space, it is shown that deep learning using the ReLU activation can achieve the minimax optimal rate and outperform the linear method when p < 2 which indicates the spatial inhomogeneity of the shape of the target function. The analysis for the mixed smooth Besov space shows that deep learning can adaptively avoid the curse of dimensionality. The bound is derived by sparse grid technique. All analyses in the paper adopted the cardinal B-spline expansion and the adaptive non-linear approximation technique, which allowed us to show the minimax optimal rate. The consequences of the analyses partly support the superiority of deep leaning in terms of adaptivity and ability to avoid curse of dimensionality. From more high level view point, these favorable property is reduced to its high feature extraction ability.
This paper did not discuss any optimization aspect of deep learning. However, it is important to investigate what kind of practical algorithms can actually achieve the optimal rate derived in this paper in an efficient way. We leave this important issue for future work.

10

Under review as a conference paper at ICLR 2019
REFERENCES
R.A. Adams and J.J.F. Fournier. Sobolev Spaces. Pure and Applied Mathematics. Elsevier Science, 2003.
Andrew Barron. Approximation and estimation bounds for artificial neural networks. In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, pp. 243­249, 1991.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930­945, 1993.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine Learning, 14(1):115­133, 1994.
Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. IEEE transactions on neural networks and learning systems, 25(8):1553­1565, 2014.
Helmut Bo¨lcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Optimal approximation with sparsely connected deep neural networks. arXiv preprint arXiv:1705.01714, 2017.
CK Chui, Xin Li, and HN Mhaskar. Neural networks for localized approximation. Mathematics of Computation, 63(208):607­623, 1994.
Albert Cohen, Wolfgang Dahmen, Ingrid Daubechies, and Ronald DeVore. Tree approximation and optimal encoding. Applied and Computational Harmonic Analysis, 11(2):192­226, 2001.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions. In Proceedings of the 33th International Conference on Machine Learning, volume 48 of JMLR Workshop and Conference Proceedings, pp. 955­963, 2016.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In The 29th Annual Conference on Learning Theory, pp. 698­728, 2016.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2(4):303­314, 1989.
Ronald A DeVore and Vasil A Popov. Interpolation of besov spaces. Transactions of the American Mathematical Society, 305(1):397­414, 1988.
Ronald A DeVore, George Kyriazis, Dany Leviatan, and Vladimir M Tikhomirov. Wavelet compression and nonlinearn-widths. Advances in Computational Mathematics, 1(2):197­214, 1993.
David L Donoho, Iain M Johnstone, et al. Minimax estimation via wavelet shrinkage. The Annals of Statistics, 26(3):879­921, 1998.
Dinh Du~ng. On recovery and one-sided approximation of periodic functions of several variables. In Dokl. Akad. SSSR, volume 313, pp. 787­790, 1990.
Dinh Du~ng. On optimal recovery of multivariate periodic functions. In Satoru Igari (ed.), ICM-90 Satellite Conference Proceedings, pp. 96­105, Tokyo, 1991. Springer Japan. ISBN 978-4-43168168-7.
Dinh Du~ng. Optimal recovery of functions of a certain mixed smoothness. Vietnam Journal of Mathematics, 20(2):18­32, 1992.
Dinh Du~ng. B-spline quasi-interpolant representations and sampling recovery of functions with mixed smoothness. Journal of Complexity, 27(6):541­567, 2011a.
Dinh Du~ng. Optimal adaptive sampling recovery. Advances in Computational Mathematics, 34(1): 1­41, 2011b.
Dinh Du~ng, Vladimir N Temlyakov, and Tino Ullrich. Hyperbolic cross approximation. arXiv preprint arXiv:1601.03978, 2016.
11

Under review as a conference paper at ICLR 2019
E´ . M. Galeev. Linear widths of ho¨lder-nikol'skii classes of periodic functions of several variables. Matematicheskie Zametki,, 59(2):189­199, 1996.
E. Gine´ and R. Nickl. Mathematical Foundations of Infinite-Dimensional Statistical Models. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 315­323, 2011.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4 (2):251­257, 1991.
Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effectively. arXiv preprint arXiv:1802.04474, 2018.
Heishiro Kanagawa, Taiji Suzuki, Hayato Kobayashi, Nobuyuki Shimizu, and Yukihiro Tagami. Gaussian process nonparametric tensor estimator and its minimax optimality. In Proceedings of the 33rd International Conference on Machine Learning (ICML2016), pp. 1632­1641, 2016.
Jason M Klusowski and Andrew R Barron. Risk bounds for high-dimensional ridge function combinations including neural networks. arXiv preprint arXiv:1607.01434, 2016.
Shiyu Liang and R Srikant. Why deep neural networks for function approximation? arXiv preprint arXiv:1610.04161, 2016. ICLR2017.
Stephane Mallat. A Wavelet Tour of Signal Processing. Academic Press, 1999.
Lukas Meier, Sara van de Geer, and Peter Bu¨hlmann. High-dimensional additive modeling. The Annals of Statistics, 37(6B):3779­3821, 2009.
Hrushikesh N Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural computation, 8(1):164­177, 1996.
Hrushikesh N Mhaskar and Charles A Micchelli. Approximation by superposition of sigmoidal and radial basis functions. Advances in Applied mathematics, 13(3):350­373, 1992.
Hrushikesh Narhar Mhaskar. Approximation properties of a multilayered feedforward artificial neural network. Advances in Computational Mathematics, 1(1):61­80, 1993.
Hadrien Montanelli and Qiang Du. Deep relu networks lessen the curse of dimensionality. arXiv preprint arXiv:1712.08688, 2017.
Guido F. Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N.d. Lawrence, and K.q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2924­2932. Curran Associates, Inc., 2014.
V Myronyuk. Kolmogorov widths of the anisotropic Besov classes of periodic functions of many variables. Ukrainian Mathematical Journal, 68(5), 2016.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning, pp. 807­814, 2010.
Michael H. Neumann. Multivariate wavelet thresholding in anisotropic function spaces. Statistica Sinica, 10(2):399­431, 2000.
J. Peetre and Duke University. Mathematics Dept. New thoughts on Besov spaces. Duke University mathematics series. Mathematics Dept., Duke University, 1976.
Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. arXiv preprint arXiv:1709.05289, 2017.
Pencho P Petrushev. Approximation by ridge functions and neural networks. SIAM Journal on Mathematical Analysis, 30(1):155­189, 1998.
12

Under review as a conference paper at ICLR 2019
Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8:143­ 195, 1999.
Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3360­3368. Curran Associates, Inc., 2016.
Garvesh Raskutti, Martin Wainwright, and Bin Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. Journal of Machine Learning Research, 13: 389­427, 2012a.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. The Journal of Machine Learning Research, 13(1):389­427, 2012b.
A. S. Romanyuk. Linear widths of the besov classes of periodic functions of many variables. ii. Ukrainian Mathematical Journal, 53(6):965­977, Jun 2001.
A. S. Romanyuk. Bilinear approximations and Kolmogorov widths of periodic Besov classes. Theory of Operators, Differential Equations, and the Theory of Functions, 6(1):222­236, 2009. Proc. of the Institute of Mathematics, Ukrainian National Academy of Sciences.
H-J Schmeisser. An unconditional basis in periodic spaces with dominating mixed smoothness properties. Analysis Mathematica, 13(2):153­168, 1987.
J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation function. ArXiv e-prints, August 2017.
Winfried Sickel and Tino Ullrich. Tensor products of Sobolev­Besov spaces and applications to approximation from the hyperbolic cross. Journal of Approximation Theory, 161(2):748­786, 2009.
Winfried Sickel and Tino Ullrich. Spline interpolation on sparse grids. Applicable Analysis, 90(3-4): 337­383, 2011.
M. Signoretto, L. De Lathauwer, and J.A.K. Suykens. Nuclear norms for tensors and their use for convex multilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.
Sergey Smolyak. Quadrature and interpolation formulas for tensor products of certain classes of functions. In Soviet Math. Dokl., volume 4, pp. 240­243, 1963.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approximator. Applied and Computational Harmonic Analysis, 2015.
Taiji Suzuki, Heishiro Kanagawa, Hayato Kobayashi, Nobuyuki Shimizu, and Yukihiro Tagami. Minimax optimal alternating minimization for kernel nonparametric tensor learning. In Advances In Neural Information Processing Systems, pp. 3783­3791, 2016.
V.N. Temlyakov. Approximation of periodic functions of several variables with bounded mixed difference. Math. USSR Sb, 41(1):53­66, 1982.
V.N. Temlyakov. Approximation of Periodic Functions. Nova Science Publishers, 1993a.
V.N. Temlyakov. On approximate recovery of functions with bounded mixed derivative. Journal of Complexity, 9:41­59, 1993b.
Vladimir Mikhailovich Tikhomirov. Diameters of sets in function spaces and the theory of best approximations. Uspekhi Matematicheskikh Nauk, 15(3):81­120, 1960.
Hans Triebel. Theory of function spaces. Monographs in mathematics. Birkha¨user Verlag, 1983. ISBN 9783764313814.
A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer, New York, 1996.
13

Under review as a conference paper at ICLR 2019

Jan Vyba´ral. Widths of embeddings in function spaces. Journal of Complexity, 24:545­570, 2008.
Robert C Williamson and Peter L Bartlett. Splines, rational functions and neural networks. In Advances in Neural Information Processing Systems, pp. 1040­1047, 1992.
Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of convergence. The Annals of Statistics, 27(5):1564­1599, 1999.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. CoRR, abs/1610.01145, 2016.
Shuanglin Zhang, Man-Yu Wong, and Zhongguo Zheng. Wavelet threshold estimation of a regression function with random design. Journal of multivariate analysis, 80(2):256­284, 2002.

A PROOF OF LEMMA 1

Proof of Lemma 1.

First note that Nm(x)

=

1 m!

m+1 j=0

(-1)j

m+1 j

(x - j)+m (see Eq.

(4.28) of

Mhaskar & Micchelli (1992) for example). Thus, if we can make an approximation of (x)m, then

by taking a summation of those basis, we obtain an approximate of Nm(x). It is shown by Yarotsky

(2016) that, for D  N and any > 0, there exists a neural network mult  (L, W, S, B) with

L = log2 3D + 5 log2(D) , W = 6d, S = LW 2 and B = 1 such that

D

sup mult(x1, . . . , xD) - xi  ,

x[0,1]d

i=1

and mult(0, . . . , 0) = 0 for y  Rd such that

d j=1

yj

=

0.

Moreover,

for

any

M

>

0,

we

can

realize the function min{M, max{x, 0}} by a single-layer neural network (0,M)(x) := (x) -

(x - M )(= min{M, max{x, 0}}). Thus, for x  R, it holds that

sup mult((0,1)(x/M ), . . . , (0,1)(x/M )) - ((0,1)(x/M ))m  .
x[0,M ]

Now, since Nm(x) = 0 for x  [0, m + 1],

1 m!

jm=+01(-1)j

m+1 j

(0,m+1-j)(x

-

j)m

=

1)m(0,1-j/(m+1))((x - j)/(m + 1))m. Therefore, letting

it also holds Nm(x)

1 m!

jm=+01(-1)j

m+1 j

(m

= +

f (x) =

1

m+1
(-1)j (m+1)m

m!

m+1 j

mult

j=0

we have that f (x) = 0 for all x  0 and

(0,1-

j m+1

)

x-j

m+1

,

.

.

.

,

(0,1-

j m+1

)

m-times

x-j m+1

sup |Nm(x)
0xm+1

- f (x)|



1 m!

m+1 j=0

m+1 j

(m + 1)m

  (m + 1)m 2m+1 2mm+1/2e-m

 e (2e)m =: , m

where we used

m+1 j=0

m+1 j

 = 2m+1 and Stirling's approximation m!  2mm+1/2e-m in the

second inequality. Hence, we also have

f (x) =

1

m+1
(-1)j

m+1

(m + 1)m

m! j

j=0

m+1-j

× mult

(0,1-

j m+1

)

m+1

=:  (x > m + 1).

,

.

.

.

,

(0,1-

j m+1

)

m+1-j m+1

,

14

Under review as a conference paper at ICLR 2019

It holds that | |  . Because of this and noting 0  Nm(x)  1, we see that g(x) := (0,1)(f (x)-

 m+1

(0,m+1)(x))

yields

sup |Nm(x) - g(x)|  2 ,
xR

supxR |g(x)|  1, and g(x) = 0 for all x  [0, m + 1]. Hence, by applying mult again, we finally obtain that

sup |M0d,0(x) - mult(g(x1), . . . , g(xd))|
x[0,1]d

dd

 sup M0d,0(x) - g(xj) + sup

g(xj) - mult(g(x1), . . . , g(xd))

x[0,1]d

j=1

x[0,1]d j=1

2d + .

We again applying (0,1), we obtain that h = (0,1)  mult(g(x1), . . . , g(xd)) satisfies M0d,0 - h L(Rd)  2d + , h(x) = 0 for all x  [0, m+1]d, and h   1. Finally, by carefully checking
the network construction, it is shown that h  (L, W, S, B) with L = 3 + 2 log 3dm +

5 log2(d  m) , W = 6dm(m + 2) + 2d, S = LW 2 and B = 2(m + 1)m. Hence, resetting

 2d

+

=

(1

+

2de

(2e)m m

)

,

this

h

is

the

desired

M .

B PROOF OF PROPOSITION 1

For the order m  N of the cardinal B-spline bases, let J(k) = {-m, -m + 1, . . . , 2k - 1, 2k}d and the quasi-norm of the coefficient (k,j)k,j for k  N+ and j  J(k) be



 (k,j )k,j .bps,q =

2k(s-d/p)

kN+

q 1/q

1/p 

|k,j |p 

.

j J (k)



Lemma 2. Under one of the conditions (4) in Proposition 1 and the condition 0 < s < min(m, m-
1 + 1/p) where m  N is the order of the cardinal B-spline bases, for any f  Bps,q(), there exists fN that satisfies

f - fN Lr() N -s/d f Bps,q

(8)

for N 1, and has the following form:

K K nk

fN (x) =

k,j Mkd,j (x) +

k,ji Mkd,ji (x),

k=0 jJ(k)

k=K+1 i=1

(9)

where (ji)in=k1  J (k), K = C1 log(N )/d , K = log(N )-1 + K + 1, nk =

N 2-(k-K) (k = K + 1, . . . , K) for  = d(1/p - 1/r)+ and  = (s - )/(2), and the real

number constants C1 > 0 and  > 0 are chosen to satisfy

K k=1

(2k

+

m)d

+

K k=K +1

nk



N

independently to N . Moreover, we can choose the coefficients (k,j) to satisfy

(k,j )k,j bps,q

f .Bps,q

Proof of Lemma 2. DeVore & Popov (1988) constructed a linear bounded operator Pk having the following form:

Pk(f )(x) =

ak,j Mkd,j (x)

j J (k)

(10)

where k,j is constructed in a certain way, where for every f  Lp([0, 1]d) with 0 < p  , it holds

f - Pk(f ) Lp  Cwr,p(f, 2-k).

(11)

15

Under review as a conference paper at ICLR 2019

Let pk(f ) := Pk(f ) - Pk-1(f ), P-1(f ) = 0.
Then, it is shown that for 0 < p, q   and 0 < s < min(m, m - 1 + 1/p), f belongs to Bps,q if and only if f can be decomposed into


f = pk(f ),
k=0

with the convergence condition (pk(f ))k=0 bsp(Lp) < ; in particular, f Bps,q (pk(f ))k=0 bsp(Lp) =: ( kN+ (2sk pk Lp )q)1/q. Here, each pk can be expressed as pk(x) =
jJ(k) k,jMkd,j(x) for a coefficient (k,j)k,j which could be different from (ak,j)k,j appearing in Eq. (10). Hence, f  Bps,q can be decomposed into


f = k,j Mkd,j (x)
k=0 jJ(k)

(12)

with convergence in the sence of Lp. (2-kd jJ(k) |k,j |p)1/p and thus

Moreover, it is shown that pk Lp

f Bps,q

(k,j )k,j .bps,q

(13)

Based on this decomposition, Du~ng (2011b) proposed an optimal adaptive recovery method such that the approximator has the form (9) under the conditions for K, K, nk given in the statement and satisfies the approximation accuracy (8). This can be proven by applying the proof of Theorem 3.1 in Du~ng (2011b) to the decomposition (12) instead of Eq. (3.8) of that paper. See also Theorem 5.4 of Du~ng (2011b). Moreover, the equivalence (13) gives the norm bound of the coefficient (k,j).

Proof of Proposition 1. Basically, we combine Lemma 1 and Lemma 2. We substitute the approximated cardinal B-spline basis M into the decomposition of fN (9). Let the set of indexes (k, j)  N × N that consists fN given in Eq. (9) be EN : fN = (k,j)EN k,jMkd,j. Accordingly, we set f := (k,j)EN k,j M kd,j . For each x  Rd, it holds that

|fN (x) - f(x)| 

|k,j ||Mkd,j (x) - M kd,j (x)|

(k,j)EN

 |k,j |1{Mkd,j (x) = 0}
(k,j)EN
 (m + 1)d(1 + K)2K(d/p-s)+ f Bps,q log(N )N (-1+d-1)(d/p-s)+ f ,Bps,q

where we used the definition of K in the last inequality. Therefore, for each f  U (Bps,q([0, 1]d)), it holds that

f - f Lr

f - fN Lr + fN - f Lr log(N )N (-1+d-1)(d/p-s)+ f Bps,q + N -s/d.

By taking

to satisfy log(N )N (-1+d-1)(d/p-s)+

 N -s/d (i.e.,

N -s/d-(-1+d-1)(d/p-s)+ log(N )-1), then we obtain the approximation error bound.



Next, we bound the magnitude of the coefficients. Each coefficient j,k satisfies |j,k|

2k(d/p-s)+ f Bps,q  2k(d/p-s)+

N (-1+d-1)(d/p-s)+ for k  K. Finally, the magni-

tudes of the coefficients hidden in M kd,j are evaluated. Remembering that M km,j(x) = M (2kx1 -

j1, . . . , 2kxd - jd), we see that we just need to bound the quantity 2k (k  K). However, this is

bounded by 2k  N -1+d-1 for k  K. Hence, we obtain the assertion.

16

Under review as a conference paper at ICLR 2019

C PROOF OF THEOREM 1

Let Nd+(e) := {s  N+d | si = 0, i  e} and for k  Nd+(e), we define

2-k

:=

(2-ki1 , . . . , 2-ki|e| )



|e|
R+

where

(i1, . . . , i|e|)

=

e.

By defining

(gk)k bq ,e :=

1/q

kN+d (e)(2 k 1 |gk|)q

for a sequence (gk)kNd+(e), then it holds that

|f |M Bp,,qe =

(wre,p(f, 2-k))k .bq,e

e{1,...,d}

Proof of Theorem 1. The result is immediately follows from Theorem 5. Let the set of indexes of (k, j) consisting of RK be EK : RK (f ) = (k,j)EK k,jMkd,j(x). As in the proof of Proposition
1, we approximate RK(f ) by a neural network given as

f(x) =

k,j M kd,j (x).

(k,j)EK

Each coefficient j,k satisfies |j,k| between

2 fk 1(1/p-s)+

M Bps,q

2K(1/p-s)+ . The difference

|RK (f ) - f(x)| 

|k,j ||Mkd,j (x) - M kd,j (x)|

(k,j)EK

 |k,j |1{Mkd,j (x) = 0}
(k,j)EK
(m + 1)d(1 + K)DK,d2K(1/p-s)+ f .MBps,q

Therefore, by taking so that (m + 1)d(1 + K)DK,d2K(1/p-s)+  2-Ks is satisfied, it holds

that |RK (f ) - f(x)| 2-Ks.

By the inequality DK,d  eK+d-1, it suffices to let



.e-K  (s+(1/p-s)+ +1)
[e(m+1)]d (1+K  )

The cardinality of

E(K) is bounded as

2  + d - 1 + d-1

nk

=0,...,K

k:K< k 1K

2K+1 K + d - 1 +

2K-

s- 2

(-K

)

+d-1

d-1

d-1

K <K 

2K+1DK,d

+

2K (1

-

2-

s- 2

)-1DK,d



(2

+

(1

-

2-

s- 2

)-1)2K DK,d

=

N.

Since each unit Mkd,j requires width W0, the whole width becomes W = N W0. The number of nonzero parameters to construct M kd,j is bounded by S = (L - 1)W02N + N .

Finally, the magnitudes of the coefficients hidden in Mkd,j are evaluated. Remembering that M kd,j (x) = M (2k1 x1 - j1, . . . , 2kd xd - jd), here maximum of 2kj is bounded by 2K N (1+1/). Hence, we obtain the assertion. Similarly, it holds that |j,k| N (1+1/){1(1/p-s)+}.

D PROOF OF THEOREM 5
D.1 PREPARATION: SPARSE GRID
Here, we give technical details behind the approximation bound. The analysis utilizes the so called sparse grid technique Smolyak (1963) which has been developed in the function approximation theory field.
As we have seen in the above, in a typical B-spline approximation scheme, we put the basis functions Mkm,j(x) on a "regular grid" for k = 1, . . . , K and (j1, . . . , jd)  J(k), and take its superposition as

17

Under review as a conference paper at ICLR 2019

f (x)  k=1,...,K jJ(k) k,jMkm,j(x), which consists of O(2Kd) terms (see Eq. (9)). Hence, the number of parameters O(2Kd) is affected by the dimensionality d in an exponential order. However, to approximate functions with mixed smoothness, we do not need to put the basis on the whole range of the regular grid. Instead, we just need to put them on a sparse grid which is a subset of the regular grid and has much smaller cardinality than the whole set. The approximation algorithm utilizing sparse grid is based on Smolyak's construction (Smolyak, 1963) and its applications to mixed smooth spaces (Du~ng, 1990; 1991; 1992; Temlyakov, 1982; 1993a;b). Du~ng (2011a) studied an optimal non-adaptive linear sampling recovery method for the mixed smooth Besov space based on the cardinal B-spline bases. We adopt this method, and combining this with the adaptive technique developed in Du~ng (2011b), we give the following approximation bound using a non-linear adaptive method to obtain better convergence for the setting p < r.

Before we state the theorem, we define an quasi-norm of a set of coefficients k,j  R for k  N+d and j  Jmd (k) := {-m, -m + 1, . . . , 2k1 - 1, 2k1 } × · · · × {-m, -m + 1, . . . , 2kd - 1, 2kd } as



(k,j )k,j mbp,q := 

2(-1/p) k 1

kNd+

q 1/q
1/p
|k,j |p   .
jJmd (k)

Theorem 5. Suppose that 0 < p, q, r   and  > (1/p - 1/r)+. Assume that the order m  N of the cardinal B-spline satisfies 0 < s < min(m, m - 1 + 1/p). Let  = (1/p - 1/r)+. Then, for any f  M Bps,q() and K > 0, there exists RK (f ) such that RK (f ) can be represented as

RK (f )(x) =

nk

k,j Mkd,j (x) +
kN+d : jJmd (k)

kNd+ :

i=1

k,ji(k)

Md
k,ji(k)

(x),

k 1K

K< k 1K

where K =

K(1 +

2 -

)

, (ji(k))in=k1



Jmd (k),

and nk

=

2K

-

- 2

(

k

1 -K )

,

and

has

the

following properties:

(i) For p  r,

f - RK (f ) r 2-KDK(1,/dmin(r,1)-1/q)+ f .M Bps,q

(ii) For p < r, f - RK (f ) r

2-KDK(1,/dr-1/q)+ f M Bps,q 2-KDK(1,-d1/q)+ f M Bps,q

(r < ), (r = ).

Moreover, the coefficients (k,j )k,j can be taken to hold (k,j )k,j mbp,q f .MBp,q

The proof is given in Appendix D.2. The total number of cardinal B-spline bases consisting of RK (f ) can be evaluated as

2K+1 K + d - 1 + d-1

nk

k:K< k 1K

2K DK,d + 2K DK,d 2K DK,d

( Eq. (16)).

Here, DK,d can be evaluated as

DK,d Kd-1 or DK,d dK .

Therefore, the total number of bases can be evaluated as

2K min{Kd-1, dK }

which is much smaller than 2Kd which is required to approximate functions in the ordinal Besov space (see Lemma 2). In this proposition, K controls the resolution and as M goes to infinity, the approximation error goes to 0 exponentially fast. A remarkable point in the proposition is in the

18

Under review as a conference paper at ICLR 2019

construction of RK (f ) in which the superposition is taken over k 1  M instead of k   K = O(K). Hence, the number of terms appearing in the summation is at most O(2K Kd-1) while the full grid takes O(2Kd) terms. This represents how the mixed smoothness is important to ease the curse of dimensionality.
Several aspects of the m-Besov space such as the optimal N -term approximation error and Kolmogorov widths have been extensively studied in the literature (see a comprehensive survey Du~ng et al. (2016)). An analogous result is already given by Du~ng (2011a) in which  > 1/p is assumed and a linear interpolation method is investigated. However, our result only requires  > (1/p - 1/q)+. This difference comes from a point that our analysis allows nonlinear adaptive interpolation instead of (linear) non-adaptive sampling considered in Du~ng (2011a). Because of this, our bound is better than the optimal rate of linear methods (Galeev, 1996; Romanyuk, 2001) and non-adaptive methods (Du~ng, 1990; 1991; 1992; Temlyakov, 1982; 1993a;b) especially in the regime of p < r (Du~ng (1992) also deals with adaptive method but does not cover p < r for adaptive method). See Proposition 2 for comparison.

D.2 PROOF OF THEOREM 5 Now we are ready to prove Theorem 5.

Proof of Theorem 5. For k = (k1, . . . , kd)  Nd+, let Pk(ii)f (x) be the function operating Pk defined in (10) to f as a function of xi with other components xj (j = i) fixed, and let

d
pk := (Pk(ii) - Pk(ii)-1)f.
i=1

(14)

Then, pk can be expressed as pk(x) = jJmd (k) k,j Mkd,j (x).

Let Tk(ii) = I - Pk(ii) xj (j = i) fixed (i.e.,

and f p,i if p < ,

be the Lp-norm of f as a function of xi with

f

p p,i

=

|f (x)|pdxi), then Eq. (11) gives

other

components

Tk(ii)f p,i

sup
|hi |2-ki

hr,ii(f ) p,i.

Thus, by applying the same argument again, it also holds

Tk(ii)Tk(jj)f p,i p,j

sup
|hi |2-ki

hr,ii(Tkj f ) p,i p,j

= sup
|hi |2-ki

Tkj rh,ii(f ) p,j p,i ( the definition of rh,ii and Fubini's theorem)

sup sup
|hi|2-ki |hj |2-kj

hr,ii(rh,jj (f )) p,j p,i,

for i = j. Thus, applying the same argument recursively, for u  [d], it holds that

Tk(ii)f
iu p

wru,p(f, 2-k)

for k  u[d](-1)|u|

Nd+(u). iu Tk(ii)

Therefore, since pk =

d i=1

(Tk(ii)-1

-

Tk(ii))f

iu

T (i)
ki-1

f , by letting e = {i | ki > 0}, we have that

=



pk p

 Tk(ii)

T (i)
ki-1



f

u[d] iu

iu

p

wru^,p(f, 2-(ku)u^ )
u[d]

wru,p(f, 2-ku )
eu

where kiu := ki (i  u) and kiu := ki - 1 (i  u), u^ = {i | kiu  0}, and (ku)u^ is a vector such that (ku)u^,i = kiu for i  u^ and (ku)u^,i = 0 for i  u^. Now let

1/q

(pk)k =bq (Lp)

2 k 1 pk Lp q
kN+d

19

Under review as a conference paper at ICLR 2019

for pk  Lp() (k  N+d ). Hence, if we set ak = eu wru,p(f, 2-ku ) for k  Nd and e = {i |

ki > 0}, we have that

(pk)k bq (Lp)

(ak) bq(Lp)

f .M Bps,q

On the other hand, following the same line of Theorem 2.1 (ii) of Du~ng (2011a), we also obtain the
opposite inequality f MBps,q (ak) bq (Lp) (pk)k bq(Lp) (note that the analogous inequality to Lemma 2.3 of Du~ng (2011a) also holds in our setting by replacing qs with ps and re(f, 2-k)p by wre,p).

Therefore, f  M Bp,q if and only if (pk)kN+d given by Eq. (14) satisfies (pk)k bq (Lp) < 

and f can be decomposed into f = kN+d pk where convergence is in M Bp,q. Moreover, it

holds that f MBp,q

(pk)k .bq(Lp) This can be shown by Theorem 2.1 of Du~ng (2011a).

Moreover, by the quasi-norm equivalence pk p 2- k 1/p( jJmd (k) |k,j |p)1/p, we also have

(k,j )k,j mbp,q

f .M Bp,q

If p  r, the assertion can be shown in the same manner as Theorem 3.1 of Du~ng (2011a).

For the setting of p < r, we need to use an adaptive approximation method. In the following, we assume p < r. For a given K, by choosing K appropriately later, we set

RK (f )(x) =

pk +

Gk (pk ),

kN+d : k 1K

kN+d :K< k 1K

where Gk(pk) is given as

Gk(pk) =

k,ji Mkd,ji (x)

1ink

where (k,ji )i|J=md1(k)| is the sorted coefficients in decreasing order of their absolute value: |k,j1 |  |k,j2 |  · · ·  |k,j|Jmd (k)| |. Then, it holds that

pk - Gk(pk) r  pk p2 k 1 nk-,

where  := (1/p - 1/r) (see the proof of Theorem 3.1 of Du~ng (2011b) and Lemma 5.3 of Du~ng (2011a)). Moreover, we also have

pk r  pk p2 k 1

for k  Nd+ with k 1 > K.

Here, we define N as Let = ( - )/(2), and

N = log2(K) . K = K(1 + 1/ ) ,

and nk = 2K- ( k 1-K) for k  Nd+ with K + 1  k 1  K.

Then, by Lemma 5.3 of Du~ng (2011a), we have that

f - RK (f )

r Lr

pk - Gk(pk)

r Lr

+

pk Lr ]r

K< k 1K

K< k 1

[ pk p2 k 1 nk-]r +

[2 k 1 pk Lp ]r.

K< k 1K

K< k 1

(15)

In the following, we require an upper bound of

k+d-1 d-1

.

Hence,

we

evaluate

this

quantity

before-

hand. This can be upper bounded by the Stering's formula as



k+d-1

2e

d-1 k

k d-1

d-1

 1+ 2 k

1+ d-1

 Dk,d.

=Dk,d

20

Under review as a conference paper at ICLR 2019

Let  > 0 be a positive real number satisfying 1 +   K/K. We can see that  can be chosen as  = 1/ + o(1). Then, we have that

DK,d

=

DK,d

(1 + (1 +

d-1 K

)K



d-1 K

)K

(1 + (1 +

K d-1

)d-1

K d-1

)d-1



DK,d

(1 + (1 +

d-1 K

)K



d-1 K

)K

1 K

1+

K d-1

+

(d - 1)(1 +

K d-1

)

 DK,d

d - 1 K-K 1 + K

d - 1 + K d-1

d-1+K

= DK,d

d-1 1+

K
(1 + )d-1

K

 DK,de(d-1)(1 + )d-1 DK,d.

(16)

d-1

(a) Suppose that q  r and r < . Then

f - RK (f )

q Lr

=

f - RK (f )

r

q r

Lr

q
 r



[2 k 1 n-k  pk Lp ]r +

[2 k 1 pk Lp ]r

K< k 1K

K< k 1



( Eq. (15))

[2 k 1 nk- pk Lp ]q +

[2 k 1 pk Lp ]q

K< k 1K

K< k 1

 N -q2-(-)Kq

[2-(-- )( k 1-K) 2 k 1 pk Lp ]q + 2-q(-)K

[2 k 1 pk Lp ]q

K< k 1K

1

K< k 1

(N -2-(-)K + 2-(-)K )q

f

q M Bp,q

 (N -)q

f

.q
M Bp,q

(b) Suppose that q > r and r < . Then, letting  = q/r(> 1) and  = 1/(1 - 1/) = q/(q - r), we have

f - RK (f )

r Lr

[2 k 1 nk- pk Lp ]r +

[2 k 1 pk Lp ]r

K< k 1K

K< k 1

( Eq. (15))

 N -r2-(-)Kr

[2-(-- )( k 1-K)2 k 1 pk Lp ]r +

[2 k 1 pk Lp ]r(2-(-) k 1 )r

K< k 1K

K< k 1

 (N -2-(-)K + 2-(-)K )r

[2-(-- )( k 1-K)2 k 1 pk Lp ]r

K< k 1K

+ [2 k 1 pk Lp ]r2-(-)( k 1-K)r

K< k 1

 1/

 (N -2-(-)K + 2-(-)K )r 

[2 k 1 pk Lp ]r +

 [2 k 1 pk Lp ]r

K< k 1K

K< k 1



 1/

 ×

[2-(-- )( k 1-K)]r +

[2-(-)( k 1-K)]r 

K< k 1K

K< k 1



(N -2-(-)K + 2-(-)K )r

f

r M

Bp,q

DKr(,1d/r-1/q)

(N -DK1/,rd-1/q)r

f

.r
M Bp,q

( Eq. (16))

(c) Suppose that r = . Then, similarly to the analysis in (b), we can evaluate

f - RK (f ) Lr

N -2-(-)K

[2-(-- )( k 1-K)2 k 1 pk Lp ] +

[2 k 1 pk Lp ](2-(-) k 1 )

K< k 1K

K< k 1

21

Under review as a conference paper at ICLR 2019

(N - 2-(-)K + 2-(-)K )DK(1,-d1/q)+ f M Bp,q N -DK(1,-d1/q)+ f .M Bp,q

E PROOFS OF THEOREMS 2 AND 3

Proof of Theorem 2. We use Proposition 4. We just need to evaluate the covering number of F^ = {f¯ | f  (L, W, S, B)} for (L, W, S, B) given in Theorem 1 where f¯ is the clipped function for a given f . Note that the covering number of F^ is not larger than that of (L, W, S, B). Thus, we may evaluate that of (L, W, S, B). From Lemma 3, the covering number is obtained as
log N (, F^, · ) N [log(N )2 + log(-1)].

From Proposition 1, it holds that

f o - RK (f o) 2 N -s/d.

Note that

f - fo

2 L2(PX )

f - fo

2 2

.

for any f : [0, 1]d  R because p(x)  R. Therefore, by applying Proposition 4 with  = 1/n, we

have that

EDn [

f - fo

2 L2

(PX

)

]

N -2s/d

+

N (log(N )2

+

log(n))

+

1 .

nn

(17)

Here, the right hand side is minimized by setting N

d
n 2s+d

up

to

log(n)2-order,

and

then

have

an

upper bound of the RHS as

n-

2s 2s+d

log(n)2.

This gives the assertion.

Proof of Theorem 3. The proof follows the almost same line as the proof of Theorem 2. By noting S = O(2K DK,d), L = O(K) and W = O(2K DK,d), Lemma 3 gives an upper bound of the covering number as
log N (, F^, · ) 2K DK,d[K log(2K DK,d) + log(-1)] 2K DK,d(K2 + log(1/)).

Letting r = 2, we have that

f o - RK (f o) 2 2-sK DKu ,d

where u = (1 - 1/q)+ for p  2 and u = (1/2 - 1/q)+ for p < 2.

Then, by noting that

f - fo

2 L2(PX )

f - fo

2 2

,

for any f : [0, 1]d  R, and by applying Proposition 4 with  = 1/n, we have that

EDn [

f - fo

2 L2

(PX

)

]

2-2sK DK2u,d

+

2K DK,d(K2 + n

log(-1))

+

1 .
n

(18)

Here, we use the following evaluations for DK,d: (a) DK,d

Kd-1, and (b) DK,d

[e(1

+

d K

)]K .

(a) For the evaluation, DK,d Kd-1, we have an upper bound of the right hand side of Eq. (18) as

2-2sK K2u(d-1)

+

2K Kd-1(K2

+

log(n)) ,

n

which is minimized by setting K =

1 1+2s

log2(n)

+

(2u-1)(d-1) 1+2s

log2 log(n)

up to log log(n)-

order. In this situation, we have the generalization error bound as

n-

2s 2s+1

2(d-1)(u+s)
log(n) 1+2s

log(n)2.

22

Under review as a conference paper at ICLR 2019

(b) For the evaluation, DK,d

[e(1

+

d K

)]K



eK ed,

Eq.

(18)

gives

an

upper

bound

of

2-2sK e2uK

+

2K eK (K2

+

log(n)) .

n

Then, the right hand side is minimized by K =

1 1+2s+(1-2u)

log2 (e)

log2(n)

.

Then, we have that

n log(n) .-

2s-2u log2(e) 1+2s+(1-2u) log2

(e)

2

This gives the assertion.

F MINIMAX OPTIMALITY

Proof of Theorem 4. First note that since PX is the uniform distribution, it holds that · L2(PX) = · L2([0,1]d). The -covering number N ( , G, L2(PX )) with respect to L2(PX ) for a function class
G is the minimal number of balls with radius measured by L2(PX )-norm needed to cover the set G (van der Vaart & Wellner, 1996). The -packing number M(, G, L2(PX )) of a function class G with respect to L2(PX ) norm is the largest number of functions {f1, . . . , fM}  G such that fi - fj L2(PX)   for all i = j. It is easily checked that

N (/2, G, L2(PX ))  M(, G, L2(PX ))  N (, G, L2(PX )).

(19)

For a given n > 0 and n > 0, let Q be the n packing number M(n, U (M Bps,q), L2(PX )) of U (M Bps,q) and N be the n covering number of that. Raskutti et al. (2012a) utilized the techniques developed by Yang & Barron (1999) to show the following inequality in their proof of Theorem 2(b)
:

inf sup EDn [
f f U (M Bps,q )

f - f

2 L2

(PX

)

]



inf
f

sup
f U (M Bps,q )

n2 2

P[

f - f

2 L2(PX )



n2 /2]

 n2

1

-

log(N )

+

n 22

n2

+

log(2)

.

2 log(Q)

Thus by taking n and n to satisfy

n 22

n2



log(N ),

8 log(N )  log(Q),

4 log(2)  log(Q),

(20)
(21) (22)

the

minimax

rate

is

lower

bounded

by

n2 4

.

This

can

be

achieved

by

properly

setting

n

for given N with respect to n > 0, M = log(N ) satisfies

n M -s log(M )(d-1)(s+1/2-1/q)+

n. Now,

(Theorem 6.24 of Du~ng et al. (2016)). Hence, it suffices to take

1 2(d-1)(s+1/2-1/q)+

M n 2s+1 log(n)

2s+1

,

  n log(n) ,n n

-

2s 2s+1

2(d-1)(s+1/2-1/q)+ 2s+1

(23) (24)

which gives the assertion.

G AUXILIARY LEMMAS
Let the -covering number with respect to L2(PX ) for a function class G be N ( , G, L2(PX )) as defined in the proof of Theorem 4.
23

Under review as a conference paper at ICLR 2019

Proposition 4 (Schmidt-Hieber (2017)). Let F be a set of functions. Let f be any estimator in F.

Define

n := EDn

1 n

n

(yi

-

f

(xi))2

-

inf
f F

1 n

n
(yi - f (xi))2

.

i=1 i=1

Assume that f o   F and all f  F satisfies f   F for some F  1. If  > 0 satisfies N (, F , · )  3, then it holds that

EDn [

f -f o

2 L2

(PX

)

]



(1+

)2

inf
f F

f - fo

2 L2(PX )

+

F

2

56

log

N

(,

F, n

·

) + 80 + 52F + n

,

for any  (0, 1].

Proof of Proposition 4. This is almost direct consequence of Lemma 8 of Schmidt-Hieber (2017). The only difference is the assumption of f   F for f  F and f = f o while Lemma 8 of Schmidt-Hieber (2017) assumed 0  f (x)  F for F > 1. However, this can be easily fixed by shifting the function value by +F then the range of f is modified to [0, 2F ]. Then, our situation is reduced to that of Lemma 8 of Schmidt-Hieber (2017) by substituting F  2F .
Lemma 3 (Covering number evaluation). The covering number of (L, W, S, B) can be bounded by
log N (, (L, W, S, B), · )  S log(-1L(B  1)L-1(W + 1)2L)  2SL log(-1L(B  1)(W + 1)).

Proof of Lemma 3. Given a network f  (L, W, S, B) expressed as
f (x) = (W (L)(·) + b(L))  · · ·  (W (1)x + b(1)),
let Ak(f )(x) =   (W (k-1)(·) + b(k-1))  · · ·  (W (1)x + b(1)),
and Bk(f )(x) = (W (L)(·) + b(L))  · · ·  (W (k)(x) + b(k)),
for k = 2, . . . , L. Corresponding to the last and first layer, we define BL+1(f )(x) = x and A1(f )(x) = x. Then, it is easy to see that f (x) = Bk+1(f )  (W (k) · +b(k))  Ak(f )(x). Now, suppose that a pair of different two networks f, g  (L, W, S, B) given by
f (x) = (W (L)(·)+b(L))· · ·(W (1)x+b(1)), g(x) = (W (L) (·)+b(L) )· · ·(W (1) x+b(1) ),
has a parameters with distance : W ( ) - W ( )    and b( ) - b( )   . Now, not that Ak(f )   maxj Wj(,k: -1) 1 Ak-1(f )  + b(k-1)   W B Ak-1(f )  + B  (B  1)(W + 1) Ak-1(f )   (B  1)k-1(W + 1)k-1, and similarly the Lipshitz continuity of Bk(f ) with respect to · -norm is bounded as (BW )L-k+1. Then, it holds that
|f (x) - g(x)|
L
= Bk+1(g)  (W (k) · +b(k))  Ak(f )(x) - Bk+1(g)  (W (k) · +b(k) )  Ak(f )(x)
k=1 L
 (BW )L-k (W (k) · +b(k))  Ak(f )(x) - (W (k) · +b(k) )  Ak(f )(x) 
k=1 L
 (BW )L-k[W (B  1)k-1(W + 1)k-1 + 1]
k=1 L
 (BW )L-k(B  1)k-1(W + 1)k  L(B  1)L-1(W + 1)L.
k=1

24

Under review as a conference paper at ICLR 2019

Thus, for a fixed sparsity pattern (the locations of non-zero parameters), the covering number is

bounded by /[L(B  1)L-1(W + 1)L] -S. There are the number of configurations of the spar-

sity pattern is bounded by

(W +1)L S

 (W + 1)LS. Thus, the covering number of the whole space

 is bounded as

(W + 1)LS /[L(B  1)L-1(W + 1)L] -S = [-1L(B  1)L-1(W + 1)2L]S,

which gives the assertion.

25

