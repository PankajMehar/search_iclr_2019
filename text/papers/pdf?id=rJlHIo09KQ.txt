Gradient-based Training of Slow Feature Analysis by Differentiable Approximate Whitening
Anonymous
Abstract
We propose Power Slow Feature Analysis, a gradient-based method to extract temporally slow features from a high-dimensional input stream that varies on a faster time-scale, as a variant of Slow Feature Analysis (SFA). While displaying performance comparable to hierarchical extensions to the SFA algorithm, such as Hierarchical Slow Feature Analysis, for a small number of output-features, our algorithm allows fully differentiable end-to-end training of arbitrary differentiable approximators (e.g., deep neural networks). We provide experimental evidence that PowerSFA is able to extract meaningful and informative low-dimensional features in the case of (a) synthetic low-dimensional data, (b) visual data, and also for (c) a general dataset for which symmetric non-temporal relations between points can be defined.
1 Introduction
Finding meaningful representations in data is a core challenge in modern machine learning as the performance in many goal-directed frameworks such as reinforcement learning or supervised learning is strongly influenced by the quality of the former. Usually, features are either domain specific or acquired through learning. Most currently successful approaches for either deep supervised learning [1] or reinforcement learning [2] rely on a training signal, i.e., a classification label or reward signal, to provide sufficient indication which features of the input data should be extracted to increase performance. However, in most real-world scenarios labels have to be provided by expert knowledge and reward signals are sparse. In unsupervised representation learning one tries to find and apply a principle by which to extract meaning from data without assuming the availability of any goal-driven metrics. Examples for such principles are based on reconstruction error (principal component analysis, autoencoder networks [3]) , statistical dependence of extracted features (independent component analysis [4]), indistinguishability of synthetically generated data from samples of the input distribution (generative adversarial nets [5]), fitting the probability distribution of input data (variational autoencoders [6]), (graph-)neighborhood
1

preservation (locally linear embedding [7], Laplacian eigenmaps [8]), and temporal coherence (slow feature analysis [9], regularized slowness optimization [10]). The latter is the focus of this work and has been shown to provide a useful proxy for extracting underlying causes from time-series data such as position, head direction, identity of spatial view similar to those observed in rodent brains [11] from ego-visual data, or object configuration (identity, position, angle) from a tabletop view of moving objects [12] when applied in the form of slow feature analysis (SFA) [9].
A graph-based generalization of SFA [13] has been used to achieve (at that time) state-of-the-art age estimation results on the MORPH dataset [14].
We propose a variant of SFA that approximately enforces the same constraints while being differentiable and thus allows training by gradient-descentderived methods. This variant makes it possible to leverage the representational power of complex models such as deep neural networks and useful ideas from that domain (dropout, batch normalization, activation regularization) to extract slow and informative features from data. To demonstrate the applicability of this approach, we provide three distinct experimental evaluations.

2 Related work

2.0.1 Slowness-based Methods

While the original proposal of SFA [9] uses non-linear basis functions as a

method to introduce non-linearity to the otherwise linear model, this classical

approach has two limitations: a) The basis functions are fixed and thus have to

be chosen beforehand by expert knowledge or trial-and-error, and b) as the re-

sulting model is shallow, its expressivity tends to scale unfavorably in the dimen-

sion of the expansion [15] compared to hierarchical extensions discussed later. In

the case of polynomial expansion, expanding to degree d on e-dimensional input

data results in

d+e d

-dimensional

expanded

data,

e.g.,

quadratically

expanding

grayscale images of 180 × 90 pixels results in an output dimension > 131 · 106.

Extracting a single feature with a linear model would thus require 5.7-times

more parameters than the modern and powerful Xception network [16].

A classical way to circumvent this problem is to apply the kernel trick [17]

that allows for implicit expansion of arbitrarily high (even infinite) dimension as

long as the solution can be expressed in inner products of the input data. But

while it is possible to formulate SFA in such a way [18] [19] and kernel-based

methods are usually very expressive (universal approximators, they carry their

own problems, as they a) still require a domain-specific choice of kernel, b) also

require strong regularization to avoid over-fitting, and c) their memory- and

evaluation complexity often scales linearly with the number of training points

[20] [21]. These properties severely limit the applicability of kernel methods

in domains with vast amounts of high-dimensional input data, e.g. in visual

processing.

An alternative approach to increase expressivity using expansions is to ap-

2

ply low-degree non-linearities repeatedly in a receptive-field fashion, interlacing them with projection steps. This has been done in Hierarchical SFA (HSFA) [11], [22] and deep architectures [1] in general. But while the latter are typically trained in an end-to-end fashion by variants of stochastic gradient-descent, HSFA is trained in a layer-wise procedure, solving the linear SFA problem consecutively for each layer as closed-form solution and thereby assuming that globally slow features can be composed of decreasingly local slow features. This assumption can be partially relaxed by adding information by-passes to the model [22]. Compared to HSFA, our method allows directly modifying parameters in different layers to optimize a global slowness objective.
Our algorithm is in line with recent work harnessing the temporal coherence prior [23] in deep, self-supervised feature learning. This can be done by having a slowness term in the loss function. To avoid the trivial, constant solution, another term is usually added. For example, a reconstruction loss in an auto-encoder [24] or one-step latent code prediction [25]. Deep temporal coherence has also been considered via the lens of similarity metric learning, for example by optimizing a contrastive loss [26][27] or a triplet loss [28][29]. These metric learning approaches manage to avoid degenerate solutions by pushing points away from each other (in feature space) that are not temporal neighbors. Our work differs from these approaches as we seek to directly approximate the optimization problem as originally stated by [9] in a deep learning setting, automatically handling the constant solution and ensuring that different features code for different information.
2.0.2 Graph-based Methods
SFA has been generalized to graph-based SFA (GSFA) [13] and generalized SFA [30]. The former adapts only the objective function, while the latter additionally generalizes the constraints to D-orthogonality (D being the degree matrix of an underlying graph). Following generalized SFA, standard SFA can be shown (cf. [30]) to be a special case of Laplacian Eigenmaps [8]. Spectral Inference Networks (SpIN) [31] utilize this connection to successfully derive a gradientbased SFA training as a special case and have been developed simultaneously to this work. SpIN are based on correcting a biased gradient when directly optimizing the Rayleigh-Quotient with respect to the parameters that define the embedding. Similar to PowerSFA, this procedure allows for employing any differentiable architecture to find these embeddings. The generalization of PowerSFA proposed in section 6.3 is close to GSFA, assuming regularity (or rather: ignoring non-regularity) in the graph for the orthogonality constraint.
SpectralNets (SN) [32] are another closely related approach in which a differentiable approximator is trained to learn spectral embeddings used in subsequent k-means clustering. However, opposed to PowerSFA and SpIN, they split a single optimization step into two parts: an ortho-normalization step based on explicitly calculating the Cholesky decomposition of the batch covariance matrix to set and freeze the weights of a linear output layer followed by a stochastic optimization step. While this can be considered end-to-end depending on the
3

paradigm, [32] do not indicate if and how this can be implemented as fully differentiable architecture.
It should be mentioned that as a special case of Laplacian eigenmaps, SFA could also be expressed as a variant of Kernel PCA using a data-dependent kernel [21]. This requires the application of an explicit Kernel PCA projection [33] or, equivalently, Nyström's method for approximating eigenfunctions [34] to embed out-of-sample points and is thus limited for large amounts of training data, suffering from similar limitations to those mentioned in 2.0.1.
2.0.3 Other Related Approaches
In section 6.3 we consider a generalization of PowerSFA's loss similar to GSFA and apply it to the NORB dataset [35]. We thereby loosely follow the experimental procedure in [36]. The authors use a siamese neural network architecture [37] for optimizing pair-wise distances of embedded points to reflect similarity and dissimilarity structure of the data. In particular, they do not enforce orthogonality of the embeddings but rely on the optimization procedure to maximize informativeness.

3 Slow Feature Analysis
Slow Feature Analysis (SFA) is based on the hypothesis that interesting highdimensional streams of data that vary quickly in time are typically caused by a low number of underlying factors that vary comparably slow. Therefore, slowness can be used as a proxy criterion by which to extract meaningful representations of these low-dimensional underlying causes even in the absence of labels.
There is strong evidence in favor of this hypothesis as it has been shown that features extracted by SFA tend to encode highly relevant information about the data-generating environments, e.g. slow features encode and disentangle object identity, rotation, and position in visual tasks [12] as well as agent position and orientation from visual first-person recordings of random movement similar to place cells in rodents or head-direction cells in primates [11].
The notion of extracting slow features from a time-series dataset has been formalized as a sequential optimization problem. Given a time-series {xt}t=0... with xt  Rd, sequentially find continuous functions gi : Rd  R with:

mgiin (gi(xt+1) - gi(xt))2 t
s.t. gi(xt) t = 0, gi(xt)2 t = 1, gi(xt)gj(xt) t = 0, j < i

(1a)
(1b) (1c) (1d)

4

where · t is the average over t. The constraints ensure that each of the extracted features is informative (decorrelated to all others, equation (1d)) and non-trivial (unit variance, equation (1c)). Originally, solutions to SFA directly were only proposed for the space of affine functions gi  G, for which a closedform solution exists, and in a kernelized version that requires strong regularization.

4 (Approximate) Whitening

While the standard implementation of linear SFA [38] is based on computing the closed-form solution to a generalized eigenvalue problem [39], another approach is to first whiten the time-series data followed a projection onto the minor components of the difference time-series {x t = xt+1 - xt}t=0...-1.
Whitened data has three important properties: a) it is mean-free (constraint (1b)), b) has unit variance if projected onto an arbitrary unit vector (constraint (1c)), and c) projections onto orthonormal vectors are decorrelated (constraint (1d)).
For a dataset X~  Rd×N with N and d being size of dimension of the dataset, respectively, the corresponding whitened dataset is defined as

X = WX~

with

W

=

D-

1 2

UT

and

C

=

UDUT



Rd×d

being the whitening

matrix

and the canonical eigendecomposition of the covariance matrix, respectively.

U's columns contain the eigenvectors of C and D contains the corresponding

eigenvalues on its diagonal. As C is typically assumed to be positive definite,

D-

1 2

is

well-defined.

One widely used method to extract eigenvector/-value pairs is power iter-

ation. Starting from a random vector u0 R Rd, repeatedly applying:

ui+1 =

Cui Cui

converges to the eigenvector u corresponding to the largest (absolute) eigenvalue . The eigenvalue can then be extracted as

 = Cu

and the spectral component corresponding to this eigenvector can be re-

moved as

C  C - uuT .

Repeating the procedure converges to the eigenvector corresponding to the next largest eigenvalue and so on. We use a previously fixed number of iterations for this method as experiments have shown that approximate whitening is enough to take meaningful optimization steps. In practice, a relatively small number of iterations results in acceptable whitening for most non-degenerate cases.

5

Data: covariance matrix C, number of iterations Niter, data dimension d
Result: whitening matrix W W  {0}d×d

for i = 0; i < d; i++ do

Sample r  U [-1, 1]d

for j = 0; j < Niter; j++ do

r

Cr Cr

end

  Cr

C  C - rrT

wi·

=

1 rT


end

return W Algorithm 1: Constructing W by power iteration

Note that in algorithm 1 each operation is differentiable with respect to C.

5 Gradient-based Slow Feature Analysis

The key idea for gradient-based SFA is that a whitening layer can be applied subsequently to any differentiable architecture (such as deep neural networks) to enforce outputs that approximately obey the SFA constraints while still being a differentiable architecture. As such, it can be trained using gradient-descent-like training procedures allowing for hierarchical architectures where every parameter is modified iteratively towards optimizing a global slowness objective as opposed to assuming a local-to-global slowness as in HSFA. To formalize, if

g~ : RN×d  RN×e

is a differentiable function approximator parameterized by  such as a neural

network and

W : RN×e  RN×e

denotes the approximate whitening procedure, then

g = W  g~ : RN×d  RN×e

is an approximator whose outputs approximately obey the SFA constraints. As a loss function, we define a general loss as

1 L(X) = N

wij gi - gj 2

ij

(2)

with gi being the i-th row of g(X) and wij being the strength of the connection between two points xi and xj similar to weights in spectral graph embeddings.

6

For optimizing slowness, we define the weight as
wij = i,j+1
with  being the Kronecker delta. This connects consecutive steps in the timeseries and disconnects the others 1.
The approximator can then be trained to minimize eq. 2 by following the negative gradient estimate of L with respect to , -L for (mini-)batches of data as in any gradient-descent procedure. In our experiments, we used the ADAM optimizer [40] with Nesterov-accelerated momentum [41] to train the approximator. Learning rate and additional hyperparameters were left at default values as implemented in the popular Keras-package [42].

6 Experiments

6.1 Synthetic Trigonometric Data
To show the general feasibility of this approach, we first show that PowerSFA finds near-optimal solutions in the linear case for synthetic data. Since standard SFA is a linear method that relies on (a) non-linear basis function expansion and (b) hierarchical processing to induce non-linearity, this means that PowerSFA could hypothetically be used a similar fashion (even though a gradient-based approach allows for more complex models in a natural way).
The data is generated by trigonometric polynomials of degree N as:

N
x(t) = t + n cos (nt) + n sin (nt)
n=1

with x, , ,   RD, coefficients in, in  N (0, 1). A noise term it  N (0, 0.01) is added for numerical stability when computing the closed form

solution.

We

implemented

a

temporal

step-size

of

2 100

,

and

generate

T

=

5000

steps

with maximum degree N = 30 and dimension D = 100. The data is whitened

with Niter = 60.

Figure 1 shows the extracted features for different variants of SFA. If the

slowness loss is optimized without any constraints on the output features, the

model finds the optimal solution to collapse all signals to a constant ( = 0),

while if unit variance is enforced only the features with slowness very close to the

smallest  are extracted multiple times and thus the representation becomes

highly redundant. When using the approximate whitening, the quality of the so-

lutions if comparable to the closed form solution gained by solving a generalized

eigenvalue problem. The ordering was achieved for representational purposes

1Note that this is not exactly the same as the SFA objective as it does not include an ordering of the features, but instead might rather be called a slow subspace loss. Any rotation of the data obeys the constraints and any optimal solution is just a rotated version of the solution of an ordered loss.

7

Figure 1: First five features (100 steps) learned by a linear model in different settings. From left to right: 1) only slowness loss optimized, no constraints, 2) slowness loss with enforced unit variance, 3) slowness loss with enforced whitening, 4) closed form solution. The -values indicate slowness (lower is slower).
by weighting the output features with a monotonically decreasing weight. In Figure 2, the covariance matrices of the extracted signals are visualized.
Figure 2: Covariance matrices for five outputs in different settings. 1) slowness loss without constraints leads to nearly constant signals with (co)variances close to zero, 2) unit variance enforced leads to highly correlated output signals, 3 & 4) (approximate) whitening and the closed form solution show fully decorrelated output signals (identity covariance).
Note that it is not a sensible approach to use PowerSFA to optimize a linear model for such low-dimensional data as the closed-form solution is easily attainable. For this reason, this experiment should be understood as a proof-ofconcept and a general demonstration of applicability rather than a recommendation for a use-case.
8

6.2 Pose Estimation from Visual Data
. Figure 3: Three samples from the anvil -dataset in different x, y positions and rotational angle .
SFA can be used to extract slowly varying underlying causes from highdimensional data, such as object position, identity, and rotation from visual simulations. In [12], textured three-dimensional fish-objects that change in xand y-position as well as in-depth rotation  have been used and it has been shown that HSFA features encode position and angle well. Since the code used to generate the stimulus data was outdated and could not be executed anymore, we re-implemented a similar scenario with a 3D model of an anvil (Figure 3, [43]) using the random walk procedure described in [12] to generate the configurations.
(a) (b) Figure 4: Spread of the predicted x- and y-position of the anvil -dataset for a trained network (4a) and a randomly initialized network (4b). RMSE for the trained network on unseen data are 9% and 8% for x- and y- position respectively. RMSE for the random network on unseen data are 21% and 21%.
A current limitation of our model (cf. section 7) is that it does not scale well in the number of output features, rendering the relatively high number of 512 output features used in [12] infeasible. We used an output dimension of 10 with a comparable architecture that replaces quadratic expansion by ELUnonlinearities (a Keras-summary is given in Appendix A). Due to the low
9

number of output features, we were not able to successfully extract the rotational angle .
In line with the original publication, we computed linear regression to predict cos(x) and cos(y) and used the inverse transformation to extract the object position. The results are shown in Figure 4 and are comparable to those presented in [12] as they were able to achieve a RMSE of 9% for x-position and 7% for y-position.
6.3 NORB
While gradient-based SFA is the main contribution of this work, previous work on generalizing SFA [30] has shown that the SFA optimization problem is strongly related to a special case of the problem solved by a more general spectral embedding method, i.e., Laplacian Eigenmaps [8], or, with small differences, graph-based SFA [13]. For this reason, we defined a more general loss function in equation 2.
Figure 5: The embedded object from the NORB dataset. Samples differ in azimuth, elevation and lighting.
In fact, weights wij can be defined between any two points xi and xj of a given dataset, not just consecutive ones, thus allowing to optimize neighborhoodrespecting embeddings for general graphs. This is similar (but not fully equivalent) to spectral embeddings on graph data, as used in algorithms such as Laplacian eigenmaps. Our approach exhibits four significant differences:
1. It does not find ordered features, but a rotation of an optimal embedding, 2. the found solution should be assumed to be only locally optimal (or a
saddle point), 3. it does ignore the graph (ir)regularity when enforcing the orthogonality
constraint, and 4. it allows a natural and scalable way to embed unseen points. Computing an out-of-sample embedding requires the same forward-pass through the differentiable architecture as the training embeddings. In particular, its complexity does not scale with the number of observed points used for training
10

as is usually the case in other approximations, such as Nyström approximation [34].

(a) (b)

(c) (d)

Figure 6: Cylindrical embedding of NORB plane with azimuth colored. 6a and 6b show the embedded training data from the front and the side of the cylinder respectively, while 6c and 6d show the test data for the same configuration. Circumference of the cylinder encodes the rotation of the plane.

(a) (b)

(c) (d)

Figure 7: Cylindrical embedding of NORB plane with elevation colored. 7a and 7b show the embedded training data from the front and the side of the cylinder respectively, while 7c and 7d show the test data for the same configuration. Height on the cylinder encodes the photograph's elevation angle.

We demonstrate the usefulness of such an approach by embedding an object of the NORB dataset [35], a collection of photographs of toys taken at different elevations, azimuths, and under different lighting conditions. Following the experimental procedure of [36], we embedded photographs of a toy plane (Figure 5) in 972 configurations (18 azimuths × 6 lighting conditions × 9 elevations angles) that were randomly split into a train- and test-set of sizes 660 and 312 respectively and the connection weights wij were chosen as 1 if xi and xj differed only in one step either in rotation (i.e., one azimuth) or elevation (i.e., one level) and 0 otherwise. The weights were independent of lighting condition.
The convolutional neural network architecture used was the MobileNet [44] scaled with  = 0.5.
Figures 6 and 7 show the three-dimensional embedding that was found in this setting. The data was embedded in a cylindrical shape in which the circumference encodes the rotation angle of the embedded object and the length along the cylinder encodes the elevation configuration of the object for the train-set

11

and in the out-of-sample case of the test-set. In [36], a similar cylindrical encoding was found, however, their results exhibit a more clean-cut embedding. We assume that this is due to DrLIM's maximization of distance for dissimilar samples that our model does not implement.
7 Discussion
We propose a new way of extracting informative slow features from quickly varying inputs based on differentiable whitening of processed batches of the input data. To experimentally show the feasibility of the method, we trained a linear model to extract slowly varying output signals from synthetic time-series by gradient-descent and showed that the differentiable whitening ensures informativeness of the extracted features when optimizing a slowness loss function. Furthermore, the features corresponded to those found by closed-form SFA.
To show applicability to visual time-series data, we trained a convolutional neural network on an input stream of an object randomly rotating in-depth and moving in a 2-dimensional plane. Gradient-based SFA preserves the position of the object in a low-dimensional representation as does a hierarchical version of closed-form SFA. The rotation was not extracted successfully due to computational constraints in the naive implementation for a large number of output features (cf. 7).
In an experiment on a non-time-series image dataset on which symmetric similarity relations between the data can be defined, we show that a generalization of gradient-based SFA in the spirit of graph-based SFA is able to find a low-dimensional representation that preserves and disentangles the configuration parameters used to define the similarity, in this case, azimuth and elevation of a photographed toy. This representation generalizes well to previously unseen configurations of the object.
While the algorithm is still in a prototypical state, the proof-of-concept results presented in this paper show promise for gradient-based SFA by differentiable whitening to extract meaningful representations for goal-oriented learning while leveraging the expressive power of modern architectures, such as convolutional neural networks. In particular, differentiable whitening ensures non-redundancy of the output features.
More research has to be dedicated to explore the computational limitations of this method and possibly lower the complexity for a larger number of output features. Furthermore, closed-form SFA has a strong theoretical framework describing the optimal responses in an idealized setting. At this point, we are unclear how well this framework translates to the proposed method of slowness extraction as the used models typically suffer from local optima when being iteratively trained.
One limitation of PowerSFA is that it currently does not scale favorably in the number of output features e. We see two main reasons for this: the necessary batch size to get a meaningful estimate of the batch-covariance estimate and its calculation. The latter is due to the complexity of a naive Re×Nbatch · RNbatch×e
12

matrix-multiplication being O(Nbatche2), while the former is due to the whitening procedure expecting a covariance matrix of full rank and thus Nbatch  e samples.
To reduce the lower bound on the batch-size at batch t, a convex mixture with the covariance matrix of the previous batch might be applicable:
Ct = (1 - )C + Ct-1
Note, that only the current batch's covariance matrix C is considered parameterdependent and allows to propagate a gradient for training. Thus, large values for  might cause a significant bias in the gradient-estimate. This has not been implemented in the proof-of-concept algorithm for this paper.
References
[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.
[2] R. S. Sutton and A. Barto, Reinforcement Learning: An Introduction. MIT Press, 1998.
[3] H. Bourlard and Y. Kamp, "Auto-association by multilayer perceptrons and singular value decomposition," Biological Cybernetics, vol. 59, pp. 291­294, Sep 1988.
[4] P. Comon, "Independent component analysis, a new concept?," Signal Processing, vol. 36, no. 3, pp. 287 ­ 314, 1994. Higher Order Statistics.
[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," in Advances in Neural Information Processing Systems 27 (Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, eds.), pp. 2672­2680, Curran Associates, Inc., 2014.
[6] D. P. Kingma and M. Welling, "Auto-Encoding Variational Bayes," ArXiv e-prints, Dec. 2013.
[7] S. T. Roweis and L. K. Saul, "Nonlinear dimensionality reduction by locally linear embedding," SCIENCE, vol. 290, pp. 2323­2326, 2000.
[8] M. Belkin and P. Niyogi, "Laplacian eigenmaps for dimensionality reduction and data representation," Neural Comput., vol. 15, pp. 1373­1396, June 2003.
[9] L. Wiskott and T. Sejnowski, "Slow feature analysis: unsupervised learning of invariances.," Neural Computation, vol. 14, no. 4, pp. 715­770, 2002.
13

[10] Y. Bengio and J. S. Bergstra, "Slow, decorrelated features for pretraining complex cell-like networks," in Advances in Neural Information Processing Systems 22 (Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, eds.), pp. 99­107, Curran Associates, Inc., 2009.
[11] M. Franzius, H. Sprekeler, and L. Wiskott, "Slowness and sparseness lead to place, head-direction, and spatial-view cells," PLOS Computational Biology, vol. 3, pp. 1­18, 08 2007.
[12] M. Franzius, N. Wilbert, and L. Wiskott, "Invariant object recognition and pose estimation with slow feature analysis," Neural Computation, vol. 23, no. 9, pp. 2289­2323, 2011. PMID: 21671784.
[13] A. N. Escalante-B. and L. Wiskott, "How to solve classification and regression problems on high-dimensional data with a supervised extension of slow feature analysis," J. Mach. Learn. Res., vol. 14, pp. 3683­3719, Dec. 2013.
[14] A. N. Escalante-B., Extensions of Hierarchical Slow Feature Analysis for Efficient Classification and Regression on High-Dimensional Data. PhD thesis, Ruhr-University Bochum, 2017.
[15] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein, "On the Expressive Power of Deep Neural Networks," ArXiv e-prints, June 2016.
[16] F. Chollet, "Xception: Deep learning with depthwise separable convolutions," CoRR, vol. abs/1610.02357, 2016.
[17] B. Schölkopf and A. J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. Cambridge, MA, USA: MIT Press, 2001.
[18] A. Bray and D. Martinez, "Kernel-based extraction of slow features: complex cells learn disparity and translation invariance from natural images," in NIPS. Vol. 15., 2002.
[19] W. Böhmer, S. Grünewälder, H. Nickisch, and K. Obermayer, "Regularized sparse kernel slow feature analysis," in Machine Learning and Knowledge Discovery in Databases (D. Gunopulos, T. Hofmann, D. Malerba, and M. Vazirgiannis, eds.), (Berlin, Heidelberg), pp. 235­248, Springer Berlin Heidelberg, 2011.
[20] I. Steinwart, "Sparseness of support vector machines," J. Mach. Learn. Res., vol. 4, pp. 1071­1105, Dec. 2003.
[21] Y. Bengio, O. Delalleau, N. L. Roux, J.-F. Paiement, P. Vincent, and M. Ouimet, "Learning eigenfunctions links spectral embedding and kernel pca," Neural Computation, vol. 16, 2004.
[22] A. N. Escalante-B. and L. Wiskott, "Improved graph-based SFA: information preservation complements the slowness principle," CoRR, vol. abs/1601.03945, 2016.
14

[23] Y. Bengio, A. Courville, and P. Vincent, "Representation learning: A review and new perspectives," IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 1798­1828, 2013.
[24] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun, "Unsupervised feature learning from temporal data," arXiv preprint arXiv:1504.02518, 2015.
[25] R. Goroshin, M. F. Mathieu, and Y. LeCun, "Learning to linearize under uncertainty," in Advances in Neural Information Processing Systems, pp. 1234­1242, 2015.
[26] D. Jayaraman and K. Grauman, "Slow and steady feature analysis: higher order temporal coherence in video," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3852­3861, 2016.
[27] H. Mobahi, R. Collobert, and J. Weston, "Deep learning from temporal coherence in video," in Proceedings of the 26th Annual International Conference on Machine Learning, pp. 737­744, ACM, 2009.
[28] A. Jansen, M. Plakal, R. Pandya, D. P. Ellis, S. Hershey, J. Liu, R. C. Moore, and R. A. Saurous, "Unsupervised learning of semantic audio representations," arXiv preprint arXiv:1711.02209, 2017.
[29] X. Wang and A. Gupta, "Unsupervised learning of visual representations using videos," in Proceedings of the IEEE International Conference on Computer Vision, pp. 2794­2802, 2015.
[30] H. Sprekeler, "On the Relation of Slow Feature Analysis and Laplacian Eigenmaps," Neural Computation, vol. 23, pp. 3287­3302, Dec. 2011.
[31] D. Pfau, S. Petersen, A. Agarwal, D. Barrett, and K. Stachenfeld, "Spectral Inference Networks: Unifying Spectral Methods With Deep Learning," ArXiv e-prints, June 2018.
[32] U. Shaham, K. Stanton, H. Li, B. Nadler, R. Basri, and Y. Kluger, "SpectralNet: Spectral Clustering using Deep Neural Networks," ArXiv e-prints, Jan. 2018.
[33] B. Schölkopf, A. Smola, E. Smola, and K.-R. Müller, "Nonlinear component analysis as a kernel eigenvalue problem," Neural Computation, vol. 10, pp. 1299­1319, 1998.
[34] C. K. I. Williams and M. Seeger, "Using the nyström method to speed up kernel machines," in Advances in Neural Information Processing Systems 13 (T. K. Leen, T. G. Dietterich, and V. Tresp, eds.), pp. 682­688, MIT Press, 2001.
15

[35] Y. LeCun, F. J. Huang, and L. Bottou, "Learning methods for generic object recognition with invariance to pose and lighting," in Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR'04, (Washington, DC, USA), pp. 97­104, IEEE Computer Society, 2004.
[36] R. Hadsell, S. Chopra, and Y. Lecun, "Dimensionality reduction by learning an invariant mapping," in In Proc. Computer Vision and Pattern Recognition Conference (CVPR'06, IEEE Press, 2006.
[37] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah, "Signature verification using a "siamese" time delay neural network," in Proceedings of the 6th International Conference on Neural Information Processing Systems, NIPS'93, (San Francisco, CA, USA), pp. 737­744, Morgan Kaufmann Publishers Inc., 1993.
[38] T. Zito, N. Wilbert, L. Wiskott, and P. Berkes, "Modular toolkit for data processing (mdp): a python data processing framework," Frontiers in Neuroinformatics, vol. 2, p. 8, 2008.
[39] P. Berkes and L. Wiskott, "Slow feature analysis yields a rich repertoire of complex cell properties," Journal of Vision, vol. 5, no. 6, 2005.
[40] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," CoRR, vol. abs/1412.6980, 2014.
[41] T. Dozat, "Incorporating nesterov momentum into adam." http://cs229.stanford.edu/proj2015/054_report.pdf, 2015.
[42] F. Chollet et al., "Keras." https://github.com/fchollet/keras, 2015. [43] Pino4et, "Anvil lowpoly." https://www.turbosquid.com/FullPreview/Index.cfm/ID/1067450,
2016. [Online, Royalty Free 3D Object]. [44] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen, "Inverted
residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation," CoRR, vol. abs/1801.04381, 2018.
16

Appendices

A Keras Description (HSFA-like Architecture)

Layer (type)

Output Shape

Param #

=================================================================

input_1 (InputLayer)

(None, 156, 156, 1)

0

_________________________________________________________________

conv2d_1 (Conv2D)

(None, 147, 147, 32)

3232

_________________________________________________________________

activation_1 (Activation) (None, 147, 147, 32)

0

_________________________________________________________________

dropout_1 (Dropout)

(None, 147, 147, 32)

0

_________________________________________________________________

conv2d_2 (Conv2D)

(None, 28, 28, 32)

102432

_________________________________________________________________

activation_2 (Activation) (None, 28, 28, 32)

0

_________________________________________________________________

dropout_2 (Dropout)

(None, 28, 28, 32)

0

_________________________________________________________________

conv2d_3 (Conv2D)

(None, 25, 25, 32)

16416

_________________________________________________________________

activation_3 (Activation) (None, 25, 25, 32)

0

_________________________________________________________________

dropout_3 (Dropout)

(None, 25, 25, 32)

0

_________________________________________________________________

conv2d_4 (Conv2D)

(None, 11, 11, 32)

16416

_________________________________________________________________

activation_4 (Activation) (None, 11, 11, 32)

0

_________________________________________________________________

dropout_4 (Dropout)

(None, 11, 11, 32)

0

_________________________________________________________________

conv2d_5 (Conv2D)

(None, 8, 8, 32)

16416

_________________________________________________________________

activation_5 (Activation) (None, 8, 8, 32)

0

_________________________________________________________________

dropout_5 (Dropout)

(None, 8, 8, 32)

0

_________________________________________________________________

conv2d_6 (Conv2D)

(None, 3, 3, 32)

16416

_________________________________________________________________

activation_6 (Activation) (None, 3, 3, 32)

0

_________________________________________________________________

dropout_6 (Dropout)

(None, 3, 3, 32)

0

_________________________________________________________________

flatten_1 (Flatten)

(None, 288)

0

_________________________________________________________________

dense_1 (Dense)

(None, 10)

2890

_________________________________________________________________

power_whitening_1 (PowerWhit (None, 10)

0

=================================================================

Total params: 174,218 Trainable params: 174,218

17

Non-trainable params: 0

_________________________________________________________________

