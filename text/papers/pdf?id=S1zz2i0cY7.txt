Under review as a conference paper at ICLR 2019

INTEGER NETWORKS FOR DATA COMPRESSION WITH LATENT-VARIABLE MODELS
Anonymous authors Paper under double-blind review
ABSTRACT
We consider the problem of using variational latent-variable models for data compression. For such models to produce a compressed binary sequence, which is the universal data representation in a digital world, the latent representation needs to be subjected to entropy coding. Range coding as an entropy coding technique is optimal, but it can fail catastrophically if the computation of the prior differs even slightly between the sending and the receiving side. Unfortunately, this is a common scenario when floating point math is used and the sender and receiver operate on different hardware or software platforms, as numerical round-off is often platform dependent. We propose using integer networks as a universal solution to this problem, and demonstrate that they enable reliable cross-platform encoding and decoding of images using variational models.

1 INTRODUCTION

The task of information transmission in today's world is largely divided into two separate endeavors:

source coding, or the representation of data (such as audio or images) as sequences of bits, and chan-

nel coding, representing sequences of bits as analog signals on imperfect, physical channels such

as radio waves (Cover and Thomas, 2006). This decoupling has substantial benefits, as the binary

representations of arbitrary data can be seamlessly transmitted over arbitrary physical channels by

only changing the underlying channel code, rather than having to design a new code for every pos-

sible combination of data source and physical channel. Hence, the universal representation of any

compressed data today is the binary channel, a representation which consists of a variable number

of

binary

symbols,

each

with

probability

1 2

,

and

no

noise

(i.e.

uncertainty).

As a latent representation, the binary channel unfortunately is a severe restriction compared to the richness of latent representations defined by many variational latent-variable models in the literature (e.g., Kingma and Welling, 2014; Sønderby et al., 2016; van den Oord et al., 2017), and in particular

models targeted at data compression (Theis et al., 2017; Ágústsson et al., 2017; Ballé et al., 2018).
Variational latent-variable models such as VAEs (Kingma and Welling, 2014) consist of an encoder model distribution e(y | x) bringing the data x into a latent representation y, and a decoder model distribution d(x | y), which represents the data likelihood conditioned on the latents. Given an encoder e, we observe the marginal distribution of latents m(y) = Ex[e(y | x)], where the expectation runs over the (unknown) data distribution. The prior p(y) is a variational estimate of the marginal
(Alemi et al., 2018).

By choosing the parametric forms of these distributions and the training objective appropriately, many such models succeed in representing relevant information in the data they are trained for quite compactly (i.e., with a small expected Kullback­Leibler (KL) divergence between the encoder and the prior, Ex DKL[e p]), and so may be called compressive in a sense. However, not all of them can be directly used for practical data compression, as the representation needs to be further converted into binary (entropy encoded). This conversion is typically performed by range coding, or arithmetic coding (Rissanen and Langdon, 1981). Range coding is asymptotically optimal: the length of the binary sequence quickly converges to the expected KL divergence in bits, for reasonably large sequences (such as, for one image). For this to hold, the following requirements must be satisfied:

1

Under review as a conference paper at ICLR 2019

Figure 1: The same image, decoded with a model computing the prior using integer arithmetic (left), and the same model using floating point arithmetic (right). The image was decoded correctly, beginning in the top-left corner, until floating point round-off error caused a small discrepancy between the sender's and the receiver's copy of the prior, at which point the error propagated catastrophically.

· The representation must be discrete-valued, i.e. have a finite number of states, and be noiseless ­ i.e. the conditional entropy of the encoder must be zero:

H[e] = Ex Eye[- log e(y | x)] = 0.

(1)

· All scalar elements of the representation y must be brought into a total ordering, and the prior needs to be written using the chain rule of calculus (as a product of conditionals), as the algorithm can only encode or decode one scalar random variable at a time.
· Both sides of the binary channel (i.e. sender and receiver) must be able to evaluate the prior, and they must have identical instances of it.

The latter point is crucial, as range coding is extremely sensitive to differences in p between sender and receiver ­ so sensitive, in fact, that even small perturbations due to floating point round-off error can lead to catastrophic error propagation. Unfortunately, numerical round-off is highly platform dependent, and in typical data compression applications, sender and receiver may well employ different hardware or software platforms. Round-off error may even be non-deterministic on one and the same computer. Figure 1 illustrates a decoding failure in a model which computes p using floating point math, caused by such computational non-determinism in sender vs. receiver. Recently, latent-variable models have been explored that employ artificial neural networks (ANNs) to compute hierarchical or autoregressive priors (Sønderby et al., 2016; van den Oord et al., 2017), including some of the best-performing learned image compression models (Ballé et al., 2018; Minnen et al., 2018; Klopp et al., 2018). Because ANNs are typically based on floating point math, these methods are vulnerable to catastrophic failures when deployed on heterogeneous platforms.
To address this problem, and enable use of powerful learned variational models for real-world data compression, we propose to use integer arithmetic in these ANNs, as floating-point arithmetic cannot presently be made deterministic across arbitrary platforms. We formulate a type of quantized neural network we call integer networks, which are specifically targeted at generative and compression models, and at preventing computational non-determinism in computation of the prior. Because full determinism is a feature of many existing, widely used image and video compression methods, we also consider using integer networks end to end for computing the representation itself.

2 INTEGER NEURAL NETWORKS

ANNs are typically composite functions that alternate between linear and elementwise nonlinear operations. One linear operation followed by a nonlinearity is considered one layer of the network. To ensure that such a network can be implemented deterministically on a wide variety of hardware platforms, we restrict all the data types to be integral, and all operations to be implemented either with basic arithmetic or lookup tables. Because integer multiplications (including matrix multiplications or convolutions) increase the dynamic range of the output compared to their inputs, we introduce an additional step after each linear operator, where we divide each of its output by a learned parameter.

2

Under review as a conference paper at ICLR 2019

15 7

gQReLU(v) gQtanh(v)

10 3 0
5 -3

0 -5

0

5 10 15 20 v

-7 -40

-20

0 v

20 40

Figure 2: Left: Example nonlinearity implementing a saturating rectifier for 4-bit unsigned integer

outputs, given by gQReLU(v) = max(min(v, 15), 0). This nonlinearity can be implemented deter-

ministically either using a lookup table or simply using a clipping operation. The corresponding

scaled cumulative of a generalized Gaussian with  = 4 used for computing gradients is plotted in

cyan, and other choices of  in gray. Right: Example nonlinearity approximating hyperbolic tan-

gent

for

4-bit

signed

integer

outputs,

given

by

gQtanh(v)

=

Q(7

tanh(

v 15

)).

This

nonlinearity

can

be

implemented deterministically using a lookup table. The corresponding scaled hyperbolic tangent

used for computing gradients is plotted in cyan.

Concretely, we define the relationship between inputs u and outputs w of one layer as:

v = (Hu + b) c, w = g(v).

(2) (3)

In order, the inputs u are subjected to a linear transform H (a matrix multiplication, or a convolution); a bias vector b is added; the result is divided elementwise by a vector c, yielding an intermediate result vector v; and finally, an elementwise nonlinearity g is applied to v.

The activations w and all intermediate results, as well as the parameters H, b, and c are all defined as integers. However, they may use differing number formats. For v to be integral, we define here to perform rounding division (equivalent to division followed by rounding to the nearest integer). In programming languages such as C, this can be implemented with integer operands m, n as

m

n

=

Q(

m n

)

=

(m

+

n

//

2)

//

n,

(4)

where Q rounds to the nearest integer and // is floor division; here, the addition can be folded into the bias b as an optimization. We constrain the linear filter coefficients H and the bias vector b to generally use signed integers, and the scaling vector c to use unsigned integers. We implement the accumulators of the linear transform with larger bit width than the activations and filter coefficients, in order to reflect the potentially increased dynamic range of multiplicative operations. We assume here that the bias and scaling vectors, as well as the intermediate vector v, have the same bit width as the accumulators.

The elementwise nonlinearity g must be saturating on both ends of its domain, because integers can only represent finite number ranges. In order to maximize utility of the dynamic range, we scale nonlinearities such that their range matches the bit width of w, while their domain can be scaled somewhat arbitrarily. Depending on the range of the nonlinearity, the activations w may use a signed or unsigned number format. For instance, a reasonable choice of number formats and nonlinearity would be:

H : 8-bit signed b, v : 32-bit signed (same as accumulator)
c : 32-bit unsigned w : 8-bit unsigned gQReLU(v) = max(min(v, 255), 0)

In this example, the nonlinearity can be implemented with a simple clipping operation. Refer to figure 2, left, for a visualization (for visualization purposes, the figure shows a smaller bit width).

3

Under review as a conference paper at ICLR 2019

Another example is:

H : 4-bit signed

b, v : 16-bit signed (same as accumulator)

c : 16-bit unsigned

w : 4-bit signed

gQtanh(v)

=

Q

7

tanh(

v 15

)

Here, the nonlinearity approximates the hyperbolic tangent, a widely used nonlinearity. It may be best implemented using a lookup table (see figure 2, right, for a visualization). We scale its range to fill the 4-bit signed integer number format of w by multiplying its output with 7. The domain can be scaled somewhat arbitrarily, since v has a larger bit width than w. When it is chosen too small, w may not utilize all integer values, leading to a large quantization error. When it is chosen too large, overflow may occur in v, or the size of the lookup table may grow too large for practical purposes. Therefore, it is best to empirically determine the input scaling. Here, we chose a scaling factor of 15 as an example.

3 TRAINING INTEGER NEURAL NETWORKS

To effectively accumulate small gradient signals, we train the networks entirely using floating point computations, rounded to integers after every computational operation, while the backpropagation is done with full floating point precision. More concretely, we define the integer parameters H, b, and c as functions of their floating point equivalents H , b , and c , respectively:

 Q(h1/s(h1))) 

H = 

...

, 

Q(hN /s(hN ))

b = Q 2K b ,

c = Q 2K r(c ) .

(5)
(6) (7)

Here, we simply rescale each element of b using the bit-width of the kernel K, and round it to the nearest integer. The reparameterization mapping r is borrowed from Ballé (2018) and ensures that values of c are positive while gracefully scaling down gradient magnitudes near zero:

2
r(c ) = max c , 1 + 2 - 2.

(8)

Before rounding the linear filter coefficients in H = [h1, . . . , hN ] , we apply a special rescaling function s to each of its filters h :

s(h ) = max

(-2K-1)-1

min
i

hi,

(2K-1

-

1)-1

max
i

hi,

.

(9)

This maximizes accuracy of the integer approximations by rescaling each filter such that it utilizes the dynamic range of the kernels between -2K-1 and 2K-1 - 1 as much as possible. To prevent division by zero, we ensure the divisor is larger than or equal to a small constant (for example,
= 10-20).

In order to backpropagate gradient signals into the parameters, one cannot simply take gradients of the loss function with respect to H , b , or c , since the rounding function Q has zero gradients almost everywhere. A simple remedy is to replace the derivative of Q with the identity function. Further, we treat the rescaling divisor s as if it were a constant. That is, we compute the derivatives of the loss function with respect to H , b , and c as with the chain rule of calculus, but overriding:

h 1 := ,
h s(h )

b := 2K , b

c := 2K r (c ), c

(10)

where r is the replacement gradient function for r as proposed by Ballé (2018). After training is completed, we compute the integer parameters H, b and c one more time, and from then on use them for evaluation. Note that further reparameterization of the kernels H , such as Sadam (Ballé, 2018), or of the biases b or scaling parameters c , is possible by simply chaining reparameterizations.

4

Under review as a conference paper at ICLR 2019

In addition to rounding the parameters, it is necessary to round the activations. To obtain gradients for the rounding division , we simply substitute the gradient of floating point division. To estimate gradients for the rounded activation functions, we replace their gradient with the corresponding nonrounded activation function, plotted in cyan in figure 2. In the case of QReLU, the gradient of the clipping operation is a box function, which can lead to training getting stuck, since if activations consistently hit one of the bounds, no gradients are propagated back (this is sometimes called the "dead unit" problem). As a remedy, we replace the gradient instead with

gQReLU(v) := exp v

-

2v 2L -

1

-

1



,

(11)

where  =

1 



1 

, and L is the bit width of w. This function corresponds to a scaled generalized

Gaussian probability density with shape parameter . In this context, we can think of  as a tem-

perature parameter that makes the function converge to the gradient of the clipping operation as 

goes to infinity. Although this setting permits an annealing schedule, we simply chose  = 4 and

obtained good results. The integral of this function is plotted in figure 2 (left) in cyan, along with

other choices of  in gray.

4 COMPUTING THE PRIOR WITH INTEGER NETWORKS

Suppose our prior on the latent representation is p(y | z), where z summarizes other latent variables of the representation (it may be empty). To apply range coding, we need to impose a total ordering on the elements of y and write it as a chain of conditionals:

p(y | z) = p(yi | y:i, z),
i

(12)

where y:i denotes the vector of all elements of y preceding the ith. A common assumption is that p is a known distribution, with parameters i computed by an ANN g:

p(y | z) = p(yi | i) with i = g(y:i, z)
i

(13)

We simply propose here to compute g deterministically using an integer network, discretizing the parameters  to a reasonable accuracy. If p(yi | i) itself cannot be computed deterministically, we can precompute all possible values and express it as a lookup table over yi and i.

As an example, consider the prior used in the image compression model proposed by Ballé et al. (2018), which is a modified Gaussian with scale parameters conditioned on another latent variable:

p(y | z) =

N

(0,

i2)



U

(-

1 2

,

1 2

)

(yi)

with 

=

g(z).

i

We reformulate the scale parameters  as:

(14)

i = exp

log(min)

+

log(max )-log(min )

L-1

i

,

(15)

where  = g(z) is computed using an integer network. The last activation function in g is chosen to have integer outputs of L levels in the range [0, L - 1]. Constants min, max, and L determine the discretized selection of scale parameters used in the model. The discretization is chosen to be logarithmic, as this choice minimizes Ex DKL[e p] for a given number of levels.
During training, we can simply backpropagate through this reformulation, and through g as described in the previous section. After training, we precompute all possible values of p as a function of yi and i and form a lookup table, while g is implemented with integer arithmetic.

5 COMPUTING THE REPRESENTATION WITH INTEGER NETWORKS

For certain applications, it can be useful not only to be able to deploy a compression model across heterogenous platforms, but to go even further in also ensuring identical reconstructions of the data across platforms. To this end, it can be attractive to make the entire model robust to non-determinism. To use integer networks in the encoder or decoder, one can use the equivalent construction as in (13):

5

Under review as a conference paper at ICLR 2019

define e or d as a known distribution, with parameters computed by an integer network. To allow the use of range coding, we're especially interested in discrete-valued representations here, such
as studied in van den Oord et al. (2017), Jang et al. (2017), Theis et al. (2017), Ágústsson et al. (2017), and Ballé et al. (2018), among others. These approaches typically employ biased gradient estimators.

Jang et al. (2017) and Ágústsson et al. (2017) are concerned with producing gradients for categorical distributions and vector quantization (VQ), respectively. In both methods, the representation is found by evaluating an ANN followed by an arg max function, while useful gradients are obtained by substituting the arg max with a softmax function. Since arg max can be evaluated deterministically in a platform-independent way, and evaluating a softmax function with rounded inputs is feasible, integer networks can be combined with these models without additional modifications.

Theis et al. (2017) and Ballé et al. (2018) differ mostly in the details of interaction between the encoder and the prior. These two approaches are particularly interesting for image compression, as they scale well: Image compression models are often trained with a rate­distortion objective with a Lagrange parameter , equivalent to  in the -VAE objective (Higgins et al., 2017; Alemi et al., 2018). Depending on the parameter, the latent representation carries vastly different amounts of information, and the optimal number of latent states in turn varies with that. While the number of latent states is a hyperparameter that needs to be chosen ahead of time in the categorical/VQ case, the latter two approaches can extend it as needed during training, because the latent states are organized along the real line. Further, for categorical distributions as well as VQ, the required dimensionality of the function computing the parameters grows linearly with the number of latent states due to their use of the arg max function. In the latter two models, the number of states can grow arbitrarily without increasing the dimensionality of g.

Both Theis et al. (2017) and Ballé et al. (2018) use deterministic encoder distributions (i.e. de-

generating to delta distributions) during evaluation, but replace them with probabilistic versions for

purposes of estimating Ex DKL[e p] during training. Theis et al. (2017) propose to use the following

encoder distribution:

e(y

|

x)

=

U (y

|

Q(g(x))

-

1 2

,

Q(g(x))

+

1 2

),

(16)

where U is the uniform distribution and g is an ANN. They replace the gradient of the quantizer with

the identity. During evaluation, y = Q(g(x)) is used as the representation. Ballé et al. (2018) use

the following distribution during training:

e(y

|

x)

=

U (y

|

g(x)

-

1 2

,

g(x)

+

1 2

),

(17)

which makes y shift-invariant. During evaluation, they determine the representation as y = Q(g(x) - o), where o is a sub-integer offset chosen such that the mode (or, if it cannot be esti-
mated easily, the median) of the distribution is centered on one of the quantization bins.

If g is implemented with integer networks, the latter approach becomes equivalent to the former, because g then inherently computes integer outputs, and this is effectively equivalent to the quantization in (16). However, we've found that training with this construction leads to instabilities, such that the prior distribution never converges to a stable set of parameters. The reason may be that with quantization in e, the marginal m(y) = Ex e(y | x) resembles a piecewise constant function, while the prior p must be forced to be smooth, or Ex DKL[e p] would not yield any useful gradients. Because the prior is a variational approximation of the marginal, this means that the prior must be regularized (which we did not attempt here ­ we used the nonparametric density model described in Ballé et al. (2018)). On the other hand, when using (17) without quantization, the marginal is typically a smooth density, and the prior can approximate it closely without the need for regularization.

As a remedy for the instabilities, we propose the following trick: We simply use (17) during training, but define the last layer of g without a nonlinearity and with floating point division, such that the representation is

e(y

|

x)

=

U (y

|

(H u

+

b)/c

-

1 2

,

(H u

+

b)/c

+

1 2

),

(18)

during training, where u is the input to the last layer and / represents elementwise floating point

division, and

y = Q (Hu + b)/c - o

(19)

during evaluation. This can be rewritten strictly using integer arithmetic as:

y = Hu + b - Q(o c) c,

(20)

6

Under review as a conference paper at ICLR 2019

40 34

32
35 30

PSNR PSNR

30 Ballé et al. (2018) dito, integer prior

0.5 1 bits per pixel

1.5

28 ReLU (128) QReLU (128)
26 QReLU (256)

0.2 0.4 0.6 0.8 1 bits per pixel

1.2

Figure 3: Rate­distortion performance of image compression models with integer priors (left and up is better). Left: performance of Ballé et al. (2018) model vs. the same model with an integer prior. The performance is identical, but the latter can be deployed across different hardware platforms. Right: performance of Ballé (2018) ReLU model with 128 filters per layer vs. the same model, with integer transforms and QReLU activation functions and 128 or 256 filters per layer. The approximation capacity of integer networks is diminished vs. floating point networks, but doubling the number of filters per layer more than compensates for the loss.

compressed on decompressed on
Ballé et al. (2018) dito, integer prior
Ballé et al. (2018) dito, integer prior

CPU 1 CPU 1 CPU 1 CPU 1 GPU 1 GPU 1 CPU 1 GPU 1 CPU 2 GPU 2 CPU 1 GPU 1
Tecnick dataset: 100 RGB images of 1200 × 1200 pixels 0% 71% 54% 66% 63% 41% 0% 0% 0% 0% 0% 0%
CLIC dataset: 2021 RGB images of various pixel sizes 0% 78% 68% 78% 77% 52% 0% 0% 0% 0% 0% 0%

GPU 1 CPU 2
59% 0%
78% 0%

GPU 1 GPU 2
34% 0%
54% 0%

CPU 1: Intel Xeon E5-1650 GPU 1: NVIDIA Titan X (Pascal) CPU 2: Intel Xeon E5-2690 GPU 2: NVIDIA Titan X (Maxwell)

Table 1: Decompression failure rates due to floating point round-off error on Tecnick and CLIC image datasets. When compressing and decompressing on the same CPU platform (first column), the Ballé et al. (2018) model decompresses all images correctly. However, when compressing on a GPU or decompressing on a different platform, a large percentage of the images fail to be decoded correctly. Implementing the prior of the same model using integer networks ensures correct decompression across all tested platforms.

where represents elementwise multiplication, and the rounded product can be folded into the bias b as an optimization. This way, the representation is computed deterministically during evaluation, while during training, the marginal still resembles a smooth function, such that no regularization of the prior is necessary.
6 EXPERIMENTAL RESULTS
In order to assess the efficacy of integer networks to enable platform-independent compression and decompression, we re-implemented the image compression model described in Ballé et al. (2018), which is defined with a hyperprior. We compare the original model with a version in which the network hs computing the prior is replaced with an integer network. We used the same network architectures in terms of number of layers, filters, etc., and the same training parameters as in the original paper. The rate­distortion performance of the model was assessed on the Kodak dataset and is shown in figure 3 (left). The modified model performs identically to the original model, as it maps out the same rate­distortion frontier. However, it is much more robust to cross-platform com-
7

Under review as a conference paper at ICLR 2019
pression and decompression (table 1). We tested compression and decompression on four different platforms (two CPU platforms and two GPU platforms) and two different datasets, Tecnick (Asuni and Giachetti, 2014) and CLIC (Challenge On Learned Image Compression, 2018).1 The original model fails to correctly decompress more than half of the images on average when compression and decompression occurs on different platforms. The modified model brings the failure rate down to 0% in all cases.
It should be noted that the decreased accuracy of integer arithmetic generally leads to a lower approximation capacity than with floating point networks. We found that when implementing the models described in Ballé (2018) using integer networks throughout, the rate­distortion performance decreased (figure 3, right). The loss in approximation capacity can be compensated for by increasing the number of filters per layer. However, note that this may not necessarily increase the size of the model parameters or the runtime, as the storage requirements for integer parameters (kernels, biases, etc.) are lower than for floating point parameters, and integer arithmetic is computationally less complex than floating point arithmetic in general.
7 DISCUSSION
There is a large body of recent research considering quantization of ANNs in a number of applications, mostly image recognition, for a variety of applications. Courbariaux, Bengio, and David (2014) train classification networks on lower precision multiplication. Courbariaux and Bengio (2016) and Rastegari et al. (2016) perform quantization down to bi-level (i.e., 1-bit integers) at inference time to reduce computation in classification networks. More recently, Wu et al. (2018) and others have used quantization during training as well as inference, to reduce computation on gradients as well as activations, and Anonymous (2018) do non-uniform quantization to remove floating point computation, replacing it completely with integer offsets into an integer lookup table.
While the quantization of neural networks is not a new topic, the results from the above techniques focus almost exclusively on classification networks. Denton et al. (2014), Han et al. (2015), and others have demonstrated that these types of networks are particularly robust to capacity reduction. Models used for image compression, like many generative models, are much more sensitive to this, as they tend to underfit. As illustrated in Ballé (2018) and in figure 3 (right), this class of models is much more sensitive to reductions of capacity, both in terms of network size and the expressive power of the activation function. This may explain why our experiments with post-hoc quantization of network activations have never yielded competitive results for this class of model (not shown).
As illustrated in figure 1 and table 1, small floating point inconsistencies in variational latent-variable models can have disastrous effects when employing the models for data compression across different hardware or software platforms. Our approach to network quantization is the first we are aware of which specifically addresses non-deterministic computation, as opposed to computational complexity. It enables a variety of possible variational model architectures and distributions to be effectively used for platform-independent data compression. While we aren't assessing its effects on computational complexity here, it is conceivable that complexity reductions can also be achieved with the same approach; this is a topic for future work.
REFERENCES
Ágústsson, Eiríkur Þór et al. (2017). "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations". In: Advances in Neural Information Processing Systems 30, pp. 1141­1151.
Alemi, Alexander A. et al. (2018). "Fixing a Broken ELBO". In: arXiv e-prints. arXiv: 1711 . 00464.
Anonymous, Under Review (2018). No Multiplication? No Floating Point? No Problem! Training Networks for Efficient Inference.
Asuni, N. and A. Giachetti (2014). "TESTIMAGES: A large-scale archive for testing visual devices and basic image processing algorithms". In: STAG: Smart Tools and Apps for Graphics.
1Reviewer note: Larger scale reliability experiments have been started, but results weren't ready in time for submission. We expect to add these in a later version of the paper.
8

Under review as a conference paper at ICLR 2019
Ballé, Johannes (2018). "Efficient Nonlinear Transforms for Lossy Image Compression". In: Picture Coding Symposium (PCS), 2018.
Ballé, Johannes et al. (2018). "Variational image compression with a scale hyperprior". In: arXiv e-prints. presented at the 6th Int. Conf. on Learning Representations. arXiv: 1802.01436.
Challenge On Learned Image Compression (2018). Mobile and Professional Datasets. Workshop in CVPR 2018. URL: http://www.compression.cc/challenge/.
Courbariaux, Matthieu and Yoshua Bengio (2016). "BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1". In: CoRR abs/1602.02830. arXiv: 1602. 02830. URL: http://arxiv.org/abs/1602.02830.
Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David (2014). "Low precision arithmetic for deep learning". In: CoRR abs/1412.7024. arXiv: 1412 . 7024. URL: http : / / arxiv . org/abs/1412.7024.
Cover, Thomas M. and Joy A. Thomas (2006). Elements of Information Theory. 2nd ed. Wiley. Denton, Emily et al. (2014). "Exploiting Linear Structure Within Convolutional Networks for Effi-
cient Evaluation". In: CoRR abs/1404.0736. arXiv: 1404.0736. URL: http://arxiv.org/ abs/1404.0736. Han, Song, Huizi Mao, and William J. Dally (2015). "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding". In: CoRR abs/1510.00149. arXiv: 1510.00149. URL: http://arxiv.org/abs/1510.00149. Higgins, Irina et al. (2017). "-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework". In: Proc. of Int. Conf. on Learning Representations. URL: https : / / openreview.net/forum?id=Sy2fzU9gl. Jang, Eric, Shixiang Gu, and Ben Poole (2017). "Categorical Reparameterization with GumbelSoftmax". In: Proc. of Int. Conf. on Learning Representations. URL: https://openreview. net/forum?id=rkE3y85ee. Kingma, Diederik P. and Max Welling (2014). "Auto-Encoding Variational Bayes". In: arXiv eprints. presented at the 2nd Int. Conf. on Learning Representations. arXiv: 1312.6114. Klopp, Jan P. et al. (2018). "Learning a Code-Space Predictor by Exploiting Intra-ImageDependencies". In: Proc. of 29th British Machine Vision Conference. Minnen, David, Johannes Ballé, and George Toderici (2018). "Joint Autoregressive and Hierarchical Priors for Learned Image Compression". In: Advances in Neural Information Processing Systems 31. Accepted. Oord, Aäron van den, Oriol Vinyals, and Koray Kavukcuoglu (2017). "Neural Discrete Representation Learning". In: Advances in Neural Information Processing Systems 31. Rastegari, Mohammad et al. (2016). "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks". In: CoRR abs/1603.05279. arXiv: 1603.05279. URL: http:// arxiv.org/abs/1603.05279. Rissanen, Jorma and Glen G. Langdon Jr. (1981). "Universal modeling and coding". In: IEEE Transactions on Information Theory 27.1. DOI: 10.1109/TIT.1981.1056282. Sønderby, Casper Kaae et al. (2016). "Ladder variational autoencoders". In: Advances in Neural Information Processing Systems 29, pp. 3738­3746. Theis, Lucas et al. (2017). "Lossy Image Compression with Compressive Autoencoders". In: arXiv e-prints. presented at the 5th Int. Conf. on Learning Representations. arXiv: 1703.00395. Wu, Shuang et al. (2018). "Training and Inference with Integers in Deep Neural Networks". In: CoRR abs/1802.04680. arXiv: 1802 . 04680. URL: http : / / arxiv . org / abs / 1802 . 04680.
9

