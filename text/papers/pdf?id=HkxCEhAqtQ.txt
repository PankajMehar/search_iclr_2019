Under review as a conference paper at ICLR 2019
ACCELERATED GRADIENT FLOW FOR PROBABILITY DISTRIBUTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
This paper presents a methodology and numerical algorithms for constructing accelerated gradient flows on the space of probability distributions. In particular, we extend the recent variational formulation of accelerated gradient methods in Wibisono et al. (2016) from vector valued variables to probability distributions. The variational problem is modeled as a mean-field optimal control problem. The maximum principle of optimal control theory is used to derive Hamilton's equations for the optimal gradient flow. The Hamilton's equation are shown to achieve the accelerated form of density transport from any initial probability distribution to a target probability distribution. A quantitative estimate on the asymptotic convergence rate is provided based on a Lyapunov function construction, when the objective functional is displacement convex. Two numerical approximations are presented to implement the Hamilton's equations as a system of N interacting particles. The continuous limit of the Nesterov's algorithm is shown to be a special case with N = 1. The algorithm is illustrated with numerical examples.
1 INTRODUCTION
Optimization on the space of probability distributions is important to a number of machine learning models including variational inference (Blei et al., 2017), generative models (Goodfellow et al., 2014; Arjovsky et al., 2017), and policy optimization in reinforcement learning (Sutton et al., 2000). A number of recent studies have considered solution approaches to these problems based upon a construction of gradient flow on the space of probability distributions (Zhang et al., 2018; Liu & Wang, 2016; Frogner & Poggio, 2018; Chizat & Bach, 2018; Richemond & Maginnis, 2017). Such constructions are useful for convergence analysis as well as development of numerical algorithms.
In this paper, we propose a methodology and numerical algorithms that achieve accelerated gradient flows on the space of probability distributions. The proposed numerical algorithms are related to yet distinct from the accelerated stochastic gradient descent (Jain et al., 2017) and Hamiltonian Markov chain Monte-Carlo (MCMC) algorithms (Neal et al., 2011; Cheng et al., 2017). The proposed methodology extends the variational formulation of (Wibisono et al., 2016) from vector valued variables to probability distributions. The original formulation of Wibisono et al. (2016) was used to derive and analyze the convergence properties of a large class of accelerated optimization algorithms, most significant of which is the continuous-time limit of the Nesterov's algorithm (Su et al., 2014). In this paper, the limit is referred to as the Nesterov's ordinary differential equation (ODE).
The extension proposed in our work is based upon a generalization of the formula for the Lagrangian in Wibisono et al. (2016): (i) the kinetic energy term is replaced with the expected value of kinetic
1

Under review as a conference paper at ICLR 2019

energy; and (ii) the potential energy term is replaced with a suitably defined functional on the space of probability distributions. The variational problem is to obtain a trajectory in the space of probability distributions that minimizes the action integral of the Lagrangian.

The variational problem is modeled as a mean-field optimal problem. The maximum principle of the optimal control theory is used to derive the Hamilton's equations which represent the first order optimality conditions. The Hamilton's equations provide a generalization of the Nesterov's ODE to the space of probability distributions. A candidate Lyapunov function is proposed for the convergence analysis of the solution of the Hamilton's equations. In this way, quantitative estimates on convergence rate are obtained for the case when the objective functional is displacement convex (McCann, 1997). Table 1 provides a summary of the relationship between the original variational formulation in Wibisono et al. (2016) and the extension proposed in this paper.

We also consider the important special case when the objective functional is the relative entropy functional D(|) defined with respect to a target probability distribution . In this case, the accelerated gradient flow is shown to be related to the continuous limit of the Hamiltonian MonteCarlo algorithm (Cheng et al., 2017) (Remark 2). The Hamilton's equations are finite-dimensional for the special case when the initial and the target probability distributions are both Gaussian. In this case, the mean evolves according to the Nesterov's ODE. For the general case, the Lyapunov function-based convergence analysis applies when the target distribution is log-concave.

As a final contribution, the proposed methodology is used to obtain a numerical algorithm. The algorithm is an interacting particle system that empirically approximates the distribution with a finite but large number of N particles. The difficult part of this construction is the approximation of the interaction term between particles. For this purpose, two types of approximations are described: (i) Gaussian approximation which is asymptotically (as N  ) exact in Gaussian settings; and (ii) Kernel approximation which is computationally more demanding but asymptotically exact for a general class of distributions.

The outline of the remainder of this paper is as follows: Sec. 2 provides a brief review of the variational formulation in Wibisono et al. (2016). The proposed extension to the space of probability distribution appears in Sec. 3 where the main result is also described. The numerical algorithm appears in Sec. 4. The Appendix contains the details of the proof of the main result.

Notation: The gradient and divergence operators are denoted as  and ·, respectively. With

multiple independent variables, z denotes the gradient with respect to the variable z. The space of

absolutely continuous (with respect to the Lebesgue measure) probability measures on Rd with finite

second moments is denoted by Pac,2(Rd). The Wasserstein gradient and the Gâteaux derivative of a

functional

F

:

Pac,2(Rd)



R

is

denoted

as

F ()

and

F 

()

respectively

(Carmona

&

Delarue,

2017, Ch. 5). The probability distribution of a random variable Z is denoted as Law(Z).

State-space Gradient flow Lagrangian
Lyapunov funct.

Vector

Rd X t = -f (Xt)

et +t

1 2

|e-t

u|2

-

et f (x)

1 2

|x

+

e-t y

-

x¯|2

+et (F () - F ())

Probability distribution

P2(Rd)  dXt = -f (Xt) dt + 2 dBt

et+t E

1 2

|e-t U |2

-

et F~(,

X)

1 2

E[|Xt

+

e-t Yt

-

Tt (Xt)|2]

+et (F () - F ())

Table 1: Summary of the variational formulations for vectors and probability distributions.

2

Under review as a conference paper at ICLR 2019

2 REVIEW OF THE VARIATIONAL FORMULATION OF WIBISONO ET AL. (2016)

The basic problem is to minimize a C1 smooth convex function f of d real variables: minxRd f (x).

The standard form of the gradient descent algorithm for this problem is an ODE:

dXt dt

=

-f (Xt),

t0

(1)

Accelerated forms of this algorithm are obtained based on a variational formulation due to Wibisono

et al. (2016). The formulation is briefly reviewed here using an optimal control formalism. The

Lagrangian L : R+ × Rd × Rd  R is defined as

L(t, x, u) := et+t 1 |e-t u|2 - et f (x) 2

(2)

kinetic energy potential energy

where t  0 is the time, x  Rd is the state, u  Rd is the velocity or control input, and the

time-varying parameters t, t, t satisfy the following scaling conditions: t = log p - log t,

t = p log t + log C, and t = p log t where p  2 and C > 0 are constants.

The variational problem is Minimize
u
Subject to The Hamiltonian function is



J(u) = L(t, Xt, ut) dt

0

dXt dt

=

ut,

X0 = x0

(3)

H(t, x, y, u) = y · u - L(t, x, u) where y  Rd is dual variable and y · u denotes the dot product between vectors y and u.

(4)

According to the Pontryagin's Maximum Principle, the optimal control ut =

arg max H(t, Xt, Yt, v) = et-t Yt. The resulting Hamilton's equations are
v

dXt dt

=

+yH(t, Xt, Yt, ut)

=

et-t Yt,

X0 = x0

(5a)

dYt dt

=

-xH(t, Xt, Yt, ut)

=

-et+t+t f (Xt),

Y0 = y0

(5b)

The system (5) is an example of accelerated gradient descent algorithm. Specifically, if the parameters

t, t, t are defined using p = 2, one obtains the continuous-time limit of the Nesterov's accelerated

algorithm. It is referred to as the Nesterov's ODE in this paper.

For this system, a Lyapunov function is as follows:

1 V (t, x, y) =

x + e-t y - x¯ 2 + et (f (x) - f (x¯))

2

(6)

where x¯  arg minx f (x). It is shown in Wibisono et al. (2016) that upon differentiating along the

solution trajectory,

d dt

V

(t,

Xt,

Yt)



0.

This

yields

the

following

convergence

rate:

f (Xt) - f (x¯)  O(e-t ), t  0

(7)

3 VARIATIONAL FORMULATION FOR PROBABILITY DISTRIBUTIONS

3.1 MOTIVATION AND BACKGROUND

Let F : Pac,2(Rd)  R be a functional on the space of probability distributions. Consider the problem

of minimizing F(). The (Wasserstein) gradient flow with respect to F() is

t t

=

 · (tF(t))

(8)

3

Under review as a conference paper at ICLR 2019

where F() is the Wasserstein gradient of F.

An important example is the relative entropy functional where F() = D(|) :=

Rd

log(

(x)  (x)

)(x)

dx

where





Pac,2(Rd) is referred to as the target distribution.

The gra-

dient

of

relative

entropy

is

given

by

F()

=



log(

 

).

The

gradient

flow

t t

=

- · (t log()) + t

(9)

is the Fokker-Plank equation (Jordan et al., 1998). The gradient flow achieves the density transport

from an initial probability distribution 0 to the target (here, also equilibrium) probability distribution ; and underlies the construction and the analysis of Markov chain Monte-Carlo (MCMC) algorithms. The simplest MCMC algorithm is the Langevin stochastic differential equation (SDE):
 dXt = -f (Xt) dt + 2 dBt, X0  0

where Bt is the standard Brownian motion in Rd.

The main problem of this paper is to construct an accelerated form of the gradient flow (8). The proposed solution is based upon a variational formulation. As tabulated in Table 1, the solution represents a generalization of Wibisono et al. (2016) from its original deterministic finite-dimensional to now probabilistic infinite-dimensional settings.

The variational problem can be expressed in two equivalent forms: (i) The probabilistic form is described next in the main body of the paper; and (ii) The partial differential equation (PDE) form appears in the Appendix. The probabilistic form is stressed here because it represents a direct generalization of the Nesterov's ODE and because it is closer to the numerical algorithm.

3.2 PROBABILISTIC FORM OF THE VARIATIONAL PROBLEM

Consider the stochastic process {Xt}t0 that takes values in Rd and evolves according to:

dXt dt

=

Ut,

X0  0

where the control input {Ut}t0 also takes values in Rd, and 0  Pac,2(Rd) is the probability

distribution of the initial condition X0. It is noted that the randomness here comes only from the

random initial condition.

Suppose the objective functional is of the form F() = F~(, x)(x) dx. The Lagrangian L :

R+ × Rd × Pac,2(Rd) × Rd  R is defined as

L(t, x, , u) := et+t

1 |e-t u|2 - et F~(, x) 2
kinetic energy potential energy

(10)

where the parameters t, t, t are defined exactly the same as in the finite-dimensional case. The

stochastic optimal control problem is:



Minimize Subject to

J(u) = E

L(t, Xt, t, Ut) dt

0

dXt dt

=

Ut,

X0  0

(11)

where t = Law(Xt)  Pac,2(Rd) is the probability density function of the random variable Xt.

The Hamiltonian function H : R+ × Rd × Pac,2(Rd) × Rd × Rd  R for this problem is given by (Carmona & Delarue, 2017, Sec. 6.2.3):

H(t, x, , y, u) := u · y - L(t, x, , u)

(12)

4

Under review as a conference paper at ICLR 2019

where y  Rd is the dual variable.
Remark 1. The variational problem (11) is an example of a mean-field (McKean-Vlasov) optimal control problem. This is because the Lagrangian depends also upon the law of the stochastic process; cf., (Carmona & Delarue, 2017, Ch. 6).

3.3 MAIN RESULT

Theorem 1. Consider the variational problem (11).

(i) The optimal control Ut = et-t Yt where the optimal trajectory {(Xt, Yt)}t0 evolves according to the Hamilton's equations:

dXt dt

= Ut

=

et-t Yt,

X0  0

dYt dt

= -et+t+t F(t)(Xt),

Y0 = 0(X0)

(13a) (13b)

where 0 is any convex function and t := Law(Xt). (ii) Suppose also that the functional F is displacement convex and  is its minimizer. Define

the energy along the optimal trajectory

V

(t)

=

1 2

E[|Xt

+

e-t Yt

-

Tt (Xt)|2]

+

et (F()

-

F())

(14)

where the map Tt : Rd  Rd is the optimal transport map from t to . Suppose also that

the following technical assumption holds:

E[(Xt + e-t Yt - Tt (Xt)) ·

d dt

Tt

(Xt

)]

=

0.

Then

dV dt

(t)



0.

Consequently, the following rate of convergence is obtained along the optimal

trajectory:

F(t) - F()  O(e-t ), t  0

(15)

Proof sketch. The Hamilton's equations are derived using the standard mean-field optimal control theory Carmona & Delarue (2017). The Lyapunov function argument is based upon the variational inequality characterization of a displacement convex function (Ambrosio et al., 2008, Eq. 10.1.7). The detailed proof appears in the Appendix. We expect that the technical assumption is not necessary. This is the subject of the continuing work.

3.4 RELATIVE ENTROPY AS THE FUNCTIONAL

In the remainder of this paper, we assume that the functional F() = D(|) is the relative entropy where   Pac,2(Rd) is a given target probability distribution. In this case the Hamilton's equations
are given by

dXt dt

=

et-t Yt,

X0  0

(16a)

dYt dt

=

-et+t+t (f (Xt) +  log(t(Xt)),

Y0 = 0(X0)

(16b)

where t = Law(Xt) and f = - log(). Moreover, if f is convex (or equivalently  is log-

concave), then F is displacement convex with the unique minimizer at  and the convergence estimate is given by D(t|)  O(e-t ).

Remark 2. The Hamilton's equations (16) with the relative entropy functional is related to the

under-damped Langevin equation (Cheng et al., 2017). The difference is that the deterministic term

 log() in (16) is replaced with a random Brownian motion term in the under-damped Langevin

equation.

5

Under review as a conference paper at ICLR 2019

3.5 QUADRATIC GAUSSIAN CASE

Suppose the initial distribution 0 and the target distribution  are both Gaussian, denoted as

N (m0, 0) and N (x¯, Q), respectively. This is equivalent to the objective function f (x) being

quadratic

of

the

form

f (x)

=

1 2

(x

-

x¯)

Q-1(x - x¯). Therefore, this problem is referred to as the

quadratic Gaussian case. The following Proposition shows that the mean of the stochastic process

(Xt, Yt) evolves according to the Nesterov ODE (5):

Proposition 1. (Quadratic Gaussian case) Consider the variational problem (11) for the quadratic Gaussian case. Then

(i) The stochastic process (Xt, Yt) is Gaussian. The Hamilton's equations are given by:

dXt dt

= et-t Yt,

dYt dt

= -et+t+t (Q-1(Xt - x¯) - -t 1(Xt - mt))

where mt and t are the mean and the covariance of Xt.

(ii) Upon taking the expectation of both sides, and denoting nt := E[Yt]

dmt dt

= et-t nt,

dnt dt

= -et+t+t Q-1(mt - x¯)

f (mt)

which is identical to Nesterov ODE (5).

Proof sketch. Fix t. Consider the resulting pair (Xt, Yt) from (23) and let ~t = Law(Xt). The proof follows by observing that t Gaussian is a fixed-point of the map t  ~t.

4 NUMERICAL ALGORITHM

The proposed numerical algorithm is based upon an interacting particle implementation of the Hamilton's equation (16). Consider a system of N particles {(Xti, Yti)}Ni=1 that evolve according to:

dXti dt

=

et-t Yti,

X0i  0

dYti dt

= -et+t+t (f (Xti) +

It(N)(Xti) ),

interaction term

Y0i = 0(X0i)

The interaction term It(N) is an empirical approximation of the  log(t) term in (16). We propose two types of empirical approximations as follows:

1. Gaussian approximation: Suppose the density is approximated as a Gaussian N (mt, t). In this case,  log(t(x)) = -t-1(x - mt). This motivates the following empirical approximation of the
interaction term:

It(N)(x) = -(tN)-1(x - mt(N))

(18)

where mt(N) := N -1

N i=1

Xti

is

the

empirical

mean

and

t(N )

:=

1 N -1

m(tN)) is the empirical covariance.

N i=1

(Xti

-

m(tN ) )(Xti

-

6

Under review as a conference paper at ICLR 2019

Algorithm 1 Interacting particle implementation of the accelerated gradient flow

Input: 0, 0, N , t0, t, p, C, K
Output: {Xki }Ni=,1K,k=0
Initialize {X0i}Ni=1 i.i.d 0, Y0i = 0(X0i) Compute I0(N)(X0i) with (18) or (19)

for k = 0 to K - 1 do

tk+

1 2

= tk +

1 2

t

Yki+

1 2

= Yki -

1 2

C ptk2p+-121 (f

(Xki

)

+

Ik(N

)(Xki ))t

Xki +1

=

Xki

+

p tpk++112

Ykit

Compute Ik(N+1) (Xki+1) with (18)or (19)

Yki+1

=

Yki+

1 2

-

1 2

C

pt2kp+-121

(f

(Xki +1

)

+

Ik(N+1)

(Xki +1

))t

tk+1

=

tk+

1 2

+

1 2

t

end for

Even though the approximation is asymptotically (as N  ) exact only under the Gaussian assumption, it may be used in a more general settings, particularly when the density t is unimodal. The situation is analogous to the (Bayesian) filtering problem, where an ensemble Kalman filter is used as an approximate solution for non-Gaussian distributions (Evensen, 2003).

2. Kernel approximation: This is based upon the so-called kernel-approximation of the weighted

Laplacian operator (Coifman & Lafon, 2006; Hein et al., 2007). For a C2 function f , the weighted

Laplacian is defined as f

:=

1 



·

(f ).

Denote e(x)

=

x as the coordinate function on

Rd. It is a straightforward calculation to show that  log() = e. This allows one to use the

kernel-approximation of the weighted Laplacian to approximate the interaction term as follows:

It(N)(Xti) = 1

N j=1

k

(Xti, Xtj )(Xj

- Xi)

N j=1

k

(Xti, Xtj )

(19)

where the kernel k (x, y) =  g (x,y) is constructed empirically in terms of the Gaussian

N i=1

g

(y,X i )

kernel g (x, y) = exp(-|x - y|2/(4 )). The positive parameter is referred to as the kernel

bandwidth. It is known that the approximation is asymptotically exact as  0 and N   (Hein

et al., 2007).

The resulting interacting particle algorithm with a Gaussian or kernel approximation of the interaction term is tabulated in Table 1. The symplectic method proposed in (Betancourt et al., 2018) is used to carry out the numerical integration. The algorithm is applied to two examples as described in the following sections.
Remark 3. For the case where there is only one particle ( N = 1), the interaction term is zero and the system (17) reduces to the Nesterov ODE (5).

4.1 GAUSSIAN EXAMPLE
Consider the Gaussian example as described in Sec. 3.5. The simulation results for the scalar (d = 1) case with initial distribution 0 = N (2, 4) and target distribution N (x¯, Q) where x¯ = -5.0 and Q = 0.25 is depicted in Figure 1-(a)-(b). For this simulation, the numerical parameters are as follows: N = 100, 0(x) = 0.5(x - 2), t0 = 1, t = 0.1, p = 2,C = 0.625, and K = 400. The numerical result illustrates the convergence result for the case of Gaussian target distribution.

7

Under review as a conference paper at ICLR 2019

102

100

10 2

10 4

10 6

KL( t| ) O(e t)

t100 101
(a) (b)

t0 t1 t=t0

t=t1

(c)

t2 t=t2

100 10 1 10 2 10 3
100

KL( t| ) O(e t)
t 101
(d)

Figure 1: Simulation result for the Gaussian case (Example 4.1): (a) The time traces of the particles; (b) The KL-divergence as a function of time. Simulation result for the non-Gaussian case (Example 4.2): (c) The time traces of the particles; (d) The KL-divergence as a function of time.

4.2 NON-GAUSSIAN EXAMPLE

This

example

involves

a

non-Gaussian

target

distribution



=

1 2

N

(-m,

2)

+

1 2

N

(m,

2)

which

is a mixture of two one-dimensional Gaussians where m = 2.0 and 2 = 0.8. The simulation results

are depicted in Figure 1-(c)-(d). The numerical parameters are same as in the Example 4.1. The

interaction term is approximated using a kernel approximation with = 0.01. The numerical result

illustrates the performance of the kernel-approximation algorithm for non-Gaussian distributions and

shows the convergence result for the case of non-Gaussian target distribution.

5 CONCLUSION
The main contribution of this paper is to extend the variational formulation of Wibisono et al. (2016) to obtain theoretical results and numerical algorithms for accelerated gradient flow in the space of probability distributions. In continuous-time settings, bounds on convergence rate are derived based on a Lyapunov function argument. Two numerical algorithms based upon an interacting particle representation are presented and illustrated with examples. Analysis of these algorithms in finite-N and finite t is a subject of continuing investigation. As has been the case in finite-dimensional settings, the theoretical framework is expected to be useful in this regard.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2008.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Michael Betancourt, Michael I Jordan, and Ashia C Wilson. On symplectic optimization. arXiv preprint arXiv:1802.03653, 2018.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859­877, 2017.
Rene Carmona and François Delarue. Probabilistic Theory of Mean Field Games with Applications I-II. Springer, 2017.
Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin mcmc: A non-asymptotic analysis. arXiv preprint arXiv:1707.03663, 2017.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
Ronald R Coifman and Stéphane Lafon. Diffusion maps. Applied and computational harmonic analysis, 21(1):5­30, 2006.
Geir Evensen. The ensemble kalman filter: Theoretical formulation and practical implementation. Ocean dynamics, 53(4):343­367, 2003.
Charlie Frogner and Tomaso Poggio. Approximate inference with wasserstein gradient flows. arXiv preprint arXiv:1806.04542, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Matthias Hein, Jean-Yves Audibert, and Ulrike von Luxburg. Graph laplacians and their convergence on random neighborhood graphs. Journal of Machine Learning Research, 8(Jun):1325­1368, 2007.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker­planck equation. SIAM journal on mathematical analysis, 29(1):1­17, 1998.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2378­2386, 2016.
Robert J McCann. A convexity principle for interacting gases. Advances in mathematics, 128(1): 153­179, 1997.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2(11):2, 2011.
9

Under review as a conference paper at ICLR 2019

Pierre H Richemond and Brendan Maginnis. On wasserstein reinforcement learning and the fokkerplanck equation. arXiv preprint arXiv:1712.07185, 2017.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov's accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pp. 2510­2518, 2014.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000.
Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, pp. 201614734, 2016.
Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as wasserstein gradient flows. arXiv preprint arXiv:1808.03030, 2018.

A PDE FORMULATION OF THE VARIATIONAL PROBLEM

An equivalent pde formulation is obtained by considering the stochastic optimal control problem (11)

as a deterministic optimal control problem on the space of the probability distributions. Specifically, the process {t}t0 is a deterministic process that takes values in Pac,2(Rd) and evolves according to the continuity equation

t t

=

- · (tut)

where ut : Rd  Rd is now a time-varying vector field. The Lagrangian L : R+ × Pac,2(Rd) × L2(Rd; Rd)  R is defined as:

L(t, , u) := et+t

1 |e-t u(x)|2(x) dx - et F ()

Rd 2

The optimal control problem is:

Minimize Subject to



L(t, t, ut) dt

0

t t

+



·

(tut)

=

0

(20) (21)

The Hamiltonian function H : R+ × Pac,2(Rd) × C(Rd; R) × L2(Rd; Rd)  R is H(t, , , u) := , u L2() - L(t, , u)
where   C(Rd; R) is the dual variable.

(22)

B RESTATEMENT OF THE MAIN RESULT AND ITS PROOF
We restate Theorem 1 below which now includes the pde formulation as well. Theorem 2. Consider the variational problem (11)-(21).
10

Under review as a conference paper at ICLR 2019

(i) For the probabilistic form (11) of the variational problem, the optimal control Ut = et-t Yt, where the optimal trajectory {(Xt, Yt)}t0 evolves according to the Hamilton's odes:

dXt dt

= Ut

= et-t Yt,

X0  0

dYt dt

= -et+t+t F (t)(Xt),

Y0 = 0(X0)

(23a) (23b)

where 0 is a convex function, and t = Law(Xt). (ii) For the pde form (21) of the variational problem, the optimal control is ut = et-t t(x),
where the optimal trajectory {(t, t)}t0 evolves according to the Hamilton's pdes:

t t

=

- · (t et-t t),

initial condn. 0

ut

t t

= -et-t |t|2 2

- et+t+t F ()

(24a) (24b)

(iii) The solutions of the two forms are equivalent in the following sense:

Law(Xt) = t, Ut = ut(Xt), Yt = t(Xt)

(iv) Suppose additionally that the functional F is displacement convex and  is its minimizer.

Define

V

(t)

=

1 2 E(|Xt

+

e-t Yt

- Tt (Xt)|2)

+

et (F ()

-

F ())

(25)

where the map Tt : Rd  Rd is the optimal transport map from t to . Suppose also that

the following technical assumption holds:

E[(Xt + e-t Yt - Tt (Xt)) ·

d dt

Tt

(Xt

)]

=

0.

Then

dV dt

(t)



0.

Consequently, the following rate of convergence is obtained along the optimal

trajectory

F (t) - F ()  O(e-t ), t  0

Proof. (i) The Hamiltonian function defined in (12) is equal to

H(t, x, , y, u) = y · u - et-t 1 |u|2 + et+tt F~(, x) 2
after inserting the formula for the Lagrangian. According to the maximum principle in probabilis-

tic form for (mean-field) optimal control problems (see (Carmona & Delarue, 2017, Sec. 6.2.3)), the optimal control law Ut = arg minv H(t, Xt, t, Yt, v) = et-t Yt and the Hamilton's equations are

dXt dt

= +yH(t, Xt, t, Yt, Ut) = Ut

= et-t Yt

dYt dt

=

-xH(t, Xt, t, Yt, Ut) - E~[H(t, X~t, t, Y~t, U~t)(Xt)]

where X~t, Y~t, U~t are independent copies of Xt, Yt, Ut. The derivatives

xH(t, x, , y, u) = et+t+t xF~(, x)

H(t, x, , y, u) = et+t+t F~(, x)

It follows that

dYt = -et+t+t dt

xF~(t, Xt) + E~[F~(t, X~t)(Xt)]

= -et+t+t F ()(Xt)

where we used the definition F () = F~(x, )(x) dx and the identity (Carmona & Delarue,

2017, Sec. 5.2.2 Example 3)

F ()(x) = xF~(, x) + F~(, x~)(x)(x~) dx~

11

Under review as a conference paper at ICLR 2019

(ii) The Hamiltonian function defined in (22) is equal to

H(t, , , u) =

(x) · u(x) - 1 et-t |u(x)|2 (x) dx + et+t+t F () 2

after inserting the formula for the Lagrangian. According to the maximum principle for pde
formulation of mean-field optimal control problems (see (Carmona & Delarue, 2017, Sec. 6.2.4)) the optimal control vector field is ut = arg minv H(t, t, t, v) = et-t t and the Hamilton's equations are:

t t

=

H +  (t, t, t, ut)

=

- · (tut )

t t

=

H -  (t, t, t, ut)

=

-(

·

u

-

et -t

1 2

|ut |2

+

et +t +t

F 

(t))

inserting the formula ut = et-t t concludes the result.

(iii) Consider the (t, t) defined from (24). The distribution t is identified with a stochastic

process X~t such that

dX~t dt

= et-t t(X~t) and Law(X~t) = t.

Then define Y~t

= t(X~t).

Taking the time derivative shows that

dY~t dt

=

d dt

t(X~t)

=

2t(X~t)

dX~t dt

+



t t

(Xt)

=

et-t 2t(X~t)t(X~t)

-

et-t 2t(X~t)t(Xt)

-

et+t+t 

F 

(t)(X~t)

=

-et+t+t 

F 

(t)(X~t)

= -et+t+t F (t)(X~t)

with

the

initial

condition

Y~0

=

0(X~0),

where

we

used

the

identity



F 

()

=

F ()

(Car-

mona & Delarue, 2017, Prop. 5.48). Therefore the equations for X~t and Y~t are identical. Hence

one can identify (Xt, Yt) with (X~t, Y~t).

(iv) The energy functional

1 V (t) = E
2

|Xt + e-t Yt - Tt (Xt)|2

+ et (F () - F ())

first term

second term

Then the derivative of the first term is

E (Xt + e-t Yt - Tt (Xt)) · (et-t Yt -  te-t Yt - et+t F (t)(Xt) + (Tt (Xt)))

where (Tt (Xt)) :=

d dt

Tt

(Xt

).

Using

the

scaling

condition

 t

=

et

the derivative of the

first term simplifies to

E (Xt + e-t Yt - Tt (Xt)) · (-et+t F (t)(Xt) + (Tt (Xt)))
Upon using the technical assumption, E[(Xt + e-t Yt - Tt (Xt)) · (Tt (Xt))] = 0 the derivative of the first term simplifies to

E (Xt + e-t Yt - Tt (Xt)) · (-et+t F (t)(Xt))

The derivative of the second term is

d (second
dt

term)

=

tet (F (t)

-

F ())

+

et

d dt F (t)

= et+t (F (t) - F ()) + et E[F (t)(Xt)et-t Yt]

12

Under review as a conference paper at ICLR 2019

where we used the scaling condition t = et and the chain-rule for the Wasserstein gradient (Ambrosio et al., 2008, Ch. 10, E. Chain rule). Adding the derivative of the first and second term yields:

dV (t) = et+t dt

F (t) - F () - E

(Xt - Tt (Xt)) · F (t)(Xt)

which is negative by variational inequality characterization of the displacement convex function

F () (Ambrosio et al., 2008, Eq. 10.1.7).

We expect that the technical assumption can be removed. This is the subject of the continuing work.

13

