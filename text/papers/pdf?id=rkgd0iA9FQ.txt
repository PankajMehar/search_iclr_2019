Under review as a conference paper at ICLR 2019

CONVERGENCE GUARANTEES FOR RMSPROP AND ADAM IN NON-CONVEX OPTIMIZATION AND AN EMPIRICAL COMPARISON TO NESTEROV ACCELERATION
Anonymous authors Paper under double-blind review

ABSTRACT
RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear. Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants. In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways. First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time.
Next we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups. Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter 1. We show that at very high values of the momentum parameter (1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses. On the other hand, NAG can sometimes do better when ADAM's 1 is set to the most commonly used value: 1 = 0.9, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance.
We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates.

1 INTRODUCTION

Many optimization questions arising in machine learning can be cast as a finite sum optimization

problem of the form:

minx f (x) where f (x)

=

1 k

k i=1

fi

(x).

Most neural network problems

also fall under a similar structure where each function fi is typically non-convex. A well-studied

algorithm to solve such problems is Stochastic Gradient Descent (SGD), which uses updates of the

form: xt+1 := xt - f~it (xt), where  is a step size, and f~it is a function chosen randomly from {f1, f2, . . . , fk} at time t. Often in neural networks, "momentum" is added to the SGD update to yield a two-step update process given as: vt+1 = µvt - f~it (xt) followed by xt+1 = xt + vt+1.
This algorithm is typically called the Heavy-Ball (HB) method (or sometimes classical momentum),

with µ > 0 called the momentum parameter (Polyak, 1987). In the context of neural nets, another

variant of SGD that is popular is Nesterov's Accelerated Gradient (NAG), which can also be thought

of as a momentum method (Sutskever et al., 2013), and has updates of the form vt+1 = µvt - f~it (xt + µvt) followed by xt+1 = xt + vt+1 (see Algorithm 1 for more details).

Momentum methods like HB and NAG have been shown to have superior convergence properties compared to gradient descent in the deterministic setting both for convex and non-convex functions (Nesterov, 1983; Polyak, 1987; Zavriev & Kostyuk, 1993; Ochs, 2016; O'Neill & Wright, 2017; Jin et al., 2017). While (to the best of our knowledge) there is no clear theoretical justification in

1

Under review as a conference paper at ICLR 2019
the stochastic case of the benefits of NAG and HB over regular SGD (Yuan et al., 2016; Kidambi et al., 2018; Wiegerinck et al., 1994; Orr & Leen, 1994; Yang et al., 2016; Gadat et al., 2018); in practice, these momentum methods, and in particular NAG, have been repeatedly shown to have good convergence and generalization on a range of neural net problems (Sutskever et al., 2013; Lucas et al., 2018; Kidambi et al., 2018).
The performance of NAG (as well as HB and SGD), however, are typically quite sensitive to the selection of its hyper-parameters: step size, momentum and batch size (Sutskever et al., 2013). Thus, "adaptive gradient" algorithms such as RMSProp (Algorithm 2) (Tieleman & Hinton, 2012) and ADAM (Algorithm 3) (Kingma & Ba, 2014) have become very popular for optimizing deep neural networks (Melis et al., 2017; Xu et al., 2015; Denkowski & Neubig, 2017; Gregor et al., 2015; Radford et al., 2015; Bahar et al., 2017; Kiros et al., 2015). The reason for their widespread popularity seems to be the fact that they are believed to be easier to tune than SGD, NAG or HB. Adaptive gradient methods use as their update direction a vector which is the image under a linear transformation (often called the "diagonal pre-conditioner") constructed out of the history of the gradients, of a linear combination of all the gradients seen till now. It is generally believed that this "pre-conditioning" makes these algorithms much less sensitive to the selection of its hyperparameters. A precursor to these RMSProp and ADAM was outlined in Duchi et al. (2011).
Despite their widespread use in neural net problems, adaptive gradients methods like RMSProp and ADAM lack theoretical justifications in the non-convex setting - even with exact/deterministic gradients (Bernstein et al., 2018). Further, there are also important motivations to study the behavior of these algorithms in the deterministic setting because of usecases where the amount of noise is controlled during optimization, either by using larger batches (Martens & Grosse, 2015; De et al., 2017; Babanezhad et al., 2015) or by employing variance-reducing techniques (Johnson & Zhang, 2013; Defazio et al., 2014).
Further, works like Wilson et al. (2017) and Keskar & Socher (2017) have shown cases where SGD (no momentum) and HB (classical momentum) generalize much better than RMSProp and ADAM with stochastic gradients. Wilson et al. (2017) also show that ADAM generalizes poorly for large enough nets and that RMSProp generalizes better than ADAM on a couple of neural network tasks (most notably in the character-level language modeling task). But in general it's not clear and no heuristics are known to the best of our knowledge to decide whether these insights about relative performances (generalization or training) between algorithms hold for other models or carry over to the full-batch setting.
A summary of our contributions In this work we try to shed some light on the above described open questions about adaptive gradient methods in the following two ways.
· To the best of our knowledge, this work gives the first convergence guarantees for adaptive gradient algorithms in the context of non-convex optimization. We show run-time bounds for (stochastic and deterministic) RMSProp and deterministic ADAM to reach approximate criticality on smooth non-convex functions. Recently, Reddi et al. (2018) have shown in the setting of online convex optimization that there are certain sequences of convex functions where ADAM and RMSprop fail to converge to asymptotically zero average regret. We contrast our findings with Theorem 3 in Reddi et al. (2018). Their counterexample for ADAM is constructed in the stochastic optimization framework and is incomparable to our result about deterministic ADAM. Our proof of convergence to approximate critical points establishes a key conceptual point that for adaptive gradient algorithms one cannot transfer intuitions about convergence from online setups to their more common use case in offline setups.
· Our second contribution is empirical investigation into adaptive gradient methods, where our goals are different from what our theoretical results are probing. We test the convergence and generalization properties of RMSProp and ADAM and we compare their performance against NAG on a variety of autoencoder experiments on MNIST data, in both full and mini-batch settings. In the full-batch setting, we demonstrate that ADAM with very high values of the momentum parameter (1 = 0.99) outperforms carefully tuned NAG and RMSProp, in terms of getting lower training and test losses. We show that as the net size keeps increasing, RMSProp fails to generalize pretty soon. In the mini-batch experiments we see exactly the same behaviour for large enough nets.
2

Under review as a conference paper at ICLR 2019

2 NOTATIONS AND PSEUDOCODES

Firstly we define the smoothness property that we assume in our proofs for all our non-convex objectives. This is a standard assumption used in the optimization literature.
Definition 1. L-smoothness If g : Rd  R is at least once differentiable then we call it L-smooth for some L > 0 if for all x, y  Rd the following inequality holds,

f (y)  f (x) +

f (x), y - x

+

L 2

y-x 2.

We need one more definition that of square-root of diagonal matrices,

Definition 2. Square root of the Penrose inverse If v  Rd and V = diag(v) then we define,

V

-

1 2

:=

iSupport(v)

1 vi

ei

eTi

,

where

{ei}{i=1,...,d}

is

the

standard

basis

of

Rd

Now we list out the pseudocodes used for NAG, RMSProp and ADAM in theory and experiments,

Nesterov's Accelerated Gradient (NAG) Algorithm
Algorithm 1 NAG
1: Input : A step size , momentum µ  [0, 1), and an initial starting point x1  Rd, and we are given query access to a (possibly noisy) oracle for gradients of f : Rd  R.
2: function NAG(x1, , µ) 3: Initialize : v1 = 0 4: for t = 1, 2, . . . do 5: vt+1 = µvt + f (xt) 6: xt+1 = xt - (f (xt) + µvt+1) 7: end for
8: end function

RMSProp Algorithm

Algorithm 2 RMSProp

1: Input : A constant vector Rd 1d  0, parameter 2  [0, 1), step size , initial starting point x1  Rd, and we are given query access to a (possibly noisy) oracle for gradients of f : Rd  R.

2: function RMSPR O P(x1, 2, , ) 3: Initialize : v0 = 0 4: for t = 1, 2, . . . do

5: gt = f (xt) 6: vt = 2vt-1 + (1 - 2)g2t

7: Vt = diag(vt + 1d)

8:

xt+1

=

xt

-

Vt-

1 2

gt

9: end for

10: end function

3

Under review as a conference paper at ICLR 2019

ADAM Algorithm
Algorithm 3 ADAM
1: Input : A constant vector Rd 1d > 0, parameters 1, 2  [0, 1), a sequence of step sizes {t}t=1,2.., initial starting point x1  Rd, and we are given oracle access to the gradients of f : Rd  R.
2: function ADAM(x1, 1, 2, , ) 3: Initialize : m0 = 0, v0 = 0 4: for t = 1, 2, . . . do 5: gt = f (xt) 6: mt = 1mt-1 + (1 - 1)gt 7: vt = 2vt-1 + (1 - 2)gt2 8: Vt = diag(vt)
1 -1
9: xt+1 = xt - t Vt2 + diag(1d) mt
10: end for
11: end function

3 CONVERGENCE GUARANTEES FOR ADAM AND RMSPROP

Previously it has been shown in Rangamani et al. (2017) that mini-batch RMSProp can off-the-shelf do autoencoding on depth 2 autoencoders trained on MNIST data while similar results using nonadaptive gradient descent methods requires much tuning of the step-size schedule. Here we give the first result about convergence to criticality for stochastic RMSProp.

Theorem 3.1. Stochastic RMSProp converges to criticality (Proof in subsection A.1) Let

f

:

Rd



R be L-smooth and be of the form f

=

1 k

k i=1

fi

s.t.

(a)

each

fi

is

at

least

once

differentiable, (b) f <  is an upperbound on the norm of the gradients of fi and (c) f has

a minimizer, i.e., there exists x such that f (x) = minxRd f (x). Let the gradient oracle be s.t when invoked at some xt  Rd it uniformly at random picks it  {1, 2, .., k} and returns,

fit (xt) = gt. Then corresponding to any ,  > 0 and a starting point x1 for Algorithm 2, we

can define, T



1
4

2Lf2 (f2 +)(f (x1)-f (x)) (1-2 )

s.t. we are guaranteed that the iterates of Algorithm

2 using a constant step-length of,  = 1
T

2(1-2)(f (x1)-f (x)) f2 L

will

find

an

-critical point in at

most T steps in the sense that, mint=1,2...,T E[ f (xt) 2]  2.

Next we analyze deterministic RMSProp which corresponds to the full-batch RMSProp experiments in Section 5.3.

Theorem 3.2. Convergence of deterministic RMSProp - the version with standard speeds (Proof in subsection A.2) Let f : Rd  R be L-smooth and let  <  be an upperbound
on the norm of the gradient of f . Assume also that f has a minimizer, i.e., there exists x such that
f (x) = minxRd f (x). Then the following holds for Algorithm 2 with a deterministic gradient oracle:

For any

,

>

0,

using a constant step length of t

=



=

(1-2 )
L 2+

for t

=

1, 2, ..., guarantees

that f (xt) 

for some t 

1
2

×

2L(2

+)(f (x1)-f (1-2 )

(x

))

,

where

x1

is

the

first

iterate

of

the

algorithm.

One might wonder if the  parameter introduced in the algorithms above is necessary to get convergence guarantees for RMSProp. Towards that in the following theorem we show convergence of another variant of deterministic RMSProp which does not use the  parameter and instead uses other assumptions on the objective function and step size modulation. But these tweaks to eliminate the need of  come at the cost of the convergence rates getting weaker.
Theorem 3.3. Convergence of deterministic RMSProp - the version with no  shift (Proof in subsection A.3) Let f : Rd  R be L-smooth and let  <  be an upperbound on the norm of the gradient of f . Assume also that f has a minimizer, i.e., there exists x such that

4

Under review as a conference paper at ICLR 2019

f (x) = minxRd f (x), and the function f be bounded from above and below by constants B and

Bu as Bl  f (x)  Bu for all x  Rd. Then for any

>

0,

T

=

O(

1
4

)

s.t.

the

Algorithm

2

with

a deterministic gradient oracle and  = 0 is guaranteed to reach a t-th iterate s.t. 1  t  T and

f (xt)  .

In Section 5.3 we show results of our experiments with full-batch ADAM. Towards that, we analyze deterministic ADAM albeit in the small 1 regime. We note that a small 1 does not cut-off contributions to the update direction from gradients in the arbitrarily far past (which are typically significantly large), and neither does it affect the non-triviality of the pre-conditioner which does not depend on 1 at all.

Theorem 3.4. Deterministic ADAM converges to criticality (Proof in subsection A.4) Let f : Rd  R be L-smooth and let  <  be an upperbound on the norm of the gradient of f .
Assume also that f has a minimizer, i.e., there exists x such that f (x) = minxRd f (x). Then the following holds for Algorithm 3:

For any

> 0, 1 <

+

and



>

-1

2 +

1 (1-1

)

,

there

exist

step

sizes

t

>

0,

t

=

1, 2, . . .

and

a

natural number T (depending on 1, ) such that f (xt)  for some t  T .

In particular if one sets 1 =

+2 , 

=

2, and t

=

gt 2 L(1-1t )2 3(

4 +2)2

where gt

is the gradient

of

the

objective

at

the

tth

iterate,

then

T

can

be

taken

to

be

9L2
6

[f

(x2

)

-

f (x)],

where

x2

is

the

second iterate of the algorithm.

Our motivations towards the above theorem were primarily rooted in trying to understand the situations where ADAM can converge at all (given the negative results about ADAM as in Reddi et al. (2018)). But we point out that it remains open to tighten the analysis of deterministic ADAM and obtain faster rates than what we have shown in the theorem above.
Remark. It is sometimes believed that ADAM gains over RMSProp because of its "bias correction term" which refers to the step length of ADAM having an iteration dependence of the following form, 1 - 2t/(1 - 1t). In the above theorem, we note that the 1/(1 - 1t) term of this "bias correction term" naturally comes out from theory!

4 EXPERIMENTAL SETUP
For testing the empirical performance of ADAM and RMSProp, we perform experiments on fully connected autoencoders using ReLU activations and shared weights. Let z  Rd be the input vector to the autoencoder, {Wi}i=1,.., denote the weight matrices of the net and {bi}i=1,..,2 be the bias vectors. Then the output z^  Rd of the autoencoder is defined as z^ = W1 (. . . (W -1(W a + b +1) + b +2) . . . ) + b2 where a = (W (. . . (W2(W1z + b1) + b2) . . . ) + b ). This defines an autoencoder with 2 - 1 hidden layers using weight matrices and 2 bias vectors. Thus, the parameters of this model are given by x = [vec(W1) ... vec(W ) b1 ... b2 ] (where we imagine all vectors to be column vectors by default). The loss function, for an input z is then given by: f (z; x) = z - z^ 2.
Such autoencoders are a fairly standard setup that have been used in previous work (Arpit et al., 2015; Baldi, 2012; Kuchaiev & Ginsburg, 2017; Vincent et al., 2010). There have been relatively fewer comparisons of ADAM and RMSProp with other methods on a regression setting. We were motivated by Rangamani et al. (2017) who had undertaken a theoretical analysis of autoencoders and in their experiments had found RMSProp to have good reconstruction error for MNIST when used on even just 2 layer ReLU autoencoders.
To keep our experiments as controlled as possible, we make all layers in a network have the same width (which we denote as h). Thus, given a size d for the input image, the weight matrices (as defined above) are given by: W1  Rh×d, Wi  Rh×h, i = 2, . . . , . This allowed us to study the effect of increasing depth or width h without having to deal with added confounding factors. For all experiments, we use the standard "Glorot initialization" for the weights (Glorot & Bengio, 2010), where each element in the weight matrix is initialized by sampling from a uniform distribution with [-limit, limit], limit = 6/(fanin + fanout), where fanin denotes the number of input units in the

5

Under review as a conference paper at ICLR 2019
weight matrix, and fanout denotes the number of output units in the weight matrix. All bias vectors were initialized to zero. No regularization was used. We performed autoencoder experiments on the MNIST dataset for various network sizes (i.e., different values of and h). We implemented all experiments using TensorFlow (Abadi et al., 2016) using an NVIDIA GeForce GTX 1080 Ti graphics card. We compared the performance of ADAM and RMSProp with Nesterov's Accelerated Gradient (NAG). All experiments were run for 105 iterations. We tune over the hyper-parameters for each optimization algorithm using a grid search as described in the Appendix (Section B). To pick the best set of hyper-parameters, we choose the ones corresponding to the lowest loss on the training set at the end of 105 iterations. Further, to cut down on the computation time so that we can test a number of different neural net architectures, we crop the MNIST image from 28 × 28 down to a 22 × 22 image by removing 3 pixels from each side (almost all of which is whitespace).
Full-batch experiments We are interested in first comparing these algorithms in the full-batch setting. To do this in a computationally feasible way, we consider a subset of the MNIST dataset (we call this: mini-MNIST), which we build by extracting the first 5500 images in the training set and first 1000 images in the test set in MNIST. Thus, the training and testing datasets in miniMNIST is 10% of the size of the MNIST dataset. Thus the training set in mini-MNIST contains 5500 images, while the test set contains 1000 images. This subset of the dataset is a fairly reasonable approximation of the full MNIST dataset (i.e., contains roughly the same distribution of labels as in the full MNIST dataset), and thus a legitimate dataset to optimize on.
Mini-batch experiments To test if our conclusions on the full-batch case extend to the mini-batch case, we then perform the same experiments in a mini-batch setup where we fix the mini-batch size at 100. For the mini-batch experiment, we consider the full training set of MNIST, instead of the mini-MNIST dataset considered for the full-batch experiments.
5 EXPERIMENTAL RESULTS
5.1 RMSPROP AND ADAM ARE SENSITIVE TO CHOICE OF 
The  parameter is a feature of the default implementations of RMSProp and ADAM such as in TensorFlow. Most interestingly this strictly positive parameter is crucial for our proofs. In this section we present experimental evidence that attempts to clarify that this isn't merely a theoretical artefact but its value indeed has visible effect on the behaviours of these algorithms. We see in Figure 1 that on increasing the value of this fixed shift parameter , ADAM in particular, is strongly helped towards getting lower gradient norms and lower test losses though it can hurt its ability to get lower training losses. The plots are shown for optimally tuned values for the other hyper-parameters.
Figure 1: Optimally tuned parameters for different  values. 1 hidden layer network of 1000 nodes; Left: Loss on training set; Middle: Loss on test set; Right: Gradient norm on training set
5.2 TRACKING min(HESSIAN) OF THE LOSS FUNCTION
To check whether NAG, ADAM or RMSProp is capable of consistently moving from a "bad" saddle point to a "good" saddle point region, we track the most negative eigenvalue of the Hessian
6

Under review as a conference paper at ICLR 2019
min(Hessian). Even for a very small neural network with around 105 parameters, it is still intractable to store the full Hessian matrix in memory to compute the eigenvalues. Instead, we use the Scipy library function scipy.sparse.linalg.eigsh that can use a function that computes the matrix-vector products to compute the eigenvalues of the matrix (Lehoucq et al., 1998). Thus, for finding the eigenvalues of the Hessian, it is sufficient to be able to do Hessian-vector products. This can be done exactly in a fairly efficient way (Townsend, 2008). We display a representative plot in Figure 2 which shows that NAG in particular has a distinct ability to gradually, but consistently, keep increasing the minimum eigenvalue of the Hessian while continuing to decrease the gradient norm. However unlike as in deeper autoencoders in this case the gradient norms are consistently bigger for NAG, compared to RMSProp and ADAM. In contrast, RSMProp and ADAM quickly get to a high value of the minimum eigenvalue and a small gradient norm, but somewhat stagnate there. In short, the trend looks better for NAG, but in actual numbers RMSProp and ADAM do better.
Figure 2: Tracking the smallest eigenvalue of the Hessian on a 1 hidden layer network of size 300. Left: Minimum Hessian eigenvalue. Right: Gradient norm on training set.
5.3 COMPARING PERFORMANCE IN THE FULL-BATCH SETTING In Figure 3, we show how the training loss, test loss and gradient norms vary through the iterations for RMSProp, ADAM (at 1 = 0.9 and 0.99) and NAG (at µ = 0.9 and 0.99) on a 3 hidden layer autoencoder with 1000 nodes in each hidden layer trained on mini-MNIST. Appendix D.1 and D.2 have more such comparisons for various neural net architectures with varying depth and width and input image sizes, where the following qualitative results also extend.
Conclusions from the full-batch experiments of training autoencoders on mini-MNIST: · Pushing 1 closer to 1 significantly helps ADAM in getting lower training and test losses and at these values of 1, it has better performance on these metrics than all the other algorithms. One sees cases like the one displayed in Figure 3 where ADAM at 1 = 0.9 was getting comparable or slightly worse test and training errors than NAG. But once 1 gets closer to 1, ADAM's performance sharply improves and gets better than other algorithms.
Figure 3: Full-batch experiments on a 3 hidden layer network with 1000 nodes in each layer; Left: Loss on training set; Middle: Loss on test set; Right: Gradient norm on training set
7

Under review as a conference paper at ICLR 2019
Figure 4: Mini-batch experiments on a network with 5 hidden layers of 1000 nodes each; Left: Loss on training set; Middle: Loss on test set; Right: Gradient norm on training set
· Increasing momentum helps NAG get lower gradient norms though on larger nets it might hurt its training or test performance. NAG does seem to get the lowest gradient norms compared to the other algorithms, except for single hidden layer networks like in Figure 2.
5.4 CORROBORATING THE FULL-BATCH BEHAVIORS IN THE MINI-BATCH SETTING
In Figure 4, we show how training loss, test loss and gradient norms vary when using mini-batches of size 100, on a 5 hidden layer autoencoder with 1000 nodes in each hidden layer trained on the full MNIST dataset. More such mini-batch comparisons on autoencoder architectures with varying depths and widths can be seen in Appendix D.3, showing that qualitatively the same pattern follows.
Conclusions from the mini-batch experiments of training autoencoders on full MNIST dataset:
· Mini-batching does seem to help NAG do better than ADAM on small nets. However, for larger nets, the full-batch behavior continues, i.e., when ADAM's momentum parameter 1 is pushed closer to 1, it gets better generalization (significantly lower test losses) than NAG at any momentum tested.
· In general, for all metrics (test loss, training loss and gradient norm reduction) both ADAM as well as NAG seem to improve in performance when their momentum parameter (µ for NAG and 1 for ADAM) is pushed closer to 1. This effect, which was present in the full-batch setting, seems to get more pronounced here.
· As in the full-batch experiments, NAG continues to have the best ability to reduce gradient norms while for larger enough nets, ADAM at large momentum continues to have the best training error.
6 CONCLUSION
To the best of our knowledge, we present the first theoretical guarantees of convergence to criticality for the immensely popular algorithms RMSProp and ADAM in their most commonly used setting of optimizing a non-convex objective. By our experiments, we have sought to shed light on the important topic of the interplay between adaptivity and momentum in training nets. By choosing to study textbook autoencoder architectures where various parameters of the net can be changed controllably we highlight the following two aspects that (a) the value of the gradient shifting hyperparameter  has a significant influence on the performance of ADAM and RMSProp and (b) ADAM seems to perform particularly well (supersedes Nesterov accelerated gradient method) when its momentum parameter 1 is very close to 1. For the task of training autoencoders on MNIST we have verified these conclusions across different widths and depths of nets as well as in the full-batch and the mini-batch setting (with large nets) and under compression of the input/out image size. Curiously enough, this regime of 1 being close to 1 is currently not within the reach of our proof techniques of showing convergence for ADAM. Our experiments give strong reasons to try to advance theory in this direction in future work.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Mart´in Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for largescale machine learning. In OSDI, volume 16, pp. 265­283, 2016.
Devansh Arpit, Yingbo Zhou, Hung Ngo, and Venu Govindaraju. Why regularized auto-encoders learn sparse representation? arXiv preprint arXiv:1505.05561, 2015.
Reza Babanezhad, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub Konecny, and Scott Sallinen. Stop wasting my gradients: Practical svrg. arXiv preprint arXiv:1511.01942, 2015.
Parnia Bahar, Tamer Alkhouli, Jan-Thorsten Peter, Christopher Jan-Steffen Brix, and Hermann Ney. Empirical investigation of optimization algorithms in neural machine translation. The Prague Bulletin of Mathematical Linguistics, 108(1):13­25, 2017.
Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, pp. 37­49, 2012.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd: compressed optimisation for non-convex problems. arXiv preprint arXiv:1802.04434, 2018.
Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein. Automated inference with adaptive batches. In Artificial Intelligence and Statistics, pp. 1504­1513, 2017.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pp. 1646­1654, 2014.
Michael Denkowski and Graham Neubig. Stronger baselines for trustable results in neural machine translation. arXiv preprint arXiv:1706.09733, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Se´bastien Gadat, Fabien Panloup, Sofiane Saadane, et al. Stochastic heavy ball. Electronic Journal of Statistics, 12(1):461­529, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­256, 2010.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pp. 315­323, 2013.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of existing momentum schemes for stochastic optimization. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJTutzbA-.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arxiv. org, 2014.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems, pp. 3294­3302, 2015.
9

Under review as a conference paper at ICLR 2019
Oleksii Kuchaiev and Boris Ginsburg. Training deep autoencoders for collaborative filtering. arXiv preprint arXiv:1708.01715, 2017.
Richard B Lehoucq, Danny C Sorensen, and Chao Yang. ARPACK users' guide: solution of largescale eigenvalue problems with implicitly restarted Arnoldi methods, volume 6. Siam, 1998.
James Lucas, Richard Zemel, and Roger Grosse. Aggregated momentum: Stability through passive damping. arXiv preprint arXiv:1804.00325, 2018.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408­2417, 2015.
Ga´bor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589, 2017.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pp. 372­376, 1983.
Peter Ochs. Local convergence of the heavy-ball method and ipiano for non-convex optimization. arXiv preprint arXiv:1606.09070, 2016.
Michael O'Neill and Stephen J Wright. Behavior of accelerated gradient methods near critical points of nonconvex problems. arXiv preprint arXiv:1706.07993, 2017.
Genevieve B Orr and Todd K Leen. Momentum and optimal stochastic search. In Proceedings of the 1993 Connectionist Models Summer School, pp. 351­357. Psychology Press, 1994.
Boris T Polyak. Introduction to optimization. translations series in mathematics and engineering. Optimization Software, 1987.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Akshay Rangamani, Anirbit Mukherjee, Ashish Arora, Tejaswini Ganapathy, Amitabh Basu, Sang Chin, and Trac D Tran. Critical points of an autoencoder can provably recover sparsely used overcomplete dictionaries. arXiv preprint arXiv:1708.03735, 2017.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139­1147, 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Technical Report, 2012.
Jamie Townsend. A new trick for calculating Jacobian vector products. https://j-towns. github.io/2017/06/12/A-new-trick.html, 2008. [Online; accessed 17-May-2018].
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371­3408, 2010.
Wim Wiegerinck, Andrzej Komoda, and Tom Heskes. Stochastic dynamics of learning with momentum in neural networks. Journal of Physics A: Mathematical and General, 27(13):4425, 1994.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4151­4161, 2017.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pp. 2048­2057, 2015.
10

Under review as a conference paper at ICLR 2019 Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016. Kun Yuan, Bicheng Ying, and Ali H Sayed. On the influence of momentum acceleration on online
learning. Journal of Machine Learning Research, 17(192):1­66, 2016. SK Zavriev and FV Kostyuk. Heavy-ball method in nonconvex optimization problems. Computa-
tional Mathematics and Modeling, 4(4):336­341, 1993.
11

Under review as a conference paper at ICLR 2019

Appendix

A PROOFS OF CONVERGENCE OF (STOCHASTIC) RMSPROP AND ADAM

A.1 PROVING STOCHASTIC RMSPROP (PROOF OF THEOREM 3.1)

Proof. We define t := maxk=1,..,t fik (xk) and we solve the recursion for vt as, vt = (1 -

2)

t k=1

2t-k (gk2

+

).

This

lets

us

write

the

following

bounds,

min(Vt-

1 2

)





1 maxi=1,..,d(vt)i
1 1 - 2t t2 + 

1

maxi=1,..,d((1 - 2)

t k=1

2t-k (gk2

+

1d)i)

Now we define, t := mink=1,..,t,i=1,..,d(fik (xk))2i and this lets us get the following bounds,

max

(Vt-

1 2

)



1 mini=1,..,d(

 (vt)i)

1 (1 - 2t ) ( + t)

Now we invoke the bounded gradient assumption about the fi functions and replace in the above equation the eigenvalue bounds of the pre-conditioner by worst-case estimates µmax and µmin defined as,

min(Vt-

1 2

)



max(Vt-

1 2

)



1 := µmin
f2 + 

1

 (1 - 2) 

:=

µmax

Using the L-smoothness of f between consecutive iterates xt and xt+1 we have,

f (xt+1)  f (xt) +

f (xt), xt+1 - xt

L +
2

xt+1 - xt

2

We

note

that

the

update

step

of

stochastic

RMSProp

is

xt+1

=

xt

-

(Vt

)-

1 2

gt

where

gt

is

the

stochastic gradient at iterate xt. Let Ht = {x1, x2, .., xt} be the set of random variables corresond-

ing to the first t iterates. The assumptions we have about the stochastic oracle give us the following

relations, E[gt] = f (xt) and E[ gt 2]  f2 . Now we can invoke these stochastic oracle's prop-

erties and take a conditional (on Ht) expectation over xt of the L-smoothness inequation to get,

E[f (xt+1) | Ht]  f (xt) - E  f (xt) - E  f (xt) - E

f

(xt

),

(Vt

)-

1 2

gt

f

(xt

),

(Vt

)-

1 2

gt

f

(xt

),

(Vt

)-

1 2

gt

| Ht | Ht | Ht

2L + 2E

(Vt

)-

1 2

gt

2 | Ht

+

µm2 ax

2L 2

E

gt 2 | Ht

+

µm2 ax

2f2 2

L

(1)

We now separately analyze the middle term in the RHS above,

12

Under review as a conference paper at ICLR 2019

d

E[

f

(xt

),

(Vt

)-

1 2

gt

| Ht] = E[

(f

(xt

))i

(Vt

)i-i

1 2

(gt

)i

|

Ht]

i=1

 E µmin(f (xt))i(gt)i
i|(f (xt))i(gt)i0

+ µmax(f (xt))i(gt)i | Ht
i|(f (xt))i(gt)i<0
 µmin(f (xt))2i
i|(f (xt))i(gt)i0
+ µmax(f (xt))i2
i|(f (xt))i(gt)i<0
 µmin f (xt) 2

We substitute the above into equation 1 and take expectations over Ht to get,

E[f (xt+1) - f (xt)]  -µmin

f (xt)

2

+

µ2max

2 f2 L 2

= E[

f (xt)

2] 

1 µmin E[f (xt) - f (xt+1)] +

f2 L µ2max 2 µmin

(2)

Doing the above replacements to upperbound the RHS of equation 2 and summing the inequation over t = 1 to t = T and taking the average and replacing the LHS by a lowerbound of it, we get,

min E[
t=1...T

f (xt)

2] 

T

1 µmin

E[f

(x1)

-

f

(xT

+1

)]

+

f2 L µ2max 2 µmin



1 T µmin

(f (x1) - f (x)) +

f2 L 2

µ2max µmin

Replacing into the RHS above the optimal choice of,

we get,

 = 1 T

2 (f (x1) - f (x)) f2 Lµ2max

=

1 T

2(1 - 2) (f (x1) - f (x)) f2 L

min E[ f (xt) 2]  2
t=1...T

1 T µmin

(f (x1) -

f (x))

×

Lf2 2

µ2max µmin

=

1 T

2Lf2(f2 + ) (f (x1) - f (x)) (1 - 2)

Thus stochastic RMSProp with the above step-length is guaranteed is reach -criticality in number

of iterations given by, T



1
4

2Lf2 (f2 +)(f (x1)-f (x)) (1-2 )

13

Under review as a conference paper at ICLR 2019

A.2 PROVING DETERMINISTIC RMSPROP - THE VERSION WITH STANDARD SPEED (PROOF OF THEOREM 3.2)

Proof. By the L-smoothness condition and the update rule in Algorithm 2 we have,

f (xt+1)



f (xt) - t

f

(xt

),

Vt-

1 2

f

(xt

)

+

t2

L 2

Vt-

1 2

f

(xt

)

2

= f (xt+1) - f (xt)  t

Lt 2

Vt-

1 2

f

(xt

)

2-

f

(xt

),

Vt-

1 2

f

(xt

)

(3)

For

0

<

t2

<

 1
1-2t t2+

we

now

show

a

strictly

positive

lowerbound

on

the

following

function,

2

f

(xt

),

Vt-

1 2

f

(xt

)

- t2

f (xt) 2

L

Vt-

1 2

f

(xt

)

2

(4)

We define t := maxi=1,..,t f (xi) and we solve the recursion for vt as, vt = (1 -

2)

t k=1

2t-k (g2k

+

).

This

lets

us

write

the

following

bounds,

f

(xt

),

Vt-

1 2

f

(xt

)



min

(Vt-

1 2

)

f (xt)

2

f (xt) 2 maxi=1,..,d(vt)i

 f (xt) 2

maxi=1,..,d((1 - 2)

t k=1

2t-k (gk2

+

1d)i)

 f (xt) 2 1 - 2t t2 + 

(5)

Now we define, t := mink=1,..,t,i=1,..,d(f (xk))2i and this lets us get the following sequence of inequalities,

Vt-

1 2

f

(xt

)

2



2max

(Vt-

1 2

)

f (xt)

2

f (xt) 2 (mini=1,..,d( (vt)i))2



f (xt) 2 (1 - 2t)( +

t)

(6)

So combining equations 6 and 5 into equation 4 and from the exit line in the loop we are assured that f (xt) 2 = 0 and combining these we have,

2

-t2

f (xt) 2 +

f

(xt

),

Vt-

1 2

f

(xt

)

L

Vt-

1 2

f

(xt

)

2



2 L

 -t2

+

 1
1-2t t2+

1

(1-2t )(+ t)

 

 2(1 - 2t )( + t) L

-t2 +

1 1 - 2t t2 + 

Now

our

definition

of

t2

allows

us

to

define

a

parameter

0

<

t

:=

 1
1-2t t2+

-

t2

and

rewrite

the above equation as,

2

-t2

f (xt) 2 +

f

(xt

),

Vt-

1 2

f

(xt

)

L

Vt-

1 2

f

(xt

)

2

 2(1 - 2t )( + t)t L

We can as well satisfy the conditions needed on the variables, t and t by choosing,

t2

=

1 2

min
t=1,...

11 =
1 - 2t t2 +  2

1 =: 2 2 + 

14

(7)

Under review as a conference paper at ICLR 2019

and

t = min
t=1,..

1 - 2 = 1

1 - 2t t2 + 

2

Then the worst-case lowerbound in equation 7 becomes,

1 2 + 

2

-t2

f (xt) 2 +

f

(xt

),

Vt-

1 2

f

(xt

)

L

Vt-

1 2

f

(xt

)

2

 2(1 - 2) × 1 1 L 2 2 + 

This now allows us to see that a constant step length t =  > 0 can be defined as,



=

(1-2 )
L 2+

and this is such that the above equation can be written as,

L 2

Vt-

1 2

f

(xt

)

2-

f

(xt

),

Vt-

1 2

f

(xt

)

 -2

f (xt) 2 . This when substituted back into equation 3 we have,

This gives us,

f (xt+1) - f (xt)  -2 f (xt) 2 = -2 f (xt) 2

f (xt)

2



1 2

[f

(xt)

-

f

(xt+1)]

T
=

f (xt)

2



1 2 [f (x1) - f (x)]

t=1

= min
t=1,,.T

f (xt)

2



T

1 2



[f

(x1

)

-

f

(x

)]

(8) (9)

Thus for any given

>

0,

T

satisfying,

T

1 2



[f

(x1

)

-

f

(x

)]



2 is a sufficient condition to ensure

that the algorithm finds a point xresult := arg mint=1,,.T f (xt) 2 with f (xresult) 2  2.

Thus we have shown that using a constant step length of  = (1-2) deterministic RMSProp can
L 2+

find an

-critical point in T =

1
2

×

f (x1)-f (x) 2

=

1
2

×

2L(2+)(f (x1)-f (x)) (1-2 )

steps.

15

Under review as a conference paper at ICLR 2019

A.3 PROVING DETERMINISTIC RMSPROP - THE VERSION WITH NO ADDED SHIFT (PROOF OF THEOREM 3.3)
Proof. From the L-smoothness condition on f we have between consecutive iterates of the above algorithm,

f (xt+1)  f (xt) - t

f

(xt

),

Vt-

1 2

f

(xt

)

+

L 2

t2

Vt-

1 2

f

(xt

)

2

=

f

(xt

),

Vt-

1 2

f

(xt

)



1 t

(f (xt)

-

f (xt+1))

+

Lt 2

Vt-

1 2

f

(xt

)

2

(10)

(11)

Now the recursion for vt can be solved to get, vt

=

(1 - 2)

t k=1

2t-k g2k .

1
Then Vt2

  =  =1
maxiSupport(vt )

(vt )i

maxiSupport(vt )

1 (1-2 )

t k=1

2t-k (gk2 )i

 =  .1
maxiSupport(vt)  (1-2)

t k=1

2t-k

1  (1-2t )

Substituting this in a lowerbound on the

LHS of equation 10 we get,



1 (1 - 2t )

f (xt)

2

f

(xt

),

Vt-

1 2

f

(xt

)



1 t

(f

(xt)

-

f

(xt+1))+

Lt 2

Vt-

1 2

f

(xt

)

2

Summing the above we get,

T
t=1 

1 (1 - 2t )

f (xt)

2



T t=1

1 t

(f (xt)

-

f (xt+1))

+

T t=1

Lt 2

Vt-

1 2

f

(xt

)

2

(12)

Now we substitute t =

 and invoke the definition of B
t

and Bu to write the first term on the

RHS of equation 12 as,

T t=1

1 t [f (xt) -

f (xt+1)]

=

f (x1) 

+

T t=1

f (xt+1) - f (xt+1)

t+1

t

- f (xT +1) T +1

= 

f (x1) 
Bu -

- B

f (xT +1) + 1

T +1





T + 1 + Bu

T
f
t=1
 (T

 (xt+1)( t + 1 - 1)

+

 1 - t)



Now we bound the second term in the RHS of equation 12 as follows. Lets first define a function

P (T ) as follows, P (T ) =

T t=1

t

Vt-

1 2

f

(xt

)

2 and that gives us,

P (T ) - P (T

- 1) = T

d i=1

g2T ,i vT ,i

= T

d i=1

(1 - 2)

gT2 ,i

T k=1

2T

-k

g2k,i

=

T (1 - 2)

d i=1

gT2 ,i

T k=1

2T

-k

g2k,i



d  (1 - 2) T

=

T
[P (t) -
t=2

P (t

- 1)]

=

P (T ) - P (1)



d (1 - 2)

T t=2

1 t



d  2(1 - 2) ( T

- 2)

=

P (T )



P (1)

+

d  2(1 - 2) ( T

-

2)

16

Under review as a conference paper at ICLR 2019

So substituting the above two bounds back into the RHS of the above inequality 12and removing the factor of 1 - 2T < 1 from the numerator, we can define a point xresult as follows,

f (xresult) 2 := arg min
t=1,..,T

f (xt)

2



1 T

T

t=1

f (xt) 2



Bu

-

 Bl T

+

1

+

Bu

 (T

+

1

-

1)

T  

L + P (1) +

d

 ( T - 2)

2 2(1 - 2)

Thus it follows that for T

=

O(

1
4

)

the

algorithm

2

is

guaranteed

to

have

found

at

least

one

point

xresult such that, f (xresult) 2  2

A.4 PROVING ADAM (PROOF OF THEOREM 3.4)
Proof.
Let us assume to the contrary that gt > for all t = 1, 2, 3. . . .. We will show that this assumption will lead to a contradiction. By L-smoothness of the objective we have the following relationship between the values at consecutive updates,

f (xt+1)  f (xt) +

f (xt), xt+1 - xt

L +
2

xt+1 - xt 2

Substituting the update rule using a dummy step length t > 0 we have,

f (xt+1)  f (xt) - t f (xt),

1
Vt2 + diag(1d)

-1
mt

+ Lt2 2

1 -1 2
Vt2 + diag(1d) mt

= f (xt+1) - f (xt)  t

- gt,

1
Vt2 + diag(1d)

-1
mt

+ Lt 2

1 -1 2
Vt2 + diag(1d) mt

(13)

1 -1

gt, Vt2 +diag(1d)

mt

The RHS in equation 13 above is a quadratic in t with two roots: 0 and

1

.-1 2

L 2

Vt2 +diag(1d)

mt

So the quadratic's minimum value is at the midpoint of this interval, which gives us a candidate

1 -1

tth-step

length

i.e

t

:=

1 2

·

gt, Vt2 +diag(1d)

mt

1 -1 2 and the value of the quadratic at this point

L 2

Vt2 +diag(1d)

mt

1 -1

is

-

1 4

·

(

gt,

Vt2 +diag(1d)
1

-1

mt

)2
2

.

That

is

with

step

lengths

being

this

t

we

have

the

following

L 2

Vt2 +diag(1d)

mt

guarantee of decrease of function value between consecutive steps,

1 -1

1 ( gt, Vt2 + diag(1d) mt )2

f (xt+1)

-

f (xt)



- 2L

·

1

-1 2

Vt2 + diag(1d) mt

(14)

17

Under review as a conference paper at ICLR 2019

Now we separately lower bound the numerator and upper bound the denominator of the RHS above.

1 -1
Upperbound on Vt2 + diag(1d) mt

We have, max

1 -1
Vt2 + diag(1d)

 1  Further we note that the recursion of
+mini=1..d (vt)i

vt can be solved as, vt = (1 - 2)

t k=1

2t-k

gk2

.

Now

we

define,

t := mink=1,..,t,i=1,..,d(gk2 )i

and this gives us,

max

1 -1

Vt2 + diag(1d)

 +

1 (1 - 2t ) t

(15)

We solve the recursion for mt to get, mt = (1 - 1) defining t := maxi=1,..,t f (xi) we have, mt

kt =(11-1t-k1tg)k.t

Then by triangle inequality and . Thus combining this estimate

of mt with equation 15 we have,

1 -1
Vt2 + diag(1d) mt

 (1 - 1t )t  (1 - 1t )t

 + t(1 - 2t )



1 -1
Lowerbound on gt, Vt2 + diag(1d) mt
To analyze this we define the following sequence of functions for each i = 0, 1, 2.., t

1 -1
Qi = gt, Vt2 + diag(1d) mi

(16)

This gives us the following on substituting the update rule for mt,

1 -1
Qi - 1Qi-1 = gt, Vt2 + diag(1d) (mi - 1mi-1)
1 -1
= (1 - 1) gt, Vt2 + diag(1d) gi

At i = t we have, Qt - 1Qt-1  (1 - 1) gt 2 min

1 -1
Vt2 + diag(1d)

Lets define, t-1 := maxi=1,..,t-1 f (xi) and this gives us for i  {1, .., t - 1},
1 -1
Qi - 1Qi-1  -(1 - 1) gt t-1max Vt2 + diag(1d)

We note the following identity,

Qt - 1t Q0 = (Qt - 1Qt-1) + 1(Qt-1 - 1Qt-2) + 12(Qt-2 - 1Qt-3) + .. + 1t-1(Q1 - 1Q0) Now we use the lowerbounds proven on Qi - 1Qi-1 for i  {1, .., t - 1} and Qt - 1Qt-1 to lowerbound the above sum as,
18

Under review as a conference paper at ICLR 2019

Qt - 1t Q0  (1 - 1) gt 2 min

1 -1
Vt2 + diag(1d)

1 -1
- (1 - 1) gt t-1max Vt2 + diag(1d)

 (1 - 1) gt 2 min

1 -1
Vt2 + diag(1d)

t-1
1j
j=1

- (1 - 1t ) gt t-1max

1 -1
Vt2 + diag(1d)

(17)

We can evaluate the following lowerbound, min

1 -1
Vt2 + diag(1d)

 1

.

+ maxi=1,..,d(vt)i

Next we remember that the recursion of vt can be solved as, vt = (1 - 2)

t k=1

2t-k gk2

and

we

define, t := maxi=1,..,t f (xi) to get,

min

1 -1

Vt2 + diag(1d)

 +

1 (1 - 2t )t2

(18)

Now we combine the above and equation 15 and the known value of Q0 = 0 (from definition and initial conditions) to get from the equation 17,

Qt  -(1 - 1t )

gt

t-1 +

1 (1 - 2t ) t

+ (1 - 1)

gt

2
+

1 (1 - 2t )t2

 gt 2

(1 - 1)

- (1 - 1t )

 +  (1 - 2t)

 gt

(19)

In the above inequalities we have set t = 0 and we have set, t = t-1 = . Now we examine the following part of the lowerbound proven above,

(1 - 1)

- (1 - 1t ) =  gt (1 - 1) - (1 - 1t )( +  (1 - 2t ))

 + (1 - 2t )2

 gt

 gt ( +  (1 - 2t))

 = (1 - 1t ) = (1 - 1t )

gt (1-1) (1-1t )

-

1

-

(1 - 2t )

 gt ( +  (1 - 2t))



gt (1 - 1) (1 - 1t )

-

1

-  gt

 (1-2t )

-1+

(1-1 ) gt (1 -1t )

( +  (1 - 2t))

Now we remember the assumption that we are working under i.e gt > . Also by definition

0

<

1

<

1

and

hence

we

have

0

<

1

-

1t

<

1.

This

implies,

(1-1) gt (1-1t )

>

(1-1 ) 1 

> 1 where

the last inequality follows because of our choice of as stated in the theorem statement. This allows

us to define a constant,

(1-1 ) 1 

-1

:=

1

>

0

s.t

(1-1) gt (1-1t )

- 1 > 1

Similarly our definition of  allows us to define a constant 2 > 0 to get,



 (1 - 2t )

 -1

+

(1-1) gt (1-1t )

  < 1

=  - 2

19

Under review as a conference paper at ICLR 2019

Putting the above back into the lowerbound for Qt in equation 19 we have,

Qt  gt 2

(1 - 12)12 ( + )

Now we substitute the above and equation 16 into equation 14 to get,

1 -1

1 ( gt, Vt2 + diag(1d) mt )2

f (xt+1)

-

f (xt)



- 2L

·

1

-1 2

Vt2 + diag(1d) mt

- 1 2L

Qt2
1 -1 2
Vt2 + diag(1d) mt

- 1 2L

g 4 (1-12)12
t (+)
(1-1t ) 2 

2

 - gt 4 2L

(1 - 12)21222 ( + )2(1 - 1t )22

(20) (21)

=

(1 - 12)21222 2L( + )2(1 - 1t )22

T
=

(1 - 12)21222 2L( + )22

t=2

= min
t=2,..,T

f (xt) 4  [f (xt) - f (xt+1)]

f (xt) 4  [f (x2) - f (xT +1)]

f (xt)

4

T

2L( + )22 (1 - 12)21222

[f

(x2

)

-

f

(x)]

Observe that if,

2L2( + )2 T  2 4(1 - 12)21222 [f (x2) - f (x)]

then the RHS of the inequality above is less than or equal to 4 and this would contradict the assumption that f (xt) > for all t = 1, 2, . . ..

As a consequence we have proven the first part of the theorem which guarantees the existence of positive step lengths, t s.t ADAM finds an approximately critical point in finite time.

Now

choose

1

=

1

i.e

2

=

1  1-1

i.e

1

=

+2

= 1(1 - 1) =

+2 (1 -

+2 )

=

(

2 +2)2

.

This also gives a easier-to-read condition on  in terms of these parameters i.e  > . Now choose

 = 2 i.e 2 =  and making these substitutions gives us,

T

18L4
2

[f (x2) - f (x)] 

18L

2 [f (x2)

-

f (x)]



9L2 6 [f (x2)

-

f (x)]

24

2 ( +2)

2

86

1 +2

We substitute these choices in the step length found earlier to get,

20

Under review as a conference paper at ICLR 2019

t

=

1 L

·

1 -1
gt, Vt2 + diag(1d) mt
1 -1 2
Vt2 + diag(1d) mt

=1· L

Qt
1 -1 2
Vt2 + diag(1d) mt

1 L

gt 2

2 (1 -12 ) (+)

(1-1t ) 2 

=

gt L(1 -

2
1t )2

3(

4 + 2)2

:= t

In the theorem statement we choose to call as the final t the lowerbound proven above. We check below that this smaller value of t still guarantees a decrease in the function value that is sufficient for the statement of the theorem to hold.

A consistency check! Let us substitute the above final value of the step length t =

1

gt 2

2 (1 -12 ) (+)

L (1-1t ) 2

=

 L(1-1t )2

gt 2

(1 -12 ) (+)

, the bound in equation 16 (with t replaced by



), and the bound in equation 20 (at the chosen values of 1 = 1 and 2 = ) in the original equation 14 to measure the decrease in the function value between consecutive steps,

f (xt+1) - f (xt)  t

- gt,

1
Vt2 + diag(1d)

-1
mt

+ Lt 2

1 -1 2
Vt2 + diag(1d) mt

 t

-Qt

+

Lt 2

1 -1 2
Vt2 + diag(1d) mt



L(1

 -

1t )2

gt 2

(1 - 12) ( + )

- gt 2

(1 - 12)12 ( + )

L +
2

 L(1 - 1t )2

gt 2

(1 - 12) ( + )

(1 - 1t ) 2 

The RHS above can be simplified to be shown to be equal to the RHS in equation 21 at the same values of 1 and 2 as used above. And we remember that the bound on the running time was derived from this equation 21.

21

Under review as a conference paper at ICLR 2019
B HYPERPARAMETER TUNING
Here we describe how we tune the hyper-parameters of each optimization algorithm. NAG has two hyper-parameters, the step size  and the momentum µ. The main hyper-parameters for RMSProp are the step size , the decay parameter 2 and the perturbation . ADAM, in addition to the ones in RMSProp, also has a momentum parameter 1. We vary the step-sizes of ADAM in the conventional way of t =  1 - 2t /(1 - 1t ). For tuning the step size, we follow the same method used in Wilson et al. (2017). We start out with a logarithmically-spaced grid of five step sizes. If the best performing parameter was at one of the extremes of the grid, we tried new grid points so that the best performing parameters were at one of the middle points in the grid. While it is computationally infeasible even with substantial resources to follow a similarly rigorous tuning process for all other hyper-parameters, we do tune over them somewhat as described below.
NAG The initial set of step sizes used for NAG were: {3e-3, 1e-3, 3e-4, 1e-4, 3e-5}. We tune the momentum parameter over values µ  {0.9, 0.99}.
RMSProp The initial set of step sizes used were: {3e-4, 1e-4, 3e-5, 1e-5, 3e-6}. We tune over 2  {0.9, 0.99}. We set the perturbation value  = 10-10, following the default values in TensorFlow, except for the experiments in Section 5.1. In Section 5.1, we show the effect on convergence and generalization properties of ADAM and RMSProp when changing this parameter .
Note that ADAM and RMSProp uses an accumulator for keeping track of decayed squared gradient vt. For ADAM this is recommended to be initialized at v0 = 0. However, we found in the TensorFlow implementation of RMSProp that it sets v0 = 1d. Instead of using this version of the algorithm, we used a modified version where we set v0 = 0. We typically found setting v0 = 0 to lead to faster convergence in our experiments.
ADAM The initial set of step sizes used were: {3e-4, 1e-4, 3e-5, 1e-5, 3e-6}. For ADAM, we tune over 1 values of {0.9, 0.99}. For ADAM, We set 2 = 0.999 for all our experiments as is set as the default in TensorFlow. Unless otherwise specified we use for the perturbation value  = 10-8 for ADAM, following the default values in TensorFlow.
Contrary to what is the often used values of 1 for ADAM (usually set to 0.9), we found that we often got better results on the autoencoder problem when setting 1 = 0.99.
C EFFECT OF THE  PARAMETER ON ADAPTIVE GRADIENT ALGORITHMS
In Figure 5, we show the same effect of changing  as in Section 5.1 on a 1 hidden layer network of 1000 nodes, while keeping all other hyper-parameters fixed (such as learning rate, 1, 2). These other hyper-parameter values were fixed at the best values of these parameters for the default values of , i.e.,  = 10-10 for RMSProp and  = 10-8 for ADAM.

(a) Loss on training set

(b) Loss on test set

(c) Gradient norm on training set

Figure 5: Fixed parameters with changing  values. 1 hidden layer network of 1000 nodes

22

Under review as a conference paper at ICLR 2019
D ADDITIONAL EXPERIMENTS
D.1 ADDITIONAL FULL-BATCH EXPERIMENTS ON 22 × 22 SIZED IMAGES In Figures 6, 7 and 8, we show training loss, test loss and gradient norm results for a variety of additional network architectures. Across almost all network architectures, our main results remain consistent. ADAM with 1 = 0.99 consistently reaches lower training loss values as well as better generalization than NAG.
(a) 1 hidden layer; 1000 nodes (b) 3 hidden layers; 1000 nodes (c) 5 hidden layers; 1000 nodes each each
(d) 3 hidden layers; 300 nodes (e) 3 hidden layer; 3000 nodes (f) 5 hidden layer; 300 nodes Figure 6: Loss on training set; Input image size 22 × 22
(a) 1 hidden layer; 1000 nodes (b) 3 hidden layers; 1000 nodes (c) 5 hidden layers; 1000 nodes each each
(d) 3 hidden layers; 300 nodes (e) 3 hidden layer; 3000 nodes (f) 5 hidden layer; 300 nodes Figure 7: Loss on test set; Input image size 22 × 22
23

Under review as a conference paper at ICLR 2019
(a) 1 hidden layer; 1000 nodes (b) 3 hidden layers; 1000 nodes (c) 5 hidden layers; 1000 nodes each each
(d) 3 hidden layers; 300 nodes (e) 3 hidden layer; 3000 nodes (f) 5 hidden layer; 300 nodes Figure 8: Norm of gradient on training set; Input image size 22 × 22
24

Under review as a conference paper at ICLR 2019
D.2 ARE THE FULL-BATCH RESULTS CONSISTENT ACROSS DIFFERENT INPUT DIMENSIONS? To test whether our conclusions are consistent across different input dimensions, we do two experiments where we resize the 22 × 22 MNIST image to 17 × 17 and to 12 × 12. Resizing is done using TensorFlow's tf.image.resize images method, which uses bilinear interpolation.
D.2.1 INPUT IMAGES OF SIZE 17 × 17 Figure 9 shows results on input images of size 17 × 17 on a 3 layer network with 1000 hidden nodes in each layer. Our main results extend to this input dimension, where we see ADAM with 1 = 0.99 both converging the fastest as well as generalizing the best, while NAG does better than ADAM with 1 = 0.9.

(a) Training loss

(b) Test loss

(c) Gradient norm

Figure 9: Full-batch experiments with input image size 17 × 17

D.2.2 INPUT IMAGES OF SIZE 12 × 12
Figure 10 shows results on input images of size 12 × 12 on a 3 layer network with 1000 hidden nodes in each layer. Our main results extend to this input dimension as well. ADAM with 1 = 0.99 converges the fastest as well as generalizes the best, while NAG does better than ADAM with 1 = 0.9.

(a) Training loss

(b) Test loss

(c) Gradient norm

Figure 10: Full-batch experiments with input image size 12 × 12

25

Under review as a conference paper at ICLR 2019 D.3 ADDITIONAL MINI-BATCH EXPERIMENTS ON 22 × 22 SIZED IMAGES In Figure 11, we present results on additional neural net architectures on mini-batches of size 100 with an input dimension of 22 × 22. We see that most of our full-batch results extend to the minibatch case.
(a) 1 hidden layer; 1000 nodes (b) 3 hidden layers; 1000 nodes (c) 9 hidden layers; 1000 nodes each each
(d) 1 hidden layer; 1000 nodes (e) 3 hidden layers; 1000 nodes (f) 9 hidden layers; 1000 nodes each each
(g) 1 hidden layer; 1000 nodes (h) 3 hidden layers; 1000 nodes (i) 9 hidden layers; 1000 nodes each each
Figure 11: Experiments on various networks with mini-batch size 100 on full MNIST dataset with input image size 22 × 22. First row shows the loss on the full training set, middle row shows the loss on the test set, and bottom row shows the norm of the gradient on the training set.
26

