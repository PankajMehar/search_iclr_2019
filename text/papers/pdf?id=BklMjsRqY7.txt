Under review as a conference paper at ICLR 2019
ACCUMULATION BIT-WIDTH SCALING FOR ULTRALOW PRECISION TRAINING OF DEEP NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Efforts to reduce the numerical precision of computations in deep learning training have yielded systems that aggressively quantize weights and activations, yet employ wide high-precision accumulators for partial sums in inner-product operations to preserve the quality of convergence. The absence of any framework to analyze the precision requirements of partial sum accumulations results in conservative design choices. This imposes an upper-bound on the reduction of complexity of multiply-accumulate units. We present a statistical approach to analyze the impact of reduced accumulation precision on deep learning training. Observing that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, we derive a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. We apply our analysis to three benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet. In each case, with accumulation precision set in accordance with our proposed equations, the networks successfully converge to the single precision floating-point baseline. We also show that reducing accumulation precision further degrades the quality of the trained network, proving that our equations produce tight bounds. Overall this analysis enables precise tailoring of computation hardware to the application, yielding area- and power-optimal systems.
1 INTRODUCTION
Over the past decade, deep learning techniques have been remarkably successful in a wide spectrum of applications through the use of very large and deep models trained using massive datasets. This training process necessitates up to 100's of ExaOps of computation and Gigabytes of storage. It is, however, well appreciated that a range of approximate computing techniques can be brought to bear to significantly reduce this computational complexity (Chen et al., 2018) - and amongst them, exploiting reduced numerical precision during the training process is extremely effective and has already been widely deployed (Gupta et al., 2015).
There are several reasons why reduced precision deep learning has attracted the attention of both hardware and algorithms researchers. First, it offers well defined and scalable hardware efficiency, as opposed to other complexity reduction techniques such as pruning (Han et al., 2015b;a), where handling sparse data is needed. Indeed, parameter complexity scales linearly while multiplication hardware complexity scales quadratically with precision bit-width (Zhou et al., 2016). Thus, any advance towards truly binarized networks (Hubara et al., 2016) corresponds to potentially 30x 1000x complexity reduction in comparison to single precision floating-point hardware. Second, the mathematics of reduced precision has direct ties with the statistical theory of quantization (Widrow and Kolla´r, 2008). In the context of deep learning, this presents an opportunity for theoreticians to derive analytical trade-offs between model accuracy and numerical precision (Lin et al., 2016; Sakr et al., 2017).
Most ongoing efforts on reduced precision deep learning solely focus on quantizing representations and always assume wide accumulators, i.e., ideal summations. The reason being reduced precision accumulation can result in severe training instability and accuracy degradation, as illustrated in Figure 1 (a) for ResNet 18 (ImageNet) model training. This is especially unfortunate, since the hardware complexity in reduced precision floating-point numbers (needed to represent small gradients during training) (Wang et al., 2018; Micikevicius et al., 2017) is dominated by the accumulator bit-width. To illustrate this dominance we developed a model underpinned by the hardware synthe-
1

Under review as a conference paper at ICLR 2019

FP32 Accumulation

FP16 Accumulation

1.5×

2.2~3.3×

1.5~2.2×

Predicted Accumulation Precision Range via our Analysis

(a) (b)
Figure 1: The importance of accumulation precision: (a) convergence curves of an ImageNet ResNet 18 experiment using reduced precision accumulation. The current practice is to keep the accumulation in full precision to avoid such divergence. (b) estimated area benefits when reducing the precision of a floating-point unit (FPU). The terminology FPa/b denotes an FPU whose multiplier and adder use a and b bits, respectively. Our work enables convergence in reduced precision accumulation and gains an extra 1.5×  2.2× area reduction.
sis of low-precision floating-point units (FPU), that translates precision into area complexity of the FPU. Comparisons obtained from this model are shown in Figure 1 (b). We observe that accumulating in high precision severely limits the hardware benefits of reduced precision computations. This presents a new angle to the problem of reduced precision deep learning training which concerns determining suitable accumulation precision and forms the basis of our paper. Our findings are that the accumulation precision requirements in deep learning training are nowhere near 32-b, and in fact could enable further complexity reduction of FPUs by a factor of 1.5  2.2×.
1.1 RELATED WORKS
Our work is concerned with establishing theoretical foundations to the accumulation bit precision requirements in deep learning. While this topic has never been addressed in the past, there are prior works in both deep learning and high performance computing communities that align well with ours.
Most early works on reduced precision deep learning consider fixed-point arithmetic or a variation of it (Gupta et al., 2015). However, when considering quantization of signals involving the backpropagation algorithm, finding a suitable fixed-point configuration becomes challenging due to a weak handle on the scalar dynamic range of the back-propagated signals. Thus, hardware solutions have been sought, and, accordingly, other number formats were considered. Flexpoint (Ko¨ster et al., 2017) is a hybrid version between fixed-point and floating-point where scalars in a tensor are quantized to 16-b fixed-point but share 5-b of exponenent to adjust the dynamic range. Similarly, WAGE (Wu et al., 2018) augments Flexpoint with stochastic quantization and enables integer quantization. All of these schemes focused on representation precision, but mostly used 32-bit accumulation. Another option is to use reduced precision floating-point as was done in MPT (Micikevicius et al., 2017), which reduces the precision of most signals to 16-b floating-point, but observes accuracy degradation when reducing the accumulation precision from 32-b. Recently, Wang et al. (2018) quantize all representations to 8-b floating-point and experimentally find that the accumulation could be in 16-b with algorithmic contrivance, such as chunk-based accumulation, to enable convergence.
The issue of numerical errors in floating-point accumulation has been classically studied in the area of high performance computing. Robertazzi and Schwartz (1988) were among the first to statistically estimate the effects of floating-point accumulation. Assuming a stream of uniformly and exponentially distributed positive numbers, estimates for the mean square error of the floatingpoint accumulation were derived via quantization noise analysis. Because such analyses are often intractable (due to the multiplicative nature of the noise), later works on numerical stability focus on worst case estimates of the accumulation error. Higham (1993) provide upper bounds on the error magnitude by counting and analyzing round-off errors. Following this style of worst case analysis, Castaldo et al. (2008) provide bounds on the accumulation error for different summing algorithms, notably using chunk-based summations. Different approaches to chunking are considered and their benefits are estimated. It is to be noted that these analyses are often loose as they are agnostic to the application space. To the best of our knowledge, a statistical analysis on the accumulation precision specifically tailored to deep learning training remains elusive.
2

Under review as a conference paper at ICLR 2019

IFM Height (IH) Kernel Height (KW)
IFM Height (IH) Product Height (PH)

OFM Height (OH) Kernel Height (KW)

OuNtpuumtbCehra(nOnCeNl ) OFM Height (OH) InpNuutmCbhearn(nICelN)

Input Feature Maps (IFM)

Output Feature Maps (OFM)

Input Feature Maps (IFM) Derivatives

InpNuutmCbhearn(nICelN)

Output Feature Maps (OFM) Derivatives
Kernel Width (KW)

OuNtpuumtbCehra(nOnCeNl )

Elementwise Product of Input Activations and Output Derivatives

Weight Gradients (2D Slice)

Kernel Width (KW)

Batch Size (BS)

IFM Width (IW)

OFM Width (OW)

Accumulation length = ICN x KW x KH

IFM Width (IW)

OFM Width (OW)

Accumulation length = OCN x KW x KH

Product Width (PW)
Accumulation length = BS x PW x PH

(a) (b) (c)
Figure 2: The three GEMM calls, and hence accumulations, in one iteration of the back-propagation algorithm: (a) the forward propagation (FWD), (b) the backward propagation (BWD), and (c) gradient computation (GRAD). The accumulation of these three GEMMs is across multiple dimensions (mini-batch size, feature maps, output channels etc.) and their lengths are usually very long.

1.2 CONTRIBUTIONS

Our contribution is both theoretical and practical. We introduce the variance retention ratio (VRR) of a reduced precision accumulation in the context of the three deep learning dot products. The VRR is used to assess the suitability, or lack thereof, of a precision configuration. Our main result is the derivation of an actual formula for the VRR that allows us to determine accumulation bit-width for precise tailoring of computation hardware. Experimentally, we verify the validity and tightness of our analysis across three benchmarking networks (CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet).

2 BACKGROUND ON FLOATING-POINT ARITHMETIC

The following basic floating-point definitions and notations are used in our work: Floating-point representation: A b-bit floating-point number a has a signed bit, e exponent bits, and m mantissa bits so that b = 1 + e + m. Its binary representation is (Bs, B1, . . . , Be, B1 , . . . , Bm)  {0, 1}b and its value is equal to:
a = (-1)Bs × 2E × (1 + M )

where E = -(2e-1 - 1) +

e i=1

Bi2(e-i)

and

M

=

m i=1

Bi

2-i.

Such number is called a

(1, e, m) floating-point number.

Floating-point operations: One of the most prevalent arithmetic functions used in deep learning is

the dot product between two vectors which is the building block of the generalized matrix multiplica-

tion (GEMM). A dot product is computed in a multiply-accumulate (MAC) fashion and thus requires

two floating-point operations: multiplication and addition. The realization of an ideal floating-point

operation requires a certain bit growth at the output to avoid loss of information. For instance, in a

typical MAC operation, if c  c + a × b where a is (1, ea, ma) and b is (1, eb, mb), then c should

be (1, max(ea, eb) + 2, ma + mb + 1 + E), which depends on the bit-precision and the relative exponent difference of the operands E. However, it is often more practical to pre-define the precision of c as (1, ec, mc), which requires rounding immediately after computation. Such rounding might cause an operand to be completely or partially truncated out of the addition, a phenomenon

called "swamping" (Higham, 1993), which is the primary source of accumulation errors.

3 ACCUMULATION VARIANCE

The second order statistics (variance) of signals are known to be of great importance in deep learning. For instance, in prior works on weight initialization (Glorot and Bengio, 2010; He et al., 2015), it is customary to initialize random weights subject to a variance constraint designed so as to prevent vanishing or explosion of activations and gradients. Thus, such variance engineering induces fine convergence of DNNs. Importantly, in such analyses, the second order output statistics of a dot product are studied and expressed as a function of that of the accumulated terms, which are assumed to be independent and having similar variance. A fundamental assumption is: V ar(s) = nV ar(p), where V ar(s) and V ar(p) are the variances of the sum and individual product terms, respectively, and n is the length of the dot product. One intuition concerning accumulation with reduced precision is that, due to swamping, some product terms may vanish from the summation, resulting in a lower variance than expected: V ar(s) = n~V ar(p), where n~ < n. This constitutes a violation of a key assumption and effectively leads to the re-emergence of the difficulties in training neural networks with improper weight initialization which often harms the convergence behavior (Glorot and Bengio, 2010; He et al., 2015).

3

Under review as a conference paper at ICLR 2019

break point abnormal behavior

full swamping: # >  
"#$ = 1 X X X X X X
1XXXX

1 X X X X Stage 4: 4 bits swamped

" =

1XXXX 1XXXX

Stage 3: 3 bits swamped Stage 2: 2 bits swamped

1XXXX

Stage 1: 1 bit swamped

1XXXX

Truncation

no swamping: #   # 

Figure 4: Illustration of the difference between full

Figure 3: Snapshot of measured weight gradient vari- swamping and partial swamping when macc = 6 and

ance as a function of layer index for our ResNet 18 mp = 4. The bit-shift of pi due to exponent difference

experiment. An abnormal variance is observed for the may cause partial (e.g., stage 1-4) or full truncation of

reduced precision accumulation.

pi's bits, called swamping.

To explain the poor convergence of our ResNet 18 run (Figure 1 (a)), we evaluate the behavior of accumulation variance across layers. Specifically, we check the three dot products of a backpropagation iteration: the forward propagation (FWD), the backward propagation (BWD), and the gradient computation (GRAD), as illustrated in Figure 2. Indeed, there is an abnormality in reduced precision GRAD as shown in Figure 3. It is also observed that the abnormality of variance is directly related to accumulation length. From Figure 3, the break point corresponds to the switch from the first to the second residual block. The GRAD accumulation length in the former is much longer (4×) than the latter. Thus, evidence points to the direction that for a given precision, there is a an accumulation length for which the expected variance cannot be properly retained due to swamping. Motivated by these observations, we propose to study the trade-offs among accumulation variance, length, and mantissa precision.

4 MANTISSA PRECISION REQUIREMENTS ANALYSIS

We assume enough exponent precision throughout and treat reduced precision floating-point arithmetic as an unbiased form of approximate computing, as is customary. Thus, our work focuses on associating second order statistics to mantissa precision.

We consider the accumulation of n terms {pi}in=1 which correspond to the element-wise product

terms in a dot product. The We assume, as in (He et al.,

goal is 2015),

to compute the that the product

correct nth partial terms {pi}ni=1 are

sum sn where si =

i i

=1

pi

.

statistically independent, zero-

mean, and have the same variance p2. Thus, under ideal computation, the variance of sn (which is

equal to its second moment) should be V ar(sn)ideal = np2. However, due to swamping effects, the

variance of sn under reduced precision is V ar(sn)swamping = V ar(sn)ideal.

Let the product terms {pi}ni=1 and partial sum terms {si}in=1 have mp and macc mantissa bits, re-

spectively.

Our

key

contribution

is

a

formula

for

the

variance

retention

ratio

V

RR

=

.V ar(sn)swamping
V ar(sn)ideal

The VRR is a function of n, mp, and macc only, which needs no simulation to be computed. Fur-

thermore, to preserve quality of computation under reduced precision, it is required that V RR  1.

As it turns out, the VRR for a fixed precision is a curve with "knee" with respect to n, where a break

point in accumulation length beyond which a certain mantissa precision is no longer suitable can be

easily identified. Accordingly, for a given sum, the mantissa precision requirements can be readily

estimated.

Before proceeding, we formally define swamping. As illustrated in Figure 4, the bit-shift of pi due to exponent difference may cause partial (e.g., stage 1-4) or full truncation of pi's bits, called swamping. We define (1) "full swamping" which occurs when |si| > 2macc |pi+1|, and (2) "partial swamping" which occurs when 2macc-mp |pi+1| < |si|  2macc |pi+1|. These two swamping types will be fully considered in our analysis.

4.1 VARIANCE RETENTION RATIO In the lemma below, we first present a formula for the VRR when only full swamping is considered.

4

Under review as a conference paper at ICLR 2019

Lemma 1. The variance retention ratio of a length n accumulation using macc mantissa bits, and when only considering full swamping, is given by:

V RRfull swamping =

n-1 i=2

iqi

+

nq~n

kn

(1)

where qi = 2Q

2macc i

1 - 2Q

2macc i-1

, q~n = 1 - 2Q

2macc n

,k =

normalization constant, and Q denotes the elementary Q-function.

n-1 i=2

qi

+

q~n

is

a

The proof is provided in Appendix A. A preliminary check is that a very large value of macc in (1) causes all {qi}ni=-11 terms to vanish and q~n to approach unity. This makes V RR  1 for high precision as expected. On the other hand, if we assume macc to be small, but let n  , we get nq~n  0 because the Q-function term will approach 1 exponentially fast as opposed to the n term which is linear. Furthermore, the terms inside the summation having a large i will vanish by the same
argument, while the n term in the denominator will make the ratio decrease and we would expect V RR  0. This means that with limited precision, there is little hope to achieve a correct result
when the accumulation length is very large. Also, the rapid change in VRR from 0 to 1 indicates
that VRR can be used to provide sharp decision boundary for accumulation precision.

The above result only considers full swamping and is thus incomplete. Next we augment our analysis to take into account the effects of partial swamping. The corresponding formula for the VRR is provided in the following theorem.

Theorem 1. The variance retention ratio of a length n accumulation using mp and macc mantissa bits for the input products and partial sum terms, respectively, is given by:

V RR =

n-1 i=2

(i

-

)+qi1{i>}

+

mp jr =2

(n

-

jr

)+qi1{n>jr }

+

nk3

nk

(2)

where (x)+ =

x if x > 0 0 otherwise

, 1A =

1 if A is true 0 otherwise

,

 = 2macc-3mp
3

mp j=1

2j

(2j

-

1)(2j+1

-

1),

qi

= 2Q

2macc i

1 - 2Q

2macc -3mp 3

jr -1 j=1

2j (2j

-

1)(2j+1

-

1),

q = N 2Qjr

jr -1

2macc-mp+jr -1 n

1 - 2Q 2macc-mp+jr
n

,

k = k1 + k2 + k3, k1 =

n-1 i=2

qi1{i>},

k2

=

mp jr =2

qi1{n>jr },

and

k3 = 1 - 2Q

2macc -mp +1 n

.

2macc i-1

, jr =

The proof is provided in Appendix B. Observe the dependence on macc, mp, and n. Therefore, in
what follows, we shall refer to the VRR in (2) as V RR(macc, mp, n). Once again, we verify the extremal behavior of our formula. A very large value of macc in (2) causes k1  k2  0 and k3  1. This makes V RR  1 for high precision as expected. In addition, assuming small macc and letting n  , we get nk3  0 because k3 decays exponentially fast due to the Q-function term. By the same argument, qjr  0 for all jr and qi  0 for all but small values of i. Thus, the numerator will be small, while the denominator will increase linearly in n causing V RR  0. Thus, once more, we
establish that with limited accumulation precision, there is little hope for a correct result.

4.2 VRR WITH CHUNK BASED ACCUMULATIONS
Next we consider an accumulation that uses chunking. In particular, assume n = n1 × n2 so that the accumulation is broken into n2 chunks, each of length n1. Thus, n2 accumulations of length n1 are performed and the n2 intermediate results are added to obtain sn. This simple technique is known to greatly improve the stability of sums (Castaldo et al., 2008). The VRR can be used to theoretically explain such improvements. For simplicity, we assume two-level chunking (as described above) and same mantissa precision macc for both inter-chunk and intra-chunk accumulations. Applying the above analysis, we may obtain a formula for the VRR as provided in the corollary below, which is proved in Appendix C.
Corollary 1. The variance retention ratio of an length n = n1 × n2 accumulation with chunking, where n1 is the chunk size and n2 is the number of chunks, using mp and macc mantissa bits for the

5

Under review as a conference paper at ICLR 2019

VRR

1.00 0.95 0.90 0.85 0.80 0.75 0.70 0.65 0.60
100

VRR vs. Chunk Size

n= n= n= n=

83150000000000000,,, mmm, maaacccacccc===c =7568

101 Chunk S1i0z2e

103

(a) (b)

(c)

Figure 5: Normalized variance lost as a function of accumulation length for different values of macc for (a) a normal accumulation (no chunking) and (b) a chunk-based accumulation (chunk size of 64). The "knees" in

each plot correspond to the maximum accumulation length for a given precision which indicates how the VRR

is to be used to select a suitable precision. (c) VRR as a function of chunk-size for several accumulation setups.

The dashed lines correspond to the value of the VRR when no chunking is used. The flat maximas indicate that

the exact choice of a chunking size is not of paramount importance.

input products and partial sum terms, respectively, is given by:

V RRchunking = V RR(macc, mp, n1) × V RR (macc, min (macc, mp + log2(n1)) , n2)

(3)

4.3 VRR WITH SPARSITY

It is common to encounter sparse operands in deep learning dot products. Since addition of zero is an identity operation, the effective accumulation length is often less than as described by the network topology. Indeed, for a given accumulation, supposedly of length n, if we can estimate the non-zero ratio (NZR) of its incoming product terms, then the effective accumulation length is N ZR × n.

Thus, when an accumulation is known to have sparse inputs with known NZR, a better estimate of the VRR is

V RRsparsity = V RR(macc, mp, N ZR × n).

(4)

Similarly, when considering the VRR with chunking, we may use knowledge of sparsity to obtain the effective intra-accumulation length as N ZR × n1. This change is reflected both in the VRR of the intra-chunk accumulation and the input precision of the inter-chunk accumulation:

V RRchunking and sparsity = V RR(macc, mp, N ZR × n1) × V RR (macc, min (macc, mp + log2(N ZR × n1)) , n2)

(5)

In practice, the NZR can be estimated by making several observations from baseline data. Using an estimate of the NZR makes our analysis less conservative.

4.4 USAGE OF ANALYSIS

For a given accumulation setup, one may compute the VRR and observe how close it is from the ideal value of 1 in order to judge the suitability of the mantissa precision assignment. It turns out that when measured as a function of accumulation length n for a fixed precision, the VRR has a breakdown region. This breakdown region can very well be observed when considering what we define as the normalized exponential variance lost:

v(n) = en(1-V RR)

(6)

In Figure 5 (a,b) we plot v(n) for different values of macc when considering both normal accumulation and chunk-based accumulation with a chunk-size of 64. The value of mp is set to 5-b, corresponding to the product of two numbers in (1,5,2) floaintg-point format (Wang et al., 2018). It is clearly be seen that for a given macc there exists a maximum value of n beyond which the amount of lost variance is significant. We consider macc to be suitable for a given n only if v(n) < 50.
In addition, when performing chunk-based accumulation, the chunk size is a hyperparameter that, a priori, cannot be determined trivially. Castaldo et al. (2008) identified an optimal chunk size minimizing the loose upper bound on the accumulation error they derived. In practice, they did not find the accumulation error to be sensitive to the chunk-size. Neither did Wang et al. (2018) who performed numerical simulations. By sweeping the chunk size and observing the accumulation

6

Under review as a conference paper at ICLR 2019

Layer(s) FWD BWD GRAD

CIFAR-10 ResNet 32g

Conv 0 ResBlock 1 ResBlock 2

(6,5) (6,5)

(7,5)

N/A (6,5)

(7,5)

(11,8)

(11,8)

(10,6)

Resblock 3 (7,5) (8,5) (9,6)

Layer(s) FWD BWD GRAD

Conv 0 (9,6) N/A (15,10)

ImageNet ResNet 18

ResBlock 1 ResBlock 2 Resblock 3

(7,5) (8,5) (8,5)

(8,6) (9,6) (9,6)

(15,9)

(12,8)

(10,6)

ResBlock 4 (9,6) (10,6) (9,5)

ImageNet AlexNet
Layer Conv 1 Conv 2 Conv 3 Conv 4 Conv 5 FC 1 FC 2 FWD (7,5) (9,5) (9,5) (8,5) (8,5) (9,6) (8,5) BWD N/A (8,5) (8,5) (10,8) (8,5) (8,5) (8,5) GRAD (10,7) (9,6) (8,6) (6,5) (6,5) (6,5) (6,5)
Table 1: The predicted precisions required for all accumulations of our considered networks. Each table entry is an ordered tuple of two values which correspond to the predicted mantissa precision of both normal and chunk-based accumulations, respectively. The precision requirements of FWD and BWD are typically smaller than those of GRAD. The latter needs the most precision for layers/blocks close to the input as the size of the feature maps is highest in the early stages. The benefits of chunking are non linear but range from 1 to 6 bits.

behavior on synthetic data, it was found that chunking significantly reduces accumulation error as long as the chunk size is not too small nor too large. Using our analysis, we provide a theoretical justification. Figure 5 (c) shows the VRR for various accumulation setups, including chunking when the chunk size is sweeped. For each case we see that chunking raises the VRR to a value close to unity. Furthermore, the VRR curve in that regime is "flat", meaning that a specific value of chunk size does not matter as long as it is not too small nor too large. In our upcoming chunking experiments we use a chunk size of 64 as was done by Wang et al. (2018).

5 NUMERICAL RESULTS

Using the above analysis, we predict the mantissa precisions required by the three GEMM functions for training the following networks: ResNet 32 on the CIFAR-10 dataset, ResNet 18 and AlexNet on the ImageNet dataset. We use the same configurations as (Wang et al., 2018), in particular, we use 6-b of exponenents in the accumulations, and quantize the intermediate tensors to (1,5,2) floating-point format and keep the final layer's precision in 16 bit.
The predicted precisions for each network and layer/block are listed in Table 1 for the case of normal and chunk-based accumulation with a chunk size of 64. Several insights are to be noted.
· The required accumulation precision for CIFAR-10 ResNet 32 is in general lower than that of the ImageNet networks. This is simply because, the network topology imposes shorter dot products.
· Though, topologically, the convolutional layers in the two ImageNet networks are similar, the precision requirements do vary. Specifically, the GRAD accumulation depends on the feature map dimension which is mostly dataset dependent, yet AlexNet requires less precision than ResNet 18. This is because the measured sparsity of the operands was found to be much higher for AlexNet.
· Figure 5 suggests that chunking decreases the precision requirements significantly. This is indeed observed in Table 1, where we see that the benefits of chunking reach up to 6-b in certain accumulations, e.g., the GRAD acccumulation in the first ResBlock of ImageNet ResNet 18.
Because our predicted precision assignment ensures the VRR of all GEMM accumulations to be close to unity, we expect reduced precision training to converge with close fidelity to the baseline. Since our work focuses on accumulation precision, in our experiments, the baseline denotes accumulation in full precision. For a fair comparison, all upcoming results use (1,5,2) representation precision which was recently demonstrated to be adequate (Wang et al., 2018). Thus, the effects of reduced precision representation are not taken into account.
The goal of our experiments is to investigate both the validity and conservatism of our analysis. In Figure 6, we plot the convergence curves when training with our predicted accumulation precision for a normal accumulation. The runs corresponding to chunk-based accumulations were also performed but are omitted since the trend is similar. Furthermore, we repeat all experiments with precision perturbation (PP), meaning a specific reduction in precision with respect to our prediction. For instance, P P = 0 indicates our prediction while P P = -1 corresponds to a one bit reduction. Finally, in order to better visualize how the accumulation precision affect convergence, we plot in Figure 6 (d) the accuracy degradation as a function of precision perturbation for each of our three networks with both normal and chunk-based accumulations. The following is to be noted:
· When P P = 0, the converged accuracy always lies within 0.5% of the baseline. A strong indicator of the validity of our analysis.

7

Under review as a conference paper at ICLR 2019

(a) (b)

Accuracy Degradation (%)

2.5

CIFAR-10 ResNet 32 ImageNet ResNet 18

2.0 ImageNet AlexNet

1.5

1.0

0.5

0.0

2 Precision Pe1rturbation

0

(c) (d)

Figure 6: Convergence curves for (a) CIFAR-10 ResNet 32, (b) ImageNet ResNet 18, and (c) ImageNet

AlexNet, and (d) final accuracy degradation with respect to the baseline as a function of precision perturbation

(PP). The solid and dashed lines correspond to the no chunking and chunking case, respectively. Using our

predicted precision assignment, the converged test error is close to the baseline (no more than 0.5% degradation)

but increases significantly when the precision is further reduced.

· When P P < 0, a noticeable accuracy degradation is observed. The converged accuracy is no longer within 0.5% of the baseline (except for CIFAR-10 ResNet 32 with normal accumulation). Furthermore, a clear trend observed is that the higher the perturbation, the worse the degradation.
· ImageNet ResNet 18 appears to be very sensitive to precision perturbation, specifically when the learning rate is high (before epoch 60). Significant instability is observed for P P < 0.
· ImageNet AlexNet is somewhat more robust to perturbation than the two ResNets. In fact, while P P = -1 causes a degradation strictly > 0.5%, it is not much worse than the P P = 0 case. This observation aligns with the observation from neural net quantization that Alexnet is robust due to its over-parameterized network structure (Zhu et al., 2016). But the trend of increasing degradation remains the same.
· Figure 6 (d) suggests that the effects of PP are more pronounced for a chunk-based accumulation, to which we provide a simple explanation: since the precision assignment itself is lower (Table 1), a perturbation of a specific number of bits corresponds to a relatively higher change compared to the assignment for the normal accumulation. For example, decreasing one bit from a 6-b assignment is more important than decreasing one bit from a 10-b assignment. Further justification can be obtained by comparing Figures 5 (a) and 5 (b) where consecutive lines are less closely aligned for the chunk-based accumulation, indicating more sensitivity to precision perturbation.

Thus, overall, our precision requirements predictions are not only adequate, but also close to the limits beyond which training becomes unstable. These are very encouraging signs that our analysis is both valid and tight.

6 CONCLUSION

We have presented an analytical method to predict the precision required for partial sum accumulation in the three GEMM functions in deep learning training. Our results prove that our method is able to accurately pinpoint the minimum precision needed for the convergence of benchmark networks to the full-precision baseline. While we have demonstrated the applicability of our work to computer vision datasets, in principle, the theoretical concepts are application agnostic. On the practical side, this analysis is a useful tool for hardware designers implementing reduced-precision FPUs, who in the past have resorted to computationally prohibitive brute-force emulations. We believe this work addresses a critical missing link on the path to truly low-precision floating-point hardware for DNN training.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Castaldo, A. M., Whaley, R. C., and Chronopoulos, A. T. (2008). Reducing floating point error in dot product using the superblock family of algorithms. SIAM journal on scientific computing, 31(2):1156­1174.
Chen, C.-Y., Choi, J., Gopalakrishnan, K., Srinivasan, V., and Venkataramani, S. (2018). Exploiting approximate computing for deep learning acceleration. In Design, Automation, Test in Europe Conference Exhibition (DATE).
Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249­256.
Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. (2015). Deep learning with limited numerical precision. In International Conference on Machine Learning, pages 1737­1746.
Han, S., Mao, H., and Dally, W. J. (2015a). Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149.
Han, S., Pool, J., Tran, J., and Dally, W. (2015b). Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pages 1135­1143.
He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026­1034.
Higham, N. J. (1993). The accuracy of floating point summation. SIAM Journal on Scientific Computing, 14(4):783­799.
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y. (2016). Binarized neural networks. In Advances in neural information processing systems, pages 4107­4115.
Ko¨ster, U., Webb, T., Wang, X., Nassar, M., Bansal, A. K., Constable, W., Elibol, O., Gray, S., Hall, S., Hornof, L., et al. (2017). Flexpoint: An adaptive numerical format for efficient training of deep neural networks. In Advances in Neural Information Processing Systems, pages 1742­1752.
Lin, D., Talathi, S., and Annapureddy, S. (2016). Fixed point quantization of deep convolutional networks. In International Conference on Machine Learning, pages 2849­2858.
Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaev, O., Venkatesh, G., et al. (2017). Mixed precision training. arXiv preprint arXiv:1710.03740.
Robertazzi, T. G. and Schwartz, S. C. (1988). Best "ordering" for floating-point addition. ACM Transactions on Mathematical Software (TOMS), 14(1):101­110.
Sakr, C., Kim, Y., and Shanbhag, N. (2017). Analytical guarantees on numerical precision of deep neural networks. In International Conference on Machine Learning, pages 3007­3016.
Wang, N., Choi, J., Brand, D., Chen, C.-Y., and Gopalakrishnan, K. (2018). Training deep neural networks with 8-bit floating point numbers. In Advances in neural information processing systems.
Widrow, B. and Kolla´r, I. (2008). Quantization noise. Cambridge University Press, 2:5.
Wu, S., Li, G., Chen, F., and Shi, L. (2018). Training and inference with integers in deep neural networks. arXiv preprint arXiv:1802.04680.
Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y. (2016). Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160.
Zhu, C., Han, S., Mao, H., and Dally, W. J. (2016). Trained ternary quantization. CoRR, abs/1612.01064.
9

Under review as a conference paper at ICLR 2019

APPENDIX
A PROOF OF LEMMA 1

In order to compute the VRR, we first need to compute V ar(sn)swamping = Eswamping s2n . To do so, we rely on the law of total expectation. Indeed, assume that A is the set of events that describe all
manners in which the accumulation resulting in sn can happen, and let P (A) be the probability of event A  A. Hence, by the law of total expectation, we have that

V ar(sn)swamping =

E sn2 A P (A)

AA

(7)

It is in fact a difficult task to enumerate and describe all events in A. Thus we consider a reduced set of events A^  A which is representative enough of the manners in which the accumulation occurs, yet tractable so that it can be used as a surrogate to A in (7).

We consider a scenario where the first occurrence of full swamping is at iteration i of the summation for i = 2 . . . n - 1. This happens if:

|si| > 2macc |pi+1| & |si | < 2macc |pi +1| for i = 1 . . . i - 1.

(8)

Instead of looking at the actual absolute value of an incoming product term, we replace it by its typical value of p. Furthermore, we simplify the condition of no swamping prior to iteration i by only considering the accumulated sum at the previous iteration. This means we consider a simplified scenario where the accumulation is monotonic in the iterations leading to full swamping. Hence, our simplified condition for the first occurrence of full swamping at iteration i is given by:

|si-1| < 2macc p < |si|.

For mathematical tractability, we will treat each inequality independently. Finally, we assume that once full swamping occurs, the computation is stuck and stops. This means that if full swamping happens at iteration i, then result of the accumulation sn = si.

Thus, the event set A^ we construct for our analysis consists of the mutually exclusive events
{Ai}in=-21 where Ai is the event that full swamping occurs for the first time at iteration i. The condition for event i to happen is given by (8). By Central Limit Theorem, we have that si  N (0, ip2), so that:

P (Ai) = 2Q 2macc i

1 - 2Q 2macc i-1

= qi.

(9)

Furthermore, by our assumption that the computation is stuck and stops after the first full swamping, we have:

E s2n Ai = E s2i = ip2.

(10)

We also add to our space of events, the event An where no full swamping occurs over the course of the accumulation. This event happens if |sn| < 2macc p and thus has probability P (An) = 1 -

2Q

2macc n

= q~n. Since this event corresponds to the ideal scenario, we have E s2n An = np2.

Thus, under the above conditions we have:

1 V ar(sn)swamping = k

n

E

s2n Ai

P (Ai)

=

p2 k

i=2

n-1
iqi + nq~n
i=2

(11)

where k =

n-1 i=2

qi

+

q~n

is

a

normalization

constant

needed

as

A^

does

not

describe

the

full

proba-

bility space.

Consequently we obtain (1) as formula for the VRR completing the proof for Lemma 1.

10

Under review as a conference paper at ICLR 2019

B PROOF OF THEOREM 1

First, we tion that

do not change the description of once full swamping occurs, i.e.,

the |si|

e>ve2nmtsa{ccAip},in=t-h2e1

above. Indeed, computation is

we keep our assumpstuck and stops. The

probability of this event is P (Ai) as above. However, to account for partial swamping, we alter the

result of E s2n Ai . Indeed, partial swamping causes additional loss of variance. When the input

product terms have mp bits of mantissa, then before event Ai can occur, the computation should go through each of the mp stages described in Figure 4. We again assume a typical scenario for each of the mp stages of partial swamping preceding a full swamping event whereby the accumulation is monotonic and the magnitude of the incoming product term is close to its typical value p. Under this assumption, stage j is expected to happen for the following number of iterations:

Nj = 2macc+1-(mp-j) = 2macc-mp+j+1

(12)

for j = 1 . . . mp. At stage j, j least significant bits in the representation of the incoming product
term are truncated (swamped). The variance lost because of this truncation, which we call fractional variance loss E[fj2], can be computed by assuming the truncated bits are equally likely to be 0 or 1,
so that:

E[fj2]

=

p2

2j -1 

1 2j

k=0



2-mp k

2


=

p22-2mp

(2j

-

1)(2j+1 6

-

1)

(13)

Hence, the total fractional variance lost before the occurrence of even Ai is NjE[fj2]. Thus, we update the value of variance conditioned on Ai as follows:

E s2n Ai = ip2 - Nj E[fj2] +



=

p2

i

-

2macc -3mp 3

mp  2j(2j - 1)(2j+1 - 1)

j=1

+

(14)

where we used the operator (x)+ =

x if x > 0 0 otherwise

in order to guarantee that the variance is

positive. Effectively, we neglect the events Ai where i is so small that the variance retained is less than the variance lost due to partial swamping. In other words, an event whereby full swamping

occurs very early in the accumulation is considered to have zero probability and we replace P (Ai) in (9) by:

P (Ai) = qi1{i>}

(15)

where qi

= 2Q

2macc i

1 - 2Q

2macc i-1

as in (9), 1 is the indicator function, and  =

2macc -3mp 3

mp j=1

2j (2j

-

1)(2j+1

-

1)

as

in

(14).

In addition, some boundary conditions need to be accounted for. These include the cases when no

full swamping happens before the accumulation is complete but partial swamping does happen. We

again consider a typical scenario as above and append our event set A with mp - 1 boundary events

Ajr

mp ,
jr =2

where

the

event

Ajr

corresponds

to

the

case

where

the

computation

has

gone

through

stage jr - 1 of partial swamping but has not reached stage jr yet. The condition for this event is

p2macc-mp+jr-1 < |sn| < p2macc-mp+jr and occurs typically for up to Njr-1 iterations. The

total fractional variance lost is:

p2


jr -1


Nj

2j -1

1 2j

j=1

k=0



2-mp k

2


=

p2

2macc -3mp 3

jr -1
2j (2j

- 1)(2j+1 - 1)

j=1

(16)

11

Under review as a conference paper at ICLR 2019

Hence,

E s2n Ajr = p2 (n - jr )+

(17)

where jr

=

2macc -3mp 3

jr -1 j=1

2j (2j

- 1)(2j+1

- 1).

And

the

probability

of

event

Ajr

is

given

by:

P (Ajr ) = qjr 1{n>jr }

where qjr = Njr-12Q

2macc-mp+jr -1 n

1 - 2Q 2macc-mp+jr
n

reflects the number of iterations the event may occur for.

(18) , the multiplication by Njr-1

Finally, the event An is updated and corresponds to the case where neither partial nor full swamping occurs. The condition for this event is |sn| < 2macc-mp+1 and has a probability P (An) = 1 - 2Q .2macc-mp+1
n

Putting things together, we use the law of total expectation as in (7) to compute:

V ar(sn)swamping =



p2 k

n-1
 (i

-

)+qi1{i>}

+

mp  (n - jr )+qi1{n>jr } + nk3

i=2 jr =2

(19)

where k = k1 + k2 + k3, k1 =

n-1 i=2

P (Ai),

k2

=

mp jr =2

P (Ajr ),

and

k3

=

P (An).

Hence,

the

formula for the VRR in (2) in the theorem follows and this concludes the proof.

C PROOF OF COROLLARY 1

Applying the above analysis, we may compute the variance of the intermediate results as p2n1V RR(macc, mp, n1). To compute the variance of the final result, first note that the mantissa precision of the incoming terms to the inter-chunk accumulation (the results from the intra-chunk
accumulation) is min (macc, mp + log2(n1)). The reason being that since the intra-chunk accumulation uses macc mantissa bits, the mantissa cannot grow beyond macc due to the rounding nature of the floating-point accumulation. However, if macc is large enough and n1 is small enough, it is most likely that the mantissa has not grown to the maximum. Assuming accumulation of terms
having statistically similar absolute value as was done for the VRR analysis, then the bit growth of
the mantissa is logarithmic in n1 and starts at mp.

Hence, the variance of the computed result sn when chunking is used is:

V ar(sn)chunking = p2n1V RR(macc, mp, n1) × n2V RR (macc, min (macc, mp + log2(n1)) , n2)

(20)

and hence the VRR with chunking can be computed using (3) in Corollary 1. This completes the proof.

12

