Under review as a conference paper at ICLR 2019
SWITCHING LINEAR DYNAMICS FOR VARIATIONAL BAYES FILTERING
Anonymous authors Paper under double-blind review
ABSTRACT
System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.
1 INTRODUCTION
Learning dynamics from raw data (also known as system identification) is a key component of model predictive control and model-based reinforcement learning. Problematically, environments of interest often give rise to very complex and highly nonlinear dynamics which are seemingly difficult to approximate. However, switching linear dynamical systems (SLDS) approaches claim that those environments can often be broken down into simpler units made up of areas of equal and linear dynamics (Ackerson & Fu, 1970; Chang & Athans, 1978). Not only are those approaches capable of good predictive performance, which often is the sole goal of learning a system's dynamics, they also encode valuable information into so called switching variables which determine the dynamics of the next transition. For example, when looking at the movement of an arm, one is intuitively aware of certain restrictions of possible movements, e.g. constraints to the movement due to joint constraints or obstacles. The knowledge is present without the need to simulate; it's explicit. Exactly this kind of information will be encoded when successfully learning switching dynamics. Our goal in this work will therefore entail the search for richer representations in the form of latent state space models which encode knowledge about the underlying system dynamics. In turn, we expect this to improve the accuracy of our simulation as well. Such a representation alone could then be used in a reinforcement learning approach that possibly only takes advantage of the learned latent features but not necessarily its learned dynamics.
To learn richer representations, we identify one common problem with prevalent recurrent Variational Autoencoder models (Karl et al., 2017a; Krishnan et al., 2015; Chung et al., 2015; Fraccaro et al., 2016): the non-probabilistic treatment of the transition dynamics often modeled by a powerful nonlinear function approximator. From the history of the Autoencoder to the Variational Autoencoder, we know that in order to detect features in an unsupervised manner, probabilistic treatment of the latent space is paramount. As our starting point, we will build on previously proposed approaches by Krishnan et al. (2017) and Karl et al. (2017a). The latter already made use of locally linear dynamics, but only in a deterministic fashion. We extend their approaches by a stochastic switching LDS model and show that such treatment is vital for learning richer representations and simulation accuracy.
2 BACKGROUND
We consider discretized time-series data consisting of continuous observations xt  X  Rnx and control inputs ut  U  Rnu that we would like to model by corresponding latent states zt  Z  Rnz . We'll denote sequences of variables by x1:T = (x1, x2, ..., xT ).
1

Under review as a conference paper at ICLR 2019

s1 s2 s3 u1 u2
z1 z2 z3

w s2 s3 u1 u2
z1 z2 z3

x1 x2 x3
(a) SLDS graphical model.

x1 x2 x3
(b) Our generative model.

Figure 1: (a) st denote discrete switch variables, zt are continuous latent variables, xt continuous observed variables, ut are (optional) continuous control inputs. (b) By introducing a special latent variable w used for initial state inference, we want to make explicit that the first step is treated
differently from the rest of the sequence.

2.1 SWITCHING LINEAR DYNAMICAL SYSTEMS

Switching Linear Dynamical System models (SLDS) enable us to model nonlinear time series data

by splitting it into sequences of linear dynamical models. At each time t = 1, 2, ..., T , a discrete switch variable st  1, ..., S chooses of a set LDSs a system which is to be used to transform our continuous latent state zt to the next time step (Barber, 2012).

zt = A(st)zt-1 + B(st)ut-1 + (st) xt = H(st)zt + (st)

(st)  N (0, Q(st)) (st)  N (0, R(st))

(1)

Here A  Rnz×nz is the state matrix, B  Rnz×nu control matrix, the transition noise with covariance matrix Q and  the emission/sensor noise with covariance matrix R. Finally, the observation matrix H  Rnx×nz defines a linear mapping from latent to observation space which we will replace by a nonlinear transformation parameterized by a neural net. These equations imply the following

joint distribution:

T
p(x1:T , z1:T , s1:T | u1:T ) = p(xt | zt) p(zt | zt-1, ut-1, st) p(st | zt-1, ut-1, st-1) (2)
t=1
with p(z1 | z0, u0, s1) = p(z1) being the initial state distribution. The corresponding graphical model is shown in figure 1a.

2.2 STOCHASTIC GRADIENT VARIATIONAL BAYES

p(x) = p(x, z) dz = p(x | z)p(z) dz

(3)

Given the simple graphical model in equation (3), Kingma & Welling (2014) and Rezende et al.
(2014) introduced the Variational Autoencoder (VAE) which overcomes the intractability of posterior inference of q(z | x) by maximizing the evidence lower bound (ELBO) of the model log-likelihood.

LELBO(x; , ) = Eq(z|x)[ln p(x | z)] - DKL(q(z | x) || p(z))  log p(x)

(4)

Their main innovation was to approximate the intractable posterior distribution by a recognition

network q(z|x) from which they can sample via the reparameterization trick to allow for stochastic backpropagation through both the recognition and generative model at once. Assuming that the latent

state is normally distributed, a simple transformation allows us to obtain a Monte Carlo gradient estimate of Eq(z|x) [ln p(x|z)] w.r.t. to . Given that z  N (µ, 2), we can generate samples by
drawing from an auxiliary variable  N (0, 1) and applying the deterministic and differentiable

transformation z = µ +  .

2.3 THE CONCRETE DISTRIBUTION

One simple and efficient way to obtain samples d from a k-dimensional categorical distribution with class probabilities  is the Gumbel-Max trick:

d = one_hot (argmax[gi + log i]) , with g1, . . . , gk  Gumbel(0, 1)

(5)

2

Under review as a conference paper at ICLR 2019

However, since the derivative of the argmax is 0 everywhere except at the boundary of state changes, where it is undefined, we can't learn a parameterization by backpropagation. The Gumbel-Softmax trick approximates the argmax by a softmax which gives us a probability vector (Maddison et al., 2017; Jang et al., 2017). We can then draw samples via

dk =

exp(log k

n i=1

exp(log

+ gk/) i + gi/)

,

with g1, . . . , gk  Gumbel(0, 1)

(6)

This softmax computation approaches the discrete argmax as temperature   0, for    it

approaches a uniform distribution.

3 RELATED WORK
Our model can be viewed as a Deep Kalman Filter (Krishnan et al., 2015) with structured inference (Krishnan et al., 2017). In our case, structured inference entails another stochastic variable model with parameter sharing inspired by Karl et al. (2017b) and Karl et al. (2017a) which pointed out the importance of backpropagating the reconstruction error through the transition. We are different to a number of stochastic sequential models like Bayer & Osendorfer (2014); Chung et al. (2015); Shabanian et al. (2017); Goyal et al. (2017) by directly transitioning the stochastic latent variable over time instead of having an RNN augmented by stochastic inputs. Fraccaro et al. (2016) has a transition over both a deterministic and a stochastic latent state sequence, wanting to combine the best of both worlds.
Previous models (Watter et al., 2015; Karl et al., 2017a; Fraccaro et al., 2017) have already combined locally linear models with recurrent Variational Autoencoders, however they provide a weaker structural incentive for learning latent variables determining the transition function. Van Steenkiste et al. (2018) approach a similar multi bouncing ball problem (see section 5.1) by first distributing the representation of different balls into their own entities without supervision and then structurally hardwiring a transition function with interactions based on an attention mechanism.
Recurrent switching linear dynamical systems (Linderman et al., 2016) uses message passing for approximate inference, but has restricted itself to low-dimensional observations and a multi-stage training process. Tackling the problem of propagating state uncertainty over time, various combinations of neural networks for inference and Gaussian processes for transition dynamics have been proposed (Eleftheriadis et al., 2017; Doerr et al., 2018). However, these models have not been demonstrated to work with high-dimensional observation spaces like images. One feature a switching LDS model may learn are interactions which have recently been approached by employing Graph Neural Networks (Battaglia et al., 2016; Kipf et al., 2018). These methods are similar in that they predict edges which encode interactions between components of the state space (nodes).

4 PROPOSED APPROACH

Our goal is to fit a series of continuous state and switching variables to a given sequence of observations. We assume a nonlinear mapping between observations and latent space which we generally approximate by neural networks, apart from the transition which is modeled by a locally linear function. Our graphical inference model is shown in figure 2a and our generative model in figure 1b.

4.1 INFERENCE

4.1.1 STRUCTURED INFERENCE OF CONTINUOUS LATENT STATE

We split our inference model q(zt | zt-1, x1:T , u1:T ) into two parts: 1) transition model qtrans(zt | zt-1, st, ut-1) and 2) inverse measurement model qmeas(zt | xt, ut) as previously proposed in Karl et al. (2017b). These two parts will give us an independent prediction about the new
state zt which will be combined in a manner akin to a Bayesian update in a Kalman Filter.

q(zt | zt-1, xt, ut-1)  qmeas(zt | xt, ut) × qtrans(zt | zt-1, ut-1) = N µq, q2 qmeas(zt | xt, ut) = N µmeas, m2 eas where [µmeas, m2 eas] = f(xt, ut)
qtrans(zt | zt-1, st, ut-1) = N µtrans, t2rans where [µtrans, t2rans] = f(zt-1, st, ut-1)

(7)

3

Under review as a conference paper at ICLR 2019

The densities of qmeas and qtrans are multiplied resulting in another Gaussian density:

µq

=

µtransm2 eas m2 eas

+ +

µmeas t2rans

t2rans

,

q2

=

m2 east2rans m2 eas + t2rans

(8)

This update scheme is highlighted in figure 2b. The transition model is (partially) shared with the
generative transition model p(zt | zt-1, ut-1) that acts as the prior to the approximate posterior q(zt | zt-1, xt, ut-1). Specifically, we share the computation of the transition mean µtrans but not the variance t2rans between inference and generative model. This sharing of variables is essential for good performance as it forces the reconstruction error to be backpropagated through the transition

model.

We found empirically that conditioning the inverse measurement model qmeas(zt | xt, ut) solely on the current observation xt instead of the entire remaining trajectory to lead to better results. We hypothesize that the recurrent model needlessly introduces very high-dimensional and complicated
dynamics which are harder to approximate with our locally linear transition model. The transition model qtrans(zt | zt-1, st, ut-1) is implement as a locally linear model as in (1).

For the initial state z1 we do not have a conditional prior from the transition model as in the rest of the sequence. Other methods (Krishnan et al., 2015) have used a standard normal prior, however this
is not a good fit. We therefore decided that instead of predicting z1 directly to predict an auxiliary variable w that is then mapped deterministically to a starting state z1. A standard Gaussian prior is then applied to w. Alternatively, we could specify a more complex or learned prior for the initial state
like the VampPrior (Tomczak & Welling, 2017). Empirically, this has lead to worse results.

q(w | x1:T , u1:T ) = N w; µw, w2 z1 = f(w)

where [µw, w2 ] = f(x1:T , u1:T )

(9)

While we could condition on the entire sequence, we restrict it to just the first couple of observations.

b2 b3

ut-1

xt

ut-1

xt

w s2 s3 u1 u2
z1 z2 z3

qmeas(st | xt)

qmeas(zt | xt)

st-1 qtrans(st | ·) ×

st

qtrans(zt | ·) ×

...

zt-1 q(st | xt, zt-1, st-1, ut-1) zt-1 q(zt | xt, zt-1, st, ut-1)

x1 x2 x3
(a) Inference model.

p(xt | zt)
(b) High-level overview.

Figure 2: (a) Depicts the inference model. bt is the hidden state of the backward RNN of q(st | xt, ut). Initial inference of w may be conditioned on the entire sequence of observations, or just a subsequence. We've omitted the arrows for sake of clarity for the rest of the graph.
(b) Shows schematically how we combine the transition with the inverse measurement model in the
inference network. Transitions are (partially) shared with the generative model.

4.1.2 INFERENCE OF SWITCHING VARIABLES

Following Maddison et al. (2017) and Jang et al. (2017), we can reparameterize a dis-
crete latent variable with the Gumbel-softmax trick. Again, we split our inference network q(st | st-1, zt-1, x1:T , u1:T ) in an identical fashion into two components: 1) Transition model qtrans(st | st-1, zt-1, ut-1) and 2) inverse measurement model qmeas(st | xt, ut). The transition model here is implemented via a neural network as we require quick changes of dynamics while the
inverse measurement model is parametrized by a backward LSTM. However, for the case of concrete
variables, we cannot do the same Gauss multiplication as in the continuous case. Therefore, we
let each network predict the logits of a Concrete distribution and our inverse measurement model q(st | xt, ut) produces an additional vector , which determines the value of a gate deciding how the two predictions are to be weighted:

q(st | st-1, zt-1, x1:T , u1:T ) = Concrete(, ) with  = trans + (1 - )meas

trans = qtrans(st | st-1, zt-1, ut-1)

[meas, ] = qmeas(st | xt, ut)

(10)

4

Under review as a conference paper at ICLR 2019

Temperature  is set as a hyperparameter and the transition model is again shared (in this case fully shared) with the generative model and acts as a prior. Therefore, if the prior is good enough to explain the next observation,  will be pushed to 1 which ignores the measurement and minimizes the KL between prior and posterior by only propagating the prior. If the prior is not sufficient, information from the inverse measurement model can flow by decreasing  and incurring a KL penalty.

Since the concrete distribution is a relaxation of the categorical, our sample will not be a one-hot vector, but a vector whose elements sum up to 1. We face two options here: we could take a categorical sample by choosing the linear system corresponding to the highest value in the sample (hard forward pass) and only use the relaxation for our backward pass. This, however, means that we will follow a biased gradient. Alternatively, we can use the relaxed version for our forward pass and aggregate the linear systems based on their corresponding weighting (see (13)). Here, we lose the discrete switching of linear systems, but maintain a valid lower bound. We note that the hard forward pass has led to worse results and focus on the soft forward pass for this paper.

Lastly, we could go further away from the theory and instead treat the switching variables also as normally distributed. Our mixing coefficients for linear systems would then be determined by a linear combination of these latent variables:

 = softmax(W st + b)  RM

(11)

Intuitively, this is a normal VAE which acts as a feature detector to choose the transition dynamics. If this worked better than the approach with Concrete variables, it would highlight still existing optimization problems of discrete random variables. Our inference scheme for continuous switching variables is then identical to the one described in the previous section for continuous latent space z. We compare both modeling approaches throughout our experimental section.

4.2 GENERATIVE MODEL Omitting the conditioning on control inputs u, our generative model is described by

p(xt) =

p(xt | zt)p(zt | zt-1, st)p(st | st-1, zt-1)p(zt-1, st-1)

st zt

(12)

which is close to the one of the original SLDS model (see figure 1a). Latent states zt are continuous and represent the state of the system while states st are the switching variables determining the transition (may be modeled by a Concrete or Normal distribution). Differently to the original model, we do not condition the likelihood of the current observation p(xt | zt) directly on the switching variables. This limits the influence of the switching variables to choosing a proper transition dynamic
for the continuous latent space. The likelihood model is parameterized by a neural network with
either a Gaussian or a Bernoulli distribution as output depending on the data.

Transition model p(zt | zt-1, st, ut-1) p(st | st-1, zt-1, ut-1). We follow (1) and maintain a set of M base matrices { A(i), B(i), Q(i) | i. 0 < i < M } as our linear dynamical systems to choose from. Unless we're doing a hard forward pass where we choose exactly one element of this set, we create our final transition matrices by a linear combination of these base matrices:

M
At(st) = s(ti)A(i),
i=1

M
B(st) = s(ti)B(i),
i=1

M

Q(st) =

st(i)Q(i)

i=1

(13)

Both transition models ­ the continuous state transition p(zt | zt-1, st, ut-1) and concrete switching variables transition p(st | st-1, zt-1, ut-1) ­ are (partially) shared with the inference network.

4.3 TRAINING

Our objective function is the commonly used evidence lower bound for our hierarchical model.

L,(x1:T | u1:T )  Eq(z1:T ,s1:T |x1:T ,u1:T )[log p(x1:T | z1:T , s1:T , u1:T )] - DKL(q(z1:T , s1:T | x1:T , u1:T ) || p(z1:T , s1:T | u1:T ))

(14)

5

Under review as a conference paper at ICLR 2019

This can be factorized over time, so the loss for xt becomes:

L,(xt | u1:T ) = Eq(st|st-1,zt-1,x1:T ,u1:T ) Eq(zt|st,zt-1,x1:T ,u1:T )[log p(xt | zt, st)]

(15)

- Est-1 Ezt-1 [DKL(q(st | st-1, zt-1, x1:T , u1:T ) || p(st | st-1, zt-1, ut-1))]

- Ezt-1 [Est [DKL(q(zt | zt-1, st, x1:T , u1:T ) || p(zt | zt-1, st, ut-1))]]

The full derivation can be found in appendix A. We learn the parameters of our model by backpropagation through time and we (generally) approximate the expectations with one sample by using the reparametrization trick. The exception is the KL between two Concrete random variables in which case we take 10 samples for the approximation. For the KL on the switching variables, we further introduce a scaling factor  < 1 (as first suggested in Higgins et al. (2016), although they suggested increasing the KL term) to down weigh its importance. More details on the training procedure can be found in appendix B.2.

5 EXPERIMENTS
In this section, we evaluate our approach on a diverse set of physics and robotics simulations based on partially observable system states or high-dimensional images as observations. We show that our model outperforms previous models and that our switching variables learn meaningful representations.
Models we compare to are Deep Variational Bayes Filter (DVBF) (Karl et al., 2017a), DVBF Fusion (Karl et al., 2017b) (called fusion as they do the same Gauss multiplication in the inference network) which is closest to our model but doesn't have a stochastic treatment of the transition, the Kalman VAE (KVAE) (Fraccaro et al., 2017) and a LSTM (Hochreiter & Schmidhuber, 1997).

(a) Multi agent maze envi- (b) Variable encoding free (c) Variable encoding walls (d) System activation for

ronment.

space for agent 2.

for agent 1.

deterministic transition.

Figure 3: Figures (b) and (c) depict an agent's position colored by the average value of a single latent variable s marginalized over all control inputs u and velocities. Figure (d) highlights a representative activation for a single transition system for the deterministic treatment of the transition dynamics. It doesn't generalize to the entire maze and stays fairly active in proximity to the wall.

5.1 MULTIPLE BOUNCING BALLS IN A MAZE
Our first experiment is a custom 3-agent maze environment simulated with Box2D. Each agent is fully described by its x and y coordinates and its current velocity and has the capability to accelerate in either direction. We learn in a partially observable setting and limit the observations to the agents' positions, therefore x  R6 while the true state space is in R12 and u  R6. First, we train a linear regression model on the latent space z to see if we have recovered a linear encoding of the unobserved velocities. We achieve an R2 score of 0.92 averaged over all agents and velocity directions.
Our focus shifts now to our switching variables which we expect to encode interactions with walls. We provide a visual confirmation of that in figure 3 where we see switching variables encoding all space where there is no interaction in the next time step, and variables which encode walls, distinguishing between vertical and horizontal ones. In figure 3d one can see show that if the choice of locally linear transition is treated deterministically, we don't learn global features of the same kind. To confirm our visual inspection, we train a simple decision tree based on latent space s in order to predict interaction with a wall. Here, we achieve an F1 score of 0.46. It is difficult to say what a good value should look like as collisions with low velocity are virtually indistinguishable from no collision.
6

Under review as a conference paper at ICLR 2019

Figure 4: First row: data, second row: filtered reconstructions, third row: predictions. The first 4 steps are used to find a stable starting state, predictions start with step 5.

We compare our prediction quality to several other methods in table 1 where we outperform all of our chosen baselines. Also, modeling switching variables by a Normal distribution outperforms the Concrete distribution in all of our experiments. Aside from known practical issues with training a discrete variable via backpropagation, we explore one reason why that may be in section 5.4, which is the greater susceptibility to the scale of temporal discretization. We provide plots of predicted trajectories in appendix D. Transitioning multiple agents with a single transition matrix comes with scalability issues with regards to switching dynamics which we explore further in appendix C.
Table 1: Mean squared error (MSE) on predicting future observations. Static refers to constantly predicting the first observation of the sequence.

PREDICTION STEPS
STATIC LSTM DVBF DVBF FUSION OURS (CONCRETE) OURS (NORMAL)

1
5.80E-02 3.07E-01 1.10E-01 4.90E-03 1.06E-02 3.39E-03

REACHER
5
5.36E-01 7.76E-01 3.08E-01 2.97E-02 5.73E-02 1.85E-02

10
1.25E+00 1.22E+00 6.07E-01 8.25E-02 1.56E-01 4.97E-02

3-BALL MAZE

1 5 10

1.40E-02 7.20E-02 6.20E-02 4.33E-03 2.28E-03 1.30E-03

5.74E-01 1.58E-01 1.36E-01 2.03E-02 1.22E-02 5.52E-03

2.65E+00 2.60E-01 1.82E-01 4.88E-02 3.40E-02 1.38E-02

5.2 REACHER
We then evaluate our model on the Roboschool reacher environment. To make things more interesting, we learn only on partial observations, removing time derivative information (velocities), leaving us with just the positions or angles of various joints as observations. Table 1 shows a comparison of various methods on predicting the next couple of time steps. One critical point is the possible collision1 between lower and upper joint which is one we'd like our model to capture. We again learn a linear classifier based on latent space s to see if this is successfully encoded and reach an F1 score of 0.46.
5.3 BALL IN A BOX ON IMAGE DATA
Finally, we evaluate our method on high-dimensional image observations using the single bouncing ball environment used by Fraccaro et al. (2017). They simulated 5000 sequences of 20 time steps each of a ball moving in a two-dimensional box, where each video frame is a 32 × 32 binary image. There are no forces applied to the ball, except for the fully elastic collisions with the walls. Initial position and velocity are randomly sampled.
In figure 5a we compare our model to both the smoothed and generative version of the KVAE. The smoothed version receives the final state of the trajectory after the n predicted steps which is fed into the smoothing capability of the KVAE. One can see that our model learns a better transition model, even outperforming the smoothed KVAE for longer sequences. For short sequences, KVAE performs better which highlights the value of it disentangling the latent space into separate object and dynamics representation. A sample trajectory is plotted in figure 4.
1We roughly identify a collision to be the point where the lower joint decelerates by over a fixed value of 2.
7

Under review as a conference paper at ICLR 2019

5.4 SUSCEPTIBILITY TO THE SCALE OF TEMPORAL DISCRETIZATION
In this section, we'd like to explore how the choice of t when discretizing a system influences our results. In particular, we'd expect our model with discrete (concrete) switching latent variables to be more susceptible to it than when modeled by a continuous distribution. This is because in the latter case the switching variables can scale the various matrices more freely, while in the former scaling up one system necessitates scaling down another. For empirical comparison, we go back to our custom maze environment (this time with only one agent as this is not pertinent to our question at hand) and learn the dynamics on various discretization scales. Then we compare the absolute error's growth for both approaches in figure 5b which supports our hypothesis. While the discrete approximation even outperforms for small t, there is a point where it rapidly becomes worse and gets overtaken by the continuous approximation. This suggests that t was simply chosen to be too large in both the reacher and the ball in a box with image observations experiment.

Fraction of incorrect pixels Absolute Error (log scale)

·10-2 KVAE (smoothed)
2.5 KVAE (generative) Ours (generative)
2
1.5
1
0.5

10-1

Normal Concrete

10-2

0 2 4 6 8 10 12 14 16 steps predicted into the future
(a) Fraction of incorrectly predicted pixels.

10-2  t (log scale)

10-1

(b) Discretization scale susceptibility.

Figure 5: (a) Our dynamics model is outperforming even the smoothed KVAE for longer trajectories. (b) Modeling switching variables as Concrete random variables scales less favorably.

6 DISCUSSION
We want to emphasize some subtle differences to previously proposed architectures that make an empirical difference, in particular for the case when st is chosen to be continuous. In Watter et al. (2015) and Karl et al. (2017a), the latent space is already used to draw transition matrices, however they do not extract features such as walls or joint constraints. There are a few key differences from our approach. First, our latent switching variables st are only involved in predicting the current observation xt through the transition selection process. The likelihood model therefore doesn't need to learn to ignore some input dimensions which are only helpful for reconstructing future observations but not the current one. There is also a clearer restriction on how st and zt may interact: st may now only influence zt by determining the dynamics, while previously zt influenced both the choice of transition function as well as acted inside the transition. These two opposing roles lead to conflicting gradients as to what should be improved. Furthermore, the learning signal for st is rather weak so that scaling down the KL-regularization was necessary to detect good features. Lastly, a (locally) linear transition may not be a good fit for variables determining dynamics as such variables may change very abruptly.
7 CONCLUSION
We have shown that our construction of using switching variables encourages learning a richer and more interpretable latent space. In turn, the richer representation led to an improvement of simulation accuracy in various tasks. In the future, we'd like to look at other ways to approximate the discrete switching variables and exploit this approach for model-based control on real hardware systems. Furthermore, addressing the open problem of disentangling latent spaces is essential to fitting simple dynamics and would lead to significant improvements of this approach.
8

Under review as a conference paper at ICLR 2019
REFERENCES
G Ackerson and K Fu. On state estimation in switching environments. IEEE Transactions on Automatic Control, 15(1):10­17, 1970.
D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pp. 4502­4510, 2016.
Justin Bayer and Christian Osendorfer. Learning stochastic recurrent networks. arXiv preprint arXiv:1411.7610, 2014.
Chaw-Bing Chang and Michael Athans. State estimation for discrete systems with switching parameters. IEEE Transactions on Aerospace and Electronic Systems, (3):418­425, 1978.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pp. 2980­2988, 2015.
Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc Toussaint, and Sebastian Trimpe. Probabilistic recurrent state-space models. arXiv preprint arXiv:1801.10395, 2018.
Stefanos Eleftheriadis, Tom Nicholson, Marc Deisenroth, and James Hensman. Identification of gaussian process state space models. In Advances in Neural Information Processing Systems, pp. 5309­5319, 2017.
Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. Sequential neural models with stochastic layers. In Advances in neural information processing systems, pp. 2199­2207, 2016.
Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition and nonlinear dynamics model for unsupervised learning. In Advances in Neural Information Processing Systems, pp. 3604­3613, 2017.
Anirudh Goyal, Alessandro Sordoni, Marc-Alexandre Côté, Nan Ke, and Yoshua Bengio. Z-forcing: Training stochastic recurrent networks. In Advances in Neural Information Processing Systems, pp. 6713­6723, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Irina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles Blundell, Shakir Mohamed, and Alexander Lerchner. Early Visual Concept Learning with Unsupervised Deep Learning. 2016. URL http://arxiv.org/abs/1606.05579.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical Reparameterization with Gumbel-Softmax. International Conference on Learning Representations, pp. 1­13, nov 2017. URL http:// arxiv.org/abs/1611.01144.
Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data. In Proceedings of the International Conference on Learning Representations (ICLR), 2017a.
Maximilian Karl, Maximilian Soelch, Philip Becker-Ehmck, Djalel Benbouzid, Patrick van der Smagt, and Justin Bayer. Unsupervised real-time control through variational empowerment. arXiv preprint arXiv:1710.05101, 2017b.
9

Under review as a conference paper at ICLR 2019
D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014.
T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel. Neural relational inference for interacting systems. In Proceedings of the 35th International Conference on Machine Learning, 2018.
Rahul G. Krishnan, Uri Shalit, and David Sontag. Deep Kalman Filters. arXiv preprint arXiv:1511.05121, (2000):1­7, 2015. URL http://arxiv.org/abs/1511.05121.
Rahul G. Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state space models. In AAAI, pp. 2101­2109, 2017.
Scott W. Linderman, Andrew C. Miller, Ryan P. Adams, David M. Blei, Liam Paninski, and Matthew J. Johnson. Recurrent switching linear dynamical systems. 2016. URL http://arxiv.org/ abs/1610.08466.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. In Proceedings of the International Conference on Learning Representations (ICLR), pp. 1­17, 2017. ISBN 0780365402. URL http://arxiv. org/abs/1611.00712.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, ICML'14, pp. II­1278­ II­1286. JMLR.org, 2014.
Samira Shabanian, Devansh Arpit, Adam Trischler, and Yoshua Bengio. Variational Bi-LSTMs. 2017. URL http://arxiv.org/abs/1711.05717.
Jakub M Tomczak and Max Welling. Vae with a vampprior. arXiv preprint arXiv:1705.07120, 2017. Sjoerd van Steenkiste, Michael Chang, Klaus Greff, and Jürgen Schmidhuber. Relational neural
expectation maximization: Unsupervised discovery of objects and their interactions. In Proceedings of the International Conference on Learning Representations (ICLR), 2018. Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pp. 2746­2754, 2015.
10

Under review as a conference paper at ICLR 2019

A LOWER BOUND DERIVATION

For brevity we omit conditioning on control inputs u1:T .

log p(xT ) = log
z1:T

s1:T

q(s1:T , z1:T

|

x1:T

)

p

(x1:T | z1:T )p q(s1:T , z1:T

(z1:T , | x1:T

s1:T )

)



z1:T

s1:T

q(s1:T , z1:T

| x1:T ) log

p(x1:T | z1:T )p(z1:T , s1:T ) q(s1:T , z1:T | x1:T )

T

= Est [Ezt [p(xt | zt, st)]] - DKL(q(z1:T , s1:T | x1:T ) || p(z1:T , s1:T ))

t=1

A.1 FACTORIZATION OF THE KL DIVERGENCE
The dependencies on data xT and uT as well as parameters  and  are omitted in the following for convenience.

DKL(q(z1, s2, . . . , sT , zT ) || p(z1, s2, . . . , sT , zT )) (Factorization of the variational approximation)

= ···

q(z1)q(s2 | z1) . . . q(sT | zT -1, sT -1)q(zT | zT -1, sT )

z1 s2

sT zT

log q(z1)q(s2 | z1) . . . q(sT | zT -1, sT -1)q(zT | zT -1, sT ) p(z1, s2, . . . , sT , zT )

(Factorization of the prior)

= ···

q(z1)q(s2 | z1) . . . q(sT | zT -1, sT -1)q(zT | zT -1, sT )

z1 s2

sT zT

log

q(z1)q(s2 p(z1)p(s2

| |

z1) . . . q(sT z1) . . . p(sT

| |

zT -1, sT -1)q(zT zT -1, sT -1)p(zT

| |

zT -1, sT ) zT -1, sT )

(Expanding the logarithm by the product rule)

=

z1

q(z1)

log

q(z1) p(z1)

+

z1

s1

q(z1)q(s1

|

z1)

log

q(s1 p(s1

| |

z1) z1)

T
+
t=2

z1

···
s2 sT

zT

q(z1)q(s2

|

z1) . . . q(zT

|

zT -1, sT ) log

q(zt p(zt

| |

zt-1, st) zt-1, st)

T
+
t=3

z1

···
s2 sT

zT

q(z1)q(s2

|

z1) . . . q(zT

|

zT -1, sT ) log

q(st p(st

| |

zt-1, st-1) zt-1, st-1)

(Ignoring constants)

= DKL(q(z1) || p(z1)) + Ez1q(z1)[DKL(q(s2 | z1) || p(s2 | z1))]
T -1
+ Est,zt-1 [DKL(q(zt | zt-1, st) || p(zt | zt-1, st))]
t=2 T -1
+ Est-1,zt-1 [DKL(q(st | zt-1, st-1) || p(st | zt-1, st-1))]
t=3

11

Under review as a conference paper at ICLR 2019

Table 2: Dimensionality of environments.

Dimensionality of
Reacher Hopper Multi Agent Maze Image Ball in Box

Observation Space
7 8 4 32 × 32

Control Input Space
2 3 6 0

Ground Truth State Space
9 15 12 4

B DETAILS OF THE EXPERIMENTAL SETUP
B.1 ENVIRONMENTS
B.1.1 ROBOSCHOOL REACHER
To generate data, we follow a Uniform distribution U  [-1, 1] as the exploration policy. Before we record data, we take 20 warm-up steps in the environment to randomize our starting state. We take the data as is without any other preprocessing.
B.1.2 MULTI AGENT MAZE
Observations are normalized to be in [-1, 1]. Both position and velocity is randomized for the starting state. We again follow a Uniform distribution U  [-1, 1] as the exploration policy.
B.2 TRAINING
Overall, training the Concrete distribution has given us the biggest challenge as it was very susceptible to various hyperparameters. We made use of the fact that we can use a different temperature for the prior and approximate posterior (Maddison et al., 2017) and we do independent hyperparameter search over both. For us, the best values were 0.75 for the posterior and 2 for the prior. Additionally, we employ an exponential annealing scheme for the temperature hyperparameter of the Concrete distribution. This leads to a more uniform combination of base matrices early in training which has two desirable effects. First, all matrices are scaled to a similar magnitude, making initialization less critical. Second, the model initially tries to fit a globally linear model, leading to a good starting state for optimization. We also tried increasing the number of samples taken (up to 100) to approximate the KL between the Concrete distributions, however we have not observed an improvement of performance. We therefore restrict ourselves to 10 samples for all experiments.
In all experiments, we train everything end-to-end with the ADAM optimizer.(Kingma & Ba, 2015) We start with learning rate of 5e-4 and use an exponential decay schedule with rate 0.97 every 2000 iterations.
B.3 NETWORK ARCHITECTURE
For most networks, we use MLPs implemented as residual nets (He et al., 2016) with ReLU activations.
Networks used for the reacher and maze experiments.
· qmeas(zt | ·): MLP consisting of two residual blocks with 256 neurons each. We only condition on the current observation xt although we could condition on the entire sequence. This decision was taken based on empirical results.
· qtrans(zt | ·): In the case of Concrete random variables, we just combine the base matrices and apply the transition dynamics to zt-1. For the Normal case, the combination of matrices is preceded by a linear combination with softmax activation. (see equation 11)
· qmeas(st | ·): is implemented by a backward LSTM with 256 hidden units. We reuse the preprocessing of qmeas(zt | xt) and take the last hidden layer of that network as the input to the LSTM.
12

Under review as a conference paper at ICLR 2019

· qtrans(st | ·): MLP consisting of one residual block with 256 neurons. · qinitial(w | ·): MLP consisting of two residual block with 256 neurons optionally followed
by a backward LSTM. We only condition on the first 3 or 4 observations for our experiments. · qinitial(s2): The first switching variable in the sequence has no predecessor. We there-
fore require a replacement for qtrans(st | ·) in the first time step, which we achieve by independently parameterizing another MLP. · p(xt | zt): MLP consisting of two residual block with 256 neurons. · p(zt | ·): Shared parameters with qtrans(zt | ·). · p(st | ·): Shared parameters with qtrans(st | ·).
We use the same architecture for the image ball in a box experiment, however we increase number of neurons of qmeas(zt | ·) to 1024.
B.4 HYPERPARAMETERS
Table 3: Overview of hyperparameters.

# episodes episode length batch size dimension of z dimension of s posterior temperature prior temperature temperature annealing steps temperature annealing rate  (KL-scaling of switching variables)

Multi Agent Maze
50000 20 256 32 16 0.75 2 100 0.97 0.1

Reacher
20000 30 128 16 8 0.75 2 100 0.97 0.1

Image Ball in Box
5000 20 256 8 8 0.67 2 100 0.98 0.1

C ON SCALING ISSUES OF SWITCHING LINEAR DYNAMICAL SYSTEMS
Let's consider a simple representation of a ball in a rectangular box where its state is represented by its position and velocity. Given a small enough t, we can approximate the dynamics decently by just 3 systems: no interaction with the wall, interaction with a vertical or horizontal wall (ignoring the corner case of interacting with two walls at the same time). Now consider the growth of required base systems if we increase the number of balls in the box (even if these balls cannot interact with each other). We would require a system for all combinations of a single ball's possible states: 32. This will grow exponentially with the number of balls in the environment.
One way to alleviate this problem that requires only a linear growth in base systems is to independently turn individual systems on and off and let the resulting system the sum of all activated systems. A base system may then represent solely the transition for a single ball being in specific state, while the complete system is then a combination of N such systems where N is the number of balls. Practically, this can be achieved by replacing the softmax by a sigmoid activation function or by replacing the categorical variable s of dimension M by M Bernoulli variables indicating whether a single system is active or not. We do this for our multiple agents in a maze environment.
Theoretically, a preferred approach would be to disentangle multiple systems (like balls, joints) and apply transitions only to their respective states. This, however, would require a proper and unsupervised separation of (mostly) independent components. We defer this to future work.

13

Under review as a conference paper at ICLR 2019
D FURTHER RESULTS
D.1 3-AGENT MAZE
Figure 6: Comparison of actual and predicted 20 step trajectories. The diamond marker denotes the starting position of a trajectory. D.2 IMAGE BALL IN A BOX
Figure 7: First row: data, second row: reconstructions, third row: predictions. The first 4 steps are used to find a stable starting state, predictions start with step 5.
14

