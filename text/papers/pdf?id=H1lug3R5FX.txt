Under review as a conference paper at ICLR 2019
ON THE GEOMETRY OF ADVERSARIAL EXAMPLES
Anonymous authors Paper under double-blind review
ABSTRACT
Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.
1 INTRODUCTION
Deep learning at scale has led to breakthroughs on important problems in computer vision (Krizhevsky et al. (2012)), natural language processing (Wu et al. (2016)), and robotics (Levine et al. (2015)). Shortly thereafter, the interesting phenomena of adversarial examples was observed. A seemingly ubiquitous property of machine learning models where perturbations of the input that are imperceptible to humans reliably lead to confident incorrect classifications (Szegedy et al. (2013); Goodfellow et al. (2014)). What has ensued is a standard story from the security literature: a game of cat and mouse where defenses are proposed only to be quickly defeated by stronger attacks (Athalye et al. (2018)). This has led researchers to develop methods which are provably robust under specific attack models (Madry et al. (2018); Wong & Kolter (2018); Sinha et al. (2018); Raghunathan et al. (2018)). As machine learning proliferates into society, including security-critical settings like health care (Esteva et al. (2017)) or autonomous vehicles (Codevilla et al. (2018)), it is crucial to develop methods that allow us to understand the vulnerability of our models and design appropriate counter-measures.
In this paper, we propose a geometric framework for analyzing the phenomenon of adversarial examples. We leverage the observation that datasets encountered in practice exhibit low-dimensional structure despite being embedded in very high-dimensional input spaces. This property is colloquially referred to as the "Manifold Hypothsis": the idea that low-dimensional structure of `real' data leads to tractable learning. We model data as being sampled from class-specific low-dimensional manifolds embedded in a high-dimensional space. We consider a threat model where an adversary may choose any point on the data manifold to perturb by in order to fool a classifier. In order to be robust to such an adversary, a classifier must be correct everywhere in an -tube around the data manifold. Observe that, even though the data manifold is a low-dimensional object, this tube has the same dimension as the entire space the manifold is embedded in. Our analysis argues that adversarial examples are a natural consequence of the difference between the relative ease of learning a low-dimensional, brittle, decision boundary that is correct on realistic data and the difficulty in learning a high-dimensional, robust, decision boundary. The high codimension, the difference between the dimension of the data manifold and the dimension of the embedding space, is a key source of the pervasiveness of adversarial examples.
Our paper has three parts. First, we develop a geometric framework, inspired by the manifold reconstruction literature, that formalizes the manifold hypothesis described above and our attack model.
1

Under review as a conference paper at ICLR 2019

M2 
M1 rch2 

M1 

M2

rch2 

 M1

M2

rch2 

Figure 1: Examples of the decision axis , shown here in green, for different data manifolds. Intuitively, the decision axis captures an optimal decision boundary between the data manifolds. It's optimal in the sense that each point on the decision axis is as far away from each data manifold as possible. Notice that in the first example, the decision axis coincides with the maximum margin line.

Second, we apply this framework to analyze simplified data manifolds and prove the following results: (1) we show that the choice of norm to restrict an adversary is important in that there exists a tradeoff between being robust to different norms: we present a classification problem where improving robustness under the ·  norm requires a loss of (1 - 1/ d) in robustness to the · 2 norm; (2) we show that a common approach, training against adversarial examples drawn from balls around the training set, is insufficient to learn robust decision boundaries with realistic amounts of data; and (3) we show that nearest neighbor classifiers do not suffer from this insufficiency, due to geometric properties of their decision boundary away from data, and thus represent a potentially robust classification algorithm. Our final contribution is an experimental validation of these results. We use the decision boundary learned by Wong & Kolter (2018) to empirically validate a tradeoff in robustness and we provide datasets where varying the codimension also varies the robustness to adversarial examples under standard attack models like FGSM, BIM, and PGD. We verify the nearest neighbor result on the MNIST dataset and find that is it substantially more robust than standard neural network training and is comparable with the state-of-the-art approach of Madry et al. (2018).
2 RELATED WORK
This paper approaches the problem of adversarial examples using techniques and intuition from the manifold reconstruction literature. Both fields have a great deal of prior work, so we focus on only the most related papers here.
2.1 ADVERSARIAL EXAMPLES
Some previous work has considered the relationships between adversarial examples and high dimensional geometry. Franceschi et al. (2018) explore the robustness of classifiers to random noise in terme of distance to the decision boundary, under the assumption that the decision boundary is locally flat. The contemporaneous work of Gilmer et al. (2018) experimentally evaluated the setting of two concentric under-sampled 499-spheres embedded in R500, and concluded that adversarial examples occur on the data manifold. In contrast, we present a geometric framework for proving robustness guarantees for learning algorithms, that makes no assumptions on the decision boundary. We carefully sample the data manifold in order to highlight the importance of codimension; adversarial examples exist even when the manifold is perfectly classified. Additionally we explore the importance of the spacing between the constituent data manifolds, sampling requirements for learning algorithms, and the relationship between model complexity and robustness.
The decision and medial axes defined in Section 3 are maximum margin decision boundaries. Hard margin SVMs define define a linear separator with maximum margin, maximum distance from the training data (Cortes & Vapnik (1995)). Kernel methods allow for maximum margin decision boundaries that are non-linear by using additional features to project the data into a higher-dimensional feature space (Shawe-Taylor & Cristianini (2004)). The decision and medial axes generalize the notion of maximum margin to account for the arbitrary curvature of the data manifolds. There have
2

Under review as a conference paper at ICLR 2019
been attempts to incorporate maximum margins into deep learning (Sun et al. (2016); Liu et al. (2016); Liang et al. (2017); Elsayed et al. (2018)), often by designing loss functions that encourage large margins at either the output (Sun et al. (2016)) or at any layer (Elsayed et al. (2018)). In contrast, the decision axis is defined on the input space and we use it as an analysis tool for proving robustness guarantees.
2.2 MANIFOLD RECONSTRUCTION
Manifold reconstruction is the problem of discovering the structure of a k-dimensional manifold embedded in Rd, given only a set of points sampled from the manifold. A large vein of research in manifold reconstruction develops algorithms that are provably good: if the points sampled from the underlying manifold are sufficiently dense, these algorithms are guaranteed to produce a geometrically accurate representation of the unknown manifold with the correct topology. The output of these algorithms is often a simplicial complex, a set of simplices such as triangles, tetrahedra, and higher-dimensional variants, that approximate the unknown manifold. In particular these algorithms output subsets of the Delaunay triangulation, which along with their geometric dual the Voronoi diagram, have properties that aid in proving geometric and topological guarantees (Edelsbrunner & Shah (1997)).
The field first focused on curve reconstruction in R2 (Amenta et al. (1998)) and subsequently in R3 (Dey & Kumar (1999)). Soon after algorithms were developed for surface reconstruction in R3, both in the noise-free setting (Amenta & Bern (1999); Amenta et al. (2002)) and in the presence of noise (Dey & Goswami (2004)). We borrow heavily from the analysis tools of these early works, including the medial axis and the reach. However we emphasize that we have adapted these tools to the learning setting. To the best of our knowledge, our work is the first to consider the medial axis under different norms.
In higher-dimensional embedding spaces (large d), manifold reconstruction algorithms face the curse of dimensionality. In particular, the Delaunay triangulation, which forms the bedrock of algorithms in low-dimensions, of n vertices in Rd can have up to (n d/2 ) simplices. To circumvent the curse of dimensionality, algorithms were proposed that compute subsets of the Delaunay triangulation restricted to the k-dimensional tangent spaces of the manifold at each sample point (Boissonnat & Ghosh (2014)). Unfortunately, progress on higher-dimensional manifolds has been limited due to the presence of so-called "sliver" simplices, poorly shaped simplices that cause in-consistences between the local triangulations constructed in each tangent space (Cheng et al. (2005); Boissonnat & Ghosh (2014)). Techniques that provably remove sliver simplices have prohibitive sampling requirements (Cheng et al. (2000); Boissonnat & Ghosh (2014)). Even in the special case of surfaces (k = 2) embedded in high dimensions (d > 3), algorithms with practical sampling requirements have only recently been proposed (Khoury & Shewchuk (2016)). Our use of tubular neighborhoods as a tool for analysis is borrowed from Dey et al. (2005) and Khoury & Shewchuk (2016).
In this paper we are interested in learning robust decision boundaries, not reconstructing the underlying data manifolds, and so we avoid the use of Delaunay triangulations and their difficulties entirely. In Section 6 we present robustness guarantees for two learning algorithms in terms of a sampling condition on the underlying manifold. These sampling requirements scale with the dimension of the underlying manifold k, not with the dimension of the embedding space d.
3 THE GEOMETRY OF DATA
We model data as being sampled from a set of low-dimensional manifolds embedded in highdimensional space. A k-dimensional manifold M  Rd is a topological space with the defining property that, at every point x  M, there exists a neighborhood U  M containing x such that there exists a homeomorphism h : U  Rk (continuous bijection with continuous inverse). Informally M locally "looks like" Rk, even though its global topology may be very different from Rk. A sphere is an example of a 2-manifold; in a small enough region around each point, a sphere looks like R2. The special case of 1-manifolds are called curves and 2-manifolds surfaces, and for clarity we will use the simplest word hereafter.
The codimension of M is d - k, the difference between the dimension of the manifold and the dimension of the embedding space. At x  M, the tangent space TxM at x is a k-dimensional
3

Under review as a conference paper at ICLR 2019
linear subspace while the normal space NxM is a (d - k)-dimensional linear subspace. In this language, the "Manifold Hypothesis" states that in practice, data is often sampled from manifolds with high codimension.
In this paper we are primarily interested in the classification problem. Thus we model data as being sampled from C distinct manifolds {M1, . . . , MC}, one for each class. When we wish to refer to the entire space from which a dataset is sampled, we refer to M = i[C]Mi. A finite sample X  Rn×d of size n with labels y  [C]n is sampled from M if each sample Xi (the ith row of X) lies on or near Myi , the data manifold corresponding to class yi. What we mean by "near" will be defined rigorously below.
Consider a ball B centered at some point c  Rd and imagine growing B by increasing the radius. For nearly all starting points c, the ball B eventually intersects one, and only one, of the Mi's. Thus the nearest point to c on M lies on Mi. (Note that the nearest point on Mi need not be unique.) Define the decision axis  of M as the set of points c such that the boundary of B intersects two or more of the Mi, but the interior of B does not intersect any of the manifolds; that is the decision axis  is the set of points that have two or more closest points on distinct manifolds. See Figure 1. The decision axis is inspired by the medial axis, which was first proposed by Blum (1967) in the context of image analysis and subsequently modified for the purposes of curve and surface reconstruction by Amenta et al. (1998; 2002). We have modified the definition to account for multiple constituent manifolds each of a different class and have renamed our variant in order to avoid confusion in the future.
The decision axis  can intuitively be thought of as a decision boundary that is optimal in the following sense. First,  separates the constituent manifolds when they do not intersect (Lemma 5). Second, each point of  is as far away from the constituent manifolds as possible. As shown in the leftmost example in Figure 1, in the case of two linearly separable circles of equal radius, the decision axis is exactly the line that separates the data with maximum margin. For arbitrary manifolds,  generalizes the notion of maximum margin to account for the arbitrary curvature of the constituent manifolds.
Let T  Rd be any set. The reach rchp (T ; M) of M is defined as infxM,y x - y p. When M is compact, the reach is achieved by the point on M that is closest to T under the · p norm. We will drop M from the notation when it is understood from context.
Finally, an -tubular neighborhood of M is defined as M = {x  Rd : d(x, M)  }. That is, M is the set of all points whose distance to M under some metric d(·, ·) is less than . Note that while M is k-dimensional, M is always d-dimensional. Tubular neighborhoods are how we rigorously define adversarial examples. Consider a classifier f : Rd  [C] for M. An -adversarial example is a point x  Mi such that f (x) = i. A classifier f is robust to all -adversarial examples when f correctly classifies not only M, but all of M . Thus the problem of being robust to adversarial examples is rightly seen as one of generalization. In this paper we will be primarily concerned with exploring the conditions under which we can provably learn a decision boundary that correctly classifies M . When < rch M, the decision axis  is one decision boundary that correctly classifies M (Corollary 7).
We place no restriction on the choice of distance metric d(·, ·). So long as everything is defined consistently, d(·, ·) can be any metric, including the metrics induced by the norms · 2 or · . The decision axis under · 2 is in general not identical to the decision axis under · . We will use 2 and  when we wish to make the distinction clear. In Section 4 we will prove that since 2 is not identical to  there exists a tradeoff in the robustness of any decision boundary between the two norms.
4 A PROVABLE TRADEOFF IN ROBUSTNESS BETWEEN NORMS
Schott et al. (2018) explore the vulnerability of robust classifiers to attacks under different norms. In particular, they take the robust pretrained classifier of Madry et al. (2018), which was trained to be robust to · -perturbations, and subject it to · 0 and · 2 attacks. They show that accuracy drops to 0% under · 0 attacks and to 35% under · 2. Here we explain why poor robustness under the norm · 2 should be expected.
4

Under review as a conference paper at ICLR 2019

Accuracy
1.0 0.8 0.6 0.4 0.2 0.0
0.0

Accuracy
1.0

Robustness Tradeoff with FGSM Attack

Robustness Tradeoff with FGSM Attack

0.8 0.6 0.4

d=2 d=3 d=4

0.2 d=2 d=3 d=4
0.0 0.0 0.2 0.4 0.6 0.8 1.0
Figure 2: As the dimension increases, the rch2 (; S1  S2) decreases, and so an ·  robust classifier is less robust to · 2 attacks. The dashed lines are placed at 1/ d, where our theoretical 0.2 0.4 results s0.u6 ggest we0.8 should1.s0tart finding · 2 adversarial examples. We use the robust ·  loss of Wong & Kolter (2018)

We say a decision boundary Df for a classifier f is -robust in the · p norm if < rchp Df . In words, starting from any point x  M, a perturbation x must have p-norm greater than rchp Df to cross the decision boundary.
Consider two concentric d-spheres S1, S2  Rd+1 with radii r1 < r2 respectively, and let S = S1  S2. The most robust decision boundary to · p-perturbations is p. The decision axis under
· 2, 2, is just the d-sphere with radius (r1 + r2)/2. However,  is not identical to 2 in this setting; in fact most  of approaches S1 as d increases.
The geometry of a · -ball B centered at m  Rd with radius  is that of a hypercube centered at m with side length 2. To find a point on  we place B tangent to the north pole q of S1 so that the corners of B touch S2. The north pole has coordinate representation q = (0, . . . , 0, r1), the center m = (0, . . . , 0, r1 + ), and a corner of B can be expressed as p = (, . . . , , r1 + 2). Additionally we have the constraint that p 2 = r2 since p  S2. Then we can solve for  as

r22 =

p

2 2

= (d - 1)2 + (r1 + 2)2

= (d + 3)2 + 4r1 + r12;

 = -2r1 +

r12

+ d

3r22 +3

+

d(r22

-

r12)

,

where the last step follows from the quadratic formula and the fact that  > 0. For fixed r1, r2, the value  scales as O(1/ d).
From this we conclude that the minimum distance from S1 to  under the · 2 norm is upper bounded as rch2   O(rch2 2/ d). If a classifier f is trained to learn , an adversary, starting on S1, can construct an · 2 adversarial example for a perturbation as small as O(1/ d). Thus we should expect f to be less robust to · 2-perturbations. Figure 2 verifies this result experimentally.
Schott et al. (2018) state that "the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L metric (its highly susceptible to L2 and L0 perturbations)" (emphasis ours). We disagree; the Madry et al. (2018) classifier performed exactly as intended. It learned a decision boundary that is robust under · , which we have shown is quite different from the most robust decision boundary under · 2.
Interestingly, the proposed models of Schott et al. (2018) also suffer from this tradeoff. Their model ABS has accuracy 80% to · 2 attacks but drops to 8% for · . Similarly their model ABS Binary has accuracy 77% to ·  attacks but drops to 39% for · 2 attacks.
No single decision boundary can be optimally robust in all norms.

5

Under review as a conference paper at ICLR 2019


p k
2

 

µ`

Lower Bound

Figure 3: Left: To construct an -cover we place sample points, shown here in black, along a regular grid with spacing . The blue points are the furthest points of  from the sample. To cover  we need  = 2/ k. Right: An illustration of the lower bound technique used in Equation 2. The volume vol  shown in the black dashed lines, is bounded from below by placing a (d - k)dimensional ball of radius  at each point of , shown in green. In this illustration, a 1-dimensional
manifold is embedded in 2 dimensions, so these balls are 1-dimensional line segments.

5 X IS A POOR MODEL OF M

Madry et al. (2018) suggest training a robust classifier with the help of an adversary which, at
each iteration, produces -perturbations around the training set that are incorrectly classified. In our notation, this corresponds to learning a decision boundary that correctly classifies X = {x  Rd : d(x, Xi)  for some training point Xi}. We believe this approach is insufficiently robust in practice, as X is often a poor model for M . In this section, we show that there are very simple settings where the volume vol X is a vanishingly small percentage of vol M . In Section 7.1 we experimentally verify these observations by showing that in high-dimensional space it is easy to find
adversarial examples even after training against a strong adversary.

Define  = {x  Rd :  x1, . . . , xk  µ and xk+1 = . . . = xd = 0}; that is  is a subset of the x1-. . .-xk-plane bounded between the coordinates [ , µ]. A -cover of a manifold M in the norm
· 2 is a finite set of points X such that for every x  M there exists Xi such that x - Xi 2  . It is easy to construct an explicit -cover X of : place sample points at the vertices of a regular

grid, shown in Figure reffig:grid by the black vertices. The centers of the cubes of this regular grid,

shown in blue in Figure 3, are the furthest points from the samples. The distance from the vertices

of the grid to the centers is k/2 where  is the spacing between points alongan axis of the grid.

To construct a -cover we need k/2 =  which gives a spacing of  = 2/ k. The size of this

sample is |X| =

 k(µ- ) 2

k
. Note that |X| scales exponentially in k, the dimension of , not in

d, the dimension of the embedding space.

Recall that  is the -tubular neighborhood of . The -balls around X, which comprise X, cover  and so any robust approach that guarantees correct classification within X will achieve perfect accuracy on . However, we will show that X covers only a vanishingly small fraction of . An

upper bound on the volume is

vol X



vol B|X|

=

d/2

(

d 2

+

1)

d

 k(µ - ) 2

k

=

d/2

(

d 2

+

1)

(d-k)

k

k(µ - ) 2

. (1)

Next we bound the volume vol  from below. Intuitively, a lower bound on the volume can be derived by placing a (d - k)-dimensional ball in the normal space at each point of  and integrating
the volumes. Figure 3 illustrates the technique.

vol 



vold-k Bd-k

volk 

=

(d-k)/2



d-k 2

+1

d-k(µ -

)k .

(2)

6

Under review as a conference paper at ICLR 2019

Ratio
1.0 0.8 0.6 0.4 0.2
0 20

Ratio
1.0

Sample Size
800

0.8 0.6 0.4 0.2
0 40 60

20 40 60
80 100 Dimension

k=1 k=2 k=3
80

Sample
800

Sizek=1

k=2

600 k=3

600 400

400 200

200 100

20 40 60

20 40 60
80 100 Codimension

1-Sample of  Lower Bound for  1-Sample of  Lower Bound for 
80 100

Figure 4: We plot the lower bound in Equation 3 on the left. As the dimension of the embedding space increases, the percentage of volume not covered by 1-balls around the 1-sample approaches 1. On the right we plot the number of samples necessary to cover , shown in blue, against the number of samples necessary to cover 1, shown in orange, as the dimension increases.

Combining Equations 1 and 2 gives a lower bound on the percentage of  that is not covered by

X.

1-

vol X vol 



1-

k/2 

d-k 2

+

1

d 2

+

1

k

k 2

.

(3)

Notice that the factors involving  and (µ - ) cancel. Figure 4 (left) shows that this expression approaches 1 as the codimension (d - k) of  increases. Suppose we set  = 1 and construct a 1-cover of . Figure 4 (right) compares the number of points necessary to construct a 1-cover of  with a lower bound on the number necessary to cover 1. (We compute a lower bound by generously assuming that the spheres are disjoint.) The number of points necessary to cover  with balls of radius 1 depends only on k, not the embedding dimension d. However the number of points necessary to cover 1 increases as  dk/2 . Notice that this lower bound scales polynomially in d and exponentially in k.

Our lower bound of  dk/2 samples is similar to the work of Schmidt et al. (2018) who prove that, in the simple Gaussian setting, robustness requires as much as ( d) more samples. Their arguments are statistical while ours are geometric.

Approaches that produce robust classifiers by generating adversarial examples in the -balls centered on the training set do not accurately model M , and it will take many more samples to do so. If the method behaves arbitrarily outside of the -balls that define X , adversarial examples will still
exist and it will likely be easy to find them. The reason deep learning has performed so well on a
variety of tasks, in spite of the brittleness made apparent by adversarial examples, is because it is much easier to perform well on M than it is to perform well on M .

The properties of volume detailed here for k-flats also hold for arbitrary manifolds. In the general case we have

1-

vol X vol M



1-



k/2

(

d-k 2

+

(

d 2

+

1)

1)

k volk M

|X

|,

(4)

which, for any fixed training set size |X|, fixed k, and   1, approaches 1 as d   (as the codimension increases). In Appendix H we explore this bound for d-spheres, for which asymptotically tight bounds on the size of a minimum -cover are not known.

We draw the following conclusions. First, it takes many fewer samples to accurately model M than to model M . Second, high classification accuracy on M does not imply high accuracy in M . Third, even moderate under-sampling of M leads to a significant loss of coverage of M in high
codimension, because the volume of the union of balls centered at the samples shrinks faster than the of volume of M . However this does not mean that robust classification is infeasible. In Section 6
we prove that, for learning algorithms with well understood decision boundaries, it is possible to guarantee high classification accuracy on M given sufficiently dense samples of M.

7

Under review as a conference paper at ICLR 2019

6 PROVABLY ROBUST CLASSIFIERS

Adversarial training, the process of training on adversarial examples generated in a · p-ball around the training data, is a very natural approach to constructing robust models (Madry et al. (2018)). In our notation this corresponds to training on samples drawn from X for some . While natural, we
show that there are simple settings where this approach is much less sample-efficient than nearest neighbor classifiers, if the only guarantee is correctness in X .
Define a learning algorithm L with the property that, given a training set X  M sampled from a manifold M, L outputs a model fL such that for every x  X with label y, and every x^  Brch M(x), fL(x^) = fL(x) = y. That is, L learns a model that outputs the same label for any · pperturbation of x up to rchp p as it outputs for x. Theorem 1 gives a sufficient sampling condition for fL to correctly classify M for all manifolds M. We also provide a sufficient sampling condition for a nearest neighbor classifier fnn to correctly classify M , which is about a factor of 2 less dense in terms of  than that of fL. Further on we will show a setting where this gap is necessary, leading to a gap in the necessary sizes of the training sets of fnn and L that is exponential in the dimension of M.
Theorem 1. Let M  Rd be a k-dimensional manifold and let < rchp p for any p. Let fnn be a nearest neighbor classifier and let fL be the output of a learning algorithm L as described above. Let Xnn, XL  M denote the training sets for fnn and L respectively. We have the following sampling guarantees:

1. If Xnn is a -cover for   2(rchp p - ) then fnn correctly classifies M . 2. If XL is a -cover for   rchp p - then fL correctly classifies M .

In Appendix B we provide additional robustness results for nearest neighbors including: (1) a similar robustness guarantee as in Theorem 1 when noise is introduced into the samples and (2) that the decision boundary Dfnn of fnn approaches the decision axis as the sample density increases.
The bounds on  in Theorem 1 are sufficient, but they are not always necessary. There exists manifolds where the bounds in Theorem 1 are pessimistic, and less dense samples corresponding to larger values of  would suffice.

Next we will show a setting where bounds on  similar to those in Theorem 1 are necessary. In
this setting, the difference of a factor of 2 between the sampling requirements of fnn and fL leads to an exponential gap between the sizes of Xnn and XL necessary to achieve the same amount of
robustness.

Consider two subsets of k-flats 1, 2, as defined in Section 5, where 1 lies in the subspace xd = 0 and 2 lies in the subspace xd = 1; thus rch2 2 = 1. In the · 2 norm we can show that the gap in Theorem 1 is necessary for  = 1  2. Furthermore the bounds we derive for -covers for 
for both fnn and fL are tight. Combined with well-known properties of covers, we get that the ratio |XL|/|Xnn| is exponential in k.

Theorem 2. Let  = 1  2 as described above. Let Xnn, XL   be minimum training sets necessary to guarantee that fnn and fL correctly classify M . Then we have that

|XL| |Xnn|

=

2k

 1-
1- 2

k
.

(5)

Corollary 3. For the setting described in Theorem 2, we have that

for all < 1.

|XL| |Xnn|



2k/2.

(6)

Finally we present an exponential lower bound on the number of linear regions necessary to represent
a decision boundary that is robust to · 2-perturbations of at most  rch2 2 -  , in the simple case of two concentric (d - 1)-spheres.

8

Under review as a conference paper at ICLR 2019

Accuracy
1.0 0.8 0.6 0.4 0.2 0.0
0.0

FGSM Attack vs Codimension (Adam)
0.2 0.4 0.6 0.8

Accuracy
1.0
0.8
codim=1 0.6codim=10
codim=100 codim=500
0.4

BIM Attack vs Codimension (Adam)

Accuracy
1.0

Adverarial Training with BIM Attack (Adam)

Accuracy
1.0

Adverarial Training with BIM Attack (Adam)

0.8 0.8
codim=1 0.6 0.6codim=10
codim=100 codim=500 0.4 0.4

0.2 0.2 0.2
 0.0 0.0
1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 0.0 0.0 0.2 0.4 0.6 0.8

cocdoimdi=m1=1 cocdoimdi=m1=0 10 cocdoimdi=m1=00100 cocdoimdi=m5=00500
1.0  1.0 

Figure 5: As the codimension increases the robustness of decision boundaries learned by Adam on naturally trained networks decreases steadily. Left: Effectiveness of FGSM attacks as codimension increases. Center: BIM attacks. Right: Training using the adversarial training procedure of Madry et al. (2018) is no guarantee of robustness; as the codimension increases it becomes easier to find adversarial examples using BIM attacks. Appendix D.4 shows the performance on nearest neighbor on this data, which is essentially perfect accuracy for all .

Theorem 4. Let S1, S2  Rd be two concentric (d - 1)-spheres with radii r1 < r2 respectively and let S = S1  S2. Let f : Rd  R be a fully connected neural network with ReLU activations. Suppose that f correctly classifies Srch2 2- for some   [0, rch2 2]. Said differently, the decision boundary of f lies in a  -tubular neighborhood of the decision axis, Df  2 . Then the number of linear regions N into which f subdivides Rd is lower bounded as

N



 2

(

d+1 2

(

d 2

)

)

r1 + rch S 4

d-1 2
.

(7)

 d-1

Written asymptotically, N  

d r1+rch S 2d 

2

Prior work has experimentally verified that increasing the size of deep networks improves robustness (Madry et al. (2018)). Theorem 4 proves that there are settings in which robustness requires larger models.
Proofs for all of the results in this section are provided in Appendices A and C.

7 EXPERIMENTS
7.1 HIGH CODIMENSION REDUCES ROBUSTNESS
Section 5 suggests that as the codimension increases it should become easier to find adversarial examples. To verify this, we introduce two synthetic datasets, CIRCLES and PLANES, which allow us to carefully vary the codimension while maintaining dense samples. The CIRCLES dataset consists of two concentric circles in the x1-x2-plane, with rch2 2 = 1. We densely sample 1000 random points on each circle for both the training and the test sets. The PLANES dataset consists of two 2-dimensional planes, the first in the xd = 0 and the second in xd = 2, so that rch2 2 = 1. We sample the training set at the vertices of the grid described in Section 5, and the test set at the centers of the grid cubes, the blue points in Figure 3. Further details are provided in Appendix G and visualizations in Appendix J.
We consider two attacks, the fast gradient sign method (FGSM) (Goodfellow et al. (2014)) and the basic iterative method (BIM) (Kurakin et al. (2016)) under · 2. We train with the adversarial training procedure proposed by Madry et al. (2018) and use the implementations provided in the cleverhans library (Papernot et al. (2018)). Further implementation details are provided in Appendix G. Our experimental results are averaged over 20 retrainings of our model architecture, using Adam (Kingma & Ba (2015)). Further implementation details are provided in Appendix G. Figure 5(Left, Center) shows FGSM and BIM attacks on the CIRCLES dataset as we vary the codimension. For both attacks we see a steady decrease in robustness as we increase the codimension, on average.
Madry et al. (2018) propose training against a PGD adversary to improve robustness. Section 7.1 suggests that this should be insufficient to guarantee robustness, as X is often a poor model for

9

Under review as a conference paper at ICLR 2019

Accuracy
1.0 0.8 0.6 0.4 0.2 0.0
0.0

Accuracy
1.0

Robustness of Natural Model vs NN

Accuracy
1.0

Robustness of Robust Model vs NN

Accuracy
1.0

Robustness of Robust Model vs NN to NN Attack

Robustness of Natural Model vs NN

0.8
0.6 Accuracy 1.0 0.4 0.8

Robustness of Robust Model vs NN

0.8
Accuracy 0N.6atural1.0 1-NN
0.4 0.8

Robustness of Robust Model vs NN to NN Attack

0.8
0R.6obust 1-NN
0.4

Robust 10-NN

0.2 N0.6atural 1-NN

0.2 0.R6 obust 1-NN

0.2 Robust 10-NN

 0.0 0.0

0.4

0.1

0.2

0.3

0.4

0.0

0.5

0.0 0.4

0.1

0.2

0.3

0.4

0.5

0.0 0.0

0.1

0.2

0.3

0.4

0.5

Figure 6:0.2 Robustness of nearest neighb0.2ors on MNIST. Left: Performance on l BIM attack against

0.1

0.2

0.3

0.4

a

naturally0.5

0.0 0.0

tr0a.1 ined0.2mode0.3l.

Center: The0.4

0.5

0.0 0.0

same0.1

for the0.2

ad0.3 versa0.4rially0.5trained

convolutional

models

of

Madry et al. (2018). Right: Performance of the robust model and nearest neighbors on examples

generated by a custom attack on nearest neighbors.

M . We train against a PGD adversary with = 1 under · 2-perturbations on the PLANES dataset. Figure 5 (Right) shows that it is still easy to find adversarial examples for < 1 and that as the codimension increases we can find adversarial examples for decreasing values of . In contrast, nearest neighbor achieves perfect robustness for all epsilon on this data (see Appendix D.4 for details).
7.2 MNIST
To explore performance on a more realistic dataset, we compared nearest neighbors with robust and natural models on MNIST. We considered three attacks: BIM under l norm against the natural and robust models as well as a custom attack against nearest neighbors. Each of these attacks are generated from the MNIST test set. Architecture details can be found in Appendix G. Figure 6 (Left) shows that nearest neighbors is substantially more robust to BIM attacks than the naturally trained model. Figure 6 (Center) shows that nearest neighbors is comparable to the robust model up to
= 0.3, which is the value for which the robust model was trained. After = 0.3, nearest neighbors is substantially more robust to BIM attacks than the robust model. At = 0.5, nearest neighbors maintains accuracy of 78% to adversarial perturbations that cause the accuracy of the robust model to drop to 0%. In Appendix D.2 we provide a similar result for FGSM attacks.
Figure 6 (Right) shows the performance of nearest neighbors and the robust model on adversarial examples generated for nearest neighbors. The nearest neighbor attacks are generated as follows: iteratively find the k nearest neighbors and compute an attack direction by walking away from the neighbors in the true class and toward the neighbors in other classes. We find that nearest neighbors is able to be tricked by this approach, but the robust model is not. This indicates that the errors of these models are distinct and suggests that ensemble methods may effectively get the best of both worlds. Additionally, a closer investigation shows strong qualitative differences between the BIM adversarial examples and the examples generated for nearest neighbors. Appendix L argues that the adversarial examples that fool nearest neighbor line up better with human intuition.
8 CONCLUSION
We have presented a geometric framework for proving robustness guarantees for learning algorithms. Our framework is general and can be used to describe the robustness of any classifier. We have shown that no single model can be simultaneously robust to attacks under all norms, that nearest neighbor classifiers are theoretically more sample efficient than adversarial training, and that robustness requires larger deep ReLU networks. In particular, we have highlighted the role of codimension in contributing to adversarial examples and verified our theoretical contributions with experimental results.
We believe that a geometric understanding of the decision boundaries learned by deep networks will lead to both new geometrically inspired attacks and defenses. In Appendix E we provide a novel gradient-free geometric attack in support of this claim. Finally we believe future work into the geometric properties of decision boundaries learned by various optimization procedures will provide new techniques for black-box attacks.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Nina Amenta and Marshall W. Bern. Surface reconstruction by voronoi filtering. Discrete & Computational Geometry, 22, 1999.
Nina Amenta, Marshall W. Bern, and David Eppstein. The crust and the beta-skeleton: Combinatorial curve reconstruction. Graphical Models and Image Processing, 1998.
Nina Amenta, Sunghee Choi, Tamal K. Dey, and N. Leekha. A simple algorithm for homeomorphic surface reconstruction. International Journal of Computational Geometry and Applications, 2002.
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In ICML, 2018.
Harry Blum. A transformation for extracting new descriptors of shape. Models for Perception of Speech and Visual Forms, 1967.
Jean-Daniel Boissonnat and Arijit Ghosh. Manifold reconstruction using tangential delaunay complexes. Discrete & Computational Geometry, 51, 2014.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In ICLR, 2018.
Siu-Wing Cheng, Tamal K. Dey, Herbert Edelsbrunner, Michael A. Facello, and Shang-Hua Teng. Sliver exudation. Journal of the ACM, 47, 2000.
Siu-Wing Cheng, Tamal K. Dey, and Edgar A. Ramos. Manifold reconstruction from point samples. In Proceedings of the Symposium on Discrete Algorithms SODA, 2005.
Siu-Wing Cheng, Tamal K. Dey, and Jonathan Richard Shewchuk. Delaunay Mesh Generation. CRC Press, Boca Raton, Florida, December 2012.
Felipe Codevilla, Matthias Mu¨ller, Alexey Dosovitskiy, Antonio Lo´pez, and Vladlen Koltun. Endto-end driving via conditional imitation learning. In ICRA, 2018.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20, 1995.
Tamal K. Dey. Curve and Surface Reconstruction: Algorithms with Mathematical Analysis. Cambridge University Press, 2007.
Tamal K. Dey and Samrat Goswami. Provable surface reconstruction from noisy samples. In Proceedings of the Symposium on Computational Geometry (SoCG), 2004.
Tamal K. Dey and Piyush Kumar. A simple provable algorithm for curve reconstruction. In Proceedings of the Symposium on Discrete Algorithms (SODA), 1999.
Tamal K. Dey, Joachim Giesen, Edgar A. Ramos, and Bardia Sadri. Critical points of the distance to an epsilon-sampling of a surface and flow-complex-based surface reconstruction. In Proceedings of the Symposium on Computational Geometry (SoCG), 2005.
Herbert Edelsbrunner and Nimish R. Shah. Triangulating Topological Spaces. International Journal of Computational Geometry and Applications, August 1997.
Gamaleldin F. Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large margin deep networks for classification. CoRR, abs/1803.05598, 2018. URL http://arxiv. org/abs/1803.05598.
Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. Nature, 2017.
Jean-Yves Franceschi, Alhussein Fawzi, and Omar Fawzi. Robustness of classifiers to uniform lp and gaussian noise. In AISTATS, 2018.
11

Under review as a conference paper at ICLR 2019
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian J. Goodfellow. Adversarial spheres. CoRR, abs/1801.02774, 2018. URL http://arxiv.org/abs/1801.02774.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2014.
Marc Khoury and Jonathan Richard Shewchuk. Fixed points of the restricted delaunay triangulation operator. In Proceedings of the Symposium on Computational Geometry (SoCG), 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In ICLR Workshop Track, 2016.
Sergey Levine, Nolan Wagener, and Pieter Abbeel. Learning contact-rich manipulation skills with guided policy search. In ICRA, 2015.
Xuezhi Liang, Xiaobo Wang, Zhen Lei, Shengcai Liao, and Stan Z. Li. Soft-margin softmax for deep classification. In ICONIP, 2017.
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural networks. In ICML, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the Asia Conference on Computer and Communications Security. ACM, 2017.
Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, and Rujun Long. Technical report on the cleverhans v2.1.0 adversarial examples library. arXiv preprint arXiv:1610.00768, 2018.
Maithra Raghu, Ben Poole, Jon M. Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. In ICML, 2017.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In ICLR, 2018.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. In NIPS, 2018.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially robust neural network model on MNIST. CoRR, abs/1805.09190, 2018. URL https: //arxiv.org/abs/1805.09190.
John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. In ICLR, 2018.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In ICLR, 2018.
Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural networks: A theoretical view. In AAAI, 2016.
12

Under review as a conference paper at ICLR 2019

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL http://arxiv.org/abs/1312.6199.
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In NIPS, 2017.
Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In ICML, 2018.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.
Tianhang Zheng, Changyou Chen, and Kui Ren. Distributionally adversarial attack. CoRR, abs/1808.05537, 2018. URL http://arxiv.org/abs/1808.05537.

A OMITTED PROOFS
A.1 AUXILIARY LEMMAS Lemma 5. Let M1, M2  Rd be k-dimensional manifolds such that MM2 = . Let p be their decision axis for any p and let  : [0, 1]  Rd be any path such that (0)  M1 and (1)  M2. Then   M = , that is  must cross the decision axis.

Proof. Define f1, f2 : [0, 1]  R as f1(t) = d((t), M1) and f2(t) = d((t), M2). Consider the function g(t) = f1(t) - f2(t). Since M1  M2 =  and  starts on M1 and terminates on M2 the function g(0) < 0 and g(1) > 0. Then, since g is continuous, the Intermediate Value Theorem
implies that there exists t1  [0, 1] such that g(t1) = 0. Thus d((t1), M1) = d((t1), M2), which implies that (t1) is on the decision axis .

Theorem 6. Let f be any classifier on M = M1  M2. The maximum accuracy achievable, assuming a uniform distribution, on M is

1

-

1 2

vol(M1 vol(M1

 

M2 M2

) )

.

(8)

Proof. It is clearly optimal to classify points in vol(M1 \ M2) as class 1 and to classify points in

vol(M2 \ M1) as class 2. Such a classifier can only be wrong when points lie in this intersection.

For

points

in

this

intersection,

the

probability

of

a

misclassification

is

1 2

for

any

classification

that

f

makes. Thus, the probability of misclassification is

1 2

vol(M1 vol(M1

 

M2) M2)

.

Corollary 7. For < rchp (p; M) there exists a decision boundary that correctly classifies M . Proof. For < rchp p, M  p =  and so p is one such decision boundary.
13

Under review as a conference paper at ICLR 2019

A.2 PROOF OF THEOREM 1
Proof. We begin by proving (1). Let q  M be any point in M . Suppose without loss of generality that q  Mi for some class i. The distance d(q, Mj) from q to any other data manifold Mj, and thus any sample on Mj, is lower bounded by d(q, Mj)  2 rchp p - . It is then both necessary and sufficient that there exists a x  Mi such that d(q, x) < 2 rchp p - for fnn(q) = i. (Necessary since a properly placed sample on Mj can achieve the lower bound on d(q, Mj).) The distance from q to the nearest sample x on Mi is d(q, x)  +  for some  > 0. The question is how large can we allow  to be and still guarantee that fnn correctly classifies M ? We need
d(q, x)  +   2 rchp p -  d(q, Mj)
which implies that   2(rchp p - ). It follows that a -cover with  = 2(rchp p - ) is sufficient, and in some cases necessary, to guarantee that fnn correctly classifies M .
Next we prove (2). As before let q  Mi . It is both necessary and sufficient for q  Brchp p (x) for some sample x  Mi to guarantee that fL(q) = i, by definition of L. The distance to the nearest sample x on Mi is d(q, x)  +  for some  > 0. Thus it suffices that   rchp p - .

A.3 PROOF OF THEOREM 2 AND COROLLARY 3

Proof. Let q  1. Since 1 is flat, the distance to from q to the nearest sample x  1 is bounded as q - x 2  2 + 2. For fnn(q) = 1 we need that q - x 2  2 - , and so it suffices that   2 1 - . In this setting, this is also necessary; should  be any larger a property placed sample
on 2 can claim q in its Voronoi cell. 
Similarly for fL(q) = 1 we need that q - x 2  1, and so it suffices that   1 - 2. In this setting, this is also necessary; should  be any larger, q lies outside of every · 2-ball B1(x) and so L is free to learn a decision boundary that misclassifies q.

Let N (, M) denote the size of the minimum -cover of M. Since  is flat (has no curvature)

and since the intersection of  with a d-ball centered at a point on  is a k-ball, a standard volume

argument can be applied in the affine subspace aff  to conclude that N (, )   volk /k .

So we have

 N ( 1 - 2, ) = 2k  1 -

k
.

N (2 1 - , )

1- 2

(9)

Since  is constant in both settings, the factor volk  as well as the constant factors hidden by (·) cancel. (Note that we are using the fact that 1, 2 are compact, and so their k-dimensional volumes
are finite.)

Proof. We have that

 lim  1 -

k
=

1

k
.

1 1 - 2

2

Furthermore the expression is monotonically decreasing on the interval [0, 1]. The result follows.

B ADDITIONAL THEORETICAL RESULTS
A finite sample X of M is said to exhibit Hausdorff noise up to  if X  M . That is every sample lies in a  -tubular neighborhood of M, not necessarily on M. We can show a similar result to Theorem 1 for fnn under moderate amounts of Hausdorff noise. Theorem 8. Let X be a finite set sampled from M such that X  M for some  < rchp p; that is X lies near M, in a  -tubular neighborhood. If X is a -cover with   2(rchp p - ) -  , then fnn correctly classifies M .
Proof. Let q  Mi . The distance from q to any sampled in Mj for j = i is lower bounded as d(q, Mj )  2 rchp p - -  . It is then both necessary and sufficient that there exists a sample
14

Under review as a conference paper at ICLR 2019

x  Mi such that d(q, x)  2 rchp p - -  . The distance from q to the nearest sample in Mi is upper bounded by the -cover condition as d(q, x)  + . It suffices that

d(q, x)  +   2 rchp p - -   d(q, Mj ),

which implies that   2(rchp p - ) -  .

Theorem 9. Let Let   Dfnn be

z a

 Dfnn be a point on the linear facet of Dfnn and

decision note that

boundary of fnn for a -cover  is a Voronoi facet, let  =

X pq

with  be the

< 1. dual

Delaunay edge of  such that p  M1 and q  M2. Define d(z, M1) = 1 rch2 2 where z1 is

a nearest point on M1 to z and d(z, M2) = 2 rch2 2, with 1 < 2 < 1. Then there exists a

decision

axis

point

m





such

that

d(z, m)



 2 +(22 -12 )+2 2 1+( 2 -12 )

2

rch2

2.

Proof. If z  M then the result holds, so suppose that z  M .
The decision boundary Dfnn is the union of a subset of (d - 1)-dimensional Voronoi cells (along with their lower dimensional faces) of the Voronoi diagram Vor X of X with the following property. For every Voronoi (d - 1)-cell   Dfnn , its dual Delaunay edge  = pq has endpoints p, q  X such that p  M1 and q  M2. That is, p and q have different class labels. In particular pq crosses . For every point z  , d(z, p) = d(z, q)  mini d(z, Xi); that is, p, q minimize the distance from z to any sample point in X. In the interior of  this inequality is strict, while on the boundary of  it may be realized by more points than just p and q. (See Appendix I for a brief review of Voronoi diagrams and Delaunay triangulations.)
Let   Dfnn be a Voronoi (d - 1)-cell that contains z and let  = pq be 's dual Delaunay edge. Imagine growing a ball B centered at z by increasing the radius r. Due to the properties of Voronoi cells outlined above, the fact that z  M , and the assumption that  = 0, the following three events occur in order as we increase r. First B intersects the manifold to which z is closest, without loss of generality M1. Second B intersects M2. Notice that at this point B has not intersected any sample points in X, since p and q are on M1 and M2 respectively and are the closest samples to z. Third B intersects p and q, when r = d(z, p). Let r1, r2, r3 denote the value of the radius at these three event points respectively. Similarly let B1, B2, B3 denote the balls centered at z with radii r1, r2, r3 respectively.Let z1  B1  M1 and let z2  M2. Since M1 is the closer of the two manifolds to z, the line segment zz2 must intersect . Let  : [0, 1]  Rd parameterize the line segment zz2, where (0) = z, (1) = z2, and z - (t) 2 = r2t. We will show that there exists a decision axis point m   that is close to z.
The ball B2 is tangent to M2 at z2 but contains some portion of M1. Our approach will be to move the center of B2 along  from z to z2 while maintaining tangency at z2. That is we consider the balls Bt = B((t), (t) - z2 2) as t increase from 0 to 1. For some t, Bt  M1 =  which means that we have crossed the decision axis. We will prove that t must be small which implies that z - m 2  z - (t) 2  r2t is small.
We begin by considering the triangle z1zz2. Using the law of cosines we derive an expression for the angle z1zz2 as

z1 - z2

2 2

=

r12

+

r22

-

2r1r2

cos

z1zz2

cos z1zz2 =

r12 + r22 - z1 - z2 2r1r2

2
2.

As t increases the event BtM1 =  occurs when the distances from (t) to any point x  B2M1 is greater than r2(1 - t). Due to the -cover condition at z1 and the fact that B2  B3 where B3 is
the event where a ball centered at z intersects a sample point, every such x must lie in a ball B(z1, g) for g  . Thus the event Bt  M1 =  occurs for the minimum t such that

z1 - (t) 2 - g  r2(1 - t)

z1 - (t)

2 2

 r22(1 - t)2 + 2gr2(1 - t) + g2.

15

Under review as a conference paper at ICLR 2019

First we derive an expression for z1 - (t) 2 again using the law of cosines and substituting the expression for z1zz2.

z1 - (t)

2 2

= r12 + r22t2 - 2r1r2t cos z1zz2

= r12 + r22t2 - t(r12 + r22 -

z1 - z2

2 2

)

= (1 - t)r12 + (t - 1)tr22 + t

z1 - z2

2 2

.

So then

z1 - (t)

2 2

 r22(1 - t)2 + 2gr2(1 - t) + g2 holds if and only if

(1 - t)r22 + (t - 1)tr22 + t

z1 - z2

2 2



r22(1

-

t)2

+

2gr2(1

-

t)

+

g2

t

g2 - r12 + 2gr2 + r22 z1 - z2 2 - r12 + 2gr2 + r22



g2 - r12 + 2gr2 + r22 z1 - z2 2 - r12 + r22



2

+ (22 - 12) + 22 1 + (2 - 12)

C A LOWER BOUND ON MODEL EXPRESSIVENESS

C.1 A SIMPLE EXAMPLE

Consider the case of two concentric circles C1, C2 with radii r1 < r2 respectively, as illustrated in
Figure 7. Each circle represents a different class of data. Suppose that we train a parametric model
f (x; ) with p parameters so that for x  C1, f (x; ) > 0 and for x  C2, f (x; ) < 0. How does the number of parameters p necessary to ensure that such a decision boundary can be expressed by f (·; ) increase as the gap between C1 and C2 decreases?

In the important special case where f is parameterized by a fully connected deep network with layers, h hidden units per layer, and ReLU activations, Raghu et al. (2017) prove that f subdivides the input space into convex polytopes. In each convex polytope, f defines a linear function that
agrees on the boundary of the polytope with its neighbors. They showed that, when the inputs are in R2, the number of polytopes in the subdivision is at most O(h2 ) (Raghu et al. (2017)[Theorem 1]).

Let Sf denote the subdivision of space into convex polytopes induced by f . Consider the decision boundary Df = {x  Rd : f (x; ) = 0} of f . Df can be constructed by examining each polytope P  Sf and solving the linear equation fP (x) = 0 where fP is the linear function defined on P by f . Since fP is linear the solution is either (1) the empty set, (2) a single line segment, or (3) all of P . Case (3) is a degenerate case and there are ways to perturb f by an infinitesimally small amount
such that case (3) never occurs and the classification accuracy is unchanged. Thus we conclude that
Df is a piecewise-linear curve comprised of line segments. (In higher dimensions Df is composed
of subsets of hyperplanes.) See Figure 7.

Suppose that Df separates C1 from C2 and let s  Df be a line segment of the decision boundary.

Since s lies in the space between C1 and C2, the length |s|  2 r22 - r12, which is tight when s is

tangent to C1 and touches C2 at both of its endpoints. For Df to separate C1 from C2, Df must make

a full rotation of 2 around the origin. The portion of this rotation that s can contribute is upper

bounded by

.

arccos

r1 r2

2 arccos

r1 r2

.

Thus

the

number

of

line

segments

that

comprise

Df

is

lower

bounded

by

As

r2



r1,

the

minimum

number

of

segment

necessary

to

separate

C1

from

C2



arccos

r1 r2

 .

Since each polytope P  Sf can contribute at most one line segment to Df , the size of the model

necessary to represent a decision boundary that separates C1 from C2 also increases as the circles

get closer together.

16

Under review as a conference paper at ICLR 2019

Now consider the set of all perturbations under the · 2 norm of points in C1, which we will denote as C1 = {x  R2 : x - C1  }, and similarly for C2. Suppose that f a fully connected network described as above has sufficiently many parameters to represent a decision boundary that separates C1 from C2. Is f also capable of learning a robust decision boundary that separates C1 from C2?

C2 Df
r2

C2 Df

Df

r1





C1 C1

Figure 7: Separating two classes of data sampled from C1 and C2 may require a decision boundary Df with only a few linear segments. However a decision boundary Df that is robust to perturbations must lie in gap between C1 and C2. Learning a robust decision boundary may require more linear segments and thus a more expressive model. As we increase , demanding a more ro-
bust decision boundary, the gap between C1 and C2 decreases, and so the number of linear segments increases towards .

For Df to separate C1 from C2 it must lie in the region between C1 and C2. In this setting each

segment

can

contribute

at

most

2

arccos

r1 + r2 -

to the full 2 rotation around the origin. The mini-

mum number of line segments that comprise a robust decision boundary Df is lower bounded by



arccos

r1 + r2 -

. As



r2 -r1 2

this

quantity

approaches

.

Even

if

f

is

capable

of

separating

C1

from

C2 we can choose

such

that



arccos

r1 + r2 +

 (h2 ).

This simple example shows that learning decision boundaries that are robust to -adversarial examples may require substantially more powerful models than what is required to learn the original distributions. Furthermore the amount of additional resources necessary is dependent upon the amount of robustness required.

This example also demonstrates the relative inefficiency of deep networks compared to higher order

models. For example suppose that we first lift C1 and C2 to a parabola in R3 via map (x1, x2) =

(x1, x2, x21 + x22). That is, we construct the sets C1+ = {(x1, x2) : (x1, x2)  C1} and similarly

for C2+. After applying , (C1+) and (C2+) are linearly separable for any

<

r2

-r1 2

.

The linear

decision boundary in R3 maps back to a circle in R2 that separates C1 and C2.

C.2 PROOF OF THEOREM 4

Proof. For f to be robust to -adversarial examples for  rch2 2 -  the decision boundary Df   . The boundary of  is comprised of two disjoint (d - 1)-spheres, which we will denote as 1 and 2 with radii r1 + rch2 2 -  and r1 + rch2 2 +  respectively. (It is standard in topology to use the  symbol to denote the boundary of a topological space.)

The isoperimetric inequality states that a (d - 1)-sphere minimizes the (d - 1)-dimensional volume

(thought of as "surface area") across all sets with fixed d-dimensional volume (thought of as "vol-

ume"). Since Df  and so we have that

 , the d-dimensional surf 1  surf Df .

volume

enclosed

by

Df

is

at

least

as

large

as

that

of

1

Now consider any (d - 1)-dimensional linear facet  of the decision boundary Df . The normal

space of  is 1-dimensional; let n denote a unit vector orthogonal to . (There are two possible

choices n and -n.) Due to the spherical symmetry of  and the fact that    , the diameter

of  is maximized and intersects 2

when  is tangent to 1 at (r1 + . In pursuit of an upper bound, we

rch2 will

2 -  )n (or -(r1 + rch2 2 -  )n) assume without loss of generality that

17

Under review as a conference paper at ICLR 2019

 has these properties. Let o denote the origin, x = (r1 + rch2 2 -  )n, and y    2 . We consider the right triangle oxy with right angle at x. By basic properties of right triangles,

diam  2



x-y 2 =

(r1 + rch2 2 +  )2 - (r1 + rch2 2 -  )2 =

4 (r1 + rch2 2). It

follows that  is contained in a (d - 1)-dimensional ball of radius 4 (r1 + rch2 2). In particular

the (d-1)-dimensional volume of  is bounded as vold-1()  vold-1 B(0, 4 (r1 + rch2 2)). The (d - 1)-dimensional volume of Df (again thought of as "surface area"), is equal to the sum of the (d-1)-dimensional volumes of the linear facets that comprise Df . Combining these inequalities

gives the result.

2

d 2

(

d 2

)

(r1

+

rch2 2)d-1

=

surf

1



surf

Df

 N vold-1 B(0, 4 (r1 + rch2 2))

 2

(

d+1 2

(

d 2

)

)

r1 + rch2 2 4



N

 d-1 2

(

d+1 2

)

(4

(r1

+

rch2

2))

d-1 2

d-1 2
N

D ADDITIONAL EXPERIMENTS

We present additional experiments to support our theoretical predictions. We reproduce the results of Section 7 using different optimization algorithms (Section D.1) and attack methods (Section D.2). These additional experiments are consistent with our conclusions in Section 7. Additionally we provide evidence that adversarial perturbations lie mostly in the directions of the normal space (Section D.3). Finally we show that a nearest neighbor classifier is robust in high codimension (Section D.4).

D.1 REPRODUCING RESULTS USING SGD

In Section 7.1 we showed that increasing the codimension reduces the robustness of the decision boundaries learned by Adam on CIRCLES. In Figure 8 we reproduce this result using SGD. Again we see that as we increase the codimension the robustness decreases. SGD presents with much less variances than Adam, which we attribute to implicit regularization that has been observed for SGD (Soudry et al. (2018))

Accuracy
1.0

FGSM Attack vs Codimension (SGD)

Accuracy
1.0

BIM Attack vs Codimension (SGD)

0.8 0.8

codim=1

codim=1

0.6

codim=10

0.6

codim=10

codim=100

codim=100

codim=500

codim=500

0.4 0.4

0.2 0.2

0.0 0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.0 0.2 0.4 0.6 0.8 1.0

Figure 8: As in the case of training with Adam, we see a steady decrease in robustness on the CIRCLES dataset as the codimension increases when training with SGD.

Next we consider the adversarial training procedure of Madry et al. (2018) using SGD instead of Adam. We note that the authors of Madry et al. (2018) use Adam in their own experiments. Figure 9 shows that the result is consist with the result in Section 7.1. Again SGD presents with lower variance.

18

Under review as a conference paper at ICLR 2019

Accuracy
1.0

Adverarial Training with BIM Attack (SGD)

0.8
codim=1 0.6 codim=10
codim=100 codim=500
0.4

0.2

0.0 0.0 0.2 0.4 0.6 0.8 1.0
Figure 9: Adverarial training with a PGD adversary, as in Figure 5, using SGD. Similarly we see a drop in robustness as the codimension increases.

D.2 REPRODUCING RESULTS USING FGSM
In Section 7.1 we evaluated the robustness of nearest neighbors against BIM attacks under the ·  on MNIST. In Figure 10 we evaluate the robustness of nearest neighbors against FGSM attacks under the ·  on MNIST. We use the naturally pretrained (natural) and adversarially pretrained (robust) convolutional models provided by Madry et al. (2018)1. Figure 10 (Left) shows that nearest neighbors is substantially more robust to FGSM attacks than the naturally trained model. Figure 10 (Right) shows that nearest neighbors is comparable to the robust model up to = 0.3, which is the value for which the robust model was trained. After = 0.3, nearest neighbors is substantially more robust to FGSM attacks than the robust model. At = 0.5, nearest neighbors maintains accuracy of 78% to adversarial perturbations that cause the accuracy of the robust model to drop to 39%.

Accuracy
1.0

Robustness of Natural Model vs NN

Accuracy
1.0

Robustness of Robust Model vs NN

0.8 0.8

0.6

Natural

0.6

Robust

1-NN

1-NN

0.4 0.4

0.2 0.2

0.0 0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.0 0.1 0.2 0.3 0.4 0.5

Figure 10: Robustness of nearest neighbors against the naturally trained (left) and adversarially trained (right) convolutional models of Madry et al. (2018) against FGSM attacks under the ·  norm on MNIST.

D.3 ADVERSARIAL PERTURBATIONS ARE IN THE DIRECTIONS NORMAL TO THE DATA MANIFOLD
Let x be an adversarial perturbation generated by FGSM with = 1 at x  M. Note that the adversarial example is constructed as x^ = x + x. In Figure 11 we plot a histogram of the angles (x, NxM) between x and the normal space NxM for the CIRCLES dataset in codimensions 1, 10, 100, and 500. In codimension 1, 88% of adversarial perturbations make an angle of less than 10 with the normal space. Similarly in codimension 10, 97%, in codimension 100, 96%, and in codimension 500, 93%. As Figure 11 shows, nearly all adversarial perturbations make an angle less than 20 with the normal space. Our results are averaged over 20 retrainings of the model using SGD.
Throughout this paper we've argued that high codimension is a key source of the pervasiveness of adversarial examples. Figure 11 shows that adversarial perturbations are well aligned with the
1https://github.com/MadryLab/mnist_challenge
19

Under review as a conference paper at ICLR 2019

normal space. When the codimension is high, there are many directions normal to the manifold and thus many directions in which to construct adversarial perturbations.

Codimension 1
800

Codimension 10
800

600 600

400 400

200 200

00 800

20 40 60
Codimension 100

80

00 800

20 40 60
Codimension 500

80

600 600

400 400

200 200

0 0 20 40 60 80

00

20 40 60 80

Figure 11: Histograms of the angle deviations of FGSM perturbations from the normal space for the CIRCLES dataset in codimensions 1 (upper right), 10 (upper left), 100 (lower left), 500 (lower right). Nearly all perturbations make an angle of less than 20 with the normal space.

D.4 NEAREST NEIGHBORS IS ROBUST IN HIGH CODIMENSION

In Section 7.1 we showed that the robustness of learned decision boundaries decreased as the codimension increased. In Figure 12 we repeat the experiment in Figure 5, in which we measured the robustnesss of our neural network models to FGSM attacks as the codimension increased. We repeat this experiment using nearest neighbors to classify the adversarial examples generated by FGSM. Figure 12 shows that nearest neighbors is robust even when the codimension is high, as long as the low-dimensional data manifold is well sampled. This is a consquence of the fact that the Voronoi cells of the samples are elongated in the directions normal to the data manifold when the sample is dense.

Accuracy
1.0

FGSM Attack vs Codimension for NN

Accuracy
1.0

BIM Attack vs Codimension for NN

0.8 0.8

codim=1

codim=1

0.6

codim=10

0.6

codim=10

codim=100

codim=100

codim=500

codim=500

0.4 0.4

0.2 0.2

0.0 0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.0 0.2 0.4 0.6 0.8 1.0

Figure 12: The FGSM (left) and BIM (right) perturbations that fooled our deep networks are correctly classified by a nearest neighbor classifier. Nearest neighbor classifiers are robust in highcodimension settings because their decision boundaries are elongated in the directions normal to the data manifold.

20

Under review as a conference paper at ICLR 2019
E A GRADIENT-FREE GEOMETRIC ATTACK
Most current attacks rely on the gradient of the loss function at a test sample to find a direction towards the decision boundary. Partial resistance against such attacks can be achieved by obfuscating the gradients, but Athalye et al. (2018) showed how to circumvent such defenses. Brendel et al. (2018) propose a gradient-free attack for · 2, that starts from a misclassifed point and walks toward the original point.
In this section we propose a gradient-free attack that only requires oracle access to a model, meaning we only query the model for a prediction. Consider a point x  Xtest and the · p-ball Br(x) centered at x of radius r. To construct an adversarial perturbation x  Br(x), giving an adversarial example x^ = x+x, we project every point in Xtest onto Br(x) and query the oracle for a prediction for each point. If y  Xtest projected to a point y that the model classified differently than x, we take x = y - x, otherwise x = 0. This incredibly simple attack reduces the accuracy of the pretrained robust model of Madry et al. (2018) for ·  and = 0.3 to 90.6%, less than two percent shy of the current SOTA for whitebox attacks, 88.79% (Zheng et al. (2018)).
Simple datasets, such as CIRCLES and PLANES, allow us to diagnose issues in learning algorithms in settings where we understand how the algorithm should behave. For example Athalye et al. (2018) state that the work of Madry et al. (2018) does not suffer from obfuscated gradients. In Appendix F we provide evidence that Madry et al. (2018) does suffer from the obfuscated gradients problem, failing one of Athalye et al. (2018)'s criteria for detecting obfuscated gradients.
F THE MADRY DEFENSE SUFFERS FROM OBFUSCATED GRADIENTS
Athalye et al. (2018) identified the problem of "obfuscated gradients", a type of a gradient masking (Papernot et al. (2017)) that many proposed defenses employed to defend against adversarial examples. They identified three different types of obfuscated gradients: shattered gradients, stochastic gradeints, and exploding/vanishing gradients. They examined nine recently proposed defenses, concluded that seven suffered from at least one type of obfuscated gradient, and showed how to circumvent each type of obfuscated gradient and thus each defense that employed obfuscated gradients.
Regarding the work of Madry et al. (2018), Athalye et al. (2018) stated "We believe this approach does not cause obfuscated gradients". They note that "our experiments with optimization based attacks do succeed with some probability". In this section we provide evidence that the defense of Madry et al. (2018) does suffer from obfuscated gradients, specifically shattered gradients. Shattered gradients occur when a defence causes the gradient field to be "nonexistent or incorrect" (Athalye et al. (2018)). Specifically we provide evidence that the defense of Madry et al. (2018) works by shattering the gradient field of the loss function around the data manifolds.
In Figure 13 (Left) we show the normalized gradient field of the loss function for a network trained on a 2-dimensional version of our PLANES dataset using the adversarial training procedure of Madry et al. (2018) with a PGD adversary. While the gradients have meaningful directions, Figure 13 (Left) shows that magnitude of the gradient field is nearly 0 everywhere around the data manifolds, which are at y = 0 and y = 2. The only notable gradients are near the decision axis which is at y = 1.
One criteria that Athalye et al. (2018) propose for identifying obfuscated gradients is whether onestep attacks perform better than iterative attacks. The reason this criteria is useful for identifying obfuscated gradients is because one-step attacks like FGSM first normalize the gradient, ignoring its magnitude, then take as large of a step as allowed in the direction of the normalized gradient. So long as the gradient on the manifold points towards the decision boundary, FGSM will be effective at finding an adversarial example.
In Figure 14 we show the adversarial examples generated using PGD (left), FGSM (center), and BIM (right) for = 1 starting at the test set for the PLANES dataset. FGSM produces adversarial examples at the decision axis y = 1, exactly where we would expect. Notice that all of the adversarial perturbation is normal to the data manifold, suggesting that the gradient on the manifold points towards the decision boundary. However the adversarial examples produced by PGD lie closer to the manifold from which the example was generated.
21

Under review as a conference paper at ICLR 2019

Normalized Gradient Field of Loss Function
2.0

1.5

1.0

0.5

0.0 -10

-5

0

5 10

Figure 13: (Left) The normalized gradient field of the loss for an adversarially trained network. (Right) The magnitudes of the gradient. Notice that the gradients are largely 0 except at the decision axis y = 1.

PGD splits the total perturbation between both the normal and the tangent spaces of the data manifold, as shown by the arrows in Figure 14. This suggests that, when trained adversarially, the network learned a gradient field that has small but correct gradients on the data manifold, but gradients that curve in the tangent directions immediately off the manifold.
Lastly notice that BIM, another iterative method, also produces adversarial examples that are near the decision axis. Athalye et al. (2018) cite success with iterative based optimization procedures as evidence against obfuscated gradients. However BIM also ignores the magnitude of the gradient, as it simply applies FGSM iteratively. The network has learned a gradient field that is overfit to the particulars of the PGD attack. BIM successfully navigates this gradient field, while PGD does not. While the network is robust to PGD attacks at test time, it is less robust to FGSM and BIM attacks.

PGD Adversarial Examples
2.0
1.5
1.0
0.5

FGSM Adversarial Examples
2.0
1.5
1.0
0.5

BIM Adversarial Examples
2.0
1.5
1.0
0.5

-10 -5

5

10 -10

-5

5

10 -10

-5

5 10

Figure 14: Adverarial examples generated using PGD (left), FGSM (center), and BIM (right). While the network is robust to PGD attacks, FGSM and BIM attacks are more effective because they ignore the magnitude of the gradient. For PGD we draw arrows from the test sample to the adversarial example generated from that point to aid the reader.

G IMPLEMENTATION DETAILS
In Section 7 we introduced two synthetic datasets, CIRCLES and PLANES. The CIRCLES dataset consists of two concentric circles, the first with radius r1 = 1 and the second with radius r2 = 3, so that the rch = 1. The PLANES dataset consists of two 2-dimensional planes, the first in the subspace defined by xd = 0, and the second in xd = 2, so that rch = 1. The first two axis of both planes are bounded as -10  x1, x2  10, while x3 = . . . = xd-1 = 0. Both planes are sampled as described in Section 5, so that X1 covers the underlying planes, where X is the training set.
22

Under review as a conference paper at ICLR 2019

We consider three attacks, FGSM, BIM, and PGD, primarily under the · 2 norm. For the iterative attacks BIM and PGD, we set the number of iterations to 30 with a step size of step = 0.05 per iteration.
Our controlled experiments on synthetic data consider a fully connected network with 1 hidden layer, 100 hidden units, and ReLU activations. This model architecture is more than capable of representing a nearly perfect robust decision boundary for both CIRCLES and PLANES, the latter of which is linearly separable. We set the learning rate for Adam as  = 0.1, which we found to work best for our datasets. The parameters for the exponential decay of the first and second moment estimates were set to 1 = 0.9 and 2 = 0.999. We set the learning rate for SGD as  = 0.1 and decrease the learning rate by a factor of 10 every 100 epochs. We train all of our models for 250 epochs, following Wilson et al. (2017).
All of our experiments are implemented using PyTorch. When comparing against a published result we use publicly available repositories, if able. For the robust loss of Wong & Kolter (2018), we use the code provided by the authors2.The provided implementation3 of the adversarial training procedure of Madry et al. (2018) considers a PGD adversary with · -perturbations. We reimplemented their adversarial training procedure for · 2-perturbations following their implementation and using the PGD attack implemented in the cleverhans library (Papernot et al. (2018)).
The models of Madry et al. (2018) consist of two convolutional layers with 32 and 64 filters respectively, each followed by 2 × 2 max pooling. After the two convolutional layers, there are two fully connected layers each with 1024 hidden units.

H VOLUME ARGUMENTS FOR d-SPHERES

Let S  Rd+1 be a unit d-sphere embedded in Rd+1. The volume of S is given by

vol S

=

d/2((1

+ )d - (1

(1

+

d 2

)

-

)d) ,

(10)

where  denotes the gamma function. Let X  S be a finite sample of size n of S. The set X is

the set of all perturbations of points in X under the norm · 2. How well does X approximate

S as a function of n, d and ?

To answer this question we start by upper bounding the ratio vol X / vol S . We can derive an upper bound by generously assuming that the balls B(Xi, ) are disjoint. We have that

vol X vol S



n vol B vol S

= (1 +

nd )d - (1 -

)d .

(11)

This gives a lower bound on the percentage of S that is not covered by X .

1

-

vol X vol S

 1 - (1 +

nd )d - (1 -

)d .

(12)

In Figure 15 we show three different views of this bound. In Figure 15 (Left) we set n = 1012 and plot four different values of ; in each case the percentage of volume of S that is not covered by X quickly approaches all of S . Similarly, in Figure 15 (Center), if we fix = 1 and plot four different values of n, in each case we have the same result. Finally in Figure 15 (Right) we plot a lower bound on number of samples necessary to cover S by X for four different values of ; in each case the
number of samples necessary grows exponentially with the dimension.

I VORONOI DIAGRAMS AND DELAUNAY TRIANGULATIONS

Let X  Rd be a finite set of n points. The Voronoi diagram of X, denoted Vor X, under the metric d(·, ·) is a subdivision of Rd into n cells where each cell is defined as

Vor v = {x  Rd : d(x, v)  d(x, u), u  X\{v}}.

(13)

2https://github.com/locuslab/convex_adversarial 3https://github.com/MadryLab/mnist_challenge

23

Under review as a conference paper at ICLR 2019

Ratio
1.0

Ratio
1.0

Sample Size

Ratio
1.0 0.8 0.6 0.4

0.8 0.6 0.4 0.2

=0.1 =0.2 =0.3 =0.4

Ratio
1.0 0.8 0.6 0.4

=0.1 =0.2 =0.3 =0.4

0.8 0.6 0.4 0.2

=0.1 =0.2 =0.3 =0.4

n = 1×104 n = 1×106 n = 1×108 n = 1×1010

8 × 1025 6 × 1025 4 × 1025 2 × 1025

=0.1 =0.2 =0.3 =0.4

0.2 0.0 10 20 30
0.0 Dimension 10 20 30 40 50

0.2

40

50 0.0

10 20 30
10 20 30 40 50 Dimension

40

50

0 10 20 30 40 50

Figure 15: Three different perspectives on our lower bound in Equation 3. (Left, Center) In each case the percentage of S not covered by X approaches all of S . (Right) The number of points necessary to cover S by X grows exponentially with the dimension.

In words, the Voronoi cell Vor v of v  X is the set of all points in Rd that are closer to v than any other sample point u = v in X. The Voronoi diagram is then defined as the set of all Voronoi cells, Vor X = {Vor v : v  X}. When d(·, ·) is induced by the norm · 2, the Voronoi cells are convex. See Figure 16.

Figure 16: The Voronoi diagram of a set of points in R2 (left) and its dual the Delaunay triangulation (right).
The Delaunay triangulation of X, denoted Del X is a triangulation of the convex hull of X into d-simplices. Every d-simplex   Del X, as well as every lower-dimensional face of  , has the defining property that there exists an empty circumscribing ball B such that the vertices of  lie on the boundary of B and the interior of B is free from any points in X. See Figure 16. This empty circumscribing ball property of Delaunay triangulations implies many desirable properties that are useful in mesh generation (Cheng et al. (2012)) and manifold reconstruction (Edelsbrunner & Shah (1997)). The Delaunay triangulation of a point set always exists, but is not unique in general. There exists a well known duality between the Voronoi diagram and the Delaunay triangulation of X. For every j-dimensional face   Vor X there exist a dual (d - j)-dimensional simplex denoted   Del X whose d - j + 1 vertices are the d - j + 1 vertices of X whose Voronoi cells intersect at . In particular, every d-cell of Vor X is dual to the vertex of Del X that generates that cell, and every (d - 1)-face of Vor X is dual to an edge of Del X. A nearest neighbor classifier fnn given a query point q simply returns the class of the point in X that generated the Voronoi cell in which q lies. Thus the decision boundary of fnn is the union of (d - 1) and lower dimensional Voronoi faces. Furthermore, when X is a dense sample of a manifold M, the Voronoi cells are well known to be elongated in the directions normal to M Dey (2007). This fact underlies many of our results.
24

Under review as a conference paper at ICLR 2019
J VISUALIZATION OF DATASETS
In Figure 17 we provide visualizations of our two synthetic datasets, CIRCLES (left) and PLANES (right).
Figure 17: We create two synthetic datasets which allow us to perform controlled experiments on the affect of codimension on adversarial examples.
25

Under review as a conference paper at ICLR 2019
K VISUALIZATION OF DECISION BOUNDARIES
In Figure 18 we provide visualizations of the decision boundaries learned by (a-d) our fully connected network architecture with cross entropy loss for various optimization procedures and various training lengths, (e) our fully connected network architecture trained using the robust loss of Wong & Kolter (2018) for · -perturbations, and (f) a nearest neighbor classifier for · 2 on the training set. Specifically we train on the CIRCLES dataset, embedded in R3. The training set is entirely contained in the xy-plane. We then visualize cross sections of the decision boundary for various values of z  [-5, 5]. We color points labeled as in the same class as the outer circle with the color blue and points labeled as in the same class as the inner circle as orange. Figure 18 shows the cross sections of the decision boundaries, averaged over 20 retrainings. The visualization shows how various optimization algorithms learn decision boundaries that extend into the normal directions where no data is provided.
(a) Decision boundary learned by running SGD for 25 epochs, averaged over 20 trainings.
(b) Decision boundary learned by running SGD for 250 epochs, averaged over 20 trainings.
(c) Decision boundary learned by running Adam for 25 epochs, averaged over 20 trainings.
(d) Decision boundary learned by running Adam for 250 epochs, averaged over 20 trainings.
(e) Decision boundary learned using the robust loss of Wong & Kolter (2018) for the ·  norm and running Adam for 250 epochs, averaged over 20 trainings.
(f) Decision boundary of a nearest neighbor classifier for the · 2 norm. Figure 18: The training set lies entirely in the xy-plane, shown here at z = 0. We visualize cross sections of the decision boundary for z  [-5, 5] for various optimization algorithms training for different lengths of time. The results show how various optimization algorithm learn decision boundaries that extend into the normal directions in which there is no data provided. We average the decision boundary over 20 retrainings, so faded results indicated how frequently a point was labeled a specific class.
26

Under review as a conference paper at ICLR 2019
L COMPARING NEAREST NEIGHBOR ADVERSARIAL EXAMPLES WITH BIM ATTACK
Unperturbed
Nearest Neighbor
BIM
Figure 19: Comparison of adversarial examples for nearest neighbor with adversarial examples for Madry et al. (2018). The top row is the original data that the examples were generated from. Each figure is labelled with the predictions from robust neural network. We observe an immediate qualitative difference between the nearest neighbor examples and the BIM examples: the nearest neighbors ones are starting to look like numbers from a target class! In fact, we can reasonably argue that the classifications of the robust model that don't change represent as much of an error and being fooled by a standard adversarial example. For example the center right image would be classified as an 8 by most people, but the neural network is confident it is a 0. This provides evidence that nearest neighbors is doing a better job of the learning the human decision boundary between numbers.
27

