Under review as a conference paper at ICLR 2019
UNDERSTANDING STRAIGHT-THROUGH ESTIMATOR IN TRAINING ACTIVATION QUANTIZED NEURAL NETS
Anonymous authors Paper under double-blind review
ABSTRACT
Training activation quantized neural networks involves piecewise constant loss functions with the sampled gradient vanishing almost everywhere, which is undesirable for back-propagation. An empirical way around this issue is to use a straight-through estimator (STE) (Bengio et al., 2013) in the backward pass, so that the resulting unusual "gradient" becomes non-trivial. In this paper, we make the first theoretical justification for the concept of STE, by considering the problem of learning a one-hidden-layer convolutional network with binarized ReLU activation and Gaussian input data. We refer to the unusual "gradient" based on STE as coarse gradient, which essentially is not the gradient of any function. Apparently, the choice of STE is not unique. We prove that if the STE is properly chosen, the negative expected coarse gradient is a descent direction for minimizing the population loss, and the associated coarse gradient descent algorithm converges to a local minimum (more rigorously, a critical point) of the population loss minimization problem. Moreover, we show that a relatively poor choice of STE may lead to instability of the training algorithm near certain local minima, which is also validated by our CIFAR-10 experiments.
1 INTRODUCTION
Deep neural networks (DNN) have achieved the remarkable success in many machine learning applications such as computer vision (Krizhevsky et al., 2012; Ren et al., 2015), natural language processing (Collobert & Weston, 2008) and reinforcement learning (Mnih et al., 2015; Silver et al., 2016). However, the deployment of DNN typically require hundreds of megabytes of memory storage for the trainable full-precision floating-point parameters, and billions of floating-point operations to make a single inference. To achieve the compression and acceleration, many recent efforts have been made to the training of quantized DNN, in the hope of maintaining the performance of their float counterparts (Courbariaux et al., 2015; Rastegari et al., 2016; Cai et al., 2017).
Training fully quantized DNN amounts to solving a very challenging optimization problem. It calls for minimizing a piecewise constant and highly nonconvex empirical risk function f (w) subject to a discrete set-constraint w  Q that characterizes the quantized weights. In particular, weight quantization of DNN have been extensively studied in the literature; see for examples (Li et al., 2016; Zhu et al., 2016; Zhou et al., 2017; Li et al., 2017; Hou & Kwok, 2018). On the other hand, the gradient f (w) in training activation quantized DNN is almost everywhere (a.e.) zero, which makes the standard back-propagation inapplicable. The arguably most effective way around this issue is nothing but to construct a non-trivial search direction by properly modifying the chain rule. Specifically, one can replace the a.e. zero derivative of quantized activation function composited in the chain rule with a related surrogate. This proxy derivative used in the backward pass only is referred as the straight-through estimator (STE) (Bengio et al., 2013).
1.1 RELATED WORKS
The concept of STE was originally introduced in lecture 9c of (Hinton, 2012) for training networks with the hard threshold activation 1{x>0} (a.k.a. binary neuron). (Hinton, 2012) proposed to simply back-propagate through the hard threshold function as if it had been the identity function. (Bengio et al., 2013) proposed a STE variant which uses the derivative of the sigmoid function instead. In the
1

Under review as a conference paper at ICLR 2019

training of DNN with weights and activations constrained to ±1, (Hubara et al., 2016) substituted the derivative of the signum activation function with 1{|x|1} in the backward pass. Later the idea of STE was readily extended to the training of DNN with general quantized ReLU activations (Hubara et al., 2018; Zhou et al., 2016; Cai et al., 2017; Choi et al., 2018), where some other proxies took place including the derivatives of vanilla ReLU and clipped ReLU. Despite all the empirical success of STE, to our best knowledge, there is almost no theoretical understanding of why it works.
Similar scenarios, where the derivative of certain layer composited in the loss function is not desirable for back-propagation, have also been brought up recently by (Wang et al., 2018) and (Athalye et al., 2018). The former proposed an implicit weighted nonlocal Laplacian layer as the classifier to improve the generalization accuracy of DNN. In the backward pass, the derivative of a pre-trained fully-connected layer was used as a surrogate. To circumvent the defense in adversarial attack (Szegedy et al., 2013), (Athalye et al., 2018) introduced the so-called backward pass differentiable approximation to deal with the obfuscated gradients, which shares the same spirit as STE. Again, neither of these two papers theoretically justified the proposed training approach.
Another line of research studies the convergence of (stochastic) gradient descent algorithm for learning shallow ReLU nets with one or two linear layers and Gaussian input data. Some works consider the empirical risk minimization with finite input samples (Zhong et al., 2017; Soltanolkotabi, 2017), while some others consider the minimization of population loss averaged over the whole data space (Brutzkus & Globerson, 2017; Tian, 2017; Li & Yuan, 2017; Du et al., 2018). The advantage of using the population loss model is that it admits analytic formulas for both the objective function and gradient, which facilitates the analysis. It is of the common interest to analyze whether (or under what conditions) the (stochastic) gradient descent with the standard back-propagation converges to the global minimum of the regression problem and thus recovers the true weights.
1.2 MAIN CONTRIBUTIONS
Throughout this paper, we shall refer to the resultant composite "gradient" through STE as coarse gradient. The coarse gradient is basically not the gradient of any function. Our key contribution is to provide the first theoretical understanding of STE by analyzing the coarse gradient descent algorithm for learning a one-hidden-layer network with binary activation and Gaussian data. We consider two representative STEs: the derivatives of the identity function (Hinton, 2012) and the vanilla ReLU (Cai et al., 2017), and adopt the model of population loss minimization. We derive the explicit form of the expected coarse gradients corresponding to the two STEs, and show that the negative expected coarse gradient based on vanilla ReLU is a descent direction for the minimizing the population loss, whereas this is not necessarily true for the one based on the identity function. Moreover, we prove that the former guarantees the convergence to a critical point (a saddle point or a (local) minimizer) and the latter can be unstable sometimes near certain local minima. Indeed, in our experiment on CIFAR-10 using ResNet-20, it is observed that the training algorithm using the identity STE is repelled from good minima and converges to an inferior one with higher training loss and decreased generalization accuracy. This is an implication of the poor performance of the identity STE, not because of the slow convergence of its corresponding coarse gradient descent, instead due to the fact that the algorithm can never reach a good local minimum.
Notations. · denotes the Euclidean norm of a vector or the spectral norm of a matrix. 0n  Rn represents the vector of all zeros, whereas 1n  Rn the vector of all ones. In is the identity matrix of order n. For any w, z  Rn, w z = w, z = i wizi is their inner product. w z denotes the Hadamard product whose ith entry is given by (w z)i = wizi.

2 LEARNING ONE-HIDDEN-LAYER CNN WITH BINARY ACTIVATION

In this paper, we consider a one-hidden-layer network model (Du et al., 2018) that outputs the

prediction

m

y(Z, v, w) := vi(Zi w) = v (Zw)

i=1

for some input Z  Rm×n. Here w  Rn and v  Rm are the trainable weights in the first and second linear layer, respectively; Zi denotes the ith row vector of Z; the activation function  acts

2

Under review as a conference paper at ICLR 2019

component-wise on the vector Zw, i.e., (Zw)i = ((Zw)i) = (Zi w). The first layer serves as a convolutional layer, where each row Zi can be viewed as a patch sampled from Z and the weight filter w is shared among all patches, and the second linear layer is the classifier. The label is generated according to y(Z) = (v) (Zw) for some true (non-zero) parameters v and w.
Moreover, we use the following squared sample loss

(v, w; Z) := 1 (y(Z, v, w) - y(Z))2 = 1

v

(Zw) - y(Z)

2
.

22

(1)

Unlike in (Du et al., 2018), the activation function  here is not ReLU, but the binary function (x) = 1{x>0}, same as the hard threshold activation (Hinton, 2012).

We assume that the entries of Z  Rm×n are i.i.d. sampled from the Gaussian distribution N (0, 1)

(Zhong et al., 2017; Brutzkus & Globerson, 2017). The legitimacy of this assumption comes from

the use of batch normalization (Ioffe & Szegedy, 2015) in most architectures, which sends normal-

ized inputs to the linear layers. Since (v, w; Z) = (v, w/c; Z) for any scalar c > 0, without loss of generality, we take w = 1 and cast the learning task as the following population loss

minimization problem:

min f (v, w) := EZ [ (v, w; Z)] ,
v Rm ,wRn

(2)

where the sample loss (v, w; Z) is given by (1).

2.1 BACK-PROPAGATION AND COARSE GRADIENT DESCENT

With the Gaussian assumption on Z, as will be shown in section 2.2, it is possible to find the analytic expressions of f (v, w) and its gradient

f (v, w) :=

f v

(v,

w)

f w

(v,

w)

.

The information about f (v, w), however, is not available for the network training. In fact, we can

only use the expectation of the sample gradient, namely,



EZ

(v, w; Z) v

and EZ

(v, w; Z) . w

We remark that the expected partial gradient EZ

 w

(v,

w;

Z)

is

not

the

same

as

f w

(v,

w)

=

 EZ [

(v,w;Z)] w

.

By

the

standard

back-propagation

or

chain

rule,

we

readily

check

that

 (v, w; Z) = (Zw) v (Zw) - y(Z) v

(3)

and

 (v, w; Z) = Z

 (Zw)

v

v (Zw) - y(Z) .

(4)

w

Note that  is zero a.e., which makes (4) inapplicable to the training. The idea of STE is to simply

replace the a.e. zero component  in (4) with a related non-trivial function µ (Hinton, 2012; Bengio

et al., 2013; Hubara et al., 2016; Cai et al., 2017), which is the derivative of some (sub)differentiable

function µ. More precisely, back-propagation using the STE µ gives the following non-trivial sur-

rogate

of

 w

(v,

w;

Z),

to

which

we

refer

as

the

coarse

(partial)

gradient

gµ(v, w; Z) = Z µ (Zw) v v (Zw) - y(Z) .

(5)

Using the STE µ to train the one-hidden-layer convolutional neural network (CNN) with binary activation gives rise to the (full-batch) coarse gradient descent described in Algorithm 1.

2.2 PRELIMINARIES

Let us present some preliminaries about the landscape of the population loss function f (v, w).

To this end, we define the angle between w and w as (w, w) := arccos

w w w w

for any

w = 0n. Recall that the label is given by y(Z) = (v) Zw, we elaborate on the expressions of f (v, w) and f (v, w).

3

Under review as a conference paper at ICLR 2019

Algorithm 1 Coarse gradient descent for learning one-hidden-layer CNN with STE µ .

Input: initialization v0  Rm, w0  Rn, learning rate .

for t = 1, 2, . . . do

vt+1 = vt - EZ

 v

(vt

,

wt

;

Z)

wt+1 = wt - EZ [gµ(vt, wt; Z)]

end for

Lemma 1. Suppose all entries of Z follow i.i.d. N (0, 1). If w = 0n, the population loss f (v, w) is given by

1 v
8

Im + 1m1m v - 2v

1 - 2 (w, w) 

Im + 1m1m

v + (v)

Im + 1m1m v .

In

addition,

f (v, w)

=

1 8

(v)

Im + 1m1m v for w = 0n.

All the technical proofs in this paper will be detailed in the appendix. Lemma 2. If w = 0n and (w, w)  (0, ), the partial gradients of f (v, w) w.r.t. v and w are

f 1

1

(v, w) = v

4

Im + 1m1m

v- 4

1 - 2 (w, w) 

Im + 1m1m

v

(6)

and respectively.

f (v, w) = - v v w 2 w

In

-

ww w

2

w

,

In

-

ww w

2

w

(7)

For any v  Rm, (v, 0m) is impossible to be a local minimizer. The only possible (local) minimizers of the model (2) are located at

1. Stationary points where the gradients given by (6) and (7) vanish simultaneously (which may not be possible), i.e.,

v v = 0 and v = Im + 1m1m -1

1 - 2 (w, w) 

Im + 1m1m

v.

(8)

2. Non-differentiable points where (w, w) = 0 and v = v, or (w, w) =  and v = Im + 1m1m -1(1m1m - Im)v.

Among them, {(v, w) : v = v, (w, w) = 0} are obviously the global minimizers of (2). We show that the stationary points, if exist, can only be saddle points, and {(v, w) : (w, w) = , v = Im + 1m1m -1(1m1m - Im)v} are the only potential spurious local minimizers.

Proposition 1.

If

the

true

parameter

v

satisfies

(1mv)2

<

m+1 2

v

2, then

(v, w) : v = (Im + 1m1m)-1

(m

+

-(1mv)2 1) v 2 - (1mv)2

Im

+

1m1m

v,

(w, w)

=

 2

(m + 1) (m + 1) v 2

v 2 - (1mv)2

(9)

give the saddle points obeying (8), and {(v, w) : (w, w) = , v = Im + 1m1m -1(1m1m - Im)v} are the spurious local minimizers. Otherwise, the model (2) has no saddle points or spurious local minimizers.

We further prove that the underlying true gradient f (v, w) given by (6) and (7), is Lipschitz continuous under a boundedness condition.
Lemma 3. For any differentiable points (v, w) and (v~, w~) with min{ w , w~ } = cw > 0 and max{ v , v~ } = Cv, there exists a Lipschitz constant L > 0 depending on Cv and cw, such that
f (v, w) - f (v~, w~)  L (v, w) - (v~, w~) .

4

Under review as a conference paper at ICLR 2019

3 MAIN RESULTS

We are most interested in the complex case where both the saddle points and spurious local mini-
mizers are present. Our main results are concerned with the behaviors of the coarse gradient descent
summarized in Algorithm 1 when the derivatives of the vanilla ReLU and identity function serve
as the STE, respectively. Intuitively, the ReLU STE is supposed to outperform the identity STE, because ReLU is obviously a better approximation to the activation function (x) = 1{x>0}.
Theorem 1. Let {(vt, wt)} be the sequence generated by Algorithm 1 with ReLU µ(x) = max{x, 0}. Suppose vt  Cv and wt  cw for all t with some Cv, cw > 0. Then if the learning rate  > 0 is sufficiently small, for any initialization (v0, w0), the objective sequence {f (vt, wt)} is monotonically decreasing, and {(vt, wt)} converges to a saddle point or a (local) minimizer of the population loss minimization (2). In addition, if 1mv = 0 and m > 1, the descent and convergence properties do not hold for Algorithm 1 with the identity function µ(x) = x near the local minimizers satisfying (w, w) =  and v = (Im + 1m1m)-1(1m1m - Im)v.
Remark 1. The convergence guarantee for the coarse gradient descent using vanilla ReLU is estab-
lished under the assumption that there are infinite training samples. When there are only a few data,
in a coarse scale, the empirical loss is roughly descending along the direction of negative coarse
gradient, as illustrated by Figure 1. As the sample size increases, the empirical loss gains mono-
tonicity and smoothness. This is different from the conventional gradient descent widely studied in
the existing literature, which enjoys the descent property regardless of the sample size.

In the rest of this section, we sketch the mathematical analysis for the main results.

sample size = 10

sample size = 50

sample size = 1000

Figure 1: The plots of the empirical loss moving by one step in the direction of negative coarse gradient v.s. the learning rate (step size)  for different sample sizes.

3.1 DERIVATIVE OF THE VANILLA RELU AS STE

If we choose the derivative of ReLU µ(x) = max{x, 0} as the STE in (5), it is easy to see

µ (x) = (x), and we have the following expressions of EZ

 v

(v,

w;

Z)

and EZ grelu(v, w; Z)

for Algorithm 1.

Lemma 4. The expected partial gradient of (v, w; Z) w.r.t. v is

 f

EZ

(v, w; Z) v

= (v, w). v

(10)

Let µ(x) = max{x, 0} in (5). The expected coarse gradient w.r.t. w is

EZ grelu(v, w; Z)

= h(v, v) 2 2

w w

- cos

(w, w) 2

v v 2

w w

+ w

,1

w w

+ w

(11)

where h(v, v) = v 2 + (1mv)2 - (1mv)(1mv) + v v.

The key observation is that the coarse partial gradient EZ grelu(v, w; Z) has non-negative correla-

tion with the true gradient

f w

(v,

w),

and

-EZ

grelu(v, w; Z)

together with -EZ

 v

(v,

w;

Z)

form a descent direction for minimizing the population loss.

1We redefine the second term as 0n in the case (w, w) = , or equivalently,

w w

+ w = 0n.

5

Under review as a conference paper at ICLR 2019

Lemma 5. If w = 0n and (w, w)  (0, ), then the inner product between the expected coarse and true gradients w.r.t. w is

f

EZ

grelu(v, w; Z)

, (v, w) w

= sin((w, w)) (v v)2  0. 2( 2)3 w

Moreover, if further v  Cv, there exists a constant A > 0 depending only on Cv, such that

2
EZ grelu(v, w; Z)  A

f 2

f

(v, w) v

+

EZ

grelu(v, w; Z)

, (v, w) w

. (12)

Clearly, when

EZ

grelu(v, w; Z)

,

f w

(v,

w)

> 0, EZ grelu(v, w; Z) is roughly in the same

direction

as

f w

(v,

w).

Moreover,

since by

Lemma

4,

EZ

 v

(v,

w;

Z)

=

f v

(v,

w),

we expect

that the coarse gradient descent behaves like the gradient descent directly on f (v, w). We would like

to highlight the significance of the estimate (12) in guaranteeing the descent property of Algorithm

1. By the Lipschitz continuity of f specified in Lemma 3, it holds that

f (vt+1, wt+1) - f (vt, wt)  f (vt, wt), vt+1 - vt + f (vt, wt), wt+1 - wt v w

+

L (

vt+1 - vt

2+

wt+1 - wt

2)

2

= -  - L2 2

f (vt, wt)

2 L2 +

v 2

EZ grelu(vt, wt; Z)

2

-

f w

(vt

,

wt),

EZ

grelu(vt, wt; Z)

a)
-

 - (1 + A) L2

2

f (vt, wt) 2 v

-  - AL2 2

f w

(vt

,

wt

),

EZ

grelu(vt, wt; Z)

,

(13)

where a) is due to (12). Therefore, if  is small enough, we have monotonically decreasing population loss until convergence.

Lemma 6. When Algorithm 1 converges, EZ

 v

(v,

w;

Z)

and EZ grelu(v, w; Z)

vanish simul-

taneously, which only occurs at the

1. Saddle points where (8) is satisfied according to Proposition 1.

2.

Minimizers of (2) where v (w, w) = .

= v, (w, w) = 0, or v

= (Im + 1m1m)-1(1m1m - Im)v,

3.2 DERIVATIVE OF THE IDENTITY FUNCTION AS STE

Now we consider the derivative of identity function. As opposed to the ReLU case, similar results to Lemmas 5 and 6 are not valid anymore. It happens that the coarse gradient derived from the identity STE does not vanish at the local minimum, and Algorithm 1 may never converge there.
Lemma 7. Let µ(x) = x in (5). Then the expected coarse partial gradient w.r.t. w is

EZ gid(v, w; Z)

= 1 2

v 2 w - (v v)w . w

If (w, w) =  and v = Im + 1m1m -1(1m1m - Im)v,

EZ gid(v, w; Z)

=

 2(m - 1) 2(m + 1)2

(1m

v

)2



0,

(14)

i.e., EZ gid(v, w; Z) does not vanish at the spurious local minimizers if 1mv = 0 and m > 1.

6

Under review as a conference paper at ICLR 2019

Lemma 8. If w = 0n and (w, w)  (0, ), then the inner product between the expected coarse and true gradients w.r.t. w is

f

EZ

gid(v, w; Z)

, (v, w) w

= sin((w, w)) (v v)2  0. ( 2)3 w

(15)

When (w, w)  , v  Im + 1m1m -1(1m1m - Im)v, if 1mv = 0 and m > 1, we have

2
EZ gid(v, w; Z)

2

f v

(v,

w)

+

EZ

gid(v, w; Z)

,

f w

(v,

w)

 +.

(16)

Lemma 7 suggests that if 1mv = 0, the coarse gradient descent will never converge near the spurious minimizers with (w, w) =  and v = Im + 1m1m -1(1m1m - Im)v, because
EZ gid(v, w; Z) does not vanish there. By the positive correlation implied by (15), for some proper
(v0, w0), the iterates {(vt, wt)} may move towards a spurious local minimizer in the beginning. But when {(vt, wt)} approaches it, the correlation in (15) goes to 0, and the descent property (13) does not hold with EZ [gid(v, w; Z)] because of (16), hence the instability arises.

4 EXPERIMENTS
In practice, the vanilla ReLU may not deliver the best empirical performance. Compared with the identity function and vanilla ReLU, the clipped ReLU proposed in (Cai et al., 2017) approximates the quantized ReLU better. The plots of the 2-bit quantized ReLU and its associated clipped ReLU are in Figure 3 in the appendix. Besides the two STEs discussed above, we include the STE using derivative of the clipped ReLU for comparisons on training 2-bit and 4-bit activation networks for MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky, 2009) classifications. Log-tailed ReLU has also been proposed, we do not consider it here since it gives similar performance to the clipped ReLU as reported in (Cai et al., 2017). We emphasize that we are not claiming the superiority of the quantization approach used here, as it is nothing but the HWGQ (Cai et al., 2017), except we consider the uniform quantization. In all of our experiments, the weights are kept float.
The optimizer we use is the stochastic (coarse) gradient descent with momentum = 0.9 for all experiments. We train 50 epochs for LeNet-5 (LeCun et al., 1998) on MNIST, and 200 epochs for VGG-11 (Simonyan & Zisserman, 2014) and ResNet-20 (He et al., 2016) on CIFAR-10. The parameters/weights are initialized with those from their pre-trained full-precision counterparts. The schedule of the learning rate is specified in Table 2 in the appendix.
The resolution  for the quantized ReLU needs to be carefully chosen to maintain the full-precision level accuracy. To this end, we follow (Cai et al., 2017) and resort to a modified batch normalization layer (Ioffe & Szegedy, 2015) without the scale and shift, whose output components approximately follow a Gaussian distribution. Then the  that fits the layer input the best can be pre-computed by a variant of Lloyd's algorithm (Lloyd, 1982; Yin et al., 2018) applied to a set of simulated 1-D half-Gaussian data. After determining the , it will be fixed during the whole training process. Since the original LeNet-5 does not have batch normalization, we add one prior to each activation layer.

4.1 COMPARISON RESULTS
The experimental results are summarized in Table 1, where we record both the training losses and validation accuracies. Among the three STEs, the derivative of clipped ReLU gives the best overall performance, followed by vanilla ReLU and then by the identity function. For deeper networks, clipped ReLU is the best performer. But on the shallow LeNet-5 network, vanilla ReLU exhibits comparable performance to clipped ReLU, which is in line with our theoretical finding that ReLU is a superior STE for learning the one-hidden-layer (shallow) CNN.

4.2 REPELLED FROM IMPROVED MINIMA
We report the phenomenon of being repelled from a good minimum on ResNet-20 with 4-bit activations when using the identity STE. By Table 1, the coarse gradient descent algorithms using the

7

Under review as a conference paper at ICLR 2019

MNIST CIFAR10

Network LeNet5 VGG11 ResNet20

BitWidth
2 4 2 4 2 4

Straight-through estimator

identity

vanilla ReLU

clipped ReLU

2.6 × 10-2/98.49 5.1 × 10-3/99.24 5.4 × 10-3/99.23 6.0 × 10-3/98.98 9.0 × 10-4/99.32 8.8 × 10-4/99.24

0.19/86.58 3.1 × 10-2/90.19
1.56/46.52
1.38/54.16

0.10/88.69 1.5 × 10-3/92.01
1.50/48.05
0.25/86.59

0.02/90.92 1.3 × 10-3/92.08
0.24/88.39
0.04/91.24

Table 1: Training loss/validation accuracy (%) on MNIST and CIFAR-10 with quantized activations and float weights, for STEs using derivatives of the identity function, vanilla ReLU and clipped ReLU at bit-widths 2 and 4.

vanilla and clipped ReLUs converge to the neighborhoods of the minima with validation accuracies (training losses) of 86.59% (0.25) and 91.24% (0.04), respectively, whereas that using the identity STE gives 54.16% (1.38). Note that the landscape of the empirical loss function does not depend on which STE is used in the training. Then we initialize training with the two improved minima and use the identity STE. To see if the algorithm is stable there, we start the training with a tiny learning rate of 10-5. For both initializations, the training loss and validation error significantly increase within the first 20 epochs. At epoch 20, we switch to the normal schedule of learning rate and run 200 additional epochs. The training using the identity STE ends up with a much worse minimum. This is because the coarse gradient with identity STE does not vanish at the good minima in this case.

Figure 2: When initialized with the improved minima produced by the vanilla (orange) and clipped (blue) ReLUs on ResNet-20 with 4-bit activations and float weights, the coarse gradient descent using the identity STE ends up being repelled from there. The learning rate is set to 10-5 until epoch 20.
5 CONCLUDING REMARKS
We provided the first theoretical justification for the concept of STE. We considered two STEs: the derivatives of the identity function and the vanilla ReLU, in training one-hidden-layer CNN with binary activation. We derived the explicit formulas of the expected coarse gradients corresponding to the two STEs, and showed that the negative expected coarse gradient based on vanilla ReLU is a descent direction for minimizing the population loss, whereas the identity STE is not. Our experiments on MNIST and CIFAR-10 datasets verified the theoretical findings. Looking ahead, we believe that the closeness between the quantized ReLU and the anti-derivative of STE plays an essential role in the performance of the coarse gradient descent. Hence, we would like to explore and quantify this relationship in the future work.
ACKNOWLEDGMENTS
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Yoshua Bengio, Nicholas Le´onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave gaussian quantization. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, pp. 160­167. ACM, 2008.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123­3131, 2015.
Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minimum. arXiv preprint arXiv:1712.00779, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Geoffrey Hinton. Neural networks for machine learning, coursera. Coursera, video lectures, 2012.
Lu Hou and James T Kwok. Loss-aware weight quantization of deep networks. arXiv preprint arXiv:1802.08635, 2018.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training neural networks with weights and activations constrained to +1 or -1. arXiv preprint arXiv:1602.02830, 2016.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. Journal of Machine Learning Research, 18:1­30, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016.
9

Under review as a conference paper at ICLR 2019
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems, pp. 5811­5821, 2017.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
Stuart P. Lloyd. Least squares quantization in pcm. IEEE Trans. Info. Theory, 28:129­137, 1982.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pp. 525­542. Springer, 2016.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91­99, 2015.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Mahdi Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information Processing Systems, pp. 2007­2017, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, and Stanley J Osher. Deep neural nets with interpolating function as output activation. In Advances in Neural Information Processing Systems, 2018.
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Binaryrelax: A relaxation approach for training deep neural networks with quantized weights. arXiv preprint arXiv:1801.06313, 2018.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv preprint arXiv:1612.01064, 2016.
10

Under review as a conference paper at ICLR 2019

APPENDIX
A. THE PLOTS OF QUANTIZED AND CLIPPED RELUS quantized ReLU

clipped ReLU

Figure 3: The plots of 2-bit quantized ReLU (x) (with 22 = 4 quantization levels including 0) and the associated clipped ReLU ~(x).  is the resolution determined in advance of the network training.

B. THE SCHEDULE OF LEARNING RATE

Network
LeNet5 VGG11 ResNet20

Epoch
50 200 200

initial 0.1 0.01 0.01

Learning rate decay rate milestone
0.1 [20,40] 0.1 [80,140] 0.1 [80,140]

Table 2: The schedule of the learning rate.

C. ADDITIONAL SUPPORTING LEMMAS

Lemma 9. Let z  Rn be a Gaussian random vector with entries i.i.d. sampled from N (0, 1). Given nonzero vectors w, w~  Rn with the angle , we have

1 -

E 1{z w>0}

=

, 2

E

1{z

w>0, z

w~ >0}

=

, 2

and

E z1{z w>0}

= 1 2

w w

, E z1{z w>0, z w>0}

= cos(/2) 2

w w

+

w~ w~

w w

+

w~ w~

.2

Proof of Lemma 9. The third identity was proved in Lemma A.1 of (Du et al., 2018). To show

the first one, without loss of generality we assume w = [w1, 0n-1] with w1 > 0, then

E

1{z

w>0}

=

P(z1

>

0)

=

1 2

.

We further assume w~ = [w~1, w~2, 0n-2] . It is easy to see that

-

E 1{z w>0, z w~>0} = P(z w > 0, z w~ > 0) =

. 2

To prove the last identity, we use polar representation of two-dimensional Gaussian random vari-

ables,

where r

is

the radius

and



is

the angle

with dPr

=

r exp(-r2/2)dr

and

dP

=

1 2

d.

Then

2Same as in Lemma 4, we redefine E z1{z w>0, z w>0} = 0n in the case (w, w) = .

11

Under review as a conference paper at ICLR 2019

E zi1{z w>0, z w>0} = 0 for i  3. Moreover,

E z11{z w>0, z w>0}

1 =
2


r2 exp
0

r2 -
2



2
dr

cos()d = 1 +cos()

-

 2

+

2 2

and

E z21{z w>0, z w>0}

1 =
2


r2 exp
0

r2 -
2



2
dr

sin()d = sin() .

-

 2

+

2 2

Therefore,

E

z1{z

w>0, z

w >0}

=

cos(/2) 2

[cos(/2),

sin(/2),

0n-2]

= cos(/2) 2

w w

+

w~ w~

w w

+

w~ w~

,

where the last equality holds because

w w

and

/2.

w w

+

w~ w~

w w

+

w~ w~

are two unit-normed vectors with angle

Lemma 10. For any nonzero vectors w and w~ with w~  w = c > 0, we have

1.

|(w, w)

-

(w~, w)|



 2c

w - w~

.

2.

1 w

In -

ww w

2

w

In -

ww w

2

w

-

1 w~

In -

w~ w~ w~

2

w

In -

w~ w~ w~

2

w



1 c2

w - w~

.

Proof of Lemma 10. 1. Since by Cauchy-Schwarz inequality,

w~, w - cw~ = w~ w - c w~  0, w~

we have w~ - w 2 = 

1- c w~
w - cw~ w~

w~ -
2
= c2

w - cw~ w~
w - w~ w w~

2

2
.

1- c w~

2
w~ +

w-

cw~

2

w~

(17)

Therefore,

|(w, w) - (w~, w)|  (w, w~) = 

w w~ ,

w w~

 
  sin 

w w

,

w~ w~

2

 
= 2

w - w~ w w~

  w - w~ , 2c

where

we

used

the

fact

sin(x)



2x 

for

x



[0,

 2

]

and

the

estimate

in

(17).

2. Since

In

-

ww w

2

w is the projection of w onto the complement space of w, and likewise for

In

-

w~ w~ w~ 2

w, the angle between

In

-

ww w2

w and

In

-

w~ w~ w~ 2

w is equal to the angle

between w and w~. Therefore,

In

-

ww w

2

w

In

-

w~ w~ w~

2

w

w w~

, =,,

In

-

ww w

2

w

In

-

w~ w~ w~

2

w

w w~

12

Under review as a conference paper at ICLR 2019

and thus

1

In

-

ww w

2

w

-

1

In

-

w~ w~ w~ 2

w

w

In

-

ww w

2

w

w~

In

-

w~ w~ w~ 2

w

=

w w

2

-

w~ w~ 2

=

w - w~ w w~



1 c2

w - w~

.

The second equality above holds because

w w

2

-

w~ 2 w~ 2 =

1 w 2+

1 w~

2

-

2 w, w~ w 2 w~ 2 =

w - w~ 2 w 2 w~ 2 .

D. MAIN PROOFS

Lemma 1. Suppose all entries of Z follow i.i.d. N (0, 1). If w = 0n, the population loss f (v, w) is given by

1 v
8

Im + 1m1m v - 2v

1 - 2 (w, w) 

Im + 1m1m

v + (v)

Im + 1m1m v .

In

addition,

f (v, w)

=

1 8

(v)

Im + 1m1m v for w = 0n.

Proof of Lemma 1. We notice that

1 f (v, w) =
2

v

EZ[(Zw)

(Zw)]v - 2v

EZ[(Zw)

(Zw)]v

+ (v) EZ[(Zw) (Zw)]v .

Let Zi be the ith row vector of Z. Since w = 0n, using Lemma 9, we have
EZ (Zw)(Zw) ii = E (Zi w)(Zi w) = E 1{Zi w>0} and for i = j,

1 =,
2

1

EZ (Zw)(Zw)

ij = E (Zi w)(Zj w)

=E

1{Zi w>0}

E

1{Zj w>0}

=. 4

Therefore, EZ (Zw)(Zw)

= EZ (Zw)(Zw)

=

1 4

Im + 1m1m . Furthermore,

EZ (Zw)(Zw)

 - (w, w)

ii = E 1{Zi w>0,Zi w>0} =

, 2

and EZ (Zw)(Zw)

ij

=

1 4

.

So,

EZ (Zw)(Zw)

1 =
4

1 - 2(w, w) 

Im + 1m1m

.

Then it is easy to validate the first claim. Moreover, if w = 0n, then

f (v, w) = 1 (v) 2

EZ[(Zw)

(Zw)]v = 1 (v) 8

Im + 1m1m v.

Lemma 2. If w = 0n and (w, w)  (0, ), the partial gradients of f (v, w) w.r.t. v and w are

f 1

1

(v, w) = v

4

Im + 1m1m

v- 4

1 - 2 (w, w) 

Im + 1m1m

v

and

f v v (v, w) = -
w 2 w

In

-

ww w

2

w

,

In

-

ww w

2

w

respectively.

13

Under review as a conference paper at ICLR 2019

Proof of Lemma 2. The first claim is trivial, and we only show the second one. Since (w, w) =

arccos

w w w

is differentiable w.r.t. w at (w, w)  (0, ), we have

f (v, w) = v v  (w, w) = - v v w 2w - (w w)w

w 2 w

2

w3

1

-

(w w)2 w2

= - v v 2 w

In

-

ww w2

w

.

In

-

ww w2

w

Proposition 1.

If

the

true

parameter

v

satisfies

(1mv)2

<

m+1 2

v

2, then

(v, w) : v = (Im + 1m1m)-1

(m

+

-(1mv)2 1) v 2 - (1mv)2

Im

+

1m1m

v,

(w, w)

=

 2

(m

(m + 1) + 1) v 2

v 2 - (1mv)2

give the saddle points obeying (8), and {(v, w) : (w, w) = , v = Im + 1m1m -1(1m1m - Im)v} are the spurious local minimizers. Otherwise, the model (2) has no saddle points or spurious local minimizers.

Proof of Proposition 1.

Suppose v

v

=

0

and

f v

(v,

w)

=

0,

then

by

Lemma

1,

0 = v v = (v) (Im + 1m1m)-1

1 - 2 (w, w) 

Im + 1m1m

v.

(18)

From (18) it follows that

2 (w, w)(v) 

(Im +1m1m)-1v = (v)

(Im +1m1m)-1

Im + 1m1m

v =

v 2. (19)

On the other hand, from (18) it also follows that

2 (w, w) - 1 

(v)

(Im + 1m1m)-1v = (v)

(Im

+

1m1m)-11m(1mv)

=

(1 m

v)2 ,
+1

where we used (Im + 1m1m)1m = (m + 1)1m. Taking the difference of the two equalities above

gives

(v)

(Im + 1m1m)-1v =

v 2 - (1mv)2 . m+1

(20)

By

(19),

we

have

(w, w)

=

 2

,(m+1) v 2
(m+1) v 2-(1mv)2

which

requires

 (m + 1) v 2 2 (m + 1) v 2 - (1mv)2

< ,

or equivalently,

(1mv)2

<

m+ 2

1

v

2.

Furthermore,

since

f v

(v,

w)

=

0,

we

have

v = (Im + 1m1m)-1

1 - 2 (w, w) 

Im + 1m1m

v

= (Im + 1m1m)-1

(m

+

-(1mv)2 1) v 2 - (1mv)2

Im

+

1m1m

v.

Next, we check the local optimality of the staionary points. By ignoring the scaling and constant terms, we rewrite the objective function as

f~(v, ) := v Im + 1m1m v - 2v

1-

2 



Im + 1m1m

v, for   [0, ].

14

Under review as a conference paper at ICLR 2019

It is easy to check that its Hessian matrix

2f~(v, ) =

2(Im + 1m1m)

4 

(v)

4 

v

0

is indefinite. Therefore, the stationary points are saddle points.

Moreover,

if

(1mv)2

<

m+1 2

v

2, at the point (v, ) = ((Im + 1m1m)-1(1m1m - Im)v, ),

we have

v v = (v) (Im + 1m1m)-1(1m1m - Im)v

=

v 2 - 2(v)

(Im

+ 1m1m)-1v

=

2(1mv)2 m+1

-

v 2 < 0,

(21)

where we used (20) in the last identity. We consider an arbitrary point (v + v,  + ) in the neighborhood of (v, ) with   0. The perturbed objective value is

f~(v + v,  + ) = (v + v) Im + 1m1m (v + v) - 2(v + v)

2 + (v + v)

v.



1m1m - Im v

On the right hand side, since v = (Im + 1m1m)-1(1m1m - Im)v is the unique minimizer to the quadratic function f~(v, ), we have if v = 0m,

(v + v) Im + 1m1m (v + v) - 2(v + v) 1m1m - Im v > f~(v, ).

Moreover, for sufficently small v , it holds that  · (v + v) v > 0 for  < 0 because of (21). Therefore, f~(v + v,  + ) > f~(v, ) whenever (v, ) is small and non-zero, and ((Im + 1m1m)-1(1m1m - Im)v, ) is a spurious local minimizer of f~.

To prove the second claim, suppose (1mv)2



m+1 2

v

2, then either

f w

(v,

w)

does

not

exist,

or

f v

(v,

w)

and

f w

(v,

w)

do

not

vanish

simultaneously,

and

thus

there

is

no

stationary

point.

At the point (v, ) = ((Im + 1m1m)-1(1m1m - Im)v, ), we have

v v = 2(1mv)2 - v 2  0. m+1

If v

v > 0, since f~(v, ) =

1 4

[0m,

2 

v

v]

, a small perturbation [0m, ]

with  < 0

will give a strictly decreased objective value, so (v, ) = (Im + 1m1m)-1(1m1m - Im)v,  is

not a local minimizer. If v v = 0, then f~(v, ) = 0n+1, the same conclusion can be reached by

examining the second order necessary condition.

Lemma 3. For any differentiable points (v, w) and (v~, w~) with min{ w , w~ } = cw > 0 and max{ v , v~ } = Cv, there exists a Lipschitz constant L > 0 depending on Cv and cw, such that
f (v, w) - f (v~, w~)  L (v, w) - (v~, w~) .

Proof of Lemma 3. It is easy to check that Im + 1m1m = m + 1. Then

f (v, w) - f (v~, w~) v v

=

Im + 1m1m

(v - v~) +

2 ((w, w) - (w~, w))v 

 (m + 1) v - v~

2 v +

|(w, w) - (w~, w)|



 (m + 1) v - v~ + v w - w~ cw



v m+1+

cw

(v, w) - (v~, w~) ,

where the last inequality is due to Lemma 10.1.

15

Under review as a conference paper at ICLR 2019

We further have

f f

v v

(v, w) - (v~, w~) =

w w

2 w

In

-

ww w

2

w

v~ v -

In

-

ww w

2

w

2 w~

In

-

w~ w~ w~ 2

w

In

-

w~ w~ w~ 2

w

 v v 2 w

In

-

ww w

2

w

- v v

In

-

ww w

2

w

2 w~

In

-

w~ w~ w~ 2

w

In

-

w~ w~ w~ 2

w

v v +
2 w~

In

-

w~ w~ w~ 2

w

In

-

w~ w~ w~ 2

w

- v~ v 2 w~



|v v| 2cw2

w - w~

v +
2cw

v - v~



(Cv + cw) 2c2w

v

(v, w) - (v~, w~) ,

In

-

w~ w~ w~ 2

w

In

-

w~ w~ w~ 2

w

where the second last inequality is to due to Lemma 10.2. Combining the two inequalities above validates the claim.

Lemma 4. The expected partial gradient of (v, w; Z) w.r.t. v is

 f

EZ

(v, w; Z) v

= (v, w). v

Let µ(x) = max{x, 0} in (5). The expected coarse gradient w.r.t. w is

EZ grelu(v, w; Z)

= h(v, v) 2 2

w w

- cos

(w, w) 2

v v 2

where h(v, v) = v 2 + (1mv)2 - (1mv)(1mv) + v v.

w w

+ w

,3

w w

+ w

Proof of Lemma 4.

The first claim is true because

 v

(v,

w;

Z)

is

linear

in

v.

By

(5),

gµ(v, w; Z) = Z µ (Zw) v v (Zw) - (v) (Zw) .

Using the fact that µ =  = 1{x>0}, we have

EZ [grelu(v, w; Z)] = EZ = EZ
Invoking Lemma 9, we have

mm
vi(Zi w) - vi(Zi w)

i=1 i=1

mm

vi1{Zi w>0} -

vi1{Zi w>0}

i=1 i=1

m
Zivi(Zi w)
i=1 m
1{Zi w>0}viZi
i=1

E Zi1{Zi w>0,Zj w>0} =

1 w 2 w 1 w
2 2 w

if i = j, if i = j,

and

E Zi1{Zi w>0,Zj w>0}

 

cos((w,w

)/2)

= 2

 1
2 2

w w

w w

+w

w w

+w

if i = j, if i = j.

3We redefine the second term as 0n in the case (w, w) = , or equivalently,

w w

+ w = 0n.

.

16

Under review as a conference paper at ICLR 2019

Therefore,

m
EZ [grelu(v, w; Z)] = vi2E
i=1

Zi1{Zi w>0}

mm
+ vivj E
i=1 j=1 j=i

Zi1{Zi w>0,Zj w>0}

m mm

- viviE Zi1{Zi w>0,Zi w>0} -

vivjE Zi1{Zi w>0,Zj w>0}

i=1

i=1 j=1

j=i

= 1 2 2

v 2 + (1mv)2

w - cos (w, w) v v w 2 2

w w

+ w

w w

+ w

- 1 2 2

(1mv)(1mv) - v

v

w ,
w

and the result follows.

Lemma 5. If w = 0n and (w, w)  (0, ), then the inner product between the expected coarse and true gradients w.r.t. w is

f

EZ

grelu(v, w; Z)

, (v, w) w

= sin((w, w)) (v v)2  0. 2( 2)3 w

Moreover, if further v  Cv, there exists a constant A > 0 depending only on Cv, such that

2
EZ grelu(v, w; Z)  A

f 2

f

(v, w) v

+

EZ

grelu(v, w; Z)

, (v, w) w

.

Proof of Lemma 5. By Lemmas 2 and 5, we have

f v v (v, w) = -
w 2 w

In

-

ww w2

w

In

-

ww w2

w

and

EZ grelu(v, w; Z)

= h(v, v) 2 2

w w

- cos

(w, w) 2

v v 2

w w

+ w

.

w w

+ w

17

Under review as a conference paper at ICLR 2019

Notice that

In

-

ww w

2

w = 0n and

w

= 1, if (w, w) = 0, , then we have

f

EZ

grelu(v, w; Z)

, (v, w) w

= cos = cos = cos = cos

(w, w) 2
(w, w) 2
(w, w) 2
(w, w) 2

(v v)2

1

In

-

ww w2

w

,

w

( 2)3

w

In

-

ww w2

w

w w

+ w

(v v)2 ( 2)3

w 2 - (w w)2 w 2w - w(w w) w + w w

(v v)2 ( 2)3

w 2 - (w w)2 w 4 - w 2(w w)2 2( w 2 + w (w w))

(v v)2

w 2 - (w w)2

4(  w )3 w 2 - (w w)2 w + (w w)

= cos

(w, w)

(v v)2 

1

-

w w w

2 4( )3 w

= cos

(w, w) 2

(v v)2 1 - cos((w, w)) 4( )3 w

= sin((w, w)) (v v)2. 2( 2)3 w

To show the second claim, without loss of generality, we assume w = 1. Denote  := (w, w). By Lemma 1, we have

f 1

1

(v, w) = v 4

Im + 1m1m

v- 4

1 - 2 

Im + 1m1m

v.

By Lemma 4,

EZ grelu(v, w; Z)

= h(v, v) w - cos 2 2

 2

v v 2

w + w w + w

,

(22)

where

h(v, v) = v 2 + (1mv)2 - (1mv)(1mv) + v v

= v Im + 1m1m v - v (1m1m - Im)v

= v Im + 1m1m v - v

1m1m +

1 - 2 

Im

v + 2

1-  

v v

= 4v

f (v, w) + 2

1- 

v v,

v 

(23)

and by the first claim,

f

EZ

grelu(v, w; Z)

, (v, w) w

= sin () (v v)2. 2( 2)3

18

Under review as a conference paper at ICLR 2019

Hence, for some A depending only on Cv, we have

2
EZ grelu(v, w; Z)

=

2v

f
v

(v,

w)

w

+

cos

2

 2

v v 2

w-

w + w w + w

+ 1 -  - cos  2

v v w 2 2

 6Cv2

f (v, w)

2
+ cos2

 v

 2

3(v v)2 2

w-

w + w w + w

2

+ 1 -  - cos  2 3(v v)2  2 2

 6Cv2

f (v, w)

2
+ cos2



32 (v

v)2 +

1 -  - cos



2 3(v v)2

 v

2 8

 2 2

 6Cv2

f (v, w)

2 + 3 cos2



sin2



(v

v)2

+

3

sin() (v

v)2

 v

82

2

2

A

f 2

f

(v, w) v

+

EZ

grelu(v, w; Z)

, (v, w) w

,

where the equality is due to (22) and (23), the first inequality is due to Cauchy-Schwarz inequality,

the second inequality holds because the angle between w and

w+w w+w

is

 2

and

w-

w+w w+w



 2

,

whereas

the

third

inequality

is

due

to

sin(x)



2x 

,

cos(x)



1

-

2x 

,

and

1 - 2x - cos(x)

2


cos(x) - 1 + 2x



cos(x) + 1 - 2x  sin(x)(2 cos(x)) = sin(2x) 

for

all

x



[0,

 2

].

Lemma 6. When Algorithm 1 converges, EZ

 v

(v,

w;

Z)

and EZ grelu(v, w; Z)

vanish simul-

taneously, which only occurs at the

1. Saddle points where (8) is satisfied according to Proposition 1.

2.

Minimizers of (2) where v (w, w) = .

= v, (w, w) = 0, or v

= (Im + 1m1m)-1(1m1m - Im)v,

Proof of Lemma 6. By Lemma 4, suppose we have

EZ

 (v, w; Z)
v

1 =
4

Im + 1m1m

v-

1 4

1 - 2 (w, w) 

Im + 1m1m

v = 0m (24)

and

EZ grelu(v, w; Z)

= h(v, v) 2 2

w w

- cos

(w, w) 2

v v 2

w w

+ w

w w

+ w

= 0n,

(25)

where h(v, v) = v 2 + (1mv)2 - (1mv)(1mv) + v v. By (25), we must have (w, w) = 0 or (w, w) =  or v v = 0.
If (w, w) = 0, then by (24), v = v, and (25) is satisfied.
If (w, w) = , then by (24), v = (Im + 1m1m)-1(1m1m - Im)v, and (25) is satisfied. If v v = 0, then by (24), we have the expressions for v and (w, w) from Proposition 1, and (25) is satisfied.

19

Under review as a conference paper at ICLR 2019

Lemma 7. Let µ(x) = x in (5). Then the expected coarse partial gradient w.r.t. w is

EZ gid(v, w; Z)

= 1 2

v 2 w - (v v)w . w

If (w, w) =  and v = Im + 1m1m -1(1m1m - Im)v,

EZ gid(v, w; Z)

=

 2(m - 1) 2(m + 1)2

(1m

v

)2



0,

i.e., EZ gid(v, w; Z) does not vanish at the spurious local minimizers if 1mv = 0 and m > 1,.

Proof of Lemma 7. By (5),

gµ(v, w; Z) = Z µ (Zw) v v (Zw) - (v) (Zw) .

Using the facts that µ = 1 and  = 1{x>0}, we have

mm

m

EZ [gid(v, w; Z)] = EZ

vi1{Zi w>0} -

vi1{Zi w>0}

i=1 i=1

viZi
i=1

mm

mm

=

vivj E Zi1{Zj w>0} -

vivj E Zi1{Zj w>0}

i=1 j=1

i=1 j=1

= 1

v 2 w - (v v)w .

2 w

In the last equality above, we called the third identity in Lemma 9. If (w, w) =  and v = Im + 1m1m -1(1m1m - Im)v, then

EZ [gid(v, w; Z)]

= 1 |v 2

(v + v)|

= 1 2

(v)

(1m1m - Im) Im + 1m1m

-1

Im + 1m1m -1(1m1m - Im) + Im

= 2 2

(v)

(1m1m - Im) Im + 1m1m

-1

Im + 1m1m

-11m(1mv)

= 2 2(m + 1)2

(v)

(1m1m - Im)1m(1mv)

=

 2(m - 1) 2(m + 1)2

(1mv



)2

.

v

In the third equality, we used the identity (Im + 1m1m)1m = (m + 1)1m twice.

Lemma 8. If w = 0n and (w, w)  (0, ), then the inner product between the expected coarse and true gradients w.r.t. w is

f

EZ

gid(v, w; Z)

, (v, w) w

= sin((w, w)) (v v)2  0. ( 2)3 w

When (w, w)  , v  Im + 1m1m -1(1m1m - Im)v, if 1mv = 0 and m > 1, we have

2
EZ gid(v, w; Z)

2

f v

(v,

w)

+

EZ

gid(v, w; Z)

,

f w

(v,

w)

 +.

20

Under review as a conference paper at ICLR 2019

Proof of Lemma 8. By Lemmas 2 and 4, we have

f (v, w) = - v v w 2 w

In

-

ww w2

w

In

-

ww w2

w

and

EZ gid(v, w; Z)

= 1 2

v 2 w - (v v)w . w

Since

In

-

ww w

2

w = 0n and

w

= 1, if (w, w) = 0, , then we have

f

EZ

gid(v, w; Z)

, (v, w) w

= (v v)2 (w)

In

-

ww w

2

w

( 2)3 w

In

-

ww w2

w

= (v v)2 ( 2)3 w

1

-

(w w)2 w2

= (v v)2

1

-

(w w)2 w2

( 2)3 w

1

-

(w

w)2 w2

= (v v)2 sin((w, w)). ( 2)3 w

When (w, w)  , v 

Im + 1m1m -1(1m1m - Im)v, both

f v

(v,

w)

and

EZ

gid(v, w; Z)

,

f w

(v,

w)

converge to 0.

But if 1mv = 0 and m > 1,

EZ gid(v, w; Z)



 2(m-1) 2(m+1)2

(1m

v

)2

>

0,

which

completes

the

proof.

Theorem 1. Let {(vt, wt)} be the sequence generated by Algorithm 1 with ReLU µ(x) =
max{x, 0}. Suppose vt  Cv and wt  cw for all t with some Cv, cw > 0. Then if the learning rate  > 0 is sufficiently small, for any initialization (v0, w0), the objective sequence
{f (vt, wt)} is monotonically decreasing, and {(vt, wt)} converges to a saddle point or a (local) minimizer of the population loss minimization (2). In addition, if 1mv = 0 and m > 1, the descent and convergence properties do not hold for Algorithm 1 with the identity function µ(x) = x near the local minimizers satisfying (w, w) =  and v = (Im + 1m1m)-1(1m1m - Im)v.

Proof of Theorem 1 . If (wt, w) = 0 or , then (wT , w) = 0 or  for all T  t, and the

original problem reduces to a quadratic program in terms of v. So {vt} will converge to v or

(Im + 1m1m)-1(1m1m - Im)v by choosing a suitable step size . In either case, we have

EZ

 v

(vt,

wt

;

Z)

and EZ grelu(vt, wt; Z)

both converge to 0. Else if (wt, w)  (0, ),

we define for any a  [0, 1] that

vt(a) := vt - a(vt+1 - vt) = vt - aEZ

 (vt, wt; Z) v

and wt(a) := wt - a(wt+1 - wt) = wt - aEZ grelu(vt, wt; Z) ,

which satisfy

vt(0) = vt, vt(1) = vt+1, wt(0) = wt, wt(1) = wt+1.

Since by the assumption, vt  Cv and wt  cw for all t, for sufficiently small  > 0,

it holds that vt(a)  Cv and wt(a)  cw for all a  [0, 1]. Possibly at some point a0

where

(wt(a0), w)

=

0

or

,

the

partial

gradient

f w

(vt(a0

),

wt(a0))

does

not

exist.

Otherwise,

f w

(vt(a),

wt

(a))

is uniformly bounded for all a  [0, 1]/{a0}, which makes it integrable over

the interval [0, 1].

21

Under review as a conference paper at ICLR 2019

Then for some constants L and A depending on Cv and cw, we have

f (vt+1, wt+1) = f (vt + (vt+1 - vt), wt + (wt+1 - wt))

= f (vt, wt) + 1 f (vt(a), wt(a)), vt+1 - vt da 0 v
+ 1 f (vt(a), wt(a)), wt+1/2 - wt da 0 w
= f (vt, wt) + f (vt, wt), vt+1 - vt + f (vt, wt), wt+1/2 - wt v w

+ 1 f (vt(a), wt(a)) - f (vt, wt), vt+1 - vt da

0 v

v

+ 1 f (vt(a), wt(a)) - f (vt, wt), wt+1/2 - wt

0 w

w

 f (vt, wt) -  - L2 2

f (vt, wt) 2 v

da

-

f w

(vt

,

wt),

EZ

grelu(vt, wt; Z)

L2 +
2

EZ grelu(vt, wt; Z)

2

 f (vt, wt) -  - (1 + A) L2 2

f (vt, wt) 2 v

-  - AL2 2

f w

(v

t

,

wt

),

EZ

grelu(vt, wt; Z)

.

(26)

The third equality is due to the fundamental theorem of calculus. In the first inequality, we called

Lemma 3 for (vt, wt) and (vt(a), wt(a)) with a  [0, 1]/{a0}. In the last inequality, we used

Lemma

5.

So

when



<

2 (1+A)L

,

we

have

f (vt+1, wt+1)



f (vt, wt).

Summing up the inequality (26) over t from 0 to  and using f  0, we have

 L  1 - (1 + A)
2
t=0
 f (v0, w0) < .

f (vt, wt)

2
+

1 - AL

v 2

f w

(vt,

wt),

EZ

grelu(vt, wt; Z)

Hence,

lim f (vt, wt) = 0 t v

and

lim
t

f w

(vt,

wt),

EZ

grelu(vt, wt; Z)

Invoking Lemma 5 again, we further have

= 0.

lim
t

EZ grelu(vt, wt; Z)

= 0.

Invoking Lemma 6, we have that coarse gradient descent with ReLU µ(x) only converges to a saddle point or a minimizer.

The second claim follows from Lemmas 7 and 8.

22

