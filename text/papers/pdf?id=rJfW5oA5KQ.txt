Under review as a conference paper at ICLR 2019

APPROXIMABILITY OF DISCRIMINATORS IMPLIES GENERALIZATION IN GANS
Anonymous authors Paper under double-blind review

ABSTRACT
While Generative Adversarial Networks (GANs) have empirically produced impressive results on learning complex real-world distributions, recent works have shown that they suffer from lack of diversity or mode collapse. The theoretical work of Arora et al. (2017a) suggests a dilemma about GANs' statistical properties: powerful discriminators cause overfitting, whereas weak discriminators cannot detect mode collapse.
By contrast, we show in this paper that GANs can in principle learn distributions in Wasserstein distance (or KL-divergence in many cases) with polynomial sample complexity, if the discriminator class has strong distinguishing power against the particular generator class (instead of against all possible generators). For various generator classes such as mixture of Gaussians, exponential families, and invertible and injective neural networks generators, we design corresponding discriminators (which are often neural nets of specific architectures) such that the Integral Probability Metric (IPM) induced by the discriminators can provably approximate the Wasserstein distance and/or KL-divergence. This implies that if the training is successful, then the learned distribution is close to the true distribution in Wasserstein distance or KL divergence, and thus cannot drop modes. Our preliminary experiments show that on synthetic datasets the test IPM is well correlated with KL divergence or the Wasserstein distance, indicating that the lack of diversity in GANs may be caused by the sub-optimality in optimization instead of statistical inefficiency.

1 INTRODUCTION

In the past few years, we have witnessed great empirical success of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) in generating high-quality samples in many domains. Various ideas have been proposed to further improve the quality of the learned distributions and the stability of the training. (See e.g., (Arjovsky et al., 2017; Odena et al., 2016; Huang et al., 2017; Radford et al., 2016; Tolstikhin et al., 2017; Salimans et al., 2016; Jiwoong Im et al., 2016; Durugkar et al., 2016; Xu et al., 2017) and the reference therein.)
However, understanding of GANs is still in its infancy. Do GANs actually learn the target distribution? Recent works (Arora et al., 2017a;b; Dumoulin et al., 2016) hasve both theoretically and empirically brought the concern to light that distributions learned by GANs suffer from mode collapse or lack of diversity -- the learned distribution tend to miss a significant amount of modes of the target distribution. The main message of this paper is that the mode collapse can be in principle alleviated by designing proper discriminators with strong distinguishing power against specific families of generators (such as special subclasses of neural network generators.)

1.1 BACKGROUND ON MODE COLLAPSE IN GANS

We mostly focus on the Wasserstein GAN (WGAN) formulation (Arjovsky et al., 2017) in this paper. Define the F-Integral Probability Metric (F-IPM) (Mu¨ller, 1997) between distributions p, q as

WF (p, q) := sup EXp[f (X)] - EXq[f (X)] .
f F

(1)

1

Under review as a conference paper at ICLR 2019

Given samples from distribution p, WGAN sets up a family of generators G, a family of discriminators F, and aims to learn the data distribution p by solving

min WF (p^n, q^m)
qG

(2)

where p^n denotes "the empirical version of the distribution p", meaning the uniform distribution over a set of n i.i.d samples from p (and similarly q^m.)

When F = {all 1-Lipschitz functions}, IPM reduces to the Wasserstein-1 distance W1. In practice, parametric families of functions F such as multi-layer neural networks are used for approximating
Lipschitz functions, so that we can empirically optimize this objective eq. (2) via gradient-based algorithms as long as distributions in the family G have parameterized samplers. (See Section 2 for
more details.)

One of the main theoretical and empirical concerns with GANs is the issue of "modecollapse"(Arora et al., 2017a; Salimans et al., 2016) -- the learned distribution q tends to generate high-quality but low-diversity examples. Mathematically, the problem apparently arises from the fact that IPM is weaker than W1, and the mode-dropped distribution can fool the former (Arora et al., 2017a): for a typical distribution p, there exists a distribution q such that simultaneously the followings happen:

WF (p, q)  and W1(p, q) 1.

(3)

where , hide constant factors. In fact, setting q = p^N with N = R(F )/2, where R(F ) is a complexity measure of F (such as Rademacher complexity), q satisfies eq. (3) but is clearly a mode-dropped version of p when p has an exponential number of modes.

Reasoning that the problem is with the strength of the discriminator, a natural solution is to increase

it to larger families such as all 1-Lipschitz functions. However, Arora et al. (Arora et al., 2017a)

points out that Wasserstein-1 distance doesn't have good generalization properties: the empirical

Wasserstein distance used in the optimization is very far from the population distance. Even for

a spherical Gaussian distribution distribution q is exactly equal to

p = N(0, p, letting

qd1^mId×and)d

(or p^n

many other typical distributions), when the be two empirical versions of q and p with

m, n = poly(d), we have with high probability,

W1(p^n, q^m) 1 even though W1(p, q) = 0.

(4)

Therefore even when learning succeeds (p = q), it cannot be gleaned from the empirical version of W1.
The observations above pose a dilemma in establishing the statistical properties of GANs: powerful discriminators cause overfitting, whereas weak discriminators result in diversity issues because IPM doesn't approximate the Wasserstein distance. The lack of diversity has also been observed empirically by (Srivastava et al., 2017; Di & Yu, 2017; Borji, 2018; Arora et al., 2017b).

1.2 AN APPROACH TO DIVERSITY: DISCRIMINATOR FAMILIES WITH RESTRICTED
APPROXIMABILITY

This paper proposes a resolution to the conundrum by designing a discriminator class F that is
particularly strong against a specific generator class G. We say that a discriminator class F (and its
IPM WF ) has restricted approximability w.r.t. a generator class G, if F can distinguish any pairs of distributions p, q  G approximately as well as all 1-Lipschitz functions can do:

WF has restricted approximability w.r.t. G p, q  G, L(W1(p, q)) WF (p, q) U (W1(p, q)),

(5)

where L(·) and U (·) are two monotone nonnegative functions with L(0) = U (0) = 0. The paper mostly focuses on L(t) = t with 1    2 and U (t) = t, although we use the term "restricted approximability" more generally for this type of result (without tying it to a concrete definition of ). In other words, we are looking for discriminators F so that F-IPM can approximate the Wasserstein distance W1 for pairs of distributions p, q  G.
A discriminator class F with restricted approximability resolves the dilemma in the following way.

2

Under review as a conference paper at ICLR 2019
First, F avoids mode collapse ­ if the IPM between p and q is small, then by the left hand side of eq. (5), p and q are also close in Wasserstein distance and therefore significant mode-dropping cannot happen. 1
Second, we can pass from population-level guarantees to empirical-level guarantees ­ as shown in Arora et al. (2017a), classical capacity bounds such as the Rademacher complexity of F relate WF (p, q) to WF (p^n, q^m). Therefore, as long as the capacity is bounded, we can expand on eq. (5) to get a full picture of the statistical properties of Wasserstein GANs:
p, q  G, L(W1(p, q)) WF (p, q)  WF (p^n, q^m) U (W1(p, q)). Here the first inequality addresses the diversity property of the distance WF , and the second approximation addresses the generalization of the distance, and the third inequality provides the reverse guarantee that if the training fails to find a solution with small IPM, then indeed p and q are far away in Wasserstein distance.2 To the best of our knowledge, this is the first theoretical framework that tackles the statistical theory of GANs with polynomial samples.
The main body of the paper will develop techniques for designing discriminator class F with restricted approximability for several examples of generator classes including simple classes like mixtures of Gaussians, exponential families, and more complicated classes like distributions generated by invertible neural networks. In the next subsection, we will show that properly chosen F provides diversity guarantees such as inequalities eq. (5).
1.3 DESIGN OF DISCRIMINATORS WITH RESTRICTED APPROXIMABILITY
We start with relatively simple families of distributions G such as Gaussian distributions and exponential families, where we can directly design F to distinguish pairs of distribution in G. As we show in Section 3, for Gaussians it suffices to use one-layer neural networks with ReLU activations as discriminators, and for exponential families to use linear combinations of the sufficient statistics.
In Section 4, we study the family of distributions generated by invertible neural networks. We show that a special type of neural network discriminators with one additional layer than the generator has restricted approximability3. We show this discriminator class guarantees that W1(p, q)2 WF (p, q) W1(p, q) where here we hide polynomial dependencies on relevant parameters (Theorem 4.2). We remark that such networks can also produce an exponentially large number of modes due to the non-linearities, and our results imply that if WF (p, q) is small, then most of these exponential modes will show up in the learned distribution q.
One limitation of the invertibility assumption is that it only produces distributions supported on the entire space. The distribution of natural images is often believed to reside approximately on a low-dimensional manifold. When the distribution p have a Lebesgue measure-zero support, the KL-divergence (or the reverse KL-divergence) is infinity unless the support of the estimated distribution coincides with the support of p.4 Therefore, the KL-divergence is fundamentally not the proper measurement of the statistical distance for the cases where both p and q have low-dimensional supports.
The crux of the technical part of the paper is to establish the approximation of Waserstein distance by IPMs for generators with low-dimensional supports. We will show that a variant of an IPM can still be sandwiched by Wasserstein distance as in form of eq. (5) without relating to KL-divergence (Theorem 4.5). This demonstrates the advantage of GANs over MLE approach on learning distributions with low-dimensional supports. As the main proof technique, we develop tools for approximating the log-density of a smoothed neural network generator.
We demonstrate in synthetic and controlled experiments that the IPM correlates with the Wasserstein distance for low-dimensional dimensions with measure-zero support and correlates with KLdivergence for the invertible generator family (where computation of KL is feasible) (Section 5,
1Informally, if most of the modes of p are -far away from each other, then as long as W1(p, q) , q has to contain most of the modes of p.
2We also note that the third inequality can hold for all p, q as long as F is a subset of Lipschitz functions. 3This is consistent with the empirical finding that generators and discriminators with similar depths are often near-optimal choices of architectures. 4The formal mathematical statement is that Dkl(p q) is infinity unless p is absolutely continuous with respect to q.
3

Under review as a conference paper at ICLR 2019
details deferred into Appendix F and G.) The theory suggests the possibility that when the KLdivergence or Wasserstein distance is not measurable in more complicated settings, the test IPM could serve as a candidate alternative for measuring the diversity and quality of the learned distribution. We also remark that on real datasets, often the optimizer is tuned to carefully balance the learning of generators and discriminators, and therefore the reported training loss is often not the test IPM (which requires optimizing the discriminator until optimality.) Anecdotally, the distributions learned by GANs can often be distinguished by a well-trained discriminator from the data distribution, which suggests that the IPM is not well-optimized (See (Lopez-Paz & Oquab, 2016) for analysis of for the original GANs formulation.) We conjecture that the lack of diversity in real experiments may be caused by sub-optimality of the optimization, rather than statistical inefficiency.
1.4 RELATED WORK
Various empirical proxy tests for diversity, memorization, and generalization have been developed, such as interpolation between images (Radford et al., 2016), semantic combination of images via arithmetic in latent space (Bojanowski et al., 2017), classification tests (Santurkar et al., 2017), etc. These results by and large indicate that while "memorization" is not an issue with most GANs, lack of diversity frequently is.
As discussed thoroughly in the introduction, Arora et al. (2017a;b) formalized the potential theoretical sources of mode collapse from a weak discriminator, and proposed a "birthday paradox" that convincingly demonstrates this phenomenon is real. Many architectures and algorithms have been proposed to remedy or ameliorate mode collapse ((Dumoulin et al., 2016; Srivastava et al., 2017; Di & Yu, 2017; Borji, 2018; Lin et al., 2017)) with varying success. Feizi et al. (2017) showed provable guarantees of training GANs with quadratic discriminators when the generators are Gaussians. However, to the best of our knowledge, there are no provable solutions to this problem in more substantial generality.
The inspiring work of Zhang et al. (Zhang et al., 2017) shows that the IPM is a proper metric (instead of a pseudo-metric) under a mild regularity condition. Moreover, it provides a KL-divergence bound with finite samples when the densities of the true and estimated distribution exist. Our Section 4.1 can be seen as an extension of (Zhang et al., 2017, Proposition 2.9 and Corollary 3.5). The strength in our work is that we develop statistical guarantees in Wasserstein distance for distributions such as injective neural network generators, where the data distribution resides on a low-dimensional manifold and thus does not have proper density.
Liang (2017) considers GANs in a non-parametric setup, one of the messages being that the sample complexity for learning GANs improves with the smoothness of the generator family. However, the rate they derive is non-parametric ­ exponential in the dimension ­ unless the Fourier spectrum of the target family decays extremely fast, which can potentially be unrealistic in practical instances.
The invertible generator structure was used in Flow-GAN (Grover et al., 2018), which observes that GAN training blows up the KL on real dataset. Our theoretical result and experiments show that successful GAN training (in terms of the IPM) does imply learning in KL-divergence when the data distribution can be generated by an invertible neural net. This suggests, along with the message in (Grover et al., 2018), that the real data cannot be generated by an invertible neural network. In addition, our theory implies that if the data can be generated by an injective neural network (Section 4.2), we can bound the closeness between the learned distribution and the true distribution in Wasserstein distance (even though in this case, the KL divergence is no longer an informative measure for closeness.)
2 PRELIMINARIES AND NOTATION
The notion of IPM (recall the definition in eq. (1)) includes a number of statistical distances such as TV (total variation) and Wasserstein-1 distance by taking F to be 1-bounded and 1-Lipschitz functions respectively. When F is a class of neural networks, we refer to the F-IPM as the neural net IPM.5
5This was defined as neural net distance in (Arora et al., 2017a).
4

Under review as a conference paper at ICLR 2019

There are many distances of interest between distributions that are not IPMs, two of which we will particularly focus on: the KL divergence Dkl(p q) = Ep[log p(X) - log q(X)] (when the densities exist), and the Wasserstein-2 distance, defined as W2(p, q)2 = inf E(X,Y )[ X - Y 2] where
 be the set of couplings of (p, q). We will only consider distributions with finite second moments,
so that W1 and W2 exist.

For any distribution p, we let p^n be the empirical distribution of n i.i.d. samples from

p. The Rademacher complexity of a function class F on a distribution p is Rn(F, p) =

E

supf F

|

1 n

n i=1

if

(Xi

)|

where Xi  p i.i.d.

and i  {±1} are independent.

We define

Rn(F , G) = suppG Rn(F , p) to be the largest Rademacher complexity over p  G. The training

IPM loss (over the entire dataset) for the Wasserstein GAN, assuming discriminator reaches opti-

mality, is Eq^n [WF (p^n, q^n)]6. Generalization of the IPM is governed by the quantity Rn(F , G), as

stated in the following result (see Appendix A.1 for the proof):

Theorem 2.1 (Generalization, c.f. (Arora et al., 2017a)). For any p  G, we have that

q  G, Ep^n |WF (p, q) - Eq^n [WF (p^n, q^n)] |  4Rn(F , G).

Miscellaneous notation. We let N(µ, ) denote a (multivariate) Gaussian distribution with mean µ and covariance . For quantities a, b > 0 a b denotes that a  Cb for a universal constant C > 0
unless otherwise stated explicitly.

3 RESTRICTED APPROXIMABILITY FOR BASIC DISTRIBUTIONS

3.1 GAUSSIAN DISTRIBUTIONS

As a warm-up, we design discriminators with restricted approximability for relatively simple parameterized distributions such Gaussian distributions, exponential families, and mixtures of Gaussians. We first prove that one-layer neural networks with ReLU activation are strong enough to distinguish Gaussian distributions with the restricted approximability guarantees.
We consider the set of Gaussian distributions with bounded mean and well-conditioned covariance G = p = N(µ, ) : µ 2  D, m2 inId  m2 axId . Here D, min and max are considered as given hyper-parameters. We will show that the IPM WF induced by the following discriminators has restricted approximability w.r.t. G:

F := x  ReLU(v x + b) : v 2  1, |b|  D ,

(6)

Theorem 3.1. The set of one-layer neural networks (F defined in eq. (6)) has restricted approximability w.r.t. the Gaussian distributions in G in the sense that for any p, q  G

 · W1(p, q) WF (p, q)  W1(p, q).



with



=

1 d

.min
max

Moreover,

F

satisfies

Rademacher

complexity

bound

Rn(F , G)

D+max n

d.

 Apart from absolute constants, the lower and upper bound differ by a factor of 1/ d.7 We point out that the 1/ d factor is not improvable unless using functions more sophisticated than Lipschitz
functions of one-dimensional projections of x. Indeed, WF (p, q) is upper bounded by the maximum Wasserstein distance between one-dimensional projections of p, q, which is on the order of W1(p, q)/ d when p, q have spherical covariances. The proof is deferred to Section B.1.

Extension to mixture of Gaussians. Discriminator family F with restricted approximability can also be designed for mixture of Gaussians. We defer this result and the proof to Appendix C.

6In the ideal case we can take the expectation over q, as the generator q is able to generate infinitely many samples.
7As shown in (Feizi et al., 2017), the optimal discriminator for Gaussian distributions are quadratic functions.

5

Under review as a conference paper at ICLR 2019

3.2 EXPONENTIAL FAMILIES

Now we consider exponential families and show that the linear combinations of the sufficient statis-

tics are a family of discriminators with restricted approximability. Concretely, let G = {p :  





Rk} be an exponential family, where p(x)

=

1 Z ()

exp(

, T (x)

),

x



X



Rd:

here

T : Rd  Rk is the vector of sufficient statistics, and Z() is the partition function. Let the discrim-

inator family be all linear functionals over the features T (x): F = {x  v, T (x) : v 2  1}.

Theorem 3.2. Let G be the exponential family and F be the discriminators defined above. Assume that the log partition function log Z() satisfies that I 2 log Z() I. Then we have for any

p, q  G,

 

Dkl(p

q)  WF (p, q) 

 


Dkl(p q).

(7)

If we further assume X has diameter D and T (x) is L-Lipschitz in X . Then,

D 

W1(p,

q)

WF (p, q)  L · W1(p, q)

(8)

Moreover, F has Rademacher complexity bound Rn(F , G) 

.sup Ep [

T (X)

2 2

]

n

We note that the log partition function log Z() is always convex, and therefore our assumptions only require in addition that the curvature (i.e. the Fisher information matrix) has a strictly positive lower bound and a global upper bound. For the bound eq. (8), some geometric assumptions on the sufficient statistics are necessary because the Wasserstein distance intrinsically depends on the underlying geometry of x, which are not specified in exponential families by default. The proof of eq. (7) follows straightforwardly from the standard theory of exponential families. The proof of eq. (8) requires machinery that we will develop in Section 4 and is therefore deferred to Section B.2.

4 RESTRICTED APPROXIMABILITY FOR NEURAL NET GENERATORS

In this section, we design discriminators with restricted approximability for neural net generators, a family of distributions that are widely used in GANs to model real data.
In Section 4.1 we consider the invertible neural networks generators which have proper densities. In Section 4.2, we extend the results to the more general and challenging setting of injective neural networks generators, where the latent variables are allowed to have lower dimension than the observable dimensions (Theorem 4.5) and the distributions no longer have densities.

4.1 INVERTIBLE NEURAL NETWORK GENERATORS

In this section, we consider the generators that are parameterized by invertible neural networks8. Concretely, let G be a family of neural networks G = {G :   }. Let p be the distribution of

X = G(Z), Z  N(0, diag(2)).

(9)

where G is a neural network with parameters  and   Rd standard deviation of hidden factors. By allowing the variances to be non-spherical, we allow each hidden dimension to have a different
impact on the output distribution. In particular, the case  = [1k, 1d-k] for some  1 has the ability to model data around a "k-dimensional manifold" with some noise on the level of .

We are interested in the set of invertible neural networks G. We let our family G consist of standard -layer feedforward nets x = G(z) of the form

x = W (W -1(· · · (W1z + b1) · · · ) + b -1) + b ,

where Wi  Rd×d are invertible, bi  Rd, and  : R  R is the activation function, on which we make the following assumption:

8Our techniques also applies to other parameterized invertible generators but for simplicity we only focus on neural networks.

6

Under review as a conference paper at ICLR 2019

Assumption 1 (Invertible generators). Let RW , Rb, ,  > 0,   (0, 1] be parameters which are considered as constants (that may depend on the dimension). We consider neural networks G that are parameterized by parameters  = (Wi, bi)i[ ] belonging to the set

 = (Wi, bi)i[ ] : max Wi op , Wi-1 op  RW , bi 2  Rb, i  [ ] .

The activation |(-1) /(-1)

function  |  . The

is twice-differentiable with (0) = standard deviation of the hidden factors

0,  satisfy

(t) i 

 [,

[- 1, 1].

1],

and

Clearly, such a neural net is invertible, and its inverse is also a feedforward neural net with activation -1. We note that a smoothed version of Leaky ReLU (Xu et al., 2015) satisfies all the conditions on the activation functions. Further, some assumptions on the neural networks are necessary because arbitrary neural networks are likely to be able to implement pseudo-random functions which can't be distinguished from random functions by even any polynomial time algorithms.
Lemma 4.1. For any   , the function log p can be computed by a neural network with at most + 1 layers, O( d2) parameters, and activation function among {-1, log -1 , (·)2} of the form

1 f(x) = 2

h1, diag(-2)h1

+

1d, log -1 (hj) + C,

k=2

(10)

where h = W (x - b ), hk = Wk(-1(hk+1) - bk) for k  { - 1, . . . , 1}, and the parameter  = ((Wj, bj)j=1, C) satisfies    = { : Wj op  RW , bj 2  Rb, |C|  ( - 1)d log RW }. As a direct consequence, the following family F of neural networks with activation functions above
of at most + 2 layers contains all the functions {log p - log q : p, q  G} :

F = {f1 - f2 : 1, 2  }.

(11)

We note that the exact form of the parameterized family F is likely not very important in practice, since other family of neural nets also possibly contain good approximations of log p - log q (which
can be seen partly from experiments in Section G.)

The proof builds on the change-of-variable formula log p(x) = log (G-1(x)) +

log

| det



G-1 (x) x

|

(where



is

the

density

of

Z



N(0,

diag(2)))

and

the

observation

that

G- 1

is

a feedforward neural net with layers. Note that the log-det of the Jacobian involves computing the

determinant of the (inverse) weight matrices. A priori such computation is non-trivial for a given

G. However, it's just some constant that does not depend on the input, therefore it can be repre-

sentable by adding a bias on the final output layer. This frees us from further structural assumptions

on the weight matrices (in contrast to the architectures in flow-GANs (Gulrajani et al., 2017)). We

defer the proof of Lemma 4.1 to Section D.2.

Theorem 4.2. Suppose G = {p :   } is the set of invertible-generator distributions as defined

in eq. (9) satisfying Assumption 1. Then, the discriminator class F defined in Lemma 4.1 has

restricted approximability w.r.t. G in the sense that for any p, q  G,



W1(p, q)2 Dkl(p q) + Dkl(q p)  WF (p, q)

d 2

(W1(p,

q)

+

d

exp(-10d))

,

When n max d, -8 log 1/ , we have the generalization bound Rn(F , G)  gen :=

d4 log 4n

n

.

The proof of Theorem 4.2 uses the following lemma that relates the KL divergence to the IPM when the log densities exist and belong to the family of discriminators.
Lemma 4.3 (Special case of (Zhang et al., 2017, Proposition 2.9)). Let  > 0. Suppose F satisfies that for every q  G, there exists f  F such that f - (log p - log q)   , and that all the functions in F are L-Lipschitz. Then,

Dkl(p q) + Dkl(q p) -   WF (p, q)  L · W1(p, q).

(12)

We outline a proof sketch of Theorem 4.2 below and defer the full proof to Appendix D.3. As we choose the discriminator class as in Lemma 4.1 which implements log p - log q for any p, q  G,

7

Under review as a conference paper at ICLR 2019

by Lemma 4.3, WF (p, q) is lower bounded by Dkl(p q) + Dkl(q p). It thus suffices to (1) lower bound this quantity by the Wasserstein distance and (2) upper bound WF (p, q) by the Wasserstein distance.

To establish (1), we will prove in Lemma D.3 that for any p, q  G,
W1(p, q)2  W2(p, q)2 Dkl(p q) + Dkl(q p).
Such a result is the simple implication of transportation inequalities by Bobkov-Go¨tze and Gozlan (Theorem D.1), which state that if X  p (or q) and f is 1-Lipschitz implies that f (X) is subGaussian, then the inequality above holds. In our invertible generator case, we have X = G(Z) where Z are independent Gaussians, so as long as G is suitably Lipschitz, f (X) = f (G(Z)) is a sub-Gaussian random variable by the standard Gaussian concentration result (Vershynin, 2010).

The upper bound (2) would have been immediate if functions in F are Lipschitz globally in the whole space. While this is not strictly true, we give two workarounds ­ by either doing a truncation argument to get a W1 bound with some tail probability, or a W2 bound which only requires the Lipschitz constant to grow at most linearly in x 2. This is done in Theorem D.2 as a straightforward extension of the result in (Polyanskiy & Wu, 2016).

Combining the restricted approximability and the generalization bound, we immediately obtain that if the training succeeds with small expected IPM (over the randomness of the learned distributions), then the estimated distribution q is close to the true distribution p in Wasserstein distance.

Corollary 4.4. In the setting of Theorem 4.2, with high probability over the choice of training data p^n, we have that if the training process returns a distribution q  G such that Eq^n [WF (p^n, q^n)] 

train, then with gen :=

d4 log 4n

n

,

we

have

W1(p, q)  W2(p, q) (train + gen)1/2.

(13)

We note that the training error is measured by Eq^m [WF (p^n, q^m)], the expected IPM over the randomness of the learned distributions, which is a measurable value because one can draw fresh samples from q to estimate the expectation. It's an important open question to design efficient algorithms to achieve a small training error according to this definition, and this is left for future work.

4.2 INJECTIVE NEURAL NETWORK GENERATORS

In this section we consider injective neural network generators (defined below) which generate distributions residing on a low dimensional manifold. This is a more realistic setting than Section 4.1 for modeling real images, but technically more challenging because the KL divergence becomes infinity, rendering Lemma 4.3 useless. Nevertheless, we design a novel divergence between two distributions that is sandwiched by Wasserstein distance and can be optimized as IPM.

Concretely, we consider a family of neural net generators G = G : Rk  Rd where k < d and G is injective function. 9 Therefore, G is invertible only on the image of G, which is a kdimensional manifold in Rd. Let G be the corresponding family of distributions generated by neural nets in G.

Our key idea is to design a variant of the IPM, which provably approximates the Wasserstein distance. Let p denote the convolution of the distribution p with a Gaussian distribution N(0, 2I).
We define a smoothed F-IPM between p, q as

d~F (p, q) inf (WF (p, q) +  log 1/)1/2,
0

(14)

Clearly d~F can be optimized as WF with an additional variable  introduced in the optimization. We show that for certain discriminator class (see Section E for the details of the construction) such that d~F approximates the Wasserstein distance.
Theorem 4.5 (Informal version of Theorem E.1). Let G be defined as above. The exists a discriminator class F such that for any pair of distributions p, q  G, we have

W1(p, q) d~F (p, q) poly(d) · W1(p, q)1/6 + exp(-(d)).

(15)

9In other words, G(x) = G(y) if x = y.

8

Under review as a conference paper at ICLR 2019
Furthermore, when n poly(d), we have the generalization bound
log n Rn(F , G) poly(d) n Here poly(d) hides polynomial dependencies on d and several other parameters that will be defined in the formal version (Theorem E.1.)
The direct implication of the theorem is that if d~(p^n, q^n) is small for n poly(d), then W (p, q) is guaranteed to be also small and thus we don't have mode collapse.
5 SIMULATION
We perform two sets of synthetic experiments to confirm they are consistent with our theory. We briefly describe them here, and details are deferred to Appendix F and G:
(a) We learn the unit circle and the Swiss roll curve in 2D with neural net generators and neural net discriminators (Appendix F). Results show that the IPM is well-correlated with the Wasserstein distance and visual sample quality. We note that such test can only be done in low-dimension because in high-dimension Wasserstein distance between two distributions is not efficiently computable given generators or densities for the two distributions.
(b) We learn invertible neural net generators with discriminators of restricted approximability and vanilla architectures (Appendix G). We show that the IPM is well-correlated with the KL divergence, both along training and when we consider two generators that are perturbations of each other (the purpose of the latter being to eliminate any effects of the optimization).
6 CONCLUSION
We present the first polynomial-in-dimension sample complexity bounds for learning various distributions (such as Gaussians, exponential families, invertible neural networks generators) using GANs with convergence guarantees in Wasserstein distance (for distributions with low-dimensional supports) or KL divergence. The analysis technique proceeds via designing discriminators with restricted approximability ­ a class of discriminators tailored to the generator class in consideration which have good generalization and mode collapse avoidance properties. We hope our techniques can be in future extended to other families of distributions with tighter sample complexity bounds. This would entail designing discriminators that have better restricted approximability bounds, and generally exploring and generalizing approximation theory results in the context of GANs. We hope such explorations will prove as rich and satisfying as they have been in the vanilla functional approximation settings.
REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In International Conference on Machine Learning, pp. 224­ 232, 2017a.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans actually learn the distribution? do gans learn the distribution? some theory and empirics. ICLR, 2017b.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space of generative networks. arXiv preprint arXiv:1707.05776, 2017.
Ali Borji. Pros and cons of gan evaluation measures. arXiv preprint arXiv:1802.03446, 2018.
James Demmel, Ioana Dumitriu, and Olga Holtz. Fast linear algebra is stable. Numerische Mathematik, 108(1):59­91, 2007.
9

Under review as a conference paper at ICLR 2019
Xinhan Di and Pengqian Yu. Max-boost-gan: Max operation to boost generative ability of generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1156­1164, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
I. Durugkar, I. Gemp, and S. Mahadevan. Generative Multi-Adversarial Networks. ArXiv e-prints, November 2016.
Soheil Feizi, Changho Suh, Fei Xia, and David Tse. Understanding gans: the lqg setting. arXiv preprint arXiv:1710.10793, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan: Combining maximum likelihood and adversarial learning in generative models. In AAAI Conference on Artificial Intelligence, 2018.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5769­5779, 2017.
Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative adversarial networks. In Computer Vision and Patter Recognition, 2017.
D. Jiwoong Im, H. Ma, C. Dongjoo Kim, and G. Taylor. Generative Adversarial Parallelization. ArXiv e-prints, December 2016.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media, 2013.
Tengyuan Liang. How well can generative adversarial networks (gan) learn densities: A nonparametric view. arXiv preprint arXiv:1712.08244, 2017.
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in generative adversarial networks. arXiv preprint arXiv:1712.04086, 2017.
David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. arXiv preprint arXiv:1610.06545, 2016.
Valentina Masarotto, Victor M Panaretos, and Yoav Zemel. Procrustes metrics on covariance operators and optimal transportation of gaussian processes. arXiv preprint arXiv:1801.01990, 2018.
Alfred Mu¨ller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429­443, 1997.
Hoi Nguyen, Terence Tao, and Van Vu. Random matrices: tail bounds for gaps between eigenvalues. Probability Theory and Related Fields, 167(3-4):777­816, 2017.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Yury Polyanskiy and Yihong Wu. Wasserstein continuity of entropy and outer bounds for interference channels. IEEE Transactions on Information Theory, 62(7):3992­4002, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations, 2016.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by backpropagating errors. nature, 323(6088):533, 1986.
10

Under review as a conference paper at ICLR 2019

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, 2016.
Shibani Santurkar, Ludwig Schmidt, and Aleksander Madry. A classification-based perspective on gan distributions. arXiv preprint arXiv:1711.00970, 2017.
Bernhard A Schmitt. Perturbation bounds for matrix square roots and pythagorean sums. Linear algebra and its applications, 174:215­227, 1992.
Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Information Processing Systems, pp. 3310­3320, 2017.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26­ 31, 2012.
Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Scho¨lkopf. Adagan: Boosting generative models. arXiv preprint arXiv:1701.02386, 2017.
Ramon van Handel. Probability in high dimension. Technical report, PRINCETON UNIV NJ, 2014.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.
Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. To appear, 2018. URL https://www.stat.berkeley.edu/~wainwrig/nachdiplom/ Chap5_Sep10_2015.pdf.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of empirical measures in wasserstein distance. arXiv preprint arXiv:1707.00087, 2017.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. arXiv preprint, 2017.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discriminationgeneralization tradeoff in gans. arXiv preprint arXiv:1711.02771, 2017.

A PROOFS FOR SECTION 2

A.1 PROOF OF THEOREM 2.1

Fixing p^n, consider a random sample q^n. It is easy to verify that the F -distance satisfies the triangle inequality, so we have

|WF (p, q) - Eq^n [WF (p^n, q^n)]|  Eq^n [|WF (p, q) - WF (p^n, q^n)]  Eq^n [WF (p, p^n) + WF (q, q^n)] = WF (p, p^n) + Eq^n [WF (q, q^n)].
Taking expectation over p^n on the above bound yields

Ep^n [|WF (p, q) - Eq^n [WF (p^n, q^n)]|]  Ep^n [WF (p, p^n)] + Eq^n [WF (q, q^n)]. So it suffices to bound Ep^n [WF (p, p^n)] by 2Rn(F , G) and the same bound will hold for q. Let Xi be the samples in p^n. By symmetrization, we have

WF (p, p^n) = E

sup
f F

1 n

n

f (Xi) - Ep[f (X)]

i=1

 2E

sup
f F

1n n if (Xi)
i=1

= 2E[Rn(F , p)]  2Rn(F , G).

Adding up this bound and the same bound for q gives the desired result.

11

Under review as a conference paper at ICLR 2019

B PROOFS FOR SECTION 3

B.1 PROOF OF THEOREM 3.1

Recall that our discriminator family is F = x  (v x + b) : v 2  1, |b|  D .

Restricted approximability The upper bound WF (p1, p2)  W1(p1, p2) follows directly from the fact that functions in F are 1-Lipschitz.

We now establish the lower bound. First, we recover the mean distance, in which we use the fol-

lowing simple fact: a linear discriminator is the sum of two ReLU discriminators, or mathematically

t = (t) - (-t). Taking v =

µ1 -µ2 µ1 -µ2

2

,

we

have

µ1 - µ2 2 = v µ1 - v µ2 = Ep1 [v X] - Ep2 [v X] = Ep1 [(v X)] - Ep2 [(v X)] + -Ep1 [(-v X)] + Ep2 [(-v X)]  Ep1 [(v X)] - Ep2 [(v X)] + Ep1 [(-v X)] - Ep2 [(-v X)] .

Therefore at least one of the above two terms is greater than µ1 - µ2 2 /2, which shows that WF (p1, p2)  µ1 - µ2 2 /2.
For the covariance distance, we need to actually compute Ep[(v X + b)] for p = N(µ, ). Note that X =d 1/2Z + µ, where Z  N(0, Id). Further, we have v X =d 1/2v 2 W + v µ for W  N(0, 1), therefore

Ep[(v X + b)] = E  1/2v W + v µ + b
2

=

1/2v E 
2

v µ+b W+
1/2v 2

= 1/2v R
2

v µ+b .
1/2v 2

(Defining R(a) = E[max {W + a, 0}] for W  N(0, 1).) Therefore, the neuron distance between the two Gaussians is

WF (p1, p2) = sup
v 21,|b|D



11/2v

v R

µ1 + b  -

2 11/2v 2



12/2v

v R

µ2 + b 

2 21/2v 2

,

As a  max {a + w, 0} is strictly increasing for all w, the function R is strictly increasing. It is also a basic fact that R(0) = 1/ 2.

Consider any fixed v. By flipping the sign of v, we can let v µ1  v µ2 without changing

i1/2v

. Now, letting b = -v
2

(µ1 - µ2)/2 (note that |b|  D is a valid choice), we have

v

v µ1 + b =

(µ1 - µ2)  0, 2

v

µ2 + b = - v

(µ1 - µ2)  0. 2

As R is strictly increasing, for this choice of (v, b) we have



11/2v

v R

µ1 + b  -

2 11/2v 2

 R(0)

11/2v

-
2

12/2v 2



21/2v

v R

µ2 + b 

2 21/2v 2

= 1 2

11/2v

-
2

12/2v 2

.

Ranging over v 2  1 we then have WF (p1, p2)  1 sup 2 v 21

11/2v

-
2

21/2v 2 .

12

Under review as a conference paper at ICLR 2019

The quantity in the supremum can be further bounded as

11/2v

-
2

21/2v 2

=

|v (1 - 2)v| 

|v (1 - 2)v|

.

11/2v

+
2

21/2v 2

max(11/2) + max(21/2)

Choosing v = vmax(1 - 2) gives

WF (p1, p2)



1 2

sup
v 21

11/2v

-
2

12/2v 2 

1 - 2 op . 22max

Now, using the perturbation bound

11/2 - 12/2

1 · op min(1) + min(2)

1 1 - 2 op  2min

1 - 2 op ,

(cf. (Schmitt, 1992, Lemma 2.2)), we get

WF (p1, p2)



1 2 2max

· 2min

11/2 - 21/2

  min  op 2max d

11/2 - 21/2

.
Fr

Combining the above bound with the bound in the mean difference, we get

WF (p1, p2)



1 2

µ1 - µ2 2 +  min 2 2dmax

11/2 - 12/2 Fr

  min 2 2dmax

µ1 - µ2

2 2

+

U

inf
U =U U

=Id

11/2 - U 12/2

2 Fr

=  min

· W2(p1, p2)   min

· W1(p1, p2)

2 2dmax

2 2dmax

(16)

The last equality following directly from the closed-form expression of the W2 distance between two Gaussians (Masarotto et al., 2018, Proposition 3). Thus the claimed lower bound holds with c = 1/(2 2).

KL Bound We use the W2 distance to bridge the KL and the F-distance, which uses the machinery developed in Section D. Let p1, p2 be two Gaussians distributions with parameters i = (µi, i) 
. By the equality

Dkl(p1 p2)+Dkl(p2 p1) = (Ep1 [log p1(X)]-Ep2 [log p1(X)])+(Ep2 [log p2(X)]-Ep1 [log p2(X)]),

it suffices to upper bound the term only involving log p1(X) (the other follows similarly), which by Theorem D.2 requires bounding the growth of  log p1(x) 2. We have

 log p1(x) 2 = -1 1(x - µ1) 2  m-i2n x - µ1 2 .

Further Epi [

x - µ1

22]  tr(i) +

µi - µ1

2 2



dm2 ax

+

4D2

for

i

=

1,

2,

therefore

by

(a

trivial

variant of) Theorem D.2(c) we get

 Ep1 [log p1(X)] - Ep2 [log p1(X)]  m-i2n( dmax + 2D)W2(p1, p2).

The same bound holds for log p2. Adding them up and substituting the bound appendix B.1 gives

that

 

Dkl(p1 p2) + Dkl(p2 p1)

dmax + m2 in

2D

W2(p1,

p2)

dmax( dmax m3 in

+

D)

WF

(p1,

p2).

Generalization We wish to bound for all  = (µ, )  

Rn(F , p) = Ep

1n

sup
v 21,|b|D

n i(v
i=1

Xi + b)

.

13

Under review as a conference paper at ICLR 2019

As  : R  R is 1-Lipschitz, by the Rademacher contraction inequality (Ledoux & Talagrand, 2013), we have

1n

Ep

sup
v 21,|b|D

n i(v
i=1

Xi + b)

 2Ep

The right hand side can be bounded directly as

1n

sup
v 21,|b|D

n i(v
i=1

Xi + b)

.

1n

1n

1n

Ep

sup
v 21,|b|D

n i(v
i=1

Xi + b)

= Ep

sup (b + v
v 21,|b|D

µ) n

i + n

iv

i=1 i=1

(Xi - µ)



sup |b + v µ|E
v 21,|b|D

1n n i
i=1

+ Ep

sup
v 21

1n

v, n

i(Xi - µ)

i=1

 2DE

1n n i
i=1

+ Ep

1n n i(Xi - µ)
i=1 2

 1n

2

 1n

2

 2D E  n i  + Ep  n i(Xi - µ) 

i=1 i=1 2



=

2D + tr()  2D +max

d .

nn

B.2 PROOF OF THEOREM 3.2

KL bounds Recall the basic property of exponential family that A() = Ep [T (X)]. Suppose p = p1 and q = p2 . Then,

WF (p, q) =

sup
v 21

Ep1

[

v, T (X)

] - Ep2 [

v, T (X)

]

= sup v, A(1) - A(2) = A(1) - A(2) 2 .
v 21

By the assumption on 2A we have that

 1 - 2 2  WF (p1 , p2 )   1 - 2 2 Moreover, the exponential family also satisfies that

(17)

1
Dkl(p1 p2 ) = A(2) - A(1) - A(1), 2 - 1 =  2A(2 + t)dt
0

where  = 1 - 2. Using the assumption we have that  1 - 2 2   2A(2 + t) 



1 - 2

2

and

therefore

1 2



1 - 2

2  Dkl(p1

p2 ) 

1 2



1 - 2

2. Combining this with

eq. (17) we complete the proof.

Wasserstein bounds We show eq. (8). As diam(X ) = D, there exists x0  X such that x - x0  D for all x  X . Hence for any 1-Lipschitz function f : Rd  R we have that |f (X) - f (x0)|  x - x0 2  D. By the Hoeffding Lemma, f (X) is D2/4-sub-Gaussian. Applying Theorem D.1(a), we get that for any p, q  G,

W1(p, q) 

D2 2 Dkl(p q)

D 

·

WF (p, q).

Generalization For any    we compute the Rademacher complexity

Rn(F , p) = Ep

1n

sup
v 21

n i
i=1

v, T (Xi)

= Ep

1n n iT (Xi)
i=1 2



 1n

2

Ep  n iT (Xi)  =

Ep [ T (X) 22] . n

i=1 2

14

Under review as a conference paper at ICLR 2019

C RESULTS ON MIXTURE OF GAUSSIANS

We consider mixture of k identity-covariance Gaussians on Rd:

kk
p = wiN(µi, Id), wi  0, wi = 1.
i=1 i=1

We assume that    = { µi 2  D, wi  exp(-Bw) i  [k]}. We will use a one-hidden-layer neural network that implements (a slight modification of) log p:



k

 F = f1 - f2 : fi = log

wj(i) exp µ(ji) x + b(ji) : exp(-Bw)  wj(i)  1,

 j=1



µj(i)

2



D,

0



bj(i)



 -D2 .


Theorem C.1. The family F is suitable for learning mixture of k Gaussians. Namely, we have that

(1) (Restricted approximability) For any 1, 2  , we have

1 D2 +

1

·

W12(p1 ,

p2 )



WF (p1 ,

p2 )



2D

·

W1(p1 ,

p2 ).

(2) (Generalization) We have for some absolute constant C > 0 that

sup Rn(F , p)  C


k(log k + D2 + Bw)d log n . n

C.1 THE GAUSSIAN CONCENTRATION RESULT
The Gaussian concentration result (Vershynin, 2010, Proposition 5.34) will be used here and in later proofs, which we provide for convenience. Lemma C.2 (Gaussian concentration). Suppose X  N(0, Id) and f : Rd  R is L-Lipschitz, then f (X) is L2-sub-Gaussian.

C.2 PROOF OF THEOREM C.1

Restricted approximability For the upper bound, it suffices to show that each

k
f (x) = log wj exp µj x + bj
j=1

(18)

is D-Lipschitz. Indeed, we have

 log f (x) 2 =

k j=1

wj

exp(µj

x

+

bj )µj

k j=1

wj

exp(µj

x

+

bj )


2

k j=1

wj

exp(µj

x

+

bj )

µj

k j=1

wj

exp(µj

x

+

bj )

2

 D.

This further shows that every discriminator f1 - f2  F is at most 2D-Lipschitz, so by Theorem D.2(a) we get the upper bound.

We now establish the lower bound. As F implements the KL divergence, for any two p1, p2  P, we have
WF (p1, p2)  Dkl(p1 p2) + Dkl(p2 p1).
We consider regularity properties of the distributions p1, p2 in the Bobkov-Gotze sense (Theorem D.1(a)). Suppose p1 = wjN(µj, Id). For any 1-Lipschitz function f : Rd  R, we have
j
f (X) =d f (N(µj, Id)).
j=1

15

Under review as a conference paper at ICLR 2019

Letting Xj  N(µj, Id) be the mixture components. By the Gaussian concentration (Lemma C.2), each f (Xj) is 1-sub-Gaussian, so we have for any   R

kk

k

E[ef(X)] =

wj E[ef(Xj)] 

eE[f (Xj )]+2/2 = e2/2

wj eE[f (Xj )] .

j=1

j=1

j=1

I

Now, term I is precisely the MGF of a discrete random variable on Y  R which takes value E[f (Xj)] with probability wj. For Z  N(0, 1) we have

|E[f (Xj)] - E[f (Z)]| = |E[f (µj + Z)] - E[f (Z)]|  E[|f (µj + Z) - f (Z)|]  µj 2  D.
Therefore the values {E[f (Xj)]}j[k] lie in an interval of length at most 2D. By the Hoeffding's Lemma, Y is D2-sub-Gaussian, so we have I  exp(E[Y ] + D22/2), and so

E[ef(X)]  exp

2 2

+

E[Y

]

+

D22/2

= exp

2(D2 + 1)

E[Y ] +

2

.

Therefore f (X) is at most (D2+1)-sub-Gaussian, and thus X satisfies the Bobkov-Gozlan condition with 2 = D2 + 1. Applying Theorem D.1(a) we get

WF (p1, p2)  Dkl(p1

p2) + Dkl(p2

p1) 

1 D2 +

1

·

W (p1,

p2).

Generalization Reparametrize the one-hidden-layer neural net eq. (18) as

k

f(x) = log exp(µj x + bj + log wj).

j=1

cj

It then suffices to bound the Rademacher complexity of f for    = µj 2  D, cj  [-(D2 + Bw), 0] . Define the metric

(,  ) = max max
j[k]

µj - µj 2 , |cj - cj |

and the Rademacher process

1n

1n

k

Y = n if(Xi) = n i log exp(µj Xi + cj),

i=1 i=1 j=1

we show that Y is suitably Lipschitz in  (in the  metric) and use a one-step discretization bound. Indeed, we have

µj Y 2 =

1n n i
i=1

exp(µj Xi

k j=1

exp(µj

+ cj) Xi +

cj

)

Xi

1 n n
2 i=1

Xi 2

and

1n

|cj Y| = n

i

i=1

Therefore, for any  > 0 we have

exp(µj Xi + cj)

k j=1

exp(µj

Xi

+

cj )

 1.

 E sup |Y - Y |  Ck (E [ X1 2] + 1)   Ck(D + d)
(, )

(19)

for some constant C > 0.

We now bound the expected supremum of the max over a covering set. Let N (, , ) be a covering set of  under , and N (, , ) be the covering number. As  looks at each µi, cj separately, its covering number can be upper bounded by the product of each separate covering:

k
N (, , )  N (B2(D), · 2 , )·N ([-(D2+Bw), 0], |·|, )  exp

kd log 3D + k log 2(D2 + Bw) 

.

j=1

16

Under review as a conference paper at ICLR 2019

Now, for each invididual process Y is the i.i.d. average of random variables of the form

i log

k j=1

exp(µj

X +cj ).

The

log-sum-exp

part

is

D-Lipschitz

in

X,

so

we

can

reuse

the

analysis

done precedingly (in the Bobkov-Gotze part) to get that log

k j=1

exp(µj

X

+

cj )

is

D2(D2

+

1)-

sub-Gaussian. Further, its expectation is bounded as (for X  p = viN(i, Id))


k


k


k



EXp log exp(µj X + cj ) = viEXN(i,Id) log exp(µj X + cj )

j=1

i=1 j=1

kk

kk



vi log

EXN(i,Id)[exp(µj X + cj )] 

vi log

exp(µj i +

µj

2 2

/2

+

cj

)

i=1 j=1

i=1 j=1

 log k + (2D2 + Bw).

This shows that the term i log

k j=1

exp(µj

X

+

cj )

is

(log

k

+

D2

+

Bw )2

+

D2(D2

+

1)-sub-

Gaussian, and thus we have by sub-Gaussian maxima bounds that

E max |Y|  C
N (,,)

(log k + D2 + Bw)2 + D2(D2 + 1) · log N (, , ) n

 C log k + D2 + Bw · kd log D2 + Bw . n

(20)

By the 1-step discretization bound and combining eq. (19) and appendix C.2, we get

E sup |Y|  E

sup |Y - Y | + E max |Y|



, ,(, )

N (,,)

  Ck(D + d) + C

log k + D2 + Bw · kd log D2 + Bw .

n

Choosing  = c/n for sufficiently small c (depending on D2, Bw) gives that

E sup |Y|  C


kd(log k + D2 + Bw) log n n

D PROOFS FOR SECTION 4

D.1 BOUNDING KL BY WASSERSTEIN
The following theorem gives conditions on which the KL divergence can be lower bounded by the Wasserstein 1/2 distance. For a reference see Section 4.1 and 4.4 in van Handel (2014). Theorem D.1 (Lower bound KL by Wasserstein). Let p be any distribution on Rd and Xi iid p be the i.i.d. samples from p.
(a) (Bobkov-Gotze) If f (X1) is 2-sub-Gaussian for any 1-Lipschitz f : Rd  R, then W1(p, q)2  22Dkl(p q) for all q.
(b) (Gozlan) If f (X1, . . . , Xn) is 2-sub-Gaussian for any 1-Lipschitz f : (Rd)n  R, then W2(p, q)2  22Dkl(p q) for all q.
Theorem D.2 (Upper bounding f -contrast by Wasserstein). Let p, q be two distributions on Rd with positive densities and denote their probability measures by P, Q. Let f : Rd  R be a function.
(a) (W1 bound) Suppose f is L-Lipschitz, then Ep[f (X)] - Eq[f (X)]  L · W1(p, q).
(b) (Truncated W1 bound) Let D > 0 be any diameter of interest. Suppose for any p  {p, q} we have

17

Under review as a conference paper at ICLR 2019

(i) f is L(D)-Lipschitz in the ball of radius D; (ii) We have Ep[f 2(X)]  M ; (iii) We have P ( X  D)  ptail(D),
then we have
Ep[f (X)] - Eq[f (X)]  L(D) · W1(p, q) + 4 M ptail(D).

(c) (W2 bound) Suppose f (x)  c1 x + c2 for all x  Rd, then we have

Ep[f (X)] - Eq[f (X)] 

c1 2

Ep[

X

2] + c1 2

Eq[ X 2] + c2

· W2(p, q).

D.1.1 PROOF OF THEOREM D.2
Proof. (a) This follows from the dual formulation of W1. (b) We do a truncation argument. We have
Ep[f (X)] - Eq[f (X)] = Ep[f (X)1 { X  D}] - Eq[f (X)1 { X  D}]
I
+ Ep[f (X)1 { X > D}] - Eq[f (X)1 { X > D}] .
II
Term II has the followng bound by Cauchy-Schwarz:

II  Ep[f 2(X)] · P ( X > D) + Eq[f 2(X)] · Q( X > D)  2 M ptail(D).
We now deal with term I. By definition of the Wasserstein distance, there exists a coupling (X, Y )   such that X  P , Y  Q, and E[ X - Y ] = W1(p, q). On this coupling, we have
I = E[f (X)1 { X  D} - f (Y )1 { Y  D}] = E[(f (X) - f (Y ))1 { X  D, Y  D}] +E[f (X)1 { X  D, Y > D}] - E[f (Y )1 { Y  D, X > D}]
(i)
 L(D)E[ X - Y 1 { X  D, Y  D}] + E[|f (X)|1 { Y  D}] +E[|f (Y )|1 { X  D}]
(ii)
 L(D)E[ X - Y ] + E[f 2(X)] · ( Y  D) + E[f 2(Y )] · ( X  D)
= L(D) · W1(p, q) + Ep[f 2(X)] · Q( X  D) + Eq[f 2(X)] · P ( X  D)
 L(D) · W1(p, q) + 2 M ptail(D).
Above, inequality (i) used the Lipschitzness of f in the D-ball, and (ii) used Cauchy-Schwarz. Putting terms I and II together we get
Ep[f (X)] - Eq[f (X)]  L(D) · W1(p, q) + 4 M ptail(D).

(c) This part is a straightforward extension of (Polyanskiy & Wu, 2016, Proposition 1). For completeness we present the proof here. For any x, y  Rd we have

11

|f (x) - f (y)| =

f (tx + (1 - t)y), x - y dt 

f (tx + (1 - t)y) x - y dt

00

1

 (c1t x + c1(1 - t) y + c2) x - y dt = (c2 + c1 x /2 + c1 y /2) x - y .

0

18

Under review as a conference paper at ICLR 2019

By definition of the W2 distance, there exits a coupling (X, Y )   such that X  P , Y  Q, and E[ X - Y 2] = W22(p, q). On this coupling, taking expectation of the above bound, we get

E[|f (X) - f (Y )|]  c2E[ X - Y

]+

c1 2

(E [

X

X - Y ] + E[ Y

X - Y ])



c2

E[ X - Y

2] + c1 2

E[ X 2] · E[ X - Y 2] + E[ Y 2] · E[ X - Y 2]

=

c2

+

c1 2

Ep[

X

2] + c1 2

Eq[ X 2] · W2(p, q).

Finally, the triangle inequality gives

Ep[f (X)] - Eq[f (X)] = E[f (X) - f (Y )]  E[|f (X) - f (Y )|],

so the left hand side is also bounded by the preceding quantity.

D.2 PROOF OF LEMMA 4.1

It is straightforward to see that the inverse of x = G(z) can be computed as z = W1-1(-1(W2-1-1(· · · -1(W -1(x - b )) · · · ) - b2) - b1).
So G- 1 is also a -layer feedforward net with activation -1.

(21)

We now consider the problem of representing log p(x) by a neural network. Let  be the density of Z  N(0, diag(2)). Recall that the log density has the formula

p(x) = log 

(G- 1(x)

+ log det G-1(x) x

.

First consider the inverse network that implements G- 1. By eq. (21), this network has layers ( - 1 hidden layers), d2 + d parameters in each layer, and -1 as the activation function. Now, as log  has the form log (z) = a() - i zi2/(2i2), we can add one more layer on top of z with the square activation and the inner product with --2/2 to get this term.

Second, we show that by adding some branches upon this network, we can also compute the log determinant of the Jacobian. Define h = W -1(x - b ) and backward recursively hk-1 = Wk--11(-1(hk) - bk-1) (so that z = h1), we have

G- 1(x) x

=

W1-1diag(-1

(h2

))W2-1

·

·

·

W

-1 -1

diag(-1

(h

))W -1.

Taking the log determinant gives

log det G- 1(x) = C + x

1, log -1 (hk) .

k=2

As (h , . . . , h2) are exactly the (pre-activation) hidden layers of the inverse network, we can add one branch from each layer, pass it through the log -1 activation, and take the inner product with

1.

Finally, by adding up the output of the density branch and the log determinant branch, we get a neural network that computes log p(x) with no more than + 1 layers and O( d2) parameters, and
choice of activations within {-1, log -1 , (·)2}.

D.3 PROOF OF THEOREM 4.2

We state a similar restricted approximability bound here in terms of the W2 distance, which we also

prove.



W2(p, q)2 Dkl(p q) + Dkl(q p)  WF (p, q)

d 2 · W2(p, q).

The theorem follows by combining the following three lemmas, which we show in sequel.

19

Under review as a conference paper at ICLR 2019

Lemma D.3 (Lower bound). There exists a constant c = c(RW , Rb, ) > 0 such that for any 1, 2  , we have

WF (p1 , p2 )  c · W2(p1 , p2 )2  c · W1(p1 , p2 )2.

Lemma D.4 (Upper bound). There exists constants Ci = Ci(RW , Rb, ) > 0, i = 1, 2 such that for any 1, 2  , we have



(1)

(W1 bound) WF (p1 , p2 ) 

C1 d 2

·

W1(p1 , p2 ) +

d exp(-10d) .



(2)

(W2 bound) WF (p1 , p2 ) 

C2 d 2

· W2(p1 , p2 ).

Lemma D.5 (Generalization error). Consider n samples Xi iid p for some   . There exists a constant C = C(RW , Rb, ) > 0 such that when n  C max d, -8 log n , we have

Rn(F , p ) 

Cd4 log n 4n .

D.4 PROOF OF LEMMA D.3

We show that p satisfies the Gozlan condition for any    and apply Theorem D.1. Let Xi iid p for i  [n]. By definition, we can write
Xi = G(Zi), Zi iid N(0, diag(2)).
Let Zi = (zi/i)di=1 and G(z) = G((izi)id=1), then we have G(z) = G(z) and that Zi are i.i.d. standard Gaussian. Further, suppose G is L-Lipschitz, then for all z1, z2  Rd we have

G(z1) - G(z2) = |G(z1) - G(z2)|  L z1 - z2 2  L z1 - z2 2 ,

the last inequality following from i  1. Therefore G is also L-Lipschitz. Now, for any 1-Lipschitz f : (Rd)n  R, we have

f (G(z1), . . . , G(zn)) - f (G(z1), . . . , G(zn)) 

n

1/2 2

G(zi) - G(zi)

2

i=1

n 1/2

 L·

zi - zi

2 2

i=1

= L z1:n - z 1:n .
2

Therefore the mapping (z1, . . . , zn)  f (G(z1), . . . , G(zn)) is L-Lipschitz. Hence by Lemma C.2, the random variable

f (X1, . . . , Xn) = f (G(Z1), . . . , G(Zn))
is L2-sub-Gaussian, and thus the Gozlan condition is satisfied with 2 = L2. By definition of the network G we have

L

Wk op · -1  RW -1 = C(RW , ).

k=1

Now, for any 1, 2  , we can apply Theorem D.1(b) and get

Dkl(p1

p2 )



1 2C

2

W2

(p1

,

p2

)2

,

and the same holds with p1 and p2 swapped. As log p1 - log p2  F , by Lemma 4.3, we obtain

WF (p1 , p2 )  Dkl(p1

p2 ) + Dkl(p2

p1 )



1 C2

W2

(p1

,

p2

)2



1 C2

W1

(p1

,

p2

)2

,

The last bound following from the fact that W2  W1.

20

Under review as a conference paper at ICLR 2019

D.5 PROOF OF LEMMA D.4

We are going to upper bound WF by the Wasserstein distances through Theorem D.2. Fix 1, 2 

. By definition of F, it suffices to upper bound the Lipschitzness of log p(x) for all   . Recall

that

1 log p(x) = 2

h , diag(-2)h

-1
+

1d, log -1 (hk) +C(),

k=1

I

II

where h1, . . . , h (= z) are the hidden-layers of the inverse network z = G- 1(x), and C() is a constant that does not depend on x.

We first show the W2 bound. Clearly log p(x) is differentiable in x. As  each layer hk is C(RW , Rb, k)-Lipschitz in x, so term II is altogether d

  has norm bounds,

-1 k=1

C (RW

,

Rb,

k)

=

C(RW , Rb, ) d-Lipschitz in x. For term I, note that h is C-Lipschitz in x, so we have

xI

2



1 mini i2

h (x) 2

xh

(x)

2



1 2 (C

x

2+h

(0)) · C



C 2 (1 +

x 2).

Putting together the two terms gives

 log p(x)

2



C 2 (1 +

x

2) +

C 2

 d



C 2 (

x

 2 + d).

(22)

Further, under either p1 or p2 (for example p1 ), we have

Ep1 [

X

2 2

]



E[

G1 (Z)

2 2

]



C (RW

,

Rb,

)E[(

Z

2 + 1)2]  Cd.

Therefore we can apply Theorem D.2(c) and get

C



 Cd

Ep1 [log p(x)] - Ep2 [log p(x)]  2 2 Cd + d W2(p1 , p2 )  2 W2(p1 , p2 ).

We now turn to the W1 bound. The bound eq. (22) already implies that for X 2  D,

 log p(x)

2



C 2 (D + d).



Choosing D = K d, for a sufficiently large constant K, by the bound X 2  C( Z 2 + 1) we

have the tail bound

P( X 2  D)  exp(-20d).  On the other hand by the bound | log p(x)|  C(( x 2 + 1)2/2 + d( x 2 + 1)) we get under either p1 or p2 (for example p1 ) we have

Ep1

(log p(X))2

C  4 E

X



2 2

+

d(

X

2 + 1)

2

C d2  4 .

 Thus we can substitute D = K d, L(D) = C(1 + K) d/2, M = Cd2/4, and ptail(D) = exp(-2d) into Theorem D.2(b) and get



C(1 + K) d

C d2

Ep1 [log p(x)] - Ep2 [log p(x)] 

 Cd



2 W1(p1 , p2 ) + 4 4 exp(-20d)

 2 W1(p1 , p2 ) + d exp(-10d) .

D.6 PROOF OF LEMMA D.5
For any log-density neural network F(x) = log p(x), reparametrize so that (Wi, bi) represent the weights and the biases of the inverse network z = G-1(x). By eq. (21), this has the form
(Wi, bi) - (W --1i+1, -W --1i+1b -i+1), i  [ ].

21

Under review as a conference paper at ICLR 2019

Consequently the reparametrized  = (Wi, bi)i[ ] belongs to the (overloading )  =  = (Wi, bi)i=1 : max Wi op , Wi-1 op  RW , bi 2  RW Rb, i  [ ] . (23)

As F = {F1 - F2 : 1, 2  }, the Rademacher complexity of F is at most two times the quantity

Rn := Rn({F :   }, p ) =

1n

sup


n iF(Xi)
i=1

,

We do one additional re-parametrization. Note that the log-density network F(x) = log p(x) has the form

F(x) =  (G-1(x))+log

det G- 1(x) x

+K() =

1 2

h , diag(-2)h

+log det G- 1(x) +K(). x

(24)

The constant C() is the sum of the normalizing constant for Gaussian density (which is the same

across all , and as we are taking subtractions of two log p, we can ignore this) and the sum of
log det(Wi), which is upper bounded by d RW . We can additionally create a parameter K = K()  [0, d RW ] for this term and let   (, K).

For any (reparametrized) ,   , define the metric

(,  ) = max Wi - Wi op , bi - bi 2 , |K - K | : i  [d] .

Then we have, letting Y

=

1 n

n i=1

i F (Xi )

denote

the

Rademacher

process,

the

one-step

dis-

cretization bound (Wainwright, 2018, Section 5).

Rn  E

sup

|Y - Y | + E

sup |Yi | .

, ,(, )

iN (,,)

(25)

We deal with the two terms separately in the following two lemmas.

Lemma D.6 (Discretization error). There exists a constant C = C(RW , Rb, ) such that, for all ,    such that (,  )  , we have

|Y

-

Y

|



C 2

1 n

n

 d(1 +

Xi 2) +

Xi

2 2

· .

i=1

Lemma D.7 (Expected max over a finite set). There exists constants 0, C (depending on RW , Rb, but not d, ) such that for all   02n,

E

max |Y|
iN (,,)



C d2

log

max

{RW ,Rb} 



+

C d2  4n .

Substituting the above two Lemmas into the bound eq. (25), we get that for all   min {RW , Rb} and   02n,

Rn



C 2 E[ d(1 +

X

2) +

X

22]

·



+

C d2

log

max {RW 


,Rb }

+

C d2  4n .

I II

As As

X E[

= G

Z

2 2

]

(Z), we have = k + 2(d -

X2 k) 

C d, we

Z2 have

+ G (0) E[ d(1 +

2  C( Z 2 +1) for some constant C > 0. X 2) + X 22]  Cd for some constant C,

giving

that

I



Cd 2

· .

Choosing



=

c/(dn)

guarantees

that

I



1 2n

.

For

this

choice

of

,

term

II

has the form

II



C d2

log(nd max {RW , Rb}) 

+

C d2  4n



Cd2 log 

n

+

C d2  4n ,

the last bound holding if n  d. Choosing  = n log n/4, which will be valid if n/ log n 

-8-0 2, we get

II  Cd2

log n 4n = C

d4 log n 4n .

This term dominates term I and is hence the order of the generalization error.

22

Under review as a conference paper at ICLR 2019

D.6.1 PROOF OF LEMMA D.6

Fix ,  such that (,  )  . As Y is the empirical average over n samples and |i|  1, it suffices to show that for any x  Rd,

C |F(x) - F (x)|  2

 d(1 +

x 2) +

x

2 2

· .

For the inverse network G- 1(x), let hk(x)  Rd denote the k-th hidden layer:

h1(x) = (W1x+b1), · · · , h -1(x) = (W -1h -2(x)+b -1), h (x) = W h -1(x)+b = G- 1(x).

Let hk(x) denote the layers of G-1(x) accordingly. Using this notation, we have

1 F(x) = 2

h , diag(-2)h

-1
+ 1d, log -1 (hk) + K.

k=1

Lipschitzness of hidden layers We claim that for all k, we have

k
hk 2  (RW )k x 2 + Rb (RW )j ,
j=1

(26)

and consequently when (,  )  , we have


k-1



hk - hk 2  C(RW , Rb, k)(1 + x 2), C(RW , Rb, k) = O  j(RW )j(1 + Rb) .

j=0

(27)

We induct on k to show these two bounds. For eq. (26), note that h0 = x 2 and

hk 2 = (Wkhk-1 + bk) 2  (RW hk-1 2 + RW Rb),

so an induction on k shows the bound. For eq. (27), note that

h1 - h1 2  W1 - W1 op x 2 + b1 - b1 2  (1 + x 2),
so the base case holds. Now, suppose the claim holds for the (k - 1)-th layer, then for the k-th layer we have

hk - hk 2 =

(Wkhk-1 + bk) - (Wkhk-1 + bk) 2  

Wkhk-1 - Wkhk-1

+
2

bk - bk 2





+

Wk op

hk-1 - hk-1

+
2

Wk - Wk op

hk-1 2

k-1
   + RW C(RW , Rb, k - 1)(1 + x 2) +  (RW )k-1 x 2 + Rb (RW )j
j=1

k-1
 (1 + x 2) RW C(RW , Rb, k - 1) + (1 + Rb) (RW )j ,
j=1

` verifying the result for layer k.

C(RW ,Rb,k)

Dealing with (·)2 and log -1 For the log -1 term, note that |(log -1 ) | = |-1 /-1 |   by assumption. So we have the Lipschitzness

-1  -1

1d, log -1 (hk) - log -1 (hk)  d

hk - hk 2

k=1

k=1

 -1



 d  C(RW , Rb, k) ·(1 + x 2) = C d(1 + x 2) · .

k=1

C

23

Under review as a conference paper at ICLR 2019

For the quadratic term, let A = diag(-2) for shorthand. Using the bound (1/2)| u, Au -

v, Av

|

A op ( v 2

u-v 2+

u-v

2 2

/2),

we

get

1 2 | h , A h - h , A h |  A op

h

2

h -h

2+

h -h

2 2

/2



1 2

C · C(RW , Rb, )(1 +

x 2)2 + C(RW , Rb, )22(1 +

x 2)2/2

C  2 (1 +

x 2)2 · .

Putting together Combining the preceding two bounds and that |K - K |  , we get

|F(x) - F (x)| 

C 2 (1 +

 x 2)2 + C d(1 +

x 2) + 1





C 2 ( d(1 +

x

2) +

x

2 2

)

·

.

D.6.2 PROOF OF LEMMA D.7

Tail decay at a single  Fixing any   , we show that the random variable

1n

1n

Y = n iF(xi) = n i

i=1 i=1

-1

h (xi), Ah (xi) +

1d, log -1 (hk(xi)) + K .

k=1

is suitably sub-exponential. To do this, it suffices to look at a single x and then use rules for independent sums.

First, each 1d, log -1(hk(x)) is sub-Gaussian, with mean and sub-Gaussianity parameter O(Cd). Indeed, we have

1d, log -1(hk(x)) = 1d, log -1(hk(G (z))) = 1d, log -1(hk(G (z))) ,

where z = [z1:k, z(k+1):d/] is standard Gaussian. Note that (1) Lipschitzness of G is bounded

by that (see eq.

of G, (27)),

which is (3) v 

some
d j=1

C(RW , Rb, log -1(vj)

),(2) all hidden-layers are C(RW , is d-Lipschitz. Hence the above

Rb, term

)-Lipschitz is a C d-

Lipschitz function of a standard Gaussian, so is Cd-sub-Gaussian by Gaussian concentration C.2.

To bound the mean, use the bound

 E 1d, log -1(hk(G (z)))  E[ dC z 2]  Cd.

As we have - 1 terms of this form, their sum is still Cd-sub-Gaussian with a O(Cd) mean (absorbing into C).

Second, the term h , Ah is a quadratic function of a sub-Gaussian random vector, hence is subexponential. Its mean is bounded by E[ A op h 22]  Cd/2. Its sub-exponential parameter is 1/2 times the sub-Gaussian parameter of h , hence also Cd/2. In particular, there exists a constant 0 > 0 such that for all   02,

E[exp( h , Ah )]  exp

Cd C2d22 2  + 4

.

(See for example (Vershynin, 2010) for such results.) Also, the parameter K is upper bounded by d RW = Cd.

Putting together, multiplying by i (which addes up the squared mean onto the sub-Gaussianity / sub-exponentiality and multiplies it by at most a constant) and summing over n, we get that Y is mean-zero sub-exponential with the MGF bound

 E[exp(Y)] = E exp n i



exp

C d2 2 4n

,

  02n.

-1

h (xi), Ah (xi) +

1d, log -1 (hk(xi))

k=1

n
(28)

24

Under review as a conference paper at ICLR 2019

Bounding the expected maximum We use the standard covering argument to bound the expected

maximum. Recall that (,  ) = max Wi - Wi op , bi - bi 2 , |K - K | . Hence, the covering number of  is bounded by the product of independent covering numbers, which further by the volume argument is

N (, , ) 

1 + 2RW

d2
·

1 + 2Rb

d
·

1 + CdRW

 

k=1

k=1

 exp C d2 log 3RW + d log 3Rb 

.

Using Jensen's inequality and applying the bound appendix D.6.2, we get that for any   02n,

E max Y
iN (,,)





N (,,)

11

  log E 

exp(Yi )  

i=1

C d2 2 log N (, , ) + 4n



C d2

log

max {RW 

,Rb }



+

C d2  4n .

E PROOFS FOR SECTION 4.2

E.1 FORMAL THEOREM STATEMENT

Towards stating the theorem more quantitatively, we will need to specify a few quantities of the generator class that will be relevant for us.

First, for lution of

notational simplicity, we override the definition of p and a Gaussian distribution. Concretely, let Dz

p =

by {z

a :

trzuncated vdelrosgio2nd,ozf

the convo Rk} be

a truncated region in the latent space (which contains an overwhelming large part of the probability mass), and the let Dx = {x : z  Dz, G(z) - x 2   d log2 d} be the image of Dz under G.

Recall that

G(z) =  (Wl (Wl-1 . . .  (W1z + b1) + . . . bl-1) + bl) .

Then, let p(x) be the distribution obtained by adding Gaussian noise with variance 2 to a sample from G, and truncates the distribution to a very high-probability region (both in the latent variable and observable domain.) Formally, let p be a distribution over Rd, s.t.

p (x) 

e-

z

2 e-

G (z)-x 2

2
dz, x  Dx

zDz

(29)

For notational convenience, denote by f : Rk  R the function f (z) = - z 2 - G(z) - x 2/2, and denote by z a maximum of f . Furthermore, whenever clear from the context, we will drop  from p and G.

We introduce several regularity conditions for the family of generators G:

Assumption E.1. We assume the following bounds on the partial derivatives of f : we denote S :=

maxzDz: z-z  2( G(z)-x 2) , and min := maxzDz: z-z  min(2 G(z)-x 2).

Similarly, we denote t(z)

:=

k3 max|I|=3



G (z)-x I

2
(z).

and T

=

maxz:zDz

|t(z)|.

We

will

denote

by

R

an

upper

bound

on

the

quantity

1 Ll

L j=i

min(Wj

)

and

by

LG

an

upper

bound

on the quantity LG := 2lLl

.l 1
i=1 min(Wi)

Finally, we assume the inverse activation function is Lipschitz, namely |-1(x)--1(y)|  L|x-

y|.

Note on asymptotic notation: For notational convenience, in this section, , , as well as the
Big-Oh notation will hide dependencies on R, LG, S, T (in the theorem statements we intentionally emphasize the polynomial dependencies on d.) The main theorem states that for certain F, d~F
approximates the Wasserstein distance.

25

Under review as a conference paper at ICLR 2019

Theorem E.1. Suppose the generator class G satisfies the assumption E.1 and let F be the family of functions as defined in Theorem E.2. Then, we have that for every p, q  G,

W1(p, q) d~F (p, q) poly(d) · W1(p, q)1/6 + exp(-d).

(30)

Furthermore, when n

poly(d) we have Rn(F , G)

poly(d)

log n

n

.

Here

hides dependencies

on R, LG, S, and T .

The main ingredient in the proof will be the theorem that shows that there exists a parameterized family F that can approximate the log density of p for every p  G.

Theorem E.2. Let G satisfy the assumptions in Assumption E.1. For  = O(poly(1/d)), there

exists

a

family

of

neural

networks

F

of

size

poly(

1 

,

d)

such

that

for

every

distribution

p



G,

there

exists N  F satisfying:

 (1) N approximates log p for typical x: given input x = G(z) + r, for r  10 d log d, and z  10 d log d for  = O(poly(1/d)) it outputs N (x), s.t.

|N (x) - log p(x)| = Opoly(d)( log(1/)) + exp(-d)

(2) N is globally an approximate lower bound of p: on any input x, N outputs N (x)  log p(x) +

Opoly(d)( log(1/)) + exp(-d).

(3) N approximates the entropy in the sense that: the output N (x) satisfies |Ep N (x) - H(p)| = Opoly(d)( log(1/)) + exp(-d)

Moreover,

every

function

in

F

has

Lipschitz

constant

O(

1 4

poly(d)).

The approach will be as follows: we will approximate p(x) essentially by a variant of Laplace's method of integration, using the fact that

p(x) = C

e dz-

z

2-

G(z)-x 2

2

zDz

for a normalization constant C that can be calculated up to an exponentially small additive factor. When x is typical (in case (1) of Theorem E.2), the integral will mostly be dominated by it's maximum value, which we will approximately calculate using a greedy "inversion" procedure. When x is a atypical, it turns out that the same procedure will give a lower bound as in (2).

We are ready to prove Theorem E.1, assuming the correctness of Theorem E.2:

Proof of Theorem E.1. By Theorem E.2, we have that there exist neural networks N1, N2  F that approximate log p and log q respectively in the sense of bullet (1)-(3) in Theorem E.2. Thus we have that by bullet (2) for distribution q, and bullet (3) for distribution q, we have

E [N1(x) - N2(x)]  E [log p] - E [log q] - O( log 1/)
p p p

(31)

Similarly, we have

E [N2(x) - N1(x)]  E[log q] - E[log p] - O( log 1/)
q q q

(32)

Combining the equations above, setting f = N1(x) - N2(x), we obtain that

WF (p, q)  Ep [f ] - Eq [f ]  Dkl(p q) + Dkl(q p) - O( log 1/) (33)

Therefore, by definition, and Bobkov-Go¨tze theorem (Dkl(p, q) W1(p, q))

W1(p, q)  W1(p, q) + O() (Dkl(p q) + Dkl(q p))1/2 + O()  (WF (p, q) + O( log(1/))1/2 + O()  O(d~F (p, q))

(34)

Thus we prove the lower bound.

Proceeding to the upper bound, notice that WF (p, q)

1 4

W1 (p ,

q)

since

every

function

in

F

is

O(poly(d)

1 4

)-Lipschitz

by

Theorem

E.2.

We

relate

W1(p, q)

to

W1(p, q),

more

precisely

26

Under review as a conference paper at ICLR 2019

we prove: W1(p, q)  (1 + e-d)W1(p, q). Having this, we'd be done: namely, we simply set  = W 1/6 to get the necessary bound.

Proceeding to the claim, consider the optimal coupling C of p, q, and consider the induced coupling Cz on the latent variable z in p, q. Then,

W1(p, q) =

G(z) - G(z ) 1dCz(z, z )det

zRd

 G (z ) z

det

G(z ) z

Consider the coupling C~z on the latent variables of p, q, specified as C~z(z, z ) = C(z, z )(1 - Pr[z / Dz])2. The coupling C~ of p, q specified by coupling z's according to C~z and the (truncated) Gaussian noise to be the same in p, q, we have that

W1(p, q) 

G(z) - G(z ) 1dC~z(z, z )det
zDz

 G (z ) z

det

G(z ) z



(1 + e-d)
zDz

G(z) - G(z

)

1dCz(z, z

)det( G(z) )det z

G(z ) 

 (1 + e-d)W1(p, q)

The generalization claim follows completely analogously to Lemma D.5, using the Lipschitzness bound of the generators in Theorem E.2.

The rest of the section is dedicated to the proof of Theorem E.2, which will be finally in Section E.3.

E.2 TOOLS AND HELPER LEMMAS

First, we prove several helper lemmas:

Lemma E.3 (Quantitative bijectivity).

G(z~) - G(z)



1 Ll

L j=i

min(Wj

)

z~ - z

.

Proof. The proof proceeds by reverse induction on l. We will prove that

hi - h~i



1L

Ll-i

min(Wj )
j=i

z~ - z

The claim trivial holds for i = 0, so we proceed to the induction. Suppose the claim holds for i.

Then,

Wihi + bi - (Wih~i + bi)

1 
min(Wi)

hi - h~i

and

(Wihi + bi) - (Wih~i + bi)

 L min(Wi)

hi - h~i

by Lipschitzness of -1. Since hi-1 = (Wihi + bi) and h~i-1 = (Wihi + bi),

hi-1 - h~i-1



1 Ll-(i-1)

L
min(Wj )
j=i-1

z~ - z

as we need.

Lemma E.4 (Approximate inversion). Let x  Rd be s.t. z, G(z) - x  . Then, there is a

neural network N of size O(ld2), activation function -1 and Lipschitz constant Ll

l max(Wi) i=1 m2 in(Wi)

which recovers a z^, s.t. z^ - z  2lLl

l1 i=1 min(Wi)

27

Under review as a conference paper at ICLR 2019

Proof. N will iteratively produce estimates h^i, s.t.

(1) h^0 = x

(2) h^i

= -1(argminh

Wih + bi - -1(h^i-1)

2 2

)

We will prove by induction that |hi - h^i|  2iLi

i j=1

.1
min(Wj )

The

claim

trivial

holds

for

i

=

0,

so we proceed to the induction. Suppose the claim holds for i. Then,

min Wi+1h + bi+1 - h^i
h

 Wi+1hi+1 + bi+1 - -1(h^i)

= -1(hi) - -1(h^i)

 L hi - h^i



2iLi+1

i j=1

1 min(Wj )

where the last inequality holds by the inductive hypothesis, and the next-to-last one due to Lipschitzness of -1.

Hence, denoting h~ = argminh Wi+1h + bi+1 - -1(h^i) 22, we have

Wi+1h~ - W i + 1hi+1) = Wi+1h~ + bi+1 - -1(h^i) + -1(h^i) - Wi+1hi+1 - bi+1  Wi+1h~ + bi+1 - -1(h^i) + -1(h^i) - Wi+1hi+1 - bi+1 = Wi+1h~ + bi+1 - -1(h^i) + -1(h^i) - -1(hi)



2i+1Li+1

i j=1

1 min(Wj )

This implies that

Wi+1(h~ - hi+1)



2

i
Li+1
j=1

1 min(Wj )

which in turns means

h~ - hi+1



2i+1Li+1

i+1 j=1

1 min(Wj )

which completes the claim.

Turning to the size/Lipschitz constant of the neural network: all we need to notice is that h^i = -1(Wi(h^i-1 - bi)), which immediately implies the Lipschitzness/size bound by simple induction.

We also introduce a few tools to get a handle on functions that can be approximated efficiently by neural networks of small size/Lipschitz constant. Lemma E.5 (Composing Lipschitz functions). If f : Rd2  Rd3 and g : Rd1  Rd2 are L, KLipschitz functions respectively, then f  g : Rd1  Rd3 is LK-Lipschitz.
Proof. The proof follows by definition essentially: f (g(x)) - f (g(x ))  L g(x) - g(x )  LK x - x
Lemma E.6 (Calculating singular value decomposition approximately, (Demmel et al., 2007)). There is a neural network with size O(n3poly(log(1/ ))) that given a symmetric matrix A  Rn×n with minimum eigenvalue gap mini=j |i - j|   and eigenvectors {ui} outputs {u~i, ~i} s.t. : (1) | u~i, u~j |  , i = j and u~i = 1 ± . (2) |u~i - ui|  /, |~i - i|  , i.
28

Under review as a conference paper at ICLR 2019

(Note the eigenvalue/eigenvector pairs for A are unique since the minimum eigenvalue gap is nonzero).

Lemma E.7 (Backpropagation, (Rumelhart et al., 1986)). Given a neural network f : Rm  R

of depth l and size N , there is a neural network of size O(N + l) which calculates the gradient

f i

,

i



[m].

E.3 PROOF OF THEOREM E.2

We will proceed to prove the two parts one at a time.
First, we prove the following lemma, which can be seen as a quantitative version of Laplace's method for evaluating integrals:
 Lemma E.8("Tail" bound for integral at z). Let x = G(z) + r, for r  10 d log d, and z  10 d log d. The, for  = O(poly(1/d)), and
 d
 = 100 log(1/) R

it holds that

ef(z)dz  

ef (z) dz

z: z-z >,zDz

zDz

Proof. Let's write

ef(z)dz =

ef(z)dz +

ef (z) dz

zDz

z: z-z 

z: z-z >,zDz

(35)

To prove the claim of the Lemma, we will lower bound the first term, and upper bound the latter, from which the conclusion will follow.
Consider the former term. Taylor expanding in a neighborhood around z we have

f (z) = f (z) + (z - z)

2f (z)(z

-

z)

±

T 2

z - z

3

where the first-order term vanishes since z is a global optimum. Furthermore, 2f (z) 0 for the same reason. Hence, by Taylor's theorem with remainder bounds, and using the fact that ex  1+x,
we have

ef(z)dz  1 - T 3 f (z)

(z - z) 2f (z)(z - z)

z: z-z 

2 z: z-z 

The integral on the right is nothing more than the (unnormalized) cdf of a Gaussian with covariance matrix (-2f (z))-1.

Moreover,

-2f (z)

is

positive

definite

with

smallest

eigenvalue

bounded

by

R2 d2

,

since

-2f (z) = 2(

z

2)

+

1 2

2(

G(z) - x

2)

=I+

1 2

2

(

G(z) - x

2)

and

2( G(z) - x 2)

Gi(z) Gi(z) + (Gi(z) - xi)2Gi(z)

ii

where Gi is the i-thcoordinate of G. We claim

i Gi(z) Gi(z)

R2 d

I,

and

i(Gi(z) -

xi)2Gi(z) 2  d log d. The latter follows from the bound on r and Cauchy-Schwartz. For the

29

Under review as a conference paper at ICLR 2019

former, note that we have

v Gi(z) Gi(z) v = v, Gi(z) 2

ii

=
i

lim Gi(z + v) - Gi(z) 2
0

By Lemma E.3,

G(z +

v) - G(z)

 R , so i, s.t. |Gi(z +

v)

-

Gi(z)|



R .
d

Hence,

lim Gi(z + v) - Gi(z) 2  R2 0 d
i

from which

i Gi(z) Gi(z)

R2 d

follows.

Using

standard

Gaussian

tail

bounds,

since







log(1/) R

 d,

we

have

(z - z) 2f (z)(z - z)  (1 - )det(4(-2f (z)))1/2
z: z-z 

(36)

We proceed to the latter term in eq. (35). We have

f (z) - f (z) =

z 2 +

G(z) - x 2 -
2

z 2-

G(z) - x 2 2



G(z) - x 2 - G(z) - x 2 - 2 2

z 2 - z 2

=

G(z) - G(z) - r 2

2

-

G(z) - G(z) + r 2 2

-

z 2 - z 2

1 

(R

z - z 2

-

r )2 -

r2 2

-

z 2 - z 2

2 

(R

z - z 2

-

r )2 -

r2 2

-

z - z 2 - 2 z - z

z

where 1 follows from the definition of x, and 2 by triangle inequality.

Note also that

1 2

R

z - z



r

since  

2r R

, which in turn implies

(R z - z - 2

r )2 -

r2 2

-

z - z 2 - 2 z - z

z



3 16

R2 2

z - z

Finally,

2



R2 32

z

, so that it follows

z - z 2

3R2 162

-1

- 2 z - z

z

 3R2 322

z - z 2

Putting

these

estimates

together,

we

get

f (z)

<

f (z)

-

3R2 322

z - z

2, which implies

ef(z)dz  ef(z)

e-

3R2 322

z-z 2

z: z-z >

z: z-z >

The

integral

on

the

right

is

again

the

unnormalized

cdf

of

a

Gaussian

with

covariance

matrix

3R2 322

I

,

so by Gaussian tail bounds again, and using that the smallest eigenvalue of -2f (z) is lower-

bounded

by

R2 2

we

have

ef(z)  ef(z)det(4(-2f (z)))1/2
z: z-z >

30

Under review as a conference paper at ICLR 2019

as we wanted. Putting this together with eq. (36), we get the statement of the theorem.

With this in mind, we can prove part (1) of our Theorem E.2 restated below:

Theorem E.9.

There is a neural network N

of

size

poly(

1 

,

R,

LG,

T

,

S,

d)

with

Lipschitz

constant

O(poly(R,LG, d, T, S), 1/4) which given as input x = G(z) + r, for r  10 d log d, and

z  10 d log d for  = O(poly(1/d)) outputs N (x), s.t.

|N (x) - log p(x)| = Opoly(d)( log(1/)) + exp(-d)

Proof. It suffices to approximate

ef (z) dz

(37)

zDz

up to a multiplicative factor of 1 ± Opoly(d)( log(1/)), since the normalizing factor satisfies

e-

z

2 e-

G (z)-x 2

2
dz = (1 ± exp(-d))det(4I)-1/2det(4/2I)-1/2

xDx zDz

We will first present the algorithm, then prove that it: (1) Approximates the integral as needed. (2) Can be implemented by a small, Lipschitz network as needed.

The algorithm is as follows:

Algorithm 1 Discriminator family with restricted approximability for degenerate manifold

1: Parameters: Matrices E1, E2, . . . , Er  R, matrices W1, W2, . . . , Wl.

2:

Let 

=

100

LG 

log(1/)

R2

and let S be the trivial 2-net of the matrices with spectral norm bounded

by O(1/2).

3: Let z^ = Ninv(x) be the output of the "invertor" circuit of Lemma E.4. 4: Calculate g = f (z^), H = 2f (z^) by the circuit implied in Lemma E.7.

5: Let M be the nearest matrix in S to H and Ei, i  [r] be s.t. M + Ei has ()-separated eigenvalues. (If there are multiple Ei that satisfy the separation condition, pick the smallest i.)
6: Let (ei, i) be approximate eigenvector/eigenvalue pairs of H + Ei calculated by the circuit

implied in Lemma E.6.

7: Approximate Ii = log |ci| eci ei,g + i c2i i dci , i  [r] by subdividing the interval (0, ) into intervals of size 2 and evaluating the resulting Riemannian sum instead of the integral.

8: Output i Ii.

First, we will show (1), namely that the Algorithm 1 approximates the integral of interest. We'll

use an approximate version of Lemma E.8 ­ with a slightly different division of where the "bulk"

of the integral is located. As in Algorithm 1, let z^ = Ninv(x) be the output of the "invertor" circuit

of Lemma E.4.

and let 

=

100d

LG 

log(1/)

R2

and denote by B the set B

=

{z

:

| z - z^, ei

|



}. Furthermore, let's define how the matrices Ei, i  [r] are to be chosen. Let S be an 2-

net of the matrices with spectral norm bounded by O(1/2). We claim that there exist matrices

E1, E2, . . . , Er, r = (d log(1/)), s.t. if M  S, at least one of the matrices M + Ei, i  [r]

has eigenvalues that are ()-separated and

Ei 2 

d 

.

Indeed,

let

let

E

be

a

random

Gaussian

matrix

with

entrywise

variance

1 2

.

By

Theorem

2.6

in

(Nguyen

et

al.,

2017),

for

any

fixed

matrix

A, with probability 3/4, mini |i(A + E) - i+1(A + E)| = (). The number of matrices in S

is bounded by 2O(d log(1/)), so Ei, i  [r] exist by the probabilistic method.

31

Under review as a conference paper at ICLR 2019

We can write the integral of interest as

ef(z)dz +

ef (z) dz

zB

zB¯

Note that

z - z = z - z^ + z^ - z  z - z^ - z^ - z

 4LG z^ - z R

This means that {z :

z - z



4LG R



}



B,

so

by

Lemma

E.8,

we

have

ef(z)  O() ef(z)dz

zB¯

z

which means that to prove the statement of the Theorem, it suffices for us to approximate zB ef(z)dz.

Consider the former term first. By Taylor's theorem with remainder, expanding f in a neighborhood near z^, we get

f (z) = f (z^) + (z - z^)

f (z^) + (z - z^)

2f (z^)(z

-

z^)

±

T 2

z - z^ 3

For notational convenience, same as in Algorithm 1, let's denote by H := (z - z^) 2f (z^)(z - z^), and g := f (z^). Steps 5-8 effectively perform a change of basis in the eigenbasis of H and evaluate the integral in this basis ­ however to ensure Lipschitzness (which we prove later on), we will need to perturb H slightly.

Let M be the closest matrix to 2f (z^) in the 2-net S and let ei be approximate eigenvectors of

H~ = M + Ei in the sense of Lemma E.6, s.t. the eigenvalues of M + Ei are ()-separated.



Since

Ei



d 

,

we

have

|(z - z^) H~ (z - z^) - (z - z^) H(z - z^)| = O()

Hence,

f (z) = f (z^) + (z - z^)

f (z^) + (z - z^)

H~ (z

-

z^)

±

T 2

z - z^ 3 ± O()

Towards rewriting f in the approximate basis ei, let z - z^ = i ciei for some scalars ci. We have

f (z) = f (z^) +

ci ei, g +

c2i i

±

T 2

(

ci2)3/2

ii

i

By

Taylor's

theorem

with

remainder,

ex

=

1

+

x

±

e

x2 2

,

if

x

<

1.

Hence,

ef(z)dz =
zB

1

±

ed3/2

T 2

3

ef (z^)
i

eci ei,g + i c2i i
|ci |

Calculating the integral by subdividing (0, ) into intervals of size 2, and approximating the integral

by the accompanying Riemannian sum, and taking into account |ci| = O() and i, g

=

O(

1 2

),

we get a multiplicative approximation of

eci ei,g + i ci2i
|ci |
of the order e = 1 + O(), which is what we want.

32

Under review as a conference paper at ICLR 2019

We turn to implementing the algorithm by a small neural network with good Lipschitz constant. Both

the Lipschitz constant and the size will be handled by the composition Lemma E.5 and analyzing

each step of Algorithm 1. Steps 3 and 4 are handled by our helper lemmas: the invertor circuit

by Lemma E.4 has Lipschitz constant LG; calculating the Hessian 2f (z^) can be performed by a

polynomially sized neural network by Lemma E.7 and since the third partial derivatives are bounded,

so

the

output

of

this

network

is

O(

T 2

)-Lipschitz

as

well.

We

turn

to

the

remaining

steps.

Proceeding to the eigendecomposition, by Lemma E.6, we can perform an approximate eigendecomposition of H~ with a network of size O(poly(d)) ­ so we only need to handle the Lipschitzness.
We will show that the result of Steps 5-6, the vectors uj and scalars j are Lipschitz functions of H.

Suppose that H and H are s.t. H - H  2 first. Then, H, H are mapped to the same
matrix M in S, {uj} and {uj} are the eigenvectors of H + Ei and H + Ei for some i  [r]. First, by Weyl's theorem, since H + Ei = M + (H - M ) + Ei, the eigenvalues of H + Ei are () - H - M = ()-separated. Furthermore, since H + Ei = H + Ei + (H - H), by

Wedin's theorem, uj(H + Ei) - uj(H + Ei) = O

H -H

1 2

.

If, on the other hand,

H - H > 2, and Ea, Eb are the perturbation matrices used for H and H , since the eigenvalues

of H + Ea are ()-separated, by Wedin's theorem,

11

uj(H + Eb) - uj(H + Ea)  O

( H-H 

2+

Eb - Ea 2)

=O

H -H 

2

so we get that the map to the vectors uj is O(1/)-Lipschitz. A similar analysis shows that the map to the eigenvalues j is also Lipschitz.

Finally, we move to calculating the integral in Step 7: the trivial implementation of the integral by a neural network has size O(poly()) and as a function of ei, g, i is O(1/)-Lipschitz, which proves the statement of the theorem.

Moving to Part (2) of Theorem E.2, we prove:
Theorem E.10. Let N be the neural network N used in Theorem E.9. The network additionally satisfies N (x)  log p(x) + OR,LG,d( log(1/)) + exp(-d), x  D.

Proof. Recalling the proof of Theorem E.9, and reusing the notation there, we can express

q(x) = exp(f (z)) =

ef(z)dz +

ef (z) dz

z

zB

zB¯

Since the neural network N ignores the latter term, and we need only produce an upper bound, it suffices to show that N approximates

ef (z) dz
z: z-z^ 
up to a multiplicative factor of 1 - O( log(1/)). However, if we consider the proof of Theorem E.9, we notice that the approximation consider there indeed serves our purpose: Taylorexpanding same as there, we have

ef(z)dz = 1 ± ed3/2T 3 ef(z^)

zB

i

eci ei,g + i c2i i
|ci |

This integral can be evaluated in the same manner as in Theorem E.9, as our bound on T holds universally on neighborhood of radius Dx.

Finally, part (3) follows easily from (1) and (2): 
Proof of Part 3 of Theorem E.2. For points x, s.t. z, G(z) - x  10 d log d, it holds that p(x) = O(exp(-d)). On the other hand, by the Lipschitzness of N , we have N (x) = O 1 ( x ).
4
Since x  Dx implies x = O(poly(d)) the claim follows.
33

Under review as a conference paper at ICLR 2019
F EXPERIMENTS ON SYNTHETIC 2D DATASETS
Our theoretical results on neural network generators in Section 4 convey the message that mode collapse will not happen as long as the discriminator family F has restricted approximability with respect to the generator family G. In particular, the IPM WF (p, q) is upper and lower bounded by the Wasserstein distance W1(p, q) given the restricted approximability. In this section, we perform synthetic experiments with WGANs that learn various curves in two dimensions. In particular, we will train GANs that learn the unit circle and a "swiss roll" curve (Gulrajani et al., 2017) ­ both distributions are supported on a one-dimensional manifold in R2, therefore the KL divergence does not exist, but one can use the Wasserstein distance to measure the quality of the learned generator. We show that WGANs are able to learn both distributions pretty well, and the IPM WF is strongly correlated with the Wasserstein distance W1. These ground truth distributions are not covered in our Theorems 4.2 and 4.5, but our results show evidence that restricted approximability is still quite likely to hold here.
Ground truth distributions We set the ground truth distribution to be a unit circle or a Swiss roll curve, sampled from
Circle : (x, y)  Uniform( (x, y) : x2 + y2 = 1 ) Swiss roll : (x, y) = (z cos(4z), z sin(4z)) : z  Uniform([0.25, 1]).
Generators and discriminators We use standard two-hidden-layer ReLU nets as both the generator class and the discriminator class. The generator architecture is 2-50-50-2, and the discriminator architecture is 2-50-50-1.
We use the RMSProp optimizer (Tieleman & Hinton, 2012) as our update rule, the learning rates are 10-4 for both the generator and discriminator, and we perform 10 steps on the discriminator in between each generator step.
Metric We compare two metrics between the ground truth distribution p and the learned distribution q along training:
(1) The neural net IPM WF (p, q), computed on fresh batches from p, q through optimizing a separate discriminator from cold start.
(2) The Wasserstein distance W1(p, q), computed on fresh batches from p, q using the POT package10. As data are in two dimensions, the empirical Wasserstein distance W1(p^, q^) does not suffer from the curse of dimensionality and is a good proxy of the true Wasserstein distance W1(p, q) (Weed & Bach, 2017).
Result See Figure 1 for the circle experiment and Figure 2 for the Swiss roll experiment. On both datasets, the learned generator is very close to the ground truth distribution at iteration 10000. Furthermore, the neural net IPM and the Wasserstein distance are well correlated. At iteration 500, the generators have not quite learned the true distributions yet (by looking at the sampled batches), and the IPM and Wasserstein distance are indeed large.
G EXPERIMENTS ON INVERTIBLE NEURAL NET GENERATORS
We further perform synthetic WGAN experiments with invertible neural net generators (cf. Section 4.1) and discriminators designed with restricted approximability (Lemma 4.1). In this case, the invertibility guarantees that the KL divergence can be computed, and our goal is to demonstrate that the empirical IPM WF (p, q) is well correlated with the KL-divergence between p and q on synthetic data for various pairs of p and q (The true distribution p is generated randomly from a ground-truth neural net, and the distribution q is learned using various algorithms or perturbed version of p.)
10https://pot.readthedocs.io/en/stable/index.html
34

Under review as a conference paper at ICLR 2019

F-IPM Wasserstein

1.5 realG fakeG
1.0 0.5 0.0 0.5 1.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5

1.0 realG fakeG
0.5
0.0
0.5
1.0 1.0 0.5 0.0 0.5 1.0

3.0 2.5 2.0 1.5 1.0 0.5 0.0
0

F-IPM 0.8 Wasserstein
0.6 0.4 0.2 0.0 2000 4000 6000 8000 10000

(a) Iteration 500.

(b) Iteration 10000.

(c) Comparing IPM and Wasserstein.

Figure 1: Experiments on the unit circle dataset. The neural net IPM, the Wasserstein distance, and the sample quality are correlated along training. (a)(b): Sample batches from the ground truth and the learned generator at iteration 500 and 10000. (c): Comparing the F-IPM and the Wasserstein distance. RealG and fakeG denote the ground truth generator and the learned generator, respectively.

F-IPM Wasserstein

1.0

realG fakeG

0.5

0.0

0.5

1.0

1.5

2.0

2.5

2 10 1 2

1.5 realG 1.0 fakeG 0.5 0.0 0.5 1.0 1.5
10 1 2

6

F-IPM Wasserstein

1.50

5 1.25

4 1.00

3 0.75

2 0.50

1 0.25

0 0

2000 4000 6000 8000 100000.00

(a) Iteration 500.

(b) Iteration 10000.

(c) Comparing IPM and Wasserstein.

Figure 2: Experiments on the unit circle dataset. The neural net IPM, the Wasserstein distance, and the sample quality are correlated along training. (a)(b): Sample batches from the ground truth and the learned generator at iteration 500 and 5000. (c): Comparing the F-IPM and the Wasserstein distance. RealG and fakeG denote the ground truth generator and the learned generator, respectively.

G.1 SETUP
Data The data is generated from a ground-truth invertible neural net generator (cf. Section 4.1), i.e. X = G(Z), where G : Rd  Rd is a -layer layer-wise invertible feedforward net, and Z is a spherical Gaussian. We use the Leaky ReLU with negative slope 0.5 as the activation function , whose derivative and inverse can be very efficiently computed. The weight matrices of the layers are set to be well-conditioned with singular values in between 0.5 to 2.
We choose the discriminator architecture according to the design with restricted approximability guarantee (Lemma 4.1, eq. (10) eq. (11)). As log -1 is a piecewise constant function that is not differentiable, we instead model it as a trainable one-hidden-layer neural network that maps reals to reals. We add constraints on all the parameters in accordance with Assumption 1.
Training To train the generator and discriminator networks, we generate stochastic batches (with batch size 64) from both the ground-truth generator and the trained generator, and solve the min-max problem in the Wasserstein GAN formulation. We perform 10 updates of the discriminator in between each generator step, with various regularization methods for discriminator training (specified later). We use the RMSProp optimizer (Tieleman & Hinton, 2012) as our update rule.
Evaluation metric We evaluate the following metrics between the true and learned generator.
(1) The KL divergence. As the density of our invertible neural net generator can be analytically computed, we can compute their KL divergence from empirical averages of the difference of

35

Under review as a conference paper at ICLR 2019

the log densities:

Dkl(p , p) = EXp n [log p (X) - log p(X)],

where p and p are the densities of the true generator and the learned generator. We regard the KL divergence as the "correct" and rather strong criterion for distributional closeness.

(2) The training loss (IPM WF train). This is the (unregularized) GAN loss during training. Note: as typically in the training of GANs, we balance carefully the number of steps for discriminator
and generators, the training IPM is potentially very far away from the true WF (which requires sufficient training of the discriminators).

(3) The neural net IPM (WF eval). We report once in a while a separately optimized WGAN loss in which the learned generator is held fixed and the discriminator is trained from scratch to
optimality. Unlike the training loss, here the discriminator is trained in norm balls but with no other regularization. By doing this, we are finding f  F that maximizes the contrast and we regard the f found by stochastic optimization an approximate maximizer, and the loss obtained an approximation of WF .

Our theory shows that for our choice of G and F, WGAN is able to learn the true generator in KL divergence, and the F-IPM (in evaluation instead of training) should be indicative of the KL
divergence. We test this hypothesis in the following experiments.

G.2 CONVERGENCE OF GENERATORS IN KL DIVERGENCE
In our first experiment, G is a two-layer net in d = 10 dimensions. Though the generator is only a shallow neural net, the presence of the nonlinearity makes the estimation problem non-trivial. We train a discriminator with the architecture specified in Lemma 4.1), using either Vanilla WGAN (clamping the weight into norm balls) or WGAN-GP (Gulrajani et al., 2017) (adding a gradient penalty). We fix the same ground-truth generator and run each method from 6 different random initializations. Results are plotted in Figure 3.
Our main findings are two-fold:
(1) WGAN training with discriminator design of restricted approximability is able to learn the true distribution in KL divergence. Indeed, the KL divergence starts at around 10 - 30 and the best run gets to KL lower than 1. As KL is a rather strong metric between distributions, this is strong evidence that GANs are finding the true distribution and mode collapse is not happening.
(2) The WF (eval) and the KL divergence are highly correlated with each other, both along each training run and across different runs. In particular, adding gradient penalty improves the optimization significantly (which we see in the KL curve), and this improvement is also reflected by the WF curve. Therefore the quantity WF can serve as a good metric for monitoring convergence and is at least much better than the training loss curve.

Figure 3: Learning an invertible neural net generator on synthetic data. The x-axis in all the graphs indicates the number of steps. The left-most figure shows the KL-divergence between the true distribution p and learned distribution q at different steps of training, the middle the estimated IPM (evaluation) between p and q, and the right one the training loss. We see that the estimated IPM in evaluation correlates well with the KL-divergence. Moving average is applied to all curves.
36

Under review as a conference paper at ICLR 2019
To test the necessity of the specific form of the discriminator we designed, we re-do the same experiment with vanilla fully-connected discriminator nets. Results (in Appendix G.4) show that IPM with vanilla discriminators also correlate well with the KL-divergence. This is not surprising from a theoretical point of view because a standard fully-connected discriminator net (with some overparameterization) is likely to be able to approximate the log density of the generator distributions (which is essentially the only requirement of Lemma 4.3.) For this synthetic case, we can see that the inferior performance in KL of the WGAN-Vanilla algorithm doesn't come from the statistical properties of GANs, but rather the inferior training performance in terms of the convergence of the IPM. We conjecture similar phenomenon occurs in training GANs with real-life data as well.
G.3 PERTURBED GENERATORS
Figure 4: Scatter plot of KL divergence and neural net IPM on perturbed generator pairs. Correlation between log(Dkl(p q) + Dkl(q p)) and log WF is 0.7315. Dashed line is WF (p, q) = 100(Dkl(p q) + Dkl(q p)).
In this section, we remove the effect of the optimization and directly test the correlation between p and its perturbations. We compare the KL divergence and neural net IPM on pairs of perturbed generators. In each instance, we generate a pair of generators (G, G ) (with the same architecture as above), where G is a perturbation of G by adding small Gaussian noise. We compute the KL divergence and the neural net IPM between G and G . To denoise the unstable training process for computing the neural net IPM, we optimize the discriminator from 5 random initializations and pick the largest value as the output. As is shown in Figure 4, there is a clear positive correlation between the (symmetric) KL divergence and the neural net IPM. In particular, majority of the points fall around the line WF = 100Dkl, which is consistent with our theory that the neural net distance scales linearly in the KL divergence. Note that there are a few outliers with large KL. This happens mostly due to the perturbation being accidentally too large so that the weight matrices become poorly conditioned ­ in the context of our theory, they fall out of the good constraint set as defined in Assumption 1.
G.4 EXPERIMENTS WITH VANILLA DISCRIMINATOR G.4.1 CONVERGENCE OF GENERATORS IN KL DIVERGENCE We re-do the experiments of Section G.2 with vanilla fully-connected discriminator nets. We use a three-layer net with hidden dimensions 50-10, which has more parameters than the architecture with restricted approximability. Results are plotted in Figure 5. We find that the generators also converge well in the KL divergence, but the correlation is slightly weaker than the setting with restricted approximability (correlation still presents along each training run but weaker across different runs). This suggests that vanilla discriminator structures might be practically quite satisfying for getting a good generator, though specific designs may help improve the quality of the distance WF .
37

Under review as a conference paper at ICLR 2019
Figure 5: Learning an invertible neural net generator on synthetic data with vanilla fully-connected discriminator nets. The x-axis in all the graphs indicates the number of steps. The left-most figure shows the KLdivergence between the true distribution p and learned distribution q at different steps of training, the middle the estimated IPM (evaluation) between p and q, and the right one the training loss. We see that the estimated IPM in evaluation correlates well with the KL-divergence. Moving average is applied to all curves. G.4.2 PERTURBED GENERATORS Correlation between KL and neural net IPM is computed with vanilla fully-connected discriminators and plotted in Figure 6. The correlation (0.7489) is roughly the same as for discriminators with restricted approximability (0.7315).
Figure 6: Scatter plot of KL divergence and neural net IPM (with vanilla discriminators) on perturbed generator pairs. Correlation between log(Dkl(p q) + Dkl(q p)) and log WF is 0.7489. Dashed line is WF (p, q) = 3 Dkl(p q) + Dkl(q p).
38

