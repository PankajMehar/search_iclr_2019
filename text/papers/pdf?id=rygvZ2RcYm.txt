Under review as a conference paper at ICLR 2019
KNOWLEDGE REPRESENTATION FOR REINFORCEMENT LEARNING USING GENERAL VALUE FUNCTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Reinforcement learning (RL) is a very powerful approach for learning good control strategies from data. Value functions are a key concept for reinforcement learning, as they guide the search for good policies. A lot of effort has been devoted to designing and improving algorithms for learning value functions. In this paper, we argue that value functions are also a very natural way of providing a framework for knowledge representation for reinforcement learning agents. We show that generalized value functions provide a unifying lens for many algorithms, including policy gradient, successor features, option models and policies, and other forms of hierarchical reinforcement learning. We also demonstrate the potential of this representation to provide new, useful algorithms.
1 INTRODUCTION
Value functions are at the heart of reinforcement learning algorithms (RL) as a very useful tool in guiding the progress towards good policies. Usually, a value function estimate, representing the expected discounted return of a policy, is maintained in order to guide policy improvement. However, the goal of having realistic AI agents requires us to consider how they may go beyond the goal of solving one task, to a setting in which such agents can acquire and maintain diverse knowledge about the world. This brings about the question of how such knowledge should be expressed and maintained in an RL agent.
In this paper, we highlight the fact that value functions can in fact be used as the main building block for knowledge representation in RL agents. We build on the framework of General Value Functions (GVFs) (White, 2015; Sutton & Tanner, 2005; Sutton et al., 2011; Schaul & Ring, 2013), which have been proposed in prior work as a way to capture knowledge about the world in a flexible fashion. GVFs represent long-term predictions about different aspects of an agent's observations. For example, an RL agent interested in hockey can ask questions like "what is the expected time until I hit the puck, given my current strategy?" or "what are the chances of my team winning this game?" and represent the answers to these questions as GVFs. This form of knowledge representation provides two clear benefits. First, since it is grounded in actual observations, it can be learned incrementally from a single stream of experience, using off-policy learning methods. Second, as illustrated by the example questions above, it can be used to model the world at different time scales (Modayil et al., 2014). The Horde architecture (Sutton et al., 2011) illustrates in fact the successful large-scale learning of GVFs about a diversity of signals and at many different time scales, in parallel, from a single stream of data. This aspect of GVFs is very appealing, as it allows using the experience (which may be difficult to acquire) in order to learn many different things. GVFs also have the advantage of being able to profit from a long history of RL algorithm development for value function learning (Sutton & Barto, 1998).
Our main contribution in this paper is to unify and interpret several different RL algorithms, including policy gradients (Sutton et al., 1999a) and different ways of achieving temporal abstraction, such as successor features (Barreto et al., 2017), option models and value functions (Sutton et al., 1999b), feudal networks (Vezhnevets et al., 2017) and universal value functions (Schaul et al., 2015) through the lens of GVFs. We also explore how GVFs can be used to facilitate the development of new algorithmic ideas in RL, and we illustrate the benefits of this approach through two algorithms: a new approach to policy gradient, which produces much more stable and data-efficient convergence, and a combination of feudal networks and successor features.
1

Under review as a conference paper at ICLR 2019

2 BACKGROUND

Consider a Markov Decision Process (MDP) with state space S, action space A, and transition

function P (s |s, a). A task is specified using an extrinsic reward function R : S × A × S  [0, 1],

and a discount value   [0, 1], which define the expected discounted return: E[

 t=0

tRt+1]

=

E [

 t=0

(

t i=1

)Rt+1],

also

known

as

the

value

function

of

a

given

policy

.

General Value Functions (GVFs) Sutton et al. (2011) provide a unified way of expressing predic-
tions about signals other than extrinsic rewards, under policies that are different from the behavior
policy, and under flexible, state-dependent discount schemes. Given a policy  : S × A  [0, 1], a (possibly multi-dimensional) cumulant function C : S × A × S  RK and a continuation function  : S  [0, 1], the associated generalized value function (GVF) vC,, : S  RK predicts the
expected cumulant-based return, discounted by the continuation function:

vC,,(s) = E

 t=0

t i=1

(Si

)

Ct

S0 = s, A0:   .

Note that this definition extends easily to predictions associated with different initial conditions (e.g. for pairs (s, a), rather than just states).

Usual value functions are trivially GVFs vR,,, where the cumulant is simply the MDP reward function, and the discount factor is constant across the state space. Similarly, when the cumulant C
depends on state-action pairs, the corresponding GVF is equivalent to the state action Q-values.

In the following sections, we will explain how we can use GVFs to express different RL algorithms, gaining insight both into their behavior, as well as into paths for their possible improvement.

3 GVFS FOR SUCCESSOR FEATURES

Successor representation. Under any distribution over the state space, as induced by a fixed pol-
icy, one can define a prediction for the discounted number of times a state s is visited. That is, we predict the one-hot cumulant I(s) such that Ct = 1 if St = s and 0 otherwise1. For a fixed starting state s0, the associated prediction defines a measure over the state space S: µ(s|s0, , ). Dayan (1993) introduced the idea of using this representation of states through their possible futures,
called successor representation, as a way to provide ideal features This becomes useful in describing predictions about other cumulants C: vC,,(s0) = Cdµ(·; s0, , ) (see appendix for proof).

Successor features Successor features (SF) (Barreto et al., 2017) generalize the idea of the suc-

cessor representation to the case of function approximation in a natural way. Given a function

 : S × A × S  RK defining feature representations for s  a  s transitions, the SF vector is

defined as

(s, a; ) = E [

 t=0

t(St,

At+1,

St+1)|S0

=

s,

A1

=

a,

]

.

If the reward function is linear in the features (i.e. R(s, a, s ) = (s, a, s )T w for some weight vec-

tor w), SFs become very useful in the context of transfer learning, where different tasks correspond

to different w. Once SFs for a given policy are computed, they can be used for zero-shot policy

evaluation, and in generalized policy improvement.

SF (Barreto et al., 2017) are naturally formulated as a GVF v,,, where  is a multi-dimensional cumulant and  is constant A key result on SFs Barreto et al. (2017) can then be expressed and
generalised immediately in the language of GVFs as follows:
Proposition 1. For a fixed pair (, ), given a cumulant C : S × A × S  Rk, and w  Rk, vwT C,,(s) = wT vC,,(s).

This property is exploited in Barreto et al. (2017) for transfer learning. Given a new task corresponding to scalar cumulant C : S × A  R, the value of any given policy can be evaluated on this new task by approximating C with the L2 projection on the space spanned by a multi-dimensional cumulant C. That is, given wC := arg minw(wT C -C )2, one can estimate vC ,,(s)  wCT vC,,(s).
We will use the GVF version of SF in our experiments.

1Similar one-hot cumulants can be defined for (s, a) pairs or s  a  s transitions and derive the corresponding measure over spaces S × A and S × A × S, respectively.

2

Under review as a conference paper at ICLR 2019

R  () s, a GVF

R  ()

 log ()
×  ()
s0 GVF
 v
(a) The policy gradient algorithm describes the interdependence between the GVFs corresponding to the evaluation of a policy of a specific task (i.e. R) and the gradient of that evaluation with respect to the policy parameters (i.e. v)

C1 

C2 

 

...

 

Ck

 s0

() GVF

SF1 

SF2 

 

...

 

SFk

s, a GVF

 log ()

C1 

[w1, w2, · · · , wk]

×

C2 

 

...

 



×

Ck

×  v

(b) The GVFs involved in the computation of policy gradients can be naturally decoupled by using successor features to approximate the second GVF. The policy is evaluated on a set of cumulants (i.e. features) to compute successor features, which are then used to approximate the gradient v (see Proposition 1).

Figure 1: GVFs for Policy Gradient Methods

4 GVFS FOR POLICY GRADIENT METHODS
Performance gradient estimation techniques of score function and likelihood ratio, as introduced by Aleksandrov et al. (1968) and Rubinstein (1969), are based on sampling trajectories to estimate both the value of a policy and the gradient used for improving the policy. Given a score function f that depends on a random variable sampled from a parameterised distribution q(), the gradient with respect to the parameters is given by EXq()[f (X)] = EXq() [f (X) log q(X; )]. This principle is used in several popular RL algorithms, including REINFORCE (Williams, 1992), actor-critic (Sutton, 1984), policy gradient, eg. (Sutton et al., 1999a; Konda, 2000; Bhatnagar et al., 2009; Lillicrap et al.; Schulman et al., 2015) and other algorithms that use an estimate of the value function to compute the gradient of the policy parameters. The policy gradient theorem (Sutton et al., 1999a) provides a formal description of the procedure used in this broad class of algorithms.
We are now going to re-express this result in the language of GVFs, by re-casting the gradient computation in terms of an explicit cumulant.
Theorem 1. Let  be a policy parameterised by a vector ,  be a constant continuation function, and C : S × A  R be a one-dimensional cumulant. The gradient of the general value function vC,, (s) with respect to  is itself a general value function that depends on the cumulant:
C^(s, a) := vC,, (s, a) log (a|s).
That is, vC,, (s) = vC^,, (s).
Figure 1a provides a visual depiction of this result and the appendix contains the proof.
The policy gradient theorem shows that these widely used methods are based on the estimation of two inter-dependent GVFs: the cumulant for one GVF is obtained as the output of another GVF. Note that this separation means we can easily consider other ways of estimating these cumulants.
4.1 BOOTSTRAPPED POLICY GRADIENT
To illustrate the intuition that we gain from Theorem 1, we first run a procedure that is designed to follow the analytic description of the policy gradient: given policy parameters , (1) generate a number of rollouts from random initial states (uniformly sampled over the entire state space); (2) use TD() on the sampled data to compute an estimate for the value function vR,, ; (3) compute cumulants C^ in Theorem 1 based on the estimates in (1); and (4) generate a separate set of rollouts to estimate the GVF vR,, , using the same TD() procedure. We use TD() to estimate GVFs since it has the flexibility of reusing previous estimates (i.e., bootstrapping).

3

Under review as a conference paper at ICLR 2019
(a) This plot compares the best combination of hyper-parameters for each algorithm ­ each plot corresponds to a fixed number N of rollouts for estimating GVFs.
(b) Plots corresponding to individual hyper-parameter configurations illustrate the increased stability of the bootstrapped policy gradient approach when gradients are approximated with successor features. Figure 2: Policy gradient updates with TD() estimates. L1 norm of the exact value function vs. number of updates to policy parameters. We ran all versions of the algorithm over different eligibility trace parameters , constant learning rates for the value function td, and constant learning rates pg for policy parameters. For performance on various hyper-parameter settings, see Figure 6 (in Appendix). Mean and variance for the value of the policy as a function of policy updates are shown in blue when only the critic value is bootstrapped, in red when all GVF estimates are bootstrapped, and green when successor features are also used.
4.2 USING SUCCESSOR FEATURES FOR POLICY GRADIENT Theorem 1 provides a natural linear approximation to the gradient v. Given a finite number of auxiliary predictions (i.e. cumulants {Ci}in=1) and a linear approximation to an input cumulant C^  wiCi, we can compute vC^,,  wivCi,,, using the successor features idea (see Figure 1b for a corresponding illustration). The intuition is that successor features would help in tracking the continual change in cumulants that is a natural by-product of policy gradient. This should help when the gradient estimates are unstable; sampled data would be used to estimate the long-term accumulation of a fixed set of features instead of a set of parameter gradients.
4.3 EMPIRICAL ILLUSTRATION In Figure 2a, we illustrate the importance of bootstrapping not only the critic value vC,, but also in any General Value Functions involved in the computation of the gradient with respect to the policy parameters. All algorithms were run on a simplified version of the four-room environment described in (Sutton et al., 1999a), with discount factor set to 0.9, and the feature map used for linear value function approximation was computed using tile-coding (Sutton & Barto, 1998) (exact details of the experiments are in the appendix). Using small domains allows us to compute the exact value functions, and hence to compare the algorithms against ground truth. As can be seen, the proposed bootstrapped policy gradient approach provides significant improvement in data efficiency. While the advantage of bootstrapping compared to Monte Carlo estimation has been observed before, the effect is magnified through the use of multiple GVFs.
4

Under review as a conference paper at ICLR 2019

Figure 3: Actor-Critic with bootstrapped gradient estimates. L1 norm of the exact value function vs. number of rollouts used for policy updates. Top: four-room maze introduced in (Sutton et al., 1999a))( 60 states and  15 total features). Bottom: large maze grid-world with sparse reward ( 3000 states and  90 total features). Each plot corresponds to a fixed number of steps L taken by
the agent for every rollout, The plots displayed here correspond to the highest performing hyperparameter setting: A is the learning rate for updates to the policy parameters, C is the learning rate for updates to the critic component, and G is the learning rate for updates to the gradient estimates. For performance on specific hyperparameter setting, see Figure 7 and 8 (in Appendix). Mean and
variance for the value of the policy as a function of policy updates are shown in blue for baseline
actor-critic, in green when actor-critic maintains bootstrapped estimates for parameter updates.

We tested the approach with the same tile-coding features as used for value function approximation. The results shown in Figure 2b demonstrate that estimating policy gradients using bootstrapped successor features exhibits much stronger robustness to changes in hyper-parameters.

Algorithm 1: Actor-Critic with GVFs for gradient estimation.

Input: policy (a|s, ); critic state-value fn. v^(s, w); general-value fn. g^(s, ).

Algorithm parameters: step size  > 0, w > 0, and  > 0.

repeat

Initialize S (first state of episode); I  1

repeat

A  (a|s, ) and take action A, observe S , R

crit  ww

R +

+ v^(S , w I crit

w) w

- v^(S, w) v^(S, w)

grad  crit log (A|S, ) + g^(S , ) - g^(S, )

   + Igradg^(S, )

   + g^(S, )

I  I and S  S

until end of episode

until convergence

Next, we tested the approach in the context of Actor-Critic (Konda, 2000), an approach commonly used to scale the policy gradient algorithm to large or continuous environments. We modified the actor-critic algorithm in (Konda, 2000) to both the value function corresponding to the MDP reward function (i.e. the critic) and the gradients w.r.t. to the policy parameters. That is, just as the original actor-critic algorithm uses parameters w to approximate bootstrap values for the critic function (i.e. vR,,(s)  v^w(s), we use a similar approximation for the GVF corresponding to policy gradient
5

Under review as a conference paper at ICLR 2019

R Q(·, 0)

C ( )

(1 - ())

()

Q(·, 1) ...
Q(·, k)

s0 GVF Q(s0, )

(a) GVFs for Option-value function

R M (W, ) R + RI (g(M)) W (g(M), )

s GVF

s GVF

vM
(b) GVFs for Feudal Networks.

vW

Figure 4: Left: Every option-value corresponds to a separate GVF, concerned both with external
reward R, as well as all other GVFs. Right: In FuN, manager (M) and worker (W) are trained using separate GVFs: vM, concerned with external return as corresponding to W's policy; vW,
conditioned on goals specified by M.

estimates (i.e. vC^,,(s)  g^(s, )). By doing so, we replace updates to the policy parameters that consider only  log  · Q with long term estimates of the GVFs derived from the analytic form of the policy gradient. These estimates contain gradient information from past gradient computations (i.e. they are bootstrapped). See Algorithm 2 in the appendix for details on the proposed procedure and other implementation details. See Figure 3 for a comparative analysis of the baseline actorcritic algorithm and the aforementioned modification; the results clearly demonstrate the benefit of a principled approach in estimating GVFs corresponding to policy gradients in an end-to-end algorithmic framework.

5 GVFS FOR HIERARCHICAL REINFORCEMENT LEARNING

Hierarchical reinforcement learning refers to a collection of methods which aim to structure the policy of an agent into "chunks" which have a temporal extent and can be re-used for different tasks. A large variety of methods have been proposed, including options (Sutton et al., 1999b; Precup, 2000), MAXQ (Dietterich, 2000), feudal architectures (Dayan & Hinton, 1993; Vezhnevets et al., 2017) and others. We will now illustrate how GVFs can be used in a natural way to analyze and extend such approaches. We will focus on the options framework and on feudal architectures, since these two approaches have recently been shown to yield algorithms that can learn the hierarchical structure from scratch (Bacon et al., 2017; Vezhnevets et al., 2017).

GVFs for options An option  is defined as a tuple containing a set of states in which the option can be initiated, an internal policy  : S × A  [0, 1], which is used to pick actions when the option is executing, and a termination function  : S  [0, 1]. As in other recent work (Bacon et al., 2017), we ignore the initiation set in this paper. The typical execution model for options uses a policy over options, , which chooses options from a set . Whenever an option is picked, its policy chooses actions until the termination condition is met. Note that at a state s, an option  will continue with probability 1 - (s).
Options also have associated reward and transitions models (Sutton et al., 1999b). For example, the reward model of an option is given by:

r(s, )

=

E[

 t=0

tRt+1|St

=

s, t

=

]

= E[R(s, At+1) + (1 - (St+1))r(St+1, )|St = s, At+1  (s, ·), St+1  p(·|s, At+1)]

It is easy to see then that r(s, ) can be written as a GVF which has as cumulant the environment reward R, the continuation function is (1 - (s)) and the policy is . A similar development can be done for option transition models.

Another important quantity in the context of options is the option-value function, Q (s, ), which generalizes the action-value function, and is used to drive option construction algorithms such as
option-critic. Indeed, the option-critic architecture (Bacon et al., 2017) uses a policy gradient
algorithm to learn the internal policies and termination functions for a fixed number of options  = {1, · · · , k}. We now show that the option-value function can be expressed as a GVF.

6

Under review as a conference paper at ICLR 2019

Proposition 2. Given a set of options  and a fixed policy over options , define the cumulant C for option  as C(s, a, s ) = R(s, a) + (s )E [Q(s ,  ) |   ( · |s )]. Then:
Q (s, ) = vC,(1-), (s, ).
The proof is in the appendix. Figure 4a illustrates the statement. The GVF view highlights the fact that the option-value function depends both on a short term signal coming from the reward and a long-term signal summarizing the performance of other options. This leads to a very hard task, since each option has to be aware of the performance of the rest of the options. In the option-critic framework, this can lead to degenerate solutions, e.g. when each option considers itself less qualified than the others and wishes to cede control immediately (which leads to all options collapsing to singlestep actions), or when one option decides it is better than the others, and hence never terminates and tries to learn the optimal policy internally. Note that the option models, on the other hand, achieve a separation of concerns naturally. Hence, in the context of transfer learning, option models may be more useful than option-value functions.

GVFs for feudal networks Feudal Networks (FuN) (Vezhnevets et al., 2017) is an HRL approach designed to provide an explicit separation of concerns between levels of hierarchy. It uses a network which consists of two parts. The top level, or the Manager M, chooses goals g(M) at a slower time scale in a latent state-space that is itself learned. The lower level, or the Worker W, operates at a faster time scale and produces primitive actions, conditioned on the goals received from the Manager. The Worker is motivated to follow the goals by a combination of intrinsic and extrinsic reward. No gradients are propagated between W and M; M receives its learning signal from the environment alone, adjusting g(M) to maximize the extrinsic reward using transition policy gradient (Vezhnevets et al., 2017).
We now show how GVFs can be used to provide a general version of the transition policy gradient introduced in Vezhnevets et al. (2017).
Proposition 3. Let z  Z  Rk represent some latent variables used to model a hierarchical policy parameterised as: (a|s) = (a|s, z; 2)d(z|s; 1). Let v be a set of random variables corresponding to predictions about . Given continuation function  and cumulant C : S×A×Z  R, 1 vC,,(s) = vC^1,,(s) and 2 vC,,(s) = vC^2,,(s) with
C^1,t = vC,,(st, zt, vt)1 log (zt, vt|st; 1), and C^2,t = vC,,(st, at, zt)2 log (at|st, zt; 2).
The proof is in the appendix. This result allows the manager in FuN to move from fixed temporal difference in state representations (e.g. vt := st+c), to any general prediction conditioned on . It also shows that the transition policy gradient can be used for broader choices of latent attributes than those used in Vezhnevets et al. (2017), where log (zt, vt) was proportional to the cosine distance between (st+c - st) and zt.
When vC,,(s, a) in Prop. 3 is independent of z, low-level action choices that ignore z (i.e. (a|s, z; 2) = (a|s; 2)) end up as valid local minima for the corresponding optimisation problem. One way to avoid this kind of degeneracy is to use intrinsic rewards corresponding to some notion of alignment between the observed trajectory and z (Dayan & Hinton, 1993; Barto et al., 2004; Kulkarni et al., 2016; Baranes & Oudeyer, 2013). Alternatively, one could use Universal Value Functions as GVFs, consider latents z that determine specific goals (i.e. (C, ) pairs) and train low-level controllers to achieve those goals (Schaul et al., 2015).

5.1 EMPIRICAL ILLUSTRATION

To illustrate the ideas above, we have used the following GVFs: v := vC,^,, with Ct = st+c - st

and some discount factor ^ possibly different from the discount of the GVF vR,, used for eval-

uating the agent on the given task. We assumed a von Mises-Fisher distribution for the higher-

level policy: (zt, vt|st)  exp(dcos(zt, vt)). The intrinsic reward for the low-level compo-

nent was chosen as : rtI =

t-c j=0

^ 

j (st-st-c)T zt-c-j vt-c-j zt-c-j

.

See Appendix for details on an effi-

cient recursive computation of this reward and a proof that this corresponds to an intrinsic return

RI =

 i=0



i

log

(vi,

zi|si).

We also use successor features (SF) for GVFs.

We conducted a

number of experiments in the ATARI games (Bellemare et al., 2013) for which the original FuN

7

Under review as a conference paper at ICLR 2019
Figure 5: Training curves for FuN with high-level choices over GVFs in ATARI games (Bellemare et al., 2013). In Seaquest and Enduro, agents for which the manager uses successor features consistently outperform the baseline algorithm. In Gravitar and Ms. Pacman, the proposed modifications have no impact on performance. See appendix for more results.
architecture was shown to exhibit substantial empirical improvements over corresponding network structures (Vezhnevets et al., 2017). The experimental setup and all the hyper-parameters were set to those used in the original paper on FuN (Vezhnevets et al., 2017). The results in Figure 5 show that using GVFs either reproduces the results in baseline FuN or improves the learning speed.
6 OTHER RELATED WORK
Schaul et al. (2015) address the issue of learning a large number of GVFs individually, which is not scalable and misses out on learning shared feature structure among different GVFs. They propose that an agent may want to focus on learning a special type of GVF, which is optimal with respect to a specific minimum-time-to-goal based task. Such a task can be specified using a (C, ) pair, in which C is positive at the goal state(s) and 0 otherwise, and  < 1 is constant at all states except the goal, where it is 0. In general, however, we could have GVFs that are optimal with respect to any cumulant and continuation, which we can be denoted as vC ,. Note that, if C is multidimensional, additional requirements such as having a bounded output and an ordering on the co-domain of C are necessary in this case. In Universal Value Functions (UVFs), the task (C, ) can be summarized through a goal state g, which can then be presented as an input, to allow an agent to generalize between tasks. Hence, in this approach, a shared approximator v(s, g) is estimated from data. Synthetic gradients (Jaderberg et al., 2016) allow a learner to estimate directly a gradient for certain input data points, without backpropagation. The idea is to use a secondary function approximator for this estimation. The policy gradient approach that we presented is similar in spirit, as we use a separate GVF as a gradient estimator. Further connections are left for future investigation. Auxiliary tasks Jaderberg et al. (2017) are an approach which uses secondary reward functions in order to learn a better internal representation for an RL agent. The idea of using many GVFs, corresponding to many different cumulants, is very much in the same spirit.
7 CONCLUSION AND FUTURE WORK
We proposed to use GVFs as a unifying language for describing several popular reinforcement learning algorithms. This approach provides interesting connections between algorithms and helps us understand certain pathological behaviors (such as the collapse encountered in certain temporal abstraction algorithms). Moreover, we can use the GVF framework in order to generate new algorithms, by making different choices for how these functions are encoded. We illustrated this ability by generating new versions of policy gradient and of feudal networks. In the case of policy gradient, the proposed approach, which uses bootstrapping as well as successor features, greatly improves learning speed and stability. However, more experience is needed with this algorithm, especially in conjunction with function approximation. GVFs can enable a natural injection of domain knowledge, as the designer of the system can specify cumulants and continuation functions, then let the system learn about all of them. We will investigate this approach in future work. GVFs promise to enable life-long, incremental learning, as an agent could add new GVFs to its repertoire over time, perhaps following a meta-learning or curriculum learning approach.
8

Under review as a conference paper at ICLR 2019
REFERENCES
V. M. Aleksandrov, V. I. Sysoyev, and V. V. Shemeneva. Stochastic optimization. In Engineering Cybernetics, 1968.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, 2017.
Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49­73, 2013.
Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. In NIPS, 2017.
Andrew G. Barto, Satinder Singh, and Nuttapong Chentanez. Intrinsically motivated learning of hierarchical collections of skills. In Development and Learning, pp. 112­119, 2004.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253­279, jun 2013.
Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural Actor-Critic Algorithms. Automatica, 45(11), 2009.
Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 1993.
Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In NIPS, 1993.
Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decomposition. J. Artif. Intell. Res.(JAIR), 2000.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. 2016. URL http: //arxiv.org/abs/1608.05343.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In ICLR, 2017.
Vijaymohan Konda. Actor-critic algorithms. In NIPS, 2000.
Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, and Joshua B. Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In NIPS, 2016.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. URL http://arxiv.org/abs/1509.02971.
Joseph Modayil, Adam White, and Richard S. Sutton. Multi-timescale nexting in a reinforcement learning robot. Adaptive Behavior, 2014.
Doina Precup. Temporal abstraction in reinforcement learning. PhD thesis, University of Massachusetts, 2000.
R. Y. Rubinstein. Some Problems in Monte Carlo Optimization. Ph.D. thesis, 1969.
Tom Schaul and Mark B Ring. Better Generalization with Forecasts. In IJCAI, 2013.
Tom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal value function approximators. In ICML, 2015.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, pp. 1889­1897, 2015.
9

Under review as a conference paper at ICLR 2019

Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Richard S Sutton and Brian Tanner. Temporal-difference networks. In NIPS, pp. 1377­1384, 2005.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, 1999a.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 1999b.
Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In AAMAS, 2011.
Richard Stuart Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University of Massachusetts Amherst, 1984.
Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In ICML, 2017.
Adam White. Developing a predictive approach to knowledge. PhD thesis, University of Alberta, 2015.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229­256, 1992.

A PROOFS

Proposition. Let µ be the measure corresponding to predictions on one-hot cumulants, as defined in Section 3. For every General Value Function, v(s0; C, , ) = Cdµ(·; s0, , ).

Proof. WLOG, assume C is defined over state-action pairs, i.e C : S × A  R and fix a starting state s0. Let S0, A0, S1, A1, ·, Sn, An, · · · be the random trajectory generated when following policy  from initial state S0 = s0 and using (Si) as a Bernoulli process that determines the termination of a trajectory. Following the definition of general value functions (see Section 2),

Cdµ(·; s0, , ) = C(s, a)vI(s,a),,(s0)
s,a

t

= C(s, a)E

(Si) I((St, At) = (s, a)) S0 = s0, A0:  

s,a t=0 i=1

t

=E

(Si)

C(s, a)I((St, At) = (s, a)) S0 = s0, A0:  

t=0 i=1

s,a

t

= E (Si) Ct S0 = s0, A0:   = v(s0; C, , )

t=0 i=1

Theorem. Sutton et al. (1999a). (Policy gradient theorem) Let  be a policy parameterised by a vector ,  be a constant continuation function, and C : S × A  R be a one-dimensional cumulant. The gradient of the general value function vC,, (s) with respect to  is itself a general value function that depends on the cumulant:
C^(s, a) := vC,, (s, a) log (a|s).
10

Under review as a conference paper at ICLR 2019

Proof. (sketch) Recall from Section 3 that v(s; C, , ) = Cdµ(·; s, , ) and vC,,(s, a) = Cdµ(·; s, a, , ). The derivation below follows,

vC,,()(s0) = = =

aA vC,,(s, a)(a|s; )dµ(s; s0, , )

aA (a|s; )

vC,,

(s,

a)

 (a|s;) (a|s;)

dµ(s; s0, , )

vC,,(s, a) log (a|s; )dµ(s, a; s0, (), ).

Proposition. Fix a set of options  and a meta-policy  and define the cumulant C() for every option  as Ct() = R(St, At) + (St+1; )E Q(St+1,  ) |    St+1)]. Then, Q(s0, ) = v(s0; C(), (1 - ()), ()).

Proof. Let V (s) := E[Q(s, )|  ]. Based on the definition provided in Bacon et al. (2017),

Q(s0, ) = (a|s0) R(s0, a) +  P (s |s0, a) [(1 - (s ; )Q(s , ) + (s ; )V (s )]

aA

s S

= (a|s0) R(s0, a) +  P (s |s0, a)(s ; )V (s ) +

aA

s S

+ (a|s0) P (s |s0, a)(1 - (s ; )Q(s , )

aA

s S

= E [C(s0, A, S ; ) | A  (s0); S  P (·|s0, A)]

+ E [(1 - (S ; ))Q(S , ) | A  (s0); S  P (·|s0, A)] .

Note that the statement above is equivalent to the Bellman equation for General Value Functions, where the cumulant is C() and continuation function is (1 - ()). Since the Bellman equation
is known to have a unique solution, the statement of the proposition follows as a direct consequence.

Proposition. Assume latent attributes z are elements of Z  Rk. Let 1 and 2 be two sets of parameters used to model distributions (z|s; 1) and (a|s, z; 2), correspondingly. These models describe the low-level action choice (a|s) = (a|s, z)d(z|s). Let  be a constant continuation function, and C be any cumulant defined on S × A × Z  R. Additionally, let v be a set of random variables corresponding to predictions about the policy . Then 1 v(s0; C, , ) = v(s0; C^1, , ) and 2 v(s0; C, , ) = v(s0; C^2, , ) with
C^1,t = v(st, zt, vt; C, , )1 log (zt, vt|st; 1), and C^2,t = v(st, at, zt; C, , )2 log (at|st, zt; 2).
Proof. The statement is a corollary to Theorem 1, where the gradient with respect to the flat policy is:
1 (a, z, v|s; 1, 2) = 1 (a|z, s; 2)(z, v|s; 1) = (a|z, s; 2)1 (z, v|s; 1) = (a, z, v|s; 1, 2)1 log (z, v|s; 1)
2 (a|s; 1, 2) = 2 (a|z, s; 2)(z, v|s; 1) = (z, v|s; 1)2 (a|z, s; 2) = (a, z, v|s; 1, 2)2 log (a|z, s; 2)
11

Under review as a conference paper at ICLR 2019

Now, when computing gradients,

2 v(s0; C, , ) =

v(s, a, z, v; C, , )2 (a, z, v|s; 1, 2)dµ(s; s0, , )

a,z,v

= v(s, a, z, v; C, , )(a, z, v|s; 1, 2)2 log (a|z, s; 2)dµ(s; s0, , )
a,z,v

= v(s, a, z; C, , )(a, z|s; 1, 2)2 log (a|z, s; 2)dµ(s; s0, , )
a,z

= v(s, a, z; C, , )2 log (a|z, s; 2)dµ(s, a, z; s0, , )
= v(s0; C^2, , )
A similar results can easily be derived for 1 to obtain the gradient updates described in the Proposition.

Proposition 4. Let st  RK represent the observation at time t and zt  RK be the latent attributes used to model the hierarchical policy described in Section ??. Define the intrinsic reward rtI at time

t to be

rtI =

t-c j=0

.^ j (st-st-c)T zt-c-j
 vt-c-j zt-c-j

(1)

Given a and RI

=sequet=nc0estr0tI,

z0, =

s1

,iz=10,·

·
i

· , sT , zT , the intrinsic log (vi, zi|si), where

rewards rtI can be computed log (vi, zi|si) is assumed to

iteratively, follow von

Mises-Fisher distribution.

Proof. Note that this reward value can be efficiently computed using the concept of a running sum.

Define the vector mt at every timestep to be mt :=

t-c j=0

^ 

j

zt-c-j vt-c-j zt-c-j

; it is not hard to

check that this can be computed iterative using the following update: mt :=

zt-c vt-c zt-c

+

^ 

mt-1

.

From this, computing the intrinsic reward amounts to rtI = (st - st-c)T mt.

Apart from being computationally convenient, the intrinsic reward in Equation 1 has a corresponding return which is maximized when the latent values z align with predictions v. That is,

  t-c
RI = trtI = t
t=0 t=0 j=0

^ j (st - st-c)T zt-c-j  vt-c-j zt-c-j

=  t-c t-j ^j (st - st-c)T zt-c-j

t=0 j=0

vt-c-j zt-c-j

=

  i+c^j (si+j+c - si+j )T zi

i=0 j=0

vi zi

= c


i log (vi, zi|si).
i=0

B ACTOR-CRITIC WITH GVFS FOR GRADIENT ESTIMATION
Algorithm 2 describes the modifications we hade made to the Actor-Critic algorithm (Konda, 2000) in order to illustrate the benefits of bootstraping not only the values of the critic function, but the general value functions involved in the computation of policy gradients.
To generate the results presented in Figure 3, we first defined a tile-coding feature map  : S  RK (Sutton & Barto, 1998). The policy is parameterized with a vector   RK×|A| and the critic values with w  RK using the following functional forms: v^(s, w) = wT (s) and (a|s, ) = Softmax(aT (s)). For pol-
12

Under review as a conference paper at ICLR 2019

icy gradients estimation, we used parameters   RK×|A| and g^(s, ) = T (s).

Algorithm 2: Actor-Critic with GVFs for gradient estimation.

Input: policy (a|s, ); critic state-value fn. v^(s, w); general-value fn. g^(s, ). Algorithm parameters: step size  > 0, w > 0, and  > 0.

repeat Initialize S (first state of episode); I  1

repeat

A  (a|s, ) and take action A, observe S , R

crit  ww

R +

+ v^(S , w I crit

w) w

- v^(S, w) v^(S, w)

grad  crit log (A|S, ) + g^(S , ) - g^(S, )

   + Igradg^(S, )

   + g^(S, )

I  I and S  S

until end of episode

until convergence

C DETAILS OF GVF-FUN EXPERIMENTS
We follow the same experimental procedure described in (Vezhnevets et al., 2017), where hyperparameters are tuned using randomized search on the following domain: the learning rate for the policy gradient updates is sampled from Log-Uniform(10-4.5, 10-3.5); the entropy penalty was sampled from Log-Uniform(10-4, 10-3); the weight of the intrinsic reward was sampled from Uniform(0, 1). Moreover, to increase stability, the reward is clipped in [-1, 1].
For the purpose of illustrating the concept of separation of concerns, the worker and the manager were trained with respect to GVFs of different discount values: manager's discount M = 0.99 and worker's discount W = 0.95.
We compare baseline FuN with a modified network where the goal g picked by the manager corresponds to the latent z in Proposition 3 and the manager is trained with transition policy gradient gt = AtM dcos(vt, gt()). Since the prediction horizon of the baseline FuN agent is set to c = 10, we compute successor features vt using a discount value  = 0.8 to make sure that the corresponding temporal resolution is essentially the same2. The intrinsic reward corresponding to specific goals set by the manager was computed using Equation 1. Since our intent was to test the hypothesis that a Feudal Network would be capable of reasoning over successor features, all other components were kept invariant.
See Figure 9 for results for all the games for which we have ran the experiments. Specifically, the plots in the figure provide the average and standard deviations over 5 different runs of the algorithm, where each run was based on a randomized search picking the top performing hyper-parameter setting among 20 samples, and the policy gradient algorithm is performed over 80 epochs3.

2 When a GVF is using a constant discount value  and no explicit horizon length is used, rewards obtained after log become numerically insignificant (i.e.  Rmax).
3A training epoch is equivalent to 106 steps.
13

Under review as a conference paper at ICLR 2019

Figure 6: Policy gradient updates with TD() estimates. L1 norm of the exact value function vs.

number of updates to policy parameters. Each plot corresponds to a fixed number N of rollouts for

estimating GVFs, a fixed  as eligibility for TD(), fixed learning rate for td, and fixed learning

rate pg for policy parameters. Mean and variance for the value of the policy as a function of

policy updates are shown in (blue) when only the critic value is bootstrapped, in (red) when all GVF

estimates are algorithm.

bootstrapped,

and

(green)

when

su1cc4essor

features

are

used

to

stabalized

the

learning

Under review as a conference paper at ICLR 2019
Figure 7: Actor-Critic with bootstrapped gradient estimates ­ results on four-room maze (introduced in (Sutton et al., 1999a)) L1 norm of the exact value function vs. number of rollouts used for policy updates. Each plot corresponds to a fixed number of steps L taken by the agent for every rollout, the learning rate A for updates to the policy parameters, a learning rate C for updates to the critic component, and a learning rate G for updates to the gradient estimates. Mean and variance for the value of the policy as a function of policy updates are shown in (blue) for baseline actor-critic, in (green) when actor-critic maintains bootstrapped estimates for parameter updates.
15

Under review as a conference paper at ICLR 2019
Figure 8: Actor-Critic with bootstrapped gradient estimates ­ results on large maze grid-world with sparse reward. L1 norm of the exact value function vs. number of rollouts used for policy updates. Each plot corresponds to a fixed number of steps L taken by the agent for every rollout, the learning rate A for updates to the policy parameters, a learning rate C for updates to the critic component, and a learning rate G for updates to the gradient estimates. Mean and variance for the value of the policy as a function of policy updates are shown in (blue) for baseline actor-critic, in (green) when actor-critic maintains bootstrapped estimates for parameter updates.
16

Under review as a conference paper at ICLR 2019
Figure 9: Training curves for FuN with high-level choices over GVFs. Results on ATARI games (Bellemare et al., 2013) demonstrate gains in data-efficiency for some ATARI games. In games such as Seaquest, Alien, and Enduro, agents for which the manager uses successor features consistently outperform the baseline algorithm. In Gravitar, Amidar and Ms. Pacman, the proposed modifications have no impact on performance. In Frostbite, both versions of the algorithm are usually stuck in a local minimum (average reward  250), and the noticeable improvement is due to one of the agents luckily getting out of the local minimum for a limited number of steps.
17

