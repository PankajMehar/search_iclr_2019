Under review as a conference paper at ICLR 2019
UNSUPERVISED GRAPH EMBEDDING USING DYNAMIC ROUTING BETWEEN CAPSULES
Anonymous authors Paper under double-blind review
ABSTRACT
An important task in learning representations for graph-structured data is to learn node embeddings which are then used for node classification. Recent models, however, suffer from limitations in exploiting graph information in such a way that relative positions of nodes are preserved. In this paper, we propose a novel unsupervised embedding model, named CapsG, which is, to our best of knowledge, the first model using dynamic routing between capsules to overcome these limitations. Our CapsG is constructed with two capsule layers, wherein the first layer aims to encapsulate the raw features of nodes, while the second layer produces a vector output used to infer node embedding. Experimental results show that our proposed CapsG produces new state-of-the-art results on CORA, CITESEER and POS, and obtains very competitive results on PUBMED, PPI and BLOGCATALOG in comparison with existing state-of-the-art unsupervised and semisupervised graph embedding models.
1 INTRODUCTION
Graph-structured data appears in numerous fields in our real world from social networks, citation networks, knowledge graphs and recommender systems to telecommunication networks, biological networks (Hamilton et al., 2017b; Battaglia et al., 2018; Cai et al., 2018; Chen et al., 2018). Over these domains, there have been many successful applications such as building knowledge graphs in databases and information systems (Suchanek et al., 2007; Bollacker et al., 2008); performing text classification (Yang et al., 2016; Kipf & Welling, 2017); predicting traffic in road (Cui et al., 2018); learning physics engines (Sanchez-Gonzalez et al., 2018); advertising and recommending items to users (Ying et al., 2018; Wang et al., 2018b).
In graph-structured data, nodes represent individual entities; edges represent relationships and interactions among those entities. There have been many models proposed to embed each node into an embedding, as summarized in (Zhang et al., 2018). These embeddings can be further fed into a classifier to identify labels of nodes. One of the most effective ways to learn node embeddings is to adopt the idea of word embedding model ­ Word2Vec (Mikolov et al., 2013a;b) ­ by viewing each node as a word and each graph as a text collection. Notably is DeepWalk (Perozzi et al., 2014) which firstly generates random walks starting from each node, and then uses each walk as a sequence of nodes for training a Word2Vec model. This method is then extended to Node2Vec (Grover & Leskovec, 2016) by introducing a biased random walk strategy which explores diverse neighborhoods, and balances between exploration and exploitation from a given node. LINE (Tang et al., 2015) extends Word2Vec by inducing the weighted importance of nodes, for which each node has a different weight to each of its neighbors, wherein weights can be pre-defined through algorithms such as PageRank (Page et al., 1999). DDRW (Li et al., 2016) jointly train a DeepWalk model with a L2-regularized Support Vector Classification (Fan et al., 2008) in a semi-supervised manner.
Recent research has raised attentions in applying deep neural networks to the node classification task (Wang et al., 2016; Yang et al., 2016; Duran & Niepert, 2017; Hamilton et al., 2017a; Kipf & Welling, 2017; Wang et al., 2018a; Velickovic´ et al., 2018). SDNE (Wang et al., 2016), an autoencoder-based semi-supervised model, is introduced to preserve the local and global graph structures. Another semi-supervised model is graph convolutional network (GCN) (Kipf & Welling, 2017) using a variant of convolutional neural networks (CNNs) which makes use of layer-wise propagation to exploit the features such as profile information and text attributes from the neighbors of
1

Under review as a conference paper at ICLR 2019
a given node. Existing approaches such as DeepWalk, SDNE and GCN require all nodes during training and do not consider unseen nodes. This is known as transductive setting where a model is trained using the entire input graph. Another important setup is the inductive setting wherein only a part of the input graph is used to train the model, and then the learned model is used to infer embeddings for unseen nodes (Yang et al., 2016).
Several attempts have been made for both transductive and inductive settings. Notably, EP-B (Duran & Niepert, 2017) is proposed to explore the embeddings of attributes (e.g., a bag of words) of nodes with their neighborhoods for inducing the embeddings of unseen nodes. GraphSAGE (Hamilton et al., 2017a) extends GCN in using node features and the neighborhood structures to generalize to unseen nodes. Graph attention network (GAT) (Velickovic´ et al., 2018) is another extension of GCN following LINE (Tang et al., 2015) in assigning different weights to different neighbors of a given node, but GAT learns these weights by exploring an attention mechanism technique (Bahdanau et al., 2014). However, as pointed out by Sabour et al. (2017), the convolutional neural networks (CNNs) are unsuccessful to effectively preserve the information about the relative positions of the input objects. This might concur to a problem in computer vision where the relative positions of objects such as eyes, nose and mouth in a facial image are important and should be captured properly. Therefore, because of using the variant of CNNs, it may be difficult for graph convolutional network (GCN)-based models such as GCN, GraphSAGE and GAT to avoid losing potential information, especially if it is in the inductive setting where neighbors of unseen nodes may also be unseen.
Inspired by the recent advanced capsule networks (Sabour et al., 2017), in this paper, we present CapsG ­ an unsupervised learning model that adapts dynamic routing between capsules to learn node embeddings. Our CapsG consists of two capsule layers with connections from the first to the second layer, but no connections within layers. The first layer uses groups of neurons as capsules to encapsulate corresponding nodes and also exploits features from context nodes for a given target node. The second layer has only one capsule used to aggregate the feature information from capsules in the first layer into a vector. This vector is then used to infer the embedding of the target node. The connections between two capsule layers are driven by the dynamic routing process in such a way that the model can automatically specify appropriate connections, and hence can encode the intrinsic spatial relationship between a context node and a target node constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints. This helps to preserve relative position information of nodes in the graph. From this point of view, our proposed CapsG resembles the unsupervised learning variant of graph attention networks. Our goal is to investigate whether dynamic routing between capsules can effectively capture the graph structures of data to obtain good performances for the node classification task in the transductive and inductive settings. In short, our main contributions are as follows:
· We introduce a novel use of dynamic routing between capsules to learn node embeddings in graph-structured data. Our proposed model CapsG can preserve the relative positions of nodes and solve the problem of losing potential information in GCN-based models.
· We conduct extensive experiments to evaluate the performance of our proposed CapsG on six benchmark datasets. The first three datasets containing input features for nodes are CORA, CITESEER and PUBMED (Sen et al., 2008). The experimental results show that our CapsG outperforms state-of-the-art unsupervised and semi-supervised methods on CORA and CITESEER, and obtains very competitive accuracies on PUBMED, in both the transductive and inductive settings.
· Three other benchmark datasets without node features ­ PPI (Breitkreutz et al., 2008), POS (Mahoney, 2011) and BLOGCATALOG (Zafarani & Liu, 2009) ­ are used for the transductive setting, and our CapsG also achieves very competitive performances, especially producing state-of-the-art Micro-F1 and Macro-F1 scores on POS.
2 THE PROPOSED CAPSG
A network graph G is defined as G = (V, E), in which V is a set of nodes and E  {(u, v)|u, v  V} is a set of edges. A graph embedding model aims to learn a node embedding ov for each node v  V. We consider the node classification task wherein the learned node embeddings are used as inputs for a classifier to categorize nodes into one or more classes.
2

Under review as a conference paper at ICLR 2019

Each node v  V is associated with a feature vector xv  Rd×1 representing node features if it is available. Otherwise, xv is randomly initialized and updated during training. We describe the general learning process of our proposed CapsG in Algorithm 1.

Input: A network graph G = (V, E), feature vectors {xv, v  V} and weight matrices Wi. for v  V do
xv  f (v) uv  squash (xv)
for v  V do Uniformly SAMPLE a number k of random walks of length q rooted by v.
for iteration = 1, 2, ..., n do for each random walk do SAMPLE a node v in the walk as a target. Cv  Context nodes of v  Remaining nodes. ev  DYNAMICROUTING ({uv , v  Cv}) USE ev to infer the embedding ov of target node v.
Algorithm 1: The CapsG learning process.

We firstly normalize each feature vector using a non-linear squashing function: u = squash (x) =

x2 x 1+ x 2 x

(Sabour et al., 2017). This ensures that the orientation of each feature vector is un-

changed while its length is scaled down to below 1, i.e., uv  Rd×1, v  V and uv < 1.

We then follow the DeepWalk (Perozzi et al., 2014) to uniformly sample a bunch of random walks of length q for every node in V. To construct a pair: (a list of context nodes, a target node), in each walk, we randomly sample a target node v and treat (q - 1) remaining nodes as the context nodes.
Figure 1 shows an example of a graph consisting of 6 nodes. If we uniformly sample a random walk of length q = 6 for node 1 such as {1, 2, 3, 4, 5, 6}, and select node 3 as a target node, thus remaining nodes {1, 2, 4, 5, 6} are treated as context nodes of node 3. We denote Cv be the set of context nodes of v, i.e., |Cv| = q - 1. It is to note that we sample a different index of target node in the random walk at each iteration.

Regarding our goal on investigating whether dynamic routing between capsules can effectively model the graph-structured data, we build our CapsG with two single capsule layers for a simplified architecture. In the first layer, we construct (q - 1) capsules, in which each feature vector of a context node is encapsulated by a corresponding capsule. In the second layer, we construct only one capsule which produces a vector output used to infer the embedding of the target node.

The first capsule layer consists of (q - 1) capsules, for which each capsule i  {1, 2, ..., q - 1} has a feature vector output ui  Rd×1. Vector outputs ui are multiplied by weight matrices Wi  Rk×d to produce vectors u^i  Rk×1 which are summed to produce a vector input s  Rk×1 to the capsule in the second layer. The capsule then performs the non-linear squashing function to produce a vector output e  Rk×1:

e = squash (s) ; s = ciu^i ; u^i = Wiui
i

(1)

where ci are coupling coefficients that are determined by the dynamic routing process as presented in Algorithm 2. ci helps to route the vector output ui of capsule i in the first layer to the capsule in the second layer. The vector output e is finally used to infer the embedding o of the target node.

We can see that context nodes are weighted to route to their target node. In another word, each capsule in the first layer has an individual perspective or focus on the context of the target node, in order to contribute feature information for inferring the embedding of the target node. The contribution of each context node can be represented by the length of the feature vector output of the corresponding capsule in the first layer. And the relative position of each context node to its target node can be represented by the orientation of the feature vector output. Our CapsG encodes the intrinsic spatial relationship between a context node and a target node constituting viewpoint invariant knowledge

3

Under review as a conference paper at ICLR 2019

for all capsule i  in the first layer do bi  0
for iteration = 1, 2, ..., m do c  softmax (b) s  i ciu^i e = squash (s) for all capsule i  in the first layer do bi  bi + u^i · e
Algorithm 2: The dynamic routing process is adapted from Sabour et al. (2017).

that automatically generalizes to novel viewpoints as mentioned by Sabour et al. (2017). Therefore, our CapsG model can preserve the relative positions of nodes and handle the problem of losing information in GCN-based models.
We illustrate our proposed model in Figure 1 where the length of random walks q is equal to 6, the dimension size of feature vectors d is equal to 4, and the embedding size k of nodes k is equal to 3. Thus, the first capsule layer has 5 capsules with 4 neurons for each, while the second capsule layer has 1 capsule with 3 neurons. For the target node 3 in the illustration, the vector output of the capsule in the second layer is used to infer the embedding of node 3.

1 2
43
56

5 capsules each with 4 neurons

u!
1 u"

1 capsule with 3 neurons

2 43

u# u$ u%

Dynamic rou1ng

e'

o'

5

Second capsule layer

6 First capsule layer

Figure 1: Processes in our proposed CapsG with q = 6, d = 4, k = 3 for an illustration purpose. Note that in this illustration, we use number subscripts to denote nodes themselves, not for indexes of nodes or capsules.

Learning parameters of CapsG: We learn our model's parameters including the weight matrices
Wi and node embeddings ov by minimizing the sampled softmax loss function (Jean et al., 2015) applied to target node v as follows:

LCapsG (v) = - log where V is a subset sampled from V.

exp(oTv ev) uV exp(ouTev)

(2)

Comparing CapsG with related works: If we do not use the dynamic routing process (i.e., the number of iterations in Algorithm 2: m = 1) and we use a same weight matrix W and fix the coupling coefficients ci = 1 for every capsule in the first layer, our CapsG can be seen as an extension of the GCN layer (Kipf & Welling, 2017) in an unsupervised manner using the non-linear squashing function with additional nodes at many hops away. If we do the dynamic routing process with the same weight matrix W, and the coupling coefficients ci can be seen as associated attention weights, thus in this specific case, CapsG can be seen as another kind of graph attention networks. Furthermore, it is important to note that our CapsG can directly aggregate feature information from nodes at many hops away, e.g., directly aggregating from context node 6 to target node 3. This helps us in

4

Under review as a conference paper at ICLR 2019

effectively inferring the embeddings of unseen nodes in the inductive setting. And this is different from previous works such as GCN and GraphSAGE where feature information from nodes at many hops away is indirectly aggregated by using multiple layers stacked on top of each other. This is also different from GAT since it just considers neighbors of a given node.

3 EXPERIMENTS

We evaluate our unsupervised CapsG using benchmark datasets with or without node features for the node classification task.

3.1 DATASETS

Statistic
|V | |E |
#classes #features (d) For each sampled time: #nodes in training #nodes in validation #nodes in test

CORA 2,708 5,429 7 1,433
140 1,000 1,000

CITESEER 3,327 4,732 6 3,703
120 1,000 1,000

PUBMED 19,717 44,338 3 500
60 1,000 1,000

POS 4,777 184,812 40 ­
­ ­ ­

PPI 3,890 76,584 50 ­
­ ­ ­

BLOGCATALOG 10,312 333,983 39 ­
­ ­ ­

Table 1: Statistics of the experimental datasets.

CORA, CITESEER and PUBMED (Sen et al., 2008) are given with node features, in which each dataset is a citation network where each node represents a document, and each edge represents a citation link between two documents. Each node has a class label representing the main topic of the document and a feature vector of a bag-of-words. The task is to use the learned node embeddings to classify each node into a class. To have a fair comparison, we adopt identical settings used by Duran & Niepert (2017), for each of these three datasets, we uniformly sample 20 random nodes for each class as training data, 1000 different random nodes as a validation set and 1000 different random nodes as a test set; and this sampling is repeated 10 times separately.
PPI (Breitkreutz et al., 2008), POS (Mahoney, 2011) and BLOGCATALOG (Zafarani & Liu, 2009) are given without node features, and used for the multi-label node classification task. PPI is a subgraph of the Protein-Protein Interaction network for Homo Sapiens, and the class labels represent biological states. POS is a co-occurrence network of words from the Wikipedia dump, and the class labels represent the part-of-speech tags. BLOGCATALOG is a social network of relationships of the bloggers listed on the BlogCatalog website, and the class labels represent bloggers' interests. For each of these three datasets, each node has one or more attached class labels. A certain fraction of nodes are provided to train a classifier which is then used to predict the labels of the remaining nodes. Table 1 presents the statistics of six experimental datasets.

3.2 TRAINING PROTOCOL
The transductive setting: For all six datasets, we train our CapsG using the whole input graph, i.e., all nodes are present during training. We uniformly sample 128 random walks of length 10 (q = 10) for each node in the graph once. We fix the embedding size k of nodes to 128 (k = 128) and the number of samples in the sampled softmax loss function to 256. The number m of iterations in the dynamic routing algorithm is set to 5 (m = 5). To learn our model parameters, we use the Adam optimizer (Kingma & Ba, 2014) to train our model and select the Adam initial learning rate  {5e-6, 1e-5, 5e-5, 1e-4, 5e-4}. We run up to 100 epochs and evaluate the model after each training epoch to choose the best model on the validation set.
For CORA, CITESEER and PUBMED, we set the number n of iterations in Algorithm 1 to 4 (n = 4) and sample a target node at indexes in {3, 4, 5, 6} for each iteration respectively. We fix the batch size at 64 for CORA and CITESEER; and the batch size at 128 for PUBMED. We use the same bag-

5

Under review as a conference paper at ICLR 2019
of-words feature vectors as used by Yang et al. (2016) and Kipf & Welling (2017). We use these same values of hyper-parameters for each sampled time as mentioned in Section 3.1.
For PPI, POS and BLOGCATALOG, we set the number n of iterations in Algorithm 1 to 10 (n = 10); and for each iteration, we rotationally take each node in the random walk as a target node and the remaining nodes as context nodes. We set the batch size to 128 for these three datasets. Because the datasets are given without node features, we set the dimension size d of feature vectors to 128 (d = 128), and the feature vectors are uniformly initialized at random and updated during training.
The inductive setting: As mentioned in Section 3.1, for CORA, CITESEER and PUBMED, for each sampled time, we follow the same inductive setting of firstly removing all nodes in the test set from the graph before training (i.e., all nodes in the test set are now unseen and removed before generating the random walks). We then use the same hyper-parameter values as used in the transductive setting to train our CapsG. To infer the embedding of an unseen node v in the test set, we treat the vector output ev of the second capsule layer as the embedding for this unseen node.
We implement a baseline using a single GCN layer modified to: hv = ReLU v Cv (W xv + b) , and then use hv to infer the embedding ov of target node v like in Equation 2. We do the same experimental setups for this baseline on CORA, CITESEER and PUBMED. We also use hv as the embedding of the unseen node v in the inductive setting.
3.3 EVALUATION PROTOCOL
For PPI, POS and BLOGCATALOG, the learned node embeddings are used to do the multi-label node classification task. We follow the exact same experimental setups used by Perozzi et al. (2014) and Duran & Niepert (2017) to uniformly sample a fraction of nodes at random as training set for training a one-vs-rest logistic regression classifier.1 This classifier is then used to classify the remaining nodes. We repeat this 10 times for each fraction value and then compute the mean and standard deviation of Micro-F1 and Macro-F1 scores. We monitor the scores after each training epoch, for which the best model is chosen by using 10-fold cross-validation for each fraction value and the Micro-F1 and Macro-F1 scores separately.
For CORA, CITESEER and PUBMED, we follow the exact same setups used by Duran & Niepert (2017). We monitor the accuracy on the validation set after each training epoch, in which we firstly train a one-vs-rest L2-regularized logistic regression classifier2 on the training set and then select the best model giving the highest accuracy on the validation set for each sampled time. We finally report the mean and standard deviation of the accuracy results on the test sets over 10 sampled times.
3.4 MAIN EXPERIMENTAL RESULTS
The baseline results for POS, PPI and BLOGCATALOG are taken from Duran & Niepert (2017) including: DeepWalk, LINE, Node2Vec and and EP-B. We show in Table 2 the experimental MicroF1 and Macro-F1 scores in the transductive setting where the best score is in bold, while the second best score is in underline. Our CapsG can be seen as the best model on POS. In particular, we obtain new state-of-the-art Macro-F1 accuracy results and a new highest Micro-F1 score with the fraction value of 90%, and achieve the second highest Micro-F1 scores with the fraction values of 10% and 50%. Our CapsG also gives new highest Micro-F1 and Macro-F1 scores with the fraction value of 10% and new highest Macro-F1 score with the fraction value of 50% on PPI. In addition, CapsG produces new highest Micro-F1 and Macro-F1 scores with the fraction value of 90% on BLOGCATALOG. As a consequence, our CapsG obtains very competitive state-of-the-art scores on the three datasets, except for the fraction value of 10% with Micro-F1 on BLOGCATALOG.
The baseline results for CORA, CITESEER and PUBMED are also extracted from Duran & Niepert (2017) consisting of: Bag-of-Words, DeepWalk, Planetoid, GCN and EP-B. Duran & Niepert (2017) modified the GCN implementation provided by (Kipf & Welling, 2017) into the inductive setting. We follow this to employ the GAT implementation and use the exact same sets of training, validation and test sets that we used for 10 sampled times to re-evaluate GAT using the same hyper-parameters
1We use the default parameters for training the logistic regression classifier as used by Perozzi et al. (2014). 2We employ LIBLINEAR (Fan et al., 2008) and set tolerance of termination criterion to 0.001. http: //www.csie.ntu.edu.tw/cjlin/liblinear/.
6

Under review as a conference paper at ICLR 2019

Method (Micro-F1) DeepWalk LINE Node2Vec EP-B Our CapsG
Method (Macro-F1) DeepWalk LINE Node2Vec EP-B Our CapsG

10% 45.02 ± 1.09 45.22 ± 0.86 44.66 ± 0.92 46.97 ± 0.36 46.01 ± 0.59
10% 8.20 ± 0.27 8.49 ± 0.41 8.32 ± 0.36 8.85 ± 0.33 9.71 ± 0.58

POS 50% 49.10 ± 0.52 51.64 ± 0.65 48.73 ± 0.59 49.52 ± 0.48 50.93 ± 0.66
POS 50% 10.84 ± 0.62 12.43 ± 0.81 11.07 ± 0.60 10.45 ± 0.69 13.16 ± 0.94

90% 49.33 ± 2.39 52.28 ± 1.87 49.73 ± 2.35 50.05 ± 2.23 53.92 ± 1.90
90% 12.23 ± 1.38 12.40 ± 1.18 12.11 ± 1.93 12.17 ± 1.19 14.11 ± 1.77

10% 17.14 ± 0.89 16.55 ± 1.50 17.00 ± 0.81 17.82 ± 0.77 18.52 ± 0.55
10% 13.01 ± 0.90 12,79 ± 0.48 13.32 ± 0.49 13.80 ± 0.67 15.20 ± 0.64

PPI 50% 23.52 ± 0.65 23.01 ± 0.84 23.31 ± 0.62 23.30 ± 0.37 23.15 ± 0.63
PPI 50% 18.73 ± 0.59 18.06 ± 0.81 18.57 ± 0.49 18.96 ± 0.43 19.63 ± 0.57

90% 25.02 ± 1.38 25.28 ± 1.68 24.75 ± 2.02 24.74 ± 1.30 25.08 ± 1.55
90% 20.01 ± 1.82 20.59 ± 1.59 19.66 ± 2.34 20.36 ± 1.42 20.27 ± 1.33

BLOGCATALOG 10% 50% 90% 34.48 ± 0.40 38.11 ± 0.43 38.34 ± 1.82 34.83 ± 0.39 38.99 ± 0.25 38.77 ± 1.08 35.54 ± 0.49 39.31 ± 0.25 40.03 ± 1.22 35.05 ± 0.41 39.44 ± 0.29 40.41 ± 1.59 32.31 ± 0.37 38.35 ± 0.38 40.79 ± 1.07
BLOGCATALOG 10% 50% 90% 18.16 ± 0.44 22.65 ± 0.49 22.86 ± 1.03 18.13 ± 0.33 22.56 ± 0.49 23.00 ± 0.92 19.08 ± 0.52 23.97 ± 0.58 24.82 ± 1.00 19.08 ± 0.78 25.11 ± 0.43 25.97 ± 1.25 18.40 ± 0.48 24.80 ± 0.57 26.63 ± 0.98

Table 2: Experimental multi-label node classification results on PPI, POS and BLOGCATALOG.

provided by Velickovic´ et al. (2018). For each sampled time, in the inductive setting, we remove all edges from/to all nodes in the test set before training a GAT model, but keep those edges when evaluating the trained GAT model. GAT aggregates feature information only from neighbors of a given node, while GCN uses a multiple stacked-layer structure which helps to indirectly aggregate feature information from nodes at many hops away. Thus, this is reason why GCN often works better than GAT in the inductive setting (e.g., specifically in such a case that many neighbors of unseen nodes are also unseen) as shown in Table 3.

Method (Transductive) Bag-of-Words (BoW) DeepWalk (Perozzi et al., 2014) Planetoid (Yang et al., 2016)[ ] EP-B (Duran & Niepert, 2017) GCN (Kipf & Welling, 2017)[ ] GAT (Velickovic´ et al., 2018)[ ] Our modified-GCN baseline Our CapsG GCN (Kipf & Welling, 2017)[ ]() GraphSAGE (Hamilton et al., 2017a)() GAT (Velickovic´ et al., 2018)[ ]()
Method (Inductive) Planetoid (Yang et al., 2016)[ ] EP-B (Duran & Niepert, 2017) GCN (Kipf & Welling, 2017)[ ] GAT (Velickovic´ et al., 2018)[ ] Our modified-GCN baseline Our CapsG

CORA 58.63 ± 0.68 71.11 ± 2.70 71.90 ± 5.33 78.05 ± 1.49 79.59 ± 2.02 81.72 ± 0.94 77.72 ± 2.32 80.29 ± 2.20 81.5 77.4 83.0
CORA 64.80 ± 3.70 73.09 ± 1.75 67.76 ± 2.11 68.94 ± 1.42 70.42 ± 3.16 73.47 ± 1.32

CITESEER 58.07 ± 1.72 47.60 ± 2.34 58.58 ± 6.35 71.01 ± 1.35 69.21 ± 1.25 70.80 ± 0.29 71.03 ± 1.54 71.25 ± 1.88 70.3 63.5 72.5
CITESEER 61.97 ± 3.82 68.61 ± 1.69 63.40 ± 0.98 59.55 ± 1.27 65.33 ± 3.60 69.47 ± 1.43

PUBMED 70.49 ± 2.89 73.49 ± 3.00 74.49 ± 4.95 79.56 ± 2.10 77.32 ± 2.66 79.56 ± 0.77 74.72 ± 2.64 77.15 ± 1.78 79.0 77.6 79.0
PUBMED 75.73 ± 4.21 79.94 ± 2.30 73.47 ± 2.48 71.29 ± 1.12 68.27 ± 4.99 75.11 ± 1.51

Table 3: Experimental accuracy results CORA, CITESEER and PUBMED in the transductive and inductive settings. The best score is in bold, while the second best score is in underline. Results of GraphSAGE are taken from Abu-El-Haija et al. (2018). () denotes a model evaluated by only using the data split chosen by Yang et al. (2016), this is not good to show the effectiveness of a proposed model as mentioned by Duran & Niepert (2017). [ ] denotes a semi-supervised model.

Table 3 shows that our CapsG obtains a new state-of-the-art accuracy on CITESEER and reaches the second highest accuracy on CORA in the transductive setting. More importantly, CapsG produces the highest scores on CORA and CITESEER in the inductive setting. These show the effectiveness of our CapsG using the dynamic routing process, even we only use a single and simplified architecture. Besides, we plan to investigate our CapsG in using a multiple stacked-layer structure like in GCN or a multi-head structure (Vaswani et al., 2017) like in GAT in a further study.

7

Under review as a conference paper at ICLR 2019
EP-B is the best model on PUBMED where not much neighbors of unseen nodes are also unseen, and texts from all nodes are less sparse than those in CORA and CITESEER. An advantage of EP-B is that it follows the Word2Vec CBOW model (Mikolov et al., 2013a) to simultaneously learn word embeddings on texts from all nodes, for which embeddings of words in the text of each node are averaged into a new feature vector which is then used to reconstruct the node embedding. This new kind of feature vectors helps EP-B to attain more useful information from texts, instead of using the same bag-of-words feature vectors as used in Planetoid, GCN-based models and CapsG. GCN and GAT are semi-supervised models where node labels in the training set are used during training the models, so this helps them to perform better than our CapsG on PUBMED in the transductive setting. In contrast, as mentioned in comparing our CapsG with related works in Section 2, directly aggregating feature information from nodes at many hops ways in our CapsG shows an effective approach to outperform GCN and GAT in the inductive setting on the three datasets. Although one of our main goals is to design CapsG in an unsupervised manner that therefore CapsG can work on a range of networks, we also plan to extend CapsG into a semi-supervised manner to obtain better performances in future work.
Figure 2: Visualization of learned node embeddings on PUBMED. i) Right picture: with using the shuffling process. ii) Left picture: without using the shuffling process. iii) Colors denote classes.
To prove that our CapsG can preserve the relative positions of nodes, we use the same random walks generated for CORA, CITESEER and PUBMED in the transductive setting, but we now adapt another setup of shuffling context nodes during training. For example, as shown in Figure 1, nodes {1, 2, 4, 5, 6} are context nodes of target node 3. We shuffle these context nodes to change their orders, e.g., {1, 2, 6, 5, 4}, such that context node 6 is closer to target node 3 (note that there is not an edge between nodes 6 and 3). We then do the same training and evaluation protocols as presented in Sections 3.2 and 3.3. We finally obtain results of (80.28 ± 2.17), (70.91 ± 1.99) and (77.21 ± 1.73) which are similar to the results of (80.29 ± 2.20), (71.25 ± 1.88) and (77.15 ± 1.78) for CORA, CITESEER and PUBMED (as shown in Table 3), respectively. Furthermore, we apply t-SNE (Maaten & Hinton, 2008) to visualize node embeddings learned by our CapsG on PUBMED in Figure 2. We can see that there is similar in learned node embeddings between with and without using the shuffling process. These indicate that the relative positions of nodes can be preserved in our model in order to solve the problem of losing potential information.
4 CONCLUSION
We have shown the first application of dynamic routing between capsules for the node classification task on the graph-structured data, by proposing the new unsupervised embedding model CapsG to learn node embeddings. Experimental results show that our CapsG obtains the new state-of-theart results on CORA, CITESEER and POS, and gives very competitive results on PUBMED, PPI and BLOGCATALOG. Furthermore, we prove that our CapsG can preserve the relative positions of nodes in order to avoid the loss of the useful information among nodes. In the future work, we plan to extend our CapsG into a semi-supervised manner to achieve better performances. Our code and data splits are available at: https://anonymous-url/.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-gcn: Multi-scale graph convolution for semi-supervised node classification. arXiv preprint arXiv:1802.08888, 2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pp. 1247­1250, 2008.
Bobby-Joe Breitkreutz, Chris Stark, Teresa Reguly, Lorrie Boucher, Ashton Breitkreutz, Michael Livstone, Rose Oughtred, Daniel Lackner, Jrg Bhler, Valerie Wood, Kara Dolinski, and Mike Tyers. The biogrid interaction database: 2008 update. 36:D637­40, 2008.
Hongyun Cai, Vincent W Zheng, and Kevin Chang. A comprehensive survey of graph embedding: problems, techniques and applications. IEEE Transactions on Knowledge and Data Engineering, 2018.
Haochen Chen, Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. A tutorial on network embeddings, 2018.
Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. High-order graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting, 2018.
Alberto Garcia Duran and Mathias Niepert. Learning graph representations with embedding propagation. In Advances in Neural Information Processing Systems, pp. 5119­5130, 2017.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871­1874, 2008.
Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 855­864, 2016.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1024­1034, 2017a.
William L. Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584, 2017b.
Se´bastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1­10, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.
Juzheng Li, Jun Zhu, and Bo Zhang. Discriminative deep random walk for network classification. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1004­1013, 2016.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9:2579­2605, 2008.
9

Under review as a conference paper at ICLR 2019
Matt Mahoney. Large text compression benchmark, 2011. URL http://www.mattmahoney.net/text/ text.html.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013a.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pp. 3111­3119, 2013b.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 701­710, 2014.
Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In Advances in Neural Information Processing Systems, pp. 3859­3869, 2017.
Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and control. In Proceedings of the 35th International Conference on Machine Learning, pp. 4470­ 4479, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: A core of semantic knowledge. In Proceedings of the 16th International Conference on World Wide Web, pp. 697­706, 2007.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pp. 1067­1077, 2015.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998­6008, 2017.
Petar Velickovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio`, and Yoshua Bengio. Graph Attention Networks. International Conference on Learning Representations (ICLR), 2018.
Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1225­1234, 2016.
Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo. Graphgan: Graph representation learning with generative adversarial nets. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018a.
Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. Billionscale commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#38; Data Mining, pp. 839­848, 2018b.
Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, pp. 40­48, 2016.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#38; Data Mining, pp. 974­983, 2018.
10

Under review as a conference paper at ICLR 2019 R. Zafarani and H. Liu. Social computing data repository at ASU, 2009. URL http://
socialcomputing.asu.edu. Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. Network representation learning: A
survey. IEEE Transactions on Big Data, 2018.
11

