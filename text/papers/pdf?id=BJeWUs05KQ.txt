Under review as a conference paper at ICLR 2019
DIRECTED-INFO GAIL: LEARNING HIERARCHICAL POLICIES FROM UNSEGMENTED DEMONSTRATIONS USING DIRECTED INFORMATION
Anonymous authors Paper under double-blind review
ABSTRACT
The use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging. In fact, previous work has shown that when the modes are known, learning separate policies for each mode or sub-task can greatly improve the performance of imitation learning. In this work, we discover the interaction between sub-tasks from their resulting stateaction trajectory sequences using a directed graphical model. We propose a new algorithm based on the generative adversarial imitation learning framework which automatically learns sub-task policies from unsegmented demonstrations. Our approach maximizes the directed information flow in the graphical model between sub-task latent variables and their generated trajectories. We also show how our approach connects with the existing Options framework, which is commonly used to learn hierarchical policies.
1 INTRODUCTION
Complex human activities can often be broken down into various simpler sub-activities or sub-tasks that can serve as the basic building blocks for completing a variety of complicated tasks. For instance, when driving a car, a driver may perform several simpler sub-tasks such as driving straight in a lane, changing lanes, executing a turn and braking, in different orders and for varying times depending on the source, destination, traffic conditions etc. Using imitation learning to learn a single monolithic policy to represent a structured activity can be challenging as it does not make explicit the sub-structure between the parts within the activity. In this work, we develop an imitation learning framework that can learn a policy for each of these sub-tasks given unsegmented activity demonstrations and also learn a macro-policy which dictates switching from one sub-task policy to another. Learning sub-task specific policies has the benefit of shared learning. Each such sub-task policy also needs to specialize over a restricted state space, thus making the learning problem easier.
Previous works in imitation learning (Li et al., 2017; Hausman et al., 2017) focus on learning each sub-task specific policy using segmented expert demonstrations by modeling the variability in each sub-task policy using a latent variable. This latent variable is inferred by enforcing high mutual information between the latent variable and expert demonstrations. This information theoretic perspective is equivalent to the graphical model shown in Figure 1 (Left), where the node c represents the latent variable. However, since learning sub-task policies requires isolated demonstrations for each sub-task, this setup is difficult to scale to many real world scenarios where providing such segmented trajectories is cumbersome. Further, this setup does not learn a macro-policy to combine the learned sub-task policies in meaningful ways to achieve different tasks.
In our work, we aim to learn each sub-task policy directly from unsegmented activity demonstrations. For example, given a task consisting of three sub-tasks -- A, B and C, we wish to learn a policy to complete sub-task A, learn when to transition from A to B, finish sub-task B and so on. To achieve this we use a causal graphical model, which can be represented as a Dynamic Bayesian Network as shown in Figure 1 (Right). The nodes ct denote latent variables which indicate the currently active sub-task and the nodes t denote the state-action pair at time t. We consider as given, a set of expert demonstrations, each of which is represented by  = {1, · · · , T } and has a corresponding sequence of latent factors c = {c1, · · · , cT -1}. The sub-activity at time t dictates what state-action pair was
1

Under review as a conference paper at ICLR 2019

c

c0 c1 c2

ct-2 ct-1

1 2

t-1 t 1 2 3

t-1 t

Figure 1: Left: Graphical model used in Info-GAIL Li et al. (2017). Right: Causal model in this work. The latent code causes the policy to produce a trajectory. The current trajectory, and latent code produce the next latent code

generated at time t. The previous sub-task and the current state together cause the selection of the next sub-task.
As we will discuss in Section 3, extending the use of mutual information to learn sub-task policies from unsegmented demonstrations is problematic, as it requires learning the macro-policy as a conditional probability distribution which depends on the unobserved future. This unobserved future is unknown during earlier points of interaction (Figure 1). To alleviate this, in our work we aim to force the policy to generate trajectories that maximize the directed information or causal information (Massey, 1990) flow from trajectories to latent factors of variation within the trajectories instead of mutual information. Using directed information requires us to learn a causally conditioned probability distribution (Kramer, 1998) which depends only on the observed past while allowing the unobserved future to be sequentially revealed. Further, since there exists feedback in our causal graphical model i.e., information flows from the latent variables to trajectories and vice versa, directed information also provides a better upper bound on this information flow between the latent variables and expert trajectories than does the conventional mutual information (Massey, 1990; Kramer, 1998).
We also draw connections with existing work on learning sub-task policies using imitation learning with the options framework (Sutton et al., 1998; Daniel et al., 2016). We show that our work, while derived using the information theoretic perspective of maximizing directed information, bears a close resemblance to applying the options framework in a generative adversarial imitation setting. Thus, our approach combines the benefits of learning hierarchical policies using the options framework with the robustness of generative adversarial imitation learning, helping overcome problems such as compounding errors that plague behaviour cloning.
In summary, the main contributions of our work include:
· We extend existing generative adversarial imitation learning frameworks to allow for learning of sub-task specific policies by maximizing directed information in a causal graph of subactivity latent variables and observed trajectory variables.
· We draw connections between previous works on imitation learning with sub-task policies using options and show that our proposed approach can also be seen as option learning in a generative adversarial setting.
· We show through experiments on both discrete and continuous state-action spaces, the ability of our approach to segment expert demonstrations into meaningful sub-tasks and combine sub-task specific policies to perform the desired task.
2 RELATED WORK
2.1 IMITATION LEARNING
Imitation Learning (Pomerleau, 1989) aims at learning policies that can mimic expert behaviours from demonstrations. Modeling the problem as a Markov Decision Process (MDP), the goal in imitation learning is to learn a policy (a|s), which defines the conditional distribution over actions a  A given the state s  S, from state-action trajectories  = (s0, a0, · · · , sT ) of expert behaviour. Recently, Ho & Ermon (2016) introduced an imitation learning framework called Generative Adversarial Imitation Learning (GAIL) that is able to learn policies for complex high-dimensional physics-based control tasks. They reduce the imitation learning problem into an adversarial learning framework, for which they utilize Generative Adversarial Networks (GAN) (Goodfellow et al., 2014). The generator
2

Under review as a conference paper at ICLR 2019

network of the GAN represents the agent's policy  while the discriminator network serves as a local
reward function and learns to differentiate between state-action pairs from the expert policy E and from the agent's policy . Mathematically, it is equivalent to optimizing the following,

min


max
D

E

[log

D(s,

a)]

+

EE

[1

-

log

D(s,

a)]

-

H

(

)

InfoGAIL (Li et al., 2017) and Hausman et al. (2017) solve the problem of learning from policies generated by a mixture of experts. They introduce a latent variable c into the policy function (a|s, c) to separate different type of behaviours present in the demonstration. To incentivize the network to use the latent variable, they utilize an information-theoretic regularization enforcing that there should be high mutual information between c and the state-action pairs in the generated trajectory, a concept that was first introduced in InfoGAN (Chen et al., 2016). They introduce a variational lower bound L1(, Q) of the mutual information I(c;  ) to the loss function in GAIL.

L1(, Q) = Ecp(c),a(·|s,c) log Q(c| ) + H(c)  I(c;  )

The modified objective can then be given as,

min
,q

max
D

E

[log

D

(s,

a)]

+

EE

[1

-

log

D(s,

a)]

-

1

L1

(,

q

)

-

2

H

(

)

InfoGAIL models variations between different trajectories as the latent codes correspond to trajectories coming from different demonstrators. In contrast, we aim to model intra-trajectory variations and latent codes in our work correspond to sub-tasks (variations) within a demonstration. In Section 3, we discuss why using a mutual information based loss is infeasible in our problem setting and describe our proposed approach.

2.2 OPTIONS
Consider an MDP with states s  S and actions a  A. Under the options framework (Sutton et al., 1998), an option, indexed by o  O consists of a sub-policy (a|s, o), a termination policy (b|s, o¯) and an option activation policy (o|s). After an option is initiated, actions are generated by the sub-policy until the option is terminated and a new option is selected.
Options framework has been studied widely in RL literature. A challening problem related to the options framework is to automatically infer options without supervision. Option discovery approaches often aim to find bottleneck states, i.e., states that the agent has to pass through to reach the goal. Many different approaches such as multiple-instance learning (McGovern & Barto, 2001), graph based algorithms (Menache et al., 2002; S¸ ims¸ek et al., 2005) have been used to find such bottleneck states. Once the bottleneck states are discovered, the above approaches find options policies to reach each such state. In contrast, we propose a unified framework using a information-theoretic approach to automatically discover relevant option policies without the need to discover bottleneck states.
Daniel et al. (2016) formulate the options framework as a probabilistic graphical model where options are treated as latent variables which are then learned from expert data. The option policies ((a|s, o)) are analogous to sub-task policies in our work. These option policies are then learned by maximizing a lower bound using the Expectation-Maximization algorithm (Moon, 1996). We show how this lower bound is closely related to the objective derived in our work. We further show how this connection allows our method to be seen as a generative adversarial variant of their approach.
Prior work in robot learning has also looked at learning motion primitives from unsegmented demonstrations. These primitives usually correspond to a particular skill and are analogous to options. Niekum & Barto (2011) used the Beta-Process Autoregressive Hidden Markov Model (BP-AR-HMM) to segment expert demonstrations and post-process these segments to learn motion primitives which provide the ability to use reinforcement learning for policy improvement. Alternately, Krishnan et al. (2018) use Dirichlet Process Gaussian Mixture Model (DP-GMM) to segment the expert demonstrations by finding transition states between linear dynamical segments. Similarly, Ranchod et al. (2015) use the BP-AR-HMM framework to initially segment the expert demonstrations and then use an inverse reinforcement learning step to infer the reward function for each segment. The

3

Under review as a conference paper at ICLR 2019

use of appropriate priors allows these methods to discover options without a priori knowledge of the total number of skills. However, unlike the above methods in our proposed approach we also learn an appropriate policy over the extracted options. We show how this allows us to compose the individual option policies to induce novel behaviours which were not present in the expert demonstrations.

3 PROPOSED APPROACH
As mentioned in the previous section, while prior approaches can learn to disambiguate the multiple modalities in the demonstration of a sub-task and learn to imitate them, they cannot learn to imitate demonstrations of unsegmented long tasks that are formed by a combination of many small sub-tasks. To learn such sub-task policies from unsegmented deomonstrations we use the graphical model in Figure 1 (Right), i.e., consider a set of expert demonstrations, each of which is represented by  = {1, · · · , T } where t is the state-action pair observed at time t. Each such demonstration has a corresponding sequence of latent variables c = {c1, · · · , cT -1} which denote the sub-activity in the demonstration at any given time step.
As noted before, previous approaches (Li et al., 2017; Hausman et al., 2017) model the expert sub-task demonstrations using only a single latent variable. To enforce the model to use this latent variable, these approaches propose to maximize the mutual information between the demonstrated sequence of state-action pairs and the latent embedding of the nature of the sub-activity. This is achieved by adding a lower bound to the mutual information between the latent variables and expert demonstrations. This variational lower bound of the mutual information is then combined with the the adversarial loss for imitation learning proposed in Ho & Ermon (2016). Extending this to our setting, where we have a sequence of latent variables c, yields the following lower bound on the mutual information,

L(, q) =

Ec1:tp(c1:t),at-1(·|st-1,c1:t-1) log q(ct|c1:t-1,  ) + H (c)  I ( ; c) (1)

t

Observe that the dependence of q on the entire trajectory  precludes the use of such a distribution at test time, where only the trajectory up to the current time is known. To overcome this limitation, in this work we propose to force the policy to generate trajectories that maximize the directed or causal information flow from trajectories to the sequence of latent sub-activity variables instead. As we show below, by using directed information instead of mutual information, we can replace the dependence on  with a dependence on the trajectory generated up to current time t.

The directed information flow from a sequence X to Y is given by,

I(X  Y ) = H(Y ) - H(Y X) where H(Y X) is the causally-conditioned entropy. Replacing X and Y with sequences  and c,

I(  c) = H(c) - H(c  )

= H(c) - H(ct|c1:t-1,  1:t)

t

= H(c) +

p(c1:t-1,  1:t)

t c1:t-1, 1:t

p(ct|c1:t-1,  1:t) log p(ct|c1:t-1,  1:t)
ct

(2)

Here  1:t = (s1, · · · , at-1, st). A variational lower bound, L1(, q) of the directed information, I(  c) which uses an approximate posterior q(ct|c1:t-1,  1:t) instead of the true posterior p(ct|c1:t-1,  1:t) can then be derived to get (See Appendix A.1 for the complete derivation),

L1(, q) =

Ec1:tp(c1:t),at-1(·|st-1,c1:t-1) log q(ct|c1:t-1,  1:t) + H (c)  I (  c) (3)

t

4

Under review as a conference paper at ICLR 2019

a0
MLP

a1
MLP

a2
MLP

aT-1
MLP

s0 c0 c0
MLP

s1 c1 c1
MLP

s2 c2 c2
MLP

s cT-1 T-1 cT-1
MLP

VAE pre-training
(Learn VAE and q VAE)

q VAE Directed-Info GAIL
(q DI-GAIL = q VAE . Learn  DI-GAIL)

s0 c-1

s1 c0

s2 c1

s cT-1 T-2

Figure 2: Left: VAE pre-training step. The VAE encoder uses the current state (st), and previous latent variable (ct-1) to produce the current latent variable (ct). The decoder reconstructs the action (at) using st and ct. Right: An overview of the proposed approach. We use the VAE pre-training step to learn an approximate prior over the latent variables and use this to learn sub-task policies in
the proposed Directed-Info GAIL step.

Thus, by maximizing directed information instead of mutual information, we can learn a posterior distribution over the next latent factor c given the latent factors discovered up to now and the trajectory followed up to now, thereby removing the dependence on the future trajectory. In practice, we do not consider the H(c) term. This gives us the following objective,

min max
,q D

E [log D(s, a)] + EE [1 - log D(s, a)] - 1L1(, q) - 2H()

(4)

We call this approach Directed-Info GAIL. Notice that, to compute the loss in equation 3, we need to sample from the prior distribution p(c1:t). In order to estimate this distribution, we first pre-train a
variational auto-encoder (VAE) (Kingma & Welling, 2013) on the expert trajectories, the details of
which are described in the next sub-section.

3.1 VAE PRE-TRAINING
Figure 2 (left) shows the design of the VAE pictorially. The VAE consists of two multi-layer perceptrons that serve as the encoder and the decoder. The encoder uses the current state st and the previous latent variable ct-1 to produce the current latent variable ct. We used the Gumbel-softmax trick (Jang et al., 2016) to obtain samples of latent variables from a categorical distribution. The decoder then takes st and ct as input and outputs the action at. We use the following objective, which maximizes the lower bound of the probability of the trajectories p( ), to train our VAE,

LVAE(, q; i) = - Ectq log (at|st, c1:t) + DKL(q(ct|c1:t-1,  1:t) p(ct|c1:t-1)) (5)
tt
Figure 2 (right) gives an overview of the complete method. The VAE pre-training step allows us to get approximate samples from the distribution p(c1:t) to optimize equation 4. This is done by using q to obtain samples of latent variable sequence c by using its output on the expert demonstrations. In practice, we fix the weights of the network q to those obtained from the VAE pre-training step when optimizing the Directed-Info GAIL loss in equation 4.
3.2 CONNECTION WITH OPTIONS FRAMEWORK
In Daniel et al. (2016) the authors provide a probabilistic perspective of the options framework. Although, Daniel et al. (2016) consider separate termination and option latent variables (bt and ot), for the purpose of comparison, we collapse them into a single latent variable ct, similar to our framework with a distribution p(ct|st, ct-1). The lower-bound derived in Daniel et al. (2016) which is maximized using Expectation-Maximization (EM) algorithm can then be written as (suppressing dependence on parameters),
5

Under review as a conference paper at ICLR 2019

(a) (b)

(c) (d)

Figure 3: Results on the Four Rooms environment. (a) and (b) show results for two different latent variables. The arrows in each cell indicate the direction (action) with highest probability in that state and using the given latent variable. (c) and (d) show expert and generated trajectories in this environment. Star (*) represents the start state. The expert trajectory is shown in red. The color of the generated trajectory represents the latent code used by the policy at each time step.

p( ) 

p(ct-1:t| ) log p(ct|st, ct-1)) +

p(ct| ) log (at|st, ct)

t ct-1:t

t ct

(6)

Note that the first term in equation 6 i.e., the expectation over the distribution log p(ct|st, ct-1) is the same as equation 3 of our proposed approach with a one-step Markov assumption and a conditional expectation with given expert trajectories instead of an expectation with generated trajectories. The second term in equation 6 i.e., the expectation over log (at|st, ct) is replaced by the GAIL loss in equation 4. Our proposed Directed-Info GAIL can be therefore be considered as the generative adversarial variant of imitation learning using the options framework. The VAE behaviour cloning pretraining step in equation 5 is exactly equivalent to equation 6, where we use approximate variational inference using VAEs instead of EM. Thus, our approach combines the benefits of both behavior cloning and generative adversarial imitation learning. Using GAIL enables learning of robust policies that do not suffer from the problem of compounding errors. At the same time, conditioning GAIL on latent codes learned from the behavior cloning step prevents the issue of mode collapse in GANs.

4 EXPERIMENTS
We present results on both discrete and continuous state-action environments. In both of these settings we show that (1) our method is able to segment out sub-tasks from given expert trajectories, (2) learn sub-task conditioned policies, and (3) learn to combine these sub-task policies in order to achieve the task objective.
4.1 DISCRETE ENVIRONMENT
For the discrete setting, we choose a grid world environment which consists of a 15 × 11 grid with four rooms connected via corridors as shown in Figure 3. The agent spawns at a random location in the grid and its goal is to reach an apple, which spawns in one of the four rooms randomly, using the shortest possible path. Through this experiment we aim to see whether our proposed approach is able to infer sub-tasks which correspond to meaningful navigation strategies and combine them to plan paths to different goal states.
Figure 3 shows sub-task policies learned by our approach in this task. The two plots on the left correspond to two of the four different values of the latent variable. The arrow at every state in the grid shows the agent action (direction) with the highest probability in that state for that latent variable. In the discussion that follows, we label the rooms from 1 to 4 starting from the room at the top left and moving in the clockwise direction. We observe that the sub-tasks extracted by our approach represent semantically meaningful navigation plans. Also, each latent variable is utilized for a different sub-task. For instance, the agent uses the latent code in Figure 3(a), to perform the sub-task of moving from room 1 to room 3 and from room 2 to room 4 and the code in Figure 3(b) to move in the opposite direction. Further, our approach learns to successfully combine these navigation strategies to achieve the given objectives. For example, Figure 3(c, d) show examples of how the

6

Under review as a conference paper at ICLR 2019

Context 0 Context 1
(a)

(b)

Velocity

8 6 4 2 0 2
4 c=0 6 c=1 8 c=2
3 2 1 Posi0tion 1 2 3
(c) (d)

Figure 4: Results for Directed-Info GAIL on continuous environments. (a) Our method learns to break down the Circle-World task into two different sub-activities, shown in green and blue. (b) Trajectory generated using our approach. Color denotes time step. (c) Trajectory generated in opposite direction. Color denotes time step. (d) Sub-activity latent variables as inferred by Directed-Info GAIL on Pendulum-v0. Different colors represent different context.

Peak

Ascend

Land

Pink leg

In air

Brown leg

Hopper-v2

(a)

Walker2d-v2

(b)

Figure 5: (a) and (b) show the plot of the sub-task latent variable vs time on the Hopper and Walker tasks. (c) and (d) show discovered sub-tasks using Directed-Info GAIL on these environments.

macro-policy switches between various latent codes to achieve the desired goals of reaching the apples in rooms 1 and 2 respectively.

4.2 CONTINUOUS ENVIRONMENTS
To validate our proposed approach in continuous control tasks we experiment with 5 continuous state-action environments. The first environment involves learning to draw circles on a 2D plane and is called Circle-World. In this experiment, the agent must learn to draw a circle in both clockwise and counter-clockwise direction. The agent always starts at (0,0), completes a circle in clockwise direction and then retraces its path in the counter-clockwise direction. The trajectories differ in the radii of the circles. The state s  R2 is the (x,y) co-ordinate and the actions a  R2 is a unit vector representing the direction of motion. Notice that in Circle-World, the expert trajectories include two different actions (for clockwise and anti-clockwise direction) for every state (x, y) in the trajectory, thus making the problem multi-modal in nature. This requires the agent to appropriately disambiguate between the two different phases of the trajectory.
Further, to show the scalability of our approach to higher dimensional continuous control tasks we also show experiments on Pendulum, Inverted Pendulum, Hopper and Walker environments, provided in OpenAI Gym (Brockman et al., 2016). Each task is progressively more challenging, with a larger state and action space. Our aim with these experiments is to see whether our approach can identify certain action primitives which helps the agent to complete the given task successfully. To verify the effectiveness of our proposed approach we do a comparative analysis of our results with both GAIL (Ho & Ermon, 2016) and the supervised behavior cloning approaching using a VAE. To generate expert trajectories we train an agent using Proximal Policy Optimization (Schulman et al., 2017). We used 25 expert trajectories for the Pendulum and Inverted Pendulum tasks and 50 expert trajectories for experiments with the Hopper and Walker environments.
Figures 4(a, b, c) show results on the Circle-World environment. As can be seen in Figure 4(a, b), when using two sub-task latent variables, our method learns to segment the demonstrations into

7

Under review as a conference paper at ICLR 2019

Environment
Pendulum-v0 InvertedPendulum-v2 Hopper-v2 Walker2d-v2

GAIL (Ho & Ermon, 2016)
-121.42 ± 94.13 1000.0 ± 15.23 3623.4 ± 51.0 4858.0 ± 301.7

VAE
-142.89 ± 95.57 218.8 ± 7.95 499.1 ± 86.2 1549.5 ± 793.7

Directed-Info GAIL
-125.39 ± 103.75 1000.0 ± 14.97 3662.1 ± 21.7 5083.9 ± 356.3

Table 1: A comparison of returns for continuous environments. The returns were computed using 300 episodes. Our approach gives comparable returns to using GAIL but also segments expert demonstrations into sub-tasks. The proposed Directed-Info GAIL approach improves over the policy learned from the VAE pre-training step.

two intuitive sub-tasks of drawing circles in clockwise and counterclockwise directions. Hence, our method is able to identify the underlying modes and thus find meaningful sub-task segmentations from unsegmented data. We also illustrate how the learned sub-task policies can be composed to perform new types of behavior that were unobserved in the expert data. In Figure 4(c) we show how the sub-task policies can be combined to draw the circles in inverted order of direction by swapping the learned macro-policy with a different desired policy. Thus, the sub-task policies can be utilized as a library of primitive actions which is a significant benefit over methods learning monolithic policies.
We now discuss the results on the classical Pendulum environment. Figure 4(d) shows the sub-task latent variables assigned by our approach to the various states. As can be seen in the figure, the network is able to associate different latent variables to different sub-tasks. For instance, states that have a high velocity are assigned a particular latent variable (shown in blue). Similarly, states that lie close to position 0 and have low velocity (i.e. the desired target position) get assigned another latent variable (shown in green). The remaining states get classified as a separate sub-task.
Figure 5 shows the results on the higher dimensional continuous control, Hopper and Walker, environments. Figure 5(a) shows a plots for sub-task latent variable assignment obtained on these environments. Our proposed method identifies basic action primitives which are then chained together to effectively perform the two locomotion tasks. Figure 5(b) shows that our approach learns to assign separate latent variable values for different action primitives such as, jumping, mid-air and landing phases of these tasks, with the latent variable changing approximately periodically as the agent performs the periodic hopping/walking motion.
Finally, in Table 1 we also show the quantitative evaluation on the above continuous control environments. We report the mean and standard deviations of the returns over 300 episodes. As can be seen, our approach improves the performance over the VAE pre-training step, overcoming the issue of compounding errors. The performance of our approach is comparable to the state-of-the-art GAIL (Ho & Ermon, 2016). Our method moreover, has the added advantage of segmenting the demonstrations into sub-tasks and also providing composable sub-task policies.
We further analyze our proposed approach in more detail in the Appendix. In Appendix A.4 we visualize the sub-tasks in a low-dimensional sub-space. Also, in Appendix A.5 we show results when using a larger dimensional sub-task latent variable. A video of our results on Hopper and Walker environments can be seen at https://sites.google.com/view/directedinfo-gail.

5 CONCLUSION
Learning separate sub-task policies can help improve the performance of imitation learning when the demonstrated task is complex and has a hierarchical structure. In this work, we present an algorithm that infers these latent sub-task policies directly from given unstructured and unlabelled expert demonstrations. We model the problem of imitation learning as a directed graph with sub-task latent variables and observed trajectory variables. We use the notion of directed information in a generative adversarial imitation learning framework to learn sub-task and macro policies. We further show theoretical connections with the options literature as used in hierarchical reinforcement and imitation learning. We evaluate our method on both discrete and continuous environments. Our experiments show that our method is able to segment the expert demonstrations into different sub-tasks, learn sub-task specific policies and also learn a macro-policy that can combines these sub-task.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2172­2180, 2016.
Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104(2-3):337­357, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, and Joseph J Lim. Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 1235­1245, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565­4573, 2016.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Gerhard Kramer. Directed information for channels with feedback. PhD thesis, Eidgenossiche Technische Hochschule Zurich, 1998.
Sanjay Krishnan, Animesh Garg, Sachin Patil, Colin Lea, Gregory Hager, Pieter Abbeel, and Ken Goldberg. Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning. In Robotics Research, pp. 91­110. Springer, 2018.
Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual demonstrations. In Advances in Neural Information Processing Systems, pp. 3815­3825, 2017.
James Massey. Causality, feedback and directed information. In Proc. Int. Symp. Inf. Theory Applic.(ISITA-90), pp. 303­305. Citeseer, 1990.
Amy McGovern and Andrew G Barto. Accelerating reinforcement learning through the discovery of useful subgoals. 2001.
Ishai Menache, Shie Mannor, and Nahum Shimkin. Q-cutdynamic discovery of sub-goals in reinforcement learning. In European Conference on Machine Learning, pp. 295­306. Springer, 2002.
Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6): 47­60, 1996.
Scott Niekum and Andrew G Barto. Clustering via dirichlet process mixture models for portable skill discovery. In Advances in neural information processing systems, pp. 1818­1826, 2011.
Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural information processing systems, pp. 305­313, 1989.
Pravesh Ranchod, Benjamin Rosman, and George Konidaris. Nonparametric bayesian reward segmentation for skill discovery using inverse reinforcement learning. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pp. 471­477. IEEE, 2015.
9

Under review as a conference paper at ICLR 2019 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. O¨ zgu¨r S¸ ims¸ek, Alicia P Wolfe, and Andrew G Barto. Identifying useful subgoals in reinforcement
learning by local graph partitioning. In Proceedings of the 22nd international conference on Machine learning, pp. 816­823. ACM, 2005. Richard S Sutton, Doina Precup, and Satinder P Singh. Intra-option learning about temporally abstract actions. In ICML, volume 98, pp. 556­564, 1998.
10

Under review as a conference paper at ICLR 2019

A APPENDIX
A.1 DERIVATION FOR DIRECTED-INFO LOSS The directed information flow from a sequence X to Y is given by:
I(X  Y ) = H(Y ) - H(Y X) where H(Y X) is the causally-conditioned entropy. Replacing X and Y with the sequences  and c give,

I(  c) = H(c) - H(c  )

= H(c) - H(ct|c1:t-1,  1:t)
t

= H(c) +

p(c1:t-1,  1:t) p(ct|c1:t-1,  1:t) log p(ct|c1:t-1,  1:t)

t c1:t-1, 1:t

ct

= H(c) +

p(c1:t-1,  1:t)[DKL(p(·|c1:t-1,  1:t) q(·|c1:t-1,  1:t))

t c1:t-1, 1:t

+ p(ct|c1:t-1,  1:t) log q(ct|c1:t-1,  1:t)]

ct

 H(c) +

p(c1:t-1,  1:t) p(ct|c1:t-1,  1:t) log q(ct|c1:t-1,  1:t) .

t c1:t-1, 1:t

ct

(7)

Here  1:t = (s1, · · · , at-1, st). The lower bound in equation 7 requires us to know the true posterior distribution to compute the expectation. To avoid sampling from p(ct|c1:t-1,  1:t), we use the following,

p(c1:t-1,  1:t) p(ct|c1:t-1,  1:t) log q(ct|c1:t-1,  1:t)

c1:t-1  1:t

ct

= p(c1:t-1,  1:t)p(ct|c1:t-1,  1:t) log q(ct|c1:t-1,  1:t)

c1:t-1  1:t ct

= p(ct, c1:t-1,  1:t) log q(ct|c1:t-1,  1:t)]

c1:t-1  1:t ct

= p( 1:t|ct, c1:t-1)p(ct, c1:t-1) log q(ct|c1:t-1,  1:t)

c1:t-1  1:t ct

= p(c1:t)

p( 1:t|ct, c1:t-1) log q(ct|c1:t-1,  1:t)

c1:t

 1:t

= p(c1:t)

p( 1:t|c1:t-1) log q(ct|c1:t-1,  1:t)

c1:t

 1:t

(8)

where the last step follows from the causal restriction that future provided variables (ct) do not influence earlier predicted variables ( 1:t consists of states up to time t. ct does not effect state st).
Putting the result in equation 8 in equation 7 gives,

L1(, q) =

Ec1:tp(c1:t),at-1(·|st-1,c1:t-1) log q(ct|c1:t-1,  1:t) + H (c)  I (  c) (9)

t

11

Under review as a conference paper at ICLR 2019

Environment
Discrete Circle-World Pendulum (both) Hopper-v2 Walker2d-v2

Directed Info-GAIL

Epochs Batch Size posterior 

1000 1000 2000 5000 5000

256 512 1024 4096 8192

0.1 0.01 0.01 0.01 0.001

VAE pre-training

Epochs Batch Size

500 1000 1000 2000 2000

32 16 16 32 32

Table 2: Experiment settings for all the different environments for both DirectedInfo-GAIL and VAE-pretraining step respectively.

(a) (b)
Figure 6: Latent variable assignment on the expert trajectories in Circle-World (a) with and (b) without smoothing penalty Ls. Blue and green colors represent the two different values of the context variable. The centres of the two circles are shifted for clarity.

Thus, by maximizing directed information instead of mutual information, we can learn a posterior distribution over the next latent factor c given the latent factors discovered up to now and the trajectory followed up to now, thereby removing the dependence on the future trajectory. In practice, we do not consider the H(c) term. This gives us the objective,

min max
,q D

E[log D(s, a)] + EE [1 - log D(s, a)] - 1L1(, q) - 2H().

In practice, we fix q from the VAE pre-training and only minimize over the policy  in equation 4.

A.2 IMPLEMENTATION DETAILS
Table 2 lists the experiment settings for all of the different environments. We use multi-layer perceptrons for our policy (generator), value, reward (discriminator) and posterior function representations. Each network consisted of 2 hidden layers with 64 units in each layer and ReLU as our non-linearity function. We used Adam (Kingma & Ba, 2014) as our optimizer setting an initial learning rate of 3e-4. Further, we used the Proximal Policy Optimization algorithm (Schulman et al., 2017) to train our policy network with = 0.2. For the VAE pre-training step we set the VAE learning rate also to 3e-4. For the Gumbel-Softmax distribution we set an initial temperature  = 5.0. The temperature is annealed using using an exponential decay with the following schedule  = max(0.1, exp-kt), where k = 3e - 3 and t is the current epoch.

A.3 CIRCLE-WORLD SMOOTHING

In the Circle-World experiment, we added another loss term Ls to VAE pre-training loss LV AE, which penalizes the number of times the latent variable switches from one value to another.

Ls =
t

1

-

ct-1 · ct max(||ct-1||2, ||ct||2)

12

Under review as a conference paper at ICLR 2019

(a) Hopper

(b) Walker: View 1

(c) Walker: View 2

Figure 7: PCA Visualization for Hopper and Walker environment with sub-task latent variable of size 4.

5.0

4.5

4.0

3.5

3.0

2.5

2.0

1.5

1.0 0

200 400 600 800 1000

(a) Sub-Task Latent Variable

Context 1 Context 3 Context 5
(b) PCA Visualization for Sub-Task Latent Variable

Figure 8: Results on Hopper environment with sub-task latent variable of size 8.

Figure 6 shows the segmentation of expert trajectories with and without the Ls term. We observed that without adding the smoothing penalty, the VAE learns to segment the expert trajectories into semi-circles as shown in Figure 6(a). While a valid solution, this does not match with the intuitive segmentation of the task into two sub-tasks of drawing circles in clockwise and counter-clockwise directions. The smoothing term can be thought of as a prior, forcing the network to change the latent variable as few times as possible. This helps reach a solution where the network switches between latent variables only when required. Figure 6(b) shows an example of segmentation obtained on expert trajectories after smoothing. Thus, adding more terms to the VAE pre-training loss can be a good way to introduce priors and bias solutions towards those that match with human notion of sub-tasks.
A.4 PCA VISUALIZATION OF SUB-TASKS
In Figure 7, we show the plots expert states, reduced in dimensionality using Principal Component Analysis (PCA), in Hopper and Walker environments. States are color coded by the latent code assigned at these states. We reduced the dimension of states in Hopper from 11 to 2 and in Walker from 17 to 3. These low dimensional representations are able to cover  90% of variance in the states. As can be seen in the figure, states in different parts of the space get assigned different latent variables. This further shows that our proposed approach is able to segment trajectories in such a way so that states that are similar to each other get assigned to the same segment (latent variable).
A.5 USING LARGER CONTEXT
For the following discussion we will represent a k-dimensional categorical variable as belonging to k-1 simplex. To observe how the dimensionality of the sub-task latent variable affects our proposed
13

Under review as a conference paper at ICLR 2019 approach we show results with larger dimensionality for the categorical latent variable ct. Since DirectedInfo-GAIL infers the sub-tasks in an unsupervised manner, we expect our approach to output meaningful sub-tasks irrespective of the dimensionality of ct. Figure 8 shows results for using a higher dimensional sub-task latent variable. Precisely, we assume ct to be a 8-dimensional one hot vector, i.e., ct  7. As seen in the above figure, even with a larger context our approach identifies similar basic action primitives as done previously when ct  3. This shows that despite larger dimensionality our approach is able to reuse appropriate context inferred previously. We also visualize the context values for the low-dimensional state-space embedding obtained by PCA. Although not perfectly identical, these context values are similar to the visualizations observed previously for ct  3. Thus our proposed approach is able, to some extent, infer appropriate sub-task representations independent of the dimensionality of the context variable.
14

