Under review as a conference paper at ICLR 2019
POINCARE´ GLOVE: HYPERBOLIC WORD EMBEDDINGS
Anonymous authors Paper under double-blind review
ABSTRACT
Words are not created equal. In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal. In this paper, driven by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect with the Gaussian word embeddings and their Fisher distance. We adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds. We explain how concepts from the Euclidean space such as parallel transport (used to solve analogy tasks) generalize to this new type of geometry. Moreover, we show that our embeddings exhibit hierarchical and hypernymy detection capabilities. We back up our findings with extensive experiments in which we outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection.
1 INTRODUCTION & MOTIVATION
Word embeddings are ubiquitous nowadays as first layers in neural network and deep learning models for natural language processing. They are essential in order to move from the discrete word space to the continuous space where differentiable loss functions can be minimized. The popular models of Glove (Pennington et al., 2014), Word2Vec (Mikolov et al., 2013) or FastText (Bojanowski et al., 2016), provide efficient ways to learn words vectors fully unsupervised from raw text corpora, solely based on word co-occurrence statistics. These models can be successfully applied to word similarity and other downstream tasks and, surprisingly (or not (Arora et al., 2016)), exhibit a linear algebraic structure that is also useful to solve word analogy.
However, unsupervised word embeddings still largely suffer from revealing asymmetric word relations including the latent hierarchical structure of words. This is currently one of the key limitations in automatic text understanding, e.g. for tasks such as textual entailment (Bowman et al., 2015). To address this issue, (Vilnis & McCallum, 2015; Muzellec & Cuturi, 2018) propose to move from point embeddings to probability density functions, the simplest being Gaussian or Elliptical distributions. Their intuition is that the variance of such a distribution should encode the generality/specificity of the respective word. However, this method results in losing the arithmetic properties of point embeddings (e.g. for analogy reasoning) and becomes unclear how to properly use them in downstream tasks. To this end, we propose to take the best from both worlds: we embed words as points in a Cartesian product of hyperbolic spaces and, additionally, explain how they are bijectively mapped to Gaussian embeddings with diagonal covariance matrices, where the hyperbolic distance between two points becomes the Fisher distance between the corresponding probability distribution functions (PDFs). We learn these embeddings unsupervised from raw text by generalizing the Glove method. Moreover, the linear arithmetic property used for solving word analogy has a mathematical grounded correspondence in this new space based on the notion of parallel transport. In addition, these hyperbolic embeddings exhibit hierarchical structure useful for hypernymy detection (via their PDFs) and outperform Euclidean Glove on similarity benchmarks. Finally, they can be used as inputs for downstream tasks as explained by the recent work of Ganea et al. (2018b).
We provide additional reasons for choosing the hyperbolic geometry to embed words. We explain the notion of average -hyperbolicity of a graph, a geometric quantity that measures its "democracy" (Borassi et al., 2015). A small hyperbolicity constant implies "aristocracy", namely the existence of a small set of nodes that "influence" most of the paths in the graph. It is known that real-world graphs are mainly complex networks (e.g. scale-free exhibiting power-law node degree distributions) which in turn are better embedded in a tree-like space, i.e. hyperbolic (Krioukov et al., 2010). Since,
1

Under review as a conference paper at ICLR 2019

intuitively, words form an "aristocratic" community (few generic ones defining different topics; many specific ones) and since a big subset of them exhibits a hierarchical structure (e.g. WordNet (Miller et al., 1990)), it is naturally to learn hyperbolic word embeddings. Moreover, we empirically measure very low average -hyperbolicity constants of some variants of the word log-co-occurrence graph (used by the Glove method), providing additional quantitative reasons for why spaces of negative curvature (i.e. hyperbolic) are better suited for word representations.

2 RELATED WORK
Recent supervised methods can be applied to embed any tree or directed acyclic graph in a low dimensional space with the aim of improving link prediction either by imposing a partial order in the embedding space (Vendrov et al., 2015; Vilnis et al., 2018; Athiwaratkun & Wilson, 2018), by using hyperbolic geometry (Nickel & Kiela, 2017; 2018), or both (Ganea et al., 2018a).
To learn word embeddings that exhibit hypernymy or hierarchical information, supervised methods (Vulic´ & Mrksic´, 2018; Nguyen et al., 2017) leverage external information (e.g. WordNet) together with raw text corpora. However, the same goal is also targeted by more ambitious fully unsupervised models which move away from the "point" assumption and learn various probability densities for each word (Vilnis & McCallum, 2015; Muzellec & Cuturi, 2018; Athiwaratkun & Wilson, 2017; Singh et al., 2018).
There have been two very recent attempts at learning unsupervised word embeddings in the hyperbolic space (Leimeister & Wilson, 2018; Dhingra et al., 2018). However, they suffer from either not being competitive on standard tasks, not showing the benefit of using hyperbolic spaces or not being trained on a realistically large corpus. We address these problems and, moreover, the connection with density based methods is made explicit and leveraged to improve hypernymy detection.

3 HYPERBOLIC SPACES AND THEIR CARTESIAN PRODUCT

In order to work in the hyperbolic space, we

have to choose one model, among the five iso-

metric models that exist. In this work, the one

that we choose to embed words in is the Poincare´

ball Dn = {x  Rn | x 2 < 1}, illustrated in Figure 1a for n = 2, where dark lines represent

geodesics. The distance function is given by

(a)

(b)

(c)

(d)

dDn (x, y) = cosh-1 1 + xy x - y 22/2 , x := 2/(1 - x 22) being the conformal fac-

Figure 1: Isometric deformation  of D2 into H2.

tor. We will also embed words in products of

hyperbolic spaces, and explain why later on. A product of p balls (Dn)p, with the induced product

geometry, is known to have distance function d(Dn)p (x, y)2 =

p i=1

dDn (xi,

yi)2.

Finally,

another

model of interest for us is the Poincare´ half-plane H2 = R × R+ illustrated in Figure 1d, with distance

function dH2 (x, y) = cosh-1 1 + x - y 22/(2y1y2) . Figure 2 shows an isometry  from D2 to

H2 mapping the vertical segment {0} × (-1, 1) to R+ and fixing (0, 1), sending the radius to .

4 ADAPTING GLOVE
Euclidean GLOVE. The original GLOVE (Pennington et al., 2014) algorithm is an unsupervised method for learning word representations in a Euclidean space from statistics of word co-occurences in a corpus of texts, with the aim to capture some of the words' meaning and relations geometrically.
We use standard notations: Xij is the number of times word j occurs in the context of word i; Xi = k Xik; Pij = Xij/Xi is the probability that word j appears in the context of word i. An embedding of a (target) word i is written wi, while an embedding of a context word k is written w~k.
The initial formulation of the GLOVE model suggests to learn embeddings as to satisfy the equation wiT w~k = log(Pik) = log(Xik) - log(Xi). Since Xik is symmetric in (i, k) but Pik is not, they

2

Under review as a conference paper at ICLR 2019

propose to restore the symmetry by introducing biases for each word, absorbing log(Xi) into i's bias:

wiT w~k + bi + ~bk = log(Xik).

(1)

Finally, the authors suggest to enforce this equality by optimizing a weighted least-square loss:

J=

V

f (Xij )

wiT w~j + bi + ~bj - log Xij

2
,

i,j=1

(2)

where V is the size of the vocabulary and f downweights the signal coming from frequent words (it is typically chosen to be f (x) = min{1, (x/xm)}, with  = 3/4 and xm = 100).

GLOVE in metric spaces. Note that there is no clear correspondence of the Euclidean inner-product

in a hyperbolic space. However, we are provided with a distance function. Further notice that one

could

rewrite

Eq.

(1)

with

the

Euclidean

distance

as

-

1 2

wi - w~k

2 + bi + ~bk = log(Xik), where we

absorbed the squared norms of the embeddings into the biases. We thus replace the GLOVE loss by:

J=

V

f (Xij )

-h(d(wi, w~j)) + bi + ~bj - log Xij

2
,

i,j=1

(3)

where h is a function to be chosen as a hyperparameter of the model, and d can be any differentiable distance function. Although the most direct correspondence with GLOVE would suggest h(x) = x2/2, we sometimes obtained better results with other functions, such as h = cosh2 (see sections 7 & 8).
Note that De Sa et al. (2018) also apply cosh to their distance matrix for hyperbolic MDS before applying PCA. Understanding why h = cosh2 is a good choice would be interesting future work.

5 CONNECTING GAUSSIAN EMBEDDINGS & HYPERBOLIC EMBEDDINGS

In order to endow Euclidean word embeddings with richer information, Vilnis & McCallum (2015) proposed to represent words as Gaussians, i.e. with a mean vector and a covariance matrix1, expecting the variance parameters to capture how generic/specific a word is, and hopefully entailment relations. On the other hand, Nickel & Kiela (2017) proposed to embed words of the WordNet hierarchy (Miller et al., 1990) in hyperbolic space, because this space is mathematically known to be better suited to embed tree-like metric spaces. It is hence natural to wonder: is there a connection between the two?

The Fisher geometry of Gaussians is hyperbolic. It turns out that there exists a striking connec-

tion (Costa et al., 2015). Note that a 1D Gaussian N (µ, 2) can be represented as a point (µ, ) in

R × R+. Then, the Fisher distance between two distributions relates to the hyperbolic distance in H2:

  dF N (µ, 2), N (µ ,  2) = 2dH2 (µ/ 2, ), (µ / 2,  ) .

(4)

For n-dimensional Gaussians with diagonal covariance matrices written  = diag()2, it becomes:

dF (N (µ, ), N (µ ,  )) =

n  2 2dH2 (µi/ 2, i), (µi/ 2, i) .
i=1

(5)

Hence there is a direct correspondence between diagonal Gaussians and the product space (H2)n.

Fisher distance, KL & Gaussian embeddings. The above paragraph lets us relate the WORD2GAUSS algorithm (Vilnis & McCallum, 2015) to hyperbolic word embeddings. Although one could object that WORD2GAUSS is trained using a KL divergence, while hyperbolic embeddings relate to Gaussian distributions via the Fisher distance dF , let us remind that KL and dF define the same local geometry. Indeed, the KL is known to be related to dF , as its local approximation (Jeffreys, 1946). In short, if P ( + d) and P () denote two closeby probability distributions for a small d, then KL(P ( + d)||P ()) = (1/2) ij gijdidj + O( d 3), where (gij)ij is the Fisher information metric, inducing dF .
1diagonal or even spherical, for simplicity.

3

Under review as a conference paper at ICLR 2019

Riemannian optimization. A benefit of representing words in (products of) hyperbolic spaces, as opposed to (diagonal) Gaussian distributions, is that one can use recent Riemannian adaptive optimization tools such as RADAGRAD (Be´cigneul & Ganea, 2018). Note that without this connection, it would be unclear how to define a variant of ADAGRAD (Duchi et al., 2011) intrinsic to the statistical manifold of Gaussians. Empirically, we indeed noticed better results using RADAGRAD, rather than simply Riemannian SGD (Bonnabel, 2013). Similarly, note that GLOVE trains with ADAGRAD.

6 ANALOGIES AND ENTAILMENT FOR HYPERBOLIC/GAUSSIAN EMBEDDINGS
The connection exposed in section 5 allows us to provide mathematically grounded (i) analogy computations for Gaussian embeddings using hyperbolic geometry, and (ii) hypernymy detection for hyperbolic embeddings using Gaussian distributions.

6.1 COMPUTING ANALOGIES
A common task used to evaluate word embeddings, called analogy, consists in finding which d is to c, what b is to a. For instance, queen is to woman what king is to man. In the Euclidean space, the solution to this problem is usually taken as d = c + (b - a) = b + (c - a), which is also to b, what c is to a. But how should one intrinsically define "analogy parallelograms" in a space of Gaussian distributions? Note that Vilnis & McCallum (2015) do not evaluate their Gaussian embeddings on the analogy task, and that it would be unclear how to do so. However, since we can go back and forth between (diagonal) Gaussians and (products of) hyperbolic spaces as explained in section 5, we can use the fact that parallelograms are naturally defined in the Poincare´ ball, by the notion of gyro-translation (Ungar, 2012, section 4). In the Poincare´ ball, the two solutions d1 = c + (b - a) and d2 = b + (c - a) are respectively generalized to

d1 = c  gyr[c, a]( a  b), and d2 = b  gyr[b, a]( a  c).

(6)

The formulas for these operations are described in closed-forms in appendix C, and are easy to

implement. The fact that these two differ is due to the curvature of the space. For evaluation, we chose

the gyro-midpoint between these two solutions, defined as md1d2 := d1  ((-d1  d2)  (1/2)) = md2d1 , which is the point located on the geodesic between d1 and d2, at equal hyperbolic distance between from d1 as from d2. Note that one can rewrite Eq. (6) with tools from differential geometry
as

c  gyr[c, a]( a  b) = expc(Pac(loga(b))),

(7)

where Pxy = (x/y)gyr[y, x] denotes the parallel transport along the unique geodesic from x
to y. The exp and log maps of Riemannian geometry were related to the theory of gyrovector spaces
(Ungar, 2008) by Ganea et al. (2018b), who also mention that when continuously deforming the hyperbolic space Dn into the Euclidean space Rn, sending the curvature from -1 to 0 (i.e. the radius of Dn from 1 to ), the Mo¨bius operations , , , gyr recover their respective Euclidean
counterparts +, -, ·, Id. Hence, the analogy solutions d1, d2, md1d2 of Eq. (6) would then all recover d = c + b - a, which seems a nice sanity check.

6.2 A SCORE FOR ENTAILMENT/HYPERNYMY
Poincare´ embeddings. Nickel & Kiela (2017) use a heuristic entailment score in order to predict whether u is a v, defined for u, v  Dn as is-a(u, v) := -(1 + ( v 2 - u 2))d(u, v), where  = 103, based on the intuition that higher concepts in the hierarchy should be embedded with a lower Euclidean norm. However, such a choice is not intrinsic to the hyperbolic space. The loss that they use for training only involves the distance function, hence training is intrinsic to Dn, i.e. invariant to applying any isometry  : Dn  Dn to all word embeddings, but their "is-a" score is not.
The importance of fixing the isometry. We argue that generality of a concept as being embedded in a Poincare´ ball is better captured by a direction in hyperbolic space, i.e. by a geodesic, rather than by the distance from the origin. Why? For a 1D Gaussian N (µ, 2) representing a concept, generality can be nicely encoded in the magnitude of . As shown in section 5, we can map the space of Gaussians endorsed with the Fisher distance to the hyperbolic upper half-plane H2, where 

4

Under review as a conference paper at ICLR 2019
corresponds to the (positive) second coordinate in H2 = R × R+. Moreover, as shown in section 3, H2 can be isometrically mapped to D2, where the second coordinate   R+ corresponds to the open vertical segment {0}×(-1, 1) in D2. However, in D2, any rotation w.r.t. the origin or any (hyperbolic) translation is an isometry2. Hence, in order to map a word x  D2 to a Gaussian N (µ, 2) via H2, we first have to align {0} × (-1, 1) with whichever geodesic in D2 encodes generality by applying the correct isometry to D2 (centering, rotation). These three steps are illustrated in Figure 2.

Figure 2: We show here one of the D2 of some 20D embeddings trained unsupervised with our hyperbolic GLOVE in (D2)10. These images illustrate the three steps of applying the isometry. From
left to right: the trained embeddings; after centering; after rotation; after isometrically mapping them to H2 as explained in section 3. The isometry was obtained with the semi-supervised method described below, using 400 top and 400 bottom words from WordNet. Experimental details: h(x) = x2;
unrestricted vocabulary of 180k words; learning-rate of 0.05.

Gaussian embeddings. Vilnis & McCallum (2015) propose using is-a(P, Q) := -KL(P ||Q) for distributions P, Q, the argument being that a low KL(P ||Q) indicates that we can encode Q
easily as P , implying that Q entails P . However, we would like to mitigate this statement. Indeed, if P = N (µ, ) and Q = N (µ,  ) are two 1D Gaussian distributions with same mean, then KL(P ||Q) = z2 - 1 - log(z) where z := / , which is not a monotonic function of z. This breaks
the idea that the magnitude of the variance should encode the generality/specificity of the concept.

Another entailment score for Gaussian embeddings. What would constitute a good number for the variance's magnitude of a n-dimensional Gaussian distribution N (µ, )? It is known that 95%

of its mass is contained within a hyperellipsoid of volume V = Vn det(), where Vn denotes

the volume of a ball of radius 1 in Rn. For simplicity, we propose dropping the dependence in µ

and define a simple score is-a(,  ) := log(V) - log(V) =

n i=1

(log(i)

-

log(i)).

Note

that

using difference of logarithms has the benefit of removing the scaling constant Vn, and makes the

entailment score invariant to a rescaling of the covariance matrices: is-a(r, r ) = is-a(,  ).

Computing the score for hyperbolic embeddings. To compute our is-a score between two hyperbolic embeddings, we first map them to Gaussians and then apply the above proposed is-a score. However, as previously explained, in order to map a point x  (Dn)p to a diagonal gaussian N (µ, ), one first has to fix the correct isometry. To do so, we start by identifying two sets G and S of potentially generic and specific words, respectively. For the re-centering, we then compute the means g and s of G and S respectively, and m := (s + g)/2, and Mo¨bius translate all words by the global mean with the operation w  m  w. For the rotation, we set u := ( m  g)/ m  g 2, and rotate all words so that u is mapped to (0, 1). Figure 2 illustrates these steps. In order to identify the two sets G and S, we propose (i) a fully unsupervised method, and (ii) a semi-supervised one, using a (varying) number of words from the WordNet hierarchy. In (i), we first define a restricted vocabulary of the 50k most frequent words among the unrestricted one of 200k words, and rank them by frequency; we then define G as the 5k most frequent ones, and S as the 5k least frequent ones of the 50k vocabulary (to avoid extremely rare words which might have received less signal during training). In (ii), we define G as k top words from the top 4 levels of the WordNet hierarchy, and S as k bottom words from the bottom 3 levels. Results are described in section 8. As can be seen, the semi-supervised method works better, but using only k = 20 top and bottom words -i.e. a very small amount of supervision - already gives good improvements.
2See http://bulatov.org/math/1001 for intuitive animations describing hyperbolic isometries.

5

Under review as a conference paper at ICLR 2019

h(x) log(x2)
x x2
cosh(x) cosh2(x) cosh3(x) cosh4(x) cosh5(x) cosh10(x)

davg 18950.474 18.9505
4.3465 3.68 2.3596 1.9716 1.7918 1.6888 1.4947

avg 8498.6474
0.7685 0.088 0.0384 0.0167 0.0102 0.0072 0.0056 0.0026

2avg /davg 0.8969 0.0811 0.0405 0.0209 0.0142 0.0103 0.0081 0.0066 0.0034

Table 1: -hyperbolicity, average distance and their ratio computed via sampling for the metrics given by different h functions.

7 EMBEDDING SYMBOLIC DATA IN A CONTINUOUS SPACE WITH MATCHING
HYPERBOLICITY
Why would we embed words in a hyperbolic space? Given some symbolic data, such as a vocabulary along with similarity measures between words - in our case, co-occurence counts Xij - can we understand in a principled manner which geometry would represent it best? Choosing the right metric space to embed words can be understood as selecting the right inductive bias - an essential step.
-hyperbolicity. A particular quantity of interest describing qualitative aspects of metric spaces is the -hyperbolicity, introduced by Gromov (1987). However, for various reasons discussed in appendix B, we used the averaged -hyperbolicity, denoted avg, defined by Albert et al. (2014). Intuitively, a low avg of a finite metric space characterizes that this space has an underlying hyperbolic geometry, i.e. an approximate tree-like structure, and that the hyperbolic space would be well suited to isometrically embed it. We also report the ratio 2  avg/davg (invariant to metric scaling), where davg is the average distance in the finite space, as suggested by Borassi et al. (2015), whose low value also characterizes the "hyperbolicness" of the space.
Computing avg. Since our methods are trained on a weighted graph of co-occurences, we want to compute its hyperbolicity avg. However, in order to do so, one needs to be provided with a distance d(i, j) for each pair of words (i, j), while our symbolic data is only made of similarities measures. Note that we cannot associate the value - log(Pij) to d(i, j), as this quantity is not symmetric. Instead, inspired from Eq. (3), we associate to words i, j the distance3 h(d(i, j)) := - log(Xij) + bi + bj  0 with the choice bi := log(Xi), i.e. d(i, j) = h-1(log((XiXj)/Xij)).
Table 1 shows values for different choices of h. The discrete metric spaces we obtained for our symbolic data of co-occurences appear to have a very low hyperbolicity, i.e. to be very much "hyperbolic", which suggests to embed words in (products of) hyperbolic spaces. We report in section 8 empirical results for h(x) = x2 and h(x) = cosh(x).
8 EXPERIMENTS: SIMILARITY, ANALOGY, ENTAILMENT
We trained all models on a corpus provided by Omer Levy and which was also used to report the results on some works on word embeddings. The preprocessing of the corpus is explained in detail in these works (Levy & Goldberg, 2014; Levy et al., 2015). The dataset has been obtained from an English Wikipedia dump and contains 1.4 billion tokens. Words that appear less than 100 times in the corpus have been discarded leaving 189,533 unique tokens. The co-occurrence matrix contains 700 million non-zero entries, for a symmetric window size of 10.
We report results for both 100D embeddings trained in a 100D Poincare´ ball, and for 50x2D embeddings, which were trained in the Cartesian product of 50 2D Poincare´ balls. Note that in the
3One can replace log(x) with log(1 + x) to avoid computing the logarithm of zero.
6

Under review as a conference paper at ICLR 2019

case of both models, one word will be represented by a 100D array. For the Poincare´ models we employ both h(x) = x2 and h(x) = cosh2(x).
We compare against a 100D Euclidean GloVe model which was trained using the hyperparameters suggested in the work that introduced GloVe.
All models were trained for 50 epochs, on the full corpus of 189,533 word types. The vanilla GloVe model has been trained using AdaGrad. All hyperbolic models have been trained using RAdaGrad. For the Euclidean baseline as well as for models with h(x) = x2 we used a learning rate of 0.05. For Poincare´ models with h(x) = cosh2(x) we used a learning rate of 0.01. All models have biases.
For another experiment setting we initialize our Poincare´ model with pretrained parameters. These were obtained by training the same model and only accounting for the most frequent 50,000 word (instead of considering the full 189,533. This will be referred to as the "initialization trick". For fairness, we also trained the vanilla (Euclidean) GloVe model in the same fashion.
Word similarity is assessed using a number of well established benchmarks shown in the Table 2.

Experiment name
100D Vanilla GloVe
100D Vanilla GloVe w/ init trick
100D Poincare´ GloVe h(x) = cosh2(x) w/ init trick
50x2D Poincare´ GloVe h(x) = cosh2(x)
50x2D Poincare´ GloVe h(x) = x2

Rare Word 0.3798 0.3787
0.4187
0.4111 0.4106

WordSim 0.5901 0.5668
0.6209
0.6367 0.5844

SimLex 0.2963 0.2964
0.3208
0.3084 0.3007

SimVerb 0.1675 0.1639
0.1915
0.1811 0.1725

MC 0.6524 0.6562
0.7833
0.7748 0.7586

RG 0.6894 0.6757
0.7578
0.725 0.7236

Table 2: Word similarity results for 100-dimensional models.

For word analogy, we evaluate on the Google benchmark introduced by Mikolov and its two splits that contain semantic and syntactic analogy queries. We also use a benchmark by MSR that is also commonly employed in other word embedding works. For the Euclidean baselines we use 3COSADD. After applying hyperbolic parallel transport, the best candidate is selected to be the closest word to this point in terms of either the cosine distance, or the Poincare´ distance.

Experiment name
100D Vanilla GloVe
100D Vanilla GloVe w/ init trick
100D Poincare´ GloVe h(x) = cosh2(x), w/ init. trick
50x2D Poincare´ GloVe h(x) = x2

Method 3COSADD 3COSADD
Cosine dist
Poincare´ dist

SemGoogle 0.6005 0.6427
0.6641
0.644

SynGoogle 0.5869 0.595
0.6209
0.5967

Google 0.5931 0.6167
0.6339
0.6182

MSR 0.4868 0.4826
0.518
0.4587

Table 3: Word analogy results for 100-dimensional models.

7

Under review as a conference paper at ICLR 2019

sixties dance daughter vapor ronaldo mechanic algebra

seventies, eighties, nineties, 60s, 70s, 60s, 1960s, 80s, 90s, 70s dancing, dances, music, singing, musical, performing, hiphop, pop, folk, dancers wife, married, mother, cousin, son, niece, granddaughter, husband, sister, eldest vapour, refrigerant, liquid, condenses, supercooled, fluid, gaseous, gases, droplet, ammonia cristiano, ronaldinho, rivaldo, messi, zidane, romario, pele, zinedine, xavi, robinho electrician, fireman, machinist, welder, technician, builder, janitor, trainer, brakeman, fitter algebras, homological, heyting, geometry, subalgebra, quaternion, calculus, mathematics, unital, algebraic

Table 4: Nearest neighbors for some words for a 100D hyperbolic embedding model. Poincare´ distance has been used for selecting neighbors.

reptile
food music
sport

amphibians, carnivore, crocodilian, fish-like, dinosaur, spinosaurus, alligator, triceratops meal, lunch, breakfast, coffee, rice, seafood, fish performance, composition, contemporary, vocal, folk, rock, jazz, electroacoustic competition, challenge, fitness, outdoor, cycling, tennis, pankration

Table 5: Some words selected from the 100 nearest neighbors and ordered according to the I hypernymy score function for a 50x2D hyperbolic embedding model using h(x) = x2.

Figure 3: Hierarchies for some concepts captured by a 10x2D model trained with h(x) = x2. Note how different 2D spaces preserve different semantic features better than others.
9 CONCLUSION AND FUTURE WORK
We propose to adapt the GloVe algorithm to hyperbolic spaces, and to leberage a connection between statistical manifolds of Gaussian distributions and hyperbolic geometry, in order to better interpret entailment relations between hyperbolic embeddings. We justify the choice of products of hyperbolic spaces as an embedding space via this connection to Gaussian distributions, and via computations of the hyperbolicity of the symbolic data on which GloVe is based. Future work include running experiments to the hyperboloid model, which has been reported to lead to lesser numerical instabilities.
REFERENCES
Re´ka Albert, Bhaskar DasGupta, and Nasim Mobasheri. Topological implications of negative curvature for biological and social networks. Physical Review E, 89(3):032811, 2014.
8

Under review as a conference paper at ICLR 2019

Supervised methods Unsupervised methods
Our models 50x2D, Poincare GloVe, f(dist)=cosh-dist-sq, vocab=180k
50x2D, Poincare GloVe, f(dist)=dist-sq, vocab=180k
50x2D, Poincare GloVe, f(dist)=cosh-dist-sq, vocab=50k
50x2D, Poincare GloVe, f(dist)=dist-sq, vocab=50k

(Kiela et al., 2015) (Weeds et al., 2014) (Nguyen et al., 2017) SGNS from (Nguyen et al., 2017)
WordNet 20+20 WordNet 400+400 Unsupervised 5k+5k WordNet 20+20 WordNet 400+400 Unsupervised 5k+5k WordNet 20+20 WordNet 400+400 Unsupervised 5k+5k WordNet 20+20 WordNet 400+400 Unsupervised 5k+5k

0.75 0.75 0.87 0.48
0.749 0.761 0.574 0.771
0.76 0.647 0.739 0.813 0.630 0.752 0.810 0.649

Table 6: WBLESS results

Supervised methods Unsupervised methods

OrderEmb PARAGRAM + CF WN-Basic WN-WuP WN-LCh WN-Eucl from (Nickel & Kiela, 2017) WN-Poincare Word2Gauss-DistPos SGNS-Deps Frequency SLQS-Slim Vis-ID

0.191 0.32 0.24
0.214 0.214 0.389 0.512 0.206 0.205 0.279 0.229 0.253

Our models (supervised and unsupervised) 50x2D, Poincare GloVe, f(dist)=cosh-dist-sq, 50ep, vocab=180k
50x2D, Poincare GloVe, f(dist)=dist-sq, 50ep, vocab=180k 50x2D, Poincare GloVe, f(dist)=cosh-dist-sq, 50ep, vocab=50k
50x2D, Poincare GloVe, f(dist)=dist-sq, 50ep, vocab=50k

WordNet 20+20 WordNet 400+400 Unsupervised 5k+5k WordNet 20+20 WordNet 400+400 Unsupervised 5k+5k WordNet 20+20 WordNet 400+400 Unsupervised 5k+5k WordNet 20+20 WordNet 400+400 Unsupervised 5k+5k

0.359 0.411 0.277 0.365 0.413 0.310 0.368 0.412 0.295 0.365 0.430 0.309

Table 7: Hyperlex results. Baseline results are mostly taken from (Nickel & Kiela, 2017; Vulic´ et al., 2017)

9

Under review as a conference paper at ICLR 2019
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. Transactions of the Association for Computational Linguistics, 4:385­399, 2016.
Ben Athiwaratkun and Andrew Wilson. Multimodal word distributions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1645­1656, 2017.
Ben Athiwaratkun and Andrew Gordon Wilson. Hierarchical density order embeddings. arXiv preprint arXiv:1804.09843, 2018.
Gary Be´cigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. arXiv preprint, 2018.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016.
Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic Control, 58(9):2217­2229, 2013.
Michele Borassi, Alessandro Chessa, and Guido Caldarelli. Hyperbolicity measures democracy in real-world networks. Physical Review E, 92(3):032812, 2015.
Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 632­642, 2015.
Wei Chen, Wenjie Fang, Guangda Hu, and Michael W Mahoney. On the hyperbolicity of small-world and treelike random graphs. Internet Mathematics, 9(4):434­491, 2013.
Sueli IR Costa, Sandra A Santos, and Joa~o E Strapasson. Fisher information distance: a geometrical reading. Discrete Applied Mathematics, 197:59­69, 2015. URL https://arxiv.org/pdf/ 1210.2354.pdf.
Christopher De Sa, Albert Gu, Christopher Re´, and Frederic Sala. Representation tradeoffs for hyperbolic embeddings. In International Conference on Machine Learning, 2018.
Bhuwan Dhingra, Christopher Shallue, Mohammad Norouzi, Andrew Dai, and George Dahl. Embedding text in hyperbolic spaces. In Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12), pp. 59­69, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Octavian-Eugen Ganea, Gary Be´cigneul, and Thomas Hofmann. Hyperbolic entailment cones for learning hierarchical embeddings. In International Conference on Machine Learning, 2018a.
Octavian-Eugen Ganea, Gary Be´cigneul, and Thomas Hofmann. Hyperbolic neural networks. In Advances in Neural Information Processing Systems, 2018b.
Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pp. 75­263. Springer, 1987.
Harold Jeffreys. An invariant form for the prior probability in estimation problems. Proc. R. Soc. Lond. A, 186(1007):453­461, 1946.
Douwe Kiela, Laura Rimell, Ivan Vulic´, and Stephen Clark. Exploiting image generality for lexical entailment detection. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), volume 2, pp. 119­124, 2015.
Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Maria´n Boguna´. Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.
Matthias Leimeister and Benjamin J Wilson. Skip-gram word embeddings in hyperbolic space. arXiv preprint arXiv:1809.01498, 2018.
10

Under review as a conference paper at ICLR 2019
Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pp. 171­180. Association for Computational Linguistics, 2014. doi: 10.3115/v1/W14-1618. URL http://www.aclweb.org/anthology/W14-1618.
Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211­ 225, 2015. ISSN 2307-387X. URL https://transacl.org/ojs/index.php/tacl/ article/view/570.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111­3119, 2013.
George A Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J Miller. Introduction to wordnet: An on-line lexical database. International journal of lexicography, 3(4): 235­244, 1990.
Boris Muzellec and Marco Cuturi. Generalizing point embeddings using the wasserstein space of elliptical distributions. arXiv preprint arXiv:1805.07594, 2018.
Kim Anh Nguyen, Maximilian Ko¨per, Sabine Schulte im Walde, and Ngoc Thang Vu. Hierarchical embeddings for hypernymy detection and directionality. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 233­243, 2017.
Maximilian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. In International Conference on Machine Learning, 2018.
Maximillian Nickel and Douwe Kiela. Poincare´ embeddings for learning hierarchical representations. In Advances in Neural Information Processing Systems, pp. 6341­6350, 2017.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pp. 1532­43, 2014.
Sidak Pal Singh, Andreas Hug, Aymeric Dieuleveut, and Martin Jaggi. Wasserstein is all you need. arXiv preprint arXiv:1808.09663, 2018.
Abraham A Ungar. Beyond the Einstein addition law and its gyroscopic Thomas precession: The theory of gyrogroups and gyrovector spaces, volume 117. Springer Science & Business Media, 2012.
Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis Lectures on Mathematics and Statistics, 1(1):1­194, 2008.
Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. arXiv preprint arXiv:1511.06361, 2015.
Luke Vilnis and Andrew McCallum. Word representations via gaussian embedding. ICLR, 2015.
Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. Probabilistic embedding of knowledge graphs with box lattice measures. arXiv preprint arXiv:1805.06627, 2018.
Ivan Vulic´ and Nikola Mrksic´. Specialising word vectors for lexical entailment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pp. 1134­1145, 2018.
Ivan Vulic´, Daniela Gerz, Douwe Kiela, Felix Hill, and Anna Korhonen. Hyperlex: A large-scale evaluation of graded lexical entailment. Computational Linguistics, 43(4):781­835, 2017.
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir, and Bill Keller. Learning to distinguish hypernyms and co-hyponyms. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pp. 2249­2259. Dublin City University and Association for Computational Linguistics, 2014.
11

Under review as a conference paper at ICLR 2019
A MORE EXPERIMENTS IN LOWER DIMENSIONS
We show here extensive results in lower dimensions. Throughout this section, w denotes the target vector, c denotes the context vector, cosh-dist-sq and dist-sq denote using h(x) = cosh(x)2 and h(x) = x2 respectively; 50ep denotes training with 50 epochs; MIX-Poincare means that we used products of hyperbolic spaces; Vanilla GloVe designate the baseline; For analogy, we use either Poincare distance or cosine similarity to find the nearest neighbor after a gyro-parallel-transport. For analogy, Poincare distance or cosine similarity were used to compute the similarity score.
Figure 4: Full 180K vocabulary, similarity results. 12

Under review as a conference paper at ICLR 2019
Figure 5: Full 180K vocabulary, analogy results. 13

Under review as a conference paper at ICLR 2019
Figure 6: Restricted 50K (most frequent) vocabulary, similarity results. 14

Under review as a conference paper at ICLR 2019
Figure 7: Restricted 50K (most frequent) vocabulary, analogy results. 15

Under review as a conference paper at ICLR 2019

B -HYPERBOLICITY
Let us start by defining the -hyperbolicity, introduced by Gromov (1987). The hyperbolicity (x, y, z, t) of a 4-tuple (x, y, z, t) is defined as half the difference between the biggest two of the following sums: d(x, y) + d(z, t), d(x, z) + d(y, t), d(x, t) + d(y, z). The -hyperbolicity of a metric space is defined as the supremum of these numbers over all 4-tuples. Following Albert et al. (2014), we will denote this number by worst, and by avg the average of these over all 4-tuples, when the space is a finite set. An equivalent and more intuitive definition holds for geodesic spaces, i.e. when we can define triangles: its -hyperbolicity (worst) is the smallest  > 0 such that for any triangle xyz, there exists a point at distance at most  from each side of the triangle. Chen et al. (2013) and Borassi et al. (2015) analyzed worst and avg for specific graphs, respectively. A low hyperbolicity of a graph indicates that it has an undelrying hyperbolic geometry, i.e. that it is approximately tree-like, or at least that there exists a taxonomy of nodes. Conversely, a high hyperbolicity of a graph suggests that it possesses long cycles, or could not be embedded in a low dimensional hyperbolic space without distorsion. For instance, the Euclidean space Rn is not -hyperbolic for any  > 0, and is hencedescribed as -hyperbolic, while the Poincare´ disk D2 is known to have a -hyperbolicity of log(1 + 2) 0.88. On the other-hand, a product D2 × D2 is -hyperbolic, because a 2D plane R2 could be isometrically embedded in it using for instance the first coordinates of each D2. However, if D2 would constitute a good choice to embed some given symbolic data, then most likely D2 × D2 would as well. This stems from the fact that -hyperbolicity (worst) is a worst case measure which does not reflect what one could call the "hyperbolic capacity" of the space. Furthermore, note that computing worst requires O(n4) for a graph of size n, while avg can be approximated via sampling. Finally, avg is robust to adding/removing a node from the graph, while worst is not. For all these reasons, we choose avg as a measure of hyperbolicity.
C CLOSED-FORM FORMULAS OF MO¨ BIUS OPERATIONS
We show closed form expressions for the most common operations in the Poincare´ ball, but we refer the reader to (Ungar, 2008; Ganea et al., 2018b) for more details.

Mo¨bius addition.

The Mo¨bius addition of x and y in Dn is defined as

x  y :=

(1 + 2 x, y + y 1 + 2 x, y

2)x + (1 - + x2y2

x

2)y .

(8)

We define x y := x c (-y).

Mo¨bius scalar multiplication. The Mo¨bius scalar multiplication of x  Dn \ {0} by r  R is

defined as

r  x := tanh(r tanh-1( x )) x , x

(9)

and r  0 := 0.

Exponential and logarithmic maps. For any point x  Dn, the exponential map expx : TxDn  Dn and the logarithmic map logx : Dn  TxDn are given for v = 0 and y = x by:

expx(v) = x 

tanh

x v 2

v v

,

logx(y)

=

2 x

tanh-1(

-xy )

-x  y -xy

.

(10)

Gyro operator and parallel transport. Parallel transport is given for x, y  Dn, v  TxD by

the

formula

Pxy (v)

=

x y

· gyr[y, -x]v.

Gyr4

is

the

gyroautomorphism

on

Dn

with

closed

form

expression shown in Eq. 1.27 of (Ungar, 2008):

gyr[u, v]w = (u  v)  {u  (v  w)} = w + 2 Au + Bv D
4https://en.wikipedia.org/wiki/Gyrovector_space

(11)

16

Under review as a conference paper at ICLR 2019

where the quantities A,B,D have closed form expression and, thus, are easy to implement.

A = - u, w v 2 + v, w + 2 u, v · v, w B = - v, w u 2 - u, w D = 1 + 2 u, v + u 2 v 2

(12) (13) (14)

17

