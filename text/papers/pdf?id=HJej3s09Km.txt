Under review as a conference paper at ICLR 2019
ON THE EFFECT OF THE ACTIVATION FUNCTION
ON THE DISTRIBUTION OF HIDDEN NODES IN A DEEP NETWORK
Anonymous authors Paper under double-blind review
ABSTRACT
We analyze the joint probability distribution on the lengths of the vectors of hidden variables in different layers of a fully connected deep network, when the weights and biases are chosen randomly according to Gaussian distributions, and the input is in {-1, 1}N . We show that, if the activation function  satisfies a minimal set of assumptions, satisfied by all activation functions that we know that are used in practice, then, as the width of the network gets large, the "length process" converges in probability to a length map that is determined as a simple function of the variances of the random weights and biases, and the activation function . We also show that this convergence may fail for  that violate our assumptions.
1 INTRODUCTION
The size of the weights of a deep network must be managed delicately. If they are too large, signals blow up as they travel through the network, leading to numerical problems, and if they are too small, the signals fade away. The practical state of the art in deep learning made a significant step forward due to schemes for initializing the weights that aimed in different ways at maintaining roughly the same scale for the hidden variables before and after a layer [9, 4]. Later work [7, 14, 2] took into account the effect of the non-linearities on the length dynamics of a deep network, informing initialization policies in a more refined way.
In this paper, we continue this line of work, theoretically analyzing what might be called the "length process". That is, for a given input, chosen for simplicity from {-1, 1}N , we study the probability distribution over the lengths of the vectors of hidden variables, when the parameters of a deep network are chosen randomly. We analyze the case of fully connected networks, with the same activation function  at each hidden node and N hidden variables in each layer. As in [14], we consider the case where weights between nodes are chosen from a zero-mean Gaussian with variance w2 /N , and where the biases are chosen from a zero-mean distribution with variance b2. Our first result holds for activation functions  that satisfy the following properties: (a) the restriction of  to any finite interval is bounded; (b) as z gets large, |(z)| = exp(o(z2)); (c)  is measurable. We refer to such  as permissible. Note that conditions (a) and (c) both hold for any non-decreasing .
We show that, for all permissible  and all w and b, as N gets large, the length process converges in probability to a length map that is a simple function of , w and b. This length map was first discovered in [14], where it was claimed that it holds for all ; it has since been used in a number of other papers [15, 17, 12, 10, 16, 1, 13, 5].
In Section 4, to motivate our new analysis, we provide examples of  that are not permissible that lead the length processes with arguably surprising properties. For example, we show that, for arbitrarily small positive w, even if b = 0, for (z) = 1/z, the distribution of values of each of the hidden nodes in the second layer diverges as N gets large. For finite N , each node has a Cauchy distribution, which already has infinite variance, and as N gets large, the scale parameter of the Cauchy distribution gets larger, leading to divergence. We also show that the hidden variables in the
1

Under review as a conference paper at ICLR 2019

second layer may not be independent, even for some permissible  like the ReLU. The results of this section contradict claims made in [14, 10].
Section 5 describes some simulation experiments verifying some of the findings of the paper, and illustrating the dependence among the values of the hidden nodes.
Our analysis of the convergence of the length map borrows ideas from Daniely, et al. [2], who studied the properties of the mapping from inputs to hidden representations resulting from random Gaussian initialization. Their theory applies in the case of activation functions with certain smoothness properties, and to a wide variety of architectures. Our analysis treats a wider variety of values of w and b, and uses weaker assumptions on .

2 PRELIMINARIES

2.1 NOTATION
For n  N, we use [n] to denote the set {1, 2, . . . , n}. If T is a n × m × p tensor, then, for i  [n], let Ti,:,: = Ti,j,k jk, and define Ti,j,:, etc., analogously.

2.2 THE FINITE CASE
Consider a deep fully connected width-N network with D layers. Let W  RD×N×N . An activation function  maps R to R; we will also use  to denote the function from RN to RN obtained by applying  componentwise. Computation of the neural activity vectors x0,:, ..., xD,:  RN and preactivations h1,:, ..., hD,:  RN proceeds in the standard way as follows:
h ,: = W ,:,:x -1,: + b ,: x ,: = (h ,:), for = 1, . . . , D.

We will study the process arising from fixing an arbitrary input x0,:  {-1, 1}N and choosing the

parameters independently at random: the entries of W are sampled from Gauss

0,

w2 N

, and the

entries of b from Gauss 0, b2 . For each

 [D], define q

=

1 N

N i=1

h2,i.

Note that for all  1, all the components of h ,: and x ,: are identically distributed.

2.3 THE WIDE-NETWORK LIMIT
For the purpose of defining a limit, assume that, for a fixed, arbitrary function  : N  {-1, 1}, for finite N , we have x0,: = ((1), ..., (N )). For > 0, if the limit exists (in the sense of "convergence in distribution"), let x be a random variable whose distribution is the limit of the distribution of x ,1 as N goes to infinity. Define h and q similarly.

2.4 TOTAL VARIATION DISTANCE

If P their

and Q are densities,

probability dT V (P, Q)

distributions, then dT V (P,

=

1 2

|p(x) - q(x)| dx.

Q)

=

supE

P

(E)

-

Q(E),

and

if

p

and

q

are

3 CONVERGENCE IN PROBABILITY

In this section we characterize the length map of the hidden nodes of a deep network, for all activation functions satisfying the following assumptions.

Definition 1 An activation function  is permissible if, (a) the restriction of  to any finite interval is bounded; (b) |(x)| = exp(o(x2)) as |x| gets large.1; and (c)  is measurable.

Conditions (b) and (c) ensure that a key integral can be computed. The proof of Lemma 1 is in Appendix A.

1

This

condition

may

be

expanded

as

follows,

limsupx

log

|(x)| x2

=

0

and

limsupx-

log

|(x)| x2

=

0.

2

Under review as a conference paper at ICLR 2019

Lemma 1 If  is permissible, then, for all positive constants c, the function g defined by g(x) = (cx)2 exp(-x2/2) is integrable.

Now, we recall the definition of a length map from [14]; we will prove that the the length process

converges to this length map. Define q~0, ..., q~D and r~0, ..., r~D recursively as follows. First q~0 =

r~0 = 1. Then, for > 0,

q~ = w2 r~ -1 + b2

and r~ = EzGauss(0,1)[( q~ z)2].

If  is permissible, then, since (cz)2 exp(-z2/2) is integrable for all c, we have that q~0, ..., q~D, r~0, ..., r~D are well-defined finite real numbers.

The following theorem shows that the length map q0, ..., qD converges in probability to q~0, ..., q~D.

Theorem 2 For any permissible , w, b  0, any depth D, and any ,  > 0, there is an N0 such that, for all N  N0, with probability 1 - , for all  {0, ..., D}, we have |q - q~ |  .

The rest of this section is devoted to proving Theorem 2. Our proof will use the weak law of large numbers.

Lemma 3 ([3]) For any random variable X with a finite expectation, and any ,  > 0, there is an N0 such that, for all N  N0, if X1, ..., XN are i.i.d. with the same distribution as X, then

1N Pr E[X] - N Xi >
i=1

 .

In order to divide our analysis into cases, we need the following lemma, whose proof is in Appendix B.

Lemma 4 If  is permissible and not zero a.e., for all w > 0, for all  {0, ..., D}, q~ > 0 and r~ > 0.

We will also need a lemma that shows that small changes in  lead to small changes in Gauss(0, 2).

Lemma 5 (see [8]) There is an absolute constant C such that, for all 1, 2 > 0,

dT V

(Gauss(0,

12),

Gauss(0,

22))



C

|1 -2 | 1

.

The following technical lemma is proved in Appendix C.

Lemma 6 q  [r, s],

Iaf i(spqezrm)2isesxipbl(e-, zfo2r/2a)lld0z

< 

r  s,  and

fo--raall (>qz0)2,

there is an a exp(-z2/2)

 dz

0 such  .

that,

for

all

Armed with these lemmas, we are ready to prove Theorem 2.

First, if  is zero a.e., or if w = 0, Theorem 2 follows directly from Lemma 3, together with a union bound over the layers. Assume for the rest of the proof that (x) is not zero a.e., and that
w > 0, so that q~ > 0 and r~ > 0 for all .

For each

 [D], define r

=

1 N

N i=1

x2,i

.

Our proof of Theorem 2 is by induction. The inductive hypothesis is that, for any ,  > 0 there
is an N0 such that, if N  N0, then, with probability 1 - , for all  , |q - q~ |  and |r - r~ |  .

The base case holds because q0 = q~0 = r0 = r~0 = 1, no matter what the value of N is.
Now for the induction step; choose > 0, 0 < < min{q~ /4, r~ } and 0 <   1/2. (Note that these choices are without loss of generality.) Let  (0, ) take a value that will be described later, using quantities from the analysis. By the inductive hypothesis, whatever the value of , there is an N0 such that, if N  N0, then, with probability 1 - /2, for all  - 1, we have |q - q~ |  and |r - r~ |  . Thus, to establish the inductive step, it suffices to show that, after conditioning

3

Under review as a conference paper at ICLR 2019

on the random choices before the th layer, if |q -1 - q~ -1|  , and |r -1 - r~ -1|  , there is an N such that, if N  N , then with probability at least 1 - /2 with respect only to the random
choices of W ,:,: and b ,:, that |q - q~ |  and |r - r~ |  . Given such an N , the inductive step can be satisfied by letting N0 be the maximum of N0 and N .

Let us do that. For the rest of the proof of the inductive step, let us condition on outcomes of the
layers before layer , and reason about the randomness only in the th layer. Let us further assume that |q -1 - q~ -1|  and |r -1 - r~ -1|  .

Recall that q

=

1 N

N i=1

h2,i

.

Since we have conditioned on the values of h -1,1, ..., h -1,N ,

each component of h ,i is obtained by taking the dot-product of x -1,: = (h -1,:) with W ,i,:

and adding an independent b ,i. Thus, conditioned on h -1,1, ..., h -1,N , we have that h ,1, ..., h ,N

are independent. Also, since x -1,: is fixed by conditioning, each h ,i has an identical Gaussian

distribution.

Since each component of W and b has zero mean, each h ,i has zero mean.
Choose an arbitrary i  [N ]. Since x -1,: is fixed by conditioning and W ,i,1, ..., W ,i,N and b ,i are independent,

E[q

]

=

E[h2,i]

=

b2

+

w2 N

x2-1,j = b2 + w2 r -1 d=ef q .

j

(1)

We wish to emphasize the q is determined as a function of random outcomes before the th layer, and thus a fixed, nonrandom quantity, regarding the randomization of the th layer. By the inductive hypothesis, we have

|E[q ] - q~ | = |E[h2,i] - q~ | = |q - q~ | = w2 |r -1 - r~ -1|  w2 .

(2)

The key consequence of this might be paraphrased by saying that, to establish the portion of the inductive step regarding q , it suffices for q to be close to its mean. Now, we want to prove something similar for r . We have

1 E[r ] = N

N

E[x2,i]

=

1 N

N

E[(h ,i)2] = E[(h ,1)2],

i=1 i=1

since h ,1, ..., h ,N are i.i.d. Recall that, earlier, we showed that h ,i  Gauss(0, q ). Thus

E[r ] = EzGauss(0,q )[(z)2] = EzGauss(0,1)[( q z)2] =

1 2

( q z)2 exp(-z2/2) dz.

which gives

|E[r ] - r~ |  EzGauss(0,q )[(z)2] - EzGauss(0,q~ )[(z)2] .

Since |q - q~ |  w2 and we may choose

to ensure



q~ 2w2

,

we

have

q~

/2



q

 2q~ .

For  > 0 and   (0, 1/2) to be named later, by Lemma 6, we can choose a such that, for all q  [q~ /2, 2q~ ],

-a (qz)2 exp(-z2/2) dz  /2 and
-

 (qz)2 exp(-z2/2) dz  /2
a

and

1 2q

a -a

exp

-

z2 2q

dz  1 - . Choose such an a.

4

Under review as a conference paper at ICLR 2019

We claim that

a -a

(qz)2

exp(-z2/2)

dz

-

(qz)2 exp(-z2/2) dz   for all q~ /2 <

q  2q~ . Choose such a q. We have

a (qz)2 exp(-z2/2) dz -

 ( qz

)2

exp(-z2/2)

dz

-a

=

-a

 ( qz

)2

exp(-z2/2)

dz

+

 (qz)2 exp(-z2/2) dz

-

a

 2 max

-a (qz)2 exp(-z2/2) dz,



 ( qz

)2

exp(-z2/2)

dz

-

a

 .

So now we are trying to bound

a -a

 ( q

z)2 exp(-z2/2)

dz

-

a -a

 ( q~

z)2

exp(-z2/2)

dz

using q~ /2  q  2q~ .

Using changes of variables, we have

aa

( q z)2 exp(-z2/2) dz - ( q~ z)2 exp(-z2/2) dz

-a
= 1 q


aq
 (z)2 exp
-a q

- z2 2q

-a
dz - 1 q~



a q~
 (z)2 exp
-a q~

- z2 2q~

dz .

Since  is permissible, ditioning Gauss(0, q )

2 on

i[s-baounqd,eadonq

 [-a 2q~ ], and P~

 , a 2q~ ]. If P is the distribution obtained byconby conditioning Gauss(0, q~ ) on [-a q~ , a q~ ],

then if M =

2

supz[-a2q~

 ,a 2q~

]

(z)2,

since

q

 2q~ ,



1 q

aq

-a q

(z)2 exp(- z2 ) dz - 1 2q q~

a q~ 
-a q~

(z)2 exp(- z2 ) dz 2q~

 M dT V (P, P~).

But since, for  < 1/2, conditioning on an event of probability at least 1 -  only changes a
distribution by total variation distance at most 2, and therefore, applying Lemma 5 along with the fact that |q - q~ |  w2 , for the constant C from Lemma 5, we get

dT V (P, P~)  4 + dT V(Gauss(0, q ), Gauss(0, q~ ))  4 + C| q- q~ | q~

=

4

+

 C|q | q+

- q~ | q~ | q~

 4 + C w2 . q~

Tracing back, we have
aa
( q z)2 exp(-z2/2) dz - (
-a -a
which implies

q~ z)2 exp(-z2/2) dz  M

C 4 +

w2

q~

|E[r ] - r~ |  ( q z)2 exp(-z2/2) dz - ( q~ z)2 exp(-z2/2) dz

M

C 4 +

w2

+ 2.

q~

If



=

min{

24M

,

1 3

},



=

12 ,

and

/2.

= min

2,

2w2

,

q~ 2w2

,

q~ 6CM w2

this implies |E[r ] - r~ | 

5

Under review as a conference paper at ICLR 2019

Recall that q is an average of N identically distributed random variables with a mean between 0
and 2q~ (which is therefore finite) and r is an average of N identically distributed random variables, each with mean between 0 and r~ + /2  2r~ . Applying the weak law of large numbers (Lemma 3), there is an N such that, if N  N , with probability at least 1 - /2, both |q - E[q ]|  /2 and |r - E[r ]|  /2 hold, which in turn implies |q - q~ |  and |r - r~ |  , completing the proof of the inductive step, and therefore the proof of Theorem 2.

4 DIVERSITY OF BEHAVIOR IN THE DISTRIBUTION OF HIDDEN NODES

In this section, we show that, for some activation functions, the probability distribution of hidden nodes can have some surprising properties.

4.1 NON-GAUSSIAN

In this subsection, we will show that the hidden variables are sometimes not Gaussian. Our proof will refer to the Cauchy distribution.

Definition 2 A distribution over the reals that, for x0  R and  > 0, has a density f given by

f (x) =



1

( )1+

x-x0 

2

is a Cauchy distribution, denoted by Cauchy(x0, ). Cauchy(0, 1) is the

standard Cauchy distribution.

Lemma
1n n i=1

7 ([6]) Xi has

If the

X1, ..., Xn are i.i.d. same distribution.

random

variables

with

a

Cauchy

distribution,

then

Lemma 8 ([11]) If U and V are zero-mean normally distributed random variables with the same variance, then U/V has the standard Cauchy distribution.

The following shows that there is a  such that the limiting h2 is not defined. It contradicts claims made on line 7 of Section A.1 of [14] and line 7 of Section 2.2 of [10].

Proposition 9 There is a  such that, for every w > 0, if b = 0, then (a) for finite N , h2,1 does not have a Gaussian distribution, and (b) h2,1 diverges as N goes to infinity.

Proof: Consider  defined by (y) =

1/y if y = 0 0 if y = 0.

Fix a value of N and w > 0, and take b = 0. Each component of h1,: is a sum of zero-mean

Gaussians with variance w2 /N ; thus, for all i, h1,i  Gauss(0, w2 ). Now, almost surely, h2,1 =

N j=1

W2,1,j

(h1,j

)

=

N j=1

W2,1,j

/h1,j

.

By

Lemma

8,

for

each

j,

W2,1,j /h1,j

has

a

Cauchy

distribution, and since (N W2,1,1), ..., (N W2,1,N )  Gauss(0, N w2 ), recalling thath1,1, ..., h1,N 

Gauss(0, w2 ), Lemma 7, h2,1

we =

have that N W2,1,1/h1,1, ...,

N j=1

W2,1,j (h2,j )

=

1 N

N W2,1,N /h1N are i.i.d.

N j=1

N

W2,1,j (h1,j

)

is

Cauchy(0, N). Applying also Cauchy(0, N ).



So, for all N , h2,1 is Cauchy(0, N ). Suppose that h2,1 converged in distribution to some dis-

tribution P . Since the cdf of P can have at most countably many discontinuities, we can cover

the real line by a countable set of finite-length intervals [a1, b1], [a2, b2], ... whose endpoints are points of continuity for P . Since Cauchy(0, N ) converges to P in distribution, for any i,

P ([ai, bi])



limN 

|bi-ai | N

=

0. Thus, the probability assigned by P

to the entire real line

is 0, a contradiction.

4.2 INDEPENDENCE
The following contradicts a claim made on line 8 of Section A.1 of [14].
Theorem 10 If  is either the ReLU or the Heaviside function, then, for every w > 0, b  0, and N  2, (h2,1, ..., h2,N ) are not independent.

6

Under review as a conference paper at ICLR 2019

Proof: We will show that E[h22,1h22,2] = E[h22,1]E[h22,2], which will imply that h2,1 and h2,2 are not independent.

As mentioned earlier, because each component of h1,: is the dot product of x0,: with an independent row of W1,:,: plus an independent component of b1,:, the components of h1,: are independent, and since x1,: = (h1,:), this implies that the components of x1,: are independent. Since each row of W1,:,: and each component of the bias vector has the same distribution, x1,: is i.i.d.

We have

 

 2

E[h22,1]

=

E


 

W2,1,ix1,i + b2,1

 

i[N ]

=

E [W2,1,iW2,1,j x1,ix1,j ] +

E [W2,1,ix1,ib2,1] + E b22,1 .

(i,j)[N ]2

i[N ]

The components of W2,:,: and x1,:, along with b2,1, are mutually independent, so terms in the double

sum with i = j have zero expectation, and E[h22,1] = i[N] E W22,1,i E x12,i + E[b22,1]. For a random variable x with the same distribution as the components of x1,:, this implies

E[h22,1] = w2 E x2 + b2.

(3)

Similarly,
E[h22,1h22,2]
 

2 

2

=

E


 

W2,1,ix1,i + b2,1 

W2,2,ix1,i + b2,2

 

i[N ]

i[N ]

= E[W2,1,iW2,1,j W2,2,rW2,2,sx1,ix1,j x1,rx1,s]
(i,j,r,s)[N ]4

+ 2 E[W2,1,iW2,1,j W2,2,rx1,ix1,j x1,rb2,2]+2 E[W2,1,iW2,2,rW2,2,sx1,ix1,rx1,sb2,1]

(i,j,r)[N ]3

(i,r,s)[N ]3

+ 4 E[W2,1,iW2,2,rx1,ix1,rb2,1b2,2]
(i,r)[N ]2

+ E[W2,1,iW2,1,j x1,ix1,j b22,2] + E[W2,2,rW2,2,sx1,rx1,sb22,1]

(i,j)[N ]2

(r,s)[N ]2

+ 2 E[W2,1,ix1,ib2,1b22,2] + 2 E[W2,2,rx1,rb22,1b2,2]

i[N ]

r[N ]

+ E[b22,1b22,2]

=

E[W22,1,iW22,2,r]E[x21,i]E[x12,r] +

E[W22,1,iW22,2,i]E[x14,i]

(i,r)[N ]2,i=r

i[N ]

+ E[W22,1,i]E[x21,i]E[b22,2] + E[W22,2,r]E[x21,r]E[b22,1]

i[N ]

r[N ]

+ E[b12,2b22,2]

=

(N 2

-

N )w4 E[x2]2 N2

+

N w4 E[x4] N2

+

2N w2 E[x2]b2 N

+

b4

=

w4 E[x2]2

+

w4 (E[x4] - N

E[x2]2)

+

2w2 b2E[x2]

+

b4.

Putting this together with (3), we have

E[h22,1h22,2]

- E[h22,1]E[h22,2]

=

w4 (E[x4] - E[x2]2) . N

7

(4)

Under review as a conference paper at ICLR 2019

(a) N = 10

(b) N = 100

(c) N = 1000

Figure 1: Histograms ofh[2, :], averaged over 100 random initializations, for N  {10, 100, 1000}, along with Cauchy(0, N ) (shown in green) and Gauss(0, 2) for  estimated from the data (shown in red).

Now, we calculate the difference using (4) for the Heaviside and ReLU functions.

Heaviside. Suppose  is Heaviside function, i.e. (z) is the indicator function for z > 0. In this

case, since the components of h1,: are symmetric about 0, the distribution of x1,: is uniform over

{0, 1}N .

Thus E[x4]

=

E[x2]

=

1/2, and so (4) gives E[h22,1h22,2] - E[h22,1]E[h22,2]

=

3w4 4N

=

0.

ReLU. Next, we consider the case that  is the ReLU. Recalling that, for all i, h1,i  Gauss(0, w2 ),

we

have

E[x2]

=

1
2w2

 0

z2

exp

-z2 2w2

dz.

By

symmetry

this

is

1 2

EzGauss(0,w2

)

[z

2

]

=

w2 /2.

Similarly,

E[x4]

=

1 2

EzGauss(0,w2

)

[z

4

]

=

34 2

.

Plugging

these

into

(4)

we

get

that,

in

the

case

the

 is the ReLU, that

E[h22,1h22,2] - E[h22,1]E[h22,2] = w4

(3/2)w4 - w4 /4 N

= 5w8 > 0, 4N

completing the proof.

4.3 UNDEFINED LENGTH MAP

Here, we show, informally, that for  at the boundary of the second condition in the definition of permissibility, the recursive formula defining the length map q~ breaks down. Roughly, this condition cannot be relaxed.

Proposition 11 For any  > 0, if  is defined by (x) = exp(x2), there exists a w, b s.t. q~ , r~ is undefined for all  2.

Proof:

Suppose

w2

+

b2

=

1 42

.

Then

q~1

=

1 42

,

so

that

r~1

=

1 2


(
-

q~1z) exp

- z2 2

dz = 1


exp(

2 -

q~1z2) exp

- z2 2

dz

= 1


exp(z2/2) exp

- z2

dz = ,

2 -

2

and downsteam values of q~ and r~ are undefined.

5 EXPERIMENTS
Our first experiment fixed x[0, :] = (1, ..., 1), w = 1, b = 0, (z) = 1/z. For each N  {10, 100, 1000}, we (a) initialized the weights100 times, (b) plotted the histograms of all of the values of h[2, :], along with the Cauchy(0, N ) distribution from the proof of Proposition 9, and Gauss(0, 2) for  estimated from the data. Consistent with the theory, the Cauchy(0, N ) distribution fits the data well.
To illustrate the fact that the values in the second hidden layer are not independent, for N = 1000 and the parameters otherwise as in the other experiment, we plotted histograms of the values seen

8

Under review as a conference paper at ICLR 2019
Figure 2: Histograms of h[2, :] for nine random weight initializations.
in the second layer for nine random initializations of the weights in Figure 2. When some of the values in the first hidden layer have unusually small magnitude, then the values in the second hidden layer coordinately tend to be large. This is in contrast with the claim made at the end of Section 2.2 of [10]. Note that this is consistent with Theorem 2 establishing convergence in probability for permissible , since the  used in this experiment is not permissible.
REFERENCES
[1] M. Chen, J. Pennington, and S. S. Schoenholz. Dynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks. arXiv preprint arXiv:1806.05394, 2018.
[2] A. Daniely, R. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In Advances In Neural Information Processing Systems, pages 2253­2261, 2016.
[3] W. Feller. An introduction to probability theory and its applications. John Wiley & Sons, 2008. [4] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249­256, 2010. [5] S. Hayou, A. Doucet, and J. Rousseau. On the selection of initialization and activation function for deep neural networks. arXiv preprint arXiv:1805.08266, 2018. [6] M. Hazewinkel. Cauchy distribution. In Encyclopaedia of Mathematics: Volume 6. Springer Science & Business Media, 2013. [7] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026­1034, 2015. [8] B. Klartag. A central limit theorem for convex sets. Inventiones mathematicae, 168(1):91­131, 2007. [9] Y. A. LeCun, L. Bottou, G. B. Orr, and K. Mu¨ller. Efficient backprop. In Neural networks: Tricks of the trade. Springer, 1998.
9

Under review as a conference paper at ICLR 2019

[10] J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. ICLR, 2018.
[11] R. Lupton. Statistics in theory and practice. Princeton University Press, 1993.
[12] J. Pennington, S. Schoenholz, and S. Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pages 4785­4795, 2017.
[13] J. Pennington, S. S. Schoenholz, and S. Ganguli. The emergence of spectral universality in deep networks. arXiv preprint arXiv:1802.09979, 2018.
[14] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances in neural information processing systems, pages 3360­3368, 2016.
[15] S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016.
[16] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington. Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.
[17] G. Yang and S. Schoenholz. Mean field residual networks: On the edge of chaos. In Advances in neural information processing systems, pages 7103­7114, 2017.

A PROOF OF LEMMA 1

Choose c

> 0.

Since

limsupx

log

|(x)| x2

=

0

and

limsupx-

log

|(x)| x2

=

0, we also have

limsupx

log

|(cx)| x2

=

0

and

limsupx-

log

|(cx)| x2

=

0.

Thus, there is an a such that, for all

x



[-a, a], log |(cx)|



x2 8

,

which

implies

(cx)2



exp

x2 4

. Since  is permissible, it is

bounded on [-a, a]. Thus, we have

(cx)2 exp(-x2/2) dx

-a a



= (cx)2 exp(-x2/2)dx + (cx)2 exp(-x2/2)dx + (cx)2 exp(-x2/2)dx

- -a a

-a

 exp(-x2/4)dx + sup (cx)2

-

x[-a,a]

a
exp(-x2/2)dx + exp(-x2/4)dx
-a a

<

completing the proof.

B PROOF OF LEMMA 4

The proof is by induction. The base case holds since q~0 = r~0 = 1. To prove the inductive step, we need the following lemma.

Lemma 12 If  is not zero a.e., then, for all c > 0, EzGauss(0,1)((cz)2) > 0.

Proof: If µ is the Lebesgue measure, since

µ({x  R : 2(cx) > 0}) = lim µ({x : 2(cx) > 1/n}  [-n, n]) > 0,
n

there exists n such that µ({x : 2(cx) > 1/n}  [-n, n]) > 0. For such an n, we have

EzGauss(0,1)((cz)2)



1 e-n2/2µ({x n

:

2(cx)

>

1/n}



[-n, n])

>

0.

Returning to the proof of Lemma 4, by the inductive hypothesis, r~ -1 > 0, which, since w > 0, implies q~ > 0. Applying Lemma 12 yields r~ > 0.
10

Under review as a conference paper at ICLR 2019

C PROOF OF LEMMA 6

Since (x)2

limsupx

 exp

x2 4s

log |(x)| x2

=

0 there

. Now, choose q 

is an b [r, s].

such that, for all x 
For a = b/ r, we

 b, then

log |(x)| have



x2 8s

,

which

implies

 (qx)2 exp(-x2/2) dx

a

1 = q


(z)2 exp
aq

- z2 2q

dz



1 q


exp
aq

z2 4s

exp

- z2 2q



1 

q


exp
 aq

- z2 4q

dz

1

z2



exp - dz.

q b 4q

dz

By
then
a -

ing(cirveeqasxsi)n2geaxbp(-(ifxq2nx/e)2c2)eedsxsxapr(y-, x,2w/ceo2m) cpalendtixnegntshuereproo1qf..

 b

exp

A

-

z2 4q

dz  

symmetric argument

which yields

11

