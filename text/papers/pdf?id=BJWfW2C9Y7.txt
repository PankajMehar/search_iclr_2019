Under review as a conference paper at ICLR 2019

PREDICTIVE LOCAL SMOOTHNESS FOR STOCHASTIC GRADIENT METHODS
Anonymous authors Paper under double-blind review

ABSTRACT
Stochastic gradient methods are dominant in nonconvex optimization especially for deep models but have low asymptotical convergence due to the fixed smoothness. To address this problem, we propose a simple yet effective method for improving stochastic gradient methods named predictive local smoothness (PLS). First, we create a convergence condition to build a learning rate which varies adaptively with local smoothness. Second, the local smoothness can be predicted by the latest gradients. Third, we use the adaptive learning rate to update the stochastic gradients for exploring linear convergence rates. By applying the PLS method, we implement new variants of three popular algorithms: PLS-stochastic gradient descent (PLSSGD), PLS-accelerated SGD (PLS-AccSGD), and PLS-AMSGrad. Moreover, we provide much simpler proofs to ensure their linear convergence. Empirical results show that the variants have better performance gains than the popular algorithms, such as, faster convergence and alleviating explosion and vanish of gradients.

1 INTRODUCTION

In this paper, we consider the following nonconvex optimization:

minxRd

f (x)

:=

1 n

n i=1

fi(x),

(1)

where x is the model parameter, and neither f nor the individual fi (i  [n]) are convex, such as, deep models. Stochastic gradient descent (SGD) is one of the most popular algorithms for minimizing the loss function in equation 1. It iteratively updates the parameter by using the product of a learning rate and the negative gradient of the loss, which is computed on a minibatch drawn randomly from training set. Unfortunately, small learning rate makes SGD painfully slow to converge, and high learning rate causes SGD to diverge. Therefore, choosing a proper learning rate (or step size) becomes a challenge.

A popular adaptive method adjusts automatically the learning rate by using some forms of the past gradients to scale coordinates of the gradient. For example, AdaGrad Duchi et al. (2011) is the first adaptive algorithm to update the sparse gradients by dividing positive square root of averaging the squared past gradients, and its several variants (e.g., Adadelta Zeiler (2012), RMSProp Tieleman & Hinton (2012), Adam Kingma & Ba (2015), Nadam Dozat (2016), and AMSGrad Reddi & Kumar (2018)) have been widely and successfully applied into train deep models. Specifically, they use the exponential moving averages of squared past gradients to manage the rapidly decayed learning rate. Most of these algorithms are to establish their convergence guarantees based on a maximum of the Lipschitz constant L, which governs smoothness of the loss function in whole parameter space, often called L-smoothness Bottou et al. (2016); Reddi et al. (2016).

However, the maximum of the L-smoothness results in a small learning rate as it is inversely proportional to L1 Bottou et al. (2016). Since the L-smoothness changes with the parameter and
the given data, it leads to a local smoothness of the loss function near the optimum for selecting the learning rate. Specifically, the local smoothness is often computed by the the Hessian matrix 2f (x),
such as data dependent local smoothness (e.g., local SVRG Vainsencher et al. (2015)) and parameter
dependent local smoothness (e.g., Newton method Nocedal & Wright. (2006)). Unfortunately, the
Hessian matrix results in a high computational cost and a high memory requirement. Recently,

1A large L results in a low learning rate to slowly move the loss function from a point f (x0) to local minimum loss f (x) with an equilibrium parameter x, and vice versa.

1

Under review as a conference paper at ICLR 2019

Stochastic Quasi-Newton (SQN) Byrd et al. (2016) and Adaptive Quasi-Newton (adaQN) Keskar & Berahas (2016a) are to partly reduce the expensive cost by using the Hessian-vector product Nocedal & Wright. (2006). However, these algorithms are still difficult to train a large number of the parameters of deep models due to the loop computation of the Hessian-vector product.

To address the above problem, we exploit a simple yet effective method to predict the parameter

dependent local smoothness for the adaptive learning rate. To ensure the convergence of the stochastic

algorithms, an ideal local smoothness L(xt) of the loss function is based on an ideal neighborhood

Nrxt (x) of an equilibrium x

it is x.

difficult to apply the Nrxt Since the past parameter

with (x)
xt-1

a radius xt - x into the stochastic has been known,

by using the current parameter xt. However, algorithms due to the unknown equilibrium we consider a local smoothness L(xt) on a

neighborhood Nrxt-1 (xt) of xt with a radius xt - xt-1 . Because Nrxt (x) and Nrxt-1 (xt) have

a common point xt or region, L(xt) is naturally used instead of L(xt). Based on the definition of the

L-smoothness (or Lipschitz continuous gradients) Bottou et al. (2016), L(xt) on this neighborhood

is easily predicted by using the past gradient f (xt-1) and the current gradient f (xt), that is,

L(xt) =

.f (xt)-f (xt-1) 2
xt-xt-1 2

Obviously, it only needs to storage the past parameter xt-1 and

gradient f (xt-1), and perform twice subtractions and norm computations.

This paper, therefore, provides a local smoothness strategy to study the adaptive learning rate. In this strategy there are two important problems that how to easily build a direct functional relationship between the learning rate and the local smoothness, and how to calculate the local smoothness. To address these problems, the stochastic gradient algorithms are transformed into a linear dynamical system Lessard et al. (2016); Hu et al. (2017) by using the local smoothness to linearize the gradient function. The functional relationship is obtained by constructing the convergence condition for the linear dynamical system. Based on the above discussion, the local smoothness is simply predicted by using the current gradient and the latest gradient. Overall, our main contributions are summarized as

· We propose a predictive local smoothness (PLS) method to adjust automatically learning rate for stochastic gradient algorithms. Our PLS method will lead these algorithms to drive a loss function to fast converge to a local minimum.
· We apply PLS into SGD, SQN Byrd et al. (2016), AMSGrad Reddi & Kumar (2018) and AccSGD Kidambi et al. (2018). Correspondingly, we establish four PLS-SGD, PSLSQN, PLS-AMSGrad and PLS-AccSGD algorithms, and provide corresponding theoretical conditions to ensure their linear convergence.
· We also provide an important empirical result that PLS can alleviate the exploding and vanishing gradients in the classical algorithms (e.g., SGD, AMSGrad Reddi & Kumar (2018) and AccSGD Kidambi et al. (2018)) for training deep model with least squares regression.

Note that due to the limited space PLS-AccSGD is provided in subsection 6.1 in Supplementary Material. We do not provide PSL-SQN since it is similar to PLS-SGD. But, we empirically verify that PSL-SQN is faster than SQN in the subsection 4.1.

2 PRELIMINARIES

In this section, we introduce some notations, local smoothness assumption, and three popular stochastic gradient algorithms: SGD, SQN Byrd et al. (2016), AMSGrad Reddi & Kumar (2018), and AccSGD Kidambi et al. (2018).

Notation. f (x) denotes exact gradient of f at x, while fit (x) denotes a stochastic gradient

of f , where it is sampled uniformly at random from [1, · · · , n] and n is the number of samples.

Since it is sampled in an independent and identically distributed (IID) manner from {1, · · · , n},

the

expectation

of

smoothness

Lit (xt)

is

denoted

as

L(xt)

=

E [Lit (xt)]

=

1 n

n i=1

Li(xt

).

A

1

or 2-norm of a vector x is denoted as x , and its square is x 2. A positive-definite matrix A is

denoted as A 0. The Kronecker product of matrices A and B is denoted as A  B. We denote

a d × d identity matrix by Id. A neighborhood of a point x1  Rd with radius rx2 is denoted as Nrx2 (x1) = {y  Rd| x1 - y < rx2 = x1 - x2 }.

2

Under review as a conference paper at ICLR 2019

Assumption 1. We say f is local smoothness on a set C  Rd if there is a constant L such that

f (x) - f (y)  L x - y , for  x, y  C.

(2)

Assumption 1 is an essential foundation for convergence guarantees of most stochastic gradient methods as the gradient of f is controlled by L with respect to the parameter vector.

SGD. Stochastic Gradient Descent (SGD) simply computes the gradient of the parameters by uni-

formly randomly choosing a single or a few training examples. Its update is given by

xt+1 = xt - tfit (xt), 

(3)

where t is the learning rate. t is usually set to a decay form 0/ t in practice. This setting leads to

slower convergence. Moreover, the gradient of the loss of a deep model with rectified linear units

(ReLU) Nair & Hinton (2010) often explodes when large initialization 0.

SQN. Stochastic Quasi-Newton (SQN) Byrd et al. (2016) employs iterations of the form

xt+1 = xt - tHtfit (xt),

(4)

where Ht+1 = (I - kskykT )Ht(I - kykskT ) + ksksTk with sk = xt - xt-1, yk = fit (xt) - fit-1 (xt-1) and k = ykT sk. In practice, the quasi-Newton matrix Ht+1 is not formed explicitly, and is computed by the Hessian-vector product Nocedal & Wright. (2006).

AMSGrad. AMSGrad Reddi & Kumar (2018) is an exponential moving average variant of the

popular Adam algorithm Kingma & Ba (2015) in the scale gradient method. AMSGrad uses the

factors 1t = 1/t and 2 to exponentially move the momentums of the gradient and the squared gradient, respectively. 1t = 1 = 0.9 and 2 = 0.999 are typically recommended in practice. The key update is described as follows:

 

mt+1

= 1tmt + (1 - 1t)fit (xt),


 

vt+1 vt+1

= =

2vt + (1 - 2) max{vt+1, vt},

(fit (xt))2 , xt+1 = xt -

t

mt+1
vt+1

.

(5)

AccSGD. Accelerated SGD (AccSGD) proposed in Jain et al. (2017) is much better than SGD, HB

Polyak (1964) and NAG Nesterov (1983) in the momentum method. An intuitive version of AccSGD

is presented in Kidambi et al. (2018). Particularly, AccSGD takes three parameters: long learning rate parameter   1, statistical advantage parameter   , and 

learning rate t, = 1 - 0.72/.

This update can alternatively be stated by:

mt+1 xt+1

= =

mt + (1
0.7
0.7+(1-)

- ) (xt -

xtt-fi0t (.7xt t)f) i+t (x0t.7)+1-(,1-) mt+1.

(6)

3 PREDICTIVE LOCAL SMOOTHNESS

The popular adaptive learning rate methods are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients Reddi & Kumar (2018). These methods indirectly adjust the learning rate as they can be essentially viewed as gradient normalization. In this section, we study the local smoothness to directly and adaptively adjust the learning rate, propose a predictive local smoothness (PLS) method, and apply this method into SGD, AMSGrad Reddi & Kumar (2018), and AccSGD Kidambi et al. (2018). Before showing our PLS method, we first give two local smoothness sequences definition.

Definition 1. Let x be an equilibrium of the local minimum f (x) and {xt}t0 be a updating parameter procedure, where x0 is an initial point. A corresponding neighborhood sequence of x is denoted by {Nrxt (x)}t0, where rxt = x - xt . An ideal local smoothness sequence of x is defined as {L(xt)}t0 which satisfies that,

f (x) - f (y)  L(xt) x - y , for  y  Nrxt (x).

(7)

A backward neighborhood sequence on {xt}t1 is denoted by {Nrxt-1 (xt)}t1, where rxt-1 = xt - xt-1 . A backward local smoothness sequence is defined as {L(xt)}t1 which satisfies that

f (xt) - f (y)  L(xt) xt - y , for  y  Nrxt-1 (xt).

(8)

This definition reveals that L(xt) governs the ideal local smoothness between the equilibrium x and xt to strictly ensure the convergence of the updating parameter procedure. However, L(xt) cannot be computed due to the unknown x. Clearly, L(xt) is easily calculated by using xt and xt-1.

3

Under review as a conference paper at ICLR 2019

Procedure PLS

· Build the learning rate  = (L(xt)) by linearizing f (xt).

· Predict the local smoothness L(xt) =

.f (xt)-f (xt-1)
xt-xt-1 + 1

·

Apply (L(xt)) 

1 L(xt )+

2

to update xt+1 by any stochastic gradient algorithms.

Figure 1: Predictive Local Smoothness

3.1 PLS METHOD

PLS is an adaptive learning rate method based on the local smoothness. The local smoothness varies based on the updating parameters {xt}t0 in stochastic gradient algorithms. According to the definition 1, the local smoothness sequence {L(xt)}t0 are ideally used to adjust the learning rate in othfethneeiugnhkbnorohwonodxse. qSuienncceeth{eNrerxits(xac)o}mtm0.onHopwoienvtesre, qituiesndcieffiocnul{txtot}ctom1 pinutbeo{tLh ({xNt)r}xtt-10(bxetc)}autse1 and {Nrxt (x)}t0, {L(xt)}t1 can be predicted by {L(xt)}t1, which is easily computed by using the current gradient and the latest gradient. In this paper, our key idea is to use the predictive local smoothness sequence {L(xt)}t1 instead of the unknown {L(xt)}t0 to adjust automatically the learning rate t. PLS is described as the following three steps.

Building adaptive learning rate with local smoothness. We firstly create a functional relationship between t and L(xt) by using the convergence conditions of stochastic gradient algorithms. Following the local smoothness in equation 7, since f (x) = 0, f (xt) is linearized by f (xt) = (L(xt)  Id) (xt - x). Using L(xt) instead of L(xt), we consider the following linearization

f (xt) = (L(xt)  Id) (xt - x),

(9)

where L(xt) is computed by equation 10 in the neighborhood Nrxt-1 (xt), and L(xt) = 2f (xt) if f is twice continuously differentiable. Stochastic gradient algorithms can use the linearization in
equation 9 to transform it into a simple time-varying linear system. The convergence of the algorithms
is achieved by studying the stability of this linear system Lessard et al. (2016); Hu et al. (2017). Therefore, the stability condition is naturally used to construct the functional relationship between t and L(xt), t = (L(xt)). This shows that the learning rate is adaptively tuned by L(xt).

Predicting the local smoothness. We secondly predict the local Lipschitz constant L(xt) by using the current gradient f (xt) and the latest gradient f (xt-1). By using the local smoothness in
equation 7, L(xt) on Nrxt-1 (xt) is predicted by

L(xt) =

f (xt) - f (xt-1) xt - xt-1 + 1

,

(10)

where 1 is a parameter to prevent xt - xt-1 going to zero. This predictive Lipschitz constant L(xt) is utilized to adjust automatically the learning rate t for computing the parameter xt+1. In the next subsections, we prove that t is inversely proportional to L(xt), t  (1/(L(xt) + 2)), where
2 is another parameter to avoid the learning rate to be over large in the later updating process.

Applying the adaptive learning rate into any stochastic gradient algorithms. We thirdly use the adaptive learning rate t = (L(xt)) to update the parameter xt+1 in the stochastic gradient algorithm. Overall, Fig. 1 summarizes the proposed adaptive local smoothness method. This method can be applied into the adaptive method based on exponential moving averages (e.g., AdaGrad Duchi et al. (2011), Adadelta Zeiler (2012), RMSProp Tieleman & Hinton (2012), Adam Kingma & Ba (2015), Nadam Dozat (2016) and AMSGrad Reddi & Kumar (2018)), the momentum methods (e.g., HB Polyak (1964), NAG Nesterov (1983) and AccSGD Jain et al. (2017); Kidambi et al. (2018)) and the Quasi-Newton methods (e.g., SQN Byrd et al. (2016)). Next, we will apply PLS into SGD, and AMSGrad Reddi & Kumar (2018) to show its effectiveness. Moreover, due to the limited space, PLS is also used to improve AccSGD Kidambi et al. (2018) in subsection 6.1 in Supplementary Material.

Remark 1. In essence, both PSL and SQN Byrd et al. (2016) have a similarity and a difference. The similarity is that they calculate the Quasi-quadratic differential to accelerate the convergence of stochastic gradient methods. The difference is that SQN employs the Quasi-Newton matrix by

4

Under review as a conference paper at ICLR 2019

equation 4 and PSL uses the predictive local smoothness by equation 10. Compared to SQN, PSL has two advantages. First, PSL requires low computational time and low memory since it only needs to storage the past parameter and gradient, and perform twice subtractions and norm computations. Second, SQN can still be improved by using our PLS, and we empirically verify that PLS-SQN is better than SQN in subsection 4.1.
Remark 2. Compared to the related local smoothness methods Kpotufe & Garg (2013); Johnson & Zhang (2013); Vainsencher et al. (2015), which require that the loss function is twice differentiable, PLS only requires the differentiable loss function. In addition, the unknown global smoothness is estimated along with the optimization Malherbe & Vayatis (2017), while we provide an effective method to predict unknown local smoothness using equation 10.

3.2 PLS-SGD

Algorithm 1 PLS-SGD.

In this subsection, we introduce a PLS-SGD algorithm. By using the linearization in equation 9 and computing the expectation, the updating rule of SGD is converted into the linear system:
xt+1 - x = ((1 - tLit (xt))  Id) (xt - x), (11)

1: input: 0, 1, 2.
2: initialize: x0. 3: for t = 1, · · · , T - 1 do
4: Randomly pick it from {1, · · · , n}; 5: gt = fit (xt);

where it is sampled in an IID manner from  = {1, · · · , n}. Then, the convergence condition of S-
GD is obtained by employing the stability condition
of the linear system in equation 11, which shows that

6:

Lit (xt) =

;gt -gt-1
xt-xt-1 + 1

7:

t

=

0 (Lit (xt)+

2) ;

8: xt+1 = xt - tfit (xt);

9: end for

xt converges to x at a given linear rate . Now we

present a linear convergence condition for the SGD as follows.

Theorem 12. Consider the linear system in equation 11. Assume that it is sampled in an IID manner

from a uniform distribution, the assumption 1 holds and there exists an equilibrium x  Rd such

that

f (x)

=

0.

Let

µ

=

1 n

n i=1

Li

(xt),

and



=

1 n

ni=1Li2(xt). For a fixed linearconvergence

rate 0 <  < 1, if µ2 > (1 - 2) holds, then we have µ-

µ2 - (1-2 ) 

<

t

<

µ+

µ2

- 

(1-2

)

,

and the linear system is exponentially stable, that is, xt - x 2  t x0 - x 2.

Theorem 1 provides a simple condition for the linear convergence of SGD, which benefits from our
PLS method. The condition reveals that the functional relationship between t, µ and . In the
updating process, since it is sampled uniformly at random from , it only uses µ = Lit (xt) and  = L2it (xt). Hence, we have (1 - )/Lit (xt) < t < (1 + )/Lit (xt) and set t = 0/Lit (xt), where 0 is an initialized learning rate and 1 -   0  1 + , and Lit (xt) can be predicted by using the equation 10. Similar to 1, 2 is another parameter to stop Lit (xt) going to zero, and avoid the learning rate to be over large in the latter updating process. In our PLS-SGD algorithm, therefore, the
learning rate t is set to 0/(Lit (xt) + 2). The adaptive learning rate results in that PLS-SGD has a faster (linear) convergence rate than the traditional SGD. PLS-SGD is summarized in Algorithm 1.

3.3 PLS-AMSGRAD

Similar to PLS-SGD, we integrate the proposed PLS method into the classical adaptive method based on exponential moving averages, AMSGrad Reddi & Kumar (2018), and propose a PLSAMSGrad algorithm. The Lipschitz linearization in equation 9 is used to linearize the updating rules in equation 5 of AMSGrad as:

mt+1 xt+1 - x

= (Ait  Id)

mt xt - x

, where Ait =

1t - t1t
vt+1

(1 - 1t)Lit (xt) 1 - (1-1t)tLit (xt)
vt+1

, (12)

vt+1 = max{vt+1, vt}, vt+1 = (2  Id) vt + (1 - 2)L2it (xt)  Id (xt - x)2. In fact, mt is the momentum method to manage the velocity of the gradient. Using this linearization, we provide a much simpler convergence analysis of AMSGrad by studying the linear system in equation 12. The linear convergence condition of AMSGrad is described as

2All the proofs for the Theorems and the linear systems are provided in Supplementary Material.

5

Under review as a conference paper at ICLR 2019

Theorem 2. Consider the linear system in equation 12. Assume that it is sampled in an IID manner

from a uniform that f (x) =

distribution, the assumption 1 holds and there existsan equilibrium 0. For a fixed linear convergence rate  = maxt{ 1t}, if there

x  exists

Rd such a2×2

positive definite matrix P 0 such that

1 n

n

ATi P Ai - 2P  0,

i=1

(13)

or the following condition holds, for any it   = {1, · · · , n},



1 - 1t vt+1 1 + 1t

1 Lit (xt)

<

t

<

1 + 1t vt+1 1 ,

1 - 1t

Lit (xt)

(14)

then the linear system is exponentially stable, that is,

mt+1 xt+1 - x

2



cond(P )t

m0 x0 - x

,
2

where cond(P ) is the condition number of P and Cond(P ) = 1(P )/p(P ), where 1(P ) and

p(P ) denote the largest and smallest singular values of the matrix P .

Compared to the convergence analysis of AMSGrad

in Reddi & Kumar (2018), Theorem 2 establishes Algorithm 2 PLS-AMSGrad.

simpler conditions in equation 13 and equation 14 for its linear convergence. The 2 × 2 linear matrix

inequality (LMI) condition in equation 13 is built

by using the control theory (e.g., integral quadratic

constraint Lessard et al. (2016); Hu et al. (2017)) to

study the stability of the linear system equation 12.

It is easily solved by LMI toolbox Boyd et al. (1994).

Although the condition in equation 13 is not very

clear to the relationship between t and Lit (xt), the condition in equation 13 directly reveals its func-

tional relationship, that is,t = t vt+1/Lit (xt),

where

1-1t 1+ 1t

< t

<

.1+1t
1- 1t

Based on the equa-

1: input: 0 > 0, {1t > 0}Tt=-01, 2, 1, 2. 2: initialize: x0 = 0, u0 = v0 = v0 = 0.

3: for t = 1, · · · , T - 1 do

4: Randomly pick it from {1, · · · , n};

5: gt = fit (xt);

6:

Lit (xt) =

;gt -gt-1
xt-xt-1 + 1

7:

t

=

0 (Lit (xt)+

2) ;

8: mt+1 = 1tmt + (1 - 1t)gt;

9: vt+1 = 2vt + (1 - 2)gt2;

10: vt+1 = max{vt+1, vt};

11:

xt+1

=

xt

-

t

mt+1
vt+1

;

tion 5, vt+1 tends to zero as it is a linear system, 12: end for

0 < 2 < 1, and the gradient goes to zero. For sim-



plification, t

=

0/Lit (xt), where 0 is an initialized learning rate and

1-1 1+ 1

<

0

<

1+1 1- 1

,

since

1t Lit

= 1. (xt) is

Similar to computed

PbLySe-qSuGatDio, nL1it0(,xatn)disthalesolesaarmnipnlgedrautenifot rims slyetattora(nLdito(mxt0)f+rom2)

[1, or

· · · , n], that
 0 t(Lit (xt)+

is,
2)

for avoiding the over-large learning rate in the latter updating process. Thus, PLS-AMSGrad is

summarized in Algorithm 2.

Remark 2. Based on the Lemma, the 2 × 2 LMI in equation 13 is equivalent to the condition in equation 14. The former is obtained by constructing the Lyapunov function in the control theory, while the latter is built by calculating the spectral radius of the weight matrix in the linear system in equation 12, which is defined as the magnitude of the largest eigenvalue of the weight matrix.

4 EXPERIMENTS
In this section, we present empirical results to confirm the effectiveness of the PLS method on linear predictors (convex) and neural networks (nonconvex). We compare PLS-SGD, PLS-SQN, PLS-AMSGrad and PLS-AccSGD with SGD, SQN Byrd et al. (2016), adaQN Keskar & Berahas (2016a), AMSGrad Reddi & Kumar (2018) and AccSGD Kidambi et al. (2018).
4.1 PSL FOR CONVEX OPTIMIZATION
In this subsection, we compare PLS-SGD and PLS-SQN with SGD, SQN and adaQN by using convex linear predictor problems (e.g., logistic regression and least squares regression). The code of SGD, SQN and adaQN is downloaded in the github3 Keskar & Berahas (2016b). Based on this code, we
3https://github.com/keskarnitish/minSQN

6

Under review as a conference paper at ICLR 2019

Training loss

100 logistic regression

101 least squares regression

SGD

SGD

SQN adaQN

100

SQN adaQN

10-1

PLS-SGD PLS-SQN

10-1

PLS-SGD PLS-SQN

Training loss

10-2

10-2 0

10-3

5 10 15 20

0

5 10 15 20

#grad / n

#grad / n

Figure 2: Logistic regression (convex) and least squares regression (convex) on mushroom using

SGD, SQN, adaQN, PLS-SGD and PLS-SQN.

Traning loss

Traning loss

0.6 0.4 0.2
0 0
3 2

SGD PLS-SGD

50 #grad / n

100

AMSGrad PLS-AMSGrad

Test loss

0.6 0.4 0.2
0 0
3 2

SGD PLS-SGD

50 #grad / n

100

AMSGrad PLS-AMSGrad

Traning loss

60 50 40 30
0
30
28

Traning loss

Test loss

11

26

00 0 50 100 0 50 100

#grad / n

#grad / n

0

Traning loss

0.3 0.3

60

AccSGD

AccSGD

0.2

PLS-AccSGD

0.2

PLS-AccSGD

50

Test loss

0.1 0.1

40

30

00 0 50 100 0 50 100 0

#grad / n

#grad / n

Classification

SGD PLS-SGD

50 #grad / n

100

AMSGrad PLS-AMSGrad

Test loss

60 50 40 30
0
30
28

SGD PLS-SGD

50 #grad / n

100

AMSGrad PLS-AMSGrad

Test loss

26

50 #grad / n

100

AccSGD PLS-AccSGD

0
60 50 40

50 #grad / n

100

AccSGD PLS-AccSGD

Test loss

50 #grad / n

30

100 0

50

#grad / n

Reconstruction

100

Traning loss

Figure 3: Performance comparison of SGD, AMSGrad, AccSGD, PLS-SGD, PLS-AMSGrad and PLS-AccSGD on MNIST using neural networks with two fully-connected hidden layers. The left two columns show the training loss and test loss for classification, while right two columns show the training loss and test loss for reconstruction.

implement PLS-SGD and PLS-SQN. We do experiments on mushroom dataset4, which contains 8124 data points with 112 dimensions. We use mini-batches of size 256 and tune the learning rate for the best SGD, SQN and adaQN in all experiments. We report the training loss with respect to iterations in Figure 2, and have the following observations. Figure 2 shows the effectiveness of PLS. Specifically, PLS-SGD has faster convergence and lower loss than SGD, and PLS-SQN is also faster convergent than SQN. Moreover, PLS-SGD and PLS-SQN are much better than adaQN.

4.2 PSL FOR NONCONVEX OPTIMIZATION
In this subsection, we compare PLS-SGD, PLS-AMSGrad and PLS-AccSGD with SGD, AMSGrad and AccSGD by studying nonconvex multiclass classification and image reconstruction using neural network with least squares regression (LSR) loss and 2-regularization. The weight parameters of the neural network are initialized by using the normalized strategy choosing uniformly from [- 6/(nin + nout), 6/(nin + nout)], where nin and nout are the numbers of input and output layers of the neural network, respectively. We use mini-batches of size 100 in all experiments.
Datasets. MNIST5 contains 60,000 training samples and 10,000 test samples with 784 dimensional image vector and 10 classes, and CIFAR106 includes 50,000 training samples and 10,000 test samples with 1024 dimensional image vector and 10 classes, and 512 dimensional features are extracted by deep residual networks He et al. (2016) for verifying the effectiveness of our methods.
4https://archive.ics.uci.edu/ml/datasets/mushroom 5http://yann.lecun.com/exdb/mnist/ 6https://www.cs.toronto.edu/~kriz/cifar.html

7

Under review as a conference paper at ICLR 2019

Adaptive learning rates Adaptive learning rates Adaptive learning rates

PLS-SGD-1th layer

0.015

PLS-AMSGrad-1th layer

1.5

PLS-AccSGD-1th layer

PLS-SGD-2th layer

PLS-AMSGrad-2th layer

PLS-AccSGD-2th layer

0.1

PLS-SGD-3th layer

0.01 PLS-AMSGrad-3th layer

1 PLS-AccSGD-3th layer

0.05 0.005 0.5

0 0 20 40 60 80 100 #grad / n

0 0 20 40 60 80 100 #grad / n

0 0 20 40 60 80 100 #grad / n

Figure 4: Adaptive learning rates of three layers of neural networks for classification task on MNIST dataset using PLS-SGD, PLS-AMSGrad and PLS-AccSGD.

Classification. We train the neural networks with two fully-connected hidden layers of 500 ReLU units and 10 output linear units to investigate the performance of all algorithms on MNIST and CIFA10 datasets. The 2-regularization is 1e-4 (MNIST) and 1e-2 (CIFAR10). A grid search is used to determine the learning rate that provides the best performance for SGD, AMSGrad and AccSGD. We set the adaptive learning rate t =0/(Lit (xt) + 2) for PLS-SGD, PLS-AMSGrad and PLS-AccSGD, where 0 is chosen as 0.001 or 0.002. To enable fair comparison, we set typical parameters 1 =0.9 and 2 =0.999 for AMSGrad and PLS-AMSGrad, and set =1000 and  =10 for AccSGD and PLS-AccSGD Reddi & Kumar (2018); Kidambi et al. (2018). For convenience, both parameters 1 and 2 are same, = 1 = 2, and chosen as 0.01 for PLS-SGD and PLS-AMSGrad, and 0.001 for PLS-AccSGD.
We report the training loss and test loss with respect to iterations on MNIST in the left two columns of Figure 3. We can see that PLS-SGD, PLS-AMSGrad and PLS-AccSGD preform much better than SGD, AMSGrad and AccSGD. The important reason is that our PLS method can directly adjust the learning rate from a small initialization value to a suitable value. In practice, a large fixed learning rate results in the explosion of the loss of neural network with ReLU using SGD, AMSGrad and AccSGD since the loss will go to infinity when the learning rate is larger than 0.011 in our experiments. We observe that the learning rate fast increases in the initial stage and slowly varies in the late stage in Figure 4. Moreover, there are similar observations on CIFAR10. Due to the limited space, the losses and learning rate are plotted in Figure 5 in Supplementary Material.
Reconstruction. We train a deep fully connected neural network to reconstruct the images in comparison with all algorithms on MNIST dataset. Its structure is represented as 784-1000-500- 200-500-1000-784 with the first and last 784 nodes representing the input and output respectively. In this experiment, we provides the best performance for SGD, AMSGrad and AccSGD by searching from a grid learning rates. The initial learning rate 0 is set to 5e-7, 1e-2 and 1e-7 for PLS-SGD, PLSAMSGrad and PLS-AccSGD, respectively. In addition, we also set to t = 0/( t(Lit (xt) + 2)) for PLS-AMSGrad, 1 =0.9, 2 =0.999, =1000 and =10. Moreover, = 1 = 2, and chosen as 0.01 for PLS-SGD and PLS-AccSGD, and 0.1 for PLS-AMSGrad.
The training loss and test loss with respect to iterations are reported in the right two columns of Figure 3. We can still see that PLS-SGD, PLS-AMSGrad and PLS-AccSGD have significant better performance than SGD, AMSGrad and AccSGD since our PLS method adaptively adjusts the learning rate to prevent the explosion of the LSR loss with large learning rate. The adaptive learning rate is shown in Figure 6 in Supplementary Material. We also observe that the learning rate is initialized a small value, fast increases in the earlier stage and slowly varies in the latter stage.

5 CONCLUSIONS
This paper introduced an adaptive local smoothness method for stochastic gradient descent algorithms. This method adjusted automatically learning rate by using the latest gradients to predict the local smoothness. We proposed PLS-SGD, PLS-AMSGrad and PLS-AccSGD algorithms by applying our adaptive local smoothness method into the popular SGD, AMSGrad and AccSGD algorithms. We proved that our proposed algorithms enjoyed the linear convergence rate by studying the stability of their transformed linear systems. Moreover, our proof was significantly simpler than the convergence analyses of SGD, AMSGrad and AccSGD. Experimental results verified that the proposed algorithms provided better performance gain than SGD, AMSGrad and AccSGD.
8

Under review as a conference paper at ICLR 2019
REFERENCES
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. arXiv:1606.04838v1, pp. 1­93, 2016.
S. Boyd, L.E. Ghaoui, E. Feron, and V. Balakrishnan. Linear Matrix Inequalities in System and Control Theory. SIAM, 1994.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008­1031, 2016.
T. Dozat. Incorporating nesterov momentum into adam. In ICLR workshop, 2016.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121­2159, July 2011.
K.M. He, X.Y. Zhang, S.Q. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016.
B. Hu, P. Seiler, and A. Rantzer. A unified analysis of stochastic optimization methods using jump system theory and quadratic constraints. In COLT, pp. 1157­1189, 2017.
P. Jain, S.M. Kakade, R. Kidambi, P. Netrapalli, and A. Sidford. Accelerating stochastic gradient descent. arXiv:1704.08227, pp. 1­55, 2017.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In NIPS, pp. 315­323, 2013.
N.S. Keskar and A.S. Berahas. adaqn: An adaptive quasi-newton algorithm for training rnns. In G. Manco J. Vreeken P. Frasconi, N. Landwehr (ed.), ECML-PKDD, volume 9851, pp. 1­16. Springer, Cham, 2016a.
N.S. Keskar and A.S. Berahas. minSQN: Stochastic Quasi-Newton Optimization in MATLAB, 2016b. URL https://github.com/keskarnitish/minSQN/. [Online].
R. Kidambi, P. Netrapalli, P. Jain, and S.M. Kakade. On the insufficiency of existing momentum schemes for stochastic optimization. In ICLR, 2018.
D.P. Kingma and J.L. Ba. Adam: a method for stochastic optimization. In ICLR, 2015.
S. Kpotufe and V.K. Garg. Adaptivity to local smoothness and dimension in kernel regression. In NIPS, 2013.
L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57­95, 2016.
C. Malherbe and N. Vayatis. Global optimization of lipschitz functions. In ICML, pp. 2314­2323, 2017.
V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, pp. 807­814, 2010.
Y. Nesterov. A method of solving a convex programming problem with convergence rate. Soviet Mathematics Doklady, 27(2):372­376, 1983.
J. Nocedal and S.J. Wright. Numerical Optimization. Springer Science+Business Media, 2006.
B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964.
S.J. Reddi and S. Kumar. On the convergence of adam and beyond. In ICLR, 2018.
S.J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex optimization. In ICML, pp. 314­323, 2016.
9

Under review as a conference paper at ICLR 2019 T. Tieleman and G. Hinton. Rmsprop: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural Networks for Machine Learning, 2012. D. Vainsencher, H. Liu, and T. Zhang. Local smoothness in variance reduced optimization. In NIPS,
pp. 2179­2187, 2015. Matthew D. Zeiler. Adadelta: An adaptive learning rate method. CoRR, abs/1212.5701, 2012.
10

Under review as a conference paper at ICLR 2019

6 SUPPLEMENTARY MATERIAL

6.1 PLS-ACCSGD

In this subsection, we present a PLS-AccSGD algorithm. Similar to PLS-AMSGrad, our PLS method is integrated into the classical momentum method, AccSGD Jain et al. (2017); Kidambi et al. (2018). Using the Lipschitz linearization in the equation 9, the updating rules in the equation 6 of AccSGD is simply linearized as:

mt+1 - x xt+1 - x

= (Bit  Id)

mt - x xt - x

,

(15)

where Bt =

 b

(1 - )(1 - atLit (xt)) (1 - b)(1 - tLit (xt)) + b(1 - )(1 - atLit (xt))

,

=

1-

0.72  

,

a

=

 0.7

,

b

=

1- 0.7+(1-)

,



and



are

defined

in

the

equation

6.

This

linearization

leads

us

to

provide

a

much

simpler proof for linear convergence analysis of AccSGD by studying the stability of the linear

system in the equation 15. We have following Theorem 3 for its convergence condition.

Theorem 3. Consider the linear system in the equation 15. Assume that it is sampled in

an IID manner from a uniform distribution, the assumption 1 holds and there exists an equi-

librium x  Rd such that f (x) = 0. For a fixed linear convergence rate 0 <  <

max

1

-

0.72 



,

maxt

(1-tLit (xt)) +0.7

, if there exists a 2 × 2 positive definite matrix P

0

such that

1 n

n

BiT P Bi - 2P  0,

i=1

(16)

or the following condition holds, for any it   = {1, · · · , n},

0.72

 + 0.7 1

1

0<1-

<  and 1 - 



Lit (xt)

<

t

<

, Lit (xt)

(17)

then the linear system is exponentially stable, that is,

mt+1 - x xt+1 - x


2

where cond(P ) is the condition number of P .

cond(P )t

m0 - x x0 - x

,
2

Theorem 3 shows that the linear convergence conditions in the equation 16 and the equation 17 are

Algorithm 3 PLS-AccSGD.

simpler than the convergence analysis of AccSGD 1: input: 0, ,   , 1, 2.

Kidambi et al. (2018). Similar to PLS-AMSGrad, the 2 × 2 LMI condition equation 16 is built by us-

2:

initialize:

x0, u0

= 0,  = 1 -

0.72 



.

3: for t = 1, · · · , T - 1 do

ing the control theory and is easily solved by LMI

toolbox Boyd et al. (1994). The condition in the

equation 17 directly opens the learning rate t is a

functional relationship with the local smoothness

Lit (xt),

t

=

,0
Lit (xt)

where 0

is an initialized

learning rate,

1

-



+0.7 

<

0

<

1.

This

revels

PLS 0 can be set to a negative value. The rea-

son is that the eigenvalues of the weight matrix in

the

system

in

the

equation

15

are

1-

0.72  

and

(1-tLit (xt +0.7

))

.

The

stability

of

the

system

in

the

4: Randomly pick it from {1, · · · , n};

5: gt = fit (xt);

6:

Lit (xt) =

;gt -gt-1
xt-xt-1 + 1

7: t = 0/(Lit (xt) + 2);

8:

mt+1 = mt + (1 - )

xt

-

t 0.7

gt

;

9:

xt+1

=

0.7 0.7+(1-)

(xt

-

tgt)

10:

+

1- 0.7+(1-)

mt+1

;

11: end for

equation 15 needs to satisfy the condition in the equation 17. Similar to PLS-SGD, Lit (xt) is also sampled uniformly at random from [1, · · · , n], that is, Lit (xt) is computed by the equation 10. To
prevent the over-large learning rate, the learning rate t is set to 0/(Lit (xt) + 2) in our PLSAccSGD, which is outlined in Algorithm 3. Following Remark 2, the 2 × 2 LMI in the equation 16

is equivalent to the condition in the equation 17.

6.2 PROOFS
Proof of Theorem 1: First, we prove that SGD in equation 3 is converted into the stochastic linear system in equation 11. By putting the Lipschitz linearization in equation 9 into the equation 3, we

11

Under review as a conference paper at ICLR 2019

have

xt+1 = xt - t (Lit (xt)  Id) (xt - x).

(18)

By adding -x into the both sides of the equation 18 and combining like terms, it holds xt+1 - x = ((1 - tLit (xt))  Id) (xt - x). We thus have the equation 11.

Second, we construct the Lyapunov function V (xt) = (xt - x)T (p  Id) (xt - x), where p > 0, to prove the stability of the system in the equation 11. Defining

E[V (xt)] = E[V (xt+1) - 2V (xt)] = E[(xt+1 - x)T (p  Id) (xt+1 - x) - 2(xt - x)T (p  Id) (xt - x)]

= (xt - x)T E[(1 - tLit (xt))2] - 2 (p  Id) (xt - x).

(19)

Then for any xt = x, E[V (xt)] < 0 if E[(1 - tLit (xt))2] - 2 < 0, which implies

E[L2it (xt)]t2 - 2E[Lit (xt)]t + 1 - 2 < 0

(20)

Since

it

is

sampled

in

an

IID

manner

from

{1, · · ·

, n},

we

let

µ

=

E [Lit (xt)]

=

1 n

n i=1

Li(xt),

and  = E

L2it (xt)

=

1 n

n i=1

Li2(xt).

So,

the

equation

20

is

satisfied

if

we

have

the

condition

in

Theorem 1 as

µ2 > (1 - 2).

(21)

This implies

µ - µ - (1 - 2)

µ + µ - (1 - 2)



< t <

. 

(22)

By using the nonnegativity of the equation 19, we have

(xl+1 - x)T (p  Id) (xl+1 - x)  2(xl - x)T (p  Id) (xl - x).

(23)

Inducting from l = 1 to t, we see that for all t

(xt - x)T (p  Id) (xt - x)  2t(x0 - x)T (p  Id) (x0 - x),

(24)

which implies xt - x 2  t x0 - x 2, where p is a positive number. The proof is complete.

Proof of Theorem 2: First, we prove that AMSGrad is converted into the stochastic linear system in the equation 12. By putting the Lipschitz linearization in the equation 9 into the equation 5, we have

mt+1 = (1t  Id) mt + (1 - 1t) (Lit (xt)  Id) (xt - x), vt+1 = (2  Id) vt + (1 - 2) (Lit (xt)  Id)2 (xt - x)2.

(25a) (25b)

By adding

-x

into the

both

sides

of xt+1

=

xt

-

t

mt+1
vt+1

in

the equation 5 and substituting the

equation 25a and the equation 25b into the equation 5, it holds

xt+1

-

x

=

xt

-

x

-

t

1tmt

+

(1

-

1t)(Lit (xt) vt+1



Id)

(xt

-

x)

= t1t  Id mt + vt+1

1 - (1 - 1t)tLit (xt)  Id (xt - x), vt+1

(26)

where vt+1 = max{vt+1, vt}. So, we have the equation 12 by combining the equation 25a with the equation 26.

Second, we construct the Lyapunov function V (t) = tT (P  Id) t, where t =

mt xt - x

and

P 0 is a 2 × 2 positive matrix, to prove the stability of the system in the equation 12. Defining

E[V (t)] = E[V (t+1) - 2V (t)] = E[tT+1 (At  Id) t+1 - 2tT (At  Id) t] = tT E[ATit P Ait ] - 2P  Id t.

(27)

12

Under review as a conference paper at ICLR 2019

Then if the condition in the equation 13 is satisfied, then E[V (xt)] < 0 for any xt = x. By using the nonnegativity of the equation 19, we have

lT+1 (P  Id) l+1  2lT (P  Id) l. Inducting from l = 1 to t, we see that for all t

(28)

tT (P  Id) t  2t0T (P  Id) 0,

(29)

which implies

mt+1 xt+1 - x


2

cond(P )t

m0 x0 - x

, where cond(P ) is the condition number of
2

P and Cond(P ) = 1(P )/p(P ), where 1(P ) and p(P ) denote the largest and smallest singular

values of the matrix P .

Third, we prove the another condition in the equation 14. If it holds AiTt P Ait - 2P  0 for any it   = {1, · · · , n}, then we have the equation 13. Moreover, AiTt P Ait - 2P  0 is equivalence to a simple condition that the eigenvalues of Ait is less than . Hence, we consider the eigenvalues of Ait calculated by

t - 1t tI - At =  t1t
vt+1

-(1 - 1t)Lit (xt)

t -

1 - (1-1t)tLit (xt)
vt+1

  = 0,

(30)

(t - 1t) t - 1 - (1 - 1t)tLit (xt) vt+1

+ (1 - 1t)Lit (xt)

t1t vt+1

= 0,

(31)

where  =

2t -

1 + 1t - (1 - 1t)tLit (xt) t + 1t = 0, vt+1

t

=

1 + 1t

-

(1-1t)tLit (xt)
vt+1
2

 ±
,

2

1

+

1t

-

(1-1t)tLit (xt)
vt+1

- 41t.

(32) (33)

Similar to the Proof of Proposition1 Lessard et al. (2016), if  < 0, then the magnitudes of the roots satisfy |t| < 1t <  = maxt{ 1t}. Then  < 0 implies that

2
1 + 1t - (1 - 1t)tLit (xt) < 41t, vt+1

(34)

-2 1t < 1 + 1t - (1 - 1t)tLit (xt) < 2 1t,

vt+1

 1 - 1t

2

1 - 1t

vt+1

1 Lit (xt)

<

t

<

 1 + 1t

2

1 - 1t

vt+1

1 .

Lit (xt)

(35) (36)

The proof is complete.

Proof of Theorem 3: First, we prove that AccSGD is converted into the stochastic linear system in the equation 15. By putting the Lipschitz linearization in the equation 9 into the equation 6, we have

mt+1 = (  Id) mt + (1 - )

xt

-

t 0.7

(Lit (xt)



Id) (xt

-

x)

,

xt+1

=

0.7

0.7 + (1 -

)

(xt

-

t (Lit (xt)



Id) (xt

-

x))

+

0.7

1- + (1 -

)



Id

(37a)
mt+1. (37b)

Let

a

=

 0.7

and

b

=

1- 0.7+(1-)

,

we

have

mt+1 = (  Id) mt + (1 - ) (xt - at (Lit (xt)  Id) (xt - x)) , xt+1 = (1 - b) (xt - t (Lit (xt)  Id) (xt - x)) + (b  Id) mt+1.

(38a) (38b)

13

Under review as a conference paper at ICLR 2019

Traning loss

Traning loss

0.5 0.5

SGD

SGD

0.4

PLS-SGD

0.4

PLS-SGD

0.3 0.3

Test loss

0.2 0.2

0.1 0.1

0 0 20 40 60 80 100 #grad / n

0 0 20 40 60 80 100 #grad / n

0.5 1

AMSGrad

AMSGrad

0.4

PLS-AMSGrad

0.8

PLS-AMSGrad

0.3 0.6

Test loss

0.2 0.4

0.1 0.2

0 0 20 40 60 80 100 #grad / n

0 0 20 40 60 80 100 #grad / n

0.1 0.08

AccSGD PLS-AccSGD

0.1 0.08

AccSGD PLS-AccSGD

0.06 0.06

Test loss

0.04 0.04

0.02 0.02

0 0 20 40 60 80 100 #grad / n

0 0 20 40 60 80 100 #grad / n

Adaptive learning rates

Adaptive learning rates

Adaptive learning rates

0.5 0.4 0.3 0.2 0.1
0 0
0.06
0.04

PLS-SGD-1th layer PLS-SGD-2th layer PLS-SGD-3th layer
20 40 60 80 100 #grad / n
PLS-AMSGrad-1th layer PLS-AMSGrad-2th layer PLS-AMSGrad-3th layer

0.02

0 0 20 40 60 80 100 #grad / n
1.5 PLS-AccSGD-1th layer PLS-AccSGD-2th layer
1 PLS-AccSGD-3th layer
0.5
0 0 20 40 60 80 100 #grad / n

Traning loss

Figure 5: Performance comparison of SGD, AMSGrad, AccSGD, PLS-SGD, PLS-AMSGrad and PLS-AccSGD on CIFAR10 using neural network with two fully-connected hidden layers. The left, middle, and right columns show the training loss, test loss, and adaptive learning rate, respectively.

Learning rates

x 10-5 1.5
1 0.5

PLS-SGD-4th layer PLS-SGD-5th layer PLS-SGD-6th layer

0 0 20 x 10-6
1.5
1

40 60 #grad / n

80 100

PLS-SGD-4th layer PLS-SGD-5th layer PLS-SGD-6th layer

0.5

0 0 50 100 150 200 #grad

Learning rates

Learning rates

0.05 0.04 0.03 0.02 0.01
0 0
0.25 0.2
0.15 0.1
0.05 0 0

PLS-AMSGrad-4th layer PLS-AMSGrad-5th layer PLS-AMSGrad-6th layer
20 40 60 80 100 #grad / n
PLS-AMSGrad-4th layer PLS-AMSGrad-5th layer PLS-AMSGrad-6th layer
50 100 150 200 #grad

Learning rates

Learning rates

x 10-6 6

4
2
0 0 20 x 10-6
6

PLS-AccSGD-4th layer PLS-AccSGD-5th layer PLS-AccSGD-6th layer

40 60 #grad / n

80 100

4
2 PLS-AccSGD-4th layer PLS-AccSGD-5th layer PLS-AccSGD-6th layer
0 0 50 100 150 200 #grad

Learning rates

Figure 6: Adaptive learning rates of different layers of neural network for reconstruction using PLS-SGD, PLS-AMSGrad and PLS-AccSGD on MNIST. The down row shows the adaptive learning rate in the first 200 iterations.

By adding -x into the both sides of the equation 38a and the equation 38b, and substituting the equation 38a into the equation 38b, it holds

mt+1 - x = (  Id) (mt - x) + ((1 - ) (1 - atLit (xt))  Id) (xt - x),

(39a)

xt+1 - x = ((1 - b) (1 - tLit (xt))  Id) (xt - x) + (b  Id) (mt+1 - x)

= (b  Id) (mt - x) + (((1 - b)(1 - tLit (xt)) + b(1 - )(1 - atLit (xt)))  Id) (xt - x).

(39b)

Thus, we have the equation 15 by combining the equation 39a with the equation 39b.

14

Under review as a conference paper at ICLR 2019

Second, we construct the Lyapunov function V (t) = tT (P  Id)t, where t =

mt - x xt - x

,P

is a 2 × 2 positive matrix, to prove the stability of the system in the equation 15. Defining

0

E[V (t)] = E[V (t+1) - 2V (t)] = E[tT+1 (P  Id) t+1 - 2tT (P  Id) t] = tT E[BiTt P Bit ] - 2P  Id t.

(40)

Then if the equation 16 is satisfied, then E[V (t)] < 0 for any t = 0. By using the nonnegativity of the equation 40, we have

lT+1 (P  Id) l+1  2lT (P  Id) l.

(41)

Inducting from l = 1 to t, we see that for all t

tT (P  Id) t  2t0T (P  Id) 0,

(42)

which implies

mt+1 - x xt+1 - x


2

of P .

cond(P )t

m0 - x x0 - x

, where cond(P ) is the condition number
2

Third, we certify the another condition in the equation 17. If it holds BiTt P Bit - 2P  0 for any it   = {1, · · · , n}, then we have the equation 16. Moreover, BiTt P Bit - 2P  0 is equivalence to a simple condition that the eigenvalues of Bit is less than . Hence, we consider the eigenvalues of Bit , which is calculated as follows.

By adding a product of -b and the first row of Bt into the second row of Bt, Bt is rewritten as Bt:

tI - Bt =

Bt =

 0

(1 - )(1 - atLit (xt)) (1 - b)(1 - tLit (xt))

,

t -  0

-(1 - )(1 - atLit (xt)) t - (1 - b)(1 - tLit (xt))

= 0.

(43) (44)

The two eigenvalues of Bt is

1t = , 2t = (1 - b)(1 - tLit (xt)). Since 1t > 0 and 2t > 0, we have

(45)

0 < 1t =  < , 0 < 2t = (1 - b)(1 - tLit (xt)) < .

(46)

By

substituting



=

1-

0.72  

and

b

=

1- 0.7+(1-)

into

the

equation

46,

it

holds

the

condition

in

the

equation 17. The proof is complete.

6.3 FIGURES
In the classification experiment, we select the learning rate from {0.011, 0.0090.008, 0.007, 0.006, 0.05, 0.004} for providing the best performance of the SGD, AMSGrad and AccSGD algorithms. In the reconstruction experiment, the learning rate is chosen from {6, 5, 4, 3, }e-7 for SGD and AccSGD and {10, 7, 5, 3}e-2 for AMSGrad due to the explosion of the LSR loss with large learning rate, and select the learning rate from these sets for providingthe best performance of the algorithms. To prevent the over-fitting, the learning rate t is set to 0/ t for AMSGrad (except MNIST).
Figure 5 reports the adaptive learning rate, the training loss and test loss with respect to iterations on CIFAR10 for classification, while Figure 6 shows the adaptive learning rate with respect to iterations on MNIST for reconstruction.

15

