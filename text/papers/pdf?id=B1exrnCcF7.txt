Under review as a conference paper at ICLR 2019
DISJOINT MAPPING NETWORK FOR CROSS-MODAL MATCHING OF VOICES AND FACES
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a novel framework, called Disjoint Mapping Network (DIMNet), for cross-modal biometric matching, in particular of voices and faces. Different from the existing methods, DIMNet does not explicitly learn the joint relationship between the modalities. Instead, DIMNet learns a shared representation for different modalities by mapping them individually to their common covariates. These shared representations can then be used to find the correspondences between the modalities. We show empirically that DIMNet is able to achieve better performance than the current state-of-the-art methods, with the additional benefits of being conceptually simpler and less data-intensive.
1 INTRODUCTION
A person's face is predictive of their voice. Biologically, the genetic, physical and environmental influences that affect the face also affect the voice. Humans have been shown to be able to associate voices of unknown individuals to pictures of their faces Kamachi et al. (2003). Humans also show improved ability to memorize and recall voices when previously exposed to pictures of the speaker's face, but not imposter faces McAllister et al. (1993); Schweinberger et al. (2007; 2011). Cognitively, studies indicate that neuro-cognitive pathways for voices and faces share common structure Ellis (1989), possibly following parallel pathways within a common recognition framework Belin et al. (2004; 2011). The above studies lend credence to the hypothesis that it may be possible to find associations between voices and faces algorithmically as well. With this in perspective, this paper focuses on the task of devising computational mechanisms for cross-modal matching of voice recordings and images of the speakers' faces.
The specific problem we look at is the one wherein we have an existing database of samples of people's voices and images of their faces, and we aim to automatically and accurately determine which voices match to which faces. This problem has seen significant research interest, in particular since the recent introduction of the VoxCeleb corpus Nagrani et al. (2017), which comprises collections of video and audio recordings of a large number of celebrities. The existing approaches Nagrani et al. (2018b;a); Kim et al. (2018) have generally attempted to directly relate subjects' voice recordings and their face images, in order to find the correspondences between the two. Nagrani et al. (2018b) formulates the mapping as a binary selection task: given a voice recording, one must successfully select the speaker's face from a pair of face images (or the reverse ­ given a face image, one must correctly select the subject's voice from a pair of voice recordings). They model the mapping as a neural network that is trained through joint presentation of voices and faces to determine if they belong to the same person. In Kim et al. (2018); Nagrani et al. (2018a), the authors attempt to learn common embeddings (i.e., vector representations) for voices and faces that can be compared to one another to identify associations. The networks that compute the embeddings are also trained through joint presentation of voices and faces, to maximize the similarity of embeddings derived from them if they belong to the same speaker. In all cases, the voice and face are implicitly assumed to directly inform about one another.
In reality, though, it is unclear how much these models capture the direct influence of the voice and face on one another, and how much is explained through implicit capture of higher-level variables such as gender, age, ethnicity etc., which individually predict the two. These higher-level variables,
1

Under review as a conference paper at ICLR 2019

Concat

Direct dependence between voices and faces
Negative Positive Pair Pair

Positive Pair

Triplet Construction
... ...

Triplet Loss

Pair Construction
... ... ... ...
Contrastive Loss

Pair Construction
... ... Binary Classification

Match or not?
(a) Seeing Faces and Hearing Voices

(b) Learnable PINs

...

Negative Pair
(c) Learning Face-Voice Association

Individual dependence between each modality and covariates

Voice Embedding Face Embedding
Embeddings of Single Face or Voice as Input

Identity 1 Identity 2 Identity 3
Identity

... ... ...
Multi-task Classification
...

Gender

No Need for Pair or Triplet Construction

Nationality
Multiple Covariates Supervision

(d) The Proposed DIMNet

Figure 1: Overview of the proposed DIMNet and its comparison to the existing approaches. (a) Seeing faces and hearing voices from Nagrani et al. (2018b). (b) Learnable PINs from Nagrani et al. (2018a). (c) Learning face-voice association from Kim et al. (2018). (d) Our proposed DIMNets. DIMNets present a joint voice-face embedding framework via multi-task classification and require no pair construction (i.e., both voices and faces can be input sequentially without forming pairs).

which we will refer to as covariates1 can, in fact, explain much of our ability to match voices to faces (and vice versa) under the previously mentioned "select-from-a-pair" test (where a voice must be used to distinguish the speaker's face from a randomly-chosen imposter). For instance, simply matching the gender of the voice and the face can result in an apparent accuracy of match of up to 75% in a gender-balanced testing setting. Even in a seemingly less constrained "verification" test, where one must only verify if a given voice matches a given face, matching them based on gender alone can result in an equal error rate of 33% (Appendix B). Even matching the voice and the face by age (e.g.matching older-looking faces to older-sounding voices) could result in match accuracy that's significantly better than random.
Previous studies Nagrani et al. (2018b); Kim et al. (2018) attempt to disambiguate the effect of multiple covariates through stratified tests that separate the data by covariate value. The results show that at least some of the learned associations are explained by the covariate, indicating that their learning approaches do utilize the covariate information, albeit only implicitly.
In this paper, we propose a novel framework to learn mappings between voices and faces that do not consider any direct dependence between the two, but instead explicitly exploit their individual dependence on the covariates. In contrast to existing methods where supervision is provided through the correspondence of voices and faces, our learning framework, Disjoint Mapping Network (DIMNet), obtains supervision from common covariates such as gender, ethnicity, identity (ID) etc., applied separately to voices and faces, to learn common embeddings for the two. The comparison between the existing approaches and DIMNets are illustrated in Fig. 1.
DIMNet comprises individual feature learning modules which learn identically-dimensioned features for data from each modality, and a unified input-modality-agnostic classifier that attempts to predict covariates from the learned feature. Data from each modality are presented separately during learning; however the unified classifier forces the feature representations learned from the individual modalities to be comparable. Once trained, the classifier can be removed and the learned feature representations are used to compare data across modalities.
The proposed approach greatly simplifies the learning process and, by considering the modalities individually rather than as coupled pairs, makes much more effective use of the data. Moreover, if multiple covariates are known, they can be simultaneously used for the training through multi-task learning in our framework (see Fig. 2).
Compared to current methods Nagrani et al. (2018b;a); Kim et al. (2018), DIMNets achieve consistently better performance, indicating that direct supervision through covariates is more effective in these settings. We find that of all the covariates, ID provides the strongest supervision. The results obtained from supervision through other covariates also match what may be expected.
1To be clear, these are covariates, factors that vary jointly with voice and face, possibly due to some other common causative factors such as genetics, environment, etc. They are not claimed to be causative factors themselves.
2

Under review as a conference paper at ICLR 2019

Multi-task Classification Network via Covariates

Voice Embedding Network

Identity Supervision

...

...

Modality Switch Cross-modal Training Data (Voice and Face)

Cross-modal Common Embeddings

Gender Supervision

...

Face Embedding Network

Figure 2: Our DIMNet framework. The input training data can be either voice or face, and there is no need for voices and faces to form pairs. Modality switch is to control which embedding network (voice or face) to process the data. While the embeddings are obtained, a multi-task classification network is applied to supervise the learning.

Our contributions are summarized as follows:
· We propose DIMNets, a framework that formulates the problem of cross-modal matching of voices and faces as learning common embeddings for the two through individual supervision from one or more covariates, in contrast to current approaches that attempt to map voices to faces directly. An overview of our framework is given in Fig. 2.
· In this framework, we can make full use of multiple kinds of label information (provided by covariates) with a multi-task objective function.
· We achieve the state-or-art results on multiple tasks. We are also able to isolate and analyze the effect of the individual covariate on the performance.
Moreover, we note that the proposed framework is applicable in any setting where matching of different type of data which have common covariates is required.

2 THE PROPOSED FRAMEWORK
Our goal is to learn common vector representations for both voices and faces, that permit them to be compared to one another. In the following sections we first describe how we learn them from their relationship to common covariates. Subsequently, we describe how we will use them for comparison of voices to faces.
2.1 LEVERAGING COVARIATES TO LEARN EMBEDDINGS
The relationship between voices and faces is largely predicted by covariates ­ factors that individually relate to both the voice and the face. To cite a trivial example, a person's gender relates their voice to their face: male subjects will have male voices and faces, while female subjects will have female voices and faces. More generally, many covariates may be found that relate to both voice and face Lippert et al. (2017).
Our model attempts to find common representations for both face images and voice recordings by leveraging their relationship to these covariates (rather than to each other). We will do so by attempting to predict covariates from voice and face data in a common embedding space, such that the derived embeddings from the two types of data can be compared to one another.
Let V represent a set of voice recordings, and F represent a set of face images. Let C be the set of covariates we consider. For the purpose of this paper, we assume that all covariates are discrete valued (although this is not necessary). Every voice recording in V and every face in F can be related to each of the covariates in C. For every covariate C  C we represent the value of that covariate for any voice recording v as C(v), and similarly the value of the covariate for any face f as C(f ). For example, C could be ID, gender, or nationality. When C is ID, C(v) and C(f ) are the ID of voice v and face f , respectively.
Let Fv(v; v) : v  Rd be a voice embedding function with parameters v that maps any voice recording v into a d-dimensional vector. Similarly, let Ff (f ; f ) be a face embedding function that

3

Under review as a conference paper at ICLR 2019

maps any face f into a d-dimensional vector. We aim to learn v and f such that the embeddings of the voice and face for any person are comparable.
For each covariate C  C we define a classifier HC (x; C ) with parameter C , which assigns any input x  Rd to one of the values taken by C. The classifier HC(·) is agnostic to which modality its input x was derived from; thus, given an input voice v, it operates on features Fv(v; v) derived from the voice, whereas given a face f , it operates on Ff (f ; f ).
For each v (or f ) and each covariate C, we define a loss L(HC (Fv(v; v); C ), C(v)) between the covariate predicted by HC(.) and the true value of the covariate for v, C(v). We can now define a total loss L over the set of all voices V and the set of all faces F, over all covariates as

L(v, f , {C }) = C

L(HC (Fv(v; v); C ), C(v))

CC

vV

(1)

+ L(HC (Ff (f ; f ); C ), C(f ))
f F

In order to learn the parameters of the embedding functions, f and v, we perform the following

optimization.

v, f = arg min min L(v, f , {C }) v ,f {C }

(2)

2.2 DISJOINT MAPPING NETWORKS

In DIMNet, we instantiate Fv(v; v), Ff (f ; f ) and HC (x; C ) as neural networks. Fig. 2 shows the network architecture we use to train our embeddings. It comprises three components. The first, labelled Voice Network in the figure, represents Fv(v; v) and is a neural network that extracts ddimensional embeddings of the voice recordings. The second, labelled Face Network in the figure, represents Ff (f ; f ) and is a network that extracts d-dimensional embeddings of face recordings. The third component, labelled Classification Networks in the figure, is a bank of one or more classi-
fication networks, one per covariate considered. Each of the classification networks operates on the d-dimensional features output by the embedding networks to classify one covariate, e.g.gender.

The training data comprise voice recordings and face images. Voice recordings are sent to the voiceembedding network, while face images are sent to the face-embedding network. This switching operation is illustrated by the switch at the input in Fig.2. In either case, the output of the embedding network is sent to the covariate classifiers.

As can be seen, at any time the system either operates on a voice, or on a face, i.e.the operations on voices and faces are disjoint. During the learning phase too, the updates of the two networks are disjoint ­ loss gradients computed when the input is voice only update the voice network, while loss gradients derived from face inputs update the face network, while both contribute to updates of the classification networks.

In our implementation, specifically, Fv(·) is a convolutional neural network that operates on MelSpectrographic representations of the speech signal. The output of the final layer is pooled over time to obtain a final d-dimensional representation. Ff (·) is also a convolutional network with a pooled output at the final layer that produces a d-dimensional representation of input images. The classifiers HC(·) are all simple multi-class logistic-regression classifiers comprising a single softmax layer.

Finally, in keeping with the standard paradigms for training neural network systems, we use the

cross-entropy loss to optimize the networks. Also, instead of the optimization in Eq. 2, the actual

optimization performed is the one below. The difference is inconsequential.

v, f, {C } = argmin L(v, f , {C })
v ,f ,{C }

(3)

2.3 TRAINING THE DIMNET
All parameters of the network are trained through backpropagation, using stochastic gradient descent. During training, we construct the minibatches with a mixture of speech segments and face images, as the network learns more robust cross-modal features with mixed inputs. Taking voice as an example, we compute the voice embeddings using Fv(v; v), and obtain the losses using classifiers HC(·) for all the covariates. We back-propagate the loss gradient to update the voice network as

4

Under review as a conference paper at ICLR 2019

well as the covariate classifiers. The same procedure is also applied to face data: the backpropagated loss gradients are used to update the face network and the covariate classifiers. Thus, the embedding functions are learned using the data from their modalities individually, while the classifiers are learned using data from all modalities.

2.4 USING THE EMBEDDINGS

Once trained, the embedding networks Fv(v; v) and Ff (f ; f ) can be used to extract embeddings from any voice recording or face image.mGiven a voice recording v and a face image f , we can

now

compute

a

similarity

between

the

two

through

the

cosine

similarity

S(v, f )

=

.Fv Ff
|Fv ||Ff |

We

can

employ this similarity to evaluate the match of any face image to any voice recording. This enables

us, for instance, to attempt to rank a collection of faces f1, · · · , fK in order of estimated match
to a given voice recording v, according to S(v, fi), or conversely, to rank a collection of voices v1, · · · , vK according to their match to a face f , on order of decreasing S(vi, f ).

3 EXPERIMENTS
We ran experiments on matching voices to faces, to evaluate the embeddings derived by DIMNets. The details of the experiments are given below and Appendix A.
Datasets. Our experiments were conducted on the Voxceleb Nagrani et al. (2017) and VGGFace Parkhi et al. (2015) datasets, which are specified in appendix A.1. We use the intersection of the two datasets, i.e.subjects who figure in both corpora, for our final corpus, which thus includes 1,225 IDs with 667 males and 558 females from 36 nationalities. The data are split into train/validation/test sets, following the settings in Nagrani et al. (2018b). Details can be found in Appendix A.1. We use ID, gender and nationality as our covariates, all of which are provided by the datasets. Separated data preprocessing pipelines are employed to audio segments and face images (see Appendix A.2).
Training. The detailed network configurations are elaborated in appendix A.3. Note that the classification networks are single-layer softmax units with as many outputs as the number of unique values the class can take (2 for gender, 32 for nationalities, and 924 for IDs in our case). The networks are trained to minimize the cross entropy loss, following the typical settings of stochastic gradient descent (SGD) in appendix A.3
Testing. We use the following protocols for evaluation:
· 1:2 Matching. Here, we are give a probe input from one modality (voice or face), and a gallery of two inputs from the other modality (face or voice), including one that belongs to the same subject as the probe, and another of an "imposter" that does not match the probe. The task is to identify which entry in the gallery matches the probe. We report performance in terms of matching accuracy ­ namely what fraction of the time we correctly identify the right instance in the gallery.
To minimize the influence of random selection, we construct as many testing instances as possible through exhaustive enumeration all positive matched pairs (of voice and face). To each pair, we include a randomly drawn imposter in the gallery. We thus have a total of 4,678,897 trials in the validation set, and 6,780,750 trials in the test set.
· 1:N Matching. This is the same as the 1:2 matching, except that the gallery now includes N - 1 imposters. Thus, we must now identify which of the N entries in the gallery matches the probe. Here too results are reported in terms of matching accuracy. We use the same validation and test sets as the 1:2 case, by augmenting each trial with N - 2 additional imposters. So the number of trials in validation and test sets is the same as earlier.
· Verification. We are given two inputs, one a face, and another a voice. The task is to determine if they are matched, i.e.both belong to the same subject. In this problem setting the similarity between the two is compared to a threshold to decide a match. The threshold can be adjusted to trade off false rejections (FR), i.e.wrongly rejecting true matches, with false alarms (FA), i.e.wrongly accepting mismatches. We report results in terms of equal error rate, i.e.when FR = FA. We construct our validation and test sets from those used for the 1:2 matching tests, by separating each trial into two, one comprising a matched pair, and the other a mismatched pair. Thus, our validation and test sets are exactly twice as large as those for the 1:2 test.

5

Under review as a conference paper at ICLR 2019

· Retrieval. The gallery comprises a large number of instances, one or more of which might match the probe. The task is to order the gallery such that the entries in the gallery that match the probe lie at the top of the ordering. Here, we report performance in terms of Mean Average Precision (MAP) Manning et al. (2008). Here we use the entire collection of 58,420 test faces as the gallery for each of our 21,799 test voices, when retrieving faces from voices. For the reverse (retrieving voices from faces), the numbers are reversed.
Each result is obtained by averaging the performances of 5 models, which are individually trained.
Covariates in Training and Testing. We use the three covariates provided in the dataset, namely identity (I), gender (G), and nationality (N) for our experiments. The treatment of covariates differs for training and test.
· Training. For training, supervision may be provided by any set of (one two or three) covariates. We consider all combinations of covariates, I, G, N, (I,G), (I,N), (G,N) and (I,G,N). Increasing the number of covariates effectively increases the supervision provided to training. All chosen covariates were assigned a weight of 1.0.
· Testing. As explained in Appendix B, simply recognizing a covariate such as gender can result in seemingly significant matching performance. For instance, just recognizing the subjects' gender from their voice and images can result in a 33% EER for verification, and 25% error in matching for the 1 : 2 tests. In order to isolate the effect of covariates on performance hence we also stratify our test data by them. Thus we construct 4 testing groups based on the covariates, including the unstratified (U) group, stratified by gender (G), stratified by nationality (N), and stratified by gender and nationality (G, N). In each group the test set itself is separated into multiple strata, such that for all instances within any stratum the covariate values are the same.

3.1 CROSS-MODAL MATCHING

In this section we report results on the 1:2 and 1:N matching tests. In order to ensure that the embedding networks do indeed leverage on accurate modelling of covariates, we first evaluate the classification accuracy of the classification networks for the covariates themselves. Table 1 shows the results.

method

gender classification nationality classification

voice face

voice

face

DIMNet-I - - - -

DIMNet-G 97.48 99.22

-

-

DIMNet-N

-

-

74.86

60.13

DIMNet-IG 97.70 99.42

-

-

DIMNet-IN

-

- 74.17 60.27

DIMNet-GN 97.59 99.06

74.62

60.50

DIMNet-IGN 97.69 99.15 74.37 59.88

The rows of the table show the covariates used to super- Table 1: Acc. (%) of covariate prediction. vise the learning. Thus, for instance, the row labelled
"DIMNet-I" shows results obtained when the networks have been trained using ID alone as covari-
ate, the row labelled "DIMNet-G" shows results when supervision is provided by gender, "DIMNet-
IG" has been trained using ID and gender, etc.

The columns of the table show the specific covariate being evaluated. Since the identities of subjects in the training and test set do not overlap, we are unable to evaluate the accuracy of ID classification. Note that we can only test the accuracy of the classification network for a covariate if it has been used in the training. Thus, classification accuracy for gender can be evaluated for DIMNet-G, DIMNetGN and DIMNet-IGN, while that for nationality can be evaluated for DIMNet-N, DIMNet-GN and DIMNet-IGN.

The results in Table 1 show that gender is learned very well, and in all cases gender recognition accuracy is quite high. Nationality, on the other hand, is not a well-learned classifier, presumably because the distribution of nationalities in the data set is highly skewed Nagrani et al. (2018b), with nearly 65% of all subjects belonging to the USA. It is to be expected therefore that nationality as a covariate will not provide sufficient supervision to learn good embeddings.

1:2 matching. Table 2 shows the results for the 1:2 matching tests. In the table, the row labelled "SVHF-Net" gives results obtained with the model of Nagrani et al. Nagrani et al. (2018b).

The columns are segregated into two groups, one labelled "voice  face" and the other labelled "face  voice". In the former, the probe is a voice recording, while the gallery comprises faces. In the later the modalities are reversed. Within each group the columns represent the stratification of the test set. "U" represents test sets that are not stratified, and include the various covariates in the same proportion that they occur in the overall test set. The columns labelled "G" and "N" have

6

Under review as a conference paper at ICLR 2019

been stratified by gender and nationality, respectively, while the column "G,N" represents data that have been stratified by both gender and nationality. In the stratified tests, we have ensured that all data within a test instance have the same value for the chosen covariate. Thus, for instance, in a test instance for voice  face in the "G" column, the voice and both faces belong to the same gender. This does not reduce the overall number of test instances, since it only requires ensuring that the gender of the imposter matches that of the probe instance.

We make several observations. First, DIMNet-I performs better than SVHFNet, improving the accuracies by 2.45%-4.02% for the U group, and 7.01%-8.38% for the G group. It shows that mapping voices and faces to their common covariates is an effective strategy to learn representations for crossmodal matching.

method
Nagrani et al. (2018b) DIMNet-I DIMNet-G DIMNet-N DIMNet-IG DIMNet-IN DIMNet-GN DIMNet-IGN

voice  face (ACC %)

U G N G, N

81.00 63.90 -

-

83.45 70.91 81.97 69.89

72.90 50.32 71.92 50.21

57.53 55.33 53.04 51.96

84.12 71.32 82.65 70.39

82.95 70.04 81.04 68.59

75.92 56.66 72.94 53.48

83.73 70.76 81.75 69.17

face  voice (ACC %)

U G N G, N

79.50 63.40 -

-

83.52 71.78 82.41 70.90

72.47 50.48 72.15 50.61

56.20 54.34 53.90 51.97

84.03 71.65 82.96 70.78

82.86 70.91 81.91 70.22

73.78 54.90 72.63 53.45

83.63 71.42 82.50 70.46

Table 2: Comparison of performance of 1:2 matching comparisons, for models trained using different sets of covariates.

Second, DIMNet-I produces significantly better embeddings that DIMNet-G and DIMNet-N, highlighting the rather unsurprising fact that ID provides the more useful information than the other two covariates. In particular, DIMNet-G respectively achieves 72.90% and 72.47% for voice to face and face to voice matching using only gender as a covariate. This verifies our hypothesis that we can achieve almost 75% matching accuracy by only using the gender. These numbers also agree with the performance expected from the numbers in Table 1 and the analysis in Appendix B. As expected, nationality as a covariate does not provide as good supervision as gender. DIMNet-IG is marginally better than DIMNet-I, indicating that gender supervision provides additional support over ID alone.

Third, we note that while DIMNet-I is able to achieve good performance on the dataset stratified by gender, DIMNet-G only achieves random performance. The performance achieved by DIMNet-G on the U dataset is hence completely explained by gender matching. Once again, the numbers match our expectations (Appendix B).

accuracy (%)

1:N matching. We also experiment for N > 2. Unlike SVHF-Net Nagrani et al. (2018b) that needs to train different models for different N in this setting, we use the same model for different N . The results in Fig. 3 shows accuracy as a function of N for various models.

90 80 70 60 50 40

SVHF-Net DIMNets-I DIMNets-G DIMNets-N DIMNets-IG DIMNets-IN DIMNets-GN DIMNets-IGN

All the results in Fig. 3 are consistent with Table 2. As 30

expected, the performance of all methods degrades with 20

increasing N . In general, DIMNets that use ID as supervision outperform SVHF-Net by a considerable margin,

10 2 3 4 5 6 7 8 9 10
size of gallery

showing that DIMNets are able to make best use of the Figure 3: Performance of 1:N matching

ID information. We obtain the best results when both ID

and gender are used as supervision covariates. However, The results obtained using only gender

information as covariate is much worse, which is also consistent with our analysis in Appendix B.

3.2 CROSS-MODAL VERIFICATION

For verification, we need to determine whether an audio segment and a face image are from the same ID or not. We report the equal error rate (EER) for verification in Table 3.
In general, DIMNets that use ID as a covariate achieve an EER of about 25%, which is considerably lower than the 33% expected if the verification were based on gender matching alone. The results

method
DIMNet-I DIMNet-G DIMNet-N DIMNet-IG DIMNet-IN DIMNet-GN DIMNet-IGN

verification (EER %) U G N G, N
24.95 34.95 25.92 35.74 34.86 49.69 35.13 49.67 45.89 46.97 47.89 48.87 24.56 34.84 25.54 35.73 25.54 36.22 27.25 37.39 33.28 46.65 34.77 48.08 25.00 35.76 26.80 37.30

in Table 3 show that using both gender and ID information as co- Table 3: Verification results.

variates can further improve the performance over using ID alone,

well validating the superiority of our multi-task learning framework. Using proper combination of

covariates is crucial to the performance. ID is arguably the most effective covariate supervision.

More interestingly, nationality is seen to be an ineffective covariate, while gender alone as a covari-

ate produces results that well matches our expectation.

7

Under review as a conference paper at ICLR 2019

(a) Training Set

Marilu Henner Gillian Jacobs Jason Segel Katherine Jenkins Paul Dano Harry Treadaway Jason Earles Kathryn Prescott

voice male female

face

(b) Testing Set

Cote de Pablo David Tennant Dermot Mulroney Casey Wilson Cheryl Ladd Ellen Wong Cedric the Entertainer Dean Cain

Figure 4: Visualization of voice and face embeddings using multi-dimensional scaling Wickelmaier (2003) . The left panel shows subjects from the training set, while the right panel is from the test set.

3.3 CROSS-MODAL RETRIEVAL

We also perform retrieval experiments using voice or face as query. Table 4 lists the mean average precision (mAP) of the retrieval for various models.
The columns in the table represent the covariate being retrieved. Thus, for example, in the "ID" column, the objective is to retrieve gallery items with the same ID as the query, whereas in the "gender" column the objective is to retrieve the same gender.

method
DIMNet-I DIMNet-G DIMNet-N DIMNet-IG DIMNet-IN DIMNet-GN DIMNet-IGN

voice  face (mAP %) ID gender nationality
4.25 89.57 43.26 1.07 97.84 41.56 1.24 56.99 45.69 4.42 93.10 43.22 3.94 89.72 43.95 1.89 95.89 45.20 4.07 92.30 44.10

face  voice (mAP %) ID gender nationality
4.17 88.50 43.68 1.15 97.15 41.97 1.03 56.90 49.30 4.23 92.16 43.86 3.99 88.39 45.93 1.64 93.95 48.39 4.05 91.31 45.82

Table 4: Retrieval performance (mAP).

We note that ID-based DIMNets produce the best features for retrieval, with the best performance obtained with DIMNet-IG. Also, as may be expected, the covariates used in training result in the best retrieval of that covariate. Thus, DIMNet-G achieves an mAP of nearly 98% on gender, though on retrieval of ID it is very poor. On the other hand, DIMNet-I is not nearly as successful at retrieving gender. As in other experiments, nationality remains a poor covariate in general.

3.4 COMPARISONS TO THE CURRENT STATE-OF-THE-ART

We compare DIMNet with the state-ofthe-art Nagrani et al. (2018a). The results are reported in Table 5. Note that it is fair comparison because the DIMNet models

Seen-Heard

Unseen-Unheard

U G N A G, N, A U G N A G, N, A

Nagrani et al. (2018a) 87.0 74.2 85.9 86.6 74.0 78.5 61.1 77.2 74.9 58.8

DIMNet-I

95.1 90.8 93.4 95.2 88.9 82.5 71.0 81.1 77.7 62.8

DIMNet-IG

94.7 89.8 93.2 94.8 87.8 83.2 71.2 81.9 78.0 62.8

in this section are trained with and evalu- Table 5: AUCs (%) of DIMNets under different testing groups. ated on the same released datasets in Na-

grani et al. (2018a). We compute the area under the curve (AUC) for different testing groups.

It is clear that DIMNets produce better features than Nagrani et al. (2018a) for pair-wise verification on both seen-heard and unseen-unheard scenarios. Specifically, DIMNets achieve 8%-15% absolute and 3%-10% absolute improvements on seen-heard and unseen-unheard test set, respectively. Compared to DIMNet-IG, DIMNet-I performs better on the seen-heard test set while DIMNet-IG is better on the unseen-unheard test set. It implies that introducing useful covariates improves the generalization capability of DIMNet.

4 DISCUSSIONS AND CONCLUDING REMARKS
We have proposed that it is possible to learn common embeddings for multi-modal inputs, particularly voices and faces, by mapping them individually to common covariates. In particular, the proposed DIMNet architecture is able to extract embeddings for both modalities that achieves consistently better performance than the methods that directly map faces to voices.
The approach also provides us the ability to tease out the influence of each of the covariates of voice and face data, in determining their relation. The results show that the strongest covariate, not unexpectedly, is ID. The results also indicate that prior results by other researchers who have attempted to directly match voices to faces may perhaps not be learning any direct relation between the two, but implicitly learning about the common covariates, such as ID, gender, etc.

8

Under review as a conference paper at ICLR 2019
Our experiments also show that although we have achieved possibly the best reported performance on this task, thus far, the performance is not anywhere close to prime-time. In the 1 : N matching task, performance degrades rapidly with increasing N , indicating a rather poor degree of true match.
To better understand the problem, we have visualized the learned embeddings from DIMNet-I in Fig. 4 to provide more insights. The visualization method we used is multi-dimensional scaling (MDS) Wickelmaier (2003), rather than the currently more popular t-SNE van der Maaten & Hinton (2008). This is because MDS tends to preserve distances and global structure, while t-SNE attempts to retain statistical properties and highlights clusters, but does not preserve distances.
From Fig. 4, we immediately notice that the voice and face data for a subject are only weakly proximate. While voice and face embeddings for a speaker are generally relatively close to each other, they are often closer to other subjects. Interestingly, the genders separate (even though gender has not been used as a covariate for this particular network), showing that at least some of the natural structure of the data is learned. Fig. 4 shows embeddings obtained from both training and test data. We can observe similar behaviors in both, showing that the the general characteristics observed are not just the outcome of overfitting to training data. The visualization in Fig. 4 also shows that there is still significant room for improvement. For example, it may be possible to force compactness of the distributions of voice and face embeddings through modified loss functions such as the center loss Wen et al. (2016) or angular softmax loss Liu et al. (2016; 2017), or through an appropriately designed loss function that is specific to this task.
REFERENCES
P. Belin, P.E.G. Bestelmeyer, M. Latinus, and R. Watson. Understanding voice perception. British Journal of Psychology, 108:711­725, 2011. 1
Pascal Belin, Shirley Fecteau, and Catherine Bedard. Thinking the voice: neural correlates of voice perception. Trends in cognitive sciences, 8(3):129­135, 2004. 1
Andrew W Ellis. Neuro-cognitive processing of faces and voices. In Handbook of research on face processing, pp. 207­215. Elsevier, 1989. 1
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 11
Miyuki Kamachi, Harold Hill, Karen Lander, and Eric Vatikiotis-Bateson. Putting the face to the voice': Matching identity across modality. Current Biology, 13(19):1709­1714, 2003. 1
Changil Kim, Hijung Valentina Shin, Tae-Hyun Oh, Kaspar Alexandre, Mohamed Elgharib, and Wojciech Matusik. On learning associations of faces and voices. arXiv preprint arXiv:1805.05553, 2018. 1, 2
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 11
Christoph Lippert, Riccardo Sabatini, M Cyrus Maher, Eun Yong Kang, Seunghak Lee, Okan Arikan, Alena Harley, Axel Bernal, Peter Garst, Victor Lavrenko, et al. Identification of individuals by trait prediction using whole-genome sequencing data. Proceedings of the National Academy of Sciences, 114(38):10166­10171, 2017. 3
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural networks. In ICML, 2016. 9
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In CVPR, 2017. 9
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schu¨tze. Evaluation in information retrieval. In Introduction to Information Retrieval, chapter 8, pp. 159­160. Cambridge University Press, 2008. 6
Hunter A McAllister, Robert HI Dale, Norman J Bregman, Allyssa McCabe, and C Randy Cotton. When eyewitnesses are also earwitnesses: Effects on visual and voice identifications. Basic and Applied Social Psychology, 14(2):161­170, 1993. 1
9

Under review as a conference paper at ICLR 2019
A. Nagrani, J. S. Chung, and A. Zisserman. Voxceleb: a large-scale speaker identification dataset. In INTERSPEECH, 2017. 1, 5
Arsha Nagrani, Samuel Albanie, and Andrew Zisserman. Learnable pins: Cross-modal embeddings for person identity. arXiv preprint arXiv:1805.00833, 2018a. 1, 2, 8
Arsha Nagrani, Samuel Albanie, and Andrew Zisserman. Seeing voices and hearing faces: Crossmodal biometric matching. arXiv preprint arXiv:1804.00326, 2018b. 1, 2, 5, 6, 7, 11
Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. Deep face recognition. In BMVC, 2015. 5
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. The kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011. 11
Stefan R Schweinberger, David Robertson, and Ju¨rgen M Kaufmann. Hearing facial identities. Quarterly Journal of Experimental Psychology, 60(10):1446­1456, 2007. 1
Stefan R Schweinberger, Nadine Kloth, and David MC Robertson. Hearing facial identities: Brain correlates of face­voice integration in person identification. Cortex, 47(9):1026­1037, 2011. 1
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579­2605, 2008. 9
Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recognition. In ECCV, 2016. 9
Yandong Wen, Mahmoud Al Ismail, Bhiksha Raj, and Rita Singh. Optimal strategies for matching and retrieval problems by comparing covariates. arXiv preprint arXiv:1807.04834, 2018. 12
Florian Wickelmaier. An introduction to mds. Sound Quality Research Unit, Aalborg University, Denmark, 46(5), 2003. 8, 9
Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE Signal Processing Letters, 23(10):1499­1503, 2016. 11
10

Under review as a conference paper at ICLR 2019

APPENDIX A EXPERIMENTAL DETAILS

A.1 DATASET

The Voxceleb dataset consists of 153,516 audio segments from 1,251 speakers. Each audio segment is taken from an online video clip with an average duration of 8.2 seconds. For the face dataset, we used a manually filtered version of VGGFace. After face detection, there remain 759,643 images from 2,554 subjects. The data are split into train/validation/test sets, following the settings in Nagrani et al. (2018b). Details are shown in Table 6

# of samples speech segments face images IDs genders nationalities testing instances

train 112,697 313,593
924 2 32 -

validation 14,160 36,716 112 2 11
4,678,897

test 21,799 58,420
189 2 18 6,780,750

total 148,656 408,729 1,225
2 36 11,459,647

Table 6: Statistics for the data appearing in VoxCeleb and VGGFace.

A.2 PREPROCESSING
We employ separated data preprocessing pipelines for audio segments and face images. For audio segments, we use an energy-based voice activity detector Povey et al. (2011) to isolate speechbearing regions of the recordings. Subsequently, 64-dimensional log mel-spectrograms are generated, using an analysis window of 25ms, with hop of 10ms between frames. We perform mean and variance normalization of each mel-frequency bin.
For training, we randomly crop out regions of varying lengths of 300 to 800 frames (so the size of the input spectrogram ranges from 300 × 64 to 800 × 64 for each mini-batch, around 3 to 8 seconds). For the face data, facial landmarks in all images are detected using MTCNN Zhang et al. (2016). The cropped RGB face images of size 128 × 128 × 3 are obtained by similarity transformation. Each pixel in the RGB images is normalized by subtracting 127.5 and then dividing by 127.5. We perform data augmentation by horizontally flipping the images with 50% probability in minibatches (effectively doubling the number of face images).
A.3 TRAINING
The details of network architectures are shown in Table 7. For the voice network, we use 1D convolutional layers, where the convolution is performed along the axis that corresponds to time. The face network employs 2D convolutional layers. For both, the convolutional layers are followed by batch normalization (BN) Ioffe & Szegedy (2015) and rectified linear unit activations (ReLU) Krizhevsky et al. (2012). The final face embedding is obtained by averaging the feature maps from the final layer, i.e.through average pooling. The final voice embedding is obtained by averaging the feature maps at the final convolutional layer along the time axis alone.
We follow the typical settings of SGD for optimization. Minibatch size is 256. The momentum and weight decay values are 0.9 and 0.001 respectively. To learn the networks from scratch, the learning rate is initialized at 0.1 and divided by 10 after 16K iterations and again after 24K iterations. The training is completed at 28K iterations.
APPENDIX B EXPECTED PERFORMANCE BASED ON GENDER MATCHING
In this appendix we discuss the performance to be expected in the matching and verification tests, when the matching is done based purely on gender.
We assume below that in any distribution of human-subject data, the division of subjects between male and female genders to be half and half.
11

Under review as a conference paper at ICLR 2019

layer

embedding network

Conv

classification network

AvgPool FC

voice
(3, 256)/2,1 (3, 256)/1,1
(3, 256)/1,1 (3, 384)/2,1 (3, 384)/2,1
(3, 384)/1,1 (3, 256)/2,1 (3, 576)/2,1
(3, 576)/1,1 (3, 256)/2,1 (3, 864)/2,1
(3, 864)/1,1 (3, 64)/2,1
t×1

face
(3 × 3, 64)/2,1 (3 × 3, 64)/1,1
(3 × 3, 64)/1,1 (3 × 3, 128)/2,1 (3 × 3, 128)/1,1
(3 × 3, 128)/1,1 (3 × 3, 256)/2,1 (3 × 3, 256)/1,1
(3 × 3, 256)/1,1 (3 × 3, 512)/2,1 (3 × 3, 512)/1,1
(3 × 3, 512)/1,1 (3 × 3, 64)/2,1
h×w×1

64 × 924, 64 × 2, 64 × 32

Table 7: The detailed CNNs architectures. The numbers within the parentheses represent the size and number of filters, while the subscripts represent the stride and padding. So, for example, (3, 64)/2,1 denotes a 1D convolutional layer with 64 filters of size 3, where the stride and padding are 2 and 1 respectively, while (3 × 3, 64)/2,1 represents a 2-D convolutional layer of 64 3 × 3 filters, with stride 2 and padding 1 in both directions. Note that 924, 2, and 32 are the number of unique values taken by the ID, gender, and nationality
covariates, respectively.

It is to be noted that gender is merely an illustrative example here; the analysis can be extended to other covariates. For a more detailed analysis of covariates with more values and unbalanced distributions, please refer to Wen et al. (2018).

B.1 ACCURACY OF 1:2 MATCHING BASED ON GENDER
We show that the equal-error-rate for 1:2 matching can be as high as 25%, through gender matching alone.
The problem is as follows: a probe input (voice or face), and a gallery consisting of two inputs (face or voice), one of which is from the same subject as the probe. We must identify which of the two is the true match.

B.1.1 PERFECT GENDER IDENTIFICATION
Consider the situation where we are able to identify the gender of the subject of the data (face or voice) perfectly.
There are two possibilities: (a) both probe instances are the same gender, and (b) they are different genders. Each of the two possibilities occurs with a probability of 0.5
We employ the following simple strategy: If the two gallery instances are different genders, then we select the instance whose gender matches the probe. In this case, clearly, the probability of error is 0. If the two instances are the same gender, we select one of them randomly with a probability of 0.5. The probability of error here is 0.5.
Thus, the overall probability of error is
P rob(error) = 0.5 × 0 + 0.5 × 0.5 = 0.25.

B.1.2 IMPERFECT GENDER IDENTIFICATION
Now let us consider the situation where gender identification itself is imperfect, and we have error rates ef and ev in identifying the gender of faces and voices, respectively. Assume the error rates are known. We will assume below that gallery entries are faces, and probe entries are voices. (The equations are trivially flipped to handle the converse case).

12

Under review as a conference paper at ICLR 2019

Since we are aware that we sometimes make mistakes in identifying gender, we modify our strategy as follows: when the two gallery items are found to have different genders, we select the entry with the same gender as the probe P of the time (so that if the gender classification was correct, we would have a match error rate of (1-P )). When both gallery items are found to be the same gender, we choose randomly.
The actual error can now be computed as follows. The gallery items are both of the same gender in 0.5 of the trials, and of mismatched gender in the remaining 0.5 of the trials.
When both gallery items have the same gender, regardless of the strategy chosen, the probability of error is 0.5 (by symmetry).
When both gallery items are of mismatched gender, we have 8 combinations of correctness of gender-classification. Table 8 lists all eight, along with the probability of matching error (in the final column). Taking type 1 as an example, we have probability (1 - ev)(1 - ef )2 that the gender

type probe gallery1 gallery2

P rob(error, type)

1 (1 - ev)(1 - ef )2 · (1 - P )

2×

(1 - ev)ef (1 - ef ) · 0.5

3 × (1 - ev)(1 - ef )ef · 0.5

4 5×

×

×

(1 - ev)(ef )2 · P ev(1 - ef )2 · P

6×

×

evef (1 - ef ) · 0.5

7×

× ev(1 - ef )ef · 0.5

8×

×

×

ev(ef )2 · (1 - P )

Table 8: the possible error types with probabilities.

of both probe and galleries are correctly classified. In this case, our strategy gives us an error of (1 - P ). For type 2, the gender of probe and one of the gallery items is correctly classified, while the other gallery item is misclassified, we have an error of 0.5. If we go through all the cases, the total error P rob(error) can be computed as

8

P rob(error) = 0.25 + 0.5

P rob(error, type)

type=1

= 0.25 + 0.5(2ef ev - ev - ef + 1 +P (2ef + 2ev - 4ef ev - 1))

Our objective is to minimize P rob(error), so we must choose P to minimize the above term. I.e. we must solve
arg min 2ef ev - ev - ef + 1 + P (2ef + 2ev - 4ef ev - 1)
P
s.t. 0  P  1.
Its easy to see that the solution for P is 1.0 if its multiplicative factor is negative in the above equation, and 0 otherwise, i.e.
P = 1, if ef + ev < 2ef ev + 0.5 0, else
The corresponding match error rates are P rob(error) = 0.25 + 0.5(ef + ev - 2evef ) and 0.75 + ef ef - 0.5(ev + ef ) respectively.
Although complicated looking, the solution is, in fact, quite intuitive. When gender classification is either better than random for both modalities (i.e.ef , ev > 0.5) or worse than random for both (ef , ev < 0.5), the best strategy is to select the gallery item that matches the gender of the probe. If either of these is random (i.e.either ef or ev is 0.5) , the choice of P does not matter, and the error is 0.5. If one of the two is correct more than half the time, and the other is wrong more than half the time (e.g.ef < 0.5, ev > 0.5), the optimal choice is to select the gallery item that is classified as mismatched in gender with the probe.

13

Under review as a conference paper at ICLR 2019

B.2 ACCURACY OF 1:N MATCHING BASED ON GENDER
We now consider the best achievable performance on 1:N matching, when the only information known is the gender of the voices and faces.

B.2.1 PERFECT GENDER IDENTIFICATION

Consider the situation where the gender of the faces and voices in each test trial is perfectly known.

We employ the following strategy: we randomly select one of the gallery instances that have the

same gender as the probe instance. If there are K imposter gallery instances of the same gender as

the

probe

instance,

the

expected

accuracy

is

1 K +1

.

The

probability

of

randomly

having

K

of

N

-

1

imposters of the same gender as the probe is given by

P rob(K; N - 1) = N - 1 0.5N-1 K

The overall accuracy is given by:

giving us the error

N-1 P rob(K; N - 1) P rob(correct) =
K +1
K =0

N -1
= 0.5N-1

N -1

1

K K+1

K=0

0.5N-1 N N =
NK
k=1

0.5N-1(2N - 1) =
N (2 - 0.5N-1) =,
N

P (error)

=

1

-

(2

-

0.5N -1 ) .

N

B.2.2 IMPERFECT GENDER IDENTIFICATION

Consider now that the gender recognition is erroneous for voices with probability ev and for faces with probability ef . Note that regardless of the error in gender recognition, the probability of any noisy gallery entry having any gender remains 0.5.

To account for the possible error in gender classification, we consider the following stochastic policy: with probability P we select one of the gallery entries with the same gender assigned to probe (by the gender classifier), and with probability 1 - P we choose one of the entries with the opposite gender assigned to the probe.

Let  represent the probability that the genders assigned to probe and the corresponding gallery entry by their respective classifiers are identical.

 = evef + (1 - ev)(1 - ef ).

The equation above considers both possibilities: that both the probe and its matching gallery entry
are correctly classified, and that both of them are misclassified. It follows that the probability and its matching gallery entries are assigned different genders is 1 - .

Given that we have selected the correct gender for retrieval from the the gallery (i.e.that the gender

we have selected is the same as that assigned to the gallery entry matching the probe by the face

classifier), using the same analysis as in Section B.2.1, we obtain the following probability of being

correct:

P (correct|correct gender) = (2 - 0.5N-1) N

14

Under review as a conference paper at ICLR 2019

The probability of selecting the correct gender is given by P (correct gender) = P  + (1 - P )(1 - )

Since the probability of being correct when we choose the wrong gender is 0, the overall probability of being correct is

P (correct) = (P  + (1 - P )(1 - )) (2 - 0.5N-1) N
(2 - 0.5N-1) = (P (2 - 1) + 1 - )
N

(4)

Maximizing the probability requires us to solve

arg max P (2 - 1) + 1 - , s.t. 1  P  0
P

which gives us the optimal P as

P = 1 if  > 0.5 0 otherwise

and the optimal error as

P (error) =

1

-



(2-0.5N -1 ) N

,

if

 > 0.5

1

-

(1

-

)

(2-0.5N -1 ) N

otherwise.

B.3 EER OF VERIFICATION BASED ON GENDER
Here we show that the equal-error-rate for verification (determining if the the subjects in two recordings are the same) can be as high as 33%, through gender matching alone.
The problem is as follows: we are given a pair of inputs, one (features extracted from) a face, and the other a voice. We must determine whether they are both from the same speaker.
The test set include some number of "positives", where both do belong to the same subject, and some "negatives", where both do not. If a positive is falsely detected as a negative, we have an instance of false rejection. If a negative is wrongly detected as a positive, we have an instance of false acceptance.
Let FR represent the `false rejection rate", i.e.the fraction of all positives that are wrongly rejected. Let FA represent the "false acceptance rate", i.e.the fraction of negatives that are wrongly accepted. Any classifier can generally be optimized to trade off FR against FA. The "Equal Error Rate" (EER) is achieved when FR = FA.
Among the "positive" test pairs, both voice and face in each pair have the same gender. We assume the "negative" test instances are drawn randomly, i.e., 0.5 of all negative pairs have the same gender, while the remaining 0.5 do not.

B.3.1 PERFECT GENDER IDENTIFICATION
Consider the situation where we know the subject's gender for both the voices and faces (or, alternately, are able to identify the gender from the voice or face perfectly).
We employ the following strategy: if the gender of the voice and face are different, we declare it as a negative 100% of the time. If the two are from the same gender, we randomly call it a positive P of the time, where 0  P  1.0.
Using this strategy, the false acceptance rate is:
FA = 0.5 × 0 + 0.5 × P = 0.5P.
Here we're considering that using our strategy we never make a mistake on the 50% of negative pairs that have mismatched genders, but are wrong P of the time on the negative pairs with matched genders.

15

Under review as a conference paper at ICLR 2019

Among the positives, where all pairs are gender matched, our strategy of accepting only a fraction P of them as positives will give us a false rejection rate FR = 1 - P .

The equal error rate is achieved when FR = FA, i.e.

0.5P = 1 - P,

giving us P

=

2 3

,

i.e.the

best

EER

is

achieved

when

we

accept

gender-matched

pairs

two-thirds

of

the time.

The

EER

itself

is

0.5P

=

1 3

.

Thus, merely by being able to identify the gender of the subject accurately, we are able to verification EER of 0.33.

B.3.2 IMPERFECT GENDER IDENTIFICATION
Now let us consider the situation where gender identification itself is imperfect, and we have error rates ef and ev in identifying the gender of the face and the voice, respectively. Assume these error rates are known.
To account for this, we modify our strategy: when we find the genders of the voice and face to match, we accept the pair as positive P of the time, but when they are mismatched we still accept them as positive Q of the time.
Let  represent the probability that we will correctly call the polarity of the gender match between the voice and the face. I.e.  is the probability that if the two have the same gender, we will correctly state that they have the same gender, or if they are of opposite gender, we will correctly state they are of opposite gender.
 = (1 - ef )(1 - ev) + ef ev.
This combines two terms: that we call the genders of both the voice and face correctly, and that we call them both wrongly (which also results in finding the right polarity of the relationship). Its easy to see that 0    1, and to verify that when gender identification is perfect,  = 1.0. The probability of calling the polarity of the gender relationship wrongly is 1 - .
Among the positive test pairs, all pairs are gender matched. We will correctly call  of these as gender matched. Using our strategy, our error on these instances is (1 - P ). We will incorrectly call 1 -  of these as gender mismatched, and the error on these instances is (1 - Q). So the overall false rejection rate is given by
FR = (1 - P ) + (1 - )(1 - Q) = 1 - P - (1 - )Q

Among the negative pairs, half are gender matched, and half are gender mismatched. Using the same logic as above, the error on the gender-matched negative pairs is P + (1 - )Q. Among the gender mismatched pairs the error is Q + (1 - )P . The overall false acceptance rate is given by
FA = 0.5(P + (1 - )Q) + 0.5(Q + (1 - )P ) = 0.5(P + Q).
Equating FA and FR as the condition for EER, we obtain 1 - P - (1 - )Q = 0.5(P + Q)
= (3 - 2)Q + (1 + 2)P = 2.

Since at EER, the EER equals FA, and we would like to minimize it, we obtain the following solution to determine the optimal P and Q:
arg min P + Q
P,Q
s.t.1  P, Q  0, (3 - 2)Q + (1 + 2)P = 2.
For  > 0.5 it is easy to see that the solution to this is obtained at
Q=0 2
P= . 1 + 2

16

Under review as a conference paper at ICLR 2019

For  < 0.5 the optimal solution is at

P =0 2
Q = 3 - 2 .

That is, when the probe and gallery classifiers are likely to make the same error more than half the

time, the optimal solution is to always reject pairs detected as having mismatched genders, and to

accept

matched-gender

pairs

2 1+2

of

the

time.

The

optimal

EER

is

1 1+2

.

When they are more likely to make different errors, the optimal solution is to always reject pairs

detected

as

having

matched

genders,

and

to

accept

mismatched-gender

pairs

2 3-2

of

the

time.

The

optimal

EER

now

is

1 3-2

.

Note that if an operating point other than EER were chosen to quantify performance (e.g.FA = FR for  = 1, or for some fixed FA or FR), the above analysis can be modified to accommodate it, provided a feasible solution exists.

17

