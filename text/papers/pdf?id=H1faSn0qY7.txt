Under review as a conference paper at ICLR 2019
DL2: TRAINING AND QUERYING NEURAL NETWORKS WITH LOGIC
Anonymous authors Paper under double-blind review
ABSTRACT
We present DL2, a system for training and querying neural networks with logical constraints. The key idea is to translate these constraints into a differentiable loss with desirable mathematical properties and to then either train with this loss in an iterative manner or to use the loss for querying the network for inputs subject to the constraints. We empirically demonstrate that DL2 is effective in both training and querying scenarios, across a range of constraints and data sets.
1 INTRODUCTION
We present DL2, a system for training neural networks to satisfy logical constraints and query networks for inputs that influence their decisions subject to constraints. DL2 enables users to enforce background knowledge in the network or to interact with the network in order to learn about its behavior. The key insight behind DL2 (acronym for Deep Learning with Differentiable Logic) is to translate a logical formula into a non-negative loss with two properties: (P1) if an input has a loss of zero for a formula, then that input satisfies the formula, and (P2) the loss is differentiable almost everywhere. Property P1 enables us to obtain inputs to the neural network which satisfy the formula by minimizing the formulas' DL2 loss, while property P2 allows use of off-the-shelf gradient-based optimization to query or train the network with logical constraints. Compared to prior work, DL2 is more general as it supports constraints which could not be handled so far.
Training with DL2 DL2 trains networks with constraints by optimizing their differentiable loss. To make optimization tractable, we exclude constraints on inputs that capture convex sets and include them as constraints to the optimization goal. We then optimize with projected gradient descent (PGD), which has been shown successful for training with robustness constraints (Madry et al., 2017). The expressiveness of DL2 along with the tractable optimization through PGD enables us to train with new, interesting constraints. For example, we can express constraints over probabilities which are not explicitly computed by the network, such as the following constraint for CIFAR-100:
x. ppeople(x) <  ppeople(x) > 1 - This constraint says that for any network input x (network is parameterized by ), the probability of people (ppeople) is either very small or very large. However, CIFAR-100 does not have the class people, and thus we define it as a function of other probabilities: ppeople = pbaby + pboy + pgirl + pman + pwoman. We show that with a similar constraint (but with 20 classes), DL2 increases the prediction accuracy of CIFAR-100 networks in the semi-supervised setting. DL2 also outperforms other approaches with similar constraints (as they cannot support this kind of constraints).
DL2 can also constrain regression tasks. For example, GalaxyGAN (Schawinski et al., 2017) is a generator of galaxy images which preserves flux. With DL2, the constraint for flux preservation is naturally captured by sum(x) = sum(GalaxyGAN(x)) ­ this denotes that the sum of input pixels should be equal to the sum of output pixels.
Global training with an oracle We also show how to train with constraints that place restrictions on inputs which may not exist in the training set. Prior approaches for training with constraints (e.g., Xu et al. (2018)) focus on the given training set to locally train the network to meet the constraints. With DL2, we can, for the first time, query for inputs outside the training set and use them to globally train the network. Our approach splits the task of global training between: (i) the optimizer, which trains the network to meet the constraints for the given inputs, and (ii) the oracle,
1

Under review as a conference paper at ICLR 2019
which provides the optimizer with new inputs that aim to violate the constraints. To illustrate the kind of constraints we support, consider the following Lipshcitz condition:
z1  L(x1, ), z2  L(x2, ).||p(z1) - p(z2)||2 < L||z1 - z2||2 Here, for two inputs from the training set (x1, x2), any point in their -neighborhood (z1, z2) must satisfy the condition. This constraint is inspired by recent works (e.g., Gouk et al. (2018); Balan et al. (2017)) which showed that neural networks are more stable if satisfying the Lipschitz condition.
Querying with DL2 We also design an SQL-like language for querying with the network. This enables users to interact with neural networks by posing declarative queries, without worrying how the queries are answered. To illustrate, a recent paper (Song et al., 2018) shows how to generate adversarial examples with AC-GANs (Odena et al., 2016), which are class-conditional generators. The authors use this generator to create images from a certain class (e.g., 1) which fools a classifier (to classify as, e.g., 7). With DL2, this can be declaratively phrased as:
f i n d n[100] where n i n [-1, 1],
c l a s s (M_NN1(M_ACGAN_G(n, 1))) = 7 r e t u r n M_ACGAN_G(n, 1)
Queries are automatically compiled to a DL2 loss and optimized with an off-the-shelf gradient optimizer (e.g., L-BFGS-B) to find solutions. For example, the image to the right was returned by DL2. Our language can naturally capture many prior works at the declarative level, including finding neurons responsible for a given prediction (Olah et al., 2018), inputs that differentiate two networks (Pei et al., 2017), and adversarial example generation (e.g., Szegedy et al. (2013)).
Main Contributions The DL2 system is based on the following contributions:
· An approach for training and querying neural networks with logical constraints based on compiling constraints into a differentiable loss with desirable properties ((P1) and (P2)).
· A training procedure which extracts constraints on inputs that capture convex sets and includes them as PGD constraints, making optimization tractable.
· A declarative language for posing queries over neural network's inputs, outputs, and internal neurons. Queries are compiled into a differentiable loss and optimized with L-BFGS-B.
· An extensive evaluation demonstrating the effectiveness of DL2 in querying and training neural networks. Among other experimental results, we show for the first time, the ability to successfully train networks with constraints on inputs not in the training set.
2 RELATED WORK
Adversarial example generation (Pei et al., 2017; Goodfellow et al., 2014b) can be seen as querying the network with a fixed single query, while adversarial training (Madry et al., 2017) aims to enforce one specific constraint. Most works aiming to train neural networks with logic impose soft constraints, often through an additional loss (Pathak et al., 2015; Xu et al., 2018), and one work (Ma´rquez-Neila et al., 2017) shows that hard constraints have no empirical advantage over soft constraints. Probabilistic Soft Logic (PSL) (Kimmig et al., 2012) translates logic into continuous functions over the interval [0, 1]. However, as we demonstrate, PSL is not amenable to gradientbased optimization as gradients may easily become zero. Hu et al. (2016) build on PSL and show a teacher-student framework which distills rules into the training of neural networks. The idea is to formulate rule satisfaction as a convex problem with a closed-form solution. However, this formulation is restricted to rules over random variables and cannot express rules over probability distributions. In contrast, DL2 can express such constraints, for example p1 > p2, which requires that the network probability for class 1 is greater than for 2. Also, the convexity and the closed-form solution stem from the linearity of the rules in the neural network's output, meaning that non-linear constraints (e.g., Lipschitz condition, expressible with DL2) are fundamentally beyond the reach of this method. The work of Xu et al. (2018) is also restricted to constraints over random variables and is intractable for complicated constraints. Fu & Su (2016) reduce the satisfiability of floating-point formulas into numerical optimization, however, while similar, their loss is not differentiable. Like previous solutions, they also do not support constraints on probability distributions. Finally, unlike DL2, none of these approaches support constraints for regression tasks.
2

Under review as a conference paper at ICLR 2019

3 FROM LOGIC TO A DIFFERENTIABLE LOSS

In this section, we present our logical constraint language and show how to systematically translate constraints in this language to a differentiable loss. To simplify presentation, we treat all tensors as vectors with matching dimensions.

Logical Language Our language consists of quantifier-free logical constraints where constraints can be composed with conjunction (), disjunction () and negation (¬). Atomic constraints (literals) are comparisons of terms (here  {=, =, , <, , >}). Comparisons are defined for
scalars and applied element-wise on vectors. A term t is: (i) A variable z or a constant c , represent-
ing real-valued vectors; constants can be samples from the dataset. (ii) An expression over terms, including arithmetic expressions or function applications f : Rm  Rn, for m, n  Z+. Functions can be defined over the variables, constants, and also network parameters 1, . . . , l. Functions can
be the application of a neural network with parameters , the application of a specific neuron in
the network, or a computation over multiple neural networks. The only assumption on functions is
that they are differentiable (almost everywhere) in the variables and network parameters. We write t(z1, . . . , zk, c1, . . . , cj, 1, . . . l) to emphasize the variables, constants, and network parameters
that t can be defined over (that is, t may refer to only a subset of these symbols). We sometimes
omit the constants and network parameters (which are also constant) and abbreviate the variables by
z¯, i.e., we write t(z¯). Similarly, we write (z¯) to denote a constraint defined over the variables z¯.
When the variables are not important, we simply write .

Translation into loss Given a formula , we define the corresponding loss L() recursively on the structure of . The obtained loss is non-negative: for any assignment x¯ to the variables z¯, we have L()(x¯)  R0. Further, the translation has two properties: (P1) any x¯ for which the loss is zero (L()(x¯) = 0) is a satisfying assignment to  (denoted by x¯ |= ) and (P2) the loss is differentiable
almost everywhere. This construction avoids pitfalls of other approaches (see Appendix A).

We next formally define the translation rules. Since comparisons are applied element-wise (i.e., on scalars), atomic constraints are transformed into a conjunction of scalar comparisons:

L(t1

n

t2) := L

ti1

i=1

t2i

The comparisons = and  are translated based on a function d : R × R  R which is a continuous, differentiable almost everywhere, distance function with d(x1, x2)0 and d(x1, x2)=0  x1=x2:

L(t1 = t2) := d(t1, t2);

L(t1  t2) := 1t1>t2 · d(t1, t2)

Here, 1t1>t2 is an indicator function: it is 1 if t1 > t2, and 0 otherwise. The function d is a parameter of the translation and in our implementation we use the absolute distance |t1 - t2|. For the other comparisons, we define the loss as follows: L(t1 < t2) = L(t1 +  t2), for a small constant
> 0, L(t1 = t2) = L(t1 < t2  t2 < t1), and L(t1 > t2) and L(t1  t2) are defined analogously.

Conjunctions and disjunctions of formulas  and  are translated into loss as follows:

L(  ) := L() + L();

L(  ) := L() · L()

Note that L(  ) = 0 if and only if L() = 0 and L() = 0, which by construction is true if  and  are satisfied, and similarly L(  ) = 0 if and only if L() = 0 or L() = 0.
Translating negations Negations are handled by first eliminating them from the constraint through rewrite rules, and then computing the loss of their equivalent, negation-free constraint. Negations of atomic constraints are rewritten to an equivalent atomic constraint that has no negation (note that = is not a negation). For example, the constraint ¬(t1  t2) is rewritten to t2 < t1, while negations of conjunctions and disjunctions are rewritten by repeatedly applying De Morgan's laws: ¬(  ) is equivalent to ¬  ¬ and ¬(  ) is equivalent to ¬  ¬.
With our construction, we get the following theorem:
Theorem 1 For all x¯, if L()(x¯) = 0, then x¯ satisfies  (x¯ |= ). Conversely, for any , there is a   0 with lim 0  = 0 such that for all x¯, L()(x¯) >  implies that x¯ does not satisfy  (x¯ |= ).

3

Under review as a conference paper at ICLR 2019

Essentially, as we make smaller (and  approaches 0), we get closer to an if and only if theorem: if x¯ makes the loss 0, then we have a satisfying assignment; otherwise, if x¯ makes the loss > , then x¯ is not a satisfying assignment.

4 CONSTRAINED NEURAL NETWORKS

In this section, we present our method for training neural networks with constraints. We first define the problem, then provide our min-max formulation, and finally, discuss how we solve the problem. To simplify notation, we sometimes write a predicate  to denote its indicator function 1.

Training with constraints To train with a single constraint, we consider the following maximization problem over neural network weights :

arg max ES1,...,SmD z¯. (z¯, S¯, c¯, ) .


Here, S1, . . . , Sm (abbreviated by S¯) are independently drawn from an underlying data distribution D and  is a constraint over variables z¯, constants S¯ and c¯, and neural network weights . This

objective is bounded between 0 and 1, and it attains 1 if and only if the probability that the neural

network satisfies the constraint  is 1. We extend this definition to multiple constraints, by forming

a convex combination of their respective objectives: for w with

t i=1

wi

=

1

and

wi

>

0

for

all

i,

we consider

t

arg max wi · ES1,...,SmD z¯. i(z¯, S¯, c¯, ) .
 i=1

(1)

As standard, we train with the empirical objective where instead of the (unknown) distribution D, we use the training set T to draw samples.

To use our system, the user specifies the constraints 1, . . . , t along with their weights w1, . . . , wt. In the following, to simplify the presentation, we assume that there is only one constraint.

Formulation as min-max optimization We can rephrase training networks with constraints as minimizing the expectation of the maximal violation. The maximal violation is an assignment to the variables z¯ which violates the constraint (if it exists). That is, it suffices to solve the problem arg min ES1,...SmT [maxz1,...,zk ¬(z¯, S¯, c¯, )]. Assume that one can compute, for a given S¯ and , an optimal solution x¯S¯, for the inner maximization problem:

x¯S¯, = arg max ¬(z¯, S¯, c¯, ).
z 1 ,...,z k

(2)

Then, we can rephrase the optimization problem in terms of x¯S¯,:

arg min ES1,...SmT [¬(x¯S¯,, S¯, c¯, )].


(3)

The advantage of this formulation is that it splits the problem into two sub-problems and the overall optimization can be seen as a game between an oracle (solving (2)) and an optimizer (solving (3)).

Solving the optimization problems We solve (2) and (3) by translating the logical constraints
into differentiable loss (as shown in Sec. 3). Inspired by Theorem 1, for the oracle (Eq. (2)), we approximate the inner maximization by a minimization of the translated loss L(¬):

x¯S¯, = arg min L(¬)(z¯, S¯, c¯, ).
z 1 ,...,z k

(4)

For the optimizer, given a solution x¯S¯, from the oracle, we optimize the following loss using Adam

(Kingma & Ba, 2014):

ES1,...,SmT [L()(x¯S¯,, S¯, c¯, )].

(5)

Constrained optimization In general, the loss in (4) can sometimes be difficult to optimize. To

illustrate, assume that the random samples are input-label pairs (x, y) and consider the constraint:

(z, (x, y), ) = ||x - z||  = logit(z)y > .

4

Under review as a conference paper at ICLR 2019

Our translation of this constraint to a differentiable loss produces L(¬)(z, (x, y), ) = max(0, ||x - z|| - ) + max(0, logit(z)y - ).

This function is difficult to minimize because the magnitude of the two terms is different. This
causes first-order methods to optimize only a single term in an overly greedy manner, as reported
by Carlini & Wagner (2017). However, some constraints have a closed-form analytical solution, e.g., the minimization of max(0, ||x - z|| - ) can be solved by projecting into the L ball. To leverage this, we identify logical constraints which restrict the variables z to convex sets that have an efficient algorithm for projection, e.g., line segments, L2, L or L1 balls (Duchi et al., 2008). Note that in general, projection to a convex set is a difficult problem. We exclude such constraints from  and add them as constraints of the optimization. We thus rewrite (4) as:

x¯S¯,

=

min L(¬)(z¯, S¯, c¯, ),
z1 D1 (S¯),...,zk Dk (S¯)

(6)

where the Di denote functions which map random samples to a convex set. To solve (6), we employ Projected Gradient Descent (PGD) which was shown to have strong performance in the case of adversarial training with L balls (Madry et al., 2017).

Training procedure Our overall training procedure is shown in Algorithm 1. We first form a mini-batch of random samples from the training set T . Then, the oracle finds a solution for (4) using the formulation in (6). This solution is given to the optimizer, which solves (5). Note that if  has no variables z1, . . . , zk (that is, k = 0), the oracle becomes trivial and the loss is computed directly.

Algorithm 1: Training with constraints.
input : Training set T , network parameters , and a constraint (z¯, S¯, c¯, )
for epoch = 1 to nepochs do Sample mini-batch of m-tuples S¯ = S1, . . . , Sm  T . Using PGD, compute x¯  arg minz1D1(S¯),...,zkDk(S¯) L(¬)(z¯, S¯, c¯, ). Perform Adam update with L()(x¯, S¯, c¯, ).

5 QUERYING NETWORKS

We build on DL2 and design a declarative query language where neural networks are first class objects and queries can refer to their inputs, outputs and internal layers. This language enables one to separate the phrasing of the question from the optimization used to answer that question. Interestingly, the specific (hardcoded) questions on neural network understanding investigated by prior works can be naturally phrased as DL2 queries: neurons responsible for a given prediction (Olah et al., 2018), inputs that differentiate two networks (Pei et al., 2017), and adversarial examples (e.g., Szegedy et al. (2013)). Concretely, we support the following class of queries:

f i n d z1(m1), . . . , zk(mk) where (z¯) [ i n i t z1 = c1, . . . , zk = ck] [ r e t u r n t(z¯)]

Here, find defines the variables and their shape in parentheses, where defines the constraint (over the

fragment described in Sec. 3), init defines initial values for (part or all of) the variables, and return

defines a target term to compute at the end of search; if missing, z1, . . . , zk are returned. Networks

(loaded so to be used in the queries) and constants are defined outside the queries. We note that

the user can specify tensors in our language (we do not assume these are simplified to vectors). In

queries, we write comma (,) for conjunction (); in for box-constraints and class for constraining

the target label, which is interpreted into constraints over the labels' probabilities. For example, the

k

atomic constraint class (NN(x)) = y is rewritten into

NN(x).p[i] < NN(x).p[y], for NN

i=1,i=y

whose output is NN(x).p, a probability distribution over k classes.

Examples Fig. 1 shows few interesting queries. The first two are defined over networks trained for CIFAR-10, while the last is for MNIST. The goal of the first query is to find an adversarial example i of shape (32, 32, 3), classified as a truck (class 9) where the distance of i to a given deer image (deer) is between 6 and 24, with respect to the infinity norm. Fig. 1b is similar, but the goal is to find i classified as a deer where a specific neuron is deactivated. The last query's goal is to find i classified differently by two networks where part of i is fixed to pixels of the image nine.

5

Under review as a conference paper at ICLR 2019

f i n d i[32, 32, 3] f i n d i[32, 32, 3]

f i n d i[28, 28]

where i i n [0, 255], where i i n [0, 255],

where i i n [0, 1],

c l a s s (NN(i)) = 9,

i - deer  < 25,

i[0:9,:] = nine[0:9,:],

i - deer  < 25, NN(i).l1[0, 1, 1, 31] = 0, c l a s s (NN1(i)) = 8,

i - deer  > 5

c l a s s (NN(i)) = 4

c l a s s (NN2(i)) = 9

(a) Adversarial example.

(b) Neuron deactivated.

(c) Diffing networks.

Figure 1: DL2 queries enable to declaratively search for inputs satisfying constraints over networks.

Solving queries As with training, we compile the constraints to a loss, but unlike training, we optimize with L-BFGS-B. While training requires batches of inputs in PGD optimization, querying looks for one assignment, and thus there is more time to employ the more sophisticated, but slower, L-BFGS-B. We discuss further optimizations in Appendix B.

6 EXPERIMENTAL EVALUATION

We now present a thorough experimental evaluation on the effectiveness of DL2 for querying and training neural networks with logical constraints. Our system is implemented in PyTorch (Paszke et al., 2017) and evaluated on an Nvidia GTX 1080 Ti and Intel Core i7-7700K with 4.20 GHz.

6.1 TRAINING WITH DL2

We evaluated DL2 on various tasks (supervised, semi-supervised and unsupervised learning) across four datasets: MNIST, FASHION (Xiao et al., 2017), CIFAR-10, and CIFAR-100 (Krizhevsky & Hinton, 2009). In all experiments, one of the constraints was cross-entropy (see Sec. 4), to optimize for high prediction accuracy. For each experiment, we describe additional logical constraints.

Supervised

learning

We consider two types

MNIST

FASHION

CIFAR-10

of constraints for super-

Baseline DL2 Baseline DL2 Baseline DL2

vised learning: global constraints, which have z-s, and training set con-

RobustnessT RobustnessG

P C P C

99.39 96.10 99.38 35.02

98.61 97.30 99.45 92.18

90.56 95.20 91.45 00.00

89.70 96.30 89.41 80.20

90.98 91.10 90.59 07.16

87.08 93.08 78.86 21.00

straints, where the only variables are from the training set (no z-s). Note

LipschitzT LipschitzG

P 99.48 97.95 92.10 87.49 90.59 90.24 C 07.20 99.20 06.40 99.53 07.16 99.60 P 99.44 99.24 92.22 82.27 90.51 87.32 C 00.00 99.90 00.00 89.38 00.00 99.55

that none of prior work applies to global constraints in general. Furthermore, because of limitations of their encoding explained

C-similarityT C-similarityG SegmentG

P C P C P C

98.16 18.97

97.44 37.70

88.13 21.80

87.18 48.18

91.52 89.10 91.06 49.74
-

90.70 99.60 90.30 57.31
-

in Sec. 2, they are not able Figure 2: Supervised learning, P/C is prediction/constraint accuracy. to handle complex training

set constraints considered in our experiments (e.g., constraints between probability distributions). To ease notation, we write random samples (the S-s) as xi and yi for inputs from the training set (xi) and their corresponding label (yi).

For local robustness (Szegedy et al., 2013), the training set constraint says that if two inputs from the dataset are close (their distance is less than a given 1, with respect to L2 norm), then the KL divergence of their output probabilities is smaller than 2:

||x1 - x2||2 < 1 = KL(p(x1)||p(x2)) < 2

(RobustnessT)

Second, the global constraint requires that for any input x, whose classification is y, inputs in its neighborhood which are also valid images (pixels are between 0 and 1), have a high probability for y. For numerical stability, instead of the probability we check that the corresponding logit is larger than a given threshold :

z  L(x, )  [0, 1]d. logit(z)y > 

(RobustnessG)

6

Under review as a conference paper at ICLR 2019

Similarly, we have two definitions for the Lipschitz condition. The training set constraint requires that for every two inputs from the training set, the distance between their output probabilities is less than the Lipschitz constant (L) times the distance between the inputs:

||p(x1) - p(x2)||2 < L||x1 - x2||2

(LipschitzT)

The global constraint poses the same constraint for valid images in the neighborhood x1 and x2:

z1  L(x1, )  [0, 1]d, z2  L(x2, )  [0, 1]d.||p(z1) - p(z2)||2 < L||z1 - z2||2 (LipschitzG)

We also consider a training set constraint called C-similarity, which imposes domain knowledge constraints for CIFAR-10 networks. The constraint requires that inputs classified as a car have a higher probability for the label truck than the probability for dog:

y = car = p(x)truck > p(x)dog + 

(C-similarityT)

The global constraint is similar but applied for valid images in the -neighborhood of x:

z  L(x, )  [0, 1]d.y = car = p(z)truck > p(z)dog + 

(C-similarityG)

Finally, we consider a Segment constraint which requires that if an input z is on the line between two inputs x1 and x2 in position , then its output probabilities are on position  on the line between
the output probabilities:

z. z =  · x1 + (1 - ) · x2 = - · logit(z)y1 - (1 - ) · logit(z)y2 <  (SegmentG)

Fig. 2 shows the prediction accuracy (P) and the constraint accuracy (C) when training with (i) crossed-entropy only (CE) and (ii) CE and the constraint. Results indicate that DL2 enables to significantly improve constraint accuracy in all benchmarks (e.g. from 0% to 99% for LipschitzG), while prediction accuracy decreases by a small amount. The decrease may be expected in light of a recent work (Tsipras et al. (2018)), which shows that adversarial robustness comes with decrease of prediction accuracy. Since adversarial robustness is one kind of a DL2 constraint, we suspect that we observe a similar phenomenon here.

Semi-supervised learning For semi-supervised learning, Method

Accuracy (%)

we focus on the CIFAR-100 dataset, and split the training set into labeled, unlabeled and validation set in ratio of 20/60/20. In the spirit of the experiments of Xu et al. (2018), we consider the constraint which requires that the probabilities of groups of classes have either very high probability or very

Baseline Semantic loss Rule distillation DL2

47.03 51.82 45.56 53.48

low probability. A group consists of classes of a similar type (e.g., the classes baby, boy, girl, man, and woman are part of

Figure 3: Semi-supervised training.

the people group), and the group's probability is the sum of its classes' probabilities. Formally, our

constraint consists of 20 groups and its structure is:

(ppeople <  ppeople > 1 - )  ...  (pinsects <  pinsects > 1 - )

for a small . We use this constraint to compare the performance of several approaches. For all approaches, we use the Wide Residual Network (Zagoruyko & Komodakis (2016)) as the network architecture. As a baseline, we train in a purely-supervised fashion, without using the unlabeled data. We also compare to semantic loss (Xu et al., 2018) and rule distillation (Hu et al., 2016). Note that this constraint is restricting the probability distribution and not samples drawn from it which makes other methods inapplicable (as shown in Sec. 2). As these methods cannot encode our constraint, we replace them with a closest approximation (e.g., the exactly-one constraint from Xu et al. (2018) for semantic loss). Details are shown in Appendix C. Fig. 3 shows the prediction accuracy on the test set for all approaches. Results indicate that our approach outperforms all existing works.

Unsupervised learning Lastly, we consider a regression task in an unsupervised setting. We consider the problem of training MLP (Multilayer perceptron) to predict the minimum distance from a source to every node in an unweighted graph, G = (V, E). One can notice that minimum distance is a function with certain properties (e.g., triangle inequality) which form a logical constraint listed below. Source is denoted as 0.

Approach
Supervised (regression) Unsupervised (baseline) Unsupervised (with DL2)

MSE
0.0516 0.4938 0.0998

Figure 4: Unsupervised training.

v  G, d(v)  0  ((v,v )E(d(v) = d(v ) + 1))  ((v,v )E(d(v) d(v ) + 1))

7

Under review as a conference paper at ICLR 2019

MNIST

© ©Nr. #



FASHION
# © ©

CIFAR-10
# © ©

GTSRB

ImageNet

# © © # © ©

1 10 0.4 0.4 10 0.4 0.4 10 0.4 0.4 10 0.6 0.6 10 1.6 1.6

2 10 1.0 1.0 10 1.00 1.00 10 1.2 1.2 10 3.0 3.0 10 80.7 80.7

3 10 1.0 1.0 10 0.9 0.9 10 3.4 3.4 10 3.1 3.1 10 81.0 81.0

4 10 1.5 1.5 10 1.6 1.6 9 9.8 1.9 0 120.0 0.0 10 94.4 94.4

5 10 1.0 1.0 10 1.0 1.0 8 16.5 1.1 10 3.2 3.2 10 80.2 80.2

6 9 15.7 4.1 8 25.5 1.9 9 5.7 1.7 8 27.4 4.2 9 81.4 77.2

7 10 1.0 1.0 10 1.0 1.0 10 1.1 1.1 10 3.0 3.0 10 74.0 74.0

8 6 48.6 1.0 6 51.7 6.2 9 5.9 1.0 9 15.8 4.2 10 78.4 78.4

9 10 1.4 1.4 10 1.5 1.5 8 10.5 1.7 0 120.0 0.0 10 86.6 86.6

10 10 1.8 1.8 10 1.7 1.7 10 2.6 2.6 0 120.0 0.0 10 92.2 92.2

11 6 50.0

3.3 7 42.0

8.7 7 35.0 17.1 0 120.0 0.0 8 96.6 90.7

12 10 2.0 2.0 10 2.0 2.0 9 28.8 18.7 10 6.2 6.2 0 120.0 0.0

13 5 63.5

7.1 7 39.3

4.7 7 30.2

7.6 9 21.9 11.0 0 120.0 0.0

14 0 120.0

0.0 0 120.0

0.0 7 68.21 46.01

-

- --

--

15 3 79.08 71.27 9 35.02 25.58

7 66.02 42.88

-

- --

--

16 1 108.2

2.0 1 108.2

2.0 8 34.4 13.1 4 75.6 9.0 0 120.0 0.0

17 10 2.9 2.9 10 3.2 3.2 5 61.6 4.0 0 120.0 0.0 0 120.0 0.0

18 10 4.0 4.0 10 4.0 4.0 7 50.6 24.9 0 120.0 0.0 0 120.0 0.0

©Table 1: Results for queries: (#) number of completed instances (out of 10), is the average ©running time in seconds, and the average running time of successful runs (in seconds).

Additionally, we constrain d(0) = 0. Next, we train the model in an unsupervised fashion with the DL2 loss. In each experiment, we generate random graphs with 15 vertices and split the graphs into training (300), validation (150) and test set (150). As an unsupervised baseline, we consider a model which always predicts d(v) = 1. We also train a supervised model with the mean squared error (MSE) loss. Remarkably, our approach was able to obtain an error very close to supervised model, without using any labels at all. This confirms that loss generated by DL2 can be used to guide the network to satisfy even very complex constraints with many nested conjunctions and disjunctions.
6.2 QUERYING WITH DL2
We evaluated DL2 for the task of querying with constraints, which was implemented in TensorFlow. We considered five image datasets, and for each, we considered at least two classifiers, and for some we also considered a generator and a discriminator (trained using GAN (Goodfellow et al., 2014a)). Table 3 (Appendix D) provides statistics on the networks. Our benchmark consists of 18 template queries (Appendix D), which are instantiated with the different networks, classes, and images.
Table 1 shows the results (- denotes an inapplicable query). Queries ran with a timeout of 2 minutes. Results indicate that our system often finds solutions. It is unknown whether queries for which it did not find a solution even have a solution. We observe that the success of a query depends on the dataset. For example, queries 9-11 are successful for all datasets but GTSBR. This may be attributed to the robustness of GTSBR networks against the adversarial examples that these queries aim to find. Query 14, which leverages a discriminator to find adversarial examples, is only successful for the CIFAR dataset. A possible explanation can be that discriminators were trained against real images or images created by a generator, and thus the discriminator performs poorly in classifying arbitrary images. Query 15, which leverages the generators, succeeds in all tested datasets, but has only few successes in each. As for overall solving time, our results indicate that, in general, successful executions terminate relatively quickly. Results also indicate that our system scales well to large networks (e.g., for ImageNet).
7 CONCLUSION
We presented DL2, a system for training and querying neural networks. DL2 supports an expressive logical fragment and provides translation rules into a differentiable (almost everywhere) loss, which is zero only for inputs satisfying the constraints. To make training tractable, we handle input constraints which capture convex sets through PGD. We also introduce a declarative language for querying networks which uses the logic and the translated loss. Experimental results indicate that DL2 is effective in both, training and querying neural networks.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Radu Balan, Maneesh Singh, and Dongmian Zou. Lipschitz properties for deep convolutional networks. CoRR, abs/1701.05217, 2017.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39­57. IEEE, 2017.
William W. Cohen, Fan Yang, and Kathryn Mazaitis. Tensorlog: Deep learning meets probabilistic dbs. CoRR, abs/1707.05390, 2017.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning, pp. 272­279. ACM, 2008.
Zhoulai Fu and Zhendong Su. Xsat: a fast floating-point satisfiability solver. In International Conference on Computer Aided Verification, pp. 187­209. Springer, 2016.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems (NIPS), pp. 2672­2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014b.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J. Cree. Regularisation of neural networks by enforcing lipschitz continuity. CoRR, abs/1804.04368, 2018.
Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with logic rules. arXiv preprint arXiv:1603.06318, 2016.
Angelika Kimmig, Stephen Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. A short introduction to probabilistic soft logic. In Proceedings of the NIPS Workshop on Probabilistic Programming: Foundations and Applications, pp. 1­4, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. CoRR, abs/1706.06083, 2017.
Pablo Ma´rquez-Neila, Mathieu Salzmann, and Pascal Fua. Imposing hard constraints on deep networks: Promises and limitations. arXiv preprint arXiv:1706.02025, 2017.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. The building blocks of interpretability. Distill, 2018. doi: 10.23915/ distill.00010. https://distill.pub/2018/building-blocks.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural networks for weakly supervised segmentation. In Proceedings of the IEEE international conference on computer vision, pp. 1796­1804, 2015.
Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore: Automated whitebox testing of deep learning systems. In Proceedings of the 26th Symposium on Operating Systems Principles, Shanghai, China, October 28-31, 2017, pp. 1­18, 2017. doi: 10.1145/3132747.3132785. URL http://doi.acm.org/10.1145/3132747.3132785.
9

Under review as a conference paper at ICLR 2019
Kevin Schawinski, Ce Zhang, Hantian Zhang, Lucas Fowler, and Gokula Krishnan Santhanam. Galaxygan: Generative adversarial networks for recovery of galaxy features. Astrophysics Source Code Library, 2017.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Generative adversarial examples. CoRR, abs/1805.07894, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. There is no free lunch in adversarial robustness (but there are unexpected benefits). arXiv preprint arXiv:1805.12152, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss function for deep learning with symbolic knowledge. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5502­5511, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
10

Under review as a conference paper at ICLR 2019

MNIST, FASHION

CIFAR-10

 PGD Iterations

Params

 PGD Iterations

Params

RobustnessT RobustnessG LipschitzT LipschitzG ClassesT ClassesG SegmentG

0.2 0.2 0.1 0.2 0.01

50 50 5

1 = 7.8, 2 = 2.9 1 = 0.3,  = 0.52
L = 0.1 L = 0.1
= 100

0.04 0.1 0.1 0.1 0.2 0.2 -

7 5 10 -

1 = 13.8, 2 = 0.9 1 = 0.03,  = 0.52
L = 1.0 L = 1.0  = 0.01  = 0.01
-

Table 2: Hyperparameters used for supervised learning experiment

A COMPARISON OF DL2 WITH PRIOR APPROACHES

XSAT (Fu & Su, 2016) also translates logical constraints into numerical loss, but its atomic con-

straints are translated into non-differentiable loss, making the whole loss non-differentiable. Proba-

bilistic soft logic (e.g., Cohen et al. (2017); Hu et al. (2016)) translates logical constraints into differ-

entiable loss, which ranges between [0, 1]. However, using their loss to find satisfying assignments

with gradient methods can be futile, as the gradient may be zero. To illustrate, consider the toy exam-

ple

of

(z)

:=

(z

=

(

1 1

)).

PSL

translates

this

formula

into

the

loss

LPSL()

=

max{z0

+

z1

- 1,

0}

(it

assumes

z0, z1



[0, 1]).

Assuming

optimization

starts

from

x

=

(

0.2 0.2

)

(or

any

pair

of

numbers

such

that

z0

+

z1

-

1



0),

the

gradient

is

z LPSL ()(x)

=

(

0 0

),

which

means

that

the

optimization

cannot continue from this point, even though x is not a satisfying assignment to . In contrast, with

our translation, we obtain L()(z) = |z0 - 1| + |z1 - 1|, for which the gradient for the same x is

zL()(x) =

-1 -1

.

B OPTIMIZATION FOR QUERYING NETWORKS

Here we discuss how the loss compilation can be optimized for L-BFGS-B. While our translation is defined for arbitrary large constraints, in general, it is hard to optimize for a loss with many terms. Thus, we mitigate the size of the loss by extracting box constraints out of the expression. The loss is then compiled from remaining constraints. Extracted box constraints are passed to the L-BFGS-B solver which is then used to find the minimum of the loss. This "shifting" enables us to exclude a dominant part of  from the loss, thereby making our loss amenable to optimization. To illustrate the benefit, consider the query in Fig. 1a. Its box constraint, i in [0,255], is a syntactic sugar to a conjunction with 2 · 32 · 32 · 3 = 6, 144 atomic constraints (two for each variables, i.e., for every index j, we have ij  0 and ij  255). In contrast, the second constraint consists of 9 atomic constraints (one for each possible class different from 9, as we shortly explain), and the third and fourth constraints are already atomic. If we consider 6, 155 atomic constraints in the loss, finding a solution (with gradient descent) would be slow. For larger inputs (e.g., inputs for ImageNet, whose size is 224 · 224 · 3 > 150, 000), it may not terminate in a reasonable time. By excluding the box constraints from the loss, the obtained loss consists of only 11 terms, making it amenable for gradient optimization. We note that while a solution is not found (and given enough timeout), we restart L-BFGS-B and initialize the variables using MCMC sampling.

C EXPERIMENTS DETAILS
Here we describe implementation details (including hyperaparameters) used during our experiments.
Supervised learning For our experiments with supervised learning we used batch size 128, Adam optimizer with learning rate 0.0001. All other parameters are listed in 2. Additionally, for CIFAR10 experiments we use data augmentation with random cropping and random horizontal flipping. Experiments with Segment constraints are done by first embedding images in 40-dimensional space using PCA. In lower dimensional space it is sensible to consider linear interpolation between images which is not the case otherwise. Note that this experiment is not performed for CIFAR-10 because we do not observe good prediction accuracy with baseline model using lower dimensional embed-

11

Under review as a conference paper at ICLR 2019

dings. This is likely because dimensionality of CIFAR-10 images is much higher than MNIST or FASHION.

Semi-supervised learning All methods use the same Wide Residual Network model. We use depth 28 and widening factor 10. Neural network is optimized using Adam with learning rate 0.001. We use  = 0.6 as weighting factor for DL2 loss.

For semantic loss experiment we follow the encoding from Xu et al. (2018). Please consult the original work to see how exactly-one constraint is encoded into semantic loss. Since rule distillation does not support our constraint, we use the following approximation (following notation from Hu et al. (2016)):

rl(X, Y ) =

(Y )

(7)

Y G(Y )

We denote G(Y ) as set of labels sharing the same group as Y . Note that rule is meant to encourage putting more probability mass into the groups which already have high probability mass. This should result in the entire probability mass collapsed in one group in the end, as we want. We use t = max(0, 1.0 - 0.97t) as mixing factor. Other constants used are C = 1 and  = 1.
Unsupervised learning Our model is 4-layered MLP with hidden layers of size 1000. We also use dropout 0.3. Network is optimized using Adam with learning rate 0.0001.

D ADDITIONAL DETAILS FOR SECTION 6.2
Here we provide statistics on the networks used in the experiments of Sec. 6.2, as well as the query templates.
Dataset and networks Our benchmark consists of five image datasets, each with different wellestablished neural networks' architectures. For each dataset, we consider at least two classifiers, and for some we also consider a generator and a discriminator (trained using GAN Goodfellow et al. (2014a)). We trained most networks ourselves, except for the C_VGG and the ImageNet classifiers, for which the weights were available to download. Table 3 summarizes the networks that we used, their architecture, and accuracy. Each row shows the dataset, the type of the network (classifier, generator, or discriminator), the network signature, and the architecture of the network. For example, the first row describes a classifier that takes as input images of size 28 × 28 pixels, each ranging between 0­1, and returns a probability distribution over ten classes.

12

Under review as a conference paper at ICLR 2019

Query
1 e v a l N(var)
2 f i n d i[shape] where c(N(i))=c
3 f i n d i[shape] where c(N(i))=c, pix con
4 f i n d i[shape] where c(N(i))=c, N(i).p[c] > 0.8, pix con
5 f i n d i[shape] where c(N(i))=c i n i t i=var
6 f i n d i[shape] where c(N(i))=c, pix con, i - var  < dist i n i t i=var
7 f i n d i[shape] where c(N(i))=c, pix con i n i t i=var
8 f i n d i[shape] where c(N(i))=c, i[mask] i n range, i[nm]=var[nm] i n i t i=var
9 f i n d i[shape] where c(N(i))=c, pix con, N(i).p[c] > 0.8 i n i t i=var
10 f i n d i[shape] where c(N(i))=c, pix con, N(i).p[c] > 0.8, N(i).p[cv] < 0.1 i n i t i=var
11 f i n d i[shape] where c(N(i))=c, pix con, N(i).p[c] > 0.8, N(i).p[cv] < 0.1, i-var  < dist i n i t i=var

Query
12 f i n d i[shape] where pix con, c (N2 (i))=c2 , c (N1 (i))=c1
13 f i n d i[shape] where pix con, c (N2 (i))=c, i - var  < dist, c (N1 (i))=cv , i n i t i=var
14 f i n d i[shape] where c(N1(i))=c1, c (N2 (i))=c2 , N1(i).p[c1] > 0.5, N2(i).p[c1] < 0.1, N2(i).p[c2] > 0.5, N1(i).p[c2] < 0.1, pix con,D(i) < 0.1
15 f i n d i[100] where i i n [-1,1], c (N1 (G(i)))=c1 , N1(G(i)).p[c1]> 0.3, c (N2 (G(i)))=c2 , N2(G(i)).p[c2]> 0.3
16 f i n d i[shape] where c(N1(i))=cv, c (N2 (i))=c, i[mask] i n range, i[nm]=var[nm] i n i t i=var
17 f i n d i[shape] where c(N1(i))=c1, c (N2 (i))=c2 , N1(i).p[c1] > 0.5, N1(i).p[c2] < 0.1, pix con
18 f i n d i[shape] where c(N1(i))=c1, c (N2 (i))=c2 , N1(i).p[c1] > 0.6, N1(i).p[c2] < 0.1, N2(i).p[c2] > 0.6, N2(i).p[c1] < 0.1, pix con

Figure 5: The template queries used to evaluate DL2.

13

Under review as a conference paper at ICLR 2019

Dataset MNIST
Fashion MNIST
CIFAR
GTSRB ImageNet

Type
C C G D G D
C C G D
C C C G D
C C
C C C

Network
M NN1: [0, 1]28×28  [0, 1]10 M NN2: [0, 1]28×28  [0, 1]10 M G: [-1, 1]100  [0, 1]28×28 M D: [0, 1]28×28  [0, 1] M ACGAN G: [-1, 1]100 × {0, . . . , 9}  [0, 1]28×28 M ACGAN D: [0, 1]28×28  [0, 1] × [0, 1]10
FM NN1 : [0, 1]28×28  [0, 1]10 FM NN2 : [0, 1]28×28  [0, 1]10 FM G: [-1, 1]100  [0, 1]28×28 FM D : [0, 1]28×28  [0, 1]
C NN1 : [0, 255]32×32×3  [0, 1]10 C NN2 : [0, 255]32×32×3  [0, 1]10 C VGG : [0, 255]32×32×3  [0, 1]10 C G : [-1, 1]100  [0, 255]32×32×3 C D : [0, 255]32×32×3  [0, 1]
G LeNet : [0, 1]32×32  [0, 1]43 G VGG : [0, 1]32×32  [0, 1]43
I V16 : [0, 255]224×224×3  [0, 1]1000 I V19 : [0, 255]224×224×3  [0, 1]1000 I R50 : [0, 255]224×224×3  [0, 1]1000

Architecture
Tensorflow Tutorial M NN1 with an additional layer DC-GAN DC-GAN AC-GAN AC-GAN
Tensorflow Tutorial FM NN1 with an additional layer DC-GAN DC-GAN
4-layer-model 6-layer-model VGG-16-based DC-GAN DC-GAN
based on LeNet based on VGG
VGG-16 from Keras VGG-19 from Keras ResNet-50 from Keras

Accuracy
0.992 0.990
-
0.917 0.910
-
0.712# 0.756# 0.935#
-
0.914 0.973
0.715 0.727 0.759

Table 3: The datasets and networks used to evaluate DL2. The reported accuracy is top-1 accuracy and it was either computed by the authors (), users that implemented the work (#), or by us (). Note that for GTSRB the images have dimensions 32×32×3, but the Cs take inputs of 32×32(×1),
which are pre-processed grayscale versions.

14

