Under review as a conference paper at ICLR 2019
KNOWLEDGE DISTILLATION FROM FEW SAMPLES
Anonymous authors Paper under double-blind review
ABSTRACT
Current knowledge distillation methods require full training data to distill knowledge from a large "teacher" network to a compact "student" network by matching certain statistics between "teacher" and "student" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both "teacher" and "student" have the same feature map sizes at each corresponding block, we add a 1 × 1 conv-layer at the end of each block in the student network, and align the block-level outputs between "teacher" and "student" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed into the previous conv-layer so that no extra parameters and computation are introduced. Experiments show that the proposed method can recover a student network's top-1 accuracy on ImageNet from 0.2% to 62.7% with just 1000 samples in a few minutes, and is effective on various ways for constructing student networks.
1 INTRODUCTION
Deep neural networks (DNNs) have demonstrated extraordinary success in a variety of fields such as computer vision (Krizhevsky & Hinton, 2012; He et al., 2016), speech recognition (Hinton et al., 2012), and natural language processing (Mikolov et al., 2010). However, DNNs are resource-hungry which hinders their wide deployment to some resource-limited scenarios, especially low-power embedded devices in the emerging Internet-of-Things (IoT) domain. To address this limitation, extensive works have been done to accelerate or compress deep neural networks. Putting those works on designing (Chollet, 2016) or automatically searching efficient network architecture aside (Zoph & Le, 2016), most studies try to optimize DNNs from four perspectives: network pruning (Han et al., 2016; Li et al., 2016), network decomposition (Denton et al., 2014; Jaderberg et al., 2014), network quantization (or low-precision networks) (Gupta et al., 2015; Courbariaux et al., 2016; Rastegari et al., 2016) and knowledge distillation (Hinton et al., 2015; Romero et al., 2015).
Among these method categories, knowledge distillation is somewhat different since it allows designable output network ("student" net). The concept was proposed by (Bucila et al., 2006; Ba & Caruana, 2014; Hinton et al., 2015) for transferring knowledge from a large "teacher" model to a compact yet efficient "student" model by matching certain statistics between "teacher" and "student". Further research introduced various kinds of matching mechanisms in the field of DNN optimization. The distillation procedure designs a loss function based on the matching mechanisms and enforces the loss during a full training process. Hence, all these methods usually require time-consuming training procedure along with fully annotated large-scale training dataset.
Meanwhile, some network pruning (Li et al., 2016; Liu et al., 2017) and decomposition (Zhang et al., 2016; Kim et al., 2016) methods can produce extremely small networks, but with large accuracy drops so that time-consuming fine-tuning is required for possible accuracy recovery. Usually, it may still not be able to recover the accuracy drops with the original cross-entropy loss due to its low representation capacity. Hence, knowledge distillation may be used to alleviate the problem, since the small student's performance can sometimes be trained to match the teacher's. For instance, Crowley et al. (2017) uses cheap group convolutions and pointwise convolutions to build a small student network and adopts knowledge distillation to transfer knowledge from a full-sized "teacher" network to the "student" network. However, it still suffers from high training cost.
1

Under review as a conference paper at ICLR 2019

(1) Design Teacher
...
Student ...

(2) Align Teacher
...
Student ...

Least square error

(3) Absorb Teacher
...
Student ...

1x1 conv
Figure 1: Three steps of our few-sample knowledge distillation. (1) design student network from scratch or by compressing teacher-net; (2) add 1×1 conv-layer at the end of each block of student-net (before ReLU), and align teacher and student by estimating the parameter using least-squared regression; (3) absorb the added 1×1 conv-layer into previous conv-layer to obtain final student-net.
As is known, children can learn knowledge concept from adults with few examples. This cognition phenomenon has motivated the development of the few-shot learning (Fei-Fei et al., 2006; Bart & Ullman, 2005), which aims to learn information about object categories from a few training samples, and focuses more on image classification task. Nevertheless, it inspires people to consider the possibility of knowledge distillation from few samples. Some recent works on knowledge distillation address this problem by constructing "pseudo" training data (Kimura et al., 2018; Lopes et al., 2017) with complicated heuristics and heavy engineering, which are still costly.
This paper proposes a novel and simple 3-step method for few-sample knowledge distillation (FSKD) as illustrated in Figure 1, including student-net design, teacher-student alignment, and absorbing added conv-layer. We assume that both "teacher" and "student" nets have the same feature map sizes at each corresponding block. However, the relatively small student-net can be obtained in various ways, such as pruning/decomposing the teacher-net, and fully redesigned network with random initialization. We add a 1×1 conv-layer at the end of each block of the student-net and align the block-level outputs between "teacher" and "student", which is done by estimating the parameters of the added layer with few samples using least square regression. Since the added 1×1 conv-layers have relatively few parameters, we can get a good approximation from a small number of samples. We further prove that the added 1×1 conv-layer can be absorbed by the previous conv-layer when certain conditions fulfill, without introducing extra parameters and computation cost to the student-net. Our major contributions can be summarized as follows:
(1) To the best of our knowledge, we are the first to show that knowledge distillation can be done with few samples within minutes on desktop PC.
(2) The proposed few-sample knowledge distillation (FSKD) method is widely applicable not only for fully redesigned student networks but also for compressed networks from pruning and decomposition-based methods.
(3) We demonstrate significant performance improvement of the student network by FSKD. For example, FSKD can recover a student network's top-1 accuracy on ImageNet from only 0.2% to 62.7% with just 1000 samples.

2 RELATED WORK
Knowledge Distillation (KD) transfers knowledge from a pre-trained large "teacher" network (or even an ensemble of networks) to a small "student" network, for facilitating the deployment at test time. Originally, this is done by regressing the softmax output of the teacher model (Hinton et al., 2015). The soft continuous regression loss used here provides richer information than the label based loss, so that the distilled model can be more accurate than training on labeled data with cross-entropy loss. Later, various works have extended this approach by matching other statistics, including intermediate feature responses (Romero et al., 2015; Chen et al., 2016), gradient (Srinivas & Fleuret, 2018), distribution (Huang & Wang, 2017), Gram matrix (Yim et al., 2017), etc. These methods require a large amount of data (known as the "transfer set") to transfer the knowledge, whereas we aim to provide a solution in the case that we only have limited number of samples. We need emphasize that our FSKD has a totally different philosophy on aligning intermediate responses to the very close knowledge distillation method FitNet (Romero et al., 2015). FitNet re-trains the whole student-net with intermediate supervision using a larger amount of data, while our FSKD only
2

Under review as a conference paper at ICLR 2019
estimates parameters for the added 1 × 1 conv-layer with few samples. We will verify in experiments that our FSKD is not only more efficient but also more accurate than FitNet.
Network Pruning methods obtain a small network by pruning weights from a trained larger network, which can keep the accuracy of the larger model if the prune ratio is set properly. Han et al. (2015) proposes to prune the individual weights that are near zero. Recently, channel pruning has become increasingly popular thanks to its better compatibility with off-the-shelf computing libraries, compared with weights pruning. Different criteria have been proposed to select the channel to be pruned, including norm of weights (Li et al., 2016), scales of multiplicative coefficients (Liu et al., 2017), statistics of next layer (Luo et al., 2017), etc. Meanwhile, Network Decomposition methods try to factorize heavy layers in DNNs into multiple lightweight ones. For instance, it may adopt low-rank decomposition to fully-connection layers (Denton et al., 2014), and different kinds of tensor decomposition to conv-layers (Zhang et al., 2016; Kim et al., 2016). However, aggressive network pruning or network decomposition usually lead to large accuracy drops, thus fine-tuning is required to alleviate those drops (Li et al., 2016; Liu et al., 2017; Zhang et al., 2016). As aforementioned, KD is more accurate than directly training on labeled data, it is of great interest to explore KD on extremely pruned or decomposed networks, especially under the few-sample setting.
Learning with few samples has been extensively studied under the concept of one-shot or fewshot learning. One category of methods directly model few-shot samples with generative models (Fei-Fei et al., 2006; Lake et al., 2011), while most other methods study the problem under the notion of transfer learning (Bart & Ullman, 2005; Ravi & Larochelle, 2017). In the latter category, meta-learning methods (Vinyals et al., 2016; Finn et al., 2017) solve the problem in a learning to learn fashion, which has been recently gaining momentum due to their application versatility. Most studies are devoted to the image classification task, while it is a less-explored problem for knowledge distillation from few samples. Recently, some works tried to address this problem. Kimura et al. (2018) constructs pseudo-examples using the inducing point method, and develops a complicated algorithm to optimize the model and pseudo-examples alternatively. Lopes et al. (2017) records per-layer meta-data for the teacher network in order to reconstruct a training set, and then adopts a standard training procedure to obtain the student network. Both are still very costly due to the complicated and heavy training procedure. On the contrary, we aim for a simple solution for knowledge distillation from few samples.
3 FEW-SAMPLE KNOWLEDGE DISTILLATION (FSKD)
3.1 OVERVIEW
Our few-sample knowledge distillation (FSKD) method consists of three steps as shown in Figure 1. First, we design a student network either by pruning/decomposing the teacher network, or by fully redesigning a small student network with random initialization. Second, we add a 1×1 conv-layer at the end of each block of the student network and align the block-level outputs between "teacher" and "student" by estimating the parameters for the added layer from few samples. Third, we absorb the added 1×1 conv-layer into the previous conv-layer, so that no extra parameters and computations are introduced into the student network.
Two reasons make this idea work efficiently. First, the 1×1 conv-layers have relatively few parameters, which does not require too much data for the estimation. Second, the block-level output from teacher network provides rich information as shown in FitNet (Romero et al., 2015). Below, we will first provide the theoretical derivation why the added 1×1 conv-layer could be absorbed into the previous conv-layer. Then we provide details on how we do the block-level output alignment.
3.2 ABSORBABLE 1 × 1 CONV-LAYER
Let's first give some mathematic notions for different kinds of convolutions before moving to the theoretical derivation. A regular convolution consists of multi-channel and multi-kernel filters which build both cross-channel correlations and spatial correlations. Formally, a regular convolution layer can be represented by a 4-dimensional tensor W  Rno×ni×kw×kh , where no and ni are the number of output and input channels respectively, and kh and kw are the spatial height and width of the kernel respectively. Usually, squared spatial kernel is used so that kw = kh = k. The point-wise (PW) convolution, also known as 1 × 1 convolution (Lin et al., 2014) can be represented by a tensor
3

Under review as a conference paper at ICLR 2019

P  Rno×ni×1×1, which is actually degraded from a 4-dimensional tensor to a 2-dimensional matrix. The depth-wise (DW) convolution (Chollet, 2016) does per-channel 2D convolution for each input channel, so that it can be represented by a tensor D  R1×ni×kh×kw . Due to no-correlation among output channels, it usually follows by a point-wise convolution to model their correlations. This combination (DW + PW) is also named as depth-wise separable convolution by Chollet (2016). Theorem 1. A pointwise convolution with tensor Q  Rno×ni×1×1 can be absorbed into the previous convolution layer with tensor W if the following conditions are satisfied.
c1. W corresponds to a regular convolution, i.e., W  Rno×ni×kw×kh , or a point-wise convolution, i.e., W  Rno×ni×1×1.
c2. The output channel number of W equals to the input channel number of Q, i.e., no = ni. c3. No non-linear activation layer like ReLU (Nair & Hinton, 2010) between W and Q.
Due to the space limitation, we put the proof in the appendix.
The absorbed convolution can be represented by a tensor W  Rno×ni×kw×kh . It is easy to have the following corollary when no extra parameters and computations are introduced after absorbing. Corollary 1. When the following condition is satisfied for Q,
c4. the number of input and output channels of Q equals to the number of output channel of W, i.e., ni = no = no, Q  Rno×no×1×1,
the absorbed convolution W has the same parameters and computation cost as W, i.e. both W , W  Rno×ni×kw×kh .

3.3 BLOCK-LEVEL ALIGNMENT AND ABSORBING

Now we consider the knowledge distillation problem. Suppose xs and xt are the block-level output vectors for the student network and teacher network respectively, we add a 1 × 1 conv-layer Q at the end of each block of student network before non-linear activation, which satisfies condition c1  c4.

As Q is degraded to the matrix form, it can be estimated with least squared regression as

Q = arg min
Q

N i=1

Q  xsi - xit

,

(1)

where N is the number of samples used. The number of parameters of Q is no × no, where no is the number of output channels in the block, which is usually not too large so that we can estimate Q with

a limited number of samples.

Suppose there are M corresponding blocks in the teacher and student networks, to achieve our goal, we need minimize the following loss function

MN

L(Qj) =

Qj  xsij - xtij ,

j=1 i=1

(2)

where Qj represents the tensor for the added 1 × 1 conv-layer of the j-th block. In practice, we do not optimize this loss all together using SGD due to that too much hyper-parameters need tuning in SGD. Instead, we minimize each of the M term for Qj in the student network block-by-block sequentially. Once we estimate Q1 of the first block by Eq.(1), we immediately replace the older block with the newer merged/absorbed block, and continue this procedure until we reach the last
block in the student-net. FSKD has the following advantages:

(1) Parameter Q can be solved with a small number of samples by aligning the block-level responses between teacher and student networks.
(2) The alignment procedure is very efficient, which can be usually done within several minutes for the entire network.
(3) The alignment procedure itself does not require class labels of input data due to its regression nature. However, if we fully redesign the student network from scratch with random weights, we may leverage SGD on a few labeled samples to initialize the network. Our FSKD can produce significant performance gains over SGD for this kind of student network design.
(4) Our FSKD works extremely well for student network obtained by aggressively pruning/decomposing the teacher network. It beats the standard fine-tuning based solution on the number of data required, processing speed, and accuracy of the output network.

4

Under review as a conference paper at ICLR 2019

VGG-16 Scheme-A + FSKD Scheme-A + FitNet Scheme-A + FSKD Scheme-A + FitNet Scheme-A + Fine-tuning Scheme-A + Full fine-tuning Scheme-B + FSKD Scheme-B + FitNet Scheme-B + FSKD Scheme-B + FitNet Scheme-B + Fine-tuning Scheme-B + Full fine-tuning

Top-1 Before (%) 92.66 85.42 85.42 85.42 85.42 85.42 85.42 47.90 47.90 47.90 47.90 47.90 47.90

Top-1 After (%) -
92.37 91.23 92.46 92.13 90.25 92.54 90.17 88.76 91.21 90.68 83.36 91.53

FLOPs 3.13 × 108 2.06 × 108 2.06 × 108 2.06 × 108 2.06 × 108 2.06 × 108 2.06 × 108
1.33 × 108 1.33 × 108 1.33 × 108 1.33 × 108 1.33 × 108 1.33 × 108

FLOPs reduced -
34% 34% 34% 34% 34% 34% 58% 58% 58% 58% 58% 58%

#Samples -
100 100 500 500 500 50000 100 100 500 500 500 50000

Table 1: Performance comparison between FitNet, fine-tuning, FSKD with student-nets from filter pruning of VGG-16 with scheme-A/B on CIFAR-10. "Full fine-tuning" means fine-tuning with full training data.

Top-1 accuracy (%) Top-1 accuracy (%) Top-1 accuracy (%)

Student-net accuracy vs #samples (scheme-A)
93

92

91

90 89
0

FSKD FitNet Fine-tuning with full data
100 200 300 400 500 #samples
(a)

Student-net accuracy vs #samples (scheme-A)
92

90

88

86 84 82
0

FSKD FitNet Fine-tuning with full data
100 200 300 400 500 #samples
(b)

Correlation coefficients before and after FSKD
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2 After
0.1 Before 0.0
c1 2 c2 1 c2 2 c3 1 c3 2 c3 3 c4 1 c4 2 c4 3 c5 1 c5 2 c5 3 Layer

Figure 2: Accuracy vs #samples on CIFAR-10. Student-net by filter Figure 3: Layer-level output correla-

pruning of VGG-16 with (a) scheme-A (b) scheme-B.

tion between "teacher" and "student"

before and after FSKD on student-

nets (scheme-A) by filter pruning.

4 EXPERIMENT

We perform extensive experiments on different image classification datasets to verify the effectiveness of FSKD on various student network construction methods. Student networks can be obtained either from compressing the teacher network or redesigning network structure with random initialization (termed "zero student network"). For the former case, we evaluate FSKD on three previous compression methods, filter pruning (Li et al., 2016), network slimming (Liu et al., 2017), and network decoupling (Guo et al., 2018). We implement the code with PyTorch, and conduct experiments on a desktop with Intel i7-7700K CPU and NVidia 1080TI GPU.

4.1 STUDENT NETWORK FROM COMPRESSING TEACHER NETWORK
FILTER PRUNING
We first obtain the student networks using the filter pruning method proposed by Li et al. (2016), which prunes out convolutional filters according to the L1 norm of their weights. The L1 norm of filter weights are sorted and the smallest portion of filters will be pruned to reduce the number of filter-channels in a convolutional layer.
We make a full study of VGG-16 (Simonyan & Zisserman, 2015) on CIFAR-10 dataset to evaluate the performance of FSKD. Following Li et al. (2016), we first prune half of the filters in conv1_1, conv4_1, conv4_2, conv4_3, conv5_1, conv5_2, conv5_3 while keeps the other layer unchanged (scheme-A). We also propose another more aggressive pruning scheme named scheme-B, which pruned 10% more filters in the aforementioned layers (60% in total), and also pruned 20% filters for the remaining layers. Scheme-A reduces 34% of total FLOPs with 7% of accuracy drop. Scheme-B reduces 58% of FLOPs with almost 50% of accuracy drop. We use those two pruned networks as student networks in this study.

5

Under review as a conference paper at ICLR 2019

VGG-19 VGG-19 (70% Pruned) + FSKD VGG-19 (70% Pruned) + FitNet VGG-19 (70% Pruned) + Fine-tuning ResNet-164 ResNet-164 (60% Pruned) + FSKD ResNet-164 (60% Pruned) + FitNet ResNet-164 (60% Pruned) + Fine-tuning DenseNet-40 DenseNet-40 (60% Pruned) + FSKD DenseNet-40 (60% Pruned) + FitNet DenseNet-40 (60% Pruned) + Fine-tuning

Top-1 Before (%) 93.38 15.90 15.90 15.90 95.07 54.46 54.46 54.46 94.18 88.24 88.24 88.24

Top-1 After (%) -
93.41 90.47 62.86
94.19 88.94 60.94
93.62 91.37 88.98

FLOPs 7.97 × 108 3.91 × 108 3.91 × 108 3.91 × 108
4.99 × 108 2.75 × 108 2.75 × 108 2.75 × 108
5.33 × 108 2.89 × 108 2.89 × 108 2.89 × 108

FLOPs reduced -
51% 51% 51%
45% 45% 45%
46% 46% 46%

-

Table 2: Performance comparison between FSKD, FitNet and fine-tuning on different network structures obtained

by network slimming with 100 samples randomly selected from CIFAR-10 training set.

VGG-19 VGG-19 (50% Pruned) + FSKD VGG-19 (50% Pruned) + FitNet VGG-19 (50% Pruned) + Fine-tuning ResNet-164 ResNet-164 (40% Pruned) + FSKD ResNet-164 (40% Pruned) + FitNet ResNet-164 (40% Pruned) + Fine-tuning DenseNet-40 DenseNet-40 (40% Pruned) + FSKD DenseNet-40 (40% Pruned) + FitNet DenseNet-40 (40% Pruned) + Fine-tuning

Top-1 Before (%) 72.08 9.24 9.24 9.24 76.56 46.07 46.07 46.07 73.21 60.62 60.62 60.62

Top-1 After (%) -
71.98 69.52 48.75
76.11 73.87 57.45
73.26 71.08 62.36

FLOPs 7.97 × 108 5.01 × 108 5.01 × 108 5.01 × 108
5.00 × 108 3.33 × 108 3.33 × 108 3.33 × 108
5.33 × 108 3.71 × 108 3.71 × 108 3.71 × 108

FLOPs reduced -
37% 37% 37%
33% 33% 33%
30% 30% 30%

-

Table 3: Performance comparison between FSKD, FitNet and fine-tuning on different network structures obtained by network slimming with 500 samples randomly selected from CIFAR-100 training set.

For the few-sample setting, we randomly select 100 (10 for each category) and 500 (50 for each category) images from the CIFAR-10 training set, and keep them fixed in all experiments. Table 1 lists the results of different methods of recovering a pruned network, including FitNet (Romero et al., 2015), fine-tuning with limited data and full training data. Regarding the processing speed, FSKD can be done in 19.3 seconds for student-net from scheme-B with 500 samples, while FitNet requires 157.3 seconds when converged, which is about 8.1× slower. This verifies our previous claim that FSKD is more efficient than FitNet. It can be seen that in the few-sample setting, FSKD provides better accuracy recovery than both FitNet and the fine-tuning procedure adopted in Li et al. (2016). For instance, for scheme-B with only 500 samples, FSKD can recover the accuracy from 47.9% to 91.2%, while few-sample fine-tuning can only recover the accuracy to 83.36%. When full training set available, it will take about 30 minutes for full fine-tuning to reach similar accuracy as FSKD. This demonstrates the big advantages of FSKD over full fine-tuning based solutions.
Figure 2 further studies the performance with different amount of training samples available. It can be observed that our FSKD keep outperforming FitNet under the same training samples. In particular, FitNet experiences a noticeable accuracy drop when the number of samples is less than 100, while FSKD can still recover the accuracy of the pruned network to a high level. It is also interesting to note that fine-tuning experiences even larger accuracy drops than FitNet when the data amount is limited. This verifies that knowledge distillation methods like FitNet provides richer information than fine-tuning/training with label based loss.
We further illustrate the per-layer (block) feature responses difference between teacher and student before and after using FSKD in Figure 3. Before applying FSKD, the correlation between teacher and student is broken due to the aggressive compression. However, after FSKD, the per-layer correlations between teacher and student are restored. This verifies the ability of FSKD for recovering lost information. We do see a decreasing trend with layer depth increased, which is possibly due to error accumulation through multiple convolutional layers.

NETWORK SLIMMING
We then study the student network from another channel pruning method named network slimming (Liu et al., 2017), which removes insignificant filter channels and corresponding feature maps using

6

Under review as a conference paper at ICLR 2019

VGG-16 (teacher) Decoupled (T = 1mix) + FSKD Decoupled (T = 2) + FSKD Decoupled (T = 3) + FSKD Decoupled (T = 4) + FSKD
ResNet-18 (teacher) Decoupled (T = 2) + FSKD Decoupled (T = 3) + FSKD Decoupled (T = 4) + FSKD Decoupled (T = 5) + FSKD

Top-1 Before (%) 68.4 0.12 0.24 1.57 54.6
67.1 0.21 3.99 26.5 53.6

Top-1 After (%) -
51.3 62.7 67.1 67.6
49.5 61.9 65.1 66.3

GFLOPs 15.47 2.73 3.76 5.54 7.31
1.83 0.55 0.75 0.95 1.15

FLOPs Reduced -
82.4% 75.7% 64.2% 52.7%
70.0% 59.0% 48.1% 37.2%

Table 4: Performance of FSKD on different student nets obtained by network decoupling VGG-16 and ResNet-18 with different parameters T on ImageNet dataset.

sparsified channel scaling factors. Network slimming consists of three steps: sparse regularized training, pruning and fine-tuning. Here, we replace the time-consuming fine-tuning step with our FSKD, and follow the original paper (Liu et al., 2017) to conduct experiments to prune different network architectures on different datasets. We apply FSKD on networks pruned from VGG-19, ResNet-164, and DenseNet-40 (Huang et al., 2017), on both CIFAR-10 and CIFAR-100 datasets. Table 2 lists results on CIFAR-10, while Table 3 lists results on CIFAR-100. Note that the pruningratio (like 70% in Table 2) means the portion of channels that are removed in comparison to the total number of channels in the network. It shows that our FSKD consistently outperforms FitNet and fine-tuning with a noticeable margin under the few-sample setting on all evaluated networks and both datasets.
NETWORK DECOUPLING
Network decoupling (Guo et al., 2018) decomposes a regular convolution layer into the sum of several blocks, where each block consists of a depth-wise (DW) convolution layer and a point-wise (PW, 1×1) convolution layer. The ratio of compression increases as the number of blocks decreases, but the accuracy of the compressed model will also drop. Since each decoupled block ends with a 1×1 convolution, we can apply FSKD at the end of each decoupled block.
Following (Guo et al., 2018), we obtain student networks by decoupling VGG-16 and ResNet-18 pre-trained on ImageNet with different T values, where T stands for the number of DW + PW blocks that a conv-layer decouples out. For VGG-16, we also decouple half of the convolution layer with T = 1 and the other half T = 2, and denote the case as "T = 1mix". We evaluate the resulted network performance on the validation set of the ImageNet classification task. We randomly select one image from each of the 1000 classes in ImageNet training set to obtain 1000 samples as our FSKD training set. Table 4 shows the top-1 accuracy of student network before and after applying FSKD on VGG-16 and ResNet-18.
It is quite interesting to see that in the case of T = 1mix for VGG-16 and T = 2 for ResNet-18, we can recover the accuracy of student network from nearly random guess (0.12%, 0.21%) to a much higher level (51.3% and 49.5%) with only 1000 samples. In all the other cases, FSKD can recover the accuracy of a highly-compressed network to be comparable with the original network. One possible explanation is that the highly-compressed networks still inherit some representation power from the teacher network i.e., the depth-wise 3×3 convolution, while lacking the ability to output meaningful predictions due to the inaccurate/degraded 1 × 1 convolution. The FSKD calibrates the 1 × 1 convolution by aligning the block-level response between "teacher" and "student" so that the lost information in 1 × 1 convolution is compensated, and reasonable recovery is achieved.
4.2 ZERO STUDENT NETWORK
Finally, we evaluate FSKD on fully redesigned student network with a different structure from the teacher and random initialized parameters (named as zero student-net). We conduct experiments on CIFAR-10 and CIFAR-100 with VGG-19 as the teacher network and a shallower VGG-13 as the student network. Due to the similar structure between VGG-13 and VGG-19, they can be easily aligned in block-level.
7

Under review as a conference paper at ICLR 2019

Top-1 accuracy (%) Top-1 accuracy (%)

Zero student-net accuracy vs #samples (CIFAR-10)
SGD 80 SGD+FSKD
FitNet 70 FitNet+FSKD

60

50

40

30

20

10

0 100

200 500
#samples

1000

Zero student-net accuracy vs #samples (CIFAR-100)
SGD SGD+FSKD 40 FitNet FitNet+FSKD
30

20

10

0 500

1000

1500

#samples

2000

(a) (b)

Figure 4: Accuracy vs #samples for zero student network on (a) CIFAR-10, (b) CIFAR-100.

The random initialized network does not contain any information about the training set. Simply training this network with few samples will lead to poor generalization ability, as shown in Figure 4. We propose two schemes to combine FSKD with training: SGD+FSKD and FitNet+FSKD. In the SGD+FSKD case, we first use stochastic gradient descent (SGD) to train the student network on the given number of labeled few samples with 150 epochs (multi-step learning-rate decay at every 50 epochs from 0.01 to 0.0001). Then we apply FSKD to the obtained student network using the same few-sample set. We repeat these two steps until the training loss converges. In the FitNet+FSKD case, we keep the same few-sample set, and simply replace the SGD with FitNet to add supervision on intermediate responses during training.
We compare the results from four different recovery methods: running SGD until convergence, SGD+FSKD, running FitNet until convergence, and FitNet+FSKD. In order to better simulate the few-sample setting, we do not apply data augmentation to the training set. We randomly pick 100, 200, 500, 1000 samples from the 10-category CIFAR-10 training set, and 500, 1000, 1500, 2000 samples from the 100-category CIFAR-100 training set, and keep these few-sample sets fixed in this study.
Figure 4 shows the comparison results on the four recovery methods and four few-sample sets. It shows that FSKD+SGD takes a big jump over pure SGD, and FSKD+FitNet also takes a big jump over pure FitNet. FSKD+SGD performs much better than FitNet on CIFAR-10, while this is not true on CIFAR-100. There are two possible reasons. First, we did not enable data augmentation so that few-sample SGD is underfitting, which provides much less information than what the student-net can get from the teacher in FitNet. Second, CIFAR-100 is much more difficult than CIFAR-10 so that the performance is more sensitive to the number of samples. However, FSKD+SGD can still achieve accuracy on par with FitNet. We should also note here that due to no data augmentation and few-sample settings, the accuracy of all our evaluated methods has gaps to that of the teacher networks. Nevertheless, it still demonstrates the advantages of our FSKD over FitNet and fine-tuning based methods.

5 CONCLUSION
We proposed a novel, simple and effective method for knowledge distillation from few samples. The method works for student networks constructed in various ways, including compression from teacher networks and fully redesigned networks with random initialization on various datasets. Notably, we can recover a student network with 0.2% top-1 accuracy to 62.7% out of 68.4% with just 1000 samples on ImageNet dataset. The method outperforms existing knowledge distillation methods by a large margin in the few-sample setting, while requires significantly less computation budget. This advantage will bring many potential applications and extensions for FSKD.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NIPS, 2014.
Evgeniy Bart and Shimon Ullman. Cross-generalization: Learning novel classes from a single example by feature replacement. In CVPR. IEEE, 2005.
Cristian Bucila, Rich Caruana, Alexandru Niculescu-Mizil, et al. Model compression. In SIGKDD. ACM, 2006.
Tianqi Chen, Ian Goodfellow, Jonathon Shlens, et al. Net2net: Accelerating learning via knowledge transfer. In ICLR, 2016.
François Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.
M. Courbariaux, Y. Bengio, Jean-Pierre David, et al. Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1. In ICLR, 2016.
Elliot J Crowley, Gavin Gray, and Amos Storkey. Moonshine: Distilling with cheap convolutions. arXiv preprint arXiv:1711.02613, 2017.
Emily Denton, Zaremba, Yann Lecun, et al. Exploiting linear structure within convolutional networks for efficient evaluation. In NIPS, 2014.
Li Fei-Fei, Rob Fergus, Pietro Perona, et al. One-shot learning of object categories. IEEE Trans PAMI, 2006.
Chelsea Finn, Pieter Abbeel, Sergey Levine, et al. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017.
Jianbo Guo, Yuxi Li, Weiyao Lin, Yurong Chen, and Jianguo Li. Network decoupling: From regular to depthwise separable convolutions. In BMVC, 2018.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, et al. Deep learning with limited numerical precision. In ICML, 2015.
Song Han, Jeff Pool, John Tran, William Dally, et al. Learning both weights and connections for efficient neural network. In NIPS, 2015.
Song Han, Huizi Mao, Bill Dally, et al. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In NIPS, 2016.
K. He, X. Zhang, J. Sun, et al. Deep residual learning for image recognition. In CVPR, 2016.
G. Hinton, O. Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Geoffrey Hinton, Li Deng, Dong Yu, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6), 2012.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In CVPR, 2017.
Zehao Huang and Naiyan Wang. Like what you like: Knowledge distill via neuron selectivity transfer. arXiv preprint arXiv:1707.01219, 2017.
M. Jaderberg, A. Vedaldi, A. Zisserman, et al. Speeding up convolutional neural networks with low rank expansions. In BMVC, 2014.
Y. Kim, E. Park, S. Yoo, et al. Compression of deep convolutional neural networks for fast and low power mobile applications. In ICLR, 2016.
Akisato Kimura, Zoubin Ghahramani, Koh Takeuchi, et al. Few-shot learning of neural networks from scratch by pseudo example optimization. In BMVC, 2018.
9

Under review as a conference paper at ICLR 2019
A. Krizhevsky and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, et al. One shot learning of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 33, 2011.
Hao Li, Asim Kadav, Durdanovic I, et al. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
Min Lin, Qiang Chen, and Shuicheng nd others Yan. Network in network. In ICLR, 2014. Zhuang Liu, Jianguo Li, Zhiqiang Shen, et al. Learning efficient convolutional networks through
network slimming. In ICCV, 2017. Raphael Gontijo Lopes, Stefano Fenu, Thad Starner, et al. Data-free knowledge distillation for deep
neural networks. arXiv preprint arXiv:1710.07535, 2017. J. Luo, J. Wu, W. Lin, et al. Thinet: A filter level pruning method for deep neural network compression.
In ICCV, 2017. Tomás Mikolov, Martin Karafiát, Lukás Burget, et al. Recurrent neural network based language
model. In INTERSPECH, 2010. Vinod Nair and Geoffrey Hinton. Rectified linear units improve restricted boltzmann machines. In
ICML, 2010. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In ECCV, 2016. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, et al. Fitnets: Hints for thin deep nets. In
ICLR, 2015. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015. Suraj Srinivas and Francois Fleuret. Knowledge transfer with jacobian matching. arXiv preprint
arXiv:1803.00443, 2018. Oriol Vinyals, Charles Blundell, Tim Lillicrap, et al. Matching networks for one shot learning. In
NIPS, 2016. Junho Yim, Donggyu Joo, Jihoon Bae, et al. A gift from knowledge distillation: Fast optimization,
network minimization and transfer learning. In CVPR, 2017. X. Zhang, J. Zou, J. Sun, et al. Accelerating very deep convolutional networks for classification and
detection. IEEE TPAMI, 38(10), 2016. B. Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In ICLR, 2016.
10

Under review as a conference paper at ICLR 2019

APPENDIX: PROOF OF THEOREM 1

Proof. When W is a point-wise convolution with tensor W  Rno×ni×1×1, both W and Q are degraded into matrix form. It is obvious that when condition c1  c3 satisfied, the theorem holds with W = Q  W in this case, where  indicates matrix multiplication.

When W is a regular convolution with tensor W  Rno×ni×kw×kh , the proof is non-trivial. Fortunately, recent work on network decoupling (Guo et al., 2018) presents an important theoretic result as the basis of our derivation.
Lemma 1. Regular convolution can be exactly expanded to a sum of several depth-wise separable convolutions. Formally,  W  Rno×ni×kw×kh ,  {Pk, Dk}Kk=1, where Pk  Rno×ni×1×1, Dk  R1×ni×kh×kw ,

s.t. (a)K  khkw;

(b)W =

K
Pk  Dk,
k=1

(3)

where  is the compound operation, which means performing Dk before Pk.

Please refer to Guo et al. (2018) for the details of proof for this Lemma. When W is applied to an input patch x  Rni×kh×kw , we obtain a response vector y  Rno as

y = W  x,

(4)

where yo =

ni i=1

Wo,i



xi,

o



[no],

i



[ni],

and



here

means

convolution

operation.

Wo,i

=

W[o, i, :, :] is a tensor slice along the i-th input and o-th output channels, xi = x[i, :, :] is a tensor

slice along the i-th channel of 3D tensor x.

When point-wise convolution Q is added after Q without non-linear activation between them, we

have

y = Q  (W  x).

(5)

With Lemma-1, we have

KK
y = (Q  k=1 Pk  Dk)  x = ( k=1(Q  Pk)  Dk)  x As both Q and Pk are degraded into matrix form, denoting Pk = Q  Pk and W = we have y = W  x. This proves the case when W is a regular convolution.

(6)

K k=1

Pk



Dk ,

11

