Under review as a conference paper at ICLR 2019
ON THE LOSS LANDSCAPE OF A CLASS OF DEEP
NEURAL NETWORKS WITH NO BAD LOCAL VALLEYS
Anonymous authors Paper under double-blind review
ABSTRACT
We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.
1 INTRODUCTION
It has been empirically observed in deep learning (Dauphin et al., 2014; Goodfellow et al., 2015) that the training problem of over-parameterized1 deep CNNs (LeCun et al., 1990; Krizhevsky et al., 2012) does not seem to have a problem with bad local minima. In many cases, local search algorithms like stochastic gradient descent (SGD) frequently converge to a solution with zero training error even though the training objective is known to be non-convex and potentially has many local minima (Auer et al., 1996; Safran & Shamir, 2018), even for simple models like deep linear networks (Kawaguchi, 2016). This indicates that the problem of training practical over-parameterized neural networks is still far from the worst-case scenario where the problem is known to be NP-hard (Blum & Rivest., 1989; Sima, 2002; Livni et al., 2014; Shalev-Shwartz et al., 2017). A possible hypothesis is that the loss landscape of these networks is"well-behaved" so that it becomes amenable to local search algorithms like SGD and its variants. As not all neural networks have a well-behaved loss landscape, it is interesting to identify sufficient conditions on their architecture so that this is guaranteed. In this paper our motivation is to come up with such a class of networks in a practically relevant setting, that is we study multi-class problems with the usual empirical cross-entropy loss and deep (convolutional) networks and almost no assumptions on the training data, in particular no distributional assumptions. Thus our results directly apply to the networks which we use in the experiments.
Bad local valleys

Global Minimum

Global Minimum

Figure 1: An example loss landscape with bad local valleys (left) and without bad local valley (right).

Our contributions. We identify a family of deep networks with skip connections to the output layer whose loss landscape has no bad local valleys (see Figure 1 for an illustration). Our setting is for the empirical loss and there are no distributional assumptions on the training data. Moreover, we study directly the standard cross-entropy loss for multi-class problems. There are little assumptions on the network structure which can be arbitrarily deep and can have convolutional layers (weight sharing) and skip-connections between hidden layers. From a practical perspective, one can generate
1These are the networks which have more parameters than necessary to fit the training data
1

Under review as a conference paper at ICLR 2019

an architecture which fulfills our conditions by taking an existing CNN architecture and then adding skip-connections from a random subset of N neurons in the network to the output layer (see Figure 2 for an illustration). For these networks we show that there always exists a continuous path from any point in parameter space on which the loss is non-increasing and gets arbitrarily close to zero.
Beside the theoretical analysis, we show in experiments that despite achieving zero training error, the aforementioned class of neural networks generalize well in practice when trained with SGD whereas an alternative training procedure guaranteed to achieve zero training error has significantly worse generalization performance and is overfitting. Thus we think that the presented class of neural networks offer an interesting test bed for future work to study the implicit bias/regularization of SGD.

2 DESCRIPTION OF NETWORK ARCHITECTURE

We consider a family of deep neural networks which have d input units, H hidden units, m output units and satisfy the following conditions:

1. Every hidden unit of the first layer can take as input an arbitrary subset of units of the input layer.
2. Every hidden unit at higher layers can take as input an arbitrary subset of hidden units from an arbitrary subset of previous hidden layers.
3. Any group of hidden units lying on the same layer can have non-shared or shared weights, in which case the number of incoming units has to be equal.
4. There exist N hidden units which are connected to the output nodes with independent weights (N denotes the number of training samples).
5. The output of every hidden unit j in the network, denoted as fj : Rd  R, is given as

fj(x) = j bj +

fk (x)ukj

k:kj

where x  Rd is an input vector of the network, j : R  R is the activation function of unit j, bj  R is the bias of unit j, and ukj  R the weight from unit k to unit j.

This definition covers a class of deep fully connected and convolutional neural networks with an additional condition on the number of connections to the output layer. In particular, while conventional architectures have just connections from the last hidden layer to the output, we require in our setting that there must exist at least N neurons, "regardless" of their hidden layer, that are connected to the output layer. Essentially, this means that if the last hidden layer of a traditional network has just L < N neurons then one can add connections from N - L neurons in the hidden layers below it to the output layer so that the network fulfills our conditions.
Similar skip-connections have been used in DenseNet (Huang et al., 2017) which are different from identity skip-connections as used in ResNets (He et al., 2016). In Figure 2 we illustrate a network with and without skip connections to the output layer which is analyzed in this paper. We note that several architectures like DenseNets Huang et al. (2017) already have skip-connections between hidden layers in their original architecture, whereas our special skip-connections go from hidden layers directly to the output layer. As our framework allow both kinds to exist in the same network (see Figure 2 for an example), we would like to separate them from each other by making the convention that in the following skip-connections, if not stated otherwise, always refer to ones which connect hidden neurons to output neurons.
We denote by d the dimension of the input and index all neurons in the network from the input layer to the output layer as 1, 2, . . . , d, d + 1, . . . , d + H, d + H + 1, . . . , d + H + m which correspond to d input units, H hidden units and m output units respectively. As we only allow directed arcs from lower layers to upper layers, it follows that k < j for every k  j. Let N be the number of training samples. Suppose that there are M hidden neurons which are directly connected to the output with independent weights where it holds N  M  H. Let {p1, . . . , pM } with pj  {d + 1, . . . , d + H} be the set of hidden units which are directly connected to the output units. Let in(j) be the set of incoming nodes to unit j and uj = [ukj]kin(j) the weight vector of the j-th unit. Let U = (ud+1, . . . , ud+H , bd+1, . . . , bd+H ) denote the set of all weights and biases of all

2

Under review as a conference paper at ICLR 2019

Figure 2: Left: An example neural network represented as directed acyclic graph. Right: The same network with skip connections added from a subset of hidden neurons to the output layer. All neurons with the same color can have shared or non-shared weights.

hidden units in the network. Let V  RM×m be the weight matrix which connects the M hidden neurons to the m output units of the network. An important quantity in the following is the matrix   RN×M defined as

 fp1 (x1) . . . fpM (x1) 

= 

...

...

 

(1)

fp1 (xN ) . . . fpM (xN )

As  depends on U , we write U or (U ) as a function of U . Let G  RN×m be the output of the network for all training samples. In particular, Gij is the value of the j-th output neuron for training sample xi. It follows from our definition that

M
Gij = i:, V:j = fpk (xi)Vkj , i  [N ], j  [m]
k=1
Let (xi, yi)Ni=1 be the training set where yi denotes the target class for sample xi. In this paper we want to analyze the commonly used cross-entropy loss given as

1N

(U, V ) =

- log

N

i=1

eGiyi

m k=1

eGik

(2)

The cross-entropy loss is bounded from below by zero but this value is not attained. In fact the global

minimum of the cross-entropy loss need not exist e.g. if a classifier achieves zero training error

then by upscaling the function to infinity one can drive the loss arbitrarily close to zero. Due to this

property, we do not study the global minima of the cross-entropy loss but the question if and how one

can achieve zero training error. Moreover, we note that sufficiently small cross-entropy loss implies

zero training error as shown in the following lemma.

Lemma 2.1

If (U, V ) <

log(2) N

,

then

the

training

error

is

zero.

Proof:

We note that if (U, V ) <

log(2) N

,

then

it

holds

due

to

the

positivity

of

the

loss,

maxi=1,...,N - log

eGiyi

m k=1

eGik

N
 - log
i=1

eGiyi

m k=1

eGik

< log(2).

This implies that for all i = 1, . . . , N ,

log 1 +

eGik-Giyi < log(2) =

eGik-Giyi < 1.

k=yi

k=yi

In particular: maxk=yi eGik-Giyi < 1 and thus maxk=yi Gik - Giyi < 0 for all i = 1, . . . , N which implies the result.

3

Under review as a conference paper at ICLR 2019

3 MAIN RESULT

The following conditions are required for the main result to hold.

Assumption 3.1

1. All activation functions {d+1, . . . , d+H } are real analytic and strictly

increasing

2. Among M neurons {p1, . . . , pM } which are connected to the output units, there exist N neurons (N  M ) whose activation functions satisfy one of the following conditions:

· pj is bounded and limt- pj (t) = 0,  1  j  M
· pj are the softplus activation function (see Equation (3)), and from every neuron pj there exists a backward path to the first hidden layer such that on this path there is no
neuron with skip-connection to the output layer.

3. Let n1 be the number of units in the first hidden layer and denote by Si for i  [d + 1, d + n1] their support, then for all r = s  N and i = j  [d + 1, d + n1], it holds
xr|Si = xs|Sj ,
where we assume that |Si| = |Sj| for all i, j  [d + 1, d + n1].

The first condition of Assumption 3.1 is satisfied for softplus, sigmoid, tanh, etc, whereas the second condition is fulfilled for sigmoid and softplus. For softplus activation function (smooth approximation of ReLU),

 (t)

=

1 

log(1

+

et),

for some  > 0,

(3)

we require an additional assumption on the network architecture. The third condition is needed for CNN architectures and could be violated if they have very small receptive fields. However, if the condition is violated for the given training set then after an arbitrarily small random perturbation of all training inputs it will be satisfied with probability 1. Note that the M neurons which are directly connected to the output units can lie on different hidden layers in the network. Also there is no condition on the width of every individual hidden layer as long as the total number of hidden neurons in the network is larger than N so that our condition M  N is feasible.

Overall, we would like to stress that Assumption 3.1 covers a quite large class of interesting network architectures but nevertheless allows us to show quite strong results on their empirical loss landscape.

The following key lemma shows that for almost all U , the matrix (U ) has full rank.

Lemma 3.2 Under Assumption 3.1, the set of U such that (U ) has not full rank N has Lebesgue measure zero.

While we conjecture that the result of Lemma 3.2 holds for softplus activation function without the additional condition as mentioned in Assumption 3.1, the proof of this is considerably harder for such a general class of neural networks since one has to control the output of neurons with skip connection from different layers which depend on each other. However, please note that the condition is also not too restrictive as it just might require more connections from lower layers to upper layers but it does not require that the network is wide.
We are now ready to state our main result. In the following, we define the -sublevel set of  as L = {(U, V ) | (U, V ) < } . We define a local valley to be a connected component of a certain sublevel set L. A bad local valley is a local valley on which the loss cannot be made "arbitrarily small". Intuitively, a typical example of a bad local valley is a small neighborhood around a sub-optimal strict local minimum.

Theorem 3.3 The following holds under Assumption 3.1:

1. There exist uncountably many solutions with zero training error. 2. The loss landscape of  does not have any bad local valley.

4

Under review as a conference paper at ICLR 2019

3. There exists no suboptimal strict local minimum.

4. There exists no local maximum.

Proof:

1. By Lemma 3.2 the set of U such that (U ) has not full rank N has Lebesgue measure zero. Given U such that  has full rank, the linear system (U )V = Y has for every possible target output matrix Y  RN×m at least one solution V . As this possible for almost all U , there exist uncountably many solutions achieving zero training error.

2. Let C be a non-empty, connected component of some -sublevel set L for  > 0. Suppose by contradiction that the loss on C cannot be made arbitrarily small, that is there exists an > 0 such that (U, V )  for all (U, V )  C, where < . By definition,
L can be written as the pre-image of an open set under a continuous function, that is L = -1({a | a < }), and thus L must be an open set (see Proposition A.2). Since C is a non-empty connected component of L, C must be an open set as well, and thus C has non-zero Lebesgue measure. By Lemma 3.2 the set of U where (U ) has not full rank has
measure zero and thus C must contain a point (U, V ) such that (U ) has full rank. Let Y
be the usual zero-one one-hot encoding of the target network output. As (U ) has full rank, there always exist V  such that (U )V  = Y t, where t = log m-1 Note that the loss
e 2 -1
of (U, V ) is

(U, V ) = - log

et et + (m - 1)

= log(1 + (m - 1)e-t ) = . 2

As the cross-entropy loss (U, V ) is convex in V and (U, V ) <  we have for the line segment V () = V + (1 - )V  for   [0, 1],

(U, V ())  (U, V ) + (1 - )(U, V ) <  + (1 - ) < . 2

Thus the whole line segment is contained in L and as C is a connected component it has to be contained in C. However, this contradicts the assumption that for all (U, V )  C it holds
(U, V )  . Thus on every connected component C of L the training loss can be made
arbitrarily close to zero and thus the loss landscape has no bad valleys.

3. Let (U0, V0) be a strict suboptimal local minimum, then there exists r > 0 such that

(U, V ) > (U0, V0) > 0 for all (U, V )  B((U0, V0), r) \ {(U0, V0)} where B(·, r)

denotes a closed ball of radius r. Let  = min

(U, V ) which exists

(U,V )B (U0,V0),r

as  is continuous and the boundary B (U0, V0), r of B (U0, V0), r is compact. Note

that  > (U0, V0) as (U0, V0) is a strict local minimum. Consider the sub-level set

D

= L .+(U0,V0)

As (U0, V0) <

+(U0 ,V0 ) 2

it

holds

(U0, V0)



D.

Let

E

be

the

2

connected component of D which contains (U0, V0), that is, (U0, V0)  E  D. It holds

E  B (U0, V0), r

as (U, V )

<

+(U0 ,V0 ) 2

<

 for all (U, V )



E.

Moreover,

(U, V )  (U0, V0) > 0 for all (U, V )  E and thus  can not be made arbitrarily small

on a connected component of a sublevel set of  and thus E would be a bad local valley

which contradicts 3.3.2.

4. Suppose by contradiction that (U, V ) is a local maximum. Then the Hessian of  is negative
semi-definite. However, as submatrices of negative semi-definite matrices are again negative
semi-definite, then also the Hessian of  w.r.t V must be negative semi-definite However, 
is always convex in V and thus its Hessian restricted to V is positive semi-definite. The only matrix which is both p.s.d. and n.s.d. is the zero matrix. It follows that 2V (U, V ) = 0. One can easily show that

N
V2:j  =
i=1

eGij

m k=1

eGik

1-

eGij

m k=1

eGik

i:iT:

From Assumption 3.1 it holds that there exists j  [N ] s.t. pj is strictly positive, and

thus some entries of i: must be strictly positive. Moreover, one has

eGij

m k=1

eGik

 (0, 1).

5

Under review as a conference paper at ICLR 2019
It follows that some entries of V2:j  must be strictly positive. Thus 2V:j  cannot be identically zero, leading to a contradiction. Therefore  has no local maximum.
Theorem 3.3 shows that there are infinitely many solutions which achieve zero training error, and the loss landscape is nice in the sense that from any point in the parameter space there exists a continuous path that drives the loss arbitrarily close to zero (and thus a solution with zero training error) on which the loss is non-increasing.
While the networks are over-parameterized, we show in the next Section 4 that the modification of standard networks so that they fulfill our conditions leads nevertheless to good generalization performance, often even better than the original network. We would like to note that the proof of Theorem 3.3 also suggests a different algorithm to achieve zero training error: one initializes all weights, except the weights to the output layer, randomly (e.g. Gaussian weights), denoted as U , and then just solves the linear system (U )V = Y to obtain the weights V to the output layer. Basically, this algorithm uses the network as a random feature generator and fits the last layer directly to achieve zero training error. The algorithm is successful with probability 1 due to Lemma 3.2. Note that from a solution with zero training error one can drive the cross-entropy loss to zero by upscaling to infinity but this does not change the classifier. We will see, that this simple algorithm shows bad generalization performance and overfitting, whereas training the full network with SGD leads to good generalization performance. This might seem counterintuitive as our networks have more parameters than the original networks but is inline with recent observations in Zhang et al. (2017) that state-of-the art networks, also heavily over-parameterized, can fit even random labels but still generalize well on the original problem. Due to this qualitative difference of SGD and the simple algorithm which both are able to find solutions with zero training error, we think that our class of networks is an ideal test bed to study the implicit regularization/bias of SGD, see e.g. Soudry et al. (2018).
4 EXPERIMENTS
The main purpose of this section is to investigate the generalization ability of practical neural networks with skip-connections added to the output layer to fulfill Assumption 3.1.
Datasets. We consider MNIST and CIFAR10 datasets. MNIST contains 5.5 × 104 training samples and 104 test samples, and CIFAR10 has 5 × 104 training samples and 104 test samples. We do not use any data pre-processing in all of our experiments. For every experiment on CIFAR10 described below, we consider both settings with and without data-augmentation. For data-augmentation, we follow the procedure as described in Zagoruyko & Komodakis (2016) by considering random crops of size 32 × 32 after 4 pixel padding on each side of the training images and random horizontal flips with probability 0.5.
Network architectures. For MNIST, we use a plain CNN architecture with 13 layers, denoted as CNN13 (see Table 3 in the appendix for more details about this architecture). For CIFAR10 we use VGG11, VGG13, VGG16 (Simonyan & Zisserman, 2015) and DenseNet121 (Huang et al., 2017). As the VGG models were originally proposed for ImageNet and have very large fully connected layers, we adapted these layers for CIFAR10 by reducing their width from 4096 to 128. For each given network, we create the corresponding skip-networks by adding skip-connections to the output so that our condition M  N from the main theorem is satisfied. In particular, we aggregate all neurons of all the hidden layers in a pool and randomly choose from there a subset of N neurons to be connected to the output layer (see e.g. Figure 2 for an illustration). As existing network architectures have a large number of feature maps per layer, the total number of neurons is often very large compared to number of training samples, thus it is easy to choose from there a subset of N neurons to connect to the output. In the following, we test both sigmoid and softplus activation function ( = 20) for each network architecture and their skip-variants. We use the standard cross-entropy loss and train all models with SGD+Nesterov momentum for 300 epochs. The initial learning rate is set to 0.1 for Densenet121 and 0.01 for the other architectures. Following Huang et al. (2017), we also divide the learning rate by 10 after 50% and 75% of the total number of training epochs. Note that we do not use any explicit regularization like weight decay or dropout.
6

Under review as a conference paper at ICLR 2019

In all our experiments, the conditions of Assumption 3.1 are satisfied for the sigmoid activation

function, and thus the main results of Theorem 3.3 hold. However, for softplus, we do not check if

the additional condition in Assumption 3.1 that there exists a backward path from all N skip-neurons

to the first hidden layer only visiting neurons which are not skip-neurons as this is quite costly. Our

main goal in the experiments is to investigate the influence of the additional skip-connections to the

output layer on the generalization performance. We report the test accuracy for the original models

and the ones with skip-connections to the output layer. For the latter one we have two different

algorithms: standard SGD for training the full network as described above (SGD) and the randomized

procedure (rand). The latter one uses a slight variant of the simple algorithm described at the end of

the last section: randomly initialize the weights of the network U up to the output layer by drawing

each

of

them

from

a

truncated

Gaussian

distribution

with

zero

mean

and

variance

2 d

where

d

is

the

number of weight parameters and the truncation is done after ±2 standard deviations (standard keras

initialization), then use SGD to optimize the weights V for a linear classifier with fixed features

(U ) which is a convex optimization problem. Note that the rand algorithm cannot be used with

data augmentation in a straightforward way and thus we skip it for this part.

Our experimental results are summarized in Table 1 for MNIST and Table 2 for CIFAR10. For skip-models, we report mean and standard deviation over 8 random choices of the subset of N neurons connected to the output.

CNN13 CNN13-skip (SGD)

Sigmoid activation function 11.35
98.40 ± 0.07

Softplus activation function 99.20
99.14 ± 0.04

Table 1: Test accuracy (%) of CNN13 on MNIST dataset. CNN13 denotes the original architecture from Table 3 while CNN13-skip denotes the corresponding skip-model. There are in total 179, 840 hidden neurons from the original CNN13 (see Table 3), out of which we choose a random subset of N = 55, 000 neurons to connect to the output layer to obtain CNN13-skip.

Model
VGG11 VGG11-skip (rand) VGG11-skip (SGD)
VGG13 VGG13-skip (rand) VGG13-skip (SGD)
VGG16 VGG16-skip (rand) VGG16-skip (SGD)
Densenet121 Densenet121-skip (rand) Densenet121-skip (SGD)

Sigmoid activation function

C-10

C-10+

10 62.81 ± 0.39 72.51 ± 0.35

10 85.55 ± 0.09

10 61.50 ± 0.34 70.24 ± 0.39

10 86.48 ± 0.32

10 61.57 ± 0.41 70.61 ± 0.36

10 86.42 ± 0.31

86.41 52.07 ± 0.48 81.47 ± 1.03

90.93 -
90.32 ± 0.50

Softplus activation function

C-10

C-10+

78.92 64.49 ± 0.38 80.57 ± 0.40

88.62 -
89.32 ± 0.16

80.84 61.42 ± 0.40 81.94 ± 0.40

90.58 -
91.06 ± 0.12

81.33 61.46 ± 0.34 81.91 ± 0.24

90.68 -
91.00 ± 0.22

89.31 55.39 ± 0.48 86.76 ± 0.49

94.20 -
93.23 ± 0.42

Table 2: Test accuracy (%) of several CNN architectures with/without skip-connections on CIFAR10 (+ denotes data augmentation). For each model A, A-skip denotes the corresponding skip-model in which a subset of N hidden neurons "randomly selected" from the hidden layers are connected to the
output units. For Densenet121, these neurons are randomly chosen from the first dense block. The names in open brackets (rand/SGD) specify how the networks are trained: rand (U is randomized and fixed while V is learned with SGD), SGD (both U and V are optimized with SGD).

7

Under review as a conference paper at ICLR 2019
Discussion of results. First of all, we note that adding skip connections to the output improves the test accuracy in almost all networks (with the exception of Densenet121) when the full network is trained with SGD. In particular, for the sigmoid activation function the skip connections allow for all models except Densenet121 to get reasonable performance whereas training the original model fails. This effect can be directly related to our result of Theorem 3.3 that the loss landscape of skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training error. The exception is Densenet121 which gets already good performance for the sigmoid activation function for the original model. We think that the reason is that the original Densenet121 architecture has already quite a lot of skip-connections between the hidden layers which thus improves the loss surface already so that the additional connections added to the output units are not necessary anymore.
The second interesting observation is that we do not see any sign of overfitting for the SGD version even though we have increased for all models the number of parameters by adding skip connections to the output layer and we know from Theorem 3.3 that for all the skip-models one can easily achieve zero training error. This is in line with the recent observation of Zhang et al. (2017) that modern heavily over-parameterized networks can fit everything (random labels, random input) but nevertheless generalize well on the original training data when trained with SGD. This is currently an active research area to show that SGD has some implicit bias (Neyshabur et al., 2017; Brutzkus et al., 2018; Soudry et al., 2018) which leads to a kind of regularization effect similar to the linear least squares problem where SGD converges to the minimum norm solution. Our results confirm that there is an implicit bias of SGD as we see a strong contrast to the (rand) results obtained by using the network as a random feature generator and just fitting the last layer which also leads to solutions with zero training error with probability 1 as shown in Lemma 3.2 and the proof of Theorem 3.3. For this (rand) version we see that the test accuracy gets worse as one is moving from simpler networks (VGG11) to more complex ones (VGG16 and Densenet121) which is a sign of overfitting. Thus we think that our class of networks is also an interesting test bed to understand the implicit regularization effect of SGD. It seems that SGD selects from the infinite pool of solutions with zero training error one which generalizes well, whereas the randomized feature generator selects one with much worse generalization performance.
5 RELATED WORK
In the literature, many interesting theoretical results have been developed on the loss landscape of neural networks Haeffele & Vidal (2017); Choromanska et al. (2015); Kawaguchi (2016); Safran & Shamir (2016); Hardt & Ma (2017); Yun et al. (2017); Venturi et al. (2018); Zhang et al. (2018) and the behavior of SGD for the minimization of training objective has been also analyzed for various settings (Andoni et al., 2014; Sedghi & Anandkumar, 2015; Janzamin et al., 2016; Gautier et al., 2016; Brutzkus & Globerson, 2017; Soltanolkotabi, 2017; Soudry & Hoffer, 2017; Zhong et al., 2017; Tian, 2017; Du et al., 2018; Wang et al., 2018) to name a few. Most of current results on the loss landscape of neural networks are however limited to shallow networks (one hidden layer), deep linear networks and/or make assumptions on the distribution of the data. An interesting recent exception is Liang et al. (2018) where they show for binary classification one neuron with a skip-connection to the output layer and exponential activation function is enough to eliminate all bad local minima under mild conditions on the loss function. More closely related in terms of the setting are (Nguyen & Hein, 2017; 2018) where they study the loss surface of fully connected and convolutional networks if one of the layers has more neurons than the number of training samples for the standard multi-class problem. However, the presented results are stronger as we show that our networks do not have any suboptimal strict local minima and there is less over-parameterization if the number of classes is small.
6 CONCLUSION
We have identified a class of deep neural networks whose loss landscape has no bad local valleys. While our networks are over-parameterized and can easily achieve zero training error, they generalize well in practice when trained with SGD. Interestingly, a simple different algorithm using the network as random feature generator also achieves zero training error but has significantly worse generalization performance. Thus we think that our class of models is an interesting test bed for studying the implicit regularization effect of SGD.
8

Under review as a conference paper at ICLR 2019
REFERENCES
A. Andoni, R. Panigrahy, G. Valiant, and L. Zhang. Learning polynomials with neural networks. ICML, 2014.
T. M. Apostol. Mathematical analysis. Addison Wesley, 1974.
P. Auer, M. Herbster, and M. K. Warmuth. Exponentially many local minima for single neurons. NIPS, 1996.
A. Blum and R. L Rivest. Training a 3-node neural network is np-complete. NIPS, 1989.
A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. ICML, 2017.
A. Brutzkus, A. Globerson, E. Malach, and S. Shalev-Shwartz. Sgd learns over-parameterized networks that provably generalize on linearly separable data. ICLR, 2018.
A. Choromanska, M. Hena, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. AISTATS, 2015.
Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. NIPS, 2014.
S. Du, J. Lee, Y. Tian, A. Singh, and B. Póczos. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. ICML, 2018.
A. Gautier, Q. Nguyen, and M. Hein. Globally optimal training of generalized polynomial neural networks with nonlinear spectral methods. NIPS, 2016.
I. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network optimization problems. ICLR, 2015.
B. D. Haeffele and R. Vidal. Global optimality in neural network training. CVPR, 2017.
M. Hardt and T. Ma. Identity matters in deep learning. ICLR, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CVPR, 2016.
G. Huang, Z. Liu, L. Maaten, and K. Weinberger. Densely connected convolutional networks. CVPR, 2017.
M. Janzamin, H. Sedghi, and A. Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv:1506.08473, 2016.
K. Kawaguchi. Deep learning without poor local minima. NIPS, 2016.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. NIPS, 2012.
Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. Handwritten digit recognition with a back-propagation network. NIPS, 1990.
S. Liang, R. Sun, J. D. Lee, and R. Srikant. Adding one neuron can eliminate all bad local minima. arXiv:1805.08671, 2018.
R. Livni, S. Shalev-Shwartz, and O. Shamir. On the computational efficiency of training neural networks. NIPS, 2014.
B. Mityagin. The zero set of a real analytic function. arXiv:1512.07276, 2015.
B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in deep learning. NIPS, 2017.
Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. ICML, 2017.
9

Under review as a conference paper at ICLR 2019
Q. Nguyen and M. Hein. Optimization landscape and expressivity of deep cnns. ICML, 2018. V. D. Nguyen. Complex powers of analytic functions and meromorphic renormalization in qft.
arXiv:1503.00995, 2015. I. Safran and O. Shamir. On the quality of the initial basin in overspecified networks. ICML, 2016. I. Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks.
ICML, 2018. H. Sedghi and A. Anandkumar. Provable methods for training neural networks with sparse connectiv-
ity. ICLR Workshop, 2015. S. Shalev-Shwartz, O. Shamir, and S. Shammah. Failures of gradient-based deep learning. ICML,
2017. J. Sima. Training a single sigmoidal neuron is hard. Neural Computation, 14:2709­2728, 2002. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
ICLR, 2015. M. Soltanolkotabi. Learning relus via gradient descent. NIPS, 2017. D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural
networks. ICLR Workshop 2018, 2017. D. Soudry, E. Hoffer, M. S. Nacson, and N. Srebro. The implicit bias of gradient descent on separable
data. ICLR, 2018. Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications
in convergence and critical point analysis. ICML, 2017. L. Venturi, A. S. Bandeira, and J. Bruna. Spurious valleys in two-layer neural network optimization
landscapes. arXiv:1802.06384, 2018. G. Wang, G. B. Giannakis, and J. Chen. Learning relu networks on linearly separable data: Algorithm,
optimality, and generalization. arXiv:1808.04685, 2018. C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. ICLR, 2017. S. Zagoruyko and N. Komodakis. Wide residual networks. BMCV, 2016. C. Zhang, S. Bengio, M. Hardt, B. Recht, and Oriol Vinyals. Understanding deep learning requires
re-thinking generalization. ICLR, 2017. H. Zhang, J. Shao, and R. Salakhutdinov. Deep neural networks with multi-branch architectures are
less non-convex. arXiv:1806.01845, 2018. K. Zhong, Z. Song, P. Jain, P. Bartlett, and I. Dhillon. Recovery guarantees for one-hidden-layer
neural networks. ICML, 2017.
A MATHEMATICAL TOOLS
In the proof of Lemma 3.2 we make use of the following property of analytic functions.
Lemma A.1 (Nguyen, 2015; Mityagin, 2015) If f : Rn  R is a real analytic function which is not identically zero then the set {x  Rn | f (x) = 0} has Lebesgue measure zero.
We recall the following standard result from topology (see e.g. Apostol (1974), Theorem 4.23, p. 82), which is used in the proof of Theorem 3.3. Proposition A.2 Let f : Rm  Rn be a continuous function. If U  Rn is an open set then f -1(U ) is also open.
10

Under review as a conference paper at ICLR 2019

B PROOF OF LEMMA 3.2

Proof: We assume w.l.o.g. that {p1, . . . , pN } is a subset of the neurons with skip connections to the output layer (see Assumption 3.1). In the following, we will show that there exists a weight configuration U such that the submatrix 1:N,1:N has full rank. Using then that the determinant is an analytic function together with Lemma A.1, we will conclude that the set of weight configurations U such that  has not full rank has Lebesgue measure zero.
We remind that all the hidden units in the network are indexed from the first hidden layer till the higher layers as d + 1, . . . , d + H. For every hidden neuron j  [d + 1, d + H], uj denotes the associated weight vector
uj = [ukj]kin(j)  R|in(j)|, where in(j) = the set of incoming units to unit j.
Let n1 be the number of units of the first hidden layer.

1. We first pick for all hidden units of the first layer the weights {ud+1, . . . , ud+n1 } such that

fk(xi)ukj = fk(xi )ukj i = i , j  [d + 1, d + n1]

kj

kj

Note that if a unit j belongs to the first hidden layer, then every incoming unit k  j must come from the input layer, that is, k  [1, d]. Thus the above condition can be rewritten as

(xi)kukj = (xi )kukj i = i , j  [d + 1, d + n1]

kj

kj

(4)

Intuitively, this condition guarantees that the values of every individual unit from the first hidden layer are different across all training samples. Note that the above sums can be rewritten as an inner product of some input patch and the corresponding weight vector. Thus using condition iii) from the Assumptions 3.1, the set of weights of the first hidden layer which do not satisfy the condition (4) has Lebesgue measure zero.
2. We choose {ud+n1+1, . . . , ud+H } s.t. every weight vector uj has exactly one 1 in and 0 elsewhere.
3. Let  := (d+1, . . . , d+H ) be a tuple of positive scalars and let   R such that pj () = 0 for every j  [N ]. For every neuron pj(j  [N ]) which has a skip-connection to the output layer, let us pick the bias

bpj =  - pj

fk(xj )ukpj .

kpj

The biases for the other hidden neurons are set to zero, that is, bj = 0 for every j  {d + 1, . . . , d + H} \ {p1, . . . , pN } .
4. In the following, we consider a family of configurations of network parameters of the form (juj, bj)dj=+dH+1 where (uj, bj)jd=+dH+1 are chosen as described above. By our construciton so far, the output of each hidden neuron is

fpj (xi) = pj  + pj

fk(xi) - fk(xj ) ukpj

kpj

j  [N ],

fj(xi) = j j fk(xi)ukj j  {d + 1, . . . , d + H} \ {p1, . . . , pN } .
kj

5. Now, one can show for every  > 0 and every j  [N ] that

(5)

fk(xi)ukpj =

fk(xi )ukpj i = i .

kpj

kpj

Moreover,

if one sorts all

kpj fk(x1)ukpj , . . . , kpj fk(xN )ukpj order is invariant w.r.t. every positive tuple .

elements of the set in increasing order then this

11

Under review as a conference paper at ICLR 2019

Proof: The statement is true for every j  [N ] where pj  [d+1, d+n1] by our construction in (4). Note that for those j  [N ] where pj / [d + 1, d + n1], it holds by our contruction
in 2) that upj has exactly one 1 in its entries and zero elsewhere. For this purpose, let c(j) be the index of an incoming unit to every hidden unit j  {d + n1 + 1, . . . , d + H} such
that uc(j)j = 1 and ukj = 0 for every k = c(j). With this notation, it holds for every
j  [N ] where pj / [d + 1, d + n1] that

fk(xi)ukpj = fc(pj)(xi)
kpj

(6)

By considering the value of the sum kpj fk(x)ukpj at different training samples, it holds for every i = i that

  


fk(xi)ukpj <

fk(xi )ukpj

kpj

kpj

fc(pj)(xi) < fc(pj)(xi )

c-(1pj ) fc(pj )(xi) < c-(1pj ) fc(pj )(xi )

c(pj )

fk(xi)ukc(pj ) < c(pj )

fk(xi )ukc(pj )

kc(pj )

kc(pj )

fk(xi)ukc(pj) <

fk(xi )ukc(pj )

kc(pj )

kc(pj )

where the first step follows from (6), the second step follows from the fact that all activation functions are strictly increasing by Assumption 3.1 and thus there exists an inverse function, the third step follows from (5), and the last step follows from the fact that c(pj) > 0.
Now, if c(pj) is already a hidden unit of the first hidden layer then we are done. Otherwise, one can repeatedly apply the same chain of inequalities to c(pj), c(c(pj)), . . . until one eventually reaches a neuron from the first hidden layer.

In summary, we have shown that the order of all elements from the set

kpj fk(x1)ukpj , . . . , kpj fk(xN )ukpj is fully determined by the order

of elements from the set

kqj (x1)kukqj , . . . , kqj (xN )kukqj where qj =

c(c(. . . c(pj) . . .))  [d + 1, d + n1] is some neuron in the first hidden layer. Moreover,

this order is independent of the chosen . Note that this order can be different for different

neurons qj in the first hidden layer, and thus can be different for different pj's.

6. Let  be a permutation such that it holds for every j = 1, 2, . . . , N that

(j) =

arg max

fk (xi )ukpj

i{1,...,N }\{(1),...,(j-1)} kpj

(7)

It follows from our previous argument that  is invariant w.r.t. every  > 0. By definition, it holds that

fk(xj )ukpj >

fk(xi )ukpj  i, j  [N ], i > j

kpj

kpj

Since  is independent of every positive tuple , it can be fixed in the beginning by (7) with some fixed choice of  > 0. Thus one can assume w.l.o.g. that  is the identity permutation as otherwise one can relabel the training data according to  and the rank of  is invariant under relabeling. Thus it holds for every  > 0 that

ij :=

fk(xi)ukpj -

fk(xj)ukpj < 0  i, j  [N ], i > j

kpj

kpj

(8)

Now, we are ready to show that there exists a tuple  = (d+1, . . . , d+N ) > 0 for which  has full rank. We consider two cases:

12

Under review as a conference paper at ICLR 2019

· In the first case, the activation functions pj : R  R for every j  [N ] are strictly
increasing, bounded and limt- pj (t) = 0. In the following, let l(j) denote the layer index of the hidden unit j. For every hidden unit j  {d + 1, . . . , d + H} we set

j = max

1,

max maxi>k
k[N ]|l(pk)=l(j)

p-k1( ) -  fc(pk)(xi) - fc(pk)(xk)

(9)

where > 0 is an arbitrarily small constant which will be specified later. There are a

few remarks we want to make for Eq. (9) before proceeding with our proof. First, the
second term in (9) can be empty if there is no skip-connection unit pk which lies on the same layer as unit j, in which case j is simply set to 1. Second, j's are well-defined by constructing the values fc(pk)(xr), r = 1, . . . , N by a forward pass through the network (note that the network is a directed, acyclic graph; in particular, in the formula of j, one has l(c(pk)) < l(pk) = l(j) and thus the computation of j is feasible given the values of hidden units lying below the layer of unit j, namely fc(pk)). Third, since we are considering the parameter configuration of the form (juj, bj)dj=+dH+1 and we want to show that there exists {j}jd=+dH+1 for which  has full rank, we need to guarantee that by choosing the values of {j}jd=+dH+1 all the potential parameter sharing conditions of multiple hidden units from the same layer can still be satisfied. It turns out that this is indeed guaranteed by
our construction. In particular, if j and j are two hidden units from the same layer, i.e.
l(j) = l(j ), then it follows from (9) that j = j . Moreover, the weight vectors uj and uj can be always chosen to be identical while still satisfying the properties mentioned in the first two construction steps of the proof. Thus it holds that juj = j u for every two hidden units j, j lying on the same layer, meaning that all the potential weight sharing

conditions between multiple hidden units of the same layer in the network are satisfied.

The main idea of choosing the values of pj (extracted from (9)) is to obtain

ij = fpj (xi)   i, j  [N ], i > j.

(10)

To see this, one first observes from (9) that  > 0 and thus it follows from (8) that

ij = fc(pj)(xi) - fc(pj)(xj ) < 0  i, j  [N ], i > j.

(11)

Since (9) holds for all the hidden units j in the network, one can replace j in (9) with every skip-connection unit pj in order to obtain

pj

>

max
i>j

p-j1( ) -  fc(pj)(xi) - fc(pj)(xj )

 j  [N ]

which combined with (11) leads to

pj (fc(pj)(xi) - fc(pj)(xj ))  p-j1( ) -   i, j  [N ], i > j.

From (5) one has for every i, j  [N ], i > j that

fpj (xi) = pj  + pj

fk(xi) - fk(xj ) ukpj

kpj

= pj  + pj (fc(pj)(xi) - fc(pj)(xj ))

 pj  + p-j1( ) -  =

which finishes the proof of (10).
Coming back to the main proof of the lemma, since pj (j  [N ]) are bounded there exists a finite positive constant C such that it holds that

|ij|  C  i, j  [N ] By the Leibniz-formula one has

(12)

NN

det(1:N,1:N ) = pj () +

sign() (j)j

j=1

SN \{}

j=1

(13)

13

Under review as a conference paper at ICLR 2019

where SN is the set of all N ! permutations of the set {1, . . . , N } and  is the identity permutation. Now, one observes that for every permutation  = , there always exists at
least one component j where (j) > j in which case it follows from (10) and (12) that

N

sign() (j)j

SN \{}

j=1

 N ! CN-1

N j=1

pj

()

By choosing = 2N!CN-1 , we get that

N 1N 1N

det(1:N,1:N )  pj () - 2 pj () = 2 pj () = 0

j=1

j=1

j=1

and thus  has full rank.
· In the second case we consider the softplus activation function under the condition that there exist N neurons with skip-connection to the output layer which have a path backward through the network which does not contain any skip-connection neurons.
We choose the  for the non-skip connection neurons as before noting again that by the particular choice of u it holds,

fk(xi)ukpj = fc(pj)(xi)
kpj
Then we set all p1 , . . . , pN to  we get

fpj (xi) = pj  + 

fk(xi) - fk(xj ) ukpj

kpj

j  [N ],

= pj  +  fc(pj)(xi) - fc(pj)(xj )

fj(xi) = j fc(j)(xi) j  {d + 1, . . . , d + H} \ {p1, . . . , pN } .

(14)

Note that by assumption the path c(k)(pj) does not contain any skip connection unit and will eventually end up at some neuron qj  [d + 1, d + n1] of the first hidden layer after
some Lj steps. Thus we can write

fpj (xi) = pj  +  g(xi) - g(xj ) ,

where

g(xi) = c(pj)(c(c(pj))(. . . ( (xi)kukqj ) . . .))  i  [N ].
kqj

Moreover, it holds that g(xi) < g(xj) for every i > j. Note that softplus fulfills for

t

<

0, (t)



1 

e

t

,

whereas

for

t

>

0 one has (t)



1 

+ t.

The latter property

implies

(K)(t)



K 

+ t.

Finally,

this

together

implies

that

there

exist

positive

constants

c1, c2, c3, c4 such that it holds for all j  [N ] that

N
| (j)j |  c1e-c2 (c3 + )N-1.
j=1

This can be made arbitrarily small by increasing . We thus get

N

lim


det(1:N,1:N )

=

pj () = 0

j=1

So far, we have shown that there always exist U such that  has full rank. Every entry of  is real analytic as  is analytic by Assumption 3.1 (note that also the softplus activation function  is analytic). The set of low rank matrices  can be characterized by a system of

14

Under review as a conference paper at ICLR 2019

equations such that all the

M N

determinants of all N × N sub-matrices of  are zero.

As

the determinant is a polynomial in the entries of the matrix and thus an analytic function of

the entries and composition of analytic functions are again analytic, we conclude that each

determinant is an analytic function of U . As shown above, there exists at least one U such that

one of these determinant functions is not identically zero and thus by Lemma A.1, the set of U

where this determinant is zero has measure zero. But as all submatrices need to have low rank

in order that  has low rank, it follows that the set of U where  has low rank has just measure zero.

C EXPERIMENT DETAILS

Layer

Output size

Input: 28 × 28

28 × 28 × 1

3 × 3 conv - 64, stride 1 28 × 28 × 64

3 × 3 conv - 64, stride 1 28 × 28 × 64

3 × 3 conv - 64, stride 2 14 × 14 × 64

3 × 3 conv - 128, stride 1 14 × 14 × 128

3 × 3 conv - 128, stride 1 14 × 14 × 128

3 × 3 conv - 128, stride 2 7 × 7 × 128

3 × 3 conv - 256, stride 1 7 × 7 × 256

1 × 1 conv - 256, stride 1 7 × 7 × 256

3 × 3 conv - 256, stride 2 4 × 4 × 256

3 × 3 conv - 256, stride 1 4 × 4 × 256

3 × 3 conv - 256, stride 2 2 × 2 × 256

3 × 3 conv - 256, stride 1 2 × 2 × 256

3 × 3 conv - 256, stride 2 1 × 1 × 256

Fully connected, 10 output units

#neurons
50176 50176 12544 25088 25088
6272 12544 12544
4096 4096 1024 1024
256

Table 3: The architecture of CNN13 for MNIST dataset. There are in total 179, 840 hidden neurons.

15

