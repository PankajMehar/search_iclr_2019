Under review as a conference paper at ICLR 2019
TREE-STRUCTURED RECURRENT SWITCHING LINEAR DYNAMICAL SYSTEMS FOR MULTI-SCALE MODELING
Anonymous authors Paper under double-blind review
ABSTRACT
Many real-world systems studied are governed by complex, nonlinear dynamics. By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions. Here, we develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet also more accurate models. Our probabilistic model achieves this multi-scale property through of a hierarchy of locally linear dynamics that jointly approximate global nonlinear dynamics. We call it the tree-structured recurrent switching linear dynamical system. To fit this model, we present a fully-Bayesian sampling procedure using Pólya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling. Through a variety of synthetic and real examples, we show how these models outperform existing methods in both interpretability and predictive capability.
1 INTRODUCTION
Complex systems can often be described at multiple levels of abstraction. A computer program can be characterized by the list of functions it calls, the sequence of statements it executes, or the assembly instructions it sends to the microprocessor. As we zoom in, we gain an increasingly nuanced view of the system and its dynamics. The same is true of many natural systems. For example, brain activity can be described in terms of high-level psychological states or via detailed ion channel activations; different tasks demand different levels of granularity. One of our principal aims as scientists is to identify appropriate levels of abstraction for complex natural phenomena and to discover the dynamics that govern how these systems behave at each level of resolution.
Modern machine learning offers a powerful toolkit to aid in modeling the dynamics of complex systems. Bayesian state space models and inference algorithms enable posterior inference of the latent states of a system and the parameters that govern their dynamics (Särkkä, 2013; Barber et al., 2011; Doucet et al., 2001). In recent years, this toolkit has been expanded to incorporate increasingly flexible components like Gaussian processes (Frigola et al., 2014) and neural networks (Chung et al., 2015; Johnson et al., 2016; Gao et al., 2016; Krishnan et al., 2017) into probabilistic time series models. In neuroscience, sequential autoencoders offer highly accurate models of brain activity (Pandarinath et al., 2018). However, while these methods offer state of the art predictive models, their dynamics are specified at only the most granular resolution, leaving the practitioner to tease out higher level structure post hoc.
Here we propose a probabilistic generative model that provides a multi-scale view of the dynamics through a hierarchical architecture. We call it the tree-structured recurrent switching linear dynamical system, or TrSLDS. The model builds on the recurrent SLDS (Linderman et al., 2017) to approximate latent nonlinear dynamics through a hierarchy of locally linear dynamics. Once fit, the TrSLDS can be queried at different levels of the hierarchy to obtain dynamical descriptions at multiple levels of resolution. As we proceed down the tree, we obtain higher fidelity, yet increasingly complex, descriptions. Thus, depth offers a simple knob for trading off interpretability and flexibility. The key contributions are two-fold: first, we introduce a new form of tree-structured stick breaking for multinomial models that strictly generalizes the sequential stick breaking of the original rSLDS,
1

Under review as a conference paper at ICLR 2019

while still permitting Pólya-gamma data augmentation (Polson et al., 2013) for efficient posterior inference; second, we develop a hierarchical prior that links dynamics parameters across levels of the tree, thereby providing descriptions that vary smoothly with depth.
The paper is organized as follows. Section 2 provides background material on switching linear dynamical systems and their recurrent variants. Section 3 presents our tree-structured model and Section 4 derives an efficient fully-Bayesian inference algorithm for the latent states and dynamics parameters. Finally, in Section 5 we show how our model yields multi-scale dynamics descriptions for synthetic data from two standard nonlinear dynamical systems--the Lorenz attractor and the FitzHugh-Nagumo model of neural action potentials--as well as for a real dataset of neural responses to visual stimuli in a macaque monkey.

2 BACKGROUND

Let xt  Rdx and yt  Rdy denote the latent state and the observation of the system at time t respectively. The system can be described using a state-space model:

xt = f (xt-1, wt; ), wt  Fw (state dynamics)

yt = g(xt, vt; ),

vt  Fv

(observation)

(1) (2)

where  are the dynamics parameters,  are the emission (observation) parameters, and wt and vt are the state and observation noise respectively. For simplicity, we restrict ourselves to systems of the
form:

xt = f (xt-1; ) + wt, wt  N (0, Q),

yt = g(xt; ) + vt,

vt  N (0, S).

(3) (4)

If the state space model is completely specified then recursive Bayesian inference can be applied to obtain an estimate of the latent states using the posterior p (x0:T |y1:T ) (Doucet et al., 2001). However in many applications, the parametric form of the state space model is unknown. While there exist
methods that perform smoothing to obtain an estimate of x0:T (Barber, 2006; Fox et al., 2009; Djuric & Bugallo, 2006), we are often interested in not only obtaining an estimate of the continuous latent sates but also in learning the dynamics f (·; ) that govern the dynamics of the system. This is known
as the dual estimation problem Haykin (2001).

In the simplest case, we can take a parametric approach to solving this dual estimation problems. When f (·; ) and g(·; ) are assumed to be linear functions, the posterior distribution over latent states is available in closed form and the parameters can be learned via expectation maximization. On the other hand, we have nonparametric methods that use Gaussian processes and neural networks to learn highly nonlinear dynamics and observations (Zhao & Park, 2016; 2017a; Frigola et al., 2014; Sussillo et al., 2016). Switching linear dynamical systems (SLDS) (Ackerson & Fu, 1970; Chang & Athans, 1978; Hamilton, 1990; Ghahramani & Hinton, 1996; Murphy, 1998) balance between these two extremes, approximating the dynamics by stochastically transitioning between a small number of linear regimes.

2.1 SWITCHING LINEAR DYNAMICAL SYSTEMS

SLDS approximate nonlinear dynamics by switching between a discrete set of linear regimes. A discrete latent state zt  {1, . . . , K} determines the linear dynamics at time t,

xt = xt-1 + Azt xt-1 + bzt + wt, wt  N (0, Qzt )

(5)

where Ak, Qk  Rdx×dx and bk  Rdx for k = 1, . . . , K. Typically, zt is endowed with Markovian dynamics, Pr(zt|zt-1 = k) = k. The conditionally linear dynamics allow for fast and efficient

learning of the model and can utilize the learning tools developed for linear systems (Haykin, 2001).

While SLDS can estimate the continuous latent states x0:T , the assumption of Markovian dynamics

for the discrete latent states severely limits their generative capacity.

2.2 RECURRENT SWITCHING LINEAR DYNAMICAL SYSTEMS
Recurrent switching linear dynamical systems (rSLDS) (Linderman et al., 2017), also known as augmented SLDS (Barber, 2006), are an extension of SLDS where the transition density of the

2

Under review as a conference paper at ICLR 2019

(sequential) stick breaking

tree-structured stick breaking

Figure 1: State probability allocation through stick-breaking in standard rSLDS and the TrSLDS.

discrete latent state depends on the previous location in latent space

zt|xt-1, {R, r}  SB (t) , t = Rxt-1 + r,

(6) (7)

where R  RK-1×dx is a matrix of hyper-planes and r  RK-1 is a bias vector. SB : RK-1  [0, 1]K maps from the reals to the simplex via stick-breaking:

SB() = S(1B) (), · · · , S(KB)() , S(kB) = (k)  (-j ) ,
j<k

(8)

for k = 1, . . . , K -1 and S(KB) =

K -1 k=1



(-k

)

where

k

is

the

kth

component

of

of

;

Fig.

1

illustrates the stick-breaking procedure . By including this recurrence in the transition density of zt,

the rSLDS partition the latent space into K pieces, where each piece follows its own linear dynamics.

It is through this combination of locally linear dynamical systems that the rSLDS approximates

equation 3; the partitioning of the space allows for a more interpretable visualization of the underlying

dynamics.

Recurrent SLDS can be learned efficiently and in a fully Bayesian manner, and experiments empirically show that they are adept in modeling the underlying generative process in many cases. However, the stick breaking process used to partition the space poses problems for inference due to its dependence on the permutation of the discrete states {1, · · · , K} (Linderman et al., 2017).

3 TREE-STRUCUTRED RECURRENT SWITCHING LINEAR DYNAMICAL SYSTEMS
Building upon the rSLDS, we propose the tree-structured recurrent switching linear dynamical system (TrSLDS). Rather than sequentially partitioning the latent space using stick breaking, we use a tree-structured stick breaking (Adams et al., 2010) procedure to partition the space.
Let T denote a tree structure with a finite set of nodes { , 1, · · · , N }. Each node n has a parent node denoted by par(n) with the exception of the root node, , which has no parent. For simplicity, we restrict our scope to balanced binary trees where every internal node n is the parent of two children, left(n) and right(n). Let child(n) = {left(n), right(n)} denote the set of children for internal node n. Let Z  T denote the set of leaf nodes, which have no children. Let depth(n) denote the depth of a node n in the tree, with depth( ) = 0.
At time instant t, the discrete latent state zt is chosen by starting at the root node and traversing down the tree until one of the K leaf nodes are reached. The traversal is done through a sequence of left/right choices by the internal nodes. Unlike in standard regression trees where the choices are deterministic Lakshminarayanan (2016), we model the choices as random variables. We can think of this as a stick breaking process. We start at the root node with a unit-length stick  = 1, which we divide between its two children. The left child receives a fraction left( ) = ( ) and the right child receives the remainder right( ) = 1 - ( ), where () = (1 + e- )-1 is the logistic function. The parameter   R specifies the left/right balance. This process is repeated recursively, subdividing n into two pieces at each internal node until we reach the leaves of the tree. The stick

3

Under review as a conference paper at ICLR 2019

assigned to each leaf node is then,

n =

(par(n))I[n=left(par(n))] 1 - (par(n)) I[n=right(par(n))] par(n) 1

n= , n= .

(9)

We incorporate this into the TrSLDS by allowing n to be a function of the continuous latent state,

n(xt-1, Rn, rn) = Rnxt-1 + rn.

(10)

The parameters Rn and rn specify a linear hyperplane in the continuous latent state space. As the continuous latent state xt-1 evolves, the left/right choices become more or less probable. This in turn changes the probability distribution k(xt-1, , T ) over the K leaf nodes, where  = {Rn, rn}nT .
In the TrSLDS, these leaf nodes correspond to the discrete latent states of the model, so that

p (zt = k | xt-1, , T ) = k(xt-1, , T ).

(11)

Fig. 1 illustrates the tree-structured stick breaking procedure.

3.1 A HIERARCHICAL DYNAMICS PRIOR THAT RESPECTS THE TREE STRUCTURE

Similar to standard rSLDS, the dynamics are conditionally linear given a leaf node zt. It is intuitive to expect that nearby regions in latent space have similar dynamics. In the context of the tree-structured stick breaking partitions that share a common parent should have similar dynamics. We explicitly model this by enforcing a hierarchical tree-structured prior on the dynamics.
Let {An, bn} be the dynamics parameters associated with node n. Even though only the discrete states are associated with the leaf nodes, we will introduce dynamics at the internal nodes as well. These internal dynamics serve as a link between the leaf node dynamics via a hierarchical prior,

vec([An, bn])| vec([Apar(n), bpar(n)])  N (vec([Apar(n), bpar(n)]), n),

(12)

where vec(·) is the vectorization operator. The prior on the root node is

vec ([A , b ])  N (0,  ) .

(13)

We impose the following constraint on the covariance matrix of the prior

n = depth(n) ,

(14)

where   (0, 1) is a hyper parameter that dictates how "close" a parent and child are to one another. The prior over the parameters can be written as, where the affine term and the vec(·) operator are
dropped for compactness,

p({An}nT ) = p(A )

p(Ai|A )

p(Aj |Ai) . . . p(Az|Apar(z)).

ichild( )

jchild(i)

zZ

(15)

It is through this hierarchical tree structured prior that allows TrSLDS to obtain a multi-scale view of the system. Parents are given the task of learning a higher level description of the dynamics while children are tasked with learning the nuances of the dynamics. The use of hierarchical priors also allows for neighboring sections of latent space to share common underlying dynamics inherited from their parent. TrSLDS can be queried at different levels, where levels deeper in the tree provide more resolution.
TrSLDS shares some features with regression trees (Lakshminarayanan, 2016), even though regression trees are primarily used for standard, static regression problems. The biggest difference is that our tree-structured model has stochastic choices. Moreover, the internal nodes of regression trees have no influence on equation 5; the hierarchical structure is only used for partitioning the latent space.
In the next section we show an alternate view of TrSLDS which we will refer to as the residual model in which internal nodes do contribute to the dynamics. Nevertheless, this "residual model" will turn out to be equivalent to the TrSLDS.

4

Under review as a conference paper at ICLR 2019

3.2 RESIDUAL MODEL

Let {A~n, ~bn} be the linear dynamics of node n. and let path(n) = ( , . . . , n) be the sequence of nodes visited to arrive at node n. In contrast to TrSLDS, the dynamics for a leaf node are now
determined by all the nodes in the tree

p(xt|xt-1, ~ , zt) = N (xt|xt-1 + A¯zt xt-1 + ¯bzt , Q~zt ),

(16)

A¯zt =

A~j , ¯bzt =

~bj ,

j path(zt )

j path(zt )

(17)

We model the dynamics to be independent a priori, where once again the vec(·) operator and the affine term aren't shown for compactness,

p({A~n}nT ) = p(A~n),
nT
and ~ n = ~depth(n)~ where ~  (0, 1).

p(A~n) = N (0, ~ n),

(18)

The residual model offers a different perspective of TrSLDS. The covariance matrix can be seen as representing how much of the dynamics a node is tasked with learning. The root node is given the broadest prior because it is present in equation 17 for all leaf nodes; thus it is given the task of learning the global dynamics. Nodes deeper in the tree become more associated with certain regions of the space, so they are tasked with learning more localized dynamics which is represented by the prior being more sharply centered on 0. The model ultimately learns a multi-scale view of the dynamics where the root node captures a coarse estimate of the system while lower nodes learn a much finer grained picture.

3.3 EQUIVALENCE OF TRSLDS AND RESIDUAL MODEL

We show the equivalence of TrSLDS and residual model yield the same joint distribution.
Theorem 1. TrSLDS and the residual model are equivalent if the following conditions are true: A = A~ , An = jpath(n) A~j, Qz = Q~z z  leaves(T ),  = ~ and  = ~

Proof. Let T be a balanced binary tree with K leaf nodes. To show that the models are equal, it
suffices to show the equivalence of the likelihood and the prior between models. For compactness, we drop the affine term and the vec(·) operator. The likelihood of TrSLDS is

T

p(x1:T |z1:T , ) = N (xt|xt-1 + Azt xt-1, Qzt ),
t=1

(19)

and the likelihood of the residual model is

T
p(x1:T |z1:T , ~ ) = N xt|xt-1 + A¯zt xt-1, Q~zt .
t=1

(20)

where A¯zt is defined in equation 17. Substituting Azt = jpath(zt) A~j into equation 20 equates the likelihoods. All that is left to do is to show the equality of the priors.

We can express An = jpath(n) A~j recursively

An = A~n + Apar(n).

Plugging equation 21 into ln p(An|Apar(n))

1 ln p(An|Apar(n)) = - 2

An - Apar(n) T n-1

An - Apar(n)

+C

= -1 2

T
A~n + Apar(n) - Apar(n) n-1

A~n + Apar(n) - Apar(n)

=

-

1 2

A~nT

-n 1A~n

+

C

=

-

1 2

A~nT

depth(n)

-1 A~n + C

(21)
(22) + C (23)
(24) (25)

5

Under review as a conference paper at ICLR 2019

because  = ~ and  = ~, equation 25 is equivalent to the kernel of p(A~n) implying that the priors are equal. Since this is true n  T , the joint distributions of the two models are the same.

4 BAYESIAN INFERENCE

The linear dynamic matrices , the hyper-planes  = {Rn, rn}nT \Z , the emission parameters  , the continuous latent states x0:T and the discrete latent states z1:T must be inferred from the data.
Under the Bayesian framework, this is achieved by computing the posterior

p (x0:T , z0:T , , , |y1:T )

=

p (x0:T , z1:T , , , , y1:T ) , p (y1:T )

(26)

where Z = p(y1:T ), the marginal likelihood, is

p(y1:T ) =

p (x0:T , z1:T , , , , y1:T ) x0:T    .

z1:T

(27)

Obtaining the posterior (26) requires the evaluation of (27) which is usually intractable in practice. We perform fully Bayesian inference via Gibbs sampling (Brooks et al., 2011) the sample from the posterior distribution equation 26. The structure of the model allows for closed form conditional posterior distributions that are easy to sample from. For clarity, the conditional distributions for the TrSLDS are given below:

1. The linear dynamic parameters (Ak, bk) and state variance Qk of a leaf node k are conjugate with a Matrix Normal Inverse Wishart (MNIW) prior
T
p((Ak, bk), Qk|x0:T , z1:T )  p((Az, bz), Qz) N (xt|xt-1+Azt xt-1+bzt , Qzt )1[zt=k].
t=1
2. The linear dynamic parameters of an internal node n are conditionally Gaussian

p((An, bn)|-n)  p((An, bn)|(Apar(n), bpar(n)))

p((Aj, bj)|(An, bn)).

jchild(n)

3. If we assume the observation model is linear and with Gaussian noise then emission parameters  = {(C, d), S} are also conjugate with a MNIW prior

T
p((C, d), S|x1:T , y1:T )  p((C, d), S) N (yt|Cxt + d, S).
t=1

4. The choice parameters are logistic regressions which follow from the conditional

T

p (|x0:T , z1:T )  p () p (zt|xt-1, )

t=1

T

= p ()

Bern j| RpTar(j)xt-1 + rpar(j)

t=1 jpath(zt)\

.

Each of these Bernoulli probabilities is amenable to Pólya-gamma augmentation Linderman et al. (2015), Polson et al. (2013). Let tn be the auxiliary Pólya-gamma random variables introduced at time t for an internal node n. We can express the posterior over the hyper-
planer for an internal node n as:

T
p((Rn, rn)|x0:T , z1:T , 1n:T )  p((Rn, rn)) N (tn|tn/tn, 1/tn)1(npath(zt)) (28)
t=1

where tn

=

RnT xt-1 + rn

and tn

=

1[j

=

left child(n)] -

1 2

1[j

=

right child(n)],

j  child(n). Placing a Gaussian prior makes the posterior conditionally conjugate.

6

Under review as a conference paper at ICLR 2019

5. Conditioned on the discrete latent states, the continuous latent states are conditionally
Gaussian. However, the presence of the tree-structured recurrence potentials (xt-1, zt) introduced through equation 11 destroys the Gaussinity of the conditional. When the model is augmented with PG random variables tn, the augmented recurrence potential ,(xt-1, zt, tn), becomes effectively Gaussian, allowing for the use of message passing for efficient sampling. Linderman et al. (2017) show how to perform message-passing using the
Pólya-gamma augmented recurrence potentials (xt, zt, wt). In the interest of space, we show the details in the appendix.

6. The discrete latent variables z1:T are conditionally independent given x1:T thus

p (zt = k|x1:T , , ) =

p (xt|xt-1, k) p (zt lleaves(T ) p (xt|xt-1, l

= k|xt-1, ) ) p (zt = l|xt-1,

)

,

k  leaves(T ).

7. The posteriors of the Pólya-Gamma random variables are also Pólya-Gamma: tn|zt, n, xt-1  P G(1, tn).
Due to the complexity of the model, good initialization is critical for the Gibbs sampler to converge in a reasonable amount of iterations. Details of the initialization procedure are contained in the appendix.

5 EXPERIMENTS
We demonstrate the potential of the proposed model by testing them on a number of non-linear dynamical systems. The first, FitzHugh-Nagumo, is a common nonlinear system utilized throughout the neuroscience to describe an action potential. We show that the proposed method can offer different angles of the system. We also compare our model with other approaches and show that we can achieve state of the art performance. We then move on to a Lorenz attractor, a chaotic nonlinear dynamical system and show that the proposed model can once again break down the dynamics and offer an interesting perspective. Finally, we apply the proposed method on the data from [cite graf]

5.1 FITZHUGH-NAGUMO
The FitzHugh-Nagumo (FHN) model is a 2-dimensional reduction of the Hodgkin-Huxley model which is completely described by the following system of differential equations:

v

=

v

-

v3 3

-

w

+

Iext,

 w = v + a - bw.

(29) (30)

We set the parameters to a = 0.7, b = 0.8,  = 12.5 and Iext  N (0.7, 0.04). We trained our model with 100 trajectories where the starting points were sampled uniformly from a (-3, 3) × (-3, 3) cube. Each of the trajectories were of length of 430, where the last 30 data points of the trajectories were used for testingn. he results are displayed in figure 2.
7

Under review as a conference paper at ICLR 2019

A true latent states D root node vector eld

G generated trajectory from leaf level

B inferred latent states E 2nd layer vector eld

H

true 2nd level leaf level

C

true vector eld

nullcline

F

leaf layer vector eld

I

time
k-step prediction performance

log speed

prediction horizon (k-steps)

Figure 2: TrSLDS applied to model the FitzHugh-Nagumo nonlinear oscillator. (a) The model was trained with 100 trajectories with different starting points. (b)The model can infer the latent trajectories, up to an affine transformation. (c) The true vector field of FHN is shown where color represents log-speed. The two nullclines are plotted in yellow and green. (d) & (e) & (f) The vector fields display the multi-scale view learned from the model. As we go deeper in the tree, the resolution increases as well which is evident from the vector fields.. (g) A deterministic trajectory from the leaf nodes, projected onto a trajectory FHN for clarity. (h) Plotting w and v over time, we see that the second level captures some of the oscillations but ultimately converges to a fixed point. The model learned by the leaf nodes provides a much better approximation. (i) It is evident from the plot that TrSLDS can accurately perform multi-step prediction of the true trajectory from FHN.

To quantitatively measure the predictive power of TrSLDS, we compute the k-step mean squred error, MSEk, and it's normalized version, Rk2, on a test set where MSEk and Rk2 are defined as

1 T -k MSEk = T - k

yt+k - y^t+k

2 2

,

t=0

Rk2 = 1 -

(T - k)MSEk

T -k t=0

yt+k - y¯

2 2

(31)

where y¯ is the average of a trial and y^t+k is the prediction at time t + k which is obtained by (i) using the the samples produced by the sampler to obtain an estimate of x^T (ii) propagate x^T for k time steps forward to obtain x^t+k and then (iii) obtain y^t+k according to equation 3. We compare the model to LDS. SLDS and rSLDS for k = 1, . . . 30 over the last 30 time steps for all 100 trajectories.
Figure 2i displays the comparisons.

5.2 LORENZ ATTRACTOR
Lorenz attractors are chaotic systems whose nonlinear dynamics are defined by the following differential equations
x1 =  (x2 - x1) , x2 = x1( - x3) - x2,
x3 = x1x2 - x3.
The parameters of the Lorenz were set to  = 10,  = 28 and  = 8/3. The data consisted of 50 trajectories, each of length of 230 where the first 200 points are used for training and the last 30 are

8

Under review as a conference paper at ICLR 2019

used for testing. We fit the data using 4 states which corresponds to a tree of depth 3. The results are shown in Figure 3.

A

C

2E

simulated trajectories

true

BD

leaf level

2nd level
F

time k-step prediction performance

prediction horizon (k-steps)
Figure 3: (a) The 50 trajectories used to train the model are plotted where the red "x" displays the starting point of the trajectory.(b) The inferred latent states are shown, colored by their discrete latent state. (c) We see that the second layer approximates the Lorenz with 2 ellipsoids. A Lorenz starting at the same initial point is shown for comparison. (d) Going one level lower in the tree, we see that in order to capture the nuances of the dynamics, each of the ellipsoids must be split in half. A trajectory from the Lorenz is shown for comparison. (e) Plotting the dynamics, it is evident that the leaf nodes improve on it's parent's approximation. (f) The Rk2 shows the predictive power of TrSLDS.
The butterfly shape of the Lorenz lends itself to being roughly approximated by two ellipsoids as a rough estimate of the dynamics; this is exactly what TrSLDS learns in the second level of the tree. As is evident from Figure 2b, the two ellipsoids don't capture the nuances of the dynamics. Thus, the model partitions each of the ellipsoid to obtain a finer description. We can see that embedding the system with a hierarchical tree-structured prior allows for the children to build off it's parents approximations.
5.3 NEURAL DATA
To validate the model and inference procedure, we used the neural spike train data recorded from the primary visual cortex of an anesthetized macaque monkey collected by Graf et al. (2011). The dataset is composed of short trials where the monkey viewed periodic temporal pattern of motions of 72 orientations, each repeated 50 times. Previous state space modeling of the dataset showed that for each orientation of the drifting grating stimulus, the neural response oscillates over time, but in a stimulus dependent geometry (Zhao & Park, 2017b). We used 25 trials each from a subset of 4 stimulus orientations grouped in two (140 and 150 degrees vs 230 and 240 degrees). Each trial contained 140 neurons, and their spike trains were binarized with a 10 ms window. We truncated the onset and offset neural responses, resulting in 111 time bins per trial.
The TrSLDS with 3-dimensional latent state and 4 leaf nodes converged to Fig. 4. We observe that the population-wide modulation is captured by temporal oscillations in the form of rings in the state space. Furthermore, the discrete states segment the oscillations into different phases. This is similar to the nonlinear limit cycle case of FHN, but in this case, the nonlinear dynamics is probably due to
9

Under review as a conference paper at ICLR 2019

the biased subsampling of the electrode array that preferentially responds to certain phases of the oscillation.

Spike Train Observation

Average Inferred Latent Trajectory

orientation: 230-240 deg orientation:140-150 deg

ori=140 ms ori=150

ori=230 ms ori=240

200 ms

Figure 4: Inference results from neural data. (left) Spike raster plot showing two single trials in response to drifting gratings of two different orientations. (middle) Inferred 3-dim latent trajectory averaged over the 25 repeated trials, displayed over time. Color corresponds to discrete states. Yellow and green discrete states share their parent node. (right) Same as the middle, except viewed in the state space. Note the two ring structures corresponding to the two orientation groups.

6 CONCLUSION AND FUTURE WORK
In this paper, we propose tree-structured recurrent switching linear dynamical systems (TrSLDS) which is an extension of Linderman et al. (2017) rSLDS. The tree-structured stick breaking removes the dependence on the permutation of the discrete latent states. The tree-structured stick breaking paradigm naturally lends itself to imposing a tree-structured hierarchical prior on the dynamics. The structure of the prior allows for a multi-scale view of the system; one can query at different levels of the trees to see different scales of the resolution. We also developed a fully Bayesian approach to learning the parameters of the model. The analysis of the Graf data suggests that the method can also be used to analyze neural data.
REFERENCES
Guy A Ackerson and King-Sun Fu. On state estimation in switching environments. IEEE Transactions on Automatic Control, 15(1):10­17, 1970.
Ryan P Adams, Zoubin Ghahramani, and Michael I Jordan. Tree-Structured Stick Breaking for Hierarchical Data. In J D Lafferty, C K I Williams, J Shawe-Taylor, R S Zemel, and A Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 19­27. Curran Associates, Inc., 2010.
David Barber. Expectation Correction for Smoothed Inference in Switching Linear Dynamical Systems. Technical report, 2006.
David Barber, A Taylan Cemgil, and Silvia Chiappa. Bayesian time series models. Cambridge University Press, 2011.
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov Chain Monte Carlo. CRC press, 2011.
10

Under review as a conference paper at ICLR 2019
Chaw-Bing Chang and Michael Athans. State estimation for discrete systems with switching parameters. IEEE Transactions on Aerospace and Electronic Systems, (3):418­425, 1978.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pp. 2980­2988, 2015.
Petar Djuric and Mónica Bugallo. Cost-Reference Particle Filtering for Dynamic Systems with Nonlinear and Conditionally Linear States, 9 2006.
Arnaud Doucet, Nando Freitas, and Neil Gordon. An Introduction to Sequential Monte Carlo Methods. In Sequential Monte Carlo Methods in Practice, pp. 3­14. Springer New York, New York, NY, 2001. doi: 10.1007/978-1-4757-3437-9{\_}1.
Elena A. Erosheva and S. McKay Curtis. Dealing with Reflection Invariance in Bayesian Factor Analysis. Psychometrika, 82(2):295­307, 6 2017. ISSN 0033-3123. doi: 10.1007/s11336-017-9564-y.
Emily Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky. Nonparametric Bayesian Learning of Switching Linear Dynamical Systems. In D Koller, D Schuurmans, Y Bengio, and L Bottou (eds.), Advances in Neural Information Processing Systems 21, pp. 457­464. Curran Associates, Inc., 2009.
Roger Frigola, Yutian Chen, and Carl Edward Rasmussen. Variational Gaussian Process State-Space Models. In Z Ghahramani, M Welling, C Cortes, N D Lawrence, and K Q Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 3680­3688. Curran Associates, Inc., 2014.
Yuanjun Gao, Evan W Archer, Liam Paninski, and John P Cunningham. Linear dynamical neural population models through nonlinear embeddings. In Advances in neural information processing systems, pp. 163­171, 2016.
John Geweke and Guofu Zhou. Measuring the Pricing Error of the Arbitrage Pricing Theory. Review of Financial Studies, 9(2):557­587, 4 1996. ISSN 0893-9454. doi: 10.1093/rfs/9.2.557.
Zoubin Ghahramani and Geoffrey E Hinton. Switching state-space models. Technical report, University of Toronto, 1996.
Arnulf B. Graf, Adam Kohn, Mehrdad Jazayeri, and J. Anthony Movshon. Decoding the activity of neuronal populations in macaque primary visual cortex. Nature neuroscience, 14(2):239­245, February 2011. ISSN 1546-1726. doi: 10.1038/nn.2733.
James D Hamilton. Analysis of time series subject to changes in regime. Journal of econometrics, 45 (1):39­70, 1990.
Simon S Haykin. Kalman Filtering and Neural Networks. John Wiley &amp; Sons, Inc., New York, NY, USA, 2001. ISBN 0471369985.
Matthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. Composing graphical models with neural networks for structured representations and fast inference. In Advances in neural information processing systems, pp. 2946­2954, 2016.
Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state space models. 2017.
Balaji Lakshminarayanan. Decision Trees and Forests: A Probabilistic Perspective. Technical report, UCL (University College London), 2016.
Scott Linderman, Matthew Johnson, and Ryan P Adams. Dependent Multinomial Models Made Easy: Stick-Breaking with the Polya-gamma Augmentation. In C Cortes, N D Lawrence, D D Lee, M Sugiyama, and R Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 3456­3464. Curran Associates, Inc., 2015.
11

Under review as a conference paper at ICLR 2019

Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 914­922, Fort Lauderdale, FL, USA, 9 2017. PMLR.
Kevin P Murphy. Switching Kalman filters. Technical report, Compaq Cambridge Research, 1998.
Chethan Pandarinath, Daniel J O'Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky, Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg, et al. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature methods, pp. 1, 2018.
Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian Inference for Logistic Models Using Pólya­Gamma Latent Variables. Journal of the American Statistical Association, 108(504): 1339­1349, 2013. doi: 10.1080/01621459.2013.829001.
Simo Särkkä. Bayesian filtering and smoothing, volume 3. Cambridge University Press, 2013.
David Sussillo, Rafal Józefowicz, L. F Abbott, and Chethan Pandarinath. LFADS - Latent Factor Analysis via Dynamical Systems. CoRR, abs/1608.06315, 2016.
Yuan Zhao and Il Memming Park. Interpretable nonlinear dynamic modeling of neural trajectories. In Advances in Neural Information Processing Systems (NIPS), 2016.
Yuan Zhao and Il Memming Park. Variational recursive dual filtering. (under review), July 2017a.
Yuan Zhao and Il Memming Park. Variational Latent Gaussian Process for Recovering SingleTrial Dynamics from Population Spike Trains. Neural Computation, 29(5), May 2017b. doi: 10.1162/NECO_a_00953.

A INITIALIZATION

We initialized the Gibbs sampler using the following initialization procedure: (i) probabilistic PCA

was performed on the data, y1:T to initialize the emission parameters, {C, d} and the continuous latent states, x1:T . (ii) To initialize the dynamics of the nodes ,, and the hyperplanes, , we propose

greedily fitting the proposed model using MSE as the loss function. We first optimize over the root

node

1T

arg min

A ,b

T
t=0

xt+1 - xt - A xt - b

2 2

,

(32)

and obtain A, b (Note that A, b can obtained in closed form by computing their corresponding OLS estimates). Fixing A and b, we then optimize over the second level in the tree

1T

arg min
A1,b1,A2,b2,R ,r

T
t=0

xt+1 - (v )x^1 - (-v )x^2

x^i = xt + (A + Ai) xt + (b + bi) ,

v = RT xt + r .

,

(33)
(34) (35)

This procedure would continue until we reach the leaf nodes of the tree.  and  are then used to initialize the dynamics and the hyperplanes, respectively. In our simulations, we used stochastic gradient descent with momentum to perform the optimization. (iii) The discrete latent states, z1:T , were initialized by performing hard classification using  and the initial estimate of x0:T .

B DEALING WITH ROTATIONAL INVARIANCE
A well known problem with these types of model is it's susceptibility to rotational and scaling transformation, thus we can only learn the dynamics up to an affine transformation Erosheva &

12

Under review as a conference paper at ICLR 2019 Curtis (2017). During Gibbs sampling the parameters will continuously rotate and scale, which slows down the mixing of the chains. One possible solution to the issue is if we constrained C to have some special structure which would make the model identifiable; this would require sampling from manifolds which is usually inefficient. Similar to Geweke & Zhou (1996), we use the following procedure to prevent the samples from continuously rotating and scaling:
· Once we obtain a sample from the conditional posterior of the emission parameters {C, D}, we normalize the columns of C.
· RQ decomposition is performed on C to obtain U, O where U  Rdy×dx is an upper triangular matrix and O  Rdx×dx is an orthogonal matrix.
· We set C = U and rotate all the parameters of the model using O.
13

