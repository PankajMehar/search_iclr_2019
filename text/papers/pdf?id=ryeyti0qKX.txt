Under review as a conference paper at ICLR 2019
ON THE STATISTICAL AND INFORMATION THEORETICAL CHARACTERISTICS OF DNN REPRESENTATIONS
Anonymous authors Paper under double-blind review
ABSTRACT
It has been common to argue or imply that a regularizer can be used to alter a statistical property of a hidden layer's representation and thus improve generalization or performance of deep networks. For instance, dropout has been known to improve performance by reducing co-adaptation, and representational sparsity has been argued as a good characteristic because many data-generation processes have only a small number of factors that are independent. In this work, we analytically and empirically investigate the popular characteristics of learned representations, including correlation, sparsity, dead unit, rank, and mutual information, and disprove many of the conventional wisdom. We first show that infinitely many Identical Output Networks (IONs) can be constructed for any deep network with a linear layer, where any invertible affine transformation can be applied to alter the layer's representation characteristics. The existence of ION proves that the correlation characteristics of representation can be either low or high for a well-performing network. Extensions to ReLU layers are provided, too. Then, we consider sparsity, dead unit, and rank to show that only loose relationships exist among the three characteristics. It is shown that a higher sparsity or additional dead units do not imply a better or worse performance when the rank of representation is fixed. We also develop a rank regularizer and show that neither representation sparsity nor lower rank is helpful for improving performance even when the data-generation process has only a small number of independent factors. Mutual information I(zl; x) and I(zl; y) are investigated as well, and we show that regularizers can affect I(zl; x) and thus indirectly influence the performance. Finally, we explain how a rich set of regularizers can be used as a powerful tool for performance tuning.
1 INTRODUCTION
A learned representation can significantly affect the performance of deep networks, and the representation's distributed and deep natures are the essential elements for the success of deep learning (Bengio et al., 2013). As a consequence, deep networks have a greater expressiveness compared to the other machine learning algorithms (Hinton et al., 1986) or shallow networks (Montufar et al., 2014; Telgarsky, 2015; Eldan & Shamir, 2016; Raghu et al., 2016). Besides the distributed and deep natures that have been intensively studied, a hidden layer's representation characteristics are considered to be important as well. Nonetheless, a relatively smaller number of studies have been completed on the topic, and the goal of this work is to understand the representation characteristics better. Therefore, the meaning of representation in this work is restricted to the activation vector of a single hidden layer and a unit refers to a neuron of the hidden layer.
A few previous studies considered manipulating statistical characteristics of representations such as reducing covariance among hidden units (Cogswell et al., 2015; Xiong et al., 2016), encouraging representational sparsity (Glorot et al., 2011), or forcing parsimonious representations via clustering (Liao et al., 2016). In some of the similar works, a popular argument has been that the representation regularization reduces the generalization error via altering a representation characteristic. This argument, however, has not been rigorously studied. Another popular argument has been the reduction of effective capacity via regularization. This argument has been recently disproved by Zhang et al. (2016) where they empirically show that explicit regularization methods like L2 weight decay and dropout cannot sufficiently limit the effective capacity of deep networks.
1

Under review as a conference paper at ICLR 2019

Table 1: Representation characteristics.

Characteristic

Symbol

Expression

ACTIVATION AMPLITUDE |z¯| Ei[| zl,i |]

COVARIANCE

c¯ Ei=j [ci,j ], where ci,j {Cl}i,j = E[(zl,i -µzl,i )(zl,j -µzl,i )]

CORRELATION SPARSITY DEAD UNIT
RANK MUTUAL INFORMATION

¯ Ei=j [i,j ], where i,j {Cl}i,j /zl,i zl,j = E[(zl,i -µzl,i )(zl,j -µzl,i )]/zl,i zl,j

Ps Ei,n[1(zln,i)], where 1 is an indicator function whose output is 1 only when zln,i = 0

Pd Ei[1(zl,i)], where 1 is an indicator function whose output is 1 only when zln,i = 0 for all n = 1, .., N

r

rank(Cl); numerical evaluations are approximated as the stable rank

Cl

2 F

/

Cl

2 2

Ix I(zl; x)

MUTUAL INFORMATION Iy I(zl; y)

Figure 1: Visualization of the learned representations for a 6-layer MLP trained with MNIST dataset. A single unit's activation histogram (upper plots) and two randomly chosen units' activation scatter plots (lower plots) are shown for the fifth layer's representation, where each color corresponds to a different class. The plots were generated using 10,000 test samples of MNIST dataset. (Upper) It can be seen that the baseline has a large classwise variance and inter-class overlaps, and BN and CR (originally known as DeCov (Cogswell et al., 2015)) show similar properties. Dropout looks completely different where activation values are more spread out for the active classes. L1R (L1 Representation regularizer) typically allow only one or two classes to be activated per unit. (Lower) While the baseline shows modest linearity, the others show quite different representation characteristics depending on the choice of regularizer. Dropout shows an extremely high class-wise correlation, but BN shows very low correlation. CR shows almost no correlation. Since L1R increases sparsity on representation, a class is activated over at most one of the two randomly chosen units.
Since a novel information-theoretic analysis method was proposed for deep learning (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017), information theoretic characteristics of representation have become an important research topic. In their work, mutual information I(zl; x) and I(zl; y) are used to address the learning dynamics and generalization of deep learning, where zl is the hidden layer l's representation, x is the input, and y is the output. It is further discussed that a good representation is the one that contains a minimal amount of information from the input while containing a sufficient amount of information from the output. In Achille & Soatto (2017), the Information Bottleneck Lagrangian (Tishby et al., 2000) is decomposed into the sum of a crossentropy term and a regularization term. The regularization term turns out to be I(zl; x) that needs to be minimized. Some of the recent works will be additionally addressed in Section 5.
2 REPRESENTATION CHARACTERISTICS
In this section, we briefly address the most popular statistical characteristics and information theoretic characteristics of representations. Consider a neural network NA whose architecture A is fixed and the weights for the lth layer are given by {Wl} and {bl} after training. Notation-wise, we simply write NA = (W, b) to define a network and y or NA(x) to refer to its deterministic output for a given input x. The layer index l is omitted when the meaning is obvious. The lth layer's activation vector for the given input x is noted as zl(x) or simply zl, and the ith element of zl is noted as zl,i. The mean, variance, and standard deviation of zl,i are defined as µzl,i , vzl,i , and zl,i , respectively. The covariance of zl is defined as Cl. Then, the basic representation characteristics can be summarized as in Table 1. Six of them are statistical characteristics, and the last two are information theoretic characteristics.
2

Under review as a conference paper at ICLR 2019

Previous studies on statistical characteristics are often based on regularizers. Srivastava et al. (2014) addresses dropout for preventing co-adaptation among hidden units by randomly putting zeros in a layer's activation vector. Ioffe & Szegedy (2015) explains batch normalization (BN) that reduces internal covariate shift via normalizing activations of each unit to speed up network training. Cogswell et al. (2015) suggest DeCov that utilizes a penalty loss function to reduce activation covariance among hidden units. Choi & Rhee (2018) considers extension to class-wise regularization and provides four representation regularizers. Among them, CR (Covariance Regularizer) is equivalent to DeCov, and we adopt the notation of CR. Glorot et al. (2011) explains L1 representation regularization, called L1R in this work, that applies L1 penalty on activations. These representation regularization methods have distinct effects on representation characteristics, and examples of the learned representations are shown in Figure 1. The representation regularizers are described in Appendix B, and a quantitative analysis of representation characteristics are provided in Appendix C.
Because the true distribution of data is not accessible, the numerical results in the following sections are evaluated using the empirical distribution of the test dataset. Then, the expectations in Table 1 are with respect to the empirical distribution. For instance, Cl is calculated as the covariance matrix of N activation vectors {zl1, ..., zlN } where zln corresponds to the activation vector for the n'th test data sample, xn. Rank can be calculated by examining Cl, but often there are tiny eigenvalues that hinder a proper assessment of the rank. Therefore, we evaluate stable rank instead, and it will be explained further in Section 4. Two information-theoretical characteristics, I(zl; x) and I(zl; y), are estimated using upper and lower bounds explained in Kolchinsky & Tracey (2017); Kolchinsky et al. (2017). Further details are provided in Section 5. ReLU is the only activation function that is considered in this work. When ReLU is used, ACTIVATION AMPLITUDE, COVARIANCE, and CORRELATION are calculated using only the positive activation values such that the numerical evaluations can provide meaningful insights on what is happening to the non-zero representation values.

3 SCALING, PERMUTATION, COVARIANCE, AND CORRELATION
After training is completed for a deep network NA, the output of the network becomes a deterministic function of the input x. Without an activation function, i.e. a linear layer, zl = WlT zl-1 + bl. When ReLU is applied to layer l, the activation vector becomes zl = ReLU (WTl zl-1 + bl) = max(WlT zl-1 + bl, 0). In this section, we investigate the most flexible affine transformation that can be applied to a layer's representation zl without influencing the output NA(x) for any arbitrary input vector x. While complicated transformations over multiple layers can be explored, we limit our focus to manipulating only the weights of layer l and layer l + 1 for the analytical tractability. Because scaling and permutation are well known results, covariance and correlation are the main focus of this section.

3.1 IDENTICAL OUTPUT NETWORK (ION)

We first consider a linear layer l. For a linear layer, it turns out that any affine transformation can be applied as long as the transformation does not cause an information loss.
Theorem 1. (ION for a linear layer) For a deep network NA = (W, b) whose layer l is linear, there exists NA = (W, b) that satisfy the following conditions:

 x, NA(x) = NA(x);  x, zl = Q(zl - m),

(1) (2)

where Q is any nonsingular square matrix of a proper size and m is any vector of a proper size.

The first condition says that the two networks generate identical outputs for any input x. The second condition says that zl can be affinely transformed using any nonsingular matrix Q. The proof is straightforward and can be found in Appendix A.
While simple, Theorem 1 has significant implications on the representation characteristics of zl. Let's inspect covariance and correlation (normalized version of covariance) first. If NA is a network that is globally optimal for a task and has at least one linear layer l, then NA's covariance Cl can be
whitened to have Cl = I by choosing m as the expected mean and Q as a whitening matrix. The

3

Under review as a conference paper at ICLR 2019

resulting network NA will have zero correlation between any pair of units in layer l, but will be globally optimal, too. In fact, there are infinitely many globally optimal networks with different covariance characteristics, and one can easily construct an ION with an arbitrary covariance matrix
Cl as long as its rank is the same as Cl's rank. With this result, it becomes unclear why one should pursue a lower correlation when training a deep network. Unless regularization for a low correlation somehow helps optimization to reach a better performing network, there seems to be no reason to pursue low (or high) correlation.
For dead neurons, a similar claim can be made. If globally optimal NA has no dead neurons in layer l and Cl is not full rank, one can make an affine transformation to align the null spaces of Cl to some of the neurons. Then, the resulting network NA will be still globally optimal, but with some dead neurons in layer l. For higher layers of classification tasks, typically the rank of Cl is close to the number of classes. For classification tasks with only 210 classes, it is possible to construct an ION that has as many dead neurons as the size of Cl's null space. This can be done without negatively affecting the performance, and the wisdom of `reduce the number of dead neurons' becomes dubious.
For scaling and permutation, their influences are rather insignificant. As for the scaling that can affect activation amplitude, it often has no effect on the network's performance. For instance, scaled activation amplitude can affect the probability of classification tasks when softmax is in the last layer, but the class with the highest probability remains the same anyway. When representation regularizers are used, often activation amplitude is squashed to reduce the cost of representation penalty function but the network can still perform well. As reported in Choi & Rhee (2018), such an activation squashing can make covariance much smaller, but the effect is removed when correlation is calculated. As for the permutation, it is considered to be meaningless because the index number itself is not important.
Before discussing further, a similar result is developed for ReLU layers. The resulting Q, however, is much more limited. The proof can be found in Appendix A as well.
Theorem 2. (ION for a ReLU layer) For a deep network NA = (W, b) whose activation function of layer l is ReLU, there exists NA = (W, b) that satisfy the following conditions:

 x, NA(x) = NA(x);  x, zl = Q zl,

(3) (4)

where Q is any permuted positive diagonal matrix of a proper size. Furthermore, it can be shown that any Q that satisfy the above two conditions must be a permuted positive diagonal matrix.

Using a permuted positive diagonal matrix Q, covariance can be affected by independently scaling activation amplitudes of layer l's units. As explained above, such a scaling is canceled out when calculating correlation and therefore a linear transformation cannot affect correlation while keeping the output identical. There are a few possibilities for overcoming the limitations of Theorem 2, and they are discussed in the following subsection.
For rank and mutual information, the invertible affine transformation has no effect. They are discussed in the following sections.

3.2 POSSIBLE EXTENSIONS OF ION
We discuss three possible extensions of ReLU's ION.
Insertion of a linear layer One way to overcome the limitations of Q is to insert an extra linear layer near the target ReLU layer and to consider its implications. When representation characteristics are analyzed or interpreted, researchers do not care much about the layer's activation function, regularization, etc. The activation vector's representation characteristics are the essential components for understanding and assessing the representations. Therefore, one can apply the insights from Theorem 1 when the extra linear layer shows similar statistical properties as the ReLU layer. In Table 2, statistical properties of immediately neighboring ReLU and linear layers are compared. Compared to the representation characteristics of the first ReLU layer, the 5th ReLU layer and the inserted 6th linear layer show very similar characteristics. Then the correlation and dead unit characteristics are

4

Under review as a conference paper at ICLR 2019

Table 2: Comparison of statistical characteristics for linear and ReLU layers. A 7-layer MLP was used with MNIST dataset, and only the sixth layer was linear, and the others were ReLU layers. Statistical characteristics of the first layer (ReLU), fifth layer (ReLU), and sixth layer (linear) are compared. It can be seen that representation characteristics of fifth and sixth are very similar because they are both located in the upper part of the network. Note that the characteristics of the sixth layer were calculated only using positive activation values for fair comparison.

Regularizer ACTIVATION AMPLITUDE COVARIANCE CORRELATION SPARSITY DEAD UNIT RANK

Baseline (1st)

1.16

0.08 0.14 0.00 0.00 2.22

Baseline (5th)

2.22

0.45 0.25 0.32 0.07 2.27

Baseline (6th)

2.75

0.62 0.25 0.47 0.00 2.78

Dropout (1st)

0.76

0.04 0.27 0.86 0.00 3.50

Dropout (5th)

2.19

1.23 0.70 0.53 0.00 2.19

Dropout (6th)

1.85

0.97 0.53 0.48 0.01 1.52

BN (1st) 0.80 0.03 0.10 0.49 0.00 4.07

BN (5th) 1.01 0.10 0.19 0.51 0.00 4.59

BN (6th) 1.39 0.19 0.23 0.48 0.00 4.26

not so important as the consequence of Theorem 1, and the same might be conjectured for the 5th ReLU layer.
Non-affine transformations over multiple layers In the ION derivations, we have considered only an affine transformation applied to the layers l and l + 1 only. If we remove the constraints and borrow the results from expressivity of DNN and universal approximation theorem, it might be possible to derive more powerful and general results. In the extreme case, one can divide a deep network NA into two parts: NAlower and N .Aupper Then, NAlower (x) = zl and NA(x) = NAupper (NAlower (x)).
In theory, there exist NAlower and NAupper that can result in NA(x) = NAupper (NAlower (x)) while
allowing zl to have a completely different characteristics compared to zl. Such NAlower and N ,Aupper however, might be infeasibly large or fail to learn in the way we desire. Therefore, it might be more practical to consider a reasonable extension of Theorem 1 and Theorem 2.
Comparable Performance Network (CPN) According to Theorem 2, only permuted positive diagonal matrices can form IONs. If we ignore the result and apply an affine transformation in the
same way as in the ION of a linear layer, the resulting network NA will not form an ION but instead we might be able to find a Comparable Performance Network (CPN) that achieves a comparable performance while showing different representation characteristics. We tried this brute-force method, and two sample results along with the baseline and a positive diagonal matrix case are shown in Table 3. In the first row where Q is identity, the values are for the original network NA. In the next row, `Random positive diagonal', uniformly random values between 0 and 1 (U (0, 1)) were used as the diagonal values. Note that this choice of Q satisfies Theorem 2, and therefore the error performance remains the same while affecting activation amplitude and covariance only. In the `Ones with random positive diagonal', Q was chosen as a matrix of ones with its diagonals replaced with random values chosen from U (0, 1). We randomly generated 100 of such random matrices and selected the one that resulted in a higher correlation while showing a comparable performance. Despite of the very
high correlation of 0.80, the selected network NA can perform comparably well. In the last row, we applied a whitening filter where Q and m were calculated while ignoring ReLU. The resulting network does not end up with zero correlation because the whitening is not perfect in the presence of ReLU. But the correlation is considerably reduced to 0.09 from 0.28 while achieving a slightly worse error rate of 5.48%.
To find the examples in Table 3, all we had to do was to construct a meaningful matrix Q or to try 100 random matrices and choose one. The fact that it is almost painless job to find a CPN also implies that the relevant representation characteristics might not be essential for achieving high performance.

4 SPARSITY, DEAD UNIT, AND RANK
Sparsity and dead unit have been considered as important representation characteristics. Rank of Cl, however, has received much less attention so far. In this section, we investigate the three and show that rank might be the most fundamental characteristics.
5

Under review as a conference paper at ICLR 2019

Table 3: Statistical characteristics of representations transformed by CPNs. The original network is 6-layer MLP on the MNIST dataset, and the 5th layer representations were transformed. To improve the performance, the weights to the output layer were fine tuned after applying Q.

Q matrix

Error (%) ACTIVATION AMPLITUDE COVARIANCE CORRELATION SPARSITY DEAD UNIT

Identity

2.54

6.79

4.29 0.28 0.36 0.13

Random positive diagonal Ones with
random positive diagonal
Whitening

2.54 2.76 5.48

3.37 158.88
1.22

1.04 723.55
0.96

0.28 0.36 0.13 0.80 0.00 0.00 0.09 0.49 0.02

4.1 ANALYTICAL RELATIONSHIP

In Table 1, sparsity is defined as Ps = Ei,n[1(zln,i)]. This can be interpreted as the probability of zln,i (unit i's activation for n'th test sample xn) being zero, because 1(zln,i) = 1 when zln,i = 0 and 1(zln,i) = 0 when zln,i = 0. Similarly, dead unit is defined as Pd = Ei[1(zl,i)] and it can be interpreted
as the probability of zl,i (unit i's activation) being always zero or at least for all M test samples.
Because 1(zl,i)  1(zln,i) for any pair of (i, n), Pd  Ps can be shown by taking expectations on
both sides. The rank r in Table 1 is defined as the rank of Cl. For layer l with M units, this means that r out of M linearly independent dimensions are used by the codewords {z1l , ..., zNl } and that the other M - r dimensions form a null space of Cl. When dead units are considered, M Pd units need to be constant zero by the definition of the dead unit and it implies that at least M Pd dimensions need to be included in the null space. Therefore, M Pd  M - r. These results can be summarized
as below.

Pd  Ps

(5)

M Pd  M - r

(6)

Between sparsity Ps and rank r, there is no clear relationship. The codeword znl for a test sample xn can be very sparse, and yet the set of codewords {zl1, ..., zlN } collectively might use all of the M dimensions. Conversely, rank r can be very small and yet Ps can also be very small when {zln,1, ..., zln,M } are strongly correlated and the basis vectors are not sparse over the M units.

From the viewpoint of signal processing or information theory, sparsity is a property that is related to individual signals or individual codewords while rank is a property that is related to the total number of dimensions used by the set of signals or the entire codebook. Therefore, sparsity is not directly responsible for the efficiency of the signals or codebook while rank is directly responsible for the efficiency. From the viewpoint of deep learning, rank can be associated with the maximum number of latent factors that are independent. As for the dead unit, we know from equation 6 that it is upper bounded as a function of rank. When the bound is met, the value of Pd is merely an artifact of how the representation vectors {zl} are aligned with the eigenvectors of Cl. If each neuron is aligned to an eigenmode of Cl, then Pd = 1 - r/M will be achieved. From our experience, however, such a perfect alignment never happens when using the backpropagation based learning process. This has been true even when L1R or other advanced representation regularizers were applied. ION, however, can easily meet the requirement for a linear layer.

Motivated by the above discussion, we have designed a rank regularizer and examined common wisdom that says `most of the data generation processes have a small number of independent factors and therefore increasing sparsity of representation can be helpful.' For instance, see Bengio et al. (2013). We first explain the design of rank regularizer.

4.2 RANK REGULARIZER

In deep learning, a low-rank approximation of convolutional filters (Jaderberg et al., 2014; Lebedev

et al., 2014; Tai et al., 2015) and weight matrices (Xue et al., 2013; 2014; Nakkiran et al., 2015;

Masana et al., 2017) has been widely used for network compression and fast network training. Some

of the works applied a singular value decomposition to weight matrices after network training ends

but not to representations. In this work, as L1 representation regularizer was designed to encourage

a higher sparsity by adding a penalty loss term L1R =

|zln,i|, Rank Regularizer (RR) is

ni

6

Under review as a conference paper at ICLR 2019

designed to encourage a lower rank of representations and used during network training. Because
the usual definition of rank can be very sensitive to the tiny singular values, we use stable rank of activation matrix Z = [zl1, . . . , zNl MB ]T as a surrogate. Note that NMB instead of N activation vectors are used for each mini-batch. Stable rank of Z is defined as

RR =

Z Z

2 F 2 2

=

i s2i maxi si2

,

(7)

where Z F is the Forbenius norm, Z 2 is the spectral norm, and {si} are the singular values of Z.

From

i s2i maxi s2i

,

it

can

be

clearly

seen

that

stable

rank

is

upper

bounded

by

the

usual

rank

that

counts

strictly positive singular values. Because the spectral norm is based on a singular value decomposition,

calculating stable rank's derivative for every mini-batch is a computationally heavy operation. To

reduce the computational burden, we introduce an approximation using a special case of Holder's

inequality.

RR =

Z

2 F

Z

2 2

trace(ZT Z)

=

Z

2 2

 trace(ZT Z) = Z 1 Z  (maxi

i,n(zin)2

NM B n=1

|zin|)(maxn

M i=1

|zin|)

(8) (9)

The inequality Z 2  Z 1 Z  was used where Z 1 is the maximum absolute column sum of the matrix Z (sum of all activation values of unit i) and Z  is the maximum absolute row sum of the matrix Z (sum of all activation values of sample n). Then the gradient of approximation RR can be as below.

RR zin



Z

2 F

zin -

Z1Z

Z

2 F

·

(

Z 1  zin

·Z

Z

2 1

+ Z

Z

2 

1·



Z  zin

)

,

where



Z

2 F

zin

= 2zin,

Z 1 zin

=

1(i=i)

·

sign(zin),



Z zin

= 1(n=n) · sign(zin),

NM B

M

i = arg max

|zin|, and n = arg max

|zin|.

1iM n=1

1nNMB i=1

(10)

4.3 A CONTROLLED EXPERIMENT ON DATA GENERATION PROCESS
We have designed two datasets where the number of independent factors is fully controlled. The first dataset is a synthetic 10-class classification dataset that was created using Python scikit-learn library (Pedregosa et al., 2011). The number of independent Gaussian factors, d, was controlled to be 10, 50, 100, 250, and 500, and the independent factors were mixed using a randomly generated 1000 × d rotation matrix. The second dataset is a PCA-controlled MNIST data that was created by including only the top 10, 50, 100, 250, and 500 dimensions of MNIST's PCA dimensions.
For the two datasets, we have chosen NA to be the same 5-layer MLP as before and repeatedly performed training while applying either L1R or RR with different loss weights. The results are shown in Figure 2 and Figure 3. The sparsity and rank plots show that indeed sparsity is increased and rank is reduced by increasing the loss weight. The accuracy performance, however, does not show any meaningful dependency on d. For instance, even when d = 10 and there were only ten independent factors in the data generation process, strongly applying L1R or RR did not result in improved performance. On the contrary, the accuracy often suffered when loss weight was increased.
According to the discussion in subsection 4.1, it is not surprising that the level of learned representation's sparsity does not affect the accuracy performance. Perhaps it is more surprising that even the level of learned representation's rank does not affect the accuracy performance. We move on to the analysis of mutual information for a further discussion on this issue.

7

Under review as a conference paper at ICLR 2019

(a) Synthetic data

(b) PCA-controlled MNIST data

Figure 2: Effect of L1R (L1 Representation Regularizer): representation sparsity (Ps) and accuracy are shown as a function of L1R's loss weight. Each line corresponds to a different number of independent factors. While sparsity is well controlled, accuracy does not show any meaningful dependency on the number of independent factors used in the data generation process.

(a) Synthetic data

(b) PCA-controlled MNIST data

Figure 3: Effect of RR (Rank Regularizer): representation rank (r) and accuracy are shown as a function of RR's loss weight. Each line corresponds to a different number of independent factors. While rank is well controlled, accuracy does not show any meaningful dependency on the number of independent factors used in the data generation process.

5 MUTUAL INFORMATION

So far, we have investigated popular statistical characteristics of representation zl where none of the statistical characteristics showed a strong and clear relationship to deep network's performance. In this section, we examine two information-theoretical characteristics: I(zl; x) and I(zl; y). In the original and pioneering work of Shwartz-Ziv & Tishby (2017), the two characteristics of zl were used to explain the concept of information bottleneck on deep networks. Basically, the work shows that the task-relevant information should be maximized via I(zl; y) while the task-irrelevant information should be minimized via I(zl; x). A further development was made in Achille & Soatto (2017), where the Information Bottleneck Lagrangian L(p(zl | x)) = H(y | zl) + I(zl; x) was explained the first term is the usual cross entropy cost function, the second term is a penalty term on I(zl; x), and  is parameter for controlling a trade-off between sufficiency (the first term) and minimality (the second term). In Achille & Soatto (2018), they develop `information dropout' method that implicitly minimize I(zl; x). In their limited performance experiments, they showed that information dropout can improve MNIST classification performance by about 0.25% for the best case. In another work by Kolchinsky et al. (2017), an upper bound derived using a non-parametric estimator of mutual information and a variational approximation is used to develop a gradient-based optimization method. They showed I(zl; x) and I(zl; y) are indeed reduced by the method, but did not report anything about performance.
In this work, we neither tried the aforementioned techniques nor explicitly implemented an I(zl; x) regularizer. Instead, we simply applied the twelve regularizers (including baseline) and calculated the upper and lower bounds of I(zl; x) and I(zl; y). The bounds can be calculated using the results of Kolchinsky & Tracey (2017) where a pairwise distance function between mixture components is used. They prove that the Chernoff -divergence and the Kullback-Leibler divergence provide lower and upper bounds when they are chosen as the distance function, respectively. Figure 4 shows the results for the last hidden layer together with the generalization error where the same network and dataset as in Figure 1 were used, and regularizers were also applied to the last hidden layer. One

8

Under review as a conference paper at ICLR 2019
Figure 4: Mutual information and generalization error can observe that all the regularizers end up with almost the same I(zl; y) value. But the bounds of I(zl; x) can be seen to be strongly dependent on which regularizer is used, and the upper and lower bounds show a similar pattern as the generalization error's pattern. In fact, the correlation between the lower bound and generalization error can be calculated to be 0.84, and the correlation between the upper bound and generalization error can be calculated to be 0.78. Therefore, it can be surmised that the regularizers might be indirectly affecting the performance by influencing I(zl; x). When a mutual information regularizer is excluded, the rest of the representation regularizers fail to provide general reasoning on why any of the statistical characteristics should be pursued. In fact, one can argue that even a single-neuron in layer l (activation becomes a scalar) can be a sufficient condition for encoding to have a chance to achieve the maximum possible I(zl; y), i.e., lossless in terms of relevant information. Such an encoding on a scalar activation might be very inefficient, and a practical learning method might never reach such an encoding. Nonetheless, there is no reason why such an encoding should be impossible. Obviously, many of the statistical characteristics become meaningless for such a scalar representation, and it is high time to reconsider the so-called conventional wisdom on representation characteristics.
6 IMPROVING PERFORMANCE WITH REPRESENTATION REGULARIZERS
In the previous sections, we have investigated representation characteristics and their relationship to the performance. All the results, except for mutual information that is shown in Figure 4, indicate that there might be no firm ground to believe that zl's representation characteristics are strongly related to the performance. But there have been numerous reports that performance was improved by utilizing newly designed regularizers. In this section, we investigate if (representation) regularizers can indeed consistently improve the performance for a given task condition. Here, a task condition means a learning task with small data size, a small layer width, a specific dataset, a large number of classes, or a specific optimizer. We perform experiments on MNIST, CIFAR-10, and CIFAR-100 datasets using twelve regularizers, and the representation regularizers are explained in Appendix B. The details of experimental settings and architecture parameters can be found in Appendix D.
6.1 CONSISTENTLY WELL PERFORMING REGULARIZER
We analyze if there is a consistent dependency between a regularizer and its effect on the performance when a particular regularizer is applied to a particular task condition. Our results, as shown in Table 4, Table 8, and Table 9 indicate that there is no consistently well-performing regularizer for a specific task condition. As an example, consider the entire CIFAR-10 dataset results in Table 4. While task conditions change over different columns, the data remains common for all the tasks. If there is a representation characteristic that fits the data-generation process well and one of the regularizers can match the representation well, it might have outperformed across all the columns. In the table, the best performing regularizer for each task (column) is marked in bold, and any other regularizer whose performance overlaps with the best one is highlighted in green. Looking at the bold and green-highlight patterns, one can easily conclude that there is no single regularizer that works well for all the tasks of CIFAR-10. A similar observation can be made for other task conditions. For instance, one can examine the data size of 1k. For the 1k columns of the three tables, there is no single regularizer that always performs distinctively well.
9

Under review as a conference paper at ICLR 2019

Table 4: Condition experiment results for CIFAR-10 CNN model. The best performing regularizer in each condition (each column) is shown in bold, and other regularizers whose performance range overlaps with the best one are highlighted in green. For the default condition, the standard values of data size=50k and layer width=128 were used, and Adam optimizer was applied. For the other columns, all the conditions were the same as the default except for the condition indicated on the top part of the columns. Regularizers were applied to the fully-connected layer.

Regularizer
Baseline L1W L2W Dropout BN CR cw-CR VR cw-VR L1R RR cw-RR Best improvement

Default
26.64 ± 0.16 26.46 ± 0.39 25.71 ± 0.98 26.38 ± 0.21 31.97 ± 3.10 24.96 ± 0.63 22.99 ± 0.58 21.44 ± 0.88 21.58 ± 0.21 20.63 ± 0.50 26.46 ± 0.25 26.29 ± 0.41
6.01

Data Size

1k 56.07 ± 0.36 56.64 ± 0.91 56.57 ± 0.22 56.11 ± 0.83 56.49 ± 0.32 57.40 ± 2.11 53.50 ± 1.05 53.90 ± 0.97 51.93 ± 1.09 52.39 ± 0.99 57.09 ± 1.08 57.55 ± 0.46
4.14

5k 43.95 ± 0.43 44.32 ± 0.66 44.87 ± 0.81 44.78 ± 0.41 43.75 ± 0.76 45.16 ± 0.94 42.15 ± 0.64 42.33 ± 0.57 43.00 ± 0.95 40.92 ± 0.33 44.35 ± 1.09 44.71 ± 1.59
3.03

Layer Width

32 28.54 ± 0.63 28.65 ± 1.14 28.54 ± 0.30 27.66 ± 0.51 28.83 ± 0.47 26.45 ± 0.22 26.40 ± 0.62 24.96 ± 0.26 25.81 ± 0.64 25.49 ± 0.61 26.58 ± 0.66 26.62 ± 0.77
3.58

512 28.52 ± 1.06 27.96 ± 0.72 27.79 ± 0.83 28.43 ± 0.88 28.20 ± 0.40 28.65 ± 1.21 28.54 ± 1.01 26.61 ± 0.47 26.46 ± 0.25 27.81 ± 0.43 26.87 ± 0.58 27.12 ± 0.46
2.06

Optimizer

Momentum 25.78 ± 0.37 25.73 ± 0.40 26.35 ± 0.54 25.95 ± 0.57 25.50 ± 0.55 26.72 ± 0.61 25.93 ± 0.59 25.01 ± 0.41 24.42 ± 0.31 25.13 ± 0.52 23.92 ± 0.37 24.34 ± 0.27
1.86

RMSProp 28.52 ± 1.21 28.30 ± 0.99 28.02 ± 0.88 27.69 ± 0.38 28.38 ± 0.86 27.94 ± 0.43 27.77 ± 0.88 26.06 ± 0.72 26.19 ± 1.35 26.49 ± 0.96 25.80 ± 0.85 26.10 ± 0.59
2.72

In fact, we have experimented many more settings than what are shown in this paper. The hope was to find a strong match between a task condition and a representation regularizer, but we have failed to find anything that looks consistent. Many of the previous works on regularizers have compared their regularizer with only a small number of known regularizers. When many regularizers are compared over many different tasks as in our work, one can easily conclude that there is no obvious relation to declare where a certain representation characteristic is advantageous.

6.2 PERFORMANCE IMPROVEMENT USING REGULARIZERS AS A SET
Even though no single representation characteristics consistently outperforms, it can be seen that one can improve performance by using the twelve regularizers as a set and by choosing the best performing regularizer for the given task. This is in line with the usual theme of tuning in many areas of deep learning. Looking into Table 4 more carefully, we can see that cw-VR and L1R often had the best performance for CIFAR-10 test cases. In our experiments, we have observed that one of representation regularizers often outperform weight regularizers (L1W, L2W), dropout, and BN. Even though representation regularizers do not seem to have a direct impact on the performance, they might have indirect effects on mutual information as we have seen in Section 5 or on the optimization process. When many representation regularizers are tried as a set, perhaps there is a larger chance of one of such indirect effects improving the performance.

7 DISCUSSION AND CONCLUSION
We have studied the most popular statistical characteristics and information theoretic characteristics of DNN representations. All the statistical characteristics that were studied failed to show any general or causal pattern for improving performance. Some of the conventional wisdom were analytically dismissed. Empirical results consistently showed that none of the studied statistical characteristics is a requirement for achieving good performance. While we could not identify any systematic pattern, the popular regularizers have been frequently observed to provide a healthy performance improvement. To understand this phenomenon, we have tried applying twelve different regularizers over many classification tasks with different task conditions. The results show that still no systematic pattern can be found, but the set of regularizers can be used as a very compelling tool for tuning the performance. In contrast to the statistical characteristics, information theoretic characteristic I(zl; x) showed a strong correlation with the performance of a classification task. Regularizers were able to affect the mutual information, and possibly they ended up affecting the performance as well. In this work, we have directly addressed and dismissed several conventional wisdom. However, perhaps the most important contribution of this work is to provide an early work on developing rigorous and general theories and methodologies that can be used to better understand the learned representations.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentangling in deep representations. arXiv preprint arXiv:1706.01350, 2017.
Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798­1828, 2013.
Daeyoung Choi and Wonjong Rhee. Utilizing class information for dnn representation shaping. arXiv preprint arXiv:1809.09307, 2018.
Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfitting in deep networks by decorrelating representations. arXiv preprint arXiv:1511.06068, 2015.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory, pp. 907­940, 2016.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 315­323, 2011.
Geoffrey E Hinton et al. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, volume 1, pp. 12. Amherst, MA, 1986.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.
Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances. Entropy, 19(7):361, 2017.
Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck. arXiv preprint arXiv:1705.02436, 2017.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. arXiv preprint arXiv:1412.6553, 2014.
Renjie Liao, Alex Schwing, Richard Zemel, and Raquel Urtasun. Learning deep parsimonious representations. In Advances in Neural Information Processing Systems, pp. 5076­5084, 2016.
Marc Masana, Joost van de Weijer, Luis Herranz, Andrew D Bagdanov, and Jose MAlvarez. Domainadaptive deep network compression. network, 16:30, 2017.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in neural information processing systems, pp. 2924­2932, 2014.
Preetum Nakkiran, Raziel Alvarez, Rohit Prabhavalkar, and Carolina Parada. Compressing deep neural networks using a rank-constrained topology. In Sixteenth Annual Conference of the International Speech Communication Association, 2015.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825­2830, 2011.
11

Under review as a conference paper at ICLR 2019
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. arXiv preprint arXiv:1606.05336, 2016.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929­1958, 2014.
Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015.
Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101, 2015.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In Information Theory Workshop (ITW), 2015 IEEE, pp. 1­5. IEEE, 2015.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000.
Wei Xiong, Bo Du, Lefei Zhang, Ruimin Hu, and Dacheng Tao. Regularizing deep convolutional neural networks with a structured decorrelation constraint. In Data Mining (ICDM), 2016 IEEE 16th International Conference on, pp. 519­528. IEEE, 2016.
Jian Xue, Jinyu Li, and Yifan Gong. Restructuring of deep neural network acoustic models with singular value decomposition. In Interspeech, pp. 2365­2369, 2013.
Jian Xue, Jinyu Li, Dong Yu, Mike Seltzer, and Yifan Gong. Singular value decomposition based lowfootprint speaker adaptation and personalization for deep neural network. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pp. 6359­6363. IEEE, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
12

Under review as a conference paper at ICLR 2019

APPENDIX

A PROOFS
Theorem 1 For a deep network NA = (W, b) whose layer l is linear, there exists NA = (W, b) that satisfy the following conditions:
 x, NA(x) = NA(x);  x, zl = Q(zl - m), where Q is any nonsingular square matrix of a proper size and m is any vector of a proper size.

Proof. Proof is based on a simple construction. Choose the weights of NA as below.

T
Wl

=

Q WTl

bl = Q(bl - m)

T
Wl+1

=

WlT+1

Q-1

bl+1 = bl+1 + WlT+1 m

For all the other layers, choose the same as NA's weights. Then, clearly zl-1 = zl-1 and therefore

zl

=

T
Wl

zl-1 +bl

=

Q(WlT

zl-1 + bl - m)

=

Q(zl - m).

Also,

zl+1

=

T
Wl+1zl

+ bl+1

=

WTl+1 Q-1 Q(zl - m) + bl+1 + WlT+1 m = zl+1. Because the activation vector of layer l + 1 is

the same for NA and NA, the resulting outputs NA(x) and NA(x) are exactly the same as well.

Theorem 2 For a deep network NA = (W, b) whose activation function of layer l is ReLU, there exists NA = (W, b) that satisfy the following conditions:
 x, NA(x) = NA(x);  x, zl = Q zl,
where Q is any permuted positive diagonal matrix of a proper size. Furthermore, it can be shown that any Q that satisfy the above two conditions must be a permuted positive diagonal matrix.

Proof. For simplicity, we denote a+ = ReLU(a) and a- = ReLU(-a). Then a = a+ + a-. We

denote hl = WlT zl + bl, which is the representation before applying the activation function, such

that zl

=

hl+.

T
If we choose Wl

=

Q WTl

T
and Wl+1

=

WlT+1 Q-1, our focus is to find an

invertible matrix Q that satisfy WlT+1 Q-1(Q hl)+ + bl+1 = WlT+1 hl+ + bl+1 for all x. This

reduces down to finding Q that satisfies (Q hl)+ = Q hl+. We denote i'th row of Q as qiT , and the

statement mentioned above can be written as: (qiT hl)+ = qTi hl. For qTi that satisfy (qTi hl)  0,

obviously (qiT hl)+ = qiT hli. If we substitute hl with hl+ - h-l , then qiT h-l = 0. For qTi that

satisfy qTi hl < 0, we can derive the condition qiT h+l = 0.

Now, we will show that Q should be a permuted positive diagonal matrix using the statements proved above. For a permuted positive diagonal matrix, {qiT } are linearly independent and each qTi has only one positive element. Because Q needs to be invertible (otherwise information loss occurs and
NA(x) = NA(x) cannot be achieved), it is trivial that each qTi is linearly independent. To show each qiT has only one positive element, let's assume qiT has more than one non-zero elements. If we denote qiTk as the qiT 's k-th element, and hlk as the hl's k-th element, we can divide the element indexes as follow.
A = {k| qiTk = 0 and hlk > 0}
B = {k| qiTk = 0 and hlk < 0}

13

Under review as a conference paper at ICLR 2019

Then, we can consider hl such that A =  and B = . If qTi hl >= 0, then

qiTj hlj >

qiTj hlj .

jA

jB

In this case, the right side should be zero because qTi hl- = 0. However, qiTj should be zero to satisfy qTi hl- = 0 because hl can be chosen arbitrary in the range that A =  and B =  and qiT hl >= 0. This is a contradiction due to the definition of B. We can prove the case of qiT hl < 0 similarly, which shows that each qTi has only one non-zero element. To show the qTi 's one element is positive, we denotes the qiT 's only one element as qTij, in which j is the index of the non-zero element. When qTij < 0, we can consider hl such that hlj < 0. In this case, (qiT hl) > 0, but qTi h-l > 0, which contradicts the condition qiT hl- = 0. If we consider hl such that hlj > 0, (qiT hl) < 0, but qTi h+l > 0, which contradicts the condition qiT h+l > 0. Therefore, qiTj is positive.

B REPRESENTATION REGULARIZERS

In this section, we briefly introduce the representation regularizers that are used in our experiments.

Based on the notations provided in Section 2, we define class-wise statistics that are calculated using

only class k's samples out of a total of K labels in the mini-batch. Class-wise mean, covariance, and

variance are defined as below.

µ(zkl,)i = EnSk [zln,i].

(11)

ci(,kj) = EnSk [(zln,i - µ(zkl,)i)(zln,j - µz(kl,)j )].

(12)

vz(kl,)i = c(i,ki).

(13)

Here, Sk is the set that contains indexes of the samples with class label k. Note that superscripts with and without parenthesis indicate class label and sample index, respectively. Penalty loss functions of

the representation regularizers are summarized in Table 5.

Table 5: Penalty loss functions of representation regularizers.

Penalty loss function

Description on regularization term

CR = (ci,j )2
i=j

cw-CR =

(c(i,kj))2

k i=j

V R =

vzl,i

i

cw-V R =
k

vz(kl ,)i
i

L1R =

| znl,i |

ni

RR =

Zl

2 F

Zl

2 2

cw-RR =
k

Z(l k)

2 F

Zl(k)

2 2

Covariance of representations calculated from all-class samples. Covariance of representations calculated from the same class samples. Variance of representations calculated from all-class samples. Variance of representations calculated from the same class samples. Absolute amplitude of representations calculated from all-class samples. Stable rank of representations calculated from all-class samples.
Stable rank of representations calculated from the same class samples.

C REPRESENTATION CHARACTERISTICS

In this section, we investigate the statistical characteristics of the learned representations when different regularizers are applied. We used the same network and dataset as the ones used for

14

Under review as a conference paper at ICLR 2019

generating Figure 1. All the regularizers were applied only to the fifth layer, and the representation characteristics were calculated using the fifth layer as well. In Table 6, typical evaluation results of statistical characteristics are shown. We can confirm that the statistical characteristics targeted by each representation regularizer are indeed manipulated as expected (Bold). In particular, RR and cw-RR designed in this work to regularize the stable rank work as expected. The two weight regularizers (L1W: L1 Weight Regularizer, L2W: L2 Weight Regularizer) have similar characteristic values as the baseline's, and this can be taken for granted because the regularizers do not directly regularize representations. A few conventional beliefs mentioned in the paper are quantitatively confirmed as well. A large number of dead units is known to be harmful because they do not contribute toward improving the performance of deep networks. Our result shows even 39% of DEAD UNIT caused by L1R does not hurt the performance, which is in line with our analysis in Section 4. For dropout, COVARIANCE is reduced as in Cogswell et al. (2015), but CORRELATION is actually increased compared to the baseline. In fact, COVARIANCE is reduced simply because ACTIVATION AMPLITUDE is reduced as mentioned in Section 3, and the correlation between two active units is actually made larger by applying dropout. Therefore, it cannot be said that the relationship between a pair of neurons becomes weaker by applying dropout. This is in contrary to the `reduction of coadaptation' idea. Note that we have excluded the inactive neurons for the evaluations. If the inactive ones are included with their zero values, the covariance and correlation values will be different.
Table 6: Statistical characteristics of learned representations.

Regularizer Error ACTIVATION AMPLITUDE COVARIANCE CORRELATION SPARSITY DEAD UNIT RANK

Baseline 2.85

4.93

2.08 0.27 0.34 0.13 2.41

L1W

2.85

4.53

1.95 0.28 0.29 0.01 2.32

L2W

3.02

4.76

2.23 0.29 0.34 0.09 2.26

Dropout 2.70

2.72

0.87 0.42 0.58 0.06 2.75

BN 2.81

1.35

0.24 0.28 0.52 0.00 5.14

CR 2.50

0.50

0.01 0.19 0.40 0.03 7.12

cw-CR 2.49

0.63

0.02 0.31 0.51 0.07 3.60

VR 2.65

1.35

0.15 0.26 0.40 0.08 3.92

cw-VR 2.42

0.63

0.02 0.36 0.53 0.06 3.90

L1R 2.35

1.29

0.03 0.40 0.97 0.39 5.94

RR 2.81

7.23

226.2

0.90 0.43 0.18 1.00

cw-RR 2.57

10.31

96.3 0.91 0.31 0.22 1.00

D EXPERIMENT DETAILS

D.1 DEFAULT SETTINGS
By default, ReLU activation function and Adam optimizer were used, and a learning rate was set to 0.0001. Validation performance was evaluated with different loss weights {0.001, 0.01, 0.1, 1, 10, 100}, and the one with the best validation performance for each regularizer and condition was chosen for testing. We used the validation data for MNIST dataset and the last 10,000 samples of training data for CIFAR-10/100 datasets as validation data. After a loss weight was fixed, the validation data was merged back into the training data in the case of CIFAR-10/100. Five training trials were performed, and the means of the five trials were reported. Note that a mini-batch size was set to 100 for MNIST and CIFAR-10 tasks but to 500 for CIFAR-100 due to the class-wise statistic calculation. In this work, we carried out all the experiments using TensorFlow 1.5.

D.2 ARCHITECTURE PARAMETERS
We used a 6-layer MLP with 100 units per hidden layer for MNIST image classification tasks. We used a CNN with four convolutional layers and one fully-connected layer for CIFAR-10/100 image classification tasks. Architecture details are described in Table 7.

D.3 EXPERIMENTAL CONDITIONS
Default conditions are shown in bold, and the full experimental conditions are listed below. · Training data size: 1k, 5k, 50k · Layer width: (MNIST) 2, 8, 100 / (CIFAR-10/100): 32, 128, 512

15

Under review as a conference paper at ICLR 2019

Table 7: Architecture hyperparameters of CIFAR-10/100 CNN model.

Layer Convolutional layer-1 Convolutional layer-2 Max-pooling layer-1 Convolutional layer-3 Max-pooling layer-2 Convolutional layer-4 Max-pooling layer-3 Fully connected layer

Parameter Number of filters=32, Filter size=3 × 3, Convolution stride=1 Number of filters=64, Filter size=3 × 3, Convolution stride=1
Pooling size=2 × 2, Pooling stride=2 Number of filters=128, Filter size=3 × 3, Convolution stride=1
Pooling size=2 × 2, Pooling stride=2 Number of filters=128, Filter size=3 × 3, Convolution stride=1
Pooling size=2 × 2, Pooling stride=2
Number of units=128

· Optimizer (CIFAR-10): Adam, Momentum (lr=0.01, momentum=0.9), RMSProp (lr=0.0001) · Number of classes (CIFAR-100): 16, 64, 100

D.4 EXPERIMENTAL RESULTS (MNIST, CIFAR-100)

Table 8: Condition experiment results for MNIST MLP model. The best performing regularizer in each condition (each column) is shown in bold, and other regularizers whose performance range overlaps with the best one are highlighted in green. For the default condition, the standard values of data size=50k and layer width=100 were used and Adam optimizer was applied. For other columns, all the conditions were the same as the default except for the condition indicated on the top part of the columns. Regularizers were applied to the fifth hidden layer.

Regularizer
Baseline L1W L2W Dropout BN CR cw-CR VR cw-VR L1R RR cw-RR Best improvement

Default
2.85 ± 0.11 2.85 ± 0.06 3.02 ± 0.40 2.70 ± 0.08 2.81 ± 0.12 2.50 ± 0.05 2.49 ± 0.10 2.65 ± 0.11 2.42 ± 0.06 2.35 ± 0.08 2.81 ± 0.10 2.57 ± 0.08
0.5

Data Size

1k 11.41 ± 0.19 11.64 ± 0.27 11.38 ± 0.18 10.29 ± 0.23 10.81 ± 0.04 11.63 ± 0.24 10.62 ± 0.05 14.42 ± 0.14 10.44 ± 0.18 11.60 ± 0.20 10.92 ± 0.17 10.89 ± 0.19
1.12

5k 6.00 ± 0.07 5.96 ± 0.11 5.86 ± 0.10 5.59 ± 0.11 5.60 ± 0.10 6.05 ± 0.06 5.80 ± 0.15 6.90 ± 0.22 5.90 ± 0.12 6.20 ± 0.13 6.61 ± 0.05 6.60 ± 0.17
0.41

Layer Width

2 31.62 ± 0.07 31.67 ± 0.15 31.66 ± 0.13 62.09 ± 1.32 42.08 ± 0.93 34.80 ± 0.25 31.50 ± 0.11 32.39 ± 0.13 30.34 ± 0.06 64.39 ± 0.26 38.35 ± 0.20 38.57 ± 0.12
1.28

8 10.52 ± 0.57 11.02 ± 0.58 10.65 ± 0.23 13.94 ± 1.05 7.51 ± 0.58 10.25 ± 0.74 10.81 ± 1.11 9.22 ± 0.28 10.01 ± 0.63 88.65 ± 0.00 12.31 ± 0.16 12.63 ± 0.39
3.01

Table 9: Condition experiment results for CIFAR-100 CNN model. The best performing regularizer in each condition (each column) is shown in bold, and other regularizers whose performance range overlaps with the best one are highlighted in green. For the default condition, the standard values of data size=50k, layer width=128, and number of classes=100 were used. For other columns, all the conditions were the same as the default except for the condition indicated on the top part of the columns. Regularizers were applied to the fully-connected layer.

Regularizer
Baseline L1W L2W Dropout BN CR cw-CR VR cw-VR L1R RR cw-RR Best improvement

Default
61.26 ± 0.52 60.97 ± 0.64 60.23 ± 0.31 63.88 ± 0.72 60.93 ± 0.39 59.88 ± 0.50 57.03 ± 0.73 57.68 ± 0.94 56.75 ± 0.64 56.03 ± 0.81 62.68 ± 0.35 62.62 ± 0.31
5.23

Data Size

1k 90.89 ± 0.30 91.33 ± 0.37 90.53 ± 0.39 90.22 ± 0.48 91.18 ± 0.36 91.70 ± 0.14 90.85 ± 0.29 91.43 ± 0.32 90.45 ± 0.22 91.15 ± 0.35 91.20 ± 0.27 90.62 ± 0.34
0.67

5k 82.21 ± 0.72 82.3 ± 0.6 82.05 ± 0.70 81.68 ± 0.81 82.01 ± 0.58 82.47 ± 0.41 81.29 ± 0.62 81.85 ± 0.38 81.03 ± 0.57 81.98 ± 0.47 81.32 ± 0.36 81.57 ± 0.14
1.18

Layer Width

32 62.41 ± 0.34 62.23 ± 0.58 62.78 ± 0.36 64.08 ± 0.99 62.18 ± 1.49 60.47 ± 0.63 61.41 ± 0.67 61.35 ± 0.45 60.67 ± 0.59 61.11 ± 0.31 68.54 ± 0.46 68.11 ± 0.31
1.94

512 61.30 ± 0.64 60.92 ± 0.47 61.55 ± 0.99 64.31 ± 0.37 62.16 ± 0.57 60.70 ± 0.94 58.02 ± 0.25 56.87 ± 0.74 56.91 ± 0.73 56.46 ± 0.62 59.29 ± 0.32 59.15 ± 0.29
4.84

Number of Classes

16 45.75 ± 0.73 45.08 ± 1.53 45.28 ± 1.59 45.73 ± 1.57 44.55 ± 1.43 44.55 ± 1.10 43.50 ± 1.21 42.33 ± 1.03 41.38 ± 0.53 42.51 ± 1.43 44.16 ± 0.80 44.10 ± 0.65
4.37

64 58.02 ± 0.40 58.08 ± 1.18 57.47 ± 0.66 59.14 ± 0.46 57.72 ± 0.66 56.76 ± 0.86 54.24 ± 0.64 54.32 ± 0.40 54.23 ± 1.06 53.65 ± 1.00 60.25 ± 0.35 60.03 ± 0.41
4.37

16

