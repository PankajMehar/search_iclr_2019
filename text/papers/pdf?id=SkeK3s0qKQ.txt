Under review as a conference paper at ICLR 2019
EPISODIC CURIOSITY THROUGH REACHABILITY
Anonymous authors Paper under double-blind review
ABSTRACT
Rewards are sparse in the real world and most today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself -- thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward -- making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory -- which incorporates rich information about environment dynamics. This allows us to overcome the known "couch-potato" issues of prior work -- when the agent finds a way to instantly gratify itself by exploiting actions which lead to unpredictable consequences. We test our approach in visually rich 3D environments in VizDoom and DMLab. In VizDoom, our agent learns to successfully navigate to a distant goal at least 2 times faster than the state-of-the-art curiosity method ICM. In DMLab, our agent generalizes well to new procedurally generated levels of the game -- reaching the goal at least 2 times more frequently than ICM on test mazes with very sparse reward.
1 INTRODUCTION
Many real-world tasks have sparse rewards. For example, animals searching for food may need to go many miles without any reward from the environment. Standard reinforcement learning algorithms struggle with such tasks because of reliance on simple action entropy maximization as a source of exploration behaviour.
Multiple approaches were proposed to achieve better explorative policies. One way is to give a reward bonus which facilitates exploration by rewarding novel observations. The reward bonus is summed up with the original task reward and optimized by standard RL algorithms. Such an approach is motivated by neuroscience studies of animals: an animal has an ability to reward itself for something novel ­ the mechanism biologically built into its dopamine release system. How exactly this bonus is formed remains an open question.
Many modern curiosity formulations aim at maximizing "surprise" -- inability to predict the future. This approach makes perfect sense but, in fact, is far from perfect. To show why, let us consider a thought example. Imagine an agent is put into a 3D maze. There is a precious goal somewhere in the maze which would give a large reward. Now, the agent is also given a remote control to a TV and can switch the channels. Every switch shows a random image (say, from a fixed set of images). The curiosity formulations which optimize surprise would rejoice because the result of the channel switching action is unpredictable. The agent would stay in front of the TV forever -- instead of looking for a goal in the environment (this was indeed observed in (Burda et al., 2018)). So, should we call the channel switching behaviour curious? Maybe, but it is unproductive for the original sparse-reward goal-reaching task. What would be a definition of curiosity which does not suffer from such "couch-potato" behaviour?
We propose a new curiosity definition based on the following intuition. If the agent knew the observation after changing a TV channel is only one step away from the observation before doing that -- it probably would not be so interesting to change the channel in the first place (too easy). This
1

Under review as a conference paper at ICLR 2019

Observations in memory
Reachable from memory in  k steps (not novel)

Far from memory ­ takes > k steps to reach
(novel)

Figure 1: We define novelty through reachability. The nodes in the graph are observations, the edges -- possible transitions. The blue nodes are already in memory, the green nodes are reachable from the memory within k = 2 steps (not novel), the orange nodes are further away -- take more than k steps to reach (novel). In practice, the full possible transition graph is not available, so we train a neural network approximator to predict if the distance in steps between observations is larger or smaller than k.

intuition can be formalized as giving a reward only for those observations which take some effort to reach (outside the already explored part of the environment). The effort is measured in the number of environment steps. To estimate it we train a neural network approximator: given two observations, it would predict how many steps separate them. The concept of novelty via reachability is illustrated in Figure 1. To make the description above practically implementable, there is still one piece missing though. For determining the novelty of the current observation, we need to keep track of what was already explored in the environment. A natural candidate for that purpose would be episodic memory: it stores instances of the past which makes it easy to apply the reachability approximator on pairs of current and past observations.
Our method works as follows. The agent starts with an empty memory at the beginning of the episode and at every step compares the current observation with the observations in memory to determine novelty. If the current observation is indeed novel -- takes more steps to reach from observations in memory than a threshold -- the agent rewards itself with a bonus and adds the current observation to the episodic memory. The process continues until the end of the episode, when the memory is wiped clean.
We benchmark our method on a range of tasks from visually rich 3D environments VizDoom and DMLab. We conduct the comparison with other methods -- including the state-of-the-art curiosity method ICM (Pathak et al., 2017) -- under the same budget of environment interactions. First, we use the VizDoom environments from prior work to establish that our re-implementation of the ICM baseline is correct -- and also demonstrate at least 2 times faster convergence of our method with respect to the baseline. Second, in the randomized procedurally generated environments from DMLab our method turns out to be more robust to spurious behaviours than the method ICM: while the baseline learns a persistent firing behaviour in navigational tasks (thus creating interesting pictures for itself), our method learns a reasonable explorative behaviour. In terms of quantitative evaluation, our method reaches the goal at least 2 times more often in the procedurally generated test levels in DMLab with a very sparse reward. Third, when comparing the behaviour of the agent in the complete absence of rewards, our method covers at least 4 times more area (measured in discrete (x, y) coordinate cells) than the baseline ICM. Finally, we demonstrate that our curiosity bonus does not significantly deteriorate performance of the plain PPO algorithm (Schulman et al., 2017) in two tasks with dense reward in DMLab. The implementation of our method will be made publicly available.

2 EPISODIC CURIOSITY
We consider an agent which interacts with an environment. The interactions happen at discrete time steps over the episodes of limited duration T . At each time step t, the environment provides the agent with an observation ot from the observational space O (we consider images), samples an action at from a set of actions A using a probabilistic policy (ot) and receives a scalar reward rt  R together with the new observation ot+1 and an end-of-episode indicator. The goal of the agent is to optimize the expectation of the discounted sum of rewards during the episode S = t trt.
In this work we primarily focus on the tasks where rewards rt are sparse -- that is, zero for most of the time steps t. Under such conditions commonly used RL algorithms (e.g., PPO Schulman et al. (2017)) do not work well. We further introduce an episodic curiosity (EC) module which alleviates this problem. The purpose of this module is to produce a reward bonus bt which is further summed

2

Under review as a conference paper at ICLR 2019

a2 a3 o3
a1 o2 positive

a4 o4

o5 a5

o6 a6

o1

Embedding network

Comparator network

negative

a7 o7

Reachability network

o8

Figure 2: Left: siamese architecture of reachability (R) network. Right: R-network is trained based on a sequence of observations that the agent encounters while acting. The temporally close (within threshold) pairs of observations are positive examples, while temporally far ones -- negatives.

up with the task reward rt to give an augmented reward rt = rt + bt. The augmented reward has a nice property from the RL point of view -- it is a dense reward. Learning with such reward is faster, more stable and often leads to better final performance in terms of the cumulative task reward S.
In the following section we describe the key components of our episodic curiosity module.

2.1 EPISODIC CURIOSITY MODULE
The episodic curiosity (EC) module takes the current observation o as input and produces a reward bonus b. The module consists of both parametric and non-parametric components. There are two parametric components: an embedding network E : O  Rn and a comparator network C : Rn × Rn  [0, 1]. Those parametric components are trained together to predict reachability as parts of the reachability network -- shown in Figure 2. There are also two non-parametric components: an episodic memory buffer M and a reward bonus estimation function B. The high-level overview of the system is shown in Figure 3. Next, we give a detailed explanation of all the components.
Embedding and comparator networks. Both networks are designed to function jointly for estimating within-k-step-reachability of one observation oi from another observation oj as parts of a reachability network R(oi, oj) = C(E(oi), E(oj)). This is a siamese architecture similar to (Zagoruyko & Komodakis, 2015). The architecture is shown in Figure 2. R-network is a classifier trained with a logistic regression loss: it predicts values close to 0 if probability of two observations being reachable from one another in k steps is low, and values close to 1 when this probability is high. Inside the episodic curiosity the two networks are used separately to save up both computation and memory.
Episodic memory. The episodic memory buffer M stores embeddings of past observations from the current episode, computed with the embedding network E. The memory buffer has a limited capacity K to avoid memory and performance issues. At every step, the embedding of the current observation might be added to the memory. What to do when the capacity is exceeded? One solution we found working well in practice is to substitute a random element in memory with the current element. This way there are still more fresh elements in memory than older ones, but the older elements are not totally neglected.
Reward bonus estimation module. The purpose of this module is to check for reachable observations in memory and if none are found -- assign larger reward bonus to the current time step. The check is done by comparing embeddings in memory to the current embedding via comparator network. Essentially, this check insures that no observation in memory can be reached by taking only a few actions from the current state -- our characterization of novelty.

2.2 BONUS COMPUTATION ALGORITHM.
At every time step, the current observation o goes through the embedding network producing the embedding vector e = E(o). This embedding vector is compared with the stored embeddings in the memory buffer M = e1, . . . , e|M| via the comparator network C where |M| is the current

3

Under review as a conference paper at ICLR 2019

Current embedding

Current observation

Embedding network

Memory buffer

Comparator network

Reachability Reward

buffer

bonus

estimation

module

Append to memory if large curiosity reward

Reward bonus

Figure 3: The use of episodic curiosity (EC) module for reward bonus computation. The module take a current observation as input and computes a reward bonus which is higher for novel observations. This bonus is later summed up with the task reward and used for training an RL agent.

number of elements in memory. This comparator network fills the reachability buffer with values

ci = C(ei, e), i = 1, |M|.

(1)

Then the similarity score between the memory buffer and the current embedding is computed from the reachability buffer as (with a slight abuse of notation)

C(M, e) = F c1, . . . , c|M|  [0, 1].

(2)

where the aggregation function F is a hyperparameter of our method. Theoretically, F = max would be a good choice, however, in practice it is prone to outliers coming from the parametric embedding and comparator networks. Empirically, we found that 90-th percentile works well as a robust substitute to maximum.

As a curiosity bonus, we take

b = B(M, e) = ( - C(M, e)),

(3)

where   R+ and   R are hyperparameters of our method. We found that in practice  = 0.5

works

well

--

making

b

belong

to

the

range

[-

 2

,

 2

].

The

value

of



depends

on

the

scale

of

task

rewards -- we will discuss how to select it in the experimental section.

After the bonus computation, the observation embedding is added to memory if the bonus b is larger than a novelty threshold bnovelty. This check is necessary for the following reason. If every observation embedding is added to the memory buffer, the observation from the current step will always be reachable from the previous step. Thus, the reward would never be granted. The threshold bnovelty induces a discretization in the embedding space. Intuitively, this makes sense: only "distinct enough" memories are stored. As a side benefit, the memory buffer stores information with much less redundancy. We found that in practice bnovelty = 0 works well.

2.3 REACHABILITY NETWORK TRAINING
If the full transition graph in Figure 1 was available, there would be no need of a reachability network and the novelty could be computed analytically through the shortest-path algorithm. However, normally we have access only to the sequence of observations which the agent receives while acting. Fortunately, as suggested by (Savinov et al., 2018), even a simple observation sequence graph could still be used for training a reasonable approximator to the real step-distance. This procedure is illustrated in Figure 2. This procedure takes as input a sequence of observations o1, . . . , oN and forms pairs from those observations. The pairs (oi, oj) where |i - j|  k are taken as positive (reachable) examples while the pairs with |i - j| > k become negative examples. The hyperparameter  is necessary to create a gap between positive and negative examples. In the end, the network is trained with logistic regression loss to output the probability of the positive (reachable) class.
We generally follow the training protocol proposed by (Savinov et al., 2018). This training is inspired by "motion babbling" behaviour children perform during the first months of their life, while laying in the cradle. We put the agent into exactly the same conditions where it will be eventually tested: same episode duration and same action set. The agent takes random actions from a discrete

4

Under review as a conference paper at ICLR 2019
(a) (b) (c) (d)
Figure 4: Examples of tasks considered in our experiments: (a) VizDoom static maze goal reaching, (b) DMLab randomized maze goal reaching, (c) DMLab key-door puzzle, (d) DMLab collect good objects / avoid bad objects.
action set with equal probabilities. Given the environment interaction budget (2.5M 4-repeated steps in DMLab, 300K 4-repeated steps in VizDoom), the agent fills in the replay buffer with observations coming from its interactions with the environment, and forms training pairs by sampling from this replay buffer randomly. We provide the details of R-network training in the supplementary material.
3 EXPERIMENTAL SETUP
We test our method in multiple environments from VizDoom (Kempka et al., 2016) and DMLab (Beattie et al., 2016). The experiments in VizDoom allow us to verify that our re-implementation of the previous state-of-the-art curiosity method ICM (Pathak et al., 2017) is correct. The experiments in DMLab allow us to extensively test the generalization of our method as well as baselines -- DMLab provides convenient procedural level generation capabilities which allows us to train and test RL methods on hundreds of levels. The examples of tasks are shown in Figure 4.
Environments. Both VizDoom and DMLab environments provide rich maze-like 3D environments. The observations are given to the agent in the form of images. For VizDoom, we use 84 × 84 grayscale images as input. For DMLab, we use 84 × 84 RGB images as input. The agent operates with a discrete action set which comprises different navigational actions. For VizDoom, the standard action set consists of 3 actions: move forward, turn left/right. For DMLab, it consists of 9 actions: move forward/backward, turn left/right, strafe left/right, turn left/right+move forward, fire. For both VizDoom and DMLab we use all actions with the repeat of 4, as typical in the prior work. We only use RGB input of the provided RGBD observations and remove all head-on display information from the screen, leaving only the plain first-person view images of the maze. The rewards and episode durations differ between particular environments and will be further specified in the corresponding experimental sections.
Basic RL algorithm. We choose the commonly used PPO algorithm from the open-source implementation1 as our basic RL algorithm. The policy and value functions are represented as CNNs to reduce number of hyperparameters -- LSTMs are harder to tune and such tuning is orthogonal to the contribution of the paper. We apply PPO to the sum of the task reward and the bonus reward coming from specific curiosity algorithms. The hyperparameters of the PPO algorithm are given in the supplementary material. We use only two sets of hyperparameters: one for all VizDoom environments and the other one for all DMLab environments.
Baseline methods. The simplest baseline for our approach is just the basic RL algorithm applied to the task reward. As suggested by the prior work and our experiments, this is a relatively weak baseline in the tasks where reward is sparse. As the second baseline, we take the state-of-the-art curiosity method ICM (Pathak et al., 2017). As follows from the results in (Pathak et al., 2017; Fu et al., 2017), ICM is superior to methods VIME (Houthooft et al., 2016), #Exploration (Tang et al., 2017) and EX2 (Fu et al., 2017) on the curiosity tasks in visually rich 3D environments. Finally, as a sanity check, we introduce a novel baseline method which we call Grid Oracle. Since we can access current (x, y) coordinates of the agent in all environments, we are able to directly
1https://github.com/openai/baselines
5

Under review as a conference paper at ICLR 2019

(a) (b)

(c)

Figure 5: Examples of maze layouts considered in our experiments: (a) VizDoom static maze goal reaching, (b) DMLab randomized maze goal reaching, (c) DMLab randomized maze goal reaching with doors.

discretize the world in 2D cells and reward the agent for visiting as many cells as possible during the episode (the reward bonus is proportional to the number of cells visited). At the end of the episode, cell visit counts are zeroed. The reader should keep in mind that this baseline uses privileged information not available to other methods (including our own method EC). While this privileged information is not guaranteed to lead to success in any particular RL task, we do observe this baseline to perform strongly in many tasks, especially in complicated DMLab environments. The Grid Oracle baseline has two hyperparameters: the weight for combining Grid Oracle reward with the task reward and the cell size.

Hyperparameter tuning. As DMLab environments are procedurally generated, we perform tuning on the validation set, disjoint with the training and test sets. The tuning is done on one of the environments and then the same hyperparameters are re-used for all other environments. VizDoom environments are not procedurally generated, so there is no trivial way to have proper training/validation/test splits -- so we tune on the same environment (as typical in the prior RL work for the environments without splits). When tuning, we consider the mean final reward of 10 training runs with the same set of hyperparameters as the objective -- thus we do not perform any seed tuning. All hyperparameter values are listed in the supplementary material. Note that although bonus scalar  depends on the range of task rewards, the environments in VizDoom and DMLab have similar ranges within each platform -- so our approach with re-using  for multiple environments works.
4 EXPERIMENTS
In this section, we describe the specific tasks we are solving and experimental results for all considered methods on those tasks. There are 4 methods to report: PPO, PPO + ICM, PPO + Grid Oracle and PPO + EC (our method). First, we test static-maze goal reaching in VizDoom environments from prior work to verify that our baseline re-implementation is correct. Second, we test the goal-reaching behaviour in procedurally generated mazes in DMLab. Third, we train no-reward (pure curiosity) maze exploration on the levels from DMLab and report Grid Oracle reward as an approximate measure of the maze coverage. Finally, we demonstrate that our curiosity bonus does not significantly deteriorate performance in two dense reward tasks in DMLab. All the experiments were conducted under the same environment interaction budget for all methods (R-network pre-training is included in this budget). The videos of all trained agents in all environments are available online2.
For additional experiments we refer the reader to the supplementary material: there we show that R-network can successfully generalize between environments, demonstrate stability of our method to hyperparameters and present an ablation study.
4.1 STATIC MAZE GOAL REACHING.
The goal of this experiment is to verify our re-implementation of the baseline method is correct. We use the MyWayHome task from VizDoom. The agent has to reach the goal in a static 3D maze in the time limit of 525 4-repeated steps (equivalent to 1 minute). It only gets a reward of +1 when it reaches the goal (episode ends at that moment), the rest of the time the reward is zero.
The task has three sub-tasks (following the setup in (Pathak et al., 2017)): "Dense", "Sparse" and "Very Sparse". The layout of the maze is demonstrated in Figure 5(c). The goal is always at the same room but the starting points are different in those sub-tasks. For the "Dense" subtask, the
2https://sites.google.com/view/episodic-curiosity

6

Under review as a conference paper at ICLR 2019

Episode rewards Episode rewards Episode rewards

Dense
1.00 0.75 0.50 0.25 0.00
Num0 ber1of tra2ining3 step4s (in5millio6ns)

Sparse
1.00 0.75 0.50 0.25 0.00
Num0ber 1of tra2ining3 step4s (in5milli6ons)

Very Sparse

1.00

0.75 PPO

0.50 0.25

PPO + ICM Ours

0.00

Num0ber 1of tra2ining3 step4s (in5milli6ons)

Figure 6: Task reward as a function of training step for VizDoom tasks. Higher is better. We shift the curves for our method by the number of environment steps used to train R-network -- so the comparison is fair. We run every method with a repeat of 3 (same as in prior work (Pathak et al., 2017)) and show all runs. No seed tuning is performed.

agent starts in one of the random locations in the maze, some of which are close to the goal. In this sub-task, the reward is relatively dense (hence the name): the agent is likely to bump into the goal by a short random walk. Thus, this is an easy task even for standard RL methods. The other two sub-tasks are harder: the agent starts in a medium-distant room from the goal ("Sparse") or in a very distant room ("Very Sparse"). Those tasks are hard for standard RL algorithms because the probability of bumping into a rewarding state by a random walk is very low.
The training curves are shown in Figure 6. By analysing them, we draw a few conclusions. First, our re-implementation of the ICM baseline is correct and the results are in line with those published in (Pathak et al., 2017). Second, our method works on-par with the ICM baseline in terms of final performance, quickly reaching 100% success rate in all three sub-tasks. Finally, in terms of convergence speed our algorithm is significantly faster than the state-of-the-art method ICM -- our method reaches 100% success rate at least 2 times faster. Note that to make the comparison of the training speed fair, we shift our training curves by the environment interaction budget used for training R-network.

4.2 PROCEDURALLY GENERATED RANDOM MAZE GOAL REACHING.
In this experiment we aim to evaluate maze goal reaching task generalization on a large scale. We train on hundreds of levels and then test also on hundreds of hold-out levels. We use "Explore Goal Locations Large" (we will denote it "Sparse") and "Explore Obstructed Goals Large" (we will denote it "Sparse + Doors") levels in the DMLab simulator. In those levels, the agent starts in a random location in a randomly generated maze (both layout and textures are randomized at the beginning of the episode). Within the time limit of 1800 4-repeated steps (equivalent to 2 minutes), the agent has to reach the goal as many times as possible. Every time it reaches a goal, it is respawned into another random location in the maze and has to go to the goal again. Every time the goal is reached, the agent gets a reward +10, the rest of the time the reward is zero. The second level is a variation of the first one with doors which make the paths in the maze longer. The layouts of the levels are demonstrated in Figure 5(b,c).
We found out that the standard task "Sparse" is actually relatively easy even for the plain PPO algorithm. The reason is that the agent starting point and the goal are sampled on the map independently of each other -- and sometimes both happen to be in the same room which simplifies the task. To test the limits of the algorithms, we create a gap between the starting point and the goal which eliminates same-room initialization. We report the results for both the original task "Sparse" and its harder version "Very Sparse". Thus, there are overall three tasks considered in this section: "Sparse", "Very Sparse" and "Sparse + Doors".
The results demonstrate that our method can reasonably adapt to ever-changing layouts and textures -- see Table 1 and training curves in Figure 7. We outperform the baseline method ICM in all three environments using the same environment interaction budget of 20M 4-repeated steps. The environment "Sparse" is relatively easy and all methods work reasonably. In the "Very Sparse" and "Sparse + Doors" settings our advantage with respect to PPO and ICM is more clear. On those levels, the visual inspection of the ICM learnt behaviour reveals an important property of this method: it is confused by the firing action and learns to entertain itself by firing until it runs out of ammunition. A similar finding was reported in a concurrent work (Burda et al., 2018): the agent was given an action which switched the content on a TV screen in a maze, along with the movement actions. Instead of

7

Under review as a conference paper at ICLR 2019

Episode rewards

Episode rewards

Sparse
60
40
20
0
Nu0mber of 5training1s0teps (in15millions)
No Reward
600
400
200
0
Num0 ber of t5raining1s0teps (in15millions)

Episode rewards

Episode rewards

Very Sparse
40 30 20 10 0
Nu0mber of 5training1s0teps (in15millions)
No Reward - Fire
600
400
200
0
Num0 ber of t5raining1s0teps (in15millions)

Episode rewards

Episode rewards

Sparse + Doors
12.5 10.0 7.5 5.0 2.5 0.0
Num0 ber of t5raining1s0teps (in15millions)
Dense 1
30

20

10 PPO

0

PPO + ICM PPO + EC (ours)

Nu0mber of 5training1s0teps (in15millions)

Figure 7: Reward as a function of training step for DMLab tasks. Higher is better. We shift the curves for our method by the number of environment steps used to train R-network -- so the comparison is fair. We run every method 10 times and show all runs. No seed tuning is performed.
moving, the agent learns to switch channels forever. While one might intuitively accept such "couchpotato" behaviour in intelligent creatures, it does not need to be a consequence of curious behaviour. In particular, we are not observing such dramatic firing behaviour for our curiosity formulation: according to Figure 1, an observation after firing is still one step away from the one before firing, so it is not novel (note that firing still could happen in practice because of the entropy term in PPO). Thus, our formulation turns out to be more robust than ICM's prediction error in this scenario. Note that we do not specifically look for an action set which breaks the baseline -- just use the standard one for DMLab, in line with the prior work (e.g., (Espeholt et al., 2018)).
The result of this experiment suggests to look more into how methods behave in extremely-sparse reward scenarios. The limiting case would be no reward at all -- we consider it in the next section.
4.3 NO REWARD/AREA COVERAGE.
This experiment aims to quantitatively establish how good our method is in the scenario when no task reward is given. One might question why this scenario is interesting -- however, before the task reward is found for the first time, the agent lives in the no-reward world. How it behaves in this case will also determine how likely it is to stumble into the task reward in the first place.
We use one of the DMLab levels -- "Sparse" from the previous experiment. We modify the task to eliminate the reward and name the new task "No Reward". To quantify success in this task, we report the reward coming from Grid Oracle for all compared methods. This reward provides a discrete approximation to the area covered by the agent while exploring.
The training curves are shown in Figure 7 and the final test results in Table 1. The result of this experiment is that our method and Grid Oracle both work, while the ICM baseline is not working -- and the qualitative difference in behaviour is bigger than in the previous experiments. As can be seen from the training curves, after a temporary increase, ICM quality actually decreases over time, rendering a sharp disagreement between the prediction-error-based bonus and the area coverage metric. By looking at the video2, we observe that the firing behaviour of ICM becomes even more prominent, while our method still shows reasonable exploration.
Finally, we try to find out if the ICM baseline behaviour above is due to the firing action only. Could it learn exploration of randomized mazes if the Fire action is excluded from the actions set? For that purpose, we create a new version of the task -- we call it "No Reward - Fire". This task demonstrates qualitatively similar results to the one with the full action set -- see Table 1. By looking at the videos2, we hypothesise that the agent can most significantly change its current view when it is close to the wall -- thus increasing one-step prediction error -- so it tends to get stuck near "interesting" diverse textures on the walls.

8

Under review as a conference paper at ICLR 2019

Table 1: Reward in DMLab tasks (mean ± std) for all compared methods. Higher is better. We report Grid Oracle reward in tasks with no reward. The results for our method are reported from less training steps to account for R-network pre-training. The Grid Oracle method is given for reference -- it uses privileged information unavailable to other methods. No seed tuning is performed.

Method
PPO PPO + ICM PPO + EC (ours)
PPO + Grid Oracle

Sparse
27.0 ± 5.1 23.8 ± 2.8 26.2 ± 1.9
56.7 ± 1.3

Very Sparse Sparse+Doors No Reward No Reward - Fire

8.6 ± 4.3 11.2 ± 3.9 24.7 ± 2.2

1.5 ± 0.1 2.7 ± 0.2 8.5 ± 0.6

191 ± 12 72 ± 2 475 ± 8

217 ± 19 87 ± 3 492 ± 10

54.3 ± 1.2

29.4 ± 0.5

796 ± 2

795 ± 3

Dense 1
22.8 ± 0.5 20.9 ± 0.6 19.9 ± 0.7
20.9 ± 0.6

Dense 2
9.41 ± 0.02 9.39 ± 0.02 9.53 ± 0.03
8.97 ± 0.04

The results suggest that in an environment completely without reward, the ICM method will exhaust its curiosity very quickly -- passing through a sharp peak and then degrading into undesired behaviour. This observation raises concerns: what if ICM passes the peak before it reaches the first task reward in the cases of real tasks? Supposedly, it would require careful tuning per-game. Furthermore, in some cases, it would take a lot of time with a good exploration behaviour to reach the first reward, which would require to stay at the top performance for longer -- which is problematic for the ICM method but still possible for our method.
4.4 DENSE REWARD TASKS.
A desirable property of a good curiosity bonus is to avoid hurting performance in dense-reward tasks (in addition to improving performance for sparse-reward tasks). We test this scenario in two levels in the DMLab simulator: "Rooms Keys Doors Puzzle" (which we denote "Dense 1") and "Rooms Collect Good Objects Train" (which we denote "Dense 2"). In the first task, the agent has to collect keys and reach the goal object behind a few doors openable by those keys. The rewards in this task are rather dense (key collection/door opening is rewarded). In the second task the agent has to collect good objects (give positive reward) and avoid bad objects (give negative reward). The episode lasts for 900 4-repeated steps (equivalent to 1 minute) in both tasks.
The results show that our method indeed does not significantly deteriorate performance of plain PPO in those dense-reward tasks -- see Table 1. The training curves for "Dense 1" are shown in Figure 7 and for "Dense 2" -- in the supplementary material. Note that we use the same bonus weight in this task as in other DMLab tasks before. All methods work similarly besides the Grid Oracle in the "Dense 2" task -- which performs slightly worse. Video inspection2 reveals that Grid Oracle -- the only method which has ground-truth knowledge about area it covers during training -- sometimes runs around excessively and occasionally fails to collect all good objects.
5 DISCUSSION
Our method is at the intersection of multiple topics: curiosity, episodic memory and temporal distance prediction. In the following, we discuss the relation to the prior work on those topics.
Curiosity in visually rich 3D environments. Recently, a few works demonstrated the possibility to learn exploration behaviour in visually rich 3D environments like DMLab (Beattie et al., 2016) and VizDoom (Kempka et al., 2016). (Pathak et al., 2017) trains a predictor for the embedding of the next observation and if the reality is significantly different from the prediction -- rewards the agent. In that work, the embedding is trained with the purpose to be a good embedding for predicting action taken between observations -- unlike an earlier work (Stadie et al., 2015) which obtains an embedding from an autoencoder. It was later shown by (Burda et al., 2018) that the perceptive prediction approach has a downside -- the agent could become a "couch-potato" if given an action to switch TV channels. This observation is confirmed in our experiments by observing a persistent firing behaviour of the ICM baseline in the navigational tasks with very sparse or no reward. By contrast, our method does not show this behaviour. Another work (Fu et al., 2017) trains a temporal distance predictor and then uses this predictor to establish novelty: if the observation is easy to classify versus previous observations, it is novel. This method does not use episodic memory, however, and the predictor is used in way which is different from our work.

9

Under review as a conference paper at ICLR 2019
General curiosity. Curiosity-based exploration for RL has been extensively studied in the literature. For an overview, we refer the reader to the works (Oudeyer & Kaplan, 2009; Oudeyer et al., 2007). The most common practical approaches could be divided into three branches: predictionerror-based, count-based and goal-generation-based. Since the prediction-based approaches were discussed before, in the following we focus on the latter two branches.
The count-based approach suggests to keep visit counts for observations and concentrate on visiting states which has been rarely visited before -- which bears distant similarity to how we use episodic memory. This idea is natural for discrete observation spaces and has solid theoretical foundations. Its extension to continuous observation spaces is non-trivial, however. The notable step in this direction was taken by works (Bellemare et al., 2016; Ostrovski et al., 2017) which introduce a trained observation density model which is later converted to a function behaving similarly to counts. The way conversion is done has some similarity to prediction-error-based approaches: it is the difference of the density in the example before and after training of this example which is converted to count. The experiments in the original works operate on Atari games (Bellemare et al., 2013) and were not benchmarked on visually rich 3D environments. Another approach (Tang et al., 2017) discretises the continuous observation space by hashing and then uses the count-based approach in this discretised space. This method is appealing in its simplicity, however, the experiments in (Pathak et al., 2017; Fu et al., 2017) show that it does not perform well in visually rich 3D environments.
Finally, our concept of novelty through reachability is reminiscent of generating the goals which are reachable but not too easy -- a well-studied topic in the prior work. The work (Held et al., 2017) uses a GAN to differentiate what is easy to reach from what is not and then generate goals at the boundary. Another work (Baranes & Oudeyer, 2013) defines new goals according to the expected progress the agent will make if it learns to solve the associated task. The recent work (Pe´re´ et al., 2018) learns an embedding for the goal space and then samples increasingly difficult goals from that space. In a spirit similar to those works, our method implicitly defines goals that are at least some fixed number of steps away by using the reachability network. However, our method is easier to implement than other goal-generation methods and quite general.
Episodic memory. Two recent works (Blundell et al., 2016; Pritzel et al., 2017) were inspired by the ideas of episodic memory in animals and proposed an approach to learn the functioning of episodic memory along with the task for which this memory is applied. Those works are more focused on repeating successful strategies than on exploring environments -- and are not designed to work in the absence of task rewards.
Temporal distance prediction. The idea to predict the distance between video frames has been studied extensively. Usually this prediction is an auxiliary task for solving another problem. (Sermanet et al., 2017) trains an embedding such that closer in time frames are also closer in the embedding space. Multiple works (Fu et al., 2017; Savinov et al., 2018; Aytar et al., 2018) train a binary classifier for predicting if the distance in time between frames is within a certain threshold or not. While (Sermanet et al., 2017; Aytar et al., 2018) use only the embedding for their algorithms, (Fu et al., 2017; Savinov et al., 2018) also use the classifier trained together with the embedding. As mentioned earlier, (Fu et al., 2017) uses this classifier for density estimation instead of comparison to episodic memory. (Savinov et al., 2018) does compare to the episodic memory buffer but solves a different task -- given an already provided exploration video, navigate to a goal -- which is complementary to the task in our work.
6 CONCLUSION
In this work we propose a new model of curiosity based on episodic memory and the ideas of reachability. This allows us to overcome the known "couch-potato" issues of prior work and outperform the previous curiosity state-of-the-art method ICM in visually rich 3D environments. In the future, we want to make policy aware of memory not only in terms of receiving reward, but also in terms of acting. Can we use memory content retrieved based on reachability to guide exploration behaviour in the test time? This could open opportunities to learn exploration in new tasks in a few-shot style -- which is currently a big scientific challenge.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, and Nando de Freitas. Playing hard exploration games by watching youtube. arXiv preprint arXiv:1805.11592, 2018.
Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49­73, 2013.
Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Ku¨ttler, Andrew Lefrancq, Simon Green, V´ictor Valde´s, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pp. 1471­1479, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253­279, 2013.
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.
Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.
Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2577­2587, 2017.
David Held, Xinyang Geng, Carlos Florensa, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366, 2017.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pp. 1109­ 1117, 2016.
Michal Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jas´kowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In Computational Intelligence and Games (CIG), 2016 IEEE Conference on, pp. 1­8. IEEE, 2016.
Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Re´mi Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.
Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers in neurorobotics, 1:6, 2009.
Pierre-Yves Oudeyer, Frederic Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265­286, 2007.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by selfsupervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.
Alexandre Pe´re´, Se´bastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised learning of goal spaces for intrinsically motivated goal exploration. arXiv preprint arXiv:1803.00781, 2018.
Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. arXiv preprint arXiv:1703.01988, 2017.
Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time-contrastive networks: Self-supervised learning from video. arXiv preprint arXiv:1704.06888, 2017.
11

Under review as a conference paper at ICLR 2019 Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with
deep predictive models. arXiv preprint arXiv:1507.00814, 2015. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip
DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2753­2762, 2017. Sergey Zagoruyko and Nikos Komodakis. Learning to compare image patches via convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4353­4361, 2015.
12

Under review as a conference paper at ICLR 2019

SUPPLEMENTARY MATERIAL
The supplementary material is organized as follows. First, we provide training details for R-network. Second, we list hyperparameter values and the details of hyperparameter search for all methods. Then we show experimental results which suggest that R-network can generalize between environments: we transfer one general R-network from all available DMLab30 levels to our tasks of interest and also transfer R-networks between single environments. After that, we present the results from a stability/ablation study which suggests our method is stable with respect to its most important hyperparameters and the components we used in the method are actually necessary for its performance (and measure their influence). Finally, we provide the training curves for the "Dense 2" task in the main text.
S1 REACHABILITY NETWORK TRAINING DETAILS
For training R-network, we use mini-batches of 64 observation pairs (matched within episodes). The training is run for 50K mini-batch iterations for VizDoom and 200K mini-batch iterations for DMLab. At the beginning of every pass through the buffer, we re-shuffle it. We use Adam optimizer with learning rate 10-4. The R-network uses a siamese architecture with two branches (see Figure 2 in the main text), each branch is Resnet-18 with 512 outputs, with a fully-connected network applied to the concatenated output of the branches. The fully-connected network has four hidden layers with 512 units, batch-normalization and ReLU is applied after each layer besides the last one, which is a softmax layer. Observations are RGB-images with resolution 160 × 120 pixels.
S2 HYPERPARAMETERS
The hyperparameters of different methods are given in Table S1 for DMLab environment and in Table S2 for VizDoom environment. The hyperparameters for DMLab are tuned on the "Sparse" environment for all methods -- because all methods work reasonably on this environment (it is unfair to tune a method on an environment where it fails and also unfair to tune different methods on different environments). We use the PPO algorithm from the open-source implementation3. For implementation convenience, we scale both the bonus and the task reward (with a single balancing coefficient it would not be possible to turn off one of those rewards).

Table S1: Hyper-parameters used for DMLab environment.

learning rate PPO entropy coefficient Task reward scale Curiosity bonus scale ICM forward inverse ratio ICM curiosity loss strength EC memory size

Plain PPO
0.00019 0.0011
1.0 0.0 -

ICM
0.00025 0.0042
1.0 0.55 0.96 64
-

Grid Oracle
0.00025 0.0066
1.0 0.052
-

EC
0.00025 0.0021
1.0 0.030
200

3 https://github.com/openai/baselines 13

Under review as a conference paper at ICLR 2019

Table S2: Hyper-parameters used for VizDoom environment.

learning rate PPO entropy coefficient Task reward scale Curiosity bonus scale ICM forward inverse ratio ICM curiosity loss strength EC memory size

Plain PPO
0.00025 0.01 5.0 0.0 -

ICM
0.00025 0.01 5.0 0.01 0.2 10 -

Grid Oracle
0.00025 0.01 5.0 1.0 -

EC
0.00025 0.01 5.0 1.0 200

S3 R-NETWORK GENERALIZATION STUDY
One of the promises of our approach is its potential ability to generalize between tasks. In this section we verify if this promise holds4 .
S3.1 TRAINING R-NETWORK ON ALL DMLAB-30 TASKS
Could we train a single R-network for all available levels -- and then use this general network for all our tasks of interest? Since different games have different dynamics models, the notion of closely reachable or far observations also changes from game to game. Can R-network successfully handle this variability? Table S3 suggests that using a single R-network slightly hurts the performance compared to using an R-network trained specifically for the task (Baseline 2 in the table). However, it still definitely helps solving the task (both in the "No Reward" and "Very Sparse" setting) compared to using the baseline PPO (Baseline 1 in the table). The R-network is trained using 10M environment interactions equally split across all 30 DMLab-30 tasks.

Table S3: Reward on the tasks "No Reward" and "Very Sparse" after 14.5M training steps using a single R-network. Two baselines (PPO and PPO + EC) are also provided.

Method
PPO + EC with single R-network
Baseline 1 - PPO Baseline 2 - PPO + EC

No Reward
357.8 ± 9.4
166.7 ± 20.1 470.2 ± 11.6

Very Sparse
15.8 ± 0.8
7.2 ± 3.4 23.8 ± 1.7

S3.2 TRAINING R-NETWORK ON ONE LEVEL AND TESTING ON ANOTHER
This experiment is similar to the previous one but in a sense is more extreme. Instead of training on all levels (including the levels of interest and other unrelated levels), can we train R-network on just one task and use if for a different task? Table S4 suggests we can obtain reasonable performance by transferring the R-network between similar enough environments. The performance is unsatisfactory only in one case (using the R-network trained on "Dense 2"). Our hypothesis is that the characteristics of the environments are sufficiently different in that case: single room versus maze, static textures on the walls versus changing textures.
S4 STABILITY/ABLATION STUDY
The experiments are done both in "No Reward" and "Very Sparse" environments. The "No Reward" environment is useful to avoid the situations where task reward would hide important behavioural
4Some of the generalization and ablation studies are ongoing: results are reported before 20M steps. We do not expect the conclusions to change significantly though. Results will be updated accordingly in the final version of the paper.
14

Under review as a conference paper at ICLR 2019

Table S4: Reward on the tasks "No Reward" (after 20M steps) and "Very Sparse" (after 10.5M steps) when the R-network is trained on different tasks (rows).

R-network training environment
Dense 1 Dense 2 Sparse + Doors
Very Sparse

No Reward
332.7 ± 6.0 42.0 ± 1.9 375.5 ± 6.0
438.7 ± 11.4

Very Sparse
14.1 ± 1.8 0.8 ± 0.5 12.3 ± 1.1
19.8 ± 1.7

differences between different flavors of our method (this "hiding" effect can be easily observed for different methods comparison in the dense reward tasks -- but the influence of task reward still remains even in sparser cases). As in the main text, for the "No Reward" task we report the Grid Oracle reward as a discrete approximation to the area covered by the agent trajectories.
S4.1 POSITIVE EXAMPLE THRESHOLD IN R-NETWORK TRAINING
Training the R-network requires a threshold k to separate negative from positive pairs. The trained policy implicitly depends on this threshold. Ideally, the policy performance should not be too sensitive to this hyper-parameter. We conduct a study where the threshold is varied from 2 to 10 actions (or equivalently from 8 to 40 frames as each action is repeated 4 times). Table S5 shows that the EC performance is reasonably robust to the choice of this threshold both in the "No Reward" and the "Very Sparse" tasks.

Table S5: Reward in the "No Reward" and "Very Sparse" tasks after 10.5M steps using different positive example thresholds k when training the R-network.

Threshold k
2 3 4 5 7 10

No Reward
422.3 ± 12.6 381.1 ± 7.7 436.2 ± 7.0 483.1 ± 12.6 480.1 ± 5.6 464.9 ± 7.2

Very Sparse
18.9 ± 1.6 19.2 ± 0.7 24.4 ± 1.4 20.7 ± 0.7 23.4 ± 2.0 17.9 ± 1.4

S4.2 MEMORY SIZE IN EC MODULE
The EC-module relies on an explicit memory buffer to store the embeddings of past observations and define novelty. One legitimate question is to study the impact of the size of this memory buffer on the performance of the EC-module. As observed in table S6, the memory size has very little impact on the performances of the EC algorithm in terms of area coverage ("No Reward" task) and little impact in the "Very Sparse" task.

Table S6: Reward for different values of the memory size for the two tasks "No Reward" (after 20M training steps) and "Very Sparse" (after 11.5M training steps).

Memory size
100 200 350 500

No Reward
440.5 ± 6.0 448.0 ± 6.3 444.6 ± 6.3 452.3 ± 6.0

Very Sparse
18.0 ± 1.9 17.7 ± 1.8 21.2 ± 1.3 21.1 ± 1.8

15

Under review as a conference paper at ICLR 2019

S4.3 ENVIRONMENT INTERACTION BUDGET FOR TRAINING R-NETWORK
The sample complexity of our EC method includes two parts: the sample complexity to train the Rnetwork and the sample complexity of the policy training. In the worst case ­ when the R-network does not generalize across environments ­ the R-network has to be trained for each environment and the total sample complexity is then the sum of the previous two sample complexities. It is then crucial to see how many steps are needed to train R-network such that it can capture the notion of reachability. R-network trained using a number of environment steps as low as 1M already gives good performances both when the policy is trained in the "No Reward" setting and also in the "Very Sparse" task, see Table S7.

Table S7: Reward of the policy trained on the "No Reward" and "Very Sparse" tasks with a Rnetwork trained using a varying number of environment interactions (from 100K to 5M).

Interactions
100K 300K 1M 2.5M 5M

No Reward
355.9 ± 14.2 353.5 ± 6.6 386.0 ± 14.3 417.0 ± 8.3 443.7 ± 4.9

Very Sparse
12.8 ± 0.8 16.2 ± 0.6 19.3 ± 0.8 24.0 ± 1.8 18.9 ± 1.3

S4.4 IMPORTANCE OF TRAINING DIFFERENT PARTS OF R-NETWORK
The R-network is composed of an Embedding network and a Comparator network. How important is the quality of the embedding network for the final performance of our method? In particular, can we use a random embedding while still getting good results? According to the results in Table S8, we get a reasonable performance in the "No Reward" environment. In the the "Very Sparse" environment, the performance decreases compared to our method with a tuned embedding (Baseline 2) but still provides a significant gain compared to plain PPO.

Table S8: Reward on the "No Reward" and "Very Sparse" tasks after 20M training steps using a random embedding as part of the R-network.

Method
PPO + EC with random embedding
Baseline 1 - PPO Baseline 2 - PPO + EC

No Reward
386.7 ± 15.3
191.1 ± 11.4 448.0 ± 6.3

Very Sparse
14.9 ± 1.2
8.6 ± 4.3 24.7 ± 2.2

S5 ADDITIONAL DMLab TRAINING CURVES

10.0

7.5 Figure S1: Reward as a function of training step

5.0

for the DMLab task "Dense 2". Higher is better. We shift the curves for our method by the number

2.5 PPO of environment steps used to train R-network --

0.0

PPO + ICM PPO + EC (ours)

so the comparison between different methods is fair. We run every method 10 times and show all

Num0 ber of t5raining1s0teps (in15millions) runs. No seed tuning is performed.

We show additional training curves from the main text experimental section in Figure S1.

16

Episode rewards

