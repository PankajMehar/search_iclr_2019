Under review as a conference paper at ICLR 2019

TOWARDS UNDERSTANDING REGULARIZATION IN BATCH NORMALIZATION

Anonymous authors Paper under double-blind review
ABSTRACT
Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second, learning dynamics of BN and the regularization show that training converged with large maximum and effective learning rate. Third, generalization of BN is explored by using statistical mechanics. Experiments demonstrate that BN in convolutional neural networks share the same traits of regularization as the above analyses.

1 INTRODUCTION

Batch Normalization (BN) is an indispensable component in many deep neural networks (He et al., 2016; Huang et al., 2017). Experimental studies (Ioffe & Szegedy, 2015) suggested that BN improves convergence and generalization by enabling large learning rate and preventing overfitting in training. Understanding BN theoretically is a key question.

Notations. This work denotes a scalar and a vector by using lowercase letter (e.g. x) and bold

lowercase letter (e.g. x) respectively. BN is investigated in a single-layer perceptron that is a building

block of deep models, consisting of a kernel layer, a BN layer, and a nonlinear activation function. Its

forward computation can be written by

y = g(h^), h^ =  h - µB +  and h = wTx,

(1)

B

where g(·) denotes an activation function such as ReLU, h and h^ are the hidden values before and

after normalization, w and x are kernel weight vector and network input respectively. In BN, µB and B represent mean and standard deviation of h. They are estimated within a batch of M samples.  is a scale parameter and  is a shift parameter.

Despite the simplicity of the above basic network, it builds up the blocks of deep networks. It has been widely adopted in theoretical studies such as proper initialization (Krogh & Hertz, 1992; Advani & Saxe, 2017), dropout (Wager et al., 2013), weight decay and data augmentation (Bo¨s, 1998). Our analyses assume that neurons at the BN layer are independent similar to (Salimans & Kingma, 2016; van Laarhoven, 2017), as the mean and the variance are estimated individually for each neuron. But we get rid of Gaussian assumption on the network input and the weight vector in theorem 1 that is our main result, meaning our assumption is milder than those in (Yoshida et al., 2017; Ba et al., 2016; Salimans & Kingma, 2016). Overall, several frequently-used notations are summarized in Table 2 in Appendix for reference.

1.1 HIGHLIGHTS OF RESULTS Out main results are organized below in three aspects. · First, Sec.2 decomposes BN into population normalization (PN) and gamma decay, which is an explicit regularization form of µB and B. These statistics have different impacts: (1) µB discourages reliance on a single neuron and encourages different neurons to have equal magnitude, in the sense that corrupting individual neuron does not harm generalization. This phenomenon was also found empirically in a recent work (Morcos et al., 2018), but has not been established analytically. (2) B reduces kurtosis of the input distribution as well as correlations between neurons. (3) The

1

Under review as a conference paper at ICLR 2019

regularization strengths of these statistics are inversely proportional to the batch size M , indicating that BN with large batch would decrease generalization. (4) Removing either one of µB and B could imped convergence and generalization. · Second, by using ordinary differential equations (ODEs), Sec.3 shows that gamma decay enables the network trained with BN to converge with large maximum and effective learning rate, leading to faster training speed compared to the network trained without BN or trained with weight normalization (WN) (Salimans & Kingma, 2016) that is a counterpart of BN. · Third, Sec.4 compares generalization errors of BN, WN, and vanilla SGD by using statistical mechanics. The "large-scale" regime is of interest, where number of samples P and number of neurons N are both large but their ratio P/N is finite. In this regime, the generalization errors are quantified both analytically and empirically. Numerical results in Sec.5 show that BN in CNNs has the same traits of regularization as disclosed by the above analyses.

2 A PROBABILISTIC INTERPRETATION OF BN

We treat µB and B as random variables to derive regularization of BN. Since one sample x is seen many times in training and at each time it is presented with other samples in a batch that is drawn randomly, µB and B can be treated as injected random noise for x.

Loss Function. Training a neural network typically involves minimizing a negative log likelihood function with respect to a set of parameters  = {w, , }. Then the loss function can be defined by

1P P

(h^j) = - 1 P

P

log p(yj |h^j ; ) +  w 22,

j=1

j=1

(2)

where p(yj|h^j; ) represents the likelihood function of the network and P is number of training

samples. As Gaussian distribution is often employed as prior distribution for the weight parameters,

we have a weight decay 

w

2 2

(Krizhevsky

et

al.,

2012)

that

is

a

popular

technique

in

deep

learning.

Prior. By following (Teye et al., 2018), we find that BN also induces Gaussian priors for µB and

B. We have µB  population mean and

Nsta(nµdPar,dMP2de)vaiantdionBrespeNcti(vePly,,

4a+Mn2d),

where M is kurtosis

is batch size, that measures

µP the

and P are peakedness

of the distribution of h. These priors tell us that µB and B would produce Gaussian noise in training. There is a tradeoff regarding this noise. For example, when M is small, training could diverge due

to large noise. This is supported by experiment of BN (Wu & He, 2018) where training diverges

when M = 2 in ImageNet (Russakovsky et al., 2015). When M is large, the noise is reduced because

µB and B get close to µP and P . It is known that M > 30 would provide a moderate noise, as the sample statistics converges in probability to the population statistics by the weak Law of Large

Numbers. This is also supported by experiment (Ioffe & Szegedy, 2015) where BN with M = 32

already works well in ImageNet.

2.1 A REGULARIZATION FORM

The loss function in Eqn.(2) can be written as an expected loss by integrating over the priors of µB

and B, impose

rtehgautliasr,izP1ationPj=o1nEthµeB

,B [ (h^j)] where E[·] denotes expectation. We show that µB and scale parameter , but result in different regularization strengths.

B In

theorem 1, we employ ReLU activation function as a concrete example that is widely used in practice.

In general, the results can be extended to the other activation functions as shown in Appendix C.2.

Theorem 1 (Regularization of µB, B). Let (h^) be the loss function of BN and the activation

function be ReLU. Then

1 P

P
EµB,B (h^j )

j=1

1P P

(h¯j) + (h)2

and

+2

11

(h) = 8M F + 2M P

P

(h¯ j ),

j=1

j=1

(3)

from B

from µB

where

h¯ j

=

 hj -µP
P

+

represents

the

population

normalization

(PN)

and

hj

=

wTxj .

 (h)

is

a data-dependent coefficient of gamma decay,  is the kurtosis of distribution of h, F represents

Fisher Information Matrix (FIM) of , and (·) is a sigmoid function.

2

Under review as a conference paper at ICLR 2019

From theorem 1, we have several observations that are of both theoretical and practical values.

· First, it decomposes BN into population normalization (PN) and gamma decay. PN replaces the batch statistics in BN by population statistics. In gamma decay, computation of (h) is datadependent, making it differed from weight decay where the coefficient is determined empirically. In essentials, Eqn.(3) represents the randomness in BN in a deterministic way, not only enabling us to apply methodologies such as ODEs and statistical mechanics to analyze BN, but also inspiring us to imitate BN's performance by WN without computing batch statistics in numerical study.

· Second, PN is closely connected to WN that is independent from sample mean and variance. WN

(Salimans & Kingma, 2016) is variance, where  is a learnable

defined by parameter.

Le|tw|weTa|x|c2htdhiaatgnoonraml ealleizmeesntthoefwtheeigchotvvaericatnocretomhatarvixe

unit of x

be a and all the off-diagonal elements be zeros. h¯j can be rewritten as

h¯ j

=

 wT xj - µP P

+

=

wT xj  ||w||2

+ b,

(4)

where  analyses

= of

 a

and

b

=

-

regularization

afo||rwµPB||2N+.

.

Eqn.(4)

removes

the

estimations

of

statistics

and

eases

our

· Third, µB and B produce different parts in (h). The strength from µB depends on the expectation of (h¯j)  [0, 1], which represents excitation or inhibition of a neuron, meaning that a neuron with larger output may exposure to larger regularization, encouraging different neurons to have equal magnitude. This is consistent with empirical result (Morcos et al., 2018) which prevented reliance on single neuron to improve generalization. The strength from B works as a complement for µB. For a single neuron, F represents the norm of gradient, implying that BN punishes large gradient norm. For multiple neurons, F is the FIM of , meaning that BN would penalize correlations among neurons. Both B and µB are important, removing either one of them would imped performance. We observe in experiments in Sec.5 that BN in CNNs share similar traits of regularization. However, in deep models the priors for B and µB become multivariate Gaussian distributions where relationships between layers may not be neglected. In this case, we didn't find meaningful analytical form for BN.

3 OPTIMIZATION WITH REGULARIZATION

We show that BN converges with large maximum and effective learning rate (lr) that are larger than a network trained without BN. Our result explains why large lr can be used in practice in BN (Ioffe & Szegedy, 2015). Our analyses are conducted in three stages. First, we establish dynamical equations of a teacher-student (T-S) model in thermodynamic limit and acquire the fixed point. Second, we investigate eigenvalues of the corresponding Jacobian matrix at this fixed point. Finally, we calculate the maximum and the effective lr.

Teacher-Student Model. We first introduce useful techniques from statistical mechanics (SM). In SM, a student network is dedicated to learn relationship between an input and an output with a weight vector w as parameters. It is useful to characterize behavior of the student by using a teacher network that uses w as a ground-truth vector. We treat the single-layer perceptron as the student, which is optimized by minimizing the euclidian distance between its output and the supervision provided by a teacher without BN. The student and the teacher have identical activation function.

Loss Function. We

1 P

P j=1

g(wTxj ) - 

teacher, while g( N 

define a loss

 g( N



wT xj w2

)

function 2 + 2.

of

the

above

T-S

model

by

1 P

P j=1

Here g(wTxj) represents supervision

(xj ) from

= the

wT xj w2

)

is 

the

output

of

student

trained

to

mimic

its

teacher.

The

student

is

defined by Eqn.(4) where  = N  and the bias term is merged into w. This loss function represents

BN using WN with gamma decay and it is sufficient to study the lr of different approaches. Let

 = {w, } be a set of parameters updated by learning rate. The update rules for w and  are

SGD,

i.e.

j+1

=

j

-





(xj j

)

where



denotes

a



wj+1

-

wj

=

j

(

j wj

N
2

xj

-

w~ jTxj

wj

2 2

wj )

and

 j +1

-

j

=

j (

N wjTxj wj 2

- j),

(5)

3

Under review as a conference paper at ICLR 2019



where w~ denotes a normalized weight vector of the student, that is, w~ =

N

w w

,
2

and

j

=

g (w~ jTxj)[g(wTxj) - g(w~ jTxj)] represents the gradient1 for clarity of notation.

Order Parameters. As we are interested in the "large-scale" regime where both N and P are

large and their ratio P/N is finite, it is difficult to examine a student with parameters in high

dimensions directly. Therefore, we transform the weight vectors to order parameters that fully

characterize interactions between the student and the teacher. In this case, the parameter vector can be

reparameterized by using a vector of three elements including , R, and L. In particular,  measures

norm of the normalized weight vector w~ , that is, w~ Tw~ measures angle (overlapping ratio) between the weight

= N2 vectors

owfwTswt22ud=enNt an2d.

The parameter R teacher. We have

R=

w~ Tw w~ w

=

1 N

w~ T

w

where

the

norm

of

the

ground-truth

vector

is

1 N

wT

w

=

1.

Moreover,

L represents norm of the

original

weight

vector w

and

L2

=

1 N

wT

w.

With the

above

definitions,

relationship

between

R

and

L

can

be

represented

by

RL

=

1 N

wTw.

3.1 LEARNING DYNAMICS OF ORDER PARAMETERS

Now we transform update equations (5) by using order parameters. To this end, we define three

variables 2, L2 and RL. The update rule for variable 2 can be obtained by 2 j+1 - 2 j =

1 N

2jw~ jTxj - 2

2

j

following update rule of . Similarly, the update rules for variables

L2 and RL are

RL j+1 -

RL

j

=

1 N

j Lj

j

wTxj

-

Rj Lj



j

w~ jT

xj

and

L2 j+1 -

L2 j =

1 N

 2 ( 2 )j (L2 )j

j

2xjTxj

-

N

2 (L2

)j

j

2

(w~ jT

xj

)2

by multiplying both sides of (5) by w.

To define the learning dynamics, we turn the above update rules into ODEs. We take 2 as an example.

Its differential equation can be defined by

d2 dt

= limt0

( 2 )j +1 -( 2 )j t

=

2

w~ Tx

x - 22,

where t

=

j N

is a normalized sample index that can be treated as a continuous time variable.

We have

t

=

1 N

that

approaches

zero

in

the

thermodynamic

limit

when

N



.

· x denotes expectation

over

the

distribution

of

x.

The

differential

equations

of

dRL dt

and

dL2 dt

can

be

defined

in

the

same

way. We simplify notations by representing I1 = w~ Tx x, I2 = 2xTx x and I3 = wTx x.

We obtain a dynamical system

d =  I1 - , dt 

dR dt

=

  L2 I3

-

R  L2 I1

-

2

2R 2L4

I2

and

dL dt

=

2

2 2L3

I2.

More results are provided in Appendix C.4.

(6)

3.2 FIXED POINT OF THE DYNAMICAL SYSTEM

To investigate lr of BN, we derive the fixed point of (6) by setting d/dt = dR/dt = dL/dt = 0. The fixed points of BN, WN, and vanilla SGD (without BN and WN) are given in Table 1. In the thermodynamic limit, the optima for (0, R0, L0) would be (1, 1, 1). Our main interest is the overlapping ratio R0 between the student and the teacher, because it optimizes the

(0, R0, L0) BN (0, 1, L0)
WN (1, 1, L0) SGD (1, 1, 1)

max (R)

(0I3 -I1) 0 R

-

 0

/

I2 2R

 (I3 -I1 R

)

/

I2 2R

 (I3 -I1 R

)

/

I2 2R

eff (R)
0 L20
 L02


Table 1: Comparisons of fixed points, max for

direction of the weight vector. We see that R0 for all R, and eff for R. A fixed point is denoted as three approaches attain optimum `1'. Intuitively, in BN (0, R0, L0).

and WN, this optimal solution does not depend on L0 because their weight vectors are normalized. In other words, WN and BN are easier to optimize

than vanilla SGD where both R0 and L0 have to be optimized. In BN, 0 depends on the activation

function.

For

ReLU,

we

have

0bn

=

1 2+1

,

meaning

that

norm

of

the

normalized

weight

vector

relies

on the decay factor. In WN, we have 0wn = 1 as WN has no regularization on .

3.3 MAXIMUM AND EFFECTIVE LEARNING RATES

With the above fixed points, we derive the maximum and the effective lr. Specifically, we analyze eigenvalues and eigenvectors of the Jacobian matrix corresponding to (6). We are interested in the

1g (x) denotes the first derivative of g(x).

4

Under review as a conference paper at ICLR 2019

lr to approach R0. We find that this optimum value only depends on its eigenvalue denoted as R,

R

=

I2 R

Appendix

C2L.4020)(.Tmhaexy-areeffgi)v, ewnhienreTabmleax1.anWdedeeffmaorensmtraaxteimthuamt aRnd<e0ffeifctainvde

lr (proposition only if max >

1 in eff ,

such that the fixed point R0 is stable for all approaches (proposition 2 in Appendix C.5). It is able

to show that max of BN (mbnax) is larger than WN and SGD, enabling R to converge with a larger

learning rate. Moreover, the

With ReLU, effective lr's

we find in Table

1thaartecmbonanxsistentm{wwanxit,shgdp}re+vi2ous(pwroorpko(svitainonLa3airnhoAvpenp,e2n0d1ix7)C. .6).

4 GENERALIZATION ANALYSIS

To investigate generalization of BN, we adopt a teacher-student model with identity activation

function, which minimizes output and y is the student's

aoultopsust.fuWnectcioonmpP1are

jP=1(y¯j - yj)2, where y¯ represents the BN with WN+gamma decay and vanilla

teacher's SGD. All

of them share the same teacher network whose output is defined by y¯ = wTx + , where x is drawn

frreosmistNth(is0,noN1is)ea.nd is an unobserved Gaussian noise. We are interested to see how different methods

For vanilla SGD, the student is computed by y = wTx with w being the weight vector to optimize, where w has the same dimension as w to be a realizable learning problem. The loss function

of vanilla SGD is

sgd

=

1 P

P j=1

(wTxj

-

wTxj )2,

whose

solution

asymptotically

approaches

the Moore­Penrose pseudo inverse solution w = xTx + xTy¯. For BN, the student is defined as

y

=

 wTx-µB
B

+

.

As

our

main

interest

is

the

weight

vector,

we

freeze

the

bias

similar

to

vanilla

SGD by setting  = 0. Therefore, the µB)/B 2. For WN+gamma decay,
Then the loss function is defined by

loss function is written as bn =

the student is computed similar

wn

=

1 P

P j=1

 wTxj - N

1 P
to

P j=1

wTxj - (wT 

Eqn.(4) by y = N 

xj
wT w

- x.
2

 wTxj
w2

2+



22. In the T-S

Wmoitdhetlhweiathboidveendtietyfinuintiiot,nesx, pthreestshioreneoafpprboeaccohmeessare=stu2dM1iedafutenrdaeprptlhyeinsgamtheeoTr-eSmm1od(Aelp,pwehnedrixe

C.3). their

generalization errors can be strictly compared with the other factors ruled out.

4.1 GENERALIZATION ERRORS

We provide closed-form solutions of the generalization errors for vanilla SGD and WN+gamma decay. They are compared with numerical solutions of BN.

gen gen

vanilla SGD. The solution of generalization error depends on

the rank of correlation matrix  = xTx. Here we define an

effective load  = P/N that is the ratio between number of

samples P and number of input neurons N (number of learnable

parameters). The acquired by using

generalization the distribution

error denoted of eigenvalues

oafssggfedonllcoawninbge

(Advani & Saxe, 2017). If  < 1,

sgd gen

=

1-+

2/(1

- ).

Otherwise,

sgd gen

=

2/(1 - ) where

is the injected noise to

the teacher. The in blue curve in

values of the top of

Fsggeidgn.1w. itIht

respect to  are plotted first decreases but then

increases as and it would

 increases from 0 to 1, decrease again when  >

sgd
1ge. n

diverges

at



=

1,

WN+gamma decay. The decay term turns the correlation

matrix to  = xTx + I that is positive definite. Fol-

lowing statistical mechanics (Krogh & Hertz, 1992), the Figure 1: Top: generalization error

generalization error is

wn gen

=

2

(G) 

-



2

G 

where

G = 1--+

 + (1 + )2

1 2

 + (1 - )2

1 2

2 .

We see that of  and .

wn
Lgeent

can the

be computed quantitatively given the values variance of noise injected to the teacher

gen v.s. effective load . `WN+gamma

decay' has two cases  = 0.25. BN has M

 =

= 32.

B2M1ottoanmd:

generalization error at  = 1.

be 0.25. Fig.1 bottom shows that no other curves could outperform the curve when  = 0.25, a value

5

Under review as a conference paper at ICLR 2019

equal would

to the noise magnitude. The  smaller exhibit overtraining around  = 1, but

than they

0.25 (e.g. green curve of  = still perform significantly better

2tM1hananvdanMilla=SG3D2).

Numerical Solutions of BN. We employ SGD with M = 32 to find solutions of w for BN. The generalization error is evaluated as difference between the validation and the training loss. The

4.o -.----,-------,-----,-----,

(a)

train - BN M=64 train - BN M=256

3.0--

train - PN + gamma decay eval - BN M=64

number of input neurons is 4096 and the number of training samples can be varied to change . The results are marked  2.0

eval - BN M=256
eval --P-N-+-g-a-m-m-a--dec-a-y--------

as black squares in the top of Fig.1. After applying theorem 1 to the T-S model, BN is equivalent to WN+gamma decay

-----1.0+-tw::-..--=;;;Siwf--,c.=_--+-________..._----- t-----------------------------

wofh`en=1/=2M2'M1(M. It=is

seen that BN gets in line with the curve 32) and thus quantitatively validates our

derivations. Their generalization errors are further compared

in the bottom of Fig.1 at  = 1, where vanilla SGD clearly

diverges, while BN and WN+gamma decay are comparable.

0
(b)
u
f 80

20k 40k step

60k

5 EXPERIMENTS IN CNNS
This section shows that BN in CNNs follows similar traits of regularization as our analyses. We employ a 6-layered CNN similar to (Salimans & Kingma, 2016). For all experiments,

uu::::s
ta
·.-C0.., 75
":t'aC 70
ta 65 0

BN M=64 BN M=256 PN + gamma decay

20k

40k

step

60k

the network architecture is fixed while the normalization layers Figure 2: Training and evaluation loss

can be changed. We adopt CIFAR10 (Krizhevsky, 2009) that in (a) and validation accuracy in (b).

contains 60k images of 10 categories (50k images for training

and 10k images for test). All models are trained by using SGD with momentum on a single GPU,

while the initial learning rates are scaled proportionally for different batch sizes (Goyal et al., 2017).

In order to study regularization of BN, we discard any other trick such as weight decay and data

augmentation. More empirical setting can be found in Appendix B.

Evaluation of Theorem 1. We compare BN with PN+gamma decay where the population statistics and the regularization coefficient are estimated by using sufficient amount of training samples. BN trained with a normal batch size M = 64 is treated as baseline in Fig.2. When batch size increases, BN would imped both loss and accuracy. For example, when increasing M to 256, performance decreases because the regularization from the batch statistics reduces in large batch, resulting in overtraining (see the gap between train and validation loss when M = 256).

In comparison, we train PN by using 10k training samples to estimate statistics. This further reduces regularization. We see that the release of regularization can be complemented by gamma decay, making PN even outperformed BN. This empirical result verifies our derivation of regularization for BN. Similar trend can be observed by experiment in a downsampled version of ImageNet (see Appendix B.1). Nevertheless, we would like to point out that PN+gamma decay is of interest in theoretical analysis, but it is computation-demanding when applied in practice because evaluating µP , P and (h) may require sufficiently large number of samples.

(a) (b)

Study of Regularization. We study the regulation strengthes of vanilla SGD, BN, WN, WN+mean-only BN, and WN+varianceonly BN. Fig.3 compares their training and validation losses. We see that the generalization error of BN is much lower than WN and vanilla SGD. The reason has been disclosed in this work: stochastic behaviors of µB and B in BN improves generalization.

Figure 3: Study of regularization.

To investigate µB and B individually, we decompose their contributions by running a WN with mean-only BN as well as a WN with variance-only BN, to simulate their respective regularization. As shown in Fig.3, improvements from the mean-only and the variance-only BN over WN verify our conclusion that noises from µB and B have different regularization strengths. Both of them are essential to produce good result.

6

Under review as a conference paper at ICLR 2019

Parameter Norm. We further demonstrate impact of BN to the norm of parameters. We compare BN with vanilla SGD. A network is first trained by BN in order to converge to a local minima where the parameters do not change much. At this local minima, the weight vector is frozen and denoted as wbn. Then this network is finetuned by using vanilla SGD with a small learning rate 10-3 and its

kernel

parameters

are

initialized

by

wsgd

=



wbn 

,

where



is

the

moving

average

of

B .

Fig.7 in Appendix B.2 visualizes the results. As µB and B are removed in vanilla SGD, it is found from the last two figures that the training loss decreases while the validation loss increases, implying that reduction in regularization makes the network converged to a sharper local minimum that generalizes less well. The magnitudes of kernel parameters wsgd at different layers are also displayed in the first four figures. All of them increase after freezing BN, due to the release of regularization on these parameters.

Batch size. To study BN with different batch sizes, we train different networks but only add BN at one layer at a time. The regularization on the  parameter is compared in Fig.4 (a) when BN is located at different layers. The values of 2 increase along with the batch size M due to the weaker regularization for the larger batches. The increase of 2 also makes all validation losses increased as shown in Fig.4 (b).

(a)

BN+dropout. Despite the better generalization of BN with

smaller batch sizes, large-batch training is more efficient in (b)

real cases. Therefore, improving generalization of BN with

large batch is more desiring. However, gamma decay requires

estimating the population statistics that increases computations.

We also found that treating the decay coefficient as a constant

hardly improves generalization for large batch. Therefore, we

utilize dropout as an alternative to compensate for the insufficient

regularization. Dropout has also been analytically viewed as a

regularizer (Wager et al., 2013). We add a dropout after each

BN layer to impose regularization.

Figure 4: Values of 2 increase along

Fig.5 plots the results. The generalization of BN deteriorates with M as shown in (a), due to the

significantly when M increases from 64 to 256. This is observed lack of regularization in large batch,

by the much higher validation loss (top) and lower validation making the validation losses increased

accuracy (bottom) when M = 256. If a dropout layer with ratio as well in (b).

0.125 is added after each BN layer for M = 256, the validation

loss is suppressed and accuracy increased by a great margin. This superficially contradicts with

the original claim that BN reduces the need for dropout (Ioffe & Szegedy, 2015). There are two

differences between our study and previous work.

First, in pervious study the batch size was fixed at a quite small value (e.g. 32), at which the regularization was already quite strong. Therefore, an additional dropout could not further cause better regularization, but on the contrary increases the instability in training and yields a lower accuracy. However, our study explores relatively large batch that degrades the regularization of BN, and thus dropout with a small ratio can complement. Second, usual trials put dropout before BN and cause BN to have different variances during training and test. In contrast, dropout follows BN in this study and thus the problem can be alleviated. The improvement by applying dropout after BN has also been observed by a recent work (Li et al., 2018).

WN+dropout. Since BN can be treated as WN trained with regularization in this study, combining WN with regularization should be able to match the performance of BN. As WN outperforms BN in running speed (without calculating statistics) and it suits better in RNNs than BN, an improvement of its generalization is also of great importance. Fig.5 shows that WN

Figure 5: BN and WN with dropout.

7

Under review as a conference paper at ICLR 2019
can also be regularized by dropout. We apply dropout after each WN layer with ratio 0.25. We found that the improvement on both validation accuracy and loss is surprising. The accuracy increases from 0.73 to 0.80, surpassing `BN M=256' and on par with `BN M=64'.
6 RELATED WORK
Neural Network Analysis. Many studies analysed neural networks (Opper et al., 1990; Saad & Solla, 1996; Bs & Opper, 1998; Pennington & Bahri, 2017; Zhang et al., 2017b; Brutzkus & Globerson, 2017; Raghu et al., 2017; Mei et al., 2016; Tian, 2017). For example, for a multilayer network with linear activation function, Glorot & Bengio (2010) explored its SGD dynamics and Kawaguchi (2016) showed that every local minimum is global. Tian (2017) studied the critical points and convergence behaviors of a 2-layered network with ReLU units. Zhang et al. (2017b) investigated a teacher-student model when the activation function is harmonic. In (Saad & Solla, 1996), the learning dynamics of a committee machine were discussed when the activation function is error function erf(x). Unlike previous work, this work analyzes regularization emerged in BN and its impact to both learning and generalization, which are still unseen in the literature. Normalization. Many normalization methods have been proposed recently. For example, BN (Ioffe & Szegedy, 2015) was introduced to stabilize the distribution of input data of each hidden layer. Weight normalization (WN) (Salimans & Kingma, 2016) decouples the lengths of the network parameter vectors from their directions, by normalizing the parameter vectors to unit length. The dynamic of WN was studied by using a single-layer network (Yoshida et al., 2017). Moreover, Li et al. (2018) diagnosed the compatibility of BN and dropout (Srivastava et al., 2014) by reducing the variance shift produced by them. van Laarhoven (2017) showed that weight decay has no regularization effect when using together with BN or WN. Ba et al. (2016) demonstrated when BN or WN is employed, back-propagating gradients through a hidden layer is scale-invariant with respect to the network parameters. Santurkar et al. (2018) gave another perspective of the role of BN during training instead of reducing the covariant shift. They argued that BN results in a smoother optimization landscape and the Lipschitzness is strengthened in networks trained with BN. However, both analytical and empirical results of regularization in BN are still desirable. Our study explores regularization, optimization, and generalization of BN in the scenario of online learning. Regularization. Ioffe & Szegedy (2015) conjectured that BN implicitly regularizes training to prevent overfitting. Zhang et al. (2017a) categorized BN as an implicit regularizer from experimental results. Szegedy et al. (2015) also conjectured that in the Inception network, BN behaves similar to dropout to improve the generalization ability. Gitman & Ginsburg (2017) experimentally compared BN and WN, and also confirmed the better generalization of BN. In the literature there are also implicit regularization schemes other than BN. For instance, random noise in the input layer for data augmentation has long been discovered equivalent to a weight decay method, in the sense that the inverse of the signal-to-noise ratio acts as the decay factor (Krogh & Hertz, 1992; Rifai et al., 2011). Dropout (Srivastava et al., 2014) was also proved able to regularize training by using the generalized linear model (Wager et al., 2013).
7 DISCUSSIONS AND FUTURE WORK
This work investigated regularization emerged in BN. By utilizing a single-layer perceptron, BN was decomposed into PN and gamma decay, where the regularization strengths from µB and B are different and their impacts in training were explored. Moreover, convergence and generalization of BN with regularization were derived and compared with vanilla SGD, WN, and WN+gamma decay, showing that BN enables training to converge with large maximum and effective learning rate, as well as leads to better generalization. Our analytical results explain many existing empirical phenomena. Experiments in CNNs showed that BN in deep networks share the same traits of regularization. Furthermore, a combination of dropout and BN might ameliorate BN when batch size goes large. Our result also encourages us to combine WN and dropout, outperforming BN in some senses without estimating batch statistics. In future work, we are interested in finding analytical form of regularization for BN in deep networks, although it might involve multivariate Gaussian prior distributions, making it a nontrivial problem. Moreover, investigating the other normalizers such as instance normalization (IN) (Ulyanov et al., 2016) and layer normalization (LN) (Ba et al., 2016) is also important. Understanding
8

Under review as a conference paper at ICLR 2019
the characteristics of these normalizers should be the first step to analyze some recent best practices such as group normalization (Wu & He, 2018) that merged IN and LN, and switchable normalization (Luo et al., 2018) that chose BN, IN, and LN in each normalization layer. Furthermore, devising an efficient counterpart of gamma decay is desirable in the community and will be investigated in the future, as it may improve generalization of WN that is independent of batch statistics.
REFERENCES
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv:1710.03667 [physics, q-bio, stat], October 2017. URL http://arxiv.org/abs/1710.03667. arXiv: 1710.03667.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450, 2016. Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In
ICML, 2017. Siegfried Bo¨s. Statistical mechanics approach to early stopping and weight decay. Physical Review E, 58(1):833,
1998. Siegfried Bs and Manfred Opper. Dynamics of batch training in a perceptron. In Journal of Physics A:
Mathematical and General, volume 31(21), pp. 4835, 1998. Igor Gitman and Boris Ginsburg. Comparison of Batch Normalization and Weight Normalization Algorithms
for the Large-scale Image Classification. arXiv:1709.08145 [cs], September 2017. URL http://arxiv. org/abs/1709.08145. arXiv: 1709.08145. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010. Priya Goyal, Piotr Dolla´r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv preprint arXiv:1706.02677, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR, 2017. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. Kenji Kawaguchi. Deep learning without poor local minima. In NIPS, 2016. Alex Krizhevsky. Learning multiple layers of features from tiny images. In Technical Report, 2009. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. Anders Krogh and John A. Hertz. Generalization in a linear perceptron in the presence of noise. Journal of Physics A: Mathematical and General, 25(5):1135, 1992. Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between dropout and batch normalization by variance shift. In arXiv:1801.05134, 2018. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In arXiv:1608.03983, 2016. Ping Luo, Jiamin Ren, and Zhanglin Peng. Differentiable learning-to-normalize via switchable normalization. arXiv:1806.10779, 2018. Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. In arXiv:1607.06534, 2016. Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. In ICLR, 2018.
9

Under review as a conference paper at ICLR 2019
M. Opper, W. Kinzel, J. Kleinz, and R. Nehl. On the ability of the optimal perceptron to generalise. In Journal of Physics A: Mathematical and General, volume 23(11), pp. 581, 1990.
Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random matrix theory. In ICML, 2017.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl Dickstein. On the expressive power of deep neural networks. In ICML, 2017.
Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal Vincent. Adding noise to the input of a model trained with a regularized objective. arXiv:1104.3250 [cs], April 2011. URL http://arxiv.org/abs/1104.3250. arXiv: 1104.3250.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. In ICJV, 2015.
David Saad and Sara A. Solla. Dynamics of on-line gradient descent learning for multilayer neural networks. In NIPS, 1996.
Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In arXiv:1602.07868, 2016.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). arXiv:1805.11604 [cs, stat], May 2018. URL http://arxiv.org/abs/1805.11604. arXiv: 1805.11604.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. In Journal of Machine Learning Research, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision. arXiv:1512.00567 [cs], December 2015. URL http: //arxiv.org/abs/1512.00567. arXiv: 1512.00567.
Mattias Teye, Hossein Azizpour, and Kevin Smith. Bayesian uncertainty estimation for batch normalized deep networks. In ICML, 2018.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. In ICML, 2017.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv:1607.08022, 2016.
Twan. van Laarhoven. L2 regularization versus batch and weight normalization. In arXiv:1706.05350, 2017. Stefan Wager, Sida Wang, and Percy Liang. Dropout Training as Adaptive Regularization. arXiv:1307.1493 [cs,
stat], July 2013. URL http://arxiv.org/abs/1307.1493. arXiv: 1307.1493. Yuxin Wu and Kaiming He. Group normalization. arXiv:1803.08494, 2018. Yuki Yoshida, Ryo Karakida, Masato Okada, and Shun ichi Amari. Statistical mechanical analysis of online
learning with weight normalization in single layer perceptron. In Journal of the Physical Society of Japan, 2017. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, , and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017a. Qiuyi Zhang, Rina Panigrahy, and Sushant. Sachdeva. Electron-proton dynamics in deep learning. In arXiv:1702.00458, 2017b.
10

Under review as a conference paper at ICLR 2019

APPENDICES
A NOTATIONS

Table 2: Several notations are summarized for reference.

µB, B2 µP , P2
x, y
y¯

batch mean, batch variance population mean, population variance input of a network, output of a network
ground truth of an output

h, h^ hidden value before and after BN

h¯ hidden value after population normalization ,  scale parameter, shift parameter

g(·) w, w
w~

activation function weight vector, ground truth weight vector
normalized weight vector

M, N, P  

batch size, number of neurons, sample size an effective load value  = P/N regularization strength (coefficient)

  eff , max R L max, min
gen

Kurtosis of a distribution gradient of the activation function effective, maximum learning rate overlapping ratio (angle) between w~ and w
norm (length) of w maximum, minimum eigenvalue
generalization error

B MORE EMPIRICAL SETTINGS AND RESULTS All experiments in Sec.5 are conducted in CIFAR10 with a CNN architecture similar to (Salimans & Kingma, 2016) that is summarized as `conv(3,32)-conv(3,32)-conv(3,64)-conv(3,64)-pool(2,2)fc(512)-fc(10)', where `conv(3,32)' represents a convolution with kernel size 3 and 32 channels, `pool(2,2)' is max-pooling with kernel size 2 and stride 2, and `fc' indicates a full connection. We follow a configuration for training by using SGD with a momentum value of 0.9 and continuously decaying the learning rate by a factor of 10-4 each step. For different batch sizes, the initial learning rate is scaled proportionally with the batch size to maintain a similar learning dynamics (Goyal et al., 2017).
B.1 RESULTS IN DOWNSAMPLED IMAGENET Besides CIFAR10, we also evaluate theorem 1 by employing a downsampled version of ImageNet (Loshchilov & Hutter, 2016), which contains identical 1.2 million data and 1k categories as the original ImageNet, but each image is scaled to 32×32. We train ResNet18 in downsampled ImageNet by following the training protocol used in (He et al., 2016). In particular, ResNet18 is trained by using SGD with momentum of 0.9 and the initial learning rate is 0.1, which is then decayed by a factor of 10 after 30, 60, and 90 training epochs. In downsampled ImageNet, we observe similar trends as those presented in CIFAR10. For example, we see that BN would imped both loss and accuracy when batch size increases. When increasing M to 1024 as shown in Fig.6, both the loss and validation accuracy decrease because the regularization from the random batch statistics reduces in large batch size, resulting in overtraining. This can be seen by the gap between the training and the validation loss. Nevertheless, we see that the reduction of regularization can be complemented when PN is trained with adaptive gamma decay, which makes PN performed comparably to BN in downsampled ImageNet.
B.2 IMPACT OF BN TO THE NORM OF PARAMETERS We demonstrate the impact of BN to the norm of parameters. We compare BN with vanilla SGD, where a network is first trained by BN in order to converge to a local minima when the parameters do not change much. At this local minima, the weight vector is frozen and denoted as wbn. Then this
11

Under review as a conference paper at ICLR 2019

6.0

_.l_
- t rain - BN M=32

5.5 ______ ......,,_ - train - BN M=1024 5.0 I, - - ·11 - train - PN adaptive gamma decay
eval - BN M=32
-1-+-a----+----+----+-,,/
eval - BN M=1024

&n 4.5

eval - PN adaptive gamma decay

- 4.0

3.5

3.0

2.5 0

50k 100k 150k 200k

step

(a) Comparisons of train and validation loss.

35

um30
.u::.::_s 25
 20

·.-0.., 15
-·- 10
m> 5
0

BN M=32 BN M=1024 PN+adaptive gamma decay

0

50k

100k 150k 200k

step

(b) Comparisons of validation accuracy.

Figure 6: Results of downsampled ImageNet. (a) plots training and evaluation loss. (b) shows validation accuracy. The models are trained on 8 GPUs.

network is finetuned by using vanilla SGD with a small learning rate 10-3 with the kernel parameters

initialized

by

wsgd

=



wbn 

,

where



is

the

moving

average

of

B .

Fig.7 below visualizes the results. As µB and B are removed in the vanilla SGD, it is found from the last two figures that the training loss decreases while the validation loss increases, meaning that the reduction in regularization makes the network converged to a sharper local minimum that generalizes less well. The magnitudes of kernel parameters wsgd at different layers are also displayed in the first four figures. All of them increase after freezing BN, due to the release of regularization on these parameters.

(a) (b) (c)

train

(d) (e) (f)

Figure 7: Study of parameter norm. Vanilla SGD is finetuned from a network pretrained by BN on CIFAR10.

The first four figures show the magnitude of the kernel parameters in different layers in finetuning, compared to

the effective finetuning.

norm

of

BN

defined

as



w B

.

The

last

two

figures

compare

the

training

and

validation

losses

in

12

Under review as a conference paper at ICLR 2019

C PROOF OF RESULTS
C.1 PROOF OF THEOREM 1
Theorem 1 (Regularization of µB, B). Let  be the strength (coefficient) of the regularization and the activation function be ReLU. Then

1 P

P

EµB,B (h^j )

j=1

1 P (h¯j) + 2, P
j=1

+2

11

and  = 8M F + 2M P

P

(h¯ j ),

j=1

from B

from µB

where h¯j

=

 wTxj -µP
P

+ ,  is the kurtosis of the distribution of wTx, F

is a Fisher Information

Matrix of , and (·) is a sigmoid function.

Proof.

Let h^j

=



wT

xj -µB B

+  and h¯j

=



wT

xj -µP P

+ .

We prove theorem 1 by performing a

Taylor expansion on a function A(h^j) at h¯j, where A(h^j) is a function of h^j defined according to a

particular activation function. The negative log likelihood function of the single-layer perceptron can

be generally defined as - log p(yj|h^j) = A(h^j) - yjh^j, which is similar to the loss function of the

generalized linear models with different activation functions.

1 P

P

EµB,B [l(h^j )]

=

1 P

P

EµB ,B

A(h^j) - yjh^j

j=1

j=1

1 =
P

P (A(h¯j) - yjh¯j) + 1 P

P

EµB ,B

-yj(h^j - h¯j) + A(h^j) - A(h¯j)

j=1

j=1

1 =
P

P

l(h¯j) + 1 P

P

EµB ,B

(A (h¯j) - yj)(h^j - h¯j)

j=1

j=1

1 +
P

P
EµB ,B

A (h¯j) (h^j - h¯j)2 2

j=1

1 =

P

l(h¯j) + Rf + Rq,

P

j=1

where A (·) and A (·) denote the first and second derivatives of function A(·). The first and second

order terms in the expansion are represented by Rf and Rq respectively. To derive the analytical

forms of Rf to have

and

Rq ,

we

take a

second-order

Taylor

expansion of of

1 B

and

1 B2

around P , it suffices

1 B



1 P

1 + (- P2 )(B - P ) +

1 P3

(B

-

P

)2

and

1 B2



1 P2

2 + (- P3 )(B - P ) +

3 P4

(B

-

P

)2.

13

Under review as a conference paper at ICLR 2019

By applying the distributions of µB and B in the paper, Rf can be derived as

Rf = 1 P

P

EµB ,B

(A (h¯j) - yj)(h^jh¯j)

j=1

1P

= P

EµB ,B

j=1

(A (h¯j) - yj)

wT xj 

-

µB

-  wT xj

-

µP

B P

1 =
P

P
EµB ,B

(A (h¯jyj)

wT xj

j=1

1-1 B P

+  - µB + µP B P

1 =
P

P

(A (h¯j ) - yj )(wT xj - µP )EB

j=1

1-1 B P

1 =

P



+

2 (A

(h¯ j )

-

yj) wT

xj

-

µP

.

P 4M
j=1

P

This data

Rf term can be understood as below. Let h = be pxy. We establish the following relationship

wT

x-µP P

and the distribution of the population

E(x,y)pxy EµB,B (A (h¯) - y)h = EµB,B Expx Ey|xpy|x (A (h¯) - y)h

= EµB,B Expx (E [y|x] - Ey|xpy|x [y])h

= 0.

Since the sample mean converges in probability to the population mean by the Weak Law of

Large Numbers, for all > 0 and a constant number K (K > 0 and P > K), we have

sRmfal-l gEiv(xe,ny)mpoxdyeEraµtBel,yBlar(gAe

(h¯) - y)h number of

< data

p4o+Mi2nts.

The above equation means that P (the above inequality holds

Rf is when

sufficiently P > 30).

On the other hand, Rq can be derived as

Rq = 1 P

P
EµB ,B

A (h¯j) (h^j - h¯j)2 2

j=1

1 P A (h¯j)

= P

2 EµB,B

j=1

wT xj (

-

µB

+

-

wT xj 

- µP

+ )2

B P

1 P A (h¯j)

= P

2 EµB,B

j=1

(wT xj)2( 1 B

-

1 P

)2

-

2µP

wT

xj

(

1 B

-

1 )2 P

+ ( µB B

-

µP )2 P

1 P 2A (h¯j)

P
j=1

2

(wT xj - µP )2EµB,B

( 1 - 1 )2 B P

+ EµB,B

µB - µP 2 B

1 =

P 2A (h¯j )

wT (

xj

-

µP

)2



+

2

+

1 3( + 2) (1 + )

.

P
j=1

2

P 4M M 4M

Note that estimator

 2 l(h¯ j 2
of the

) =A Fisher

In(h¯fojr)m( watTioxnjP-MµaPtr)i2x,

we

have

F ()

=

1 P

(FIM) with respect to

thPje=s1cAale(ph¯ajr)a(mweTtexrjP-µ, Pac)c2obrdeienng

an to

the definition of Fisher information. Then, by neglecting O(1/M 2) high-order term in Rq, we get

Rq  + 2 F ()2 + µd2A 2, 8M 2M
where µd2A indicates the mean of the second derivative of A(h).

14

Under review as a conference paper at ICLR 2019

C.2 THEOREM 1 WITH RELU

For the ReLU non-linear activation function, that is f (h) = max(h, 0), we use its continuous

approximation softplus function f (h) = log(1 + exp(h)) to derive the partition function A(h). In

this case, shown in

we have µd2A theorem 1.

=

1 P

P j=1

(h¯ j ).

Therefore,

we

have



=

+2 8M

F

+

1 2M

1 P

P j=1

 (h¯ j

)

as

C.3 THEOREM 1 WITH IDENTITY ACTIVATION FUNCTION

For the

raeglouslasrfizuantciotinoncoinntrthibeuftioornmfrLom=µPB1

P j=1
can be

wTxj - neglected.

 (wT xj )/B We also have

2, 

we have F = 2 and = 0 for Gaussian input

distribution. The exact expression of theorem 1 is also possible for such linear regression problem.

Under the condition of Gaussian input x  N (0, 1/N ), h = wTx is also a random variable

satisfying a normal distribution N (0, 1), it can be derived that E B-1

=

 M
2P

(

M -2 2

)

(

M -1 2

)

and

E

B-2

=

M P2

(

M -1 2

-1)

(

M -1 2

)

,

therefore

M  (M - 3)/2   (M - 2)/2

 = 1+

- 2M

.

2 (M - 1)/2

 (M - 1)/2

Furthermore, the expression of  can be a simple linear regression, contributions

simplified from µB to

aths e re=gu4laM3ri.zaItfiotnhetebrimasistenrmegliescnteedglaencdtetdhuins



=

1 4M

.

Note

that

if

one

uses

mean

square

error

without

being

divided

by

2

during

linear

regression,

the

values

for



should

be

multiplied

by

2

as

well,

where



=

1 2M

.

C.4 DYNAMICAL EQUATIONS

Here we

that

is,

1 N

discuss the wT w =

dynamical equations of BN. 1. We introduce a normalized

Let the weight

length vector

ooffttheeacsthuedre'sntwaesigwht=vectNorbeww1,.

Then the overlapping ratio between teacher and student, the length of student's vector, and the

length

of

student's

normalized

weight

vector

are

1 N

wT w

=

QR

=

R,

1 N

wT w

=

Q2

=

2,

and

1 N

wT

w

=

L2

respectively,

where

Q

=

.

And

we

have

1 N

wT w

=

LR.

We write the dynamical equations of BN as follows,

 dQ  dt

=



I1 Q

- Q,


dR dt

=



Q L2

I3

-



R L2

I1

-

2

Q2 R 2L4

I2,

 

dL

dt

=



2

Q2 2L3

I2

,

where I1 =

w~ Tx x, I2 =

2xTx x, and I3 =

wTx

x,

which

are

the

terms

presented

in

d2 dt

,

dRL dt

,

and

dL2 dt

.

They

are

used

to

simplify

notations.

Proposition 1. Let bQn, bRn be the eigenvalues of the Jacobian matrix at 0 = (Q0, 1, L0) corresponding to the order parameters Q and R respectively in BN. Then

bQn

=

 Q0

I1 Q

- Q0,

Rbn

=

I2 2R

Q0 2L02

(mbnax

- ebffn),

where mbnax and ebnff are the maximum and effective learning rates respectively in BN.

Proof.

Firstly, note that the change of L will not change I1, I2 and I3. Thus we have

I1 L

=

I2 L

=

I3 L

=

0.

And we also neglect the term proportional to 2 because of the learning rate decays to

15

Under review as a conference paper at ICLR 2019

a small value when converged. At fixed point R0 = 1, we have (QI3 - I1)/Q = 0, thus the Jacobian of BN is



 Q0

I1 Q

- 2

 I1 Q0 R

J bn

=

 



0 0

 L20

Q0  I3 R

-

I1 R

- Q20

2Q20 I2

2L03 R

and the eigenvalues of Jbn can be obtained by inspection

- 2Q02 I2
2L04 R

0

0

 

,



0

Qbn 

=

 Q0

I1 Q

- 2,

bRn bLn

= =

 L20
0.

Q0  I3 R

-

I1 R

- Q02

- =2Q20 I2
2L40 R

I2 Q0 R 2L02

mbnax - ebnff

,

Since

0

=

Q0,

we

have

mbnax

=

( (0I3-I1)
0  R

-



0)/

I2 2R

and

ebnff

=

.0
L02

C.5 STABLE FIXED POINTS OF BN

Proposition 2. When activation function is ReLU or sigmoid, then (i) Qbn < 0, and (ii) Rbn < 0 iff mbnax > ebnff .



Proof. Firstly, when activation function is ReLU, we derive I1 = Q(R+2

which gives



1-R2+2R arcsin(R)) 4

-

Q2 2

,

I1 = -Q + R + 2

1 - R2 + 2R arcsin(R) .

Q 4

Therefore

at

the

fixed

point

of

BN

0bn

=

(

1 2+1

,

1),

we

have

Qbn

=

1 (
Q0

I1 Q

- 2)

=

( 1 (-1 Q0

+

1 2Q0

-

2 )

=

-

-

1 2

<

0.

Note that xT x approximately equals 1. We get

I2 = [g (s) (g(t) - g(s))]2DsDt

u,v

+ +

+ +

+ +

= v2DsDt + s2DsDv - 2 stDsDt

0 0  0 -

0  -

Q2 R + 2R 1 - R2 + 2 arcsin(R) Q(R + 2 1 - R2 + 2R arcsin(R))

=+

-

.

2 4

2

At the fixed point we have

I2 R

=

-Q0

<

0.

Therefore, we conclude that Rbn

<

0 iff mbnax

>

ebnff .

Secondly,

when

activation

function

is

sigmoid,

we

employ

 erf(x/ 2)

to

analyze

sigmoid

function.

Therefore, we have

I1 = [g (u) (g(v) - g(u) u]DuDv

u,v

+ +

+ +

= g (u)g(v)uDuDv - g (u)g(u)uDuDv.

- -

- -

By using the Fourier Transformation of integral on multivariate Gaussian probability density, that is,

dx1 · · · dxn (2 )n |C |

exp{- 1 2

XT

C -1 X }

×

f1(x1)

·

·

·

fn(xn)

=

dy1 · · · dyn (2)n

exp{- 1 Y 2

T CY

}

×

f~1(y1) ·

·

·

f~n(yn),

16

Under review as a conference paper at ICLR 2019

where f~j(y) =

dx 2

fj

(x)eiyx.

It suffices to have

I1 = ( + Q2)

2QR

- 2Q2

2 + 2Q2 - Q2R2 ( + Q2) 1 + 2Q2

and

I2 = 

4 (arcsin
1 + 2Q2

1

1 + 3Q2

- arcsin

1 ).
1 + 3Q2 2(1 + 2Q2) - 2Q2R2

At the fixed point 0bn

=

(

1 2+1

,

1),

it can be shown that

 (I1 /Q) Q

<

0 and thus bQn

=



(



(I1 /Q) Q

-



)

<

0, we have

I2 R

<

0.

We also conclude that bRn

<

0 iff mbnax

>

ebnff .

C.6 MAXIMUM LEARNING RATE OF BN Proposition 3. When the activation function is ReLU, then mbnax  m{wanx,sgd} + 2, where mbnax and m{wanx,sgd} indicate the maximum learning rates of BN, WN, and vanilla SGD respectively.



Proof. From the above results, we have I1 = Q(R+2 I1/R  0 at the fixed point of BN. And we get

1-R2+2R arcsin(R)) 4

-

Q2 2

,

which

gives

I3 = [g (s) (g(t) - g(s) t]DsDt
u,v

= g (s)g(t)tDsDt - g (s)g(s)tDsDt

u,v u,v

+ +

+ +

= t2DsDt - stDsDt

0 0

0 -

 + 2R =

1

-

R2

+

2

arcsin(R)

-

QR .

4 2

Then

we

derive

that

I2 R

<

0

and



(I3 -I1 R

)

/

I2 2R

has

the

same

value

at

their

respective

fixed

point

of

BN,

WN

and

vanilla

SGD.

At

the

fixed

point

of

BN,

Q0

=

0

=

1 2+1

<

1,

then

we

have

mbnax

=

( (0I3 - I1) Q0R

-



0)/

I2 2R

=

(I3 - R

I1) / I2 2R

+

(1

-

1 ) I1 / I2 0 R 2R

-



0/

I2 2R

 (I3 - I1) / I2 + 2 R 2R

= m{wanx,sgd} + 2.

17

