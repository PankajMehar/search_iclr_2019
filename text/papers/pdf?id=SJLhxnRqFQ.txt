Under review as a conference paper at ICLR 2019
ADVERSARIALLY LEARNED MIXTURE MODEL
Anonymous authors Paper under double-blind review
ABSTRACT
The Adversarially Learned Mixture Model (AMM) is a generative model for unsupervised or semi-supervised data clustering. The AMM is the first adversarially optimized method to model the conditional dependence between inferred continuous and categorical latent variables. Experiments on the MNIST and SVHN datasets show that the AMM allows for semantic separation of complex data when little or no labeled data is available. The AMM achieves unsupervised clustering error rates of 3.32% and 20.4% on the MNIST and SVHN datasets, respectively. A semi-supervised extension of the AMM achieves a classification error rate of 5.60% on the SVHN dataset.
1 INTRODUCTION
Semi-supervised or unsupervised representation learning enables the utilization of all available data when tackling problems where there are little or no labeled examples. This is a common scenario in many applications of machine learning, such as medical image analysis, where it is reinforced by the expense of obtaining expert labeled examples. Moreover, machine-learned representations are more likely to be used for subsequent tasks if they are interpretable and meaningful. Deep generative modelling is a suitable approach to this problem, as derived models have been shown capable of learning from both labeled and unlabeled examples, embedding data according to desired latent variable distributions, and producing realistic data examples generated from samples of those latent variables.
The Generative Adversarial Network (GAN) has recently emerged as a powerful framework for modeling complex data distributions without having to approximate intractable likelihoods. In the formulation by Goodfellow et al. (2014), a GAN consists of two networks: a generator G that is trained to yield unique samples from the data distribution, and a discriminator D that is trained to distinguish between generated and true data samples.
Dumoulin et al. (2016) and Donahue et al. (2017) have proposed the ALI and BiGAN models that add an inference process, i.e., the ability to map data samples to points in the latent space, to the GAN framework. A second generator for inference, or encoder, is added to the original GAN generator and the discriminator is adapted for the two-dimensional space of data inputs and latent representations. A variant of the resulting model is also introduced by Dumoulin et al. (2016) for conditional data generation, but still assumes that the class of the data is always observed, as inference of categorical variables is not included.
Adversarial approaches for the inference of both continuous and categorical variables are actively researched. Chen et al. (2016) introduce a hybrid adversarial method that is capable of modelling both continuous and categorical latent variables for unsupervised clustering and feature disentanglement. Another hybrid adversarial method is introduced by Makhzani et al. (2016) where adversarial objectives on continuous and categorical latent variables are optimized for unlabeled examples and categorical cross entropy on categorical variables is optimized for labeled examples. Li et al. (2017) and Deng et al. (2017) point toward fully adversarial semi-supervised classification using inferred categorical variables by introducing a "three player" adversarial game, but stop short by adding auxiliary "collaborative" objectives. In each of these methods, it is assumed that categorical and continuous latent variables are independently distributed. This independence assumption results in discontinuities in the latent space between categories, which removes the notion of inter-categorical proximity.
Another notable family of generative models, Variational Autoencoders (VAEs), maximize the posterior distribution of latent representations given the data instead of using an adversarial approach. As VAEs integrate inference, semi-supervised classification can be performed by conditioning the
1

Under review as a conference paper at ICLR 2019

continuous latent variable of the VAE on the class label (Kingma et al., 2014; Dilokthanakul et al., 2016; Maaløe et al., 2017). However, the quality of VAE results depend on the expressiveness of the inference distribution and every time the assumptions about the inference or data distributions are changed a new objective function needs to be derived. In this way, variational optimization is not as versatile as adversarial training.
We present the Adversarially Learned Mixture Model (AMM). The AMM is, to our knowledge, the first generative model inferring both continuous and categorical latent variables to perform either unsupervised or semi-supervised clustering of data using a single adversarial objective. This is enabled, in part, by explicitly modelling the dependence between continuous and categorical latent variables, which eliminates discontinuities between categories in the latent space. Semi-supervised clustering and classification is enabled by a simplified formulation of the "three player game", presented by Li et al. (2017). In this paper we show that the AMM achieves an unsupervised clustering error rate of 3.32% and 20.4% on the MNIST (LeCun & Cortes, 2010) and SVHN datasets (Netzer et al., 2011) respectively, and that a semi-supervised extension, SAMM, achieves a classification error rate of 5.60% on the SVHN dataset . To support the reproducibility of the methodology and experiments presented, a public version of the code will be made available.

2 METHOD

2.1 PRELIMINARIES

The ALI and BiGAN models are trained by matching two joint distributions of images x  RD and their latent code z  RL. The two distributions to be matched are the inference distribution q(x,z) and the synthesis distribution p(x,z), where,

q(x,z) = q(x)q(z | x), p(x,z) = p(z)p(x | z).

(1) (2)

Samples of q(x) are drawn from the training data and samples of p(z) are drawn from a prior distribution, usually N (0,1). Samples from q(z | x) and p(x | z) are drawn from neural networks that are optimized during training. Dumoulin et al. (2016) show that sampling from q(z | x) = N (µ(x),2(x)I)
is possible by employing the reparametrization trick (Kingma & Welling, 2013), i.e. computing

z = µ(x)+(x) ,  N (0,I),

(3)

where is element wise vector multiplication.

A conditional variant of ALI has also been explored by Dumoulin et al. (2016) where an observed class-conditional categorical variable y has been introduced. The joint factorization of each distribution to be matched are:

q(x,y,z) = q(x,y)q(z | y,x), p(x,y,z) = p(y)p(z)p(x | y,z).

(4) (5)

Samples of q(x,y) are drawn from the data. Samples of p(z) are drawn from a continuous prior on z, and samples of p(y) are drawn from a categorical prior on y, both of which are marginally independent. Samples from q(z | y,x) and p(x | y,z) are drawn from neural networks that are optimized during
training.

In the following sections we present graphical models for q(x,y,z) and p(x,y,z) that build off of conditional ALI. Where conditional ALI requires the full observation of categorical variables, the models we present will account for both unobserved and partially observed categorical variables. We finally show how they can be optimized using a single adversarial objective.

2.2 ADVERSARIALLY LEARNED MIXTURE MODEL
The AMM is an adversarial generative model for deep unsupervised clustering of data. Figure 1 presents an overview of the model.
Like conditional ALI, a categorical variable is introduced to model the labels. However, the unsupervised setting now requires a different factorization of the inference distribution in order to

2

Under review as a conference paper at ICLR 2019

z  q(z | x, y)

SAMM

z  q(z | x, y) y  q(y | x)
x  q(x) Gz(x, Gy(x))

x, y  q(x, y) Gz(x, y)
D(x, y, z)

AMM y  p(y)
z  p(z | y) x  p(x | y, z)
Gx(y, Gz(y))

Figure 1: Overview of the unsupervised (AMM) and semi-supervised (SAMM) model with the
first option (Equation (6)) for the inference distribution. AMM consists of two generators, encoder Gz(x,Gy(x)) and decoder Gx(y,z), and a discriminator D(x,y,z). SAMM includes an additional generator for labeled data, Gz(x,y).

enable inference of the categorical variable y, namely:

q1(x,y,z) = q(x)q(y | x)q(z | x,y),

(6)

or

q2(x,y,z) = q(x)q(z | x)q(y | x,z).

(7)

Samples of q(x) are drawn from the training data, and samples from q(y | x), q(z | x,y) or q(z | x),

q(y | x,z) are generated by neural networks. The reparametrization trick is not directly applicable

to discrete variables and multiple methodologies have been introduced to approximate categorical

samples (Jang et al., 2016; Maddison et al., 2017). We follow Kendall & Gal (2017) and sample from

q(y | x) by computing

hy(x) = µy(x)+y(x) ,  N (0,I), y(x) = softmax(hy(x)).

(8) (9)

Then, we can sample from q(z | x,y) by computing

z(x,hy(x)) = µz(x,hy(x))+z(x,hy(x)) ,  N (0,I).

(10)

A similar sampling strategy can be used to sample from q(y | x,z) in (7).

The factorization of the synthesis distribution p(x,y,z) also differs from conditional ALI:

p(x,y,z) = p(y)p(z | y)p(x | y,z).

(11)

The product p(y)p(z | y) can be conveniently given by a mixture model. Samples from p(y) are drawn from a multinomial prior, and samples from p(z | y) are drawn from a continuous prior, for example, N (µy=k,1). Samples from p(z | y) can alternatively be generated by a neural network by again employing the reparameterization trick. Namely,

z(y) = µ(y)+(y) ,  N (0,I).

(12)

This approach effectively learns the parameters of N (µy=k,y=k).

2.2.1 ADVERSARIAL VALUE FUNCTION

We follow Dumoulin et al. (2016) and define the value function that describes the unsupervised game between the discriminator D and the generator G as:

minmaxV
GD

(D,G)

=

Eq(x)

[log(D(x,Gy

(x),Gz

(x,Gy

(x))))]

+ Ep(y,z) [log(1 - D(Gx (y,Gz (y)),y,Gz (y)))]

= q(x)q(y | x)q(z | x,y)log(D(x,y,z))dxdydz

(13)

+ p(y)p(z | y)p(x | y,z)log(1-D(x,y,z))dxdydz.

3

Under review as a conference paper at ICLR 2019

There are four generators in total: two for the encoder Gy(x) and Gz(x,Gy(x)), which map the data samples to the latent space; and two for the decoder Gz(y) and Gx(y,Gz(y)), which map samples from the prior to the input space. Gz(y) can either be a learned function, or be specified by a known
prior (see Algorithm 1 for a detailed description of the optimization procedure).

Algorithm 1 AMM training procedure using distributions (6) and (11).

Gy (x) ,Gz (x,Gy (x)) ,Gz (y) ,Gx (y,Gz (y)) ,D while not done do

Initialize AMM parameters

x(1),...,x(M)  q(x)

Sample from data and priors

y(1),...,y(M)  p(y)

z(j)  p(z | y = y(j)), j = 1,...,M

x~(j)  p(x | y = y(j),z = z(j)), j = 1,...,M

Sample from conditionals

y~(i)  q(y | x = x(i)), i = 1,...,M

z~(i)  q(z | x = x(i),y = y~(i)), i = 1,...,M

(qi)  D(x(i),y~(i),z~(i)), i = 1,...,M

Compute discriminator predictions

(pj)  D(x~(j),y(j),z(j)), j = 1,...,M

LD



-

1 M

M i=1

log(q(i)

)-

1 M

M j=1

log(1-p(j))

LGx

(y,Gz

(y))

=

LGz

(y)



-

1 M

M i=1

log(p(i))

LGy

(x)

=

LGz

(x,Gy

(x))



-

1 M

D  D -D LD

Mi=1log(1-(qi))

Gx(y,Gz (y))  Gx(y,Gz (y)) -Gx(y,Gz (y)) LGx(y,Gz (y)) Gz(y)  Gz(y) -Gz(y) LGz(y) Gy(x)  Gy(x) -Gy(x) LGy(x)

Gz (x,Gy (x))  Gz (x,Gy (x)) -Gz (x,Gy (x)) LGz (x,Gy (x))

Compute discriminator losses
Compute x generator losses
Compute y and z generator loss Update discriminator parameters
Update generator parameters

2.3 SEMI-SUPERVISED ADVERSARIALLY LEARNED MIXTURE MODEL

The Semi-Supervised Adversarially Learned Mixture Model (SAMM) is an adversarial generative

model for supervised or semi-supervised clustering and classification of data. The objective for

training SAMM involves two adversarial games to match pairs of joint distributions. The supervised

game matches inference distribution (4) to synthesis distribution (11) and is described by the following

value function:

minmaxV
GD

(D,G)

=

Eq(x,y)[log(D(x,y,Gz

(x,y)))]+

Ep(y,z)

[log(1-D(Gx(y,Gz

(y)),y,Gz

(y)))]

= q(x,y)q(z | x,y)log(D(x,y,z))dxdydz

+ p(y)p(z | y)p(x | y,z)log(1-D(x,y,z))dxdydz.

(14)

The unsupervised game matches either of the inference distributions, (6) or (7) to the synthesis
distribution (11). In the case using distribution (6), the unsupervised game is described by (13). The generator for semi-supervised learning has three components: encoders Gz(x, Gy(x)) and Gz(x,y) map the labeled and unlabeled data samples, respectively, to the latent space, and a decoder Gx(y,Gz(y)) maps samples of y and z to the input space, where Gz(z) can either be a learned function or be specified by a prior. The encoder for labeled data again consists of two generators
(Figure 1). A detailed description of the training algorithm is given in algorithm 2 of the appendix.
In practice, optimization of each of the generators and the discriminator can be done simultaneously
for both the unsupervised and semi-supervised updates.

3 RELATED WORKS
Unsupervised clustering using hybrid adversarial approaches are proposed by both Makhzani et al. (2016) (AAE) and Chen et al. (2016) (InfoGAN). For AAE, the synthesis generator is optimized

4

Under review as a conference paper at ICLR 2019

by minimizing the per-example L2 loss between between input data {xi} and their reconstructions {x i = Gxi (Gy(xi),Gz(xi))}, while the inference generator is optimized using both the L2 objective and an adversarial objective. For InfoGAN, the inference generator is optimized by maximizing the
per-example Mutual Information (MI) between samples of categorical latent variables {yi  p(y)} and continuous latent variables {zi  p(z)} and their "reconstructions" {{yi,zi} = Gy,z(Gx(yi,zi)},
while the synthesis generator is optimized using both the MI objective and an adversarial objective.

Recent approaches using self-supervision with data augmentation have also been proposed for
unsupervised discrete representation learning by Kilinc & Uysal (2018) (LALNets) and Hu et al.
(2017) (IMSAT). In IMSAT, a network is trained to maximize the Mutual Information between input
data and a discrete representation, similar to InfoGAN. In addition to this objective, the network is regularized by encouraging the representation of original, unperturbed data {xi} to be close to that of transformed data {T (xi)}. In the LALNets framework, a network is trained to distinguish unperturbed data and augmented data, and then k-means clustering is performed on the latent space representation
of the unperturbed data to obtain cluster labels.

On the other end of the generative spectrum, Dilokthanakul et al. (2016) and Jiang et al. (2017) offer non-adversarial, VAE-based approaches for unsupervised clustering. Like in the AMM, the combination of priors for the latent variables y and z is modeled as a Gaussian mixture model, where y corresponds to the mixture components.

Multiple adversarial methodologies have been proposed for supervised or semi-supervised learning (Springenberg, 2015; Salimans et al., 2016; Miyato et al., 2017; Dai et al., 2017), but they suffer from the same limitation as the original GAN: they do not provide inference. Gan et al. (2017), Li et al. (2017) and Deng et al. (2017) introduce a third player to the adversarial game. Although this extra player allows to infer categorical variables, these approaches are not fully adversarial as auxiliary "collaborative" terms are added to the objective function. Moreover, categorical and continuous latent variables are modeled independently.
The adversarial and hybrid-adversarial approaches thus far discussed all model y and z as being conditionally independent from each other. This may be an ideal prior structure for inference, for example, in learning disentangled representations of x sampled from a limited domain (Chen et al., 2016). However, the independence assumption cannot account for the notion of proximity between categories because z is identically distributed for each category in y. Therefore, the distance between categories is equal and indeterminate. AMM and SAMM are presented as adversarial approaches to model conditional dependencies between y and z, but they do not preclude the independence assumption. The proposed methods can model y and z as conditionally independent with inference distribution

q(x,y,z) = q(x)q(y | x)q(z | x),

(15)

and synthesis distribution

p(x,y,z) = p(y)p(z)p(x | y,z);

(16)

however, analysis of this graphical model is left for future work.

4 EVALUATION

AMM and SAMM are evaluated using two image datasets: MNIST (LeCun & Cortes, 2010) and SVHN
(Netzer et al., 2011). The provided training and testing splits are used for MNIST experiments with 5000
randomly selected examples left out of the training set for validation. The same training, testing, and
validation splits as Dumoulin et al. (2016) are used for SVHN. Preprocessing is limited to scaling image intensities on the range [0,1]. Detailed architectures for each experiment are shown in Section B of the appendix. We optimize all networks using Adam (Kingma & Ba, 2014) with  = 0.0002 and 1 = 0.5. All kernel weights are initialized using a Gaussian distribution with standard deviation 0.02, all biases are
initialized to 0.0. Following the criteria in Jiang et al. (2017), the performance is evaluated as follows:

ACC = max
nN

M i=1

1{yi =
M

n(y~z~,i

)}

,

(17)

where M is the number of samples, yi is the true label, and y~z~,i is the predicted cluster label, and N is the set of all mappings between cluster labels and true labels.

5

Under review as a conference paper at ICLR 2019

4.1 GRADIENT PENALTY

The gradient penalty introduced by Gulrajani et al. (2017) is added to the discriminator loss to help stabilize training of AMM and SAMM models. This penalty keeps the gradients of the discriminator with respect to the inputs x, y, and z on the same order of magnitude. The penalty applied to the discriminator loss is

L =  Ex^,y^,z^

(x^,y^,z^)Px^,y^,z^

(||x^,y^,z^D(x^,y^,z^)||2 -1)2

,

(18)

where points (x^,y^,z^) are drawn at random on straight lines between real or prior samples (x,y~,z~) and synthesized or inferred samples (x~,y,z). The gradient penalty for Jensen-Shannon GAN introduced
by Roth et al. (2017) has also been explored, but did not produce better results. The regularization term is set to  = 10.0, and  = 0.01 for MNIST and SVHN experiments, respectively.

4.2 MNIST
In this section, the AMM is evaluated on the task of unsupervised clustering of hand-drawn digits using the MNIST dataset. To model p(y)p(z | y), a 64 dimensional mixture of Gaussians is used with 10, 20, and 30-components across 3 experiments. A multinomial prior is used for p(y) with uniform probability for each class. The mean and variance of each component distribution are learned using the reparameterization trick via (12). Table 1 reports the test-set clustering error-rate mean and standard deviation over 10 trials, with the 10-component AMM achieving a 3.32 ± 0.39 percent error rate. Figure 2 visualizes results from 1 of the 10 trials.

(a) (b) Cluster matrix

(c) Reconstruction

(d) Interpolation

(e) t-SNE projection

Figure 2: Unsupervised clustering of MNIST data with 10 mixture components. (a) Comparing test image membership and randomly generated digits for each mixture component. (b) Cluster matrix. (c) Reconstructions of input images: original data on the left of each pair. (d) Interpolation between examples: original data samples are shown in the first and last columns with linearly interpolated generations between. (e) t-SNE projection of testing samples, color-coded for the MNIST class labels (0 to 9).

6

Under review as a conference paper at ICLR 2019

Table 1: Test set clustering error rate and standard deviation for MNIST data. Methods using data augmentation are denoted with .

MODEL

K

GMVAE (DILOKTHANAKUL ET AL., 2016) 30

VADE (JIANG ET AL., 2017)

10

CATGAN (SPRINGENBERG, 2015) INFOGAN (CHEN ET AL., 2016) AAE (MAKHZANI ET AL., 2016) IMSAT (HU ET AL., 2017) 

20 10 30 10

LALNETS (KILINC & UYSAL, 2018) 

10

AMM AMM AMM

10 20 30

MNIST
7.23±1.60 5.54
9.70 5.00 4.10±1.13 1.59±0.40
1.68±0.08
3.32±0.39 3.99±0.79 4.01±1.11

SVHN
-
42.7±3.90
23.2±1.30
35.0±5.24 20.4±0.80

4.3 SVHN
4.3.1 UNSUPERVISED CLUSTERING
In this section, unsupervised clustering is revisited. The SVHN dataset is used to investigate how confounding attributes, such as color and contrast, affects the semantic separation of digits. To model p(y)p(z | y) a 64 dimensional mixture of Gaussians is used with 10, 20, and 30-components across 3 experiments. A multinomial prior is used for p(y) with uniform probability for each class, and the mean and variance of each component distribution are learned using the reparameterization trick via (12).
Figure 3a shows random samples drawn from each component distribution generated by Gx. Figure 3b is a t-SNE projection of test samples drawn from Gz onto a 2D manifold. We show in 3c that AMM learns a smooth latent manifold as we interpolate between examples from SVHN. The cluster matrix for the SVHN test set is shown in figure 3d, demonstrating an overall classification error rate of 20.4% ± 0.80. From these results, we can make a few observations. First, the generated images are realistic looking. This is the case for every cluster other than cluster 5, which appears to be a noise cluster. Secondly, each cluster only contains a single digit. Finally, we observe that for each digit there are multiple clusters, one containing a dark background with light numbers (ie. cluster 28), and another containing a light background with dark numbers (ie. cluster 12). This is important, as it shows that the AMM has learned semantically meaningful clusters.

4.3.2 SEMI-SUPERVISED CLUSTERING AND CLASSIFICATION

It is evident from the last experiment that the confounders introduced by the SVHN dataset made unsupervised semantic clustering more difficult. In this section we show how SAMM can be used to guide clustering along predefined categories using only a small amount of labeled data. To this end, we limit the samples drawn from q(x,y) to a random selection of 1000 examples from the training set. To model p(y)p(z | y) we use a 64 dimensional mixture of 10 spherical Gaussians, where the the mean and variance of each component distribution are learned using the reparameterization trick via (12). There is considerable class imbalance in the SVHN dataset, so a multinomial prior is used for p(y) with each class probability set to the frequency observed in the labeled subset of the training data.
Table 2 reports the test-set error-rate mean and variance over 10 trials. SAMM achieves 5.60±0.45 percent error rate, which is an improvement over the ALI baseline. Figure 4 shows visualizations of results from 1 of the 10 trials. Finally, given that we have defined p(y)p(z | y) we can use Bayes' theorem to derive p(y | z) and get a classifier given an image embedding z~:

y~z~ = argmax[p(z = z~| y = k)p(y = k)]
k

(19)

Figures 4e and 4f compare the confusion matrices for predictions given by y~z~ and those given by y~ from Gy. The similarity between each is further evidence that the inference network has learned to
embed data according to the desired distribution.

7

Under review as a conference paper at ICLR 2019

0 10 20 1 11 21 2 12 22 3 13 23 4 14 24 5 15 25 6 16 26 7 17 27 8 18 28 9 19 29
(a) Randomly generated images

(b) t-SNE projection

9 8 7 6 5 4 3 2 1 0

(c) Interpolation

(d) Cluster matrix
Figure 3: Unsupervised clustering of SVHN data with 30 mixture components. (a) Randomly generated images for each mixture component, mixture component indices are indicated to the left of each row. (b) t-SNE projection of testing samples, color-coded for the SVHN class label (0 to 9). (c) Interpolation between examples: input images are shown in the first and last columns. (d) Cluster matrix.
5 CONCLUSION
The AMM is presented as a generative model for unsupervised or semi-supervised data clustering with several contributions. The AMM is the first fully adversarially optimized method to model the conditional dependence between categorical and continuous latent variables, providing impressive unsupervised clustering and competitive semi-supervised classification results on benchmark datasets. In contrast with other semi-supervised approaches, we have shown that the use of additional losses or discriminators (Gan et al. (2017); Li et al. (2017); Deng et al. (2017)) are redundant additions to frameworks, as the AMM yields similar or better results to these methods. In contrast with Dai et al. (2017), our strong semi-supervised performance and qualitatively good image generation demonstrate that semi-supervised performance and image generation are not necessarily opposing goals. As a fully adversarial framework, the AMM provides a simple, yet powerful formulation as a foundation for
8

Under review as a conference paper at ICLR 2019

(a) Test images

(b) Random

(c) Interpolation

(d) t-SNE projection

(e) y~z~

(f) y~

Figure 4: Semi-supervised clustering and classification of SVHN data with 10 mixture components. (a) Test image predictions: rows correspond to the predicted class. (b) Randomly generated images for each mixture component. (c) Interpolation between examples: original data samples in first and last columns. (d) t-SNE projection of testing samples, color-coded for the SVHN class label (0 to 9). Confusion matrix for predictions given an image embedding (e) and given the generator Gy (e).

Table 2: Semi-supervised test set missclassification rate and standard deviation for SVHN data.  and  denote similar encoder/classifier architectures.

MODEL

SVHN (N = 1000)

AAE (MAKHZANI ET AL., 2016)

17.70±0.24

IMPROVEDGAN (SALIMANS ET AL., 2016) ]  8.11±1.30

ALI (DUMOULIN ET AL., 2016)

7.42±0.65

VAT SMALL (MIYATO ET AL., 2017) 

6.83±0.24

TRIPLEGAN (LI ET AL., 2017) 

5.77±0.17

SGAN (DENG ET AL., 2017) 

5.73±0.12

VAT LARGE (MIYATO ET AL., 2017) 

4.28±0.10

(DAI ET AL., 2017) 

4.25±0.03

SAMM 

5.60±0.45

future work. For example, the distribution of mixture components can be something be other than a Gaussian without the need to derive an new evidence lower bound. Furthermore, by learning conditional dependencies between y and z, the AMM preserves the notion of proximity between classes in the latent space, which could be useful for applications in metric space learning and few shot learning. On top of this, learning the mean and variance of mixture components gives access to subsequent analysis of the likelihood function, which could provide an interpretable insight of the learned distributions. With that, the mixture coefficients are the only terms to be integrated into the framework. As a whole, the AMMs demonstrably strong performance validates its use in future work and extension into other domains.
9

Under review as a conference paper at ICLR 2019
REFERENCES
Xi Chen, Xi Chen, et al. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems 29, pp. 2172­2180. 2016.
Zihang Dai, Zhilin Yang, et al. Good semi-supervised learning that requires a bad GAN. In Advances in Neural Information Processing Systems, pp. 6510­6520, 2017.
Zhijie Deng, Hao Zhang, et al. Structured generative adversarial networks. In Advances in Neural Information Processing Systems 30, pp. 3902­3912. 2017.
Nat Dilokthanakul, Pedro A. M. Mediano, et al. Deep unsupervised clustering with Gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.
Jeff Donahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. In International Conference on Learning Representations, 2017.
Vincent Dumoulin, Ishmael Belghazi, et al. Adversarially learned inference. In International Conference on Learning Representations, 2016.
Zhe Gan, Liqun Chen, et al. Triangle generative adversarial networks. In Advances in Neural Information Processing Systems 30, pp. 5251­5260, 2017.
Ian Goodfellow, Jean Pouget-Abadie, et al. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pp. 2672­2680. 2014.
Ishaan Gulrajani, Faruk Ahmed, et al. Improved training of Wasserstein GANs. arXiv preprint arXiv:1704.00028, 2017.
Weihua Hu, Takeru Miyato, et al. Learning discrete representations via information maximizing self-augmented training. arXiv preprint arXiv:1702.08720, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
Zhuxi Jiang, Yin Zheng, et al. Variational deep embedding: An unsupervised and generative approach to clustering. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, 2017.
Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems 30, pp. 5580­5590. 2017.
Ozsel Kilinc and Ismail Uysal. Learning latent representations in neural networks for clustering through pseudo supervision and graph-based activity regularization. arXiv preprint arXiv:1802.03063, 2018.
Diederik Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2013.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma, Shakir Mohamed, et al. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems 27, pp. 3581­3589. 2014.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/mnist/.
Chongxuan Li, Kun Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. arXiv preprint arXiv:1703.02291, 2017.
Lars Maaløe, Marco Fraccaro, and Ole Winther. Semi-supervised generation with cluster-aware generative models. arXiv preprint arXiv:1704.00637, 2017.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. In International Conference on Learning Representations, 2017.
10

Under review as a conference paper at ICLR 2019 Alireza Makhzani, Jonathon Shlens, et al. Adversarial autoencoders. In International Conference
on Learning Representations, 2016. Takeru Miyato, Shin-ichi Maeda, et al. Virtual adversarial training: a regularization method for
supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017. Yuval Netzer, Tao Wang, et al. Reading digits in natural images with unsupervised feature learning.
In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Kevin Roth, Aurelien Lucchi, et al. Stabilizing training of generative adversarial networks through
regularization. In Advances in Neural Information Processing Systems 30, pp. 2015­2025. 2017. Tim Salimans, Ian Goodfellow, et al. Improved techniques for training GANs. In Advances in Neural
Information Processing Systems 29, pp. 2234­2242. 2016. Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative
adversarial networks. arXiv preprint arXiv:1511.06390, 2015.
11

Under review as a conference paper at ICLR 2019

APPENDIX A SAMM ALGORITHM

Algorithm 2 outlines the SAMM training procedure.

Algorithm 2 SAMM training procedure using distributions (4), (6), and (11).

Gy (x) ,Gz (x,Gy (x)) ,Gz (y) ,Gx (y,Gz (y)) ,D while not done do
x(u1),...,xu(M)  q(x) yu(1),...,yu(M)  p(y) zu(j)  p(z | y = yu(j)), j = 1,...,M x~u(j)  p(x | y = yu(j),z = zu(j)), j = 1,...,M y~u(i)  q(y | x = x(ui)), i = 1,...,M z~u(i)  q(z | x = xu(i),y = y~u(i)), i = 1,...,M
x(1),...,x(M) , y~(1),...,y~(M)  q(x,y)

Initialize SAMM parameters Sample from unlabeled data and priors
Sample from conditionals
Sample from labeled data and priors

y(1),...,y(M)  p(y)

z(j)  p(z | y = y(j)), j = 1,...,M

x~(j)  p(x | y = y(j),z = z(j)), j = 1,...,M

Sample from conditionals

z~(i)  q(z | x = x(i),y = y~(i)), i = 1,...,M

(qiu)  D(x(ui),y~u(i),z~u(i)), i = 1,...,M (pju)  D(x~(uj),yu(j),zu(j)), j = 1,...,M (qi)  D(x(i),y~(i),z~(i)), i = 1,...,M

Compute predictions for unlabeled data Compute predictions for labeled data

(pj)  D(x~(j),y(j),z(j)), j = 1,...,M

LDu



-

1 2M

M i=1

log((qiu)

)

-

1 2M

M j=1

log(1-p(ju))

Compute discriminator losses

LD



-

1 2M

iM=1log((qi))

-

1 2M

M j=1

log(1-(pj))

LGyu

(x)

=

LGzu

(x,Gy

(x))



-

1 2M

M i=1

log(1

-

(qiu)

)

LGz

(x,Gy

(x))



-

1 2M

iM=1log(1-q(i))

Compute inference losses

LGxu

(y,Gz

(y))

=

LGzu

(y)



-

1 2M

LGx

(y,Gz(y)) = LGz

(y)



-

1 2M

M i=1

log(p(iu)

)

M i=1

log(p(i)

)

Compute x generator losses

D  D -D (LDu +LD )

Update discriminator parameters

   -Gz(x,Gy(x))

Gz (x,Gy (x))

Gz (x,Gy (x))

Update z inference parameters

Gy(x)  Gy(x) -Gy(x) LGyu (x)

LGzu (x,Gy(x)) +LGz (x,Gy(x)) Update y inference parameters

   -Gx(y,Gz(y))

Gx(y,Gz (y))

Gx(y,Gz (y))

LGxu (y,Gz(y)) +LGx (y,Gz(y))

Update x synthesis parameters

Gz(y)  Gz(y) -Gz(y) LGzu (y) +LGz (y)

Update z synthesis parameters

APPENDIX B EXPERIMENT INFORMATION

B.1 MODEL ARCHITECTURES

Tables 3, 4, 5 and 6 detail the model architectures for the MNIST experiments. Tables 7, 8, 9 and 10 detail the model architectures for the SVHN experiments. Model inputs and outputs are highlighted in boldface. "Leak" denotes Leaky ReLU activations. "ExpN" denotes the following activation function:

ExpN(x) = exp(0.5 x) ,  N (0,I).

(20)

12

Under review as a conference paper at ICLR 2019

Table 3: MNIST: Gz(x)Gy(x,Gz(x))

Name
x
y1 z1 y1a z1a y2 z2 y2a z2a y3 z3 y3a z3a y4 z4 y4a z4a y5 z5 y5a z5a yµ y y
zµ z z

Input
-
x
x
y1 + z1 z1 y1a z1a
y2 + z2 z2 y2a z2a
y3 + z3 z3 y3a z3a
y4 + z4 z4 y4a z4a
y5 + z5 z5 y5a y5a
yµ + y z5a z5a
zµ + z

Channels
1 32 32 32 32 32 32 32 32 64 64 64 64 64 64 64 64 128 128 128 128 k k k 64 64 64

Width
2 2 3 3 3 3 3 3 4 4 1 1 1 1 -

Stride
1 1 2 2 2 2 1 1 1 1 1 1 1 1 -

Dropout
0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 -

BatchNorm
yes yes yes yes yes yes yes yes yes yes -

Activation
Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 ExpN Softmax ExpN -

Table 4: MNIST: Gz(y)

Name
y z1 z2 zµ z z

Input
y z1 z2 zeros zµ + z

Channels
k 64 64 64 64 64

Width
1 1 1 -

Stride
1 1 1 -

Dropout
0.2 -

BatchNorm
yes yes -
-

Activation
Leak 0.2 Leak 0.2
ExpN
-

13

Under review as a conference paper at ICLR 2019

Table 5: MNIST: Gx(y,Gz(y))

Name
y
z
y1 z1 y1a z1a y2 z2 y2a z2a y3 z3 y3a z3a y4 z4 y4a z4a y5 z5 y5a z5a y6 z6 x

Input
-
-
y
z
y1 + z1 z1 y1a z1a
y2 + z2 z2 y2a z2a
y3 + z3 z3 y3a z3a
y4 + z4 z4 y4a z4a
y5 + z5 z5 y5a z5a
y6 + z6

Channels
k 64 128 128 128 128 64 64 64 64 64 64 64 64 32 32 32 32 32 32 32 32 1 1 1

Width
1 1 4 4 3 3 3 3 3 3 2 2 -

Stride
1 1 1 1 1 1 2 2 2 2 1 1 -

Dropout
0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 -

BatchNorm
yes yes yes yes yes yes yes yes yes yes -

Activation
Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Sigmoid

Table 6: MNIST: D(x,y,z)

Name
x y z x1 x2 x3 x4 x5 y1 y2 z1 z2 p0 p1 p2 p

Input
x x1 x2 x3 x4 y y1 z z1 x5 | y2 | z2 p0 p1 p1

Channels
1 k 64 32 32 64 64 128 64 64 64 64 256 256 256 1

Width
2 3 3 3 4 1 1 1 1 1 1 1

Stride
1 2 2 1 1 1 1 1 1 1 1 1

Dropout
0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2

BatchNorm
-

Activation
Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 -

14

Under review as a conference paper at ICLR 2019

Table 7: SVHN: Gz(x)Gy(x,Gz(x))

Name
x
x1 x2 x3 x4 x5 x6 x7 y1 z1 y1a z1a y2 z2 y2a z2a yµ y y
zµ z z

Input
-
x
x1 x2 x3 x4 x5 x6 x7 x7 y1 + z1 z1 y1a z1a y2 + z2 z2 y2a y2a yµ + y z2a z2a zµ + z

Channels
1 96 96 96 192 192 192 384 192 192 192 192 96 96 96 96 k k k 64 64 64

Width
3 3 3 3 3 3 3 1 1 1 1 Avg6 Avg6 1 1 1 1 -

Stride
Max2 Max2 1 1 1 1 -

Dropout
-

BatchNorm
yes yes yes yes yes yes yes yes yes -
yes yes -
-

Activation
Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2
Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 ExpN Softmax ExpN -

Table 8: SVHN: Gz(y)

Name
y zµ z z

Input
y y zµ + z

Channels
k 64 64 64

Width
1 1 -

Stride
1 1 -

Dropout
-

BatchNorm
-

Activation
ExpN -

15

Under review as a conference paper at ICLR 2019

Table 9: SVHN: Gx(y,Gz(y))

Name
y
z
y1 z1 y1a z1a y2 z2 y2a z2a y3 z3 y3a z3a y4 z4 y4a z4a y5 z5 y5a z5a y6 z6 x

Input
-
-
y
z
y1 + z1 z1 y1a z1a
y2 + z2 z2 y2a z2a
y3 + z3 z3 y3a z3a
y4 + z4 z4 y4a z4a
y5 + z5 z5 y5a z5a
y6 + z6

Channels
k 64 512 512 512 512 256 256 256 256 256 256 256 256 128 128 128 128 128 128 128 128 4 4 3

Width
1 1 4 4 3 3 3 3 3 3 2 2 -

Stride
1 1 1 1 1 1 2 2 1 1 1 1 -

Dropout
-

BatchNorm
yes yes yes yes yes yes yes yes yes yes -

Activation
Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Sigmoid

Table 10: SVHN: D(x,y,z)

Name
x y z
x1 x2 x3 x4 x5 x6 x7 x8 x9 y1 y2 z1 z2 p0 p1 p2 p

Input
x x1 x2 x3 x4 x5 x6 x7 x8 y y1 z z1 x5 | y2 | z2 p0 p1 p1

Channels
1 k 64 96 96 96 192 192 192 384 192 96 96 96 96 96 288 288 288 1

Width
3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1

Pooling
Max2 Max2 Avg6 1 1 1 1 1 1 1

Dropout
0.1 0.1 0.1 0.1 0.1 0.1 0.1

BatchNorm
-

Activation
Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 Leak 0.2 -

16

