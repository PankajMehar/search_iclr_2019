Under review as a conference paper at ICLR 2019
GENERALIZABLE ADVERSARIAL TRAINING VIA SPECTRAL NORMALIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Deep neural networks (DNNs) have set benchmarks on a wide array of supervised learning tasks. Trained DNNs, however, often lack robustness to minor adversarial perturbations to the input, which undermines their true practicality. Recent works have increased the robustness of DNNs by fitting networks using adversarially-perturbed training samples, but the improved performance can still be far below the performance seen in non-adversarial settings. A significant portion of this gap can be attributed to the decrease in generalization performance due to adversarial training. In this work, we extend the notion of margin loss to adversarial settings and bound the generalization error for DNNs trained under several well-known gradient-based attack schemes, motivating an effective regularization scheme based on spectral normalization of the DNN's weight matrices. We also provide a computationally-efficient method for normalizing the spectral norm of convolutional layers with arbitrary stride and padding schemes in deep convolutional networks. We evaluate the power of spectral normalization extensively on combinations of datasets, network architectures, and adversarial training schemes.
1 INTRODUCTION
Despite their impressive performance on many supervised learning tasks, deep neural networks (DNNs) are often highly susceptible to adversarial perturbations imperceptible to the human eye (Szegedy et al., 2013; Goodfellow et al., 2014b). These "adversarial attacks" have received enormous attention in the machine learning literature over recent years (Goodfellow et al., 2014b; Moosavi Dezfooli et al., 2016; Carlini & Wagner, 2016; Kurakin et al., 2016; Papernot et al., 2016; Carlini & Wagner, 2017; Papernot et al., 2017; Madry et al., 2018; Tramèr et al., 2018). Adversarial attack studies have mainly focused on developing effective attack and defense schemes. While attack schemes attempt to mislead a trained classifier via additive perturbations to the input, defense mechanisms aim to train classifiers robust to these perturbations. Although existing defense methods result in considerably better performance compared to standard training methods, the improved performance can still be far below the performance in non-adversarial settings (Athalye et al., 2018; Schmidt et al., 2018).
A standard adversarial training scheme involves fitting a classifier using adversarially-perturbed samples (Szegedy et al., 2013; Goodfellow et al., 2014b) with the intention of producing a trained classifier with better robustness to attacks on future (i.e. test) samples. Madry et al. (2018) provides a robust optimization interpretation of the adversarial training approach, demonstrating that this strategy finds the optimal classifier minimizing the average worst-case loss over an adversarial ball centered at each training sample. This minimax interpretation can also be extended to distributionally-robust training methods (Sinha et al., 2018) where the offered robustness is over a Wasserstein-ball around the empirical distribution of training data.
Recently, Schmidt et al. (2018) have shown that standard adversarial training produces networks that generalize poorly. The performance of adversarially-trained DNNs over test samples can be significantly worse than their training performance, and this gap can be far greater than the generalization gap achieved using standard empirical risk minimization (ERM). This discrepancy suggests that the overall adversarial test performance can be improved by applying effective regularization schemes during adversarial training.
1

Under review as a conference paper at ICLR 2019

accuracy

1.0 0.8 0.6 0.4 0.2 0.0 0

FGM training

1.0

0.8

0.6

train 0.4

valid train (SN)

0.2

valid (SN)

20000 40000 60000 80000 0.0 0

training steps

PGM training

1.0

0.8

0.6

0.4

0.2

20000 40000 60000 80000 0.0 0 training steps

WRM training
20000 40000 60000 80000 training steps

Figure 1: Adversarial training performance with and without spectral normalization (SN) for AlexNet fit on CIFAR10. The gain in the final test accuracies for FGM, PGM, and WRM after spectral normalization are 0.09, 0.11, and 0.04, respectively (see Table 1 in the Appendix). For FGM and PGM, perturbations have 2 magnitude 2.44.

In this work, we propose using spectral normalization (SN) (Miyato et al., 2018) as a computationallyefficient and statistically-powerful regularization scheme for adversarial training of DNNs. SN has been successfully implemented and applied for DNNs in the context of generative adversarial networks (GANs) (Goodfellow et al., 2014a), resulting in state-of-the-art deep generative models for several benchmark tasks (Miyato et al., 2018). The theoretical results of Bartlett et al. (2017) and Neyshabur et al. (2017a) also suggest that SN can close the margin-based generalization gap for DNNs in standard non-adversarial settings.
On the theoretical side, we first extend the standard notion of margin loss to adversarial settings. We then leverage the PAC-Bayes generalization framework (McAllester, 1999) to prove generalization bounds for spectrally-normalized DNNs in terms of our defined adversarial margin loss. Our approach parallels the approach used by Neyshabur et al. (2017a) to derive generalization bounds in non-adversarial settings. We obtain adversarial generalization error bounds for three well-known gradient-based attack schemes: fast gradient method (FGM) (Goodfellow et al., 2014b), projected gradient method (PGM) (Kurakin et al., 2016; Madry et al., 2018), and Wasserstein risk minimization (WRM) (Sinha et al., 2018). Our theoretical analysis shows that the adversarial generalization component will vanish by applying SN to all layers for sufficiently small spectral norm values.
On the empirical side, we show that SN can significantly improve test performance after adversarial training. We perform numerical experiments for three standard datasets (MNIST, CIFAR-10, SVHN) and various standard DNN architectures (including AlexNet (Krizhevsky et al., 2012), Inception (Szegedy et al., 2015), and ResNet (He et al., 2016)); in almost all of the experiments we obtain a better test performance after applying SN. Figure 1 shows the training and validation performance for AlexNet fit on the CIFAR10 dataset using FGM, PGM, and WRM, resulting in adversarial test accuracy improvements of 9, 11, and 4 percent, respectively. Furthermore, we numerically validate the correlation between the spectral-norm capacity term in our bounds and the actual generalization performance. To perform our numerical experiments, we develop a computationally-efficient approach for normalizing the spectral norm of convolution layers with arbitrary stride and padding schemes. We provide the TensorFlow code as spectral normalization of convolutional layers can also be useful for other deep learning tasks. To summarize, the main contributions of this work are:
1. Proposing SN as a regularization scheme for adversarial training of DNNs,
2. Extending concepts of margin-based generalization analysis to adversarial settings and proving margin-based generalization bounds for three gradient-based adversarial attack schemes,
3. Developing an efficient method for normalizing the spectral norm of convolutional layers in deep convolution networks,
4. Numerically demonstrating the improved test and generalization performance of DNNs trained with SN.
2

Under review as a conference paper at ICLR 2019

2 PRELIMINARIES

In this section, we first review some standard concepts of margin-based generalization analysis in learning theory. We then extend these notions to adversarial training settings.

2.1 SUPERVISED LEARNING, DEEP NEURAL NETWORKS, GENERALIZATION ERROR
Consider samples {(x1, y1), . . . , (xn, yn)} drawn i.i.d from underlying distribution PX,Y . We suppose X  X and Y  {1, 2, . . . , m} where m represents the number of different labels. Given loss function and function class F = {fw, w  W} parameterized by w, a supervised learner aims to find the optimal function in F minimizing the expected loss (risk) averaged over the underlying distribution P .
We consider Fnn as the class of d-layer neural networks with h hidden units per layer and activation functions  : R  R. Each fw : X  Rm in Fnn maps a data point x to an m-dimensional vector. Specifically, we can express each fw  Fnn as fw(x) = Wd(Wd-1 · · · (W1x) · · · )). We use Wi 2 to denote the spectral norm of matrix Wi, defined as the largest singular value of Wi, and Wi F to denote Wi's Frobenius norm.
A classifier fw's performance over the true distribution of data can be different from the training performance over the empirical distribution of training samples P^. The difference between the empirical and true averaged losses, evaluated on respectively training and test samples, is called the generalization error. Similar to Neyshabur et al. (2017a), we evaluate a DNN's generalization performance using its expected margin loss defined for margin parameter  > 0 as

L(fw) := P fw(X)[Y ]   + max fw(X)[j] ,
j=Y

(1)

where fw(X)[j] denotes the jth entry of fw(X)  Rm. For a given data point X, we predict the
label corresponding to the maximum entry of fw(X). Also, we use L(fw) to denote the empirical margin loss averaged over the training samples. The goal of margin-based generalization analysis is to provide theoretical comparison between the true and empirical margin risks.

2.2 ADVERSARIAL ATTACKS, ADVERSARIAL TRAINING

A supervised learner observes only the training samples and hence does not know the true distribution
of data. Then, a standard approach to train a classifier is to minimize the empirical expected loss over function class F = {fw : w  W}, which is

1n min wW n
i=1

fw(xi), yi .

(2)

This approach is called empirical risk minimization (ERM). For better optimization performance, the loss function is commonly chosen to be smooth. Hence, 0-1 and margin losses are replaced by smooth surrogate loss functions such as the cross-entropy loss. However, we still use the margin loss as defined in (1) for evaluating the test and generalization performance of DNN classifiers.

While ERM training usually achieves good performance over DNNs, several recent observations reveal that adding some adversarially-chosen perturbation to each sample can significantly drop the trained DNN's performance. Given norm function · and adversarial noise power > 0, the adversarial additive noise for sample (x, y) and classifier fw is defined to be

wadv(x) := argmax fw(x + ), y .


(3)

To provide adversarial robustness against the above attack scheme, a standard technique, which is called adversarial training, follows ERM training over the adversarially-perturbed samples by solving

1n min wW n
i=1

fw xi + wadv(xi) , yi .

(4)

Nevertheless, (3) and hence (4) are generally non-convex and intractable optimization problems. Therefore, several schemes have been proposed in the literature to approximate the optimal solution

3

Under review as a conference paper at ICLR 2019

of (3). In this work, we analyze the generalization performance of the following three gradient-based methods for approximating the solution to (3). We note that several other attack schemes such as DeepFool (Moosavi Dezfooli et al., 2016), CW attacks (Carlini & Wagner, 2017), target and least-likely attacks (Kurakin et al., 2016) have been introduced and examined in the literature, which can lead to interesting future directions for this work.

1. Fast Gradient Method (FGM) (Goodfellow et al., 2014b): FGM approximates the solution to (3) by considering a linearized DNN loss around a given data point. Hence, FGM perturbs (x, y) by adding the following noise vector:

wfgm(x) := argmax T x fw(x), y .


(5)

For the special case of -norm · , the above representation of FGM recovers the fast gradient sign method (FGSM) where each data point (x, y) is perturbed by the -normalized sign vector of the loss's gradient. For 2-norm · 2, we similarly normalize the loss's gradient vector to have Euclidean norm.
2. Projected Gradient Method (PGM) (Kurakin et al., 2016): PGM is the iterative version of FGM and applies projected gradient descent to solve (3). PGM follows the following update rules for a given r number of steps:

1  i  r :

wpgm,i+1(x) :=

wpgm,i(x) +  w(i) ,

B , · (0)

w(i) := argmax T x fw(x + wpgm,i(x)), y .
 1

(6)

Here, we first find the direction w(i) along which the loss at the ith perturbed point changes the most, and then we move the perturbed point along this direction by stepsize  followed by projecting the resulting perturbation onto the set { :   } with -bounded norm.
3. Wasserstein Risk Minimization (WRM) (Sinha et al., 2018): WRM solves the following variant of (3) for data-point (x, y) where the norm constraint in (3) is replaced by a norm-squared Lagrangian penalty term:

wwrm(x) := argmax


fw(x + ), y

- 2



2.

(7)

As discussed earlier, the optimization problem (3) is generally intractable. However, in the case of Euclidean norm · 2, if we assume x (fw(x), y)'s Lipschitz constant is upper-bounded by , then WRM optimization (7) results in solving a convex optimization problem and can be efficiently solved
using gradient methods.

To obtain efficient adversarial defense schemes, we can substitute wfgm, wpgm, or wwrm for wadv in (4). Instead of fitting the classifier over true adversarial examples, which are NP-hard to obtain, we can
instead train the DNN over FGM, PGM, or WRM-adversarially perturbed samples.

2.3 ADVERSARIAL GENERALIZATION ERROR
The goal of adversarial training is to improve the robustness against adversarial attacks on not only the training samples but also on test samples; however, the adversarial training problem (4) focuses only on the training samples. To evaluate the adversarial generalization performance, we extend the notion of margin loss defined earlier in (1) to adversarial training settings by defining the adversarial margin loss as

Ladv(fw) = P

fw(X + wadv

X)

[Y ]   + max
j=Y

fw

X + wadv(X) [j]

.

(8)

Here, we measure the margin loss over adversarially-perturbed samples, and we use Ladv(fw) to
denote the empirical adversarial margin loss. We also use Lfgm(fw), Lpgm(fw), and Lw rm(fw) to denote the adversarial margin losses with FGM (5), PGM (6), and WRM (7) attacks, respectively.

4

Under review as a conference paper at ICLR 2019

3 MARGIN-BASED ADVERSARIAL GENERALIZATION BOUNDS

As previously discussed, generalization performance can be different between adversarial and nonadversarial settings. In this section, we provide generalization bounds for DNN classifiers under adversarial attacks in terms of the spectral norms of the trained DNN's weight matrices. The bounds motivate regularizing these spectral norms in order to limit the DNN's capacity and improve its generalization performance under adversarial attacks.

We use the PAC-Bayes framework (McAllester, 1999; 2003) to prove our main results. To derive adversarial generalization error bounds for DNNs with smooth activation functions , we first extend a recent result on the margin-based generalization bound for the ReLU activation function (Neyshabur et al., 2017a) to general 1-Lipschitz activation functions.

Theorem 1. Consider Fnn = {fw : w  W} the class of d hidden-layer neural networks with h units per hidden-layer with 1-Lipschitz activation  satisfying (0) = 0. Suppose that X , X's support

set, is norm-bounded as x 2  B, x  X . Also assume for constant M  1 any fw  Fnn

satisfies

i : 1  Wi 2  M, M w

w :=

d
Wi 2 1/d.
i=1

Here w denotes the geometric mean of fw's spectral norms across all layers. Then, for any ,  > 0, with probability at least 1 -  for any fw  Fnn we have:

L0(fw)  L (fw) + O

B2d2h

log(dh)erm(fw)

+

d

log

dn

log 

M

2n

,

where we define complexity score erm(fw) :=

d i=1

Wi

2 2

.d

Wi

2 F

i=1

Wi

2 2

Proof. We defer the proof to the Appendix. The proof is a slight modification of Neyshabur et al. (2017a)'s proof of the same result for ReLU activation.

We now generalize this result to adversarial settings where the DNN's performance is evaluated under adversarial attacks. We prove three separate adversarial generalization error bounds for FGM, PGM, and WRM attacks.

For the following results, we consider Fnn, the class of neural nets defined in Theorem 1. Moreover, we assume that the training loss (y^, y) and its first-order derivative are 1-Lipschitz. Similar to Sinha et al. (2018), we assume the activation  is smooth and its derivative  is 1-Lipschitz. This class of activations include ELU (Clevert et al., 2015) and tanh functions but not the ReLU function. However, our numerical results in Table 1 from the Appendix suggest similar generalization performance between ELU and ReLU activations.

Theorem 2. Consider Fnn, X in Theorem 1 and training loss function satisfying the assumptions

stated above. We consider an FGM attack with noise power according to Euclidean norm · 2.

For any fw  Fnn assume   x fw(x), y 2 holds for constant  > 0, any y  Y, and any

x



B

,

·

(X )
2

-close to X's support set. Then, for any ,  > 0 with probability 1 -  the following

bound holds for the FGM margin loss of any fw  Fnn

L0fgm(fw)  Lfgm(fw) + O

(B +

)2d2h

log(dh) fg,m(fw)

+

d

log

dn log M 

2n

,

where fg,m(fw) :=

d i=1

Wi

2(1+( /)(

d i=1

Wi 2)

d i=1

i j=1

Wj

2)

2

.d

Wi

2 F

i=1

Wi

2 2

Proof. We defer the proof to the Appendix.

Note that the above theorem assumes that the change rate for the loss function around test samples is at least , which gives a baseline for measuring the attack power . In our numerical experiments, we validate this assumption over standard image recognition tasks. Next, we generalize this result to adversarial settings with PGM attack, i.e. the iterative version of FGM attack.

5

Under review as a conference paper at ICLR 2019

Theorem 3. Consider Fnn, X and training loss function for which the assumptions in Theorem 2 hold. We consider a PGM attack with noise power given Euclidean norm · 2, r iterations for attack, and stepsize . Then, for any ,  > 0 with probability 1 -  the following bound applies to
the PGM margin loss of any fw  Fnn

Lp0gm(fw)  Lpgm(fw) + O

(B +

)2d2h

log(dh)

p,gm,r, (fw )

+

d

log

rdn log 

M

2n

.

Here we define p,gm,r,(fw) as the following expression

d i=1

Wi 2

1 - (2/)rlip( 1 + (/)
1 - (2/)lip(

 fw)r  fw)

d i=1

Wi 2

di i=1 j=1

Wj

2

2d i=1

Wi Wi

2

F 2

,

2

where lip(  fw) :=

d i=1

Wi 2

chitz constant of x (fw x), y .

d i=1

i j=1

Wj

2 provides an upper-bound on the Lips-

Proof. We defer the proof to the Appendix.

In the above result, notice that if lip(  fw)/ < 1/(2) then for any number of gradient steps the PGM margin-based generalization bound will grow the FGM generalization error bound in Theorem
2 by factor 1/ 1 - (2/)lip(  fw) . We next extend our adversarial generalization analysis to WRM attacks.
Theorem 4. For neural net class Fnn and training loss satisfying Theorem 2's assumptions, consider a WRM attack with Lagrangian coefficient  and Euclidean norm · 2. Given parameter 0 <  < 1, assume lip(  fw) defined in Theorem 3 is upper-bounded by (1 -  ) for any fw  Fnn. For any  > 0, the following WRM margin-based generalization bound holds with probability 1 -  for any fw  Fnn:

L0wrm(fw)  Lw rm(fw) + O

(B

+

1 

d i=1

Wi

2)2d2h

log(dh)wrm(fw)

+

d

log

dn log M 

2n

where we define

w rm(fw) :=

d

1d

di

i=1

Wi

2

1+  - lip(

(  fw) i=1

Wi

2)
i=1 j=1

Wj

2

d 2
i=1

Wi Wi

2

F 2

.

2

Proof. We defer the proof to the Appendix.

As discussed by Sinha et al. (2018), the condition lip(  fw) <  for the actual Lipschitz constant of   fw is in fact required to guarantee WRM's convergence to the global solution. Notice that the WRM generalization error bound in Theorem 4 is bounded by the product of 1 and
-lip( fw)
the FGM generalization bound in Theorem 2.

4 SPECTRAL NORMALIZATION OF CONVOLUTIONAL LAYERS
To control the Lipschitz constant of our trained network, we need to ensure that the spectral norm associated with each linear operation in the network does not exceed some pre-specified . For fully-connected layers (i.e. regular matrix multiplication), please see Appendix B. For convolutional layers, (Sedghi et al., 2018) develops a computationally-efficient method for computing all the singular values including the largest one, i.e. the spectral norm. While elegant, the method only applies to convolution filters with stride 1 and zero-padding. However, in practice the normalization factor depends on the stride size and padding scheme governing the convolution operation. Here we develop an efficient approach for effiecntly computing the maximum singular value, i.e. spectral norm, of convolutional layers wth arbitary stride and padding schemes. Note that, as also discussed

6

Under review as a conference paper at ICLR 2019

by Gouk et al. (2018), the ith convolutional layer output feature map i is a linear operation of the input X:

M
i(X) = Fi,j
j=1

Xj ,

where X has M feature maps, Fi,j is a filter, and denotes the convolution operation (which also encapsulates stride size and padding scheme). For simplicity, we ignore the additive bias terms here.
By vectorizing X and letting Vi,j represent the overall linear operation associated with Fi,j, we see that

i(X) = [V1,1 . . . V1,M ] X,

and therefore the overall convolution operation can be described using

 V1,1 . . . V1,M 

(X) =  

...

...

...

 X = W X. 

VN,1 . . . VN,M

While explicitly reconstructing W is expensive, we can still compute (W ), the spectral norm of W ,
by leveraging the convolution transpose operation implemented by several modern-day deep learning packages. This allows us to efficiently performs matrix multiplication with W T without explicitly constructing W . Therefore we can approximate (W ) using a modified version of power iteration
(Algorithm 1), wrapping the appropriate stride size and padding arguments into the convolution and convolution transpose operations. After obtaining (W ), we compute WSN in the same manner as for the fully-connected layers. Like Miyato et al., we exploit the fact that SGD only makes small updates to W from training step to training step, reusing the same u~ and running only one iteration per step. Unlike Miyato et al., rather than enforcing (W ) = , we instead enforce the looser constraint (W )  :

WSN = W/ max(1, (W )/),

(9)

which we observe to result in faster training for supervised learning tasks.

Algorithm 1 Convolutional power iteration
Initialize u~ with a random vector matching the shape of the convolution input for t = 0, ..., T - 1 do
v~  conv(W, u~)/ conv(W, u~) 2 u~  conv_transpose(W, v~)/ conv_transpose(W, v~) 2 end for   v~ · conv(W, u~)

5 NUMERICAL EXPERIMENTS
In this section we provide an array of empirical experiments to validate both the bounds we derived in Section 3 and our implementation of spectral normalization described in section 4. We show that spectral normalization improves both test accuracy and generalization for a variety of adversarial training schemes, datasets, and network architectures.
All experiments are implemented in TensorFlow (Abadi et al., 2016). For each experiment, we cross validate 4 to 6 values of  (see (9)) using a fixed validation set of 500 samples. For PGM, we used r = 15 iterations and  = 2 /r. Additionally, for FGM and PGM we used 2-type attacks (unless specified) with magnitude = 0.05EP^[ X 2] (this value was approximately 2.44 for CIFAR10). For WRM, we implemented gradient ascent as discussed by Sinha et al. (2018). Additionally, for WRM training we used a Lagrangian coefficient of 0.002EP^[ X 2] for CIFAR10 and SVHN and a Lagrangian coefficient of 0.04EP^[ X 2] for MNIST in a similar manner to Sinha et al. (2018). The code will be made readily available.
7

Under review as a conference paper at ICLR 2019

5.1 VALIDATION OF SPECTRAL NORMALIZATION IMPLEMENTATION AND BOUNDS
We first demonstrate the effect of the proposed spectral normalization approach on the final DNN weights by comparing the 2 norm of the input x to that of the output fw(x). As shown in Figure 2(a), without spectral normalization ( =  in (9)), the norm gain can be large. Additionally, because we are using cross-entropy loss, the weights (and therefore the norm gain) can grow arbitrarily high if we continue training as reported by Neyshabur et al. (2017b). As we decrease , however, we produce more constrained networks, resulting in a decrease in norm gain. At  = 1, the gain of the network cannot be greater than 1, which is consistent with what we observe.
Figure 2(b) shows that the 2 norms of the gradients with respect to the training samples are nicely distributed after spectral normalization. Additionally, this figure suggests that the minimum gradient 2-norm assumption (the  condition in Theorems 2 and 3) holds for spectrally-normalized networks.
The first column of Figure 3 shows that, as observed by Bartlett et al. (2017), AlexNet trained using ERM generates similar margin distributions for both random and true labels on CIFAR10 unless we normalize the margins appropriately. We see that even without further correction, ERM training with SN allows AlexNet to have distinguishable performance between the two datasets. This observation suggests that SN as a regularization scheme enforces the generalization error bounds shown for spectrally-normalized DNNs by Bartlett et al. (2017) and Neyshabur et al. (2017a). Additionally, the margin normalization factor (the capacity norm  in Theorems 1-4) is much smaller for networks trained with SN. As demonstrated by the other columns in Figure 3, a smaller normalization factor results in larger normalized margin values and much tighter margin-based generalization bounds (a factor of 102 for ERM and a factor of 105 for FGM and PGM) (see Theorems 1-4).

AlexNet Norm Gain

=1.0 =1.3 =1.6 =2.0 =4.0 =

ERM FGM

0.0 x 0(.f5w(x)) 2 1.0 PGM

0.0 0x.2(fw(x0).)42 0.6 WRM

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 ||f(x)||/||x||
(a) 2 norm gain due to the network f trained using ERM.

0.00 0x.2(5fw(x))0.250

0.00 0x.2(5fw(x)0).520 0.75

(b) Distributions of the 2 norms of the gradients with respect to training samples. Training regularized with SN.

Figure 2: Validation of SN implementation and distribution of the gradient norms using AlexNet trained on CIFAR10.

ERM
True Random

ERM normalized

FGM normalized

PGM normalized

margins
ERM SN

0.0 0.5 1.0 11.e5 4 2 0 2 4 1e 69

ERM SN normalized

FGM SN normalized

0 1 2 1e 39
PGM SN normalized

0.5 0.0

0.5 1e 2

0 1 1e 4

0 1 1e 4

Figure 3: Effect of SN on distributions of unnormalized (leftmost column) and normalized (other three columns) margins for AlexNet fit on CIFAR10.

8

Under review as a conference paper at ICLR 2019

5.2 SPECTRAL NORMALIZATION IMPROVES GENERALIZATION AND ADVERSARIAL
ROBUSTNESS

The phenomenon of overfitting random labels described by Zhang et al. (2016) can be observed even for adversarial training methods. Figure 4 shows how the FGM, PGM, or WRM adversarial training schemes only slightly delay the rate at which AlexNet fits random labels on CIFAR10, and therefore the generalization gap can be quite large without proper regularization. After introducing spectral normalization, however, we see that the network has a much harder time fitting both the random and true labels. With the proper amount of SN (chosen via cross validation), we can obtain networks that struggle to fit random labels while still obtaining the same or better test performance on true labels.
We also observe that training schemes regularized with SN result in networks more robust to adversarial attacks. Figure 5 shows that even without adversarial training, AlexNet with SN becomes more robust to FGM, PGM, and WRM attacks. Adversarial training improves adversarial robustness more than SN by itself; however we see that we can further improve the robustness of the trained networks significantly by combining SN with adversarial training.

training accuracy

1.0 0.8 0.6 0.4 0.2 0.0 0

Random labels

1.0

0.8

0.6

ERM 0.4

FGM PGM

0.2

WRM

20000 40000 60000 80000 0.0 0

training steps

Random labels (SN)

1.0

0.8

0.6

0.4

0.2

20000 40000 60000 80000 0.0 0 training steps

True labels (SN)
20000 40000 60000 80000 training steps

Figure 4: Fitting random and true labels on CIFAR10 with AlexNet using adversarial training.

error error

FEGRMM 2traatintaincgks
0.8 No SN SN
0.6

0.4

0.2 024

0.6

FFGGMM

2 2

tartatiancinkgs

0.5

0.4

0.3

024

PEGRMM 2traatintaincgks
0.8

0.6 0.4

0.2 024

PPGGMM

2 2

tartatiancinksg

0.6

0.5

0.4

0.3

024

PEGRMM traatintaincgks
0.8 0.6 0.4 0.2
0.0 0.2 0.4
PPGGMM tartatiancinkgs
0.8 0.6 0.4
0.0 0.2 0.4

0.26 WERRMMtarattiancinkgs
0.25 0.24 0.23 0.22 0.21
0.0 0.2 0.4 0.6
WWRRMM tartatiancinkgs
0.40
0.35
0.30
0.25
024

Figure 5: Robustness of AlexNet trained on CIFAR10 to various adversarial attacks.

5.3 OTHER DATASETS AND ARCHITECTURES
We demonstrate the power of regularization via SN on several combinations of datasets, network architectures, and adversarial training schemes. The datasets we evaluate are CIFAR10, MNIST, and SVHN. We fit CIFAR10 using the AlexNet and Inception networks described by Zhang et al. (2016), 1-hidden-layer and 2-hidden-layer multi layer perceptrons (MLPs) with ELU activation and 512 hidden nodes in each layer, and the ResNet architecture (He et al. (2016)) provided in TensorFlow for fitting CIFAR10. We fit MNIST using the ELU network described by Sinha et al. (2018) and the 1-hidden-layer and 2-hidden-layer MLPs. Finally, we fit SVHN using the same AlexNet architecture
9

Under review as a conference paper at ICLR 2019

we used to fit CIFAR10. Our implementations do not use any additional regularization schemes including weight decay, dropout (Srivastava et al., 2014), and batch normalization (Ioffe & Szegedy, 2015).

Table 1 in the Appendix reports the pre and post-SN test accuracies for all 42 combinations evaluated. Figure 1 in the Introduction and Figures 7-9 in the Appendix show examples of training and validation curves on some of these combinations. We see that the validation curve generally improves after regularization with SN, and the observed improvements in validation accuracy are confirmed by the test accuracies reported in Table 1. Figure 6 visually summarizes Table 1, showing how SN can often significantly improve the test accuracy (and therefore decrease the generalization gap) for several of the combinations. We see that networks fitted on MNIST generally do quite well even in the presence of adversarial attacks, and therefore improvements to these networks are more modest compared to networks fitted on SVHN or CIFAR10.

Test acc with SN

ERM training
1.0 0.8 0.6 0.4 CIFAR10 0.2 MNIST 0.0 SVHN
0.0 0.5 1.0 Test acc without SN

FGM training
1.0 0.8 0.6 0.4 0.2 0.0
0.0 0.5 1.0 Test acc without SN

PGM training
1.0 0.8 0.6 0.4 0.2 0.0
0.0 0.5 1.0 Test acc without SN

WRM training
1.0 0.8 0.6 0.4 0.2 0.0
0.0 0.5 1.0 Test acc without SN

Figure 6: Test accuracies before and after SN for various datasets and network architectures. Please see Table 1 in the Appendix for more details. The vertical distance from each point to the y = x line equals the test accuracy gain after SN.

6 RELATED WORKS
Providing theoretical guarantees for adversarial robustness of various classifiers has been studied in multiple works. Wang et al. (2017) targets analyzing the adversarial robustness of the nearest neighbor approach. Gilmer et al. (2018) studies the effect of the complexity of the data-generating manifold on the final adversarial robustness for a specific trained model. Fawzi et al. (2018) proves lower-bounds for the complexity of robust learning in adversarial settings, targeting the population distribution of data. Xu et al. (2009) shows that the regularized support vector machine (SVM) can be interpreted via robust optimization. Fawzi et al. (2016) analyzes the robustness of a fixed classifier to random and adversarial perturbations of the input data. While all of these works seek to understand the robustness properties of different classification function classes, unlike our work they do not focus on the generalization aspects of learning over DNNs under adversarial attacks.
Concerning the generalization aspect of adversarial training, Sinha et al. (2018) provides optimization and generalization guarantees for WRM under the assumptions discussed after Theorem 4. However, their generalization guarantee only applies to the Wasserstein cost function, which is different from the 0-1 or margin loss and does not explicitly suggest a regularization scheme. In a recent related work, Schmidt et al. (2018) numerically shows the wide generalization gap in PGM adversarial training and theoretically establishes lower-bounds on the sample complexity of linear classifiers in Gaussian settings. While our work does not provide sample complexity lower-bounds, we study the broader function class of DNNs where we provide upper-bounds on adversarial generalization error and suggest an explicit regularization scheme for adversarial training over DNNs.
We note that understanding the generalization aspects of deep learning has been a topic of great interest in machine learning (Zhang et al., 2016). In addition to margin-based bounds (Bartlett et al., 2017; Neyshabur et al., 2017a), various other tools including VC dimension (Anthony & Bartlett, 2009), norm-based capacity scores (Bartlett & Mendelson, 2002; Neyshabur et al., 2015), and flatness of local minima (Keskar et al., 2016; Neyshabur et al., 2017b) have been used to analyze generalization properties of DNNs. Recently, Arora et al. (2018) introduced a compression approach to further improve the margin-based bounds presented by Bartlett et al. (2017) and Neyshabur et al. (2017a). The PAC-Bayes bound has also been considered and computed by Dziugaite & Roy (2017), resulting in non-vacuous bounds for MNIST.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge university press, 2009.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, pp. 274­283, 2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241­6250, 2017.
Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39­57. IEEE, 2017.
Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Gintare Karolina Dziugaite and Daniel M Roy. Entropy-sgd optimizes the prior of a pac-bayes bound: Datadependent pac-bayes priors via differential privacy. arXiv preprint arXiv:1712.09376, 2017.
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from adversarial to random noise. In Advances in Neural Information Processing Systems, pp. 1632­1640, 2016.
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv preprint arXiv:1802.08686, 2018.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014b.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630­645. Springer, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.
11

Under review as a conference paper at ICLR 2019
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
David McAllester. Simplified pac-bayesian margin bounds. In Learning theory and Kernel machines, pp. 203­215. Springer, 2003.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual conference on Computational learning theory, pp. 164­170. ACM, 1999.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In COLT, pp. 1376­1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 5949­5958, 2017b.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP), pp. 582­597. IEEE, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506­519. ACM, 2017.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. arXiv preprint arXiv:1805.10408, 2018.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1): 1929­1958, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1­9, 2015.
Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations, 2018.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389­434, 2012.
Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors to adversarial examples. arXiv preprint arXiv:1706.03922, 2017.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. Journal of Machine Learning Research, 10(Jul):1485­1510, 2009.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
12

Under review as a conference paper at ICLR 2019

Appendices

A FURTHER EXPERIMENTAL RESULTS

Table 1: Train and test accuracies before and after spectral normalization for various datasets, network architectures, and training schemes. The amount of spectral normalization was selected from 4-6 values of  via cross validation on 500 samples. For each row, the greater test accuracy is bolded (both are bolded in the event of a tie).  adversarial training was performed with magnitude 0.1.

Dataset Architecture Training Train acc Test acc Train acc (SN)

CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 MNIST MNIST MNIST MNIST MNIST MNIST MNIST MNIST MNIST MNIST MNIST MNIST SVHN SVHN SVHN SVHN

AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet ELU-AlexNet ELU-AlexNet ELU-AlexNet ELU-AlexNet Inception Inception Inception Inception 1-layer MLP 1-layer MLP 1-layer MLP 1-layer MLP 2-layer MLP 2-layer MLP 2-layer MLP 2-layer MLP ResNet ResNet ResNet ResNet ELU-Net ELU-Net ELU-Net ELU-Net 1-layer MLP 1-layer MLP 1-layer MLP 1-layer MLP 2-layer MLP 2-layer MLP 2-layer MLP 2-layer MLP AlexNet AlexNet AlexNet AlexNet

ERM
FGM 2 FGM  PGM 2 PGM  WRM
ERM
FGM 2 PGM 2 WRM
ERM
PGM 2 PGM  WRM
ERM
FGM 2 PGM 2 WRM
ERM
FGM 2 PGM 2 WRM
ERM
PGM 2 PGM  WRM
ERM
FGM 2 PGM 2 WRM
ERM
FGM 2 PGM 2 WRM
ERM
FGM 2 PGM 2 WRM
ERM
FGM 2 PGM 2 WRM

1.00 0.79 0.98 0.54 1.00 0.51 0.99 0.50 0.99 0.44 1.00 0.61 1.00 0.79 0.97 0.52 0.98 0.53 1.00 0.60 1.00 0.85 0.99 0.53 0.98 0.48 1.00 0.66 0.98 0.49 0.60 0.36 0.57 0.36 0.60 0.41 0.99 0.51 0.57 0.36 0.93 0.35 0.87 0.35 1.00 0.80 0.99 0.49 0.98 0.44 1.00 0.63 1.00 0.99 0.98 0.97 0.99 0.97 0.95 0.92 1.00 0.98 0.88 0.88 1.00 0.96 0.92 0.88 1.00 0.98 0.97 0.91 1.00 0.96 0.97 0.88 1.00 0.93 0.97 0.76 1.00 0.78 1.00 0.83

1.00 0.93 0.67 0.92 0.86 0.76 1.00 0.68 0.88 1.00 1.00 1.00 0.62 1.00 0.68 0.60 0.55 0.62 0.79 0.66 0.66 0.73 1.00 1.00 0.72 1.00 1.00 1.00 1.00 0.95 1.00 1.00 1.00 0.92 1.00 1.00 1.00 0.98 1.00 0.95 0.85 0.87

*  =  (i.e. no spectral normalization) achieved the highest validation accuracy.

Test acc (SN)
0.79 0.63 0.56 0.62 0.54 0.65 0.79 0.60 0.61 0.60 0.86 0.58 0.56 0.67 0.53 0.46 0.46 0.50 0.56 0.49 0.48 0.52 0.83 0.55 0.53 0.66 0.99* 0.97 0.97 0.93 0.98* 0.96 0.96 0.88 0.98 0.96 0.97 0.90 0.93* 0.83 0.81 0.84

13

Under review as a conference paper at ICLR 2019

accuracy

AlexNet FGM ( = 0.1) AlexNet PGM ( = 0.1) AlexNet FGM ( = 0.3) AlexNet PGM ( = 0.3)
1.0 1.0 1.0 1.0

0.8 0.8 0.8 0.8

0.6 0.6 0.6 0.6

0.4 0.2 0.0 0

train valid train (SN) valid (SN) 25000 50000 75000 training steps

0.4 0.2 0.0 0

25000 50000 75000 training steps

0.4 0.2 0.0 0

25000 50000 75000 training steps

0.4 0.2 0.0 0

25000 50000 75000 training steps

Figure 7: Adversarial training performance with and without spectral normalization for AlexNet fit on CIFAR10.

accuracy

Inception PGM
1.0

Inception WRM
1.0

ResNet PGM
1.0

ResNet WRM
1.0

0.8 0.8 0.8 0.8

0.6 0.6 0.6 0.6

0.4 0.2 0.0 0

train 0.4

valid train (SN)

0.2

valid (SN)

2t5ra0i0n0ing5s0t0e0p0s 75000 0.0 0

0.4 0.2 2t5ra0i0n0ing5s0t0e0p0s 75000 0.0 0

0.4 0.2 2t5ra0i0n0ing5s0t0e0p0s 75000 0.0 0

t2ra0i0n0in0g st4e0p0s00

Figure 8: Adversarial training performance with and without spectral normalization for Inception and ResNet fit on CIFAR10.

accuracy

1.0 0.8 0.6 0.4 0.2 0.0 0

ELU-AlexNet FGM

1.0

0.8

0.6

train 0.4

valid train (SN)

0.2

valid (SN)

20000 40000 60000 80000 0.0 0

training steps

ELU-AlexNet PGM

1.0

0.8

0.6

0.4

0.2

20000 40000 60000 80000 0.0 0 training steps

ELU-AlexNet WRM
20000 40000 60000 80000 training steps

Figure 9: Adversarial training performance with and without spectral normalization for AlexNet with ELU activation functions fit on CIFAR10.

B SPECTRAL NORMALIZATION OF FULLY-CONNECTED LAYERS
For fully-connected layers, we approximate the spectral norm of a given matrix W using the approach described by Miyato et al. (2018): the power iteration method. For each W , we randomly initialize a vector u~ and approximate both the left and right singular vectors by iterating the update rules
v~  W u~/ W u~ 2
14

Under review as a conference paper at ICLR 2019

u~  W T v~/ W T v~ 2.
The final singular value can be approximated with (W )  v~T W u~. Like Miyato et al., we exploit the fact that SGD only makes small updates to W from training step to training step, reusing the same u~ and running only one iteration per step. Unlike Miyato et al., rather than enforcing (W ) = , we instead enforce the looser constraint (W )   as described by Gouk et al. (2018):
WSN = W/ max(1, (W )/),
which we observe to result in faster training in practice for supervised learning tasks.

C PROOFS

C.1 PROOF OF THEOREM 1

First let us quote the following two lemmas from (Neyshabur et al., 2017a).

Lemma 1 (Neyshabur et al. (2017a)). Consider Fnn = {fw : w  W} as the class of neural nets parameterized by w where each fw maps input x  X to Rm. Let Q be a distribution on parameter

vector chosen independently from the n training samples. Then, for each  > 0 with probability

at least 1 -  for any w and any random perturbation u satisfying Pru maxxX fw+u(x) -

fw(x)





 4



1 2

we

have

L0(fw)  L (fw) + 4

K L(Pw+u

Q)

+

log

6n 

.

n-1

(10)

Lemma 2 (Neyshabur et al. (2017a)). Consider a d-layer neural net fw with 1-Lipschitz activation

function  where (0) = 0. Then for any norm-bounded input x 2  B and weight perturbation

u:

Ui

2

1 d

Wi

2, we have the following perturbation bound:

d

fw+u(x) - fw(x) 2  eB

Wi 2

i=1

d i=1

Ui 2 . Wi 2

(11)

To

prove

Theorem

1,

consider

fw

with

weights

w.

Since

(1 +

1 d

)d



e

and

1 e



(1 -

1 d

)d-1,

for

any

weight vector w such that

Wi

2-

Wi 2



1 d

Wi

2 for every i we have:

dd

d

d
(1/e) d-1

Wi 2 

Wi 2  e

Wi 2.

i=1 i=1

i=1

(12)

We apply Lemma 1, choosing Q to be a zero-mean multivariate Gaussian distribution with diagonal

covariance matrix, where each entry of the ith layer Ui has standard deviation i =

Wi w

2

with

 chosen later in the proof. Note that w defined earlier in the theorem is the geometric average of

spectral norms across all layers. Then for the ith layer's random perturbation vector ui  N (0, i2I),

we get the following bound from (Tropp, 2012) with h representing the width of the ith hidden layer:

Pr w

Ui 2 > t Wi 2

t2  2h exp(- 2h2 ).

(13)

We now use a union bound over all layers for a maximum union probability of 1/2, which implies the

normalized w

Ui 2 for each layer is upper-bounded by 
Wi 2

2h log(4hd). Then for any w satisfying

Wi

2-

Wi

2



1 d

Wi

2 for all i's

max
x 2B

d

fw+u(x) - fw(x) 2  eB

Wi 2

i=1

d Ui 2 i=1 Wi 2

(a) d

 e2B

Wi 2

d

Ui 2

i=1 i=1 Wi 2

15

Under review as a conference paper at ICLR 2019

d
= e2Bwd-1 w
i=1

Ui 2 Wi 2

 e2dBwd-1 2h log(4hd).

(14)

Here (a) holds, since

1 Wj

d i=1

Wi 2 

e Wj

d i=1

Wi

2 is true for each j. Hence we choose



=


30dBwd-1 h log(4hd)

for

which

the

perturbation

vector

satisfies

the

assumptions

of

Lemma

2.

Then, we bound the KL-divergence term in Lemma 1 as

d
KL(Pw+u Q) 
i=1

Wi

2 F

2i2

=

302d2B2w2dh log(4hd) 22

d

i=1

Wi

2 F

Wi

2 2

(b)


302e2d2B2

d i=1

Wi

2 2

h

log(4hd)

d

Wi

2 F

22

i=1

Wi

2 2

= O d2B2h log(hd)

d i=1

Wi

2 2

d

2
i=1

Wi

2 F

Wi

2 2

.

Note that (b) holds, because we assume

Wi 2 - Wi 2



1 d

Wi 2

implying

1 Wj

d i=1

Wi

2



(1

-

1 d

)-(d-1)

1 Wj

d i=1

Wi 2 

e Wj

d i=1

Wi

2 for each j. There-

fore, Lemma 1 implies with probability 1 -  we have the following bound hold for any w satisfying

Wi

2-

Wi

2



1 d

Wi

2 for all i's,

L0(fw)  L (fw) + O

B2d2h

log(dh)erm(fw)

+

log

n 

2n

.

(15)

Then, we can give an upper-bound over all the functions in Fnn by finding the covering number of the

set of w's where for each feasible w we have the mentioned condition satisfied for at least one of w's.

We

only

need

to

form

the

bound

for

(

 2B

)1/d



w



(

n 2B

)1/d

which

can

be

covered

using

a

cover

of size dn1/2d as discussed in (Neyshabur et al., 2017a). Then, from the theorem's assumption we

know each

Wi

2

will

be

in

the

interval

[

1 M

w,

M

w]

which

we

want

to

cover

such

that

for

any



in the interval there exists a  satisfying | - |  /d. For this purpose we can use a cover of size

2 log1+1/d M  2(d + 1) log M ,1 which combined for all i's gives a cover with size O((d log M )d)

whose logarithm is growing as d log(d log M ). This together with (15) completes the proof.

C.2 PROOF OF THEOREM 2

We start by proving the following lemmas providing perturbation bound for FGM attacks.

Lemma 3. Consider a d-layer neural net fw with 1-Lipschitz and 1-smooth (1-Lipschitz derivative)

activation  where (0) = 0. Let training loss : (Rm, Y)  R also be 1-Lipschitz and 1-smooth

for any fixed label y  Y. Then, for any input x, label y, and perturbation vector u satisfying

i :

Ui

2



1 d

Wi

2 we have

x fw+u(x), y) - x fw(x), y) 2

dd

 e2

Wi 2

i=1 i=1

Ui 2 + Wi 2

i
x2
j=1

Wj 2

i j=1

Uj 2 Wj 2

.

(16)

Proof. Since for a fixed y satisfies the same Lipschitzness and smoothness properties as , then z (z, y) 2  1 and applying the chain rule implies:

x fw+u(x), y) - x fw(x), y) 2

1Note

that

log

1 x



1

-

x

implying

log

1

1-

1 d+1



1 d+1

and hence (log(1 + 1/d))-1  d + 1.

16

Under review as a conference paper at ICLR 2019

= xfw+u(x) ( ) fw+u(x), y) - xfw(x) ( ) fw(x), y) 2

 xfw+u(x) ( ) fw+u(x), y) - xfw(x) ( ) fw+u(x), y) 2

+ xfw(x) ( ) fw+u(x), y) - xfw(x) ( ) fw(x), y) 2

d

 xfw+u(x) - xfw(x) 2 +

Wi 2 ( ) fw+u(x), y) - ( ) fw(x), y) 2

i=1

d

 xfw+u(x) - xfw(x) 2 +

Wi 2 fw+u(x) - fw(x) 2

i=1



dd

xfw+u(x) - xfw(x) 2 + e x 2

Wi 2 2

i=1 i=1

Ui 2 . Wi 2

(17)

The above result is a conclusion of Lemma 2 and the lemma's assumptions implying xfw(x) 2 

d i=1

Wi 2 for every x. Now, we define k =

xfw(k+) u(x) - xfw(k)(x) 2 where fw(k)(x) :=

Wk(Wk-1 · · · (W1x)) · · · ) denotes the DNN's output at layer k. With (17) in mind, we complete

this lemma's proof by showing the following inequality via induction:

k



e(1

+

1 )k d

k

Wi 2

k

i=1 i=1

Ui 2 + Wi 2

i-1
x2
j=1

Wj 2

i-1 j=1

Uj 2 Wj 2

.

(18)

The

above

equation

will

prove

the

lemma

because

for

k



d

we

have

(1

+

1 d

)k



(1

+

1 d

)d



e.

For

k = 0, 0 = 0 since fw(0)(x) = x and does not change with w. Given that (18) holds for k we have

k+1 = xfw(k++u1)(x) - xfw(k+1)(x) 2

= x(Wk+1 + Uk+1)(fw(k+) u(x)) - xWk+1(fw(k)(x)) 2

= xfw(k+) u(x) (fw(k+) u(x))(Wk+1 + Uk+1)T - xfw(k)(x) (fw(k)(x))WkT+1 2

 xfw(k+) u(x) (fw(k+) u(x))(Wk+1 + Uk+1)T - xfw(k+) u(x) (fw(k)(x))(Wk+1 + Uk+1)T

+ xfw(k+) u(x) (fw(k)(x))(Wk+1 + Uk+1)T - xfw(k+) u(x) (fw(k)(x))WkT+1 2

+ xfw(k+) u(x) (fw(k)(x))WkT+1 - xfw(k)(x) (fw(k)(x))WkT+1 2



(1

+

1 )

d

Wk+1

2

xfw(k+) u(x)

2

fw(k+) u(x) - fw(k)(x)

2

+ Uk+1 2 xfw(k+) u(x) 2  (fw(k)(x)) 2 + Wk+1 2  (fw(k)(x)) 2k

2

 (1 + 1 )k+1 k+1 d

Wi

2

i=1

k

ex2

Wi 2

i=1

k i=1

Ui 2 Wi 2

+ (1 + 1 )k d

k+1

Wi

2

i=1

Uk+1 2 + Wk+1 2

Wk+1 2k

 e(1 + 1 )k+1 d

k+1

Wi

2

i=1

Uk+1 2 + Wk+1 2

k
x 2 Wi 2
i=1

k i=1

Ui 2 Wi 2

+ Wk+1 2k

 e(1 + 1 )k+1 k+1 d

Wi

2

k+1

i=1 i=1

Ui 2 + Wi 2

i-1
x2
j=1

Wj 2

i-1 j=1

Uj 2 Wj 2

.

Therefore, combining (17) and (18) the lemma's proof is complete

Before presenting the perturbation bound for FGM attacks, we first prove the following simple

lemma.

Lemma 4. Consider vectors z1, z2 and norm function · . If max{ z1 , z2 }  , then

z1

z1 -

z2 z2

2 

z1 - z2 .

(19)

17

Under review as a conference paper at ICLR 2019

Proof. Without loss of generality suppose z2  z1 and therefore   z1 . Then,

z1 z1 - z2 z2 =

1 z1

(z1 - z2) -

z1

- z1

z2

1 z2 z2

 z1

z1 - z2 + z1

2 z1

z1 - z2

2

 

z1 - z2 .

z1 - z2

Lemma 5. Consider a d-layer neural network function fw with 1-Lipschitz, 1-smooth activation

 where (0) = 0. Consider FGM attacks with noise power according to Euclidean norm || · ||2.

Suppose   x (fw(x), y) 2 holds over the -ball around the support set X . Then, for any

norm-bounded perturbation vector u such that

Ui

2



1 d

Wi

2. i, we have

wfg+mu(x) - wfgm(x)

2

2e2 

dd
Wi 2
i=1 i=1

Ui 2 + Wi 2

i
x2
j=1

Wj 2

i j=1

Uj 2 Wj 2

.

Proof. The FGM attack according to Euclidean norm is simply the DNN loss's gradient normalized to have -Euclidean norm. The lemma is hence a direct result of combining Lemmas 3 and 4.

To prove Theorem 2, we apply Lemma 1 together with the result in Lemma 5. Similar to the proof for

Theorem 1, given weights w we consider a zero-mean multivariate Gaussian perturbation vector u

with diagonal covariance matrix where each element in the ith layer ui varies with the scaled standard

deviation i =

Wi w

2



with



properly

chosen

later

in

the

proof.

Consider

weights

w

for

which

1

i :

Wi 2 -

Wi

2

 d

Wi 2.

(20)

Since ui  N (0, i2I), (Tropp, 2012) shows the following bound holds

Pr w

Ui 2 > t Wi 2



2h

exp(-

t2 2h2

).

(21)

Then we apply a union bound over all layers for a maximum union probability of 1/2 implying the

normalized w

Ui 2 Wi 2

for each layer is upper-bounded by 

2h log(4hd). Now, if the assumptions

of Lemma 5 hold for perturbation vector u given the choice of , for the FGM attack with noise

power according to Euclidean norm · 2 we have

fw+u x + wfg+mu(x) - fw x + wfgm(x) 2

(22)

 fw+u x + wfg+mu(x) - fw x + wfg+mu(x) 2 + fw x + wfg+mu(x) - fw x + wfgm(x) 2

d

 fw+u x + wfg+mu(x) - fw x + wfg+mu(x) 2 +

Wi 2 wfg+mu(x) - wfgm(x) 2

i=1

dd
 e(B + ) Wi 2
i=1 i=1

Ui 2 + 2e2 d

Wi 2


i=1

Wi

d 2 2
i=1

Ui 2 + B i

Wi 2

j=1

Wj

2

i j=1

Uj 2 Wj 2

dd
 e2(B + ) Wi 2
i=1 i=1

Ui 2 + 2e5 d

Wi 2


i=1

Wi

d 2 2
i=1

Ui 2 + B i Wj 2 i

Uj 2

Wi 2

j=1

j=1 Wj 2

dd

di

 2e5d(B + ) 2h log(4hd)

Wi 2 + 

Wi

2 2

1/B +

Wj 2

i=1 i=1

i=1 j=1

.

18

Under review as a conference paper at ICLR 2019

Hence we choose



= 8e5d(B + )

2h log(4hd)

d i=1

Wi 2 1 + 

d i=1

Wi

2(1/B +

d i=1

i j=1

Wj

2)

,

(23)

for which the assumptions of Lemmas 1 and 5 hold. Assuming B  1, similar to Theorem 1's proof

we can show for any w such that

Wi 2 -

Wi 2



1 d

Wi 2 we have

d
KL(Pw+u Q) 
i=1

Wi

2 F

2i2

d
= O d2(B + )2h log(hd) i=1

Wi

2 2

1+ 

 O d2(B + )2h log(hd)

d i=1

Wi

2 2

1+ 

d i=1

Wi

2

2

d i=1

Wi

2

2

d i=1

i j=1

Wj

2

2d

Wi

2 F

i=1

Wi

2 2

d i=1

i j=1

Wj

2

2d

Wi

2 F

i=1

Wi

2 2

Then, applying Lemma 1 reveals that given any  > 0 with probability at least 1 -  for any w such

that

Wi 2 -

Wi 2



1 d

Wi 2 we have

L0fgm(fw)  Lfgm(fw) + O

(B +

)2d2h

log(dh)

fg,m (fw )

+

log

n 

2n

(24)

where fg,m(fw) :=

d i=1

Wi

2(1 + 

d i=1

Wi 2

d i=1

i j=1

Wj

2)

2

.d

Wi

2 F

i=1

Wi

2 2

Note that similar to our proof for Theorem 1 we can find a cover of size O((d log M )ddn1/2d)

for the spectral norms of the weights feasible set, where for any Wi 2 we have ai such that

Wi 2 - ai  ai/d. Applying this covering number bound to (24) completes the proof.

C.3 PROOF OF THEOREM 3

We use the following two lemmas to extend the proof of Theorem 2 for FGM attacks to show Theorem 3 for PGM attacks.

Lemma 6. Consider a d-layer neural network function fw with 1-Lipschitz, 1-smooth activation 

where (0) = 0. We consider PGM attacks with noise power according to Euclidean norm || · ||2, r

iterations and stepsize . Suppose   x fw(x), y 2 holds over the -ball around the support

set X . Then for any perturbation vector u such that

Ui

2

1 d

Wi

2 for every i we have

wpg+mu,r(x) - wpgm,r(x)

2



e2(2/)

1 - (2/)r 1 - (2/)

lip( lip(

 fw)r  fw)

dd
× Wi 2
i=1 i=1

Ui 2 + ( Wi 2

x

2+

)

i j=1

Wj 2

i j=1

Uj 2 Wj 2

.

(25)

Here lip(  fw) denotes the actual Lipschitz constant of x (fw(x), y).

Proof. We use induction to show this lemma for different r values. The result for case r = 1 is a direct consequence of Lemma 5. Suppose that the result is true for r = k. Then, Lemmas 3 and 4 imply

wpg+mu,k+1(x) - wpgm,k+1(x) 2

 2 

x (fw+u(x + wpg+mu,k(x))) - x (fw(x + wpgm,k(x)))

2

 2 

x (fw+u(x + wpg+mu,k(x))) - x (fw(x + wpg+mu,k(x)))

2

2 +


x (fw(x + wpg+mu,k(x))) - x (fw(x + wpgm,k(x)))

2

19

Under review as a conference paper at ICLR 2019

 2 e2 d 

Wi 2

d

i=1 i=1

Ui 2 + ( x Wi 2

2+

i
)
j=1

Wj

2

i j=1

Uj 2 Wj 2

+ 2 lip( 

 fw)

xwpg+mu,k(x) - xwpgm,k(x)

2

 2 e2 d 

Wi 2

d

i=1 i=1

Ui 2 + ( x Wi 2

2+

i
)
j=1

Wj

2

i j=1

Uj 2 Wj 2

2 +

lip(





fw

)e2(2/)

1 - (2/)k 1 - (2/)

lip( lip(

 fw)k  fw)

d i=1

Wi

2

d i=1

Ui 2 + ( Wi 2

x

2+

)

i j=1

Wj 2

i j=1

Uj 2 Wj 2

= e2(2/) 1 - (2/)k+1 lip(  fw)k+1 1 - (2/) lip(  fw)

dd
× Wi 2
i=1 i=1

Ui 2 + ( Wi 2

x

2+

)

i j=1

Wj 2

i j=1

Uj 2 Wj 2

,

where the last line follows from the equality

k i=0

si

=

1-sk+1 1-s

.

Therefore,

by

induction

the

lemma

holds for every value r  1.

Lemma 7. Consider a d-layer neural network function fw with 1-Lipschitz, 1-smooth activation  where (0) = 0. Also, assume that training loss is 1-Lipschitz and 1-smooth. Then,

lip x fw(x), y

d di

 lip(  fw) :=

Wi 2

Wj 2.

i=1 i=1 j=1

(26)

Proof. First of all note that according to the chain rule

lip x fw(x), y = lip xfw(x) ( ) fw(x), y

 lip xfw(x) + lip(fw)2

d

 lip xfw(x) +

Wi 22.

i=1

Considering the above result, we complete the proof by inductively proving lip xfw(x) 

(

d i=1

Wi 2)

d i=1

i-1 j=1

Wj

2. For d = 1, xfw(x) is constant and hence the result holds.

Assume the statement holds for d = k. Due to the chain rule,

xfw(k+1)(x) = xWk+1 fw(k)(x) = xfw(k)(x)  fw(k)(x) WkT+1 and therefore for any x and v

xfw(k+1)(x + v) - xfw(k+1)(x) 2  xfw(k)(x + v)  fw(k)(x + v) WkT+1 - xfw(k)(x + v)  fw(k)(x + v) WkT+1 2  Wk+1 2 xfw(k)(x + v)  fw(k)(x + v) - xfw(k)(x)  fw(k)(x) 2  Wk+1 2 xfw(k)(x + v)  fw(k)(x + v) - xfw(k)(x)  fw(k)(x + v) 2
+ Wk+1 2 xfw(k)(x)  fw(k)(x + v) - xfw(k)(x)  fw(k)(x) 2

 Wk+1 2 xfw(k)(x + v) - xfw(k)(x) 2 + xfw(k)(x) 2  fw(k)(x + v) -  fw(k)(x) 2

 Wk+1 2 lip xfw(k)(x) + lip fw(k)(x) v 2

20

Under review as a conference paper at ICLR 2019

k+1

k+1 i-1

 ( Wi 2)

Wj 2 v 2,

i=1 i=1 j=1

which shows the statement holds for d = k + 1 and therefore completes the proof via induction.

In order to prove Theorem 3, we note that for any norm-bounded x 2  B and perturbation vector

u such that i,

Ui

2



1 d

Wi

2 we have

fw+u(x + wpg+mu,r(x)) - fw(x + wpgm,r(x)) 2

 fw+u(x + wpg+mu,r(x)) - fw(x + wpg+mu,r(x)) 2

+ fw(x + wpg+mu,r(x)) - fw(x + wpgm,r(x)) 2

d

 e(B + )

Wi 2

i=1

d i=1

Ui Wi

2 2

+

e2

(2/)

1 - (2/)r 1 - (2/)

lip( lip(

 fw)r  fw)

dd

×

Wi

2 2

i=1 i=1

Ui 2 + (B + Wi 2

i
)
j=1

Wj

2

i j=1

Uj 2 Wj 2

d

 e(B + )

Wi 2

i=1

d i=1

Ui 2 + e2(2/) 1 - (2/)rlip(  fw)r

Wi 2

1 - (2/)lip(  fw)

dd

×

Wi

2 2

i=1 i=1

Ui 2 + (B + Wi 2

i
)
j=1

Wj

2

i j=1

Uj 2 Wj 2

The last inequality holds since as shown in Lemma 7 lip x (fw(x), y)  lip(  fw) :=

d i=1

Wi 2

d i=1

i j=1

Wj

2. Here the upper-bound lip(

 fw) for w changes by a factor

at most e2/r for w such that

Wi

2-

Wi

2



1 rd

Wi

2. Therefore, given w if similar to the

proof for Theorem 2 we choose a zero-mean multivariate Gaussian distribution Q for u with the ith

layer ui's standard deviation to be i =

Wi w~

2

where



= .

8d(B + )

2h log(4hd)e4(/) 1-e2(2/)rlip( fw)r (
1-e2/r (2/)lip( fw)

d i=1

Wi 2) 1 +

d i=1

i j=1

Wj

2

Then for any w satisfying

Wi

2-

Wi

2



1 rd

Wi

2, applying union bound shows that the

assumption of Lemma 1 Pru maxxX

fw+u(x + wpg+mu,r(x)) - fw(x + wpgm,r(x))



 4



1 2

holds for Q, and further we have

d
KL(Pw+u Q) 
i=1

Wi

2 F

2i2

= O d2(B + )2h log(hd)×

d i=1

Wi

2 1-e2(2/)r lip( fw)r 2 1-e2/r (2/)lip( fw)

1

+

 

2

d i=1

Wi

2

 O d2(B + )2h log(hd)×

d i=1

i j=1

Wj 2 2

d

Wi

2 F

i=1

Wi

2 2

d i=1

Wi

2 1-(2/)r lip( fw)r 2 1-(2/)lip( fw)

1

+

 

2

d i=1

Wi

2

d i=1

i j=1

Wj 2 2

d

Wi

2 F

i=1

Wi

2 2

Applying the above bound to Lemma 1 shows that for any  > 0 the following holds with probability

1 -  for any w where

Wi 2 -

Wi 2



1 rd

Wi 2:

Lp0gm(fw)  Lpgm(fw) + O

(B +

)2d2h

log(dh)

p,gm,r, (fw )

+

log

n 

2n

,

21

Under review as a conference paper at ICLR 2019

where we consider p,gm,r,(fw) as the following expression

d i=1

Wi 2

1 - (2/)rlip( 1 + (/)
1 - (2/)lip(

 fw)r  fw)

d i=1

Wi 2

di i=1 j=1

Wj

2

2d i=1

Wi Wi

2

F 2

.

2

Using a similar argument to our proof of Theorem 2, we can properly cover the spectral norms for

each Wi with 2rd log M points, such that for any feasible Wi 2 value, satisfying the assumptions,

we have value ai in our cover where

Wi

2 - ai



1 rd

ai.

Therefore,

we

can

cover

all

feasible

combinations of spectral norms with (2rd log M )ddn1/2d, which combined with the above discussion

completes the proof.

C.4 PROOF OF THEOREM 4

We first show the following lemma providing a perturbation bound for WRM attacks.

Lemma 8. Consider a d-layer neural net fw satisfying the assumptions of Lemma 3. Then, for any

weight perturbation u such that

Ui

2



1 d

Wi 2 we have

wwr+mu(x) - wwrm(x)

2





-

e2 lip(

 fw)

dd
× Wi 2
i=1 i=1

Ui 2 + Wi 2

x 2+

d j=1

Wj

2



i
Wj 2
j=1

i j=1

Uj 2 Wj 2

.

In the above inequality, lip(  fw) denotes the Lipschitz constant of x (fw(x), y).

Proof. First of all note that for any x we have

wwrm(x) 2  (

d i=1

Wi 2)/, because we

assume lip(  fw) <  implying WRM's optimization is a convex optimization problem with the

global

solution

wwrm(x)

satisfying

wwrm(x)

=

1 



 fw(x + wwrm(x)) which is norm-bounded by

lip( fw) 



(

d i=1

Wi

2)/. Moreover, applying Lemma 3 we have

wwr+mu(x) - wwrm(x) 2

=

1 

x

(fw+u(x

+

wwr+mu(x))

-

1 

x

(fw(x + wwrm(x))

2



1 

x

(fw+u(x

+

wwr+mu(x))

-

1 

x

(fw(x + wwr+mu(x))

2

+

1  x

(fw(x + wwr+mu(x)) -

1  x

(fw(x + wwrm(x))

2

 e2 

d

Wi 2

d

i=1 i=1

Ui 2 + Wi 2

x 2+

d j=1

Wj

2



i j=1

Wj 2

i j=1

Uj 2 Wj 2

lip( +

 fw)



wwr+mu(x) - wwrm(x)

2

which shows the following inequality and hence completes the proof:

1 - lip(  fw) 

 e2 

d

Wi 2

d

i=1 i=1

wwr+mu(x) - wwrm(x) 2

Ui 2 + Wi 2

x 2+

d j=1

Wj

2



i
Wj 2
j=1

i j=1

Uj 2 Wj 2

.

Combining the above lemma with Lemma 2, for any norm-bounded x 2  B and perturbation

vector u where

Ui

2

1 d

Wi

2,

fw+u(x + wwr+mu(x)) - fw(x + wwrm(x)) 2

22

Under review as a conference paper at ICLR 2019

 fw+u(x + wwr+mu(x)) - fw(x + wwr+mu(x)) 2 + fw(x + wwr+mu(x)) - fw(x + wwrm(x)) 2

 e(B +

d j=1

Wj

2)

d



Wi 2

i=1

d i=1

Ui 2 + Wi 2

d i=1

Wi

2

× e2

d

 - lip(  fw) i=1

Ui 2 + B + Wi 2

d j=1

Wj

2



i j=1

Wj 2

i j=1

Uj 2 Wj 2

 e(B +

d j=1

Wj

2
)

d



Wi 2

i=1

d i=1

Ui 2 + Wi 2

d i=1

Wi

2

× e2

d

 - lip(  fw) i=1

Ui 2 + B + Wi 2

d j=1

Wj

2



i j=1

Wj 2

i j=1

Uj 2 Wj 2

.

Similar to the proofs of Theorems 2,3, given w we choose a zero-mean multivariate Gaussian

distribution Q with diagonal covariance matrix for random perturbation u, with the ith layer ui's

standard deviation parameter i =

Wi w~

2

where

= 8e5d 2h log(4hd)(B +

d j=1

Wj

2/)



d i=1

Wi 2

1+ 1
-lip( fw)

d i=1

i j=1

Wj

2

.

Using a union bound suggests the assumption of Lemma 1 Pru maxxX fw+u(x + wwr+mu(x)) -

fw(x + wwrm(x))





 4



1 2

holds

for Q.

Then,

for

any w

satisfying

Wi 2 - Wi 2 

1 4d/

Wi

2 we have lip(

 fw)  (e/4)2lip(

 fw)



(e /4 )2 (1

- )



1- 1- /2





(1

-

 2

)

which implies the guard-band  for w applies to w after being modified by a factor 2. Hence,

d
KL(Pw+u Q) 
i=1

Wi

2 F

2i2

d

 O d2 B +

Wj 2/ 2h log(hd)

j=1

×

d i=1

Wi

2 2

1+ 1
-lip( fw)

d i=1

2

d

 O d2 B +

Wj 2/ 2h log(hd)

j=1

×

d i=1

Wi

2 2

1+ 1
-lip( fw)

d i=1

2

i j=1

Wj 2 2

d

Wi

2 F

i=1

Wi

2 2

i j=1

Wj 2 2

d

Wi

2 F

i=1

Wi

2 2

Using this bound in Lemma 1 implies that for any  > 0 the following bound will hold with

probability 1 -  for any w where

Wi 2 -

Wi 2



1 4d/

Wi 2:

Lw0 rm(fw)  Lwrm(fw) + O

B+

d i=1

Wi

2/

2d2h log(dh) wrm(fw) + d log

n 

2n

.

where we define wrm(fw) to be

d

1d

di

i=1

Wi

2 1 +  - lip(

(  fw) i=1

Wi 2)
i=1 j=1

Wj

2

d 2
i=1

Wi Wi

2

F 2

.

2

Using a similar argument to our proofs of Theorems 2 and 3, we can cover the possible spectral norms for each Wi with O((8d/ ) log M ) points, such that for any feasible Wi 2 value satisfying

23

Under review as a conference paper at ICLR 2019

the theorem's assumptions, we have value ai in our cover where

Wi

2 - ai



1 4d/

ai.

Therefore,

we can cover all feasible combinations of spectral norms with O(((8d/ ) log M )ddn1/2d), which

combined with the above discussion finishes the proof.

24

