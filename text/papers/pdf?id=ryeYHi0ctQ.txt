Under review as a conference paper at ICLR 2019
DPSNET: END-TO-END DEEP PLANE SWEEP STEREO
Anonymous authors Paper under double-blind review
ABSTRACT
Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.
1 INTRODUCTION
Various image understanding tasks, such as semantic segmentation Couprie et al. (2013) and human pose/action recognition Shotton et al. (2011); Wang et al. (2016), have been shown to benefit from 3D scene information. A common approach to reconstructing 3D geometry is by multiview stereo, which infers depth based on point correspondences among a set of unstructured images Hartley & Zisserman (2003); Scho¨nberger et al. (2016). To solve for these correspondences, conventional techniques employ photometric consistency constraints on local image patches. Such photo-consistency constraints, though effective in many instances, can be unreliable in scenes containing textureless and reflective regions.
Recently, convolutional neural networks (CNNs) have demonstrated some capacity to address this issue by leveraging semantic information inferred from the scene. The most promising of these methods employ a traditional stereo matching pipeline, which involves computation of matching cost volumes, cost aggregation, and disparity estimation Flynn et al. (2016); Kendall et al. (2017); Huang et al. (2018); Chang & Chen (2018). Some are designed for binocular stereo Ummenhofer et al. (2017); Kendall et al. (2017); Chang & Chen (2018) and cannot readily be extended to multiple views. The CNN-based techniques for multiview processing Flynn et al. (2016); Huang et al. (2018) both follow the plane-sweep approach, but require plane-sweep volumes as input to their networks. As a result, they are not end-to-end systems that can be trained from input images to disparity maps.
In this paper, we present Deep Plane Sweep Network (DPSNet), an end-to-end CNN framework for robust multiview stereo. In contrast to previous methods that employ the plane-sweep approach Huang et al. (2018); Flynn et al. (2016), DPSNet fully models the plane-sweep process, including construction of plane-sweep cost volumes, within the network. This is made possible through the use of a differentiable warping module inspired by spatial transformer networks Jaderberg et al. (2015) to build the cost volumes. With the proposed network, plane-sweep stereo can be learned in an end-to-end fashion. Additionally, we introduce a cost aggregation module based on local cost-volume filtering Rhemann et al. (2011) for context-aware refinement of each cost slice. Through this cost-volume regularization, the effects of unreliable matches scattered within the cost volume are reduced considerably.
1

Under review as a conference paper at ICLR 2019

DPSNet

(b) Reference (c) GT Depth (d) DeMoN

(a) Input unstructured image sets ( 2), output depth map. (e) COLMAP (f) DeepMVS (g) Ours
Figure 1: Results of DPSNet with comparisons to state-of-the-art methods.
With this end-to-end network for plane-sweep stereo and the proposed cost aggregation, we obtain state-of-the-art results over several standard datasets. Ablation studies indicate that each of these technical contributions leads to appreciable improvements in reconstruction accuracy.
2 RELATED WORK
CNN-based depth estimation has been studied for stereo matching, depth from single images, and multiview stereo. Recent work in these areas are briefly reviewed in the following.
Stereo matching Methods for stereo matching address the particular case of depth estimation where the input is a pair of rectified images captured by a stereo rig. Various network structures have been introduced for this problem. Zbontar & LeCun (2016) present a Siamese network structure to compute matching costs based on the similarity of two image patches. The estimated initial depth is then refined by traditional cost aggregation and refinement as post-processing. Mayer et al. (2016) directly stack several convolution and deconvolution layers upon the matching costs and train the network to minimize the distance between the estimates and ground truth. Liang et al. (2018) propose a CNN that estimates initial disparity and then refines it using both prior and posterior feature consistency in an end-to-end manner. Kendall et al. (2017) leverage geometric knowledge in building a cost volume from deep feature representations. It also enables learning of contextual information in a 3D volume and regresses disparity in an end-to-end manner. Chang & Chen (2018) introduce a pyramid pooling module for incorporating global contextual information into image features and a stacked hourglass 3D CNN to extend the regional support of contextual information.
Depth from single images Similar to these stereo matching approaches, single-image methods extract CNN features to infer scene depths and perform refinements to increase depth accuracy. The first of these methods was introduced by Eigen et al. (2014), which demonstrated that CNN features could be utilized for depth inference. Later, Liu et al. (2016) combined a superpixel-based conditional random field (CRF) to a CNN to improve the quality of depth estimates from single images. To facilitate training, recent studies Zhou et al. (2017); Wang et al. (2018); Mahjourian et al. (2018); Yin & Shi (2018) present an end-to-end learning pipeline that utilizes the task of view synthesis as supervision for single-view depth and camera pose estimation. These systems consist of a depth network and a pose estimation network which simultaneously train on sequential images with a loss computed from images warped to nearby views using the estimated depth. View synthesis has similarly been used as supervision by warping between stereo image pairs Garg et al. (2016); Godard et al. (2017). In contrast to these single-image works which employ warping as a component of view synthesis for self-supervised learning, our network computes warps with respect to multiple depth planes to produce plane-sweep cost volumes both for training and at test time. The cost volumes undergo further processing in the form of cost aggregation and regularization to improve the robustness of depth estimates.
Multi-view stereo In multi-view stereo, depth is inferred from multiple input images acquired from arbitrary viewpoints. To solve this problem, some methods recover camera motion between the unstructured images but are designed to handle only two views Ummenhofer et al. (2017); Li et al. (2018). The DeMoN system Ummenhofer et al. (2017) consists of encoder-decoder networks for optical flow, depth/motion estimation, and depth refinement. By alternating between estimating optical flow and depth/motion, the network is forced to use both images in estimating depth, rather than resorting to single-image inference. Li et al. (2018) perform monocular visual odometry in an
2

Under review as a conference paper at ICLR 2019

unsupervised manner. In the training step, the use of stereo images with extrinsic parameters allows 3D depth estimation to be estimated with metric scale.
Among networks that can handle an arbitrary number of views, camera parameters are assumed to be known or estimated by conventional geometric methods. Ji et al. (2017) introduce an end-to-end learning framework based on a viewpoint-dependent voxel representation which implicitly encodes images and camera parameters. The voxel representation restricts the scene resolution that can be processed in practice due to limitations in GPU memory. Im et al. (2018) formulate a geometric relationship between optical flow and depth to refine the estimated scene geometry, but is designed for image sequences with a very small baseline, i.e., an image burst from a handheld camera. Huang et al. (2018) compute a set of plane-sweep volumes using calibrated pose data as input for the network, which then predicts a depth map. Its estimated depth map is refined using a conventional CRF. By contrast, our proposed DPSNet is developed to be trained end-to-end from input images to the depth map. Moreover, it leverages conventional multiview stereo concepts by incorporating context-aware cost aggregation.

3 APPROACH

Our Deep Plane Sweep Network (DPSNet) is inspired by traditional multiview stereo practices and consists of four parts: feature extraction, cost volume generation, cost aggregation and depth map regression. The overall framework is shown in figure 2.

3.1 MULTI-SCALE FEATURE EXTRACTION
We first pass a reference image and target images through seven convolutional layers (3 × 3 filters except for the first layer, which has a 7 × 7 filter) to encode them, and extract hierarchical contextual information from these images using a spatial pyramid pooling (SPP) module He et al. (2014) with four fixed-size average pooling blocks (16 × 16, 8 × 8, 4 × 4, 2 × 2). The multi-scale features extracted by SPP have been shown to be effective in many visual perception tasks such as visual recognition He et al. (2014), scene parsing Zhao et al. (2017) and stereo matching Huang et al. (2018). After upsampling the hierarchical contextual information to the same size as the original feature map, we concatenate all the feature maps and pass them through 2D convolutional layers. This process yields 32-channel feature representations for all the input images, which are next used in building cost volumes.

3.2 COST VOLUME GENERATION

We propose to generate cost volumes for the multiview images by adopting traditional plane sweep stereo Collins (1996). The basic idea of plane sweep stereo is to back-project the image set onto successive virtual planes in the 3D space and measure photo-consistency among the warped images for each pixel. For this cost volume generation network, we first set the number of virtual planes perpendicular to the z-axis of the reference viewpoint [0, 0, 1] and uniformly sample them in the inverse-depth space as follows:

dl

=

(L

× dmin) , l

(l

=

1, .., L),

(1)

where L is the total number of depth labels and dmin is the minimum scene depth as specified by the user.

Then, we warp all the paired features Fi, (i = 1, .., N ) into the coordinates of the reference feature (of size W idth × Height × CHannel) using pre-computed intrinsics K and extrinsic parameters
[R|t] of the camera:

F~il(u) = Fi(u~l),

u~l  K[R|t]

(K-1u)dl 1

,

(2)

where u, u~l are the homogeneous coordinates of a pixel in the reference view and the projected coordinates onto the paired view, respectively. F~il(u) denotes the warped features of the paired image through the lth virtual plane. Unlike the traditional plane sweeping method which utilizes a

3

Under review as a conference paper at ICLR 2019

Reference Image Feature

 Pair Image

Context Network (2D Convolution)

Reference Image

Warping through  plane (sweep)

( + ) Cost
Volume

Initial

Residual



4W × 4H × 3 Input Image

1 



W × H × CH Feature Extraction (1/4 Downsampled)
* 2D CNN

W × H × 2CH Feature Concatenate : Volume Generation * No learnable para.

 Cost Volume

W × H × 2CH × L Cost Volume Generation * 3D CNN

W×H×L Averaging Cost
(  {, . . }) * No learnable para.

W×H×L Cost Aggregation
+ Upsampling * 2D CNN

4W×4H×1 Depth regression
(SoftMax) * No learnable para.

Figure 2: Overview of the DPSNet pipeline.

+ Aggregated Volume

distance metric, we use a concatenation of features in learning a representation and carry this through to the cost volume as proposed in Kendall et al. (2017). We obtain a 4D volume (W ×H ×2CH ×L) by concatenating the reference image features and the warped image features for all of the depth labels. In Eq. (2), we assume that all images are captured by the same camera, but it can be directly extended to images with different intrinsics. For the warping process, we use a spatial transformer network Jaderberg et al. (2015) for all hypothesis planes, which does not require any learnable parameters. In Table 2, we find that concatenating features improves performance over the absolute difference of the features.
Given the 4D volume1, our DPSNet learns a cost volume generation of size W × H × L by using a series of 3D convolutions on the concatenated features. All of the convolutional layers consist of 3 × 3 × 3 filters and residual blocks. In the training step, we only use one paired image (while the other is the reference image) to obtain the cost volume. In the testing step, we can use any number of paired images (N  1) by averaging all of the cost volumes.

3.3 COST AGGREGATION
The key idea of cost aggregation Rhemann et al. (2011) is to regularize the noisy cost volume through edge-preserving filtering He et al. (2013) within a support window. Inspired by traditional cost volume filtering, we introduce a context-aware cost aggregation method in our end-to-end learning process. The context network takes each slice of the cost volume and the reference image features extracted from the previous step, and then outputs the refined cost slice. We run the same process for all the cost slices. The final cost volume is then obtained by adding the initial and residual volumes as shown in figure 2 (top).
Here, we use dilated convolutions in the context network for cost aggregation to better exploit contextual information Chen et al. (2018); Yu & Koltun (2016). The context network consists of seven convolutional layers with 3 × 3 filters, where each layer has a different receptive field (1, 2, 4, 8, 16, 1, and 1). We jointly learn all the parameters, including those of the context network. All cost slices are processed with shared weights of the context network. Then, we upsample the cost volume, whose size is equal to the feature size, to the original size of the images via bilinear interpolation. We find that this leads to moderate performance improvement as shown in Table 2.

3.4 DEPTH REGRESSION
We regress continuous depth values using the method proposed in Kendall et al. (2017). The probability of each label l is calculated from the predicted cost cl via the softmax operation (·). The predicted label ^l is computed as the sum of each label l weighted by its probability. With the predicted label, the depth is calculated from the number of labels L and minimum scene depth dmin as
1We implement a 5D volume that includes a batch dimension.

4

Under review as a conference paper at ICLR 2019

ETH3D Scenes11 RGBD SUN3D MVS

Data -sets

Method
COLMAP DeMoN DeepMVS
Ours COLMAP DeMoN DeepMVS
Ours COLMAP DeMoN DeepMVS
Ours COLMAP DeMoN DeepMVS
Ours COLMAP DeMoN DeepMVS
Ours

Abs Rel
0.3841 0.3105 0.2305 0.0689
0.6232 0.2137 0.2816 0.1470
0.5389 0.1569 0.2938 0.1538
0.6249 0.5560 0.2100 0.0558
0.3228 0.1911 0.1783 0.0991

Abs diff
0.8430 1.3291 0.6628 0.2290
1.3267 2.1477 0.6040 0.3234
0.9398 1.3525 0.6207 0.5235
2.2409 1.9877 0.5967 0.2430
0.6148 0.7259 0.4315 0.3649

Error metric

Sq Rel RMSE

1.257 19.970 0.6151 0.0930

1.4795 2.6065 1.1488 0.6725

3.2359 1.1202 0.4350 0.1071

2.3162 2.4212 0.9436 0.4269

1.7608 0.5238 0.4297 0.2149

1.5051 1.7798 0.8684 0.7226

3.7148 3.6575 3.4020 2.6034

0.3727 0.8909 0.1435 0.7136

36.7047 2.3700

0.3649 1.0588

0.9733 1.0210 0.2041 0.7025

RMSE log
0.5001 0.2469 0.3019 0.1542
0.6612 0.2060 0.3633 0.1906
0.7151 0.2018 0.3506 0.2263
0.8680 0.3909 0.2699 0.1396
0.2401 0.2451 0.1842

 < 1.25
0.4819 0.6411 0.6737 0.8868
0.3266 0.7332 0.5622 0.7892
0.2749 0.8011 0.5493 0.7842
0.3897 0.4963 0.6881 0.9502
0.8654 0.7333 0.8576 0.8625

Accuracy metric  < 1.252  < 1.253

0.6633

0.8401

0.9017

0.9667

0.8867

0.9414

0.9441

0.9682

0.5541

0.7180

0.9219

0.9626

0.7388 0.9317

0.8951 0.9672

0.5001

0.7241

0.9056

0.9621

0.8052 0.8959

0.9217 0.9402

0.5674 0.7258 0.8940 0.9726

0.6716 0.8263 0.9687 0.9804

0.9032

0.9268

0.8978

0.9512

0.9114

0.9424

0.9376

0.9629

Table 1: Comparison results. Multi-view stereo methods: COLMAP, DeMoN, DeepMVS, and Ours. Datasets: MVS, SUN3D, RGBD, Scenes11 and ETH3D.

follows:

d~ =

L

× dmin ~l

,

L
~l = l × (cl).
l=1

We set L and dmin to 64 and 0.5, respectively.

(3)

3.5 TRAINING LOSS

Let  be the set of all the learnable parameters in our network, which includes feature extraction,
cost volume generation and cost aggregation (plane sweep and depth regression have no learnable parameters). Let d^, d~ denote the predicted depth from the initial and refined cost volumes, respectively, and let dgt be the corresponding supervision signal. The training loss is then formulated
as

L() = |d^x - dgxt|H + |d~x - dgxt|H,
x

(4)

where | · |H denotes the Huber norm, referred to as SmoothL1Loss in PyTorch. The weight value  for depth from the initial cost volume is set to 0.7.

4 EXPERIMENTS
4.1 IMPLEMENTATION DETAILS
In the training procedure, we use image sequences, ground-truth depth maps for reference images, and the provided camera poses from public datasets, namely SUN3D, RGBD, and Scenes112. We train our model from scratch for 1200K iterations in total. All models were trained end-to-end with the ADAM optimizer (1 = 0.9, 2 = 0.999). We use a batch size of 16 and set the learning rate to 2e-4 for all iterations. The training is performed with a customized version of PyTorch on an Nvidia 1080 GPU, which usually takes four days.
4.2 COMPARISON WITH STATE-OF-THE-ART METHODS
In our evaluations, we use common quantitative measures of depth quality: absolute relative error (Abs Rel), absolute difference error (Abs diff), square relative error (Sq Rel), root mean square error
2https://github.com/lmb-freiburg/demon

5

Under review as a conference paper at ICLR 2019
(a) Images (b) GT depth (c) DeMoN (d) COLMAP (e) DeepMVS (f) Ours Figure 3: Comparison of depth map results on MVS, SUN3D, RGBD and Scenes11 (top to bottom).
(a) Images (b) GT depth (c) DeMoN (d) COLMAP (e) DeepMVS (f) Ours Figure 4: Depth map results on four-view image sequences from the ETH3D dataset. and its log scale (RMSE and RMSE log) and inlier ratios ( < 1.25i where i  {1, 2, 3}). All are standard metrics used in a public benchmark suite3. For our comparisons, we choose state-of-the-art methods for traditional geometry-based multiview stereo (COLMAP) Scho¨nberger & Frahm (2016), depth from unstructured two-view stereo (DeMoN) Ummenhofer et al. (2017) and CNN-based multiview stereo (DeepMVS) Huang et al. (2018). We estimate the depth maps from two unstructured views using the test sets in MVS, SUN3D, RGBD and Scenes11, as done for DeMoN4. The results are reported in Table 1. Our DPSNet provides the best performance on nearly all of the measures. Of particular note, DPSNet accurately recovers scene depth in homogeneous regions as well as along object boundaries as exhibited in figure 3. DeMoN generally produces good depth estimates but often fails to reconstruct scene details such as the keyboard (third row) and fine structures (first, second and fourth rows). By contrast, DPSNet estimates accurate depth maps at those regions because the differential feature warping penalizes inaccurate reconstructions, playing a role similar to the left-right consistency check that has been used in stereo matching Garg et al. (2016). The first and third rows of figure 3 exhibit problems of COLMAP and DeepMVS in handling textureless regions. DPSNet instead produces accurate results, courtesy of the cost aggregation network. 3http://www.cvlibs.net/datasets/kitti/ 4We use the provided camera intrinsics and extrinsics.
6

Under review as a conference paper at ICLR 2019

1

0 1

0 1

0 1

0

(a) (b) (c) (d)

(e)

Figure 5: (a) Reference image & GT depth. (b) Regressed depth. (c), (d) Slice of volume along

a label (far & Close), and (e) along the green row in (a) (Column-Cost layer axis). The color bar

ranges from 0 to 1. Before (top) and after (bottom) cost volume aggregation.

Method
(a) Cost (Difference) (b) W/O Aggregation (c) Stacked hourglass (d) Uniform depth (e) Ours

Abs Rel
0.1067 0.1260 0.1012 0.1493 0.1010

Abs diff
0.3344 0.3955 0.3242 0.5078 0.3219

Error metric Sq Rel RMSE
0.1515 0.6679 0.2111 0.7866 0.1450 0.6544 0.3561 1.0110 0.1420 0.6432

RMSE log
0.1807 0.2178 0.1774 0.2409 0.1734

 < 1.25
0.8548 0.8252 0.8687 0.8138 0.8645

Accuracy metric  < 1.252  < 1.253

0.9359

0.9648

0.9144

0.9499

0.9404

0.9626

0.9067

0.9416

0.9407

0.9661

Table 2: Ablation Experiment. (a) With cost volume generated by absolute differences of features. (b) Without cost aggregation. (c) With cost aggregation by stacked hourglass. (d) With planes swept on uniform samples of the depth domain (whereas ours are uniformly sampled on the inverse depth domain). (e) With our complete DPSNet. Datasets: MVS, SUN3D, RGBD, Scenes11.

To examine multi-view performance, we also conducted comparisons on four-view image sequences from the ETH3D dataset. Since DeMoN is designed for two views, we select its best result from among all image pairs formed by the reference image and its neighboring images. Although DPSNet is not trained on the ETH3D dataset, it still shows robust performance on homogeneous regions and object boundaries. Several examples are exhibited in figure 4. By utilizing additional views, problems caused by viewpoint ambiguity can be alleviated, as explained in the following ablation studies.

4.3 ABLATION STUDY
An extensive ablation study was conducted to examine the effects of different components on DPSNet performance. We summarize the results in Table 2.
Cost Volume Generation In Table 2 (a) and (e), we compare the use of cost volumes generated using the traditional absolute difference Collins (1996) and using the concatenation of features from the reference image and warped image. The absolute difference is widely used for depth label selection via a winner-take-all strategy. However, we observe that feature concatenation provides better performance in our network than the absolute difference. A possible reason is that the CNN may learn to extract 3D scene information from the tensor of stacked features. The tensor is fed into the CNN to produce an effective feature for depth estimation, which is then passed through our cost aggregation network for the initial depth refinement.
Cost Aggregation For our cost aggregation sub-network, we compare DPSNet with and without it in Table 2 (e) and (b), respectively. It is shown that including the proposed cost aggregation leads to significant performance improvements. Examples of depth map refinement with the cost aggregation are displayed in figure 5.
Our cost aggregation is also compared to using a stacked hourglass to aggregate feature information along the depth dimension as well as the spatial dimensions as done recently for stereo match-

7

Under review as a conference paper at ICLR 2019

Winner Margin Curvature

Before Aggregation

0.7001

0.4614

After Aggregation

0.7136

0.4836

Table 3: Confidence measures on cost volumes. Datasets: MVS, SUN3D, RGBD, Scenes11.

0.15 0.06 0.13 0.05
0.04 0.11 0.03 0.09 0.02
0.01 0.07 0
2 3 4 5 6 7 8 9 10 11 12 13 14 15 abs_rel sq_rel

(a) Error metrics.

(b) Images (c) GTs (d) 2-view (e) 3-view

Figure 6: Depth map results w.r.t. the number of images.

(f) 5-view

ing Chang & Chen (2018). Although the stacked hourglass is shown in Table 2 (c) to enhance depth results, its improvement is smaller than ours, which uses reference image features to guide the aggregation. figure 5 (a), (b) show examples of the depth map results before and after our cost aggregation. It demonstrates that the cost aggregation sub-network outputs more accurate depth, especially on homogeneous regions.
For further analysis of cost aggregation, we display slices of 3D cost volumes after the softmax operation (in Eq. (3)) that span depth labels and the rows of the images. The cost slices in figure 5 (c), (d) show that our feature-guided cost aggregation regularizes noisy cost slices while preserving edges well. The cleaner cost profiles that ensue from the cost aggregation lead to clearer and edgepreserving depth regression results. As mentioned in a recent study Hu & Mordohai (2012), a cost profile that gives confident estimates should have a single, distinct minimum (or maximum), while an ambiguous profile has multiple local minima or multiple adjacent labels with similar costs, making it hard to exactly localize the global minimum. Based on two quantitative confidence measures Hu & Mordohai (2012) on cost volumes in Table 3, the proposed aggregation improves the reliability of the correct match corresponding to the minimum cost.
Depth Label Sampling In the plane sweep procedure, depth labels can be sampled in either the depth domain or the inverse depth domain, which provides denser sampling in areas closer to a camera. Table 2 (d) and (e) show that uniform depth label sampling in the inverse depth domain produces more accurate depth maps in general.
Number of Images We examine the performance of DPSNet with respect to the number of input images. As displayed in figure 6a, a greater number of images yields better results, since cost volume noise is reduced through averaging over more images, and more viewpoints help to provide features from areas unseen in other views. figure 6 shows that adding input views aids in distinguishing object boundaries. Note that the performance improvement plateaus when seven or more images are used.

5 DISCUSSION
We developed a multiview stereo network whose design is inspired by best practices of traditional non-learning-based techniques. The plane sweep algorithm is formulated as an end-to-end network via a differentiable construction of plane sweep cost volumes and by solving for depth as a multilabel classification problem. Moreover, we propose a context-aware cost aggregation method that leads to improved depth regression without any post-processing. With this incorporation of traditional multiview stereo schemes into a deep learning framework, state-of-the-art reconstruction results are achieved on a variety of datasets.
Directions exist for improving DPSNet. One is to integrate semantic instance segmentation into the cost aggregation, similar to the segment-based cost aggregation method of Mei et al. (2013). Another direction is to improve depth prediction by employing viewpoint selection in constructing cost volumes Gallup et al. (2008); Scho¨nberger et al. (2016), rather than by simply averaging the estimated cost volumes as currently done in DPSNet. Lastly, the proposed network requires pre-calibrated intrinsic and extrinsic parameters for reconstruction. Lifting this restriction by additionally estimating camera poses in an end-to-end learning framework is an important future challenge.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In CVPR, 2018.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Trans. on Patt. Anal. and Mach. Intel., 2018.
Robert T Collins. A space-sweep approach to true multi-image matching. In CVPR, 1996.
Camille Couprie, Cle´ment Farabet, Laurent Najman, and Yann Lecun. Indoor semantic segmentation using depth information. In ICLR, 2013.
David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In NIPS, 2014.
John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. Deepstereo: Learning to predict new views from the worlds imagery. In CVPR, 2016.
David Gallup, Jan-Michael Frahm, Philippos Mordohai, and Marc Pollefeys. Variable baseline/resolution stereo. In CVPR, 2008.
Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, and Ian Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. In ECCV, 2016.
Cle´ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estimation with left-right consistency. In CVPR, 2017.
Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.
Kaiming He, Jian Sun, and Xiaoou Tang. Guided image filtering. IEEE Trans. on Patt. Anal. and Mach. Intel., 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014.
Xiaoyan Hu and Philippos Mordohai. A quantitative evaluation of confidence measures for stereo vision. IEEE Trans. on Patt. Anal. and Mach. Intel., 2012.
Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In CVPR, 2018.
Sunghoon Im, Hae-Gon Jeon, and In So Kweon. Robust depth estimation from auto bracketed images. In CVPR, 2018.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In NIPS, 2015.
Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: an end-to-end 3d neural network for multiview stereopsis. In CVPR, 2017.
Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. End-to-end learning of geometry and context for deep stereo regression. In ICCV, 2017.
Ruihao Li, Sen Wang, Zhiqiang Long, and Dongbing Gu. Undeepvo: Monocular visual odometry through unsupervised deep learning. In ICRA, 2018.
Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Linbo Qiao, Wei Chen, Li Zhou, and Jianfeng Zhang. Learning deep correspondence through prior and posterior feature constancy. In CVPR, 2018.
Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single monocular images using deep convolutional neural fields. IEEE Trans. on Patt. Anal. and Mach. Intel., 2016.
9

Under review as a conference paper at ICLR 2019
Reza Mahjourian, Martin Wicke, and Anelia Angelova. Unsupervised learning of depth and egomotion from monocular video using 3d geometric constraints. In CVPR, 2018.
Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In CVPR, 2016.
Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, and Xiaopeng Zhang. Segment-tree based cost aggregation for stereo matching. In CVPR, 2013.
Christoph Rhemann, Asmaa Hosni, Michael Bleyer, Carsten Rother, and Margrit Gelautz. Fast cost-volume filtering for visual correspondence and beyond. In CVPR, 2011.
Johannes L Scho¨nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In ECCV, 2016.
Johannes Lutz Scho¨nberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016.
Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard Moore, Alex Kipman, and Andrew Blake. Real-time human pose recognition in parts from single depth images. In CVPR, 2011.
Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In CVPR, 2017.
Chaoyang Wang, Jose Miguel Buenaposada, Rui Zhu, and Simon Lucey. Learning depth from monocular videos using direct methods. In CVPR, 2018.
Pichao Wang, Wanqing Li, Zhimin Gao, Jing Zhang, Chang Tang, and Philip O Ogunbona. Action recognition from depth maps using deep convolutional neural networks. IEEE Trans. on Hum.Mac. Sys., 2016.
Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In CVPR, 2018.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. ICLR, 2016. Jure Zbontar and Yann LeCun. Stereo matching by training a convolutional neural network to
compare image patches. J. of Machine Learning Research, 2016. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing
network. In CVPR, 2017. Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth
and ego-motion from video. In CVPR, 2017.
10

