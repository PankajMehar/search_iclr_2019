Under review as a conference paper at ICLR 2019
RANDOM MASK: TOWARDS ROBUST CONVOLUTIONAL NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Robustness of neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. In this paper, we design a new CNN architecture that by itself has good robustness. We introduce a simple but powerful technique, Random Mask, to modify existing CNN structures. We show that CNNs with Random Mask achieve state-of-the-art performance against black-box adversarial attacks without applying any adversarial training. We next investigate the adversarial examples which "fool" a CNN with Random Mask. Surprisingly, we find that these adversarial examples often "fool" humans as well. This raises fundamental questions on how to define adversarial examples and robustness properly.
1 INTRODUCTION
Deep learning (LeCun et al., 2015), especially deep Convolutional Neural Network (CNN) (LeCun et al., 1998), has led to state-of-the-art results spanning many machine learning fields, such as image classification (He et al., 2016; Hu et al., 2017; Huang et al., 2017; Simonyan & Zisserman, 2014), object detection (Redmon et al., 2016; Girshick, 2015; Ren et al., 2015), image captioning (Vinyals et al., 2015; Xu et al., 2015) and speech recognition (Bengio et al., 2003; Hinton et al., 2012).
Despite the great success in numerous applications, recent studies have found that deep CNNs are vulnerable to some well-designed input samples named as Adversarial Examples (Szegedy et al., 2013) (Biggio et al., 2013). Take the task of image classification as an example, for almost every commonly used well-performed CNN, attackers are able to construct a small perturbation on an input image to cause the model to give an incorrect output label. Meanwhile, the perturbation is almost imperceptible to humans. Furthermore, these adversarial examples can easily transfer among different kinds of CNN architectures (Papernot et al., 2016b).
Such adversarial examples raise serious concerns on deep neural network models as robustness is crucial in many applications. Just as Goodfellow (2018) suggests, both robustness and traditional supervised learning seem fully aligned. Recently, there is a rapidly growing body of work on this topic. One important line of research is adversarial training (Szegedy et al., 2013) (Madry et al., 2017) (Goodfellow et al., 2015) (Huang et al., 2015). Although adversarial training gains some success, a major difficulty is that it tends to overfit to the method of adversarial example generation used at training time (Buckman et al., 2018). Xie et al. (2017) and Guo et al. (2017) propose defense methods by introducing randomness and applying transformations to the inputs respectively. Dhillon et al. (2018) introduces random drop during the evaluation of a neural network. However, Athalye et al. (2018) contends that such transformation and randomness only provide a kind of "obfuscated gradient" and can be attacked by taking expectation over transformation (EOT) to get a meaningful gradient. Papernot et al. (2016a) and Katz et al. (2017) consider the non-linear functions in the networks and try to achieve robustness by adjusting them. There are also detection-based defense
1

Under review as a conference paper at ICLR 2019
methods (Metzen et al., 2017) (Grosse et al., 2017) (Meng & Chen, 2017), which add a process of detecting whether an input is adversarial.
In this paper, different from most of the existing methods, we take another approach to tackle the adversarial example problem. In particular, we aim to design a new CNN architecture which by itself enjoys robustness, without appealing to techniques such as adversarial training. To this end, we introduce Random Mask as a new ingredient of CNN. To be specific, we randomly select a set of neurons and remove them from the network before training. Then the architecture of the network is fixed during the training and testing process. Note that Random Mask is different from dropout where neurons are randomly masked in each step during training, and it can be applied very easily to common CNN structures such as ResNet with only a few changes of code. We find that applying Random Mask to the shallow layers of the network is crucial for robustness. CNNs with properly designed Random Mask are far more robust than standard CNNs. In fact, our experimental results demonstrate that CNNs with Random Mask achieve state-of-the-art results against black-box attacks even when comparing with defense methods using adversarial training. Furthermore, CNNs with Random Mask maintain a high accuracy on normal test data, while low test accuracy is often regarded as a major weakness in many methods designed for achieving robustness.
We next take a closer look at the adversarial examples generated particularly against CNNs with Random Mask. We investigate the adversarial examples that can "fool" our proposed architecture, i.e., the examples that are perturbed version of the original image, but are classified to a different label by the network. Surprisingly, we find that the adversarial examples which can "fool" a CNN with Random Mask often "fool" humans as well. It is difficult for humans to correctly classify these adversarial example, and in many cases humans make the same "incorrect" prediction as our network. Figure 1 shows a few adversarial examples generated by PGD (Basic iterative method) (Kurakin et al., 2016) with respect to a CNN with Random Mask as well as the labels the network outputs. (Please also see Figure 8 in Appendix F.1 for the original images and labels from CIFAR-10.) They are different from the typical adversarial examples generated against commonly used CNNs, which usually look like noisy versions of the original images and are easy to be correctly classified by humans.
These observations raise important questions: 1) How should we define adversarial examples? 2) How should we define robustness? Currently, an adversarial example is usually defined as a perturbed datum which lies in the neighborhood of the original data but has a different classification output by the network; and the robustness of a method is measured according to the proportion of these adversarial examples. However, if an adversarial example can also fool humans, it is more appropriate to say that the example does change the semantics of the datum to a certain extent. After all, why two images close to each other in terms of some distance (e.g., ) must belong to the same class? How close should they be so that they belong to the same class? Without complete answers to these questions, one should be very careful when measuring the robustness of a model merely according to currently-defined adversarial examples. Robustness is a subtle issue. We argue that one needs to rethink the robustness and adversarial examples from the definitions.
Dog Bird Frog Dog Auto- Ship Dog Ship Bird Frog Bird mobile
Figure 1: Adversarial examples (generated by PGD) that can "fool" a CNN with Random Mask. The labels here are the outputs of the network being "fooled". The original images from CIFAR-10 and more examples can be found in Figure 8 in Appendix F.1.
2

Under review as a conference paper at ICLR 2019
Our main contributions are summarized as follows:
· We develop a very simple but effective method, Random Mask. We show that combining with Random Mask, existing CNNs can be significantly more robust while maintaining high generalization performance. In fact, CNNs equipped with Random Mask achieve state-of-art performance against black-box attacks, even when comparing with methods using adversarial training (See Table 1).
· We investigate the adversarial examples generated against CNNs with Random Mask. We find that adversarial examples that can "fool" a CNN with Random Mask often fool humans as well. This observation requires us to rethink what are the right definitions of adversarial examples and robustness.
2 RANDOM MASK
We propose Random Mask, a method to modify existing CNN structures. It randomly selects a set of neurons and removes them from the network before training. Then the architecture of the network is fixed during the training and testing process. To apply Random Mask on a selected layer Layer(j), suppose the input is Xj and the output is convj(Xj)  Rmj×nj×cj . We randomly generate a binary mask mask(j)  {0, 1}mj×nj×cj by sampling uniformly within each channel. The drop rate of the sampling process is called the ratio (or drop ratio) of Random Mask. Then we mask the neurons in position (x, y, c) of the output of Layer(j) if the (x, y, c) element of mask(j) is zero. More specifically, after Random Mask, we will not compute these masked neurons and make the next layer regard these neurons as having value zero during computation. A simple visualization of Random Mask is shown in Figure 2. The Random Mask in fact decreases the computation cost in each epoch since there are fewer effective connections.
Figure 2: Random Mask
In the standard setting, convolutional filter will be applied uniformly to every position of the feature map of the former layer. The success of this implementation is due to the reasonable assumption that if one feature is useful to be computed at some spatial position (x, y), then it should also be useful to be computed at a different position (x , y ). Thus the original structure is powerful for feature extraction. Moreover, this structure leads to parameter sharing which makes the training process more efficient. However, the uniform application of filter also prevents the CNN from noticing the distribution of features. In other words, the network only focuses on the existence of a kind of feature but pays little attention to how this kind of feature distributes on the whole feature map (of the former layer). Yet the pattern of feature distribution is important for humans to recognize and classify a photo, since empirically people would rely on some structured feature to perform classification. With Random Mask, each filter may only extract features from partial positions. More specifically, for one filter, only features which distribute consistently with the mask pattern can be extracted. Hence filters in a network with Random Mask may capture more information on the organizations of local features. Just think of a toy example: when Random Mask for a filter masks all the neurons but one row in the channel, then if a kind of feature usually distributes in a column, it can not have strong response because the filter can only capture a small portion of the feature.
3

Under review as a conference paper at ICLR 2019
We do a straightforward experiment to verify our intuition. We sample some images from ImageNet which can be correctly classified with high probability by both CNNs with and without Random Mask. We then randomly shuffle the images by patches, and compare the accuracy of classifying the shuffled images (See Appendix A). We find out that the accuracy of the CNN with Random Mask is consistently lower than that of normal CNN. This result shows that CNNs without Random Mask cares more on whether a feature exists while CNNs with Random Mask will limit the feature to be extracted.
While the features that can be learned by each masked filter is limited, the randomness helps us get plenty of diversified patterns. Our experiments show that they are enough for learning features. In other words, CNNs will maintain a high test accuracy after applying Random Mask. Besides, adding convolutional filters may help our CNN with Random Mask to increase test accuracy (See Section 3.3). Furthermore, our structure is naturally compatible to ensemble methods, and randomness makes ensemble more powerful (See Section 3.3).
However, it might not be appropriate to apply Random Mask to deep layers. The distribution of features is meaningful only when the location in feature map is highly related to the location in the original input image, and the receptive field of each neuron of deep layers is too large. In Section 3.3, there are empirical results which support our intuition.
3 EXPERIMENTS
In this section, we provide extensive experimental analyses on the performance and properties of Random Mask network structure. We first test the robustness of Random Mask (See Section 3.1). Then we take a closer look at the adversarial examples that can "fool" our proposed architecture (See Section 3.2). After that we explore properties of Random Mask, including where to and how to apply Random Mask, by a series of comparative experiments (See Section 3.3). Some settings used in our experiments are listed below:
Network Structure. We apply Random Mask on the target network, ResNet-18 (He et al., 2016). The 5-block structure of ResNet-18 is shown in the Appendix C. The blocks are labeled 0, 1, 2, 3, 4 and the 0th block is the first convolution layer. We divide these five blocks into two parts - the relative shallow ones (the 0th, 1st, 2nd blocks) and the deep ones (the 3rd, 4th blocks). For simplicity, we would like to regard each of these two parts as a whole in this section to avoid being trapped by details. We use "-Shallow" and "-Deep" to denote that we apply Random Mask with drop ratio  to the shallow blocks and to the deep blocks in ResNet-18 respectively.
Attack Framework. The accuracy under black-box attack serves as a common criterion of robustness. We will use it when selecting model parameters and comparing Random Mask to other similar structures. To be more specific, by using FGSM (Goodfellow et al., 2015), PGD (Kurakin et al., 2016) with l norm and CW attack (Carlini & Wagner, 2016) (See Appendix B for more details on these attack approaches), we generate adversarial examples against different neural networks. The performances on adversarial examples generated against different networks are quite consistent. For brevity, we only show the defense performance against part of the adversarial examples generated by using DenseNet-121 (Huang et al., 2017) on dataset CIFAR-10 in this section, and leave more experimental results obtained by using other adversarial examples in the Appendix G. We use FGSM16, PGD16, PGD32, CW40 to denote attack method FGSM with step size
= 16, PGD with perturbation scale  = 16 and step number 20, PGD with perturbation scale  = 32 and step number 40, CW attack with confidence  = 40 respectively. The step sizes of both PGD methods are selected to be = 1. We would like to point out that these attacks are really powerful that a normal network cannot resist these attacks.
4

Under review as a conference paper at ICLR 2019

3.1 ROBUSTNESS VIA RANDOM MASK
Random Mask is not specially designed for adversarial defense, but as Random Mask introduces information that is essential for classifying correctly, it also brings robustness. As mentioned in Section 2, normal CNN structures may allow adversary to inject features imperceptible to humans into images that can be recognized by CNN. Yet Random Mask limits the process of feature extraction, so noisy features are less likely to be preserved.

3.1.1 ROBUSTNESS TO BLACK-BOX ATTACK
The results of our experiments show the strengths of applying Random Mask to adversarial defense. In fact, Random Mask can help existing CNNs reach state-of-the-art in black-box defense (See Table 1). In Section 3.3, we will provide more experimental results to show that this asymmetric structure performs better than normal convolution and enhances robustness.

Model
Normal ResNet-18 Vanilla (Madry) Random Mask

FGSM
26.99% 85.60% 86.31%

PGD
7.56% 86.00% 90.30%

Test Accuracy
95.33% 87.30% 90.08%

Table 1: Performance of black-box defense under the setting of Madry et al. (2017) (See Appendix G.1 for the complete setting). We use model under adversarial training in Madry et al. (2017) as a vanilla model. It is regarded as a state-of-art adversarial defense method.Our model is only trained on clean data. The ratio of Random Mask here is selected to balance the performance of robustness and generalization. See Figure 7 in Appendix E for complete results on the performance of Random Mask with different ratios.

3.1.2 ROBUSTNESS TO RANDOM NOISE
Beside the robustness against black-box attack, we also evaluate the robustness to random noise. Note that although traditional network structures are vulnerable to adversarial examples, they are still robust to the images perturbed with small Gaussian noises. To see whether our structure also enjoys such property, or even has better robustness in this sense, we feed input images with random Gaussian noises to the network with Random Mask. More specifically, in order to obtain noises of similar scale with the adversarial examples, we generate i.i.d. Gaussian random variables x  N (0, 2), where   {1, 2, 4, 8, 12, 16, 20, 24, 28, 32}, clip them to the range [-2, 2] and then add them to every pixel of the input image. The results of the experiments are shown in Figure 3.
3.2 ADVERSARIAL EXAMPLES?
We evaluate the performance of CNNs with Random Mask under white-box attack (See Appendix G.2). With neither obfuscated gradient nor gradient masking, Random Mask can still improve defense performance under various kinds of white-box attack. Also, by checking adversarial images that are misclassified by our network, we find most of them have vague edges and can hardly be recognized by humans. This result coincides with the theoretical analysis in Shafahi et al. (2018) Fawzi et al. (2018) that real adversarial examples may be inevitable in some way. See Appendix F.2 for a randomly selected set of them. In contrast, adversarial examples generated against normal CNNs are more like simply adding some non-sense noise which can be ignored by human. This phenomenon also demonstrates that Random Mask really helps networks to catch more information related to real human perception. Moreover, just as Figure 1 shows, with the help of Random Mask, we are able to find small perturbations that can actually change the semantic meaning of

5

Under review as a conference paper at ICLR 2019
Figure 3: Input images with random Gaussian noises, Random Mask versus normal network. The network with Random Mask has far better robustness than the original network.
images for humans. So should we still call them "adversarial examples"? How can we get more reasonable definitions of adversarial examples and robustness? These questions seem severe due to our findings.
3.3 PROPERTIES OF RANDOM MASK We then show some properties of Random Mask including the appropriate positions to apply Random Mask, the benefit of breaking symmetry, the diversity introduced by randomness and the extensibility of Random Mask via structure adjustment and ensemble methods. We conduct a series of comparative experiments and we will continue to use black-box defense performance as a criterion of robustness. Masking Shallow Layers versus Masking Deep Layers. In the last paragraph of Section 2 , we give an intuition that deep layers in a network should not be masked. To verify this, we do extensive experiments on ResNet-18 with Random Mask applied to different parts. We apply Random Mask with different ratios on the shallow blocks and on the deep blocks respectively. Results in Table 2 accord closely with our intuition. Comparing the success rate of black-box attacks on the model with the same drop ratio but different parts being masked, we find that applying Random Mask to shallow layers enjoys significantly lower adversarial attack success rates. This verifies that shallow layers play a more important role in limiting feature extraction than the deep layers. Moreover, only applying Random Mask on shallow blocks can achieve better performance than applying Random Mask on both shallow and deep blocks, which also verifies our intuition that dropping elements with large receptive fields is not beneficial for the network. In addition, we would like to point out that ResNet-18 with Random Mask significantly outperforms the normal network in terms of robustness. Random Mask versus Channel Mask. As our Random Mask applies independent random masks to different channels in a layer, we actually break the symmetry of the original CNN structure. To see whether this asymmetric structure would help, we try to directly drop whole channels instead of neurons using the same drop ratio as the Random Mask and train it to see the performance. This channel mask does not hurt the symmetry while also leading to the same decrease in convolutional operations. Table 2 shows that although our Random Mask network suffers a small drop in test accuracy due to the high drop ratio, we have a great gain in the robustness, compared with the channel-masking network. Random Mask versus Same Mask. The randomness in generating masks in different channels and layers allows each convolutional filter to focus on different patterns of feature distribution. We show the essentialness of generating various masks per layer via experiments that compare Random Mask to a method that only
6

Under review as a conference paper at ICLR 2019

Network Structure
Normal ResNet-18
0.3-Shallow 0.5-Shallow 0.7-Shallow
0.3-Deep 0.5-Deep 0.7-Deep 0.3-Shallow, 0.3-Deep 0.3-Shallow, 0.7-Deep 0.7-Shallow, 0.7-Deep 0.7-Shallow, 0.3-Deep
0.5-Shallow 0.9-Shallow
0.5-ShallowDC 0.9-ShallowDC 0.5-ShallowSM 0.9-ShallowSM 0.5-Shallow×2 0.9-Shallow×2 0.9-Shallow×4 Normal ResNet-18EN 0.5-Shallow×2,EN 0.5-ShallowEN 0.9-ShallowEN

FGSM16
14.91% 23.29% 30.86% 48.57% 14.62% 10.76% 11.23% 24.15% 11.26% 27.43% 40.58% 30.86% 79.93% 12.15% 19.00% 48.86% 39.40% 20.78% 68.83% 59.64% 16.24% 19.84% 31.38% 81.95%

PGD16
2.96% 14.53% 26.50% 47.76% 1.95% 2.57% 3.24% 12.67% 7.94% 32.72% 42.95% 26.50% 83.08%
4.68% 19.33% 44.04% 50.40% 12.51% 66.86% 59.15%
2.22% 11.86% 27.58% 85.14%

PGD32
2.26% 5.73% 10.33% 21.39% 1.30% 4.52% 2.64% 6.75% 5.77% 16.26% 19.00% 10.33% 55.02% 4.05% 10.08% 19.81% 29.23% 5.38% 37.51% 32.29% 1.46% 5.30% 9.97% 56.02%

CW40
8.23% 36.95% 54.02% 73.70% 7.88% 7.19% 10.10% 29.65% 23.31% 62.47% 68.58% 54.02% 89.67% 12.72% 44.80% 72.07% 65.38% 34.00% 82.74% 78.88%
8.58% 37.37% 58.07% 91.36%

Test Accuracy
95.33% 94.03% 93.39% 91.83% 95.16% 94.94% 94.61% 94.16% 93.44% 89.78% 91.23% 93.39% 87.68% 94.97% 93.27% 92.57% 74.28% 94.12% 90.49% 90.57% 96.12% 95.24% 94.56% 89.45%

Table 2: Experiments in Section 3.3. -ShallowDC, -ShallowSM, -Shallow×n and -ShallowEN mean dropping channels with ratio , applying same mask with ratio , increasing channel number to n times with
mask ratio  for every channel and ensemble five models with different masks of same ratio  respectively.

randomly generates one mask per layer and uses it in every channel. Table 2 shows that applying the same mask to each channel will decrease the test accuracy. This may result from the limitation of expressivity due to the monotone masks at every masked layer. In fact, we can illustrate such limitation using simple calculations. Since the filters in our base network ResNet-18 is of size 3 × 3, each element of the feature maps after the first convolutional layer can extract features from at most 9 pixels in the original image. This means that if we use the same mask and the drop ratio is 90%, only at most 9 × 10% of the input image can be caught by the convolutional layer, which would cause severe loss of input information.
Increase the Number of Channels. In order to compensate the loss of masking many neurons in each channel, it is reasonable that we may need more convolutional filters for feature extraction. Therefore, we try to increase the number of channels at masked layers. Table 2 shows that despite ResNet-18 is a well-designed network structure, increasing channels does help the network with Random Mask to get higher test accuracy while maintaining good robustness performance.
Ensemble Methods. Thanks to the diversity of Random Mask, we may directly use several networks with the same structure but different Random Masks and ensemble them. Table 2 shows that such ensemble methods can improve a network with Random Mask in both test accuracy and robustness.
7

Under review as a conference paper at ICLR 2019
4 CONCLUSION AND FUTURE DIRECTIONS
In conclusion, we introduce and experiment on Random Mask, a modification of existing CNNs that makes CNNs capture more information including the pattern of feature distribution. We show that CNNs with Random Mask can achieve much better robustness while maintaining high test accuracy. More specifically, by using Random Mask, we reach state-of-the-art performance in black-box defense settings. Another insight resulting from our experiments is that the adversarial examples generated against CNNs with Random Mask actually change the semantic information of images and can even "fool" humans. We hope that this finding can inspire more people to rethink adversarial examples and the robustness of neural networks.
REFERENCES
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. CoRR, abs/1802.00420, 2018. URL http://arxiv. org/abs/1802.00420.
Yoshua Bengio, Re´jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137­1155, 2003.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim S rndic´, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pp. 387­402. Springer, 2013.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. 2018.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. CoRR, abs/1608.04644, 2016. URL http://arxiv.org/abs/1608.04644.
Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. arXiv preprint arXiv:1803.01442, 2018.
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv preprint arXiv:1802.08686, 2018.
Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 1440­1448, 2015.
Ian Goodfellow. Defense against the dark arts: An overview of adversarial example security research and future research directions. arXiv preprint arXiv:1806.04169, 2018.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. URL http://arxiv.org/abs/1412. 6572.
Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.
Chuan Guo, Mayank Rana, Moustapha Cisse´, and Laurens van der Maaten. Countering adversarial images using input transformations. CoRR, abs/1711.00117, 2017. URL http://arxiv.org/abs/1711. 00117.
8

Under review as a conference paper at ICLR 2019
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal processing magazine, 29(6):82­97, 2012.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 7, 2017.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.
Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesva´ri. Learning with a strong adversary. arXiv preprint arXiv:1511.03034, 2015.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pp. 97­117. Springer, 2017.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135­147. ACM, 2017.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial perturbations. arXiv preprint arXiv:1702.04267, 2017.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP), pp. 582­597. IEEE, 2016a.
Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016b. URL http: //arxiv.org/abs/1605.07277.
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779­788, 2016.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91­99, 2015.
Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial examples inevitable? arXiv preprint arXiv:1809.02104, 2018.
9

Under review as a conference paper at ICLR 2019
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL http: //arxiv.org/abs/1312.6199.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3156­3164, 2015.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan L. Yuille. Mitigating adversarial effects through randomization. CoRR, abs/1711.01991, 2017. URL http://arxiv.org/abs/1711. 01991.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048­2057, 2015.
10

Under review as a conference paper at ICLR 2019
A RANDOM SHUFFLE

Figure 4: An example image that is random shuffled after being divided into 1 × 1, 2 × 2, 4 × 4 and 8 × 8 patches respectively.

In this part, we show results of our Random Shuffle experiment. Intuitively, by dropping randomly selected neurons in the neural network, we may let the network learn the relative margins and features better than normal networks. In randomly shuffled images, however, some global patterns of feature distributions are destroyed, so we expect that CNNs with Random Mask would have some trouble extracting feature information and might have worse performance than normal networks. In order to verify our intuition, we compare the test accuracy of a CNN with Random Mask to that of a normal CNN on randomly shuffled images. Specifically speaking, in the experiments, we first train a 0.7-Shallow network along with a normal network on ImageNet dataset. Then we select 5000 images from the validation set which are predicted correctly with more than 99% confidence by both normal and masked networks. We resize these images to 256 × 256 and then center crop them to 224 × 224. After that, we random shuffle them by dividing them into k × k small patches k  {2, 4, 8}, and randomly rearranging the order of patches. Figure 4 shows one example of our test images after random shuffling. Finally, we feed these shuffled images to the networks and see their classification accuracy. The results are shown in Table 3.

Model

2×2 4×4 8×8

Normal ResNet-18 99.58% 82.66% 17.56% 0.7-Shallow 97.36% 64.00% 11.94%

Table 3: The accuracy by using normal and masked networks to classify randomly shuffled test images.
From the results, we can see that our network with Random Mask always has lower accuracy than the normal network on these randomly shuffled test images, which indeed accords with our intuition. By randomly shuffling the patches in images, we break the relative positions and margins of the objects and pose negative impact to the network with Random Mask since it may rely on such information to classify. Note that randomly shuffled images are surely difficult for humans to classify, so this experiment might also imply that the network with Random Mask is more similar to human perception than the normal one.

B ATTACK APPROACHES
We first give an overview of how to attack a neural network in some mathematical notations. Let x be the input to the neural network and f be the function which represents the neural network with parameter . The output label of the network to the input can be computed as c = arg maxi f(x). In order to perform an adversarial attack, we add a small perturbation x to the original image and get an adversarial image xadv = x + x. The new input xadv should look visually similar to the original x. Here we use the commonly

11

Under review as a conference paper at ICLR 2019
used -norm metric to measure similarity, i.e., we require that||x||  . The attack is considered successful if the predicted label of the perturbed image cadv = arg maxi f(xadv) is different from c.
Generally speaking, there are two types of attack methods: Targeted Attack, which aims to change the output label of an image to a specific (and different) one, and Untargeted Attack, which only aims to change the output label and does not restrict which specific label the modified example should let the network output.
In this paper, we mainly use the following three attack approaches. J denotes the loss function of the neural network and y denotes the true label of x.
· Fast Gradient Sign Method (FGSM). FGSM (Goodfellow et al., 2015) is a one-step untargeted method which generates the adversarial example xadv by adding the sign of the gradients multiplied by a step size to the original benign image x. Note that FGSM controls the l-norm between the adversarial example and the original one by the parameter . xadv = x + · sign(xJ (x, y)).
· Basic iterative method (PGD). PGD is a multiple-step attack method which applies FGSM multiple times. To make the adversarial example still stay "close" to the original image, the image is projected to the l-ball centered at the original image after every step. The radius of the l-ball is called perturbation scale and is denoted by . x0adv = x, xakd+v1 = Clipx, xkadv + · sign(xakdv J (xkadv, y)) .
· CW Attack. Carlini & Wagner (2016) shows that constructing an adversarial example can be formulated as solving the following optimization problem: xadv = arg min c · g(x ) + ||x - x||22,
x
where c · g(x ) is the loss function that evaluates the quality of x as an adversarial example and the term ||x - x||22 controls the scale of the perturbation. More specifically, in the untargeted attack setting, the loss function g(x) can be defined as:
g(x) = max{max (f (x)i) - f (x)y, -},
i=y
where the parameter  is called confidence.
C RESNET-18 ARCHITECTURE
Here we briefly introduce the architecture of ResNet-18, the target model to which we apply Random Mask. ResNet-18 contains 5 blocks: the 0th block is one single 3 × 3 convolutional layer, and each of the rest contains four 3 × 3 convolutional layers. Figure 5 shows the whole structure of ResNet-18. In our framework, applying Random Mask to a block means applying Random Mask to every layer in it.
D TRAINING PROCESS ON CIFAR-10
To guarantee our experiments are reproducible, here we show more details on the training process of our network with Random Mask. When training ResNet-18 on CIFAR-10, we first subtract per-pixel mean. Then we apply a zero-padding of width 4, a random horizontal flip and a random crop of size 32 × 32. No other data augmentation method is used. We apply SGD with momentum parameter 0.9, weight decay parameter 5 × 10-4 and mini-batch size 128 to train the data for 350 epochs. The learning rate starts from 0.1 and is divided by 10 when the number of epochs reaches 150 and 250. In accordance to the original ResNet-18 architecture, we do not use Dropout during training. Figure 6 shows the train and test curve of four typical models.
12

Under review as a conference paper at ICLR 2019
Figure 5: The architecture of ResNet-18
Figure 6: Train and Test Curve
E INFLUENCE OF DROP RATIO IN RANDOM MASK
The drop ratio is one of the key parameters in Random Mask. Figure 7 shows the results of using different ratios when masking the shallow layers only. We can see the trade-off between robustness and generalization.
13

Under review as a conference paper at ICLR 2019
Figure 7: Relationship between defense rate and test accuracy with respect to different drop ratios. Note that experiments show that Random Masks with the same ratio usually perform similarly. Each red star represents a specific drop ratio with its value written near the star.
F ADVERSARIAL EXAMPLES GENERATED BY APPLYING RANDOM MASK
F.1 ADVERSARIAL EXAMPLES THAT CAN "FOOL" HUMAN Figure 8 shows some adversarial examples generated from CIFAR-10 along with the corresponding original images. These examples are generated from CIFAR-10 against ResNet-18 with Random Mask of drop ratio 0.8 on the 0th, 1st, 2nd blocks and another ResNet-18 with Random Mask of drop ratio 0.9 on the 1st, 2nd blocks. We use attack method PGD with perturbation scale  = 16 and  = 32. We also show some adversarial examples generated from Tiny-ImageNet1 along with the corresponding original images in Figure 9.
Dog Bird Frog Dog Automo- Ship Dog Ship Bird Frog Bird bile
Airplane Airplane Truck Horse Truck Automo- Bird Automo- Ship Bird Deer bile bile
Figure 8: The adversarial examples (upper) shown in Figure 1 along with the original images (lower) from CIFAR-10.
1https://tiny-imagenet.herokuapp.com/ 14

Under review as a conference paper at ICLR 2019

Mushroom Monarch But- Ladybug Black Widow Sulphur But-

terfly

terfly

Teddy

Mushroom

Tarantula Black Widow

Fly

Ladybug Egyptian Cat Brain Coral Goldfish

Figure 9: Adversarial examples (upper) generated from Tiny-ImageNet against ResNet-18 with Random Mask of ratio 0.9 on the 1st, 2nd blocks. along with the original images (lower). The attack methods are PGD
with scale 64 and 32, step size 1 and step number 40 and 80 respectively.

F.2 RANDOMLY SELECTED ADVERSARIAL EXAMPLES
See Figure 10 for a randomly sampled set of images from Tiny-ImageNet along with the corresponding adversarial examples generated against ResNet-18 with Random Mask and normal ResNet-18.

G MORE EXPERIMENTAL RESULTS
G.1 BLACK-BOX DEFENSE UNDER MADRY'S SETTING
Here we list the black-box settings in Madry's paper (Madry et al., 2017). In their experiments, ResNets are trained by minimizing the following loss:

min E(x,y)D


max L(, x + , y)
S

.

The outer minimization is achieved by gradient descent and the inner maximization is achieved by generating PGD adversarial examples with step size 2, the number of steps 7 and the perturbation scale 8. After training, in their black-box attack setting, they generate adversarial examples from naturally trained neural networks and test them on their models. Both FGSM and PGD adversarial examples have step size or perturbation scale 8 and PGD runs for 7 gradient descent steps with step size 2. For our models, we apply Random Mask to shallow blocks with drop ratio 0.85. For the ensembled Random Mask, we use 5 different 0.85-Shallow Random Mask models. The ratio is selected by considering the trade-off of robustness and generalization performance. When doing attacks, we generate the adversarial examples in the same way as Madry's paper does.

G.2 WHITE-BOX See Table 4 for the performance of CNNs with Random Mask under white-box attacks.

15

Under review as a conference paper at ICLR 2019

Network Structure
Normal ResNet-18 0.5-Shallow 0.7-Shallow

FGSM1
81.24% 85.22% 85.70%

FGSM2
65.78% 68.65% 69.69%

FGSM4
51.24% 52.04% 54.51%

PGD2
24.26% 42.35% 49.30%

PGD4
3.40% 9.11% 19.88%

PGD8
0.02% 0.42% 3.28%

Acc
95.33% 93.39% 91.83%

Table 4: White-box defense performance. FGSM1, FGSM2, FGSM4 refer to FGSM with step size 1,2,4 respectively. PGD2, PGD4, PGD8 refer to PGD with perturbation scale 2,4,8 and step number 4,6,10 respectively.

G.3 TRANSFERABILITY AND GRAY-BOX DEFENSE
Here we show the gray-box defense ability of Random Mask and the transferability of the adversarial examples generated against Random Mask. We generate gray-box attacks in the following two ways. One way is to generate adversarial examples against one trained neural network and test those images on a network with the same structure but different initialization. The other way is specific to our Random Mask models. We generate adversarial examples on one trained network with Random Mask and test them on a network with the same drop ratio but different Random Mask. In both of these two ways, the adversarial knows some information on the structure of the network, but does not know the parameters of it. To see the transferability of the generated adversarial examples, we also test them on DenseNet-121 and VGG-19 (Simonyan & Zisserman, 2014).

Source Target
Normal ResNet-18 0.5-Shallow
0.5-ShallowDIF 0.7-Shallow
0.7-ShallowDIF DenseNet-121
VGG-19

Normal ResNet-18
13.91% 28.83% 28.91% 49.14% 48.23% 20.23% 15.83%

0.5-Shallow
20.90% 20.22% 19.37% 31.30% 31.77% 22.38% 17.43%

0.7-Shallow
28.42% 23.24% 22.11% 23.26% 23.54% 28.05% 20.85%

Table 5: Results on gray-box attacks and transferability. We use FGSM with step size 16 to generate the adversarial examples on source networks and test them on target networks. For target networks, Normal ResNet-18, 0.5-Shallow and 0.7-Shallow represent the networks with the same structure as the corresponding source networks but with different initialization values. 0.5-ShallowDIF and 0.7-ShallowDIF represent the networks with the same drop ratios as the corresponding source networks but with different random masks.

Table 5 shows that Random Mask can also improve the performance under gray-box attacks. In addition, we find that CNNs with Random Mask have similar performance on adversarial examples generated by our two kinds of gray-box attacks. This phenomenon indicates that CNNs with Random Mask of same ratios have similar properties and catch similar information.
G.4 EXPERIMENTS ON MNIST
See Table 6 for experimental results on dataset MNIST.
16

Under review as a conference paper at ICLR 2019

Network Structure
Normal ResNet-18 0.5-Shallow 0.7-Shallow 0.9-Shallow

PGD
2.43% 7.42% 19.16% 42.83%

Acc
99.49% 99.34% 99.29% 99.10%

Table 6: Experiments on MNIST. We generate adversarial examples against DenseNet-121 by PGD. PGD runs for 40 steps with step size of 0.01 × 255. The perturbation scale is 0.3 × 255.

G.5 FULL INFORMATION FOR EXPERIMENTS IN SECTION 3.3
In this part, we will show more experimental results on Random Mask using different adversarial examples, different attack methods and different mask settings. More specifically, we choose 5000 test images from CIFAR-10 which are correctly classified by the original network to generate FGSM and PGD adversarial examples, and 1000 test images for CW attack.
For FGSM, we try step size  {8, 16, 32}, namely FGSM8, FGSM16, FGSM32, to generate adversarial examples.
For PGD, we have tried more extensive settings. Let { , T, } be the PGD setting with step size , the number of steps T and the perturbation scale , then we have tried PGD settings (1, 8, 4), (2, 4, 4), (4, 2, 4), (1, 12, 8), (2, 6, 8), (4, 3, 8), (1, 20, 16), (2, 10, 16), (4, 5, 16), (1, 40, 32), (2, 20, 32), (4, 10, 32) to generate PGD adversarial examples. From the experimental results, we observe the following phenomena. First, we find that the larger the perturbation scale is, the stronger the adversarial examples are. Second, for a fixed perturbation scale, the smaller the step size is, the more successful the attack is, as it searches the adversarial examples in a more careful way around the original image. Based on these observation, we only show strong PGD attack results in the Appendix, namely the settings (1, 20, 16) (PGD16), (2, 10, 16) (PGD2,16) and (1, 40, 32) (PGD32). Nonetheless, our models also perform much better on weak PGD attacks.
For CW attack, we have also tried different confidence parameters . However, we find that for large , the algorithm cannot find an adversarial example for some neural networks such as VGG because of its logit scale. For smaller , the adversarial examples have weak transfer ability, which means they can be easily defensed even by normal networks. Therefore, in order to balance these two factors, we choose  = 40 (CW40) for DenseNet-121, ResNet-50, SENet-18 (Hu et al., 2017) and  = 20 (CW20) for ResNet-18 as a good choice to compare our models with normal ones. The step number for choosing the parameter c is set to 30.
Note that the noise of FGSM and PGD is considered in the sense of l norm and the noise of CW is considered in the sense of l2 norm. All adversarial examples used to evaluate can fool the original network. The following is our experimental results. DC means we replace Random Mask with a decreased number of channels in the corresponding blocks to achieve the same drop ratio. SM means we use the same mask on all the channels in a layer. ×n means we multiply the number of the channels in the corresponding blocks by n times. EN means we ensemble five models with different masks of the same drop ratio.

17

Under review as a conference paper at ICLR 2019
Figure 10: Randomly sampled images from Tiny-ImageNet dataset. The network structure used to generate these images is ResNet-18 with Random Mask of ratio 0.9 on the 1st, 2nd blocks. The attack method is PGD with perturbation scale 64, step size 1 and step number 80. For each image, we show the image generated against network with Random Mask (upper), the image generated against the normal ResNet-18 (middle) and the original image (lower).
18

Under review as a conference paper at ICLR 2019

Network
Normal ResNet-18 0.3-Shallow 0.5-Shallow 0.7-Shallow 0.75-Shallow 0.8-Shallow 0.85-Shallow 0.9-Shallow 0.95-Shallow 0.3-Deep 0.5-Deep 0.7-Deep
0.5-Shallow, 0.5-Deep 0.3-Shallow, 0.3-Deep 0.3-Shallow, 0.7-Deep 0.7-Shallow, 0.7-Deep 0.7-Shallow, 0.3-Deep
0.5-ShallowDC 0.7-ShallowDC 0.9-ShallowDC 0.5-ShallowSM 0.7-ShallowSM 0.9-ShallowSM 0.5-Shallow×2 0.7-Shallow×2 0.9-Shallow×2 0.9-Shallow×4 Normal ResNet-18EN 0.5-Shallow×2,EN 0.5-ShallowEN 0.7-ShallowEN 0.85-ShallowEN 0.9-ShallowEN

FGSM8
29.78% 55.40% 66.87% 79.50% 83.12% 85.49% 88.18% 94.08% 96.16% 28.51% 25.01% 23.94% 58.49% 51.03% 36.16% 64.85% 74.73% 36.39% 43.81% 49.53% 77.30% 82.59% 67.06% 51.25% 68.82% 88.00% 82.96% 35.89% 55.25% 69.35% 81.98% 90.13% 95.37%

FGSM16
14.91% 23.29% 30.86% 48.57% 59.22% 63.01% 65.27% 79.93% 87.36% 14.62% 10.76% 11.23% 26.34% 24.15% 11.26% 27.43% 40.58% 12.15% 17.74% 19.00% 48.86% 48.03% 39.40% 20.78% 30.94% 68.83% 59.64% 16.24% 19.84% 31.38% 51.81% 66.51% 81.95%

FGSM32
11.53% 7.73% 6.65% 10.51% 17.16% 15.57% 18.33% 43.70% 59.05% 8.78% 10.24% 10.48% 11.08% 10.82% 9.16% 10.09% 9.12% 8.24% 8.32% 7.23% 12.50% 12.30% 16.25% 10.29% 7.22% 28.55% 19.44% 9.92% 8.36% 7.73% 8.57% 17.91% 43.42%

PGD16
2.96% 14.53% 26.50% 47.76% 56.08% 63.16% 69.40% 83.08% 89.98%
1.95% 2.57% 3.24% 19.36% 12.67% 7.94% 32.72% 42.95% 4.68% 7.51% 19.33% 44.04% 57.62% 50.40% 12.51% 29.83% 66.86% 59.15% 2.22% 11.86% 27.58% 50.79% 71.38% 85.14%

PGD2,16
3.44% 16.00% 28.65% 49.62% 58.81% 65.16% 71.21% 83.72% 90.13%
2.43% 3.81% 4.07% 20.77% 14.06% 8.00% 33.84% 45.32% 5.52% 9.20% 20.88% 46.58% 57.83% 50.20% 13.89% 31.74% 69.42% 60.72% 2.56% 13.44% 29.68% 53.88% 72.30% 85.79%

PGD32
2.26% 5.73% 10.33% 21.39% 26.87% 32.86% 36.12% 55.02% 68.25% 1.30% 4.52% 2.64% 9.07% 6.75% 5.77% 16.26% 19.00% 4.05% 4.52% 10.08% 19.81% 24.81% 29.23% 5.38% 11.44% 37.51% 32.29% 1.46% 5.30% 9.97% 23.28% 38.25% 56.02%

CW40
8.23% 36.95% 54.02% 73.70% 77.82% 81.75% 85.46% 89.67% 90.24%
7.88% 7.19% 10.10% 43.59% 29.65% 23.31% 62.47% 68.58% 12.72% 19.34% 44.80% 72.07% 79.55% 65.38% 34.00% 60.17% 82.74% 78.88% 8.58% 37.37% 58.07% 77.74% 87.37% 91.36%

Acc
95.33% 94.03% 93.39% 91.83% 91.46% 91.18% 90.15% 87.68% 84.53% 95.16% 94.94% 94.61% 92.39% 94.16% 93.44% 89.78% 91.23% 94.97% 94.23% 93.27% 92.57% 89.81% 74.28% 94.12% 93.01% 90.49% 90.57% 96.12% 95.24% 94.56% 93.31% 91.77% 89.45%

Table 7: Extended experimental results of Section 3.3. Adversarial examples generated against DenseNet-121. The model trained on CIFAR-10 achieves 95.62% accuracy on test set. -ShallowDC, -ShallowSM, -Shallow×n and -ShallowEN mean dropping channels with ratio , applying same mask with ratio , increasing channel number to n times with mask ratio  for every channel and ensemble five models with different masks of same ratio  respectively.

19

Under review as a conference paper at ICLR 2019

Network
Normal ResNet-18 0.3-Shallow 0.5-Shallow 0.7-Shallow 0.75-Shallow 0.8-Shallow 0.85-Shallow 0.9-Shallow 0.95-Shallow 0.3-Deep 0.5-Deep 0.7-Deep
0.5-Shallow, 0.5-Deep 0.3-Shallow, 0.3-Deep 0.3-Shallow, 0.7-Deep 0.7-Shallow, 0.7-Deep 0.7-Shallow, 0.3-Deep
0.5-ShallowDC 0.7-ShallowDC 0.9-ShallowDC 0.5-ShallowSM 0.7-ShallowSM 0.9-ShallowSM 0.5-Shallow×2 0.7-Shallow×2 0.9-Shallow×2 0.9-Shallow×4 Normal ResNet-18EN 0.5-Shallow×2,EN 0.5-ShallowEN 0.7-ShallowEN 0.85-ShallowEN 0.9-ShallowEN

FGSM8
26.99% 48.76% 59.66% 74.00% 78.37% 81.67% 86.31% 92.89% 95.07% 25.96% 25.21% 24.36% 53.46% 43.32% 34.09% 61.22% 70.43% 32.86% 37.96% 48.54% 73.96% 80.80% 69.15% 46.50% 63.37% 84.28% 78.24% 29.66% 49.16% 63.38% 77.25% 88.56% 94.31%

FGSM16
13.91% 21.32% 30.48% 47.11% 56.05% 59.14% 63.16% 77.90% 85.40% 15.46% 9.21% 9.49% 24.65% 20.55% 11.05% 28.11% 39.15% 13.89% 16.23% 19.10% 47.63% 48.37% 43.55% 21.37% 29.90% 64.47% 56.31% 14.37% 19.81% 30.25% 50.07% 65.23% 79.47%

FGSM32
3.57% 9.54% 11.60% 15.65% 21.44% 19.60% 22.23% 45.63% 59.91% 7.18% 1.44% 2.60% 7.08% 4.14% 1.58% 13.78% 13.94% 3.71% 5.05% 11.37% 16.60% 15.26% 20.26% 6.06% 12.07% 31.90% 23.28% 3.97% 6.73% 11.05% 13.80% 22.50% 44.67%

PGD16
1.42% 8.14% 18.46% 39.61% 49.14% 55.84% 64.73% 81.70% 88.31% 1.18% 2.17% 2.36% 13.48% 7.31% 6.01% 27.12% 36.88% 1.93% 4.30% 14.34% 36.19% 53.69% 50.68% 6.86% 20.70% 62.65% 52.38% 0.98% 7.02% 18.15% 41.59% 65.68% 82.97%

PGD2,16
1.84% 9.51% 21.44% 43.17% 52.21% 59.63% 67.03% 82.50% 89.64% 1.28% 2.63% 3.08% 15.65% 9.36% 6.77% 30.24% 39.81% 2.89% 5.96% 16.01% 40.86% 54.78% 50.80% 8.65% 24.08% 65.24% 55.63% 1.24% 8.50% 21.14% 44.78% 68.31% 84.05%

PGD32
0.96% 4.02% 7.70% 16.09% 20.31% 26.61% 31.33% 54.12% 66.52% 0.88% 2.31% 1.31% 6.99% 4.46% 4.56% 13.51% 15.45% 2.19% 2.65% 7.04% 15.52% 22.90% 28.82% 3.59% 7.76% 33.06% 25.91% 0.54% 3.46% 6.71% 15.85% 32.77% 54.46%

CW20
2.19% 38.87% 60.65% 79.04% 81.59% 82.78% 86.06% 90.29% 90.97%
2.66% 3.15% 6.62% 49.95% 32.92% 24.11% 69.15% 74.57% 6.10% 15.44% 50.62% 73.68% 82.34% 71.62% 39.12% 67.02% 85.08% 82.26% 2.00% 38.12% 63.90% 80.86% 88.26% 90.52%

Acc
95.33% 94.03% 93.39% 91.83% 91.46% 91.18% 90.15% 87.68% 84.53% 95.16% 94.94% 94.61% 92.39% 94.16% 93.44% 89.78% 91.23% 94.97% 94.23% 93.27% 92.57% 89.81% 74.28% 94.12% 93.01% 90.49% 90.57% 96.12% 95.24 % 94.56% 93.31% 91.77% 89.4 %

Table 8: Extended experimental results of Section 3.3. Adversarial examples are generated against ResNet-18. The model trained on CIFAR-10 achieves 95.27% accuracy on test set. -ShallowDC, -ShallowSM, -Shallow×n and -ShallowEN mean dropping channels with ratio , applying same mask with ratio , increasing channel number to n times with mask ratio  for every channel and ensemble five models with different masks of same ratio  respectively.

20

Under review as a conference paper at ICLR 2019

Network
Normal ResNet-18 0.3-Shallow 0.5-Shallow 0.7-Shallow 0.75-Shallow 0.8-Shallow 0.85-Shallow 0.9-Shallow 0.95-Shallow 0.3-Deep 0.5-Deep 0.7-Deep
0.5-Shallow, 0.5-Deep 0.3-Shallow, 0.3-Deep 0.3-Shallow, 0.7-Deep 0.7-Shallow, 0.7-Deep 0.7-Shallow, 0.3-Deep
0.5-ShallowDC 0.7-ShallowDC 0.9-ShallowDC 0.5-ShallowSM 0.7-ShallowSM 0.9-ShallowSM 0.5-Shallow×2 0.7-Shallow×2 0.9-Shallow×2 0.9-Shallow×4 Normal ResNet-18EN 0.5-Shallow×2,EN 0.5-ShallowEN 0.7-ShallowEN 0.85-ShallowEN 0.9-shallowen

FGSM8
29.33% 45.32% 56.26% 70.57% 77.18% 80.33% 84.81% 92.17% 94.43% 27.78% 27.24% 24.81% 48.78% 42.18% 33.11% 56.39% 66.33% 31.56% 37.52% 44.00% 69.40% 77.25% 64.32% 41.51% 58.59% 83.05% 75.74% 32.70% 44.90% 59.64% 73.45% 87.58% 93.87%

FGSM16
15.14% 18.89% 27.32% 42.40% 53.01% 56.21% 61.02% 77.68% 85.54% 15.03% 10.29% 9.99% 21.04% 18.20% 11.08% 24.14% 36.31% 13.64% 15.72% 16.90% 41.82% 44.94% 39.76% 18.47% 25.92% 63.73% 55.03% 15.49% 16.55% 26.21% 45.60% 62.24% 79.15%

FGSM32
3.88% 9.16% 10.72% 14.98% 19.68% 18.03% 21.50% 45.93% 60.71% 8.07% 2.47% 2.50% 6.66% 5.26% 2.27% 12.18% 13.09% 4.87% 5.38% 10.30% 14.27% 13.80% 19.21% 6.02% 11.20% 29.22% 21.59% 4.93% 6.41% 10.17% 12.99% 21.81% 46.44%

PGD16
0.42% 4.36% 12.36% 33.73% 42.67% 52.45% 62.62% 81.20% 88.63% 0.42% 0.52% 0.67% 7.51% 3.96% 2.45% 21.86% 30.13% 0.80% 1.79% 8.93% 29.83% 49.52% 50.16% 3.67% 14.75% 58.85% 48.93% 0.32% 2.70% 11.41% 33.62% 62.84% 82.71%

PGD2,16
0.96% 5.81% 15.29% 37.56% 46.72% 55.54% 63.80% 82.50% 89.16% 0.60% 1.10% 1.01% 9.79% 5.44% 3.54% 24.88% 33.96% 1.30% 2.91% 11.35% 33.51% 51.13% 49.15% 4.80% 18.29% 61.82% 51.78% 0.84% 4.00% 14.23% 38.49% 64.88% 83.32%

PGD32
0.08% 0.92% 3.48% 12.38% 15.88% 22.17% 29.35% 53.44% 66.69% 0.12% 0.50% 0.20% 2.52% 1.65% 1.29% 9.01% 12.09% 0.28% 0.56% 3.46% 9.60% 21.24% 28.24% 0.62% 4.34% 28.09% 20.52% 0.06% 0.64% 2.48% 12.06% 29.10% 53.87%

CW40
0.00% 1.98% 8.92% 33.08% 39.10% 47.52% 53.71% 66.70% 71.82% 0.00% 0.00% 0.00% 5.09% 1.64% 0.55% 22.25% 30.68% 0.11% 0.44% 4.95% 26.65% 46.44% 45.18% 1.32% 13.77% 50.11% 47.08% 0.00% 1.42% 8.12% 32.60% 54.29% 67.71%

Acc
95.33% 94.03% 93.39% 91.83% 91.46% 91.18% 90.15% 87.68% 84.53% 95.16% 94.94% 94.61% 92.39% 94.16% 93.44% 89.78% 91.23% 94.97% 94.23% 93.27% 92.57% 89.81% 74.28% 94.12% 93.01% 90.49% 90.57% 96.12% 95.24% 94.56% 93.31% 91.77% 89.45%

Table 9: Extended experimental results of Section 3.3. Adversarial examples are generated against ResNet-50. The model trained on CIFAR-10 achieves 95.69% accuracy on test set. -ShallowDC, -ShallowSM, -Shallow×n and -ShallowEN mean dropping channels with ratio , applying same mask with ratio , increasing channel number to n times with mask ratio  for every channel and ensemble five models with different masks of same ratio  respectively.

21

Under review as a conference paper at ICLR 2019

Network
Normal ResNet-18 0.3-Shallow 0.5-Shallow 0.7-Shallow 0.75-Shallow 0.8-Shallow 0.85-Shallow 0.9-Shallow 0.95-Shallow 0.3-Deep 0.5-Deep 0.7-Deep
0.5-Shallow, 0.5-Deep 0.3-Shallow, 0.3-Deep 0.3-Shallow, 0.7-Deep 0.7-Shallow, 0.7-Deep 0.7-Shallow, 0.3-Deep
0.5-ShallowDC 0.7-ShallowDC 0.9-ShallowDC 0.5-ShallowSM 0.7-ShallowSM 0.9-ShallowSM 0.5-Shallow×2 0.7-Shallow×2 0.9-Shallow×2 0.9-Shallow×4 Normal ResNet-18EN 0.5-Shallow×2,EN 0.5-ShallowEN 0.7-ShallowEN 0.85-ShallowEN 0.9-ShallowEN

FGSM8
25.53% 46.12% 57.05% 72.67% 78.23% 82.27% 85.80% 92.93% 94.77% 23.76% 23.01% 22.87% 51.20% 42.34% 31.43% 57.26% 68.66% 30.81% 34.57% 43.46% 71.27% 79.48% 65.85% 44.13% 60.51% 85.26% 78.65% 27.87% 45.04% 60.42% 76.08% 88.64% 94.40%

FGSM16
17.47% 23.30% 31.01% 48.17% 58.19% 61.61% 65.92% 79.13% 87.13% 16.66% 12.19% 11.61% 25.49% 22.07% 12.17% 29.36% 41.32% 14.77% 17.32% 17.61% 49.21% 49.66% 42.59% 21.71% 30.89% 67.91% 60.05% 17.80% 20.25% 31.08% 51.49% 67.26% 81.32%

FGSM32
8.56% 10.48% 11.07% 15.20% 21.20% 19.70% 22.73% 48.34% 63.36%
9.52% 6.56% 6.63% 10.43% 7.84% 6.34% 13.99% 13.72% 6.08% 8.04% 10.54% 16.27% 15.65% 21.87% 9.49% 11.58% 32.51% 24.45% 8.81% 9.69% 10.77% 13.19% 23.30% 48.52%

PGD16
1.38% 9.57% 21.42% 44.95% 53.56% 61.55% 69.82% 84.55% 90.63% 1.12% 1.73% 2.12% 15.05% 8.47% 6.19% 31.68% 40.56% 2.13% 3.65% 15.15% 41.34% 58.96% 52.63% 8.91% 24.82% 67.34% 57.88% 1.02% 7.46% 20.98% 46.98% 71.44% 85.81%

PGD2,16
1.78% 10.19% 23.17% 46.90% 55.32% 63.83% 70.55% 84.63% 90.61%
1.26% 2.05% 2.54% 17.41% 9.68% 6.25% 32.58% 42.30% 2.61% 4.54% 15.51% 43.00% 60.00% 52.91% 9.61% 26.79% 69.14% 59.63% 1.24% 9.02% 22.56% 49.31% 72.42% 86.28%

PGD32
0.84% 4.44% 7.90% 19.67% 23.24% 31.05% 34.95% 55.58% 69.07% 0.66% 2.05% 1.29% 7.43% 4.44% 4.82% 15.16% 17.58% 1.83% 2.07% 7.12% 16.93% 26.78% 30.38% 3.47% 8.36% 36.78% 29.79% 0.52% 3.30% 7.09% 18.95% 37.16% 56.14%

CW40
0.00% 2.66% 14.61% 39.89% 47.33% 51.14% 57.36% 65.63% 69.14% 0.00% 0.00% 0.19% 12.26% 1.70% 1.53% 30.00% 35.71% 0.00% 0.19% 7.41% 34.92% 48.28% 43.22% 2.65% 21.52% 52.96% 50.19% 0.00% 2.84% 13.61% 39.51% 56.36% 66.99%

Acc
95.33% 94.03% 93.39% 91.83% 91.46% 91.18% 90.15% 87.68% 84.53% 95.16% 94.94% 94.61% 92.39% 94.16% 93.44% 89.78% 91.23% 94.97% 94.23% 93.27% 92.57% 89.81% 74.28% 94.12% 93.01% 90.49% 90.57% 96.12% 95.24% 94.56% 93.31% 91.77% 89.45%

Table 10: Extended experimental results of Section 3.3. Adversarial examples are generated against SENet-18. The model trained on CIFAR-10 achieves 95.15% accuracy on test set. -ShallowDC, -ShallowSM, -Shallow×n and -ShallowEN mean dropping channels with ratio , applying same mask with ratio , increasing channel number to n times with mask ratio  for every channel and ensemble five models with different masks of same ratio  respectively.

22

Under review as a conference paper at ICLR 2019

Network
Normal ResNet-18 0.3-Shallow 0.5-Shallow 0.7-Shallow 0.75-Shallow 0.8-Shallow 0.85-Shallow 0.9-Shallow 0.95-Shallow 0.3-Deep 0.5-Deep 0.7-Deep
0.5-Shallow, 0.5-Deep 0.3-Shallow, 0.3-Deep 0.3-Shallow, 0.7-Deep 0.7-Shallow, 0.7-Deep 0.7-Shallow, 0.3-Deep
0.5-ShallowDC 0.7-ShallowDC 0.9-ShallowDC 0.5-ShallowSM 0.7-ShallowSM 0.9-ShallowSM 0.5-Shallow×2 0.7-Shallow×2 0.9-Shallow×2 0.9-Shallow×4 Normal ResNet-18EN 0.5-Shallow×2,EN 0.5-ShallowEN 0.7-ShallowEN 0.85-ShallowEN 0.9-ShallowEN

FGSM8
37.67% 50.06% 57.35% 71.75% 76.81% 79.46% 85.51% 92.58% 95.24% 36.72% 35.93% 34.05% 52.05% 47.36% 40.38% 59.19% 67.14% 37.37% 42.39% 47.41% 69.61% 79.69% 67.77% 46.93% 60.23% 83.32% 77.68% 39.42% 49.51% 60.43% 74.11% 87.48% 94.14%

FGSM16
20.25% 23.54% 30.52% 47.35% 56.69% 61.45% 66.55% 80.68% 87.10% 18.97% 13.80% 13.06% 24.88% 22.74% 13.50% 28.00% 40.57% 16.99% 19.90% 21.12% 46.57% 48.86% 44.38% 21.74% 29.72% 66.44% 58.73% 19.53% 19.29% 31.07% 50.89% 67.75% 82.46%

FGSM32
5.40% 9.53% 11.13% 15.47% 19.44% 21.36% 25.35% 51.90% 64.22% 9.65% 2.99% 4.04% 6.74% 5.04% 3.28% 12.13% 13.80% 6.62% 6.74% 11.43% 14.85% 13.87% 20.74% 7.11% 11.07% 33.11% 24.40% 6.69% 7.16% 10.50% 13.54% 25.62% 52.59%

PGD16
9.88% 14.99% 22.27% 39.66% 47.88% 56.21% 64.09% 81.65% 88.50%
9.21% 10.24% 10.36% 17.83% 14.44% 11.38% 29.88% 37.63% 9.76% 11.93% 20.10% 37.31% 52.11% 49.64% 14.52% 23.52% 61.73% 52.07% 8.67% 13.71% 21.54% 41.26% 65.68% 83.05%

PGD2,16
12.81% 18.66% 26.87% 44.42% 53.76% 61.01% 67.77% 83.90% 90.04% 11.48% 12.77% 12.70% 21.75% 17.82% 14.52% 34.28% 42.54% 12.35% 15.39% 23.16% 43.96% 56.12% 51.28% 18.17% 28.11% 66.76% 57.72% 12.13% 17.73% 27.09% 47.68% 69.68% 85.46%

PGD32
6.72% 9.28% 10.85% 15.91% 18.76% 23.16% 27.53% 52.47% 64.64% 6.40% 8.95% 7.46% 10.37% 9.08% 8.12% 14.12% 16.00% 8.07% 8.01% 10.82% 15.26% 20.35% 27.31% 8.09% 11.56% 30.33% 23.55% 6.46% 8.74% 10.99% 16.01% 29.26% 53.42%

Acc
95.33% 94.03% 93.39% 91.83% 91.46% 91.18% 90.15% 87.68% 84.53% 95.16% 94.94% 94.61% 92.39% 94.16% 93.44% 89.78% 91.23% 94.97% 94.23% 93.27% 92.57% 89.81% 74.28% 94.12% 93.01% 90.49% 90.57% 96.12% 95.24% 94.56% 93.31% 91.77% 89.45%

Table 11: Extended experimental results of Section 3.3. Adversarial examples are generated against VGG-19. The model trained on CIFAR-10 achieves 94.04% accuracy on test set. -ShallowDC, -ShallowSM, -Shallow×n and -ShallowEN mean dropping channels with ratio , applying same mask with ratio , increasing channel number to n times with mask ratio  for every channel and ensemble five models with different masks of same ratio  respectively.

23

