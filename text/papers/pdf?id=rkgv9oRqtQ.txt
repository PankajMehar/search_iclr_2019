Under review as a conference paper at ICLR 2019
COMPOUND DENSITY NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Despite the huge success of deep neural networks (NNs), finding good mechanisms for quantifying their prediction uncertainty is still an open problem. It was recently shown, that using an ensemble of NNs trained with a proper scoring rule leads to results competitive to those of Bayesian NNs. This ensemble method can be understood as finite mixture model with uniform mixing weights. We build on this mixture model approach and increase its flexibility by replacing the fixed mixing weights by an adaptive, input-dependent distribution (specifying the probability of each component) represented by an NN, and by considering uncountably many mixture components. The resulting model can be seen as the continuous counterpart to mixture density networks and is therefore referred to as compound density networks. We empirically show that the proposed model results in better uncertainty estimates and is more robust to adversarial examples than previous approaches.
1 INTRODUCTION
Deep neural networks (NNs) have achieved state-of-the-art performance in many application areas, such as computer vision (Krizhevsky et al., 2012) and natural language processing (Collobert et al., 2011). However, despite achieving impressive prediction accuracy on these supervised machine learning tasks, NNs do not provide good ways of quantifying predictive uncertainty. This is undesirable for many mission critical applications, where taking wrong predictions with high confidence could have fatal consequences (e.g. in medical diagnostics or autonomous driving).
A principled and the most explored way to quantify the uncertainty in NNs is through Bayesian inference. In the so-called Bayesian neural networks (BNNs) (Neal, 1995), the NN parameters are treated as random variables and the goal of learning is to infer the posterior probability distribution of the parameters given the training data. Since exact Bayesian inference in NNs is computationally intractable, different approximation techniques have been proposed (Neal, 1995; Blundell et al., 2015; Herna´ndez-Lobato & Adams, 2015; Ritter et al., 2018, etc.). Given the (approximate) posterior, the final predictive distribution is obtained as the expected distribution under the posterior, where the expectation can be seen as an ensemble of an uncountably infinite number of predictors, where the prediction of each model is weighted by the posterior probability of the corresponding parameters.
Based on a Bayesian interpretation of dropout (Srivastava et al., 2014), Gal & Ghahramani (2016) proposed to apply it not only during training but also when making predictions to estimate predictive uncertainty. Interestingly, dropout has been also interpreted as ensemble model (Srivastava et al., 2014) where the predictions are averaged over the different NNs resulting from different dropoutmasks. Inspired by this, Lakshminarayanan et al. (2017) proposed to use an simple NN ensemble to quantify the prediction uncertainty, i.e. to train a set of independent NNs using a proper scoring rule and defining the final prediction as the arithmetic mean of the outputs of the individual models, which corresponds to defining a uniformly-weighted mixture model. It is argued, that the model is able to encode two sources of uncertainty by calibrating the prediction uncertainty in each component and capturing the "model uncertainty" by averaging over the components. Note, that this should hold for any kind of predictive model that is defined in terms of a mixture distribution.
In this paper, we therefore aim at further investigating the potential that lies in using mixture distributions for uncertainty quantification. The flexibility of a mixture model can be increased by learning input-conditioned mixture weights, like it is done by mixture density networks (MDN) (Bishop,
1

Under review as a conference paper at ICLR 2019

1994). Furthermore, one can consider uncountably many component distributions instead of a finite set, which turns the mixture distribution into a compound distribution. We combine both by deriving the continuous counterpart of MDNs, which we call compound density networks (CDNs). This model corresponds to a compound distribution in which both, component and mixing distribution, are parametrized based on NNs. We experimentally show that CDNs allow for better uncertainty quantification and are more robust to adversarial examples than previous approaches.
This paper is organized as follows. In Section 2 we give a brief introduction to MDNs. We then formally define CDNs in Section 3. We review related work in Section 4 and present a detailed experimental analysis in Section 5. Finally we conclude our paper in Section 6.

2 MIXTURE DENSITY NETWORKS

Let D = {xn, yn}nN=1 be a i.i.d dataset and let us define the following conditional mixture model

K

p(y|x) = p (y|k(x)) p(k(x)|(x)) ,

(1)

k=1

and an NN that maps x onto both the parameters (x) of the mixing distribution and the parameters {k(x)}Kk=1 of the K mixture components. The complete system is called mixture density network (MDN) and was proposed by Bishop (1994). That is, an MDN is an NN parametrizing a conditional
mixture distribution, where both the mixture components and the mixture coefficients depend on input x.1 MDNs can be trained by maximizing the log-likelihood of the parameters of the NN given
the training set D using gradient-based optimizers such as stochastic gradient descent (SGD) and its
variants.

MDNs belong to a broader class of models called mixture of experts (MoE) (Jacobs et al., 1991) which differ from standard mixture models by assuming that the mixture coefficients depend on the input.2 Because of its formulation as a mixture distribution, the predictive distribution of an MDN can handle multimodality better than a standard discriminative neural network. Furthermore, the mixture coefficients allow us to encode uncertainty about the prediction, namely by modelling the probability from which component distribution a data point was sampled.

3 COMPOUND DENSITY NETWORKS

We aim at generalizing the MDN from a finite mixture distribution to a mixture of an uncountable set of components. The continuous counterpart of a conditional mixture distribution in eq. (1) is given by the conditional compound probability distribution

p(y|x) = p(y|(x))p((x)|(x)) d(x) ,

(2)

where (x) turns from a discrete into a continuous random variable.
We now want to follow the approach of MDNs to model the parameters of the components and the mixing distribution by NNs. To handle the continuous state space of (x) in the case of a compound distributions, the key idea is now to let (x) be given by a stochastic NN f (x; ) = (x) with stochastic parameters . Since given x, f is a deterministic map from  to (x), it is possible to replace the mixing distribution p((x)|(x)) = p(f (x; )|(x)) by a distribution p(|(x)) over . We further assume, that the parameters (x) of the mixing distribution are given by some parametrized function g(x; ) = (x) which can also be modelled based on NNs. In correspondence to MDNs, the complete system is called compound density network (CDN) and it is summarized by the following equation

p(y|x; ) = p(y|f (x; ))p(|g(x; )) d = Ep(|g(x;))[ p(y|f (x; ))] .

(3)

1For instance, as in the original publication, the mixture components could be K Gaussians, with k(x) being input-specific means and variances, and the mixture probabilities could be given by (applying the softmax function to the unnormalized) mixing weights (x), both computed by one NN.
2See (Bishop, 2006, ch. 5.6 and ch. 14.5.3) and (Murphy, 2012, ch. 11.2.4) for a detailed discussion of
MDNs.

2

Under review as a conference paper at ICLR 2019

xh

(x)

W1  p(W1|1(x))

W2  p(W2|2(h))

1(x) = g1(x; 1)

2(h) = g2(h; 2)

Figure 1: An example of a probabilistic hypernetwork applied to a two-layer MLP.

As for MDNs, CDNs training corresponds to maximizing its likelihood function, which is given by

N

log p(D|) = log Ep(|g(xn;))[ p(yn|f (xn; )] .
n=1

(4)

As we now have to deal with an integral instead of a finite sum, this log-likelihood function is intractable. Fortunately, applying the reparametrization trick (Kingma & Welling, 2014) to   p(|g(x; )) enables Monte Carlo integration, while still being able to optimize the log-likelihood
via backpropagation in conjunction with SGD.

To avoid overfitting, we can regularize the mixing distribution p(|g(x; )) by penalizing its KLdivergence w.r.t. some simple distribution p()3, leading to the regularized objective function

N
L() = log p(D|) -  DKL[ p(|g(xn; )) p()] ,

(5)

n=1

where  controls the regularization strength. We summarize the training procedure of CDNs in
the pseudo-code provided in Appendix A. Note, that CDNs correspond to an abstract framework
for modelling compound distributions with NNs. In the following section, we present a concrete example on how CDNs can be implemented.4

3.1 PROBABILISTIC HYPERNETWORKS

Ha et al. (2017) proposed to (deterministically) generate the parameters of an NN by another NN, which they call the hypernetwork. 5 We would like to follow this approach for modelling a CDN, that is, we aim at modelling the mixing distribution p(|g(x; )) over network parameters by NNs.
Since now the hypernetworks maps x to a distribution over parameters instead of a specific value ,
we refer to them as probabilistic hypernetworks. In the following we will describe this idea in more
detail.

Let f be a multi-layer perceptron6 (MLP) with L-layers, parametrized by a set of layers' weight
matrices7  = {Wl}Ll=1, that computes the parameters (x) = f (x; ) of the CDNs component distribution in eq. (3). Let h1, . . . , hl-1 denote the states of the hidden layers, and let us define h0 = x, and hL = f (x; ). We now assume the weight matrices {Wl}lL=1 to be random variables and to be independent from each other given the state of the previous hidden layer. We define a
series of probabilistic hypernetworks g = {gl}lL=1 (parametrized by  = {l}lL=1), where gl maps hl-1 to the parameters of the distribution of Wl, and let the joint distribution over  be given by

L
p(|g(x; )) = p(Wl|gl(hl-1; l)) .
l=1

(6)

An illustration of a stochastic two-layer network f (x; ) computing (x), with parameters distributions given by probabilistic hypernetworks as defined in eq. (6) is given in Figure 1. To make

3Note, that another option is Lp regularization on . 4An alternative implementation corresponding to a form of adaptive dropout is described in Appendix B. 5Specifically, they propose to apply a hypernetwork to compute the weight matrix of a recurrent NN at each
time-step, given the current input and the previous hidden state. 6Note, that probabilistic hypernetworks can analogously be applied to model the distribution over parame-
ters of any other kind of network. 7We assume that the bias parameters are absorbed into the weight matrix.

3

Under review as a conference paper at ICLR 2019

the definition of our model complete, we need to define the concrete statistical model we pick for p(Wl|gl(hl-1; )). This is done in the following section.

3.2 PROBABILISTIC HYPERNETWORKS WITH MATRIX VARIATE NORMALS

A statistical model that was recently applied as the posterior over weight matrices in BNNs (Louizos & Welling, 2016; Sun et al., 2017; Zhang et al., 2018; Ritter et al., 2018) is the matrix variate normal (MVN) distribution (Gupta & Nagar, 1999). An MVN is parametrized by three matrices: a mean matrix M and two covariance factor matrices A and B. It is connected to the multivariate Gaussian by the following equivalence

X  MN (X; M, A, B)  vec(X)  N (vec(X); vec(M), A  B) ,

(7)

where vec(X) denotes the vectorization of X. Due to the Kronecker factorization of the covariance, an MVN requires less parameters compared to a multivariate Gaussian, which motivates us to use it as the distribution over weight matrices in this work. Furthermore, we assume that the covariance factor matrices are diagonal matrices, following Louizos & Welling (2016). That is, we choose the mixture distribution of the CDN to be

LL
p(|g(x; )) = MN (Wl|gl(hl-1; l)) = MN (Wl|Ml, diag(al), diag(bl)) ,
l=1 l=1

(8)

where gl maps the state hl-1 of the previous hidden layer onto the l-th MVN's parameters {Ml, al, bl}, defining the distribution over Wl.

In practice, the procedure of sampling  and computing f (x; ) = hL now can be described by

hl = (hl-1Wl) , where Wl  MN (Wl|gl(hl-l; l)) ,

(9)

for l = 1, . . . , L, where  is an arbitrary point-wise nonlinear activation function, which may differ for hidden and output layers.

Note that using the mixing distribution defined in eq. (8) allows us to apply the reparametrization

trick (Appendix C.1). For the KL-divergence-based regularization, a straight forward choice for

the simple distribution is p() =

L l=1

MN

(Wl|0,

I,

I),

which

corresponds

to

assuming

that

each

layer's weight matrix is standard matrix normal distributed. In this case, the KL-term can be com-

puted in closed form, as noted by Louizos & Welling (2016) and shown in Appendix C.2. Finally,

we use a vector-scaling parametrization similar to the one used by Ha et al. (2017) and Krueger et al.

(2017) for the mean matrices {Ml}lL=1, which we explain in detail in Appendix D.

4 RELATED WORK

Various approaches for quantifying predictive uncertainty in NNs have been proposed. Applying Bayesian inference to NNs, i.e. treating the network parameters as random variables and estimating the posterior distribution given the training data based on Bayes' theorem, results in Bayesian neural networks (BNNs) (MacKay, 1992; Neal, 1995; Graves, 2011; Blundell et al., 2015; Louizos & Welling, 2016; Sun et al., 2017; Louizos & Welling, 2017; Ritter et al., 2018; Zhang et al., 2018, etc). Since the true posterior distribution is intractable, BNNs are trained based on approximate inference methods such as variational inference (VI) (Peterson, 1987; Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015), Markov Chain Monte Carlo (MCMC) (Neal, 1995), or Laplace approximation (MacKay, 1992; Ritter et al., 2018). The final prediction is then given by the expectation of the network prediction (given the parameters) w.r.t. the approximate posterior distribution. Louizos & Welling (2016) proposed to train BNNs based on VI with an MVN as the approximate posterior of each weight matrix (leading to a model they refer to as variational matrix Gaussian (VMG)). Multiplicative normalizing flow (MNF) (Louizos & Welling, 2017) models the approximate posterior as a compound distribution, where the mixing density is given by a normalizing flow. Zhang et al. (2018) also use an MVN approximate posterior and apply approximate natural gradient (Amari, 1998) based maximization on the VI objective, which results in an algorithm called noisy K-FAC. Meanwhile, the Kronecker-factored Laplace approximation (KFLA) (Ritter et al., 2018) extends the classical Laplace approximation by using an MVN approximate posterior with tractable and efficiently computed covariance factors, based on the Fisher information matrix.

4

Under review as a conference paper at ICLR 2019
There have been several concurrent works (Krueger et al., 2017; Louizos & Welling, 2017; Pawlowski et al., 2017; Sheikh et al., 2017) applying hypernetworks (Ha et al., 2017) to model the posterior distribution over network parameters in BNNs. In contrast to CDNs, the hypernetworks in these approaches are used to transform random noise drawn from a simple distribution into a random variable with a complicated distribution. That is, they use hypernetworks to sample from an implicit distribution of the parameters, without explicitly specifying the statistical model (like the MVN in our case). Krueger et al. (2017) and Louizos & Welling (2017) use normalizing flows, while Pawlowski et al. (2017) and Sheikh et al. (2017) use arbitrary NNs as their hypernetworks. The approach by Sheikh et al. (2017) is the one most closely related to ours, since they use an objective analogous to eq. (4). Note, that the main difference between CDNs and these hypernet-BNNs is that the approximate posterior in a Bayesian setting does not depend on the current input point, while the mixing distribution of the CDN does.
Gal & Ghahramani (2016) developed a theoretical framework that relates dropout training in NNs to approximate Bayesian inference and, as a result, proposed to approximate the predictive distribution by an average over the different networks resulting from independently sampled dropout-masks, a technique which they referred to as MC-dropout and which they applied to estimate the prediction uncertainty in NNs. Recently, Lakshminarayanan et al. (2017), proposed to use an ensemble of NNs in conjunction with a proper scoring rule and adversarial training to quantify the prediction uncertainty of deep NNs, leading to a model referred to as Deep Ensemble. The Deep Ensemble provides a non-Bayesian way to quantify prediction uncertainty, and is in this sense related to the approaches of Guo et al. (2017) and Hendrycks & Gimpel (2017).
5 EXPERIMENTS
We consider several standard tasks in our experimental analysis: the 1D toy regression problem introduced by Herna´ndez-Lobato & Adams (2015) (Section 5.1), classification under out-ofdistribution data (Section 5.2), and detection of and defense against adversarial examples (Szegedy et al., 2014) (Section 5.3). We consider the following recent models (described in section 4) as the baselines for our CDN model: MNF, KFLA, noisy K-FAC, MC-dropout, and Deep Ensemble.8
For the methods trained with VI, i.e. MNF and noisy K-FAC, we perform a weighting of the KLterm, in analogy to eq. (5), where  denotes the weighting parameter. For other methods, e.g. Deep Ensemble and MC-dropout, we perform regularization by weight decay and let  denote the corresponding hyperparameter. In all of the experiments, we consider   {10-n}n5=1, and pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy. The hyperparameter  for KFLA is chosen from {1, 10, 20, 30, 40}. More details about the hyperparameter search can be found in Appendix F. For BNNs and the CDN we estimate the predictive distribution p(y|x), based on 100 samples of network parameters from the posterior distribution or the mixing distribution, respectively. For the Deep Ensemble, the number of the mixture components is set to 5 and the adversarial perturbation strength to 1% of the input range, as recommended by Lakshminarayanan et al. (2017). We use Adam (Kingma & Ba, 2015) with default hyperparameters for optimization in all experiments and the implementations provided by Louizos & Welling (2017)9 and Zhang et al. (2018)10 for MNF and noisy K-FAC, respectively. The implementation of all models and experiments will be made available to the public once the review process is concluded.
5.1 TOY REGRESSION
The training data for the toy regression task is generated following Herna´ndez-Lobato & Adams (2015): we sample 20 input points x  U[-4, 4] and their target values y = x3 + , where  N (0, 32). For all investigated models, we use an MLP with size 1-100-1 as the predictive network. For the CDN, we use hypernetworks (g1 and g2) with 10 hidden units each.
8We also investigated the VMG, an MoE, and an MDN. Due to space restrictions, results for these methods are reported in Appendix E.
9https://github.com/AMLab-Amsterdam/MNF_VBNN 10https://github.com/gd-zhang/noisy-K-FAC
5

Under review as a conference paper at ICLR 2019

100 80 60 40 20 0 20 40 60
4 3 2 10 1 2 3 4
(a) CDN

80 60 40 20 0 20 40 60
4 3 2 10 1 2 3 4
(b) MNF

80 60 40 20 0 20 40 60
4 3 2 10 1 2 3 4
(c) KFLA

60 40 20 0 20 40 60
4 3 2 10 1 2 3
(d) noisy K-FAC

4

60 40 20 0 20 40 60
4 3 2 10 1 2 3 4
(e) Deep Ensemble

80 60 40 20 0 20 40 60
4 3 2 10 1 2 3
(f) MC-dropout

4

Figure 2: Comparison of the predictive distributions given by the CDN and the baselines. Black lines corresponds to the true noiseless function, red dots correspond to samples, orange lines and shaded regions correspond to the empirical mean and the ±3 standard deviation of the predictive distribution, respectively.

The results are presented in Figure 2. Our model gives a significantly better approximation of the true function than all the baselines and as desired expresses higher uncertainty in regions with less training data and more confidence in more densely sampled regions.
5.2 OUT-OF-DISTRIBUTION CLASSIFICATION
Following Lakshminarayanan et al. (2017), we train all models on the MNIST training set and investigate their performance on the MNIST test set and the notMNIST dataset11, which contains images (of the same size and format as MNIST) of letters from the alphabet instead of handwritten digits. On such an out-of-distribution test set, the predictive distribution of an ideal model should have maximum entropy, i.e. it should have a value of ln 10  2.303 which would be achieved if all ten classes are equally probable. The NN used for this experiment is an MLP with a 784-100-10 architecture.

Empirical CDF Empirical CDF

CDN MNF 1.0

KFLA noisy K-FAC

0.8

0.6

0.4

0.2

0.0 0.0

0.5 1.0 1.5 Predictive uncertainty
(a) MNIST

MC-Dropout DeepEnsemble
2.0 2.5

CDN MNF 1.0

KFLA noisy K-FAC

0.8

0.6

0.4

0.2

0.0 0.0

0.5 1.0 1.5 Predictive uncertainty
(b) notMNIST

Deep Ensemble MC-Dropout
2.0 2.5

Figure 3: CDFs of the empirical entropy of the predictive distribution of the models, where the y-axis denotes the fraction of predictions having entropy less than the corresponding value on the xaxis. Confident models should have its CDF close to the top-left corner of the figure, while uncertain models to the bottom-right.

11Available at http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html.

6

Under review as a conference paper at ICLR 2019

We present the results in Figure 3, where we plotted the cumulative distribution function (CDF) of the empirical entropy of the predictive distribution, following Louizos & Welling (2017). A CDF curve close to the top-left corner of the figure implies that the model yields mostly low entropy predictions, indicating that the model is very confident. While one wishes to observe high confidence on data points similar to those seen during training, the model should express uncertainty when exposed to out-of-distribution data. That is, we prefer a model to have a CDF curve closer to the bottom-right corner on notMNIST, as this implies it makes mostly uncertain (high entropy) predictions, and a curve closer to the upper-left corner for MNIST. As the results show, our model has very high confidence on the test set of MNIST while having the lowest confidence on notMNIST compared to all baseline models.
5.3 ADVERSARIAL EXAMPLES
To investigate the robustness and detection performance of the CDN w.r.t. adversarial examples (Szegedy et al., 2014), we apply the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) to a 10% fraction (i.e. 1000 samples) of the MNIST and CIFAR-10 test set. We do so, by making use of the implementation provided by Cleverhans (Papernot et al., 2018). We use the same MLP (with 784-100-10 architecture) as before for experiments on MNIST, and the LeNet5 convolutional network (LeCun et al., 1998) for experiments on CIFAR-10. Out of implementation reasons, we only model the parameters of the fully-connected layers of LeNet5 as random variables for CDN and KFLA. The probabilistic hypernetworks are two-layer MLPs with 100 hidden units. Note, that we do not use adversarial training when training the Deep Ensemble in this experiment to allow for a fair comparison.

Accuracy Average predictive entropy

CDN MNF 1.0

KFLA noisy K-FAC

Deep Ensemble MC-Dropout

0.8

0.6

0.4

0.2

0.0 0.0

0.2 0.4 0.6 Perturbation strength
(a) Accuracy

0.8

1.0

CDN MNF 2.5

KFLA noisy K-FAC

Deep Ensemble MC-Dropout

2.0

1.5

1.0

0.5

0.0 0.0

0.2 0.4 0.6 Perturbation strength
(b) Entropy

0.8

1.0

Figure 4: Prediction accuracy and average entropy of models trained on MNIST when attacked by FGSM-based adversarial examples (Goodfellow et al., 2015) with varying perturbation strength.

MNIST Figure 4 presents the accuracy and the average empirical entropy of the predictive distribution w.r.t. adversarial examples for MNIST with varying levels of perturbation strength (between 0 and 1). For all models we chose the hyperparameters that yield the highest uncertainty, while leading to an accuracy > 0.95 on the original test set. We observe that the CDN is significantly more robust to adversarial examples than all baseline models. It results in a prediction accuracy around 0.4 even under high perturbation strength. The accuracy of the baseline models is significantly worse for all investigated regularization hyperparameters, see Appendix F. The prediction entropy for adversarial examples is also higher for the CDN than for most other models (only MC-dropout shows higher uncertainty in the beginning). Furthermore, with a good choice of regularization parameter the prediction uncertainty of the CDN is steadily increases with increasing perturbation strength.
We demonstrate how the performance of our model depends on the choice of regularization strength  in Figure 5. It is clearly visible that  acts as a hyperparameter that balances the trades-off between detection (uncertainty) and robustness (accuracy). When allowing for lower uncertainty, the CDN gets surprisingly robust, showing a prediction accuracy of over 0.8 even for a perturbation strength of 1. Even when we increase the strength of the adversarial examples by computing them based on 10 different samples from our model, the CDN is still significantly more robust than all baseline models w.r.t. the weaker adversarial examples relying on a single sample.

7

Under review as a conference paper at ICLR 2019

Accuracy or % of max entropy Accuracy or % of max entropy

= 0.01 1.0

= 0.001

= 0.0001

0.8

0.6

0.4

0.2

0.0 0.0

0.2 0.4 0.6 Perturbation strength
(a) 1 sample

0.8

1.0

= 0.01 1.0

= 0.001

= 0.0001

0.8

0.6

0.4

0.2

0.0 0.0

0.2 0.4 0.6 Perturbation strength
(b) 10 samples

0.8

1.0

Figure 5: Accuracy and average entropy of our proposed model under FGSM attack. We use a single sample of adversarial example in the left figure, and use the average of 10 adversarial examples to take into account the stochasticity of our model in the right figure. Circle indicates accuracy, while cross indicates entropy. The y-axis represents both the accuracy and the relative entropy to the maximum entropy (i.e. ln 10).

Accuracy Average predictive entropy

CDN MNF 1.0

KFLA noisy K-FAC

MC-Dropout

0.8

0.6

0.4

0.2

0.0 0.0

0.2 0.4 0.6 Perturbation strength
(a) Accuracy

0.8

1.0

CDN MNF 2.5

KFLA noisy K-FAC

MC-Dropout

2.0

1.5

1.0

0.5

0.0 0.0

0.2 0.4 0.6 Perturbation strength
(b) Entropy

0.8

1.0

Figure 6: Prediction accuracy and average entropy of models trained on CIFAR-10 when attacked by FGSM-based adversarial examples with varying perturbation strength.

CIFAR-10 Figure 6 shows that, leading to second best results w.r.t. accuracy as well as uncertainty, our model is competitive to other state of the art models on CIFAR-10. Note, that for the CDN we do not use probabilistic hypernetworks for the convolution layers of LeNet5, i.e. we only treat the parameters of the fully-connected layers as random variables. Treating all parameters as random variables, as it is done by the BNNs in this experiment (except from KFLA) could potentially improve the performance of the CDN.
6 CONCLUSION
We introduce compound density networks (CDNs), a new framework that allows for better uncertainty quantification on neural network (NN), and corresponds to a compound distribution (i.e. mixture with uncountable components) in which both the component distribution and the mixing distribution are parametrized by NNs. CDNs are inspired by the success of recently proposed ensemble methods and represent a continuous generalization of mixture density networks (MDNs) (Bishop, 1994). They can be implemented by using a hypernetwork to map the input to a distribution over the parameters of the target NN, that models a predictive distribution. An extensive experimental analysis showed that CDNs are able to produce promising results in terms of uncertainty quantification. Especially, when facing FGSM-based adversarial attacks, the predictions of our model are significantly more robust than those of previous models, while simultaneously providing a better chance of detecting the attack by showing increased uncertainty.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251­ 276, 1998.
Jimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Advances in Neural Information Processing Systems, pp. 3084­3092, 2013.
Christopher M Bishop. Mixture density networks. 1994.
Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. ISBN 9780387-31073-2.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. pp. 1613­1622, 2015.
Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493­2537, 2011.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050­1059, 2016.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.
Alex Graves. Practical Variational Inference for Neural Networks. In Advances in Neural Information Processing Systems 24, pp. 2348­2356. 2011.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 1321­1330, 06­11 Aug 2017.
Arjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 1999.
David Ha, Andrew Dai, and Quoc V. Le. HyperNetworks. In Proceedings of the Second International Conference on Learning Representations (ICLR 2017), 2017.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proceedings of International Conference on Learning Representations, 2017.
Jose´ Miguel Herna´ndez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In International Conference on Machine Learning, pp. 1861­ 1869, 2015.
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pp. 5­13. ACM, 1993.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79­87, 1991.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference for Learning Representations, 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the Second International Conference on Learning Representations (ICLR 2014), April 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
9

Under review as a conference paper at ICLR 2019
David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron Courville. Bayesian Hypernetworks. arXiv:1710.04759 [cs, stat], October 2017. arXiv: 1710.04759.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pp. 6402­6413, 2017.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix gaussian posteriors. In International Conference on Machine Learning, pp. 1708­1716, 2016.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational Bayesian neural networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 2218­ 2227, 2017.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computation, 4(3):448­472, 1992.
Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012. ISBN 0262018020, 9780262018029.
Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD thesis, University of Toronto, 1995.
Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, and Rujun Long. Technical report on the cleverhans v2.1.0 adversarial examples library. arXiv preprint arXiv:1610.00768, 2018.
Nick Pawlowski, Andrew Brock, Matthew C. H. Lee, Martin Rajchl, and Ben Glocker. Implicit Weight Uncertainty in Neural Networks. arXiv:1711.01297 [cs, stat], November 2017. arXiv: 1711.01297.
Carsten Peterson. A mean field theory learning algorithm for neural networks. Complex systems, 1: 995­1019, 1987.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural networks. In International Conference on Learning Representations, 2018.
Abdul-Saboor Sheikh, Kashif Rasul, Andreas Merentitis, and Urs Bergmann. Stochastic maximum likelihood optimization via hypernetworks. In Advances in Neural Information Processing Systems, 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning structured weight uncertainty in bayesian neural networks. In Artificial Intelligence and Statistics, pp. 1283­1292, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. 2014.
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as variational inference. In Proceedings of the 35th International Conference on Machine Learning, pp. 5852­5861, 2018.
10

Under review as a conference paper at ICLR 2019

APPENDIX A PSEUDOCODE FOR TRAINING CDNS

Algorithm 1 The training procedure of CDN.
Require: Mini-batch size M , number of samples S used for Monte Carlo integration (eq. (4)), regularization strength , and learning rate .

1: while the stopping criterion is not satisfied do

2: {xm, ym}mM=1  D 3: for m = 1, . . . , M ; s = 1, . . . , S do

4: ms  p(|g(xm; ))

5: s(xm) = f (xm; ms)

6: end for

7:

log p(D|) =

M m=1

log

1 S

S s=1

p(ym|s(xm))

8:

DKL =

M m=1

DKL

[

p(|g(xm

;

))

p())]

9: L() = log p(D|) - DKL

10:    + L()

11: end while

Sample mini-batch from dataset Use reparametrization trick
Eq. 4 with Monte Carlo integration
Update parameters 

APPENDIX B CDNS WITH LATENT VARIABLES
In Section 3, we introduce CDNs as a compound distribution where the stochasticity of f (x; ) comes from treating  as random variables. Another approach to achieve stochasticity on f is by introducing latent variable z and letting the parameter  to be fixed (learned with MLE). The CDN would then be descibed by
p(y|x) = p(y|x, z; )p(z|x)dz

:= p(y|f (x; z, ))p(z|g(x; ))dz

= Ep(z|g(x;))[ p(y|f (x; z, ))] .

(10)

The log-likelihood function is therefore

N

log p(D|, ) = log Ep(z|g(xn;))[ p(yn|f (xn; z, ))] ,
n=1
and the objective function is

(11)

L(, ) = log p(D|, ) - DKL[p(z|g(xn; )) p(z)] .

(12)

The training algorithm in Algorithm 1 only needs to be trivially modified to accommodate , i.e. in the gradient steps. In the following section, we present an instance of this framework.

B.1 A VARIANT OF ADAPTIVE DROPOUT

This model relies on injecting noise into the hidden activations of the neural network f , similar to dropout (Srivastava et al., 2014). However, in this model, we condition the dropout's Gaussian on the activations of the previous hidden layer, similar to adaptive Bernoulli dropout proposed by Ba & Frey (2013).

Let zl  p(zl|gl(hl-1; l)) be the noise vector of layer l. We then multiplicatively apply this noise

to the activations hl, i.e. hl zl, where denotes the Hadamard product. Let z := {zl}lL=-11 and let

g(x; ) := {gl(hl-1; l)}lL=1. Let p(z|g(x; )) =:

L-1 l=1

p(zl|g(hl;

l))

be

the

mixing

distribu-

tion. Each of the p(zl|g(hl; l)) can be any distribution as long as we can apply reparametrization

trick. Then, we can immediately apply latent CDNs training procedure on this model. One example

of p(zl|gl(hl-1; l)) is N (zl|1, gl(hl-1; l)), which then the model resembles Gaussian dropout

(Srivastava et al., 2014) with adaptive scaling.

11

Under review as a conference paper at ICLR 2019

APPENDIX C DETAILS ABOUT THE MATRIX VARIATE NORMAL (MVN)
Let X  Rr×c and p(X) := MN (X|M, diag(a), diag(b)). Taken from Louizos & Welling (2016), the procedure to sample from p(X) using reparametrization trick (Kingma & Welling, 2014) and the closed-form expression of the KL-divergence to MN (0, I, I) are presented in the following sections.

C.1 REPARAMETRIZATION TRICK

Let E  Rr,c. Sampling p(X) can be done by

E  MN (0, I, I)  ij  N (0, 1)  i = 1, . . . , r  j = 1, . . . , c

X

=

M

+

diag(a)

1 2

E

diag(b)

1 2

(13) (14)

C.2 KL-DIVERGENCE TO STANDARD MVN

Let Ir  Rr×r, Ic  Rc×c be identity matrices, the KL-divergence between p(X) and MN (X|0, I, I) is given by



DKL[ p(X)

MN (0, I, I)] =

1 2

c rc

ai

bj +

M

2 F

- rc - c

log ai - r

log bj .

i=r j=1

i=1 j=1

(15)

APPENDIX D VECTOR SCALING PARAMETRIZATION

The naive formulation of gl can be very expensive in term of number of parameters. Suppose Wl  Rr×c and gl is a two layer MLP with k hidden units. Then gl would have rk + krc + kr + kc many parameters, which quickly becomes very large for a moderately sized NNs. The majority of
the parameters are needed to define the mean matrix Ml. Following the approach of Ha et al. (2017) and Krueger et al. (2017), we make a trade-off between expressiveness of gl on the mean matrix with the number of parameter by instead replacing Ml with a matrix Vl of the same size and a vector dl  Rr, which is the output of gl. Thus, now gl maps hl-1 - {dl, al, bl} and we can get Ml by

dfl1vl1

Mlf

=

dlf2vl2

 



 ... 

.



dflr vlr

(16)

That is, each element of dl is being used to scale the corresponding row of Vl. Note that although Vl is a parameter matrix with the same size of Ml, it crucially is not an output of gl as in the naive parametrization. Thus the number of parameter of gl is now rk + rc + 2kr + kc, which is more manageable and implementable for larger weight matrices.

APPENDIX E EXPERIMENTAL RESULTS FOR ADDITIONAL MODELS
In this section we compare the CDN descibed in the main text with the CDN proposed in Appendix B.1 (CDN-dropout), the BNN proposed by Louizos & Welling (2016) (VMG), a mixture of experts (MoE) and an MDN on the MNIST dataset. For both MDN and MoE, we use two-layer MLP with 100 hidden units, with 5 mixture components. Specifically for MoE, the mixing distribution is also given by another NN of the same architecture. The results for the out-of-distribution prediction and adversarial attack experiments are presented in Figure 7 and Figure 8, respectively. Note that the analysis presented in the main text applies here as well.

12

Under review as a conference paper at ICLR 2019

Prob. hypernets CDN-dropout 1.0

MVG MDN

0.8

0.6

0.4

0.2

0.0 0.0

0.5 1.0 1.5
(a) MNIST

MoE 2.0 2.5

Prob. hypernets CDN-dropout 1.0

MVG MDN

0.8

0.6

0.4

0.2

0.0 0.0

0.5 1.0 1.5
(b) notMNIST

MoE 2.0 2.5

Figure 7: CDF of the prediction entropy on MNIST and notMNIST test set of additional models.

Accuracy Average predictive entropy

Prob. hypernets 1.0

CDN-dropout

0.8

0.6

0.4

0.2

0.0 0.0

0.2 0.4 0.6 Perturbation strength
(a) Accuracy

0.8

VMG 1.0

Prob. hypernets 2.5

CDN-dropout

2.0

1.5

1.0

0.5

0.0 0.0

0.2 0.4 0.6 Perturbation strength
(b) Entropy

0.8

VMG 1.0

Figure 8: Prediction accuracy and average entropy of additional models trained on MNIST when attacked by FGSM-based adversarial examples (Goodfellow et al., 2015) with varying perturbation strength.

APPENDIX F HYPERPARAMETER SEARCH

We do hyperparameter search on  (or  for KFLA) on all baselines based on the accuracy, uncertainty on MNIST, and uncertainty on notMNIST. The criteria are as follows: we pick an instance of a model with accuracy > 0.95, with qualitatively low uncertainty on MNIST, and with qualitatively high uncertainty on notMNIST.


0.1 0.01 0.001 0.0001 0.00001

CDN
0.21 0.969 0.977 0.976 0.977

MNF
0.982 0.981 0.98 0.981 0.979

n. K-FAC
0.976 0.981 0.981 0.979 0.979

Deep Ens.
0.891 0.96 0.982 0.985 0.984

MC-dropout
0.84 0.931 0.971 0.978 0.978

MVG
0.098 0.299 0.89 0.936 0.952

CDN-dropout
0.971 0.973 0.978 0.978 0.977

Table 1: Test accuracy on MNIST vs . MVG is the BNN proposed by Louizos & Welling (2016), while CDN-dropout is the model proposed in Appendix B.

13

Under review as a conference paper at ICLR 2019

 KFLA
1 0.812 10 0.978 20 0.978 30 0.978 40 0.978
Table 2: Test accuracy on MNIST vs  for KFLA.

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

0.0 0.0

0.5

= 0.001 = 0.0001 1.0 1.5

= 1e-05

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

2.0 2.5 0.0 0.0 0.5
(a) CDN

= 0.001 = 0.0001 1.0 1.5

= 1e-05 2.0 2.5

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

0.0 0.0

0.5

= 0.001 = 0.0001 1.0 1.5

= 1e-05

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

2.0 2.5 0.0 0.0 0.5
(b) MNF

= 0.001 = 0.0001 1.0 1.5

= 1e-05 2.0 2.5

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

0.0 0.0

0.5

= 0.001 = 0.0001 1.0 1.5

= 1e-05

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

2.0 2.5 0.0 0.0
(c) noisy K-FAC

0.5

= 0.001 = 0.0001 1.0 1.5

= 1e-05 2.0 2.5

Figure 9: Empirical CDF curve of uncertainty on MNIST (left) and notMNIST (right)

14

Under review as a conference paper at ICLR 2019

=1 = 10 1.0

0.8

0.6

0.4

0.2

0.0 0.0

0.5

= 20 = 30 1.0 1.5

= 40 = 1 = 10
1.0

0.8

0.6

0.4

0.2

2.0 2.5 0.0 0.0
(d) KFLA

0.5

= 20 = 30 1.0 1.5

= 40 2.0 2.5

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

0.0 0.0

0.5

= 0.001 = 0.0001 1.0 1.5

= 1e-05

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

2.0 2.5 0.0 0.0
(e) Deep Ensemble

0.5

= 0.001 = 0.0001 1.0 1.5

= 1e-05 2.0 2.5

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

0.0 0.0

0.5

= 0.001 = 0.0001 1.0 1.5

= 1e-05

= 0.1 = 0.01 1.0

0.8

0.6

0.4

0.2

2.0 2.5 0.0 0.0
(f) MC-dropout

0.5

= 0.001 = 0.0001 1.0 1.5

= 1e-05 2.0 2.5

Figure 9: Empirical CDF curve of uncertainty on MNIST (left) and notMNIST (right) (continued).

15

Under review as a conference paper at ICLR 2019

Accuracy

Accuracy

= 0.1 = 0.01 1.0

= 0.001 = 0.0001

= 0.00001

= 0.1 = 0.01 2.5

= 0.001 = 0.0001

= 0.00001

0.8 2.0

Predictive entropy

0.6 1.5

0.4 1.0

0.2 0.5

0.0 0.0

0.2 Per0tu.4rbation str0e.n6gth 0.8

1.0 0.0 0.0
(a) CDN

0.2 Per0tu.4rbation str0e.n6gth 0.8

1.0

= 0.1 = 0.01 1.0

= 0.001 = 0.0001

= 0.00001 = annealed

= 0.1 = 0.01 2.5

= 0.001 = 0.0001

= 0.00001 = annealed

0.8 2.0

Predictive entropy

0.6 1.5

0.4 1.0

0.2 0.5

0.0 0.0

0.2 0.4 0.6 0.8 Perturbation strength

1.0 0.0 0.0
(b) MNF

0.2 0.4 0.6 0.8 Perturbation strength

1.0

= 0.1 = 0.01 1.0

= 0.001 = 0.0001

= 0.00001

= 0.1 = 0.01 2.5

= 0.001 = 0.0001

= 0.00001

0.8 2.0

Predictive entropy

0.6 1.5

0.4 1.0

0.2 0.5

0.0 0.0

0.2 0.4 0.6 0.8 Perturbation strength

1.0

0.0 0.0

(c) noisy K-FAC

0.2 0.4 0.6 0.8 Perturbation strength

1.0

=1 = 10 1.0

= 20 = 30

0.8

0.6

0.4

0.2

0.0 0.0

0.2 0.4 0.6 0.8 Perturbation strength

= 40 = 1 = 10
2.5

= 20 = 30

2.0

Predictive entropy

1.5

1.0

0.5

1.0 0.0 0.0
(d) KFLA

0.2 0.4 0.6 0.8 Perturbation strength

= 40 1.0

Figure 10: Accuracy (left) and uncertainty (right) w.r.t. adversarial examples.

16

Accuracy

Accuracy

Under review as a conference paper at ICLR 2019

Accuracy

= 0.1 = 0.01 1.0

= 0.001 = 0.0001

= 0.00001

= 0.1 = 0.01 2.5

= 0.001 = 0.0001

= 0.00001

0.8 2.0

Predictive entropy

0.6 1.5

0.4 1.0

0.2 0.5

0.0 0.0

0.2 0.4 0.6 0.8 Perturbation strength

1.0

0.0 0.0

(e) Deep Ensemble

0.2 0.4 0.6 0.8 Perturbation strength

1.0

Accuracy

= 0.1 = 0.01 1.0

= 0.001 = 0.0001

= 0.00001

= 0.1 = 0.01 2.5

= 0.001 = 0.0001

= 0.00001

0.8 2.0

Predictive entropy

0.6 1.5

0.4 1.0

0.2 0.5

0.0 0.0

0.2 Per0tu.4rbation str0e.n6gth 0.8

1.0

0.0 0.0

(f) MC-dropout

0.2 Per0tu.4rbation str0e.n6gth 0.8

1.0

Figure 10: Accuracy (left) and uncertainty (right) w.r.t. adversarial examples (continued).

17

