Under review as a conference paper at ICLR 2019
CAN I TRUST YOU MORE? MODEL-AGNOSTIC HIERARCHICAL EXPLANATIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Interactions such as double negation in sentences and scene interactions in images are common forms of complex dependencies captured by state-of-the-art machine learning models. We propose Mahe´, a novel approach to provide Model-agnostic hierarchical e´xplanations of how powerful machine learning models, such as deep neural networks, capture these dependencies, either context-dependent or contextfree. Specifically, Mahe´ provides context-dependent explanations by a novel local interpretation algorithm that effectively captures any-order interactions, and obtains context-free explanations through generalizing context-dependent interactions to explain global behaviors. Experimental results show that Mah´e obtains improved local interaction interpretations over state-of-the-art methods and successfully provides explanations of interactions that are context-free.
1 INTRODUCTION
State-of-the-art machine learning models, such as deep neural networks, are exceptional at modeling complex dependencies in structured data, such as text (Vaswani et al., 2017; Tai et al., 2015), images (He et al., 2016; Huang et al., 2017), and DNA sequences (Alipanahi et al., 2015; Zeng et al., 2016). However, there has been no clear explanation on what type of dependencies are captured in the black-box models that perform so well (Ribeiro et al., 2018; Murdoch et al., 2018).
In this paper, we make one of the first attempts at solving this important problem through interpreting two forms of structures, i.e., context-dependent representations and context-free representations. A context-dependent representation is the one in which a model's prediction depends specifically on a data instance level (such as a sentence or an image). In order to illustrate the concept, we consider an example in image analysis. A yellow round-shape object can be identified as the sun or the moon given its context, either bright blue sky or dark night. A context-free representation is one where the representation behaves similarly independent of instances (i.e., global behaviors). In a hypothetical task of classifying sentiment in sentences, each sentence carries very different meaning, but when "not" and "bad" depend on each other, their sentiment contribution is almost always positive - i.e., the structure is context-free.
To investigate context-dependent and context-free structure, we lend to existing definitions in interpretable machine learning (Ribeiro et al., 2016; Kim et al., 2018). A context-dependent interpretation is a local interpretation of the dependencies at or within the vicinity of a single data instance. Conversely, a context-free interpretation is a global interpretation of those dependencies and applies to entire classes of data. We formalize the notion of a dependency within context-dependent and -free representations as an interaction relationship between the prediction and input features. Interactions are appropriate because they can describe arbitrarily complex relationships between prediction and features, which are commonly captured by state-of-the-art models like deep neural networks (Tsang et al., 2017; Murdoch et al., 2018).
In this work, we propose Mah´e, a framework for explaining the context-dependent and contextfree structures of any complex prediction model, with a focus on explaining neural networks. The context-dependent explanations are built based on recent work on local intepretations (such as (Ribeiro et al., 2016; Murdoch et al., 2018; Singh et al., 2018)). Specifically, Mah´e takes as input a model to explain and a data instance, and returns a hierarchical explanation, a format proposed by Singh et al. (2018) to show local group-variable relationships used in predictions (Figure 1). To provide context-free explanations, Mah´e generalizes those context-dependent interactions with con-
1

Under review as a conference paper at ICLR 2019

Input into complex ML model:
this movie is not bad 1 2 3 4 5

4

{4, 5} are interacting

Mahe linear LIME

Fit Interaction () Ranking

Hierarchical Explanation

0.6 none (linear LIME) this movie is not bad

0.8 {4, 5} 0.9 {2, 4, 5} ......

not bad movie not bad ...

5 (not, bad)

Step 1

Step 2

Step 3

Figure 1: An overview of the steps used to obtain a context-dependent hierarchical explanation. Step 1 inputs a data instance of interest (e.g. a sentence) into a complex model, in this case a classifier. Step 2 locally perturbs the data instance and obtains their predictions results from the model. Instead of only fitting a linear model as linear LIME does to the perturbed samples and outputs, Mahe´ fits a neural network to them to learn the highly nonlinear decision boundary used to classify the instance. The nonlinearity indicates that there should be an interaction between variables, and an interpretation of the neural network is used to extract the interactions (Tsang et al., 2017). Attribution scores of those interactions can then be shown for the data instance, as displayed in Step 3.

sistent behavior in a model and determines whether a local representation in the model is responsible for the global behavior. In this case, Mah´e takes as input a model and representative data corresponding to an interaction of interest and returns whether or not that interaction is context-free. We conduct experiments on both synthetic datasets and real-world application datasets, which shows that Mah´e's context-dependent explanations can significantly outperform state-of-the-art methods for local interaction interpretation, and Mah´e is capable of successfully finding context-free explanations of interactions. In addition, we identify promising cases where the methodology for context-free explanations can successfully edit models. Our contributions are as follows: 1) Mahe´ achieves the task of improved context-dependent explanations based on performance and generality, compared to state-of-the-art methods for local interaction interpretation, 2) Mah´e is the first to provide context-free explanations of interactions in deep learning models, and 3) Mah´e provides a promising direction for modifying context-free interactions in deep learning models without significant performance degradation.
2 RELATED WORKS
Attribution Interpretability: A common form of interpretation is feature attribution, which is concerned with how features of a data instance contribute to a model output. Within this category, there are two distinct approaches: additive and sensitivity attribution. Additive attribution interprets how much each feature contributes to the model output when these contributions are summed. In contrast, sensitivity attribution interprets how sensitive a model output is to changes in features. Examples of additive attribution techniques include LIME (Ribeiro et al., 2016) and CD (Murdoch et al., 2018). Examples of sensitivity attribution methods include Integrated Gradients (Sundararajan et al., 2017), DeepLIFT (Shrikumar et al., 2017), and SmoothGrad (Smilkov et al., 2017). Unlike previous approaches, Mahe´ provides additive attribution interpretations that consist of non-additive groups of variables (interactions) in addition to the normal additive contributions of each variable.
Interaction Interpretability: An interaction in its generic form is a non-additive effect between features on an outcome variable. Only until recently has there been development in interpreting nonadditive interactions despite often being learned in complex machine learning models. The difficulty interpreting non-additive interactions stems from their lack of exact functional identity compared to, for example, a multiplicative interaction. Methods that exist to interpret non-additive interactions are NID (Tsang et al., 2017) and Additive Groves (Sorokina et al., 2008). In contrast, many more methods exist to interpret specific interactions, namely multiplicative ones. Notable methods include CD (Murdoch et al., 2018), Tree-Shap (Lundberg & Lee, 2017), and GLMs with multiplicative interactions (Purushotham et al., 2014). Unlike previous methods, our approach provides local interpretations of the more challenging non-additive interaction.
Locally Interpretable Model-Agnostic Explanations (LIME): LIME (Ribeiro et al., 2016) is a very popular type of model interpretation. Its popularity comes from additive attribution interpretations to explain the output of any prediction model. The original and most popular version of LIME uses a linear model to approximate model predictions in the local vicinity of a data instance. Since its introduction, variants of LIME have been proposed, for example Anchors (Ribeiro et al., 2018) and LIME-SUP (Hu et al., 2018). While Anchors generates a form of context-free explanation, its

2

Under review as a conference paper at ICLR 2019

method of selecting fully representative features for a prediction does not consider interactions. For example, Anchors assumes that (not, bad) "virtually guarentees" a sentiment prediction to be positive, whereas in Mah´e this is not necessarily true; only their interaction is positive (See Table 5 for an example). LIME-SUP touches upon interactions but does not study their interpretation.

3 INTERACTION EXPLANATIONS

3.1 INTERACTIONS IN ADDITIVE ATTRIBUTION EXPLANATIONS

Traditionally, additive attribution methods are based on linear functions (Lundberg & Lee, 2017). For an data instance x  Rp, weights w  Rp and bias b, attribution scores are given by wixi in

p

(x) = wixi + b

(1)

i=1

Given a model output to explain f (·) and a set of n data points {x(i)}ni=1 that are infinitesimally close or local to x, a linear approximation of D = {(x(1), f (x(1))), . . . , (x(n), f (x(n)))} will accurately fit to the functional surface of f at the data instance, such that (x) = f (x). Because it is possible in such scenarios that (x) = f (x)  b, there must be some nonzero distances between x and
x(i) to obtain informative attribution scores. LIME, as it was originally proposed, uses a linear
approximation as above where samples are generated in a nonzero local vicinity of x (Ribeiro et al., 2016). The drawback of linear LIME is that there is often an error = |f (x) - (x)| > 0.

For complex models f , the functional surface at x can be nonlinear. Because D consists of x(i)
with distance d > 0 from x, a closer fit to f (x) in its nonlinear vicinity, i.e. {f (x(i))}ni=1, can be achieved with the following generalization of Eq. 1:

p

(x) = gi(xi) + b,

(2)

i=1

where gi(·) can be any function, for example one that is arbitrarily nonlinear. This function is called a generalized additive model (GAM) (Hastie & Tibshirani, 1990), and now attribution scores can be given by gi(xi) for each feature i. For the purposes of interpreting individual feature attribution, the GAM may be enough. However, if we would like broader explanations, we can also obtain
non-additive attributions or interactions between variables (Lou et al., 2013), which provide an even
better fit to the complex local vicinity. Augmenting Eq. 2 with non-additive interactions yields:

pK
K (x) = gi(xi) + gi(xI ) + b,
i=1 i=1

(3)

where gi(·) is again any function, xI  R|I| are interacting variables corresponding to the variable indices I, and {Ii}Ki=1 is a set of K interactions. Attribution scores can now be obtained from each gi and gi. In this paper, we learn gi and gi using Multilayer Perceptrons (MLPs). Link functions can be used to convert  or K to classification.

4 Mahe´ FRAMEWORK
In this section, we introduce our Mah´e framework, which can provide context-dependent and context-free explanations of interactions. To provide context-dependent explanations, we propose to use a two-step procedure that first identifies what variables interact locally, then learns a model of interactions (as Eq. 3) to provide a local interaction score at the data instance in question. The procedure of first detecting interactions then building non-additive models for them has been studied previously (Lou et al., 2013; Tsang et al., 2017); however, previous works have not focused on using the same non-additive models to provide local interaction attribution scores, which enable us to visualize interactions of any size as demonstrated later in §5.2.3.

4.1 CONTEXT-DEPENDENT EXPLANATIONS
Local Interaction Detection: To perform interaction detection on samples local to data instance x, we first sample n points with mean x  Rp to generate x(1), . . . , x(n), x(i)  N (x, 2I) for all i = 1, . . . , n. In cases where data features are binary, sampled points consist of randomly

3

Under review as a conference paper at ICLR 2019
perturbed features from x, and distances from x are computed by a distance metric (e.g. cosine distance for text/images, 2 for continuous features) as done in previous perturbation explanation methods (Ribeiro et al., 2016; Lundberg & Lee, 2017; Ribeiro et al., 2018). Sample weights (e.g. the frequency each sample appears in the sampled dataset) are given by a kernel function of sample distances. We use a Gaussian kernel with stddev , truncating all samples beyond a 1 local vicinity.
For a model output f (·) to explain, our framework is flexible to any interaction detection method that applies to the dataset D = {(x(1), f (x(1))), . . . , (x(n), f (x(n)))} . Since we seek to detect non-additive interactions, we use the neural interaction detection (NID) framework (Tsang et al., 2017), which interprets learned neural network weights to obtain interactions. To the best of our knowledge, this detection method is the only polynomial-time algorithm that accurately ranks anyorder non-additive interactions after training one model, compared to alternative methods that must train an exponential number O(2p) of models. In addition, using NID carries intuition because a neural network can approximate any nonlinear interacting function (Hornik, 1991), and the local vicinity of f (x) (e.g. a deep net) can be an arbitrary form of such function.
Hierarchical Interaction Attributions: Upon obtaining an interaction ranking from NID, GAMs with interactions (Eq. 3) can be learned for different top-K interactions (Tsang et al., 2017). In the Mahe´ framework, there are L + 1 different levels of a hierarchical explanation which constitutes our context-dependent explanation. When presenting the hierarchy, the first level shows the additive attributions of individual features, such as the attributions provided by a trained (·) in Eqs. 1 or 2. Subsequently, the parameters w of (·; w, b) are frozen before interaction models are added to construct K(·) in Eq. 3. The next levels of the hierarchy can be presented either as the interaction attributions of {gi(·)}iK=1 or gK (·) alone (Eq. 3), where at each level K is increased and {gi(·)}Ki=1, bias b are retrained. The practice of training interaction models gi on the residual of  has been used previously to construct interpretable models (Lou et al., 2013). Since K is trained at each hierarchical level on D, the fit of each K can also be explained via predictive performance. The stopping criteria for the number of hierarchical levels can depend on the predictive performance or user preference.
4.2 CONTEXT-FREE EXPLANATIONS
In order to provide context-free explanations, we propose determining whether the local interactions assumed to be context-dependent in §4.1 can generalize to explain global behavior in f . To this end, we first define ideal conditions for which a generic local explanation can generalize.
Definition 1 (Generalizing Local Explanations). Let f (·) be the model output we wish to explain, and Xf be the data domain of f . Let a local explanation of f at x  Xf be some explanation E that is true for f (x) and depends on samples x  Xf that are only in the local vicinity of x, i.e. m(x, x )  d provided a distance metric m and distance d  0. The local explanation E is a global explanation if the following two conditions are met: 1) Explanation E is true for f at all data samples in Xf , including samples outside the local vicinity of x, i.e. all samples xg  Xf satisfying m(x, xg) > d. 2) There exists a sample x  Xf and a local modification to f (x ) (modifying f (x ) in the vicinity m(x , x )  d) that changes E for all samples in Xf while still meeting condition 1).
For example, consider a simple linear regression model we wish to explain, f (x) = w1x1 + w2x2. Let its local explanation be the feature attributions w1x1 and w2x2. This local explanation is a global explanation because 1) for all values of x1 and x2, the feature attributions are still w1x1 and w2x2, and 2) if any of the weights are changed, e.g. w1  w1, the attribution explanation will change, but the feature attributions are still w1x1 and w2x2 for all values of x1 and x2.
Our context-free explanation of interaction I is: whenever local interaction I exists, its attribution will in general have the same polarity (or sign). Since it is impossible to empirically prove that a local explanation is true for all data instances globally (via Definition 1), this work is focused on providing evidence of a context-free interaction representation. This evidence can be obtained by checking whether our explanation is consistent with the two conditions from Definition 1 for the interaction of interest I: 1) For representative data instances in the domain of f , if local interaction I exists, does it always have the same attribution polarity? The representative data instances be separated from each other at an average distance beyond d. 2) Can local interaction I at a single data instance x~ be used to negate I's attribution polarity for all representative data instances where I exists?
4

Under review as a conference paper at ICLR 2019

We propose to modify an interaction attribution of the model's output f (x) at data instance x by

utilizing a trained model gk(xI) of interaction Ik, where 1  k  K (Eq. 3). Let g~k(·) be a

modified version of gk(·). We can then define a modified form of Eq. 3:

K

~K (x) = (x) + g~k(xI ) +

gi(xI ).

(4)

i=1,i=k

Without retraining ~K (·), we use ~K and the same local vicinity {x(i)}in=1 in D to generate a new dataset D~ = {(x(1), ~K (x(1)), . . . , (x(n), ~K (x(n)))}. Finally, we can modify the interaction attribution of f (x) by fine-tuning f (·) on dataset D~. In this paper, we modify interactions by negating them: g~k(·) = -cgk(·), where -c negates the interaction attribution with a specified magnitude c.

How can modifying a local interaction affect interactions out-

side its local vicinity? This would suggest that the mani- 3

fold hypothesis is true for f (·)'s representations of these in-

teractions (Figure 2). The manifold hypothesis states that

7

similar data lie near a low-dimensional manifold in a highdimensional space (Turk & Pentland, 1991; Lee et al., 2003; 4

2

Cayton, 2005; Narayanan & Mitter, 2010). Various studies

5

1

have suggested that the hypothesis applies to the data representations learned by neural networks (Rifai et al., 2011; Basri & Jacobs, 2016). The hypothesis is frequently used to visualize how deep networks represent data clusters (Maaten & Hinton, 2008; LeCun et al., 2015), and it was previously applied to representations of interactions (Reed et al., 2014), but not for neural networks.
Part of our objective is to generalize our explanation as much as possible. In the case of language-related tasks, we addition-

6 8

local Interaction e.g. (not, bad)

Figure 2: An illustration of the hypothesis that certain local interactions, which are similar (left), are represented at a common manifold (right) in a model.

ally generalize based on our meaning of a local interaction and

the distance metric we use, m. In this paper, local interactions for language tasks do not have word

interactions fixed to specific positions; instead, these interactions are only defined by the words

themselves (the interaction values) and their positional order. For example, the ("not", "bad") inter-

action would match in the sentences: "this is not bad" and "this does not seemed to be that bad". For

comparing texts and measuring the size of local vicinities, we use Levenshtein (edit) distance (Lev-

enshtein, 1966), which allows us to compare sentences with different word counts. 1

5 EXPERIMENTS
5.1 EXPERIMENTAL SETUP
We evaluate the effectiveness of Mah´e first on synthetic data and then on four real-world datasets. To evaluate context-dependent explanations of Mah´e, we first evaluate the accuracy of Mahe´ at local interaction detection and modeling on the outputs of complex base models trained on synthetic ground truth interactions. We compare Mahe´ to Shap-Tree (Lundberg et al., 2018), ACDMLP (Singh et al., 2018), and ACD-LSTM (Murdoch et al., 2018; Singh et al., 2018), which are local interaction modeling baselines for the respective models they explain: XGBoost (Chen & Guestrin, 2016), multilayer perceptrons (MLP), and long short-term memory networks (LSTM) (Hochreiter & Schmidhuber, 1997).
In all other experiments, we study Mah´e's explanations of state-of-the-art level models trained on real-world datasets. The state-of-the-art models are: 1) DNA-CNN, a 2-layer 1D convolutional neural network (CNN) trained on MYC-DNA binding data 2 (Mordelet et al., 2013; Yang et al., 2013; Alipanahi et al., 2015; Zeng et al., 2016; Wang et al., 2018), 2) Sentiment-LSTM, a 2-layer bi-directional LSTM trained on the Stanford Sentiment Treebank (SST) (Socher et al., 2013; Tai et al., 2015), 3) ResNet152, an image classifier pretrained on ImageNet (Russakovsky et al., 2015; He et al., 2016), and 4) Transformer, a machine translation model pretrained on WMT-14 En Fr (Vaswani et al., 2017; Ott et al., 2018).
1Unfortunately, for image-related tasks, we could not generalize our definition of local interactions despite the translation invariance of deep convnets.
2The motif and flanking regions of DNA sequences in the training set are shuffled to simulate unalignment.

5

Under review as a conference paper at ICLR 2019

standardized MSE
R-Precision

6 Shap-Tree
ACD-MLP

4

ACD-LSTM Mah´e

2

0

tree mlp lstm

tree mlp lstm

tree mlp lstm


tree mlp lstm

F1 F2 F3 F4

1.0

0.5

0.0

tree mlp lstm

tree mlp lstm

tree mlp lstm


tree mlp lstm

F1 F2 F3 F4

(a) interaction fit (std. MSE; lower is better) (b) interaction detection (R-precision; higher is better)

Figure 3: Results of synthetic experiments with Mah´e and different baselines explaining base models XGBoost (tree), MLP, LSTM trained on F1-4 (Table 1) at  = 0.6 (max= 3.2) are shown. (a) shows the average local fit in MSE of Mah´e and baselines on the base models' representations of
respective interactions. (b) shows the average R-precision of interaction rankings from each baseline. *Shap-Tree cannot detect or fit to a three-way interaction. We assume interaction order is unknown, and ACD-MLP and ACD-LSTM require exhaustive search of all possible interactions.

The following hyperparameters are used in our experiments. We use n = 5k local-vicinity samples for D (n = 1k for synthetic experiments), with 80%-10%-10% train-validation-test splits to train and evaluate Mahe´. The distance metrics for vicinity size are: 2 distance for synthetic experiments, cosine distance for DNA-CNN and ResNet152, and Levenshtein distance for Sentiment-LSTM and Transformer. We use on-off superpixel and word approaches to binary feature representation for explaining ResNet152 and Sentiment-LSTM respectively (Ribeiro et al., 2016; Lundberg & Lee, 2017), and the other experiments for real-world datasets use perturbation distributions that randomly perturbs features to belong to the same categories of original features, as in (Ribeiro et al., 2018).

For the hyperparameters of the neural networks in Mah´e, we

use MLPs with 50-30-10 first-to-last hidden layer sizes to Table 1: Data generating functions

perform interaction detection in the NID framework (Tsang with interactions

et al., 2017). These MLPs are trained with 1 regularization 1 = 5e-4. The learning rate used is always 5e-3 except for Transformer experiments, whose learning rate of 5e-4 helped with interaction detection under highly unbal-
anced output classes. The MLP-based interaction models in

F1(x) = F2(x) = F3(x) =

10x1x2 + x1x2 +
exp(|x1 + x2|) +

10 i=3

xi

10 i=3

xi

10 i=3

xi

the GAM (Eq. 3) always have architectures of 30-10. They
are trained with 2 regularization of 2 = 1e-5 and learning rate of 1e-3. Because learning GAMs can be slow, we make

F4(x) =

10x1x2x3 +

10 i=4

xi

a linear approximation of the univariate functions in Eq. 3, such that gi(xi) = xi. This approxima-

tion also allows us to make direct comparisons between Mah´e and linear LIME, since xi is exactly

the linear part (Eq. 1). All neural networks train with early stopping, and Level L is decided where

validation performance does not improve more than 10%. c ranges from 3 to 4 in our experiments.

5.2 CONTEXT-DEPENDENT EXPLANATIONS
5.2.1 SYNTHETIC EXPERIMENTS
In order to evaluate Mah´e's context-dependent explanations, we first compare them to state-of-themethods for local interaction interpretation. A standard way to evaluate the accuracy of interaction detection and modeling methods has been to experiment on synthetic data because ground truth interactions are generally unknown in real-world data (Hooker, 2004; Sorokina et al., 2008; Lou et al., 2013; Tsang et al., 2017). Similar to Hooker (2007), we evaluate interactions in a subset region of a synthetic function domain. We generate synthetic data using functions F1 - F4 (Table 1) with continuous features uniformly distributed between -1 to 1, train complex base models (as specified in §5.1) on this data, and run different local interaction interpretation methods on data instances at 20 randomly sampled locations on the synthetic function domain. Figure 3a shows the performance of fitting to ground truth interactions assuming they are known, and Figure 3b shows the R-precision of detected interactions for different functions and baselines. Compared to ShapTree, ACD-MLP, and ACD-LSTM, the Mahe´ framework is the only one capable of detection and fitting, and it is the only model-agnostic approach.

5.2.2 EVALUATING ON REAL-WORLD DATA
In this section, we demonstrate our approaches to evaluating Mah´e's context-dependent explanations on real-world data. We first evaluate the prediction performance of Mahe´ on the test set of D as interactions are added in Eq. 3, i.e. K increases. We run Mahe´ 10 times on each of 40 randomly

6

Under review as a conference paper at ICLR 2019

Table 2: Average prediction performance (lower is better; 1-AUC for Transformer, MSE otherwise) with (K > 0) and without (K = 0) interactions for random sentences in the test sets of respective base models. Only results with detected interactions are shown. For each model, at least 80% of all tested sentences possessed interactions, yielding  320 sentences for each performance statistic.

K DNA-CNN Sentiment-LSTM ResNet152 Transformer

linear LIME 0 9.8e-3 ± 8.4e-4 5.5e-2 ± 3.8e-3 0.23 ± 0.027 0.22 ± 0.069

Mahe´

1 9e-3 ± 1.2e-3 2.1e-2 ± 3.7e-3 0.20 ± 0.025 0.05 ± 0.013

Mah´e

L 6e-3 ± 1.1e-3 1.3e-2 ± 3.9e-3 0.19 ± 0.028 0.05 ± 0.013

(a) Explanation A of negative prediction

(b) Explanation B of negative prediction

Figure 4: Example of choices Mechanical Turk users select from. (a) is linear LIME, (b) is Mahe´.

selected data instances from the test sets associated with DNA-CNN ( = 0.4), Sentiment-LSTM ( = 6), and ResNet152 ( = 0.4). For Transformer ( = 6), stability is examined on a specific grammar (cet) translation, to be detailed in §5.3. The local vicinity samples and model initializations in Mahe´ are randomized in every trial. Table 2 shows the average prediction error as K starts from 0, which is linear LIME, and is increased to the last hierarchical level L. As seen, including interactions results in significant performance improvements.
An alternative approach to evaluating Mah´e is to ask human evaluaters to select between the better explanation linear LIME and Mahe´. We recruit 60 Amazon Mechanical Turk users to select between explanations of Sentiment-LSTM. To minimize the difference between the two explanations, Mahe´ only shows the K = 1 interaction from the interaction ranking and aggregates its attribution (see Figure 4) with that of subsumed features in the linear part of Mahe´ (linear LIME, §5.1). Only sentences for which there is a significant difference between linear LIME and Mah´e are shown, i.e. there is any polarity difference between the attributions of linear and interaction explanations. To reduce ambiguities of uninterpretable explanations arising from a misbehaving model - an issue also faced by Sundararajan et al. (2017) in interpretation evaluation - we only show explanations of sentences that the model classified correctly. Further details about this experiment can be found in Supplementary B. The result of this experiment is that the majority of preferred explanations (65%, p = 0.029) with interactions, supporting the inclusion of interactions in hierarchical explanations.
5.2.3 HIERARCHICAL EXPLANATIONS Examples of context-dependent explanations in hierarchical format for ResNet152, SentimentLSTM, and Transformer are shown in Figure 6, Table 5, and Table 6 respectively after page 8.
5.3 CONTEXT-FREE EXPLANATIONS
In this section, we show examples of context-free explanations of interactions found by Mah´e. We first study English-to-French translations made with Transformer on a specific type of known interaction, the presence of a French word for "this" or "that", cet, which only appears when the noun it modifies begins with a vowel. The presence of cet is used as the prediction variable in local interaction extraction. To minimize the sources of cet, we limit original sentence lengths to 15 words. We modify Transformer (§4.2) through differentiating the max value of the cet output neurons over all translated words, and to have enough sentences, we perform translations on WikiText-103 (Merity et al., 2016) in addition to the WMT-14 test set. The results of context-free experiments on cet interactions of adjacent English words are shown in Table 3. The interactions always have positive polarities towards cet, and after modifying Transformer at a single data instance for a given interaction, its polarity almost always become negative. Examples of new translations from the modified Transformer are shown in the "after" rows in Table 4, where cet now disappears from the translations. Transformer BLEU score only decreases by an average percent difference of -2.7%

7

Under review as a conference paper at ICLR 2019

Table 3: cet interactions before and af-
ter modifying Transformer. Ns is number of samples, and %cet is % of Ns samples + or - contributing to cet.

Table 4: Examples of En.-Fr. translations before and after
modifying Transformer. Interacting elements are bolded. BLEU change is the % change in test BLEU score from modifying the bolded interaction in Transformer.

Before modifying

Interaction

Ns %cet +

(this, event) (this, article) (this, incident) (this, album) (this, arrangement) (that, afternoon) (this, location) (this, effect)

38 33 31 24 22 22 20 19

1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0

After modifying
Ns %cet -
39 1.0 34 1.0 29 0.93 40 1.0 36 1.0 27 1.0 22 0.95 20 0.95

English Fr. before Fr. after
English Fr. before Fr. after
English Fr. before Fr. after

Sample English-French Translations

BLEU change

This event took place on 10 August 2008. Cet e´ve´nement a eu lieu le 10 Mars 2008. Cette rencontre a eu lieu le 10 Mars 2008.

(-3.7%)

This incident made it into the music video. Cet incident a e´te´ inte´gre´ dans le vide´o musical. C'est pas mal du tout ca!

(-3.4%)

The initial language of this article was French.
La langue initiale de cet article e´tait le Franc¸ais. La langue originale du pre´sent article e´tait le Franc¸ais. (-2.8%)

after modification, indicating that the original learned representation stays largely intact. Results for  = 6 are shown with the avg. pairwise edit distance between sentences being  = 10.5.

In experiments on Sentiment-LSTM, we also observe that the polarities of certain interactions are almost always the same (see Figure 5) throughout all sentences in IMDB movie reviews data (Maas et al., 2011) and the test set of SST. In this case, the words of matching interactions can be separated by any number of words in-between. As before, whenever the model's local interaction attribution at a single data instance is negated, the attribution is almost always the opposite sign for the rest of the sentences. A notable insight about SentimentLSTM is that it appears to represent (too, bad) and (only, worse) as globally positive sentiments, and Mahe´'s modification in large part rectifies this misbehavior (Figure 5). The modifications to Sentiment-LSTM only cause an average reduction of 1.5% test accuracy. Results for  = 16,  = 24.8 are shown, and words in detected interactions are separated by 1.3 words on average.

Before model modification

((+-))

After model modification positive sentiment negative sentiment

(not, but) (not, bad) (too, bad) (only, worse) (not, just) (if, you, liked) (not, nice)
(if, you)

9928.9.6%%(-()+) 9978..51%% ((-+)) 9946..40%% ((-+) ) 9945..37%% ((-+) ) 9956..77%% ((+-)) 9957..53%%((-+) ) 9925..73%%((+-)) 911.050%.0(%-) (+)

01 Consistency of Attribution Polarity (+/-)

Experiments on DNA-CNN and ResNet152 show similar results at fixed interaction positions (§4.2). For DNA-CNN, out of the 94 times a 6-way interaction of the CACGTG motif (Sharon et al., 2008) was detected in the test set, every time
yielded a positive attribution polarity towards DNA-protein

Figure 5: Interaction polarity consistency in Sentiment-LSTM before and after model modification. (mean and std. errors shown)

affinity, and the same was true after modifying the model in the opposite polarity (cosine distance

 = 0.35,  = 0.408). For ResNet152, context-free interactions are also found (cosine distance

 = 0.4,  = 0.663). However, because superpixels are used, the interactions found may contain

artifacts caused by superpixel segmenters, yielding less intuitive interactions (see Supplementary A).

5.4 LIMITATIONS
Although Mahe´ obtains accurate local interactions on synthetic data using NID, there is no guarantee that NID finds correct interactions. Mahe´ faces common issues of model-agnostic perturbation methods in interpreting high-dimensional feature spaces, choice of perturbation distribution, and speed (Ribeiro et al., 2016; 2018). Finally, an exhaustive search is used for context-free explanations.

6 CONCLUSION
In this work, we proposed Mahe´, a model-agnostic framework of providing context-dependent and context-free explanations of local interactions. Mahe´ has demonstrated the capability of outperforming existing approaches to local interaction interpretation and has shown that local interactions can be context-free. In future work, we wish to make the process of finding context-free interactions more efficient, and study to what extent model behavior can be changed by editing its interactions or univariate effects. Finally, we would like to study the interpretations provided by Mah´e more closely to find new insights into structured data.

8

Under review as a conference paper at ICLR 2019

Original Image linear LIME (level 1)

Mahé (level 2)

2.183

Mahé (level 3)

Mahé (level 4)

0.317

00

Original Image

R2 = 0.838

-2.183

R2 = 0.84

(a) prediction: stretcher

linear LIME (level 1)

Mahé (level 2)

0.76

R2 = 0.859
Mahé (level 3)

R2 = 0.869

-0.317

Mahé (level 4)

0.094

00

Original Image

-0.76

R2 = 0.769

R2 = 0.779

R2 = 0.81

(b) prediction: window screen

linear LIME (level 1)

Mahé (level 2) Mahé (level 3)

2.259

R2 = 0.825

-0.094

Mahé (level 4)

0.122

00

Original Image

R2 = 0.871

-2.259

R2 = 0.872

R2 = 0.881

(c) prediction: Pomeranian

linear LIME (level 1)

Mahé (level 2) Mahé (level 3)

4.371

R2 = 0.901

-0.122

Mahé (level 4)

0.134

00

R2 = 0.877

-4.371

R2 = 0.882

R2 = 0.894

R2 = 0.901

-0.134

(d) prediction: water buffalo

Figure 6: Examples of context-dependent explanations in hierarchical format for ResNet152. Interaction attributions of {gi(·)}iK=1 are show at each K - 1 level, K  1 (§4.1). Colors in superpixels represent attribution scores and their polarity. Cyan regions positively contribute to the predicton,
and red regions negatively contribute. Boundaries between overlapping interactions are merged
when their attribution polarities match. From these examples, it is often seen that superpixels be-
longing to the same entity (or class in (d)) interact. Also shown is that ResNet152 selects interacting
entities that support its prediction. These interpretations cannot be seen in linear LIME.

9

Under review as a conference paper at ICLR 2019

Table 5: Examples of context-dependent hierarchical explanations on Sentiment-LSTM. The interaction attribution of gK(·) is shown at each K - 1 level, K  1 (§4.1) in color. Green means positively contributing to sentiment, and red means the opposite. Attribution visualizations at level
1 are normalized to the max attribution magnitude (max magn.) shown. Interaction attributions at levels  2 are normalized across levels to a single max magn. score shown. Top-5 attributions by
magnitude are shown for linear LIME.

Method

Level Fit (R2) Hierarchical Explanation

linear LIME 1

0.621 the film is really not so much bad as bland

Mahe´ Mahe´ Mahe´

2 0.751 not, bad 3 0.916 not, bad, bland 4 0.926 film, not, bad, bland

Max magn. 0.744
0.119

Method

Level Fit (R2) Hierarchical Explanation

linear LIME 1

0.936 it 's also the year 's sweetest movie

Mahe´ Mahe´

2 0.963 sweetest, movie 3 0.968 year, sweetest

Max magn. 2.073 0.170

Method

Level Fit (R2) Hierarchical Explanation

linear LIME 1

0.923 for a shoot 'em up, ballistic is oddly lifeless

Mahe´ Mah´e

2 0.989 is, oddly, lifeless 3 0.995 shoot lifeless

Max magn. 2.200 0.216

Table 6: Examples of context-dependent hierarchical explanations on Transformer. The interaction
attribution of gK(·) is shown at each K - 1 level, K  1 (§4.1) in color. Green contributes towards cet translations, and red contributes the opposite. Attribution normalization follows that same
approach as Table 5. Top-5 attributions by magnitude are shown for linear LIME.

Method linear LIME Mah´e

Level
1 2

Fit (R2) 0.782 0.948

Hierarchical Explanation
this article was last updated on substance in august 2012 this, article

Method linear LIME Mahe´

Level
1 2

Fit (R2) 0.696 0.96

Hierarchical Explanation
this effect takes part in making lead slightly less reactive chemically this, effect

Max magn. 0.657 3.707
Max magn. 0.643 2.459

Method linear LIME Mahe´

Level
1 2

Fit (R2) 0.734 0.926

Hierarchical Explanation
the population size of this bird has not yet been quantified or estimated this, bird

Max magn.
0.605 1.211

REFERENCES
Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J Frey. Predicting the sequence specificities of dna-and rna-binding proteins by deep learning. Nature biotechnology, 33 (8):831, 2015.
Ronen Basri and David Jacobs. Efficient representation of low-dimensional manifolds using deep networks. arXiv preprint arXiv:1602.04723, 2016.
Lawrence Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep, 12(1-17):1, 2005.
Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785­794. ACM, 2016.
Trevor J Hastie and Robert J Tibshirani. Generalized additive models, 1990.

10

Under review as a conference paper at ICLR 2019
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Giles Hooker. Discovering additive structure in black box functions. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 575­580. ACM, 2004.
Giles Hooker. Generalized functional anova diagnostics for high-dimensional functions of dependent variables. Journal of Computational and Graphical Statistics, 16(3):709­732, 2007.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4 (2):251­257, 1991.
Linwei Hu, Jie Chen, Vijayan N Nair, and Agus Sudjianto. Locally interpretable models and effects based on supervised partitioning (lime-sup). arXiv preprint arXiv:1806.00663, 2018.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.
Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International Conference on Machine Learning, pp. 2673­2682, 2018.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Kuang-Chih Lee, Jeffrey Ho, Ming-Hsuan Yang, and David Kriegman. Video-based face recognition using probabilistic appearance manifolds. In Computer vision and pattern recognition, 2003. proceedings. 2003 ieee computer society conference on, volume 1, pp. I­I. IEEE, 2003.
Vladimir I Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pp. 707­710, 1966.
Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intelligible models with pairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 623­631. ACM, 2013.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pp. 4765­4774, 2017.
Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for tree ensembles. arXiv preprint arXiv:1802.03888, 2018.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142­150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/P11-1015.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579­2605, 2008.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
Fantine Mordelet, John Horton, Alexander J Hartemink, Barbara E Engelhardt, and Raluca Gorda^n. Stability selection for regression-based models of transcription factor­dna binding specificity. Bioinformatics, 29(13):i117­i125, 2013.
W James Murdoch, Peter J Liu, and Bin Yu. Beyond word importance: Contextual decomposition to extract interactions from lstms. arXiv preprint arXiv:1801.05453, 2018.
11

Under review as a conference paper at ICLR 2019
Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. In Advances in Neural Information Processing Systems, pp. 1786­1794, 2010.
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. arXiv preprint arXiv:1806.00187, 2018.
Sanjay Purushotham, Martin Renqiang Min, C-C Jay Kuo, and Rachel Ostroff. Factorized sparse learning models with interpretable high order feature interactions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 552­561. ACM, 2014.
Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of variation with manifold interaction. In International Conference on Machine Learning, pp. 1431­ 1439, 2014.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135­1144. ACM, 2016.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic explanations. In AAAI Conference on Artificial Intelligence, 2018.
Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent classifier. In Advances in Neural Information Processing Systems, pp. 2294­2302, 2011.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211­252, 2015.
Eilon Sharon, Shai Lubliner, and Eran Segal. A feature-based approach to modeling protein­dna interactions. PLoS computational biology, 4(8):e1000154, 2008.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. arXiv preprint arXiv:1704.02685, 2017.
Chandan Singh, W James Murdoch, and Bin Yu. Hierarchical interpretations for neural network predictions. arXiv preprint arXiv:1806.05337, 2018.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vie´gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631­1642, 2013.
Daria Sorokina, Rich Caruana, Mirek Riedewald, and Daniel Fink. Detecting statistical interactions with additive groves of trees. In Proceedings of the 25th international conference on Machine learning, pp. 1000­1007. ACM, 2008.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International Conference on Machine Learning, pp. 3319­3328, 2017.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.
Michael Tsang, Dehua Cheng, and Yan Liu. Detecting statistical interactions from neural network weights. arXiv preprint arXiv:1705.04977, 2017.
Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of cognitive neuroscience, 3 (1):71­86, 1991.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998­6008, 2017.
12

Under review as a conference paper at ICLR 2019 Meng Wang, Cheng Tai, Weinan E, and Liping Wei. Define: deep convolutional neural networks
accurately quantify intensities of transcription factor-dna binding and facilitate evaluation of functional non-coding variants. Nucleic acids research, 46(11):e69­e69, 2018. Lin Yang, Tianyin Zhou, Iris Dror, Anthony Mathelier, Wyeth W Wasserman, Raluca Gorda^n, and Remo Rohs. Tfbsshape: a motif database for dna shape features of transcription factor binding sites. Nucleic acids research, 42(D1):D148­D155, 2013. Haoyang Zeng, Matthew D Edwards, Ge Liu, and David K Gifford. Convolutional neural network architectures for predicting dna­protein binding. Bioinformatics, 32(12):i121­i127, 2016.
13

Under review as a conference paper at ICLR 2019

SUPPLEMENTARY MATERIALS

A CONTEXT-FREE EXPLANATIONS IN RESNET152

prediction: bulbul

prediction: tow truck, tow car, wrecker

prediction: fig

prediction: bustard

1.0 / 1.0

0.925 / 0.825

1.0 / 1.0

0.949 / 0.947

Figure 7: Context-free explanations of interactions in ResNet152 for each image, evaluated over 40 tests before and after modification. Each test superimposes the interaction of interest onto a randomly selected background image from the test set of ImageNet. Corresponding predictions are shown above each image, and the percentage of consistent interaction polarity before and after modification are shown below in that order. The red color indicates a negative interaction attribution polarity before modification. After modification, the polarities become positive.

B DETAILS OF MECHANICAL TURK EXPERIMENT

Figure 8: Example of Mechanical Turk interface used by workers to select between explanations provided by linear LIME and Mahe´.
To determine whether human evaluators agree with the explanations provided by Mahe´, we recruit 60 Amazon Mechanical Turk users to label the more accurate explanation between linear LIME explanations and Mahe´ explanations of Sentiment-LSTM (Figure 8). To minimize the difference between the two explanations, Mah´e only shows the top-1 interaction from the interaction ranking (K = 1, §4.1) and aggregates its attribution (see Figure 4) with that of subsumed features in the linear part of Mahe´ (linear LIME, §5.1). For the comparison, we show users randomly selected sentences of lengths 5 to 12 from the test set of SST with the following conditions. We exclude sentences that do not have valid interactions, i.e. interactions are not detected. Only sentences for which there is a significant difference between linear LIME and Mahe´ are shown, i.e. there is any polarity difference between the attributions of linear and interaction explanations. To reduce ambiguities of uninterpretable explanations arising from a misbehaving model, we only show explanations of sentences that the model classified correctly. Besides conditions on sentences, we only allow each user to select up to four explanations. For a single sentence, a majority vote of the selections of 5 different users decides which explanation is more accurate. The ordering of comparisons is randomized.
C MORE EXAMPLES OF INTERACTIONS WITH CONSISTENT POLARITIES IN SENTIMENT-LSTM
14

Under review as a conference paper at ICLR 2019

Table 7: Shown below are more examples of interactions with consistent polarities found by Mahe´ in Sentiment-LSTM. Num samples is the number of sentences from which the same interaction is found. Percent polarity is the percentage of interactions that have the specified attribution polarity. Avg. separation is the average separation of words in detected interactions.

Interaction
(not, good) (falls, flat) (not, funny) (not, miss) (still, love) (bad, worst) (never, off)

num samples
213 169 155 133 103 44 36

percent polarity

96.7% 97.6% 97.4% 97.7% 98.1% 95.5% 100%

negative negative negative positive positive positive positive

avg. separation between words
1.4 0.17 0.65 0.66 0.19 5.5 1.4

15

