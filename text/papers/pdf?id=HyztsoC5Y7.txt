Under review as a conference paper at ICLR 2019
LEARNING TO ADAPT IN DYNAMIC, REAL-WORLD ENVIRONMENTS THROUGH META-REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.2
1 INTRODUCTION
Both model-based and model-free reinforcement learning (RL) methods generally operate in one of two regimes: all training is performed in advance, producing a model or policy that can be used at test-time to make decisions in settings that approximately match those seen during training; or, training is performed online (e.g., as in the case of online temporal-difference learning), in which case the agent can slowly modify its behavior as it interacts with the environment. However, in both of these cases, dynamic changes such as failure of a robot's components, encountering a new terrain, environmental factors such as lighting and wind, or other unexpected perturbations, can cause the agent to fail. In contrast, humans can rapidly adapt their behavior to unseen physical perturbations and changes in their dynamics (Braun et al., 2009): adults can learn to walk on crutches in just a few seconds, people can adapt almost instantaneously to picking up an object that is unexpectedly heavy, and children that can walk on carpet and grass can quickly figure out how to walk on ice without having to relearn how to walk. How is this possible? If an agent has encountered a large number of perturbations in the past, it can in principle use that experience to learn how to adapt. In this work, we propose a meta-learning approach for learning online adaptation.
Motivated by the ability to tackle real-world applications, we specifically develop a model-based meta-reinforcement learning algorithm. In this setting, data for updating the model is readily available at every timestep in the form of recent experiences. But more crucially, the meta-training process for training such an adaptive model can be much more sample efficient than model-free meta-RL approaches (Duan et al., 2016; Wang et al., 2016; Finn et al., 2017). Further, our approach foregoes the episodic framework on which model-free meta-RL approaches rely on, where tasks are pre-defined to be different rewards or environments, and tasks exist at the trajectory level only. Instead, our method considers each timestep to potentially be a new "task," where any detail or setting could have changed
2Videos available at: https://sites.google.com/view/learning2adapt
1

Under review as a conference paper at ICLR 2019
Figure 1: We implement our sample-efficient meta-reinforcement learning algorithm on a real legged millirobot, enabling online adaptation to new tasks and unexpected occurrences such as losing a leg (shown here), novel terrains and slopes, errors in pose estimation, and pulling payloads.
at any timestep. This view induces a more general meta-RL problem setting by allowing the notion of a task to represent anything from existing in a different part of the state space, to experiencing disturbances, or attempting to achieve a new goal.
Learning to adapt a model alleviates a central challenge of model-based reinforcement learning: the problem of acquiring a global model that is accurate throughout the entire state space. Furthermore, even if it were practical to train a globally accurate dynamics model, the dynamics inherently change as a function of uncontrollable and often unobservable environmental factors, such as those mentioned above. If we have a model that can adapt online, it need not be perfect everywhere a priori. This property has previously been exploited by adaptive control methods (Åström and Wittenmark, 2013; Sastry and Isidori, 1989); but, scaling such methods to complex tasks and nonlinear systems is exceptionally difficult. Even when working with deep neural networks, which have been used to model complex nonlinear systems (Kurutach et al., 2018), it is exceptionally difficult to enable adaptation, since such models typically require large amounts of data and many gradient steps to learn effectively. By specifically training a neural network model to require only a small amount of experience to adapt, we can enable effective online adaptation in complex environments while putting less pressure on needing a perfect global model.
The primary contribution of our work is an efficient meta reinforcement learning approach that achieves online adaptation in dynamic environments. To the best knowledge of the authors, this is the first meta-reinforcement learning algorithm to be applied in a real robotic system. Our algorithm efficiently trains a global model that is capable to use its recent experiences to quickly adapt, achieving fast online adaptation in dynamic environments. We evaluate two versions of our approach, recurrence-based adaptive learner (ReBAL) and gradient-based adaptive learner (GrBAL) on stochastic and simulated continuous control tasks with complex contact dynamics (Fig. 2). In our experiments, we show (1) a quadrupedal "ant" adapting to the failure of different legs, as well as a "half-cheetah" robot adapting to the failure off different joints, navigating terrains with different slopes, and walking on floating platforms of varying buoyancy. Our model-based meta RL method attains substantial improvement over prior approaches, including standard model-based methods, online model-adaptive methods, model-free methods, and prior meta-reinforcement learning methods, when trained with similar amounts of data. In all experiments, meta-training across multiple tasks is sample efficient, using only the equivalent of 1.5 - 3 hours of real-world experience, roughly 10× less than what model-free methods require to learn a single task. Finally, we demonstrate GrBAL on a real dynamic legged millirobot (see Fig 2). To highlight not only the sample efficiency of our meta model-based reinforcement learning approach, but also the importance of fast online adaptation in the real world, we show the agent's learned ability to adapt online to tasks such as a missing leg, novel terrains and slopes, miscalibration or errors in pose estimation, and new payloads to be pulled.
2 RELATED WORK
Advances in learning control policies have shown success on numerous complex and high dimensional tasks (Schulman et al., 2015; Lillicrap et al., 2015; Mnih et al., 2015; Levine et al., 2016; Silver et al., 2017). While reinforcement learning algorithms provide a framework for learning new tasks, they primarily focus on mastery of individual skills, rather than generalizing and quickly adapting to new scenarios. Furthermore, model-free approaches (Peters and Schaal, 2008) require large amounts of system interaction to learn successful control policies, which often makes them impractical for real-world systems. In contrast, model-based methods attain superior sample efficiency by first learning a model of system dynamics, and then using that model to optimize a policy (Deisenroth
2

Under review as a conference paper at ICLR 2019
et al., 2013; Lenz et al., 2015; Levine et al., 2016; Nagabandi et al., 2017b). A key challenge with model-based RL approaches is the difficulty of learning a global model that is accurate for the entire state space. Our approach alleviates the need to learn a single global model by allowing the model to be adapted automatically to different scenarios online based on recent observations.
Prior online adaptation approaches (Tanaskovic et al., 2013; Aswani et al., 2012) have aimed to learn an approximate global model and then adapt it at test time. Dynamic evaluation algorithms (Rei, 2015; Krause et al., 2017; 2016; Fortunato et al., 2017), for example, learn an approximate global distribution at training time and adapt those model parameters at test time to fit the current local distribution via gradient descent. Such work in model adaptation (Levine and Koltun, 2013; Gu et al., 2016; Fu et al., 2015; Weinstein and Botvinick, 2017) has shown that a perfect global model is not necessary, and prior knowledge can be fine-tuned to handle small changes. These methods, however, face a mismatch between what the model is trained for and how it is used at test time. In this paper, we bridge this gap by explicitly training a model for fast and effective adaptation. As a result, our model achieves more effective adaptation compared to these prior works, as validated in our experiments.
Our problem setting relates to meta-learning, a long-standing problem of interest in machine learning that is concerned with enabling artificial agents to efficiently learn new tasks by learning to learn (Thrun and Pratt, 1998; Schmidhuber and Huber, 1991; Naik and Mammone, 1992; Lake et al., 2015). A meta-learner can control learning through approaches such as deciding the learner's architecture (Baker et al., 2016), or by prescribing an optimization algorithm or update rule for the learner (Bengio et al., 1990; Schmidhuber, 1992; Younger et al., 2001; Andrychowicz et al., 2016; Li and Malik, 2016; Ravi and Larochelle, 2018). Another popular meta-learning approach involves simply unrolling a recurrent neural network (RNN) that ingests the data (Santoro et al., 2016; Munkhdalai and Yu, 2017; Munkhdalai et al., 2017; Mishra et al., 2017) and learns internal representations of the algorithms themselves. In particular, Duan et al. (2016); Wang et al. (2016); Sung et al. (2017); Al-Shedivat et al. (2017) developed model-free meta reinforcement learning methods. In this work, we propose a model-based meta RL approach for learning online adaptation, suggest two instantiations of this general approach, and show results of our approach on simulated agents as well as a real legged robot. To our knowledge, no prior work has developed a model-based meta reinforcement learning algorithm.
3 PRELIMINARIES
In this section, we present model-based reinforcement learning, and introduce the meta-learning formulation, where we describe the two main meta-learning approaches.
3.1 MODEL-BASED REINFORCEMENT LEARNING Reinforcement learning agents aim to perform actions that maximize some notion of cumulative reward. Concretely, consider a Markov decision process (MDP) defined by the tuple (S, A, p, r, , 0, H). Here, S is the set of states, A is the set of actions, p(s |s, a) is the state transition distribution, r : S × A  R is a bounded reward function, 0 : S  R+ is the initial state distribution,  is the discount factor, and H is the horizon. A trajectory segment is denoted by  (i, j) := (si, ai, ..., sj, aj, sj+1). Finally, the sum of expected rewards from a trajectory is the return. In this framework, RL aims to find a policy  : S  A that prescribes the optimal action to take from each state in order to maximize the expected return.
Model-based RL aims to solve this problem by learning the transition distribution p(s |s, a), which is also referred to as the dynamics model. This can be done using a function approximator p^(s |s, a) to approximate the dynamics, where the weights  are optimized to maximize the log-likelihood of the observed data D. In practice, this model is then used in the process of action selection by either producing data points from which to train a policy, or by producing predictions and dynamics constraints to be optimized by a controller.
3.2 META-LEARNING Meta-learning is concerned with automatically learning learning algorithms that are more efficient and effective than learning from scratch. These algorithms leverage data from previous tasks to acquire a learning procedure that can quickly adapt to new tasks. These methods operate under the assumption that the previous meta-training tasks and the new meta-test tasks are drawn from the same task distribution (T ) and share a common structure that can be exploited for fast learning.
3

Under review as a conference paper at ICLR 2019

In the supervised learning setting, we aim to learn a function f with parameters  that minimizes a supervised loss LT . Then, the goal of meta-learning is to find a learning procedure, denoted as  = u(DTtr , ), that can learn a range of tasks T from small datasets DTtr .

We can formalize this meta-learning problem setting as optimizing for the parameters of the learning

procedure ,  as follows:

min
,

ET (T )

L(DTtest,  )

s.t.  = u(DTtr , )

(1)

where DTtr , DTtest are sampled without replacement from the meta-training dataset DT .

Once meta-training optimizes for the parameters , , the learning procedure u(·, ) can then be used to learn new held-out tasks from small amounts of data. We will also refer to the learning procedure u as the update function.

Gradient-based meta-learning. Model-agnostic meta-learning (MAML) (Finn et al., 2017) aims

to learn the initial parameters of a neural network such that taking one or several gradient descent

steps from this initialization leads to effective generalization (or few-shot generalization) to new tasks.

Then, when presented with new tasks, the model with the meta-learned initialization can be quickly

fine-tuned using a few data points from the new tasks. Using the notation from before, MAML uses

gradient descent as a learning algorithm:

u(DTtr , ) =  - L(DTtr , )

(2)

The learning rate  may be a learnable paramter (in which case  = ) or fixed as a hyperparameter,

leading to  = . Despite the update rule being fixed, a learned initialization of an overparameterized

deep network followed by gradient descent is as expressive as update rules represented by deep

recurrent networks (Finn and Levine, 2017).

Recurrence-based meta-learning. Another approach to meta-learning is to use recurrent models. In this case, the update function is always learned, and  corresponds to the weights of the recurrent model that update the hidden state. The parameters  of the prediction model correspond to the remainder of the weights of the recurrent model and the hidden state. Both gradient-based and recurrence-based meta-learning methods have been used for meta model-free RL (Finn et al., 2017; Duan et al., 2016). We will build upon these ideas to develop a meta model-based RL algorithm that enables adaptation in dynamic environments, in an online way.

4 META-LEARNING FOR ONLINE MODEL ADAPTATION

In this section, we present our approach for meta-learning for online model adaptation. As explained in Section 3.2, standard meta-learning formulations require the learned model ,  to adapt after seeing M data points from some new "task." Our notion of task is slightly more fluid, where every segment of a trajectory can be considered to be a different "task," and the past M observations can be considered as providing more information about the current task setting. Since changes in system dynamics, terrain details, or other environmental changes can occur at any time, we consider (at every time step) the problem of adapting to the M past time steps.

In this work, we use the notion of environment E to denote different settings or configurations
of a particular problem, ranging from malfunction in the joints of a robot to the state of external disturbances. We assume a distribution of environments (E) that share some common structure, such as the same observation and action space, but may differ in their dynamics pE (s |s, a). We denote a trajectory by E (i, j), which represents a sequence of states and actions (si, ai, ..., sj, aj, sj+1) sampled from environment E. We pose the meta-RL problem in this setting as an optimization over (,
) with respect to a maximum likelihood meta-objective. The meta-objective is the likelihood of the data under a predictive model p^ (s |s, a) with parameters  , where  = u(E (t - M, t - 1), ) corresponds to model parameters that were updated using the past M data points. Concretely, this
corresponds to the following optimization:

min
,

EE (E )
tU ([T -K-1])

L(E (t, t + K), E )

s.t.: E = u(E (t - M, t - 1), ),

where the loss L corresponds to the negative log likelihood of the data under the model:

(3)

L(E (t, t + K), E )

1 t+K

- K

log p^E (sk+1|sk, ak).

k=t

(4)

4

Under review as a conference paper at ICLR 2019

Algorithm 1 Model-Based Meta-Reinforcement Algorithm 2 Online Model Adaptation

Learning (train time)

(test time)

Require: Distribution E over tasks Require: Learning rate   R+

Require: Number of sampled tasks N , dataset D

Require: Task sampling frequency nS  Z+

1: Randomly initialize 

2: for i = 1, ... do

3: if i mod nS = 0 then 4: Sample E  (E)

5: Collect E using Alg. 2 6: D  D  {E }

7: end if

8: for j = 1 . . . N do

9: E (t - M, t - 1), E (t, t + K)  D

10: E  u(E (t - M, t - 1), )

11: Lj  L(E (t, t + K), E )

12: end for

N

13:







-



1 N

Lj

j=1

N

14:







-



1 N

Lj

j=1

15: end for

16: Return (, ) as (, )

Require: Meta-learned parameters ,  Require: controller(), H, r, nA 1: D   2: for each timestep t do 3:   u (D(t - M, t - 1), ) 4: a  controller(, r, H, nA) 5: Execute a, add result to D 6: end for 7: Return rollout D
Figure 2: Two real-world and four simulated environments on which our method is evaluated and adaptation is crucial for success (e.g., adapting to different slopes and leg failures)

In the meta-objective in Equation 3, note that the past M points are used to adapt  into  , and the loss of this  is evaluated on the future K points. Thus, we use the past M timesteps to provide insight into how to adapt our model to perform well for nearby future timesteps. As outlined in Algorithm 1, the update rule u for the inner update and a gradient step on  for the outer update allow us to optimize this meta-objective of adaptation. Thus, we achieve fast adaptation at test time by being able to fine-tune the model using just M data points.

While we focus on reinforcement learning problems in our experiments, this meta-learning approach could be used for a learning to adapt online in a variety of sequence modeling domains. We present our algorithm using both a recurrence and a gradient-based meta-learner, as we discuss next.

Gradient-Based Adaptive Learner (GrBAL). GrBAL uses a gradient-based meta-learning to perform online adaptation; in particular, we use MAML (Finn et al., 2017). In this case, our update rule is prescribed by gradient descent ( 5.)

E

=

u(E (t

-

M, t

-

1), )

=

E

+



1 M

t-1
log p^E (sm+1|sm, am)

m=t-M

(5)

Recurrence-Based Adaptive Learner (ReBAL). ReBAL, instead, utilizes a recurrent model, which learns its own update rule (i.e., through its internal gating structure. In this case,  and u corresponds to the weights of the recurrent model that update its hidden state.
5 MODEL-BASED META-REINFORCEMENT LEARNING
Now that we have discussed our approach for enabling online adaptation, we next propose how to build upon this idea to develop a model-based meta-reinforcement learning algorithm. First, we explain how the agent can use the adapted model to perform a task, given parameters  and  from optimizing the meta-learning objective.
Given  and , we use the agent's recent experience to adapt the model parameters:  = u ( (t - M, t), ). This results in a model p^ that better captures the local dynamics in the current setting, task, or environment. This adapted model is then passed to our controller, along

5

Under review as a conference paper at ICLR 2019

with the reward function r and a planning horizon H. We use a planning H that is smaller than the adaptation horizon K, since the adapted model is only valid within the current context. We use model predictive path integral control (MPPI) (Williams et al., 2015), but, in principle, our model adaptation approach is agnostic to the model predictive control (MPC) method used.
The use of MPC compensates for model inaccuracies by preventing accumulating errors, since we replan at each time step using updated state information. MPC also allows for further benefits in this setting of online adaptation, because the model p^E itself will also improve by the next time step. After taking each step, we append the resulting state transition onto our dataset, reset the model parameters back to , and repeat the entire planning process for each timestep. See Algorithm 2 for this adaptation procedure. Finally, in addition to test-time, we also perform this online adaptation procedure during the meta-training phase itself, to provide on-policy rollouts for meta-training. For the complete meta-RL algorithm, see Algorithm 1.

6 RESULTS
Our evaluation aims to answer the following questions: (1) is adaptation actually changing the model?, (2) does our approach enable fast adaptation to varying dynamics, tasks, and environments, both inside and outside of the training distribution?, (3) how does our method's performance compare to that of other methods?, (4) how does GrBAL and ReBAL compare?, (6) how does meta model-based RL compare to meta model-free RL in terms of sample efficiency and performance for these tasks?, and (7) can our method learn to adapt online on a real robot, and if so, how do it perform? We next present our set-up and results, motivated by these questions. Videos of all results are online.3
We first conduct a comparative evaluation of our algorithm, on a variety of simulated robots using the MuJoCo physics engine (Todorov et al., 2012). For all of our environments, we model the transition probabilities as Gaussian random variables with mean parameterized by the neural network model, and fixed variance. In this case, maximum likelihood estimation corresponds to minimizing the mean squared error. We now describe the setup of our environments (Fig. 2), where each agent requires different types of adaptation:

Half-cheetah (HC): disabled joint. For each rollout during meta-training, we randomly sample a joint to be disabled (i.e., the agent cannot apply torques to that joint).

HC: sloped terrain. For each rollout during meta-training, we randomly select an upward or downward slope of low steepness.

HC: pier. In this task, the cheetah runs over a series of blocks that are floating on water. Each block moves up and down when stepped on, and the changes in the dynamics are rapidly changing, due to each block having different damping and friction properties. The HC is meta-trained by varying these block properties, and tested on a specific (randomly-selected) configuration of block properties.

Ant: crippled leg. For each meta-training rollout, we randomly sample a leg to cripple on this quadrupedal robot. This causes unexpected and drastic changes to the underlying dynamics.

6.1 EFFECT OF ADAPTATION

First, we analyze the effect of the model adaptation, and show results from three environments: HC pier, HC sloped terrain running up and down a steep hill, and ant crippled leg, where the particular crippled leg is not seen during training. Figure 3 displays the distribution shift between the pre-update and post-update model prediction errors of three GrBAL runs, showing that using the past M timesteps to update  (pre) into  (post) does indeed reduce model error on predicting the following K timesteps.

Figure 3: Histogram of normalized K-step prediction errors of GrBAL, showing the improvement of the post-update model's predictions over the pre-update ones.

3Videos available at: https://sites.google.com/view/learning2adapt

6

Under review as a conference paper at ICLR 2019
6.2 SAMPLE EFFICIENCY For our first comparative evaluation, we evaluate both GrBAL and ReBAL against a state-of-theart model-free meta-RL method (MAML-RL) Finn et al. (2017) and a purely model-free method (TRPO) Schulman et al. (2015). Here, we train both model-free approaches until convergence, and they receive the equivalent of about two days of real-world experience. Our model-based meta-RL approach results in superior or equivalent performance than a model-free agent trained with 1000 times more data. When comparing it against a model-free meta-RL method, our approach asymptotic performance falls behind. However, it is able to accomplish the task with suboptimal performance requiring, again, the equivalent of 1000 times less data. In the case of half-cheetah disabled, the performance reached closely matches the asymptotic performance of the model-free meta-RL method.

Figure 4: Compared to single-task model-free RL and model-free meta-RL methods, our method is achieves good performance with 1000 × less data. Dotted lines indicate performance at convergence.

6.3 FAST ADAPTATION & GENERALIZATION

For our second comparative evaluation, we evaluate both

GrBAL and ReBAL against 1) a non-adaptive model-based

method ("MB"), which employs a feedforward neural net-

work as the dynamics model and selects actions using

MPC, but does not perform adaptation (Nagabandi et al.,

2017a), 2) an adaptive control method ("MB + DE") based

on dynamic-evaluation, where gradient steps are taken to

adapt a model at run time (Krause et al., 2017), and 3)

TRPO (Schulman et al., 2015). In this comparison, we

operate in the low data regime, giving all of these methods

the same amount of data, in the interest of developing

efficient algorithms for real-world applications. This translates to 1.5-3 hours of real-world experience. We also

Figure 5: Simulated results in a variety of dynamic test environments. GreBAL outper-

provide the performance of a MB oracle, which is trained forms other methods, even the MB oracle, in

using unlimited data, and using only data from the given all the task where fast adaptation is neces-

test task (rather than being trained on the training tasks sary. These results highlights the difficulty of

and being required to generalize).

training a global model, and the importance

of adaptation. In these experiments, we test the ability of each approach

to adapt to sudden changes in the environment, as well as

to generalize beyond the training environments. We evaluate the fast adaptation (F.A.) component on

the HC disabled joint, ant crippled leg, and the HC pier. On the first two, we cause a joint/leg of the

robot to malfunction in the middle of a rollout. We evaluate the generalization component also on the

tasks of HC disabled joint and ant crippled leg, but this time, the leg/joint that malfunctions has not

been seen as crippled during training. The last environment that we test generalization on is the HC

slopped terrain for a hill, where the agent has to run up and down a steep slope, which is outside of

the gentle slopes that it experienced during training. The results, shown in Fig. 5, show returns that

are normalized such that the MB oracle achieves a return of 1.

In all tasks, due to low quantity of training data, TRPO performs poorly. Although MB+DE achieves better generalization than MB, the slow nature of its adaptation causes it to fall behind MB in the

7

Under review as a conference paper at ICLR 2019

Carpet Styrofoam Turf

GrBAL MB GrBAL MB GrBAL MB

Left Str Z-z F-8
4.07 3.26 7.08 5.28 3.94 3.26 6.56 5.21 3.90 3.75 7.55 6.01 4.09 4.06 7.48 6.54 1.99 1.65 2.79 3.40 1.87 1.69 3.52 2.61

Table 1: Trajectory following costs for real-world Gr-

BAL and MB results when tested on three terrains that

were seen during training. Tested here for left turn

(Left), straight line (Str), zig-zag (Z-z), and figure-8

shapes (F-8). The methods perform comparably, in-

dicating that online adaptation is not needed in the training terrains, but including it is not detrimental.

Figure 6: GrBAL clearly outperforms both MB and MB+DE, when tested on tasks that (1) require online

adaptation, and/or (2) were never seen during training.

environments that require fast adaptation. On the other hand, our approach surpasses the other approaches in all of the tasks. In fact, in the HC pier and the fast adaptation of ant environments, our approach surpasses the model-based oracle. This result showcases the importance of adaptation in stochastic environments, where even a model trained with a lot of data cannot be robust to unexpected occurrences or disturbances. ReBAL displays its strengths on tasks where longer sequential inputs allow it to better asses current task settings, but overall, GrBAL seems to perform better for both of these cases of generalization and fast adaptation.

6.4 REAL-WORLD RESULTS
To test our meta model-based RL method's sample efficiency, as well as its ability to perform fast and effective online adaptation, we applied GrBAL to a real legged millirobot. Due to the cost of running real robot experiments, we choose the better performing method (i.e., GrBAL) to evaluate on the real robot. This small 6-legged robot, as shown in Fig. 1 and Fig. 2, presents a modeling and control challenge in the form of highly stochastic and dynamic movement. This robot is an excellent candidate for online adaptation for many reasons: the rapid manufacturing techniques and numerous custom-design steps used to construct this robot make it impossible to reproduce the same dynamics each time, its linkages and other body parts deteriorate over time, and it moves very quickly and dynamically with bounding-style gaits; hence, its dynamics are strongly dependent on the terrain or task at hand.
The state space of the robot is a 24-dimensional vector, including center of mass positions and velocities, center of mass pose and angular velocities, back-EMF readings of motors, encoder readings of leg motor angles and velocities, and battery voltage. We define the action space to be velocity setpoints of the rotating legs. The action space has a dimension of two, since one motor on each side is coupled to all three of the legs on that side. All experiments are conducted in a motion capture room. Computation is done on an external computer, and the velocity setpoints are streamed over radio at 10 Hz to be executed by a PID controller on the microcontroller on-board of the robot.
We meta-train a dynamics model for this robot using the meta-objective described in Equation 3, and we train it to adapt on entirely real-world data from three different training terrains: carpet, styrofoam, and turf. We collect approximately 30 minutes of data from each of the three training terrains. This data was entirely collected using a random policy, in conjunction with a safety policy, whose sole purpose was to prevent the robot from exiting the area of interest.
Our first group of results (Table 1) show that, when data from a random policy is used to train a dynamics model, both a model trained with a standard supervised learning objective (MB) and a GrBAL model achieve comparable performance for executing desired trajectories on terrains from the training distribution.
Next, we test the performance of our method on what it is intended for: fast online adaptation of the learned model to enable successful execution of new, changing, or out-of-distribution tasks at test time. Similar to the comparisons above, we compare GrBAL to a model-based method (MB) that involves neither meta-training nor online adaptation, as well as a dynamic evaluation method that involves online adaptation of that MB model (MB+DE). Our results (Fig. 6) demonstrate that GrBAL substantially outperforms MB and MB+DE, and, unlike MB and MB+DE, and that GrBAL

8

Under review as a conference paper at ICLR 2019
can quickly 1) adapt online to a missing leg, 2) adjust to novel terrains and slopes, 3) account for miscalibration or errors in pose estimation, and 4) compensate for pulling payloads. None of these environments were seen during training time, but the agent's ability to learn how to learn enables it to quickly adapt its prior knowledge and fine-tune to learn new tasks online. Furthermore, the poor performance of the MB and MB+DE baselines demonstrate not only the need for adaptation, but also the importance of good initial parameters to adapt from (in this case, meta-learned parameters). The qualitative results of these experiments in Fig. 7 show that the robot is able to use our method to adapt online and effectively follow the target trajectories, even in the presence of new tasks and unexpected perturbations at test time.
Figure 7: The dotted black line indicates the desired trajectory in the xy plane. By effectively adapting online, our method prevents drift from a missing leg, prevents sliding sideways down a slope, accounts for pose miscalibration errors, and adjusts to pulling payloads (left to right). Note that none of these tasks were seen during training time, and they require fast and effective online adaptation for success.
7 CONCLUSION
In this work, we present an approach for model-based meta reinforcement learning that enables fast, online adaptation in dynamic environments. We show that meta-learning a model for online adaptation results in a method that is able to adapt to unseen situations or sudden and drastic changes in the environment, and is also sample efficient to train. We provide two instantiations of our approach (ReBAL and GrBAL), and we provide a comparison with other prior methods on a range of continuous control tasks. We show that our approach is practical for real-world in contrast to less efficient model-free meta-reinforcement learning approaches, and that the capability to adapt quickly is particularly important under complex real-world dynamics.
REFERENCES
M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and P. Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. CoRR, abs/1710.03641, 2017.
M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W. Hoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning to learn by gradient descent by gradient descent. CoRR, abs/1606.04474, 2016.
K. J. Åström and B. Wittenmark. Adaptive control. Courier Corporation, 2013.
A. Aswani, P. Bouffard, and C. Tomlin. Extensions of learning-based model predictive control for real-time application to a quadrotor helicopter. In American Control Conference (ACC), 2012. IEEE, 2012.
B. Baker, O. Gupta, N. Naik, and R. Raskar. Designing neural network architectures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.
Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. Université de Montréal, Département d'informatique et de recherche opérationnelle, 1990.
D. A. Braun, A. Aertsen, D. M. Wolpert, and C. Mehring. Learning optimal adaptation strategies in unpredictable motor tasks. Journal of Neuroscience, 2009.
M. P. Deisenroth, G. Neumann, J. Peters, et al. A survey on policy search for robotics. Foundations and Trends R in Robotics, 2(1­2):1­142, 2013.
Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. Rl$^2$: Fast reinforcement learning via slow reinforcement learning. CoRR, abs/1611.02779, 2016.
9

Under review as a conference paper at ICLR 2019
C. Finn and S. Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. CoRR, abs/1710.11622, 2017.
C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. CoRR, abs/1703.03400, 2017.
M. Fortunato, C. Blundell, and O. Vinyals. Bayesian recurrent neural networks. arXiv preprint arXiv:1704.02798, 2017.
J. Fu, S. Levine, and P. Abbeel. One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. CoRR, abs/1509.06841, 2015.
S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep q-learning with model-based acceleration. In International Conference on Machine Learning, pages 2829­2838, 2016.
B. Krause, L. Lu, I. Murray, and S. Renals. Multiplicative lstm for sequence modelling. arXiv preprint arXiv:1609.07959, 2016.
B. Krause, E. Kahembwe, I. Murray, and S. Renals. Dynamic evaluation of neural sequence models. CoRR, abs/1709.07432, 2017.
T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.
B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 2015.
I. Lenz, R. A. Knepper, and A. Saxena. Deepmpc: Learning deep latent features for model predictive control. In Robotics: Science and Systems, 2015.
S. Levine and V. Koltun. Guided policy search. In International Conference on Machine Learning, pages 1­9, 2013.
S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research (JMLR), 2016.
K. Li and J. Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015.
N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In NIPS 2017 Workshop on Meta-Learning, 2017.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 2015.
T. Munkhdalai and H. Yu. Meta networks. arXiv preprint arXiv:1703.00837, 2017.
T. Munkhdalai, X. Yuan, S. Mehri, T. Wang, and A. Trischler. Learning rapid-temporal adaptations. arXiv preprint arXiv:1712.09926, 2017.
A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. CoRR, abs/1708.02596, 2017a.
A. Nagabandi, G. Yang, T. Asmar, R. Pandya, G. Kahn, S. Levine, and R. S. Fearing. Learning image-conditioned dynamics models for control of under-actuated legged millirobots. arXiv preprint arXiv:1711.05253, 2017b.
D. K. Naik and R. Mammone. Meta-neural networks that learn by learning. In Neural Networks, 1992. IJCNN., International Joint Conference on, volume 1, pages 437­442. IEEE, 1992.
J. Peters and S. Schaal. Reinforcement learning of motor skills with policy gradients. Neural networks, 2008.
S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. International Conference on Learning Representations (ICLR), 2018.
M. Rei. Online representation learning in recurrent neural language models. CoRR, abs/1508.03854, 2015.
A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. One-shot learning with memory-augmented neural networks. arXiv preprint arXiv:1605.06065, 2016.
10

Under review as a conference paper at ICLR 2019
S. S. Sastry and A. Isidori. Adaptive control of linearizable systems. IEEE Transactions on Automatic Control, 1989.
J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 1992.
J. Schmidhuber and R. Huber. Learning to generate artificial fovea trajectories for target detection. International Journal of Neural Systems, 1991.
J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.
D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 2017.
F. Sung, L. Zhang, T. Xiang, T. Hospedales, and Y. Yang. Learning to learn: Meta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529, 2017.
M. Tanaskovic, L. Fagiano, R. Smith, P. Goulart, and M. Morari. Adaptive model predictive control for constrained linear systems. In Control Conference (ECC), 2013 European. IEEE, 2013.
S. Thrun and L. Pratt. Learning to learn: Introduction and overview. In Learning to learn. Springer, 1998. E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In IROS, pages 5026­5033.
IEEE, 2012. J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and
M. Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016. A. Weinstein and M. Botvinick. Structure learning in motor control: A deep reinforcement learning model.
CoRR, abs/1706.06827, 2017. G. Williams, A. Aldrich, and E. Theodorou. Model predictive path integral control using covariance variable
importance sampling. CoRR, abs/1509.01149, 2015. A. S. Younger, S. Hochreiter, and P. R. Conwell. Meta-learning with backpropagation. In International Joint
Conference on Neural Networks. IEEE, 2001.
11

Under review as a conference paper at ICLR 2019
A ENVIRONMENTS
Here we present all the detailed tasks an environments that are used in the result sections in the Appendix. Half-cheetah (HC): disabled joint. For each meta-training rollout, we randomly sample a joint to be disabled (i.e., the agent cannot apply torques to that joint). At test time, we evaluate performance in three different situations: (a) disabling a joint seen at train time, (b) disabling an unseen joint, and (c) switching between disabled joints during a rollout. HC: sloped terrain. During meta-training, we choose terrain of varying gentle upward and downward slopes. In this task, it is especially important to incorporate past experience into the model, since the cheetah has no means of directly observing the incline. At test time, we evaluate performance on (d) a gentle upward slope, (e) a steep hill that goes up and down, and (f ) a steep upward slope. HC: pier. In this task, the cheetah runs over a series of blocks that are floating on water. Each block moves up and down when stepped on, and the changes in the dynamics are drastic and rapid, due to each block having different damping and friction properties. The HC is meta-trained on varying these block properties, and tested on (g) a specific configuration of block properties. Ant: crippled leg. For each meta-training rollout, we randomly sample a leg on a quadrupedal robot and disable it. Disabling a leg unexpected drastically changes the dynamics. We evaluate on (h) crippling a leg from the training distribution, (i) crippling a leg from outside the training distribution, and (j) crippling a leg in the middle of a rollout. 7-DoF arm: force perturbations. We train a 7-DoF robot arm to carry an object to a goal position while applying random perturbation forces to the object. At test time, we evaluate with (k) a constant low force to the object, (l) a force 3× stronger than during training, and (m) a force that randomly changes every 50 time-steps. This allow us to evaluate the ability of our method to adapt online to perturbations that clearly lie outside the training distribution.
B MODEL PREDICTION ERRORS: PRE-UPDATE VS. POST-UPDATE
In this section, we show the effect of adaptation in the case of GrBAL. In particular, we show the histogram of the K step normalized error, as well as the per-timestep visualization of this error during a trajectory. Across all tasks and environments, the post-updated model p^ achieves lower prediction error than the pre-updted model p^ .
Figure 8: Histogram of the K step normalized error across different tasks. GrBAL accomplishes lower model error when using the parameters given by the update rule.
12

Under review as a conference paper at ICLR 2019

Figure 9: At each time-step we show the K step normalized error across different tasks. GrBAL accomplishes lower model error using the parameters given by the update rule.

C EFFECT OF META-TRAINING DISTRIBUTION
To see how training distribution affects test performance, we ran an experiment that used GrBAL to train models of the 7-DOF arm, where each model was trained on the same number of datapoints during meta-training, but those datapoints came from different ranges of force perturbations. We observe (in the plot below) that
1. Seeing more during training is helpful during testing -- a model that saw a large range of force perturbations during training performed the best
2. A model that saw no perturbation forces during training did the worst
3. The middle 3 models show comparable performance in the "constant force = 4" case, which is an out-of-distribution task for those models. Thus, there is not actually a strong restriction on what needs to be seen during training in order for adaptation to occur at train time (though there is a general trend that more is better)

D REWARD FUNCTIONS
For each MuJoCo agent, the same reward function is used across its various tasks. Table 2 shows the reward functions used for each agent. We denote by xt the x-coordinate of the agent at time t, eet refers to the position of the end-effector of the 7-DoF arm, and g corresponds to the position of the desired goal.

Table 2: Reward functions

Half-cheetah Ant 7-DoF Arm

Reward function

xt+1 -xt 0.01

- 0.05

at

2 2

xt+1 -xt 0.0e

- 0.005

at

2 2

+

0.05

-

eet - g

2 2

13

Under review as a conference paper at ICLR 2019

Figure 10: Effect of the meta-training distribution on test performance

E HYPERPARAMETERS

Below, we list the hyperparameters of our experiments. In all experiments we used a single gradient step for the update rule of GrBAL. The learning rate (LR) of TRPO corresponds to the Kullback­Leibler divergence constraint. # Task/itr corresponds to the number of tasks sampled for collecting data to train the model or model, whereas # TS/itr is the total number of times steps collected (for all tasks). Finally, T refers to the horizon of the task.

Table 3: Hyperparameters for the half-cheetah tasks

GrBAL ReBAL MB TRPO

LR Inner LR Epochs K M Batch Size # Tasks/itr # TS/itr T

nA Train H Train nA Test H Test

0.001 0.01 50 32 32 500

32

64000 1000 1000

10

2500 15

0.001 -

50 32 32 500

32

64000 1000 1000

10

2500 15

0.001 -

50 - - 500

64

64000 1000 1000

10

2500 15

0.05 -

-

- - 50000

50

50000 1000 -

---

Table 4: Hyperparameters for the ant tasks

LR Inner LR Epochs K M Batch Size # Tasks/itr # TS/itr T nA Train H Train nA Test H Test

GrBAL 0.001 0.001

50

10 16 500

32

24000 500 1000

15

1000 15

ReBAL 0.001 -

50 32 16 500

32

32000 500 1000

15

1000 15

MB 0.001 -

70 - - 500

10

10000 500 1000

15

1000 15

TRPO 0.05 -

-

- - 50000

50

50000 500 -

---

Table 5: Hyperparameters for the 7-DoF arm tasks

LR Inner LR Epochs K M Batch Size # Tasks/itr # TS/itr T na Train H Train na Test H Test

GrBAL 0.001 0.001

50

32 16 1500

32

24000 500 1000

15

1000 15

ReBAL 0.001 -

50 32 16 1500

32

24000 500 1000

15

1000 15

MB 0.001 -

70 - - 10000 10

10000 500 1000

15

1000 15

TRPO 0.05 -

-

- - 50000

50

50000 500 -

- --

14

