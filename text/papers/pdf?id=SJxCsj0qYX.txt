Under review as a conference paper at ICLR 2019
STACKELBERG GAN: TOWARDS PROVABLE MINIMAX EQUILIBRIUM VIA MULTI-GENERATOR ARCHI-
TECTURES
Anonymous authors Paper under double-blind review
ABSTRACT
We study the problem of alleviating the instability issue in the GAN training procedure via new architecture design. The discrepancy between the minimax and maximin objective values could serve as a proxy for the difficulties that the alternating gradient descent encounters in the optimization of GAN. In this work, we give new results on the benefits of multi-generator architecture of GANs. We show that the minimax gap shrinks to as the number of generators increases with rate O(1/ ). This improves over the best-known result of O(1/ 2). At the core of our techniques is a novel application of Shapley-Folkman lemma to the generic minimax problem, where in the literature the technique was only known to work when the objective function is restricted to the Lagrangian function of a constraint optimization problem. Our proposed Stackelberg GAN performs well experimentally in both synthetic and real-world datasets, improving Fréchet Inception Distance by 14.61% over the previous multi-generator GANs on the benchmark datasets.
1 INTRODUCTION
Generative Adversarial Nets (GANs) are an emerging object of study in machine learning, computer vision, natural language processing, and many other domains. In machine learning, study of such a framework has led to significant advances in adversarial defenses (Xiao et al., 2018; Samangouei et al., 2018) and machine security (Athalye et al., 2018; Samangouei et al., 2018). In computer vision and natural language processing, GANs have resulted in improved performance over standard generative models for images and texts (Goodfellow et al., 2014), such as variational autoencoder (Kingma & Welling, 2013) and deep Boltzmann machine (Salakhutdinov & Larochelle, 2010). A main technique to achieve this goal is to play a minimax two-player game between generator and discriminator under the design that the generator tries to confuse the discriminator with its generated contents and the discriminator tries to distinguish real images/texts from what the generator creates. Despite a large amount of variants of GANs, many fundamental questions remain unresolved. One of the long-standing challenges is designing universal, easy-to-implement architectures that alleviate the instability issue of GAN training. Ideally, GAN is supposed to solve the minimax optimization problem (Goodfellow et al., 2014), but in practice alternating gradient descent methods do not clearly privilege minimax over maximin or vice versa (page 35, Goodfellow (2016)), which may lead to instability in training if there exists a large discrepancy between the minimax and maximin objective values. The focus of this work is on improving the stability of such minimax game in the training process of GANs. To alleviate the issues caused by the large minimax gap, our study is motivated by the so-called Stackelberg competition in the domain of game theory. In the Stackelberg leadership model, the players of this game are one leader and multiple followers, where the leader firm moves first and then the follower firms move sequentially. It is known that the Stackelberg model can be solved to find a subgame perfect Nash equilibrium. We apply this idea of Stackelberg leadership model to the architecture design of GANs. That is, we design an improved GAN architecture with multiple generators (followers) which team up to play against the discriminator (leader). We therefore name our model Stackelberg GAN. Our theoretical and experimental results establish that: GANs with multi-generator architecture have smaller minimax gap, and enjoy more stable training performances.
1

Under reviUewnadsear croenvfieerwencaespaapceornafteIrCeLnRce20p1a9per at ICLR 2019
Under review as a conference paper at ICLR 2019

(a) Step 0.

(a) Step 0.

(b) Step 6k.

(c) Step 19k.

(d) Step 25k.

(b) Step 6k.

(c) Step 13k.

(d) Step 19k.

(e) Step 25k.

(f) Step 0.

(g) Step(6ek).Step 0. (h) Step 13k(f.) Step 6k(i.) Step 19k. (g) Step(j1)9Skte.p 25k. (h) Step 25k.

Figure 1: StackFeilgbuerrgeG1A: NSFtsaitcagkbueilrlibzeeesr1gth:GeLtAreaNifntisnFtgapbigrioluiczeredesu,rtehTeoontrpaatioRnyino2gDwpm:roiSxctetuadrenuordefa8orGndaauGtsosAiyanN2sD. tTrmoapiixntiunregoof n8 Ga atuosysia2nDs. mToipxture of 8 Gaussians. Left eGRaoAcwhN:gtSernataiennraidntaogrrdeRGweaGoAxictawAhhNcN:8tglytSegtrrnealtaeanieiannerrFdfraiidnnanatsaetigogsorgon.rdhrnwIuoeetieGxnoritsmteshaeAhneocdo,m-Nw8tdMlBebsbygtlotryeoelNhafsenat,itaotInetherSosreinanenacmTdsvetghioeos.rocrdtanrIRfolieatebwmnltsoumoahshtowieisordoocm.ewenhd:stebwsiRsawSloitredtihethfsgeiaaod,tnththchureoostetkoatpeeaceupddvFnheliyetbsdibroygmtc.aerfBhuloiorbwndmoegrueetrhettorcobiiG:docoymeenlhiASonspRrgwit.NsaioadcWirwdtcrekhoee:tkpod,nrcSepurawoatoeltanitdbpcenha.skpdenieeeenreyrlbdgbgetym.ehraGeBogowtandoAiceettthNhcobomer8liotonrRwrgag.oideWncwrnooie:enprcSrgpraaeteantsdwcop.skrieoetelhnebtndeh1rssag0ettmogebonlneeesr,agteeonarceehrantosoefrm.wbhliecshoins

Our Contributions. This paper tackles the problem of instability during the GAN training procedure with both theoretical and experimental results. We study this problem by new architecture design.

(a) Step 0. ·

OWuer(pbf)rroSatpmepoe1s3wek.othrke

Sistag(cce)kSneteelprba2e5lrkgs. inGcAeNit

framework of multiple generators in the can be applied to all variants of GANs,

GAN architecture. e.g., vanilla GAN,

Wasserstein GAN, etc. It is built upon the idea of jointly optimizing an ensemble of GAN

losses w.r.t. all pairs of discriminator and generator.

Differences from prior work. Although the idea of having multiple generators in the GAN

(d) Step 0. arch(ei)teStceptu1r3ek. is not (tf)oSttaepll2y5kn. ew, e.g., MIX+GAN (Arora et al., 2017), MGAN (Hoang et al.,

eGFRaioAgcwuhNr:getSe·r2nat:aieWnSnraidtenataocgprrkdwrFFBowoeeniiGolipxiggbtetotaAhhueutcsrcoroNer8tgeloumeySgGtl2t,rotelAaa:cTeRnricah.SNneokroeritnpeRnawsarslttgrbciRoa:oy.gekbrnrIohSeipegtelwlitntisbzGamcsheF:ecekoAsormkiSw,gdgtNehtewbsu2capdaGellfbotrneanh0irrthAfesreafodepad1a,fmrt:Nrtaihbgaee8nesreeSaMcairedw)sGncedtevibt,hgaoaiatGeAnAsciMyrcrbpotklacAkNrh.rfiiDlioleoesbAwiNmIrclfzuttso-nebhrietohDGdwaitciesbdaocrucirveanhn-rtAegmcoisehGiinwtionnensaoGNwgoriiArgndttntremAhrrdeeeagwaNoda(nsueNe.iturGposliononltttItytoiis(phettaph,nrGnpd2Snlsag8woieedDyhbnitdhsspnggaoymi.moehritwettcBoonnoohhisnkxnecdgoseheetoreeeteuaattrtwdnhcrlobtaSeseoubeoameatistrtoltroitelsuognaheafr.Rsrrgie,cm.8nloeeg1no.kdWvnn2G0,twerpheeGso0raea2:egetrlapu1iacbS0mtAeGptsooalots7ne1eAanybinrmNdeac)r.7lN.snrk2g,eoeW)atseDsaeh.advtlG,aborenTetaacmeehnroshdAtcrraapdgiieiatac-xtonNpnhrhtGesuuresoeirdsmeMmeofergwroogobeAtfplhewhdnep8aNnieseceotGelhdroerr(aaw.nkaiaDutscL.otreshduoeesareaifurgrs)atnlusennoFIaingsimantnei.fkegroraLddMaeaugretrrbmeodfceGysetri,olstAymcarNelbi.nm,in(s2Hiee0nmdo1aat6baon)lsre,gaththehametevraeeillxo.ia,tsnurs2fiere0ensk1ieoo8tyeff)

DemGaMtGetriAAccoGfatfhdNNuleAi.err,te,leNeelsx2Wo.cn0astwc(Ouas1Hciepnesiur8ts[toesslrPhrt)eywaro,w·ifrenoasristcl.agtshndtrenhesemWt.etaegeuoui.rdpnterctmetcearnwuaattGiltaupolsoplrioo.roAtt,rerprreioa:e2koona"Nwln.kl0Inip,iyrtoi,ee1sOnsoatrey8inhgmsskotu)etcaee.dcf,hrw.otnkiAvodveiutoSfeIefd,tiafseithnnlrrattnseferetaaaeirbcihvdr.rasonlmgamecrooeeiigiuntbn.nkeoloumfh,eagesuc.,ecgcanfMutw,ceiiiiglhhtilnneewmsabnsteh2oIcraenstneuXabbgathrh0spsoetiprdnekreg+ongtrraio1ttouiaarbGwicigseianrsetdp7GrsetnonyhnAeerttatsr)deahgdiAbaaensNb,epntbedegneadhoraNupnneetiiS(fartrtaltddhsnAweeticdithteefetiraeocreocsraraadeecranonycaoovGaukrleostrrommcitl.oueiawnofsetonAmeulhbgrerjabeai.s,oiIaiawlitteNnsmnrlnsihntttrgnaecaahvohgesoucldttaeh.ylreowsltueat,yGrktaashrniint2"sapoqAbiaeho0SeantltpabtuxrieaN1hnfhthtltevseimi7epaySgthemoeeay)oaesicratumenxifnaepwetrakvzadtninneGtepiuseoiconnrdonerssplsAaadlkgeuindbgriettiMeneNgoievgaoeomdtmhyrmanfsGbalehrsr,bstpegcleuAbwotioaekpriannesl.rondlNssoGpgtsoaraetbigesra.ebhglp,wkaaAmid(culeeHbslvd.ibcevNbmtaeiGeGriyoalwooalnonirg.de)aAsrilupsAglneltiiIIooaNlitatptbtnngnacfaiNhfpnetleyoobotdsres,nsail.eoAlaetiiowznocoaiffrserghanroesGsnsrrmigAttaeantahiheoldpNlwteenedhpmsteweam,tlrrai.yyeGsaaelo.Is.igtiAnfnedoig.r(n,iqNert2onhghvlug0mat.aae]sacl1nrrlcai8uaelhplw)naniaa.-ddiecfbyiioa)gtraynhImlnltieosn.MxwWfotorsIuaeXrrimfir+aaelpnnGlgladiuAglcylieiNatstnrsisies(lzci.ArgaaaTrthltioiohtonrlringyass

ocsuetettonfarnnamenteladr.,cc,aaewhsr2sdth0sg,ai1eGicrn7nyhAe),tbrdNhaetiehtscoDGGaewcearSortu.iiAAcltutfsaoTahfhreNNlsceooai.sktrg,tu,heteeelehts2eWolcnseba0saetcnteucaxsr1he·yreoesprge8snessrae)weetwGsnaWId,rwsirsaensssAinstiur.ieithstrvgyemnhdmNe.tec,ehtohip.ribpptrnpontmsetiliarioresmtbobGtilowoadoholnedtpreAteviripaweeanornaSelaNiwlgniopakltntiwayhfrot,tetteocshheeoreyilknoreaemkotar,euceca.fdfsmlthnwarw.bpiAndiroftewgIf,erfhiearodtlesdegeotwrenheichr.mpslevGwgaeriorcmyilrlben.Aaeumyea,otuicdfpgoNgMirveeiiiaronhhlnntsgcitwmatIaisdsitliuXumbteethrsyeouaempeae.a+nanraotuprbfddGwbiianxpfaolndy)oenlaAenercytrIanddehtmnmaltlNhelneueeeoge.qMoxaweaaeuiWS(ftldlInrAsalrsotXhaeiela.esgiitraamws+crfirnyoucveaeGkpneorsgi)taieltadgnofAiughorlcIgrahljibeNafi.anetottpternsmsvoisi(rlnaGzacftAigrsuolgartaey.lihrrtlhlM,oeyGioittoanriu2swrlpolAingyaslA0nlpfheN1otkNyi7gmrsa)emoni(anazDdunseiunrdrpultagthrtmMrioiae-uordnoGrgsnideAkwisunnecaNomslrtrer.hikm(bmeeH.etbGioarlnae)aAolanI.oNtf,ngfo2gr0ean1rec6rh)a,ittteohcrestrueirneacrwreeoamrskeussltw(ispeelleel.

generators. MGAN (HoaTnhgeeot rael.m, 2011a8n),dvaCriooruoslglaenryera2t)o.rsUanrelickoemtbhineepdraesvaiomuisxtwuroerokf,poroubrarbeilsiustlitc has no assumption on the

·

TeWxheperoperrseosmvivee1tphaoanwtdtehCscmretoooaonmrfnodtgdilrenelaaalnissrmreydtr,waa2xGti)ion.tdrAhUsuWetaNanhaxlnlsieiidptkswytuehdrSgiemtttsahaehscpepcrsxoiktsipmutivherrotierealnnivbanaipnecktotohyrsuoorgsa,awnabtswGsduettosthhiArurtieeknimoN,nosgtofupeneummatgnridoboeeordeennnrdersauepeoottellorhftnnargwhedstteasohenseaoerexnnnusrodmapntthaaroiodesnferisodisrdsusrenicsmmnlodricpvniclirmt-ysaeeicoacpionsenparnenaiovcosmsnetiw(otxesytiirmhete.nyeehrb.abalto)veofeIrtn,hgeebMneounlIuoteXgsirs+nhaeGstscotAaeropasNfa,dcvw(idaAtryeeir.oposuIerhnasnodws

on their non-convexity. that Stackelberg GAN

With extra condeittioanl.o,n2th0e17ex)pi,srtehsasebivleoepstsooewsearacorhfeigeeenvnseereamto-brasl,pewdperwoshixtohiwmltehaaattrenSeteadqckuweilebliegbrghrtiGsuAmaNndwainthexOtr(a1r/egu)lgareiznaetrioantors (see Theorem 3). This

is able to achievteer-map,pwrohxiicmhaitdemiesqpcuoriuolirbvareigusemsowtvhiteehrwOete(hi1ge/h)btsgeebsnetei-rnkagtnortosowo(sefnearTrahewesouareylmtfir3no).m(TAhuinsrioforarme.tWael.,fin2d01it7s)ligwhthlyich requires generators as

ogunefnneeearccaehtsosgraesrn.yemFbraeotacolankru.ymsTeoaat2nshthelOeeecxm(op1nmr/etrsaas2ritvy)o,e. itpnhAotewhteegtrSehonteafececrkaoiecclrhbemegreoignnfeGirmoAautaoNrrxwatpelerrceoahapdbnpylileqyamlueloeq,wuswasilhismwerapeeiligncihinottsvstcefhoalerliaanliplgtleprlaictuatrieotnheoftetchheniSqhuaepwleays-

· TWheeoprreomve1thaancootdonftnhlCtyhesotemkrroancilinloonaiwrvmtyeona2rxpi)tn.todiUgumwannilloiziutkraymketgitowbahpneehrsppehranrerorivngtbihkoulesuemsamoswebnt(ojhZetrekcha,ntsaiouvnumiengrbrf(eeeuArstnuoarlcftlot.ghi,roeaa2nsn0eenir1tosa8ataro)lse.r.s,ssuTt2imrnh0icpci1rsttei7eoard)nes.esotsWuon(lstttehseheeeainlLstoaiggnhroattenergtbihaonautfnMudnsIcXtthi+oannGotAhfNaat

i2s0a17h)e, uwrihsitliec tmhios dpealpwerh2picrohvdidoeess

not exactly match the theoretical analysis in (Arora formal guarantees for the exact model of Stackelberg

et al., GAN.

· We empirically study the performance of Stackelberg GAN for various synthetic and real datasets. We observe that without any human assignment, surprisingly, each generator automatically learns balanced number of modes without any mode being dropped (see Figure 1). Compared with other multi-generator GANs with the same network capacity, our experiments show that Stackelberg GAN enjoys 26.76 Fréchet Inception Distance on CIFAR10 dataset while prior results achieve 31.34 (smaller is better), achieving an improvement of 14.61%.

2

Under review as a conference paper at ICLR 2019

Generators(G)
!~#$

...

Discriminator(D)

Real Fake

%~#&

Loss= (), + +  +  (., +

Figure 2: Architecture of Stackelberg GAN. We simply ensemble the losses of various generator and discriminator pair with equal weights.
2 STACKELBERG GAN

Before proceeding, we define some notations and formalize our model setup in this section.

Notations. We will use bold lower-case letter to represent vector and lower-case letter to represent

scalar. Specifically, we denote by   Rt the parameter vector of discriminator and   Rg the

parameter vector of generator. Let D(x) be the outputted probability of discriminator given input x, and let G(z) represent the generated vector given random input z. For any function f (u), we

denote by (concave)

f (v) := closure of

sfu,pwuh{iucThvis-deffi(nue)d}atshethceofnujungctaitoenfuwnhcotisoeneopfigfra. pLhe(tsculbfg(rcalpfh))bies

the the

convex convex

(concave) closed hull of that of function f . We will use I to represent the number of generators.

2.1 MODEL SETUP
Preliminaries. The key ingredient in the standard GAN is to play a zero-sum two-player game between a discriminator and a generator -- which are often parametrized by deep neural networks in practice -- such that the goal of the generator is to map random noise z to some plausible images/texts G(z) and the discriminator D(·) aims at distinguishing the real images/texts from what the generator creates. For every parameter implementations  and  of generator and discriminator, respectively, denote by the payoff value

(; ) := ExPd f (D(x)) + EzPz f (1 - D(G (z))), where f (·) is some concave, increasing function. Hereby, Pd is the distribution of true images/texts and Pz is a noise distribution such as Gaussian or uniform distribution. The standard GAN thus solves the following saddle point problems:

inf sup (; ), or sup inf (; ).

Rg Rt

Rt Rg

(1)

For different choices of function f , problem (1) leads to various variants of GAN. For example, when f (t) = log t, problem (1) is the classic GAN; when f (t) = t, it reduces to the Wasserstein GAN. We refer interested readers to the paper of Nowozin et al. (2016) for more variants of GANs.

Stackelberg GAN. Our model of Stackelberg GAN is inspired from the Stackelberg competition in the domain of game theory. Instead of playing a two-player game as in the standard GAN, in Stackelberg GAN there are I + 1 players with two firms -- one discriminator and I generators. One can make an analogy between the discriminator (generators) in the Stackelberg GAN and the leader (followers) in the Stackelberg competition.

Stackelberg GAN is a general framework which can be built on top of all variants of standard GANs. The objective function is simply an ensemble of losses w.r.t. all possible pairs of generators and

discriminator: (1, ..., I ; ) := GAN therefore solves the following

iIs=ad1dl(epio; in)t.

Thus it is very problems:

easy

to

implement.

The

Stackelberg

w

:=

inf
1,...,I Rg

sup
Rt

1 I

(1,

...,

I ;

),

or

q

:=

sup
Rt

inf
1,...,I Rg

1 I

(1

,

...,

I

;

).

3

Under review as a conference paper at ICLR 2019

We term w - q the minimax (duality) gap. We note that there are key differences between the naïve ensembling model and ours. In the naïve ensembling model, one trains multiple GAN models independently and averages their outputs. In contrast, our Stackelberg GAN shares a unique discriminator for various generators, thus requires jointly training. Figure 2 shows the architecture of our Stackelberg GAN. How to generate samples from Stackelberg GAN? In the Stackelberg GAN, we expect that each generator learns only a few modes. In order to generate a sample that may come from all modes, we use a mixed model. In particular, we generate a uniformly random value i from 1 to I and use the i-th generator to obtain a new sample. Note that this procedure in independent of the training procedure.

3 ANALYSIS OF STACKELBERG GAN
In this section, we develop our theoretical contributions and compare our results with the prior work.

3.1 MINIMAX DUALITY GAP

We begin with studying the minimax gap of Stackelberg GAN. Our main results show that the minimax gap shrinks as the number of generators increases.

To the

psreoccoenedda, rdgeunmoetentboyfhi((uii;)·):.=InitnufitiiveRlgy(, -his(eriv;e·)s)as(uani)a, pwphroexreimthaeteccoonnjuvegxaitfiecoaptieornatoiofn-is

w.r.t. w.r.t

the second argument due to the conjugate operation. Denote by clhi the convex closure of hi:



 t+2

t+2 t+2



clhi(u)

:=

inf
{aj },{uji

}


j

=1

aj

hi

(uji

)

:

u

=

aj uji , aj

j=1

j=1

=

1, aj



0 .

clhi represents the convex relaxation of hi because the epigraph of clhi is exactly the convex hull of

epigraph

of

hi

by

the

definition

of

clhi.

Let

m inimax

=

inf 1,...,I Rg

supRt

1 I

(1,

...,

I

;

)

-

inf 1,...,I Rg

supRt

1 I

(1,

...,

I

;

),

and

maximin

=

supRt

inf 1,...,I Rg

1 I

(1,

...,

I

;

)

-

supRt

inf 1,...,I Rg

1 I

(1,

...,

I

;

),

where

(1,

...,

I

;

)

:=

I i=1

cl(i;

)

and

cl(i

;

)

is

nthoen-ccoonncvaevxeitcyloosfuorebjoefctiv(eif;un)ctwio.rn.t.w.ar.rtg.uamrgeunmt e.ntTh.eFreofroerex,ampmlaex,imitinis+equmalintimoa0x

measures the if and only if

(i; ) is concave and closed w.r.t. discriminator parameter .

We have the following guarantees on the minimax gap of Stackelberg GAN.

Theorem 1. Let i := supuRt {hi(u) - clhi(u)}  0 and worst := maxi[I] i . Denote by t the number of parameters of discriminator, i.e.,   Rt. Suppose that hi(·) is continuous and domhi is compact and convex. Then the duality gap can be bounded by

0  w - q  minimax + m aximin + ,

provided that the number of generators I > t+1 worst.

Remark 1. Theorem 1 makes mild assumption on the continuity of loss and no assumption on the model capacity of discriminator and generators. The analysis instead depends on their non-

convexity as being parametrized by deep neural networks. In particular, i measures the divergence

between the function value of hi and its convex relaxation clhi; When (i; ) is convex w.r.t. argument i, i is exactly 0. The constant worst is the maximal divergence among all generators,

which does not grow with the only one generator and when

increase of I. This each generator for

iesxabmecpaluesheas wthorest

measures the divergence of same architecture, we have

ofwdoirsstcr=imin1at=or...W. =hen Ith. eSidmisiclarrimlyi,ntahetotrerismcsoncmainviemasxucahndaslmogaxisimtiinc

characterize the non-convexity regression and support vector

mmiancihminaex,duamliintiymagxa=p of Smtaaxcimkeinlb=er0g

and we GAN.

have

the

following

straightforward

corollary

about

the

Corollary 2. Under the settings of Theorem 1, when (i; ) is concave and closed w.r.t. discriminator parameter  and the number of generators I > t+1 worst, we have 0  w - q  .

4

Under review as a conference paper at ICLR 2019

3.2 EXISTENCE OF APPROXIMATE EQUILIBRIUM

The results of Theorem 1 and Corollary 2 are independent of model capacity of generators and discriminator. When we make assumptions on the expressive power of generator as in (Arora et al., 2017), we have the following guarantee (2) on the existence of -approximate equilibrium.

Theorem 3. Under the settings of Theorem 1, suppose that for any  > 0, there exists a generator G

such that ExPd,zPz G(z) - x 2  . Let the discriminator and the generators be L-Lipschitz RGw.er.l1ta.,t.ie.n.dp, GuWtsoIarnkadn. dWpaahrdialimescmertieamrnsyi,n1reae,ftfso.o.Rpr.re,ttDcs,tIhivaevsleyuR.cbghTe,hethennadtfe((ofvoror11at,,sen..od..y..m,,toe>IIev;;ma0lpu),)ietrhiVceVarVelly-e+Rxiin,s.vt,eIst=igatt+in1gthwoersptegrefonremraatno(c2res)

of multi-generator GAN, little is known about how many generators are needed so as to achieve

certain equilibrium guarantees. Probably the most relevant prior work to Theorem 3 is that of (Arora

et

al.,

2017).

In

particular,

Arora

et

al.

(2017)

showed

that

there

exist

I

=

100t
2

2

generators

and

one

discriminator such that -approximate equilibrium can be achieved, provided that for all x and any

 > 0, bound

there exists a generator of function |f |, i.e., f 

G such that [-, ]. In

cEozmpPazrisGo(nz, )T-heoxre2m3

. Hereby,  improves over

is a this

global result

upper in two

aspects: a) the assumption on the expressive power of generators in (Arora et al., 2017) implies our

condition ExPd,zPz G(z) - x 2  . Thus our assumption is weaker. b) The required number of generators in Theorem 3 is as small as t+1 worst. We note that worst 2 by the definition of

worst. Therefore, Theorem 3 requires much fewer generators than that of (Arora et al., 2017).

4 ARCHITECTURE, CAPACITY AND MODE COLLAPSE/DROPPING

In this section, we empirically investigate the effect of network architecture and capacity on the mode collapse/dropping issues for various multi-generator architecture designs. Hereby, the mode dropping refers to the phenomenon that generative models simply ignore some hard-to-represent modes of real distributions, and the mode collapse means that some modes of real distributions are "averaged" by generative models. For GAN, it is widely believed that the two issues are caused by the large gap between the minimax and maximin objective function values (see page 35, Goodfellow (2016)).

Our experiments verify that network capacity (change of width and depth) is not very crucial for resolving the mode collapse issue, though it can alleviate the mode dropping in certain senses. Instead, the choice of architecture of generators plays a key role. To visualize this discovery, we test the performance of varying architectures of GANs on a synthetic mixture of Gaussians dataset with 8 modes and 0.01 standard deviation. We observe the following phenomena:

Naïvely increasing capacity of one-generator architecture does not alleviate mode collapse. It shows that the multi-generator architecture in the Stackelberg GAN effectively alleviates the mode collapse issue. Though naïvely increasing capacity of one-generator architecture alleviates mode dropping issue, for more challenging mode collapse issue, the effect is not obvious (see Figure 3).

(a) GAN with 1 generator of architecture 2-128-2.

(b) GAN with 1 generator of architecture 2-128-256512-1024-2.

(c) Stackelberg GAN with 8 generators of architecture 2-16-2.

Figure 3: Comparison of mode collapse/dropping issue of one-generator and multi-generator architec-

tures with varying model capacities. (a) and (b) show that increasing the model capacity can alleviate

the mode dropping issue, though it does not alleviate the mode collapse issue. (c) Multi-generator

architecture with even small capacity resolves the mode collapse issue.

Stackelberg GAN outperforms multi-branch models. We compare performance of multi-branch GAN and Stackelberg GAN with objective functions:

(Multi-Branch GAN)



1 I

I

i; 

i=1

vs.

(Stackelberg GAN)

1 I

I

(i; ).

i=1

5

Under review as a conference paper at ICLR 2019

Hereby, the multi-branch GAN has made use of extra information that the real distribution is Gaussian

cmoimxtpuorenemnot.deHl wowitehvperro, beavbeinlittyhidsiswtreibuotbisoenrvfuentchtaiotnwI1ith

sIia=m1 epNmi (oxd)e,l

so that each i tries to fit one capacity, Stackelberg GAN

significantly outperforms multi-branch GAN (see Figure 4 (a)(c)) even without access to the extra

information. The performance of Stackelberg GAN is also better than multi-branch GAN of much

larger capacity (see Figure 4 (b)(c)).

(a) 8-branch GAN with generator architecture 2-16-2.

(b) 8-branch GAN with generator architecture 2-128256-512-1024-2.

(c) Stackelberg GAN with 8 generators of architecture 216-2.

Figure 4: Comparison of mode collapse issue of multi-branch and multi-generator architectures with varying model capacities. (a) and (b) show that increasing the model capacity can alleviate the mode dropping issue, though it does not alleviate the mode collapse issue. (c) Multi-generator architecture with much smaller capacity resolves the mode collapse issue.

Generators tend to learn balanced number of modes when they have same capacity. We observe that for varying number of generators, each generator in the Stackelberg GAN tends to learn equal number of modes when the modes are symmetric and every generator has same capacity (see Figure 5).

(a) Two generators.

(b) Four generators.

(c) Six generators.

Figure 5: Stackelberg GAN with varying number of generators of architecture 2-128-256-512-1024-2.

5 EXPERIMENTS
In this section, we verify our theoretical contributions by the experimental validation.
5.1 MNIST DATASET We first show that Stackelberg GAN generates more diverse images on the MNIST dataset (LeCun et al., 1998) than classic GAN. We follow the standard preprocessing step that each pixel is normalized via subtracting it by 0.5 and dividing it by 0.5. The detailed network setups of discriminator and generators are in Table 4. Figure 6 shows the diversity of generated digits by Stackelberg GAN with varying number of generators. When there is only one generator, the digits are not very diverse with many repeated "1"'s and much fewer "2"'s. As the number of generators increases, the generated images tend to be more diverse. In particular, for 10-generator Stackelberg GAN, each generator is associated with one or two digits without any digit being missed.

5.2 FASHION-MNIST DATASET
We also observe better performance by the Stackelberg GAN on the Fashion-MNIST dataset. FashionMNIST is a dataset which consists of 60,000 examples. Each example is a 28 × 28 grayscale image associating with a label from 10 classes. We follow the standard preprocessing step that each pixel is normalized via subtracting it by 0.5 and dividing it by 0.5. We specify the detailed network setups of discriminator and generators in Table 4. Figure 7 shows the diversity of generated fashions by Stackelberg GAN with varying number of generators. When there is only one generator, the generated images are not very diverse without

6

Under review as a conference paper at ICLR 2019
Figure 6: Standard GAN vs. Stackelberg GAN on the MNIST dataset without cherry pick. Left Figure: Digits generated by the standard GAN. It shows that the standard GAN generates many "1"'s which are not very diverse. Middle Figure: Digits generated by the Stackelberg GAN with 5 generators, where every two rows correspond to one generator. Right Figure: Digits generated by the Stackelberg GAN with 10 generators, where each row corresponds to one generator.
Figure 7: Generated samples by Stackelberg GAN on CIFAR-10 dataset without cherry pick. Left Figure: Examples generated by the standard GAN. It shows that the standard GAN fails to generate bags. Middle Figure: Examples generated by the Stackelberg GAN with 5 generators, where every two rows correspond to one generator. Right Figure: Examples generated by the Stackelberg GAN with 10 generators, where each row corresponds to one generator. any bags being found. As the number of generators increases, the generated images tend to be more diverse. In particular, for 10-generator Stackelberg GAN, each generator is associated with one class without any class being missed. 5.3 CIFAR-10 DATASET We then implement Stackelberg GAN on the CIFAR-10 dataset. CIFAR-10 includes 60,000 32×32 training images, which fall into 10 classes (Krizhevsky & Hinton, 2009)). The architecture of generators and discriminator follows the design of DCGAN in (Radford et al., 2015). We train models with 5, 10, and 20 fixed-size generators. The results show that the model with 10 generators performs the best. We also train 10-generator models where each generator has 2, 3 and 4 convolution layers. We find that the generator with 2 convolution layers, which is the most shallow one, performs the best. So we report the results obtained from the model with 10 generators containing 2 convolution layers. Figure 8a shows the samples produced by different generators. The samples are randomly drawn instead of being cherry-picked to demonstrate the quality of images generated by our model. For quantitative evaluation, we use Inception score and Fréchet Inception Distance (FID) to measure the difference between images generated by models and real images. Results of Inception Score. The Inception score measures the quality of a generated image and is correlated well with human's judgment (Salimans et al., 2016). We report the Inception score obtained by our Stackelberg GAN and other baseline methods in Table 1. For fair comparison, we only consider the baseline models which are completely unsupervised model and do not need any label information. Instead of directly using the reported Inception scores by original papers, we replicate the experiment of MGAN using the code, architectures and parameters reported by their original papers, and evaluate the scores based on the new experimental results. Table 1 shows that our model achieves a score of 7.62 in CIFAR-10 dataset, which outperforms the state-of-the-art models.
7

Under review as a conference paper at ICLR 2019

(a) Samples on CIFAR-10.

(b) Samples on Tiny ImageNet.

Figure 8: Examples generated by Stackelberg GAN on CIFAR-10 (left) and Tiny ImageNet (right) without cherry pick, where each row corresponds to samples from one generator.
For fairness, we configure our Stackelberg GAN with the same capacity as MGAN, that is, the two models have comparative number of total parameters. When the capacity of our Stackelberg GAN is as small as DCGAN, our model improves over DCGAN significantly. Results of Fréchet Inception Distance. We then evaluate the performance of models on CIFAR-10 dataset using the Fréchet Inception Distance (FID), which better captures the similarity between generated images and real ones (Heusel et al., 2017). As Table 1 shows, under the same capacity as DCGAN, our model reduces the FID by 20.74%. Meanwhile, under the same capacity as MGAN, our model reduces the FID by 14.61%. This improvement further indicates that our Stackelberg GAN with multiple light-weight generators help improve the quality of the generated images.

Table 1: Quantitative evaluation of various GANs on CIFAR-10 dataset. All results are either reported by the authors themselves or run by us with codes provided by the authors. Every model is trained without label. Methods with higher inception score and lower Fréchet Inception Distance are better.

Model

Inception Score Fréchet Inception Distance

Real data WGAN (Arjovsky et al., 2017) MIX+WGAN (Arora et al., 2017) Improved-GAN (Salimans et al., 2016) ALI (Dumoulin et al., 2017) BEGAN (Berthelot et al., 2017) MAGAN (Wang et al., 2017) GMAN (Durugkar et al., 2016) DCGAN (Radford et al., 2015) Ours (capacity as DCGAN) D2GAN (Nguyen et al., 2017) MGAN (our run) (Hoang et al., 2018) Ours (capacity 1×MGAN 1.8×DCGAN)

11.24 ± 0.16 3.82 ± 0.06 4.04 ± 0.07 4.36 ± 0.04 5.34 ± 0.05
5.62
5.67 6.00 ± 0.19 6.40 ± 0.05 7.02 ± 0.07 7.15 ± 0.07 7.52 ± 0.1 7.62 ± 0.07

37.7 29.88 31.34 26.76

5.4 TINY IMAGENET DATASET We also evaluate the performance of Stackleberg GAN on the Tiny ImageNet dataset. The Tiny ImageNet is a large image dataset, where each image is labelled to indicate the class of the object inside the image. We resize the figures down to 32 × 32 following the procedure described in (Chrabaszcz et al., 2017). Figure 8b shows the randomly picked samples generated by 10-generator Stackleberg GAN. Each row has samples generated from one generator. Since the types of some images in the Tiny ImageNet are also included in the CIFAR-10, we order the rows in the similar way as Figure 8a .
6 CONCLUSIONS
In this work, we tackles the problem of instability during GAN training procedure, which is caused by the huge gap between minimax and maximin objective values. The core of our techniques is a multi-generator architecture. We show that the minimax gap shrinks to as the number of generators increases with rate O(1/ ). This improves over the best-known results of O(1/ 2). Experiments verify the effectiveness of our proposed methods.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (GANs). arXiv preprint arXiv:1703.00573, 2017.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics. In International Conference on Learning Representations, 2018.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
David Berthelot, Thomas Schumm, and Luke Metz. BEGAN: boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.
D Bertsekas. Min common/max crossing duality: A geometric view of conjugacy in convex optimization. Lab. for Information and Decision Systems, MIT, Tech. Rep. Report LIDS-P-2796, 2009.
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an alternative to the CIFAR datasets. arXiv preprint arXiv:1707.08819, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. In International Conference on Learning Representations, 2017.
Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. arXiv preprint arXiv:1611.01673, 2016.
Arnab Ghosh, Viveka Kulharia, Vinay Namboodiri, Philip HS Torr, and Puneet K Dokania. Multiagent diverse generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 8513­8521, 2017.
Ian Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6626­6637, 2017.
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. MGAN: Training generative adversarial nets with multiple generators. 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Tu Nguyen, Trung Le, Hung Vu, and Dinh Phung. Dual discriminator generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2670­2680, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
9

Under review as a conference paper at ICLR 2019 Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep Boltzmann machines. In
International Conference on Artificial Intelligence and Statistics, pp. 693­700, 2010. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016. Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. In International Conference on Learning Representations, 2018. Ross M Starr. Quasi-equilibria in markets with non-convex preferences. Econometrica: Journal of the Econometric Society, pp. 25­38, 1969. Ruohan Wang, Antoine Cully, Hyung Jin Chang, and Yiannis Demiris. MAGAN: Margin adaptation for generative adversarial networks. arXiv preprint arXiv:1704.03817, 2017. Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial examples with adversarial networks. arXiv preprint arXiv:1801.02610, 2018. Hongyang Zhang, Junru Shao, and Ruslan Salakhutdinov. Deep neural networks with multi-branch architectures are less non-convex. arXiv preprint arXiv:1806.01845, 2018.
10

Under review as a conference paper at ICLR 2019
A SUPPLEMENTARY EXPERIMENTS

(a) Step 0.

(b) Step 6k.

(c) Step 13k.

(d) Step 19k.

(e) Step 25k.

(f) Step 0.

(g) Step 6k.

(h) Step 13k.

(i) Step 19k.

(j) Step 25k.

(k) Step 0.

(l) Step 6k.

(m) Step 13k.

(n) Step 19k.

(o) Step 25k.

Figure 9: Effects of generator architecture of Stackelberg GAN on a toy 2D mixture of Gaussians, where the number of generators are set to be 8. Top Row: The generators have one hidden layer. Middle Row: The generators have two hidden layers. Bottom Row: The generators have three hidden layer. It shows that with the number of hidden layers increasing, each generator tends to learn more modes. However, mode collapse never happens for all three architectures.

Figure 9 shows how the architecture of generators affects the distributions of samples by each generators. The enlarged versions of samples generated by Stackleberg GAN with architectures shown in Table 5 and Table 6 are deferred to Figures 10, 11, 12 and 13.

B PROOFS OF MAIN RESULTS

B.1 PROOFS OF THEOREM 1 AND COROLLARY 2: MINIMAX DUALITY GAP

The statement 0  w - q is by the weak duality. Thus it suffices to prove the other side of the inequality. We first show that

inf
1,...,I Rg

sup
Rt

1 I

(1

,

...,

I

;

)

-

sup
Rt

inf
1,...,I Rg

1 I

(1,

...,

I

;

)



.

Denote by

p(u) := inf sup
1,...,I Rg Rt
We have the following lemma. Lemma 4. We have

(1, ..., I ; ) - uT 

.

sup
Rt

inf
1,...,I Rg

(1,

...,

I ;

)

=

(clp)(0)



p(0)

=

inf
1,...,I Rg

sup
Rt

(1,

...,

I ;

).

Proof. By the definition of p(0), we have p(0) = inf1,...,IRg supRt (1, ..., I ; ). Since (clp)(·) is the convex closure of function p(·) (a.k.a. weak duality theorem), we have (clp)(0)  p(0).

We now show that

sup
Rt

inf
1,...,I Rg

(1,

...,

I

;

)

=

(clp)(0).

11

Under review as a conference paper at ICLR 2019

Note that p(u) = inf1,...,I Rg p1,...,I (u), where p1,...,I (u) = supRt {(1, ..., I ; ) - uT } = (-(1, ..., I ; ·))(-u), and that

inf
uRt

{p1

,...,I

(u)

+

uT

µ}

=

-

sup {uT
uRt

(-µ)

-

p1 ,...,I

(u)}

= -(p1,...,I )(-µ) (by the definition of conjugate function)

= -(-(1, ..., I ; ·))(µ)

(3)

= (1, ..., I ; µ). (by conjugate theorem)

So we have

(clp)(0) = sup inf {p(u) + uT µ} (by Lemma 8)
µRt uRt

=

sup
µRt

inf
uRt

1

inf
,...,I

Rg

{p1

,...,I

(u)

+

uT

µ}

(by the definition of p(u))

=

sup
µRt

inf
1,...,I Rg

inf
uRt

{p1

,...,I

(u)

+

uT

µ}

=

sup
µRt

inf
1,...,I Rg

(1,

...,

I ;

µ).

(by Eqn. (3))

By Lemma 4, it suffices to show p(0) - (clp)(0)  (t + 1)worst. We have the following lemma. Lemma 5. Under the assumption in Theorem 1, p(0) - (clp)(0)  (t + 1)worst.
Proof. We note that

p(u) := inf sup
1,...,I Rg Rt

(1, ..., I ; ) - uT 

I

= inf sup
1,...,I Rg Rt

cl(i; ) - uT 
i=1

(by the definition of )

I

= inf
1,...,I Rg

-cl(i; ·)
i=1

(-u) (by the definition of conjugate function)

= inf

inf

1,...,I Rg u1+...+uI =-u

I
(-cl(i; ·))(ui)
i=1

(by Lemma 7)

I

= inf

inf

1,...,I Rg u1+...+uI =-u

(-(i; ·))(ui)
i=1

(by conjugate theorem)

=

inf
u1+...+uI =-u

inf
1,...,I Rg

{(-(1;

·))(u1)

+

...

+

(-(I ;

·))(uI )}

=:

u1

inf
+...+uI

=-u{h1

(u1

)

+

...

+

hI

(uI )},

(by the definition of hi(·))

where u1, ..., uI , u  Rt. Therefore,

II

p(0) = inf

hi(ui), s.t.

ui = 0.

u1,...,uI Rt i=1

i=1

Consider the subset of Rt+1:
Yi := yi  Rt+1 : yi = [ui, hi(ui)] , ui  domhi ,
Define the vector summation Y := Y1 + Y2 + ... + YI .
Since hi(·) is continuous and domhi is compact, the set

i  [I].

{(ui, hi(ui)) : ui  domhi}

12

Under review as a conference paper at ICLR 2019

is compact. So Y, conv(Y), Yi, and conv(Yi), i  [I] are all compact sets. According to the definition of Y and the standard duality argument (Bertsekas, 2009), we have

p(0) = inf {w : there exists (r, w)  Y such that r = 0} ,

and clp(0) = inf {w : there exists (r, w)  conv (Y) such that r = 0} .

We are going to apply the following Shapley-Folkman lemma.

Lemma 6 (Shapley-Folkman, Starr (1969)). Let Yi, i  [I] be a collection of subsets of Rm. Then

for every y  conv(

I i=1

Yi),

there is 

a

subset

I (y)



[I ]

of

size at 

most

m

such

that

y

Yi +

conv(Yi) .

iI(y)

iI(y)

We apply Lemma 6 to prove Lemma 5 with m = t + 1. Let (r, w)  conv(Y) be such that r = 0, and w = clp(0).

Applying the above Shapley-Folkman lemma to the set Y =

I i=1

Yi,

we

have

that

there

are

a

subset

I  [I] of size t + 1 and vectors

such that

(ri, wi)  conv(Yi), i  I and ui  domhi, i  I, ui + ri = r = 0,

(4)

iI iI

hi(ui) + wi = clp(0).

(5)

iI iI
Representing elements of the convex hull of Yi  Rt+1 by Carathéodory theorem, we have that for each i  I, there are vectors {uji }jt+=21 and scalars {aji }tj+=21  R such that

t+2
aij = 1, aij  0, j  [t + 2],
j=1

t+2 t+2

ri = aij uji =: ui  domhi,

wi = aji hi(uji ).

j=1

j=1

Recall that we define





 t+2

t+2 t+2



clhi(u)

:=

inf
{aj },{uji

}


j

=1

aj

hi

(uji

)

:

u

=

aj uji , aj

j=1

j=1

=

1, aj



0 ,

(6)

and i := supuRt {hi(u) - clhi(u)}  0. We have for i  I, 
t+2
wi  clhi  aijuji  (by the definition of clhi(·))
j=1

t+2
 hi  aji uji  - i (by the definition of i )
j=1
= hi (ui) - i . (by Eqn. (6))
Thus, by Eqns. (4) and (6), we have

(7)

I
ui = 0, ui  domhi, i  [I].
i=1

(8)

13

Under review as a conference paper at ICLR 2019

Therefore, we have as desired.

I
p(0) = hi(ui) (by Eqn. (8))
i=1
 clp(0) + i (by Eqns. (5) and (7))
iI
 clp(0) + |I|worst = clp(0) + (t + 1)worst, (by Lemma 6)

By Lemmas 4 and 5, we have proved that

inf
1,...,I Rg

sup
Rt

1 I

(1

,

...,

I

;

)

-

sup
Rt

inf
1,...,I Rg

1 I

(1,

...,

I

;

)



.

To prove Theorem 1, we note that

w

-

q

:=

inf
1,...,I Rg

sup
Rt

1 I

(1

,

...,

I

;

)

-

sup
Rt

inf
1,...,I Rg

1 I

(1

,

...,

I

;

)

=

inf
1,...,I Rg

sup
Rt

1 I

(1,

...,

I ; )

-

inf
1,...,I Rg

sup
Rt

1 I

(1

,

...,

I ;

)

+

inf
1,...,I Rg

sup
Rt

1 I

(1,

...,

I

;

)

-

sup
Rt

inf
1,...,I Rg

1 I

(1,

...,

I

;

)

+

sup
Rt

inf
1,...,I Rg

1 I

(1,

...,

I

;

)

-

sup
Rt

inf
1,...,I Rg

1 I

(1,

...,

I

;

)

 m inimax + m aximin + ,
as desired. When (i; ) is concave and closed w.r.t. discriminator parameter , we have cl = . Thus, minimax = maximin = 0 and 0  w - q  .

B.2 PROOFS OF THEOREM 3: EXISTENCE OF APPROXIMATE EQUILIBRIUM

We first show that the equilibrium value V is 2f (1/2). For the discriminator D which only outputs

1/2, it has we have V

payoff 2f (1/2)  2f (1/2). We

for all possible implementations now show that V  2f (1/2). We

onfotgeetnheartabtoyrsasGsum1 ,p.t.i.o, nG,fIo.r

Therefore, any  > 0,

there exists a closed neighbour of implementation of generator G such that ExPd,zPz G(z) -

x 2   for all G in the neighbour. Such a neighbour exists because the generator is Lipschitz w.r.t. its parameters. Let the parameter implementation of such neighbour of G be . The Wasserstein

distance between G and Pd is . Since the function f and the discriminator are Lf -Lipschitz and

L-Lipschitz, respectively, we have

EzG f (1 - D(z)) - ExPd f (1 - D(x))  O(Lf L). Thus, for any fixed , we have

sup ExPd f (D(x)) + EzG f (1 - D(z))
Rt

 O(Lf L) + sup ExPd f (D(x)) + ExPd f (1 - D(x))
Rt

 O(Lf L) + 2f (1/2)  2f (1/2), (  +0)

which implies 2f (1/2). This

mtheaatnssutphat Rthted(isc1r,i.m..,inaIt;or)ca=nn2oft

(1/2) for all 1, ..., do much better than

I  . a random

So we guess.

have

V

=

The above analysis implies that the equilibrium is achieved when D only outputs 1/2. Denote by  the small closed neighbour of such  such that (1, ..., I ; ) is concave w.r.t.    for any fixed 1, ..., I  . We thus focus on the loss on   Rt and   Rg:

I
(1, ..., I ; ) := [ExPd f (D(x)) + EzPz f (1 - D(Gi (z)))] ,   , 1, ..., I  .
i=1

14

Under review as a conference paper at ICLR 2019

Since (1, ..., I ; ) is concave w.r.t.    for all 1, ..., I  , by Corollary 2, we have

inf
1,...,I 

sup


1 I

(1

,

...,

I ;

)

-

sup


inf
1,...,I 

1 I

(1

,

...,

I ;

)



.

The

optimal

implementations

of

1,

...,

I

is

achieved

by

argmin1,...,I 

sup

1 I

(1

,

...,

I ;

).

C USEFUL LEMMAS

Lemma 7. Given the function

(f1 + ... + fI )() := f1() + ... + fI (),

where fi : Rt convolution



R,

i



[I ]

are

closed

proper

convex

functions.

Denote

by

f1



...



fI

the

infimal

(f1



...



fI)(u)

:=

inf
u1 +...+uI

=u{f1(u1)

+

...

+

fI(uI

)},

u  Rt.

Provided that f1 + ... + fI is proper, then we have

(f1 + ... + fI )(u) = cl(f1  ...  fI)(u), u  Rt.

Proof. For all   Rt, we have

f1() + ... + fI () = sup{T u1 - f1(u1)} + ... + sup{T uI - fI(uI )}
u1 uI

= sup {T (u1 + ... + uI ) - f1(u1) - ... - fI(uI )}
u1 ,...,uI

= sup sup

T u - f1(u1) - ... - fI(uI )

u u1+...+uI =u

Therefore,

= sup
u

T

u

-

inf
u1 +...+uI

=u

f1(u1)

-

...

-

fI(uI

)

= sup T u - (f1  ...  fI)(u)
u
= (f1  ...  fI)().

(9)

cl(f1  ...  fI)(u) = cl(f1  ...  fI)(u) = (f1  ...  fI)(u) = (f1 + ... + fI )(u),

where the conjugate

first equality theorem, and

thhoeldlassbt eecqauuasleity(fh1olds..b.ycofnIj)uigsactionngvtehxe,

the second quality is by both sides of Eqn. (9).

standard

Lemma 8 (Proposition 3.4 (b), Bertsekas (2009)). For any function p(u), denote by q(µ) := infuRt {p(u) + µT u}. We have supµRt q(µ) = clp(0).

D NETWORK SETUP

15

Under review as a conference paper at ICLR 2019

Table 2: Architecture and hyperparameters for the mixture of Gaussians dataset.

Operation Input Dim Output Dim BN? Activation

Generator G(z) : z  N (0, 1)

2

Linear

2

16 

Linear

16

2

Tanh

Discriminator

Linear

2

512

Leaky ReLU

Linear

512

256

Leaky ReLU

Linear

256

1

Sigmoid

Number of generators

8

Batch size for real data

64

Number of iterations

200

Slope of Leaky ReLU

0.2

Learning rate 0.0002

Optimizer Adam

Table 3: Architecture and hyperparameters for the MNIST datasets.

Operation Input Dim Output Dim BN? Activation

Generator G(z) : z  N (0, 1)

2

Linear

100

512 

Linear

512

784

Tanh

Discriminator

Linear

2

512

Leaky ReLU

Linear

512

256

Leaky ReLU

Linear

256

1

Sigmoid

Number of generators

10

Batch size for real data

100

Slope of Leaky ReLU

0.2

Learning rate 0.0002

Optimizer Adam

Table 4: Architecture and hyperparameters for the Fashion-MNIST datasets.

Operation Input Dim Output Dim BN? Activation

Generator G(z) : z  N (0, 1) Linear Linear Linear Linear Linear
Discriminator Linear Linear Linear
Number of generators Batch size for real data
Number of iterations Slope of Leaky ReLU
Learning rate Optimizer

2 128 256 512 1024
2 512 256 10 100 500 0.2 0.0002 Adam

2 128 256 512 1024 784 512 256 1

   
Tanh
Leaky ReLU Leaky ReLU
Sigmoid

16

Under review as a conference paper at ICLR 2019

Table 5: Architecture and hyperparameters for the CIFAR-10 dataset.

Operation Kernel Strides Feature maps BN? BN center? Activation

G(z) : z  Uniform[-1, 1] Fully connected
Transposed convolution Transposed convolution
D(x) Convolution Convolution Convolution Fully connected Number of generators Batch size for real data Batch size for each generator Number of iterations Slope of Leaky ReLU Learning rate
Optimizer Weight, bias initialization

100

8×8×128

5×5 2×2

64

5×5 2×2

3

8×8×256

5×5 2×2

128

5×5 2×2

256

5×5 2×2

512

1

10

64

64

100

0.2

0.0002

Adam(1 = 0.5, 2 = 0.999)

N (µ = 0,  = 0.01), 0

  
   

 ReLU  ReLU  Tanh  Leaky ReLU  Leaky ReLU  Leaky ReLU  Sigmoid

Table 6: Architecture and hyperparameters for the Tiny ImageNet dataset.

Operation Kernel Strides Feature maps BN? BN center? Activation

G(z) : z  Uniform[-1, 1] Fully connected
Transposed convolution Transposed convolution
D(x) Convolution Convolution Convolution Fully connected Number of generators Batch size for real data Batch size for each generator Number of iterations Slope of Leaky ReLU Learning rate
Optimizer Weight, bias initialization

100

8×8×256

5×5 2×2

128

5×5 2×2

3

8×8×256

5×5 2×2

128

5×5 2×2

256

5×5 2×2

512

1

10

64

64

300

0.2

0.00001

Adam(1 = 0.5, 2 = 0.999)

N (µ = 0,  = 0.01), 0

  
   

 ReLU  ReLU  Tanh  Leaky ReLU  Leaky ReLU  Leaky ReLU  Sigmoid

17

Under review as a conference paper at ICLR 2019
Figure 10: Examples generated by Stackelberg GAN with 10 generators on CIFAR-10 dataset, where each row corresponds to samples from one generator.
18

Under review as a conference paper at ICLR 2019
Figure 11: Examples generated by Stackelberg GAN with 10 generators on CIFAR-10 dataset, where each row corresponds to samples from one generator.
19

Under review as a conference paper at ICLR 2019
Figure 12: Examples generated by Stackelberg GAN with 10 generators on Tiny ImageNet dataset, where each row corresponds to samples from one generator.
20

Under review as a conference paper at ICLR 2019
Figure 13: Examples generated by Stackelberg GAN with 10 generators on Tiny ImageNet dataset, where each row corresponds to samples from one generator.
21

