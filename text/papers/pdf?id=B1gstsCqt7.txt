Under review as a conference paper at ICLR 2019
SPARSE DICTIONARY LEARNING BY DYNAMICAL NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
A dynamical neural network consists of a set of interconnected neurons that interact over time continuously. It can exhibit computational properties in the sense that the dynamical system's evolution and/or limit points in the associated state space can correspond to numerical solutions to certain mathematical optimization or learning problems. Such a computational system is particularly attractive in that it can be mapped to a massively parallel computer architecture for power and throughput efficiency, especially if each neuron can rely solely on local information (i.e., local memory). Deriving gradients from the dynamical network's various states while conforming to this last constraint, however, is challenging. We show that by combining ideas of top-down feedback and contrastive learning, a dynamical network for solving the 1-minimizing dictionary learning problem can be constructed, and the true gradients for learning are provably computable by individual neurons. Using spiking neurons to construct our dynamical network, we present a learning process, its rigorous mathematical analysis, and numerical results on several dictionary learning problems.
1 INTRODUCTION
A network of simple neural units can form a physical system that exhibits computational properties. Notable examples include Hopfield network (Hopfield, 1982) and Boltzmann machine (Ackley et al., 1985). Such systems have global states that evolve over time through only local interactions among neural units. Typically, one is interested in a system whose motion converges towards locally stable limit points, with the limit points representing the computational objective of interest. For example, a Hopfield network's limit points correspond to stored memory information and that of a Boltzmann machine, a data representation. These computational systems are interesting for both engineering and neuroscience research. From a hardware implementation standpoint, such computational models allow the mapping of neurons to a massively parallel architecture (Davies et al., 2018; Merolla et al., 2014). By allocating private local memory to each processing element, the so-called von Neumann memory bottleneck in modern computers can be eliminated, delivering much greater power and throughput efficiency (e.g., see Kung (1982)). For neuroscience, such computational models obey the fundamental physical locality constraints of biological neurons, providing a direction for understanding the brain.
We are interested in using such systems to solve the 1-minimizing sparse coding and dictionary learning problem, which has fundamental importance in many areas, e.g., see Mairal et al. (2014). It is well-known that even just the sparse coding problem, with a prescribed dictionary, is non-trivial to solve, mainly due to the non-smooth objective involving an 1-norm (Efron et al., 2004; Beck & Teboulle, 2009). Remarkably, a dynamical network known as the LCA network (Rozell et al., 2008) can be carefully constructed so that its limit points are identical to the solution of the sparse coding problem. Use of a dynamical network thus provides an alternative and potentially more power efficient method for sparse coding to standard numerical optimization techniques. Nevertheless, while extending numerical optimization algorithms to also learning the underlying dictionary is somewhat straightforward, there is very little understanding in using dynamical networks to learn a dictionary with provable guarantees due to the challenging locality constraints.
In this work, we devise a new network topology and learning rules that enable dictionary learning. In particular, we show that the gradients for learning are provably computable by individual neurons
1

Under review as a conference paper at ICLR 2019

WW

Coding layer
-11
F
Input layer

.... ....

1 
(a)

-1

Coding layer
- 1 -  11
F
Input layer

....
- 1 -  1
B
....

1 -  1 1 -  
(b)

Figure 1: The network topologies discussed in this work. (a) is known as the LCA network that can perform sparse coding. We propose the network in (b) for dictionary learning.

using only local information. On a high level, our learning strategy is similar to the contrastive learning procedure developed in training Boltzmann machines, which also gathers much recent interest in deriving implementations of backpropagation under the same neuron locality constraints (Ackley et al., 1985; Movellan, 1990; O'Reilly, 1996; Xie & Seung, 2003; Scellier & Bengio, 2017; Whittington & Bogacz, 2017). During training, the network is run in two different configurations ­ a "normal" one and a "perturbed" one.1 The networks' limit points under these two configurations will be identical if the weights to be trained are already optimal, but different otherwise. The learning process is a scheme to so adjust the weights to minimize the difference in the limit points. In Boltzmann machine, the weight adjustment can be formulated as minimizing a KL divergence objective function.
For dictionary learning, we adopt a neuron model whose activation function corresponds to the unbounded ReLU function rather than the bounded sigmoid-like function in Hopfield networks or Boltzmann machines, and a special network topology where connection weights have dependency. Interestingly, the learning processes are still similar: We also rely on running our network in two configurations. The difference in states after a long-enough evolution, called limiting states in short, is shown to hold the gradient information of a dictionary learning objective function which the network minimizes, as well as the gradient information for the network to maintain weight dependency. Comparisons between this work, Hopfield network, and Boltzmann machine can be found in Appendix C.1.
1.1 RELATED WORK
Dictionary learning is thought to be related to the formation of receptive fields in visual cortex (Olshausen & Field, 1996) and has been widely studied. The typical architecture studied is a feedforwardonly, two-layer neural network with inhibitory lateral connections among the second layer neurons as shown in Figure 1(a) (Földiak, 1990; Zylberberg et al., 2011; Brito & Gerstner, 2016; Hu et al., 2014; Seung & Zung, 2017; Vertechi et al., 2014; Brendel et al., 2017). The lateral connections allow the coding neurons to compete among themselves and hence induce sparseness in neural activities, giving dynamics more complex than conventional deep neural networks which do not have intra-layer connections.2 In Rozell et al. (2008), it is shown that the coding neuron activations can correspond to a sparse coding solution if the connection weights are set according to a global dictionary D as F = DT , W = -DT D + I.3 To enable learning in this network (that is, each neuron locally adjusts their connection weights to adapt the dictionary; see Section 2.2 for the definition of weight locality), one must address the following two questions:
· How does individual neuron compute the gradient for learning locally? · How do the neurons collectively maintain the global weight consistency between F and W ?
The first line of work, Földiak (1990); Zylberberg et al. (2011); Brito & Gerstner (2016), adopts the Hebbian/anti-Hebbian heuristics for learning the feedforward and lateral weights, respectively, and empirically demonstrated that such learning yielded Gabor-like receptive fields if trained with
1In Botlzmann machine, the two configurations are called the free-running phase and the clamped phase. 2This should not be confused with the conventional recurrent neural networks. Although RNNs also have intra-layer connections, these connections are still uni-directional over a sequence of input. 3The exact formulation depends on the neuron model. In the spiking neuron formulation, we in fact have W -  = -DT D where  is the firing thresholds. See Section 3.1 for more details.
2

Under review as a conference paper at ICLR 2019

natural images. However, unlike the network in Rozell et al. (2008), this learning heuristic does not correspond to a rigorous learning objective, and hence cannot address any of the two above questions. Recently, this learning strategy is linked to minimizing a similarity matching objective function between input and output correlations (Hu et al., 2014). This formulation is somewhat different from the common autoencoder-style dictionary learning formulation discussed in this work.
Another line of work, Vertechi et al. (2014); Brendel et al. (2017), notes the importance of balance between excitation and inhibition among the coding neurons, and proposes that the learning target of lateral connections should be to maintain such balance; that is, the inhibitory lateral weights should grow according to the feedforward excitations. This idea provides a potential solution to ensure weight consistency between F and W . Nevertheless, similar to the first line of work, both Vertechi et al. (2014); Brendel et al. (2017) resort to pure Hebbian rule when learning the feedforward weights F (or equivalently, learning the dictionary), which does not necessarily follow a descending direction that minimizes the dictionary learning objective function and hence the convergence to a local minimum cannot be guaranteed. Detailed discussions are provided in Appendix C.2.
1.2 CONTRIBUTIONS
The major advance in this work is to recognize the inadequacy of the customary feedforward-only architecture, and to introduce top-down feedback connections shown in Figure 1(b). As will later be shown, this network structure allows the true learning gradients to be provably computable from the resulting network dynamics. Further, the existence of feedback allows us to devise a separate mechanism that acts as an inner loop during learning to continuously ensure weight consistency among all connections. Combining these two, we can successfully address both the above questions and the dictionary learning problem.
We will focus our discussion on a network that uses spiking neurons as the basic units that are suited for digital circuit implementations with high computational efficiency. Note that this does not result in a loss of generality. The principles of LCA network can be applied to both continuous-valued and spiking neurons (Shapero et al., 2014; Tang et al., 2017), and similarly the results established in this paper can be easily applied to construct a network of continuous-valued neurons for dictionary learning.

2 BACKGROUND

2.1 INTEGRATE-AND-FIRE SPIKING NEURON MODEL AND NETWORK DYNAMICS

An integrate-and-fire neuron has two internal state variables that govern its dynamics: the current

µ(t) and the potential (t). The key output of a neuron is a time sequence of spikes ­ spike train ­

that it produces. A neuron's spike train is generated by its potential (t); (t) is in turn driven by the

current µ(t), which is in turn driven by a constant bias  (bias in short) and the spike trains of other

neurons to which it is connected. Specifically, each neuron has a configured firing threshold  > 0.

When (t) reaches , say at time tk, a spike given by the Dirac delta function (t - tk) is generated

and (t) is reset to 0: (tk+) = 0. For t > tk and before (t) reaches  again, (t) =

t tk

µ(s)

ds.

In a system of N neurons ni, i = 1, 2, . . . , N , let j(t) = k (t - tj,k) denote the spike train of neuron nj. The current µi(t) of ni is given in terms of its bias i and the spike trains {j(t)}:

µi(t) = i + j=i Wij (  j )(t),

(1)

where

(t)

=

1 

e-t/

for

t



0,

(t)

=

0

for

t

<

0

and



is

the

convolution operator.

Neuron

nj

inhibits (excites) ni if Wij < 0 (Wij > 0). If Wij = 0, neurons ni and nj are not connected. For

simplicity, we consider only  = 1 throughout the paper. Equation 1 yields the dynamics

µ (t) =  - µ(t) + W · (t),

(2)

where the vectors µ(t) and (t) denote the N currents and spike trains (see Appendix B.1 for the full derivation.)

The network dynamics can be studied via the filtered quantities of average current and spike rate:

u(t) d=ef 1

t
µ(s) ds,

t0

a(t) d=ef 1

t
(s) ds.

t0

(3)

3

Under review as a conference paper at ICLR 2019

In terms of u(t) and a(t), Equation 2 becomes

u (t) =  - u(t) + W a(t) + (µ(0) - u(t))/t

(4)

The trajectory (u(t), a(t)) has interesting properties. In particular, Theorem 1 below (cf. Tang et al. (2017)) shows that any limit point (u, a) satisfies u -a  0, a  0 and (u -a) a = 0 where is elementwise product. These properties are crucial to Section 3.

Theorem 1. Let  = diag(),  = [1, 2, . . . , N ], then

u(t) - a(t) =  + (W - ) · a(t) + (t)

(5)

where max(u(t), 0) - a(t)  0 and (t)  0.

As with all other theorems, Theorem 1 is given in a conceptual form where the corresponding rigorous " -" versions are detailed in the Appendix.

2.2 PARALLEL MODEL OF DYNAMICAL NEURAL NETWORKS
We view the dynamical network as a computational model where each neuron evolves in parallel and asynchronously. One-sided communication in the form of a one-bit signal from Neuron nj to Neuron ni occurs only if the two are connected and only when the former spikes. The network therefore can be mapped to a massively parallel architecture, such as Davies et al. (2018), where the connection weights are stored distributively in each processing element's (PE) local memory. In the most general case, we assume the architecture has the same number of PEs and neurons; each PE hosts one neuron and stores the weights connected towards this neuron, that is, each PE stores one row of the W matrix in Equation 2. With proper interconnects among PEs to deliver spike messages, the dynamical network can be realized to compute sparse coding solutions.
This architectural model imposes a critical weight locality constraint on learning algorithms for dynamical networks: The connection weights must be adjusted with rules that rely only on locally available information such as connection weights, a neuron's internal states, and the rate of spikes it receives. The goal of this paper is to enable dictionary learning under this locality constraint.

3 DICTIONARY LEARNING

In dictionary learning, we are given P images x(p)  RM0, p = 1, 2, . . . , P . The goal is to find a dictionary consisting of a prescribed number of N atoms, D = [d1, d2, . . . , dN ], D  RM×N such that each of the P images can be sparsely coded in D. We focus here on non-negative dictionary and
formulate our minimization problem as

P

arg min

l(D, x(p), a(p)),

a(p)0,D0 p=1

1 l(D, x, a) =
2

x - Da

2 2

+

1

Sa

1+

2 2

D

2 F

,

(6)

S being a positive diagonal scaling matrix.

Computational methods such as stochastic online training (Aharon & Elad, 2008) is known to be effective for dictionary learning. With this method, one iterates on the following two steps, starting with a random dictionary.

1. Pick a random image x  x(p) and obtain sparse code a for the current dictionary D and image x, that is, solve Equation (6) with D fixed.
2. Use gradient descent to update D with a learning rate . The gradient D with respect to D is in a simple form and the update of D is

D(new)  D -  (Da - x)aT + 2D .

(7)

Implementing these steps with a dynamical network is challenging. First, previous works have only
shown that Step 1 can be solved when the configuration uses the dictionary D in the feedforward connection weights and DT D as the lateral connection weights (Shapero et al. (2014), c.f. Figure 1(a)
and below). For dictionary learning, both sets of weights evolve without maintaining this exact

4

Under review as a conference paper at ICLR 2019

relationship, casting doubt if Step 1 can be solved at all. Second, the network in Figure 1(a) only has F = DT , rendering the needed term Da uncomputable using information local to each neuron. Note that in general, gradients to minimize certain objective functions in a neural network can be mathematically derived, but often times they cannot be computed locally, e.g., standard backpropagation and general gradient calculations for spiking networks (Huh & Sejnowski, 2017). We now show that our design depicted in Figure 1(b) can indeed implement Steps 1 and 2 and solve dictionary learning.

3.1 SPARSE CODING ­ GETTING a

Non-negative sparse coding (Equation 6 with D fixed) is a constrained optimization problem. The standard approach (cf. Boyd & Vandenberghe (2004)) is to augment l(D, x, a) with non-negative slack variables, with which the optimal solutions are characterized by the KKT conditions. Consider now Figure 1(b) that has explicit feedback weights B whose strength is controlled by a parameter . Equation 5, reflecting the structure of the coding and input neurons, takes the form:

e (t) f (t)

d=ef

u(t) - a(t) v(t) - b(t)

=

-(1 - )1s (1 - )x

+

-H B

F -I

a (t) b (t)

+ (t)

(8)

(u(t), v(t)) and (a(t), b(t)) denote the average currents and spike rates for the coding and input

neurons, respectively, and H d=ef W + . When  = 0, F T = B = D, H = F B = DT D and at a

limit e0 =

p-oin1ts(e-0,DaT0)D, tah0e

network + DT x

is equivalent to Figure and that e0  0, a0 

1(a). Equation 0 and e0 a0

8 is simplified and reduces to = 0. This shows that a0 and

-e0 are the optimal primal and slack variables that satisfy the KKT conditions. In particular a0 is

the optimal sparse code.

We extend this previously established result (Tang et al., 2017) in several aspects: (1)  can be set to any values in [0, 1); all a are the optimal sparse code, (2) H needs not be F B exactly; H - F B being small suffices, and (3) as long as t is large enough, a(t) solves an approximate sparse coding problem. These are summarized as follows (where the rigorous form is presented in the Appendix).
Theorem 2. Let F T = B = D,   [0, 1) and H - F B be small. Then for t large enough, a(t) is close to an exact solution a~ to Equation 6 (D fixed) with S replaced by S~ where S - S~ is small.

The significant implication is that despite slight discrepancies between H and F B, the average spike rate a(t) at t large enough is a practical solution to Step 1 of the stochastic learning procedure.

3.2 DICTIONARY ADJUSTMENT ­ UPDATING F, B AND H

To obtain the learning gradients, we run the network for a long enough time to sparse code twice: at  = 0 and  =  > 0, obtaining e~0, e~, a~0, a~ and b~0, b~ at those two configurations. We use tilde to denote the obtained states and loosely call them as limiting states. Denote 1 -  by c.

Theorem 3. The limiting states satisfy

(Ba~ - x)  gD, (H - F B)a~  gH ,

gD d=ef b~ - b~0 gH d=ef cH(a~0 - a~) + (ce~0 - e~)

(9) (10)

We now show Theorem 3 lays the foundation for computing all the necessary gradients that we need. Equation 9 shows that (recall B = D)
Da~ - x  -1gD.

In other words, the spike rate differences at the input layer reflect the reconstruction error of the sparse code we just computed. Following Equation 7, this implies that the update to each weight can be approximated from the spike rates of the two neurons that it connects, while the two spike rates surely are locally available to the destination neuron that stores the weight. Specifically, each coding neuron has a row of the matrix F = DT ; each input neuron has a row of the matrix B = D. These neurons each updates its row of matrix via

Fi(jnew)  Fij - D -1(a~)i (gD)j + 2Fij Bi(jnew)  Bij - D -1(a~)j (gD)i + 2Bij

(11)

5

Under review as a conference paper at ICLR 2019

Note that F T = B = D is maintained.

Ideally, at this point the W and  stored distributively in the coding neurons will be updated to

H(new) where H(new) = F (new)B(new). Unfortunately, each coding neuron only possesses one row

of the matrix F (new) and does not have access to any values of the matrix B(new). To maintain H to be close to DT D throughout the learning process, we do the following. First we aim to modify H

to

be

closer

to

FB

(not

F (new)B(new))

by

reducing

the

cost

function

(H )

=

1 2

(H - F B)a~

2 2

.

The gradient of this cost function is H  = (H - F B)a~a~T which is computable as follows.

Equation 10 shows that

H   G d=ef -1gH a~T

Using this approximation, coding neuron nC,i has the information to compute the i-th row of G. We modify H by -H G where H is some learning rate. This modification can be thought of as
a catch-up correction because F and B correspond to the updated values from a previous iteration.

Because the magnitude of that update is of the order of D, we have H -F B  D and G  D. Thus H should be bigger than D lest H G  H D be too small to be an effective correction. In practice, H  15D works very well.

In addition to this catch-up correction, we also make correction of H due to the update of -D2F and -D2B to F and B. These updates lead to a change of -2DF B + O(D2 ). Consequently, after Equation 11, we update H by

Hi(jnew)  Hij - H -1(gH )i(a)j - 2D2Hij .

(12)

Note that the update to H involves update to the weights W as well as the thresholds  (recall that H d=ef W + ). Combining the above, we summarize the full dictionary learning algorithm below.

Algorithm 1 Dictionary Learning
Initialization: Pick a random dictionary D  0 with atoms of unit Euclidean norm. Configure F  DT , B  D, s  [1, 1, . . . , 1]T , and H  F B. repeat
1. Online input: Pick a random image x from {x(p)} 2. Sparse coding: Run the network at   0 and at    > 0. 3. Dictionary update: Compute the vectors gD and gH distributively according to Equations 9 and 10. Update F , B and H according to Equations 11 and 12. Project the weights to non-
negative quadrant. 4. Scaling update: Set the scaling vector s to diag(H). This scaling helps maintain each atom
of the dictionary to be of similar norms. until dictionary is deemed satisfactory

3.3 DISCUSSIONS
Dictionary norm regularization. In dictionary learning, typically one needs to control the norms of atoms to prevent them from growing arbitrarily large. The most common approach is to constrain the atoms to be exactly (or at most) of unit norms, achieved by re-normalizing each atom after a dictionary update. This method however cannot be directly adopted in our distributed setting. Each input neuron only has a row of the matrix B but not a column of B ­ an atom ­ so as to re-normalize.
We chose instead to regularize the Frobenius norm of the dictionaries, translating to a simple decay term in the learning rules. This regularization alone may result in learning degenerate zero-norm atoms because sparse coding tends to favor larger-norm atoms to be actively updated, leaving smaller-norm ones subject solely to continual weight decays. By choosing a scaling factor s set to diag(H), sparse coding favors smaller-norm atoms to be active and effectively mitigates the problem of degeneracy.
Boundedness of network activities. Our proposed network is a feedback nonlinear system, and one may wonder whether the network activities will remain bounded. While we cannot yet rigorously guarantee boundedness and stability under some a priori conditions, currents and spike rates remain bounded throughout learning for all our experiments. One observation is that the feedback excitation amounts to F Ba(t) and the inhibition is Ha(t). Therefore when H = F B and  < 1, the feedback excitation is nullified, keeping the network from growing out of bound.
6

Under review as a conference paper at ICLR 2019

=0 (no feedback)

=0.7 (with feedback)

=0 (no feedback)

=0.7 (with feedback)

Coding

Coding

t=0

t=20

t=40

t=0

t=20

t=40

Input

Input

Time
(a) Random dictionary (training sample No.1)

Time
(b) Learned dictionary (training sample No.99900)

Figure 2: Network spike patterns. In the figures, each row corresponds to one neuron, and the bars indicate the spike timings. One notable difference between the left and right figures is in the spike patterns of the input neurons. Before learning, significant perturbation in spike patterns can be observed starting at t = 20 when the feedback is present. In contrast, little change in spike patterns is seen after learning. Recall that the perturbation in spike rates reflects the reconstruction error. This shows the network is able to learn a proper dictionary that minimizes reconstruction error. Data is from learning with Dataset A; only a subset of the neurons are shown.

Network execution in practice. Theoretically, an accurate spike rate can only be measured at a very large T as precision increases at a rate of O(1/t). In practice, we observed that a small T suffices for dictionary learning purpose. Stochastic gradient descent is known to be very robust against noise and thus can tolerate the low-precision spike rates as well as the approximate sparse codes due to the imperfect H  F B. For faster network convergence, the second network  =  is ran right after the first network  = 0 with all neuron states preserved.
Weight symmetry. The sparse code and dictionary gradient are computed using the feedforward and feedback weights respectively. Therefore a symmetry between those weights is the most effective for credit assignment. We have assumed such symmetry is initialized and the learning rules can subsequently maintain the symmetry. One interesting observation is that even if the weights are asymmetric, our learning rules still will symmetrize them. Let Ei(jp) = Fj(ip) - Bi(jp) be the weight difference at the p-th iteration. It is straightforward to show Ei(jp) = p-1Ei(j1),  = 1 - D2. Hence Ei(jp)  0 as p gets bigger. In training deep neural networks, symmetric feedforward and feedback weights are important for similar reasons. The lack of local mechanisms for the symmetry to emerge makes backpropagation biologically implausible and hardware unfriendly, see for example Liao et al. (2016) for more discussions. Our learning model may serve as a building block for the pursuit of biologically plausible deep networks with backpropagation-style learning.
4 NUMERICAL EXPERIMENTS
We examined the proposed learning algorithm using three datasets. Dataset A. 100K randomly sampled 8 × 8 patches from the grayscale Lena image to learn 256 atoms. Dataset B. 50K 28 × 28 MNIST images (LeCun et al., 1998) to learn 512 atoms. Dataset C. 200K randomly sampled 16 × 16 patches from whitened natural scenes (Olshausen & Field, 1996) to learn 1024 atoms. These are standard datasets in image processing (A), machine learning (B), and computational neuroscience (C).4 For each input, the network is ran with  = 0 from t = 0 to t = 20 and with  = 0.7 from t = 20 to t = 40, both with a discrete time step of 1/32. Note that although this time window of 20 is relatively small and yields a spike rate precision of only 0.05, we observed that it is sufficient for gradient calculation and dictionary learning purpose.
We explored two different connection weight initialization schemes. First, we initialize the weights to be fully consistent with respect to a random dictionary. Second, we initialized the weights to be
4For Dataset A and C, the patches are further subtracted by the means, normalized, and split into positive and negative channels to create non-negative inputs (Hoyer, 2004).
7

Under review as a conference paper at ICLR 2019

Consistency Symmetry

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3
101

Consistent Init Random Init
102 103 104 Number of training samples

105

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3
101

Consistent Init Random Init
102 103 104 Number of training samples

105

Figure 3: Network weight consistency and symmetry during learning. Consistency is measured as 1 - H - F B F / H F . Symmetry is measured as the average normalized inner product between the i-th row of F and the i-th column of B for i = 1 . . . N . Data is from learning with Dataset A.

Objective function of test set Objective function of test set Objective function of test set

10.5 10 9.5 9 8.5 8 7.5 7 101

Dataset A (Lena)
SpikngNet (consistent init) SpikingNet (random init) SGD (=D) SGD (=2D) SGD (=0.5D)

102 103 104 Number of training samples

105

11 10
9 8 7 6 5 4 3 2 101

Dataset B (MNIST) SpikingNet (consistent init)
SpikingNet (random init) SGD (=D)
SGD (=2D) SGD (=0.5D)
102 103 104 105 Number of training samples

26 25 24 23 22 21 20
102

Dataset C (SparseNet) SpikingNet (consistent init)
SpikingNet (random init) SGD (=D)
SGD (=2D) SGD (=0.5D)
103 104 105 Number of training samples

Figure 4: Comparison of convergence of learning with dynamical neural network and SGD.

asymmetric. In this case, we set F T and B to be column-normalized random matrices and the entries of H to be random values between [0, 1.5] with the diagonal set to 1.5.
4.1 NETWORK DYNAMICS
We first show the spike patterns from a network with fully consistent initial weights in Figure 2. It can be seen that the spike patterns quickly settle into a steady state, indicating that a small time window may suffice for spike rate calculations. Further, we can observe that feedback only perturbs the input neuron spike rates while keeping the coding neuron spike rates approximately the same, validating our results in Section 3.1 and 3.2.
Another target the algorithm aims at is to approximately maintain the weight consistency H  F B during learning. Figure 3 shows that this is indeed the case. Note that our learning rule acts as a catch-up correction, and so an exact consistency cannot be achieved. An interesting observation is that as learning proceeds, weight consistency becomes easier to maintain as the dictionary gradually converges.
Although we have limited theoretical understanding for networks with random initial weights, Figure 3 shows that our learning procedure can automatically discover consistent and symmetric weights with respect to a single global dictionary. This is especially interesting given that the neurons only learn with local information. No neuron has a global picture of the network weights.
4.2 CONVERGENCE OF DICTIONARY LEARNING
The learning problem is non-convex, and hence it is important that our proposed algorithm can find a satisfying local minimum. We compare the convergence of spiking networks with the standard stochastic gradient descent (SGD) method with the unit atom norm constraint. For simplicity, both algorithms use a batch size of 1 for gradient calculations. The quality of the learned dictionary D = F T is measured using a separate test set of 10K samples to calculate a surrogate dictionary learning objective (Mairal et al., 2009). For a fair comparison, the weight decay parameters in spiking networks are chosen so that the average atom norms converge to approximately one.
Figure 4 shows that our algorithm indeed converges and can obtain a solution of similar, if not better, objective function values to SGD consistently across the datasets. Surprisingly, our algorithm can even reach a better solution with fewer training samples, while SGD can be stuck at a poor local minimum especially when the dictionary is large. This can be attributed to the 1-norm reweighting
8

Under review as a conference paper at ICLR 2019
heuristic that encourages more dictionary atoms to be actively updated during learning. Finally, we observe that a network initialized with random non-symmetric weights still manages to reach objective function values comparable to those initialized with symmetric weights, albeit with slower convergence due to less accurate gradients. From Figure 3, we see the network weights are not symmetric before 104 samples for Dataset A. On the other hand, from Figure 4 the network can already improve the dictionary before 104 samples, showing that perfectly symmetric weights are not necessary for learning to proceed.
5 CONCLUSION
We have presented a dynamical neural network formulation that can learn dictionaries for sparse representations. Our work represents a significant step forward that it not only provides a link between the well-established dictionary learning problem and dynamical neural networks, but also demonstrates the contrastive learning approach to be a fruitful direction. We believe there is still much to be explored in dynamical neural networks. In particular, learning in such networks respects data locality and therefore has the unique potential, especially with spiking neurons, to enable low-power, high-throughput training with massively parallel architectures.
REFERENCES
David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for Boltzmann machines. Cognitive science, 9(1):147­169, 1985.
Michal Aharon and Michael Elad. Sparse and redundant modeling of image content using an image-signature-dictionary. SIAM Journal on Imaging Sciences, 1(3):228­247, 2008.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183­202, 2009.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK, 2004.
Wieland Brendel, Ralph Bourdoukan, Pietro Vertechi, Christian K Machens, and Sophie Denéve. Learning to represent signals spike by spike. arXiv preprint arXiv:1703.03777, 2017.
Carlos S. N. Brito and Wulfram Gerstner. Nonlinear Hebbian learning as a unifying principle in receptive field formation. PLoS Comput Biol, 12(9):1­24, 2016.
Alfred M Bruckstein, Michael Elad, and Michael Zibulevsky. On the uniqueness of nonnegative sparse solutions to underdetermined systems of equations. IEEE Transactions on Information Theory, 54(11):4813­4820, 2008.
Kendra S Burbank. Mirrored STDP implements autoencoder learning in a network of spiking neurons. PLoS Comput Biol, 11(12):e1004566, 2015.
Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro, 38(1):82­99, 2018.
Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. Least angle regression. The Annals of statistics, 32(2):407­499, 2004.
Michael Elad and Michal Aharon. Image denoising via learned dictionaries and sparse representation. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, volume 1, pp. 895­900. IEEE, 2006.
Peter Földiak. Forming sparse representations by local anti-Hebbian learning. Biological cybernetics, 64(2):165­170, 1990.
Yoav Freund and David Haussler. Unsupervised learning of distributions on binary vectors using two layer networks. In Advances in neural information processing systems, pp. 912­919, 1992.
9

Under review as a conference paper at ICLR 2019
Geoffrey E Hinton and James L McClelland. Learning representations by recirculation. In Neural information processing systems, pp. 358­366, 1988.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527­1554, 2006.
J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci., 79(8):2554­2558, 1982.
J. J. Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. Proc. Natl. Acad. Sci., 1:3088­3092, 1984.
Patrik O Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine learning research, 5(Nov):1457­1469, 2004.
Tao Hu, Cengiz Pehlevan, and Dmitri B Chklovskii. A hebbian/anti-hebbian network for online sparse dictionary learning derived from symmetric matrix factorization. In 2014 48th Asilomar Conference on Signals, Systems and Computers, pp. 613­619. IEEE, 2014.
Dongsung Huh and Terrence J Sejnowski. Gradient descent for spiking neural networks. arXiv preprint arXiv:1706.04698, 2017.
Hsiang-Tsung Kung. Why systolic architectures? IEEE computer, 15(1):37­46, 1982.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Qianli Liao, Joel Z Leibo, and Tomaso A Poggio. How important is weight symmetry in backpropagation? In AAAI, pp. 1837­1844, 2016.
Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning, pp. 689­696. ACM, 2009.
Julien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing. Foundations and Trends R in Computer Graphics and Vision, 8(2-3):85­283, 2014.
Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spikingneuron integrated circuit with a scalable communication network and interface. Science, 345 (6197):668­673, 2014.
Javier R Movellan. Contrastive Hebbian learning in the continuous hopfield model. In Connectionist models: Proceedings of the 1990 summer school, pp. 10­17, 1990.
Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381:13, 1996.
Randall C O'Reilly. Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm. Neural computation, 8(5):895­938, 1996.
Marc'Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efficient learning of sparse representations with an energy-based model. In Advances in neural information processing systems, pp. 1137­1144, 2007.
Christopher J Rozell, Don H Johnson, Richard G Baraniuk, and Bruno A Olshausen. Sparse coding via thresholding and local competition in neural circuits. Neural computation, 20(10):2526­2563, 2008.
Ron Rubinstein, Alfred M Bruckstein, and Michael Elad. Dictionaries for sparse representation modeling. Proceedings of the IEEE, 98(6):1045­1057, 2010.
Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energybased models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.
10

Under review as a conference paper at ICLR 2019
H Sebastian Seung and Jonathan Zung. A correlation game for unsupervised learning yields computational interpretations of hebbian excitation, anti-hebbian inhibition, and synapse elimination. arXiv preprint arXiv:1704.00646, 2017.
Samuel Shapero, Mengchen Zhu, Jennifer Hasler, and Christopher Rozell. Optimal sparse approximation with integrate and fire neurons. International journal of neural systems, 24(05):1440001, 2014.
Ping Tak Peter Tang. Convergence of LCA Flows to (C)LASSO Solutions. ArXiv e-prints, March 2016.
Ping Tak Peter Tang, Tsung-Han Lin, and Mike Davies. Sparse coding by spiking neural networks: Convergence theory and computational results. ArXiv e-prints, 2017.
Pietro Vertechi, Wieland Brendel, and Christian K Machens. Unsupervised learning of an efficient short-term memory network. In Advances in Neural Information Processing Systems, pp. 3653­ 3661, 2014.
James CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5): 1229­1262, 2017.
Xiaohui Xie and H Sebastian Seung. Equivalence of backpropagation and contrastive Hebbian learning in a layered network. Neural computation, 15(2):441­454, 2003.
Joel Zylberberg, Jason Timothy Murphy, and Michael Robert DeWeese. A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of v1 simple cell receptive fields. PLoS Comput Biol, 7(10):e1002250, 2011.
11

Under review as a conference paper at ICLR 2019

Appendices

A DETAILED DESCRIPTION OF PROPOSED NETWORK STRUCTURE

We propose a novel network topology with feedback shown in Figure 1(b). The figure shows two
"layers" of neurons. The lower layer consists of M neurons we call input neurons, nI,i for i = 1, 2, . . . , M ; the upper layer consists of N neurons we call coding neurons nC,i for i = 1, 2, . . . , N .

Each coding neuron nC,i receives excitatory signals from all the input neurons nI,j with a weight of

Fij  0. That is, each coding neuron has a row of the matrix F  RN0×M . In addition, neuron nC,i receives inhibitory signals from all other coding neurons nC,j with weight -Wij  0. W denotes this

matrix

of

weights:

W



N ×N
R0

and

diag(W )

=

0.

The

firing

thresholds

are



=

[1, 2, . . . , N ]T

and the matrix W + ,  = diag(), appears often and will denote it as H d=ef W + . Each

neuron nC,i also receives a constant negative bias of -(1 - )1si where 0   < 1 is an important

parameter that will be varied during the learning process to be detailed momentarily.

Each input neuron nI,i, i = 1, 2, . . . , M , with firing threshold fixed to be 1, receives a bias of (1 - )xi. Typically xi corresponds to the i-th pixel value of an input image in question during the

learning process. In addition, it receives excitatory spikes from each of the coding neurons with

weights

Bij



0.

That

is

each

input

neuron

has

a

row

of

the

matrix

B



M ×N
R0

.

These

excitatory

signals from the coding neurons constitute the crucial feedback mechanism we devised here that

enables dictionary learning.

B PROOF OF THEOREMS

B.1 THEOREM 1: SNN DYNAMICS, TRAJECTORY, AND LIMIT POINTS

In the simplest case when none of the neurons are inter-connected and i(0) < i for all i, then µi(t) = i for all i and all t  0. Hence those neurons ni with i > 0 produces a spike train of constant inter-spike interval of i/i; those neurons with i  0 will have no spiking activities. When however the neurons are inter-connected, the dynamics becomes non-trivial. It turns out that
one can so describe the dynamics mathematically that useful properties related to the current and
spike train can be derived. Consequently, a network of spiking neurons can be configured to help
solve certain practical problems.

Given a system of N neurons ni, i = 1, 2, . . . , N , we use vector notations µ(t) and (t) to denote the N currents and spike trains. The vector  and  are the input biases and firing thresholds. The

convolution (  )(t) is the N -vector whose i-th component is (  i)(t). For simplicity, we consider only  = 1 throughout the paper. Thus (t) = e-t for t  0 and 0 otherwise. Equation 1 in

vector form is

µ(t) =  + W (  )(t)

(13)

where W  RN×N and Wii = 0, encodes the inhibitory/excitatory connections among the neurons.

Because

d dt

(



)(t)

=

(t)

-

(



)(t),

we

have

µ (t) =  - µ(t) + W · (t).

(14)

Filtering Equation 14 yields

u (t) =  - u(t) + W a(t) + (µ(0) - u(t))/t u(t) -  a(t) =  + (W - ) a(t)
+ (µ(0) - u(t))/t - u (t)

(15)

where  = diag(). Theorem B.1 has been established previously in Tang et al. (2017) in a slightly different form. We attach the proof consistent to our notations below for completeness. It is established under the following assumptions:

· The currents of all neurons remain bounded from above, µ(t)   B for all t  0 for some B > 0. This implies no neuron can spike arbitrarily fast, and the fact that neurons
cannot spike arbitrarily rapidly implies the currents are bounded from below as well

12

Under review as a conference paper at ICLR 2019

· There is a positive number r > 0 such that whenever the numbers ti,k and ti,k+1 exist, ti,k+1 - ti,k  1/r. This assumption says that unless a neuron stop spiking althogether
after a certain time, the duration between consecutive spike cannot become arbitrarily long.

Theorem B.1.

As t  , u (t),

1 t

(µ(0)

-

u(t))

and

max(u(t),

0)

-



a(t)

all

converge

to

0.

Proof. Let

A = { i | neuron-i spikes infinitely often }

(A stands for "active"), and

I = { i | neuron-i stop spiking after a finite time }

(I stands for "inactive"). First consider i  I. Let ti,k be the time of the final spike. For any t > ti,k,

1 ti,k

1t

ui(t) = t 0

µi(s) ds + t

µi(s) ds
ti,k

1 ti,k

1

= t0

µi(s) ds + t i(t)

1 = iai(t) + t i(t)

Note that i(t)  i always. If i(t)  0, then

0  max(ui(t), 0) - iai(t)  i/t.

If i(t) < 0,

-iai(t)  max(ui(t), 0) - iai(t)  0.

Since i  I, ai(t)  0 obviously. Thus

max(ui(t), 0) - iai(t)  0.

Consider the case of i  A. For any t > 0, let ti,k be the largest spike time that is no bigger than t. Because i  A, ti,k   as t  .

1 ti,k

1t

ui(t) = t 0

µi(s) ds + t

µi(s) ds
ti,k

1t

=

iai(t) + t

µi(s) ds.
ti,k

Furthermore, note that because of the assumption ti,k+1 - ti,k  1/r always, where r > 0, lim inf ai(t)  r. In otherwords, there is a time T large enough such that ai(t)  r/2 for all i  A and t  T . Moreover, 0  t - ti,k  ti,k+1 - ti,k  1/r and µi(t)  [B-, B+]. Thus

1 t

t ti,k

µi(s) ds



1 t

[B-, B+]/r



0.

When this term is eventually smaller in magnitude than iai(t), we have

ui(t) - iai(t)  0.

or equivalently,

max(ui(t), 0) - iai(t)  0.

Applying Theorem B.1 to Equation 15 yields the following.
Theorem B.2. Given any > 0, there exists T > 0 such that for all t > T ,
(u(t) -  a(t)) - ( + (W - ) a(t))  < .
The following theorem characterizes limit points of the trajectory (u(t), a(t)). Recall that (u, a) is a limit point if given any > 0, there exists a time T > 0 large enough such that (u(T ), a(T )) is within to (u, a). Theorem B.3. Given any limit point (u, a), we must have  + (W - ) a  0, a ( + (W - ) a) = 0 and a  0, where is the elementwise product. Proof. Theorem B.1 shows that u -  a  0 and the elementwise product a (u -  a) = 0. But Theorem B.2 shows that u -  a =  + (W - ) a and the theorem here is established.

13

Under review as a conference paper at ICLR 2019

W

....

1 2



Figure 5: A 1-layer LCA network for sparse coding.

B.2 NON-NEGATIVE SPARSE CODING BY SPIKING NEURAL NETWORKS

Given a non-negative dictionary D



M ×N
R0

,

a

positive

scaling

vector

s

=

[s1, s2, . . . , sN ]T



RN>0

and an image x  RM0, the non-negative sparse coding problem can be formulated as

a = arg min l (a) , l(a) = 1

a0

2

x-Da

2 2

+



Sa

1

(16)

where S = diag(s). Using the well-known KKT condition in optimization theory, see for example Boyd & Vandenberghe (2004), a is an optimal solution iff there exists e  RN such that all of the following hold:

0  l(a) - e (stationarity)

e a = 0 (complementarity)

a  0, e  0

(feasibility)

(17)

where l is the generalized gradient of l. Note that the generalized gradient l(a) is DT Da - DT x + s  a 1 and that |ai| = 1 when ai > 0 and equals the interval [-1, 1] when ai = 0. Straightforward derivation then shows that a is an optimal solution iff

DT x -  s - DT Da  0 ; and a (DT x -  s - DT Da) = 0.

(18)

We now configure a N -neuron system depicted in Figure 5 so as to solve Equation 16. Set  = diag(DT D) as the firing thresholds and set  = DT x - s as the bias. Define the inhibition matrix to be -(DT D - ),  = diag(). Thus neuron-j inhibits neuron-i with weight -dTi dj  0. In this configuration, it is easy to establish µ(t)   C for all t  0 for some C > 0 as all connections are inhibitions. From Theorem B.3, any limit point (u, a) of the trajectory (u(t), a(t)) satisfies  + (W - )a  0 and a ( + (W - )a) = 0. But  = DT x -  s and W -  = -DT D. Thus a solves Equation 16. And in particular, if the solution to Equation 16 is unique, the trajectory
can only have one limit point, which means in fact the trajectory converges to the sparse coding
solution. This result can be easily extended to the network in Figure 1(a) by expanding the bias into another layer of input neuron with F = DT .

B.3 THEOREM 2: SPARSE CODING WITH FEEDBACK PERTURBATION

Equation 5, reflecting the structure of the coding and input neurons, takes the form:

e (t) f (t)

d=ef

u(t) - a(t) v(t) - b(t)

=

-(1 - )1s (1 - )x

+

-H B

F -I

a (t) b (t)

+ (t)

(19)

(u(t), v(t)) and (a(t), b(t)) denote the average currents and spike rates for the coding and input
neurons, respectively, and H d=ef W + . Note that max(u(t), 0) - a(t), max(v(t), 0) - b(t) and (t) all converge to 0 as t  .
Theorem B.4. Consider the configuration F T = B = D and   [0, 1). Suppose the soma currents and thus spike rates a(t)  are bounded. Let H = DT D + (1c)-1H , c = 1 - , be such that 4 H 1 a(t)  < min{si}. Then, for any > 0 there is T > 0 such that for all t > T , a(t) - a^(t)  < and a^(t) solves Equation 16 with S replaced by S^ where S - S^  < min{si}/2.

14

Under review as a conference paper at ICLR 2019

Proof. Consider  > 0 and define the vectors a^(t) and u^(t) for t  0 by each of their components:

(a^i(t), u^i(t)) =

(a,i(t), ia,i(t))

if a,i(t)   ,

(0, min(u,i(t), 0)) otherwise,

where  is the diagonal of DT D. Denote the perturbations a(t) d=ef a^(t) - a(t), u(t) d=ef
u^(t) - u(t), e^(t) d=ef u^(t) - a^(t), and e(t) d=ef e^(t) - e(t). This construction of u^(t) and a^(t) ensures a(t)  < , e^(t)  0, and e^(t) a^(t) = 0. Recall that max(u(t), 0) - a(t)  0 (Theorem B.1); thus u(t)  < 2 at t large enough.

Next, observe that v(t)  0 always (t)  0 always, for any setting  in [0, 1). Thus Theorem B.1 implies

b(t) - [(1 - )x + Ba(t)]  0

(20)

as t  . From Equation 19, this implies that

e(t) = c(DT x - 1s - DT Da(t) - 1H a(t) + (t))

for some (t) where (t)   0. Thus

(c)-1e^(t) = DT x - 1^s - DT Da^(t)

where ^s = s - ((t) + (t)), (t) = H a(t) and (t) = 1-1(DT Da(t) + e(t)/c + (t)). By assumption on H , s - (t) > (3/4)s > 0. Moreover, (t)  can be made arbitrarily small by
taking t and 1/ large enough. Thus there exist , T > 0 such that for all t > T , a(t)  < and ^s(t) - s  < min{si}/2, implying in particular ^s(t) > s/2 > 0. Finally, note that

a^(t)  0, (c)-1e^(t)  0, (c)-1e^(t) a^(t) = 0,

which shows (recall Equation 18) that a^(t) solves Equation 16 with S replaced by S^ and the proof is complete.

At present, we cannot establish a priori that the currents stay bounded when  > 0. Nevertheless, the theorem is applicable in practice as long as the observed currents stay bounded by some C for 0  t  T and C/T is small enough. See Section 3.3 for further comments.

B.4 THEOREM 3: GRADIENT CALCULATIONS FROM CONTRASTIVE LEARNING
Theorem B.5. Given any > 0, there is a T > 0 such that for all t, t > T , (Ba(t ) - x) - (b(t ) - b0(t))  < , cH(a0(t) - a(t )) - (H - F B)a(t ) + (ce0(t) - e(t ))  < .
Proof. Equation 20 implies that (Ba(t ) - x) - (b(t ) - b0(t))  0 as t, t  ,
establishing Equation 21. From Equations 19 and 20 - cs - cHa0(t) + cF x - ce0(t)  0, and, - cs - Ha(t) + cF x + F Ba(t) - e(t)  0.
Equation 22 thus follows.

(21) (22)

C COMPARISONS WITH PRIOR WORK
C.1 COMPARISONS OF DYNAMICAL NEURAL NETWORKS
Table 1 provides a summary of the development of three types of dynamical neural networks: Hopfield network, Boltzmann machine, and sparse coding network.

15

Under review as a conference paper at ICLR 2019

Neuron model Activation
Topology

Hopfield network
Binary or continuous (Hopfield, 1982; 1984) Binary: Thresholding Continuous: Any bounded, differentiable, strictly increasing function
Arbitrary symmetric bidirectional connections

Learning

Binary: Hebbian rule Continuous: contrastive learning (Movellan, 1990)

Limit point Many local minimum

Usage

Associative memory, constraint satisfaction problem

Neuron model Activation

Boltzmann machine
Binary or continuous (for visible units) (Ackley et al., 1985; Freund & Haussler, 1992) Logistic

Topology
Learning Limit point
Usage
Neuron model Activation

BM: Arbitrary symmetric bidirectional connections RBM:Two-layer with symmetric forward/backward (Hinton et al., 2006)
BM: contrastive learning RBM: contrastive divergence
Many local minimum Generative model, constraint satisfaction problem
Sparse coding network Continuous or spiking (Rozell et al., 2008; Shapero et al., 2014) Rectified linear

Topology Two-layer with feedforward, lateral, and feedback connections

Learning Contrastive learning with weight consistency

Limit point Usage

Likely unique (Bruckstein et al., 2008) Representation learning with sparse prior, image denoising and super-resolution, compressive sensing

Table 1: Comparison between dynamical neural networks. Text in boldface indicates the new results established in this work.

C.2 COMPARISONS OF DICTIONARY LEARNING NETWORKS

As we discussed in Section 1.1, there are several prior work that qualitatively demonstrate dictionary learning results in dynamical neural networks. The prior work Földiak (1990); Zylberberg et al. (2011); Brito & Gerstner (2016); Hu et al. (2014); Seung & Zung (2017); Vertechi et al. (2014); Brendel et al. (2017) employ a feedforward-only network topology as shown in Figure 1(a), and are unable to compute the true gradient for dictionary learning from local information. These work hence rely on additional heuristic or assumptions on input data for learning to work. In contrast, we propose to introduce feedback connections as shown in Figure 1(b), which allows us to solve the fundamental problem of estimating the true gradient. Recall the dictionary learning objective function (Equation 6 in the main text)

P

arg min

l(D, x(p), a(p)),

a(p)0,D0 p=1

1 l(D, x, a) =
2

x - Da

2 2

+

1

Sa

1+

2 2

D

2 F

,

(23)

and the true stochastic gradient of the learning problem is (Equation 7 in the main text)

D(new)  D -  (Da - x)aT + 2D .

(24)

16

Under review as a conference paper at ICLR 2019

Here we provide a detailed discussion on the difference and limitations of prior work.

The first line of work is the so-called Hebbian/anti-Hebbian network Földiak (1990); Zylberberg et al. (2011); Brito & Gerstner (2016); Hu et al. (2014); Seung & Zung (2017). The principle of learning

in these work is to apply Hebbian learning to learn excitatory feedforward weights (strengthen the excitatory weights if both input and coding neurons have strong activations) and anti-Hebbian learning

for inhibitory lateral weights (strengthen the inhibitory weights if both coding neurons have strong

activations). Due to heuristic nature of the learning rules, it is unclear whether this approach can

solve the dictionary learning problem in Equation 23. Zylberberg et al. (2011) argues that if for some

batch of successive inputs, the activities of the coding neurons are uncorrelated (i.e., their computed sparse codes are uncorrelated), and all the neurons have the same average activations, the Hebbian

learning rule can approximate the true stochastic gradient. Meanwhile, Zylberberg et al. (2011) does not provide arguments for the weight consistency between feedforward and lateral weights to be

ensured by anti-Hebbian learning. Hu et al. (2014) argues that this learning framework arises from

a different objective function other than 23. Instead, learning finds a dictionary for the following

objective function

arg min

XT X - AT A

2 F

,

A

(25)

where X  RM×P and A  RN×P are formed by stacking the input x and the sparse codes a along the columns, respectively. This formulation is somewhat different from the dictionary learning

objective function we are interested in.

The second line of work Vertechi et al. (2014); Brendel et al. (2017) proposes to learn the lateral weights according to the feedforward weights instead of using anti-Hebbian rules to address global weight consistency, although the learning of feedforward weight still follows Hebbian rules, giving the following update equation

D(new)  D +  xaT - 2D .

(26)

It can be seen that Equation 26 is not an unbiased estimate of the true stochastic gradient in Equation 24. Hence in theory the convergence of learning to an optimal solution cannot be guaranteed. Nontheless, Vertechi et al. (2014); Brendel et al. (2017) show that under the assumption that the input is whitened and centered, empirically Equation 26 can progressively learn a dictionary with improved reconstruction performance. For the general case of non-whitened input, the authors proposed a modified dictionary learning objective function:

l=

1 2

(xc

-

Da)T

C -1 (xc

-

Da)

+

1

a

1

(27)

where xc is the mean-removed training sample and C is the covariance, different from the common dictionary learning objective that we are interested in Equation 23. Note that in Vertechi et al. (2014); Brendel et al. (2017), it is unclear whether the non-negative constraints D  0, a  0 exist in the learning objective, although fundamentally the two constraints cannot be omitted: D  0 is needed so that the input neurons are only excitatory; a  0 is needed because spike rates cannot go negative.
Once these two constraints are incorporated, the objective function in Equation 27 may not be very
meaningful: the input to be estimated xc may have negative entries, while the estimation Da is always non-negative.

In this work, we directly estimate the true stochastic gradient for dictionary learning, and therefore we do not need to make additional assumptions on the training input. As discussed in the main text, obtaining such estimate requires adding the feedback connections with the resulting non-trivial network dynamics. We provide extensive analysis and proofs and show that dictionary learning can be solved under this setting.

Finally, we note that the need for feedback has been repeatedly pointed out in training autoencoder networks (Hinton & McClelland, 1988; Burbank, 2015). Autoencoder networks do not have the lateral connections as presented in the sparse coding network. Reconstruction errors there are computed by running the network, alternating between a forward-only and a backward-only phase. In contrast, we compute reconstruction errors by having our network evolve simultaneously with both feedforward and feedback signals tightly coupled together. Nevertheless, these models do not form strong back-coupled dynamical neural networks. Instead, they rely on staged processing much similar to a concatenation of feedforward networks. For our network, the dictionary learning relies

17

Under review as a conference paper at ICLR 2019
Figure 6: The figure shows a random subset of the dictionaries learned in spiking networks. They show the expected patterns of edges and textures (Lena), strokes and parts of the digits (MNIST), and Gabor-like oriented filters (natural scenes), similar to those reported in prior works (Rubinstein et al., 2010; Ranzato et al., 2007; Hoyer, 2004).
only on locations of the dynamics' trajectories at large time which need not be close to a stable limit point. Simple computations between these locations that corresponding to two different network configurations yield the necessary quantities such as reconstruction error or gradients for minimizing a dictionary learning objective function.
D ADDITIONAL NUMERICAL EXPERIMENT RESULTS
D.1 VISUALIZATION OF LEARNED DICTIONARIES In Section 4, we presented the convergence of dictionary learning by dynamical neural networks on three datasets: Lena, MNIST, and SparseNet. Figure 6 shows the visualization of the respectively learned dictionaries. Unsurprisingly, these are qualitatively similar to the well-known results from solving dictionary learning using canonical numerical techniques. D.2 IMAGE DENOISING USING LEARNED DICTIONARIES Here we further demonstrate the applicability of the dictionary learned by our dynamical neural networks. We use the dictionary learned from Dataset A (the Lena image) for a denoising task using a simple procedure similar to Elad & Aharon (2006): First we extract 8 × 8 overlapping patches from the noisy 512 × 512 Lena image generated with Gaussian noise. We then solve for the sparse coefficients of each patch in the non-negative sparse coding problem. Using the sparse coefficients, we can reconstruct the denoised patches, and a denoised image can be obtained by properly aligning and averaging these patches. On average, each patch is represented by only 5.9 non-zero sparse coefficients. Figure 7 shows a comparison between the noisy and the denoised image.
E RELATIONSHIPS BETWEEN CONTINUOUS AND SPIKING NEURON MODEL
FOR SPARSE CODING Although in this work we focus our discussions and analysis on spiking neurons, the learning strategy and mechanism can be applied to networks with continuous-valued neurons. The close relationships between using spiking and continuous-valued neurons to solve sparse approximation problems has
18

Under review as a conference paper at ICLR 2019

(a) Noisy image (PSNR=18.69dB)

(b) Denoised image (PSNR=29.31dB)

Figure 7: Image denoising using learned dictionary.

been discussed by Shapero et al. (2014); Tang et al. (2017). Here we attempt to provide an informal discussion on the connections between the two neuron models.

Following the derivation in Section 3, the dynamics of the spiking networks can be described using the average current and spike rates.

u (t) =  - u(t) + W a(t) + (µ(0) - u(t))/t

(28)

where u(t) and a(t) can be related by Theorem 1 as an "activation function".

a(t) = -1 max(u(t), 0) + (t), (t)  0

(29)

Equation 28 and 29 are closedly related to the dynamics of a network of continuous-valued neuron Rozell et al. (2008).

uc(t) = c - uc(t) + W ac(t) ac(t) = max(uc(t), 0)

(30) (31)

where uc(t) is the internal state variable of each neuron, ac(t) is the continuous activation value of each neuron, c is the input to each neuron, and W is the connection weight between neurons. One can immediately see the similarity. Note that although such "ReLU" type, asymmetric activation
function was not discussed in Rozell et al. (2008), it was later shown in Tang (2016) that this network
dynamics can solve a non-negative sparse coding problem.

19

