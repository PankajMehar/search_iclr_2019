Under review as a conference paper at ICLR 2019

NESTEROV'S METHOD IS THE DISCRETIZATION OF A DIFFERENTIAL EQUATION WITH HESSIAN DAMPING
Anonymous authors Paper under double-blind review

ABSTRACT
Su et al. (2014) made a connection between Nesterov's method and an ordinary differential equation (ODE). We show if a Hessian damping term is added to the ODE from Su et al. (2014), then Nesterov's method arises as a straightforward discretization of the modified ODE. Analogously, in the strongly convex case, a Hessian damping term is added to Polyak's ODE, which is then discretized to yield Nesterov's method for strongly convex functions. Despite the Hessian term, both second order ODEs can be represented as first order systems.
Established Liapunov analysis is used to recover the accelerated rates of convergence in both continuous and discrete time. Moreover, the Liapunov analysis can be extended to the case of stochastic gradients which allows the full gradient case to be considered as a special case of the stochastic case. The result is a unified approach to convex acceleration in both continuous and discrete time and in both the stochastic and full gradient cases.

1 INTRODUCTION

Su et al. (2014) made a connection between Nesterov's method for a convex, L-smooth function, f , and the second order, ordinary differential equation (ODE)

x¨

+

3 x

+

f (x)

=

0

t

(A-ODE)

However Su et al. (2014) did not show that Nesterov's method arises as a discretization of (A-ODE). In order to obtain such a discretization, we consider the following ODE, which has an additional Hessian damping term with coefficient 1/ L.

x¨

+

3 x

+

f (x)

=

-

1

D2f (x) · x + 1 f (x)

t Lt

(H-ODE)

Notice that (H-ODE) is a perturbation of (A-ODE), and the perturbation goes to zero as L  . Similar ODEs have been studied by Alvarez et al. (2002), they have been shown to accelerate
gradient descent in continuous time in (Attouch et al., 2016).

Next, we consider the case where f is also µ-strongly convex, and write Cf := L/µ for the condition number of f . Then Nesterov's method in the strongly convex case arises as discretization of the
following second order ODE

x¨

+

 2 µx

+

f

(x)

=

-

1

D2f (x) · x + µf (x)

L

(H-ODE-SC)

(H-ODE-SC) is a perturbation of Polyak's ODE (Polyak, 1964)

x¨

+

 2 µx

+

f

(x)

=

0

which is accelerates gradient when f is quadratic see (Scieur et al., 2017).
In each case, both continuous and discrete, as well and convex and strongly convex, it is possible to provide a proof of the rate using a Liapunov function. These proofs are already established in the literature: we give citations below, and also provide proof in the Appendix.

1

Under review as a conference paper at ICLR 2019

Moreover, the analysis for Nesterov's method in the full gradient can be extended to prove acceleration in the case of stochastic gradients. Acceleration of stochastic gradient descent has been established by Lin et al. (2015) and Frostig et al. (2015), see also Jain et al. (2018). A direct acceleration method with a connection to Nestero'v method was done by Allen-Zhu (2017). Our analysis unifies the continuous time ODE with the algorithm, and includes full gradient acceleration as a special case. The analysis proceeds by first rewriting (H-ODE) (and (H-ODE-SC)) as first order systems involving f , and then replacing the f with g = f + e. Both the continuous and discrete time methods achieve the accelerated rate of convergence, provided |e| goes to zero quickly enough. The condition on |e|, is given below in (12) and (13) - it is faster than the corresponding rate for stochastic gradient descent. When e = 0 we recover the full gradient case.
The renewed interested in the continuous time approach began with the work of Su et al. (2014) and was followed Wibisono et al. (2016); Wilson et al. (2016). Continuous time analysis also appears in Flammarion & Bach (2015), Lessard et al. (2016), and Krichene et al. (2015). However, continuous time approaches to optimization have been around for a long time. Polyak's method Polyak (1964) is related to successive over relaxation for linear equations (Varga, 1957) which were initially used to accelerate solutions of linear partial differential equations (Young, 1954). A continuous time interpretation of Newton's method can be found in (Polyak, 1987) or Alvarez et al. (2002). The mirror descent algorithm of Nemirovskii et al. (1983) has a continuous time interpretation (Bubeck et al., 2015). The Liapunov approach for acceleration had already appeared in Beck & Teboulle (2009) for FISTA.
The question of when discretizations of dynamical systems also satisfy a Liapunov function has been studied in the context of stabilization in optimal control Levant (1993). More generally, Stuart & Humphries (1996) studies when a discretization of a dynamical system preserves a property such as energy dissipation.

2 AN ODE REPRESENTATION FOR NESTEROV'S METHOD

2.1 CONVEX CASE

Despite the Hessian term, (H-ODE-SC) can be represented as the following first order system. Lemma 2.1. The second order ODE (H-ODE) is equivalent to the first order system

x

=

2 t

(v

- x) -

1 f (x),
L

v

=

-

t 2

f

(x).

(1st-ODE)

Proof. Solve for v in the first line of (1st-ODE)

v = t (x + 1 f (x)) + x 2L

differentiate to obtain

v

=

1 (x

+

1

f (x)) + t (x¨ + 1

D2f (x) · x ) + x .

2L

2L

Insert into the second line of (1st-ODE)

1 (x

+

1

f (x)) + t (x¨ + 1

D2f (x) · x ) + x = - t f (x).

2L

2L

2

Simplify to obtain (H-ODE).

The system (1st-ODE) can be discretized using the forward Euler method with a constant time step, h, to obtain Nesterov's method.

Definition 2.2. Define yk as the following convex combination of xk and vk.

yk

=

kxk k

+ 2vk . +2

(1)

2

Under review as a conference paper at ICLR 2019

Let h > 0 be a given small time step/learning rate and let tk = h(k + 2). The forward Euler method for (1st-ODE) with gradients evaluated at yk is given by

 xk+1

-

xk

=

2h tk (vk

-

xk )

-

h f (yk), L

  

vk+1

-

vk

=

- htk 2

f (yk)

(FE-C)

Remark 2.3. The forward Euler method simply comes from replacing x with (xk+1 - xk)/h and similarly for v. Normally the velocity field is simply evaluated at xk, vk. The only thing different about (FE-C) from the standard forward Euler method is that f is evaluated at yk instead of xk. However, this is still an explicit method. More general multistep methods and one leg methods in
this context are discussed in Scieur et al. (2017).

Recall the standard Nesterov's method from Nesterov (2013, Section 2.2)

 xk+1

=

yk

-

1 L

f

(yk

)

 

yk

=

xk+1

+

k

k +

3 (xk+1

-

xk )

(Nest)

 Theorem 2.4. The discretization of (H-ODE) given by (FE-C)(1) with h = 1/ L and tk = h(k+2)

is equivalent to the standard Nesterov's method (Nest).

 Proof. (FE-C) with h = 1/ L and tk = h(k + 2) becomes

xk+1

-

xk

=

k

2 +

2 (vk

-

xk )

-

1 L

f

(yk

)

k+2 vk+1 - vk = - 2L f (yk)

Eliminate the variable v using (1) to obtain (Nest).

2.2 STRONGLY CONVEX CASE

Now

we

consider

µ-strongly

convex,

and

L-smooth

functions,

f,

and

write

Cf

:=

L µ

for

the

condi-

tion number. We first show that (H-ODE-SC) can be represented as a first order system.

Lemma 2.5. The second order ODE (H-ODE-SC) is equivalent to the first order system

x

=

 µ(v

-

x)

-

1

f (x),

v

=

µ(x

-

v)

-

L

1 µ

f

(x).

(1st-ODE-SC)

Proof. Solve for v in the first line of (1st-ODE-SC)

1 v= 

(x + 1

f (x)) + x

µL

differentiate to obtain

1 v = 

(x¨ + 1

D2f (x) · x ) + x .

µL

Insert into the second line of (1st-ODE-SC)

1 

(x¨ + 1

D2f (x) · x ) + x = -x -

1

1 +

f (x).

µL

Lµ

Simplify to obtain (H-ODE-SC).

3

Under review as a conference paper at ICLR 2019

System (1st-ODE-SC) can be discretized using a forward Euler method with a constant time step h

to obtain Nesterov's method. Let h > 0 be a small time step, and apply the forward Euler method

for

(1st-ODE-SC)

evaluated at yk: 
xk+1 - xk 

=

1

 hµ + hµ (vk

-

xk )

-

h f (yk), L



   

vk+1

-

vk

=

1

h +

µ hµ (xk

-

vk )

-

h µ

f

(yk

)

(FE-SC)

where,

yk = (1 - h)xk + hvk,

 hµ h = 1 + hµ .

(2)

Now we recall the usual Nesterov's method for strongly convex functions from Nesterov (2013,

Section 2.2)

xk+1

=

yk

-

1 L

f

(yk

)

1 - Cf -1

(SC-Nest)

yk+1 = xk+1 +

(xk+1 - xk)

1 + Cf -1



Theorem 2.6. The discretization of (H-ODE-SC) given by (FE-SC) with h = 1/ L is equivalent

to the standard Nesterov's method (SC-Nest).



Proof. (FE-SC) with h = 1/ L becomes

xk+1 - xk vk+1 - vk

= =



Cf -1 1+ Cf -1

(vk

-

xk )

-

Cf -1
1+ Cf -1

(xk

-

vk )

-

1 L

f

(yk )

1 Lµ

f

(yk )

Eliminate the variable vk using the definition of yk to obtain (SC-Nest).

3 LIAPUNOV ANALYSIS

3.1 CONVEX CASE: CONTINUOUS AND DISCRETE TIME

Definition 3.1. Define the continuous time Liapunov function E(t, x, v) := t2(f (x) - f ) + 2|v - x|2

(3)

Define the discrete time Liapunov function Ek by

Ek = E(tk-1, xk, vk)

(4)

Proposition 3.2. Let f be a convex and L-smooth function. Let (x(t), v(t)) be a solution to

(1st-ODE), then

dE(t, x(t), v(t))  - t2 |f (x)|2. dt L

where E(t, x, v) in given by (3). In particular, for all t > 0,

f (x(t))

-

f



2 t2

|v0

-

x|2.

Furthermore, let xk, vk be given by (FE-C). Then for all k  0,

Ek+1  Ek - h2(f (xk) - f ) +

h - 1 L

t2kh|f (yk)|2.

In particular, if

h  1 L

then Ek is decreasing. When equality holds in (5),

f (xk)

-

f



(k

2 + 1)2

(f (x0) - f ) + |v0 - x|2

.

(5)

4

Under review as a conference paper at ICLR 2019

Most of the results stated above are already known, but for completeness we refer the proofs in Appendix A. Since (FE-C) is equivalent to Nesterov's method, the rate is known. The proof of the rate using a Liapunov function can be found in Beck & Teboulle (2009). Refer to ? which shows that we can use the constant time step. The discrete Liapunov function (4) was used in Su et al. (2014); Attouch & Peypouquet (2016) to prove a rate.

3.2 STRONGLY CONVEX CASE: CONTINUOUS AND DISCRETE TIME

Definition 3.3. Define the continuous time Liapunov function E(x, v)

E(x, v) = f (x) - f  + µ |v - x|2 2

Define the discrete time Liapunov function by

Ek

=

E (xk ,

vv )

=

f (xk)

-

f

+

µ 2

|vk

-

x|2.

Proposition 3.4. Let (x, v) be the solution of (1st-ODE-SC), then



dE(x,

v)



 - µE(x, v)

-

1

|f (x)|2 - µ

µ |v - x|2.

dt L 2

In particular, for all t > 0, E(x(t), v(t))  exp(-µt)E(x0, v0).

Next,

let

xk, vk

be

given

by

(FE-SC)

with

initial

condition

(x0, v0).

For

h



1 ,
L

we

have

Ek+1 - Ek  -hµEk.

In particular, for h = 1 ,
L

Ek+1  (1 - Cf -1)Ek.

(6) (7) (8)
(9) (10)

The discrete Liapunov function Ek was used to prove a rate in the strongly convex case by Wilson et al. (2016). The proof of (10) can be found in Wilson et al. (2016, Theorem 6). For completeness
we also provide the proof in Appendix E.

4 STOCHASTIC ACCELERATED METHOD

In the appendix we present results in continuous and discrete time for (non-accelerated) stochastic gradient descent. We also present results in continuous time for the stochastic accelerated case in the Appendix.
We present the results in discrete time here.

4.1 CONVEX STOCHASTIC CASE: DISCRETE TIME

In this section we consider stochastic gradients, which we write as a gradient plus an error term

f (yk) = f (yk) + ek

(11)

The stochastic gradient can be abstract, or it can error be a mini-batch gradient when f is a sum. Moreover, we can include the case where
ek = f (y~) - I f (y~) - (f (yk) - I f (yk))
corresponding to a correction by a snapshot of the full gradient at a snapshot location, which is updated every m iterations, as inJohnson & Zhang (2013). The combination of gradient reduction and momentum was discussed in Allen-Zhu (2017).
In order to obtain the accelerated rate, our Liapuonov analysis requires that the |ei| be decreasing fast enough. This can also be accomplished in the minibatch setting by using larger minibatches. In

5

Under review as a conference paper at ICLR 2019

this case, the rate of decrease of ei required gives a schedule for minibatch sizes. A similar result was obtained in Attouch & Peypouquet (2016).

When we replace gradients with (11) the Forward Euler scheme (FE-C) becomes

 xk+1

-

xk

=

2h tk (vk

-

xk )

-

h L

(f

(yk

)

+

ek ),

  

vk+1

-

vk

=

-h tk 2

(f (yk)

+

ek ),

(Sto-FE-C)

where yk is given by (1), h is a constant time step, and tk := h(k + 2). In Appendix C, we study the continuous version of (Sto-FE-C) and obtain a rate of convergence using a Liapunov function.

Definition 4.1. Define the discrete stochastic Liapunov function E~k := Ek + Ik, for k  0, where Ek is given by (4) and and, e-1 := 0 and for k  0,

k
Ik := h 2ti vi - x, ei-1 .

i=0

Theorem 4.2. Assume that the sequence ek satisfies

+

i|ei| < +

(12)

i=1

and

set

h

=

1 .
L

Then,

supi1

|vi

-

x|

<

+

and

E~k+1  E~k,

k0

We immediately have the following result.

Corollary 4.3.

Suppose ek

satisfies (12) and h =

1 .
L

Then, for k

 0,

f (xk) -

f



(k

C + 1)2 ,

with
+

C = 2L((f (x0) - f ) + |v0 - x|2) + 2 sup |vi - x| (i + 3)|ei|.
i1 i=0

Remark 4.4. The assumption on ek is satisfied, for example, by a sequence of the form |ek| = 1/k

for any  > 2. By comparison for SGD, the corresponding condition is satisfied by such sequences

with  > 1. Thus the norm of the noise needs to go to zero faster for accelerated SGD compared to

regular SGD (see Appendix B) in order to obtain the rate.



Remark 4.5. In Theorem 4.2, we focus on the maximum possible time step h = 1/ L. The

result is still

h

 i=k+1

2ti

true if we shrink the time vi - x, ei-1 , see Attouch

step. In this & Peypouquet

case, Ik (2016).

can

be

defined

using

the

tails

4.2 STRONGLY CONVEX STOCHASTIC CASE: DISCRETE TIME

In this section, we consider that stochastic gradient, which we write as a gradient plus an error, as in section 4.1. In Appendix B.2, we study the Stochastic gradient descent and Appendix C.2 is devoted to the analysis of the continuous framework of Stochastic Accelerated method. The Forward Euler scheme (FE-SC) becomes

 xk+1 

-

xk

=

h(vk

-

xk )

-

h L

(f

(yk

)

+

ek ),

h

  

vk+1

-

vk

=

h(xk

-

vk )

-

µ (f (yk)

+

ek ),

(Sto-FE-SC)

where ek is a given error and yk = (1 - h)xk + hvk,

hµ h = 1 + hµ .

Inspired by the continuous framework (Appendix C.2), we define a discrete Lyapunov function.

6

Under review as a conference paper at ICLR 2019

Definition 4.6. Define E~k := Ek + Ik, where Ek is given by (7) and

Ik

:=

 hµ

(1

-

hµ)k

k

(1 - hµ)-i

vi - x, ei-1 ,

i=0

with the convention e-1 = 0.

Then we obtain the following convergence result for sequences generated by (Sto-FE-SC).

Theorem 4.7. Let xk, vk be two sequences generated by the scheme (Sto-FE-SC) with initial con-

dition

(x0, v0).

Suppose

that

h

=

1 L

and

the

sequence

(ek )k

satisfies

+
(1 -
i=0

Cf -1)-iei < +.

(13)

Then,

E~k+1 (1 - Cf -1)kE~k.

In addition, supi0 |vi - x|  M for a positive constant M and

f (xk)

-

f

+

µ 2

|vk

-

x|2



A(1

-

Cf -1)k,

with

A

=

f (x0)

-

f

+

µ 2

|v0

-

x|2

+

M

+
(1

-

Cf -1)-iei-1

i=0

We include the proof of Theorem 4.7 since this result is new.

Proof of Theorem 4.7. First we prove that

Ek+1 - Ek  - Cf -1Ek

-

Cf -1

h(xk

- vk)

-

1 
µ

(f

(yk

)

+ ek), ek

- Cf -1 vk - x, ek

For the term Ik, we obtain

Ik+1 - Ik 

k+1
Cf -1(1 - Cf -1)k (1 - Cf -1) (1 -

i=0

k
- (1 - Cf -1)-i vi - x, ei-1

i=0

Cf -1)-i vi - x, ei-1

= - Cf -1Ik + Cf -1 vk+1 - x, ek .

Putting all together, we obtain

E~k+1 - E~k = Ek+1 - Ek + Ik+1 - Ik  - Cf -1E~k

+

1 L

|ek |2

+

Cf -1 
L

(vk

-

xk )

+

1 µ

f

(yk

),

ek

+ Cf -1 vk+1 - vk, ek

7

Under review as a conference paper at ICLR 2019

And by definition of vk+1 - vk, we have

E~k+1 - E~k



-

Cf -1

h(xk

- vk) -

1 Lµ

(f

(yk

)

+

ek ),

ek

+

Cf -1

h(xk

- vk) -

1 Lµ

(f

(yk

)

+

ek ),

ek

 - Cf -1E~k.

We conclude, as in the convex case, applying discrete Gronwall Lemma and (13).

REFERENCES
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1200­1205. ACM, 2017.
Felipe Alvarez, Hedy Attouch, Je´ro^me Bolte, and P Redont. A second-order gradient-like dissipative dynamical system with hessian-driven damping.-application to optimization and mechanics. Journal de mathe´matiques pures et applique´es, 81(8):747­780, 2002.
Hedy Attouch and Juan Peypouquet. The rate of convergence of Nesterov's accelerated forwardbackward method is actually faster than 1/k2. SIAM J. Optim., 26(3):1824­1834, 2016. ISSN 1052-6234. doi: 10.1137/15M1046095. URL https://doi.org/10.1137/ 15M1046095.
Hedy Attouch, Juan Peypouquet, and Patrick Redont. Fast convex optimization via inertial dynamics with hessian driven damping. Journal of Differential Equations, 261(10):5734­5783, 2016.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183­202, 2009.
Se´bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3-4):231­357, 2015.
Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size. In Conference on Learning Theory, pp. 658­695, 2015.
Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization. In International Conference on Machine Learning, pp. 2540­2548, 2015.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent for least squares regression. In Conference On Learning Theory, pp. 545­604, 2018.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pp. 315­323, 2013.
Walid Krichene, Alexandre Bayen, and Peter L Bartlett. Accelerated mirror descent in continuous and discrete time. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2845­ 2853. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/ 5843-accelerated-mirror-descent-in-continuous-and-discrete-time. pdf.
Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57­95, 2016.
Arie Levant. Sliding order and sliding accuracy in sliding mode control. International journal of control, 58(6):1247­1263, 1993.

8

Under review as a conference paper at ICLR 2019
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for first-order optimization. In Advances in Neural Information Processing Systems, pp. 3384­3392, 2015.
Arkadii Nemirovskii, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity and method efficiency in optimization. 1983.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964.
Boris T Polyak. Introduction to optimization. translations series in mathematics and engineering. Optimization Software, 1987.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400­407, 1951. ISSN 00034851. URL http://www.jstor.org/ stable/2236626.
Damien Scieur, Vincent Roulet, Francis Bach, and Alexandre d'Aspremont. Integration methods and accelerated optimization algorithms. arXiv preprint arXiv:1702.06751, 2017.
AM Stuart and AR Humphries. Dynamical systems and numerical analysis, volume 2 of Cambridge Monographs on Applied and Computational Mathematics. Cambridge University Press, Cambridge, 1996.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov's accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pp. 2510­2518, 2014.
Richard S Varga. A comparison of the successive overrelaxation method and semi-iterative methods using chebyshev polynomials. Journal of the Society for Industrial and Applied Mathematics, 5 (2):39­46, 1957.
Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, pp. 201614734, 2016.
Ashia C Wilson, Benjamin Recht, and Michael I Jordan. A lyapunov analysis of momentum methods in optimization. arXiv preprint arXiv:1611.02635, 2016.
David Young. Iterative methods for solving partial difference equations of elliptic type. Transactions of the American Mathematical Society, 76(1):92­111, 1954.
A CONTINUOUS FRAMEWORK: ODE AND RATE
Proof of Prof 3.2. By definition of E, we have dE  2t(f (x) - f ) + t2 f (x), x dt +4 v - x, v  2t(f (x) - f ) + 2t f (x), v - x - t2 |f (x)|2 L -2t v - x, f (x)  2t(f (x) - f  - x - x, f (x) ) - t2 |f (x)|2. L
The proof is concluded by convexity, f (x) - f  - x - x, f (x)  0.
9

Under review as a conference paper at ICLR 2019

Proof of Proposition 3.4. Using (1st-ODE-SC), we obtain

dE(x, v) =

f (x), x

 + µ

v - x, v

dt

=

 µ

f (x), v

-

x

- 1

|f (x)|2

-

 µµ

v

-

x, v

-

x

-

 µ

f (x), v

-

x

L = -µ f (x), x - x - 1 |f (x)|2 - µ µ |v - x|2 + |v - x|2 - |x - x|2
L2

By strong convexity, we have

dE(x, v)  -µ f (x) - f  + µ |x - x|2 - 1 |f (x)|2

dt 

2

L

µ -

µ

|v - x|2 + |v - x|2 - |x - x|2

2

 -µE(x, v) - 1 |f (x)|2 - µ µ |v - x|2.

L2

B STOCHASTIC GRADIENT DESCENT

B.1 CONVEX CASE: CONTINUOUS AND DISCRETE TIME

Let e : [0, +)  Rd be a integrable function. Consider the gradient descent x = -(f (x) + e(t)).
Then define the Lyapunov function, E~, by E~(t, x) = E(t, x) + I(t),

(14)

where, and,

E(t, x) = t(f (x) - f ) + 1 |x - x|2, 2
t
I(t) = x(s) - x + sf (x(s)), e(s) ds.
0

Then the following result holds.

Proposition B.1. Let x be a solution of (14) with initial condition x0. Then,
dE~(t, x)  -t|f (x)|2. dt
In addition, if f is L-smooth, sups0 |x(s) - x| < +, sups0 s|f (x(s))| < + and

f (x(t)) - f   1 t

f (x0) - f  +

1 2

|x0

-

x|2

+

sup
s0

|x(s)

-

x

+

sf

(x(s))|

e

L1 (0,+)

.

Proof. For all t > 0, we have

·

dE(t,x) dt

=

(f (x) -

f

-

f (x), x - x

) - t|f (x)| -

x - x + tf (x), e ,

·

dI (t) dt

=

x - x + tf (x), e .

Then, since f is convex, we obtain the first result. We deduce that E~ is decreasing. Arguing as Attouch et al. (2016) along with the co-coercivity inequality, we prove that sups0 |x(s) - x| < +, sups0 s|f (x(s))| < + which concludes the proof.

10

Under review as a conference paper at ICLR 2019

The discretization of (14) is

xk+1 - xk = -h(f (xk) + ek),

(15)

where ek = e(hk).

Define E~k by

E~k = Ek + Ik,

where, for tk := hk,

Ek

=

tk(f (xk)

-

f)

+

1 2 |xk

-

x|2,

and,
k-1
Ik = h xi - x - ti+1(f (xi) + ei), ei .
i=0

Proposition B.2. Let xk be the sequence generated by (15) with initial condition x0. Assume that h satisfies, for all k  0,

1

h(Ltk+1

+

1)

-

2tk+1



0



h



. L

(16)

Then the E~k is decreasing. In addition if (ek)k and (tk+1|ek|2)k are summable, supi0 |xi - x| < +, supi0 |ti+1f (xi)| < + and

f (xk)

-

f



1 tk

1 2

|x0

-

x|2

+

sup
i0

|xi

-

x

+

ti+1f

(xi)|

+
(|ei|
i=0

+

ti+1|ei|2)

.

Proof. By L-smoothness and convexity of f , we have

Ek+1 - Ek  -htk+1 f (xk), f (xk) + ek

+(Ltk+1

+

1)

h2 2

|f

(xk )

+

ek |2

- xk - x, ek

+h(f (xk) - f  - f (xk), xk - x )



((Ltk+1

+

1)h

-

2tk+1)

h 2

|f

(xk )

+

ek |2

-h xk - x, ek + tk+1h f (xk) + ek, ek .

In addition,

Ik+1 - Ik = h xk - x - tk+1(f (xk) + ek), ek ,

therefore,

E~k+1

-

E~k



((Ltk+1

+

1)h

-

2tk+1

)

h 2

|f

(xk )

+

ek |2



0,

when h satisfies (16). We conclude the proof with the same argument as Proposition B.1.

B.2 STRONGLY CONVEX CASE: CONTINUOUS AND DISCRETE TIME

Let us study the equation

x = -(f (x) + e(t)),

(17)

for an error function, e satisfying

+
eµs|e(s)| ds < +.
0

(18)

This condition on the error function is classical Robbins & Monro (1951). The case e = 0 is satisfied trivially and corresponds to the gradient descent ODE.

11

Under review as a conference paper at ICLR 2019

We define the function E : [0, +) × Rd  [0, +) by

E(t, x) = 1 |x - x|2 + I(t), 2

where,

t
I(t) = e-µt eµs x(s) - x, e(s) ds.

0

Then we have the following result.

Proposition B.3. Let x be a solution of (17) with initial data x0 and suppose that e satisfies (18).
Then, dE(t, x)  -µE(t, x). dt
In addition, supt0 |x - x| < + and

1 |x - x|2  e-µt 2

1 2

|x0

-

x|2

+

sup
s0

|x(s)

-

x|

+
eµs|e(s)| ds
0

.

Proof. For all t > 0,

dE(t, x) = - x - x, f (x) - x - x, e - µI(t) + x - x, e dt  - µ |x - x|2 - µI(t) = -µE(t, x). 2

Therefore E(t, x(t)) is decreasing and then for all t > 0,

1 |x(t) 2

-

x|2



1 2

|x0

-

x|

+

t
|x(s) - x|eµs|e(s)| ds.
0

By Gronwall Lemma and (18), we deduce that supt0 |x - x| < + and the proof is concluded.

The discretization of (17) is

xk+1 - xk = -h(f (xk) + ek),

where ek = e(hk). We define Ek, for k  1, by

Ek

=

1 2

|xk

-

x|2

+

Ik ,

where,

k
Ik = (1 - hµ)kh (1 - hµ)-i xi - x, ei-1 ,

i=0

with the notation e-1 = 0.

Proposition B.4.

Assume that h 

1 L

.

Then,

Ek+1 - Ek  -hµEk.

In addition, if the sequence (1 - hµ)-i|ei| is summable, supi1 |xi - x| < + and we deduce,

1 2

|xk

-

x|2



(1

-

hµ)k

1 2

|x0

-

x|2

+

h

sup
i1

|xi

-

x|

+
(1
i=0

-

hµ)-i-1|ei|

.

Proof. First, as usual, we have

1 2 |xk+1

-

x|2

-

1 2 |xk

-

x|2

=

-h f (xk), xk - x

- h ek, xk - x

+

h2 2

|f (xk)

+

ek |2



-

hµ 2

|xk

-

x|2

+

h(f 

-

f (xk))

+

h2 2

|f (xk)

+

ek |2



-

hµ 2

|xk

-

x|2

-

h 2L

|f

(xk

)|2

+

h2 2

|f (xk)

+

ek |2 .

12

Under review as a conference paper at ICLR 2019

In addition,

k+1

k

Ik+1 - Ik = h(1 - hµ)k (1 - hµ) (1 - hµ)-i xi - x, ei-1 - (1 - hµ)-i xi - x, ei-1

i=0
= -hµIk + h xk+1 - x, ek .

i=0

Combining these two inequalities,

Ek+1 - Ek



-hµEk + h xk+1 - xk, ek

-

h 2L

|f

(xk

)|2

+

h2 2

|f (xk)

+

ek |2



-hµEk

+

h 2

h- 1 L

|f (xk)|2

-

h2 2

|ek |2

 -hµEk,

when

h



1 L

.

In order to conclude, we also need to establish that Ek is bounded below. That follows from discrete Gronwall's inequality, as was already done in the continuous case in Proposition B.3.

C STOCHASTIC ACCELERATED CONTINUOUS TIME
In this section, we consider that an error e(t) is made in the calculation of the gradient. C.1 CONVEX CASE We study the following perturbation of system (1st-ODE),

x

=

2 t

(v

-

x)

-

1 (f (x)
L

+

e(t)),

v

=

-

t 2

(f

(x)

+

e(t)).

where e is a function satisfying

(Sto-1st-ODE)

The corresponding ODE is

+
s|e(s)| < +.
0

(19)

x¨

+

3 x

+

1

D2f (x) · x +

1 + 1 f (x) = -

1 + 1 e(t) - 1 e (t).

tL

tL

tL L

We follow the argument from Attouch et al. (2016, section 5) to define a Lyapunov function for this system. Let E~ be defined by

E~(t, x, v) = E(t, x, v) + I(t, x, v),

where,

E(t, x, v) = t2(f (x) - f ) + 2|v - x|2,

and I(t, x, v) = t s 2(v - x) + s f (x), e(s) ds. 0L
Lemma C.1. Let (x, v) be a solution of (Sto-1st-ODE) with initial condition (x(0), v(0)) = (x0, v0) and suppose that e satisfies (19). Then

dE~ (t, x, v)  - t2 |f (x)|2. dt L

In addition, supt0 |v(t) - x| < + and supt0 |tf (x)| < +.

13

Under review as a conference paper at ICLR 2019

Proof. Following the proof of Proposition 3.2, we have

dE (t, x, v)  - t2 |f (x)|2 - t2 f (x), e(t) - 2t v - x, e(t) . dt L L

In addition,

dI (t, x, v) = t2 f (x), e(t) + 2t v - x, e(t) . dt L

Then, In particular, E~ is decreasing and

dE~  - t2 |f (x)|2. dt L

t
t2(f (x) - f ) + 2|v - x|2  2|x0 - x|2 - s 2(v - x) + Csf (x), e(s) ds.
0
Using the inequality of co-coercitivity, we obtain

1 |tf (x)| 2L

+

2|v

-

x|



2|x0

-

x|2

+

1 2L

+

2

+

t 0

1 |sf (x)| + 2|v - x| L

|se(s)| ds.

Using (19), we conclude applying Gronwall Lemma.

Then we deduce

Proposition C.2. Let (x, v) be a solution of (Sto-1st-ODE) with initial condition (x(0), v(0)) =

(x0, v0) and suppose that e satisfies (19). Then,

f (x(t))

-

f



1 t2

2|v0 - x|2 + sup 2(v(s) - x) + s f (x(s)) s0 L

+
s|e(s)| ds
0

.

C.2 STRONGLY CONVEX STOCHASTIC CASE: CONTINUOUS TIME

Define the perturbed system of (1st-ODE-SC) by

x = µ(v - x) - 1 (f (x) + e(t)),

v

=

µ(x

-

v)

-

L
1µ (f (x)

+

e(t)).

(Sto-1st-ODE-SC)

where e is a locally integrable function.

Definition C.3. Define the continuous time Liapunov function E(x, v) E(x, v) = f (x) - f  + µ |v - x|2 2
Define the perturbed Liapunov function E~, by

(20)

E~(t, x, v) := E(x, v) + I(t, x, v),

I(t, x) := e-µt

t
e µs

 µ(v(s)

-

x)

+

1

f (x), e(s)

0L

Proposition C.4. We have,



d E~(t, x, v)  -µE~ - 1 |f (x)|2 - µµ |v - x|2.

dt L 2

ds.

Proof. Using (8), we obtain

d E~(t, x, v) 

d

E(x,

v)

-

 µI

(t,

x)

+

 µ(v

-

x)

+

1

f (x), e

dt



dt -µE(x, v) -

 µ(v

-

x)

+

1

L f (x), e - 1

|f (x)|2

-

µµ |v

-

x|2

L L2

-µI(t, x) +

 µ(v

-

x)

+

1

f (x), e

L

 -µE~(t, x, v) - 1 |f (x)|2 - µµ |v - x|2

L2

14

Under review as a conference paper at ICLR 2019


Lemma C.5. Suppose f is bounded from below and s  e µse(s)  L1. Let (x, v) be a solution of (Sto-1st-ODE-SC), then supt0 |v(t) - x| < + and supt0 |f (x)| < +.

Proof.

Same

as

Attouch

et

al.

(2016,

Lemma

5.2),

using

the

fact

that

1 2L

|f

(x)|2



f (x) - f ,

t  E~(x(t), v(t)) is decreasing and Gronwall's inequality.

Then, combining the two previous result, we obtain: Corollary C.6. Suppose that s  ese(s) is a L1(0, +) function. Let (x, v) be a solution of (Sto-1st-ODE-SC) with initial condition (x(0), v(0)) = (x0, v0). Then,
f (x(t)) - f  + µ |v(t) - x|2  Ce-t, 2

where,

C

=

f (x0)

-

f

+

µ 2

|v0

-

x|2

+

ese(s)

L1(0,+) sup
s0

 µ(v(s)

-

x)

+

1 f (x) L

.

Proof. By Proposition C.4 and Gronwall's Lemma, we have E~(t, x(t), v(t))  e-µtE~(0, x0, v0).

This is equivalent to

f (x(t)) - f  + µ |v(t) - x|2 2



e-µt

f (x0) - f  +

µ 2

|v0

-

x|2

+

t

 µ(v(s)

-

x)

+

1

f (x)


|e µsg(s)|ds

0L

 Ce-µt,

which concludes the proof with Lemma C.5.

D PROOF THEOREM 4.2

First, using the convexity and the L-smoothness of f , we obtain the following classical inequality (see Attouch & Peypouquet (2016) or Su et al. (2014) in the case ek = 0),

f (xk+1) - f 



k

k +

2

(f (xk)

-

f)

+

k

2 +

2

f (yk), vk - x

+ h ek, f (yk) + ek + L

h2 - h 2L

|f (yk) + ek|2.

Then, we have

tk2 (f (xk+1) - f ) - tk2-1(f (xk) - f )



kt2k k+2

-

tk2-1

(f (xk) - f ) +

2t2k k+2

f (yk), vk

- x

+ ht2k L

ek, f (yk) + ek

+

h - 1 2L

ht2k|f (yk) + ek|2.

By defintion of vk+1, we have

2|vk+1 - x|2 - 2|vk - x|2

=

-2htk vk - x, f (yk) + ek

+

h2t2k 2

|f (yk)

+

ek |2 .

In addition,

Ik+1 - Ik = 2htk vk+1 - x, ek .

15

Under review as a conference paper at ICLR 2019

Combining these three previous inequalities, we obtain

E~k+1 - E~k



-h2(f (xk) - f ) +

h - 1 L

t2kh|f (yk) + ek|2

+2htk ek, vk+1 - vk + htk2 ek, f (yk) + ek L



-h2(f (xk) - f ) +

h - 1 L

tk2 h|f (yk) + ek|2

htk2 L

-

h2tk2

ek, f (yk) + ek .

Since

h

=

1 ,
L

we

deduce

that

E~k

is

decreasing.

In

particular,

2|vk

-

x|2



2|v0

-

x|2

+

1 L

k-1
|vi

-

x|(i

+

3)|ei|.

i=0

and the discrete version of Gronwall Lemma gives the result since (i+3)|ei| is a summable sequence due to (12).

E PROOF OF PROPOSITION 3.4

To

simplify,

we

denote

h

=

hµ 1+h µ

.

Note,

however,

since

the

gradients

are

evaluated

at

yk ,

not

xk ,

the first step is to use strong convexity and L-smoothness to estimate the differences of E in terms

of gradients evaluated at yk.

Lemma E.1. Suppose that f is a µ-stgrongly convex and L-smooth function, then

f (xk+1) - f (xk) 

f (yk), yk - xk

-

µ 2

|yk

-

xk |2

+

h 2

h - 2 L

|f (yk)|2.

(21)

Proof. First, we remark that

f (xk+1) - f (xk) = f (xk+1) - f (yk) + f (yk) - f (xk)



f (yk), xk+1 - yk

+

L 2

|xk+1

-

yk |2

+ f (yk), yk - xk

-

µ 2

|yk

-

xk |2 .

Since the first line of (1st-ODE-SC) can be rewritten as

xk+1

=

yk

-

h L

f

(yk

),

we obtain (21).

Proof of Proposition 3.4. Once (9) is established, since the expression on the right hand side is monotone in h, the largest choice of h is given by h = 1 , which leads immediately to (10).
L
In the proof we will estimate the linear term yk - xk, f (yk) in terms of yk - x, f (yk) plus a correction which is controlled by the gap (the negative quadratic) in (21) and the quadratic term in E.

The second term in the Liapunov function gives, using 1-smoothness of the quadratic term in E.

µ 2

|vk+1 - x|2 - |vk - x|2

=

µ vk - x, vk+1 - vk

+

µ 2

|vk+1

-

vk |2

=

-µh -h µ

vk - x, vk - xk vk - x, f (yk)

+

µ 2

|vk+1

-

vk |2 .

16

Under review as a conference paper at ICLR 2019

Before going on, using the fact from (2), that yk is a convex combination of xk and vk, we have

h(vk -xk)

=

1

h - h

(vk

-yk

)

=

 h µ(vk -yk)

and

vk -yk

=

1

- h h

(yk

- xk )

=

1 hµ (yk -xk)

which gives

µ 2

|vk+1 - x|2 - |vk - x|2

= -hµµ vk - x, vk - yk

-hµ vk - yk, f (yk)

-

 hµ

yk

-

x, f (yk)



µ

2

|vk+1 - 
hµ µ

-

2

vk |2 |vk -

x|2

+

|vk

-

yk |2

-

|yk

-

x|2

- yk - xk, f (yk)

-

 hµ

f (yk)

-

f

+

µ 2

|yk

-

x|2

µ 2

|yk

-

xk |2

+

2h µ

yk - xk, f (yk)

+

h2 µ

|f (yk)|2

,

by strong convexity. Then using the L-smoothness of f , we obtain

µ 2

|vk+1 - x|2 - |vk - x|2

 +

-hµEk - yk-

µ 2

+

h

µL 2

-

µ 2h

xk, f (yk) |yk - xk|2

+

h2 2

|f (yk)|2.

(22)

Combining (21) and (22), we have 
Ek+1 - Ek  -h µEk +

h2 - h L

|f (yk)|2 +

hµL µ -
2 2h

|xk - yk|2

which concludes the proof of (9).

17

