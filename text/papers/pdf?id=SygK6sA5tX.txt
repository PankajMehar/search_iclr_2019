Under review as a conference paper at ICLR 2019
GRAPH CLASSIFICATION WITH GEOMETRIC SCATTERING
Anonymous authors Paper under double-blind review
ABSTRACT
One of the most notable contributions of deep learning is the application of convolutional neural networks (ConvNets) to structured signal classification, and in particular image classification. Beyond their impressive performances in supervised learning, the structure of such networks inspired the development of deep filter banks referred to as scattering transforms. These transforms apply a cascade of wavelet transforms and complex modulus operators to extract features that are invariant to group operations and stable to deformations. Furthermore, ConvNets inspired recent advances in geometric deep learning, which aim to generalize these networks to graph data by applying notions from graph signal processing to learn deep graph filter cascades. We further advance these lines of research by proposing a geometric scattering transform using graph wavelets defined in terms of random walks on the graph. We demonstrate the utility of features extracted with this designed deep filter bank in graph classification, and show its competitive performance relative to other methods, including graph kernel methods and geometric deep learning ones, on both social and biochemistry data.
1 INTRODUCTION
Over the past decade, numerous examples have established that deep neural networks (i.e., cascades of linear operations and simple nonlinearities) typically outperform traditional "shallow" models in various modern machine learning applications, especially given the increasing Big Data availability nowadays. Perhaps the most well known example of the advantages of deep networks is in computer vision, where the utilization of 2D convolutions enable network designs that learn cascades of convolutional filters, which have several advantages over fully connected network architectures, both computationally and conceptually. Indeed, in terms of supervised learning, convolutional neural networks (ConvNets) hold the current state of the art in image classification, and have become the standard machine learning approach towards processing big structured-signal data, including audio and video processing. See, e.g., Goodfellow et al. (2016, Chapter 9) for a detailed discussion.
Beyond their performances when applied to specific tasks, pretrained ConvNet layers have been explored as image feature extractors by freezing the first few pretrained convolutional layers and then retraining only the last few layers for specific datasets or applications (e.g., Yosinski et al., 2014; Oquab et al., 2014). Such transfer learning approaches provide evidence that suitably constructed deep filter banks should be able to extract task-agnostic semantic information from structured data, and in some sense mimic the operation of human visual and auditory cortices, thus supporting the neural terminology in deep learning. An alternative approach towards such universal feature extraction was presented in Mallat (2012), where a deep filter bank, known as the scattering transform, is designed, rather than trained, based on predetermined families of distruptive patterns that should be eliminated to extract informative representations. The scattering transform is constructed as a cascade of linear wavelet transforms and nonlinear complex modulus operations that provides features with guaranteed invariance to a predetermined Lie group of operations such as rotations, translations, or scaling. Further, it also provides Lipschitz stability to small diffeomorphisms of the inputted signal. Scattering features have been shown to be effective in several audio (e.g., Bruna & Mallat, 2013a; Ande´n & Mallat, 2014; Lostanlen & Mallat, 2015) and image (e.g., Bruna & Mallat, 2013b; Sifre & Mallat, 2014; Oyallon & Mallat, 2015) processing applications, and their advantages over learned features are especially relevant in applications with relatively low data availability, such as quantum chemistry (e.g., Hirn et al., 2017; Eickenberg et al., 2017; 2018).
1

Under review as a conference paper at ICLR 2019
Following the recent interest in geometric deep learning approaches for processing graph-structured data (see, for example, Bronstein et al. (2017) and references therein), we present here a generalization of the scattering transform from Euclidean domains to graphs. Similar to the Euclidean case, our construction is based on a cascade of bandpass filters, defined in this case using graph signal processing (Shuman et al., 2013) notions, and complex moduli, which in this case take the form of absolute values (see Sec. 3). While several choices of filter banks could generally be used with the proposed cascade, we focus here on graph wavelet filters defined by lazy random walks (see Sec. 2). These wavelet filters are also closely related to diffusion geometry and related notions of geometric harmonic analysis, e.g. the diffusion maps algorithm of Coifman & Lafon (2006) and the associated diffusion wavelets of Coifman & Maggioni (2006). Therefore, we call the constructed cascade geometric scattering, which also follows the same terminology from geometric deep learning.
We note that similar attempts at generalizing the scattering transform to graphs have been presented in Chen et al. (2014) as well as Zou & Lerman (2018); Gama et al. (2018). The latter two works are most closely related to the present paper. In them, the authors focus on theoretical properties of the proposed graph scattering transforms, and show that such transforms are invariant to graph isomorphism. The geometric scattering transform that we define here also possesses the same invariance property, and we expect similar stability properties to hold for the proposed construction as well. However, in this paper we focus mainly on the practical applicability of geometric scattering transforms for graph-structured data analysis, with particular emphasis on the task of graph classification, which has received much attention recently in geometric deep learning (see the numerical results in Sec. 4 for a listing of many new graph classification algorithms).
In supervised graph classification problems one is given a training database of graph/label pairs {(Gi, yi)}Ni=1  G × Y sampled from a set of potential graphs G and potential labels Y. The goal is to use the training data to learn a model f : G  Y that associates to any graph G  G a label y = f (G)  Y. These types of databases arise in biochemistry, in which the graphs may be molecules and the labels some property of the molecule (e.g., its toxicity), as well as in various types of social network databases. Until recently, most approaches were kernel based methods, in which the model f was selected from the reproducing kernel Hilbert space generated by a kernel that measures the similarity between two graphs; one of the most successful examples of this approach is the Weisfeiler-Lehman graph kernel of Shervashidze et al. (2011). Numerous feed forward deep learning algorithms, though, have appeared over the last few years. In many of these algorithms, task based (i.e., dependent upon the labels Y) graph filters are learned from the training data as part of the larger network architecture. These filters act on a characteristic signal xG that is defined on the vertices of any graph G, e.g., xG is the vector of degrees of each vertex (we remark there are also edge based algorithms, such as Gilmer et al. (2017) and references within, but these have largely been developed for and tested on databases not considered in Sec. 4). Here, we propose an alternative to these methods in the form of a geometric scattering classifier (GSC) that leverages graph-dependent (but not label dependent) scattering transforms to map each graph G to the scattering features extracted from xG. Furthermore, inspired by transfer learning approaches such as Oquab et al. (2014), we apply the scattering cascade as frozen network layers on xG, followed by several fully connected classification layers (see Fig. 2). We note that while the formulation in Sec. 3 is phrased for a single signal xG, it naturally extends to multiple signals by concatenating their scattering features.
We evaluate the quality of the scattering features and the resulting classification by comparing it to numerous graph kernel methods and deep learning methods over thirteen datasets (seven biochemistry ones and six social network ones) commonly studied in related literature. In terms of classification accuracy on individual datasets, we show that the proposed GSC approach obtains near state of the art results on most datasets, despite only learning the fully connected layers that come after the geometric scattering transform. Furthermore, while other methods may excel on specific datasets, when considering average accuracy over all datasets, or even within each field (i.e., biochemistry or social networks), our proposed GSC outperforms nearly all feed forward neural network approaches, and is competitive with the state of the art graph kernel method of Kriege et al. (2016) and the recent graph recurrent neural network approach of Taheri et al. (2018). We regard this result as crucial in establishing the universality of the graph features extracted by geometric scattering, as they provide an effective task-independent representation of analyzed graphs.
The remainder of this paper is organized as follows. In Section 2 we define the graph wavelets. The geometric scattering transform is presented in Section 3. Empirical results are reported in Section 4, and a short conclusion is given in Section 5. Additional details are provided in the appendices.
2

Under review as a conference paper at ICLR 2019

2 GRAPH RANDOM WALKS AND GRAPH WAVELETS

We define graph wavelets as the difference between lazy random walks that have propagated at different time scales, which mimics classical wavelet constructions found in Meyer (1993) as well as more recent constructions found in Coifman & Maggioni (2006). The underpinnings for this construction arise out of graph signal processing, and in particular the properties of the graph Laplacian.

Let G = (V, E, W ) be a weighted graph, consisting of n vertices V = {v1, . . . , vn}, edges E  {(v , vm) : 1  , m  n}, and weights W = {w(v , vm) > 0 : (v , vm)  E}. Note that unweighted graphs are considered as a special case, by setting w(v , vm) = 1 for each (v , vm)  E.
Define the n × n (weighted) adjacency matrix AG = A of G by A(v , vm) = w(v , vm) if (v , vm)  E and zero otherwise, where we use the notation A(v , vm) to denote the ( , m) entry of the matrix A so as to emphasize the correspondence with the vertices in the graph and to reserve sub-indices for enumerating objects. Define the (weighted) degree of vertex v as deg(v ) = m A(v , vm) and the corresponding diagonal n × n degree matrix D given by D(v , v ) = deg(v ), D(v , vm) = 0, = m. Finally, the n × n graph Laplacian matrix LG = L on G is defined as L = D - A.

The graph Laplacian is a symmetric, real valued positive semi-definite matrix, and thus has n nonnegative eigenvalues. Furthermore, if we set 0 = (0, . . . , 0)T to to be the n × 1 vector of all zeroes, and 1 = (1, . . . , 1)T to be the analogous vector of all ones, then it is easy to see that L1 = 0.
Therefore 0 is an eigenvalue of L and we write the n eigenvalues of L as 0 = 0  1  · · ·  n-1 with corresponding n × 1 orthonormal eigenvectors 1/ n = 0, 1, . . . , n-1. If the graph G is
connected, then 1 > 0. In order to simplify the following discussion we assume that this is the
case, although the discussion below can be amended to include disconnected graphs as well.

Since 0 is constant and every other eigenvector is orthogonal to 0, it is natural to view the eigenvectors k as the Fourier modes of the graph G, with a frequency magnitude k. Let x : V  R be a signal defined on the vertices of the graph G, which we will consider as an n × 1 vector with entries x(v ). It follows that the Fourier transform of x can be defined as x(k) = x · k, where x · y is the standard dot product. This analogy is one of the foundations of graph signal process-
ing and indeed we could use this correspondence to define wavelet operators on the graph G, as in
Hammond et al. (2011). Rather than follow this path, though, we instead take a related path similar
to Coifman & Maggioni (2006); Gama et al. (2018) by defining the graph wavelet operators in terms
of random walks defined on G, which will avoid diagonalizing L and will allow us to control the
"spatial" graph support of the filters directly.

Define

the

n×n

transition

matrix

of

a

lazy

random

random

walk

as

P

=

1 2

D-1A + I . Note that

the row sums of P are all one and thus the entry P(v , vm) corresponds to the transition probability

of walking from vertex v to vm in one step. Powers of P run the random walk forward, so that in particular Pt(v , vm) is the transition probability of walking from v to vm in exactly t steps. We

will use P as a left multiplier, in which case P acts a diffusion operator. To understand this idea

more precisely, first note that a simple calculation shows that P1 = 1 and furthermore if the graph

G is connected, every other eigenvalue of P is contained in [0, 1). Note in particular that L and

P share the eigenvector 1. It follows that Ptx responds most significantly to the zero frequency

x(0) of x while depressing the non-zero frequencies of x (where the frequency modes are defined

in terms of the graph Laplacian L, as described above). On the spatial side, the value Ptx(v ) is the

weighted average of x(v ) with all values x(vm) such that vm is within t steps of v in the graph G.

High frequency responses of x can be recovered in multiple different fashions, but we utilize multi-

scale wavelet transforms that group the non-zero frequencies of G into approximately dyadic bands.

As shown in Mallat (2012, Lemma 2.12), wavelet transforms are provably stable operators in the

Euclidean domain, and the proof of Zou & Lerman (2018, Theorem 5.1) indicates that similar results

on graphs may be possible. Furthermore, the multiscale nature of wavelet transforms will allow the

resulting geometric scattering transform (Sec. 3) to traverse the entire graph G in one layer, which

is valuable for obtaining global descriptions of G. Following Coifman & Maggioni (2006), define the n × n diffusion wavelet matrix at the scale 2j as

j = P2j-1 - P2j = P2j-1 (I - P2j-1 )

(1)

Since Pt1 = 1 for every t, we see that j1 = 0 for each j  1. Thus each jx partially recovers x(k) for k  1. The value jx(v ) aggregates the signal information x(vm) from the vertices vm

3

Under review as a conference paper at ICLR 2019

jj

(a) Sample graph of the bunny manifold

(b) Minnesota road network graph

Figure 1: Diffusion wavelets j for increasing scale j left to right, centered at two different locations in two different graphs. Yellow/green vertices correspond to positive values of the wavelet, blue negative values, and purple vertices are approximately zero. The red circles indicate the vertex upon which the wavelet is centered. Both graphs are freely available from PyGSP (2018).

that are within 2j steps of v , but does not average the information like the operator P2j . Instead,
it responds to sharp transitions or oscillations of the signal x within the neighborhood of v with radius 2j (in terms of the graph path distance). Generally, the smaller j the higher the frequencies jx recovers in x. These high frequency wavelet coefficients up to the scale 2J are denoted by:

(J)x = {jx : 1  j  J }

(2)

Since 2J controls the maximum scale of the wavelet, in the experiments of Sec. 4 we select J such that 2J  diam(G). Figure 1 plots the diffusion wavelets at different scales on two different graphs.

3 GEOMETRIC SCATTERING ON GRAPHS

A geometric wavelet scattering transform follows a similar construction as the (Euclidean) wavelet scattering transform of Mallat (2012), but leverages a graph wavelet transform. In this paper we utilize the wavelet transform defined in (2) of the previous section, but remark that in principle any graph wavelet transform could be used (see, for example, Zou & Lerman (2018)). In Section 3.1 we define the graph scattering transform and in Section 3.2 we describe several of its desirable properties as compared to other geometric deep learning algorithms on graphs.

3.1 GEOMETRIC SCATTERING DEFINITIONS

Machine learning algorithms that compare and classify graphs must be invariant to graph isomorphism, i.e., re-indexations of the vertices and corresponding edges. A common way to obtain invariant graph features is via summation operators, which act on a signal x = xG that can be defined on any graph G, e.g., x(v ) = deg(v ) for each vertex v in G. The geometric scattering transform, which is described in the remainder of this section, follows such an approach.

The simplest of such summation operators computes the sum of the responses of the signal x. As
described in Verma & Zhang (2018), this invariant can be complemented by higher order summary
statistics of x, the collection of which is referred to as a "capsule." For example, the un-normalized qth moments of x yield the following "zero" layer geometric scattering capsule:

n
Sx(q) = x(v )q, 1  q  Q
=1

(3)

We can also replace (3) with normalized moments of x, in which case we store its mean (q = 1), variance (q = 2), skew (q = 3), kurtosis (q = 4) and so on. In the numerical experiments described in Sec. 4 we take Q = 2, 3, 4 depending upon the database. Higher order moments are not considered as they become increasingly unstable, and we report results for both normalized and un-normalized moments. In what follows we discuss the un-normalized moments, since their presentation is simpler, but the same principles apply to normalized moments.

From (3) it follows that the invariants Sx(q) do not capture the full variability of x and hence the graph G upon which the signal x is defined. We thus complement these moments with summary

4

Under review as a conference paper at ICLR 2019

...

q q

x

P2j-1

I - P2j-1

|...|

...

q q

Sx

P2j-1

I - P2j-1

|...|

P2j -1

I - P2j -1

|...|

...

q q

j j 1qQ
(a) Representative zeroth-, first-, and second-order cascades of the graph scattering transform.

G = (V, E, W )

x:V R

AdjaceAnc(yvmi,atvrijx):

Signal vector: x(vi )

Diffusion wavelets: j = P2j-1 - P2j

P

=

1 2

(D-1 A

+

I)

j

(a)
Scattering
x  Sx

Fully connected layers:
- Cx(G)

(b) Graph scattering classifier (GSC) architecture yielding class Cx(G) from graph G and signal x.
Figure 2: Illustration of (a) the proposed scattering feature extraction (see eqs. 3, 4, and 5), and (b) its application for graph classification.

statistics derived from the wavelet coefficients of x, which in turn will lead naturally to the graph ConvNet structure of the geometric scattering transform.

Observe, analogously to the Euclidean setting, that in x(v ) over V , we have captured the zero frequency

computing of x since

S

xn=(11)x, (wvh)ich=isxth·e1su=mmantixon(0o)f.

Higher order moments of x can incorporate the full range of frequencies in x, e.g. Sx(2) =

n =1

x(v

)2

=

n k=1

x(k)2,

but they are

mixed into one

invariant

coefficient.

We can separate

and recapture the high frequencies of x by computing its wavelet coefficients (J)x, which were

defined in (2). However, (J)x is not invariant to permutations of the vertex indices; in fact, it is

covariant (or equivariant). Before summing the individual wavelet coefficient vectors jx, though, we must first apply a pointwise nonlinearity. Indeed, define the n × 1 vector d(v ) = deg(v ), and

note that jx · d = 0 since one can show that d is a left eigenvector of P with eigenvalue 1. If G is a regular graph then d = c1 from which it follows that jx · 1 = 0. For more general graphs d(v )  0 for v  V , which implies that for many graphs 1 · d will be the dominating coefficient in

an expansion of 1 in an orthogonal basis containing d; it follows that in these cases |jx · 1| 1.

We thus apply the absolute value non-linearity, to obtain nonlinear covariant coefficients |(J)x| = {|jx| : 1  j  J}. We use absolute value because it is covariant to vertex permutations, nonexpansive, and when combined with traditional wavelet transforms on Euclidean domains, yields
a provably stable scattering transform for q = 1. Furthermore, initial theoretical results in Zou &
Lerman (2018); Gama et al. (2018) indicate that similar graph based scattering transforms possess certain types of stability properties as well. As in (3), we extract an invariant capsule from |jx| by computing its moments, which define the first layer geometric scattering invariants:

n
Sx(j, q) = |jx(v )|q, 1  j  J, 1  q  Q
=1

(4)

First layer geometric scattering coefficients aggregate complimentary multiscale geometric descriptions of G into a collection of invariant multiscale statistics. These invariants give a finer partition of the frequency responses of x. For example, whereas Sx(2) mixed all frequencies of x, we see that Sx(j, 2) only mixes the frequencies of x captured by the graph wavelet j.

First layer geometric scattering invariants can be augmented with second layer geometric scattering invariants by iterating the graph wavelet and absolute value transforms, which leads naturally to the

5

Under review as a conference paper at ICLR 2019

structure of a graph ConvNet. These invariants are defined as:

n
Sx(j, j , q) = |j |jx|(vi)|q, 1  j < j  J, 1  q  Q

(5)

i=1

which consists of reapplying the wavelet transform operator (J) to each |jx| and computing the summary statistics of the magnitudes of the resulting coefficients. The intermediate covariant coefficients |j |jx|| and resulting invariant statistics Sx(j, j , q) couple two scales 2j and 2j within the graph G, thus creating features that bind patterns of smaller subgraphs within G with patterns of larger subgraphs (e.g., circles of friends of individual people with larger community structures in social network graphs). The transform can be iterated additional times, leading to third layer features and beyond, and thus has the general structure of a graph ConvNet.

The collection of graph scattering coefficients Sx = {Sx(q), Sx(j, q), Sx(j, j , q)} (illustrated in Fig. 2(a)) provides a rich set of multiscale invariants of the graph G. They can be used as the input to graph classification or regression models. In Sec. 4, we describe numerical experiments for graph classification problems in which invariant graph scattering coefficients are utilized in conjunction with logistic regression classifiers and fully connected neural network classifiers, the latter of which (illustrated in Fig. 2(b)) leads to near state of the art performance.

3.2 GRAPH SCATTERING COMPARED TO OTHER FEED FORWARD GRAPH CONVNETS
We give a brief comparison of the geometric scattering transform with other graph ConvNets. In particular we seek to isolate the key principles for building accurate graph ConvNet classifiers.
We begin by remarking that like several other successful graph neural networks, the graph scattering transform is covariant or equivariant to vertex permutations until the final features are extracted. This idea has been discussed in depth in various articles, including Kondor et al. (2018b), so we limit the discussion to observing that the geometric scattering transform thus propagates nearly all of the information in x through the multiple wavelet and absolute value layers, since only the absolute value operation removes information on x. As in Verma & Zhang (2018), we aggregate covariant responses via multiple summary statistics, which is referred to as a capsule. In the scattering context, at least, this idea is in fact not new and has been previously used in the Euclidean setting for the regression of quantum mechanical energies in Eickenberg et al. (2018; 2017) and texture synthesis in Bruna & Mallat (2018). We also point out, that unlike many deep learning classifiers (graph included) though, a graph scattering transform extracts invariant statistics at each layer. These intermediate layer statistics, while necessarily losing some information in x (and hence G), provide important coarse geometric invariants that eliminate needless complexity in the subsequent classifier or regression. Furthermore, such layer by layer statistics have proven to be useful in characterizing signals of other types, e.g., texture synthesis in Gatys et al. (2015).
A graph wavelet transform (J)x decomposes the geometry of G through the lens of x, along different scales. Graph ConvNet algorithms also obtain multiscale representations of G, but several works including Atwood & Towsley (2016a); Zhang et al. (2018), propagate information via a random walk. While random walk operators like Pt act at different scales on the graph G, per the analysis in Sec. 2 we see that Pt for any t will be dominated by the low frequency responses of x. While subsequent nonlinearities may be able to recover this high frequency information, the resulting transform will most likely be unstable due to the suppression and then attempted recovery of the high frequency content of x. Alternatively, features derived from Ptx may lose the high frequency responses of x, which are useful in distinguishing similar graphs. The graph wavelet coefficients (J)x, on the other hand, respond most strongly within bands of nearly non-overlapping frequencies, each with a center frequency kj that depends on j.
Finally, graph labels are often complex functions of both local and global subgraph structure within G. While graph ConvNets are adept at learning local structure within G, as detailed in Verma & Zhang (2018) they require many layers to obtain features that aggregate macroscopic patterns in the graph. This is due in large part to the use of fixed size filters, which often only incorporate information from the neighbors of any individual vertex. The training of such networks is difficult due to the limited size of many graph classification databases (see Table 2 in Appendix B). Geometric scattering transforms have two advantages in this regard: (a) the wavelet filters are designed; and (b) they are multiscale, thus incorporating macroscopic graph patterns in every layer.

6

Under review as a conference paper at ICLR 2019

WL PK Graphlet GK DGK WL-OA
S2S-N2N-PP
DGCNN graph2vec
2D CNN PSCN(k=10)
DCNN GCAPS-CNN
GSC (unnorm.) GSC (norm.)
GSC (logistic-norm.)

Biochem. 75.96 73.99 72.56 69.87 75.32 77.96
78.68
74.75 72.46 N/A 75.77 57.72 75.69
76.93 74.42 70.49

Social 69.68 N/A 63.96 64.27 64.84 N/A1
73.58
64.16 N/A 70.77 69.75 N/A1 71.78
71.43 72.42 66.34

Overall 73.17 N/A 65.22 67.38 70.63 79.97
76.13
70.78 N/A N/A 73.09 55.68 73.73
74.49 73.53 68.64

Feed-forward networks

Graph kernel methods

RNN

(a) Average accuracy within biochemistry data, social graphs, and both together (overall). Yellow is the top result in each class, while green indicates the best among feed forward architectures.

+7.24
91.64

+8.49

· CCN · GSC

+4.13
76.27
-4.13

+1.90
80.37
-1.90 +3.36
75.54
-3.36

-7.24
+2.32
77.80
-2.32

85.09
+7.04 -8.49
70.62
-7.04

+7.16
65.68
-7.16

NCI1 NCI109 MUTAG PTC
(b) Accuracy ± standard deviation for CCN and GSC (un-normalized) on biochemistry data.

Figure 3: Classification accuracy (by percent correct) of the proposed method (GSC) and 14 other methods. The aggregated results in (a) are based on five biochemistry datasets and four social graph datasets. CCN is omitted from this table, as its accuracy is only reported for a handful of datasets; instead, a detailed comparison of GSC (unnormalized) with CCN is shown in (b).

4 GRAPH CLASSIFICATION RESULTS
To evaluate the proposed geometric scattering features, we test their effectiveness for graph classification on thirteen datasets commonly used for this task. Out of these, seven datasets contain biochemistry graphs that describe molecular structures of chemical compounds, as described in the following works that introduced them: NCI1 and NCI109, Wale et al. (2008) ; MUTAG, Debnath et al. (1991) ; PTC, Toivonen et al. (2003) ; PROTEINS and ENZYMES, Borgwardt et al. (2005) ; and D&D, Dobson & Doig (2003). In these cases, each graph has several associated vertex features x that represent chemical properties of atoms in the molecule, and the classification is aimed to characterize compound properties (e.g., protein types). The other six datasets, which are introduced in Yanardag & Vishwanathan (2015), contain social network data extracted from scientific collaborations (COLLAB), movie collaborations (IMDB-B & IMDB-M), and Reddit discussion threads (REDDIT-B, REDDIT-5K, REDDIT-12K). In these cases there are no inherent graph signals in the data, and therefore we compute general node characteristics (e.g., degree, eccentricity, and clustering coefficient) over them, as is considered standard practice in relevant literature (see, for example, Verma & Zhang (2018)). A detailed description of each of these datasets appear in their respective references, and are briefly summarized in Appendix B for completeness.
In all cases, we iterate over all graphs in the database and for each one we associate graph-wide features by (1) computing the scattering features of each of the available graph signals (provided or computed), and (2) concatenating the features of all such signals. Then, these features are passed into a fully connected neural network with two or three fully connected layers that are trained to classify the graph represented by these features. We note that the scattering transform described in Sec. 3 is based on the computation of Q moments (i.e., q = 1, . . . , Q) over the entire graph, and these moments can either be normalized or un-normalized, which yields two settings reported separately. Furthermore, to establish the quality of the scattering features independently from the implementation of the classifier, we also report the result of linear classification over them using logistic regression. These are only reported for normalized moments, but comparable linear classification results can also be achieved for un-normalized ones.
We evaluate the classification results of our three geometric scattering classification (GSC) settings using ten-fold cross validation (as explained in Appendix C) and compare them to 14 prominent methods for graph classification. Out of these, six are graph kernel methods, namely: Weisfeiler-
1Accuracy for these methods was reported for less than 3/4 of considered social graph datasets, but with biochemistry data they reach 7/9 of all considered datasets.

7

Under review as a conference paper at ICLR 2019
Lehman graph kernels (WL, Shervashidze et al., 2011), propagation kernel (PK, Neumann et al., 2012), Graphlet kernels (Shervashidze et al., 2009), Random walks (RW, Ga¨rtner et al., 2003), deep graph kernels (DGK, Yanardag & Vishwanathan, 2015), and Weisfeiler-Lehman optimal assignment kernels (WL-OA, Kriege et al., 2016). Seven other methods are recent geometric feed forward deep learning algorithms, namely: deep graph convolutional neural network (DGCNN, Zhang et al., 2018), Graph2vec (Narayanan et al., 2017), 2D convolutional neural networks (2DCNN, Tixier et al., 2017), covariant compositional networks (CCN, Kondor et al., 2018a), Patchy-san (PSCN, Niepert et al., 2016, with k = 10), diffusion convolutional neural networks (DCNN, Atwood & Towsley, 2016b), and graph capsule convolutional neural networks (GCAPS-CNN, Verma & Zhang, 2018). Finally, one method is the recently introduced recurrent neural network autoencoder for graphs (S2SN2N-PP, Taheri et al., 2018). Following the standard format of reported classification performances for these methods (per their respective references, see also Appendix A), our results are reported in the form of average accuracy ± standard deviation (in percentages) over the ten cross-validation folds. We remark here that many of them are not reported for all datasets, and hence, we mark N/A when appropriate. For brevity, the comparison is reported here in Fig. 3 in summarized form, as explained below, and in full in Appendix A.
Since the scattering transform is independent of training labels, it provides universal graph features that might not be specifically optimal in each individual dataset, but overall provide stable classification results. Further, careful examination of the results of previous methods (feed forward algorithms in particular) shows that while some may excel in specific cases, none of them achieves the best results in all reported datasets. Therefore, to compare the overall classification quality of our GSC methods with related methods, we consider average accuracy aggregated over all datasets, and within each field (i.e., biochemistry and social networks) in the following way. First, out of the thirteen datasets, classification results on four datasets (NCI109, ENZYMES, IMDB-M, REDDIT12K) are reported significantly less frequently than the others, and therefore we discard them and use the remaining nine for the aggregation. Next, to address reported values versus N/A ones, we set an inclusion criterion of 75% reported datasets for each method. This translates into at most one N/A in each individual field, and at most two N/A overall. For each method that qualifies for this inclusion criterion, we compute its average accuracy over reported values (ignoring N/A ones) within each field and over all datasets; this results in up to three reported values for each method.
The aggregated results of our GSC and 13 of the compared methods appears in Fig. 3(a), which shows that our GSC approach outperforms all other feed forward methods in terms of universal average accuracy2. The CCN method is omitted from these aggregated results, as its results in Kondor et al. (2018a) are only reported on four biochemistry datasets. For completeness, detailed comparison of GSC (with normalized moments) with this method, which appears in Fig. 3(b), shows that our method outperforms it on two datasets while CCN outperforms GSC on the other two.
5 CONCLUSION
We presented the geometric scattering transform as a deep filter bank for feature extraction on graphs. This transform generalizes the scattering transform, and augments the theoretical foundations of geometric deep learning. Further, our evaluation results on graph classification show the potential of the produced scattering features to serve as universal representations of graphs. Indeed, classification with these features with relatively simple classifier models reaches high accuracy results on most commonly used graph classification datasets, and outperforms both traditional and recent deep learning feed forward methods in terms of average classification accuracy over multiple datasets. We note that this might be partially due to the scarcity of labeled big data in this field, compared to more traditional ones (e.g., image or audio classification). However, this trend also correlates with empirical results for the classic scattering transform, which excels in cases with low data availability. Finally, the geometric scattering features provide a new way for computing and considering global graph representations, independent of specific learning tasks. Therefore, they raise the possibility of embedding entire graphs in Euclidean space (albeit high dimensional) and computing meaningful distances between graphs with them, which can be used for both supervised and unsupervised learning, as well as exploratory analysis of graph-structured data.
2It should be noted, though, that if NCI109 and ENZYMES were included, the GCAPS-CNN would outperform the GSC. However, many other methods would not be comparable then.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Joakim Ande´n and Ste´phane Mallat. Deep scattering spectrum. IEEE Transactions on Signal Processing, 62(16):4114­4128, August 2014.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems 29, pp. 1993­2001, 2016a. URL http://papers.nips. cc/paper/6212-diffusion-convolutional-neural-networks.pdf.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1993­2001, 2016b.
Karsten M Borgwardt, Cheng Soon Ong, Stefan Scho¨nauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl 1): i47­i56, 2005.
Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine, 34(4): 18­42, 2017.
Joan Bruna and Ste´phane Mallat. Audio texture synthesis with scattering moments. arXiv:1311.0407, 2013a.
Joan Bruna and Ste´phane Mallat. Invariant scattering convolution networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1872­1886, August 2013b.
Joan Bruna and Ste´phane Mallat. Multiscale sparse microcanonical models. arXiv:1801.02013, 2018.
X. Chen, X. Cheng, and S. Mallat. Unsupervised deep Haar scattering on graphs. In Conference on Neural Information Processing Systems (NIPS), Montreal, Quebec, Canada, 2014.
Ronald R. Coifman and Ste´phane Lafon. Diffusion maps. Applied and Computational Harmonic Analysis, 21:5­30, 2006.
Ronald R. Coifman and Mauro Maggioni. Diffusion wavelets. Applied and Computational Harmonic Analysis, 21(1):53­94, 2006.
Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2):786­797, 1991.
Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of molecular biology, 330(4):771­783, 2003.
Michael Eickenberg, Georgios Exarchakis, Matthew Hirn, and Ste´phane Mallat. Solid harmonic wavelet scattering: Predicting quantum molecular energy from invariant descriptors of 3D electronic densities. In Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 6540­6549, 2017.
Michael Eickenberg, Georgios Exarchakis, Matthew Hirn, Ste´phane Mallat, and Louis Thiry. Solid harmonic wavelet scattering for predictions of molecule properties. Journal of Chemical Physics, 148:241732, 2018.
Fernando Gama, Alejandro Ribeiro, and Joan Bruna. Diffusion scattering transforms on graphs. arXiv:1806.08829, 2018.
Thomas Ga¨rtner, Peter Flach, and Stefan Wrobel. On graph kernels: Hardness results and efficient alternatives. In Learning theory and kernel machines, pp. 129­143. Springer, 2003.
Leon Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. In Advances in Neural Information Processing Systems 28, pp. 262­270, 2015.
9

Under review as a conference paper at ICLR 2019
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, 2017.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org.
David K. Hammond, Pierre Vandergheynst, and Re´mi Gribonval. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis, 30:129­150, 2011.
Matthew Hirn, Ste´phane Mallat, and Nicolas Poilvert. Wavelet scattering regression of quantum chemical energies. Multiscale Modeling and Simulation, 15(2):827­863, 2017. arXiv:1605.04654.
Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant compositional networks for learning graphs. arXiv preprint, pp. arXiv:1801.02144, 2018a.
Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant compositional networks for learning graphs. arXiv:1801.02144, 2018b.
Nils M. Kriege, Pierre-Louis Giscard, and Richard Wilson. On valid optimal assignment kernels and applications to graph classification. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 1623­1631. Curran Associates, Inc., 2016.
Vincent Lostanlen and Ste´phane Mallat. Wavelet scattering on the pitch spiral. In Proceedings of the 18th International Conference on Digital Audio Effects, pp. 429­432, 2015.
Ste´phane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):1331­1398, October 2012.
Yves Meyer. Wavelets and Operators, volume 1. Cambridge University Press, 1993.
Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint, pp. arXiv:1707.05005, 2017.
Marion Neumann, Novi Patricia, Roman Garnett, and Kristian Kersting. Efficient graph kernels by randomization. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 378­393. Springer, 2012.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pp. 2014­2023, 2016.
Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1717­1724, 2014.
Edouard Oyallon and Ste´phane Mallat. Deep roto-translation scattering for object classification. In Proceedings in IEEE CVPR 2015 conference, 2015. arXiv:1412.8659.
PyGSP. Graph signal processing in python (https://pygsp.readthedocs.io/en/ stable/index.html), Accessed in September 2018.
Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt. Efficient graphlet kernels for large graph comparison. In David van Dyk and Max Welling (eds.), Proceedings of the 12th International Conference on Artificial Intelligence and Statistics, volume 5 of Proceedings of Machine Learning Research, pp. 488­495, Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, 2009. PMLR.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-Lehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539­ 2561, 2011.
10

Under review as a conference paper at ICLR 2019
D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83­98, 2013.
Laurent Sifre and Ste´phane Mallat. Rigid-motion scattering for texture classification. arXiv:1403.1687, 2014.
Aynaz Taheri, Kevin Gimpel, and Tanya Berger-Wolf. Learning graph representations with recurrent neural network autoencoders. In KDD Deep Learning Day, 2018.
Antoine Jean-Pierre Tixier, Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis. Classifying graphs as images with convolutional neural networks. arXiv preprint, pp. arXiv:1708.02218, 2017.
Hannu Toivonen, Ashwin Srinivasan, Ross D King, Stefan Kramer, and Christoph Helma. Statistical evaluation of the predictive toxicology challenge 2000­2001. Bioinformatics, 19(10):1183­1193, 2003.
Saurabh Verma and Zhi-Li Zhang. Graph capsule convolutional neural networks. arXiv preprint, pp. arXiv:1805.08090, 2018.
Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. Knowledge and Information Systems, 14(3):347­375, 2008.
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365­1374. ACM, 2015.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems 27, pp. 3320­3328, 2014.
Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph classification. In AAAI Conference on Artificial Intelligence, pp. 4438­ 4445, 2018.
Dongmian Zou and Gilad Lerman. Graph convolutional neural networks via scattering. arXiv:1804:00099, 2018.
11

Under review as a conference paper at ICLR 2019

APPENDIX A FULL COMPARISON TABLE

Table 1: Comparison of the proposed graph scattering classifier (GSC) with graph kernel methods and deep learning methods on biochemistry & social graph datasets.

Graph kernel methods RNN

Feed-forward networks

All results come from the respective papers that introduced the methods, with the exception of: (1) social network results of WL, from Tixier et al. (2017); (2) biochemistry and social results of DCNN, from Verma & Zhang (2018); (3) biochemistry, except for D&D, and social result of GK,
3DCNN using different training/test split
12

WL PK Graphlet WL-OA GK DGK

NCI1
84.46 ± 0.45 82.54 ± 0.47
70.5 ± 0.2 86.1 ± 0.2 62.28 ± 0.29 80.3 ± 0.4

NCI109
85.12 ± 0.29
N/A 69.3 ± 0.2 86.3 ± 0.2 62.60 ± 0.19 80.3 ± 0.3

D&D
78.34 ± 0.62 78.25 ± 0.51 79.7 ± 0.7 79.2 ± 0.4 78.45 ± 0.26 73.09 ± 0.25

PROTEINS
72.92 ± 0.56 73.68 ± 0.68
72.7 ± 0.6 76.4 ± 0.4 71.67 ± 0.55 75.7 ± 0.50

MUTAG
84.11 ± 1.91 76.00 ± 2.69
85.2 ± 0.9 84.5 ± 1.7 81.39 ± 1.74 87.4 ± 2.7

PTC
59.97 ± 1.60 59.50 ± 2.44
54.7 ± 2.0 63.6 ± 1.5 57.26 ± 1.41 60.1 ± 2.5

ENZYMES
55.22 ± 1.26
N/A 30.6 ± 1.2 59.9 ± 1.1 26.61 ± 0.99 53.4 ± 0.9

S2S-P2P-NN
DGCNN graph2vec
2D CNN CCN
PSCN (k = 10) DCNN
GCAPS-CNN
GSC (norm.) GSC (unnorm.) GSC (logistic-norm.)

83.72 ± 0.4
74.44 ± 0.47 73.22 ± 1.81
N/A 76.27 ± 4.13 76.34 ± 1.68
56.61 ± 1.04 82.72 ± 2.38
78.86 ± 2.22 80.37 ± 1.90 69.76 ± 2.65

83.64 ± 0.3
N/A 74.26 ± 1.47
N/A 75.54 ± 3.36
N/A 57.47 ± 1.22 81.12 ± 1.28
76.79 ± 2.10 77.80 ± 2.32 68.50 ± 2.45

N/A 79.37 ± 0.94
N/A N/A N/A 76.27 ± 2.15 58.09 ± 0.53 77.62 ± 4.99
75.72 ± 4.00 78.86 ± 3.72 75.30 ± 2.60

76.61 ± 0.5
75.54 ± 0.94 73.30 ± 2.05 77.12 ± 2.79
N/A 75.00 ± 2.51
61.29 ± 1.60 76.40 ± 4.17
71.60 ± 4.09 74.67 ± 2.90 72.42 ± 3.23

89.86 ± 1.1
85.83 ± 1.66 83.15 ± 9.25
N/A 91.64 ± 7.24 88.95 ± 4.37 56.60 ± 2.89
N/A 81.90 ± 5.42 85.09 ± 8.49 72.72 ± 11.73

64.54 ± 1.1
58.59 ± 2.47 60.17 ± 6.86
N/A 70.62 ± 7.04 62.29 ± 5.68
563 66.01 ± 5.91
64.01 ± 6.85 65.68 ± 7.16 62.23 ± 6.65

63.96 ± 0.6 51.00 ± 7.29
N/A N/A N/A N/A 42.44 ± 1.76 61.83 ± 5.39 50.33 ± 7.67 53.33 ± 4.94 38.67 ± 7.77

Graph kernel RNN

WL PK Graphlet WL-OA GK DGK

COLLAB
77.82 ± 1.45
N/A 73.42 ± 2.43 80.7 ± 0.1 72.84 ± 0.28 73.0 ± 0.2

IMDB-B 71.60 ± 5.16
N/A 65.4 ± 5.95
N/A 65.87 ± 0.98
66.9 ± 0.5

IMDB-M
N/A N/A N/A N/A 43.89 ± 0.38 44.5 ± 0.5

REDDIT-B
78.52 ± 2.01
N/A 77.26 ± 2.34 89.3 ± 0.3 77.34 ± 0.18 78.0 ± 0.3

REDDIT-5K 50.77 ± 2.02
N/A 39.75 ± 1.36
N/A 41.01 ± 0.17
41.2 ± 0.1

REDDIT-12K 34.57 ± 1.32
N/A 25.98 ± 1.29
N/A N/A 32.2 ± 0.1

S2S-P2P-NN
DGCNN graph2vec
2D CNN CCN
PSCN (k = 10) DCNN
GCAPS-CNN
GSC (norm.) GSC (unnorm.) GSC (logistic-norm.)

81.75 ± 0.8
73.76 ± 0.49
N/A 71.33 ± 1.96
N/A 72.60 ± 2.15 52.11 ± 0.71 77.71 ± 2.51
79.26 ± 1.50 76.50 ± 1.20 72.64 ± 2.27

73.8 ± 0.7
70.03 ± 0.86
N/A 70.40 ± 3.85
N/A 71.00 ± 2.29 49.06 ± 1.37 71.69 ± 3.40
71.30 ± 4.10 71.30 ± 2.87 63.70 ± 3.69

51.19 ± 0.5
47.83 ± 0.85
N/A N/A N/A 45.23 ± 2.84 33.49 ± 1.42 48.50 ± 4.1
47.33 ± 2.35 47.73 ± 4.42 41.53 ± 3.50

86.50 ± 0.8
N/A N/A 89.12 ± 1.7 N/A 86.30 ± 1.58 N/A 87.61 ± 2.51
87.70 ± 2.87 86.30 ± 2.56 80.60 ± 2.22

52.28 ± 0.5
48.70 ± 4.54
N/A 52.21 ± 2.44
N/A 49.10 ± 0.7
N/A 50.10 ± 1.72
51.43 ± 2.43 51.63 ± 2.08 48.41 ± 3.41

42.47 ± 0.1
N/A N/A 48.13 ± 1.47 N/A 41.32 ± 0.42 N/A N/A 42.34 ± 1.18 38.39 ± 1.19 N/A

Feed-forward networks

Under review as a conference paper at ICLR 2019
from Yanardag & Vishwanathan (2015); (4) D&D of GK is from Niepert et al. (2016); and (5) for Graphlets, biochemistry results from Kriege et al. (2016), social results from Tixier et al. (2017).
APPENDIX B DETAILED DATASET DESCRIPTIONS
The details of the datasets used in this work are as follows (see the main text in Sec. 3 for references):
NCI1 contains 4,110 chemical compounds as graphs, with 37 node features. Each compound is labeled according to is activity against non-small cell lung cancer and ovarian cancer cell lines, and these labels serve as classification goal on this data.
NCI109 is similar to NCI1, but with 4,127 chemical compounds and 38 node features. MUTAG consists of 188 mutagenic aromatic and heteroaromatic nitro compounds (as graphs) with
7 node features. The classification here is binary (i.e., two classes), based on whether or not a compound has a mutagenic effect on bacterium. PTC is a dataset of 344 chemical compounds (as graphs) with nineteen node features that are divided into two classes depending on whether they are carcinogenic in rats. PROTEINS dataset contains 1,113 proteins (as graphs) with three node features, where the goal of the classification is to predict whether the protein is enzyme or not. D&D dataset contains 1,178 protein structures (as graphs) that, similar to the previous one, are classified as enzymes or non-enzymes. ENZYMES is a dataset of 600 protein structures (as graphs) with three node features. These proteins are divided into six classes of enzymes for classification. COLLAB is a scientific collaboration dataset contains 5K graphs. The classification goal here is to predict whether the graph belongs to a subfield of Physics. IMDB-B is a movie collaboration dataset with contains 1K graphs. The graphs are generated on two genres: Action and Romance, the classification goal is to predict the correct genre for each graph. IMDB-M is similar to IMDB-B, but with 1.5K graphs & 3 genres: Comedy, Romance, and Sci-Fi. REDDIT-B is a dataset with 2K graphs, where each graph corresponds to an online discussion thread. The classification goal is to predict whether the graph belongs to a Q&A-based community or discussion-based community. REDDIT-5K consists of 5K threads (as graphs) from five different subreddits. The classification goal is to predict the corresponding subreddit for each thread. REDDIT-12K is similar to REDDIT-5k, but with 11,929 graphs from 12 different subreddits.
Table 2 summarizes the size of available graph data (i.e., number of graphs, and both max & mean number of vertices within graphs) in these datasets, as previously reported in the literature.
Graph signals for social network data: None of the social network datasets has ready-to-use node features. Therefore, in the case of COLLAB, IMDB-B, and IMDB-M, we use the eccentricity, degree, and clustering coefficients for each vertex as characteristic graph signals. In the case of REDDIT-B, REDDIT-5K and REDDIT-12K, on the other hand, we only use degree and clustering coefficient, due to presence of disconnected graphs in these datasets.
APPENDIX C TECHNICAL DETAILS
The computation of the scattering features described in Section 3 is based on several design choices, akin to typical architecture choices in neural networks. Most importantly, it requires a choice of 1. which statistical moments to use (normalized or unnormalized), 2. the number of wavelet scales to use (given by J), and 3. the number of moments to use (denoted by Q). The configuration used for each dataset in this work is summarized in Table 3, together with specific settings used in the downstream classification layers, as descibed below.
13

Under review as a conference paper at ICLR 2019

# of graphs in data: Max # of vertices: Mean # of vertices: # of classes:

NCI1
4110 111 29.8
2

NCI109 D&D PROTEINS MUTAG PTC

4127 111 29.6
2

188 28 17.93 2

1178 5748 284.32
2

344 109 25.56
2

1113 620 39.0
2

ENZYMES
600 126 32.6
6

COLLAB
5000 492 74.49
3

IMDB BM

1000 136 19.77
2

1500 89 13 3

REDDIT B 5K

2000 3783 429.61
2

5000 3783 508.5
5

12K
11929 3782 391.4 11

Table 2: Basic statistics of the graph classification databases
Once the scattering coefficients are generated through the above processes, they are fed into two or three fully connected layers (see Table 3 for specifics) and then a softmax layer is used to compute the class probabilities. Cross entropy loss is minimized during the training process and ReLU is used as the activation function between fully connected layers. Besides, we use mini batch training with batch size 64 and ADAM optimization technique for training. Finally, L2 norm regularization is used to avoid overfittings.
Cross validation procedure: Classification evaluation was done with standard ten-fold cross validation procedure. First, the entire dataset is randomly split into ten subsets. Then, in each iteration (or "fold"), nine of them are used as training and validation, and the other one is used for testing classification accuracy. In total, after ten iterations, each of the subsets has been used once for testing, resulting in ten reported classification accuracy numbers for the examined dataset. Finally, the mean and standard deviation of these ten accuracies are computed and reported.
It should be noted that when using fully connected layers, each iteration also performs automatic tuning of the trained classifier, as follows. First, nine iterations are performed, each time using eight subsets (i.e., folds) as training and the remaining one as validation set, which is used to determine the optimal epoch for network training. Then, the classifier is retrained with all nine subsets. After nine iterations, each of the training/validation subsets has been used once for validation, and we obtain nine classification models, which in turn produce nine predictions (i.e., class probabilities) for each data point in the test subset of the main cross validation. To obtain the final result of this cross validation iteration, we sum up all these predictions and select the class with the highest probability as our final classification result. These results are then compared to the true labels (in the test set) on the test subset to obtain classification accuracy for this fold.
Software & hardware environment: Geometric scattering and related classification code were implemented in Python with TensorFlow. All experiments were performed on HPC environment using an intel16-k80 cluster, with a job requesting one node with four processors and two Nvidia Tesla k80 GPUs.

14

Under review as a conference paper at ICLR 2019

Scattering

Fully connected

Database

Moment J Q # Hidden units 1 # Hidden units 2 # Hidden units 3

NCI1

un-normalized 5 3 normalized 4 3

NCI109

un-normalized 5 4 normalized 5 3

D&D

un-normalized 5 2 normalized 5 4

PROTEINS

un-normalized 5 3 normalized 4 3

MUTAG

un-normalized 4 4 normalized 5 4

PTC un-normalized 5 4 normalized 5 3

ENZYMES un-normalized 4 4 normalized 5 4

COLLAB

un-normalized 4 3 normalized 5 4

IMDB-B

un-normalized 4 4 normalized 4 3

IMDB-M

un-normalized 4 4 normalized 5 3

REDDIT-B

un-normalized 5 3 normalized 5 4

REDDIT-5K un-normalized 5 3 normalized 4 4

REDDIT-12K un-normalized 5 4 normalized 5 4

40 60 60 60 20 20 20 20 40 60 50 50 60 60 60 50 50 50 50 60 60 60 50 60 100 100

20 30 30 30 0 0 10 10 20 0 20 25 30 30 30 20 20 20 20 30 30 30 20 30 50 50

0 0 15 15 0 0 0 5 0 0 0 0 15 15 0 0 10 10 0 0 15 15 10 15 25 25

Table 3: Settings of the geometric scattering classifier

15

