Under review as a conference paper at ICLR 2019
TWO-TIMESCALE NETWORKS FOR NONLINEAR VALUE FUNCTION APPROXIMATION
Anonymous authors Paper under double-blind review
ABSTRACT
A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control. Many of the algorithms for learning values, however, are designed for linear function approximation--with a fixed basis or fixed representation. Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning. In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with the representation learned at a slower timescale. The approach facilitates use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms. We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation. We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.
1 INTRODUCTION
Value function approximation--estimating the expected returns from states for a policy--is heavily reliant on the quality of the representation of state. One strategy has been to design a basis--such as radial basis functions (Sutton and Barto, 1998) or a Fourier basis (Konidaris et al., 2011)--for use with a linear function approximator and temporal difference (TD) learning (Sutton, 1988). For low-dimensional observation vectors, this approach has been effective, but can be onerous to extend to high-dimensional observations, potentially requiring significant domain expertise. Another strategy has been to learn the representation, such as with basis adaptation or neural networks. Though there is still the need to specify the parametric form, learning these representations alleviates the burden of expert specification. Further, it is more feasible to scale to high-dimensional observations, such as images, with neural networks (Mnih et al., 2015; Silver et al., 2016). Learning representations necessitates algorithms for nonlinear function approximation.
Despite the deficiencies in specification for fixed bases, linear function approximation for estimating value functions has several benefits over nonlinear estimators. They enable least-squares methods, which can be much more data-efficient for policy evaluation (Bradtke and Barto, 1996; Szepesvari, 2010; van Seijen and Sutton, 2015), as well as robust to meta-parameters (Pan et al., 2017). Linear algorithms can also make use of eligibility traces, which can significantly speed learning (Sutton, 1988; Dann et al., 2014; White and White, 2016), but have not been able to be extended to nonlinear value function approximation. Additionally, there have been a variety of algorithms derived for the linear setting, both for on-policy and off-policy learning (Sutton et al., 2009; Maei, 2011; van Seijen and Sutton, 2014; van Hasselt et al., 2014; Mahadevan et al., 2014; Sutton et al., 2016; Mahmood et al., 2017). These linear methods have also been well-explored theoretically (Tsitsiklis and Van Roy, 1997; Maei, 2011; Mahmood and Sutton, 2015; Yu, 2015) and empirically (Dann et al., 2014; White and White, 2016), with some insights into improvements from gradient methods (Sutton et al., 2009), true-online traces (van Seijen and Sutton, 2014) and emphatic weightings (Sutton et al., 2016). These algorithms are easy to implement, with relatively simple objectives. Objectives for nonlinear value function approximation, on the other hand, can be quite complex (Maei et al., 2009), resulting in more complex algorithms (Menache et al., 2005; Di Castro and Mannor, 2010; Bhatnagar et al., 2013).
1

Under review as a conference paper at ICLR 2019

In this work, we pursue a simple strategy to take advantage of the benefits of linear methods, while still learning the representation. The main idea is to run two learning processes in parallel: the first learns nonlinear features using a surrogate loss and the second estimates the value function as a linear function of those features. We show that these Two-timescale Networks (TTNs) converge, because the features change on a sufficiently slow scale, so that they are effectively fixed for the fast linear value function estimator.
Similar ideas have previously been explored for basis adaptation, but without this key aspect of TTNs--namely the separation of the loss for the representation and value function. This separation is critical because it enables simpler objectives--for which the gradient can be easily sampled--to drive the representation, but still enables use of the mean squared projected Bellman error (MSPBE)--on which all the above linear algorithms are based. This separation avoids the complexity of the nonlinear MSPBE, but maintains the useful properties of the (linear) MSPBE. A variety of basis adaptation approaches have used a two-timescale approach, but with the same objective for the representation and the values (Menache et al., 2005; Di Castro and Mannor, 2010; Bhatnagar et al., 2013; J et al., 2016). Yu and Bertsekas (2009) provided algorithms for basis adaptation using other losses, such as Bellman error using Monte carlo samples, taking derivatives through fixed point solutions for the value function. Levine et al. (2017) periodically compute a closed form least-squares solution for the last layer of neural network, with a Bayesian update to prevent too much change. Because these methods did not separate the value learn and basis adaptation, the resulting algorithms are more complex. The strategy of using two different heads--one to drive the representation and one to learn the values--has yet to be systematically explored.
We show that TTNs are a promising direction for nonlinear function approximation, allowing us to leverage linear algorithms while retaining the flexibility of nonlinear function approximators. We first discuss a variety of possible surrogate losses, and their potential for learning a useful representation. We then show that TTNs converge, despite the fact that a linear algorithm is used with a changing representation. This proof is similar to previous convergence proofs for policy evaluation, but with a relaxation on the requirement that features be independent, which is unlikely for learned features. We then show empirically that TTNs are effective compared to other nonlinear value function approximations and that they can exploit several benefits of linear value approximations algorithms. In particular, for both low-dimensional and high-dimensional (image-based) observations, we show (a) the utility of least-squares (or batch) methods, (b) advantages from eligibility traces and (c) gains from being able to select amongst different linear policy evaluation algorithms. We demonstrate that TTNs can be effective for control with neural networks, enabling use of fitted Q-iteration within TTNs as an alternative to target networks.

2 BACKGROUND

We assume the agents act in a finite Markov Decision Process (MDP), with notation from (White, 2017). The dynamics of the MDP are defined by the 3-tuple (S, A, P ), where S is the set of states, A the set of actions and P : S × A × S  [0, 1] the transition probability function. The task in this environment is defined by a reward function R : S × A × S  R and a discount function  : S × A × S  [0, 1]. At each time step, the agent takes an action At according to a policy  : S × A  [0, 1] and the environment returns reward Rt+1, next state St+1 and discount t+1.

The goal in policy evaluation is to compute the value function: the expected sum of discounted
rewards from every state under a fixed policy . The value function V : S  R is defined recursively from each state s  S as

V(s) d=ef E[Rt+1 + t+1V(St+1)|St = s] = (s, a) P (s, a, s )(r + V(s )). (1)

aA

s S

When using linear function approximation, this goal translates into finding parameters w  Rd to approximate the value function

V^ (s) d=ef x(s) w  V(s)

where x : S  Rd is a feature function.

More generally, a nonlinear function V^ (s) could be learned to estimate V.

(2)

To formulate this learning problem, we need to consider the objective for learning the function V^ . Let V, V^  R|S| be the vectors of values for V, V^ . The recursive formula (1) defines a Bellman

2

Under review as a conference paper at ICLR 2019

operator B where the fixed point satisfies BV = V. Consider a restricted value function class, such as the set of linear value functions V^  F = {Xw | w  Rd} where X  R|S|×d is a matrix
with the i-th row set to x(s) for ith state s  S. Then, it may no longer be possible to satisfy
the recursion. Instead, an alternative is to find a projected fixed point F BV^ = V^ where the projection operator F projects BV^ to the space spanned by this linear basis:

F V d=ef arg min

V¯ - V

2 d

V¯ F

where d  R|S| is a vector which weights each state in the weighted norm

V

2 d

=

sS d(s)V (s)2.

Many linear policy evaluation algorithms estimate this projected fixed point, including TD (Sutton,

1988), least-squares TD (Bradtke and Barto, 1996) and gradient TD (Sutton et al., 2009).

The objective formulated for this projected fixed-point, however, is more complex for nonlinear
function approximation. For linear function approximation, the projection operator simplifies into a closed form solution involving only the features X. Letting t = Rt+1 + V^ (St+1) - V^ (St), the resulting mean-squared projected Bellman error (MSPBE) can be written as

MSPBE(w) d=ef

F BV^ - V^

2 d

=

E[txt] E[xtxt

]-1 E[txt]

(3)

where E[txt] = sS d(s) E[t|St = s]x(s). For nonlinear function classes, the projection does not have a closed form solution and may be expensive to compute. Further, the projection involves the value function parameters, so the projection changes as parameters change. The nonlinear MSPBE and resulting algorithm are more complex (Maei et al., 2009), and have not seen widespread use.

Another option is simply to consider different objectives. However, as we discuss below, other objectives for learning the value function either are similarly difficult to optimize or provide poor value estimates. In the next section, we discuss some of these alternatives and introduce Two-timescale Networks as a different strategy to enable nonlinear value function approximation.

3 TWO-TIMESCALE NETWORKS AND SURROGATE OBJECTIVES

We first introduce Two-timescale Networks (TTNs), and then describe different surrogate objectives that can be used in TTNs. We discuss why these surrogate objectives within TTNs are useful to drive the representation, but are not good replacements for the MSPBE for learning the value function.

TTNs use two concurrent optimization processes: one for the parameters of the network  and one
for the parameters of the value function w. The value function is approximated as V^ (s) d=ef x(s) w where the features x : S  Rd are a parametrized function and   Rm is adjusted to provide better features. For a neural network,  consists of all the parameters in the hidden layers, to produce the final hidden layer x(s). The two optimization processes maintain different time scales, with the parameters  for the representation changed as a slow process, and the parameters w for the value estimate changed as a fast process relative to .

The separation between these two processes could be problematic, since the target problem-- estimating the value function--is not influencing the representation! The slow process is driven by a completely separate objective than the fast process. However, the key is to select this surrogate loss for the slow process so that it is related to the value estimation process, but still straightforward to compute the gradient of the loss. We use V^ (s) as the output of the fast part, which corresponds to the value estimate used by the agent. To distinguish, Y^ (s) denotes the output for the slow-part (depicted in Figure 1), which may or may not be an estimate of the value, as we discuss below.

Consider first the mean-squared TD error (MSTDE), which corresponds to

d(s) E[t2|St = s].
sS

(4)

Notice that this does not correspond to the mean-squared Bellman error (MSBE), for which it is more difficult to compute gradients

BV^ - V^

2 d

=

d(s) (E[t|St = s])2 .

sS

(5)

3

Under review as a conference paper at ICLR 2019

Using the MSTDE as a surrogate loss, with Y^ (s) = x(s) w¯ , the slow part of the network minimizes

Lslow() = min d(s) E[t(, w¯ )2|St = s]
w¯ RdsS

t(, w¯ )d=efRt+1 +t+1x(St+1) w¯ -x(St) w¯ .

This slow part has its own weights w¯ associated with estimating the value function, but learned instead

w Y^ (s)

according to the MSTDE. The

advantage here is that stochastic

gradient descent on the MSTDE

is straightforward, with gradient t{,w¯ }[t+1Y^ (St+1) - Y^ (St)] where {,w¯ }Y^ (St) is the gradient of the neural network, including the
head of the slow part which uses

s

w V^ (s)

Hidden layers

x (s) 

Outputs

weights w¯ . Using the MSTDE has

been found to provide worse value estimates than the MSPBE--which

Figure 1: Two-Timescale Network architecture

we re-affirm in our experiments. It could, nonetheless, play a useful role as a surrogate loss, where it

can inform the representation towards estimating values.

There are a variety of other surrogate losses that could be considered, related to the value function. However, many of these losses are problematic to sample incrementally, without storing large amounts of data. For example, the mean-squared return error (MSRE) could be used, which takes samples of return and minimizes mean-squared error to those sampled returns. Obtaining such returns requires waiting many steps, and so delays updating the representation for the current state. Another alternative is the MSBE. The gradient of the nonlinear MSBE is not as complex as the gradient of the nonlinear MSPBE, because it does not involve the gradient of a projection. However, like the MSPBE, it suffers from the same issue that the gradient involves the product of expectations:

{,w¯ }

BY^ -Y^

2 d

=

d(s)E[t|St = s]{,w¯ } E[t+1x(St+1) w¯ -x(St) w¯ |St = s]. (6)

sS

Sampling the gradient requires two independent samples. For these reasons, we explore the MSTDE as the simplest surrogate loss involving the value function.

Finally, surrogate losses could also be defined that are not directly related to the value function. Two

natural choices are losses based on predicting the next state and reward. The output of the slow part

could correspond to a vector of values, such as Yt = St+1  Rn, Yt =

St+1 Rt+1

.

Lslow()

=

min
W¯ Rd×n

sS

d(s)

E

Yt - Y^ (St)

2 2

St = s

(7)

The ability to predict the next state and reward is intuitively useful for enabling prediction of value, that also has some theoretical grounding. Szepesvari (2010, Section 3.2.1) shows that the Bellman error is small, if the features can capture a horizon of immediate rewards and expected next states. For linear encoders, Song et al. (2016) show that an optimal set of features enables predictions of next state and reward. This argument does not extend to nonlinear encoders, but nonetheless provides some motivation for the utility of predicting next state and reward as a surrogate loss.

4 CONVERGENCE OF TWO-TIMESCALE NETWORK ALGORITHM
Training TTNs is fully online, using a single transition from the environment at a time. Stochastic gradient descent is used to reduce the surrogate loss, Lslow() and a linear policy evaluation algorithm, such as GTD or LSTD, is coupled to the network to obtain the prediction vector by reducing the MSPBE(w) relative to . The full procedure is summarized in Algorithm 1, in Appendix A.
To prove that TTNs converge, we simply need to ensure two key properties. First, the network needs to evolve sufficiently slowly relative to the linear prediction weights. In our theoretical results, this

4

Under review as a conference paper at ICLR 2019

is achieved by ensuring that the step sizes t and t of the network and the linear policy evaluation algorithm respectively decay to zero at different rates. In particular, t/t  0 as t  . With this relative disparity in magnitudes, one can assume that the network is essentially quasi-static, while the faster linear component is equilibrated relative to the static features. Second, the linear prediction algorithms need to converge for any set of features provided by the neural network, particularly linearly dependent features. The analysis for the convergence of the neural network is general, enabling any network architectures that are twice continuously differentiable. We prove that the TTNs converge asymptotically to the stable equilibria of an ODE which completely captures the dynamics of the algorithm. We provide the precise result in Appendix B.

5 EXPERIMENTS

We investigate the performance of TTNs versus a variety of other nonlinear policy evaluation algorithms, as well as the impact of choices within TTNs. We particularly aim to answer (a) is it beneficial to optimize the MSPBE to obtain value estimates, rather than using value estimates from surrogate losses like the MSTDE; (b) do TTNs provide gains over other nonlinear policy evaluation algorithms; and (c) can TTNs benefit from the variety of options in linear algorithms, including leastsquares approaches, eligibility traces and different policy evaluation algorithms. More speculatively, we also investigate if TTNs can provide a competitive alternative to deep Q-learning in control.
Experiments were performed on-policy in five environments. We use three classic continuous-state domains: Puddle World, a continuous-state grid world with high-magnitude negative rewards for walking through a puddle; Acrobot, where a robot has to swing itself up; and Cartpole, which involves balancing a pole. We also use two game domains: Catcher, which involves catching falling apples; and Puck World, in which the agent has to chase a puck (Tasfi, 2016). Catcher includes both a variant with 4-dimensional observations--position and velocity of the paddle, and (x,y) of the apple-- and one with image-based observations--with two consecutive 64-by-64 grayscale images as input. This domain enables us to analyze the benefit of the algorithms, on the same domain, both with low-dimensional and high-dimensional observations. We describe the policies evaluated for these domains in Appendix D. We include a subset of results in the main body, with additional results in the appendix. Results in Cartpole are similar to Acrobot; Cartpole results are only in the appendix.
The value estimates are evaluated using root-mean-squared value error (RMSVE), where value error is (V(s) - V^ (s))2. The optimal values for a set of 500 states are obtained using extensive rollouts from each state and the RMSVE is computed across these 500 states. For the algorithms, we use the following settings, unless specified otherwise. For the slow part (features), we minimize the mean-squared TD error (MSTDE) using the AMSGrad optimizer (Reddi et al., 2018) with 1 = 0 and 2 = 0.99. The network weights use Xavier initialization (Glorot and Bengio, 2010); the weights for the fast part were initialized to 0. In Puddle World, the neural network consists of a single hidden layer of 128 units with ReLU activations. In the other environments, we use 256 units instead. To choose hyperparameters, we first did a preliminary sweep on a broad range and then chose a smaller range where the algorithms usually made progress, summarized in Appendix D. Results are reported for hyperparameters in the refined range, chosen based on RMSVE over the latter half of a run.

Puck World Nonlinear TD- Reg

Root Mean Square Error

Nonlinear-GTD

ABTD

Nonlinear-TD

ABBE

TTN

Number of Steps

Figure 2: TTN comparison to other nonlinear value function approximation algorithms. For the TTN, MSTDE is used as the surrogate loss for the slow part of the network (feature learning) and LSTD is used for the fast part.
TTN vs. competitors. We compare to the following algorithms: nonlinear TD, nonlinear GTD (Maei et al., 2009), Adaptive Bases (ABBE and ABTD) (Di Castro and Mannor, 2010), nonlinear TD + LSTD regularization (inspired by Levine et al. (2017)). We describe these algorithms in more depth in Appendix D. All of the algorithms involve more complex updates compared to TTNs,

5

Under review as a conference paper at ICLR 2019

except for nonlinear TD, which corresponds to a semi-gradient TD update with nonlinear function approximation. For TTNs, we use LSTD for the linear, fast part.

In Figure 2, TTN is able to perform as well or better than the competitor algorithms. Especially in Puddle World, its error is significantly lower than the second best algorithm. Interestingly, Nonlinear GTD also performs well across domains, suggesting an advantage for theoretically-sound algorithms.

The utility of optimizing the MSPBE. First, we show that the TTN benefits from having a second

head learning at a faster timescale. To do so, we compare the prediction errors of both heads, the fast

one optimizing the MSPBE (using TD) and the slow one optimizing the MSTDE. As a baseline, we

include TTN with a fixed representation (a randomly initialized neural network) to highlight that the

slow process is indeed improving the representation.

Catcher

In Figure 3, we see that optimizing the MSPBE indeed gives better results than optimizing the

MSTDE-fixed

MSTDE. Even with a random representation, MSPBE can actually learn and performs much better than the MSTDE. Additionally, we can conclude that using the MSTDE, despite be-

Root Mean Square Error

MSPBE-fixed

MSPBE

ing a poor objective to learn the value function,

can still be effective for driving feature-learning

MSTDE

since it outperforms the fixed representation.

Linear algorithms and eligibility traces.

Number of Steps

TTNs give us the flexibility to choose any linear policy evaluation algorithm for the fast part. We Figure 3: Comparison of fast and slow predictions.

compare several choices: TD, least-squares TD (LSTD) (Bradtke and Barto, 1996), forgetful LSTD

(FLSTD) (van Seijen and Sutton, 2015), emphatic TD (Sutton et al., 2016), gradient TD (the TDC

variant) (Sutton et al., 2009) and their true-online versions (van Seijen and Sutton, 2014; van Hasselt

et al., 2014) to learn the value function. GTD and ETD are newer temporal difference methods

which have better convergence properties and can offer increased stability. The true-online variants

modify the update rules to improve the behavior of the algorithms when learning online and seem

to outperform their counterparts empirically (van Seijen and Sutton, 2014). Least-squares methods

summarize past interaction, but are often avoided due to quadratic computation in the number of

features. For TTNs, however, there is no computational disadvantage to using LSTD methods, for two

reasons. It is common to choose deep but skinny architectures (Mnih et al., 2015; Hessel et al., 2017).

Furthermore, if the last layer is fully connected, then we already need to store O(d2) weights and use

O(d2) time to compute a forward pass--the same as LSTD. We include FLSTD, which progressively

forgets older interaction, as this could be advantageous when the feature representation changes over

time. For TTN, incremental versions of the least-squares algorithms are used to maintain estimates of

the required quantities online.

All of these linear algorithms can use eligibility traces to increase their sample efficiency by propa-

gating TD errors back in time. The trace parameter  can also provide a bias-variance tradeoff for the

value estimates (Sutton, 1988; Dann et al., 2014). For nonlinear function approximation, eligibility

traces can no longer be derived for TD. Though invalid, we can naively extend them to this case by

keeping one trace per weight, giving us nonlinear TD().

Puddle world

Catcher

Catcher

Root Mean Square Error

TDC TD TOETD

FLSTD

LSTD

Number of Steps
(a)

Root Mean Square Error

TDC
TD TOETD

LSTD

FLSTD Number of Steps
(a)

Root Mean Square Error

ETD TOTD

TOETD
LSTD TDC

TD
Step size (10^x)
(b)

Figure 4: a) & b) Linear methods on Puddle World and Catcher. c) Step size sensitivity in Catcher.
The results overall indicate that TTNs can benefit from the ability to use different linear policy evaluation algorithms and traces, in particular from the use of least-squares methods as shown in

6

Under review as a conference paper at ICLR 2019

Figure 4 for Puddle World and Catcher. The dominance of LSTD versus the other linear algorithms is consistent, including in terms of parameter sensitivity, persists for the other three domains. We additionally investigated sensitivity to , and found that most of the TTN variants benefit from a nonzero  value and, in many cases, the best setting is high, near 1. One exception is the least-squares methods, where LSTD performs similarly for most values of . Nonlinear TD(), on the other hand, performs markedly worse as  increases. This is unsurprising considering the naive addition of eligibility traces is unsound. We include these sensitivity plots in the appendix, in Figure 8.

Surrogate loss functions. For all the previous experiments, we optimized the MSTDE for the slow

part of the network, but as discussed in Section 3, other objectives can be used. We compare a variety

of objectives, by choosing different Yt in (7), including Yt = Rt+1 (Reward); Yt = St+1 (Next State); and Yt = Rt+1 + Y^ (St+1). (Semi-gradient MSTDE). In Puck World, in Figure 5 a), we
can see that every auxiliary loss performed well. This does not appear to be universally true, as in

Acrobot we found that the MSTDE was a less effective surrogate loss, leading to slower learning

(see Figure 5 b). Alternate losses such as the semi-gradient MSTDE and next state predictions

were more successful in that domain. From Figure 5 c), we can a potential reason why: directly

using the MSTDE in that domain resulted in poor performance. Interestingly, despite the poor

performance from MSTDE, TTN with MSTDE as a surrogate loss was still able to learn quite

effectively--albeit more slowly--despite the fact that directly using the MSTDE in that domain

resulted in poor performance. Figure 5 c) generally shows that separating out value learning improves

upon directly using the surrogate objective. Using nonlinear TD to drive representation learning

within TTNs (TTN with semi-MSTDE) provided some improvement over nonlinear TD, even though

nonlinear TD already performed well in this domain. These results, therefore, do suggest some level

of robustness to the surrogate loss.

Puck World

Acrobot

Acrobot

Root Mean Square Error

MSTDE Next State semi-MSTDE Reward

Root Mean Square Error

Number of Steps
(a)

semi-MSTDE

MSTDE Reward

Root Mean Square Error

MSTDE semi-MSTDE
TTN with semi-MSTDE

Next State
Number of Steps
(b)

TTN with MSTDE
Lambda
(c)

Figure 5: a) & b) Comparison of surrogate losses on Puck World and Acrobot c) Comparison of learning with TTN and directly on losses
Control Although the focus of this work is policy evaluation, we also provide some preliminary results for the control setting. For control, we include some standard additions to competitor learning algorithms to enable learning with neural networks. The DQN algorithm (Mnih et al., 2015) utilizes two main tricks to stabilize training: experience replay--storing past transitions and replaying them multiple times--and a target network--which keeps the value function in the Q-learning targets fixed, updating the target network infrequently (e.g., every k = 10, 000 steps).
We use an alternative strategy to target networks for TTN. The use of a target network is motivated by fitted Q-iteration (FQI) (Ernst et al., 2005), which updates towards fixed Q-values with one sweep through a batch of data. TTNs provide a straightforward mechanism to instead directly use FQI, where we can solve for the weights on the entire replay buffer, taking advantage of the closed form solution for linear regression towards the Q-values from the last update. Batch FQI requires storing all data, whereas we instead have a sliding window of experience. We therefore additionally incorporate a regularization term, which prevents the weights from changing too significantly between updates, similarly to Levine et al. (2017). Each FQI iteration requires solving a least squares problem on the entire buffer, an operation costing O(nd2) computation where d is the number of features in the last layer of the network and n is the size of the buffer; we update the network every k steps, which reduces the per-step computation to O(nd2/k). As before, the slow part drives feature-learning by minimizing the MSTDE for state-action values.
The experimental details differ for control. On nonimage Catcher, we do a sweep over slow and reg, the regularization parameter, for TTN and sweep over the learning rate and the number of steps over which is annealed for DQN. On image Catcher, runs require significantly more computation so

7

Under review as a conference paper at ICLR 2019

Non-Image Catcher

Average Return
(100 Episodes)

TTN-FQI

DQN

Average Return (1000 Episodes)

Image Catcher

TTN-FQI

DQN

Episode
(a)

Episode
(b)

Figure 6: a) Comparison of returns obtained by each algorithm on a) non-image Catcher and b) image Catcher.

we only tune hyperparameters by hand. The FQI update in TTNs was done every 100 (3000) steps for non-image (image) Catcher. We run each algorithm 10 times (5 times) for 200 thousand steps (10 million steps) on the non-image (image) Catcher.
We see that TTN is able to perform well on both versions of Catcher, in Figure 6, particularly learning more quickly than DQN. For TTN, the average return starts increasing right from the beginning of training, whereas DQN has an initial period where the average return is very low. Further, we found TTN with FQI to be much less sensitive to the frequency of updating, particularly allowing for more frequent updates. This suggests it is a promising direction for improving sample efficiency in control, whilst still maintaining stability when training neural networks. This result highlights another setting where it is beneficial to have the flexibility to select a value learning algorithm separately from the algorithm which drives representation learning in the neural network.

6 DISCUSSION AND CONCLUSION
In this work, we proposed Two-timescale Networks as a new strategy for policy evaluation with nonlinear function approximation. As opposed to many other algorithms derived for nonlinear value function approximation, TTNs are intentionally designed to be simple to promote ease-of-use. The algorithm combines a slow learning process for adapting features and a fast process for learning a linear value function, both of which are straightforward to train. By leveraging these two timescales, we are able to prove convergence guarantees for a broad class of choices for both the fast and slow learning components. We highlighted several cases where the decoupled architecture in TTNs can improve learning, particularly enabling the use of linear methods--which facilitates use of least-squares methods and eligibility traces.
This work has only begun the investigation into which combinations for surrogate losses and linear value function approximation algorithms are most effective. We provided some evidence that, when using stochastic approximation algorithms rather than least-squares algorithms, the addition of traces can have a significant effect within TTNs. This contrasts nonlinear TD, where traces were not effective. The ability to use traces is potentially one of the most exciting outcomes for TTNs, since traces have been so effective for linear methods. More generally, TTNs provide the opportunity to investigate the utility of the many linear value function algorithms, in more complex domains with learned representations. For example, emphatic algorithms have been found to have improved asymptotic properties (Sutton et al., 2016), but to the best of our knowledge, have not been used with neural networks.
Another promising direction for TTNs is for off-policy learning, where many value functions are learned in parallel. Off-policy learning can suffer from variance due to large magnitude corrections (importance sampling ratios). With a large collection of value functions, it is more likely that some of them will cause large updates, potentially destabilizing learning in the network if trained in an end-to-end fashion. TTNs would not suffer from this problem, because a different objective can be used to drive learning in the network. We provide some preliminary experiments in the appendix suggesting that this might indeed be the case (Appendix C.6).
8

Under review as a conference paper at ICLR 2019
REFERENCES
M. Benaïm, J. Hofbauer, and S. Sorin. Stochastic approximations and differential inclusions. SIAM Journal on Control and Optimization, 44(1):328­348, 2005.
S. Bhatnagar, V. S. Borkar, and P. K. J. Feature Search in the Grassmanian in Online Reinforcement Learning. J. Sel. Topics Signal Processing, 2013.
V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):291­294, 1997.
V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University Press, 2008.
S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning, 1996.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
H.-F. Chen. Stochastic approximation and its applications, volume 64. Springer Science & Business Media, 2006.
C. Dann, G. Neumann, and J. Peters. Policy evaluation with temporal differences: a survey and comparison. The Journal of Machine Learning Research, 2014.
D. Di Castro and S. Mannor. Adaptive Bases for Reinforcement Learning. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, 2010.
D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6(Apr):503­556, 2005.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, 2010.
M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. G. Azar, and D. Silver. Rainbow - Combining Improvements in Deep Reinforcement Learning. arXiv, 2017.
P. K. J, S. Bhatnagar, and V. S. Borkar. Actor-Critic Algorithms with Online Feature Adaptation. ACM Trans. Model. Comput. Simul., 2016.
G. Konidaris, S. Osentoski, and P. S. Thomas. Value function approximation in reinforcement learning using the Fourier basis. In International Conference on Machine Learning, 2011.
H. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications, volume 35. Springer Science & Business Media, 2003.
H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Unconstrained Systems. Springer Science & Business Media, 2012.
D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017.
N. Levine, T. Zahavy, D. J. Mankowitz, A. Tamar, and S. Mannor. Shallow Updates for Deep Reinforcement Learning. In International Conference on Learning Systems, 2017.
Y. Liang, M. C. Machado, E. Talvitie, and M. H. Bowling. State of the Art Control of Atari Games Using Shallow Reinforcement Learning. In International Conference on Autonomous Agents and Multiagent Systems, 2016.
L. Ljung. Analysis of recursive stochastic algorithms. IEEE transactions on automatic control, 22(4):551­575, 1977.
H. Maei. Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta, 2011.
H. Maei, C. Szepesvári, S. Bhatnagar, D. Precup, D. Silver, and R. S. Sutton. Convergent temporal-difference learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems, 2009.
S. Mahadevan, B. Liu, P. S. Thomas, W. Dabney, S. Giguere, N. Jacek, I. Gemp, and J. Liu. Proximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces. CoRR abs/1405.6757, 2014.
A. R. Mahmood and R. Sutton. Off-policy learning based on weighted importance sampling with linear computational complexity. In Conference on Uncertainty in Artificial Intelligence, 2015.
9

Under review as a conference paper at ICLR 2019
A. R. Mahmood, H. Yu, and R. S. Sutton. Multi-step Off-policy Learning Without Importance Sampling Ratios. arXiv:1509.01240v2, 2017.
I. Menache, S. Mannor, and N. Shimkin. Basis function adaptation in temporal difference reinforcement learning. Annals of Operations Research, 2005.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015.
J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pages 2863­2871, 2015.
Y. Pan, E. S. Azer, and M. White. Effective sketching methods for value function approximation. In Conference on Uncertainty in Artificial Intelligence, Amsterdam, Netherlands, 2017.
A. Ramaswamy and S. Bhatnagar. Stochastic recursive inclusion in two timescales with an application to the lagrangian dual problem. Stochastics, 88(8):1173­1187, 2016.
S. J. Reddi, S. Kale, and S. Kumar. On the Convergence of Adam and Beyond. In International Conference on Learning Systems, 2018.
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 2016.
Z. Song, R. E. Parr, X. Liao, and L. Carin. Linear feature encoding for reinforcement learning. In Advances in Neural Information Processing Systems, pages 4224­4232, 2016.
R. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press, 1998.
R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 1988.
R. S. Sutton, H. Maei, D. Precup, and S. Bhatnagar. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In International Conference on Machine Learning, 2009.
R. S. Sutton, A. R. Mahmood, and M. White. An emphatic approach to the problem of off-policy temporaldifference learning. The Journal of Machine Learning Research, 2016.
C. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool Publishers, 2010.
N. Tasfi. Pygame Learning Environment. 2016.
J. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 1997.
H. van Hasselt, A. R. Mahmood, and R. Sutton. Off-policy TD () with a true online equivalence. In Conference on Uncertainty in Artificial Intelligence, 2014.
H. van Seijen and R. Sutton. A deeper look at planning as learning from replay. In International Conference on Machine Learning, 2015.
H. van Seijen and R. S. Sutton. True online TD(lambda). In International Conference on Machine Learning, 2014.
A. M. White and M. White. Investigating practical, linear temporal difference learning. In International Conference on Autonomous Agents and Multiagent Systems, 2016.
M. White. Unifying task specification in reinforcement learning. In International Conference on Machine Learning, pages 3742­3750, 2017.
H. Yu. On convergence of emphatic temporal-difference learning. In Annual Conference on Learning Theory, 2015.
H. Yu and D. P. Bertsekas. Basis function adaptation methods for cost approximation in MDP. In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, 2009.
10

Under review as a conference paper at ICLR 2019

A TTN ALGORITHM

Algorithm 1 Training of TTNs

1: procedure TRAIN(w, , w¯ , )

 is a fixed policy

2: Initialize , w¯ with Xavier initialization, w to 0 and thestarting state s according to the

environment

3: while training do

4: a  action chosen by (s)

5: r, s  Environment(s, a)

Get reward and next state

6: , w¯  GradientDescent on Lslow using sample (s, r, s ) 7: w  Update on Lvalue using sample (s, r, s ) 8: s  s

9: end while

10: return learned parameters w, , w¯

11: end procedure

B CONVERGENCE PROOF OF TWO-TIMESCALE NETWORKS
B.1 DEFINITIONS & NOTATIONS
- Let R+ denote the set of non-negative real numbers, N = {0, 1, 2, . . . } and · denote the Euclidean norm or any equivalent norm.
- A map f : Rd  Rd is Lipschitz if f (x) - f (y)  L( x - y ), for some L  (0, ), x, y  Rd.
- A set-valued map h : Rd  {subsets of Rd} is called a Marchaud map, if it satisfies the following conditions:
1. For each x  Rd, h(x) is convex and compact. 2. For each x  Rd, K  (0, ) such that supyh(x) y  K(1 + x ). 3. h is upper-semicontinuous, i.e., if {xn}nN  x and {yn}nN  y, where xn  Rd,
yn  h(xn), n  N, then y  h(x).
- For x1, x2  Rd and D  Rk×k a diagonal matrix, we define the inner-product < x1, x2 >D
1
x1 Dx2. We also define the semi-norm x D < x, x >D2 . If all the diagonal elements of D are strictly positive, then · D is a norm.
- For any set X, let X° denote the interior of X and X denote the boundary of X.

B.2 ASSUMPTIONS

Assumption 1: The pre-determined, deterministic, step-size sequence {t}tN satisfies

t > 0, t  N,

t = ,

t2 < .

tN tN

Assumption 2: The Markov chain induced by the given policy  is ergodic, i.e., aperiodic and irreducible.
Assumption 2 implies that the underlying Markov chain is asymptotically stationary and henceforth it guarantees the existence of a unique steady-state distribution d over the state space S (Levin and Peres, 2017), i.e., limt P(St = s) = d(s), s  S.

Assumption 3: Given a realization of the transition dynamics of the MDP in the form of a
sample trajectory O = {S0, A0, R1, S1, A1, R2, S2, . . . }, where the initial state S0  S is chosen arbitrarily, while the action A At  (St, ·), the transitioned state S St+1  P (St, At, ·) and the reward R Rt+1 = R(St, At, St+1).

11

Under review as a conference paper at ICLR 2019

For brevity, let ¯ = (, w¯ ) and ¯ be the feature matrix corresponding to the feature parameter ¯, i.e.

 x(s1) 

 x(s2) 

¯

 



...

  

,

x(s|S|) |S|×d

(8)

where x(s) is the row-vector corresponding to state s. Further, define the |S| × |S|-matrix P  as follows:

Ps,s (s, a)P (s, a, s ), s, s  S.
aA

(9)

Also, let J () = LST DE() E E t2+1|St .

Definition 1. A function  : U  Rd1  X  Rd2 is Frechet differentiable at x  U if there exists a bounded linear operator x : Rd1  Rd2 such that the limit

(x + y) - x lim
0

(10)

exists and is equal to x(y). We say  is Frechet differentiable if Frechet derivative of  exists at every point in its domain.

Slower timescale

NN

Faster timescale

TD GTD2 TDC

ETD

LSTD

LSPE

Figure 7: TTN model
To analyze the long-run behaviour of our algorithm, we employ the ODE based analysis (Borkar, 2008; Kushner and Yin, 2003; Ljung, 1977) of the stochastic recursive algorithms. Here, we consider a deterministic ordinary differential equation (ODE) whose asymptotic flow is equivalent to the long-run behaviour of the stochastic recursion. Then we analyze the qualitative behaviour of the solutions of the ODE to determine the asymptotically stable sets. The ODE-based analysis is elegant and conclusive and it further guarantees that the limit points of the stochastic recursion will almost surely belong to the compact connected internally chain transitive invariant set of the equivalent ODE. Since the algorithm follows a multi-timescale stochastic approximation framework, we will also resort to the more generalized multi-timescale differential inclusion based analysis proposed in (Borkar, 1997; Ramaswamy and Bhatnagar, 2016).

Note that there exists only a unilateral coupling between the neural network and the various policy evaluation algorithms (See Figure 7), with the policy evaluation algorithms depending on the feature vectors ¯t (which are being calibrated by the neural network) but not vice-versa. Therefore, one can independently analyze the asymptotic behaviour of the neural network. The following lemma characterizes the limiting behaviour of the neural network.
Define the filtration {Ft}tN, a family of increasing natural -fields, where Ft  {¯i, Ri; 0  i  t} .

12

Under review as a conference paper at ICLR 2019

Lemma 1. Let   Rm+d be a compact, convex subset with smooth boundary. Let  be Frechet

differentiable.

Further,

let

¯ (-

1 2

J )(¯)

be

Lipschitz

continuous.

Let

K

be

the

set

of

asymptotically

stable equilibria of the following ODE contained inside :

d dt

¯(t)

=

¯(t)(-

1 2

¯J

)(¯(t)),

¯(0)  ° and t  R+.

Then the stochastic sequence {¯t}tN generated by the TTN converges almost surely to K.

Proof. We employ here the ODE based analysis as proposed in (Borkar, 2008; Kushner and Clark, 2012). Firstly, we recall here the stochastic recursion which updates ¯t:

¯t+1 =  ¯t + tt+1 ¯t Y^¯(St) - t+1¯t Y^¯(St+1) ,

(11)

where  is the projection onto a pre-determined compact and convex subset   Rm+d, i.e., (x) = x, for x  °, while for x / °, it is the nearest point in  w.r.t. the Euclidean distance (or
equivalent metric). Here, t+1 Rt+1 + t+1Y^¯t (St+1) - Y^¯t (St) is the temporal difference. Also, ¯t Y^¯  R(m+d)×|S| is the Jacobian of Y^¯ at ¯ = ¯t and ¯t Y^¯(s) is the column corresponding to state s.

Now the above equation can be rewritten as

¯t+1 = 

¯t + t

h1(¯t) + Mt1+1 +

1 t

,

(12)

where h1(¯)

E t+1 ¯Y^¯(St) - t+1¯Y^¯(St+1) , the noise term M1t+1

t+1 ¯t Y^¯(St) - t+1¯t Y^¯(St+1) - E t+1 ¯t Y^¯(St) - t+1¯t Y^¯(St+1) |Ft and the

bias

1 t

E t+1 ¯t Y^¯(St) - t+1¯t Y^¯(St+1) |Ft - h1(¯t).

Further,

¯t+1

=

¯t

+

 t

¯t + t

h1(¯t) + M1t+1 + t

1 t

- ¯t

= ¯t + t t (h1(¯t)) + ¯t M1t+1 + ¯t

1 t

+ o(t)

,

(13)

where  is the Frechet derivative (defined in Eq. (10). Note that  is single-valued since  is convex and also the above limit exists since the boundary  is assumed smooth. Further, for x  °,
we have

x(y)

=

lim
0



(x

+

y) - x

x+

= lim

0

y-x = y (for sufficiently small

),

(14)

i.e., x (·) is an identity map for x  °.

A few observations are in order:

C1: ¯ (h1(¯)) is a Lipschitz continuous function in ¯. This follows from the hypothesis of the Lemma.
C2: ¯t M1t+1 is a truncated martingale difference noise. Indeed, it is easy to verify that the noise sequence {M1t+1}tN is a martingale-difference noise sequence w.r.t to the filtration {Ft+1}tN, i.e., Mt1+1 is Ft+1-measurable and integrable, t  N and E M1t+1|Ft = 0 a.s., t  N. Also, since (·) is a continuous linear operator, we have (Mt1+1) to be Ft+1-measurable and integrable, t  N likewise.

13

Under review as a conference paper at ICLR 2019

C3: ¯t

1 t

 0 as t   a.s. Indeed,

¯t

1 t

=

 lim

¯t +

0

1 t

- ¯t

 ¯t +  lim
0

1 t

- 

¯t

¯t +  lim
0

1 t

-

¯t

=

1 t

.

By taking t  , C3 follows directly from the ergodicity (Levin and Peres, 2017) (Assumption

2) and finiteness of the underlying Markov chain.

C4: o(t)  0 as t   (follows from Assumption 1). C5: Iterates {¯t}tN are stable (forcefully), i.e. bounded almost surely, since t  , t  N
(ensured by the projection operator ) and  is compact (i.e., closed and bounded).

C6: K0  (0, ), such that

E ¯t M1t+1 2|Ft  K0 1 + ¯t 2 a.s.

(15)

This follows directly from the finiteness of the Markov chain and from the assumption that the boundary  is smooth.

Now, by appealing to Theorem 2, Chapter 2 of (Borkar, 2008)), we conclude that the stochastic recursion (11) asymptotically tracks the following ODE

d dt

¯(t)

=

¯(t)(h1(¯(t)),

¯(0)  ° and t  R+

=

¯(t)(-

1 2

¯J )(¯(t)),

¯(0)  ° and t  R+.

(16)

In other words, the stochastic recursion (11) converges to the asymptotically stable equilibria of the ODE (16) contained inside .

Remark 1. It is indeed non-trivial to determine the constraint set  without prior adequate
knowledge about the limit set of the ODE (16). A pragmatic approach to overcome this concern is to
initiate the stochastic recursion with an arbitrary convex, compact set  with a smooth boundary and gradually spread to the whole of Rm+d (Chen, 2006).

Remark 2. It is also important to characterize the hypothesis of the above lemma (i.e.,

¯ (-

1 2

J

)(¯)

is

Lipschitz

continuous)

with

respect

to

the

features

Y^¯.

To achieve that one

has to consider the non-projected form of the ODE (16). Apparently, when one considers the

spreading approach proposed in the above remark, then it is essentially encouraged to consider the

non-projected form since the limiting flow of the ODE arising from the projected stochastic recursion

is more likely to lie inside the compact, convex set as  becomes larger. Thereupon, it is easy to

observe that the condition Y^¯ is twice continuously-differentiable is sufficient to ensure the Lipschitz

continuity

of

¯ (-

1 2

J

)(¯).

Additionally,

in

that

case

K

=

{¯|¯J (¯)

=

0}

which

is

the

set

of

local extrema of J.

B.3 TD() ALGORITHM
One can directly apply the TD() with linear function approximation algorithm to estimate the value function with respect to the features provided by the neural network. The TD() algorithm is provided in Algorithm 2.

Here et, wt  Rd. Further, t+1 difference.

Rt+1 + t+1wt xt (St+1) - wt xt (St) is the temporal

14

Under review as a conference paper at ICLR 2019

Algorithm 2 TD() algorithm
Parameters: t > 0,   [0, 1]; Initialization: w0 = 0, e0 = 0;
For each transition (St, Rt+1, St+1) in O, do: et+1 = xt (St) + t+1et; wt+1 = wt + t Rt+1 + t+1wt xt (St+1) - wt xt (St) et;

(17) (18)

Assumption 4-TD(): The pre-determined, deterministic, step-size sequence {t}tN satisfies:

t > 0, t  N,

t = ,
tN

t2 < ,
tN

lim t = 0. t t

Note that the step-size schedules {t}tN and {t}tN satisfy

t t

 0, which implies that {t}

converges to 0 relatively faster than {t}. This disparity in terms of the learning rates induces an

asynchronous convergence behaviour asymptotically (Borkar, 1997), with feature parameter sequence

{¯t} converging slower relative to the TD() sequence {wt}. The rationale being that the increment

term of the underlying stochastic gradient descent of the neural network is smaller compared to that of

the TD() recursion (18), since the neural network SGD is weighted by the step-size schedule {t}tN which is smaller than {t}tN for all but finitely many t. This unique pseudo heterogeneity induces
multiple perspectives, i.e., when viewed from the faster timescale recursion (recursion controlled

by t), the slower timescale recursion (recursion controlled by t) seems quasi-static (`almost a constant'), while viewed from the slower timescale, the faster timescale recursion seems equilibrated.

Further, it is analytically admissible (Borkar, 1997) to consider the slow timescale stochastic recursion (i.e., the neural network SGD) to be quasi-stationary (i.e., ¯t  ¯, t  N), while analyzing the asymptotic behaviour of the relatively faster timescale stochastic recursion (18). Thereupon, we

obtain the following directly from Theorem 1 of (Tsitsiklis and Van Roy, 1997).

Lemma 2. Assume ¯t  ¯, t  N. Let Assumptions 1-3 and 4-TD() hold. Then for any   [0, 1], the stochastic sequence {wt}tN generated by the TD() algorithm (Algorithm 2) within the TTN setting converges almost surely to the unique limit w, where w satisfies

T ()(¯w) = ¯w,

(19)

with T ()J (s)

(1 - )

 i=0

iE

(with 0 = 1).

i j=0

 [j ] Rj +1

+

[i+1]J (Si+1)

S0

=

s

and [j] = ij=0i

For other single-timescale prediction methods like ETD and LSPE, similar results follow. Regarding the least squares method LSTD, which offers the significant advantage of non-dependency on stepsizes (albeit computationally expensive) couples smoothly within the TTN setting without any additional consideration.

B.4 GTD2 ALGORITHM
However, one cannot directly apply the original GTD2 and TDC algorithms to the TTN setting, since a necessary condition required for the convergence of these algorithms is the non-singularity of the fea-
ture specific matrices E xt (St)xt (St) and E (xt (St) - t+1xt (St+1)) xt (St) . Please refer Theorem 1 and Theorem 2 of (Sutton et al., 2009). Without the non-singularity assumption, it is indeed hard to guarantee the almost sure boundedness of the GTD2/TDC iterates. In the TTN setting that we consider in this paper, one cannot explicitly assure this condition, since the features are apparently administered by a neural network and it is not directly intuitive on how to control the neural network to generate a collection of features with the desired non-singularity characteristic. Henceforth, one has to consider the projected versions of these algorithms. We consider here the projected GTD2 algorithm provided in Algorithm 3.

15

Under review as a conference paper at ICLR 2019

Algorithm 3 GTD2 algorithm

Parameters: t, t; Initialization: u0  U, w0  W ;

For each transition (St, Rt+1, St+1) in O do: wt+1 = W wt + t tu+t 1xt (St) - wt xt (St) xt (St) ut+1 = U ut + t (xt (St) - t+1xt (St+1)) wt xt (St)

; ;

(20) (21)

Here ut, wt  Rd. Further, tu+1 difference.

Rt+1 + t+1u xt (St+1) - u xt (St) is the temporal

Here, W is the projection operator onto a pre-determined convex, compact subset W  Rd with a smooth boundary W . Therefore, W maps vectors in Rd to the nearest vectors in W w.r.t. the
Euclidean distance (or equivalent metric). Convexity and compactness ensure that the projection is unique and belongs to W . Similarly, U is a pre-determined convex, compact subset of Rd with a
smooth boundary U .

Projection is required since the stability of the iterates {wt}tN and {ut}tN are hard to guarantee otherwise.

Assumption 4-GTD2: The pre-determined, deterministic, step-size sequences {t}tN and {t}tN satisfy

t, t > 0, t  N,

t = t = ,

tN tN

t2 + t2 < ,
tN

lim t = 0, t t

lim t = 0. t t

Define the filtration {Ft}tN, a family of increasing natural -fields, where Ft  {wi, ui, ¯i, Ri; 0  i  t} .

Similar to the TD() case, here also we follow the quasi-stationary argument. Henceforth, we analyze the asymptotic behaviour of GTD2 algorithm under the assumption that feature vector ¯t is quasi-static, i.e. ¯t  ¯ = (, w¯ ) . Lemma 3. Assume ¯t  ¯ = (, w¯ ) , t  N. Let Assumptions 1-3 and 4-GTD2 hold. Then

(u, w)

lim inf
t

(u, w)

- (ut, wt)

 (u, w) w  Au ,
uA

where A is the set of asymptotically stable equilibria of the following ODE:

d dt

u(t)

=

uU(t)

¯ Dd (I - t+1P )¯u(t)

,

u(0)  U°, t  R+

and Au is the asymptotically stable equilibria of the following ODE:

d dt

w(t)

=

wW(t)

¯ Dd u - ¯ Dd ¯ w(t) , w(0)  W° and t  R+,

with u defined in Eq. (30).

(22) (23)

Proof. The two equations in the modified GTD2 algorithm constitute a multi-timescale stochastic

approximation recursion, where there exists a bilateral coupling between the stochastic recursions

(20) and (21).

Since the step-size sequences {t}tN

and {t}tN

satisfy

t t



0, we have t



0

faster than t  0. This disparity in terms of the learning rates induces a pseudo heterogeneous

rate of convergence (or timescales) between the individual stochastic recursions which results in a

16

Under review as a conference paper at ICLR 2019

pseudo asynchronous convergence behaviour when considered over a finite time window. Also note that the coherent long-run behaviour of the multi-timescale stochastic recursion will asymptotically follow this short-term behaviour with the window size extending to infinity(Borkar, 1997; 2008). This pseudo behaviour induces multiple viewpoints, i.e., when observed from the faster timescale recursion (recursion controlled by t), the slower timescale recursion (recursion controlled by t) appears quasi-static (`almost a constant'), while observed from the slower timescale, the faster timescale recursion seems equilibrated. Further, it is analytically admissible (Borkar, 1997) to consider the slow timescale stochastic recursion (21) to be quasi-stationary (i.e., ut  u, t  N), while analyzing the limiting behaviour of the relatively faster timescale stochastic recursion 20.

Analysis of the faster time-scale recursion: The faster time-scale stochastic recursion of the GTD2 algorithm is the following:

wt+1 = W wt + t tu+t 1xt (St) - wt xt (St) xt (St) .

(24)

Under the previously mentioned quasi-stationary premise that ut  u and ¯t  ¯ = (, w¯) , t  N, thereupon, we analyze the long-term behaviour of the following recursion:

wt+1 = W wt + t tu+1xt - wt xt xt ,

(25)

where xt = x(St) and tu+1 Rt+1 + t+1u xt+1 - u xt.

The above equation can be rearranged as the following:

wt+1 = W

wt + t

h2(wt) + M2t+1 +

2 t

,

where the noise Mt2+1

tu+1xt - wt xt xt - E tu+1xt - wt xt xt|Ft ,

h2(w)

E tu+1xt - wt xt xt

and the bias

2 t

=

E tu+1xt - wt xt xt|Ft

-

E tu+1xt - wt xt xt .

Similar to Equation (13), we can rewrite the above recursion as follows:

wt = wt + t Wwt (h2(wt)) + Wwt M2t+1 + Wwt

2 t

+ o(t)

,

(26)

where Wwt (·) is the Frechet derivative (defined in Equation (10)) of the projection operator W . A few observations are in order:

D1: The iterates {wt}tN are stable, i.e., suptN wt <  a.s. This immediately follows since W is bounded.
D2: {wWt Mt2+1 }tN is a martingale-difference noise sequence with respect to the filtration {Ft+1}tN. This follows directly since {M2t+1}tN is a martingale-difference noise sequence with respect to the same filtration.

D3: {Wwt Mt2+1 }tN are square-integrable and K2  (0, ) such that

E wWt M2t+1 2|Ft  K2 1 + wt 2 a.s., t  N.

(27)

This follows directly from the finiteness of the underlying Markov chain and from the assumption that the boundary W is smooth.

D4: wW (h2(w)) is Lipschitz continuous with respect to w. Proof similar to C1.

D5: wWt

2 t

 0 as t   a.s. Proof similar to C3.

Now by appealing to Theorem 2, Chapter 2 of (Borkar, 2008) along with the above observations, we conclude that the stochastic recursion 24 asymptotically tracks the following ODE almost surely:

d dt

w(t)

=

Ww(t)(h2(w(t)),

w(0)  W° and t  R+.

= wW(t) E tu+1xt - E xtxt w(t) , w(0)  W° and t  R+.

(28)

17

Under review as a conference paper at ICLR 2019

Therefore, wt converges asymptotically to the stable equilibria of the above ODE contained inside W almost surely.

Qualitative analysis of the solutions of ODE (28): A trivial qualitative analysis of the long-run
behaviour of the flow induced by the above ODE attests that the stable limit set is indeed the solutions of the following linear system inside W (This follows since wW (y) = y for w  W° and also because wW (·) does not contribute any additional limit points on the boundary other than the roots of h2 since W is smooth).

E xtxt E tu+1xt - E xtxt E xtxt w = 0.  E xtxt w = E tu+1xt .

(29)

Note that E xtxt = ¯ Dd ¯.
Claim 1: The above linear system of equations is consistent, i.e., E tu+1xt  R(¯ Dd ¯), i.e., the range-space of ¯ Dd ¯: To see that, note that the above system can indeed be viewed as the least squares solution to the ¯w = u with respect to the weighted-norm · Dd , where

u(s) = R¯(s) + t+1

Ps,s u x(s ) -

Ps,s u x(s ),

s S

s S

(30)

where R¯ is the expected reward. (Note that E tu+1xt = ¯ Dd u).

The least-squares solution w0  Rd (which certainly exists but may not be unique) satisfies

< ¯w, u - ¯w0 >Dd = 0, w  Rd  < w, Dd-1¯ Dd (u - ¯w0) >Dd = 0, w  Rd.

Now choose w = Dd-1¯ Dd (u - ¯w0). Then

¯ Dd (u - ¯w0) = 0  ¯ Dd ¯w0 = ¯ Dd u. [ End of proof of Claim 1 ]

Since ¯ Dd ¯ may be singular (i.e., ¯ Dd ¯ is not invertible), the above least squares solution may not be unique and hence the collection of asymptotically stable equilibria of the flow induced by the ODE (28) may not be singleton for every u. Let's denote the asymptotically stable equilibria of the flow induced by the said ODE to be the set Au, where  = Au  W .

Analysis of the slower time-scale recursion: The slower time-scale stochastic recursion of the GTD2 algorithm is the following:

ut+1 = U ut + t (xt - t+1xt+1) wt xt , ut  Rd, u0  U.

(31)

Note that since

t t



0, the stochastic recursion (21) is managed on a faster timescale relative to

the the neural network stochastic recursion (11) and henceforth, we continue to maintain here the

quasi-stationary condition ¯t  ¯ = (, w¯) .

Now the above equation can be rearranged as follows:

ut+1 = U

ut + t

E wt+t1

+ Mt3+1 +

3 t

,

(32)

where wt+t1 (xt - t+1xt+1) wt xt = (xt - t+1xt+1) xt wt, the noise term Mt3+1

wt+t1 - E

wt+t1|Ft

and the bias

3 t

E tw+t1|Ft - E tw+t1 .

Similar to Equation (13), we can rewrite the above recursion as follows:

ut+1 = ut + t uUt E tw+t1

+ uUt Mt3+1 + uUt

3 t

+ o(t)

,

(33)

where uUt (·) is the Frechet derivative (defined in Equation (10)) of the projection operator U . Now the above equation can be interpreted in terms of stochastic recursive inclusion as follows:

ut+1 = ut + t uUt E tw+t1

+ uUt M3t+1 + Uut

3 t

+ o(t)

,

(34)

18

Under review as a conference paper at ICLR 2019

with Uut E tw+t1  h3(ut), where the set-valued map h3 : Rd  {subsets of Rd} is defined as

h3(u) Uu E wt+1 , where w  Au .

(35)

Indeed h3(u) = uU (Bw) , where B = E (xt - t+1xt+1) xt and w  Au . It is easy to
verify that B = ¯ Dd (I - t+1P )¯.
Here, one cannot directly apply the multi-timescale stochastic approximation results from (Borkar, 1997) since the said paper assumes that the limit point from the slower timescale recursion is unique (Please see Chapter 6 of (Borkar, 2008)). But in our setting, the slower timescale recursion (24) has several limit points (note that the stable limit set Au is not singleton). This is where our analysis differs from that of the seminal paper on GTD2 algorithm, where it is assumed that both the matrices E xtxt and E (xt - t+1xt+1)xt are certainly non-singular. However, in our TTN setting, one cannot guarantee this condition, since the features are apparently provided by a neural network and it is hard to fabricate the neural network to generate a collection of features with the desired non-singularity properties. In order to analyze the limiting behaviour of the GTD2 algorithm under the relaxed singularity setting, henceforth one has to view the stochastic recursion (31) as a stochastic recursion inclusion (Benaïm et al., 2005) and apply the recent results from (Ramaswamy and Bhatnagar, 2016) which analyzes the asymptotic behaviour of general multi-timescale stochastic recursive inclusions.
A few observations are in order:

E1: For each u  U , h3(u) is a singleton: This follows from the definition of h3 and Claim 1 above, where we established that each w  Au is the least squares solution to the linear system of equations ¯w = u. Therefore, it further implies that that h3 is a Marchaud map as well.
E2: suptN ( wt + ut ) <  a.s. This follows since W and U are bounded sets.
E3: {Uut M3t+1 }tN is a martingale-difference noise sequence with respect to the filtration {Ft+1}tN. This follows directly since {Mt3+1}tN is a martingale-difference noise sequence with respect to the same filtration.

E4: {Uut Mt3+1 }tN are square-integrable and K3  (0, ), such that E Uut Mt3+1 2 Ft  K3 1 + ut 2 + wt 2 a.s., t  N.

(36)

This follows directly from the finiteness of the underlying Markov chain and from the assumption that the boundary U is smooth.

E5: uUt

3 t

 0 as t   a.s. Proof similar to C3. This implies that the bias is asymptotically

irrelevant.

E6: For each u  U , the set Au is a globally attracting set of the ODE (28) and is also Lyapunov stable. Further, there exists K4  (0, ) such that supwAu w  K4(1 + u ). This follows since Au  W and W is bounded.

E7: The set-valued map q : U  {subsets of Rd} given by q(u) = Au is upper-semicontinuous:
Consider the convergent sequences {un}nN  u and {wn}nN  w with un  U and wn  q(un) = Au. Note that w  W , u  U since W and U are compact. Also ¯ Dd ¯wn = ¯ Dd un (from Claim 1). Now taking limits on both sides we get

lim
n

¯

Dd ¯wn

=

lim
n

¯

Dd un



¯ Dd ¯w = ¯ Dd u.

This implies that w  Au = q(u). The claim thus follows.

Thus we have established all the necessary conditions demanded by Theorem 3.10 of (Ramaswamy and Bhatnagar, 2016) to characterize the limiting behaviour of the stochastic recursive inclusion (34).

19

Under review as a conference paper at ICLR 2019

Now by appealing to the said theorem, we obtain the following result on the asymptotic behaviour of the GTD2 algorithm:

(u, w)

lim inf (u, w)
t

- (ut, wt)

 (u, w) w  Au ,
uA

where A is the set of asymptotically stable equilibria of the following ODE:

d u(t) = h3(u(t)), dt

u(0)  U°, t  R+.

(37) (38)

One can obtain similar results for projected TDC.

We now state our main result:

Theorem 1. Let   Rm+d be a compact, convex subset with smooth boundary. Let  be Frechet

LdieftfeKrebnetiathbeles.eFt oufrtahseyrm, lpettoti¯ca(l-ly21staJb)le(¯e)qbueiliLbirpiascohfittzhecofonltlionwuoinugs.OADlsEo,cloent tAasinsuedmipntsioidnes

1-3 :

hold.

d dt

¯(t)

=

¯(t)(-

1 2

¯J

)(¯(t)),

¯(0)  ° and t  R+.

Then the stochastic sequence {¯t}tN generated by the TTN converges almost surely to K (sample path dependent). Further,

TD() Convergence: Under the additional Assumption 4-TD(), we obtain the following result: For any   [0, 1], the stochastic sequence {wt}tN generated by the TD() algorithm (Algorithm 2) within the TTN setting converges almost surely to the unique limit w, where w satisfies

T ()(¯ w) = ¯ w, with T () defined in Lemma 2 and ¯  K (sample path dependent).

(39)

GTD2 Convergence: Let W, U  Rd be compact, convex subsets with smooth boundaries. Let Assumption 4-GTD2 hold. Let W and U be Frechet differentiable. Then the stochastic
sequences {wt}tN and {ut}tN generated by the GTD2 algorithm (Algorithm 3) within the TTN
setting satisfy

(u, w)

lim inf
t

(u, w)

- (ut, wt)

 (u, w) w  Au ,
uA

where A is the set of asymptotically stable equilibria of the following ODE:

d dt

u(t)

=

uU(t)

¯ Dd (I - t+1P )¯ u(t)

,

u(0)  U°, t  R+

and Au is the asymptotically stable equilibria of the following ODE:

d dt

w(t)

=

Ww(t)

¯ Dd u - ¯ Dd ¯ w(t) , w(0)  W° and t  R+,

with ¯  K (sample path dependent) and u defined in Eq. (30).

C ADDITIONAL EXPERIMENTS
C.1 NONIMAGE CATCHER Sensitivity to  for both non-image catcher and puddleworld are shown in Fig. 8.

20

Under review as a conference paper at ICLR 2019
Figure 8: Sensitivity plots for   {0, 0.3, 0.5, 0.9, 0.99, 1}. C.2 IMAGE CATCHER We also ran policy evaluation experiments on image-based catcher with 2 stacked 64x64 frames as input. The policy evaluated was the same as was used in the non-image setting. Similar to the non-imaged based catcher experiments, we have similar plots.
Figure 9: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the fast part of the TTN.
(a) (b) (c) Figure 10: a) Sensitivity plots for   {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison of least-squares methods
(a) (b) Figure 11: a) Comparison of fast and slow predictions. b) Comparison of surrogate loss functions.
21

Under review as a conference paper at ICLR 2019

C.3 CARTPOLE
In the classic Cartpole environment, the agent has to balance a pole on a cart. The state is given by vector of 4 numbers (cart position, cart velocity, pole angle, pole velocity). The two available actions are applying a force towards the left or the right. Rewards are +1 at every timestep and an episode terminates once the pole dips below a certain angle or the cart moves too far from the center. We use the OpenAI gym implementation (Brockman et al., 2016).
The policy to be evaluated consists of applying force in the direction the pole is moving with probability 0.9 (stabilizing the pole) or applying force in the direction of the cart's velocity with probability 0.1. We inject some stochasticity so that the resulting policy does not perform overly well, which would lead to an uninteresting value function.
Cartpole
ABBE

Root Mean Square Error

Nonlinear TD- Reg
TTN Nonlinear-TD

Nonlinear-GTD

ABTD

Number of Steps

Figure 12: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the

fast part of the TTN.

Cartpole

Cartpole

ETD TOETD

TDC

ETD

Root Mean Square Error

TDC

LSTD TD

Root Mean Square Error

TOTD FLSTD

TD TOETD LSTD

TOTD
Step size (10^x)
(a)

Nonlinear TD
Lambda
(b)

Figure 13: a) Sensitivity plots for   {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities.

Cartpole

Cartpole

Root Mean Square Error

FLSTD
TOETD LSTD

TDC TD

Root Mean Square Error

MSTDE

Reward Next State

semi-MSTDE

Number of Steps
(a)

Number of Steps
(b)

Figure 14: a) Comparison of least-squares methods b) Comparison of surrogate loss functions.

22

Under review as a conference paper at ICLR 2019

C.4 ACROBOT
In the classic Acrobot domain, the agent consisting of two links has to swing up past a certain height. The agent observes a 4-dimensional state consisting of the angles and the angular velocities of each link. The avaiable actions are three possible levels of torque to be applied to the joint.
The evaluated policy is obtained by training an agent with true-online Sarsa on a tile coding representation and then fixing its learned epsilon-greedy policy.
Acrobot

Root Mean Square Error

TTN with MSTDE

ABBE

Nonlinear TD- Reg

TTN with next state

ABTD

Nonlinear-TD

Nonlinear-GTD Number of Steps

Figure 15: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the

fast part of the TTN.

Acrobot

Acrobot

Root Mean Square Error

TDC

ETD TOTD

TOETD

TD LSTD

Root Mean Square Error

TOETD FLSTD

TOTD

TD TDC

ETD

LSTD

Nonlinear TD

Step size (10^x)
(a)

Lambda
(b)

Figure 16: a) Sensitivity plots for   {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities.

Acrobot

Acrobot

FLSTD

Root Mean Square Error

TOETD

TDC

TD

Root Mean Square Error

semi-MSTDE

MSTDE Reward

LSTD

Next State

Number of Steps
(a)

Number of Steps
(b)

Figure 17: a) Comparison of least-squares methods b) Comparison of surrogate loss functions.

23

Under review as a conference paper at ICLR 2019

C.5 PUCK WORLD
In Puck World (Tasfi, 2016), the agent has to move in a two-dimensional box towards a good puck while staying away from a bad puck. The 8-dimensional state consists of (player x location, player y location, player x velocity, player y velocity, good puck x location, good puck y location, bad puck x location, bad puck y location). Each action increases the agent's velocity in one of the four cardinal directions apart from a "None" action which does nothing. The reward is the negative distance to the good puck plus a penalty of -10 + x if the agent is within a certain radius of the bad puck, where x  [-2, 0] depends on the distance to the bad puck (the reward is slightly modified from the original game to make the value function more interesting).
The policy moves the agent towards the good puck, while having a soft cap on the agent's velocity. In more detail, to choose one action, it is defined by the following procedure: First, we choose some eligible actions. The None action is always eligible. The actions which move the agent towards the good puck are also eligible. For example, if the good puck is Northeast of the agent, the North and East actions are eligible. If the agent's velocity in a certain direction is above 30, then the action for that direction is no longer eligible. Finally, the agent picks uniformly at random from all eligible actions.
Puck World
Nonlinear TD- Reg

Root Mean Square Error

Nonlinear-GTD

ABTD

Nonlinear-TD

ABBE

TTN

Number of Steps

Figure 18: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the

fast part of the TTN.

Puck World

Puck World

TDC

TDC

TD

Root Mean Square Error

TD TOETD
TOTD

ETD LSTD

Root Mean Square Error

ETD TOTD

TOETD

FLSTD

Nonlinear TD

LSTD

Step size (10^x)
(a)

Lambda
(b)

Figure 19: a) Sensitivity plots for   {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities.

24

Under review as a conference paper at ICLR 2019

Puck World

Puck World

Root Mean Square Error

TD
LSTD FLSTD

TDC

TOETD

Root Mean Square Error

MSTDE Next State semi-MSTDE Reward

Number of Steps
(a)

Number of Steps
(b)

Figure 20: a) Comparison of least-squares methods b) Comparison of surrogate loss functions.

C.6 OFF-POLICY CATCHER
We run a preliminary experiment to check if TTN can have an advantage in the off-policy setting. The target policy is the same as the one used for other Catcher experiments (described in Appendix D).
The behaviour policy is slightly different. If the apple is within 20 units (the target policy is 25 units), then the agent takes the action in the direction of the apple with probability 0.7 and one of the other two actions with probability 0.15 each. If the apple is not within range, then the agent takes the None action 10% of the time and one of the other two with equal probability. This combination of behaviour and target policies results in importance sampling ratios in the range of 0 to 8.7, moderately large values.
We try TTN with three off-policy algorithms (TD, TDC and LSTD) and compare to off-policy Nonlinear TD. For TTN, the features are learnt optimizing the MSTDE on the behaviour policy while the values are learned off-policy. The main difference between TTN and Nonlinear TD is the fact that Nonlinear TD does off-policy updates to the entire network while TTN only changes the linear part with off-policy updates.
Off-Policy Catcher

Root Mean Square Error

TTN-TD
TTN-TDC Nonlinear TD TTN-LSTD

Number of Steps
Figure 21: Comparison of TTN and nonlinear TD From figure C.6, we see that TTN can outperform nonlinear TD in terms of average error and also has reduced variance (smaller error bars). This seems to suggest that the TTN architecture can grant additional stability in the off-policy setting.
D EXPERIMENTAL DETAILS
Description of policies The policy evaluated in Puddle World randomly took the north and east actions with equal probability, with episodes initialized in the southwest corner. The policy evaluated in Catcher increased the velocity in the direction of the apple if the agent was within 25 units or else chose the "None" action with a 20% chance of a random choice.
25

Under review as a conference paper at ICLR 2019

Competitor details Nonlinear TD uses the semi-gradient TD update with nonlinear function approximation. This is known not to be theoretically-sound and there exist counterexamples where the weights and predictions diverge (Tsitsiklis and Van Roy, 1997). Nevertheless, since this is the simplest algorithm, we use this as a baseline. Nonlinear GTD, conversely, has proven convergence results as it is an extension of the gradient TD methods to nonlinear function approximation.
Castro et al. proposed three variants of adaptive bases algorithms, each optimizing a different objective. We include two of them: ABTD, which is based on the plain TD update and ABBE, which is derived from the MSTDE. We omit ABPBE, which optimizes the MSPBE, since the algorithm is computationally inefficient, requiring O(d2m) memory, where d is the number of features in the last layer and m is the number of parameters for the bases (ie. weights in the neural network). Also, the derivation is similar in spirit to that of nonlinear GTD, so we would expect both to perform similarly.
Levine et al.'s algorithm was proposed for the control setting, combining DQN with periodic LSTD resolving for the last layer's weights. We adapt their idea for policy evaluation by adding regularization to the last layer's weights to bias them towards the LSTD solution and using semi-gradient TD on this objective. Then, we can then train in a fully online manner, which makes the algorithm comparable to the other competitors. This algorithm is labeled as Nonlinear TD-Reg.

Hyperparameters Here we present the refined hyperparameter ranges that were swept over for the experimental runs.
First, we outline the hyperparameters of all the algorithms and, afterwards, we give the values tested for each hyperparameter. There is one range for Puddle World and another for Catcher (both versions) since different ranges worked well for each domain. For the two-timescale networks, for each environment, the learning rate for the slow part was set to a fixed value which was sufficiently low for the fast part to do well but was not tuned extensively. This was done for all the experiments except for those on surrogate losses, where we swept over a range of values (since different losses could have different optimization properties).
Two-timescale Networks: For all algorithms, the learning rate for the slow part slow. TD, ETD, TOTD, TOETD : learning rate , trace parameter  TDC : primary weights learning rate , secondary weights learning rate , trace parameter  LSTD: initializer for the A-1 matrix inv, trace parameter  FLSTD: initializer for the A matrix , learning rate , forgetting rate , trace parameter 
Castro MSTDE, Castro TD: final layer learning rate , other layers learning rate 
Nonlinear GTD : primary weights learning rate , secondary weights learning rate 
Nonlinear TD : learning rate , trace parameter 
Nonlinear TD - Reg : learning rate , learning rate towards LSTD solution (regularization) , initializer for the A matrix 
For all algorithms that used eligibility traces, the range swept over was   {0, 0.3, 0.5, 0.9, 0.99, 1} The other hyperparameters are shown individually below.
Puddle world Two-timescale Networks:
slow = 10-3

TD, ETD, TOTD, TOETD:   10c, c  {-3.5, -3.25, ..., -1.25}

TDC: LSTD:

  10c, c  {-2, -1.75, ..., -0.25}   10c, c  {-3, -2.5, -2, -1.5}
inv  10c, c  {0, 0.25, ..., 3.75}

26

Under review as a conference paper at ICLR 2019

FLSTD:

  10c, c  {-3, -2, -1, 0}   10c, c  {-4, -3}
  10c, c  {-6, -5, -4}

Castro MSTDE, Castro TD:

  10c, c  {-4, -3.75, ..., -1.25}   10c, c  {-4, -3.75, ..., -1.25}

Nonlinear GTD:

  10c, c  {-3, -2.75, ..., -0.25}   10c, c  {-5, -4.5, ..., -0.5}

Nonlinear TD:

  10c, c  {-6, -5.75, ..., -2.25}

Nonlinear TD - Reg:

  10c, c  {-3.5, -3.0, ..., -1.5}   10c, c  {-3.5, -3, ..., -0.5}
  10c, c  {-3, -2, -1, 0}

Other environments Two-timescale Networks:

slow = 10-2.5
TD, ETD, TOTD, TOETD:   10c, c  {-3.5, -3.25, ..., -1.25}

TDC:

  10c, c  {-4, -2.75, ..., -1.25}   10c, c  {-3, -2}

LSTD:

inv  10c, c  {-2, -1.75, ..., 0.75}

FLSTD:

  10c, c  {-3, -2, -1, 0}   10c, c  {-4, -3}
  10c, c  {-5, -4, -3}

Castro MSTDE, Castro TD:

  10c, c  {-6, -5.75, ..., -4}   10c, c  {-6, -5.75, ..., -4}

Nonlinear GTD:

  10c, c  {-3, -2.75, ..., -0.25}   10c, c  {-5, -4.5, ..., -0.5}

Nonlinear TD:

  10c, c  {-4, -3.75, ..., -1.25}

Nonlinear TD - Reg:

  10c, c  {-6, -5.5, ..., -3}   10c, c  {-3.5, -3, ..., -0.5}
  10c, c  {-2, -1, 0, 1}

27

Under review as a conference paper at ICLR 2019
Control experiments For both control experiments, we modified the Catcher environment slightly from its default settings. The agent is given only 1 life and an episode terminates after it fails to catch a single apple. The reward for catching an apple is +1 and is -1 at the end of an episode. The discount factor is set to 0.99. For image-based Catcher, we stack two consecutive frames which we treat as the state. This is done so that the agent can perceive movements in the paddle, thus making the state Markov. Both DQN and TTN use an -greedy policy. For DQN, is annealed from 1.0 to 0.01 (0.1) for nonimage (image) Catcher over a certain number of steps. For TTN, is fixed to a constant value, 0.01 (0.1) for nonimage (image) Catcher. For both algorithms, we use a replay buffer of size 10000 (200000) for nonimage (image) Catcher. The buffer is initialized with 5000 (50000) transitions from a random policy. The minibatch size for DQN and feature learning was 32. TTN uses the AMSGrad optimizer with 1 = 0 and 2 = 0.99 and DQN uses the ADAM optimizer with default settings, 1 = 0.9, 2 = 0.999. For image catcher, due to the long training times, the hyperparameters were manually tuned. For nonimage catcher, we concentrated our hyperparameter tuning efforts on the most important ones and use the following strategy. We first used a preliminary search to find a promising range of values, followed by a grid search. For TTN with FQI, we focused tuning on the step sizes for the features and the regularization factor.For DQN, we focused on adjusting the step size and the number of steps over which , the probability of picking a random action, was annealed. The other hyperparameters were left to reasonable values.
28

Under review as a conference paper at ICLR 2019

Network architectures For TTN, this describes the neural network which learns the features. The layer marked as "features" is used to predict state-action values for the fast, linear part.
Image catcher - TTN
To predict the next state, we add an action embedding as done in (Oh et al., 2015).

Input Conv1 Conv2 Conv3 Dense1 (features)
From Dense1, Dense2 (reward prediction)
From Dense1, add action embedding Reshape Deconv1 Deconv2 Deconv3
Conv4 (state prediction)

2x64x64 grayscale image (stacked frames) 32 filters, 7x7 kernel, stride 4, ReLU 64 filters, 5x5 kernel, stride 2, ReLU 64 filters, 3x3 kernel, stride 1, ReLU 256 units, ReLU
3 units (one per action), Linear
256 units, Linear Into 8x8 image 16 filters, 5x5 kernel, stride 2, ReLU 16 filters, 5x5 kernel, stride 2, ReLU 16 filters, 5x5 kernel, stride 2, ReLU 1 filter, 3x3 kernel, stride 1, Linear

Image catcher - DQN

Input Conv1 Conv2 Conv3 Dense1 Dense2 (Q-values)

2x64x64 grayscale image (stacked frames) 32 filters, 7x7 kernel, stride 4, ReLU 64 filters, 5x5 kernel, stride 2, ReLU 64 filters, 3x3 kernel, stride 1, ReLU 256 units, ReLU 3 units, Linear

Nonimage catcher - TTN

Input Dense1 Dense2 Dense3 (features) Dense4 (state and reward prediction)

4-dimensional state 128 units, ReLU 128 units, ReLU 128 units, ReLU 15 units, Linear

Nonimage catcher - DQN

Input Dense1 Dense2 Dense3 Dense4 (Q-values)

4-dimensional state 128 units, ReLU 128 units, ReLU 128 units, ReLU 3, Linear

29

