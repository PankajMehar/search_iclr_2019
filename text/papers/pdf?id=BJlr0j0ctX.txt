Under review as a conference paper at ICLR 2019

LABEL SMOOTHING AND LOGIT SQUEEZING: A REPLACEMENT FOR ADVERSARIAL TRAINING?
Anonymous authors Paper under double-blind review

ABSTRACT
Adversarial training is one of the strongest defenses against adversarial attacks, but it requires adversarial examples to be generated for every mini-batch during optimization. The expense of producing these examples during training often precludes adversarial training from use on large and high-resolution image datasets. In this study, we explore the mechanisms by which adversarial training improves classifier robustness, and show that these mechanisms can be effectively mimicked using simple regularization methods, including label smoothing and logit squeezing. Remarkably, using these simple regularization methods in combination with Gaussian noise injection, we are able to achieve strong adversarial robustness ­ often exceeding that of adversarial training ­ using no adversarial examples.
Deep Neural Networks (DNNs) have enjoyed great success in many areas of computer vision, such as classification (Krizhevsky et al., 2012), object detection (Girshick, 2015), and face recognition (Najibi et al., 2017). However, the existence of adversarial examples has raised concerns about the security of computer vision systems (Szegedy et al., 2013; Biggio et al., 2013). For example, an attacker may cause a system to mistake a stop sign for another object (Evtimov et al., 2017) or mistake one person for another (Sharif et al., 2016). To address security concerns for high-stakes applications, researchers are searching for ways to make models more robust to attacks.
Many defenses have been proposed to combat adversarial examples. Approaches such as feature squeezing, denoising, and encoding (Xu et al., 2017; Samangouei et al., 2018; Shen et al., 2017; Meng & Chen, 2017) have had some success at pre-processing images to remove adversarial perturbations. Other approaches focus on hardening neural classifiers to reduce adversarial susceptibility. This includes specialized non-linearities (Zantedeschi et al., 2017), modified training processes Papernot et al. (2016), and gradient obfuscation Athalye et al. (2018).
Despite all of these innovations, adversarial training (Goodfellow et al., 2014), one of the earliest defenses, still remains among the most effective and popular strategies. In its simplest form, adversarial training minimizes a loss function that measures performance of the model on both clean and adversarial data as follows

minimize Ladv() = L(, xi, yi) + (1 - )L(, xi,adv, yi),
 i

(1)

where L is a standard (cross entropy) loss function, (xi, yi) is an input image/label pair,  contains

the classifier's trainable parameters,  is a hyper-parameter, and xi,adv is an adversarial example for

image x. Madry et al. (2017) pose adversarial training as a game between two players that similarly

requires computing adversarial examples on each iteration.

A key drawback to adversarial training methods is their computational cost; after every mini-batch of training data is formed, a batch of adversarial examples must be produced. To train a network that resists strong attacks, one needs to train with the strongest adversarial examples possible. For example, networks hardened against the inexpensive Fast Gradient Sign Method (FGSM, Goodfellow et al. (2014)) can be broken by a simple two-stage attack (Trame`r et al., 2017). Current state-of-theart adversarial training results on MNIST and CIFAR-10 use expensive iterative adversaries (Madry et al., 2017), such as the Projected Gradient Descent (PGD) method, or the closely related Basic Iterative Method (BIM) (Kurakin et al., 2016). Adversarial training using strong attacks may be 10-100 times more time consuming than standard training methods. This prohibitive cost makes it difficult to scale adversarial training to larger datasets and higher resolutions.

1

Under review as a conference paper at ICLR 2019

In this study, we show that it is possible to achieve strong robustness ­ comparable to or greater than the robustness of adversarial training with a strong iterative attack ­ using fast optimization without adversarial examples. We achieve this using standard regularization methods, such as label smoothing (Warde-Farley & Goodfellow, 2016) and the more recently proposed logit squeezing (Kannan et al., 2018). While it has been known for some time that these tricks can improve the robustness of models, we observe that an aggressive application of these inexpensive tricks, combined with random Gaussian noise, are enough to match or even surpass the performance of adversarial training on some datasets. For example, using only label smoothing and augmentation with random Gaussian noise, we produce a CIFAR-10 classifier that achieves over 73% accuracy against black-box iterative attacks, compared to 64% for a state-of-the-art adversarially trained classifier (Madry et al., 2017). In the white-box case, classifiers trained with logit squeezing and label smoothing get  50% accuracy on iterative attacks in comparison to  47% for adversarial training. Regularized networks without adversarial training are also more robust against non-iterative attacks, and more accurate on non-adversarial examples.
Our goal is not just to demonstrate these defenses, but also to dig deep into what adversarial training does, and how it compares to less expensive regularization-based defenses. We begin by dissecting adversarial training, and examining ways in which it achieves robustness. We then discuss label smoothing and logit squeezing regularizers, and how their effects compare to those of adversarial training. We then turn our attention to random Gaussian data augmentation, and explore the importance of this technique for adversarial robustness. Finally, we combine the regularization methods with random Gaussian augmentation, and experimentally compare the robustness achievable using these simple methods to that achievable using adversarial training.

1 WHAT DOES ADVERSARIAL TRAINING DO?

Adversarial training injects adversarial examples into the training data as SGD runs. During training, adversarial perturbations are applied to each training image to decrease the logit corresponding to its correct class. The network must learn to produce logit representations that preserve the correct labeling even when faced with such an attack. At first glance, it seems that adversarial training might work by producing a large "logit gap," i.e., by producing a logit for the true class that is much larger than the logit of other classes. Surprisingly, adversarial training has the opposite effect ­ we will see below that it decreases the logit gap. To better understand what adversarial training does, and how we can replicate it, we now break down the different strategies for achieving robustness.

1.1 A SIMPLE LINEARIZED MODEL OF ADVERSARIAL ROBUSTNESS

This section presents a simple metric for adversarial robustness that will help us understand adver-
sarial training. Consider an image x, and its logit representation z (i.e. pre-softmax activation)
produced by a neural network. Let zy denote the logit corresponding to the correct class y. If we add a small perturbation  to x, then the corresponding change in logits is approximately T xzy under a linear approximation, where xzy is the gradient of zy with respect to x.

Under a linearity assumption, we can calculate the step-size L needed to move an example from class y to another class y¯. A classifier is susceptible to adversarial perturbation  if the perturbed
logit of the true class is smaller than the perturbed logit of any other class:

zy + T xzy < zy¯ + T xzy¯.

(2)

Assuming a one-step  attack such as FGSM, the perturbation  can be expressed as

 = - L · sign(xzy - xzy¯),

(3)

where L is the -norm of the perturbation. Using this choice of , Equation 2 becomes

zy - zy¯ < - L · sign(xzy - xzy¯)T (xzy¯ - xzy) = L xzy - xzy¯ 1

where · 1 denotes the 1-norm of a vector. Therefore the smallest -norm of the perturbation required is the ratio of "logit gap" to "gradient gap", i.e.,

L>

zy - zy¯ xzy - xzy¯

.
1

(4)

2

Under review as a conference paper at ICLR 2019

description
MNIST naturally trained ( = 0.3) MNIST adv trained ( = 0.3)
CIFAR10 naturally trained ( = 8(/255)) CIFAR10 adv trained ( = 8(/255))

empirical FGSM

on X-ent on logits (CW)

7.05%

0.23%

95.25%

95.41%

13.33%

10.64%

56.22%

55.57%

Equation 4
0.0% 56.73% 0.0% 54.97%

Table 1: Experimental and predicted accuracy of classifiers for MNIST and CIFAR-10. The predicted accuracy is the percentage of images for which < L. The empirical accuracy is the percent of images that survive a perturbation of size . Attacks on both the cross-entropy (X-ent) and logits as in Carlini & Wagner (2017) (CW) are presented.

Equation 4 measures robustness by predicting the smallest perturbation size needed to switch the class of an image. While the formula for L makes linearity assumptions, the approximation L fairly accurately predicts the robustness of classifiers of the CIFAR-10 dataset (where perturbations are small and linearity assumptions cause little distortion). It is also a good ballpark approximation on MNIST, even after adversarial training (see Table 1).
Maximal robustness occurs when L is as large as possible. From equation 4, we observe 3 different strategies for hardening a classifier:
· Increase the logit gap: Maximize the numerator of equation 4 by producing a classifier with relatively large zy.
· Squash the adversarial gradients: Train a classifier that has small adversarial gradients xzy¯ for any class y¯. In this case a large perturbation is needed to significantly change zy¯.
· Maximize gradient coherence: Produce adversarial gradients xzy¯ that are highly correlated with the gradient for the correct class xzy. This will shrink the denominator of equation 4, and produce robustness even if adversarial gradients are large. In this case, one cannot decrease zy without also decreasing zy¯, and so large perturbations are needed to change the class label.

Frequency

6000 4000 2000
0 0

|zy - zy¯| Distribution
10 20 30 40 50 60
|zy - zy¯| |zy - zy¯| Distribution

70

20000

10000

0 0 100 200 300 400 500 600
|zy - zy¯|

Frequency

Frequency

L Distribution 30000 adv trained 20000 naturally trained
10000
0 0.0 0.2 0.4 0.6 0.8 1.0 1.2
L

8000 6000 4000 2000
0 -1.0

cos(zy, zy¯) Distribution

-0.5

0.0

0.5

cos(zy, zy¯)

1.0

Frequency

Figure 1: The effect of adversarial training on the numerator and denominator for L on MNIST. (left-top) The numerator of Equation 4 (i.e, the logit gap). (left-bottom) The denominator of Equation 4. (right-top) A histogram of values of L calculated by applying Equation 4 to test data. (right-bottom) Cosine between the gradient vectors of the logits (i.e., gradient coherence).
The most obvious strategy for achieving robustness is to increase the numerator in equation 4 while fixing the denominator. Remarkably, our experimental investigation reveals that adversarial training does not rely on this strategy at all, but rather it decreases the logit gap and gradient gap simultaneously. This can be observed in Figure 1, which shows distributions of logit gaps for naturally and

3

Under review as a conference paper at ICLR 2019

adversarially trained models on MNIST. Note that the cross entropy loss actually limits adversarial training from increasing logit gaps. The accuracy of the classifier goes down in the presence of adversarial examples, and so the cross entropy loss is minimized by smaller logit gaps that reflect the lower level of certainty in the adversarial training environment.

Frequency Frequency

10000 8000 6000 4000 2000
0 0

zy Distribution
20 step adv trained 2 step adv trained naturally trained
10 20 30 40 50
zy

15000 10000

zy Distribution
20 step adv trained 2 step adv trained naturally trained

5000

0 0 5 10 15 20 25 30
zy

(a) MNIST logit distributions.

(b) CIFAR10 logit distributions.

Figure 2: Adversarial training squishes the logits. Training on stronger adversaries squishes more.

Adversarial training succeeds by minimizing the denominator in Equation 4; it simultaneously squeezes the logits and crushes the adversarial gradients. Figure 1 shows that the adversarial gradients shrink dramatically more than the logit gaps, and so the net effect is an increase in robustness.
If we closely examine the phenomenon of shrinking the logit gaps, we find that this shrink is due in part to an overall shrink in the size of the logits themselves, (i.e., |zi| for any class i). To see this, we plot histograms of the logits when classifiers are adversarially trained with strong adversaries1, weak adversaries2, and with no adversarial examples. Figure 2 shows that adversarial training does indeed squash the logits, although not enough to fully explain the decrease in |zy - zy¯| in Figure 1. 3
We have seen that adversarial training works by squashing adversarial gradients and slightly increasing gradient coherence. But the fact that it cannot do this without decreasing the logit gap leads us to suspect that these quantities are inherently linked. This leads us to ask an important question: If we directly decrease the logit gap, or the logits themselves, using an explicit regularization term, will this have the desired effect of crushing the adversarial gradients?

2 EASY WAYS TO IMITATE ADVERSARIAL TRAINING: LABEL SMOOTHING &
LOGIT SQUEEZING

There are two approaches to replicating the effect on the logits produced by adversarial training. The first is to replicate the decrease in logit gap seen in Figure 1. This can be achieved by label smoothing. A second approach to replicating adversarial training is to just directly crush all logit values and mimic the behavior in Figure 2. This approach is known as "logit squeezing," and works by adding a regularization term to the training objective that explicitly penalizes large logits.

Label smoothing Label smoothing converts "one-hot" label vectors into "one-warm" vectors that represents a low-confidence classification. Because large logit gaps produce high-confidence classifications, label-smoothed training data forces the classifier to produce small logit gaps. Label smoothing is a commonly used trick to prevent over-fitting on general classification problems, and it was first observed to boost adversarial robustness by Warde-Farley & Goodfellow (2016), where it was used as an inexpensive replacement for the network distillation defense (Papernot et al., 2016). A one-hot label vector yhot is smoothed using the formula

ywarm

=

yhot

-



×

(yhot

-

1 ),
Nc

1MNIST: 40-step PGD with step-size 0.01 and =0.3. CIFAR-10: 7-step PGD with step-size 2 and = 8. 2MNIST: 2-step PGD with step-size 0.2 and =0.3. CIFAR-10: 2-step PGD with step-size 5 and = 8. 3Similar plots for the effect of adversarial training on CIFAR-10 are in Appendix A (Figure 5).

4

Under review as a conference paper at ICLR 2019

where   [0, 1] is the smoothing parameter, and Nc is the number of classes. If we pick  = 0 we get a hard decision vector with no smoothing, while  = 1 creates an ambiguous decision by
assigning equal certainty to all classes.

Logit squeezing A second approach to replicating adversarial training is to just directly crush all logit values and mimic the behavior in Figure 2. This approach is known as "logit squeezing," and works by adding a regularization term to the training objective that explicitly penalizes large logits. Kannan et al. (2018) were the first to introduce logit-squeezing as an alternative to a "logit pairing" defense. Logit squeezing relies on the loss function

minimize

L(, xi, yi) + ||z(xi)||p,



i

(5)

where  is the squeezing parameter (i.e., coefficient for the logit-squeezing term). The logitsqueezing term could be any p-norm. In our experiments, we consider the 2 norm.

Can such simple regularizers really replicate adversarial training? Our experimental results suggest that simple regularizers can hurt adversarial robustness, which agrees with the findings in Zantedeschi et al. (2017). However, these strategies become highly effective when combined with a simple trick from the adversarial training literature -- data augmentation with Gaussian noise.

3 GAUSSIAN NOISE SAVES THE DAY
Adding Gaussian noise to images during training (i.e, Gaussian noise augmentation) can be used to improve the adversarial robustness of classifiers (Kannan et al., 2018; Zantedeschi et al., 2017). However, the effect of Gaussian noise is not well understood (Kannan et al., 2018). Here, we take a closer look at the behavior of Gaussian augmentation through systematic experimental investigations, and see that its effects are more complex than one might think.
3.1 GAUSSIAN NOISE SYNERGISTICALLY INTERACTS WITH REGULARIZERS
Label smoothing and logit squeezing become shockingly effective at hardening networks when they are combined with Gaussian noise augmentation. From the robustness plots in Figure 3, we can see that training with Gaussian noise alone produces a noticeable change in robustness, which seems to be mostly attributable to a widening of the logit gap and slight decrease in the gradient gap ( xzy - xzy¯ 1). The small increase in robustness from random Gaussian augmentation was also reported by Kannan et al. (2018). We also see that label smoothing alone causes a very slight drop in robustness; the shrink in the gradient gap is completely offset by a collapse in the logit gap.
Surprisingly, Gaussian noise and label smoothing have a powerful synergistic effect. When used together they cause a dramatic drop in the gradient gap, leading to a surge in robustness. A similar effect happens in the case of logit squeezing, and results are shown in Appendix B (Figure 7).
3.2 GAUSSIAN NOISE HELPS REGULARIZATION PROPERTIES GENERALIZE "OFF THE MANIFOLD"
Regularization methods have the potential to squash or align the adversarial gradients, but these properties are only imposed during training on images from the manifold that the "true" data lies on. At test time, the classifier sees adversarial images that do not "look like" training data because they lie off of, but adjacent to, the image manifold. By training the classifier on images with random perturbations, we teach the classifier to enforce the desired properties for input images that lie off the manifold.
The generalization property of Gaussian augmentation seems to be independent from, and sometimes conflicting with, the synergistic properties discuss above. In our experiments below, we find that smaller noise levels lead to a stronger synergistic effect, and yield larger L and better robustness to FGSM attacks. However, larger noise levels enable the regularization properties to generalize further off the manifold, resulting in better robustness to iterative attacks or attacks that escape the flattened region by adding an initial random perturbation. See the results on MNIST in Table 2 and

5

Under review as a conference paper at ICLR 2019

the results on CIFAR-10 in Table 3 for various values of  (standard deviation of Gaussian noise). For more comprehensive experiments on the different parameters that contribute to the regularizers see Table 5 for MNIST and Tables 6 & 7 in Appendices C & D.

Frequency

30000 20000 10000
0 0
80000 60000 40000 20000
0 0

|zy - zy¯| Distribution
10 20 30 40 50 60
|zy - zy¯| |zy - zy¯| Distribution
100 200 300 400 500 600
|zy - zy¯|

Frequency

Frequency

L Distribution
40000 label smooth + rand rand
20000 naturally trained label smooth
0 0.0 0.2 0.4 0.6 0.8 1.0 1.2
L

15000 10000 5000
0 -1.0

cos(zy, zy¯) Distribution

-0.5

0.0

0.5

cos(zy, zy¯)

1.0

Frequency

Figure 3: Label smoothing by itself worsens robustness. With the addition of Gaussian augmentation, robustness improves. This enhanced robustness results from both gradients getting squashed and gradients becoming more aligned ­ two effects that shrink the denominator in Equation 4. Plots use MNIST test samples with  = 0.5 and  = 0.75. The y-axis of the logit gap is chopped at 30k.

3.3 WHAT IS THE DIFFERENCE BETWEEN LOGIT SQUEEZING AND LABEL SMOOTHING
Label smoothing (i.e. reducing the variance of the logits) is helpful because it causes the gradient gap to decrease. The decreased gradient gap may be due to smaller element-wise gradient amplitudes, the alignments of the adversarial gradients, or both. To investigate the causes, we plot the 1 norm of the gradients of the logits with respect to the input image 4 and the cosine of the angle between the gradients (Figure 4). We see that in label smoothing (with Gaussian augmentation), both the gradient magnitude decreases and the gradients get more aligned. Larger smoothing parameters  cause the gradient to be both smaller and more aligned.
When logit squeezing is used with Gaussian augmentation, the magnitudes of the gradients decrease. The distribution of the cosines between gradients widens, but does not increase like it did for label smoothing. These effects are very similar to the behavior of adversarial training in Figure 1. Interestingly, in the case of logit squeezing with Gaussian noise, unlike label smoothing, the numerator of Equation 4 increases as well. This increase in the logit gap disappears once we take away Gaussian augmentation (See Appendix B Figure 7). Simultaneously increasing the numerator and decreasing the denominator of Equation 4 potentially gives a slight advantage to logit squeezing.
3.4 MNIST RESULTS FOR VARIOUS DEFENSE PARAMETERS
There are multiple factors that can affect the robustness of the MNIST classifier 5. While regularizers do not yield more robustness than adversarial training for MNIST, the results are promising given that these relatively high values of robustness come at a cheap cost in comparison to adversarial training. In Table 2 we notice that as we increase the number of training iterations k, we get more robust models for both logit squeezing and label smoothing 6. We get more robust models when we use larger smoothing () and squeezing () parameters, and when Gaussian augmentation is used with standard deviation  that is greater than the desired (the maximum perturbation size).
4This plot is moved to the appendix due to space limitations. See Appendix B (Figure 6). 5Similar to other experiments, we use the same architecture and hyper-parameters of Madry et al. (2017) 6For comprehensive results with more sensitivity analysis on the parameters, see Table 5 in Appendix C.

6

Under review as a conference paper at ICLR 2019

Frequency

80000 60000 40000 20000
0 0
80000 60000 40000 20000
0 0

|zy - zy¯| Distribution
10 20 30 40 50
|zy - zy¯| |zy - zy¯| Distribution
100 200 300 400 500 600
|zy - zy¯|

Frequency

Frequency

30000 20000 10000

L Distribution
label smooth (0.75) + rand (0.5) label smooth (0.2) + rand (0.5) logit squeeze (2.0) + rand (0.5) logit squeeze (0.5) + rand (0.5) naturally trained

0 0.0 15000

0.2 0.4 0.6 0.8 1.0 L
cos(zy, zy¯) Distribution

1.2

10000

5000

0 -1.0

-0.5

0.0

0.5

cos(zy, zy¯)

1.0

Frequency

Figure 4: Label smoothing plus Gaussian augmentation works by decreasing the gradient gap. This is done by both shrinking the gradients and aligning them. The effects get larger with the increase in the smoothing parameter. Logit squeezing plus Gaussian noise works by increasing the logit gap and decreasing the gradient gap. Increasing the squeezing parameter, strengthens these effects.

defense params

 

k

0 0.5 0.5 400k

0 2.0 0.0 400k

0 2.0 0.3 400k

0 2.0 0.5 400k

0 5.0 0.5 400k

0.2 0 0 400k

0.2 0 0.1 400k

0.2 0 0.3 400k

0.2 0 0.7 400k

0.95 0 0.3 100k

0.95 0 0.3 400k

0.95 0 0.3 2M

Madry et al. (2017) adv tr.

Train 99.96% 63.11% 99.99% 99.94% 99.84% 100.00% 100.00% 100.00% 99.76% 99.78% 99.99% 100.00% 100%

Test 99.21% 64.14% 97.17% 99.21% 99.18% 99.36% 99.39% 99.40% 99.04% 99.38% 99.44% 99.25% 98.8%

White-box 40-step PGD X-ent CW 72.44% 72.39% 0.00% 0.00% 76.08% 76.28% 79.13% 78.36% 78.21% 77.29% 0.00% 0.00% 15.59% 17.91% 70.97% 73.71% 66.34% 70.96% 60.70% 61.93% 74.02% 75.46% 83.60% 85.39% 93.20% 93.9 %

black-box

FGSM 87.58% 17.69% 82.11% 87.75% 87.73% 23.88% 59.14% 81.60% 87.27% 94.29% 93.97% 85.39% 96.08%

PGD 89.48% 24.52% 85.80% 89.47% 89.25% 29.33% 63.73% 85.38% 88.60% 77.21% 85.00% 87.22% 96.81%

Table 2: Accuracy of different MNIST classifiers against PGD and FGSM attacks on X-ent and CW losses under the white-box and black-box threat models. Attacks have maximum  perturbation
= 0.3. The iterative white-box attacks have an initial random step. The naturally trained model is used for generating black-box attacks. We use CW loss for the black-box attack.

4 AGGRESSIVE LABEL SMOOTHING AND LOGIT SQUEEZING OUTPERFORM ADVERSARIAL TRAINING ON CIFAR-10
We trained CIFAR-10 classifiers using aggressive values for the smoothing and squeezing parameters on the CIFAR10 data set. We compare our results to those of Madry et al. (2017). Note that the adversarially trained model from Madry et al. (2017) has been trained for 80,000 iterations on adversaries which are generated using a 7-step PGD. Keeping in mind that each step requires a
7

Under review as a conference paper at ICLR 2019

forward and backward pass, the running time of training for 80,000 iterations on 7-step PDG examples is equivalent to 640,000 iterations of training with label smoothing or logit squeezing. A short version of our results on white-box attacks are summarized in Table 3. The results of some of our black-box experiments are summarized in Table 47. While logit squeezing seems to outperform label smoothing in the white-box setting, label smoothing is slightly better under the black-box threat.
We see that aggressive logit squeezing with squeezing parameter  = 10 and  = 20 results in a model that is more robust than the adversarially trained model from Madry et al. (2017). Interestingly, it also achieves higher test accuracy on clean examples.

defense params
  k 0.95 0 20 160k 0.95 0 30 160k 0.95 0 30 240k 0.8 0 30 320k
0 10 20 160k 0 10 30 80k 0 10 30 160k Madry et al. (2017)

Train 99.90% 99.64% 99.70% 99.72% 99.92% 99.45% 99.82% 100.00%

Test 92.88% 90.70% 90.55% 90.23% 92.68% 89.89% 90.49% 87.25%

White-box 20-step PGD + Rand
xent CW 43.00% 41.29% 53.93% 40.68% 47.27% 37.25% 43.51% 42.96% 52.55% 48.78% 48.46% 45.51% 52.30% 49.73% 45.84% 46.90%

FGSM xent CW 75.25% 74.51% 64.77% 70.38% 64.36% 69.44% 74.68% 72.66% 76.37% 75.79% 68.19% 67.25% 72.08% 71.08% 56.22% 55.57%

Table 3: White-box attacks on CIFAR-10 models. We use  attacks with = 8. For the 20-step PGD, similarly to Madry et al. (2017), we use an initial random perturbation. We do not use a
random perturbation for the FGSM attack since it decreased the attack's effectiveness.

defense params
  k 0.95 0 20 160k 0.95 0 30 160k 0.95 0 30 240k 0.8 0 30 320k
0 10 20 80k 0 10 20 160k 0 10 30 80k 0 10 30 160k Madry et al. (2017)

7-step PGD xent CW 71.58% 71.96% 68.33% 68.88% 67.88% 68.59% 67.55% 68.32% 70.27% 70.78% 70.75% 71.49% 66.53% 67.46% 67.05% 67.79% 63.39%* 64.38%*

Black-box 7-step PGD + Rand
xent CW 72.44% 73.16% 69.09% 69.63% 68.84% 69.63% 68.49% 69.28% 71.29% 71.86% 71.63% 72.22% 67.52% 68.40% 68.30% 68.89% 63.39%* 64.38%*

FGSM xent CW 74.01% 74.68% 70.85% 71.71% 70.53% 71.32% 70.03% 70.89% 72.47% 73.30% 73.48% 73.82% 69.30% 69.99% 69.94% 70.61% 67.00% 67.25%

Table 4: Black-box attacks on CIFAR-10 models. Attacks are  with = 8. Similar to Madry et al. (2017), We build 7-step PGD attacks and FGSM attacks for a public adversarially trained model.
*Values taken from the original paper by Madry et al. (2017).

5 CONCLUSION
We studied the robustness of adversarial training, label smoothing, and logit squeezing through a linear approximation L that relates the magnitude of adversarial perturbations to the logit gap and the difference between the adversarial directions for different labels. Using this simple model, we observe how adversarial training achieves robustness and try to imitate this robustness using label smoothing and logit squeezing. The resulting methods perform well on MNIST, and can get results on CIFAR-10 that excel over adversarial training in both robustness and accuracy on clean examples. By demonstrating the effectiveness of these simple regularization methods, we hope this work can help make robust training easier and more accessible to practitioners.
7For more complete results see Table 6 and Table 7 in Appendix D.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. CoRR, abs/1802.00420, 2018. URL http://arxiv.org/abs/1802.00420.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim S rndic´, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pp. 387­402. Springer, 2013.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39­57. IEEE, 2017.
Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. Robust physical-world attacks on deep learning models. arXiv preprint arXiv:1707.08945, 1, 2017.
Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 1440­1448, 2015.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135­147. ACM, 2017.
Mahyar Najibi, Pouya Samangouei, Rama Chellappa, and Larry S Davis. Ssh: Single stage headless face detector. In ICCV, pp. 4885­4894, 2017.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP), pp. 582­597. IEEE, 2016.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 1528­1540. ACM, 2016.
Shiwei Shen, Guoqing Jin, Ke Gao, and Yongdong Zhang. Ape-gan: Adversarial perturbation elimination with gan. ICLR Submission, available on OpenReview, 4, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Trame`r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.
9

Under review as a conference paper at ICLR 2019 David Warde-Farley and Ian Goodfellow. 11 adversarial perturbations of deep neural networks.
Perturbations, Optimization, and Statistics, pp. 311, 2016. Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. arXiv preprint arXiv:1704.01155, 2017. Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. Efficient defenses against adver-
sarial attacks. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 39­49. ACM, 2017.
10

Under review as a conference paper at ICLR 2019

Appendix: Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training?

A ADVERSARIAL TRAINING EFFECTS ON CIFAR-10
Similarly to what we observed about adversarial training on MNIST, adversarial training on CIFAR10 works by greatly shrinking the adversarial gradients and also shrinking the logit gaps. The shrink in the gradients is much more dramatic than the shrink in the logit gap, and overwhelms the decrease in the numerator of Equation 4. See Figure 5.

Frequency

8000 6000 4000 2000
0 0
40000 30000 20000 10000
0 0

|zy - zy¯| Distribution
5 10 15 20 25 30
|zy - zy¯| |zy - zy¯| Distribution
5 10 15 20 25
|zy - zy¯|

35 30

Frequency

Frequency

L Distribution
40000
adv trained
30000
naturally trained
20000
10000
0 0 10 20 30 40 50 60 L

4000 3000 2000 1000
0 -1.0

cos(zy, zy¯) Distribution

-0.5

0.0

cos(zy, zy¯)

0.5

Frequency

Figure 5: The effect of adversarial training on the numerator and denominator of the expression
for L on CIFAR-10. (left-top) The numerator of Equation 4 (i.e, the logit gap). (left-bottom) The denominator of Equation 4. (right-top) A histogram of values of L calculated by applying Equation 4 to test data. (right-bottom) Cosine between the gradient vectors of the logits (i.e., gradient
coherence).

B THE MAGIC OF GAUSSIAN AUGMENTATION PLUS LOGIT SQUEEZING
As shown empirically in Table 5, and analytically using the linear approximation in Equation 4 evaluated in Figure 7, logit squeezing worsens robustness when Gaussian augmentation is not used. However, when fused with Gaussian augmentation, logit squeezing achieves good levels of robustness. This addition of Gaussian augmentation has three observed effects: the gradients get squashed, the logit gap increases, and the gradients get slightly more aligned. The increase in the logit gaps increases the numerator in Equation 4. This gives a slight edge to logit squeezing in comparison to label smoothing, that mostly works by decreasing the denominator in Equation 4.
C FULL RESULTS ON MNIST
The results for all of our experiments on MNIST are summarized in Table 5. As can be seen, Gaussian random augmentation by itself ( =  = 0) is effective in increasing robustness on black-box attacks. It does not, however, significantly increase robustness on white-box attacks. Models that are only trained with either logit squeezing or label smoothing without random Gaussian augmentation ( = 0), can be fooled by an adversary that has knowledge of the model parameters (white-box) with accuracy 100%. In the black-box setting, they are also not robust.
Increasing the magnitude of the noise () generally increases robustness but degrades the performance on the clean examples. Keeping the number of iterations k constant, for extremely large 
11

Under review as a conference paper at ICLR 2019

Frequency

80000 60000 40000 20000

|zy| Distribution
label smooth (0.75) + rand (0.5) label smooth (0.2) + rand (0.5) logit squeeze (2.0) + rand (0.5) logit squeeze (0.5) + rand (0.5) naturally trained

0 0 100 200 300 400
|zy|

Figure 6: Label smoothing decreases the difference between the adversarial gradients (the denominator of Equation 4) by shrinking the gradient magnitudes. The gradients magnitudes are depicted here using a histogram.

Frequency

80000 60000 40000 20000
0 0
60000 40000 20000
0 0

|zy - zy¯| Distribution
10 20 30 40 50 60 70
|zy - zy¯| |zy - zy¯| Distribution
100 200 300 400 500 600
|zy - zy¯|

Frequency

Frequency

40000 30000 20000 10000
0 0.0
8000 6000 4000 2000
0 -1.0

L Distribution logit squeeze + rand rand naturally trained logit squeeze
0.2 0.4 0.6 0.8 L
cos(zy, zy¯) Distribution

-0.5

0.0

0.5

cos(zy, zy¯)

1.0

Frequency

Figure 7: Random Gaussian augmentation helps logit squeezing. Graphs are for MNIST test examples with  = 0.5 and  = 0.5.

12

Under review as a conference paper at ICLR 2019

values, the robustness also starts to drop. At the expense of extra computations k, the robustness and the accuracy on clean examples improves.

defense params   k

Train

White-box

black-box

40-step PGD

Test X-ent CW FGSM PGD

0 0 0.5 100k 0 0 0.3 400k 0 0 0.5 400k 0 0.5 0.5 100k 0 0.5 0.5 400k 0 2.0 0.0 400k 0 2.0 0.3 400k 0 2.0 0.5 100k 0 2.0 0.5 400k 0 5.0 0.5 100k 0 5.0 0.5 400k 0 10.0 0.5 100k 0 10.0 0.5 400k 0 0 0.3 400k 0 0 0.5 400k 0 0 0.7 400k 0 0 1.0 400k 0.2 0 0 400k 0.2 0 0.1 400k 0.2 0 0.2 400k 0.2 0 0.3 400k 0.2 0 0.4 400k 0.2 0 0.5 400k 0.2 0 0.6 400k 0.2 0 0.7 400k 0.5 0 0.3 400k 0.5 0 0.5 400k 0.5 0 0.7 400k 0.75 0 0 400k 0.75 0 0.3 400k 0.75 0 0.5 400k 0.75 0 0.7 400k 0.95 0 0 400k 0.95 0 0.3 100k 0.95 0 0.3 400k 0.95 0 0.3 2M 0.95 0 0.5 100k 0.95 0 0.5 400k 0.95 0 0.5 2M Madry et al. (2017) adv tr.

99.89% 99.97% 99.99% 99.83% 99.96% 63.11% 99.99% 99.69% 99.94% 99.27% 99.84% 74.16% 13.27% 100.00% 100.00% 99.90% 98.66% 100.00% 100.00% 100.00% 100.00% 100.00% 99.99% 99.92% 99.76% 100.00% 99.97% 99.63% 100.00% 100.00% 99.90% 99.44% 100.00% 99.78% 99.99% 100.00% 99.20% 99.82% 99.93% 100%

98.97% 99.07% 99.09% 99.06% 99.21% 64.14% 97.17% 99.05% 99.21% 99.02% 99.18% 75.76% 12.93% 99.42% 99.23% 99.08% 98.17% 99.36% 99.39% 99.41% 99.40% 99.33% 99.20% 99.16% 99.04% 99.36% 99.29% 98.90% 99.37% 99.41% 99.23% 98.71% 99.21% 99.38% 99.44% 99.25% 99.01% 99.23% 98.98% 98.8%

5.18% 3.27% 11.24% 57.57% 72.44% 0.00% 76.08% 67.93% 79.13% 65.53% 78.21% 3.15% 9.40% 2.90% 11.75% 13.68% 1.85% 0.00% 15.59% 55.50% 70.97% 76.86% 76.81% 74.28% 66.34% 74.61% 79.12% 70.17% 0.00% 75.21% 78.35% 70.11% 0.00% 60.70% 74.02% 83.60% 61.95% 74.15% 77.92% 93.20%

1.95% 0.83% 17.52% 55.77% 72.39% 0.00% 76.28% 66.38% 78.36% 62.00% 77.29% 2.43% 8.88% 1.06% 19.64% 22.95% 9.33% 0.00% 17.91% 56.63% 73.71% 80.07% 79.55% 75.19% 70.96% 75.10% 79.24% 70.16% 0.00% 75.62% 79.60% 69.95% 0.00% 61.93% 75.46% 85.39% 60.80% 73.19% 82.05% 93.9 %

84.23% 79.36% 87.28% 85.91% 87.58% 17.69% 82.11% 85.09% 87.75% 85.53% 87.73% 57.35% 10.60% 78.67% 87.57% 87.53% 82.19% 23.88% 59.14% 69.40% 81.60% 86.71% 87.79% 87.87% 87.27% 81.66% 87.32% 86.85% 15.85% 79.91% 87.20% 86.97% 16.76% 94.29% 93.97% 85.39% 84.83% 86.72% 87.33% 96.08%

86.21% 81.61% 89.44% 87.48% 89.48% 24.52% 85.80% 86.29% 89.47% 86.64% 89.25% 61.55% 11.00% 80.67% 89.71% 89.14% 83.09% 29.33% 63.73% 76.46% 85.38% 88.75% 89.50% 89.43% 88.60% 86.26% 89.28% 87.49% 21.26% 84.10% 88.83% 87.99% 19.81% 77.21% 85.00% 87.22% 86.02% 87.87% 88.78% 96.81%

Table 5: Accuracy of different models trained on MNIST with a 40 step PGD attack on the crossentropy (X-ent) loss and the Carlini-Wagner (CW) loss under the white-box and black-box threat models. Attacks are  attacks with a maximum perturbation of = 0.3. The iterative white-box attacks have an initial random step. The naturally trained model was used for generating the attacks for the black-box threat model. We use the CW loss for the FGSM attack in the blackbox case. k is the number of training iterations.

13

Under review as a conference paper at ICLR 2019

D COMPLETE RESULTS ON CIFAR-10

Here we take a deeper look at reguarlized training results for CIFAR-10. The conclusions that can be drawn in this case are parallel with those of MNIST discussed in Appendix C. It is worth noting that while the results of logit squeezing outperform those from label smoothing in the white-box setting, training with large squeezing coefficient  often fails and results in low accuracy on test data. This breakdown of training rarely happens for label smoothing (even for very large smoothing parameters ).

defense params

White-box

20-step PGD

FGSM

   k Train Test xent CW xent CW

0.2 0 20 80k 0.2 0 30 80k 0.2 0 40 80k 0.5 0 20 80k 0.5 0 30 80k 0.9 0 30 80k 0.2 0 30 160k 0.5 0 30 160k 0.8 0 30 160k 0.95 0 20 160k 0.95 0 30 160k 0.95 0 40 160k 0.95 0 30 240k 0.8 0 30 320k 0.95 0 40 320k 0 10 20 80k 0 10 20 160k 0 10 30 80k 0 10 30 160k
Madry et al. (2017)

99.88% 99.67% 98.99% 99.85% 99.50% 99.30% 99.72% 99.70% 99.73% 99.90% 99.64% 98.94% 99.70% 99.72% 94.10% 99.77% 99.92% 99.45% 99.82% 100.00%

92.94% 91.08% 87.97% 92.57% 90.82% 90.59% 90.64% 90.58% 90.83% 92.88% 90.70% 87.81% 90.55% 90.23% 86.12% 92.16% 92.68% 89.89% 90.49% 87.25%

31.27% 38.26% 37.12% 31.37% 38.30% 33.89% 37.67% 40.22% 39.20% 43.00% 53.93% 49.23% 47.27% 43.51% 46.82% 45.46% 52.55% 48.46% 52.30% 45.84%

32.16% 39.26% 36.28% 32.55% 39.46% 33.21% 39.60% 39.96% 37.73% 41.29% 40.68% 35.93% 37.25% 42.96% 19.27% 41.39% 48.78% 45.51% 49.73% 46.90%

75.10% 71.93% 67.22% 75.76% 72.83% 2.04% 76.07% 74.50% 74.29% 75.25% 64.77% 61.42% 64.36% 74.68% 55.11% 72.28% 76.37% 68.19% 72.08% 56.22%

69.88% 67.30% 62.01% 73.45% 71.32% 1.11% 69.48% 71.55% 72.81% 74.51% 70.38% 64.95% 69.44% 72.66% 45.48% 71.42% 75.79% 67.25% 71.08% 55.57%

Table 6: White-box attacks on the CIFAR-10 models. All attacks are  attacks with = 8. For the 20-step PGD, similar to Madry et al. (2017), we use an initial random perturbation.

E OTHER METRICS OF SIMILARITY BETWEEN ADVERSARIAL TRAINING, LOGIT SQUEEZING, AND LABEL SMOOTHING
While it seems that logit squeezing, label smoothing, and adversarial training have a lot in common when we look at quantities affecting the linear approximation L, we wonder whether they still look similar with respect to other metrics. Here, we look at the sum of the activations in the logit layer for every logit (Figure 9) and the sum of activations for every neuron of the penultimate layer (Figure 8). The penultimate activations are often seen as the "feature representation" that the neural network learns. By summing over the absolute value of the activations of all test examples for every neuron in the penultimate layer, we can identify how many of the neurons are effectively inactive.
When we perform natural training, all neurons become active for at least some images. After adversarial training, this is no longer the case. Adversarial training is causing the effective dimensionality of the deep feature representation layer to decrease. One can interpret this as adversarial training learning to ignore features that the adversary can exploit ( 400 out of the 1024 neurons of the penultimate layer are deactivated). Shockingly, both label smoothing and logit squeezing do the same ­ they deactivate roughly 400 neurons from the deep feature representation layer.

14

Under review as a conference paper at ICLR 2019

defense params   k

7-step PGD xent CW

Black-box 7-step PGD+Rand xent CW

FGSM xent CW

0.2 0 20 80k 0.2 0 30 80k 0.2 0 40 80k 0.5 0 20 80k 0.5 0 30 80k 0.9 0 30 80k 0.2 0 30 160k 0.5 0 30 160k 0.8 0 30 160k 0.95 0 20 160k 0.95 0 30 160k 0.95 0 40 160k 0.95 0 30 240k 0.8 0 30 320k 0.95 0 40 320k 0 10 20 80k 0 10 20 160k 0 10 30 80k 0 10 30 160k
Madry et al. (2017)

71.74% 67.53% 64.39% 71.17% 67.31% 10.07% 67.83% 67.91% 67.85% 71.58% 68.33% 63.99% 67.88% 67.55% 64.20% 70.27% 70.75% 66.53% 67.05% 63.39%*

72.44% 68.26% 65.42% 71.88% 68.25% 10.08% 68.72% 68.74% 68.86% 71.96% 68.88% 64.80% 68.59% 68.32% 65.01% 70.78% 71.49% 67.46% 67.79% 64.38%*

72.89% 68.46% 65.34% 72.45% 68.36% 10.08% 68.73% 68.77% 68.88% 72.44% 69.09% 65.13% 68.84% 68.49% 64.90% 71.29% 71.63% 67.52% 68.30% 63.39%*

73.73% 69.32% 62.01% 72.93% 69.44% 10.02% 69.48% 69.89% 69.69% 73.16% 69.63% 65.83% 69.63% 69.28% 65.91% 71.86% 72.22% 68.40% 68.89% 64.38%*

74.51% 70.51% 67.13% 74.00% 70.32% 10.07% 70.66% 70.51% 70.51% 74.01% 70.85% 66.98% 70.53% 70.03% 66.83% 72.47% 73.48% 69.30% 69.94% 67.00%

74.99% 71.36% 68.01% 74.65% 70.96% 10.04% 71.43% 71.37% 71.29% 74.68% 71.71% 67.91% 71.32% 70.89% 67.66% 73.30% 73.82% 69.99% 70.61% 67.25%

Table 7: Black-box attacks on the CIFAR-10 models. All attacks are  attacks with = 8. Similar to Madry et al. (2017), We build 7-step PGD attacks and FGSM attacks for the public adversarial trained model of MadryLab. We then use the built attacks for attacking the different models. *: Since we do not have the Madry model, we cannot evaluate it under the PGD attack with and without random initialization and therefore we use the same value that is reported by them for both.

15

Under review as a conference paper at ICLR 2019

35000 30000 25000 20000

|Sum of Activations| in Penultimate Layer
adv trained naturally trained logit squeeze + rand label smooth + rand rand

|Sum of Activations|

15000

10000

5000

0 0

200 400 600 800 #Neuron in Penultimate Layer

1000

Figure 8: For MNIST, we plot the cumulative magnitude of activations for all neurons in feature layer of a network produced by natural training, adversarial training, natural training with random noise, label smoothing ( = 0.2) with random noise, and logit squeezing ( = 0.5) with random noise. In all cases, the noise is Gaussian with  = 0.5. Interestingly, the combination of Gaussian noise and label smoothing, similar to the combination of Gaussian noise and logit squeezing, deactivates roughly 400 neurons. This is similar to adversarial training. In some sense it seems that all three methods are causing the "effective" dimensionality of the deep feature representation layer to shrink.

250000 200000 150000

|Sum of Activations| in Logit Layer
adv trained naturally trained logit squeeze + rand label smooth + rand rand

|Sum of Activations|

100000

50000

0 0

246 #Neuron in Logit Layer

8

Figure 9: For MNIST, we plot the cumulative sum of activation magnitudes for all neurons in logit layer of a network produced by natural training, adversarial training, natural training with random noise, label smoothing (LS = 0.2) with random noise, and logit squeezing ( = 0.5) with random noise. In all cases, the noise is Gaussian with  = 0.5.

16

