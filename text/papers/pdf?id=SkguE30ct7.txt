Under review as a conference paper at ICLR 2019
NEURAL MODEL-BASED REINFORCEMENT LEARNING FOR RECOMMENDATION
Anonymous authors Paper under double-blind review
ABSTRACT
There is a great interest in applying reinforcement learning (RL) to recommendation systems. However, in this setting, an online user is the environment; neither the reward function nor the environment dynamics is clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we developed a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to better long turn reward for the user and higher click rate for the system.
1 INTRODUCTION
Recommendation systems have become a crucial part of almost all online service platforms. A typical interaction between the system and its users is -- users are recommended a page of items and provide feedback; and then the system recommends a new page of items. A common way of building recommendation systems is to estimate a model which minimizes the discrepancy between the model prediction and the immediate user response according to some loss function. In other words, these models do not explicitly take into account the long term user interest. However, user's interest can evolve over time based on what she observes, and the recommender's action may significantly influence such evolution. In some sense the recommender is guiding users' interest by displaying particular items and hidding the rest. Thus, a recommendation strategy which takes user's long term interest into account is more favorable.
Reinforcement learning (RL) is a learning paradigm where a policy will be obtained to take actions in an environment so as to maximize the expected long term reward. In the recommendation system setting, the environment will correspond to the logged online user, the reward function will be the user's interest, and the policy will correspond to the recommendation policy. Although RL framework has been successfully applied to many game settings, such as Atari games (Mnih et al., 2015) and GO (Silver et al., 2016), it met a few challenges in the recommendation system setting.
First, a user's interest (reward function) driving her behavior is typically unknown, yet it is critically important for the use of reinforcement learning algorithms. Essentially, without the reward function, neither model-based nor model-free reinforcement learning algorithms can be used. Thus, in existing use of RL for recommendation systems, the reward functions are typically manually designed which may or may not reflect user's true interests (Zhao et al., 2018a; Zheng et al., 2018).
Second, model-free reinforcement learning typically requires lots of interactions with the environment in order to learn a good policy. This is impractical in the recommendation system setting. A logged online user will quickly abandon the service if the recommendation looks random and do not meet user's interests. Thus, to avoid the large sample complexity of the model-free approach, a model-based RL approach is more preferable. In a related but a different setting where one wants to train a robot policy, recent work showed that model-based reinforcement learning is much more sample efficient (Nagabandi et al., 2017; Deisenroth et al., 2015; Clavera et al., 2018). However, these approaches can not be used in the recommendation setting, since a user behavior model typically consists of sequences of discrete choices under complex session context.
1

Under review as a conference paper at ICLR 2019

To address the above challenges, we propose a novel model-based reinforcement learning framework for recommendation systems, where we first recover a user behavior model from historical data, and then optimize a policy in response to this model. Our technical innovations are:

1. We develop a generative adversarial network formulation to imitate user behavior dynamics and recover her reward function. These two components are estimated simultaneously via a joint mini-max optimization algorithm;
2. Using this model as the simulation environment, we develop a cascaded DQN algorithm to obtain a recommendation policy. The cascaded design of action-value function allows us to handle a large number of items with time complexity only linear in the number.

In our experiments with real data, we showed that this generative adversarial model has a better fit to user behavior in terms of held-out likelihood and click prediction. Since our model can take historical items clicked by the user into account, these results show the benefits of explicitly modeling the influence of history on user's interests, and imply the potential usefulness of reinforcement learning. Based on the learned user model and reward, we show that the estimated recommendation policy leads to better cumulative long term reward for the user. Furthermore, in the case of model mismatch, our model-based policy can also quickly adapt to the new dynamics with much few number of user interactions compared to model-free approaches.

2 RELATED WORK
Commonly used recommendation algorithms typically use simple user model. For instance, logistic regression based wide&deep networks (Cheng et al., 2016) basically assume an user choose each item independently; collaborative competitive filtering (Yang et al., 2011) takes into account the context of an user's choice, but assume each pageview is independent from each other. Session-based RNN (Hidasi et al., 2016) improves upon previous approaches by modeling user's history, but this model does not recover user's reward function and can not be used subsequently for reinforcement learning. Contextual bandit based approaches, such as LinUCB (Li et al., 2010), can deal with adversarial user behaviors, but the reward is used and updated in a Bayesian framework, not directly usable by a reinforcement learning framework.

Zhao et al. (2018b;a); Zheng et al. (2018) used model-free RL for recommender systems, which may require many user interactions and the reward function is manually designed. Model-based reinforcement learning has been commonly used in robotics applications, and resulted in much reduced (interaction) sample complexity to obtain good policy (Deisenroth et al., 2015; Nagabandi et al., 2017; Clavera et al., 2018). The advantage of model-based approach is that potentially large amount of off-policy data can be pooled and used to learn a good environment dynamics model, whereas model-free approach can only use expensive on-policy data for learning. However, previous model-based approach are typically designed based on physics or Gaussian processes, and not tailored for complex sequences of user behaviors.

3 SETTING AND OVERVIEW OF THE APPROACH

We assume a typical interaction setting between the recommendation system and its users be ­ in each pageview, users are recommended a page of k items and provide feedback by clicking on only one of these items; and then the system recommends a new page of k items.

Since reinforcement learning can explicitly take into account longer term rewards, it holds the promise to improve user's long term engagement with an online platform. In the reinforcement learning language, a recommendation system wants to learn a recommendation policy (s) such that the long term expected reward is maximized, i.e.



 = arg max E

tr(st, At, at) , where s0  p0, At  , st+1  P (·|st, At).

(st ,I t )

t=0

(1)

Below we will elaborate on several key aspects of the above RL framework as instantiated in recommendation system setting:

1. Environment: will correspond to a logged online user who can click on one of the k items

displayed by the recommendation system in each pageview (or interaction).

2. State st  S: will correspond to an ordered sequence of user's historical clicks.

3. Action At 

I k

:

will correspond to a subset of k items chosen by the recommendation

system from I to display to the user. The notation

I k

means all subset of size k of I.

2

Under review as a conference paper at ICLR 2019

4. State Transition P (·|st, At) : S ×

I k

 P(S): will correspond to a user behavior model

which returns the transition probability for st+1 given previous state st and displayed items

At. It is equivalent to the distribution (st, At) over user's action defined in our user

model, which will be introduced in section 4.1.

5. Reward Function r(st, At, at) : S ×

I k

× I  R: will correspond to user's reward or

satisfaction in state st and after making her choice at  At from the set displayed by the

recommendation system. Here we assume that the reward to the recommendation system

is the same as user's interests. Thus, a recommendation algorithm which optimizes its long

term reward is designed to satisfy the user in a long run. One can also include the online

platform's benefit to the reward, but in this paper we will focus on user's satisfaction.

6.

Policy (st, It)

:

S × 2I



P(

I k

):

will correspond to a recommendation algorithm

which takes user's state st and returns the probability distribution of a subset of It to be

displayed. Here we introduce the notation It  I to represent available items at time t.

Remark. We note that in the above mapping, Environment, State and State Transition are as-
sociated with the user, the Action and Policy are associated with the recommendation system, and
the Reward Function is associated with both the recommendation system and the user. Here we use the notation r(st, At, at) to emphasize the dependency of the reward on the recommendation
action, since user can only choose from the display set. However, the value of the reward is actually
determined by user's state and the clicked item once the item is in the display set. Thus, in section 4.1 when we discuss on the user model, we simply denote r(st, at) = r(st, At, at) and assume at  At is true. In fact, r(st, At, at) = r(st, at) · 1(at  At).

When both the reward function and the state transition model are given, the optimal policy  in Eq. (1) can be estimated by repeated query to the model using algorithms such as Qlearning (Watkins, 1989). Thus, our first goal is to learn user's reward function. Furthermore, since model-based reinforcement learning is more sample efficient, we will also estimate a user behavior model associate with the reward function. Given both the reward function and state transition model, we can then use an RL algorithm to obtain a recommendation policy.

More specifically, our solution consists of two major innovations. The first component is a generative adversarial network for the user behavior model and reward function. We will design a position weighted sequence model to embed the user's historical sequence of clicks into the user state st, and then use this state to define the reward function of the user and the probability of user clicking on a specific item in a pageview. Our formulation unifies the two estimation problems in a single mini-max optimization, and learn a consistent pair of user behavior model and reward function based on the sequence of user clicking history. The second component is a cascaded design of actionvalue function which allows us to learn a policy efficiently given a large action space faced by the recommendation system. In the following two sections, we will explain these two components of our framework (as shown in figure 1) in more details.

... state : user's past choices

state updated

reward

available articles

system

display set

user user's choice

Figure 1: An illustration of the interaction between user and recommendation system. Green arrows

represent the recommendation framework and orange arrows represent user's framework.

4 GENERATIVE ADVERSARIAL NETWORK FOR USER BEHAVIOR

In this section, we will propose a model for the sequence of user clicking behavior, discussion its parametrization and parameter estimation.

4.1 USER BEHAVIOR AS REWARD MAXIMIZATION
We assume that during each pageview, the user is presented with k items, out of which she clicks one. The sequence of items selected by the user before session t is an ordered list of observed features,

3

Under review as a conference paper at ICLR 2019

and we view it or embedding of it as the state of the user, i.e. st := h F1:t-1 = [f1, · · · , ft-1] ,

where f·  Rd is the feature vector of the selected items, and h(·) is an embedding function. One

can also view define M step

F1:t-1 as historical

a matrix of historical features of clicked items, features of click items as Ft-M:t-1 := [ft-M ,

and similarly · · · , ft-1].

one

can

also

Suppose in pageview t, the user is presented with a set of k items At = {a1, · · · , ak} and their associated features F t = {f1t, · · · , fkt} by the recommendation system, and a selection at  At is taken by the user according to her preference function at  (st, At)  k-1 where k-1
is the probability simplex. More specifically, we assume that user clicks items to maximize its
own interests. That is we assume that user's clicking probability is the results of the following
optimization

User Model:

(st, At) = arg max E k-1

r(st, at)

-

1  E[R()],

(2)

where R() is a concave regularization function and  controls the strength of the regularization. After clicking an item at, the user state will be updated to st+1 = h F1:t = {f1, · · · , ft-1, fatt } .

Remark. We assume the case when the user does not click any item to be a special item and consider
its reward as zero or a constant to be learned. Alternatively, it can also be included as an item of zero
feature vector, so equation Eq. (2) can generate this case with simple modification. Besides, when no item is clicked, the state of user will not be updated (i.e., st+1 = st).

Furthermore, if we allow   k-1 to be arbitrary mappings (i.e., not limited in a specific parameter space), the optimization problem in Eq. (2) has a closed form solution, where user clicks on an item according to an exponential distribution in the follow lemma (See Appendix A for a proof).

Lemma 1. Let the regularization be R() = log() and   k-1 include arbitrary mappings. Then the optimal solution  for the problem in Eq. (2) has a closed form

(st, At)i =

aj

exp(r(st, ai)) At exp(r(st,

aj

))

.

(3)

Furthermore, at each time t, the user's decision according to her optimal policy  is equivalent to

he following discrete choice model

at = arg max  r(st, a) + t, aAt
where t follows a Gumbel distribution.

(4)

This lemma makes it clear that the user essentially greedily picks an item according to its reward function (exploitation), and yet the Gumbel noise t allows the user to deviate and explore other less rewarding items. Similar model has also appeared in the econometric choice model (Manski, 1975; McFadden, 1973), but previous econometric models did not take into account diverse features and user state evolution. Furthermore, to the best of our knowledge, such models have not been considered in model-based RL. Furthermore, other regularization R() can also be used in our framework, which may induce different user behaviors. In these cases, the relations between  and r are also different, and may not appear in closed form.

4.2 MODEL PARAMETERIZATION In the following, we will first define the state embedding st = h(F1:t-1), and then define the reward function r(st, at) based on the state and the embedding of click at.

More specifically, we propose to use a simple and effective position weighting scheme for the state embedding. Let W  Rm×n be a matrix where the number of rows m correspond to the number of history steps to take into account, and each of the n columns will correspond to one set of importance
weights on positions. Then the embedding map h can be designed as

st = h(F1:t-1) := vec  Ft-m:t-1W + B



dn×1
R

,

(5)

where B  Rd×n is a bias matrix, (·) is a nonlinear activation function such as the ReLU and ELU unit applied elementwisely to its arguments, and vec[·] turns the input matrix into a long vector by concatenating the matrix columns. Alternatively, one can also use an LSTM to capture the history. However, the advantage of our position weighting parametrization is that the history embedding is obtained by a shallow network which is more efficient for forward computation and gradient backpropagation than recurrent networks.

4

Under review as a conference paper at ICLR 2019

The user clicks at corresponds to an item with feature fatt . Thus we will use fatt as the surrogate for user click and parametrize the reward function and user behavior model as

r(st, at) := r 

V

st fatt

+b

,

(s, At)  exp

r



V

st fatt

+b

(6)

where V , V  R ×(dn+d) are weight matrices, b, b  R1×(dn+d) are bias vectors and r, r  R are the final regression parameter. See Figure 2 for an illustration of the overall parametrization.

!#$( !#$% weightmatrix

)%% )%*

)%+

×    =

)(%)(*

)(+

#$%

concat



!#/

     

%'()
%'()*+ ./#
%'(+

!"#$
!"#$
 !"#$

'()

'()*+

'(+

%'.



   



/.'

Figure 2: Architecture of our GAN model with reward function and user behavior model parameterized similarly by either position weight (PW) or LSTM.

4.3 LEARNING USER BEHAVIOR MODEL In practice, both the user reward function r(st, at) and the behavior model (st, At) are unknown and need to be estimated from data. Given a sequence of T observed user clicks, {a1true, a2true, . . . , aTtrue} and the corresponding item features {f1, f2, . . . , fT }, we can estimate the user utility and behavior model by solving the following mini-max optimization

min max E
 

T

r(sttrue, at)

-

1 R() 

t=1

T
- r(sttrue, attrue)
t=1

(7)

where  denotes all parameters in the r function,   { : S ×

I k

 k-1} denotes a family of

mappings, and we use sttrue = {f1, . . . , ft-1} to emphasize that this is observed in the data. The

meaning of the optimization problem is that a good reward function should give the observed user

behavior higher reward than the user behavior model, and thus the user model can approach the real

user behavior by maximizing this reward function.

The mini-max optimization problem in Eq. (7) can be solved by iteratively updating the  and r, but the process may be unstable due to the non-convex problems. We will explain how we can make use of a special closed form to help initialize the mini-max optimization after the statement of the following lemma. See Appendix A for a proof.

Lemma 2. Consider the case when regularization is defined as R() = log() and  includes all

mappings from S ×

I k

to k-1. Then the optimization problem in Eq. (7) is equivalent to the

following maximum likelihood estimation

T
max
 t=1

exp(r(sttrue, attrue)) atAt exp(r(sttrue, at

))

.

(8)

This lemma enables us to learn a  in a more stable and efficient way by solving a single max-
imization problem. We can simply apply standard algorithms such as SGD or ADAM to find an approximate solution ^. Then we can use this ^ as the initialization for the optimization problem Eq. (7) with other R().

5 CASCADING Q-NETWORKS FOR RECOMMENDATION POLICY

Using the estimated user behavior model and the corresponding reward function as the simulation

environment, we can then use reinforcement learning to obtain a recommendation policy. Note that

the recommendation policy needs to deal with a combinatorial action space

I k

, where each action

is a set of k items chosen from a larger set I of K candidates. Two challenges associated with this

problem include the potentially high computational complexity of the combinatorial action space

and the development of a framework for estimating the long-term reward (the Q function) from a

5

Under review as a conference paper at ICLR 2019

combination of items. Our contribution is designing a novel cascade of Q-networks consistent with each other for handling the combinatorial action space. Based on this cascade of Q-networks, we can also design an algorithm to estimate them from interaction with the environment.

5.1 CASCADING Q-NETWORKS
We assume that each time when a user visits the online platform, the recommendation system need to choose a subset A of k items, i.e., A  I. We will use the Q-learning framework where an optimal action-value function Q(s, A) will be learned and satisfies

Q(st, At) = E r(st, At, at) +  max Q(st+1, A ) , at  At. A I

(9)

Once the action-value function is learned, an optimal policy for recommendation can be obtained

as

(st, It) = arg max Q(st, At),

(10)

At I t

where It  I is the set of items available at time t. Furthermore, we assume that once an item

is clicked by an user, it becomes unavailable and will be excluded from being recommended again.

The challenge is that the action space of the recommendation system contains

K k

many choices,

which can be very large even for moderate K (e.g. 1000) and k (e.g. 5). Furthermore, an item put

in different combinations can have different probability of being clicked, which is indicated by the

user model and is in line with reality. For instance, interesting items may compete with each other

for user attention. Thus, the policy in Eq. (10) will be very expensive to compute. To address this

challenge, we will design not just one but a set of k related Q-functions which will be used in a

cascading fashion for finding the maximum in Eq. (10).

Let {a1

an , a2

,re·c·o· m, amke}nd=atiaorngbmeaAxA=Q{(as1,,Aa2),.

·

· · , ak} and the optimal action The key idea of our cascading

given state Q-function

s be A = framework

comes from the following fact:

max Q(s, a1, a2, · · · , ak) = max max Q(s, a1, a2, · · · , ak) ,

a1,a2,··· ,ak

a1 a2,...,ak

(11)

which also implies that there is a cascade of mutually consistent Q1, Q2, . . . , Qk such that:

a1 = arg max Q1(s, a1) a1

a2

=

arg

max a2

Q2(s,

a1,

a2)

with with

Q1(s, a1) := max Q(s, a1, · · · , ak), a2,··· ,ak
Q2(s, a1, a2) := max Q(s, a1, · · · , ak), a3,··· ,ak

······

ak = arg max Qk(s, a1, · · · , ak-1, ak) with Qk(s, a1, · · · , ak) := Q(s, a1, · · · , ak). ak
Thus, if we have access to all these Qj functions, then we can obtain an optimal action in O(k|I|) computations by applying these function in a cascading manner. See algorithm 1 and Figure 3 for a summary. However, these cascade of Qj functions are usually not available and need to be estimated from data.

5.2 PARAMETRIZATION AND ESTIMATION OF CASCADING Q-NETWORKS
Note that the set of Qj functions need to satisfy a large set of constraints which may not be easy to strictly enforce. That is at the optimal point, the value of Qj is the same as Q for all j, i.e.,

Qj(s, a1, · · · , aj ) = Q(s, a1, · · · , ak), j = 1, . . . , k.

(12)

Instead of coding these constraints in our neural network parametrization, we will take them into account in a soft and approximate way in our model fitting process.

More specially, we will use the same embedding for the state s as in Eq. (5), and parametrize each Qj function as

Qj (s, a1:j-1, aj ; j ) = qj  Lj s , fa1 , . . . , faj-1 , faj + cj , j = 1, . . . , k

(13)

where Lj  R ×(dn+dj), cj  R , and qj  R are the set j of parameters. Now the problem left is how we can estimate these functions Qj and take into account constraints in Eq. (12).

Different from standard Q-learning, our cascading Q-learning process will then be learning k parametrized functions Qj(st, a1:j-1, aj; j) as approximations of Qj. To enforce the constraints

6

Under review as a conference paper at ICLR 2019

in Eq. (12) in a soft and approximate way, we can define the loss as

y - Qj 2, where y = r(st, At, at) + Qk(st+1, a1, · · · , ak; k), j = 1, . . . , k.

(14)

That is all Qj is fitting against the same target. Then the parameters k can be updated by performing gradient steps over the above loss.

The overall cascading Q-learning algorithm is summarized in Algorithm 2 in Appendix B, where we employ the cascading Q functions to search the optimal action efficiently (line 9). Besides, both the experience replay (Mnih et al., 2013) and -exploration techniques are applied. The system's experiences at each time-step are stored in a replay memory set M (line 11) and then a minibatch

of data will be sampled from the replay memory to update Qj (line 13 and 14). An exploration to the action space is executed with probability (line 8).

,- ,.

,2

Algorithm 1 Search using Qj Cascades

1: function ARGMAX Q(s, A, 1, · · · , k) 2: Let A be empty.

3: I = A \ s

remove clicked items.

4: for j = 1 to k do

5: aj = arg max Qj (s, a1:j-1, aj ; j )
aj I\A
6: Update A = A  {aj} 7: end for 8: return A = (a1, · · · , ak) 9: end function

6 EXPERIMENTS

Argmax

Argmax

... Argmax

!"#(%, '#; )# !"*(%, '#, '*; )* !"0(%, '#, ... , '0 1# , '0; )0

3

,- ,.

,2

Figure 3: Cascading Q-networks

We conduct three sets of experiments to evaluate our user model (called GAN model) and the resulting RL recommendation policy. (1) The first set of experiments in section 6.2 show that GAN is a better predictive user model. By taking both the context where the item is displayed in and the evolution of user's interest into account, GAN outperforms other baseline models in terms of both likelihood and precision. (2) In section 6.3 we compare the recommendation policies developed from different user models. It is revealed from experiments that recommendation policies generated from GAN model can not only help users achieve higher long term reward but also help the system achieve higher click rate. (3) In section 6.4 we test how recommendation policies can benefit from GAN model to better capture users' interest with fewer interactions. A comparison is made among RL policy with two versions of our GAN model (GAN(PW) and GAN(LSTM)), RL policy without GAN model and multi-arm bandit algorithm which does not make an assumption on user behavior.

6.1 DATASET AND FEATURE DESCRIPTION
We will use two real world datasets for our experiments: (1) MovieLens public dataset and (2) an online news article recommendation dataset from Ant Financial. The datasets are either preprocessed or collected to follow our recommendation setting in Figure 1. More details can be found in Appendix C.

6.2 PREDICTIVE PERFORMANCE OF USER MODEL

A series of models are designed to predict user behavior by utilizing the interaction data with users,

among which logistic regression (LR) and collaborative filtering (CF) type models are popular. Be-

sides, collaborative competitive filtering (CCF) model significantly outperforms standard CF ap-

proaches as reported by Yang et al. (2011). Thus, we choose the most widely used LR model and

the advanced CF model (CCF) as our baseline: (1) Logistic regression (LR). The click-through

rate(CTR)

likelihood

function

can

be

defined

using

logistic

function

as

pi

=

1+exp

1 (-r(fi

))

,

where

r(·) is a utility/reward function learned by optimizing iC log 1+exp(-r(fi)) + iC log 1+

exp(r(fi)) where C is the set of clicked items; (2) Collaborative competitive filtering (CCF). CCF predicts users' action by comparing the items in the current display set F . More precisely, the

likelihood formulation of CCF is pi = exp(r(fi))/ fjF exp(r(fj)).

7

Under review as a conference paper at ICLR 2019

GAN, CCF, and LR formulate the probability distribution of user behavior in different ways. For both datasets, we randomly split the users into train(50%), validation(12.5%) and test(37.5%) subsets. The predictive performances are only accessed on users in test set and all results shown in this section are averaged over 5 random splits. We consider the following metrics to evaluate the predictive performance of each user model and report results in table 1.

· NLL is the negative log-likelihood loss value averaged over all users in the test set,

1 N

N1 u=1 Tu

Tu t=1

-

log

ptu,i

,

where

i

is

the

item

clicked

by

the

user

observed

in

the

data,

Tu is number of events associated with user u and ptu,i is the probability that u clicks item i at

time t. Different user models formulate the probability ptu,i in different ways.

· Prec@k is top-k precision. When a user clicks on one of the displayed items, we count it as an

event. For each event, the user model can rank the displayed items by likelihood. Prec@k is the

proportion of top-k likelihood items that are truly clicked by the user. An average is taken over

all events for each user first before average over all users.

Table 1: Predictive performance evaluation: loss and precision of LR, CCF and GAN models assessed on a test set of 25,000 users for the news article dataset and 500 users for MovieLens dataset.

Model LR CCF
GAN(PW) GAN(lstm)

(1) Ant Financial news dataset

NLL

prec@1

prec@2

2.017(±0.007) 0.375(±0.002) 0.609(±0.001)

1.457(±0.003) 0.377(±0.001) 0.611(±0.001)

1.368(±0.001) 0.419(±0.001) 0.658(±0.001)

1.365(±0.001) 0.421(±0.002) 0.659(±0.002)

(2) MovieLens dataset

NLL

prec@1

prec@2

0.743(±0.001) 0.615(±0.007) 0.738(±0.012)

1.164(±0.009) 0.657(±0.008) 0.752(±0.011)

1.141(±0.013) 0.666(±0.007) 0.754(±0.013)

1.124(±0.021) 0.674(±0.005) 0.763(±0.012)

As shown in table 1, our GAN user model significantly performs better than baseline models for click predictions. It is noted that the loss of LR is much lower than other models in the MovieLens dataset. This could be caused by the feature reduction process, since we learn the wide&deep layers by minizing the loss function of LR. The learned reduced features tend to achieve a lower loss for LR model. Besides, by comparing the results of GAN(PW) and GAN(lstm), we find that the position weighted(PW) scheme performs almost the same as the LSTM version, but PW is more favorable due to its simple architecture and efficiency. Thus in the rest of the paper, we will use GAN(PW) for experiments and simply referring it as GAN.

Another interesting comparison is shown in Figure 4 and more similar figures are provided in Appendix D.1. The blue curve is the trajectory of a user's actual choices of movies over time. The orange curves are simulated trajectories predicted by GAN and CCF. More specifically, at each time t we collect the item with highest likelihood estimated by each model to form a predicted trajectory. To capture the evolution of user's interest, we perform k-means clustering on movie feature vectors to separate the movies into 80 categories. Each data point (t, c) on the curve represents time step t and the category c of the item clicked by the user. The upper sub-figure reveals that GAN performs much better as time goes by. By contrast, the items predicted by CCF as shown in the lower subfigure are concentrated consistently on several categories. This indicates an obvious drawback of static model - it fails to capture the evolution of user's interests.

GANprediction
Figure 4: Comparison of the true trajectory(blue) of user's choices, the simulated trajectory predicted by GAN model (orange curve in upper sub-figure) and the simulated trajectory predicted by CCF (orange curve in the lower sub-figure) for the same user. Y axis represents 80 categories of movies.

6.3 RECOMMENDATION POLICIES GENERATED FROM USER MODELS With a learned user model, one can immediately derive a greedy policy to recommend k items with highest estimated likelihood. In this section, we abuse the notation LR, CCF and GAN to represent their greedy recommendation policy in correspondence. Apart from that, we include the RL policy based on GAN model in the experiments. We employ a deep Q network to parametrize the RL policy, so we call it GAN-DQN.
8

Under review as a conference paper at ICLR 2019

Since we cannot make real interactions with online users at this moment, we use real data from

the news article online platform to fit a user model, which we then use as a test environment. To

make the experimental results trustful and solid, we fit test model on a randomly sampled set of

100 users and keep this set isolated. It is noteworthy that the 20 dimensional feature vectors are

user-item cross features, so this user model is learned over user-specific features. Now we regard

these 100 users as test users and the fitted model as test model. Four recommendation policies(LR,

CCF, GAN, GAN-DQN) will then be evaluated among test users. All of these policies are learned

from a set of 2,500 users without overlapping with the test users. The performance are evaluated

by two metrics: (1) Cumulative reward: For each recommendation action, we can observe users' behavior and compute the her reward r(st, at) using the test model. It should be noted that we

never use the reward of test users when we train the policy. The policy is only learned on training

set. The numbers shown in table 2 is the cumulative reward averaged over time horizon first and

then

averaged

over

all

users.

It

can

be

formulated

as

1 100

100 1 u=1 T

T t=1

rut ,

where

rut

is

the

reward

received by at time t. (2) CTR(click through rate): it is the ratio of the number of clicks the system

receives and the number of steps it is run. This information is not only accessible in our experiment

but also in practice. The value displayed in table 2 is also averaged over 100 test users.

Three sets of experiments are conducted and the results are summarized in table 2. Since users' behavior is not deterministic, each policy is evaluated repeatedly for 50 times on test users. There are several observations: (1) Greedy policy built on GAN model is significantly better than the policies built on other models. (2) RL policy learned from GAN is better than the greedy policy. (3) Although DQN is trained to optimize the cumulative reward, it can be seen from the results that by optimizing users' reward, the recommendation system also achieves a higher click through rate.

Table 2: Results of recommendation performances based on different user models.

model LR CCF GAN GAN-CDQN

k=2

reward

CTR

11.93(±0.27) 0.38(±0.008)

19.13(±0.24) 0.58(±0.007)

21.57(±0.40) 0.65(±0.012)

22.76(±0.21) 0.69(±0.007)

k=3

reward

CTR

14.55(±0.24) 0.46(±0.007)

21.83(±0.25) 0.67(±0.008)

23.67(±0.44) 0.72(±0.013)

24.05(±0.26) 0.74(±0.008)

k=5

reward

CTR

15.40(±0.23) 0.49(±0.008)

22.78(±0.23) 0.70(±0.007)

24.35(±0.42) 0.74(±0.012)

25.36(±0.30) 0.77(±0.009)

Since table 2 only shows average value taken over all test users, we compare the policies in user level in figure 5. GAN-DQN policy contributes higher averaged cumulative reward for most users. A similar figure which compares the click rate is attached in Appendix D.

cumulative rewardcumulative rewardcumulative reward cumulative rewardcumulative rewardcumulative reward

11223500505500

k = 2, GAN-DQN vs LR
GAN-DQN LR
20 40 60 80 100

13122005055500

k = 2, GAN-DQN vs CCF
GAN-DQN CCF
20 40 60 80 100

1122330550505050

k = 3, GAN-DQN vs LR
20 40 60 80

100 3322110505055050

k = 3, GAN-DQN vs CCF
20 40 60 80 100

2112330500555050

k = 5, GAN-DQN vs LR
20 40100 users60 80

100 332211505050050

k = 5, GAN-DQN vs CCF
20 40100 users60 80 100

Figure 5: Cumulative rewards among 100 users under the recommendation policies based on different user models. The experiments are repeated for 50 times and standard deviation is plotted as the shaded area.
6.4 USER MODEL ASSISTED ONLINE ADAPTIVE POLICIES
Former results in section 6.2 and 6.3 have demonstrated that GAN is a better user model and its recommendation policy can achieve higher CTR compared to other user models. A question may arise as what if the recommender does not make any assumption on users and why the user model would be useful. In this section, we demonstrate by experiments how in general the user model helps recommendation polices capture users' interest faster and better. The RL policy assisted by GAN user model is compared with other policies that are learned from and adapted to online users: (1) CDQN with GAN: cascaded deep Q-learning where the function Q interacts with the learned

9

Under review as a conference paper at ICLR 2019

GAN user model before it is adapted to online users. (2) CDQN without GAN: cascaded deep Qlearning without pre-trained by the GAN model. It interacts with and adapts to online users directly. (3) LinUCB: a classic contextual bandit algorithm which assumes adversarial user behavior. We choose its stronger version - LinUCB with hybrid linear models (Li et al., 2010) - to compare with.

The experiment setting is similar to section 6.3. All policies are evaluated on a separated set of 100 users associated with a test model. Three sets of results corresponding to different sizes of display set are plotted in figure 6. It shows how the CTR increases as each policy interacts with and adapts to 100 users over time. In fact, the performances of users' cumulative reward according to different policies are also similar, for which we attach the figure in Appendix D.3.

click rate click rate click rate

k=2
0.7

0.6

0.5

0.4 DQN with GAN

0.3

DQN without GAN LinUCB(hybrid)

0 num50b0er o1f0i0n0tera1c5t0i0ons2000

0.80 k = 3

0.75

0.70

0.65

0.60

0.55

0.50 0.45 0.40

DQN with GAN DQN without GAN LinUCB(hybrid)

0.350 num50b0er o1f0i0n0tera1c5t0i0ons2000

k=5
0.80

0.75

0.70

0.65

0.60

0.55

0.50 0.45

DQN with GAN DQN without GAN LinUCB(hybrid)

0.400 num50b0er o1f0i0n0tera1c5t0i0ons2000

Figure 6: Comparison of the averaged click rate among 100 users under different adaptive recommendation policies. X-axis represents how many times the recommender interacts with online users. Here the recommender interact with 100 users each time, so each interaction represents 100 online data points. Y -axis is the click rate. Each point (x, y) in this figure means a click rate y is achieved after x many times of interactions with the users.

It can be easily seen from figure 6 that the CDQN policy pre-trained over a GAN user model can achieve a better initial status even when it is applied to a new set of users. It can then adapt to online users fast without wasting too many trails or possibly losing the users. Without the user model, CDQN can also adapt to the users during its interaction with them. However, it takes around 1,000 iterations (i.e., 100,000 interactive data points) to achieve similar performance as the CDQN policy assisted by GAN user model. LinUCB(hybrid) is also capturing users' interest during its interaction with users, but similarly, it takes too many interactions. In Appendix D.3, another figure is attached to compare the cumulative reward received by the user instead of click rate. Generally speaking, GAN user model provides a dynamic environment for RL policies to interact with to help the recommendation policy achieve a more satisfying status before applying to online users.

7 CONCLUSION AND FUTURE WORK
In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we developed a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. Although we have try very hard to make our experiments realistic, and the experiments show clear benefits of our current method in offline and realistic simulation setting, an even stronger results could be obtained via online experimentation and A/B testing. Such experiments will be left as our future study.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, DLRS 2016. ACM, 2016. doi: 10.1145/2988450.2988454.
Ignasi Clavera, Anusha Nagabandi, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt: Meta-learning for model-based control. CoRR, 2018.
M. P. Deisenroth, D. Fox, and C. E. Rasmussen. Gaussian processes for data-efficient learning in robotics and control. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015. ISSN 0162-8828. doi: 10.1109/TPAMI.2013.218.
Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. In ICLR, 2016.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pp. 661­670. ACM, 2010.
Charles F. Manski. Maximum score estimation of the stochastic utility model of choice. Journal of Econometrics, pp. 205 ­ 228, 1975. ISSN 0304-4076.
D. McFadden. Conditional logit analysis of qualitative choice behaviour. In P. Zarembka (ed.), Frontiers in Econometrics, pp. 105­142. Academic Press New York, 1973.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. CoRR, abs/1708.02596, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, King's College, Oxford, May 1989. (To be reprinted by MIT Press.).
Shuang-Hong Yang, Bo Long, Alexander J Smola, Hongyuan Zha, and Zhaohui Zheng. Collaborative competitive filtering: learning recommender using context of user choice. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pp. 295­304. ACM, 2011.
Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. Deep reinforcement learning for page-wise recommendations. 2018a.
Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Dawei Yin, Yihong Zhao, and Jiliang Tang. Deep reinforcement learning for list-wise recommendations. CoRR, 2018b.
Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. Drn: A deep reinforcement learning framework for news recommendation. 2018.
11

Under review as a conference paper at ICLR 2019

A LEMMA

A.1 PROOF OF LEMMA 1

Lemma 1. Let the regularization be R() = log() and   k-1 include arbitrary mappings. Then the optimal solution  for the problem in Eq. (2) has a closed form

(st, At)i =

aj

exp(r(st, ai)) At exp(r(st,

aj

))

.

(3)

Furthermore, at each time t, the user's decision according to her optimal policy  is equivalent to

he following discrete choice model

at = arg max  r(st, a) + t, aAt

(4)

where t follows a Gumbel distribution.

Proof. First, recall the problem defined in Eq. (2):

(st, At) = arg max E
k-1

r(st, at)

1 -  E[R()].

Denote t = (st, At). Since  can be an arbitrary mapping (i.e.,  is not limited in a specific parameter space), t can be an arbitrary vector in k-1. Recall the notation At = {a1, · · · , ak}. Then the expectation taken over random variable at  At can be written as

E

r(st, at)

1 -  E[R()] =

k

ti r(st ,

ai)

-

1 

k

it log ti.

i=1 i=1

(15)

By simple computation, the optimal vector t  k-1 which maximizes Eq. (15) is

ti =

exp(r(st, ai)) j=1k exp(r(st, aj

))

,

(16)

which is equivalent to Eq. (2). Next, we show the equivalence of Eq. (16) to the discrete choice model interpreted by Eq. (4).

The cumulative distribution function for the Gumbel distribution is F (; ) = P[ ] = e-e-
and the probability density is f () = e-e- e-. Using the definition of the Gumbel distribution, the probability of the event [at = ai] where at is defined in Eq. (4) is

Pi := P at = ai = P r(st, ai) + i r(st, aj) + j, for all i = j

= P j i + r(st, ai) - r(st, aj), for all i = j .

Suppose we know the random variable i. Then we can compute the choice probability Pi conditioned on this information. Let Bij = i + r(st, ai) - r(st, aj) and Pi|E be the conditional probability; then we have

Pi|i = P[j
i=j

Bij ] =

e-e-Bij .

i=j

In fact, we only know the density of i. Hence, using the Bayes theorem, we can express Pi as



Pi =

Pi|i f (i)di =

e-e-Bij f (i)di

-

- i=j

k



= e-e-Bij ee-i e-e-i e-i di =

- j=1

-

k
e-e-Bij
j=1

e-i di

12

Under review as a conference paper at ICLR 2019

Now, let us look at the product itself.

kk

e-e-Bij = exp -

e-Bij

j=1

j=1

k

= exp - e-i

e-(r(st,ai)-r(st,aj ))

j=1

Hence where Q =



Pi =

exp(-e-i Q)e-i di

-

k j=1

e-(r(st,ai)-r(st,aj ))

=

Z/ exp(r(st,

ai)).

Next, we make a change of variable y

=

e-i .

The Jacobian of the inverse transform is J

=

di dy

=

-

1 y

.

Since

y

>

0,

the

absolute

of

Jacobian

is

|J |

=

1 y

.

Therefore,



Pi = exp(-Qy)y|J|dy = exp(-Qy)dy
00

11 = Q = exp(-r(st, ai)) j exp(r(st, aj))

=

exp(r(st, ai)

k j=1

exp(r(st,

aj

))

.

A.2 PROOF OF LEMMA 2

Lemma 2. Consider the case when regularization is defined as R() = log() and  includes all

mappings from S ×

I k

to k-1. Then the optimization problem in Eq. (7) is equivalent to the

following maximum likelihood estimation

T
max
 t=1

exp(r(sttrue, attrue)) atAt exp(r(sttrue, at

))

.

(8)

Proof. This lemma is a straight forward result of lemma 1. First, recall the problem defined in Eq. (7):

min max E
 

T

r (sttrue ,

at)

-

1 R()


t=1

T
- r(sttrue, attrue)
t=1

We make a assumption that there is no repeated pair (sttrue, at) in Eq. (7). This is a very soft assumption because sttrue is updated overtime, and at is in fact representing its feature vector fatt ,

which is in space Rd. With this assumption, we can let  map each pair (sttrue, at) to the optimal

vector

t

which

maximize

r (sttrue ,

at)

-

1 

R(t

)

since

there

is

no

repeated

pair.

Using

Eq.

(16),

we have

max E


T

r (sttrue ,

at)

-

1 R()


T
= max


E

r (sttrue ,

at)

-

1 

R()

t=1 t=1

T
=
t=1

k

tir(st,

ai)

-

1 

k

ti log ti

i=1 i=1

T1 = log

t=1

k
exp(r(sttrue, ai)) .
i=1

Eq. (7) can then be written as

T1

min log





kT
exp(r(sttrue, ai)) - r(sttrue, attrue),

t=1 i=1

t=1

which is the negative log-likelihood function and is equivalent to lemma 2.

13

Under review as a conference paper at ICLR 2019
B ALOGRITHM BOX
Algorithm 2 Cascading deep Q-learning (CDQN) with Experience Replay 1: Initialize replay memory M to capacity N 2: Initialize parameter j of Qj with random weights for each 1  j  k 3: for iteration i = 1 to L do 4: Sample a batch of users U from training set 5: Initialize the states s0 to a zero vector for each u  U 6: for t = 1 to T do 7: for each user u  U simultaneously do 8: With probability select a random subset At of size k 9: Otherwise, At = ARGMAX Q(stu, It, 1, · · · , k) 10: Recommend At to user u, observe user action at  (st, At) and update user state
st+1 11: Add tuple st, At, r(st, at), st+1 to M 12: end for 13: Sample random minibatch B iid. M 14: For each j, update j by SGD over the loss y - Qj(st, A1t :j; j) 2 for B 15: end for 16: end for 17: return 1, · · · , k
C DATASET DESCRIPTION
(1) MovieLens public dataset contains large amounts of movie ratings collected from their website. We randomly sample 1,000 active users from this dataset. On average, each of these active users rated more than 500 movies (including short films), so we assume they rated almost every movie that they watched and thus equate their rating behavior with watching behavior. MovieLens dataset is the most suitable public dataset for our experiments, but it is still not perfect. In fact, none of the public datasets provides the context in which a user's choice is made. Thus, we simulate this missing information in a reasonable way. For each movie watched(rated) on the date d, we collect a list of movies released within a month before that day d. On average, movies run for about four weeks in theater. Even though we don't know the actual context of user's choice, at least the user decided to watch the rated movie instead of other movies in theater. Besides, we control the maximal size of each displayed set by 40. Features: In MovieLens dataset, only titles and IDs of the movies are given, so we collect detailed movie information from Internet Movie Database(IMDB). Categorical features as encoded as sparse vectors and descriptive features are encoded as dense vectors. The combination of such two types of vectors produces 722 dimensional raw feature vectors. To further reduce dimensionality, we use logistic regression to fit a wide&deep networks (Cheng et al., 2016) and use the learned input and hidden layers to reduce the feature to 10 dimension.
(2) An online news article recommendation dataset from Ant Financial is anonymously collected from Ant Financial news article online platform. It consists of 50,000 users' clicks and impression logs for one month, involving dozens of thousands of news. It is a time-stamped dataset which contains user features, news article features and the context where the user clicks the articles. The size of the display set is not fixed, since a user can browse the news article platform as she likes. On average a display set contains 5 new articles, but it actually various from 2 to 10. Features: The news article raw features are approximately of dimension 100 million because it summarizes the key words in the article. Apparently it is too expensive to use these raw features in practice. The features we use in the experiments are 20 dimensional dense vector embedding produced from the raw feature by wide&deep networks. The reduced 20 dimensional features are widely used in this online platform and revealed to be effective in practice.
14

Under review as a conference paper at ICLR 2019

D MORE FIGURES FOR EXPERIMENTAL RESULTS
D.1 FIGURES FOR SECTION 6.2
An interesting comparison is shown in Figure 4 and more similar figures are provided here. The blue curve is the trajectory of a user's actual choices of movies over time. The orange curves are simulated trajectories predicted by GAN and CCF, respectively. Similar to what we conclude in section 6.2, these figures reveal the good performances of GAN user model in terms of capturing the evolution of users' interest.

GANprediction

GANprediction

Figure 7: Two more examples: comparison of the true trajectory(blue) of user's choices, the simulated trajectory predicted by GAN model (orange curve in upper sub-figure) and the simulated trajectory predicted by CCF (orange curve in the lower sub-figure) for the same user. Y -axis represents 80 categories of movies.

D.2 FIGURES FOR SECTION 6.3
We demonstrate the policy performance in user level in figure 5 by comparing the cumulative reward. Here we attach the figure which compares the click rate. In each sub-figure, red curve represents GAN-DQN policy and blue curve represents the other. GAN-DQN policy contributes higher averaged click rate for most users.

click rate

click rate

0000100.......26024080

k = 2, GAN-DQN vs LR
GAN-DQN LR
20 40 60 80 100

click rate

0001000.......26080240

k = 2, GAN-DQN vs CCF
GAN-DQN CCF
20 40 60 80 100

0001000.......22840060

k = 3, GAN-DQN vs LR
20 40 60 80

click rate

100 0000010.......24268000

k = 3, GAN-DQN vs CCF
20 40 60 80 100

0000001.......20862040

k = 5, GAN-DQN vs LR
20 40 100 users 60 80

1.0 0.8 0.6 0.4 0.2 100 0.00

click rate

k = 5, GAN-DQN vs CCF
20 40 100 users 60 80 100

click rate

Figure 8: Comparison of click rates among 100 users under the recommendation policies based on different user models. In each figure, red curve represents GAN-DQN policy and blue curve represents the other. The experiments are repeated for 50 times and standard deviation is plotted as the shaded area. This figure is similar to figure 5, except that it plots the value of click rates instead of user's cumulative rewards.

D.3 FIGURES FOR SECTION 6.4
This figure shows three sets of results corresponding to different sizes of display set. It reveals how users' cumulative reward(averaged over 100 users) increases as each policy interacts with and adapts to 100 users over time. It can be easily that the CDQN policy pre-trained over a GAN user model can adapt to online users much faster then other model-free policies and can reduce the risk of losing
15

Under review as a conference paper at ICLR 2019

the user at the beginning. The experiment setting is similar to section 6.3. All policies are evaluated on a separated set of 100 users associated with a test model. We need to emphasize that the GAN model which assists the CDQN policy is learned from a training set of users without overlapping test users. It is different from the test model which fits the 100 test users.

cumulative reward cumulative reward cumulative reward

24 k = 2

22

20

18

16

14 12 10

DQN with GAN DQN without GAN LinUCB(hybrid)

80 num5b0e0r o1f0i0n0tera15c0t0ion2s000

26 k = 3

24

22

20

18

16 DQN with GAN

14

DQN without GAN LinUCB(hybrid)

120 num5b0e0r o1f0i0n0tera15c0t0ion2s000

28 k = 5

26

24

22

20

18 16 14

DQN with GAN DQN without GAN LinUCB(hybrid)

120 num5b0e0r o1f0i0n0tera15c0t0ion2s000

Figure 9: Comparison of the averaged cumulative reward among 100 users under different adaptive recommendation policies. X-axis represents how many times the recommender interacts with online users. Here the recommender interact with 100 users each time, so in fact each interaction represents 100 online data points. Y -axis is the click rate. Each point (x, y) in this figure means a click rate y is achieved after x many times of interactions with the users.

16

