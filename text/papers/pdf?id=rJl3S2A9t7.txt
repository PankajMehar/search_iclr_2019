Under review as a conference paper at ICLR 2019
POLICY OPTIMIZATION VIA STOCHASTIC RECURSIVE GRADIENT ALGORITHM
Anonymous authors Paper under double-blind review
ABSTRACT
This paper proposes the StochAstic Recursive graAdient Policy Optimization (SARAPO) algorithm based on the SARAH algorithm, which is a novel variance reduction algorithm in machine learning. We provide first some theoretical analysis using stochastic differential equations. Furthermore, rich experiments further exemplify the advantage of SARAPO over existing policy gradient methods such as SVRPO and TRPO.
1 INTRODUCTION
Reinforcement Learning (RL) (Sutton & Barto, 1998) is a dynamic learning approach to interact with the environment and make the best actions. It achieves remarkable performance in several tasks including control and Atari games. Among all existing RL algorithms, policy gradient (Sutton et al., 1999) is the most fundamental method for RL, and we could use the REINFORCE(Williams, 1992), among many baselines, to estimate the policy gradient.
However, policy gradient suffers from sample inefficiency and high variance during the training phase. To remedy this problem, trust region and line search optimization method (Nocedal & Wright, 2006) [Numerical Optimization] are introduced to policy gradient algorithm (Kakade, 2001). Trust region policy optimization (TRPO) was first proposed by (Schulman et al., 2015). It merges the trust region method and the natural gradient theory (Amari, 1998) into the policy gradient framework. TRPO achieves a more robust performance on continuous control tasks. The basic idea is to make the policy move toward the direction that improves mean episode rewards under the constraint that it cannot be too far away from the old policy distribution. Compared to traditional gradient descent which corresponds to the L2 constraint, constraints that uses KL divergence has been widely adopted as an alternative choice. The method of KL constraint was firstly proposed by Amari (1998). Then in 2001, Kakade merges the natural gradient into the policy gradient (Kakade, 2001) (See Martens (2014) for further extensions). Natural gradient solves the problem of zero gradients in plateau areas, in which KL constraint is highly related with the ellipsoid constraint that is associate with the population Fisher information matrix. Furthermore, TRPO also uses line search to avoid reaching a point outside feasible set/constraint.
SVRG (Johnson & Zhang, 2013) is a kind of stochastic variance reduced method. It overcomes the fallback of SGD of large variance and potentially accelerates the RL training. Previous works used SVRG to accelerate policy gradient (SVRPG) and its TRPO variants (SVRPO) (Xu et al., 2017; Papini et al., 2018). The variance reduced version of policy gradient allows a multi-step optimization for each batch of data, while the vanilla policy gradient optimizes the policy once each iteration. In theory, SVRG accommodates with larger stepsize for each iteration and observably outperforms traditional PG and TRPO, respectively. Furthermore, the introduction of stochastic variance reduction not only improves sample efficiency but also stabilizes the training process by reducing the variance.
In this paper, we focus on an alternative variance reduction method called SARAH, recently proposed by Nguyen et al. (2017) whose convergence rate matches that of SVRG in the convex case and has the stable convergence property that SVRG does not possess. We propose a new algorithm named SARAPO which hybrids the SARAH method with TRPO, and analyze the dynamics via its approximating stochastic differential equation (SDE). In experiments, we show the performance and
1

Under review as a conference paper at ICLR 2019

sample efficiency of SARAPO, SVRPO, and TRPO and find that SARAPO outperforms TRPO and matches the performance of SVRPO in most of the tasks.
The rest of this paper is organized as the following: we introduce some background, propose and analyze our algorithm in Section 2. In Section 3, we compare SARAH and SVRG from a dynamical system theoretical viewpoint. We present the numerical results of experiments in Section 4 and conclude our papers in Section 5.

2 METHODOLOGY

The reinforcement learning task can be considered as solving a discrete time Markov Decision Process (MDP) M = {S, A, P, R, , }. Where S is the set of states, A is the set of actions, R : S × A  R is the reward function,  is the discount factor and  the initial state distribution. The policy gradient method operate by directly maximizing the expected sum of rewarads:



L(w) = Ew

trt(st, at)

t=0

with respect to the parameters w of the stochastic policy w(a|s). The gradient of the objective function wL is given by:



wL(w) = Ew

w log (at|st)Q(st, at)

t=0

(1)

The expectatioin is calculated by Monte-Carlo simulation by sampling n timesteps {st, at, rt}tn=1 of policy w.

2.1 TRUST REGION POLICY GRADIENT
As the policy gradient is highly unstable and sample inefficient, TRPO introduces the trust region method to policy gradient to limit how far the policy go away in each updates. Instead of the l2 norm in trust region method, TRPO takes KL as the measure of distance. After turning the constraint problem into the penalized problem, TRPO optimizes the surrogate loss in every iterations:

max
w

Lwold (w) = Ewold

w(a|s) Awold (s, a) wold (a|s)

- DKL(wold , w)

(2)

Use first order approximation to the first term of the objective and second order approximation to

the second term of the objective. In every iteration, TRPO takes an update of:

vt = wL wt+1 = wt + tF -1vt

(3) (4)

Where F -1 is the Fisher Information Matrix (FIM). The initial step size  is calculated by limiting the kl distance within the range :

t = 2/(vtF -1vt) Then a line search is needed to decide the final step size.

(5)

2.2 STOCHASTIC GRADIENT RECURSIVE POLICY OPTIMIZATION
SARAH and SVRG are all stochastic variance reduction gradient algorithms in optimization theory. They reduce the variance during optimization process and reaches a faster convergence rate. Indstead of calculating the full gradient L. SVRG calculate a mini batch of size m in each inner loop, it gets an estimate of the full gradient with less complexity:
vt = v~ + wLm - wLm
where v~ and w~ are saved gradients and saved parameters in the previous outer loop. And Lm is the objective function estimated using m samples.

2

Under review as a conference paper at ICLR 2019

In Sarah, it estimate each gradients using the gradient estimated one step before and the difference between the gradients of current parameter and the previous parameter on the mini batch.
vt = vt-1 + wLm(wt) - wLm(wt-1)
The details of calculating the estimated gradients are listed in Algorithm 1

Algorithm 1 SARAPO

Initialization:

Initialize w0 = w1 to be a randomized parameter.

for t = 1 to T do

if t%m == 0 then

Set w~ = wt Run policy wt for N timesteps. Store a batch of trajectories D. Calculate the baseline gradients:

1 v~0 = |D|

Lx(wt)

xD

else Draw a mini-batch Dt of size M from D Calculate the estimated gradient v^(w):

1

v~t

=

v~t-1

+

|Dt|

(Lx(wt)
xDt

-

Lx(wt-1))

end if
Update policy parameter with Algorithm 2
end for Output: w  w~T

After calculating the estimate of v. We do a step of the TRPO update, where the Fisher information matrix is estimated by the second derivative of the KL distance. Notice that the second derivative of KL can approximate the FIM only at the point of the old distribution. So we use the KL distance between the current parameters and the parameters one step before to calculate H^ . Futher, the FIM is estimated with only a mini batch of data. In the last step of TRPO, we search for  that can improve the value of the objective function.

Algorithm 2 Inner loop details

estimate the Fisher Information Matrix H^

=



2 wn  wm

K

L(wt

(·

sn), w(·

sn))|wt

Update policy parameter with mini-batch Ij:

wt+1  wt + tH^ -1(wt) · v^t.

Initialize t according to equation 5 linesearch j, j/2, j/4, · · · ,for improving Lw~(w)

3 THEORY OF DYNAMICAL SYSTEMS
The core inner-loop update rule of Stochastic Recursive Gradient Method (SARAH) (Nguyen et al., 2017) is, assuming the minibatch size is 1,
vt+1 =vt + Lit+1 (wt) - Lit+1 (wt-1), wt+1 =wt - vt+1. Here we provide a simple alternative viewpoint of (random) dynamical systems. As we could see from Algorithm 1, we applied a scaled-form of SARAH in combination with Trust Region Policy
3

Under review as a conference paper at ICLR 2019

Optimization by the inverse Fisher Information matrix F -1. Seeing this, the inner loop update has

the essential form of

vt+1 =vt + Lit+1 (wt) - Lit+1 (wt-1), wt+1 =wt - F -1vt+1.

(6)

For simplicity of analysis, we consider the quadratic approximation approach. Let the quadratic

objective

function

L(w)

:=

1 n

n i=1

Li(w),

with

1

Li(w)

=

w 2

Hiw + gi w.

We sketch the analysis using ODE and SDE, in order to better understand the dynamics.

3.1 ODE ANALYSIS

By rescaling the time t = -1s according to the stepsize , we define a new process V (s) = v -1s and X(s) = w -1s .

Then the newly defined process is converted from equation 6 and has the form V (s + ) =V (s) + Lis+ (X(s)) - Lis+ (X(s - )) =V (s) + HX(s) - HX(s - ) + es+,
where es+ is a martingale difference noise term of O(1), and X(s + ) =X(s) - F -1V (s + ).
Substituting X(s) - X(s - ) with -F -1V (s), we derive the update rule for V (s) -1(V (s + ) - V (s)) = -HF -1V (s) + es+.
From the standard weak convergence theory, one easily conclude the following:

Theorem 1 When   0+, the scaled SARAH update rule (6) can be approximated by the following

ODE system

d V (s) = -HF -1V (s), ds d X(s) = F -1V (s). ds

(7)

Note in (7), the dynamics of V (s) forms an autonomous ODE, which is independent of the dynamics of X(s).

Left multiplying both sides of the first line of equation 7 by F -1, one obtains
d (F -1V (s)) = -F -1H(F -1V (s)). ds Noticing V (0) = HX0, we can easily verify that the V (s) can be solved as V (s) = F (F -1V (s)) = F exp(-s · F -1H)F -1HX0 = H exp(-s · F -1H)X0.

(8)

By combining V (s)'s solution with the update rule for X(s), we have

d ds

X (s)

=

-

exp(-s

·

F

-1H

)F

-1 H X0 ,

and has the expressed solution

X(s) = X0 -

s
exp(-r · F -1H)dr F -1HX0 = X0 -
0

s 0

 k=0

(-r)k k!

(F -1H)k+1drX0

= X0 -



(-r)k+1 (k + 1)!

(F

-1H )k+1

·

X0

=

X0

-

(I

-

exp(-s

·

F

-1 H ))X0 .

k=0

As a result, we reach the following solution to (7):

X(s) = exp(-s · F -1H)X0.

(9)

From (8) and (9), both the vectors X(s) and V (s) exponentially decays with rate matrix F -1H

which is consistent with the theory of natural gradient ascent (Kakade, 2001).

4

Under review as a conference paper at ICLR 2019

3.2 SDE ANALYSIS

The ODE analysis simply neglects the effect of noise. Here we turns to develop a stochastic differential equation approximation tool to exploit the effect of noise. Denote Hi = H + Ei, H being the Hessian for L(w). Further assume that the noise term Ei is diag(1, . . . , d) where i are i.i.d. standard normal, for simplicity.

Define the coordinate-wise product of two vectors (x1, . . . , xd) · (y1, . . . , yd) = (x1y1, . . . , xdyd) . Let W (s) = (W1(s), . . . , Wd(s)) be a d-dimensional standard Wiener processes. Then under standard assumptions, the inner loop has an update rule for V (s) which can be written as
V (s + ) - V (s) = - HF -1V (s) -   · (F -1V (s))
= - HF -1V (s) - 1/2 (W (s + ) - W (s)) · (F -1V (s)).

As   0+, it becomes a SDE system

d V (s) = -HF -1V (s) - 1/2

d W (s)

· (F -1V (s)),

ds ds

which can be simplified as

dV (s) = -HF -1V (s)ds - 1/2(F -1V (s)) · dW (s).

(10)

From the above analysis, one can verify that the approximating SDE in (10) introduces a small (size of O(1/2)) noise deviation from its ODE curve, and hence enjoys similar property as the natural gradient descent update. In the case where H is convex, the dynamics validates the property of innerloop convergence in (Nguyen et al., 2017) [Theorem 1a & 1b], and hence the multi-loop geometric convergence [Theorem 2]. The per-step cost, however, turns to be evaluations of two individual gradients instead of the whole-batched gradients.

Remark 1 One may also conduct a similar analysis for SVRG and conclude the same approximating ODE for scaled SVRG. The approximating SDE, nevertheless, shall differ by an extra noise term that is proportional to the distance between the current iterate and the control point. This well-explains the inner-loop convergence and stability properties that SARAH enjoy upon SVRG.

4 EXPERIMENTS
We designed experiments to test the performance of SARAPO and SVRPO. Our implementation is based on the modular rl. We used the commonly used Mujoco testing environment on several continuous control tasks: Swimmer, Half-Cheetah, Walker and Hopper. We compared our implementation of SARAPO with SVRPO and the original TRPO. The result is shown in Figure 1
We fix part of our parameters according to Xu et al. (2017) and Schulman et al. (2015). We set the Number of steps per Iteration to be 50000 and Discount to be 0.995. We use a Gaussian Policy with a diagonal covariance matrix, whose mean is parameterized by a multi-layer perceptron (MLP). For the Mujoco tasks, we use two hidden layers of size (64, 64) with tanh activation function. We fine tuned other parameters as follows, minibatch size is tuned between 1000 or 5000, inner loop iterations is between 50 or 20, inner loop max kl between 0.005 and 0.01, and the cg damping factor. We list the detailed parameters of each experiments in the appendix.
As the result varies a lot accross different runs, we chose five different random seed and plot our result as the average of the five runs. As the outer loop iterations are fixed, the x-axis is proportional to the number of samples we have sampled, the curve corresponds to the sample efficiency.
In Figure 1 We compared the learning curve of three algorithms mentioned above: SARAPO, SVRPO and TRPO. We see that in Half-Cheetah environment, SARAPO observably performs better than other two algorithms and achieves high score in 500 epochs. In Walker and Hopper, the three algorithms have minor difference. SARAPO rises faster in Swimmer but SVRPO reaches a better score.
To get a clearer view of our result, we list two tables.

5

Under review as a conference paper at ICLR 2019

Figure 1: A comparison of variance reduced policy optimization and traditional TRPO. The result is the mean of five runs on different random seeds.

Table 4 presents the number of iterations needed to cross the threshold which is 90 percent of the best score. In two of the tasks, SARAPO reaches the threshold the fastest and in the other two tasks, SVRPO outperforms the other two.

Tasks
Swimmer Half-Cheetch
Walker Hopper

Threshold
302 4561 4669 3198

SARAPO
52 216 343 74

SVRPO
72 251 274 65

TRPO
71 402 330 78

Table 1: Number of Iteration Below the Threshold

Tasks
Swimmer Half-Cheetch
Walker Hopper

SARAPO
343.0 6845.1 5232.9 3578.1

SVRPO
355.7 5291.4 5529.2 3625.8

TRPO
336.5 5080.9 5265.2 3692.8

Table 2: Performance of Policy

Table 2 shows the final performance of policy which is the average of best reward over five different seeds.
We did more experiments on the phenomenons during the training process. We record three statistics in each mini iterations: the norm of estimatedronghe gradients, the improvement of the policy loss, and the kl distance between the old policy and the current policy. Figure /refstats mix the result of all three statistcs of three different tasks. The first row shows the training process of Swimmer, the second row is the Hopper task and the third row is Half-Cheetah. To see the details, we choose an interval that the policy is still improving and limit our x-axis to be in that range and only one random seed is chosen to plot the data.
In the first column, we plot the l2 norm of the estimated gradient of SVRG and SARAH.
Then we calculate the loss function both before an update and after an update. For SARAPO and SVRPO, we sum the improvement in each mini iterations to get the improvement of the outer it-

6

Under review as a conference paper at ICLR 2019
Figure 2: An illustration of the policy loss improvement, the estimated gradients norm and the kl during training. The first row is the results on the Swimmer task. The second row is the results on the Hopper task. The third row is the results on the HalfCheetah. The pictures are zoomed in to show more details and the x-axis is normalized to be the number of outer loop iterations. erations. We compare the result with that in TRPO. From the second colum of Figure ??, we see that both SARAH and SVRG reach a better optima than SGD. Generally, a larger improvement corresponds to high mean episode rewards. For the random seed chosen to plot Figure ??, SVRPO outperforms SARAPO in the first few iterations in Swimmer, and SARAPO outperforms SVRPO at last in Half-Cheetah. The third column shows the kl divergence between the old policy and the updated policy. We see that when  is set to be 0.01, the kl seldom exceed 0.01 in TRPO. SARAPO stays under 0.02 and SVRPO stays under 0.03. The variance reduced gradient methods are able to find a better solution slightly outside the kl range.
Figure 3: Comparison of Stochastic Recursive Proximal Policy Gradient to PPO 7

Under review as a conference paper at ICLR 2019
Finally, we do an experiment on Swimmer of the comparison between SARAPPO, SVRPPO and PPO. Both SARAPPO and SVRPPO reaches a better reward than PPO.
5 CONCLUSION
In this work, we propose the Stochastic Recursive Gradient Policy Optimization (SARAPO) algorithm based on the novel SARAH algorithm for variance reduction (Nguyen et al., 2017). We provide from both theory and practical experiments on its advantage over existing policy gradient methods. We hope this work can inspire future works and ultimately obtain higher performance in the reinforcement learning tasks.
REFERENCES
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural Comput., 10(2):251­276, February 1998. ISSN 0899-7667. doi: 10.1162/089976698300017746. URL http://dx. doi.org/10.1162/089976698300017746.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pp. 315­323, 2013.
Sham Kakade. A natural policy gradient. In Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, NIPS'01, pp. 1531­1538, Cambridge, MA, USA, 2001. MIT Press. URL http://dl.acm.org/citation.cfm? id=2980539.2980738.
James Martens. New perspectives on the natural gradient method. CoRR, abs/1412.1193, 2014.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Taka´c. SARAH: A novel method for machine learning problems using stochastic recursive gradient. In International Conference on Machine Learning, pp. 2613­2621, 2017.
Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA, second edition, 2006.
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochastic variance-reduced policy gradient. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4026­4035, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/papini18a.html.
John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust region policy optimization. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML'15, pp. 1889­1897. JMLR.org, 2015. URL http://dl.acm.org/citation.cfm?id=3045118.3045319.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'99, pp. 1057­1063, Cambridge, MA, USA, 1999. MIT Press. URL http://dl.acm.org/citation.cfm?id= 3009657.3009806.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
Tianbing Xu, Qiang Liu, and Jian Peng. Stochastic variance reduction for policy gradient estimation. CoRR, abs/1710.06034, 2017.
8

Under review as a conference paper at ICLR 2019

A SVRGPPO

Algorithm 3 SVRG Proximal Policy Gradient

Input:

Initialization:

Initialize w~0 randomly

for = 1 to L do

Generate N transitions D by executing the current policy w~ .

Lw~

=

1 N

t

[

w w~

(at |st ) (at |st )

A^t

- KL[w~

(·|st), w(·|st)]]

Compute whole gradient g~ = L(w~ )

w0 = w~ for j = 1 to J do

Draw a minibatch Dj from D

Compute gradient g = g~ + Lj(wj) - Lj(w~ )

wj+1 = Adam(wj, g)

end for

Compute

d

=

1 N

t KL[w~ (·|st), wJ (·|st)]

if d < dtarg/1.5,   /2

if d > dtarg × 1.5,    × 2

w~ +1 = wJ

Update value estimation using D

end for

Output: w  wL

B HYPERPARAMETERS

Tasks
Swimmer Half-Cheetch
Walker Hopper

Minibatch Size
5000 5000 1000 1000

Inner Loop Iteration
20 50 20 20

Max KL
0.005 0.005 0.005 0.005

Cg Damping Factor
0.1 0.1 0.1 0.1

Table 3: Best Hyper-Parameter of SARAPO

Tasks
Swimmer Half-Cheetch
Walker Hopper

Minibatch Size
1000 5000 5000 5000

Inner Loop Iteration
20 50 50 20

Max KL
0.01 0.01 0.01 0.01

Cg Damping Factor
0.1 0.1 0.1 0.1

Table 4: Best Hyper-Parameter of SVRPO

C THE MEAN EPISODE REWARD OF THE EXPERIMENTS IN FIGURE ??

9

Under review as a conference paper at ICLR 2019
Figure 4: The Mean Episode Reward of the experiments in Figure ?? 10

