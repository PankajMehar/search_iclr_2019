Under review as a conference paper at ICLR 2019
CO-MANIFOLD LEARNING WITH MISSING DATA
Anonymous authors Paper under double-blind review
ABSTRACT
Representation learning is typically applied to only one mode of a data matrix, either its rows or columns. Yet in many applications, there is an underlying geometry to both the rows and the columns. We propose utilizing this coupled structure to perform co-manifold learning: uncovering the underlying geometry of both the rows and the columns of a given matrix, where we focus on a missing data setting. Our unsupervised approach consists of three components. We first solve a family of optimization problems to estimate a complete matrix at multiple scales of smoothness. We then use this collection of smooth matrix estimates to compute pairwise distances on the rows and columns based on a new multi-scale metric that implicitly introduces a coupling between the rows and the columns. Finally, we construct row and column representations from these multi-scale metrics. We demonstrate that our approach outperforms competing methods in both data visualization and clustering.
1 INTRODUCTION
Dimension reduction plays a key role in exploratory data analysis, data visualization, clustering and classification. Techniques range from the classical PCA and nonlinear manifold learning to deep autoencoders (Tenenbaum et al., 2000; Roweis & Saul, 2000; Belkin & Niyogi, 2003; Coifman & Lafon, 2006; Vincent et al., 2008; Rifai et al., 2011; Kingma & Welling, 2014). These techniques focus on only one mode of the data, often the observations (columns) which are are measurements in a high-dimensional feature space (rows), and exploit correlations among the features to reduce the dimension of the feature vectors and obtain the underlying low-dimensional geometry of the observations. Yet for many data matrices, for example in gene expression studies, recommendation systems, and word-document analysis, correlations exist among both observations and features. In these cases, we seek a method that can exploit the correlations among both the rows and columns of a data matrix to better learn lower-dimensional representations of both. Biclustering methods, which extract distinct biclusters along both rows and columns, give a partial solution to performing simultaneous dimension reduction on the rows and columns of a data matrix. Such methods, however, can break up a smooth geometry into artificial clusters. A more general viewpoint to consider is that data matrices possess geometric relationships between their rows (features) and columns (observations) such that both modes lie on low-dimensional manifolds. Furthermore, the relationships between the rows may be informed by the relationships between the columns, and vice versa. Several recent papers (Gavish & Coifman, 2012; Ankenman, 2014; Mishne et al., 2016; Shahid et al., 2016; Mishne et al., 2017; Yair et al., 2017) exploit this coupled relationship to co-organize matrices and infer underlying row and column embeddings.
Further complicating the story is that such matrices may suffer from missing values, due to measurement corruptions and limitations. Missing values can sabotage efforts to learn the low dimensional manifold underlying the data. Specifically, kernel-based methods rely on calculating a similarity matrix between observations, whose eigendecomposition yields a new embedding of the data. As the number of missing entries grows, the distances between points are increasingly distorted, resulting in poor representation of the data in the low-dimensional space (Gilbert & Sonthalia, 2018). Matrix completion algorithms assume the data is low-rank and fill in the missing values by fitting a global linear subspace to the data. Yet, this may fail when the data lies on a nonlinear manifold.
Manifold learning in the missing data scenario has been addressed by a few recent papers. Nonlinear Principle Component Analysis (NLPCA) (Scholz et al., 2005) uses an autoencoder neural network, where the middle layer serves to learn a low-dimensional embedding of the data, and the
1

Under review as a conference paper at ICLR 2019

-
multiscale metric

co-cluster

rows

columns

embedding

Figure 1: The three components of our approach: 1) smooth estimates of a matrix with missing entries at multiple scales via co-clustering, 2) a multi-scale metric using the smooth estimates across all scales, yielding an affinity kernel between rows/columns, and 3) nonlinear embeddings of the rows and columns. The multiscale metric between two columns (red and orange) is a weighted Euclidean distance between those columns at multiple scales, given by solving the co-clustering for increasing values of the cost parameters r and c.

trained autoencoder is used to fill in missing values. Missing Data Recovery through Unsupervised Regression (Carreira-Perpin & Lu, 2011) first fills in the missing data with linear matrix completion methods, then calculates a non-linear embedding of the data and incorporates this embedding in an optimization problem to fill in the missing values. Recently Gilbert & Sonthalia (2018) proposed MR-MISSING which first calculates an initial distance matrix using only non-missing entries and then uses the increase-only-metric-repair (IOMR) method to fix the distance matrix so that it is a metric from which they calculate an embedding. None of these methods consider the co-manifold setting, where the coupled structure of the rows and the columns can be used to fill in the data, and to calculate an embedding.
In this paper, we introduce a new method for performing joint dimension reduction on the rows and columns of a data matrix, which we term co-manifold learning, in the missing data setting. We build on two recent lines of work on co-organizing the rows and columns of a data matrix (Gavish & Coifman, 2012; Mishne et al., 2016; 2017) and convex optimization methods for performing coclustering (Chi et al., 2017; 2018). The former provide a flexible framework for jointly organizing rows and columns but lacks algorithmic convergence guarantees. The latter provides convergence guarantees but does not take full advantage of the multiple scales of the data revealed in the solution. Instead of inferring biclusters at a single scale, we use a multi-scale optimization framework to fill in the data, imposing smoothness on both the rows and the columns at fine to coarse scales. The scale of the solution is encoded in a pair of joint cost parameters along the rows and columns. The solutions to the optimization for each such pair yields a smooth estimate of the data along both the rows and columns, whose values are used to fill in the missing values of the given matrix. We define a new multi-scale metric based on the filled-in matrix across all scales, which we use to calculate nonlinear embeddings of the rows and columns. Thus our approach yields three results: a collection of smoothed estimates of the matrix, pairwise distances on the rows and columns that better estimate the geometry of the complete data matrix, and corresponding nonlinear embeddings (see Figure 1). We will demonstrate in experimental results that our method reveals meaningful representations in coupled datasets with missing entries, whereas other methods are capable of revealing a meaningful representation only along one of the modes.
The paper is organized as follows. We present the optimization framework in Section 2, the new multi-scale metric for co-manifold learning in Section 3 and experimental results in Section 4.
2 CO-CLUSTERING AN INCOMPLETE DATA MATRIX
We seek a collection of complete matrix approximations of a partially observed data matrix X  Rm×n that have been smoothed along their row and columns to varying degrees. This collection
2

Under review as a conference paper at ICLR 2019

will serve in computing row and column multi-scale distances to better estimate the row and column pairwise distance of the complete data matrix. Let [m] denote the set of indices {1, . . . , m}, and let   [m] × [n] be a subset of the indices that correspond to observed entries of X, and let P denote the projection operator of m × n matrices onto an index set , i.e. [P(X)]ij is xij if (i, j)   and is 0 otherwise. We seek a minimizer U(r, c) of the following function.

f (U; r, c)

=

1 2

P(X) - P(U)

2 F

+

r Jr (U)

+

cJc(U).

(1)

The quadratic term quantifies how well U approximates X on the observed entries, while the two
roughness penalties, Jr(U) and Jc(U), incentivize smoothness across the rows and columns of U. The nonnegative parameters r and c tune the tradeoff between how well U agrees with X over  and how smooth U is with respect to its rows and columns. By tuning r and c, we obtain estimates of X at varying scales of row and column smoothness.

In this paper, we use roughness penalties of the following forms

Jr(U) =

( Ui· - Uj· 2) and Jc(U) =

( U·i - U·j 2),

(i,j)Er

(i,j)Ec

where Ui· (U·i) denotes the ith row (column) of the matrix U. The index sets Er and Ec denote the edge sets of row and column graphs that encode a preliminary data-driven assessment of the similarities between rows and columns of the data matrix. The function , which maps [0, ) into [0, ), will be explained shortly. The convergence properties of our co-clustering procedure will
rely on the following two assumptions.

Assumption 2.1 The row and column graphs Er and Ec are connected, i.e. the row graph is connected if for any pair of rows, indexed by i and j with i = j, there exists a sequence of indices i  k  · · ·  l  j such that (i, k), . . . , (l, j)  Er. A column graph is connected under analogous conditions.

Assumption 2.2 The function  : [0, )  [0, ) is (i) concave and continuously differentiable on (0, ), (ii) vanishes at the origin, i.e. (0) = 0, (iii) is increasing on [0, ), and (iv) has finite
directional derivative at the origin.

Variations on the optimization problem of minimizing (1) have been previously proposed in the literature. When there is no data missing, i.e.  = [m] × [n] and  is a linear mapping, minimizing the objective in (1) produces a convex biclustering problem (Chi et al., 2017). Additionally, if either r or c is zero, then we obtain convex clustering (Pelckmans et al., 2005; Hocking et al., 2011; Lindsten et al., 2011; Chi & Lange, 2015). If we take  to be a nonlinear concave function, problem (1) reduces to an instance of concave penalized regression-based clustering (Pan et al., 2013; Marchetti & Zhou, 2014; Wu et al., 2016).

Replacing Jr(U) and Jc(U) by quadratic row and column Laplacian penalties

1 Jr(U) = 2

Ui· - Uj·

2 2

and

1 Jc(U) = 2

U·i - U·j

2 2

,

(i,j)Er

(i,j)Ec

gives a version of matrix completion on graphs (Kalofolias et al., 2014; Rao et al., 2015). Shahid et al. (2016) also use row and column Laplacian penalties to perform joint linear dimension reduction on the rows and columns of the data matrix. Our work generalizes both Shahid et al. (2016) and Chi et al. (2017) in that we seek the flexibility of performing non-linear dimension reduction on the rows and columns of the data matrix and seek more general manifold organization than co-clustered structure.

2.1 CO-CLUSTERING ALGORITHM
We now introduce a majorization-minimization (MM) algorithm (Sun et al., 2017) for solving the minimization in (1). The basic strategy behind an MM algorithm is to convert a hard optimization problem into a sequence of simpler ones. The MM principle requires majorizing the objective function f (U) by a surrogate function g(U | U~ ) anchored at U~ . Majorization is a combination of

3

Under review as a conference paper at ICLR 2019

Algorithm 1 CO-CLUSTER-MISSING(P(X), r, c)

1: Initialize U0, w~r,ij , and w~c,ij 2: repeat 3: X~  P(X) + Pc (Ut)
4: {Ut+1, nr, nc}  CONVEX-BICLUSTER

X~ , r, c, {w~r,ij }, {w~c,ij }

5: w~r,ij   ( Ut+1,i· - Ut+1,j· 2) for all (i, j)  Er 6: w~c,ij   ( Ut+1,·i - Ut+1,·j 2) for all (i, j)  Ec 7: until convergence
8: Return U(r, c) = Ut, X~ , nr, nc

the tangency condition g(U | U~ ) = f (U~ ) and the domination condition g(U | U~ )  f (U) for all U  Rm×n. The associated MM algorithm is defined by the iterates Ut+1 = arg min g(U | Ut).
U
It is straightforward to verify that the MM iterates generate a descent algorithm driving the objective
function downhill, i.e. that f (Ut+1)  f (Ut) for all t.

The following function

g(U | U~ )

=

1 2

X~ - U

2 F

+

r

w~r,ij Ui· - Uj· 2 + c

w~c,ij U·i - U·j 2 + 

(i,j)Er

(i,j)Ec

majorizes our objective function (1) at U~ , where  is a constant that does not depend on U and w~r,ij and w~c,ij are weights that depend on U~ , i.e.

w~r,ij =  ( U~ i· - U~ j· 2) and w~c,ij =  ( U~ ·i - U~ ·j 2).

(2)

We give a detailed derivation of this majorization in Appendix A.

Minimizing g(U | U~ ) is equivalent to minimizing the objective function of the convex biclustering
problem for which efficient algorithms have been introduced (Chi et al., 2017). Thus, in the t + 1th
iteration, our MM algorithm solves a convex biclustering problem where the missing values in X have been replaced with the values of U~ = Ut and the weights w~r,ij and w~c,ij have been computed based on U~ = Ut according to (2).

Algorithm 1 summarizes our MM algorithm, CO-CLUSTER-MISSING, which returns a smooth output matrix U(r, c), a filled-in matrix X~ = P(X) + Pc (U(r, c)) as well as nr and nc, which are respectively the number of distinct rows and distinct columns in U(r, c). The CO-CLUSTERMISSING algorithm has the following convergence guarantee whose proof is in Appendix B.

Proposition 1 Under Assumption 2.1 and Assumption 2.2, the sequence Ut generated by Algorithm 1 has at least one limit point, and all limit points are stationary points of (1).

In the rest of this paper, we use the following function  which satisfies Assumption 2.2

1 (z) =

z1

d ,

2 0 +

where is a small positive number, e.g. 10-12. We briefly explain the rationale in our choice.By the monotone convergence theorem, as tends to zero, (z) converges to the mapping z  z.

Thus, ( Ui· - Uj· 2) approximates a snowflake metric d(Ui·, Uj·) = Ui· - Uj· 2. When the approximate snowflake metric is employed in the penalty term, small differences between rows

and columns are penalized significantly more than larger differences resulting in more aggressive

smoothing of small noisy variations and less smoothing of more significant systematic variations.

Note that the weights are continuously updated throughout the optimization as opposed to the fixed

weights in (Chi et al., 2017). This introduces a notion of the scale of the solution into the weights.

2.2 CO-CLUSTERING AT MULTIPLE SCALES
Initializing Algorithm 1 is very important as the objective function in (1) is not convex. The matrix U(0) is initialized to be the mean of all non-missing values. The connectivity graphs Er and Ec

4

Under review as a conference paper at ICLR 2019
Algorithm 2 Co-manifold learning on an Incomplete Data Matrix 1: Initialize Er, Ec 2: Set d(X·i, X·j) = 0 and d(X·i, X·j) = 0 3: Set nr = m, nc = n, k = k0, and l = l0 4: while nr > 1 do 5: while nc > 1 do 6: U(l,k), X~ (l,k), nr, nc  CO-CLUSTER-MISSING P(X), r = 2l, c = 2k 7: Update row distances: d (Xi·, Xj·) += d X~ (i·l,k), X~ j(l·,k) 8: Update column distances: d (X·i, X·j ) += d X~ (·il,k), X~ (·jl,k) 9: k  k + 1 10: end while 11: l  l + 1 12: end while 13: Calculate affinities Ar(Xi·, Xj·) and Ac(X·i, X·j) 14: Calculate embeddings r, c
are initialized at the beginning using k-nearest-neighbor graphs, and remain fixed throughout all considered scales. If we observed the complete matrix, employing a sparse Gaussian kernel is a natural way to quantify the local similarity between pairs of rows and pairs of columns. The challenge is that we do not have the complete data matrix X but only the partially observed one P(X). Therefore, we rely only on the observed values to calculate the k-nearest-neighbor graph, using the distance used by Ram et al. (2013) in an image inpainting problem.
To obtain a collection of estimates at multiple scales, we need to solve the optimization problem for pairs of r, c. We start with small values of r = 2l0 and c = 2k0 , where l0, k0 < 0. We calculate the co-clustering (Algorithm 1) and obtain the smooth estimate U(l0,k0) = U(2l0 , 2k0 ), the filled-in data matrix X~ (l0,k0), and nr and nc which are the number of distinct row and column clusters, respectively, identified at that scale. Keeping r fixed, we keep increasing c by power of 2 and biclustering the data until the algorithm converges to one cluster along the columns. We then increase r by power of 2 and reset c = 2k0 . We repeat this procedure at increasing scales of r = 2l, c = 2k, until nr = nc = 1, indicating we have converged to a single global bicluster. Thus we traverse a solution surface at logarithmic scale (Chi & Steinerberger, 2018). This yields a collection of filled-in matrices at all scales X~ (l,k) .
l,k
3 CO-MANIFOLD LEARNING
Kernel-based manifold learning relies on constructing a "good" similarity measure between points, and a dimension reduction method based on this similarity. The eigenvectors of these kernels is typically used as the new low-dimensional coordinates for the data. Here we leverage having calculated an estimate of the filled-in matrix at multiple scales X~ (l,k) , to define a new metric between
l,k
rows and columns. This metric will encompass all bi-scales as defined by joint pairs of optimization cost parameters r, c. Given a new metric we employ diffusion maps to obtain a new embedding of the rows and columns. Note that other methods can be used for embedding based on our new metric. The full algorithm is given in Algorithm 2.
3.1 MULTI-SCALE METRIC
We define a new metric to estimate the geometry both locally and globally of the complete data matrix. For a given pair r, c, we calculate the Euclidean distance between rows for the filled-in matrix at that joint scale, weighted by the cost parameters:
d X~ i(·l,k), X~ (jl·,k) = (rc) X~ (i·l,k) - X~ j(l·,k) 2
5

Under review as a conference paper at ICLR 2019

where X~ (l,k) = P(X) + Pc (U(l,k)). Having solved for multiple paris from the solution surface, we sum over all the distances to obtain a multi-scale distance on the data rows:

d(Xi·, Xj·) =

d X~ (i·l,k), X~ (jl·,k) .

l,k

An analogous multi-scale distance is computed for pairs of columns.

This metric takes advantage of solving the optimization for multiple pairs of cost parameters and filling in the missing values with increasingly smooth estimates. Note that if there are no missing values, this metric is just the Euclidean pairwise distance scaled by a scalar, so that we recover the embedding of the complete matrix. In our simulations, we set  = -1/2 to favor local over global structure. As opposed to the partition-tree based metric of Mishne et al. (2017), this metric takes into account all joint scales of the data as the matrix U is smoothed across rows and columns simultaneously, thus fully taking advantage of the coupling between both modes.

3.2 DIFFUSION MAPS
Having calculated a multi-scale metric on the rows and columns throughout the joint optimization procedure, we can now construct a pair of low-dimensional embeddings based on these distances. Specifically we use diffusion maps (Coifman & Lafon, 2006), but any dimension reduction technique relying on the construction of a distance kernel could be used instead. We briefly review the construction of the diffusion maps for the columns (observations) of a matrix but the same can be applied to the rows (features). Given a distance between two columns of the matrix d(X·i, X·j), we construct an affinity kernel on the columns. We choose an exponential function, but other kernels can be considered depending on the application:
A[i, j] = exp{-d2(X·i, X·j)/2},
where  is a scale parameter. The exponential function enhances locality, as pairs of samples whose distance exceed  have negligible affinity. One possible choice for  is to be the median of distances within the data.
We derive a row-stochastic matrix on the columns by normalizing the rows of A: P = D-1A,
where D is a diagonal matrix whose elements are given by D[i, i] = j A[i, j]. The eigendecomposition of P yields a sequence of positive decreasing eigenvalues: 1 = 0  1  ..., and right eigenvectors { } . Retaining only the first d eigenvalues and eigenvectors, the mapping  embeds the columns into the Euclidean space Rd:
 : X·i  11(i), 22(i), ..., dd(i) T.
The embedding integrates the local connections found in the data into a global representation, which enables visualization of the data, organizes the data into meaningful clusters, and identifies outliers and singular samples. This embedding is also equipped with a noise-robust distance, the diffusion distance. For more details on diffusion maps, see (Coifman & Lafon, 2006).

4 NUMERICAL EXPERIMENTS

We run experiments on three datasets, and evaluate results both qualitatively and quantitatively:

· linkage A synthetic dataset with a one-dimensional manifold along the rows and a two-

dimensional manifold along the columns. Let {zi}iN=11  R3 be points along a helix and let {yj}jN=21  R3 be a two dimensional surface. We analyze the matrix of Euclidean distances between the two spatially distant sets of points to reveal the underlying geometry of both

rows and columns,

X[i, j] = zi - yj 2.

(3)

Other functions of the distance can also be used such as the elastic or Coulomb potential

operator (Coifman & Gavish, 2011). Missing values correspond to having access to only

some of the distances between pairs of points across the two sets. Note that this is unlike

MDS as we do not have pairwise distances between all datapoints, but rather distances

between two sets of points with different geometries.

6

Under review as a conference paper at ICLR 2019

linkage

NLPCA

DM Co-manifold

linkage2
lung500
Figure 2: Comparing row and column embeddings of NLPCA, DM, Ours, for three datasets with 50% missing entries. For each dataset, top / bottom row is embedding of rows / columns of X.
· linkage2 A synthetic dataset with a clustered structure along the rows and a twodimensional manifold along the columns. Let {xi}iN=11  R3 be composed of points in 3 Gaussian clouds in 3D and let {yj}Nj=21  R3 be a two dimensional surface as before.
· lung500 A real-world dataset composed of 56 lung cancer patients and their gene expression (Lee et al., 2010). We selected the 500 genes with the greatest variance from the original collection of 12,625 genes. Subjects belong to one of four subgroups; they are either normal subjects (Normal) or have been diagnosed with one of three types of cancers: pulmonary carcinoid tumors (Carcinoid), colon metastases (Colon), and small cell carcinoma (Small Cell).
The rows and columns of the data matrix are randomly permuted so their natural order does not play a role in inferring the geometry. In Figure 2, we compare our embeddings to both NLPCA with missing data completion (Scholz et al., 2005) and Diffusion maps (DM) (Coifman & Lafon, 2006) on the missing data, where both methods are applied to each mode separately, while our method takes into account the coupled geometry. Comparing to Diffusion maps demonstrates how missing values corrupt the embedding. In all examples 50% of the entries are missing. For each of the three methods we display the embedding for both the rows (top) and the columns (bottom), Both NLPCA and DM reveal the underlying 2D surface structure on the rows in only one of the linkage datasets, and err greatly on the other. DM correctly infers a 1D path for the linkage dataset but it is increasingly noisy. For NLPCA the 1D embedding is not as smooth and clean the embedding
7

Under review as a conference paper at ICLR 2019

11

0.8 0.8

0.6 0.6

0.4 0.4

0.2 0.2

0 10 20 30 40 50 60 70 80
percentage of missing values

0 10 20 30 40 50 60 70 80
percentage of missing values

Figure 3: Comparing k-means clustering applied to embedding of data using ours (blue), diffusion maps of missing data matrix (red), and NLPCA (yellow) for increasing percentages of missing values. We calculate the adjusted Rand Index compared to the ground-truth labels of (left) the 4 cancer types for the lung500 dataset, and (right) 3 Gaussian clusters of the linkage2 dataset

inferred by the co-manifold approach. Our method reveals the 2D surface in both cases. For the lung500 data, NLPCA and DM embed the cancer samples such that the normal subjects (yellow) are close to the Colon type (cyan), whereas our method separates the normal subjects from the cancer types. This is due to taking into account the coupled structure of the genes and the samples. All three methods reveal a smooth manifold structure to the genes, which is different than the assumed clustered structure a biclustering method would infer. For plots presenting the datasets and filled-in values at multiple scales see Appendix C.
Manifold learning is not used only for data visualization but also for calculating new data representations that can then be used for signal processing and machine learning tasks. The left panel of Figure 3 compares clustering the embedding of the cancer patients in lung500 by each method for increasing percentage of missing values in the data, where we averaged over 30 realizations of missing entries. We use the Adjusted Rand Index (ARI) (Hubert & Arabie, 1985), to measure the similarity between the k-means clustering of the embedding and the ground-truth labels. Our embedding (blue plot) gives the best clustering result and its performance is only slightly degraded by increasing the percentage of missing values, as opposed to Diffusion maps (red plot). This demonstrates that the metric we calculate is a good estimate of the metric of the complete data matrix. NLPCA (yellow plot) performs worst.
The right panel of Figure 3 compares clustering the embedding of the three Gaussian clusters in linkage2 for increasing percentage of missing values in the data, where we averaged over 30 realizations of missing entries. Our embedding (blue plot) gives the best clustering result and its performance is unaffected by increasing the percentage of missing values, as opposed to Diffusion maps (red plot) which is greatly degraded by the missing values. NLPCA (yellow plot) does not perform as well as our approach, with performance decreasing as the percentage of missing values increases.
5 CONCLUSIONS
In this paper we presented a new method for learning nonlinear manifold representations of both the rows and columns of a matrix with missing data. We proposed a new optimization problem to obtain a smooth estimate of the missing data matrix, and solved this problem for different values of the cost parameters, which encode the smoothness scale of the estimate along the rows and columns. We leverage calculating these multi-scale estimates into a new metric that aims to capture the geometry of the complete data matrix. This metric is then used in a kernel-based manifold learning technique to obtain new representations of both the rows and the columns. In future work we will investigate additional metrics in a general co-manifold setting and relate them to optimal transport problem and Earth Mover's Distance (Coifman & Leeb, 2013).

8

Under review as a conference paper at ICLR 2019
REFERENCES
Jerrod I. Ankenman. Geometry and Analysis of Dual Networks on Questionnaires. PhD thesis, Yale University, 2014.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373­1396, 2003.
Miguel A. Carreira-Perpin and Zhengdong Lu. Manifold learning and missing data recovery through unsupervised regression. In Data Mining (ICDM), 2011 IEEE 11th International Conference on, pp. 1014­1019, 2011.
Eric C. Chi and Kenneth Lange. Splitting methods for Convex Clustering. Journal of Computational and Graphical Statistics, 24(4):994­1013, 2015.
Eric C. Chi and Stefan Steinerberger. Recovering trees with convex clustering. ArXiv e-prints, 2018.
Eric C. Chi, Genevera I. Allen, and Richard G. Baraniuk. Convex Biclustering. Biometrics, 73(1): 10­19, 2017.
Eric C. Chi, Brian R. Gaines, Will Wei Sun, Hua Zhou, and Jian Yang. Provable convex co-clustering of tensors. arXiv:1803.06518 [stat.ME], 2018.
Ronald R. Coifman and Matan Gavish. Harmonic analysis of digital data bases. In Wavelets and Multiscale analysis, pp. 161­197. Springer, 2011.
Ronald R. Coifman and Ste´phane Lafon. Diffusion Maps. Applied and Computational Harmonic Analysis, 21(1):5­30, 2006.
Ronald R. Coifman and William E. Leeb. Earth mover's distance and equivalent metrics for spaces with hierarchical partition trees. Technical report, Yale University, 2013. Technical report YALEU/DCS/TR1482.
Matan Gavish and Ronald R. Coifman. Sampling, denoising and compression of matrices by coherent matrix organization. Applied and Computational Harmonic Analysis, 33(3):354 ­ 369, 2012.
Anna C. Gilbert and Rishi Sonthalia. Unrolling swiss cheese: Metric repair on manifolds with holes. arXiv preprint arXiv:1807.07610, 2018.
Toby Hocking, Jean-Philippe Vert, Francis Bach, and Armand Joulin. Clusterpath: An algorithm for clustering using convex fusion penalties. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 745­752, June 2011.
Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classification, 2(1):193­218, 1985.
Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, and Pierre Vandergheynst. Matrix completion on graphs. arXiv:1408.1717 [cs.LG], 2014.
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of the International Conference on Learning Representations (ICLR), 2014.
Mihee Lee, Haipeng Shen, Jianhua Z. Huang, and J.S. Marron. Biclustering via sparse singular value decomposition. Biometrics, 66(4):1087­1095, 2010.
Fredrik Lindsten, Henrik Ohlsson, and Lennart Ljung. Just relax and come clustering! A convexification of k-means clustering. Technical report, Linko¨pings Universitet, 2011.
Yuliya Marchetti and Qing Zhou. Solution path clustering with adaptive concave penalty. Electronic Journal of Statistics, 8(1):1569­1603, 2014.
Robert R. Meyer. Sufficient conditions for the convergence of monotonic mathematical programming algorithms. Journal of Computer and System Sciences, 12(1):108­121, 1976.
9

Under review as a conference paper at ICLR 2019
Gal Mishne, Ronen Talmon, Ron Meir, Jackie Schiller, Maria Lavzin, Uri Dubin, and Ronald R. Coifman. Hierarchical coupled-geometry analysis for neuronal structure and activity pattern discovery. IEEE Journal of Selected Topics in Signal Processing, 10(7):1238­1253, 2016.
Gal Mishne, Ronen Talmon, Israel Cohen, Ronald R. Coifman, and Yuval Kluger. Data-driven tree transforms and metrics. IEEE Transactions on Signal and Information Processing over Networks, 2017. in press.
Wei Pan, Xiaotong Shen, and Binghui Liu. Cluster analysis: Unsupervised learning via supervised learning with a non-convex penalty. Journal of Machine Learning Research, 14:1865­1889, 2013.
K. Pelckmans, J. De Brabanter, J. Suykens, and B. De Moor. Convex clustering shrinkage. In PASCAL Workshop on Statistics and Optimization of Clustering Workshop, 2005.
Idan Ram, Michael Elad, and Israel Cohen. Image processing using smooth ordering of its patches. IEEE transactions on image processing, 22(7):2764­2774, 2013.
Nikhil Rao, Hsiang-Fu Yu, Pradeep K. Ravikumar, and Inderjit S. Dhillon. Collaborative filtering with graph information: Consistency and scalable methods. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2107­2115. Curran Associates, Inc., 2015.
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive autoencoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 833­840, 2011.
Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323­2326, 2000.
Matthias Scholz, Fatma Kaplan, Charles L Guy, Joachim Kopka, and Joachim Selbig. Non-linear PCA: a missing data approach. Bioinformatics, 21(20):3887­3895, 2005.
Nauman Shahid, Nathanael Perraudin, Vassilis Kalofolias, Giles Puy, and Pierre Vandergheynst. Fast robust PCA on graphs. IEEE Journal of Selected Topics in Signal Processing, 10(4):740­ 756, 2016.
Ying Sun, Prabhu Babu, and Daniel P. Palomar. Majorization-minimization algorithms in signal processing, communications, and machine learning. IEEE Transactions on Signal Processing, 65 (3):794­816, 2017.
Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319­2323, Dec. 2000.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th International Conference on Machine learning (ICML-08), pp. 1096­1103. ACM, 2008.
Chong Wu, Sunghoon Kwon, Xiaotong Shen, and Wei Pan. A new algorithm and theory for penalized regression-based clustering. Journal of Machine Learning Research, 17(188):1­25, 2016.
Or Yair, Ronen Talmon, Ronald R. Coifman, and Ioannis G. Kevrekidis. Reconstruction of normal forms by learning informed observation geometries from data. Proceedings of the National Academy of Sciences, 114(38):E7865­E7874, 2017.
10

Under review as a conference paper at ICLR 2019

A DERIVATION OF MAJORIZATION

We first construct a majorization of the data-fidelity term. It is easy to verify that the following function of U

g1(U | U~ )

=

1 2

X~ - U

2F ,

(4)

where

X~

=

P(X)

+

Pc (U~ ),

majorizes

the

data-fidelity

term

1 2

P(X) - P(U)

2 F

at

U~ .

We next construct a majorization of the penalty term. Recall that the first-order Taylor approximation of a differentiable concave function provides a tight bound on the function. Therefore, under Assumption 2.2, we have the following inequality

(z)  (z~) +  (z~)(z - z~), for all z, z~  [0, ).

Thus, we can majorize the penalty term rJr(U) + cJc(U) with the function

g2(U | U~ ) = r

w~r,ij Ui· - Uj· 2 + c

w~c,ij U·i - U·j 2 + ,

(i,j)Er

(i,j)Ec

(5)

where  is a constant that does not depend on U and w~r,ij and w~c,ij (2) are weights that depend on U~ . The sum of functions (4) and (5)

g(U | U~ ) = g1(U | U~ ) + g2(U | U~ )

(6)

=

1 2

X~ - U

2 F

+

r

w~r,ij Ui· - Uj· 2 + c

w~c,ij U·i - U·j 2 + 

(i,j)Er

(i,j)Ec

majorizes our objective function (1) at U~ .

B CONVERGENCE

The MM algorithm generates a sequence of iterates that has at least one limit point, and the limit points are stationary points of the objective function

f (U)

=

1 2

P(X) - P(U)

2 F

+

r Jr (U)

+

cJc(U).

(7)

To reduce notational clutter, we suppress the dependency of f on r and c since they are fixed during Algorithm 1. We prove Proposition 1 in three stages. First, we show that all limit points of the MM algorithm are fixed points of the MM algorithm map. Second, we show that fixed points of the MM algorithm are stationary points of f in (7). Finally, we show that the MM algorithm has at least one limit point.

B.1 LIMIT POINTS ARE FIXED POINTS
The convergence theory of MM algorithms relies on the properties of the algorithm map (U) that returns the next iterate given the last iterate. For easy reference, we state a simple version of Meyer's monotone convergence theorem Meyer (1976), which is instrumental in proving convergence in our setting.
Theorem 1 Let f (U) be a continuous function on a domain S and (U) be a continuous algorithm map from S into S satisfying f ((U)) < f (U) for all U  S with (U) = U. Then all limit points of the iterate sequence Uk = (Uk-1) are fixed points of (U).
In order to apply Theorem 1, we need to identify elements in the assumption with specific functions and sets corresponding to the problem of minimizing (7). Throughout the following proof, it will sometimes be convenient to work with the column major vectorization of a matrix. The vector b = vec(B) is obtained by stacking the columns of B on top of each other.
The function f : Take S = Rm×n and f : S  R to be the objective function in (7) and majorize f with g(U | U~ ) given in (6). The function f is continuous. Let (U~ ) = arg min g(U | U~ ) denote
U

11

Under review as a conference paper at ICLR 2019

the algorithm map for the MM algorithm. Since g(U | U~ ) is strongly convex in U, it has a unique global minimizer. Consequently, f ((U)) < f (U) for all (U) = U.
Continuity of the algorithm map : Continuity of  follows from the fact that the solution to the convex biclustering problem is jointly continuous in the weights and data matrix (Chi et al., 2017)[Proposition 2]. The weight w~r,ij(U~ ) =  ( Ui· - Uj· 2) is a continuous function of U~ , since  is continuous according to Assumption 2.2. The weight w~c,ij(U~ ) is likewise continuous in U~ . The data matrix passed into the convex biclustering algorithm is X~ = P(X) + Pc (U~ ), which is a continuous function of U~ since the projection mapping Pc is continuous.

B.2 FIXED POINTS ARE STATIONARY POINTS

Let Lij = (ei - ej)T  I and L~ij = I  (ei - ej)T, where  denotes the Kronecker product. Then vec(Ui· - Uj·) = Liju and vec(U·i - U·j) = L~iju.
The directional derivative of f in the direction v at a point u is given by

 ( Liju 2; v) =

 ( Lij u 2) Lij v,

Lij u Lij u 2

 ( Lij u 2) Lij v 2

Lij u = 0 otherwise.

A point u is a stationary point of f , if for all direction vectors v

0  P(u - x), v + r

 ( Liju 2; v) + c

 ( L~iju 2; v),

(i,j)Er

(i,j)Ec

where P(u - x) = vec(P(U) - P(X)).

A point u is a fixed point of , if 0 is in the subdifferential of g(u | u), i.e.

0  {P(u - x)} + r

 ( Lij u 2) Lij u 2 + c

 ( L~ij u 2) L~ij u 2,

(i,j)Er

(i,j)Ec

where the set on the right is the subdifferential g(u | u).

(8)

If Liju = 0, then  Liju 2 =

LT Lij u
ij Lij u 2

. On the other hand, if Liju = 0, then  Liju 2 =

 0 2 = {d : d 2  1}.

Fix an arbitrary direction vector v. The inner product of v with an element in the set on right hand side of (8) is given by

P(u - x), v + r

 ( Lij u 2) dij , v + c

 ( L~ij u 2) dij , v ,

(i,j)Er

(i,j)Ec

(9)

where dij   Lij u 2 and d~ij   L~ij u 2.

Then the supremum of the right hand side of (9) over all dij   Liju 2 and d~ij   L~iju 2 is nonnegative, because 0  g(u | u). Consequently, all fixed points of  are also stationary points
of f .

B.3 THE MM ITERATE SEQUENCE HAS A LIMIT POINT
To ensure the existence of a limit point, we show that the function f is coercive, i.e. f (Ut)   for any sequence Ut F  . Recall that according to Assumption 2.1 we assume that the row and column edge sets Er and Ec form connected graphs. Therefore, Jr(U) = Jc(U) = 0 if and only if U = a11T (Chi et al., 2017, Proposition 3). The edge-incidence matrix of the column graph c  R|Ec|×n encodes its connectivity and is defined as
1 If node i is the head of edge l,  c,li = -1 If node i is the tail of edge l, 0 otherwise.

12

Under review as a conference paper at ICLR 2019

The row edge-incidence matrix r  R|Er|×m is defined similarly. Assume that  non-empty, i.e. at least one entry of the matrix has been observed. Finally, assume that  is also coercive.
Note that any sequence Ut = at11T + Bt where Bt, 11T = 0. Note that Jr(Ut) = Jr(Bt) and Jc(Ut) = Jc(Bt). Let Ut be a diverging sequence, i.e. Ut F  . There are two cases to consider.
Case I: Suppose that Bt F  . Let

L=

I  r c  I

 R|Er|m+|Ec|n×mn,

and let min denote the smallest singular value of L. Note that the null space of L is the span of 1. Therefore, since 1, bt = 0

Lbt 2  min Bt F.

(10)

Also note that

Lbt =

vec(r Bt ) vec(BtcT)

.

Since the mapping x = xT1 xT2 T  max{ x1 2, x2 2} is a norm, and all finite dimensional norms are equivalent, there exists some  > 0 such that

 Lbt 2  max rBt F, BtcT F .

(11)

By the triangle inequality

max rBt F, BtcT F



  max

Lij bt 2,

 L~ij bt 2 .

(i,j)Er

(i,j)Ec



(12)

Let M = max{|Er|, |Ec|} then



 max

Lij bt 2,

 L~ij bt 2  M max

(i,j)Er

(i,j)Ec



max Lij bt 2, max L~ij bt 2

(i,j)Er

(i,j)Ec

. (13)

Putting inequalities (10), (11), (12), and (13) together gives us

min M

Bt

F



max

max Lij bt 2, max L~ij bt 2

(i,j)Er

(i,j)Ec

.

Since  is increasing according to Assumption 2.2, it follows that



min M

Bt

F

 max  max Lijbt 2 ,  max L~ijbt 2

(i,j)Er

(i,j)Ec

.

Inequality (15) implies that

min{r, c}M 

min M

Bt

F

 min{r, c} max {Jr(Ut), Jc(Ut)}  rJr(Ut) + cJc(Ut).

Consequently, since  is increasing and Bt F   implies that f (Ut)  .

(14) (15)

13

Under review as a conference paper at ICLR 2019

15

10

5

0

3 2

2 1

10

0 -1

-1 -2

Figure 4: Points in 3D used to generate the Euclidean distance matrix X in the linkage dataset. Rows correspond to the helix, columns to the 2D surface. Points are colored corresponding to the embedding of rows and columns in Figure 2.

Case II: Suppose Bt F  B for some B. Then |at|  . Note that we have the following inequality

f (Ut) 

(xij - bk,ij - at)2

(i,j)

 at2 - 2at(xij - bk,ij )
(i,j)

= ||a2t - 2at

(xij - bk,ij )

(i,j)

 ||a2t - 2at sup

(xij - bk,ij )

Bt FB (i,j)

= || at2 - 2atC = || (at - C)2 - C2 ,

where C = ||-1 sup

(i,j)(xij - bk,ij ).

Bt FB

The function (at - C)2 diverges since |at|  . Therefore, the function f is coercive.

C FILLING IN MISSING DATA
We present the original underlying structure of 3D points used to generate the Euclidean distance matrix X for the datasets linkage and linkage2 in Figure 4 and Figure 7. In Figure 5 and Figure 8, on the left we plot the original complete matrix where the rows and columns have been ordered according to the geometry of the 3D points. On the right we plot the matrix we analyze whose rows and columns have been permuted and 50% of the entries have been removed. In Figure 6 and Figure 9 we display the matrix X~ (l,k) for three pairs of values l, k to demonstrate the smoothing that is occurring across the different scales of the rows and columns.

14

Under review as a conference paper at ICLR 2019

Figure 5: linkage dataset: (Left) Complete matrix X. (Right) Matrix whose rows and columns and columns have been permuted and 50% of the values have been removed.

Figure 6: linkage dataset: Filled-in matrices X~ at multiple scales: X~ (-3,-2),X~ (1,0),X~ (5,2). Rows and columns have been reordered based on the manifold embedding following (Ankenman, 2014).

1.5

1

0.5

0

-0.5 3 2.5

2 1.5

1 0.5

0 -0.5 -1 -1.5

-1

0 -0.5

1 0.5

1.5

Figure 7: Points in 3D used to generate the Euclidean distance X in the linkage2 dataset. Rows correspond to the three 3D Gaussians, columns to the 2D surface. Points are colored corresponding to the embedding of rows and columns in Figure 2

Figure 8: linkage2 dataset: (Left) Complete matrix X. (Right) Matrix whose rows and columns and columns have been permuted and 50% of the values have been removed.
15

Under review as a conference paper at ICLR 2019
Figure 9: linkage2 dataset: Filled-in matrices X~ at multiple scales: X~ (-4,-3),X~ (-1,1),X~ (5,-3) . Rows and columns have been reordered based on the manifold embedding following (Ankenman, 2014).
16

