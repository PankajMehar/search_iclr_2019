Under review as a conference paper at ICLR 2019

THEORETICAL ANALYSIS OF AUTO RATE-TUNING BY BATCH NORMALIZATION
Anonymous authors Paper under double-blind review

ABSTRACT
Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T -1/2 in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T -1/4 is also shown for stochastic gradient descent.

1 INTRODUCTION

Batch Normalization (abbreviated as BatchNorm or BN) (Ioffe & Szegedy, 2015) is one of the most important innovation in deep learning, widely used in modern neural network architectures such as ResNet (He et al., 2016), Inception (Szegedy et al., 2017), and DenseNet (Huang et al., 2017). It also inspired a series of other normalization methods (Ulyanov et al., 2016; Ba et al., 2016; Ioffe, 2017; Wu & He, 2018).

BatchNorm consists of standardizing the output of each layer to have zero mean and unit variance. For a single neuron, if x1, . . . , xB is the original outputs in a mini-batch, then it adds a BatchNorm layer which modifies the outputs to

x-µ BN(xi) =   + ,

(1)

where µ

=

1 B

B i=1

xi

and 2

=

1 B

Bi=1(xi - µ)2 are the mean and variance within the mini-

batch, and ,  are two learnable parameters. BN appears to stabilize and speed up training, and

improve generalization. The inventors suggested (Ioffe & Szegedy, 2015) that these benefits derive

from the following:

1. By stabilizing layer outputs it reduces a phenomenon called Internal Covariate Shift, whereby the training of a higher layer is continuously undermined or undone by changes in the distribution of its inputs due to parameter changes in previous layers.,
2. Making the weights invariant to scaling, appears to reduce the dependence of training on the scale of parameters and enables us to use a higher learning rate;
3. By implictly regularizing the model it improves generalization.

But these three benefits are not fully understood in theory. Understanding generalization for deep models remains an open problem (with or without BN). Furthermore, in demonstration that intuition can sometimes mislead, recent experimental results suggest that BN does not reduce internal covariate shift either (Santurkar et al., 2018), and the authors of that study suggest that the true explanation for BN's effectiveness may lie in a smoothening effect (i.e., lowering of the Hessian norm) on the objective. Another recent paper (Kohler et al., 2018) tries to quantify the benefits of BN for simple machine learning problems such as regression but does not analyze deep models.

1

Under review as a conference paper at ICLR 2019
Provable quantification of Effect 2 (learning rates). Our study consists of quantifying the effect of BN on learning rates. Ioffe & Szegedy (2015) observed that without BatchNorm, a large learning rate leads to a rapid growth of the parameter scale. Introducing BatchNorm usually stabilizes the growth of weights and appears to implicitly tune the learning rate so that the effective learning rate adapts during the course of the algorithm. They explained this intuitively as follows. After BN the output of a neuron z = BN(w x) is unaffected when the weight w is scaled, i.e., for any scalar c > 0,
BN(w x) = BN((cw) x).
Taking derivatives one finds that the gradient at cw equals to the gradient at w multiplied by a factor 1/c. Thus, even though the scale of weight parameters of a linear layer proceeding a BatchNorm no longer means anything to the function represented by the neural network, their growth has an effect of reducing the learning rate.
Our paper considers the following question: Can we rigorously capture the above intuitive behavior? Theoretical analyses of speed of gradient descent algorithms in nonconvex settings study the number of iterations required for convergence to a stationary point (i.e., where gradient vanishes). But they need to assume that the learning rate has been set (magically) to a small enough number determined by the smoothness constant of the loss function -- which in practice are of course unknown. With this tuned learning rate, the norm of the gradient reduces asymptotically as T -1/2 in T iterations. In case of stochastic gradient descent, the reduction is like T -1/4. Thus a potential way to quantify the rate-tuning behavior of BN would be to show that even when the learning rate is fixed to a suitable constant, say 0.1, from the start, after introducing BN the convergence to stationary point is asymptotically just as fast (essentially) as it would be with a hand-tuned learning rate required by earlier analyses. The current paper rigorously establishes such auto-tuning behavior of BN (See below for an important clarification about scale-invariance).
We note that a recent paper (Wu et al., 2018) introduced a new algorithm WNgrad that is motivated by BN and provably has the above auto-tuning behavior as well. That paper did not establish such behavior for BN itself, but it was a clear inspiration for our analysis of BN.
Scale-invariant and scale-variant parameters. The intuition of Ioffe & Szegedy (2015) applies for all scale-invariant parameters, but the actual algorithm also involves other parameters such as  and  whose scale does matter. Our analysis partitions the parameters in the neural networks into two groups W (scale-invariant) and g (scale-variant). The first group, W = {w(1), . . . , w(m)}, consists of all the parameters whose scales does not affect the loss, i.e., scaling w(i) to cw(i) for any c > 0 do not change the loss (see Definition 2.1 for a formal definition); the second group, g, consists of all other parameters that are not scale-invariant. In a feedforward neural network with BatchNorm added at each layer, the layer weights are scale-invariant. And this fact is also true for Weight Normalization (Salimans & Kingma, 2016), Layer Normalization (Ba et al., 2016), Group Normalization (Wu & He, 2018) (see Table 1 in Ba et al. (2016) for a summary).
1.1 OUR CONTRIBUTIONS
In this paper, we show that the scale-invariant parameters do not require rate tuning. To illustrate this, we consider the case in which we set learning rates separately for scale-invariant parameters W and scale-variant parameters g. Under some assumptions on the smoothness and the boundedness of the noise, we show that
1. In full-batch gradient descent, if the learning rate for g is set optimally, then no matter how the learning rates for W is set, (W ; g) converges to a first-order stationary point in the rate O(T -1/2), which asymptomatically matches with the convergence rate of gradient descent with optimal choice of learning rates for all parameters (Theorem 3.1);
2. In stochastic gradient descent, if the learning rate for g is set optimally, then no matter how the learning rate for W is set, (W ; g) converges to a first-order stationary point in the rate O(T -1/4 polylog(T )), which asymptomatically matches with the convergence rate of gradient descent with optimal choice of learning rates for all parameters (up to a polylog(T ) factor) (Theorem 4.2).
2

Under review as a conference paper at ICLR 2019

In the usual case where we set a unified learning rate for all parameters, our results imply that we only need to set a learning rate that is suitable for g. This means introducing scale-invariance into neural networks potentially reduces the efforts to tune learning rates, since there are less number of parameters we need to concern in order to guarantee an asymptomatically fastest convergence.
Note that we study the dynamics with assumptions that the loss is smooth. However, BN introduces non-smoothness in extreme cases due to division by zero when the input variance is zero (see equation 1). Note that the suggested implementation of BN in Ioffe & Szegedy (2015) uses a smoothening constant in the whitening step, but it does not preserve scale-invariance. In order to avoid this issue, we describe a simple modification of the smoothening that maintains scaleinvariance. Also, our result cannot be applied to neural networks with ReLU, but it is applicable for its smooth approximation softplus (Dugas et al., 2001).

1.2 RELATED WORKS

Previous work for understanding Batch Normalization. Only a few recent works try to theoretically understand BatchNorm. Santurkar et al. (2018) was described earlier. Kohler et al. (2018) aims to find theoretical setting such that training neural networks with BatchNorm is faster than without BatchNorm. In particular, the authors analyzed three types of shallow neural networks, but rather than consider gradient descent, the authors designed task-specific training methods when discussing neural networks with BatchNorm. Bjorck et al. (2018) observes that the higher learning rates enabled by BatchNorm improves generalization.

Convergence of adaptive algorithms. Our analysis is inspired by the proof for WNGrad (Wu et al., 2018), where the author analyzed an adaptive algorithm, WNGrad, motivated by Weight Normalization (Salimans & Kingma, 2016).Other works analyzing the convergence of adaptive methods are (Ward et al., 2018; Li & Orabona, 2018; Zou & Shen, 2018; Dongruo Zhou, 2018).

Invariance by Batch Normalization. Cho & Lee (2017) proposed to run riemmanian gradient

descent on Grassmann manifold G(1, n) since the weight matrix is scaling invariant to the loss

function. Zhang & Li (2018) observed that the effective stepsize is proportional to

.w
wt 2

2 GENERAL FRAMEWORK

In this section, we introduce our general framework in order to study the benefits of scale-invariance.

2.1 MOTIVATING EXAMPLES OF NEURAL NETWORKS

Scale-invariance is common in neural networks with BatchNorm. We formally state the definition of scale-invariance below:
Definition 2.1. (Scale-invariance) Let F(w,  ) be a loss function. We say that w is a scale-invariant parameter of F if for all c > 0, F(w,  ) = F(cw,  ); if w is not scale-invariant, then we say w is a scale-variant parameter of F.

We consider the following L-layer "fully-batch-normalized" feedforward network  for illustration:

L() = EZDB

1 B

B

fyb (BN(W (L)(BN(W (L-1) · · · (BN(W (1)xb))))))

.

b=1

(2)

Z = {(x1, y1), . . . , (xB, yB)} is a mini-batch of B pairs of input data and ground-truth label from a data set D. fy is an objective function depending on the label, e.g., fy could be a cross-entropy loss
in classification tasks. W (1), . . . , W (L) are weight matrices of each layer.  : R  R is a nonlinear
activation function which processes its input elementwise (such as ReLU, sigmoid). Given a batch of inputs z1, . . . , zB  Rm, BN(zb) outputs a vector z~b defined as

z~b,k

:=

k

zb,k - k

µk

+ k,

(3)

where µk = Eb[B][zb,k] and k2 = Eb[B][(zb,k - µk)2] are the mean and variance of zb, k and k are two learnable parameters which rescale and offset the normalized outputs to retain the

3

Under review as a conference paper at ICLR 2019

representation power. The neural network  is thus parameterized by weight matrices W (i) in each layer and learnable parameters k, k in each BN.
BN has the property that the output is unchanged when the batch inputs z1,k, . . . , zB,k are scaled or shifted simultaneously. For zb,k = wk x^b being the output of a linear layer, it is easy to see that wk is scale-invariant, and thus each row vector of weight matrices W (1), . . . , W (L) in  are scaleinvariant parameters of L(). In convolutional neural networks with BatchNorm, a similar argument can be done. In particular, each filter of convolutional layer normalized by BN is scale-invariant.
With a general nonlinear activation, other parameters in , the scale and shift parameters k and k in each BN, are scale-variant. When ReLU or Leaky ReLU (Maas et al., 2013) are used as the activation , the vector (1, . . . , m, 1, . . . , m) of each BN at layer 1  i < L (except the last one) is indeed scale-invariant. This can be deduced by using the the (positive) homogeneity of ReLU and noticing that the output of internal activations is processed by a BN in the next layer. Nevertheless, we are not able to analyze either ReLU or Leaky ReLU activations because we need the loss to be smooth in our analysis. We can instead analyse smooth activations, such as sigmoid, tanh, softplus (Dugas et al., 2001), etc.

2.2 FRAMEWORK

Now we introduce our general framework. Let  be a neural network parameterized by . Let D be a dataset, where each data point z  D is associated with a loss function Fz() (D can be the set of all possible mini-batches). We partition the parameters  into (W ; g), where W = {w(1), . . . , w(m)} consisting of parameters that are scale-invariant to all Fz, and g contains the remaining parameters. The goal of training the neural network is to minimize the expected loss over the dataset:
L(W ; g) := EzD[Fz(W ; g)].

In order to illustrate the optimization benefits of scale-invariance, we consider the process of training this neural network by stochastic gradient descent with separate learning rates for W and g:

wt(+i)1  wt(i) - w,twt(i) Fzt (t),

gt+1  gt - g,tgt Fzt (t).

(4)

2.3 THE INTRINSIC OPTIMIZATION PROBLEM

Thanks to the scale-invariant properties, the scale of each weight w(i) does not affect loss values. However, the scale does affect the gradients. Let V = {v(1), . . . , v(m)} be the set of normalized weights, where v(i) = w(i)/ w(i) 2. The following simple lemma can be easily shown:

Lemma 2.2 (Implied by Ioffe & Szegedy (2015)). For any W and g,

1

w(i) Fz(W ; g) =

w(i)

v(i) Fz(V ; g),
2

gFz(W ; g) = gFz(V ; g).

(5)

To make w(i) Fz(W ; g) 2 to be small, one can just scale the weights by a large factor. Thus there are ways to reduce the norm of the gradient that do not reduce the loss.

For this reason, we define the intrinsic optimization problem for training the neural network. Instead of optimizing W and g over all possible solutions, we focus on parameters  in which w(i) 2 = 1 for all w(i)  W . This does not change our objective, since the scale of W does not affect the loss.

Definition 2.3 (Intrinsic optimization problem). Let U = { | w(i) 2 = 1 for all i} be the intrinsic domain. The intrinsic optimization problem is defined as optimizing the original problem in U:

min L(W ; g).
(W ;g)U

(6)

For {t} being a sequence of points for optimizing the original optimization problem, we can define {~t}, where ~t = (Vt; gt), as a sequence of points optimizing the intrinsic optimization problem.

In this paper, we aim to show that training neural network for the original optimization problem by gradient descent can be seen as training by adaptive methods for the intrinsic optimization problem, and it converges to a first-order stationary point in the intrinsic optimization problem with no need for tuning learning rates for W .

4

Under review as a conference paper at ICLR 2019

2.4 ASSUMPTIONS ON THE LOSS

We assume Fz(W ; g) is defined and twice continuously differentiable at any  satisfying none of w(i) is 0. Also, we assume that the expected loss L() is lower-bounded by Lmin.

Furthermore, for V = {v(1), . . . , v(m)}, where v(i) = w(i)/ w(i) 2, we assume that the following bounds on the smoothness:

 v(i)

v(j

)

Fz

(V

;

g)

 Lvijv,
2

v


(i)



g

Fz

(V

;

g

)

 Livg,
2

2gFz(V ; g)

 Lgg.
2

In addition, we assume that the noise on the gradient of g in SGD is upper bounded by Gg:

E

gFz(V ; g) - EzD [gFz(V ; g)]

2 2

 Gg2.

Smoothed version of motivating neural networks. Note that the neural network  illustrated in
Section 2.1 does not meet the conditions of the smooothness at all since the loss function could be non-smooth. We can make some mild modifications to the motivating example to smoothen it 1:

(1). The activation could be non-smooth. A possible solution is to use smooth nonlinearities, e.g., sigmoid, tanh, softplus (Dugas et al., 2001), etc. Note that softplus can be seen as a smooth approximation of the most commonly used activation ReLU.

(2). The formula of BN shown in equation 3 may suffer from the problem of division by zero. To avoid this, the inventors of BN, Ioffe & Szegedy (2015), add a small smoothening parameter > 0

to the denominator, i.e.,

z~b,k

:=

k

zb,k - µk k2 +

+ k,

(7)

However, when zb,k = wk x^b, adding a constant directly breaks the scale-invariance of wk. We can preserve the scale-invariance by making the smoothening term propositional to wk 2, i.e., replacing with wk 2. By simple linear algebra and letting u := Eb[B][x^b], S := Varb[B](x^b), this smoothed version of BN can also be written as

z~b,k

:=

k

wk (x^b - wk S+

u)
I

+

k .

(8)

Since the variance of inputs is usually large in practice, for small , the effect of the smoothening

term is negligible except in extreme cases.

Using the above two modifications, the loss function is already smooth. However, the scale of scale-variant parameters may be unbounded during training, which could cause the smoothness unbounded. To avoid this issue, we can either project scale-variant parameters to a bounded set, or use weight decay for those parameters (see Appendix C for a proof for the latter solution).

2.5 KEY OBSERVATION: THE GROWTH OF WEIGHTS

The following lemma is our key observation. It establishes a connection between the scale-invariant property and the growth of weight scale, which further implies an automatic decay of learning rates:
Lemma 2.4. For any scale-invariant weight w(i) in the network , we have:

1. wt(i) and wt(i) L(t) are always perpendicular;

2.

wt(+i)1

2 2

=

wt(i)

2 2

+

w2 ,t

wt(i) F (t)

2 2

=

wt(i)

2 2

+

w2 ,t

wt(i)

2 2

vt(i) F (~t) 22.

Proof. Let  be all the parameters other than w(i). Taking derivatives with respect to c for the both sides of Fz(w(i),  ) = Fz(cw(i),  ), we have

0

=

 c

Fz

(cw(i),



).

1Our results to this network are rather conceptual, since the smoothness upper bound can be as large as M O(L), where L is the number of layers and M is the maximum width of each layer.

5

Under review as a conference paper at ICLR 2019

The right hand side equals cw(i) L(cw(i),  ) w(i), then the first proposition follows by taking c = 1. Applying Pythagorean theorem and Lemma 2.2, the second proposition directly follows.
Using Lemma 2.4, we can show that performing gradient descent for the original problem is equivalent to performing an adaptive gradient method for the intrinsic optimization problem:
Theorem 2.5. Let  be a projection operator which maps a vector w to w/ w 2. Then

vt(+i)1 = 

vt(i)

-

w,t Gt

vt(i)

F

(~t

)

,

Gt(+i)1

=

Gt(i)

+

w2 ,t Gt(i)

vt(i) , F (~t)

2 2

(9)

Remark 2.6. Wu et al. (2018) noticed that Theorem 2.5 is true for Weight Normalization by direct calculation of gradients. Inspiring by this, they proposed a new adaptive method called WNGrad. Our theorem is more general since it holds for any normalization methods as long as it induces scale-invariant properties to the network. The adaptive update rule derived in our theorem can be seen as WNGrad with projection to unit sphere after each step.

Proof for Theorem 2.5. Let Gt =

wt(i)

2 2

.

The

second

equation

is

by

Lemma

2.4.

By

Lemma

2.2,

wt(+i)1 = wt(i) -

w,t wt(i)

v(i) Fz(~t)
2

=

wt(i) 2

vt(i)

-

w,t Gt

v(i) Fz(~t)

,

which implies equation 9.

While popular adaptive gradient methods such as AdaGrad (Duchi et al., 2011), RMSprop (Hinton
et al.), Adam (Kingma & Ba, 2014) adjust learning rates for each single coordinate, this implicit
adaptive gradient method described in Theorem 2.5 sets adaptive learning rates 1/G(ti) for each scale-invariant parameter respectively. Sometimes we call w,t/Gt(i) the effective learning rate of vt(i) (or wt(i)) in this paper.

3 TRAINING BY FULL-BATCH GRADIENT DESCENT

In this section, we rigorously analyze the effect related to the scale-invariant properties in training
neural network by full-batch gradient descent. We use the framework introduced in Section 2.2 and
assumptions from Section 2.4. We focus on the full-batch training, i.e., zt is always equal to the whole training set and Fzt () = L().

3.1 SETTINGS AND MAIN THEOREM

Assumptions on learning rates. We consider the case that we use fixed learning rates for both W and g, i.e., w,0 = · · · = w,T -1 = w and g,0 = · · · = g,T -1 = g. We assume that g is tuned carefully to g = (1 - cg)/Lgg for some constant cg  (0, 1). For w, we do not make any assumption, i.e., w can be set to any positive value.
Theorem 3.1. Consider the process of training  by gradient descent with g = 2(1 - cg)/Lgg and arbitrary w > 0. Then  converges to a stationary point in the rate of

min
0t<T

L(Vt; gt) 2  O~

1 T

1 
w

+

w2

+

1+gw

,

(10)

where Vt = {vt(1), . . . , vt(m)} with vt(i) = wt(i)/ wt(i) 2, O~ suppresses polynomial factors in Livjv, Livg, Lgg, w0(i) 2, w0(i) -2 1, L(0) - Lmin for all i, j, and we see Lgg = (1).

This matches the asymptotic convergence rate of GD by Carmon et al. (2018).

3.2 PROOF SKETCH

The high level idea is to use the decrement of loss function to upper bound the sum of the squared

norm of the gradients. Note that

L(Vt; gt)

2 2

=

m i=1

vt(i) L(Vt; gt)

2 2

+

gt L(Vt; gt)

2 2

.

6

Under review as a conference paper at ICLR 2019

For the first part

m i=1

vt(i) L(Vt; gt) 22, we have

m T -1 i=1 t=0

vt(i) L(Vt; gt)

m T -1

2 2

=

wt(i)

i=1 t=0

m



wT(i)

2 2

·

i=1

2 2

wt(i) L(Wt; gt)

wT(i)

2 2

-

w0(i)

2 2

w2

2 2

(11)

Thus the core of the proof is to show that the monotone increasing wT(i) 2 has an upper bound for all T . It is shown that for every w(i), the whole training process can be divided into at most two

phases.

In the first phase, the effective learning rate w/

wt(i)

2 2

is

larger

than

some

threshold

1 Ci

(defined in Lemma 3.2) and in the second phase it is smaller.

Lemma 3.2 (Taylor Expansion).

Let Ci

:=

1 2

m j=1

Lvijv

+

Livg 2 m/(cg Lgg )

=

O~(1).

Then

m

L(t+1) - L(t) = -

w

wt(i) L(t)

2 2

i=1

1-

Ciw

wt(i)

2 2

-

1 2 cgg

gt L(t)

2 2

.

(12)

If wt(i) 2 is large enough and that the process enters the second phase, then by Lemma 3.2 in

each

step

the

loss

function

L

will

decrease

by

w 2

wt(i) L(t)

2 2

=

wt(+i)1

2 2

-

wt(i)

2w

2 2

(Recall that

wt(+i)1

2 2

-

wt(i)

2 2

=

w2 ,t

wt(i) L(t)

2 2

by Lemma 2.4).

Since L is lower-bounded, we can

conclude

wt(i)

2 2

is

also

bounded

.

For the second part, we can also show that by Lemma 3.2

T -1 t=0

gt L(Vt; gt)

2 2



2 g cg

L(0)

-

L(T )

+

m i=1

Ci

wT(i)

2 2

w0(i)

2 2

Thus we can conclude O~(1 ) convergence rate of
T

L(t)

2 as follows.

min
0t<T

L(Vt; gt)

2 2



1 T

T -1

t=0

mm

vt(i) L(Vt; gt)

2 2

+

gt L(Vt; gt)

2 2

i=1 i=1

 O~( 1 ) T

The full proof is postponed to Appendix A.

4 TRAINING BY STOCHASTIC GRADIENT DESCENT

In this section, we analyze the effect related to the scale-invariant properties when training a neural network by stochastic gradient descent. We use the framework introduced in Section 2.2 and assumptions from Section 2.4.

4.1 SETTINGS AND MAIN THEOREM

Assumptions on learning rates. As usual, we assume that the learning rate for g is chosen carefully and the learning rate for W is chosen rather arbitrarily. More specifically, we consider the case that the learning rates are chosen as

w,t = w · (t + 1)-,

g,t = g · (t + 1)-1/2.

We assume that the initial learning rate g of g is tuned carefully to g = (1 - cg)/Lgg for some constant cg  (0, 1). Note that this learning rate schedule matches the best known convergence rate
O(T -1/4) of SGD in the case of smooth non-convex loss functions (Ghadimi & Lan, 2013).

For the learning rates of W , we only assume that 0    1/2, i.e., the learning rate decays equally
as or slower than the optimal SGD learning rate schedule. w can be set to any positive value. Note that this includes the case that we set a fixed learning rate w,0 = · · · = w,T -1 = w for W by taking  = 0.

7

Under review as a conference paper at ICLR 2019

Remark 4.1. Note that the auto-tuning behavior induced by scale-invariances always decreases the learning rates. Thus, if we set  > 1/2, there is no hope to adjust the learning rate to the optimal strategy (t-1/2). Indeed, in this case, the learning rate 1/Gt in the intrinsic optimization process decays exactly in the rate of ~ (t-), which is the best possible learning rate can be achieved without
increasing the original learning rate.
Theorem 4.2. Consider the process of training  by gradient descent with w,t = w · (t + 1)- and g,t = g · (t + 1)-1/2, where g = 2(1 - cg)/Lgg and w > 0 is arbitrary. Then  converges to a stationary point in the rate of

min E
0t<T

L(Vt; gt)

2 2

O~ 
O~

log T T
(logT )3/2 T

0   < 1/2;  = 1/2.

(13)

where Vt = {vt(1), . . . , vt(m)} with vt(i) = wt(i)/ wt(i) 2, O~ suppresses polynomial factors in w, w-1, g-1, Lvijv, Lvi g, Lgg, w0(i) 2, w0(i) 2-1, L(0) - Lmin for all i, j, and we see Lgg = (1).

Note that this matches the asymptotic convergence rate of SGD, within a polylog(T ) factor.

4.2 PROOF SKETCH

We delay the full proof into Appendix B and give a proof sketch in a simplified setting where there

is

no

g

and





[0,

1 2

).

We

also

assume

there's

only

one

wi,

that

is,

m

=

1

and

omit

the

index

i.

By Taylor expansion, we have

E [L(t+1)]  L(t) -

w,t vt 2

wt L(t)

2 2

+

E

w2 ,tLvv

wt

2 2

wt Fzt (t)

2 2

(14)

We can lower bound the effective learning rate

w,T wT 2

and

upper

bound

the

second

order

term

re-

spectively in the following way:

(1).

For

all

0





<

1 2

,

the

effective

learning

rate

w,T wT 2

= ~ (T -1/2);

(2).

T w2 ,tLvv

t=0

wt

2 2

wt Fzt (t)

2 2

=

O~

log

wT2 w02

= O~(log T ).

Taking expectation over equation 14 and summing it up, we have

E

T -1 t=0

w,t wt 2

vt L(t)

2 2

 L(0) - E[L(T )] + E

T -1 w2 ,tLvv

t=0

wt

2 2

wt Fzt (t)

2 2

.

Plug the above bounds into the above inequality, we complete the proof.

T -1

(T -1/2) · E

vt L(t)

2 2

 L(0) - E[L(T )] + O~(log T ).

t=0

5 CONCLUSIONS AND FUTURE WORKS

In this paper, we studied how scale-invariances in neural networks with BN help optimization, and showed that (stochastic) gradient descent can achieve asymptotic best convergence rate without tuning learning rates for scale-invariant parameters. Thus, our analysis implies that scale-invariance in nerual networks introduced by BN reduces the efforts for tuning learning rate.
However, our analysis only applies to smooth loss functions. In modern neural networks, ReLU or Leaky ReLU are often used, which makes the loss non-smooth. It would have more implications by showing similar results in non-smooth settings. Also, we only considered gradient descent in this paper. It can be shown that if we perform (stochastic) gradient descent with momentum, the norm of scale-invariant parameters will also be monotone increasing and the effective learning rate will decrease automatically. It would be interesting to use it to show similar convergence results for more gradient methods.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
Johan Bjorck, Carla Gomes, and Bart Selman. Understanding batch normalization. arXiv preprint arXiv:1806.02375, 2018.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points of non-convex, smooth high-dimensional functions. 2018.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In Advances in Neural Information Processing Systems, pp. 5225­5235, 2017.
Ziyan Yang Yuan Cao Quanquan Gu Dongruo Zhou, Yiqi Tang. On the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Charles Dugas, Yoshua Bengio, Franc¸ois Be´lisle, Claude Nadeau, and Rene´ Garcia. Incorporating second-order functional knowledge for better option pricing. In Advances in neural information processing systems, pp. 472­478, 2001.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341­2368, 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261­2269. IEEE, 2017.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. In Advances in Neural Information Processing Systems, pp. 1945­1953, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning-Volume 37, pp. 448­456. JMLR. org, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas Hofmann. Towards a theoretical understanding of batch normalization. arXiv preprint arXiv:1805.10694, 2018.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. arXiv preprint arXiv:1805.08114, 2018.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing. Citeseer, 2013.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pp. 901­909, 2016.
9

Under review as a conference paper at ICLR 2019

Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization?(no, it is not about internal covariate shift). arXiv preprint arXiv:1805.11604, 2018.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In AAAI, volume 4, pp. 12, 2017.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
Xiaoxia Wu, Rachel Ward, and Le´on Bottou. Wngrad: Learn the learning rate in gradient descent. arXiv preprint arXiv:1803.02865, 2018.
Yuxin Wu and Kaiming He. Group normalization. arXiv preprint arXiv:1803.08494, 2018.
Guoqiang Zhang and Haopeng Li. Effectiveness of scaled exponentially-regularized linear units (serlus). arXiv preprint arXiv:1807.10117, 2018.
Fangyu Zou and Li Shen. On the convergence of adagrad with momentum for training deep neural networks. arXiv preprint arXiv:1808.03408, 2018.

A PROOF FOR FULL-BATCH GRADIENT DESCENT

By the scale-invariant property of w(i), we know that L(W ; g) = L(V ; g). Also, the following identities about derivatives can be easily obtained:

 w(i)w(j)

L(W

;

g)

=

w(i)

1 2 w(j)

2

 v(i)v(j)

L(V

;

g)

 w(i)g

L(W

;

g)

=

w(i)

1 2 w(j)

2

 v(i)g

L(V

;

g)

2gL(W ; g) = 2gL(V ; g).

Thus, the assumptions on the smoothness imply

 L(W ; g) 

Lvijv

w(i)w(j)

2 w(i) 2 w(j) 2

 w(i)

g

L(W

;

g)


2

Lvi g w(i) 2

g2 L(W ; g)

 Lgg.
2

(15) (16) (17)

Proof for Lemma 3.2. Using Taylor expansion, we have   (0, 1), such that for wt(i ) = (1 - )wt(i) + wt(+i)1,

m

L(t+1) - L(t) 

wt(i) wt(i) L(t) + gt gt L(t)

i=1

1m m +
2
i=1 j=1

wt(i)
2

wt(j)
2

Lvijv wt(i ) 2 wt(j ) 2

m
+
i=1

wt(i)
2

gt 2

Livg wt(i )
2

1m +
2

gt

2 2

Lgg

.

i=1

10

Under review as a conference paper at ICLR 2019

Note that wt(+i)1 - wt(i) = wwt(i) L(t) is perpendicular to wt(i), we have wt(i ) 2  (wt(+i)1 - wt(i)) + wt(i) 2  wt(i) 2.
Thus,

m

L(t+1) - L(t) 

wt(i) wt(i) L(t) + gt gt L(t)

i=1

1m m +
2

wt(i) 2 wt(j) 2

i=1 j=1

m
+ wt(i) 2 gt 2
i=1

1m +
2

gt

2 2

Lgg .

i=1

Lvi g wt(i) 2

Livjv wt(i) 2 wt(j) 2

By the inequality of arithmetic and geometric means, we have

wt(i)
2

wt(j)
2

Livjv  1

wt(i) 2 wt(j) 2

2

wt(i) 2
2

Lvijv wt(i)

2 2

+

1 2

wt(j) 2
2

Lvijv

wt(j)

2 2

wt(i) 2

gt 2

Livg  wt(i) 2

wt(i)

2 2

Livg 2 m/(cg Lgg ) wt(i) 2

+

1 4 cg

gt

2 2

Lgg m

.

2

Taking wt(i) = -wwt(i) L(t), gt = -ggt L(t), we have

m

L(t+1) - L(t)  -

w

wt(i) L(t)

2 2

-

g

gt L(t)

2 2

i=1



1 +
2

m

m


Livjv + 2Livg2m/(cgLgg)

i=1 j=1

+

1 (1
2

+

cg/2)g2

gt L(t)

2 2

Lgg .

w2 wt(i)

2

wt(i) L(t)

2 2

2

We

can

complete

the

proof

by

replacing

1 2

m j=1

Lvijv

+

Lvi g2m/(cgLgg)

with

Ci.

Using the assumption on the smoothness, we can show that the gradient with respect to w(i) is essentially bounded:

Lemma A.1. For any W and g, we have

w(i) L(W ; g) 2 

Liviv . w(i) 2

(18)

Proof. A.1 Fix all the parameters except w(i). Then L(W ; g) can be written as a function f (w(i))
on the variable w(i). Let S = {w(i) | w(i) 2 = 1}. Since f is continuous and S is compact, there must exist vm(i)in  S such that f (vm(i)in)  f (w(i)) for all w(i)  S. Note that w(i) is scale-invariant, so wm(i)in is also a minimum in the entire domain and f (wm(i)in) = 0.

For an arbitrary w(i), let v(i) = w(i)/ w(i) 2. Let h : [0, 1]  S be a curve such that h(0) = vm(i)in, h(1) = v(i), and h goes along the geodesic from vm(i)in to v(i) on the unit sphere S with constant speed. Let H( ) = f (h( )). By Taylor expansion, we have

f (v(i)) = H(1) = H(0) + H () = 2f (h())h ().

Thus,

f (v(i)) 2  Lvi v and

w(i) L(W ; g) 2 

.Liviv
w(i) 2

11

Under review as a conference paper at ICLR 2019

The following lemma gives an upper bound to the weight scales. Lemma A.2. For all T  0, we have

1. for i = 1, . . . , m, let ti be the maximum time 0   < T satisfying

w(i)

2 2



2Ciw

(ti = -1 if no such  exists), then wt(ii+) 1 2 can be bounded by

wt(ii+) 1

2 2



w0(i)

2 2

+

w

2Ci + (Lvi v)2

w w0(i)

2 2

;

(19)

2. the following inequality on weight scales at time T holds:

m i=1

wT(i)

2 2

-

w0(i)

2w

2 2

+

1 T -1 2 cgg

t=0

gt L(t)

m

2 2



L(0)

-

Lmin

+

Ki,

i=1

(20)

where Ki =

+Ci w

w0(i)

2 2

1 2

2Ci + (Livv)2

w

w0(i)

2 2

= O~(w2 + 1).

Proof. A.2 For every i = 1, . . . , m, let St(i) := -w

t-1  =0

w(i) L( )

2 2

1 - Ciw

w(i)

2 2

. Also

let Gt := -(1/2)cgg

t-1  =0

gt L(t)

2 2

.

By Lemma 3.2 we know that L(T )  L(0) +

m i=1

ST(i)

+

GT

.

Upper Bound for Weight Scales at ti + 1.

If ti = -1, then

wt(ii+) 1

2 2

=

w0(i) 22. For ti  0,

we can bound

wt(ii+) 1

2 2

by

Lemma

A.1:

2

wt(ii+) 1

2 2

=

wt(ii)

2 2

+

w2

wt(ii) L(ti )

 2Ciw + w2
2

Lvi v w0(i) 2

2

= w

2Ci + (Livv)2

w w0(i)

2 2

.

In either case, we have

wt(ii+) 1

2 2



w0(i)

2 2

+

w

2Ci + (Lvi v)2

w w0(i)

2 2

.

Upper Bound for Weight Scales at T .

Since

w(i)

2 2

is

non-decreasing

with



and

wt(ii+) 1

2 2

>

2Ciw,

T -1
ST(i) - St(ii+) 1 = -w
 =ti+1

w(i) L( )

2 2

1-

 - w T -1 2
 =ti+1

w(i) L( )

2 2

=- 1 2w

wT(i)

2 2

-

wt(ii+) 1

2 2

Ciw

w(i)

2 2

- 1 2w

wT(i)

2 2

+

1 2w

w0(i)

2 2

+

w

2Ci + (Livv)2

w w0(i)

2 2

-

wT(i)

2
2+

w0(i)

2 2

+

1

2w 2w 2

2Ci + (Livv)2

w w0(i)

2 2

,

12

Under review as a conference paper at ICLR 2019

where we use the fact that

wt(i)

2 2

=

w0(i)

2 2

+

w2

bound St(ii+) 1 by

t-1  =0

w(i) L( )

2 2

at

the

third

line.

We can

St(ii+) 1  w

ti

w(i) L( )

2 2

 =0

Ciw

w0(i)

2 2

(

wt(ii+) 1

2 2

-

w0(i)

2 2

)

·

Ci

w0(i)

2 2

 w

2Ci + (Lvi v)2

w w0(i)

2 2

· Ci .

w0(i)

2 2

Combining them together, we have

wT(i)

2 2

-

w0(i)

2w

2 2



-ST(i)

+

St(ii+) 1

+

1 2

2Ci + (Lvi v)2

w w0(i)

2 2

 -ST(i) +

Ciw

w0(i)

2 2

+

1 2

 -ST(i) + Ki.

2Ci + (Lvi v)2

w w0(i)

2 2

Taking sum over all i = 1, . . . , m and also subtracting GT on the both sides, we have

m i=1

wT(i)

2 2

-

w0(i)

2w

2 T -1
2 + (1/2)cgg
t=0

mm

gt L(t)

2 2



-

ST(i) - GT +

Ki

i=1 i=1

m

 L(0) - Lmin + Ki.

i=1

where Lmin  L(T )  L(0) +

m i=1

ST(i)

+

GT

is

used

at

the

second

line.

Combining the lemmas above together, we can obtain our results.

Proof for Theorem 3.1. By Lemma A.2, we have

m T -1 i=1 t=0

2 m T -1

vt(i) L(Vt; gt)

=
2

i=1 t=0

wt(i)

2 2

2
wt(i) L(Wt; gt) 2

m

i=1

wT(i)

2 2

·

wT(i)

2 2

-

w0(i)

2 2

w2

 max
1im

wT(i)

2 2

w

m
·
i=1

wT(i)

2 2

-

w0(i)

2 2

w

4

mm
L(0) - Lmin + Ki +
i=1 i=1

w0(i)

2 2

w

 O~

1 w

+ w4

.

T -1 t=0

gt L(Vt; gt)

2 2



2 cg g

m
L(0) - Lmin + Ki
i=1

 O~ 1 + 1w2 . g

m
L(0) - Lmin + Ki
i=1

13

Under review as a conference paper at ICLR 2019

Combining them together we have

T · min
0t<T

m

2

vt(i) L(Vt; gt)

+
2

gt

2 2

i=1



m T -1

2 T -1

vt(i) L(Vt; gt)

+
2

gt L(Vt; gt)

2 2

i=1 t=0

t=0

 O~

1 w

+ w4

+

1 + w2 g

.

Thus min0t<T L(Vt, gt) 2 converges in the rate of

min L(Vt, gt) 2 = O~ 1

0t<T

T

1 
w

+ w2

+

1+gw

.

B PROOF FOR STOCHASTIC GRADIENT DESCENT

Let Ft = {z0, . . . , zt-1} be the filtration, where {·} denotes the sigma field.
We use Lt := Lzt (t), Ft := Fzt (t) for simplicity. As usual, we define vt(i) = wt(i)/ wt(i) 2. We use the notations vt(i) Lt := vt(i) Lzt (Vt, gt) and vt(i) Ft := vt(i) Fzt (Vt, gt) for short. Let wt(i) = -w,twt(i) Ft, gt = -g,tgt Ft. Lemma B.1. For any a0, . . . , aT  [0, B] with a0 > 0,

T t=1

at

t-1  =0

a

 log2

T -1
at/a0
t=0

+ 1 + 2B/a0

Proof. Let ti be the minimum 1  t  T such that

t-1  =0

a



a0

· 2i.

Let k

be

the

maximum i

such that ti exists. Let tk+1 = T + 1. Then we know that

ti+1 -1 t=ti

at

t-1  =0

a



a0 · 2i + B a0 · 2i

1+ B a0

· 2-i.

Thus,

T t=1

at

t-1  =0

a

k i=0

1

+

B a0

·

2-i

=

k+1+

2B a0



log2

T -1 t=0

at

/a0

+ 1 + 2B/a0.

Lemma B.2. Fix T > 0. Let

1 Ci := 2

m

Lvijv + Livg2m/(cgLgg)

j=1

U

:=

1

+

cg 2

/2

Lgg

G2g

T -1
Si := -
t=0

w,t

wt(i)

2 2

T -1

vt(i) Lt

2 2

+

Ci

t=0

w2 ,t

wt(i)

4 2

vt(i) Ft

2 2

R

:=

-

1 2 cg

T -1

g,t

gt Lt

T -1

2 2

+

U

g2,tGg

t=0 t=0

Then E[LT ] - L0 

m i=1

E[Si]

+

E[R].

14

Under review as a conference paper at ICLR 2019

Proof. Conditioned on Ft, by Taylor expansion, we have

m

E[Lt+1 | Ft]  Lt - w,t

wt(i) Lt

2 2

-

g,t

gt Lt

2 2

+

E[Qt

|

Ft]

i=1

(21)

where Qt is

1m m Qt = 2
i=1 j=1

Livjv

wt(i)

wt(j)

wt(i) 2 wt(j) 2

22

m
+
i=1

Lvi g wt(i) 2

wt(i)

2

gt

2

+

1 Lgg 2

gt

2 2



By the inequality

ab



1 2

a

+

1 2

b,

we

have

wt(i)

2 wt(j)

2

Livjv wt(i) 2 wt(j)

2



1 2

wt(i)

2 2

Lvijv wt(i)

2 2

+

1 2

wt(j)

2 2

Livjv

wt(j)

2 2

wt(i) 2 gt 2

Livg  wt(i) 2

wt(i)

2 2

Livg 2 m/(cg Lgg )

wt(i)

2 2

+

1 4 cg

gt

2 2

Lgg m

.

Note that E[

gt

2 2

|

Ft]



g2,t

gt Ft

2 2

+

Gg2

. Thus,



E[Qt | Ft] 

m

1m 2

Livjv + 2Lvi g2m/(cgLgg)

i=1 j=1

w2 ,t

wt(i)

2 2

wt(i) Ft

2 2

+

1

+ cg/2 Lgg 2

·

g2,t

gt Ft

2 2

+

G2g

m
 Ci
i=1

w2 ,t

wt(i)

2 2

wt(i) Ft

2 2

+

(1

-

cg

/2)g,t

gt Ft

2 2

+

U g2,t

Taking this into equation 21 and summing up for all t, we have

m T -1

E[LT ] - L0  -

E

i=1 t=0

m T -1

+ Ci E

i=1 t=0

w,t

wt(i)

2 2

vt(i) Lt

2 2

-

1 2 cg

T -1
E

g,t

gt Lt

2 2

t=0

w2 ,t

wt(i)

2 2

wt(i) Ft

2 2

T -1

+U

g2,t Gg ,

t=0

and the right hand side can be expressed as

m i=1

E[Si]

+

E[R]

by

definitions.

Lemma B.3. For any T  0, 1  i  m, we have

w,T 

wT(i)

2 2

~ (T -1/2) ~ ((T log T )-1/2)

if 0   < 1/2; if  = 1/2.

15

Under review as a conference paper at ICLR 2019

Proof. By Lemma 2.4, we have

wt(+i)1

4 2

=

=

wt(i)

2 2

+

w2 ,t

wt(i)

2 2

vt(i) Ft

2 2

2

wt(i)

4 2

+

2w2 ,t

vt(i) Ft

2 2

+

w4 ,t

wt(i)

4 2

vt(i) Ft

4 2



wt(i)

4 2

+

2w2 (t

+

1)-2

( Liviv )2

+

w4 (t

+

1)-4

Liviv w0(i) 2





wt(i)

4 2

+

(t

+

1)-2

2w2

( Liviv )2

+

w4

Liviv w0(i) 2

4 

4



w0(i)

4 2

+

t1 ( + 1)2
 =0

 2w2 (Lviiv)2 + w4

Liviv w0(i) 2

4 .

For 0   < 1/2,

T -1 1  =0 ( +1)2

=

O(T 1-2), so

w,T

wT(i)

2 2

T -1  =0

1 ( +1)2

= O(log T ), so

w,T

wT(i)

2 2

= ~ ((T

log T )-1/2).

= ~ (T -1/2); for 

= 1/2,

Lemma B.4. For T > 0,

·

Si can be bounded by Si  -

w,T

wT(i)

2 2

T -1 t=0

vt(i) Lt

2 2

+

O~(log

T

);

· R can be bounded by R  -~ (T -1/2)

T -1 t=0

gt Lt

2 2

+

O~(log

T

).

Proof. Fix i  {1, . . . , m}. First we bound Si. Recall that

Note that

T -1
Si := -
t=0

w,t

wt(i)

2 2

w,t wt(i)

2 2

is

non-increasing,

so

T -1

vt(i) Lt

2 2

+

Ci

t=0

w2 ,t

vt(i)

2 2

vt(i) Ft 22.

T -1
-
t=0

w,t

wt(i)

2 2

vt(i) Lt

2 2



-

w,T T -1

wT(i)

2 2

t=0

vt(i) Lt

2 2

.

(22)

Also note that

wt(i)

2 2



w0(i)

2 2

+

t-1  =0

w2 ,

wt(i) Ft

2 2

.

By

Lemma

B.1,

T -1
Ci
t=0

w2 ,t

wt(i)

2 2

T -1

wt(i) Ft

2 2



Ci

t=0

w2 ,t

wt(i) Ft

2 2

w0(i)

2 2

+

t-1  =0

w2 ,

wt(i) Ft

2 2

 Ci log2

wT(i)

2 2

w0(i)

2 2

+ 1 + 2w2 (Liviv)2

w0(i)

4 2

= O~(log T ).

(23)

We can get the bound for Si by combining equation 22 and equation 23.

Now we bound R. Recall that

R

:=

-

1 2 cg

T -1

g,t

gt Lt

T -1

2 2

+

U

g2,t Gg .

t=0 t=0

The first term can be bounded by -~ (T -1/2)

T -1 t=0

gt Lt

2 2

by noticing

that g,t



g,T

=

(T -1/2). The second term can be bounded by O~(log T ) since

T -1 t=0

g2,t

=

T -1 t=0

O~(1/t)

=

O~(log T ).

16

Under review as a conference paper at ICLR 2019

Proof for Theorem 4.2. Combining Lemma B.2 and Lemma B.4, for 0   < 1/2, we have

T -1

Lmin - L0  E[LT ] - L0  -~ (T -1/2)

E

L(Vt; gt)

2 2

+ O~(log T ).

t=0

Thus,

min E
0t<T

L(Vt; gt)

2 2

1 T -1

 T

E

t=0

Similarly, for  = 1/2, we have

L(Vt; gt)

2 2

 O~

log T T

.

T -1

Lmin - L0  E[LT ] - L0  -~ ((T log T )-1/2)

E

L(Vt; gt)

2 2

+ O~(log T ).

t=0

Thus,

min E
0t<T

L(Vt; gt)

2 2



1 T

T -1
E

t=0

L(Vt; gt)

2 2

 O~

(logT )3/2 T

.

C PROOF FOR THE SMOOTHNESS OF THE MOTIVATING NEURAL NETWORK

In this section we prove that the modified version of the motivating neural network does meet the assumptions in Section 2.4. More specifically, we assume:

· We use the network structure  in Section 2.1 with the smoothed variant of BN as described in Section 2.4;

· The objective fy(·) is twice continuously differentiable, lower bounded by fmin and Lipschitz (|fy(y^)|  f );

· The activation (·) is twice continuously differentiable and Lipschitz (|fy(y^)|  );

·

We

add

an

extra

weight

decay

(L2

regularization)

term

 2

g

2 2

to

the

loss

in

equation

2

for

some  > 0.

First, we show that gt (containing all scale and shift parameters in BN) is bounded during the training process. Then the smoothness follows compactness using Extreme Value Theorem.

We use the following lemma to calculate back propagation: Lemma C.1. Let x1, . . . , xB be a set of vectors. Let u := Eb[B][xb] and S := Varb[B](xb). Let

zb

:=

w 

(xb - u) w S+ I

+

.

Let y := f (z1, . . . , zB) = f (z) for some function f . If f (z) 2  G, then

y

 G B



y

 G B



xb y

2

3  G B.

Proof. Let x~  RB be the vector where x~b := (w

(xb -u))/ w S+ I . It is easy to see

x~

2 2

 B.

Then

y 

=

f (z)

x~

  G x~ 2  G B.

y 



 f (z) 1  G B.

17

Under review as a conference paper at ICLR 2019

For xb, we have

1

xb u

=

I B



xb

w

2 S+

I

=

xb

1 B

B
(xb w)2 -

b =1

2 = ww
B

(xb - u).

1B

2

B xb w 

b =1

Then

xb zb

(1b=b - 1/B) w S+ I w - w (xb - u) ·

=

w

2 S+ I

w

1
S+ I

·

2 B

ww

(xb - u)

Since (w

(xb - u))2  B

w

2 S+

I,

xb zb 2  

1

w

(1b=b
S+ I

- 1/B)

w

2+

1 w S+ I

·2

w

2



3 .

Thus,

xb y 2 

B y 2 b =1 zb

1/2

B

xb zb

2 2

1/2 3   G B.

b =1

Lemma C.2. If g0 2 is bounded by a constant, there exists some constant K such that gt 2  K.

Proof. Fix a time t in the training process. Consider the process of back propagation. Define

B mi
Ri =
b=1 k=1

2



 xb(i,k)

Fz

()

,

where x(bi,k) is the output of the k-th neuron in the i-th layer in the b-th data sample in the batch. By the Lipschitzness of the objective, RL can be bounded by a constant. If Ri can be bounded by a constant, then by the Lipschitzness of  and Lemma C.1, the gradient of  and  in layer i can also
be bounded by a constant. Note that

gt+1,k

=

gt,k

-

g,t



 gt,k

Fz

(t

)

-

g,tgt,k

Thus  and  in layer i can be bounded by a constant since

|gt+1,k |



(1

-

g,t )|gt,k |

+

g,t

·

1 

 gt,k

Fz

(t)

.

Also Lemma C.1 and the Lipschitzness of  imply that Ri-1 can be bounded if Ri and  in the layer i can be bounded by a constant. Using a simple induction, we can prove the existence of K for
bounding the norm of gt 2 for all time t.

Theorem C.3. If g0 2 is bounded by a constant, then  satisfies the assumptions in Section 2.4.

Proof. Let C be the set of parameters  satisfying g  K and w(i) 2 = 1 for all 1  i  m. By Lemma C.2, C contains the set of ~ associated with the points lying between each pair of t and t+1 (including the endpoints).
It is easy to show that Fz(~) is twice continously differentiable. Since C is compact, by the Extreme Value Theorem, there must exist such constants Livjv, Lvi g, Lgg, Gg for upper bounding the smoothness and the difference of gradients.

18

