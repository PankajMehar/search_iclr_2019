Under review as a conference paper at ICLR 2019
PIE: PSEUDO-INVERTIBLE ENCODER
Anonymous authors Paper under double-blind review
ABSTRACT
We consider the problem of information compression from high dimensional data. Where many studies consider the problem of compression by non-invertible transformations, we emphasize the importance of invertible compression. We introduce new class of likelihood-based Auto-Encoders with pseudo bijective architecture, which we call Pseudo Invertible Encoders. We provide the theoretical explanation of their principles. We evaluate Gaussian Pseudo Invertible Encoder on MNIST, where our model outperforms WAE and VAE in sharpness of the generated images.
1 INTRODUCTION
We consider the problem of information compression from high dimensional data. Where many studies consider the problem of compression by non-invertible transformations, we emphasize the importance of invertible compression as there are many cases where one cannot or will not decide a priori what part of the information is important and what part is not. Compression of images for person ID in a small company requires less resolution then person ID at an airport. To loose part of the information without harm to the future purpose of viewing the picture requires knowing the purpose upfront. Therefore, the fundamental advantage of invertible information compression is that compression can be undone if a future purpose so requires.
Recent advances of classification models have demonstrated that deep learning architectures of proper design do not lead to information loss while still being able to achieve state-of-the-art in classification performance. These i-RevNet models Jacobsen et al. (2018) implement a small but essential modification of the popular RevNet models while achieving invertibility and a performance similar to the standard RevNet Gomez et al. (2017). This is of great interest as it contradicts the intuition that information loss is essential to achieve good performance in classification Tishby & Zaslavsky (2015). Despite the requirement of the invertibility, flow-based generating models Dinh et al. (2014; 2016); Jimenez Rezende & Mohamed (2015); Kingma & Dhariwal (2018) demonstrate that the combination of bijective mappings allows one to transform the raw distribution of the input data to any desired distribution and perform the manipulation of the data.
On the other hand, Auto-Encoders have provided the ideal mechanism to reduce the data to the bare minimum while retaining all essential information for a specific task, the one implemented in the loss function. Variational Auto Encoders (VAE) Kingma & Welling (2013) and Wasserstein Auto Encoders (WAE) Tolstikhin et al. (2017) are performing best. They provide an approach for stable training of autoencoders, which demonstrate good results at reconstruction and generation. However, both of these methods involve the optimization of the objective defined on the pixel level. We would emphasise the importance of avoiding the separate decoder part and training the model without relying on the reconstuction quality directly.
Combining the best of Invertible mappings and Auto-Encoders, we introduce Pseudo Invertible Encoder. Our model combines bijectives with restriction and extension of the mappings to the dependent sub-manifolds Fig. 1. The main contributions of this paper are the following:
· We introduce new class of likelihood-based Auto-Encoders, which we call Pseudo Invertible Encoders. We provide the theoretical explanation of their principles.
· We demonstrate the properties of Gaussian Pseudo Invertible Encoder in manifold learning. · We compare our model with WAE and VAE on MNIST, and report that the sharpness of
the images, generated by our models is better.
1

Under review as a conference paper at ICLR 2019
Figure 1: Schematic representation of the proposed mechanism of dimensionality reduction.
2 RELATED WORK
2.1 INVERTIBLE MODELS ResNets He et al. (2016) enable Networks to grow even more and thus memory consumption becomes a bottleneck. Gomez et al. (2017) propose a Reversible Residual Network (RevNet) where each layer's activations can be reconstructed from the activations of the next layer. By replacing the residual blocks with coupling layers, they mimic the behaviour of residual blocks while being able to retrieve the original input of the layer. RevNet replaces the residual blocks of ResNets, but also accommodates non-invertible components to train more efficiently. By adding a downsampling operator to the coupling layer, i-RevNet circumvents these non-invertible modules (Jacobsen et al., 2018). With this they show that losing information is not a necessary condition to learn representations that generalize well on complicated problems. Although i-RevNet circumvents non-invertible modules, data is not compressed and the model is only invertible up to the last layer. All their methods do not allow dimensionality reduction. In current research we build a pseudo invertible model which performs dimensionality reduction.
2.2 AUTOENCODERS Auto-Encoders were first introduced by Rummerhart (1986) as an unsupervised learning algorithm. They are now widely used as a technique for dimension reduction by compressing input data. By training an encoder and a decoder network, and measuring the distance between original and reconstructed data, data can be represented in a latent space. This latent space can then be used for supervised learning algorithms. Instead of learning a compressed representation of the input data Kingma & Welling (2013) propose to learn the parameters of a probability distribution that represent the data. Tolstikhin et al. (2017) introduced new class of models - Wasserstein Auto Encoders, which use Optimal Transport to be trained. These methods require the optimization of the objective function which includes the terms defined on pixel level. Our model does not require such optimization. Moreover, it only perform encoding at training time.
3 THEORY
Here we introduce the approach for obtaining dimensionality reduction invertible mappings. Our method is based on the restriction of the mappings to low-dimensional manifolds, and extension of the inverse mappings with certain constraints (Fig. 2).
3.1 RESTRICTION-EXTENSION APPROACH Given data xi  X  RD. Assuming that X is a d-dimensional manifold, with d < D, we seek to find a mapping G : RD  Rd invertible on X . In other words, we are looking for a pair of
2

Under review as a conference paper at ICLR 2019

Figure 2: The schematic representation of the Restriction-Extension approach. The invertible mapping X  Z is preformed by using the dependent sub-manifold R = g(Z) and a pair extended functions G~, G~-1.

associated functions G and G-1 such that
G(X ) = Z  Rd G-1(Z) = X

(1)

Let R be an open set in RD-d. We use this residual manifold in order to match the dimensionalities of the hidden and initial spaces. Here we introduce the function g : Rd  RD-d. With no loss of generality we can say that R = g(Z). We use the pair of extended functions G~ : RD  Rd × RD-d
and G~-1 : Rd × RD-d  RD to rewrite Eq. 1:

G~(X ) = Z × R G~-1(Z × R) = X

(2)

Rather then searching for the invertible dimensionality reduction mapping directly, we seek to find G~, the invertible transformation with certain constraints, expressed by R.

In search for G~, we focus on F : RD  RD, F  F , where F is a parametric family of functions invertible on RD. We select the function F with parameters  which satisfy the constraint:

F-1  PRd×R  F = idX

(3)

where PRd×R is the orthogonal projection from Rd × RD-d to Rd × R.

Taking into account constraint 3, we derive F(x) = [z, r], where z  Z and r  R. By combining this with Eq. 2 we have the desired pair of functions:

G(x) = z, G-1(z) = F-1([z, g(z)])

(4)

The obtained function G is Pseudo Invertible Endocer, or shortly PIE.

3.2 LOG LIKELIHOOD MAXIMIZATION
As we are interested high dimensional data such as images, the explicit choice of parameters  is impossible. We choose  as a maximizer of the log likelihood of the observed data given the prior p (x):

 = arg max[log p(x)]


After a change of variables according to Eq. 4 we obtain

p(x) = p(F(x)) det

F xT

(5) (6)

3

Under review as a conference paper at ICLR 2019

(a) General Flow

(b) Multi-scale RealNVP

(c) PIE

Figure 3: Schematic representation of three types of bijective mappings currently used in normalizing flows. The circles represent the variables. The basic invertible mappings are depicted with blue edges. Green edges represent the aggregation of the variables in objective function. In general normalizing flow (a) all the variables are mapped in the same manner and are propagated through the same number of flows. The multi-scale architecture used in RealNVP (b) transform different variables with different number of flows and afterwards map them to the same distribution. Our model (c) progressively discards part of the variables by hardly constraining their distributions.

Taking into account the constraint 3 we derive the joint distribution for F(x) = [z, r] p(F(x)) = p(z, r) = p(r|z)p(z)

(7)

p(F(x))dx =

p(r|z)p(z)drdz =

(r - g(z))p(z)drdz

X R=g(Z) Z

RZ

p(F(x)) = (r - g(z))p(z)

Dirac's delta function can me viewed as a limit of sequence of Gaussians: (x) = lim N (x|0, 2I)
0

Let us fix

2=

2 0

1. Then

(x)  N (x|0,

2 0

I)

(r - g(z))  N (r|g(z),

2 0

I)

Finally, for the log likelihood we have:

log p(x)  log p(z) + log N (r|g(z), 02I) + log det

F xT

(8) (9) (10)
(11) (12)
(13)

We choose prior distribution p(z) as Standard Gaussian. We search for the parameters by using Gradient Descent.

3.3 COMPOSITION OF BIJECTIVES

The method relies on the function F. This choice is challenging by itself. The currently known classes of real-value bijectives are limited. To overcome this issue, we approximate F with a composition of basic bijectives from certain classes:

F = FK  Fk-1  . . .  F2  F1

(14)

where Fj = Fj(·|j)  Fj, j = 1 . . . K.

Taking into account that a composition of PIE is also PIE, we create a final dimensionality reduction mapping from a sequence of PIEs:

X  Y1  Y2  · · ·  YL  Z1

(15)

4

Under review as a conference paper at ICLR 2019

Downsampling

1 × 1 Conv, Linear

Coupling

x yx

Kl times Convolutional

Kl times Linear

Split method z

x ...

... z

Figure 4: Architecture of the Pseudo-Invertible Encoder. PIE consists of convolutional and linear
blocks which can be repeated multiple times, as denoted by the three dots between the block structure at the bottom. Each block has Kl repetitions of coupling layers and 1 × 1 convolutions.

such that

D > dim Y1 > dim Y2 > . . . > dim YL > d

where L < D - d. Then the log likelihood is represented as

(16)

L L Kl

log p(x)  log p(z) + log N (rl|gl(zl), 20I) +

log | det(Jkl)|

l=1 l=1 k=1

(17)

where Jkl is the Jacobian of the k-th function of the l-th PIE. The approximation error here depends only on , according to the Eq. 10. For the simplicity we will now refer to the whole model as PIE.
The building blocks of this model are PIE blocks.

3.4 RELATION TO NORMALIZING FLOWS
If we choose the distribution p(z) in Eq. 17 as Standard Gaussian, gl(·) = 0, l and 0 = 1, then the model can be viewed as Normalizing Flow with multi-scale architecture Dinh et al. (2016); Kingma & Dhariwal (2018) Fig. 3. It was demonstrated in Dinh et al. (2016) that the model with such architecture achieves semantic compression.

4 PSEUDO-INVERTIBLE ENCODER
This section introduces the basic bijectives for the Pseudo-Invertible Encoder (PIE). We explain what each building bijective consists of and how it fits in the global architecture as shown in Fig. 4.
4.1 ARCHITECTURE
PIE is composed of a series of convolutional blocks followed by linear blocks, as depicted in Fig. 4. The convolutional PIE blocks consist of series of coupling layers and 1×1 convolutions. We perform invertible downsampling of the image at the beginning of the convolutional block, by reducing the spatial resolution and increasing the number of channels, keeping the overall number of the variables the same. At the end of the convolutional PIE block, the split of variables is performed. One part of the variables is projected to the residual manifold R while others is feed to the next block. The linear PIE blocks are constructed in the same manner. However, the downsampling is not performed and 1 × 1 convolutions are replaced invertible linear mappings.

5

Under review as a conference paper at ICLR 2019

x1 xP
x2

×+ s1 b1

y1

s2 b2

Uy

× + y2

x1 ÷

(a) Forward -

x P -1

s1 b1

s2 b2

x2 ÷ -

y1

U -1

y

y2

(b) Inverse
Figure 5: Structure of a coupling block. P partitions the input into two groups of equal length. U unites these group together. In the inverse P -1 and U -1 are the reverse of these operations respectively.

4.2 COUPLING LAYER
In order to enhance the flexibility of the model, we utilize affine coupling layers Fig. 5. We modify the version, introduced in Dinh et al. (2016). Given input data x, the output y is obtained by using the mapping:

y1 = s1(x2) y2 = s2(y1)

x1 + b1(x2) x2 + b2(y1)



x2 = (y2 - b2(y1))/s2(y1) x1 = (y1 - b1(x2))/s1(x2)

(18)

Here multiplication and division are performed element-wise. The scalings s1, s2 and the biases b1, b2 are the functions, parametrized with neural networks. The invertibility is not required for this functions. x1, x2 are the non-intersecting partitions of x. For convolutional blocks we partition the tensors by splitting them into halves along the channels. In case of the linear blocks, we just split
the features into halves.

The log determinant of the Jacobian of coupling layer is given by:

log det

F xT

= sum(log |s1|) + sum(log |s2|)

where log | · | is calculated element-wise.

4.3 INVERTIBLE 1 × 1 CONVOLUTION AND LINEAR TRANSFORMATION

The affine couplings operate on non-intersecting parts of the tensor. In order to capture the various correlations between channels and features, the different mechanism of channel permutations were proposed. Kingma & Dhariwal (2018) demonstrated that invertible 1×1 convolutions perform better then fixed permutations and reversing of the order of channels Dinh et al. (2016).

We parametrize Invertible 1 × 1 Convolutions and invertible linear mappings with Householder Matrices Householder (1958). Given the vector v, the Householder Matrix is computed as:

H(v)

=

I

-

2

vvT vT v

(19)

6

Under review as a conference paper at ICLR 2019

z xP
r (r - g(z))

z z x P -1
0

y

(a) Forward

(b) Inverse

Figure 6: Structure of the split method. P partitions the input into two sub samples. U unites these sub samples together.

The obtained matrix is orthogonal. Therefore, its inverse is just its transpose, which makes the computation of the inverse easier comparing to Kingma & Dhariwal (2018). The log determinant of the Jacobian of such transformation is equal to 0.

4.4 DOWNSAMPLING

We use invertible downsampling to progressively reduce the spatial size of the tensor and increase

the number of its channels. The downsampling with the checkerboard patterns Jacobsen et al.

(2018);

Dinh

et

al.

(2016)

transforms

the

tensor

of

size

C ×H ×W

into

a

tensor

of

size

4C ×

H 2

×

W 2

,

where H, W are the height and the width of the image, and C is the number of the channels. The

log determinant of the Jacobian of Downsampling is 0 as it just performs permutation.

4.5 SPLIT

All the discussed blocks transform the data while preserving its dimensionality. Here we introduce

Split block Fig. 6, which is responsible for the projection, restrictions and extension, described

in Section 3. It reduces the dimensionality of the data by splitting the variables into two non-

intersecting parts z, r of dimensionalities d and D - d, respectively. z is kept and is to be processed

by the subsequent blocks. r is constrained to match N (r|g(z),

2 0

I).

The

mappings

is

defined

as

z = x|Rd

r  N (r|g(z),

2 0

I)

 x = [z, g(z)]

(20)

5 EXPERIMENTS
5.1 MANIFOLD LEARNING
For this experiment we trained a Gaussian PIE on the MNIST digits dataset. We build PIE with 2 convolutional blocks, each splitting the data in the last layer to 50% of the input size. Next we add three linear blocks to PIE, reducing the dimensions to 64, 10 and the last block does not reduce the dimensions any further. For each affine transformation we use the three biggest possible Householder reflections. For this experiment we set Kl equal to 3. Optimization is done with the Adam optimizer (Kingma & Ba, 2014). The model diminishes the number of dimensions fro R784 to R10.
This experiment shows the ability of PIE to learn a manifold with three different constraints; 2 = 0.01, 2 = 0.1 and 2 = 1.0. The results are shown in Fig. 7. As the constraint gets to loose, as shown in the right column, the model is not able to reconstruct anymore (Fig. 7a). Lower values for 2 perform better in terms of reconstruction. Too low values, however, sample fuzzy images (Fig. 7b). Narrowing down the distribution to sample from increases the models probability to produce accurate images. This is shown in Fig. 7c where samples are taken from N(0, 0.5). For both 2 = 0.01 and 2 = 0.1 reconstructed images are more accurate.

7

Under review as a conference paper at ICLR 2019

(a)

(b)

(c) (d)

(e)

2 = 0.01

2 = 0.1

2 = 1.0

Figure 7: Experiment on MNIST dataset with z = 10. (a) shows reconstructions on test data. Row 1 and 3 are original images, row 2 and 4 are reconstructions from z-space. (b) and (c) both show reconstruction of a samples z-space. (b) is sampled from N (0, 1), (c) is sampled from N (0, 0.5).
(d) is a linear interpolation between a picture on the left and the right of the image. All digits shown are reconstructed from z-space. At last (e) shows UMAP from z=10.

8

Under review as a conference paper at ICLR 2019

True VAE WAE PIE

Sharpness 0.18 0.08 0.07 0.49

Table 1: Results for experiment on sharpness on three different models and original images. For all three models a sample of 8 dimensions was taken. The generated images where convolved with Laplace filter and then the variance of activations was averaged over 10000 samples images. Higher values are better.

Fig. 7d shows for each model the linear interpolation from one latent space to another. Both lower values of 2 (0.01, 0.1) show digits that are quite accurate. When the constraint is loosened to
= 1.0 the interpolation is unable to show distinct values. This experiment shows that tightening the constraint by decreasing 2 increases the power of the manifold learned by the model. This is shown again in Fig. 7e where we diminished the number of dimensions even further from R10 to R2 utilizing UMAP (McInnes & Healy, 2018). With 2 = 1.0 UMAP created a manifold with a good Gaussian distribution. However, from the manifold created by PIE it was not able to separate distinct digits from each other. Tightening the constraint with a lower 2 moves the manifold created by UMAP further away from a Gaussian distribution, while it is better able to seperate classes from each other.
5.2 IMAGE SHARPNESS
It is a well-known problem in VAEs that generated images are smoothened. WAE (Tolstikhin et al., 2017) improves over VAEs by minimizing the optimal transport. To test the sharpness of generated images we convolve the grey-scaled images with the Laplace filter. This filter acts as an edge detector. We compute the variance of the activations and average them over 10000 sampled images. If an image is blurry, it means there are less edges and thus more activations will be close to zero, leading to a smaller variance. In this experiment we compare the sharpness of images generated by PIE with WAE, VAE and the sharpeness of the original images. For VAE and WAE we take the architecture as described in Radford et al. (2015). For PIE we take the architecture as described in section 5.1.
Table 1 shows the results for this experiment. PIE outperforms both VAE and WAE in terms of sharpeness of generated images. Images generated by PIE are even more sharp then original images from the MNIST dataset. An explanation for this is the use of a checkerboard pattern in the downsampling layer of the PIE convolutional block. With this technique we capture intrinsic properties of the data and are thus able to reconstruct sharper images.
6 CONCLUSION
In this paper we have proposed the new class of Auto Encoders, which we call Pseudo Invertible Encoder. We provided a theory which bridges the gap between Auto Encoders and Normalizing Flows. The experiments demonstrate that the proposed model learns the manifold structure and generate sharp images.
7 ACKNOWLEDGEMENTS
One of the authors is sponsored by STW project "Imagine: in search for the unknown".
REFERENCES
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components estimation. CoRR, abs/1410.8516, 2014. URL http://arxiv.org/abs/1410.8516.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. CoRR, abs/1605.08803, 2016. URL http://arxiv.org/abs/1605.08803.
9

Under review as a conference paper at ICLR 2019
Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2214­ 2224. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 6816-the-reversible-residual-network-backpropagation-without-storing-activations. pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Alston S. Householder. Unitary triangularization of a nonsymmetric matrix. J. ACM, 5(4):339­ 342, October 1958. ISSN 0004-5411. doi: 10.1145/320941.320947. URL http://doi.acm. org/10.1145/320941.320947.
Jo¨rn-Henrik Jacobsen, Arnold W. M. Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks. CoRR, abs/1802.07088, 2018. URL http://arxiv.org/abs/1802.07088.
D. Jimenez Rezende and S. Mohamed. Variational Inference with Normalizing Flows. ArXiv eprints, May 2015.
D. P. Kingma and P. Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. ArXiv e-prints, July 2018.
D. P Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints, December 2013. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980. L. McInnes and J. Healy. UMAP: Uniform Manifold Approximation and Projection for Dimension
Reduction. ArXiv e-prints, February 2018. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015. URL http:// arxiv.org/abs/1511.06434. D. E. Rummerhart. Learning internal representations by error propagation. Parallel Distributed Processing: I. Foundations, pp. 318­362, 1986. URL https://ci.nii.ac.jp/naid/ 10009703828/en/. Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In Information Theory Workshop (ITW), 2015 IEEE, pp. 1­5. IEEE, 2015. I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein Auto-Encoders. ArXiv e-prints, November 2017. Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein autoencoders. arXiv preprint arXiv:1711.01558, 2017.
10

