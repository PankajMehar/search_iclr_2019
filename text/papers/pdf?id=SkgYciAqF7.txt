Under review as a conference paper at ICLR 2019
SKIM-PIXELCNN
Anonymous authors Paper under double-blind review
ABSTRACT
Pixel convolutional neural network (PixelCNN) has provided promising results in image generation. However, it requires heavy computation time for inference, which deters its use in practice. Here, we propose a new generation method based on PixelCNN, dubbed Skim-PixelCNN that remarkably reduces inference time by skimming easy pixels. On top of a vanilla PixelCNN, we introduce two main components: an efficient generator that generates a set of next pixels in one shot and a confidence estimator that measures the confidence of the generated pixels. Based on the confidence, our model decides whether it skims or redraw the pixel using the vanilla PixelCNN. From the quantitative and qualitative experiments on diverse public image datasets, we show that our method can significantly reduce the computational overhead while its generation performance is comparable to or even improved that of the vanilla PixelCNN.
1 INTRODUCTION
Recent advances in auto-regressive (AR) models, such as pixel convolutional neural network (PixelCNN) (van den Oord et al., 2016c), have achieved impressive success in generating complex data including images (van den Oord et al., 2016c;b; Salimans et al., 2017; Dahl et al., 2017; Chen et al., 2018), video (van den Oord et al., 2016a), and audio (Tamamori et al., 2017). Compared to the other generative models, such as generative adversarial network (GAN) (Goodfellow et al., 2014) and variational autoencoder (VAE) (Kingma & Welling, 2014), the AR models are known to learn a tractable data distribution p(x) from a test example x and can be easily extended for both discrete and continuous data. Due to their nature, the AR models provide stable training while they alleviate the mode collapsing problem (van den Oord et al., 2016c). However, these models should sequentially infer each element of the data x  Rn×n requiring O(n2) times more than a simple point estimator such as GAN. This mostly limits the use of the AR models in practice despite their advantages.
To tackle the problem, we assume that all the pixels of an image are not equally important (Xiao et al., 2015; Fu et al., 2017; Cao et al., 2015; Vasilescu & Terzopoulos, 2002; Nam et al., 2017) and many pixels can be adequately generated by a simpler model (Seo et al., 2018) or observed from their nearby neighbors (Bansal et al., 2018). By following the assumption, we aim to reduce the generation time of the AR model by skimming the less essential pixels.
In this paper, we introduce a new PixelCNN-based AR approach that enhances the generation time by skimming the pixels, dubbed Skim-PixelCNN (Figure 1). The Skim-PixelCNN introduces a skimming module on top of the vanilla PixelCNN that consists of two components. One is an efficient generator that approximates the original pixel distribution of the vanilla PixelCNN given the observed pixel values so far. The other is a confidence estimator that measures how much we can believe the generated pixel. The confidence estimator decides whether we need to re-generate the current pixel or simply skip the computationally heavy sampling of the vanilla PixelCNN by using the pixel generated by our module.
We evaluate the proposed Skim-PixelCNN on widely used public image datasets such as CelebA (Liu et al., 2015) and the subset of ImageNet (Russakovsky et al., 2015). By incorporating our method with PixelCNN++ (Salimans et al., 2017), which is one of the state-of-the-art AR algorithms in image generation, we show that it is possible to skim a significant portion of images and speed up the process by a large margin. Interestingly, the skimming procedure even enhances the quality of the generated images to some extent in terms of Fre´chet Inception Distance score (FID) (Heusel et al., 2017).
1

Under review as a conference paper at ICLR 2019

Observed Region Generated Region Skimmed Region Inferred Result
Figure 1: Concept of Skim-PixelCNN. Left two figures are a training pair of a minibatch. Right two figures show the results of our Skim-PixelCNN. A skimmed region (white: do skim, black: do not skim) and the corresponding generated images are shown.

The main contributions of this work can be summarized as follows: (1) we introduce a new skimming module that can generate the pixels in an efficient way and provide the confidence of each pixel. (2) A unified training scheme of the skimming and the PixelCNN modules is proposed that can be easily augmented on top of the other PixelCNN variants giving an additional enhancement. By doing so, our model achieves the state of the art performance on the generation speed of the AR models. (3) Compared to the vanilla PixelCNN, Our model remarkably reduces the generation time by skimming while it even improves the image quality in terms of FID.

2 PRELIMINARY

2.1 AUTOREGRESSIVE GENERATIVE MODEL

Autoregressive generative model is a probabilistic model to assign a probability p(x) of an image x including n × n pixels. This method considers the image x as a sequence {xi|i = 1, · · · n2} where
xi is taken from x row by row, and the probability p(x) is defined by an AR manner as follows:

n2
p(x) = p(xi|x1, ..., xi-1),
i=1

(1)

From the formulation, the AR model provides a tractable data distribution p(x).

2.2 PIXEL CONVOLUTIONAL NEURAL NETWORK
To express the causal distribution in equation (1), a recurrent neural network (PixelRNN) or a convolutional network using a causal mask (PixelCNN) has been proposed to construct a neural model (van den Oord et al., 2016c). The mask is adapted to keep the network from observing the future contexts and hold the auto-regressive property. While the PixelCNN is known to be faster in both training and convergence, the generation results are less plausible than those by PixelRNN. To solve the problem, the variants of PixelCNN (van den Oord et al. (2016b); Salimans et al. (2017); Chen et al. (2018)) are proposed.
However, we note that the models still have to generate all the pixels sequentially. Hence, the inference speed is very slow compared to the other generation algorithm such as VAE (Kingma & Welling (2014)) and GAN (Goodfellow et al. (2014)).

3 PROPOSED METHOD
3.1 OVERVIEW
The proposed Skim-PixelCNN mainly consists of three parts: a fast and efficient generator, a confidence estimator and a vanilla PixelCNN. The former two networks are referred to as a skimming module. Incorporated with the PixelCNN, the generator of the skimming module is design to approximate the conditional data distribution that the PixelCNN learns from the image. The confidence estimator outputs the confidence values of the pixels generated by the fast generator. As shown in Figure 2, our model decides whether the PixelCNN should redraw the pixel of interest or skim it; i.e use the pixel that is generated by the fast generator as it is. The detailed explanation will be described in the following sections.

2

Under review as a conference paper at ICLR 2019

t
Skim-module

PixelCNN T Skim

t+1 PixelCNN
F Skim

....
t+2
Skim-module

....

Generation (t) Confidence (t)

Generation (t+2) Confidence (t+2)

Figure 2: Overview of the proposed Skim-PixelCNN. Given an observed region, we first generate the next pixels and calculate their confidence. If the confidence is higher than the threshold (t-th step), we skim the pixel. If the confidence is lower (t + 1-th step), the pixel is redrawn by the PixelCNN.

3.2 APPROXIMATING PIXEL DISTRIBUTION OF PIXELCNN

Given the pixel values xi = {x1, · · · , xi}, PixelCNN defines the distribution of the future pixels x(i,j] = {xi+1, · · · , xj} as follows:

j

p(x(i,j]|xi) =

p(xl|x1, ..., xl-1).

l=i+1

(2)

Here, n2 is the total number of pixels in the image and  denotes the parameters of PixelCNN. The index i, j are assumed to satisfy the condition j > i, i, j  [1, n2]. In PixelCNN, computing the
equation (2) takes linear time because all the previous pixel values are required to infer the distribution
of the current pixel.

To approximate the distribution p(x(i,j]|xi) in constant time, we introduce a set of prior pixels
z(i,j-1] = fW (xi), where we assume that they are i.i.d, given the observation xi. The variable z is referred to as the prior pixels in that it represents the previous pixels as well as the prior assumption of the pixels. Here, W is the model parameter of the fast generator fW (·). We design fW (·) to follow the distribution p(xl|x1, ..., xl-1) of PixelCNN. In practice, fW (·) has a U-net architecture (Ronneberger et al., 2015) that is an auto-encoder having skip-connections.

Based on this, we define an approximated distribution q(x(i,j-1]|xi, z(i,j-1]) as follows:

j

q,(x(i,j]|xi, z(i,j-1])  p(xi+1|xi)

q(xl|xi, zi+1, ..., zl-1).

l=i+2

(3)

Similar to PixelCNN, we use the masked CNN operations to model the inference of the conditional distribution q(·|·). Once the fast generator calculates the set of prior pixels z(i,j-1], we can infer the distribution of equation (3) in constant time because we can run the j - i + 1 masked CNN operations
simultaneously.

Although we can use a separate model for q(·|·), in practice, we simply used the conditional distribution of PixelCNN p(·|·) for computational efficiency. Therefore, the equation (3) becomes,

j

q,W (x(i,j]|xi, z(i,j-1])  p(xi+1|xi)

p(xl|xi, zi+1, ..., zl-1).

l=i+2

(4)

Note that the equality between p(x(i,j]|xi) and q,W (x(i,j]|xi, z(i,j-1]) would hold if z(i,j-1]( = fW (xi)) approaches to x(i,j-1]. To clearly show the dependency on W , we use q,W (x(i,j]|xi, fW (xi)) instead of q,W (x(i,j]|xi, z(i,j-1]) in the rest of the paper.

3

Under review as a conference paper at ICLR 2019

Then, we optimize the network parameters  and W by minimizing the negative log-likelihood (NLL) of q,W (x(i,j] = x^(i,j]|xi, fW (xi)) where x^(i,j] is a set of pixel values that are sampled from the PixelCNN. By minimizing the following loss function

min
,W

-Ep

(x(i,j]

|xi

)

[log

q,W

(x(i,j

]

|xi

,

fW

(xi

))],

(5)

we guide the fast generator fW (·) to generate the prior pixels z that is likely to be sampled from the parametric distribution of the PixelCNN.

Along with the equation (5), we also minimize the original NLL of the PixelCNN that makes the overall minimization task:

min
,W

-

log

p

(x((gi,)j]|xi)

-

Ep (x(i,j] |xi ) [log

q,W

(x(i,j]|xi,

fW

(xi))],

(6)

where x(g) denotes the ground truth pixel value in the generated region of the training image. Note that both p(x) and its approximated distribution q,W (x) approaches to the true data distribution p(x) when 1) our fast generator generates the prior pixels z close to the true pixels x and 2) the NLL of PixelCNN distribution approaches to the data distribution. Based on our analysis and extensive experiments, we show that our model can satisfy these conditions in both analytic and empirical perspectives. See the Appendix for the detailed information of the derivation and our algorithm.

3.3 CONFIDENCE ESTIMATION FOR SKIMMING

Using the fast generator fW (·), our model can generate the pixels based on the previous pixels. However, because the proposed method operates in AR manner, the approximation error can be accumulated and this may cause an unsuccessful image generation. To mitigate the problem, we introduce an auxiliary module that checks whether the generated pixel needs a re-touch or not. This function measures the confidence of the model and is referred to as a confidence estimator.

First, we define the confidence of the generated pixels as follows:

fk = q,W (xk = x^k|xi, fW (xi)),

(7)

where x^k  p(xk|xk-1) and k  {1, · · · , j}. The confidence value fk provides a measure of how likely the generated pixel from q,W (·) is drawn from p(xk|xk-1). Based on the confidence value
fk, our model decides whether it can skim the pixel xk or not. More specifically,

0, hk = 1,

1 - fk < , skim, 1 - fk  , draw,

(8)

where  [0, 1] is a gauge threshold. For example, when = 0, our model always redraws the pixel using PixelCNN no matter how our confidence is high. When = 1, our model always skims the pixel. We define a binary variable hk that scores the hardness of the generation task for the current pixel k. When the task is hard (hk = 1), our model switches the task to the PixelCNN for a better chance of the generation. Note that our model becomes a vanilla PixelCNN when hk = 1 for every k.
However, the confidence estimator in the current form is not useful in practice because we cannot calculate the confidence unless we explicitly infer the sampled value x^k in an auto-regressive manner; i.e. we need first to go forward using PixelCNN to see the next pixel and come backward to calculate the confidence to decide whether we use PixelCNN or not, which is nonsense.

To detour this problem, we introduce a network gV (·) that approximates the binary decision variable

hk as follows:

h(i,j] gV (xi, fW (xi)).

(9)

Here, h(i,j] = {hi+1, · · · , hj} denotes the set of binary decision from the confidence measure in equation (7). In our experiments, gV (·) is implemented by a U-net architecture with a sigmoid activation output that makes the equation (9) equivalent to the logistic regression problem.

3.4 TRAINING DETAILS
To train the proposed model, we randomly select the location v(i)  [1, n2] for each image x(i) in a training batch. Then, we generate l(i) = min(B, n2 - v(i)) pixels after v(i), where B denotes the

4

Under review as a conference paper at ICLR 2019

number of pixels the skimming module considers. To calculate equation (6), we minimize the loss of the training image x(k), k = 1 · · · N, and the locations v(i)  [1, n2] as,

min - 1 ,W N

N

log p(x(i)v(i)+l(i) )

-

1 NM

N

M

log

q,W

(x^((vi)((ij)

) ,v(i)

+l(i)

]

|x(i)v(i)

).

i=1 i=1 j=1

(10)

Here,

x^(i)(j)
(v (i) ,v (i) +l(i) ]

for

j



{1

·

·

·M}

denotes

M

number

of

the

generated

images

from

PixelCNN

distribution p(x(v(i),v(i)+l(i)]|x(i)v(i) ) for i-th training data. From the experiment, we found that M = 1 sample is enough to train the model. This training scheme guides the Skim-PixelCNN to fit

the original PixelCNN distribution as well as to generate a number of unseen pixels, simultaneously.

To train gV (·), binary cross-entropy loss is used with the target variable h in equation (9). For estimating V , we freeze other parameters during the back-propagation. See Appendix A for the
detailed procedure of the training process.

3.5 IMAGE GENERATION
The image generation using the proposed Skim-PixelCNN is conducted in a similar way to the conventional PixelCNN. The main difference is that the skimming stage is added by using the proposed fast generator and the confidence estimator. The generation procedure is mainly divided into two sub-procedures; an anchor generation and skimming procedure or PixelCNN sampling. To aid the generation, we infer the first K rows of an image using the vanilla PixelCNN and exploit the generated region as an anchor. K is a design parameter, but we found that K = 4 lines are enough for generating the full image. After generating the anchor pixels, we iterate the skimming procedure and the PixelCNN sampling to fill the remaining region. For each consecutive row, we first generate the pixels using the fast generator and estimate the confidence of the generated pixels. If the confidence f of the generated pixel is high (the hardness h = 0  1 - f < ), we skim the pixel or vice versa. This process is repeated until all the pixel values in the image are calculated. In the implementation, we found that it is sufficient to update the generated pixels and their confidence once per each row. See the Appendix A for the detailed procedure.

4 RELATED WORK
Recently, there have been a number of successful studies (Kingma & Welling, 2014; Goodfellow et al., 2014; van den Oord et al., 2016c) in inferring the image distribution and generating a new image based on the distribution. These generation methods can be largely categorized into three groups: VAE (Kingma & Welling, 2014; Rezende et al., 2014), GAN (Goodfellow et al., 2015; Radford et al., 2016) and autoregressive generation models (van den Oord et al., 2016c; Salimans et al., 2017).
The studies based on VAE (Gregor et al., 2015; Sohn et al., 2015; Walker et al., 2016) infer the image distribution conditioned on a latent random variable by approximating variational Bayes inference using a deep network. These methods utilize the powerful representation capacity of VAE to embed data information into the latent space. A number of works using GAN (Zhu et al., 2017; Kim et al., 2017; Reed et al., 2016a;c; Dong et al., 2017; Karras et al., 2018; Zhang et al., 2017) take advantage of the adversarial training where an image generator competes against a real/fake image discriminator that is modeled by neural networks. GAN has shown that it can generate the realistic images of high-resolution (Karras et al., 2018; Zhang et al., 2017). It is widely used in many applications such as style transfer (Kim et al., 2017; Zhu et al., 2017) and attribute-to-image generation (Reed et al., 2016a;c; Dong et al., 2017; Zhang et al., 2017).
On the other hand, auto-regressive (AR) image generation methods (van den Oord et al., 2016b;c; Salimans et al., 2017; Larochelle & Murray, 2011) offer a tractable way to infer the complex image distribution. Inspired by the restricted Boltzmann machine, (Larochelle & Murray, 2011) pioneered a neural network based AR method to describe the distribution of images. (van den Oord et al., 2016c) proposed AR based image generation methods that either uses an RNN or a CNN. Based on the method, many studies (van den Oord et al., 2016b; Reed et al., 2016b; Salimans et al., 2017; Dahl et al., 2017) have been published to improve the generation performance (Salimans et al., 2017) for various applications such as conditional generation (van den Oord et al., 2016b; Reed

5

Under review as a conference paper at ICLR 2019

pixels / sec bits / dim loss

Figure 3: Generated images of  {1.0, 0.8, 0.6, 0.4, 0.2, 0.0} on the CelebA dataset. The image in the left of the triplet denotes a skimmed region (white: do skim, black: do not skim), the mid is a confidence map (red: high, blue: low) and the right shows the generated result. See the Appendix D.2 for more generated images.

Generation speed (except Anchors)

Skimming ratio

600 59.4x 1.0

480 0.8

360 0.6

240 0.4

120 0

1.0x 1.2x 1.7x 2.4x 3.1x 4.1x 5.2x 5.9x 7.4x 11.0x 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

0.2 0.0

gauge threshold ()

(a) Generation speed (pixels/sec) over  [0.0, 1.0].

No skim

Full skim

3.5

3.2

2.9

2.6

2.3 0

10 20 30 40 50
epochs

(b) NLL Curve.

2.6×10$%

Test  / dim

2.3
2.0
1.7
1.4 0

10 20 30 40 50
epochs

(c) 1 loss curve.

Figure 4: Quantitative analysis of the proposed Skim-PixelCNN on CelebA dataset. (a) shows the generation speed over the threshold . (b) shows the NLL of the inference with and without skimming. (c) presents the 1-loss between the prior z and the pixel value x for every epoch.

et al., 2016b) and super-resolution (Dahl et al., 2017). PixelCNN has many potentially positive perspectives such as its powerful interpretability, tractable inference and stable learning. However, due to its auto-regressive nature, AR models suffer from a slow sequential inference which limits the application of PixelCNN in a real situation. To resolve the problem, we proposed the skimming method to boost the inference speed of the PixelCNN that can expand the potential applicability of the method.

5 EXPERIMENTS
In this section, we demonstrate the image generation performance of the proposed Skim-PixelCNN algorithm. We mainly analyze the inference speed and image generation quality of the proposed method in the various datasets.
5.1 EXPERIMENTAL SETTING
We used PixelCNN++ (Salimans et al., 2017) as our base model. The number of channels of the network was set to 160 and the number of logistic mixtures was set to 10. See (Salimans et al., 2017) for the detailed explanation of the parameters. The fast generator fW (·) and the confidence estimator gV (·) were both implemented by U-net structured autoencoder (Ronneberger et al., 2015) with four 4 × 4 convolution layers and four 4 × 4 convolution-transpose layers. The channel size of each convolution filter was chosen to be {64, 128, 256, 512} and that of the convolution-transpose filter was set to be {256, 128, 64, 3}. We used batch normalization (Ioffe & Szegedy, 2015) for both networks and stride is set to two for all the filters. To train the model, we used Adam optimizer (Kingma & Ba, 2015) with the default parameters as mentioned in the original paper, i.e., learning rate = 0.0001, 1 = 0.9 and 2 = 0.999. Every module was trained from scratch. Also, we tested implementations from both the original PixelCNN++ and the fast-PixelCNN++ (Ramachandran et al., 2017).
We mainly used CelebA (Liu et al., 2015) and CIFAR-10 dataset (Krizhevsky, 2009) for test. For CelebA dataset, we randomly pick 36, 000 images for training and 3, 000 images for validation. We

6

Under review as a conference paper at ICLR 2019

Table 1: The speed up and visual quality of the results on CelebA dataset. We measure the speed-up of the inference time with (SU) and without anchors (SU \A) compared to that of PixelCNN++. Skimming ratio (SR) without anchors and FID (lower is better) are also displayed. We measured the inference time without performing the skimming module for = 0 case.

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
SU 1.0x 1.2x 1.7x 2.2x 2.8x 3.5x 4.1x 4.5x 5.3x 6.8x 12.9x SU \A 1.0x 1.2x 1.7x 2.4x 3.1x 4.1x 5.2x 5.9x 7.4x 11.0x 59.4x SR (%) 0.0 15.3 43.0 59.3 69.1 77.1 82.1 84.5 88.0 92.5 100 FID 56.8 50.1 45.1 42.5 41.6 42.3 43.4 45.9 48.7 53.9 85.0

Generation time for 64 × 64 image

Inference time ( = 0.0)

373.06 sec (6 min 13 sec)

Inference time without anchors ( = 0.0) 326.43 sec (5 min 26 sec)

generated 64 × 64 images for CelebA dataset and generated 32 × 32 images for CIFAR-10 dataset. The proposed model was trained by using four to eight GPUs (NVIDIA Tesla P40). The generation results using ImageNet (Russakovsky et al., 2015) dataset are also attached in the Appendix D.2. For quantitative evaluation, we employed negative log-likelihood (NLL) and Fre´chet Inception Distance score (FID) (Heusel et al., 2017) as a measure of the model performance and visual quality of the generated images, respectively. We computed NLL from the test dataset and measured FID on the 10,000 generated images for each . NLL was computed only for  {0, 1} because otherwise an exact pixel should be auto-regressively sampled for the input of our skimming module.
5.2 ANALYSIS
Figure 3 shows that the proposed Skim-PixelCNN successfully generates images despite a significant amount of skimming (white region). We observed that the confidence was mostly low (blue) in the eyes, mouth and boundary regions of the face where the PixelCNN is used to generate those regions. This shows that the model finds it relatively hard to describe the details compared to the other homogeneous regions of the image that matches with our intuition. This result supports our assumption that the PixelCNN inference on the complicated pixels helps the skimming module to more precisely generate the future pixels. More results are reported in the Appendix D.
The graphs in Figure 4 present the quantitative analysis regarding the inference time and the NLL of the proposed Skim-PixelCNN. In Figure 4a, the relation between inference time and the skimming ratio is reported. The results show that the inference speed is significantly improved as the more pixels are skimmed. Table 1 further supports this that our Skim-PixelCNN generates a fairly good quality of images while it speeds up the generation procedure 5  10 times faster than the base model. Note that it takes over 6 minutes to generate a single 64 × 64 image via the vanilla PixelCNN++. By employing the proposed Skimming scheme, the generation time can be reduced to less than a minute.
Surprisingly, we found that the skimming module can improve the perceptual visual quality of the generated images compared to the vanilla PixelCNN++ (Table 1). The Skim-PixelCNN benefits from increasing the skimming ratio to some extent in terms of FID showing a U-shaped trend over the variation. Note that a lower FID score identifies a better model. One possible explanation for this observation is that the fast generator learns the mean-prior of the images, and it guides the AR model to prevent generating erroneous images. The confidence maps and the graph illustrated in Figure 3, 4a, and 4c support this conjecture. Complex local details such as eyes, mouths and contours have largely harder than the backgrounds and remaining faces. Moreover, constantly decreasing 1 distance between the results from our fast generator and the test data shows that our fast generator captures the overall distribution of the images as training goes on.
In Figure 4b and Figure 4c, the graphs show the results supporting the convergence of the proposed method. The graph in Figure 4b shows the NLL of the base PixelCNN++ and that of our proposed method under the full-skim case, i.e. we fully believe the fast generator for entire pixels. Note that the NLL of both cases converged and the PixelCNN achieved noticeably lower NLL compared to the fully skimming the pixels at every epoch. This is already expected in Section 3.2 that the PixelCNN++ approaches more closely to the data distribution than the skimming module. This

7

Under review as a conference paper at ICLR 2019

Table 2: The generation time per image and the skimming ratio over using Fast-PixelCNN++ (Ramachandran et al., 2017). CelebA is tested with the image size of 32×32 to compare CIFAR-10. The generation time of Fast-PixelCNN++ (1.0x) is 23.8 sec.

Dataset

CIFAR-10 32 × 32

CelebA 32 × 32

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
SU 1.0x 1.2x 1.4x 1.6x 2.1x 4.9x 1.0x 1.5x 2.9x 4.2x 4.6x 4.9x SU \A 1.0x 1.2x 1.5x 1.7x 2.4x 11.1x 1.0x 1.6x 3.2x 4.3x 6.8x 11.1x SR (%) 0.00 24.9 36.5 43.7 57.0 87.5 0.00 41.0 64.8 77.8 83.2 87.5

Table 3: Negative log-likelihood (NLL) of our Skim PixelCNN on CIFAR-10 and CelebA Datasets. Pix, No and Full each denotes PixelCNN++, No-skim and Full-skim, respectively.
Dataset CIFAR-10 32 × 32 CelebA 32 × 32 CelebA 64 × 64
Method Pix No Full Pix No Full Pix No Full NLL 2.92 2.93 3.78 2.72 2.71 3.24 2.47 2.40 2.87

supports the necessity of re-generating procedure by using the PixelCNN++, especially when the skimming module finds the pixel has a low confidence.
The graph in Figure 4c presents the 1 distance between the generated prior pixel z and the corresponding ground-truth pixel x in the test data reconstruction. Interestingly, although our algorithm does not explicitly enforce z to get close to x, z approaches to x as the model converges. Combined with the result in Figure 4b, this result supports the convergence conditions claimed in section 3.2. Regarding the convergence, we compared the NLL of the converged PixelCNN++ distribution from the proposed scheme and that of PixelCNN++ with CelebA dataset from the original paper (Salimans et al., 2017). Table 3 shows the NLL for CIFAR-10 and CelebA dataset. For all the datasets, we can see that our method can make the NLL converges closely to that of PixelCNN++ in the no-skim scenario. Both models were trained from scratch. This result shows that our training method can approach the data distribution comparable to PixelCNN++.
5.2.1 FAST GENERATION MODULE
To show that our module can be easily combined on top of the other pixelCNN variants, we incorporated our skimming method to the fast version of the PixelCNN++ algorithm (Ramachandran et al., 2017). Fast PixelCNN++ is the state-of-the-art (SOTA) among the PixelCNN variants that caches the previously gathered information to enhance the generation speed. Table 2 shows the generation time using the incorporated model (skim+Fast PixelCNN++). It is noteworthy that we could further push the limit beyond the speed of the baseline algorithm by incorporating the proposed skimming module. From the results, we showed the fastest generation performance of the kind. Generated CIFAR-10 samples by (skim+Fast PixelCNN++) are in Appendix D.2. From the images, we confirmed that the proposed method could generated image with skimming, despite that CIFAR-10 is more difficult to learn where to skim compared to CelebA because of its diverse objects and patterns.
6 CONCLUSION
In this paper, we proposed the efficient PixelCNN-based auto-regressive (AR) method, dubbed SkimPixelCNN. By introducing the skimming module, Skim-PixelCNN can decide whether it will generate a pixel using a computationally light model or a heavy but precise AR model. Because our module is not restricted to a specific model, it can be easily applied on top of the other PixelCNN variants and provides an additional speed enhancement that achieved SOTA performance. The quantitative and qualitative experiments on the various image datasets support that the proposed skimming procedure can remarkably enhance the slow image generation of the AR models while it even improves the image quality over the base model.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Aayush Bansal, Yaser Sheikh, and Deva Ramanan. PixelNN: Example-based image synthesis. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=Syhr6pxCW.
Chunshui Cao, Xianming Liu, Yi Yang, Yinan Yu, Jiang Wang, Zilei Wang, Yongzhen Huang, Liang Wang, Chang Huang, Wei Xu, Deva Ramanan, and Thomas S. Huang. Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks. In The IEEE International Conference on Computer Vision (ICCV), 2015.
XI Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregressive generative model. In Proceedings of the 35th International Conference on Machine Learning, pp. 864­872, 2018. URL http://proceedings.mlr.press/v80/chen18h. html.
Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In The IEEE International Conference on Computer Vision (ICCV), 2017.
Hao Dong, Simiao Yu, Chao Wu, and Yike Guo. Semantic image synthesis via adversarial learning. In The IEEE International Conference on Computer Vision (ICCV), 2017.
Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. DRAW: A recurrent neural network for image generation. In Proceedings of the 32nd International Conference on Machine Learning, pp. 1462­1471, 2015. URL http://proceedings.mlr.press/v37/ gregor15.html.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems 30, pp. 6626­6637, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, Proceedings of Machine Learning Research, pp. 448­456, 2015. URL http: //proceedings.mlr.press/v37/ioffe15.html.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hk99zCeAb.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 1857­1865, 2017. URL http: //proceedings.mlr.press/v70/kim17a.html.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
9

Under review as a conference paper at ICLR 2019
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29­37, 2011. URL http://proceedings.mlr.press/v15/larochelle11a.html.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In The IEEE International Conference on Computer Vision (ICCV), 2015.
Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual attention networks for multimodal reasoning and matching. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations, 2016.
Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang, Yang Zhang, Mark A Hasegawa-Johnson, Roy H Campbell, and Thomas S Huang. Fast generation for convolutional autoregressive models. arXiv preprint arXiv:1704.06001, 2017.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In Proceedings of The 33rd International Conference on Machine Learning, pp. 1060­1069, 2016a. URL http://proceedings.mlr. press/v48/reed16.html.
Scott Reed, Aa¨ron van den Oord, Nal Kalchbrenner, Victor Bapst, Matt Botvinick, and Nando de Freitas. Generating interpretable images with controllable structure. 2016b.
Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. Learning what and where to draw. In Advances in Neural Information Processing Systems, pp. 217­225, 2016c.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, pp. 1278­1286, 2014. URL http://proceedings.mlr. press/v32/rezende14.html.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234­241. Springer, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211­252, 2015.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the pixelCNN with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum? id=BJrFC6ceg.
Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Neural speed reading via skim-RNN. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=Sy-dQG-Rb.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In Advances in Neural Information Processing Systems, pp. 3483­3491, 2015.
Akira Tamamori, Tomoki Hayashi, Kazuhiro Kobayashi, Kazuya Takeda, and Tomoki Toda. Speakerdependent wavenet vocoder. In Proceedings of Interspeech, pp. 1118­1122, 2017.
Aa¨ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016a.
10

Under review as a conference paper at ICLR 2019
Aa¨ron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelCNN decoders. In Advances in Neural Information Processing Systems, pp. 4790­4798, 2016b.
Aa¨ron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In Proceedings of The 33rd International Conference on Machine Learning, pp. 1747­1756, 2016c. URL http://proceedings.mlr.press/v48/oord16.html.
M Alex O Vasilescu and Demetri Terzopoulos. Multilinear analysis of image ensembles: Tensorfaces. In European Conference on Computer Vision (ECCV), pp. 447­460. Springer, 2002.
Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert. An uncertain future: Forecasting from static images using variational autoencoders. In European Conference on Computer Vision (ECCV), pp. 835­851. Springer, 2016.
Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, and Zheng Zhang. The application of two-level attention models in deep convolutional neural network for fine-grained image classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N. Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In The IEEE International Conference on Computer Vision (ICCV), 2017.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In The IEEE International Conference on Computer Vision (ICCV), 2017.
11

Under review as a conference paper at ICLR 2019

Appendices

A TRAINING AND GENERATION DETAILS

Algorithm 1 Training procedure of the Skim-PixelCNN.

Require: Training image (x(i)), i = 1...N

1: Initialize network parameters 0, W0, and V0.

2: repeat

3: v(i)  U nif orm([1, n2]).

4: k(i) = v(i) + l(i), l(i) = min (B, n2 - v(i)).

5:

x~(i)
k(i) :v (i)



p (xk(i()i):v(i) |x(i)k(i) ).

6:

z(i)
k(i) :v (i)



fW (x(i)k(i) ).

7:

f (i)
k(i) :v (i)

=

q,W (·|x(i)k(i) ).

8:

f (i)(d)
k(i) :v (i)



f (i)
k(i) :v (i)

by

equation

(8).

9:

Update

, W

by

equation

(10)

using

x~k(i()i)

:v(i)

,

z(i)
k(i)

:v(i)

.

10:

Update V

by

equation

(9)

using

f (i)(d)
k(i) :v (i)

.

11: until converge

Algorithm 2 Generation procedure of the Skim-PixelCNN.
Require: Trained network parameters , W, V . 1: Initialize empty image d(i) = 0, i  [1, R  C]. 2: i = 1.
3: 4: # Generate Anchor images 5: Set d[1] randomly. 6: repeat 7: d[i + 1]  p(x|d[ i]). 8: i  i + 1. 9: until i  A (4  C in the paper),
10: 11: # Skim-PixelCNN inference 12: repeat 13: if mod (i, C) == 0 then 14: Set l = min(B, n2 - i). 15: p[i : i + l] = fW (d[ i]). 16: c[i : i + l] = gV (p[i : i + l]). 17: end if 18: k  1 19: repeat 20: if c[i + k] < then 21: d[i + k]  p(x|d[ i + k]). 22: else 23: d[i + k] = p[i + k]. 24: end if 25: k  k + 1. 26: until k  l 27: i  k + k 28: until i  R  C

To supplement the explanation for the training and inference of the proposed Skim-PixelCNN, this

section presents the pseudo-codes of the processes in Algorithm (1) and (2). In Algorithm (1), the

term

f (i)(d)
k(i) :v (i)

denotes

the

binary

decision

vector

for

the

image

region

x .(i)
k(i) :v (i)

See

Section

3

for

other

12

Under review as a conference paper at ICLR 2019

Figure 5: Network framework of skimming module.

notations and equations. In Algorithm (2), operations in line 14 to 18 describes the fast-generation procedure for skimming. We confirmed from the experiments that it is enough to update the generation results for each row. We note that the index i searches every RC number of pixels, and conduct PixelCNN inference for every pixel if no skimming occurred. The term l denotes the number of future pixels we generate using the proposed skimming module. B denotes the number of the pixel the skimming module considers.

B SUPPLEMENTARY EXPLANATION ON THE PROPOSED APPROXIMATION

In this section, we show that the cost function of the Skim-PixelCNN is a regularized version of the original PixelCNN. At the extremum, the approximated pixel distribution Skim-PixelCNN is equivalent to that of the original PixelCNN. In Skim-PixelCNN, our approximate distribution q(x(i,j+1]|xi, z(i,j]) can be reformulated as follows:

j+1

q,(x(i,j+1]|xi, z(i,j])  p(xi+1|xi)

q(xl|xi, zi+1, ..., zl-1)

(11)

l=i+2

= p(xi+1) · q(xi, zi+1, xi+2) ·, · · · , · q(xi, z(i,j-1], xj ) · q(xi, z(i,j], xj+1)

p (xi )

q(xi, zi+1)

q(xi, z(i,j-1])

q(xi, z(i,j])

(12)

= p(xi+1) · p(xi+2) ·, · · · , · p(xj+1) · p(xi+1) ·, · · · , · q(xi, z(i,j], xj+1)

p(xi) p(xi+1)

p(xj ) q(xi, zi+1)

p (xj +1 )

p (x(i,j+1]|xi)

R(p ,q,z(i,j])

(13)

Therefore, our proposed cost function can be represented as the negative log-likelihood of the PixelCNN with a regularizer - log R(p, q, z(i,j]):

min
,

-Ep

(x(i,j+1]

|xi

)

[log

p

(x(i,j+1]

|xi

))

+

log

R(p

,

q

,

z(i,j

]

)]

(14)

Note that the proposed cost function is equivalent to that of the original PixelCNN when

log R(p, q, z(i,j]) = 0, which is true under the condition of z(i,j] = x(i,j] and q(·|·) = p(·|·).

Here, z(i,j] = fW (xi). By minimizing the equation (14), R(p, q, z(i,j]) enforces the direction

of the optimization to estimate the probability ratio of q and p while it minimize the gap between

q (xi ,z(i,j ] ,xj +1 ) q (xi ,z(i,j ] )

so

that

z(i,j]

=

fW (xi)

approaches

to

x(i,j].

C IMPLEMENTATION DETAILS

This section shows detailed framework of the skimming module: a fast generator and a confidence estimator. The fast generator and the confidence estimator consist of identical figure except the last block, as in Figure 5. The last activation of the fast generator is tangent hyperbolic function and that of confidence estimator is defined as sigmoid function. Also, the output channel number of the last block are set to 3 and 1, respectively. The detailed information for the channel, filter, and stride configuration is described in Section 5.1

13

Under review as a conference paper at ICLR 2019

Table 4: Inference time speedup (SU), inference time except anchors speedup (SU \A), skimming ratio (SR) for varying and percentile, batch size = 100, we report the average of experiments.

PixelCNN++

Inference Time

27785.1 sec (7 hour 41 min)

Inference Time except anchors 26048.5 sec (7 hour 14 min)

Percentile
1.0 (max) 0.9 0.8 0.7 0.6 0.5 (median) 0.4 0.3 0.2 0.1 0.0 (min)

SU
1.4x 3.5x 5.8x 7.1x 8.0x 8.9x 9.2x 10.0x 10.9x 11.9x 12.9x

= 0.9
SU \A
1.4x 4.2x 8.6x 12.0x 15.0x 18.7x 20.2x 25.2x 32.2x 43.7x 63.0x

SR
30.8 77.9 89.9 93.3 94.9 96.2 96.6 97.6 98.5 99.3 100.0

SU
1.3x 3.0x 4.3x 5.9x 6.5x 7.2x 7.9x 8.8x 9.3x 10.8x 12.9x

= 0.8
SU \A
1.3x 3.5x 5.6x 8.7x 10.4x 12.4x 14.8x 18.2x 20.7x 30.9x 62.4x

SR
23.6 72.8 83.7 90.1 92.0 93.5 94.8 96.1 96.7 98.3 100.0

SU
1.2x 2.4x 3.2x 4.4x 5.4x 6.4x 6.9x 7.5x 8.2x 9.7x 12.6x

= 0.7
SU \A
1.2x 2.6x 3.8x 5.7x 7.7x 10.0x 11.4x 13.4x 15.9x 23.0x 56.7x

SR
15.5 63.6 74.9 83.9 88.7 91.6 92.8 94.1 95.3 97.2 99.8

= 0.6

= 0.5

= 0.4

Percentile SU SU \A SR SU SU \A SR SU SU \A SR

1.0 (max) 0.9 0.8 0.7 0.6 0.5 (median) 0.4 0.3 0.2 0.1 0.0 (min)

1.0x 2.2x 2.7x 3.3x 4.1x 4.9x 5.8x 6.6x 7.5x 8.7x 12.3x

1.0x 2.3x 3.1x 3.9x 5.2x 6.7x 8.6x 10.6x 13.3x 18.0x 49.5x

3.2 1.0x 1.0x 58.8 1.8x 1.9x 69.0 2.2x 2.4x 75.9 2.6x 2.9x 82.3 3.1x 3.6x 86.6 3.7x 4.5x 89.9 4.5x 5.8x 92.2 5.2x 7.3x 94.1 6.3x 9.8x 96.0 7.4x 13.1x 99.6 11.1 34.3

4.1 1.0x 1.0x 48.3 1.5x 1.5x 60.8 1.7x 1.8x 67.0 2.2x 2.4x 73.7 2.4x 2.7x 79.4 2.8x 3.1x 84.3 3.3x 3.9x 87.9 3.9x 4.8x 91.4 4.6x 6.1x 93.9 6.1x 9.3x 98.7 10.1 25.4

1.3 36.1 45.1 59.1 64.7 69.8 75.7 80.8 85.2 90.8 97.6

D SUPPLEMENTARY RESULTS
D.1 PARALLEL INFERENCE
Baseline PixelCNN method can enjoy parallel inference for multiple images supported by Graphics Processing Unit (GPU), like other deep learning algorithms. In the proposed Skim-PixelCNN, the parallel inference is little tricky because the skimming decision for the pixel should be different among images. Therefore, to utilize the parallel calculation, we uniformly use a confidence map which is a maximum of the confidence map for every image in the batch. Clearly, this is the upper bound of the skimming and does not harm the generation performance. However, PixelCNN sometimes inevitably samples unsuccessful images and in this case, the Skim-PixelCNN cannot sufficiently skim the region. To avoid the situation, we set the final confidence value of the pixel as an upper few percentiles of the all the confidence value of the pixel for every batch. The figure shows the example of the images. From the results, we confirmed that this parallelization does not harm the generation quality of the proposed method. Table 4 shows inference time (except anchors) speedup and skimming ratio for varying and percentiles.
14

Under review as a conference paper at ICLR 2019

Figure 6: Generated images from CIFAR-10 dataset. Left: Generated images by skimming, Mid: Skimmed region, Right: PixelCNN++ Generation result.

(a) Balloon Flower

(b) Ladybug

(c) Tree-martin

(d) Pizza

(e) Sorrel

(f) Brown bear

(g) Sandbar

Figure 7: ImageNet generation examples (3).

(h) Coral reef

D.2 ADDITIONAL GENERATION RESULTS
In addition to the results presented in the paper, we show supplement generation examples in below figures. Figure 8 and 9 shows the additional facial image generation results by Skim-PixelCNN among
 [1.0, 0.0]. Figure 6 shows the CIFAR-10 image generation result with the proposed method trained in unsupervised manner. In Figure 7, 10 and 11, additional ImageNet generation examples are presented with larger image size. Also, we show the generation results with parallel inference in Figure 13, which follows the method described in Section D.1.

15

Under review as a conference paper at ICLR 2019

Figure 8: More selected CelebA generation result in 64 × 64. according to {1.0, 0.9, 0.8, 0.7, 0.6, 0.5}.
16



Under review as a conference paper at ICLR 2019
Figure 9: More selected CelebA generation result in 64×64. according to  {0.4, 0.3, 0.2, 0.1, 0.0}. 17

Under review as a conference paper at ICLR 2019
(a) Balloon Flower (b) Ladybug
(c) Tree-martin (d) Pizza
Figure 10: ImageNet generation examples (1). 18

Under review as a conference paper at ICLR 2019
(a) Sorrel (b) Brown bear
(c) Sandbar (d) Coral Figure 11: ImageNet generation examples (2).
Figure 12: Generated images by PixelCNN++ with batch size 100 19

Under review as a conference paper at ICLR 2019

(a) Attention heatmap for = 0.4 and p = 90. (b) Generated images for = 0.4 and p = 90.

(c) Attention heatmap for = 0.8 and p = 90. (d) Generated images for = 0.8 and p = 90.

(e) Attention heatmap for = 0.4 and p = 30. (f) Generated images for = 0.4 and p = 30.

(g) Skimmed region of 13b. (h) Skimmed region of 13d. (i) Skimmed region of 13f.

Figure 13: Confidence map, generated images and skimmed region for varying parameters with batch

size = 100.

20

