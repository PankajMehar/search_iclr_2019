Under review as a conference paper at ICLR 2019
Efficient Dictionary Learning with Gradient Descent
Anonymous authors Paper under double-blind review
Abstract
Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem ­ complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well.
1 Introduction
Many central problems in machine learning and signal processing are most naturally formulated as optimization problems. These problems are often both nonconvex and highdimensional. High dimensionality makes the evaluation of second-order information prohibitively expensive, and thus randomly initialized first-order methods are usually employed instead. This has prompted great interest in recent years in understanding the behavior of gradient descent on nonconvex objectives (18; 14; 17; 11). General analysis of first- and second-order methods on such problems can provide guarantees for convergence to critical points but these may be highly suboptimal, since nonconvex optimization is in general an NP-hard probem (4). Outside of a convex setting (28) one must assume additional structure in order to make statements about convergence to optimal or high quality solutions. It is a curious fact that for certain classes of problems such as ones that involve sparsification (25; 6) or matrix/tensor recovery (21; 19; 1) first-order methods can be used effectively. Even for some highly nonconvex problems where there is no ground truth available such as the training of neural networks first-order methods converge to high-quality solutions (40).
Dictionary learning is a problem of inferring a sparse representation of data that was originally developed in the neuroscience literature (30), and has since seen a number of important applications including image denoising, compressive signal acquisition and signal classification (13; 26). In this work we study a formulation of the dictionary learning problem that can be solved efficiently using randomly initialized gradient descent despite possessing a number of saddle points exponential in the dimension. A feature that appears to enable efficient optimization is the existence of sufficient negative curvature in the directions normal to the stable manifolds of all critical points that are not global minima 1. This property ensures that the regions of the space that feed into small gradient regions under gradient flow do not dominate the parameter space. Figure 1 illustrates the value of this property: negative curvature prevents measure from concentrating about the stable manifold. As a consequence randomly initialized gradient methods avoid the "slow region" of around the saddle point.
1As well as a lack of spurious local minimizers, and the existence of large gradients or strong convexity in the remaining parts of the space
1

Under review as a conference paper at ICLR 2019
Figure 1: Negative curvature helps gradient descent. Red: "slow region" of small gradient around a saddle point. Green: stable manifold associated with the saddle point. Black: points that flow to the slow region. Left: global negative curvature normal to the stable manifold. Right: positive curvature normal to the stable manifold ­ randomly initialized gradient descent is more likely to encounter the slow region.
The main results of this work is a convergence rate for randomly initialized gradient descent for complete orthogonal dictionary learning to the neighborhood of a global minimum of the objective. Our results are probabilistic since they rely on initialization in certain regions of the parameter space, yet they allow one to flexibly trade off between the maximal number of iterations in the bound and the probability of the bound holding.
While our focus is on dictionary learning, it has been recently shown that for other important nonconvex problems such as phase retrieval (8) performance guarantees for randomly initialized gradient descent can be obtained as well. In fact, in Appendix C we show that negative curvature normal to the stable manifolds of saddle points (illustrated in Figure 1) is also a feature of the population objective of generalized phase retrieval, and can be used to obtain an efficient convergence rate.
2 Related Work
Easy nonconvex problems. There are two basic impediments to solving nonconvex problems globally: (i) spurious local minimizers, and (ii) flat saddle points, which can cause methods to stagnate in the vicinity of critical points that are not minimizers. The latter difficulty has motivated the study of strict saddle functions (36; 14), which have the property that at every point in the domain of optimization, there is a large gradient, a direction of strict negative curvature, or the function is strongly convex. By leveraging this curvature information, it is possible to escape saddle points and obtain a local minimizer in polynomial time.2 Perhaps more surprisingly, many known strict saddle functions also have the property that every local minimizer is global; for these problems, this implies that efficient methods find global solutions. Examples of problems with this property include variants of sparse dictionary learning (38), phase retrieval (37), tensor decomposition (14), community detection (3) and phase synchronization (5).
Minimizing strict saddle functions. Strict saddle functions have the property that at every saddle point there is a direction of strict negative curvature. A natural approach to escape such saddle points is to use second order methods (e.g., trust region (9) or curvilinear search (15)) that explicitly leverage curvature information. Alternatively, one can attempt to escape saddle points using first order information only. However, some care is needed: canonical first order methods such as gradient descent will not obtain minimizers if initialized at a saddle point (or at a point that flows to one) ­ at any critical point, gradient descent simply stops. A natural remedy is to randomly perturb the iterate whenever needed. A line of recent works shows that noisy gradient methods of this form efficiently optimize strict saddle functions (24; 12; 20). For example, (20) obtains rates on strict saddle functions that match the optimal rates for smooth convex programs up to a polylogarithmic dependence on dimension.3
2This statement is nontrivial: finding a local minimum of a smooth function is NP-hard. 3This work also proves convergence to a second-order stationary point under more general smoothness assumptions.
2

Under review as a conference paper at ICLR 2019

Randomly initialized gradient descent? The aforementioned results are broad, and nearly optimal. Nevertheless, important questions about the behavior of first order methods for nonconvex optimization remain unanswered. For example: in every one of the aforemented benign nonconvex optimization problems, randomly initialized gradient descent rapidly obtains a minimizer. This may seem unsurprising: general considerations indicate that the stable manifolds associated with non-minimizing critical points have measure zero (29), this implies that a variety of small-stepping first order methods converge to minimizers in the large-time limit (23). However, it is not difficult to construct strict saddle problems that are not amenable to efficient optimization by randomly initialized gradient descent ­ see (12) for an example. This contrast between the excellent empirical performance of randomly initialized first order methods and worst case examples suggests that there are important geometric and/or topological properties of "easy nonconvex problems" that are not captured by the strict saddle hypothesis. Hence, the motivation of this paper is twofold: (i) to provide theoretical corroboration (in certain specific situations) for what is arguably the simplest, most natural, and most widely used first order method, and (ii) to contribute to the ongoing effort to identify conditions which make nonconvex problems amenable to efficient optimization.

3 Dictionary Learning over the Sphere

Suppose we are given data matrix Y = y1, . . . yp  Rn×p. The dictionary learning problem asks us to find a concise representation of the data (13), of the form Y  AX, where X is a sparse matrix. In the complete, orthogonal dictionary learning problem, we restrict the matrix A to have orthonormal columns (A  O(n)). This variation of dictionary learning is useful for finding concise representations of small datasets (e.g., patches from a single image, in MRI (32)).

To analyze the behavior of dictionary learning algorithms theoretically, it useful to posit that Y = A0X0 for some true dictionary A0  O(n) and sparse coefficient matrix X0  Rn×p, and ask whether a given algorithm recovers the pair (A0, X0).4 In this work, we further assume that the sparse matrix X0 is random, with entries i.i.d. Bernoulli-Gaussian5. For
simplicity, we will let A0 = I; our arguments extend directly to general A0 via the simple change of variables q  A0q.

(34) showed that under mild conditions, the complete dictionary recovery problem can be

reduced to the geometric problem of finding a sparse vector in a linear subspace (31). Notice

that because A0 is orthogonal, row(Y ) = row(X0). Because X0 is a sparse random matrix,

the rows of X0 are sparse vectors. Under mild conditions (34), they are the sparsest vectors

in the row space of Y , and hence can be recovered by solving the conceptual optimization

problem

min qY 0 s.t. qY = 0.

This is not a well-structured optimization problem: the objective is discontinuous, and the

constraint set is open. A natural remedy is to replace the 0 norm with a smooth sparsity

surrogate, and to break the scale ambiguity by constraining q to the sphere, giving

1 min fDL(q)  p

p

hµ (q  y k )

s.t.

q  Sn-1.

k=1

(1)

Here, we choose hµ(t) = µ log(cosh(t/µ)) as a smooth sparsity surrogate. This objective was analyzed in (35), which showed that (i) although this optimization problem is nonconvex,
when the data are sufficiently large, with high probability every local optimizer is near a
signed column of the true dictionary A0, (ii) every other critical point has a direction of strict negative curvature, and (iii) as a consequence, a second-order Riemannian trust region method efficiently recovers a column of A0.6 The Riemannian trust region method is of mostly theoretical interest: it solves complicated (albeit polynomial time) subproblems that
involve the Hessian of fDL.

4This problem exhibits a sign permutation symmetry: A0X0 = (A0)(X0) for any signed
permutation matrix . Hence, we only ask for recovery up to a signed permutation. 5[X0]ij = V ij ij , with V ij  N (0, 1), ij  Bern() independent. 6Combining with a deflation strategy, one can then efficiently recover the entire dictionary A0.

3

Under review as a conference paper at ICLR 2019

Figure 2: Left: The separable objective for n = 3. Note the similarity to the dictionary learning objective. Right: The objective for complete orthogonal dictionary learning (discussed in section 6) for n = 3.

In practice, simple iterative methods, including randomly initialized gradient descent are
also observed to rapidly obtain high-quality solutions. In the sequel, we will give a geometric
explanation for this phenomenon, and bound the rate of convergence of randomly initialized gradient descent to the neighborhood of a column of A0. Our analysis of fDL is probabilistic in nature: it argues that with high probability in the sparse matrix X0, randomly initialized gradient descent rapidly produces a minimizer.

To isolate more clearly the key intuitions behind this analysis, we first analyze the simpler

separable objective

n

min fSep(q)  hµ(qi) s.t. q  Sn-1.

(2)

i=1

Figure 2 plots both fSep and fDL as functions over the sphere. Notice that many of the key geometric features in fDL are present in fSep; indeed, fSep can be seen as an "ultrasparse" version of fDL in which the columns of the true sparse matrix X0 are taken to have only one nonzero entry. A virtue of this model function is that its critical points and their stable
manifolds have simple closed form expressions (see Lemma 1).

4 Outline of Important Geometric Features

Our problems of interest have the form
min f (q) s.t. q  Sn-1,
where f : Rn  R is a smooth function. We let f (q) and 2f (q) denote the Euclidean gradient and hessian (over Rn), and let grad [f ] (q) and Hess [f ] (q) denote their Riemannian counterparts (over Sn-1). We will obtain results for Riemannian gradient descent defined by the update
q  expq(- grad[f ](q))
for some step size  > 0, where expq : TqSn-1  Sn-1 is the exponential map. The Riemannian gradient on the sphere is given by grad[f ](q) = (I - qq)f (q).
We let A denote the set of critical points of f over Sn-1 ­ these are the points q¯ s.t. grad [f ] (q¯) = 0. We let A denote the set of local minimizers, and A" its complement. Both fSep and fDL are Morse functions on Sn-1,7 we can assign an index  to every q¯  A, which is the number of negative eigenvalues of Hess [f ] (q¯).

Our goal is to understand when gradient descent efficiently converges to a local minimizer. In the small-step limit, gradient descent follows gradient flow lines  : R  M, which are solution curves of the ordinary differential equation

 (t) = -grad [f ] ((t))

To each critical point   A of index , there is an associated stable manifold of dimension dim(M) - , which is roughly speaking, the set of points that flow to  under gradient flow:

W s()  q  M

lim (t) = 
t  a gradient flow line s.t. (0) = q

.

7Strictly speaking, fDL is Morse with high probability, due to results of (38).

4

Under review as a conference paper at ICLR 2019

Figure 3: Negative curvature and efficient gradient descent. The union of the light blue, orange and yellow sets is the set C. In the light blue region, there is negative curvature normal to C, while in the orange region the gradient norm is large, as illustrated by the
arrows. There is a single global minimizer in the yellow region. For the separable objective, the stable manifolds of the saddles and maximizers all lie on C (the black circles denote the
critical points, which are either maximizers " ", saddles " ", or minimizers " "). The red dots denote C with  = 0.2.

Our analysis uses the following convenient coordinate chart (w) = w, 1 - w 2  q(w)
where w  B1(0). We also define two useful sets: C  {q  Sn-1|qn  w }

(3)

C  q  Sn-1

qn  1 +  . w

(4)

Since the problems considered here are symmetric with respect to a signed permutation of the coordinates we can consider a certain C and the results will hold for the other symmetric sections as well. We will show that at every point in C aside from a neighborhood of a global minimizer for the separable objective (or a solution to the dictionary problem that may only be a local minimizer), there is either a large gradient component in the direction of the minimizer or negative curvature in a direction normal to C. For the case of the separable objective, one can show that the stable manifolds of the saddles lie on this boundary, and hence this curvature is normal to the stable manifolds of the saddles and allows rapid progress away from small gradient regions and towards a global minimizer 8. These regions are depicted in Figure 3.
In the sequel, we will make the above ideas precise for the two specific nonconvex optimization problems discussed in Section 3 and use this to obtain a convergence rate to a neighborhood of a global minimizer. Our analysis are specific to these problems. However, as we will describe in more detail later, they hinge on important geometric characteristics of these problems which make them amenable to efficient optimization, which may obtain in much broader classes of problems.

8The direction of this negative curvature is important here, and it is this feature that distinguishes these problems from other problems in the strict-saddle class where this direction may be arbitrary
5

Under review as a conference paper at ICLR 2019

5 Separable Function Convergence Rate

In this section, we study the behavior of randomly initialized gradient descent on the separable function fSep. We begin by characterizing the critical points:

Lemma 1 (Critical points of fSep). The critical points of the separable problem (2) are

A = PSn-1 [a] a  {-1, 0, 1}n, a > 0 .

(5)

For

every





A

and

corresponding

a(),

for

µ

<

c n log n

the

stable

manifold

of



takes

the

form

W s() = PSn-1 [ a() + b ]

supp(a())  supp(b) = , b <1

(6)

where c > 0 is a numerical constant.

Proof. Please see Appendix A

By inspecting the dimension of the stable manifolds, it is easy to verify that that there are 2n global minimizers at the 1-sparse vectors on the sphere ±ei, 2n maximizers at the least sparse vectors and an exponential number of saddle points of intermediate sparsity. This is because the dimension of W s() is simply the dimension of b in 6, and it follows directly from the stable manifold theorem that only minimizers will have a stable manifold of dimension n - 1. The objective thus possesses no spurious local minimizers.
When referring to critical points and stable manifolds from now on we refer only to those that are contained in C or on its boundary. It is evident from Lemma 1 that the critical points in A" all lie on C and that W s() = C , and there is a minimizer at its center
A"
given by q(0) = en.

5.1 The effect of negative curvature on the gradient

We now turn to making precise the notion that negative curvature normal to stable manifolds

of saddle points enables gradient descent to rapidly exit small gradient regions. We do this by

defining vector fields u(i)(q), i  [n - 1] such that each field is normal to a continuous piece

of C and points outwards relative to C defined in 4. By showing that the Riemannian

gradient projected in this direction is positive and proportional to , we are then able to show

that gradient descent acts to increase (q(w)) =

qn w

-1

geometrically.

This

corresponds

to the behavior illustrated in the light blue region in Figure 3.

Lemma 2 (Separable objective gradient projection). For any w  C, i  [n - 1], we define a vector u(i)  Tq(w)Sn-1 by



0

u(ji)

=

 sign(wi)

-

|wi qn

|

j / {i, n}, j = i, j = n.

(7)

If µ log

1 µ



wi

and

µ

<

1 16

,

then

u(i)grad[fSep](q(w))  c w  , where c > 0 is a numerical constant.

Proof. Please see Appendix A.

Since we will use this property of the gradient in C to derive a convergence rate, we will be interested in bounding the probability that gradient descent initialized randomly with respect to a uniform measure on the sphere is initialized in C. This will require bounding the volume of this set, which is done in the following lemma:

6

Under review as a conference paper at ICLR 2019

Lemma 3 (Volume of C). For C defined as in (4) we have

Vol(C ) Vol(Sn-1)



1 2n

-

log(n) 
n

Proof. Please see Appendix D.3.

5.2 Convergence rate

Using the results above, one can obtain the following convergence rate:

Theorem 1 (Gradient descent convergence rate for separable function). For any 0 < 0 < 1,

r > µ log

1 µ

, Riemannian gradient descent with step size  < min

c1 n

,

µ 2

on the separable

objective

(2)

with

µ

<

 c2 n log

n

,

enters

an

L

ball

of

radius

r

around

a

global

minimizer

in



Cn

1

T< 

r2 + log 0

iterations with probability

P  1 - 2 log(n)0,

where ci, C > 0 are numerical constants.

Proof. Please see Appendix A.

We have thus obtained a convergence rate for gradient descent that relies on the negative curvature around the stable manifolds of the saddles to rapidly move from these regions of the space towards the vicinity of a global minimizer. This is evinced by the logarithmic dependence of the rate on . As was shown for orthogonal dictionary learning in (38), we also expect a linear convergence rate due to strong convexity in the neighborhood of a minimizer, but do not take this into account in the current analysis.

6 Dictionary Learning Convergence Rate

The proofs in this section will be along the same lines as those of Section 5. While we will not describe the positions of the critical points explicitly, the similarity between this objective and the separable function motivates a similar argument. It will be shown that initialization in some C will guarantee that Riemannian gradient descent makes uniform progress in function value until reaching the neighborhood of a global minimizer. We will first consider the population objective which corresponds to the infinite data limit

fDpoLp(q)



E fDL(q)
X0

=

Exi.i.d. BG()

hµ(qx)

.

(8)

and then bounding the finite sample size fluctuations of the relevant quantities. We begin
with a lemma analogous to Lemma 2: Lemma 4 (Dictionary learning population gradient). For w  C, r < |wi|, µ < c1r5/2 the dictionary learning population objective 8 obeys

u(i)grad[fDpoLp](q(w))  cr3

where c depends only on , c1 is a positive numerical constant and u(i) is defined in 7.

Proof. Please see Appendix B

Using this result, we obtain the desired convergence rate for the population objective, presented in Lemma 11 in Appendix B. After accounting for finite sample size fluctuations in the gradient, one obtains a rate of convergence to the neighborhood of a solution (which is some signed basis vector due to our choice A0 = I)

7

Under review as a conference paper at ICLR 2019

Theorem 2 (Gradient descent convergence rate for dictionary learning). For any 1 > 0 >

0, s

>

µ ,
42

Riemannian gradient


descent

with

step

size



<

c5 s n log np

on

the

dictionary

learning

objective

1

with

µ

<

, c6 0
n5/4



(0,

1 2

),

enters

a

ball

of

radius

c3s

from

a

target

solution

in

T < C2

11 + n log

 s

0

iterations with probability

P  1 - 2 log(n)0 - Py - c8p-6

where

y

=

,c7 (1-)0
n3/2

Py

is

given

in

Lemma

10

and

ci, Ci

are

positive

constants.

Proof. Please see Appendix B

The two terms in the rate correspond to an initial geometric increase in the distance from the set containing the small gradient regions around saddle points, followed by convergence to the vicinity of a minimizer in a region where the gradient norm is large. The latter is based on results on the geometry of this objective provided in (38).

7 Discussion
The above analysis suggests that second-order properties - namely negative curvature normal to the stable manifolds of saddle points - play an important role in the success of randomly initialized gradient descent in the solution of complete orthogonal dictionary learning. This was done by furnishing a convergence rate guarantee that holds when the random initialization is not in regions that feed into small gradient regions around saddle points, and bounding the probability of such an initialization. In Appendix C we provide an additional example of a nonconvex problem that for which an efficient rate can be obtained based on an analysis that relies on negative curvature normal to stable manifolds of saddles - generalized phase retrieval. An interesting direction of further work is to more precisely characterize the class of functions that share this feature.
The effect of curvature can be seen in the dependence of the maximal number of iterations T on the parameter 0. This parameter controlled the volume of regions where initialization would lead to slow progress and the failure probability of the bound 1 - P was linear in 0, while T depended logarithmically on 0. This logarithmic dependence is due to a geometric increase in the distance from the stable manifolds of the saddles during gradient descent, which is a consequence of negative curvature. Note that the choice of 0 allows one to flexibly trade off between T and 1 - P. By decreasing 0, the bound holds with higher probability, at the price of an increase in T . This is because the volume of acceptable initializations now contains regions of smaller minimal gradient norm. In a sense, the result is an extrapolation of works such as (23) that analyze the 0 = 0 case to finite 0.
Our analysis uses precise knowledge of the location of the stable manifolds of saddle points. For less symmetric problems, including variants of sparse blind deconvolution (41) and overcomplete tensor decomposition, there is no closed form expression for the stable manifolds. However, it is still possible to coarsely localize them in regions containing negative curvature. Understanding the implications of this geometric structure for randomly initialized first-order methods is an important direction for future work.
One may hope that studying simple model problems and identifying structures (here, negative curvature orthogonal to the stable manifold) that enable efficient optimization will inspire approaches to broader classes of problems. One problem of obvious interest is the training of deep neural networks for classification, which shares certain high-level features with the problems discussed in this paper. The objective is also highly nonconvex and is conjectured to contain a proliferation of saddle points (11), yet these appear to be avoided by first-order methods (16) for reasons that are still quite poorly understood beyond the two-layer case (39).

8

Under review as a conference paper at ICLR 2019
References
[1] Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal tensor decomposition via alternating rank-1 updates. arXiv preprint arXiv:1402.5180, 2014.
[2] Radu Balan, Pete Casazza, and Dan Edidin. On signal reconstruction without phase. Applied and Computational Harmonic Analysis, 20(3):345­356, 2006.
[3] Afonso S Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In Conference on Learning Theory, pages 361­382, 2016.
[4] Dimitri P Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
[5] Nicolas Boumal. Nonconvex phase synchronization. SIAM Journal on Optimization, 26(4):2355­2377, 2016.
[6] Michael M Bronstein, Alexander M Bronstein, Michael Zibulevsky, and Yehoshua Y Zeevi. Blind deconvolution of images using optimal sparse representations. IEEE Transactions on Image Processing, 14(6):726­736, 2005.
[7] Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985­2007, 2015.
[8] Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma. Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval. arXiv preprint arXiv:1803.07726, 2018.
[9] Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust region methods, volume 1. Siam, 2000.
[10] John V Corbett. The pauli problem, state reconstruction and quantum-real numbers. Reports on Mathematical Physics, 57:53­68, 2006.
[11] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in highdimensional non-convex optimization. In Advances in neural information processing systems, pages 2933­2941, 2014.
[12] Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Barnabas Poczos, and Aarti Singh. Gradient descent can take exponential time to escape saddle points. arXiv preprint arXiv:1705.10412, 2017.
[13] Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE Transactions on Image processing, 15(12):3736­ 3745, 2006.
[14] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points?online stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797­842, 2015.
[15] Donald Goldfarb. Curvilinear path steplength algorithms for minimization which use directions of negative curvature. Mathematical programming, 18(1):31­40, 1980.
[16] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544, 2014.
[17] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv preprint arXiv:1609.05191, 2016.
[18] Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
9

Under review as a conference paper at ICLR 2019
[19] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 665­674. ACM, 2013.
[20] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
[21] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. IEEE Transactions on Information Theory, 56(6):2980­2998, 2010.
[22] Ken Kreutz-Delgado. The complex gradient operator and the cr-calculus. arXiv preprint arXiv:0906.4835, 2009.
[23] Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin Recht. First-order methods almost always avoid saddle points. arXiv preprint arXiv:1710.07406, 2017.
[24] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pages 1246­1257, 2016.
[25] Kiryung Lee, Yihong Wu, and Yoram Bresler. Near optimal compressed sensing of sparse rank-one matrices via sparse power factorization. arXiv preprint, 2013.
[26] Julien Mairal, Francis Bach, Jean Ponce, et al. Sparse modeling for image and vision processing. Foundations and Trends R in Computer Graphics and Vision, 8(2-3):85­283, 2014.
[27] Jianwei Miao, Tetsuya Ishikawa, Bart Johnson, Erik H Anderson, Barry Lai, and Keith O Hodgson. High resolution 3d x-ray diffraction microscopy. Physical review letters, 89(8):088303, 2002.
[28] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.
[29] Liviu Nicolaescu. An invitation to Morse theory. Springer Science & Business Media, 2011.
[30] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607, 1996.
[31] Qing Qu, Ju Sun, and John Wright. Finding a sparse vector in a subspace: Linear sparsity using alternating directions. In Advances in Neural Information Processing Systems, pages 3401­3409, 2014.
[32] Saiprasad Ravishankar and Yoram Bresler. Mr image reconstruction from highly undersampled k-space data by dictionary learning. IEEE transactions on medical imaging, 30(5):1028­1041, 2011.
[33] Yoav Shechtman, Yonina C Eldar, Oren Cohen, Henry Nicholas Chapman, Jianwei Miao, and Mordechai Segev. Phase retrieval with application to optical imaging: a contemporary overview. IEEE signal processing magazine, 32(3):87­109, 2015.
[34] Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In Conference on Learning Theory, pages 37­1, 2012.
[35] Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere. In Sampling Theory and Applications (SampTA), 2015 International Conference on, pages 407­410. IEEE, 2015.
[36] Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint arXiv:1510.06096, 2015.
10

Under review as a conference paper at ICLR 2019
[37] Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In Information Theory (ISIT), 2016 IEEE International Symposium on, pages 2379­2383. IEEE, 2016.
[38] Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere i: Overview and the geometric picture. IEEE Transactions on Information Theory, 63(2):853­884, 2017.
[39] Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.
[40] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
[41] Yuqian Zhang, Yenson Lau, Han-wen Kuo, Sky Cheung, Abhay Pasupathy, and John Wright. On the global geometry of sphere-constrained sparse blind deconvolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4894­4902, 2017.
11

Under review as a conference paper at ICLR 2019

A Proofs - Separable Objective

Proof of Lemma 1: (Critical separable objective)

point

structure

of.

Denoting

by

tanh(

q µ

)

a

vector

in

Rn

elements

tanh(

q µ

)i

=

tanh(

qi µ

)

we

have

grad[fSep](q)i

=

(I

-

qq)

q tanh(
µ

)

.

Thus

critical

points

are

ones

where

either

tanh(

q µ

)

=

0

(which

cannot

happen

on

Sn-1)

or

tanh(

q µ

)

is

in

the

nullspace

of

(I

-

qq),

which

implies

tanh(

q µ

)

=

cq

for

some

constant

b.

The

equation

tanh(

x µ

)

=

bx

has

either a

single

solution

at the

origin

or

3

solutions

at

{0, ±r(b)} for some r(b). Since this equation must be solves simultaneously for every element

of q, we obtain i  [n] : qi  {0, ±r(b)}. To obtain solutions on the sphere, one then uses

the freedom we have in choosing b (and thus r(b)) such that q = 1. The resulting set of

critical points is thus

A = PSn-1 {-1, 0, 1}n \ {0} . To prove the form of the stable manifolds, we first show that for qi such that |qi| = q  and any qj such that |qj| +  = |qi| and sufficiently small  > 0, we have

-grad[fSep](q)isign(qi) > -grad[fSep](q)isign(qj)

(9)

For ease of notation we now assume qi, qj > 0 and hence  = qi - qj, otherwise the argument can be repeated exactly with absolute values instead. The above inequality can then be

written as

(qi - qj)

n

tanh(

qk µ

)qk

-

tanh(

qi µ

)

+

tanh(

qj µ

)

>

0.

k=1

h

n-1

If we now define s2 =

qk2 and qn =

k=1

k = i, n

1 - s2 - (qj + )2we have

 h=



tanh(

qj + µ

)

(qj

+

)

+

tanh(

1-s2

-(qj µ

+)2

)

1 - s2 - (qj + )2

+

tanh(

qk µ

)qk

-

tanh(

qj

+ µ

)

+

tanh(

qj µ

)

k=i,n


    =      

tanh(

qk µ

)qk

+

tanh(

qj µ

)qj

k=i,n 

+ tanh(

1-s2 µ

-qj2

)

1 - s2 - qj2

h1
- sech2( qj ) 1 µµ

h2


     + O(2)     

where the O(2) term is bounded. Defining a vector r  Rn by

k

=

i,

n

:

rk

=

qk ,

ri

=

tanh( qj µ

)qj ,

rn

=

1 - s2 - qj2

12

Under review as a conference paper at ICLR 2019

we have r 2 = 1. Since tanh(x) is concave for x > 0, and |ri|  1, we find

h1 =

n

tanh(

rk µ

)rk



1 tanh( )
µ

n

rk2

=

1 tanh( ).
µ

k=1

k=1

From |qi| =

q



it follows

that qi



1 n

and thus

qj



1 n

- .

Using

this inequality and

properties of the hyperbolic secant we obtain

h2



4 exp(-2 qj µ

- log µ)



exp( 2 µ

-

2 µn

- log µ + log 4)

and

plugging

in

µ

=

c n log n

for

some

c

<

1



2 exp(

-

2

log

n

-

log

c

+

1

log

n

+

log

log

n

+

log

4).

µc

2

We

can

bound

this

quantity

by

a

constant,

say

h2



1 2

,

by

requiring

A



2

-

log

c

+

1 (

-

2 ) log

n

+

log

log

n



- log

8

µ 2c

and for and c < 1, using - log n + log log n < 0 we have

A

<

2

-

log c

-

2 (

-

1) log

n.

µc

Since  can be taken arbitrarily small, it is clear that c can be chosen in an n-independent manner such that A  - log 8. We then find

h1

-

h2



1 tanh( )
µ

-

1 2



 tanh( n log n)

-

1 2

>

0

since this inequality is strict,  can be chosen small enough such that O(2) < (h1 - h2) and hence

h > 0,

proving 9.

It follows that under negative gradient flow, a point with |qj| < ||q|| cannot flow to a point q such that |qj| = ||q ||. From the form of the critical points, for every such j, q must
thus flow to a point such that qj = 0 (the value of the j coordinate cannot pass through 0 to a point where |qj| = ||q || since from smoothness of the objective this would require
passing some q with qj = 0, at which point grad [fSep] (q )j = 0).

As for the maximal magnitude coordinates, if there is more than one coordinate satisfying

|qi1 | = |qi2 | = q , it is clear from symmetry that at any subsequent point q along

the gradient flow line qi1 = qi2 . These coordinates cannot change sign since from the smoothness of the objective this would require that they pass through a point where they have magnitude smaller than 1/ n, at which point some other coordinate must have a larger

magnitude (in order not to violate the spherical constraint), contradicting the above result for

non-maximal elements. It follows that the sign pattern of these elements is preserved during

the flow. Thus there is a single critical point to which any q can flow, and this is given by

setting all the coordinates by a positive constant to

with |qj| < q  to ensure the resulting

0 and vector

multiplying the remaining coordinates is on Sn. Denoting this critical point

by , there is a vector b such that q = PSn-1 [a() + b] and supp(a())  supp(b) = ,

13

Under review as a conference paper at ICLR 2019

b  < 1 with the form of a() given by 5 . The collection of all such points defines the stable manifold of .

Proof of Lemma 2: (Separable objective gradient projection). i) We consider the

sign(wi) = 1 case; the sign(wi) = -1 case follows directly. Recalling that

u(i)grad[fSep](q(w)) = tanh

wi µ

- tanh

qn µ

wi qn

,

we

first

prove

tanh

wi µ

- tanh

qn µ

wi qn

 c(qn

- wi)

(10)

for some c > 0 whose form will be determined later. The inequality clearly holds for wi = qn. To verify that it holds for smaller values of wi as well, we now show that

 tanh wi wi µ

- tanh

qn µ

wi qn

-

c(qn

-

wi)

<0

which will ensure that it holds for all wi. We define s2 = 1 - ||w||2 + wi2 and denote qn = s2 - wi2 to extract the wi dependence, giving

 tanh
wi

wi µ

- tanh

qn µ

wi qn

-

c(qn

-

wi)



=

1 µ

sech2

wi µ

+

1 µ

sech2



s2 -wi2 µ

wi2 s2 -wi2

- tanh

s2 -wi2 µ

s2 (s2 -wi2 )3/2

+ c(  wi
s2 -wi2

+ 1)



e + e4

-2

wi µ

µ

-2

s2 -wi2 µ



- tanh

s2 -wi2 µ

s2 (s2 -wi2 )3/2

+ 2c

Where in the last inequality we used properties of the sech function and qn  wi. We thus want to show

4

e-2

wi µ

+

e-2

qn µ

+ 2c  tanh

qn

µµ

and

using

log(

1 µ

)µ



wi



qn

and c =

1-µ2 1+µ2

-8µ

2

we have

qn2 + wi2 qn3

4

e-2

wi µ

+

e-2

qn µ

+ 2c

µ

8e-2

wi µ

1 - µ2

 µ + 2c  8µ + 2c  1 + µ2

= tanh

1 log( )

 tanh

qn

1

µ µ qn

< tanh

qn µ

qn2 + wi2 qn3

and

it

follows

that

10

holds.

For

µ

<

1 16

we

are

guaranteed

that

c>

0.

14

Under review as a conference paper at ICLR 2019

From examining the RHS of 10 (and plugging in qn = s2 - wi2) we see that any lower

bound on the gradient of an element wj applies also to any element |wi|  |wj|. Since for

|wj |

=

||w||

we

have

qn

-

wj

=

wj  ,

for

every

log(

1 µ

)µ



wi

we

obtain

the

bound

u(i)grad[fSep](q(w))  c w  

Proof of Theorem 1: (Gradient descent convergence rate for separable function).
We obtain a convergence rate by first bounding the number of iterations of Riemannian gradient descent in C0 \C1, and then considering C1\Br.

From

Lemma

16

we

obtain

C0 \C1



C0 \B1/n+3.

Choosing

c2

so

that

µ

<

1 2

,

we

can

apply

Lemma 2, and for u defined in 7, we thus have

|wi|

>

µ

log(

1 µ

)



u(i)grad[fSep](q(w))

>

c||w||0.



Since from Lemma 7 the Riemannian gradient norm is bounded by n, we can choose c1, c2

such

that

µ

log(

1 µ

)

<

1 2 n+3

,



<

1 6 n2+3n

.

Lemma

17

with

r

=

µ

log(

1 µ

),

b

=

1 n+3

,

M

This choice of  then satisfies the conditions 
= n, which gives that after a gradient step

of

 

c 1+

n    (1 + c~)

2 n+3

(11)

for some suitably chosen c~ > 0. If we now define by w(t) the t-th iterate of Riemannian

gradient descent and (t)  qn(t)
w(t)

- 1, (0)  0, for iterations such that w(t)  C \C1 we



find

(t)  (t-1) (1 + c~)  0 (1 + c~)t

and the number of iterations required to exit C0 \C1 is

t1

=

log(

1 0

)

.

log(1 + c~)

(12)

To bound the remaining iterations, we use Lemma 2 to obtain that for every w  C0 \Br,

grad[fSep](q(w)) 2 

u(i)grad[fSep](q(w)) ||u(i)||2

2

 02c2r2

where

we

have

used

||u(i)||2

=

1+

wi2 qn2



2.

We

thus

have

T -1

2

grad[fSep](q(w)(i))

i=0

t1 -1

2 T -1

2

= grad[fSep](q(w)(i)) + grad[fSep](q(w)(i))

i=0 i=t1

>

02c2 (n + 3)

t1

+

(T

- t1)c2r2.

(13)

Choosing



<

1 2L

where

L

is

the

gradient

Lipschitz

constant

of

fs,

from

Lemma

5

we

obtain

2 fSep(q(0)) - fSep 

T -1

2

> grad[fSep](q(i)) .

i=0

According

to

Lemma

B,

L

=

1/µ

and

thus

the

above

holds

if

we

demand



<

µ 2

.

Combining

12 and 13 gives

15

Under review as a conference paper at ICLR 2019

2 T<

fSep(q(0)) - fSep c2r2

+

1

-

02 (n+3)r2

log(

1 0

)

.

log(1 + c~)

To

obtain

the

final

rate,

we

use

in

g(w0) - g

 n

and

c~

<1

1 log(1+c~)

<

C~ c~

for

some

C~ > 0. Thus one can choose C > 0 such that

Cn

1

T< 

r2

+ log( ) 0

.

(14)

From Lemma 1 the ball Br contains a global minimizer of the objective, located at the origin.

The probability of initializing in C0 is simply given from Lemma 3 and by summing over
A
the 2n possible choices of C0 , one for each global minimizer (corresponding to a single signed basis vector).

Lemma 5 (Riemannian gradient descent iterate bound). For a Riemannian gradient descent

algorithm

on

the

sphere

with

step

size

tk

<

1 2L

,

where

L

is

a

lipschitz

constant

for

f (q),

one has

f (q1) - f (q )  f (q1) - f (qT )

 tk 2

grad [f ] (qk)

2.

Proof. Just as in the euclidean setting, we can obtain a lower bound on progress in function

values of iterates of the Riemannian gradient descent algorithm from a lower bound on the Riemannian gradient. Consider f : Sn-1  R, which has L-lipschitz gradient. Let qk denote the current iterate of Riemannian gradient descent, and let tk > 0 denote the step size. Then
we can form the Taylor approximation to f  Expqk (v) at 0qk :

f^ : B1(0qk )  Tqk Sn-1  R : v  f (qk) + v, f (qk) .

From Taylor's theorem, we have for any v  B1(0qk )  Tqk Sn-1

|f^(v)

-

f



Expqk (v)|



1 2

Hess[f ](qk)

v - 0qk 2,

where the matrix norm is the operator norm on Rn×n. Using the gradient-lipschitz property

of f , we readily compute

Hess[f ](qk)  2f (qk) + | f (qk), qk |  2L,

since f (0) = 0 and qk  Sn-1. We thus have
f  Expqk (v)  f (qk) + v, f (qk) + L v 2.
If we put v = -tkgrad[f ](qk) and write qk+1 = Expqk (-tkgrad [f ] (qk)), the previous expression becomes

f (qk+1)  f (qk) - tk grad [f ] (qk) 2 + tk2 L grad [f ] (qk) 2



f (qk)

-

tk 2

grad [f ] (qk) 2

if

tk

<

1 2L

.

Thus

progress

in

objective

value

is

guaranteed

by

lower-bounding

the

Riemannian

gradient.

16

Under review as a conference paper at ICLR 2019

As in the euclidean setting, summing the previous expression over iterations k now yields

T -1

f (qk) - f (qk+1) = f (q1) - f (qT )

k=1



tk 2

T -1

grad [f ] (qk)

2;

k=1

in addition, it holds f (q1) - f (qT )  f (q1) - f (q ). Plugging in a constant step size gives the desired result.

Lemma 6 (Lipschitz constant of f ). For any x1, x2  Rn, it holds

f (x1) - f (x2)

1 µ

x1 - x2 .

Proof. It will be enough to study a single coordinate function of f . Using a derivative given in section D.1, we have for x  R
d tanh(x/µ) = 1 sech2 x . dx µ µ

A bound on the magnitude of the derivative of this smooth function implies a lipschitz constant for x  tanh(x/µ). To find the bound, we differentiate again and find the critical points of the function. We have, using the chain rule,

d 1 sech2 x

dx µ

µ

-4 = sech
µ

x µ

·

(ex/µ

1 + e-x/µ)2

· 1 ex/µ - 1 e-x/µ µµ

=

-

1 µ2

ex/µ - e-x/µ (ex/µ + e-x/µ)3

.

The denominator of this final expression vanishes nowhere. Hence, the only critical point satisfies x/µ = -x/µ, which implies x = 0. Therefore it holds

d

tanh(x/µ) 

1 sech2(0) =

1 ,

dx µ µ

which shows that tanh(x/µ) is (1/µ)-lipschitz.

Now let x1 and x2 be any two points of Rn. Then one has

1/2

f (x1) - f (x2) =

(tanh(x1i/µ) - tanh(x2i/µ))2

i

1/2

= |tanh(x1i/µ) - tanh(x2i/µ)|2

i

 1 x1i - x2i 2 1/2 µµ µ
i

1 =
µ

x1 - x2

,

completing the proof.

Lemma 7 (Separable objective gradient bound). The separable objective gradient obeys 
wg(w)  2n 
grad[f ](q)  n

17

Under review as a conference paper at ICLR 2019

Proof. Recalling that the Euclidean gradient is given by fSep(q)i = tanh

qi µ

we use

Jensen's inequality, convexity of the L2 norm and the triangle inequality to obtain

gs(w) 2  fSep(q) 2 + tanh

qn µ

2

w qn2

2



2n

while

grad[fSep](q)

=

(I - qq)fSep(q)



fSep(q)

 =n

B Proofs - Dictionary Learning

Proof of Lemma 4:(Dictionary learning population gradient). For simplicity we consider the case sign(wi) = 1. The converse follows by a similar argument. We have
u(i)grad[fDpoLp](q(w)) =

q(w)x

Ex tanh

µ

-xn

wi qn

+

xi

(15)

Following the notation of (38), we write xj = bjvj where bj  Bern(), vj  N (0, 1) and denote the vectors of these variables by J , v respectively. Defining Y (n) = q(w)jxj, X(n) =
j=n
qnvn, Y is Gaussian conditioned on a certain setting of J . Using Lemma 40 in (38) the first term in 15 is

-

wi qn2

Ev,J

|bn

=1

tanh

Y (n) + X(n) µ

X (n)

=

-

wi µ

Ev,J

|bn =1

sech2

Y (n) + X(n) µ

and similarly the second term in 15 is, with X(i) = wivi, Y (i) = q(w)jxj
j=i

 wi Ev,J |bi=1 tanh

Y (i) + X(i) µ

X (i)

=

wi µ Ev,J |bi=1

sech2

q(w)x µ

if we now define X = q(w)jxj we have
j=n,i

u(i)grad[fDpoLp](q(w)) =

=


wi µ

Ev,J |bi=1 sech2 -Ev,J |bn=1 sech2

q  (w)x µ
q  (w)x µ

 

=

wi µ Ev,J

 

sech2 -sech2

X +bn qn vn +wi vi µ
X +qn vn +wi bi vi µ

 



=

wi(1 - µ

) Ev,J \{n,i}



sech2 -sech2

X +wi vi µ
X +qn vn µ

 

(16)

18

Under review as a conference paper at ICLR 2019

B.1 Bounds for E sech2(Y )

We already have a lower bound in Lemma 20 of (38) that we can use for the second term, so

we need an upper bound for the first term. Following from p. 865, we define Y  N (0, Y2 ) ,

Z = exp

-2Y µ

, and defining  = 1 - 1 for some T > 1 we have
T

sech2(Y /µ) =

4Z (1 + Z)2



4Z (1 + Z)2

=


bk Z k+1

k=0

Where bk = (-)k(k + 1). Using B.3 from Lemma 40 in (38) we have



1 1E

bkZk+1 Y >0 =

bkE e-2(k+1)Y /µ Y >0

k=0

k=0


= bk exp

1 2

2(k + 1) µ

2
Y2

c

2(k + 1) µ Y

k=0

Where c(x) is the complementary Gaussian CDF (The exchange of summation and expec-

tation is justified since Y > 0 implies Z  [0, 1], see proof of Lemma 18 in (38) for details).

Using the following bounds 1
2

1 x

-

1 x3

e-x2/2  c(x)  1
2

1 x

-

1 x3

+

3 x5

e-x2/2 by

applying the upper (lower) bound to the even (odd) terms in the sum, and then adding a

non-negative quantity, we obtain





1


(-)k(k

2 k=0

1

+

1)

 

2(k+1) µ

Y

-



1



2(k+1) µ

Y

3



+ 1


k(k + 1) 

2 k=0



3

2(k+1) µ

Y


 5

and using



(-)k

=

1 1+

,

bk (k+1)3



0,



|bk | (k+1)5

 2 (from Lemma 17 in (38)) and

k=0

k=0

k=0

taking T   so that   1 we have


bkE 1Zk+1 Y >0
k=0



1 2 2

1

2 µ

Y

+ 1 2

6

5

2 µ

Y

giving the upper bound


E sech2(Y /µ) = E 1 - tanh2(Y /µ)  8 bkE 1Zk+1 Y >0
k=0
 2 µ + 3µ5  Y 2 2Y5

while the lower bound (Lemma 20 in (38)) is

2µ  Y

- 2µ3 2Y3

- 3µ5 2 2Y5

E

sech2(Y )

19

Under review as a conference paper at ICLR 2019

B.2 Gradient bounds
After conditioning on J \{n, i} the variables X + qnvn, X + qivi are Gaussian. We can thus plug the bounds into 16 to obtain

u(i)grad[fDpoLp](q(w)) 

2 

wi(1

-

)



EJ

\{n,i}

 

1
 - ( ) - ( )X2 +wi2

µ2 X2 +wi2 3/2

3µ4 4 X2 +wi2 5/2

1
-  - ( )X2 +qn2

3µ4 4 X2 +qn2 5/2


 









2



wi

(1

-

)

 

EJ \{n,i}

 X2 +qn2 - X2 +wi2
X2 +qn2 X2 +wi2

- -µ2 3µ4
wi3 2wi5

 

the term in the expectation is positive since qn > ||w|| (1 + ) > wi giving





2 

wi(1

-

)

 

EJ \{n,i}

X2 + qn2 - X2 + wi2

- -µ2 3µ4
wi3 2wi5


 

. To extract the  dependence we plug in qn > wi (1 + ) and develop to first order in  (since the resulting function of  is convex) giving





2 

wi(1

-

)

 

EJ \{n,i} - µ2
wi3

 wi2
X2 +wi2
- 3µ4
2wi5

 



2 (1 - ) 

wi3

-

µ2 wi2

-

3µ4 2wi4



Given some  and r such that wi > r, if we now choose µ such that µ <

1+

3 4

r3

3



-1

r

we have the desired result. This can be achieved by requiring µ < c1r5/2  for a suitably

chosen c1 > 0.

Lemma 8 (Point-wise concentration of projected gradient). For u(i) defined in 7, the gradient of the objective 1 obeys
P u(i)grad[fDL](q) - E u(i)grad[fDL](q)  t
 2 exp - pt2 4 + 2 2t

Proof of Lemma 8: (Point-wise concentration of projected gradient). If we denote by xi a column of the data matrix with entries xij  BG(), we have

u(i)grad[fDL](q(w))

1 p q(w)xk = tanh
pµ
k=1

xki

-

xnk

wi qn

1 p

p

Zk

k=1

20

Under review as a conference paper at ICLR 2019

. Since tanh(x) is bounded by 1, |Zk| 

xik

-

xkn

wi qn

 uT xk

.

Invoking

Lemma

21

from (38) E [|Zk|m]

and

u

2

=

1

+

wi2 qn2

 EZN (0,2) [|Z|m]

 2 we obtain



m 2 (m

-

1)!!



 m-2 22

m!

 2

and using Lemma 36 in (38) with R = 2,  = 2 we have

P [|gDL(w)i - E [gDL(w)i]|  t]  2 exp - pt2 4 + 2 2t

Lemma 9 (Projection Lipschitz Constant). The Lipschitz constant for u(i)grad[fDL](q(w))

is

 L=2 n X 

X  +1 µ

Proof of Lemma 9: (Projection Lipschitz Constant). We have

|u(j)grad[fDL](q(w)) - u(j)grad[fDL](q(w ))|

=

 1p

tanh(

q

(w)xi µ

)

xij

-

xin qn (w)

wj

p
i=1

-tanh(

q  (w µ

)xi

)

xji

-

xni qn (w

) wj

 

1 p q(w)xi

q(w )xi

 tanh(

)s(w) - tanh(

)s(w )

pµ

µ

i=1

where

we

have

defined

s(w)

=

xji

-

xn qn (w)

wj

.

Using

q(w), q(w

)



C



qn(w), qn(w

)



1 2n

we have

|s(w) - s(w )| = xni

wj - wj qn(w) qn(w )



 |xn| 2 n w - w

Lemma 25 in (38) gives

q(w)x tanh(

)

-

tanh(

q(w

)x )

 2 n

x

w-w

µ µµ

We also use the fact that tanh is bounded by 1 and s(w) is bounded by X . We can then use Lemma 23 in (38) to obtain

|u(j)grad[fDL](q(w)) - u(j)grad[fDL](q(w ))|

 2 n

p

1 (

pµ

xi

2 

+

xi )

w-w

i=1

21

Under review as a conference paper at ICLR 2019

 2 n X 

X  +1 µ

w-w

 we thus have L = 2 n X 

X µ

+1

.

Lemma 10 (Uniformized gradient fluctuations). For all w  C, i  [n], with probability P > Py

we have

u(i)grad[fDL](q(w)) -E u(i)grad[fDL](q(w))

 y(, )

where

Py

  2 exp 


+n

- log

1 4

py(,)2 + log(n) 448+n2y4(lo,gµ()np) + log(np)
y(,)


 

Proof: B

Proof of Lemma 10:(Uniformized gradient fluctuations). For X  Rn×p with i.i.d.

BG() entries, we define the event E  {1  X   4 log(np)}. We have

P[Ec ]  (np)-7 + e-0.3np

For

any





(0, 1)

we

can

construct

an

-net

N

for

C

\B2 

(0)

1/20 5(n-1)

with

at

most

(3/)n

points. Using Lemma 9, on E, grad[fDL](q)i is L-Lipschitz with

 4 log(np)

L=8 n

+ log(np)

µ

.

If

we

choose



=

y(,) 2L

we

have

|N |  ( 6L )n y(, )

. We then denote by Eg the event

max
wN,i[n]

u(i)grad[fDL](q(w)) -E u(i)grad[fDL](q(w))

 y(, ) 2

and obtain that on Eg  E

sup |gDL(w)i - E [gDL(w)i]|  y(, )
wC ,i[n]

.

Setting

t

=

b() 2

in

the

result

of

Lemma

8

gives

that

for

all

w



C , i



[n],

P

u(i)grad[fDL](q(w)) -E u(i)grad[fDL](q(w))

 y(, ) 2

 2 exp - 1 py(, )2 4 4 + 2 2y(, )

and thus

P Egc

  2 exp 

-

1 4

py(,)2 4+ 2y(,)2

 

+n log

6L b()

+ log(n)

22

Under review as a conference paper at ICLR 2019

Lemma 11 (Gradient descent convergence rate for dictionary learning - population). For

any

1

>

0

>

0

and

s

>

µ ,
42

Riemannian

gradient

descent


with

step

size



<

c2 s n

on

the

dictionary

learning

population

objective

8

with

µ

<

, c4 0
n5/4



(0,

1 2

),

enters

a

ball

of

radius

c3s from a target solution in

T < C1

11 + n log

 s

0

iterations with probability

P  1 - 2 log(n)0

where the ci, Ci are positive constants.

Proof of Lemma 11: (Gradient descent convergence rate for dictionary learning - population).
The rate will be obtained by splitting C0 into three regions. We consider convergence to Bs2(0) since this set contains a global minimizer. Note that the balls in the proof are defined with respect to w.

B.3

C0

\B2 
1/20 5

(0)

The analysis in this region is completely analogous to that in the first part of the proof of Lemma 1. For every point in this set we have
1 w  > 20 5(n - 1)

. From Lemma 16 we know that

n-1 (2+ (t) ) (t) +n

<

1 20 5



w(t)



B2  (0)
1/20 5

hence

in

this

set  < 8. If we choose r =  1 , since for every point in this region r3 < 1, we have

<r5/2 
23

 40 5(n-1)

1+

3 4

r3

3



-1

r

=

z(r,

)

and

we

thus

demand

µ

<


 0
40 5(n-1)

5/2  23



r5/2  23

and

obtain from Lemma 4 that for |wi| > r

u(i)grad[fDpoLp](q(w))



cDL (8000(n - 1))3/2

.

We

now

require  < 

1
360 5n(n-1)

=

b-r 3M

we

can

apply

Lemma

17

with

b=

1 ,r 20 5(n-1)

=

 1 , M = n (since the maximal norm of the Riemannian gradient is n from

40 5(n-1)

Lemma 12), obtaining that at every iteration in this region



  1+

ncDL



2(8000(n - 1))3/2

and the maximal number of iterations required to obtain  > 8 and exit this region is given

by

t1 = log

log(8/0)



1

+

ncDL
2(8000(n-1))3/2

(17)

B.4

B2 
1/20 5

(0)\Bs2

(0)

According

to

Proposition

7

in

(38),

which

we

can

apply

since

s



µ , µ
42

<

9 50

,

in

this

region

we have

wwgDpoLp(w)  c w

A simple calculation shows that wgDpoLp(w) =

 w

 grad[fDpoLp](q(w)) where  is the map

defined in 3, and thus

23

Under review as a conference paper at ICLR 2019

w

 w

 grad[fDpoLp](q(w))

=

w

w

-

w2 qn

grad[fDpoLp](q(w)) w

> c

(18)

. Defining h(q) =

w 2

2

,

and

denoting

by

q

an update of Riemannian gradient descent with

step size , we have (using a Lagrange remainder term)



h(q )

h(q ) = h(q) +

+



2h(q dt 2

)

(
=t

-

t)

0

R

=

w 2- 2

grad[fDpoLp](q),

h(q) q

+R

where in the last line we used q

=

cos(g)q

-

sin(g)

grad[fDpoLp ](q ) g

where

g



grad[fDpoLp](q) .

Since

grad[fDpoLp](q),

h(q) q

=

grad[fDpoLp](q), (I

-

qq)

h(q) q

and

(I - qq) h(q) = (I - qq) q

w -qn

=

w -qn

- ( w 2 - qn2 )q = 2(1 - w 2)

w

-

w2 qn

we obtain (using 18)

w2 =
2

w

2
+ 2(1 -

w

2)

2

< w 2 - 2(1 - 2

grad[fDpoLp](q),

w

-

w2 qn

w 2) w c + R

+R

It remains to bound R. Denoting r =

w -qn


grad[f ](q) we have

2h(q )

q  2h(q) q

h(q)  2q

2

=
=t



qq  =t + q 2 =t

cos2(gt) grad[fDpoLp](q)2 - grad[fDpoLp](q)n2 = +g2 sin2(gt) - cos(gt) w 2 - qn2
+g sin(gt)r(1 + 2 cos(gt))

hence for some C > 0, if grad[fDpoLp](q) < M we have

R < CM22

and

thus

choosing



<

(1-

w 2) w CM2

c

we

find

w 2 < w 2 - 2(1 - w 2) w c

24

Under review as a conference paper at ICLR 2019

and in our region of interest w 2 < w 2 - c~s for some c~ > 0 and thus summing over iterations, we obtain for some C~2 > 0

t2

=

C~2 . s

(19)



From Lemma 12, M =

n

and

thus

with

a

suitably

chosen

c2

>

0,



<

c2 s n

satisfies

the

above requirement on  as well as the previous requirements, since  < 1.

B.5 Final rate and distance to minimizer

Combining these results gives, we find that when initializing in C0 , the maximal number of iterations required for Riemannian gradient descent to enter Bs2(0) is

T



t1

+ t2

<

C1 

11 n log +
0 s

for some suitably chosen C1, where t1, t2 are given in 17,19. The probability of such an initialization is given by the probability of initializing in one of the 2n possible choices of C,
which is bounded in Lemma 3.

Once w  Bs2(0), the distance in Rn-1 between w and a solution to the problem (which is a signed basis vector, given by the point w = 0 or an analog on a different symmetric section of

the sphere) is no larger than s, which in turn implies that the Riemannian distance between

(w) and a solution is no larger than c3s for some c3 > 0. We note that the conditions on µ

can

be

satisfied

by

requiring

µ

<

.c4 0
n5/4

Lemma 12 (Dictionary learning gradient upper bound). The dictionary learning population

gradient obeys

 wgDpoLp(w)  2n
 grad[fDpoLp](q)  n

while in the finite sample case

w gDL (w)

2



 2n

X





grad[fDL](q)  n X 

where X is the data matrix with i.i.d. BG() entries.

Proof. Denoting x  (x, xn) we have

wgDpoLp(w) 2 =

E tanh

qx µ

x

-

xn

w qn

2

and using Jensen's inequality, convexity of the L2 norm and the triangle inequality to obtain

E

qx

2

qx

tanh

x + tanh

µµ

w xn qn

2

E

x 2+

w xn qn

2

 2n

while

grad[fDpoLp](q)  fDpoLp(q)

qx = E tanh µ x

  n

25

Under review as a conference paper at ICLR 2019

Similarly, in the finite sample size case one obtains

w gDL (w)

2 1 p

p

i=1

xi 2 +

xni

w qn

2
 2n

X

2 

grad[fDL](q)

1 p

p

tanh

i=1



 nX

qxi µ

xi

Proof of Theorem 2: (Gradient descent convergence rate for dictionary learning). The proof will follow exactly that of Lemma 11, with the finite sample size fluctuations decreasing the guaranteed change in  or ||w|| at every iteration (for the initial and final stages respectively) which will adversely affect the bounds.

B.6

C0

\B2 
1/20 5

(0)

To control the fluctuations in the gradient projection, we choose

y(, 0)

=

0cDL 2(8000(n - 1))3/2

which can

be

satisfied

by

choosing

y(, 0) =

c7 (1-)0 n3/2

for

an

appropriate

c7

>0

.

According

to Lemma 10, with probability greater than Py we then have

u(i)grad[fDL](q(w)) -E u(i)grad[fDL](q(w))

 y(, )

With the same condition on µ as in Lemma 11, combined with the uniformized bound on

finite sample fluctuations, we have that at every point in this set

u(i)grad[fDpoLp](q(w))



cDL 2(8000(n - 1))3/2



. According to Lemma 12 the Riemannian gradient norm is bounded by M = n X .

Choosing

r, b

as

in

Lemma

11,

we

require



<

360

X

1
 5n(n-1)

=

b-r 3M

and

obtain

from

Lemma 17



 

1

+

ncDL 4(8000(n - 1))3/2



t1 = log

log(8/0)



1

+

ncDL
4(8000(n-1))3/2

(20)

B.7

B2 
1/20 5

(0)\Bs2

(0)

From Theorem 2 in (38) there are numerical constants cb, c such that in this region



wwgDL(w) = w

 w

grad[f ](q(w)) c 

ww

with probability P > 1 - cbp-6. Following the same analysis as in Lemma 11, since

from Lemma 12 the norm of the gradient gradient is bounded by n||X|| we require



<

(1- w 2) w c C n||X ||2

which

is

satisfied

by

requiring

<

c~s n||X ||2

for

some

chosen

c~ > 0.

We

then obtain

t3

=

C2 s

(21)

26

Under review as a conference paper at ICLR 2019

for a suitably chosen C2 > 0.

B.8 Final rate and distance to minimizer

The final bound on the rate is obtained by summing over the terms for the three regions as

in the population case, and convergence is again to a distance of less than c3s from a local minimizer. The probability of achieving this rate is obtained by taking a union bound over

the probability of initialization in C0 (given in Lemma 3) and the probabilities of the bounds on the gradient fluctuations holding (from Lemma 10 and (38)). Note that the fluctuation

bound events imply by construction the event E = {1  X   4 log(np)} hence we

can replace

X



in the conditions on 

above by 4


log(np). The conditions on , µ can

be

satisfied

by

requiring



<

c5 s n log np

,

µ

<

c6 0 n5/4

for

suitably

chosen

c5, c6

> 0.

The

bound

on

the number of iterations can be simplified to the form in the theorem statement as in the

population case.

C Generalized Phase Retrieval

We show below that negative curvature normal to stable manifolds of saddle points in strict saddle functions is a feature that is found not only in dictionary learning, and can be used to obtain efficient convergence rates for other nonconvex problems as well, by presenting an analysis of generalized phase retrieval that is along similar lines to the dictionary learning analysis. We stress that this contribution is not novel since a more thorough analysis was carried out by (8). The resulting rates are also suboptimal, and pertain only to the population objective.

Generalized phase retrieval is the problem of recovering a vector x  Cn given a set of magnitudes of projections yk = |xak| onto a known set of vectors ak  Cn. It arises in numerous domains including microscopy (27), acoustics (2), and quantum mechanics (10)

(see (33) for a review). Clearly x can only be recovered up to a global phase. We consider the

setting where the elements of every ak are i.i.d. complex Gaussian, (meaning (ak)j = u + iv for u, v  N (0, 1/ 2)). We analyze the least squares formulation of the problem (7) given

by

1p

min f (z) =

zCn

2p

yk2 - |zak|2

2
.

k=1

Taking the expectation (large p limit) of the above objective and organizing its derivatives using Wirtinger calculus (22), we obtain

E[f ] = x 4 + z 4 - x 2 z 2 - |xz|2

(22)

E[f ] =

zE[f ] zE[f ]

 (2 z 2 - x 2)I - xx z 

=

(2 z 2 -

x 2)I - xxT

. z

For the remainder of this section, we analyze this objective, leaving the consideration of finite sample size effects to future work.

C.1 The geometry of the objective
In (37) it was shown that aside from the manifold of minima A  xei,
the only critical points of E[f ] are a maximum at z = 0 and a manifold of saddle points given by
A" \ {0}  z z  W, z = x 2

27

Under review as a conference paper at ICLR 2019

where W  {z|zx = 0}. We decompose z as z = w + ei x , x

(23)

where  > 0, w  W . This gives z 2 = w 2 + 2. The choice of w, ,  is unique up to

factors of 2 in , as can be seen by taking an inner product with x. Since the gradient

decomposes as follows:

zE[f ] =

2 z 2I-

x 2 I - xx

(w + ei

x )
x

= 2 z 2 - x 2 w + 2ei z 2 - x 2 x x

(24)

the directions ei

x x

,

w w

are unaffected by gradient descent and thus the problem reduces

to a two-dimensional one in the space (, w ). Note also that the objective for this two-

dimensional problem is a Morse function, despite the fact that in the original space there

was a manifold of saddle points. It is also clear from this decomposition of the gradient that

the stable manifolds of the saddles are precisely the set W .

It is evident from 24 that the dispersive property does not hold globally in this case. For z / B||x|| we see that gradient descent will cause  to decrease, implying positive curvature normal to the stable manifolds of the saddles. This is a consequence of the global geometry
of the objective. Despite this, in the region of the space that is more "interesting", namely
B||x||, we do observe the dispersive property, and can use it to obtain a convergence rate for gradient descent.

We define a set that contains the regions that feeds into small gradient regions around saddle points within B||x|| by

Q0  {z(, w )|  0}.

We will show that, as in the case of orthogonal dictionary learning, we can both bound
the probability of initializing in (a subset of) the complement of Q0 and obtain a rate for convergence of gradient descent in the case of such an initialization. 9

We now define four regions of the space which will be used in the analysis of gradient descent:

S1  S2  S3 

z

z

2

1 2

x2

z

1 2

x 2<

z 2  (1 - c)

x2

z (1 - c) x 2 < z 2  x 2

S4  z x 2 < z 2  (1 + c) x 2

defined

for

some

c

<

1 4

.

These

are

shown

in

Figure

4.

We now define

z

 z - zE[f ]  w

+  ei

x x

(25)

and using 24 obtain

 = 1 - 2( z 2 - x 2)  w = 1- 2 z 2- x 2 w .

(26a) (26b)

9Q0 is equivalent to the complement of the set C used in the analysis of the separable objective and dictionary learning.
28

Under review as a conference paper at ICLR 2019

Figure 4: The projection of the objective of generalized phase retrieval on the (

 x

,

w x

)

plane. The full red curves are the boundaries between the sets S1, S2, S3, S4 used in the analysis. The dashed red line is the boundary of the set Q0 that contains small gradient regions around critical points that are not minima. The maximizer and saddle point are

shown in dark green, while the minimizer is in pink.

These are used to find the change in , w at every iteration in each region: On S1:   (1 +  x 2) w w On S2:   (1 + 2c x 2) w w

On S3:

1- x 2 w  w

 1 - (1 - 2c) x 2 w

    (1 + 2c x 2)

On S4:

1 - (1 + 2c) x 2 w  w

 1- x 2 w

(1 - 2c x 2)    

(27a) (27b) (27c) (27d)
(27e) (27f )
(27g) (27h)

C.2 Behavior of gradient descent in i4=1Si

We now show that gradient descent initialized in S1\Q0 cannot exit i4=1Si or enter Q0 .

Lemma 14 guarantees that gradient descent initialized in 4i=1Si remains in this set. From

equation 27 we see that a gradient descent step can only decrease  if z  S4. Under the

mild

assumption

02

<

7 16

x 2 we are guaranteed from Lemma 13 that at every iteration

  0. Thus the region with  < 0 can only be entered if gradient descent is initialized in

it. It follows that initialization in S1\Q0 rules out entering Q0 at any future iteration of

gradient descent. Since this guarantees that regions that feed into small gradient regions are

avoided, an efficient convergence rate can again be obtained.

C.3 Convergence rate

Theorem 3 (Gradient descent convergence rate for generalized phase retrieval). Gradient

descent

on

22

with

step

size



<

4

c x

2

,

c

<

1 4

,

initialized

uniformly

in

S1

converges

to

a

point

29

Under review as a conference paper at ICLR 2019

z

such

that

dist(z,

A)

<

 5c

x

in

T

<

log x
2
log(1+ x

2)

+

log(2) 2 log(1+2c

x

2)

log(2c) log( 4 )

+ log(1-(1-2c)

x

7
2) log(1+2c

x

2)

iterations with probability



P1-

8 erf


2n ,
x

Proof. Please see Appendix C.3.

We find that in order to prevent the failure probability from approaching 1 in a high

dimensional setting, if we assume that x does not depend on n we require that  scale

like

1 n

.

This is simply the consequence of the well-known concentration of volume of a

hypersphere around the equator. Even with this dependence the convergence rate itself

depends only logarithmically on dimension, and this again is a consequence of the logarithmic

dependence of  due to the curvature properties of the objective.



Lemma

13.

For

any

iterate

z

of

gradient

descent

on

22,

assuming



<

4

c x

2

,

c

<

1 4

and

defining  as in 25, we have i)

z

4
Si 

w 2

x2 2

i=1

ii)

z



S4





2



7 16

x2

4
Proof of Lemma 13. i) From 27 we see that in Si the quantity w 2 cannot increase,
i=2
hence this can only happen in S1. We show that for some z  S1, a point with w =
(1 - ) x ,  < 1 cannot reach a point with w = x by a gradient descent step. This
22
would mean

1 -  2 w 2 + 22 - x 2

= 1 -  (1 - )2 x 2 + 22 - x 2

and since 2  0 this implies

= x 2

w (1 - ) x
2

1 +  x 2 (2 - ) (1 - )  1

by considering the product of these two factors, this in turn implies

1 (2 - )   x 2 (2 - )  1 2b



where we have used  <

b

c x

2

,

c

<

1 4

.

Thus if we choose b = 4 this inequality cannot be

satisfied.

Additionally, if we initialize in S1  Q0 then we cannot initialize at a point where w

= x
2

and hence the inequality is strict.

30

Under review as a conference paper at ICLR 2019

ii) Since only a step from S4 can decrease , we have that for the initial point z 2 > x 2.

Combined with

w 2

x2 2

this

gives

2  x 2 2
and using the lower bound (1 - 2 x 2 c)   we obtain

2

x

2
(1 - 2

x 2 c)2 

x

2
(1 - 4

x 2 c)

22

1 x2  (1 - )
2b 2



where

in

the

last

inequality

we

used

c

<

1 4

,



<

b

c x

2

.

Choosing

b

=

4

gives

2 7 x 2 16

If

we

require

02

<

7 16

x 2 this also ensures that the next iterate cannot lie in the small

gradient regions around the stable manifolds of the saddles.

Lemma 14. Defining z as in 25, under the conditions of Lemma 13 and we have
i)
44
z  Si  z  Si
i=2 i=2
ii) z  S1  z  S1  S2

Proof of Lemma 14. We use the fact that for the next iterate we have

z 2 = 1 - (2 z 2 - x 2) 2 w 2

+

1 - 2( z 2 -

x 2)

2
2

(28)



We

will

also

repeatedly

use



<

b

c x

2

,

c

<

1 4

and

z



4
Si 

w 2

x2 2

which

is

a

shown

i=1

in Lemma 13.

4
C.4 z  S3  z  Si
i=2

We want to show

x2 2

<
(1)

z

2  (1 + c) x 2.
(2)

1) We have z  S3  z 2 = (1 - ) x 2 for some   c and using 28 we must show

x2 2

1 -  x 2 (1 - 2) 2 w 2

+

1 + 2

x 2

2
2

or equivalently

A- x 2 2

31

Under review as a conference paper at ICLR 2019





x

2


-2(1 - 2) + (1 - 2)2 x 2 +4  + 2 x 2 2

w2 B



and

using



<

b

c x

2

,

c

<

1 4

-

x

2 -2 <

x

 c < -2

x 4B

bb

while on the other hand

Ac- x 2 <- x 2 24

thus picking b = 4 guarantees the desired result.

2) By a similar argument, z 2  (1 + c) x 2 is equivalent to



A

x

2


-2(1 - 2) +  x 2 (1 - 2)2 +4  +  x 2 2 2

w2 

 x 2 (c + )  B

. Since

w 2

x2 2

and

z 2

x 2  2 

x2 2

we

obtain

A    x 4 + 4 x 2  +  x 4 2

x2 2

11

1

< +2 1+

c x2

2b b

8b

. If we choose b = 4 we thus have A < B which implies

z 2 < (1 + c) x 2

4
C.5 z  S4  z  Si
i=2

We have z  S4  z 2 = w 2 + 2 = (1 + ) x 2 for some   c .

1)

x2 2

<

z

2 is equivalent to





x

2


A



-(

+

1 )

x

2

2

-4(1 + 2) +  x 2 (1 + 2)2

+4 - +  x 2 2 2

w2 B

. We have

B  -4 x 2 (1 + 2) w 2 + 2  - 15 x 2 8b

where the last inequality used

w 2

x2 2

and

z 2

x 2 (1 + c)  2 

x

2

(

1 2

+ c).

The choice b = 4 gaurantees A  B which ensures the desired result.

2) This is trivial since z 2  (1 + c) x 2 and in S4 both  and w decay at every iteration (ref eq).

32

Under review as a conference paper at ICLR 2019

4
C.6 z  S2  z  Si
i=2

1) We use z  S2 

z 2=

w

2

+

2

=

(

1 2

+ )

x

2

for

some



1 2

-c

.

Using

a

similar

argument as in the previous section, we are required to show

-

x 2<



x

2


+

4 - + 2 x 2 w 2 2(1 - 2) + (1 - 2)2 x 2


 2

B

where B  -

x2 b

implies that b = 4 gives the

desired

result.

2) The condition is equivalent to

A



x

2


+

4 - + 2 x 2 w 2 2(1 - 2) + (1 - 2)2 x 2

 + 2

x2

 ( 1 + c) x 2  B 2

One

can

show

by

looking

for

critical

points

of

A()

in

the

range

0

 



1 2

that

A

is

maximized at  = 0, since there is only one critical point at  =

4- bc+2

8

c b

c b

and A() < 0,

while

A( 1 )  2



-2

cc b + b2

w2

1 +

x2

2

A(0)  1

1 2+

x2

2b 2b 2

and in both cases b = 4 ensures A  B.

C.7 z  S1  z  S1  S2

We must show z

 (1 - c)

x 2 using

z

2 = (1 - )

x2 2

for 0    1.

z 2=

1 +  x 2 2 w 2 +

x2 1 + 2( + 1)

2
2

2



A

x

2


+

2 + 2 x 2 w 2

2( + 1) + ( + 1)2

x2 4

 ( 1 - c) x 2  B 2

 - x 2 2

and

since

A



1 2b

2

+

1 b

x2 2

and

B

x2 4

once

again

b=4

suffices

to

obtain

the

desired

result.

Lemma 15. For z parametrized as in 23, w 2 < c x 2  2 > (1 - c) x 2   dist(z, A) < 5c x

33

Under review as a conference paper at ICLR 2019

Proof of Lemma 15. Once w 2 < c x 2 for some z  S3  S4 we have z 2 = 2 + w 2  (1 - c) x 2

2  (1 - c) x 2 - w 2 > (1 - 2c) x 2

For some z = w + ei

x x

we have

dist2(z, A) = min


eix - w - ei

x x

=

w 2 + min


eix - ei

x x

2

2

= w 2 + (1 -  )2 x 2 = z 2 + x 2 - 2 x x

if we assume z 2  (1 + c) x 2

(29)

dist2(z, A)  (c + 2) x 2 - 2 x

(30)

 plugging in the value of  from 29 and using fact that - 1 - x  -1 + x for x < 1 we have

dist2(z, A) < (c + 2)

x

2

-

 21

-

2c

x

2  5c

x

2

Alternatively, if 2 > (1 - c) x 2 we have from 30

dist2(z, A)  (c + 2) x 2 - 2 x

< (c + 2)

x

2

-

 21

-

c

x

2  3c

x2

which

gives

the

desired

result.

In

particular,

if

we

choose

c

=

1 35

we

converge

to

dist2(z, A)

<

x 7

2

,

a

region

which

is

strongly

convex

according

to

(38).

Proof gence

oraf tTehfeoorrgeemne3r:al(iGzerdadpihenatsedreestcreinetvaclo)nver-.

We

now

bound

the

number

of

iter-

ations that gradient descent, after random initialization in S1, requires to reach a point

where one of the convergence criteria detailed in Lemma 15 is fulfilled. From Lemma 14, we

4
know that after initialization in S1 we need to consider only the set Si. The number of
i=1
iterations in each set will be determined by the bounds on the change in , ||w|| detailed in

27.

C.7.1 Iterations in S1
Assuming we initialize with some  = 0. Then the maximal number of iterations in this region is

0(1 +  x 2)t1 = x 2

log x

t1 =

0

log(1 + 

2
x

2)

since after this many iterations

z 2  2 

x 2

2

.

34

Under review as a conference paper at ICLR 2019

4
C.7.2 Iterations in Si
i=2
The convergence criteria are w 2 < c x 2 or 2 > (1 - c) x 2.
After exiting S1 and assuming the next iteration is in S2, the maximal number of iterations required to reach S3  S4 is obtained using

and is given by

  (1 + 2 x 2 c)

x (1 + 2 x 2 c)t2 = (1 - c) x 2 2

t2 =

log 2(1 - c) log(1 + 2 x 2 c)



log(2) 2 log(1 + 2 x

2 c)

since after this many iterations z 2  2  (1 - c) x 2. For every iteration in S3  S4 we are guaranteed
w  1 - (1 - 2c) x 2 w

thus using Lemmas 13.i and 15 the number of iterations in S3  S4 required for convergence

is given by

x2

1 - (1 - 2c)

x2

t3+4
=c

x2

2

log(2c)

t3+4 = log

1 - (1 - 2c)

x2

The only concern is that after an iteration in S3  S4 the next iteration might be in S2. To account for this situation, we find the maximal number of iterations required to reach S3  S4 again. This is obtained from the bound on  in Lemma 13.

Using this result, and the fact that for every iteration in S2 we are guaranteed 

(1 + 2

x

2 c)

the

number of 

iterations

required

to

reach

S3  S4

again

is

given

by

7

x

(1 + 2

x

2 c)tr

 = 1-c

x

4



tr

=

log 4 1-c
7
log(1 + 2 x

2 c)



log( 4 )
7
log(1 + 2 x

2 c)



C.8 Final rate

The final rate to convergence is

T < t1 + t2 + t3+4tr

=

log x
2
log(1+ x

2)

+

log(2) 2 log(1+2c

x

2)

log(2c) log( 4 )

+ log(1-(1-2c)

x

7
2) log(1+2c

x

2)

C.9 Probability of the bound holding
The bound applies to an initialization with   0, hence in S1\Q0 . Assuming uniform initialization in S1, the set Q0 is simply a band of width 20 around the equator of the

35

Under review as a conference paper at ICLR 2019

ball

B

x

 /2

(in

R2n,

using

the

natural

identification

of

Cn

with

R2n).

This

volume

can

be

calculated by integrating over 2n - 1 dimensional balls of varying radius.



Denoting r =

0 2 x

and by V (n) =

n/2

n 2

(

n 2

)

the hypersphere volume, the probability of

initializing in S1  Q0 (and thus in a region that feeds into small gradient regions around

saddle points) is

P(fail)

=

Vol(Q0 )

Vol(B

x

 /2

)

r

V (2n - 1)

(1

-

x2)

2n-1 2

dx

= -r V (2n)

V

(2n

-

1)

r

e-

2n-1 2

x2

dx

 -r V (2n)

1 n (n)

2n - 1

=

n

-

1 2

n

-

1 2

(

2n-1 2

)

erf

(

r) 2



8 erf( nr)



. For small  we again find that P(fail) scales linearly with , as was the case for the previous problems considered.

D Auxiliary Lemmas

D.1 Separable objective

gs(w) = tanh wi - tanh qn wi wi µ µ qn

2gs(w) = 1 sech2

wiwj

µ

wi µ

- tanh

qn µ

1 qn ij

+ 1 sech2 µ

qn µ

1 qn2

- tanh

qn µ

1 qn3 wiwj

D.2 Dictionary Learning

wgDpoLp(w) = E tanh

q(w)x µ

x - xn w qn(w)

D.3 Properties of C

Proof of Lemma 3: (Volume of C). We are interested in the relative volume

Vol(C ) Vol(Sn-1 )



V .

Using

the

standard

solid

angle

formula,

it

is

given

by

 x1/(1+)

1

V

=

lim
0

n/2

e-

 

x21

n


i=2

e-

 

xi2

dxi

dx1

0 -x1/(1+)



= lim 1

e-

 

x2

0 

0

x erf(
(1 + )

 n-1 ) dx


36

Under review as a conference paper at ICLR 2019

changing variables to x~ =

x  (1+)



V

=

(1+ ) 

e-(1+)2x2 erfn-1(x)dx

0

This integral admits no closed form solution but one can construct a linear approximation around small  and show that it is convex. Thus the approximation provides a lower bound for V and an upper bound on the failure probability.

From

symmetry

considerations

the

zero-order

term

is

V0

=

1 2n

.

The

first-order

term

is

given

by



V = 1 - 2 x2e-x2 erfn-1(x)dx

 =0 n



0

We now require an upper bound for the second integral since we are interested in a lower bound for V. We can express it in terms of the second moment of the L norm of a
Gaussian vector as follows:

 x

1 

x2e-x2 erfn-1(x) = 1 

x2e-x2  1 i

e-ti2 dtidx

0 0 -x

x

= 1 2

x2 e-x2/2 1 2 i 2

e-t2i /2dtidx

0 -x

1 =
4n

X

2 

dµ(X

)

1 =
4n

Var [ X ] + (E [ X ])2

where µ(X) is the Gaussian measure on the vector X  Rn. We can bound the first term using

Var [

X

]



maxVar
i

[|Xi|]

=

Var

[|Xi|]

<

Var

[Xi]

=

1

To bound the second term, we use the fact that for a standard Gaussian vector X (Xi  N (0, 1)) and any  > 0 we have

exp (E [ X

])  E

exp

max |Xi|
i

 E exp ( |Xi|) = nE [exp ( |Xi|)]
i
(using convexity and non-negativity of the exponent respectively)



nE [exp ( |Xi|)] = 2n exp (Xi) dµ(Xi)

0

 2nE [exp (Xi)] = 2n exp

2 2

37

Under review as a conference paper at ICLR 2019

taking the log of both sides gives

E

max |Xi|
i

 log(2n) +  2

and the bound is minimized for  = 2 log(2n) giving

E max |Xi|  2 log(2n)  2 log(n)
i
Combining these bounds, the leading order behavior of the gradient is

V  3 - 4 log(2n)  - log(n) .

 =0

4n

n

This linear approximation is indeed a lower bound, since using integration by parts twice we have



2V 2

=

1 

e-(1+ )2 x2

0

-6(1 + )x2 +4(1 + )3x4

erfn-1(x)dx


= - 2(n - 1) e-(1+)2x2 1 - 2(1 + )2x2 e-x2 erfn-2(x)dx 
0



4(n - 1)(n - 2)(1 + ) =

e-((1+)2+2)x2 erfn-3(x)dx > 0

3/2

0

where the last inequality holds for any n > 2 since the integrand is non-negative everywhere. This gives

V



1 2n

-

log(n) 
n

Lemma

16.

Bs()(0)  C

 B2 n-1s()(0)

where

s() =

1 .
(2+)+n

Bs()(0)

is

the

largest

L ball contained in C , and B2 n-1s()(0) is the smallest L2 ball containing C (where these

balls are defined in terms of the w vector). All three intersect only at the points where all

the coordinates of w have equal magnitude. Additionally, C  B1/2+(0) and this is the smallest L ball containing C.

Proof. Given the surface of some L ball for w , we can ask what is the minimal  such

that Cm intersects this surface. This amounts to finding the minimal qn given some w .

Yet this is clearly obtained by setting all the coordinates of w to be equal to w  (this is

possible since we are guaranteed qn 

w 

w



1 n

),

giving

1 - (n - 1) w

w

2
 = 1 + m

1 w  = (1 + m)2 + n - 1

38

Under review as a conference paper at ICLR 2019

thus,

given

some

,

the

maximal

L

ball

that

is

contained

in

C

has

radius

1 .
(2+)+n

The

minimal

L

norm

containing

C

can

be

shown

by

a

similar

argument

to

be

B

(0),

1/ 1+(1+)2

where one instead maximizes qn with some fixed w .

Given some surface of an L2 ball, we can ask what is the minimal C such that C  Br2(0).

This is equivalent to finding the maximal M such that CM intersects the surface of the

L2 ball. Since qn is fixed, maximizing  is equivalent to minimizing w . This is done by

setting

w

=

w n-1

,

which

gives

1- w 2 w n - 1 = 1 + M

n-1 =w
(2 + M )M + n

The statement in the lemma follows from combining these results.

Lemma 17 (Geometric Increase in ). For w  C0 \Bb (where  

qn w

- 1),

assume

|wi| > r  u(i)grad[f ](q(w))  c(w) where u(i) is defined in 7 and 1 > b > r. Then if

grad[f ](q(w)) < M and we define

q  expq(-grad[f ](q))

for



<

b-r 3M

,

defining



in an analogous way to  we have 

n

   1 + c(w)

2

Proof: D.3

Proof of Lemma 17:(Geometric Increase in ). Denoting g  grad[f ](q) , we have

q = cos(g)q - sin(g) grad[f ](q) g
hence, using Lagrange remainder terms,

qn = wi

g

qn - grad[f ](q)n - cos(t)(g - t)dtqn

0

g

+

sin(t)(g

-

t)dt

grad[f ](q)n g

0

g

wi - grad[f ](q)i - cos(t)(g - t)dtwi

0

g

+

sin(t)(g

-

t)dt

grad[f ](q)i g

0

.

We

assume

wi

>

0,

and

the

converse

case

is

analogous.

From

convexity

of

1 1+x

qn  wi

 g



sin(t)(g-t)dt

qn wi

+



 wi

-

0

wi g





grad[f ](q)i

-

wi qn

grad[f ](q)n

= qn + sin(g) wi wig

grad[f ](q)i

-

wi qn

grad[f ](q)n

39

Under review as a conference paper at ICLR 2019

= qn + sin(g) u(i)grad[f ](q(w)) wi wig

We

now

use



<

b-r 3M

<

 2M

 g

<

 2

 sin(g) 

g 2

and

consider

two

cases.

If

|wi| > r

we use the bound on the gradient projection in the lemma statement to obtain



qn  qn +  c(w)  qn +

n c(w)

wi wi 2wi

wi 2

hence



qn - 1 

qn

-1+

n c(w) =  1 +

n c(w)

wi w  2

2

(31)

If |wi| < r we rule out the possibility that |wi| =

w

 by demanding  <

b-r 3M

.

Since

b(b

-

r)

<

1

we

have

1

+

1 3

b(b

-

r)

<

1 + b(b - r) hence the requirement on  implies

1 + b(b - r) - 1 -2g + 4g2 + 4g2b(b - r)

< = gb

2g2b

. If we now combine this with the fact that after a Riemannian gradient step cos(g)qi - sin(g)  qi  cos(g)qi + sin(g), the above condition on  implies the inequality (), which in turn ensures that |wi| < r  |wi| < w :

|wi|

<

|wi|

+

sin(g)

<

r

+

g

<
()

(1

-

g22)b

-

g

< cos(g) w  - sin(g)  w 

Due to the above analysis, it is evident that any wi such that |wi| = w  obeys |wi| > r, from which it follows that we can use 31 to obtain



qn - 1 =    1 + n c(w)

w

2

40

