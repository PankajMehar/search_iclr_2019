Under review as a conference paper at ICLR 2019
DETERMINISTIC PAC-BAYESIAN GENERALIZATION
BOUNDS FOR DEEP NETWORKS VIA GENERALIZING
NOISE-RESILIENCE
Anonymous authors Paper under double-blind review
ABSTRACT
The ability of overparameterized deep networks to generalize well has been linked to the fact that stochastic gradient descent (SGD) finds solutions that lie in flat, wide minima in the training loss ­ minima where the output of the network is resilient to small random noise added to its parameters. So far this observation has been used to provide generalization guarantees only for neural networks whose parameters are either stochastic or compressed. In this work, we present a general PAC-Bayesian framework that leverages this observation to provide a bound on the original network learned ­ a network that is deterministic and uncompressed. What enables us to do this is a key novelty in our approach: our framework allows us to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves generalize to the interactions between the matrices on test data, thereby implying a wide test loss minimum. We then apply our general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data). In this setup, we provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weight matrices ­ a guarantee that would not have been possible with prior approaches.
1 INTRODUCTION
Modern deep neural networks contain millions of parameters and are trained on relatively few samples. Conventional wisdom in machine learning suggests that such models should massively overfit on the training data, as these models have the capacity to memorize even a randomly labeled dataset of similar size (Zhang et al., 2017). Yet these models have achieved state-of-the-art generalization error on many real-world tasks. This observation has spurred an active line of research (Neyshabur et al., 2015; Soudry et al., 2018; Brutzkus et al., 2018) that has tried to understand what properties are possessed by stochastic gradient descent (SGD) training of deep networks that allows these networks to generalize well.
One particularly promising line of work (Neyshabur et al., 2017; Arora et al., 2018) has been bounds that utilize the noise-resilience of deep networks on training data (i.e., how much the training loss of the network changes with noise injected into the parameters, or roughly, how wide is the training loss minimum). While these have yielded generalization bounds that do not have a severe exponential dependence on depth (unlike other bounds that grow with the product of spectral norms of the weight matrices), these bounds are quite limited: they either apply to a stochastic version of the classifier (where the parameters are drawn from a distribution) or a compressed version of the classifier (where the parameters are modified and represented using fewer bits).
In this paper, we revisit the PAC-Bayesian analysis of deep networks in Neyshabur et al. (2017; 2018). We provide a general framework that allows one to use noise-resilience of the deep network on training data to provide a bound on the original deterministic and uncompressed network. We achieve this by arguing that if on the training data, the interaction between the `activated weight matrices' (weight matrices where the weights incoming from/outgoing to inactive units are zeroed out)
1

Under review as a conference paper at ICLR 2019
satisfy certain conditions which results in a wide training loss minimum, these conditions themselves generalize to the weight matrix interactions on the test data.
After presenting this general PAC-Bayesian framework, we specialize it to the case of deep ReLU networks, showing that we can provide a generalization bound that accomplishes two goals simultaneously: i) it applies to the original network and ii) it does not scale exponentially with depth in terms of the products of the spectral norms of the weight matrices; instead our bound scales with more meaningful terms that capture the interactions between the weight matrices and do not have such a severe dependence on depth in practice. We note that all but one of these terms are indeed quite small on networks in practice. However, one particularly (empirically) large term that we use is the reciprocal of the magnitude of the network pre-activations on the training data (and so our bound would be small only in the scenario where the pre-activations are not too small). We emphasize that this drawback is more of a limitation in how we characterize noise-resilience through the specific conditions we chose for the ReLU network, rather than a drawback in our PAC-Bayesian framework itself. Our hope is that, since our technique is quite general and flexible, by carefully identifying the right set of conditions, in the future, one might be able to derive a similar generalization guarantee that is smaller in practice.
To the best of our knowledge, our approach of generalizing noise-resilience of deep networks from training data to test data in order to derive a bound on the original network that does not scale with products of spectral norms, has neither been considered nor accomplished so far, even in limited situations.
2 BACKGROUND AND RELATED WORK
One of the most important aspects of the generalization puzzle that has been studied is that of the flatness/width of the training loss at the minimum found by SGD. The general understanding is that flatter minima are correlated with better generalization behavior, and this should somehow help explain the generalization behavior (Hochreiter & Schmidhuber, 1997; Hinton & van Camp, 1993; Keskar et al., 2017). Flatness of the training loss minimum is also correlated with the observation that on training data, adding noise to the parameters of the network results only in little change in the output of the network ­ or in other words, the network is noise-resilient.
Most generalization bounds that do not make use of noise-resilience of the network have generalization guarantees that in practice have strong exponential dependence on the depth of the network. More precisely, these bounds scale either with the product of the spectral norms of the weight matrices (Neyshabur et al., 2018; Bartlett et al., 2017) or their Frobenius norms (Golowich et al., 2018). In practice, the weight matrices have a spectral norm that is as large as 2 or 3, and an even larger Frobenius norm that scales with H where H is the width of the network i.e., maximum number of hidden units per layer. 1 Thus, the generalization bound scales as say, 2D or HD 2, where D is the depth of the network.
At a high level, the reason these bounds suffer from such an exponential dependence on depth is that they effectively perform a worst case approximation of how the weight matrices interact with each other. For example, the product of the spectral norms arises from a naive approximation of the Lipschitz constant of the neural network, which would hold only when the singular values of the weight matrices all align with each other. However, in practice, for most inputs to the network, the interactions between the activated weight matrices are not as adverse.
By using noise-resilience of the networks, prior approaches (Arora et al., 2018; Neyshabur et al., 2017) have been able to derive bounds that replace the above worst-case approximation with smaller terms that realistically capture these interactions. However, these works are limited in critical ways. Arora et al. (2018) use noise-resilience of the network to modify and "compress" the parameter representation of the network, and derive a generalization bound on the compressed network. While this bound enjoys a better dependence on depth because its applies to a compressed network, the
1To understand why these values are of this order in magnitude, consider the initial matrix that is randomly 
initialized with independent entries with variance 1 H. It can be shown that the spectral norm of this matrix, with high probability, lies near its expected value, near 2 and the Frobenius norm near its expected value which
 is H. Since SGD is observed not to move too far away from the initialization regardless of H (Nagarajan & Kolter, 2017), these values are more or less preserved for the final weight matrices.
2

Under review as a conference paper at ICLR 2019
main drawback of this bound is that it does not apply on the original network. On the other hand, Neyshabur et al. (2017) take advantage of noise-resilience on training data by incorporating it within a PAC-Bayesian generalization bound (McAllester, 1999a). However, their final guarantee is only a bound on the expected test loss of a stochastic network.
In this work, we revisit the idea in Neyshabur et al. (2017), by pursuing the PAC-Bayesian framework (McAllester, 1999a) to answer this question. The standard PAC-Bayesian framework provides generalization bounds for the expected loss of a stochastic classifier, where the stochasticity typically corresponds to Gaussian noise injected into the parameters output by the learning algorithm. However, if the classifier is noise-resilient on both training and test data, one could extend the PAC-Bayesian bound to a standard generalization guarantee on the deterministic classifier.
Other works have used PAC-Bayesian bounds in different ways in the context of neural networks. Langford & Caruana (2001); Dziugaite & Roy (2017) optimize the stochasticity and the weights of the network in order to numerically compute good (i.e., non-vacuous) generalization bounds on the stochastic network. Neyshabur et al. (2018) derive generalization bounds on the original, deterministic network by working from the PAC-Bayesian bound on the stochastic network. However, as stated earlier, their work does not make use of noise resilience in the networks learned by SGD.
OUR CONTRIBUTIONS The key contribution in our work is a general PAC-Bayesian framework for deriving generalization bounds while leveraging the noise resilience of a deep network. While our approach is applied to deep networks, we note that it is quite general (as will be apparent from Section 3) and can be applied to other classifiers.
In our framework, we consider a set of conditions that when satisfied by the network, makes the output of the network noise-resilient at a particular input datapoint. For example, these conditions could characterize the interactions between the activated weight matrices at a particular input. To provide a generalization guarantee, we assume that the learning algorithm has found weights such that these conditions hold for the weight interactions in the network on training data (which effectively implies a wide training loss minimum). Then, as a key step, we generalize these conditions over to the weight interactions on test data (which effectively implies a wide test loss minimum) 2. Thus, with the guarantee that the classifier is noise-resilient both on training and test data, we derive a generalization bound on the test loss of the original network.
Our framework relies on a novel technique to convert the PAC-Bayesian bound on a stochastic classifier to the generalization bound on the deterministic classifier. This technique is arguably more powerful than known PAC-Bayesian approaches, in leveraging noise-resilience, and is thus of independent theoretical interest.
Finally, we apply our framework to a specific set up of ReLU based feedforward networks. In particular, we first instantiate the above abstract framework with a set of specific conditions, and then use the above framework to derive a bound on the original network. While very similar conditions have already been identified in prior work (Arora et al., 2018; Neyshabur et al., 2017), our contribution here is in showing how these conditions generalize from training to test data. Crucially, like these works, our bound does not have severe exponential dependence on depth in terms of products of spectral norms.
We note that in reality, all but one of our conditions on the network do hold on training data as necessitated by the framework. The strong, non-realistic condition we make is that the pre-activation values of the network are sufficiently large, although only on training data; however, in practice a small proportion of the pre-activation values can be arbitrarily small. Our generalization bound scales inversely with the smallest absolute value of the pre-activations on the training data, and hence in practice, our bound would be large.
It is worth noting that Arora et al. (2018); Neyshabur et al. (2017) too require similar assumptions about pre-activation values; their assumptions are more realistic in that they effectively allow for a small proportion of units to have small pre-activation values. However, even under our stronger condition that no such units exist, it is not apparent how these approaches would yield a similar bound on the deterministic, uncompressed network without generalizing their conditions to test data.
2Note that we can not directly assume these conditions to hold on test data, as that would be `cheating' from the perspective of a generalization guarantee.
3

Under review as a conference paper at ICLR 2019

We hope that in the future our work could be developed further to accommodate the more realistic conditions from Arora et al. (2018); Neyshabur et al. (2017).

3 A GENERAL PAC-BAYESIAN FRAMEWORK

In this section, we present our general PAC-Bayesian framework that uses noise-resilience of the network to convert a PAC-Bayesian generalization bound on the stochastic classifier to a generalization bound on the deterministic classifier.

NOTATION.
 norm of a Consider a K

-vcLelceattsosKr,leLaan(rdn in)gd2tea,nsok tFewthdheeerneKottLhee-tdhliaevbeserplgeeecdntrdcaaelt,aanpdoidFnertsonbo(exten,ityuh)seanro2ernmdoraormwf anomffraoatmvriexac,ntroeurs,npdeecrtliyvitenhlgye.

distribution D over X × {1, 2, , K} where X  RN and x  X , x  B. We consider a neural

network of D layers (we will mostly care about D > 2) mapping from RN  RK. The layers are fully

connected with H units in each hidden layer, and with ReLU activations  () on all the hidden units

and linear activations on the output units. The network such that the output of the network is computed as f output for the kth class as f (x; W) [k]. We denote

(isx;pWara)m=etWeriDzed(bWyDW-1=(W(W1,1Wx2)), an, Wd tDhe) the value of the hth hidden unit on the dth

layer before and after the activation by gd (x; W) [h] and f d (x; W) [h] respectively. We define

Jd d (x; W) = gd (x; W) gd (x; W) to be the Jacobian of the pre-activations of layer d with respect to the pre-activations of layer d for d  d (each row in this Jacobian corresponds to a unit

in layer d). In short, we will call this, Jacobian d d. We will denote the parameters of a random

initialization of the network by Z = (Z1, Z2, , ZD). In our PAC-Bayesian analysis, we will use

U = (U1, U2, , UD) from a Gaussian, and

 W

N +

(0, 2) to denote parameters whose U to denote the entrywise addition of

entries are sampled independently the two sets of parameters. We use

W

2 F

to denote dD=1

Wd

2 F

.

Finally,

given

a

training

set

S

of

m

samples,

we

let

(x, y)



S

to

denote uniform sampling from the set. See Appendix A for more notations.

TRADITIONAL PAC-BAYESIAN BOUNDS. The PAC-Bayesian framework (McAllester, 1999a;b) allows us to derive generalization bounds for a stochastic classifier. Specifically, let W~ be a random variable in the parameter space whose distribution is learned based on training data S. Let P be a prior distribution in the parameter space chosen independent of the training data. Let L(W~ , x, y) be the loss of the classifier W~ on a datapoint (x, y), where the loss between 0 and 1; in this paper, we take the loss to be the 0-1 classification error. The PAC-Bayesian framework yields the following generalization bound on the stochastic classifier that holds with probability 1 -  over the draw of the training set S of m samples:
EW~ [E(x,y)D[L(W~ , x, y)]]  EW~ [E(x,y)S[L(W~ , x, y)]] + 2 2(KL(W~ P ) + ln m ) (m - 1)
Typically, and in the rest of this discussion, W~ is a Gaussian with covariance 2I for some  > 0 centered at the weights W learned based on the training data. Furthermore, we will set P to be a Gaussian with covariance 2I centered at the random initialization of the network like in Dziugaite & Roy (2017), instead of at the origin, like in Neyshabur et al. (2018). This is because the resulting KL-divergence ­ which depends onthe distance between the means of the prior and the posterior ­ is known to be smaller, and to save a H factor in the bound (Nagarajan & Kolter, 2017).

3.1 OUR FRAMEWORK
To extend the above PAC-Bayesian bound to a standard generalization bound on a deterministic classifier W, we need to replace the training and the test loss of the stochastic classifier with that of the original, deterministic classifier. However, in doing so, we will have to introduce extra terms in the upper bound to account for the perturbation suffered by the train and test loss under the Gaussian perturbation of the parameters. To tightly bound these two terms, we need that the network is noise-resilient on training and test data respectively. Our hope is that if the learning algorithm has found weights such that the network is noise-resilient on the training data, we can then generalize this noise-resilience over to test data as well, allowing us to better bound the excess terms.

4

Under review as a conference paper at ICLR 2019

Below, we lay out the key components of our framework, and constraints on how these should be designed; following this we present our main result and some intuition about our proof technique.

INPUT-DEPENDENT PROPERTIES OF WEIGHTS In our framework, we characterize noise-resilience through a set of say, R (abstract) conditions on how the activated weight matrices interact with each other for a given input. To define these noise-resilience-related conditions, we will consider R sets of the form {r(1)(W, x, y), r(2)(W, x, y), , } for r = 1, 2, 3, , R; here (rl)(W, x, y) computes a scalar value which corresponds to an input-dependent property of the weights (and l is an index for the elements within a set). The reader can think of each set here as the set of properties of a particular layer in the network (the reason for grouping them in this manner will be apparent soon). For example, in the next section, for ReLU networks, we will consider D sets, each consisting of the pre-activations of each layer {gd (x; W) [1], , gd (x; W) [H]} for d = 1, 2, D. (Although, as we will see, we will actually consider even more sets of properties amounting to R = O(D2) sets in total.)

Next, we define conditions that bound the properties. To do this, for each set, we define a function

Tr(W, x, y) that aggregates the values of the properties in that set; then, a condition of the form Tr(W, x, y) > 0 would imply a bound on the properties in the set. For example, on ReLU networks,

we will define Td(W, x, y) = pre-activation values of the dth

mlayinehHr;=t1hegdco(nxd;iWtio)n[Thd](-W, x(,fyo)r

some  > 0) which aggregates the > 0 imposed on all the training data,

would imply that these pre-activation values on the training set have magnitude at least .

For convenience, we instantiate the Rth set of properties to be the set of K

classifier outputs {f D (x; W) [1], , f D (x; W) [K]} and (f (x; W) [y] - maxjy f (x; W) [j]) 2. Note that the sign of

define TR (W ,

x,TyR)

(W, x, y) corresponds

= to

the 0-1 error.

CONSTRAINT 1: AGGREGATION OF THE PROPERTIES In this framework, we impose the following constraint on how these aggregator functions are designed:

Tr(W, x, y) - Tr(W, x, y)

 max
l

(rl)(W, x, y) - r(l)(W, x, y)

(1)

This limits us to functions that are linear in terms of the extreme values/absolute values from the corresponding set of properties, like the example function Td(W, x, y) = minhH=1 gd (x; W) [h] - .

CONSTRAINT 2: ORDERING OF THE SETS OF PROPERTIES Next, we impose a crucial constraint

on how the properties are grouped and indexed, that will help us show that these properties themselves

are noise-resilient. Roughly speaking, we want that for a given input, the properties in the r-th set are noise-resilient, whenever the first r - 1 sets of properties satisfy the bounds imposed on them through the conditions Tq(W, x, y) > 0 for q = 1, 2, r - 1. For example, in the case of

ReLU networks, we will show that the perturbation in the pre-activation values of the dth layer i.e.,

maxh norms

gd of

(x; the

W) [h]-gd (x; W + U ) weights) w.h.p over U 

[h] , is small (i.e., doesn't N (0, 2I) as long as the

scale with the product of the spectral absolute pre-activation values in the

layers below d - 1 are large (and a few other norm-bounds on the lower layer weights are satisfied).

We the for

apfloelrrtmthuearblpiazrteeicotehndeiinnabgthoqeve<prrroe.qpTuehrirteieenms, etahnsetfbfooyllloldowewfisi.nniFgnogsrhaaonsuyeldtrohfoelRdx:,plreetssxionXs b1e(su)c,hth, atRT(q(W) th, xat,

bound y) > 0

P rUN (0,2I)

max
l

(rl)(W + U , x, y) - r(l)(W, x, y)

> r() 2



q<r, max
l

q(l)(W

+

U

,

x,

y)

-

q(l) (W ,

x,

y)

<

q () 2

 R1 m .

(2)

In other words, when the first r - 1 conditions Tq(W, x, y) > 0 are satisfied at an x, the event that i) the properties involved in the rth condition suffers a large perturbation and that ii) the properties involved in the first r - 1 functions do not suffer much perturbation ­ which implies that the first r - 1 conditions are still approximately satisfied under perturbation ­ together hold, must have a small probability. (When we instantiate the framework, we have to derive closed form expressions for r() satisfying the above inequality, like we will do in Lemma E.1 for ReLU networks.)

5

Under review as a conference paper at ICLR 2019

THEOREM STATEMENT In this setup, we have the following `margin-based' generalization guaran-

tee on the original network. That is, we bound the 0-1 test error of the network by a margin-based

error on the training data. Our generalization guarantee ­ which scales linearly with the number of

conditions R ­ holds under the setting that the training algorithm always finds weights such that on the training data, the condition that Tr(W, x, y) is sufficiently large is satisfied for all r = 1, , R - 1.

TLwehittehoprreobmbea3tbh.1iel.imtyLae1xti-mculamossv>evra0ltu.heLe eodtfraw1s,uocf hs2a,tmhap,tlefosRrSbaeflrloarms=eDt1om, f,p,fooRsri,taivneyr

constants, where (W)weharv. eTthheant,,

R for

a=nyclas>s

2. 0,

if W satisfies

Tr(W, x, y) > r for all r < R and for all (x, y)  S, then3

Pr(x,y)D

f (x; W) [y] < max f (x; W) [j]
jy

Pr(x,y)S

f

(x;

W

)

[y]

<

max
jy

f

(x;

W)

[j]

+

class

+

2O~

 R

2KL(N (W, ()2I) P ) 

m-1



The crux of our on the training

proof (in Appendix D) lies data to test data one after

in generalizing the conditions Tr(W, x, y) the other. Crucially, after we generalize

>the rfirssattirsfi-ed1

conditions from training data to test data, we will have from Equation 2 that the rth set of properties

are noise-resilient on both training and test data. Given the noise-resilience on the rth set of properties

on test data, we can generalize the rth condition to test data too.

We emphasize a key, fundamental tool that we develop for use in this theorem. Specifically, in Theorem C.1 we present a new technique to convert a generic PAC-Bayesian bound on a stochastic classifier, to a generalization bound on the deterministic classifier. We argue in Section C.1 this technique is more powerful than prior approaches (Neyshabur et al., 2018; Langford & Shawe-Taylor, 2002; McAllester, 2003) in leveraging the noise-resilience of a generic classifier.

The high level argument is that, to convert the PAC-Bayesian bound, previous works relied on a looser output perturbation bound, one that holds on all possible inputs, with high probability over all perturbations i.e., a bound on maxx f (x; W) - f (x; W + U )  w.h.p over draws of U . In contrast, our technique relies on a subtly different but significantly tighter bound; specifically, we rely on a bound on the output perturbation that holds with high probability given an input i.e., a bound on f (x; W) - f (x; W + U)  w.h.p over draws of U for each x. When we do instantiate our framework as in the next section, this subtle difference is critical in being able to bound the output perturbation without suffering from a factor proportional to the product of the spectral norms of the weight matrices (which is the case in Neyshabur et al. (2018)).

4 APPLICATION OF OUR FRAMEWORK TO RELU NETWORKS
In this section, we apply our framework to feedforward ReLU networks and derive a generalization bound on the original network that does not scale with the product of spectral norms of the weight matrices. Informally, we consider a setting where the learning algorithm satisfies the following conditions on the training data that make it noise-resilient on training data: a) the Jacobian of any layer with respect to a lower layer, has rows with a small 2 norm, b) the 2 norm of the hidden layers are all small and c) the pre-activation values are all sufficiently large in magnitude. We cast these conditions in terms of the functions Tr in the general framework. These properties are quite similar to those already explored in Arora et al. (2018); Neyshabur et al. (2017); we provide more intuition about these properties, and how we cast them in our framework in Appendix E.1.
We prove in Lemma E.1 in Appendix E a guarantee equivalent to the abstract inequality in Equation 2. Essentially, we show that under random perturbations of the parameters, the perturbation in the output of the network and the perturbation in the input-dependent properties involved in (a), (b), (c) themselves can all be bounded in terms of each other. The highlight of these perturbation bounds is that none of them grow with the spectral norms of the network.
Having set up the conditions of the framework as required, we then instantiate the bound provided by the framework as described below. Our generalization bound scales with the bounds on the properties
3We use O~() to hide logarithmic factors.

6

Under review as a conference paper at ICLR 2019

in (a) and (b) above as satisfied on the training data, and with the reciprocal of the property in (c)

i.e., the smallest absolute value of the pre-activations on the training data. Additionally, our bound

has an explicit quadratic dependence on the depth of the network, which arises from the fact that we generalize O(D2) conditions. Most importantly, our bound does not have a dependence on the

product of the spectral norms of the weight matrices.

Theorem. F.1 (shorter version; see Appendix F for the complete statement) Let d d , d, d be a

set of draw

positive constants. For of samples from Dm, for

any any

Wma,rigfiWn sclaatsiss>fie0s,thaendfoallnoywin>g

0, with probability three conditions for

1- every

over the training

data point (x, y)  S:

(a) Sufficiently small maximum row 2 norm of Jacobian d d, maxh J d d (x; W)[h]  d d for all d, d such that 1  d  d  D. (b) Sufficiently small 2 norm of layer d's output: f d (x; W)  d for all d such that 0  d < D. (c) Sufficiently large pre-activations on layer d: minh gd (x; W) [h]  d for d such that 1  d < D.
then, for such W, we have that:

Pr(x,y)D

f (x; W) [y] < max f (x; W) [j]
jy

Pr(x,y)S

f

(x;

W

)

[y]

<

max
jy

f

(x;

W)

[j]

+

class

+ O~ D2

W -Z

2 F

((m - 1)()2)

Here 1  equals O~(H max{Bjac, Bout, Bhidden-preact, Boutput-preact}), where4:

Bjac

=

max
1d <dD

dd =d +1

d


d d-1

d d

d

, Bout

=

max
dD-1

dd=1

d d d-1 d

Bhidden-preact

=

max
dD-1

dd

=1 d H

d d d

-1

,

Boutput-preact

=

2 dD=1 D d d-1 H class

In Figure 1, we demonstrate that in practice the row 2 norms of the Jacobian and the 2 norms of the hidden layer outputs are orders of magnitude smaller than products of the spectral norms. In Figure 2, we train networks of varying depth and demonstrate that the terms Bjac, Bout, Boutput-preact also do not exhibit such severe exponential dependence on depth. (In Appendix F, we demonstrate that these terms decay with width, although our emphasis is not on the width-dependence.)
However, the bottleneck in our bound is Bhidden-preact, which scales inversely with the magnitude of the smallest absolute pre-activation value of the network. In practice, this term can be arbitrarily large, even though it does not depend on the product of spectral norms/depth (Figure 2). This is because some hidden units can have arbitrarily small absolute pre-activation values ­ although this is true only for a small proportion of these units. To give an idea of the typical magnitude of the pre-activation values, in Figure 2, we plot a modified term called `median-based'-Bhidden-preact, where we replace d in the denominator with a different constant that bounds the median absolute pre-activation value for each training datapoint; `median-based'-Bhidden-preact turns out to be a significantly smaller quantity.
As noted before, we emphasize that the drawback highlighted above is a limitation in how we characterize noise-resilience through our conditions rather than a drawback in our general PACBayesian framework itself. Improving this dependence is an important direction for future work. For example, one way to do this, would be to modify our analysis to allow perturbations large enough to flip a small proportion of the activation states; one could potentially formulate such realistic conditions by drawing inspiration from the conditions in Neyshabur et al. (2017); Arora et al. (2018).
However, we note that even though these prior approaches made more realistic assumptions about the magnitudes of the pre-activation values, the key limitation in these approaches is that even under our non-realistic assumption, their approaches would yield bounds only on stochastic/compressed networks. Generalizing noise-resilience from training data to test data is crucial to extending these bounds to the original network, which we accomplish.
4Note that 0 is an upper bound on the `zeroth' layer i.e., on x , which we assumed to be B.
7

Under review as a conference paper at ICLR 2019

Max 2 row norm of J10/d l2 Norm of dth layer's output

8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 10
depth d

120 100 80 60 40 20
0 1 2 3 4 5 6 7 8 9 10 depth d

Figure 1: In all these plots, we train a network with D = 11, H = 1280 to minimize cross-entropy

loss on the MNIST dataset. Left: Each black point corresponds to the maximum row 2 norm of the

Jacobian 10 d. Observe that for any d, these quantities are nowhere near as large as a naive upper

bound that would roughly scale as 1d0=d Wd 2 = 210-d. Right: Each black point corresponds to a

particular training example x, and has y-value equal to the 2 norm of the output of layer d for that

datapoint. A naive upper bound on this value would be at least 100 times larger than the observed value for d =

x 10.

dd=1

Wd

2  10  2d, which would be

Upper bound Upper bound Upper bound

Bjac vs Depth
9 8 7 6 5 4 3 2 13 4 5 6 7 8 9 10 11
Depth

Bout vs Depth
18 16 14 12 10 8 6 4 2 03 4 5 6 7 8 9 10 11
Depth

Boutput-preact vs Depth 70 60 50 40 30 20 10 03 4 5 6 7 8 9 10 11
Depth

Upper bound Upper bound

Bhidden-preact vs Depth
9 ×107 8 7 6 5 4 3 2 1 03 4 5 6 7 8 9 10
Depth

11

`median'-Bhidden-preact vs Depth 400 350 300 250 200 150 100 50
03 4 5 6 7 8 9 10 11 Depth

Figure right)

2: In the above figure, we Bhidden-preact (bottom left)

plot and

t`hme evdailaunes-boafsBedja'c-B(thoidpdelne-pfrte)acBt o(ubt o(ttotopmmridigdhlet,)

Boutput-preact (top plotted only for

comparison) computed on networks of width 40 with varying depth, trained on the MNIST dataset.

While it is hard to quantify how Bhidden-preact varies, all other values do not increase with depth as

rapidly as the product of the spectral norms (i.e., for a additive increase of 1 in the depth, the bound

does not increase by a multiplicative factor of 2.).

5 SUMMARY AND FUTURE WORK
In this work, we introduced a novel PAC-Bayesian framework for leveraging the noise-resilience of neural networks on training data, to derive a generalization bound on the original uncompressed, deterministic network. The main philosophy of our approach is to first generalize the noise-resilience from training data to test data; this allows us to convert a PAC-Bayesian bound on a stochastic network to a standard margin-based generalization bound. We also build a fundamental technique for converting PAC-Bayesian bounds, that is more powerful than prior approaches. We apply our approach to ReLU based networks and derive a bound that scales with terms that capture the interactions between the weight matrices better than the product of spectral norms.
Our work brings up two concrete directions for future work. The most important direction is that of removing the dependence on our strong assumption that the magnitude of the pre-activation values of the network are not too small on training data. Another interesting direction would be to tighten the dependence on D2 in our bound in Theorem F.1. More generally, our framework for generalizing noise-resilience is a promising tool for tapping the noise-resilience of the network to provide generalization guarantees on the original network; we hope that by further studying and applying this tool, we will be better equipped to answer the generalization puzzle in the future.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In The 35th International Conference on Machine Learning, ICML, 2018.
Peter L. Bartlett, Dylan J. Foster, and Matus J. Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, pp. 6241­6250, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns overparameterized networks that provably generalize on linearly separable data. International Conference on Learning Representations, 2018.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. Computational Learning Theory, COLT 2018, 2018.
Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, COLT, 1993.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Flat minima. Neural Computation, 9(1):1­42, 1997.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. International Conference on Learning Representations, 2017.
John Langford and Rich Caruana. (not) bounding the true error. In Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001], pp. 809­816, 2001.
John Langford and John Shawe-Taylor. Pac-bayes & margins. In Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, pp. 423­430, 2002.
David McAllester. Simplified pac-bayesian margin bounds. In Learning Theory and Kernel Machines, pp. 203­215. Springer Berlin Heidelberg, 2003.
David A. McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355­363, 1999a.
David A. McAllester. Pac-bayesian model averaging. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT 1999, pp. 164­170, 1999b.
Vaishnavh Nagarajan and J. Zico Kolter. Generalization in deep networks: The role of distance from initialization. Deep Learning: Bridging Theory and Practice Workshop in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 2017.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. International Conference on Learning Representations Workshop Track, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. Advances in Neural Information Processing Systems to appear, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. International Conference on Learning Representations, 2018.
9

Under review as a conference paper at ICLR 2019 Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. International Conference on Learning Representations, 2018. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. International Conference on Learning Representations, 2017.
10

Under review as a conference paper at ICLR 2019

APPENDIX

A NOTATIONS (CONTINUED)

We will use upper-case symbols to denote matrices, and lower-case bold-face symbols to denote vectors. In order to make the mathematical statements/derivations easier to read, if we want to emphasize a term, say x, we write, x.

Recall that we consider a neural netork of depth D (i.e., D - 1 hidden layers and one output layer) mapping from RN  RK, where K is the number of class labels in the learning task. The layers are fully connected with H units in each hidden layer, and with ReLU activations  () on all the hidden

units and linear activations on the output units. We denote the parameters of the network using the

symbol W, which in WD  RK×H and for

turn denotes a set all other layers d

of1w, Deig, hWt dmatRriHce×sHW. 1W, We w2,ill u, sWe Dth.eHneortea,tiWon1

 RH×N , and Wd to denote

the first d weight matrices. We denote the vector of weights input to the hth unit on the dth layer

(which corresponds to the hth row in Wd) as whd .

For any input x  RN , we denote the function computed by the network on that input as f (x; W) = WD (WD-1 (W1x)). For any d = 1, , D - 1, we denote the output of the dth hidden layer after the activation by f d (x; W). We denote the corresponding pre-activation values for that layer by gd (x; W). We denote the value of the hth hidden unit on the dth layer after and before the activation by f d (x; W) [h] and gd (x; W) [h] respectively. Note that for the output layer d = D, these two values are equal as we assume only a linear activation. For d = 0, we define f 0 (x; W) = x. As a result, we have the following recursions:

f d (x; W) =  gd (x; W) , d = 1, 2, , D - 1 f D (x; W) = gD (x; W) = f (x; W) , gd (x; W) = Wdf d-1 (x; W) , d = 1, 2, , D gd (x; W) [h] = wdh  f d-1 (x; W) [h], d = 1, 2, , D

For layers d, d such that d  d, let us define Jd d (x; W) to be the Jacobian corresponding to the pre-activation values of layer d with respect to the pre-activation values of layer d on an input x. That is,

Jd

d (x; W)

=

gd (x; W) gd (x; W)

In other words, this corresponds to the product of the `activated' portion of the matrices Wd+1, Wd+2, , Wd, where the weights corresponding to inactive inputs are zeroed out. In short, we will call this `Jacobian d d'. Note that each row in this Jacobian corresponds to a unit on the dth layer, and each column corresponds to a unit on the dth layer.

We will denote the parameters of a random initialization of the network by Z = (Z1, Z2, , Zd). Let D be an underlying distribution over RN × {1, 2, , K} from which the data is drawn.

In our PAC-Bayesian analysis, we will use U to whose entries are sampled independently from a only the first d of the randomly sampled weight

mdGeaanturositcseeiasan, s.aenFtduorWfthDe+rmwUeodirgetoh, twdmeenawotrtiielcleaussneUetU1w,dUotr2ok,dwe,hnUeortDee

the d f (x;

random matrices are added to the first d weight matrices in W. Note W + Ud) is the output of a network where the first d weight matrices

that W + U0 = W. Thus, have been perturbed. In

our analysis, we will also need to study a perturbed network where the hidden units are frozen to

be at the activation state they were at before the perturbation; we will use the notation W[+Ud] to

denote the weights of such a network.

For our statements regarding probability of events, we will use , , and ¬ to denote the intersection, union and complement of events (to disambiguate from the set operators).

11

Under review as a conference paper at ICLR 2019

B USEFUL LEMMAS

In this section, we present some results. The first two results below will be useful for our noise resilience analysis.

HOEFFDING BOUND

Lemma B.1. with mean µi

For and

via=ri1a,n2c,e,i2n.,TlehteXn fiobreailnl dtepe0n, dweenht araven:dom

variables

sampled

from

a

Gaussian

n
Pr (Xi - µi)  t
i=1

 exp

-

2

t2 in=1

i2

.

Or alternatively, for   (0, 1] Pr i=n1(Xi - µi) 

2

n i=1

i2

ln

1 







Note that an identical inequality holds the probability that the event in=1 Xi

-goµoid>sytmhomldestr,iicsalaltymfoorstthtwe iecveenthtefina=i1luXrei

- µi  -t, and so probability in the

above inequalities.

PRODUCT OF AN ENTRYWISE GAUSSIAN MATRIX AND A VECTOR Lemma B.2. Let U be a H1 × H2 matrix where each entry is sampled from N (0, 2). Let x be an arbitrary vector in RH2 . Then, U x  N (0, x 222I).
Proof. U x is a random vector sampled from a multivariate Gaussian with mean E[U x] = 0 and co-variance E[U xxT U T ]. The (i, j)th entry in this covariance matrix is E[(uiT x)(ujT x)] where ui and uj are the ith and jth row in U . When i = j, E[(uTi x)(ujT x)] = E[ uTi x 2] = hH=21 E[u2ih]xh2 = 2 x 22. When i  j, since ui and uj are independent random variables, we will have E[(uTi x)(uTj x)] = Hh=21 E[uihxh] Hh=21 E[ujhxh] = 0.

KL DIVERGENCE OF GAUSSIANS. We will use the following KL divergence equality to bound the

generalization error in our PAC-Bayesian analyses.

Lemma B.3. N (µ2, 2I).

Let P be the spherical Gaussian Then, the KL-divergence between Q

N (µ1, 2I) and P is:

and

Q

be

the

spherical

Gaussian

KL(Q P ) =

µ2 - µ1 2 22

C PAC-BAYESIAN THEOREM

In this section, we will present our main PAC-Bayesian theorem that will guide our result about generalization. Concretely, our result extends the generalization bound provided by conventional PAC-Bayesian analysis McAllester (2003) ­ which is a generalization bound on the expected loss of a distribution of classifiers i.e., a stochastic classifier ­ to a generalization bound on a deterministic classifier. The way we reduce the PAC-Bayesian bound to a standard generalization bound, is different from the one pursued in previous works (Neyshabur et al., 2018; Langford & Shawe-Taylor, 2002; McAllester, 2003).
The generalization bound that we state below is a bit more general than standard generalization bounds. Typically, generalization bounds are on the classification error; however, as discussed in the main paper we will be dealing with generalizing multiple different conditions on the interactions between the weights of the network from the training data to test data.

12

Under review as a conference paper at ICLR 2019

So for

to state a bound r = 1, 2, R (as

that is general enough, we consider initially considered in the main paper

a set of generic functions in Section 3). Recall that

Tr (W , each of

x, y) these

functions compute a scalar value that corresponds to some input-dependent property of the network with parameters W for the datapoint (x, y). As an example, this property could simply be the margin of the function on the yth class i.e., f (x; W) [y] - maxjy f (x; W) [j], though we consider many

other input-dependent properties of the network when we apply this theorem.

Theorem C.1. Let P be a prior distribution over the parameter space that is chosen independent of

the training dataset. Let U be a random variable

and r network

>0 W

for r to be

= 1, 2, R, be noise-resilient

a set of functions with respect to all

sampled entrywise from N (0, 2). Let Tr(, , ) and their corresponding margins. We define the these functions, at a given data point (x, y) if:

PrUN (0,2) r 

Tr(W, x, y) - Tr(W

+ U, x, y)

>

r 2

 1 . m

(3)

LfreotmµDD (, {th(Tatr

, r)}Rr=1, W) denote the the network with weights

probability over the random draw of a point (x, y) drawn W is not noise-resilient at (x, y) according to Equation 3.

That is, let µD({(Tr, r)}Rr=1, W) =

Pr(x,y)D PrUN (0,2) r 

Tr(W, x, y) - Tr(W

+ U, x, y)

>

r 2

> 1 m

Similarly, let µ^S({(Tr, r)}rR=1, W) denote the fraction of data points (x, y) in a dataset S for which the network is not noise-resilient according to Equation 3. Then for any , with probability 1 -  over the draws of a sample set S = {(xi, yi)  D i = 1, 2, , m}, for any W we have:

Pr(x,y)D [r



Tr

(W

,

x,

y)

<

0]



1 m

(x,y)S

1

[r



Tr(W, x, y) < r] + µ^S({(Tr, r)}Rr=1, W)

+µD({(Tr, r)}Rr=1, W) + 2

2KL(N (W, 2I) m-1

P ) + ln

2m 

+

m2- 1 .

The reader maybe curious about how one would bound the term µD in the above bound, as this term corresponds to noise-resilience with respect to test data. This is precisely what we bound later when
we generalize the noise-resilience-related conditions satisfied on train data over to test data.

C.1 KEY ADVANTAGE OF OUR BOUND.

The above approach differs from previous approaches used by Neyshabur et al. (2018); Langford & Shawe-Taylor (2002); McAllester (2003) in how strong a noise-resilience we require of the classifier to provide the generalization guarantee. The stronger the noise-resilience requirement, the more price we have to pay when we jump from the PAC-Bayesian guarantee on the stochastic classifier to a guarantee on the deterministic classifier. We argue that our noise-resilience requirement is a much milder condition and therefore promises tighter guarantees.

Mµ(xDo,ryae)ncdoµn^DcSraeantredelyfb,ootrtoh(axor,rnyilvy)eaastSla:arregaesaosnaOb(le1genmer)a.liIznatoiothnegruwaroarndtse,ewine

our setup, we would need that would want the following for

Pr(x,y) PrUN (0,2) r 

Tr(W, x, y) - Tr(W

+ U, x, y)

>

r 2

> 1 m

= O(1 m).

Previous works require a noise resilience condition of the form that with high probability a particular perturbation does not perturb the classifier output on any input. For example, the noise-resilience condition used in Neyshabur et al. (2018) written in terms of our notations, would be:

PrUN (0,2) x  r 

Tr(W, x, y) - Tr(W

+ U, x, y)

>

r 2



1 .
2

13

Under review as a conference paper at ICLR 2019

The main difference between the above two formulations is in what makes a particular perturbation (un)favorable for the classifier. In our case, we deem a perturbation unfavorable only after fixing the datapoint. However, in the earlier works, a perturbation is deemed unfavorable if it perturbs the classifier output sufficiently on some datapoint from the domain of the distribution. While this difference is subtle, the earlier approach would lead to a much more pessimistic analysis of these perturbations. In our analysis, this weakened noise resilience condition will be critical in analyzing the Gaussian perturbations more carefully than in Neyshabur et al. (2018) i.e., we can bound the perturbation in the classifier output more tightly by analyzing the Gaussian perturbation for a fixed input point.
Note that one wayour noise resilience condition would seem stronger in that on a given datapoint we want less than 1 m mass of the perturbations to be unfavorable for us, while in previous bounds, there can be as much as 1 2 probability mass of perturbations that are unfavorable. In our analysis, this will only weaken our generalization bound by a ln m factor in comparison to previous bounds (while we save other significant factors).
C.2 PROOF FOR THEOREM C.1
Proof. The starting point of our proof is a standard PAC-Bayesian theorem McAllester (2003) which bounds the generalization error of a stochastic classifier. Let P be a data-independent prior over the parameter space. Let L(W, x, y) be any loss function that takes as input the network parameter, and a datapoint x and its true label y and outputs a value in [0, 1]. Then, we have that, with probability 1 -  over the draw of S  Dm, for every distribution Q over the parameter space, the following holds:

EW~ Q E(x,y)D L(W~ , x, y)



EW~ Q



1 m

(x,y)S

L(W~ ,

x,

y)

+

2

2K L(Q

P ) + ln

2m 

m-1

(4)

In other words, the statement tells us that except for a  proportion of bad draws of m samples, the test loss of the stochastic classifier W~  Q would be close to its train loss. This holds for every possible distribution Q, which allows us to cleverly choose Q based on S. As is the convention, we choose Q to be the distribution of the stochastic classifier picked from N (W, 2I) i.e., a Gaussian perturbation of the deterministic classifier W.

RELATING TEST LOSS OF STOCHASTIC CLASSIFIER TO DETERMINISTIC CLASSIFIER. Now our

task end,

is to bound the loss for the let us define the following

deterministic margin-based

classifier variation

W of

,thPirs(xlo,ys)sDfo[rsromTerc(W 0,:x,

y)

<

0].

To

this

Lc(W, x, y) =

1 0

r  Tr(W, x, y) < cr otherwise,

waenxidpllesbcooteuwdnedLht0ahvoeeftPeasr(tdxLe,ty1e)r2mDoi[nfitshrteicsTtcrol(acWhssai,sfitxiec,rycb)lya<stsh0iefi] ee=rxEupse(ixcn,tgye)dthDLe 1[PLA20Co(fW-Btha,eyxes,styoi)ac]nh. abFsoitruiscnt,dcw.laesswifiilelrb; othuennd

the we

We will split the expected loss of the deterministic classifier into an expectation over datapoints for which it is noise-resilient with respect to Gaussian noise and an expectation over the rest. To write this out, we define, for a datapoint (x, y), N(W, x, y) to be the event that W is noise-resilient at (x, y) as defined in Equation 3 in the theorem statement:

14

Under review as a conference paper at ICLR 2019

E(x,y)D [L0(W, x, y)] = E(x,y)D [ L0(W, x, y) N(W, x, y)] Pr(x,y)D [N(W, x, y)] + E(x,y)D [ L0(W, x, y) ¬N(W, x, y)] Pr(x,y)D [¬N(W, x, y)]

1 µD ({(Tr ,r )}rR=1,W)
 E(x,y)D [ L0(W, x, y) N(W, x, y)] Pr(x,y)D [N(W, x, y)] + µD({(Tr, r)}Rr=1, W)

(5)

To further continue the upper bound on the left hand side, we turn our attention to the stochastic classifier's loss on the noise-resilient part of the distribution D (we will lower bound this term in terms of the first term on the right hand side above). For simplicity of notations, we will write D to denote the distribution D conditioned on N(W, x, y). Also, let U(W~ , x, y) be the favorable event that for a given data point (x, y) and a draw of the stochastic classifier, W~ , it is the case that for every r, Tr(W, x, y) - Tr(W~ , x, y)  r 2. Then, the stochastic classifier's loss L1 2 on D is:
EW~ Q E(x,y)D L1 2(W~ , x, y) = E(x,y)D EW~ Q L1 2(W~ , x, y)
splitting the inner expectation over the favorable and unfavorable perturbations, and using linearity of expectations,
= E(x,y)D EW~ Q L1 2(W~ , x, y) U(W~ , x, y) PrW~ Q U(W~ , x, y) + E(x,y)D EW~ Q L1 2(W~ , x, y) ¬U(W~ , x, y) PrW~ Q ¬U(W~ , x, y) to lower bound this, we simply ignore the second term (which is positive)  E(x,y)D EW~ Q L1 2(W~ , x, y) U(W~ , x, y) PrW~ Q U(W~ , x, y) .

Next, we use the following fact: if L1 2(W~ , x, y) = 0, then for all r, Tr(W~ , x, y)  r 2 and if W~ is a favorable perturbation of W, then for all r, Tr(W, x, y)  Tr(W~ , x, y) - r 2 > 0 i.e., L1 2(W~ , x, y) = 0 implies L0(W, x, y) = 0. Hence if W~ is a favorable perturbation then, L1 2(W~ , x, y)  L0(W, x, y). Therefore, we can lower bound the above expression by replacing the stochastic classifier with the deterministic classifier (and thus ridding ourselves of the expectation over Q):
 E(x,y)D L0(W, x, y)PrW~ Q U(W~ , x, y) .

Since the favorable perturbations for a (that is, PrW~ Q U(W~ , x, y)  1 - 1

fixmed)d, awtaephoaivnet :drawn

from

D

have

sufficiently

high

probability



1 - 1 m

E(x,y)D [L0(W, x, y)] .

Thus, we have a lower bound on the stochastic classifier's loss that is in terms of the deterministic classifier's loss on the noise-resilient datapoints. Rearranging it, we get an upper bound on the latter:
15

Under review as a conference paper at ICLR 2019

E(x,y)D [L0(W, x, y)] 

1

1

-

1 m

EW~ Q E(x,y)D L1 2(W~ , x, y)

 1 + m1- 1 EW~ Q E(x,y)D L1 2(W~ , x, y)  EW~ Q E(x,y)D L1 2(W~ , x, y)
+ m1- 1 EW~ Q E(x,y)D L1 2(W~ , x, y)

1
 EW~ Q E(x,y)D L1 2(W~ , x, y)

+

1 m

-

1

Thus, we have an upper bound on the expected loss of the deterministic classifier W on the noiseresilient part of the distribution. Plugging this back in the first term of the upper bound on the deterministic classifier's loss on the whole distribution D in Equation 5 we get :

E(x,y)D [L0(W, x, y)]  EW~ Q E(x,y)D L1 2(W~ , x, y) µD({(Tr, r)}rR=1, W)

+ m1- 1

Pr(x,y)D [N(W, x, y)] +

rearranging, we get:

 EW~ Q E(x,y)D L1 2(W~ , x, y) Pr(x,y)D [N(W, x, y)] + µD({(Tr, r)}rR=1, W) + m1- 1 Pr(x,y)D [N(W, x, y)]

1
rewriting the expectation over D explicitly as an expectation over D conditioned on N(W, x, y), we get:

 EW~ Q E(x,y)D L1 2(W~ , x, y) N(W, x, y) Pr(x,y)D [N(W, x, y)]

µD({(Tr, r)}rR=1, W) + m1- 1

the first term above is essentially an expectation of a loss over the distribution D with the loss set to

be zero thus we

over can

the non-noise-resilient datapoints and upper bound it with the expectation of

set the

to be L1 2

L1 2 loss

over the noise-resilient datapoints; over the whole distribution D:

+

EW~ Q E(x,y)D L1 2(W~ , x, y) + µD({(Tr, r)}Rr=1, W) + m1- 1 (6)
Now observe that we can upper bound the first term here using the PAC-Bayesian bound by plugging in L1 2 for the generic L in Equation 4; however, the bound would still be in terms of the stochastic classifier's train error. To get the generalization bound we seek, which involves the deterministic classifier's train error, we need to take one final step mirroring these tricks on the train loss.
RELATING THE STOCHASTIC CLASSIFIER'S TRAIN LOSS TO DETERMINISTIC CLASSIFIER'S TRAIN LOSS. Our analysis here is almost identical to the previous analysis. Instead of working with the distribution D and D we will work with the training data set S and a subset of it S for which noise resilience property is satisfied by W. Below, to make the presentation neater, we use (x, y)  S to denote uniform sampling from S.

16

Under review as a conference paper at ICLR 2019

First, we upper bound the stochastic classifier's train loss (L1 2) as follows:

EW~ Q E(x,y)S L1 2(W~ , x, y) = E(x,y)S EW~ Q L1 2(W~ , x, y) splitting over the noise-resilient points S ((x, y)  S for which N(W, x, y) holds) like in Equation 5, we can upper bound as:

 E(x,y)S EW~ Q L1 2(W~ , x, y) Pr(x,y)S [(x, y)  S]

+ µ^S({(Tr, r)}rR=1, W)

(7)

We can upper bound the first term by first splitting it over the favorable and unfavorable perturbations like we did before:
E(x,y)S EW~ Q L1 2(W~ , x, y) = E(x,y)S EW~ Q L1 2(W~ , x, y) U(W~ , x, y) PrW~ Q U(W~ , x, y) + E(x,y)S EW~ Q L1 2(W~ , x, y) ¬U(W~ , x, y) PrW~ Q ¬U(W~ , x, y)

To upper bound this, we apply a similar argument. First, if L1 2(W~ , x, y) = 1, then r such that

Tr(W~ , x, y) < r 2 and if W~ is a favorable perturbation then for that value of r, Tr(W, x, y) <

Tr(W~ , x, y) + r 2 < r. Thus if W~ is a favorable perturbation then, L1(W, x, y) = 1 when-

ever L1 2(W~ , x, y) = 1 i.e., L1 2(W~ , x, y)  L1(W, x, y). Next, we use the fact that the unfa-

vorable PrW~ Q

perturbations ¬U(W~ , x, y)

for 1

afixed datapoint drawn from S have m. Then, we get the following upper

sufficiently low probability i.e., bound on the above equations, by

replacing the stochastic classifier with the deterministic classifier (and thus ignoring the expectation

over Q):

 E(x,y)S EW~ Q

L1(W, x, y)

U(W~ , x, y)

PrW~ Q

U(W~ , x, y)
1



+ E(x,y)S EW~ Q

L1 2(W~ , x, y)
1

¬U(W~ , x, y)

1m 



E(x,y)S

[L1 (W ,

x,

y)]

+

1 m

Plugging this back in the first term of Equation 7, we get: 17

Under review as a conference paper at ICLR 2019

EW~ Q E(x,y)S L1 2(W~ , x, y)

 E(x,y)S [L1(W, x, y)] + 1m Pr(x,y)S [(x, y)  S]
+ µ^S({(Tr, r)}rR=1, W) E(x,y)S [L1(W, x, y)] Pr(x,y)S [(x, y)  S]
+ µ^S({(Tr, r)}rR=1, W) + 1m Pr(x,y)S [(x, y)  S]

1

E(x,y)S

[L1 (W ,

x,

y)]

Pr(x,y)S

[(x,

y)



S]

+

1 m

+ µ^S({(Tr, r)}Rr=1, W)

since the first term is effectively the expectation of a loss over the whole distribution with the loss

set to be zero on the setting the loss to be

non-noise-resilient L1 over the whole

points and set distribution:

to

L1

over

the

rest,

we

can

upper

bound

it

by

 E(x,y)S [L1(W, x, y)] + µ^S({(Tr, r)}Rr=1, W) + 1m

Applying the above upper bound and the bound in Equation 6 into the PAC-Bayesian result of

Equation

4

yields

our

result

(Note

that

combining

these

equations

would

produce

the

term

1 m

+

1 m-1

which is at most m2-1 , which we reflect in the final bound. ).

D PROOF FOR THEOREM 3.1
In this section, we present the proof for the (abstract) generalization guarantee presented in Section 3. Our proof is based on the following recursive inequality that holds for all r < R (we will prove a similar, but slightly different inequality for r = R):

Pr(x,y)D [qr Tq(W, x, y) < 0]  Pr(x,y)D [q<r Tq(W, x, y) < 0]

+

O~

 

2KL(N (W, 2I) P ) 

m-1



(8)

generalization error for condition r

That is, we bound the probability mass of test points such that any one of the first r condition fails, in terms of the probability mass of points where one of the first r - 1 conditions fail and an

error that corresponds to how much error there can be in generalizing the rth condition from the training data. First, we consider the base case when r = 1, and apply the PAC-Bayes-based guarantee

from Theorem C.1 assumption that on

on all

the the

singleton set of functions consisting only training data, the condition T1(W, x, y) >

of T1. 1 is

First we satisfied.

have from our Thus, the first

term in the upper bound in Theorem C.1 is zero. Next, we can show that the terms µ^S and µ^D

would be zero too. This follows from the fact that Equation 2 holds in this framework. Applying this equation for r = 1, for  = , we get that for all x  X the following inequality holds:

PrU

max
l

(1l)(W + U , x, y) - 1(l)(W, x, y)

>

1() 2

 1 . Rm

Since,  was chosen such that 1()  1, we effectively have:

PrU

max
l

(1l)(W + U , x, y) - (1l)(W, x, y)

>

1 2

 R1 m .

18

Under review as a conference paper at ICLR 2019

Then, using Equation 1, we have that

PrU

T1(W

+ U , x, y) - T1(W, x, y)

>

1 2

 R1 m .

Effectively this establishes that the noise-resilience requirement of Equation 3 holds on all possible inputs, for T1, thus proving our claim that the terms µ^S and µ^D would be zero. Thus, we will get that

Pr(x,y)D

[T1

(W

,

x,

y)

<

0]



O~

 

2KL(N (W, 2I) P ) 

m-1



which proves the recursion statement for the base case.

To prove the recursion for some arbitrary r < R, we again apply the PAC-Bayes-based guarantee

from Theorem C.1, but on the set of first r functions. Again, we will have that the first term in the

guarantee would be zero, since the corresponding conditions are satisfied on the training data. Now, to

bound the proportion of bad to Equation 3 for any input

points µ^S and µ^D, that satisfies the r

we -1

claim that the network is conditions, Tq(W, x, y)

noise-resilient according > 0 for q = 1, 2, , r - 1.

Since these conditions are assumed to be satisfied by a margin on the training data, it immediately implies that µ^S({(Tq, q)}qr=1, W) = 0. On the test data, we can bound µD({(Tq, q)}rq=1, W) in terms of Pr(x,y)D [q<r Tq(W, x, y) < 0], thus giving rise to the recursion in Equation 8.

Now, to prove our claim, consider an input x such that Tq(W, x, y) > 0 for q = 1, 2, , r - 1. We then have the following:

PrU

q  r 

Tq(W, x, y) - Tq(W + U , x, y)

>

q 2

 PrU

q  r 

Tq(W, x, y) - Tq(W

+ U, x, y)

>

q ( ) 2

because q()  q

 PrU

q  r  max
l

q(l)(W

+ U , x, y) - q(l)(W, x, y)

>

q ( ) 2

the above inequality follows from the constraint we imposed on Tr in Equation 1

r
 Pr
q=1

max
l

(ql)(W + U , x, y) - (ql)(W, x, y)

> q()  2

q

<

q,

max
l

(ql)(W, x, y) - (ql)(W

+ U, x, y)

< q () 2

r


1  1

q=1 R m

m

Thus, we have that x satisfies the noise-resilience condition from Equation 3 if it also satisfies Tq(W, x, y) > 0 for q = 1, 2, , r - 1. This proves the recursion in Equation 8. Finally, we can apply a similar argument for TR with a small change since the first term in the guarantee from Theorem C.1 is not explicitly assumed to be zero; we will get an inequality in terms of the number of training points that are not classified correctly by a margin, giving rise to the margin-based bound:
19

Under review as a conference paper at ICLR 2019

Pr(x,y)D

[qR

Tq(W, x, y)

<

0]



1 m

(x,y)S

1[TR(W, x, y)

<

R]

+ Pr(x,y)D [q<R Tq(W, x, y) < 0]

+

O~

 

2KL(N (W, 2I) P ) 

m-1



By using the fact that the test error is upper bounded by the left hand side in the above equation, applying the recursion on the right hand side R times, we get our final result.

E PERTURBATION BOUNDS

E.1 OVERVIEW OF THE BOUNDS.

In this section, we will quantify the noise resilience of a network in different aspects. Each of our bounds has the following structure: we fix an input point (x, y), and then say that with high probability over a Gaussian perturbation of the network's parameters, a particular input-dependent
property of the network (say the output of the network, or the pre-activation value of a particular unit
h at a particular layer d, or say the Frobenius norm sof its active weight matrices), changes only by a small magnitude proportional to the variance 2 of the Gaussian perturbation of the parameters.

A key feature of our bounds is that they do not involve the product of the spectral norm of the weight

matrices and hence save us an exponential factor in the final generalization bound. Instead, the

bound in the perturbation of a particular property will be in terms of i) the magnitude of the some

`preceding' properties (typically, these are properties of the lower layers) of the network, and ii) how

those preceding properties themselves respond to perturbations. For example, an upper bound in the perturbation of the dth layer's output would involve the 2 norm of the lower layers d < d, and how

much they would blow up under these perturbations. More concretely, the bound on the perturbation in the norm of rows of the Jacobian d d depends on i) the magnitude of the norms related to all the lower layer Jacobians, and also the `higher' Jacobians of layer d i.e., d d for d > d and ii) the

perturbation suffered by the higher Jacobians of layer d. Second, the bound on the perturbation of

both the Jacobians

2
d

norm and the d for each d

pre-activations of layer < d and the 2 norms of

dalldelopwenerdloayneir)stdhe<mdaxiii)mthuemperortwurb2atnioonrmsuoffferthede

by the 2 norms of all lower layers.

Intuitively, when the Jacobian of layer d with respect to some d has small 2 row norms, it diminishes the effect of noise injected into the matrix immediately after layer d/immediately before layer d. Similarly, when the 2 norm of the output of the dth layer is small, it diminishes the effect of noise injected into the subsequent matrix. Finally, assuming sufficiently large pre-activation values prevents hidden units from crossing non-linearities with high probability; this then allows us to propagate the Gaussian noise `linearly' through the network.

E.2 SOME NOTATIONS.
To formulate our lemma statement succinctly, we design a notation wherein we define a set of `tolerance parameters' which we will use to denote the extent of perturbation suffered by a particular property of the network.
Let C^ denote a `set' (more on what we mean by a set below) of positive tolerance values, consisting of the following elements:
1. ^d d for each layer d = 1, 2, , D, and d = 1, , d (a tolerance value for the 2 norm of each row of the Jacobians at layer d)
2. ^d, for each layer d = 1, , D - 1 (a tolerance value for the 2 norm of the output of layer d) 3. ^d for each layer d = 1, , D. (a tolerance value for the magnitude of the pre-activations of
layer d)

20

Under review as a conference paper at ICLR 2019

Notes about (abuse of) notation:
· We call C^ a `set' to denote a group of related constants into a single symbol. Each element in this set has a particular semantic associated with it, unlike the standard notation of a set, and so when we refer to, say ^d d  C^, we are indexing into the set to pick a particular element.
· We will use the subscript C^d to index into a subset of only those tolerance values corresponding to layers from 1 until d.
Next we define two events. The first event formulates the scenario that for a given input, a particular perturbation of the weights until layer d brought about very little change in the properties of these layers (within some tolerance levels). The second event formulates the scenario that the perturbation did not flip the activation states of the network. Definition E.1. Given an input x, and an arbitrary set of constants C^, for any perturbation U of W, we denote by PERT-BOUND(W+U , C^, x) the event that:
· for each ^d d  C^, the maximum perturbation in the 2 norm of a row of the Jacobian d d is bounded as maxh J d d (x; W)[h] - J d d (x; W+U )[h]  ^d d .
· for each ^d  C^, the perturbation in the 2 norm of layer d activations is bounded as f d (x; W) - f d (x; W+U )  ^d .
· for each ^d  C^, the maximum perturbation in the preactivation of hidden units on layer d is bounded as maxh f d (x; W) [h] - f d (x; W+U ) [h]  ^d .
NOTE: A subtle point in the above definition (which we take advantage of, when we state our results) is that if we supply only a subset of C^ (say C^d instead of the whole of C^) to the above event, PERT-BOUND(W + U , , x), then it would denote the event that the perturbations suffered by only that subset of properties is within the respective tolerance values.
Next, we define the event that the perturbations do not affect the activation states of the network. Definition E.2. For any perturbation U of the matrices W, let UNCHANGED-ACTSd(W + U , x) denote the event that none of the activation states of the first d layers change on perturbation.

E.3 MAIN LEMMA.

Our results here are styled similar to the equations required by Equation 2 presented in the main paper. For a given input point and for a particular property of the network, roughly, we bound the the probability that a perturbation affects the value of the property while none of the preceding preceding properties themselves are perturbed beyond a certain tolerance level.
Lemma E.1. Let C^ be a set of constants. For any ^ > 0, define C^ in terms of C^, for all d = 1, 2, , D as follows:

d

^d

d

=

 max
d=d+1 h

J d d (x; W)[h]

J d-1

d (x; W)

F

+ ^d-1

 d H

d

^d = 
d =1

J d d (x; W)

F

f d-1 (x; W) + ^d-1

2DH 2 ln ^

d

^d

=



d =1

max
h

J d d (x; W)[h]

^d d = 0

^0 = 0

f d-1 (x; W) + ^d-1

2DH 2 ln ^

DH 4 ln ^

d = d - 1, , 1

21

Under review as a conference paper at ICLR 2019

Let Ud be sampled entrywise from N (0, 2) for any d. Then, the following statements hold good: 1. Bound on perturbation of 2 norm on the rows of the Jacobians d d. For all d = d, d - 1, , 1:
P rU ¬PERT-BOUND(W + U , {^d d }, x)  PERT-BOUND(W + U , C^d-1 {^d d }dd=d+1, x)  UNCHANGED-ACTSd-1(W + U , x)  ^ 2. Bound on perturbation of of 2 norm of the output of layer d. For all d = 1, 2, , D,
P rU ¬PERT-BOUND(W + U , {^d }, x)  PERT-BOUND(W + U , C^d-1 {^d d }dd=1, x)  UNCHANGED-ACTSd-1(W + U , x)  ^
3. Bound on perturbation of pre-activations at layer d. For all d = 1, 2, , D, P rU ¬PERT-BOUND(W + U , {^d }, x)  PERT-BOUND(W + U , C^d-1 {^d d }dd=1  {^d}, x)  UNCHANGED-ACTSd-1(W + U , x)  ^

Proof. For the most part of this discussion, we will consider a perturbed network where all the hidden units are frozen to be at the same activation state as they were at, before the perturbation (we denote the weights of such a network by W[+U] and its output at the dth layer by f d (x; W[+U])). By having the activations states frozen, the Gaussian perturbations propagate as Gaussian perturbations and we can enjoy the well-established properties of the Gaussian even after they propagate.

NOTE. We will prove the parts of the above lemma in an order different from how it is stated; specifically, we will study the perturbation of the Jacobian's row-wise 2 norm the last, as it is slightly more involved. However, the reason for stating the lemma in that order is to keep it consistent with the order in which our generalization analysis applies parts of it later.

PERTURBATION BOUND ON THE 2 NORM OF LAYER d. We bound the change in the 2 norm of

the dth layer's output by applying a triangle inequality5 after splitting it into a sum of vectors. Each

summand here (which we define as introducing noise in weight matrix

vd for d after

each d having

 d) is the difference introduced noise into

in the dth layer output on all the first d - 1 weight

matrices.

f d (x; W[+Ud]) - f d (x; W)  f d (x; W[+Ud]) - f d (x; W)

because the activations are ReLU, we can replace this with the perturbation of the pre-activation

 gd (x; W[+Ud]) - gd (x; W)





d d =1

gd 

(x;

W

[+Ud

])

- gd
=vd

(x;

W

[+Ud

-1

]) 

dd
 vd =

d =1

d =1

vd2,h
h

(9)

Here, vd,h is the perturbation in the preactivation of hidden unit h on layer d, brought about by perturbation of the dth weight matrix in a network where the first d - 1 weight matrices have already been perturbed.
5Specifically, for two vectors a, b, we have from triangle inequality that b  a + b - a and a  b + a - b . As a result of this, we have: - a - b  a - b  a - b . We use this inequality in our proof.
22

Under review as a conference paper at ICLR 2019

Now, for each h, we bound vd,h in Equation 9. Since the activations have been frozen we can rewrite each vd,h as the product of the hth row of the unperturbed network's Jacobian d d , followed by only the perturbation matrix Ud , and then the output of the layer d - 1. Concretely, we have6 7:

1×Hd

Hd ×Hd-1

Hd -1 ×1

vd,h = J d d (x; W)[h] Ud f d-1 (x; W[+Ud-1])

spherical Gaussian

What do these random variables vd,h look like? Conditioned on Ud-1, the second part of our expansion of vd,h, namely, Ud f d-1 (x; W[+Ud-1]) is a multivariate spherical Gaussian (see Lemma B.2) of the form N (0, 2 f d-1 (x; W[+Ud-1]) 2I). As a result, conditioned on Ud-1, vd,h is a univariate Gaussian N (0, 2 J d d (x; W)[h] 2 f d-1 (x; W[+Ud-1]) 2).
Then, we can apply a standard Gaussian tail bound (see Lemma B.1) to conclude that with probability 1 - ^ DH over the draws of Ud (conditioned on any Ud-1), vd,h is bounded as:

vd,h   J d d (x; W)[h] f d-1 (x; W[+Ud-1])

2DH 2 ln ^ .

(10)

Then, by a union bound over all the hidden units on layer d, and for each d, we have that with probability 1 - ^, Equation 9 is upper bounded as:

d

d

vd2,h 



h d=1

J d d (x; W)
F

f d-1 (x; W[+Ud-1])

2DH 2 ln ^ .

(11)

Finally, we prove the probability bound in the lemma statement. To simplify notations, let us denote C^d-1 {^d d }dd=1 by C^prev. Furthermore, we will drop redundant symbols in the arguments of the events we have defined. Then, recall that we want to we upper bound the following probability (we ignore the arguments W + U and x for brevity):

Pr (¬PERT-BOUND ({^d }))  PERT-BOUND(C^prev)  UNCHANGED-ACTSd-1
Recall that Equation 11 is a bound on the perturbation of the 2 norm of the dth layer's output when the activation states are explicitly frozen. If the perturbation we randomly draw happens to satisfy UNCHANGED-ACTSd-1 then this bound holds good even in the case where the activation states are not explicitly frozen. Furthermore, when PERT-BOUND(C^prev) holds, the bound in Equation 11 can be upper-bounded by ^d as defined in the lemma statement, because under PERT-BOUND(C^prev), the middle term in Equation 11 can be upper bounded using triangle inequality as f d-1 (x; W[+Ud-1])  f d-1 (x; W) + ^d-1. Hence, the event above happens only for the perturbations for which Equation 11 fails and hence we have that the above is upper bounded by ^.
PERTURBATION BOUND ON THE PREACTIVATION VALUES OF LAYER d. Following the same analysis as above, the bound we are seeking here is essentially maxh dd=1 vd,h . The bound follows similarly from Equation 10.
6Below, we have used Hd to denote the number of units on the dth layer (and this equals H for the hidden units and K for the output layer).
7Note that the succinct formula below holds good even for the corner case d = d, where the first Jacobian-row term becomes a vector with zeros on all but the hth entry and therefore only the hth row of the perturbation matrix Ud will participate in the expression of vd,h.

23

Under review as a conference paper at ICLR 2019

PERTURBATION BOUND ON THE 2 NORM OF THE ROWS OF THE JACOBIAN d d. We split this term like we did in the previous section, and apply triangle equality as follows:

max J d d (x; W)[h] - J d d (x; W[+Ud])[h]
h

 max J d d (x; W)[h] - J d d (x; W[+Ud])[h]
h





d d =1

J

d

d (x; W[+Ud ])[h]

-

Jd

d (x; W[+Ud-1])[h]

 =ydh 

dd

 max
h d=1

yhd

= max
h d=1

(yd,h,h )2
h

(12)

JHaecroeb,iwanedhadvebdroeufignhetdaybohdu ttboybpeetrhtuervbeincgtotrhtehdatcthorwreesigphotnmdsattroixt,hgeidveifnfetrheantctheeinfirthstedhth-

row of the 1 matrices

have already been perturbed. We use h to iterate over the units in the dth layer and h to iterate over

the units in the dth layer.

Now, under the frozen activation states, when we perturb the weight matrices from 1 uptil d, since these matrices are not involved in the Jacobian d d, fortunately, the Jacobian d d is not perturbed (as the set of active weights in d d are the same when we perturb W as W[+Ud ]). So, we will only need to bound yd,h,h for d > d.

WJoafchio)abtthidaeonehdsththdreowd-iso1tfrfitbohurettiJhoaencpooebfritayundrbd,he,ddhn leioit)wotkohrlekikp:eerftourrbdati>ondm? aWtriexcUandexanpdanidii8)

yd ,h,h the hth

as the product column of the

1×Hd

Hd ×Hd-1

Hd -1 ×1

yd,h,h = J d d (x; W )[h] Ud J d-1 d (x; W [+Ud-1])[, h]

spherical Gaussian

Conditioned on , h] is a

Ud-1, the second part of multivariate spherical

this expansion, namely, Ud J d-1 d (x; W[+Ud-1])[ Gaussian (see Lemma B.2) of the form

N (0, 2 J d-1 d (x; W[+Ud-1])[, h] 2I). As a result, conditioned on Ud-1, yd,h,h is

a univariate Gaussian N (0, 2 J d d (x; W)[h] 2 J d-1 d (x; W[+Ud-1])[, h] 2).

Then, by applying a standard Gaussian tail bound we have that with probability 1 - draws of Ud conditioned on Ud-1, each of these quantities is bounded as:

^ DH2

over the

yd,h,h   J d d (x; W )[h] J d-1 d (x; W [+Ud-1])[, h]

DH2 2 ln ^

By a union bound, we then get that with probability 1 - ^ over the draws of Ud, we can upper bound Equation 12 as:

8Again, note that the below succinct formula works even for corner cases like d = d or d = d.

24

Under review as a conference paper at ICLR 2019

D
max
h d=1

(yd,h,h )2 
h

D

 max J d d (x; W)[h]

d =d +1

h

J d-1 d (x; W [+Ud-1])
F

DH 4 ln ^

Finally we get the result of the lemma by a similar argument as in the case of the perturbation bound on the output of each layer.

F PROOF FOR THEOREM F.1

In this section, we will prove our the generalization bound for the specific case of ReLU based deep networks, that we proved in our main paper.

F.1 NOTATIONS.

To make the presentation of our theorem cleaner, we will set up some notations. First, we define the following positive constants which will correspond to norm bounds on different properties of the network on the training data:

1. d d for each layer d = 1, 2, , D, and d = 1, , d (we will use this to bound 2 norms of rows in the Jacobians of the network for a training input)
2. d, for each hidden layer d = 1, , D - 1, (we will use this to bound 2 norms of the outputs of the layers of the network on a training input)
3. d for each layer d = 1, , D, (we will use this to bound magnitudes of the preactivations values of the network on a training input).

We will also simplify our

define 0 to be an discussion, we will

2-norm bound on the input x. As group these constants into a `set'

we did C .

in

the

previous

section,

to

Given these training set related constants, we also define C  to be the following constants corresponding to weaker norm-bounds related to the test data:

1. d d = 2d d for each layer d = 1, 2, , D, and d = 1, , d (we will use this to bound 2 norms of rows in the Jacobians of the network for a test input)
2. d = 2d, for each hidden layer d = 1, , D - 1, (we will use this to bound 2 norms of the outputs of the layers of the network on a test input)
3. d = d 2 for each layer d = 1, , D, (we will use this to bound magnitudes of the preactivations values of the network on a test input).

Note about (abuse of) notation: We reiterate a point about our notation which we also made in Appendix E. We call C  and C  a `set' to denote a group of related constants by a single symbol. Each element in this set has a particular semantic associated with it, unlike the standard notation of a set.
Now, for any given set of constants C , for a particular weight configuration W, and for a given input x, we define the following event which holds when the network satisfies certain norm-bounds defined by the constants C (that are favorable for noise-resilience). Definition F.1. For a set of constants C , for network parameters W and for any input x, we define NORM-BOUND(W, C , x) to be the event that all the following hold good:

1. for all d d  C , maxh J d d (x; W)[h] < d d (Rows of Jacobian do not have too large an 2 norm).

25

Under review as a conference paper at ICLR 2019

2. for all d  C , f d (x; W) < d (Output of the layer does not have too large an 2 norm). 3. for all d  C , minh f d (x; W) [h] > d. (Pre-activation values are not too small).
NOTE: (Similar to a note under Definition E.1) A subtle point in the above definition (which we will make use of, to state our theorems) is that if we supply only a subset of C to the above event, NORM-BOUND(W, , x) then it would denote the event that only those subset of properties satsify the respective norm bounds.
F.2 MAIN RESULT
Below, we present our main result for this section, a generalization bound on a class of networks that satisfy certain norm bounds on the training data. We provide a more intuitive presentation of these bounds after the proof in Appendix F.4. Theorem F.1. Consider a set of positive training-set related constants C . Then, for any  > 0, with probability 1 -  over the draw of samples from Dm, for any W such that, for every (x, y)  S, NORM-BOUND(W, C , x) holds, we have that:

Pr(x,y)D

f (x; W) [y] < max f (x; W) [j]
jy

1 1 m (x,y)S

f

(x;

W

)

[y]

<

class

+

max
jy

f

(x;

W

)

[j

]

+ 6D2

1 m-

1



+ 6mD-2 1

D
2
d=1

Wd

2

1 ()2

+ ln 6D2m 

where

1 

is

 40 H

ln 3D3Hm × max{Bjac, Bout, Bhidden-preact, Boutput-preact}; here we define:

Bjac

=

max
1d <dD

dd=d+1 d d d-1 d d

d

,

Bout

=

max
dD-1

dd=1

d d d-1 d

Bhidden-preact

=

max
dD-1

dd

=1 d H

d d d

-1

Boutput-preact = 2 dD=1 D d d-1 H class

REMARK Observe that the above generalization bound applies to a specific class of W that satisfies certain norm bounds on the training data. One could extend the bound to all values of W for theoretical purposes, by `covering' all possible values of the constants defined in the norm bounds (see Neyshabur et al. (2018) for example) at the cost of extra factors. However, from a practical point of view, the statement in this form is sufficient because we are interested in a generalization bound for a small class of networks that SGD restricts itself to, and not all possible networks. In practice, to apply this bound, one could run SGD on many random draws of the training dataset and identify training-set related constants C  such that the network learned from all these random draws satisfy the norm bounds defined by C .
26

Under review as a conference paper at ICLR 2019

F.3 PROOF FOR THEOREM F.1

Proof. To apply the framework, we will have to first define the abstract functions Tr and quantities r used in Theorem 3.1 . These functions will be composed of the following properties corresponding to
different values of d, d and h (the  functions described in the main framework): Jd d (x; W)[h] , f d (x; W) and f d (x; W) [h].

Definition F.2. We define the following set of functions T , and the corresponding set of margin values C :

Td

d,jac(W, x, y)

=

2d

d

-

max
h

J d d (x; W)[h]

,

Td,output(W, x, y) = 2d - f d (x; W) ,

Td,preact(W, x, y)

=

min
h

f d (x; W) [h]

-

d , 2

d d = d d

d = d

d

=

d 2

for d = 1, , D d = 1, , d for d = 1, 2, , D - 1 for d = 1, 2, , D - 1

and for the output layer D:

TD,preact(W ,

x,

y)

=

0.5(f

(x;

W

)[y]

-

max
jy

f

(x;

W )[j ])

D

=

class 2

We will use the notation C  2 to denote the values in the margin set divided by 2. Note that the above functions satisfy the constraint Equation 1 as required by our abstract framework.

ON THE CHOICE OF THE ABOVE FUNCTIONS AND MARGIN VALUES. Below, we justify our choice

of Tr

(thWes,exf,uyn)ct<io0nscoannvdemrt atorgninosr,mb-yboduesncdrsiboinngthheoiwnpiunte-qdueapleitnidesenotfpthroepfeorrtmiesTorf(Wthe,

x, y) <  network.

+

r

or

When we apply the above quantities of T and  in Theorem C.1, the guarantees will be in terms of the test loss Pr(x,y)D [T (W, x) < 0] and train loss Pr(x,y)S [T (W, x) < ] which would be equivalent to the norm-bounds shown in the table below. Observe that the second column (the

0-margin error column) corresponds to the event that a datapoint does not satisfy the weaker normbound defined by test-set constants C  (which was defined in the beginning of this section). On the
other hand, the last column (the -margin error column) corresponds to the event that a datapoint does not satisfy the norm-bounds defined by the train-set constants C .

T, Td d,jac, d d
Td,output, d
Td,prdeac<t,Dd) (for TD,preact, D

Table 1: Margin-based test and train errors.

T (W, x) < 0 maxh J d d (x; W)[h] > 2d d = d d

T (W, x) <  maxh J d d (x; W)[h] > d d

f d (x; W) > 2d = d

f d (x; W) > d

minh f d (x; W) [h] < d 2 = d f (x; W)[y] < maxjy f (x; W)[j]

minh f d (x; W) [h] < d f (x; W)[y] <
maxjy f (x; W)[j] + class

Now to apply the abstract generalization bound in Theorem 3.1, recall that we need to come up with an ordering of the functions above such that we can realize an inequality of the form given in Equation 2. To this end, we induce an ordering based on the perturbation bounds presented in Lemma E.1.
27

Under review as a conference paper at ICLR 2019

The order in which we traverse the properties is as follows. We will go from layer 1 uptil D; within each layer we will first deal with the Jacobians d d, d d - 1, so on until d 1 in that order9 . Then we

will deal with these two will

the not

2 norm actually

of the output of the layer and the pre-activation values matter). Note that T   3D2 since there are at most

(the D+

order between 2 properties in

each layer.

Definition F.3. Sequence 1:

T1 1,jac,

T1,output, T1,preact,

followed by

T2 1,jac,

T2,output, T2,preact,

 

followed by TD D,jac, , TD 1,jac, TD,output TD,preact.

The next step in our proof is to show that inequalities of the form presented in Equation 2 hold good. To do this, we instantiate Lemma E.1 with  =  as in Theorem F.1, ^ = 3D21m and C^ = C  2.
Then, it can be verified that the values of the tolerance parameters C^ in Lemma E.1 can be upper bounded by the corresponding value in C  2. Succinctly, we say:

C^  C  2

(13)

Given this, we will focus on showing that an inequality of the form Equation 2 holds for

Td d,jac. A similar approach would apply for the other properties. For brevity, we will define

Cprev

=

Cd-1

{d

}d
d d=d+1

and

C^prev

=

C^d-1

{^d

}d
d d=d+1

First, we note that the precondition for Equation 2 translates to NORM-BOUND(W + U , Cprev, x) (as tshheonwsnoindoTeasblUe N1)C. HGAivNeGnEthDi-sAcConTdSidti-o1n(,Wwe+fiUrs,txa)rg. uTehtihsaitsifbePcEaRusTe-B, tOheUNevDe(nWt P+EURT, -CB^prOevU, Nx)D(hWold+s, U , C^prev, x) implies that the pre-activation values of layer d - 1 suffer a perturbation of at most ^d-1, which from Equation 13 is at most d-1 2 = d-1 4 i.e., maxh f d-1 (x; W + U ) [h] - f d-1 (x; W) [h]  d-1 4. However, since NORM-BOUND(W, Cprev, x) holds, we have that the pmlaryeinearhctdifv-da-t1i1oo(nfxvt;haWelune)es[thowf]otrhkisddola-yn1eor2t .chhaFavrneogmaemtthhaeegisrneiattcuwtdioveaeotqifouanattsliteoaantsest,(wi.dee-.,1ht=ahveesdit-gh1nat2otfhbteehfehoirpderdepe-eanrctutuinvribatatsitoeionvnednio.eeas.t, not change) under this perturbation. We can similarly argue for the layers below d - 1, thus proving that UNCHANGED-ACTSd-1(W + U , x) holds under PERT-BOUND(W + U , C^prev, x).

Then, observe that the guarantee from Lemma E.1 boils down to:

P rU ¬PERT-BOUND(W + U , {^d d }, x)  PERT-BOUND(W + U , Cprev 2, x)  3D21m
Now, from Equation 13 we have that, if PERT-BOUND(W + U , {^d d }, x) holds, then PERT-BOUND(W + U , {d d 2}, x) holds. This implies that we can replace the first term as below:
P rU ¬PERT-BOUND(W + U , {d d 2}, x)  PERT-BOUND(W + U , Cprev 2, x)  3D21m
This inequality is precisely of the same form as Lemma E.1. Through identical arguments for the other functions, our generalization bound holds.
9Note that the Jacobian for d d is nothing but an identity matrix regardless of the input datapoint; thus we do not need any generalization analysis to bound its value on a test datapoint. But to make the presentation simpler in some ways, we will include it in our generalization analysis.

28

Under review as a conference paper at ICLR 2019

Note that in the final bound, we use Lemma B.3 to simplify the KL-divergence term between the posterior centered at W and the prior at the random initialization Z.

F.4 INTERPRETING THE BOUNDS.

Below we show empirically how the terms Bjac, Bout, Bhidden-preact, Boutput-preact vary with the width of a network after fixing the depth D of the network to be 6. For all examples, we use SGD with learning rate 0.01 to train the network on the MNIST dataset by minimizing cross entropy loss until all datapoints are classified by a margin of at least 10.

WBoeutpsuet-epreinactFdiegcurreeasse3stwhaitththweidfitrhstastw1oteHrmasnBd jtahc,isBiosutbaerceauwsiedtthhe-innduempeernadtoernto;fothnisthteeromthiesrwhiadnthd-, independent, while the denominator of this term, which is Hclass, increases with width (we have chosen class to be a constant of 10 in all our experiments). Furthermore, in Figure 2 we see that all these three terms show an increase with depth that is only nearly linear or mildly exponential ­
these terms certainly do not grow as rapidly as the product of the spectral norms. This is because, in practice the terms d d and d are width-independent, and do not grow as rapidly as the product of the spectral norms.

As far as the term Bhidden-preact is concerned, recall from our discussion in the main paper that the

minimum pre-activation the term Bhidden-preact can

value d of the network tends to be be quite large; furthermore, this term

quite small in practice, and therefore exhibits considerable variance across

different widths/depths and different training runs. As we noted, for future work, it would be

important to understand how the dependence of the bound on this term can be better improved.

Theoretically speaking, the best scenario for Bhidden-preact can be realized when the preactivation values

of each layer (which has a total 2 norm that is width-independent in practice) are equally spread

out across the to be as large

hidden as (1

unHits). .TThehnenw, eBwhididlelnh-parevaect

that the would

smallest pre-activation value is behave like the other terms, in

guaranteed terms of its

width and depth dependence. In fact, we simulate this in practice, by plotting a redefined version of Bhidden-preact where instead of plugging in the minimum pre-activation value, we plug in the median

pre-activation value (that is, for each datapoint we compute the median pre-activation value for that

lmbhaanaayessddeesriid,aism-naBanilphlsdariodertd-hqevaneuac-npirtrtiieevafiatacnsittom'dionvatnhawllrev.yiatlShlowuewreieetehisFssptiwogevnciuadtlrlyteuthoea4astohnasfendmtdwhdaiFelisldpigamtthhsue.radeni(2da1nfthoaercHrhdooe)swpsitnhathlpalersdaoatchrttiaiegcpioeon,tiahtnhletersB)t`h.emidWrdmeeedns-ipoaBrbnejas-acbcet,araBvsneeoddutt-ht,BhaBehtoisd`udimtpneuncet--pedprrieteaahacncett-'

29

Under review as a conference paper at ICLR 2019

Upper bound Upper bound Upper bound

2.4 2.3 2.2 2.1 2.0 1.9 1.8 1.7 40

Bjac vs Width 80 160 320 640 1280
Width

5.0 4.5 4.0 3.5 3.0 2.5 2.0 40

Bout vs Width 80 160 320 640 1280
Width

16 14 12 10 8 6 4 2 40

Boutput-preact vs Width 80 160 320 640 1280
Width

Figure 3: In the above figure, we plot the values of Bjac (left) Bout (middle) Boutput-preact (right) computed on a network of depth 6 trained on the MNIST dataset, for varying widths. Observe that the first two values do not increase with width. Furthermore, the last value, Boutput-preact shows a strong decrease with width as 1 H (for every doubling of the width, the value drops by 2.)

Upper bound Upper bound

Bhidden-preact vs Width 1.2 ×108 1.0 0.8 0.6 0.4 0.2 0.0 40 80 160 320 640 1280
Width

median-Bhidden-preact vs Width

50

45

40

35

30

25

20

15

10 40

80 160 320 640 Width

1280

Figure 4: In the above figure, we plot the values of Bhidden-preact (left) and `median-based'-Bhidden-preact (right) computed on a network of depth 6, trained on a subset of the MNIST dataset, for varying widths. While it is hard to quantify how Bhidden-preact varies, observe that the latter quantity decays with width.

30

