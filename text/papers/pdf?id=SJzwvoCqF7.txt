Under review as a conference paper at ICLR 2019
ON TIGHTER GENERALIZATION BOUNDS FOR DEEP NEURAL NETWORKS: CNNS, RESNETS, AND BEYOND
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a generalization error bound for a general family of deep neural networks based on the depth and width of the networks, as well as the spectral norm of weight matrices. Through introducing a novel characterization of the Lipschitz properties of neural network family, we achieve a tighter generalization error bound. We further obtain a result that is free of linear dependence on norms for bounded losses. Besides the general deep neural networks, our results can be applied to derive new bounds for several popular architectures, including convolutional neural networks (CNNs), residual networks (ResNets), and hyperspherical networks (SphereNets). When achieving same generalization errors with previous arts, our bounds allow for the choice of much larger parameter spaces of weight matrices, inducing potentially stronger expressive ability for neural networks.
1 INTRODUCTION
We aim to provide a theoretical justification for the enormous success of deep neural networks (DNNs) in real world applications (He et al., 2016; Collobert et al., 2011; Goodfellow et al., 2016). In particular, our paper focuses on the generalization performance of a general class of DNNs. The generalization bound is a powerful tool to characterize the predictive performance of a class of learning models for unseen data. Early studies investigate the generalization ability of shallow neural networks with no more than one hidden layer (Bartlett, 1998; Anthony & Bartlett, 2009). More recently, studies on the generalization bounds of deep neural networks have received increasing attention (Dinh et al., 2017; Bartlett et al., 2017; Golowich et al., 2017; Neyshabur et al., 2015; 2017). There are two major questions of our interest in these analysis of the generalization bounds:
(Q1) Can we establish tighter generalization error bounds for deep neural networks in terms of the network dimensions and structure of the weight matrices?
(Q2) Can we develop generalization bounds for neural networks with special architectures?
For (Q1), (Neyshabur et al., 2015; Bartlett et al., 2017; Neyshabur et al., 2017; Golowich et al., 2017) have established results that characterize the generalization bounds in terms of the depth D and width p of networks and norms of weight matrices. For example, Neyshabur et al. (2015) provide an exponential bound on D based on the Frobenius norm Wd F, where Wd is the weight matrix of d-th layer; Bartlett et al. (2017); Neyshabur et al. (2017) provide a polynomial bound on p and D based on Wd 2 (spectral norm) and Wd 2,1 (sum of the Euclidean norms for all rows of Wd). Golowich et al. (2017) provide a nearly size independent bound based on Wd F. Nevertheless, the generalization bound depends on other than the spectral norms of the weight matrices may be too loose. In specific, Wd F ( Wd 2,1) is in general p (p) times larger than Wd 2. Given m training data points and suppose Wd 2 = 1 for ease of discussion, Bartlett et al. (2017) and Neyshabur et al. (2017) demonstrate generalization error bounds as O( D3p2/m), and Golowich et al. (2017) achieve a bound O(pD/2 min(m-1/4, D/m)), where O(·) represents the rate by ignoring logarithmic factors. In comparison, we show a tighter generalization error bound as O( Dp2/m), which is significantly smaller than existing results and achieved based on a new Lipschitz analysis for DNNs in terms of both the input and weight matrices. We notice that some recent results characterize the generalization bound in more structured ways, e.g., by considering specific error-resilience parameters (Arora et al., 2018), which can achieve empirically improved generalization bounds than
1

Under review as a conference paper at ICLR 2019

Table 1: Comparison of existing results with ours on norm based generalization error bounds for

DNNs. For ease of illustration, we suppose the upper bound of input norm R and the Lipschitz

constant

1 

of

the

class

of

loss

functions

G

are

generic

constants.

We

use

Bd,2,

Bd,F,

and

Bd,21

as the upper bounds of Wd 2, Wd F, and Wd 2,1 respectively. For notational convenience, we

suppose for all d

the width = 1, . . . ,

pd D,

= p for where

WalldlaFye=rsOd (=1p,).a.n.d,

D. We further show Wd 2,1 = O(p) in

the results when Wd generic scenarios.

2=1

Generalization Bound Neyshabur et al. (2015)
Bartlett et al. (2017)
Neyshabur et al. (2017) Golowich et al. (2017)
Our results

Original Results

O 2D ·Dd=1Bd,F
m

O Dd=1Bd,2
m

D Bd2,/231 d=1 Bd2,/23

3/2

O Dd=1Bd,2
m

D2p

D Bd2,F d=1 Bd2,2

O Dd=1Bd,F · min

41m

, 

D m

Theorem 1: O

dD=1Bd,2 Dp2 m

Wd 2 = 1 O 2D·pD/2
m
 O D3p2
m
 O D3p2
m

O

pD/2 · min 

41m ,

O Dp2
m

D m

existing ones based on the norms of weight matrices. However, it is not clear how the weight matrices explicitly control these parameters, which makes the results less interpretable. Thus, we do not compare with these types of results. We summarize the comparison between existing norm based generalization bounds with our results in Table 1, as well as the results when Wd 2 = 1 for more explicit comparison in terms of the network sizes (i.e, depth and width).

For (Q2), we consider several widely used architectures to demonstrate, including convolutional

neural networks (CNNs) (Krizhevsky et al., 2012), residual networks (ResNets) (He et al., 2016),

and hyperspherical networks (SphereNets) (Liu et al., 2017b). By taking their structures of weight

matrices into consideration, we provide tight characterization of their resulting capacities. In partic-

ular, we consider orthogonal filters and normalized weight matrices, which show good performance

in both optimization and generalization (Mishkin & Matas, 2015; Xie et al., 2017). This is closely

related with normalization frameworks, e.g., batch normalization (Ioffe & Szegedy, 2015) and layer

normalization (Ba et al., 2016), which have achieved great empirical performance (Liu et al., 2017a;

He et al., 2016). Take CNNs as an example. By incorporating the orthogonal structure of convolu-

tional filters, achieve O

we achieve O

D-1 k2 s

D3p2

k

D 2

s

/m

 Dk2/ m , while Bartlett et
and Golowich et al. (2017)

al. (2017); achieve O

Neyshabur et al.

D
p 2 min

41m ,

(2017)

D m

,

where k is the filter size that satisfies k p and s is stride size that is usually of the same or-

der with k; see Section 4.1 for details. Here we achieve stronger results in terms of both depth D

and width p for CNNs, where our bound only depend on k rather than p. Some recent result achieved

results that is free of the linear dependence on the weight matrix norms by considering networks with

bounded outputs (Zhou & Feng, 2018). We can achieve similar results using bounded loss functions

as discussed in Section 3.2, but do not restrict ourselves to this scenario in general. Analogous

improvement is also attained for ResNets and SphereNets. In addition, we consider some widely

used operations for width expansion and reduction, e.g., padding and pooling, and show that they

do not increase the generalization bound. Further numerical evaluation is provided for quantitative

comparison in Section 4.5.

Our tighter bounds result in potentially stronger expressive power, hence higher training/testing accuracy for the DNNs. In particular, when achieving the same order of generalization errors, we allow the choice of a larger parameter space with deeper/wider networks and larger matrix spectral norms. We further show numerically that a larger parameter space can lead to better empirical performance. Quantitative analysis for the expressive power of DNNs is of great interest on its own, which includes (but not limited to) studying how well DNNs can approximate general class of functions and distributions (Cybenko, 1989; Hornik et al., 1989; Funahashi, 1989; Barron, 1993; 1994; Lee et al., 2017; Petersen & Voigtlaender, 2017; Hanin & Sellke, 2017), and quantifying the computation hardness of learning neural networks; see e.g., Shamir (2016); Eldan & Shamir (2016); Song et al. (2017). We defer our investigation toward this to future efforts.

2

Under review as a conference paper at ICLR 2019

Notation. Given an integer n > 0, we define [n] = {1, . . . , n}. Given a vector x  Rp, we denote

xi as the i-th entry, and xi:j as a sub-vector indexed from i-th to j-th entries of x. Given a matrix A  Rn×m, we denote Aij as the entry corresponding to i-th row and j-th column, Ai (Ai) as

the i-th row (column), AI1I2 as a submatrix of A indexed by the set of rows I1  [n] and columns

I2  [m], A 2,1 =

A
n
i=1

as a generic Ai 2. We

norm, write

{aAi}ni2=1as=th{eas1p,e.c.t.ra, lanno}rams,a

A set

F as the Frobenius norm, containing a sequence of

and size

n. Given two real values a, b  R+, we write a ( )b if a  ()cb for some generic constant

c > 0. We use the standard notations O (·) and  (·) to denote limiting behaviors ignoring constants,

and O (·) and  (·) to further ignore logarithmic factors.

2 PRELIMINARIES

We provide a brief introduction to the generalization error bound; see Kearns & Vazirani (1994); Mohri et al. (2012) for more details. To start with, the output of a class of D-layer neural networks with bounded norm for weight matrices WD = {Wd  Rpd×pd-1 }dD=1 is denoted as
FD, · = f (WD, x) = fWD (· · · fW1 (x))  RpD | d  [D], Wd  WD, Wd  Bd , (1)
where x  Rp0 is an input, fWd (y) = d (Wd · y) : Rpd-1  Rpd with an entry-wise activation function d(·), e.g., the rectified linear unit (ReLU) activation (Nair & Hinton, 2010) and the sigmoid activation, and {Bd}dD=1 are real positive constants. For ease of discussion, we fix the activation function to be the ReLU throughout this paper. The extension to more general activations, e.g., Lipschitz continuous functions, is straightforward. We will specify the norm · and the corresponding upper bounds Bd, e.g., · 2 and Bd,2, or · F and Bd,F, when necessary.

Then we denote a class of loss functions measuring the discrepancy between a DNN's output f (WD, x) and the corresponding observation y  Ym for a given input x  Xm as

G FD, · = g(f (WD, x) , y)  R | x  Xm, y  Ym, f (·, ·)  FD, · , where the sets of bounded inputs Xm and the corresponding observations Ym are Xm = {xi  Rp0 | xi 2  R for all i  [m]}  X and Ym = {yi  [pD] for all i  [m]}  Y. Then the empirical Rademacher complexity (ERC) of G FD, · given Xm and Ym is

Rm G FD, ·

=E

sup

{±1}m gG,f (·,·)FD, ·

1m m

i · g (f (WD, xi) , yi)

i=1

where {±1}m  Rm is the set of vectors only containing entries +1 and -1, and with Rademacher entries, i.e., i = +1 or -1 with equal probabilities.

Xm, Ym , (2)  Rm is a vector

Take the classification as an example. For multi-class classification, suppose pD = Nclass is the

number of classes. Consider G with bounded outputs, namely the ramp risk. Specifically, for an

input x belonging to class y  [Nclass], we denote  = (f (WD, x))y - maxi=y (f (WD, x))i. For

a given real value  > 0, the class of ramp

risk functions with parameter  is G FD, · =

g (f (WD, x) , y) |fD  FD, · , where g is

1 

-Lipschitz

continuous,

defied

in

(3).

For

conveni-

 0,

>

g

(f

(WD, x) , y)

=



1-

 

,

  [0, ]

 1,

 < 0,

(3)

ence, we denote g (f (WD, x) , y) as g (f (WD, x)) (or g) in the rest of the paper.

Then the generalization error bound for PAC learning (Bartlett et al., 2017) (Lemma 3.1) states the
following. Given any real value   (0, 1) and g  G, with probability at least 1 - , we have that the generalization error is upper bounded with respect to (w.r.t.) the ERC of G as

E [g

(f

(WD ,

x))]

-

1 m

m i=1

g

(f

(WD ,

xi))



2Rm

G

FD, ·

+3

log(

2 

2m

)

.

(4)

The right hand side of (4) is viewed as a guaranteed error bound for the gap between the testing and
the empirical training performance. Since the ERC is generally the dominating term in (4), a small Rm is desired for DNNs given the loss function G. Analogous results hold for regression tasks; see e.g., Mohri et al. (2012) for details. Analyzing the ERC of DNNs and several popular architectures
in terms of their sizes and weight matrix structures will be the main focus of our paper.

3

Under review as a conference paper at ICLR 2019

3 GENERALIZATION ERROR BOUND FOR DNNS
Our major focus here is to provide tighter bounds w.r.t. the network sizes based on the spectral norm of weight matrices. A refined result, which is free of the linear dependence on the spectral norm, is provided when the loss function is additionally bounded.

3.1 A TIGHTER ERC BOUND FOR DNNS

We first provide the ERC bound for the class of DNNs defined in (1) and Lipschitz loss functions in

the following theorem. The proof is provided in Appendix A.

Theorem

1.

Let

G

be

a

class

of

1 

-Lipschitz

loss

functions

and

FD,

·

2

be

the

class

of

D-layer

networks defined in (1) with spectrally bounded weight matrices, where d is the ReLU activation

and pd = p for all d  [D]. Then the ERC satisfies

Rm G FD, · 2



= OR ·

D
d=1

Bd,2

·

m

 Dp2 log  Dm · maxd Bd,2  .
mind Bd,2

(5)

For convenience, we treat R/ as a constant here. We achieve O(Dd=1Bd,2 · Dp2/m) in Theorem 1, which is tighter than existing results based on the network sizes and norms of weight

matrices, dence on

as D,

sih.eo.w, nO(in2DTabDd=le1B1.d,FI/npmar)ti,cwulhairc,hNceaynshbaebusirgentifiacl.an(2tl0y15la)rgsehrowthaann

exponential depenours. Bartlett et al.

(2017); Neyshabur et al. (2017) demonstrate polynomial dependence on sizes and the spectral norm

of weights, i.e., O(Dd=1Bd,2 · D3p2/m). Our result in (5) is tighter by an order of D, which is significant in practice. Note that p2 may reduce to p in Bartlett et al. (2017); Neyshabur et al. (2017)

when all weight matrices reduce to rank 1. This is a degenerate case as the overall network is reduced

to a rank 1 matrix. Even then, our bound is still tighter if p  D2, which usually holds in practice

for deep networks. More recently, Golowich et al. (2017) demonstrate a bound w.r.t the Frobenius

norm as O dD=1Bd,F · min

D m

,

m-

1 4

·

log

3 4

(m)

log (C1)

,

where

C1

=

R·dD=1 Bd,F
supxX f (WD ,x)

.
2

This

shows a tighter dependence on network sizes. Nevertheless, Wd F is generally p times larger

than Wd 2, which results in an exponential dependence pD/2 compared with the bound based on

the spectral norm. Moreover, log(C1) is linear on D except that the stable ranks Wd F / Wd 2

across

alllayers

are

close

to

1.

In

addition,

it

has

m-

1 4

dependence

rather

than

m-

1 2

except when

D = O ( m). Note that our bounded is obtained using a novel characterization of Lipschitz prop-

erties of DNNs, which may be of independent interest from the learning theory point of view. We

refer to Appendix A for detailed analysis.

We also remark that when achieving the same order of generalization errors, our result allows the

choices of larger dimensions (D, p) and spectral norms of weight matrices, which lead to stronger

expressive power for DNNs. For example, when achieving the same bound with Wd 2 = 1 in

spectral norm based results in Golowich et al. (2017)),

(e.g. they

in ours) and Wd F only have Wd 2 =

O=(11/inpF)roinbeFnrioubsennoiurms nboarsmedbaresseudltrses(ue.lgts.,.

The later results in a much smaller space for eligible weight matrices, which may lead to weaker

expressive ability of DNNs. We also demonstrate numerically that when norms of weight matrices

are constrained to be very small, both training and testing performance degrade significantly; see

further discussion in Section 4.5. A quantitative analysis for the tradeoff between the expressive

ability and the generalization for DNNs is deferred to a future effort.

3.2 A SPECTRAL NORM FREE ERC BOUND

When, in addition, the loss function is bounded, we have the ERC bound free of the linear dependence on the spectral norm, as in the following corollary. The proof is provided in Appendix B.

Corollary 1. In addition to the conditions in Theorem 1, suppose we further let the class G be bounded, i.e., |g|  b for all g  G. Then the ERC satisfies

Rm G FD, · 2



= O min

R

D d=1

Bd,2

,

b



· 1 m



Dp2 log

 Dm · maxd Bd,2 mind Bd,2

.

4

Under review as a conference paper at ICLR 2019

The boundedness of G holds for certain loss functions, e.g., the ramp risk defined in (3). When b

is constant (e.g., b = 1 for the ramp risk) and R

D d=1

Bd,2

is

large

compared

with

the

margin

,

we have that the ERC reduces to O( Dp2/m). This is close to the VC dimension of DNNs, which

can be significantly tighter than existing norm based bounds in general. Similar results, which are

free of linear dependence on

D d=1

Bd,2,

are

also

achieved

recently

(Zhou

&

Feng,

2018;

Arora

et al., 2018). However, they are different in the sense that Arora et al. (2018) depend on error-

resilience parameters, such as Jacobian matrices, intermediate layer output and operations, which

are not explicitly interpretable as matrix norms, and Zhou & Feng (2018) study the bound only for

CNNs rather than general DNNs. Similar norm free results hold for the architectures discussed in

Section 4 using argument for Corollary 1, which we skip for ease of discussion.

4 EXPLORING NETWORK STRUCTURES

The generic result in Section 3 does not highlight explicitly the potential impacts for specific structures of the network and weight matrices. In this section, we consider several popular architectures of DNNs, including convolutional neural networks (CNNs) (Krizhevsky et al., 2012), residual networks (ResNets) (He et al., 2016), and hyperspherical networks (SphereNets) (Liu et al., 2017b), and provide sharp characterization of the corresponding generalization bounds. In particular, we consider orthogonal filters and normalized weight matrices, which have shown good performance in both optimization and generalization (Mishkin & Matas, 2015; Xie et al., 2017). Such constraints can be enforced using regularizations on filters and weight matrices, which can be very efficient to implement in practice. This is also closely related with normalization approaches, e.g., batch normalization (Ioffe & Szegedy, 2015) and layer normalization (Ba et al., 2016), which have achieved tremendous empirical success.

4.1 CNNS WITH ORTHOGONAL FILTERS

CNNs are one of the most powerful architectures in deep learning, especially in tasks related to

images and videos (Goodfellow et al., 2016). We consider a tight characterization of the general-

ization bound for CNNs by generating the weight matrices using orthogonal filters. Specifically, we

generate the weight matrices using a circulant approach, as follows. For the convolutional operation

at the d-th layer, we have nd channels of convolu-

tion filters, each of which is generated from a kd-

dimensional feature using a stride side sd. Sup-

pose that

and

pd-1 sd

sd are

dinivteigdeerssb, tohtehnkwd eahnadvpedp-d1=, i.en.d,·spkddds--d11.

This is equivalent to fixing the weight matrix at

the d-th layer to be generated as in (6), where

for all j



[nd], each Wd(j)



R pd-1 sd

×pd-1

is formed in a circulant-like way using a vector

w(j)  Rkd with unit norms for all rows as in (7).

Wd = Wd(1) · · · Wd(nd)

 Rpd×pd-1 , (6)

 w(j) 0 · · · · · · · · · · · · · · · · · · · · · · 0 

  

Rpd-1 -kd
0 · · · 0 w(j) 0 · · · · · · · · · · · · · · · · 0

  



Wd(j)

=

 

Rsd







Rpd-1 -kd -sd
...

. (7)




 

w((sjd) +1):kd 0 · · · · · · · · · · · · 0 w1(j:s)d

 

Rpd-1 -kd

When the stride size sd = lowing lemma establishes

1th, aWt dw(jh) ecnorrews(pjo)ndnj=sd t1oaarestoarntdhaorgdocniarcl uvleacnttomrsawtriitxh(uDnaivt iEs,u2c0li1d2e)a.nTnhoerfmosl-,

the generalization bound only depend on sd and kd that are independent of the width pd. The proof

is provided in Appendix C.

Corollary 2.

Let G

be the class of

1 

-Lipschitz

loss

functions

and

FD,

·

2

be the class of D-layer

CNNs defined in (1), where d is the ReLU activation, sd = s, kd = k, and s divides both k and

pd for all d  [D]. Suppose the weight matrices in CNNs are formed as in (6) and (7) in all layers

d  [D], where

w(j)

nd j=1

are

orthogonal

vectors

with

unit

Euclidean

norms,

i.e.,

w(j)

w(i) = 0

for all i, j  [nd] and i = j with w(j) 2 = 1 for all j  nd. Then the ERC for CNNs satisfies

Rm G FD, · 2

 R·
= O

k
s

D/2

·

m

 D k nd log  Dm  .
d=1

5

Under review as a conference paper at ICLR 2019



Since nd  k in our setting, the ERC for CNNs is orthogonal filtered considered in Corollary 2, we

proportional to have Wd F =

Dpkd2ainndsteWaddof2,1

Dp2. For the = pd, which

lead to the bounds of CNNs in existing results in Table 2. In practice, one usually has kd pd,

which exhibit a significant improvement over existing results, i.e., Dk2

D3p2. On the other

hand, it is widely used in practice that kd = µsd for some small constant µ  1 in CNNs, then we have (kd/sd)D/2 pD/2 resulted from Rm G FD, · F .
Remark 1. Corollary 2 demonstrates the generalization bound of CNNs when the input at each layer is a vector. For matrix inputs, e.g., images, similar results hold by consider-

Table 2: Comparison with existing norm based

bounds of CNNs. We suppose R and  are generic

constants for ease of illustration. The results of

CNNs in existing works are obtained by substitut-

ing the corresponding norms of the weight matri-

ces generated by orthogonal filters, i.e.,

k/s,

Wd

F

=

 p,

and

Wd

2,1 = p.

Wd

2

=

ing vectorized input and permuting columns of Wd. Specifically, suppose kd and pd-1

are integers for ease of discussion. Consider

the input tained by

as a pd-1 vectorizing

daimepnds-io1na×l

vepcdto-r1

obin-

put matrix. When the 2-dimensional (matrix)

convolutional filters are of size kd × kd,

we form the rows of each nating kd vectors {w(j,i)

Wd(j) }i=k1d

by concatepadded with

0's, each of which is a concatenation of one

row of the filter of size kd with some zeros

as follows:

Generalization Bound Neyshabur et al. (2015)
Bartlett et al. (2017)
Neyshabur et al. (2017)

CNNs

O 2D·p

D 2

m

D-1

O

(

k s

)

2· 

D3 p2

m

D-1

O

(

k s

)

2· 

D3 p2

m

Golowich et al. (2017)

O

D
p 2 min

41m ,

D m

Our results

O

(

k s

)

D 2 

 Dk2

m


w(j,1) 0 · · · · · · 0 · · · · · · · · · w(j, kd) 0 · · · · · · 0 0 · · · · · · · · · · · · 0 .



R

kd
R

pd-1 kd


- kd

 R kd

R

pd-1 kd


- kd

Rpd-1 -pd-1

Correspondingly,

the

stride

size

is

sd2 kd

on

average

and

we

have

Wd

2

kd sd

if

w(j,i) 2 = 1 for all

i, j; see Appendix E for details. This is equivalent to permuting the columns of Wd generated as in

(7) by vectorizing the matrix filters in order to validate the convolution of the filters with all patches

of the matrix input.

Remark 2. A more practical scenario for CNNs is when a network has a few fully connected layers

after the convolutional layers. Suppose we have DC convolutional layers and DF fully connected

layers. From the analysis in Corollary 2, when sd = kd for convolutional layers and Wd 2 = 1 for

fully connected layers, we have that the overall ERC satisfies O

R·

DCk2+DF p2 m

.

4.2 RESNETS WITH STRUCTURED WEIGHT MATRICES

Residual networks (ResNets) (He et al., 2016) is one of the most powerful architectures that allows

training of tremendously deep networks. Specifically, denote the class of D-layer residual networks

with bounded norms for weight matrices VD =

Vd  Rpd×qd

D d=1

,

UD

=

Ud  Rqd×pd

D d=1

as

FDRN, · = f (VD, UD, x) = fVD,UD (· · · fV1,U1 (x))  RpD Vd  BVd , Ud  BUd , (8)

where fVd,Ud (x) =  (Vd ·  (Udx) + x) for all d = 2, . . . , D and fV1,U1 (x) =  (V1 ·  (U1x)). We then provide an upper bound of the ERC for ResNets in the following corollary. The proof is

provided in Appendix D.

Corollary

3.

Let

G

be

the

class

of

1 

-Lipschitz

loss

functions

and

FDRN, ·

2

be

the

class

of

D-

layer ResNets with with spectrally bounded weight matrices defined in (8), where d is the ReLU

activation and pd = p, qd = q for all d  [D]. Then the ERC satisfies

Rm G FDRN, · 2

=O

R·

D d=1

(BVd,2BUd,2+

1)

·

Dpq · log (C1)

,

m

 where C1 =  Dm · maxd {BVd,2 + BUd,2}/ (mind {BVd,2 + BUd,2} · mind {BVd,2BUd,2 + 1}).

6

Under review as a conference paper at ICLR 2019

Compared with the D-layer networks without shortcuts (1), ResNets have a stronger dependence

on the input layer. When

due to the BVd,2 and

shortcuts BUd,2 are

structure, which leads to (BVd of order 1/ D, we still have

,2BUd,2 +

D d=1

(BVd ,2

1) dependence for BUd,2 + 1) = O(1).

each This

partially explains the observation in practice that ResNets have good performance when the weight

matrices have relatively small scales. Also note that to achieve the same bound for Rm G FDRN, · F , we require BVd,F, BUd,F  c, which leads to a much smaller parameter space than the space corresponding to BVd,2, BUd,2  c for the same c.

4.3 HYPERSPHERICAL NETWORKS

We also consider the hyperspherical networks (SphereNets) (Liu et al., 2017b), which demonstrate improved performance than the vanilla DNNs. In specific, the class of D-layer SphereNets is

FDSN, · = f WD, x = fWD · · · fW1 (x)  RpD Wd  BWd ,

(9)

where fWd (x) =  Wdx for all d  [D]. Here, Wd = SWd Wd, where SWd is a diagonal matrix with the i-th diagonal entries being the Euclidean norm of the i-th row of Wd. Note that we do

not normalize the input x as in (Liu et al., 2017b) for ease of the discussion. A direct result of

applying Theorem 1 to (9) implies that Rm G FDSN, · 2

=O

R·

D d=1

BWd ,2 ·

m

Dp2

. Such a self-

normalization architecture has a benefit that BWd,2 is small (close to 1) in general when the weights

are spread out. In addition, it has lower computational costs than the weight normalization based on

the spectral norm directly, and effective empirical results have been observed (Liu et al., 2017b;a).

4.4 EXTENSION TO WIDTH-CHANGE OPERATIONS

Change the width for certain layers is a widely used operation, e.g., for CNNs and ResNets, which can be viewed as a linear transformation in many cases. In specific, denote x{d}  Rpd as the output of the d-th layer. Then we can use a transformation matrix Td  Rpd+1×pd to denote the operation to change the dimension between the output of the d-th layer and the input of the (d + 1)-th layer
as fWd+1 x{d} =  Wd+1Tdx{d} . Denote the set of layers with width changes by IT  [D]. Combining with Theorem 1, we have that the ERC satisfies

Rm G FD, · 2

= O R · Dd=1Bd,2 · tIT Tt 2 · m

Dp2

.

(10)

Next, we illustrate several popular examples to show that tIT Tt 2 is a size independent constant. We refer to Goodfellow et al. (2016) for more operations of changing the width.

Width Expansion. Two popular types of width expansion are padding and 1 × 1 convolution. For
ease of discussion, suppose pd+1 = s · pd for some positive integer s  1. Taking padding with 0 as an example, where we plug in (s - 1) zeros before each entry of x{d}, which is equivalent to setting Td  Rspd×pd with (Td)ij = 1 if i = js, and (Td)ij = 0 otherwise. This implies that Td 2 = 1.

For 1 × 1 convolution, suppose that the convolution features are {c1, . . . , cs}. Then we expand

width by performing convolution (essentially entry-wise product) using s features respectively. This

is equivalent to setting Td  Rspd×pd with (Td)ij = ck if i = j +(k-1)s for k  [s], and (Td)ij = 0

otherwise. It implies that Td 2 =

s i=1

c2i .

When

s i=1

ci2



1,

we

have

Td

2  1.

Width Reduction. Two popular types of width reduction are average pooling and max pooling.

Suppose pd+1

=

pd s

is an

integer

for some positive

integer

s.

For

average pooling,

we

pool

each

nonoverlapping

s

features

into

one

feature.

This

is

equivalent

to

setting

Td



R pd s

×pd

with

(Td)ij

=

1/s if j = (i - 1)s + k for k  [s], and (Td)ij = 0 otherwise. This implies that Td 2 = 1/s.

For max pooling, we choose the largest entry in each nonoverlapping feature segment of length s.

Denote the set Is

=

{(i - 1) × s + 1, . . . , i · s}.

This is equivalent

to setting Td



R pd s

×pd

with

(Td)ij = 1 if |(x{d})j|  |(x{d})k|  k  Is, k = j, and (Td)ij = 0 otherwise. This implies that

Td 2 = 1. For pooling with overlapping features, similar results hold.

7

Under review as a conference paper at ICLR 2019

4.5 NUMERICAL EVALUATION

To better illustrate the difference between our result and existing ones, we demonstrate some com-

parison results in Figure 1 using real data. In specific, we train a simplified VGG19-net (Simonyan & Zisserman, 2014) using 3 × 3 convolution fil-

15 10

0.99
} 0.07
0.92

0.98
} 0.07
0.91

0.81
} 0.05
0.76

log10 (·) Accuracy

ters (with unit norm constraints) on the

CIFAR-10 dataset (Krizhevsky & Hinton,

2009). We first compare with the capacity terms in Bartlett et al. (2017) (Bound1),

5

Training Accuracy Testing Accuracy

0.26 0.03
0.23

}

Neyshabur et al. (2017) (Bound2), and

Golowich et al. (2017) (Bound3) by ignor-

ing

the

common

factor

R m

as

follows:

Ours

Bound1 Bound2 Bound3
(a)

1

1/5

1/25

1/100

Filter Norm

(b)

· Ours: Dd=1Bd,2

k

D d=1

nd

· Bound1: Dd=1Bd,2

D Bd2/,231 d=1 Bd2/,23

3/2

· Bound2: Dd=1Bd,2 D2p 
· Bound3: Dd=1Bd,F D

D pdBd2,F d=1 Bd2,2

Figure 1: Panel (a) shows comparison results for the same VGG19 network trained on CIFAR10 using unit norm filters. The vertical axis is the logarithmic scale of the corresponding bounds. Panel (b) shows the training accuracy (red diamond), testing accuracy (blue cross), and the empirical generalization error using different scales of the filters listed on the horizontal axes.

Note that we do not compare with (Zhou & Feng, 2018; Arora et al., 2018) since we are interested in results with explicit dependence on weight matrix structure (i.e., norms), without further restriction on the network (e.g., bounded activation). Moreover, since we may have more filters nd than their dimension k, we do not assume orthogonality here. Thus we simply use the upper bounds of norms

Bd rather than the form as in Table 2. Following the analysis of Theorem 1, we have

k

D d=1

nd

dependence rather than

Dp2 as k

D d=1

nd

is

the

total

number

free

parameter

for

CNNs,

where

nd

is the number of filters at d-th layer.

For the same network and corresponding weight matrices, we see from Figure 1 (a) that our result

(104  105) is significantly smaller than Bartlett et al. (2017); Neyshabur et al. (2017) (108  109)

and Golowich et al. (2017) (1014  1015). As we have discussed, our bound benefits from tighter

dependence on the dimensions. Note that k

D d=1

nd

is

approximately

of

order

Dk2,

which

is

sig-

nificantly smaller than

D d=1

Bd2,/231

/Bd2,/23

3 in Bartlett et al. (2017) and D2p

D d=1

pd Bd2,F /Bd2,2

in

Neyshabur et al. (2017) (both are of order D3p2). In addition, this verifies that spectral dependence

is significantly tighter than Frobenius norm dependence in Golowich et al. (2017). Further, we show

the training accuracy, testing accuracy, and the empirical generalization error using different scales

on the norm of the filters in Figure 1 (b). We see that the generalization errors decrease when the

norm of filters decreases. However, note that when the norms are too small, the accuracies drop

significantly due to a potentially much smaller parameter space. Thus, the scales (norms) of the

weight matrices should be nether too large (induce large generalization error) nor too small (induce

low accuracy) and choosing proper scales is important in practice as existing works have shown. On

the other hand, this also support our claim that when Rm G FD, · F (or other existing bound) attains the same order with our Rm G FD, · 2 , we have better training/testing performance.

5 DISCUSSION

Our investigation on the generalization bound establishes that the spectral norm is a tighter charac-

terization for the norm based analysis of the ERC on DNNs, compared with other norms (e.g., Frobe-

nius norm). This is also closely related the efficient optimization of DNNs. For example, effective

initializations generally require the spectral norms (rather than the Frobenius norm) of weight matri-

ces to be approximately constant (Glorot & Bengio, 2010). Therefore, properly choosing the struc-

ture of weight matrices can significantly affect both generalization and optimization performance

of DNNs, as is observed in various applications (Goodfellow et al., 2016). On the other hand, the

lower bound in Golowich et al. (2017) states that Rm G FD, · 2

=  R·

D d=1

Bd,2



p m

, which

implies that further improvement maybe achievable.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Martin Anthony and Peter L Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 2009.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930­945, 1993.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine Learning, 14(1):115­133, 1994.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE Transactions on Information Theory, 44(2):525­536, 1998.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241­6250, 2017.
Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493­2537, 2011.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4):303­314, 1989.
Philip J Davis. Circulant Matrices. American Mathematical Soc., 2012.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933, 2017.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory, pp. 907­940, 2016.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks. Neural Networks, 2(3):183­192, 1989.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249­256, 2010.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. arXiv preprint arXiv:1712.06541, 2017.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT Press Cambridge, 2016.
Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width. arXiv preprint arXiv:1710.11278, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359­366, 1989.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
9

Under review as a conference paper at ICLR 2019
Michael J Kearns and Umesh Virkumar Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097­1105, 2012. Holden Lee, Rong Ge, Andrej Risteski, Tengyu Ma, and Sanjeev Arora. On the ability of neural nets to express distributions. arXiv preprint arXiv:1702.07028, 2017. Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In IEEE Conference on Computer Vision and Pattern Recognition, volume 1, 2017a. Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep hyperspherical learning. In Advances in Neural Information Processing Systems, pp. 3953­3963, 2017b. Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015. Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. MIT Press, 2012. Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In International Conference on Machine Learning, pp. 807­814, 2010. Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pp. 1376­1401, 2015. Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017. Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. arXiv preprint arXiv:1709.05289, 2017. Ohad Shamir. Distribution-specific hardness of learning neural networks. arXiv preprint arXiv:1609.01037, 2016. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural networks. In Advances in Neural Information Processing Systems, pp. 5520­5528, 2017. Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation. arXiv preprint arXiv:1703.01827, 2017. Pan Zhou and Jiashi Feng. Understanding generalization and optimization performance of deep cnns. arXiv preprint arXiv:1805.10767, 2018.
10

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 1
Our analysis is based on the characterization of the Lipschitz property of a given function on both input and parameters. Such an idea can potentially provide tighter bound on the model capacity in terms of these Lipschitz constants and the number of free parameters, including other architectures of DNNs. We first provide an upper bound for the Lipschitz constant of f (WD, x) in terms of the input x. Lemma 1. Given WD, for any f (WD, ·)  FD, · 2 and x1, x2  Rp0 , we have
D
f (WD, x1) - f (WD, x2) 2  x1 - x2 2 · Bd.
d=1
Proof. We prove by induction. Specifically, we have
f (WD, x1) - f (WD, x2) 2 =  (WDf (WD-1, x1)) -  (WDf (WD-1, x2)) 2
(i)
 WDf (WD-1, x1) - WDf (WD-1, x2) 2  WD 2 · f (WD-1, x1) - f (WD-1, x2) 2  BD · f (WD-1, x1) - f (WD-1, x2) 2 ,
where (i) comes from the entry-wise 1­Lipschitz continuity of (·). For the first layer, we have
f (W1, x1) - f (W1, x2) 2 =  (W1x1) -  (WDx2) 2  W1x1 - W1x2 2  B1 · x1 - x2 2 .
By repeating the argument above, we complete the proof.

Next, we provide an upper bound for the Lipschitz constant of f (WD, x) in terms of the parameters WD .

Lemma 2. Given any x  Rp0 satisfying x 2  R, for any f (WD, x) , f

with WD = {Wd}dD=1 and WD =

D
Wd , we have
d=1

WD, x

 FD, · 2

D

f (WD, x) - f WD, x  R D
2

Wd - Wd 2F.

d=1

Proof. Given x and two sets of weight matrices {Wd}dD=1,

D
Wd , we have
d=1

fWD fWD-1 (· · · fW1 (x)) - fWD fWD-1 · · · fW1 (x)

2

 fWD fWD-1 (· · · fW1 (x)) - fWD fWD-1 (· · · fW1 (x)) 2

+ fWD fWD-1 (· · · fW1 (x)) - fWD fWD-1 · · · fW1 (x)
(i)
 WD · fWD-1 (· · · fW1 (x)) - WD · fWD-1 (· · · fW1 (x)) 2

2

+ WD · fWD-1 (· · · fW1 (x)) - WD · fWD-1 · · · fW1 (x) 2



WD - WD ·
F

fWD-1 (· · · fW1 (x)) 2

+

WD ·
2

fWD-1 (· · · fW1 (x)) - fWD-1

· · · fW1 (x)

,
2

(11)

11

Under review as a conference paper at ICLR 2019

where (i) is from the entry-wise 1­Lipschitz continuity of (·). On the other hand, for any d  [D],

we further have

(i)
fWd (· · · fW1 (x)) 2  Wd · fWd-1 (· · · fW1 (x)) 2  Wd 2 · fWd-1 (· · · fW1 (x)) 2

(ii) d
 Wi 2 · x 2 .
i=1

(12)

where (i) is from the entry-wise 1­Lipschitz continuity of (·) and (ii) is from recursively applying

the same argument.

Combining (11) and (12), we obtain

fWD fWD-1 (· · · fW1 (x)) - fWD fWD-1 · · · fW1 (x)

2

D-1

D-2



Wd 2 ·

x 2·

WD - WD

+
F

WD 2 ·

Wd 2 ·

d=1

d=1

x 2·

WD-1 - WD-1

F

+

WD-1 ·
2

fWD-2 (· · · fW1 (x)) - fWD-2

· · · fW1 (x)

2

(i)


D d=1

Bd,2

·

x2

D

mind Bd,2

d=1

Wd - Wd

F

D d=1

Bd,2

·

x

 2D

mind Bd,2

where (i) is from recursively applying arguments above.

D
Wd - Wd F2 ,
d=1

Then we provide an upper bound for the ERC of a class of function that is Lipschitz on both param-

eters and input based on the number of parameters of DNNs.

Lemma 3. Suppose g(w, x) is Lw-Lipschitz over w  Rh with w 2  K and  =

supgG,xXm |g(w, x)|. Then the ERC of G = {g(w, x)} satisfies

 

h

log

 KLw m



Rm (G) = O 

  h . m

Proof. For any w, w  Rh and x, we have

 (g1, g2) = |g1(x) - g2(x)| = |g (w, x) - g (w, x)|  Lw w - w 2 .

(13)

Since g is a parametric function with h parameters, then we have the covering number of G under

the metric  in (13) satisfies

N (G, , ) 

3K Lw

h
.



Then using the standard Dudley's entropy integral bound on the ERC (Mohri et al., 2012), we have

the ERC satisfies

Rm (G)

inf  + 1

supgG (g,0)

>0

m

log N (G, , ) d.

(14)

We then define

 = sup  (g, 0) = sup |g(w, x)| .

g(w,x)G,xXm

gG,xXm

Then we have

Rm (G)

inf  + 1 

>0

m

h log KLw d 

 inf  + 
>0

h

log

K Lw 

(i) 

m


h log KLw m  h , m

where (i) is obtained by taking  =  h/m.

12

Under review as a conference paper at ICLR 2019

From

Lemma

1

and

1 

-Lipschitz

continuity

of

g,

we

have

  LxR  R ·

D d=1

Bd,2

.



From Lemma 2, we have

Lw



 R D·

D d=1

Bd,2

.

mind Bd,2

Moreover, when pd = p for all d  [D], we have

K=

D

Wd

2 F



 pD

·

max
d

Bd,2.

d=1

Combining the results above with Lemma 3 and h = Dp2, we have

Rm (G)



R·

D d=1

Bd,2

Dp2 log 

Dm maxd Bd,2 mind Bd,2

.

m

(15)

B PROOF OF COROLLARY 1

The analysis follows Theorem 1, except that the bound for  in (15) satisfies

  min

R· b,

D d=1

Bd,2

,



since

g

satisfies

|g|



b

and

1 

-Lipschitz

continuous.

Then

we

have

the

desired

result.

C PROOF OF COROLLARY 2

We first show that using unit norm filters for all d  [D] and nd  kd, we have

Wd 2 =

kd , sd

(16)

First note that when nd = kd, due to the orthogonality of

w(j)

kd j=1

,

for

all

i, q



[kd],

i

=

q,

we

have

kd

wi(j)

2
=1

and

kd
wq(j) · wi(j) = 0.

j=1

j=1

(17)

When nd = kd, we have for all i  [pd-1], the diagonal entries of Wd Wd satisfy

kd
Wd Wd ii =
j=1

kd

kd sd

Wd(j)

i

=
2 j=1 h=1

w((ij%) sd)+(h-1)sd

2 (=i) kd . sd

(18)

where (i) is from (17). For the off-diagonal entries of Wd Wd, i.e., for i = q, i, q  [pd], we have

kd
Wd Wd iq =
j=1

Wd(j) q

Wd(j) i

kd
=

kd sd
w((ij%) sd)+(h-1)sd · w((qj%) sd)+(h-1)sd (=i) 0,

j=1 h=1

(19)

13

Under review as a conference paper at ICLR 2019

where (i) is from (17). Combining (18) and (19), we have that Wd Wd is a diagonal matrix with

Wd Wd

2

=

kd sd

=

Wd 2 =

kd . sd

For nd < nk, we have that Wd is a row-wise submatrix of that when nd = kd, denoted as Wd. Let

S



nd kd
R sd

×pd

be

a

row-wise

submatrix

of

an

identity

matrix

corresponding

to

sampling

the

row

of

Wd to form Wd. Then we have that (16) holds, and since

Wd =
2

S · WdWd · S

2 2

=

kd . sd

Suppose k1 = · · · = kD = k for ease of discussion. Then following the same argument as in the proof of Theorem 1 using the fact that the number of parameters in each layer is no more than knd rather than p2, we have

R· Rm (G)
R· =

D d=1

dBd,2



k

D d=1

nd

log



Dm maxd Bd,2 mind Bd,2



m

D d=1

d

k sd

·

k 

D d=1

nd

log

  Dm

.

m

D PROOF OF COROLLARY 3

The analysis is analogous to the proof for Theorem 1, but with different construction of the intermediate results. We omit  for ease of discussion. We first provide an upper bound for the Lipschitz constant of f (VD, UD, x) in terms of x. Lemma 4. Given VD and UD, for any f (VD, UD, ·)  FD, · 2 and x1, x2  Rp0 , we have
f (VD, UD, x1) - f (VD, UD, x2) 2  x1 - x2 2 · Dd=1 (dBUd,2BVd,2 + 1) . (20)

Proof. Consider the ResNet layer, for any x1, x2  Rk, we have
f (VD, UD, x1) - f (VD, UD, x2) 2 = fVD,UD (· · · fV1,U1 (x1)) - fVD,UD (· · · fV1,U1 (x2)) 2 =  DVD ·  UD · fVD-1,UD-1 (· · · fV1,U1 (x1)) + fVD-1,UD-1 (· · · fV1,U1 (x1))
-  DVD ·  UD · fVD-1,UD-1 (· · · fV1,U1 (x2)) + fVD-1,UD-1 (· · · fV1,U1 (x2)) 2
(i)
 DVD ·  UD · fVD-1,UD-1 (· · · fV1,U1 (x1)) - DVD ·  UD · fVD-1,UD-1 (· · · fV1,U1 (x2))
+ fVD-1,UD-1 (· · · fV1,U1 (x1)) - fVD-1,UD-1 (· · · fV1,U1 (x2)) 2
(ii)
 (D VD 2 UD 2 + 1) · fVD-1,UD-1 (· · · fV1,U1 (x1)) - fVD-1,UD-1 (· · · fV1,U1 (x2)) 2 ,
where (i) is the fact that  is 1­Lipschitz, and (ii) is from repeating the arguments of (i) and (ii). By recursively applying the argument above, we have the desired result.

2

Next, we provide an upper bound for the Lipschitz constant of f (VD, UD, x) in terms of VD and UD .

Lemma 5. Given any x  Rp0 satisfying x 2  R, for any f (VD, UD, x) , f WD, x  FD, · 2

with WD = {Wd}dD=1 and WD =

D
Wd , we have
d=1

f (VD, UD, x) - f VD, UD, x
2

D  R 2D · ( Vd 2 Ud 2 + 1) ·
d=1

D 2D

2

VD - VD +

UD - UD .

FF

d=1

d=1

14

Under review as a conference paper at ICLR 2019

Proof. Given x and two sets of weight matrices {Wd}Dd=1,

D
Wd , we have
d=1

fVD,UD fVD-1,UD-1 (· · · fV1,U1 (x)) - fVD,UD fVD-1,UD-1 · · · fV1,U1 (x)

2

 fVD,UD fVD-1,UD-1 (· · · fV1,U1 (x)) - fVD,UD fVD-1,UD-1 (· · · fV1,U1 (x)) 2

+ fVD,UD fVD-1,UD-1 (· · · fV1,U1 (x)) - fVD,UD fVD-1,UD-1 · · · fV1,U1 (x)
(i)
 VD UD · fVD-1,UD-1 (· · · fV1,U1 (x)) - VD UD · fVD-1,UD-1 (· · · fV1,U1 (x))

2 2

+ VD UD · fVD-1,UD-1 (· · · fV1,U1 (x)) - VD UD · fVD-1,UD-1 · · · fV1,U1 (x)

(ii)



VD - VD ·
F

UD 2 ·

fVD-1,UD-1 (· · · fV1,U1 (x)) 2

2

+ fVD-1,UD-1 (· · · fV1,U1 (x)) - fVD-1,UD-1 · · · fV1,U1 (x) 2

+

VD ·
2

U fD VD-1,UD-1 (· · · fV1,U1 (x)) - U fD VD-1,UD-1

· · · fV1,U1 (x)

,
2

(21)

where (i) and (ii) from the entry-wise 1­Lipschitz continuity of (·). In addition, we have

U fD VD-1,UD-1 (· · · fV1,U1 (x)) - U fD VD-1,UD-1 · · · fV1,U1 (x) 2

 U fD VD-1,UD-1 (· · · fV1,U1 (x)) - U fD VD-1,UD-1 (· · · fV1,U1 (x)) 2

+ U fD VD-1,UD-1 (· · · fV1,U1 (x)) - U fD VD-1,UD-1 · · · fV1,U1 (x) 2



UD - UD ·
F

fVD-1,UD-1 (· · · fV1,U1 (x)) 2

+

UD ·
2

fVD-1,UD-1 (· · · fV1,U1 (x)) - fVD-1,UD-1

· · · fV1,U1 (x)

.
2

(22)

On the other hand, for any d  [D], we further have

fVd,Ud (· · · fV1,U1 (x)) 2
(i)
 Vd 2 Ud 2 · fVd-1,Ud-1 (· · · fV1,U1 (x)) 2 +
(ii) d
 ( Vi 2 Ui 2 + 1) · x 2 .
i=1

fVd-1,Ud-1 (· · · fV1,U1 (x)) 2

(23)

where (i) is from the entry-wise 1­Lipschitz continuity of (·) and (ii) is from recursively applying the same argument.

Combining (21), (22) and (23), we obtain

fVD,UD (· · · fV1,U1 (x)) - fVD,UD · · · fV1,U1 (x) 2



VD - VD ·
F

UD 2 +

UD - UD ·
F

VD
2

fVD-1,UD-1 (· · · fV1,U1 (x)) 2

+



(i)


R

2D ·

VD VD + 1
22

fVD-1,UD-1 (· · · fV1,U1 (x)) - fVD-1,UD-1

D d=1

(BVd,2BUd,2

+

1)

·

maxd

{BVd,2

+

BUd,2}

mind {BVd,2BUd,2 + 1}

· · · fV1,U1 (x)

D 2D

2

·

VD - VD +

UD - UD ,

FF

d=1

d=1

2

where (i) is from recursively applying arguments above.

15

Under review as a conference paper at ICLR 2019

Let q1 = · · · = qD = q. Combining Lemma 3, Lemma 4, Lemma 5, and h = 2Dpq, we have

Rm (G)

R·

D d=1

(BVd,2BUd,2

+

1)

·

Dpq · log

{ }
 Dm·maxd BVd,2+BUd,2
{ } { }mind BVd,2+BUd,2 ·mind BVd,2BUd,2+1



m

.

E SPECTRAL BOUND FOR Wd IN CNNS WITH MATRIX FILTERS

We provide further discussion on the upper bound of the spectral norm for the weight matrix Wd in CNNs with matrix filters. In particular, by denoting Wd using submatrices as in (6), i.e.,

Wd = Wd(1) · · · Wd(nd)

 Rpd×pd-1 ,

we have that each block matrix Wd(j) is of the form





Wd(j)

=

  



Wd(j) (1, 1) Wd(j) (2, 1)
...

Wd(j) (1, 2) Wd(j) (2, 2)
...

 Wd(j)

, 1pd-1kd
sd

Wd(j)

, 2pd-1kd
sd

··· ···

Wd(j) Wd(j)

1, 2,

ppdd--11

. . .  ...

· · · Wd(j)

pd-1 sd

kd

,

pd-1


  ,   



where Wd(j) (i, l)  R

pd-1 kd sd

×pd-1

for

all

i




pd-1 kd sd

(24) and l  pd-1 . Particularly, off-

diagonal

blocks

are

zero

matrices,

i.e.,

Wd(j)

(i,

l)

=

0

for

i

=

l.


For

diagonal

blocks,

we

have

 w(j,1) 0 · · · · · · · · · · · · 0 · · · · · · · · · · · · · · · w(j, kd) 0 · · · · · · · · · · · · · · · · · · · · · 0 

  R kd

R

pd-1 kd


- kd

 R kd



R

pd-1 kd


- kd

  

 

0 · · · 0 w(j,1) 0 · · · · · · · · · · · · 0 · · · · · · · · · · · · · · · w(j, kd) 0 · · · · · · · · · · · · · ·0

 



Wd(j)

(i,

i)

=

 

sd R kd

 R kd

R

pd-1 kd


- kd



 R kd

R

pd-1 kd


- kd-

sd
kd



 

.









 

w(j,1)

 sd

kd

0···
R

·········

pd-1 kd


- kd

0

·

·

·

·

·

·

·

·

·

...  w(j, kd)
 R kd

0

·

·

·

···
R

·

·········

pd-1 kd


- kd

·

·

·

·0

w{(js,11d)}

     

(25)

where

w{(js,11d)}

=

w1(j:,1s)d kd

sd
 R kd

and w(j,1)
sd
kd

=

w(j,1)

 kd

-

sd
kd

+1

sd
  R kd . Combining (24)
: kd

and

(25),

we

have

that

the

stride

for

Wd(j)

is

.sd2
kd

Using

the

same

analysis

for

Corollary

2.

We

have

Wd 2 = 1 if

i

w(j,i)

2 2

=

kd sd

.

For

image

inputs,

we

need

an

even

smaller

matrix

Wd(j)

(i,

i)

with


fewer

rows

than

(25),

denoted

as

 w(j,1) 0 · · · · · · · · · · · · 0 · · · · · · · · · · · · · · · w(j, kd) 0 · · · · · · · · · · · · · · · · · · · · · 0 

  R kd

R

pd-1 kd


- kd

 R kd



R

pd-1 kd


- kd

  

 

0 · · · 0 w(j,1) 0 · · · · · · · · · · · · 0 · · · · · · · · · · · · · · · w(j,

kd) 0 · · · · · · · · · · · · · ·0

 



Wd(j)

(i,

i)

=

 

sd R kd

 R kd



R

pd-1 kd


- kd

  .R kd

R

pd-1 kd


- kd-

sd
kd

  

...


 

 

0 · · · · · · · · · · · · · · · · · · 0 w(j,1) 0 · · · · · · · · · · · · · · · 0 · · · · · · · · · · · · · · · w(j,

kd )

 

 R

pd-1 kd


- kd

 R kd

R

pd-1 kd


- kd

 R kd

 (26)

16

Under review as a conference paper at ICLR 2019

Then Wd 2  1 still holds if of Wd generated using (25).

i

w(j,i)

2 2

=

kd sd

since

Wd

generated using

(26)

is

a

submatrix

17

