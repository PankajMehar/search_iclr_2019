Under review as a conference paper at ICLR 2019
Analyzing Inverse Problems with Invertible Neural Networks
Anonymous authors Paper under double-blind review
Abstract
For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is well-defined, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task ­ so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a specific measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters.
1 Introduction
When analyzing complex physical systems, a common problem is that the system parameters of interest cannot be measured directly. For many of these systems, scientists have developed sophisticated theories on how measurable quantities y arise from the hidden parameters x. We will call such mappings the forward process. However, the inverse process is required to infer the hidden states of a system from measurements. Unfortunately, the inverse is often both intractable and ill-posed, since crucial information is lost in the forward process.
To fully assess the diversity of possible inverse solutions for a given measurement, an inverse solver should be able to estimate the complete posterior of the parameters, conditioned on an observation. This makes it possible to quantify uncertainty, reveal multi-modal distributions, and identify degenerate and unrecoverable parameters ­ all highly relevant for applications in natural science. In this paper, we ask if invertible neural networks (INNs) are a suitable model class for this task. INNs are characterized by three properties: (i) The mapping from inputs to outputs is bijective, i.e. its inverse exists, (ii) both forward and inverse mapping are efficiently computable, and (iii) both mappings have a tractable Jacobian, which allows explicit computation of posterior probabilities.
Networks that are invertible by construction offer a unique opportunity: We can train them on the well-understood forward process x  y and get the inverse y  x for free by running them backwards at prediction time. To counteract the inherent information loss of the forward process, we introduce additional latent output variables z, which capture the information about x that is not contained in y. Thus, our INN learns to associate hidden parameter values x with unique pairs [y, z] of measurements and latent variables. Forward training learns the mapping [y, z] = f (x) and implicitly determines its inverse x = f -1(y, z) = g(y, z). Additionally, we make sure that the density p(z) of the latent variables is shaped as a Gaussian distribution. Thus, the INN represents the desired
1

Under review as a conference paper at ICLR 2019

SL USL USL SL

forward (simulation): x  y

x

(Bayesian) NN

y

forward (simulation): x  y
y x INN
z

inverse (prediction): y  x Standard (Bayesian) Neural Network

inverse (sampling): [y, z]  x Invertible Neural Network

Figure 1: Abstract comparison of standard approach (left) and ours (right). The standard direct approach requires a discriminative, supervised loss (SL) term between predicted and true x, causing problems when y  x is ambiguous. Our network uses a supervised loss only for the well-defined forward process x  y. Generated x are required to follow the prior p(x) by an unsupervised loss (USL), while the latent variables z are made to follow a Gaussian distribution, also by an unsupervised loss. See details in Section 3.3.
posterior p(x|y) by a deterministic function x = g(y, z) that transforms ("pushes") the known distribution p(z) to x-space, conditional on y.
Compared to standard approaches (see Fig. 1, left), INNs circumvent a fundamental difficulty of learning inverse problems: Defining a sensible supervised loss for direct posterior learning is problematic since it requires prior knowledge about that posterior's behavior, constituting a kind of hen-end-egg problem. If the loss does not match the possibly complicated (e.g. multi-modal) shape of the posterior, learning will converge to incorrect or misleading solutions. Since the forward process is usually much simpler and better understood, forward training diminishes this difficulty. Specifically, we make the following contributions:

· We show that the full posterior of an inverse problem can be estimated with invertible networks, both theoretically in the asymptotic limit of zero loss, and practically on synthetic and real-world data from astrophysics and medicine.
· The architectural restrictions imposed by invertibility do not seem to have detrimental effects on our network's representational power.
· While forward training is sufficient in the asymptotic limit, we find that a combination with unsupervised backward training improves results on finite training sets.
· In our applications, our approach to learning the posterior compares favourably to approximate Bayesian computation (ABC) and conditional VAEs. This enables identifying unrecoverable parameters, parameter correlations and multimodalities.

2 Related work
Modeling the conditional posterior of an inverse process is a classical statistical task that can in principle be solved by Bayesian methods. Unfortunately, exact Bayesian treatment of real-world problems is usually intractable. The most common (but expensive) solution is to resort to sampling, typically by a variant of Markov Chain Monte Carlo (Robert and Casella (2004); Gamerman and Lopes (2006)). If a model y = s(x) for the forward process is available, approximate Bayesian computation (ABC) is often preferred, which embeds the forward model in a rejection sampling scheme for the posterior p(x|y) (Sunnåker et al. (2013); Lintusaari et al. (2017); Wilkinson (2013)).
Variational methods offer a more efficient alternative, approximating the posterior by an optimally chosen member of a tractable distribution family (Blei et al. (2017)). Neural networks can be trained to predict accurate sufficient statistics for parametric posteriors (Papamakarios and Murray (2016); Siddharth et al. (2017)), or can be designed to learn a mean-field distribution for the network's weights via dropout variational inference (Gal and Ghahramani (2015); Kingma et al. (2015)). Both ideas can be combined (Kendall and Gal (2017)) to differentiate between data-related and model-related uncertainty. However, the restriction to limited distribution families fails if the true distribution is too complex (e.g. when it requires multiple modes to represent ambiguous or degenerate solutions)
2

Under review as a conference paper at ICLR 2019

and essentially counters the ability of neural networks to act as universal approximators. Conditional GANs (cGAN; Mirza and Osindero (2014); Isola et al. (2017)) overcome this restriction in principle, but often lack satisfactory diversity in practice (Zhu et al. (2017b)). For our tasks, conditional variational autoencoders (cVAEs; Sohn et al. (2015)) perform better than cGANs, and are also conceptually closer to our approach (see appendix Sec. 2), and hence serve as a baseline in our experiments.
Normalizing flows, introduced to the field of neural networks by Rippel and Adams (2013); Rezende and Mohamed (2015), have the potential to solve these problems. They gradually transform normal densities into target densities and rely on bijectivity to guarantee the validity of these mappings. Thus, they possess property (i) of an INN, and are usually designed to fulfill requirement (iii) as well. Besides variants of the original architectures (Tomczak and Welling (2016); Berg et al. (2018); Trippe and Turner (2018)), the most common realizations use auto-regressive flows, where the density is decomposed according to the Bayesian chain rule (Kingma et al. (2016); Huang et al. (2018); Germain et al. (2015); Papamakarios et al. (2017); Oord et al. (2016); Kolesnikov and Lampert (2017); Salimans et al. (2017); Uria et al. (2016)). These networks successfully learned unconditional generative distributions for artificial data and standard image sets (e.g. MNIST, CelebA, LSUN bedrooms), and some encouraging results for conditional modeling exist as well (Oord et al. (2016); Salimans et al. (2017); Papamakarios et al. (2017); Uria et al. (2016)).
Although flow-based networks are invertible in principle, actual computation of their inverse is too costly to be practical, i.e. property (ii) is not fulfilled. This precludes the possibility of bi-directional or cyclic training, which has been shown to be very beneficial in generative adversarial nets and auto-encoders (Zhu et al. (2017a); Dumoulin et al. (2016); Donahue et al. (2017); Teng et al. (2018)). In fact, optimization for cycle consistency forces such models to converge to invertible architectures, making fully invertible networks a natural choice. True INNs can be built using coupling layers, as introduced in the NICE (Dinh et al. (2014)) and RealNVP (Dinh et al. (2016)) architectures. Despite their simple design and training, these networks were rarely studied: Danihelka et al. (2017); Grover et al. (2017) showed that minimization of an adversarial loss is superior to maximum likelihood training in RealNVPs, Gomez et al. (2017) used a NICE-like design as a memory-efficient alternative to residual networks, Jacobsen et al. (2018) demonstrated that the lack of information reduction from input to representation does not cause overfitting, and the Glow architecture by Kingma and Dhariwal (2018) incorporates invertible 1x1 convolutions into RealNVPs to achieve impressive image manipulations. This line of research inspired us to extend RealNVPs for the task of computing posteriors in real-world inverse problems from natural and life sciences.

3 Methods

3.1 Problem specification

We consider a common scenario in natural and life sciences: Researchers are interested in a set of variables x  RD describing some phenomenon of interest, but only variables y  RM
can actually be observed, for which the theory of the respective research field provides a
model y = s(x) for the forward process. Since the transformation from x to y incurs an information loss, the intrinsic dimension m of y is in general smaller than D, even if the nominal dimensions satisfy M > D. Hence we want to express the inverse model as a conditional probability p(x|y), but its mathematical derivation from the forward model is
intractable in the applications we are going to address.

We aim at approximating p(x|y) by a tractable model q(x|y), taking advantage of the possibility to create an arbitrary amount of training data {(xi, yi)}Ni=1 from the known forward model s(x) and a suitable prior p(x). While this would allow for training of a
standard regression model, we want to approximate the full posterior probability. To this end, we introduce a latent random variable z  RK drawn from a multi-variate standard normal distribution and reparametrize q(x|y) in terms of a deterministic function g of y and z, represented by a neural network with parameters :

x = g(y, z; ) with z  p(z) = N (z; 0, IK ).

(1)

3

Under review as a conference paper at ICLR 2019

Note that we distinguish between hidden parameters x representing unobservable real-world properties and latent variables z carrying information intrinsic to our model.

In contrast to standard methodology, we propose to learn the model g(y, z; ) of the inverse process jointly with a model f (x; ) approximating the known forward process s(x):

[y, z] = f (x; ) = [fy(x; ), fz(x; )] = g-1(x; ) with fy(x; )  s(x).

(2)

Functions f and g share the same parameters  and are implemented by a single invertible neural network. Our experiments show that joint bi-directional training of f and g avoids many complications arising in e.g. cVAEs or Bayesian neural networks, which have to learn
the forward process implicitly.

The relation f = g-1 is enforced by the invertible network architecture, provided that the nominal and intrinsic dimensions of both sides match. When m  M denotes the intrinsic dimension of y, the latent variable z must have dimension K = D - m, assuming that the
intrinsic dimension of x equals its nominal dimension D. If the resulting nominal output dimension M + K exceeds D, we augment the input with a vector x0  RM+K-D of zeros and replace x with the concatenation [x, x0] everywhere. Combining these definitions, our network expresses q(x|y) as

q x = g(y, z; ) | y = p(z) Jx -1,

g(y, z; )

Jx = det

[y, z] y,fz(x)

(3)

with Jacobian determinant Jx. When using coupling layers, according to Dinh et al. (2016), computation of Jx is simple, as each transformation has a triangular Jacobian matrix.

3.2 Invertible architecture

To create a fully invertible neural network, we follow the architecture proposed by Dinh et al.
(2016): The basic unit of this network is a reversible block consisting of two complementary
affine coupling layers. Hereby, the block's input vector u is split into two halves, u1 and u2, which are transformed by an affine function with coefficients exp(si) and ti (i  {1, 2}), using element-wise multiplication ( ) and addition:

v1 = u1 exp(s2(u2)) + t2(u2), v2 = u2 exp(s1(v1)) + t1(v1).

(4)

Given the output v = [v1, v2], these expressions are trivially invertible:

u2 = (v2 - t1(v1)) exp(-s1(v1)), u1 = (v1 - t2(u2)) exp(-s2(u2)). (5)

Importantly, the mappings si and ti can be arbitrarily complicated functions of v1 and u2 and need not themselves be invertible. In our implementation, they are realized by a succession of several fully connected layers with leaky ReLU activations.

A deep invertible network is composed of a sequence of these reversible blocks. To increase model capacity, we apply a few simple extensions to this basic architecture. Firstly, if the dimension D is relatively small, but a complex transformation has to be learned, we find it advantageous to pad both the in- and output of the network with an equal number of zeros. This does not change the intrinsic dimensions of in- and output, but enables the network's interior layers to embed the data into a larger representation space in a more flexible manner. Secondly, we insert permutation layers between reversible blocks, which shuffle the elements of the subsequent layer's input in a randomized, but fixed, way. This causes the splits u = [u1, u2] to vary between layers and enhances interaction among the individual variables. Kingma and Dhariwal (2018) use a similar architecture with learned permutations.

3.3 Bi-directional training
Invertible networks offer the opportunity to simultaneously optimize for losses on both the inand output domains to make training more effective. Hereby, we perform forward and backward iterations in an alternating fashion, accumulating gradients from both directions before performing a parameter update. For the forward iteration, we penalize deviations between

4

Under review as a conference paper at ICLR 2019
simulation outcomes yi = s(xi) and network predictions fy(xi) with a loss Ly yi, fy(xi) . Depending on the problem, Ly can be any supervised loss, e.g. squared loss for regression or cross-entropy for classification.
The loss for latent variables penalizes the mismatch between the joint distribution of network outputs q y = fy(x), z = fz(x) = p(x)/|Jyz| and the product of marginal distributions of simulation outcomes p y = s(x) = p(x)/|Js| and latents p(z) as Lz q(y, z), p(y) p(z) . We block the gradients of Lz with respect to y to make sure that the resulting updates only affect the predictions of z and do not worsen the predictions of y. Thus, Lz enforces two things: firstly, the generated z must follow the desired normal distribution p(z); secondly, y and z must be independent, and not encode the same information twice. Since Lz is implemented as Maximum Mean Discrepancy D (see Sec. 3.4), which only requires samples from the distributions to be compared, the Jacobian determinants Jyz and Js do not have to be known explicitly. In Sec. 1 of the appendix, we prove the following theorem:
Theorem: If an INN f (x) = [y, z] is trained as proposed, and both the supervised loss Ly = E[(y-fy(x))2] and the unsupervised loss Lz = D q(y, z), p(y) p(z) reach zero, sampling according to Eq. 1 with g = f -1 returns the true posterior p(x | y^) for any measurement y^.
Although Ly and Lz are sufficient asymptotically, a small amount of residual dependency between y and z remains after a finite amount of training. This causes q(x|y) to deviate from the true posterior p(x|y). To speed up convergence, we also define a loss Lx on the input side, implemented again by MMD. It matches the distribution of backward predictions q(x) = p y = fy(x) p z = fz(x) /|Jx| against the prior data distribution p(x) through Lx p(x), q(x) . In the appendix, Sec. 1, we prove that Lx is guaranteed to be zero when the forward losses Ly and Lz have converged to zero. Thus, incorporating Lx does not alter the optimum, but improves convergence in practice.
3.4 Maximum mean discrepancy
Maximum Mean Discrepancy (MMD) is a kernel-based method for comparison of two probability distributions that are only accessible through samples (Gretton et al. (2012)). While a trainable discriminator loss is often preferred for this task in high-dimensional problems, especially in GAN-based image generation, MMD also works well, is easier to use and much cheaper, and leads to more stable training (Tolstikhin et al. (2017)). The method requires a kernel function as a design parameter, and we found that kernels with heavier tails than Gaussian are needed to get meaningful gradients for outliers. We achieved best results with the Inverse Multiquadratic k(x, x ) = /( + x - x 22), reconfirming the suggestion from Tolstikhin et al. (2017).
4 Experiments
We first demonstrate the capabilities of INNs on two well-behaved synthetic problems and then show results for two real-world applications from the fields of medicine and astrophysics. Additional details on the datasets and network architectures are provided in the appendix.
4.1 Artificial data
Gaussian mixture model: To test basic viability of INNs for inverse problems, we train them on a standard 8-component Gaussian mixture model p(x). The forward process is very simple: The first four mixture components (clockwise) are assigned label y = red, the next two get label y = blue, and the final two are labeled y = green and y = purple (Fig. 2). The true inverse posteriors p(x|y) consist of the mixture components corresponding to the given label. We observe the following: (i) The INN learns very accurate approximations of the posteriors and does not suffer from mode collapse. (ii) The coupling block architecture does not reduce the network's representational power ­ results are similar to standard networks of comparable size (see appendix Sec. 2). (iii) Bidirectional training works best, whereas forward training alone (using only Ly and Lz) captures the conditional relationships properly, but
5

Under review as a conference paper at ICLR 2019
Ground truth INN, all losses INN, only Ly + Lz INN, only Lx Figure 2: Viability of INN for a basic inverse problem. The task is to produce the correct (multi-modal) distribution of 2D points x, given only the color label y. When trained with all loss terms from Sec. 3.3, the INN output matches ground truth almost exactly (2nd image). The ablations (3rd and 4th image) show that we need Ly and Lz to learn the conditioning correctly, whereas Lx helps us remain faithful to the prior.
places too much mass in unpopulated regions of x-space. Conversely, pure inverse training (just Lx) learns the correct x-distribution, but loses all conditioning information. Inverse kinematics: For a task with a more complex and continuous forward process, we simulate a simple inverse kinematics problem in 2D space: An articulated arm moves vertically along a rail and rotates at three joints. These four degrees of freedom constitute the parameters x. Their priors are given by a normal distribution, which favors a pose with 180 angles and centered origin. The forward process is to find the coordinates of the end point y, given a configuration x. The inverse problem asks for the posterior probability over all possible inputs x that place the arm's end point at a given y position. An example for a fixed y is shown in Fig. 3, where we compare our INN to a conditional variational autoencoder (cVAE), as presented by Sohn et al. (2015). The conceptual difference between our method and the cVAE is highlighted in the appendix, Fig. 7. The chosen y is a difficult example, because it is an unlikely end point position according to the prior p(x), it is outside the prior's 97% quantile (Fig. 7, right), so that p(x|y) considerably deviates from p(x). In this case, due to the computationally cheap forward process, we can use approximate Bayesian computation (ABC, see appendix Sec. 7) to compute the ground truth posterior. Compared to ground truth, we find that both INN and cVAE recover the two symmetric modes well. However, the true end points of x-samples produced by the cVAE tend to miss the target y by a wider margin. This is because the forward process x  y is only learned implicitly in the cVAE training scheme. See appendix for quantitative analysis and details.
Figure 3: Distribution over articulated poses x, conditioned on the end point y. The desired end point y is at the intersection of the gray lines, and we sampled x from the true (ABC) or learned (INN, cVAE) posterior p(x|y). The prior is shown for reference. A dotted line on the left represents the rail the arm is based on, and the faint colored lines indicate sampled arm configurations. Their actual end points deviate slightly from the target y; contour lines enclose the regions containing 97% of these end points. The articulated arm with the highest estimated likelihood is emphasized for illustrative purposes.
4.2 Real-world applications After demonstrating the viability on synthetic data, we apply our method to two real world problems from medicine and astronomy. While we focus on the medical task in the following, the astronomy application is shown in Fig. 5.
6

Under review as a conference paper at ICLR 2019

Table 1: Quantitative results in medical application. We measure the accuracy of

point/MAP estimates as detailed in Sec. 4.2. Best results within measurement error are

bold, and we determine uncertainties (±) by statistical bootstrapping. The parameter

sO2 is the most important in this application, whereas Error all covers all recoverable

parameters, i.e. sO2 , x^ is conditioned on

vhb the

and amie. Re-simulation observation y.

error

measures

how

well

the

MAP

estimate

Method
Classical NN INN INN, only Ly, Lz INN, only Lx cVAE ABC
Simulation noise

Error sO2
0.057 ± 0.003 0.041 ± 0.002
0.066 ± 0.003 0.861 ± 0.033 0.050 ± 0.002 0.036 ± 0.001

Error all
0.56 ± 0.01 0.57 ± 0.02
0.71 ± 0.02 1.70 ± 0.02 0.74 ± 0.02 0.54 ± 0.02

Re-simulation error
0.397 ± 0.008 0.327 ± 0.007
0.506 ± 0.010 2.281 ± 0.045 0.314 ± 0.007 0.284 ± 0.005
0.129 ± 0.001

In medical science, the functional state of biological tissue is of interest for many applications, such as tumor detection. Tumors are expected to show changes in oxygen saturation sO2 (Hanahan and Weinberg (2011)). Such changes influence the reflectance of the tissue, which
can be measured by multispectral cameras (Lu and Fei (2014)). We can simulate these measurements from a tissue model involving sO2 , blood volume fraction vhb, scattering magnitude amie, anisotropy g and tissue layer thickness d (Wirkert et al. (2016)). While these simulations can determine the spectrum (y) for a given tissue, inverting the measurements to
recover the underlying functional properties (x) is an active field of research. State-of-the-art
approaches (Wirkert et al. (2016; 2017); Claridge and Hidovic-Rowe (2013)) learn the inverse
process directly, but cannot express the estimate's uncertainty, which can be vital for an
accurate diagnosis.

Competitors. We train an INN for this problem, along with two ablations (only forward or only inverse training), as well as a cVAE and a network using the method of Kendall and Gal (2017), with dropout sampling and additional aleatoric error terms for each parameter. The latter also provides a point-estimate baseline (classical NN) when used without dropout and error terms, which matches the current state-of-the-art results in Wirkert et al. (2017). Finally, we compare to ABC, approximating p(x|y) with the 256 nearest neighbors to y. Note that with enough samples ABC produces the true posterior. We performed 50 000 simulations to generate samples for ABC at test time, taking one week on a GPU, but still measure inconsistencies in the posteriors. The learning-based methods can use the training set of 15 000 samples generated offline, and can be trained within minutes.

Error measures. We are interested in both the accuracy (point estimates), and the shape of the posterior distributions. For point estimates x^, i.e. MAP estimates, we compute the deviation from ground-truth values x in terms of the RMSE over test set observations y,
RMSE = Ey[ x^ - x 2]. The scores are reported both for the main parameter of interest sO2 , and the parameter subspace of sO2 , vhb, amie, which we found to be the only recoverable parameters. Furthermore, we check the re-simulation error: We apply the simulation s(x) to the point estimate, and compare the simulation outcome to the conditioning y. To evaluate
the shape of the posteriors, we compute the calibration error for the sampling-based methods, based on the fraction of ground truth inliers qinl. for a given confidence interval q. The reported error is the median of |qinl. - q| over all q. All values are computed over 5000 test-set observations y, or 1000 observations in the case of re-simulation error. Each posterior uses
4096 samples, or 256 for ABC, and MAP estimates are found using the mean-shift algorithm.

Quantitative results. Evaluation results for all methods are presented in Table 1. The
INN matches or outperforms other methods in terms of point estimate error. Its accuracy deteriorates slightly when trained without Lx, and entirely when trained without the conditioning losses Ly and Lz, just as in Fig. 2. The calibration error, which summarizes the correctness of the posterior distribution in one number, is as follows for the individual
methods: INN 0.34%; Dropout 1.91%; cVAE 2.19%; ABC 0.90%. The INN has a big lead
over cVAE and Dropout, and even over ABC. See calibration curves in appendix, Fig. 10.

Qualitative results. Fig. 4 shows generated parameter distributions for one fixed measurement y, comparing the INN to cVAE, Dropout sampling and ABC. The three former methods

7

Under review as a conference paper at ICLR 2019

1.00

ABC

0.75

0.50

INN

0.25

0.00

cVAE

-0.25

-0.50

Dropout

-0.75

01
sO2

0.0
vhb

0.1

0 2k
amie

4k 0.6 0.8 1.0
d

0.8 0.9
g

Correlation Matrix

-1.00

Figure 4: Sampled posterior of 5 parameters for fixed y in medical application. For a fixed observation y, we compare the estimated posteriors p(x | y) of different methods. The final row also includes the point estimate (dashed green line). Ground truth values x

(dashed black line) and prior of x over all data (gray area) are provided for reference.

1.0 0.5

0.0

-0.5

Ionizing

Ionizing

Luminosity Emission Rate

Cloud Density

Expansion Velocity

Age of the Cluster

Correlation Matrix

-1.0

Figure 5: Astrophysics application. Properties x of star clusters in interstellar gas clouds are inferred from multispectral measurements y. We train an INN on simulated data, and show the sampled posterior of 5 parameters for one y (colors as in Fig. 4, second row). The peculiar shape of the prior is due to the dynamic nature of the simulations. We find multi-modal posteriors as well as strong correlations, see details in appendix, Sec. 5.

use a sample count of 160 000 to produce smooth curves. Due to the sparse posteriors of 256 samples in the case of ABC, kernel density estimation was applied to its results, with a bandwidth of  = 0.1. The results produced by the INN provide relevant insights: First, we find that the posteriors for layer thickness d and anisotropy g match the shape of their priors, i.e. y holds no information about these parameters ­ they are unrecoverable. This finding is supported by the ABC results, whereas the other two methods misleadingly suggest a roughly Gaussian posterior. Second, we find that the sampled distributions for the blood volume fraction vhb and scattering amplitude amie are strongly correlated (rightmost plot). This phenomenon is not an analysis artifact, but has a sound physical explanation: As blood volume fraction increases, more light is absorbed inside the tissue. For the sensor to record the same intensities y as before, scattering must be increased accordingly. In Fig. 9 in the appendix, we show how the INN is applied to real multispectral images.
5 Conclusion
We have shown that the full posterior of an inverse problem can be estimated with invertible networks, both theoretically and practically on problems from medicine and astrophysics. We share the excitement of the application experts to develop INNs as a generic tool, helping them to better interpret the data and models, and to improve the experimental setups. In future work, we plan to apply our procedure to forward processes that can not be fully encoded in a neural network, by incorporating domain knowledge into the method.

8

Under review as a conference paper at ICLR 2019
References
Jack A. Baldwin, Mark M. Phillips, and Roberto Terlevich. Classification parameters for the emission-line spectra of extragalactic objects. PASP, 93:5­19, February 1981. doi: 10.1086/130766.
Rianne van den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester normalizing flows for variational inference. arXiv:1803.05649, 2018.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859­877, 2017.
Ela Claridge and Dzena Hidovic-Rowe. Model based inversion for deriving maps of histological parameters characteristic of cancer from ex-vivo multispectral images of the colon. IEEE Trans Med Imaging, November 2013.
Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and Peter Dayan. Comparison of maximum likelihood and GAN-based training of Real NVPs. arXiv:1705.05263, 2017.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv:1605.08803, 2016.
Chris Donahue, Akshay Balsubramani, Julian McAuley, and Zachary C Lipton. Semantically decomposing the latent spaces of generative adversarial networks. arXiv:1705.07904, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv:1606.00704, 2016.
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with Bernoulli approximate variational inference. arXiv:1506.02158, 2015.
Dani Gamerman and Hedibert F Lopes. Markov Chain Monte Carlo: Stochastic simulation for Bayesian inference. Chapman and Hall/CRC, 2006.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, pages 881­889, 2015.
Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. In Advances in Neural Information Processing Systems, pages 2211­2221, 2017.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723­773, 2012.
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan: Bridging implicit and prescribed learning in generative models. arXiv:1705.08868, 2017.
Douglas Hanahan and Robert A. Weinberg. Hallmarks of cancer: The next generation. Cell, 144(5):646­674, March 2011. ISSN 00928674. doi: 10.1016/j.cell.2011.02.013. URL http://linkinghub.elsevier.com/retrieve/pii/S0092867411001279.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive flows. arXiv:1804.00779, 2018.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 1125­1134, 2017.
Jörn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-RevNet: Deep invertible networks. arXiv:1802.07088, 2018.
9

Under review as a conference paper at ICLR 2019
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems, pages 5580­5590, 2017.
Lisa J. Kewley, Michael A. Dopita, Claus Leitherer, Romeel Davé, Tiantian Yuan, Mark Allen, Brent Groves, and Ralph Sutherland. Theoretical evolution of optical strong lines across cosmic time. The Astrophysical Journal, 774(2):100, 2013. URL http: //stacks.iop.org/0004-637X/774/i=2/a=100.
Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. arXiv:1807.03039, 2018.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pages 2575­2583, 2015.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pages 4743­4751, 2016.
Alexander Kolesnikov and Christoph H. Lampert. PixelCNN models with auxiliary variables for natural image modeling. In International Conference on Machine Learning, pages 1905­1914, 2017.
Jarno Lintusaari, Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Corander. Fundamentals and recent developments in approximate bayesian computation. Systematic Biology, 66(1):e66­e82, 2017. doi: 10.1093/sysbio/syw077.
Guolan Lu and Baowei Fei. Medical hyperspectral imaging: a review. Journal of Biomedical Optics, 19(1):10901, January 2014. ISSN 1560-2281. doi: 10.1117/1.JBO.19.1.010901.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014.
Aäron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with PixelCNN decoders. In Advances in Neural Information Processing Systems, pages 4797­4805, 2016.
George Papamakarios and Iain Murray. Fast -free inference of simulation models with bayesian conditional density estimation. In Advances in Neural Information Processing Systems, pages 1028­1036, 2016.
George Papamakarios, Iain Murray, and Theo Pavlakou. Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, pages 2335­ 2344, 2017.
Eric W. Pellegrini, Jack A. Baldwin, and Gary J. Ferland. Structure and feedback in 30 Doradus. II. Structure and chemical abundances. The Astrophysical Journal, 738(1):34, 2011. URL http://stacks.iop.org/0004-637X/738/i=1/a=34.
Daniel Rahner, Eric W. Pellegrini, Simon C. O. Glover, and Ralf S. Klessen. Winds and radiation in unison: A new semi-analytic feedback model for cloud dissolution. Monthly Notices of the Royal Astronomical Society, 470:4453­4472, 10 2017.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning, pages 1530­1538, 2015.
Oren Rippel and Ryan Prescott Adams. High-dimensional probability estimation with deep density models. arXiv:1302.5125, 2013.
Christian Robert and George Casella. Monte Carlo Statistical Methods. Springer, 2004.
Ralf S. Klessen and Simon C. O. Glover. Physical processes in the interstellar medium. Saas-Fee Advanced Course, 43:85, 2016.
10

Under review as a conference paper at ICLR 2019
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. arXiv:1701.05517, 2017.
N Siddharth, Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, and Philip HS Torr. Learning disentangled representations with semi-supervised deep generative models. In Advances in Neural Information Processing Systems, pages 5925­5935, 2017.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In Advances in Neural Information Processing Systems, pages 3483­3491, 2015.
Mikael Sunnåker, Alberto Giovanni Busetto, Elina Numminen, Jukka Corander, Matthieu Foll, and Christophe Dessimoz. Approximate bayesian computation. PLoS computational biology, 9(1):e1002803, 2013.
Yunfei Teng, Anna Choromanska, and Mariusz Bojarski. Invertible autoencoder for domain adaptation. arXiv:1802.06869, 2018.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders. arXiv:1711.01558, 2017.
Jakub M Tomczak and Max Welling. Improving variational auto-encoders using householder flow. arXiv:1611.09630, 2016.
Brian L Trippe and Richard E Turner. Conditional density estimation with bayesian normalising flows. arXiv:1802.04908, 2018.
Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural autoregressive distribution estimation. Journal of Machine Learning Research, 17(205): 1­37, 2016.
Richard David Wilkinson. Approximate bayesian computation (abc) gives exact results under the assumption of model error. Statistical applications in genetics and molecular biology, 12(2):129­141, 2013.
Sebastian J Wirkert, Hannes Kenngott, Benjamin Mayer, Patrick Mietkowski, Martin Wagner, Peter Sauer, Neil T Clancy, Daniel S Elson, and Lena Maier-Hein. Robust near real-time estimation of physiological parameters from megapixel multispectral images with inverse monte carlo and random forest regression. International journal of computer assisted radiology and surgery, 11(6):909­917, 2016.
Sebastian J. Wirkert, Anant S. Vemuri, Hannes G. Kenngott, Sara Moccia, Michael Götz, Benjamin F. B. Mayer, Klaus H. Maier-Hein, Daniel S. Elson, and Lena Maier-Hein. Physiological Parameter Estimation from Multispectral Images Unleashed. In Medical Image Computing and Computer-Assisted Intervention ­ MICCAI 2017, Lecture Notes in Computer Science, pages 134­141. Springer, Cham, September 2017. ISBN 9783-319-66178-0 978-3-319-66179-7. doi: 10.1007/978-3-319-66179-7_16. URL https: //link.springer.com/chapter/10.1007/978-3-319-66179-7_16.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In CVPR, pages 2223­2232, 2017a.
Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing Systems, pages 465­476, 2017b.
11

Under review as a conference paper at ICLR 2019

Appendix
Anonymous authors Paper under double-blind review

1 Proof of correctness of generated posteriors

Lemma: If some bijective function f : x  z transforms a probability density pX (x) to pZ (z), then the inverse function f -1 transforms pZ (z) back to pX (x).

Proof: We denote the probability density obtained through the reverse transformation as pX (x). Therefore, we have to show that pX (x) = pX (x). For the forward direction, via the change-of-variables formula, we have

pZ (z) = pX x = f -1(z) det[z(f -1)]

(6)

with the Jacobian zf -1  fi-1/zj. For the reverse transformation, we have pX (x) = pZ z = f (x) |det[xf ]| .

(7)

We can substitute pZ from Eq. 6 and obtain pX (x) = pX x = f -1(f (x)) det (z(f -1))(xf )

(8)

= pX (x) det (zf -1)(xf )

(9)

= pX (x) |det[I]| = pX (x).

(10)

In Eq. 9, the Jacobians cancel out due to the inverse function theorem, i.e. the Jacobian z(f -1) is the matrix inverse of xf .

Theorem: If an INN f (x) = [y, z] is trained as proposed, and both the supervised loss Ly = E[(y-fy(x))2] and the unsupervised loss Lz = D q(y, z), p(y) p(z) reach zero, sampling according to Eq. 1 with g = f -1 returns the true posterior p(x | y^) for any measurement y^.

Proof: We denote the chosen latent distribution as pZ(z), the distribution of observations as pY (y), and the joint distribution of network outputs as q(y, z). As shown by Gretton
et al. (2012), if the MMD loss converges to 0, the network outputs follow the prescribed

distribution:

Lz = 0  q(y, z) = pY (y)pZ (z)

(11)

Suppose we take a posterior conditioned on a fixed y^, i.e. p(x | y^), and transform it using

the forward pass of our perfectly converged INN. From this we obtain an output distribution

q^(y, z). Because Ly = 0, we know that the output distribution of y (marginalized over z) must be q^(y) = (y - y^). Also, because of the independence between z and y in the output,

the distribution of z-outputs is still q^(z) = pZ(z). So the joint distribution of outputs is

q^(y, z) = (y - y^)pZ (z)

(12)

When we invert the network, and repeatedly input y^ while sampling z  pZ(z), this is the same as sampling [y, z] from the q^(y, z) above. Using the Lemma from above, we know that
the inverted network will output samples from p(x | y^).

Corollary: If the conditions of the theorem above are fulfilled, the unsupervised reverse loss Lx = D q(x), pX (x) between the marginalized outputs of the inverted network, q(x), and the prior data distribution, pX (x), will also be 0. This justifies using the loss on the prior to speed up convergence, without altering the final results.

12

Under review as a conference paper at ICLR 2019
Proof: Due to the theorem, the estimated posteriors generated by the INN are correct, i.e. q(x | y^) = p(x | y^). If they are marginalized over observations y^ from the training data, then q(x) will be equal to pX (x) by definition. As shown by Gretton et al. (2012), this is equivalent to Lx = 0.
2 Artificial data ­ Gaussian mixture
In Sec. 4.1, we demonstrate that the proposed INN can approximate the true posteriors very well and is not hindered by the required coupling block architecture. Here we show how some existing methods do on the same task, using neural networks of similar size as the INN.

INN, all losses

cGAN

Larger cGAN Generator + MMD

cVAE

Dropout sampling

Figure 6: Results of several existing methods for the Gaussian mixture toy example.

cGAN Training a conditional GAN of network size comparable to the INN (counting only the generator) and only two noise dimensions turned out to be challenging. Even with additional pre-training to avoid mode collapse, the individual modes belonging to one label are reduced to nearly one-dimensional structures.
Larger cGAN In order to match the results of the INN, we trained a more complex cGAN with 2M parameters instead of the previous 10K, and a latent dimension of 128, instead of 2. To prevent mode collapse, we introduced an additional regularization: an extra loss term forces the variance of generator outputs to match the variance of the training data prior. With these changes, the cGAN can be seen to recover the posteriors reasonably well.
Generator + MMD Another option is to keep the cGAN generator the same size as our INN, but replace the discriminator with an MMD loss (cf. Sec. 3.4). This loss receives a concatenation of the generator output x and the label y it was supplied with, and compares these batch-wise with the concatenation of ground truth (x, y)-pairs. Note that in contrast to this, the corresponding MMD loss of the INN only receives x, and no information about y. For this small toy problem, we find that the hand-crafted MMD loss dramatically improves results compared to a learned discriminator.
cVAE We also compare with a conditional Variational Autoencoder of the same size. There is some similarity between the training setup of our method (Fig. 7, right) and that of a cVAE (Fig. 7, left), as the forward and inverse pass of an INN can also be seen as an encoder-decoder pair. The main differences are that the cVAE learns the relationship x  y only indirectly, since there is no explicit loss for it, and that the INN requires no reconstruction loss, since it is bijective by construction. On the Gaussian mixture toy problem, the trained cVAE generates correct posteriors for the uni-modal labels, but introduces too much variance in the multi-modal cases.
Dropout sampling The method of dropout sampling with learned error terms is by construction not able to produce multi-modal outputs, and therefore fails on this task.
2.1 Latent space analysis
To analyze how the latent space is structured for this task, we choose a fixed label y and sample z from a dense grid. For each z, we compute x through our inverse network and colorize this point in latent (z) space according to the distance from the closest mode in

13

Under review as a conference paper at ICLR 2019

reconstruction loss ELBO loss USL
USL SL

[x, y]  µz , z
Encoder
xy

z

forward (simulation): x  y
y x INN
z

Decoder
[z, y]  x

inverse (sampling): [y, z]  x

Conditional Variational Autoencoder

Invertible Neural Network

Figure 7: Abstraction of the cVAE training scheme compared to our INN from Fig. 1.

x-space. We can see that our network learns to shape the latent space such that each mode receives the expected fraction of samples (Fig. 8).

Figure 8: Layout of latent space for one fixed label y, colored by mode closest to x = g(y, z). For each latent position z, the hue encodes which mode the corresponding x belongs to and the luminosity encodes how close x is to this mode. Note that colors used here do not relate to those in Fig. 2, and encode the position x instead of the label y. The
first three columns correspond to labels green, blue and red Fig. 2. White circles mark areas that contain 50% and 90% of the probability mass of latent prior p(z).

3 Artificial data ­ inverse kinematics

A short video demonstrating the structure of our INN's latent space can be found under https://gfycat.com/SoggyCleanHog, for a slightly different arm setup.

To judge the quality of posteriors, we quantify both the re-simulation error, as in Sec. 4.2

of the paper, as well as the reduced r2ed. value of sampled normalized expected log-likelihood of the generated samples. as 2red. = 2/, with  denoting the degrees of freedom).

x, which corresponds to the (The reduced 2red. is defined

r2ed.

=

1 4 Ey Exp(x|y)

xi - µi i

(13)

with i  {1, 2, 3, 4} for each x-dimension, and i, µi of the prior. With the expectation over y from the test set, the ground truth value of r2ed. is 1.

We find that the re-simulation error of the INN is dramatically lower than that of the cVAE, with slightly better quality of the prior, confirming the observations in Fig. 3.

Table 2: Quantitative evaluation of the inverse kinematics experiment

Method
Ground truth INN cVAE

Re-simulation error

0.0000 0.0049 0.0142

± 0.0001 ± 0.0001

2red.
1.000 0.834 0.763

± 0.011 ± 0.007

14

Under review as a conference paper at ICLR 2019

4 Multispectral measurements of biological tissue
The following figure shows the results when the INN trained in Sec. 4.2 is applied pixel-wise to multispectral endoscopic footage. In addition to estimating the oxygenation sO2 , we measure the uncertainty in the form of the 68% confidence interval.

0.70

0.13

0.65 0.60

0.12 0.11 0.10

0.55

0.09

0.50 0.45

0.08 0.07 0.06

c) RGB image

a) Median sO2

0.40
b) Est. uncertainty

0.05

Figure 9: INN applied to real footage to predict oxygenation sO2 and uncertainty. The clips (arrows) on the connecting tissue cause lower oxygenation (blue) in the small

intestine. Uncertainty is low in crucial areas and high only at some edges and specularities.

5 Star cluster spectral data
Star clusters are born from a large reservoir of gas and dust that permeates the galaxy, the interstellar medium. The process is governed by the complex interplay of competing physical agents such as gravity, turbulence, magnetic fields, and radiation; with stellar feedback playing a decisive regulatory role S. Klessen and C. O. Glover (2016). To study this, astronomers measure emission lines from chemical elements such as hydrogen or oxygen Baldwin et al. (1981); Kewley et al. (2013). Their relative intensities depend on the ionization potential, the spectrum of the ionizing radiation, the gas density, and the absolute intensity of the radiation Pellegrini et al. (2011). We investigate the dynamical feedback of young star clusters on their parental environment in a 1D model Rahner et al. (2017) which describes the complete temporal evolution of the system and allows us to generate synthetic emission line maps. Similar to the medical application, the reverse of this mapping is highly degenerate and ill-posed. We train our forward model to predict the observable quantities y (e.g. emission line ratios) from composite simulation outputs x (e.g. ionizing luminosity and emission rate, cloud density, expansion velocity, and age of the youngest cluster in the system). Fig. 5 shows the resulting marginal posteriors for these five parameters given one specific y.
Note that our network recovers a decidedly multimodal distribution of x that visibly deviates from the prior p(x). Note also the strong correlations in the system. For example, the measurements y investigated may correspond to a young cluster with large expansion velocity, or to an older system that expands slowly. Finding these ambiguities in p(x|y) and identifying degeneracies in the underlying model are pivotal aspects of astrophysical research, and a method to effectively approximate full posterior distributions has the potential to lead to a major breakthrough in this field.

6 Calibration curve for tissue parameter estimation
In Sec. 4.2, we report the median calibration error for each method. The following figure plots the calibration error, qinliers - q, against the level of confidence q. Negative values mean that a model is overconfident, while positive values say the opposite.

15

Under review as a conference paper at ICLR 2019

Calibration error qInliers - q

0.06 ABC INN
0.04
cVAE 0.02 MC Dropout
0.00
-0.02
-0.04
-0.06 0.0 0.2 0.4 0.6 0.8 1.0
Confidence q Figure 10: Calibration curves for all four methods compared in Sec. 4.2.

7 Approximate Bayesian computation (ABC)
While there is a whole field of research concerned with ABC approaches and their efficiencyaccuracy tradeoffs, our use of the method here is limited to the essential principle of rejection sampling. When we require N samples of x from the posterior p(x | y) conditioned on some y, there are two basic ways to obtain them:
Threshold: We set an acceptance threshold , repeatedly draw x-samples from the prior, compute the corresponding y-values (via simulation) and keep those where dist(y, y) < , until we have accepted N samples. The smaller we want , the more simulations have to be run, which is why we use this approach only for the experiment in Sec. 4.1, where we can afford to run the forward process millions or even billions of times.
Quantile: Alternatively, we choose what quantile q of samples shall be accepted, and then run exactly N/q simulations. All sampled pairs (x, y) are sorted by dist(y, y) and the N closest to y form the posterior. This allows for a more predictable runtime when the simulations are costly, as in the medical application in Sec. 4.2.

8 Details of datasets and network architectures
Table 3 summarizes the datasets used throughout the paper. The architecture details are given in the following.
Table 3: Dimensionalities and training set sizes for each experiment.

Section
4.1 4.2 4.3 4.4

training data
106 106 15 000 8 772

dim(x)
2 4 13 19

dim(y)
8 2 8 69

dim(z)
2 2 13 17

see also
Wirkert et al. (2016) Pellegrini et al. (2011)

8.1 Artificial data ­ Gaussian mixture
INN: 3 invertible blocks, 3 fully connected layers per affine coefficient function with ReLU activation functions in the intermediate layers, zero padding to a nominal dimension of 16, Adam optimizer, decaying learning rate from 10-3 to 10-5, batch size 200. The inverse multiquadratic kernel was used for MMD, with  = 0.2 in both x- and z-space.
16

Under review as a conference paper at ICLR 2019

Dropout sampling: 6 fully connected layers with ReLU activations, Adam optimizer, learning rate decay from 10-3 to 10-5, batch size 200, dropout probability p = 0.2.

cGAN: 6 fully connected layers for the generator and 8 for the discriminator, all with leaky

ReLU activations. Adam was used for the generator, SGD for the discriminator, learning

rates decaying from 2 · 10-3 to 2 · 10-6, batch size 256. Initially 100 iterations training with

L

=

1 N

i

g(zi, yi) - xi

2 2

,

to

separate

the

differently

labeled

modes,

followed

by

pure

GAN

training.

Larger cGAN: 2 fully connected layers with 1024 neurons each for discriminator and generator, batch size 512, Adam optimizer with learning rate 8 · 10-4 for the generator, SGD with learning rate 1.2 · 10-3 and momentum 0.05 for the discriminator, 1.6 · 10-3 weight decay for both, 0.25 dropout probabiliy for the generator at training and test time. Equal weighting of discriminator loss and penalty of output variance L = (Vari[g(zi, yi)] - Vari[xi])2
Generator with MMD: 8 fully connected layers with leaky ReLU activations, Adam optimizer, decaying learning rate from 10-3 to 10-6, batch size 256. Inverse multiquadratic kernel,  = 0.5.

cVAE: 3 fully connected layers each for encoder and decoder, ReLU activations, learning rate 2 · 10-2, decay to 2.5 · 10-5, Adam optimizer, batch size 25, reconstruction loss weighted
50:1 versus KL divergence loss.

8.2 Artificial data ­ inverse kinematics
INN: 6 affine coupling blocks with 3 fully connected layers each and leaky ReLU activations. Adam optimizer, decaying learning rate from 10-2 to 10-4, multiquadratic kernel with  = 1.2.
cVAE: 4 fully connected layers each for encoder and decoder, ReLU activations, learning rate 5 · 10-3, decay to 1.6 · 10-5, Adam optimizer, batch size 250, reconstruction loss weighted 15:1 versus KL divergence loss.

8.3 Functional parameter estimation from multispectral tissue images
INN: 3 invertible blocks, 4 fully connected layers per affine coefficient function with leaky ReLUs in the intermediate layers, zero padding to double the original width. Adam optimizer, learning rate decay from 2 · 10-3 to 2 · 10-5, batch size 200. Inverse multiquadratic kernel with  = 1, weighted MMD terms by observation distance with decaying  = 0.2 to 0.
Dropout sampling/point estimate: 8 fully connected layers, ReLU activations, Adam with decaying learning rate from 10-2 to 10-5, batch size 100, dropout probability p = 0.2.
cVAE: 4 fully connected layers each for encoder and decoder, ReLU activations, learning rate 10-3, decay to 3.2 · 10-6, Adam optimizer, batch size 25, reconstruction loss weighted 103:1 versus KL divergence loss.

8.4 Impact of star clusters on the dynamical evolution of the galactic gas
INN: 5 invertible blocks, 4 fully connected layers per affine coefficient function with leaky ReLUs in the intermediate layers, no additional zero padding. Adam optimizer with decaying learning rate from 2 · 10-3 to 1.5 · 10-6, batch size 500. Kernel for latent space: k(z, z ) = exp(- z - z 2) with  = 0.02. Kernel for x-space: k(x, x ) = - x - x 11//42. Due to the complex nature of the prior distributions, this was the kernel found to capture the details correctly, whereas the peak of the inverse multiquadratic kernel was too broad for this purpose.

17

