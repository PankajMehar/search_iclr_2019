Under review as a conference paper at ICLR 2019
CONFIDENCE CALIBRATION IN DEEP NEURAL NETWORKS THROUGH STOCHASTIC INFERENCES
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a generic framework to calibrate accuracy and confidence (score) of a prediction through stochastic inferences in deep neural networks. We first analyze relation between variation of multiple model parameters for a single example inference and variance of the corresponding prediction scores by Bayesian modeling of stochastic regularization. Our empirical observation shows that accuracy and score of a prediction are highly correlated with variance of multiple stochastic inferences given by stochastic depth or dropout. Motivated by these facts, we design a novel variance-weighted confidence-integrated loss function that is composed of two cross-entropy loss terms with respect to ground-truth and uniform distribution, which are balanced by variance of stochastic prediction scores. The proposed loss function enables us to learn deep neural networks that predict confidence calibrated scores using a single inference. Our algorithm presents outstanding confidence calibration performance and improves classification accuracy with two popular stochastic regularization techniques--stochastic depth and dropout-- in multiple models and datasets; it alleviates overconfidence issue in deep neural networks significantly by training networks to achieve prediction accuracy proportional to confidence of prediction.
1 INTRODUCTION
Deep neural networks have achieved remarkable performance in various tasks, but have critical limitations in reliability of their predictions. One example is that inference results are often overly confident even for unseen or ambiguous examples. Since many practical applications including medical diagnosis, autonomous driving, and machine inspection require accurate uncertainty estimation as well as high prediction accuracy for each inference, such an overconfidence issue makes deep neural networks inappropriate to be deployed for real-world problems in spite of their impressive accuracy.
Regularization is a common technique in training deep neural networks to avoid overfitting problems and improve generalization performance (Srivastava et al., 2014; Huang et al., 2016; Ioffe & Szegedy, 2015). Although regularization is effective to learn robust models, its objective is not directly related to generating score distributions aligned with uncertainty of predictions. Hence, existing deep neural networks are often poor at calibrating prediction accuracy and confidence.
Our goal is to learn deep neural networks that are able to estimate uncertainty of each prediction while maintaining accuracy. In other words, we propose a generic framework to calibrate prediction score (confidence) with accuracy in deep neural networks. Our algorithm starts with an observation that variance of prediction scores measured from multiple stochastic inferences is highly correlated with accuracy and confidence of the prediction based on the average score. Based on its Bayesian interpretation, we employ stochastic regularization such as stochastic depth or dropout to obtain multiple stochastic inference results. By exploiting the empirical observation with the theoretical interpretation, we design a novel loss function to enable deep neural network to predict confidencecalibrated scores based only on a single prediction, without multiple stochastic inferences. Our contribution is summarized below:
· We provide a generic framework to estimate uncertainty of a prediction based on stochastic inferences in deep neural networks, which is supported by empirical observations and theoretical analysis.
1

Under review as a conference paper at ICLR 2019
· We propose a novel variance-weighted confidence-integrated loss function in a principled way, which enables deep neural networks to produce confidence-calibrated predictions even without performing stochastic inferences and introducing hyper-parameters.
· The proposed framework presents outstanding performance to reduce overconfidence issue and estimate accurate uncertainty in various combinations of network architectures and datasets.
The rest of the paper is organized as follows. We first discuss prior research related to our algorithm, and describe theoretical background for Bayesian interpretation of our approach in Section 2 and 3, respectively. Section 4 presents our confidence calibration algorithm through stochastic inferences, and Section 5 illustrates experimental results.
2 RELATED WORK
Uncertainty estimation is a critical problem in deep neural networks and receives growing attention from machine learning community. Bayesian approach is a common tool to provide a mathematical framework for uncertainty estimation in deep neural networks. However, exact Bayesian inference is not tractable in deep neural networks due to its high computational cost, and various approximate inference techniques--MCMC (Neal, 1996), Laplace approximation (MacKay, 1992) and variational inference (Barber & Bishop, 1998; Graves, 2011; Hoffman et al., 2013)­have been proposed. Recently, Bayesian interpretation of multiplicative noise is employed to estimate uncertainty in deep neural networks (Gal & Ghahramani, 2016; McClure & Kriegeskorte, 2016). There are several approaches outside Bayesian modeling, which include post-processing (Niculescu-Mizil & Caruana, 2005; Platt, 2000; Zadrozny & Elkan, 2001; Guo et al., 2017) and deep ensembles (Lakshminarayanan et al., 2017). All the post-processing methods require a hold-out validation set to adjust prediction scores after training, and the ensemble-based technique employs multiple models to estimate uncertainty.
Stochastic regularization is a common technique to improve generalization performance by injecting random noise to deep neural networks. The most notable method is Srivastava et al. (2014), which randomly drops their hidden units by multiplying Bernoulli random noise. There exist several variants, for example, dropping weights (Wan et al., 2013) or skipping layers (Huang et al., 2016). Most stochastic regularization methods exploit stochastic inferences during training, but perform deterministic inferences using the whole network during testing. On the contrary, we also use stochastic inferences to obtain diverse and reliable outputs during testing.
Although the following works do not address uncertainty estimation, their main idea is related to our objective more or less. Label smoothing (Szegedy et al., 2016) encourages models to be less confident, by preventing a network from assigning the full probability to a single class. A similar loss function is discussed to train confidence-calibrated classifiers in Lee et al. (2018), but it focuses on how to discriminate in-distribution and out-of-distribution examples, rather than estimating uncertainty or alleviating miscalibration of in-distribution examples. On the other hand, Pereyra et al. (2017) claims that blind label smoothing and penalizing entropy enhances accuracy by integrating loss functions with the same concept with Szegedy et al. (2016); Lee et al. (2018), but improvement is marginal in practice.
3 BAYESIAN INTERPRETATION OF STOCHASTIC REGULARIZATION
This section describes Bayesian interpretation of stochastic regularization in deep neural networks, and discusses relation between stochastic regularization and uncertainty modeling.
3.1 STOCHASTIC METHODS FOR REGULARIZATIONS
One popular class of regularization techniques is stochastic regularization, which introduces random noise to a network for perturbing its inputs or weights. We focus on the multiplicative binary noise injection, where random binary noise is applied to the inputs or weights by elementwise multiplication since such stochastic regularization techniques are widely used (Srivastava et al., 2014; Wan
2

Under review as a conference paper at ICLR 2019

et al., 2013; Huang et al., 2016). Note that input perturbation can be reformulated as weight perturbation. For example, dropout--binary noise injection to activations--is intertpretable as weight perturbation that masks out all the weights associated with the dropped inputs. Therefore, if a classification network modeling p(y|x, ) with parameters  is trained with stochastic regularization methods by minimizing the cross entropy, the loss function can be defined by

LSR()

=

-

1 N

N

log p (yi|xi, ^i)

i=1

(1)

where ^i =  i is a set of perturbed parameters by elementwise multiplication with random noise sample i  p( ), and (xi, yi)  D is a pair of input and output in training dataset D.

At inference time, the network is parameterized by the expectation of the perturbed parameters,  = E[] =  E[ ], to predict an output y^, i.e.,

y^ = arg max p (y|x, ) .
y

(2)

3.2 BAYESIAN MODELING
Given the dataset D with N examples, Bayesian objective is to estimate the posterior distribution of the model parameter, denoted by p(|D), to predict a label y for an input x, which is given by

p(y|x, D) = p(y|x, )p(|D)d.


(3)

A common technique for the posterior estimation is variational approximation, which introduces
an approximate distribution q() and minimizes Kullback-Leibler (KL) divergence with the true posterior DKL(q()||p(|D)) as follows:

N

LVA() = -

q() log p(yi|xi, )d + DKL(q()||p()).

i=1 

(4)

The intractable integral and summation over the entire dataset in Equation 4 is approximated by Monte Carlo method and mini-batch optimization resulting in

L^VA()

=

-

N MS

M

S
log p (yi|xi, ^i,j) + DKL (q()||p()) ,

i=1 j=1

(5)

where ^i,j  q() is a sample from the approximate distribution, S is the number of samples, and M is the size of a mini-batch. Note that the first term is data likelihood and the second term is
divergence of the approximate distribution with respect to the prior distribution.

3.3 INTERPRETING STOCHASTIC REGULARIZATIONS AS BAYESIAN MODEL

Suppose that we train a classifier with 2 regularization by a stochastic gradient descent method. Then, the loss function in Equation 1 is rewritten as

L^SR()

=

-

1 M

M

log p (yi|xi, ^i) + ||||22,

i=1

(6)

where 2 regularization is applied to the deterministic parameters  with weight . Optimizing this loss function is equivalent to optimizing Equation 5 if there exists a proper prior p() and q() is approximated as a Gaussian mixture distribution (Gal & Ghahramani, 2016). Note that Gal & Ghahramani (2016) cast dropout training as an approximate Bayesian inference. Thus, we can interpret training with stochastic depth (Huang et al., 2016) within the same framework by simple modification. (See Appendix A and B for details.) Then, the predictive distribution of a model trained with stochastic regularization is approximately given by

p^(y|x, D) = p(y|x, )q()d.


(7)

3

Under review as a conference paper at ICLR 2019

Accuracy (%)

100 100 100 100 100

Sample Coverage (%) Accuracy (%)

Sample Coverage (%) Average Score (%)

80 80 80 80 80

60 60 60 60 60

40 40 40 40 40

20

Accuracy

20

Sample Coverage

0 0.05Norm0a.l1ized V0a.1r5iance0.2 0

20 Average Score 20 Sample Coverage
0 0.05Norm0a.l1ized V0a.1r5iance0.2 0

20
0 30.0 50S.0core (%70).0 90.0

(a) Prediction uncertainty characteristics with stochastic depth in ResNet-34
100 100 100 100 100

Sample Coverage (%) Accuracy (%)

Sample Coverage (%) Average Score (%)

80 80 80 80 80

60 60 60 60 60

40 40 40 40 40

20

Accuracy

20

Sample Coverage

0 0.05Norm0.a1lized0.V15arian0c.e2 0.25 0

20 Average Score 20 Sample Coverage
0 0.05Norm0.a1lized0.V15arian0c.e2 0.25 0

20
0 30.0 50S.0core (%70).0 90.0

(b) Prediction uncertainty characteristics with dropout in VGGNet with 16 layers

Accuracy (%)

Figure 1: Uncertainty observed from multiple stochastic inferences with two stochastic regularization methods, (a) stochastic depth and (b) dropout. We present (left, middle) tendency of accuracy and score of the average prediction with respect to normalized variance of stochastic inferences and (right) relation between score and accuracy. In regularization methods, average accuracy and score drop gradually as normalized variance increases. The red lines indicate coverage (cumulative ratio) of examples. We present results from CIFAR-100.

Following Gal & Ghahramani (2016) and Teye et al. (2018), we estimate the predictive mean and uncertainty by Monte Carlo approximation by drawing parameter samples {^i}iT=1 as

Ep^[y

=

c]



1 T

T

p^(y = c|x, ^i)

and

Covp^[y]  Ep^[yy ] - Ep^[y]Ep^[y] ,

i=1

(8)

where y = (y1, . . . , yC) denotes a score vector of C class labels. Equation 8 means that the average prediction and its variance can be computed directly from multiple stochastic inferences.

4 CONFIDENCE CALIBRATION THROUGH STOCHASTIC INFERENCE
We present a novel confidence calibration technique for prediction in deep neural networks, which is given by a variance-weighted confidence-integrated loss function. We present our observation that variance of multiple stochastic inferences is closely related to accuracy and confidence of predictions, and provide an end-to-end training framework for confidence self-calibration. Then, prediction accuracy and uncertainty are directly accessible from the predicted scores obtained from a single forward pass. This section presents our observation from stochastic inferences and technical details about our confidence calibration technique.
4.1 EMPIRICAL OBSERVATIONS
Equation 8 suggests that variation of models provides variance of multiple stochastic predictions for a single example. Figure 1 presents how variance of multiple stochastic inferences given by stochastic depth or dropout is related to accuracy and confidence of the corresponding average prediction, where the confidence is measured by the maximum score of the average prediction. In the figure, accuracy and score of each bin are computed with the examples belonging to the corresponding bin of the normalized variance. We present results from CIFAR-100 with ResNet-34 and VGGNet with 16 layers. The histograms illustrate the strong correlation between the predicted variance and the reliability--accuracy and confidence--of a prediction; we can estimate accuracy and uncertainty of an example effectively based on its prediction variances given by multiple stochastic inferences.

4

Under review as a conference paper at ICLR 2019

4.2 VARIANCE-WEIGHTED CONFIDENCE-INTEGRATED LOSS

The strong correlation of accuracy and confidence with predicted variance observed in Figure 1 shows great potential to make confidence-calibrated prediction by stochastic inferences. However, variance computation involves multiple stochastic inferences by executing multiple forward passes. Note that this property incurs additional computational cost and may produce inconsistent results.

To overcome these limitations, we propose a generic framework for training accuracy-score cali-
brated networks whose prediction score from a single forward pass directly provides confidence of
the prediction. This objective is achieved by designing a loss function, which augments a confidence-
calibration loss to the standard cross-entropy loss, while the two terms are balanced by the vari-
ance measured by multiple stochastic inferences. Specifically, our variance-weighted confidenceintegrated loss LVWCI(·) for the whole training data (xi, yi)  D is defined by a linear interpolation of the standard cross-entropy loss with ground-truth LGT(·) and the cross-entropy with the uniform distribution LU(·), which is formally given by

N
LVWCI() = (1 - i)LG(i)T() + iLU(i)()
i=1

1N =
T

T
-(1 - i) log p(yi|xi, ^i,j) + iDKL(U (y)||p(y|xi, ^i,j)) + i

i=1 j=1

(9)

where i  [0, 1] is a normalized variance, ^i,j(=  i,j) is a sampled model parameter with binary noise for stochastic prediction, T is the number of stochastic inferences, and i is a constant.

The two terms in our variance-weighted confidence-integrated loss pushes the network toward opposite directions; the first term encourages the network to fit the ground truth label while the second term forces the network to make a prediction close to the uniform distribution. These terms are linearly interpolated by an instance-specific balancing coefficient i, which is given by normalizing the prediction variance of an example obtained from multiple stochastic inferences. Note that the normalized variance i is distinct for each training example and is used to measure model uncertainty. Therefore, optimizing our loss function produces gradient signals, forcing the prediction toward the uniform distribution for the examples with high uncertainty derived by high variance while intensifying prediction confidence of the examples with low variance.

By training deep neural networks using the proposed loss function, we can estimate uncertainty of each testing example with a single forward pass. Unlike the ordinary models, a prediction score of our model is well-calibrated and represents confidence of the prediction, which means that we can rely more on the predictions with high scores.

4.3 CONFIDENCE-INTEGRATED LOSS

Our claim is that an adaptive combination of cross-entropy losses with respect to ground-truth and uniform distribution is a reasonable method to learn uncertainty. As a special case of our varianceweighted confidence-integrated loss, we also present a blind version of the combination, which can be used as a baseline uncertainty estimation technique. This baseline loss function is referred to as confidence-integrated loss, which is given by

LCI() = LGT() + LU()

N
= - log p(yi|xi, ) + DKL(U (y)||p(y|xi, )) + ,
i=1

(10)

where p(y|xi, ) is the predicted distribution with model parameter  and  is a constant. The main idea of this loss function is to regularize with the uniform distribution by expecting the score distributions of uncertain examples to be flattened first while the distributions of confident ones remain intact, where the impact of the confidence-integrated loss term is controlled by a global hyper-parameter .

The proposed loss function is also employed in Pereyra et al. (2017) to regularize deep neural networks and improve classification accuracy. However, Pereyra et al. (2017) does not discuss confidence calibration issues while presenting marginal accuracy improvement. On the other hand, Lee

5

Under review as a conference paper at ICLR 2019

et al. (2018) discusses a similar loss function but focuses on differentiating between in-distribution and out-of-distribution examples by measuring loss of each example based only on one of the two loss terms depending on its origin.
Contrary to the existing approaches, we employ the loss function in Equation 10 to estimate prediction confidence in deep neural networks. Although the confidence-integrated loss makes sense intuitively, blind selection of a hyper-parameter  limits its generality compared to our varianceweighted confidence-integrated loss.

4.4 RELATION TO OTHER CALIBRATION APPROACHES

There are several score calibration techniques (Guo et al., 2017; Zadrozny & Elkan, 2002; Naeini
et al., 2015; Niculescu-Mizil & Caruana, 2005) by adjusting confidence scores through post-
processing, among which Guo et al. (2017) proposes a method to calibrate confidence of predictions by scaling logits of a network using a global temperature  . The scaling is performed before applying the softmax function, and  is trained with validation dataset. As discussed in Guo et al. (2017), this simple technique is equivalent to maximize entropy of the output distribution p(yi|xi). It is also identical to minimize KL-divergence DKL(p(yi|xi)||U (y)) because

DKL(p(yi|xi)||U (y)) = p(yic|xi) log p(yic|xi) - p(yic|xi) log U (yc) = -H(p(yi|xi)) + c (11)
cC

where c is a constant. We can formulate another confidence-integrated loss with the entropy as

N
LCI() = - log p(yi|xi, ) - H(p(yi|xi, )),
i=1

(12)

where  is a constant. Equation 12 suggests that temperature scaling in Guo et al. (2017) is closely related to our framework.

5 EXPERIMENTS
5.1 EXPERIMENTAL SETTING AND IMPLEMENTATION DETAILS
We choose four most widely used deep neural network architectures to test our framework: ResNet (He et al., 2016), VGGNet (Simonyan & Zisserman, 2015), WideResNet (Zagoruyko & Komodakis, 2016) and DenseNet (Huang et al., 2017). We employ stochastic depth in ResNet as proposed in Huang et al. (2016) while employing dropouts (Srivastava et al., 2014) before every fc layer except for the classification layer in other architectures. Note that, as discussed in Section 3.3, both stochastic depth and dropout inject multiplicative binary noise to within-layer activations or residual blocks, they are equivalent to noise injection into network weights. Hence, training with 2 regularization term enables us to interpret stochastic depth and dropout by Bayesian models.
We evaluate the proposed framework on two benchmarks, Tiny ImageNet and CIFAR-100. Tiny ImageNet contains 64 × 64 images with 200 object classes whereas CIFAR-100 has 32 × 32 images of 100 object kinds. There are 500 training images per class in both datasets. For testing, we use the validation set of Tiny ImageNet and the test set of CIFAR-100, which contain 50 and 100 images per class, respectively. To test the two benchmarks with the same architecture, we resize images in Tiny ImageNet to 32 × 32.
All networks are trained with stochastic gradient decent with the momentum of 0.9 for 300 epochs. We set the initial learning rate to 0.1 and exponentially decay it with factor of 0.2 at epoch 60, 120, 160, 200 and 250. Each batch consists of 64 training examples for ResNet, WideResNet and DenseNet and 256 for VGG architectures. To train networks with the proposed variance-weighted confidence-integrated loss, we draw T samples of network parameters i for each input image, and compute the normalized variance  by running T forward passes. The number of samples T is set to 5. The normalized variance is estimated as Bhattacharyya coefficients between individual predictions and the average prediction.

6

Under review as a conference paper at ICLR 2019

Table 1: Classification accuracy and calibration scores for several combinations of network architectures and datasets. We compare models trained with baseline, CI and VWCI losses. Since CI loss involves a hyper-parameter , we present mean and standard deviation of results from models with five different 's. In addition, we also show results from the oracle CI loss, CI[Oracle], which are the most optimistic values out of results from all 's in individual columns. Note that the numbers corresponding to CI[Oracle] may come from different 's. Refer to Appendix C for the full results.

Dataset

Architecture ResNet-34

Tiny ImageNet

VGG-16

WideResNet-16-8

DenseNet-40-12

ResNet-34

CIFAR-100

VGG-16

WideResNet-16-8

DenseNet-40-12

Method
Baseline CI
VWCI CI[Oracle]
Baseline CI
VWCI CI[Oracle]
Baseline CI
VWCI CI[Oracle]
Baseline CI
VWCI CI[Oracle]
Baseline CI
VWCI CI[Oracle]
Baseline CI
VWCI CI[Oracle]
Baseline CI
VWCI CI[Oracle]
Baseline CI
VWCI CI[Oracle]

Accuracy[%]
50.82 50.09 ± 1.08
52.80 51.45
46.58 46.82 ± 0.81
48.03 47.39
55.92 55.80 ± 0.44
56.66 56.38
42.50 40.18 ± 1.68
43.25 41.21
77.19 77.56 ± 0.60
78.64 78.54
73.78 73.75 ± 0.35
73.87 73.78
77.52 77.35 ± 0.21
77.74 77.53
65.91 64.72 ± 1.46
67.45 66.20

ECE
0.067 0.134 ± 0.079
0.027 0.035
0.346 0.226 ± 0.095
0.053 0.122
0.132 0.115 ± 0.040
0.046 0.050
0.020 0.059 ± 0.061
0.025 0.025
0.109 0.134 ± 0.131
0.034 0.029
0.187 0.183 ± 0.079
0.098 0.083
0.103 0.133 ± 0.091
0.038 0.074
0.074 0.070 ± 0.040
0.026 0.019

MCE
0.147 0.257 ± 0.098
0.076 0.171
0.595 0.435 ± 0.107
0.142 0.320
0.237 0.288 ± 0.100
0.136 0.208
0.154 0.152 ± 0.082
0.089 0.094
0.304 0.251 ± 0.128
0.089 0.087
0.486 0.489 ± 0.214
0.309 0.285
0.278 0.297 ± 0.108
0.101 0.211
0.134 0.138 ± 0.055
0.094 0.053

NLL
2.050 2.270 ± 0.212
1.949 2.030
4.220 3.224 ± 0.468
2.373 2.812
1.974 1.980 ± 0.114
1.866 1.851
2.423 2.606 ± 0.208
2.410 2.489
1.020 1.064 ± 0.217
0.908 0.921
1.667 1.526 ± 0.175
1.277 1.289
0.984 1.062 ± 0.180
0.891 0.931
1.238 1.312 ± 0.125
1.161 1.206

Brier Score
0.628 0.665 ± 0.037
0.605 0.620
0.844 0.761 ± 0.054
0.659 0.701
0.593 0.594 ± 0.017
0.569 0.572
0.716 0.748 ± 0.035
0.712 0.726
0.345 0.360 ± 0.057
0.310 0.321
0.437 0.436 ± 0.034
0.391 0.396
0.336 0.356 ± 0.044
0.314 0.327
0.463 0.482 ± 0.028
0.439 0.456

5.2 EVALUATION METRIC

We measure classification accuracy and calibration scores--expected calibration error (ECE), maxi-

mum calibration error (MCE), negative log likelihood (NLL) and brier score--of the trained models.

Let Bm be a set of indices of test examples whose scores for the ground-truth labels fall into interval

(

m-1 M

,

m M

],

where

M

is

the

number

of

bins

(M

=

20).

ECE

and

MCE

are

formally

defined

by

ECE =

M

|Bm| N

|acc(Bm)

-

conf (Bm )|

and

MCE = max |acc(Bm) - conf(Bm)|
m{1,...,M }

m=1

where N is the number of the test samples. Also, accuracy and confidence of each bin are given by

acc(Bm)

=

1 |Bm|

1(y^i
iBm

=

yi)

and

1 conf(Bm) = |Bm| iBm pi

where 1 is an indicator function, y^i and yi are predicted and true label of the i-th example and pi is
its predicted confidence. NLL and Brier score are another metrics for calibration and are defined as

N NC

NLL = - log p(yi|xi, ) and Brier =

(p(y^i = j|xi, ) - 1(yi = j))2

i=1 i=1 j=1

where C is the number of classes. We note that low values for all these calibration scores means that the network is well-calibrated.

7

Under review as a conference paper at ICLR 2019

Table 2: Comparison between VWCI and TS of multiple datasets and architectures.

Dataset Tiny ImageNet
CIFAR-100

Architecture ResNet-34 VGG-16 WideResNet-16-8 DenseNet-40-12 ResNet-34 VGG-16 WideResNet-16-8 DenseNet-40-12

Method
TS (case 1) TS (case 2)
VWCI
TS (case 1) TS (case 2)
VWCI
TS (case 1) TS (case 2)
VWCI
TS (case 1) TS (case 2)
VWCI
TS (case 1) TS (case 2)
VWCI
TS (case 1) TS (case 2)
VWCI
TS (case 1) TS (case 2)
VWCI
TS (case 1) TS (case 2)
VWCI

Accuracy[%]
50.82 47.20 52.80
46.58 46.53 48.03
55.92 53.95 56.66
42.50 41.63 43.25
77.67 77.40 78.64
73.66 72.69 73.87
77.52 76.42 77.74
65.91 64.96 67.45

ECE
0.162 0.021 0.027
0.358 0.028 0.053
0.200 0.027 0.046
0.037 0.024 0.025
0.133 0.036 0.034
0.197 0.031 0.098
0.144 0.028 0.038
0.095 0.082 0.026

MCE
0.272 0.080 0.076
0.604 0.067 0.142
0.335 0.224 0.136
0.456 0.109 0.089
0.356 0.165 0.089
0.499 0.074 0.309
0.400 0.101 0.101
0.165 0.163 0.094

NLL
2.241 2.159 1.949
4.425 2.361 2.373
2.259 1.925 1.866
2.436 2.483 2.410
1.162 0.886 0.908
1.770 1.193 1.277
1.285 0.891 0.891
1.274 1.306 1.161

Brier Score
0.660 0.661 0.605
0.855 0.671 0.659
0.627 0.595 0.569
0.717 0.728 0.712
0.354 0.323 0.310
0.445 0.389 0.391
0.361 0.332 0.314
0.468 0.481 0.439

5.3 RESULTS
Table 1 presents accuracy and calibration scores for several combinations of network architectures and benchmark datasets. The models trained with VWCI loss consistently outperform the models with CI loss, which are special cases of VWCI, and the baseline on both classification accuracy and confidence calibration performance. Performance of CI is given by average and variance from 5 different cases of (= 1, 10-1, 10-2, 10-3, 10-4)1 and CI[Oracle] denotes the most optimistic value among the 5 cases in each column. Note that VWCI presents outstanding results in most cases even when compared with CI[Oracle] and that performance of CI is sensitive to the choice of 's (See Appendix C for details.). These results imply that the proposed loss function balances two conflicting loss terms effectively using variance of multiple stochastic inferences while performance of CI varies depending on hyper-parameter setting in each dataset.
We also compare the proposed framework with the state-of-the-art post-processing method, temperature scaling (TS) (Guo et al., 2017). The main distinction between post-processing methods and our work is the need for held-out dataset: our method allows to calibrate scores during training without additional data while Guo et al. (2017) requires held-out validation sets to calibrate scores. To illustrate the effectiveness of our framework, we compare our approach with TS in the following two scenarios: 1) using the entire training set for both training and calibration and 2) using 90% of training set for training and the remaining 10% for calibration. Table 2 presents that case 1 suffers from poor calibration performance and case 2 loses accuracy substantially due to training data reduction although it shows comparable calibration scores to VWCI. VWCI, on the other hand, presents consistently good results in terms of both classification accuracy and calibration performance.
A critical benefit of our variance-driven weight in the VWCI loss is the capability to maintain examples with high accuracy and high confidence. This is an important property for building real-world decision making systems with confidence interval, where the decisions should be both highly accurate and confident. Figure 2 illustrates portion of test examples that have higher accuracy and confidence than varying thresholds in ResNet-34, where VWCI presents better coverage than CI[Oracle] by controlling weights of two loss terms effectively based on variance of multiple stochastic inferences. Note that coverage of CI often depends on the choice of  significantly as demonstrated in Figure 2(right) while VWCI maintains higher coverage than CI using accurately calibrated predic-
1These 5 values of  are selected favorably to CI based on our preliminary experiment.
8

Under review as a conference paper at ICLR 2019

Sample Coverage (%) Sample Coverage (%)

40.0 80.0 60.0

20.0 40.0

VWCI CI[ =0.1; Acc.,ECE,MCE,NLL,Brier]

20.0

VWCI CI[ = 0.01; NLL] CI[ = 0.1; ECE,MCE,Brier] CI[ = 1; Acc.]

0.0 1.0

0.95 Thre0s.9hold 0.85

0.8 0.0 1.0

0.95 Thre0s.9hold 0.85

0.8

Figure 2: Coverage of ResNet-34 models with respect to confidence interval on Tiny ImageNet (left) and CIFAR-100 (right). Coverage is computed by the portion of examples with higher accuracy and confidence than thresholds shown in x-axis. To compare VWCI to CI[Oracle], we present results from multiple CI models with oracle 's for individual metrics, which are shown in graph legends.

tion scores. These results imply that using the predictive uncertainty for balancing the loss terms is preferable to setting with a constant coefficient.

6 CONCLUSION
We presented a generic framework for uncertainty estimation of a prediction in deep neural networks by calibrating accuracy and score based on stochastic inferences. Based on Bayesian interpretation of stochastic regularization and our empirical observation results, we claim that variation of multiple stochastic inferences for a single example is a crucial factor to estimate uncertainty of the average prediction. Motivated by this fact, we design the variance-weighted confidence-integrated loss to learn confidence-calibrated networks and enable uncertainty to be estimated by a single prediction. The proposed algorithm is also useful to understand existing confidence calibration methods in a unified way, and we compared our algorithm with other variations within our framework to analyze their characteristics.

REFERENCES
D. Barber and Christopher Bishop. Ensemble learning for multi-layer networks. In NIPS, 1998.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In ICML, 2016.
Alex Graves. Practical variational inference for neural networks. In NIPS, 2011.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In ICML, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. J. Mach. Learn. Res., 14(1):1303­1347, May 2013. ISSN 1532-4435. URL http: //dl.acm.org/citation.cfm?id=2502581.2502622.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In ECCV, 2016.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR, pp. 2261­2269, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.

9

Under review as a conference paper at ICLR 2019
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In NIPS, pp. 6405­6416, 2017.
Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. In ICLR, 2018.
Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dolla´r. Focal loss for dense object detection. In ICCV, 2017.
David J. C. MacKay. A practical bayesian framework for backpropagation networks. Neural Comput., 4(3):448­472, May 1992. ISSN 0899-7667. doi: 10.1162/neco.1992.4.3.448. URL http://dx.doi.org/10.1162/neco.1992.4.3.448.
Patrick McClure and Nikolaus Kriegeskorte. Representation of uncertainty in deep neural networks through sampling. CoRR, abs/1611.01639, 2016.
Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In AAAI, 2015.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, 1996. ISBN 0387947248.
Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In ICML, 2005.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, 2017.
John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. 10, 06 2000.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CVPR, 2016.
Mattias Teye, Hossein Azizpour, and Kevin Smith. Bayesian uncertainty estimation for batch normalized deep networks. arXiv preprint arXiv:1802.06455, 2018.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In ICML, 2013.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In ICML, 2001.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In KDD, 2002.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.
10

Under review as a conference paper at ICLR 2019

APPENDIX

A STOCHASTIC DEPTH AS APPROXIMATE BAYESIAN INFERENCE

ResNet (He et al., 2016) proposes to add skip connections to the network. If xi denotes the output of the ith layer and fi(x) represents a typical convolutional transformation, we can obtain the forward propagation

xi = fi(xi-1) + xi-1

(13)

and fi(x) is commonly defined by fi(x) = Wi · (B(Wi · (B(x))))

(14)

where Wi and Wi are weight matrices, (·) denotes convolution, and B and  indicates batch normalization and ReLU function, respectively.

ResNet with stochastic depth (Huang et al., 2016) randomly drops a subset of layers and bypass them with short-cut connection. Let ei  {0, 1} denotes a Bernoulli random variable which indicates whether the ith residual block is active or not. The forward propagation is extended from
Equation 13 to

xi = eifi(xi-1) + xi-1.

(15)

Now we can transform stochasticity from layers to the parameter space as follows:

eifi(xi-1) + xi-1 = ei Wi · (B(Wi · (B (xi-1)))) + xi-1

(16)

= ei4 Wi · (B(Wi · (B (xi-1)))) + xi-1

(17)

= ei4

Wi · (i( Wi

·

(B

(xi-1)) i

-

µi

)

+

i)

+ xi-1

(18)

= eiWi · (ei

i i

T

1 i

(ei

Wi

· (eii(xi-1 + i)) - eiµi) 1

) + xi-1

(19)

= Wi · (

i i

T

1 i

(Wi

· (i(xi-1 + i)) - eiµi) 1

) + xi-1

(20)

= fWi,Wi ,i,i ,i,i (xi-1) = fi (xi-1)

(21)

ei = e4i since it is a Bernoulli random variable. [Wi, Wi , i, i , i, i ] in this block drop at once or not.

All stochastic parameters i =

11

Under review as a conference paper at ICLR 2019

B APPROXIMATION OF KL-DIVERGENCE

Let   RD, p() = N (0, I) and q() =

2 i=1

ei

N

(i,

2I)

with

a

probability

vector

e

=

(e1, e2) where ei  [0, 1] and

2 i=1

ei

= 1.

In our work, 1 denotes the deterministic model

parameter [Wi, Wi , i, i , i, i ] and 2 = 0. The KL-Divergence between q() and p() is

KL(q()||p()) =

q ( )

log

q ( ) p()

d

(22)

= q() log q()d - q() log p()d.

(23)

We can re-parameterize the first entropy term with  = i +  i where i  N (0, I).

2
q() log q()d = ei
i=1 2
= ei
i=1

N (; i, i2I) log q()d N ( i; 0, I) log q(i +  i)d

Using q(i + 

i)



ei(2)-D/2-1

exp(-

1 2

T i

i) for large enough D,

2
q() log q()d  ei
i=1
= 2 ei 2
i=1
 2 ei 2
i=1

N ( i; 0, I) log

ei(2)-D/2-1

exp(-

1 2

T i

i)

d

log ei - log  +

N(

i; 0, I)

T i

id

i - D log 2

- log  - D(1 + log 2)

-

1 H (e).

2

For the second term of the Equation 23,

2
q() log p()d = ei N (; i, i2I) log N (; 0, I)d

i=1

= -1 2

2

ei iT i + D .

i=1

Then we can approximate

KL(q()||p()) 

2

ei 2

iT i + D - log  - D(1 + log 2)

-

1 H (e).

2

i=1

For a more general proof, see Gal & Ghahramani (2016).

(24) (25)
(26) (27) (28)
(29) (30)
(31)

12

Under review as a conference paper at ICLR 2019

C FULL EXPERIMENTAL RESULTS

We present the full results of Table 1 including all scores from individual CI models with different 's. Table 3 and 4 show the results on Tiny ImageNet and CIFAR-100, respectively.
Table 3: Classification accuracy and calibration scores of the models trained with various architectures on Tiny ImageNet supplementing Table 1. This table includes results from all models trained with CI loss with different 's. In addition to the architectures presented in Table 1, we also include results from ResNet-18.

Dataset

Architecture

ResNet-18

ResNet-34

Tiny ImageNet

VGG-16

WideResNet-16-8

DenseNet-40-12

Method
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI

Accuracy[%]
46.38 46.48 47.20 47.03 47.58 47.92 48.57
50.82 48.89 50.17 49.16 51.45 50.77 52.80
46.58 47.26 47.39 47.11 46.94 45.40 48.03
55.92 55.29 55.53 56.12 56.38 55.66 56.66
42.50 41.20 41.21 40.61 40.67 37.23 43.25

ECE
0.029 0.022 0.022 0.021 0.055 0.241 0.026
0.067 0.132 0.127 0.119 0.035 0.255 0.027
0.346 0.325 0.296 0.259 0.122 0.130 0.053
0.132 0.126 0.120 0.116 0.050 0.161 0.046
0.020 0.030 0.036 0.025 0.037 0.169 0.025

MCE
0.086 0.073 0.060 0.157 0.111 0.380 0.054
0.147 0.241 0.227 0.219 0.171 0.426 0.076
0.595 0.533 0.536 0.461 0.327 0.320 0.142
0.237 0.208 0.237 0.238 0.456 0.301 0.136
0.154 0.156 0.122 0.097 0.094 0.291 0.089

NLL
2.227 2.216 2.198 2.193 2.212 2.664 2.129
2.050 2.257 2.225 2.223 2.030 2.614 1.949
4.220 3.878 3.542 3.046 2.812 2.843 2.373
1.974 1.987 1.949 1.949 1.851 2.163 1.866
2.423 2.489 2.514 2.550 2.501 2.975 2.410

Brier Score
0.674 0.672 0.666 0.667 0.666 0.742 0.651
0.628 0.668 0.653 0.663 0.620 0.722 0.605
0.844 0.830 0.795 0.763 0.701 0.717 0.659
0.593 0.598 0.592 0.590 0.572 0.619 0.569
0.716 0.726 0.735 0.739 0.732 0.810 0.712

13

Under review as a conference paper at ICLR 2019

Table 4: Classification accuracy and calibration scores of models trained with various methods on CIFAR-100 supplementing Table 1. This table includes results from all models trained with CI loss with different 's. In addition to the architectures presented in Table 1, we also include results from ResNet-18.

Dataset

Architecture

ResNet-18

ResNet-34

CIFAR-100

VGG-16

WideResNet-16-8

DenseNet-40-12

Method
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI
Baseline CI[ = 10-4] CI[ = 10-3] CI[ = 0.01] CI[ = 0.1]
CI[ = 1] VWCI

Accuracy[%]
75.61 75.03 75.51 74.95 75.94 75.61 76.09
77.19 77.38 76.98 77.23 77.66 78.54 78.64
73.78 73.19 73.70 73.78 73.68 73.62 73.87
77.52 77.04 77.46 77.53 77.23 77.48 77.74
65.91 66.20 63.61 65.13 65.86 62.82 67.45

ECE
0.097 0.104 0.087 0.069 0.065 0.340 0.045
0.109 0.105 0.101 0.074 0.029 0.362 0.034
0.187 0.189 0.183 0.163 0.083 0.291 0.098
0.103 0.109 0.104 0.074 0.085 0.295 0.038
0.074 0.064 0.086 0.052 0.019 0.127 0.026

MCE
0.233 0.901 0.219 0.183 0.961 0.449 0.128
0.304 0.259 0.261 0.206 0.087 0.442 0.089
0.486 0.860 0.437 0.425 0.285 0.399 0.309
0.278 0.280 0.272 0.211 0.239 0.485 0.101
0.134 0.141 0.177 0.127 0.053 0.193 0.094

NLL
1.024 1.055 0.986 0.998 1.018 1.492 0.976
1.020 1.000 0.999 0.921 0.953 1.448 0.908
1.667 1.679 1.585 1.375 1.289 1.676 1.277
0.984 1.011 0.974 0.931 1.015 1.378 0.891
1.238 1.236 1.360 1.249 1.206 1.510 1.161

Brier Score
0.359 0.369 0.357 0.358 0.349 0.475 0.342
0.345 0.341 0.344 0.331 0.321 0.461 0.310
0.437 0.446 0.434 0.420 0.396 0.487 0.391
0.336 0.345 0.339 0.327 0.336 0.434 0.314
0.463 0.463 0.496 0.471 0.456 0.523 0.439

14

