Under review as a conference paper at ICLR 2019
DIRECTIONAL ANALYSIS OF STOCHASTIC GRADIENT DESCENT VIA VON MISES-FISHER DISTRIBUTIONS IN DEEP LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Although stochastic gradient descent (SGD) is a driving force behind the recent success of deep learning, our understanding of its dynamics in a high-dimensional parameter space is limited. In recent years, some researchers have used the stochasticity of minibatch gradients, or the signal-to-noise ratio, to better characterize the learning dynamics of SGD. Inspired from these work, we here analyze SGD from a geometrical perspective by inspecting the stochasticity of the norms and directions of minibatch gradients. We propose a model of the directional concentration for minibatch gradients through von Mises-Fisher (VMF) distribution, and show that the directional uniformity of minibatch gradients increases over the course of SGD. We empirically verify our result using deep convolutional networks and observe a higher correlation between the gradient stochasticity and the proposed directional uniformity than that against the gradient norm stochasticity, suggesting that the directional statistics of minibatch gradients is a major factor behind SGD.
1 INTRODUCTION
Stochastic gradient descent (SGD) has been a driving force behind the recent success of deep learning. Despite a series of work on improving SGD by incorporating the second-order information of the objective function (Roux et al., 2008; Martens, 2010; Dauphin et al., 2014; Martens & Grosse, 2015; Desjardins et al., 2015), SGD is still the most widely used optimization algorithm for training a deep neural network. The learning dynamics of SGD however has not been well characterized beyond that it converges to an extremal point (Bottou, 1998) due to the non-convexity and highdimensionality of a usual objective function used in deep learning.
Gradient stochasticity, or the signal-to-noise ratio (SNR) of stochastic gradient, has been proposed as a tool for analyzing the learning dynamics of SGD. Shwartz-Ziv & Tishby (2017) identified two phases in SGD based on this. In the first phase, "drift phase", the gradient mean is much higher than its standard deviation, during which optimization progresses rapidly. This drift phase is followed by the "diffusion phase", where SGD behaves similarly to Gaussian noise with very small means. Similar observations were made by Li & Yuan (2017) and Chee & Toulis (2018) who have also divided the learning dynamics of SGD into two phases.
Shwartz-Ziv & Tishby (2017) have proposed that such phase transition is related to information compression. However, Saxe et al. (2018) have reported that the information compression is not generally associated to this transition. Unlike Shwartz-Ziv & Tishby (2017), we notice that there are two aspects to the gradient stochasticity. One is the L2 norm of the minibatch gradient (the norm stochasticity), and the other is the directional balance of minibatch gradients (the directional stochasticity). SGD converges or terminates when either the norm of the minibatch gradient vanishes to zeros, or when the angles of the minibatch gradients are uniformly distributed and their non-zero norms are close to each other. That is, the gradient stochasticity, or the SNR of stochastic gradient, is driven by both of these aspects, and it is necessary for us to investigate not only the holistic SNR but also the SNR of the minibatch gradient norm and that of the minibatch gradient angles.
In this paper, we use a von Mises-Fisher (VMF) distribution, which is often used in directional statistics (Mardia & Jupp, 2009), and its concentration parameter  to characterize the directional
1

Under review as a conference paper at ICLR 2019

1.0 0.5 y 0.0 -0.5 -1.0



qqqqqqqqqqqqqq q
qq q

q qqqq

qqqqqq

q qqqqqqqqqqqqqqqqqq

0 5 50

q
qqq qq q q
q q
q qqqqqq

q
qqqqq qqqq

q qqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

-1.0 -0.5 0.0 0.5 1.0 x
Figure 1: Characteristics of the VMF distribution in a 2-dimensional space. 100 random samples are drawn from VMF(µ, ) where µ = (1, 0) and  = {0, 5, 50}.

balance of minibatch gradients and understand the learning dynamics of SGD from the perspective of directional statistics of minibatch gradients. We prove that SGD increases the directional balance of minibatch gradients. We empirically verify this with deep convolutional networks with various techniques, including batch normalization (Ioffe & Szegedy, 2015) and residual connections (He et al., 2015), on MNIST and CIFAR-10 (Krizhevsky & Hinton, 2009). Our empirical investigation further reveals that the proposed directional stochasticity is a major drive behind the gradient stochasticity compared to the norm stochasticity, suggesting the importance of understanding the directional statistics of stochastic gradient.

2 PRELIMINARIES

Norms and Angles Unless explicitly stated, a norm refers to L2 norm. · and ·, · thus correspond to L2 norm and the Euclidean inner product on Rd, respectively. We use xn  x
to indicate that "a random variable xn converges to x in distribution." Similarly, xn P x means
convergence in probability. An angle  between d-dimensional vectors u and v is defined by



=

180 

cos-1

u,v uv

.

Loss functions

A

loss

function

of

a

neural

network

is

written

as

f (w)

=

1 n

n i=1

fi(w),

where

w  Rd is a trainable parameter. fi is "a per-example loss function" computed on the i-th data

point. We use I and m to denote a minibatch index set and its batch size, respectively. Further, we

call

fI(w)

=

1 m

iI fi(w) "a minibatch loss function given I". During optimization, we write a

parameter w at the i-th iteration in the t-th epoch as wti, and w00 is an initial parameter. We use nb

to refer to the number of minibatches in a single epoch.

von Mises-Fisher Distribution We use the von Mises-Fisher (VMF) distribution to model the directions of vectors. The definition of the VMF distribution is as follows:
Definition 1. (von Mises-Fisher Distribution, Banerjee et al. (2005)) The pdf of the VMF(µ, ) is given by
fd(x; µ, ) = Cd() exp(µ x)
on the hypersphere Sd-1  Rd. Here, the concentration parameter  determines how the samples from this distribution are concentrated on the mean direction µ and Cd() is constant determined by d and .
If  is zero, then it is uniform distribution on a unit hypersphere, and as   , it becomes a point mass on the unit hypersphere (Figure 1). The maximum likelihood estimates for µ and  are

2

Under review as a conference paper at ICLR 2019

Gradient norm stochasticity

0.20

0.15 q q

q

0.10

q q
q

q

0.05 0.00

q
q q q q q q qq

2 8 32 128 512 4096 32768 Batch size

(a) Var( g^(w) )/E g^(w)

SGD
(b) Directional balance in SGD iterations

Figure 2: The directions of minibatch gradients become important when a batch size m is sufficiently

large. (a) The gradient norm stochasticity of g^(w) with respect to various batch sizes at 5 random

points w with mean (black line) and mean±std. (shaded area) in a log-linear scale; (b) If the

gradient norm stochasticity is sufficiently low, then the directions of g^i(w)'s must be balanced to

satisfy

3 i=1

g^i(w)



0.

µ^ = r¯ =

n i=1

xi

ni=in=1 1xxi i

n

and ^



r¯(d-r¯2 ) 1-r¯2

where

(Banerjee et al., 2005).

xi's

are

random

samples

from

the

VMF

distribution

and

3 THEORECTICAL MOTIVATION

3.1 ANALYSIS OF THE GRADIENT NORM STOCHASTICITY

It is a usual practice for SGD to use a minibatch gradient g^(w) = -wfI(w) instead of a full batch

gradient satisfies

g(w) = -wf (w). Eg^(w) = g(w) and

The minibatch index set I

Cov(g^(w), g^(w))



1 mn

is

drawn from {1,. .

n i=1

gi(w)gi

(w)

.

,n} randomly. for n m

g^(w) where

n is the number of full data and gi(w) = -wfi(w) (Hoffer et al., 2017). As the batch size m

increases, the randomness of g^(w) decreases. Hence E g^(w) tends to g(w) , and Var( g^(w) ),

which is the variance of the norm of the minibatch gradient, vanishes. The convergence rate analysis

is as the following:

Theorem 1. Let g^(w) be a minibatch gradient induced from the minibatch index set I of batch size m from {1, . . . , n} and suppose  = maxi,j{1,...,n} | gi(w), gj(w) |. Then

0  E g^(w) - g(w)



2(n - m) m(n - 1)

×

E

g^(w)

 +

g(w)



(n - m) m(n - 1) g(w)

and

Var(

g^(w)

)

2(n - m) m(n - 1) 

.

Hence,

Var( g^(w) )

2(n - m)



E g^(w)

 m(n - 1) × g(w) 2 .

(1)

Proof. See Supplemental A.

According to Theorem 1, the large batch size m reduces the variance of g^(w) centered at E g^(w) with convergence rate O(1/m). We empirically verify this by estimating the gradient norm stochasticity at various random points while varying the minibatch size, using a fullyconnected neural network (FNN) with MNIST, as shown in Figure 2(a) (see Supplemental E for more details.)
This theorem however only demonstrate that the gradient norm stochasticity is (l.h.s. of (1)) is low at random initial points. It may blow up after SGD updates, since the upper bound (r.h.s. of (1))

3

Under review as a conference paper at ICLR 2019

Density Density Density

10 8 6 4 2 0 89.8

89.9 90.0 90.1 Angle
(a) Asymtotic

90.2

10 8 6 4 2 0 89.8

89.9 90.0 90.1 Angle

(b) Initial epoch

90.2

10 8 6 4 2 0 89.8

89.9 90.0 90.1 Angle

90.2

(c) Training accuracy of > 99.9%

Dimension 100000 300000

635200 1000000 2000000

Asymtotic Seed 1 Seed 2

Seed 3 Seed 4 Seed 5

Figure 3:

(a) Asymtotic angle densities (2) of (u, v)

=

180 

cos-1

u, v

where u and v are in-

dependent uniformly random unit vectors for each large dimension d. As d  , (u, v) tends

to less scattered from 90 (in degree). (b­c) We apply SGD on FNN for MNIST classification with

the batch size 64 and the fixed learning rate 0.01 starting from five randomly initialized parame-

ters.

We draw a density plot (u,

g^j (w)) g^j (w))

) for 3000 minibatch gradients (black) at w = w00 (b)

and w = wfi0nal, with training accuracy of > 99.9%, (c) when u is given. After SGD iterations, the

density of (u, g^j(w)) converges to an asymptotic density (red). The dimension of FNN is 635,200.

inversely proportional to g(w) . This implies that the learning dynamics and convergence of SGD,

measured in terms of the vanishing gradient, i.e.,

nb i=1

g^i(w)



0,

is

not

necessarily

explained

by

the vanishing norms of minibatch gradients, but rather by the balance of the directions of g^i(w)'s,

which motivates our investigation of the directional statistics of minibatch gradients.

3.2 UNIFORMITY MEASUREMENT VIA ANALYSIS OF ANGLES

In order to investigate the directions of minibatch gradients and how they balance, we start from an angle between two vectors. First, we analyze an asymtotic behavior of angles between uniformly random unit vectors in a high-dimensional space.
Theorem 2. Suppose that u and v are mutually independent d-dimensional uniformly random unit vectors. Then,

as d  .

 d

180 cos-1

u, v

- 90

N

0,

180

2



Proof. See Supplemental B.

According to Theorem 2, the angle between two independent uniformly random unit vectors is normally distributed and becomes incresingly more concentrated as d grows (Figure 3(a)). If SGD iterations indeed drive the directions the directions of minibatch gradients to be uniform, then, at least, the distribution of angles between minibatch gradients and a given uniformly sampled unit vector follows asymptotically

N 90,

180

2
.

d

(2)

Figures 3(b) and 3(c) show that the distribution of the angles between minibatch gradients and a given uniformly sampled unit vector converges to an asymptotic distribution (2) after SGD iterations. Although we could measure the uniformity of minibatch gradients how the angle distribution between minibatch gradients is close to (2), it is not as trivial to compare the distributions as to compare numerical values. This necessitates another way to measure the uniformity of minibatch gradients.

4

Under review as a conference paper at ICLR 2019

(a) From w00 to w by i xi

(b) From w00 to w10 by SGD iterations

Figure 4: (a) If the point is slightly moved from w00 to w by i xi where xi = (pi - w00)/ pi - w00 and xi = (pi - w )/ pi - w , then i xi < i xi which is equivalent to (w ) < (w00); (b) SGD iterations (left) from w00 to w10 is approximately equivalent to the circumstance (right) when g^i(w00)'s are sufficently parallel to (pi - w00)'s for each i.

3.3 UNIFORMITY MEASUREMENT VIA VMF DISTRIBUTION

To model the uniformity of minibatch gradients, we propose to use the VMF distribution in Definition 1. The concentration parameter  measures how uniform the directions of unit vectors are distributed. By Theorem 1, with a large batch size, the norm of minibatch gradient is nearly deterministic, and µ^ is almost parallel to the direction of full batch gradient. In other words,  measures how much the directions of minibatch gradients concentrate around the full batch gradient.

The following Lemma 1 introduces the relationship between the norm of averaged unit vectors and ^, the estimator of .

Lemma 1. The approximated estimator of  induced from the d-dimensional unit vectors

{x1, x2, · · · , xnb },

r¯(d - r¯2) ^ = 1 - r¯2 ,

is a strictly increasing function on

function of u =

nb i=1

xi

,

then

[0, 1], where r¯ =

nb i=1

xi

nb

h(·) is Lipschitz continuous

. If we consider on [0, nb(1 - )]

^ = h(u) as a for any > 0.

Moreover, h(·) and h (·) are strictly increasing and increasing on [0, nb), respectively.

Proof. See Supplemental C.1.

Consider

^(w) = h

nb pi - w i=1 pi - w

,

which is measured from the directions from the current location w to the fixed points pi's, where

h(·) is a function defined in Lemma 1. Since h(·) is an increasing function, we may focus only

on

nb pi-w i=1 pi-w

to see how ^ behaves with respect to its argument. Lemma 2 implies that the

estimated uniformity ^ increases if we move away from w00 to w = w00 + small (Figure 4(a)). In other words, ^(w ) < ^(w00).

pi -w00 i pi-w00

with a

Lemma 2. Let p1, p2, · · · , pnb be d-dimensional vectors. If all pi's are not on the ray from the current location w, then there exists positive number  such that

for all  (0, ].

nb pj - w - j=1 pj - w -

nb i=1
nb i=1

pi -w pi -w
pi -w pi -w

< nb pi - w i=1 pi - w

Proof. See Supplemental C.2.

5

Under review as a conference paper at ICLR 2019

We make the connection between the observation above and SGD by first viewing pi's as local minibatch solutions.
Definition 2. For a minibatch index set Ii, pi(w) = arg minw N(w;ri)fIi (w ) is a local minibatch solution of Ii at w, where N (w; ri) is a neighborhood of radius ri at w. Here, ri is determined by w and Ii for pi(w) to exist uniquely.

Under this definition, pi(w) is local minimum of a minibatch loss function fIi near w. Then we reasonably expect that the direction of g^i(w) = -wfIi (w) is similar to that of pi(w) - w.

Each epoch of SGD with a learning rate  computes a series of wtj = wt0 + 

j i=1

g^i(wti-1

)

for all j  {1, . . . , nb}. If -wf (·) is Lipschitz continuous, the negative gradient of the i-th

iteration in the t-th epoch satisfies g^i(wti-1)  g^i(wt0) for a small . Moreover, Theorem 1 implies

g^i(wti-1)  g^i(wt0) =  for all i  {1, . . . , nb} with a large minibatch size or at early stages

of SGD iterations.

For example, as in Figure 4(b), suppose that t = 0, nb = 3 and  = 1, and assume that pi(w00) = pi(w10) = pi for all i = 1, 2, 3. Then,

^(w00) = h

3 pi - w00 i=1 pi - w00



3
, and ^(w10)  h 
j=1

pj - w00 -  pj - w00 - 

3 i=1

g^i(w0i-1

)

3 i=1

g^i(w0i-1

)

 .

If g^i(w00) is parallel to pi - w00 for each i,

3 pj - w00 -  j=1 pj - w00 - 

3 i=1
3 i=1

g^i (w0i-1 ) g^i (w0i-1 ) g^i (w0i-1 ) g^i (w0i-1 )

3 pj - w00 -  =
j=1 pj - w00 - 

3 i=1
3 i=1

pi -w00 pi -w00 pi -w00 pi -w00

.

Then, by Lemma 2, we have ^(w10) < ^(w00).

Since g^i(w00) may not be perfectly parallel to pi - w00, we further show that a small movement



i

pi -w00 pi -w00

reducing ^ can be replaced by 

i

g^i (w0i-1 ) g^i (w0i-1 )

in Theorem 3 below.

Theorem 3. Let p1(wt0), p2(wt0), · · · , pnb (wt0) be d-dimensional vectors, and all pi(wt0)'s are not on a single ray from the current location wt0. If

nb i=1

pi(wt0) - wt0 pi(wt0) - wt0

nb
-
i=1

g^i(wti-1) g^i(wti-1)



(3)

for a sufficiently small  > 0, then there exists positive number  such that

nb pj (wt0) - wt0 - j=1 pj (wt0) - wt0 - for all  (0, ].

nb i=1
nb i=1

g^i (wti-1 ) g^i (wti-1 ) g^i (wti-1 ) g^i (wti-1 )

<

nb i=1

pi(wt0) - wt0 pi(wt0) - wt0

(4)

Proof. See Supplemental C.3.

This Theorem 3 asserts that ^(·) decreases even with some perturbation along the averaged direction

i

pi (w)-w pi (w)-w

.

With additional assumptions on each minibatch loss function, we have a sufficient

condition for (3), summarized in Corollary 3.1.

Corollary 3.1. Let pi be the local minibatch solution of each fIi . Suppose a region R satisfying:

For all w, w  R, pi(w) = pi(w ) = pi

for all i = 1, · · · , nb. Further, assume that Hessian matrices of fIi 's are positive definite, wellconditioned and bounded in the sense of matrix L2 norm on R. If SGD moves from wt0 to wt0+1 on R with a large batch size and a small learning rate, then ^(wt0) > ^(wt0+1). Moreover, we can estimate ^(wt0) and ^(wt0+1) by minibatch gradients on wt0 and wt0+1, respectively.

6

Under review as a conference paper at ICLR 2019

Proof. See Supplemental D.

Without the corollary above, we need to solve pi(wt0) = arg minwN(wt0;r) fIi (w) for all i 

{1, . . . , ns}, where ns is the number of samples to estimate , in order to compute ^(wt0). Corol-

lary 3.1 however implies that we can compute ^(wt0) by using

g^i (wt0 ) g^i (wt0 )

instead of

,pi (wt0 )-wt0
pi (wt0 )-wt0

significantly reducing computational overhead.

In Practice

Although the number of all possible minibatches in each epoch is nb =

n m

,

it

is

often the case to use nb  n/m minibatches at each epoch in practice to go from wt0 to wt0+1.

Assuming that these nb minibatches were selected uniformly at random, the average of the nb nor-

malized minibatch gradients is the maximum likelihood estimate of µ, just like the average of all nb

normalized minibatch gradients. Thus, we expect with a large nb,

(mn )
i=1

pi(wt0) - wt0 pi(wt0) - wt0

nb
-
i=1

g^i(wti-1) g^i(wti-1)

 ,

and that SGD in practice also satisfies ^(wt0) > ^(wt0+1).

4 EXPERIMENTS

4.1 SETUP
In order to empirically verify our theory on directional statistics of minibatch gradients, we train various types of deep neural networks using SGD and monitor the following metrics for analyzing the learning dynamics of SGD:
· Training loss · Validation loss · Gradient stochasticity (GS) Ewfi(w) /tr(Cov(wfi(w), wfi(w))  · Gradient norm stochasticity (GNS)  E wfi(w) / Var( wfi(w) )  · Directional Uniformity  
The latter three quantities are statistically estimated using ns = 3, 000 minibatches.
We train the following types of deep neural networks (Supplemental E):
· FNN: a fully connected network with a single hidden layer · DFNN: a fully connected network with three hidden layers · CNN: a convolutional network with 14 layers (Krizhevsky et al., 2012)
In the case of the CNN, we also evaluate its variant with skip connections (+Res) (He et al., 2015). As it was shown recently by Santurkar et al. (2018) that batch normalization (Ioffe & Szegedy, 2015) improves the smoothness of a loss function in terms of its Hessian, we also test adding batch normalization to each layer right before the ReLU (Nair & Hinton, 2010) nonlinearity (+BN). We use MNIST for the FNN, DFNN and their variants, while CIFAR-10 (Krizhevsky & Hinton, 2009) for the CNN and its variants.
We train each model variant using SGD with minibatches of size 64 and a fixed learning rate of 0.01. These were selected so that the training accuracy of > 99.9%. We repeat each setup five times starting from different random initializations and report both the mean and standard deviation.

4.2 DIRECTIONAL UNIFORMITY INCREASES
FNN and DFNN We first observe that  decreases over training regardless of the network's depth in Figure 5 (a,b). We however also notice that  decrease monotonically with the FNN, but less so with its deeper variant (DFNN). We conjecture this is due to the less-smooth loss landscape of a deep

7

Under review as a conference paper at ICLR 2019

106 q 5 × 105
 2 × 105 105
5 × 104
2 × 104 1

q

qq

q qq q q

q

qq q

qqqq qqqqqqq

2 × 106 q 106
 5 × 105 2 × 105 105

5 20 50 150 Epoch

1

q

q

q qq q

qq

q qq

q

q

qq

q

q

q
qqq q q

2 × 105 105
q 5 × 104
2 × 104

5 20 50 150 Epoch

1

q qq q

q q

q

q

q qq

q qq

q q q qq

qq q

q q

qq qq

5 20 Epoch

80 200

106 5 × 105 q

2 × 105



5

×

105 104

2 × 104 104
5 × 103 1

q q

q

q

q

q

q q

q qq qq

qq

q qq

q

qq

q

q
q qqqqq

5 20 Epoch

80 200

(a) FNN MNIST 98.18 ± 0.04%

(b) DFNN MNIST 98.30 ± 0.05%

(c) CNN CIFAR-10 53.78 ± 3.22%

(d) CNN+Res CIFAR-10 63.39 ± 1.68%

106 q 5 × 105
 2 × 105 105
5 × 104
2 × 104 1

q

q qq

qq

q

qq q

q

q qqqq

q qqqqqq

5 20 50 150 Epoch

2 × 106 q 106
 5 × 105 2 × 105 105
1

2 × 105 105
 5 × 104 q

q

q

q q qq

q

q qq

q

qq q q q

qqqqqqq

5 20 50 150 Epoch

2 × 104 1

q qq qq

q

q

q qq q q

qq q

qq

qq q q qq

q qq

q

q

5 20 Epoch

80 200

106 5 × 105

2 × 105 q



5

×

105 104

2 × 104 104
5 × 103 1

q

q

q q qq

qq

q

q

q q

q

qq

qq

q qq

qqqqqqqqq

5 20 Epoch

80 200

(e) FNN+BN MNIST 98.37 ± 0.08%

(f) DFNN+BN MNIST 98.51 ± 0.11%

(g) CNN+BN CIFAR-10 73.00 ± 1.16%

(h) CNN+Res+BN CIFAR-10 72.71 ± 1.02%

Figure 5: We show the average  (black curve) ± std. (shaded area), as the function of the number of training epochs(in log-log scale) for all considered setups. We report the name of architecture, dataset and maximum valid accuracy(mean±std.) of all epochs. Although  decreases eventually over time in all the cases, batch normalization (+BN) significantly reduces the variance of the directional stochasticity ((a­d) vs. (e­h)). We also observe that the skip connections make  decrease monotonically ((c,g) vs. (d,h)). Note the differences in the y-scales.

neural network. This difference between FNN and DFNN however almost entirely vanishes when batch normalization (+BN) is applied (Figure 5 (e,f)). This was expected as batch normalization is known to make the loss function behave better, and our theory assumes a smooth objective function.
CNN The CNN is substantially deeper than either FNN or DFNN and is trained on a substantially more difficult problem of CIFAR-10. In other words, the assumptions underlying our theory may not hold as well. Nevetheless, as shown in Figure 5 (c),  eventually drops below its initial point, although this trend is not monotonic and  fluctuates significantly over training. The addition of batch normalization (+BN) helps with the fluctuation but  does not monotonically decrease (Figure 5 (g)). On the other hand, we observe the monotonic decrease of  when skip connections (+Res) are introduced (Figure 5 (c) vs. Figure 5 (d)) albeit still with some level of fluctuation especially in the early stage of learning. When both batch normalization and skip connections are used (+Res+BN), the behaviour of  matches with our prediction without much fluctuation.
4.3 DIRECTIONAL UNIFORMITY AND OTHER METRICS
The gradient stochasticity (GS) was used by Shwartz-Ziv & Tishby (2017) as a main metric for identifying two phases of SGD learning in deep neural networks. This quantity includes both the gradient norm stochasticity (GNS) and the directional uniformity , implying that either or both of GNS and  could drive the gradient stochasticity. We thus investigate the relationship among these three quantities as well as training and validation losses. We focus on CNN, CNN+BN and CNN+Res+BN trained on CIFAR-10.
From Figure 6 (a,c,e), it is clear that the proposed metric of directional uniformity  correlates better with the gradient stochasticity than the gradient norm stochasticity does. This was especially prominent during the early stage of learning, suggesting that the directional statistics of minibatch gradients is a major explanatory factor behind the learning dynamics of SGD. This difference in correlations is much more apparent from the scatterplots in Figure 6 (b,d,f). We show these plots created from other four training runs per setup in Supplemental F.
8

Under review as a conference paper at ICLR 2019

normSNR

normSNR

normSNR

Ratio

1.00 0.75 0.50 0.25 0.00
1

5 20 Epoch
(a) CNN

80

200

5 × 105

q 10.0
q

qq

q

q

2 × 105

qq qqqqqqq

qqqqqqq

 5 × 110045 q

q

q q

q qqqq
q q

q

qq q

qqqqqqqqqqq

qq

qqqq q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqq

5.0
2.0 1.0 0.5

0.1 0.2

0.5 1.0

SNR

(b) CNN

Ratio

1.00 0.75 0.50 0.25 0.00
1

5 20 Epoch
(c) CNN+BN

80

200

105
5 × 104

2 × 104

qqqqqqqq q q qqqqqqqqqqqqqqqqq qq qqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qq

qq
q qqq
q q q

qq

q q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

0.05

0.10 SNR

0.20

(d) CNN+BN

q

q
10.0 5.0
2.0 1.0 0.5 0.50

Ratio

1.00 0.75 0.50 0.25 0.00
1

5 20 Epoch
(e) CNN+Res+BN

80

200

105
 5 × 104
2 × 104

q

q

qqq

q

q qqqqqqqq q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqqqqqqqqqqq

qq

qq qq

qqqqqqqqqqqqqqqq

q

qqqqqqqqq q qqqqqqqqqqqqqqqqqqqqqqqqqq qqqqq q

q

0.05

0.10 0.20 SNR

0.50

(f) CNN+Res+BN

10 5
2 1

 Train loss

normSNR Valid loss

SNR

q SNR- SNR-normSNR

Figure 6: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid loss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and directional uniformity . We normalized each quantity by its maximum value over training for easier comparison on a single plot. In all the cases, SNR (orange) and  (red) are almost entirely correlated with each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggest that the SNR is largely driven by the directional uniformity.

5 CONCLUSION
Stochasticity of gradients is a key to understanding the learning dynamics of SGD (Shwartz-Ziv & Tishby, 2017) and has been pointed out as a factor behind the success of SGD (see, e.g., LeCun et al., 2012; Keskar et al., 2016). In this paper, we provide a theoretical framework using von Mises-Fisher distribution, under which the directional stochasticity of minibatch gradients can be estimated and analyzed, and show that the directional uniformity increases over the course of SGD. Through the extensive empirical evaluation, we have observed that the directional uniformity indeed improves
9

Under review as a conference paper at ICLR 2019
over the course of training a deep neural network, and that its trend is monotonic when batch normalization and skip connections were used. Furthermore, we demonstrated that the stochasticity of minibatch gradients is largely determined by the directional stochasticity rather than the gradient norm stochasticity.
Our work in this paper suggests two major research directions for the future. First, our analysis has focused on the aspect of optimization, and it is an open question how the directional uniformity relates to the generalization error although handling the stochasticity of gradients has improved SGD (Neelakantan et al., 2015; Hoffer et al., 2017; Smith et al., 2017; Jin et al., 2017). Second, we have focused on passive analysis of SGD using the directional statistics of minibatch gradients, but it is not unreasonable to suspect that SGD could be improved by explicitly taking into account the directional statistics of minibatch gradients during optimization.
REFERENCES
Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, and Suvrit Sra. Clustering on the unit hypersphere using von mises-fisher distributions. Journal of Machine Learning Research, 6(Sep): 1345­1382, 2005.
Le´on Bottou. Online algorithms and stochastic approximations. In David Saad (ed.), Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998. URL http://leon.bottou.org/papers/bottou-98x. revised, oct 2012.
Le´on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010, pp. 177­186. Springer, 2010.
George Casella and Roger L Berger. Statistical inference, volume 2. Duxbury Pacific Grove, CA, 2002.
Jerry Chee and Panos Toulis. Convergence diagnostics for stochastic gradient descent with constant learning rate. In International Conference on Artificial Intelligence and Statistics, pp. 1476­1485, 2018.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pp. 2933­2941, 2014.
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, et al. Natural neural networks. In Advances in Neural Information Processing Systems, pp. 2071­2079, 2015.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­256, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, pp. 1729­1739, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
10

Under review as a conference paper at ICLR 2019
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097­1105. Curran Associates, Inc., 2012.
Yann A LeCun, Le´on Bottou, Genevieve B Orr, and Klaus-Robert Mu¨ller. Efficient backprop. In Neural networks: Tricks of the trade, pp. 9­48. Springer, 2012.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
Kanti V Mardia and Peter E Jupp. Directional statistics, volume 494. John Wiley & Sons, 2009.
James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735­742, 2010.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408­2417, 2015.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807­814, 2010.
Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.
Nicolas L Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradient algorithm. In Advances in neural information processing systems, pp. 849­856, 2008.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization?(no, it is not about internal covariate shift). arXiv preprint arXiv:1805.11604, 2018.
Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning. 2018.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don't decay the learning rate, increase the batch size. CoRR, abs/1711.00489, 2017. URL http://arxiv.org/abs/1711.00489.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
11

Under review as a conference paper at ICLR 2019

SUPPLEMENTARY MATERIAL

A PROOFS FOR THEOREM 1

In proving Theorem 1, we use Lemma A.1. Define selector random variables(Hoffer et al., 2017)

as below:

si =

1, 0,

if i  I if i / I

.

Then we have

1n

g^(w) = m

gi(w)si.

i=1

Lemma A.1. Let g^(w) be a minibatch gradient induced from the minibatch index set I with batch size m from {1, . . . , n}. Then

0  E g^(w) 2 -

g(w)

2



2(n - m) m(n - 1) .

(5)

where  = maxi,j{1,...,n} | gi(w), gj (w) |.

Proof. By Jensen's inequality, 0  E g^(w) 2 - g(w) 2. Note that

n
E g^(w) 2 =

n1 m2 gi(w), gj(w) E[sisj].

i=1 j=1

Since

E[si sj ]

=

m n

ij

+

m(m-1) n(n-1)

(1

- ij ),

E g^(w) 2 - g(w) 2 =

1 mn

-

m-1 mn(n - 1)

n
gi(w), gi(w)

i=1

+

m-1 mn(n - 1)

-

1 n2

nn
gi(w), gj(w)

i=1 j=1

=

1 mn

-

m-1 mn(n - 1)

n m-n n n

gi(w), gi(w) + mn2(n - 1)

gi(w), gj(w)

i=1 i=1 j=1



1 mn

-

m-1 mn(n - 1)

n

+

n-m mn2(n -

n2 1)

2(n - m)

=

m(n

-

 1)

where  = maxi,j{1,...,n} | gi(w), gj (w) |.

Theorem 1. Let g^(w) be a minibatch gradient induced from the minibatch index set I with batch size m from {1, . . . , n} and suppose  = maxi,j{1,...,n} | gi(w), gj(w) |. Then

0  E g^(w)

-

g(w)

 2(n - m) ×



m(n - 1) E g^(w) + g(w)

 (n - m) m(n - 1) g(w)

and

Var(

g^(w)

)

2(n - m) m(n - 1) 

.

Hence,

Var( g^(w) )  E g^(w)

2(n - m) m(n - 1)

×

 g(w) 2 .

12

Under review as a conference paper at ICLR 2019

Proof. By Jensen's inequality, we have g(w) = Eg^(w)  E g^(w) and {E g^(w) }2  E g^(w) 2. From the second inequality and Lemma A.1,

{E g^(w) }2  E g^(w) 2 

g(w)

2+

2(n - m) 

m(n - 1)

or

E g^(w) - g(w)

E g^(w) + g(w)



2(n - m) m(n - 1) 

.

Hence

E g^(w)



g(w)

+

2(n - m) m(n - 1)

×

E

g^(w)

 +

g(w)



(n - m) m(n - 1) g(w)

.

Further,

Var( g^(w) ) = E g^(w) 2 - {E g^(w) }2

 E g^(w) 2 - Eg^(w) 2



g(w)

2+

2(n

-

m) 

-

m(n - 1)

g(w)

2

=

2(n - m) .
m(n - 1)

B PROOFS FOR THEOREM 2

For our proofs, Slutsky's theorem and delta method are key results to describe limiting behaviors of

random variables in distribution sense.

Theorem B.1. (Slutsky's theorem, Casella & Berger (2002)) Let {xn}, {yn} be a sequence of random variables that satisfies xn  x and yn P  when n goes to infinity and  is constant. Then
xnyn  cx

Theorem B.2. (Delta method, that satisfies n(yn - µ)  N

Casella (0, 2).

& Berger (2002)) Let yn be a sequence of random variables For a given smooth function f : R  R, suppose that f (µ)

exists and is not 0 where f

isa derivative. Then n[f (yn) - f (µ)] 

N (0, 2[f

(µ)]2).

Lemma B.1. Suppose that u and v are mutually independent d-dimensional uniformly random unit vectors. Then, d u, v  N (0, 1) as d  .

Proof. Note that d-dimensional uniformly random unit vectors u can be generated by normalization

of d-dimensional multivariate standard normal random vectors x  N (0, Id). That is,

u

x .

x

Suppose that two independent uniformly random unit vector u and v are generated by two indepen-

dent d-dimensional standard normal vector x = (x1, x2, · · · , xd) and y = (y1, y2, · · · , yd). Denote

them

xy

u = and v = .

xy

By SLLN, we have

x  1 a.s. d

(Use

1
d

x/ d

d i=1

xi2



E[x12]

P 1. Similarly,

= y

1). Since almost sure convergence

 /d

P

1.

Moreover,

by

CLT,

implies

convergence

in

probability,

x, y d

 =d

1d d xiyi
i=1

 N (0, 1).

Therefore, by Theorem B.1 (Slutsky'stheorem), d u, v  N (0, 1).

13

Under review as a conference paper at ICLR 2019

Theorem 2. Suppose that u and v are mutually independent d-dimensional uniformly random unit vectors. Then,

as d  .

 d

180 cos-1

u, v

- 90

N

0,

180

2



Proof.

Suppose that µ

=

0, 

=

1, and f (·)

=

180 

cos-1(·).

Since

180 

d dx

cos-1(x)

=

-

180  1-x2

,

we

have

f

(µ)

=

-

180 

.

Hence,

by

Lemma

B.1

and

Theorem

B.2

(Delta

method),

 d

180 cos-1

u, v

- 90

N

0,

180

2
.



C PROOFS FOR THEOREM 3

C.1 PROOF OF LEMMA 1

Lemma 1. The approximated estimator of  induced from the d-dimensional unit vectors {x1, x2, · · · , xnb },
r¯(d - r¯2) ^ = 1 - r¯2 ,

where r¯ function

= of

u

nb i=1

xi

nb

=

is a strict increasing function on [0, 1]. We can also consider ^ = h(u)

nb i=1

xi

.

Then h(·) is Lipschitz continuous on [0, nb(1 -

)] for any

>

as 0.

Moreover, h(·) and h (·) are strict increasing and increasing on [0, nb), respectively.

Proof. Note that

nb i=1

xi



nb i=1

||xi

||

=

nb.

Therefore,

we

have

r¯



[0, 1].

If

d

=

1,

then

^ = r¯ and this increases on [0, 1]. For d > 1,

d^ d + r¯4 + (d - 3r¯2)

= dr¯

(1 - r¯2)2

and its numerator is always positive for d > 2. When d = 2,

d^ dr¯

=

r¯4 - 3r¯2 + 4 (1 - r¯2)2

=

(r¯2

-

3 2

)2

+

7 4

(1 - r¯2)2

> 0.

So ^ increases as r¯ increases.

The

Lipschitz

continuity

of

h(·)

directly

comes

from

the

continuity

of

d^ dr¯

since

d^ 1 d^ =.
du nb dr¯

Recall that any continuous function on the compact interval [0, nb(1 - )] is bounded. Hence the derivative of ^ with respect to u is bounded. This implies the Lipschitz continuity of h(·).

h(·)

is

strictly

increasing

since

r¯ =

u nb

.

Further,

1 d2^ h (u) = nb2 dr¯2
2r¯5 + (4 - 8d)r¯3 + (8d - 6)r¯ = nb2(1 - r¯2)4 > 0

due to r¯  [0, 1]. Therefore h (·) is also increasing on [0, nb).

14

Under review as a conference paper at ICLR 2019

C.2 PROOF OF LEMMA 2

Lemma 2. Let p1, p2, · · · , pnb be d-dimensional vectors. If all pi's are not on the ray from the current location w, then there exists positive number  such that

nb pj - w - j=1 pj - w -

nb i=1
nb i=1

pi -w pi -w
pi -w pi -w

< nb pi - w i=1 pi - w

for all  (0, ].

Proof. Without loss of generality, we regard w as the origin. Let f ( ) =

f

nb j=1
(0) <

pj - pj -

nb i=1
nb i=1

pi pi pi pi

0. Now denote

2
,
xj

then f (0) =

=

pj pj

, pj(

)

=

nb pi i=1 pi
pj -

2
. Therefore, we only need to show

nb i=1

xi

and

u

=

-

nb i=1

xi.

That is,

pj( ) = pj + u. Since

f( ) =

nb pj ( ) , nb pj ( ) j=1 pj ( ) j=1 pj ( )

,

we have

f ( ) = 2 nb pj( ) , d j=1 pj ( ) d

nb pj ( ) j=1 pj ( )

and

d nb pj ( ) d j=1 pj ( )

nb
=
j=1

pj (

)

u-

u,pj ( ) pj ( )

pj ( ) 2

pj (

) .

Hence

f ( )=2

nb j=1

pj( )

nb
,

pj ( ) j=1

pj( ) u -

u,pj ( ) pj ( )

pj (

)

pj ( ) 2

.

Note that pj(0) = pj and xj = 1. We have

f (0) = 2

nb j=1

pj pj

nb
,
j=1

pj

u-

u,pj pj

pj

pj 2

=2

nb

pj

nb
,

1

j=1 pj j=1 pj

u - u, pj pj

=2

nb nb
xj ,
j=1 j=1

1 pj

u - u, xj xj

nb 1 = 2 - u,
j=1 pj

u - u, xj xj

nb
= -2

u 2 - u, xj 2

j=1 pj

nb
 -2

u 2 - u 2 xj 2

j=1 pj

=0

pj pj

Since the equality holds when u, xj 2 = u 2 xj 2 for all j, we can have strict inequality when all pi's are not on the same ray from the origin.

15

Under review as a conference paper at ICLR 2019

C.3 PROOF OF THEOREM 3

The proof of Theorem 3 is very similar to Lemma 2. Theorem 3. Let p1(wt0), p2(wt0), · · · , pnb (wt0) be d-dimensional vectors, and all pi(wt0)'s are not on the ray from the current location wt0. If

nb i=1

pi(wt0) - wt0 pi(wt0) - wt0

nb
-
i=1

g^i(wti-1) g^i(wti-1)



(6)

for sufficiently small  > 0, then there exists positive number  such that

nb pj (wt0) - wt0 - j=1 pj (wt0) - wt0 -
for all  (0, ].

nb i=1
nb i=1

g^i (wti-1 ) g^i (wti-1 ) g^i (wti-1 ) g^i (wti-1 )

<

nb pi(wt0) - wt0 i=1 pi(wt0) - wt0

(7)

Proof. Similarly, we regard wt0 as the origin 0. For simplicity, write pi(0) and g^i(wti-1) as pi and

g^i, respectively. Let f ( ) =

nb pj - j=1 pj -

nb i=1
nb i=1

pi pi pi pi

2 and f~( ) =

nb pj - j=1 pj -

nb i=1
nb i=1

g^i g^i |
g^i
g^i

2
.

Denote u = -

nb j=1

pi pi

,t=

nb i=1

pi pi

-

nb i=1

g^i g^i

and p~j( ) = pj + (u + t). Then

f~( ) =

nb p~j ( ) i=1 p~j ( )

2
.

Now we differentiate f~( ) with respect to , that is,

f~ ( ) = 2

nb

p~j( )

nb
,

p~j( ) (u + t) -

u+t,p~j ( p~j ( )

)

p~j (

)

.

j=1 p~j ( ) j=1

p~j ( ) 2

Recall that p~j(0) = pj. Rewrite

pj pj

= xj and use f (0) in the proof of Lemma 2

f~ (0) = 2

nb j=1

pj pj

nb
,
j=1

pj

(u + t) -

u+t,pj pj

pj

pj 2

=2

nb u + t - u + t, - u,

pj pj

j=1 pj

pj pj

nb 1 = 2 - u,
j=1 pj

u + t - u + t, xj xj

nb 1 = 2 - u,
j=1 pj

u - u, xj xj

nb
+ 2 - u,
j=1

nb
= f (0) - 2

1

j=1 pj

u, t - t, xj u, xj

1 pj

t - t, xj xj

Since f (0) < 0 by the proof of Lemma 2,

f~ (0) < 0



nb
2
j=1

1 pj

t, xj u, xj - u, t < |f (0)|.

16

Under review as a conference paper at ICLR 2019

By using xj = 1 and applying the Cauchy inequality,

nb
2
j=1

1 pj

t, xj u, xj - u, t

nb
2

t

xj

j=1 nb
=4
j=1

ut pj

 4nb u t minj pj

 4n2b  minj pj

u xj + u t pj

nb

u

xi = nb .

i=1

Define r = minj pj . If then

|f (0)|r  < 4nb2 ,
f~( ) < 0.

D PROOFS FOR COROLLARY 3.1

Corollary 3.1. Let pi's be local minibatch solutions for each fIi 's. Suppose that a region R satisfies:
For all w, w  R, pi(w) = pi(w ) = pi
for all i = 1, · · · , nb. Further, assume that Hessian matrices of fIi 's are positive definite, wellconditioned and bounded in the sense of matrix L2 norm on R. If SGD moves wt0 to wt0+1 on R with a large batch size and a small learning rate, then ^(wt0) > ^(wt0+1). Moreover, we can estimate ^(wt0) and ^(wt0+1) by minibatch gradients on wt0 and wt0+1, respectively.

Proof. Recall that wt0+1 = wt0 + 

nb i=1

g^i(wti-1)

where



is

a

learning

rate.

To

prove

Corollary

3.1, we need to show ^(wt0+1) < ^(wt0) which is equivalent to

nb pj (wt0+1) - wt0 -  j=1 pj (wt0+1) - wt0 - 

nb i=1

g^i(wti-1)

nb i=1

g^i(wti-1)

<

nb pi(wt0) - wt0 i=1 pi(wt0) - wt0

.

(8)

Since w2fIi (·) 2 is bounded on R, wfIi (·) is Lipschitz continuous on R(Bottou, 2010). If the batch size is sufficiently large and the learning rate  is sufficiently small, g^i(wti-1)  g^i(wt0)   for all i by Theorem 1. Therefore, we have

nb nb
 g^i(wti-1)   
i=1 i=1

g^i(wti-1) g^i(wti-1)

If we denote   as , we can convert (8) to (9).

nb pj (wt0+1) - wt0 - j=1 pj (wt0+1) - wt0 -

nb i=1
nb i=1

g^i (wti-1 ) g^i (wti-1 ) g^i (wti-1 ) g^i (wti-1 )

<

nb pi(wt0) - wt0 i=1 pi(wt0) - wt0

.

(9)

Since both wt0+1 and wt0 are in R for small learning rate, we have pi(wt0+1) = pi(wt0) = pi by the assumption. That is, (9) is equivalent to (7). In (7), g^i(wti-1)/ g^i(wti-1) cannot be replaced by (pi - wt0)/ pi - wt0 in general. Hence we introduce Definition D.1 and Lemma D.1 to connect the direction of the minibatch gradient with the corresponding local minibatch solution.

17

Under review as a conference paper at ICLR 2019

Definition D.1. The condition number c(A) of a matrix A is defined as

c(A) = max(A) min(A)

where max(A) and min(A) are maximal and minimal singular values of A, respectively. If A is positive-definite matrix, then
c(A) = max(A) . min(A)
Here max(A) and min(A) are maximal and minimal eigenvalues of A, respectively.

Lemma D.1. If the condition number of the positive definite Hessian matrix of fIi at the local minibatch solution, pi, denoted by Hi = w2fIi (pi) is approximately 1(well-conditioned), then the direction to pi from w is almost parallel to its negative gradient at w. That is, for all w  R,

pi - w - g^i(w)

pi - w

g^i(w)

0

where g^i(w) = -wfIi (w).

Proof. By the second order Taylor expansion,

fIi (w)



fIi (pi)

+

1 (w
2

-

pi)

Hi(w - pi).

Hence, g^i(w) = -wfIi (w)  -Hi(w - pi)
Denote pi - w as x. Then, we only need to show

x - Hix

2
0

x Hix

Note that Hi is positive definite, so we can diagonalize it as Hi = Pi iPi where Pi is an orthonormal transition matrix for Hi.

x - Hix x Hix

2
=2-2

x

Hix

x Hix

= 2 - 2 (Pix) iPix Pix Pi iPix

= 2 - 2 (Pix) iPix Pix iP x
 2 - 2 j j (Pix)j2 Pix iPix
 2 - 2 min Pix 2 Pix max Pix

= 2 - 2 min  0 max

Lemma D.1 proposed that well-conditioned Hessian matrix of fIi at pi makes g^i(w)/ g^i(w) be replaced by (pi - w)/ pi - w for all w  R. Using this, we prove Lemma D.2.

Lemma D.2. Let w be a parameter in R. If the condition number of Hessian matrix of fIi is

sufficiently near 1(well-conditioned) and

w-wt0 pi -wt0

is sufficiently near 0, then

pi - wt0 pi - wt0

-

g^i(w) g^i(w)

 nb

for sufficiently small .

18

Under review as a conference paper at ICLR 2019

Proof. We have

pi - wt0 pi - wt0

-

g^i(w) g^i(w)



pi - wt0 pi - wt0

-

g~(w) g~(w)

+



pi - wt0 pi - wt0

-

g~(w) g~(w)

+

+

pi - wt0 pi - wt0

-

pi - w pi - w

g~(w) - g^i(w) g~(w) g^i(w)

g~(w) g~(w)

-

pi - wt0 pi - wt0

+ pi - w - g^i(w)

pi - w

g^i(w)

++

2 1-

pi - wt0, pi - w pi - wt0 pi - w

+

=3 +

2 1-

pi - wt0 2 - pi - wt0, w - wt0 pi - wt0 pi - w

for sufficently small (See Lemma D.1). Now we only need to show

2 1-

pi - wt0 2 - pi - wt0, w - wt0 pi - wt0 pi - w

<

(10)

Since

w-wt0 pi -wt0

is sufficiently small, we have

pi - wt0 2 -

pi - wt0, w - wt0

= pi - wt0

pi - wt0 -

pi - wt0 pi - wt0

 pi - wt0 ( pi - wt0 - w - wt0 )

=

pi - wt0 2 1 -

w - wt0 pi - wt0

 0.

, w - wt0

By using the above non-negativeness, we have the following inequality.

1

pi - wt0 2 - pi - wt0, w - wt0 pi - wt0 pi - w



pi - wt0 2 - pi - wt0, w - wt0 pi - wt0 ( pi - wt0 + w - wt0 )

pi -wt0
= -1 + ,w-wt0

pi -wt0 pi -wt0
pi -wt0 w-wt0

w-wt0 w-wt0



1

-pi -wt0
w-wt0
+ pi-wt0
w-wt0

1

.

(11)

As

w-wt0 pi -wt0

 0+, (11) is monotonically increasing to 1. This implies that (10) holds for suffi-

ciently small .

With small learning rate, wti-1's are in R for all i  {1, . . . , nb}. As a result, by Lemma D.2, we

have

pi - wt0 pi - wt0

-

g^i(wti-1) g^i(wti-1)

 nb

(12)

for sufficiently small . This implies (6) since

nb i=1

pi - w pi - w

nb
-
i=1

g^i(wti-1) g^i(wti-1)

nb

i=1

pi - w pi - w

-

g^i(wti-1) g^i(wti-1)

.

19

Under review as a conference paper at ICLR 2019

Then we can apply Theorem 3 and ^(wt0) > ^(wt0+1) holds.

For the last statement, "Moreover, we can estimate ^(wt0) and ^(wt0+1) by minibatch gradients on wt0 and wt0+1, respectively.", recall that

^(wt0) = h

nb pi(wt0) - wt0 i=1 pi(wt0) - wt0

where h(·) is increasing and Lipschitz continuous(Lemma 1). By Lemma D.1, we have

pi(wt0) - wt0 pi(wt0) - wt0

-

g^i(wt0) g^i(wt0)

for sufficiently small  > 0. Therefore,

 <
nb

nb pi(wt0) - wt0 i=1 pi(wt0) - wt0

-

nb g^i(wt0) i=1 g^i(wt0)



nb i=1

pi(wt0) - wt0 pi(wt0) - wt0

nb
-
i=1

g^i(wt0) g^i(wt0)

where rhs is bounded by . Hence, Lipschitz continuity of h(·) implies that

h

nb pi(wt0) - wt0 i=1 pi(wt0) - wt0

-h

nb g^i(wt0) i=1 g^i(wt0)

0

as   0. That is,

^(wt0)  h

nb g^i(wt0) i=1 g^i(wt0)

.

Since t is arbitrary, we can apply this for all other w  R including wt0+1.

E EXPERIMENTAL DETAILS

E.1 MODEL ARCHITECTURE
For all cases, their weighted layers do not have biases, and dropout (Srivastava et al., 2014) is not applied. We use Xavier initializations(Glorot & Bengio, 2010) and cross entropy loss functions for all experiments.
FNN The FNN is a fully connected network with a single hidden layer. It has 800 hidden units with ReLU (Nair & Hinton, 2010) activations and a softmax output layer.
DFNN The DFNN is a fully connected network with three hidden layers. It has 800 hidden units with ReLU activations in each hidden layers and a softmax output layer.
CNN The network architecture of CNN is introduced in He et al. (2016) as a CIFAR-10 plain network. The first layer is 3 × 3 convolution layer and the number of output filters are 16. After that, we stack of {4, 4, 4} layers with 3 × 3 convolutions on the feature maps of sizes {32, 16, 8} and the numbers of filters {16, 32, 64}, respectively. The subsampling is performed with a stride of 2. All convolution layers are activated by ReLU and the convolution part ends with a global average pooling(Lin et al., 2013), a 10-way fully-conneted layers, and softmax. Note that there are 14 stacked weighted layers.
+BN We apply batch normalization right before the ReLU activations on all hidden layers.
+Res The identity skip connections are added after every two convolution layers before ReLU nonlinearity (After batch normalization, if it is applied on it.).

E.2 DATA AUGMENTATION
We use neither data augmentations nor preprocessings except scaling pixel values into [0, 1] both MNIST and CIFAR-10.

20

normSNR

normSNR

Under review as a conference paper at ICLR 2019

F OTHER FOUR TRAINING RUNS IN FIGURE 6
We show plots from other four training runs in Figure 6. For all runs, the curves of GS (inverse of SNR) and  are strongly correlated while GNS (inverse of normSNR) is less correlated to GS.

Ratio

1.00 0.75 0.50 0.25 0.00
1

5 20 Epoch

80

(a) CNN, initialization 1

1.00

0.75

0.50

0.25

0.00 1

5 20 Epoch

80

(c) CNN+BN, initialization 1

106

200

5 × 105
 2 × 105
105 5 × 104

qqq q qq qq qqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q

0.1 0.2

0.5 1.0

SNR

(b) CNN, initialization 1

q
q q

10

5

2 1 2.0

200

8 7

× ×

110044

6 × 104

5 × 104

4 × 104

 3 × 104

2 × 104

q q

qq qqq

q

q

q qqqqq qqq

qq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q q

q qq
q

qqqqqqqqqqqqqqqqqqqqq

qq q qqqqqqqqqqqqq q

qqq

qq qqqqqqqqqqqqqqq

q

q qq q

q qqqqq

qqqqqqq
qqqqqqq qqqqqq qqqqqqqqqqqqq qqq qqqqqqqqq

0.05 0.10 SNR

0.20

(d) CNN+BN, initialization 1

10.0 5.0
2.0 1.0 0.5

Ratio

Ratio

1.00

2 × 105

q
q 10.0

0.75

0.50

0.25

0.00 1

5 20 Epoch

80

(e) CNN+Res+BN, initialization 1

200

105

5 × 104
2 × 104

q qqqqqq

qq

q qq

qq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q

qqqqqqq

q qqqqqq q qqqqqqqqqqq qq
q q

qq

q
q qqqqqqqqqqqqqqqqqqqqqqqqqqqqq

0.05

0.10 0.20 SNR

0.50

(f) CNN+Res+BN, initialization 1

5.0
2.0 1.0 0.5

 Train loss

normSNR Valid loss

SNR

q SNR- SNR-normSNR

Figure 7: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid loss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and directional uniformity . We normalized each quantity by its maximum value over training for easier comparison on a single plot. In all the cases, SNR (orange) and  (red) are almost entirely correlated with each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggest that the SNR is largely driven by the directional uniformity.

normSNR

21

Under review as a conference paper at ICLR 2019

normSNR

normSNR

Ratio

1.00 0.75 0.50 0.25 0.00
1

5 20 Epoch

80

(a) CNN, initialization 2

200

2 × 105
 105
5 × 104

qqq qq

qqqq qqqqqqq

qqq

q

qq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q

q

qq

q

qqqqq qq

q qqq

qqqqqqq

q q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqq

q q

qqqq qq

q

0.1 0.2

0.5

SNR

(b) CNN, initialization 2

q
q q

10

5

2 1

Ratio

1.00

0.75

0.50

0.25

0.00 1

5 20 Epoch

80

(c) CNN+BN, initialization 2

200

105 5 × 104

2 × 104

qq q

qqq q q qq

qq

q q q qqqqqqqqqqqqqqqqqqqqq qqqqqqq

qqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qq q

q q q qqqqqqqqqq q q

qqqqqq qq q q

q qqq

qq

q

qqqqqqqqqqqqqqqqqqq q

q

q
10.0 5.0
2.0 1.0 0.5

0.05 0.10 0.20 SNR

0.50

(d) CNN+BN, initialization 2

Ratio

1.00

0.75

0.50

0.25

0.00 1

5 20 Epoch

80

(e) CNN+Res+BN, initialization 2

2 × 105

q
10.0

200

105

5 × 104
2 × 104

q

q q

qq

qq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqq

qqq

q q

qqqqqqqqqqqqqqq

q qq qqqqqqqqqqqq qqqqqqqq

0.05

0.10 0.20 SNR

0.50

(f) CNN+Res+BN, initialization 2

5.0
2.0 1.0 0.5

 Train loss

normSNR Valid loss

SNR

q SNR- SNR-normSNR

Figure 8: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid loss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and directional uniformity . We normalized each quantity by its maximum value over training for easier comparison on a single plot. In all the cases, SNR (orange) and  (red) are almost entirely correlated with each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggest that the SNR is largely driven by the directional uniformity.

22

normSNR

Under review as a conference paper at ICLR 2019

normSNR

normSNR

Ratio

1.00 0.75 0.50 0.25 0.00
1

5 20 Epoch

80

(a) CNN, initialization 3

200

106 5 × 105
 2 × 105
105 5 × 104

qq q

qqqq qqqq

q qq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qqqqqqqqqqq

qqqqqqqq

q

q qqq

0.1 0.2

0.5 1.0

SNR

(b) CNN, initialization 3

q 10

q qq

5

2 1 2.0

Ratio

1.00

0.75

0.50

0.25

0.00 1

5 20 Epoch

80

(c) CNN+BN, initialization 3

200

6 × 104 5 × 104 4 × 104
 3 × 104
2 × 104

qqq qqqqqqqqqqqqqqqqqqqqq

q q

q qqqqqq qq

q qq

q qqqqqqq

qq qq

q

qq q

qqqqqqqq

qqq q

qq qq qq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q

qq

qq

q qq

qq

q q

q
q q
q qq qq

qq q

qqq

q qqqqq

qqq

qq

qqqqqq

qqqq

qq q

q qq q

qq

0.05 0.10 SNR

0.20

(d) CNN+BN, initialization 3

10.0 5.0
2.0 1.0 0.5

Ratio

1.00

0.75

0.50

0.25

0.00 1

5 20 Epoch

80

(e) CNN+Res+BN, initialization 3

200

2 × 105 105
 5 × 104
2 × 104

q

qq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q

qqqqqq q

qq

q

qq

q

q qq
qqqqqqqqq qq

qqq qqqqqqqqqqqqqqqqqqq

qqqq

0.05

0.10 0.20 SNR

0.50

(f) CNN+Res+BN, initialization 3

10.0 5.0
2.0 1.0 0.5

 Train loss

normSNR Valid loss

SNR

q SNR- SNR-normSNR

Figure 9: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid loss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and directional uniformity . We normalized each quantity by its maximum value over training for easier comparison on a single plot. In all the cases, SNR (orange) and  (red) are almost entirely correlated with each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggest that the SNR is largely driven by the directional uniformity.

23

normSNR

Under review as a conference paper at ICLR 2019

normSNR

normSNR

Ratio

1.00 0.75 0.50 0.25 0.00
1

5 20 Epoch

80

(a) CNN, initialization 4

200

5 × 105

2 × 105
 105

5 × 104

q

q 10.0

qqqqqq q

qqqqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqq qqqqq

q q q q q

5.0 2.0 1.0

2 × 104

q qqqqq

0.5

0.1 0.2

0.5 1.0

SNR

(b) CNN, initialization 4

Ratio

1.00

0.75

0.50

0.25

0.00 1

5 20 Epoch

80

(c) CNN+BN, initialization 4

200

105
5 × 104

2 × 104

q qq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q

qqqqqqqqqqqqqqqqqqqqqqqqq

qq qqqq

q

q qqq q q qq qq

q qqqqqqq qqqqqqqqqqqqqqqqqqqqqqq qqqqqq
qqqqqqqqqqqqqqqqq q

q

0.05

0.10 SNR

0.20

q q

(d) CNN+BN, initialization 4

10.0 5.0
2.0 1.0 0.5

Ratio

1.00

0.75

0.50

0.25

0.00 1

5 20 Epoch

80

(e) CNN+Res+BN, initialization 4

2 × 105

q
10.0

200

105

5 × 104
2 × 104

qqqqqqqqqqqqqqqqqqq

qq qqqqqqqqqq

q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qqqqqqqqqqqqqqqqqq
qq

qqqqqqqq

q

qq

0.05

0.10 0.20 SNR

0.50

(f) CNN+Res+BN, initialization 4

5.0
2.0 1.0 0.5

 Train loss

normSNR Valid loss

SNR

q SNR- SNR-normSNR

Figure 10: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid loss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and directional uniformity . We normalized each quantity by its maximum value over training for easier comparison on a single plot. In all the cases, SNR (orange) and  (red) are almost entirely correlated with each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggest that the SNR is largely driven by the directional uniformity.

24

normSNR

