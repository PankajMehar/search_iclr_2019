Under review as a conference paper at ICLR 2019
STABILITY OF STOCHASTIC GRADIENT METHOD WITH MOMENTUM FOR STRONGLY CONVEX LOSS FUNCTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
While momentum-based methods, in conjunction with the stochastic gradient descent, are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In practice, the momentum parameter is often chosen in a heuristic fashion with little theoretical guidance. In this work we use the framework of algorithmic stability to provide an upper-bound on the generalization error for the class of strongly convex loss functions, under mild technical assumptions. Our bound decays to zero inversely with the size of the training set, and increases as the momentum parameter is increased. We also develop an upper-bound on the expected true risk, in terms of the number of training steps, the size of the training set, and the momentum parameter.
1 INTRODUCTION
A fundamental issue for any machine learning algorithm is its ability to generalize from the training dataset to the test data. A classical framework used to study the generalization error in machine learning is PAC learning (Vapnik and Chervonenkis, 1971; Valiant, 1984). However, the associated bounds using this approach can be conservative. Recently, the notion of uniform stability, introduced in the seminal work of Bousquet and Elisseeff (Bousquet and Elisseef, 2002), is leveraged to analyze the generalization error of the stochastic gradient method (SGM) (Hardt et al., 2016). The result in (Hardt et al., 2016) is a substantial step forward, since SGM is widely used in many practical systems. This method is scalable, robust, and widely adopted in a broad range of problems.
To accelerate the convergence of SGM, a momentum term is often added in the iterative update of the stochastic gradient (Goodfellow et al., 2016). This approach has a long history, with proven benefits in various settings. The heavy-ball momentum method was first introduced by Polyak (Polyak, 1964), where a weighted version of the previous update is added to the current gradient update. Polyak motivated his method by its resemblance to a heavy ball moving in a potential well defined by the objective function. Momentum methods have been used to accelerate the backpropagation algorithm when training neural networks (Rumelhart et al., 1986). Intuitively, adding momentum accelerates convergence by circumventing sharp curvatures and long ravines of the sublevel sets of the objective function (Wilson et al., 2018). For example, Ochs et al. has presented an illustrative example to show that the momentum can potentially avoid local minima (Ochs et al., 2015). Nesterov has proposed an accelerated gradient method, which converges as O(1/k2) where k is the number of iterations (Nesterov, 1983). However, the Netstrov momentum does not seem to improve the rate of convergence for stochastic gradient (Goodfellow et al., 2016, Section 8.3.3). In this work, we focus on the heavy-ball momentum.
Although momentum methods are well known to improve the convergence in SGM, their effect on the generalization error is not well understood. In this work, we first build upon the framework in (Hardt et al., 2016) to obtain a bound on the generalization error of SGM with momentum (SGMM) for the case of strongly convex loss functions. Our bound is independent of the number of training iterations and decreases inversely with the size of the training set. Secondly, we develop an upper-bound on the optimization error, which quantifies the gap between the empirical risk of SGMM and the global optimum. Our bound can be made arbitrarily small by choosing sufficiently many iterations and a sufficiently small learning rate. Finally, we establish an upper-bound on the
1

Under review as a conference paper at ICLR 2019

expected true risk of SGMM as a function of various problem parameters. We note that the class of strongly convex loss functions appears in several important machine learning problems, including linear and logistic regression with a weight decay regularization term.
Other related works: convergence analysis of first order methods with momentum is studied in (Nesterov, 1983; Ochs et al., 2014; Su et al., 2014; Ghadimi et al., 2015; Lessard et al., 2016; Yang et al., 2016; Loizou and Richta´rik, 2018; Gadat et al., 2016). Most of these works consider the deterministic setting for gradient update. Only a few works have analyzed the stochastic setting (Yang et al., 2016; Loizou and Richta´rik, 2018; Gadat et al., 2016). Our convergence analysis results are not directly comparable with these works due to their different assumptions regarding the properties of loss functions. In particular, we analyze the convergence of SGMM for a smooth and strongly convex loss function as in (Hardt et al., 2016), which is new.
Our main focus in this work is on the generalization, and hence true risk, of SGMM. We are aware of only one similar work in this regard, which provides stability bounds for quadratic loss functions (Chen et al., 2018). In this paper, we obtain stability bounds for the general case of strongly convex loss functions. In addition, unlike (Chen et al., 2018), our results show that machine learning models can be trained for multiple epochs of SGMM with bounded generalization errors.
Notation: We use E[·] to denote the expectation and · to represent the Euclidean norm of a vector. We use lower-case bold font to denote vectors. We use sans-serif font to denote random quantities. Sets and scalars are represented by calligraphic and standard fonts, respectively.

2 GENERALIZATION ERROR AND STABILITY

We consider a general supervised learning problem, where S = {z1, · · · , zn} denotes the set of samples of size n drawn i.i.d. from some space Z with an unknown distribution D. We assume a
learning model described by parameter vector w. Let f (w; z) denote the loss of the model described by parameter w on example z  Z. Our ultimate goal is to minimize the true or population risk:

R(w) = EzDf (w; z).

(1)

Since the distribution D is unknown, we replace the objective by the empirical risk, i.e.,

RS (w)

=

1 n

n

f (w; zi).

i=1

(2)

We assume w = A(S) for a potentially randomized algorithm A(·). In order to find an upper-bound on the true risk, we consider the generalization error, which is the expected difference of empirical and true risk:

g = ES,A[R(A(S)) - RS(A(S))].

(3)

Finally, to upper bound g, we consider uniform stability:

Definition 1 Let S and S denote two data sets from space Zn such that S and S differ in at most one example. Algorithm A is s-uniformly stable if for all data sets S, S , we have

sup EA[f (A(S); z) - f (A(S ); z)]  s.
z

(4)

It is shown in (Hardt et al., 2016) that uniform stability implies generalization in expectation:

Theorem 1 (Hardt et al., 2016) If A is an s-uniformly stable algorithm, then the generalization error of A is upper-bounded by s.

Theorem 1 shows that it is enough to control the uniform stability of an algorithm to upper bound the generalization error.

2.1 ASSUMPTIONS ON THE LOSS FUNCTION In our analysis, we will assume that the loss function satisfies the following properties.

2

Under review as a conference paper at ICLR 2019

Definition 2 A function f :   R is L-Lipschitz if for all u, v   we have |f (u) - f (v)|  L u-v .

Definition 3 A function f :   R is -smooth if for all u, v   we have f (u) - f (v)   u-v .

Definition 4 A function f :   R is -strongly convex if for all u, v   we have f (u) 

f (v)

+

f (v)T (u

-

v)

+

 2

u-v

2.

We assume that the parameter space  is a convex set. Furthermore, for the loss function to be L-Lipschitz and and strongly convex, we further assume that  is compact. Since  is compact, the SGMM update requires projection.

2.2 STOCHASTIC GRADIENT METHOD WITH MOMENTUM

The update rule for projected SGMM is given by:

wt+1 = P wt + µ(wt - wt-1) - wf (wt; zit )

(5)

where P denotes the Euclidean projection onto ,  > 0 is the learning rate1, µ > 0 is the momen-
tum parameter, it is a randomly selected index, and f (wt; zit ) is the loss evaluated on sample zit . In SGMM, we run the update (5) iteratively for T steps and let wT denote the final output. Note that there are two typical approaches to select it. The first approach is to select it  {1, · · · , n} uniformly at random at each iteration. The second approach is to permutate {1, · · · , n} randomly once and
then select the examples repeatedly in a cyclic manner. Our results are valid for both approaches.
The key quantity of interest in this paper is the generalization error for SGMM given by:

g = ES,A[R(wT ) - RS (wT )] = ES,i0,··· ,iT -1 [R(wT ) - RS (wT )] since the randomness in A arises from the choice of i0, · · · , iT -1.

3 MAIN RESULTS

In the following, we assume that the loss function f (·; z) is -smooth, L-Lipschitz, and -strongly convex for all z.

Theorem 2 (Stability bound) Suppose that the SGMM update (5) is executed for T steps with con-

stant learning rate  and momentum µ.

Provided that

 +

-

1 2



µ

<

 3(+)

and 



2 +

,

SGMM satisfies s-uniform stability where

s

n

2L2( + )  - 3µ( + )

.

(6)

The result in Theorem 2 implies that the stability bound decreases inversely with the size of the training set. It increases as the momentum parameter µ increases. These properties are also verified in our experimental evaluation.

Theorem 3 (Convergence bound) Suppose that the SGMM update (5) is executed for T steps with constant learning rate  and momentum µ. Then we have

ES,A[RS(w^ T ) - RS(wS )] 

µW0 (1 - µ)T

+

(1 - µ)W1 2T

-

W2 2

-

µW3 2(1 - µ)

+

L2 2(1 - µ)

(7)

where w^ T

denotes the average of T

steps of the algorithm, i.e., w^ T

=

1 T +1

T t=0

wt,

RS (w)

=

1 n

n i=1

f

(w;

zi),

wS

= arg minw RS(w), W0

= ES,A[RS(w0) - RS(wT )], W1

= ES,A[

w0 -

wS

2], W2 = ES,A[

w^ T - wS

2],

and

W3

=

1 T +1

T t=0

ES

,A

[

wt - wt-1

2].

1In the following, we assume  is constant over time. In practice, time-decaying  is used in some applications. Please note that our results hold for time-decaying .

3

Under review as a conference paper at ICLR 2019

Theorem 3 bounds the optimization error, i.e., the expected difference between the empirical risk achieved by SGMM and the global minimum. Upon setting µ = 0 and  = 0 in (7), we can recover the classical bound on optimization error for SGM (Nemirovski and Yudin., 1983), (Hardt et al., 2016, Theorem 5.2). The first two terms in (7) vanish as T increases. The terms with negative sign improve the convergence due to the strongly convexity. The last term depends on the learning rate, , the momentum parameter µ, and the Lipschitz constant L. This term can be controlled by selecting  sufficiently small.

Proposition 1 (Upper-bound on true risk) Suppose that the SGMM update (5) is executed for T steps with constant learning rate  and momentum µ, satisfying the conditions in Theorem 2. Then,

for

sufficiently

small

µ

and

setting



=

1-µ L

W1 T

,

we

have:

ES,A[R(w^ T )]  ES,A[RS(wS )] +

µW0 (1 - µ)T

+L

W1 - W2 -

µW3

2L2( + ) +

T 2 2(1 - µ) nC

(8)



where

C

=

1

-

3µL(+) T (1-µ) W1

and

w^ T

as

well

as

the

constants

W0, · · ·

, W3

are

defined

in

Theorem

3.

Proposition 1 provides a bound on the expected true risk of SGMM in terms of the global minimum of the empirical risk. The bound in (8) is obtained by combining Theorem 2 and Theorem 3 and minimizing the expression over . The choice of  simplifies considerably when µ is sufficiently small, as stated in Proposition 1. Due to the page constraint, the proof of this result is provided in the supplementary material. Note that the first two terms in (8) vanish as T increases. The last term in (8) vanishes as the number of samples n increases.

4 PROOF OF THEOREM 2 (STABILITY BOUND)

Following (Hardt et al., 2016), we track the divergence of two different iterative sequences of update rules with the same starting point. However, our analysis is more involved as the presence of momentum term requires a more careful bound on the iterative expressions.
To keep the notation uncluttered, we first consider SGMM without projection and defer the discussion of projection to the end of this proof. Let S = {z1, · · · , zn} and S = {z1, · · · , zn} be two samples of size n that differ in at most one example. Let wT and wT denote the outputs of SGMM on S and S , respectively. We consider the updates wt+1 = Gt(wt) + µ(wt - wt-1) and wt+1 = Gt(wt) + µ(wt - wt-1) with Gt(wt) = wt - wf (wt; zit ) and Gt(wt) = wt - wf (wt; zit ), respectively, for t = 1, · · · , T . We denote t = wt - wt . Suppose w0 = w0, i.e., 0 = 0. We first establish an upper-bound on EA[t+1] in terms of EA[t] and EA[t-1] in the following lemma, whose proof is provided in the supplementary document.

Lemma 1

Provided that  

2 +

,

an

upper-bound

on

EA[t+1]

is

given

by

EA[t+1] 

1 + µ -  +

2L

EA[t] + µEA[t-1] +

. n

(9)

Using the result of Lemma 1, in the following, we develop an upper bound on EA[T ]. Let us

consider the recursion

EA[~t+1] =

1 + µ -  +

EA[~t]

+

µEA[~t-1]

+

2L n

(10)

with ~0 = 0 = 0. Upon inspecting (10) it is clear that

EA[~t] 

1 + µ -  +

EA[~t-1],

t  1,

(11)

as we simply drop the remainder of positive terms. Substituting (11) into (10), we have

EA[~t+1] 

1+µ+

µ 1+µ-

 +

-

 +

EA[~t]

+

2L n



1 + 3µ -  +

EA[~t]

+

2L n

(12)

4

Under review as a conference paper at ICLR 2019

where

the

second

inequality

holds

due

to

µ



 +

-

1 2

.

Noting that EA[~t]  EA[t] for all t including T , we have

EA[T ]



2L n

T

1 + 3µ - 

t


2L( + )

 +  n  - 3µ( + )

t=1

where

the

second

expression

holds

since

0



µ

<

 3(+)

is

assumed.

Applying the L-Lipschitz property on f (·, z), it follows that

(13)

EA[|f (wT ; z) - f (wT ; z)|]  LEA[T ] 

n

2L2( + )  - 3µ( + )

.

(14)

Since this bound holds for all S, S and z, we obtain an upper-bound on the uniform stability and the proof is complete. Our stability bound in Theorem 2 holds for the projected SGMM update (5) because Euclidean projection does not increase the distance between projected points (the argument is essentially analogous to (Hardt et al., 2016, Lemma 4.6)). In particular, note that Lemma 1 holds for the projected SGMM.

5 PROOF OF THEOREM 3 (CONVERGENCE BOUND)

Again, we first consider SGMM without projection and discuss the extension to projection at the end

of this proof. Our proof is inspired by the convergence analysis in (Yang et al., 2016; Ghadimi et al.,

2015) for a convex loss function with bounded variance and time-decaying learning rate. Different

from these works, we analyze the convergence of SGMM for a smooth and strongly convex loss

function with constant learning rate. To facilitate the convergence analysis, we define:

pt

=

1

µ -

µ (wt

-

wt-1)

(15)

with p0 = 0. Substituting into the SGMM update, the parameter recursion is given by 
wt+1 + pt+1 = wt + pt - 1 - µ wf (wt; zit )

(16)

It follows that

wt+1 + pt+1 - w 2 = wt + pt - w 2 +

 1-µ

2

wf (wt; zit )

2

-

2 1 - µ (wt

+

pt

-

w)T

wf (wt;

zit ).

(17)

Substituting pt (15) into (17), the recursion (16) can be written as

wt+1 + pt+1 - w 2 = wt + pt - w 2 +

 1-µ

2

wf (wt; zit )

2

-

2µ (1 - µ)2 (wt

-

wt-1)T

wf (wt;

zit )

-

2 1 - µ (wt

-

w)T

wf (wt;

zit ).

(18)

Upon taking the expectation with respect to it in (18) we have

Eit wt+1 + pt+1 - w 2 

wt + pt - w 2 +

 1-µ

2L2

-

2µ (1 - µ)2 (wt

-

wt-1)T wRS (wt)

-

2 1 - µ (wt

-

w)T

w RS (wt )

(19)

where we use the fact that wf (wt; zit )  L, due to L-Lipschitz, and that Eit [wf (wt; zit )] = wRS(wt). Furthermore, since RS(·) is a -strongly convex function, for all wt and wt-1, we

have

(wt

-

w)T

w RS (wt )



RS (wt )

-

RS (w)

+

 2

wt - w

2,

(wt

-

wt-1)T

w RS (wt )



RS (wt )

-

RS (wt-1)

+

 2

wt - wt-1

2.

(20)

5

Under review as a conference paper at ICLR 2019

Substituting (20) in (19), we have

Eit [|wt+1 + pt+1 - w

2] 

wt + pt - w

2 -  1-µ

wt - w

2 - 2µ (1 - µ)2

RS (wt) - RS (wt-1)

-

2 1-µ

RS(wt) - RS(w)

2L2

µ

+ (1 - µ)2 - (1 - µ)2

wt - wt-1

2.

(21)

Taking expectation over i0, · · · , it for a given S, summing (21) for t = 0, · · · , T , and rearranging terms, we have

2 1-µ

T

EA [RS (wt )

-

RS (w)]



2µ (1 - µ)2 EA[RS(w0)

-

RS(wT )]

-

 1-µ

T

EA[ wt - w 2]

t=0 t=0

- µ (1 - µ)2

T

EA[

wt - wt-1

2] + EA[

w0 - w

2] +

2L2(T + 1) (1 - µ)2 .

t=0

(22)

Since · is a convex function, for all wT and w, we have

w^ T

-w

2



T

1 +1

T

wt - w 2.

t=0

(23)

Furthermore, due to convexity of RS(·), we have

RS(w^ T ) -

RS (w)



T

1 +1

T

RS(wt) - RS(w) .

t=0

(24)

Taking expectation over S, applying inequalities (23) and (24) into (22), and substituting w = wS, we obtain (7) and the proof is complete.

Our convergence bound in Theorem 3 can be extended to projected SGMM (5). Let use denote
yt+1 = wt + µ(wt - wt-1) - wf (wt; zit ). Then, for any feasible w  , (17) holds for yt+1, i.e.,

yt+1 +

1

µ -

µ

(yt+1

-

wt)

-

w

2=

wt + pt - w

2+

 1-µ

2

wf (wt; zit )

2

-

2 1 - µ (wt

+

pt

-

w)T wf (wt; zit ).

(25)

Note that the LHS of (25) can be written as

yt+1

+

1

µ -

µ (yt+1

-

wt)

-

w

2

=

1

1 -

µ

yt+1 -

µwt + (1 - µ)w

.

We note that µwt + (1 - µ)w   for any w   and wt   since  is convex. Now in projected SGMM, we have

wt+1 - µwt + (1 - µ)w 2 = P(yt+1) - µwt + (1 - µ)w 2  yt+1 - µwt + (1 - µ)w 2

(26)

since projection a point onto  moves it closer to any point in . This shows inequality (19) holds, and the convergence results do not change.

6

Under review as a conference paper at ICLR 2019

test error - train error

0.7

= 0.9 = 0.5

0.6 = 0

0.5

0.4

0.3

0.2

0.1

0.0 0 100 200 300 400 500 n (number of training samples)

train error

0.10 0.08 0.06 0.04 0.02 0.00 0

= 0.9 = 0.5 =0
100 200 300 400 n (number of training samples)

500

(a) Generalization error with respect to cross entropy

(b) Training error (cross entropy)

Figure 1: Generalization performance (cross entropy) of logistic regression for notMNIST dataset with T = 1000 iterations and minibatch size 10.

train accuracy - test accuracy

0.175 0.150 0.125 0.100 0.075 0.050 0.025 0.000 0

= 0.9 = 0.5 =0

100 200 300 400 n (number of training samples)

500

train accuracy

1.000 0.995 0.990 0.985 0.980 0.975 0.970
0

(a) Generalization error with respect to classification accuracy

= 0.9 = 0.5 =0
100

200 300 400 n (number of training samples)

(b) Training accuracy

500

Figure 2: Generalization performance (classification accuracy) of logistic regression for notMNIST dataset with T = 1000 iterations and minibatch size 10.

6 EXPERIMENTAL EVALUATION
In this section, we validate the insights obtained in our theoretical results in experimental evaluation. Our main goal is to study how adding momentum affects the convergence and generalization of SGM. We study the performance of SGMM when applied to the notMINIST dataset. Please note that similar results are provided for the MNIST dataset in the supplementary document. We train a logistic regression model with the weight decay regularization using SGMM for binary classification on the two-class notMNIST dataset that contains the images from letter classes "C" and "J", which leads to a smooth and strongly convex loss function. We set the learning rate  = 0.01. The weight decay coefficient and the minibatch size are set to 0.001 and 10, respectively. We use 100 SGMM realizations to evaluate the average performance. We compare the training and generalization performance of SGM without momentum with that of SGMM under µ = 0.5 and µ = 0.9, which are common momentum values used in practice (Goodfellow et al., 2016, Section 8.3.2).
The generalization error (with respect to cross entropy) and training error versus the number of training samples, n, under SGMM with fixed T = 1000 iterations are shown in Figures 1a and 1b, respectively, for µ = 0, 0.5, 0.9. In Figures 2a and 2b, we plot the generalization error (with respect to classification accuracy) and the training accuracy as a function of the number of training samples for the same dataset. First, we observe that the generalization error (with respect to both cross entropy and classification accuracy) decreases as n increases for all values of µ, which is suggested by our stability upper-bound in Theorem 2. In addition, for sufficiently large n, we observe that the generalization error increases with µ, consistent with Theorem 2. On the other hand, the training error increases as n increases, which is expected. We can observe that adding momentum reduces training error as it improves the convergence rate. The training accuracy also improves by adding momentum as illustrated in Fig. 2b.
7

Under review as a conference paper at ICLR 2019

train error

= 0.9

3.0

= 0.5 =0

2.5

2.0

1.5

1.0

0.5

0.00.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 epoch

test error

3.5 = 0.9

3.0

= 0.5 =0

2.5

2.0

1.5

1.0

0.5

0.00.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 epoch

(a) Training error versus epoch

(b) Test error versus epoch

Figure 3: Training and test error of logistic regression (cross entropy loss) for notMNIST dataset with n = 500 training samples and minibatch size 10.

train accuracy

1.0

0.8

0.6

0.4

0.2

= 0.9 = 0.5

=0

0.00.0 2.5 5.0 7.5 1e0p.0och 12.5 15.0 17.5 20.0

test accuracy

1.0 0.8 0.6 0.4 0.2 = 0.9
= 0.5 =0 0.00.0 2.5 5.0 7.5 1e0p.0och 12.5 15.0 17.5 20.0

(a) Training accuracy versus epoch

(b) Test accuracy versus epoch

Figure 4: Training and test accuracy of logistic regression for notMNIST dataset with n = 500 training samples and minibatch size 10.

In order to study the optimization error of SGMM, we show the training error and test error versus the number of epochs, under SGMM trained with n = 500 samples in Figures 3a and 3b, respectively. We plot the classification accuracy for training and test datasets in Figures 4a and 4b, respectively. We observe that the training error decreases as the number of epochs increases for all values of µ, which is consistent with the convergence analysis in Theorem 3. Furthermore, as expected, we see that adding momentum improves the training error and accuracy. However, as the number of epochs increases, we note that the benefit of momentum on the test error and accuracy becomes negligible. This happens because adding momentum also results in a higher generalization error thus penalizing the gain in training error.
7 CONCLUSIONS
We study the generalization error and convergence of SGMM for the class of strongly convex loss functions, under mild technical conditions. We establish an upper-bound on the generalization error, which decreases with the size of the training set, and increases as the momentum parameter is increased. Secondly, we analyze the convergence of SGMM during training, by establishing an upper-bound on the gap between the empirical risk of SGMM and the global minimum. Our proposed bound reduces to a classical bound on the optimization error of SGM (Nemirovski and Yudin., 1983) for convex functions, when the momentum parameter is set to zero. Finally, we establish an upper-bound on the expected difference between the true risk of SGMM and the global minimum of the empirical risk, and illustrate how it scales with the number of training steps and the size of the training set. Although our results are established for the case when the learning rate is constant, they can be easily extended to the case when the learning rate decreases with the number of iterations. We also present experimental evaluations on the notMNIST dataset and show that the numerical plots are consistent with our theoretical bounds on the generalization error and the convergence gap.
8

Under review as a conference paper at ICLR 2019
REFERENCES
V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, 16(2):264­280, 1971.
L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134­1142, 1984. O. Bousquet and A. Elisseef. Stability and generalization. Journal of Machine Learning Research,
2:499­526, 2002. M. Hardt, B. Recht, and Y Singer. Train faster, generalize better: Stability of stochastic gradient
descent. Preprint available at arXiv:1509.01240v2, 2016. I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Cambridge, USA: The MIT Press,
2016. B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 4(5):1­17, 1964. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by backpropagating
errors. Nature, 323:533­536, 1986. A. C. Wilson, B. Recht, and M. I. Jordan. A lyapunov analysis of momentum methods in optimiza-
tion. Preprint available at arXiv:1611.02635v4, 2018. P. Ochs, T. Brox, and T. Pock. IPIASCO: Inertial proximal algorithm for strongly convex optimiza-
tion. Journal of Mathematical Imaging and Vision, 53(2):171­181, 2015. Y. Nesterov. A method of solving a convex programming problem with convergence O(1/k2).
Soviet Mathematics Doklady, 27(2):372­376, 1983. P. Ochs, Y. Chen, T. Brox, and T. Pock. IPIANO: Inertial proximal algorithm for nonconvex opti-
mization. SIAM Journal on Imaging Sciences, 7(2):1388­1419, 2014. W. Su, S. Boyd, and E. Cande`s. A differential equation for modeling Nesterov's accelerated gradi-
ent method: Theory and insights. In Proc. Advances in Neural Information Processing Systems (NIPS), 2014. E. Ghadimi, H. R. Feyzmahdavian, and M. Johansson. Global convergence of the heavy-ball method for convex optimization. In Proc. European Control Conference (ECC), Linz, Austria, 2015. L. Lessard, B. Recht, and A Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57­95, 2016. T. Yang, Q. Lin, and Z Li. Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization. Preprint available at arXiv:1604.03257v2, 2016. N. Loizou and P. Richta´rik. Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods. Preprint available at arXiv:1712.09677v2, 2018. S. Gadat, F. Panloup, and S. Saadane. Stochastic heavy ball. Preprint available at arXiv:1609.04228v2, 2016. Y. Chen, C. Jin, and B. Yu. Stability and convergence trade-off of iterative optimization algorithms. Preprint available at arXiv:1804.01619, 2018. A. Nemirovski and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley Interscience, 1983.
9

Under review as a conference paper at ICLR 2019

SUPPLEMENTARY DOCUMENT FOR STABILITY OF STOCHASTIC GRADIENT METHOD WITH MOMENTUM FOR STRONGLY CONVEX LOSS FUNCTIONS
Anonymous authors Paper under double-blind review

1 LEMMA 1 AND ITS PROOF (STABILITY BOUND)

Lemma 1

Provided that  

2 +

,

an

upper-bound

on

EA[t+1]

is

given

by

EA[t+1] 

1 + µ -  +

2L

EA[t] + µEA[t-1] +

. n

(A.1)

Proof: At step t, with probability 1 - 1/n, the example is the same in both S and S , i.e., zit = zit ,

which implies Gt = Gt. Then Gt becomes

1

-

 +

-expansive

for





2 +

(see,

e.g.,

(Hardt

et

al., 2016, Appendix A). Hence, we have

t+1 = µ(wt - wt) - µ(wt-1 - wt-1) + Gt(wt) - Gt(wt)

 µ wt - wt + µ wt-1 - wt-1 + Gt(wt) - Gt(wt)



1 + µ -  +

t + µt-1.

(A.2)

With probability 1/n, the selected example is different in S and S . In this case, we have

t+1 = µ(wt - wt) - µ(wt-1 - wt-1) + Gt(wt) - Gt(wt)

 µ wt - wt + µ wt-1 - wt-1 + Gt(wt) + Gt(wt) - Gt(wt) - Gt(wt)



1 + µ -  +

t + µt-1 +

Gt(wt) - Gt(wt)



1 + µ -  +

t + µt-1 +

wt - Gt(wt)

+

wt - Gt(wt)





1+µ- +

t + µt-1 + 2L

(A.3)

where the last inequality holds due to the L-Lipschitz property. Combining (A.2) and (A.3), we have

EA[t+1]  (1 - 1/n)

1 + µ -  +

EA[t] + µEA[t-1]

+ 1/n

1 + µ -  +

EA[t] + µEA[t-1] + 2L

=

1 + µ -  +

2L

EA[t] + µEA[t-1] +

. n

(A.4)

2 PROOF OF PROPOSITION 1 (UPPER-BOUND ON TRUE RISK)

The expected true risk estimate under parameter w can be decomposed into a stability error term and an optimization one. The optimization error is the gap between the empirical risk and the optimal expected empirical risk given by

opt = ES,A[RS (w) - RS (wS )]

(B.1)

1

Under review as a conference paper at ICLR 2019

where RS(wS ) = arg minw RS(w). Adding the stability error in Theorem 2, we have
ES,A[R(w)]  ES,A[RS (w)] + s  ES,A[RS (wS )] + opt + s.

(B.2)

In the following lemma, we show that stability results similar to Theorem 2 hold even if we consider the average parameter w^ T instead of wT .

Lemma 2 Suppose that the SGMM update is executed for T steps with constant learning rate  and

momentum

µ.

Provided

that

 +

-

1 2



µ

<

 3(+)

and





2 +

,

then

the

average

of

the

first

T

steps of SGMM has uniform stability with

2L2( + ) s  n  - 3µ( + ) .

(B.3)

Proof:

Let us

define w^ t

=

1 t

T k=1

wk

and

^t

=

w^ t - w^ t

where w^ t is obtained as specified in the

proof of Theorem 2. Following the proof of Theorem 2, we have

E[~k+1] 

1 + 3µ -  +

E[~k ]

+

2L .
n

(B.4)

for k = 0, · · · , T . Defining t =

,t
k=1

~k

t

we

have

^T



T

due

to

the

convexity

of

· . Summing

(B.4) for k = 0, · · · , T and dividing by T , we have E[^T ]  E[T ]  2L(+) . Applying

n -3µ(+)

the L-Lipschitz property on f (·, z), we have

2L2( + ) E[|f (w^ T ; z) - f (w^ T ; z)|]  n  - 3µ( + ) ,

(B.5)

which holds for all S, S and z.

By our convergence analysis in Theorem 3, we have

opt



µW0 (1 - µ)T

+

(1 - µ)W1 2T

-

W2 2

-

µW3 2(1 - µ)

+

L2 2(1 - µ) .

By our uniform stability analysis in Lemma 2, we have

2L2( + ) s  n  - 3µ( + )

Adding this to the upper-bound of s in (B.3), we have

ES,A[R(w^ T )]  ES,A[RS(wS)] +

µW0 (1 - µ)T

+

(1 - µ)W1 2T

-

W2 2

-

µW3 2(1 - µ)

L2 2L2( + ) + 2(1 - µ) + n  - 3µ( + ) .

(B.6)

Choosing sufficiently small µ such that  µ( + ), we have C  1. Then we can optimize the upper-bound in (B.6) by setting

1-µ =

W1 .

LT

Substituting (B.7) into (B.6), we obtain the upper-bound in Proposition 1.

(B.7)

2

Under review as a conference paper at ICLR 2019

test error - train error

0.6

= 0.9 = 0.5

0.12

= 0.9 = 0.5

0.5 = 0 0.10 = 0

0.4 0.08

train error

0.3 0.06

0.2 0.04

0.1 0.02

0.0 0 100 200 300 400 500 0.00 0 100 200 300 400 500

n (number of training samples)

n (number of training samples)

(a) Generalization error with respect to cross entropy

(b) Training error (cross entropy)

Figure 1: Generalization performance (cross entropy) of logistic regression for MNIST dataset with T = 1000 iterations and minibatch size 10.

train accuracy - test accuracy

0.200 0.175 0.150 0.125 0.100 0.075 0.050 0.025 0.000 0

= 0.9 = 0.5 =0

100 n (nu2m0b0er of traini3n0g0samples) 400

500

train accuracy

1.00 0.99 0.98 0.97 0.96
0

(a) Generalization error with respect to classification accuracy

= 0.9 = 0.5 =0
100

200 300 400 n (number of training samples)

(b) Training accuracy

500

Figure 2: Generalization performance (classification accuracy) of logistic regression for MNIST dataset with T = 1000 iterations and minibatch size 10.

3 ADDITIONAL EXPERIMENTAL RESULTS
In this section, we present further experimental results, to demonstrate how momentum affects the convergence and generalization of SGM when applied to the MINIST dataset. We train a logistic regression model with the weight decay regularization using SGMM for binary classification on the two-class MNIST dataset that contains the images from digit classes "2" and "9". All parameters are set the same as those for the notMNIST dataset in the main paper.
In Figures 1a and 1b, the generalization error (with respect to cross entropy) and training error are shown versus the number of training samples, n, for µ = 0, 0.5, 0.9. In Figures 2a and 2b, we illustrate the generalization error (with respect to classification accuracy) and the training accuracy as a function of n for the MNIST dataset. The results are consistent with those for the notMNIST dataset. We see that the generalization error (with respect to both cross entropy and classification accuracy) decreases as n increases for all values of µ. Furthermore, we observe that the generalization error increases with µ when n is sufficiently large.
We plot the training error and test error versus the number of epochs for the MNIST dataset in Figures 3a and 3b, respectively. We also show the training accuracy and test accuracy versus the number of epochs in Figures 4a and 4b, respectively. Similar to the case of the notMNIST dataset, we observe that the training error decreases as the number of epochs increases for all values of µ. In addition, one can observe that adding momentum improves training error and accuracy. However, this improvement becomes negligible as the number of epochs increases, which is expected.

3

Under review as a conference paper at ICLR 2019

train error test error

1.75

= 0.9 = 0.5

1.50 = 0

1.25

1.00

0.75

0.50

0.25

0.000.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 epoch

1.75

= 0.9 = 0.5

1.50 = 0

1.25

1.00

0.75

0.50

0.25

0.000.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 epoch

(a) Training error versus epoch

(b) Test error versus epoch

Figure 3: Training and test error of logistic regression (cross entropy loss) for MNIST dataset with n = 500 training samples and minibatch size 10.

1.0 0.8 0.8 0.6 0.6

train accuracy test accuracy

0.4 0.4

0.2

= 0.9 = 0.5

=0

0.00.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0

epoch

0.2 = 0.9 = 0.5 =0
0.00.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 epoch

(a) Training accuracy versus epoch

(b) Test accuracy versus epoch

Figure 4: Training and test accuracy of logistic regression for MNIST dataset with n = 500 training samples and minibatch size 10.

4

