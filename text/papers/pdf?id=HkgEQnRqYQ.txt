Under review as a conference paper at ICLR 2019
ROTATE: KNOWLEDGE GRAPH EMBEDDING BY RELATIONAL ROTATION IN COMPLEX SPACE
Anonymous authors Paper under double-blind review
ABSTRACT
We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.
1 INTRODUCTION
Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.
Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts. For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother's husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links.
Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b; Yang et al., 2014; Trouillon et al., 2016). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns.
In this paper, we propose such an approach called RotatE for knowledge graph embedding. Our motivation is from Euler's identity ei = cos  + i sin , which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation from the source entity to the target entity. Given a triplet (h, r, t), we expect that t = h  r, where
1

Under review as a conference paper at ICLR 2019

Model
SE (Bordes et al., 2011) TransE (Bordes et al., 2013)
TransX DistMult (Yang et al., 2014) ComplEx (Trouillon et al., 2016) HolE (Nickel et al., 2016) ConvE (Dettmers et al., 2017)
RotatE

Score Function

- Wr,1h - Wr,2t - h+r-t
- gr,1(h) + r - gr,2(t) r, h, t
Re( r, h, t ) r, h  t
(vec(([r, h]  ))W ), t

h, t  Rk, Wr,·  Rk×k h, r, t  Rk h, r, t  Rk h, r, t  Rk h, r, t  Ck h, r, t  Rk h, r, t  Rk

- hr-t 1

h, r, t  Ck, |ri| = 1

Table 1: The score functions fr(h, t) of several knowledge graph embedding models, where · denotes the generalized dot product,  denotes the Hadamard product,  denotes circular correlation,  denotes activation function and  denotes 2D convolution. · denotes conjugate for complex
vectors, and 2D reshaping for real vectors in ConvE model. TransX represents a wide range of
TransE's variants, such as TransH (Wang et al., 2014), TransR (Lin et al., 2015b), and STransE (Nguyen et al., 2016), where gr,i(·) denotes a matrix multiplication with respect to relation r.

h, r, t  Ck are the embeddings, the modulus |ri| = 1 and  denotes the Hadamard (element-wise) product. It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/i = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = ei3 is a combination of other two relations r1 = ei1 and r2 = ei2 if and only if r3 = r1  r2 (i.e. 3 = 1 + 2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory. We evaluate RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.

2 RELATED WORK
Predicting missing links with knowledge graph embedding (KGE) methods has been extensively investigated in recent years. The general methodology is to define a score function for the triplets. Formally, let E denote the set of entities and R denote the set of relations, then a knowledge graph is a collection of factual triplets (h, r, t), where h, t  E and r  R. Since entity embeddings are usually represented as vectors, the score function usually takes the form fr(h, t), where h and t are head and tail entity embeddings. The score function fr(h, t) measures the salience of a candidate triplet (h, r, t). The goal of the optimization is usually to score true triplet (h, r, t) higher than the corrupted false triplets (h , r, t) or (h, r, t ). Table 1 summarizes different score functions fr(h, t) in previous state-of-the-art methods as well as the model proposed in this paper. These models generally capture only a portion of the relation patterns. For example, TransE represents each relation as a bijection between source entities and target entities, and thus implicitly models inversion and composition of relations, but it cannot model symmetric relations; ComplEx extends DistMult by introducing complex embeddings so as to better model asymmetric relations, but it cannot infer the composition pattern. The proposed RotatE model leverages the advantages of both.
A related problem is how to effectively draw negative samples for training knowledge graph embeddings. This problem has been explicitly studied by Cai & Wang (2017), who proposed a generative adversarial learning framework to draw negative samples. However, such a framework requires training the embedding model with a discrete negative sample generator jointly, which is costly and
1The p-norm of a complex vector v is defined as v p = p |vi|p. We use L1-norm for all distancebased models in this paper and drop the subscript of · 1 for brevity.
2

Under review as a conference paper at ICLR 2019

Model
SE TransE TransX DistMult ComplEx
RotatE

Score Function
- Wr,1h - Wr,2t - h+r-t
- gr,1(h) + r - gr,2(t) h, r, t
Re( h, r, t )
- hr-t

Symmetry
    


Antisymmetry
    


Inversion
    


Composition
    


Table 2: The pattern modeling and inference abilities of several models.

difficult to do. We propose a self-adversarial sampling scheme which only relies on the current model, without requiring any additional optimization component, making it much more efficient.

3 ROTATE: RELATIONAL ROTATION IN COMPLEX VECTOR SPACE

In this section, we introduce our proposed RotatE model. We first introduce three important relation patterns that are widely studied in the literature of link prediction on knowledge graphs. Afterwards, we introduce our proposed RotatE model, which defines relations as rotations in complex vector space. We also show that the RotatE model is able to model and infer all three relation patterns.

3.1 MODELING AND INFERRING RELATION PATTERNS
The key of link prediction in knowledge graph is to infer the connection patterns, e.g., relation patterns, with observed facts. According to the existing literature (Trouillon et al., 2016; Toutanova & Chen, 2015; Guu et al., 2015; Lin et al., 2015a), three types of relation patterns are very important and widely spread in knowledge graphs: symmetry, inversion and composition. We give their formal definition here:
Definition 1. A relation r is symmetric (antisymmetric) if x, y r(x, y)  r(y, x) ( r(x, y)  ¬r(y, x) )
A clause with such form is a symmetry (antisymmetry) pattern.
Definition 2. Relation r1 is inverse to relation r2 if x, y r2(x, y)  r1(y, x)
A clause with such form is a inversion pattern.
Definition 3. Relation r1 is composed of relation r2 and relation r3 if x, y, z r2(x, y)  r3(y, z)  r1(x, z)
A clause with such form is a composition pattern.
According to the definition of the above three types of relation patterns, we provide an analysis of existing models on their abilities in inferring and modeling these patterns. Specifically, we provide an analysis on TransE, TransX, DistMult, and ComplEx.2 We did not include the analysis on HolE and ConvE since HolE is equivalent to ComplEx (Hayashi & Shimbo, 2017), and ConvE is a black box that involves two-layer neural networks and convolution operations, which are hard to analyze. The results are summarized into Table 2. We can see that no existing approaches are capable of modeling all the three relation patterns.

3.2 MODELING RELATIONS AS ROTATIONS IN COMPLEX VECTOR SPACE

In this part, we introduce our proposed model that is able to model and infer all the three types of

relation patterns. Inspired by Euler's identity, we map the head and tail entities h, t to the complex embeddings, i.e., h, t  Ck; then we define the functional mapping induced by each relation r as

an element-wise rotation from the head entity h to the tail entity t. In other words, given a triple

(h, r, t), we expect that:

t = h  r, where |ri| = 1,

(1)

2See discussion at Appendix A

3

Under review as a conference paper at ICLR 2019

(a) TransE models r as translation in real line.

(b) RotatE models r as rotation in complex plane.

(c) RotateE: an example of modeling symmetric relations r with ri = -1

Figure 1: Illustrations of TransE and RotatE with only 1 dimension of embedding.

and  is the Hadmard (or element-wise) product. Specifically, for each element in the embeddings,

we have ti = hiri. Here, we constrain the modulus of each element of r  Ck, i.e., ri  C, to be |ri| = 1. By doing this, ri is of the form eir,i , which corresponds to a counterclockwise

rotation by r,i radians about the origin of the complex plane, and only affects the phases of the

entity embeddings in the complex vector space. We refer to the proposed model as RotatE due to its

rotational nature. According to the above definition, for each triple (h, r, t), we define the distance

function of RotatE as:

dr(h, t) = h  r - t

(2)

By defining each relation as a rotation in the complex vector spaces, RotatE can model and infer all the three types of relation patterns introduced above. Formally, we have following results3:
Lemma 1. RotatE can infer the symmetry/antisymmetry pattern. (See proof in Appendix B)
Lemma 2. RotatE can infer the inversion pattern. (See proof in Appendix C)
Lemma 3. RotatE can infer the composition pattern. (See proof in Appendix D)

These results are also summarized into Table 2. We can see that the RotatE model is the only model that can model and infer all the three types of relation patterns.

Connection to TransE. From Table 2, we can see that TransE is able to infer and model all the other relation patterns except the symmetry pattern. The reason is that in TransE, any symmetric relation will be represented by a 0 translation vector. As a result, this will push the entities with symmetric relations to be close to each other in the embedding space. RotatE solves this problem and is a able to model and infer the symmetry pattern. An arbitrary vector r that satisfies ri = ±1 can be used for representing a symmetric relation in RotatE, and thus the entities having symmetric relations can be distinguished. Different symmetric relations can be also represented with different embedding vectors. Figure 1 provides illustrations of TransE and RotatE with only 1-dimensional embedding and shows how RotatE models a symmetric relation.

3.3 OPTIMIZATION

Negative sampling has been proved quite effective for both learning knowledge graph embedding

(Trouillon et al., 2016) and word embedding (Mikolov et al., 2013). Here we use a loss function

similar to the negative sampling loss (Mikolov et al., 2013) for effectively optimizing distance-based

models:

L = - log ( - dr(h, t)) -

n

1 k

log

 (dr (hi ,

ti)

-

),

i=1

(3)

where  is a fixed margin,  is the sigmoid function, and (hi, r, ti) is the i-th negative triplet.

We also propose a new approach for drawing negative samples. The negative sampling loss samples the negative triplets in a uniform way. Such a uniform negative sampling suffers the problem of inefficiency since many samples are obviously false as training goes on, which does not provide

3We relegate all proofs to the appendix.

4

Under review as a conference paper at ICLR 2019

Dataset FB15k WN18 FB15k-237 WN18RR

#entity 14,951 40,943 14,541 40,943

#relation 1,345 18 237 11

#training 483,142 141,442 272,115 86,835

#validation 50,000 5,000 17,535 3,034

#test 59,071 5,000 20,466 3,134

Table 3: Number of entities, relations, and observed triples in each split for four benchmarks.

any meaningful information. Therefore, we propose an approach called self-adversarial negative sampling, which samples negative triples according to the current embedding model. Specifically, we sample negative triples from the following distribution:

p(hj, r, tj|{(hi, ri, ti)}) =

exp fr(hj, tj) i exp fr(hi, ti)

(4)

where  is the temperature of sampling. Moreover, since the sampling procedure may be costly,

we treat the above probability as the weight of the negative sample. Therefore, the final negative

sampling loss with self-adversarial training takes the following form:

n

L = - log ( - dr(h, t)) - p(hi, r, ti) log (dr(hi, ti) - )
i=1

(5)

In the experiments, we will compare different approaches for negative sampling.

4 EXPERIMENTS
4.1 EXPERIMENTAL SETTING
We evaluate our proposed model on four widely used knowledge graphs. The statistics of these knowledge graphs are summarized into Table 3.
· FB15k (Bordes et al., 2013) is a subset of Freebase (Bollacker et al., 2008), a large-scale knowledge graph containing general knowledge facts. Toutanova & Chen (2015) showed that almost 81% of the test triplets (x, r, y) can be inferred via a directly linked triplet (x, r , y) or (y, r , x). Therefore, the key of link prediction on FB15k is to model and infer the symmetry/antisymmetry and inversion patterns.
· WN18 (Bordes et al., 2013) is a subset of WordNet (Miller, 1995), a database featuring lexical relations between words. This dataset also has many inverse relations. So the main relation patterns in WN18 are also symmetry/antisymmetry and inversion.
· FB15k-237 (Toutanova & Chen, 2015) is a subset of FB15k, where inverse relations are deleted. Therefore, the key of link prediction on FB15k-237 boils down to model and infer the symmetry/antisymmetry and composition patterns.
· WN18RR (Dettmers et al., 2017) is a subset of WN18. The inverse relations are deleted, and the main relation patterns are symmetry/antisymmetry and composition.
Hyperparameter Settings. We use Adam (Kingma & Ba, 2014) as the optimizer and fine-tune the hyperparameters on the validation dataset. The ranges of the hyperparameters for the grid search are set as follows: embedding dimension k  {125, 250, 500, 1000}, batch size b  {512, 1024, 2048}, self-adversarial sampling temperature   {0.5, 1.0}, and fixed margin   {3, 6, 9, 12, 18, 24, 30}. Both the real and imaginary parts of the entity embeddings are uniformly initialized, and the phases of the relation embeddings are uniformly initialized between 0 and 2. No regularization is used since we find that the fixed margin  could prevent our model from over-fitting.
Evaluation Settings. We evaluate the performance of link prediction in the filtered setting: we rank test triples against all other candidate triples not appearing in the training, validation, or test set, where candidates are generated by corrupting subjects or objects: (h , r, t) or (h, r, t ). Mean Rank (MR), Mean Reciprocal Rank (MRR) and Hits at N (H@N) are standard evaluation measures for these datasets and are evaluated in our experiments.

5

Under review as a conference paper at ICLR 2019

FB15k

WN18

MR MRR H@1 H@3 H@10 MR MRR H@1 H@3 H@10

TransE [] - .463 .297 .578 .749 - .495 .113 .888 .943

DistMult [] 42 .798 -

- .893 655 .797 -

- .946

HolE

- .524 .402 .613 .739 - .938 .930 .945 .949

ComplEx

- .692 .599 .759 .840 - .941 .936 .945 .947

ConvE

51 .657 .558 .723 .831 374 .943 .935 .946 .956

pRotatE RotatE

43 .799 .750 .829 .884 254 .947 .942 .950 .957 40 .797 .746 .830 .884 309 .949 .944 .952 .959

Table 4: Results of several models evaluated on the FB15K and WN18 datasets. Results of [] are taken from (Nickel et al., 2016) and results of [] are taken from (Kadlec et al., 2017). Other results are taken from the corresponding original papers.

FB15k-237

WN18RR

MR MRR H@1 H@3 H@10 MR MRR H@1 H@3 H@10

TransE [] 357 .294 -

- .465 3384 .226 -

- .501

DistMult 254 .241 .155 .263 .419 5110 .43 .39 .44 .49

ComplEx 339 .247 .158 .275 .428 5261 .44 .41 .46 .51

ConvE 244 .325 .237 .356 .501 4187 .43 .40 .44 .52

pRotatE RotatE

178 .328 .230 .365 177 .338 .241 .375

.524 2923 .462 .417 .479 .533 3340 .476 .428 .492

.552 .571

Table 5: Results of several models evaluated on the FB15k-237 and WN18RR datasets. Results of [] are taken from (Nguyen et al., 2017). Other results are taken from (Dettmers et al., 2017).

Baseline. Apart from RotatE, we propose a variant of RotatE as baseline, where the modulus of

the entity embeddings are also constrained: |hi| = |ti| = C, and the distance function is thus

2C

sin

h+r -t 2

(See Equation 16 at Appendix F for a detailed derivation). In this way, we can

investigate how RotatE works without modulus information and with only phase information. We

refer to the baseline as pRotatE. It is obvious to see that pRotatE can also model and infer all the

three relation patterns.

4.2 MAIN RESULTS
We compare RotatE to several state-of-the-art models, including TransE (Bordes et al., 2013), DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), HolE (Nickel et al., 2016), and ConvE (Dettmers et al., 2017), as well as our baseline model pRotatE, to empirically show the importance of modeling and inferring the relation patterns for the task of predicting missing links.
Table 4 summarizes our results on FB15k and WN18. We can see that RotatE outperforms all the state-of-the-art models. The performance of pRotatE and RotatE are similar on these two datasets. Table 5 summarizes our results on FB15k-237 and WN18RR, where the improvement is much more significant. The difference between RotatE and pRotatE is much larger on FB15k-237 and WN18RR, where there are a lot of composition patterns. This indicates that modulus is very important for modeling and inferring the composition pattern.
Moreover, the performance of these models on different datasets is consistent with our analysis on the three relation patterns (Table 2):
· On FB15K, the main relation patterns are symmetry/antisymmetry and inversion. We can see that ComplEx performs well while TransE does not perform well since ComplEx can infer both symmetry/antisymmetry and inversion patterns while TransE cannot infer symmetry pattern. Surprisingly, DistMult achieves good performance on this dataset although it cannot model the antisymmetry and inversion patterns. The reason is that for most of the relations in FB15K, the types of head entities and tail entities are different. Although DistMult gives the same score to a true triplet (h, r, t) and its opposition triplet (t, r, h), (t, r, h) is usually impossible to be valid since the entity type of t does not match the head entity type of h. For example, DistMult assigns the same score to (Obama, nationality, USA) and (USA, nationality, Obama). But

6

Under review as a conference paper at ICLR 2019

Countries (AUC-PR)

DistMult

ComplEx

ConvE

RotatE

S1 1.00 ± 0.00 0.97 ± 0.02 1.00 ± 0.00 1.00 ± 0.00

S2 0.72 ± 0.12 0.57 ± 0.10 0.99 ± 0.01 1.00 ± 0.00

S3 0.52 ± 0.07 0.43 ± 0.07 0.86 ± 0.05 0.95 ± 0.00

Table 6: Results on the Countries datasets. Other results are taken from (Dettmers et al., 2017).

(USA, nationality, Obama) can be simply predicted as false since USA cannot be the head entity of the relation nationality.
· On WN18, the main relation patterns are also symmetry/antisymmetry and inversion. As expected, ComplEx still performs very well on this dataset. However, different from the results on FB15K, the performance of DistMult significantly decreases on WN18. The reason is that DistMult cannot model antisymmetry and inversion patterns, and almost all the entities in WN18 are words and belong to the same entity type, which do not have the same problem as FB15K.
· On FB15k-237, the main relation pattern is composition. We can see that TransE performs really well while ComplEx does not perform well. The reason is that, as discussed before, TransE is able to infer the composition pattern while ComplEx cannot infer the composition pattern.
· On WN18RR, one of the main relation patterns is the symmetry pattern since almost each word has a symmetric relation in WN18RR, e.g., also see and similar to. TransE does not well on this dataset since it is not able to model the symmetric relations.
4.3 INFERRING RELATION PATTERNS ON COUNTRIES DATASET
We also evaluate our model on the Countries dataset (Bouchard et al., 2015; Nickel et al., 2016), which is carefully designed to explicitly test the capabilities of the link prediction models for composition pattern modeling and inferring. It contains 2 relations and 272 entities (244 countries, 5 regions and 23 subregions). Unlike link prediction on general knowledge graphs, the queries in Countries are of the form locatedIn(c, ?), and the answer is one of the five regions. The Countries dataset has 3 tasks, each requiring inferring a composition pattern with increasing length and difficulty. For example, task S2 requires inferring a relatively simpler composition pattern:
neighborOf(c1, c2)  locatedIn(c2, r)  locatedIn(c1, r), while task S3 requires inferring the most complex composition pattern:
neighborOf(c1, c2)  locatedIn(c2, s)  locatedIn(s, r)  locatedIn(c1, r).
In Table 6, we report the results with respect to the AUC-PR metric, which is commonly used in the literature. We can see that RotatE outperforms all the previous models. The performance of RotatE is significantly better than other methods on S3, which is the most difficult task.
4.4 IMPLICIT RELATION PATTERN INFERENCE
In this section, we verify whether the relation patterns are implicitly represented by RotatE relation embeddings. We ignore the specific positions in the relation embedding r and plot the histogram of the phase of each element in the relation embedding, i.e., {r,i}.
Symmetry pattern requires the symmetric relations to have property r  r = 1, and the solution is ri = ±1. We investigate the relation embeddings from a 500-dimensional RotatE trained on WN18. Figure 2a gives the histogram of the embedding phases of a symmetric relation similar to. We can find that the embedding phases are either  (ri = -1) or 0, 2 (ri = 1). It indicates that the RotatE model does infer and model the symmetry pattern. Figure 2b is the histogram of relation hypernym, which shows that the embedding of a general relation does not have such a ±1 pattern.
Inversion pattern requires the embeddings of a pair of inverse relations to be conjugate. We use the same RotatE model trained on WN18 for an analysis. Figure 2c illustrates the element-wise addition of the embedding phases from relation r1 = hypernym and its inversed relation r2 = hyponym. All the additive embedding phases are 0 or 2, which represents that r1 = r-2 1. This case shows that the inversion pattern is also inferred and modeled in the RotatE model.

7

Under review as a conference paper at ICLR 2019

150 100
50
00 1 2
(a) similar to

20 15 10
5
00 1 2
(b) hypernym

200 100
00 1 2
(c) hypernym  hyponym

30 20

15 10

15 10

400 300 200

10 5 5 100

00 1 2 00 1 2 00 1 2 00 1 2

(d) for1

(e) winner

(f) for2

(g) for-2 1 winnerfor1

Figure 2: Histograms of relation embedding phases {r,i} (ri = eir,i ), where for1 represents relation award nominee/award nominations./award/award nomination/nominated for, winner represents relation award category/winners./award/award honor/award winner and for2 represents award category/nominees./award/award nomination/nominated for. The symmetry, inversion and composition pattern is represented in Figure 2a, 2c and 2g,
respectively.

uniform KBGAN (Cai & Wang, 2017)

FB15k-237 MR MRR H@10
- .242 .422 - .278 .453

WN18RR

WN18

MR MRR H@10 MR MRR H@10

- .186 .459 - .433 .915

- .210 .479 - .705 .949

self-adversarial

227 .298 .475 3259 .223 .510 316 .736 .947

Table 7: TransE with different negative sampling techniques. The results in first 2 rows are taken from (Cai & Wang, 2017), where KBGAN uses a ComplEx negative sample generator.

Composition pattern requires the embedding phases of the composed relation to be the addition of the other two relations. Since there is no significant composition pattern in WN18, we study the inference of the composition patterns on FB15k-237, where a 1000-dimensional RotatE is trained. Figure 2d - 2g illustrate such a r1 = r2  r3 case, where 2,i + 3,i = 1,i or 2,i + 3,i = 1,i + 2.
More results of implicitly inferring basic patterns are presented in the appendix.
4.5 COMPARING DIFFERENT NEGATIVE SAMPLING TECHNIQUES
In this part, we compare different negative sampling techniques including uniform sampling, our proposed self-adversarial technique, and the KBGAN model (Cai & Wang, 2017), which aims to optimize a generative adversarial network to generate the negative samples. We re-implement a 50dimension TransE model with the margin-based ranking criterion that was used in (Cai & Wang, 2017), and evaluate its performance on FB15k-237, WN18RR and WN18 with self-adversarial negative sampling. Table 7 summarizes our results. We can see that self-adversarial sampling is the most effective negative sampling technique.
5 CONCLUSION
We have proposed a new knowledge graph embedding method called RotatE, which represents entities as complex vectors and relations as rotations in complex vector space. Our experimental results show that the RotatE model outperforms all existing state-of-the-art models on four large-scale benchmarks. Moreover, RotatE also achieves state-of-the-art results on a benchmark that is explicitly designed for composition pattern inference and modeling. A deep investigation into RotatE relation embeddings shows that the three relation patterns are implicitly represented in the relation embeddings. In the future, we plan to evaluate the RotatE model on more datasets and release our source codes.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pp. 1247­1250. AcM, 2008.
Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. Learning structured embeddings of knowledge bases. In AAAI, volume 6, pp. 6, 2011.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems, pp. 2787­2795, 2013.
Guillaume Bouchard, Sameer Singh, and Theo Trouillon. On approximate reasoning capabilities of low-rank vector spaces. AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches, 2015.
Liwei Cai and William Yang Wang. Kbgan: Adversarial learning for knowledge graph embeddings. arXiv preprint arXiv:1711.04071, 2017.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. arXiv preprint arXiv:1707.01476, 2017.
Kelvin Guu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space. arXiv preprint arXiv:1506.01094, 2015.
Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi Liu, Hua Wu, and Jun Zhao. An endto-end model for question answering over knowledge base with cross-attention combining global knowledge. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 221­231, 2017.
Katsuhiko Hayashi and Masashi Shimbo. On the equivalence of holographic and complex embeddings for link prediction. arXiv preprint arXiv:1702.05563, 2017.
Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge base completion: Baselines strike back. arXiv preprint arXiv:1705.10744, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. Modeling relation paths for representation learning of knowledge bases. arXiv preprint arXiv:1506.00379, 2015a.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In AAAI, volume 15, pp. 2181­2187, 2015b.
Farzaneh Mahdisoltani, Joanna Biega, and Fabian M Suchanek. Yago3: A knowledge base from multilingual wikipedias. In CIDR, 2013.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111­3119, 2013.
George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11): 39­41, 1995.
Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Phung. A novel embedding model for knowledge base completion based on convolutional neural network. arXiv preprint arXiv:1712.02121, 2017.
Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark Johnson. Stranse: a novel embedding model of entities and relationships in knowledge bases. arXiv preprint arXiv:1606.08140, 2016.
Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, et al. Holographic embeddings of knowledge graphs. In AAAI, volume 2, pp. 3­2, 2016.
9

Under review as a conference paper at ICLR 2019
Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pp. 697­706. ACM, 2007.
Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pp. 57­66, 2015.
The´o Trouillon, Johannes Welbl, Sebastian Riedel, E´ ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International Conference on Machine Learning, pp. 2071­2080, 2016.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In AAAI, volume 14, pp. 1112­1119, 2014.
Chenyan Xiong, Russell Power, and Jamie Callan. Explicit semantic ranking for academic search via knowledge graph embedding. In Proceedings of the 26th international conference on world wide web, pp. 1271­1279. International World Wide Web Conferences Steering Committee, 2017.
Bishan Yang and Tom Mitchell. Leveraging knowledge bases in lstms for improving machine reading. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1436­1446, 2017.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014.
Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. Collaborative knowledge base embedding for recommender systems. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 353­362. ACM, 2016.
10

Under review as a conference paper at ICLR 2019

APPENDIX
A DISCUSSION ON THE ABILITY OF PATTERN MODELING AND INFERENCE
No existing models are capable of modeling all the three relation patterns. For example, TransE cannot model the symmetry pattern because it would yield r = 0 for symmetric relations; TransX can infer and model the symmetry/antisymmetry pattern when gr,1 = gr,2, e.g. in TransH (Wang et al., 2014), but cannot infer inversion and composition as gr,1 and gr,2 are invertible matrix multiplications; due to its symmetric nature, DistMult is difficult to model the asymmetric and inversion pattern; ComplEx addresses the problem of DisMult and is able to infer both the symmetry and asymmetric patterns with complex embeddings. Moreover, it can infer inversion rules because the complex conjugate of the solution to arg maxr Re( x, r, y ) is exactly the solution to arg maxr Re( y, r, x ). However, ComplEx cannot infer composition rules, since it does not model a bijection mapping from h to t via relation r. These concerns are summarized in Table 2.

B PROOF OF LEMMA 1
Proof. if r(x, y) and r(y, x) hold, we have y = rxx = ry  rr = 1
Otherwise, if r(x, y) and ¬r(y, x) hold, we have y = rxx = ry  rr = 1

C PROOF OF LEMMA 2
Proof. if r1(x, y) and r2(y, x) hold, we have y = r1  x  x = r2  y  r1 = r-2 1

D PROOF OF LEMMA 3
Proof. if r1(x, z), r2(x, y) and r3(y, z) hold, we have z = r1  x  y = r2  x  z = r3  y  r1 = r2  r3

E PROPERTIES OF ROTATE

A useful property for RotatE is that the inverse of a relation can be easily acquired by complex conjugate. In this way, the RotatE model treats head and tail entities in a uniform way, which is potentially useful for efficient 1-N scoring (Dettmers et al., 2017):

h  r - t = (h  r - t)  r = t  r - h

(6)

Moreover, considering the embeddings in the polar form, i.e., hi = mh,ieih,i , ri = eir,i , ti = mt,ieit,i , we can rewrite the RotatE distance function as:

k
hr-t =
i=1

(mh,i

-

mt,i)2

+

4mh,imt,i

sin2

h,i

+

r,i 2

-

t,i

(7)

This equation provides two interesting views of the model:

(1) When we constrain the modulus mh,i = mt,i = C, the distance function is reduced to

2C

sin

h+r -t 2

.

We can see that this is very similar to the distance function of TransE:

h + r - t . Based on this intuition, we can show that:

Theorem 4. RotatE can degenerate into TransE. (See proof at Appendix F)

which indicates that RotatE is able to simulate TransE. (2) The modulus provides the lower bound of the distance function, which is mh - mt .

11

Under review as a conference paper at ICLR 2019

DistMult ComplEx
ConvE
RotatE

MR 5926 6351 1671
1767

YAGO3-10 MRR H@1 H@3 .34 .24 .38 .36 .26 .40 .44 .35 .49
.495 .402 .550

H@10 .54 .55 .62
.670

Table 8: Results of several models evaluated on the YAGO3-10 datasets. Other results are taken from (Dettmers et al., 2017).

Benchmark embedding dimension k batch size b negative samples n  

FB15k

1000

2048

128 1.0 24

WN18

500

512

1024

0.5 12

FB15k-237

1000

1024

256 1.0 9

WN18RR

500

512

1024

0.5 6

Countries S1

500

512 64 1.0 0.1

Countries S2

500

512 64 1.0 0.1

Countries S3

500

512 64 1.0 0.1

YAGO3-10

500

1024

400 1.0 24

Table 9: The best hyperparameter setting of RotatE on several benchmarks.

F PROOF OF THEOREM 4

Proof. By further restricting |hi| = |ti| = C, we can rewrite h, r, t by h = Ceih = C cos h + iC sin h r = eir = cos r + i sin r t = Ceit = C cos t + iC sin t
Therefore, we have h  r - t = C ei(h+r) - eit = C ei(h+r-t) - 1 = C cos(h + r - t) - 1 + i sin(h + r - t)
= C (cos(h + r - t) - 1)2 + sin2(h + r - t)

(8) (9) (10) (11)
(12) (13)
(14)

= C 2 - 2 cos(h + r - t)

(15)

= 2C sin h + r - t 2

(16)

If the embedding of (h, r, t) in TransE is h , r , t , let h = ch , r = cr , t = ct and C = 1/c , we have

lim h  r - t = h + r - t
c0

G LINK PREDICTION ON YAGO3-10
YAGO3-10 is a subset of YAGO3 (Mahdisoltani et al., 2013), which consists of entities that have a minimum of 10 relations each. It has 123,182 entities and 37 relations. Most of the triples deal with descriptive attributes of people, such as citizenship, gender, profession and marital status. Table 8 shows that the RotatE model also outperforms state-of-the-art models on YAGO3-10.
12

Under review as a conference paper at ICLR 2019

RotatE

TransE

MR MRR H@1 H@3 H@10 MR MRR H@1 H@3 H@10

negative sampling loss

w/ adv 177 .338 .241 .375 .533 170 .332 .233 .372 .531

w/o adv 185 .297 .205 .328 .480 175 .297 .202 .331 .486

margin-based ranking criterion

w/ adv 225 .322 .225 .358 .516 167 .333 .237 .370 .522

w/o adv 199 .293 .202 .324 .476 164 .306 .212 .340 .493

Table 10: Results of ablation study on FB15k-237, where "adv" represents "self-adversarial".

H HYPERPARAMETERS
We list the best hyperparameter setting of RotatE w.r.t the validation dataset on several benchmarks in Table 9.
I ABLATION STUDY
Table 10 shows our ablation study of self-adversarial sampling and negative sampling loss on FB15k-237. We also re-implement a 1000-dimension TransE and do ablation study on it. From the table, We can find that self-adversarial sampling boosts the performance for both models, while negative sampling loss is only effective on RotatE; in addition, our re-implementation of TransE also outperforms all the state-of-the-art models on FB15k-237.
J MORE RESULTS OF IMPLICIT BASIC PATTERN INFERENCE
We provide more histograms of embedding phases in Figure 3 - 5.

13

Under review as a conference paper at ICLR 2019

20 15 10
5
00

1 (a) has part

10
5
2 00 1 2 (b) instance hypernym

80 60 40 20
00

1 (c) also see

100 50

100

50

2 00

1

(d) verb group

150

100

50

2

00 1 2 00 1 2

(e) derivationally related form

(f) similar to

Figure 3: Histograms of embedding phases from two general relations and four symmetric relations on WN18. ( k = 500 )

14

Under review as a conference paper at ICLR 2019

300 200

300 200

100

100

00 1 2
(a) /celebrities/celebrity/celebrity friends./celebrities/friendship/friend

00 1 2
(b) /award/award winner/awards won./award/award honor/award winner

300

300

200

200

100

100

00 1 2
(c) /location/statistical region/places exported to./location/imports and exports/exported to

00

1

(d) /base/popstra/celebrity/breakup./base/popstra/breakup/participant

2

300

400

200 100

300 200 100

00 1 2 00 1 2

(e) /base/popstra/celebrity/dated./base/popstra/dated/participant

(f) /government/legislative session/members./government/government position held/legislative sessions

Figure 4: Histograms of embedding phases from six symmetric relations on FB15k-237. (k = 1000)

15

Under review as a conference paper at ICLR 2019

200

100

00

1

(a) member of domain topic  synset domain topic of

150

100

50

00

1

(c) synset domain usage of  member of domain usage

200
100
2 00 1 2 (b) has part  part of

150

100

50

2 00

1

(d) synset domain region of  member of domain region

2

200 100

100 50

00 1 2 00 1 2
(e) member meronym  member holonym (f) instance hypernym  instance hyponym
Figure 5: Histograms of element-wise additions of inversed relation embedding phases on WN18. (k = 500)

16

