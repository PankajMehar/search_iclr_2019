Under review as a conference paper at ICLR 2019
GENEVAL: A BENCHMARK SUITE FOR EVALUATING GENERATIVE MODELS
Anonymous authors Paper under double-blind review
ABSTRACT
Generative models are important for several practical applications, from low level image processing tasks, to model-based planning in robotics. More generally, the study of generative models is motivated by the long-standing endeavor to model uncertainty and to discover structure by leveraging unlabeled data. Unfortunately, the lack of an ultimate task of interest has hindered progress in the field, as there is no established way to compare models and, often times, evaluation is based on mere visual inspection of samples drawn from such models. In this work, we aim at addressing this problem by introducing a new benchmark evaluation suite, dubbed GenEval. GenEval hosts a large array of distributions capturing many important properties of real datasets, yet in a controlled setting, such as lower intrinsic dimensionality, multi-modality, compositionality, independence and causal structure. Any model can be easily plugged for evaluation, provided it can generate samples. Our extensive evaluation suggests that different models have different strenghts, and that GenEval is a great tool to gain insights about how models and metrics work. We offer GenEval to the community 1 and believe that this benchmark will facilitate comparison and development of new generative models.
1 INTRODUCTION
Modeling uncertainty is a fundamental problem for machine learning. In unsupervised settings, an ideal model should be able to describe all the possible events consistent with the provided context. In supervised settings, there is often not a single correct output for a given input, and models need to be able to express the space of correct outputs and their relative likelihood.
One standard approach to this problem is to build a probability model of the output space, or more generally, an energy-based model that assigns a score to every possible output. This kind of model is useful for comparing two possibilities, for example. Recently there have been a number of proposed models that do not necessarily assign any score to possible outputs, but using a pseudo-random number generator, produce hallucinations that in some way resemble the true outputs. This kind of model can be useful for examining possibilities.
In this work, we will consider the second kind of model, and take a "generative model" to be any construction that has a method for outputting data points using a pseudo-random number generator. In particular, we do not require "generative models" to be able to compute a probability of a data point. These kinds of models have become popular recently due to several works on Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), that have demonstrated the generation of realistic images (Radford et al., 2015; Karras et al., 2017).
However, this field has struggled to solve its zeroth problem (even as it has made progress on its first). Before the design of "better" models , it is necessary to agree upon good metrics and methodology to evaluate the quality of samples generated by a model. Part of the problem is that the downstream tasks of interest in the case of image generation have not been fully settled. Nevertheless, there have been a few works that make rigorous attempts to quantify the capabilities of these models, restricted to GANs parameterized as convolutional nets in the setting of natural images (Lucic et al., 2017; Huang et al., 2018).
1Available at: coming soon.
1

Under review as a conference paper at ICLR 2019
In this work, we expand upon these with a suite of benchmark synthetic distributions that we use to evaluate not only GANs, but also other forms of generative models. We deliberately avoid convolutional networks on images with the aim of decoupling the benefits of various modeling paradigms from domain specific neural architectures. If we believe that generative models should be studied as generic tools for dealing with uncertainty (as opposed to methods of generating images), this decoupling is necessary. Additionally, practical, generic, and effective methods for measuring the distance between distributions in high dimensions with access only to samples do not currently exist. On the other hand, in some cases where one of the distributions has some special structure, we can effectively measure distortion, see an example of this in Appendix sec.A; using simple synthetic distributions allows us to be sure the distributions under consideration have the correct special structures. Analysis of which methods are able to model which kinds of special structures (e.g. independence, causality, multi-modality) is critical for understanding these methods; with "real" data not fully under our control, such analysis can be difficult.
A major contribution of this work is the GenEval toolbox to evaluate models. Thanks to its general API, new models can be easily plugged in and tested using a variety of metrics against popular baseline generative models. Likewise, it's very simple to add new distributions and evaluation metrics to the existing pool to highlight special properties of a model of interest.
2 RELATED WORK
Much recent literature on generative modeling of continuous distributions has focused on modeling natural images. Restricting our attention to works aiming at generating samples, as opposed to scoring inputs, many recent models are based on Generative Adversarial Networks (Goodfellow et al., 2014; Arjovsky et al., 2017; Roth et al., 2017; Gulrajani et al., 2017; Denton et al., 2015; Karras et al., 2017), which are capable of generating very realistic high resolution images, a task that had challenged the research community for several decades (Geman and Geman, 1984).
For models trained on natural images, several metrics have been proposed, such as "inception score" (Salimans et al., 2016) and "Frechet inception" distance (Heusel et al., 2017). Unfortunately, these metrics rely on yet another model for evaluation, and are known to only partially correlate with human judgment. To further complicate matters, generation scores not only depend on the metric but also on the choice of architecture of the generator, typically a rather sophisticated convolutional neural network. To summarize, comparison between different generative models is very difficult, and it is often based on visual inspection of a handful of model samples (Denton et al., 2015; Arjovsky et al., 2017).
Several works have attempted to isolate the contribution of the estimation method by working in a controlled setting, e.g., by fitting the model to a mixture of Gaussians in two or three dimensions (Arjovsky et al., 2017; Roth et al., 2017; Gulrajani et al., 2017). This paper can be seen as a more comprehensive, thorough and extnsive study building upon these earlier attempts.
Recently, Lucic et al. (2017) have systematically compared different variants of GANs trained on natural images. While we share similar motivations, we do not restrict our focus to just GANs, but also include other popular models (Kingma and Welling, 2013; Dinh et al., 2016). Morover, we restrict our study to purely synthetic distributions with known properties which we leverage at evaluation time (or, from another perspective, we broaden our study to distributions beyond those coming from the image domain).
The choice of metrics for generative models has been intense topic of debate. Most works, overall those based on probabilistic modeling, report performance in terms of log-likelihood of samples generated by the data distribution according to the model. As discussed in earlier work (Theis et al., 2016) and in sec. 5, this is not a good metric in general, as it cannot be estimated by all models, e.g., GANs, and it is very susceptible to the scaling of the input. Interestingly, Lucic et al. (2017) propose a precision/recall metric to evaluate the quality of samples, where precision measures the distance of each model sample to the data manifold and recall measures the distance of each data point to the closest model generation. Unfortunately, such score disregards any weighting of the density, the fact that not all data points are equally likely and similarly, not all model samples have the same likelihood. Recognizing this issue, we borrow the "two-sample test" criterion introduced by Lopez-Paz and Oquab (2016) and extended by Huang et al. (2018). The two-sample test is based
2

Under review as a conference paper at ICLR 2019

on a similar motivation but it operates on actual samples, using implicit Monte Carlo weighting to take into account the density, and obviating the need for an explicit optimization over a data-manifold.

3 DISTRIBUTIONS

The benchmark focuses on distributions with support on Rd for some d. Their implementation in GenEval requires two methods: one for computing the log-density at a point, and one for sampling. GenEval implementation of distributions is modular, and allows combining distributions via products, mixtures, causal mappings, and isometric transforms. Next, we briefly describe the distributions we considered in this study.

Simple distributions Many of the distributions will be built from standard families of distributions,
such as the multivariate Gaussian, and the Slab (or uniform) distribution with constant density on a rectanguloid in Rd.

Such distributions can be embedded into a lower dimensional space Rq with q < d, yielding what we call a "flat" Guassian or Slab. For instance, the density function of a flat Slab is defined as:

p(x)  R(OT (x - m)) + (x),

where R is the uniform distribution with support in R = {x : 0  xi  ri, i = 1, . . . , q}, O is a

d × q matrix with orthogonal columns, and  is a Gaussian density with small variance.

Mixtures Starting with K densities p1, ...., pk on Rd, and a weighting vecor w on the simplex in Rd,

the density for a mixture is: p(x) =

K i=1

wipi(x).

When

the

bulk

of

the

mass

of

pi

is

disjoint

from

the bulk of the mass of pj for all i = j, the mixture model has a cluster structure. We show results

with these kinds of distributions in fig. 1, 2 and in Appendix sec. E.

Distributions with causal structure Given a function f : Rq  Rd-q, q < d and a density p

defined in Rq, we can can get a density with causal structure on Rd by considering

p(x) = p (xind)(xdep - f (xind)),

where xind are the independent first q coordinates of x (drawn from a certain distribution), and xdep is the last d - q dependent coordinates of x, and  is a noise distribution (for example zero-mean Gaussian with small variance). Note that these kinds of distributions have locally lower dimensional structure than d, especially when d is large compared to q. We show results with these kinds of distributions in fig. 5. Locally low dimensional distributions We consider three types of locally low dimensional distributions: causal distributions where f above is parameterized by a random fully-connected ReLU net or a random quadratic polynomial (to simulate non-linear manifolds), and an "image-like" distribution of shifted bumps.

For the random neural-networks, we use one-hidden-layer ReLU networks with default PyTorch (Paszke et al., 2017) initializations. For the random quadratic polynomials, we take a sample for a two dimensional distribution and we project it into a d - 2 dimensional space with a matrix of normally distributed coefficients, and then concatenate both vectors.

For the shifted bumps distribution, which is similar to the artificial triangle image distribution
considered in (Lucic et al., 2017), we take a side-length s, a set of radii r0, ..., rl, and consider all shifts of an ri × ri square on a s × s background. These form a set of s2  l points in Rs2 ; we take a random orthogonal projection of this set to Rd and place a ball of mass around each projected point.
Experiments with these distributions are shown in fig. 3.
Distributions with independence structures We can increase the intrinsic dimension of any of these previous distributions by combining them as a product: p(x) = jK=1pi(x), where p1, ..., pK are the basic components which have non-zero support on a disojint subset of coordinates. Note
that models are not provided with such information at training time. We report results with product
distributions in fig. 4 and in Appendix.

4 GENERATIVE MODELS
In this section we first describe some classical baselines, meant to provide upper and lower-bound performances on each distribution, and then some neural-network based models.

3

Under review as a conference paper at ICLR 2019

Baselines: The oracle baseline is the data distribution itself. Given a metric assessing the discrepancy between samples drawn from the data and the model distributions (see sec. 5), oracle samples provide a lower bound on the error of an ideal model perfectly fitting the data. Ideal metrics would report an error close to 0 for oracle samples.
The next baseline we consider is the kernel density estimator (KDE) (Rosenblatt, 1956; Parzen, 1962) which places a Gaussian bump of a certain width around each training point. KDE essentially memorizes the training set, and therefore, it is expected to provide great fitting to the training data but poor generalization, overall in high dimensional spaces where the curse of dimensionality would require a vast amount of training samples to finely cover the data distribution.
Next, we have the multivariate Gaussian distribution fitted by maximum likelihood (Pedregosa et al., 2011), meant to provide some sort of upper bound on the error, and the mixture of Gaussians with k components (MoGk), which we fit by using the expectation-maximization algorithm (Dempster et al., 1977). We fix the number of Gaussians in the mixture to 10 in all experiments.
Neural Models: Among the neural models, we consider the Variational Auto-Encoder (VAE) (Kingma and Welling, 2013), the real non-volume preserving density estimator (RNVP) (Dinh et al., 2016) and Generative Adversarial Network (GAN) (Goodfellow et al., 2014) with some of its variants, namely Wasserstein GAN (WGAN) (Arjovsky et al., 2017), WGAN with Gradient Penalty (WGAN-GP) (Gulrajani et al., 2017) and GAN with Noise Regularization (GAN-NR) (Roth et al., 2017). See Appendixfor a brief review of these methods.

5 METRICS

Since all models must be capable of drawing samples, we measure fitting error in terms of the distortion between two sets of points: samples drawn from the data distribution and from the model distribution. We will discuss several measures of fidelity to a distribution; and none of them are generically useful. Measuring the distortion between real-valued distributions in high (or even not-so-high) dimensions continues to be an unsolved problem, especially if we add constraints on computational efficiency. This is one of the reasons to use artificial data with special properties we control: although none of the metrics are generally useful, they can all be useful when the distributions have appropriate structure (see example in Appendix sec. A).

Optimal Transport The Optimal Transport distance (OT) u between sets S and T of points in a metric space with metric , is defined by:

u(S, T ) = min ij(si, tj) s.t. ij = 1 and ij = 1 and ij  0
i,j i j

(1)

We use the POT package 2 for computing the optimal transport, and assume  is Euclidean l2 for the rest of this work, and drop it from the notation.

Note that OT is not a distance in the standard mathematical sense, as for instance the "distance" between two sets of points sampled from the same distribution is not zero. Nonetheless, OT has some pleasing properties. For example, it is continuous with respect to perturbations of the points in the sets. Furthermore, if two sets of points covers the same low-dimensional manifold in a high dimensional space, the OT distance between them will be relatively small. On the other hand, note that in high dimensions, with few samples, optimal transport can have possibly counter-intuitive behaviors. For example, suppose we sample N points S and N points S from the uniform distribution over the unit sphere in Rd, where d is large enough to see concentration-of-measure effects, and then N points T from 0 (the point mass at the origin). If N is small compared to d, then the points in S and S are almost orthogonal, and so have distance roughly 2 between them. On the other hand, the distance of any point in S to a point sampled from T is 1. It is thus likely that u(S, S ) > u(S, T ) even though S is sampled from the same distribution as S and T is not. Two-Sample Test Two set of points, S  pd and T  pg, are close if the chance that a point from S has nearest neighbor belonging to T is 50% and vice versa. This is an instantiation of the Two-Sample Test (2S) (Huang et al., 2018). In this work, we build upon an extension of the original

2http://pot.readthedocs.io/en/stable/

4

Under review as a conference paper at ICLR 2019

formulation (Huang et al., 2018). Assume |S| = |T | = N . For each point x in S or T , define n(x) to be3
n(x) = 1, if minyS||x - y|| < minyT ||x - y|| . 0, otherwise.
Then, we define the distance as the sum of the deviations from the optimal rate:

v(S, T ) = 1/2 - n(x)/N + 1/2 - (1 - n(x))/N

xS

xT

(2)

As N gets larger, it becomes easier to distinguish the distributions via this statistic; but for different values of N , this metric may lead to different quality orderings. On the other hand, as the dimension of the distribution gets higher, the test gets weaker. Log-Likelihood In this work log-likelihood (LL) is estimated on the samples drawn from the model using the (known) data distribution - since we do not require models to necessarily be able to estimate data log-likelihood. This gives a notion of how likely points generated by the model are, regardless of the overall fit. In other words, samples drawn from a model assigning all its mass to the mode of the data distribution will have even higher likelihood that samples drawn from the actual data distribution.

5.1 SPECIAL METRICS

Our distributions have special structure that we can take advantage of, to better measure success in modeling that distribution. Mode coverage (MC) Assuming a mixture distribution for the data and a uniform distribution over modes, we measure whether samples generated from the model have even coverage of the clusters. We report "mode coverage" (MC) as the perpelxity of the mode assignments: M C = 2H(a), where H is the entropy of the cluster assignment distribution a. Causal Discovery Assuming a causal distribution (see sec. 3) and the ability of the model to perform conditional inference of a set of variables given the complementary set, we measure how close (in l2) the recovered dependent coordinates are from what the ground truth values given a set of independent coordinates drawn from the true underlying distribution.

For all the neural models with a latent space, we estimate the missing variables by optimizing over the

latent variable with an l2 reconstruction loss over the observed coordinates, starting from a random

point. For KDE, we find the nearest point from the training set in the constrained coordinates, and

return the remaining coordinates of that training point. For a mixture of Guassians, we take the nearest

mean in the constrained coordinates among all components, and then return the maximum-likelihood

estimate from that Gaussian of the remaining coordinates. See fig. 5 for results using this metric.

Independence Test Measuring the independence of high-dimensional real valued distributions is

challenging. In order to get some idea of the ability of the various models to detect independence,

we will restrict our attention to distributions that are products of distributions that are easy to vector-

quantize, and use categorical tests for independence on the quantized values. That is, suppose we

have a distribution p =

K i=1

pi

where

pi

is

supported

on

the

set

of

coordinates

ci.

Further

suppose

we have clustered pi into clusters Cij for j  {1, ..., ni}, and that we have L samples X from a

model. Then, for each coordinate group ci, we make a table of size ni × (n1n2...ni-1ni+1...nK )

with the counts Ns,t of the number of points in X that landed in each cell. Here, s indexes rows of

this table, corresponding to the ni centroids Ci1, ..., Cini for the points projected down to coordinate group ci, and t indexes columns of the table, corresponding to the product of all other clusters in all other coordinate groups.Denote by Mt = s Ns,t/L and M s = t Ns,t/L; we expect that Ns,t/L  MsMt, so we take:

2 = (Ns,t/L - MsMt)2 .
s,t

Independent OT and Two-Sample Test There is a partial remedy for dealing with the weaknesses of the distortion measures in high dimensions for distributions that we know have independence
3In our implementation, if the quantities minyS||x - y|| = minyT ||x - y|| we choose n(x) randomly in {0, 1}.

5

Under review as a conference paper at ICLR 2019

structure, see sec. 3. Suppose that the distribution p we are interested in has support in Rd. Further suppose there are groups of coordinates I0, I1, ...IL partitioning [1, ..., d] such that pIj  pIi for i = j. Here pI is the distribution given by projecting p onto the subspace spanned by the coordinates
in I. Then, for some distortion measure u, we can use the set distance given by:

K
uind(S, T ) = u(SIi , TIi ),
i=1

(3)

where SIi is the projection of S onto Ii (and likewise for T ).

In building such a distortion measure, we are using the knowledge of the true distribution. However, because we are building the distributions in the benchmark, we are free to use this information in measuring distortion, even if the models should not get access to it during training. Furthermore, note that this kind of distortion measure only can tell the difference between distributions that disagree on the marginals of the groups. Even if it shows no distortion between two distributions, they may still be different. On the other hand, because each group has smaller dimension, the distortion measure is less pre-disposed to show everything being different from everything else.

6 GENEVAL
GenEval is written in Python and it consists of three main components: models, distributions, and metrics.
Models must define fit(X) and sample(N) methods, the former to train the model on a dataset of points X, the latter to draw N samples. Models can optionally define a method to conditionally sample data given values for a set of fixed coordinates. Thanks to the general interface and modularity of GenEval, we simply incorporated the original implementations by the authors of the models whenever available. As a result, GenEval does not enforce any specific machine learning framework. For VAE(vae), GAN(gan, b), WGAN(wga, b), WGAN-GP(wga, a) and GAN-NR(gan, a) we used PyTorch (Paszke et al., 2017), while for RNVP(nvp) we used TensorFlow (Abadi et al., 2015). For Mixture of Gaussians and Kernel Density Estimation we employ the scikit-learn (Pedregosa et al., 2011) package.
Each distribution class must define the methods: sample(N) and logprob(X). Mixture distributions may also optionally define a method to estimate the most likely cluster assignment to each input data point, and product distributions can specify their components. New distributions can easily be defined via composition directly in configuration files specifying which distribution to test on. For instance, the library contains classes for a slab, a mixture of arbitrary distributions, and affine transform. Given these, one may define on-the-fly a distribution over the surface of a rotated 3D box in some higher dimensional space.
Metrics methods take as input two sets of samples (from the data distribution and from the model) and output a scalar value. Some metrics may optionally take as input a trained model and the data distribution to compute distribution dependent metrics like mode coverage.
Models and distributions are passed as input to GenEval via two Python script configuration files, which specify all their eventual hyper-parameters. Model hyper-parameters can also be specified via lists, which are used by GenEval to run a grid search over hyper-parameter values. A user may then run GenEval with configuration files that specify several distributions and several models. GenEval then launches training and evaluation for the cartesian product of all possible combinations of model and distribution and report results on a table, as those shown in the Appendix.

7 EXPERIMENTS
We used GenEval to compare the models described in sec. 4 on several distributions. For each distribution, GenEval first runs a grid search over hyper-parameters, see for instance the configuration files in Appendix sec. D. To produce the results in this section for every method we ran a very extensive grid search over hyper-parameters, which reached about 20, 000 configurations for some GAN variants; and was in the thousands for all neural methods. Afterwards, GenEval compares models across all metrics and compiles tables, such as those in Appendix E.

6

Under review as a conference paper at ICLR 2019
Figure 1: Two Sample test on Mixture Gaussians. Top: each Gaussian component has intrinsic dimensionality equal to 5, with a spherical covariance (in 5d) rotated to a random orientation 50d and non-zero mean placed at random; the ambient dimension is 50. Bottom: each Gaussian component is defined in a 50 dimensional space, and it has spherical covariance and a non-zero mean placed at random in that space. Left: varying the number of components in the mixture from 2 to 50. Right: varying the variance of each component; note that for the flat mixture (Top) only the variance in the intrinsic dimension changes.
Figure 2: Same as above but using OT for both cross-validation and evaluation. Unless otherwise stated, all experiments we discuss next have used 10,000 training samples, and unless otherwise specified in the figure, 1000 validation and test samples. On the figures showing results with OT, OT was used as a cross-validation metric for the hyperparameter search; and 2S was used for figures showing results with 2S. For all other figures, cross validation was done using 2S. Fig 1 shows two-sample test results as a function of the number of components in the mixture and the variance. First, we observe that model performance is bounded by the oracle, as expected. Second, in this experiment, GAN variants do not perform reliably better than the original GAN. Third, we observe that none of the models perform well, despite the relative simplicity of the distribution. However, the situation is more nuanced if we cross-validate and evaluate according to OT, as shown in fig. 2. For instance, while before there was a big performance gap between the oracle and KDE, it is not anymore according to OT, highlighting the importance of the choice of metric when comparing methods. In particular, notice how oracle performance according to OT gets worse as a function of the variance and number of components, suggesting a loss of reliability of this metric in those cases. Next, we evaluate on various manifold distribution and product of manifold distributions, see fig. 3 and 4. One interesting observation is that OT and 2S do not correlate very well (see discussion in sec. 5). On these tasks, Real NVP performs well in terms of the two-sample measure, followed by MoG10, with other methods performing worse. In terms of OT, KDE does best on the products of manifolds, and VAE does the best on the manifolds. In fact, VAE does better than oracle there (see two bottom left plots in fig. 3 and the note in 5), suggesting it is denoising the true distribution. In the next experiment reported in fig. 5, we analyze the ability of the models to discover causal structure on manifold distributions. We do not report the results of Real NVP here because we cannot
7

Under review as a conference paper at ICLR 2019
Figure 3: Optimal transport and two-sample test distances on two and three dimensional embedded manifolds in R50.
Figure 4: Optimal transport on products of five identical distributions, each of which has support near a two or three dimensional embedded manifold in R10. In this table, we use uind from eq. 3 (using the true groups of independent coordinates) to measure error and to validate models.
make a conditional estimate in the same way as the other neural models. We observe that all models have hard time beating the MoG10 baseline in this case. Finally, we look at a product of independent Gaussian mixture distributions in fig. 6. None of the models are able to succesfully fit this distribution, although WGAN-GP and Real NVP do the best. KDE in particular fails to fit this distribution, both in terms of two-sample and in terms of the 2 independence test. We can also see that all of the GAN variants improve on the mode coverage of the vanilla GAN.
8 DISCUSSION
Perhaps to the delight of neural generative model skeptics, one sees that on almost all distributions and all metrics that we consider, one of KDE or mixture of 10 Gaussians are competitive with (and often superior to) the neural models. These take a tiny fraction of the training cost, have essentially no hyper-parameter sweep, and have simple, well understood fitting routines. However, the neural models, especially GAN, are designed to be used where l2 in the observation space makes little sense as a metric. On most of our examples l2 in the observation space is locally meaningful, and most of our metrics rely on l2 to be (at least) locally meaningful. Thus we consider success at our benchmarks neither necessary nor sufficient for a model to be good at the perhaps more complex tasks for which neural models are designed. We discourage neural model skeptics from dismissing neural models based on these results: worse results on simple tasks against unscaleable baselines better adapted to the simple tasks does not necessarily mean an approach should be abandoned. On the other hand, neural generative model boosters should take heed of these results. They show that on datasets that are not images, with networks that are not convolutional networks, neural models do not do well. Moreover, specific to GANs, the protocols purported to improve GANs in the image setting do not reliably improve results across the metrics and datasets shown here (although cluster coverage does seem to be reliably improved). This is natural, as practitioners have spent much effort tuning models for performance in the image domain, and of course neural models will do poorly
8

Under review as a conference paper at ICLR 2019
Figure 5: Prediction error of dependent variables in causal distribution, see sec. 3 and 5.1. Hyperparameters are chosen by best OT.
Figure 6: We consider product of 5 (top) and 8 (bottom) distributions, each of which is a mixture of three Gaussians in 12 dimensions. The 2 metric, see sec. 5.1, measures the extent by which models have captured the independence structure of the product distribution. Note that higher is better for mode coverage, unlike the other metrics. 2S is computed using 1K samples, while MC and 2 use 50K samples
without well adapted architectures 4. Nevertheless, most works trying to improve or understand GANs (empirically or theoretically) discuss the training protocol independently of the relationships between the inductive biases of the neural architectures and the properties of the distributions to be modeled. Our results here suggest that these relationships cannot be ignored when studying GANs. We also see that while no single neural model is all-around superior to the others, Real NVP does do better than the other models in many cases, especially in terms of the two-sample test. However, it does not dominate, for example doing worse at products of manifolds in terms of OT, see fig. 4, and worse on mixtures of Gaussians with large numbers of components, see fig. 15. We can also see other quirks of the models. For example, we can see that "denoising" effects of a VAE can make it appear to be a good model on some datasets and metrics, but poor in others. It is one of the better performing neural models in fig. 5 when measuring causal error, showing it has learned the manifold and the functional dependence. But at the same time, it is bad in terms of twosample (fig. 3); perhaps because it is collapsing the two free components in the output. More generally, these kinds of effects show that in general, it is important to consider multiple metrics. The gold standard should be a success in a downstream task of interest, but in the absence of such a task, looking at a single metric to judge success can be misleading.
REFERENCES
https://github.com/rothk/Stabilizing_GAN, a. https://github.com/pytorch/examples/tree/master/dcgan, b.
4Note however we have checked several of the distributions where neural models perform poorly, and constructed by hand neural generators that well approximate these distributions. Thus the problem is not simply of the generator being out of the search space.
5All of the models are doing poorly here, though.
9

Under review as a conference paper at ICLR 2019
https://github.com/tensorflow/models/tree/master/research/real_ nvp.
https://github.com/pytorch/examples/tree/master/vae.
https://github.com/caogang/wgan-gp, a.
https://github.com/martinarjovsky/WassersteinGAN, b.
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane´, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vie´gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein generative adversarial networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 214­223, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR.
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 39(1):1­38, 1977.
Emily L Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, 2015.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp, 2016.
Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(6): 721­741, 1984.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and Sepp Hochreiter, 2017.
Gao Huang, Yang Yuan, Qiantong Xu, Chuan Guo, Yu Sun, Felix Wu, and Kilian Weinberger. An empirical study on evaluation metrics of generative adversarial networks, 2018. URL https: //openreview.net/forum?id=Sy1f0e-R-.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests, 2016.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.
Emanuel Parzen. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 33(3), 1962.
10

Under review as a conference paper at ICLR 2019
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825­2830, 2011.
A. Radford, L. Metz, and S. Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. arXiv preprint arXiv:1511.06434, 2015.
Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. The Annals of Mathematical Statistics, 27(3), 1956.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization, 2017.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, 2016.
L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In International Conference on Learning Representations, Apr 2016. URL http://arxiv.org/ abs/1511.01844.
11

Under review as a conference paper at ICLR 2019

A BLOCK INDEPENDENCE AND DISTORTION MEASURE RESOLUTION
We give a simple example showing how eq. 3 can increase the accuracy, or more precisely, the resolution of a distortion measure. We take a product of 10 mixtures of Gaussians in R12, each with 3 components, with modes randomly distributed over the unit sphere in R12, as in fig. 6. The overall ambient dimension is 120.
Measuring OT against a single best fit Gaussian, which is clearly a poor model of the original distribution, without leveraging the groups structure (see again eq. 3), gives a distortion of 7.3 for the oracle samples versus 8.6 for the samples from the Gaussian. If we do leverage information about the independence structure among the groups, we get a distortion of 5.9 for the oracle versus 19.3 for the Gaussian. Raising the number of mixtures in the product to 30 gives 17.4 vs 17.6 without groups and 17.5 vs 54.5 with.
It is therefore essential even in relatively low dimensions to leverage the special structure of the distributions in order to assess the real quality of the sameples generated by a model.

B OTHER METRICS

In addition to the metrics introduced in sec. 5,we also considered the Hausdorff distance.

Hausdorff Distance The Hausdorff distance (H) h between sets S and T of points in a metric space with metric , is defined by:

h(S, T ) = max min (s, t) + max min (s, t)

sS tT

tT sS

(4)

As in the case of optimal transport, this defines a metric between sets; but when we sample from
the distributions and compare the distance between the samples, we do not get a metric on the distributions. We will denote by H(P, Q; N ) the statistic obtained by sampling N points from P , N points from Q, and computing the Hasudorff distance between the sampled points. We will show results with an average version of this statistic: by H2(P, Q; N ) we take the average minimal square distance, instead of the max. That is, define:

h2(S, T )2

=

1 N

min (s, t)2 + min (s, t)2 ,
tT sS

sS

tT

(5)

and set the statistic H2(P, Q; N ) to be h2(S, T ) for N points S sampled from P and N points T sampled from Q. Note that h2 is not even a metric on sets, as it does not satisfy the triangle inequality.

The Hausdorff statistics should be taken as a measure of the difference of support between distributions. That is, as the number of sampled points gets large, two distribution with the same support (but perhaps very different densities on that support) will have relatively small difference in the Hausdorff statistic.

12

two sample error

Under review as a conference paper at ICLR 2019

C ANALYSES OF TWO-SAMPLE TEST

The Two-Sample Test depends on the number of points sampled from the distributions. We analyze how number of test points and number of training points affects the metric. We use random quadratic polynomial distribution and a distributions of shifted bumps as shown in fig. 7 and as described in sec. 3. For both distributions we observe a similar pattern. As the number of points goes up the errors also goes up for all the models. However, the degradation is much more prominent for KDE. The same applied to the size of the training set: neural models may generalize from less amount of training data.

two sample error

5 Gauss. KDE poly_manifold (1k) Gaussian
4 MoG@10 GAN WGAN WGAN-GP
3 GAN-NR VAE REAL NVP
2

poly_manifold (10k)
Gauss. KDE
4 Gaussian MoG@10 GAN WGAN
3 WGAN-GP GAN-NR VAE REAL NVP
2

11

100 num10b0e0r of test s1a0m00p0les shifted_bumps (1k)
5 Gauss. KDE Gaussian MoG@10
4 GAN WGAN WGAN-GP
3 GAN-NR VAE REAL NVP
2 1
100 num10b0e0r of test s1a0m00p0les

100000 100000

two sample error

100 num10b0e0r of test s1a0m00p0les shifted_bumps (10k)
5 Gauss. KDE Gaussian MoG@10
4 GAN WGAN WGAN-GP
3 GAN-NR VAE REAL NVP
2

100000

1

0 100 num10b0e0r of test s1a0m00p0les 100000

Figure 7: Two-Sample test for different number of test points for a random quadratic polynomial distribution (top) and for a distribution of shifted bumps (bottom), when training with 1k (left) and 10k (right) samples. Lower is better.

two sample error

13

Under review as a conference paper at ICLR 2019
D HYPER-PARAMETER SEARCH
As mentioned in the section 6, model configurations for each experiment are defined in configuration files. Each file defines a MODELS variable with a list of dictionaries describing the model. Each dictionary has three keys: name, model (refers to a model class), and args. A user may want to re-use the same model in several cases to evaluate impact of different subsets of parameters. E.g., all flavors of the GAN models implemented in a single class and evaluated separately. Hyper parameters for each case are defined in args dictionary that maps parameter to one or more possible values. All our models use multilayer perceptron as the base architecture. The default activation function is ReLU, but in order to help the model to sample from a mixture of gaussians, we also considered q-ary multi maxout activation that is a max pooling over groups of q units. The activation function is determined by maxout parameter. Zero stands for ReLU, while positive values stand of q-ary multi maxout. Parameter --nz defines the size of the latent space. By default GAN models use RMSProp optimizer unless --adam is specified. RealNVP and VAE use Adam optimizer by default. We followed the choice in the original implementations here. (Arjovsky et al., 2017) noticed that for better results in training WGAN several discriminator updates should be done per generator updates. Besides, discriminator should get 100 extra updates for first 25 steps and every 500th step. We captures these heuristics in --Diters and --boost discriminator flags. For RealNVP we use only channel based masking with 8 coupling layers (defined by --chain length). Each layer is implemented by a multilayer perceptron with optional batch and weight normalization. Below we show the complete configuration file used for multimodal distributions.
14

Under review as a conference paper at ICLR 2019
import models
MODELS = [ { ' name ' : 'KDE ' , 'args ' : { '--k e r n e l ' : ' g a u s s i a n ' , '--b a n d w i d t h ' : [ 0 . 0 0 1 , 0 . 0 1 , 0 . 1 ] }, ' model ' : m o d e l s . KDE }, { 'name ' : ' Gaussian ' , 'args ' : { '--n c o m p o n e n t s ' : 1 , }, 'model ' : models . GaussianMixture }, { ' name ' : 'MOG' , 'args ' : { '--n c o m p o n e n t s ' : 1 0 , }, 'model ' : models . GaussianMixture }, { ' name ' : 'VAE ' , 'args ' : { '--n u p d a t e s ' : [ 1 0 0 0 , 3 0 0 0 , 1 0 0 0 0 ] , '--n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--e n c o d e r n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--h i d d e n s i z e ' : [ 1 0 0 , 5 0 0 ] , '--nz ' : [ 1 0 , 3 0 , 1 0 0 ] , '--Emaxout q ' : [ 0 , 1 0 ] , '--Dmaxout q ' : [ 0 , 1 0 ] , '--l r ' : [ 3 e -3, 1 e -3, 3 e -4, 1 e -4] , '--b a t c h s i z e ' : 6 4 , }, ' model ' : m o d e l s . VAE }, { 'name ' : 'RealNVP ' , 'args ' : { '--m l p h i d d e n l a y e r s ' : 1 , '--m l p h i d d e n s i z e ' : [ 1 0 0 , 3 0 0 ] , '--c h a i n l e n g t h ' : [ 4 , 8 ] , '--t r a i n s t e p s ' : [ 1 0 0 0 , 3 0 0 0 , 1 0 0 0 0 ] , '--l e a r n i n g r a t e ' : [ 1 e -2, 3 e -3, 1 e -3, 3 e -4] , '--b a t c h s i z e ' : 6 4 , '--w e i g h t n o r m ' : [ 0 , 1 ] , '--u s e b a t c h n o r m ' : [ 0 , 1 ] , '--m a x o u t q ' : [ 0 , 1 0 ] , '--o p t i m i z e r ' : ' adam ' , }, ' model ' : models . RealNvp , }, { ' name ' : 'GAN' , 'args ' : { '--l o s s ' : ' gan ' , '--n u p d a t e s ' : [ 1 0 0 0 , 3 0 0 0 , 1 0 0 0 0 ] , '--n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--d n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--d h i d d e n s i z e ' : [ 5 0 , 5 0 0 ] , '--Gmaxout ' : [ 0 , 1 0 ] , '--Dmaxout ' : [ 0 , 1 0 ] , '--nz ' : [ 1 0 , 1 0 0 ] , '--c u d a ' : True , '--h i d d e n s i z e ' : [ 1 0 0 , 5 0 0 ] , '--b a t c h S i z e ' : 6 4 , '--l r ' : [ 1 e -3, 3 e -4, 1 e -4] , '--D i t e r s ' : [ 1 , 2 , 5 ] , '--adam ' : True , }, ' model ' : m o d e l s .GAN }, { ' name ' : 'WGAN' , 'args ' : { '--l o s s ' : ' wgan ' ,
15

Under review as a conference paper at ICLR 2019
'--b o o s t d i s c r i m i n a t o r ' : True , '--n u p d a t e s ' : [ 1 0 0 0 , 3 0 0 0 , 1 0 0 0 0 ] , '--n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--d n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--d h i d d e n s i z e ' : [ 5 0 , 5 0 0 ] , '--nz ' : [ 1 0 , 1 0 0 ] , '--Gmaxout ' : [ 0 , 1 0 ] , '--Dmaxout ' : [ 0 , 1 0 ] , '--h i d d e n s i z e ' : [ 1 0 0 , 5 0 0 ] , '--c u d a ' : True , '--b a t c h S i z e ' : 6 4 , '--l r ' : [ 1 e -3, 3 e -4, 1 e -4] , '--D i t e r s ' : [ 1 , 2 , 5 ] , '--adam ' : True , }, ' model ' : m o d e l s .GAN }, { ' name ' : 'WGAN-GP ' , 'args ' : { '--l o s s ' : ' wgan-gp ' , '--n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--d n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--d h i d d e n s i z e ' : [ 5 0 , 5 0 0 ] , '--n u p d a t e s ' : [ 1 0 0 0 , 3 0 0 0 , 1 0 0 0 0 ] , '--Gmaxout ' : [ 0 , 1 0 ] , '--Dmaxout ' : [ 0 , 1 0 ] , '--nz ' : [ 1 0 , 1 0 0 ] , '--h i d d e n s i z e ' : [ 1 0 0 , 5 0 0 ] , '--c u d a ' : True , '--b a t c h S i z e ' : 6 4 , '--l r ' : [ 1 e -3, 3 e -4, 1 e -4] , '--D i t e r s ' : [ 1 , 2 , 5 ] , '--adam ' : True , }, ' model ' : m o d e l s .GAN }, { ' name ' : 'GAN-NR ' , 'args ' : { '--l o s s ' : ' gan ' , '--n u p d a t e s ' : [ 1 0 0 0 , 3 0 0 0 , 1 0 0 0 0 ] , '--n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--d n u m h i d d e n l a y e r s ' : [ 1 , 2 ] , '--d h i d d e n s i z e ' : [ 5 0 , 5 0 0 ] , '--Gmaxout ' : [ 0 , 1 0 ] , '--Dmaxout ' : [ 0 , 1 0 ] , '--nz ' : [ 1 0 , 1 0 0 ] , '--h i d d e n s i z e ' : [ 1 0 0 , 5 0 0 ] , '--c u d a ' : True , '--b a t c h S i z e ' : 6 4 , '--l r ' : [ 1 e -3, 3 e -4, 1 e -4] , '--D i t e r s ' : [ 1 , 2 , 5 ] , '--adam ' : True , '--d i s c r e g w e i g h t ' : [ 1 . 0 , 0 . 1 , 0 . 0 1 , 0 . 0 0 1 ] , }, ' model ' : m o d e l s .GAN }, ]
E MULTIMODAL DISTRIBUTIONS
In this section, we report detailed results using various mixture of Gaussians. "ad" means ambient dimension, "id" means intrinsic dimensions where the MoGs are embedded, "mt" specifies the kind of mean allocation (on the circle "C" or at random "R"), "r" refers to the variance and "MoG" to the number of components in the mixture.
16

Under review as a conference paper at ICLR 2019

MoG2 ad50 id5 mtC r0.03 MoG5 ad50 id5 mtC r0.03 MoG10 ad50 id5 mtC r0.03 MoG20 ad50 id5 mtC r0.03 MoG50 ad50 id5 mtC r0.03 MoG10 ad50 id5 mtC r0.1 MoG10 ad50 id5 mtC r0.3 MoG10 ad50 id5 mtC r1.0 MoG10 ad50 id5 mtC r3.0 MoG2 ad50 id5 mtR r0.03 MoG5 ad50 id5 mtR r0.03 MoG10 ad50 id5 mtR r0.03 MoG20 ad50 id5 mtR r0.03 MoG50 ad50 id5 mtR r0.03 MoG10 ad50 id5 mtR r0.1 MoG10 ad50 id5 mtR r0.3 MoG10 ad50 id5 mtR r1.0 MoG10 ad50 id5 mtR r3.0 MoG2 ad50 id50 mtC r0.03 MoG5 ad50 id50 mtC r0.03 MoG10 ad50 id50 mtC r0.03 MoG20 ad50 id50 mtC r0.03 MoG50 ad50 id50 mtC r0.03 MoG10 ad50 id50 mtC r0.1 MoG10 ad50 id50 mtC r0.3 MoG10 ad50 id50 mtC r1.0 MoG10 ad50 id50 mtC r3.0 MoG2 ad50 id50 mtR r0.03 MoG5 ad50 id50 mtR r0.03 MoG10 ad50 id50 mtR r0.03 MoG20 ad50 id50 mtR r0.03 MoG50 ad50 id50 mtR r0.03 MoG10 ad50 id50 mtR r0.1 MoG10 ad50 id50 mtR r0.3 MoG10 ad50 id50 mtR r1.0 MoG10 ad50 id50 mtR r3.0

oracle
0.089 0.104 0.106 0.127 0.152 0.160 0.250 0.429 0.721 0.257 0.435 0.338 0.886 1.328 0.391 0.478 0.648 0.926 0.452 0.464 0.464 0.469 0.471 0.824 1.406 2.512 4.268 0.619 0.791 0.703 1.227 1.642 1.059 1.637 2.771 4.617

KDE
0.114 0.089 0.115 0.144 0.150 0.168 0.258 0.440 0.741 0.392 0.275 0.632 0.911 1.202 0.682 0.766 0.930 1.201 0.473 0.455 0.467 0.476 0.474 0.826 1.407 2.511 4.263 0.749 0.640 0.980 1.251 1.524 1.321 1.876 2.969 4.764

Gaussian
0.536 0.531 0.453 0.426 0.421 0.499 0.611 0.885 1.369 2.774 5.172 6.353 7.047 7.523 6.336 6.283 6.264 6.291 0.689 0.668 0.624 0.613 0.613 0.919 1.451 2.520 4.256 2.808 5.142 6.302 6.981 7.488 6.241 6.256 6.481 7.254

MOG
0.059 0.108 0.112 0.151 0.160 0.176 0.280 0.436 0.802 0.103 0.252 0.313 2.919 5.254 0.367 0.622 0.835 1.094 0.426 0.477 0.463 0.465 0.465 0.825 1.404 2.508 4.258 0.600 0.595 0.884 3.111 5.298 1.096 1.816 2.861 4.664

VAE
0.316 0.770 0.654 0.674 0.648 0.678 0.766 0.962 1.326 0.477 1.204 1.816 2.607 3.199 1.935 1.662 2.095 2.404 0.488 0.772 0.797 0.798 0.825 0.989 1.487 2.223 3.730 0.758 1.550 1.794 2.558 3.184 2.250 2.573 3.145 4.457

RealNVP
0.196 0.197 0.233 0.276 0.298 0.323 0.404 0.553 0.852 0.239 1.084 1.651 2.128 4.149 1.572 1.640 2.194 2.421 0.521 0.517 0.536 0.520 0.503 0.853 1.417 2.477 4.188 1.033 1.090 1.475 2.514 3.805 1.944 2.427 3.422 5.114

GAN
0.139 0.407 0.160 0.170 0.151 0.243 0.391 0.487 0.837 5.556 1.346 2.887 4.869 6.429 2.862 2.891 2.527 3.401 1.009 0.883 0.827 0.450 0.442 0.750 1.383 2.267 3.654 6.274 1.949 2.977 5.092 6.255 2.808 5.595 3.989 4.710

WGAN
0.316 0.484 0.314 0.918 0.456 0.469 0.581 0.815 1.136 8.248 4.635 2.736 3.239 5.750 4.341 3.175 3.534 2.989 1.111 0.639 0.678 0.573 0.486 0.847 1.282 2.148 3.646 2.061 2.242 3.293 3.657 4.636 3.565 3.043 3.626 4.790

WGAN-GP
0.154 0.154 0.144 0.165 0.180 0.186 0.289 0.491 0.906 0.336 0.638 1.000 1.890 2.256 0.887 1.368 1.148 1.796 1.054 0.428 0.613 0.462 0.404 0.771 1.253 2.179 3.644 0.805 1.024 1.352 1.742 2.783 1.552 2.357 2.898 4.331

GAN-NR
0.111 0.142 0.158 0.162 0.222 0.275 0.306 0.492 0.782 5.424 1.340 4.268 3.787 5.941 3.928 2.764 2.826 3.536 0.495 0.443 0.425 0.454 0.436 0.952 1.289 2.271 3.653 5.359 1.866 3.296 4.159 5.328 2.233 2.448 3.718 4.413

Table 1: Mixture distributions with 10000 training points using OT metric.

MoG2 ad50 id5 mtC r0.03 MoG5 ad50 id5 mtC r0.03 MoG10 ad50 id5 mtC r0.03 MoG20 ad50 id5 mtC r0.03 MoG50 ad50 id5 mtC r0.03 MoG10 ad50 id5 mtC r0.1 MoG10 ad50 id5 mtC r0.3 MoG10 ad50 id5 mtC r1.0 MoG10 ad50 id5 mtC r3.0 MoG2 ad50 id5 mtR r0.03 MoG5 ad50 id5 mtR r0.03 MoG10 ad50 id5 mtR r0.03 MoG20 ad50 id5 mtR r0.03 MoG50 ad50 id5 mtR r0.03 MoG10 ad50 id5 mtR r0.1 MoG10 ad50 id5 mtR r0.3 MoG10 ad50 id5 mtR r1.0 MoG10 ad50 id5 mtR r3.0 MoG2 ad50 id50 mtC r0.03 MoG5 ad50 id50 mtC r0.03 MoG10 ad50 id50 mtC r0.03 MoG20 ad50 id50 mtC r0.03 MoG50 ad50 id50 mtC r0.03 MoG10 ad50 id50 mtC r0.1 MoG10 ad50 id50 mtC r0.3 MoG10 ad50 id50 mtC r1.0 MoG10 ad50 id50 mtC r3.0 MoG2 ad50 id50 mtR r0.03 MoG5 ad50 id50 mtR r0.03 MoG10 ad50 id50 mtR r0.03 MoG20 ad50 id50 mtR r0.03 MoG50 ad50 id50 mtR r0.03 MoG10 ad50 id50 mtR r0.1 MoG10 ad50 id50 mtR r0.3 MoG10 ad50 id50 mtR r1.0 MoG10 ad50 id50 mtR r3.0

oracle
253.824 252.981 252.218 251.452 250.456 249.208 246.462 243.452 240.705 253.824 252.981 252.218 251.452 250.456 249.208 246.462 243.452 240.705 73.628 72.724 72.095 71.276 70.864 42.003 14.726 -14.799 -41.662 73.628 72.724 72.095 71.263 70.270 41.996 14.531 -15.569 -43.034

KDE
231.287 230.307 229.704 228.862 228.139 226.695 223.948 220.938 218.192 231.287 230.307 229.704 228.862 228.139 226.695 223.948 220.938 218.192 73.576 72.700 72.011 71.234 70.936 41.926 14.646 -14.891 -41.765 73.576 72.700 72.011 71.227 70.336 41.919 14.456 -15.642 -43.118

Gaussian
-170250.471 -153160.807 -115759.422 -104456.734 -102708.032 -129044.001 -166966.672 -297879.223 -663150.864 -4706385.463 -13034230.862 -18513951.173 -22307511.884 -24634424.153 -18525897.945 -18699702.888 -18828035.613 -19192966.205
8.073 17.434 31.006 33.946 34.300 30.298 11.334 -15.546 -41.811 -1725.184 -4787.562 -6885.307 -8293.821 -9223.065 -2059.011 -684.216 -222.121 -110.474

MOG
231.369 230.445 229.747 -6866.577 -5943.212 226.752 -9825.265 220.981 -60010.239 231.316 230.441 229.755 -5061117.625 -12267927.710 226.741 223.986 221.002 218.240 73.656 72.743 72.045 70.386 70.844 41.679 14.641 -14.922 -41.771 73.654 72.734 72.051 -1950.515 -4879.046 41.960 14.491 -15.611 -43.072

VAE
-65887.252 -253697.262 -191527.681 -197133.338 -186868.389 -191487.241 -209477.349 -248019.344 -217340.400 -426133.306 -1435202.737 -2318297.079 -3678013.955 -5016707.836 -2157861.216 -2082989.824 -2682950.001 -2769326.615
72.604 15.856 11.079
8.195 2.004 38.911 27.678 7.104 -17.932 -107.994 -423.206 -862.853 -1162.677 -1742.290 -253.427 -68.693 -23.774 -31.757

RealNVP
-6900.901 -15290.232 -23915.894 -32354.515 -32109.117 -60656.093 -36074.078 -57297.815 -58365.110 -62711.711 -390003.663 -1407379.654 -1984178.067 -8313216.364 -1320055.362 -1435225.157 -1463626.337 -1391332.768
70.636 62.414 56.194 60.910 62.431 38.265 13.503 -13.602 -40.008 38.340 -35.199 -208.406 -904.543 -2052.524 -71.988 -23.838 -34.471 -50.787

GAN
-57.331 -3945.600 -3849.664 -6436.782 -7990.157 -8093.914 -34197.371 -17369.215 -36044.445 -102122.087 -954518.627 -3477965.805 -581505.899 -12919610.973 -3339959.070 -3352350.916 -216242.356 -3652568.951
84.551 -38.952
1.525 81.715 85.098 56.746 19.792
5.036 -21.709 -217.560 -298.758 22.882 -99.933 -5468.518 29.173 -28.428 -42.190 -34.440

WGAN
-53773.787 -112854.365 -52946.030 -341928.288 -71896.103 -78326.775 -126956.270 -200859.027 -321284.461 -16675610.717 -11187066.420 -4363909.568 -4830178.304 -15440539.432 -9290551.421 -5709800.866 -6440277.438 -4575101.256
-18.458 31.335 24.857 45.758 69.406 45.851 29.601
4.401 -23.253 -466.513 -983.164 -2321.356 -2659.067 -3669.330 -711.750 -140.817 -45.174 -41.842

WGAN-GP
-6491.264 -6420.108 -5687.075 -8642.488 -4529.060 -7003.240 -15022.751 -27847.380 -30625.161 -181595.947 -173564.214 -537268.899 -1608774.782 -1798768.727 -793372.647 -748740.215 -512050.242 -690167.089
-65.469 85.773 60.130 78.979 88.844 50.840 27.557
1.838 -22.324 11.341 -103.058 -389.596 -306.468 -702.292 -34.268 -30.060 -13.998 -31.362

GAN
-4697 -3676 -6432 -6278 -7151 -12623 -7276 -8660 -47172 -86721 -924853 -394392 -2810725 -853523 -528493 -142109 -1850674 -244264
84 81 89 80 83 48 26
5 -22 -14 -287 -465 -472 -534 16 16 -21 -32

Table 2: Mixture distributions with 10000 training points using log-likelihood metric.

17

Under review as a conference paper at ICLR 2019

MoG2 ad50 id5 mtC r0.03 MoG5 ad50 id5 mtC r0.03 MoG10 ad50 id5 mtC r0.03 MoG20 ad50 id5 mtC r0.03 MoG50 ad50 id5 mtC r0.03 MoG10 ad50 id5 mtC r0.1 MoG10 ad50 id5 mtC r0.3 MoG10 ad50 id5 mtC r1.0 MoG10 ad50 id5 mtC r3.0 MoG2 ad50 id5 mtR r0.03 MoG5 ad50 id5 mtR r0.03 MoG10 ad50 id5 mtR r0.03 MoG20 ad50 id5 mtR r0.03 MoG50 ad50 id5 mtR r0.03 MoG10 ad50 id5 mtR r0.1 MoG10 ad50 id5 mtR r0.3 MoG10 ad50 id5 mtR r1.0 MoG10 ad50 id5 mtR r3.0 MoG2 ad50 id50 mtC r0.03 MoG5 ad50 id50 mtC r0.03 MoG10 ad50 id50 mtC r0.03 MoG20 ad50 id50 mtC r0.03 MoG50 ad50 id50 mtC r0.03 MoG10 ad50 id50 mtC r0.1 MoG10 ad50 id50 mtC r0.3 MoG10 ad50 id50 mtC r1.0 MoG10 ad50 id50 mtC r3.0 MoG2 ad50 id50 mtR r0.03 MoG5 ad50 id50 mtR r0.03 MoG10 ad50 id50 mtR r0.03 MoG20 ad50 id50 mtR r0.03 MoG50 ad50 id50 mtR r0.03 MoG10 ad50 id50 mtR r0.1 MoG10 ad50 id50 mtR r0.3 MoG10 ad50 id50 mtR r1.0 MoG10 ad50 id50 mtR r3.0

oracle
0.237 0.287 0.361 0.446 0.612 1.112 3.256 10.756 32.184 0.237 0.287 0.361 0.446 0.612 1.112 3.256 10.756 32.184 20.799 21.699 22.485 23.445 23.781 74.894 222.124 719.093 2099.189 20.804 21.702 22.487 23.494 25.069 74.937 224.793 749.292 2247.890

KDE
0.258 0.318 0.391 0.484 0.653 1.208 3.541 11.707 35.043 0.258 0.318 0.391 0.484 0.653 1.208 3.541 11.707 35.043 21.094 22.049 22.895 23.845 24.107 76.283 225.794 730.723 2131.176 21.094 22.051 22.896 23.890 25.495 76.321 228.979 763.307 2288.899

Gaussian
57.920 44.034 36.817 35.249 35.234 38.380 46.841 85.637 205.049 1688.426 3340.899 4480.731 5498.846 6568.851 4436.725 4389.701 4326.580 4341.083 62.118 51.226 45.508 44.976 44.580 92.325 235.964 730.422 2106.525 1630.390 3232.852 4356.962 5377.364 6471.202 4259.335 4181.012 4351.709 5581.006

MOG
0.233 0.294 0.368 2.085 2.084 1.108 7.086 10.767 53.285 0.236 0.295 0.368 1734.484 3224.388 1.118 3.249 10.872 32.344 20.821 21.732 22.514 23.861 23.829 75.530 222.485 720.277 2099.680 20.810 21.696 22.518 1811.709 3583.002 75.057 225.307 750.725 2251.496

VAE
26.330 56.186 44.635 46.903 44.958 43.889 50.803 80.277 179.895 382.969 712.163 951.298 1204.290 1464.644 852.832 851.563 981.120 1002.917 27.443 46.620 53.014 53.748 55.255 82.746 220.458 517.921 1518.626 411.953 658.708 953.083 1113.193 1405.041 1001.369 961.619 1059.480 1971.971

RealNVP
6.417 8.342 9.226 13.206 13.368 245.109 13.608 27.669 51.730 211.919 333.198 705.150 884.249 3181.403 725.129 722.119 710.919 716.591 22.756 28.345 30.658 29.253 28.791 80.491 227.260 701.075 2027.237 149.912 246.329 458.777 1019.912 1671.400 496.565 584.302 1100.161 2580.506

GAN
0.627 4.037 2.561 3.322 6.592 6.054 14.102 16.099 56.791 7457.907 418.922 1976.482 4191.913 4096.990 1964.068 1948.733 2978.250 2654.381 18.180 148.114 66.959 20.751 20.028 59.927 205.072 560.057 1496.654 6920.808 434.970 2980.873 4360.021 4350.273 3162.742 4800.571 1375.055 2141.336

WGAN
22.517 31.937 16.402 87.685 21.481 22.469 36.612 69.350 140.923 4194.667 4288.632 1415.009 1494.503 5522.390 3033.507 1964.690 1837.948 1298.073 88.884 48.010 49.907 36.795 25.153 71.082 171.029 513.860 1497.029 362.710 1011.023 2383.723 2203.143 2992.306 2475.862 1403.601 1479.128 2282.581

WGAN-GP
7.451 5.233 3.961 5.983 3.420 4.014 7.781 17.915 47.643 246.576 158.407 373.751 688.596 764.674 482.750 424.794 347.022 389.408 125.598 20.331 32.291 22.505 17.247 63.669 174.666 529.873 1483.389 195.030 341.312 570.546 512.071 1376.382 471.726 694.724 827.537 1894.595

GAN-NR
5.940 3.522 3.021 2.621 3.515 6.244 5.641 14.710 42.885 7166.328 404.435 4292.463 2085.471 3614.357 2128.474 3194.469 709.904 2183.150 17.226 19.458 17.079 21.106 20.378 67.298 178.685 562.953 1498.844 6064.530 430.872 651.277 2302.820 3516.809 138.372 241.683 1005.823 1986.461

Table 3: Mixture distributions with 10000 training points using Hausdorff metric.

MoG2 ad50 id5 mtC r0.03 MoG5 ad50 id5 mtC r0.03 MoG10 ad50 id5 mtC r0.03 MoG20 ad50 id5 mtC r0.03 MoG50 ad50 id5 mtC r0.03 MoG10 ad50 id5 mtC r0.1 MoG10 ad50 id5 mtC r0.3 MoG10 ad50 id5 mtC r1.0 MoG10 ad50 id5 mtC r3.0 MoG2 ad50 id5 mtR r0.03 MoG5 ad50 id5 mtR r0.03 MoG10 ad50 id5 mtR r0.03 MoG20 ad50 id5 mtR r0.03 MoG50 ad50 id5 mtR r0.03 MoG10 ad50 id5 mtR r0.1 MoG10 ad50 id5 mtR r0.3 MoG10 ad50 id5 mtR r1.0 MoG10 ad50 id5 mtR r3.0 MoG2 ad50 id50 mtC r0.03 MoG5 ad50 id50 mtC r0.03 MoG10 ad50 id50 mtC r0.03 MoG20 ad50 id50 mtC r0.03 MoG50 ad50 id50 mtC r0.03 MoG10 ad50 id50 mtC r0.1 MoG10 ad50 id50 mtC r0.3 MoG10 ad50 id50 mtC r1.0 MoG10 ad50 id50 mtC r3.0 MoG2 ad50 id50 mtR r0.03 MoG5 ad50 id50 mtR r0.03 MoG10 ad50 id50 mtR r0.03 MoG20 ad50 id50 mtR r0.03 MoG50 ad50 id50 mtR r0.03 MoG10 ad50 id50 mtR r0.1 MoG10 ad50 id50 mtR r0.3 MoG10 ad50 id50 mtR r1.0 MoG10 ad50 id50 mtR r3.0

oracle
2.00 5.00 9.99 19.96 49.65 9.99 9.99 9.99 9.99 2.00 5.00 9.99 19.96 49.65 9.99 9.99 9.99 9.99 2.00 5.00 9.99 19.96 49.77 9.99 9.99 9.99 9.99 2.00 5.00 9.99 19.96 49.65 9.99 9.99 9.99 9.99

KDE
2.00 5.00 9.98 19.91 49.75 9.98 9.98 9.98 9.98 2.00 5.00 9.98 19.91 49.75 9.98 9.98 9.98 9.98 2.00 5.00 9.98 19.90 49.68 9.98 9.98 9.99 9.99 2.00 5.00 9.98 19.91 49.75 9.98 9.98 9.98 9.98

Gaussian
2.00 4.99 9.92 18.94 41.52 9.92 9.91 9.94 9.97 2.00 4.99 9.87 19.32 41.79 9.88 9.88 9.87 9.87 2.00 5.00 9.99 19.97 49.82 9.99 9.99 9.99 10.00 2.00 5.00 9.95 19.31 40.94 9.95 9.95 9.96 9.95

MOG
2.00 5.00 9.98 19.75 47.61 9.98 9.98 9.99 9.99 2.00 5.00 9.99 19.94 49.44 9.99 9.99 9.98 9.97 2.00 5.00 9.98 19.91 49.56 9.97 9.98 9.98 9.99 2.00 5.00 9.98 19.93 48.41 9.99 9.98 9.99 9.98

VAE
2.00 4.90 9.39 12.21 18.71 9.51 9.44 9.20 9.43 2.00 5.00 9.88 19.44 47.06 9.76 9.93 9.91 9.87 2.00 4.93 8.99 18.89 47.83 9.47 5.13 9.93 9.94 2.00 4.92 9.96 19.24 46.33 9.90 9.86 9.96 9.91

RealNVP
1.99 4.93 9.76 18.84 46.89 9.69 9.81 9.78 9.73 2.00 4.96 9.84 18.80 46.01 9.84 9.87 9.30 9.51 1.99 4.97 9.87 19.69 49.47 9.94 9.88 9.98 9.98 2.00 4.97 9.94 18.75 36.63 9.87 9.86 9.78 9.76

GAN
2.00 4.53 9.86 19.76 49.41 9.93 9.74 9.93 9.05 1.00 4.97 8.74 9.38 17.26 8.73 8.69 7.79 7.84 1.40 4.79 9.70 19.54 48.79 9.44 9.63 9.17 9.94 1.00 4.74 7.43 8.50 26.17 7.81 4.29 9.08 9.82

WGAN
1.99 4.92 9.91 8.13 42.50 9.43 9.73 9.79 9.95 1.10 4.81 9.97 19.41 45.32 9.67 9.88 9.92 9.93 1.87 4.97 9.87 19.94 49.64 9.81 9.31 9.97 9.28 1.99 4.99 9.78 19.87 47.89 9.87 9.96 9.85 9.92

WGAN-GP
2.00 4.99 9.98 19.83 49.46 9.98 9.98 9.98 9.94 2.00 4.95 9.97 19.71 48.64 9.99 9.89 9.99 9.94 1.99 5.00 8.07 19.72 46.83 9.92 9.95 9.67 9.85 2.00 5.00 9.99 19.45 44.29 9.97 9.87 9.97 9.98

GAN-NR
2.00 4.99 9.98 19.91 46.40 9.92 9.99 9.99 9.95 1.00 4.97 5.73 14.91 15.49 6.86 7.58 9.19 7.46 1.98 4.99 9.92 19.87 49.77 7.09 9.65 9.37 9.95 1.00 4.77 8.33 13.30 21.10 9.37 9.53 9.76 9.95

Table 4: Mixture distributions with 10000 training points using mode-coverage metric.

18

