Under review as a conference paper at ICLR 2019
EFFICIENT FEDERATED LEARNING VIA VARIATIONAL DROPOUT
Anonymous authors Paper under double-blind review
ABSTRACT
As an emerging field, federated learning has recently attracted considerable attention. Compared to distributed learning in the datacenter setting, federated learning has more strict constraints on computate efficiency of the learned model and communication cost during the training process. In this work, we propose an efficient federated learning framework based on variational dropout. Our approach is able to jointly learn a sparse model while reducing the amount of gradients exchanged during the iterative training process. We demonstrate the superior performance of our approach on achieving significant model compression and communication reduction ratios with no accuracy loss.
1 INTRODUCTION
Federated Learning is an emerging machine learning approach that has recently attracted considerable attention due to its wide range of applications in mobile scenarios (McMahan et al., 2017; Konecny` et al., 2016; Smith et al., 2017). It enables geographically distributed devices such as mobile phones to collaboratively learn a shared model while keeping the training data on each phone. This is different from standard machine learning approach which requires all the training data to be centralized in a server or in a datacenter. As such, federated learning enables distributing the knowledge across phones without sharing users' private data.
Federated Learning uses some form of distributed stochastic gradient descent (SGD) and requires a parameter server to coordinate the training process. The server initializes the model and distributes it to all the participating devices. In each distributed SGD iteration, each device computes the gradients of the model parameters using its local data. The server aggregates the gradients from each device, averages them, and sends the averaged gradients back. Each device then updates the model parameters using the averaged gradients. In such manner, each device benefits from obtaining a better model than the one trained only on the locally stored private data.
While federated learning shares some common features with distributed learning in the datacenter setting (Dean et al., 2012; Li et al., 2014) since they both use distributed SGD as the core training technique, federated learning has two more strict constraints which datacenter setting does not have:
Model Constraint: Compared to datacenters, mobile devices have much less compute resources. This requires the final model learned in the federated learning setting to be computationally efficient so that it can efficiently run on mobile devices.
Communication Constraint: In datacenters, communication between the server and working nodes during SGD is conducted via Gbps Ethernet or InfiniBand network with even higher bandwidth (Wen et al., 2017). In contrast, communication in the federated learning setting relies on wireless networks such as 4G and Wi-Fi. Both uplink and downlink bandwidths of those wireless networks are at Mbps scale, which is much lower than the Gbps scale in the datacenter setting. The limited bandwidth in the federated learning setting illustrates the necessity of reducing the communication cost to accelerate the training process.
In this work, we propose an efficient federated learning framework that meets both model and communication constraints. Our approach is inspired by variational dropout (Kingma et al., 2015). Our key idea is to jointly and iteratively sparsify the parameters of the shared model to be learned as well as the gradients exchanged between the server and the participating devices during the distributed SGD training process. By sparsifying parameters, only important parameters are kept, and the final model learned thus becomes computationally efficient run on mobile devices. By sparsifying
1

Under review as a conference paper at ICLR 2019

gradients, only important gradients are transmitted, and the communication cost is thus significantly reduced. We examine the performance of our framework on three deep neural networks and five datasets that fit the federated learning setting and are appropriate to be deployed on resource-limited mobile devices. Our experiment results show that our framework is able to achieve significant model compression and communication reduction ratios with no accuracy loss.

2 RELATED WORK
Federated Learning. As an emerging field, federated learning has recently attracted a lot of attention. McMahan et al. (2017) developed a federated learning approach based on iterative model averaging to tackle the statistical challenge, especially for mobile setting. Smith et al. (2017) proposed a distributed learning framework based on multi-task learning and demonstrated their approach is able to address the statistical challenge and is robust to stragglers. Konecny` et al. (2016) developed structured and sketched update techniques to reduce uplink communication cost in federated learning. In comparison, our work ­ to the best of our knowledge ­ represents the first federated learning framework that achieves model sparsification and communication reduction in a unified manner.
Gradient Compression. Our work is related to gradient compression in distributed SGD. One line of research is to use low-bit gradient representation to reduce the gradient communication overhead. In Wen et al. (2017), the authors proposed TernGrad which only used three numerical levels to represent gradients. In Alistarh et al. (2017), the authors presented QSGD that allows dynamic trade-off between accuracy and communication cost by choosing bit width of gradients. As another line of research, some previous works aimed to reduce communication cost by transmitting only a small portion of gradients. For example, Konecny` & Richtárik (2016) proposed using the structured update to sparsify the gradients. Aji & Heafield (2017) and Lin et al. (2018) used magnitudes as indicators to measure the importance of gradients and proposed to only transmit gradients whose magnitudes are larger than a threshold. In our work, we use an additional dropout parameter as the importance indicator, and transmit gradients based on the values of the dropout parameters stochastically.
Model Sparsification. Our work is also related to model sparsification. Model sparsification aims to reduce the computational intensity of deep neural networks by pruning out redundant model parameters. In (Han et al., 2016), the authors developed a parameter pruning method that can sparsify deep neural networks by an order of magnitude without losing the prediction accuracy. The novelty of our proposed approach lies at integrating model sparsification and communication reduction into a single optimization framework based on training data distributed at different devices.

3 PRELIMINARIES

Bayesian Neural Networks. Our work is inspired by Bayesian neural networks (Polson et al., 2017). Conventional neural networks learn scalar weights via maximum likelihood estimation. In comparison, a Bayesian neural network associates each weight with a distribution. Having a distribution instead of a single scalar value takes account of uncertainty for weight estimates and is thus more robust to overfitting (Hernández-Lobato & Adams, 2015).

Consider a Bayesian neural network parameterized by weight matrix w. Let p(w) denote the prior distribution of w. Given a dataset D with N data samples (xn, yn) for n  [1, N ], the posterior distribution of w is p(w|D) = p(D|w)p(w)/p(D). Unfortunately, p(w|D) is generally computa-
tionally intractable, and thus approximation approaches are needed. One popular approximation
approach is variational inference, which uses a parametric distribution q(w) to approximate the true posterior distribution p(w|D) (Kingma & Welling, 2013). The parameters  of q(w) are learned by maximizing the variational lower bound L() defined as follows (Kingma et al., 2015):

N
L() = Eq(w)log p(yn|xn, w) - DKL(q(w)||p(w)).
n=1

(1)

The first term on the right side measures the predictive performance of q(w). The second term is the KL-divergence between q(w) and p(w), which regularizes q(w) to be close to p(w).

Variational Dropout. Dropout is one of the most effective regularization methods for training deep neural networks (Srivastava et al., 2014). In traditional dropout training, either Bernoulli or Gaussian

2

Under review as a conference paper at ICLR 2019

Server
Average Gradient "

Server

Average Average

GGrraaddiieenntt""

Non-Sparse Model

...

PDriavatate

PDriavatate

PDriavatate

Sparse Model PDriavatate

...

PDriavatate

PDriavatate

Figure 1: Left: The standard federated learning framework. The gradients transmitted from each device to the central server is non-sparse, and the shared model learned from the distributed private data is non-sparse. Right: The proposed efficient federated learning framework based on unified gradient and model sparsification. The gradients transmitted from each device to the central server is sparse, and the shared model learned from the distributed private data is sparse.

noises are added to the weights with a fixed dropout rate p (Srivastava et al., 2014). In (Kingma
et al., 2015), the authors connected dropout with Bayesian neural networks, and showed that a
different dropout rate pij can be learned for each individual weight wij in w if the Gaussian noise ij  N (1, ij = pij/(1 - pij)) is added. As a result, each weight wij in w has the following Gaussian distribution parameterized by ij = (ij, ij):

wij  N (ij , ij i2j ) = qij (wij )

(2)

where iji2j is the variance of the Gaussian distribution, and ij is enforced to be greater than zero.

Given qij (wij) in (2), the first term in (1) can be approximated by Monte Carlo estimation (Kingma et al., 2015). To compute the second term in (1), the authors in (Kingma et al., 2015) adopted a prior distribution p(w) which makes DKL(q(w)||p(w)) only depend on ij. As a result, the second term in (1) can be approximated by

DKL(q(w)||p(w)) 

-0.64  (1.87 + 1.49  logij) + 0.5  log(1 + i-j1) + C, (3)

i,j

1 where C is a constant and  is the sigmoid function 1 + e-x .
Sparsifying Bayesian Neural Networks via Variational Dropout. Given that variational dropout allows learning different dropout rates for different weights, the authors in (Molchanov et al., 2017) showed that variational dropout can be used to sparsify Bayesian neural networks by pruning the weights whose learned dropout rates are high while still achieving comparable predictive accuracies comparing to the unpruned ones. This is because a large dropout rate corresponds to large noise in the weight. The large noise causes the value of this weight to be random and unbounded, which will corrupt the model prediction. As a result, pruning this weight out would be beneficial to the prediction performance of the learned neural network.

Our work is inspired by this key finding. We adopt variational dropout to sparsify neural networks in the federated learning setting. We describe the details of our approach in the following section.

4 OUR ALGORITHM
Standard Federated Learning Framework. Figure 1 (left) illustrates the standard federated learning framework based on synchronous SGD (we leave asynchronous SGD for future work). The framework consists of a parameter server and a number of geographically distributed devices. In each distributed SGD iteration, each device computes the gradients of the model parameters based on its local data, and sends the gradients (non-sparse) to the server. The server averages the gradients aggregated from every device and sends the averaged gradients (non-sparse) to all devices. Each device then uses the received averaged gradients to update its local model (non-sparse).

3

Under review as a conference paper at ICLR 2019

Algorithm 1 Efficient Federated Learning based on Variational Dropout.

Input: N devices with local private datasets D1, D2, D3, . . . , DN . Base model parameter 0 , Dropout parameter 0, Learning rate , the threshold value T.
Output: A sparse model w shared by N decentralized devices.

Initialize: i = 0, i = 0, i = 1, ..., N .

1: for each iteration do:

2: for i = 1 : N do:

3: Device i selects a mini-batch bi from local private dataset Di. 4: Device i computes the sparse gradients gi = i Li M (i), gi = i Li M (i),
where M (i) is a 0-1 binary matrix, and each entry M (ij ) is put to 0 if ij > T otherwise 1.

5: Device i uploads (gi , gi ) to the central server. 6: The central sever receives the uploaded model parameter gradients and computes their average:

g¯

=

1 N

N i=1

gi

,

g¯

=

1 N

N i=1

gi

.

7: for i = 1 : N do:

8: Device i pulls averaged gradients from the central server and updates parameters:

i  i -  · g¯ , i  i -  · g¯.

Given that the standard federated learning framework suffers from both model and communication constraints described in the introduction section, we propose an efficient federated learning framework (Figure 1 (right)) that meets both model and communication constraints.

Our Algorithm. Algorithm 1 formulates our proposed federated learning framework based on variational dropout. Let N denote the number of devices, and D1, D2, D3, ..., DN be the dataset stored inside the N devices, respectively. The goal of our proposed federated learning framework is to have all these N devices to collectively learn a sparse model at the final stage while reducing the communication cost during the training process.
Before the training process starts, the server initializes a base model parameterized by (0, 0), sets the learning rate to be , and distributes the base model and the learning rate to all the N devices. As such, all the devices have the same initial parameters to start with. In practice, 0 are initialized with random values; 0 can be initialized with either random values or the pretrained weights from other dataset such as ImageNet (Deng et al., 2009).
During each distributed SGD iteration, the ith device first selects a mini-batch bi from its local private dataset Di, and computes the gradients of the loss function in (1), including model parameter gradient i Li and dropout parameter gradient i Li, where i, i are the parameters of the ith device and Li is the variational lower bound of the ith device.
The iterative exchange of gradients (i Li, i Li) would incur massive communication overhead and becomes the communication bottleneck in standard federated learning framework. To reduce the communication overhead, our key idea is to utilize the dropout parameters  to identify important gradients to transmit to the server (Line 4). As explained in the preliminaries section, a large dropout rate corresponds to large noise in the weight. The large noise causes the value of this weight to be random and unbounded, which will corrupt the model prediction. As a result, pruning this weight out would be beneficial to the prediction performance of the learned neural network. Recall ij = pij/(1 - pij). A large dropout rate pij leads to a large dropout parameter ij. As such, the dropout parameter ij can be viewed as an importance indicator of its corresponding model parameter ij, where higher value ij corresponds to less important ij.
Inspired by this property, the ith device is able to identify important gradients (gi , gi ) by:

gi = i Li M (i) gi = i Li M (i)

(4)

where M (i) is a (0,1)-matrix and represents Hadamard product. Each entry M (ij) in M (i) is put to 0 if ij > T otherwise 1.
As shown in the experiment section, M (i) will become highly sparse during the training process and thus (gi , gi ) will also be highly sparse. It should be emphasized that although we upload a pair of sparse gradients (gi , gi ) (Figure 1 (right)) compared to non-sparse gradients gwi in the standard federated learning framework (Figure 1 (left)), due to highly sparse (gi , gi ), the total number of exchanged gradients is significantly reduced.

4

Under review as a conference paper at ICLR 2019
The remaining steps are the same with the standard federated learning framework except that we upload gi and download g¯. The server averages the uploaded gradients denoted as (g¯, g¯) and sends back to the devices. Finally, each device downloads the averaged parameter gradients from the server and updates the model parameters with the preset learning rate .
Algorithm Variants. During our experiments, we have observed two key insights. First, even though  and  are independent variables,  is empirically negatively correlated with , which is consistent with the observation in Molchanov et al. (2017). Second, ij is suppressed by large ij and it can hardly grow back unless DKL in (1) is removed. Based on these observations, we make some variants from Algorithm 1 to further reduce the communication cost.
Specifically, each device is forced to optimize  locally. It neither uploads the gradients of  to the server nor downloads the gradients of  from the server. The rationale behind this strategy is that since  is associated with  which is synchronized every iteration during SGD,  will thus be forced to be almost the same across all the devices. Thus we can only need to upload gradients of important model parameters g and omit the gradient of the dropout parameters.
In our experiments, we have implemented both Algorithm 1 and the variants described above. Our experimental results on variants show better performance on communication cost reduction while achieving convergence and the same test accuracies as Algorithm 1 and in our experimental section we only show the results on the algorithm variants. The experimental results on Algorithm 1 is shown in the supplementary material.
5 EXPERIMENTS
5.1 EXPERIMENTAL SETUP
Datasets and Deep Neural Networks. We evaluate the performance of our framework on three deep neural networks and five datasets that fit the federated learning setting. Specifically, we examined CifarNet (Krizhevsky et al., 2012) on MNIST (LeCun et al., 1998) and MNIST-Fashion (Xiao et al., 2017) datasets; ResNet-18 (He et al., 2016) on CIFAR-10 (Krizhevsky & Hinton, 2009) and Traffic Sign (tra) datasets, and VGG-like (Simonyan & Zisserman, 2015) on SVHN (Netzer et al., 2011) dataset, where VGG-like is obtained by removing the fully connected layers of the standard VGG-16 (Simonyan & Zisserman, 2015).
Protocol. We use the standard distributed learning framework illustrated in Figure 1 (left) as the baseline. We implement the baseline and our proposed framework using PyTorch (Paszke et al., 2017) and conduct experiments on NVIDIA 1080Ti GPUs. All the experimental settings are the same for the baseline and our proposed framework. In all experiments, we set the mini-batch size to 128 and used Adam (Kinga & Adam, 2015) as the SGD optimizer. Each dataset is randomly and evenly divided into N non-overlapping parts for N devices. To achieve state-of-the-art accuracy, all the deep neural networks were pre-trained on ImageNet (Deng et al., 2009) before they are trained on each of the five datasets. This setup also emulates the practical use scenario of federated learning in real-world settings where devices are loaded with pre-trained models before they participate in federated learning when new data comes in. For fair comparison, we run both the baseline and our proposed framework to full convergence on each dataset.
5.2 RESULTS
5.2.1 ACCURACY AND MODEL SPARSITY
We first examine the Top-1 test accuracy and the model sparsity achieved by our framework when 4 devices are included. Table 1 lists the top-1 accuracy achieved by the baseline (i.e., non-sparse) and our framework (i.e., sparse) as well as the the model sparsity (defined as the percentage of non-zero weights) for each dataset. Overall, our framework is able to sparsify the model with 1.4% to 7.2% non-zero weights across three different models trained on five different datasets. The sparse models have achieved comparable accuracy compared to the non-sparse ones.
5.2.2 COMMUNICATION COST REDUCTION
In the second set of experiments, we demonstrate the benefits of our framework in reducing communication costs during training. Figure 2 plots the communication cost curves of both the baseline (red dashed line) and our framework (blue solid line) during training on five datasets. The horizontal axis
5

Under review as a conference paper at ICLR 2019

Network
CifarNet CifarNet ResNet-18 ResNet-18 VGG-like

Table 1: Top-1 accuracy and model sparsity of 4 devices.

Dataset
MNIST MNIST-Fashion
CIFAR-10 Traffic Sign
SVHN

Params
1.3M 1.3M 11.7M 11.7M 18.8M

Top-1 Accuracy (Non-Sparse) 99.48% 91.60% 93.57% 98.42% 95.30%

Top-1 Accuracy (Sparse) 99.31% 91.51% 93.38% 98.18% 95.33%

Percentage of Non-Zero Weights
2.9% 7.2% 3.8% 1.4% 3.0%

represents the epoch number during training. The vertical axis represents the percentage of gradients transmitted during both uploading and downloading stages.
We have two observations from the results. First, our framework takes more epochs than the baseline to converge. For example, as shown in Figure 2 (a), it takes 20 epochs for our framework to converge while it takes 10 epochs for the baseline. This is because compared to sending all the gradients during each SGD iteration, sending a sparse version of the gradients leads to a larger variance. Second, the percentage of gradients transmitted drops very quickly at the beginning of the training process. This indicates that it only takes a small number of SGD iterations for our framework to learn a model with a significantly high sparsity. In other words, it demonstrates the efficiency of our framework on learning sparse models in the federated setting.
Given the fact that our approach takes more epochs to converge, the key question we need to answer is whether our approach can reduce the communication costs by enough to offset the overhead caused by the extra epochs. To answer this question, Table 2 lists the data communication volumes of the baseline (i.e., non-sparse) and our approach (i.e., sparse) for each dataset. The data communication volume is the total transmission data volume (including both uploading and downloading stages) accumulated from all the SGD iterations until convergence during training. The value of the data communication volume can be seen as the area under the communication cost curves depicted in Figure 2. As shown in Table 2, our approach not only reduces the communication costs by enough to offset the overhead caused by the extra epochs, but also cuts the communication costs to 12.2% to 22.2% of the non-sparse framework across datasets. The success in reducing the communication costs by this large is attributed to our approach's fast sparsification speed illustrated in Figure 2. In fact, the transmission of sparse gradients include both gradient value itself and its corresponding index. However, the corresponding index overhead is negligible due to the high sparsity, and we omit it in calculating the communication cost (Abadi et al., 2016).

Table 2: Data communication volume and percentage of communication cost with 4 devices.

Network
CifarNet CifarNet ResNet-18 ResNet-18 VGG-like

Dataset
MNIST MNIST-Fashion
CIFAR-10 Traffic Sign
SVHN

Data Communication Volume (Non-Sparse)
11.5 GB 34.5 GB 297.5 GB 200.3 GB 702.2 GB

Data Communication Volume (Sparse) 2.1 GB 7.6 GB 55.2 GB 24.6 GB 84.9 GB

Percentage of Communication Cost
18.2% 22.2% 18.5% 12.3% 12.0%

Transmitted Gradients

1 sparse non-sparse 1 sparse non-sparse

sparse non-sparse 1

sparse non-sparse 1

sparse non-sparse 1

0.5 0.5 0.5 0.5 0.5

0 0 10 Epochs
(a) MNIST

20 00

25 Epochs

50 00

25 Epochs

50 00

25 Epochs

50 00

30 Epochs

(b) MNIST-Fashion (c) CIFAR-10

(d) Traffic Sign

(e) SVHN

60

Figure 2: Communication cost curves during training on five datasets. The horizontal axis represents the epoch number during training. The vertical axis represents the percentage of gradients transmitted.

6

Under review as a conference paper at ICLR 2019

Non-sparse 4 nodes Sparse 4 nodes 100

Non-sparse 8 nodes Sparse 8 nodes

Non-sparse 16 nodes Sparse 16 nodes

Top-1 Accuracy %

50

0 MNIST

MNIST-Fashion

Cifar10

Traffic Sign

SVHN

Figure 3: The top-1 accuracy of 4, 8 and 16 devices for non-sparse distributed learning and sparse federated learning.

20
4 devices 8 devices 16 devices

50 4 devices 8 devices 16 devices

10 25

Percentage of Total Communication Cost (%)

Percentage of Non-zero Weights (%)

0 MNIST MNIST-Fashion Cifar10 Traffic Sign SVHN
(a)

0 MNIST MNIST-Fashion Cifar10 Traffic Sign SVHN
(b)

Figure 4: (a) The percentage of non-zero weights of the model after training for 4, 8, and 16 devices; (b) the percentage of communication cost for 4, 8 and 16 devices.

5.2.3 SCALABILITY ANALYSIS
In this experiment, we examine the scalability of our framework when extending to larger number of devices. Figure 3 shows the performance of accuracy when extending to 8 and 16 devices, and Figure 4 shows the percentage of non-zero weights, and the percentage of communication cost when extending to 8 and 16 devices. We have the following observations from the results. First, our framework doesn't incur any accuracy loss for 8 and 16 devices, demonstrating its robustness across different models and datasets. Second, we observe that our framework scales well when the number of devices increases to 8 and 16. Specifically, with 8 devices, our framework achieves 3.3% to 7.7% non-zero weights and and cuts the communication costs to 14.7% to 24.3% compared to the non-sparse model. With 16 devices, our framework achieves 4.4% to 8.4% non-zero weights and cuts the communication costs to 16.1% to 26.3% compared to the non-sparse model.
We also observe that both percentage of non-zero weight and communication reduction increase slightly when the number of devices increases. This is due to the way we set up our experiments in which we randomly and evenly divided each dataset into N non-overlapping parts for N devices. As the number of devices increases, the data size allocated to each device decreases. As a consequence, the data distribution at each device becomes less similar, which makes it challenging to learn a global shared model with the same high sparsity without accuracy loss. In our experiments, we focused on maintaining the accuracy and thus compromised with a slightly increased percentage of non-zero weights and communication reduction.
7

Under review as a conference paper at ICLR 2019

5.2.4 PERFORMANCE ANALYSIS FROM RANDOM INITIALIZATION
In previous experiments, the model parameters  are pretrained with ImageNet, and in this section we will display the performance of our proposed framework with random initialization weights. Table 3, Table 4 and Table 5 are the accuracy, percentage of non-zero weights and percentage of communication cost for 4, 8, and 16 devices with random initialization weights, respectively. Firstly, we see that the accuracy has decreased 2-3% compared to models using the pretrained weights. Secondly, the percentage of the non-zero weights and total communication cost has decreased accordingly compared to the model with pretrained weights. Starting from the random initialization has some drawbacks for the Bayesian dropout training since lots of weights will be pruned away in the early stage (Molchanov et al., 2017), before the model could possibly learn some useful representations of the data. As a result, the model sparsity will be higher while the accuracy will be lower. At the same time, the total communication cost will be lower compared to pretrained model due to the higher model sparsity.

Table 3: Top-1 accuracy of 4, 8, and 16 devices for random initialization weights.

Network CifarNet CifarNet ResNet-18 ResNet-18 VGG-like

Dataset MNIST MNIST-Fashion CIFAR-10 Traffic Sign SVHN

4 devices 98.35% 89.46% 90.32% 96.31% 92.19%

8 devices 98.19% 89.51% 90.19% 96.36% 92.18%

16 devices 98.23% 89.27% 90.29% 96.33% 92.21%

Table 4: Percentage of non-zero weights of 4, 8, and 16 devices for random initialization weights.

Network CifarNet CifarNet ResNet-18 ResNet-18 VGG-like

Dataset MNIST MNIST-Fashion CIFAR-10 Traffic Sign SVHN

4 devices 1.9% 4.3% 2.4% 1.1% 1.3%

8 devices 2.8% 5.4% 3.5% 1.8% 2.0%

16 devices 3.2% 5.8% 4.1% 2.3% 2.7%

Table 5: Percentage of communication cost of 4, 8, and 16 devices for random initialization weights.

Network CifarNet CifarNet ResNet-18 ResNet-18 VGG-like

Dataset MNIST MNIST-Fashion CIFAR-10 Traffic Sign SVHN

4 devices 11.6% 12.1% 11.4% 7.3% 7.0%

8 devices 13.7% 12.6% 13.3% 9.7% 8.7%

16 devices 15.2% 13.7% 14.2% 10.5% 9.9%

6 CONCLUSION AND FUTURE WORK
In this paper, we have presented a novel federated learning framework based on variational dropout for efficiently learning deep neural networks from distributed data. Our experiments on diverse deep network architectures and datasets show that our framework achieves high model sparsity with 2.9% to 7.2% non-zero weights and cuts the communication costs from 12.0% to 18.2% with no accuracy loss. Our experiments also show that our framework scales well when the number of devices increases. While this work focuses on the synchronous distributed SGD setting, we plan to examine the performance of our framework in the asynchronous distributed SGD setting as our future work. We also plan to explore the potential of our framework in further compressing the model and reducing the communication cost by combining with the orthogonal gradient quantization approaches.
7 ACKNOWLEDGEMENT
We are grateful to NVIDIA Corporation for the donation of GPUs to support our research.
8

Under review as a conference paper at ICLR 2019
REFERENCES
http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset.
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale machine learning. In OSDI, volume 16, pp. 265­283, 2016.
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. In Conference on Empirical Methods in Natural Language Processing, 2017.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communicationefficient sgd via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pp. 1707­1718, 2017.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in neural information processing systems, pp. 1223­1231, 2012.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. IEEE, 2009.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations (ICLR), 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
José Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In International Conference on Machine Learning, pp. 1861­1869, 2015.
D Kinga and J Ba Adam. A method for stochastic optimization. In International Conference on Learning Representations (ICLR), volume 5, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In The 2nd International Conference on Learning Representations (ICLR), 2013.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pp. 2575­2583, 2015.
Jakub Konecny` and Peter Richtárik. Randomized distributed mean estimation: Accuracy vs communication. arXiv preprint arXiv:1611.07555, 2016.
Jakub Konecny`, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In OSDI, volume 14, pp. 583­598, 2014.
9

Under review as a conference paper at ICLR 2019
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. In International Conference on LearningRepresentations (ICLR), 2018.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20 th International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In Proceedings of the 34th International Conference on Machine Learning, 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 5, 2011.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Nicholas G Polson, Vadim Sokolov, et al. Deep learning: A bayesian perspective. Bayesian Analysis, 12(4):1275­1304, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning. In Advances in Neural Information Processing Systems, pp. 4427­4437, 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, pp. 1508­1518, 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
10

Under review as a conference paper at ICLR 2019

A TOP-1 ACCURACY COMPARISON OF DIFFERENT TRAINING SETTINGS
In this section, we compare the performance of Algorithm 1 with different settings. The first setting is shown as our experiment section that model using the pretrained weights from Imagenet. The second setting is that we train our neural network from random initialization weights which is also shown in experiment section. The third setting is that we share the gradients of  using the pretrained weights. The accuracy performance comparison of these settings for 4, 8 and 16 devices are summarized in Table 6, Table 7 and Table 8. To be noted here, in our experimental setting, we don't not share  in order to further reduce communication cost for both the pretrained model and random initialized model. We have two observations. Firstly, the accuracy of the setting that shares  using the pretrained weights has the same accuracy of the setting using the pretrained weights without sharing . Secondly, if the model is initialized with random weights, the accuracy will drop 2-3% compared to other two settings using pretrained weights.
Table 6: The accuracy comparison of 4 devices

Network
CifarNet CifarNet ResNet-18 ResNet-18 VGG-like

Dataset
MNIST MNIST-Fashion
CIFAR-10 Traffic Sign
SVHN

Pretrained weights without sharing 
99.31% 91.51% 93.38% 98.18% 95.33%

Pretrained weights share  99.35% 91.49% 93.41% 98.16% 95.28%

Random initial weights
98.35% 89.46% 90.32% 96.31% 92.19%

Table 7: The accuracy comparison of 8 devices

Network
CifarNet CifarNet ResNet-18 ResNet-18 VGG-like

Dataset
MNIST MNIST-Fashion
CIFAR-10 Traffic Sign
SVHN

Pretrained weights without sharing 
99.37% 91.39% 93.48% 98.35% 95.20%

Pretrained weights share  99.23% 91.36% 93.45% 98.29% 95.38%

Random initial weights
98.19% 89.51% 89.49% 96.36% 92.18%

Table 8: The accuracy comparison of 16 devices

Network
CifarNet CifarNet ResNet-18 ResNet-18 VGG-like

Dataset
MNIST MNIST-Fashion
CIFAR-10 Traffic Sign
SVHN

Pretrained weights without sharing 
99.41% 91.42% 93.52% 98.29% 95.31%

Pretrained weights share  99.33% 91.47% 93.34% 98.38% 95.29%

Random initial weights
98.23% 89.27% 90.29% 96.33% 92.21%

B PERCENTAGE OF NON-ZERO WEIGHTS COMPARISON OF DIFFERENT TRAINING SETTINGS
In this section, we report the percentage of non-zero weights for these three settings described above, and the results are shown as Figure 5.
We discover that the percentage of non-zero weights of the setting that shares  using the pretrained weights has the same level with the one uisng pretrained weights without sharing . If we train the model from random initialized weights, the percentage of non-zero weights is lower than the two other settings, it means that we will obtain a more sparse model after training. However, the model accuracy will also be lower compared to other two settings.
11

Under review as a conference paper at ICLR 2019

Non-Zero Elements Percentage (%)

Pretrained weights without sharing 

Pretrained weights share 

Random initial weights

20

15

10

5

0 MNISTMNIST-FashionCifar10 Traffic Sign SVHN
(a) 4 devices

MNISTMNIST-Fashion Cifar10 Traffic Sign SVHN
(b) 8 devices

MNISTMNIST-Fashion Cifar10 Traffic Sign SVHN
(c) 16 devices

Figure 5: The percentage of non-zero weights of 4, 8, and 16 devices for sparse federated learning with pretrained weights, random weights and pretrained weights with shared .

C PERCENTAGE OF TOTAL COMMUNICATION COMPARISON OF DIFFERENT TRAINING SETTINGS

In this section, we report the total communication percentage of these three settings described above, and the results are shown as Figure 6.
First, we find that if the pretraied model shares  during the training, then the total communication percentage is nearly twice as much as the pretrained model without sharing. The communication cost is doubled because we share a pair of gradients (gi , gi ) compared to sharing gi only. Second, if we train our model from random initialized weights, the total communication cost in the minimal among the three settings. It is reasonable because the percentage of non-zero weights from random initialized weights is the lowest and thus the total communication exchange during training is also the lowest.

Pretrained weights without sharing 

Pretrained weights share 

Random initial weights

60

Total Communication Percentage (%)

45

30 15

0

MNIST MNIST-Fashion Cifar10 Traffic Sign SVHN MNIST MNIST-Fashion Cifar10 Traffic Sign SVHN MNIST MNIST-Fashion Cifar10 Traffic Sign SVHN

(a) 4 devices

(b) 8 devices

(c) 16 devices

Figure 6: The total communication percentage of 4, 8, and 16 devices for sparse federated learning with pretrained weights, random weights and pretrained weights with shared .

12

