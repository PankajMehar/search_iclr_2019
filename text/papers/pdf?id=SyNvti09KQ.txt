Under review as a conference paper at ICLR 2019
VISCERAL MACHINES: REINFORCEMENT LEARNING WITH INTRINSIC PHYSIOLOGICAL REWARDS
Anonymous authors Paper under double-blind review
ABSTRACT
The human autonomic nervous system has evolved over millions of years and is essential for survival and responding to threats. As people learn to navigate the world, "fight or flight" responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger. We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.
1 INTRODUCTION
The biologically adaptive value of certain behaviors, e.g., the perception of motion, depth and wariness of heights, has led to theories that these are innate (Campos et al., 1992). The sympathetic nervous system (SNS) is "hard-wired" to respond to potentially dangerous situations such as these, often reducing, or by-passing, the need for conscious processing. The ability to make rapid decisions and respond to immediate threats is one way for protecting oneself from danger. Whether one is in the African savanna or driving in Boston traffic.
The SNS regulates a range of visceral functions from the cardiovascular system to the adrenal system (Jansen et al., 1995). The anticipatory response in humans to a threatening situation is for the heart rate to increase, heart rate variability to decrease, blood to be diverted from the extremities and the sweat glands to dilate. This is the body's "fight or flight" response.
While the primary role of these anticipatory responses is to help one prepare for action, they also play a part in our appraisal of a situation. The combination of sensory input, physiological responses and cognitive evaluation form emotions that influence how humans learn, plan and make decisions (Loewenstein & Lerner, 2003). Intrinsic motivation refers to being moved to act based on the way it makes one feel. For example, it is generally undesirable to be in a situation that causes fear and thus we might choose to take actions that help avoid these types of contexts in future. This is contrasted with extrinsic motivation that involves explicit goals (Chentanez et al., 2005).
Driving is an everyday example of a task in which we commonly rely on both intrinsic and extrinsic motivations and experience significant physiological changes. When traveling in a car at high-speed one may experience a heightened state of arousal. This automatic response is correlated with the body's reaction to the greater threats posed by the situation (e.g., the need to adjust steering more rapidly to avoid a pedestrian that might step into the road). Visceral responses are likely to preempt accidents or other events (e.g., a person will become nervous before losing control and hitting someone). Therefore, these signals potentially offer an advantage as a reward mechanism compared to extrinsic rewards based on events that occur in the environment, such as a collision. This paper provides a reinforcement learning (RL) framework that incorporates reward functions for achieving not only the task-specific goals but which also minimizing a cost trained on physiological responses to the environment that are correlated with stress. We ask if such a reward functions with extrinsic
1

Under review as a conference paper at ICLR 2019
Figure 1: We present a novel approach to reinforcement learning that leverages an artificial network trained on physiological signals correlated with autonomic nervous system responses.
and intrinsic components are useful in a reinforcement learning setting. We test our approach by training a model on real visceral human responses in a driving task. The key challenges of applying RL in the real-world include the amount of training data required and the high-cost associated with failure cases. For example, when using RL in autonomous driving, rewards are often sparse and skewed. Furthermore, bad actions can lead to states that are both catastrophic and expensive to recover from. While much of the work in RL focuses on mechanisms that are task or goal dependent, it is clear that humans also consider the feedback from the body's nervous system for action selection. For example, responses correlated with the affective states of fear and surprise can help signal imminent danger or failure to achieve a goal. Such mechanisms in an RL agent could help reduce the sample complexity as the rewards are continually available and could signal success or failure before the end of the episode. Furthermore, these visceral signals provide a warning mechanism that in turn could lead to safer explorations. Our work is most closely related to that in intrinsically motivated learning (Chentanez et al., 2005; Zheng et al., 2018; Haber et al., 2018; Pathak et al., 2017) that uses a combination of intrinsic and extrinsic rewards and shows benefits compared to using extrinsic rewards alone. The key distinction in our work is that we specifically aim to build intrinsic reward mechanisms that are visceral and trained on signals correlated with human affective responses. To summarize, the core contributions of this paper are to: (1) present a novel approach to learning in which the reward function is augmented with a model learned directly from human nervous system responses, (2) show how this model can be incorporated into a reinforcement learning paradigm and (3) report the results of experiments that show the model can improve both safety (reducing the number of mistakes) and efficiency (reducing the sample complexity) of learning. In summary, we argue that a function trained on visceral physiological responses could be used as an intrinsic reward or value functions for artificially intelligent system. We hypothesize that incorporating intrinsic rewards with extrinsic rewards in an RL framework (as shown in Fig 1) will both improve learning efficiency as well as reduce catastrophic failure cases that occur during the training.
2

Under review as a conference paper at ICLR 2019

2 BACKGROUND
2.1 SYMPATHETIC NERVOUS SYSTEM
The SNS is activated globally in response to fear and threats. Typically, when threats in an environment are associated with a "fight of flight" response the result is an increase in heart rate and perspiration and release of adrenaline and cortisol into the circulatory system. These physiological changes act to help us physically avoid danger but also play a role in our appraisal of emotions and ultimately our decision-making. A large volume of research has found that purely rational decision-making is sub-optimal (Lerner et al., 2015). This research could be interpreted as indicating that intrinsic rewards (e.g., physiological responses and the appraisal of an emotion) serve a valuable purpose in decision-making. Thus, automatic responses both help people act quickly and in some cases help them make better decisions. While these automatic responses can be prone to mistakes, they are vital for keeping us safe. Logical evaluation of a situation and the threat it presents is also important. Ultimately, a combination of intrinsic emotional rewards and extrinsic rational rewards, based on the goals one has, is likely to lead to optimal results.

2.2 REINFORCEMENT LEARNING

We consider the the standard RL framework, where an agent interacts with the environment (described

by a set of states S), through a set of actions (A). An action at at a time-step t leads to a distribution

over the possible future state p(st+1|st, at), and a reward r : S × A  R. In addition, we start

with sum

a distribution of initial of future rewards: Rt =

states
 i=t

p(s0) and the i-tri, where

goal of  is the

the agent is to maximize the discounted discount factor. Algorithms such as Deep

Q-Networks (DQN) learn a Neural-Network representation of a deterministic policy  : S  A that

approximates an optimal Q-function: Q(s, a) = Es p(·|s,a)[r(s, a) +  maxa A Q(s , a )].

The application of RL techniques to real-world scenarios, such as autonomous driving, is challenging due to the high sample complexity of the methods. High-sample complexity arises due to the creditassignment problem: it is difficult to identify which specific action from a sequence was responsible for a success or failure. This issue is further exacerbated in scenarios where the rewards are sparse. Reward shaping (Ng et al., 1999; Russell, 1998) is one way to deal with the sample complexity problem, in which heuristics are used to boost the likelihood of determining the responsible action.

We contrast sparse episodic reward signals in RL agents with physiological responses in humans. We conject that the sympathetic nervous system (SNS) responses for a driver are as informative and useful, and provide a more continuous form of feedback. An example of one such SNS response is the volumetric change in blood in the periphery of skin, controlled in part through vasomodulation. We propose to use a reward signal that is trained on a physiological signal that captures sympathetic nervous system activity. The key insight being that physiological responses in humans indicate adverse and risky situations much before the actual end-of-episode event (e.g. an accident) and even if the event never occurs. By utilizing such a reward function, not only is the system able to get a more continuous and dense rewards but it also allows us to reason about the credit assignment problem. This is due to the fact that an SNS response is often tied causally to the set of actions responsible for the eventual success or failure of the episode.

Our work is related, in spirit, to a recent study that used facial expressions as implicit feedback to help train a machine learning systems for image generation (Jaques et al., 2018). The model produced sketches that led to significantly more positive facial expressions when trained with input of smile responses from an independent group. However, this work was based on the idea of Social Learning Theory (Bandura & Walters, 1977) and that humans learn from observing the behaviors of others, rather than using their own nervous system response as a reward function.

3 THE PROPOSED FRAMEWORK
Our proposal is to consider a reward function that has both an extrinsic component r and an intrinsic component r~. The extrinsic component rewards behaviors that are task specific, whereas the intrinsic component specifically aims to predict a human physiological response to SNS activity and reward actions that lead to states that reduce stress and anxiety. The final reward r^ then is a function that

3

Under review as a conference paper at ICLR 2019

Figure 2: Example of the blood volume pulse wave during driving in the simulated environment. A zoomed in section of the pulse wave with frames from the view of the driver are shown. Note how the pulse wave pinches between seconds 285 and 300, during this period the driver collided with a wall while turning sharply to avoid another obstacle. The pinching begins before the collision occurs as the driver's anticipatory response is activated.

considers both the extrinsic as well as intrinsic components r^ = f (r, r~). Theoretically, the function

f (·, ·) can be fairly complex and one possibility would be to parameterize it as a Neural Network.

For simplicity, we consider linear combinations of the extrinsic and intrinsic reward in this paper.

Formally, lets consider an RL framework based on a DQN with reward r. We propose to use a

modified reward r^ that is a convex combination of the original reward r and a component that mirrors

human physiological responses r~:

r^ = r + (1 - )r~

(1)

Here  is a weighting parameter that provides a trade-off between the desire for task completion (extrinsic motivation) and physiological response (intrinsic motivation). For example, in an autonomous driving scenario the task dependent reward r can be the velocity, while r~ can correspond to physiological responses associated with safety. The goal of the system then is to complete the task while minimizing the physiological arousal response. The key challenge now is to build a computational model of the intrinsic reward r~ given the state of the agent.

In the rest of the paper we focus on the autonomous driving scenario as a canonical example and discuss how we can model the appropriate physiological responses and utilize them effectively in this framework. One of the greatest challenge in building a predictive model of SNS response is the collection of realistic ground truth data. In this work, we use high-fidelity simulations (Shah et al., 2018) to collect physiological responses of humans and then train a deep neural network to predict SNS responses that will ultimately be used during the reinforcement learning. In particular, we rely on the photoplethysmographic (PPG) signal to capture the volumetric change in blood in the periphery of the skin (Allen, 2007). The blood volume pulse waveform envelope pinches when a person is startled, fearful or anxious, which is the result of the body diverting blood from the extremities to the vital organs and working muscles to prepare them for action, the "fight or flight" response. Use of this phenomenon in affective computing applications is well established and has been leveraged to capture emotional responses in marketing/media testing (Wilson & Sasse, 2000), computer tasks (Scheirer et al., 2002) and many other psychological studies (L. Fredrickson & Levenson, 1998; Gross, 2002). The peripheral pulse can be measured unobtrusively and even without contact (Poh et al., 2010), making it a good candidate signal for scalable measurement. We leverage the pulse signal to capture

4

Under review as a conference paper at ICLR 2019

Figure 3: We used a eight-layer CNN (seven convolutional layers and a fully connected layer) to estimate the normalized pulse amplitude derived from the physiological response of the driver. The inputs were the frames from the virtual environment, AirSim.

aspects of the nervous system response and our core idea is to train an artificial network to mimic the pulse amplitude variations based on the visual input from the perspective of the driver.
To design a reward function based on the nervous system response of the driver in the simulated environment we collected a data set of physiological recordings and synchronized first person video frames from the car. Using this data we trained a convolutional neural network (CNN) to mimic the physiological response based on the input images. Fig 2 shows a section of the recorded blood volume pulse signal with pulse peaks highlighted, notice how the waveform envelope changes.
Reinforcement Learning Environments: We performed our experiments in AirSim (Shah et al., 2018) where we instantiated an autonomous car in a maze. The car was equipped with an RGB camera and the goal for the agent was to learn a policy that maps the camera input to a set of controls (discrete set of steering angles). The agent's extrinsic reward can be based on various driving related tasks, such as keeping up the velocity, making progress towards a goal, traveling large distances, and can be penalized heavily for collisions. Fig 2 shows example frames captured from the environment. The maze consisted of walls and ramps and was designed to be non-trivial to navigate for the driver.
Intrinsic Reward Architecture: We used a CNN to predict the normalized pulse amplitude derived from the physiological response of the driver. The image frames from the camera sensor in the environment served as an input to the network. The input frames were downsampled to 84 × 84 pixels and converted to grayscale format. They were normalized by subtracting the mean pixel value (calculated on the training set). The network architecture is illustrated in Fig 3. A dense layer of 128 hidden units preceded the final layer that had linear activation units and a mean square error (MSE) loss, so the output formed a continuous signal from 0 to 1.
Training the Reward Network: We recruited four participants (2 male, 2 female) to drive a vehicle around the maze and to find the exit point. All participants were licensed drivers and had at least seven years driving experience. For each participant we collected approximately 20 minutes (24,000 frames at a resolution of 256 × 144 pixels and frame rate of 20 frames-per-second) of continuous driving in the virtual environment (for a summary of data see Table 1). In addition, the PPG signal was recorded from the index finger of the non-dominant hand using a Shimmer31 GSR+ with an optical pulse sensor. The signal was recorded at 51.6Hz. The physiological signals were synchronized with the frames from the virtual environment using the same computer clock. A standard custom peak detection algorithm (McDuff et al., 2014) was used to recover the systolic peaks from the pulse waveform. The amplitudes of the peaks were normalized, to a range 0 to 1, across the entire recording. Following the experiment the participants reported how stressful they found the task (Not at all, A little, A moderate amount, A lot, A great deal). The participants all reported experiencing some stress during the driving task. The frames and pulse amplitude measures were then used to train the CNN (details in the next section). The output of the resulting trained CNN (the visceral machine) was used as the reward (r~ = CNN Output) in the proposed framework.

1http://www.shimmersensing.com/

5

Under review as a conference paper at ICLR 2019
Figure 4: Frames from the environment ordered by the predicted pulse amplitude from our CNN intrinsic reward model. A lower values indicated a higher SNS/"fight or flight" response. This is associated with more dangerous situations (e.g., driving close to walls and turning in tight spaces).
4 EXPERIMENTS AND RESULTS
We conducted experiments to answer: (1) if we can build a deep predictive model that estimates a peripheral physiological response associated with SNS activity and (2) if using such predicted responses leads to sample efficiency in the RL framework. We use DQN as a base level approach and build our proposed changes on top of it. We consider three different tasks in the domain of autonomous driving: (a) keeping the velocity high (r is instantaneous velocity), (b) traveling long straight-line distances from the origin (r is absolute distance from origin) without any mishaps and (c) driving towards a goal (r = 10 if the goal is achieved). While the velocity and distance task provides dense rewards, the goal directed task is an example where the rewards are sparse and episodic. Note that in all the three we terminate the episode with a high negative reward (r = -10) if a collision happens. 4.1 HOW WELL CAN WE PREDICT BVP AMPLITUDE? We trained five models, one for each of the four participants independently and one for all the participants combined. In each case, the first 75% of frames from the experimental recordings were taken as training examples and the latter 25% as testing examples. The data in the training split was randomized and a batch size of 128 examples was used. Max pooling was inserted between layers 2 and 3, layers 4 and 5, and layers 7 and 8. To overcome overfitting, a dropout layer (Srivastava et al., 2014) was added after layer 7 with rate d1 = 0.5. The loss during training of the reward model was the mean squared error. Each model was trained for 50 epochs after which the training root mean squared error (RMSE) loss was under 0.1 for all models. The RMSE was then calculated on the independent test set and was between 0.10 and 0.19 for all participants (see Table 1). Fig 4 illustrates how a trained CNN associates different rewards to various situations. Specifically, we show different examples of the predicted pulse amplitudes on an independent set of the frames from the simulated environment. A lower value indicates a higher stress response. Quantitatively and qualitatively these results show that we could predict the pulse amplitude and that pinching in the peripheral pulse wave, and increased SNS response, was associated with approaching (but not necessarily contacting) obstacles. The remaining results were calculated using the model from P1; however, similar data were obtained from the other models indicating that the performance generalized across participants.
6

Under review as a conference paper at ICLR 2019

Average Velocity Average Distance Travelled

Part. Gender Age Driving Exp. Was the Task # Frames Testing Loss

(Yrs)

(Yrs)

Stressful?

(RMSE)

P1 P2 P3 P4 All P.

M F F M

31 37 33 31

8 20 7 15

A lot A lot A little A little

28,968 23,005 23,789 25,972 101,734

.189 .100 .102 .116 .115

Table 1: Summary of the Data and Testing Loss of our Pulse Amplitude Prediction Algorithm.

Average Distance to Goal Travelled

2.75 2.7
2.65

6=0 6 = 0.25 6 = 0.5 6 = 0.75 6=1

Velocity

2.6

Distance
4.6 6=0 6 = 0.25
4.4 6 = 0.5 6 = 0.75 6=1
4.2
4
3.8

Goal Oriented
6 6=0 6 = 0.25
5 6 = 0.5 6 = 0.75 6=1
4
3
2

2.55 3.6

30 50 70 90 110 130 150

30 50 70 90 110 130 150

Number of Episodes

Number of Episodes

1 30 50 70 90 110 130 150
Number of Episodes

Figure 5: The graph plots average extrinsic reward per episode as the system evolves over time for

different values of . For all the three tasks we observe that using appropriately balanced visceral

rewards with the extrinsic reward leads to better learning rates when compared to either vanilla DQN

(magenta triangle  = 1) or DQN that only has the visceral component (red circle  = 0).

Average Length of Episode

33 6=0
6 = 0.25 32 6 = 0.5
6 = 0.75
6=1 31

Velocity

30

Distance
40 6=0 6 = 0.25 6 = 0.5
35 6 = 0.75 6=1
30

Goal Oriented
40 6=0 6 = 0.25 6 = 0.5
35 6 = 0.75 6=1
30

29 25 25

28 30 50 70 90 110 130 150 Number of Episodes

30 50 70 90 110 130 150 Number of Episodes

30 50 70 90 110 130 150 Number of Episodes

Figure 6: The graph plots average length per episode as the system evolves over time. For all the

three tasks we observe that using visceral reward components leads to better longer episodes when

compared to vanilla DQN ( = 1). This implies that the agent with the visceral reward component

becomes more cautious about collisions sooner.

4.2 DOES THE VISCERAL REWARD COMPONENT IMPROVE PERFORMANCE?
We then used the trained CNN as the visceral reward component in a DQN framework and used various values of  to control the relative weight when compared to the task dependent reward component. Fig 5 shows the mean extrinsic reward per episode as a function of training time. The plots are averaged over 10 different RL runs and we show plots for different values of . When  = 1 that RL agent is executing vanilla DQN, whereas  = 0 means that there is no extrinsic reward signal. For all the three tasks, we observe that the learning rate improves significantly when  is either non-zero or not equal to 1. One of the main reasons is that the rewards are non-sparse with the visceral reward component contributing effectively to the learning. Low values of  promote a risk-averse behavior in the agent and higher values  train an agent with better task-specific behavior, but require longer periods of training. It is the mid-range values of  (e.g. 0.25) that lead to optimal behavior both in terms of the learning rate and the desire to accomplish the mission.
7

Average Length of Episode Average Length of Episode

Under review as a conference paper at ICLR 2019

Figure 7: Comparison of the CNN based intrinsic reward with a reward shaping mechanism. The plots are (left) average extrinsic reward per episode and (right) length of episode as the system evolves and show the advantages of the CNN based approach in both cases.

Average Velocity Average Length of Episode

Velocity
2.8 CNN with 6 = 0.25 Heuristic with 6 = 0.25
2.7
2.6
2.5
2.4
2.3 30 50 70 90 110 130 150 Number of Episodes

Velocity
34 CNN with 6 = 0.25
32 Heuristic with 6 = 0.25
30
28
26
24
22
20 30 50 70 90 110 130 150 Number of Episodes

4.3 DOES THE VISCERAL REWARD COMPONENT HELP REDUCE COLLISIONS?
Fig 6 plots how the average length of an episode changes with training time for different values of . Note that we consider an episode terminated when the agent experiences a collision, so the length of the episode is a surrogate measure of how cautious an agent is. We observed that a low value of  leads to longer episodes sooner while the high values do not lead to much improvement overall. Essentially, a low value of  leads to risk aversion without having the desire to accomplish the task. This results in a behavior where the agent is happy to make minimal movements while staying safe.

4.4 HOW DOES THE PERFORMANCE COMPARE TO REWARD SHAPING?
Something we question is, is the CNN predicting the SNS responses doing more than predicting distances to the wall and if there are ways in which the original reward can be shaped to include that information? We did RL experiments where we compared the proposed architecture ( = 0.25) with an agent that replaced the intrinsic reward component with the reward 1 - exp[-|distance to wall|]. Note that such distance measures are often available through sensors (such as sonar, radar etc.); however, given the luxury of the simulation we chose to use the exact distance for simplicity. Fig 7 shows both the average reward per episode as well as average length per episode, for the velocity task, as a function of training time. We observed that the agent that had used the CNN for the intrinsic reward component performs better than the heuristic. We believe that the trained CNN is far richer than the simple distance-based measure and is able to capture the context around the task of driving the car in confined spaces (e.g., avoiding turning at high speeds and rolling the car).

5 CONCLUSION AND FUTURE WORK
Visceral emotional responses and heightened arousal are innate responses we experience. We have presented a novel reinforcement learning paradigm using an intrinsic reward function trained on peripheral physiological responses and extrinsic rewards based on mission goals. First, we trained a neural architecture to predict a driver's peripheral blood flow modulation based on the first-person video from the vehicle. This architecture acted as the reward in our reinforcement learning step. A major advantage of training a reward on a signal correlated with the sympathetic nervous system responses is that the rewards are non-sparse - the negative reward starts to show up much before the car collides. This leads to efficiency in training and with proper design can lead to policies that are also aligned with the desired mission. While emotions are important for decision-making (Lerner et al., 2015), they can also detrimentally effect decisions in certain contexts. Future work how to balance intrinsic and extrinsic rewards and include extensions to representations that include multiple intrinsic drives (such as hunger, fear and pain).

REFERENCES
Allen, John. Photoplethysmography and its application in clinical physiological measurement. Physiological measurement, 28(3):R1, 2007.
Bandura, Albert and Walters, Richard H. Social learning theory, volume 1. Prentice-hall Englewood Cliffs, NJ, 1977.
Campos, Joseph J, Bertenthal, Bennett I, and Kermoian, Rosanne. Early experience and emotional development: The emergence of wariness of heights, 1992.
8

Under review as a conference paper at ICLR 2019
Chentanez, Nuttapong, Barto, Andrew G, and Singh, Satinder P. Intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 1281­1288, 2005.
Gross, James J. Emotion regulation: Affective, cognitive, and social consequences. Psychophysiology, 39(3): 281­291, 2002.
Haber, Nick, Mrowca, Damian, Fei-Fei, Li, and Yamins, Daniel L. K. Learning to play with intrinsicallymotivated self-aware agents. arXiv preprint arXiv:1802.07442, 2018.
Jansen, Arthur SP, Van Nguyen, Xay, Karpitskiy, Vladimir, Mettenleiter, Thomas C, and Loewy, Arthur D. Central command neurons of the sympathetic nervous system: basis of the fight-or-flight response. Science, 270(5236):644­646, 1995.
Jaques, Natasha, Engel, Jesse, Ha, David, Bertsch, Fred, Picard, Rosalind, and Eck, Douglas. Learning via social awareness: improving sketch representations with facial feedback. arXiv preprint arXiv:1802.04877, 2018.
L. Fredrickson, Barbara and Levenson, Robert W. Positive emotions speed recovery from the cardiovascular sequelae of negative emotions. Cognition & emotion, 12(2):191­220, 1998.
Lerner, Jennifer S, Li, Ye, Valdesolo, Piercarlo, and Kassam, Karim S. Emotion and decision making. Annual Review of Psychology, 66, 2015.
Loewenstein, George and Lerner, Jennifer S. The role of affect in decision making. Handbook of affective science, 619(642):3, 2003.
McDuff, Daniel, Gontarek, Sarah, and Picard, Rosalind W. Remote detection of photoplethysmographic systolic and diastolic peaks using a digital camera. IEEE Transactions on Biomedical Engineering, 61(12):2948­2954, 2014.
Ng, Andrew Y, Harada, Daishi, and Russell, Stuart. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pp. 278­287, 1999.
Pathak, Deepak, Agrawal, Pulkit, Efros, Alexei A., and Darrell, Trevor. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, pp. 2778­2787, 2017.
Poh, Ming-Zher, McDuff, Daniel J, and Picard, Rosalind W. Non-contact, automated cardiac pulse measurements using video imaging and blind source separation. Optics express, 18(10):10762­10774, 2010.
Russell, Stuart. Learning agents for uncertain environments. In Proceedings of the eleventh annual conference on Computational learning theory, pp. 101­103. ACM, 1998.
Scheirer, Jocelyn, Fernandez, Raul, Klein, Jonathan, and Picard, Rosalind W. Frustrating the user on purpose: a step toward building an affective computer. Interacting with computers, 14(2):93­118, 2002.
Shah, Shital, Dey, Debadeepta, Lovett, Chris, and Kapoor, Ashish. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and Service Robotics, pp. 621­635. Springer, 2018.
Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1): 1929­1958, 2014.
Wilson, Gillian M and Sasse, M Angela. Do users always know what's good for them? utilising physiological responses to assess media quality. In People and computers XIV--Usability or else!, pp. 327­339. Springer, 2000.
Zheng, Zeyu, Oh, Junhyuk, and Singh, Satinder. On learning intrinsic rewards for policy gradient methods. arXiv preprint arXiv:1804.06459, 2018.
9

