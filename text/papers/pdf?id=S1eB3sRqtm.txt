Under review as a conference paper at ICLR 2019
EXPLORING CURVATURE NOISE IN LARGE-BATCH
STOCHASTIC OPTIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Using stochastic gradient descent (SGD) with large batch-sizes to train deep neural networks is an increasingly popular technique. By doing so, one can improve parallelization by scaling to multiple workers (GPUs) and hence leading to significant reductions in training time. Unfortunately, a major drawback is the socalled "generalization gap": large-batch training typically leads to a degradation in generalization performance of the model as compared to small-batch training. In this paper, we propose to correct this generalization gap by adding diagonal Fisher curvature noise to large-batch gradient updates. We provide a theoretical analysis of our method in the convex quadratic setting. Our empirical study with state-of-the-art deep learning models shows that our method not only improves the generalization performance in large-batch training, but furthermore, does so in a way where the training convergence remains desirable and the training duration is not elongated. We additionally connect our method to recent works on loss surface landscape in the experimental section.
1 INTRODUCTION
Modern datasets and network architectures in deep learning are becoming increasingly larger which results in longer training time. An ongoing challenge for both researchers and practitioners is how to scale up deep learning while keeping training time manageable. Using stochastic gradient descent (SGD) with large batch-sizes offers a potential avenue in addressing scalability issues. Increasing the batch-size used during training improves data parallelization (Goyal et al., 2017; You et al., 2018); it ensures multiple processors (GPUs) have sufficiently useful workload at each iteration hence reducing communication-to-cost ratio.
Unfortunately, a severe limitation to employing large-batch training in practice is the so-called "generalization gap". While large-batch yields considerable advantages over small-batch on training loss and error per parameter update, it has been verified empirically in LeCun et al. (2012); Keskar et al. (2016) that we have the opposite effect for test loss and error. To truly realize the benefits of using large-batches in the distributed synchronous setting, it is necessary to engineer large-batch training in a way such that this generalization gap can be closed without sacrificing too much the training performance. This is precisely the central objective of our paper.
In this paper, we propose to add diagonal Fisher curvature noise to large-batch gradient updates. We discuss the motivations behind our approach. Under the typical log-likelihood loss assumption, the difference of large-batch and small-batch gradients can be modeled as a Fisher noise. We can expect that adding this noise directly to large-batch gradients gives small-batch performance. While this remedies the generalization issue with large-batch training, the resulting convergence performance will be undesirable. To attain our end goal of designing a method which enjoys both desirable convergence and generalization performance, we reduce the amplitude of the noise by changing the covariance structure from full Fisher to diagonal Fisher. We find that in practice, this surprisingly improves convergence by a considerable amount while maintaining good generalization.
We give a theoretical analysis of our proposed method in Section 3. This is done over the convex quadratic setting which often serves as an excellent "proxy" for neural network optimization (Martens, 2010). We additionally provide numerical experiments on standard deep-learning tasks in Section 4 to demonstrate the efficacy of our proposed method. There are two primary takeaways from our empirical analysis. First, we show adding diagonal Fisher noise to large-batch
1

Under review as a conference paper at ICLR 2019

(a) SGD step
0.4

(b) Isotropic Gaussian

(c) Full Fisher

(d) Diagonal Fisher

0.3 0.2

W2

0.1 0.0

0.1 0.0 W10.2 0.4 0.0 W10.2 0.4 0.0 W10.2 0.4 0.0 W10.2 0.4

Figure 1: Noise structure in a simple two-dimensional regression problem. a) One-step SGD update. b) Onestep SGD update with isotropic Gaussian ( = 0.1) noise. c) One-step SGD update with full Fisher noise. d) One-step SGD update with diagonal Fisher noise. The full Fisher noise almost recovers the SGD noise. Observe that full Fisher noise direction is perpendicular to the loss surface which results in slower convergence than diagonal Fisher noise. Further details are discussed in Section 3.2.

gradients does not hinder much the training performance while giving a validation accuracy comparable to small-batch training, thereby correcting the "generalization gap". Second, this validation accuracy was attained in the same number of epochs used for small-batch training. This indicates that our method is data-efficient and does not require any lengthening of the optimization process.

2 PRELIMINARIES

2.1 BACKGROUND

Excess risk decomposition. Let S = {(x1, y1), . . . , (xN , yN )} be a training set of N input-target samples drawn i.i.d. from an unknown joint probability distribution D. The family of classifiers of interest to us are neural network outputs f (xi, ), where     Rd are parameters of the
network and  here denotes the parameter space of the network. Let (f (xi, ), yi) be a bounded
loss function measuring the disagreement between outputs f (xi, ) and targets yi. The expected risk
and empirical risk functions are defined to be

L ()



E(x,y)D [

(f (x, ), y)],

L()



1 N

N

(f (xi, ), yi).

i=1

The standard technique to analyze the interplay between optimization and generalization in statistical learning theory is through excess risk decomposition. After k iterations, the excess risk is defined as
L^(k)  L (k) - inf L ().

From Bottou & Bousquet (2008); Chen et al. (2018), the expected excess risk is upper-bounded by

ES [L^(k)]  ES [L (k) - L(k)] + ES [L(k) - L()],

(1)

Egen

Eopt

where  = min L() here is the empirical risk minimizer. The terms Egen and Eopt are the expected generalization and optimization errors respectively. In the machine learning literature, optimization algorithms are often studied from one perspective: either optimization or generalization. The excess risk decomposition suggests that both aspects should be analyzed together (Chen et al., 2018; Bottou & Bousquet, 2008) as the goal of a good optimization-generalization algorithm in machine learning is to minimize the excess risk in the least amount of iterations.

Related Work. In the context of large-scale learning, stochastic algorithms are very popular compared to full-batch methods due to lower computational overhead (Bottou et al., 2018; Bottou, 1991). Despite a decay in optimization performance (slower rate of convergence in the convex case (Moulines & Bach, 2011; Bottou et al., 2018)), stochastic algorithms possess good generalization properties due to the inherent noise of their gradients. There are vast bodies of research literature devoted to understanding the connection between inherent noise and generalization; for example, scaling the learning rate or batch-size (Smith & Le, 2017; Goyal et al., 2017; Hoffer et al., 2017) or studying the covariance structure of mini-batch gradients (Jastrzebski et al., 2017; Xing

2

Under review as a conference paper at ICLR 2019

et al., 2018; Zhu et al., 2018; Li et al., 2015). In the non-convex setting, the inherent noise allows SGD to efficiently escape from saddle points or shallow local minima which tends to give poor generalization (Zhu et al., 2018; Daneshmand et al., 2018; Jin et al., 2017). The trade-off between optimization and generalization was also observed for large-batch versus small-batch training in deep learning: both large-batch and small-batch training can reach similar training loss after sufficient number of iterations. Since large-batch gradients have smaller variance, large-batch typically requires less iterations to reach an optimum. However, large-batch training usually has worse test performance compared to small-batch (Hoffer et al., 2017; Masters & Luschi, 2018; Keskar et al., 2016). The choice of the batch-size has a direct impact on the trade-off between optimization and generalization.
In this paper, we aim to design a novel algorithm for large-batch training by increasing the variance of its gradients such that the resulting generalization performance matches small-batch gradient descent. Moreover, we require that this generalization performance be achieved within a number of iterations comparable to original large-batch.

2.2 MOTIVATION

We tackle the large-batch generalization problem through gradient noise injection. Let BL denote large-batch and ML = |BL| denote the large-batch size. Consider the following modification of
large-batch SGD updates

k+1 = k - kLML (k) + kD(k)k+1, k+1  N (0, Id).

(2)

where k

is the

learning

rate,

ML (k)

=

1 ML

ML i=1

i(k) is the large-batch gradient and N (0, Id)

is the multivariate Gaussian distribution with mean zero and identity convariance. We can interpret

this modification as injecting a Gaussian noise with mean zero and covariance D(k)D(k) to the

gradients. We now focus our attention on finding a suitable matrix D(k) such that the algorithm in

Eqn. 2 has desirable convergence and generalization performance.

Intrinsic SGD noise. Let B  S be a mini-batch drawn uniformly and without replacement from S and M = |B| be the size of this chosen mini-batch. We can write the SGD update rule here as
k+1 = k - kLM (k) = k - kL(k) + k(L(k) - LM (k))

k

where L(k)

=

1 N

N i=1



i (k )

is

the

full-batch

gradient.

The difference k

=

L(k) -

LM (k) is the intrinsic noise stemming from mini-batch gradients. The covariance of k is

Cov(k, k) =

N -M NM

·1 N

N
( i(k) - L(k))( i(k) - L(k))

,

i=1

(3)

This result can be found in Hu et al. (2017); Hoffer et al. (2017). Suppose that the loss function here

is the negative log-likelihood, i(k) = - log p(yi|xi, k), where p(y|x, ) is the density function

of the model's predictive distribution P . Furthermore, the term L(k)  0 in practice and hence

the

expression

in

Eqn.

3

is

exactly

N -M NM

F (k),

where

F (k)

is

the

empirical

Fisher

matrix.

Fisher information matrix. Given an input-target pair (x, y), the Fisher information matrix is defined as the expectation of the outer product of log-likelihood gradients,

E[ log p(y|x, ) log p(y|x, ) ].

(4)

The expectation here is taken with respect to the data distribution for inputs x and the model's predictive distribution for targets y. The Fisher matrix can be realized as the second-order Taylor expansion of the KL-divergence on the space of predictive distributions, hence defining a Riemannian metric on this space (Amari, 1998; Martens, 2014). As such, the Fisher matrix of neural networks captures the local curvature information of the parameter space. We give a detailed description of Fisher matrices for feed-forward and convolutional network architectures in Appendix D.

In practical situations, we often sample y from the empirical training distribution rather than the predictive distribution. Therefore, we have the empirical Fisher
1  log p(y|x, ) log p(y|x, ) . |S |
(x,y)S

3

Under review as a conference paper at ICLR 2019

For the rest of this paper, unless otherwise specified, all mentions of "Fisher matrix" or F () refers to the empirical Fisher.

Naive method to close "generalization gap". Having modeled the covariance of the mini-batch
SGD noise as a Fisher matrix, we now consider a naive approach to close "generalization gap" in large-batch training. Let BS denote small-batch and MS = |BS| denote the size of small-batch. Let us choose D(k) in Eqn. 2 to be

D(k) =

ML - MS MLMS

F (k),

(5)

where F (k) is the square-root of Fisher, F (k) F (k) = F (k). The motivation here is

that the difference between large-batch and small-batch gradients can be approximated as a Gaussian

noise

with

mean

zero

and

covariance

ML -MS ML MS

F (k)

(Jastrzebski

et

al.,

2017;

Xing

et

al.,

2018;

Zhu

et al., 2018; Li et al., 2015). However, there is an immediate issue with this naive approach: if the

Gaussian approximation is reasonable, then this algorithm should have similar behavior as small-

batch training which implies poor convergence performance. Indeed, as shown in the 2D convex

example in Fig. 1c, adding full Fisher noise recovers SGD behavior. Furthermore, on the CIFAR-

10 image classification task trained using a ResNet44 model, we see in Fig. 3c that the training

performance is almost identical to small-batch during the entire optimization process. Thus, the

choice of D(k) in Eqn. 5 does not satisfy our requirement of maintaining desirable convergence.

3 METHOD

3.1 PROPOSED ALGORITHM

Using diagonal Fisher. We consider replacing F (k) with diag (F (k)) in Eqn. 5 for D(k). Our proposed algorithm to correct the "generalization gap" in large-batch training is to inject diag-
onal Fisher noise to large-batch gradients,

k+1 = k - kLML (k) + k

ML - MS MLMS

diag (F (k))k+1, k+1  N (0, Id).

The formal statement is given in Algorithm 1. Most of the empirical analysis of Algorithm 1 in Section 4 later uses feed-forward and convolutional networks. For completeness, we provide explicit expressions of diagonal Fisher for these architectures in Appendix D.

Changing the covariance structure in this way has important implications with regards to convergence and generalization performance. In the next section, we analyze our algorithm in a simple quadratic setting and compare it to the case where the covariance is given by the Fisher matrix. Even in this simple case, it is not obvious to compare the generalization performance between the two algorithms. In Appendix C, we provide an extensive theoretical discussion about how the choice of covariance structure influences generalization performance of Eqn. 2.

Working with the assumption that the generalization error is comparable between diagonal Fisher and full Fisher, the excess risk in Eqn. 1 can be minimized by focusing only on the optimization error. We prove in Theorem 3.1 that the discrepancy in convergence behavior can be measured by the difference of their respective Frobenius norms. In Fig. 2, we illustrate this behavior on a 2D toy problem where we compare the training trajectory of adding full Fisher noise versus diagonal full Fisher noise to the true gradient. As the figure shows, the diagonal Fisher converges faster than full Fisher in the same number of iterations. In our experiments in Section 4, we observe that this phenomena translates over to the deep learning setting.

3.2 CASE-STUDY: CONVEX QUADRATIC EXAMPLE

In this subsection, we restrict our setting and take the loss function L() to be the convex quadratic,

L()



1 

A,

2

where A is a symmetric and positive-definite matrix. Observe that in this setting, the Fisher and Hessian coincide, which is simply the matrix A.

4

Under review as a conference paper at ICLR 2019

Algorithm 1 Adding diagonal Fisher noise to large-batch SGD. Differences from standard SGD are highlighted in blue

Require: Number of iterations K, initial step-size 0, large-batch BL of size ML, small-batch BS

of size MS, initial condition 0    Rd for k = 1 to K do

Full Fisher Diagonal Fisher

Zk  N (0, Id)

k = k

ML -MS ML MS

diag (F (k))Zk

4

k+1 = k - kLML (k) + k end for

2

We stress here that approximating the loss surface of a neural network with a quadratic often serves as a fertile "testing ground" when introducing new methodologies in deep learning. Analyzing the toy quadratic problem has led to important advances; for example, in learning rate scheduling (Schaul et al., 2013) and formulating SGD as approximate Bayesian inference (Mandt et al., 2017). With regards to optimization, Martens (2010) observed that much of the difficulty in neural network optimization can be captured using quadratic models.
We focus on the algorithm in Eqn. 2 and choose a constant d × d covariance matrix C. The following theorem, adapted from Bottou et al. (2018), analyzes the convergence of this optimization method. The proof is relegated to Appendix A.

0

2

4

20 2

20 2

Figure 2: Trajectory using full Fisher noise versus diagonal Fisher noise for the algorithm in Eqn. 2 used to minimize a two-dimensional quadratic function. Blue dot indicates the initial parameter value and the green dot shows the final parameter value. We used a learning rate of 0.1 for 500 iterations (plotting every 10 iterations). Observe that adding diagonal Fisher to the true gradient achieves faster convergence than full Fisher.

Theorem 3.1. Let max and min denote the maximum and minimum eigenvalue of A respectively. For a chosen 0  m-1ax, suppose that we run the algorithm in Eqn. 2 according to the decaying

step-size sequence

2

k

=

(k

, + )min

for all k  N>0 and where  is chosen such that k  0. Then for all k  N,

E[L(k)]



k

 +



where  = max

2

Tr(C AC m2 in

)

,

L(0)

.

The term of importance in this theorem is Tr(C AC). While the overall convergence rate of the algorithm is O(1/k), the discrepancy in convergence performance for different choices of the matrix C rests entirely on this term. We analyze two specific cases which are relevant for us: the first case where C is square-root of A, C = A, and the second case where C is the square-root of the
diagonal of A, C = diag (A). For the first, we have

Tr(C

AC) = Tr(A2) =

A

2 Frob

.

For the latter case, we have

Tr(C

AC) = Tr(diag (A)2) =

diag (A)

2 Frob

.

Thus, the difference in training performance between the two cases can be measured by the differ-

ence of their respective Frobenius norms.

3.3 SAMPLING RANDOM VECTOR WITH FISHER COVARIANCE
The cost of computing diagonal Fisher is roughly one forward pass while computing full Fisher is intractable in general. To highlight the difference between using full Fisher noise as opposed to diagonal Fisher noise empirically, we need to provide a practically efficient method for computing a random vector with full Fisher covariance. Martens et al. (2012) used back-propagation to obtain rank-1 unbiased estimates of the Hessian. We follow their ideas and demonstrate how to compute a random vector with true Fisher covariance.

5

Under review as a conference paper at ICLR 2019

Proposition 3.2 (Sampling True Fisher Random Vector). Let y be a random label sampled from

the model's predictive distribution.

Let v

=

L 

be the gradient vector of the loss with respect

to parameters and Jf be the Jacobian of outputs with respect to parameters. Then, Cov(v, v) =

Ex[Jf HLJf ]. If the loss L is the negative log-likelihood for an exponential family and outputs are the "natural parameters" (e.g. cross entropy, squared-loss), then this covariance matrix is just the

Fisher (Pascanu & Bengio, 2013; Martens, 2014).

The proof of this is given in Appendix B. To obtain a random vector with empirical Fisher covariance, we sample y from the empirical training distribution rather than the model's predictive distribution. This is important since we work primarily with empirical Fisher in this paper. To obtain a random vector with diagonal Fisher covariance, we can just randomly flip gradients elementwise to de-correlate between different examples.
4 EXPERIMENTS
We first empirically compute the Frobenius norm of full Fisher and diagonal Fisher, followed by the marginal variance of the gradients for a variety of training regimes. We also compare the convergence speed of diagonal Fisher noise and full Fisher noise; the convergence speed here is measured with respect to the number of parameter updates. In Section 4.2, we compute the maximum eigenvalue of the Hessian with respect to the final model parameters of different training regimes. The purpose of this is to provide an empirical understanding of the curvature of the loss surface landscape. In Section 4.3, we give the generalization performance of each method discussed previously.
Throughout our experiments, large-batch size LB is set to 4096 and small-batch size SB is set to 128 by default. The network architectures we use are simple fully-connected networks, shallow convolutional networks (LeNet (LeCun et al., 1998), AlexNet (Krizhevsky et al., 2012)), and deep convolutional networks (VGG16 (Simonyan & Zisserman, 2014), ResNet44 (He et al., 2016)). These models are evaluated on the standard deep-learning datasets: MNIST, Fashion-MNIST (LeCun et al., 1998; Xiao et al., 2017), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009).
4.1 CONVERGENCE EXPERIMENTS
In the convex quadratic setting in Section 3.2, we observed that full-batch gradient descent with diagonal Fisher noise enjoyed faster convergence than full Fisher noise. This was characterized by the difference of their Frobenius norms. We now give an empirical verification of this phenomena in the non-convex setting of deep neural networks. We compute the Frobenius norms during the training of the ResNet44 network on CIFAR-10. Fig. 3a shows that the full Fisher matrix has much larger Frobenius norm than the diagonal Fisher matrix, which suggests that using diagonal Fisher noise should have faster convergence than full Fisher noise in the deep neural network setting. To justify this intuition, we analyze the training loss (per parameter update) of ResNet44 (CIFAR-10) on the following four regimes: LB, SB, LB with diagonal Fisher noise and LB with full Fisher noise. For a fair comparison, we use the same learning rate schedule for all four regimes. Fig. 3c shows that LB with diagonal Fisher noise trains much faster than LB with full Fisher. More interestingly, LB with full Fisher matches the convergence performance of SB, indicating that the intrinsic noise of SB is accurately modeled by the Fisher. In contrast, LB with diagonal Fisher attains a convergence similar to LB, showing that our approach preserves the desired convergence behavior of LB.
Next, we give an estimation of the marginal variance of gradients for the four regimes mentioned above as well as LB with K-FAC noise. K-FAC (Martens & Grosse, 2015) is a block-diagonal approximation of the Fisher matrix used as a second-order optimization in deep learning. The purpose of this experiment is to show despite the fact that LB with diagonal Fisher trains much faster than LB with full Fisher, they share roughly the same marginal variance of the gradients. It suggests that the off-diagonal elements of the Fisher matrix is the key reason for slow convergence. The experiment is performed as follows: we freeze a partially-trained network and compute Monte-Carlo estimates of gradient variance with respect to each parameter over different mini-batches. This variance is then averaged over the parameters within each layer. The results are presented in Fig. 3b. We find that LB with diagonal Fisher noise and LB with full Fisher noise give roughly the same scale of marginal gradients variance.
6

Under review as a conference paper at ICLR 2019

frobenius norm magnitude

105

Full Fisher

10 2

104 Diag Fisher

variance of gradients

103 10 3 102

101 10 4 100

LB SB fullF diagF KfacF

train error

0 25 50 75 100 125 150 175 200

Layer3

Layer16

Layer31

Layer39

(a) Frobenius Norm vs. Num of epochs

(b) Gradients Variance (log scale) vs. Conv Layer

80

4K Baseline 128 Baseline

70

4Kgbn 4KdiagF

4K full Fisher

60 SB

4K diag Fisher
60 50

max eigenvalue

40 40
30

20
0 0 1000 2000 3000 4000 5000 6000 7000 8000

20 10 0
Res44cifar10

Res44cifar100

vgg16cifar10

vgg16cifar100

(c) Train Error vs. Iterations.

(d) Maximum Eigenvalue of the Hessian.

Figure 3: a) Frobenius norm of full Fisher matrix and diagonal Fisher matrix. The model is trained on ResNet44 with CIFAR-10. b) Estimation of gradient variances for randomly selected convolutional layers on ResNet44 (CIFAR-10). fullF: LB with full Fisher noise. diagF: LB with diagonal Fisher noise. KfacF: LB with K-FAC noise. c) Training error with respect to iterations between SB, LB, LB with full Fisher noise and diagonal Fisher noise on ResNet44 (CIFAR-10). All of the above are trained with the same learning rate. d) The maximum eigenvalue of the Hessian matrix at the end of training. 4Kgbn: LB with Ghost-BN. 4KdiagF: LB with diagonal Fisher noise.

4.2 MAXIMUM EIGENVALUE COMPARISON
While the relationship between loss surface curvature and generalization is not explicit, numerous works have suggested that the maximum eigenvalue of Hessian is possibly correlated with generalization performance (Keskar et al., 2016; Chaudhari et al., 2016; Chaudhari & Soatto, 2017; Yoshida & Miyato, 2017; Xing et al., 2018). To situate our method with this line of research, we compute the maximum eigenvalue of the Hessian of the final model for the following three regimes: LB with diagonal Fisher noise, LB with ghost batch-normalization, and SB. Ghost batch-normalization (GBN) is an adaptation of usual batch-normalization introduced in Hoffer et al. (2017) to close generalization gap. In Fig. 3b, we find that LB with diagonal Fisher gives smaller maximum eigenvalue than LB with GBN.
Computing maximum eigenvalue without any modification to the model gives inconsistent estimates even between different runs of the same training configuration. To make the maximum eigenvalue of the Hessian matrix comparable over different training trajectories, the Hessian need to be invariant under some typical weights re-parameterization such as affine transformation (Liao et al., 2018). To achieve this, we make the following modification to the trained model: (1) For the layer with batch normalization, we can just push the batch-norm layer parameters and running statistics into the layerwise parameter space so that the the layer is invariant under affine transformation; and (2) For the layer without batch normalization, re-parameterization changes the prediction confidence while the prediction remains the same. So we train a temperature parameter on the cross-entropy loss on the test data set, this encourages the model to make a calibrated prediction (prevent it from being over-confident). In Fig. 3d, we give the error bar of the maximum eigenvalue of the Hessian over different runs, which indicates the modification gives a roughly consistent estimate.
7

Under review as a conference paper at ICLR 2019

4.3 RESULTS
In this subsection, we provide the validation accuracy for a number of training methods. We point out that we do not experiment LB with full Fisher due to its exceedingly long training time. This can be seen from Fig. 3c where on ResNet44 (CIFAR-10), full Fisher does not achieve good convergence even after 8000 iterations. We use the baseline in Hoffer et al. (2017), where LB is trained using GBN with learning rate adaptation. However, instead of using square-root scaling of the learning rate in Hoffer et al. (2017), we use linear scaling in conjunction with a warmup scheme suggested by Goyal et al. (2017). We found that this improves the baseline reported in Hoffer et al. (2017).
Inspired by Smith et al. (2017), we conducted an experiment on ResNet44 (CIFAR-10/100) where we use SB for 50 epochs and then use LB for the remaining 150 epochs. We found that this was able to close the generalization gap which suggests that the noise matters only in the early stages of the optimization process. Therefore, for all experiments involving large-batch in this subsection, we terminate the noise and use standard LB after 50 epochs. For our experiments, we compared the following regimes: LB with GBN (our baseline), LB with GBN scaled by isotropic Gaussian noise scaled by trace of Fisher matrix, LB with GBN and diagonal Fisher noise, and SB. All methods were trained for the same number of epochs. The final validation accuracies of each method are reported in Table 1. While it is true that adding diagonal Fisher noise cannot completely close the "generalization gap" in some cases, we can see from Table 1 that doing so yields definite improvements. We point out that we explored other regimes such as injecting multiplicative Guassian noise with constant diagonal covariance (Hoffer et al., 2017), but found that they all perform no better than our baseline (LB with GBN). In addition, we experimented with K-FAC noise but this did not give additional benefits over diagonal Fisher noise and so we defer K-FAC results to the Appendix.
Table 1: Validation accuracy results on classification tasks. GBN stands for Ghost-BN. Isotropic+GBN stands for LB + Ghost-BN with isotropic Gaussian noise scaled by trace of the Fisher. Diag-F+GBN stands for LB + Ghost-BN with diagonal Fisher noise. All methods in each row are trained with the same number of epochs. Confidence interval is computed over 3 random seeds. LB with full Fisher requires same number of updates as SB, which renders it impractical for all the models we consider. While infeasible to compute for all model, we observe that LB with full Fisher reaches roughly the same validation accuracy (93.22) as SB in the case of ResNet44 (CIFAR-10).

Dataset

Network SB LB+GBN Isotropic+GBN Diag-F+GBN

MNIST MNIST FASHION-MNIST CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100

MLP LeNet LeNet Alexnet Alexnet VGG16 VGG16 ResNet44 ResNet44x2

98.10 99.10 91.10 87.80 59.21 93.25 72.83 93.42 75.55

97.94
98.85
88.89 86.41 ± 0.18 56.75 ± 0.18 91.78 ± 0.29 69.44 ± 0.27 91.92 ± 0.29 73.11 ± 0.22

98.08 99.02 90.29 N/A N/A 92.81 ± 0.10 71.26 ± 0.09 92.31 ± 0.02 73.62 ± 0.15

98.08
99.10
90.77 87.30 ± 0.28 58.68 ± 0.40 93.15 ± 0.05 71.94 ± 0.14 92.72 ± 0.15 74.10 ± 0.18

5 CONCLUSION
In this paper, we explored in depth the relationship between curvature noise and stochastic optimization. We proposed a method to engineer large-batch training such that we retain fast convergence performance while achieving significant gains in generalization. In addition, we highlighted the importance of noise covariance structure on optimization and generalization. An interesting future direction would be to further understand both empirically and theoretically how different noise covariance structures impact optimization and generalization performance of a learning algorithm.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251­ 276, 1998.
Le´on Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural information processing systems, pp. 161­168, 2008.
Le´on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223­311, 2018.
Lon Bottou. Stochastic gradient learning in neural networks. In In Proceedings of Neuro-Nmes. EC2, 1991.
Olivier Bousquet and Andre´ Elisseeff. Stability and generalization. Journal of machine learning research, 2(Mar):499­526, 2002.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. arXiv preprint arXiv:1710.11029, 2017.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.
Y. Chen, C. Jin, and B. Yu. Stability and Convergence Trade-off of Iterative Optimization Algorithms. ArXiv e-prints, April 2018.
H. Daneshmand, J. Kohler, A. Lucchi, and T. Hofmann. Escaping Saddles with Stochastic Gradients. ArXiv e-prints, March 2018.
Crispin Gardiner. Stochastic methods, volume 4. springer Berlin, 2009.
Priya Goyal, Piotr Dolla´r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution layers. In International Conference on Machine Learning, pp. 573­582, 2016.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. CoRR, abs/1509.01240, 2015. URL http://arxiv.org/abs/ 1509.01240.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, pp. 1731­1741, 2017.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of nonconvex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017.
C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to Escape Saddle Points Efficiently. ArXiv e-prints, March 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
9

Under review as a conference paper at ICLR 2019
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Yann A LeCun, Le´on Bottou, Genevieve B Orr, and Klaus-Robert Mu¨ller. Efficient backprop. In Neural networks: Tricks of the trade, pp. 9­48. Springer, 2012.
Qianxiao Li, Cheng Tai, et al. Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv preprint arXiv:1511.06251, 2015.
Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso Poggio. A surprising linear relationship predicts test performance in deep networks. arXiv preprint arXiv:1807.09659, 2018.
Kevin Luk and Roger Grosse. A coordinate-free construction of scalable natural gradient, 2018.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research, 18(1):4873­4907, 2017.
James Martens. Deep learning via hessian-free optimization. 2010.
James Martens. New insights and perspectives on the natural gradient method. arXiv preprint arXiv:1412.1193, 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408­2417, 2015.
James Martens, Ilya Sutskever, and Kevin Swersky. Estimating the hessian by back-propagating curvature. 2012.
Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks. CoRR, abs/1804.07612, 2018. URL http://arxiv.org/abs/1804.07612.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for nonconvex learning: Two theoretical viewpoints. arXiv preprint arXiv:1707.05947, 2017.
Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pp. 451­459, 2011.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint arXiv:1301.3584, 2013.
Grigorios A Pavliotis. Stochastic processes and applications: diffusion processes, the Fokker-Planck and Langevin equations, volume 60. Springer, 2014.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. CoRR, abs/1702.03849, 2017. URL http://arxiv.org/abs/1702.03849.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International Conference on Machine Learning, pp. 343­351, 2013.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
S. L. Smith and Q. V. Le. A Bayesian Perspective on Generalization and Stochastic Gradient Descent. ArXiv e-prints, October 2017.
10

Under review as a conference paper at ICLR 2019

Samuel L Smith, Pieter-Jan Kindermans, and Quoc V Le. Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489, 2017.
George E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical review, 36(5):823, 1930.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv preprint arXiv:1802.08770, 2018.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.
Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Imagenet training in minutes. In Proceedings of the 47th International Conference on Parallel Processing, pp. 1. ACM, 2018.
Z. Zhu, J. Wu, B. Yu, L. Wu, and J. Ma. The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Minima and Regularization Effects. ArXiv e-prints, 2018.

A PROOF OF THEOREM 3.1

The proof of this theorem follows the spirit of Bottou et al. (2018). The algorithm

k+1 = k - kAk + kCk+1, k+1  N (0, Id).

falls into the Robbins-Monro setting where the true gradient is perturbed by random noise. This perturbation can be considered as a martingale difference in the sense that

E[Ck+1|Fk] = 0
where (Fk)kN is a increasing filtration generated by the sequence of parameters (k)kN. When the step size is constant k =  for all k, it corresponds to the Euler discretization of a gradient flow with random perturbation. We begin the proof by considering the equality,

L(k+1) = L(k) +

L(k), k+1 - k

+

1 2 (k+1

-

k )

2L(k)(k+1 - k).

Using the fact that L(k) = Ak, 2L(k) = A, and from the definition of k+1, we can rewrite the above equation as

L(k+1) = L(k) +

Ak, -kAk + kCk+1

1 +
2

kAk - kCk+1

2 A

.

Now, taking the conditional expectation E[·|Fk] on both sides of the equality, we obtain by independence of the noise k+1 to Fk

E[L(k+1)|Fk] = L(k) - k

Ak

2 2

+

k2 2

Ak

2 A

+

k2 2

E[

C k+1

2A]

(6)

A simple computation shows

E[ Ck+1 A2 ] = E[(Ck+1) A(Ck+1)] = E[k+1C ACk+1] = Tr(C AC)

(7)

Moreover, we have

Ak

2 A

=

(k

A)A(Ak)

=A

Ak

2 2

 max

Ak

2 2

.

(8)

11

Under review as a conference paper at ICLR 2019

Using the results in Eqns. 7 and 8 as well as the assumption on the step-size schedule for all k:

k

<

0

<

,1
max

we

rewrite

Eqn.

6

as

E[L(k+1)|Fk]  L(k) +

k 2

max

-

1

k

Ak

2 2

+

k2 2

Tr(C



L(k )

-

k 2

Ak

2 2

+

k2 2

Tr(C

AC ).

AC )

(9)

Furthermore,

Ak

2 2

=

A(k

Ak )



min

k

2 A

=

2min L(k )

Using this above fact and then taking the expectation of Eqn. 9 leads to

E[L(k+1)]



(1

-

k min )E[L(k )]

+

k2 2

Tr(C

AC ).

We proceed by induction to prove the final result. By definition of , the result is obvious for k = 0. For the inductive step, suppose that the induction hypothesis holds for k, i.e.,

We prove the k + 1 case.

2

k

=

(k

+

, )min



E[L(k)]



k

. +

2

E[L(k+1)] 

1- k+

 (k +  + 1)

2 k +  + (k + )22min Tr(C AC)

This comes from the definition of  and also the inequality (k +  - 1)(k +  + 1)  (k + )2. This conclude the proof.

B PROOF OF PROPOSITION 3.2

For convenience, we prove it in the case of cross-entropy. In this case, the gradient vector v = Jf (p - ey) where p is the model's predictive probability and ey is the random label sampled from model's predictive distribution written in one-hot notation. We compute the covariance of v:
Ex,y[vv ] = Ex,y Jf (p - ey)(p - ey) Jf = Ex Jf Ey[(p - ey)(p - ey) ]Jf = Ex Jf (diag (p) - pp )Jf
= Ex Jf HLJf

C RELATIONSHIP BETWEEN CHOICE OF COVARIANCE STRUCTURE AND
GENERALIZATION

As in Section 3.2, we work entirely in the convex quadratic setting. Consider the algorithm

k+1 = k - kL(k) + kCk+1, k+1  N (0, Id).

(10)

Our aim in this section is to provide some theoretical discussions on how the choice of covariance structure C influences the generalization behavior of the above algorithm.

Uniform stability. Uniform stability (Bousquet & Elisseeff, 2002) is one of the most common techniques used in statistical learning theory to study generalization of a learning algorithm. Intuitively speaking, uniform stability measures how sensitive an algorithm is to perturbations of the sampling data. The more stable an algorithm is, the better its generalization will be. Recently, the uniform stability of several algorithms has been investigated for stochastic gradient methods (Hardt et al., 2015) or stochastic gradient Langevin dynamics algorithm (Mou et al., 2017; Raginsky et al., 2017). We present the precise definition.

12

Under review as a conference paper at ICLR 2019

Definition C.1 (Uniform stability). A randomized algorithm A is -stable if for all data sets S and S where S and S differ in at most one sample, we have
sup |EA[L(S ) - L(S )]|  ,
(x,y)
where L(S ) and L(S ) highlight the dependence of parameters on sampling datasets. The supremum is taken over input-target pairs (x, y) belonging to the sample domain.

The following theorem from Bousquet & Elisseeff (2002) shows that uniform stability implies generalization.
Theorem C.2 (Generalization in expectation). Let A be a randomized algorithm which is uniformly stable, then
|EA[Egen]|  ,
where Egen is the expected generalization error as defined in Eqn. 1.

Continuous-time dynamics. We like to use the uniform stability framework to analyze generalization properties of Eqn. 10. To do this, we borrow ideas from the recent work of Mou et al. (2017) which give uniform stability bounds for Stochastic Gradient Langevin Dynamics (SGLD) in non-convex learning. While the authors in that work give uniform stability bounds in both the discrete-time and continuous-time setting, we work with the continuous setting since this conveys relevant ideas while minimizing technical complications. The key takeaway from Mou et al. (2017) is that uniform stability of SGLD may be bounded in the following way

SGLD  sup H2(t, t).
S ,S

(11)

Here, t and t are the distributions on parameters  trained on the datasets S and S . The H refers to the Hellinger distance.

We now proceed to mirror the approach of Mou et al. (2017) for Eqn. 10. Our usage of stochastic

differential equations will be very soft but we refer to reader to Gardiner (2009); Pavliotis (2014)

for necessary backgrounds. For the two datasets S and S , the continuous-time analogue of Eqn. 10

are Ornstein-Uhlenbeck processes (Uhlenbeck & Ornstein, 1930):



dS

(t)

=

-AS

S

(t)

+ 

CS dW (t)

dS (t) = -AS S (t) + CS dW (t).

The solution is given by

S (t)

=

e-AS tS (0)

+

 

t
e-AS (t-u)CS dW (u),

0

In fact, this yields the Gaussian distribution

S (t)  N (µS (t), S (t)),

where

µS (t) = e-AStS (0)

and S (t) satisfies the Ricatti equation,

d dt

S

(t)

=

-(AS

S

(t)

+

S

(t)AS

)

+

CS

CS

.

Observe that AS is symmetric and positive-definite which means that it admits a diagonalization AS = PS DS PS-1. Solving the equation for the covariance matrix gives

S (t) = PS

t
e-DS (t-u)PS-1CS CS PS e-DS (t-u)du PS-1.
0

(12)

We are in the position to directly apply the framework of (Mou et al., 2017). Choosing t and t in Eqn. 11 to be the Gaussians N (µS (t), S (t)) and N (µS (t), S (t)) respectively, we obtain a

13

Under review as a conference paper at ICLR 2019

uniform stability bound for Eqn. 10. We compute the right-hand side of the bound to get derive insights on generalization. Using the standard formula for Hellinger distance between two Gaussians, we have

H2(t, t) = 1 -

det(S

)

1 4

det(S

det(

S

+S 2

)1 2

)1 4

exp

-

1 8

(µS

-

µS

)

S + S 2

-1
(µS - µS )
(13)

Choosing the noise covariance. From Eqn. 13 above, it is evident that to ensure good generalization error for Eqn. 10, we want to choose a covariance CS such that the Hellinger distance H is minimized. Since we are working within the uniform stability framework, a good choice of CS should be one where Eqn. 10 becomes less data-dependent. This is intuitive after all ­ the less
data-dependent an algorithm is; the better suited it should be for generalization.

We study Eqn. 13. Note that as time t  , the exponential term goes to 1. Hence, we focus our attention on the ratio of the determinants. Suppose that we choose CS = AS . Since AS is the Fisher for this convex quadratic example, Eqn. 10 is essentially the naive method given in Section 2.2. Simplifying the determinant of S (t) in this case,

det(S (t)) =

 2

d
det(Id - e-2DSt)

Suppose that we choose C = Id. Proceeding analogously,

det(S (t)) =

 d det(Id - e-2DSt) 2 det(DS )



We can think of choosing C = Id or C = A to be extreme cases and it is interesting to observe

that the Hellinger distance is more sensitive to dataset perturbation when C = Id. Our proposed

method of this paper was to choose C = diag (A) and experiments in Section 4 seem to suggest

that choosing the square-root of diagonal captures much of the generalization behavior of full Fisher.

Understanding precisely why this is the case poses an interesting research direction to pursue in the

future.

A simple scaling argument also highlights the importance of the trade-off between optimization
and generalization. Consider C = C. Then Theorem 3.1 suggests to take  small to reduce the variance and improve convergence. However, in that case  = 2 where  is given by the Eqn. 12 for C and

H2(t, t)

=1-

det(S

)

1 4

det(S

det(

S

+S 2

)1 2

)1 4

exp

-

1 82

(µS

-

µS

)

S + S 2

-1
(µS - µS )

and the Hellinger distance get close to one in the limit of small  (which intuitively corresponds to the large batch situation).

D FISHER INFORMATION MATRIX FOR DEEP NEURAL NETWORKS
In this section, we give a formal description of the true Fisher information matrix, rather than the empirical version, for both feed-forward networks and convolutional networks. In addition, we give the diagonal expression for both networks.
D.1 FEED-FORWARD NETWORKS
Consider a feed-forward network with L layers. At each layer i  {1, . . . , L}, the network computation is given by
zi = Wiai-1 ai = i(zi), where ai-1 is an activation vector, zi is a pre-activation vector, Wi is the weight matrix, and i : R  R is a nonlinear activation function applied coordinate-wise. Let w be the parameter vector of network obtained by vectorizing and then concatenating all the weight matrices Wi,
w = [vec(W1) vec(W2) . . . vec(WL) ] .

14

Under review as a conference paper at ICLR 2019

Furthermore, let Dv = v log p(y|x, w) denote the log-likelihood gradient. Using backpropagation, we have a decomposition of the log-likelihood gradient DWi into the outer product:
DWi = giai-1,
where gi = Dzi are pre-activation derivatives. The Fisher matrix F (w) of this feed-forward network is a L × L matrix where each (i, j) block is given by

Fi,j(w) = E[vec(DWi) vec(DWj) ] = E[ai-1aj-1  gigj ].

(14)

Diagonal version. We give an expression for the diagonal of Fi,i(w) here. The diagonal of F (w) follows immediately afterwards. Let ai2-1 and gi2 be the element-wise product of ai-1 and gi respectively. Then, in vectorized form,
diag (Fi,i(w)) = E[vec((ai2-1)(gi2) )],
where (ai2-1)(gi2) is the outer product of a2i-1 and gi2.

D.2 CONVOLUTIONAL NETWORKS

In order to write down the Fisher matrix for convolutional networks, it suffices to only consider convolution layers as the pooling and response normalization layers typically do not contain (many) trainable weights. We focus our analysis on a single layer. Much of the presentation here follows (Grosse & Martens, 2016; Luk & Grosse, 2018).

A convolution layer l takes as input a layer of activations aj,t where j  {1, . . . , J} indexes the input map and t  T indexes the spatial location. T here denotes the set of spatial locations, which
we typically take to be a 2D-grid. We assume that the convolution here is performed with a stide
of 1 and padding equal to the kernel radius R, so that the set of spatial locations is shared between
the input and output feature maps. This layer is parameterized by a set of weights wi,j,, where i  {1, . . . , I} indexes the output map and    indexes the spatial offset. The numbers of spatial locations and spatial offsets are denoted by |T | and || respectively. The computation of the
convolution layer is given by

zi,t =

wi,j, aj,t+ .

(15)



The pre-activations zi,t are then passed through a nonlinear activation function l. The loglikelihood derivatives of the weights are computed through backpropagation:

Then, the Fisher matrix here is

Dwi,j, = aj,t+Dzi,t.
tT

E[Dwi,j,Dwi ,j , ] = E

aj,t+ Dzi,t
tT

aj ,t + Dzi ,t
t T

.

Diagonal version. To give the diagonal version, it will be convenient for us to express the com-

putation of the convolution layer in matrix notation. First, we represent the activations aj,t as a J × |T | matrix Al-1, the pre-activations zi,t as a I × |T | matrix Zl, and the weights wi,j, as a I × J|| matrix Wl. Furthermore, by extracting the patches surrounding each spatial location t  T and flattening these patches into column vectors, we can form a J|| × |T | matrix Ale-xp1 which we call the expanded activations. Then, the computation is Eqn. 15 can be reformulated as the matrix

multiplication

Zl = WlAel-xp1.

Readers familiar with convolutional networks can immediately see that this is the Conv2D operation.

At a specific spatial location t  T , consider the J||-dimensional column vectors of Ale-xp1 and I-dimensional column vectors of Zl. Denote these by a(l-:,t1) and zl(t) respectively. The matrix Wl maps al(-:,t1) to zl(t). In this case, we find ourselves in the exact same setting as the feed-forward case given earlier. The diagonal is simply

E vec (al(-:,t1))2(Dzl(t))2

15

Under review as a conference paper at ICLR 2019

E KRONECKER-FACTORED APPROXIMATE CURVATURE (K-FAC)

In Section 4, we compared the diagonal approximation of the Fisher matrix to the Kroneckerfactored approximate curvature (K-FAC) (Martens & Grosse, 2015) approximation of the Fisher matrix. We give a brief overview of the K-FAC approximation in the case of feed-forward networks.

Recall that the Fisher matrix for a feed-forward network is a L × L matrix where each of the (i, j) blocks are given by Eqn. 14. Consider the diagonal (i, i) blocks. If we approximate the activations ai-1 and pre-activation derivatives gi as statistically independent, we have

Fi,i(w) = E[vec(DWi) vec(DWi) ] = E[ai-1ai-1  gigi ]  E[ai-1ai-1]  E[gigi ].

Let Ai-1 = E[ai-1ai-1] and Gi = E[gigi ]. The K-FAC approximation F^ of the Fisher matrix F

is

 A0  G1

0

F^

=

 



A1  G2 ...

 

.



0 AL-1  GL

The K-FAC approximation of the Fisher matrix can be summarized in the following way: (1) keep only the diagonal blocks corresponding to individual layers, and (2) make the probabilistic modeling assumption where the activations and pre-activation derivatives are statistically independent.

F SUPPLEMENTARY EXPERIMENTAL RESULTS
In this section, we give additional validation accuracy results to complement Table 1. The additional regimes we experiment with are: BatchChange, Multiplicative, and K-FAC. For BatchChange, we use SB for 50 epochs and LB for 150 epochs. This is inspired from Smith et al. (2017). Multiplicative stands for multiplying gradients with Gaussian noise with constant diagonal convariance structure. This idea stems from (Hoffer et al., 2017). K-FAC means we use the K-FAC approximation instead of diagonal Fisher noise in Algorithm 1. We report the results in Table 2.
Table 2: Validation accuracy results on classification tasks using BatchChange, Multiplicative, K-FAC. For reader's convenience, we report again the result of Diag-F+GBN

Dataset
CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100

Network
VGG16 VGG16 ResNet44 ResNet44x2

BatchChange
93.18 72.44 93.02 75.16

Multiplicative
90.98 68.77 91.28 71.98

K-FAC
93.06 71.86 92.81 73.84

Diag-F+GBN
93.15 ± 0.05 71.94 ± 0.14 92.72 ± 0.15 74.10 ± 0.18

16

