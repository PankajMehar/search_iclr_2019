Under review as a conference paper at ICLR 2019

A COMPREHENSIVE, APPLICATION-ORIENTED STUDY OF CATASTROPHIC FORGETTING IN DNNS
Anonymous authors Paper under double-blind review
ABSTRACT
We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.

1 INTRODUCTION
This article is in the context of sequential or incremental learning in Deep Neural Networks (DNNs). Essentially, this means that a DNN is not trained once, on a single task D, but successively on two or more sub-tasks D1, . . . , Dn, one after another. Learning tasks of this type, which we term Sequential Learning Tasks (SLTs) (see Fig. 1a), are potentially very common in real-world applications. They occur wherever DNNs need to update their capabilities on-site and over time: gesture recognition, network traffic analysis, or face and object recognition in mobile robots. In such scenarios, neural networks have long been known to suffer from a problem termed "catastrophic forgetting"(CF) (e.g., French (1999)) which denotes the abrupt and near-complete loss of knowledge from previous subtasks D1, . . . , Dk-1 after only a few training iterations on the current sub-task Dk (see Fig. 1b compared to Fig. 1c). We focus on SLTs from the visual domain with two sub-tasks each, as DNNs show pronounced CF behavior even when only two sub-tasks are involved.

sub-task D1 sub-task D2

train D1

train D2

test D1

test D1

test D2

test D1D2

0 E 2E

1.0 training D1 retraining D2

0.5 0.0 0

DDD211 D2 epoch

2

test accuracy test accuracy

1.0 training D1 retraining D2

0.5 0.0 0

DDD112 D2 epoch

2

(a) Training scheme

(b) with CF

(c) without CF

Figure 1: Scheme of incremental training experiments conducted in this article (a) and representative outcomes with (b) and without CF (c). The sequential learning tasks used in this study only have two sub-tasks: D1 and D2. During training (white background) and re-training (gray background), test accuracy is measured on D1 (blue, ), D2 (green, ) and D1  D2 (red, ). The blue curve allows to determine the presence of CF by simple visual inspection: if there is significant degradation w.r.t. the red curve, then CF has occurred.

1

Under review as a conference paper at ICLR 2019
1.1 RELATED WORK ON AVOIDING CF
The field of incremental learning is large, e.g., Parisi et al. (2018) and Gepperth & Hammer (2016). Principal recent approaches include ensemble methods (Ren et al., 2017; Fernando et al., 2017), dual-memory systems (Shin et al., 2017; Kemker & Kanan, 2017; Rebuffi et al., 2017; Gepperth & Karaoguz, 2015) and single-DNN regularization approaches which are in focus here. Whereas Goodfellow et al. (2013) suggest Dropout for alleviating CF, the EWC method (Kirkpatrick et al., 2016) proposes to add a term to the energy function that protects weights that are important for the previous sub-task(s). Importance is determined by approximating and analyzing the Fisher information matrix of the DNN. A somewhat related approach is pursued with the Incremental Moment Matching technique (IMM) (see Lee et al. (2017)), where weights from DNNs trained on a current and a past sub-tasks are "merged" using the Fisher information matrix. Other regularization-oriented approaches are proposed in Aljundi et al. (2018); Srivastava et al. (2013) and Kim et al. (2018) which focus on enforcing sparsity of neural activities by lateral interactions within a layer. In the following, we will review recent regularization approaches w.r.t. their feasibility in application scenarios.
Number of tested datasets In general, most methods referenced here are evaluated only on a few datasets, usually on MNIST (LeCun et al., 1998) and various derivations thereof (permutation, rotation, class separation). Some studies make limited use of CIFAR10, SVHN, the Amazon sentiment analysis problem, and non-visual problems such as data from Q-learning of Atari games. A largescale evaluation on a huge number of qualitatively different datasets is still missing1. Model selection and prescience Model selection (i.e., selecting DNN topology and hyperparameters) is addressed in some approaches (Goodfellow et al., 2013) but on the basis of a "prescient" evaluation where the best model is selected after all tasks have been processed, an approach which is replicated in Kirkpatrick et al. (2016). This amounts to a knowledge of future sub-tasks which is problematic in applications. Most approaches ignore model selection (Lee et al., 2017; Srivastava et al., 2013; Aljundi et al., 2018; Kim et al., 2018), and thus implicitly violate causality. Storage of data from previous sub-tasks From a technical point of view, DNNs can be retrained without storing training data from previous sub-tasks, which is done in Goodfellow et al. (2013) and Srivastava et al. (2013). For regularization approaches, however, there are regularization parameters that control the retention of previous knowledge, and thus must be chosen with care. In Kirkpatrick et al. (2016), this is , whereas two such quantities occur in Lee et al. (2017): the "balancing" parameter  and the regularization parameter  for L2-transfer. The only study where regularization parameters are obtained through cross-validation (which is avoided in other studies) is Aljundi et al. (2018) (for SNI and ) but this requires to store all previous training data.
This review shows that enormous progress has been made, but that there are shortcomings tied to applied scenarios which need to be addressed. We will formalize this in Sec. 1.2 and propose an evaluation strategy that takes these formal constraints into account when testing CF in DNNs.
1.2 INCREMENTAL LEARNING IN APPLIED SCENARIOS
When training a DNN model on SLTs, first of all the model must be able to be retrained at any time by new classes (class-incremental learning). Secondly, it must exhibit retention, or at least graceful decay, of performance on previously trained classes. Some forgetting is probably unavoidable, but it should be gradual and not immediate, i.e., catastrophic. However, if a DNN is operating in, e.g., embedded devices or autonomous robots, additional conditions may be applicable: Low memory footprint Data from past sub-tasks cannot be stored and used for re-training, or else to determine when to stop re-training. Causality Data from future sub-tasks, which are often known in academic studies but not in applications, must not be utilized in any way, especially not for DNN model selection. This point might seem trivial, but a number of studies such as Kirkpatrick et al. (2016); Goodfellow et al. (2013) and Srivastava et al. (2013) perform model selection in hindsight, after having processed all sub-tasks. Constant update complexity Re-training complexity (time and memory) must not depend on the number of previous sub-tasks, thus more or less excluding replay-based schemes such as Shin et al. (2017). Clearly, even if update complexity is constant w.r.t. the number of previous sub-tasks, it should not be too high in absolute terms either.
1Although the comparisons performed in Aljundi et al. (2018) include many datasets, the experimental protocol is unclear, so it is uncertain how to interpret these results.
2

Under review as a conference paper at ICLR 2019
1.3 CONTRIBUTION AND PRINCIPAL CONCLUSIONS
The original contributions of our work can be summarized as follows:
· We propose a novel training and model selection paradigm for incremental learning in DNNs that incorporates typical application constraints, see Sec. 1.2.
· We investigate the incremental learning capacity of various DNN approaches (Dropout, LWTA, EWC and IMM) using the largest number of qualitatively different classification datasets so far described. We find that all investigated models are afflicted by catastrophic forgetting, or else in violation of application constraints and discuss potential workarounds.
· We establish that the "permuted" type of SLTs (e.g., "permuted MNIST") are not recommended when testing for CF as they can be solved by almost any model for any dataset.
1.4 APPROACH OF THIS ARTICLE
We collect a large number of visual classification datasets, from each of which we construct SLTs according to a common scheme, and compare several recent DNN models using these SLTs. The experimental protocol is such that application constraints, see Sec. 1.2, are enforced.
2 METHODS
In the following section, the used models, hyper-parameters, the sequential learning tasks, the tested datasets and the general procedure for evaluation of the experiments are described.
2.1 USED DNN MODELS
For all DNN models described here, we use a TensorFlow (v1.7) implementation under Python (v3.4 and later). The source code for all processed models, the experimentgenerator and evaluation routine can be found on our public available repository (dummy_link_for_double_blind_review_test_with_same_length.org).
FC A normal, fully-connected (FC) feed-forward DNN with a variable number and size of hidden layers, each followed by ReLU, and a softmax readout layer minimizing cross-entropy. CONV A convolutional neural network (CNN) based on the work of Cirean et al. (2011). It is optimized to perform well on image classification problems like MNIST. We use a fixed topology: two conv-layers with 32 and 64 filters of size 5 × 5 plus ReLU and 2 × 2 max-pooling, followed by a fc-layer with 1024 neurons and softmax readout layer minimizing a cross-entropy energy function. EWC The Elastic Weight Consolidation (EWC) model presented by Kirkpatrick et al. (2016). LWTA A fully-connected DNN with a variable number and size of hidden layers, each followed by a Local Winner Takes All (LWTA) transfer function as proposed in Srivastava et al. (2013). IMM The Incremental Moment Matching model as presented by Lee et al. (2017). We examine the weight-transfer and L2-transfer techniques in our experiments, using the provided implementation. D-FC and D-CONV Motivated by Goodfellow et al. (2013) we combine the FC and CONV models with Dropout as an approach to solve the CF problem. Only FC and CONV are eligible for this, as EWC and IMM include dropout by default, and LWTA is incompatible with Dropout.
2.2 HYPER-PARAMETERS AND MODEL SELECTION
We perform model selection in all our experiments by a combinatorial hyper-parameter optimization, whose limits are imposed by the computational resources available for this study. In particular, we vary the number of hidden layers L  {2, 3} and their size S  {200, 400, 800} (CNNs excluded), the learning rate 1  {0.01, 0.001} for sub-task D1, and the re-training learning rate 2  {0.001, 0.0001, 0.00001} for sub-task D2. The batch size (batchsize) is fixed to 100 for all experiments, and is used for both training and testing. As in other studies, we do not use a fixed number of training iterations, but specify the number of training epochs (i.e., passes through the whole dataset) as E = 10 for each processed dataset (see Sec. 2.3), which allows an approximate comparison of different datasets. The number of training/testing batches per epoch, B, can be calculated from the batch size and the currently used dataset size. The set of all hyper-parameters for
3

Under review as a conference paper at ICLR 2019

a certain model, denoted P, is formed as a Cartesian product from the allowed values of the hyperparameters L, S, 1, 2 and complemented by hyper-parameters that remain fixed (E, batchsize) or are particular to a certain model. For all models that use dropout, the dropout rate for the input layer is fixed to 0.2, and to 0.5 for all hidden layers. For CNNs, the dropout rate is set to 0.5 for both
input and hidden layers. All other hyper-parameters for CNNs are fixed, e.g., number and size of layers, the max-pooling and filter sizes and the strides (2 × 2) for each channel. These decisions were made based on the work of Goodfellow et al. (2013). The LWTA block size is fixed to 2, based on the work of Srivastava et al. (2013). The model parameter  for EWC is set to 1/ 2 (set but not described in the source code of Kirkpatrick et al. (2016)). For all models except IMM, the momentum parameter for the optimizer is set to µ = 0.99 (Sutskever et al., 2013). For the IMM models, the SGD optimizer is used, and the regularizer value for the L2-regularization is set to 0.01 for L2-transfer and to 0.0 for weight transfer.

2.3 DATASETS

We select the following datasets (see Tab. 1). In order to construct SLTs uniformly across datasets, we choose the 10 best-represented classes (or random classes if balanced) if more are present.
MNIST (LeCun et al., 1998) is the common benchmark for computer vision systems and classification problems. It consist of gray scale images of handwritten digits (0-9). EMNIST (Cohen et al., 2017) is an extended version of MNIST with additional classes of handwritten letters. There are different variations of this dataset: we extract the ten best-represented classes from the By Class variation containing 62 classes. Fruits 360 (Murean & Oltean, 2017) is a dataset comprising fruit color images from different rotation angles spread over 75 classes, from which we extract the ten best-represented ones. Devanagari (Acharya, 2015) contains gray scale images of Devanagari handwritten letters. From the 46 character classes (1.700 images per class) we extract 10 random classes. FashionMNIST (Xiao et al., 2017) consists of images of clothes in 10 classes and is structured like the MNIST dataset. We use this dataset for our investigations because it is a "more challenging classification task than the simple MNIST digits data (Xiao et al., 2017)". SVHN (Netzer & Wang, 2011) is a 10-class dataset based on photos of house numbers (0-9). We use the cropped digit format, where the number is centered in the color image. CIFAR10 (Krizhevsky, 2009) contains color images of real-world objects e.g, dogs, airplanes etc. NotMNIST (Bulatov Yaroslav) contains grayscale images of the 10 letter classes from "A" to "J", taken from different publicly available fonts. MADBase (Abdelazeem Sherif & El-Sherif Ezzat) is a modified version of the "Arabic Digits dataBase", containing grayscale images of handwritten digits written by 700 different persons.

Table 1: Overview of each dataset's detailed properties. Image dimensions are given as width × height ×

channels. Concerning data imbalance, the largest percentual difference in sample count between any two

classes is given for training and test data, a value of 0 indicating a perfectly balanced dataset.

Dataset

Properties

image size

number of elements train test

class balance (%) train test

CIFAR10

32 × 32 ×3 50.000 10.000 0 0

Devanagari

32 × 32 ×1 18.000 2.000 0.3 2.7

EMNIST

28 × 28 ×1 345.035 57.918 2.0 2.0

FashionMNIST

28 × 28 ×1 60.000 10.000 0 0

Fruits 360

100 ×100 ×3 6.148 2.052 4.0 4.2

MADBase

28 × 28 ×1 60.000 10.000 0 0

MNIST

28 × 28 ×1 55.000 10.000 2.2 2.4

NotMNIST

28 × 28 ×1 529.114 18.724 0 0

SVHN

32 × 32 ×3 73.257 26.032 12.6 13.5

2.4 SEQUENTIAL LEARNING TASKS (SLTS) As described in Sec. 1, each SLT consists of two sub-tasks D1 and D2. For each dataset (see Sec. 2.3), these are defined by either subdividing classes into disjunct groups, or by applying differ-
4

Under review as a conference paper at ICLR 2019

ent spatial permutations to all image data (see Tab. 2). SLTs resulting from a subdivision of classes are denoted, e.g., D9-1a. The first and second digits represent the number of classes assigned to D1 and D2, respectively. The last letter is used to distinguish between different choices of classes for both sub-tasks. From each dataset, we create three SLTs of the type D9-1, eight SLTs of the type
D5-5 and one SLT termed DP10-10, resulting from permutation.

Table 2: Overview of all SLTs. The assignment of classes to sub-tasks (STs) are disjunct, except for DP10-10

where two different random image permutations are applied for D1 and D2.

SLT D5-5

D9-1

ST a b c d e f g h a b c DP10-10

D1 0-4 02468 34689 02567 01345 03489 05678 02368 0-8 1-9 0,2-9 0-9

D2 5-9 13579 01257 13489 26789 12567 12349 14579 9 0 1 0-9

3 EXPERIMENTS

This study presents just one, albeit very large, experiment, whose experimental protocol implements the constraints from Sec. 1.2. Every DNN model from Sec. 2.1 is applied to each SLT as defined in Sec. 2.4 while taking into account model selection, see Sec. 2.2.
A precise definition of our application-oriented experimental protocol is given in Alg. 1.For a given model m and an SLT (D1 and D2), the first step is to determine the best hyper-parameter vector p for sub-task D1 only (see lines 1-4), which determines the model mp used for re-training.
In a second step, mp (from line 5) is used for re-training on D2, with a different learning rate 2 which is varied separately. We introduce two criteria for determining the ( 2-dependent) quality of a re-training phase (lines 6-10): "best", defined by the highest test accuracy on D1  D2, and "last", defined by the test accuracy on D1  D2 at the end of re-training. Although the "best" criterion violates the application constraints of Sec. 1.2 (requires D1), we include it for comparison purposes. Finally, the result is computed as the highest 2-dependent quality (line 11).
Independently of the second step, another training of mp is conducted using D1  D2, resulting in what we term baseline accuracy.
Evaluation for IMM differs slightly: in line 5, a copy of mp is kept, termed mp1 , and the weights of mp are re-initialized. After selecting the best re-trained model m2p as a function of 2, final performance qp is obtained by "merging" the models m1p and m2p and testing the result.

Algorithm 1: The application-oriented evaluation strategy used in this study.

Data: model m, SLT with sub-tasks D1, D2, hyper-parameter value set P Result: incremental learning quality for model with hyper-parameters p: qp 1 forall the p  P do // determine accuracy for all hyper-parameters when training on D1 2 for t  0 to E · B do 3 train(mp, D1train, 1) 4 qp,t test(mp, D1test,t)

5 mp  model mp,t with maximum qp,t

6 7

formalpl t,h2e 2 dmop

8 for t  0 to E · B do

9 train(mp, 2 , D2train, 2) 10 qp,t, 2 test(mp, 2 , D2test,t)

// find best model with max. accuracy on D1 // find best 2 value

11 qp  max 2 best/lastt qp,t, 2

// find parameter set with the best accuracy on D2

4 FINDINGS
The results of the experiment described in Sec. 3 are summarized in Tab. 3, and in Tab. 4 for IMM. They lead us to the following principal conclusions:
5

Under review as a conference paper at ICLR 2019

Permutation-based SLTs should not be used when investigating CF. We find that DP10-10, the SLT based on permutation, does not show CF for any model and dataset, which is exemplary visualized for the FC model in Fig. 2 which fails completely for the other SLTs.

1.0

accuracy

0.5

02 mDDD211ax.Db2aseline

4

6

8 epoc1h0 12 14 16 18 20

CIFAR10SVHN FashionMNNoItSMTNISDTevdaantaaMgsaAerDti BaseMNIST EMNIST Fruits 0

Figure 2: Best FC experiments for DP10-10. The blue surfaces (epochs 0-10) represent the accuracy on D1, the green (covered here) and red surfaces the accuracy on D2 and D1  D2 during re-training (epochs 10-20).
The white bars indicate baseline performance.

All examined models exhibit CF. While this is not surprising for FC and CONV, D-FC as proposed in Goodfellow et al. (2013) performs poorly (see Fig. 3), as does LWTA (Srivastava et al., 2013). For EWC and IMM, the story is slightly more complex and will be discussed below.

accuracy

1.0

0.5

02 DDDm112ax.Db2aseline

4

6

8 epoc1h0 12 14 16 18 20

SVHN CIFAR1F0ashionMDNeIvSaTnagNaortiMdaNtIaSsMTeNtISTMADBasEeMNIST Fruits 0

Figure 3: Best D-FC experiments for SLT D5-5, to be read as Fig. 2 and showing the occurrence of CF.

EWC is mildly effective against CF for simple SLTs. Our experiments shows that EWC is ef-
fective against CF for D9-1, at least when the "best" evaluation criterion is used, which makes use of D1. This, in turn, violates the application requirements of Sec. 1.2. For the "last" criterion not making use of D1, EWC performance, though still significant, is much less impressive. We can see the origins of this difference illustrated in Fig. 4.

test accuracy test accuracy test accuracy

1.0

0.8

0.6

0.4 baseline max

0.2 0.0

ttteeesssttt:::DDD112 D2

0 2 4 6 8ep1o0ch12 14 16 18 20

1.0

0.8

0.6

0.4 baseline max

0.2 0.0

ttteeesssttt:::DDD121 D2

0 2 4 6 8ep1o0ch12 14 16 18 20

1.0

0.8

0.6

0.4 baseline max

0.2 0.0

ttteeesssttt:::DDD211 D2

0 2 4 6 8ep1o0ch12 14 16 18 20

(a) flat linear forgetting

(b) steeper linear forgetting

(c) catastrophic forgetting

Figure 4: Illustrating the difference between the "best" and "last" criterion for EWC. Shown is the accuracy over time for the best model on SLT D9-1c using EMNIST (a), D9-1a using EMNIST (b) and D9-1b using Devanagari (c). The blue curve ( ) measures the accuracy on D1, green ( ) only on D2 and red ( ) the D1  D2 during the training (white) and the re-training phase (gray). Additionally, the baseline (dashed line) is indicated. In all three experiments, the "best" strategy results in approximately 90% accuracy, occurring at the beginning of re-training when D2 has not been learned yet. Here, the magnitude of the best/last difference is a good indicator of CF which clearly happens in (c), partly in (b) and slightly or not at all in (a).

EWC is ineffective against CF for more complex problems. Tab. 3 shows that EWC cannot prevent CF for D5-5 type SLTs, see Fig. 5 for a visualization.

6

Under review as a conference paper at ICLR 2019

Apparently, the EWC mechanism cannot protect all the weights relevant for D1 here, which is likely to be connected to the fact that the number of samples in both sub-tasks is similar. This is not the
case for D9-1 type tasks where EWC does better and where D2 has about 10% of the samples in D1.

SVHN NotMNIST MNIST MADBase Fruits F MNIST EMNIST Devanagari CIFAR10 DS
accuracy

1.0

0.5

0 DDD112 D2

2

4

6

8 epoc1h0 12 14 16 18 20

max. baseline

Fruits SVHN CIFAR1D0evanaFgaasrhidioantMaNsNoeIttSMTNIMSTADBasMe NIST EMNIST0

Figure 5: Best EWC experiments for SLT D5-5d constructed from all datasets, to be read as Fig. 2. We observe that CF happens for all datasets.

Table 3: Summary of incremental learning quality qp , see Alg. 1, over SLTs of type D9-1, D5-5 and DP10-10. For aggregating results over SLTs of the same type, the minimal value of qp is taken.For DP10-10 and D5-5 type tasks, CF (failure to retain D1) is indicated by qualities < 0.5 = , failure to learn D2 by qualities of  .
The corresponding value for D9-1 type tasks is  = 0.1. Only when  is exceeded, re-training can be said to

be successful. Each cell contains two qualities evaluated according to the "best" and "last" criteria, see Alg. 1.

SLT Model FC

D-FC CONV D-CONV LWTA EWC

D5-5

.30/.28 .26/.23 .31/.10 .30/.18 .31/.30 .32/.20

D9-1

.45/.10 .37/.10 .45/.10 .48/.10 .45/.10 .36/.08

DP10-10

.54/.52 .44/.43 .52/.50 .56/.55 .54/.51 .57/.46

D5-5

.49/.42 .46/.26 .49/.45 .49/.11 .11/.10 .40/.23

D9-1

.86/.10 .84/.09 .88/.10 .89/.09 .86/.09 .88/.09

DP10-10

.98/.98 .98/.98 .95/.95 1.0/1.0 .97/.96 1.0/.96

D5-5

.50/.48 .50/.48 .50/.48 .50/.48 .50/.48 .36/.08

D9-1

.88/.09 .88/.09 .89/.09 .89/.09 .88/.09 .92/.51

DP10-10

.99/.99 .99/.99 1.0/1.0 1.0/1.0 .99/.99 1.0/.98

D5-5

.46/.45 .46/.44 .47/.45 .46/.46 .46/.46 .55/.47

D9-1

.78/.10 .77/.10 .81/.10 .81/.10 .78/.10 .85/.50

DP10-10

.90/.88 .88/.87 .92/.92 .92/.92 .90/.89 .95/.95

D5-5

.32/.14 .46/.13 .14/.09 .14/.09 .28/.11 .34/.03

D9-1

.34/.09 .38/.21 .14/.09 .23/.09 .38/.09 .55/.13

DP10-10

1.0/.97 1.0/.99 .90/.88 .97/.96 .98/.12 .98/.90

D5-5

.49/.49 .49/.49 .49/.49 .49/.10 .50/.49 .40/.26

D9-1

.89/.10 .91/.10 .89/.10 .90/.10 .94/.10 .99/.70

DP10-10

.99/.99 .99/.99 .99/.99 .99/.99 .99/.98 1.0/.99

D5-5

.49/.48 .49/.47 .48/.11 .48/.15 .10/.09 .50/.31

D9-1

.88/.10 .88/.10 .88/.10 .88/.10 .87/.10 .99/.71

DP10-10

.99/.99 .98/.98 .99/.99 .99/.99 .98/.98 1.0/.98

D5-5

.49/.49 .49/.49 .49/.49 .50/.49 .50/.49 .57/.50

D9-1

.87/.10 .86/.10 .88/.10 .88/.10 .87/.10 .88/.31

DP10-10

.97/.97 .97/.97 .98/.98 .98/.98 .97/.97 .99/.94

D5-5

.30/.22 .28/.08 .20/.08 .20/.08 .40/.20 .28/.16

D9-1

.60/.07 .35/.07 .67/.07 .58/.07 .61/.07 .26/.10

DP10-10

.81/.80 .50/.50 .20/.20 .84/.84 .82/.79 .39/.29

7

Under review as a conference paper at ICLR 2019

test accuracy test accuracy
accuracy

IMM is effective for all SLTs but unfeasible in practice. As we can see from Tab. 4, wtIMM (we performed the same experiments for l2IMM and obtained slightly better but qualitatively similar results) clearly outperforms all other models compared in Tab. 3. Especially for the D5-5 type SLTs, a modest incremental learning quality is attained, which is however quite far away from the baseline accuracy, even for MNIST-derived SLTs. This is in contrast to the results reported in Lee et al. (2017) for MNIST: we attribute this discrepancy to the application-oriented model selection procedure using only D1 that we perform. In contrast, in Lee et al. (2017), a model with 800/800/800 neurons, for which good results on MNIST are well established, is chosen beforehand, thus arguably making implicit use of D2. A significant problem of IMM is the determination of the balancing parameter , exemplarily illustrated in Fig. 6. Our results show that the optimal value cannot simply be guessed from the relative sizes of D1 and D2, as it is done in Lee et al. (2017), but must be determined by cross-validation, thereby requiring knowledge of D1 (violates constraints). Apart from these conceptual issues, we find that the repeated calculation of the Fisher matrices is quite time and memory-consuming (>4h and >8GB), to the point that the treatment of SLTs from certain datasets becomes impossible even on high-end machine/GPU combinations when using complex models. This is why we can evaluate IMM only for a few datasets. It is possible that this is an artifact of the TensorFlow implementation, but in the present state IMM nevertheless violates not one but two application constraints from Sec. 1.2. Fig. 7 and Fig. 8 give a visual impression of training an IMM model on D9-1 and D5-5 type SLTs, again illustrating basic feasibility, but also the variability of the "tuning curves" we use to determine the optimal balancing parameter .

1.0

0.8

0.6

0.4 baseline best

0.2 0.0

train:D1;test:D1 train:D2;test:D2

epoch0 2 4 6 8 10 12 14 16 18 20

1.0 Mode =0.39, max=0.83

0.8

0.6

0.4 0.2 0.0
0.0

Mean-IMM;test:All Mode-IMM;test:All
alpha0.2 0.4 0.6

Mean =0.60, max=0.79
0.8 1.0

Figure 6: Accuracy measurements of best IMM model on SLT D5-5f for Devanagari dataset. On the left-hand side, the blue curve ( ) measures the accuracy of the first DNN trained on D1, the green curve ( ) the accuracy of the second DNN trained on D2. Additionally, the baseline (dashed line) is delineated. The right-hand side shows the tested accuracy on D1  D2 of the merged DNN as a function of , both for mean-IMM (red ) and the mode-IMM (orange ) variants.

1.0

0.5

0 DmD21ean

2

4

6

8

10

12

14

16ep1o8ch20|

0.0 alpha

0.2

0.4

0.6

0.8

1.0

mode

max. baseline

CIFAR10 SVHN FashionMNISFTruidtastaseMtNIST DevanagaMriADBase 0

Figure 7: Best wtIMM experiments for SLT D5-5b constructed from datasets we were able to test. The blue surfaces (epochs 0-10) represent the test accuracy during training on D1, the green surfaces the test accuracy on D2 during training on D2 (epochs 10-20). The white bars in the middle represent baseline accuracy, whereas the right part shows accuracies on D1  D2 for different  values, computed for mean-IMM (orange surfaces) and mode-IMM (red surfaces).

5 CONCLUSIONS
The primary conclusion from the results in Sec. 4 is that CF still represents a major problem when training DNNs. This is particularly true if DNN training happens under application constraints as
8

Under review as a conference paper at ICLR 2019

wtIMM
accuracy

1.0

0.5

0 DDm21ean

2

4

6

8

10

12

14

16ep1o8ch20|

0.0 alpha

0.2

0.4

0.6

0.8

1.0

mode

max. baseline

CIFAR10 Fruits SVHN DevadnaatgaasreMitNIST FashionMNMISATDBase 0

Figure 8: Best wtIMM experiments for the tested datasets for SLT D9-1c, to be read as Fig. 7.

Table 4: Summary of incremental learning quality qp , see Alg. 1, for the IMM model, evaluated on SLTs of type D9-1, D5-5 (DP10-10 is omitted because near-perfect performance was always attained). For aggregating results over SLTs of the same type, the minimal value of qp (the best) is taken, as the presence of CF is indicated by a single occurrence of it in any SLT of the same type. To be interpreted as Tab. 3.

SLT CIFAR10 Devanagari F MNIST MADBase MNIST

SVHN

Model D5-5 D9-1 D5-5 D9-1 D5-5 D9-1 D5-5 D9-1 D5-5 D9-1 D5-5 D9-1

mode .31 .43 .73 .85 .70 .78 .91 .91 .84 .87 .56 .60

mean .30 .43 .67 .85 .62 .78 .82 .92 .82 .88 .50 .59

outlined in Sec. 1.2. Some of these constraints may be relaxed depending on the concrete application: if some prior knowledge about future sub-task exists, it can be used to simplify model selection and improve results. If sufficient resources are available, a subset of previously seen data may be kept in memory and thus allow a "best" type evaluation/stopping criterion for re-training, see Alg. 1.
In general application scenarios without prior knowledge or extra resources, however, an essential conclusion we draw from Sec. 4 is that model selection must form an integral part of training a DNN on SLTs. Thus, a wrong choice of hyper-parameters based on D1 can be disastrous for the remaining sub-tasks, which is why application scenarios require DNN variants that do not have extreme dependencies on hyper-parameters such as layer number and layer sizes.
Furthermore, from the comparison of "last" and "best" evaluation criterion in Fig. 4, the importance of finding a good stopping criterion for DNN re-training can be observed. Such a criterion would unambiguously indicate when to stop re-training before too much knowledge about D1 is lost, ideally without making use of D1. This is possible, e.g., in prototype-based classification methods such as LVQ and variants, see Nova & Este´vez (2014), where the change of internal parameters due to re-training can provide a measure of damage to knowledge about D1.
Lastly, our findings indicate workarounds that would make EWC or IMM practicable in at least some application scenarios. If model selection is addressed, a small subset of D1 may be kept in memory for both methods: to determine optimal values of  for IMM and to determine when to stop re-training for EWC. Fig. 6 shows that small changes to  do not dramatically impact final accuracy for IMM, and Fig. 4 indicates that accuracy loss as a function of re-training time is gradual in most cases for EWC. The inaccuracies introduced by using only a subset of D1 would therefore not be very large for both algorithms.
To conclude, this study shows that the consideration of applied scenarios significantly changes the procedures to determine CF behavior, as well as the conclusions as to its presence in latestgeneration DNN models. We propose and implement such a procedure, and as a consequence claim that CF is still very much of a problem for DNNs. More research, either on generic solutions, or on workarounds for specific situations, needs to be conducted before the CF problem can be said to be solved. A minor but important conclusion is that the use of "permuted" SLTs should be avoided in future studies on CF.
ACKNOWLEDGMENTS
We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.
9

Under review as a conference paper at ICLR 2019
REFERENCES
Abdelazeem Sherif and El-Sherif Ezzat. AHDBase. URL http://datacenter.aucegypt. edu/shazeem/.
Shailesh Acharya. Deep Learning Based Large Scale Handwritten Devanagari Character Recognition. 2015.
Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selfless Sequential Learning. 2018.
Bulatov Yaroslav. Machine Learning, etc: notMNIST dataset. URL http://yaroslavvb. blogspot.com/2011/09/notmnist-dataset.html.
Dan C. Cirean, Ueli Meier, Jonathan Masci, Luca M. Gambardella, and Ju¨rgen Schmidhuber. Flexible, high performance convolutional neural networks for image classification. IJCAI International Joint Conference on Artificial Intelligence, pp. 1237­1242, 2011. ISSN 10450823. doi: 10.5591/978-1-57735-516-8/IJCAI11-210.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending MNIST to handwritten letters. Proceedings of the International Joint Conference on Neural Networks, 2017-May:2921­2926, 2017.
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. PathNet: Evolution Channels Gradient Descent in Super Neural Networks. 2017. ISSN 1701.08734.
R French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, 3(4): 128­135, 1999. ISSN 13646613. doi: 10.1016/S1364-6613(99)01294-2.
A Gepperth and C Karaoguz. A bio-inspired incremental learning architecture for applied perceptual problems. Cognitive Computation, 2015. accepted.
Alexander Gepperth and Barbara Hammer. Incremental learning algorithms and applications. European Symposium on Artificial Neural Networks ({ESANN}), (April):357­368, 2016.
Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks. 2013. ISSN 17518113. doi: 10.1088/1751-8113/44/8/085201.
Ronald Kemker and Christopher Kanan. FearNet: Brain-Inspired Model for Incremental Learning. pp. 1­16, 2017.
Hyo-Eun Kim, Seungwook Kim, and Jaehwan Lee. Keep and Learn: Continual Learning by Constraining the Latent Space for Knowledge Preservation in Neural Networks. 2018.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. 2016. ISSN 0027-8424. doi: 10.1073/pnas.1611835114.
Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. . . . Science Department, University of Toronto, Tech. . . . , pp. 1­60, 2009. ISSN 1098-6596. doi: 10.1.1.222.9220.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Apllied to Document Recognition. 1998.
Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming Catastrophic Forgetting by Incremental Moment Matching. (Nips):1­16, 2017.
Horea Murean and Mihai Oltean. Fruit recognition from images using deep learning. pp. 1­18, 2017. doi: 10.13140/RG.2.2.22059.95527.
Yuval Netzer and Tao Wang. Reading digits in natural images with unsupervised feature learning. Nips, pp. 1­9, 2011.
10

Under review as a conference paper at ICLR 2019
David Nova and Pablo A. Este´vez. A review of learning vector quantization classifiers. Neural Computing and Applications, 25(3):511­524, Sep 2014. ISSN 1433-3058. doi: 10.1007/ s00521-013-1535-3. URL https://doi.org/10.1007/s00521-013-1535-3.
German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual Lifelong Learning with Neural Networks: A Review. pp. 1­29, 2018.
Sylvestre-alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCaRL : Incremental Classifier and Representation Learning. pp. 2001­2010, 2017.
Boya Ren, Hongzhi Wang, Jianzhong Li, and Hong Gao. Life-long learning based on dynamic combination model. Applied Soft Computing Journal, 56:398­404, 2017. ISSN 15684946. doi: 10.1016/j.asoc.2017.03.005.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual Learning with Deep Generative Replay. (Nips), 2017. ISSN 10495258.
Rupesh Kumar Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Ju¨rgen Schmidhuber. Compete to Compute. Nips, pp. 2310­2318, 2013. ISSN 10495258.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, (2010):8609­8613, 2013. ISSN 15206149. doi: 10.1109/ICASSP.2013.6639346.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. pp. 1­6, 2017.
11

