Under review as a conference paper at ICLR 2019

PROVABLE ONLINE DICTIONARY LEARNING AND SPARSE CODING
Anonymous authors Paper under double-blind review

ABSTRACT
We consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, where the sparse weights forming the linear combination are known as coefficients. Since both the dictionary and coefficients parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. Yet, these provide guarantees only on the recovery of the dictionary, without explicit recovery guarantees on the coefficients. Moreover, any estimation error in the dictionary adversely impacts the ability to successfully localize and estimate the coefficients. This potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest. To this end, we develop a simple online alternating optimizationbased algorithm for dictionary learning, which recovers both the dictionary and coefficients exactly. Specifically, we show that ­ when initialized appropriately ­ the algorithm linearly converges to the true factors. Our algorithm is also scalable and amenable for large scale distributed implementations in neural architectures, by which we mean that it only involves simple linear and non-linear operations. Finally, we corroborate these theoretical results via empirical evaluation of convergence properties, and phase transition in sample complexity.

1 INTRODUCTION

Sparse models avoid overfitting by favoring simple yet highly expressive representations. Since

signals of interest may not be inherently sparse, expressing them as a sparse linear combination

of a few columns of a dictionary is used to exploit the sparsity properties. Of specific interest are

overcomplete dictionaries, since they provide a flexible way of capturing the richness of a dataset,

while yielding sparse representations that are robust to noise; see Mallat and Zhang (1993); Chen

et al. (1998); Donoho et al. (2006). In practice however, these dictionaries may not be known,

warranting a need to learn such representations ­ known as dictionary learning (DL) or sparse

coding (Olshausen and Field, 1997). Formally, this entails learning an a priori unknown dictionary A  Rn×m and sparse coefficients x(j)  Rm from data samples y(j)  Rn generated as

y(j) = Ax(j), x(j) 0  k for all j = 1, 2, . . .

(1)

This particular model can also be viewed as an extension of the low-rank model (Pearson, 1901). Here, instead of sharing a low-dimensional structure, each data vector can now reside in a separate low-dimensional subspace. Therefore, together the data matrix admits a union-of-subspace model. As a result of this additional flexibility, DL finds applications in a wide range of signal processing and machine learning tasks, such as denoising (Elad and Aharon, 2006), image inpainting (Mairal et al., 2009), clustering and classification (Ramirez et al., 2010), and analysis of deep learning primitives (Ranzato et al., 2008; Gregor and LeCun, 2010); see also Elad (2010), and references therein.

Notwithstanding the non-convexity of the associated optimization problems (since both factors are unknown), alternating minimization-based dictionary learning techniques have enjoyed significant success in practice. Popular heuristics include regularized least squares-based (Olshausen and Field, 1997; Lee et al., 2007; Mairal et al., 2009; Lewicki and Sejnowski, 2000; Kreutz-Delgado et al., 2003), and greedy approaches such as the method of optimal directions (MOD) (Engan et al., 1999) and k-SVD (Aharon et al., 2006). However, these are difficult to analyze in theory.

1

Under review as a conference paper at ICLR 2019

To this end, motivated from a string of recent theoretical works (Gribonval and Schnass, 2010; Jenatton et al., 2012; Geng and Wright, 2014), provable algorithms for DL have been proposed recently to explain the success of aforementioned alternating minimization-based algorithms (Agarwal et al., 2014; Arora et al., 2014; 2015). However, these works exclusively focus on guarantees for dictionary recovery. On the other hand, for applications of DL in tasks such as classification and clustering ­ which rely on coefficient recovery ­ it is crucial to have guarantees on coefficients recovery as well.
Contrary to conventional prescription, a sparse approximation step after recovery of the dictionary does not help; since any error in the dictionary ­ which leads to an error-in-variables (EIV) (Fuller, 2009) model for the dictionary ­ degrades our ability to even recover the support of the coefficients (Wainwright, 2009). Further, when this error is non-negligible, the existing results guarantee recovery of the sparse coefficients only in 2-norm sense (Donoho et al., 2006). As a result, there is a need for scalable dictionary learning techniques with guaranteed recovery of both factors.

1.1 SUMMARY OF OUR CONTRIBUTIONS

In this work, we present a simple online DL algorithm motivated from the following regularized

least squares-based problem, where S(·) is a nonlinear function that promotes sparsity.

pp

min

{ }A,

x(j)

p
j=1
j=1

y(j) - Ax(j)

2 2

+

S(x(j)).

j=1

(P1)

Although our algorithm does not optimize this objective, it leverages the fact that the problem (P1) is convex w.r.t A, given the sparse coefficients {xj}. Following this, we recover the dictionary by choosing an appropriate gradient descent-based strategy (Arora et al., 2015; Engan et al., 1999). To recover the coefficients, we develop an iterative hard thresholding (IHT)-based update step (Haupt and Nowak, 2006; Blumensath and Davies, 2009), and show that ­ given an appropriate initial estimate of the dictionary and a mini-batch of p data samples at each iteration of the online algorithm ­ alternating between this IHT-based update for coefficients, and a gradient descent-based update for the dictionary, recovers the true factors exactly.

In addition to achieving exact recovery of the two factors, our algorithm ­ Neurally plausible alternating Optimization-based Online Dictionary Learning (NOODL) ­ has linear convergence properties. Furthermore, it is scalable, and involves simple operations, making it an attractive choice for practical DL applications. Our major contributions are summarized as follows:

· Provable coefficient recovery: To the best of our knowledge, this is the first result on exact recovery of the sparse coefficients {x(j)}, including their support recovery, for the DL problem. The proposed IHT-based strategy to update coefficient under the EIV model, is of independent interest for recovery of the sparse coefficients via IHT, which is challenging even when the dictionary is known; see also Yuan et al. (2016).
· Unbiased estimation of factors and linear convergence: The recovery guarantees on the coefficients also helps us to get rid of the bias incurred by the prior-art in dictionary estimation. Furthermore, our technique geometrically converges to the true factors.
· Online nature and neural implementation: The online nature of algorithm, makes it suitable for machine learning applications with streaming data. In addition, the separability of the coefficient update allows for distributed implementations in neural architectures (only involves simple linear and non-linear operations) to solve large-scale problems. To showcase this, we also present a prototype neural implementation of NOODL.

In addition, we also verify these theoretical properties of NOODL through experimental evaluations on synthetic data, and compare its performance with state-of-the-art provable DL techniques.

1.2 RELATED WORKS

With the success of the alternating minimization-based techniques in practice, a push to study the DL problem began when Gribonval and Schnass (2010) showed that for m = n, the solution pair (A, X) lies at a local minima of the following non-convex optimization program, where X = [x(1), x(2), . . . , x(p)] and Y = [y(1), y(2), . . . , y(p)], with high probability over the randomness of the coefficients,

min X 1 s.t. Y = AX, Ai = 1,  i  [m],
A,X

(2)

2

Under review as a conference paper at ICLR 2019

Table 1: Comparison of provable algorithms for dictionary learning.

Conditions

Recovery Guarantees

Method

Initial Gap of Dictionary

Maximum Sparsity

Sample Complexity

Dictionary

Coefficients

NOODL (this work) Arora15(``biased'') Arora15(``unbiased'')
Barak et al. (2015)¶ Agarwal et al. (2014)
Spielman et al. (2012) (for n  m)

O

1 log(n)

N/A O (1/poly(m))
N/A



O

n µ log(n)

O(m(1-)) for  > 0 O ( 6 n/µ)  O( n)

 mk2
 (mk) poly(m) nO(d)/poly(k/m) (m2)
(n2)

No bias O( k/n) Negligible bias §
No bias No bias

No bias N/A N/A N/A N/A N/A

Dictionary recovery reported in terms of column-wise error.  See Section 5 for description.  This procedure is not online. § The bias is not explicitly quantified. The authors claim it will be Negligible. ¶ Here, d = ( 1 log(m/n)) for column-wise error of .

Following this, Geng and Wright (2014) and Jenatton et al. (2012) extended these results to the overcomplete case (n < m), and the noisy case, respectively. Concurrently, Jung et al. (2014; 2016) studied the nature of the DL problem for S(·) = · 1 (in (P1)), and derived a lower-bound on the minimax risk of the DL problem. However, these works do not provide any algorithms for DL.
Motivated from these theoretical advances, Spielman et al. (2012) proposed an algorithm for the under-complete case n  m that works up-to a sparsity of k = O( n). Later, Agarwal et al. (2014) and Arora et al. (2014) proposed clustering-based provable algorithms for the overcomplete setting, motivated from MOD (Engan et al., 1999) and k-SVD (Aharon et al., 2006), respectively. Here, in addition to requiring stringent conditions on dictionary initialization, Agarwal et al. (2014) alternates between solving a quadratic program for coefficients and an MOD-like (Engan et al., 1999) update for the dictionary, which is too expensive in practice. Recently, a DL algorithm that work for almost linear sparsity was proposed by Barak et al. (2015); however, as shown in Table 1, this algorithm may result in exponential running time. Finally, Arora et al. (2015) proposed a provable online DL algorithm, which provided improvements on initialization, sparsity, and sample complexity, and is closely related to our work. We summarize the provable techniques in Table 1.
The provable DL techniques discussed above implicitly assume that coefficients can be recovered ­ after dictionary recovery ­ via some sparse approximation technique. However, as alluded to earlier, the guarantees for coefficient recovery ­ when the dictionary is known approximately ­ may be limited to some 2 norm bounds (Donoho et al., 2006). This means that, the resulting coefficient estimates may not even be sparse. In addition, for practical applications, there is a need for efficient online algorithms that do not rely on computationally expensive post-processing steps for the coefficient recovery. These serve as the primary motivations for our work.
Notation. Given an integer n, we denote [n] = {1, 2, . . . , n}. The bold upper-case and lowercase letters are used to denote matrices M and vectors v, respectively. Mi, M(i,:), Mij, and vi (and v(i)) denote the i-th column, i-th row, (i, j) element of a matrix, and i-th element of a vector, respectively. We denote D(v) as a diagonal matrix with elements of a vector v on the diagonal. The superscript (·)(n) denotes the n-th iterate, while the subscript (·)(n) is reserved for the n-th data sample. Given a matrix M, we use M-i to denote a resulting matrix without i-th column, M and M F as the spectral norm and Frobenius norm. Given a vector v, we use v , v 0, and v 1 to denote the 2 norm, 0 (number of non-zero entries), and 1 norm, respectively. We also use standard notations O(·), (·) (O(·), (·)) to indicate the asymptotic behavior (ignoring logarithmic factors). Further, we use g(n) = O(f (n)) to indicate that g(n)  Lf (n) for a small enough constant L, which is independent of n. We use c(·) for constants parameterized by the quantities in
(·). T (z) := z · 1|z| denotes the hard-thresholding operator, where "1" is the indicator function.
We use supp(·) for the support (the set of non-zero elements) and sign(·) for the element-wise sign.

2 ALGORITHM
We now detail the specifics of our algorithm ­ NOODL, outlined in Algorithm 1. NOODL recovers both the dictionary and the coefficients exactly given an appropriate initial estimate A(0) of the dictionary. Specifically, we require A(0) to be ( 0, 2)-close to A for 0 = O(1/ log(n)), where ( , )-closeness is defined as follows. This implies that, the initial dictionary estimate needs to be column-wise, and in spectral norm sense, close to A, which can be achieved via certain initialization algorithms, such as the initialization algorithm presented in Arora et al. (2015).

3

Under review as a conference paper at ICLR 2019

Algorithm 1: NOODL: Neurally plausible alternating Optimization-based Online Dictionary Learning.

Input: Fresh data samples y(j)  Rn for j  [p] at each iteration t generated as per (1), where
|xi|  C for i  supp(x). Parameters A, x(r) and  (r) chosen as per A.5 and A.6. No. of iterations T = (log(1/ T )) and R = (log(1/R)), for target tolerances T and R. Output: The dictionary A(T ) and coefficient estimates x((tj)) for j  [p] at each iterate t.
Initialize: Estimate A(0), which is ( 0, 2)-near to A for 0 = O(1/ log(n)) for t = 0 to T - 1 do
Predict: (Estimate Coefficients)

for j = 1 to p do

Initialize: x((j0)) = TC/2(A(t) y(j))

(3)

for r = 0 to R - 1 do

Update: x((rj)+1) = T (r) (x((jr)) - x(r) A(t) (A(t)x((jr)) - y(j))) end

end

x((tj)) := x((jR)) for j  [p]

Learn: (Update Dictionary)

Form empirical gradient estimate:

g(t)

=

1 p

pj=1(A(t)x((tj)) - y(j))sign(x((tj)))

Take a gradient descent step: A(t+1) = A(t) - A g(t)

end Normalize: A(it+1) = A(it+1)/ A(it+1)  i  [m]

(4)
(5) (6)

Definition 1 (( , )-closeness). A dictionary A is ( , )-close to A if A - A   A , and if there is a permutation  : [m]  [m] and a collection of signs  : [m]  {±1} such that (i)A(i) - Ai  ,  i  [m].
Due to the streaming nature of the incoming data, NOODL takes a mini-batch of p data samples at the t-th iteration of the algorithm, as shown in Algorithm 1. It then proceeds by alternating between two update stages: coefficient estimation ("Predict") and dictionary update ("Learn") as follows.
Predict Stage: For a general data sample y = Ax, the algorithm begins by forming an initial coefficient estimate x(0) based on a hard thresholding (HT) step as shown in (3). Given this initial estimate x(0), the algorithm iterates over R = (log(1/R)) IHT-based steps (4) to achieve a target tolerance of R, such that (1 - x)R  R. Here, x(r) is the learning rate, and  (r) is the threshold at the r-th iterate of the IHT. In practice, these can be fixed to some constants for all iterations; see A.6 for details. Finally at the end of this stage, we have estimate x(t) := x(R) of x.
Learn Stage: Using this estimate of the coefficients, we update the dictionary at t-th iteration A(t) by an approximate gradient descent step (6), using the empirical gradient estimate (5) and the learning rate A = (m/k); see also A.5. Finally, we normalize the columns of the dictionary and continue to the next batch. The running time of each step t of NOODL is therefore O(mnp log(1/R)). For a target tolerance of T , such that A(iT ) - Ai  T , i  [m], we choose T = (log(1/ T )).
NOODL uses an initial HT step and an approximate gradient descent-based strategy as in Arora et al. (2015). Following which, our IHT-based coefficient update step yields an estimate of the coefficients at each iteration of the online algorithm. Coupled with the guaranteed progress made on the dictionary, this also removes the bias in dictionary estimation. Further, the simultaneous recovery of both factors also avoids an often expensive post-processing step for recovery of the coefficients.

3 MAIN RESULT
We start by introducing a few important definitions. First, as discussed in the previous section we require that the initial estimate A(0) of the dictionary is ( 0, 2)-close to A. In fact, we require this closeness property to hold at each subsequent iteration t, which is a key ingredient in our analysis. This initialization achieves two goals. First, the (i)A(i) - Ai  0 condition ensures that

4

Under review as a conference paper at ICLR 2019

the signed-support of the coefficients are recovered correctly (with high probability) by the hard thresholding-based coefficient initialization step, where signed-support is defined as follows.
Definition 2. The signed-support of a vector x is defined as sign(x) · x(supp(x)).
Next, the A-A  2 A condition keeps the dictionary estimates close to A and is used in our analysis to ensure that the gradient direction (5) makes progress. Further, in our analysis, we ensure t (defined as A(it) - Ai  t) contracts at every iteration, and assume 0, t = O(1/ log(n)). Also, we assume that the dictionary A is fixed (deterministic) and µ-incoherent, defined as follows. Definition 3. A matrix A  Rn×m with unit-norm columns is µ-incoherent if for all i = j the inner-product between the columns of the matrix follow | Ai, Aj |  µ/ n.

The incoherence parameter measures the degree of closeness of the dictionary elements. Smaller

values (i.e., close to 0) of µ are preferred, since they indicate that the dictionary elements do not

resemble each other. This helps us to effectively tell dictionary elements apart (Donoho and Huo,

2001; Cande`s and Romberg, 2007). We assume that µ = O(log(n)) (Donoho and Huo, 2001). Next,

we assume that the coefficients are drawn from a distribution class D defined as follows.

Definition 4 (Distribution class D). The coefficient vector x belongs to an unknown distribution D, where the support S = supp(x) is at most of size k, Pr[i  S] = (k/m) and Pr[i, j  S] =

(k2/m2). S] = 1, and

Moreover, the distribution is normalized when i  S, |xi |  C for some constant

such C

that E[xi|i  1. In addition,

S] the

= 0 and non-zero

E[xi2 |i  entries are

pairwise independent conditioned on the support.

The randomness of the coefficient is necessary for our finite sample analysis of the convergence. Here, there are two sources of randomness. The first is the randomness of the support, where the non-zero elements are assumed to pair-wise independent. The second is the value an element in the support takes, which is assumed to be zero mean with variance one, and bounded in magnitude. Similar conditions are also required for support recovery of sparse coefficients, even when the dictionary is known (Wainwright, 2009; Yuan et al., 2016). Note that, although we only consider the case |xi|  C for ease of discussion, analogous results may hold more generally for xis drawn from a distribution with sufficiently (exponentially) small probability of taking values in [-C, C].

Recall that, given the coefficients, we recover the dictionary by making progress on the least squares objective (P1) (ignoring the term penalizing S(·)). Note that, our algorithm is based on finding an appropriate direction to ensure descent than the geometry of the objective. To this end, we adopt a gradient descent-based strategy for dictionary update. However, since the coefficients are not exactly known, this results in an approximate gradient descent-based approach, where the empirical gradient estimate is formed as (5). In our analysis, we establish the conditions under which both the empirical gradient vector (corresponding to each dictionary element) and the gradient matrix concentrate around their means. To ensure progress at each iterate t, we show that the expected gradient vector is ((k/m), (m/k), 0)-correlated with the descent direction, defined as follows.
Definition 5. A vector g(t) is (-, + , t)-correlated with a vector z if g(t), z(t) - z  - z(t) - z 2 + + g(t) 2 - t.

This can be viewed as a local descent condition which leads to the true dictionary columns; see also

(Cande`s et al., 2015), (Chen and Wainwright, 2015) and (Arora et al., 2015). In convex optimiza-

tion literature, objective. We

this condition show that for

NisOimOpDliLe,dbty=the0,2w-hi-cshtrofancgilciotantveesxliitnye,aarncdo1n/v2erg+e-nscmeotoothAnesws iotfhothuet

incurring any bias. Overall our specific model assumptions for the analysis can be formalized as:

A.1 A is µ-incoherent (Def. 3), where µ = O(log(n)), A = O( m/n) and m = O(n); A.2 The coefficients are drawn fromthe distribution class D, as per Def. 4; A.3 The sparsity k satisfies k = O( n/µ log(n)); A.4 A(0) is ( 0, 2)-close to A (Def. 1), and 0 and each subsequent t are t, 0 = O(1/ log(n)); A.5 The step-size for dictionary update satisfies A = (m/k); A.6 The step-size and threshold for coefficient estimation satisfies x(r) < c1( t, µ, n, k) < 1 and
 (r) = c2(k) for small constants c1 and c2.

We are now ready to state our main result. A summary of the notation followed by a details of the analysis is provided in Appendix A and Appendix B, respectively.

5

Under review as a conference paper at ICLR 2019
Theorem 1 (Main Result). Suppose that assumptions A.1-A.6 hold, and Algorithm 1 is provided with p = (mk2) new samples generated according to model (1) at each iteration t. Then for some 0 <  < 1/2, the estimate A(t) at (t)-th iteration satisfies
A(it) - Ai 2  (1 - )t A(i0) - Ai 2, for all t = 1, 2, . . . .
Furthermore, given R = log(n), with probability at least (1 - alg) for some small constant alg, the coefficient estimate xi(t) at t-th iteration has the correct signed-support and satisfies
(xi(t) - xi )2 = O(k(1 - )t/2 A(i0) - Ai ), for all i  supp(x).
Our main result establishes that when the model satisfies A.1A.3, the errors corresponding to the dictionary and coefficients geometrically decrease to the true model parameters, given appropriate dictionary initialization and learning parameters (step sizes and threshold); see A.4A.6. In other words, to attain a target tolerance of T where Ai(T ) - Ai  T , we require T = (log(1/ T )). Moreover, given R = (log(1/R)) IHT iterations (for a target decay tolerance parameter R which removes the effect of the initial x(0) and is defined as R  (1 - x)R), the error in the coefficients only depends on the error in the dictionary and drops geometrically at every iteration t. Also, note that NOODL can tolerate i.i.d. noise, as long as the noise variance is controlled to enable the concentration results to hold; we consider the noiseless case here for simplicity.
The introduction of IHT step adds complexity in the analysis of both the dictionary and coefficients. The primary condition that allows us to make progress on both factors is the signed-support recovery (Def. 2). To analyze the coefficients, in addition to deriving conditions on the parameters to preserve the correct signed-support, we analyze the recursive update step and decompose the noise term into a component that depends on the error in the dictionary, and the other that depends on the initial coefficient estimate. Further, for the dictionary update, we analyze the interactions between elements of the coefficient vector introduces by the IHT-based update step and show that the gradient vector for the dictionary update is ((k/m), (m/k), 0)-correlated with the descent direction. In the end, this leads to exact recovery of the coefficients and removal of bias in the dictionary estimation. Note that our analysis pipeline is standard for the convergence analysis for iterative algorithms. However, the introduction of the IHT-based strategy for coefficient update makes the analysis highly involved as compared to existing results, e.g., the simple HT-based coefficient estimate in Arora et al. (2015).
NOODL has an overall running time of O(mnp log(1/R) log(1/ T )) to achieve A(iT ) - Ai  T , i  [m], with a total sample complexity of p · T = (mk2). Thus, the IHT-based update in NOODL introduces a factor of log(1/R) in the computational complexity as compared to Arora et al. (2015) (has a total sample complexity of p · T = (mk)) in order to remove the bias, and does not have the exponential running time and sample complexity as Barak et al. (2015); see Table 1.
4 NEURAL IMPLEMENTATION OF NOODL
The neural plausibility of our algorithm implies that it can be implemented as a neural network. This is because, NOODL employs simple linear and non-linear operations (such as inner-product and hard-thresholding) and the coefficient updates are separable across data samples, as shown in (4) of Algorithm 1. To this end, we present a neural implementation of the our algorithm in Fig. 1, which showcases the applicability of NOODL in large-scale distributed learning tasks, motivated from the implementations described in (Olshausen and Field, 1997) and (Arora et al., 2015).
The neural architecture shown in Fig. 1(a) has three layers ­ input layer, weighted residual evaluation layer, and the output layer. The input to the network is a data and step-size pair (y(j), x) to each input node. Given an input, the second layer evaluates the weighted residuals as x y(j)-A(t)x((jr)) . Finally, the output layer neurons evaluate the IHT iterates x((jr)+1) (4). We illustrate the operation of this architecture using the timing diagram in Fig. 1(b). The main stages of operation are as follows.
Initial Hard Thresholding Phase: The coefficients initialized to zero, and an input (y(j), 1) is provided to the input layer at a time instant = 0, which communicates these to the second layer. Therefore, the residual at the output of the weighted residual evaluation layer evaluates to y(j) at
= 1. Next, at = 2, this residual is communicated to the output layer, which results in evaluation
6

Under review as a conference paper at ICLR 2019

Figure 1: A neural implementation of NOODL. Panel (a) shows the neural architecture, which consists of three layers: an input layer, a weighted residual evaluation layer, and an output layer. Panel (b) shows the operation of the neural architecture in panel (a). The update of x((jr)+1) is given by (4).

=0 Output: x  0 Residual: 0 Input: (y(j), 1)

(a) Neural implementation of NOODL
=1 =2 =3 =4

=5

0 x((j0)) = T (A(t) y(j))

x((j0))

x((j1))

x((j1))

y(j)

y(j)

x(y(j) - A(t)x((j0))) x(y(j) - A(t)x((j0))) x(y(j) - A(t)x((1j)))

. . . = 2R + 1 . . . x((jR)) . . . x(y(j) - A(t)x((jR)-1))

Hebbian Learning: Residual sharing and dictionary update.

. (y(j), x)

.

. . . . . (y(j), 1)

(b) The timing sequence of the neural implementation.

of the initialization x((j0)) as per (3). This iterate is communicated to the second layer for the next
residual evaluation. Also, at this time, the input layer is injected with (y(j), x) to set the step size parameter x for the IHT phase, as shown in Fig. 1(b).

Iterative Hard Thresholding (IHT) Phase: Beginning = 3, the timing sequence enters the IHT phase. Here, the output layer neurons communicate the iterates x((jr)+1) to the second layer for evaluation of subsequent iterates as shown in Fig. 1(b). The process then continues till the time instance = 2R + 1, for R = (log(1/R)) to generate the final coefficient estimate x((jt)) := x((jR))
for the current batch of data. At this time, the input layer is again injected with (y(j), 1) to prepare the network for residual sharing and gradient evaluation for dictionary update.

Dictionary Update Phase: The procedure now enters the dictionary update phase, denoted as "Heb-
bian Learning" in the timing sequence. In this phase, each output layer neuron communicates the final coefficient estimate x((tj)) = x((jR)) to the second layer, which evaluates the residual for one last time (with x = 1), and shares it across all second layer neurons ("Hebbian learning"). This allows each second layer neuron to evaluate the empirical gradient estimate (5), which is used to update the
current dictionary estimate (stored as weights) via a approximate gradient descent step.

This completes one outer iteration of Algorithm 1, and the process continues for T = (log(1/ T )) to achieve A(iT ) - Ai  T , i  [m], with each step receiving a new mini-batch of data.

5 EXPERIMENTS

In this section, we analyze the convergence properties and sample complexity of NOODL via experimental evaluations. Additional results are shown in Appendix E.

5.1 CONVERGENCE ANALYSIS

We compare the performance of our algorithm NOODL with the current state-of-the-art alternating optimization-based algorithms presented in Arora et al. (2015). First of these, Arora15(``biased''), is a simple neurally plausible method which incurs a bias and has a sample complexity of (mk). The other one, referred to as Arora15(``unbiased''), incurs no bias as per Arora et al. (2015), but the sample complexity results were not established.

Data Generation: We generate a 1000 × 1500 matrix, whose entries are drawn from the standard

normal distribution. We then normalize the columns of this matrix to form the ground-truth dictionary A; i.e., here n = 1000 and m = 1500. Next, we perturb this dictionary with random Gaussian

noise, such that the unit-norm columns of the resulting matrix, A(0) are 2/ log(n) away from A, in

2-norm sense, i.e., Ai(0) - Ai each iteration, we generate p =

 2/ log(n); this satisfies the initialization assumptions in A.4. At 5000 samples by multiplying the dictionary A with a coefficient

matrix X  Rm×p with at most k = 10, 20, 50, and 100, entries per column, drawn from the

Radamacher distribution. We report the results in terms of relative Frobenius error for all the ex-

7

Under review as a conference paper at ICLR 2019

Dictionary Recovery Across Techniques
Relative Frobenius Error

Performance of NOODL
Relative Frobenius Error

k = 10, A = 30

10-2 10-4 10-6 10-8 10-10

50 100 150
Iterations
(a-i)

Relative Frobenius Error

Relative Frobenius Error

Relative Frobenius Error

k = 20, A = 30
10-2 10-4 10-6

k = 50, A = 15
10-2 10-4 10-6

k = 100, A = 15
100
10-2
10-4 10-6

10-8

10-8

10-8

10-10

10-10 20 40 60 80 100

Iterations

(b-i)

20 40 60
Iterations
(c-i)

10-10 80

10 20 30 40
Iterations
(d-i)

Relative Frobenius Error

Relative Frobenius Error

Relative Frobenius Error

10-2 10-4 10-6 10-8 10-10

50 100 150
Iterations
(a-ii)

10-2 10-4 10-6 10-8 10-10

10-2
10-4
10-6
10-8
10-10 20 40 60 80 100
Iterations
(b-ii)

20 40 60
Iterations
(c-ii)

10-2
10-4
10-6
10-8
10-10 80

10 20 30
Iterations
(d-ii)

40

Success Probability

Success Probability

Phase Transition
1

0.8

0.6

0.4
m=n 0.2 m = 2n
m = 4n 0
0 0.5 1 1.5 p=m
(e-i) Dictionary

2

1

0.8

0.6

0.4
m=n 0.2 m = 2n
m = 4n 0
0 0.5 1 1.5 2 p=m
(e-ii) Coefficients

Figure 2: Comparative analysis of convergence properties. Panels (a-i), (b-i), (c-i), and (d-i) show the convergence of NOODL, Arora15(``biased''), and Arora15(``unbiased'') for different sparsity levels for n = 1000, m = 1500 and p = 5000. Since NOODL also recovers the coefficients, we show the corresponding recovery of the dictionary, coefficients, and overall fit in panels (a-ii), (b-ii), (c-ii), and (d-ii), respectively. Further, panels (e-i) and (e-ii) show the phase transition in samples p (per iteration) with the size of the dictionary m averaged across 10 Monte Carlo simulations for the two factors. Here, n = 100, k = 3, x = 0.2,  = 0.1, 0 = 2/ log(n), A is chosen as per A.5. A trial is considered successful if the relative Frobenius error incurred by A and X is below 5 × 10-7 after 50 iterations.

periments, i.e., for a recovered matrix M, we report M - M F/ M F. Note that, although our phase transition analysis for NOODL shows that a batch size of p = m suffices, we use p = 5000
in our convergence analysis for a fair comparison with related techniques.

Discussion: Fig. 2 panels (a-i), (b-i), (c-i), and (d-i) show the performance of the aforementioned methods for k = 10, 20, 50, and 100, respectively. Here, for all experiments we set x = 0.2 and  = 0.1. We terminate NOODL when the error in dictionary is less than 10-10, and others when they converge. Also, for coefficient update, we terminate when change in the iterates is below 10-12. For k = 10, 20 and k = 50, we note that Arora15(``biased'') and Arora15(``unbiased'') incur significant bias, while NOODL converges to A linearly. NOODL also converges for significantly higher choices of sparsity k, i.e., for k = 100 as shown in panel (d), beyond k = O( n), indicating a potential for improving this bound. Also, in panels (a-ii), (b-ii), (c-ii) and (d-ii) we show the corresponding performance of NOODL in terms of the error in the overall fit ( Y - AX F/ Y F), and the error in the coefficients and the dictionary, in terms of relative Frobenius error metric discussed above. We observe that the error in dictionary and coefficients drops linearly as indicated by our main result.

5.2 PHASE TRANSITIONS

Fig. 2 panels (e-i) and (e-ii), shows the phase transition in number of samples with respect to the

size of the

p m

=

0.75

dictionary m. We observe a sharp phase for the coefficients. This phenomenon is

transition

at

p m

=

1

similar one observed

for by

the dictionary, Agarwal et al.

and at (2014)

(however, theoretically they required p = O(m2)). Here, we confirm number of samples required

by NOODL are linearly dependent on the dictionary elements m.

6 CONCLUSIONS
We present NOODL, to the best of our knowledge, the first provable online algorithm for exact recovery of both factors of the dictionary learning (DL) model. Here, we pose the DL problem as a factorization task, and develop an algorithm that alternates between: (a) an iterative hard thresholding (IHT)-based step for coefficient recovery, and (b) a gradient descent-based update for the dictionary, resulting in a simple and scalable algorithm, suitable for large-scale distributed implementations. Further, we show that once initialized appropriately, the sequence of estimates produced by NOODL converge linearly to the true dictionary and coefficients without incurring any bias in the estimation. Complementary to our theoretical and numerical results, we also design an implementation of NOODL in a neural architecture for use in practical applications. Finally, the analysis of this inherently non-convex problem impacts matrix and tensor factorization tasks arising in signal processing, collaborative filtering, and machine learning.

8

Under review as a conference paper at ICLR 2019
REFERENCES
AGARWAL, A., ANANDKUMAR, A., JAIN, P., NETRAPALLI, P. and TANDON, R. (2014). Learning sparsely used overcomplete dictionaries. In COLT.
AHARON, M., ELAD, M. and BRUCKSTEIN, A. (2006). k-svd: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on signal processing, 54 4311­4322.
ARORA, S., GE, R., MA, T. and MOITRA, A. (2015). Simple, efficient, and neural algorithms for sparse coding. In COLT.
ARORA, S., GE, R. and MOITRA, A. (2014). New algorithms for learning incoherent and overcomplete dictionaries. In COLT.
BARAK, B., KELNER, J. A. and STEURER, D. (2015). Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing. ACM.
BLUMENSATH, T. and DAVIES, M. E. (2009). Iterative hard thresholding for compressed sensing. Applied and computational harmonic analysis, 27 265­274.
CANDE` S, E. and ROMBERG, J. (2007). Sparsity and incoherence in compressive sampling. Inverse problems, 23 969.
CANDE` S, E. J., LI, X. and SOLTANOLKOTABI, M. (2015). Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61 1985­2007.
CHEN, S. S., DONOHO, D. L. and SAUNDERS, M. A. (1998). Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing, 20 33­61. https://doi.org/10.1137/S1064827596304010
CHEN, Y. and WAINWRIGHT, M. J. (2015). Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees. CoRR, abs/1509.03025.
DONOHO, D., ELAD, M. and TEMLYAKOV, V. N. (2006). Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52 6­18.
DONOHO, D. L. and HUO, X. (2001). Uncertainty principles and ideal atomic decomposition. IEEE Transactions on Information Theory, 47 2845­2862.
ELAD, M. (2010). Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing. 1st ed. Springer Publishing Company, Incorporated.
ELAD, M. and AHARON, M. (2006). Image denoising via sparse and redundant representations over learned dictionaries. IEEE Transactions on Image processing, 15 3736­3745.
ENGAN, K., AASE, S. O. and HUSOY, J. H. (1999). Method of optimal directions for frame design. In In Proceedings of 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing., vol. 5. IEEE.
FULLER, W. A. (2009). Measurement error models, vol. 305. John Wiley & Sons.
GENG, Q. and WRIGHT, J. (2014). On the local correctness of 1-minimization for dictionary learning. In 2014 IEEE International Symposium on Information Theory (ISIT),. IEEE.
GERSHGORIN, S. A. (1931). Uber die abgrenzung der eigenwerte einer matrix. . 749­754.
GREGOR, K. and LECUN, Y. (2010). Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML). Omnipress.
GRIBONVAL, R. and SCHNASS, K. (2010). Dictionary identification and sparse matrix-factorization via 1 -minimization. IEEE Transactions on Information Theory, 56 3523­3539.
HANSON, D. and WRIGHT, F. T. (1971). A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42 1079­1083.
9

Under review as a conference paper at ICLR 2019
HAUPT, J. and NOWAK, R. (2006). Signal reconstruction from noisy random projections. IEEE Transactions on Information Theory, 52 4036­4048.
JENATTON, R., GRIBONVAL, R. and BACH, F. (2012). Local stability and robustness of sparse dictionary learning in the presence of noise. Research report. https://hal.inria.fr/hal-00737152
JUNG, A., ELDAR, Y. and GO¨ RTZ, N. (2014). Performance limits of dictionary learning for sparse coding. In Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European. IEEE.
JUNG, A., ELDAR, Y. C. and GRTZ, N. (2016). On the minimax risk of dictionary learning. IEEE Transactions on Information Theory, 62 1501­1515.
KREUTZ-DELGADO, K., MURRAY, J. F., RAO, B. D., ENGAN, K., LEE, T. and SEJNOWSKI, T. J. (2003). Dictionary learning algorithms for sparse representation. Neural computation, 15 349­396.
LEE, H., BATTLE, A., RAINA, R. and NG, A. Y. (2007). Efficient sparse coding algorithms. In Advances in Neural Information Processing Systems.
LEWICKI, M. S. and SEJNOWSKI, T. J. (2000). Learning overcomplete representations. Neural Comput., 12 337­365. http://dx.doi.org/10.1162/089976600300015826
MAIRAL, J., BACH, F., PONCE, J. and SAPIRO, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the 26th Annual International Conference on Machine Learning. ACM.
MALLAT, S. G. and ZHANG, Z. (1993). Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal Processing, 41 3397­3415.
OLSHAUSEN, B. A. and FIELD, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37 3311­3325.
PEARSON, K. (1901). On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2 559­572. https://doi.org/10.1080/14786440109462720
RAMIREZ, I., SPRECHMANN, P. and SAPIRO, G. (2010). Classification and clustering via dictionary learning with structured incoherence and shared features. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (2010). IEEE.
RANZATO, M., BOUREAU, Y. and LECUN, Y. (2008). Sparse feature learning for deep belief networks. In Advances in Neural Information Processing Systems(NIPS) 2008 (J. C. Platt, D. Koller, Y. Singer and S. T. Roweis, eds.). Curran Associates, Inc., 1185­1192.
RUDELSON, M. and VERSHYNIN, R. (2013). Hanson-wright inequality and sub-gaussian concentration. Electronic Communications in Probability, 18.
SPIELMAN, D. A., WANG, H. and WRIGHT, J. (2012). Exact recovery of sparsely-used dictionaries. In Conference on Learning Theory.
TROPP, J. (2015). An introduction to matrix concentration inequalities. Foundations and Trends R in Machine Learning, 8 1­230.
WAINWRIGHT, M. J. (2009). Sharp thresholds for high-dimensional and noisy sparsity recovery using 1-constrained quadratic programming (lasso). IEEE Transactions on Information Theory, 55 2183­2202.
YUAN, X., LI, P. and ZHANG, T. (2016). Exact recovery of hard thresholding pursuit. In Advances in Neural Information Processing Systems.
10

Under review as a conference paper at ICLR 2019

A SUMMARY OF NOTATION

The following table summarizes the definitions of some frequently used symbols in our analysis.

Table 2: Frequently used symbols

Dictionary Related

Symbol Definition

A(it) i-th column of the dictionary estimate at the t-th iterate.

t

A(it) - Ai



t

=

O(

1 log(n)

)

Upper-bound on column-wise error

at the t-th iterate.

µt

µt n

=

µ n

+2

t

Incoherence between the columns of A(t); See Claim 1.

j(t)

(jt) := |

Aj(t) - Aj , Aj

|

2 t
2

Inner-product between the error and the dictionary element.

S(t)(i, j)

S(t)(i, j) =

(jt), for j = i, i  S 0, otherwise.

A diagonal matrix of size |S| × |S| with j(t) on the diagonal for j  S.

Coefficient Related

Symbol Definition

x(ir) i-th element the coefficient estimate at the r-th IHT iterate.

C |xi |  C for i  supp(x) and C  1 Lower-bound on xi s.

S S := supp(x) where |S|  k

Support of x

R

R

:=

(1

-

x

+

x

µt n

)R



(1

-

x)R

Decay parameter for coefficients.

Ci( ) Ci( ) := |xi - xi( )| for i  supp(x) Error in non-zero elements of the

coefficient vector.

Probabilities

Symbol Definition

qi

qi

=

Pr[i



S]

=

(

k m

)

pi pi = E[xisign(xi )|xi = 0]

(t) (t) = 2k exp(-1/O( t))

g(ti) g(ti) = exp(-(k))

Symbol
qi,j T(t) H(tW) g(t)

Definition

qi,j

=

Pr[i, j



S]

=

(

k2 m2

)

T(t) = 2m exp(-C2/O( 2t ))

H(tW) = exp(-1/O( t))

g(t) = (n + m) exp(-(m log(n))

Other terms

Symbol Definition

j(r+1)

j(r+1) := ( A(jt) - Aj, Ai + Aj, Ai )xi -

A(jt), A(it) x(ir)

i=j i=j

j(t)
t j(r+1)

j(t) := ( Aj, Ai - Ai(t) + Aj - A(jt), A(it) + A(jt) - Aj, Ai )xi

t

=

i=j O( k

t)

is

an

upper-bound

on

j(t)

with

probabililty

at

least

(1 - (t))

j(r+1) := j(t) + | A(jt), Ai(t) | |xi - x(ir)|

i=j

j(t) i(R)
i(R) 
xi

j(t) := E[A(St)S(R)sign(xj)]
i(R) := R xi(r)(1 - x)R-r + i(R)
r=1
i(R) := (1 - x)R(xi(0) - xi (1 - i(t)))
 := E[(A(t)x - y)sign(xj)1Fx ]; See  below.
xi := xi(R) = xi (1 - (it)) + (iR)

1 Fx

is

the

indicator

function

corresponding

to

the

event

that

sign(x)

=

sign(x),

denoted by Fx , and similarly for the complement Fx

11

Under review as a conference paper at ICLR 2019

B PROOF OF THEOREM 1

We now prove our main result. The detailed proofs of intermediate lemmas and claims are organized in Appendix C and Appendix D, respectively. Furthermore, the standard concentration results are stated in Appendix F for completeness. Also, see Table 3 for a map of dependence between the results.

OVERVIEW

Given a ( 0, 2)-close estimate of the dictionary, the main property that allows us to make progress on the dictionary is the recovery of the correct sign and support of the coefficients. Therefore, we first show that the initial coefficient estimate (3) recovers the correct signed-support in Step I.A. Now, the IHT-based coefficient update step also needs to preserve the correct signed-support. This is to ensure that the approximate gradient descent-based update for the dictionary makes progress. Therefore, in Step I.B, we derive the conditions under which the signed-support recovery condition is preserved by the IHT update.

To get a handle on the coefficients, in Step II.A, we derive an upper-bound on the error incurred

by each non-zero coefficient vector

element of the x, and show

estimated coefficient vector, i.e., that this error only depends on

|xi - xi | for i  S for a general t (the column-wise error in the

dictionary) given enough IHT iterations R as per the chosen decay parameter R. In addition, for

analysis of the dictionary update, we develop an expression for the estimated coefficient vector in

Step II.B.

We then use the coefficient estimate to show that the gradient vector satisfies the local descent condition (Def. 5). This ensures that the gradient makes progress after taking the gradient descent-based step (6). To begin, we first develop an expression for the expected gradient vector (corresponding to each dictionary element) in Step III.A. Here, we use the closeness property Def 1 of the dictionary estimate. Further, since we use an empirical estimate, we show that the empirical gradient vector concentrates around its mean in Step III.B. Now using Lemma 15, we have that descent along this direction makes progress.

Next in Step IV.A and Step IV.B, we show that the updated dictionary estimate maintains the closeness property Def 1. This sets the stage for the next dictionary update iteration. As a result, our main result establishes the conditions under which any t-th iteration succeeds.

Our main result is as follows.

Theorem 1 (Main Result) Suppose that assumptions A.1-A.6 hold, and Algorithm 1 is provided with p = (mk2) new samples generated according to model (1) at each iteration t. Then for some 0 <  < 1/2, the estimate A(t) at (t)-th iteration satisfies
A(it) - Ai 2  (1 - )t Ai(0) - Ai 2, for all t = 1, 2, . . . .
Furthermore, given R = log(n), with probability at least (1 - alg) for some small constant alg, the coefficient estimate x(it) at t-th iteration has the correct signed-support and satisfies
(xi(t) - xi )2 = O(k(1 - )t/2 Ai(0) - Ai ), for all i  supp(x).
Here, a(ltg) = T(t) + (t) + HW + g(ti) + g(t), T(t) = 2m exp(-C2/O( t2)), (t) = 2k exp(-1/O( t)), H(tW) = exp(-1/O( t)), g(ti) = exp(-(k)), and g(t) = (n + m) exp(-(m log(n)).

STEP I: COEFFICIENTS HAVE THE CORRECT SIGNED-SUPPORT
As a first step, we ensure that our coefficient estimate has the correct signed-support (Def. 2). To this end, we first show that the initialization has the correct signed-support, and then show that the iterative hard-thresholding (IHT)-based update step preserves the correct signed-support for a suitable choice of parameters.
· Step I.A: Showing that the initial coefficient estimate has the correct signed-support­ Given a ( 0, 2)-close estimate A(0) of A, we first show that for a general sample y the

12

Under review as a conference paper at ICLR 2019

initialization step (3) recovers the correct signed-support with probability at least (1-T(t)),

where

T(t)

=

2m

exp(-

C2 O(

2 t

)

).

This

is

encapsulated

by

the

following

lemma.

Lemma t-close

1 to

(ASig.neTdh-esnu,pipf oµrt=reOco(vloegry(nb)y),ckoe=fficienn/tµinloitgi(anli)z,aatinodn

step). Suppose A(t) is t = O(1/ log(m)),

with probability at least (1 - T(t)) for each random sample y = Ax:

sign(TC/2((A(t)) y) = sign(x),

where

T(t)

=

2m

exp(-

C2 O(

2 t

)

).

Note that this result only requires the dictionary to be column-wise close to the true dictionary, and works for less stringent conditions on the initial dictionary estimate, i.e., requires t = O(1/ log(m)) instead of t = O(1/ log(m)); see also (Arora et al., 2015).

· Step I.B: The iterative IHT-type updates preserve the correct signed support­ Next, we show that the IHT-type coefficient update step (4) preserves the correct signed-support for an appropriate choice of step-size parameter x(r) and threshold  (r). The choice of these parameters arises from the analysis of the IHT-based update step. Specifically, we show that at each iterate r, the step-size x(r) should be chosen to ensure that the component corresponding to the true coefficient value is greater than the "interference" introduced by other non-zero coefficient elements. Then, if the threshold is chosen to reject this "noise", each iteration of the IHT-based update step preserves the correct signed-support.

Lemma close to

2 (IHT A, µ

update step preserves the = O(log(n)), k = n/µ

correct log(n),

signed-support). and t = O(1/

Suppose A(t) log(m)) Then,

is twith

probability at least (1 - (t) - T(t)), each iterate of the IHT-based coefficient update step

shown in (4) has the correct signed-support, if for a constant c(1r)( t, µ, k, n), the step size

is chosen as x(r)  c(1r) , and the threshold  (r) is chosen as

 (r)

=

x(r)(t

+

µt n

x(r-1) - x

1) := c(2r)(k),

for some constants

c1

and

c2.

Here,

t

=

 O( k

t),

T(t)

=

2m

exp(-

C2 O(

2 t

)

)

,and (t)

=

2k

exp(-

1 O(

t)

).

Note that, although we have a dependence on the iterate r in choice of x(r) and  (r), these can be set to some constants independent of r. In practice, this dependence allows for greater flexibility in the choice of these parameters.

STEP II: ANALYZING THE COEFFICIENT ESTIMATE

We now derive an upper-bound on the error incurred by each non-zero coefficient element. Further, we derive an expression for the coefficient estimate at the t-th round of the online algorithm x(t) := x(R); we use x instead of x(t) for simplicity.

· Step II.A: Derive a bound on the error incurred by the coefficient estimate­ Since Lemma 2 ensures that x has the correct signed-support, we now focus on the error incurred by each coefficient element on the support by analyzing x. To this end, we carefully analyze the effect of the recursive update (4), to decompose the error incurred by each element on the support into two components ­ one that depends on the initial coefficient estimate x(0) and other that depends on the error in the dictionary.

We show that the effect of the component that depends on the initial coefficient estimate

diminishes

by

a

factor

of

(1

-

x

+

x

µt n

)

at

each

iteration

r.

Therefore, for a decay

parameter R, we can choose the number of IHT iterations R, to make this component

arbitrarily small. Therefore, the error in the coefficients only depends on the per column

error in the dictionary, formalized by the following result.

13

Under review as a conference paper at ICLR 2019

Lemma 3 (Upper-bound on the error in coefficient estimation). With probability at least (1 - (t) - T(t)) the error incurred by each element i1  supp(x) of the coefficient estimate is upper-bounded as

|xi1 - xi1 |  O(t) +

(R

+

1)kx

µt n

miax|x(i0)

-

xi |

+

|xi(10)

- xi1 |

R, = O(t)

where t

=

 O( k

t), R

:=

(1

-

x

+

x

µt n

)R

,

T(t)

=

2m

exp(-

C2 O(

2 t

)

),

(t)

=

2k

exp(-

1 O(

t) ),

and

µt

is

the

incoherence

between

the

columns

of

A(t);

see

Claim

1.

This result allows us to show that if the column-wise error in the dictionary decreases at each iteration t, then the corresponding estimates of the coefficients also improve.

· Step II.B: Developing an expression for the coefficient estimate­ Next, we derive the expression for the coefficient estimate in the following lemma. This expression is used to analyze the dictionary update.

Lemma 4 (Expression for the coefficient estimate at the end of R-th IHT iteration). With probability at least (1 - T(t) - (t)) the i1-th element of the coefficient estimate, for each i1  supp(x), is given by

xi1 := xi(1R) = xi1 (1 - i(1t)) + (i1R).

Here, (i1R) is |(i1R)| = O(t), where t

 = O( k

t). Further, i(1t) = |

A(i1t)-Ai1 , Ai1

|

2

t
2

,

T(t)

=

2m

exp(-

C2 O(

2 t

)

)

and

(t)

=

2k

exp(-

1 O(

t

)

).

We again observe that the error in the coefficient estimate depends on the error in the dictionary via i(1t) and i(1R).

STEP III: ANALYZING THE GRADIENT FOR DICTIONARY UPDATE

Given the coefficient estimate we now show that the choice of the gradient as shown in (5) makes progress at each step. To this end, we analyze the gradient vector corresponding to each dictionary element to see if it satisfies the local descent condition of Def. 5. Our analysis of the gradient is motivated from Arora et al. (2015). However, as opposed to the simple HT-based coefficient update step used by Arora et al. (2015), our IHT-based coefficient estimate adds to significant overhead in terms of analysis. Notwithstanding the complexity of the analysis, we show that this allows us to remove the bias in the gradient estimate.
To this end, we first develop an expression for each expected gradient vector, show that the empirical gradient estimate concentrates around its mean, and finally show that the empirical gradient vector is ((k/m), (m/k), 0)-correlated with the descent direction, i.e. has no bias.

· Step III.A: Develop an expression for the expected gradient vector corresponding to each dictionary element­ The expression for the expected gradient vector gj(t) corresponding to j-th dictionary element is given by the following lemma.

Lemma 5 (Expression for the expected gradient vector). Suppose that A(t) is ( t, 2)near to A. Then, the dictionary update step in Algorithm 1 amounts to the following for the j-th dictionary element
E[A(jt+1)] = Aj(t) + Agj(t),

where gj(t) is given by

gj(t) = qj pj

(1

-

(jt))A(jt)

-

Aj

+

1 qj pj

(jt)

±



,

j(t) = | Aj(t) O( mqi,j pj t

- Aj , A(t) ).

Aj

|,

and

j(t)

:=

E[AS(t)S(R)sign(xj )], where

(jt)

=

· Step III.B: Show that the empirical gradient vector concentrates around its expectation­ Since we only have access to the empirical gradient vectors, we show that these concentrate around their expected value via the following lemma.

14

Under review as a conference paper at ICLR 2019

Lemma 6 (Concentration of the empirical gradient vector). Given p = (mk2) samples, the empirical gradient vector estimate corresponding to the i-th dictionary element, gi(t) concentrates around its expectation, i.e.,

gi(t) - gi(t)



o(

k m

t).

with probability at least (1 - g(ti) - (t) - T(t) - H(tW) ), where g(ti) = exp(-(k)).

· Step III.C: Show that the empirical gradient vector is correlated with the descent direction­
Next, in the following lemma we show that the empirical gradient vector gj(t) is correlated with the descent direction. This is the main result which enables the progress in the dictio-
nary (and coefficients) at each iteration t.

Lemma 7 pose A(t)

(Empirical gradient vector is ( t, 2)-near to A, k =

is correlated with O( n) and A =

the descent direction). O(m/k). Then, with

Supprob-

ability at least (1 - T(t) - (t) - H(tW) - g(ti)) the empirical gradient vector gj(t) is

((k/m), (m/k), 0)-correlated with (Aj(t) - Aj ), and for any t  [T ],

Aj(t+1) - Aj 2  (1 -  A) Aj(t) - Aj 2.

This result ensures for at any t  [T ], the gradient descent-based updates made via (5) gets the columns of the dictionary estimate closer to the true dictionary, i.e., t+1  t. Moreover, this step requires closeness between the dictionary estimate A(t) and A, in the
spectral norm-sense, as per Def 1.

STEP IV: SHOW THAT THE DICTIONARY MAINTAINS THE CLOSENESS PROPERTY

As discussed above, the closeness property (Def 1) is crucial to show that the gradient vector is correlated with the descent direction. Therefore, we now ensure that the updated dictionary A(t+1) maintains this closeness property. Lemma 7 already ensures that t+1  t. As a result, we show that A(t+1) maintains closeness in the spectral norm-sense as required by our algorithm, i.e., that it
is still ( t+1, 2)-close to the true dictionary. Also, since we use the gradient matrix in this analysis, we show that the empirical gradient matrix concentrates around its mean.

· Step IV.A: The empirical gradient matrix concentrates around its expectation: We first show that the empirical gradient matrix concentrates as formalized by the following lemma.

Lemma 8 (Concentration of the empirical gradient matrix). With probability at least

(1 - (t) - T(t) - H(tW) - g(t)),

g(t) - g(t)

is

upper-bounded

by

O

(

k m

A

), where

g(t) = (n + m) exp(-(m log(n)).

· Step IV.B: The "closeness" property is maintained after the updates made using the empirical gradient estimate: Next, the following lemma shows that the updated dictionary A(t+1)
maintains the closeness property.

Lemma 9 (A(t+1) maintains closeness). Suppose A(t) is ( t, 2) near to A with t = O(1/ log(n)), and number of samples used in step t is p = (mk2), then with probability at least (1 - T(t) - (t) - H(tW) - g(t)), A(t+1) satisfies A(t+1) - A  2 A .

STEP V: COMBINE RESULTS TO SHOW THE MAIN RESULT
Proof of Theorem 1. From Lemma 7 we have that with probability at least (1 - T(t) - (t) - H(tW) - g(ti)), gj(t) is ((k/m), (m/k), 0)-correlated with Aj . Further, Lemma 9 ensures that each iterate maintains the closeness property. Now, applying Lemma 15 we have that, for A  (m/k), with probability atleast (1 - a(ltg)) any t  [T ] satisfies
Aj(t) - Aj 2  (1 - )t Aj(0) - Aj 2  (1 - )t 20.
where for 0 <  < 1/2 with  = (k/m)A. That is, the updates converge geometrically to A. Further, from Lemma 3, we have that the result on the error incurred by the coefficients. Here,

15

Under review as a conference paper at ICLR 2019

a(ltg) = T(t) + (t) + H(tW) + g(ti) + g(t)). That is, the updates converge geometrically to A. Further, from Lemma 3, we have that the error in the coefficients only depends on the error in the dictionary, which leads us to our result on the error incurred by the coefficients. This completes the proof of our main result.

Table 3: Proof map: dependence of results.

Lemmas Lemma 1
Lemma 2
Lemma 3
Lemma 4
Lemma 5 Lemma 6 Lemma 7
Lemma 8 Lemma 9
Claims Claim 1 Claim 2
Claim 3
Claim 4
Claim 5
Claim 6 Claim 7
Claim 8
Claim 9
Claim 10

Result Signed-support recovery by coefficient initialization step IHT update step preserves the correct signed-support
Upper-bound on the error in coefficient estimation
Expression for the coefficient estimate at the end of R-th IHT iteration Expression for the expected gradient vector Concentration of the empirical gradient vector Empirical gradient vector is correlated with the descent direction Concentration of the empirical gradient matrix A(t+1) maintains closeness
Result Incoherence of A(t) Bound on j(t): the noise component in coefficient estimate that depends on t Error in coefficient estimation for a general iterate (r + 1) An intermediate result for bounding the error in coefficient calculations Bound on the noise term in the estimation of a coefficient element in the support An intermediate result for i(1R) calculations Bound on the noise term in expected gradient vector estimate An intermediate result for concentration results
Bound on variance parameter for concentration of gradient vector Bound on variance parameter for concentration of gradient matrix

Dependence ­
Claim 1, Lemma 1, and Claim 2 Claim 1, Claim 2, Claim 3, and Claim 4 Claim 5
Lemma 4 and Claim 7 Claim 8 and Claim 9 Lemma 5, Claim 7 and Lemma 6 Claim 8 and Claim 10 Lemma 5, Claim 7 and Lemma 8 Dependence ­ ­
­
Claim 2
Claim 6
Claim 3 Claim 6 and Claim 2
Lemma 1, Lemma 4 and Claim 5 Claim 5
Lemma 1, Lemma 4 and Claim 5

C APPENDIX: PROOF OF LEMMAS

We present the proofs of the Lemmas used to establish our main result. Also, see Table 3 for a map of dependence between the results, and Appendix D for proofs of intermediate results.

Proof of Lemma 1. Let y  Rn be general sample generated as y = Ax, where x  Rm is a sparse random vector with support S = supp(x) distributed according to D.4.

The initial decoding step at the t-th iteration (shown in Algorithm 1) involves evaluating the innerproduct between the estimate of the dictionary A(t), and y. The i-th element of the resulting vector can be written as

A(it), y = A(it), Ai xi + wi,

where wi = Ai(t), A-ix-i . Now, since Ai - A(it) 2  t and

Ai - Ai(t)

2 2

=

Ai 2 +

Ai(t) 2 - 2 Ai(t), Ai

= 2 - 2 A(it), Ai ,

we have

| A(it), Ai |  1 - 2t /2.

16

Under review as a conference paper at ICLR 2019

Therefore, the term

2

| A(it), Ai xi |

 (1 -

t
2

)C

=0

, if i  S, , otherwise.

Now, we focus on the wi and show that it is small. By the definition of wi we have

wi = A(it), A- ix-i =

Ai(t), A x =

Ai(t), A x.

=i S\{i}

Here, since var(x) = 1, wi is a zero-mean random variable with variance

var(wi) =

A(it), A 2.

S\{i}

Now, each term in this sum can be bounded as,

A(it), A 2 = ( A(it) - Ai, A + Ai, A )2

 2( Ai(t) - Ai, A 2 + Ai , A 2)

 2(

A(it) - Ai , A

2+

µ2 n

).

Next, A(it) - Ai , A 2 can be upper-bounded as
=i

A(it) - Ai , A 2  AS \{i} 2 2t .
S\{i}

Therefore, we have the following as per our assumptions on µ and k,

AS \{i}

2



(1

+

k

µ n

)



2,

using Gershgorin Circle Theorem (Gershgorin, 1931). Therefore, we have

Ai(t)

- Ai, A

2



2

2 t

.

S\{i}

Finally, we have that

Ai(t), A

2

 2(2

2 t

+

k

µ2 n

)

=

O(

2 t

).

S\{i}

Now, we apply the Chernoff bound for sub-Gaussian random variables wi (shown in Lemma 12) to conclude that

Pr[|wi|



C/4]



2

exp(-

C2

O(

2 t

)

).

Further, wi corresponding to each m should follow this bound, applying union bound we conclude that

Pr[max
i

|wi|



C/4]



2m

exp(-

C2 O(

2 t

)

)

:

T(t).

Proof of Lemma 2. Consider the (r + 1)-th iterate x(r+1) for the t-th dictionary iterate, where
A(it) - Ai  t for all i  [1, m] evaluated as the following by the update step described in Algorithm 1,

x(r+1) = x(r) - x(r+1)A(t) (A(t)x(r) - y)

= (I - x(r+1)A(t) A(t))x(r) - x(r+1)A(t) Ax,

(7)

where x(1) < 1 is the learning rate or the step-size parameter. Now, using Lemma 1 we know that x(0) (3) has the correct signed-support with probability at least (1 - T(t)). Further, since A(t) A can be written as

A(t) A = (A(t) - A) A + A A,

17

Under review as a conference paper at ICLR 2019

we can write the (r + 1)-th iterate of the coefficient update step using (7) as

x(r+1) = (I - x(r+1)A(t) A(t))x(r) - x(r+1)(A(t) - A) Ax + x(r+1)A Ax. Further, the j-th entry of this vector is given by

xj(r+1)=(I - x(r+1)A(t) A(t))(j,:)x(r) - x(r+1)((A(t) - A) A)(j,:)x+x(r+1)(A A)(j,:)x. (8)

We now develop an expression for the j-th element of each of the term in (8) as follows. First, we can write the first term as

(I - x(r+1)A(t) A(t))(j,:)x(r) = (1 - x(r+1))x(jr) - x(r+1)

Aj(t), A(it) x(ir).

i=j

Next, the second term in (8) can be expressed as

x(r+1)((A(t) - A) A)(j,:)x = x(r+1) Aj(t) - Aj , Ai xi
i

= x(r+1) A(jt) - Aj , Aj xj + x(r+1)

A(jt) - Aj, Ai xi.

i=j

Finally, we have the following expression for the third term,

x(r+1)(A A)(j,:)x = x(r+1)xj + x(r+1)

Aj, Ai xi.

i=j

Now using our definition of (jt) = | Aj(t) - Aj , Aj

|

2

t
2

,

combining

all

the

results

for

(8),

and

using the fact that since A(t) is close to A, vectors Aj(t) - Aj and Aj enclose an obtuse angle, we

have the following for the j-th entry of the (r + 1)-th iterate, x(r+1) is given by

xj(r+1) = (1 - x(r+1))xj(r) + x(r+1)(1 - (jt))xj + x(r+1)j(r+1).

Here j(r+1) is defined as

j(r+1) := ( A(jt) - Aj , Ai + Aj , Ai )xi -

Aj(t), A(it) xi(r).

i=j i=j

(9)

Since, Aj , Ai - A(jt), A(it) = Aj , Ai - A(it) + Aj - A(jt), A(it) , we can write (r+1)j as

j(r+1) = j(t) +

Aj(t), Ai(t) (xi - xi(r)),

i=j

(10)

where j(t) is defined as
j(t) := ( Aj , Ai - A(it) + Aj - A(jt), Ai(t) + A(jt) - Aj , Ai )xi .
i=j

(11)

Note that j(t) does not change for each iteration r of the coefficient update step. Further, by Claim 2 we show that |j(t)|  t with probability at least (1 - (t)). Next, we define j(r+1) as

j(r+1) := j(t) + | Aj(t), Ai(t) ||xi - xi(r)|.
i=j

(12)

where j(r+1)  j(r+1). Further, using Claim 1,

j(r+1)



t

+

µt n

xj - xj(r)

1 := m(ra+x1).

Therefore, for the (r + 1)-th iteration, we choose the threshold to be

 (r+1) := x(r+1)m(ra+x1),

(13)

18

Under review as a conference paper at ICLR 2019

and the step-size by setting the "noise" component of (9) to be smaller than the "signal" part, specifically, half the signal component, i.e.,

x(r+1)m(ra+x1) 

x(1-x(r+1)) (r)
2 min

+

x(r+1) 2

(1

-

2

t
2

)C,

Also, since we choose the threshold as  (r) := x(r)m(ra)x, xm(ri)n = x(r)m(ra)x, where xm(0i)n = C/2, we have the following for the (r + 1)-th iteration,

x(r+1)m(ra+x1) 

(1-x(r+1) 2

)

x(r)

m(ra)x

+

(1x(r+1)
2

-

2

t
2

)C.

Therefore, for this step we choose x(r+1) as

  ,(r+1)
x

x(r) 2

m(ra)x

m(ra+x1)

+

x(r) 2

m(ra)x

-

1 2

(1-

2 t
2

)C

Therefore, since m(ra)xs are proportional to x(r-1) - x 1 = O(k), x(r+1) can be chosen as

(14)

x(r+1)  c(r+1)( t, µ, k, n),

for a small constant c(r+1)( t, µ, k, n), x(r+1). Further, since we initialize with the hard-thresholding step, the entries in |x(0)|  C/2. Here, we define m(0a)x = C and x(0) = 1/2, and set the threshold for initial step as x(0)m(0a)x.

Proof of Lemma 3. Using the definition of i(1) as in (12), we have

i(1) = i(1t) +

| Ai(1t), A(i2t) ||xi2 - xi(2-1)|.

i2 =i1

From Claim 2 we have that |i(1t)|  t with probability at least (1 - (t)). Further, using Claim 1 , and letting Ci( ) := |xi - x(i )| = |xi( ) - xi |, i(1) can be upper-bounded as

i(1 )



i(1t)

+

µt n

Ci(2 -1).

i2 =i1

(15)

Rearranging the expression for (r + 1)-th update (9), and using (15) we have the following upperbound
Ci(1r+1)  (1 - x(r+1))Ci(1r) + x(r+1)i(1t)|xi1 | + x(r+1)i(1r+1).
Next, recursively substituting in for Ci(1r), where we define q= (1 - x(q+1)) = 1,

Ci(1r+1) Ci(10)

r

(1

-

x(q+1))

+

i(1t)|xi1

|

r+1

x(

)

r+1
(1

-

x(q+1))

+

r+1

x(

) i(1 )

r+1
(1

-

x(q+1)).

q=0

=1 q=

=1 q=

Substituting for the upper-bound of i(1) from (15),

Ci(1r+1)



i(1r+1)

+

µt n

r+1 x(

)

Ci(2

-1)

r+1
(1

-

x(q+1)).

=1 i2=i1

q=

(16)

Here, i(1r+1) is defined as

i(1r+1) = Ci(10)

r

(1

-

x(q+1))

+

((i1t)|xi1

|

+

i(1t))

r+1

x(

)

r+1
(1

-

x(q+1)).

q=0

=1 q=

(17)

Our aim now will be to express Ci(1 ) for > 0 in terms of Ci(20). Let each j( )  i( ) where j = i1, i2, . . . , ik. Similarly, let Cj(0)  Ci(0) for j = i1, i2, . . . , ik, and all x( ) = x. Then, using
Claim 3 we have the following expression for Ci(1R+1),

Ci(1R+1)



i(1R+1)

+

(k

-

1)x

µt n

R

m( )ax

1

-

x+x

µt n

R-

=1

19

Under review as a conference paper at ICLR 2019

+

(k

-

1)x

µt n

Cm(0a)x

1

-

x

+

x

µt n

R.

Here,

(1

-

x)R



(1

-

x

+

x

µt n

)R



R.

Next

from

Claim

4

we

have

that

with

probability

at

least (1 - (t)),

R

m( )ax

1

-

x

+

x

µt n

=1

R-



Cm(0a)xRR

+

x

1 (1-

µt n

)

(

2 t
2

|xmax

|

+

t ).

Therefore,

for

cx

=

µt n

/(1

-

µt n

)

Ci(1R+1)



i(1R+1)

+

(k

-

1)cx(

2 t
2

|xmax

|

+

t )

+

(R

+

1)(k

-

1)x

µt n

Cm(0a)x

R

.

Now, using the definition of i(1R+1), and using the result on sum of geometric series, we have

i(1R+1)

=

Ci(10)(1

-

x)R+1

+

(i(1t)|xi1 |

+

i(1t))

R+1
x(1

-

x)R-s+1,

s=1

= Ci(10)R + (i1t)|xi1 | + i(1t)

 Ci(10)R+1 +

2 t
2

|xm ax|

+

t

.

Therefore, Ci(1R) is upper-bounded as

Ci(1R)



(cxk

+

1)(

2 t
2

|xm ax|

+

t )

+

(R

+

1)kx

µt n

Cm(0a)x

R

+

Ci(10)R.

 Further, since k = O( n/µ log(n)), kcx < 1, therefore, we have

Ci(1R)



O(t )

+

(R

+

1)kx

µt n

Cm(0a)x

R

+

Ci(10)R,

with

probability

at

least

(1 -

(t)).

Here,

(R

+

1)kx

µt n

Cm(0a)xR

+ Ci(10)R

0 for an appropriately

large R. Therefore, the error in each non-zero coefficient is

with probability at least (1 - (t)).

Ci(1R) = O(t ).

Proof of Lemma 4. Using the expression for x(i1R) as defined in (9), and recursively substituting for x(i1r) we have

xi(1R) = (1 - x)Rx(j0) + xi1 R x(1 - (i1t))(1 - x)R-r + R xi(1r)(1 - x)R-r,

r=1

r=1

where we set all xr to be x. Further, on defining

(i1R) := R xi(1r)(1 - x)R-r + i(1R),
r=1

(18)

where i(1R) := (1 - x)R(x(i10) - xi1 (1 - i(1t))), we have

x(i1R) = (1 - x)Rxi(10) + xi1 (1 - i(1t))(1 - (1 - x)R) + R xi(1r)(1 - x)R-r,
r=1
= xi1 (1 - (i1t)) + i(1R).

(19)

Note that i(1R) can be made appropriately small by choice of R. Further, by Claim 5 we have

|i(1R)|  O(t).

with

probability

at

least

(1

- (t)),

where

t

=

 O( k

t).

20

Under review as a conference paper at ICLR 2019

Proof of Lemma 5. From Lemma 4 we have that for each j  S,

xS := xS(R) = (I - S(t))xS + S(R),
with probability at least (1 - T(t) - (t)). Further, let Fx be the event that sign(x) = sign(x),
and let 1Fx denote the indicator function corresponding to this event. As we show in Lemma 2,
this event occurs with probability at least (1 - (t) - T(t)). Using this, we can write the expected
gradient vector corresponding to the j-th sample as 1Fx
gj(t) = E[(A(t)x - y)sign(xj 1) Fx ] + E[(A(t)x - y)sign(xj)1Fx ], = E[(A(t)x - y)sign(xj 1) Fx ] ± .

Here, drops

 := with

E[(A(t)x - t. Therefore,

y)sign(xj )1Fx ]
 diminishes with

is small and depends on T(t) and (t), which in turn
t. Further, since 1Fx 1+ Fx = 1, and Pr[Fx ] =

(1 - (t) - T(t)), is very large,

gj(t) = E[(A(t)x - y)sign(xj )(1 - 1Fx )] ± ,
= E[(A(t)x - y)sign(xj)] ± .

Therefore, we can write gj(t) as

gj(t) = E[(A(t)x - y)sign(xj)] ± , = E[(1 - x)RA(St)x(S0) + A(St)(I - (St))xS + AS(t)(SR) - AS xS )sign(xj)] ± .

Since E[(1 - x)RA(St)xS(0)] can be made very small by choice of R, we absorb this term in . Therefore,

gj(t) = E[A(St)(I - S(t))xS + AS(t)S(R) - AS xS )sign(xj)] ± . Writing the expectation by sub-conditioning on the support,

gj(t) = ES [ExS [A(St)(I - S(t))xS sign(xj) - AS xS sign(xj ) + AS(t)S(R)sign(xj )|S]] ± , = ES [A(St)(I - (St))ExS [xS sign(xj )|S] - AS ExS [xS sign(xj )|S]] + E[AS(t)(SR)sign(xj )] ± , = ES [pj (1 - (jt))A(jt) - pj Aj ] + (jt) ± , where we have used the fact that ExS [sign(xj )] = 0 and introduced
(jt) = E[AS(t)S(R)sign(xj )]. Next, since pj = ExS [xjsign(xj)|j  S], therefore,
gj(t) = ES [pj (1 - (jt))Aj(t) - pj Aj] + (jt) ± .

Further, since qj = Pr[j  S] = O(k/m),

gj(t) = qj pj

(1

-

(jt))A(jt)

-

Aj

+

1 qj pj

j(t)

±



.

Further, by Claim 7 we have that

j(t)

 = O( mqi,j pj

t

A(t)

)].

This completes the proof.

Proof of Lemma 6. Let W = {j : i  supp(x(j))} and then we have that

gi(t)

=

|W | p

1 |W |

j (y(j) - A(t)x(j))sign(x(j)(i)),

21

Under review as a conference paper at ICLR 2019

where x(j)(i) denotes the i-th element of the coefficient estimate corresponding to the (j)-th sample. Here, for = |W | the summation

j 1 (y(j) - A(t)x(j))sign(x(j)(i)),

has the same distribution as j=1zj, where each zj belongs to a distribution as

z := 1 (y - A(t)x)sign(xi)|i  S.

Also, E[(y - A(t)x)sign(xi)]

=

qiE[z], where qi

=

Pr[xi

=

0]

=

(

k m

).

Therefore, since

p = (mk2), we have = pqi = (k3) non-zero vectors,

gi(t) - gi(t)

=

O(

k m

)

j=1(zj

- E[z])

.

(20)

Let wj = zj - E[z], we will now apply the vector Bernstein result shown in Lemma 11. For this, we require bounds on two parameters for these ­ L := wj and 2 := jE[ wj 2] . Note that, since the quantity of interest is a function of xi , which are sub-Gaussian, they are only bounded almost surely. To this end, we will employ Lemma 14 (Lemma 45 in (Arora et al., 2015)) to get a
handle on the concentration.

Bound on the norm w : This bound is evaluated in Claim 8, which states that with probability at least (1 - (t) - T(t) - H(tW) ),

L := w = z - E[z] = 2 (y - A(t)x)sign(xi)|i  S  2 (y - A(t)x) = O( kt ).

Bound on variance parameter E[ w 2]: Using Claim 9, we have E[ z 2] = O(k t2) + O(kt2 ). Therefore, the bound on the variance parameter 2 is given by

2 := j E[ wj 2]  j E[ zj 2]  O( k t2) + O( kt2 ).

From Claim 2

we have that with probability at least (1 - (t)), t

=

 O( k

t).

Applying vector

Bernstein inequality shown in Lemma 11 and using Lemma 14 (Lemma 45 in (Arora et al., 2015)),

choosing = (k3), we conclude

j=1 zj - E[z] = O(L) + O() = o( t),

with probability at least (1 - g(ti)), where g(ti) = exp(-(k)). Finally, substituting in (20) we have

gi(t) - gi(t)

=

O(

k m

)o(

t).

with probability at least (1 - g(ti) - (t) - T(t) - H(tW) ).

Proof of Lemma 7. Since we only have access to the empirical estimate of the gradient gi(t), we will show that this estimate is correlated with (Aj(t) - Aj ). To this end, first from Lemma 6 we have that
the empirical gradient vector concentrates around its mean, specifically,

gi(t) - gi(t)



o(

k m

t),

with probability at least (1 - g(ti) - (t) - T(t) - H(tW) ). From Lemma 5, we have the following expression for the expected gradient vector

gj(t)

=

pj qj (A(jt)

-

Aj)

+

pj qj (-(jt)A(jt)

+

1 pj qj

j(t)

±

).

Let gj(t) = 4 (A(jt) - Aj) + v, where 4 = pjqj and v is defined as

v

=

pj qj (-j(t)Aj(t)

+

1 pj qj

(jt)

±

).

(21)

Then, gi(t) can be written as gi(t) = gi(t) - gi(t) + gi(t),

22

Under review as a conference paper at ICLR 2019

= (gi(t) - gi(t)) + 4 (Aj(t) - Aj ) + v, = 4 (A(jt) - Aj) + v,

(22)

where v = v + (gi(t) - gi(t)). Let v   Ai(t) - Ai . Using the definition of v as shown in (21) we have

v

 qj pj j(t) Aj(t)

+

(jt)

+

o(

k m

t) ± .

Now for the first term, since

Aj(t)

= 1, we have j(t) = | A(jt) - Aj , Aj | =

1 2

A(jt) - Aj

2,

therefore

Further, using Claim 7

qj pj (jt) Aj(t)

=

qj pj

1 2

A(jt) - Aj

2,

j(t)

 = O( mqi,j pi1

t

A(t)

).

Now, since A(t) - A  2 A , and further since A = O( m/n), we have that

A(t)  A(t) - A + A = O(

m n

).

. Therefore, we need

j(t)

+

o(

k m

 t) ±  = O( mqi,j pi1

t

A(t)

)

qi pi 2

Aj(t) - Aj .

(23)

Here, we use the fact that  drops with decreasing t as argued in Lemma 5. Therefore, if k = O( n) we have

Now, using (22), we have

v  qipi Aj(t) - Aj .

gj(t)  4 Aj(t) - Aj + v .

Substituting for v , this implies that g(t)j 2  252 Aj(t) - Aj 2. Further, we also have the following lower-bound

gj(t), A(jt) - Aj  4 A(jt) - Aj 2 - v Aj(t) - Aj .

Here, we use the fact that R.H.S. can be minimized only if v is directed opposite to the direction of Aj(t) - Aj. Now, we show that this gradient is ( , 1/100 , 0) correlated,

gi(t), Ai(t)-Ai - 

A(it) - Ai

2

-

1 100

gi(t) 2,

 4 Ai(t) - Ai 2 - v

A(it) - Ai

-

Ai(t) - Ai

2

-

1 100

 4

A(it) - Ai 2 - 2

Ai(t) - Ai

2 - 252

A(it) -Ai 100

2
,

  Ai(t) - Ai 2  0.

gi(t) 2,

Therefore, for this choice of k, there is no bias. Further, since we choose 4 = pjqj, we have that  = (k/m), therefore + = 1/100 = (m/k). Applying Lemma 15 we have

Aj(t+1) - Aj 2  (1 -  A) A(jt) - Aj 2,

for A = O(m/k) with probability at least (1 - T(t) - (t) - g(ti)).

Proof of Lemma 8. Here, we will prove that g(t) defined as g(t) = j (y(j) - A(t)x(j))sign(x(j)) ,

23

Under review as a conference paper at ICLR 2019

concentrates around its mean. Notice that each summand (y(j) - A(t)x(j))sign(x(j)) is a random matrix of the form (y - A(t)x)sign(x) . Also, we have g(t) defined as

g(t) = E[(y - A(t)x)sign(x) ].

To bound g(t) - g(t) , we are interested in

p j=1

Wj

, where each matrix Wj is given by

Wj

=

1 p

(y(j)

- A(t)x(j))sign(x(j))

-

1 p

E[(y

-

A(t)x)sign(x)

].

Noting that E[Wj] = 0, we will employ the matrix Bernstein result (Lemma 10) to bound g(t) -

g(t) . To this end, we will bound Wj and the variance proxy

v(Wj) = max{

p j=1

E[Wj

Wj

]

,

p j=1

E[Wj

Wj ]

}.

Bound on Wj ­ First, we can bound both terms in the expression for Wj by triangle inequality as

Wj



1 p

(y(j) - A(t)x(j))sign(x(j))

+

1 p

E[(y - A(t)x)sign(x)

,



2 p

(y - A(t)x)sign(x)

.

Here, we use Jensen's inequality for the second term, followed by upper-bounding the expected value of the argument by (y - A(t)x)sign(x) .

Next, using Claim 8 we have that with probability at least (1 - (t) - T(t) - H(tW) ), y - A(t)x is

O(kt), and the fact that

sign(x)T

= 

k,



Wj



2 p

k (y - A(t)x) = O( k p k t).

Bound on the variance statistic v(Wj)­ For the variance statistic, we first look at E[WjWj ] ,

E[Wj Wj

]

=

1 p2

E[(y(j

)

- A(t)x(j))sign(x(j))

- E[(y - A(t)x)sign(x) ]

× [sign(x(j))(y(j) - A(t)x(j)) - (E[(y - A(t)x)sign(x) ) ].

Since E[(y - A(t)x)sign(x) ]E[(y - A(t)x)sign(x) ] is positive semidefinite,

E[WjWj ]

1 p2

E[(y(j)

-

A(t)x(j))sign(x(j))

sign(x(j))(y(j) - A(t)x(j))

].

Now, since each x(j) has k non-zeros, sign(x(j)) sign(x(j)) = k, and using Claim 10, with probability at least (1 - T(t) - (t))

E[WjWj ]



k p

E[(y(j) - A(t)x(j))(y(j) - A(t)x(j))

],

=

O(

k3 t2 pm

)

A

2.

Similarly, expanding E[Wj Wj], and using the fact that E[(y - A(t)x)sign(x) ] E[(y - A(t)x)sign(x) ] is positive semi-definite. Now, using Claim 8 and the fact that entries of E[(sign(x(j))sign(x(j)) ] are qi on the diagonal and zero elsewhere, where qi = O(k/m),

E[Wj Wj]

1 p

E[(sign(x(j))(y(j) - A(t)x(j))

(y(j) - A(t)x(j))sign(x((jR)))

],



1 p

E[(sign(x(j))sign(x(j))

]



O(

k mp

)O(k2

t2

)

=

O(

k3 t2 mp

).

y(j) - A(t)x(j) 2,

Now, we are ready to apply the matrix Bernstein result. Since, m = O(n) the variance statistic

comes

out

to

be

O(

k3 t2 pm

)

A

2, then as long as we choose p = (mk2), with probability at least

(1 - (t) - T(t) - H(tW) - g(t))


g(t) - g(t)  O( k p k t ) + A

O(

k3 t2 pm

),

=

O

(

k m

A

).

where g(t) = (n + m) exp(-(m log(n)).

24

Under review as a conference paper at ICLR 2019

Proof of Lemma 9. The update step for the i-th dictionary element at the s + 1 iteration can be written as

Ai(t+1) - Ai = A(it) - Ai - Agi(t), = A(it) - Ai - Agi(t) - A(gi(t) - gi(t)).

Here, gi(t) is given by the following as per Lemma 5 with probability at least (1 - T(t) - (t))

gi(t)

=

qipi(Ai(t)

-

Ai )

+

qipi(-i(t)A(it)

+

1 qi pi

(it)

±

).

Substituting the expression for gi(t) in the dictionary update step,

Ai(t+1) - Ai = (1 - Apiqi)(A(it) - Ai) - Apiqii(t)Ai(t) - A(it) - A(gi(t) - gi(t)) ± ,

where j(t) = E[A(t)(R)sign(xj )]j. Therefore, the update step for the dictionary (matrix) can be written as

A(t+1) - A = (A(t) - A)diag((1 - Apiqi)) + AU - AV ±  - A(g(t) - g(t)), (24)

where, U = A(t)diag(piqii(t)) and V = A(t)Q, with the matrix Q given by, Qi,j = qi,j ExS [i(R)sign(xj )|S],
and using the following intermediate result shown in Claim 7,

ExS [i(R)sign(xj)|S]

 i(R), = O(pj t),

 we have Qi = O( mqi,jpi t). Hence, we have

for i = j, for i = j,

Q F  O(mqi,j pi t).

Therefore,

V  A(t)Q  A(t)

Q

F

= O(mqi,j pi

t

A

)

=

O( m

k2 log(n)

)

A

.

We will now proceed to bound each term in (24). Starting with (A(t) - A)diag(1 - Apiqi), and using the fact that pi = O(1), qi = O(k/m), and A(t) - A  2 A , we have

(A(t) - A)diag(1 - Apiqi)  (1 - min Apiqi) (A(t) - A)  2(1 - (Ak/m)) A .
i

Next, since

Aj(t)

=

1,

we have (jt)

=

|

A(jt)

- Aj, Aj

|

=

1 2

Aj(t) - Aj 2, and (it) 

2t /2,

therefore

U

=

A(t)diag(piqi(it))

2



max
i

piqi

t
2

A(t) - A + A

 o(k/m) A .

Using the results derived above, and the the result derived in Lemma 8 which states that with prob-

ability at least (1 - (t) - T(t) - H(tW) - g(t)),

g(t) - g(t)

=

O(

k m

A

)) we have

A(t+1)-A = (A(t) - A)D(1-Apiqi) + A U + A V + A (g(t) - g(t)) ± ,



2(1

-

(A

k m

)

A

+

o(A

k m

)

A

+

O(A

m

k2 log(n)

)

A

+

o(A

k m

A

) ± ,

 2 A .

25

Under review as a conference paper at ICLR 2019

D APPENDIX: PROOFS OF INTERMEDIATE RESULTS

Claim each i

1 

(Incoherence [1 . . . m], then

of A(t)). If A A(t)  Rn×m is

 Rn×m is µ-incoherent µt-incoherent, where µt

and =µ

Ai- +2 n

Ai(t) t.



t holds for

Proof of Claim 1. We start by looking at the incoherence between the columns of A, for j = i,

Ai, Aj = Ai - A(it), Aj + Ai(t), Aj , = Ai - A(it), Aj + A(it), Aj - Aj(t) + A(it), Aj(t) .

Since

Ai , Aj



µ n

,

| Ai(t), Aj(t) |  Ai , Aj - Ai - A(it), Aj - Ai(t), Aj - Aj(t) ,



µ n

+2

t.

Claim 2 (Bound on probability (1 - (t)

),j(|t):j(tt)h| eisnuopipseerc-obmoupnodneednbtyintco=efOfic(ienkt

estimate t), where

that (t)

depends on = 2k exp(-

t).
1 O( t

With ) ).

Proof of Claim 2. We have the following definition for j(t) from (11),

j(t) = ( Aj , Ai - Ai(t) + Aj - Aj(t), A(it) + Aj(t) - Aj , Ai )xi.
i=j

Here, since xi are independent sub-Gaussian random variables, j(t) is a sub-Gaussian random variable with the variance parameter evaluated as shown below

var[j(t)] =

i=j ( Aj , A(it) - Ai

+

A(jt) - Aj , Ai(t)

+

Aj(t) - Aj , Ai

)2

 9k

2 t

.

Therefore, by Lemma 12

Pr[|j(t)|

>

t ]



2exp(-

t2 18k

2 t

).

Now, we need this for each j(t) for j  supp(x), union bounding over k coefficients

Pr[max |j(t)| > t]  (t),

where (t)

=

2k

exp(-

t2 18k

2 ).
t

Choosing t

=

 O( k

t), we have that (t)

=

2k

exp(-

1 O(

t

)

).

Claim 3 (Error in coefficient estimation for a general iterate (r + 1)). The error in a general iterate r of the coefficient estimation is upper-bounded as

Ci(1r+1)



i(1r+1)

+

(k

-

1)x

µt n

r

m( )ax

1

-

x

+

x

µt n

r-

=1

+

(k

-

1)x

µt n

Cm(0a)x

1

-

x

+

x

µt n

r.

Proof of Claim 3 . From (16) we have the following expression for Ci(1r+1)

Ci(1r+1)



i(1r+1)

+

µt n

r+1 x(

)

Ci(2

-1)

r+1
(1

-

x(q+1)).

=1 i2=i1

q=

Our aim will be to recursively substitute for Ci(1 -1) to develop an expression for Ci(1r+1) as a function of Cm0 ax. To this end, we start by analyzing the iterates Ci(11), Ci(12), and so on to develop an expression for Ci(1r+1) as follows.

26

Under review as a conference paper at ICLR 2019

Expression for Ci(11) ­ Consider Ci(11)

Ci(11)



i(11)

+

µt n

1
x

Ci(2 -1)

1
(1 - x),

=1 i2=i1

q=

= i(11) + x

µt n

Ci(20) .

i1 =i2

(25)

Expression for Ci(12)­ Next, Ci(12) is given by

Ci(12)



i(12)

+

x

µt n

2

Ci(2 -1)

2
(1 - x),

=1 i2=i1

q=



i(12)

+

x

µt n

Ci(21) +

Ci(20)(1 - x) .

i2 =i1

i2 =i1

Further, we know from (25) we have

Ci(21)

=

i(21)

+

x

µt n

Ci(30).

i3 =i2

Therefore, since

=,

i2=i1 i3=i2 i3=i2,i1

Ci(12)



i(12)

+

x

µt n

i(21)

+

x

µt n

Ci(30) +

Ci(20)(1 - x) ,

i2 =i1

i3 =i2

i2 =i1

=

i(12)

+

x

µt n

i(21)

+

x

µt n

x

µt n

Ci(30) +

Ci(20)(1 - x) .

i2 =i1

i3 =i2 ,i1

i2 =i1

(26)

Expression for Ci(13)­ Next, we writing Ci(13),

Ci(13)



i(13)

+

x

µt n

3 =1

Ci(2 -1)(1 - x)3- ,

i2 =i1

=

i(13)

+

x

µt n

Ci(20)(1 - x)2 + Ci(21)(1 - x) + Ci(22) ,

i2 =i1



i(13)

+

x

µt n

Ci(20)(1 - x)2 +

i(21)

+

x

µt n

Ci(30) (1 - x) + Ci(22) .

i2 =i1

i3 =i2

Here, using (26) we have the following expression for Ci(22)

Ci(22)



i(22)

+

x

µt n

i(31)

+

x

µt n

x

µt n

Ci(40) +

Ci(30)(1 - x) .

i3 =i2

i4 =i3 ,i2

i3 =i2

Substituting for Ci(22) in the expression for Ci(13), and rearranging the terms in the expression for Ci(13), we have

Ci(13)



i(13)

+

x

µt n

i(22)

+

x

µt n

(1 - x)

i(21)

+

x

µt n

i(31)

i2 =i1

i2 =i1

i3 =i2 ,i1

+

x

µt n

(1 - x)2

Ci(20)

+

2(1

-

x)(x

µt n

)

Ci(30)

+

(x

µt n

)2

Ci(40) .

i2 =i1

i3 =i2 ,i1

i4 =i3 ,i2 ,i1

(27)

Expression for Ci(14)­ Now, consider Ci(14)

Ci(14)



i(14)

+

x

µt n

4 =1

i2=i1 Ci(2 -1)(1 - x)4- ,



i(14)

+

x

µt n

i2=i1 Ci(20)(1 - x)3 + i2=i1 Ci(21)(1 - x)2 + i2=i1 Ci(22)(1 - x)1

+ i2=i1 Ci(23)(1 - x)0 .

Substituting for Ci(23) from (27), Ci(22) from (26), Ci(21) using (25), and rearranging,

Ci(14)



i(14)

+

x

µt n

i(23) + (1 - x)1

i2 =i1

i2 =i1

i(22)

+

x

µt n

i(32)

i3 =i2 ,i1

27

Under review as a conference paper at ICLR 2019

+

i(21)(1

-

x)2

+

2x

µt n

(1

-

x)

i(31)

+

(x

µt n

)2

i(41)

i2 =i1

i3 =i2 ,i1

i4 =i3 ,i2 ,i1

+

x

µt n

Ci(20)(1

-

x)3

+

3x

µt n

(1

-

x)2

Ci(30)

i2 =i1

i3 =i2 ,i1

+

3(x

µt n

)2(1

-

x)1

Ci(40)

+

(x

µt n

)3

Ci(50) .

i4 =i3 ,i2 ,i1

i5 =i4 ,i3 ,i2 ,i1

Notice that the terms have a binomial series like form. To reveal this structure, let each j( )  m( )ax where j = i1, i2, . . . , ik. Similarly, let Cj(0)  Cm(0a)x for j = i1, i2, . . . , ik. Therefore, we have

Ci(14)



i(14)

+

x

µt n

(k - 1)i(3) + i(2)

(1

-

x)1(k

-

1)

+

x

µt n

(k

-

2)

+ i(1)

(k

-

1)(1

-

x)2

+

2(k

-

2)x

µt n

(1

-

x)

+

(k

-

3)(x

µt n

)2

+

x

µt n

Ci(0)

(k

-

1)(1

-

x)3

+

3(k

-

2)x

µt n

(1

-

x)2

+

3(k

-

3)(x

µt n

)2(1

-

x)1

+

(k

-

4)(x

µt n

)3

.

Further upper-bounding the expression, we have

Ci(14)



i(14)

+

(k

-

1)x

µt n

i(3) + i(2)

(1

-

x)

+

x

µt n

+ i(1)

(1

-

x)2

+

2x

µt n

(1

-

x)

+

(x

µt n

)2

+

(k

-

1)x

µt n

Ci(0)

(1

-

x)3

+

3x

µt n

(1

-

x)2

+

3(x

µt n

)2(1

-

x)

+

(x

µt n

)3

.

Therefore,

Ci(14)



i(14)

+

(k

-

1)x

µt n

i(3)+i(2)

1

-

x

+

x

µt n

1 + i(1)

1

-

x

+

x

µt n

2

+

(k

-

1)x

µt n

Ci(0)

1

-

x

+

x

µt n

3.

(28)

Expression for Ci(1r+1)­ With this, we are ready to write the general term,

Ci(1r+1)



i(1r+1)

+

(k

-

1)x

µt n

r

m( )ax

1

-

x

+

x

µt n

r-

=1

+

(k

-

1)x

µt n

Cm(0a)x

1

-

x

+

x

µt n

r.

Claim 4 (An intermediate result for bounding the error in coefficient calculations). With probability (1 - T(t) - (t)),

R m( )ax
=1

1

-

x

+

x

µt n

R-



Ci(0)RR

+

x

1 (1-

µt n

)

(

2 t
2

|xmax

|

+

t ).

Proof of Claim 4 . Using (17), the quantity i( ) is defined as i( ) = Ci(0)(1 - x) + (i(t)|xi| + i(t)) x(1 - x) -s+1.
s=1

28

Under review as a conference paper at ICLR 2019

Therefore, we are interested in

R Ci(0)(1 - x)
=1

1

-

x+x

µt n

R-

+ ((it)|xi| + i(t)) R

1

-

x

+

x

µt n

R-

x(1 - x) -s+1.

=1 s=1

Consider

the

first

term

which

depends

on

Ci(0).

Since

(1

-

x)



(1

-

x

+

x

µt n

, we have

Ci(0)

R
(1 - x)

1

-

x

+

x

µt n

R-

 Ci(0)R

1

-

x

+

x

µt n

R  Ci(0)RR,

=1

where R is a small constant, and a parameter which determines the number of iterations R required for the coefficient update step. Now, coming back to the quantity of interest

R

i(

)

1

-

x

+

x

µt n

R-

 Ci(0)RR

=1

+ ((it)|xi | + i(t)) R

1

-

x

+

x

µt n

R-

x(1 - x) -s+1.

=1 s=1

Now, using sum of geometric series result, we have that x(1 - x) -s+1, and
s=1

R =1

1

-

x

+

x

µt n

R-

= 1-

1-x

+x

µt n

x

-x

µt n

R



x

1 (1-

µt n

)

.

Therefore, with probability at least (1 - (t)),

R m( )ax
=1

1

-

x

+

x

µt n

R-



Ci(0)RR

+

x

1 (1-

µt n

)

(

2 t
2

|xmax

|

+

t ),

where i(t) 

2 t
2

and |i(t)| = t with probability at least (1 - (t)) using Claim 2.

Claim 5 (Bound on the noise term in the estimation of a coefficient element in the support). With probability (1 - (t)), each entry i(1R) of (R) is upper-bounded as

|i(1R)|  O(t).

Proof of Claim 5. From (18) (i1R) is defined as

i(1R) := R xi(1r)(1 - x)R-r + i(1R),
r=1

where i(1R) := (1 - x)R(x(i10) - xi1 (1 - i(1t))). Further, i(1r) is as defined in (10),

i(1r) = i(1t) +

| Ai(1t), Ai(2t) |sign( A(i1t), Ai(2t) )Ci(2r-1)sign(xi2 - xi(2r)).

i2 =i1

Therefore, we have the following expression for i(1R)

(i1R) =i(1t) R x(1 - x)R-r
r=1

R
+ x

| A(i1t), Ai(2t) |sign( Ai(1t), Ai(2t) )Ci(2r-1)sign(xi2 - x(i2r))(1 - x)R-r + i(1R).

r=1 i2=i1

(29)

29

Under review as a conference paper at ICLR 2019

Now i(1R) can be upper-bounded as

RR

i(1R)  i(1t)

x(1

-

x)R-r

+

x

µt n

r=1

r=1

i2=i1 Ci(2r-1)(1 - x)R-r + i(1R),



i(1t)

+

(k

-

1)x

µt n

R

Ci(2r-1)(1 - x)R-r + i(1R).

r=1

Since from Claim 6 we have

Ci(2r-1)(1 - x)R-r  ((mt)ax|xmax| + m(ta)x)

r-1
x(1 - x)R-s + kcx(1 - x)R-r

s=1

+

kx

µt n

Cm(0a)xR-2.

Further, since 1 - (1 - x)r-1  1, we have that

R r-1
x(1 - x)R-s
r=1 s=1

=

R r=1

x(1

-

x)R-r+1

1-(1-x )r-1 x

R
 (1 - x)R-r+1
r=1



1 x

.

Therefore,

|(i1R)|



|i(1t)|

+

(k

-

1)

µt n

((mt)ax|xm ax|

+

|m(ta) x |)(1

+

kcx)

+

kx

µt n

2RCm(0a)xR-2 + i(1R).

Now,

since


each

|i(t)|

=

t

with probability at least (1 - (t)) for the t-th iterate,

and k

=

O( µ

n log(n)

),

therefore

kcx

<

1,

we

have

that

with probability at least (1 - (t)).

|i(1R)|  O(t).

Claim 6 (An intermediate result for i(1R) calculations).

For cx

=

µt n

/(1

-

µt n

),

we

have

Ci(2r-1)(1 - x)R-r ((mt)ax|xmax| + m(ta)x)

r-1
x(1 - x)R-s + kcx(1 - x)R-r
s=1

+

kx

µt n

Cm(0a)xR-2.

Proof of Claim 6. Here, from Claim 3 we have that for any i1,

Ci(1r+1)



i(1r+1)

+

k

x

µt n

r

m( )ax

1

-

x

+

x

µt n

r-

+

kx

µt n

Cm(0a)x

1

-

x

+

x

µt n

r.

=1

therefore Ci(2r-1) is given by

Ci(2r-1)



i(2r-1)

+

k

x

µt n

r-2 m( )ax

1

-

x

+

x

µt n

r-

-2+kx

µt n

Cm(0a)x

1

-

x

+

x

µt n

r-2.

=1

Further, the term of interest Ci(2r-1)(1 - x)R-r can be upper-bounded by

Ci(2r-1)(1

-

x)R-r

i(2r-1)(1

-

x)R-r+(1

-

x )R-r kx

µt n

r-2

m( )ax

1

-

x

+

x

µt n

r- -2

=1

+

kx

µt n

Cm(0a)x

1

-

x

+

x

µt n

r-2(1 - x)R-r.

From the definition of i( ) from (17), i(2r-1) can be written as

i(2r-1)

=

Cm(0a)x(1

-

x)r-1

+

(m(t)ax|xmax|

+

m(ta) x )

r-1
x(1

-

x)r-s.

s=1

Therefore, we have

i(2r-1)(1

-

x)R-r

=

Cm(0a)x(1

-

x)R-1

+

((mt)ax|xmax|

+

r-1
m(ta)x) x(1

-

x)R-s.

s=1

30

Under review as a conference paper at ICLR 2019

Next, to get a handle on m( )ax

1

-

x

+

x

µt n

r- -2, consider the following using the definition of

i( ) from (17), where x(i) = x for all i,

r

m( )ax

1-x

+

x

µt n

r-

=

r

Cm(0a)x(1 - x)

1

-

x

+

x

µt n

r-

=1 =1

r
+ ((mt)ax|xmax| + m(ta)x)

1

-

x

+

x

µt n

r-

x(1 - x) -s+1,

=1 s=1



r

Cm(0a)x

1

-

x

+

x

µt n

r + (m(t)ax|xm ax| + m(ta)x)

r

1

-

x

+

x

µt n

r- .

=1 =1

Therefore,

r-2
(1 - x)R-r m( )ax

1

-

x

+

x

µt n

r- -2  r-2 Cm(0a)x

1

-

x

+

x

µt n

r-2(1 - x)R-r

=1 =1

r-2
+ ((mt)ax|xm ax| + m(ta)x)(1 - x)R-r

1

-

x

+

x

µt n

r- -2,

=1

 (R - 2)Cm(0a)x

1

-

x

+

x

µt n

R-2

+

((mt)ax|xm ax|

+

m(ta) x )

(1-x )R-r

x

(1-

µt n

)

.

Therefore,

(1 - x)R-r r-2 m( )ax

1

-

x

+

x

µt n

r- -2

=1

 (r - 2)Cm(0a)x

1

-

x

+

x

µt n

R-2

+

((mt)ax|xmax|

+

m(ta) x

)

(1-x )R-r

x

(1-

µt n

)

.

Further,

since

(1

-

x)



(1

-

x

+

x

µt n

),

kx

µt n

Cm(0a)x

1

-

x

+

x

µt n

r-2(1

-

x)R-r



k

x

µt n

Cm(0a)x

R-2

.

Therefore,

combining

all

the

results

we

have

that,

for

a

constant

cx

=

µt n

/(1

-

µt n

),

Ci(2r-1)(1 - x)R-r  ((mt)ax|xmax| + m(ta)x)

r-1
x(1 - x)R-s + kcx(1 - x)R-r
s=1

+

kx

µt n

Cm(0a)xR-2.

Claim 7 (Bound on the noise term in expected gradient vector estimate).

E[A(t)(R)sign(xj)] is upper-bounded as,

(jt)

 = O( mqi,j pj

t

A(t)

)].

(jt) where (jt) :=

Proof of Claim 7.

(jt) = E[A(t)(R)sign(xj)] = ES [AS(t)ExS [S(R)sign(xj )|S]] From (29) we have the following definition for j(R)

(jR)= j(t)+ R x

| Aj(t), A(it) |sign( Aj(t), Ai(t) )Ci(r-1)sign(xi - xi(r))(1 - x)R-r+ j(R),

r=1 i=j

where j(t) is defined as the following (11)

j(t) = ( Aj, Ai - A(it) + Aj - A(jt), A(it) )xi + i=j A(jt) - Aj, Ai xi .
i=j

31

Under review as a conference paper at ICLR 2019

Consider ExS [(SR)sign(xj)|S], where S(R) is a vector with each element as defined in (29). Therefore, the elements of the vector ExS [S(R)sign(xj)|S]] are given by

ExS [i(R)sign(xj )|S] =

ExS [(iR)sign(xj )|S], ExS [j(R)sign(xj)|S],

for i = j, for i = j.

Consider the general term of interest

ExS [(iR)sign(xj )|S]
R
 x(1 - x)R-r ExS [isign(xj)|S]
r=1 

R

+

µt n

x(1 - x)R-r

r=1

s=i ExS [Cs(r-1)sign(xs - x(sr))sign(xj )|S] +i(R).


Further, since

we have that

ExS [xi sign(xj)|S] =

0, pj ,

for i = j, for i = j,

 := ExS [i(t)sign(xj )|S] 

3pj t 0

, for i = j, , for i = j.

(30)

Further, for s := ExS [Cs(r-1)sign(xs - xs(r))sign(xj )|S] we have that

s =

ExS [Cj(r-1)(xj - xj(r-1))sign(xj )|S]  Cj(r-1), 0,

for s = j for s = j.

In addition, for

s=i s we have that

s=i s =

Cj(r-1), 0,

for i = j for i = j.

(31)

Therefore, using the results for  and s, we have that ExS [j(R)sign(xj )|S] = i(R) for i = j,
s=i
and for i = j we have

ExS [(iR)sign(xj )|S]

 3pj

t+

µt n

R

ExS [Cj(r-1)sign(xj - x(jr))sign(xj)|S]x(1 - x)R-r + i(R),

r=1

 3pj

t+

µt n

R

Cj(r-1)x(1 - x)R-r + i(R).

r=1

(32)

Here,

from

Claim

6,

for

cx

=

µt n

/(1

-

µt n

)

we

have

Cj(r-1)(1 - x)R-r  (m(t)ax|xmax| + m(ta)x)

r-1
x(1 - x)R-s + kcx(1 - x)R-r
s=1

+

kx

µt n

Cm(0a)xR-2.

Further, due to our assumptions on sparsity, kcx  1; in additionm by Claim 2, and with probability at least (1 - (t)) we have |m(ta)x|  t, substituting,

R Cj(r-1)x(1 - x)R-r
r=1

32

Under review as a conference paper at ICLR 2019

R r-1

R

 ((mt)ax|xm ax| + m(ta)x)

x x(1 - x)R-s + kcx x(1 - x)R-r ,

r=1 s=1

r=1

 ((mt)ax|xm ax| + t)(1 + kcx),

= O(t),

with probability at least (1 - (t)). Combining results from (30), (31) and substituting for the terms in (32) using the analysis above,

ExS [i(R)sign(xj )|S]

 i(R),

 3pj

t+

µ n

t

+

i(R)

= O(pj

t),

for i = j, for i = j.

Note that since i(R) := (1 - x)R(xi(0) - xi (1 - i(t))) can be made small by choice of R. Also, since Pr[i, j  S] = qi,j, we have

j(t)

= 

OE(S [mAqS(ti),jEpxjSt[A(SR()ts)ig)n. (xj )|S]]

,

Claim 8 (An intermediate result for concentration results). With probability (1 - (t) - T(t) - H(tW) ) y - A(t)x is upper-bounded by O(kt) .

Proof of Claim 8. First, using Lemma 4 we have

xi1 := xi(1R) = xi1 (1 - (i1t)) + i(1R). Therefore, the vector xS, for S  supp(x) can be written as

xS := xS(R) = (I - (St))xS + (SR),

(33)

where x has the correct signed-support with probability at least (1 - T ) using Lemma 2. Using this result, we can write y - A(t)x as

y - A(t)x = AS xS - AS(t)(I - S(t))xS - AS(t)(SR) .

Now, since (iit) 

2 t
2

we have

y - A(t)x



AS xS - (1 -

2 t
2

)AS(t)xS

-

A(St)(SR)

,

=

((1 -

2 t
2

)(AS

-

A(St))

+

2 t
2

AS ) xS

-

A(St)S(R)

.



With xS being independent and sub-Gaussian, using Lemma 13, which is a result based on the

Hanson-Wright result (Hanson A(St) - AS  AS(t) - AS F

andWright, 1971) for  k t, we have that

sub-Gaussian random variables, and with probability at least (1 - H(tW) )

since

xS

=

((1 -

2 t
2

)(AS

-

A(St))

+

2 t
2

AS

)xS

 O(

(1 -

2 t
2

)(AS

-

A(St))

+

2 t
2

AS

F ),

where

H(tW)

=

exp(-

1 O(

t

)

).

Now, consider the

 F , since

A(St) - AS

 F kt

 F :=

(1 -

2 t
2

)(AS

-

A(St))

+

2 t
2

AS

F

 

2

(1 - 

t
2

)

k(1 -

(AS -

2

t
2

)

t

+

AS(t)) F +

2 t
2

AS

F.

2 t
2

AS F ,

Consider the  term. Using Claim 5, each (jR) is bounded by O(t). with probability at least

(1 - (t)) Therefore,

 = AS(t)(SR)  AS(t)

(SR)

=

AS(t)

 kO(t ).

33

Under review as a conference paper at ICLR 2019

Again, since

AS(t) - AS



A(St) - AS

F

 k

t,

A(St)



AS(t) - AS + AS



AS(t) - AS

+ AS

  k t + 2.



Finally, combining all the results and using the fact that AS F  k AS  2 k, ,

y - A(t)x

2

= O(

k(1 -

t
2

)

t+



2 t

k) +

A(St)

 kO(t ),

= O(kt).

Claim 9 (Bound on variance parameter for concentration of gradient vector). For z := (y -

A(t)x)sign(xi)|i  S

the variance parameter E[

z

2] is bounded as E[

z

2] = O(k

2 t

)

+

O(kt2

)

with probability at least (1 - (t) - T(t)).

Proof of Claim 9. For the variance E[ z 2], we focus on the following,

E[ z 2] = E[ (y - A(t)x)sign(xi) 2|i  S]. Here, xS is given by
xS = (I - (St))xS + S(R). Therefore, E[ z 2] can we written as

E[ (y - A(t)x)sign(xi) 2|i  S] = E[ (y - A(St)(I - S(t))xS - A(St)S(R))sign(xi) 2|i  S],  E[ (AS - A(St)(I - S(t)))xS 2|i  S] + E[ A(St)(SR)sign(xi) 2|i  S] . (34)



We will now consider each term in (34) separately. We start with . Since xS s are conditionally independent of S, E[xS xS ] = I. Therefore, we can simplify this expression as

 := E[

(AS - A(St)(I - S(t)))xS

2|i  S] = E[

AS - AS(t)(I - (St))

2 F

|i



S

].

Rearranging the terms we have the following for ,

 = E[

AS - AS(t)(I - (St))

2 F

|i



S]

=

E[

AS S(t) + (AS - A(St))(I - S(t))

2 F

|i



S].

Therefore,  can be upper-bounded as

  E[

AS (St)

2 F

|i



S]

+

E[

(AS - A(St))(I - S(t))

2 F

|i



S]

1 2
+ 2E[ AS S(t) F (AS - AS(t))(I - S(t)) F |i  S] .

3
(35)

For 1, since

A(St)

  k t + 2, we have

1 := E[

AS S(t)

2 F

|i



S]



E[

AS

(St)

2 F

|i



S]



AS

jS ((jt))2



 k( k

t + 2)

4

t
4

.

Next, since (1 - (jt))  1, we have the following bound for 2

2 := E[

(AS - AS(t))(I - (St))

2 F

|i



S]



E[

AS - AS(t)

2 F

|i



S]



AS - AS(t)

2 F

k

2t .

Further, 3 can be upper-bounded by using bounds for 1 and 2. Combining the results of upperbounding 1, 2, and 3 we have the following for (35)

  E[

(AS - A(St)(I - S(t)))xS

2|i  S] = O(k

2 t

).

34

Under review as a conference paper at ICLR 2019

Next, by Claim 5, j(R) is upper-bounded as |j(R)|  O(t). with probability (1 - (t)). Therefore, the term , the second term of (34), can be bounded as



A(St)S(R)sign(xi)

2

 ( k

t + 2)2kO(t)2

= O(kt2 ).

Finally, combining all the results, the term of interest in (34) has the following form E[ (y - A(t)x)sign(xi) 2|i  S] = O(k t2) + O(kt2).

Claim 10 (Bound on variance parameter for concentration of gradient matrix). With probabil-

ity (1 - T(t) - (t)), the variance parameter E[(y - A(t)x)(y - Ax) ] is upper-bounded by

O(

k2 t2 m

)

A

2.

Proof of Claim 10. Let Fx be the event that sign(x) = sign(x), and let 1Fx denote the indicator
function corresponding to this event. As we show in Lemma 2, this event occurs with probability at
least (1 - (t) - T(t)), therefore,

E[(y - A(t)x)(y - A(t)x) ]

= E[(y - A(t)x)(y - A(t)x) 1Fx ] + E[(y - A(t)x)(y - A(t)x) 1F¯x ], = E[(y - A(t)x)(y - A(t)x) 1Fx ] ± .

Here,  is small.
1 - 1F¯x ,

Under the event Fx , x has the correct signed-support.

Again, since 1Fx

=

E[(y - A(t)x)(y - A(t)x) ] = E[(y - A(t)x)(y - A(t)x) (1 - 1F¯x )] ± ,
= E[(y - A(t)x)(y - A(t)x) ] ± .

Now, using Lemma 4 with probability at least (1 - T(t) - (t)), xS admits the following expresssion xS := xS(R) = (I - S(t))xS + S(R).
Therefore we have y - A(t)x = (AS - AS(t)(I - (St)))xS - AS(t)S(R).
Using the expression above E[(y - A(t)x)(y - A(t)x) ] can be written as

E[(y - A(t)x)(y - A(t)x) ] = E[((AS - AS(t)(I - (St)))xS - A(St)S(R))((AS - A(St)(I - S(t)))xS - AS(t)S(R)) ].
Sub-conditioning, we have

E[(y - A(t)x)(y-A(t)x) ]
=ES [(AS - AS(t)(I - (St)))ExS [xS xS |S](AS - (I - S(t))A(St) )] - ES [A(St)ExS [S(R)xS |S](AS - (I - (St))A(St) )] - ES [(AS - AS(t)(I - (St)))ExS [xS ((SR)) |S]AS(t) ] + ES [AS(t)ExS [S(R)(S(R)) |S]AS(t) ].

Now, since ExS [xS xS |S] = I, E[(y - A(t)x)(y - A(t)x) ]  ES [(AS - A(St)(I - (St)))(AS - (I - (St))AS(t) )]


35

Under review as a conference paper at ICLR 2019

+ ES [A(St)ExS [S(R)xS |S](AS - (I - S(t))AS(t) )]



+ ES [(AS - AS(t)(I - S(t)))ExS [xS ((SR)) |S]AS(t) ]



+ ES [AS(t)ExS [(SR)(S(R)) |S]AS(t) ] .

(36)



Let's start with the first term () of (36), which can be written as

 : ES [AS AS ] + ES [AS (I - (St))AS(t) ] + ES [AS(t)(I - (St))AS ]

1 2

3

+ ES [AS(t)(I - (St))2A(St) ] .

4

(37)

Now consider each term of equation (37). First, since

m

ES [AS AS ] = ES [

AiAj 1i,jS ] =

AiAi ES [1i,jS ],

i,jS

i,j=1

and

ES [1i,jS ]

=

O(

k2 m2

),

we

can

upper-bound

1

:=

ES [AS AS ]

as

1 :=

ES [AS AS ]

=

O(

k2 m2

)

A1m×mA

=

O(

k2 m

)

A

2,

where 1m×m denotes an m × m matrix of ones. Now, we turn to 2 := ES[AS(I - (St))AS(t) ] in (37), which can be simplified as

2 

m

Ai A(jt)

ES [1i,jS ]



O

(

k2 m

)

A

A(t) .

i,j=1

Further, since A(t) is ( t, 2)-close to A, we have that A(t)  A(t) - A + A  3 A , therefore

2 :=

ES [AS (I - (St))AS(t)

]

=

O(

k2 m

)

A

2.

Similarly,

3

(37)

is

also

O(

k2 m

)

A

2.

Next, we consider 4 :=

ES [AS(t)(I - S(t))2A(St)

]

in

(37) which can also be bounded similarly as

4

=

O(

k2 m

)

A

2.

Therefore, we have the following for  in (36)

 := ES [(AS - AS(t)(I - (St)))(AS

- (I - (St))A(St)

)]

=

O(

k2 m

)

A

2.

(38)

Consider  in (36). Letting M = ExS [(SR)xS |S], and using the analysis similar to that shown in 7, we have that elements of M  Rk×k are given by

Mi,j = ExS [i(R)xj|S]

 O(i(R)), = O( t),

for i = j, for i = j.

We have the following,

 := ES [A(St)ExS [S(R)xS |S](AS - (I - (St))AS(t) )] = ES [AS(t)M(AS - (I - S(t))A(St) )].

Therefore,

since

ES [1i,jS

|S]

=

O(

k2 m2

),

and

1m×m

= m,

 := ES [AS(t)M(AS - (I - S(t))AS(t) )]

36

Under review as a conference paper at ICLR 2019

= m Mi,j Ai(t)(Aj - (1 - (jt))Aj(t) )ES [1i,jS |S] , i,j=1

= O( t) m A(it)(Aj - (1 - (jt))Aj(t) )ES [1i,jS |S] , i,j=1

= O(

t

)O(

k2 m2

)(

A(t)1m×mA

= O(

t

)O(

k2 m

)

A

2.

+ A(t)1m×mA(t) ),

Therefore,

 :=

ES [AS(t)M(AS

- (I - (St))A(St)

)]

=

O(

k2 m

)

t

A

2.

Similarly,  in (36) is also bounded as . Next, we consider  in (36). In this case, letting

ExS [S(R)((SR)) |S] = N, where N  Rk×k is a matrix whose each entry Ni,j  |i(R)||j(R)|. Further, by Claim 5, each element j(R) is upper-bounded as

|j(R)|  O(t).

with probability at least (1 - (t)). Therefore,
 = m Ni,j Ai(t)A(jt) ES [1i,jS |S] i,j=1

=

mi,ajx|i(R)||j(R)|O(

k2 m2

)

m Ai(t)A(jt)
i,j=1

.

Again, using the result on |(i1R)|, we have

 :=

ES [AS(t)NAS(t)

]

=

m

mi,ajx|(iR)

||j(R)

|O(

k2 m2

)

A(t)

A(t)

=

O(

k2 t2 m

)

A

2.

Combining all the results for , ,  and , we have,

E[(y - A(t)x)(y-A(t)x) ]

=

O(

k2 m

)

A

2

+

O(

k2 m

)

t

A

2

+

O(

k2 m

)

t

A

2

+

O(

k2 t2 m

)

A

2,

=

O(

k2 t2 m

)

A

2.

E ADDITIONAL EXPERIMENTAL RESULTS
We now present some additional results to highlight the features of NOODL. Here, we compare the performance of NOODL (for both dictionary and coefficient recovery) with the state-of-the-art provable techniques for DL (when the coefficients are recovered via a sparse approximation step after DL).
Table 4 summarizes the results of the convergence analysis shown in Fig. 2. Here, we compare the dictionary and coefficient recovery performance of NOODL with other techniques. For Arora15(``biased'') and Arora15(``unbiased''), we report the error in recovered coefficients after the HT step (XHT) and the best error via sparse approximation using Lasso (XLasso) by scanning over 50 values of regularization parameters.
We observe that NOODL exhibits significantly superior performance across the board. Also, we observe that using sparse approximation after dictionary recovery, when the dictionary suffers from a bias, leads to poor coefficient recovery. This highlights the applicability of our approach in realworld machine learning tasks where coefficient recovery is of interest. In fact, it is a testament to the fact that, even in cases where dictionary recovery is the primary goal, making progress on the coefficients is also important for dictionary recovery.
In addition, the coefficient estimation is also online in case of NOODL, while for the state-ofthe-art provable techniques (which only recover the dictionary and incur bias in estimation) need additional sparse approximation step for coefficient recovery. Moreover, these sparse approximation techniques (such as Lasso) are expensive to use in practice, and need significant tuning.

37

Under review as a conference paper at ICLR 2019

Table 4: Final error in recovery of the dictionary and the coefficients by various techniques corresponding to Fig. 2. We report the coefficient estimate after the HT step as per Arora'15 as XHT. Also, we scan across 50 values of the regularization parameter for coefficient estimation using Lasso after learning the dictionary (A), and report the optimal estimation error for the coefficients (XLasso). For k = 100, Arora'15 algorithms do not converge (shown as N/A).

Technique NOODL Arora'15 (unbiased)
Arora'15 (biased)

Recovered Factor
A
X
A XHT XLasso
A XHT XLasso

k = 10 9.44 × 10-11 1.14 × 10-11
0.011 0.078 0.005
0.013 0.077 0.006

k = 20 8.82 × 10-11 1.76 × 10-11
0.027 0.122 0.015
0.031 0.120 0.018

k = 50 9.70 × 10-11 3.58 × 10-11
0.148 0.371 0.0921
0.137 0.308 0.097

k = 100 7.33 × 10-11 4.74 × 10-11
N/A N/A N/A
N/A N/A N/A

F APPENDIX: STANDARD RESULTS

Definition 6 (sub-Gaussian Random variable). Let x  subGaussian(2). Then, for any t > 0, it holds that

Pr[|x| > t]  2 exp

t2 22

.

F.1 CONCENTRATION RESULTS

Lemma 10 (Matrix Bernstein (Tropp, 2015)). Consider a finite sequence Wk  Rn×m of independent, random, centered matrices with dimension n. Assume that each random matrix satisfies E[Wk] = 0 and Wk  R almost surely. Then, for all t  0,

Pr

Wk

t

 (n + m)exp

-t2 /2  2 +Rt/3

,

k

where 2 := max{ E[WkWk ] , E[Wk Wk] }.
kk
Furthermore,

E[

Wk ] 

22

log(n

+

m)

+

1 3

R

log(n

+

m).

k

Lemma 11 (Vector Bernstein (Tropp, 2015)). Consider a finite sequence wk  Rn of independent,
random, zero mean vectors with dimension n. Assume that each random vector satisfies E[wk] = 0 and wk  R almost surely. Then, for all t  0,

Pr

wk

t

 2nexp

-t2 /2  2 +Rt/3

,

k

where 2 := E[ wk 2] . Furthermore,
k

E[

wk ] 

22

log(2n)

+

1 3

R

log(2n).

k

Lemma 12. Chernoff Bound for sub-Gaussian Random Variables Let w be an independent subGaussian random variables with variance parameter 2, then for any t > 0 it holds that

Pr[|w|

>

t]



2exp(-

t2 22

).

Lemma 13 (Sub-Gaussian concentration (Rudelson and Vershynin, 2013)). Let M  Rn×m be a fixed matrix. Let w be a vector of independent, sub-Gaussian random variables with mean zero and variance one. Then, for an absolute constant c,

Pr[ Mx 2 -

M

F

> t]  exp(-

ct2 M

2

).

38

Under review as a conference paper at ICLR 2019

F.2 RESULTS FROM (ARORA ET AL., 2015)

Lemma 14 ((Arora et al., 2015) Lemma 45). Suppose that the distribution of Z satisfies Pr[ Z  L(log(1/))C]  ] for some constant C > 0, then

1. If p = nO(1) then Z(j)  O(L) holds for each j with probability at least(1 - ) and,

2. E[Z1 Z (L)] = n-(1).

In

particular,

if

1 p

1 p

p j=1

Z(j).

p j=1

Z(j)(1

-

1

Z

(L))

is

concentrated

with

probability

(1

-

),

then

so

is

Lemma 15 (Theorem 40 (Arora et al., 2015)). Suppose random vector g(t) correlated with high probability with z for t  [T ] where T  poly(n),

is a and

( , + , t)A satisfies

0 < A  2+ , then for any t  [T ],

E[ z(t+1) - z 2]  (1 - 2 A) z(t) - z + 2At.

In particular, if

z(0) - z



0

and

t



(

)o((1

-

2

)t)

2 0

+

,

where



=

maxt[T ]t,

then

the updates converge to z geometrically with systematic error / in the sense that

E[

z(t+1) - z

2]  (1 - 2 A)t

2 0

+

 /

.

39

