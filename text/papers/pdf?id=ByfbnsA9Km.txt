Under review as a conference paper at ICLR 2019
CROSS-ENTROPY LOSS LEADS TO POOR MARGINS
Anonymous authors Paper under double-blind review
ABSTRACT
Neural networks could misclassify inputs that are slightly different from their training data, which indicates a small margin between their decision boundaries and the training dataset. In this work, we study the binary classification of linearly separable datasets and show that linear classifiers could also have decision boundaries that lie close to their training dataset if cross-entropy loss is used for training. In particular, we show that if the features of the training dataset lie in a low-dimensional affine subspace and the cross-entropy loss is minimized by using a gradient method, the margin between the training points and the decision boundary could be much smaller than the optimal value. This result is contrary to the conclusions of recent related works such as (Soudry et al., 2018), and we identify the reason for this contradiction. In order to improve the margin, we introduce differential training, which is a novel training paradigm that uses a loss function defined on pairs of points from each class. We show that the decision boundary of a linear classifier trained with differential training indeed achieves the maximum margin. The results reveal the use of cross-entropy loss as one of the hidden culprits of adversarial examples and introduces a new direction to make neural networks robust against them.
1 INTRODUCTION
Training neural networks is challenging and involves making several design choices. Among these are the architecture of the network, the training loss function, the optimization algorithm used for training, and their hyperparameters, such as the learning rate and the batch size. Most of these design choices influence the solution obtained by the training procedure and have been studied in detail (Kingma & Ba, 2014; Hardt et al., 2015; He et al., 2016; Wilson et al., 2017; Nar & Sastry, 2018; Smith et al., 2018). Nevertheless, one choice has been mostly taken for granted when the network is trained for a classification task: the training loss function.
Cross-entropy loss function is almost the sole choice for classification tasks in practice. Its prevalent use is backed theoretically by its association with the minimization of the Kullback-Leibler divergence between the empirical distribution of a dataset and the confidence of the classifier for that dataset. Given the particular success of neural networks for classification tasks (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016), there seems to be little motivation to search for alternatives for this loss function, and most of the software developed for neural networks incorporates an efficient implementation for it, thereby facilitating its use.
Recently there has been a line of work analyzing the dynamics of training a linear classifier with the cross-entropy loss function (Soudry et al., 2018; Nacson et al., 2018a;b; Ji & Telgarsky, 2018). They specified the decision boundary that the gradient descent algorithm yields on linearly separable datasets and claimed that this solution achieves the maximum margin.1 However, these claims were observed not to hold in the simple experiments we ran. For example, Figure 1 displays a case where the cross-entropy minimization for a linear classifier leads to a decision boundary which attains an extremely poor margin and is nearly orthogonal to the solution given by the hard-margin support vector machine (SVM).
We set out to understand this discrepancy between the claims of the previous works and our observations on the simple experiments. We can summarize our contributions as follows.
1The term "maximum margin" is used for 2 norm throughout the paper.
1

Under review as a conference paper at ICLR 2019

80

70

y

60

50

40

SVM cross-entropy min.

10 0 10 x 20 30 40

Figure 1: Orange and blue points represent the data from two different classes in R2. Cross-entropy minimization for a linear classifier on the given training points leads to the decision boundary shown with the dotted line, which attains a very poor margin and is almost orthogonal to the solution given by the SVM.

1. We analyze the minimization of the cross-entropy loss for a linear classifier by using only two training points, i.e., only one point from each of the two classes, and we show that the dynamics of the gradient descent algorithm could yield a poor decision boundary, which could be almost orthogonal to the boundary with the maximum margin.
2. We identify the source of discrepancy between our observations and the claims of the recent works as the misleading abbreviation of notation in the previous works. We clarify why the solution obtained with cross-entropy minimization is different from the SVM solution.
3. We show that for linearly separable datasets, if the features of the training points lie in an affine subspace, and if the cross-entropy loss is minimized by a gradient method with no regularization to train a linear classifier, the margin between the decision boundary of the classifier and the training points could be much smaller than the optimal value. We verify that when a neural network is trained with the cross-entropy loss to classify two classes from the CIFAR-10 dataset, the output of the penultimate layer of the network indeed produces points that lie on an affine subspace.
4. We show that if there is no explicit and effective regularization, the weights of the last layer of a neural network could grow to infinity during training with a gradient method. Even though this has been observed in recent works as well, we are the first to point out that this divergence drives the confidence of the neural network to 100% at almost every point in the input space if the network is trained for long. In other words, the confidence depends heavily on the training duration, and its exact value might be of little significance as long as it is above 50%.
5. We introduce the differential training, a novel training paradigm that uses a loss function defined on pairs of points from each class ­ instead of only one point from any class. We show that the decision boundary of a linear classifier trained with differential training indeed produces the SVM solution with the maximum hard margin.

2 CLASSIFICATION OF TWO POINTS

We start with a simple binary classification problem. Given two points x  Rd and -y  Rd from two different classes, we can find a linear classifier by minimizing the cross-entropy loss function

min
wRd ,bR
or equivalently, by solving

- log

1 e-w x-b + 1

- log

ew y-b ew y-b + 1

,

min log(e-w~ x~ + 1) + log(e-w~ y~ + 1) ,
w~Rd+1

(1)

2

Under review as a conference paper at ICLR 2019

where x~ = [x 1] , -y~ = [-y 1] and w~ = [w b] . Unless the two points x and -y are equal, the function (1) does not attain its minimum at a finite value of w~. Consequently, if the gradient descent algorithm is used to minimize (1), the iterate at time k, w~[k], diverges as k increases. The following theorem characterizes the growth rate of w~[k] and its direction in the limit by using a continuous-time approximation to the gradient descent algorithm.
Theorem 1. Given two points x  Rd and -y  Rd, let x~ and -y~ denote [x 1] and [-y 1], respectively. Without loss of generality, assume x  y . If the two points are in different classes and we minimize the cross-entropy loss

min log(1 + e-w~ x~) + log(1 + e-w~ y~)
w~Rd+1
by using the continuous-time approximation to the gradient descent algorithm

dw~ e-w~ x~

e-w~ y~

dt

= x~ 1 + e-w~

x~

+ y~ 1+

e-w~

y~

with the initialization w~(0) = 0 and the learning rate , then

where x =

w~(t) lim = t log(t)

x~ + y~y-xy
xy -x2y

x -xy xy -x2y

1 x

x~

x~ 2, xy = x~ y~ and y = y~ 2.

if xy < x, if xy  x,

(2)

Note that first d coordinates of (2) represent the normal vector of the decision boundary obtained by minimizing the cross-entropy loss (1). This vector is different from x + y, which is the direction of the maximum-margin solution given by the SVM. In fact, the direction in (2) could be almost orthogonal to the SVM solution in certain cases, which implies that the margin between the points and the decision boundary could be much smaller than the optimal value. Corollary 1 describes a subset of these cases.
Corollary 1. Given two points x and -y in Rd, let  denote the angle between the solution given by (2) and the solution given by the SVM, i.e., (x + y). If x y = 1, then

cos2  

4 ,

2

+

y x

1

-

1 x

where x = x 2+1 and y = y 2+1. Consequently, as x / y approaches 0 while maintaining the condition x y = 1, the angle  converges to /2.

Remark 1. Corollary 1 shows that if x and -y have disparate norms, the minimization of the crossentropy loss with gradient descent algorithm could lead to a direction which is almost orthogonal to the maximum-margin solution. It may seem like this problem could be avoided with preprocessing the data so as to normalize the data points. However, this approach will not be effective for neural networks: if we consider an L-layer neural network, w L-1(x), and regard the first L - 1 layers, L-1(·), as a feature mapping, preprocessing a dataset {xi}iI will not produce a normalized set of features {L-1(xi)}iI . Note that we could not normalize {L-1(xi)}iI directly either, since the mapping L-1(·) evolves during training.
Remark 2. Theorem 1 shows that the norm of w keeps growing unboundedly as the training continues. The same behavior will be observed for larger datasets in the next sections as well. Since the "confidence" of the classifier for its prediction at a point x is given by

1 e-w x-b max e-w x-b + 1 , e-w x-b + 1 ,

this unbounded growth of w drives the confidence of the classifier to 100% at every point in the input space, except at the points on the decision boundary, if the algorithm is run for long. Given the lack of effective regularization for neural networks, a similar unbounded growth is expected to be observed in neural network training as well, which is mentioned in (Bartlett et al., 2017). As a result, the confidence of a neural network might be highly correlated with the training duration, and whether a neural network gives 99% or 51% confidence for a prediction might be of little importance as long as it is above 50%. In other words, regarding this confidence value as a measure of similarity between an input and the training dataset from the most-likely class should be reconsidered.

3

Under review as a conference paper at ICLR 2019

3 CLASSIFICATION OF LINEARLY SEPARABLE SETS

In this section, we examine the binary classification of a linearly separable dataset by minimizing the cross-entropy loss function. Recently, this problem has also been studied in (Soudry et al., 2018; Nacson et al., 2018b;a; Ji & Telgarsky, 2018). We restate an edited version of the main theorem of (Soudry et al., 2018), followed by the reason of the edition.
Theorem 2 (Theorem 3 of (Soudry et al., 2018)). Given two sets of points {xi}iI and {-yj}jJ that are linearly separable in Rd, let x~i and -y~j denote [xi 1] and [-yj 1] , respectively, for all i  I, j  J. Then, the iterates of the gradient descent algorithm on the cross-entropy loss function

log(1 + e-w~ x~i ) +
iI
with a sufficiently small step size will behave as

log(1 + e-w~ y~j )
jJ

(3)

w~(t) = w log(t) + (t),

where w is the solution to

w = argmin u 2 s.t. u, x~i  1, u, y~j  1 i  I, j  J,
uRd+1

(4)

and the residual grows at most as verges to the direction of w:

(t) = O(log log(t)). Consequently, the direction of w~ con-

w~(t) w

lim = .

t w~(t)

w

The solution (4) given in Theorem 2 was referred in (Soudry et al., 2018), and consequently in
the other works, as the maximum-margin solution. However, due to the misleading absence of the bias term in the notation, this is incorrect. Given the linearly separable sets of points {xi}iI and {-yj}jJ , the maximum-margin solution given by the SVM solves

minimize
w,b
subject to

w

2 2

w, xi + b  1

i  I,

(P1)

w, -yj + b  -1 j  J.

On the other hand, the solution given by Theorem 2 corresponds to

minimize
w,b
subject to

w

2 2

+

b2

w, xi + b = w~, x~i  1

i  I,

w, -yj + b = w~, -y~j  -1 j  J,

(P2)

where we define w~ = [w b] , x~i = [xi 1] and y~j = [yj - 1] for all i  I, j  J. Even though the sets of constraints for both problems are identical, their objective functions are different, and consequently, the solutions are different. As a result, the decision boundary obtained by crossentropy minimization does not necessarily attain the maximum hard margin. In fact, as the following theorem shows, its margin could be arbitrarily worse than the maximum margin.
Theorem 3. Assume that the points {xi}iI and {-yj}jJ are linearly separable and lie in an affine subspace; that is, there exist a set of orthonormal vectors {rk}kK and a set of scalars {k}kK such that
rk, xi = rk, -yj = k i  I, j  J, k  K.
Let w, · + B = 0 denote the decision boundary obtained by minimizing the cross-entropy loss, i.e. the pair (w, B) solves

min w 2 + b2 s.t. w, xi + b  1, w, -yj + b  -1 i  I, j  J.
w,b

Then the minimization of the cross-entropy loss (3) yields a margin smaller than or equal to

1

1 2

+ B2

kK 2k

where  denotes the optimal hard margin given by the SVM solution.

4

Under review as a conference paper at ICLR 2019

Remark 3. Theorem 3 shows that if the training points lie in an affine subspace, the margin obtained by the cross-entropy minimization will be smaller than the optimal margin value. As the dimension of this affine subspace decreases, the cardinality of the set K increases and the term kK 2k could become much larger than 1/2. Therefore, as the dimension of the subspace containing the training points gets smaller compared to the dimension of the input space, cross-entropy minimization with a gradient method becomes more likely to yield a poor margin. Note that this argument also holds for classifiers of the form w (x) with the fixed feature mapping (·).
The next theorem relaxes the condition of Theorem 3 and allows the training points to be near an affine subspace instead of being exactly on it. Note that the ability to compare the margin obtained by cross-entropy minimization with the optimal value is lost. Nevertheless, it highlights the fact that same set of points could be assigned a different margin by cross-entropy minimization if all of them are shifted away from the origin by the same amount in the same direction. Theorem 4. Assume that the points {xi}iI and {-yj}jJ in Rd are linearly separable and there exist a set of orthonormal vectors {rk}kK and a set of scalars {k}kK such that
rk, xi  k, rk, -yj  k i  I, j  J, k  K. Let w, · + B = 0 denote the decision boundary obtained by minimizing the cross-entropy loss, i.e. the pair (w, B) solves
min w 2 + b2 s.t. w, xi + b  1, w, -yj + b  -1 i  I, j  J.
w,b
Then the minimization of the cross-entropy loss (3) yields a margin smaller than or equal to 1
B2 kK k2
Remark 4. Both Theorem 3 and Theorem 4 consider linearly separable datasets. If the dataset is not linearly separable, (Ji & Telgarsky, 2018) predicts that the normal vector of the decision boundary, w, will have two components, one of which converges to a finite vector and the other diverges. The diverging component still has the potential to drive the decision boundary to a direction with a poor margin. In fact, the margin is expected to be small especially if the points intruding into the opposite class lie in the same subspace as the optimal normal vector for the decision boundary. In this work, we focus on the case of separable datasets as this case provides critical insight into the issues of state-of-the-art neural networks, given they can easily attain zero training error even on randomly generated datasets, which indicates the linear separability of the features obtained at their penultimate layers (Zhang et al., 2017).

4 DIFFERENTIAL TRAINING

In previous sections, we saw that the cross-entropy minimization could lead to poor margins, and the main reason for this was the appearance of the bias term in the objective function of (P2). In order to remove the effect of the bias term, consider the SVM problem (P1) and note that this problem could be equivalently written as

minimize
w
subject to

w

2 2

w, xi + yj  2 i  I, j  J

(P3)

if we only cared about the weight parameter, w. This gives the hint that if use the set of differences {xi + yj : i  I, j  J } instead of the individual sets {xi}iI and {-yj}jJ , the bias term could be excluded from the problem. Indeed, this approach allows obtaining the SVM solution with a similar
loss function, as the following theorem shows.

Theorem 5. Given two sets of points {xi}iI and {-yj}jJ that are linearly separable in Rd, if

we solve

min
wRd

log(1 + e-w (xi+yj))
iI jJ

(5)

by using the gradient descent algorithm with a sufficiently small learning rate, the direction of w

converges to the direction of maximum-margin solution, i.e.

lim w(t) = wSVM ,

t w(t)

wSVM

where wSVM is the solution of (P3).

(6)

5

Under review as a conference paper at ICLR 2019

Proof. Apply Theorem 2 by replacing the sets {xi}iI and {-yj}jJ with {xi + yj}iI,jJ and the empty set, respectively. Then the minimization of the loss function (5) with the gradient descent
algorithm leads to

ww

lim =

t w

w

where w satisfies

w = arg min w 2 such that
w

w, xi + yj  1

i  I, j  J.

Since

wSVM

is

the

solution

of

(P3),

we

obtain

w

=

1 2

wSVM,

and

the

claim

of

the

theorem

holds.

Remark 5. Theorem 5 is stated for the gradient descent algorithm, but the identical statement could

be made for the stochastic gradient method as well by invoking the main theorem of (Nacson et al.,

2018).

Minimization of the cost function (5) yields the weight parameter w^ of the decision boundary. The
bias parameter, b, could be chosen by plotting the histogram of the inner products { w^, xi }iI and { w^, -yj }jJ and fixing a value for ^b such that

w^, xi + ^b  0 i  I, w^, -yj + ^b  0 j  J.

(7a) (7b)

The largest hard margin is achieved by

^b

=

-1 2

min
iI

w^, xi

-

1 2

max
jJ

w^, -yj

.

(8)

However, by choosing a larger or smaller value for ^b, it is possible to make a tradeoff between the Type-I and Type-II errors.
The cost function (5) includes a loss defined on every pair of data points from the two classes. This cost function can be considered as the cross-entropy loss on a new dataset which contains |I| × |J| points. There are two aspects of this fact:

1. When standard loss functions are used for classification tasks, we need to oversample or undersample either of the classes if the training dataset contains different number of points from different classes. This problem does not arise when we use the cost function (5).
2. Number of pairs in the new dataset, |I| × |J|, will usually be much larger than the original dataset, which contains |I| + |J| points. Therefore, the minimization of (5) might appear more expensive than the minimization of the standard cross-entropy loss computationally. However, if the points in different classes are well separated and the stochastic gradient method is used to minimize (5), the algorithm achieves zero training error after using only a few pairs, which is formalized in Theorem 6. Further computation is needed only to improve the margin of the classifier. In addition, in our experiments to train a neural network to classify two classes from the CIFAR-10 dataset, only a few percent of |I| × |J| points were observed to be sufficient to reach a high accuracy on the training dataset.
Theorem 6. Given two sets of points {xi}iI and {-yj}jJ that are linearly separable in Rd, assume the cost function (5) is minimized with the stochastic gradient method. Define
Rx = max{ xi - xi : i, i  I}, Ry = max{ yj - yj : j, j  J }
and let  denote the hard margin that would be obtained with the SVM:

2 = maxuRd miniI,jJ xi + yj , u/ u .
If 2  5 max(Rx, Ry), then the stochastic gradient algorithm produces a weight parameter, w^, only in one iteration which satisfies the inequalities (7a)-(7b) along with the bias, ^b, given by (8).

6

y

Under review as a conference paper at ICLR 2019
5 NUMERICAL EXPERIMENTS
In this section, we present numerical experiments supporting our claims. In Figure 2, we show the decision boundaries of two linear classifiers, where one of them is trained by minimizing the cross-entropy loss, and the other through differential training. Unlike the example shown in Figure 1, here the data does not exactly lie in an affine subspace, and there are multiple samples from each class. In particular, one of the classes is composed of 10 samples from a normal distribution with mean (2, 12) and variance 25, and the other class is composed of 10 samples from a normal distribution with mean (40, 50) and variance 25. As can be seen from the figure, the cross-entropy minimization yields a margin that is smaller than differential training, even when the training dataset is not low-dimensional, which is predicted by Theorem 4.
60
40
20
differential training 0 cross-entropy min.
0 20 40 60 x
Figure 2: Classification boundaries obtained using differential training and cross-entropy minimization. The margin recovered by cross-entropy minimization is worse than differential training. even when the training dataset is not low-dimensional.
Next, we empirically evaluate if the features fed into the soft-max layer in a neural network indeed lie in a low-dimensional affine subspace. For this purpose, we train a convolutional neural network architecture to classify horses and planes from the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). Figure 3 shows the cumulative variance explained for the features that feed into the soft-max layer as a function of the number of principle components used. We observe that the features, which are the outputs of the penultimate layer of the network, lie in a low-dimensional affine subspace, and this holds for a variety of training modalities for the network. This observation is relevant to Remark 3. The dimension of the subspace containing the training points is at most 20, which is much smaller than the dimension of the feature space, 84. Consequently, cross-entropy minimization with a gradient method is expected to yield a poor margin on these features.
6 DISCUSSION
We compare our results with related works and discuss their implications for the following subjects. Adversarial examples. State-of-the-art neural networks have been observed to misclassify inputs that are slightly different from their training data, which indicates a small margin between their decision boundaries and the training dataset (Szegedy et al., 2013; Goodfellow et al., 2015; MoosaviDezfooli et al., 2017; Fawzi et al., 2017). Our results reveal that the combination of gradient methods, cross-entropy loss function and the low-dimensionality of the training dataset (at least in some domain) has a responsibility for this problem. Note that SVM with the radial basis function was shown to be robust against adversarial examples, and this was attributed to the high nonlinearity of the radial basis function in (Goodfellow et al., 2015). Given that the SVM uses neither the cross entropy loss function nor the gradient descent algorithm for training, we argue that the robustness of SVM is no surprise ­ independent of its nonlinearity. Lastly, effectiveness of differential training for neural networks against adversarial examples is our ongoing work.
7

Under review as a conference paper at ICLR 2019

variance explained

1.0

0.8

0.6

0.4 Adam+BatchNorm

0.2

Adam momentum

0.0 0 numbe2r0of princi4p0al compon6e0nts used80

Figure 3: The activations feeding into the soft-max layer could be considered as the features for a linear classifier. Plot shows the cumulative variance explained for these features as a function of the number of principle components used. Almost all the variance in the features is captured by the first 20 principle components out of 84, which shows that the input to the soft-max layer resides predominantly in a low-dimensional subspace.

Low-dimensionality of the training dataset. As stated in Remark 3, as the dimension of the affine subspace containing the training dataset gets very small compared to the dimension of the input space, the training algorithm will become more likely to yield a small margin for the classifier. This observation confirms the results of (Marzi et al., 2018), which showed that if the set of training data is projected onto a low-dimensional subspace before feeding into a neural network, the performance of the network against adversarial examples is improved ­ since projecting the inputs onto a low-dimensional domain corresponds to decreasing the dimension of the input space. Even though this method is effective, it requires the knowledge of the domain in which the training points are low-dimensional. Because this knowledge will not always be available, finding alternative training algorithms and loss functions that are suited for low-dimensional data is still an important direction for future research.
Robust optimization. Using robust optimization techniques to train neural networks has been shown to be effective against adversarial examples (Madry et al., 2018; Athalye et al., 2018). Note that these techniques could be considered as inflating the training points by a presumed amount and training the classifier with these inflated points. Consequently, as long as the cross-entropy loss is involved, the decision boundaries of the neural network will still be in the vicinity of the inflated points. Therefore, even though the classifier is robust against the disturbances of the presumed magnitude, the margin of the classifier could still be much smaller than what it could potentially be.
Differential training. We introduced differential training, which allows the feature mapping to remain trainable while ensuring a large margin between different classes of points. Therefore, this method combines the benefits of neural networks with those of support vector machines. Even though moving from 2N training points to N 2 seems prohibitive, it points out that a true classification should in fact be able to differentiate between the pairs that are hardest to differentiate, and this search will necessarily require an N 2 term. Some heuristic methods are likely to be effective, such as considering only a smaller subset of points closer to the boundary and updating this set of points as needed during training. If a neural network is trained with this procedure, the network will be forced to find features that are able to tell apart between the hardest pairs.
Nonseparable data. What happens when the training data is not linearly separable is an open direction for future work. However, as stated in Remark 4, this case is not expected to arise for the state-of-the-art networks, since they have been shown to achieve zero training error even on randomly generated datasets (Zhang et al., 2017), which implies that the features represented by the output of their penultimate layer eventually become linearly separable.
REFERENCES
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
8

Under review as a conference paper at ICLR 2019
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6240­6249, 2017.
A. Fawzi, S. Moosavi-Dezfooli, and P. Frossard. The robustness of deep networks: A geometrical perspective. IEEE Signal Processing Magazine, 34(6):50­62, Nov 2017.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. CoRR, abs/1803.07300, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097­1105, 2012.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
Z. Marzi, S. Gopalakrishnan, U. Madhow, and R. Pedarsani. Sparsity-based Defense against Adversarial Attacks on Linear Classifiers. ArXiv e-prints, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 86­94, 2017.
M. Shpigel Nacson, J. Lee, S. Gunasekar, P. H. P. Savarese, N. Srebro, and D. Soudry. Convergence of Gradient Descent on Separable Data. ArXiv e-prints, 2018a.
M. Shpigel Nacson, N. Srebro, and D. Soudry. Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate. ArXiv e-prints, 2018b.
Kamil Nar and S. Shankar Sastry. Step size matters in deep learning. CoRR, abs/1805.08890, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don't decay the learning rate, increase the batch size. In International Conference on Learning Representations, 2018.
D. Soudry, E. Hoffer, M. Shpigel Nacson, S. Gunasekar, and N. Srebro. The Implicit Bias of Gradient Descent on Separable Data. ArXiv e-prints, 2018.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In International Conference on Learning Representations, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4148­4158, 2017.
9

Under review as a conference paper at ICLR 2019 Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.
10

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 1

Theorem 1 could be proved by using Theorem 2, but we provide an independent proof here. Gradient descent algorithm with learning rate  on the cross-entropy loss (1) yields

dw~ e-w~ x~

e-w~ y~

dt

= x~ 1 + e-w~

x~

+

y~ 1

+

e-w~

.
y~

If w~(0) = 0, then w~(t) = p(t)x~ + q(t)y~ for all t  0, where

e-p x~ 2-q x~,y~ p =  1 + e-p x~ 2-q x~,y~ ,

e-q y~ 2-p x~,y~ q =  1 + e-q y~ 2-p x~,y~ .

Define

 = p x~ 2 + q x~, y~ ,  = q y~ 2 + p x~, y~ , a =  x~ 2 = x, c =  y~ 2 = y, b =  x~, y~ = xy.

Then we can write

e-

e-



=

a 1

+

e-

+

b 1

+

e-

,



=

c 1

e- + e-

e- + b 1 + e- .

Finally, define z = e and v = e so that

z z+1

z =

a+b

,

z+1

v+1

v v+1

v =

c+b

.

v+1

z+1

Without loss of generality, assume c  a. Before we proceed, note that

d dt

z v

c-b z =
z+1v

a-b - z+1 c-b v+1

,

d dt

z+1 v+1

z = (v + 1)2

v

+

1 a

-

v

b

-

z+1 z

v z +1c-b zv+1

Let

u

and

w

denote

z v

and

z+1 v+1

,

respectively.

Then,

u < 0 u > 0 w < 0 w > 0

a-b if w > c - b

a-b if w < c - b

a

if

+ b u < cw + b w

a

if + b u > cw + b

w

Lemma 1. If b = 0, then

w~(t) 1

1

lim = x~ + y~.

t log(t) x~

y~

.

Proof. Note that

d(z + log(z)) dt

=a

=

z(t) - z0 + log(z(t)/z0) = at,

d(v + log(v)) dt

=c

=

v(t) - v0 + log(v(t)/v0) = ct.

11

Under review as a conference paper at ICLR 2019

Then,

(t) log(z(t))

log(v(t))

(t)

lim = lim

= 1 = lim

= lim

,

t log(t) t log(t)

t log(t) t log(t)

and

w~(t) x~

y~

lim = t log(t)

x~ 2 +

y~ 2 .

Lemma 2. If b < 0, then there exists t0  (0, ) such that

-b z + 1 a c  v + 1  -b t  t0.

Proof.

Note

that

-b c



a-b c-b



a -b

because

b

<

0.

First

assume

z0 +1 v0 +1



a -b

.

Then,

z



0

and

v v =

v+1 c+b



v

c - b2



v0

ac - b2 ,

v+1

z+1 v+1

a v0 + 1 a

which implies that

z v

+1 +1



(z0

+ 1)

v0

ac - b2

-1

t+1

v0 + 1 a

as long as

z+1 v+1



a -b

,

and

this

can

be

satisfied

only

for

a

finite

time.

Now assume

z0 +1 v0 +1



-b c

.

Then, v  0 and

z z =

z+1 a+b



z0

ac - b2 ,

z+1

v + 1 z0 + 1 c

which implies

z+1  v+1

z0

ac - b2 t+1

z0 + 1 c

(v0 + 1)-1

as

long

as

z+1 v+1



-b c

,

and

this

can

be

satisfied

only

for

a

finite

time

as

well.

Lemma 3. If b < 0, then

ac - b2

0  z 

,

c

where t0 is given by Lemma 2.

ac - b2 0  v 
a

t  t0,

Proof.

z z =

z+1 a+b



z

ac - b2  ac - b2

z+1

v+1 z+1 c

c

v v =

v+1 c+b



v

ac - b2  ac - b2

v+1

z+1 v+1 a

a

Lemma 4. If b < 0, then

log(z)

log(v)

lim = lim = 1

t log t t log t

Proof. From Lemma 3,

v  0



v+1 c+b

 0,

z+1

and

v



ac

- a

b2 t

+

v0.

Combining these two inequalities, we have

b c+
a

(ac - b2)t + v0 + 1

1 0 z+1



z

+1



-b ac

[(ac

-

b2)t

+

v0

+ 1]

12

Under review as a conference paper at ICLR 2019

As a result,

(-b)(ac - b2) ac t + z1



z(t)



ac

- b2 c t + z2

t  t0

=

log(z) lim = 1. t log(t)

By using Lemma 2,

log(v)

log(-b/c)  log(z + 1) - log(v + 1)  log(-a/b) = lim

= 1.

t log(t)

Lemma 5. If b < 0, then

lim
t

w~(t) log(t)

=

c-b  ac - b2 x~

a-b +  ac - b2 y~

=

y - xy xy - x2y

x~

+

x - xy xy - x2y

y~

Proof. Solving the set of equations

a

pb

q

1 = lim

= lim

+ lim

,

log(t)  log(t)  log(t)

c

qb

p

1 = lim

= lim

+ lim

,

log(t)  log(t)  log(t)

we obtain

p c-b

q a-b

lim
t

log(t)

=

 ac

-

b2 ,

lim
t

log(t)

=

 ac

-

b2

Lemma 6. If b > 0, then

z

z+1

0 if a  b

lim = lim

=

t v t v + 1

a-b c-b

if a > b

Proof. Note that z  a/2 and v  c/2; therefore,

z+1

z

lim = lim

=

lim u = lim w

t v + 1 t v

t

t

if either side exists. Remember that

w < 0  u < cw2 + bw =: f (w). a + bw

We can compute

2acw + bcw2 + ab f (w) = b2w2 + 2abw + a2 .

The function f is strictly increasing and convex for w > 0. We have

f (0) = 0,

a-b a-b f c-b = c-b.
Therefore, when b  a, the only fixed point of f over [0, ) is the origin, and when a > b, 0 and (a - b)/(c - b) are the only fixed points of f over [0, ).

Figure 4 shows the curves over which u = 0 and w = 0. Since limt u = limt w, the only points (u, w) can converge to are the fixed points of f . Remember that

c-b u = u
z+1

a-b c-b

-

w

,

so when a > b, the origin (0, 0) is unstable in the sense of Lyapunov, and (u, w) cannot converge to it. Otherwise, (0, 0) is the only fixed point, and it is stable. As a result,

z

z+1

0 if a  b

lim = lim

=

t v t v + 1

a-b c-b

if a > b

13

Under review as a conference paper at ICLR 2019
u u = f (w)
a-b c-b

a-b w
c-b
Figure 4: Stationary points of function f .

Lemma 7. If a > b > 0, then

lim
t

w~(t) log(t)

=

c-b  ac - b2 x~

a-b +  ac - b2 y~

=

y - xy xy - x2y

x~

+

x - xy xy - x2y

y~.

Proof. From Lemma 6,

z

z z+1

a - b ac - b2

lim = lim z = lim

a+b

=a+b

=

,

t t t

t z + 1

v+1

c-b c-b

Consequently,

v ac - b2

lim = t t

a-b .

log(z)

log(v)

lim = lim = 1

t log(t) t log(t)

which gives the same solution as Lemma 5:

p c-b

q a-b

lim
t

log(t)

=

 ac

-

b2 ,

lim
t

log(t)

=

 ac

-

b2 .

Lemma 8. If b  a, then

w~(t) 1 lim = x~ t log(t) x~

Proof.

z log(z)

lim = a, lim

= 1,

t t

t log(t)

lim v = lim v =  t t t

log(v)

v

1 v+1

ct bt b

lim = lim t = lim

c+b

t = lim

+ lim

=

t log(t) t v t v + 1

z+1

t v + 1 t z + 1 a

p1

q

px + qy 1

lim = t log(t)

x~ 2 ,

lim = 0 = lim

t log(t)

t

log(t)

=

x~ 2 x~

Proof of Theorem 1. Lemma 1, Lemma 5, Lemma 7 and Lemma 8 prove all cases of Theorem 1.

14

Under review as a conference paper at ICLR 2019

B PROOF OF COROLLARY 1

Since x y = 1, we have xy = x~ y~ = x y - 1 = 0 < a. Then the normal vector of the decision

boundary

is

proportional

to

1 x

x

+

1 y

y.

For

the

angle

between

this

vector

and

the

solution

of

the

SVM, we can write

cos  =

1 x

x

+

1 y

y,

x

+

y

1 x

x

+

1 y

y

x+y

=

2

1 x

x

+

1 y

y

. x + y

We can obtain a lower bound for square of the denominator as

11 x+ y
x y

2
(x + y) 

2

+

y x

-

y x2

+

1 x

+

1 y

+

x y

-

x y2

 2 + y x

1- 1 x

.

As a result,

cos2  

4 .

2

+

y x

1

-

1 x

C PROOF OF THEOREM 3

Let wSVM, · +bSVM = 0 denote the hyperplane obtained as the solution of SVM, i.e., (wSVM, bSVM) is the solution to the problem

min w 2 s.t. w, xi + b  1, w, -yj + b  -1 i  I, j  J.
w,b

Assume that w = u +

m k=1

k rk ,

where

u



Rd

and

u, rk

= 0 for all k  K.

The Lagrangian of the problem (4) can be written as

1 w 2 + 1 b2 + 22

iI µi(1 - w, xi - b) +

jJ j(1 - w, yj + b),

where µi  0 for all i  I and j  0 for all j  J. KKT conditions for the optimality of w and B requires that

w = iI µixi + jJ j yj ,

B= and consequently, for each k  K,

iI µi -

jJ j ,

w, rk = iI µi xi, rk - jJ j -yj , rk = iI kµi - jJ kj = Bk.

Then, we can write w as Remember, by definition,

w = u + kK Bkrk.

wSVM = arg min w 2 s.t. w, xi + yj  2 i  I, j  J.

Since the vector u also satisfies u, xi + yj = w, xi + yj  2 for all i  I, j  J, we have

u



wSVM

=

1 

.

As

a

result,

the

margin

obtained

by

minimizing

the

cross-entropy

loss

is

1 =
w

1 u 2 + Bkrk 2

1

1 2

+ B2

. k2

15

Under review as a conference paper at ICLR 2019

D PROOF OF THEOREM 4

If B < 0, we could consider the hyperplane w, · - B = 0 for the points {-xi}iI and {yj}jJ , which would have the identical margin due to symmetry. Therefore, without loss of generality, assume B  0. As in the proof of Theorem 3, KKT conditions for the optimality of w and B requires
w = iI µixi + jJ j yj , B = iI µi - jJ j
where µi  0 and j  0 for all i  I, j  J. Note that for each k  K,

w, rk =

iI µi xi, rk - jJ j -yj , rk

= Bk + iI µi( xi, rk - k) - jJ j ( -yj , rk - k)  Bk.

Since {rk}kK is an orthonormal set of vectors,

w 2

kK w, rk 2 

kK B22k.

The result follows from the fact that w -1 is an upper bound on the margin.

E PROOF OF THEOREM 6

In order to achieve zero training error in one iteration of the stochastic gradient algorithm, it is sufficient to have

mini I xi , xi + yj > maxj J -yj , xi + yj i  I, j  J,

or equivalently,

xi + yj , xi + yj > 0 i, i  I, j, j  J.

By definition of the margin, there exists a vector wSVM  Rd with unit norm which satisfies

(9)

2 = miniI,jJ xi + yj , wSVM .

Note that wSVM is orthogonal to the decision boundary given by the SVM. Then we can write every xi + yj as
xi + yj = 2wSVM + ix + jy,
where ix, jy  Rd and ix  Rx and jy  Ry. Then, condition (9) is satisfied if
2wSVM + ix + jy, 2wSVM + ix + jy > 0 i, i  I, j, j  J,

or equivalently if 42 + 2 wSVM, ix + jy + ix + jy + ix + jy, ix + jy > 0 i, i  I, j, j  J. (10)

If

we

choose



>

5 2

max(Rx, Ry),

we

have

42 - 2(2Rx + 2Ry) - (Rx + Ry)2 > 0,

which guarantees (10) and completes the proof.

16

