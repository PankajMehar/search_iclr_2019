Under review as a conference paper at ICLR 2019
PERCEPTION-AWARE POINT-BASED VALUE ITERATION FOR PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES
Anonymous authors Paper under double-blind review
ABSTRACT
Partially observable Markov decision processes (POMDPs) are a widely-used framework to model decision-making with uncertainty about the environment and under stochastic outcome. In conventional POMDP models, the observations that the agent receives originate from fixed known distribution. However, in a variety of real-world scenarios the agent has an active role in its perception by selecting which observations to receive. Due to combinatorial nature of such selection process, it is computationally intractable to integrate the perception decision with the planning decision. To prevent such expansion of the action space, we propose a greedy strategy for observation selection. We develop a novel point-based value iteration algorithm that incorporates the greedy strategy to find near-optimal selection decision for sampled belief points. This in turn enables the solver to efficiently approximate the reachable subspace of belief simplex by essentially separating computations related to perception from planning. Lastly, we implement the proposed solver and demonstrate its performance and computational advantage in a range of robotic scenarios where the robot simultaneously performs active perception and planning.
1 INTRODUCTION
In the era of information explosion it is crucial to develop decision-making platforms that are able to judiciously extract useful information to accomplish a defined task. The importance of mining useful data appears in many applications including artificial intelligence, robotics, networked systems and Internet of things. Generally in these applications, a decision-maker, called an agent, must exploit the available information to compute an optimal strategy toward a given objective.
Partially observable Markov decision processes (POMDPs) provide a framework to model sequential decision-making with partial perception of the environment and under stochastic outcomes. The flexibility of POMDPs and their variants in modeling real-world problems has led to extensive research on developing efficient algorithms for finding near-optimal policies. Nevertheless, the majority of previous work on POMDPs either deal with sole perception or sole planning.
While independent treatment of perception and planning deteriorates performance, an integrated approach usually becomes computationally intractable. Thereupon, one must establish a trade-off between optimality and tractability when determining how much perception and planning rely on each other.
In this work, we consider joint perception and planning in POMDPs. More specifically, we consider an agent that decides about two sets of actions; perception actions and planning actions. The perception actions, such as employing a sensor, only affect the belief of the agent regarding the state of the environment. The planning actions, such as choosing navigation direction, are the ones that affect the transition of the environment from one state to another. At each time step, due to power, processing capability, and cost constraints, the agent can pick a subset of available observations along a planning action. We exploit near-optimal sensor selection schemes to partially decouple computing perception and planning policies while considering their mutual effect on the value of the overall policy.
1

Under review as a conference paper at ICLR 2019
1.1 RELATED WORK
Exact POMDP solvers optimize the value function over all reachable belief points. However, finding exact solution to POMDPs is PSPACE-complete Papadimitriou & Tsitsiklis (1987) which deems solving even small POMDPs computationally intractable. This has led to extensive search for nearoptimal algorithms. A common technique is to sample a finite set of belief points that approximate the reachable subspace of belief and apply value iteration over this set, e.g., Sondik (1978); Cheng (1988); Lovejoy (1991); Zhang & Zhang (2001); Spaan & Vlassis (2005); Pineau et al. (2006). Pineau et al. Pineau et al. (2006) proved that the errors due to belief sampling is bounded where the bound depends on the density of the belief set. A well-established offline POMDP solver is SARSOP Kurniawati et al. (2008). SARSOP, similar to HSVI Smith & Simmons (2012), aims to minimize the gap between the lower bound and upper bound on the value function by guiding the sampling toward the belief points that are reachable under union of optimal policies. In this paper, we show that the proposed near-optimal observation selection scheme leads to belief points that are on expectation close to the belief points from optimal set of observations, and hence value loss is small.
An instance of active perception is dynamic sensor selection. Kreucher et al. Kreucher et al. (2005) proposes a reinforcement learning approach that uses Re`nyi divergence to compute utility of sensing actions. Joshi & Boyd (2009) formulated a single step sensor selection problem as semi-definite programming, however, it lacks theoretical guarantee. In Kalman filtering setting, Shamaiah et al. (2010) developed a greedy selection scheme with near-optimal guarantee to minimize log-determinant of the error covariance matrix of estimated state. Some prior work such as Spaan (2008); Spaan & Lima (2009); Natarajan et al. (2015) model active perception as a POMDP. However, the most relevant work to ours are that of Araya et al. (2010); Spaan et al. (2015); Satsangi et al. (2018). Araya et al. (2010) proposed POMDP framework where the reward depends on entropy of the belief. Spaan et al. (2015) introduced POMDP-IR where the reward depends on accurate prediction about the state. Satsangi et al. Satsangi et al. (2018) established an equivalence property between POMDP and POMDP-IR. Furthermore, they employed the submodularity of value function, under some conditions, to use greedy scheme for sensor selection. The main difference of our work is that we consider active perception as a means to accomplishing the original task while in these work, the active perception is the task itself and hence the POMDP rewards are metrics to capture perception quality.
The problem of selecting an optimal set of sensors from a ground set under cardinality constraint is NP-hard Williams & Young (2007). This hardness result has motivated design of greedy algorithms since they make polynomial oracle calls to the objective function. Additionally, if the objective function is monotone non-decreasing and submodular, Nemhauser et al. Nemhauser et al. (1978) showed that a greedy selection achieves (1 - 1/e) approximation factor. Mirzasoleiman et al. (2015) and Hashemi et al. (2018) developed randomized greedy schemes that accelerate the selection process for monotone submodular and weak-submodular objective functions, respectively. Krause & Guestrin (2007); Krause & Golovin (2014) have introduced different submodular information-theoretic objectives for greedy selection and have studied the theoretical guarantees of their maximization under different constraints. Here, we use the entropy of belief to capture the level of uncertainty in state and aim to select a subset of sensors that leads to maximum expected reduction in entropy. We employ the monotonicity and submodularity of proposed objective to establish near-optimal approximation factor.
1.2 CONTRIBUTIONS
A summary of our contributions are as follows:
· Formulating the active perception problem for POMDPs: We introduce a new mathematical definition of POMDPs, called AP2-POMDP, that captures active perception as well as planning. The objective is to find deterministic belief-based policies for perception and planning such that the expected discounted cumulative reward is maximized.
· Developing a perception-aware point-based value iteration algorithm: To solve AP2POMDP, we develop a novel point-based method that approximates the value function using a finite set of belief points. Each belief point is associated with a perception action and a planning action. We use the near-optimal guarantees for greedy maximization of
2

Under review as a conference paper at ICLR 2019

monotone submodular functions to compute the perception action while the planning action is the result of Bellman optimality equation. We further prove that greedy perception action leads to an expected reward that is, on expectation, close to that of optimal perception action.

2 PROBLEM FORMULATION

In this section, we start by giving an overview of related concepts and definitions and then state the problem.

2.1 PRELIMINARIES
The standard POMDP definition models does not capture the actions related to perception. We present a different definition which we call AP2-POMDP as it models active perception actions as well as original planning actions. The active perception actions determine which subset of sensors (observations) the agent should receive. We restrict the set of states, actions, and observations to be discrete and finite.

2.1.1 POMDP WITH PERCEPTION ACTION
We formally define an AP2-POMDP below. Definition 1. An AP2-POMDP is a tuple P = (S, A, k, T, , O, R, ), where

· S is the finite set of states.
· A = Apl × Apr is the finite set of actions with Apl being the set of planning actions and Apr being the set of perception actions. Apr = {apr  {0, 1}n| |apr|0  k} constructs an n-dimensional lattice. Each component of an action apr  Apr determines whether the corresponding sensor is selected.
· k is the maximum number of sensor to be selected.
· T : S × Apl × S  [0, 1] is the probabilistic transition function.
·  = 1 × 2 × . . . × n is the partitioned set of observations, where each i corresponds to the set of measurements observable by sensor i.
· O : S × A ×   [0, 1] is the probabilistic observation function.
· R : S × Apl  R is the reward function, and
·   [0, 1] is the discount factor.

At each time step, the environment is in some state s  S. The agent takes an action apl  Apl that causes a transition to a state s  S with probability P r(s |s, apl) = T (s, apl, s ). At the same time step, the agent also picks k sensors by apr  Apr. Then it receives an observation    with probability P r(|s , apl, apr) = O(s , apl, apr, ), and a scalar reward R(s, apl).
Assumption 1. We assume that the observations from sensors are mutually independent given the current state and the previous action, i.e., I1, I2  {1, 2, . . . , n}, I1  I2 =  : P r( i1I1 i1 , i2I2 i2 |s, apl) = P r( i1I1 i1 |s, apl)P r( i2I2 i2 |s, apl).

Let (apr) = {i|apr(i) = 1} to denote the subset of sensors that are selected by apr. If Assumption 1 holds, then:



P r |s , apl, apr = P r 

i|s , apl, apr =

Oi(s , apl, i),

i(apr )

i(apr )

(1)

where P r(i|s , apl) = Oi(s , apl, i).

3

Under review as a conference paper at ICLR 2019

The belief of the agent at each time step, denoted by bt is the posterior probability distribution of states given the history of previous actions and observations, i.e., ht = (a0, 1, a1, . . . , at-1, t). A well-known fact is that due to Markovian property, a sufficient statistics to represent history of actions and observations is belief A° stro¨m (1965); Smallwood & Sondik (1973). Given the initial
belief b0, the following update equation holds between previous belief b and the belief bba, after taking action a = (apl, apr) and receiving observation :

bba,(s ) = P r

|s , apl, apr s P r(s |s, apl)b(s) P r (|apl, apr)

=

s

i(apr) Oi(s , apl, i) i(apr) Oi(s , apl, i)

s

T (s, apl, s )b(s) s T (s, apl, s )b(s)

.

(2)

The goal is to learn a deterministic policy to maximize E[

 t=0

tR(st

,

atpl)|b0].

A deterministic

policy is a mapping from belief to actions  : B  A, where B is the set of belief states. Note that

B constructs a (|S| - 1)-dimensional probability simplex.

The POMDP solvers apply value iteration Sondik (1978), a dynamic programming technique, to find the optimal policy. Let V be a value function that maps beliefs to values in R. The following recursive expression holds for V :

Vt(b)

=

max
a

b(s)R(s, a) +  P r(|b, a)Vt-1(bba,) .

sS



(3)

The value iteration converges to the optimal value function V  which satisfies the Bellman's opti-

mality equation Bellman (1957). Once the optimal value function is learned, an optimal policy can

be derived. An important outcome of (3) is that at any horizon, the value function is piecewise-linear

and convex (PWLC) Smallwood & Sondik (1973) and hence, can be represented by a finite set of

hyperplanes. Each hyperplane is associated with an action. Let 's to denote the corresponding

vectors of the hyperplanes and let t to be the set of  vectors at horizon t. Then,

Vt(b) = max .b.
t

(4)

This fact has motivated approximate point based solvers that try to approximate the value function by updating the hyperplanes over a finite set of belief points.

2.1.2 SUBMODULARITY
Since the proposed algorithm is founded upon the theoretical results from the field of submodular optimization, here, we overview the necessary definitions. Let X to denote a ground set and f a set function that maps an input set to a real number. Definition 2. Set function f : 2X  R is monotone nondecreasing if f (T1)  f (T2) for all T1  T2  X . Definition 3. Set function f : 2X  R is submodular if
f (T1  {i}) - f (T1)  f (T2  {i}) - f (T2)
for all subsets T1  T2  X and i  X \T2. The term fi(T1) = f (T1  {i}) - f (T1) is the marginal value of adding element i to set T1.
Monotonicity states that adding elements to a set increases the function value while submodularity refers to diminishing returns property.

2.2 PROBLEM DEFINITION

Having stated the required background, next, we state the problem.

Problem 1. Consider a AP2-POMDP P = (S, A, k, T, , O, R, ) and an initial belief b0. We aim to learn a policy (b) = (apl, apr) such that the expected discounted cumulative reward is

maximized, i.e,



 = argmax E[


tR(st, (bt))|b0].

t=0

(5)

4

Under review as a conference paper at ICLR 2019

1.5
1
0.5
0 1
0.5

00

0.5

(a) Entropy

1

1

0.5

0 0
0.5

10

1 0.5

(b) Level sets of entropy

Figure 1: Entropy of belief for a 3-state POMDP.

It is worth noting that the objective function does not explicitly depend on perception actions. However, the belief and hence the states depend on perception actions and as a result, so does the reward.

3 ACTIVE PERCEPTION WITH GREEDY SCHEME

For variety of performance metrics, finding an optimal subset of sensors poses a computationally

challenging combinatorial optimization problem that is NP-hard. Augmenting POMDP planning

actions with

n k

active perception actions results in a combinatorial expansion of the action space.

Thereupon, it is infeasible to directly apply existing POMDP solvers to Problem 1. Instead of con-

catenating both sets of actions and treating them similarly, we propose a greedy strategy for selecting

perception actions that aims to pick the sensors that result in minimal uncertainty about the state.

The key enabling factor is that the perception actions does not affect the transition, consequently,

we can decompose the single-step belief update in (2) into two steps:

~bbapl (s ) = T (s, a, s )~b(s),
s

(6a)

b~bapr,(s ) =

s

i(apr) Oi(s , apl, i)~b(s ) i(apr) Oi(s , apl, i)~b(s

)

.

(6b)

This in turn implies that after a transition is made, the agent should pick a subset of observations that lead to minimal uncertainty in b~bapr,.

To quantify uncertainty in state, we use Shannon entropy of the belief. For a discrete random variable x, the entropy is defined as H(x) = - i p(xi) log p(xi). An important property of entropy is its strict concavity on the simplex of belief points, denoted by B Cover & Thomas (2012). Further, the entropy is zero at the vertices of B and achieves its maximum, log |S|, at the center of B that corresponds to uniform distribution, i.e., when the uncertainty about the state is the highest. Figure 1 demonstrates the entropy and its level sets for |S| = 3. Since the observation values are
unknown before selecting the sensors, we optimize conditional entropy that yields the expected value
of entropy over all possible observations. For discrete random variables x and y, conditional entropy is defined as H(x|y) = Ey[H(x|y)] = i p(yi)H(x|yi). Subsequently, with some algebraic manipulation, it can be shown that the conditional entropy of state given current belief with respect to apr is:

H(s|b, apr) =

...

b(s) Oij (s, apl, ij )

i1 i1

ik ik sS

ij (apr )

log

b(s) ij(apr) Oij (s, apl, ij ) s S b(s ) ij (apr) Oij (s , apl, ij )

,

(7)

where (apr) = {i1, i2, . . . , ik}. It is worth mentioning that b is the current distribution of s and is explicitly written only for the purpose of better clarity, otherwise, H(s|b, apr) = H(s|apr).

To minimize entropy, we define the objective function as the following set function:

f () = H(s|~bbapl ) - H(s|~babpl , i)
i

(8)

5

Under review as a conference paper at ICLR 2019

and the optimization problem as:

apr = arg max f ((apr)).
apr Apr

(9)

We propose a greedy algorithm, outlined in Algorithm 1 to find a near-optimal, yet efficient solution to (9). The algorithm takes as input the agent's belief and planning action. Then it iteratively adds elements from the ground set (set of all sensors) whose marginal gain with respect to f is maximal and terminates when k observations are selected.

Algorithm 1 Greedy policy for perception action
1: Input: AP2-POMDP P = (S, A, k, T, , O, R, ), Current belief b, Planning action apl. 2: Output: Perception action apr. 3: Initialize X = {1, 2, . . . , n},  = . 4: for l = 1, . . . , k do 5: j = argmaxjX \ -H(s|~babpl , i{j} i) 6:     {j}
7: end for 8: return apr corresponding to .

Next, we derive a theoretical guarantee for the performance of the proposed greedy algorithm. The following lemma states the required properties to prove the theorem. The proof of the lemma follows from monotonicity and submodularity of conditional entropy Ko et al. (1995). See the appendix for the complete proof.
Lemma 1. Let  = {1, 2, . . . , n} to represent a set of observations of the state s that conditioned on the state, are mutually independent (Assumption 1 holds). Then, f (), defined in (8), realizes the following properties:

1. f () = 0,

2. f is monotone nondecreasing, and

3. f is submodular.

The above lemma enables us to establish the approximation factor using the classical analysis in Nemhauser et al. (1978).
Theorem 1. Let  to denote the optimal subset of observations with regard to objective function f (), and g to denote the output of the greedy algorithm in Algorithm 1. Then, the following
performance guarantee holds:

H(s|~bbapl ,

i)



1 e

H(s|~bab pl

)

+

1- 1 e

H(s|~babpl ,

i).

ig

i

(10)

Remark 1. Intuitively, one can interpret the minimization of conditional entropy as pushing the
agent's belief toward the boundary of the probability simplex B. Due to convexity of POMDP value function on B Sondik (1978), this in turn implies that the agent is moving toward regions of belief space that have higher value.

Although Theorem 1 proves that the entropy of the belief point achieved by the greedy algorithm is close to the entropy of the belief point from the optimal solution, the key question is whether the value of these points are close. We assess this question in the following and show that at each time step, on expectation, the value from greedy scheme is close to the optimal value. To that end, we first show that the distance between the two belief points is upper-bounded. Thereafter, using a similar analysis as that of Pineau et al. (2006), we conclude that the difference between value function at these two points is upper-bounded.
Theorem 2. Let the agent's current belief to be b and its planning action to be apl. Consider the optimization problem in (9), and let apr and aprg to denote the optimal perception action and the perception action obtained by the greedy algorithm, respectively. It holds that:
E[ bg - b 1]  1, where b and bg are the updated beliefs according to (6) and beta1 is a constant value.

6

Under review as a conference paper at ICLR 2019

Figure 2: The belief reachability tree. The circles are belief points while squares depict branchings based on actions. Addition of perception actions leads to combinatorial expansion of number of belief points in each layer.

Proof. We outline the sketch of the proof and bring the complete proof in the appendix. First, we

show that minimizing conditional entropy of posterior belief is equivalent to maximizing Kullback-

Leibler (KL-) divergence between current belief and the posterior belief, i.e., DKL(bbapr, b). Next,

we exploit Pythagorean theorem for KL-divergence alongside its convexity to find a relation between

DKL(bbaprg ,

b) and DKL(bbaprg ,

b apr
b

,

)

.

Afterwards, using Pinkster's inequality, we prove

that the desired

total result

variation distance on boundedness of

bebtgw-eebnbb1a.prg

,

and

b apr ,
b

is bounded.

This in turn yields the

Theorem 3. Instate the notation and hypothesis of Theorem 2. Additionally, let V to be the true value function for AP2-POMDP. The following statement holds:

E[V (bg) - V (b)]  2.

Proof. The proof is omitted for brevity. See the appendix for the proof.

4 PERCEPTION-AWARE POINT-BASED VALUE ITERATION
In this section, we propose a novel point-based value iteration algorithm to approximate the value function for AP2-POMDPs. The algorithm relies on the performance guarantee of the proposed greedy observation selection in previous section. Before describing the new point-based solver, we first overview how point-based solvers operate. Algorithm 2 outlines the general procedure for a point-based solver. It starts with an initial set of belief points B0 and their corresponding  vectors. Then it performs a Bellman backup for each point to update  vectors. Next, it prunes  vectors to remove dominated ones. Afterwards, it samples a new set of belief points and repeats these steps until convergence or other termination criteria is met. The difference between solvers is in how they apply sampling and pruning. The sampling step usually depends on the reachability tree of belief space, see Figure 2. The state-of-the-art point-based methods do not traverse the whole reachability tree, but they try to have enough sample points to provide a good coverage of the reachable space.
Algorithm 2 Generic algorithm for point-based solvers
1: Input: POMDP. 2: Output: Approximate value function V . 3: Initialize B = B0 and 0. 4: while  (termination condition) do 5: B  Sample(B) 6:   BackUp(B, ) 7:   Prune(B, ) 8: end while 9: return V (b) = max .b.
Note that the combinatorial number of actions due to observation selection highly expand the size of the reachability tree. To avoid dealing with perception actions in the reachability tree, we apply the greedy scheme to make the choice of apr deterministically dependent on apl and previous belief. To that end, we modify the BackUp step of point-based value iteration. The proposed BackUp step can be combined with any sampling and pruning method in other solvers.

7

Under review as a conference paper at ICLR 2019

4.1 PROPOSED POINT-BASED SOLVER
In point-based solver each witness belief point is associated with an  vector and an action. Nevertheless, for AP2-POMDPs, each witness point is associated with two actions, apl and apr. We compute apr based on greedy maximization of (9) so that given b and apl, apr is uniquely determined. Henceforth, we can rewrite (3) using (4) to obtain:

Vt(b) = max
(apl,apr )

sS

b(s)R(s,

apl)

+





P

r(|b,

apl,

apr )

max
t-1

.bbapl ,apr ,

= max

b(s)R(s, apl)+

apl sS

max (s ) ×

Oi(s , apl, ij ) T (s, apl, s )b(s)

i1 ×...×ik t-1 s S

ij (a¯pr )

sS

ij (a¯pr )

= max

b(s)R(s, apl)+

apl sS

max

(s ) ×

Oi(s , apl, ij )T (s, apl, s )b(s)

i1 ×...×ik t-1 sS s S

ij (a¯pr )

ij (a¯pr )

.

where a¯pr = argmaxaprApr f ((apr)) and f is computed at ~babpl . This way, we can partially decouple the computation of perception action from the computation necessary for learning the
planning policy.

Inspired by the results in the previous section, we propose the BackUp step detailed in Algorithm 3 to compute the new set of  vectors from the previous ones using Bellman backup operation. What
distinguishes this algorithm from conventional Bellman backup step is the inclusion of perception
actions. Basically, we need to compute the greedy perception action for each belief point and each action (Line 7). This in turn affects computation of tb,apl, as it represents a different set for each belief point (Lines 9-13). However, notice that this added complexity is significantly lower than
concatenating the combinatorial perception actions with the planning actions and using conventional
point-based solvers.

5 SIMULATION RESULTS

To evaluate the proposed algorithm for active perception and planning, we developed a point-based

value iteration solver for AP2-POMDPs. We initialized the belief set by uniform sampling from B

Devroye (1986). To focus on the effect of perception, we did not apply a sampling step, i.e, the belief

set is fixed throughout the iterations. However, one can integrate any sampling method such as the

ones proposed in Smith & Simmons (2012); Kurniawati et al. (2008). The  vectors are initialized

by

1 1-

mins,a

R(s,

a).Ones(|S

|)

Shani

et

al.

(2013).

Furthermore,

to

speedup

the

solver,

one

can

employ a randomized backup step, as suggested in Spaan & Vlassis (2005). The solver terminates

once the difference between value functions in two consecutive iterations falls below a predefined

threshold. We also implemented a random perception policy that selects a subset of information

sources, uniformly at random, at each backup step. We implemented the solver in Python 2.7 and

ran the simulations on a laptop with 2.0 GHz Intel Core i7-4510U CPU and with 8.00 GB RAM.

5.1 ROBOTIC NAVIGATION IN 1-D GRID
The first scenario models a robot that is moving in a 1-D discrete environment. The robot can only move to adjacent cells and its navigation actions are Apl = {lef t, right, stop}. The robot's transitions are probabilistic due to possible actuation errors. The robot does not have any sensor and it relies on a set of cameras for localization. There is one camera at each cell that outputs a probability

8

Under review as a conference paper at ICLR 2019

Algorithm 3 BackUp step for AP2-POMDP

1: Input: AP2-POMDP P = (S, A, k, T, , O, R, ), Current set of belief points Bt, Current set
of  vectors t-1.
2: Output: Next set of  vectors t. 3: Initialize t = , tb,apl =  for all b  Bt and apl  Apl. 4: for apl  Apl do 5: tapl,  apl,(s) = R(s, apl) 6: for b  Bt do 7: a¯pr = Greedy argmaxaprApr f ((apr)) 8: bt,apl, =  9: for   i1 × . . . × ik , ij  (a¯pr) do 10: for   t-1 do 11: b,apl, (s) =  s S ij (a¯pr)
Oi(s , apl, ij )T (s, apl, s )(s ) 12: bt,apl,  bt,apl,  b,apl, 13: end for

14: end for 15: b,apl = apl,+

argmax .bi1 ×...×ik
ij (a¯pr )

bt ,apl ,

16: bt,apl  bt,apl  b,apl

17: end for

18: end for

19: for b  Bt do 20: b = argmaxtb,apl ,aplApl .b 21: t = t  b
22: end for

23: return t.

(a) 1-D grid

(b) 2-D grid

Figure 3: The robot moves in a grid while communicating with the cameras to localize itself. There is a camera at each state on the perimeter. The accuracy of measurements made by each camera depends on the distance of the camera from that state. The robot's objective is to reach the goal state, labeled by star, while avoiding the obstacles.

distribution over the position of the robot. The camera's certainty is higher when the robot's position is close to it. To model the effect of robot's position on the accuracy of measurements, we use a binomial distribution with its mean at the cell that camera is on. The binomial distribution represents the state-dependent accuracy. The robot's objective is to reach an specific cell in the map. For that purpose, at each time step, the robot picks a navigation action and selects k camera from the set of n cameras.
After the solver terminates, we evaluate the computed policy. To that end, we run 1000 iterations of Monte Carlo simulations. The initial state of the robot is the origin of the map and its initial belief is uniform over the map. Figure 4-(a) demonstrates the discounted cumulative reward, averaged over 1000 Monte Carlo runs, for random selection of 1 and 2 information sources, and greedy selection
9

Under review as a conference paper at ICLR 2019

(a) Average discounted cumulative reward

(b) Average entropy over time

Figure 4: Results of 1-D simulation for a map of size 12, averaged over 1000 runs for each perception policy. Left: The average discounted cumulative reward. The solid lines depict the corresponding standard deviations. Right: The average belief entropy over the time horizon of 25.

(a) Random perception algorithm

(b) Greedy perception algorithm

Figure 5: The frequency of visiting states when using different perception methods for a 2-D map of size 5*5. Dark blue and dark red are the lowest and highest frequencies, respectively.

of 1 and 2 information sources. It can be seen that the greedy perception policy significantly outperforms the random perception. Figure 4-(b) depicts the belief entropy over the time. The lowest entropy of greedy perception, compared to random perception, shows less uncertainty of the robot when taking planning actions.
5.2 ROBOTIC NAVIGATION IN 2-D GRID
The second setting is a variant of first scenario where the map is 2-D. Therefore the navigation actions of robot are Apl = {up, right, down, lef t, stop}. The rest of the setting is similar to 1D case, except the cameras' positions, as they are now placed around the perimeter of the map. Additionally, the robot has to now avoid the obstacles in the map. The reward is 10 at the goal state. -4 at the obstacles, and -1 in other states.
We applied the proposed point-based solver with both random perception and greedy perception on the 2-D example. Next, we let the robot to run for a horizon of 25 steps and we terminated the simulations once the robot reached the goal. Figure 5 illustrates the normalized frequency of visiting each state for each perception algorithm. It can be seen that the policy learned by greedy active perception leads to better obstacle avoidance.
6 CONCLUSION
In this paper, we studied joint active perception and planning in POMDP models. To capture the structure of the problem, we introduced AP2-POMDPs that have to pick a cardinality-constrained subset of observations, in addition to original planning action. To tackle the computational challenge of adding combinatorial actions, we proposed a greedy scheme for observation selection. The greedy scheme aims to minimize the conditional entropy of belief which is a metric of uncertainty about the state. We provided a theoretical analysis for the greedy algorithm that led to boundedness of value function difference between optimal and greedy solution in a single step. Furthermore, founded upon the theoretical guarantee of greedy active perception, we developed a point-based value iteration solver for AP2-POMDPs. The idea introduced in the solver to address active perception is general and can be applied on state-of-the-art point-based solvers. Lastly, we implemented and evaluated the proposed solver on a variety of robotic navigation scenarios.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Mauricio Araya, Olivier Buffet, Vincent Thomas, and Franc¸cois Charpillet. A pomdp extension with belief-dependent rewards. In Advances in neural information processing systems, pp. 64­72, 2010.
Karl J A° stro¨m. Optimal control of markov processes with incomplete state information. Journal of Mathematical Analysis and Applications, 10(1):174­205, 1965.
Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, pp. 679­684, 1957.
Hsien-Te Cheng. Algorithms for partially observable Markov decision processes. PhD thesis, University of British Columbia, 1988.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Imre Csisza´r. I-divergence geometry of probability distributions and minimization problems. The Annals of Probability, pp. 146­158, 1975.
Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pp. 260­265. ACM, 1986.
LLC Gurobi Optimization. Gurobi optimizer reference manual, 2018. URL http://www. gurobi.com.
Abolfazl Hashemi, Mahsa Ghasemi, Haris Vikalo, and Ufuk Topcu. A randomized greedy algorithm for near-optimal sensor scheduling in large-scale sensor networks. In 2018 Annual American Control Conference (ACC), pp. 1027­1032. IEEE, 2018.
Siddharth Joshi and Stephen Boyd. Sensor selection via convex optimization. IEEE Transactions on Signal Processing, 57(2):451­462, 2009.
Chun-Wa Ko, Jon Lee, and Maurice Queyranne. An exact algorithm for maximum entropy sampling. Operations Research, 43(4):684­691, 1995.
Andreas Krause and Daniel Golovin. Submodular function maximization. In Tractability: Practical Approaches to Hard Problems, pp. 71­104. Cambridge University Press, 2014.
Andreas Krause and Carlos Guestrin. Near-optimal observation selection using submodular functions. In AAAI, volume 7, pp. 1650­1654, 2007.
Chris Kreucher, Keith Kastella, and Alfred O Hero Iii. Sensor management using an active sensing approach. Signal Processing, 85(3):607­624, 2005.
Hanna Kurniawati, David Hsu, and Wee Sun Lee. Sarsop: Efficient point-based pomdp planning by approximating optimally reachable belief spaces. In Robotics: Science and systems, volume 2008. Zurich, Switzerland., 2008.
William S Lovejoy. A survey of algorithmic methods for partially observed markov decision processes. Annals of Operations Research, 28(1):47­65, 1991.
Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondra´k, and Andreas Krause. Lazier than lazy greedy. In AAAI, pp. 1812­1818, 2015.
Prabhu Natarajan, Pradeep K Atrey, and Mohan Kankanhalli. Multi-camera coordination and control in surveillance systems: A survey. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 11(4):57, 2015.
George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical programming, 14(1):265­294, 1978.
Christos H Papadimitriou and John N Tsitsiklis. The complexity of markov decision processes. Mathematics of operations research, 12(3):441­450, 1987.
11

Under review as a conference paper at ICLR 2019
Joelle Pineau, Geoffrey Gordon, and Sebastian Thrun. Anytime point-based approximations for large pomdps. Journal of Artificial Intelligence Research, 27:335­380, 2006.
Yash Satsangi, Shimon Whiteson, Frans A Oliehoek, and Matthijs TJ Spaan. Exploiting submodular value functions for scaling up active perception. Autonomous Robots, 42(2):209­233, 2018.
Manohar Shamaiah, Siddhartha Banerjee, and Haris Vikalo. Greedy sensor selection: Leveraging submodularity. In Decision and Control (CDC), 2010 49th IEEE Conference on, pp. 2572­2577. IEEE, 2010.
Guy Shani, Joelle Pineau, and Robert Kaplow. A survey of point-based pomdp solvers. Autonomous Agents and Multi-Agent Systems, 27(1):1­51, 2013.
Richard D Smallwood and Edward J Sondik. The optimal control of partially observable markov processes over a finite horizon. Operations research, 21(5):1071­1088, 1973.
Trey Smith and Reid Simmons. Point-based pomdp algorithms: Improved analysis and implementation. arXiv preprint arXiv:1207.1412, 2012.
Edward J Sondik. The optimal control of partially observable markov processes over the infinite horizon: Discounted costs. Operations research, 26(2):282­304, 1978.
Matthijs TJ Spaan. Cooperative active perception using pomdps. In AAAI 2008 workshop on advancements in POMDP solvers, 2008.
Matthijs TJ Spaan and Pedro U Lima. A decision-theoretic approach to dynamic sensor selection in camera networks. In ICAPS, 2009.
Matthijs TJ Spaan and Nikos Vlassis. Perseus: Randomized point-based value iteration for pomdps. Journal of artificial intelligence research, 24:195­220, 2005.
Matthijs TJ Spaan, Tiago S Veiga, and Pedro U Lima. Decision-theoretic planning under uncertainty with information rewards for active cooperative perception. Autonomous Agents and Multi-Agent Systems, 29(6):1157­1185, 2015.
Jason D Williams and Steve Young. Partially observable markov decision processes for spoken dialog systems. Computer Speech & Language, 21(2):393­422, 2007.
Nevin Lianwen Zhang and Weihong Zhang. Speeding up the convergence of value iteration in partially observable markov decision processes. Journal of Artificial Intelligence Research, 14: 29­51, 2001.
APPENDIX
In this section, we provide the proofs to the lemmas and theorems stated in the paper.
PROOF OF THEOREM 1
Lemma 1. Let  = {1, 2, . . . , n} to represent a set of observations of the state s that conditioned on the state, are mutually independent (Assumption 1 holds). Then, f (), defined in (8), realizes the following properties:
1. f () = 0, 2. f is monotone nondecreasing, and 3. f is submodular.
12

Under review as a conference paper at ICLR 2019

Proof. Notice that ~babpl is explicitly present to determines the current distribution of s and it is not a random variable. Therefore, for simplicity, we omit that in the following proof. It is clear that f () = H(s|~babpl ) - H(s|~bbapl ) = 0.
Let [n] = {1, 2, . . . , n}. To prove the monotonicity, consider 1  [n] and j  [n]\1. Then,

H(s|

i)

i1 {j }

(=a)H(

i|s) + H(s) - H(

i)

i1 {j }

i1 {j }

(=b)H( i|s) + H(j|s) + H(s) - H( i)

i1

i1

- H(j| i)

i1

(=c)H(s| i) + H(j|s) - H(j| i)

i1

i1

(=d)H(s| i) + H(j|s, i) - H(j| i)

i1

i1

i1

(e)
 H(s| i) + H(j| i) - H(j| i)

i1

i1

i1

=H(s| i),

i1

where (a) and (c) are due to Bayes' rule for entropy, (b) follows from the conditional independence assumption and joint entropy definition, (d) is due to the conditional independence assumption, and (e) stems from the fact that conditioning does not increase entropy.

Furthermore, from the third line of above proof, we can derive the marginal gain as:

fj(1) = H(s| i) - H(s|

i)

i1

i1 {j }

= H(j| i) - H(j|s)

i1

To prove submodularity, let 1  2  [n] and j  [n]\2. Then,

fj(1) = H(j| i) - H(j|s)

i1

(a)
 H(j|

i) - H(j|s)

i1 (2 \1 )

(=b) H(j| i) - H(j|s) = fj(2),

i2

where (a) is based on the fact that conditioning does not increase entropy, and (b) results from 1  2.
Theorem 1. Let  to denote the optimal subset of observations with regard to objective function f (), and g to denote the output of the greedy algorithm in Algorithm 1. Then, the following performance guarantee holds:

H(s|~babpl ,

i)



1 e

H(s|~bab pl

)

+

1- 1 e

H(s|~babpl ,

i).

ig

i

(11)

13

Under review as a conference paper at ICLR 2019

Proof. The properties of f stated in Lemma 1 along the theoretical analysis of greedy algorithm in
Nemhauser et al. (1978) yields f (g)  (1 - 1 )f (). e
Using the definition of f () and rearranging the terms, we obtain the desired result.

PROOF OF THEOREM 2

Before stating the proof to Theorem 2, we need to present a series of propositions and lemmas. Mutual information between two random variables is a positive and symmetric measure of their dependence and is defined as:

I(x; y)

=

x,y

px,y(x, y) log

px,y(x, y) . px (x)py (y)

Mutual information, due to its monotonicity and submodularity, has inspired many subset selection algorithms Krause & Golovin (2014). In the following proposition, we express the relation between conditional entropy and mutual information.

Proposition 1. Minimizing conditional entropy of the state with respect to a set of observations is equivalent to maximizing the mutual information of state and the set of observations. This equivalency is due to the definition of mutual information, i.e.,

I(s; i) = H(s) - H(s| i),

(12)

i i

and the fact that H(s) is computed at ~bbapl which amounts to a constant value that does not affect selection procedure. Additionally, notice that (12) is the same as the definition of normalized objective
function of greedy algorithm in (8).

Another closely-related information-theoretic concept is Kullback-Leibler (KL-) divergence. The KL-divergence, also known as relative entropy, is a non-negative and non-symmetric measure of difference between two distributions. The KL-divergence from q(x) to p(x) is:

DKL(px qx) = px(x) log
x

px(x) qx(x)

.

The following relation between mutual information and KL-divergence exists:

I(x; y)

=

x,y

px,y(x, y) log

px,y(x, y) px (x)py (y)

=

y

py(y)

x

px|y (x|y)

log

px|y (x|y) px(x)

= Ey[DKL(px|y px)],

which allows us to state the next proposition.

Proposition 2. The mutual information of state and a set of observations is the expected value of the KL-divergence from prior belief to posterior belief over all realizations of observations, i.e.,

I (s;

i) = E i i DKL(b~bapr, ~bbapl ) .

i

(13)

~bbapl is the prior belief perception action apr

before selecting observations and and receiving the observations.

b~bapr ,

is

the

posterior

belief

after

selecting

Let p0 := ~babpl , pg := b~bapr,,   ig i, and p := b~bapr,,   i i to denote prior belief (after taking planning action), posterior belief after greedy perception action, and posterior
belief after optimal perception action, respectively. So far, we have established a relation between
minimizing the conditional entropy of posterior belief and maximizing the expected KL-divergence from prior belief to posterior belief, i.e., DKL(pg p0) (See Proposition 1 and Proposition 2). To relate DKL(pg p0) and DKL(pg p), we state the next lemma. But first, we bring definitions necessary for proving the lemma.

14

Under review as a conference paper at ICLR 2019

Figure 6: The probability simplex, B, for a 3-state POMDP. Gray area illustrates the projection of hypograph of entropy, corresponding to posterior belief after greedy perception action, onto B.

Definition 4. Let p to be a probability distribution over a finite alphabet. An I-sphere with center p and radius  is defined as:
S(p, ) d=ef {q | DKL(q p) < }.
Definition 5. Let  to denote a convex set of probability distributions that intersect S(p, ). A q¯ satisfying
DKL(q¯ p) d=ef min DKL(q p),
q
is called the I-projection of p on .
Lemma 2. Instate the definition of p0, pg, and p. The following inequality holds on expectation:

DKL p0 p  DKL p0 pg + DKL (pg p) .

Proof. Consider the set g = {p  B |H(p)  H(pg) that contain probability distributions whose entropy is lower-bounded by entropy of pg. Since entropy is concave over B, its hypographs are
convex. Consequently g, the projection of a hypograph onto B, is a convex set. Furthermore, due
to monotonicity of conditional entropy, i.e., expected value of entropy over observations, we know that p0  g. Besides, Due to optimality of , it holds that

H(s|~babpl ,

i)  H(s|~babpl ,

i)

i

ig

which in turn yields p  B\g. Figure 6 demonstrates these facts for an alphabet of size 3. pg is the I-projection of p on g. Therefore, by exploiting the analogue of Pythagoras' theorem for
KL-divergence Csisza´r (1975), we conclude:

DKL p0 p  DKL p0 pg + DKL (pg p) .

A direct result of the above lemma, after taking the expectation over i[n] i, is:
E i i DKL(~bbapl b~bapr, )  E ig i DKL(~bbapl b~bapr, ) + E i[n] i DKL(b~bapr,g b~bapr, ) .

(14)

Theorem 4. The KL-divergence between pg and p is upper-bounded, i.e., E i[n] i [DKL (pg p)]  3,
where 3 is a constant. Proof. Notice that while KL-divergence is not symmetric, the following fact still holds:

argmax E


i i

DKL(b~bapr, ~bbapl )

= argmax E


i i

DKL(~bab pl

b~bapr, )

.

Notice that E i i DKL(~babpl b~bapr,) is constant. Now, using Lemma 2 along the nearoptimality result of greedy algorithm (See Theorem 1) yields the desired result.

Next, we bound the total variation distance between pg and p.

15

Under review as a conference paper at ICLR 2019

Definition 6. The total variation distance between two probability distributions p and q, over a countable state space S, is defined as:

1 (p, q) =

|p(s) - q(s)|.

2

sS

Pinsker's inequality bounds the total variation distance in terms of the KL-divergence Cover & Thomas (2012). Using this inequality, we can prove Theorem 2.
Theorem 2. Let the agent's current belief to be b and its planning action to be apl. Consider the optimization problem in (9), and let apr and aprg to denote the optimal perception action and the perception action obtained by the greedy algorithm, respectively. It holds that:

E[ bg - b 1]  1, where b and bg are the updated beliefs according to (6).

Proof. Note that the total variation distance is half of the l1-norm. Hence,

E[ bg - b 1] = 2 E[(p, q)]

(a)
 2 E[

1 2

DKL

(bg

b)]

(b)
2

1 2

E[DKL(bg

b)]

(c)
 23,

where (a) is the result of applying Pinsker's inequality, b is obtained by applying Jansen's inequality to concave square-root function, and (c) follows from Theorem 4. The proof completes by taking 1 = 23.

PROOF OF THEOREM 3

Theorem 3. Instate the notation and hypothesis of Theorem 2. Additionally, let V to be the true value function for AP2-POMDP. The following statement holds:

E[V (bg) - V (b)]  2.

Proof. We use the PWLC property of value function. Let g and  to represent the gradient of

value function at bg and b, respectively. Note that

g - g





Rmax -Rmin 1-

where Rmax

=

maxs,apl R(s, apl) and Rmin = mins,apl R(s, apl). Therefore, we can show that

E[V (bg) - V (b)] = E[g.bg - .b] = E[g.bg - g.b + g.b - .b]

(a)
 E[g.bg - g.b + .b - .b] = E[(g - ).(bg - b)]

(b)
 E[

g - 



bg - b 1]

(c)


1

Rmax 1

- -

Rmin 

,

where (a) follows from the fact that  is the gradient of optimal value function, (b) is due to Ho¨lder's inequality, and (c) is the result of Theorem 2.

16

