Under review as a conference paper at ICLR 2019
GRADIENT-BASED LEARNING FOR F-MEASURE AND
OTHER PERFORMANCE METRICS
Anonymous authors Paper under double-blind review
ABSTRACT
Many important classification performance metrics, e.g. F -measure, are nondifferentiable and non-decomposable, and are thus unfriendly to gradient descent algorithm. Consequently, despite their popularity as evaluation metrics, these metrics are rarely optimized as training objectives in neural network community. In this paper, we propose an empirical utility maximization scheme with provable learning guarantees to address the non-differentiability of these metrics. We then derive a strongly consistent gradient estimator to handle non-decomposability. These innovations enable end-to-end optimization of these metrics with the same computational complexity as optimizing a decomposable and differentiable metric, e.g. cross-entropy loss.
1 INTRODUCTION
Different classification performance metrics measures different aspects of a classifier's behavior. For example, F -measure ( Van Rijsbergen (1974)), compared to metrics such as accuracy, is better at evaluating a classifier's performance when it encounters a sample belonging to a class that occurs with low frequency. Ideally, we can acquire a classifier with very tailored behavior by optimizing the classifier with respect to a carefully chosen performance metric. Unfortunately, many performance metrics are non-differentiable and non-decomposable, which renders it very difficult to optimize neural network classifiers with these metrics as training objective. In this paper, we propose a method that enables gradient-based learning for these performance metrics. Our contributions are the following:
· We propose a learning algorithm based on empirical utility maximization for a class of performance metrics and prove its generalization and consistency.
· We propose a strongly consistent gradient estimator that enables efficient gradient-based maximization of empirical utility.
· We demonstrate experimentally that the binary F1 score of neural network classifiers can be optimized on datasets of decent scale.
We organize this paper as follows. In Section 2, we sketch our approach for binary F1 score to provide an overview. In Section 3, we present our method in its general form. We review related work in Section 4 and provide experimental results in Section 5.
2 GRADIENT-BASED LEARNING FOR BINARY F1 SCORE
2.1 PROBABILISTIC CLASSIFIER
Given a feature vector x  X  RN , a probabilistic classifier h first infers a posterior p(·|x) over a discrete output space Y and then samples an output from the posterior, i.e. h(x)  p(·|x). In practice, p(·|x) is typically the output of a neural network with softmax layer on its top. When the posterior is parameterized, e.g. being implemented as a neural network, we denote it as p(·|x) and the corresponding probabilistic classifier as htheta.
1

Under review as a conference paper at ICLR 2019

Given a posterior p(·|x), a deterministic classifier results from the inference rule h(x) = argmaxyY p(y|x).The difference between probabilistic and deterministic inference rules is negligible when the posterior is very concentrated. Although deterministic classifiers are more popular
in the literature, in this paper we only consider probabilistic classifiers and leave it as future work to
investigate the case where a probabilistic classifier is replaced by a deterministic one.

2.2 F-MEASURE

Consider the case of binary classification, where Y = {0, 1} with 1 and 0 corresponding to the
positive and negative class. Given an i.i.d. dataset D = {(x1, y1), ..., (xn, yn)} consisting of n pairs of feature vector and ground truth, let y^i denote the label predicted by a classifier h given xi (not necessarily deterministically). Let y^ = (y^1, ..., y^n) and y = (y1, ..., yn). Then the true-positive, false-positive, false-negative and true-negative rate corresponding to y^ and y are defined as

tp(y^, y) := 1 n

n

I y^i = 1  yi = 1

i=1

fn(y^, y)

:=

1 n

n

I y^i = 0  yi = 1

i=1

fp(y^, y) := 1 n

n

I y^i = 1  yi = 0

i=1

tn(y^, y)

:=

1 n

n

I y^i = 0  yi = 0

i=1

where I denotes indicator function. The precision and recall are defined as

precision(y^, y)

=

tp(y^, tp(y^, y) +

y) fp(y^, y)

recall(y^, y)

=

tp(y^, y) pD+

(1)

where p+D

:=

1 n

n i=1

I(yi

=

1)

denotes

the

proportion

of

samples

in

D

that

belong

to

positive

class. The binary F -measure is defined as ( Van Rijsbergen (1974)):

F(y^, y)

=

(1

+

2)

·



precision(y^, y) · · precision(y^, y)

recall(y^, y) + recall(y^, y)

>0

(2)

or equivalently,

F(y^, y)

=

(1

+

2)

·

(1

+

p+D - fn(y^, y) 2)p+D - fn(y^, y) +

fp(y^, y)

which is more convenient for our purpose.

>0

(3)

We will refer to F(y^, y) as data-dependent binary F-measure because it is evaluated on a specific set of data with pairs of ground truth and prediction vectors. F is non-differentiable because it is a composition of indicator functions. Nor does it decompose over samples in D. More precisely,

people are not aware of any function f that only depends on per sample ground-truth and prediction

such that

1n F(y^, y) = n f(y^i, yi)
i=1

In the following we propose an empirical utility maximization scheme for optimizing the Fmeasure of probabilistic classifiers. For ease of exposition, in this section we focus on binary F1measure, a.k.a. binary F1 score. In Section-3, we will extend the method presented in this section to a family of non-decomposable and non-differentiable performance metrics, including F-measure for multi-class classification.

2.3 GRADIENT-BASED LEARNING FOR BINARY F1 SCORE

We consider a parameterized binary probabilistic classifier h. By linearity of expectation and i.i.d. assumption,
Ey^,y [fn(y^, y)] = P(^y = 1  y = 0)
Similarly, Ey^,y [fp(y^, y)] = P(¯y = 0  y = 1). Let fn(h) := Ey^,y [fn(y^, y)] and fp(h) := Ey^,y [fp(y^, y)]. It follows from law of large number that

lim
n

fn(y^,

y)

=

fn(h )

lim
n

fp(y^,

y)

=

fp(h )

2

Under review as a conference paper at ICLR 2019

with probability 1. Thus on sufficiently large datasets,

fn(y^, y)  fn(h) fp(y^, y)  fp(h)

With these approximate identities, we have

F1(y^, y)

=

2

·

2p+D

p+D - fn(y^, y) - fn(y^, y) + fp(y^, y)



2

·

2pD+

p+D - fn(h) - fn(h) + fp(h)

:=

F¯1 (h )

which implies that the F1 score of any predictions of h on any sufficiently large dataset is close to F¯1(h). We call F¯1(h) expected utility and state the precise meaning of F¯1(y^, y)  F1(h) in Section-3. The key point is that we can optimize F¯1(h) instead of F1(y^, y) if we are interested

in the F1 score of h on large datasets. However, fn(h) and fp(h) are unknown because they are expectations taken over data distribution (and classifier's posterior). Consequently, we have to estimate them by sampling from data distribution in order to compute F¯1(h).

Let p+ = P(y = 1) denote the probability that a positive sample occurs, which can be estimated by the frequency of positive samples in training set. Let n+ denote the number of positive samples in
training set. Assume that the data distribution admits a density function (i.e. the data distribution is
absolutely continuous w.r.t. Lebesgue measure), and denote the density function by p. We have

fn(h) = P y¯ = 0  y = 1

= P(h(x) = 0)p(x, 1)dx
X

= p(0|x)p(x|1)p+dx
X

= p+ p(0|x)p(x|1)dx
X
= p+Exp(·|1)[p(0|x)]



p+ n+

n+

p(0|xi+) := fnD(h)

i=1

(4)

where x1+, ..., xn++ are feature vectors of samples belonging to the positive class in training set D. Similarly, we have

fp = (h)P ¯y = 1  y = 0

 p- n

n
p(1|x-i ) := fpD(h)

i=1

where p- := P(y = 0), n- :=

(x,y)D

I(y

=

0),

and

x-1 ,

...,

x

- n

are

feature

vectors

of

samples

in D belonging to the negative class. Thus F¯1(h) can be estimated as following:

F¯1 (h )

=

2

·

2p+

p+ - fn(h) - fn(h) + fp(h)



2

·

2p+

p+ - fnD(h) - fnD(h) + fpD(h)

:=

F^D (h )

(5)

Interestingly, although fn(y^, y) and fp(y^, y) are not differentiable, the estimators of their expec-

tations, fnD(h) and fpD(h), are differentiable w.r.t.  if p is differentiable. Because F^D is differentiable w.r.t. fn(h) and fp(h), F¯1(h) can be computed by chain rule. Consequently, gradient descent can be applied to optimize F^1.

We will refer F^D() as the empirical utility of the expected utility F¯1(h). They correspond to empirical and expected risk in the classical empirical risk minimization principle of statistical learning theory ( Vapnik (1992)). We use the term "empirical utility maximization" because we would like to maximize, instead of minimize these performance metrics. There are two fundamental questions for every empirical risk minimization style learning algorithm, as the number of samples increases:

· Generalization. Given h, does F^D(h) converge to F¯1(h)? · Consistency. Does argmaxF^D(h) converge to argmaxF¯1(h)?
We will address these two questions in Section-3. For the moment let us consider a practical issue: how to maximize empirical utility F¯D(h) efficiently with gradient descent?

3

Under review as a conference paper at ICLR 2019

2.4 GRADIENT ESTIMATOR

Large training sets are necessary for the approximation Eq (5) to be accurate. Consequently, in order
to optimize F^D(h) efficiently via minibatch gradient descent, F^D(h) has to be estimated by F^B(h), where B  D is a mini-batch. However, mini-batch gradient descent requires that

EB F^B(h) = F^D(h)

(6)

Suppose F^D(h) is decomposable, i.e.

f^, F^D(h)

=

1 |D|

f^(h(x), y)

(x,y)D

then Eq (6) simply follows from the linearity of differentiation and expectation. However, as F^D(h) is non-decomposable, it becomes unlikely that F^B(h) is an unbiased estimator of F^D(h). Fortunately, F^B(h) is a strongly consistent estimator of F^D(h). More precisely, assuming
that D contains infinitely many samples, we have

P lim F^B(h) = F^D(h) = 1
|B|

(7)

More interestingly, the noise incurred by estimating F^D(h) with F^B(h) can be further controlled. In the following we omit the dependence on h for brevity. Let D = f nD(h), f pD(h)
and B = f nB(h), f pB(h) . Let J D and J B denotes the Jacobian of  and B w.r.t. . Let || · || denote matrix norm and | · | denote vector norm. By chain rule,

F^B = B F^B · J B = D F^(h) + · J + E = D F^ · J + D F^ · E + · J + · E

(8)

where F^(h) · J (h) is true gradient and · E is negligible. The error matrix E = J B(h) -
J D(h) is intrinsic in the sense that it results immediately from estimating f nD(h) and f pD(h) with f nB(h) and f pB(h). However, we can control the error · J because | · J |  ||J || · | | and we can control ||J || by limiting || and the norm of intermediate activation when p is a neural network ( He et al. (2015)). Despite these technicalities, the trick is very easy to implement: batch-norm (Ioffe & Szegedy (2015)) and weight decay will suffice. The resultant algorithm is the following:

Algorithm 1 Gradient-based learning for binary F1 score

Require: classifier h, dataset D, batch size b, learning rate , weight decay factor 

p+



1 |D|

(x,y)D I(y = 1)

p-



1 |D|

(x,y)D I(y = 0)

while terminating criterion not satisfied do

Sample

B+

=

{(x1+,

1),

...,

(x

+ b

,

1)}

from

D

Sample B- = {(x1-, 0), ..., (x-b , 0)} from D

fn



p+ b

b i=1

p

(0|x

+ 1

)

fp



p- b

b i=1

p

(1|x

- 1

)

   F1(fn, fp) -  · ||||2

  +·

end while

4

Under review as a conference paper at ICLR 2019

3 GRADIENT-BASED LEARNING FOR A CLASS OF PERFORMANCE METRICS

Binary F -measure in fact belong to a class of metrics that are well behaved functions of the confusion matrix. In this section, we propose a gradient-based learning algorithm that extends the approach illustrated in previous section to this class of metrics. We state two theorems concerning the generalization and consistency of the proposed algorithm. We defer the proofs of these theorems to appendix. We begin with a formal specification of this class of metrics, which relies on the definition of confusion matrix:
Definition 1. Given a dataset D = {(x1, y1), ..., (xn, yn)}, let y = (y1, ..., yn) denote the vector of ground truth and y^ = (y^1, ..., y^n) denote the vector of predictions. Then the corresponding data-dependent confusion matrix C(y^, y) is defined as

C(y^, y)

1 = ij n

n

I y^k = i  yk = j

k=1

0  i, j  |Y| - 1

Given a probability measure P over X × Y × Y, i.e. triples of feature vector, ground-truth and prediction, the expected confusion matrix C¯ of a probabilistic classifier h is a |Y| × |Y| matrix defined as
C¯(h) = P(h(x) = i  y = j) 0  i, j  |Y| - 1
ij

The confusion matrix is well-defined for both single-label and multi-label classification (although these two scenarios impose different constraints on its entries). In the case of binary classification,

C(y^, y) =
ij

tn(y^, y) fp(y^, y)

fn(y^, y) tp(y^, y)

C¯(h) =
ij

tn(h ) fp(h )

fn(h ) tp(h )

As in previous section, given a training set D, we have the estimator

C^D (h )

ij

=

pj nj

nj
p (i|x jk )
k=1

where pj is the frequency of the j-th class, nj is the number of samples in D that belong to the j-th class, and xj1, ..., xjnj are feature vectors of samples that belong to the j-th class. Entry-wise convergence in probability follows from law of large number

P lim C(y^, y) = C¯(h) = 1
|D|

P lim C^D(h) = C¯(h) = 1
|D|

(9)

Many performance metrics are functions of the confusion matrix. For example, accuracy(h) =

|Y | i=1

C¯ii (h ).

F

measure for multi-class classification can be defined in term of entries of the

confusion matrix as following. The data-dependent per class frequency, false positive and false

negative, defined as functions of C(y^, y), are respectively

|Y |
pi = Cji
j=1

fpi = Cij
j=i

fni = Cji
j=i

i = 1, ..., |Y|

where we omit the dependence on y^ and y for brevity. The data-dependent macro F -measure is defined in term of these quantities as

F

=

1 + 2 |Y |

|Y | i=1

(1

pi - fni + 2)pi - fni

+ fpi

>0

and the data-dependent micro F -measure is defined in term of these quantities as

F = (1 + 2) · (1 + 2)

|Y | i=1

pi

-

|Y | i=1

fni

|Y | i=1

pi

-

|Y | i=1

fni

+

|Y | i=1

fpi

>0

5

Under review as a conference paper at ICLR 2019

Replacing C(y^, y) by C¯(h) and C^(h) respectively results in expected and empirical F -measure.
We now formally define the class of metrics that we are interested in, namely the class of wellbehaved metrics. In the following, we will consider C, C(y^, y) and C¯E as vectors of dimension |Y| × |Y| instead of matrices.
Definition 2. We say that a performance metric F : K  R, where K is a compact subset of R|Y|×|Y|, is well-behaved if F is differentiable and its gradient is continuous in the interior of K.

Please refer to appendix for a non-exhaustive list of well-behaved performance metrics. Given a well behaved performance metric F , its corresponding data-dependent, expected and empirical utility are respectively defined as F (C(y^, y)), F (C¯(h)) and F (C^(h)). The following theorem establishes asymptotic equivalence between these three kinds of utilities.
Theorem 1. If F is a well-behaved performance metric and Cn is a strongly consistent estimator of C, i.e.

P

lim F
n

Cn

= F (C)

=1

where convergence in probability is entry-wise, then F Cn is a strongly consistent estimator for F (C), i.e.
P lim F C¯D = F (C) = 1
|D|

As a consequence of this theorem, it follows from Eq (9) that

P lim F C(y^, y) = F C¯(h) = 1
|D|

P lim F C^D(h) = F C¯(h) = 1
|D|

i.e. both F C(y^, y) and F C¯(h) are asymptotically equivalent to F C¯(h) . Thus decent empirical utility F C¯(h) implies decent data-dependent utility F C(y^, y) on any sufficiently
large dataset. As a special case,

P lim F1(y^, y) = F¯1(h) = 1
|D|

P lim F^1(h) = F¯1(h) = 1
|D|

which justifies Eq (2.3) and Eq (5) when the dataset of interest is sufficiently large.
Next we consider the issue of gradient estimation in this general setting.
Theorem 2. For a well behaved performance metric F , F C^B(h) is a strongly consistent estimator of F C¯(h) , where B  D is a mini-batch. More precisely, assuming that D contains infinitely many samples, we have

P lim F C^B(h) = F C^D(h) = 1
|B|

As proved in Chen & Luss (2018), many convergence guarantees for mini-batch gradient descent holds with probability 1 for strongly consistent gradient estimator. As illustrated in Eq (8), batchnorm (Ioffe & Szegedy (2015)) and weight decay can help control the noise of estimator. Please refer to Algorithm-3 for full algorithm.

Finally, we state two theorems concerning the generalization and consistency of this algorithm. Rate of converges are omitted for brevity.
Theorem 3. (Generalization) For a well behaved performance metric F , for all > 0,

lim P F (C^D(h)) - F (C¯(h)) < = 1
|D|

Theorem 4. (Consistency) For a well behaved performance metric F , with appropriate constraints on capacity of the parametric model p (see proof for details), for all > 0,

lim P arg max F (C^D(h)) - arg max F (C¯(h)) < = 1

|D|





6

Under review as a conference paper at ICLR 2019

Algorithm 2 Gradient-based learning for well behaved performance metrics

Require: classifier h, well-behaved metric F , dataset D, batch size b, learning rate 

for i = 1, ..., |Y| do

pi



1 |D|

(x,y)D I(y = i)

end for

while terminating criterion not satisfied do

for i = 1, ..., |Y| do

Sample Bi = {(xi1, i), ..., (xib, i)} from D Compute p(·|x1i ), ..., p(·|xbi ) end for

for i = 1, ..., |Y| do

for j = 1, ..., |Y| do

Cij



p b

b k=1

p(i|xkj )

end for

end for

   F (C) - ||||2   +·

end while

4 RELATED WORK
Optimization of non-decomposable and non-differentiable performance metrics, especially F measure, has been extensively studied. The heuristic algorithm considered in Jansche (2005) and Pastor-Pellicer et al. (2013) is essentially Algorithm-(2.4) without techniques that stabilize gradient estimation. However, Jansche (2005) and Pastor-Pellicer et al. (2013) are not very well motivated theoretically and provide little mathematical insight into the algorithm. Also, as shown in Section-5, applying this heuristic algorithm without stabilization techniques can easily result in non-convergent models, even for a three-layer fully-connected network.
Another series of papers ( Joachims (2005), Kar et al. (2014) and Narasimhan et al. (2015)) study optimizing differentiable lower bounds of various non-decomposable and non-differentiable binary classification metrics for linear classifiers. Despite proved learning guarantees for linear classifier, as reported in Sanyal et al. (2018), these lower bound methods are not very promising when applied to neural networks.
Thresholding is a computationally economic method if we only consider binary classification. Koyejo et al. (2014) proves that the optimal classifier with respect to a family of binary classification metrics, including F -measure, is appropriately thresholded Bayes classifier. Given an approximation of Bayes classifier, we can approach the optimal threshold via grid search. However, it remains unknown how to generalize thresholding to multi-class classification. More importantly, for binary classification, when training set is extremely imbalanced, it can be very difficult to train a classifier that approximates Bayes classifier very well.
The computational cost of aforementioned methods roughly equals that of training classifiers with standard classification losses such as cross-entropy. As proved in Parambath et al. (2014) and Koyejo et al. (2014), optimization of many performance metrics, including F -measure, can be reduced to weighted classification. Unfortunately, the optimal weight is in general unknown and has to be approached via an expensive grid search (see Section-5). Despite its computational cost, unlike thresholding, this method can perform very well even when training set is extremely balanced because of weighting. Eban et al. (2016) proposes a similar method that performs well for neural networks coupled with the AUCPR metric.
Regarding theory, the equivalence between data-dependent and expected utility of F -measure was first proved in Nan et al. (2012) and then generalized in Dembczyn´ski et al. (2017) to p-Lipschitz binary classification performance metrics.
7

Under review as a conference paper at ICLR 2019

Table 1: Dataset statistics and results; GS and EUM refer to grid-search and expected utility method, respectively
DATASET # FEATS # SAMPS % POS F1 GS F1 EUM

Adult CIFAR10 Letter Covtype CIFAR100 KDDCup08

108 3072 16 54 3072 117

48,842 60,000 20,000 581,012 60,000 102,294

23.93 10.00 3.92 1.63 1.00 0.61

0.701 0.630 0.990 0.691 0.350 0.543

0.689 0.635 0.975 0.725 0.392 0.556

5 EXPERIMENTS

We evaluate our method on the following datasets: Letter1, Adult2, Covertype3, KDDCup084, CI-

FAR10 and CIFAR100 ( Krizhevsky (2009)). The performance metric to optimize is binary F1

score. We use a three-layer fully-connected network in our experiments, with batch-norm enabled.

The statistics of these datasets are summarized in Table-5 (number of features, number of sam-

ples, percentage of positive samples). For multi-class datasets (Letter, Covertype, CIFAR10 and

CIFAR100), we designate one class as the positive class and leave the rest as the negative class. We

compare Algorithm-(2.4) with the following baseline ( Parambath et al. (2014) and Koyejo et al.

(2014)):





arg max
,(0,1)

1 |D|

l(h(x), y)
(x,y)D

I(y

=

1) + (1 - )I(y

=

1)

where l denotes cross-entropy loss. We let  = 0.1, 0.2, 0.3, ..., 0.9, and apply gradient descent to optimize  given a fixed . As proved in Parambath et al. (2014) and Koyejo et al. (2014), this baseline should yield approximately optimal solution (at cost considerably higher than Algorithm(2.4) because we have to optimize  for every ). In our case, the baseline is 8 times slower than Algorithm-(2.4). We apply weight decay to both methods and find that in general weight decay improves the performance of Algorithm-(2.4) while damages the performance of baseline. For Covertype dataset, Algorithm-2.4 cannot converge without weight decay. We report results in Table-5, where "F1 GS" refers to the F1 score of baseline and "F1 EUM" refers to the F1 score of Algorithm2.4.

6 CONCLUSION
We propose an empirical utility maximization scheme that enables efficient gradient-based learning for a class of non-decomposable and non-differentiable classification performance metrics. We inquire into the proposed scheme mathematically and present preliminary experiments that validate our approach. We leave it as future work to experiment on deeper neural networks, larger datasets, and more complex performance metrics.

1https://archive.ics.uci.edu/ml/datasets/letter+recognition 2https://archive.ics.uci.edu/ml/datasets/adult 3https://archive.ics.uci.edu/ml/datasets/covertype 4http://www.kdd.org/kdd-cup/view/kdd-cup-2008/
8

Under review as a conference paper at ICLR 2019
REFERENCES
Jie Chen and Ronny Luss. Stochastic gradient descent with biased but consistent gradient estimators. CoRR, abs/1807.11880, 2018. URL http://arxiv.org/abs/1807.11880.
Krzysztof Dembczyn´ski, Wojciech Kotlowski, Oluwasanmi Koyejo, and Nagarajan Natarajan. Consistency analysis for binary classification revisited. In International Conference on Machine Learning, pp. 961­969, 2017.
Elad ET Eban, Mariano Schain, Alan Mackey, Ariel Gordon, Rif A Saurous, and Gal Elidan. Scalable learning of non-decomposable objectives. arXiv preprint arXiv:1608.04802, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Martin Jansche. Maximum expected f-measure training of logistic regression models. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pp. 692­699. Association for Computational Linguistics, 2005.
Thorsten Joachims. A support vector method for multivariate performance measures. In Proceedings of the 22nd international conference on Machine learning, pp. 377­384. ACM, 2005.
Purushottam Kar, Harikrishna Narasimhan, and Prateek Jain. Online and stochastic gradient methods for non-decomposable loss functions. In Advances in Neural Information Processing Systems, pp. 694­702, 2014.
Oluwasanmi O Koyejo, Nagarajan Natarajan, Pradeep K Ravikumar, and Inderjit S Dhillon. Consistent binary classification with generalized performance metrics. In Advances in Neural Information Processing Systems, pp. 2744­2752, 2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Ye Nan, Kian Ming Chai, Wee Sun Lee, and Hai Leong Chieu. Optimizing f-measure: A tale of two approaches. arXiv preprint arXiv:1206.4625, 2012.
Harikrishna Narasimhan, Purushottam Kar, and Prateek Jain. Optimizing non-decomposable performance measures: a tale of two classes. In International Conference on Machine Learning, pp. 199­208, 2015.
Shameem Puthiya Parambath, Nicolas Usunier, and Yves Grandvalet. Optimizing f-measures by cost-sensitive classification. In Advances in Neural Information Processing Systems, pp. 2123­ 2131, 2014.
Joan Pastor-Pellicer, Francisco Zamora-Mart´inez, Salvador Espan~a-Boquera, and Mar´ia Jose´ CastroBleda. F-measure as the error function to train neural networks. In International Work-Conference on Artificial Neural Networks, pp. 376­384. Springer, 2013.
Amartya Sanyal, Pawan Kumar, Purushottam Kar, Sanjay Chawla, and Fabrizio Sebastiani. Optimizing non-decomposable measures with deep networks. arXiv preprint arXiv:1802.00086, 2018.
Cornelis Joost Van Rijsbergen. Foundation of evaluation. Journal of Documentation, 30(4):365­ 373, 1974.
Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural information processing systems, pp. 831­838, 1992.
9

Under review as a conference paper at ICLR 2019

PROOFS

Proof. Theorem 1

Let K = |Y| × |Y|. We enumerate the entries of CD and C as i,D and i (i = 1, ..., K). Let Ei (i = 1, ..., K) denote the event that
lim i,D = i
|D|

and E denote the event that

lim F D = F 
|D|

We have because by continuity of F ,

K
Ei  E
i=1

i = 1, ..., K, lim i,D = i  lim D = 

|D|

|D|

 lim F D = F 
|D|

Taking complement on both sides, we have

Ec 

K cK
Ei = Eic
i=1 i=1

By monotonicity of probability measure and union bound, we have

P(Ec)  P

K
Ei
i=1

c

=P

K
Eic
i=1

KK

K

 P(Eic) = 1 - P(Ei) = 1 - 1 = 0

i=1 i=1

i=1

Consequently, we have i.e.

P(E) = 1 - P(Ec) = 1 - 0 = 1 P lim F D = F  = 1
|D|

Proof. Theorem 2
Let K = |Y| × |Y|. As in the previous proof, we enumerate the entries of C^B(h) and C^D(h) as i,B(h) and i(h) (i = 1, ..., K). Let Ei (i = 1, ..., K) denote the event that
lim F i,B(h) = F i(h)
|B|
and E denote the event that lim F B(h) = F (h)
|B|

If we have which is equivalent to

K
Ei  E
i=1
i = 1, ..., K, lim i,D(h) = i(h)
|D|
 lim F D(h) = F i(h)
|D|

(10) (11)

10

Under review as a conference paper at ICLR 2019

then the theorem will follow from an argument similar to that in the proof of theorem 3.
We now prove this condition. By chain rule, F D(h) = F D(h) · JD
where F D(h) denotes the gradient of F w.r.t. D(h) and J () denotes the Jacobian of D  h w.r.t. . By continuity of F ,

i = 1, ..., K, lim i,D(h) = i(h)  lim D(h) = (h)

|D|

|D|

 lim F D(h) = F (h)
|D|

Thus

, NF,, |D| > NJ,  | | = F D(h) - F (h)

 <
3

By definition of Jacobian,

 1,D (h )

JD() =

...

 K,D (h )

Thus

i = 1, ..., K, lim i,D(h) = i(h) 
|D|

where

 D (h )

JD() =

...

 D (h )

and the convergence is w.r.t. Euclidean norm, defined as

lim JD() = J ()
|D|

Thus , NF,, |D| > NJ,  | | =
where || · || denotes the matrix norm defined as

JD() - J ()

 <
3

||A|| = sup |Ax|
|x|1

Thus for any D such that |D| > max NF,, NJ, , by definition of matrix norm we have

F D(h) - F (h) = F D(h) · JD() - F (h)

= F (h) + J () + E - F (h)

= F (h) J () + F (h) · E + J () + E - F (h)

which implies that

= F (h) · E + J () + E  F (h) · E + J () + E  F (h) E + | | J () + | | E ++
333 =
lim F D(h) = F (h)
|D|

11

Under review as a conference paper at ICLR 2019

Proof. Theorem 3 This theorem follows from Theorem 1.

Proof. Theorem 4

To prove consistency, it suffices to prove that ( Vapnik (1992)):

lim P sup F (C^D(h)) - F (C¯(h)) < = 1

|D|



Because F is well behaved, by the union bound argument in previous proofs, it suffices to show that

lim P sup

|D|



C^D ij (h) - C¯ij (h) <

=1

1  i, j  |Y|

which is true if p has finite VC-dimension (see, for example, Koyejo et al. (2014)). The VCdimension of p can be limited to finite by various means. One way is to limit the outputs of p to discrete values, e.g. all float point values between 0 and 1 that can be represented in a computer.

12

Under review as a conference paper at ICLR 2019

WELL BEHAVED PERFORMANCE METRICS

· accuracy = tp + tn

· F -measure

· Q-Mean = 1 -

(1-tp)2 +(1-tn)2 2



· G-Mean = tp · tn

·

Jaccard =

tp tp+fp+fn

·

AUC =

fp·fn (tp+fn)(fp+tn)

· Min = min{tp, tn}

These performance metrics can be extended to multi-class classification in the same way that F measure is extended to multi-class classification.

13

