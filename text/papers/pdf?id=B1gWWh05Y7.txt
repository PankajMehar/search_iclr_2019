Under review as a conference paper at ICLR 2019
EXPLORATION IN POLICY MIRROR DESCENT
Anonymous authors Paper under double-blind review
ABSTRACT
Policy optimization is a core problem in reinforcement learning. In this paper, we investigate Reversed Entropy Policy Mirror Descent (REPMD), an on-line policy optimization strategy that improves exploration behavior while assuring monotonic progress in a principled objective. REPMD conducts a form of maximum entropy exploration within a mirror descent framework, but uses an alternative policy update with a reversed KL projection. This modified formulation bypasses undesirable mode seeking behavior and avoids premature convergence to sub-optimal policies, while still supporting strong theoretical properties such as guaranteed policy improvement. An experimental evaluation demonstrates that this approach significantly improves practical exploration and surpasses the empirical performance of state-of-the art policy optimization methods in a set of benchmark tasks.
1 INTRODUCTION
Model-free deep reinforcement learning (RL) has recently been shown to be remarkably effective in solving challenging sequential decision making problems (Schulman et al., 2015; Mnih et al., 2015; Silver et al., 2016). A central method of deep RL is policy optimization (aka policy gradient), which is based on formulating the problem as the optimization of a stochastic objective (expected return) with respect to the underlying policy parameters (Williams & Peng, 1991; Williams, 1992; Sutton et al., 1998). Unlike standard optimization, stochastic optimization requires the objective and gradient to be estimated from data, typically gathered from a process depending on current parameters, simultaneously with parameter updates. Such an interaction between estimation and updating complicates the optimization process, and often necessitates the introduction of variance reduction methods, leading to algorithms with subtle hyperparameter sensitivity. Joint estimation and updating can also create poor local optima whenever sampling neglect of some region can lead to further entrenchment of a current solution. For example, a non-exploring policy might fail to sample from high reward trajectories, preventing any further improvement since no useful signal is observed. In practice, it is well known that successful application of deep RL techniques requires a combination of extensive hyperparameter tuning, and a large, if not impractical, number of sampled trajectories. It remains a major challenge to develop methods that can reliably perform policy improvement while maintaining sufficient exploration and avoiding poor local optima, yet do so quickly.
Several ideas for improving policy optimization have been proposed in the literature, generally focusing on the goals of improving stability and data efficiency (Peters et al., 2010; Van Hoof et al., 2015; Fox et al., 2015; Schulman et al., 2015; Montgomery & Levine, 2016; Nachum et al., 2017b;c; Tangkaratt et al., 2017; Abdolmaleki et al., 2018; Haarnoja et al., 2018). Unfortunately, a notable gap remains between empirically successful methods and their underlying theoretical support. Current analyses typically assume a simplified setting that either ignores the policy parametrization or only considers linear models. These assumptions are hard to justify when current practice relies on complex function approximators, such as deep neural networks, that are highly nonlinear in their underlying parameters. This gulf between theory and practice is a barrier to wider adoption of model-free policy gradient methods.
In this paper, we focus on a setting where the policy can be a non-convex function of its parameters. We begin by considering the entropy-regularized expected reward objective, which has recently re-emerged as a foundation for state-of-the-art RL methods (Williams & Peng, 1991; Fox et al., 2015; Schulman et al., 2017a; Nachum et al., 2017b; Haarnoja et al., 2017). Our first contribution is to reformulate the maximization of this objective as a lift-and-project procedure, following the lines of Mirror Descent (Nemirovskii et al., 1983; Beck & Teboulle, 2003). Such a reformulation achieves
1

Under review as a conference paper at ICLR 2019

two things. First, it makes it easier to analyze policy optimization in the parameter space: using this reformulation we can establish a monotonic improvement guarantee with a fairly simple proof, even in a non-convex setting. We also provide a study of the fixed point properties of this setup. Second, the proposed reformulation has practical algorithmic consequences, suggesting, for example, that multiple gradient descent updates should be performed in the projection step. These considerations lead to our first practical algorithm, Policy Mirror Descent (PMD), which first lifts the policy to the entire policy-simplex, ignoring the constraint induced by its parametrization, then approximately solves the projection step by multiple gradient descent updates to the policy in the parameter space.
We then investigate additional modifications to mitigate the potential deficiencies of PMD. The main algorithm we propose, Reversed Entropy PMD (REPMD), incorporates both an entropy and relative entropy regularizer, and uses the mean seeking direction of KL divergence for projection. The benefit of this approach is twofold. First, using just the mean seeking direction of KL divergence for the projection step helps avoids poor local optima; second, this specific problem can now be efficiently solved to global optimality in certain non-trivial non-convex scenarios, such as one-layer-softmax networks. Similar guarantees can be proved for REPMD, which additionally incorporates entropy regularization, with respect to a surrogate objective SR(). We further study the properties of SR() and provide theoretical and empirical evidence that SR can effectively guide the search for good policies. Finally, we also show how this algorithm can be extended with a value function approximator, and develop an actor-critic version that is effective in practice.

1.1 NOTATION AND PROBLEM SETTING

For simplicity, we only consider finite horizon settings with finite state and action spaces. The

behavior of an agent is modelled by a policy (a|s) that specifies a probability distribution over a

finite set of actions given an observed state. At each time step t, the agent takes an action at by sampling from (at|st). The environment then returns a reward rt = r(st, at) and the next state

st+1 = f (st, at), where f is the transition function not revealed to the agent. Given a trajectory, a

sequence of states and actions  = (s1, a1, . . . , aT -1, sT ), the policy probability and the total reward

of  are defined as () =

T -1 t=1

(at|st)

and

r()

=

T -1 t=1

policy functions   , policy optimization aims to find the

r(st, at). Given a optimal policy 

set by

of parametrized maximizing the

expected reward,





arg max E
  

r(),

(1)

We use  {|  () = 1, ()  0, } to refer to the probability simplex over all possible trajectories. Without loss of generality, we also assume that the state transition function is deterministic, and the discount factor  = 1. This same simplification is also assumed in Nachum et al. (2017a).
Results for the case of a stochastic state transition function are presented in Appendix D.

2 POLICY MIRROR DESCENT

We begin with the development of the basic Policy Mirror Descent (PMD) strategy, which will form the basis for our subsequent algorithms and their analyses. As mentioned in the introduction, our analysis of this and subsequent methods focuses on the non-convex setting.
Consider the following local optimization problem: given a reference policy ¯ (usually the current policy), maximize the proximal regularized expected reward, using relative entropy as the regularizer:



=

arg max E
  

r()

-

 DKL(

¯).

(2)

Relative entropy regularization has been widely investigated in online learning and constrained
optimization (Nemirovskii et al., 1983; Beck & Teboulle, 2003), primarily as a component of the mirror descent algorithm. Observe that when ¯ is the uniform distribution, Eq. (2) reduces to entropy
regularized expected reward. It is important to note that, since we are interested in the non-convex setting and only assume that  is parametrized as a smooth function of   Rd,  is generally a non-convex subset of the simplex. Therefore, Eq. (2) is a difficult constrained optimization problem.

2

Under review as a conference paper at ICLR 2019

A useful way to decompose this constrained optimization is to consider an alternating lift-and-project procedure that isolates the different computational challenges.

(Project Step) (Lift Step)

arg min DKL( ¯),
 

where

¯

=

arg max


E


r()

-



DKL(

¯).

(3)

Crucially, the reformulation Eq. (3) remains equivalent to the original problem Eq. (2), in that it preserves the same set of solutions, as established in Proposition 1.

Proposition 1. Given a reference policy ¯,

arg max
 

E


r()

-



DKL(

¯) = arg min DKL(
 

¯).

Note that this result holds even for the non-convex setting. The reformulation Eq. (3) immediately leads to our PMD algorithm: Lift the current policy t to ¯, then perform multiple steps of gradient descent in the Project Step to update t+1 .1
When  is a convex set, one can show that PMD asymptotically converges to the optimal policy (Nemirovskii et al., 1983; Beck & Teboulle, 2003). The next proposition shows that despite the non-convexity of , PMD still enjoys desirable properties.
Proposition 2. Let t denote the policy produced at step t of the update sequence. Then PMD satisfies the following properties for an arbitrary parametrization of .

1.

(Monotonic

Improvement

Guarantee)

If

the

Project

Step

min DKL(
 

¯)

can

be

solved globally optimally, then

Et+1 r() - Et r()  0.

2.

(Global Optimum Guarantee) If the Project Step min DKL(
 

¯) can be solved glob-

ally optimally, then t converges to the global optimum , i.e.

lim
t

Et

r()

=

E

r()



E

r(),

  .

3. (Fixed Points) Assume that the Project Step is optimized by gradient descent, then the fixed points of PMD are the stationary points of the expected reward E r().

Despite these desirable properties, Proposition 2 relies on the condition that the Project Step in PMD is solved to global optimality. It is usually not practical to achieve such a stringent requirement when  is not convex in , limiting the applicability of Proposition 2.

Another shortcoming with this naive strategy is that PMD typically gets trapped in poor local optima.

Indeed, while the relative entropy regularizer help prevent a large policy update, it also tends to limit

exploration. Moreover, (Murphy, 2012), which

minimizing the KL divergence DKL( ¯) is known to be can lead to mode collapse during the learning process. Once a

mode seeking policy ¯ has

lost important modes, learning can easily become trapped at a sub-optimal policy. Unfortunately, at

such points, the relative entropy regularizer does not encourage further exploration.

3 REVERSED ENTROPY POLICY MIRROR DESCENT
We now introduce two modifications to PMD that overcome its aforementioned deficiencies. These two modifications lead to our main proposed algorithm, Reversed Entropy Policy Mirror Descent (REPMD), which retains desirable theoretical properties while achieving vastly superior performance to PMD in practice. (We consider some additional refinements to REPMD in the next section.)
1 To estimate this gradient one would need to use self-normalized importance sampling Owen (2013). We omit the details here since PMD is not our main algorithm; similar techniques can be found in the implementation of REPMD.

3

Under review as a conference paper at ICLR 2019

The first modification is to add an additional entropy regularizer to the Lift Step, to improve the exploration behavior of the algorithm. The second modification is to use a reversed, mean seeking direction of the KL divergence in the Project Step. In particular, the REPMD algorithm solves the following alternating optimization problems to update the policy t+1 at each iteration:

(Project Step) (Lift Step)

arg min DKL(¯, ),
 

where ¯,

=

arg max


E


r()

-



DKL(

t ) +  H().

(4)

The idea of optimizing the reverse direction of KL divergence has proved to be effective in previous work, such as reward augmented maximum likelihood (Norouzi et al., 2016) and UREX (Nachum et al., 2017a). Its mean seeking behavior further encourages exploration by the algorithm. As we will see in Section 5, REPMD outperforms PMD significantly in experimental evaluations.

Theorem 1 shows that REPMD still enjoys similar desirable properties to PMD in the non-convex setting, but with respect to a surrogate reward SR(), which we analyze further below.
Theorem 1. Let t denote the policy produced at step t of the update sequence. REPMD satisfies the following properties for an arbitrary parametrization of .

1. (Monotonic Improvement Guarantee) If the Project Step DKL(¯, globally optimally, then

SR(t+1 ) - SR(t )  0,

where

SR() ( +  ) log

exp r() +  log ()  +



) can be solved . (5)

2. (Global Optimum Guarantee) If the Project Step can be solved globally optimally, then t converges to the global optimum , i.e.

lim
t

SR(t

)

=

SR()



SR(),

  .

3. (Fixed points) Assume that the Project Step is optimized by gradient descent, then the fixed points of REPMD are the stationary points of the expected reward SR().
A key question is the feasibility of solving the Project Step to global optimality. It is obvious that when () =    the Project Step is a convex optimization problem, hence can be solved optimally. Furthermore, as shown in Proposition 3, for a one-layer-softmax neural network , the Project Step DKL(¯, ) can also still be solved to global optimality, affording a significant computational advantage over PMD.
Proposition 3. Assume (s) = softmax(s ). Given a reference policy ¯, the Projection Step minRd DKL(¯ ) is a convex optimization problem in .

3.1 LEARNING ALGORITHMS

We now provide some of the learning algorithms for REPMD. Despite its non-convexity, the Lift Step has an analytic solution,

¯, ()

¯() exp

r()- log ¯()  +

 ¯( ) exp

r( )- log ¯( )  +

.

(6)

The Project Step in Eq. (4), min DKL(¯, ), can be optimized via stochastic gradient

descent, given that one can sample trajectories from ¯, to estimate its gradient. To see this, note

that arg min DKL(¯,

 )

=

arg

min 

E¯ ,

- log ().

The next lemma shows

that sampling from ¯, can be done using self-normalized importance sampling (Owen, 2013) when

it is possible to draw multiply samples, following the idea of UREX (Nachum et al., 2017a).

4

Under review as a conference paper at ICLR 2019

Algorithm 1 The REPMD algorithm
Input: temperature parameters  and  , number of samples for computing gradient K 1: Random initialized  2: For t = 1, 2, . . . do 3: Set ¯ =  4: Repeat 5: Sample a mini-batch of K trajectories from ¯ 6: Compute the gradient according to Eq. (7) 7: Update  by gradient descent 8: Until converged or reach maximum of training steps

Lemma 1.

Let k

=

r(k

)- log  +

¯ (k

)

.

Given K

i.i.d.

samples {1, . . . , K } from the reference

policy ¯, we have the following unbiased gradient estimator,

K
DKL(¯, )  -
k=1

exp {k}

K j=1

exp

{j

}



log



(k

),

(7)

Derivation for the analytic solution of the Lift Step and above Lemma as well as other implementation details can be found in Appendix B.

3.2 BEHAVIOR OF SR()
Theorem 1 only establishes desirable properties for REPMD with respect to SR(), but not necessarily r(). In this section we present theoretical and empirical evidence that SR() is in fact a reasonable surrogate that can provide good guidance for learning, even when targeting desirable behavior with respect to r(). In fact, by properly adjusting the two temperature parameters  and  in REPMD, the resulting surrogate objective SR() recovers existing performance measures.
Proposition 4. SR() satisfies the following properties:
(i) SR()  max r(), as   0,   0.
(ii) SR()  E r(), as   ,   0.
Remark 1. Note that SR() also resembles the "softmax value function" that has become popular in value based RL (Nachum et al., 2017b; Haarnoja et al., 2018; Ding & Soricut, 2017). The standard softmax value can be recovered by SR() as a special case when  = 0 or  = 0.
According to Proposition 4, one should gradually decrease  to reduce the level of exploration as sufficient reward landscape information is collected during the learning process. Different choices can be made for  , depending on the policy constraint set . Given   0 and a sufficiently explored reward landscape, the resulting unconstrained policy must satisfy ¯,   as   0, where  is the globally optimal deterministic policy. Therefore, in the Project Step,  is obtained by directly projecting  into . When the policy constraint  has nice properties that support good behavior of KL projection, such as convexity,  will achieve good performance. However, in practice,  is typically non-convex, and setting   0 might not work very well, since directly projecting  into  will not always lead to a  with large expected reward.
On the other hand, as   , the stationary point of SR() will approach the stationary point of  ()r(). Fig. 1 shows the a simulation investigating the behavior of SR() for different  . Note that there is a poor local maximum for  < 0, where naive gradient method will converge to if initialization is about  < -10. When  = 0.05, the reward landscape of SR() guides  to converge to the neighbourhood of 0, thus helping avoid the poor local maximum. Later in training, when  increases to 5, SR() recovers the true expected reward landscape, ensuring that  will converge to a good local (if not the global) maximum. While it is hard for a simulation study to be comprehensive, these results demonstrate how SR() can offer reasonable guidance for maximizing the true expected reward. Further investigation on the behavior of SR() is left for future work.

5

Under review as a conference paper at ICLR 2019

reward surrogate reward surrogate reward surrogate reward

0.2 4 1.5 0.2

0 3.5 1 0

-0.2 3

-0.2

2.5 0.5

-0.4 -0.4

20

-0.6 1.5

-0.6

-0.8 1 -0.5 -0.8

-1-30 -20 -10

0

10 20 30 0.5-30 -20 -10 0

10 20 30

-1-30 -20 -10

0

10 20 30 -1-30 -20 -10 0

10 20 30

True reward

Figure 1: Simulation results for the true reward landscape and SR() with different values of  in a bandit setting with 10,000 actions. Each action is represented by a feature   R. Let  = (1, . . . , 10,000) be the feature vector. The policy is parameterized by a weight scalar   R.
The policy is defined by softmax( ). True Reward landscape shows the expected reward as a
function of . The rest figures show SR() as a function of  with different value of  .

4 AN ACTOR-CRITIC EXTENSION

Finally, we develop a natural extension of REPMD to an actor-critic formulation by incorporating a value function approximator. We refer to this final algorithm as Policy Mirror Actor-Critic (PMAC).

It is well known that data efficiency of policy-based methods can be generally improved by adding a value-based critic. For a reference policy ¯ and an initial state s, recall that the objective in the Lift Step of REPMD is

OREPMD(,

s)

=

E


r()

-



DKL(

¯) + 

H(),

where  = (s1 = s, a1, s2, a2, . . .). To incorporate value function approximation, we need to derive the temporal consistency structure for this objective, which can be done as follows. First, write

OREPMD(, s) = Ea(·|s) [r(s, a) + OREPMD(, s ) +  log ¯(a|s) - ( +  ) log (a|s)] . Let ¯, (·|s) = arg max OREPMD(, s) denote the optimal policy on state s. Further denote the soft optimal state-value function OREPMD(¯, (·|s), s) by V¯, (s), and let Q¯, (s, a) = r(s, a) + V¯, (s ) be the soft-Q function. It can then be verified that

V¯, (s) = ( +  ) log

exp Q¯, (s, a) +  log ¯(a|s)  +

a

¯, (a|s) = exp

Q¯, (s, a) +  log ¯(a|s) - V¯, (s)  +

.

;

(8)

We propose to train a soft state-value function V parameterized by , a soft Q-function Q parameterized by , and a policy  parameterized by , based on Eq. (4). The update rules for these parameters can be derived as follows.
The soft state-value function approximates the soft optimal state-value V¯, . Note that we can re-express V¯, by

V¯, (s) =( +  ) log Ea¯ exp

Q¯, (s, a) -  log ¯(a|s)  +

.

This suggests a Monte-Carlo estimate for V¯, (s): by sampling one single action a according to the reference policy ¯, we have V¯, (s)  Q¯, (s, a) -  log ¯(a|s). Then, given a replay buffer D, the soft state-value function can be trained to minimize the mean squared error,

L() = EsD

1 2

(V(s)

-

[Q (s,

a)

-



log ¯(a|s)])2

.

(9)

6

Under review as a conference paper at ICLR 2019

Figure 2: Results of MENT (red), UREX (green), and REPMD (blue) on synthetic bandit problem and algorithmic tasks. Plots show average reward with standard error during training. Synthetic bandit results averaged over 5 runs. Algorithmic task results averaged over 25 random training runs (5 runs × 5 random seeds for neural network initialization). X-axis is number of sampled trajectories.

One might note that, in principle, there is no need to include a separate state-value approximation, since it can be directly computed from a soft-Q function and reference policy, using Eq. (8). However, including a separate function approximator for the state-value can help stabilize the training (Haarnoja et al., 2018). The soft Q-function parameters  is then trained to minimize the soft Bellman error using the state-value network,

L() = E(s,a,s )D

1 2

(Q (s,

a)

-

[r(s,

a)

+

V(s

)])2

(10)

The policy parameters are updated by performing the Project Step in Eq. (4) with stochastic gradient descent,

L() = EsD DKL

exp

Q(s, ·) +  log ¯(·|s) - V(s)  +

 (·|s)

(11)

where we approximate ¯, by the soft-Q and state-value function approximations.

Finally, we also use a target state-value network (Lillicrap et al., 2015) and the trick of maintaining two soft-Q functions (Haarnoja et al., 2018; Fujimoto et al., 2018). Implementation details for PMAC are given in Appendix C.

5 EXPERIMENTS
We evaluate the main proposed methods, REPMD and PMAC, on a number of benchmark tasks against strong baseline methods. Additional implementation details are provided in Appendix E.2.
5.1 SETTINGS
We first investigate the performance of REPMD on a synthetic bandit problem and the algorithmic task suite from the OpenAI gym (Brockman et al., 2016). The synthetic multi-armed bandit problem has 10,000 distinct actions, where the reward of each action i is initialized by ri = s8i such that si is randomly sampled from a uniform [0, 1) distribution. Each action i is represented by a randomly sampled feature vector i  R20 from standard normal distribution. Note that these features are fixed during training. We further test our method on five algorithmic tasks from the OpenAI gym library, in rough order of difficulty: Copy, DuplicatedInput, RepeatCopy, Reverse, and ReversedAddition (Brockman et al., 2016). Second, we test the second PMAC approach on continuous-control benchmarks from OpenAI Gym, utilizing the Mujoco environment (Brockman et al., 2016; Todorov et al., 2012); including Hopper, Walker2d, HalfCheetah, Ant and Humanoid. The details of the algorithmic and Mujoco tasks are provided in Appendix E.1.

7

Under review as a conference paper at ICLR 2019
Figure 3: Learning curves of DDPG (red), TD3 (yellow), SAC (green) and PMAC (blue) on Mujoco tasks (with SAC+R (gray) added on Humanoid). Plots show mean reward with standard error during training, averaged over five different instances with different random seeds. X-axis is millions of environment steps.
Note that only cumulative rewards are available in both the synthetic bandit and algorithmic tasks. Therefore, value-based RL algorithms can not be applied in this setting. This restriction compels us to compare REPMD against REINFORCE with entropy regularization (MENT) (Williams, 1992), and under-appreciated reward exploration (UREX) (Nachum et al., 2017a). To the best of our knowledge, these are the state-of-the-art policy-based algorithms for these algorithmic tasks. For the continuous control tasks, we compare the second PMAC approach to deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015), an efficient off-policy deep RL methods; twin delayed deep deterministic policy gradient algorithm (TD3) (Fujimoto et al., 2018), a recent extension to DDPG by applying the double Q-learning trick to address over-estimation problem when function approximations are adopted; and Soft-Actor-Critic (SAC) (Haarnoja et al., 2018), a recent state-of-the-art off-policy algorithm on a number of benchmarks. All of these algorithms are implemented in rlkit.2 We do not include TRPO and PPO in these experiments, as their performance is dominated by SAC and TD3, as shown in (Haarnoja et al., 2018; Fujimoto et al., 2018).
5.2 COMPARATIVE EVALUATION
The results on synthetic bandit problem and algorithmic tasks are reported in Fig. 2. It is clear that REPMD substantially outperforms the baseline methods on these tasks. REPMD appears able to consistently achieve a higher score and learn substantially faster than UREX. We also find the performance of UREX is unstable. On the difficult tasks, including RepeatCopy, Reverse and ReversedAddition, UREX only finds solutions a few times out of 5 runs for each random seed, which brings the overall scores down. This observation explains the gap between the results we find here and those reported in Nachum et al. (2017a).3 Note that the performance of REPMD is sill significantly better than UREX even compared to the results reported in Nachum et al. (2017a). Fig. 3 presents the results on the continuous-control benchmarks, reporting the total mean returns on evaluation rollouts obtained by the algorithms during training. These results are averaged over five different instances with different random seeds. The solid curves corresponds to the mean and the shaded region to the standard errors over the five trials. To ensure a fair comparison with PMAC, we implemented the double-Q but not the reparameterization trick for SAC (see equation (11)-(13) in (Haarnoja et al., 2018)). This difference explains the discrepancy between the results we see here and those reported in (Haarnoja et al., 2018). We observe that without the reparameterization trick SAC cannot make any progress in the hardest problem Humanoid. Therefore, to gain further clarity,
2 https://github.com/vitchyr/rlkit 3 The results reported in (Nachum et al., 2017a) are averaged over 5 runs of random restarting, while our results are averaged over 25 random training runs (5 runs × 5 random seed for neural network initialization).
8

Under review as a conference paper at ICLR 2019
Figure 4: Ablation Study of REPMD and PMAC on ReversedAddition and Ant.
we also report the result of SAC with the reparameterization trick, denoted SAC+R, on Humanoid using the author's GitHub implementation. The results show that PMAC matches or, in many cases, surpasses all other baseline algorithms in both final performance and sample efficiency across tasks, except compared to SAC+R in Humanoid. In Humanoid, although SAC+R outperforms PMAC, its final performance is still comparable with SAC+R. Note that the reparameterization trick that benefits SAC could also be applied in PMAC; we have yet to add this modification and will add this to PMAC in future work.
5.3 ABLATION STUDY
The comparative evaluations provided in the previous sections suggest that our proposed algorithms outperform conventional RL methods on a number of challenging benchmarks. In this section, we further investigate how each novel component of Eq. (4) improves learning performance, by performing an ablation study on ReversedAddition and Ant. Importance of entropy regularizer. The main difference between the objective in Eq. (4) and the PMD objective Eq. (3) is the entropy regularizer. We demonstrate the importance of this choice by presenting the results of REPMD and PMAC without the extra entropy regularizer, i.e.  = 0. Importance of KL divergence projection. Another important difference between Eq. (4) with other RL methods is to use a Project Step to optimize the policy, rather than direct SGD of the objective function. To show the importance of the Project Step, we test REPMD and PMAC without projection, which only performs one step of gradient update at each iteration of training. Importance of direction of KL divergence. We choose PMD Eq. (3) as another baseline to prove the effectiveness of using the mean seeking direction of KL divergence in the project step. Similar to REPMD, we add a separate temperature parameter  > 0 to the original objective function in Eq. (3) to encourage policy exploration, which gives arg max E r() -  KL( ¯) +  H(). We name this algorithm PMD+entropy. The corresponding algorithms in the actor-critic setting, named PMD-AC and PMD-AC+entropy, are also implemented for comparison. The results are presented in Fig. 4, which clearly indicate all of the three major components of Eq. (4) are helpful for achieving better performance.
6 RELATED WORK
The lift-and-project approach is distinct from the previous literature on policy search, with the exception of a few recent works: Mirror Descent Guided Policy Search (MDGPS) (Montgomery & Levine, 2016), Guide Actor-Critic (GAC) (Tangkaratt et al., 2017), Maxmimum aposteriori (MPO) (Abdolmaleki et al., 2018), and Soft Actor-Critic (SAC) (Haarnoja et al., 2018). These approaches also adopt a mirror descent framework, but differ from the proposed approach in key aspects. MDGPS (Montgomery & Levine, 2016) follows a different learning principle, using the Lift Step to learn multiple local policies (rather than a single policy) then aligning these with a global policy in the Project Step. MDGPS also does not include the entropy term in the Lift objective, which we have found to be essential for exploration. MPO (Abdolmaleki et al., 2018) also neglects to add the additional entropy term; Section 5.3 shows that entropy regularization with an appropriate annealing of  significantly improves learning efficiency. Both GAC and SAC use the mode seeking direction of KL divergence in the Project Step, in opposition to the mean seeking direction we consider here
9

Under review as a conference paper at ICLR 2019
(Tangkaratt et al., 2017; Haarnoja et al., 2018). Additionally, SAC only uses entropy regularization in the Lift Step, neglecting the proximal relative entropy. The benefits of regularizing with relative entropy has been discussed in TRPO (Schulman et al., 2015) and MPO (Abdolmaleki et al., 2018), where it is noted that proximal regularization significantly improves learning stability. GAC seeks to match the mean of Gaussian policies under second order approximation in the Project Step, instead of directly minimizing the KL divergence with gradient descent. Although one might also attempt to interpret "one-step" methods in terms of lift-and-project, these approaches would obliviously still differ from REPMD, given that we use different directions of the KL divergence for the Lift and Project steps respectively.
Regarding the optimization objective, several existing methods have considered related approaches, either by considering (relative) entropy regularization during policy search, or directly using KL divergence as the target objective. As noted in Section 2, REPMD resembles policy gradient methods that maximize expected reward with an additional entropy regularizer (Williams & Peng, 1991; Fox et al., 2015; Nachum et al., 2017b). Using KL divergence (or Bregman divergences more generally) as regularizers has also been explored in Liu et al. (2015); Thomas et al. (2013); Mahadevan & Liu (2012). However, these approaches differ from the proposed method in important ways. In particular, they apply regularization to the parameters of the linear approximated value functions, whereas here KL regularization is applied directly to the policy space. The literature on relative entropy policy search also uses a similar KL divergence regularization scheme (Peters et al., 2010; Van Hoof et al., 2015), but on joint state-action distributions. Instead of KL divergence, Reward-Weighted Regression (RWR) uses a log of the correlation between  and , which is then approximated similar to a cross entropy loss (Peters & Schaal, 2007; Wierstra et al., 2008).
TRPO and PPO also have a similar formulation to Eq. (2), using a constrained version with a mean seeking KL divergence Schulman et al. (2015; 2017b). Our proposed method includes additional modifications that, as shown in Section 5, significantly improve performance. UREX also uses mean seeking KL for regularization, which encourages exploration but also complicates the optimization; as shown in Section 5, UREX is significantly less efficient than the method proposed here.
Trust PCL adopts the same objective defined in Eq. (4), including both entropy and relative entropy regularization (Nachum et al., 2017c). However, the policy update strategy is substantially different: while REPMD uses KL projection, Trust PCL minimizes a path inconsistency error (inherited from PCL) between the value and policy along observed trajectories (Nachum et al., 2017b). Although policy optimization by minimizing path inconsistency error can efficiently utilize off-policy data, this approach loses the desirable monotonic improvement guarantee.
In terms of existing theoretical analyses, similar monotonic improvement guarantee exists for TRPO, but only for an impractical formulation.4 Here, by contrast, we use the lift-and-project reformulation to establish a monotonic improvement guarantee in a simple and direct way. MPO provides a guarantee on the regularized reward of a non-parametric policy, but this depends on the assumption that a non-convex optimization problem can be solved globally. SAC obtains a similar result with respect to the optimal achievable Q-values, again relying on an assumption that a non-convex optimization problem can be solved globally. We have shown that similar results hold in the case of PMD/REPMD, but here we have also established something stronger: when the projection step cannot be efficiently solved to global optimality, we have shown that PMD/REPMD still preserve stationary points in their respective, principled objectives. MPO and SAC have no such guarantee, as far as we are aware.
7 CONCLUSION AND FUTURE WORK
We have proposed reversed entropy policy mirror descent (REPMD) as an effective new approach for policy based reinforcement learning that also guarantees monotonic improvement in a well motivated objective. We show that the resulting method achieves better exploration than both a directed exploration strategy (UREX) and undirected maximum entropy exploration (MENT). It will be interesting to further extend the follow-on PMAC actor-critic framework with further development of the value function learning approach.
4 For the monotonic improvement guarantee, TRPO must use DKmLax rather than the stanard KL divergence.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. In ICLR, 2018.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167­175, 2003.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Nan Ding and Radu Soricut. Cold-start reinforcement learning with softmax policy gradient. In Advances in Neural Information Processing Systems, pp. 2817­2826, 2017.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. arXiv preprint arXiv:1512.08562, 2015.
Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample analysis of proximal gradient td algorithms. In UAI, pp. 504­513. Citeseer, 2015.
Sridhar Mahadevan and Bo Liu. Sparse q-learning with mirror descent. arXiv preprint arXiv:1210.4893, 2012.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
William H Montgomery and Sergey Levine. Guided policy search via approximate mirror descent. In Advances in Neural Information Processing Systems, pp. 4008­4016, 2016.
Kevin P Murphy. Machine learning: a probabilistic perspective. Cambridge, MA, 2012.
Ofir Nachum, Mohammad Norouzi, and Dale Schuurmans. Improving policy gradient by exploring under-appreciated rewards. In ICLR, 2017a.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2772­2782, 2017b.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy trust region method for continuous control. In ICLR, 2017c.
Arkadii Nemirovskii, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity and method efficiency in optimization. 1983.
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, et al. Reward augmented maximum likelihood for neural structured prediction. In Advances In Neural Information Processing Systems, pp. 1723­1731, 2016.
11

Under review as a conference paper at ICLR 2019
Art B. Owen. Monte Carlo theory, methods and examples. 2013. Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational
space control. In Proceedings of the 24th international conference on Machine learning, pp. 745­750. ACM, 2007. Jan Peters, Katharina Mu¨lling, and Yasemin Altun. Relative entropy policy search. In AAAI, pp. 1607­1612. Atlanta, 2010. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889­1897, 2015. John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440, 2017a. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484­489, 2016. Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998. Voot Tangkaratt, Abbas Abdolmaleki, and Masashi Sugiyama. Guide actor-critic for continuous control. arXiv preprint arXiv:1705.07606, 2017. Philip S Thomas, William C Dabney, Stephen Giguere, and Sridhar Mahadevan. Projected natural actor-critic. In Advances in neural information processing systems, pp. 2337­2345, 2013. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012. Herke Van Hoof, Jan Peters, and Gerhard Neumann. Learning of non-parametric control policies with high-dimensional state features. In Artificial Intelligence and Statistics, pp. 995­1003, 2015. Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Episodic reinforcement learning by logistic reward-weighted regression. In International Conference on Artificial Neural Networks, pp. 407­416. Springer, 2008. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229­256, 1992. Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241­268, 1991.
12

Under review as a conference paper at ICLR 2019

A PROOFS

A.1 PROOF OF PROPOSITION 1

Proof. Note that - DKL( ¯) = -  () log () + 
Z¯ = E r() -  DKL( ¯) - Z¯ . Note the fact that Z¯ indenpendent of  given the reference policy ¯.

 ()(log ¯() + r()/ ) -  log  ¯() exp {r()/ } is

A.2 PROOF OF PROPOSITION 2

Proof. (Monotonic DKL(t+1 ¯) =

Improvement Guarantee) By min DKL( ¯)  DKL(t

the ¯).

definition of By expanding

t+1 , the KL

note that divergence

and rearranging terms, we have  DKL(t+1 t ) -  t+1 ()r()  -  t ()r(), which

gives Et+1 r() - Et r()   DKL(t+1 t )  0.

(Global Optimum Guarantee) By the Monotonic Improvement Guarantee proved above, the

sequence show 

Et r() is is the optimal

monotonically increased. policy. According to the

Let  be algorithm,

the converged fixed policy, we at convergence it must be that

  ,  = , DKL( ¯)  DKL( ¯). Using the same argument above we have   ,  = , E r()  E r(). Hence  is optimal in .

(Fixed Points) The stationary point of E r() is the  which satisfies,

r() · d() = 0 d




log

 ()

-

log

 ()

-

r() 

· d() = 0 d



( > 0)

(12)



[log ()

-

log

{() exp {r()/ }}]

·

d () d

=

0.



On the other hand, the fixed point of PMD indicates at some iteration t,

t = t+1 , where t -L-if-t S-tep ¯ -P-ro-je-ct-S-tep t+1 in Eq. (3),

which means t is the solution of the Project Step,

dDKL( ¯)

=0

d =t



[log

 ()

-

log

¯()

+

1]

·

d () d

=0

 =t









 

log 



()

-

log

 



t () exp {r()/ }

 
+ 1 ·

t (

)

exp

{r(

)/

}

 



d () d

=t

=

0

(by Lift Step in Eq. (13))

(13) (14)



[log

 ()

-

log

{t ()

exp

{r()/ }}

+

c]

·

d () d

= 0,

 =t

where we denote c = 1 + log t ( ) exp {r( )/ }. Note that for c independent of , we have,


c · d()

d =c·

d

 =t

 ()

= c · d1

= 0.

d =t

d =t

13

Under review as a conference paper at ICLR 2019

Therefore, Eq. (14) is equivalent with,



[log

 ()

-

log

{t () exp

{r()/ }}]

·

d () d

= 0.

 =t

(15)

Comparing Eq. (15) with Eq. (12), we have the fixed point condition of PMD is the same as the definition of the stationary point of E r().

A.3 PROOF OF THEOREM 1 Proof. (Monotonic Improvement Guarantee) Using DKL(¯, t+1 ) min DKL(¯, )  DKL(¯, t ) and Jensen's inequality,

SR(t+1 ) - SR(t ) = ( +  ) log


exp

r()+ log t+1 ()  +

exp

r()+ log t ()  +



=( +  ) log


exp

r()+ log t ()  +

exp

r()+ log t ()  +



· exp

 log t+1 () -  log t ()  +

=( +  ) log

¯, () · exp

 log t+1 () -  log t ()  +



( +  )

¯, () log exp

 log t+1 () -  log t ()  +



=



¯,

() log

t+1 () t ()

=



DKL(¯,

t ) - DKL(¯,

t+1 )  0.

=

(Global Optimum Guarantee) By the Monotonic Improvement Guarantee proved above, the sequence SR(t ) is monotonically increased. Let  be the converged fixed policy, we show  is the optimal policy. According to the algorithm, at convergence it must be that   ,  = He,nDceKL(¯is,optima)linDK. L(¯, ). Using the same argument above we have SR()  SR().
(Fixed Points) The stationary point of SR() is the  which satisfies,

exp  ( +  ) · 

dSR() = 0 d

r()+ log ()  +

·

  +

·

1  ()

·

d () d

 exp

r( )+ log ( )  +

=0

 -


() exp

r()- log ()  +

 ( ) exp

r( )- log ( )  +

· 1 · d() = 0. () d

( > 0)

(16)

On the other hand, the fixed point of REPMD indicates at some iteration t,

t = t+1 , where t -L-if-t S-tep ¯, -P-ro-je-ct-S-tep t+1 in Eq. (4),

(17)

14

Under review as a conference paper at ICLR 2019

which means t is the solution of the Project Step,

dDKL(¯, )

=0

d =t

 -


 -



¯,

() ·

1  ()

·

d () d

=0

=t

t () exp

r()- log t ()  +

 t ( ) exp

r( )- log t ( )  +

· 1 · d() () d

=0

=t

(by Lift Step in Eq. (17))

(18)

Comparing Eq. (18) with Eq. (16), we have the fixed point condition of REPMD is the same as the definition of the stationary point of SR().

A.4 PROOF OF PROPOSITION 3

Proof.

Note

that



=

1

exp{ }
exp{ }

,

where



is

the feature

matrix

and 

is the

policy

parameter.

Simply compute the Hessian matrix of the objective,

d2DKL(¯ d2

 )

= (() -  )

0.

Thus DKL(¯ ) is convex in .

A.5 PROOF OF PROPOSITION 4

Proof. To prove (i), note that as   0, SR()   log

 exp

r() 

, the standard softmax

value. Taking limit on  gives the hardmax value max r() as   0.

To prove (ii), we have

lim ( +  ) log exp
  

r() +  log ()  +

= lim  

( )

 ()

exp{

r()- log  +

 ()

}

r()-

log ()





()

exp{

r()- log  +



()

}

=



() [r() - 

log

 ()]

=

E


r()

+



H( )

As   0, SR()  E r().

B DETAILS OF REPMD LEARNING

This section provides some of the details of learning algorithms for REPMD. We first show the derivation of the analytic solution of the Lift Step.
Lemma 2. The lift step of Eq. (4) has the following closed form expression:

¯, ()

¯() exp

r()- log ¯()  +

 ¯( ) exp

r( )- log ¯( )  +

.

(19)

Proof. Rewrite the objective function defined in Eq. (4),

E


r()

-



DKL(

¯) + 

H() = E [r() +  log ¯()] + (


+

)H(),

(20)

which is an entropy regularized reshaped reward objective. The optimal policy of this objective can be obtained by directly applying Lemma 4 of Nachum et al. (2017b), i.e.

¯, ()  exp

r() +  log ¯()  +

= ¯() exp

r() -  log ¯()  +

.

(21)

15

Under review as a conference paper at ICLR 2019

The next lemma provides the derivation of the gradient estimation of REPMD (Lemma 1).

Proof. Note that

DKL(¯, ) = E¯

log ¯, () - log ()

=E
¯

¯, () ¯()

log ¯, () - log ()

.

,

Therefore, taking gradient on both sides,

 DKL (¯,

 )



-1 K

K k=1

¯, (k ¯ (k )

)



log (k)

1K =-

¯(k) exp

r(k)- log ¯(k)  +

K k=1 ¯(k)

 ¯( ) exp

r( )- log ¯( )  +

 log (k)

1K -
K1
k=1 K

exp{k}

K j=1

exp{j

}



log



(k

)

K
=-
k=1

exp {k}

K j=1

exp

{j

}



log



(k

).

by 19

Recall that in Algorithm 1 the project step is performed by SGD. In our implementation, the end condition of SGD is controlled by two parameters: > 0 and F STEP  {0, 1}. First, SGD halts if the change of the KL divergence is below or equal to . Second, F STEP decides the maximum number of SGD steps. If F STEP = 1, the maximum number is t at iteration t; while if F STEP = 0, there is no restriction on the maximum number of gradient steps, and stopping condition only depends on .

C DETAILS OF PMAC LEARNING

To increase the stability of the training, we include a target state value network V¯, where ¯ is an exponentially moving average of the value network weights . Different from Eq. (10), the soft
Q-function parameters  is then trained to minimize the soft Bellman error using the target state
value network,

L() = E(s,a,s )D

1 2

Q(s, a) - r(s, a) + V¯(s )

2

(22)

Our approach also use two soft-Q functions in order to mitigate the overestimation problem caused by value function approximation (Haarnoja et al., 2018; Fujimoto et al., 2018). Specifically, we apply two soft-Q function approximations, Q1 (s, a) and Q2 (s, a), and train them independently. The minimum of the two Q-functions will be used whenever the soft-Q value is needed.
The next lemma shows that the gradient of Eq. (11) can be computed by importance sampling using the reference policy,
Lemma 3. The gradient of Eq. (11) is,

L() = EsD Ea¯ exp

Q(s, a) -  log ¯(a|s) - V(s)  +

log (a|s)

.

(23)

16

Under review as a conference paper at ICLR 2019

Algorithm 2 The PMAC algorithm
Input: temperature parameters  and  , lag on target value network , number of training steps M 1: Initialize , V, V¯, Q1 , Q2 and replay buffer D 2: For t = 1, 2, . . . do
3: For each environment step do 4: a  (·|s) 5: Observe s and r from environment 6: D  D  {(s, a, r, s )}
7: Set ¯ =  8: For i = 1, . . . , M do 9: Sample a mini-batch of data {(sj, aj, rj, sj)}jB=1 from D 10: Compute gradient L(), L(), 1 L(1), 2 L(2)according to Eqs. (9) to (11) 11: Update parameters , , 1, 2 by gradient descent 12: Update ¯ by  + (1 - )¯

Proof. Let (a|s) = exp

Q(s,a)+ log ¯(a|s)-V(s)  +

, then we have,

L() =EsD

(a|s) log (a|s) - (a|s) log (a|s)

a

=EsD -

exp

Q(s, a) +  log ¯(a|s) - V(s)  +

log (a|s)

a

=EsD -

¯(a|s) exp

Q(s, a) -  log ¯(a|s) - V(s)  +

log (a|s)

a

=EsD Ea¯ - exp

Q(s, a) -  log ¯(a|s) - V(s)  +

log (a|s)

Pseudocode for PMAC is presented in Algorithm 2. The major difference between PMAC and
REPMD in the lift step is that instead of sampling K actions as described in Algorithm 1, PMAC only samples one action to construct ¯, , due to the fact both the soft-Q and state value function approximations are adopted. The value function approximations also make PMAC capable of using
off-policy data from a reply buffer. Furthermore, in the project step of PMAC, we use a fixed number
of iteration for SGD, which is given by an input parameter of the algorithm.

D STOCHASTIC TRANSITION SETTING
In Section 1.1, we assume that the state transition function is deterministic for simplicity. For completeness, we consider the general stochastic transition setting here.

D.1 NOTATIONS AND SETTINGS

Recall in Section 1.1, the policy probability of trajectory  = (s1, a1, . . . , aT -1, sT ) is denoted as

() =

T -1 t=1

(at

|st).

We define transition probability of  as f ()

T -1 t=1

f

(ss+1|st,

at).

The total probability of  under policy  and transition f is then p,f () ()f () =

T -1 t=1

(at

|st)f

(ss+1|st,

at).

We use f

{|  p,f () =  ()f () = 1, () 

0, f () > 0, } to refer to the probabilistic simplex over all possible trajectories. It is obvious that

p,f () = () and f =  under deterministic transition setting, i.e., f () = 1, .

17

Under review as a conference paper at ICLR 2019

D.2 REPMD OPTIMIZATION PROBLEM

The proposed REPMD algorithm solves Eq. (4) in the deterministic transition setting. In the stochastic setting, the corresponding problem is,

(Project Step) (Lift Step)

arg min DKL(p¯ ,f

 

,

p,f ),

where ¯,

= arg max E [r() - 
f p,f

log ()] -  DKL(p,f

pt ,f ).

(24)

which also recovers Eq. (4) as a special case when f () = 1, . Like Eq. (4), ¯, in Eq. (24) also has a closed form expression, Lemma 4. The unconstrained optimal policy of Eq. (24) has the following closed form expression:

¯, ()

¯() exp

r()- log ¯()  +

 ¯( )f ( ) exp

r( )- log ¯( )  +

.

Proof. Rewrite the maximization problem in Eq. (24) as (take t as the reference policy ¯),
maximize ()f () [r() - ( +  ) log () +  log ¯()]
 
subject to ()f () = 1.

The KKT condition of the above problem is,
f () [r() - ( +  ) log () +  log ¯() +  - ( +  )] = 0,  ()f () = 1.

Using f () > 0,  and solving the KKT condition, we obtain the expression of ¯, .

Lemma 4 recovers Lemma 2 as a special case when f () = 1, .

D.3 THEORETICAL ANALYSIS

In stochastic transition setting, we define the follow softmax approximated expected reward of 

SRf () ( +  ) log

f () exp r() +  log () ,  +



which recovers SR() when f () = 1, . The monotonic improvement property is for SRf (). Theorem 2. Assume that t is the update sequence of the REPMD algorithm in Eq. (24), then
SRf (t+1 ) - SRf (t )  0.

18

Under review as a conference paper at ICLR 2019

Proof.

Using DKL(p¯ ,f ,

pt+1 ,f )

=

min 

DKL(p¯ ,

,f

p,f )



DKL(p¯ ,f ,

pt ,f )

and Jensen's inequality,

SRf (t+1 ) - SRf (t ) = ( +  ) log


f () exp

r()+ log t+1 ()  +

f () exp

r()+ log t ()  +



=( +  ) log


f () exp

r()+ log t ()  +

f () exp

r()+ log t ()  +



· exp

 log t+1 () -  log t ()  +

=( +  ) log

¯, ()f () · exp

 log t+1 () -  log t ()  +



( +  )

¯, ()f () · log exp

 log t+1 () -  log t ()  +



=



¯,

()f ()

·

log

t+1 () t ()

=



¯,

()f ()

·

log

t+1 ()f () t ()f ()

=

DKL(p¯ ,f ,

pt

,f

)

-

DKL(p¯ ,

,f

pt+1 ,f )

 0.

SRf () also recovers corresponding performance measures in the stochastic transition setting. Proposition 5. SRf () satisfies the following properties:

(i) SRf ()  max r(), as   0,   0.

(ii)

SRf ()



E
p ,f

r(),

as





, 

 0.

Proof. To prove (i), note that as   0, SRf ()   log

 f () exp

r() 

. Taking limit on 

gives the hardmax value max r() as   0.

To prove (ii), we have

lim ( +  ) log f () exp r() +  log ()

 

 +



= lim
 



 ()f

()

exp{

r()- 

log +



()

}

(r()

-



log ())





()f

()

exp{

r()- 

log +



()

}

=



()f () [r() - 

log

 ()]

=

E
p ,f

r()

-



·

E
p ,f

log

 ()

As   0, SRf ()  Ep,f r().

D.4 LEARNING

The REPMD learning process is intact under the stochastic transition setting. Similar with Appendix B,

we can estimate the KL divergence in the projection step of Eq. (24) by drawing K i.i.d. samples

{1, . . . , K } from p¯,f , i.e., the mixture of ¯ and f , which is exactly the process of sampling from ¯ and interacting with the environment,

DKL(p¯ ,f ,

p,f )

=

E
p¯ 

log ¯, () - log ()
,f

,

=

E
p¯ ,f

¯, () ¯()

log ¯, () - log ()

.

(25)

19

Under review as a conference paper at ICLR 2019

We can then approximate the gradient of DKL(p¯ ,f ,

p,f ) by averaging these K samples accord-

ing to Eq. (25).

Theorem 3.

Let k

=

r(k

)- log  +

¯ (k

)

.

Given K

i.i.d.

samples {1, . . . , K } from the reference

policy ¯, we have the following unbiased gradient estimator,

K

DKL(p¯ ,f ,

p,f )  -

k=1

exp {k}

K j=1

exp

{j

}



log



(k

),

(26)

Proof. See the proof of Lemma 1.

Similar argument could be applied for PMAC learning objectives.

E EXPERIMENTS DETAILS
We describe the algorithmic and mujoco tasks we experimented on as well as details of experimental setup in this section.
E.1 ALGORITHMIC AND MUJOCO TASKS
In each algorithmic task, the agent operates on a tape of characters or digits. At each time step, the agent read one character or digit, and then decide to either move the read pointer one step in any direction of the tape, or write a character or digit to output. The total reward of each sampled trajectory is only observed at the end. The goal of each task is:
· Copy: Copy a sequence of characters to output. · DuplicatedInput: Duplicate a sequence of characters. · RepeatCopy: Copy a sequence of characters, reverse it, then forward the sequence again. · Reverse: Reverse a sequence of characters. · ReversedAddition: Observe two numbers in base 3 in little-endian order on a 2 × n grid
tape. The agent should add the two numbers together.
The Mujoco library contains various of continuous control tasks (Todorov et al., 2012). The specific action dimensions of each problem is summarized in Table 1.

Table 1: Action Dimensions of Mujoco Tasks Task Action Dimensions

Hopper Walker2d HalfCheetah
Ant Humanoid

3 6 6 8 17

E.2 IMPLEMENTATION DETAILS
For the synthetic bandit problem, we parameterize the policy by a weight vector   R20. Let  = (1, . . . , 10,000) be the feature matrix. The policy is defined by softmax( ). The REPMD parameters used in Fig. 2 are summarized in Table 2.
For the algorithmic tasks, policy is parameterized by a recurrent neural network with LSTM cells of hidden dimension 256 (Hochreiter & Schmidhuber, 1997). In all algorithms, N distinct environments are used to generate samples. On each environment, K random trajectories are sampled using the agent's policy to estimate gradient according to (7), which gives the batch size N × K in total. We apply the same batch training setting as in UREX (Nachum et al., 2017a), where N = 40 and
20

Under review as a conference paper at ICLR 2019

Table 2: REPMD Hyperparameters in Synthetic Bandit

Parameter

Values

  learning rate
F STEP

0.1
0.0
0.01 5 · 10-4
0

Table 3: REPMD Hyperparameters in Algorithmic Tasks

Copy DuplicatedInput RepeatCopy Reverse ReversedAddition

  learning rate clip norm

0.5 0.01 0.01 20 0.01

0.5 0.01 0.01 20 0.01

2.0 0.01 0.01 20 0.005

0.2 0.02 0.001 20 0.005

0.5 0.01 0.001 20 0.005

K = 10. F STEP of REMPD is set to 1 in all tasks (See Appendix B). The REPMD parameters used in Fig. 2 are summarized in Table 3.
We use standard gaussian policy for all experimented algorithms in the mujoco tasks. Two layer fully-connected feed-forward neural networks with hidden dimension 300 and ReLU nonlinearity are applied to parameterize policy, soft state value, and soft-Q value. We batch size 256 for all algorithms on all tasks. The lag parameter  of PMAC for target value network update is 0.01, and the number of training steps is set as M = 100 in all tasks. The other domain-dependent PMAC parameters are summarized in Table 4.

Table 4: REPMD Hyperparameters in Mujoco Tasks Walker2d Hopper HalfCheetah Ant

   learning rate  learning rate  learning rate

1.5
0.2 5 · 10-4 5 · 10-4 5 · 10-4

0.5
0.05 5 · 10-4 5 · 10-4 5 · 10-4

0.5
0.2 3 · 10-4 3 · 10-4 3 · 10-4

1.0
0.1 3 · 10-4 3 · 10-4 3 · 10-4

Humanoid
2.0
0.05 3 · 10-4 3 · 10-4 3 · 10-4

21

