Under review as a conference paper at ICLR 2019

LIPSCHITZ REGULARIZED DEEP NEURAL NETWORKS
CONVERGE AND GENERALIZE
Anonymous authors Paper under double-blind review

ABSTRACT
Generalization of deep neural networks (DNNs) is an open problem which, if solved, could impact the reliability and verification of deep neural network architectures. In this paper, we show that if the usual fidelity term used in training DNNs is augmented by a Lipschitz regularization term, then the networks converge and generalize. The convergence is in the limit as the number of data points, n  , while also allowing the network to grow as needed to fit the data. Two regimes are identified: in the case of clean labels, we prove convergence to the label function which corresponds to zero loss, in the case of corrupted labels which we prove convergence to a regularized label function which is the solution of a limiting variational problem. In both cases, a convergence rate is also provided.

1 INTRODUCTION

While deep neural networks networks (DNNs) give more accurate predictions than other machine learning methods (LeCun et al., 2015), they lack some of the performance guarantees of these methods. One step towards performance guarantees for DNNs is a proof of convergence with a rate, which could lead to quantitative error estimates. In this paper, we present such a result, for Lipschitz regularized DNNs. A proof of generalization follows as a consequence of convergence.

We begin by establishing the notation for our problem. We consider the classification problem to fix
ideas, although regularization can apply to other problems as well.
Definition 1.1. Assume the data is normalized so that the data space is X = [0, 1]d. Write Dn = x1, . . . , xn for the training data. Assume Dn is a sequence of i.i.d. random variables on X sampled from the probability distribution . We consider the classification problem with m labels which are imbedded into the probability simplex, the label space, Y  Rm. Write u0 : X  Y for the map from data to label space, so that yi = u0(xi).

Our results point towards improved generalization results using Lipschitz regularization, which we define now.

Definition 1.2. Choose norms · Y , and · X on X and Y , respectively. The Lipschitz constant (in these norms) of a function u : X0  X  Y is given by

Lip(u; X0) = sup
x1 ,x2 X0

u(x1) - u(x2) Y x1 - x2 X

When X0 is all of X, we write Lip(u; X) = Lip(u). The Lipschitz constant of the data is Lip(u0; Dn).

Write u(x; w) for the last layer of the network.1 We consider the variational problem with Lipschitz regularization

min J n[u] = 1 n

u:X Y

n

(u(xi; w), yi) +  max(Lip(u) - L0, 0)

i=1

(1)

The first term in (1) is the usual averaged loss on the training data Dn. The second term in (1) the Lipschitz regularization term: the excess Lipschitz constant of the map u, compared to the constant

1We apologize for not using the standard notation f for the last layer!

1

Under review as a conference paper at ICLR 2019

L0. Choosing  > 0 in (1) introduces a Lipschitz regularization penalty to the standard learning problem, which corresponds to  = 0. In theory we take L0 = Lip(u0), the Lipschitz constant of the data on the whole data manifold (discussed below).
Remark 1.3. In practice, Lip(u0) can be estimated by the Lipschitz constant of the data, Lip(u0; Dn). In fact, for many common data sets, the Lipschitz constant is very small and the Lipschitz constants of networks is much larger (Finlay & Oberman), so in practice we can set L0 = 0. However for clean data, if we wish to recover u0 exactly, we need L0 > 0.
If u is differentiable2, then the boundedness of X implies

Lip(u; X) = max u(x; w) X,Y
xX

where M X,Y is the matrix norm induced by the norms on X and Y . In practise, we take the -norm for Y and the 2-norm for X, and use explicit formulas for the M ,2 norm, see §B. The Lipschitz constant of u, Lip(u; X), can thus be estimated from below by the maximum of the norm
of the gradient on a minibatch

max
iI

xu(xi; w)

X,Y

 Lip(u; X)

(2)

Our analysis will apply to the problem (1) which is convex in u, and does not depend explicitly on the weights, w. Of course, once u is restricted to a fixed neural network architecture, the corresponding minimization problem becomes non-convex in the weights. Our analysis can avoid the dependence on the weights because we make the assumption that there are enough parameters so that u can perfectly fit the training data. The assumption is justified by Zhang et al. (2016). As we send n   for convergence, we require that the network also grow, in order to continue to satisfy this assumption. Our results apply to other non-parametric methods in this regime.
In Figure 1 we illustrate the solution of (1) (with L0 = 0), using synthetic one dimensional data. In this case, the labels {-1, 0, 1} are embedded naturally into Y = R, and  = 0.1. Notice that the solution matches the labels exactly on a subset of the data. In the second part of the figure, we show a solution with corrupted labels which introduce a large Lipschitz constant, in this case, the solution reduces the Lipschitz constant, thereby correcting the errors.

Figure 1: Synthetic labelled data and Lipschitz regularized solution u. Left: The solution value matches the labels perfectly on a large portion of the data set. Right: 10% of the data is corrupted by incorrect labels; the regularized solution corrects the errors.
1.1 RELATED WORK AND APPLICATIONS Generalization bounds have been obtained previously by using the Lipschitz constant of a network (Bartlett, 1997), as well as by using more general stability results (Bousquet & Elisseeff,
2We follow the common practise and treat the the architecture as differentiable for training purposes. To be rigorous we can use the L norm and appeal to Rachemacher's Theorem.
2

Under review as a conference paper at ICLR 2019

2002). More recently, (Bartlett et al., 2017) proposed the Lipschitz constant of the network as a candidate measure for the Rademacher complexity, which a measure of generalization (Shalev-Shwartz & Ben-David, 2014, Chapter 26). However, our analysis is more direct and self-contained, and unlike other recent contributions such as (Hardt et al., 2015), it does not depend on the training method.

The estimate of Lip(u; X) provided by (2) can be quite different from the the Tychonoff gradient regularization (Drucker & Le Cun, 1992),

1 |I |

xu(xi) 2

iI

since (2) corresponds to a maximum of the values of the norms, and the previous equation corresponds to average of the values. In fact, recent work on semi-supervised learning suggests that higher p-norms of the gradient are needed for generalization when the data manifold is not well approximated by the data (El Alaoui et al., 2016; Calder, 2017; Kyng et al., 2015; Slepcev & Thorpe, 2017). In Figure 2 we compare to the problems in Figure 1 using Tychonoff regularization. The Tychonoff regularization is less effective at correcting errors. The effect is more pronounced in higher dimensions.

Figure 2: Synthetic labelled data and Tychonoff regularized solution u. Left: The solution value matches the labels perfectly on a large portion of the data set. Right: 10% of the data is corrupted by incorrect labels; the regularized solution is not as effective at correcting errors. The effect is more pronounced in higher dimenions.

An upper bound for the Lipschitz constant of the model is given by the norm of the product of the weight matrices (Szegedy et al., 2013, Section 4.3). Let w = (w1, . . . , wJ ) be the weight matrices

for each layer. Then

Lip(u; X)  jJ=1 wi .

(3)

Regularization of the network using methods based on (3) has been implemented recently in (Gouk et al., 2018) and (Yoshida & Miyato, 2017). Because the upper bound in (3) does not take into account the coefficients in weight matrices which are zero due to the activation functions, the gap in the inequality can be off by factors of many orders of magnitude for deep networks (Finlay & Oberman).

Implementing (2) can be accomplished using backpropragation in the x variable on each label, which can become costly for m large. Special architectures could also be used to implement Lipschitz regularization, for example, on a restricted architecture, Liao et al. (2018) renormalized the weight matrices of each layer to be norm 1. In practise, the computational cost can be reduced by regularizing the loss applied to the model  u, instead

max x (u(xi))  Lip(  u; X)
iI

(4)

3

Under review as a conference paper at ICLR 2019

which requires only one backpropagation. Lipschitz regularization may help with adversarial examples (Szegedy et al., 2013) (Goodfellow et al., 2014) which poses a problem for model reliability (Goodfellow et al., 2018). Since the Lipschitz constant L of the loss, , controls the norm of a perturbation
(u(xi + v)) - (u(xi)) Y  L v X
maps with smaller Lipschitz constants may be more robust to adversarial examples. Finlay & Oberman implemented Lipschitz regularization of the loss, as in (4), and achieved better robustness against adversarial examples, compared to adversarial training (Goodfellow et al., 2014) alone.
Lipschitz regularization may also improve stability of GANs. 1-Lipschitz networks with are also important for Wasserstein-GANs (Arjovsky et al., 2017) (Arjovsky & Bottou, 2017). In (Wei et al., 2018) the gradient penalty away from norm 1 is implemented, augmented by a penalty around perturbed points, with the goal of improved stability. Spectral regularization for GANs was implemented in (Miyato et al., 2018).
Remark 1.4. Consider the following problem inspired by (Zhang et al., 2016). Given a labelled data set, which is Lipschitz continuous with constant L0 = 1, say. Now consider making 100 copies of each data point, and adding a small norm of noise to each image, but keeping the same labels. Call this data set Dn. Now consider a second data set D~n which is a copy of Dn, but where with probability p, a given label is randomly changed. The Lipschitz constant of D~n is O(1/ ), since two images a distance apart will have different labels. When p is small, we expect that solving (1) with L0 = 1 will lead to a good approximation of the original map, for both data sets, but will result in an an expected loss of approximately p for the second data set. See Figure 1.

2 LIPSCHITZ REGULARIZATION AND CONVERGENCE

2.1 LIMITING PROBLEM

The variational problem (1) admits Lipschitz continuous minimizers, but in general the minimizers are not unique. When L0 = Lip(u0), is it clear that u0, is a solution of (1): both the loss term and the regularization term are zero when applied to u0. In addition, any L0-Lipschitz extension of u0|Dn is also a minimizer of (1), so solutions are not unique.
Let un be any solution of the Lipschitz regularized variational problem (1). We study the limit of un as n  . Since the empirical probability measures n converge to the data distribution , we would expect the limit of un to be a solution of the continuum variational problem

min
u:X Y

J [u]



L[u; ]

+

 max(Lip(u)

-

L0,

0),

(5)

where in (5) we have introduced the following notation.
Definition 2.1. Given the loss function, , a map u : X  Y , and a probability measure, µ, supported on X, define

L[u, µ] = Exµ[ (u(x), u0(x))] = (u(x), u0(x))dµ(x)
X

to be the expectation of the loss with respect to the measure. In particular, the generalization loss of

the map u : X  Y is given by L[u, ]. Write L[u, Dn] := L[u, n] for the average loss on the data

set

Dn,

where

n

:=

1 n

xi is the empirical measure corresponding to Dn.

Remark 2.2. Generalization is defined in (Goodfellow et al., 2016, Section 5.2) as the expected

value of the loss function on a new input sampled from the data distribution. As defined, the full

generalization error includes the training data, but it is of measure zero, so removing it does not

change the value.

We would also expect the sequence of generalization losses L[un; ] to converge to zero in the case of perfect generalization.
We prove both of these results, including convergence rates, below. These results lead to an immediate proof that Lipschitz regularized DNN generalize.

4

Under review as a conference paper at ICLR 2019

2.2 LOSS FUNCTION ASSUMPTIONS

We introduce the following assumption on the loss function.

Assumption 2.3 (Loss function). The function : Y × Y  R is a loss function if it satisfies (i)

 0, (ii) (y1, y2) = 0 if and only if y1 = y2, and (iii) is strictly convex in y1.

Example 2.4 (Rm with L2 loss). Set Y = Rm, and let each label be a basis vector. Set (y1, y2) =

y1 - y2

2 2

to

be

the

L2

loss.

Example 2.5 (Classification). In classification problems, the output of the network is a probability

vector on the labels. Thus Y = p, the p-dimensional probability simplex, and each label is mapped

to a basis vector. The cross-entropy loss is given by KL(y, z) = -

p i=1

zi

log(yi/zi).

For

labels,

KL(y, ek) = - log(yk).

Example 2.6. Define the regularized cross entropy loss with parameter > 0 by

p
KL(y, z) = - zi log
i=1

yi + zi

.

For classification problems, where z = ek, we have KL(y, ek) = - log(yk + ), which is Lipschitz and strongly convex for 0  yi  1.

In Theorems 2.11 and 2.14 which follow, the cross entropy loss KL does not satisfy the Lipschitz condition required. However they apply to KL for any > 0.

2.3 CONVERGENCE RESULT FOR CLEAN LABELS

Here, we show that solutions of the random variational problem (1) converge to solutions of (5). We make the standard manifold assumption (Chapelle et al., 2006), and assume the data distribution  is a probability density supported on a compact, smooth, m0-dimensional manifold M embedded in X = [0, 1]d, where m0 d. We denote the probability density again by  : M  [0, ). Hence, the data Dn is a sequence x1, . . . , xn of i.i.d. random variables on M with probability density . Associated with the random sample we have the closet point projection map n : X  {x1, . . . , xn}  X that satisfies
x - n(x) X = min { x - xi X }
1in
for all x  X. We recall that W 1,(X; Y ) is the space of Lipschitz mappings from X to Y . Throughout this section, C, c > 0 denote positive constants depending only on M, and we assume C  1 and 0 < c < 1.

We establish that that minimizers of (5) are unique on M, which follows from the strict convexity of the loss restricted to the data manifold M, in Theorem A.1. See also Figure 3 which shows how
the solutions need not be unique off the data manifold.

Our first convergence result is in the case where Lip[u0]  L0, and so the Lipschitz regularizer is not fully active. This corresponds to the case of clean labels.

Theorem 2.7 (Convergence for clean labels). Suppose that Lip[u0]  L0 and infxM (x) > 0. If un  W 1,(X; Y ) is any sequence of minimizers of (1) then for any t > 0

t log(n) 1/m

u0 - un L(M;Y )  CL0

n

holds with probability at least 1 - Ct-1n-(ct-1).

As an immediate corollary, we can prove that the generalization loss converges to zero, and so we obtain perfect generalization.

Corollary 2.8. Assume that for some q  1 the loss satisfies

(y, y0)  C

y - y0

q Y

for all y0, y  Y.

Then under the assumptions of Theorem 2.7

(6)

L[un, ]  CLq0

t log(n) q/m n

holds with probability at least 1 - Ct-1n-(ct-1).

5

Under review as a conference paper at ICLR 2019

Proof. By (6), we can bound the generalization loss as follows

L[un, ] =

(un(x), u0(x)) dV ol(x)  CV ol(M)

un - u0

q L

(M;Y

)

.

M

The proof is completed by invoking Theorem 2.7.

We now turn to the proof of Theorem 2.7, which requires a bound on the distance between the
closest point projection n and the identity. The result is standard in probability, and we include it for completeness in Lemma 2.9 proved in §A.1. We refer the interested reader to (Penrose et al.,
2003) for more details.

Lemma 2.9. Suppose that infM  > 0. Then for any t > 0

Id - n L(M;X)  C with probability at least 1 - Ct-1n-(ct-1).

t log(n) 1/m n

We now give the proof of Theorem 2.7.

Proof of Theorem 2.7. Since Jn[un] = Jn[u0] = 0, we must have Lip[un]  L0 and u0(xi) = un(xi) for all 1  i  n. Then for any x  X we have
u0(x) - un(x) Y = u0(x) - u0(n(x)) + u0(n(x)) - un(n(x)) + un(n(x)) - un(x) Y

 u0(x) - u0(n(x)) Y + un(n(x)) - un(x) Y  2L0 x - n(x) X . Therefore, we deduce
u0 - un L(M;Y )  2L0 Id - n L(M;X).
The proof is completed by invoking Lemma 2.9.

2.4 CONVERGENCE FOR NOISY LABELS

Our second result is in the setting where Lip(u0) > L0, so the regularizer is active and we do not expect un  u0 as n  . This setting models the case where some labels have errors, and so the Lipschitz constant for the data is a large over-estimate of the Lipschitz constant of the clean labeling function. Our main result shows that minimizers of Jn converge to minimizers of J.
Remark 2.10. In the theorem which follows, the sequence un does not, in general, converge on the whole domain X. The important point is that the sequence converges on the data manifold M, and
solves the variational problem (5) off of the manifold, which ensures that the output of the DNN is
stable with respect to the input. See Figure 3.

Theorem 2.11. Suppose that infM  > 0, : Y × Y  R is Lipschitz, and let u  W 1,(X; Y ) be any minimizer of (5). Then with probability one

un - u uniformly on M as n  ,

(7)

where un is any sequence of minimizers of (1). Furthermore, every uniformly convergent subsequence of un converges on X to a minimizer of (5).

The proof of Theorem 2.11 requires a preliminary Lemma. Let HL(X; Y ) denote the collection of L-Lipschitz functions w : X  Y .

Lemma 2.12. Suppose that infM  > 0, and dim(M) = m0. Then for any t > 0

sup
wHL(X;Y )

1 n

n

w(xi) -

i=1

w dV ol(x)  CL
M

t log(n) n

1 m0 +2

(8)

holds

with

probability

at

least

1

-

2t-

m0 m0 +2

n-(ct-1).

6

Under review as a conference paper at ICLR 2019

Figure 3: Non-uniqueness of the solution off the data manifold: in the middle areas off the data manifold where the Lipschitz constant is attained, the solution is unique. In the outer area off the data manifold, the solution is not unique.

The estimate (8) is called a discrepancy result (Talagrand, 2006; Gyo¨rfi et al., 2006), and is a uniform version of concentration inequalities. We include a simple proof in Section A.2.

Proof of Theorem 2.11. By Lemma 2.12 the event that

lim
n

sup
wHL (X ;Y

)

|L[w,

n]

-

L[w,

]|

=

0

(9)

for all Lipschitz constants L > 0 has probability one. For the rest of the proof we restrict ourselves to this event.

Let un  W 1,(X; Y ) be a sequence of minimizers of (1), and let u  W 1,(X; Y ) be any minimizer of (5). Then since

(Lip(un) - L0)  J n[un]  J n[u0] = (Lip(u0) - L0)

we have Lip(un)  Lip(u0) =: L for all n. By the Arzela`-Ascoli Theorem (Rudin, 1976) there exists a subsequence unj and a function u  W 1,(X; Y ) such that unj  u uniformly as nj  . Note we also have Lip(u)  lim infj Lip(unj ). Since

|L[un, n] - L[u, ]|  |L[un, n] - L[u, n]| + |L[u, n] - L[u, ]|
 C un - u L(M;Y ) + sup |L[w, n] - L[w, ]|
wHL(X;Y )

it J

[fuol]loaws snfrom(9. )TthhearteLfo[ruenj

,

nj

]



L[u, ]

as

j



.

It

also

follows

from

(9)

that

J n[u]



J [u] = lim J n[u]
n



lim inf
n

J n[un]

=

lim inf
n

L[un,

n]

+



max(Lip(un)

-

L0,

0)

=

lim
n

L[un,

n]

+

lim inf
n



max(Lip(un)

-

L0,

0)

 L[u, ] +  max(Lip(u) - L0, 0) = J[u].

Therefore, u is a minimizer of J. By Theorem A.1, u = u on M, and so unj  u uniformly on M as j  .

Now, suppose that (7) does not hold. Then there exists a subsequence unj and  > 0 such that

max
xM

|unj

(x)

-

u(x)|

>



for all j  1. that converges

However, we uniformly on

can apply the argument above to extract a further subsequence M to u, which is a contradiction. This completes the proof.

of

unj

7

Under review as a conference paper at ICLR 2019

Finally, we prove a rate in the case where the loss is strongly convex in the first variable.

Definition 2.13. We say that is strongly convex with parameter  > 0 if

(ty1

+

(1

-

t)y2,

y0)

+

 2

t(1

-

t)

y1 - y2

2 Y

t

(y1, y0) + (1 - t)

(y2, y0)

for all y0, y1, y2  Y and 0  t  1.

(10)

We note that when is twice differentiable, this notion of strong convexity is equivalent to assuming y21  I. The definition in equation (10) is useful for non-smooth functions, such as the Lipschitz
semi-norm present in J[u].

Our final result is the following L2 convergence rate in the strongly convex setting.

Theorem 2.14. Suppose that : Y × Y  R is Lipschitz and strongly convex and let L = Lip(u0).

Then

for

any

t

>

0,

with

probability

at

least

1

-

2t-

m m+2

n-(ct-1)

all

minimizing

sequences

un

of

(1) and all minimizers u of (5) satisfy

1

 2

un - u

2 Y



dV

ol(x)



CL

M

t log(n) n

m+2
.

Before proving Theorem 2.14, we require a preliminary lemma. Lemma 2.15. If u  W 1,(X; Y ) is a minimizer of (5) and u  W 1,(X; Y ) then

 2

M

u - u

2 Y



dV

ol(x)



J [u]

-

J

[u].

Proof. We use Proposition A.4 with u1 = u and u2 = u to obtain

J [tu

+

(1

-

t)u]

+

 t(1

-

t)

2

M

u - u

2 Y



dV

ol(x)



tJ

[u]

+

(1

-

t)J [u].

Since J[tu + (1 - t)u]  J[u]

J [u]

+

 t(1

-

t)

2

M

u - u

2 Y



dV

ol(x)



tJ

[u]

+

(1

-

t)J

[u],

and so

 t
2

M

u - u

2 Y



dV

ol(x)



J [u]

-

J

[u].

Setting t = 1 completes the proof.

Proof of Theorem 2.14. Let L = Lip(u0). By Lemma 2.12

1

sup |L[w, n] - L[w, ]|  CL
wHL(X;Y )

t log(n) n

m+2

(11)

holds

with

probability

at

least

1

-

2t-

m m+2

n-(ct-1)

for

any

t

>

0.

Let

us

assume

for

the

rest

of

the

proof that (11) holds.

As in the proof of Theorem 2.11, we have Lip(un)  L and Lip(u)  L, and so

1

|J n[u] - J [u]| , |J n[un] - J [un]|  CL

t log(n) n

m+2
.

Therefore J [un] - J [u] = J n[un] - J [u] + J [un] - J n[un]  CL

t log(n) n

1 m+2
.

By Lemma 2.15 we deduce

 2

M

un - u

2 Y



dV

ol(x)



CL

which completes the proof.

t log(n) n

1 m+2
,

8

Under review as a conference paper at ICLR 2019
REFERENCES
Martin Arjovsky and Le´on Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Gunnar Aronsson, Michael Crandall, and Petri Juutinen. A tour of the theory of absolutely minimizing functions. Bulletin of the American mathematical society, 41(4):439­505, 2004.
Gilles Aubert and Pierre Kornprobst. Mathematical problems in image processing: partial differential equations and the calculus of variations, volume 147. Springer Science & Business Media, 2006.
Peter L Bartlett. For valid generalization the size of the weights is more important than the size of the network. In Advances in neural information processing systems, pp. 134­140, 1997.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6240­6249, 2017.
Ste´phane Boucheron, Ga´bor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013.
Olivier Bousquet and Andre´ Elisseeff. Stability and generalization. Journal of machine learning research, 2(Mar):499­526, 2002.
Andrea Braides. Gamma-convergence for Beginners, volume 22. Clarendon Press, 2002.
Jeff Calder. Consistency of lipschitz learning with infinite unlabeled data and finite labeled data. arXiv preprint arXiv:1710.10364, 2017.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning. MIT, 2006.
Bernard Dacorogna. Direct methods in the calculus of variations, volume 78. Springer Science & Business Media, 2007.
Harris Drucker and Yann Le Cun. Improving generalization performance using double backpropagation. IEEE Transactions on Neural Networks, 3(6):991­997, 1992.
Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J Wainwright, and Michael I Jordan. Asymptotic behavior of\ell p-based laplacian regularization in semi-supervised learning. In Conference on Learning Theory, pp. 879­906, 2016.
Christopher Elion and Luminita A Vese. An image decomposition model using the total variation and the infinity laplacian. In Computational Imaging V, volume 6498, pp. 64980W. International Society for Optics and Photonics, 2007.
Lawrence C. Evans. Partial differential equations, volume 19 of Graduate Studies in Mathematics. American Mathematical Society, 1998. ISBN 0-8218-0772-2.
Chris Finlay and Adam M Oberman. Improved robustness to adversarial examples using lipschitz regularization of the loss.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org.
Ian Goodfellow, Patrick McDaniel, and Nicolas Papernot. Making machine learning robust against adversarial inputs. Communications of the ACM, 61(7):56­66, June 2018. ISSN 00010782. doi: 10.1145/3134599. URL http://dl.acm.org/citation.cfm?doid=3234519. 3134599.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
9

Under review as a conference paper at ICLR 2019
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.
Laurence Guillot and Carole Le Guyader. Extrapolation of vector fields using the infinity laplacian and with applications to image segmentation. In Xue-Cheng Tai, Knut Mørken, Marius Lysaker, and Knut-Andreas Lie (eds.), Scale Space and Variational Methods in Computer Vision, pp. 87­ 99, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg. ISBN 978-3-642-02256-2.
La´szlo´ Gyo¨rfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A distribution-free theory of nonparametric regression. Springer Science & Business Media, 2006.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Roger A Horn, Roger A Horn, and Charles R Johnson. Matrix analysis. Cambridge university press, 1990.
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.
Rasmus Kyng, Anup Rao, Sushant Sachdeva, and Daniel A Spielman. Algorithms for lipschitz learning on graphs. In Conference on Learning Theory, pp. 1190­1223, 2015.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso Poggio. A surprising linear relationship predicts test performance in deep networks. arXiv preprint arXiv:1807.09659, 2018.
Edward James McShane. Extension of range of functions. Bulletin of the American Mathematical Society, 40(12):837­842, 1934.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Mathew Penrose et al. Random geometric graphs. Number 5. Oxford university press, 2003.
Thomas Pock, Daniel Cremers, Horst Bischof, and Antonin Chambolle. Global solutions of variational models with convex regularization. SIAM Journal on Imaging Sciences, 3(4):1122­1145, 2010.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: nonlinear phenomena, 60(1-4):259­268, 1992.
Walter Rudin. Principles of mathematical analysis. McGraw-hill New York, 1976.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014. doi: 10.1017/CBO9781107298019.
Dejan Slepcev and Matthew Thorpe. Analysis of p-laplacian regularization in semi-supervised learning. arXiv preprint arXiv:1707.06213, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Michel Talagrand. The generic chaining: upper and lower bounds of stochastic processes. Springer Science & Business Media, 2006.
AN Tikhonov and V Ya Arsenin. Solutions of Ill-Posed Problems. Winston and Sons, New York, 1977.
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training of wasserstein gans: A consistency term and its dual effect. arXiv preprint arXiv:1803.01541, 2018.
10

Under review as a conference paper at ICLR 2019

Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv:1611.03530, 2016.

A PROOFS

A.1 PROOFS FOR CLEAN LABELS
In this section we provide the proof of results stated in §2.3. Theorem A.1. Suppose the loss function satisfies Assumption 2.3. If u, v  W 1,(X; Y ) are two minimizers of (5) and infM  > 0 then u = v on M.

Proof. Let w = (u + v)/2. Then

J[w] =

1 2

u

+

1 2

v,

u0

 dV ol(x) +  max

Lip

1 2

u

+

1 2

v

,0

M



1 2

(u,

u0)

+

1 2

(v, u0)

 dV ol(x) +  max

1 2

Lip (u)

+

1 2

Lip (v)

,

0

M



1 2

(u,

u0)

+

1 2

(v, u0)

 dV ol(x) + 

1 2

max

(Lip (u)

, 0)

+

1 2

max

(Lip

(v)

,

0)

M

=

1 2

J

[u]

+

1 2

J

[v]

=

min J[u].
u

Therefore, w is a minimizer of J and so we have equality above, which yields

1 2

(u,

u0)

+

1 2

(v, u0)

 dV ol(x) =

1 2

u

+

1 2

v,

u0

 dV ol(x).

MM

Since is strictly convex in its first argument, it follows that u = v on M.

Proof of Lemma 2.9 of §2.3. There exists M such that for any 0 <  M, we can cover M

with N geodesic balls B1, B2, . . . , BN of radius , where N  C -m and C depends only on M

(Gyo¨rfi et al., 2006). Let Zi denote the number of random variables x1, . . . , xn falling in Bi. Then

Zi pi

 B(n, pi), where pi  c m. Let An denote

= the

Bi (x) dV ol(x). Since event that at least one Bi

 is

  > 0 and empty (i.e., Zi

V ol(Bi)  c = 0 for some

m
i).

we have Then by

the union bound we deduce

N
P(An)  P (Zi = 0)
i=1
 C -d(1 - c m)n = C exp (n log(1 - c m) - log( m))  C exp (-cn m - log( m)) .

Choose 0 <



M

in

the

form

n

m

=

t log(n)

with

t



n

m M

/

log(n).

Then

P(An)  Ct-1 exp (-(ct - 1) log(n)) .

In the event that An does not occur, then each Bi has at least one point, and so |x - n(x)|  C for all x  M. Therefore

t log(n) 1/m

Id - n L(M;X)  C = C

n



with probability at least 1 - Ct-1 exp (-(ct - 1) log(n)). Since Id - n L(M;X)  C d, the

result

holds

for

t



n

m M

/

log(n),

albeit

with

a

larger

constant

C.

11

Under review as a conference paper at ICLR 2019

A.2 PROOFS FOR NOISY LABELS

Here, we give the proofs of lemmas and propositions required in Section 2.4. We first give the proof
of Lemma 2.12. A key tool in the proof is Bernstein's inequality (Boucheron et al., 2013), which we recall now for the reader's convenience. For X1, . . . , Xn i.i.d. with variance 2 = E[(Xi -E[Xi])2], if |Xi|  M almost surely for all i then Bernstein's inequality states that for any > 0

P

1 n

n

Xi - E[Xi]

>

i=1

 2 exp

- n2 22 + 4M /3

.

Proof of Lemma 2.12. We note that it is sufficient to prove the result for w  HL(X; Y ) with M w dV ol(x) = 0. In this case, we have w(x) = 0 for some x  M, and so w L(X;Y )  CL. We also write m in place of m0 in the proof for simplicity.
We first give the proof for M = X = [0, 1]m. We partition X into hypercubes B1, . . . , BN of side length h > 0, where N = h-m. Let Zj denote the number of x1, . . . , xn falling in Bj. Then Zj is a Binomial random variable with parameters n and pj = Bj  dx  chm. By the Bernstein inequality we have for each j that

provided 0 <

1

P

n Zj -

 dx
Bj

>

 hm. Therefore, we deduce

 2 exp -cnh-m 2

1 n

n i=1

w(xi)



1 n

N
Zj
j=1

max w
Bj

(12) N

j=1

 dx +
Bj

max w
Bj

N

 max w  dx + CLh-m

j=1 Bj

Bj

N

 (min w + CLh)  dx + CLh-m

j=1 Bj

Bj

N
 w dx + CLh-m(hm+1 + )
j=1 Bj

(12)

= w dx + CL(h + h-m )

X

holds with probability at least 1-2h-m exp -cnh-m 2 for any 0 <  hm. Choosing = hm+1

we have that

1 n

n

w(xi) -

i=1

w dx
X

 CLh

holds for all u  HL(X; Y ) with probability at least 1 - 2h-m exp -cnhm+2 , provided h  1. By selecting nhm+2 = t log(n)

1n

sup
wHL(X;Y )

n w(xi) -
i=1

w dV ol(x)  CL
M

holds

with

probability

at

least

1

-

2t-

m m+2

n-(ct-1)

for

t



w L(X;Y )  CL, the estimate

1
t log(n) m+2 n
n/ log(n). Since we have

1n

sup
wHL(X;Y )

n w(xi) -
i=1

w dV ol(x)  CL,
M

12

Under review as a conference paper at ICLR 2019

trivially holds, and hence we can allow t > n/ log(n) as well.

We sketch here how to prove the result on the manifold M. We cover M with k geodesic balls

of radius > 0, denoted BM(x1, ), . . . , BM(xk, ), and let 1, . . . , k be a partition of unity

subordinate to this open covering of M. For > 0 sufficiently small, the Riemannian exponential

map expx : the geodesic

B(0, ball

B)M(xT,xM) MM, wihsearediTffxeMomo=rphRismm.

between the ball B(0, r)  TxM and Furthermore, the Jacobian of expx at

v  B(0, r)  TxM, denoted by Jx(v), satisfies (by the Rauch Comparison Theorem)

(1 + C|v|2)-1  Jx(v)  1 + C|v|2.

Therefore, we can run the argument above on the ball B(0, r)  Rm in the tangent space, lift
the result to the geodesic ball BM(xi, ) via the Riemannian exponential map expx, and apply the bound

1 n

n

w(xi) -

i=1

k
w dV ol(x) 
M j=1

1 n

n

j(xi)w(xi) -

i=1

jw dV ol(x)
M

to complete the proof.

Remark A.2. The exponent 1/(m0 + 2) is not optimal, but affords a very simple proof. It is possible to prove a similar result with the optimal exponent 1/m0 in dimension m0  3, but the proof is significantly more involved. We refer the reader to (Talagrand, 2006) for details.
Remark A.3. The proof of Theorem 2.11 shows that (1) -converges to (5) almost surely as n   in the L(X; Y ) topology. -convergence is a notion of convergence for functionals that ensures minimizers along a sequence of functionals converge to a minimizer of the -limit. While we do not use the language of -convergence here, the ideas are present in the proof of Theorem 2.11. We refer to (Braides, 2002) for details on -convergence.

We finally give a proposition useful in the proof of Lemma 2.15. Proposition A.4. If is strongly convex with parameter  > 0 then

J [tu1

+

(1

-

t)u2]

+

 2

t(1

-

t)

u1 - u2

2 Y



dV

ol(x)



tJ

[u1]

+

(1

-

t)J

[u2]

M

for all u1, u2  W 1,(X; Y ) and 0  t  1.

Proof. We compute J [tu1 + (1 - t)u2]

= (tu1 + (1 - t)u2, u0) dV ol(x) +  max (Lip(tu1 + (1 - t)u2), 0)
M



tJ [u1]

+

(1

-

t)J [u2]

-

 t(1
2

-

t)

M

u1 - u2

2 Y



dV

ol(x),

which completes the proof.

B INDUCED MATRIX NORMS

In some cases, we can take advantage of explicit formulas for matrix norms, which makes the estimates in (3) an explicit function of the weights. Define the induced matrix norm by

M

p,q = sup
x

Mx q xp

Then the following matrix norms formulas hold (see (Horn et al., 1990, Chapter 5.6.4))

M , = max |mij |,
i j

M 1,1 = max |mij |
j i

M 1, = max |mij |, M 2, = max
i,j i

mi2j
j

13

Under review as a conference paper at ICLR 2019

Figure 4: Comparison of different regularization methods. Lipschitz regularization preserves most of the labels (Figure 1). Tychonoff regularization smooths the solution (left). Total Variation regularization shifts the label values towards the mean (right).

C VARIATIONAL PROBLEMS IN IMAGE PROCESSING AND LIPSCHITZ EXTENSIONS

The variational problem (1) can be interpreted as a relaxation of the Lipschitz Extension problem.

min Lip[u]
u:X Y
subject to u(x) = u0(x) for x  D

(LE)

for D  X. The problem (LE) has more that one solution. Two classical results giving explicit solutions in one dimension go back to Kirzbaum and to McShane (McShane, 1934). However solving (LE) is not practical for large scale problems. There has be extensive work on the Lipschitz Extension problem, see, (Johnson & Lindenstrauss, 1984), for example. More recently, optimal Lipschitz extensions have been studied, with connections to Partial Differential Equations, see (Aronsson et al., 2004). We can interpret (1) as a relaxed version of (LE), where -1 is a parameter which replaces the unknown Lagrange multiplier for the constraint.

Variational problems are fundamental tools in mathematical approaches to image processing (Aubert & Kornprobst, 2006) and inverse problems more generally. Without regularization inverse problems can be ill-posed. The general form of the problem is

J[u] = L[u; u0] + R[u]

(13)

which combines a loss or fidelity functional, L[u, u0], which depends on the values of u and the reference image u0, and a regularization functional, R[u], which depends on the gradient, u.
The parameter  determines the relative strength of the two terms which emphasize fidelity versus

regularization.

Example C.1. For example, a typical fidelity term is the standard least-squares L[u, u0] = u -

u0

2 L2

(D)

.

The

regularization

|u(x)|

2 L2 (D)

corresponds

to

the

classical

Tychonov

regularization

(Tikhonov & Arsenin, 1977), R[u] = |u(x)| L1(D) is the Total Variation regularization model

of Rudin, Osher and Fatemi (Rudin et al., 1992).

Lipschitz regularization in not nearly as common. It appears in image processing in (Pock et al., 2010, §4.4) (Elion & Vese, 2007) and (Guillot & Le Guyader, 2009). Variational problems of the form (13) can be studied by the direct method in the calculus of variations (Dacorogna, 2007). The problem (13) can be discretized to obtain a finite dimensional convex convex optimization problem. The variational problem can also be studied by finding the first variation, which is a Partial Differential Equation (Evans, 1998), which can then be solved numerically. Both approaches are discussed in (Aubert & Kornprobst, 2006).

In Figure 4 we compare different regularization terms, in one dimension. The difference between the regularizers is more extreme in higher dimensions.

14

