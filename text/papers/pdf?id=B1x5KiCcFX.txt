Under review as a conference paper at ICLR 2019
UNDERSTANDING GANS VIA GENERALIZATION ANALYSIS FOR DISCONNECTED SUPPORT
Anonymous authors Paper under double-blind review
ABSTRACT
This paper provides theoretical analysis of generative adversarial networks (GANs) to explain its advantages over other standard methods of learning probability measures. GANs learn a probability through observations, using the objective function with a generator and a discriminator. While many empirical results indicate that GANs can generate realistic samples, the reason for such successful performance remains unelucidated. This paper focuses the situation where the target probability measure satisfies the disconnected support property, which means a separate support of a probability, and relates it with the advantage of GANs. It is theoretically shown that, unlike other popular models, GANs do not suffer from the decrease of generalization performance caused by the disconnected support property. We rigorously quantify the generalization performance of GANs of a given architecture, and compare it with the performance of the other models. Based on the theory, we also provide a guideline for selecting deep network architecture for GANs. We demonstrate some numerical examples which support our results.
1 INTRODUCTION
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) attract much attention as technology for learning a distribution and generating data. The purpose of GANs is to learn a probability measure from a given dataset and generate samples from the learned measure. It is often seen that samples generated by GANs can extract effectively features in the real world; it is difficult, for instance, to distinguish real images and generated images. By practical successes, a countless number of variations of GANs have been developed (Dziugaite et al., 2015; Arjovsky et al., 2017; Li et al., 2015; Nowozin et al., 2016; Gulrajani et al., 2017; Zhao et al., 2016), and applied to a wide range of tasks (Reed et al., 2016; Zhu et al., 2017; Gauthier, 2014).
Understanding the remarkable performance of GANs is, however, still a challenging problem. There are active discussions on the role of generators in its learning scheme (Goodfellow, 2016; Arjovsky & Bottou, 2017; Arora et al., 2018; Creswell et al., 2018), and adversarial structures with discriminators are also a target of interest (Lotter et al., 2015; Zhang et al., 2018). A gaming structure between generators and discriminators is also regarded as a useful factor in the mechanism of GANs (Mescheder et al., 2017; Arora et al., 2017; Heusel et al., 2017). Generalization performance of GANs has been investigated in several studies (Liang, 2017; Liu et al., 2017; Tolstikhin et al., 2017). In spite of these studies, it is not yet clear why GANs can generate well-extracted data better than other standard methods.
This paper introduces the disconnected support property, and explains an advantage of GANs in connection with this notion. The disconnected support property refers to a probability measures of which the support is divided into several disjoint sets, allowing non-differentiable density on the boundary. The property makes a probability measure be complex, hence it can be an obstacle for standard methods to learn the measure effectively. This property, however, is popularly seen in many real data, especially data with cluster structure, as demonstrated in Section 3.
We investigate in detail the approximation and estimation ability of GANs and some other methods, and provide novel generalization analysis of probability measures with disconnected support. Firstly, we show that the other methods suffer worse generalization performance due to complex structures of disconnected supports (Proposition 1 and Lemma 2). Secondly, our generalization analysis reveals that GANs can learn the probability measure without loss of efficiency under the the disconnected
1

Under review as a conference paper at ICLR 2019

supports (Theorem 1, Corollary 1 and ??). Additionally, we derive a guideline for choosing the number of layers or connections of the generator and discriminator from the generalization analysis. Numerical results support our theoretical findings.
We remark that the disconnected support property is different from the low-dimensional supports studied in Arjovsky & Bottou (2017), where the support of a measure generated by neural networks is disjoint to the measure of observations. In contrast, this paper considers the case in which the support of the observation measure is divided into disjoint subsets. The problem of disconnected supports is complement to the low-dimensionality, hence these two problems can be investigated separately. In this paper, to simplify the discussion, we assume that the support of a probability measure is not low-dimensional.
The contributions of this paper are summarized as follows:
1. We show that GANs perform better than other standard methods of estimating probability measures when the measure satisfies the disconnected support property.
2. We provide a new generalization error bound under a general formulation of GANs by analyzing an approximation error. The result is thus applicable to a wide range of variations of GANs.
3. Based on the generalization bound, we provide a theoretical guideline for selecting architectures of generators and discriminators.
All the proofs are given in Supplementary materials.

2 PRELIMINARIES

2.1 NOTATION

We use notations I :" r0, 1s. A j-th element of a vector b is denoted by bj, and }b}q :" pj bjqq1{q

is the q-norm (q P r0, 8s). vecp¨q is a vectorization operator for matrices. For z P N, rzs :"

t1, 2, . . . , zu is a set of positive integers no more than z. For  P R, tu denotes the largest integer

which is not larger than . For a domain  in a Euclidean space and a function f :  Ñ R,

}f }Lp

:"

 p

|f

ptq|pdtq1{p

denotes the

Lp

norm

for

p

P

r0, 8s.

For

f

:



Ñ

RD

with a

multi-

dimensional output, fd denotes a d-th coordinate of f pxq " pf1pxq, ..., fDpxqqJ. Let Hpq be the

Hölder space for   0 such as a set of -smooth functions f :  Ñ R, namely, f is Ctu-class and

its tu-th derivative is  ´ tu-Hölder continuous. b denotes a tensor product, and  a composition

of functions, namely, for functions f and f 1, f  f 1 " f pf 1p¨qq. A Borel -algebra of  is denoted

as pq. For a measurable mapping f :  Ñ 1 and B1  1, a pre-image of f is defined as

f ´1pB1q :" tt P  | B1 Q f ptqu. Let 1 : x ÑÞ t0, 1u be an indicator function such that 1pxq " 1

if x P , and 1pxq " 0 otherwise.

2.2 GENERAL FRAMEWORK OF GANS

We provide a general formulation of a learning problem with generative adversarial networks (GANs)

following Liu et al. (2017). In this paper, we consider a probability measure P ° on a measurable

space pID, q with a dimensionality D P N and  :" pIDq. Here, we set D  3. Suppose we have

a set of n observations D :" tX1, ..., Xnu which is independently and identically generated from

P °.

Let

Pn

:"

1 n


iPrns

Xi

be

an

empirical

measure

where

x

is

the

Dirac

measure

at

x.

The goal of generative networks is to estimate P ° from D. To this end, we construct a probability measure by generators. Let PZ be the uniform distribution on pID, q. For a measurable mapping g : ID Ñ ID, we define Pg as the pushforward measure: i.e., PgpBq " PZ pg´1pBqq for B P . We
call g as a generator and use G for a set of generators.

GANs employ a learning scheme with a metric with discriminators (Goodfellow et al., 2014). Let F " tf : ID Ñ Ru be a a set of discriminators. This paper considers a general metric for GANs
(Liu et al., 2017),

dF pP, P 1q :" sup EX,,P rf pXqs ´ EX,,P 1 rf pXqs,
f PF

(1)

2

Under review as a conference paper at ICLR 2019

between probability measures P and P 1.

For the learning process, we generate m noise samples Zr1, ..., Zrm from PZ and obtain generated

samples

as

Xrj

:"

gpZrj q

with

g

P

G

for

j

P

rms.

Let

Pg,m

:"

1 m


jPrms

Xj

denote

the

sampling

measure. GANs construct an estimator Pgp for P ° by learning gp with the following optimization

problem

gp P argmin dF pPn, Pg,mq .
gPG

(2)

The metric (1) covers a wide variety of GANs by selecting F. Among others, the original GAN (Goodfellow et al., 2014) is realized if F contains a logarithm of density ratio; Wasserstein-GAN (Arjovsky et al., 2017), MMD-GAN (Dziugaite et al., 2015; Li et al., 2017) and Energy-Based GAN (Zhao et al., 2016) are given if F is the set of 1-Lipschitz functions, a reproducing kernel Hilbert space, and the bounded continuous functions, respectively. The f -GAN (Nowozin et al., 2016) also belongs to this class.

We assume that F is large enough to contain functions which can work as a discriminator, namely, we assume that the following holds:

dF pP, P 1q " 0 ô P " P 1.

(3)

A sufficient condition for (3) is investigated in Zhang et al. (2018).

2.3 DEEP NEURAL NETWORKS FOR GENERATORS AND DISCRIMINATORS
In the schemes of GANs, F and G are realized by deep neural networks (DNNs). For further discussion, we formulate the function class given by DNNs.
Let L P N be a number of layers in DNNs, and D1 P N be a dimensionality of variables in an -th layer for P rL ` 1s. Here, we set DL1 `1 " D for generators and DL1 `1 " 1 for discriminators. We introduce A P RD1`1^D1 and b P RD1 as matrix and vector parametersof the -th layer. An architecture  of DNNs is defined as a set of L pairs of pA , b q as  :" ppA1, b1q, ..., pAL, bLqq. We define notations for  as follow: || :" L as the number of layers, }}0 :"  PrLs } vecpA q}0 ` }b }0 as the number of non-zero elements in , and }}8 :" maxtmax PrLs } vecpA q}8, max PrLs }b }8u be the scale of parameters in . We employ the ReLU activation function  : RD1 Ñ RD1 for each D1 P N such as pxq " pmaxtxd, 0uqdPrD1s.
We define functions of DNNs with an architecture  as rs : RD1 Ñ RD2 by
rspxq " xpL`1q, xp1q :" x, xp `1q :" pA xp q ` b q, for P rLs.
The function class of DNNs is thus given by
!) pS, B, Lq :" rs : ID Ñ R | }}0  S, }}8  B, ||  L ,
where S P N, B  0, and L P N are hyper-parameters. Here, S bounds the number of non-zero parameters of DNNs, namely, it controls the sparseness of DNNs. B is a bound for scales of parameters.

3 DISCONNECTED SUPPORT PROPERTY
3.1 INTRODUCTION AND EXAMPLE
It is often observed that data in real world data the support of its probability measure may not be connected but a union of disjoint subsets. This is typical if the data has cluster structures, as seen in many data sets for classification tasks. Moreover, the density function of the probability measure may not be smooth at a boundary of the support. Figures 1 (MNIST, (LeCun et al., 1998)) and 2 (Shelter Animal, Center) illustrate such examples in the real world. They are projected onto a 2-dimensional Euclidean space by t-SNE (Maaten & Hinton, 2008) so that they preserve the original distance structure among points. We can see that both of the data are concentrated on several disjoint

3

Under review as a conference paper at ICLR 2019
subsets and there are a clear gap or empty regions between some of the subsets. This observation suggests that the disconnected property of probability measures should be addressed in discussing estimation of probability measures, while standard analysis does not consider this phenomenon. In fact, this paper will show that the disconnected supports property has an important role in showing an advantage of GANs over standard estimation methods.

Figure 1: Plot of the MNIST data.

Figure 2: Plot of the animal data.

3.2 MATHEMATICAL FORMULATION OF DISCONNECTED SUPPORTS

Here we make a rigorous definition of disconnected supports. The property of smoothness (i.e. differentiability) is involved, which is a key factor to analyze generalization performance in the fields of the statistics (Stone, 1982; Tsybakov, 2009); Stone (1982) shows, for instance, that smoothness and a dimension of data are sufficient to characterize an optimal convergence of generalization errors.

We first prepare a family of subsets as a component in the disconnected supports:

S,J :" S  ID | A boundary of S is J combination of -smooth hyper surfaces( . The supplementary material will provide a more rigorous definition.

Now, we define the disconnected support property of probability measures as well as a probability
measure with global support, i.e., with no the disconnected support property. Let SupppP q be the support of P ., i.e, SupppP q :" tx P ID | P pVxq  0 for all open neighborhood Vx of xu. Hereafter, M  2 is the number of disjoint components of a support.

Definition 1. (Disconnected Supports / Global Support) Let M  2. A probability measure P on pID, q has M disconnected supports, if there exist
nonempty disjoint sets S1, ..., SM P S,J such that



SupppP q "

Sm.

mPrM s

A probability measure P on pID, q has a global support, if SupppP q " ID.

Figure 3 illustrates the disconnected support property.
We next formulate a notion of smoothness for P with disconnected supports. Let   1 be a parameter for a degree of smoothness of P . Definition 2. (Local Smoothness) A probability measure P with M disconnected support is locally -smooth, if there exist M pairs pSrm, Smq P S2,J ^ S2,J and  ` 1-smooth bijective measurable maps m : Srm Ñ Sm as
P pBq " PZ pm´1pBqq, @B P pSmq,
for m P rM s.

This definition of local smoothness says that a probability measure P with disconnected supports
can be generated by sufficiently smooth mappings m. It is used for considering a smooth density function of P restricted on Sm.

4

Under review as a conference paper at ICLR 2019

Lemma 1. (Locally Smooth Density Functions) If a probability measure P with M disconnected support is locally -smooth, then there exists a function pm : Sm Ñ R` such that
 P pBq " pmpxqd, B P pSmq,
B
where  is the Lebesgue measure, and pm is -smooth for all m P rM s.

We call pm as a local density function.

Note that, since P with disconnected supports is not absolutely continuous to the Lebesgue measure

on

IDz


mPrM s

Sm,

an

ordinary

density

function

cannot

be

defined.

Instead,

a

localized

version

of

density functions for each Sm is introduced, which is guaranteed by the local smoothness.

Figure 3: Illustration of a probability measure Figure 4: Illustration of a generator g. To

P with a disconnected support. SupppP q is a represent discontinuous S1 and S2, g should

union of two disjoint sets S1 and S2.

be discontinuous.

3.3 DIFFICULTY WITH DISCONNECTED SUPPORTS
As shown in this subsection, the generalization performance of many standard estimation methods is worsened with disconnected supports. We consider popular nonparametric methods, for which the generalization performance is well-studied in the asymptotics of the observation size n. The considered methods are the kernel density estimator (KDE) (Nadaraya, 1964), the nonparametric Bayes (NB) by the Dirichlet mixtures of normal distributions (Ferguson, 1973), the series density estimator (SDE) (Efromovich et al., 2008; Efromovich, 2010) and the density estimator with Gaussian process (GP) (Leonard, 1978). Bounds of the generalization errors for these methods are already known (Tsybakov, 2009; Ghosal et al., 2007; van der Vaart & van Zanten, 2008), and they are optimal in the minimax sense. Here, their performance is evaluated with respect to a root of an expected squared loss with respect to the L2-norm, namely, d2pP, P 1q :" Er}p ´ p1}2L2 s1{2 where p and p1 are densities for P and P 1.
We show the deterioration of their performance by the disconnected support property. Let Pp be an estimator for P ° by KDE, NB, SDE, or GP. If P ° has a global support and a -smooth density, the existing studies (Tsybakov, 2009; Ghosal et al., 2007; van der Vaart & van Zanten, 2008) show that
´¯ d2pP °, Ppq " O n´{p2`Dq .
These bounds are sufficiently tight, since these bounds corresponds to an optimal rate (Stone, 1982), and the performance of the methods can be improved as the density for P ° is smoother (larger ).
On the other hand, we consider a case in which P ° has the disconnected support property.
Proposition 1. (Deterioration of other standard methods) There exists P ° of the disconnected support property and locally -smooth such that
´¯ d2pP °, Ppq " O n´1{p2`Dq .
5

Under review as a conference paper at ICLR 2019

When P ° has disconnected supports, the errors are worse than those for the global support, independent of . This worse generalization error can be understood by the non-smoothness or discontinuity of the density functions on the boundaries of the disconnected sets (see Figure 3).
We next discuss other generative models for estimating a probability measure. To the best of our knowledge, other probabilistic generative methods (Koller et al., 2009) and the variational autoencoder (Kingma & Welling, 2013), their statistical generalization property is not well investigated. Here we provide a property of generators for probability measures with disconnected supports.
Lemma 2. (Discontinuous Generators for Disconnected Supports) If P ° has disconnected supports and P ° " Pg° with a generator g°, then g° is not uniformly continuous.
Lemma 2 states that a generator must be discontinuous to construct a probability measure with disconnected support sets. Intuitively, to make Pg° pBq " 0 for B P ID with pBq  0, the slope of g° at z P ID should be close to infinite for z P g°,´1pBq, hence g° cannot be uniformly continuous (see Figure 4). Because of the discontinuity, generative models with smooth functions, such as an adversarial generative model with kernel generators (Sinn & Rawat, 2018), cannot work well with disconnected supports.

4 GENERALIZATION BY GANS

We provide generalization analysis for GANs for probability measures with and without disconnected supports. For the purpose, we employ a metric dF with properly selected discriminators F and evaluate the generalization error dF pP °, Pgpq with respect to an observation size n and a sampling
size m. We assume F is realized by DNNs as F " pSf , Bf , Lf q X Fr with parameters Sf , Bf , Lf ,
where Fr is a specified functional class; for an example, Fr is 1-Lipschitz functions for WassersteinGAN. Here, we consider settings that all f P F are L1-Lipschitz continuous and }f }L8  BF with constants L1, BF  0.Generators are also constructed by DNNs as G " pSg, Bg, Lgq with parameters Sg, Bg, Lg.

A standard line of discussing generalization, we should consider statistical errors and approximation errors. We define a measure of the complexity of F

c

npF

q

:"

inf
0

4

`

12n´1{2

log N ppL1 ` 1q´1 , F , } ¨ }nq1{2d ,


where c  0 is a constant depends on F and N p , Fr, } ¨ }q is a covering number of Fr with respect to an empirical norm } ¨ }. We note that npFq bounds an expectation of the Rademacher complexity as

»

fi

npF

q



E

­sup
f PF

1 n

 ÿ

 iPrns

if



pXi

 qfl





,

where i is the i.i.d. Rademacher random variables; Prpi " 1q " Prpi " 1q " 1{2, and the expectation is about Xi and i. Using the statistics and learning theory van der Vaart & Wellner (1996); Bartlett et al. (2005), we can apply a bound for npFq as
npF q  CF n´1{,
with some constants CF  0 and   2.

Regarding approximation errors, we need to consider approximation of a discontinuous function; since Lemma 2 shows that a discontinuous generator is necessary to represent disconnected supports. To approximate such generators, DNNs in GANs has an advantage.
Lemma 3. (Approximation for Discontinuous g by DNNs) Suppose P ° has M disconnected supports and locally -smooth, and also P ° " Pg° holds with some g°. Then, for any Sg, there exist G, g9 P G, and a constant cg " cgpBg, Lgq  0 such that

}g9d ´ gd°}L2  cgM Sg´{D, @d P rDs. Furthermore, if P ° " Pg° has a global support and it is -smooth, (4) holds with M " 1.

(4)

6

Under review as a conference paper at ICLR 2019

Lemma 3 shows that G for GANs can approximate g° for disconnected supports with the rate (´{D) by Sg, and the rate is same in the case of global support. This implies an advantage of GANs in comparison with the other standard methods (Proposition 1).

Based on Lemma 3, we obtain the main theorem for generalization analysis.

Theorem 1. (Generalization of GANs) Suppose that P ° has M disconnected supports and locally -smooth, and we have n observations
and m samplings. Then, with F , an existing G, an estimator Pgp by (2), and finite constants c1 " c1pLf , Bf , Lg, Bgq, c2, c3 " c3pLf , Bf q  0, the following inequality holds with high probability,

dF pP °, Pgpq

 mpFrq

`

c1

aSg ` aSf ? m

` c2M DSg´{D ` npFrq loooooomoooooon

c

` c3

Sf n

.

looooooooooooooomooooooooooooooon

looooooooomooooooooon

":I

":I I

":I I I

(5)

Furthermore, if P ° has a global support and it is -smooth, (5) holds with M " 1.

Each of the terms I, II and III has the following role: I bounds an error by the m samplings, II bounds an error from approximation by G, and III bounds an error by n observations.

Proof Outline: By the definition of Pgp in (2) and standard calculation, we obtain the inequality

dF

pP

°,

Pgpq



2

sup
gPG

sup
f PF

 EPg,m

rf

pX

qs

´

EPg

rf

pX

 qs

`

inf
gPG

dF

pPg ,

P

°q

`

2dF pPn, P0q loooooomoooooon

.

loooooooooooooooooooooooomoooooooooooooooooooooooon loooooooomoooooooon

":iii

":i ":ii

To obtain i  I and iii  III, we apply an empirical process technique (van der Vaart & Wellner, 1996), especially convergence of integral probability measures (Sriperumbudur et al., 2012) and the
entropy control technique (Lemma 4 and 5 in the supplementary material). To show ii  II, we employ recent results on approximation ability of DNNs (Yarotsky, 2017; Petersen & Voigtlaender, 2017; Imaizumi & Fukumizu, 2018) and obtain an approximation bound for dF pPg, P °q (Lemma 3). Combining these results, we obtain the statement of Theorem 1.

Theorem 1 provides two trade-off relations with respect to Sg and Sf . The generator class G controls uncertainty by sampling and the approximation error, while Sf controls uncertainty of observations and discrimination. For balancing the trade-offs, we select the number of parameters (connections of
DNNs) with some constants cg, cf  0 as

Sg " cgmD{p2`Dq, and Sf " cf np´2q{,

(6)

for optimizing the bound (5). We then obtain the following corollary.

Corollary 1. (Convergence Rate of GANs)
Make the same assumptions as Theorem 1, and set Sf and Sg as in (6). Then, with high probability converging to 1, we obtain

´ ¯´

¯

dF pP °, Pgpq " O n´1{ ` O m´1{ ` m´{p2`Dq .

(7)

A selection of Fr determines the first term in (7), since  depends on Fr. For an example, when F is a set of 1-Lipschitz functions, the first term is O `n´1{p2`2Dq (Sriperumbudur et al., 2012).

Remark 1. (Heterogeneous Smoothness) Corollary 1 can be extended when P ° has different smoothness for each m P rM s, i.e., P ° is locally
m-smooth on a set Sm. In this case, we can easily extend our analysis in Theorem 1 and Corollary 1, and obtain the following convergence rate.

´ ¯´

¯

dF pP °, Pgpq " O n´1{ ` O m´1{ ` m´r{p2r`Dq ,

where r :" minmPrMs m.

5 DISCUSSION

We emphasize the theoretical results show that GANs do not suffer from the effect of disconnected supports. A larger  improves performance of GANs even with disconnected supports, as shown

7

Under review as a conference paper at ICLR 2019
in Theorem 1 and Corollary 1. This phenomenon is different from the result of the other methods discussed in Proposition 1. Hence, we can state that GANs have advantages over the other methods which are deteriorated by the disconnected property (Section 3.3). In other words, when data are generated from a probability measure with disconnected supports and sufficiently smooth in each of the sets, only GANs can estimate the measure effectively and the other methdos cannot. This advantage of GANs comes from the approximation power for discontinuous generators shown in Lemma 3.
The results (5) and (7) provide interpretation about performance of GANs. About convergence with n, the complexity of Fr and Sf control a trade-off between convergence and a power of discrimination. While smaller Sf reduce the errors in terms of dF , too small Sf can lose the power of discrimination to satisfy (1). Hence, setting Sf as in (6) can keep the discrimination power and does not worsen the overall rate of convergence Opn´1{q. About convergence with m, Sg controls the trade-off between the bias and variance of the estimator. An optimal way to select Sg is provided in (6) which depends on  and D, and it is more important when  is small (e.g.  " 2 as MMD-GAN). Based on the interpretation and the selection rule (6), our study can provide a guideline for a design of the architecture of DNNs.
5.1 RELATED WORKS
Compared with studies for understanding GANs (Dziugaite et al., 2015; Liang, 2017; Liu et al., 2017; Zhang et al., 2018; Biau et al., 2018; Arora et al., 2017), we show that GANs can avoid influences of disconnected supports, unlike the other methods, hence it is a source of the advantage of GANs. This paper is the first work to focus on the disconnected support property, while several discussions (Goodfellow, 2016; Liang, 2017; Creswell et al., 2018) focus on models and metrics of the learning scheme of GANs.
It is important to compare our result with other studies for generalization analysis. Although some existing studies (Dziugaite et al., 2015; Liang, 2017; Liu et al., 2017; Zhang et al., 2018; Biau et al., 2018; Arora et al., 2017) provide generalization analysis, they do not analyze an approximation effect, namely, they evaluate dF pP °, Pgpq ´ infgPGpP °, Pgq. Since we analyze the term infgPGpP °, Pgq, we can provide a more general bound and discuss the effect of disconnected support.
As we mentioned in the introduction, this paper is not along with studies for low-dimensional supports (Arjovsky & Bottou, 2017). We also note that an optimization aspect and gaming aspect of GANs are out of concerns of this paper. We focus on the statistical aspects of GANs such as sample complexity.
6 NUMERICAL EXPERIMENTS
We compare the numerical performance of GANs and the other methods with toy data with. We generate synthetic data from the following two settings: (A) Gaussian distribution restricted on a compact set (global support), and (B) a probability measure with two disconnected supports (density function is the black solid line in Figure 6). We generate n " 500, 1000, ..., 5000 observations and estimate the true probability measures with Wasserstein GAN, MMD-GAN, KDE (the Gaussian kernel and the Epanechnikov kernel), SDE (Fourier basis), and NB (Dirichlet process prior). Hyperparameters for these methods are selected by cross-validation. For GANs, we set m " n. We use dF to evaluate errors by GANs, and a root of the expected squared errors with the L2-norm for the other methods. The plots are the mean of 30 replications.
Figure 5 shows generalization errors by the methods. With the disconnected support case (B), we plot the estimated density in Figure 6. The black line shows the true density, the dashed line is by estimated densities of the other methods, and bars are histograms by GANs. The results by Wasserstein-GAN and MMD-GAN are almost same, so we omit the result by Wasserstein-GAN.
In Figure 5, we see that in the case of global support (A), the error of GANs and the other methods are comparable. In contrast, in the case of disconnected supports (B), the other standard methods show worse generalization and only GANs keep the high performance. From Figure 6, we can see that GANs can reveal the disconnected supports, while some of the other methods fail to fit. KDE with the Gaussian kernel represents the disconnected support by employing a small bandwidth. However, the small bandwidth yields a too sharp density, tending to worsen the generalization performance.
8

Under review as a conference paper at ICLR 2019

Figure 5: Generalization errors.

Figure 6: Estimated density functions with the case (B).

7 CONCLUSION
We investigate a generalization performance of GANs with a situation such that a support of real probability measures is divided into several sets. We find that GANs do not suffer from the division of supports, while some of the other nonparametric methods loss their efficiency by the division. Since real data are often distributed on such divided supports, the finding in this paper is related to the question of why GANs perform well with real datasets.
REFERENCES
Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and empirics. 2018.
Peter L Bartlett, Olivier Bousquet, Shahar Mendelson, et al. Local rademacher complexities. The Annals of Statistics, 33(4):1497­1537, 2005.
G Biau, B Cadre, M Sangnier, and U Tanielian. Some theoretical properties of gans. arXiv preprint arXiv:1803.07819, 2018.
Austin Animal Center. Shelter animal outcomes. http://www.austintexas.gov/ department/aac.
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1): 53­65, 2018.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, pp. 258­267. AUAI Press, 2015.
Sam Efromovich. Orthogonal series density estimation. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):467­476, 2010.
9

Under review as a conference paper at ICLR 2019
Sam Efromovich et al. Adaptive estimation of and oracle inequalities for probability densities and characteristic functions. The Annals of Statistics, 36(3):1127­1155, 2008.
Thomas S Ferguson. A bayesian analysis of some nonparametric problems. The annals of statistics, pp. 209­230, 1973.
Jon Gauthier. Conditional generative adversarial nets for convolutional face generation. Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, 2014(5):2, 2014.
Subhashis Ghosal, Aad Van Der Vaart, et al. Posterior convergence rates of dirichlet mixtures at smooth densities. The Annals of Statistics, 35(2):697­723, 2007.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5769­5779, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6629­6640, 2017.
Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effectively. arXiv preprint arXiv:1802.04474, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Daphne Koller, Nir Friedman, and Francis Bach. Probabilistic graphical models: principles and techniques. MIT press, 2009.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Tom Leonard. Density estimation, stochastic processes and prior information. Journal of the Royal Statistical Society. Series B (Methodological), pp. 113­146, 1978.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. Mmd gan: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems, pp. 2200­2210, 2017.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International Conference on Machine Learning, pp. 1718­1727, 2015.
Tengyuan Liang. How well can generative adversarial networks (gan) learn densities: A nonparametric view. arXiv preprint arXiv:1712.08244, 2017.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of generative adversarial learning. In Advances in Neural Information Processing Systems, pp. 5551­5559, 2017.
William Lotter, Gabriel Kreiman, and David Cox. Unsupervised learning of visual structure using predictive generative networks. arXiv preprint arXiv:1511.06380, 2015.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579­2605, 2008.
Pascal Massart. Some applications of concentration inequalities to statistics. In Annales-Faculte des Sciences Toulouse Mathematiques, volume 9, pp. 245­303. Université Paul Sabatier, 2000.
10

Under review as a conference paper at ICLR 2019
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in Neural Information Processing Systems, pp. 1823­1833, 2017.
James R Munkres. Topology. Prentice Hall, 2000.
Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1): 141­142, 1964.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. arXiv preprint arXiv:1709.05289, 2017.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation function. arXiv preprint arXiv:1708.06633, 2017.
Mathieu Sinn and Ambrish Rawat. Non-parametric estimation of jensen-shannon divergence in generative adversarial network training. In International Conference on Artificial Intelligence and Statistics, pp. 642­651, 2018.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, Gert RG Lanckriet, et al. On the empirical estimation of integral probability metrics. Electronic Journal of Statistics, 6:1550­1599, 2012.
Elias M Stein. Singular integrals and differentiability properties of functions (PMS-30), volume 30. Princeton university press, 2016.
Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.
CJ Stone. Optimal global rates of convergence for nonparametric regression. The Annals of Statistics, 10:1040­1053, 1982.
Ilya O Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Schölkopf. Adagan: Boosting generative models. In Advances in Neural Information Processing Systems, pp. 5430­5439, 2017.
Alexandre B Tsybakov. Introduction to nonparametric estimation, 2009.
AW van der Vaart and JH van Zanten. Rates of contraction of posterior distributions based on gaussian process priors. The Annals of Statistics, 36(3):1435­1463, 2008.
AW van der Vaart and Jon Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer Science & Business Media, 1996.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94: 103­114, 2017.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discriminationgeneralization tradeoff in gans. Proceedings of International Conference on Learning Representations, 2018.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.
11

Under review as a conference paper at ICLR 2019

Supplementary Materials for "Understanding GANs via disconnected Support Detection"
We introduce a new notation P f :" EX,,P rf pXqs with a probability measure P and a function f . For a set  with equipped distance d, let N p , , dq be a covering number which is a minimum number of -balls to cover .
A SOME ADDITIONAL INFORMATION
A Rigorous Definition of S,J
We consider a set represented by a combination of multiple horizon functions, which has been used in Petersen & Voigtlaender (2017). Given -smooth function h P HpID´1q with   1, a horizon function h : ID Ñ t0, 1u is defined for some d P rDs as
h " px1, . . . , xd´1, xd  hpx1, . . . , xd´1, xd`1, . . . , xDq, xd`1, ..., xDq,
where  is the Heaviside function; pxq " 1txPID|xd0u. We define a set by the intersection of J horizon functions h1 , ..., hJ ; namely the family of sets is defined by
S,J :" S  r0, 1sD | 1S " h1 b ¨ ¨ ¨ b hJ ( .
Intuitively, h is regarded as an -smooth curved surface in ID, and h describes a set which is one side of the surface. Also, S P S,J is a set which is a intersection of J sets by h1 , ..., hJ . A Support of Probability Measures Let Nx denote an open neighborhood of x P ID, and For a probability measure P , a support of P is defined as
"* SupppP q :" x P ID | P pNxq  0, @Nx P  .

B PROOFS

B.1 PROOF OF LEMMA 1

Fix m P rM s and a corresponding Srm, Sm and gm. For any B P pSmq, the definition of m yields



PX pBq " PZ pm´1pBqq "

pZ pzqdz,

m´1 pB q

where pZ is a density function of a uniform measure PZ . By changing variables x " mpzq, we have



pZ pzqdz "
m´1 pB q

pZ pm´1pxqqJm pxqdx,
B

where Jm pxq " | det gm´1pxq|. Using pZ pzq " 1 for all z P ID, we obtain the following form of PX pBq using a function pm : Srm Ñ Sm as


PX pBq " Jgm pxqdx ": pmpxqdx,
BB

(8)

and pm is -smooth since m is  ` 1-smooth and bijective.

B.2 PROOF OF LEMMA 2
Firstly, we show the first points. Suppose that g is a continuous mapping. By the generalized intermediate value theorem (Theorem 24.3 in Munkres (2000)), we know that gpIDq is connected since ID is a connected set. Thus, a support of Pg is connected. However, Pg has a disconnected support, thus there is a contradiction.

12

Under review as a conference paper at ICLR 2019

B.3 PROOF OF LEMMA 3

In this proof, a À b denotes that b is larger than a up to a finite constant. a -- b denotes that a À b and a Á b hold.
By the definition of tgmumPrMs for the measure with local smoothness, we consider an explicit form of gm. By Stein (2016), we can extend gm : Srm Ñ Sm to grm : ID Ñ Sm since boundaries of Srm are Lipschitz continuous. Then, we provide the following formulation
grmpxq " pm,1pxq, ..., m,DpxqqJ,
where m,d P HpIDq. Then, we obtain the form of g° as g° " ÿ grm b 1Srm .
mPrM s
Also, by the definition of S2,J which contains Srm, we obtain the form 1Srm " â hm,j ,
j PrJ s
with existing hm,j . Then, we have g° " ÿ grm â hm,j .
mPrM s jPrJs

Preliminarily, we apply sub-neural networks from Yarotsky (2017) and Petersen & Voigtlaender

(2017).

Let

 r` s

be

a

network

for

summation

such

that

r`spx1, ..., xD1 q

"


dPrD1 s

xd,

and

 r^ s

be

a

network

for

approximate

multiplication

such

as

| r^ spx1 ,

...,

xD1

q

´


dPrD1 s

xd|



with some  0 for all x, x1 P I (Proposition 3 in Yarotsky (2017) and Lemma 1 in Imaizumi &

Fukumizu (2018)).

We

consider

approximation


mPrM s

m,d

Â
j PrJ s

hm,j

for

each

d

P

rDs.

Let r,d,ms and

rh,m,js for d P rDs, m P rM s and j P rJs, and we will specify the networks later. Also, let

rS,ms " r^sprh,m,1sp¨q, ..., rh,m,1sp¨qq.

We consider a neural network

rds " r`spr^spr,d,1sp¨q, rS,1sp¨qq, ..., r^spr,d,M sp¨q, rS,M sp¨qqq.

Then, an approximation error is evaluated as

>>

>>

>ÿ >

m,d

â

hm,j

´



rd

> s>

>>

>mPrM s

j PrJ s

>L2

>>



ÿ

>> >>m,d â hm,j ´ r^spr,d,msp¨q, rS,msp¨qq>>

mPrM s >

j PrJ s

>
L2

>>



ÿ

>> >>m,d â hm,j ´ r,d,ms b rS,ms>>

mPrM s >

j PrJ s

>
L2

>>

` ÿ >>r,d,ms b rS,ms ´ r^spr,d,msp¨q, rS,msp¨qq>>

>
mPrM s

>L2

ÿ>

>

 >>m,d b 1Srm ´ r,d,ms b rS,ms>>L2 ` M ^

mPrM s

ÿ>

>

 >>m,d b p1Srm ´ rS,msq ` pm,d ´ r,d,msq b rS,ms>>L2 ` M ^

mPrM s

ÿ>

>

 }m,d}L8 >>1Srm ´ rS,ms>>L2 ` }m,d ´ r,d,ms}L2 }rS,ms}L8 ` M ^

mPrM s

ÿ ": T1,m ` T2,m ` M ^,

mPrM s

13

Under review as a conference paper at ICLR 2019

where the last inequality follows the Hölder's inequality.
About T1,m, there exists a corresponding r,d,ms such that
}m,d ´ r,d,ms}L2 À },d,m}1´p`1q{D, by following Theorem A.8 in Petersen & Voigtlaender (2017). Also, since m,d is bounded by its smoothness and compact support, we have }m,d}L8  8 hence
T1,m À },d,m}´1 p`1q{D.

For T2,m, we specify rh,m,js as Theorem 3.1 in Petersen & Voigtlaender (2017). Then, we evaluate the following as

}m,d ´ r,d,ms}L2

>>

>>



> >

â

hm,j

´ r^sprh,m,1sp¨q, ..., rh,m,1sp¨qq>>

>j PrJ s

>
L2

> >>

>

> >>

>



> >

â

hm,j

´

â

rh,m,j s>>

`

> >

â

hm,j

´ r^sprh,m,1sp¨q, ..., rh,m,1sp¨qq>>

>j PrJ s

j PrJ s

>
L2

>j PrJ s

>
L2

ÿ À



´> >

¯

>>hm,j1

> >L2

_

}rh,m,j1 }L2

>>hm,j

´



rh,m,j

> s>L2

`

^

jPrJs j1PrJsztju



ÿ

>>hm,j

´



rh,m,j

> s>L2

`

^.

j PrJ s

Here, the last inequality follows the boundedness of hm,j1 and rh,m,js by Theorem 3.1 in Petersen & Voigtlaender (2017). Also, Theorem 3.1 in Petersen & Voigtlaender (2017) provides an existence
of rh,m,j such that

}hm,j ´ rh,m,j s}L2  }h,m,j }1´{pD´1q. We apply the boundedness of rh,m,js, we have

T2,m À ÿ }h,m,j }´1 {pD´1q ` ^.
j PrJ s

Combining the bounds for T1,m and T2,m, we bound

>>

>>

>ÿ >

m,d â hm,j ´ rds>>

>>

>mPrM s

j PrJ s

>L2

 ÿ },d,m}´1 p`1q{D ` ÿ ÿ }h,m,j }1´{pD´1q ` pM ` 1q ^.

mPrM s

mPrM s jPrJs

Here,

we

consider

a

parameter

S

"


dPrDs

}d}1

such

that

S

--

},d,m}1

--

}h,m,j }1

--

}^}1.

Also, following Yarotsky (2017) and Petersen & Voigtlaender (2017), a proper selection of L " |^|

and B " }^}8 provides ^ À }^}´0 {D. Then, we have

>>

>>

>ÿ >

m,d

â

hm,j

´



rd

> s>

À M J S´{D.

>>

>mPrM s

j PrJ s

>L2

Since J is finite, we obtain the result.

B.4 PROOF OF THEOREM 1

By the definition of gp in (2), the following inequality holds

dF pPn, Pgp,mq " suppPnf ´ Pgp,mf q  dF pPn, Pg,mq " suppPnf ´ Pg,mf q,

f PF

f PF

(9)

14

Under review as a conference paper at ICLR 2019

for arbitrary g P G.

We consider a bound for dF pP °, Pgpq as

dF pP °, Pgpq " suppP °f ´ Pgpf q
f PF

" suppP °f ´ Pnf ` Pnf ´ Pgp,mf ` Pgp,mf ´ Pgpf q
f PF

 suppP °f ´ Pnf ` Pgp,mf ´ Pgpf q ` suppPnf ´ Pg9,mf q,

f PF

f PF

where the inequality follows (9) with an existing g9 P G. We will provide a detailed construction of g°. We continue the bound as

dF pP °, Pgpq

 suppP °f ´ Pnf ` Pgp,mf ´ Pgpf q ` suppPnf ´ P °f ` P °f ´ Pg9 f ` Pg9 f ´ Pg9,mf q

f PF

f PF

 2 sup sup |Pg,mf ´ Pgf | ` suppP °f ´ Pg9 f q ` 2 sup |Pnf ´ P °f |

gPG f PF

f PF

f PF

": i ` ii ` iii.

Here, i denotes an effect from m samplings, ii denotes an approximation error, and iii denotes an uncertainty with the n observations.

To evaluate i and iii, we provide the following lemma. This result follows a standard technique of the empirical process theory and we provide its outline for a sake of completeness.

Lemma 4. Let H be a some set of measurable functions and X1, ..., Xn ,, P be i.i.d. n observations. Suppose that EP rh2pXqs  2 and }h}L8  Ch hold with finite parameters 2  0 and Ch  0. Then, there exists a constant C and we obtain



sup
hPH

    

1 n

ÿ
iPrns

hpXiq

´

EP



rhpX

 qs







# inf 4
0

`

 Ch 12


c log N p

, H, } n

¨

}L8 q d

c

`

2 2 ` 4C n

`

 Ch n

^2 3

+ ` C

with probability at least 1 ´ 2 expp´ q for all   0.

Proof. At the beginning, we bound an expectation of the empirical process (Steinwart & Christmann, 2008; Bartlett et al., 2005; Massart, 2000; Sriperumbudur et al., 2012). Afterward, we evaluate a concentration of the empirical process around the expectation.

By applying the symmetrization and concentration techniques (Proposition 7.10 in Steinwart & Christmann (2008)), we obtain

»

fi »  fi

EP bn

­sup
hPH

 

1

 n



ÿ
iPrns

hpXiq

´


ErhpX qsfl 




2EP

bn b bn

­sup
hPH

    

1 n

ÿ
iPrns

 uihpXiqfl
 

,

where ui ,,  is the Rademacher variable which takes 0 or 1 with probability 0.5. Combining this bound with the Taralgand's inequality (Theorem A.9.1 in Steinwart & Christmann (2008)), we obtain
the following inequality



sup
hPH

    

1 n

ÿ
iPrns

hpXiq

´

EP


rhpX qs 


»

fi



p1

`

qEP bnbbn

­sup

  

1

hPH

 

n



ÿ

ui

hpXi

 qfl



iPrns



`

c 2 2 n

`

 Ch n

^2 3

`

1 ,


(10)

with probability at least 1 ´ expp´ q for all   0 and   0.

15

Under review as a conference paper at ICLR 2019

About the term with the Rademacher variable, we also apply a similar strategy (Lemma A.4 in Bartlett et al. (2005)), then obtain

»  fi »  fi

EP bnbbn

­sup

  

1

hPH  n





ÿ uihpXiqfl



iPrns





1

1 ´

1 Ebn

­sup

  

1

hPH  n





ÿ uihpXiqfl



iPrns



`

 1Ch , n1p1 ´ 1q

(11)

with probability at least 1 ´ expp´ 1q for all  1  0 and 1  0.

Let } ¨ }n be an empirical norm as }f }n2

"

n´1


iPrns

f

pXi

q2

.

About the term

"

i

Ebn

suphPH

 

1 n


iPrns

uihpXi

q

, we apply the chaining technique

and obtain

»

fi

Ebn

­sup

  

1

hPH

 

n



ÿ uihpXiqfl



iPrns





# inf 42
2 0

 Crh
` 12
2

c log N p

, H, } ¨ }nq d n

+



inf

# 42

`

12



Ch

c

log

N

p

, H, } ¨ }L8 q d

+ ,

2 0

2

n

(12)

where the last inequality follows a bound for an empirical norm and the boundedness of H.

Combining (10), (11) and (12) and changing variables, we obtain the result.

To bound I with Xr1, ..., Xrm ,, Pg, we consider the following value

 



sup
gPG

sup
f PF

 

1

 m



ÿ
iPrms

f pXriq

´



EPg

rhpX

 qs





"

sup
gPG

sup
f PF

 

1

 m



ÿ
iPrms

f



gpZiq

´

EPZ rf




gpX qs 




":

sup
hPH

    

1 m

ÿ
iPrms

hpZiq

´

EPZ



rhpX

 qs





,

where we define H " th " f  g | f P F, g P Gu. To apply Lemma 4, we investigate a covering number N p , H, } ¨ }L8 q. Lemma 5. Assume that f P F is L1-Lipschitz continuous. We obtain
log N p , H, } ¨ }L8 q  log N pp1 ` L1q´1 , G, } ¨ }L8 q ` log N pp1 ` L1q´1 , F , } ¨ }L8 q

Proof. Fix  0. Let G  G and F  F be covering sets as a set of centers of -balls for the covering G and F . Obviously, |G| " N p , G, } ¨ }L8 q and |F | " N p , F , } ¨ }L8 q. We define a subset
"* H :" h " f  g | g P G, h P F  H,

and we known |H| " |G| ^ |F |.
For any h P H, there exist g P G and f P F, then f " f  g holds. Also, by the definition of covering sets, there exist f 1 P F and g1 P G such that }f ´ f 1}L8  and }g ´ g1}L8  . Let h1 " f 1  g1, and we measure the distance
}h ´ h1}L8  }f  g ´ f 1  g1}L8 " }f  g ´ f  g1 ` f  g1 ´ f 1  g1}L8  }f  g ´ f  g1}L8 ` }f  g1 ´ f 1  g1}L8  L1}g ´ g1}L8 ` }f ´ f 1}L8  pL1 ` 1q .
Here, the third inequality follows the Lipschitz property of f P F. Here, we know that H is covered by pL1 ` 1q -balls with the center H. Since |H| " |G| ^ |F |, the result holds.

16

Under review as a conference paper at ICLR 2019

Now, we have the following entropy bound
log N p , H, } ¨ }L8 q  log N pp1 ` L1q´1 , G, } ¨ }L8 q ` log N pp1 ` L1q´1 , F , } ¨ }L8 q  pSg ` 1q logp2pL1 ` 1q ´1pLg ` 1qDgBgq
"* ` min pSf ` 1q logp2pL1 ` 1q ´1pLf ` 1qDf Bf q, Cp1 ` L1q ´

with Dg :"  PrLg`1spD ` 1q and Df :"  PrLf `1spD ` 1q. Let f :" 2pLf ` 1qDf Bf , and
N pFrq :" log N p , Fr, } ¨ }nq for brevity. Here, we apply Lemma 8 in Schmidt-Hieber (2017) for the entropy bound for G and F. Using the entropy bound and Lemma 4, we obtain

2 sup |Pnf ´ P °f |
f PF

 4 `

12 n1{2

 CF

!

min N



pFrq, pSf

` 1q logpf

´1

)1{2 q

d

`

p2 2 ` Cq1{2 n1{2

`

 Chp2{3 n

`

C q



4

`

12 n1{2

 CF N


pFrq1{2d

`

12 n1{2 pSf

`

1q1{2plog1{2

f

`

CFr

log1{2

CFr

´



log1{2

q

`

p2 2 ` Cq1{2 n1{2

`

 Chp2{3 n

`

C q



npFrq

`

1 n1{2

´ pSf

`

1q1{2A1

`

¯ A2

`

A3 n

,

(13)

with some   0, A1 " 12 log1{2 f , A2 " CFr log1{2 CFr ` p2 2 ` Cq1{2, and A3 "  Chp2{3 `

C q.

Also, we set npFrq " 4 `

12 n1{2

CF


NpL1 `1q´1

pFrq1{2d

.

Then, we have

iii



npFrq

`

1 n1{2

´ pSf

`

1q1{2A1

`

¯ A2

`

A3 n

.

(14)

About I, we define g :" 2pLf ` 1qDgBg and obtain a similar bound as

i



mpFrq

`

1 m1{2

´ pSg

`

1q1{2A11

`

¯ A12

`

A31 m

,

where A11 " 12plog1{2 f ` log1{2 gq, A2 " CFr log1{2 CFr ` CG log1{2 CG ` 2p2 2 ` Cq1{2, and A3 " 2 Chp2{3 ` Cq.

About ii, we evaluate the error from approximation by constructing a specific deep neural network for generators. We apply Lemma 3 and let g9 " pg91, ..., g9Dq be a generator specified in Lemma 3.

ii " Pg° f ´ Pg9 f 
" pf  g° ´ f  g9qdPZ

  L1 }g° ´ g9}2dPZ

¨ 1{2



ÿ  L1 

|gd°pxq ´ g9dpxq|2dPZ pxq,

dPrDs

¨ 1{2

ÿ " L1 

}gd° ´ g9d}2L2 ,

dPrDs

 cgL1DM Sg´{D,

17

Under review as a conference paper at ICLR 2019
which follows L1-Lipschitz continuity of f , the Jensen's inequality, the Cauchy-Schwartz inequality, compactness of the support ID, and uniformity of PZ . Then, we have
ii  c2M DSg´{D. Combining the result, we obtain the result of Theorem 1. B.5 PROOF OF PROPOSITION 1 When P are globally smooth, we obtain a -smooth density function on ID by its definition. Due to the smoothness, the studies for nonparametric statistics (Nadaraya, 1964; Ghosal et al., 2007; Efromovich, 2010; van der Vaart & van Zanten, 2008; Tsybakov, 2009) guarantees that the methods (KDE,NB,SDE, and GP) obtain the rate Opn´{p2`Dqq with respect to the roof of L2 norm. When P have disconnected supports and locally smooth, we consider a following specific P . Fix M " 2. Let us define supports as S1 " Sr1 " tx P ID | x1  0.5u  ID and S2 " Sr2 " tx P ID | x1  0.5u  ID. Also, g1pzq " z and g2 : Sr2 Ñ S2 as
g2pzq " pg2,1pz1q, ..., g2,DpzDqqJ, where g2,1pz1q " 0.6 ` cpz1 ´ 0.5q1{3 and g2,dpzdq " zd for d P rDszt1u with a constant c. Then, by the proof of Lemma 1, p2pxq on S2 is a quadratic function with respect to z1 is 1-times differentiable but not twice-differentiable at the boundary tx P ID | x1 " 0.6u. Hence, the studies (Nadaraya, 1964; Ghosal et al., 2007; Efromovich, 2010; van der Vaart & van Zanten, 2008; Tsybakov, 2009) provides that the generalization error of the methods is bounded by Opn´{p2`Dqq with  " 1.
18

