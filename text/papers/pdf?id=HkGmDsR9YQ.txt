Under review as a conference paper at ICLR 2019
GENERALIZATION AND REGULARIZATION IN DQN
Anonymous authors Paper under double-blind review
ABSTRACT
Deep reinforcement learning (RL) algorithms have shown an impressive ability to learn complex control policies in high-dimensional environments. However, despite the ever-increasing performance on popular benchmarks like the Arcade Learning Environment (ALE), policies learned by deep RL algorithms can struggle to generalize when evaluated in remarkably similar environments. These results are unexpected given the fact that, in supervised learning, deep neural networks often learn robust features that generalize across tasks. In this paper, we study the generalization capabilities of DQN in order to aid in understanding this mismatch between generalization in deep RL and supervised learning methods. We provide evidence suggesting that DQN overspecializes to the domain it is trained on. We then comprehensively evaluate the impact of traditional methods of regularization from supervised learning, 2 and dropout, and of reusing learned representations to improve the generalization capabilities of DQN. We perform this study using different game modes of Atari 2600 games, a recently introduced modification for the ALE which supports slight variations of the Atari 2600 games used for benchmarking in the field. Despite regularization being largely underutilized in deep RL, we show that it can, in fact, help DQN learn more general features. These features can then be reused and fine-tuned on similar tasks, considerably improving the sample efficiency of DQN.
1 INTRODUCTION
Recently, reinforcement learning (RL) has proven very successful on complex high-dimensional problems, in large part due to the increase in computational power and to the use of deep neural networks for function approximation (e.g., Mnih et al., 2015; Silver et al., 2016). Despite the generality of the proposed solutions, applying these algorithms to slightly different environments generally requires agents to learn the new task from scratch. Practitioners often realize that the learned policies rarely generalize to other domains, even when they are remarkably similar, and that the learned representations are seldom reusable.
Deep neural networks, though, are lauded for their generalization capabilities (e.g., LeCun et al., 1998). Some communities heavily rely on reusing representations learned by neural networks. In computer vision, classification and segmentation algorithms are rarely trained from scratch; instead they are initialized with pre-trained models from larger datasets like ImageNet (e.g., Razavian et al., 2014; Long et al., 2015). The field of natural language processing has also seen successes in reusing and refining weights from certain layers of neural networks using pre-trained word embeddings, with more recent techniques able to reuse all weights of the network (e.g., Howard & Ruder, 2018).
In light of the successes of traditional supervised learning methods, the current lack of generalization or reusable knowledge (e.g., policies, representation) acquired by current deep RL algorithms is somewhat surprising. In this paper we investigate whether the representation learned by deep RL methods can be generalized, or at the very least reused and refined on small variations to the task at hand. First, we evaluate the generalization capabilities of DQN (Mnih et al., 2015). We further explore whether the experience gained by the supervised learning community to improve generalization and to avoid overfitting could be used in deep RL. We employ conventional supervised learning techniques, albeit largely unexplored in deep RL, such as fine-tuning (i.e., reusing and refining the representation) and regularization. We show that a learned representation trained with regularization allows us to learn more general features capable of being reused and fine-tuned. Besides improving the generalization capabilities of the learned policies this fine-tuning procedure has the potential to
1

Under review as a conference paper at ICLR 2019

greatly improve sample efficiency on settings in which an agent might face multiple variations of the same task. Finally, the results we present here also can be seen as paving a way towards novel curriculum learning approaches for deep RL.
We perform our experiments using different game modes and difficulties of Atari 2600 games, a newly introduced feature of the Arcade Learning Environment (ALE; Bellemare et al., 2013). These game modes allow agents to be trained in one environment while being evaluated in a slightly different environment that still captures key concepts of the original environment (e.g., game sprites, agent goals, dynamics). This use of game modes is itself a novel approach for measuring our progress toward a longstanding goal of agents that can learn to be generally competent and generalize across tasks (Bellemare et al., 2013; Machado et al., 2018; Nichol et al., 2018). This paper also introduces the first baselines for the different modes of Atari 2600 games.

2 BACKGROUND

2.1 REINFORCEMENT LEARNING

Reinforcement learning (RL) is a problem where an agent interacts with an environment with the

goal of maximizing some form of cumulative long term reward. RL problems are often modeled

as a Markov decision process (MDP), defined by a 5-tuple S, A, p, r, . At a discrete time step

t the agent observes the current transition to the next state St+1

state S

St  S and according to

chooses an action At  the transition dynamics

fAunctotiopnropb(asbi|liss,tiac)al=l.y

P (St+1 = s | St = s , At = a). The agent receives a reward signal Rt+1 according to the reward

function r : S × A  R. The agents goal is to learn a policy  : S × A defined as the conditional

probability of taking action a in state s written as (a | s). The learning agent refines its policy with

tfhroemobtijmecetivt,edoeffimneadxibmyiGzint g=.the ekx=p0eckteRdtr+ektu+r1n,wthhearteis,

the cumulative discounted reward  [0, 1) is the discount factor.

incurred

Q-learning (Watkins & Dayan, 1992) is a traditional approach to learning an optimal policy

from samples obtained from interactions with the environment. It is used to learn an opti-

mal state-action value function via a bootstrapped iterative method. For a given policy  we

qdefi(sn,ea)th=e.

stat[e-action value functi]on as E Gt|St = s, At = a . The

the expected return conditioned on a state and action agent iteratively updates the state-action value function

based on samples from the environment using the update rule

Q(St,

At)



Q(St,

At)

+

[  Rt+1

+



max
a A

Q(St+1,

a)

-

Q(St,

] At)

where t denotes the current timestep and  the step size. Generally, due to the exploding size of the state space in many real-world problems, it is intractable to learn a state-action pairing for the entire MDP, with researchers and practitioners often resorting to learning an approximate to q.

DQN approximates the state-action value function such that q(s, a)  Q(s, a; ), where  denotes

the weights of a neural network. The network takes as input some encoding of the current state St and outputs |A| scalars corresponding to the state-action values for that given state. DQN is trained

to minimize

LDQN

=

E
St, At, Rt+1, St+1

[( Rt+1
 U (·)

+

max
a A

Q(St+1, a;

-)

-

Q(St,

At;

))2]

where (St, At, Rt+1, St+1) are uniformly sampled from U (·), the experience replay buffer filled with experience collected by the agent. The weights - of a duplicate network are updated less frequently for stability purposes.

2.2 SUPERVISED LEARNING
In the supervised learning problem we are given a dataset of examples represented by a matrix X  Rm×n with m training examples of dimension n, and a vector y  R1×m denoting the output target yi for each training example Xi. We want to learn a function which maps each training example Xi to its predicted output label y^i. The goal is to learn a robust model that accurately predicts yi from Xi while also being able to generalize to unseen training examples. In this paper

2

Under review as a conference paper at ICLR 2019

we focus on using a neural network parameterized by the weights  to learn the function f such that y^i = f (Xi; ). We typically train these models by minimizing

min


 2

22

+

1 m

m L(yi,

y^i)

=

min


 2

22

+

1 m

m

L(yi,

f (Xi;

))

i=1 i=1

where L is a differentiable loss function which outputs a scalar determining the quality of the predic-
tion (e.g., squared error loss). The first term is a form of regularization, i.e., 2 regularization, which encourages generalization. 2 regularization imposes a penalty on large weight vectors with  being the weighted importance of the regularization term.

Another popular regularization technique is dropout (Srivastava et al., 2014). When using dropout, during forward propagation each neural unit has a chance of being set to zero according to a Bernoulli distribution with probability p  [0, 1], referred to as the dropout rate. Dropout discourages the network from relying on a small number of neurons to make a prediction, making it hard for the network to memorize the dataset.

Prior to training, the network parameters are usually initialized through a stochastic process (e.g., Xavier initialization; Glorot & Bengio, 2010). We can also initialize the network using pre-trained weights from a different task. If we reuse one or more pre-trained layers we say the weights encoded by those layers will be fine-tuned during training (e.g., Razavian et al., 2014; Long et al., 2015).

3 THE ALE AS A PLATFORM FOR EVALUATING GENERALIZATION
The Arcade Learning Environment (ALE) is a platform used to evaluate agents across dozens of Atari 2600 games (Bellemare et al., 2013). It has become one of the standard evaluation platforms in the field and has led to a number of exciting algorithmic advances (e.g., Mnih et al., 2015). The ALE poses the problem of general competency by having agents use the same learning algorithm to perform well in as many games as possible, while learning without using game specific knowledge. Learning to play multiple games with the same agent, or learning to play a game faster by leveraging knowledge acquired in a different game is much harder, with fewer successes being known (e.g., Rusu et al., 2016; Kirkpatrick et al., 2016; Parisotto et al., 2016; Schwarz et al., 2018; Espeholt et al., 2018).
In this paper, we use the different modes and difficulties of Atari 2600 games to evaluate a neural network's ability to generalize in high-dimensional state spaces. Game modes, originally native to the Atari console, were recently added in the ALE (Machado et al., 2018). They give us modifications of the default environment dynamics and state space, often modifying sprites, velocities, and partial observability. These modes pose a tractable way to investigate generalization of RL agents in a high-dimensional environment. Instead of requiring an agent to play multiple games that are visually very different or even non-analogous, it requires agents to play games that are visually very similar and that can be played with policies that are very similar, at least from a human perspective.
We use 13 flavours (combinations of a mode and a difficulty) obtained from 4 games: FREEWAY, HERO, BREAKOUT, and SPACE INVADERS. In FREEWAY, the different modes vary the speed and number of vehicles, while different difficulties change how the player is penalized for running into a vehicle. In HERO, subsequent modes start the player off at increasingly harder levels of the game. The mode we use in BREAKOUT makes the bricks partially observable. The used modes in SPACE INVADERS allow for oscillating shield barriers, increasing the width of the player sprite, and partially observable aliens. Full explanations of specific games, their modes, and their difficulties can be found in Appendix A. Figure 1 provides screenshots showing side by side comparisons of some of the modes explored in this paper. When reading the analyses of this paper it is important to keep in mind how remarkably similar these modes are.

4 GENERALIZATION OF THE LEARNED POLICIES AND OVERFITTING
In order to test the generalization capabilities of DQN we first evaluate whether a policy learned in one flavour can perform well in a different flavour. As afformentioned, different modes and difficulties of a single game look very similar. If the representation encodes a robust policy we might expect it to be able to generalize to slight variations of the underlying reward signal, game

3

Under review as a conference paper at ICLR 2019
Figure 1: Each column shows variation between two selected flavours of each game. From left to right: FREEWAY, HERO, BREAKOUT, and SPACE INVADERS.
dynamics, or observations. Evaluating the learned policy in a similar but different flavour can be seen as evaluating generalization in RL, similar to cross-validation in supervised learning. To evaluate DQN's ability to generalize across flavours we evaluate the learned -greedy policy on a new flavour after being trained for 50M frames in the default flavour, m0d0 (mode 0, difficulty 0). We measure the cumulative reward averaged over 100 episodes in the new flavour, adhering to the evaluation protocol suggested by Machado et al. (2018). The results are summarized in Table 1. Baseline results where the agent is trained from scratch for 50M frames in the flavour we use for evaluation are summarized in the baseline column. Theoretically, this baseline can be seen as an upper bound on the performance DQN can achieve in that flavour, as it represents the agent's performance when evaluated in the same flavour it was trained on. Full baseline results with the agent's performance after different number of frames can be found in Appendix B. We can see in the results that the policies learned by DQN do not generalize well to different flavours, even when the flavours are remarkably similar. For example, in FREEWAY, a high-level policy applicable to all flavours is to go up while avoiding cars. Perhaps surprisingly, this does not seem to be what DQN learns. For example, the default flavour m0d0 and m4d0 have exactly the same sprites on the screen, the only difference is that in m4d0 some cars accelerate and decelerate over time. The close to optimal policy learned in m0d0 is only able to score 15.8 points when evaluated on m4d0, which is approximately half of what the policy learned from scratch in that flavour achieves (29.9 points). The learned policy when evaluated on flavours that differ more from m0d0 perform even worse. As previously mentioned, the different modes of HERO can be seen as giving the agent a curriculum or a natural progression. Interestingly, the agent trained in the default mode for 50M frames can progress to at least level 3 and sometimes level 4. Mode 1 starts the agent off at level 5, and performance in this mode suffers greatly during evaluation. There are very few game mechanics added to level 5, indicating that perhaps the agent is memorizing trajectories instead of learning a robust policy capable of solving each level. The results in some flavours suggest that the agent is overfitting to the flavour it is trained on. We tested this hypothesis by periodically evaluating the policy being learned in each of the other flavours of that game. This process involved taking checkpoints of the network at every 500, 000 frames and evaluating the -greedy policy in the prescribed flavour for 100 episodes, again further averaged over five runs. The obtained results in FREEWAY, the most pronounced game in which we see this overfitting trend, are depicted in Figure 2. Learning curves for all flavours can be found in Appendix C. In FREEWAY, while we see the policy's performance flattening out in m4d0, we do see the traditional bell-shaped curve associated to overfitting in the other modes. At first, improvements in the original policy do correspond to improvements in the performance of that policy in other domains. With time, it seems that it starts to refine its policy for the specific flavour it is being trained on, overfitting to that flavour. With other game flavours being significantly more complex in their dynamics and gameplay, we do not observe this prominent bell-shaped curve though. For example, in BREAKOUT, we actually observe a monotonic increase in performance throughout the evaluation process.
4

Under review as a conference paper at ICLR 2019

GAME VARIANT

m1d0

FREEWAY

m1d1

m4d0

HERO

m1d0 m2d0

BREAKOUT

m12d0

m1d0

SPACE INVADERS m1d1

m9d0

EVALUATION 0.2 (0.2) 0.1 (0.1) 15.8 (1.0) 82.1 (89.3) 33.9 (38.7) 43.4 (11.1)
258.9 (88.3) 140.4 (61.4) 179.0 (75.1)

LEARN SCRATCH 4.8 (9.3) 0.0 (0.0) 29.9 (0.7)
1425.2 (1755.1) 326.1 (130.4)
67.6 (32.4) 753.6 (31.6) 698.5 (31.3) 518.0 (16.7)

Table 1: Direct policy evaluation. Each game was initially trained in the default mode for 50M frames then evaluated in each listed game flavour. Reported numbers are the average over 5 runs. Standard deviation is reported between parentheses.

Freeway Policy Evaluation
30 25 m4d0 20 m1d0 15 m1d1 10
5

Cumulative Reward

0 10M 20M 30M 40M
Frames before evaluation

50M

Figure 2: Performance of an agent that was trained in the default mode of FREEWAY and evaluated at every 500, 000 frames in each corresponding mode. Results are averaged over five seeds. The y-axis is log scaled.

In conclusion, when looking at Table 1, it seems that the policies learned by DQN struggle to generalize to even small variations encountered in game flavours. This lack of generalization is surprising, and results as seen in FREEWAY exhibit a troubling notion of overfitting. Based on these results we aim to evaluate whether deep RL could benefit from established methods from supervised learning promoting generalization and reducing overfitting.
5 REGULARIZATION IN DEEP RL
In order to evaluate the hypothesis that the observed lack of generalization is due to overfitting, we revisit some popular regularization methods from the supervised learning literature. The two forms of regularization we test are dropout and 2 regularization.
First we want to understand the effect of regularization on evaluating the learned policy in a different flavour. We do so by applying dropout to the first four layers of the network during training, that is, the three convolutional layers and the first fully connected layer. We simultaneously apply 2 regularization on all weights in the network based on preliminary experiments that showed an additive effect when combining dropout and 2 regularization. This confirms, for example, Srivastava et al.'s (2014) result that these methods provide benefit in tandem.
We follow the same evaluation scheme described when evaluating the unregularized policy to different flavours. We evaluate the policy learned after 50M frames of the default mode of each game. A grid search was performed on FREEWAY to find reasonable hyperparameters for the dropout rate p  {0.05, 0.1, 0.2, 0.3, 0.4, 0.5} and the weighted regularization parameter   {10-2, 10-3, 10-4}. These parameters were then used for each subsequent flavour. Notably, significantly smaller dropout values were required compared to heuristics used in supervised learning, although this could be due to the small size of the network in question. We ended up choosing  = 10-4, p = 0.05 for the first three convolutional layers, and p = 0.1 for the first fully connected layer. We contrast these results with the results presented in the previous section. This evaluation protocol allows us to directly evaluate the effect of regularization on the learned policy's ability to generalize. A baseline agent trained from scratch for 50M frames in each flavour is also provided. The results are presented in Table 2 with the evaluation learning curves being available in the Appendix.
When using regularization during training we sometimes observe a performance hit in the default flavour. Dropout generally requires increased training iterations to reach the same level of performance sans-dropout. Suprisingly, we did not observe this performance hit in all games. Nevertheless, maximal performance in one flavour is not our goal. We are interested in the setting where one may be willing to take lower performance on one task in order to obtain higher performance, or adaptability, on future tasks. Nevertheless, full baseline results using regularization in the default flavour can also be found in Table 7 in the Appendix.
5

Under review as a conference paper at ICLR 2019

GAME VARIANT

FREEWAY
HERO BREAKOUT SPACE INVADERS

m1d0 m1d1 m4d0 m1d0 m2d0 m12d0 m1d0 m1d1 m9d0

EVAL. WITH REGULARIZATION
5.8 (3.5) 4.4 (2.3) 20.6 (0.7) 116.8 (76.0) 30.0 (36.7) 31.0 (8.6) 456.0 (221.4) 146.0 (84.5) 290.0 (257.8)

EVAL. WITHOUT REGULARIZATION
0.2 (0.2) 0.1 (0.1) 15.8 (1.0) 82.1 (89.3) 33.9 (38.7) 43.4 (11.1) 258.9 (88.3) 140.4 (61.4) 179.0 (75.1)

LEARN SCRATCH

4.8 0.0 29.9 1425.2 326.1 67.6 753.6 698.5 518.0

(9.3) (0.0) (0.7) (1755.1) (130.4) (32.4) (31.6) (31.3) (16.7)

Table 2: Policy evaluation using regularization. Each game was initially trained in the default mode for 50M frames with dropout and 2 regularization then evaluated on each listed flavour. Reported numbers are the average over 5 runs. Standard deviation is reported between parentheses.

Cumulative Reward

Freeway Policy Evaluation w/ Regularization
30 25 20 15 10
5
m4d0 dropout+ 2 m1d0 dropout+ 2 m1d1 dropout+ 2 0 10M 20M 30M 40M 50M
Frames before evaluation
Figure 3: Performance of an agent that was evaluated every 500, 000 frames after being trained in the default flavour of FREEWAY with dropout and 2 regularization. Results are averaged over five seeds. The y-axis is log scaled.

In most flavours, evaluating the policy trained with regularization does not negatively impact performance when compared to the performance of the policy trained without regularization. In some flavours we even see an increase in performance. Interestingly, when using regularization the agent in FREEWAY improves for all flavours and even learns a policy capable of outperforming the baseline learned from scratch in two of the three flavours. Moreover, in FREEWAY we now observe increasing performance during evaluation throughout most of the learning procedure as depicted in Figure 3. These results seem to confirm the notion of overfitting observed in Figure 2.
Despite slight improvements from these techniques, regularization by itself does not seem sufficient to enable policies to generalize across flavours. As shown in the next section, perhaps the real benefit of regularization in deep RL comes from the ability to learn more general features. These features may lead to a more adaptable representation which can be reused and subsequently fine-tuned on other flavours, which is often the case in supervised learning.

6 VALUE FUNCTION FINE-TUNING
We hypothesize that the benefit of regularizing deep RL algorithms may not come from improvements during evaluation, but instead in having a good parameter initialization that can be adapted to new tasks that are similar. We evaluate this hypothesis using two common practices in machine learning. First, we the use the weights trained with regularization as the initialization for the entire network. We subsequently fine-tune all weights in the network. This is similar to what is performed in computer vision with supervised classification methods (e.g., Razavian et al., 2014). Secondly, we evaluate reusing and fine-tuning only early layers of the network. This has been shown to improve generalization in some settings (e.g., Yosinski et al., 2014), and is sometimes used in natural language processing (e.g., Mou et al., 2016; Howard & Ruder, 2018).
When fine-tuning the entire network, we take the weights of the network trained in the default flavour for 50M frames and use them to initialize the network commencing training in the new flavour for 50M frames. We perform this set of experiments twice. Once for the weights trained without regularization, and again for the weights trained with regularization, as described in the previous section. Each run is averaged over five seeds. For comparison we provide a baseline trained from scratch for 50M and 100M frames in each flavour. Directly comparing the performance obtained after fine-tuning to the performance after 50M frames (SCRATCH) shows the benefit of re-using a representation learned in a different task instead of randomly initializing the network. Comparing the performance obtained after fine-tuning to the performance of 100M frames (SCRATCH) lets us take into consideration the whole learning process. The results are presented in Table 3.
Fine-tuning from an unregularized representation yields conflicting conclusions. Although in FREEWAY we obtained positive fine-tuning results, we note that rewards are so sparse in m1d0 and m1d1 that this initialization is likely to be simply acting as a form of optimistic initialization, biasing the agent to go up. The agent observes rewards more often, therefore, it learns quicker about the new flavour. However, the agent is still unable to reach the maximum score in these flavours.

6

Under review as a conference paper at ICLR 2019

FINE-TUNING

REGULARIZED FINE-TUNING

SCRATCH

GAME VARIANT

FREEWAY

m1d0 m1d1

m4d0

HERO

m1d0 m2d0

BREAKOUT

m12d0

m1d0 SPACE INVADERS m1d1

m9d0

10M 2.9 (3.7) 0.1 (0.2) 20.8 (1.1) 220.7 (98.2) 74.4 (31.7) 11.5 (10.7) 617.8 (55.9) 482.6 (63.4) 354.8 (59.4)

50M 22.5 (7.5) 17.4 (11.4) 31.4 (0.5) 496.7 (362.8) 92.5 (26.2) 69.1 (14.9) 926.1 (56.6) 799.4 (52.5) 574.1 (37.0)

10M 20.2 (1.9) 18.5 (2.8) 22.6 (0.7) 322.5 (39.3) 84.8 (56.1) 48.2 (4.1) 701.8 (28.5) 656.7 (25.5) 519.0 (31.1)

50M 25.4 (0.2) 25.4 (0.4) 32.2 (0.5) 4104.6 (2192.8) 211.0 (100.6) 96.1 (11.2) 1033.5 (89.7) 920.0 (83.5) 583.0 (17.5)

50M 4.8 (9.3) 0.0 (0.0) 29.9 (0.7) 1425.2 (1755.1) 326.1 (130.4) 67.6 (32.4) 753.6 (31.6) 698.5 (31.3) 518.0 (16.7)

100M 7.5 (11.5) 2.5 (7.3) 32.8 (0.2) 5026.8 (2174.6) 323.5 (76.4) 55.2 (37.2) 979.7 (39.8) 906.9 (56.5) 567.7 (40.1)

Table 3: Experiments fine-tuning the entire network with and without regularization (dropout + 2). An agent is trained with dropout + 2 regularization in the default flavour of each game for 50M frames, then DQN's parameters  were used to initialize the fine-tuning procedure on each new
flavour for 50M frames. The baseline agent is trained from scratch up to 100M frames. Standard
deviation reported between parenthesis.

The results of fine-tuning the regularized representation are more exciting. In FREEWAY we observe the highest scores on m1d0 and m1d1 throughout the whole paper. In HERO we vastly outperform fine-tuning from an unregularized representation. In SPACE INVADERS we obtain higher scores across the board on average when comparing to the same amount of experience. These results suggest that reusing a regularized representation in deep RL might allow us to learn more general features which can be more successfully fine-tuned.
Moreover, initializing the network with a regularized representation has a big impact on the agent's performance when compared to initializing the network randomly. These results are impressive when we consider the potential regularization has in reducing the sample complexity of deep RL algorithms. Such an observation also holds when we take the total number of frames seen between two flavours into consideration. When directly comparing one row of REGULARIZED FINE-TUNING to SCRATCH we are comparing two algorithms that observed 100M frames. However, to generate two rows of SCRATCH we used 200M frames while two rows of REGULARIZED FINE-TUNING used 150M frames (50M from scratch + 50M in each row). The distinction becomes bigger and bigger as more tasks are taken into consideration.
We further investigate which layers may encode general features able to be fine-tuned. Inspiration was taken from other studies that have shown that neural networks can re-learn co-adaptations when their final layers are randomly initialized, sometimes improving generalization (Yosinski et al., 2014). We conjectured DQN may benefit from re-learning the co-adaptations between early layers comprising general features and the randomly initialized layers which ultimately assign state-action values. We hypothesized that it might be beneficial to re-learn the final layers from scratch since state-action values are ultimately conditioned on the flavour at hand. Therefore, we also evaluated whether fine-tuning only the convolutional layers, or the convolutional layers and the first fully connected layer was more effective than fine-tuning the whole network. Suprisingly, this does not seem to be the case. The performance obtained when the whole network is fine-tuned (Table 3) is consistently better than when it is not (Table 4). We speculate that this might not be the case on more dissimilar tasks.
7 DISCUSSION AND CONCLUSION
Many studies have tried to explain generalization of deep neural networks in supervised learning settings (e.g., Zhang et al., 2018; Dinh et al., 2017). Analyzing generalization and overfitting in deep RL has its own issues on top of the challenges posed in the supervised learning case. Actually, generalization in RL can be seen in different ways. We can talk about generalization in RL in terms of conditioned sub-goals within an environment (e.g., Andrychowicz et al., 2017; Sutton, 1995), learning multiple tasks at once (e.g., Teh et al., 2017; Parisotto et al., 2016), or sequential task learning as in a continual learning setting (e.g., Schwarz et al., 2018; Kirkpatrick et al., 2016). In this paper we evaluated generalization in terms of small variations of high-dimensional control tasks. This provides a candid evaluation method to study how well features and policies learned by
7

Under review as a conference paper at ICLR 2019

REGULARIZED FINE-TUNING 3CONV

GAME VARIANT

FREEWAY

m1d0 m1d1

m4d0

HERO

m1d0 m2d0

BREAKOUT

m12d0

m1d0 SPACE INVADERS m1d1

m9d0

10M 0.0 (0.0) 0.0 (0.0) 7.3 (3.5) 405.1 (82.0) 232.1 (30.1) 4.3 (1.7) 669.3 (29.1) 609.8 (16.6) 436.1 (18.9)

50M 0.7 (1.4) 0.0 (0.0) 30.4 (0.6) 1949.1 (2076.4) 455.2 (170.4) 63.7 (26.6) 998.1 (78.8) 836.3 (55.9) 581.0 (12.2)

REGULARIZED FINE-TUNING 3CONV+1FC

10M 0.1 (0.1) 0.1 (0.1) 4.9 (4.8) 350.3 (52.1) 150.4 (38.5) 5.4 (0.8) 681.3 (17.2) 638.7 (19.1) 439.9 (40.3)

50M 4.9 (9.9) 10.0 (12.3) 30.7 (1.7) 3085.3 (2055.6) 307.6 (64.8) 89.1 (16.7) 989.6 (39.4) 883.4 (38.1) 586.7 (39.7)

REGULARIZED FINE-TUNING
50M 25.4 (0.2) 25.4 (0.4) 32.2 (0.5) 4104.6 (2192.8) 211.0 (100.6) 96.1 (11.2) 1033.5 (89.7) 920.0 (83.5) 583.0 (17.5)

Table 4: Experiments fine-tuning early layers of the network trained with regularization. An agent is trained with dropout + 2 regularization in the default flavour of each game for 50M frames, then DQN's parameters  were used to initialize the corresponding layers to be further fine-tuned on each new flavour. Remaining layers were randomly initialized. Compared against fine-tuning the entire network from Table 3. Standard deviation reported between parenthesis.

deep neural networks in RL problems can generalize. The approach of studying generalization with respect to the representation learning problem intersects nicely with the aforementioned problems in RL where generalization is key.
The empirical evaluation presented in this paper has shown that traditional DQN seems to generalize poorly even between very similar high-dimensional control tasks. Given this lack of generality we investigated how dropout and 2 regularization can be used to improve generalization in deep RL. Other forms of regularization in RL that have been explored in the past are sticky-actions, random initial states, entropy regularization (Zhang et al., 2018), and procedural generation of environments (Justesen et al., 2018). More related to our work, regularization in the form of weight constraints has been applied in the continual learning setting in order to reduce the catastrophic forgetting exhibited by fine-tuning on many sequential tasks (Kirkpatrick et al., 2016; Schwarz et al., 2018). Similar weight constraint methods have been explored in multitask learning (Teh et al., 2017).
Evaluation practices in RL often focuses on training and evaluating agents on exactly the same task. Consequently, regularization has traditionally been underutilized in deep RL. With a renewed emphasis on generalization in RL, regularization applied to the representation learning problem can be a feasible method to improving generalization on closely related tasks. Our results suggest that dropout and 2 regularization seem to be able to learn more general purpose features which can be adapted to similar problems. Although other communities relying on deep neural networks have shown similar successes, this is of particular importance for the deep RL community which struggles with sample efficiency (Henderson et al., 2018). This work is also related to recent metalearning procedures like MAML (Finn et al., 2017) which aim to find a parameter initialization that can be quickly adapted to new tasks. In fact, some of the results here can also be seen under the light of curriculum learning. The regularization techniques we've evaluated here seem to be effective in leveraging situations where an easier task is presented first, sometimes leading to unseen performance levels (e.g., FREEWAY).
Finally, we believe it would be extremely beneficial for the field if we were able to develop algorithms that can generalize across tasks. Ultimately we want agents that can keep learning as they interact with the world in a continual learning fashion. The ability to generalize is essential. Throughout this paper we often avoided the expression transfer learning because we believe that succeeding in slightly different environments should be actually seen as a problem of generalization. Our results suggested that regularizing and fine-tuning representations in deep RL might be a viable approach towards improving sample efficiency and generalization on multiple tasks. It is particularly interesting that fine-tuning a regularized network was the most successful approach because this might also be applicable in the continual learning settings where the environment changes without the agent being told so, and re-initializing layers of a network is obviously not an option. In this setting, the work from Kirkpatrick et al. (2016), and Schwarz et al. (2018) might be a great starting point as they provide a more thorough discussion of generalization in continual learning.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems (NIPS), pp. 5055­5065, 2017.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253­279, 2013.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Proceedings of the International Conference on Machine Learning (ICML), pp. 1019­1028, 2017.
Lasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In Proceedings of the International Conference on Machine Learning (ICML), pp. 1406­1415, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the International Conference on Machine Learning (ICML), pp. 1126­1135, 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 249­256, 2010.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.
Jeremy Howard and Sebastian Ruder. Fine-tuned language models for text classification. CoRR, abs/1801.06146, 2018.
Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and Sebastian Risi. Procedural level generation improves generality of deep reinforcement learning. CoRR, abs/1806.10729, 2018.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. CoRR, abs/1612.00796, 2016.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3431­3440, 2015.
Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and Michael Bowling. Revisiting the Arcade Learning Environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523­562, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How transferable are neural networks in NLP applications? In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 479­489, 2016.
9

Under review as a conference paper at ICLR 2019
Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman. Gotta learn fast: A new benchmark for generalization in RL. CoRR, abs/1804.03720, 2018.
Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features offthe-shelf: An astounding baseline for recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 512­519, 2014.
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016.
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 4535­4544, 2018.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Richard S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In Advances in Neural Information Processing Systems (NIPS), pp. 1038­1044, 1995.
Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), pp. 4499­4509, 2017.
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8:279­292, 1992. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in Neural Information Processing Systems (NIPS), pp. 3320­3328, 2014. Chiyuan Zhang, Oriol Vinyals, Rémi Munos, and Samy Bengio. A study on overfitting in deep reinforcement learning. CoRR, abs/1804.06893, 2018.
10

Under review as a conference paper at ICLR 2019
A GAME MODES
FREEWAY

(a) FREEWAY m0d0

(b) FREEWAY m1d0

(c) FREEWAY m4d0

In FREEWAY a chicken must cross a road containing multiple lanes of moving traffic within a prespecified time limit. In all modes of FREEWAY, the agent gets rewarded for reaching the top of the screen and is subsequently teleported to the bottom of the screen. If the chicken collides with a vehicle in difficulty 0 it gets bumped down one lane of traffic, alternatively, in difficulty 1 the chicken gets teleported to its starting position on the bottom of the screen. Mode 1 changes some vehicle sprites to include buses, adds more vehicles to some lanes, and increases the velocity of all vehicles. Mode 4 is almost identical to Mode 1; the only difference being vehicles can oscillate between two speeds.

HERO

(a) HERO m0d0

(b) HERO m1d0

(c) HERO m2d0

In HERO you control a character who must navigate a maze in order to save a trapped miner within a cave system. The agent scores points for any forward progression such as clearing an obstacle or killing an enemy. Once the miner is rescued, the level is terminated and you continue to the next level with a different maze. Some levels have partially observable rooms, more enemies, and more difficult obstacles to traverse. Past the default mode, each subsequent mode starts off at increasingly harder levels denoted by a level number increasing by multiples of 5. The default mode starts you off at level 1, mode 1 starts at level 5, and so on.

BREAKOUT

(a) BREAKOUT m0d0

(b) BREAKOUT m12d0

In BREAKOUT you control a paddle which can move horizontally along the bottom of the screen. At the beginning of the game, or on loss of life a ball is set into motion and can bounce off the paddle and collide with bricks at the top of the screen. The objective of the game is to break all the bricks without having the ball fall below your paddles horizontal plane. Subsequently, mode 12 of breakout hides the bricks from the player until the ball collides with the bricks in which case the bricks flash for a brief moment before disappearing again.

11

Under review as a conference paper at ICLR 2019

(a) SPACE INVADERS m0d0

(b) SPACE INVADERS m1d1

(c) SPACE INVADERWS m9d0

SPACE INVADERS
When playing SPACE INVADERS you control a spaceship which can move horizontally along the bottom of the screen. There is a grid of aliens which are above you and the objective of the game is to shoot-out all aliens. You are afforded some protection from the alien bullets with three barriers just above the spaceship. Difficulty 1 of space invaders widens your spaceships sprite making it harder to doge enemy bullets. Mode 1 of SPACE INVADERS causes the shields above you to oscillate horizontally. Mode 9 of SPACE INVADERS is similar to Mode 12 of BREAKOUT where the aliens are partially observable until struck with the players bullet.
B BASELINE RESULTS
In all experiments performed in this paper we utilize the neural network architecture used by Mnih et al. (2015). That is, a convolutional neural network with three convolutional layers and two fully connected layers. A visualization of this network can be found in Figure 8. Hyperparametes are generally kept consistent with Machado et al. (2018). Below we provide a table of the key hyperparameters used in the baseline experiments.
NEURAL NETWORK ARCHITECTURE

Conv 32,8x8 stride 4
ReLU

Conv 64,4x4 stride 2
ReLU

Conv 64,3x3 stride 1
ReLU

1024
fc ReLU

18
fc

Q(St, ·; <latexit sha1_base64="cqv76kuSkKevFHNmxXRkOhkDf/U=">AAACAHicbZDLSsNAFIYn9VbrLerChZvBIlSQkoig4KboxmWL9gJNCJPJpB06uTBzIpTQja/ixoUibn0Md76N0zYLrf4w8PGfczhzfj8VXIFlfRmlpeWV1bXyemVjc2t7x9zd66gkk5S1aSIS2fOJYoLHrA0cBOulkpHIF6zrj26m9e4Dk4on8T2MU+ZGZBDzkFMC2vLMg1btzoNT7NAggStHAwwZkBPPrFp1ayb8F+wCqqhQ0zM/nSChWcRioIIo1betFNycSOBUsEnFyRRLCR2RAetrjEnElJvPDpjgY+0EOEykfjHgmftzIieRUuPI150RgaFarE3N/2r9DMJLN+dxmgGL6XxRmAkMCZ6mgQMuGQUx1kCo5PqvmA6JJBR0ZhUdgr148l/onNVtq263zquN6yKOMjpER6iGbHSBGugWNVEbUTRBT+gFvRqPxrPxZrzPW0tGMbOPfsn4+AZqxJUA</latexit>

)

Figure 8: Neural network architecture used by DQN to predict state-action values.

HYPERPARAMETERS

Learning rate  Minibatch size Dropout rate convolutions Dropout rate fully connected Regularization term 

0.00025 32 0.05 0.1 0.0001

Replay buffer size Target update frequency  decay horizon  initial  final Discount factor 

1, 000, 000 4 1M frames 1.0 0.01 0.99

EVALUATION
Each baseline run is trained for up to 100M frames in each game flavour. We decay epsilon linearly over the -decay period to allow for an exploratory period at the beginning of training. We use sticky-actions with a probability of p = 0.25 of executing At-1 instead of action At (Machado et al., 2018). We allow the agent access to all 18 primitive actions in the ALE, we do not utilize the reduced action set nor the lives signal.
12

Under review as a conference paper at ICLR 2019

Furthermore, as a crude measure for environment complexity, we measure the best greedy action an agent could take in a game flavour. Simply put, we iterate through every action in A, executing this action -greeidly, with  = 0.01, at every time step for 100 episodes. These results were then averaged over 5 runs with the standard deviations between runs reported in parenthesis.

FREEWAY

GAME VARIANT m0d0 m1d0 m1d1 m4d0 m0d0 m1d0 m2d0

10M 3.0 (1.0) 0.0 (0.1) 0.0 (0.0) 4.4 (1.4) 3187.8 (78.3) 326.9 (40.3) 116.3 (11.0)

50M 31.4 (0.2) 4.8 (9.3) 0.0 (0.0) 29.9 (0.7) 9034.4 (1610.9) 1425.2 (1755.1) 326.1 (130.4)

100M 32.1 (0.1) 7.5 (11.5) 2.5 (7.3) 32.8 (0.2) 13961.0 (181.9) 5026.8 (2174.6) 323.5 (76.4)

BEST ACTION 23.0 (1.4) 5.0 (1.5) 4.2 (1.3) 7.5 (2.8) 150.0 (0.0) 75.8 (7.5) 12.0 (27.5)

m0d0

17.5 (2.0)

72.5 (7.7)

73.4 (13.5)

2.3 (1.3)

SPACE INVADERS BREAKOUT HERO

m12d0

17.7 (1.3)

67.6 (32.4)

55.2 (37.2)

1.8 (1.1)

m0d0

250.3 (16.2) 698.8 (32.2)

927.1 (85.3) 243.6 (95.9)

m1d0

203.6 (24.3) 753.6 (31.6)

979.7 (39.8) 192.6 (65.7)

m1d1

193.6 (11.0) 698.5 (31.3)

906.9 (56.5) 180.9 (101.9)

m9d0

173.0 (17.8) 518.0 (16.7)

567.7 (40.1) 174.6 (65.9)

Table 5: Baselines using vanilla DQN for all tested game variants.

GAME VARIANT

FREEWAY

m0d0

HERO

m0d0

BREAKOUT

m0d0

SPACE INVADERS m0d0

10M 4.6 (5.0) 2466.5 (630.8) 6.1 (2.7) 214.6 (13.8)

50M 25.9 (0.6) 6505.9 (1843.0) 34.1 (1.8) 623.1 (16.3)

100M 29.0 (0.8) 12446.9 (397.4) 66.4 (3.6) 617.4 (29.6)

BEST ACTION 23.0 (1.4) 150.0 (0.0) 2.3 (1.3) 243.6 (95.9)

Table 6: Baselines using dropout + 2 regularization for each default flavour.

BASELINE

BASELINE W/ REGULARIZATION

GAME VARIANT

FREEWAY

m0d0

HERO

m0d0

BREAKOUT

m0d0

SPACE INVADERS m0d0

10M 3.0 (1.0) 3187.8 (78.3) 17.5 (2.0) 250.3 (16.2)

50M 31.4 (0.2) 9034.4 (1610.9) 72.5 (7.7) 698.8 (32.2)

100M 32.1 (0.1) 13961.0 (181.9) 73.4 (13.5) 927.1 (85.3)

10M 4.6 (5.0) 2466.5 (630.8) 6.1 (2.7) 214.6 (13.8)

50M 25.9 (0.6) 6505.9 (1843.0) 34.1 (1.8) 623.1 (16.3)

100M 29.0 (0.8) 12446.9 (397.4) 66.4 (3.6) 617.4 (29.6)

Table 7: Comparison of baseline results with and without regularization in the default flavour. The baseline agent with regularization was trained with dropout and 2 regularization.

13

Under review as a conference paper at ICLR 2019

C POLICY EVALUATION LEARNING CURVES

We provide learning curves for evaluating a policy learned in the default flavour (m0d0) to each subsequent flavour of that game. Each subplot are the results of evaluating the policy from a representation trained with and without regularization.

EVALUATION
Checkpoint of the network weights  were taken during training every 500, 000 frames, up to 50M frames in total. Each checkpoint was then evaluated in the target mode for 100 episodes averaged over five runs. Hyperparameters are kept consistent with the baseline experiments in Appendix B.

Cumulative Reward Cumulative Reward Cumulative Reward

Freeway m1d0

12

m1d0 m1d0 dropout+l2

10

8

6

4

2

0
10M Fra20mMes befor3e0Mtransfer40M

50M

Freeway m1d1

10

m1d1 m1d1 dropout+l2

8

6

4

2

0
10M Fra20mMes befor3e0Mtransfer40M

50M

Freeway m4d0
m4d0 25 m4d0 dropout+l2

20

15

10

5

0
10M Fra20mMes befor3e0Mtransfer40M

50M

Cumulative Reward Cumulative Reward

Hero m1d0
400 m1d0 350 m1d0 dropout+l2

300

250

200

150

100

50

0
10M Fra20mMes befor3e0Mtransfer40M

50M

Hero m2d0
250 m2d0 m2d0 dropout+l2
200

150

100

50

0
10M Fra20mMes befor3e0Mtransfer40M

50M

Cumulative Reward

Breakout m12d0

60

m12d0 m12d0 dropout+l2

50

40

30

20

10

0
10M Fra20mMes befor3e0Mtransfer40M

50M

Cumulative Reward Cumulative Reward Cumulative Reward

Space Invaders m1d0
700 m1d0 m1d0 dropout+l2
600

500

400

300

200

100

0
10M Fra20mMes befor3e0Mtransfer40M

50M

Space Invaders m1d1
600 m1d1 m1d1 dropout+l2
500

400

300

200

100

0
10M Fra20mMes befor3e0Mtransfer40M

50M

Space Invaders m9d0
m9d0 500 m9d0 dropout+l2

400

300

200

100

0
10M Fra20mMes befor3e0Mtransfer40M

50M

Figure 9: Performance curves for policy evaluation results. The x-axis is the number of frames before we evaluated the -greedy policy from the default flavour on the target flavour. The y-axis is the cumulative reward the agent incurred.

14

