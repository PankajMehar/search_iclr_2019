Under review as a conference paper at ICLR 2019
GENERATIVE ADVERSARIAL NETWORKS FOR EXTREME LEARNED IMAGE COMPRESSION
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a framework for extreme learned image compression based on Generative Adversarial Networks (GANs), obtaining visually pleasing images at significantly lower bitrates than previous methods. This is made possible through our GAN formulation of learned compression combined with a generator/decoder which operates on the full-resolution image and is trained in combination with a multi-scale discriminator. Additionally, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, therefore only requiring the storage of the preserved region and the semantic label map. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.

Original

Ours 1567 Bytes [B]

JP2K 3138B +100% BPG 3573B +120%

Ours 1547 Bytes

BPG 3573 Bytes 128% larger JPEG 13959B +790% WebP 9437B +502%

Figure 1: Visual comparison of our result to that obtained by other codecs. Note that even when us-

ing more than twice the number of bytes, all other codecs are outperformed by our method visually.

1 INTRODUCTION
Image compression systems based on deep neural networks (DNNs), or deep compression systems for short, have become an active area of research recently. These systems (e.g. (Theis et al., 2017; Balle´ et al., 2016b; Rippel & Bourdev, 2017; Balle´ et al., 2018; Mentzer et al., 2018)) are often competitive with modern engineered codecs such as WebP (WebP), JPEG2000 (Taubman & Marcellin, 2001) and even BPG(Bellard) (the state-of-the-art engineered codec). Besides achieving competitive compression rates on natural images, they can be easily adapted to specific target domains such as stereo or medical images, and promise efficient processing and indexing directly from compressed representations (Torfason et al., 2018). However, deep compression systems are typically optimized for traditional distortion metrics such as peak signal-to-noise ratio (PSNR) or multi-scale structural similarity (MS-SSIM) (Wang et al., 2003). For very low bitrates (below 0.1 bits per pixel (bpp)), where preserving the full image content becomes impossible, these distortion metrics lose significance as they favor pixel-wise preservation of local (high-entropy) structure over preserving texture

1

Under review as a conference paper at ICLR 2019
and global structure. To further advance deep image compression it is therefore of great importance to develop new training objectives beyond PSNR and MS-SSIM. A promising candidate towards this goal are adversarial losses (Goodfellow et al., 2014) which were shown recently to capture global semantic information and local texture, yielding powerful generators that produce visually appealing high resolution images from semantic label maps (Isola et al., 2017; Wang et al., 2018).
In this paper, we propose and study a generative adversarial network (GAN)-based framework for extreme image compression, targeting bitrates below 0.1 bpp. We rely on a principled GAN formulation for deep image compression that allows for different degrees of content generation. In contrast to prior works on deep image compression which applied adversarial losses to image patches for artifact suppression (Rippel & Bourdev, 2017; Galteri et al., 2017) and generation of texture details (Ledig et al., 2017) or representation learning for thumbnail images (Santurkar et al., 2017), our generator/decoder operates on the full-resolution image and is trained with a multi-scale discriminator (Wang et al., 2018).
We consider two modes of operation (corresponding to unconditional and conditional GANs (Goodfellow et al., 2014; Mirza & Osindero, 2014)), namely
· generative compression (GC), preserving the overall image content while generating structure of different scales such as leaves of trees or windows in the facade of buildings, and
· selective generative compression (SC), completely generating parts of the image from a semantic label map while preserving user-defined regions with a high degree of detail.
We emphasize that GC does not require semantic label maps (neither for training, nor for deployment). A typical use case for GC are bandwidth constrained scenarios, where one wants to preserve the full image as well as possible, while falling back to synthesized content instead of blocky/blurry blobs for regions where there are not sufficient bits to store the original pixels. SC could be applied in a video call scenario where one wants to fully preserve people in the video stream, but a visually pleasing synthesized background serves the purpose as well as the true background. In the GC operation mode the image is transformed into a bitstream and encoded using arithmetic coding. SC requires a semantic/instance label map of the original image which can be obtained using off-the-shelf semantic/instance segmentation networks, e.g., PSPNet (Zhao et al., 2017) and Mask R-CNN (He et al., 2017), and which is stored as a vector graphic. This amounts to a small, image dimension-independent overhead in terms of coding cost. On the other hand, the size of the compressed image is reduced proportionally to the area which is generated from the semantic label map, typically leading to a significant overall reduction in storage cost.
For GC, a comprehensive user study shows that our compression system yields visually considerably more appealing results than BPG (Bellard) (the current state-of-the-art engineered compression algorithm) and the recently proposed autoencoder-based deep compression (AEDC) system (Mentzer et al., 2018). In particular, our GC models trained for compression of general natural images are preferred to BPG when BPG uses up to 95% and 124% more bits than those produced by our models on the Kodak (Kodak) and RAISE1K (Dang-Nguyen et al., 2015) data set, respectively. When constraining the target domain to the street scene images of the Cityscapes (Cordts et al., 2016) data set, the reconstructions of our GC models are preferred to BPG even when the latter uses up to 181% more bits. To the best of our knowledge, these are the first results showing that a deep compression method outperforms BPG on the Kodak data set in a user study--and by large margins. In the SC operation mode, our system seamlessly combines preserved image content with synthesized content, even for regions that cross multiple object boundaries, while faithfully preserving the image semantics. By partially generating image content we achieve bitrate reductions of over 50% without notably degrading image quality.
2 RELATED WORK
Deep image compression has recently emerged as an active area of research. The most popular DNN architectures for this task are to date auto-encoders (Theis et al., 2017; Balle´ et al., 2016b; Agustsson et al., 2017; Li et al., 2017; Torfason et al., 2018; Minnen et al., 2018) and recurrent neural networks (RNNs) (Toderici et al., 2015; 2016). These DNNs transform the input image into a bit-stream, which is in turn losslessly compressed using entropy coding methods such as Huffman coding or arithmetic coding. To reduce coding rates, many deep compression systems rely on context models
2

Under review as a conference paper at ICLR 2019

to capture the distribution of the bit stream (Balle´ et al., 2016b; Toderici et al., 2016; Li et al., 2017; Rippel & Bourdev, 2017; Mentzer et al., 2018). Common loss functions to measure the distortion between the original and decompressed images are the mean-squared error (MSE) (Theis et al., 2017; Balle´ et al., 2016b; Agustsson et al., 2017; Li et al., 2017; Balle´ et al., 2018; Torfason et al., 2018), or perceptual metrics such as MS-SSIM (Toderici et al., 2016; Rippel & Bourdev, 2017; Balle´ et al., 2018; Mentzer et al., 2018). Some authors rely on advanced techniques including multiscale decompositions (Rippel & Bourdev, 2017), progressive encoding/decoding strategies (Toderici et al., 2015; 2016), and generalized divisive normalization (GDN) layers (Balle´ et al., 2016b;a).
Generative adversarial networks (GANs) (Goodfellow et al., 2014) have emerged as a popular technique for learning generative models for intractable distributions in an unsupervised manner. Despite stability issues (Salimans et al., 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017; Mao et al., 2017), they were shown to be capable of generating more realistic and sharper images than prior approaches and to scale to resolutions of 1024 × 1024px (Zhang et al., 2017; Karras et al., 2017) for some datasets. Another direction that has shown great progress are conditional GANs (Goodfellow et al., 2014; Mirza & Osindero, 2014), obtaining impressive results for image-to-image translation (Isola et al., 2017; Wang et al., 2018; Zhu et al., 2017; Liu et al., 2017) on various datasets (e.g. maps to satellite images), reaching resolutions as high as 1024 × 2048px (Wang et al., 2018).
Arguably the most closely related work to ours is (Rippel & Bourdev, 2017), which uses an adversarial loss term to train a deep compression system. However, this loss term is applied to small image patches and its purpose is to suppress artifacts rather than to generate image content. Furthermore, it uses a non-standard GAN formulation that does not (to the best of our knowledge) have an interpretation in terms of divergences between probability distributions, as in (Goodfellow et al., 2014; Nowozin et al., 2016). We refer to Sec. 6.1 and Appendix A for a more detailed discussion. Santurkar et al. (2017) use a GAN framework to learn a generative model over thumbnail images, which is then used as a decoder for thumbnail image compression. Other works use adversarial training for compression artifact removal (for engineered codecs) (Galteri et al., 2017) and single image super-resolution (Ledig et al., 2017). Finally, related to our SC mode, spatially allocating bitrate based on saliency of image content has a long history in the context of engineered compression algorithms, see, e.g.,, (Stella & Lisin, 2009; Guo & Zhang, 2010; Gupta et al., 2013).

3 BACKGROUND

Generative Adversarial Networks: Given a data set X , Generative Adversarial Networks (GANs)
can learn to approximate its (unknown) distribution px through a generator G(z) that tries to map samples z from a fixed prior distribution pz to the distribution px. The generator G is trained in parallel with a discriminator D by searching (using stochastic gradient descent (SGD)) for a saddle
point of a mini-max objective

min max E[f (D(x))] + E[g(D(G(z)))],
GD

(1)

where G and D are DNNs and f and g are scalar functions. The original paper (Goodfellow et al.,
2014) uses the "Vanilla GAN" objective with f (y) = log(y) and g(y) = log(1 - y). This cor-
responds to G minimizing the Jensen-Shannon (JS) Divergence between the distribution of x and
G(z). The JS Divergence is a member of a more generic family of f -divergences, and Nowozin
et al. (2016) show that for suitable choices of f and g, all such divergences can be minimized with (1). In particular, if one uses f (y) = (y - 1)2 and g(y) = y2, one obtains the Least-Squares GAN (Mao et al., 2017) (which corresponds to the Pearson 2 divergence), which we adopt in this paper.
We refer to the divergence minimized over G as

LGAN := max E[f (D(x))] + E[g(D(G(z)))].
D

(2)

Conditional Generative Adversarial Networks: For conditional GANs (cGANs) (Goodfellow
et al., 2014; Mirza & Osindero, 2014), each data point x is associated with additional information s, where (x, s) have an unknown joint distribution px,s. We now assume that s is given and that we want to use the GAN to model the conditional distribution px|s. In this case, both the generator
G(z, s) and discriminator D(z, s) have access to the side information s, leading to the divergence

LcGAN := max E[f (D(x, s))] + E[g(D(G(z, s), s))].
D

(3)

3

Under review as a conference paper at ICLR 2019

Deep Image Compression: To compress an image x  X , we follow the formulation of (Agustsson et al., 2017; Mentzer et al., 2018) where one learns an encoder E, a decoder G, and a finite quantizer q. The encoder E maps the image to a latent feature map w, whose values are then quantized to L levels {c1, · · · , cL}  R to obtain a representation w^ = q(E(x)) that can be encoded to a bitstream. The decoder then tries to recover the image by forming a reconstruction x^ = G(w^). To be able to backpropagate through the non-differentiable q, one can use a differentiable relaxation of q, as in
(Mentzer et al., 2018).

The average number of bits needed to encode w^ is measured by the entropy H(w^), which can be modeled with a prior (Agustsson et al., 2017) or a conditional probability model (Mentzer et al., 2018). The trade-off between reconstruction quality and bitrate to be optimized is then

E[d(x, x^)] + H(w^).

(4)

where d is a loss that measures how perceptually similar x^ is to x. Given a differentiable estimator of the entropy H(w^), the weight  controls the bitrate of the model (high  pushes the bitrate down). However, since the number of dimensions dim(w^) and the number of levels L are finite, the entropy
is bounded by (see, e.g., (Cover & Thomas, 2012))

H(w^)  dim(w^) log2(L).

(5)

It is therefore also valid to set  = 0 and control the maximum bitrate through the bound (5) (i.e.,

adjusting L or dim(w^) through the architecture of E). While potentially leading to suboptimal

E

w

q

w^bitrates,

this G

avoids

to

model D

the

entropy

explicitly

as

a

loss

term.

x 4 GANS FORx^EXTREME IMAGE COMPRESSION

4.1 GENERATIVE COMPRESSION

x
E w
q w^

Our proposed GANs for extreme image compression can be viewed as a combination of
(conditional) GANs and learned compression. With an encoder E and quantizer q, we encode the image x to a compressed representation w^ = q(E(x)). This representation is optionally concatenated with noise v drawn from a fixed prior pv, to form the latent vector z. The decoder/generator G then tries to generate an image x^ = G(z) that is consistent with the image distribution px while also recovering the specific encoded image x to a certain degree (see inset Fig.). Using z = [w^, v], this can be expressed by
our saddle-point objective for (non-conditional) generative compression,

G x^

min max E[f (D(x))] + E[g(D(G(z))] + E[d(x, G(z))] + H(w^), (6)
E,G D
where  > 0 balances the distortion term against the GAN loss and entropy terms. Using this formulation, we need to encode a real image, w^ = E(x), to be able to sample from pw^ . However, this is not a limitation as our goal is to compress real images and not to generate completely new ones.

D Since the last two terms of (6) do not depend on the discriminator D, they do not affect its optimization directly. This means that the discriminator still computes the same f divergence LGAN as in (2), so we can write (6) as

min LGAN + E[d(x, G(z))] + H(w^).
E,G

(7)

We note that equation (6) has completely different dynamics than a normal GAN, because the latent space z contains w^, which stores information about a real image x. A crucial ingredient is the bitrate limitation on H(w^). If we allow w^ to contain arbitrarily many bits by setting  = 0 and letting L and dim(w^) be large enough, E and G could learn to near-losslessly recover x from G(z) = G(q(E(x))), such that the distortion term would vanish. In this case, the divergence between px and pG(z) would also vanish and the GAN loss would have no effect.
By constraining the entropy of w^, E and G will never be able to make d fully vanish. In this case, E, G need to balance the GAN objective LGAN and the distortion term E[d(x, G(z))], which leads to G(z) on one hand looking "realistic", and on the other hand preserving the original image. For example, if there is a tree for which E cannot afford to store the exact texture (and make d small) G can synthesize it to satisfy LGAN, instead of showing a blurry green blob.

4

Under review as a conference paper at ICLR 2019

In the extreme case where the bitrate becomes zero (i.e., H(w^)  0, e.g., by setting  =  or dim(w^) = 0), w^ becomes deterministic. In this setting, z is random and independent of x (through the v component) and the objective reduces to a standard GAN plus the distortion term, which acts as a regularizer.
We refer to the setting in (6) as generative compression (GC), where E, G balance reconstruction and generation automatically over the image. As for the conditional GANs described in Sec. 3, we can easily extend GC to a conditional case. Here, we also consider this setting, where the additional information s for an image x is a semantic label map of the scene, but with a twist: Instead of feeding the semantics to E, G and D, we only give them to the discriminator D during training1. This means that no semantics are needed to encode or decode images with the trained models (since E, G do not depend on s). We refer to this setting as GC (D+).
4.2 SELECTIVE GENERATIVE COMPRESSION
For the generative compression and its conditional variant described in the previous section, E, G automatically navigate the trade-off between generation and preservation over the entire image, without any guidance. Here, we consider a different setting, where we guide the network in terms of which regions should be preserved and which regions should be synthesized. We refer to this setting as selective generative compression (SC) (an overview of the structure is given in Fig. 8 in App. C)
For simplicity, we consider a binary setting, where we construct a single-channel binary heatmap m of the same spatial dimensions as w^. Regions of zeros correspond to regions that should be fully synthesized, whereas regions of ones correspond to regions that should be preserved. However, since our task is compression, we constrain the fully synthesized regions to have the same semantics s as the original image x. We assume the semantics s are separately stored, and thus feed them through a feature extractor F before feeding them to the generator G. To guide the network with the semantics, we mask the (pixel-wise) distortion d, such that it is only computed over the region to be preserved. Additionally, we zero out the latent feature w^ in the regions that should be synthesized. Provided that the heatmap m is also stored, we then only encode the entries of w^ corresponding to the preserved regions, greatly reducing the bitrate needed to store it.
At bitrates where w^ is normally much larger than the storage cost for s and m (about 2kB per image when encoded as a vector graphic), this approach can give large bitrate savings.
We consider two different training modes: Random instance (RI) which randomly selects 25% of the instances in the semantic label map and preserves these, and random box (RB) which picks an image location uniformly at random and preserves a box of random dimensions. While the RI mode is appropriate for most use cases, the RB can create more challenging situations for the generator as it needs to integrate the preserved box seamlessly into the generated content.

5 EXPERIMENTS

5.1 ARCHITECTURE, LOSSES, AND HYPERPARAMETERS

The architecture for our encoder E and generator G is based on the global generator network proposed in (Wang et al., 2018), which in turn is based on the architecture of (Johnson et al., 2016). We present details in Appendix C.
For the entropy term H(w^), we adopt the simplified approach described in Sec. 3, where we set  = 0, use L = 5 centers C = {-2, 1, 0, 1, 2}, and control the bitrate through the upper bound H(w^)  dim(w^) log2(L). For example, for GC, with C = 2 channels, we obtain 0.0181bpp.2 We note that this is an upper bound; the actual entropy of H(w^) is generally smaller, since the learned distribution will neither be uniform nor i.i.d, which would be required for the bound to hold with equality. When encoding the channels of w^ to a bit-stream, we use an arithmetic encoder where frequencies are stored for each channel separately and then encode them in a static (non-adaptive)

1If we assume s is an unknown function of x, another view is that we feed additional features (s) to D.

2

H(w^)/W H



WH 16·16

· C · log2(L)/W H

=

0.0181bpp,

where

W, H

are

the

dimensions

of

the

image

and

16

is

the downsampling factor to the feature map, see Section C in the Appendix.

5

Under review as a conference paper at ICLR 2019
Ours 0.035bpp 21.8dB BPG 0.039bpp 26.0dB MSE bl. 0.035bpp 24.0dB Figure 2: Visual example of images produced by our GC network with C = 4 along with the corresponding results for BPG, and a baseline model with the same architecture (C = 4) but trained for MSE only (MSE bl.), on Cityscapes. The reconstruction of our GC network is sharper and has more realistic texture than those of BPG and our network trained for MSE, even though the latter two have higher PSNR (indicated in dB for each image) than our GC network. In particular, the MSE baseline produces blurry reconstructions even though it was trained on the Cityscapes data set, demonstrating that domain-specific training is not enough to obtain sharp reconstructions at low bitrates.
manner, similar to Agustsson et al. (2017). In our experiments, this leads to 8.8% smaller bitrates compared to the upper bound. By using a context model and adaptive arithmetic encoding, we could reduce the bitrate further, either in a post processing step (as in (Rippel & Bourdev, 2017; Balle´ et al., 2016b)), or jointly during training (as in (Mentzer et al., 2018; Minnen et al., 2018)) ­ which has given  10% saving in prior these works. For the distortion term we adopt d(x, x^) = MSE with coefficient  = 10. Furthermore, we adopt the feature matching and VGG perceptual losses, LFM and LVGG, as proposed in (Wang et al., 2018) with the same weights, which improved the quality for images synthesized from semantic label maps. These losses can be viewed as a part of d(x, x^). However, we do not mask them in SC, since they also help to stabilize the GAN in this operation mode (as in (Wang et al., 2018)). We refer to Appendix D for training details.
5.2 EVALUATION
Data sets: We train GC models (without semantic label maps) for compression of diverse natural images using 188k images from the Open Images data set (Krasin et al., 2017) and evaluate them on the widely used Kodak image compression dataset (Kodak) as well as 20 randomly selected images from the RAISE1K data set (Dang-Nguyen et al., 2015). To investigate the benefits of having a somewhat constrained application domain and semantic information at training time, we also train GC models with semantic label maps on the Cityscapes data set (Cordts et al., 2016), using 20 randomly selected images from the validation set for evaluation. To evaluate the proposed SC method (which requires semantic label maps for training and deployment) we again rely on the Cityscapes data set. Cityscapes was previously used to generate images form semantic label maps using GANs (Isola et al., 2017; Zhu et al., 2017). Baselines: We compare our method to the HEVC-based image compression algorithm BPG (Bellard) (in the 4:2:2 chroma format) and to the AEDC network from (Mentzer et al., 2018). BPG is the current state-of-the-art engineered image compression codec and outperforms other recent codecs such as JPEG2000 and WebP on different data sets in terms of PSNR (see, e.g. (Balle´ et al., 2018)). We train the AEDC network (with bottleneck depth C = 4) on Cityscapes exactly following the procedure in (Mentzer et al., 2018) except that we use early stopping to prevent overfitting (note that Cityscapes is much smaller than the ImageNet dataset used in (Mentzer et al., 2018)).The soobtained model has a bitrate of 0.07 bpp and obtains a slightly better MS-SSIM than BPG at the same bpp on the validation set. To investigate the effect of the GAN term in our total loss, we train a baseline model with an MSE loss only (with the same architecture as GC and the same training parameters, see Sec. D in the Appendix), referred to as "MSE baseline".
6

Under review as a conference paper at ICLR 2019

Ours 0.0341bpp

BPG 0.102bpp

Ours 0.0339bpp

BPG 0.0382bpp

Figure 3: Visual example of images from RAISE1k produced by our GC network with C = 4 along

with the corresponding results for BPG.

User study: In the extreme compression regime realized by our GC models, where texture and sometimes even more abstract image content is synthesized, common reconstruction quality measures such as PSNR and MS-SSIM arguably lose significance as they penalize changes in local structure rather than assessing preservation of the global image content (this also becomes apparent by comparing reconstructions produced by our GC model with those obtained by the MSE baseline and BPG, see Fig. 2). Indeed, measuring PSNR between synthesized and real texture patches essentially quantifies the variance of the texture rather than the visual quality of the synthesized texture.
To quantitatively evaluate the perceptual quality of our GC models in comparison with BPG and AEDC (for Cityscapes) we therefore conduct a user study using Amazon Mechanical Turk (AMT)3. We consider two GC models with C = 4, 8 trained on Open Images, three GC (D+) models with C = 2, 4, 8 trained on Cityscapes, and BPG at rates ranging from 0.045 to 0.12 bpp. Questionnaires are composed by combining the reconstructions produced by the selected GC model for all testing images with the corresponding reconstruction produced by the competing baseline model side-byside (presenting the reconstructions in random order). The original image is shown along with the reconstructions, and the pairwise comparisons are interleaved with 3 probing comparisons of an additional uncompressed image from the respective testing set with an obviously JPEG-compressed version of that image. 20 randomly selected unique users are asked to indicate their preference for each pair of reconstructions in the questionnaire, resulting in a total of 480 ratings per pairing of methods for Kodak, and 400 ratings for RAISE1K and Cityscapes. For each pairing of methods, we report the mean preference score as well as the standard error (SE) of the per-user mean preference percentages. Only users correctly identifying the original image in all probing comparisons are taken into account for the mean preference percentage computation. To facilitate comparisons for future works, we will release all images used in the user studies.
Semantic quality of SC models: The issues with PSNR and MS-SSIM for evaluating the quality of generated content described in the previous paragraph become even more severe for SC models as a large fraction of the image content is generated from a semantic label map. Following image translation works Isola et al. (2017); Wang et al. (2018), we therefore measure the capacity of our SC models to preserve the image semantics in the synthesized regions and plausibly blend them with the preserved regions--the objective SC models are actually trained for. Specifically, we use PSPNet (Zhao et al., 2016) and compute the mean intersection-over-union (IoU) between the label map obtained for the decompressed validation images and the ground truth label map. For reference we also report this metric for baselines that do not use semantic label maps for training and/or deployment.

6 RESULTS
6.1 GENERATIVE COMPRESSION
Fig. 4 shows the mean preference percentage obtained by our GC models compared to BPG at different rates, on the Kodak and the RAISE1K data set. In addition, we report the mean preference percentage for GC models compared to BPG and AEDC on Cityscapes. Example validation images for side-by-side comparison of our method with BPG for images from the Kodak, RAISE1K, and Cityscapes data set can be found in Figs. 1, 3, and 2, respectively. Furthermore, we perform extensive visual comparisons of all our methods and the baselines, presented in Appendix F.
3https://www.mturk.com/

7

Under review as a conference paper at ICLR 2019

%%ooffUUsseerrssPPrreeffeerrrriinnggOOuurrGGCCttooBBPPGGoonnKKooddaakk

110000%%
OOuurrGGCC,,CC==44((00..003333bbpppp))

OOuurrGGCC,,CC==88((00..006666bbpppp))

7755%%

%%ooffUUsseerrssPPrreeffeerrrriinnggOOuurrGGCCttooBBPPGGoonnRRAAIISSEE11KK

110000%%
OOuurrGGCC,,CC==44((00..003333bbpppp))

OOuurrGGCC,,CC==88((00..006666bbpppp))

7755%%

GGCCCC==44pprerefeferreredd 5500%%
BBPPGG9955%%lalarrggeerr,,aanndd GGCCCC==44sstitlillpprreeffeerrreedd
2255%%

GGCCCC==88pprerefeferreredd
BBPPGG2211%%lalarrggeerr,,aanndd GGCCCC==88sstitlillpprreeffeerrreedd

5500%% GGCCCC==44pprerefeferreredd
BBPPGG112244%%lalarrggeerr,,aanndd GGCCCC==44sstitlillpprreeffeerrreedd
2255%%

GGCCCC==88pprerefeferreredd
BBPPGG4499%%lalarrggeerr,,aanndd GGCCCC==88sstitlillpprreeffeerrreedd

0.00.202 00.0.033300.0.04422 (G(GCCCC==44) )

00.0.06655 00.0.088

[b[bppp] ] 0.01.1 0.00.202

00.0.0666 00.0.088 (G(GCCCC==88) )

[b[bppp] ] 00.1.1

0.00.202 00.0.0333 (G(GCCCC==44) )

00.0.06655 00.0.07755 00.0.09988

[b[bppp] ] 0.01.313 0.00.202

00.0.0666 00.0.088 00.0.09988 (G(GCCCC==88) )

[b[bppp] ] 00.1.122 0.01.313

% of Users Preferring Our GC to BPG and AEDC on Cityscapes

100% Our GC, C=2 (0.018bpp)

Our GC, C=4 (0.036bpp)

Our GC, C=8 (0.070bpp)

75%

GC C=2 50% preferred

GC C=4 preferred

25% BPG 181% larger, and GC C=2 still preferred
[bpp]

BPG 123% larger, and GC C=4 still preferred
[bpp]

GC C=8 preferred
BPG 40% larger, and GC C=8 still preferred
0.069 AEDC

[bpp]

0.018 (GC C=2)

0.040 0.059 0.069 AEDC

0.1 0.02 0.036 0.059 0.069 0.079 0.099

(GC C=4)

AEDC

0.13 0.02

0.070 0.079 0.099 (GC C=8)

0.12 0.13

Figure 4: User study results evaluating our GC models on Kodak, RAISE1K (top) and Cityscapes

(bottom). For Kodak and RAISE1K, we use GC models trained on Open Images, without any

semantic label maps. 2Fchor Cityscapes, we used GC (D+), using 8scehmantic label maps only for D and

o%%nlooyffUdUussereBBrirPPsnsGGPgXPPrrerfeterffeaerirrnriininngggOO. uTur5rh20G.6.Ge0646C97Cs7%4ttatoonBBdPaPGrG0d.0a5a93ne38n7%dr5drAoAErEDiDsCCcooonBBmnPPCGGCpPXiriutetyfytsesccdaappoeevsser

per-user0.079155 63.125%

mean preference0.09909 53.5714%

0.12171 44.7222%

percentages.

CVPR X

0.069

CVPR X

0.069

Our GC modelsCVPR Pref with C = 447.8125% are preferred to BCPVPRGPreef ven when im83%ages produced by BPG use 95% and

110000%% OOuurrGGCC,,CC==212(2(00.40.01%188bbpmppp)Bo)PGreSTDbDEiVtsOOtuhurraGGnCCt,,hCC0o=.0=7s414e7(8(020p.0.0r33o66bdbppupp0)c).0e56d529by our OmOuBuPorrGGdSGTeCDCDl,,EsCVC=f=o88r((00K.0.07o700dbbpa0pp.k0p)2)92a73nd RA0I.0S382E761K, re0s.0p240e47ctively. Notably

7755%%

this

is

achievedCVPR STDDEV ours

even though0.07221 0.01767578125

there

is

a

CVPR STDDEV
distribouurstion shift

0.02974
between0.07094497681

the

training

and

testing

set

(recall

GGCCCC==22 that thesgeain GC mod2.e81l4s019a88r95e027t6rained on Open Imgaain ges). The gains of1.39671622228274 domain-specificity and semantic

5500%% pprerefeferreredd

label C=

maps 2 areBPG X

(pGGfCrCoeCCr=f=4e4tpprrrerraefefeerirndereddinto0g.05)B937bP45ceGhcoemvee0.0ns791aw55phpeanrGGeCCtnChC0=t.=e0889pf9pr0lrer9efaoefertrmertedeGBdGBrPCPtCGhGuCC4e=4s0=80e%8%rsssteitllialsla1rlpurgp8grerelere1ftr,fes,era%ranrenodedddnmCorietybscitasp. eFs:orOCur

GC =4

models with the gains on

2255%% BGBGPCPCGGCC1Cr=18=e2812i1q%st%styuitlillsiallparrcprrgeregaeefseferperCCBr,[br,[PVVbaebrepaPPGnepdsnRRep]ddP]dPXrterwafefreeecno2m1paan76r92da..GB03BG35b0PC83P.4C0G83lG6%%C9e9C1=12%=423t43%os%stitmlilltallpahr5oprr0greog.ree3fef5ereser7r,1r,ear%ebane[dbn[ddbpiodptp]sb] ttahi3an2.n6e66d7o%ufroA0A0E.rG0E.D06D69CG9CCCmoondeRlsAwIiSthE[b[bp1pCp]K] =. F8o.r all three data sets, BPG

00.0.01188 (G(GCCCC==22) )

00.0.04400 00.0.0559900.0.06699
Discussion: The GC models produce images with much finer detail than BPG, which suffers fromAAEEDDCC

B0P.01G.1 S0T.00D.20D2(EG(VG0C0.C0.C03C3=66=44))

00.0.0559900.0.06069.093030.00.073799 AAEEDDCC

00.0.0999 0.0369310.01.313 0.00.202

CVPR STDDEV

0.07221

00.0.07700 00.0.07799 00.0.0999 (G(GCCCC==88) )

00.1.1220.01.313

smootheodurs patches a0n.03d5592b65l1o37cking artifacts. In particular, the GC models convincingly reconstruct

texture ingainnatural objects such2.22391412140534 as trees, water, and sky, and is most challenged with scenes involving

hu22mcchhans. AEDC and the MSE baseline both88pcchhroduce blurry images.

BBPPGGXX BBPPGGPPreref f CCVVPPRRXX CCVVPPRRPPreref f BBPPGGSSTTDDDDEEVV CCVVPPRRSSTTDDDDEEVV oouursrs ggaainin
BBPPGGXX BBPPGGPPreref f CCVVPPRRXX CCVVPPRRPPreref f BBPPGGSSTTDDDDEEVV CCVVPPRRSSTTDDDDEEVV oouursrs ggaainin

00.0.044997744
We see that the gains of our models are maximal at extreme bitrates, with BPG needing 95­181%5522.6.66677%%

00.0.05599337755 3388%%

BBPPGGXX BBPPGGPPreref f

00.0.0779911555 6633.1.12255%%

00.0.09990099 5533.5.5771144%%

00.1.122117711 444.7.72222%%

more00.0.b06699its for the C = 2CC,VVP4PRRXXmodels on th00.e0.06699three datasets. For C = 8 gains are smaller but still

very large4477.8.8112255%% (BPG needingCCV2VPPR1RPP­reref4f 9% more b883i3%t%s). This is expected, since as the bitrate increases the

00.0.07711778822
classical compression measures (PSNR/MS-SSIM) become more meaningful--and our system does00.0.07722211

00.0.05566552299

BBPPGGSSTTDDDDEEVV CCVVPPRRSSTTDDDDEEVV

00.0.02299227733 00.0.022997744

00.0.03388227766

00.0.02244004477

not employ00.0.011776677557788112255 the full complooueursrxs ity of cur0r0.0.e0770n0994t44997s76t688a11te-of-the-art systems, as discussed next.

22.8.81144001199888995500227766

ggaainin

11.3.399667711662222288227744

State-o44cfch-hthe-art on Kodak: We give an overview of relevant recent learned compression methods and their00.0.05599337755 differences00.0.0779911555 to our00.0.09990099 GC method and BPG in Table 1 in the Appendix. Rippel & Bourdev

(2017) also6622.0.055888%%

used GANs (albeit5500.3.3557711%%

3322.6.66677%%

a

different

formulation)

and

were

state-of-the-art

in

MS-SSIM

in

20170,0.0.0669w9 hile the concurrent work of Minnen et al. (2018) is the current state-of-the-art in image

7799.3.3333%%

compression in terms00.0.03330033

00.0.03366993311

of

classical

metrics

(PSNR

and

MS-SSIM)

when

measured

on

the

Kodak

data00.s0.07e722t211(Kodak). Notably, all methods except ours (BPG, Rippel et al., and Minnen et al.) employ

adaptive00.0.03355599226655113377 arithmetic coding using context models for improved compression performance. Such
22.2.22339911441122114400553344
models could also be implemented for our system, and have led to additional savings of 10% in

Mentzer et al. (2018). Since Rippel et al. and Minnen et al. have only released a selection of their

decoded images (for 3 and 4 out of the 24 Kodak images, respectively), and at significantly higher

bitrates, a comparison with a user study is not meaningful. Instead, we try to qualitatively put our

results into context with theirs.

1

In Figs. 12­14 in the Appendix, we compare qualitatively to Rippel & Bourdev (2017). We can observe that even Rippel & Bourdev (2017) use 29­179% more bits, our models produce images of comparable or better quality.

In Figs. 15­18, we show a qualitative comparison of our results to the images provided by the concurrent work of Minnen et al. (2018), as well as to BPG (Bellard) on those images. First, we

8

Under review as a conference paper at ICLR 2019

Uniform, Open Images Uniform, Cityscapes Code samples generated by WGAN-GP, Cityscapes Figure 5: Sampling codes w^ uniformly (left), and generating them with a WGAN-GP (right).

see that BPG is still visually competitive with the current state-of-the-art, which is consistent with moderate 8.41% bitrate savings being reported by Minnen et al. (2018) in terms of PSNR. Second, even though we use much fewer bits compared to the example images available from Minnen et al. (2018), for some of them (Figs. 15 and 16) our method can still produce images of comparable visual quality.
Given the dramatic bitrate savings we achieve according to the user study (BPG needing 21­181% more bits), and the competitiveness of BPG to the most recent state-of-the-art (Minnen et al., 2018), we conclude that our proposed system presents a significant step forward for visually pleasing compression at extreme bitrates.

Sampling the compressed representations: In Figure 5 we explore the representation learned by our GC models (with C = 4), by sampling the (discrete) latent space of w^. When we sample uniformly, and decode with our generative compression (GC) models into images, we obtain a "soup of image patches" which reflects the domain the models were trained on (e.g. street signs and building patches on Cityscapes). Note that we should not expect these outputs to look like normal images, since nothing forces the encoder output w^ to be uniformly distributed over the discrete latent space.
However, given the low dimensionality of w^ (32 × 64 × 4 for 512 × 1024px Cityscape images), it would be interesting to try to learn the true distribution. To this end, we perform a simple experiment and train an improved Wasserstein GAN (WGAN-GP) (Gulrajani et al., 2017) on w^ extracted from Cityscapes, using default parameters and a ResNet architecture4. By feeding our GC model with samples from the WGAN-GP generator, we easily obtain a powerful generative model, which generates sharp 1024 × 512px images from scratch. We think this could be a promising direction for building high-res generative models. In Figs. 19­21 in the Appendix, we show more samples and also that when sampling the same architecture but optimized for MSE, we obtain much blurrier samples than with the WGAN-GP and more noisy "patch soups" than with uniform sampling.

50% 40% 30% 20% 10%
0.00

mIoU vs. bpp

Figure 6: Mean IoU as a function of bpp on the

Cityscapes validation set for our GC and SC networks,

and for the MSE baseline. We show both SC modes: RI

(inst.), RB (box). D+ annotates models where instance

Ours (GC, D+) Ours (SC, inst., EDG+) Ours (SC, box, EDG+)

semantic label maps are fed to the discriminator (only during training); EDG+ indicates that semantic label
maps are used both for training and deployment. The

MSE baseline BPG AEDC pix2pixHD baseline

pix2pixHD baseline (Wang et al., 2018) was trained
from scratch for 50 epochs, using the same downsampled 1024 × 512px training images as for our method.

0.04 0.08 0.12 0.16 0.20

6.2 SELECTIVE GENERATIVE COMPRESSION
Fig. 6 shows the mean IoU on the Cityscapes validation set as a function of bpp for SC networks with C = 2, 4, 8, along with the values obtained for the baselines. Additionally, we plot mean IoU for GC with semantic label maps fed to the discriminator (D+), and the MSE baseline.
In Fig. 7 we present example Cityscapes validation images produced by the SC network trained in the RI mode with C = 8, where different semantic classes are preserved. More visual results for the
4We only adjusted the architecture to output 32 × 64 × 4 tensors instead of 64 × 64 × 3 RGB images.

9

Under review as a conference paper at ICLR 2019

road (0.146 bpp, -55%)

car (0.227 bpp, -15%)

everything (0.035 bpp, -89%)

people (0.219 bpp, -33%)

building (0.199 bpp, -39%) no synth. (0.326 bpp, -0%)

Figure 7: Synthesizing different classes using our SC network with C = 8. In each image except for no synthesis, we additionally synthesize the classes vegetation, sky, sidewalk, ego vehicle, wall. The heatmaps in the lower left corners show the synthesized parts in gray. We show the bpp of each image as well as the relative savings due to the selective generation.

SC networks trained on Cityscapes can be found in Appendix F.7, including results obtained for the RB operation mode and by using semantic label maps estimated from the input image via PSPNet (Zhao et al., 2017).
Discussion: The quantitative evaluation of the semantic preservation capacity (Fig. 6) reveals that the SC networks preserve the semantics somewhat better than pix2pixHD, indicating that the SC networks faithfully generate texture from the label maps and plausibly combine generated with preserved image content. The mIoU of BPG, AEDC and the MSE baseline is considerably lower than that obtained by our SC and GC models, which can arguably be attributed to blurring and blocking artifacts. However, it is not surprising as these baseline methods do not use label maps during training and prediction.
In the SC operation mode, our networks manage to seamlessly merge preserved and generated image content both when preserving object instances and boxes crossing object boundaries (see Appendix F.7). Further, our networks lead to reductions in bpp of 50% and more compared to the same networks without synthesis, while leaving the visual quality essentially unimpaired, when objects with repetitive structure are synthesized (such as trees, streets, and sky). In some cases, the visual quality is even better than that of BPG at the same bitrate. The visual quality of more complex synthesized objects (e.g., buildings, people) is worse. However, this is a limitation of current GAN technology rather than our approach. As the visual quality of GANs improves further, SC networks will as well. Notably, the SC networks can generate entire images from the semantic label map only.
Finally, the semantic label map, which requires 0.036 bpp on average for the downscaled 1024 × 512px Cityscapes images, represents a relatively large overhead compared to the storage cost of the preserved image parts. This cost vanishes as the image size increases, since the semantic mask can be stored as an image dimension-independent vector graphic.

7 CONCLUSION
We proposed and evaluated a GAN-based framework for learned compression that significantly outperforms prior works for low bitrates in terms of visual quality, for compression of natural images. Furthermore, we demonstrated that constraining the application domain to street scene images leads to additional storage savings, and we explored combining synthesized with preserved image content with the potential to achieve even larger savings. Interesting directions for future work are to develop a mechanism for controlling spatial allocation of bits for GC (e.g. to achieve better preservation of faces; possibly using semantic label maps), and to combine SC with saliency information to determine what regions to preserve. In addition, the sampling experiments presented in Sec. 6.1 indicate that combining our GC compression approach with GANs to (unconditionally) generate compressed representations is a promising avenue to learn high-resolution generative models.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Kodak PhotoCD dataset. http://r0k.us/graphics/kodak/.
WebP Image format. https://developers.google.com/speed/webp/.
Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and Luc Van Gool. Soft-to-hard vector quantization for end-to-end learning compressible representations. arXiv preprint arXiv:1704.00648, 2017.
Martin Arjovsky and Le´on Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Johannes Balle´, Valero Laparra, and Eero P Simoncelli. End-to-end optimization of nonlinear transform codes for perceptual quality. arXiv preprint arXiv:1607.05006, 2016a.
Johannes Balle´, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression. arXiv preprint arXiv:1611.01704, 2016b.
Johannes Balle´, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. In International Conference on Learning Representations (ICLR), 2018.
Fabrice Bellard. BPG Image format. https://bellard.org/bpg/.
M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. ArXiv e-prints, April 2016.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Duc-Tien Dang-Nguyen, Cecilia Pasquini, Valentina Conotter, and Giulia Boato. Raise: a raw images dataset for digital image forensics. In Proceedings of the 6th ACM Multimedia Systems Conference, pp. 219­224. ACM, 2015.
Leonardo Galteri, Lorenzo Seidenari, Marco Bertini, and Alberto Del Bimbo. Deep generative adversarial compression artifact removal. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4826­4835, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767­5777, 2017.
Chenlei Guo and Liming Zhang. A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression. IEEE transactions on image processing, 19 (1):185­198, 2010.
Rupesh Gupta, Meera Thapar Khanna, and Santanu Chaudhury. Visual saliency guided video compression algorithm. Signal Processing: Image Communication, 28(9):1006­1022, 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross Girshick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 2980­2988. IEEE, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
11

Under review as a conference paper at ICLR 2019
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1125­1134, 2017.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, 2016.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In International Conference on Learning Representations (ICLR), 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Shahab Kamali, Matteo Malloci, Jordi PontTuset, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://storage.googleapis.com/openimages/web/index.html, 2017.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4681­4690, 2017.
Mu Li, Wangmeng Zuo, Shuhang Gu, Debin Zhao, and David Zhang. Learning convolutional networks for content-weighted image compression. arXiv preprint arXiv:1703.10553, 2017.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems, pp. 700­708, 2017.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2813­2821. IEEE, 2017.
Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Conditional probability models for deep image compression. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
David Minnen, Johannes Balle´, and George Toderici. Joint autoregressive and hierarchical priors for learned image compression. arXiv preprint arXiv:1809.02736, 2018.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Oren Rippel and Lubomir Bourdev. Real-time adaptive image compression. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 2922­2930, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
Shibani Santurkar, David Budden, and Nir Shavit. Generative compression. arXiv preprint arXiv:1703.01467, 2017.
X Yu Stella and Dimitri A Lisin. Image compression based on visual saliency at individual scales. In International Symposium on Visual Computing, pp. 157­166. Springer, 2009.
12

Under review as a conference paper at ICLR 2019
David S. Taubman and Michael W. Marcellin. JPEG 2000: Image Compression Fundamentals, Standards and Practice. Kluwer Academic Publishers, Norwell, MA, USA, 2001. ISBN 079237519X.
Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszar. Lossy image compression with compressive autoencoders. In International Conference on Learning Representations (ICLR), 2017.
George Toderici, Sean M O'Malley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet Baluja, Michele Covell, and Rahul Sukthankar. Variable rate image compression with recurrent neural networks. arXiv preprint arXiv:1511.06085, 2015.
George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and Michele Covell. Full resolution image compression with recurrent neural networks. arXiv preprint arXiv:1608.05148, 2016.
Robert Torfason, Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Towards image understanding from deep compression without decoding. In International Conference on Learning Representations (ICLR), 2018. URL https:// openreview.net/forum?id=HkXWCMbRW.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale structural similarity for image quality assessment. In Asilomar Conference on Signals, Systems Computers, 2003, volume 2, pp. 1398­ 1402 Vol.2, Nov 2003.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In IEEE Int. Conf. Comput. Vision (ICCV), pp. 5907­5915, 2017.
H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid Scene Parsing Network. ArXiv e-prints, December 2016.
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2223­2232, 2017.
13

Under review as a conference paper at ICLR 2019

A DIFFERENCES TO STATE OF THE ART

Learned Arithmetic encoding Context model Visualized bitrates [bpp]5 GAN S.o.t.a. in MS-SSIM S.o.t.a. in PSNR Savings to BPG in PSNR Savings to BPG in User Study

BPG
No Adaptive CABAC all6 No No No

Rippel et al. (2017) Yes Adaptive Autoregressive
0.08­ Non-standard No No

Minnen et al. (2018) Yes Adaptive Autoregressive
0.12­ No Yes Yes 8.41%

Ours(GC)
Yes Static None 0.033­0.066 f-div. based No No
17.2­48.7%

Table 1: Overview of differences between (Minnen et al., 2018) (s.o.t.a. in MS-SSIM and PSNR), to BPG (previous s.o.t.a. in PSNR) and (Rippel & Bourdev, 2017) (s.o.t.a. in MS-SSIM in 2017, also used GANs).

B COMPRESSION DETAILS
When encoding the channels of w^ to a bit-stream, we use an arithmetic encoder where frequencies are stored for each channel separately and then encode them in a static (non-adaptive) manner, similar to Agustsson et al. (2017). In our experiments, this leads to 8.8% smaller bitrates compared to the upper bound.
We compress the semantic label map for SC by quantizing the coordinates in the vector graphic to the image grid and encoding coordinates relative to preceding coordinates when traversing object boundaries (rather than relative to the image frame). The so-obtained bitstream is then compressed using arithmetic coding.
To ensure fair comparison, we do not count header sizes for any of the baseline methods throughout.
C ARCHITECTURE DETAILS
For the GC, the encoder E convolutionally processes the image x and optionally the label map s, with spatial dimension W × H, into a feature map of size W/16 × H/16 × 960 (with 6 layers, of which four have 2-strided convolutions), which is then projected down to C channels (where C  {2, 4, 8} is much smaller than 960). This results in a feature map w of dimension W/16 × H/16 × C, which is quantized over L centers to obtain the discrete w^. The generator G projects w^ up to 960 channels, processes these with 9 residual units (He et al., 2016) at dimension W/16 × H/16 × 960, and then mirrors E by convolutionally processing the features back to spatial dimensions W × H (with transposed convolutions instead of strided ones).
Similar to E, the feature extractor F for SC processes the semantic map s down to the spatial dimension of w^, which is then concatenated to w^ for generation. In this case, we consider slightly higher bitrates and downscale by 8× instead of 16× in the encoder E, such that dim(w^) = W/8 × H/8 × C. The generator then first processes w^ down to W/16 × H/16 × 960 and then proceeds as for GC.
For both GC and SC, we use the multi-scale architecture of (Wang et al., 2018) for the discriminator D, which measures the divergence between px and pG(z) both locally and globally.
We adopt the notation from (Wang et al., 2018) to describe our encoder and generator/decoder architectures and additionally use q to denote the quantization layer (see Sec. 3 for details). The output of q is encoded and stored.
· Encoder GC: c7s1-60,d120,d240,d480,d960,c3s1-C,q
5This refers to the bitrates of decoded images the authors have made available. 6Code available, image can be compressed from extreme bpps (< 0.1bpp) to lossless.
14

Under review as a conference paper at ICLR 2019

· Encoders SC:
­ Semantic label map encoder: c7s1-60,d120,d240,d480,d960 ­ Image encoder: c7s1-60,d120,d240,d480,c3s1-C,q,c3s1-480,d960
The outputs of the semantic label map encoder and the image encoder are concatenated and fed to the generator/decoder.
· Generator/decoder: c3s1-960,R960,R960,R960,R960,R960,R960,R960, R960,R960,u480,u240,u120,u60,c7s1-3

m x E w q w^

sF

G x^ D

Figure 8: Structure of the proposed SC network. E is the encoder for the image x and the semantic label map s. q quantizes the latent code w to w^. The subsampled heatmap multiplies w^ (pointwise) for spatial bit allocation. G is the generator/decoder, producing the decompressed image x^, and D is the discriminator used for adversarial training. F extracts features from s .

D TRAINING DETAILS
We employ the ADAM optimizer (Kingma & Ba, 2014) with a learning rate of 0.0002 and set the mini-batch size to 1. Our networks are trained for 150000 iterations on Cityscapes and for 280000 iterations on Open Images. In the second half of the Open Images training, we train with the generator/decoder in the test mode of Batch normalization(Ioffe & Szegedy, 2015) (i.e. fixing the batch statistics and training the parameters only), since we found this reduced artifacts and color shift.
E DATASET AND PREPROCESSING DETAILS
To train GC models (which do not require semantic label maps, neither during training nor for deployment) for compression of diverse natural images, we use 200k images sampled randomly from the Open Images data set (Krasin et al., 2017) (9M images). The training images are rescaled so that the longer side has length 768px, and images for which rescaling does not result in at least 1.25× downscaling as well as high saturation images (average S > 0/9 or V > 0.8 in HSV color space) are discarded (resulting in an effective training set size of 188k). We evaluate these models on the Kodak image compression dataset (Kodak) (24 images, 768 × 512px), which has a long tradition in the image compression literature and is still the most frequently used dataset for comparisons of learned image compression methods. Additionally, we evaluate our GC models on 20 randomly selected images from the RAISE1K data set (Dang-Nguyen et al., 2015), a real-world image dataset consisting of 8156 high-resolution RAW images (we rescale the images such that the longer side has length 768px). To investigate the benefits of having a somewhat constrained application domain and semantic labels at training time, we also train GC models with semantic label maps on the Cityscapes data set (Cordts et al., 2016) (2975 training and 500 validation images, 34 classes, 2048 × 1024px resolution) consisting of street scene images and evaluate it on 20 randomly selected validation images (without semantic labels). Both training and validation images are rescaled to 1024 × 512px resolution.
To evaluate the proposed SC method (which requires semantic label maps for training and deployment) we again rely on the Cityscapes data set. Cityscapes was previously used to generate images form semantic label maps using GANs (Isola et al., 2017; Zhu et al., 2017). The preprocessing for SC is the same as for GC.
15

Under review as a conference paper at ICLR 2019
F VISUALS
In the following Sections, F.1, F.2, F.3, we show the first five images of each of the three datasets we used for the user study, next to the outputs of BPG at similar bitrates. Secs. F.4 and F.5 provide visual comparisons of our GC models with Rippel & Bourdev (2017) and Minnen et al. (2018), respectively, on a subset of images form the Kodak data set. In Section F.6, we show visualizations of the latent representation of our GC models. Finally, Section F.7 presents additional visual results for SC.
16

Under review as a conference paper at ICLR 2019
F.1 GENERATIVE COMPRESSION ON KODAK Ours

BPG

0.043 bpp

0.034 bpp

0.031 bpp

0.030 bpp

0.035 bpp

0.034 bpp

0.032 bpp

0.032 bpp

0.0430 bpp

0.036 bpp

Figure 9: First 5 images of the Kodak data set, produced by our GC model with C = 4 and BPG. 17

Under review as a conference paper at ICLR 2019
F.2 GENERATIVE COMPRESSION ON RAISE1K Ours

BPG

0.038 bpp

0.036 bpp

0.053 bpp

0.035 bpp

0.038 bpp

0.034 bpp

0.044 bpp

0.036 bpp

0.0380 bpp

0.034 bpp

Figure 10: First 5 images of RAISE1k, produced by our GC model with C = 4 and BPG. 18

Under review as a conference paper at ICLR 2019
F.3 GENERATIVE COMPRESSION ON CITYSCAPES Ours

BPG

0.040 bpp

0.036 bpp

0.038 bpp

0.036 bpp

0.043 bpp

0.036 bpp

0.037 bpp

0.036 bpp

0.0400 bpp

0.036 bpp

Figure 11: First 5 images of Cityscapes, produced by our GC model with C = 4 and BPG. 19

Under review as a conference paper at ICLR 2019 F.4 COMPARISON TO RIPPEL & BOURDEV (2017)

Original

Ours, 0.0304bpp

Rippel et al., 0.0828bpp (+172%)

Figure 12: Our model loses more texture but has less artifacts on the knob. Overall, it looks compa-

rable to the output of Rippel & Bourdev (2017), using significantly fewer bits.

Original

Ours, 0.0651bpp

Rippel et al., 0.0840bpp (+29%)

Figure 13: Notice that compared to Rippel & Bourdev (2017), our model produces smoother lines

at the jaw and a smoother hat, but proides a worse reconstruction of the eye.

Original

Ours, 0.0668bpp

Rippel et al., 0.0928bpp (+39%)

Figure 14: Notice that our model produces much better sky and grass textures than Rippel & Bourdev

(2017), and also preserves the texture of the light tower more faithfully.

20

Under review as a conference paper at ICLR 2019 F.5 COMPARISON TO MINNEN ET AL. (2018)

Original

Ours, 0.0668bpp

Minnen et al., 0.221bpp 230% larger BPG, 0.227bpp Figure 15: Notice that our model yields sharper grass and sky, but a worse reconstruction of the fence and the lighthouse compared to Minnen et al. (2018). Compared to BPG, Minnen et al. produces blurrier grass, sky and lighthouse but BPG suffers from ringing artifacts on the roof of the second building and the top of the lighthouse.
21

Under review as a conference paper at ICLR 2019

Original

Ours, 0.0685bpp

Minnen et al., 0.155bpp, 127% larger

BPG, 0.164bpp

Figure 16: Our model produces an overall sharper face compared to Minnen et al. (2018), but the texture on the cloth deviates more from the original. Compared to BPG, Minnen et al. has a less blurry face and fewer artifacts on the cheek.

Original

Ours, 0.0328bpp

Minnen et al., 0.246bpp, 651% larger

BPG, 0.248bpp

Figure 17: Here we obtain a significantly worse reconstruction than Minnen et al. (2018) and BPG, but use only a fraction of the bits. Between BPG and Minnen et al., it is hard to see any differences.

22

Under review as a conference paper at ICLR 2019

Original

Ours, 0.03418bpp

Minnen et al., 0.123bpp, 259% larger,

BPG, 0.119bpp

Figure 18: Here we obtain a significantly worse reconstruction compared to Minnen et al. (2018) and BPG, but use only a fraction of the bits. Compared to BPG, Minnen et al.has a smoother background but less texture on the birds.

23

Under review as a conference paper at ICLR 2019 F.6 SAMPLING THE COMPRESSED REPRESENTATIONS

Cityscapes

Open Images

Figure 19: We uniformly sample codes from the (discrete) latent space w^ of our generative compression models (GC with C = 4) trained on Cityscapes and Open Images. The Cityscapes model outputs domain specific patches (street signs, buildings, trees, road), whereas the Open Images sam-
ples are more colorful and consist of more generic visual patches.

GC (C = 4)

MSE (C = 4)

Figure 20: We train the same architecture with C = 4 for MSE and for generative compression on Cityscapes. When uniformly sampling the (discrete) latent space w^ of the models, we see stark differences between the decoded images G(w^). The GC model produces patches that resemble

parts of Cityscapes images (street signs, buildings, etc.), whereas the MSE model outputs looks like

low-frequency noise.

24

Under review as a conference paper at ICLR 2019

GC model with C = 4

MSE baseline model with C = 4

Figure 21: We experiment with learning the distribution of E(x) by training an improved Wasserstein GAN (Gulrajani et al., 2017). When sampling form the decoder/generator G of our model by feeding it with samples from the improved WGAN generator, we obtain much sharper images than
when we do the same with an MSE model.

25

Under review as a conference paper at ICLR 2019 F.7 SELECTIVE COMPRESSION ON CITYSCAPES

road (0.077 bpp)

car (0.108 bpp)

everything (0.041 bpp)

people (0.120 bpp)

building (0.110 bpp)

no synth (0.186 bpp)

road (0.092 bpp)

car (0.134 bpp)

everything (0.034 bpp)

people (0.147 bpp)

building (0.119 bpp)

no synth (0.179 bpp)

Figure 22: Synthesizing different classes for two different images from Cityscapes, using our SC network with C = 4. In each image except for no synthesis, we additionally synthesize the classes vegetation, sky, sidewalk, ego vehicle, wall.

26

Under review as a conference paper at ICLR 2019

Figure 23: Example images obtained by our SC network (C = 8) preserving a box and synthesizing the rest of the image, on Cityscapes. The SC network seamlessly merges preserved and generated image content even in places where the box crosses object boundaries.

0.019 bpp

0.021 bpp

0.013 bpp

Figure 24: Reconstructions obtained by our SC network using semantic label maps estimated from the input image via PSPNet (Zhao et al., 2017).

27

