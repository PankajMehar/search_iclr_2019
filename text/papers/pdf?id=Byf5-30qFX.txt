Under review as a conference paper at ICLR 2019
DHER: HINDSIGHT EXPERIENCE REPLAY FOR DYNAMIC GOALS
Anonymous authors Paper under double-blind review
ABSTRACT
Dealing with sparse rewards is one of the most important challenges in reinforcement learning (RL), especially when a goal is dynamic (e.g., to grasp a moving object). Hindsight experience replay (HER) has been shown an effective solution to handling sparse rewards with fixed goals. However, it does not account for dynamic goals in its vanilla form and, as a result, even degrades the performance of existing off-policy RL algorithms when the goal is changing over time. In this paper, we present Dynamic Hindsight Experience Replay (DHER), a novel approach for tasks with dynamic goals and sparse rewards. DHER automatically assembles successful experiences from two relevant failures and learns a reliable policy to achieve the dynamic goals. We evaluate DHER on tasks of robotic manipulation and moving object tracking, and transfer the polices from simulation to physical robots. Extensive comparison and ablation studies demonstrate the superiority of our approach, showing that DHER is a crucial ingredient to enable RL to solve tasks with dynamic goals.
1 INTRODUCTION
Deep reinforcement learning has been shown an effective framework for solving a rich repertoire of complex control problems. In simulated domains, agents have been trained to perform a diverse array of challenging tasks (Mnih et al., 2015; Lillicrap et al., 2015; Duan et al., 2016). In order to train such agents, it is often the case that one has to design a reward function that not only reflects the task at hand but also is carefully shaped (Ng et al., 1999) to guide the policy optimization. Unfortunately, many of the capabilities demonstrated by reward engineering are often limited to specific tasks. Moreover, it requires both RL expertise and domain-specific knowledge to reshape the reward functions. For situations where we do not know what admissible behavior may look like, for example, using LEGO bricks to build a desired architecture, it is difficult to apply reward engineering. Therefore, it is essential to develop algorithms which can learn from unshaped and usually sparse reward signals.
Learning with sparse rewards is challenging, especially when a goal is dynamic. The difficulty posed by a sparse reward is exacerbated by the complicated environment dynamics in robotics (Andrychowicz et al., 2017). For instance, system dynamics around contacts are difficult to model and induce sensitivity in the system to small errors. Many robotic tasks also need executing multiple steps successfully over a long horizon, involve enormous search space, and require generalization to varying task instances. Policy gradient methods are breakthroughs in the challenging environments, such as PPO (Heess et al., 2017; Schulman et al., 2017), ACER (Wang et al., 2016), TRPO (Schulman et al., 2015) and so on. They are used in environments, where an agent tries to reach a target, learns to walk, runs, and so on. Recently, sampling-efficient learning is introduced and demonstrates a significant increase in performance for off-policy actor-critic DQN (Mnih et al., 2015) and DDPG (Lillicrap et al., 2015) algorithms. Hindsight experience replay (HER) is very effective for improving the performance of off-policy RL algorithms in solving goal-based tasks with sparse rewards (Andrychowicz et al., 2017). Similar to UVFA (Schaul et al., 2015a), it takes a goal state as part of input. However, it assumes the goal is fixed. As a result, this assumption actually impedes the learning of RL agents in the environments of moving goals.
In this paper, we address this challenge with a new method, Dynamic Hindsight Experience Replay (DHER), for accomplishing tasks with moving goals. We follow the goal state setting in
1

Under review as a conference paper at ICLR 2019

Failed episodes
...
... Ei
...
... Ej
...

Gripper trajectory

Goal trajectory Get a trajectory of achieved goals

Gripper trajectory Goal trajectory

Get a trajectory of desired goals

Failed twice then success
Matched

Figure 1: The framework of DHER. DHER searches relevant failed experiences and then assembles them into successful experiences.

UVFA (Schaul et al., 2015a) and HER (Andrychowicz et al., 2017). HER turns a failed episode to a success by composing a new task whose goal is achieved by that episode. Our idea allows an agent to learn from the failure one step further than HER: the agent not only sets a new goal but also hallucinates how to reach the original goal from the new one. Take playing frisbee for instance. When an agent jumps to catch the frisbee and yet misses it, the agent receives no positive feedback under the sparse reward setting. Using HER, the agent could set the end of its episode as the new goal -- position of the frisbee; with DHER, however, the agent finds a trajectory from its past experiences as the imagined path of the frisbee, and thereby extrapolates towards the original goal.
In particular, we do the following for DHER. To finish the tasks with dynamic goals needs to explore experience and understand multiple goals. DHER uses replay buffers to allow the agent to learn from a couple of failures by assembling new `experience' from different episodes. The proposed method retrieves memories to find the connection between the experience of different episodes. It largely improves the sample efficiency in dynamic goal task settings. More importantly, this strategy makes it possible to learn in the setting that both sparse rewards and dynamic goals exist.
We evaluate our method along with the state-of-the-art baselines on new environments and manipulation tasks, which have sparse rewards and dynamic goals. Our results demonstrate that DHER is clearly better than others for these tasks. We also transfer policies trained in our simulation based on DHER to a physical robot and show that DHER can be applied to solving real-world robotics problems.
We summarize our main contributions as follows: (1) We demonstrate that, both in simulation and real worlds, DHER succeeds in continuous control with a moving target. To our knowledge, this is the first empirical result on manipulation tasks that demonstrates model-free learning methods can tackle tasks of this complexity. (2) We show that with assembling new experience from two failures, the sample complexity can be reduced dramatically. We attribute this to global knowledge learning in a set of failed experience which breaks the constraint of local one-episode experience towards more robust strategies. (3) We design and implement a set of new environments and continuous tasks with dynamic goals, which would be of interest to researchers at the intersection of robotics and reinforcement learning.
2 RELATED WORK
Recent works in deep RL have shown impressive results in different domains, such as games (Bellemare et al., 2013; Silver et al., 2016), simulated control tasks (Brockman et al., 2016) and so on. There have been several proposed RL methods for playing Atari games, including DQN (Mnih et al., 2015), UVFA (Schaul et al., 2015b) and so on. UVFA trains a single neural network approximating multiple value functions for state and goal. For the continuous control, DDPG (Lillicrap et al., 2015) is a popular actor-critic algorithm that has shown impressive results in continuous control tasks. Unfortunately, these methods do not consider tasks with dynamic goals. Our work addresses this issue and can be combined with them.
2

Under review as a conference paper at ICLR 2019
Curriculum learning is also used for reinforcement learning scenarios. The idea is that solving easier problems first has advantages to learn more complex goals later and thus that learning can be optimized by presenting the problems in an optimal order, a curriculum (Bengio et al., 2009). Narvekar et al. (2017) and Florensa et al. (2018) proposed methods to automatically produce subtasks or subgoals for a given target. Narvekar et al. (2017) produce subtasks according to predefined tasks of a given domain problem. Florensa et al. (2018) use a Generative Adversarial Network (GAN) to produce goals with different difficulties. Different from these methods, our approach produces a series of goals for an episode from failed experience.
Experience replay is an important technique and introduced to break temporal correlations by mixing more and less recent experience for updating policies (Lin, 1992). It was demonstrated for its efficiency in DQN (Mnih et al., 2015). Prioritized experience replay improves the speed of training by considering prioritizing transitions in the replay buffer (Schaul et al., 2015b). HER considers modifying experience in the replay buffer for continuous control (Andrychowicz et al., 2017). In contrary, our approach assembles successful experience from two failures. Comparing with these methods, which do not consider dynamic goals, our approach uses a series of goals to assemble successful experience. As a result, our method is able to accomplish the tasks with sparse rewards and dynamic goals.
3 METHODOLOGY
We first review how HER works Andrychowicz et al. (2017), followed by details of the proposed DHER for dealing with dynamic goals.
HER is a simple and effective method of manipulating the replay buffer used in off-policy RL algorithms that allows it to learn policies more efficiently from sparse rewards. It assumes the goal being pursued does not influence the environment dynamics. After experiencing an episode {s0, s1, · · · , sT }, every transition st  st+1 along with the goal for this episode is usually stored in the replay buffer. Some of the saved episodes fail to reach the goal, providing no positive feedback to the agent. However, with HER, the failed experience is modified and also stored in the replay buffer in the following manner. The idea is to replace the original goal with a state visited by the failed episode. As the reward function remains unchanged, this change of goals hints the agent how to achieve the new goal in the environment. HER assumes that the mechanism of reaching the new goal helps the agent learn for the original goal.
3.1 DYNAMIC GOALS
In many tasks especially robotic applications, a goal is not static and changes at every timestep. The goal is part of the environment; its underlying moving law is unknown to the agent. We assume that a dynamic goal gt  G moves by following some law gt = g(t; ), where  parameterizes the law (e.g., acceleration in Newton's law of motion), and that the goal does not influence the environment dynamics, which is the same to Andrychowicz et al. (2017). An agent acts on an object to interact with the environment. For simplicity, consider a robotic arm that pushes a ball into a moving container. The success of a task is defined as f (st, gt) = 1condition( sotbj - gt  ), where sotbj is the position of the object in the state st and denotes a tolerance by the environment. 1condition is an indicator function. The agent aims to achieve any state st that satisfies f (st, gt) = 1. It receives a sparse reward rt := r(st, at, gt+1) = -1condition(f (st+1, gt+1) = 1) upon making an action at.
It is interesting to compare the above with the static goals. Expressing a static goal in the following way, gtstatic = gstatic, t, highlights the key challenge of dealing with the dynamic goal. Namely, the agent has no access to the underlying law of the dynamic goal in our setting, whereas the law of being static is known to the agent in Andrychowicz et al. (2017). In other words, the agent has no clue at all how to construct a new dynamic goal that is admissible by the environment.
3.2 DYNAMIC HINDSIGHT EXPERIENCE REPLAY
At the first glance, we shall compose a new dynamic goal gdynamic = {sto0bj, sot1bj, · · · , stoTbj } from a failed episode s0, s1, · · · , sT in order to apply HER (Andrychowicz et al., 2017) to our problem
3

Under review as a conference paper at ICLR 2019
Algorithm 1 Dynamic Hindsight Experience Replay with Experience Assembling
Require: an off-policy RL algorithm A, replay buffer R, a reward function r 1: Initialize A and replay buffer R 2: for episode = 1, 2, · · · , M do 3: Sample an initial goal g0 and an initial state s0 4: for t = 0, · · · , T - 1 do 5: Sample an action at using the behavioral policy from A: 6: at  (st|gt) 7: Execute the action at and observe a new state st+1 and a new goal gt+1 8: end for 9: for t = 0, · · · , T - 1 do 10: rt := r(st, at, gt+1) 11: Store the transitions (st|gt, at, rt, st+1|gt+1) in R (Standard experience replay) 12: end for 13: Collect failed episodes to E 14: for Ei  E do 15: Search another Ej(i = j)  E where gia,cp = gjd,eq 16: if Ej = Ø then 17: Clone a goal trajectory {g0, · · · , gm}m=min{p,q} in which gt = gjd,eq-m+t from Ej 18: for t = {0, · · · , m - 1} do 19: rt := r(si,p-m+t, ai,p-m+t, gt+1) 20: Store the transition (si,p-m+t|gt, ai,p-m+t, rt, si,p-m+t+1|gt+1) in R (DHER) 21: end for 22: end if 23: end for 24: for t = 1, · · · , N do 25: Sample a minibatch B from the replay buffer R 26: Optimize A using the minibatch B 27: end for 28: end for
setting. However, per the discussion above, this new dynamic goal gdynamic may be inadmissible by the environment, leading to no positive feedback to the agent at all.
We tackle the challenge by drawing the following two observations. One is that many episodes fail in the replay buffer, implying that the agent can actually build a new goal upon more than one episodes. The other is that the more failed experience the agent has, the more possible for the agent to use the connection between achieved goals in an episode and desired goals in some other episode.
After experiencing some episode s0, s1, · · · , sT , we store in the replay buffer every transition st  st+1 for this episode, defined as (st, at, rt, st+1), where st indicates a state st at timestep t, and at indicates an action and rt indicates a reward. Thus before t, there are a series of records {(s0, a0, r0, s1) · · · , (st, at, rt, st+1)}. A state consists of three parts: observation ot, desired goal gtde and achieved goal gtac, define as st = ot, gtac, gtde , where normally gtde = gt.
In the dynamic situation, HER fails in tasks with moving goals. For a failed experience, HER samples a trajectory and simply copies the achieved goal to the desired one, i.e., using st = ot, gtac, gde , where gde = gtac (t  t), instead. Then it can sample a lot of states in trajectories, create a lot of successful experiences, and get rewards. However, if the goal is dynamic, the construction of the new goal trajectory does not only involve the final position of the desired goal trajectory but also the previous positions. It is obvious that it is not enough to use goal gde as desired goals for every state st. It makes the agent miss the information of moving pattern of the goals, such as velocity and direction. Motivated by this issue, we consider manipulating the desired goals for a series of states in a trajectory. In this way, we can construct successful trajectories for the task.
We reuse the failed experience from the replay buffer with inverse simulation to create successful rewards for the agent. Our inverse simulation contains two main steps: First, given a failed episode, for its achieved goal trajectory, we try to find a desired goal trajectory from other episodes that could match it. Second, we assemble a new episode by matching the achieved goal trajectory of the given
4

Under review as a conference paper at ICLR 2019

Dy-Reaching

Dy-Circling

Dy-Pushing

Dy-Snake

Figure 2: The proposed tasks with dynamic goals (red objects). Arrow indicates the movement of a goal. The first row indicates initial states. The second row indicates final states.

episode to the desired goal trajectory of the founded one. Let gia,cq indicate the achieved goal of the agent at timestep q in episode i and gjd,ep indicate the desired goal at timestep p in episode j. Given a set of failed experience {E0, E1, E2, · · · }, we search and draw two failed episodes Ei and Ej (i = j), where i, j, p, q, s.t. gia,cp = gjd,eq. If we find such two failed episodes, we combine the two experience by replacing the desired goals in Ei by {gjd,et} (t  min{p, q}) that borrows from the Ej. Based on this, we end up assembling a new experience Ei based on Ei with a new "imagined" goal trajectory {gjd,e0, · · · , gjd,et} where t  min{p, q}.
More details of the proposed method are shown in Algorithm 1. Unlike HER, our DHER needs to search all failed experiences to compose a "imagined" goal trajectory. Hence, the efficiency of searching the memory is important. In our implementation, we use two hash tables to store the trajectories of achieved goals and desired goals, respectively.
4 EXPERIMENT
We run extensive experiments to examine the proposed DHER for moving goals and compare it with some competing baselines. We first introduce the environments and tasks that we want to address, followed by the experimental results. Demo videos from our experiments are available at https://sites.google.com/view/dher. To ensure reproducibility and advance further innovations, we will make our environments, code and models publicly available upon publication.
4.1 ENVIRONMENTS
Whereas grasping moving targets are fairly common in robotic applications, there are rarely existing environments featuring dynamic goals. We modify the robotic manipulation environments created by OpenAI (Brockman et al., 2016) for our experiments. As shown in Figure 2, we assign certain rules to the goals so that they accordingly move in the environments while an agent is required to control the robotic arm's grippers to reach the goal that moves along a straight line (Dy-Reaching), to reach the goal that moves in a circle (Dy-Circling), or to push a block to the goal that moves along a straight line (Dy-Pushing). In addition, we also develop a new GREEDY SNAKE environment (DySnake), in which the goal moves from one discrete cell to another. The greedy snake aims to reach the goal (and eat it). No matter what actions are taken by the agent, the underlying rule of changing the goals' positions remain the same. We assume that the law of the goal's motion is unknown to the agent. More details of the tasks are described in the appendix.
For the first three tasks, we follow the basic settings of OpenAI robotics environments (Brockman et al., 2016). States are read from the MuJoCo physics engine. An observation consists of relative
5

Under review as a conference paper at ICLR 2019

Median Success Rate Median Success Rate
Median Success Rate

0.8 0.6 0.4 0.2 0.0
0

Dy-Reaching
DDPG DDPG-DHER DDPG-HER
10 20Epoc3h0 40 50

1.0 0.8 0.6 0.4 0.2 0.0
0

Dy-Circling
DDPG DDPG-DHER DDPG-HER
10 20Epoc3h0 40 50

Dy-Pushing 0.6 DDPG
DDPG + DHER DDPG + HER
0.4
0.2
0.0 0 10 20 30 40 50 Epoch

(a) Dy-Reaching with a straight- (b) Dy-Circling with a goal that (c) Dy-Pushing with a straight-line

line goal.

moves in a circle.

goal.

Figure 3: Results on different environments.

positions of the object and the target (grippers are blocked) Goals are positions in the 3D world coordinate system with a fixed tolerance (we use = 0.01 for the tolerance). Note the goals are able to move in our tasks. The velocity we use is v = (0.011, 0, 0). The start positions of the goals are randomly chosen. Rewards are binary and sparse: r(st, at, gt) = -1condition(|sot+bj1 - gt+1|  ) where st+1 and gt+1 are respectively the state and dynamic goal after the execution of the action at in the state st. We use 3-dimensional actions, in which three dimensions correspond to the desired relative gripper position at the next timestep.
For the last task, it is very similar to the standard snake game. There is a 2D plane grid whose size is 30 × 40. A snake and a goal (food) both move in this grid. We use a 1 × 1 square as the body of the snake and do not allow the snake to grow any longer as it moves. The states of the system are represented by using the positions of the snake and food. Goals are the positions of the food. The snake has to move itself such that it resides in the same cell as the food at a certain timestep. The velocity of the goal is set to (0, 1). The start positions of the goal are set randomly in different episodes of the game, so are the start positions of the snake. Rewards are binary and sparse: r(st, at, gt+1) = -1condition(|sstn+a1ke-gt+1| = 0) where st+1 and gt+1 are the environment's state and goal after the agent executes action at in the state st. Observations are represented by the positions of the snake and food. We also add to the state the distance between the snake and food. Actions allowed in the game are the following: move up, move down, move left, and move right, for one cell per timestep.
4.2 BASELINES
For the first three tasks which call for continuous control, we consider two competing baselines:1
· DDPG, which is a model-free RL algorithm for continuous control Lillicrap et al. (2015). It learns a deterministic policy by using a stochastic counterpart to explore in the training.
· DDPG + HER, which improves the replay buffer of DDPG by the hindsight experience replay (Andrychowicz et al., 2017).
For the last task of discrete control, we use the following baselines in the experiments: DQN (Mnih et al., 2015) and DQN + HER, which uses HER to enhance the replay in DQN.
· DQN, which is a powerful model-free RL algorithm for discrete action spaces Mnih et al. (2015).
· DQN + HER, which uses HER to enhance the replay in DQN.
4.3 COMPARISON RESULTS ON THE ROBOTIC ENVIRONMENTS
For the continuous control, we present three sets of comparison results in Figure 3 for the first three tasks, respectively. Consistently, the results show our DHER algorithm outperforms the others. The
1https://github.com/openai/baselines
6

Under review as a conference paper at ICLR 2019

Median Success Rate

1.0 0.8 0.6 0.4 0.2 0.0
0

Dy-Reaching
DDPG DDPG-DHER DDPG-HER
10 20Epoc3h0 40 50

Figure 4: Low velocity: (0.001, 0, 0).

Median Success Rate

1.0 Dy-Snake

0.8

0.6

DQN DQN + DHER

0.4 DQN + HER

0.2

0 200 40E0piso6d0e0 800 1000

Figure 6: Snake with dynamic goals.

Median Success Rate

0.8 0.6 0.4 0.2 0.0
0

Dy-Reaching
DDPG DDPG-DHER DDPG-HER
10 20Epoc3h0 40 50

Figure 5: High velocity: (0.016, 0, 0).

Median Success Rate

1.0
0.8
0.6
0.4
0.2 0

Dy-Snake
DQN DQN + DHER DQN + HER+
200 40E0piso6d0e0 800 1000

Figure 7: Snake with dynamic goals. The law of the motion of food is known to agents.

two baselines are not able to catch up even after we train them for thousands of iterations. Vanilla DDPG is slightly better than the version with HER. HER does not benefit DDPG in these tasks because the goals in HER are fixed, fundamentally misleading the agent in the attempt of solving the tasks with dynamic goals.
Comparing Figure 3a and Figure 3b with Figure 3c, we find that DHER learns faster in Dy-Reaching and Dy-Circling than Dy-Pushing probably because Dy-Reaching and Dy-Circling are easier tasks than Dy-Pushing. In Dy-Pushing, all the algorithms take a fairly big amount of time to explore without receiving any positive feedback. However, the more failed experiences the agent encounters, the better change our algorithm is able to identify relevant episodes from them for assembling useful dynamic goals. As a result, DHER is able to pick up the momentum and learns faster and better than the baselines after a certain point. In Figure 3a, the performance decreases a little. The reason may be that as successful experience increases, some assembled experience is inconsistent with these successful experience.
4.3.1 COMPARISON USING DIFFERENT VELOCITIES OF GOALS
To show the performance of our method on more complex tasks with different velocities, we study different methods in Dy-Reaching environment as shown in Figures 4 and 5 with the same physical properties as the previous experiments. Overall the results show our method is much better than DDPG and HER. As a reminder, the threshold for calculating rewards we used is 0.01. In Figure 4, the task becomes an easier task because of the slower velocity. It shows that our method quickly achieves to a good result around 5 epoch. Comparing with Figure 3a, it shows that the performance with v = (0.001, 0, 0) is better than the performance with v = (0.011, 0, 0) and get 15% improvements. However, it also shows the performance with v = (0.016, 0, 0) is worse than the performance with v = (0.011, 0, 0). Both DDPG and DDPG+HER failed when v = (0.016, 0, 0) and their performance is 0. This performance is consistent because the task becomes more difficult when the velocity increases.
7

Under review as a conference paper at ICLR 2019

Sim: Dy-Circling

Transfer Real: Dy-Circling

Sim: Pouring

Transfer

Real: Pouring

Figure 8: Adapting the policies trained based on DHER from our simulation to a real robotic arm.

4.4 COMPARISON ON THE DYNAMIC SNAKE ENVIRONMENT
In the Dy-Snake environment, which is a discrete control environment, we present the results of the chasing food task in Figure 6. The results show that the proposed algorithm works best. DQN is better than HER. HER fails for this task. That HER fails for this task shows just using achieved goals is not enough for the tasks with dynamic goals and can lead wrong direction. The results also show that around 800 episodes, the performance of DQN and DHER is closed and DHER is slightly better than DQN. This is because the chasing food task is a simple task. The action space is very small and just 4 types of actions. After enough exploration, DQN also has competitive performance. However, at the beginning, DHER quickly achieve very good performance. It shows that assembling experience from two failures improves the performance very efficiently in this task.
4.5 SIM TO REAL ADAPTATION
We used policies for Dy-Circling task and a new Pouring task trained in our simulator 2 to deploy them on a physical robot. As shown in Figure 8, the policies were trained by using DHER and adapted to the real robot without any finetuning. For Dy-Circling task, the robot's gripper was blocked. There were a toy turntable, whose speed is unknown, and a blue block on the turntable. We set the position 1cm above the block as the target position. The position of the block was predicted based on traditional contour shape analysis using camera images. For Pouring task, the robot gripped a can. A man held a cup and moved it. The cup was set as the target and with a green marker. We used the marker to estimate its position.
Our policies were transferred successfully for both tasks. It was observed that the robot had learned to not only follow the current target but also step forward to the future target position. Demo videos about the experiments are available at https://sites.google.com/view/dher.
4.6 SPECIAL CASE: HOW ABOUT IF HER KNOWS THE LAW OF THE MOTION OF A GOAL
We use Dy-Snake to demonstrate the experimental results for the special case that the law of the motion of the target (food) is known to agents. Because the law of the motion of the food is very simple and controllable in Dy-Snake environment. We develop a direct extension of HER, called HER+, that modifies desire goals at every timestep based on the law of the motion of the food to create successful experience. More details of HER+ are described in the appendix.
We show the results in Figure 7. DHER and HER+ are both better than DQN at the beginning. DHER is slightly better than HER+, which shows the efficiency of DHER is comparable in this simple task.
5 CONCLUSION
We introduced a novel technique that assembles successful experience from a couple of failures. With this technique, our proposed algorithm called DHER ( Dynamic Hindsight Experience Replay) is able to address the tasks with sparse rewards and dynamic goals. Our technique can be combined with an arbitrary off-policy RL algorithm and we experimentally demonstrated that with DQN and DDPG. As far as we know, it is the first time that an agent is allowed to learn from assembled experience from two failures.
2We developed a new simulator according to our hardware. More details on our hardware setup are available in the appendix.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI. Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048­5058, 2017.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253­279, 2013. ISSN 1076-9757.
Yoshua Bengio, Jrme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41­48. ACM, 2009.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In Proceedings of the 33rd International Conference on Machine Learning, pp. 1329­1338, 2016.
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. In Proceedings of Machine Learning Research, pp. 1514­1523, 2018.
Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin A. Riedmiller, and David Silver. Emergence of locomotion behaviours in rich environments. CoRR, abs/1707.02286, 2017. URL http: //arxiv.org/abs/1707.02286.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293­321, 1992.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529, February 2015. URL http://dx.doi.org/10.1038/nature14236.
Sanmit Narvekar, Jivko Sinapov, and Peter Stone. Autonomous task sequencing for customized curriculum design in reinforcement learning. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, volume 147, pp. 149, 2017.
Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the 16th International Conference on Machine Learning, volume 99, pp. 278­287, 1999.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pp. 1312­1320, 2015a.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015b.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015. URL http://arxiv.org/abs/1502. 05477.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347.
9

Under review as a conference paper at ICLR 2019 David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Re´mi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. CoRR, abs/1611.01224, 2016. URL http://arxiv.org/abs/1611.01224.
10

Under review as a conference paper at ICLR 2019
APPENDIX
TASKS WITH DYNAMIC GOALS We have created four tasks as described below and illustrated by Figure 2. The first three tasks are based on the robotic environments. The last is based on the GREEDY SNAKE environment.
· Dy-Reaching: The task is to control the robotic arm so that its gripper can reach the target position. The target position moves from one point to another along a straight line with a constant velocity.
· Dy-Circling: The task is to control the robotic arm so that its gripper can reach the target position. The target position moves along a circle with a constant velocity.
· Dy-Pushing: in this task, a box is placed on a table in front of the robotic arm. The robot is required to move the box to the target location on the table. The target location moves from one position to another with a fixed velocity. Note that the robot's grippers are locked to prevent it from grasping. The learned behavior is actually a mixture of pushing and rolling.
· Dy-Snake: in this task, a snake and food are placed on a rectangle map and the task is to control the snake to eat the food. The food is moving from one position to another position with a fixed velocity.
There is an additional task we also tested. It is demonstrated in our demo videos. · Dy-Sliding: in this task, a puck is placed on a long slippery table and the target position is out of the robot's reach. The robot has to hit the puck with such a force that the puck slides and then stops in the appropriate target place due to friction. The target place is dynamic, moving from one position to another position with a fixed velocity.
Dy-Sliding is illustrated in Figure 9.
Dy-Sliding
Figure 9: Sliding with dynamic goals.
HARDWARE We use the Universal Robots UR10 with a gripper. We use a RealSense Camera SR300 to track the position of objects. The gripper is blocked. We use a marker on the gripper for camera calibration. During adapting the robot, for the position of the target, we use the camera to estimate it.
SIMULATION We simulate the physical system using the MuJoCo physics engine (Todorov et al., 2012) and also use MuJoCo to render the images. In our tasks, we use positions provided by MuJoCo for training policies in the simulation. In Figure 10, we demonstrate our simulation and the physics environment.
11

Under review as a conference paper at ICLR 2019

Simulation

Physics environment

Figure 10: Our simulation and the physics environment.

A SPECIAL AND SIMPLER CASE - HER+
The paper considers a general situation that the law of the motion of goals is invisible to an agent. However, we relax this assumption and assume that in some situations the law of the motion of goals is easy to obtain, i.e., the parameters of g(t; ) are known. For example, it is able to calculate the velocity of an object by observing the trajectories of the object.
In this situation, we do not need to search failed experience. With the knowledge of the velocity of an object, it is straightforward that we can calculate any trajectory of desired goals. Thus for every failed experience, at any time step t, based on the achieved goal gtac, we calculate new desired goals gtde = g(t) corresponding to the achieved goals in order to construct successful experience.
The details of HER+ are described in Algorithm 2. HER+ can be seen as a direct extension of HER. HER and HER+ can modify any failed experience to successful experience because they both assume the law of the motion of goals is known.
Algorithm 2 Hindsight Experience Replay Plus
Require: an off-policy RL algorithm A, replay buffer R, a reward function r 1: Initialize A and replay buffer R 2: for episode = 1, 2, · · · , M do 3: Sample an initial goal g0 and an initial state s0 4: for t = 0, · · · , T - 1 do 5: Sample an action at using the behavioral policy from A: 6: at  (st|gt) 7: Execute the action at and observe a new state st+1 and a new goal gt+1 8: end for 9: for t = 0, · · · , T - 1 do 10: rt := r(st, at, gt+1) 11: Store the transitions (st|gt, at, rt, st+1|gt+1) in R (Standard experience replay) 12: Sample a set of the achieved goals of Ei as additional goals for reply G 13: for gp  G do 14: Calculate a goal trajectory {g0, · · · , gp} where gt = g(t; ) (HER+) 15: for t = {0, · · · , p - 1} do 16: rt := r(si,t, ai,t, gt+1) 17: Store the transition (si,t|gt, ai,t, rt, si,t+1|gt+1) in R 18: end for 19: end for 20: end for 21: end for

12

