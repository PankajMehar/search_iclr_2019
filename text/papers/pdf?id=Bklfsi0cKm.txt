Under review as a conference paper at ICLR 2019
DEEP CONVOLUTIONAL NETWORKS AS SHALLOW GAUSSIAN PROCESSES
Anonymous authors Paper under double-blind review
ABSTRACT
We show that the output of a (residual) convolutional neural network (CNN) with an appropriate prior over the weights and biases is a Gaussian process (GP) in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike "deep kernels", has very few parameters: only the hyperparameters of the original CNN. Further, we show that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for a pair of images is similar to a single forward pass through the original CNN with only one filter per layer. The kernel equivalent to a 32-layer ResNet obtains 0.84% classification error on MNIST, a new record for GPs with a comparable number of parameters. 1
1 INTRODUCTION
Convolutional Neural Networks (CNNs) have powerful pattern-recognition capabilities that have recently given dramatic improvements in important tasks such as image classification (Krizhevsky et al., 2012). However, as CNNs are increasingly being applied in real-world, safety-critical domains, their vulnerability to adversarial examples (Szegedy et al., 2013; Kurakin et al., 2016), and their poor uncertainty estimates are becoming increasingly problematic. Bayesian inference is a theoretically principled and demonstrably successful (Snoek et al., 2012; Deisenroth & Rasmussen, 2011) framework for learning in the face of uncertainty, which may also help to address the problems of adversarial examples (Gal & Smith, 2018). Unfortunately, Bayesian inference in CNNs is extremely difficult due to the very large number of parameters, requiring highly approximate factorised variational approximations (Blundell et al., 2015; Gal & Ghahramani, 2015), or requiring the storage (Lakshminarayanan et al., 2017) of large numbers of posterior samples (Welling & Teh, 2011; Mandt et al., 2017).
Other methods such as Gaussian Processes (GPs) are more amenable to Bayesian inference, allowing us to compute the posterior uncertainty exactly (Rasmussen & Williams, 2006). This raises the question of whether it might be possible to combine the pattern-recognition capabilities of CNNs with exact probabilistic computations in GPs. Two such approaches exist in the literature. First, deep convolutional kernels (Wilson et al., 2016) parameterise a GP kernel using the weights and biases of a CNN, which is used to embed the input images into some latent space before computing their similarity. The CNN parameters of the resulting kernel then have to be optimised by gradient descent. However, the large number of kernel parameters in the CNN reintroduces the risk of overconfidence and overfitting. To avoid this risk, we need to infer a posterior over the CNN kernel parameters, which is as difficult as directly inferring a posterior over the parameters of the original CNN. Second, it is possible to define a convolutional GP (van der Wilk et al., 2017) or a deep convolutional GP (Kumar et al., 2018) by defining a GP that takes an image patch as input, and using that GP as a component in a larger CNN-like system. However, inference in such systems is very computationally expensive, at least without the use of potentially severe variational approximations (van der Wilk et al., 2017).
An alternative approach is suggested by the underlying connection between Bayesian neural networks (NNs) and GPs. In particular, Neal (1996) showed that the function defined by a single-layer fully-connected NN with infinitely many hidden units, and random independent zero-mean weights
1Code to replicate this paper is available at https://github.com/convnets-as-gps/convnets-as-gps
1

Under review as a conference paper at ICLR 2019

and biases is equivalent to a GP, implying that we can do exact Bayesian inference in such a NN by working with the equivalent GP. Recently, this result was extended to arbitrarily deep fullyconnected NNs with infinitely many hidden units at each layer (Lee et al., 2017; Matthews et al., 2018a). However, these fully-connected networks are rarely used in practice, as they are unable to exploit important properties of images such as translational invariance, raising the question of whether state-of-the-art architectures such as CNNs (LeCun et al., 1990) and ResNets (He et al., 2016a) have equivalent GP representations. Here, we answer in the affirmative, giving the GP kernel corresponding to arbitrarily deep CNNs and to (convolutional) residual neural networks (He et al., 2016a). In this case, if each hidden layer has an infinite number of convolutional filters, the network prior is equivalent to a GP.
Furthermore, we show that two properties of the GP kernel induced by a CNN allow it to be computed very efficiently. First, in previous work it was necessary to compute the covariance matrix for the output of a single convolutional filter applied at all possible locations within a single image (van der Wilk et al., 2017), which was prohibitively computationally expensive. In contrast, under our prior, the downstream weights are independent with zero-mean, which decorrelates the contribution from each location, and implies that it is necessary only to track the patch variances, and not their covariances. Second, while it is still necessary to compute the variance of the output of a convolutional filter applied at all locations within the image, the specific structure of the kernel induced by the CNN means that the variance at every location can be computed simultaneously and efficiently as a convolution.
Finally, we empirically demonstrate the performance increase coming from adding translationinvariant structure to the GP prior. Without computing any gradients, and without augmenting the training set (e.g. using translations), we obtain 0.84% error rate on the MNIST classification benchmark, setting a new record for nonparametric GP-based methods.

2 GP BEHAVIOUR IN A CNN

For clarity of exposition, we will treat the case of a 2D convolutional NN. The result applies straightforwardly to nD convolutions, dilated convolutions and upconvolutions ("deconvolutions"), since they can be represented as linear transformations with tied coefficients (see figure 1).

2.1 A 2D CONVOLUTIONAL NETWORK PRIOR

The network takes an arbitrary input image X of height H(0) and width D(0), as a C(0)×(H(0)D(0))
real matrix. Each row, which we denote x1, x2, . . . , xC(0) , corresponds to a channel of the image (e.g. C(0) = 3 for RGB), flattened to form a vector. The first activations A(1)(X) are a linear
transformation of the inputs. For j  {1, . . . , C(1)}:

C (0)

a(j1)(X) := b(j0) +

Wj(,0i)xi .

i=1

(1)

We consider a network with L hidden layers. The other activations of the network, from A(2)(X) up to A(L+1)(X), are defined recursively:

C( )

a(j +1)(X) := bj( ) +

Wj(,i) ai( )(X) .

i=1

(2)

The activations A( )(X) are C( ) × (H( )D( )) matrices. Each row aj( ) represents the flattened jth channel of the image that results from applying a convolutional filter to (A( -1)(X)).
The structure of the pseudo-weight matrices Wj(,i) and biases b(j ), for i  {1, . . . , C( )} and j  {1, . . . , C( +1)}, depends on the architecture. For a convolutional layer, each row of Wj(,i) represents a position of the filter, such that the dot product of all the rows with the image vector xi represents applying the convolutional filter Uj(,i) to the ith channel. Thus, the elements of each row

2

Under review as a conference paper at ICLR 2019

Input image's ith channel: Filter U(j0,i):

Resulting convolution

gth row gth conv. patch

=

Wj(,0i)

xi

Figure 1: The 2D convolution U(j0,i)  xi as the dot product Wj(,0i)xi. The blank elements of Wj(,0i) are zeros. The gth row of Wj(,0i) corresponds to applying the filter to the gth convolutional patch of the channel xi.

of Wj(,i) are: 0 where the filter does not apply and the corresponding element of Uj(,i) where it does, as illustrated in figure 1.
The outputs of the network are the last activations, A(L+1)(X). In the classification or regression setting, the outputs are not spatially extended, so we have H(L+1) = D(L+1) = 1, which is equivalent to a fully-connected output layer. In this case, the pseudo-weights Wj(,Li) only have one row, and the activations aj(L+1) are single-element vectors.
Finally, we define the prior distribution over functions by making the filters Uj(,i) and biases bj( ) be independent Gaussian random variables (RVs). For each layer , channels j, i and locations within the filter x, y:

Uj(,i),x,y  N 0, w2 /C( ) ,

b(j )  N 0, b2 .

(3)

Note that, to keep the activation variance constant, the weight variance is divided by the number of input channels. The weight variance can also be divided by the number of elements of the filter, which makes it equivalent to the NN weight initialisation scheme introduced by He et al. (2016a).

2.2 ARGUMENT FOR GP BEHAVIOUR

We follow the proofs by Lee et al. (2017) and Matthews et al. (2018a) to show that the output of the CNN described in the previous section, A(L+1), defines a GP indexed by the inputs, X. Their proof (Lee et al., 2017) proceeds by applying the multivariate Central Limit Theorem (CLT) to each layer in sequence, i.e. taking the limit as N (1)  , then N (2)   etc, where N ( ) is the number of hidden units in layer . By analogy, we sequentially apply the multivariate CLT by taking the limit as the number of channels goes to infinity, i.e. C(1)  , then C(2)   etc. While this is the simplest approach to taking the limits, other potentially more realistic approaches also exist (Matthews et al., 2018a).

The fundamental quantity we consider is a vector formed by concatenating the feature maps (or equivalently channels), aj( )(X) and a(j )(X ) from data points X and X ,

aj( )(X, X ) =

a(j )(X) aj( )(X )

.

(4)

This quantity (and the following arguments) can all be extended to the case of countably finite numbers of input points.

3

Under review as a conference paper at ICLR 2019

Induction base case. For any pair of data points, X and X the feature-maps corresponding to the jth channel, aj(1)(X, X ) have a multivariate Gaussian joint distribution. This is because each element is a linear combination of shared Gaussian random variables: the biases, b(j0) and the filters, U(j0,:). Following equation (1),

C (0)
a(j1)(X, X ) = b(j0)1 +
i=1

Wj(,0i) 0

0 Wj(,0i)

xi xi

,

(5)

where 1 is a vector of all-ones. While the feature maps themselves, a(j1)(X, X ), display strong correlations, different feature maps are independent and identically distributed (iid) conditioned on
X and X (i.e. aj(1)(X, X ) and aj(1)(X, X ) are iid for j = j ), because the parameters for different
feature-maps (i.e. the biases, bj(0) and the filters, U(j0,:)) are themselves iid.

Induction step. Consider the feature maps at the th layer, a(i )(X, X ), to be iid multivariate Gaussian RVs (i.e. for i = i , a(i )(X, X ) and a(i )(X, X ) are iid). Our goal is to show that, taking the number of channels at layer to infinity (i.e. C( )  ), the same properties hold at the next
layer (i.e. all feature maps, a(j +1)(X, X ), are iid multivariate Gaussian RVs). Writing eq. (2) for
two training examples, X and X , we obtain,

C( )
a(j +1)(X, X ) = bj( )1 +
i=1

Wj(,i) 0

0 Wj(,i)

(a(i )(X, X ))

(6)

We begin by showing that a(j +1)(X, X ) is a multivariate Gaussian RV. The first term is multivariate Gaussian, as it is a linear function of b(j ), which is itself iid Gaussian. We can apply the multivariate CLT to show that the second term is also Gaussian, because, in the limit as C( )  , it is the sum of infinitely many iid terms: ai( )(X, X ) are iid by assumption, and Wj(,i) are iid by definition. Note that the same argument applies to all feature maps jointly, so all elements of A( )(X, X ) (defined

by analogy with eq. 4) are jointly multivariate Gaussian.

Following Lee et al. (2017), to complete the argument, we need to show that the output feature maps are iid, i.e. aj( +1)(X, X ) and aj( +1)(X, X ) are iid for j = j . They are identically distributed, as b(j ) and Wj(,i) are iid and (a(i )(X, X )) is shared. To show that they are independent, remember that aj( +1)(X, X ) and aj( +1)(X, X ) are jointly Gaussian, so it is sufficient to show that they are uncorrelated, and we can show that they are uncorrelated because the weights, Wj(,i) are independent with zero-mean, eliminating any correlations that might arise through the shared RV, (ai( )(X, X )). See SI for a discussion of technical issues surrounding limits.

3 THE CONVNET AND RESNET KERNELS

Here we derive a computationally efficient kernel corresponding to the CNN described in the previous section. It is surprising that we can compute the kernel efficiently because the feature maps, aj( )(X), display rich covariance structure due to the shared convolutional filter. Computing and representing these covariances would be prohibitively computationally expensive. However, in many cases we only need the variance of the output, e.g. in the case of classification or regression with a final dense layer. It turns out that this propagates backwards through the convolutional network, implying that for every layer, we only need the "diagonal covariance" of the activations: the covariance
between the corresponding elements of a(j )(X) and a(j )(X ) (i.e. diag C aj( )(X), a(j )(X ) ).

3.1 GP MEAN AND COVARIANCE
A GP is completely specified by its mean and covariance (kernel) functions. These give the parameters of the joint Gaussian distribution of the RVs indexed by any two inputs, X and X . For

4

Under review as a conference paper at ICLR 2019

the purposes of computing the mean and covariance, it is easiest to consider the network as being written entirely in index notation,

C( ) H( )D( )

A(j,g+1)(X) = bj( ) +

Wj(,i),g,h (Ai(,h)(X)).

i=1 h=1

where and + 1 denote the input and output layers respectively, i and j  {1, . . . , C( +1)} denote the input and output channels, and h and g  {1, . . . , H( +1)D( +1)} denote the location within the input and output channel or feature-maps.

The mean function is thus easy to compute

C( ) H( )D( )

E A(j,g+1)(X) = E b(j ) +

E Wj(,i),g,h (Ai(,h)(X)) = 0.

i=1 h=1

as bj( ) and Wj(,i),g,h have zero mean, and Wj(,i),g,h are independent of the activations at the previous layer, (A(i,h)(X)).
Now we show that it is possible to efficiently compute the covariance function. This is surprising because for many networks, we need to compute the covariance of activations between all locations in the feature map (i.e. C A(j,g)(X), Aj(,g) (X ) ) and this object is extremely high-dimensional,
N 2(H( )D( ))2. However, it turns out that we only need to consider the covariance across different data points, X and X at the same location within the feature map, g, which is a more manageable quantity of size N 2(H( )D( )).

This is true at the output layer (L + 1): in order to achieve an output suitable for classification or regression, we use only a single output location H(L+1) = D(L+1) = 1, so it is only possible to compute the covariance at that single location. We now show that, if we only need the covariance at corresponding locations in the outputs, we only need the covariance at corresponding locations in the inputs, and this requirement propagates backwards through the network.

Formally, as the activations are composed of a sum of terms, their covariance is the sum of the covariances of all those underlying terms,

C Aj(,g+1)(X), A(j,g+1)(X ) = V b(j ) +

C( ) C( ) H( )D( ) H( )D( )

i=1 i =1 h=1

C
h =1

Wj(,i),g,h (A(i,h) (X)),

Wj(,i),g,h

(A(i

) ,h

(X

))

.

(7)

As the terms in the covariance have mean zero, and as the weights and activations from the previous layer are independent,

C A(j,g+1)(X), Aj(,g+1)(X ) = b2+

C( ) C( ) H( )D( ) H( )D( )

i=1 i =1 h=1

E
h =1

Wj(,i+,g1,h) Wj(,i+,g1,)h

E

(Ai(,h) (X))(Ai(

) ,h

(X

))

.

(8)

The weights are independent for different channels: Wj(,i+,g1,h) and Wj(,i+,g1,)h are iid for i = i , so their covariance is 0. Further, each row g of the weight matrices Wj(,i+1) only contains independent variables or zeros (figure 1), so C Wj(,i+,g1,h) , Wj(,i+,g1,h) = 0 for h = h . Thus, we can eliminate the
sums over i and h :

C( ) H( )D( )

C Aj(,g+1)(X), Aj(,g+1)(X ) = b2+

E Wj(,i+,g1,h) Wj(,i+,g1,h) E (A(i,h)(X))(Ai(,h)(X )) .

i=1 h=1

(9)

5

Under review as a conference paper at ICLR 2019

Algorithm 1 The ConvNet kernel k(X, X )
1: Input: two images, X, X  R .C(0)×(H(0)W (0)) 2: Compute vg(1)(X, X), vg(1)(X, X ), and vg(1)(X , X ); using equation (10). 3: for = 1, 2, . . . , L do 4: Compute s(g )(X, X ), s(g )(X, X ) and s(g )(X, X ) using equation (13), or some other non-
linearity. 5: Compute vg( +1)(X, X), vg( +1)(X, X ), and vg( +1)(X , X ); using equation (11). 6: end for 7: Output the scalar v1(L+1)(X, X ).

The gth row of Wi(,j+1) is zero for indices h that don't belong to its convolutional patch, so we can restrict the sum over h to that region. We also define vg(1)(X, X ), to emphasise that the covariances are independent of the output channel, j. The variance of the first layer is

vg(1)(X, X ) = C

A(j1,g)(X), A(j1,g)(X )

=

b2

+

w2 C (0)

C (0)

Xi,hXi,h.

i=1 hgth patch

(10)

And we do the same for the other layers,

vg( +1)(X, X ) = C Aj(,g+1)(X), Aj(,g+1)(X ) = b2 + w2

s(h )(X, X ),

hgth patch

where

s(h )(X, X ) = E (Ai(,h)(X))(A(i,h)(X ))

is the covariance of the activations, which is again independent of the channel.

(11) (12)

3.2 COVARIANCE OF THE ACTIVITIES

The elementwise covariance in the right-hand side of equation (11) can be computed in closed form for many choices of  if the activations are Gaussian. For each element of the activations, one needs to keep track of the 3 distinct entries of the bivariate covariance matrix between the inputs, vg( )(X, X), vg( )(X, X ) and vg( )(X , X ).

For example, for the ReLU nonlinearity ((x) = max(0, x)), one can adapt Cho & Saul (2009) in the same way as Matthews et al. (2018a, section 3) to obtain

s(g )(X, X ) =

vg( )(X, X)vg( )(X , X ) 

sin g( ) + ( - g( )) cos g( )

(13)

where g( ) = cos-1 vg( )(X, X )/ vg( )(X, X)vg( )(X , X ) .

3.3 EFFICIENCY OF THE CONVNET KERNEL
We now have all the pieces for computing the kernel, as written in Algorithm 1.
Putting together equations (11), and (13), gives us the surprising result that the diagonal covariances of the activations at layer + 1 only depend on the diagonal covariances of the activations at layer . This is very important, because it makes the computational cost of the kernel be within a constant factor of the cost of a forward pass for the equivalent CNN with 1 filter per layer.
Thus, the algorithm is more efficient that one would naively think. A priori, one needs to compute the covariance between all the elements of aj( )(X) and a(j )(X ) combined, yielding a 2H( )D( ) × 2H( )D( ) covariance matrix for every pair of points. Instead, we only need to keep track of a H( )D( )-dimensional vector per layer and pair of points.
Furthermore, the particular form for the kernel (eq. 1 and eq. 2) implies that the required variances and covariances at all required locations can be computed efficiently as a convolution.

6

Under review as a conference paper at ICLR 2019

3.4 KERNEL FOR A RESIDUAL CNN

The induction step in the argument for GP behaviour from section 2.2 depends only on the pre-
vious activations being iid Gaussian. Since all the activations are iid Gaussian, we can add skip connections between the activations of different layers while preserving GP behaviour, e.g. A( +1) and A( -s) where s is the number of layers that the skip connection spans. If we change the NN
recursion (equation 2) to

C( )

a(j +1)(X) := aj( -s)(X) + b(j ) +

Wj(,i) ai( )(X) ,

i=1

then the kernel recursion (equation 11) becomes

(14)

vg( +1)(X, X ) = vg( -s)(X, X ) + b2 + w2

s(g )(X, X ).

hgth patch

(15)

This way of adding skip connections is equivalent to the "pre-activation" shortcuts described by He et al. (2016b). Remarkably, the natural way of adding residual connections to NNs is the one that performed best in their empirical evaluations.

4 EXPERIMENTS
We evaluate our kernel on the MNIST handwritten digit classification task. Classification likelihoods are not conjugate for GPs, so we must make an approximation, and we follow Lee et al. (2017), in re-framing classification as multi-output regression.
The training set is split into N = 50000 training and 10000 validation examples. The regression targets Y  {-1, 1}N×10 are a one-hot encoding of the example's class: yn,c = 1 if the nth example belongs to class c, and -1 otherwise.
Training is exact conjugate likelihood GP regression with noiseless targets Y (Rasmussen & Williams, 2006). First we compute the N ×N kernel matrix Kxx, which contains the kernel between every pair of examples. Then we compute K-xx1Y using a linear system solver.
The test set has NT = 10000 examples. We compute the NT × N matrix Kxx, the kernel between each test example and all the training examples. The predictions are given by the row-wise maximum of KxxKx-x1Y.
For the "ConvNet GP" and "Residual CNN GP", we optimise the kernel hyperparameters by random search. We draw M random hyperparameter samples, compute the resulting kernel's performance in the validation set, and pick the highest performing run. The kernel hyperparameters are: b2, w2 ; the number of layers; the convolution stride, filter sizes and edge behaviour; the nonlinearity (we consider the error function and ReLU); and the frequency of residual skip connections (for Residual CNN GPs). We do not retrain the model on the validation set after choosing hyperparameters.
The "ResNet GP" is the kernel equivalent to a 32-layer version of the basic residual architecture by He et al. (2016a). The differences are: a 3×3 convolutional initial layer, no max-pooling, and a dense layer instead of average pooling at the end. We chose to remove the pooling because computing its output variance requires the off-diagonal elements of the filter covariance, which would invalidate the efficiency gains described in section 3.3.
Computational efficiency. Asymptotically, computing the kernel matrix takes O(N 2LD) time, where L is the number of layers in the network and D is the dimensionality of the input, and inverting the kernel matrix takes O(N 3). As such, we expect that for very large datasets, inverting the kernel matrix will dominate the computation time. However, on MNIST, N 3 is only around a factor of 10 larger than N 2LD. In practice, we found that it was more expensive to compute the kernel matrix than to invert it. For the ResNet kernel, the most expensive, computing Kxx, and Kxx for validation and test took 3h 40min on two Tesla P100 GPUs. In contrast, inverting Kxx and computing validation and test performance took 43.25 ± 8.8 seconds on a single Tesla P100 GPU.

7

Under review as a conference paper at ICLR 2019

Method NNGP (Lee et al., 2017) Convolutional GP (van der Wilk et al., 2017) Deep Conv. GP (Kumar et al., 2018) ConvNet GP Residual CNN GP ResNet GP GP + parametric deep kernel (Bradshaw et al., 2017) ResNet (Chen et al., 2018)

#samples  250 SGD SGD 27 27 ­ SGD ­

Validation error ­ ­ ­
0.71% 0.72% 0.68%
­ ­

Test error 1.21% 1.17% 1.34% 1.03% 0.96% 0.84% 0.60% 0.41%

Table 1: MNIST classification results. #samples gives the number of kernels that were randomly sampled for the hyperparameter search. "ConvNet GP" and "Residual CNN GP" are random CNN architectures with a fixed filter size, whereas "ResNet GP" is a slight modification of the architecture by He et al. (2016b). Entries labeled "SGD" used Stochastic Gradient Descent for tuning hyperparameters, by maximising the likelihood of the training set. The last two methods use parametric neural networks. The ResNet GP performs better than all other nonparametric approaches, but is still not as good as methods with many parameters.

5 RELATED WORK
Van der Wilk et al. (van der Wilk et al., 2017) also adapted GPs to image classification. They defined a prior on functions f that takes an image and outputs a scalar. First, draw a function g  GP(0, kp(X, X )). Then, f is the sum of the output of g applied to each of the convolutional patches. Their approach is also inspired by convolutional NNs, but their kernel kp is applied to all pairs of patches of X and X . This makes their convolutional kernel expensive to evaluate, requiring inter-domain inducing point approximations to remain tractable. The kernels in this work, directly motivated by the infinite-filter limit of a CNN, only apply something like kp to the corresponding pairs of patches within X and X (equation 10). As such, the CNN kernels are cheaper to compute and exhibit superior performance (Table 1), despite the use of an approximate likelihood function.
Kumar et al. (2018) define a prior over functions by stacking several GPs with van der Wilk's convolutional kernel, forming a "Deep GP" (Damianou & Lawrence, 2013). In contrast, the kernel in this paper confines all hierarchy to the definition of the kernel, and the resulting GPs is shallow.
Wilson et al. (2016) introduced and Bradshaw et al. (2017) improved deep kernel learning. The inputs to a classic GP kernel k (e.g. RBF) are preprocessed by applying a feature extractor g (a deep NN) prior to computing the kernel: kdeep(X, X ) := k(g(X; ), g(X , )). The NN parameters are optimised by gradient ascent using the likelihood as the objective, as in standard GP kernel learning (Rasmussen & Williams, 2006, Chapter 5). Since deep kernel learning incorporates a state-of-the-art NN with over 106 parameters, we expect it to perform similarly to a NN applied directly to the task of image classification. At present both CNNs and deep kernel learning display superior performance to the GP kernels in this work. However, the kernels defined here have far fewer parameters (around 10, compared to their 106).
Borovykh (2018) also suggests that a CNN exhibits GP behaviour. However, they take the infinite limit with respect to the filter size, not the number of filters. Thus, their infinite network is inapplicable to real data which is always of finite dimension.
Finally, there is a series of papers analysing the mean-field behaviour of deep NNs and CNNs which aims to find good random initializations, i.e. those that do not exhibit vanishing or exploding gradients or activations (Schoenholz et al., 2016; Yang & Schoenholz, 2017). Apart from their very different focus, the key difference to our work is that they compute the variance for a single trainingexample, whereas to obtain the GPs kernel, we additionally need to compute the output covariances for different training/test examples (Xiao et al., 2018).
6 CONCLUSIONS AND FUTURE WORK
We have shown that deep Bayesian CNNs with infinitely many filters are equivalent to a GP with a recursive kernel. We also derived the kernel for the GP equivalent to a CNN, and showed that, in

8

Under review as a conference paper at ICLR 2019
handwritten digit classification, it outperforms all previous GP approaches that do not incorporate a parametric NN into the kernel. Given that most state of the art neural networks incorporate structure (convolutional or otherwise) into their architecture, the equivalence between CNNs and GPs is potentially of considerable practical relevance. In particular, we hope to apply GP CNNs in domains as widespread as adversarial examples, lifelong learning and k-shot learning, and we hope to improve them by developing efficient multi-layered inducing point approximation schemes.
REFERENCES
JR Blum, H Chernoff, M Rosenblatt, and H Teicher. Central limit theorems for interchangeable processes. Canad. J. Math, 10:222­229, 1958.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pp. 1613­1622, 2015.
Anastasia Borovykh. A gaussian process perspective on convolutional neural networks. ResearchGate, 05 2018.
John Bradshaw, Alexander G de G Matthews, and Zoubin Ghahramani. Adversarial examples, uncertainty, and transfer testing robustness in gaussian process hybrid deep networks. arXiv preprint arXiv:1707.02476, 2017.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. arXiv preprint arXiv:1806.07366, 2018.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pp. 342­350, 2009. URL http://papers.nips.cc/ paper/3628-kernel-methods-for-deep-learning.pdf.
Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Carlos M. Carvalho and Pradeep Ravikumar (eds.), Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pp. 207­ 215, Scottsdale, Arizona, USA, 29 Apr­01 May 2013. PMLR. URL http://proceedings. mlr.press/v31/damianou13a.html.
Marc Deisenroth and Carl E Rasmussen. PILCO: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML11), pp. 465­472, 2011.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. arXiv preprint arXiv:1506.02142, 2015.
Yarin Gal and Lewis Smith. Idealised bayesian neural networks cannot have adversarial examples: Theoretical and empirical study. arXiv preprint arXiv:1806.00667, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016a. URL https://arxiv.org/pdf/1512.03385.pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630­645. Springer, 2016b.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Vinayak Kumar, Vaibhav Singh, PK Srijith, and Andreas Damianou. Deep gaussian processes with convolutional kernels. arXiv preprint arXiv:1806.01655, 2018.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.
9

Under review as a conference paper at ICLR 2019
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pp. 6402­6413, 2017.
Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, pp. 396­404, 1990.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research, 18(1):4873­4907, 2017.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018a. URL https://openreview.net/forum?id= H1-nGgWC-.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018b.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg, 1996. ISBN 0387947248.
Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning, volume 1. MIT press Cambridge, 2006.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pp. 2951­2959, 2012.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional gaussian processes. In Advances in Neural Information Processing Systems, pp. 2845­2854, 2017.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681­688, 2011.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pp. 370­378, Cadiz, Spain, 09­11 May 2016. PMLR. URL http: //proceedings.mlr.press/v51/wilson16.html.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances in neural information processing systems, pp. 7103­7114, 2017.
10

Under review as a conference paper at ICLR 2019

7 SUPPLEMENTARY INFORMATION

7.1 TECHNICAL NOTES ON LIMITS

The key technical issues in the proof (and the key differences between Lee et al. (2017) and Matthews et al. (2018a)) arise from exactly how and where we take limits. In particular, consider the activations as being functions of the activities at the previous layer,

A(4) = A(4)(A(3)(A(2)(A(1)(X))))

(16)

Now, there are two approaches to taking limits. First, both our argument in the main text, and the argument in Lee et al. (2017) is valid if we are able to take limits "inside" the network,

AL(4)

=

lim A(4)
C (3) 

lim A(3)
C (2) 

lim A(2) A(1)(X)
C (1) 

.

(17)

However, Matthews et al. (2018a;b) argue that is preferable to take limits "outside" the network. In particular, Matthews et al. (2018b) take the limit with all layers simultaneously,

AM(4)

=

lim
C 

A(4)

A(3)

A(2)

A(1)(X)

,

(18)

where C( ) = C (although they actually consider a broader class of functions). That said, similar technical issues arise if we take limits in sequence, but outside the network,

A(M4)

=

lim lim lim A(4)
C(3) C(2) C(1)

A(3)

A(2)

A(1)(X)

.

(19)

One would hope that given sufficiently well-behaved random variables (i.e. weights and biases), the exact method of taking limits would not matter, and that therefore AL(4) and AM(4) would be equal. While, Matthews et al. (2018a;b) show this is indeed the case (at least for dense networks), it is not obvious. In particular, the key technical issue is that the (multivariate) CLT gives convergence in distribution, and convergence in distribution is a relatively weak property which does not even guarantee converge of moments. Thus, when we apply the multivariate CLT, all one can show is that the a(j +1)(X, X ) converges in distribution to a multivariate Gaussian, and this raises two issues with the argument as it is presented. First, while uncorrelated jointly Gaussian random variables are independent, this is not clear for uncorrelated random variables that merely converge in distribution to a multivariate Gaussian. Second, convergence in distribution does not imply convergence of the expectation of unbounded functions, such as the relu activation function used here.
However, it is possible to apply strategies from Matthews et al. (2018b) to circumvent these issues. First, we accept that we can no longer take a(j )(X) and a(j )(X ) to be independent, and instead apply a CLT for exchangeable (rather than independent) random variables (Blum et al., 1958). The relevant variables can be shown to be exchangeable by noting that they are iid, conditioned on the activations at the previous layer. Second, to ensure convergence of expectations of unbounded functions, we require uniform integrability, and this can be shown by following arguments in Matthews et al. (2018b, Lemma 21).

11

