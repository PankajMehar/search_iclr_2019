Under review as a conference paper at ICLR 2019
GRADIENT DESCENT ALIGNS THE LAYERS OF DEEP
LINEAR NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
This paper establishes risk convergence and asymptotic weight matrix alignment -- a form of implicit regularization -- of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. In more detail, for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes): (1) the risk converges to 0; (ii) the normalized ith weight matrix asymptotically equals its rank-1 approximation uivi ; (iii) these rank-1 matrices are aligned across layers, meaning |vi+1ui|  1. In the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network -- the product of its weight matrices -- converges to the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon.
1 INTRODUCTION
Efforts to explain the effectiveness of gradient descent in deep learning have uncovered an exciting possibility: it not only finds solutions with low error, but also biases the search for low complexity solutions which generalize well (Zhang et al., 2017; Bartlett et al., 2017; Soudry et al., 2017; Gunasekar et al., 2018). This paper analyzes the implicit regularization of gradient descent and gradient flow on deep linear networks and linearly separable data. For strictly decreasing losses, the optimum is off at infinity, and we establish various alignment phenomena:
· For each weight matrix Wi, the corresponding normalized weight matrix Wi/ Wi F asymptotically equals its rank-1 approximation uivi , where the Frobenius norm Wi F satisfies Wi F  . In other words, /Wi 2 Wi F  1, and asymptotically only the rank-1 approximation of each weight matrix contributes to the final predictor, a form of implicit regularization.
· Adjacent rank-1 weight matrix approximations are aligned: |vi+1ui|  1.
· For the logistic loss, the first right singular vector v1 of W1 is aligned with the data, meaning v1 converges to the unique maximum margin predictor u¯ defined by the data. Moreover, the linear predictor induced by the network, wprod := WL · · · W1, is also aligned with the data, meaning /wprod wprod  u¯.
Simultaneously, this work proves that the risk is globally optimized: it asymptotes to 0. Alignment and risk convergence are proved simultaneously; the phenomena are coupled within the proofs. The paper is organized as follows. This introduction continues with related work, notation, and assumptions in Sections 1.1 and 1.2. The analysis of gradient flow is in Section 2, and gradient descent is analyzed in Section 3. The paper closes with future directions in Section 4; a particular highlight is a preliminary experiment on CIFAR-10 which establishes empirically that a form of the alignment phenomenon occurs on the standard nonlinear network AlexNet.
1

Under review as a conference paper at ICLR 2019

4

1 layer 4 layers

2

0

2

7.5 5.0 2.5 0.0 2.5 5.0 7.5

1.0 0.8 0.6 0.4 0.2 0.0
0

risk
||W1||2 / ||W1||F ||W2||2 / ||W2||F ||W3||2 / ||W3||F ||W4||2 / ||W4||F
50 100 150 200 250

(a) Margin maximization.

(b) Alignment and risk minimization.

Figure 1: Visualization of main results on synthetic data with a 4-layer linear network compared to a 1-layer network (a linear predictor). Figure 1a shows the convergence of 1-layer and 4-layer networks to the same linear predictor on positive (blue) and negative (red) separable data. Figure 1b shows the alignment phenomenon in the 4-layer network, plotted against the risk. Specifically, for each layer, the ratio of spectral to Frobenius norms is plotted, and converges to 1. As in the theoretical analysis, the convergence in alignment and risk occur simultaneously.

1.1 RELATED WORK
On the implicit regularization of gradient descent, Soudry et al. (2017) show that for linear predictors and linearly separable data, the gradient descent iterates converge to the same direction as the maximum margin solution. Ji & Telgarsky (2018) further characterize such an implicit bias for general nonseparable data. Gunasekar et al. (2018) consider gradient descent on fully connected linear networks and linear convolutional networks. In particular, for the exponential loss, assuming the risk is minimized to 0 and the gradients converge in direction, they show that the whole network converges in direction to the maximum margin solution. These two assumptions are on the gradient descent process itself, and specifically the second one might be hard to interpret and justify. Compared with Gunasekar et al. (2018), this paper proves that the risk converges to 0 and the weight matrices align; moreover the proof here proves the properties simultaneously, rather than assuming one and deriving the other. Lastly, for ReLU networks, Du et al. (2018) show that gradient flow does not change the difference between squared Frobenius norms of any two layers.
For a smooth (nonconvex) function, Lee et al. (2016) show that any strict saddle can be avoided almost surely with small step sizes. If there are only countably many saddle points and they are all strict, and if gradient descent iterates converge, then this implies (almost surely) they converge to a local minimum. In the present work, since there is no finite local minimum, gradient descent will go to infinity and never converge, and thus these results of Lee et al. (2016) do not show that the risk converges to 0.
There has been a rich literature on linear networks. Saxe et al. (2013) analyze the learning dynamics of deep linear networks, showing that they exhibit some learning patterns similar to nonlinear networks, such as a long plateau followed by a rapid risk drop. Arora et al. (2018) show that depth can help accelerate optimization. On the landscape properties of deep linear networks, Lu & Kawaguchi (2017); Laurent & von Brecht (2017) show that under various structural assumptions, all local optima are global. Zhou & Liang (2018) give a necessary and sufficient characterization of critical points for deep linear networks.
1.2 NOTATION, SETTING, AND ASSUMPTIONS
Consider a data set {(xi, yi)}in=1, where xi  Rd, xi  1, and yi  {-1, +1}. The data set is assumed to be linearly separable, i.e., there exists a unit vector u which correctly classifies every data point: for any 1  i  n, yi u, xi > 0. Furthermore, let  := max u =1 min1in yi u, xi > 0 denote the maximum margin, and u¯ := arg max u =1 min1in yi u, xi denote the maximum margin solution (the solution to the hard-margin SVM).
2

Under review as a conference paper at ICLR 2019

A linear network of depth L is parameterized by weight matrices WL, . . . , W1, where Wk  Rdk×dk-1 , d0 = d, and dL = 1. Let W = (WL, . . . , W1) denote all parameters of the network. The (empirical) risk induced by the network is given by

R(W )

=

R (WL, . . . , W1)

=

1 n

n

(yiWL

· · · W1xi)

=

1 n

n

i=1 i=1

wprod, zi ,

where wprod := (WL · · · W1) , and zi := yixi.

The loss is assumed to be continuously differentiable, unbounded, and strictly decreasing to 0. Examples include the exponential loss exp(x) = e-x and the logistic loss log(x) = ln 1 + e-x . Assumption 1. < 0 is continuous, limx- (x) =  and limx (x) = 0.

This paper considers gradient flow and gradient descent, where gradient flow W (t) t  0, t  R

can be interpreted as gradient descent with infinitesimal step sizes. It starts from some W (0) at

t = 0, and proceeds as

dW (t) = -R W (t) . dt

By contrast, gradient descent W (t) t  0, t  Z is a discrete-time process given by

W (t + 1) = W (t) - tR W (t) , where t is the step size at time t.

We assume that the initialization of the network is not a critical point and induces a risk no larger than the risk of the trivial linear predictor 0.
Assumption 2. The initialization W (0) satisfies R W (0) = 0 and R W (0)  R(0) = (0).

It is natural to require that the initialization is not a critical point, since otherwise gradient flow/descent will never make a progress. The requirement R W (0)  R(0) can be easily sat-
isfied, for example, by making W1(0) = 0 and WL(0) · · · W2(0) = 0. On the other hand, if R W (0) > R(0), gradient flow/descent may never minimize the risk to 0. Proofs of those claims are given in Appendix A.

2 RESULTS FOR GRADIENT FLOW

In this section, we consider gradient flow. Although impractical when compared with gradient descent, gradient flow can simplify the analysis and highlight proof ideas. For convenience, we usually use W , Wk, and wprod, but they all change with (the continuous time) t. Only proof sketches are given here; detailed proofs are deferred to Appendix B.

2.1 RISK CONVERGENCE

One key property of gradient flow is that it never increases the risk:

dR(W ) =

R(W ), dW

L
= - R(W ) 2 = -

R

2
 0.

dt dt

k=1 Wk F

(1)

We now state the main result: under Assumptions 1 and 2, gradient flow minimizes the risk, Wk and wprod all go to infinity, and the alignment phenomenon occurs.

Theorem 1. Under Assumptions 1 and 2, gradient flow iterates satisfy the following properties:

· limt R(W ) = 0.

· For any 1  k  L, limt Wk F = .

· For any 1  k  L, letting (uk, vk) denote the first left and right singular vectors of Wk,

lim
t

Wk Wk F

- ukvk

= 0.
F

3

Under review as a conference paper at ICLR 2019

Moreover, for any 1  k < L, limt vk+1, uk = 1. As a result,

lim
t

wprod

L k=1

Wk

, v1
F

= 1,

and thus limt wprod = .

Theorem 1 is proved using two lemmas, which may be of independent interest. To show the ideas,
let us first introduce a little more notation. Recall that R(W ) denotes the empirical risk induced by the deep linear network W . Abusing the notation a little, for any linear predictor w  Rd, we also use R(w) to denote the risk induced by w. With this notation, R(W ) = R(wprod), while

R(wprod)

=

1 n

n

i=1

wprod, zi

1n zi = n
i=1

(WL · · · W1zi) zi

is in Rd and different from R(W ), which has

L k=1

dk dk-1

entries,

as

given

below:

R Wk

= Wk+1 · · · WL

R(wprod)

W1

· · · Wk-1.

Furthermore, for any R > 0, let

B(R) = W max Wk F  R .
1kL

The first lemma shows that for any R > 0, the time spent by gradient flow in B(R) is finite.
Lemma 1. Under Assumption 1 and 2, for any R > 0, there exists a constant (R) > 0, such that for any t  1 and any W  B(R), R/W1 F  (R). As a result, gradient flow spends a finite amount of time in B(R) for any R > 0, and max1kL Wk F is unbounded.

Here is the proof sketch. If Wk F are bounded, then R(wprod) will be lower bounded by a positive constant, therefore if R/W1 F = WL · · · W2 R(wprod) can be arbitrarily small, then WL · · · W2 and wprod can also be arbitrarily small, and thus R(W ) can be arbitrarily close to R(0). This cannot happen after t = 1, otherwise it will contradict Assumption 2 and eq. (1).

To proceed, we need the following properties of linear networks from prior work (Arora et al., 2018; Du et al., 2018). For any time t  0 and any 1  k < L,

Wk+1(t)Wk+1(t) - Wk+1(0)Wk+1(0) = Wk(t)Wk (t) - Wk(0)Wk (0).

(2)

To see this, just notice that

R Wk+1 Wk+1

= Wk+1 · · · WL R(wprod)

W1

· · · Wk

R = Wk Wk .

Taking the trace on both sides of eq. (2), we have

Wk+1(t)

2-
F

Wk+1(0)

2 F

=

Wk (t)

2-
F

Wk (0)

2 F

.

(3)

In other words, the difference between the squares of Frobenius norms of any two layers remains a

constant. Together with Lemma 1, it implies that all Wk F are unbounded.

However, even if Wk F are large, it does not follow necessarily that wprod is also large. Lemma 2 shows that this is indeed true: for gradient flow, as Wk F get larger, adjacent layers also get more aligned to each other, which ensures that their product also has a large norm.

For 1  k  L, let k, uk, and vk denote the first singular value (the 2-norm), the first left singular vector, and the first right singular vector of Wk, respectively. Furthermore, define

D :=

max
1kL

Wk (0)

2 F

L-1

-

WL(0)

2 F

+

Wk(0)Wk (0) - Wk+1(0)Wk+1(0) ,

2

k=1

which depends only on the initialization. If for any 1  k < L, Wk(0)Wk (0) = Wk+1(0)Wk+1(0), then D = 0.

4

Under review as a conference paper at ICLR 2019

Lemma 2. The gradient flow iterates satisfy the following properties:

· For any 1  k  L,

Wk

2 F

-

Wk

2 2



D.

· For any 1  k < L,

vk+1, uk 2  1 -

/ .D+

Wk+1 (0)

2 2

+

Wk (0)

2 2

Wk+1

2 2

· Suppose max1kL Wk F  , then

/ , vwprod

L k=1

Wk F

1

 1.

The proof is based on eq. (2) and eq. (3). If Wk(0)Wk (0) = Wk+1(0)Wk+1(0), then eq. (2) gives that Wk+1 and Wk have the same singular values, and Wk+1's right singular vectors and Wk's left singular vectors are the same. If it is true for any two adjacent layers, since WL is a row vector, all layers have rank 1. With general initialization, we have similar results when Wk F is large enough so that the initialization is negligible. Careful calculations give the exact results in Lemma 2.
An interesting point is that the implicit regularization result in Lemma 2 helps establish risk convergence in Theorem 1. Specifically, by Lemma 2, if all layers have large norms, WL · · · W2 will be large. If the risk is not minimized to 0, R(wprod) will be lower bounded by a positive constant, and thus R/W1 F = WL · · · W2 R(wprod) will be large. Invoking eq. (1), Lemma 1 and eq. (3) gives a contradiction. Since the risk has no finite optimum, Wk F  .

2.2 CONVERGENCE TO THE MAXIMUM MARGIN SOLUTION
Here we focus on the exponential loss exp(x) = e-x and the logistic loss log(x) = ln(1 + e-x). In addition to risk convergence, these two losses also enable gradient descent to find the maximum margin solution.
To get such a strong convergence, we need one more assumption on the data set. Recall that  = max u =1 min1in u, zi > 0 denotes the maximum margin, and u¯ denotes the unique maximum margin predictor which attains this margin . Those data points zi for which u¯, zi =  are called support vectors. Assumption 3. The support vectors span the whole space Rd.

Assumption 3 appears in prior work Soudry et al. (2017), and can be satisfied in many cases: for example, it is almost surely true if the number of support vectors is larger than or equal to d and the
data set is sampled from some density w.r.t. the Lebesgue measure. It can also be relaxed to the situation that the support vectors and the whole data set span the same space; in this case R(wprod) will never leave this space, and we can always restrict our attention to this space.

With Assumption 3, we can state the main theorem.

Theorem 2. Under Assumptions 2 and 3, for almost all data and for losses exp and log,

then limt v1, u¯ = 1, where v1 is the first right singular vector of W1. As a result,

limt /wprod

L k=1

Wk F = u¯.

Theorem 2 relies on two structural lemmas. The first one is based on a similar almost-all argument due to Soudry et al. (2017, Lemma 8). Let S  {1, . . . , n} denote the set of indices of support
vectors.

Lemma 3. Under Assumption 3, if the data set is sampled from some density w.r.t. the Lebesgue measure, then with probability 1,



:=

min max
||=1,u¯ iS

, zi

> 0.

Let u¯ denote the orthogonal complement of span(u¯), and let  denote the projection onto u¯.
Lemma 4. Under Assumption 3, for almost all data, exp and log, and any w  Rd, if w, u¯  0 and w is larger than 1+ln(n)/ for exp or 2n/e for log, then w, R(w)  0.

With Lemma 3 and Lemma 4 in hand, we can prove Theorem 2. Let W1 denote the projection of rows of W1 onto u¯. Notice that

wprod = WL . . . W2(W1)

and

d

W1 dt

2 F

= - wprod, R(wprod) .

5

Under review as a conference paper at ICLR 2019

If W1 F is large compared with W1 F , since layers become aligned, wprod will also be large, and then Lemma 4 implies that W1 F will not increase. At the same time, W1 F  , and thus for large enough t, W1 F must be very small compared with W1 F . Many details
need to be handled to make this intuition exact; the proof is given in Appendix B.

3 RESULTS FOR GRADIENT DESCENT

One key property of gradient flow which is used in the previous proofs is that it never increases the risk, which is not necessarily true for gradient descent. However, for smooth losses (i.e, with Lipschitz continuous derivatives), we can design some decaying step sizes, with which gradient descent never increases the risk, and basically the same results hold as in the gradient flow case. Deferred proofs are given in Appendix C.
We make the following additional assumption on the loss, which is satisfied by the logistic loss log.
Assumption 4. is -Lipschitz (i.e, is -smooth), and | |  G (i.e., is G-Lipschitz).

Under Assumption 4, the risk is also a smooth function of W , if all layers are bounded.
Lemma 5. Suppose is -smooth. If R  1, then (R) = 2L2R2L-2( + G), and R(W ) is a (R)-smooth function on the set B(R) = W Wk F  R, 1  k  L .

Smoothness ensures that for any W, V  B(R), R(W ) - R(V )  R(V ), W - V + (R) W -V 2/2 (see Bubeck et al. (2015) Lemma 3.4). In particular, if we choose some R and
set a constant step size t = 1/(R), then as long as W (t + 1) and W (t) are both in B(R),

R W (t + 1) - R W (t)  R W (t) , -tR W (t)

+ (R)t2 R W (t) 2

=- 1

R W (t)

2 = - t R W (t)

2
.

2(R)

2

2
(4)

In other words, the risk does not increase at this step. However, similar to gradient flow, the gradient descent iterate will eventually escape B(R), which may increase the risk.
Lemma 6. Under Assumption 1, 2 and 4, suppose gradient descent is run with a constant step size 1/(R). Then there exists a time t when W (t)  B(R), in other words, max1kL Wk(t) F > R.

Fortunately, this issue can be handled by adaptively increasing R and correspondingly decreasing the step sizes, formalized in the following assumption.
Assumption 5. The step size t = min{1/(Rt), 1}, where Rt satisfies W (t)  B(Rt), and if W (t + 1)  B(Rt), Rt+1 = Rt.

Assumption 5 can be satisfied by a line search, which ensures that the gradient descent update is not too aggressive and the boundary R is increased properly.
With the additional Assumptions 4 and 5, exactly the same theorems can be proved for gradient descent. We restate them briefly here.
Theorem 3. Under Assumption 1, 2, 4, and 5, gradient descent satisfies

· limt R W (t) = 0. · For any 1  k  L, limt Wk(t) F = .

· limt

/wprod (t)

L k=1

Wk(t) F , v1(t)

= 1, where v1(t) is the first right singular vector

of W1(t).

Theorem 4. Under Assumption 2, 3, and Assumption 5, for the logistic loss log and almost all data,

limt v1(t), u¯

= 1, and limt /wprod(t)

L k=1

Wk(t) F = u¯.

Proofs of Theorem 3 and 4 are given in Appendix C, and are basically the same as the gradient flow

proofs. The key difference is that an error of

 t=0

t2

R(W (t))

2 will be introduced in many

6

Under review as a conference paper at ICLR 2019

2.0 risk

risk

||W 3||2 / ||W 3||F 2.0

||W 3||2 / ||W 3||F

1.5

||W 2||2 / ||W 2||F ||W 1||2 / ||W 1||F

1.5

||W 2||2 / ||W 2||F ||W 1||2 / ||W 1||F

1.0 1.0

0.5 0.5

0.0 0

0.0

20 40 60 80 100

0

20 40 60 80 100

(a) Default initialization.

(b) Initialization with the same Frobenius norm.

Figure 2: Risk and alignment of dense layers (the ratio Wi 2/ Wi F ) of (nonlinear!) AlexNet on CIFAR-10. Figure 2a uses default PyTorch initialization, while Figure 2b forces initial Frobenius
norms to be equal amongst dense layers.

parts of the proof. However, it is bounded in light of eq. (4):
 2 2
t2 R W (t)  t R W (t)  2R W (0) .
t=0 t=0
Since all weight matrices go to infinity, such a bounded error does not matter asymptotically, and thus proofs still go through.
4 SUMMARY AND FUTURE DIRECTIONS
This paper rigorously proves that, for deep linear networks on linearly separable data, gradient flow and gradient descent minimize the risk to 0, align adjacent weight matrices, and align the first right singular vector of the first layer to the maximum margin solution determined by the data. There are many potential future directions; a few are as follows.
Convergence rate. This paper only proves asymptotic convergence with no convergence rate. A convergence rate would allow the algorithm to be compared to other methods which also globally optimize this objective, would also suggest ways to improve step sizes and initialization, and ideally even exhibit a sensitivity to the network architecture and suggest how it could be improved.
Nonseparable data and nonlinear networks. Real-world data is generally not linearly separable, but nonlinear deep networks can reliably decrease the risk to 0, even with random labels (Zhang et al., 2017). This seems to suggest that a nonlinear notion of separability is at play; is there some way to adapt the present analysis?
The present analysis is crucially tied to the alignment of weight matrices: alignment and risk are analyzed simultaneously. Motivated by this, consider a preliminary experiment, presented in Figure 2, where stochastic gradient descent was used to minimize the risk of a standard AlexNet on CIFAR-10 (Krizhevsky et al., 2012; Krizhevsky & Hinton, 2009).
Even though there are ReLUs, max-pooling layers, and convolutional layers, the alignment phenomenon is occurring in a reduced form on the dense layers (the last three layers of the network). Specifically, despite these weight matrices having shape (1024, 4096), (4096, 4096), and (4096, 10) the key alignment ratios Wi 2/ Wi F are much larger than their respective lower bounds (1024-1/2, 4096-1/2, 10-1/2). Two initializations were tried: default PyTorch initialization, and a Gaussian initialization forcing all initial Frobenius norms to be just 4, which is suggested by the norm preservation property in the analysis and removes noise in the weights.
REFERENCES
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.

7

Under review as a conference paper at ICLR 2019
Peter Bartlett, Dylan Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. NIPS, 2017.
Se´bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning, 8(3-4):231­357, 2015.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent on linear convolutional networks. arXiv preprint arXiv:1806.00468, 2018.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffery Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
Thomas Laurent and James von Brecht. Deep linear neural networks with arbitrary loss: All local minima are global. arXiv preprint arXiv:1712.01473, 2017.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges to minimizers. arXiv preprint arXiv:1602.04915, 2016.
Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2017.
Yi Zhou and Yingbin Liang. Critical points of linear neural networks: Analytical forms and landscape properties. 2018.
8

Under review as a conference paper at ICLR 2019

A REGARDING ASSUMPTION 2

Suppose W1(0) = 0 while WL(0) · · · W2(0) = 0. First of all, WL(0) · · · W1(0) = 0 and thus R W (0) = R(0). Moreover,

1n

R wprod(0) , u¯

= n

(0) zi, u¯  (0) < 0,

i=1

which implies R wprod(0) = 0 and R/W1 = WL(0) · · · W2(0) R wprod(0) = 0.
On the other hand, if R W (0) > R(0), gradient flow/descent may never minimize the risk to 0. For example, suppose the network has two layers, and both the input and output have dimension 1; the network just computes the dot product of two vectors w1 and w2. Consider minimizing R(w1, w2) = exp - w1, w2 . If w1(0) = -w2(0) = 0, then R w1(0), w2(0) = exp w1 2 > exp(0). It is easy to verify that for any t, w1(t) = -w2(t), and R w1(t), w2(t)  exp(0) > 0.

B OMITTED PROOFS FROM SECTION 2

Proof of Lemma 1. Fix an arbitrary R > 0. If the claim is not true, then for any > 0, there exists

some t  1 such that

Wk

F  R for all k while

R/W1

2
F

2, which means

R W1

2
=
F

W2

· · · WL R(wprod)

2 = WL · · · W2 2 R(wprod) 2  2.
F

Since wprod  RL, we have

R(wprod), u¯

1 =
n

n

i=1

wprod, zi

zi, u¯

1 n

n

i=1

wprod, zi   -M ,

where -M = max-RLxRL (x). Since is continuous and the domain is bounded, the maximum is attained and negative, and thus M > 0. Therefore R(wprod)  M , and thus WL · · · W2  /M . Since W1 F  R, we further have wprod  R/M . In other words, after t = 1, wprod may be arbitrarily small, which implies R wprod can be arbitrarily close to R (0).

On the other hand, by Assumption 2, dR(W )/ dt = - R(W ) 2 < 0 at t = 0. This implies that R W (1) < R W (0) , and for any t  1, R W (t)  R W (1) < R W (0)  R(0), which is a contradiction.

Since the risk is always positive, we have

R W (0) 

L

R 2 dt

t=0 k=1 Wk F



 R 2 dt

t=0 W1 F

 R 2



t=0

W1

1
F

max
1kL

Wk F  R

dt



 t=1

R W1

2
1
F

max
1kL

Wk F  R

dt





(R)2

1
t=1

max
1kL

Wk F  R

dt,

which implies gradient flow only spends a finite amount of time in W max1kL Wk F  R . This directly implies that max1kL Wk F is unbounded.

9

Under review as a conference paper at ICLR 2019

Proof of Lemma 2. The first claim is true for k = L since WL is a row vector. For any 1  k < L, recall that Arora et al. (2018); Du et al. (2018) give the following relation:

Wk+1(t)Wk+1(t) - Wk+1(0)Wk+1(0) = Wk(t)Wk (t) - Wk(0)Wk (0).

(5)

Let Ak,k+1 = Wk(0)Wk (0) - Wk+1(0)Wk+1(0). By eq. (5) and the definition of singular vectors and singular values, we have

k2  vk+1WkWk vk+1
= vk+1Wk+1Wk+1vk+1 + vk+1Ak,k+1vk+1 = k2+1 + vk+1Ak,k+1vk+1  k2+1 - Ak,k+1 2.

(6)

Moreover, by taking the trace on both sides of eq. (5), we have

Wk

2 F

= tr

Wk Wk

= tr Wk+1Wk+1 + tr Wk(0)Wk (0) - tr Wk+1(0)Wk+1(0)

=

Wk+1

2 F

+

Wk (0)

2 F

-

Wk+1(0)

2 F

.

(7)

Summing eq. (6) and eq. (7) from k to L - 1, we get

L-1

Wk

2 F

-

Wk

2 2



Wk (0)

2 F

-

WL(0)

2 F

+

Ak ,k +1 2  D.

k =k

(8)

Next we prove that singular vectors get aligned. Consider uk Wk+1Wk+1uk. On one hand, similar to eq. (6), we can get that

uk Wk+1Wk+1uk = uk WkWk uk - uk Wk(0)Wk (0)uk + uk Wk+1(0)Wk+1(0)uk  uk WkWk uk - uk Wk(0)Wk (0)uk  k2 - Wk(0) 22.
On the other hand, it follows from the definition of singular vectors and eq. (8) that

(9)

uk Wk+1Wk+1uk = uk, vk+1 2k2+1 + uk Wk+1Wk+1 - vk+1k2+1vk+1 uk



uk, vk+1 2k2+1 +

Wk+1

2 F

-

Wk+1

2 2

 uk, vk+1 2k2+1 + D.

Combining eq. (9) and eq. (10), we get

k2  uk, vk+1 2k2+1 + D + Wk(0) 22.

Similar to eq. (9), we can get

k2  vk+1WkWk vk+1  k2+1 -

Wk+1(0)

2 2

.

Therefore

k2 k2+1

1-

Combining eq. (11) and eq. (12), we finally get

Wk+1(0) k2+1

2
2.

uk, vk+1 2  1 - D +

Wk (0)

2 2

+

k2+1

Wk+1(0)

2
2.

(10) (11)
(12)

Regarding the last claim, first recall that since the difference between the squares of Frobenius norms of any two layers is a constant, max1kL Wk F   implies Wk F   for any k. We further have the following.

· Since

Wk

2 F

-

Wk

2 2



D,

Wk 2   for any k, and Wk/ Wk F  ukvk .

10

Under review as a conference paper at ICLR 2019

· Since Wk 2  , | uk, vk+1 |  1.

As a result,

wprod

L k=1

Wk

, v1
F

=

L k=1

Wk Wk

F

,

v1

L
 uivi , v1
k=1
 1.

Proof of Theorem 1. Suppose for some > 0, R (W )  for any t. Then there exists some 1  j  n such that wprod, zj  , and thus wprod, zj  -1( ). On the other hand, since R(W )  R(0) = (0), wprod, zj  n (0), and thus wprod, zj  -1 n (0) . Let
-M = max -1(n (0))x -1( /n) (x) < 0, we have for any t,

and thus

R(wprod)

R(wprod), u¯  M /n.

1n =
n
i=1

wprod, zi

1 n n
i=1

wprod, zi

1 n

wprod, zj 

 -M  < 0, n

zi, u¯ 

Similar to the proof of Lemma 2, we can show that if Wk F  ,

(WL · · · W2) Wk F · · · W2

, v2
F

 1.

In other words, there exists some C > 0, such that when min1kL Wk F > C, WL · · · W2  Wk F · · · W2 F /2 > CL/2.

Lemma 1 shows that gradient flow spends a finite amount of time in W max1kL Wk F  R for any R > 0. Since the difference between the squares of Frobenius norms of any two layers is a constant, gradient flow also spends a finite amount of time in W min1kL Wk F  C . Now we have

R W (0) 

L

R 2 dt

t=0 k=1 Wk F



 R 2 dt

t=0 W1 F



= WL · · · W2 2 R(wprod) 2 dt

t=0





t=0

WL · · · W2 2 R(wprod) 21

W min
1kL

Wk

F C

dt

M 2

2
CL



 n

2

1 W min Wk F  C dt
t=0 1kL

= ,

11

Under review as a conference paper at ICLR 2019

which is a contradiction. Therefore R( )  0. This further implies Wk F  , since R(W ) has no finite optimum. Finally, invoking Lemma 2 proves the final claim of Theorem 1.
Proof of Lemma 3. Soudry et al. (2017) Lemma 8 proves that, with probability 1, there are at most d support vectors, and moreover, the i-th support vector zi has a positive dual variable i, such that
iS izi = u¯. Suppose there exists some   u¯, such that maxiS , zi  0. Since

i , zi = , izi = , u¯ = 0,
iS iS
we actually have , zi = 0 for all i  S. This is impossible under Assumption 3, since the support vectors span the whole space.

Proof of Lemma 4. For the sake of presentation, we leave out the subscript in zi and denote a data point by z generally. For any data point z and predictor w, let z and w denote their projection onto u¯. Let z  arg maxiS -w, z , and thus -w, z   w .

For exp, we have

1

w, R(w) =

- exp - w, z n

z

w, z

=

1 exp

-w, z

n

z

-w, z

 1 exp -w, z n

1

-w, z +

exp n

z ,w 0

-w, z

-w, z . (13)

The first part can be lower bounded as below (recall that -w, z = -w, z   w )

1 exp -w, z n

1

-w, z

= exp n

-w, u¯



1 exp

- w, u¯

n

exp -w, z -w, z exp  w  w .

(14)

To bound the second part, first notice that since we assume w, u¯  0, for any z,

w, z - u¯ = w, z + w, z - u¯ - z  w, z = w, z .

(15)

The reason is that every data point has margin at least , and thus z - u¯ - z = cu¯ for some c  0. Using eq. (15), we can bound the second part of eq. (13).

1 exp
n
z ,w 0
1 = exp
n
z ,w 0
1  exp
n
z ,w 0
1  exp
n
z ,w 0
 exp -w, u¯

-w, z -w, z

-w, u¯ exp -w, z - u¯ -w, z

-w, u¯ exp -w, z -w, z

-w, u¯ -1 . e

-1 e

(16)

On the third line eq. (15) is applied. The fourth line applies the property that f (x) = -xe-x  -1/e when x  0.

12

Under review as a conference paper at ICLR 2019

Combining eq. (13), eq. (14) and eq. (16), we get

w, R(w)  exp -w, u¯

1

exp n



w

 w

-1 e

.

As long as w  (1 + ln(n))/, w, R(w)  0.

For log, similar to eq. (13), we have

1 exp -w, z

w, R(w)

 n 1 + exp

-w, z

 1 exp -w, z n 1 + exp -w, z

1 exp -w, z

-w, z +

n 1 + exp -w, z

z ,w 0

-w, z

1

-w, z +

exp -w, z n

z ,w 0

-w, z . (17)

The second part of eq. (17) can be bounded again by eq. (16). To bound the first part of eq. (17), first notice that (recall w, u¯  0)

exp -w, z = exp -w, u¯ exp -w, z  exp -w, z .

(18)

Using eq. (18), and recall that -w, z = -w, z   w  0, we can bound the first part of eq. (17) as below.

1 exp -w, z n 1 + exp -w, z

-w, z

1 = exp
n

-w, u¯

exp -w, z 1 + exp -w, z

-w, z

 1 exp -w, u¯ exp -w, z n 1 + exp -w, z

 1 exp -w, u¯ 2n

-w, z

 1 exp 2n

-w, u¯

 w .

Combining eq. (17), eq. (19) and eq. (16), we get

-w, z (19)

w, R(w)  exp -w, u¯

1

 2n

w

-1 e

.

As long as w  2n/e, w, R(w)  0.

Proof of Theorem 2. Recall that

dW1 dt

=

R -
W1

=

-W2

· · · WL R(wprod)

,

and thus

d

W1

2 F

=

dt

W1,

dW1 dt

= - wprod, R(wprod) .

(20)

Let u¯ denote the projection onto span(u¯), and let  denote the projection onto u¯. Also let u¯W1 and W1 denote the projection of rows of W1 onto span(u¯) and u¯, respectively. Notice that

u¯wprod = WL · · · W2(u¯W1) , and wprod = WL · · · W2(W1) .

We further have

d

W1 dt

2 F

= - wprod, R(wprod) .

(21)

Let W1 = u11v1 + S. We have S 2  1,2  12,2 



W1

2 F

-

W1

2 2



D, where

1,2 is the second singular value of W1 and D is the constant introduced in Lemma 2. Then

W1 = u11 (v1) + S,

13

Under review as a conference paper at ICLR 2019

and W1 F  u11 (v1)

 + S F = 1 v1 + S F  1 v1 + dD.
F

It follows that



v1



W1 F - 1

dD  1

W1 F - W1 F

dD .
W1 2

(22)

Fix an arbitrary > 0. By Theorem 1, we can find some t0 large enough such that for any t  t0:

 1. dD/ W1 2  /3.

2.

/ - vwprod WL F ··· W1 F

1

 /3, or

/ + vwprod WL F ··· W1 F

1

 /3.

3. WL F · · · W1 F  3K/ , where K is the threshold given in Lemma 4, i.e., 1+ln(n)/ for exp, 2n/e for log.

4. R(W )  (0)/n, which implies wprod, zi  0 for all 1  i  n. By Lemma 3, there always exists a support vector z for which wprod, z  0, and therefore wprod, u¯  0.

Suppose for some t  t0, W1 F / W1 F  . By eq. (22) and bullet 1 above, v1 

2 /3. Bullet 2 above then gives /wprod WL F ··· W1 F  /3, which together with bullet 3

above implies wprod  K. Since also wprod, u¯  0, we can apply Lemma 4 and get that

wprod, R(wprod)

 0. In light of eq. (21), d

W1

2 F

/

dt



0.

On the other hand, since after t  t0,

wprod, zi



0, we have d

W1

2 F

/

dt



0 by eq. (20).

Therefore W1 F / W1 F will not increase, and since W1 F  , it will eventually drop

below , and will never exceed again. Therefore,

lim sup W1 F  . t W1 F

Since is arbitrary, we have

lim sup W1 F = 0, t W1 F

and thus limt v1, u¯ = 1. An application of Theorem 1 gives the other part of Theorem 2.

C OMITTED PROOFS FROM SECTION 3

Proof of Lemma 5. Given W, V  B(R), we need to show that R(W )-R(V )  (R) W - V for some (R).
Consider k = 1 first. Let w = (WL · · · W1) , and v = (VL · · · V1) . Since | |  G, R(w) , R(v)  G. We have

R - R W1 V1

= W2 · · · WL R(w) - V2 · · · VL R(v)

 W2 · · · WL R(w) - V2 W3 · · · WL R(w)

+ V2 W3 · · · WL R(w) - V2 · · · VL R(v)  RL-2G W2 - V2
+ V2 W3 · · · WL R(w) - V2 · · · VL R(v)  RL-2G W - V

+ V2 W3 · · · WL R(w) - V2 · · · VL R(v) .

(23)

14

Under review as a conference paper at ICLR 2019

Proceeding in this way, we can get

R - R  (L - 1)RL-2G W - V + RL-1 R(w) - R(v) . W1 V1 Since zi  1, is -Lipschitz, we have
R(w) - R(v)   w - v  LRL-1 W - V ,

(24) (25)

where the last inequality follows from a similar one-by-one replacement procedure as in eq. (23). Combining eq. (24) and eq. (25), we get for R  1,

R - R  (L - 1)RL-2G + LR2L-2 W - V  2LR2L-2( + G) W - V . W1 V1 The same procedure can be done for other layers, and together

R(W ) - R(V )  2L2R2L-2( + G) W - V .

Proof of Lemma 6. Recall that if W (t), W (t + 1)  B(R) and t = 1/(R),

R W (t + 1) - R W (t)  R W (t) , -tR W (t)

+ (R)t2 R W (t) 2

=- 1

2
R W (t)

2(R)

= - t R W (t)

2
.

2

2
(26)

Suppose W (t)  B(R) for all t. By Assumption 2 and eq. (26),

R W (1)  R W (0) - 1

2
R W (0) < R W (0) .

2(R)

By eq. (26), gradient descent never increases the risk, and thus for all t  1, R W (t) 

R W (1) < R W (0) . In exactly the same way as in the proof of Lemma 1, one can show

that there exists some constant (R) > 0, so that R/W1(t) F  (R) for all t. Invoking eq. (26) again, we will get


R W (0) 

1

(R)2 = ,

2(R)

t=0

which is a contradiction. Therefore W (t) must go out of B(R) at some time.

Next we prove Theorem 3 and 4. The proofs depend on several lemmas which are similar to the gradient flow ones. The following Lemma 7 is similar to Lemma 1.
Lemma 7. Under Assumption 1, 2, 4, and 5, gradient descent ensures that

· max1kL Wk(t) F is unbounded.

·

 t=0

t

=

.

· For any R > 0, t:W (t)B(R) t < .

Proof. By Assumption 5, we always have that W (t)  B(Rt). Since (Rt) = 2L2Rt2L-2(+G)  RtL-1G, we have for any 1  k  L,

Wk(t + 1) F 

Wk(t) F + t

R  Wk (t)

F



1 Wk(t) F + (Rt)

R  Wk (t)

F



Wk (t)

F

+

1 (Rt)

RtL-1

G

 Wk(t) F + 1.

(27)

15

Under review as a conference paper at ICLR 2019

Moreover, Lemma 6 shows that Rt  . Since Rt+1 = Rt as long as W (t + 1)  B(Rt), max1kL Wk(t) F is unbounded.

It then follows that for any t, by Cauchy-Schwarz,

 
t-1 t-1

     R W ( )

 =0

 =0


2 t-1
    R W ( )
 =0

2   .

since by eq. (26),

t-1

 R W ( )

 =0

we have

 t=0

t

=

.

2
 2R W (0) - 2R W (t)  2R W (0) ,

Since under Assumptions 4 and 5 gradient descent never increases the risk, it can be shown in exactly
the same as in the proof of Lemma 1 that, for W (t)  B(R), R/W1(t) F  (R) for some constant (R) > 0. Invoking eq. (26) again, we get that t:W (t)B(R) t < .

The next lemma is an analogy to Lemma 2. Lemma 8. Under Assumption 1 and 4, the gradient descent iterates satisfy the following properties:

· For any 1  k  L,

Wk

2 F

-

Wk

2 2



D

+

2R

W (0) .

·

For any 1  k < L,

vk+1, uk

2

 1 - D+3R(W (0))+

Wk+1 (0)

2 2

+

Wk (0)

/2
2

Wk+1

.2
2

· Suppose max1kL Wk F  , then

/ , vwprod

L k=1

Wk F

1

 1.

Proof. Recall that for any W ,

R Wk+1 Wk+1

= Wk+1 · · · WL R(wprod)

W1

· · · Wk

R = Wk Wk .

For gradient descent iterates, summing eq. (28) from 0 to t - 1, we get

t-1
Wk+1(t)Wk+1(t) - Wk+1(0)Wk+1(0) + 2
 =0

R Wk+1( )

R Wk+1( )

t-1
= Wk(t)Wk (t) - Wk(0)Wk (0) + 2
 =0
For any 1  k  L and any t, let

R Wk( )

R Wk( )

.

and We have

Pk(t) 2 =

t-1
Pk(t) = 2
 =0

R Wk( )

R Wk( )

,

t-1
Qk(t) = 2
 =0

R Wk( )

R .
Wk( )

Qk(t) 2  tr Qk(t) = tr Pk(t) . Moreover, invoking eq. (26),

L
tr
k=1

Pk (t)

L t-1
= 2
k=1  =0

R Wk( )

2 F

t-1 2
= 2 R W ( )
 =0

t-1 2
  R W ( )

 =0

 2R W (0) - 2R W (t)

 2R W (0) .

(28) (29)
(30)

16

Under review as a conference paper at ICLR 2019

Still let k(t), uk(t) and vk(t) denote the first singular value, left singular vector and right singular vector of Wk(t). We can then proceed basically in the same way as in the proof of Lemma 2. For example, eq. (6) becomes

k2(t)  k2+1(t) - Ak,k+1(t) 2 - Pk(t) 2  k2+1(t) - Ak,k+1(t) 2 - tr Pk(t) , (31) while eq. (7) becomes

Wk (t)

2 F

=

Wk+1(t)

2 F

+

Wk (0)

2 F

-

Wk+1(0)

2 F

-

tr

Pk (t)

+ tr

Qk+1(t)

.

(32)

Summing eq. (31) and eq. (32) from k to L - 1, and invoke eq. (30), we get

L-1

Wk (t)

2 F

-

Wk (t)

2 2

 D - tr

Pk (t)

+ tr

QL(t)

+

tr Pk (t)  D + 2R W (0) .

k =k

To prove singular vectors get aligned, we can still proceed in nearly the same way as in the proof of

Lemma 2. eq. (9) becomes

uk Wk+1Wk+1uk  k2 -

Wk (0)

2 2

-

Qk+1(t) 2,

(33)

while eq. (10) becomes

uk Wk+1Wk+1uk  uk, vk+1 2k2+1 + D + 2R W (0) .

(34)

Combining eq. (33) and eq. (34)

k2  uk, vk+1 2k2+1 + D + 2R W (0) + Qk+1(t) 2 + Wk(0) 22.

(35)

Similar to eq. (33), we can get

k2  vk+1WkWk vk+1  k2+1 -

Wk+1(0)

2 2

-

Pk(t) 2,

and thus eq. (12) becomes

k2 k2+1



1-

Wk+1(0)

2 2

+

k2+1

Pk(t) 2 .

(36)

Combining eq. (35) and eq. (36), we get

uk, vk+1 2  1 - D +

Wk (0)

2 2

+

Wk+1(0)

2 2

+

3R

k2+1

W (0)

.

The final claim of Lemma 8 can be proved in exactly the same way as Lemma 2.

Proof of Theorem 3. Summing eq. (32), we know that for any two different layers j > k,

Wk (t)

2 F

-

Wj (t)

2 F

=

Wk (0)

2 F

-

Wj (0)

2 F

- tr

Pk (t)

+ tr

Qj (t)

.

Recall eq. (30), we know that

Wk (t)

2 F

-

Wj (t)

2 F

-

Wk (0)

2 F

-

Wj (0)

2 F

 2R W (0) .

(37)

In other words, the difference between the squares of Frobenius norms of any two layers is still bounded.

The proof then goes in the same way as the proof of Theorem 1. Suppose the risk is always above

> 0. Then there exists some c( ) > 0 such that R(wprod)  c( ). By Lemma 8, there exists

some C such that if min1kL Wk(t) F > C, WL(t) · · · W2(t)  CL/2. Lemma 7, t: Wk(t) F C for some k t is finite. On the other hand, by Lemma 7,

By eq. (37) and

 i=0

t

=

,

and

thus t: Wk(t) F >C for all k t = . Therefore we have, by invoking eq. (26),

2

2R W (0)  t R W (t)

t=0

 R 2

 t
t=0

W1(t)

CL

 c( ) 2

t

t: Wk(t) F >C for all k

= ,

which is a contradiction. Therefore R W (t)  0, and since it has no finite optimum, Wk F  . The other results follow from Lemma 7.

17

Under review as a conference paper at ICLR 2019

Proof of Theorem 4. Recall that

R W1

= W2

· · · WL R(wprod),

and thus

W1(t + 1)

2 F

=

=

W1(t)

2 F

- 2t

W1(t)

2 F

- 2t

R W1(t), W1(t)

+ t2

R W1(t)

2 F

wprod(t), R wprod(t)

+ t2

R W1(t)

2
.
F

If wprod, zi  0 for all i, then W1(t + 1) F  W1(t) F .

Also recall that W1(t) denote the projection of rows of W1(t) onto u¯, the orthogonal complement of span(u¯). We have

W1(t + 1)

2 F



W1(t)

2 F

- 2t

=

W1(t)

2 F

- 2t

Invoking eq. (26) again gives

R W1(t), W1(t)

+ t2

R W1(t)

2 F

wprod(t), R wprod(t)

+ t2

R W1(t)

2
.
F

(38)

t2

R W1(t)

2
 t
F

R

W (t)

2
 2 R W (t) - R W (t + 1)

.

(39)

The proof then goes in almost the same way as the proof of Theorem 2. For any > 0, we can find some large enough time t0, such that for any t  t0,

1. W1(t) F / W1(t) F  implies that wprod(t), R wprod(t)  0.

2. wprod(t), zi  0 for all i, and thus W1(t + 1) F  W1(t) F . 
3. W1(t) F  1+ 2R(W (0))/ .

Suppose at some time t1  t0, W1(t1) F / W1(t1) F  . As long as this still holds, in

light of bullet (1) above, eq. (38) and eq. (39),

W1

2 F

will increase by at most 2R

W (t1)



2R W (0) . On the other hand, W1 F  , and thus there exists some t2 > t1 such that

W1(t2) F / W1(t2) F < .

Let t3 denote the smallest time after t2 such that W1(t3) F / W1(t3) F  (if it exists). Recall that W1(t + 1) F  W1(t) F + 1 for any t  0, and W1(t + 1) F  W1(t) F for any t  t0, we have

W1(t3) F  W1(t3) F

W1(t3) F  W1(t3 - 1) F

W1(t3 - 1) W1(t3 - 1)

F F

+

1

<

+

1

W1(t3 - 1)

.
F

After t3,

W1

2 F

will increase by at most 2R

W (0) , and thus

W1 F will increase by at

most 2R W (0) . Therefore, for any t4  t3, as long as W1(t4) F / W1(t4) F  , we

have

W1(t4) F  W1(t4) F

W1(t4) F

W1(t3) F

W1(t3) F + 2R W (0) 
W1(t3) F



+

1 W1(t3 - 1) F +

2R W (0) 2 ,
W1(t3) F

18

Under review as a conference paper at ICLR 2019

 since W1(t) F  1+ 2R(W (0))/ after t0. In other words,

lim sup W1 F  2 . t W1 F

Since is arbitrary, we have

lim sup W1 F = 0, t W1 F

and thus limt v1, u¯ = 1.

19

