Under review as a conference paper at ICLR 2019
LOGICALLY-CONSTRAINED NEURAL FITTED Q-ITERATION
Anonymous authors Paper under double-blind review
ABSTRACT
This paper proposes a method for efficient training of the Q-function for continuousstate Markov Decision Processes (MDP), such that the traces of the resulting policies satisfy a Linear Temporal Logic (LTL) property. The logical property is converted into a limit deterministic Bu¨chi automaton with which a synchronized product MDP is constructed. The control policy is then synthesized by a reinforcement learning algorithm assuming that no prior knowledge is available from the MDP. The proposed method is evaluated in a numerical study to test the quality of the generated control policy and is compared against conventional methods for policy synthesis such as MDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted value iteration).
1 INTRODUCTION
Markov Decision Processes (MDPs) are extensively used as a family of stochastic processes in automatic control, computer science, economics, etc. for modeling of sequential decision-making problems. Reinforcement Learning (RL) is a machine learning algorithm that is widely used to train an agent to interact with an MDP when the stochastic behavior of the MDP is initially unknown. However, conventional RL is mostly focused on problems in which MDP states and actions are finite. Nonetheless, many interesting real-world tasks, require actions to be taken in response to high-dimensional or real-valued sensory inputs (Doya, 2000). For example, consider the problem of drone control in which the drone state is represented as its Euclidean position (x, y, z)  R3. Apart from state space discretization and running vanilla RL, an alternative solution is to use an approximation function which is achieved via regression over the set of samples. At a given state, this function is able to estimate the value of the expected reward. Therefore, in continuous-state RL, this approximation replaces conventional RL state-action-reward look-up table which is used in finite-state MDPs. A number of methods are available to approximate the expected reward, e.g. CMACs (Sutton, 1996), kernel-based modelling (Ormoneit & Sen, 2002), tree-based regression (Ernst et al., 2005), basis functions (Busoniu et al., 2010), etc. Among these methods, neural networks offer great promise in reward modelling due to their ability to approximate any non-linear function (Hornik, 1991). There exist numerous successful applications of neural networks in RL for infinite or large-state space MDPs, e.g. Deep Q-networks (Mnih et al., 2015), TD-Gammon (Tesauro, 1995), Asynchronous Deep RL (Mnih et al., 2016), Neural Fitted Q-iteration (Riedmiller, 2005), CACLA (Van Hasselt & Wiering, 2007).
In this paper, we propose to employ feedforward networks to synthesize a control policy for infinitestate MDPs such that the generated traces satisfy a Linear Temporal Logic (LTL) property with a positive probability. LTL allows to specify complex mission tasks in a rich time-dependent formal language. By employing LTL we are able to express complex high-level control objectives that are hard to express and achieve for other methods from vanilla RL (Sutton & Barto, 1998; Smith et al., 2011) to more recent developments such as Policy Sketching (Andreas et al., 2017). Examples include liveness and cyclic properties, where the agent is required to make progress while concurrently executing components, to take turns in critical sections or to execute a sequence of tasks periodically. This work can be the foundation for further research in a number of directions including safe or logically-constrained deep reinforcement learning.
1

Under review as a conference paper at ICLR 2019

2 BACKGROUND

2.1 PROBLEM FRAMEWORK
We adopt standard MDP with continuous-state space as the model for agent-environment interaction (Durrett, 1999):
Definition 2.1 (Continuous-state Space MDP) The tuple M = (S, A, s0, P, AP, L) is an MDP over a set of states S = Rn, where A is a finite set of actions, s0 is the initial state and P : B(Rn) × S × A  [0, 1] is a Borel-measurable transition kernel which assigns to any state and any action a probability measure on the Borel space (Rn, B(Rn)) (Durrett, 1999). AP is a finite set of atomic propositions and a labeling function L : S  2AP assigns to each state s  S a set of atomic propositions L(s)  2AP.
A finite-state MDP is a special case of continuous-state space MDP in which |S| <  and P : S × A × S  [0, 1] is the transition probability function. The transition function P induces a matrix which is usually known as transition probability matrix in the literature.
Definition 2.2 (Path) In a continuous-state MDP M, an infinite path  starting at s0 is a sequence of states  = s0 -a0 s1 -a1 ... such that every transition si -ai si+1 is possible in M, i.e. si+1 belongs to the smallest Borel set B such that P (B|si, ai) = 1 (or in a discrete MDP, P (si+1|si, ai) > 0). We might also denote  as s0.. to emphasize that  starts from s0.
Definition 2.3 (Stationary Policy) A stationary (randomized) policy Pol : S × A  [0, 1] is a mapping from each state s  S, and action a  A to the probability of taking action a in state s. A deterministic policy is a degenerate case of a randomized policy which outputs a single action at a given state, that is s  S, a  A, Pol (s, a) = 1.
In an MDP M, we define a function R : S × A  R+0 that denotes the immediate scalar bounded reward received by the agent from the environment after performing action a  A in state s  S.

Definition 2.4 (Expected (Infinite-Horizon) Discounted Reward): For a policy Pol on an MDP M, the expected discounted reward is defined as (Sutton & Barto, 1998):


U Pol (s) = EPol [ n R(sn, P ol(sn))|s0 = s],
n=0

(1)

where EPol [·] denotes the expected value given that the agent follows policy Pol ,   [0, 1) is a discount factor and s0, ..., sn is the sequence of states generated by policy Pol up to time step n.

Definition 2.5 (Optimal Policy) Optimal policy Pol  is defined as follows:

Pol (s) = arg sup U Pol (s),
P olD

where D is the set of all stationary deterministic policies over the state space S.

Theorem 2.1 In any MDP M with bounded reward function and finite action space, if there exists an optimal policy, then that policy is stationary and deterministic. (Puterman, 2014)(Cavazos-Cadena et al., 2000).
An MDP M is said to be solved if the agent discovers an optimal policy Pol  : S  A to maximize the expected reward. From Definitions 2.4 and 2.5, it means that the agent has to take actions that return the highest expected reward. Note that the reward function for us as the designer is known in the sense that we know over which state (or under what circumstances) the agent will receive a given reward. The reward function specifies what the agent needs to achieve but not how to achieve it. Thus, the objective is that the agent itself comes up with an optimal policy. In the supplementary materials in Section A.1, we present fundamentals of approaches introduced in this paper for solving infinite-state MDPs.

2

Under review as a conference paper at ICLR 2019

3 LINEAR TEMPORAL LOGIC PROPERTIES

In order to specify a set of desirable constraints (i.e. properties) over the agent policy we employ
Linear Temporal Logic (LTL) (Pnueli, 1977). An LTL formula can express a wide range of properties, such as safety and persistence. LTL formulas over a given set of atomic propositions AP are syntactically defined as

 ::= true |   AP |    | ¬ |  |   .

(2)

We define the semantics of LTL formula next, as interpreted over MDPs. Given a path , the i-th state of  is denoted by [i] where [i] = si. Furthermore, the i-th suffix of  is [i..] where [i..] = si -ai si+1 -a-i-+1 si+2 -a-i-+2 si+3 -a-i-+3 ... .

Definition 3.1 (LTL Semantics) For an LTL formula  and for a path , the satisfaction relation  |=  is defined as

 |=   AP    L([0])

 |=   [1..] |= 

 |= 1  2   |= 1   |= 2  |= 1  2  j  N  {0} s.t.

 |= ¬   |= 

[j..] |= 2 & i, 0  i < j, [i..] |= 1

Using the until operator we are able to define two temporal modalities: (1) eventually,  = true; and (2) always,  = ¬¬. An LTL formula  over AP specifies the following set of words: Words() = {  (2AP) s.t.  |= }.

Definition 3.2 (Policy Satisfaction) We say that a stationary deterministic policy Pol satisfies an LTL formula  if:
P[L(s0)L(s1)L(s2)...  Words()] > 0,
where every transition si  si+1, i = 0, 1, ... is constructed by taking action Pol (si) at state si.
For an LTL formula , an alternative method to express the set of associated words, i.e., W ords(), is to employ an automaton. Limit Deterministic Bu¨chi Automatons (LDBA) (Sickert et al., 2016) are one of the most succinct and simplest automatons for that purpose (Sickert & Kret´insky`, 2016). We need to first define a Generalized Bu¨chi Automaton (GBA) and then we formally introduce an LDBA.

Definition 3.3 (Generalized Bu¨ chi Automaton) A GBA N = (Q, q0, , F, ) is a structure where Q is a finite set of states, q0  Q is the set of initial states,  = 2AP is a finite alphabet, F = {F1, ..., Ff } is the set of accepting conditions where Fj  Q, 1  j  f , and  : Q ×   2Q is a
transition relation.

Let  be the set of all infinite words over . An infinite word w   is accepted by a GBA N

if there exists an infinite run   Q starting from q0 where [i + 1]  ([i], [i]), i  0 and for

each Fj  F

inf ()  Fj = ,

(3)

where inf () is the set of states that are visited infinitely often in the sequence . The accepted language of the GBA N is the set of all infinite words accepted by the GBA N and it is denoted by L (N).

Definition 3.4 (LDBA) A GBA N = (Q, q0, , F, ) is limit deterministic if Q can be partitioned into two disjoint sets Q = QN  QD, such that (Sickert et al., 2016):

· (q, )  QD and |(q, )| = 1 for every state q  QD and for every corresponding   ,

· for every Fj  F, Fj  QD.

Intuitively, an LDBA is a GBA that has two partitions: initial (QN ) and accepting (QD). The accepting part includes all the accepting states and has deterministic transitions.

3

Under review as a conference paper at ICLR 2019

4 LOGICALLY-CONSTRAINED NEURAL FITTED Q-ITERATION

In this section, we propose an algorithm based on Neural Fitted Q-iteration (NFQ) that is able to synthesize a policy that satisfies a temporal logic property. We call this algorithm LogicallyConstrained NFQ (LCNFQ). We relate the notion of MDP and automaton by synchronizing them to create a new structure that is first of all compatible with RL and second that embraces the logical property.

Definition 4.1 (Product MDP) Given an MDP M(S, A, s0, P, AP, L) and an LDBA

N(Q, q0, , F, ) with  = 2AP, the product MDP is defined as (M  N) = MN(S, A,

Ls0,(Ps,q,)A=Pqa, LndF, F),

where S = S S is the set of

× Q, s0 = (s0, q0), AP accepting states such that

= for

Q, L = S each s =

× Q  2Q such (s, q)  F, q

that  F.

The intuition behind the transition kernel P  is that given the current state (si, qi) and action a the

new state is (sj, qj) where sj  P (·|si, a) and qj  (qi, L(sj)).

Definition 4.2 (Absorbing Set) We define the set A  B(S) to be an absorbing set if P (A|s, a) = 1 for all s  A and for all a  A. An absorbing set is called accepting if it includes F. We denote the set of all accepting absorbing sets by A.

Remark 4.1 The defined notion of absorbing set in continuous-state MDPs is equivalent to the notion of maximum end components in finite-state MDPs. Intuitively, once a trace ends up in an absorbing set (or a maximum end component) it can never escape from it (Tkachev et al., 2017).

The product MDP encompasses transition relations of the original MDP and the structure of the Bu¨chi automaton, thus it inherits characteristics of both. Therefore, a proper reward function can lead

the RL agent to find a policy that is optimal and that respects both the original MDP and the LTL property . In this paper, we propose an on-the-fly reward function that observes the current state s, the action a and observes the subsequent state s and gives the agent a scalar value according to the

following rule:

R(s, a) =

rp if s / A, s  A, rn otherwise,

(4)

where rp = M + y × m × rand (s) is a positive reward and rn = y × m × rand (s) is a neutral reward where y  {0, 1} is a constant, 0 < m M , and rand : S  (0, 1) is a function that generates a random number in (0, 1) for each state s. The role of the function rand is to break the

symmetry in LCNFQ.

In LCNFQ, the temporal logic property is initially specified as a high-level LTL formula . The LTL
formula is then converted to an LDBA N to form a product MDP MN (see Definition 4.1). In order to use the experience replay technique we let the agent run in the MDP and we store all episode traces, i.e. experiences, in the form of (s, a, s , R(s, a), q). Here s = (s, q) is the current state in the product MDP, a is the chosen action, s = (s , q ) is the resulting state, and R(s, a) is the reward. The set of past experiences is called the sample set E.

We employ n separate feedforward neural nets where n = |Q| and Q is the finite cardinality of the automaton N. Each neural net is associated with a state in the LDBA and together the neural nets approximate the Q-function in the product MDP. For each automaton state qi  Q the associated neural net is called Bqi : S × A  R. Once the agent is at state s = (s, qi) the neural net Bqi is used for the local Q-function approximation. The set of neural nets acts as a global hybrid Q-function approximator Q : S × A  R. Note that the neural nets are not fully decoupled. For example, assume that by taking action a in state s = (s, qi) the agent is moved to state s = (s , qj) where qi = qj. According to (13) the weights of Bqi are updated such that Bqi (s, a) has minimum possible error to R(s, a) +  maxa Bqj (s , a ). Therefore, the value of Bqj (s , a ) affects Bqi (s, a).
Let qi  Q be a state in the LDBA. Then define Eqi := {(·, ·, ·, ·, x)  E|x = qi} as the set of experiences within E that is associated with state qi, i.e., Eqi is the projection of E onto qi. Once the experience set E is gathered, each neural net Bqi is trained by its associated experience set Eqi . At each iteration a pattern set Pqi is generated based on Eqi :
Pqi = {(inputl, targetl), l = 1, ..., |Eqi |)},

4

Under review as a conference paper at ICLR 2019
Algorithm 1: LCNFQ input :MDP M, a set of transition samples E output :Approximated Q-function 1 initialize all neural nets Bqi with (s0, qi, a) as the input and rn as the output where a  A is a random action 2 repeat 3 for qi = |Q| to 1 do 4 Pqi = {(inputl, targetl), l = 1, ..., |Eqi |)} 5 inputl = (sl, al) 6 targetl = R(sl, al) +  max Q(sl , a )
a
7 where (sl, al, sl , R(sl, al), qi)  Eqi 8 Bqi  Rprop(Pqi ) 9 end 10 until end of trial
where inputl = (sl, al) and targetl = R(sl, al) +  max Q(sl , a ) such that
a
(sl, al, sl , R(sl, al), qi)  Eqi . The pattern set is used to train the neural net Bqi . We use Rprop (Riedmiller & Braun, 1993) to update the weights in each neural net, as it is known to be a fast and efficient method for batch learning (Riedmiller, 2005). In each cycle of LCNFQ (Algorithm 1), the training schedule starts from networks that are associated with accepting states of the automaton and goes backward until it reaches the networks that are associated to the initial states. In this way we allow the Q-value to back-propagate through the networks. LCNFQ stops when a satisfying policy is generated.
Remark 4.2 One might argue that a single neural net can be used to approximate the global Qfunction. In order to do so we first applied integer encoding to inputs in which an integer number is assigned to each automaton state in the input sl = (sl, ql). As it was expected, with this encoding we observed poor performance since this encoding allows the network to assume an ordinal relationship between automaton states. We then used one hot encoding (Harris & Harris, 2010) in which each automaton state qi  Q is replaced by a binary number of length |Q| with all bits zero except the ith bit which is one. However, the performance of the trained network was poor and therefore, we turned to the final solution of employing n separate neural nets that work together in a hybrid manner to approximate the global Q-function.
Recall that the reward function (4) only returns a positive value when the agent has a transition to an accepting state in the product MDP. Therefore, if accepting states are reachable, by following this reward function the agent is able to come up with a policy Pol  that leads to the accepting states. This means that the trace of read labels over S (see Definition 4.1) results in an automaton state to be accepting. Therefore, the trace over the original MDP is a trace that satisfies the given logical property. Recall that the optimal policy has the highest expected reward comparing to other policies. Consequently, the optimal policy has the highest expected probability of reaching to the accepting set, i.e. satisfying the LTL property.
The next section studies state space discretization as the most popular alternative approach to solving infinite-state MDPs.
5 VORONOI QUANTIZER
Inspired by (Lee & Lau, 2004), we propose a version of Voronoi quantizer that is able to discretize the state space of the product MDP S. In the beginning, C is initialized to consist of just one c1, which corresponds to the initial state. This means that the agent views the entire state space as a homogeneous region when no apriori knowledge is available. Subsequently, when the agent explores, the Euclidean distance between each newly visited state and its nearest neighbor is calculated. If this distance is greater than a threshold value  called "minimum resolution", or if the new state s has a never-visited automaton state then the newly visited state is appended to C. Therefore, as the agent continues to explore, the size of C would increase until the relevant parts of the state space are
5

Under review as a conference paper at ICLR 2019

Algorithm 2: Episodic VQ
input :MDP M, minimum resolution  output :Approximated Q-function Q 1 initialize Q(c1, a) = 0, a  A 2 repeat 3 initialize c1 = initial state 4 set c = c1 5  = arg maxaA Q(c, a) 6 repeat 7 execute action  and observe the next state (s , q) 8 if Cq is empty then 9 append cnew = (s , q) to Cq 10 initialize Q(cnew , a) = 0, a  A
11 else 12 determine the nearest neighbor cnew within Cq 13 if cnew = c then 14 if ||c - (s , q)||2 >  then 15 append cnew = (s , q) to Cq 16 initialize Q(cnew , a) = 0, a  A
17 end
18 else 19 Q(c, ) = (1 - µ)Q(c, ) + µ[R(c, ) +  max(Q(cnew , a ))]
a
20 end
21 end 22 c = cnew
23 until end of trial
24 until end of trial

partitioned. In our algorithm, the set C has n disjoint subsets where n = |Q| and Q is the finite set of

states of the automaton. Each subset Cqj , j = 1, ..., n contains the centroids of those Voronoi cells

that have the form of ciqj = (·, qj), i.e.

m i

ciqj

=

Cqj

and

C

=

n j=1

Cqj

.

Therefore,

a

Voronoi

cell

{(s, qj)  S, ||(s, qj) - ciqj ||2  ||(s, qj) - ciqj ||2}

is defined by the nearest neighbor rule for any i = i. The VQ algorithm is presented in Algorithm 2. The proposed algorithm consist of several resets at which the agent is forced to re-localize to its initial state s0. Each reset is called an episode, as such in the rest of the paper we call this algorithm episodic VQ.

6 FITTED VALUE ITERATION

In this section we propose a modified version of FVI that can handle the product MDP. The global value function v : S  R, or more specifically v : S × Q  R, consists of n number of sub-value functions where n = |Q|. For each qj  Q, the sub-value function vqj : S  R returns the value
the states of the form (s, qj). As we will see shortly, in a same manner as LCNFQ, the sub-value
functions are not decoupled.

Let P (dy|s, a) be the distribution over S for the successive state given that the current state is s and the current action is a. For each state (s, qj), the Bellman update over each sub-value function vqj is defined as:

T vqj (s) = sup{ v(y)P (dy|(s, qj), a)},
aA

(5)

where T is the Bellman operator (Herna´ndez-Lerma & Lasserre, 2012). The update in (5) is a special

case of general Bellman update as it does not have a running reward and the (terminal) reward is

embedded via value function initialization. The value function is initialized according to the following

rule:

v(s) =

rp rn

if s  A, otherwise.

(6)

6

Under review as a conference paper at ICLR 2019

Algorithm 3: FVI
input :MDP M, a set of samples {si }ki=1 = {(si, qj)}ki=1 for each qj  Q, Monte Carlo sampling number Z, smoothing parameter h
output :approximated value function Lv 1 initialize Lv 2 sample YaZ (si, qj), qj  Q, i = 1, ..., k , a  A 3 repeat 4 for j = |Q| to 1 do 5 for each state (si, qj ) and for each action a, calculate Ia((si, qj )) = 1/Z yYZa (si,qj) Lv(y) 6 for each state (si, qj ), Lvqj ((si, qj )) = supaA{Ia((si, qj ))} 7 end
8 until end of trial

The main hurdle in executing the Bellman operator in continuous state MDPs, as in (5), is that no analytical representation of the value function v and also sub-value functions vqj , qj  Q is
available. Therefore, we employ an approximation method by introducing the operator L. The
operator L constructs an approximation of the value function denoted by Lv and of each sub-value function vqj which we denote by Lvqj . For each qj  Q the approximation is based on a set of points {(si, qj)}ki=1  S which are called centers. For each qj, the centers i = 1, ..., k are distributed uniformly over S such that they uniformly cover S.

We employ a kernel-based approximator for our FVI algorithm. Kernel-based approximators have attracted a lot of attention mostly because they perform very well in high-dimensional state spaces (Stachurski, 2008). One of these methods is the kernel averager, which can be represented by the following expression for each state (s, qj):

Lvqj (s) =

k i=1

K (si

k i=1

K

- s)vqj (si (si - s)

)

,

(7)

where the kernel K : S  R is a radial basis function, such as e-|s-si|/h, and h is smoothing
parameter. Each kernel has a center si and the value of it decays to zero as s diverges from si. This means that for each qj  Q the approximation operator L in (7) is a convex combination of the values
of the centers {si}ki=1 with larger weight given to those values vqj (si) for which si is close to s. Note that the smoothing parameter h controls the weight assigned to more distant values (see Section A.2).

In order to approximate the integral in Bellman update (5) we use a Monte Carlo approximation

(Shonkwiler & Mendivil, 2009). For each center (si, qj) and for each action a, we sample the next
state yaz(si, qj) for z = 1, ..., Z times and append it to set of Z subsequent states YaZ (si, qj). We then replace the integral with

1 Ia(si, qj) = Z

Z

Lv(yaz(si, qj)).

z=1

(8)

The approximate value function Lv is initialized according to (6). In each cycle of FVI, the approximate Bellman update is first performed over the sub-value functions that are associated with accepting states of the automaton, i.e. those that have initial value of rp, and then goes backward until it reaches the sub-value functions that are associated to the initial states. In this manner, we allow the state values to back-propagate through the transitions that connects the sub-value function via (8). Once we have the approximated value function, we can generate the optimal policy by following the maximum value (Algorithm 3).

7 EXPERIMENTAL RESULTS
We describe a mission planning architecture for an autonomous Mars-rover that uses LCNFQ to follow a mission on Mars. The scenario of interest is that we start with an image from the surface of Mars and then we add the desired labels from 2AP, e.g. safe or unsafe, to the image. We assume that we know the highest possible disturbance caused by different factors (such as sand storms) on the

7

Under review as a conference paper at ICLR 2019

(a) Melas Chasma

(b) Coprates Chasma

Figure 1: Melas Chasma and Coprates Chasma, in the central and eastern portions of Valles Marineris. Map color spectrum represents elevation, where red is high and blue is low. (Image courtesy of NASA, JPL, Caltech and University of Arizona.)

rover motion. This assumption can be set to be very conservative given the fact that there might be some unforeseen factors that we did not take into account.
The next step is to express the desired mission in LTL format and run LCNFQ on the labeled image before sending the rover to Mars. We would like the rover to satisfy the given LTL property with the highest probability possible starting from any random initial state (as we can not predict the landing location exactly). Once LCNFQ is trained we use the network to guide the rover on the Mars surface.
We compare LCNFQ with Voronoi quantizer and FVI and we show that LCNFQ outperforms these methods.
7.1 MDP STRUCTURE
In this numerical experiment the area of interest on Mars is Coprates quadrangle, which is named after the Coprates River in ancient Persia (see Section A.3). There exist a significant number of signs of water, with ancient river valleys and networks of stream channels showing up as sinuous and meandering ridges and lakes. We consider two parts of Valles Marineris, a canyon system in Coprates quadrangle (Fig. 1). The blue dots, provided by NASA, indicate locations of recurring slope lineae (RSL) in the canyon network. RSL are seasonal dark streaks regarded as the strongest evidence for the possibility of liquid water on the surface of Mars. RSL extend downslope during a warm season and then disappear in the colder part of the Martian year (McEwen et al., 2014). The two areas mapped in Fig. 1, Melas Chasma and Coprates Chasma, have the highest density of known RSL.
For each case, let the entire area be our MDP state space S, where the rover location is a single state s  S. At each state s  S, the rover has a set of actions A = {left, right, up, down, stay} by which it is able to move to other states: at each state s  S, when the rover takes an action a  {left, right, up, down} it is moved to another state (e.g., s ) towards the direction of the action with a range of movement that is randomly drawn from (0, D] unless the rover hits the boundary of the area which forces the rover to remain on the boundary. In the case when the rover chooses action a = stay it is again moved to a random place within a circle centered at its current state and with radius d D. Again, d captures disturbances on the surface of Mars and can be tuned accordingly.
With S and A defined we are only left with the labeling function L : S  2AP which assigns to each state s  S a set of atomic propositions L(s)  2AP. With the labeling function, we are able to divide the area into different regions and define a logical property over the traces that the agent generates. In this particular experiment, we divide areas into three main regions: neutral, unsafe and target. The target label goes on RSL (blue dots), the unsafe label lays on the parts with very high elevation (red colored) and the rest is neutral. In this example we assume that the labels do not overlap each other.
Note that when the rover is deployed to its real mission, the precise landing location is not known. Therefore, we should take into account the randomness of the initial state s0. The dimensions of the area of interest in Fig. 1.a are 456.98 × 322.58 km and in Fig. 1.b are 323.47 × 215.05 km. The
8

Under review as a conference paper at ICLR 2019

¬t1 ¬t2

t2

¬u  ¬t

t

start q1 t1 q2 t2 q3

start q1 t q2

u u
q5 u
(a) LDBA for property in Equation (9)

u
q3 u
(b) LDBA for property in Equation (10)

Figure 2: Generated LDBAs

diameter of each RSL is 19.12 km. Other parameters in this numerical example have been set as D = 2 km, d = 0.02 km, the reward function parameter y = 1 for LCNFQ and y = 0 for VQ and FVI, M = 1, m = 0.05 and AP = {neutral, unsafe, target 1, target 2}.

7.2 SPECIFICATIONS

The first control objective in this numerical example is expressed by the following LTL formula over Melas Chasma (Fig. 1.a):

(t1  t2)  (t2  t2)  (u  u),

(9)

where n stands for "neutral", t1 stands for "target 1", t2 stands for "target 2" and u stands for "unsafe". Target 1 are the RSL (blue bots) on the right with a lower risk of the rover going to unsafe region and the target 2 label goes on the left RSL that are a bit riskier to explore. Conforming to (9) the rover has to visit the target 1 (any of the right dots) at least once and then proceed to the target 2 (left dots) while avoiding unsafe areas. Note that according to (u  u) in (9) the agent is able to go to unsafe area u (by climbing up the slope) but it is not able to come back due to the risk of falling. With (9) we can build the associated Bu¨chi automaton as in Fig. 2.a.

The second formula focuses more on safety and we are going to employ it in exploring Coprates Chasma (Fig. 1.b) where a critical unsafe slope exists in the middle of this region.

t  (t  t)  (u  u)

(10)

In (10), t refers to "target", i.e. RSL in the map, and u stands for "unsafe". According to this LTL formula, the agent has to eventually reach the target (t) and stays there ( (t  t)). However, if the agent hits the unsafe area it can never comes back and remains there forever ( (u  u)). With
(10) we can build the associated Bu¨chi automaton as in Fig. 2.b. Having the Bu¨chi automaton for
each formula, we are able to use Definition 4.1 to build product MDPs and run LCNFQ on both.

7.3 SIMULATION RESULTS
This section presents the simulation results. All simulations are carried on a machine with a 3.2GHz Core i5 processor and 8GB of RAM, running Windows 7. LCNFQ has four feedforward neural networks for (9) and three feedforward neural networks for (10), each associated with an automaton state in Fig. 2.a and Fig. 2.b. We assume that the rover lands on a random safe place and has to find its way to satisfy the given property in the face of uncertainty. The learning discount factor  is also set to be equal to 0.9.
Fig. 4 in Section A.4 gives the results of learning for LTL formulas (9) and (10). At each state s, the robot picks an action that yields highest Q(s, ·) and by doing so the robot is able to generate a control policy Pol  over the state space S. The control policy Pol  induces a policy Pol  over the state space S and its performance is shown in Fig. 4.
Next, we investigate the episodic VQ algorithm as an alternative solution to LCNFQ. Three different resolutions ( = 0.4, 1.2, 2 km) are used to see the effect of the resolution on the quality of the generated policy. The results are presented in Table 1, where VQ with  = 2 km fails to find

9

Under review as a conference paper at ICLR 2019

Table 1: Simulation results

Algorithm LCNFQ
VQ ( = 0.4) VQ ( = 1.2) VQ ( = 2)
FVI
Algorithm LCNFQ
VQ ( = 0.4) VQ ( = 1.2) VQ ( = 2)
FVI

Sample Complexity 7168 samples 27886 samples 7996 samples 40000 samples
Sample Complexity 2680 samples 8040 samples 3140 samples 25000 samples

Melas Chasma

U Pol (s0) Success Rate

0.0203

99%

0.0015

99%

0.0104

97%

0 0%

0.0133

98%

Coprates Chasma U Pol (s0) Success Rate

0.1094

98%

0.0082

98%

0.0562

96%

0 0%

0.0717

97%

Training Time(s) 95.64 1732.35 273.049 4.12
Training Time(s) 166.13 3666.18 931.33 2.16

Iteration Num. 40 2195 913 80
Iteration Num. 40 3870 2778 80

 Testing the trained agent (for 100 trials)  Average for 10 trainings

a satisfying policy in both regions, due to the coarseness of the resulted discretization. A coarse partitioning result in the RL not to be able to efficiently back-propagate the reward or the agent to be stuck in some random-action loop as sometimes the agent's current cell is large enough that all actions have the same value. In Table 1, training time is the empirical time that is taken to train the algorithm and travel distance is the distance that agent traverses from initial state to final state. We show the generated policy for  = 1.2 km in Fig. 5 in Section A.4. Additionally, Fig. 7 in Section A.5 depicts the resulted Voronoi discretization after implementing the VQ algorithm. Note that with VQ only those parts of the state space that are relevant to satisfying the property are accurately partitioned.
Finally, we present the results of FVI method in Fig 6 in Section A.4 for the LTL formulas (9) and (10). The FVI smoothing parameter is h = 0.18 and the sampling time is Z = 25 for both regions where both are empirically adjusted to have the minimum possible value for FVI to generate satisfying policies. The number of basis points also is set to be 100, so the sample complexity of FVI is 100 × Z × |A| × (|Q| - 1). We do not sample the states in the product automaton that are associated to the accepting state of the automaton since when we reach the accepting state the property is satisfied and there is no need for further exploration. Hence, the last term is (|Q| - 1). However, if the property of interest produces an automaton that has multiple accepting states, then we need to sample those states as well. Note that in Table 1, in terms of timing, FVI outperforms the other methods. However, we have to remember that FVI is an approximate DP algorithm, which inherently needs an approximation of the transition probabilities. Therefore, as we have seen in Section 6 in (8), for the set of basis points we need to sample the subsequent states. This reduces FVI applicability as it might not be possible in practice.
Additionally, both FVI and episodic VQ need careful hyper-parameter tuning to generate a satisfying policy, i.e., h and Z for FVI and  for VQ. On the other hand, the big merit of LCNFQ is that it does not need any external intervention. Further, as in Table 1, LCNFQ succeeds to efficiently generate a better policy compared to FVI and VQ. LCNFQ has less sample complexity while at the same time produces policies that are more reliable and also has better expected reward, i.e. higher probability of satisfying the given property.
8 CONCLUSION
This paper proposes LCNFQ, a method to train Q-function in a continuous-state MDP such that the resulting traces satisfy a logical property. Continuous-state MDPs are of interest since they are more accurate in capturing the dynamics of the problem. However, it does not mean that one cannot apply LCNFQ to large-scale finite-state MDPs. The product MDP is the main framework for LCNFQ to be employed. The proposed algorithm uses hybrid modes to automatically switch between neural nets when it is necessary. LCNFQ is tested in a numerical example to verify its performance.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In ICML, volume 70, pp. 166­175, 2017.
Lucian Busoniu, Robert Babuska, Bart De Schutter, and Damien Ernst. Reinforcement Learning and Dynamic Programming Using Function Approximators, volume 39. CRC press, 2010.
Rolando Cavazos-Cadena, Eugene A Feinberg, and Rau´l Montes-De-Oca. A note on the existence of optimal policies in total reward dynamic programs with compact action sets. Mathematics of Operations Research, 25(4):657­666, 2000.
Kenji Doya. Reinforcement learning in continuous time and space. Neural computation, 12(1): 219­245, 2000.
Richard Durrett. Essentials of stochastic processes, volume 1. Springer, 1999.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. JMLR, 6(Apr):503­556, 2005.
Geoffrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning, pp. 261­268. Elsevier, 1995.
R. Gray. Vector quantization. IEEE ASSP Magazine, 1(2):4­29, 1984.
David Harris and Sarah Harris. Digital design and computer architecture. Morgan Kaufmann, 2010.
One´simo Herna´ndez-Lerma and Jean B Lasserre. Further topics on discrete-time Markov control processes, volume 42. Springer Science & Business Media, 2012.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2): 251­257, 1991.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Ivan SK Lee and Henry YK Lau. Adaptive state space partitioning for reinforcement learning. Engineering applications of artificial intelligence, 17(6):577­588, 2004.
Long-H Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3/4):69­97, 1992.
Alfred S McEwen, Colin M Dundas, Sarah S Mattson, Anthony D Toigo, Lujendra Ojha, James J Wray, Matthew Chojnacki, Shane Byrne, Scott L Murchie, and Nicolas Thomas. Recurring slope lineae in equatorial regions of Mars. Nature Geoscience, 7(1):53, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, pp. 1928­1937, 2016.
Dirk Ormoneit and S´ aunak Sen. Kernel-based reinforcement learning. Machine learning, 49(2): 161­178, 2002.
Amir Pnueli. The temporal logic of programs. In Foundations of Computer Science, pp. 46­57. IEEE, 1977.
Martin L Puterman. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, 2014.
Martin Riedmiller. Concepts and facilities of a neural reinforcement learning control architecture for technical process control. Neural computing & applications, 8(4):323­338, 1999.
11

Under review as a conference paper at ICLR 2019
Martin Riedmiller. Neural fitted Q iteration-first experiences with a data efficient neural reinforcement learning method. In ECML, volume 3720, pp. 317­328. Springer, 2005.
Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Neural networks, pp. 586­591. IEEE, 1993.
Ronald W Shonkwiler and Franklin Mendivil. Explorations in Monte Carlo Methods. Springer Science & Business Media, 2009.
Salomon Sickert and Jan Kret´insky`. MoChiBA: Probabilistic LTL model checking using limitdeterministic Bu¨chi automata. In ATVA, pp. 130­137. Springer, 2016.
Salomon Sickert, Javier Esparza, Stefan Jaax, and Jan Kret´insky`. Limit-deterministic Bu¨chi automata for linear temporal logic. In CAV, pp. 312­332. Springer, 2016.
Stephen L Smith, Jana Tumova´, Calin Belta, and Daniela Rus. Optimal path planning for surveillance with temporal-logic constraints. The International Journal of Robotics Research, 30(14):1695­ 1708, 2011.
John Stachurski. Continuous state dynamic programming via nonexpansive approximation. Computational Economics, 31(2):141­160, 2008.
Richard S Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In NIPS, pp. 1038­1044, 1996.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.
Gerald Tesauro. TD-Gammon: A self-teaching Backgammon program. In Applications of Neural Networks, pp. 267­285. Springer, 1995.
Ilya Tkachev, Alexandru Mereacre, Joost-Pieter Katoen, and Alessandro Abate. Quantitative modelchecking of controlled discrete-time Markov processes. Information and Computation, 253:1­35, 2017.
Hado Van Hasselt and Marco A Wiering. Reinforcement learning in continuous action spaces. In ADPRL, pp. 272­279. IEEE, 2007.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992.
12

Under review as a conference paper at ICLR 2019

A SUPPLEMENTARY MATERIALS

A.1 BACKGROUND
The simplest way to solve an infinite-state MDP with RL is to discretize the state space and then to use the conventional methods in RL to find the optimal policy (Stachurski, 2008). Although this method can work well for many problems, the resulting discrete MDP is often inaccurate and may not capture the full dynamics of the original MDP. One might argue that by increasing the number of discrete states the latter problem can be resolved. However, the more states we have the more expensive and time-consuming our computations will be. Thus, MDP discretization has to always deal with the trade off between accuracy and the curse of dimensionality.

A.1.1 CLASSICAL Q-LEARNING

Let the MDP M be a finite-state MDP. Q-learning (QL), a sub-class of RL algorithms, is extensively

used to find the optimal policy for a given finite-state MDP (Sutton & Barto, 1998). For each state

s  S and for any available action a  A, QL assigns a quantitative value Q : S × A  R, which is

initialized with an arbitrary and finite value for all state-action pairs. As the agent starts learning and

receiving rewards, the Q-function is updated by the following rule when the agent takes action a at

state s:

Q(s, a)  Q(s, a) + µ[R(s, a) +  max(Q(s , a )) - Q(s, a)],
a A

(11)

where Q(s, a) is the Q-value corresponding to state-action (s, a), 0 < µ  1 is called learning rate or step size, R(s, a) is the reward obtained for performing action a in state s,  is the discount factor, and s is the state obtained after performing action a. Q-function for the rest of the state-action pairs remains unchanged.

Under mild assumptions, for finite-state and finite-action spaces QL converges to a unique limit,

as long as every state action pair is visited infinitely often (Watkins & Dayan, 1992). Once QL converges, the optimal policy Pol  : S  A can be generated by selecting the action that yields the

highest Q, i.e.,

Pol (s) = argmax Q(s, a),
aA

where Pol  is the same optimal policy that can be generated via DP with Bellman operation. This

means that when QL converges, we have


Q(s, a) = R(s, a)+EPol [ nR(sn, P ol(sn))|s1 = s ],
n=1

(12)

where s  B is the agent new state after choosing action a at s such that P (B|s, a) = 1.

A.1.2 NEURAL FITTED Q-ITERATION

Recall the QL update rule (11), in which the agent stores the Q-values for all possible state-action pairs. In the case when the MDP has a continuous state space it is not possible to directly use standard QL since it is practically infeasible to store Q(s, a) for every s  S and a  A. Thus, we have to turn to function approximators in order to approximate the Q-values of different state-action pairs of the Q-function. Neural Fitted Q-iteration (NFQ) (Riedmiller, 2005) is an algorithm that employs neural networks (Hornik et al., 1989) to approximate the Q-function, due to the ability of neural networks to generalize and exploit the set of samples. NFQ, is the core behind Google famous algorithm Deep Reinforcement Learning (Mnih et al., 2015).

The update rule in (11) can be directly implemented in NFQ. In order to do so, a loss function has to be introduced that measures the error between the current Q-value and the new value that has to be assigned to the current Q-value, namely

L = (Q(s, a) - (R(s, a) +  max Q(s , a )))2.
a

(13)

Over this error, common gradient descent techniques can be applied to adjust the weights of the neural network, so that the error is minimized.

13

Under review as a conference paper at ICLR 2019
In classical QL, the Q-function is updated whenever a state-action pair is visited. In the continuous state-space case, we may update the approximation in the same way, i.e., update the neural net weights once a new state-action pair is visited. However, in practice, a large number of trainings might need to be carried out until an optimal or near optimal policy is found. This is due to the uncontrollable changes occurring in the Q-function approximation caused by unpredictable changes in the network weights when the weights are adjusted for one certain state-action pair (Riedmiller, 1999). More specifically, if at each iteration we only introduce a single sample point the training algorithm tries to adjust the weights of the neural network such that the loss function becomes minimum for that specific sample point. This might result in some changes in the network weights such that the error between the network output and the previous output of sample points becomes large and failure to approximate the Q-function correctly. Therefore, we have to make sure that when we update the weights of the neural network, we explicitly introduce previous samples as well: this technique is called "experience replay" (Lin, 1992) and detailed later.
The core idea underlying NFQ is to store all previous experiences and then reuse this data every time the neural Q-function is updated. NFQ can be seen as a batch learning method in which there exists a training set that is repeatedly used to train the agent. In this sense NFQ is an offline algorithm as experience gathering and learning happens separately.
We would like to emphasize that neural-net-based algorithms exploit the positive effects of generalization in approximation while at the same time avoid the negative effects of disturbing previously learned experiences when the network properly learns (Riedmiller, 2005). The positive effect of generalization is that the learning algorithm requires less experience and the learning process is highly data efficient.
A.1.3 VORONOI QUANTIZER
As stated earlier, many existing RL algorithms, e.g. QL, assume a finite state space, which means that they are not directly applicable to continuous state-space MDPs. Therefore, if classical RL is employed to solve an infinite-state MDP, the state space has to be discretized first and then the new discrete version of the problem has to be tackled. The discretization can be done manually over the state space. However, one of the most appealing features of RL is its autonomy. In other words, RL is able to achieve its goal, defined by the reward function, with minimum supervision from a human. Therefore, the state space discretization should be performed as part of the learning task, instead of being fixed at the start of the learning process.
Nearest neighbor vector quantization is a method for discretizing the state space into a set of disjoint regions (Gray, 1984). The Voronoi Quantizer (VQ) (Lee & Lau, 2004), a nearest neighbor quantizer, maps the state space S onto a finite set of disjoint regions called Voronoi cells. The set of centroids of these cells is denoted by C = {ci}mi=1, ci  S, where m is the number of the cells. Therefore, designing a nearest neighbor vector quantizer boils down to coming up with the set C. With C, we are able to use QL and find an approximation of the optimal policy for a continuous-state space MDP. The details of how the set of centroids C is generated as part of the learning task in discussed in the body of the paper.
A.1.4 FITTED VALUE ITERATION
Finally, this section introduces Fitted Value Iteration (FVI) for continuous-state numerical dynamic programming using a function approximator (Gordon, 1995). In standard value iteration the goal is to find a mapping (called value function) from the state space to R such that it can lead the agent to find the optimal policy. The value function in our setup is (1) when Pol is the optimal policy, i.e. U Pol . In continuous state spaces, no analytical representation of the value function is in general available. Thus, an approximation can be obtained numerically through approximate value iteration, which involves approximately iterating the Bellman operator T on some initial value function (Stachurski, 2008). FVI is explored more in the paper.
14

Under review as a conference paper at ICLR 2019

A.2 KERNEL AVERAGER

It has been proven that FVI is stable and converging when the approximation operator is non-expansive (Gordon, 1995). The operator L is said to be non-expansive if:

sup |Lvtq+j 1(s) - Lvtqj (s)|  sup |vtq+j 1(s) - vtqj (s)|,

sS

sS

where Lvtqj (s) is the approximated value function at (s, qj) at iteration t of the algorithm. The kernel averager in (7) is a non-expansive approximator (Stachurski, 2008).

15

Under review as a conference paper at ICLR 2019 A.3 COPRATES QUADRANGLE
Figure 3: Coprates quadrangle (Image courtesy of NASA, JPL and USGS.)
16

Under review as a conference paper at ICLR 2019 A.4 GENERATED POLICIES

(a) The generated path in Melas Chasma when the landing location, i.e. the black rectangle, is (118, 85)

(b) The generated path in Coprates Chasma when the landing location, i.e. the the black rectangle, is (194, 74)

Figure 4: Results of learning with LCNFQ

(a) The generated path in Melas Chasma when the landing location, i.e. the black rectangle, is (118, 85)

(b) The generated path in Coprates Chasma when the landing location, i.e. the black rectangle, is (194, 74)

Figure 5: Results of learning with episodic VQ

(a) The generated path in Melas Chasma when the landing location, i.e. the black rectangle, is (118, 85)

(b) The generated path in Coprates Chasma when the landing location, i.e. the black rectangle, is (194, 74)

Figure 6: Results of learning with FVI

17

Under review as a conference paper at ICLR 2019 A.5 GENERATED VORONOI CELLS
Figure 7: VQ generated cells in Melas Chasma for different resolutions
18

