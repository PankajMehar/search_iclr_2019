Under review as a conference paper at ICLR 2019

STRUCTURED ADVERSARIAL ATTACK: TOWARDS GENERAL IMPLEMENTATION AND BETTER INTERPRETABILITY

Anonymous authors Paper under double-blind review

ABSTRACT
When generating adversarial examples to attack deep neural networks (DNNs), p norm of the added perturbation is usually used to measure the similarity between original image and adversarial example. However, such adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input. This work develops a more general attack model, i.e., the structured attack (StrAttack), which explores group sparsity in adversarial perturbations by sliding a mask through images aiming for extracting key spatial structures. An ADMM (alternating direction method of multipliers)-based framework is proposed that can split the original problem into a sequence of analytically solvable subproblems and can be generalized to implement other attacking methods. Strong group sparsity is achieved in adversarial perturbations even with the same level of p-norm distortion (p  {1, 2, }) as the state-of-the-art attacks. We demonstrate the effectiveness of StrAttack by extensive experimental results on MNIST, CIFAR-10 and ImageNet. We also show that StrAttack provides better interpretability (i.e., better correspondence with discriminative image regions) through adversarial saliency map (Papernot et al., 2016b) and class activation map (Zhou et al., 2016).

1 INTRODUCTION

Deep learning achieves exceptional suc-

cesses in domains such as image recognition (He et al., 2016; Geifman & El-

C&W Attack

Structured Attack

Yaniv, 2017), natural language processing

(Hinton et al., 2012; Harwath et al., 2016),

medical diagnostics (Chen et al., 2016; Shi

et al., 2018) and advanced control (Sil-

ver et al., 2016; Fu et al., 2017). Re-

cent studies (Szegedy et al., 2013; Good-

fellow et al., 2014; Nguyen et al., 2015;

Kurakin et al., 2016; Carlini & Wagner, 2017) show that DNNs are vulnerable to adversarial attacks implemented by generating adversarial examples, i.e., adding well-designed perturbations to original le-

Figure 1: Group sparsity demonstrated in adversarial perturbations obtained by C&W attack and our StrAttack, where `ostrich' is the original label, and `unicycle' is the misclassified label. Here each group is a region of 13×13×3 pixels and the strength of adversarial perturbations (through their

gal inputs. Delicately crafted adversarial 2 norm) at each group is represented by heatmap. C&W atexamples can mislead a DNN to recog- tack perturbs almost all groups, while StrAttack yields strong

nize them as any target image label, while group sparsity, with more semantic structure: the perturbed

the perturbations appears unnoticeable to image region matches the feature of the target object, namely, human eyes. Adversarial attacks against the frame of the unicycle.

DNNs not only exist in theoretical models

but also pose potential security threats to the real world (Kurakin et al., 2016; Evtimov et al., 2017;

Papernot et al., 2017). Several explanations are proposed to illustrate why there exist adversarial ex-

amples to DNNs based on hypotheses such as model linearity and data manifold (Goodfellow et al.,

2014; Gilmer et al., 2018). However, little is known to their origins, and convincing explanations

remain to be explored.

Besides achieving the goal of (targeted) mis-classification, an adversarial example should be as
"similar" to the original legal input as possible to be stealthy. Currently, the similarity is measured by the p norm (p = 0, 1, 2, ) of the added perturbation (Szegedy et al., 2013; Carlini & Wagner,

1

Under review as a conference paper at ICLR 2019

2017; Chen et al., 2017b;a), i.e., p norm is being minimized when generating adversarial example. However, measuring the similarity between the original image and its adversarial example by p norm is neither necessary nor sufficient (Sharif et al., 2018). Besides, no single measure can be perfect for human perceptual similarity (Carlini & Wagner, 2017) and such adversarial attacks may fail to capture key information hidden in the input such as spatial structure or distribution. Spurred by that, this work implements a new attack model i.e., structured attack (StrAttack) that imposes group sparsity on adversarial perturbations by extracting structures from the inputs. As shown in Fig. 1, we find that StrAttack identifies minimally sufficient regions that make attacks successful, but without incurring extra pixel-level perturbation power. The major contributions are summarized as below.
· (Structure-driven attack) This work is the first attempt towards exploring group-wise sparse structures when implementing adversarial attacks, but without losing p distortion performance when compared to state-of-the-art attacking methods.
· (Generality) We show that the proposed attack model covers many norm-ball based attacks such as C&W (Carlini & Wagner, 2017) and EAD (Chen et al., 2017a).
· (Efficient implementation) We develop an efficient algorithm to generate structured adversarial perturbations by leveraging the alternating direction method of multipliers (ADMM). We show that ADMM splits the original complex problem into subproblems, each of which can be solved analytically. Besides, we show that ADMM can further be used to refine an arbitrary adversarial attack under the fixed sparse structure.
· (Interpretability) The generated adversarial perturbations demonstrate clear correlations and interpretations between original and target images. With the aid of adversarial saliency map (Papernot et al., 2016b) and class activation map (Zhou et al., 2016), we show that the obtained group-sparse adversarial patterns better shed light on the mechanisms of adversarial perturbations to fool DNNs.

2 STRUCTURED ATTACK: EXPLORE GROUP STRUCTURES FROM IMAGES

In the section, we introduce the concept of StrAttack, motivated by the question: `what possible structures could adversarial perturbations have to fool DNNs?' Our idea is to divide an image into sub-groups of pixels and then penalize the corresponding group-wise sparsity. The resulting sparse groups encode minimally sufficient adversarial effects on local structures of natural images.

Let   RW ×H×C be an adversarial perturbation added to an original image X0, where W × H gives the spatial region, and C is the depth, e.g., C = 3 for RGB images. To characterize the local structures of , we introduce a sliding mask M with stride S and size r × r × C. When S = 1,
the mask moves one pixel at a time; When S = 2, the mask jumps 2 pixels at a time while sliding.
By adjusting the stride S and the mask size r, different group splitting schemes can be obtained.
If S < r, the resulting groups will contain overlapping pixels. By contrast, groups will become
non-overlapped when S = r.

A sliding mask M finally divides  into a set of groups {Gp,q } for p  [P ] and q  [Q], where
P = (W - r)/S + 1, Q = (H - r)/S + 1, and [n] denotes the integer set {1, 2, . . . , n}. Given the groups {Gp,q }, the group sparsity can be characterized through the following sparsity-inducing function (Yuan & Lin, 2006; Bach et al., 2012; Liu et al., 2015), motivated by the problem of group
Lasso (Yuan & Lin, 2006):

g() =

P p=1

Q q=1

Gp,q

2,

(1)

where Gp,q denotes the set of pixels of  indexed by Gp,q, and · 2 is the 2 norm. We refer readers to Fig. A1 for an illustrative example of our concepts on groups and group sparsity.

3 STRUCTURED ADVERSARIAL ATTACK WITH ADMM
In this section, we start by proposing a general framework to generate prediction-evasive adversarial examples, where the adversary relies only on gradients of the loss function with respect to inputs of DNNs. Our model takes into account both commonly-used adversarial distortion metrics and the proposed group-sparsity regularization that encodes spatial structures in attacks. We show that the process of generating structured adversarial examples leads to an optimization problem that is difficult to solve using the existing optimizers Adam (for C&W attack) and FISTA (for EAD attack)

2

Under review as a conference paper at ICLR 2019

(Carlini & Wagner, 2017; Chen et al., 2017a). To circumvent this challenge, we develop an efficient optimization method via alternating direction method of multipliers (ADMM).
Given an original image x0  Rn, we aim to design the optimal adversarial perturbation   Rn so that the adversarial example (x0 + ) misleads DNNs trained on natural images. Throughout this paper, we use vector representations of the adversarial perturbation  and the original image X0 without loss of generality. A well designed perturbation  can be obtained by solving optimization problems of the following form,

minimize f (x0 + , t) + D() +  g()


subject to

(x0 + )  [0, 1]n,

  ,

(2)

where f (x, t) denotes the loss function for crafting adversarial example given a target class t, D()

is a distortion function that controls the perceptual similarity between a natural image and a per-

turbed image, g() =

P p=1

Q q=1

Gp,q

2 is given by (1), and

· p signifies the p norm. In

problem (2), the `hard' constraints ensure the validness of created adversarial examples with -

tolerant perturbed pixel values. And the non-negative regularization parameters  and  place our

emphasis on the distortion of an adversarial example (to an original image) and group sparsity of

adversarial perturbation. Tuning the regularization parameters will be discussed in Appendix F.

Problem (2) gives a quite general formulation for design of adversarial examples. If we remove the
group-sparsity regularizer g() and the  constraint, problem (2) becomes the same as the C&W attack (Carlini & Wagner, 2017). More specifically, if we further set the distortion function D()
to the form of 0, 2 or  norm, then we obtain C&W 0, 2 or  attack. If D() is specified by the elastic-net regularizer, then problem (2) becomes the formulation of EAD attack (Chen et al.,
2017a).

In this paper, we specify the loss function of problem (2) as below, which yields the best known performance of adversaries (Carlini & Wagner, 2017),

f (x0 + , t) = c · max{max Z(x0 + )j - Z(x0 + )t, -},
j=t

(3)

where Z(x)j is the jth element of logits Z(x), representing the output before the last softmax layer

in DNNs, and  is a confidence parameter that is usually set to zero if the attack transferability is not

much cared. We choose D() =



2 2

for

a

fair

comparison

with

the

C&W

2 adversarial attack.

In this section, we assume that {Gp,q} are non-overlapping groups, i.e., Gp,q  Gp ,q =  for q = q

or p = p . The overlapping case will be studied in the next section.

The presence of multiple non-smooth regularizers and `hard' constraints make the existing optimizers Adam and FISTA (Carlini & Wagner, 2017; Chen et al., 2017a; Kingma & Ba, 2015; Beck & Teboulle, 2009) inefficient for solving problem (2). First, the subgradient of the objective function of problem (2) is difficult to obtain especially when {Gp,q} are overlapping groups. Second, it is impossible to compute the proximal operations required for FISTA with respect to all non-smooth regularizers and `hard' constraints. Different from the existing work, we show that ADMM, a firstorder operator splitting method, helps us to split the original complex problem (2) into a sequence of subproblems, each of which can be solved analytically.

We reformulate problem (2) in a way that lends itself to the application of ADMM,

minimize ,z,w,y

f (z + x0) + D() + 

PQ i=1

yDi

2 + h(w)

subject to z = , z = y, z = w,

(4)

where z, y and w are newly introduced variables, for ease of notation let D(q-1)P +p = Gp,q, and h(w) is an indicator function with respect to the constraints of problem (2),

h(w) =

0 if (x0 + w)  [0, 1]n, w   ,  otherwise.

(5)

ADMM is performed by minimizing the augmented Lagrangian of problem (4),

L(z, , y, w, u, v, s) = f (z + x0) + D() + 

PQ i=1

yDi

2 + h(w) + uT ( - z)

+vT (y

-

z)

+

sT (w

-

z)

+

 2

-z

2 2

+

 2

y-z

2 2

+

 2

w-z

2 2

,

(6)

3

Under review as a conference paper at ICLR 2019

where u, v and s are Lagrangian multipliers, and  > 0 is a given penalty parameter. ADMM splits all of optimization variables into two blocks and adopts the following iterative scheme,

{k+1, wk+1, yk+1} = arg min L(, zk, w, y, uk, vk, sk),
,w,y
zk+1 = arg min L(k+1, z, wk+1, yk+1, uk, vk, sk),
z
 uk+1 = uk + (k+1 - zk+1), 
vk+1 = vk + (yk+1 - zk+1),  sk+1 = sk + (wk+1 - zk+1),

(7) (8)
(9)

where k is the iteration index, steps (7)-(8) are used for updating primal variables, and the last step (9) is known as the dual update step. We emphasize that the crucial property of the proposed ADMM approach is that, as we demonstrate in Proposition 1, the solution to problem (7) can be found in parallel and exactly.

Proposition 1 When D() =  22, the solution to problem (7) is given by

k+1

=

 +2

a,

[wk+1]i =

min{1 - [x0]i, } bi > min{1 - [x0]i, } max{-[x0]i, - } bi < max{-[x0]i, - } for i  [n],
bi otherwise,

(10) (11)

[yk+1]Di =

1- 

 [c]Di

2

+ [c]Di , i  [P Q],

(12)

where a := zk - uk/, b := zk - sk/, c := zk - vk/, (x)+ = x if x  0 and 0 otherwise, [x]i denotes the ith element of x, and [x]Di denotes the sub-vector of x indexed by Di.

Proof: See Appendix B.

It is clear from Proposition 1 that introducing auxiliary variables does not increase the computational

complexity of ADMM since (10)-(12) can be solved in parallel. Moreover, if another distortion

metric (different from D() =



2 2

)

is

used,

then

ADMM

only

changes

at

the

-step

(10).

We next focus on the z-minimization step (8), which can be equivalently transformed into

minimize z

 f (x0 + z) + 2

z-a

2 2

+

 2

z-b

2 2

+

 2

z-c

2 2

,

(13)

where a := k+1 + uk/, b := wk+1 + sk/, and c := yk+1 + vk/. We recall that attacks

studied in this paper belongs to `first-order' adversaries (Madry et al., 2017), which only have access to gradients of the loss function f . Spurred by that, we solve problem (13) via a linearization

technique that is commonly used in stochastic/online ADMM (Ouyang et al., 2013; Suzuki, 2013;

Liu et al., 2018) or linearized ADMM (Boyd et al., 2011; Liu et al., 2017). Specifically, we replace the function f with its first-order Taylor expansion at the point zk by adding a Bregman divergence term (k/2) z - zk 22. As a result, problem (13) becomes

minimize z

(f +
2

(zk + x0))T

z-b

2 2

+

(z 
2

- z

zk ) -c

+ k 2
22 ,

z - zk

2 2

+

 2



z-a

2 2

(14)

where 1/k > 0 is a given decaying parameter, e.g., k =  k for some  > 0, and the Bregman

divergence term stabilizes the convergence of z-minimization step. It is clear that problem (14)

yields a quadratic program with the closed-form solution

zk+1 = (1/ (k + 3)) kzk + a + b + c - f (zk + x0) .

(15)

In summary, the proposed ADMM algorithm alternatively updates (7)-(9), which yield closed-form solutions given by (10)-(12) and (15). The convergence of linearized ADMM for nonconvex optimization was recently proved by (Liu et al., 2017), and thus provides theoretical validity of our approach. Compared to the existing solver for generation of adversarial examples (Carlini & Wagner, 2017; Papernot et al., 2016b), our algorithm offers two main benefits, efficiency and generality. That is, the computations for every update step are efficiently carried out, and our approach can be applicable to a wide class of attack formulations.

4

Under review as a conference paper at ICLR 2019

4 OVERLAPPING GROUP AND REFINED STRATTACK

In this section, we generalize our proposed ADMM solution framework to the case of generating adversarial perturbations with overlapping group structures. We then turn to an attack refining model under fixed sparse structures. We will show that both extensions can be unified under the ADMM framework. In particular, the refined approach will allow us to gain deeper insights on the structural effects on adversarial perturbations.

4.1 OVERLAPPING GROUP STRUCTURE

We recall that groups {Di} (also denoted by {Gp,q}) studied in Sec. 3 could be overlapped with each other; see an example in Fig. A1. Therefore, {Di} is in general a cover rather than a partition of [n]. To address the challenge in coupled group variables, we introduce multiple copies of the variable y
in problem (4), and achieve the following modification

minimize f (z + x0) + D() + 
,z,w,{yi }

PQ i=1

yi,Di

2 + h(w)

subject to z = , z = w, z = yi, i  [P Q],

(16)

where compared to problem (4), there exist P Q variables yi  Rn for i  [P Q], and yi,Di denotes the subvector of yi with indices given by Di. It is clear from (16) that groups {Di} become nonoverlapped since each of them lies in a different copy yi. The ADMM algorithm for solving problem (16) maintains a similar procedure as (7)-(9) except y-step (12) and z-step (15); see Proposition 2.

Proposition 2 Given the same condition of Proposition 1, the ADMM solution to problem (16) involves the -step same as (10), the w-step same as (11), and two modified y- and z-steps,





yik+1 Di =

1- 

 [ci ]Di

2

+ [ci]Di

, for i  [P Q],

 yik+1 [n]/Di = [ci][n]/Di

zk+1 = (1/ (k + 2 + P Q)) kzk + a + b + 

PQ i=1

ci

-

f

(zk

+

x0)

,

(17) (18)

where ci := zk - vik/, vi is the Lagrangian multiplier associated with equality constraint yi = z, similar to (9) we obtain vik+1 = vik + (yk+1 - zk+1), [n]/Di denotes the difference of sets [n] and Di, a and b have been defined in (13), and ci = yik+1 + vik/.

Proof: See Appendix C.
We note that updating P Q variables {yi} is decomposed as shown in (17). However, the side effect is the need of P Q times more storage space than the y-step (12) when groups are non-overlapped.

4.2 REFINED STRATTACK UNDER FIXED SPARSE PATTERN

The approaches proposed in Sec. 3 and Sec. 4.1 help us to identify structured sparse patterns in adversarial perturbations. This section presents a method to refine structured attacks under fixed group sparse patterns. Let  denote the solution to problem (2) solved by the proposed ADMM method. We define a -sparse perturbation  via ,

i = 0 if i  , for any i  [n],

(19)

where a hard thresholding operator is applied to  with tolerance . Our refined model imposes the fixed -sparse structure (19) into problem (2). This leads to

minimize f (x0 + ) + D()


subject to

(x0 + )  [0, 1]n,

 

i = 0, if i  S,

(20)

where S is defined by (19), i.e., S := {j | j  , j  [n]}. Compared to problem (2), the groupsparse penalty function is eliminated as it has been known as a priori. With the priori knowledge of group sparsity, problem (20) is formulated to optimize and refine the non-zero groups, thus achieving better performance on highlighting and exploring the perturbation structure. Problem (20) can be solved using ADMM, and its solution is presented in Proposition 3.

5

Under review as a conference paper at ICLR 2019

Proposition 3 The ADMM solution to problem (20) is given by

0

   [k+1]i =
  

min{1 - [x0]i , }

max{- [x0]i , - }

 2+

ai

i  S

 2+ 2+

ai ai

> <

min{1 - [x0]i , max{- [x0]i , -

}, },

i i

/ /

S S

otherwise,

[zk+1]i =

0 1/(k + ) k[zk]i + [a ]i - [f (zk + x0)]i

i  S i / S,

(21) (22)

for i  [n], where z =  is the introduced auxiliary variable similar to (4), a := k+1 - uk/, a := k+1 + uk/, uk+1 = uk + (k+1 - zk+1), and  and k have been defined in (6) and (14). The ADMM iterations can be initialized by , the known solution to problem (2).

Proof: See Appendix D.

5 EMPIRICAL PERFORMANCE OF STRATTACK

We evaluate the performance of the proposed StrAttack on three image classification datasets,

MNIST (Lecun et al., 1998), CIFAR-10 (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al.,

2009). To make fair comparison with the C&W 2 attack (Carlini & Wagner, 2017), we use 2 norm

as the distortion function D() =



2 2

.

And

we

also

compare

with

FGM

(Goodfellow

et

al.,

2014)

and IFGM 2 attacks (Kurakin et al., 2017) as a reference. We evaluate attack success rate (ASR)1

as well as p distortion metrics for p  {0, 1, 2, }. The detailed experiment setup is presented in

Appendix F.

For each attack method on MNIST or CIFAR-10, we choose 1000 original images from the test dataset as source and each image has 9 target labels. So a total of 9000 adversarial examples are generated for each attack method. On ImageNet, each attack method tries to craft 900 adverdarial examples with 100 random images from the test dataset and 9 random target labels for each image.

Fig. 2 compares adversarial examples generated by StrAttack and C&W attack on each dataset. We observe that the perturbation of the C&W attack has poor group sparsity, i.e., many non-zeros groups with small magnitudes. However, the ASR of the C&W attack is quite sensitive to these small perturbations. As applying a threshold to have the same 0 norm as our attack, we find that only 6.7% of adversarial examples generated from C&W attack remain valid. By contrast, StrAttack is able to highlight the most important group structures (local regions) of adversarial perturbations without attacking other pixels. For example, StrAttack misclassifies a natural image (4 in MNIST) as an incorrect label 3. That is because the pixels that appears in the structure of 3 are more significantly perturbed by our attack; see the top right plots of Fig. 2. Furthermore, the `goose-sorrel' example shows that misclassification occurs when we just perturb a small number of non-sparse group regions on goose's head, which is more consistent with human perception. We refer readers to Appendix G for more results.

By quatitatively analysis, we report p norms and ASR in Table 1 for p  {0, 1, 2, }. We show that StrAttack perturbs much fewer pixels (smaller 0 norm), but it is comparable to or even better than other attacks in terms of 1, 2, and  norms. Specifically, the FGM attack yields the worst performance in both ASR and p distortion. On MNIST and CIFAR-10, StrAttack outperforms other attacks in 0, 1 and  distortion. On ImageNet, StrAttack outperforms C&W attack in 0 and 1 distortion. Since the C&W attacking loss directly penalizes the 2 norm, it often causes smaller 2 distortion than StrAttack. We also observe that the overlapping case leads to the adversarial per-
turbation of less sparsity (in terms of 0 norm) compared to the non-overlapping case. This is not surprising, since the sparsity of the overlapping region is controlled by at least two groups. However,
compared to C&W attack, the use of overlapping groups in StrAttack still yields sparser perturba-
tions. Unless specified otherwise, we focus on the case of non-overlapping groups to generate the
most sparse adversarial perturbations. We highlight that although a so-called one-pixel attack (Su
et al., 2017) also yields very small 0 norm, it is at the cost of very large  distortion. Unlike one-pixel attack, StrAttack achieves the sparsity without losing the performance of , 1 and 2 distortion. Lastly, we evaluate the performance of StrAttack against two defense mechanisms: de-
fensive distillation (Papernot et al., 2016c) and adversarial training (Tramèr et al., 2018). StrAttack
is able to break the two defense methods with 100% ASR. More details are provided in Appendix H.

1The percentage of adversarial examples that successfully fool DNNs.

6

Under review as a conference paper at ICLR 2019

Target: 3

Add perturbation

Original Image: 4

Add perturbation

Target: 3

Target: airplane

Original Image: horse

Target: airplane

Target: sorrel

0.04
Original Image: goose

0.09
Target: sorrel

2e-3

1e-3

Figure 2: C&W attack vs StrAttack. Here each grid cell represents a 2 × 2, 2 × 2, and 13 × 13 small region in MNIST, CIFAR-10 and ImageNet, respectively. The group sparsity of perturbation is represented by heatmap. The colors on heatmap represent average absolute value of distortion scale to [0, 255]. The left two columns correspond to results of using C&W attack. The right two columns show results of StrAttack.

Table 1: Adversarial attack success rate (ASR) and p distortion values for various attacks.

Data Set MNIST

Attack Method
FGM IFGM C&W Structured +overlap

ASR
99.3 100 100 100 100

0
456.5 549.5 479.8 73.2 84.4

Best Case*
12
28.2 2.32 18.3 1.57 13.3 1.35 10.9 1.51 9.2 1.32

 ASR

Average Case*
0 12

0.57 35.8 466 39.4 3.17 0.4 100 588 30.9 2.41 0.397 100 493.4 21.3 1.9 0.384 100 119.4 18.05 2.16 0.401 100 157.4 16.2 1.95


0.717 0.566 0.528 0.47 0.508

ASR
0 99.8 99.7 100 100

Worst Case*

0 12

N.A.** 640.4 524.3 182.0 260.9

N.A. 50.98 29.9 26.9 22.9

N.A. 3.742 2.45 2.81 2.501


N.A. 0.784 0.664 0.5 0.653

FGM 98.5 3049 12.9 0.389 0.046 44.1 3048 34.2 0.989 0.113 0.2 3071 61.3 1.76 0.194

CIFAR-10

IFGM C&W

100 3051 100 2954

6.22 0.182 0.02 100 3051 13.7 0.391 0.0433 100 3060 22.9 0.655 0.075 6.03 0.178 0.019 100 2956 12.1 0.347 0.0364 99.9 3070 16.8 0.481 0.0536

Structured 100 264 3.33 0.204 0.031 100 487 7.13 0.353 0.050 100 772 12.5 0.563 0.075

+overlap 100 295 3.35 0.169 0.029 100 562 7.05 0.328 0.047 100 920 12.9 0.502 0.063

FGM

12 264917 152 0.477 0.0157 2 263585 51.3 0.18 0.00614 0 N.A. N.A. N.A. N.A.

ImageNet

IFGM C&W

100 267079 299.32 0.9086 0.02964 100 267293 723 2.2 0.0792 98 267581 1378 4.22 0.158 100 267916 127 0.471 0.016 100 263140 198 0.679 0.03 100 265212 268 0.852 0.041

Structured 100 14462 55.2 0.719 0.058 100 52328 152 1.06 0.075 100 80722 197 1.35 0.122

* Please refer to Appendix F for the definition of best case, best case and worst case. ** N.A. means not available in the case of zero ASR, +overlap means structured attack with overlapping groups.

6 STRATTACK OFFERS BETTER INTERPRETABILITY

In this section, we evaluate the effects of structured adversarial perturbations on image classification through the interpretability tools, adversarial saliency map (ASM) (Papernot et al., 2016b) and class activation map (CAM) (Zhou et al., 2016), respectively. Here ASM provides sensitivity analysis for pixel-level perturbation's impact on label classification, and CAM localizes class-specific image discriminative regions that we use to visually explain adversarial perturbations (Xiao et al., 2018). We will show that compared to C&W attack, StrAttack meets better interpretability in terms of (a) a higher ASM score and (b) a tighter connection with CAM, where the metric (a) implies interpretability at a micro-level, namely, perturbing pixels with largest impact on image classification, and the metric (b) demonstrates interpretability at a macro-level, namely, perturbations can be mapped to the most discriminative image regions localized by CAM.

Given an input image x0 and a target class t, let ASM(x0, t)  Rd denote ASM scores for every pixel of x0 corresponding to t. We elaborate on the mathematical definition of ASM in Appendix E.
Generally speaking, the ith element of ASM(x0, t), denoted by ASM(x0, t)[i], measures how much the classification score with respect to the target label t will increase and that with respect to the original label t0 will decrease if a perturbation is added to the pixel i. With the aid of ASM, we then define a Boolean map BASM  Rd to encode the regions of x0 most sensitive to targeted adversarial
attacks, where BASM(i) = 1 if ASM(x0, t) > , and 0 otherwise. Here  is a given threshold to highlight the most sensitive pixels. we then define the interpretability score (IS) via ASM,

IS() = BASM   2/  2,

(23)

where  is the element-wise product. The rationale behind (23) is that IS()  1 if the sensitive
region identified by ASM perfectly predicts the locations of adversarial perturbations. By contrast, if IS()  0, then adversarial perturbations cannot be interpreted by ASM. In Fig. 3(a), we compare IS of our proposed attack with C&W attack versus the threshold , valued by different percentiles of

7

Under review as a conference paper at ICLR 2019
ASM scores. We obsreve that our attack outperforms C&W attack in terms of IS, since the former is able to extract important local structures of images by penalizing the group sparsity of adversarial perturbations. It seems that our improvement is not significant. However, StrAttack just perturbs very few pixels to obtain this benefit, leading to perturbations with more semantic structure; see Fig. 3(b) for an illustrative example.
Besides ASM, we show that the effect of adversarial perturbations can be visually explained through the class-specific discriminative image regions localized by CAM (Zhou et al., 2016). In Fig. 3(c), we illustrate CAM and demonstrate the differences between our attack and C&W in terms of their connections to the most discriminative regions of x0 with label t0. We observe that the mechanism of StrAttack can be better interpreted from CAM: only a few adversarial perturbations are needed to suppress the feature of the original image with the true label. By replacing ASM with CAM, we can similarly compute IS in (23) averaged over 500 examples on ImageNet, yielding 0.65 for C&W attack and 0.77 for our attack. More examples of ASM and CAM can be viewed in Appendix E.
(b)
(a) (c)
Figure 3: Interpretabilicy comparison of StrAttack and C&W attack. (a) ASM-based IS vs , given from the 30th percentile to the 90th percentile of ASM scores. (b) Overlay ASM and BASM   on top of image with the true label `Tibetan Mastiff' and the target label `streetcar'. From left to right: original image, ASM (darker color represents larger value of ASM score), BASM   under StrAttack, and BASM   under C&W attack. Here  in BASM is set by the 90th percentile of ASM scores. (c) From left to right: original image with true label `stove', CAM of `stove', and perturbations with target label `water ouzel' under StrAttack and C&W.
7 RELATED WORK
This section reviews the existing literature on adversarial attack and defense. L-BFGS attack (Szegedy et al., 2013) is the first optimization-based attack that minimizes 2 norm of adversarial perturbation. The fast gradient method (FGM) (Goodfellow et al., 2014) and iterative fast gradient method (IFGM) (Kurakin et al., 2017) utilize the sign of the gradient of the loss function to determine the direction for modifying the pixels. C&W (Carlini & Wagner, 2017) attacks are a series of 0, 2, and  attacks which achieve much lower p norms with 100% attack success rate. EAD attack (Chen et al., 2017a) formulates the process of crafting adversarial examples as an elastic-net regularized optimization problem. It is able to craft 1-oriented adversarial examples and includes the C&W 2 attack as a special case. For the defense of adversarial examples, defensive distillation (Papernot et al., 2016c) introduces temperature into the softmax layer and uses a higher temperature for training and a lower temperature for testing, thus making it difficult for the attacker to obtain the gradients of logits. Two adversarial training methods (Tramèr et al., 2018; Madry et al., 2017) are proposed to increase the robustness of DNNs. The former one adds adversarial examples into the training dataset and the later one trains DNNs using adversarial examples exclusively supposing training with data balls instead of data points.
8 CONCLUSION
This work explores group-wise sparse structures when implementing adversarial attacks. Different from previous works that use p norm to measure the similarity between an original image and an adversarial example, this work incorporates group-sparsity regularization into the problem formulation of generating adversarial examples and achieves strong group sparsity in the obtained adversarial perturbations. Leveraging ADMM, we develop an efficient implementation to generate structured adversarial perturbations, which can be further used to refine an arbitrary adversarial attack under fixed group sparse structures. The proposed ADMM framewrok is general enough for implementing many state-of-the-art attacks. We perform extensive experiments using MNIST, CIFAR-10 and ImageNet datasets, showing that our structured adversarial attack (StrAttack) is much stronger than the existing attacks and its better interpretability from group sparse structures aids in uncovering the origins of adversarial examples.
8

Under review as a conference paper at ICLR 2019
REFERENCES
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. Foundations and Trends R in Machine Learning, 4(1):1­106, 2012.
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183­202, 2009.
S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R in Machine Learning, 3(1):1­122, 2011.
N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39­57. IEEE, 2017.
J. Chen, L. Yang, Y. Zhang, M. Alber, and D. Chen. Combining fully convolutional and recurrent neural networks for 3d biomedical image segmentation. In Advances in Neural Information Processing Systems, pp. 3036­3044, 2016.
P.-Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C.-J. Hsieh. Ead: elastic-net attacks to deep neural networks via adversarial examples. arXiv preprint arXiv:1709.04114, 2017a.
P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15­26. ACM, 2017b.
J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F.-F. Li. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. IEEE, 2009.
I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song. Robust physical-world attacks on machine learning models. arXiv preprint arXiv:1707.08945, 2017.
J. Fu, J. Co-Reyes, and S. Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2574­2584, 2017.
Y. Geifman and R. El-Yaniv. Selective classification for deep neural networks. In Advances in neural information processing systems, pp. 4885­4894, 2017.
J. Gilmer, L. Metz, F. Faghri, S. Schoenholz, M. Raghu, M. Wattenberg, and I. Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.
I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
D. Harwath, A. Torralba, and J. Glass. Unsupervised learning of spoken language with visual context. In Advances in Neural Information Processing Systems, pp. 1858­1866, 2016.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82­97, 2012.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. 2015 ICLR, arXiv preprint arXiv:1412.6980, 2015. URL http://arxiv.org/abs/1412.6980.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master's thesis, Department of Computer Science, University of Toronto, 2009.
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine learning at scale. 2017 ICLR, arXiv preprint arXiv:1611.01236, 2017. URL http://arxiv.org/abs/1611.01236.
9

Under review as a conference paper at ICLR 2019
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, Nov 1998. ISSN 0018-9219. doi: 10.1109/5.726791.
Q. Liu, X. Shen, and Y. Gu. Linearized admm for non-convex non-smooth optimization with convergence analysis. arXiv preprint arXiv:1705.02502, 2017.
S. Liu, S. Kar, M. Fardad, and P. K. Varshney. Sparsity-aware sensor collaboration for linear coherent estimation. IEEE Transactions on Signal Processing, 63(10):2582­2596, 2015.
S. Liu, J. Chen, P.-Y. Chen, and A. O. Hero. Zeroth-order online admm: Convergence analysis and applications. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84, pp. 288­297, April 2018.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427­436, 2015.
H. Ouyang, N. He, L. Tran, and A. Gray. Stochastic alternating direction method of multipliers. In International Conference on Machine Learning, pp. 80­88, 2013.
N. Papernot, I. Goodfellow, R. Sheatsley, R. Feinman, and P. McDaniel. cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768, 2016a.
N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372­387. IEEE, 2016b.
N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 582­597. IEEE, 2016c.
N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Celik, and A. Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506­519. ACM, 2017.
N. Parikh, S. Boyd, et al. Proximal algorithms. Foundations and Trends R in Optimization, 1(3): 127­239, 2014.
M. Sharif, L. Bauer, and M. K. Reiter. On the suitability of lp-norms for creating and preventing adversarial examples. CoRR, abs/1802.09653, 2018.
X. Shi, M. Sapkota, F. Xing, F. Liu, L. Cui, and L. Yang. Pairwise based deep ranking hashing for histopathology image classification and retrieval. Pattern Recognition, 81:14 ­ 22, 2018. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2018.03.015. URL http: //www.sciencedirect.com/science/article/pii/S0031320318301055.
D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484­489, 2016.
J. Su, D. Vargas, and S. Kouichi. One pixel attack for fooling deep neural networks. arXiv preprint arXiv:1710.08864, 2017.
T. Suzuki. Dual averaging and proximal gradient descent for online alternating direction multiplier method. In International Conference on Machine Learning, pp. 392­400, 2013.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818­2826, 2016.
10

Under review as a conference paper at ICLR 2019 F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. Ensemble adver-
sarial training: Attacks and defenses. 2018 ICLR, arXiv preprint arXiv:1705.07204, 2018. C. Xiao, J. Zhu, B. Li, W. He, M. Liu, and D. Song. Spatially transformed adversarial examples.
CoRR, abs/1801.02612, 2018. URL http://arxiv.org/abs/1801.02612. M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49­67, 2006. B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discrim-
inative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2921­2929, 2016.
11

Under review as a conference paper at ICLR 2019

APPENDIX A ILLUSTRATIVE EXAMPLE OF GROUP SPARSITY

non-overlapping groups (1,1) (1,2) (1,3) (1,4) (2,1) (2,2) (2,3) (2,4) (3,1) (3,2) (3,3) (3,4) (4,1) (4,2) (4,3) (4,4)

  G1,2 2

=

i2j

(i, j ) 12

G1,12 =

i2j

(i, j ) 1,1

overlapping groups (1,1) (1,2) (1,3) (1,4) (2,1) (2,2) (2,3) (2,4) (3,1) (3,2) (3,3) (3,4) (4,1) (4,2) (4,3) (4,4)

Figure A1: An example of 4 × 4 perturbation matrix under sliding masks with different strides. The values of
matrix elements are represented by color's intensity (white stands for 0). Left: Non-overlapping groups with r = 2 and S = 2. Right: Overlapping groups with r = 2 and S = 1. In both cases, two groups G1,1 and G1,2 are highlighted, where G1,1 is non-sparse, and G1,2 is sparse.

B PROOF OF PROPOSITION 1

We recall that the augmented Lagrangian function L(, z, w, y, u, v, s) is given by

L(z, , y, w, u, v, s) =f (z + x0) + D() + 

PQ i=1

yDi

2 + h(w) + uT ( - z)

+ vT (y - z) + sT (w - z) +  2

-z

2 2

+

 2

y-z

2 2

+

 2

w-z

2 2

.

(24)

Problem (7), to minimize L(, zk, w, y, uk, vk, sk), can be decomposed into three sub-problems:

 minimize D() +
2

-a

22,

(25)

 minimize h(w) +
w2

w-b

22,

(26)

PQ
minimize 
y

yDi

 2+ 2

y - c 22,

i=1

where a := zk - uk/, b := zk - sk/, and c := zk - vk/.

(27)

-step

Suppose D() =



2 2

,

then

the

solution

to

problem

(25)

is

easily

acquired

as

below

k+1 =

 a

 + 2

(28)

w-step Based on the definition of h(w), problem (26) becomes

minimize
w

w-b

2 2

subject to (x0 + w)  [0, 1]n, w   .

(29)

Problem (29) is equivalent to

minimize
wi

(wi - ai)22

subject to -[x0]i  wi  1 - [x0]i, |wi| 

(30)

for i  [n], where xi or [x]i represents the ith element of x, and 1 - [x0]i > 0 since [x0]i  [0, 1]. Problem (30) then yields the solution

[wk+1]i =

min{1 - [x0]i, } ai > min{1 - [x0]i, } max{-[x0]i, - } ai < max{-[x0]i, - }
ai otherwise.

(31)

12

Under review as a conference paper at ICLR 2019

y-step Problem (27) becomes

minimize
y

PQ

 yDi 2 + 2

y - c 22,

i=1

(32)

The solution is given by the proximal operator associated with the 2 norm with parameter  / (Parikh et al., 2014)

[yk+1]Di =

1-   [c]Di 2

[c]Di , i  [P Q],
+

where recall that i[P Q]Di = [n], and Di  Dj =  if i = j.

(33)

C PROOF OF PROPOSITION 2

The augmented Lagrangian of problem (16) is given by

PQ

L(z, , w, {yi}, u, vi, s) =f (z + x0) + D() + h(w) + 

yi,Di 2 + uT ( - z) + sT (w - z)

i=1

+

PQ

viT (yi

-

z)

+

 2

i=1

-z

2 2

+

 2

w-z

2 2

+

 2

PQ

i=1

yi - z 22, (34)

where u, vi and s are the Lagrangian multipliers.

ADMM decomposes the optimization variables into two blocks and adopts the following iterative scheme,

{k+1, wk+1, yik+1} = arg min L(zk, , w, yi, uk, vik, sk),
 ,w,{yi }
zk+1 = arg min L(z, k+1, wk+1, yik+1, uk, vik, sk),
z
 uk+1 = uk + (k+1 - zk+1), 
vik+1 = vik + (yik+1 - zk+1), for i  [P Q],  sk+1 = sk + (wk+1 - zk+1),

(35) (36)
(37)

where k is the iteration index. Problem (35) can be split into three subproblems as shown below,

 minimize D() +
2

-a

22,

(38)

 minimize h(w) +
w2

w-b

22,

(39)

minimize 
yi

yi,Di

 2+ 2

yi - ci

2 2

,

for

i



[P Q].

(40)

where a = zk - uk/, b = zk - sk/ and ci = zk - vik/. Each problem has a closed form solution. Note that the solutions to problem (38) and problem (39) are given (28) and (31).

yi-step Problem (40) can be rewritten as

minimize 
yi

yi,Di

 2+ 2

yi,Di - [ci]Di

2 2

+

 2

yi,[n]/Di - [ci][n]/Di

22, for i  [P Q],

which can be decomposed into

minimize 
yi,Di

yi,Di

2

+

 2

yi,Di - [ci]Di

22, for i  [P Q],

and

minimize
yi,[n]/Di

yi,[n]/Di - [ci][n]/Di 22, for i  [P Q].

(41) (42) (43)

13

Under review as a conference paper at ICLR 2019

The solution to problem (42) can be obtained through the block soft thresholding operator (Parikh et al., 2014),

yik+1 Di =

 1-
 [ci]Di 2

The solution to problem (43) is given by,

[ci]Di , for i  [P Q],
+

(44)

yik+1 [n]/Di = [ci][n]/Di , for i  [P Q].

(45)

z-step Problem (36) can be simplified to

minimize
z

 f (x0 + z) + 2

z-a

2 2

+

 2

z-b

2 2

+

 2

PQ

z - ci

2 2

,

i=1

(46)

where a := k+1 + uk/, b := wk+1 + sk/, and ci := yik+1 + vik/. We solve problem (46) using the linearization technique (Suzuki, 2013; Liu et al., 2018; Boyd et al., 2011). More

specifically, the function f is replaced with its first-order Taylor expansion at the point zk by adding

a Bregman divergence term (k/2) z - zk 22. As a result, problem (46) becomes

minimize
z

(f (zk

+

x0))T (z

-

zk )

+

k 2

z - zk

2 2

+

 2

 +
2

z-b

2 2

+

 2

PQ

z - ci 22,

i=1

z-a

2 2

(47)

whose solution is given by

zk+1 = kzk + a + b + 

PQ i=1

ci

-

f (zk

+

x0) .

k + (2 + P Q)

(48)

D PROOF OF PROPOSITION 3

We start by converting problem (20) into the ADMM form

minimize f (x0 + z) + g(z) + D() + h() + g()
,z
subject to  = z,

(49)

where z and  are optimization variables, g() is an indicator function with respect to the constraint {i = 0, if i  S}, and h() is the other indicator function with respect to the other constraints (x0 + )  [0, 1]n,    .

The augmented Lagrangian of problem (20) is given by

L(,

z,

u)

=f (z

+

x0)

+

g(z)

+

D()

+

h()

+

g()

+

uT

(

-

z)

+

 2

-z

22,

(50)

where u is the Lagrangian multiplier.

ADMM yields the following alternating steps

k+1 = arg min L(, zk, uk)

zk+1 = arg min L(k+1, z, uk)
z
uk+1 = uk + (k+1 - zk+1).

(51) (52) (53)

-step

Suppose D() =



2 2

,

problem

(51)

becomes

minimize

subject to





2 2

+

 2

-a

2 2

(x0 + )  [0, 1]n,   

i = 0, if i  S,

(54)

14

Under review as a conference paper at ICLR 2019

where a := zk - uk/. Problem (54) can be decomposed elementwise

minimize
i
subject to

2+ 

i2

- 2aii

+ a2i

=

2+ 

([x0]i + i)  [0, 1], |i|  i = 0, if i  S.

2

i

-

 2+

ai

(55)

The solution to problem (55) is then given by

0

[k+1]i

  
=
  

min{1 - [x0]i , }

max{- [x0]i , - }

 2+

ai

i  S

 2+ 2+

ai ai

> <

min{1 - [x0]i , max{- [x0]i , -

}, },

i i

/ /

S S

otherwise.

(56)

z-step

Problem (52) yields

minimize
z
subject to

 f (x0 + z) + 2

z-a

2 2

zi = 0, if i  S,

(57)

where a = k+1 + uk/. We solve problem (57) using the linearization technique (Suzuki, 2013; Liu et al., 2018; Boyd et al., 2011),

minimize
z

(f (x0

+

zk))T (z

-

zk )

+

k 2

z - zk

2 2

+

 2

z-a

2 2

subject to zi = 0, if i  S,

(58)

where k is a decaying parameter associated with the Bregman divergence term z - zk 22. In problems (57) and (58), only variables {zi} satisfying i / S are unknown. The solution to problem (58) is then given by

[zk+1]i =

0
k[zk]i+[a ]i-[f (zk+x0)]i k +

i  S i / S.

(59)

E ADVERSARIAL SALIENCY MAP (ASM) AND CLASS ACTIVATION MAPPING (CAM)

ASM(x, t)  Rd is defined by the forward derivative of a neural network given the input sample x and the target label t (Papernot et al., 2016b)

ASM(x, t)[i] =

0
 Z (x)t xi

 Z (x)j j=t xi

if

 Z (x)t xi

< 0 or

otherwise,

j=t

 Z (x)j xi

>0

(60)

where Z(x)j is the jth element of logits Z(x), representing the output before the last softmax layer in DNNs. If there exist many classes in a dataset (e.g., 1000 classes in ImageNet), then computing

j=t

 Z (x)j xi

is

intensive.

To

circumvent

the

scalability

issue

of

ASM, we

focus

on

the

logit

change

with respect to the true label t0 and the target label t only. More specifically, we consider three

quantities,



Z (x)t xi

,

-



Z (x)0 xi

,

and

 Z (x)t xi

j=t

 Z (x)j xi

, which correspond to a) promotion of the

score of the target label t, b) suppression of the classification score of the true label t0, and c) a dual

role on suppression and promotion. As a result, we modify (60) as

ASM(x, t)[i] =

0
 Z (x)t xi

 Z (x)t0 xi

if

 Z (x)t xi

< 0 or

 Z (x)t0 xi

>0

otherwise.

(61)

CAM allows us to visualize the perturbation of adversaries on predicted class scores given any pair of image and object label, and highlights the discriminative object regions detected by CNNs (Zhou et al., 2016). In Fig. A2, we show ASM and the discriminative regions identified by CAM on several ImageNet samples.

15

Under review as a conference paper at ICLR 2019

Original label: Africa elephant

Target label: dining table

Original label: Komdo dragon

Target label: tailed frog

Original label: jeep

Target label: ballon

Original label: titi

Target label: gondola

Original label: photocopier

Target label: accordion

Original label: French horn

Target label: shower curtain

(a) (b)
Figure A2: (a) Overlay ASM and BASM   on top of image with the true and the target label. From left to right: original image, ASM (darker color represents larger value of ASM score), BASM   under our attack, and BASM  under C&W attack. Here  in BASM is set by the 90th percentile of ASM scores. (b) From left to right: original image, CAM of original label, and perturbations with target label generated from the StrAttack and C&W attack, respectively.

F EXPERIMENT SETUP AND PARAMETER SETTING
In this work, we consider targeted adversarial attacks since they are believed stronger than untargeted attacks. For targeted attacks, we have different methods to choose the target labels. The average case selects the target label randomly among all the labels that are not the correct label. The best case performs attacks using all incorrect labels, and report the target label that is the least difficult to attack. The worst case performs attacks using all incorrect labels, and report the target label which is the most difficult to attack.
In our experiments, two networks are trained for MNIST and CIFAR-10, respectively, and a pretrained network is utilized for ImageNet. The model architectures for MNIST and CIFAR-10 are the same, both with four convolutional layers, two max pooling layers, two fully connected layers and a softmax layer. It can achieve 99.5% and 80% accuracy on MNIST and CIFAR-10, respectively. For ImageNet, a pre-trained Inception v3 network (Szegedy et al., 2016) is applied which can achieve 96% top-5 accuracy. All experiments are conducted on machines with NVIDIA GTX 1080 TI GPUs.
The implementations of FGM and IFGM are based on the CleverHans package (Papernot et al., 2016a). The key distortion parameter is determined by a fine-grained grid search. For IFGM, we perform 10 FGM iterations and the distortion parameter is set to /10 for effectiveness as shown in Tramèr et al. (2018). The implementation of the C&W attack is based on the opensource code provided by Carlini & Wagner (2017). The maximum iteration number is set to 1000 and it has 9 binary search steps.
In the StrAttack, the group size for MNIST and CIFAR-10 is 2 × 2 and its stride is set to 2 if the non-overlapping mask is used, otherwise the group size is 3 × 3 and stride is 2. The group size for ImageNet is 13 × 13 and its stride is set to 13. In ADMM, the parameter  achieves a trade-off between the convergence rate and the convergence value. A larger  could make ADMM converging faster but usually leads to perturbations with larger p distortion values. A proper configuration of the parameters is suggested as follows: We set the penalty parameter  = 1, decaying parameter in (14) 1 = 5,  = 2 and  = 1. Moreover, we set c defined in (3) to 0.5 for MNIST, 0.25 for CIFAR-10, and 2.5 for ImageNet. Refined attack technique proposed in Sec. 4.2 is applied for all experiments, we set  is equal to 3% quantile value of non-zero perturbation in . We observe that 73% of  can be retrained to a -sparse perturbation successfully which proof the effective of our refined attack step.
16

Under review as a conference paper at ICLR 2019

C&W attack
Target: 2

Original Image: 0

structured attack
Target: 2

Target: 3 Target: 0

Original Image: 1 Original Image: 4

Target: 3 Target: 0

Target: 2 Target: 9

Original Image: 5
1e-5
Original Image: 7

Target: 2
0.3
Target: 9

Target: 4

Original Image: 9

Target: 4

Figure A3: C&W attack vs StrAttack on MNIST with grid size 2 × 2.
G SUPPLEMENTARY EXPERIMENTAL RESULTS
Some random choice samples from MNIST (Fig. A3), CIFAR-10 (Fig. A4) and ImageNet (Fig. A5) compare StrAttack with C&W attack. For better sparse visual effect, we only show non-overlapping mask function results here. From these samples, we can discover a consistent phenomenon that our StrAttack is more interested in some particular regions, they usually appear on the objects or their edges in original images, distinctly seen in MNIST (Fig. A3) and ImageNet (Fig. A5).
H STRATTACK AGAINST DEFENSIVE DISTILLATION AND ADVERSARIAL TRAINING
In this section, we present the performance of the StrAttack against defensive distillation (Papernot et al., 2016c) and adversarial training (Tramèr et al., 2018). In defensive distillation, we evaluate the StrAttack for different temperature parameters on MNIST and CIFAR-10. We generate 9000 adversarial examples with 1000 randomly selected images from MNIST and CIFAR-10, respectively. The attack success rates of the StrAttack for different temperatures T are all 100%. The reason is that distillation at temperature T makes the logits approximately T times larger but does not change the relative values of logits. The StrAttack which works on the relative values of logits does not fail.
17

Under review as a conference paper at ICLR 2019

C&W attack
Target: automobile

Original Image: airplane

structured attack
Target: automobile

airplane ship airplane

bird

truck

bird

deer cat deer

Figure A4: C&W attack vs StrAttack on CIFAR-10 with grid size 2 × 2.

We further use the StrAttack to break DNNs training on adversarial examples (Tramèr et al., 2018) with their correct labels on MNIST. The StrAttack is performed on three neural networks: the first network is unprotected, the second is obtained by retraining with 9000 C&W adversarial examples, and the third network is retained with 9000 adversarial examples crafted by the StrAttack. The success rate and distortions on the three networks are shown in Table A1. The StrAttack can break all three networks with 100% success rate. However, adversarial training shows certain defense effects as an increase on the 1 or 2 distortion on the latter two networks over the unprotected network is observed.

Table A1: StrAttack against adversarial training on MNIST

Adversarial training
None C&W structured

Best case

ASR 1

2

100 10.9 1.51

100 16.1 1.87

100 15.6 1.86

Average case

ASR

1

2

100 18.05 2.16

100 25.1 2.58

100 25.1 2.61

Worst case

ASR 1

2

100 26.9 2.81

100 34.2 3.26

100 34.6 3.31

18

Under review as a conference paper at ICLR 2019

C&W attack
Target: television
boathouse
miniature schnauzer

Original Image: black swan

structured attack
Target: television

wombat

boathouse

jellyfish

miniature schnauzer

EntleBucher leaf beetle

flatworm
3e-4
brambling

EntleBucher
3e-4
leaf beetle

Figure A5: C&W attack vs StrAttack on ImageNet with grid size 13 × 13.

19

