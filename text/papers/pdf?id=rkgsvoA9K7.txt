Under review as a conference paper at ICLR 2019
DIRICHLET VARIATIONAL AUTOENCODER
Anonymous authors Paper under double-blind review
ABSTRACT
This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior for a continuous latent variable that exhibits the characteristic of the categorical probabilities. To infer the parameters of DirVAE, we utilize the stochastic gradient method by approximating the Gamma distribution, which is a component of the Dirichlet distribution, with the inverse Gamma CDF approximation. Additionally, we reshape the component collapsing issue by investigating two problem sources, which are decoder weight collapsing and latent value collapsing, and we show that DirVAE has no component collapsing; while Gaussian VAE exhibits the decoder weight collapsing and Stick-Breaking VAE shows the latent value collapsing. The experimental results show that 1) DirVAE models the latent representation result with the best log-likelihood compared to the baselines; and 2) DirVAE produces more interpretable latent values with no collapsing issues which the baseline models suffer from. Also, we show that the learned latent representation from the DirVAE achieves the best classification accuracy in the semi-supervised and the supervised classification tasks on MNIST, OMNIGLOT, and SVHN compared to the baseline VAEs. Finally, we demonstrated that the DirVAE augmented topic models show better performances in most cases.
1 INTRODUCTION
A Variational Autoencoder (VAE) (Kingma & Welling, 2014c) brought success in deep generative models (DGMs) with a Gaussian distribution as a prior distribution (Jiang et al., 2017; Miao et al., 2016; 2017; Srivastava & Sutton, 2017). If we focus on the VAE, the VAE assumes the prior distribution to be N (0, I) with the learning on the approximated µ^ and ^ . Also, Stick-Breaking VAE (SBVAE) (Nalisnick & Smyth, 2017) is a nonparametric version of the VAE, which modeled the latent dimension to be infinite using a stick-breaking process (Ishwaran & James, 2001).
While these VAEs assume that the prior distribution of the latent variables to be continuous random variables, recent studies introduce the approximations on discrete priors with continuous random variables (Jang et al., 2017; Maddison et al., 2017; Rolfe, 2017). The key of these approximations is enabling the backpropagation with the reparametrization technique, or the stochastic gradient variational Bayes (SGVB) estimator, while the modeled prior follows a discrete distribution. The applications of these approximations on discrete priors include the prior modeling of a multinomial distribution which is frequently used in the probabilistic graphical models (PGMs). Inherently, the multinomial distributions can take a Dirichlet distribution as a conjugate prior, and the demands on such prior have motivated the works like Jang et al. (2017); Maddison et al. (2017); Rolfe (2017) that support the multinomial distribution posterior without explicit modeling on a Dirichlet prior.
When we survey the work with a explicit modeling on the Dirichlet prior, we found a frequent approach such as utilizing a softmax Laplace approximation (Srivastava & Sutton, 2017). We argue that this approach has a limitation from the multi-modality perspective. The Dirichlet distribution can exhibit a multi-modal distribution with parameter settings, see Figure 1, which is infeasible to generate with the Gaussian distribution with a softmax function. Therefore, the previous continuous domain VAEs cannot be a perfect substitute for the direct approximation on the Dirichlet distribution.
Utilizing a Dirichlet distribution as a conjugate prior to a multinomial distribution has an advantage compared to the usage of a softmax function on a Gaussian distribution. For instance, Figure 1 illustrates the potential difficulties in utilizing the softmax function with the Gaussian distribution.
1

Under review as a conference paper at ICLR 2019
Figure 1: Illustrated probability simplex with Gaussian-Softmax, GEM, and Dirichlet distributions. Unlike the Gaussian-Softmax or the GEM distribution, the Dirichlet distribution is able to capture the multi-modality that illustrates multiple peaks at the vertices of the probability simplex.
Given the three-dimensional probability simplex, the Gaussian-Softmax distribution cannot generate the illustrated case of the Dirichlet distribution with a high probability measure at the vertices of the simplex, i.e. the multi-modality where the necessity was emphasized in Hoffman & Johnson (2016). Additionally, the Griffiths-Engen-McCloskey (GEM) distribution (Pitman, 2002), which is the prior distribution of the SBVAE, is difficult to model the multi-modality because the sampling procedure of the GEM distribution is affected by the rich-get-richer phenomenon, so a few components tend to dominate the weight of the samples. This is different from the Dirichlet distribution that does not exhibit such phenomenon, and the Dirichlet distribution can fairly distribute the weights to the components, and the Dirichlet distribution is more likely to capture the multi-modality by controlling the prior hyper-parameter (Blei et al., 2003). Then, we conjecture that an enhanced modeling on Dirichlet prior is still needed 1) because there are cases that the Gaussian-Softmax approaches, or the softmax Laplace approximation, cannot imitate the Dirichlet distribution; and 2) because the nonparametric approaches could be influenced by the biases that the Dirichlet distribution does not suffer from.
Given these motivations for modeling the Dirichlet distribution with the SGVB estimator, this paper introduces the Dirichlet Variational Autoencoder (DirVAE) that shows the same characteristics of the Dirichlet distribution. The DirVAE is able to model the multi-modal distribution that was not possible with the Gaussian-Softmax and the GEM approaches. These characteristics allow the DirVAE to be the prior of the discrete latent distribution, as the original Dirichlet distribution is.
Introducing the DirVAE requires the configuration of the SGVB estimator on the Dirichlet distribution. Specifically, the Dirichlet distribution is a composition of the Gamma random variables, so we approximate the inverse Gamma cumulative distribution function (CDF) with the finite difference approximation. This approximation on the inverse Gamma CDF becomes the component of approximating the Dirichlet distribution. We compared this approach to the previously suggested approximations, i.e. approaches with the Weibull distribution and with the softmax Gaussian distribution, and our approximation shows the best log-likelihood among the compared approximations.
Moreover, we report that we had to investigate the component collapsing along with the research on DirVAE. It has been known that the component collapsing issue is resolved by the SBVAE because of the meaningful decoder weights from the latent layer to the next layer. However, we found that SBVAE has latent value collapsing issue resulting in many near-zero values on the latent dimensions that leads to the incomplete utilization of the latent dimension. Hence, we argue that Gaussian VAE (GVAE) suffers from the decoder weight collapsing, previously limitedly defined as component collapsing; and SBVAE has a problem of the latent value collapsing. Finally, we suggest that the definition of component collapsing should be expanded to represent both cases of decoder weight and latent value collapsings. We investigated this issue because we intended to propose a better Dirichlet approximation than the previous works, and our performance gain comes from resolving the expanded version of the component collapsing. The proposed DirVAE shows neither the near-zero decoder weights nor the near-zero latent values, so the reconstruction uses the full latent dimension information in most cases.
Technically, the new approximation provides the closed-form loss function derived from the evidence lower bound (ELBO) of the DirVAE. The optimization on the ELBO enables the representation learning with the DirVAE, and we test the learned representation from the DirVAE in two folds. Firstly, we test the representation learning quality by performing the supervised and the
2

Under review as a conference paper at ICLR 2019

semi-supervised classification tasks on MNIST, OMNIGLOT, and SVHN. These classification tasks conclude that DirVAE has the best classification performances with its learned representation. Secondly, we test the applicability of DirVAE to the existing models, such as topic models with DirVAE priors on 20Newsgroup and RCV1-v2. This experiment shows that the augmentation of DirVAE to the existing neural variational topic models improves the perplexity and the topic coherence, and most of best performers were DirVAE augmented.

2 PRELIMINARIES

2.1 VARIATIONAL AUTOENCODERS

A VAE is composed of two parts: a generative sub-model and an inference sub-model. In the generative part, a probabilistic decoder reproduces x^ close to an observation x from a latent variable z  p(z), i.e. x  p(x|z) = p(x|) where  = MLP(z) is obtained from a latent variable z by a multilayer perceptron (MLP). In the inference part, a probabilistic encoder outputs a latent variable z  q(z|x) = q(z|) where  = MLP(x) is computed from the observation x by a MLP. Model parameters,  and , are jointly learned by optimizing the below ELBO with the stochastic
gradient method through the backpropagations as the ordinary neural networks by using the SGVB
estimators on the random nodes.

log p(x)  L(x) = Eq(z|x) [log p(x|z)] - KL(q(z|x)||p(z))

(1)

In GVAE (Kingma & Welling, 2014c), the prior distribution of p(z) is assumed to be a standard Gaussian distribution. In SBVAE (Nalisnick & Smyth, 2017), the prior distribution becomes a GEM distribution that produces samples with a Beta distribution and a stick-breaking algorithm.

2.2 DIRICHLET DISTRIBUTION AS A COMPOSITION OF GAMMA RANDOM VARIABLES

The Dirichlet distribution is a composition of multiple Gamma random variables. Note that the

probability density functions (PDFs) of Dirichlet and Gamma distributions are as follows:

( Dirichlet(x; ) =

k )

(k )

xkk -1 ,

Gamma(x; , )

=

 x-1e-x ()

(2)

where k, ,  > 0. In detail, if there are K independent random variables following the Gamma distributions Xk  Gamma(k, ) or X  MultiGamma(,  · 1K ) where k,  > 0 for k = 1, · · · , K, then we have Y  Dirichlet() where Yk = Xk/ Xi. It should be noted that the rate
parameter, , should be the same for every Gamma distribution in the composition. Then, the KL

divergence can be derived as the following:

KL(Q||P ) = log (k) - log (^k) + (^k - k)(^k)

(3)

for P = MultiGamma(,  ·1K) and Q = MultiGamma(^,  ·1K) where  is a digamma function. The detailed derivation is provided in Appendix B.

2.3 SGVB FOR GAMMA RANDOM VARIABLE AND APPROXIMATION ON DIRICHLET
DISTRIBUTION
This section discusses several ways of approximating the Dirichlet random variable; or the SGVB estimators for the Gamma random variables which compose a Dirichlet distribution. Utilizing SGVB requires a differentiable non-centered parametrization (DNCP) for the distribution (Kingma & Welling, 2014d). The main SGVB for Gamma random variables, used in DirVAE, is using the inverse Gamma CDF approximation explained in the next section. Prior works include two approaches: the use of the Weibull distribution and the softmax Gaussian distribution, and the two approaches are explained in this section.

Approximation with Weibull distribution. Because of the similar PDFs between the Weibull distribution and the Gamma distribution, some prior works used the Weibull distribution as a posterior distribution of the prior Gamma distribution (Zhang et al., 2018):

k Weibull(x; k, ) =

x

k-1e-(x/)k where k,  > 0 .



(4)

3

Under review as a conference paper at ICLR 2019

The paper Zhang et al. (2018) pointed out that there are two useful characteristics when approxi-
mating the Gamma distribution with the Weibull distribution. One useful property is that the KL
divergence expressed in a closed form, and the other is the simple reparametrization trick with a
closed form of the inverse CDF from the Weibull distribution. However, we noticed that the Weibull distribution has a component of e-(x/)k , and the Gamma distribution does not have the additional power term of k in the component. Since k is placed in the exponential component, small changes on k can cause a significant difference that limits the optimization.

Approximation with softmax Gaussian distribution. As in MacKay (1998); Srivastava & Sutton (2017), a Dirichlet distribution can be approximated by a softmax Gaussian distribution by using a softmax Laplace approximation. The relation between the Dirichlet parameter  and the Gaussian parameters µ,  is explained as the following:

µk

=

log

k

-

1 K

i

1 log i, k = k

1- 2 K

1 + K2

i

1 ,
i

(5)

where  is assumed to be a diagonal matrix, and we use the reparametrization trick in the usual GVAE for the SGVB estimator.

3 MODEL DESCRIPTION
Along with the inverse Gamma CDF approximation, we describe two sub-models in this section: the generative sub-model and the inference sub-model. Figure 2 describes the graphical notations of various VAEs and the neural network view of our model.

(a) GVAE

(b) SBVAE

(c) DirVAE

(d) DirVAE in the neural network view

Figure 2: Sub-figures 2a, 2b, and 2c are the graphical notations of the VAEs as latent variable models. The solid lines indicate the generative sub-models where the waved lines denote a prior distribution of the latent variables. The dotted lines indicate the inference sub-models. Sub-figure 2d denotes a neural network structure corresponding to Sub-figure 2c. Red nodes denote the random nodes which allow the backpropagation flows to the input.

Generative sub-model. The key difference between the generative models between the DirVAE and the GVAE is the prior distribution assumption on the latent variable z. Instead of using the standard Gaussian distribution, we use the Dirichlet distribution which is a conjugate prior distribution of the multinomial distribution.

z  p(z) = Dirichlet(), x  p(x|z)

(6)

Inference sub-model. The probabilistic encoder with an approximating posterior distribution q(z|x) is designed to be Dirichlet(^). The approximated posterior parameter ^ is derived by the MLP from the observation x with the softplus output function, so the outputs can be positive values constrained by the Dirichlet distribution. Here, we do not directly sample z from the Dirichlet distribution. Instead, we use the Gamma composition method described in Section 2.2. Firstly, we draw v  MultiGamma(,  · 1K). Afterwards, we normalize v with its summation vi.
The objective function to optimize the model parameters,  and , is composed of Equation (1) and (3). Equation (7) is the loss function to optimize after the composition. The inverse Gamma CDF method explained in the next paragraph enables the backpropagation flows to the input with the stochastic gradient method. Here, for the fair comparison of expressing the Dirichlet distribution

4

Under review as a conference paper at ICLR 2019

between the inverse Gamma CDF approximation method and the softmax Gaussian method, we set k = 1 - 1/K when µk = 0 and k = 1 by using Equation (5); and  = 1.
L(x) = Eq(z|x) [log p(x|z)] - ( log (k) - log (^k) + (^k - k)(^k)) (7)
Approximation with inverse Gamma CDF. A previous work Knowles (2015) suggested that, if X  Gamma(, ), and if F (x; , ) is a CDF of the random variable X, the inverse CDF can be approximated as F -1(u; , )  -1(u())1/. Hence, we can introduce an auxiliary variable u  Uniform(0, 1) to take over all the randomness of X, and we treat the Gamma sampled X as a deterministic value in terms of  and .
Even though the inverse Gamma CDF method and the decomposition of the Dirichlet distribution into Gamma random variables are found from the past works, to our knowledge, combining the two statistical results is the first finding in the machine learning field. In the next section, we demonstrate that the proposed DirVAE is one of the cornerstones in the field of DGMs with various experiments.

4 EXPERIMENTAL RESULTS

This section reports the experimental results with the following experiment settings: 1) a pure VAE model; 2) a semi-supervised classification task with VAEs; 3) a supervised classification task with VAEs; and 4) topic models with DirVAE augmentations.

4.1 EXPERIMENTS FOR REPRESENTATION LEARNING OF VAES
Baseline models. We select the following models as baseline alternatives of the DirVAE: 1) the standard GVAE; 2) the GVAE with softmax (GVAE-Softmax) approximating the Dirichlet distribution with the softmax Gaussian distribution; 3) the SBVAE with the Kumaraswamy distribution (SBVAE-Kuma) & the Gamma composition (SBVAE-Gamma) described in Nalisnick & Smyth (2017); and 4) the DirVAE with the Weibull distribution (DirVAE-Weibull) approximating the Gamma distribution with the Weibull distribution described in Zhang et al. (2018). We use the following benchmark datasets for the experiments: 1) MNIST; 2) MNIST with rotations (MNIST+rot); 3) OMNIGLOT; and 4) SVHN with PCA transformation. We provide the details on the datasets in Appendix D.1.

Experimental setting. As a pure VAE model, we compare the DirVAE with the following models: GVAE, GVAE-Softmax, SBVAE-Kuma, SBVAE-Gamma, and DirVAE-Weibull. We use 50dimension and 100-dimension latent variables for MNIST and OMNIGLOT, respectively. We provide the details of the network structure and optimization in Appendix D.2. We set  = 0.98 · 150 for MNIST and  = 0.99 · 1100 for OMNIGLOT for the fair comparison to GVAEs by using Equation (5). All experiments use the Adam optimizer (Kingma & Ba, 2014a) for the parameter
learning. Finally, we acknowledge that the hyper-parameter could be updated as Appendix C, and
the experiment result with the update is separately reported in Appendix D.2.

Quantitative result. For the quantitative comparison among the VAEs, we calculated the Monte-

Carlo estimation on the marginal negative log-likelihood, the negative ELBO, and the reconstruction

loss. The marginal log-likelihood is approximated as p(x) 

p(x|zi )p(zi ) i q(zi)

for

single

instance

x where q(z) is a posterior distribution of a prior distribution p(z), which is further derived in

Appendix A. Here, we used 100 samples for each 1, 000 randomly selected from the test data. Table

1 shows the overall performance of the alternative VAEs. The DirVAE outperforms all baselines

in both datasets from the log-likelihood perspective. The value of DirVAE comes from the better

encoding of the latent variables that can be used for classification tasks which we examine in the next

experiments. While the DirVAE-Weibull follows the prior modeling with the Dirichlet distribution,

the Weibull based approximation can be improved by adopting the proposed approach with the

inverse Gamma CDF.

Qualitative result. As a qualitative result, we report the latent dimension-wise reconstructions which are decoder outputs with each one-hot vector in the latent dimension. Figure 3a shows 50

5

Under review as a conference paper at ICLR 2019

Table 1: Negative log-likelihood, negative ELBO, and reconstruction loss of the VAEs for MNIST and OMNIGLOT dataset. The lower values are the better for all measures.

GVAE (Nalisnick & Smyth, 2017) SBVAE-Kuma (Nalisnick & Smyth, 2017) SBVAE-Gamma (Nalisnick & Smyth, 2017)
GVAE GVAE-Softmax SBVAE-Kuma SBVAE-Gamma DirVAE-Weibull
DirVAE

MNIST (K = 50)

Neg. LL Neg. ELBO Reconst. loss

96.80 98.01 100.74

- - -

- - -

94.54±0.79 98.18±0.61 99.27±0.48 102.14±0.69 114.59±11.15

98.58±0.04 103.49±0.16 102.60±1.81 135.30±0.24 183.33±2.96

74.31±0.13 79.36±0.82 83.90±0.82 113.89±0.25 150.92±3.70

87.64±0.64 100.47±0.35 81.50±0.27

OMNIGLOT (K = 100)

Neg. LL Neg. ELBO Reconst. loss

--- --- ---

119.29±0.44 130.01±1.16 130.73±2.17 128.82±1.82 140.89±3.21
108.24±0.42

126.42±0.24 139.73±0.81 132.86±3.03 149.30±0.82 198.01±2.46
120.06±0.35

98.90±0.36 123.34±1.43 119.25±1.00 136.36±1.53 145.52±3.13
99.78±0.36

reconstructed images corresponding to each latent dimension from GVAE-Softmax, SBVAE, and DirVAE. We manually ordered the digit-like figures in the ascending order for GVAE-Softmax and DirVAE. We can see that the GVAE-Softmax and the SBVAE have components without significant semantic information, which we will discuss further in Section 4.2, and the DirVAE has interpretable latent dimensions in most of the latent dimensions. Figure 3b also supports the quality of the latent values from DirVAE by visualizing learned latent values through t-SNE (Maaten & Hinton, 2008).

(a) Latent dimension-wise reconstructions of GVAESoftmax, SBVAE, and DirVAE. The DirVAE shows more meaningful latent dimensions than other VAEs. (b) (Left) GVAE, (Middle) SBVAE, (Right) DirVAE.
Figure 3: Latent dimension visualization with reconstruction images and t-SNE latent embeddings.
4.2 DISCUSSION ON COMPONENT COLLAPSING
Decoder weight collapsing, a.k.a. component collapsing. One main issue of GVAE is component collapsing that there are a significant number of near-zero decoder weights from the latent neurons to the next decoder neurons. If these weights become near-zero, the values of the latent dimensions loose influence to the next decoder, and this means an inefficient learning given a neural network structure. The same issue occurs when we use the GVAE-Softmax. We rename this component collapsing phenomenon as decoder weight collapsing to specifically address the collapsing source.
Latent value collapsing. SBVAE claims that SBVAE solved the decoder weight collapsing by learning the meaningful weights as shown in Figure 4a. However, we notice that SBVAE produces the output values, not the link parameters, from the latent dimension to be near-zero in many latent dimensions after averaging many samples obtained from the test dataset. Figure 4b shows the properties of DirVAE and SBVAE from the perspective of the latent value collapsing, which SBVAE shows many near-zero average means and near-zero average variances, while DirVAE does not. The average Fisher kurtosis and average skewness of DirVAE are 5.76 and 2.03, respectively over the dataset, while SBVAE has 20.85 and 4.35, which states that the latent output distribution from SBVAE is more skewed than that of DirVAE. We found out that these near-zero latent values prevent learning on decoder weights, which we introduce as another type of collapsing problem, as latent value collapsing that is different from the decoder weight collapsing. These results mean that SBVAE distributes the non-near-zero latent values sparsely over a few dimensions while DirVAE samples relatively dense latent values. In other words, DirVAE utilizes the full spectrum of latent dimensions compared to SBVAE, and DirVAE has a better learning capability in the decoder network. Figure 3a supports the argument on the latent value collapsing by activating each and single latent dimension with a one-hot vector through the decoder. The non-changing latent dimensionwise images of SBVAE proves that there were no generation differences between the two differently activated one-hot latent values.
6

Under review as a conference paper at ICLR 2019

18 GVAE 16 14 12 10 8 6 4 2 0 60 SBVAE
50
40
30
20
10
0

120 GVAE-Softmax
100
80
60
40
20
0 40 DirVAE 35 30 25 20 15 10 5 0

(a) Latent dimension-wise L2-norm of decoder weights of VAEs.

0.40 SBVAE 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00 0.06 DirVAE
0.05
0.04
0.03
0.02
0.01
0.00
(b) Latent values of VAEs.

Figure 4: Sub-figure 4a shows GVAE and GVAE-Softmax have component collapsing issue, while SBVAE and DirVAE do not. Sub-figure 4b shows that SBVAE has many near-zero output values in the latent dimensions.

4.3 APPLICATION 1. EXPERIMENTS OF (SEMI-)SUPERVISED CLASSIFICATION WITH VAES
Semi-supervised classification task with VAEs. There is a previous work demonstrating that the SBVAE outperforms the GVAE in semi-supervised classification task (Nalisnick & Smyth, 2017). The overall model structure for this semi-supervised classification task uses a VAE with separate random variables of z and y, which is introduced as the M2 model in the original VAE work (Kingma et al., 2014b). The detailed settings of the semi-supervised classification tasks are enumerated in Appendix D.3. Fundamentally, we applied the same experimental settings to GVAE, SBVAE, and DirVAE in this experiment, as specified by the authors in Nalisnick & Smyth (2017).
Table 2 enumerates the performances of the GVAE, the SBVAE, and the DirVAE, and the result shows that the error rate of classification result using 10%, 5% and 1% of labeled data for each dataset. In general, the experiment shows that the DirVAE has the best performance out of three alternative VAEs. Also, it should be noted that the performance of the DirVAE is more improved in the most complex task with the SVHN dataset.

Table 2: The error rate of semi-supervised classification task using VAEs.

GVAE (Nalisnick & Smyth, 2017) SBVAE (Nalisnick & Smyth, 2017)
DirVAE

MNIST (K = 50)

10% 5%

1%

3.95±0.15 4.86±0.14
4.60±0.07

4.74±0.43 5.29±0.39
5.05±0.18

11.55±2.28 7.34±0.47
7.00±0.17

MNIST+rot (K = 50)

10% 5%

1%

21.78±0.73 11.78±0.39
11.18±0.32

27.72±0.69 14.27±0.58
13.53±0.46

38.13±0.95 27.67±1.39
26.20±0.66

SVHN (K = 50)

10% 5%

1%

36.08±1.49 32.08±4.00
24.81±1.13

48.75±1.47 37.07±5.22
28.45±1.14

69.58±1.64 61.37±3.60
55.99±3.30

Supervised classification task with latent values of VAEs. Also, we tested the performance of the supervised classification task with the learned latent representation from the VAEs. We applied the vanilla version of VAEs to the datasets, and we classified the latent representation of instances with k-Nearest Neighbor (kNN) which is one of the simplest classification algorithms. Hence, this experiment can better distinguish the performance of the representation learning in the classification task. We compare DirVAE with the baselines of GVAE, GVAE-Softmax, SBVAE, and DLGMM. Finally, we used the same experimental settings as in Section 4.1, and further experimental details can be found in Appendix D.4.
Table 3 enumerates the performances from the experimented VAEs in the datasets of MNIST and OMNIGLOT. Both datasets indicated that the DirVAE shows the best performance in reducing the classification error, which we conjecture that the performance is gathered from the better representation learning. It should be noted that, to our knowledge, this is the first reported comparison of latent representation learning on VAEs with kNN in the supervised classification using OMNIGLOT dataset. We identified that the classification with OMNIGLOT is difficult given that the kNN error
7

Under review as a conference paper at ICLR 2019

rates with the raw original data are as high as 69.94%, 69.41%, and 70.10%. This high error rate mainly originates from the number of classification categories which is 50 categories in our test setting of OMNIGLOT, compared to 10 categories in MNIST.

Table 3: The error rate of kNN with the latent representations of VAEs.

GVAE (Nalisnick et al., 2016) SBVAE (Nalisnick et al., 2016) DLGMM (Nalisnick et al., 2016)
GVAE GVAE-Softmax SBVAE
DirVAE
Raw Data

MNIST (K = 50)

k=3

k=5

k = 10

28.40 9.34 9.14

20.96 8.65 8.38

15.33 8.90 8.42

27.16±0.48 25.68±2.64 10.01±0.52
5.98±0.06
3.00

20.20±0.93 21.79±2.17 9.58±0.47
5.29±0.06
3.21

14.89±0.40 18.75±2.06 9.39±0.54
5.06±0.06
3.44

OMNIGLOT (K = 100)

k=3

k=5

k = 10

--- --- ---

92.34±0.25 94.76±0.20 86.90±0.82
76.55±0.23
69.94

91.21±0.18 94.22±0.37 85.10±0.89
73.81±0.29
69.41

88.79±0.35 92.98±0.42 82.96±0.64
70.95±0.29
70.10

4.4 APPLICATION 2. EXPERIMENTS OF TOPIC MODEL AUGMENTATION WITH DIRVAE

One usefulness of the Dirichlet distribution is being a conjugate prior to the multinomial distribution, so it has been widely used in the field of topic modeling, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Recently, some neural variational topic (or document) models have been suggested, for example, ProdLDA (Srivastava & Sutton, 2017), NVDM (Miao et al., 2016), and GSM (Miao et al., 2017). NVDM used the GVAE, and the GSM used the GVAE-Softmax to make the sum-to-one positive topic vectors. Meanwhile, ProdLDA assume the prior distribution to be the Dirichlet distribution with the softmax Laplace approximation. To verify the usefulness of the DirVAE, we literally adapt the probabilistic encoder part of the DirVAE to each model. Two popular performance measures in the topic model fields, which are perplexity and topic coherence via normalized pointwise mutual information (NPMI) (Lau et al., 2014), have been used with 20Newsgroups and RCV1-v2 datasets. Further details of the experiments can be found in Appendix D.5. Table 4 indicates that the augmentation of DirVAE improves the performance in general. Additionally, the best performers from the two measurements are always the experiment cell with DirVAE augmentation except for the perplexity of RCV1-v2, which still remains competent.

Table 4: Topic modeling performances of perpexity and NPMI with DirVAE augmentations.

Perplexity NPMI

Reported Reproduced Add DirVAE
Reported Reproduced Add DirVAE

ProdLDA
1172 1219±8.87 1114±2.30
0.240 0.273±0.019 0.359±0.026

20Newsgroups (K = 50)

NVDM

GSM

837
810±2.60 752±12.17

822
954±1.22 916±1.64

0.186
0.119±0.003 0.247±0.010

0.121
0.199±0.006 0.201±0.003

LDA (Gibbs)
1314±18.50
-
0.225±0.002
-

ProdLDA
1190±45.24 992±2.19
0.194±0.005 0.193±0.004

RCV1-v2 (K = 100)

NVDM

GSM

796±6.24 809±12.60
0.023±0.002 0.131±0.015

1386±21.06 1526±6.11
0.267±0.019 0.308±0.005

LDA (Gibbs)
1126±12.66
-
0.266±0.006
-

5 CONCLUSION
Recent advances in VAEs have become one of the cornerstones in the field of DGMs. The VAEs infer the parameters of explicitly described latent variables, so the VAEs are easily included in the conventional PGMs. While this merit has motivated the diverse cases of merging the VAEs to the graphical models, we ask the fundamental quality of utilizing the GVAE where many models have latent values to be categorical probabilities. The softmax function cannot reproduce the multi-modal distribution that the Dirichlet distribution can. Recognizing this problem, there have been some previous works that approximated the Dirichlet distribution in the VAE settings by utilizing the Weibull distribution or the softmax Gaussian distribution, but the DirVAE with the inverse Gamma CDF shows the better learning performance in our experiments of the representation: the semi-supervised, the supervised classifications, and the topic models. Moreover, DirVAE shows no component collapsing and it leads to better latent representation and performance gain. The proposed DirVAE can be widely used if we recall the popularity of the conjugate relation between the multinomial and the Dirichlet distributions because the proposed DirVAE can be a brick to the construction of complex probabilistic models with neural networks.

8

Under review as a conference paper at ICLR 2019
REFERENCES
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 2003.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. International Conference on Artificial Intelligence and Statistics, 2010.
M. Hoffman and M. Johnson. Elbo surgery: yet another way to carve up the variational evidence lower bound. Neural Information Processing Systems Workshop on Advances in Approximate Bayesian Inference, 2016.
H. Ishwaran and L. F. James. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association, 2001.
E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. International Conference on Learning Representations, 2017.
Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: An unsupervised and generative approach to clustering. International Joint Conference on Artificial Intelligence, 2017.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014a.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. International Conference on Learning Representations, 2014c.
D. P. Kingma and M. Welling. Efficient gradient-based inference through transformations between bayes nets and neural nets. International Conference on Machine Learning, 2014d.
D. P. Kingma, S. Mohamed, D. J. Rezende, , and M. Welling. Semi-supervised learning with deep generative models. Neural Information Processing Systems, 2014b.
D. A. Knowles. Stochastic gradient variational bayes for gamma approximating distributions. arXiv preprint arXiv:1509.01631, 2015.
B. M. Lake, R. R. Salakhutdinov, and J. Tenenbaum. One-shot learning by inverting a compositional causal process. Neural Information Processing Systems, 2013.
J. H. Lau, D. Newman, and T. Baldwin. Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality. European Chapter of the Association for Computational Linguistics, 2014.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the Institute of Electrical and Electronics Engineers, 1998.
L. V. D. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 2008.
D. J. C. MacKay. Choice of basis for laplace approximation. Machine Learning, 1998.
C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. International Conference on Learning Representations, 2017.
Y. Miao, L. Yu, and P. Blunsom. Neural variational inference for text processing. International Conference on Machine Learning, 2016.
Y. Miao, E. Grefenstette, and P. Blunsom. Discovering discrete latent topics with neural variational inference. International Conference on Machine Learning, 2017.
T. Minka. Estimating a dirichlet distribution. Technical report, M.I.T., 2000.
V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. International Conference on Machine Learning, 2010.
9

Under review as a conference paper at ICLR 2019 E. Nalisnick and P. Smyth. Stick-breaking variational autoencoders. International Conference on
Learning Representations, 2017. E. Nalisnick, L. Hertel, and P. Smyth. Approximate inference for deep latent gaussian mixtures.
Neural Information Processing Systems Workshop on Bayesian Deep Learning, 2016. J. Pitman. Combinatorial stochastic processes. Technical report, UC Berkeley, 2002. J. T. Rolfe. Discrete variational autoencoders. International Conference on Learning Representa-
tions, 2017. A. Srivastava and C. Sutton. Autoencoding variational inference for topic models. International
Conference on Learning Representations, 2017. C. K. Snderby, T. Raiko, L. Maale, S. K. Snderby, and O. Winther. Ladder variational autoencoders.
Neural Information Processing Systems, 2016. H. Zhang, B. Chen, D. Guo, and M. Zhou. Whai: Weibull hybrid autoencoding inference for deep
topic modeling. International Conference on Learning Representations, 2018.
10

Under review as a conference paper at ICLR 2019

APPENDIX
This is an appendix for Dirichlet Variational Autoencoder. Here, we describe the derivations of key equations and experimental setting details which were used in the body of the paper. The detailed information such as model names, parameter names, or experiment assumptions is based on the main paper.

A MONTE-CARLO ESTIMATION ON THE MARGINAL LIKELIHOOD

Proposition A.1. The marginal log-likelihood is approximated as p(x) 

i

p(x|zi )p(zi q (zi )

)

,

where

q(z) is a posterior distribution of a prior distribution p(z).

Proof.

q(z) p(x) = p(x, z)dz = p(x, z) dz
z z q(z)

=

p(x|z)p(z) q(z) dz =

p(x|z)p(z) q(z)dz

z q(z) z q(z)



i

p(x|zi)p(zi) q(zi)

where

zi



q(z)

B KL DIVERGENCE OF TWO MULTI-GAMMA DISTRIBUTIONS

Proposition B.1. Define X = (X1, · · · , XK )  MultiGamma(,  · 1K ) as a vector of K independent Gamma random variables Xk  Gamma(k, ) where k,  > 0 for k = 1, · · · , K. The KL divergence between two MultiGamma distributions P = MultiGamma(,  · 1K) and Q = MultiGamma(^,  · 1K) can be derived as the following:

KL(Q||P ) = log (k) - log (^k) + (^k - k)(^k) ,

(8)

where  is a digamma function.

Proof.

Note that the derivative of a Gamma-like function

() 

can be derived as follows:

d d

() 

=

-(

()

-

() log )

=


x-1e-x log x dx .
0

Then, we have the following.

q(x) KL(Q||P ) = q(x) log dx
D p(x)


= ···
00

 ^k -1(^k)e- xk

Gamma(^k, ) log 

k

-1(k)e- xk



= ···
00

Gamma(^k, )

× (^k - k) log  + log (k) - log (^k) +

xk^k -1
dxxkk -1
(^k - k) log xk dx

= (^k - k) log  + log (k) - log (^k)


+ ···
00

^k e- xk (^ k )

xk^ k -1

(^k - k) log xk dx

11

Under review as a conference paper at ICLR 2019

= (^k - k) log  + log (k) - log (^k) + (^k - k)^k -1(^k)-^k  (^k) - (^k) log 
= (^k - k) log  + log (k) - log (^k) + (^k - k)((^k) - log ) = log (k) - log (^k) + (^k - k)(^k)

C HYPER-PARAMETER  LEARNING STRATEGY

In this section, we introduce the method of moment estimator (MME) to update the Dirichlet prior parameter . Suppose we have a set of sum-to-one proportions D = {p1, · · · , pN } sampled from Dirichlet(), then the MME update rule is as the following:

k



S N

1 pn,k where S = K
nk

µ~1,k µ~2,k

- µ~2,k - µ~21,k

for

µ~j,k

=

1 N

pjn,k .
n

(9)

After the burn-in period for stabilizing the neural network parameters, we use the MME for the hyper-parameter learning using the sampled latent values during training. We alternatively update the neural network parameters and hyper-parameter . We choose this estimator because of its closed form nature and consistency (Minka, 2000). The usefulness of the hyper-parameter update can be found in Appendix D.2.
Proposition C.1. Given a proportion set D = {p1, · · · , pN } sampled from Dirichlet(), MME of the hyper-parameter  is as the following:

k



S N

1 pn,k where S = K
n

k

µ~1,k - µ~2,k µ~2,k - µ~12,k

for µ~j,k

=

1 N

pjn,k .
n

Proof. Define µj,k = prior . Then, by the

E[pkj ] law of

as the jth moment of the kth dimension of Dirichlet distribution large number, µj,k  µ~j,k. It can be easily shown that µ1,k =

with
k i i

and µ2,k =

k 1+k i i 1+ i i

=

µ1,k

1+k 1+ i i

so that

numerator

µ1,k - µ2,k µ2,k - µ21,k

= k - k 1 + k i i i i 1 + i i
= k( i=k i) ( i i)(1 + i i)

denominator

µ1,k - µ2,k µ2,k - µ21,k

= k 1 + k - i i 1 + i i

= (

k( i=k i) i i)2(1 + i i)

k 2 i i

holds for each k = 1, · · · , K. Therefore,

i

i

=

µ1,k - µ2,k µ2,k - µ21,k



1 K

k

µ1,k - µ2,k µ2,k - µ21,k



1 K

k

µ~1,k - µ~2,k µ~2,k - µ~21,k

and hence,

S

^k = ( i)µ~1,k = N

pn,k .

in

12

Under review as a conference paper at ICLR 2019
D EXPERIMENTAL SETTINGS
In this section, we support Section 4 in the original paper with more detailed experimental settings. Our Tensorflow implementation is available at https://TO BE RELEASED.
D.1 DATASET DESCRIPTION
We use the following benchmark datasets for the experiments in the original paper: 1) MNIST; 2) MNIST with rotations (MNIST+rot); 3) OMNIGLOT; and 4) SVHN with PCA transformation. MNIST (LeCun et al., 1998) is a hand-written digit image dataset of size 28 × 28 with 10 labels, consists of 60, 000 training data and 10, 000 testing data. MNIST+rot data is reproduced by the authors of Nalisnick & Smyth (2017) consists of MNIST and rotated MNIST1. OMNIGLOT2 (Lake et al., 2013; Snderby et al., 2016) is another hand-written image dataset of characters with 28 × 28 size and 50 labels, consists of 24, 345 training data and 8, 070 testing data. SVHN3 is a Street View House Numbers image dataset with the dimension-reduction by PCA into 500 dimensions (Nalisnick & Smyth, 2017).
D.2 REPRESENTATION LEARNING OF VAES
We divided the datasets into {train,valid,test} as the following: MNIST = {45, 000 : 5, 000 : 10, 000} and OMNIGLOT = {22, 095 : 2, 250 : 8, 070}.
For MNIST, we use 50-dimension latent variables with two hidden layers in the encoder and one hidden layer in the decoder of 500 dimensions. We set  = 0.98 · 150 for the fair comparison to GVAEs using the Equation (5). The batch size was set to be 100. For OMNIGLOT, we use 100dimension latent variables with two hidden layers in the encoder and one hidden layer in the decoder of 500 dimensions. We assume  = 0.99 · 1100 for the fair comparison to the GVAEs using the Equation (5). The batch size was set to be 15.
For both datasets, the gradient clipping is used; ReLU function (Nair & Hinton, 2010) is used as an activation function in hidden layers; Xavier initialization (Glorot & Bengio, 2010) is used for the neural network parameter initialization; and the Adam optimizer (Kingma & Ba, 2014a) is used as an optimizer with learning rate 5e-4 for all VAEs except 3e-4 for the SBVAEs. The prior assumptions for each VAE is the following: 1) N (0, I) for the GVAE and the GVAE-Softmax; 2) GEM(5) for the SBVAEs; and 3) Dirichlet(0.98 · 150) (MNIST) and Dirichlet(0.99 · 1100) (OMNIGLOT) for the DirVAE-Weibull.
Additionally, DirVAE-Learning use the same  for the initial value, but the DirVAE-Learning optimizes hyper-parameter  by the following stages through the learning iterations using the MME method in Appendix C: 1) the burn-in period for stabilizing the neural network parameters; 2) the alternative update period for the neural network parameters and ; and 3) the update period for the neural network parameters with the fixed learned hyper-parameter . Table 5 shows that there are improvements in the marginal log-likelihood, ELBO, and reconstruction loss with DirVAE-Learning in both datasets. We also give the learned hyper-parameter  in Figure 5.
D.3 SEMI-SUPERVISED CLASSIFICATION TASK WITH VAES
The overall model structure for this semi-supervised classification task uses a VAE with a separate random variable of z and y, which is introduced as the M2 model in the original VAE work (Kingma et al., 2014b). However, the same task with the SBVAE uses a different model modified to ignore the relation between the class label variable y and the latent variable z, but they still share the same parent nodes: q(z, y|x) = q(z|x)q(y|x) where q(y|x) is a discrimitive network for the unseen labels. We follow the structure of SBVAE. Finally, the below are the objective functions to optimize for the labeled and the unlabeled instances of the semi-supervised classification task, respectively:
log p(x, y)  Llabeled(x, y) = Eq(z|x) [log p(x|z, y)] - KL(q(z|x)||p(z)) + log q(y|x) , (10)
1http://www.iro.umontreal.ca/ lisa/twiki/bin/view.cgi/Public/MnistVariations 2https://github.com/yburda/iwae/tree/master/datasets/OMNIGLOT 3http://ufldl.stanford.edu/housenumbers/
13

Under review as a conference paper at ICLR 2019

Table 5: Negative log-likelihood, negative ELBO, and reconstruction loss of the VAEs for MNIST and OMNIGLOT dataset. The lower values are the better for all measures.

GVAE (Nalisnick & Smyth, 2017) SBVAE-Kuma (Nalisnick & Smyth, 2017) SBVAE-Gamma (Nalisnick & Smyth, 2017)
GVAE GVAE-Softmax SBVAE-Kuma SBVAE-Gamma DirVAE-Weibull
DirVAE
DirVAE-Learning

MNIST (K = 50)

Neg. LL Neg. ELBO Reconst. loss

96.80 98.01 100.74

- - -

- - -

94.54±0.79 98.18±0.61 99.27±0.48 102.14±0.69 114.59±11.15

98.58±0.04 103.49±0.16 102.60±1.81 135.30±0.24 183.33±2.96

74.31±0.13 79.36±0.82 83.90±0.82 113.89±0.25 150.92±3.70

87.64±0.64 100.47±0.35 81.50±0.27

84.42±0.53 99.88±0.40 80.73±0.31

OMNIGLOT (K = 100)

Neg. LL Neg. ELBO Reconst. loss

--- --- ---

119.29±0.44 130.01±1.16 130.73±2.17 128.82±1.82 140.89±3.21
108.24±0.42
100.01±0.52

126.42±0.24 139.73±0.81 132.86±3.03 149.30±0.82 198.01±2.46
120.06±0.35
119.73±0.31

98.90±0.36 123.34±1.43 119.25±1.00 136.36±1.53 145.52±3.13
99.78±0.36
99.55±0.32

Figure 5: The optimized dimension-wise  values from DirVAE-Learning with MNIST.
log p(x)  Lunlabeled(x) = Eq(z,y|x) [log p(x|z, y) + H(q(y|x))] - KL(q(z|x)||p(z)) . (11)
In the above, H is an entropy function. The actual training on the semi-supervised learning optimizes the weighted sum of Equation (10) and (11) with a ratio hyper-parameter 0 <  < 1. The datasets are divided into {train,valid,test} as the following: MNIST = {45000, 5000, 10000}, MNIST+rot = {70000, 10000, 20000}, and SVHN = {65000, 8257, 26032}. For SVHN, dimension reduction into 500 dimensions by PCA is applied as preprocessing.
Fundamentally, we applied the same experimental settings to GVAE, SBVAE and DirVAE in this experiment, as specified by the authors in Nalisnick & Smyth (2017).4,5 Specifically, the three VAEs used the same network structures of 1) a hidden layer of 500 dimension for MNIST; and 2) four hidden layers of 500 dimensions for MNIST+rot and SVHN with the residual network for the last three hidden layers. The latent variables have 50 dimensions for all settings. The ratio parameter  is set to be 0.375 for the MNISTs, and 0.45 for SVHN. ReLU function is used as an activation function in hidden layers, and the neural network parameters were initialized by sampling from N (0, 0.001). The Adam optimizer is used with learning rate 3e-4 and the batch size was set to be 100. Finally, the DirVAE sets  = 0.98 · 150 by using Equation (5).
D.4 SUPERVISED CLASSIFICATION TASK WITH LATENT VALUES OF VAES
For the supervised classification task on the latent representation of the VAEs, we used exactly the same experimental settings as in D.2. Since DLGMM is basically a Gaussian mixture model with the SBVAE, DLGMM is a more complex model than the VAE alternatives. We only report the authors' result from Nalisnick et al. (2016) for the comparison purposes. Additionally, we omit the comparison with the VaDE (Jiang et al., 2017) because the VaDE is more customized to be a clustering model rather than the ordinary VAEs that we choose as baselines.
4https://github.com/enalisnick/stick-breaking dgms 5https://www.ics.uci.edu/enalisni/sb dgm supp mat.pdf
14

Under review as a conference paper at ICLR 2019

D.5 TOPIC MODEL AUGMENTATION WITH DIRVAE

For the topic model augmentation experiment, two popular performance measures in the topic model fields, which are perplexity and topic coherence via normalized pointwise mutual information (NPMI) (Lau et al., 2014), have been used with 20Newsgroups6 and RCV1-v27 datasets. 20Newsgroups has 11, 258 train data and 7, 487 test data with vocabulary size 1, 995. For the RCV1-v2 dataset, due to the massive size of the whole data, we randomly sampled 20, 000 train data and 10, 000 test data with vocabulary size 10, 000. The lower is better for the perplexity, and the higher is better for the NPMI.
The specific model structures can be found in the original papers, Srivastava & Sutton (2017); Miao et al. (2016; 2017). We replace the model prior to that of DirVAE to each model and search the hyper-parameter as Table 6 based on the ELBO term with 1, 000 randomly selected test data. We use 500-dimension hidden layers and 50 topics for 20Newsgroups, and 1, 000-dimension hidden layers and 100 topics for RCV1-v2.

Table 6: Hyper-parameter selections for DirVAE augmentations.

Add DirVAE

20Newsgroups (K = 50) ProdLDA NVDM GSM 0.98 · 150 0.95 · 150 0.20 · 150

RCV1-v2 (K = 100)

ProdLDA NVDM

GSM

0.99 · 1100 0.90 · 1100 0.01 · 1100

6https://github.com/akashgit/autoencoding vi for topic models 7http://scikit-learn.org/stable/datasets/rcv1.html
15

