Under review as a conference paper at ICLR 2019
ON GENERALIZATION BOUNDS OF A FAMILY OF RECURRENT NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Recurrent Neural Networks (RNNs) have been widely applied to sequential data analysis. Due to their complicated modeling structures, however, the theory behind is still largely missing. To connect theory and practice, we study the generalization properties of vanilla RNNs as well as their variants, including Minimal Gated Unit (MGU) and Long Short Term Memory (LSTM) RNNs. Specifically, our theory is established under the PAC-Learning framework. The generalization bound is presented in terms of the spectral norms of the weight matrices and the total number of parameters. We also establish refined generalization bounds with additional norm assumptions, and draw a comparison among these bounds. We remark: (1) Our generalization bound for vanilla RNNs is significantly tighter than the best of existing results; (2) We are not aware of any other generalization bounds for MGU and LSTM in the exiting literature; (3) We demonstrate the advantages of these variants in generalization.
1 INTRODUCTION
Recurrent Neural Networks (RNNs) have successfully revolutionized sequential data analysis, and been widely applied to many real world problems, such as natural language processing (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014), speech recognition (Graves et al., 2006; Mikolov et al., 2010; Graves, 2012; Graves et al., 2013), computer vision (Gregor et al., 2015; Xu et al., 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015), healthcare (Lipton et al., 2015; Choi et al., 2016a;b), and robot control (Ku & Lee, 1995; Lee & Teng, 2000; Yoo et al., 2006). Quite a few of these applications can be approached easily in our daily life, such as Google Translate, Google Now, Apple Siri, etc.
The sequential modeling nature of RNNs is significantly different from feedforward neural networks, though they both have neurons as the basic components. RNNs exploit the internal state (also known as hidden unit) to process the sequence of inputs, which naturally captures the dependence of the sequence. RNNs can also be viewed as nonlinear dynamical systems, and reduced to linear dynamical systems given identity activation operators. Besides the vanilla version, RNNs have many other variants. A large class of variants incorporate the so-called "gated" units to trim RNNs for different tasks. Typical examples include Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber (1997)), Gated Recurrent Unit (GRU, Jozefowicz et al. (2015)) and Minimal Gated Unit (MGU, Zhou et al. (2016)).
The success of RNNs owes not only to their special network structures and the ability to fit training data, but also to their good generalization property: They can provide accurate predictions on unseen data. For example, Graves et al. (2013) report that after training with merely 462 speech samples, deep LSTM RNNs achieve a test set error of 17.7% on TIMIT phoneme recognition benchmark, which is the best recorded score. Mikolov et al. (2010) also show that RNNs outperform significantly state-of-the-art backoff models for speech recognition. When using RNNs in Wall Street Journal task to predict the next word in textual data given the context, word error rate is reduced around 18% compared to backoff models trained on the same amount of data, and 12% when backoff model is trained on 5 times more data. Despite of the popularity of RNNs in applications, their theory is less studied than other feedforward neural networks (Bartlett et al., 2017; Neyshabur et al., 2017; Golowich et al., 2017; Li et al., 2018). There are still several long lasting fundamental questions regarding the approximation, trainability, and generalization of RNNs.
1

Under review as a conference paper at ICLR 2019

In this paper, we propose to understand the generalization ability of RNNs and their variants. We aim to answer two questions from a theoretical perspective:
Q.1) Do RNNs suffer from significant curse of dimensionality?
Q.2) What are the advantages of MGU and LSTM over vanilla RNNs?

Our theory is partially motivated by Bartlett et al. (2017), which recently propose a new technique for developing generalization bounds for feedforward neural networks based on empirical Rademacher complexity under the PAC-Learning framework. Neyshabur et al. (2017) further adapt the technique to establish their generalization bound using the PAC-Bayes approach. Then the follow-up work Zhang et al. (2018) use the PAC-Bayes approach to establish a generalization bound for vanilla RNNs. Quite different from Bartlett et al. (2017) and Zhang et al. (2018), our analysis decouples the spectral norms of weight matrices and the number of weight parameters. This makes our analysis conceptually much simpler, and also yields better generalization bound than Zhang et al. (2018).

Taking a sequence to sequence multiclass classification problem as an example, we observe m sequences of data points (xi,t, zi,t)Tt=1, where xi,t  Rdx and the class label zi,t  {1, . . . , K} for all t = 1, ..., T and i = 1, ..., m. Each sequence is drawn independently from some underlying distribution over Rdx×T × {1, . . . , K}. The vanilla RNNs
compute hi,t and yi,t iteratively as follows,

ht <latexit sha1_base64="v4ZZ+3n27QM0FDeMDBfxAnlvQnY=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYiLYGO7SaCUBG8sI5kOSI+xt9pIlu3fH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzgkQKg6777aytb2xubRd2irt7+weHpaPjlolTzXiTxTLWnYAaLkXEmyhQ8k6iOVWB5O1gfDvz209cGxFHDzhJuK/oMBKhYBSt9FgZ9TO89KaVfqnsVt05yCrxclKGHI1+6as3iFmqeIRMUmO6npugn1GNgkk+LfZSwxPKxnTIu5ZGVHHjZ/ODp+TcKgMSxtpWhGSu/p7IqDJmogLbqSiOzLI3E//zuimG134moiRFHrHFojCVBGMy+54MhOYM5cQSyrSwtxI2opoytBkVbQje8surpFWrem7Vu6+V6zd5HAU4hTO4AA+uoA530IAmMFDwDK/w5mjnxXl3Phata04+cwJ/4Hz+ALLbj6c=</latexit>

1

xt
<latexit sha1_base64="wC5NfZJuEuzbwi8IACSSOv3lOAc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsarQyJjSUmHpDAhewte7Bhb++yO2ckhN9gY6Extv4gO/+NC1yh4EsmeXlvJjPzwlQKg6777RQ2Nre2d4q7pb39g8Oj8vFJyySZZtxniUx0J6SGS6G4jwIl76Sa0ziUvB2Ob+d++5FrIxL1gJOUBzEdKhEJRtFKfvWpj9V+ueLW3AXIOvFyUoEczX75qzdIWBZzhUxSY7qem2IwpRoFk3xW6mWGp5SN6ZB3LVU05iaYLo6dkQurDEiUaFsKyUL9PTGlsTGTOLSdMcWRWfXm4n9eN8PoOpgKlWbIFVsuijJJMCHzz8lAaM5QTiyhTAt7K2EjqilDm0/JhuCtvrxOWvWa59a8+3qlcZPHUYQzOIdL8OAKGnAHTfCBgYBneIU3RzkvzrvzsWwtOPnMKfyB8/kDKESOOQ==</latexit>

U <latexit sha1_base64="VLV1WfLffWbJvU05bja4sMBfihk=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmrbQhrLZbtqlm03YnQil9Cd48aCIV3+RN/+N2zYHbX0w8Hhvhpl5YSqFQdf9dgobm1vbO8Xd0t7+weFR+fikZZJMM+6zRCa6E1LDpVDcR4GSd1LNaRxK3g7Ht3O//cS1EYl6xEnKg5gOlYgEo2ilh6pf7Zcrbs1dgKwTLycVyNHsl796g4RlMVfIJDWm67kpBlOqUTDJZ6VeZnhK2ZgOeddSRWNuguni1Bm5sMqARIm2pZAs1N8TUxobM4lD2xlTHJlVby7+53UzjG6CqVBphlyx5aIokwQTMv+bDITmDOXEEsq0sLcSNqKaMrTplGwI3urL66RVr3luzbuvVxpXeRxFOINzuAQPrqEBd9AEHxgM4Rle4c2Rzovz7nwsWwtOPnMKf+B8/gBj7Y0l</latexit>
W <latexit sha1_base64="2vPE/2toIup2Z/Lenq7o8M88OA0=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD9VOdVCuuDV3AbJOvJxUIEdzUP7qD2OWRlwhk9SYnucm6GdUo2CSz0r91PCEsgkd8Z6likbc+Nni1Bm5sMqQhLG2pZAs1N8TGY2MmUaB7Ywojs2qNxf/83ophjd+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmnXa55b8+7rlcZVHkcRzuAcLsGDa2jAHTShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwBm940n</latexit>

h
<latexit sha1_base64="69P0MQb1+ipK1vhUuUqNxbRCjpA=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBhPBU9jNRU8S8OIxgnlgsoTZyWwyZB7LzKwQlvyFFw+KePVvvPk3TpI9aGJBQ1HVTXdXlHBmrO9/e4WNza3tneJuaW//4PCofHzSNirVhLaI4kp3I2woZ5K2LLOcdhNNsYg47UST27nfeaLaMCUf7DShocAjyWJGsHXSY7Vv2Ejgwbg6KFf8mr8AWidBTiqQozkof/WHiqSCSks4NqYX+IkNM6wtI5zOSv3U0ASTCR7RnqMSC2rCbHHxDF04ZYhipV1Jixbq74kMC2OmInKdAtuxWfXm4n9eL7XxdZgxmaSWSrJcFKccWYXm76Mh05RYPnUEE83crYiMscbEupBKLoRg9eV10q7XAr8W3NcrjZs8jiKcwTlcQgBX0IA7aEILCEh4hld484z34r17H8vWgpfPnMIfeJ8/0tqQVA==</latexit>

ht <latexit sha1_base64="3p2Sf4EV1fRYc738z6OONc502Do=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5L0oicpePFYwbRCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl332yltbG5t75R3K3v7B4dH1eOTjkkyzbjPEpnox5AaLoXiPgqU/DHVnMah5N1wcjv3u09cG5GoB5ymPIjpSIlIMIpW8uvjAdYH1ZrbcBcg68QrSA0KtAfVr/4wYVnMFTJJjel5bopBTjUKJvms0s8MTymb0BHvWapozE2QL46dkQurDEmUaFsKyUL9PZHT2JhpHNrOmOLYrHpz8T+vl2F0HeRCpRlyxZaLokwSTMj8czIUmjOUU0so08LeStiYasrQ5lOxIXirL6+TTrPhuQ3vvllr3RRxlOEMzuESPLiCFtxBG3xgIOAZXuHNUc6L8+58LFtLTjFzCn/gfP4AD9SOKQ==</latexit>

Figure 1: A basic building block of vanilla RNNs

hi,t = h (U hi,t-1 + W xi,t) , and yi,t = y (V hi,t) ,

where y and h are activation operators, hi,t  Rdh is the hidden state with hi,0 = 0, and U  Rdh×dh , V  Rdy×dh , and W  Rdh×dx are weight matrices. The activation operators h and y

are entrywise, i.e., h([v1, . . . , vd] ) = [h(v1), . . . , h(vd)] , and Lipschitz with parameters h and y respectively. For simplicity, we assume h(·) = tanh(·), y(0) = 0, and y = 1. Extensions
to general activation operators are given in Section 2. For a new testing sequence (xt, zt)Tt=1, we predict the label sequence using

zt = argmaxj[yt]j, for all t = 1, . . . , T.

To establish the generalization bound, we need to define the "model complexity" of vanilla RNNs.

In this paper, we adopt the empirical Rademacher complexity (ERC, see more details in Section 2),

which has been widely used in the existing literature on PAC-Learning. For many nonparametric

function classes, we often need complicated argument to upper bound their ERC. Our analysis,

however, shows that we can upper bound the ERC of vanilla RNNs in a very simple manner by

exploiting their Lipschitz continuity with respect to (w.r.t) the model parameters, since they are

essentially in parametric forms. More specifically, denote Ft = {ft : {x1, ..., xt}  yt} as the class of mappings from the first t inputs to the t-th output computed by vanilla RNNs. For a matrix

A, A 2 denotes the spectral norm, and for a vector v, v 2 denotes the Euclidean norm. Define

xt -1 x-1

=

t

for

x

=

1.

Then,

informally

speaking,

the

"model

complexity"

of

vanilla

RNNs

satisfies

Complexity = O

d min

 d,

W

2

U U

t 2

-

1

2-1

V2

log

 td

U U

t 2

-

1

2-1

,

where d = dxdh + dh2 + dhdy. We then give the generalization bound in the following statement.

Theorem 1 (informal). Given a collection of samples S = (xi,t, zi,t)tT=1, i = 1, ..., m with xi,t 2  1 and a new testing sequence (xt, zt)Tt=1, with probability at least 1 -  over S, for every margin value  > 0 and ft  F for integer t  T , we have,



P(zt

=

zt)



R,t

+

O



Complexity m

+

log m

1 



,

(1)

where

R,t

=

1 m

m i=1

1([yi,t]zi,t



maxj=zi,t [yi,t]j

+

).

Please refer to Section 2 for a complete statement. The generalization bound in Theorem 1 can be interpreted under three different scenarios1: (I) When U 2 < 1, the generalization bound is

1To ease the discussion, we assume U 2 does not scale with t. Therefore, U 2 < 1 is equivalent to U 2  1 -  for a constant  > 0. A precise statement can be found following Theorem 2.

2

ct <latexit sha1_base64="qx063OU/We40do9XK5orrzTupSw=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYSLYGO7SaCUBG8sI5kOSI+xt9pIlu3vH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzwkRwg5737aytb2xubRd2irt7+weHpaPjlolTTVmTxiLWnZAYJrhiTeQoWCfRjMhQsHY4vp357SemDY/VA04SFkgyVDzilKCVHiu0n+GlP630S2Wv6s3hrhI/J2XI0eiXvnqDmKaSKaSCGNP1vQSDjGjkVLBpsZcalhA6JkPWtVQRyUyQzQ+euudWGbhRrG0pdOfq74mMSGMmMrSdkuDILHsz8T+vm2J0HWRcJSkyRReLolS4GLuz790B14yimFhCqOb2VpeOiCYUbUZFG4K//PIqadWqvlf172vl+k0eRwFO4QwuwIcrqMMdNKAJFCQ8wyu8Odp5cd6dj0XrmpPPnMAfOJ8/qySPog==</latexit>

1

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

ht <latexit sha1_base64="v4ZZ+3n27QM0FDeMDBfxAnlvQnY=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYiLYGO7SaCUBG8sI5kOSI+xt9pIlu3fH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzgkQKg6777aytb2xubRd2irt7+weHpaPjlolTzXiTxTLWnYAaLkXEmyhQ8k6iOVWB5O1gfDvz209cGxFHDzhJuK/oMBKhYBSt9FgZ9TO89KaVfqnsVt05yCrxclKGHI1+6as3iFmqeIRMUmO6npugn1GNgkk+LfZSwxPKxnTIu5ZGVHHjZ/ODp+TcKgMSxtpWhGSu/p7IqDJmogLbqSiOzLI3E//zuimG134moiRFHrHFojCVBGMy+54MhOYM5cQSyrSwtxI2opoytBkVbQje8surpFWrem7Vu6+V6zd5HAU4hTO4AA+uoA530IAmMFDwDK/w5mjnxXl3Phata04+cwJ/4Hz+ALLbj6c=</latexit>

1

xt
<latexit sha1_base64="wC5NfZJuEuzbwi8IACSSOv3lOAc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsarQyJjSUmHpDAhewte7Bhb++yO2ckhN9gY6Extv4gO/+NC1yh4EsmeXlvJjPzwlQKg6777RQ2Nre2d4q7pb39g8Oj8vFJyySZZtxniUx0J6SGS6G4jwIl76Sa0ziUvB2Ob+d++5FrIxL1gJOUBzEdKhEJRtFKfvWpj9V+ueLW3AXIOvFyUoEczX75qzdIWBZzhUxSY7qem2IwpRoFk3xW6mWGp5SN6ZB3LVU05iaYLo6dkQurDEiUaFsKyUL9PTGlsTGTOLSdMcWRWfXm4n9eN8PoOpgKlWbIFVsuijJJMCHzz8lAaM5QTiyhTAt7K2EjqilDm0/JhuCtvrxOWvWa59a8+3qlcZPHUYQzOIdL8OAKGnAHTfCBgYBneIU3RzkvzrvzsWwtOPnMKfyB8/kDKESOOQ==</latexit>

gt <latexit sha1_base64="ofJKBrf7yurm8y6Rs4AdIQ4IycY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glvz4eYn1YrbkNdwmySbyC1KBAZ1j9GowSlsVcIZPUmL7nphjkVKNgks8rg8zwlLIpHfO+pYrG3AT58tg5ubLKiESJtqWQLNXfEzmNjZnFoe2MKU7MurcQ//P6GUatIBcqzZArtloUZZJgQhafk5HQnKGcWUKZFvZWwiZUU4Y2n4oNwVt/eZN0mw3PbXj3zVq7VcRRhgu4hGvw4AbacAcd8IGBgGd4hTdHOS/Ou/Oxai05xcw5/IHz+QMMf44i</latexit>

<latexit sha1_base64="+wPnq5mfo5j9NMc0te1uWUaQEUA=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4Irtc9Ej04hETeSSwIbPDLEyYxzoza0I2/IQXDxrj1d/x5t84wB4UrKSTSlV3uruihDNjff/bK2xsbm3vFHdLe/sHh0fl45O2UakmtEUUV7obYUM5k7RlmeW0m2iKRcRpJ5rczv3OE9WGKflgpwkNBR5JFjOCrZO61b5hI4Grg3LFr/kLoHUS5KQCOZqD8ld/qEgqqLSEY2N6gZ/YMMPaMsLprNRPDU0wmeAR7TkqsaAmzBb3ztCFU4YoVtqVtGih/p7IsDBmKiLXKbAdm1VvLv7n9VIbX4cZk0lqqSTLRXHKkVVo/jwaMk2J5VNHMNHM3YrIGGtMrIuo5EIIVl9eJ+16LfBrwX290rjJ4yjCGZzDJQRwBQ24gya0gACHZ3iFN+/Re/HevY9la8HLZ07hD7zPH1Y1j30=</latexit>
Ug
<latexit sha1_base64="hIJlFHarrACsHKkWbPIAATdJL/4=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmCTTjPsskYnuhdRwKRT3UaDkvVRzGoeSd8Pp3cLvPnFtRKIecZbyIKZjJSLBKFrJr/vDcX1YrbkNdwmySbyC1KBAe1j9GowSlsVcIZPUmL7nphjkVKNgks8rg8zwlLIpHfO+pYrG3AT58tg5ubLKiESJtqWQLNXfEzmNjZnFoe2MKU7MurcQ//P6GUa3QS5UmiFXbLUoyiTBhCw+JyOhOUM5s4QyLeythE2opgxtPhUbgrf+8ibpNBue2/AemrVWs4ijDBdwCdfgwQ204B7a4AMDAc/wCm+Ocl6cd+dj1Vpyiplz+APn8wfbY439</latexit>

W
<latexit sha1_base64="CXq0WSt6oXSKT6+bmuVa67gCis0=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmCTTjPsskYnuhdRwKRT3UaDkvVRzGoeSd8Pp3cLvPnFtRKIecZbyIKZjJSLBKFrJr3eH4/qwWnMb7hJkk3gFqUGB9rD6NRglLIu5QiapMX3PTTHIqUbBJJ9XBpnhKWVTOuZ9SxWNuQny5bFzcmWVEYkSbUshWaq/J3IaGzOLQ9sZU5yYdW8h/uf1M4xug1yoNEOu2GpRlEmCCVl8TkZCc4ZyZgllWthbCZtQTRnafCo2BG/95U3SaTY8t+E9NGutZhFHGS7gEq7BgxtowT20wQcGAp7hFd4c5bw4787HqrXkFDPn8AfO5w/ecY3/</latexit>

rt <latexit sha1_base64="oH0n51gWY1w44tvOQOSpqHm4/NY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glv66HWB9Wa27DXYJsEq8gNSjQGVa/BqOEZTFXyCQ1pu+5KQY51SiY5PPKIDM8pWxKx7xvqaIxN0G+PHZOrqwyIlGibSkkS/X3RE5jY2ZxaDtjihOz7i3E/7x+hlEryIVKM+SKrRZFmSSYkMXnZCQ0ZyhnllCmhb2VsAnVlKHNp2JD8NZf3iTdZsNzG959s9ZuFXGU4QIu4Ro8uIE23EEHfGAg4Ble4c1Rzovz7nysWktOMXMOf+B8/gAdTI4t</latexit>

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

ht <latexit sha1_base64="v4ZZ+3n27QM0FDeMDBfxAnlvQnY=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYiLYGO7SaCUBG8sI5kOSI+xt9pIlu3fH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzgkQKg6777aytb2xubRd2irt7+weHpaPjlolTzXiTxTLWnYAaLkXEmyhQ8k6iOVWB5O1gfDvz209cGxFHDzhJuK/oMBKhYBSt9FgZ9TO89KaVfqnsVt05yCrxclKGHI1+6as3iFmqeIRMUmO6npugn1GNgkk+LfZSwxPKxnTIu5ZGVHHjZ/ODp+TcKgMSxtpWhGSu/p7IqDJmogLbqSiOzLI3E//zuimG134moiRFHrHFojCVBGMy+54MhOYM5cQSyrSwtxI2opoytBkVbQje8surpFWrem7Vu6+V6zd5HAU4hTO4AA+uoA530IAmMFDwDK/w5mjnxXl3Phata04+cwJ/4Hz+ALLbj6c=</latexit>

1

xt
<latexit sha1_base64="wC5NfZJuEuzbwi8IACSSOv3lOAc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsarQyJjSUmHpDAhewte7Bhb++yO2ckhN9gY6Extv4gO/+NC1yh4EsmeXlvJjPzwlQKg6777RQ2Nre2d4q7pb39g8Oj8vFJyySZZtxniUx0J6SGS6G4jwIl76Sa0ziUvB2Ob+d++5FrIxL1gJOUBzEdKhEJRtFKfvWpj9V+ueLW3AXIOvFyUoEczX75qzdIWBZzhUxSY7qem2IwpRoFk3xW6mWGp5SN6ZB3LVU05iaYLo6dkQurDEiUaFsKyUL9PTGlsTGTOLSdMcWRWfXm4n9eN8PoOpgKlWbIFVsuijJJMCHzz8lAaM5QTiyhTAt7K2EjqilDm0/JhuCtvrxOWvWa59a8+3qlcZPHUYQzOIdL8OAKGnAHTfCBgYBneIU3RzkvzrvzsWwtOPnMKfyB8/kDKESOOQ==</latexit>

Under review as a conference paper at ICLR 2019

O

d m

, which only has a logarithmic dependence on t; (II) When

U 2 = 1, the generaliza-

tion bound is O

dt m

, which has a linear dependence on d and t; (III) When


U 2 > 1, the

generalization bound is O

 d3 t m

, which has a polynomial dependence on d and t.

We theoretically justify that vanilla RNNs do not suffer from significant curse of dimensionality.
Because they compute outputs yt recursively using the same weight matrices, and their hidden states ht are entrywise bounded.

Compared with the generalization bound in Zhang et al. (2018), which is of the order

O dt2 W 2 V2 max{1, U t2} , m

our bound is tighter by a factor of t2 for case (I), a factor of t for case (II). Additionally, Zhang
et al. (2018) fail to incorporate the boundedness condition of hidden state into their analysis, thus the generalization bound is exponential in t for case (III). Our generalization bound, however, is still polynomial in d and t for case (III).

Moreover, (II) is closely related to a few recent results on imposing orthogonal constraints on weight matrices to stabilize the training of RNNs (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2016; Vorontsov et al., 2017; Zhang et al., 2018). We remark that from a learning theory perspective, (II) also implies that the orthogonal constraints can potentially help generalization.

We also present refined generalization bounds with additional matrix norm assumptions. These assumptions allow us to derive norm-based generalization bounds. We draw a comparison among these bounds and highlight their advantage under different scenarios.

Our theory can be further extended to several variants, including MGU and LSTM RNNs. Specifically, we show that the gated units in MGU and LSTM RNNs can introduce extra decaying factors to further reduce the dependence on d and t in generalization. Such an advantage in generalization make these RNNs do not suffer from significant curse of dimensionality either. To the best of our knowledge, these are the first results on generalization guarantees for these neural networks.

The rest of the paper is organized as follows: Section 2 presents the generalization bound of vanilla RNNs; Section 3 presents the proof outline of the generalization bound; Section 4 presents refined generalization bounds and their comparison; Section 5 presents the generalization bound of MGU and LSTM RNNs; Section 6 discusses related works and collects open problems.

Notations: the infinity

nGoivrmenbayvevctor v=mRadx, jw|evjd|e. nGoitveetnheavmecattorirxEMuclideRanmn×onr,mwbeydevno22te=the

d i=1

|vi|2,

and

spectral norm

by (2,

M 2 as 1) norm

the by

largest M 2,1

s=ingulina=r1valMue:,iof2M. G, itvheenFarofbuenncituiosnnofr,mwbeydenMoteF2th=e

trace(M M ), and the function infinity norm

by f  = sup |f |. We adopt the standard O(·) notation, which is defined as f (x) = O (g(x)) for

x   if and only if there exists M > 0 and x0, such that |f (x)|  M g(x) for x  x0. We use

O(·) to denote O(·) with hidden log factors.

2 GENERALIZATION OF VANILLA RNNS
To establish the generalization bound, we start with imposing some mild assumptions. Assumption 1. Input data are bounded, i.e., xi,t 2  Bx for all i = 1, . . . , m and t = 1, . . . , T . Assumption 2. The spectral norms of weight matrices are bounded respectively, i.e., U 2  BU ,
V 2  BV , and W 2  BW . Assumption 3. Activation operators h and y are Lipschitz with parameters h and y respectively, and h(0) = y(0) = 0. Additionally, h is entrywise bounded by b.
Assumptions 1 and 2 are moderate assumptions. Moreover, Assumption 3 holds for most commonly used activation operators, such as h(·) = tanh(·) and y(·) = ReLU(·) = max{·, 0} (1-Lipschitz).
Recall that vanilla RNNs compute hi,t and yi,t as follows,
hi,t = h (U hi,t-1 + W xi,t) and yi,t = y (V hi,t) ,

3

Under review as a conference paper at ICLR 2019

where Rdx×t

U  Rdh×dh , V  Rdy×dh , and by concatenating x1, . . . , xt as

W  Rdh×dx . columns of Xt

Given a . Recall

sequence (xt, zt)Tt=1, we that we denote Ft = {ft

define Xt  : Xt  yt}

as the class of mappings from the first t inputs to the t-th output computed by vanilla RNNs. Then

we define the functional margin for the t-th output in vanilla RNNs as

M(ft(Xt), zt) = [ft(Xt)]zt - maxj=zt [ft(Xt)]j .

We further define a ramp loss  (-M(ft(Xt), zt)) : R  R+ to each margin, where  is a piecewise linear function defined as

 (a)

=

1{a

>

0}

+

(1

+

a 

)1{-



a



0},

where 1{A} denotes the indicator function of a set A. Accordingly, the ramp risk is defined

as R(ft) = E [  (-M(ft(Xt), zt))] , and its empirical counterpart is defined as R(ft) =

1 m

m i=1

 (-M(ft(Xi,t), zi,t)) . We then present the formal statement of Theorem 1.

Theorem 2. Let activation operators h and y be given, and Assumptions 1­3 hold. Then for (xt, zt)tT=1 and S = (xi,t, zi,t)tT=1, i = 1, . . . , m drawn i.i.d. from any underlying distribution over Rdx×T × {1, . . . , K}, with probability at least 1 -  over S, for every margin value  > 0 and
every ft  Ft for integer t  T , we have

P (zt = zt)  R (ft) + O

dyBV min

 b d,hBxBW

 t -1 -1



m

log

 t dm

 t -1 -1

+

log

1 

m

,

where d = dxdh + dh2 + dhdy and  = hBU .

We remark that the generalization bound depends on the total number of weights, and the range of

hBU in three cases as indicated in Section 1. More precisely, if hBU

(1

+

1 t

)

for

constant

 > 0 bounded away from zero, the generalization bound is of the order O

dt m

, which has a

polynomial dependence on d and t. As can be seen, with proper normalization on model parameters,

vanilla RNNs do not suffer from significant curse of dimensionality.

We also highlight a tradeoff between generalization and representation of vanilla RNNs. As can be seen, when hBU is strictly smaller than 1, the generalization bound is nearly independent on t. The hidden state, however, only has limited representation ability, since its magnitude diminishes as t grows large. On the contrary, when hBU is strictly greater than 1, the representation ability of hidden state is amplified but the generalization becomes worse. As a consequence, recent empirical results show that imposing extra constraints or regularization, such as U U = I or U 2  1 (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2016; Vorontsov et al., 2017; Zhang et al., 2018), helps
balance the generalization and representation of RNNs.

3 PROOF OF MAIN RESULTS

Our analysis is based on the PAC-learning framework. Due to space limit, we only present an outline of our proof. More technical details are deferred to Appendix A. Before we proceed, we first define the empirical Rademacher complexity as follows.
Definition 1 (Empirical Rademacher Complexity). Let H be a function class and S = {s1, . . . , sm} be a collection of samples. The empirical Rademacher complexity of H given S is defined as

Empirical Rademacher Complexity: RS(H) = E

sup
hH

1 m

m i=1

ih(si)

,

where i's are i.i.d. Rademacher random variables, i.e., P( i = 1) = P( i = -1) = 0.5.

We then proceed with our analysis. Recall that Mohri et al. (2012) give an empirical Rademacher
complexity (ERC)-based generalization bound, which is restated in the following lemma with F,t = {(Xt, zt)   (-M(ft(Xt), zt)) : ft  Ft} .

4

Under review as a conference paper at ICLR 2019

Lemma 1. Given a testing sequence (xt, zt)Tt=1, with probability at least 1 -  over samples S = (xi,t, zi,t)tT=1, i = 1, . . . , m , for every margin value  > 0 and any ft  Ft, we have

P(zt = zt)  R (ft)  R (ft) + 2RS(F,t) + 3

log

2 

2m

.

(2)

Note that Lemma 1 adapts the original version (Theorem 3.1, Chapter 3.1, Mohri et al. (2012)) for the multiclass ramp loss, and we have P(zt = zt)  R(ft) by definition.

Now we only need to bound the ERC RS(F,t). Our analysis consists of three steps. First, we characterize the Lipschitz continuity of vanilla RNNs w.r.t model parameters. Next, we bound the covering number of function class Ft. At last, we derive an upper bound on RS(F,t) via the standard machinery in the PAC-learning framework. Specifically, consider two different sets of
weight matrices (U, V, W ) and (U , V , W ). Given the same activation operators and input data, denote the t-th output as yt and yt respectively. We characterize the Lipschitz property of yt 2 w.r.t model parameters in the following lemma.

Lemma 2. Under Assumptions 1­3, given input (xt)tT=1 and for any integer t  T , yt 2 is Lipschitz in U , V and W , i.e.,

yt - yt 2  LU,t U - U F + LV,t V - V F + LW,t W - W F ,

where

LU,t

=

hBV

BW

tat,

LV,t

=

BW

at,

and

LW,t

=

BV

at

with

at

=

y h Bx

.(hBU )t-1
hBU -1

The detailed proof is provided in Appendix A.2. We give a simple example to illustrate the proof technique. Specifically, we consider a single layer network that outputs y = (W x), where x is the input,  is an activation operator with Lipschitz parameter , and W is a weight matrix. Such a network is Lipschitz in both x and W as follows. Given weight matrices W and W , we have
y - y 2 = (W x) - (W x) 2   x 2 W - W F. Additionally, given inputs x and x , we have
y - y 2 = (W x) - (W x ) 2   W 2 x - x 2.
Since vanilla RNNs are multilayer networks, Lemma 2 can be obtained by telescoping.

We remark that Lemma 2 is the key to the proof of our generalization bound, which separates the spectral norms of weight matrices and the total number of parameters.

Next, we bound the covering number of Ft. Denote by N (Ft, , dist(·, ·)) the minimal cardinality of a subset C  Ft that covers Ft at scale w.r.t the metric dist(·, ·), such that for any ft  Ft, there

exists ft  C satisfying dist(ft, ft) = supXt ft(Xt) - ft(Xt) 2  . The following lemma gives an upper bound on N (Ft, , dist(·, ·)).

Lemma 3.

Under

Assumptions 1­3, N (Ft, , dist(·,

given ·)) 

any 1+

> 0, the covering number of

 6c dt ((hBU )t - 1)
(hBU - 1)

3d2
,

Ft

satisfies

where c = yhBV BW Bx max {1, hBU }.

The detailed proof is provided in Appendix A.3. We briefly explain the proof technique. Given activation operators, since vanilla RNNs are in parametric forms, ft has a one-to-one correspondence to its weight matrices U, V , and W . Lemma 2 implies that dist(·, ·) is controlled by the Frobenius norms of the difference of weight matrices. Thus, it suffices to bound the covering numbers of three weight matrices, which can be obtained by the standard machinery. The product of covering numbers of three weight matrices gives us Lemma 3.

Lastly, we give an upper bound on RS(F,t) in the following lemma.

Lemma 4. Under Assumptions 1­3, given activation operators and samples {(xi,t, zi,t)tT=1, i = 1, . . . , m}, the empirical Rademacher complexity RS(F,t) satisfies

RS (F,t) = O

dyBV min

 b d,

hBxBW

(hBU )t - 1 hBU - 1

log

 t dm


(hBU )t-1 hBU -1

m

S .

=

The detailed proof is provided in Appendix A.4. Our proof exploits the Lipschitz continuity of M and , and uses Dudley's entropy integral as the standard machinery to establish Lemma 4. Combining Lemma 1 and Lemma 4, we complete the proof.

5

Under review as a conference paper at ICLR 2019

4 REFINED GENERALIZATION BOUNDS

When additional norm constraints on weight matrices U, V and W are available, we can further refine generalization bounds. Specifically, we consider the following assumptions.

Assumption 4. The weight matrices satisfy U 2,1  MU , V 2,1  MV , and W 2,1  MW . Assumption 5. The weight matrices satisfy U F  BU,F, V F  BV,F, and W F  BW,F.

Note that Assumption 4 appears in Bartlett et al. (2017) and Assumption 5 appears in Neyshabur et al. (2017). We have an equivalent relation between matrix norms, i.e., · 2  · 2,1  d · F  d · 2. Comparing to Assumption 2, Assumptions 4 and 5 further restrict the model class. We then
establish the following generalization bounds.

Theorem 3. Let activation operators h and y be given, and Assumptions 1­3 hold. Then for (xt, zt)tT=1 and S = (xi,t, zi,t)Tt=1, i = 1, . . . , m drawn i.i.d. from any underlying distribution over Rdx×T × {1, . . . , K}, with probability at least 1 -  over S, for every margin value  > 0 and
every ft  Ft for integer t  T , the following two bounds hold:

· Suppose Assumption 4 also holds. We have

P (zt = zt)  R (ft) + O

( )(MU

+MV

+MW

)t t--11

 log

d

log

 dm

m

+

log

1 

m

,

(3)

where  = 2hyBV BW Bx, d = dxdh + dh2 + dhdy, and  = hBU . · Suppose Assumption 5 also holds. We have

( ) ( )P (zt = zt)  R(ft) + O

y h ht BU +BxBW

 t -1
-1

d ln(d) BU2 ,F+BW2 ,F+BV2 ,F

m

,

where t = min

 b d,

hBxBW

 t -1 -1

,d=

dxdh + dh2 + dhdy, and  = hBU .

(4)

The detailed proof is provided in Appendix B.1. The first bound (3) adapts the matrix covering lemma in Bartlett et al. (2017). The second bound (4) adapts the PAC-Bayes approach (Neyshabur et al., 2017) by analyzing the divergence when imposing small perturbations on the weight matrices.

We compare generalization bounds in Table 1 by differentiating ranges2 of . It can be seen that

when  > 1, both bounds (3) and (4) involve an exponential term in . Thus, Theorem 2 yields

a better result. When   1, we distinguish two cases: the extreme case and the approximately

low rank case. Specifically, remember that in the extreme case, we have · 2,1 = d · 2 and

· F = d · 2, e.g., orthogonal weight matrices. Therefore, bound (4) meets Theorem 2 for

 < 1 and is worse for  = 1. Bound (3) is worse than Theorem 2 for   1. On the other hand, if

the weight matrices are approximately low rank, we have · 2,1 d · 2 and · F

d · 2.

In this case, bound (4) improves Theorem 2 for  < 1 by reducing dependence on d. Additionally,

if t (MU + MV + MW ) < d, bound (3) yields tighter results both for  < 1 and  = 1. Note that

t (MU + MV + MW ) < d also implies that the input sequence is relatively short.

Table 1: Generalization bounds in Theorems 2 and 3 with respect to different ranges of .

Theorem 2

Theorem 3

Bound (3)

Bound (4)

 < 1 O md 

=1 >1

O

dt m



O

 d3 t m

O

t(MU +MV +MW ) m

O

t2(MU +MV +MW ) m

O

tt(MU+MV +MW ) m

O ( )d BU2 ,F+BW2 ,F+BV2 ,F
m



O dt BU2 ,F+BW2 ,F+BV2 ,F
m



O dt

BU2 ,F+BW2 ,F+BV2 ,F m

2For simplicity, we assume again that  does not scale with t. By  < 1, we mean   1 -  for a constant  > 1. A similar characterization applies to  > 1.

6

Under

review

as

a

chotnf1erenU ce<latexitsha1_base64="VLV1WfLffWbJvU05bja4sMBfihk=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmrbQhrLZbtqlm03YnQil9Cd48aCIV3+RN/+N2zYHbX0w8Hhvhpl5YSqFQdf9dgobm1vbO8Xd0t7+weFR+fikZZJMM+6zRCa6E1LDpVDcR4GSd1LNaRxK3g7Ht3O//cS1EYl6xEnKg5gOlYgEo2ilh6pf7Zcrbs1dgKwTLycVyNHsl796g4RlMVfIJDWm67kpBlOqUTDJZ6VeZnhK2ZgOeddSRWNuguni1Bm5sMqARIm2pZAs1N8TUxobM4lD2xlTHJlVby7+53UzjG6CqVBphlyx5aIokwQTMv+bDITmDOXEEsq0sLcSNqKaMrTplGwI3urL66RVr3luzbuvVxpXeRxFOINzuAQPrqEBd9AEHxgM4Rle4c2Rzovz7nwsWwtOPnMKf+B8/gBj7Y0l</latexit> <latexit sha1_base64="v4ZZ+3n27QM0FDeMDBfxAnlvQnY=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYiLYGO7SaCUBG8sI5kOSI+xt9pIlu3fH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzgkQKg6777aytb2xubRd2irt7+weHpaPjlolTzXiTxTLWnYAaLkXEmyhQ8k6iOVWB5O1gfDvz209cGxFHDzhJuK/oMBKhYBSt9FgZ9TO89KaVfqnsVt05yCrxclKGHI1+6as3iFmqeIRMUmO6npugn1GNgkk+LfZSwxPKxnTIu5ZGVHHjZ/ODp+TcKgMSxtpWhGSu/p7IqDJmogLbqSiOzLI3E//zuimG134moiRFHrHFojCVBGMy+54MhOYM5cQSyrSwtxI2opoytBkVbQje8surpFWrem7Vu6+V6zd5HAU4hTO4AA+uoA530IAmMFDwDK/w5mjnxXl3Phata04+cwJ/4Hz+ALLbj6c=</latexit>

xt
<latexit sha1_base64="wC5NfZJuEuzbwi8IACSSOv3lOAc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsarQyJjSUmHpDAhewte7Bhb++yO2ckhN9gY6Extv4gO/+NC1yh4EsmeXlvJjPzwlQKg6777RQ2Nre2d4q7pb39g8Oj8vFJyySZZtxniUx0J6SGS6G4jwIl76Sa0ziUvB2Ob+d++5FrIxL1gJOUBzEdKhEJRtFKfvWpj9V+ueLW3AXIOvFyUoEczX75qzdIWBZzhUxSY7qem2IwpRoFk3xW6mWGp5SN6ZB3LVU05iaYLo6dkQurDEiUaFsKyUL9PTGlsTGTOLSdMcWRWfXm4n9eN8PoOpgKlWbIFVsuijJJMCHzz8lAaM5QTiyhTAt7K2EjqilDm0/JhuCtvrxOWvWa59a8+3qlcZPHUYQzOIdL8OAKGnAHTfCBgYBneIU3RzkvzrvzsWwtOPnMKfyB8/kDKESOOQ==</latexit>

W <latexit sha1_base64="2vPE/2toIup2Z/Lenq7o8M88OA0=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD9VOdVCuuDV3AbJOvJxUIEdzUP7qD2OWRlwhk9SYnucm6GdUo2CSz0r91PCEsgkd8Z6likbc+Nni1Bm5sMqQhLG2pZAs1N8TGY2MmUaB7Ywojs2qNxf/83ophjd+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmnXa55b8+7rlcZVHkcRzuAcLsGDa2jAHTShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwBm940n</latexit>

<latexit sha1_base64="69P0MQb1+ipK1vhUuUqNxbRCjpA=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBhPBU9jNRU8S8OIxgnlgsoTZyWwyZB7LzKwQlvyFFw+KePVvvPk3TpI9aGJBQ1HVTXdXlHBmrO9/e4WNza3tneJuaW//4PCofHzSNirVhLaI4kp3I2woZ5K2LLOcdhNNsYg47UST27nfeaLaMCUf7DShocAjyWJGsHXSY7Vv2Ejgwbg6KFf8mr8AWidBTiqQozkof/WHiqSCSks4NqYX+IkNM6wtI5zOSv3U0ASTCR7RnqMSC2rCbHHxDF04ZYhipV1Jixbq74kMC2OmInKdAtuxWfXm4n9eL7XxdZgxmaSWSrJcFKccWYXm76Mh05RYPnUEE83crYiMscbEupBKLoRg9eV10q7XAr8W3NcrjZs8jiKcwTlcQgBX0IA7aEILCEh4hld484z34r17H8vWgpfPnMIfeJ8/0tqQVA==</latexit>

paper
h

at ICLR
ht <latexit sha1_base64="3p2Sf4EV1fRYc738z6OONc502Do=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5L0oicpePFYwbRCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl332yltbG5t75R3K3v7B4dH1eOTjkkyzbjPEpnox5AaLoXiPgqU/DHVnMah5N1wcjv3u09cG5GoB5ymPIjpSIlIMIpW8uvjAdYH1ZrbcBcg68QrSA0KtAfVr/4wYVnMFTJJjel5bopBTjUKJvms0s8MTymb0BHvWapozE2QL46dkQurDEmUaFsKyUL9PZHT2JhpHNrOmOLYrHpz8T+vl2F0HeRCpRlyxZaLokwSTMj8czIUmjOUU0so08LeStiYasrQ5lOxIXirL6+TTrPhuQ3vvllr3RRxlOEMzuESPLiCFtxBG3xgIOAZXuHNUc6L8+58LFtLTjFzCn/gfP4AD9SOKQ==</latexit>

2019

ct <latexit sha1_base64="qx063OU/We40do9XK5orrzTupSw=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYSLYGO7SaCUBG8sI5kOSI+xt9pIlu3vH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzwkRwg5737aytb2xubRd2irt7+weHpaPjlolTTVmTxiLWnZAYJrhiTeQoWCfRjMhQsHY4vp357SemDY/VA04SFkgyVDzilKCVHiu0n+GlP630S2Wv6s3hrhI/J2XI0eiXvnqDmKaSKaSCGNP1vQSDjGjkVLBpsZcalhA6JkPWtVQRyUyQzQ+euudWGbhRrG0pdOfq74mMSGMmMrSdkuDILHsz8T+vm2J0HWRcJSkyRReLolS4GLuz790B14yimFhCqOb2VpeOiCYUbUZFG4K//PIqadWqvlf172vl+k0eRwFO4QwuwIcrqMMdNKAJFCQ8wyu8Odp5cd6dj0XrmpPPnMAfOJ8/qySPog==</latexit>

1

5

EXTENSIONS

TO

MGU

AND

LSTM

ht 1

R

<latexit sha1_base64="v4ZZ+3n27QM0FDeMDBfxAnlvQnY=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYiLYGO7SaCUBG8sI5kOSI+xt9pIlu3fH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzgkQKg6777aytb2xubRd2irt7+weHpaPjlolTzXiTxTLWnYAaLkXEmyhQ8k6iOVWB5O1gfDvz209cGxFHDzhJuK/oMBKhYBSt9FgZ9TO89KaVfqnsVt05yCrxclKGHI1+6as3iFmqeIRMUmO6npugn1GNgkk+LfZSwxPKxnTIu5ZGVHHjZ/ODp+TcKgMSxtpWhGSu/p7IqDJmogLbqSiOzLI3E//zuimG134moiRFHrHFojCVBGMy+54MhOYM5cQSyrSwtxI2opoytBkVbQje8surpFWrem7Vu6+V6zd5HAU4hTO4AA+uoA530IAmMFDwDK/w5mjnxXl3Phata04+cwJ/4Hz+ALLbj6c=</latexit>
NxNt

S

<latexit sha1_base64="wC5NfZJuEuzbwi8IACSSOv3lOAc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsarQyJjSUmHpDAhewte7Bhb++yO2ckhN9gY6Extv4gO/+NC1yh4EsmeXlvJjPzwlQKg6777RQ2Nre2d4q7pb39g8Oj8vFJyySZZtxniUx0J6SGS6G4jwIl76Sa0ziUvB2Ob+d++5FrIxL1gJOUBzEdKhEJRtFKfvWpj9V+ueLW3AXIOvFyUoEczX75qzdIWBZzhUxSY7qem2IwpRoFk3xW6mWGp5SN6ZB3LVU05iaYLo6dkQurDEiUaFsKyUL9PTGlsTGTOLSdMcWRWfXm4n9eN8PoOpgKlWbIFVsuijJJMCHzz8lAaM5QTiyhTAt7K2EjqilDm0/JhuCtvrxOWvWa59a8+3qlcZPHUYQzOIdL8OAKGnAHTfCBgYBneIU3RzkvzrvzsWwtOPnMKfyB8/kDKESOOQ==</latexit>

+

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

<latexit sha1_base64="RgQFrgPP09G9wiNkCBnzA2EsptM=">AAAB6nicbVBNS8NAEJ34WetX1aOXxVYQhJL0oseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/nbX1jc2t7cJOcXdv/+CwdHTcMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb2d++4lrI2L1iJOE+xEdKhEKRtFKD5XLSr9UdqvuHGSVeDkpQ45Gv/TVG8QsjbhCJqkxXc9N0M+oRsEknxZ7qeEJZWM65F1LFY248bP5qVNybpUBCWNtSyGZq78nMhoZM4kC2xlRHJllbyb+53VTDK/9TKgkRa7YYlGYSoIxmf1NBkJzhnJiCWVa2FsJG1FNGdp0ijYEb/nlVdKqVT236t3XyvWbPI4CnMIZXIAHV1CHO2hAExgM4Rle4c2Rzovz7nwsWtecfOYE/sD5/AEoUY0J</latexit>

gt <latexit sha1_base64="ofJKBrf7yurm8y6Rs4AdIQ4IycY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glvz4eYn1YrbkNdwmySbyC1KBAZ1j9GowSlsVcIZPUmL7nphjkVKNgks8rg8zwlLIpHfO+pYrG3AT58tg5ubLKiESJtqWQLNXfEzmNjZnFoe2MKU7MurcQ//P6GUatIBcqzZArtloUZZJgQhafk5HQnKGcWUKZFvZWwiZUU4Y2n4oNwVt/eZN0mw3PbXj3zVq7VcRRhgu4hGvw4AbacAcd8IGBgGd4hTdHOS/Ou/Oxai05xcw5/IHz+QMMf44i</latexit>
<latexit sha1_base64="+wPnq5mfo5j9NMc0te1uWUaQEUA=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4Irtc9Ej04hETeSSwIbPDLEyYxzoza0I2/IQXDxrj1d/x5t84wB4UrKSTSlV3uruihDNjff/bK2xsbm3vFHdLe/sHh0fl45O2UakmtEUUV7obYUM5k7RlmeW0m2iKRcRpJ5rczv3OE9WGKflgpwkNBR5JFjOCrZO61b5hI4Grg3LFr/kLoHUS5KQCOZqD8ld/qEgqqLSEY2N6gZ/YMMPaMsLprNRPDU0wmeAR7TkqsaAmzBb3ztCFU4YoVtqVtGih/p7IsDBmKiLXKbAdm1VvLv7n9VIbX4cZk0lqqSTLRXHKkVVo/jwaMk2J5VNHMNHM3YrIGGtMrIuo5EIIVl9eJ+16LfBrwX290rjJ4yjCGZzDJQRwBQ24gya0gACHZ3iFN+/Re/HevY9la8HLZ07hD7zPH1Y1j30=</latexit>
Ug
<latexit sha1_base64="hIJlFHarrACsHKkWbPIAATdJL/4=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmCTTjPsskYnuhdRwKRT3UaDkvVRzGoeSd8Pp3cLvPnFtRKIecZbyIKZjJSLBKFrJr/vDcX1YrbkNdwmySbyC1KBAe1j9GowSlsVcIZPUmL7nphjkVKNgks8rg8zwlLIpHfO+pYrG3AT58tg5ubLKiESJtqWQLNXfEzmNjZnFoe2MKU7MurcQ//P6GUa3QS5UmiFXbLUoyiTBhCw+JyOhOUM5s4QyLeythE2opgxtPhUbgrf+8ibpNBue2/AemrVWs4ijDBdwCdfgwQ204B7a4AMDAc/wCm+Ocl6cd+dj1Vpyiplz+APn8wfbY439</latexit>

rt <latexit sha1_base64="oH0n51gWY1w44tvOQOSpqHm4/NY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glv66HWB9Wa27DXYJsEq8gNSjQGVa/BqOEZTFXyCQ1pu+5KQY51SiY5PPKIDM8pWxKx7xvqaIxN0G+PHZOrqwyIlGibSkkS/X3RE5jY2ZxaDtjihOz7i3E/7x+hlEryIVKM+SKrRZFmSSYkMXnZCQ0ZyhnllCmhb2VsAnVlKHNp2JD8NZf3iTdZsNzG959s9ZuFXGU4QIu4Ro8uIE23EEHfGAg4Ble4c1Rzovz7nysWktOMXMOf+B8/gAdTI4t</latexit> <latexit sha1_base64="+wPnq5mfo5j9NMc0te1uWUaQEUA=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4Irtc9Ej04hETeSSwIbPDLEyYxzoza0I2/IQXDxrj1d/x5t84wB4UrKSTSlV3uruihDNjff/bK2xsbm3vFHdLe/sHh0fl45O2UakmtEUUV7obYUM5k7RlmeW0m2iKRcRpJ5rczv3OE9WGKflgpwkNBR5JFjOCrZO61b5hI4Grg3LFr/kLoHUS5KQCOZqD8ld/qEgqqLSEY2N6gZ/YMMPaMsLprNRPDU0wmeAR7TkqsaAmzBb3ztCFU4YoVtqVtGih/p7IsDBmKiLXKbAdm1VvLv7n9VIbX4cZk0lqqSTLRXHKkVVo/jwaMk2J5VNHMNHM3YrIGGtMrIuo5EIIVl9eJ+16LfBrwX290rjJ4yjCGZzDJQRwBQ24gya0gACHZ3iFN+/Re/HevY9la8HLZ07hD7zPH1Y1j30=</latexit>

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

c~t <latexit sha1_base64="7IYKm8XwRp93GEETYH7gLw+FdAI=">AAAB9HicbVA9TwJBEJ3zE/ELtbTZCCZW5I5GShIbS0zkI4EL2dtbYMPe3rk7R0Iu/A4bC42x9cfY+W9c4AoFXzLJy3szmZkXJFIYdN1vZ2t7Z3dvv3BQPDw6PjktnZ23TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wSTu4XfmXJtRKwecZZwP6IjJYaCUbSSX+mjkCHP2HyAlUGp7FbdJcgm8XJShhzNQemrH8YsjbhCJqkxPc9N0M+oRsEknxf7qeEJZRM64j1LFY248bPl0XNybZWQDGNtSyFZqr8nMhoZM4sC2xlRHJt1byH+5/VSHNb9TKgkRa7YatEwlQRjskiAhEJzhnJmCWVa2FsJG1NNGdqcijYEb/3lTdKuVT236j3Uyo16HkcBLuEKbsCDW2jAPTShBQye4Ble4c2ZOi/Ou/Oxat1y8pkL+APn8wdu9ZHU</latexit> <latexit sha1_base64="dB+er7u/0aPiyGEI3sSMpHypazw=">AAAB8XicbVA9SwNBEJ3zM8avqKXNYiJYhbs0WgZsLCOYD0yOsLeZS5bs7h27e0II+Rc2ForY+m/s/Ddukis08cHA470ZZuZFqeDG+v63t7G5tb2zW9gr7h8cHh2XTk5bJsk0wyZLRKI7ETUouMKm5VZgJ9VIZSSwHY1v5377CbXhiXqwkxRDSYeKx5xR66THSs/woaR9VumXyn7VX4CskyAnZcjR6Je+eoOEZRKVZYIa0w381IZTqi1nAmfFXmYwpWxMh9h1VFGJJpwuLp6RS6cMSJxoV8qShfp7YkqlMRMZuU5J7cisenPxP6+b2fgmnHKVZhYVWy6KM0FsQubvkwHXyKyYOEKZ5u5WwkZUU2ZdSEUXQrD68jpp1aqBXw3ua+V6PY+jAOdwAVcQwDXU4Q4a0AQGCp7hFd484714797HsnXDy2fO4A+8zx/L25BR</latexit>

c

Ur <latexit sha1_base64="bUgvZyrQPSR6O7BkYzmRNV/Fvuc=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO0yhQ3bbbO7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemAqujet+O6Wt7Z3dvfJ+5eDw6PikenrW0UmmGPosEYnqhVSj4BJ9w43AXqqQxqHAbji9W/jdJ1SaJ/LRzFIMYjqWPOKMGiv5dX+o6sNqzW24S5BN4hWkBgXaw+rXYJSwLEZpmKBa9z03NUFOleFM4LwyyDSmlE3pGPuWShqjDvLlsXNyZZURiRJlSxqyVH9P5DTWehaHtjOmZqLXvYX4n9fPTHQb5FymmUHJVouiTBCTkMXnZMQVMiNmllCmuL2VsAlVlBmbT8WG4K2/vEk6zYbnNryHZq3VLOIowwVcwjV4cAMtuIc2+MCAwzO8wpsjnRfn3flYtZacYuYc/sD5/AHsGo4I</latexit>
Wg
<latexit sha1_base64="CXq0WSt6oXSKT6+bmuVa67gCis0=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmCTTjPsskYnuhdRwKRT3UaDkvVRzGoeSd8Pp3cLvPnFtRKIecZbyIKZjJSLBKFrJr3eH4/qwWnMb7hJkk3gFqUGB9rD6NRglLIu5QiapMX3PTTHIqUbBJJ9XBpnhKWVTOuZ9SxWNuQny5bFzcmWVEYkSbUshWaq/J3IaGzOLQ9sZU5yYdW8h/uf1M4xug1yoNEOu2GpRlEmCCVl8TkZCc4ZyZgllWthbCZtQTRnafCo2BG/95U3SaTY8t+E9NGutZhFHGS7gEq7BgxtowT20wQcGAp7hFd4c5bw4787HqrXkFDPn8AfO5w/ecY3/</latexit>

Uc <latexit sha1_base64="nGb0ziqbMdgWBw2LRepOfBGsyT0=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO0yhQ3bbbO7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemAqujet+O6Wt7Z3dvfJ+5eDw6PikenrW0UmmGPosEYnqhVSj4BJ9w43AXqqQxqHAbji9W/jdJ1SaJ/LRzFIMYjqWPOKMGiv5dX/I6sNqzW24S5BN4hWkBgXaw+rXYJSwLEZpmKBa9z03NUFOleFM4LwyyDSmlE3pGPuWShqjDvLlsXNyZZURiRJlSxqyVH9P5DTWehaHtjOmZqLXvYX4n9fPTHQb5FymmUHJVouiTBCTkMXnZMQVMiNmllCmuL2VsAlVlBmbT8WG4K2/vEk6zYbnNryHZq3VLOIowwVcwjV4cAMtuIc2+MCAwzO8wpsjnRfn3flYtZacYuYc/sD5/AHVT435</latexit>
Wr <latexit sha1_base64="vz/mx7uwdW27AGvZy2UCSZZe0Fc=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO0yhQ3bbbO7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemAqujet+O6Wt7Z3dvfJ+5eDw6PikenrW0UmmGPosEYnqhVSj4BJ9w43AXqqQxqHAbji9W/jdJ1SaJ/LRzFIMYjqWPOKMGiv59e5Q1YfVmttwlyCbxCtIDQq0h9WvwShhWYzSMEG17ntuaoKcKsOZwHllkGlMKZvSMfYtlTRGHeTLY+fkyiojEiXKljRkqf6eyGms9SwObWdMzUSvewvxP6+fmeg2yLlMM4OSrRZFmSAmIYvPyYgrZEbMLKFMcXsrYROqKDM2n4oNwVt/eZN0mg3PbXgPzVqrWcRRhgu4hGvw4AZacA9t8IEBh2d4hTdHOi/Ou/Oxai05xcw5/IHz+QPvKI4K</latexit>

Wc <latexit sha1_base64="ICxsCyzHFX1UVaYy4kFhbj2nPGQ=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO0yhQ3bbbO7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemAqujet+O6Wt7Z3dvfJ+5eDw6PikenrW0UmmGPosEYnqhVSj4BJ9w43AXqqQxqHAbji9W/jdJ1SaJ/LRzFIMYjqWPOKMGiv59e6Q1YfVmttwlyCbxCtIDQq0h9WvwShhWYzSMEG17ntuaoKcKsOZwHllkGlMKZvSMfYtlTRGHeTLY+fkyiojEiXKljRkqf6eyGms9SwObWdMzUSvewvxP6+fmeg2yLlMM4OSrRZFmSAmIYvPyYgrZEbMLKFMcXsrYROqKDM2n4oNwVt/eZN0mg3PbXgPzVqrWcRRhgu4hGvw4AZacA9t8IEBh2d4hTdHOi/Ou/Oxai05xcw5/IHz+QPYXY37</latexit>

tanh
<latexit sha1_base64="6gN6AKKwFjC/Dz27jWrOkvIQpPI=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LLaCp5KUgh4LXjxWsB/QhrLZbtqlm03YnQgl9Ed48aCIV3+PN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpb2zu7e8X90sHh0fFJ+fSsY+JUM95msYx1L6CGS6F4GwVK3ks0p1EgeTeY3i387hPXRsTqEWcJ9yM6ViIUjKKVutUBUjWpDssVt+YuQTaJl5MK5GgNy1+DUczSiCtkkhrT99wE/YxqFEzyeWmQGp5QNqVj3rdU0YgbP1ueOydXVhmRMNa2FJKl+nsio5ExsyiwnRHFiVn3FuJ/Xj/F8NbPhEpS5IqtFoWpJBiTxe9kJDRnKGeWUKaFvZWwCdWUoU2oZEPw1l/eJJ16zXNr3kO90mzkcRThAi7hGjy4gSbcQwvawGAKz/AKb07ivDjvzseqteDkM+fwB87nD4/Mjv8=</latexit>

<latexit sha1_base64="+wPnq5mfo5j9NMc0te1uWUaQEUA=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4Irtc9Ej04hETeSSwIbPDLEyYxzoza0I2/IQXDxrj1d/x5t84wB4UrKSTSlV3uruihDNjff/bK2xsbm3vFHdLe/sHh0fl45O2UakmtEUUV7obYUM5k7RlmeW0m2iKRcRpJ5rczv3OE9WGKflgpwkNBR5JFjOCrZO61b5hI4Grg3LFr/kLoHUS5KQCOZqD8ld/qEgqqLSEY2N6gZ/YMMPaMsLprNRPDU0wmeAR7TkqsaAmzBb3ztCFU4YoVtqVtGih/p7IsDBmKiLXKbAdm1VvLv7n9VIbX4cZk0lqqSTLRXHKkVVo/jwaMk2J5VNHMNHM3YrIGGtMrIuo5EIIVl9eJ+16LfBrwX290rjJ4yjCGZzDJQRwBQ24gya0gACHZ3iFN+/Re/HevY9la8HLZ07hD7zPH1Y1j30=</latexit>
Uo <latexit sha1_base64="Kb0zcT93dOpcc1myT8ZFtY8k+Jc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLUlsLDHxgAQuZG/Zgw37cdndMyEXfoONhcbY+oPs/DcucIWCL5nk5b2ZzMyLU86M9f1vr7S1vbO7V96vHBweHZ9UT886RmWa0JAornQvxoZyJmlomeW0l2qKRcxpN57eLfzuE9WGKfloZymNBB5LljCCrZPCejhU9WG15jf8JdAmCQpSgwLtYfVrMFIkE1RawrEx/cBPbZRjbRnhdF4ZZIammEzxmPYdlVhQE+XLY+foyikjlCjtSlq0VH9P5FgYMxOx6xTYTsy6txD/8/qZTW6jnMk0s1SS1aIk48gqtPgcjZimxPKZI5ho5m5FZII1JtblU3EhBOsvb5JOsxH4jeChWWs1izjKcAGXcA0B3EAL7qENIRBg8Ayv8OZJ78V79z5WrSWvmDmHP/A+fwDni44F</latexit>

ot <latexit sha1_base64="mpFsBHdFvf28lLVlN5O4kpHBGFw=">AAAB7HicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XOZJ48YiJBRNoyHbZwobtbrM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZelApu0PO+ndLW9s7uXnm/cnB4dHxSPT3rGpVpygKqhNKPETFMcMkC5CjYY6oZSSLBetH0duH3npg2XMkHnKUsTMhY8phTglYK6mqI9WG15jW8JdxN4hekBgU6w+rXYKRoljCJVBBj+r6XYpgTjZwKNq8MMsNSQqdkzPqWSpIwE+bLY+fulVVGbqy0LYnuUv09kZPEmFkS2c6E4MSsewvxP6+fYdwKcy7TDJmkq0VxJlxU7uJzd8Q1oyhmlhCqub3VpROiCUWbT8WG4K+/vEm6zYbvNfz7Zq3dKuIowwVcwjX4cANtuIMOBECBwzO8wpsjnRfn3flYtZacYuYc/sD5/AEYt44q</latexit> <latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>
Wo <latexit sha1_base64="8dlnrEaEbzsGhOr739+jqrRqLDQ=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLUlsLDHxgAQuZG/Zgw37cdndMyEXfoONhcbY+oPs/DcucIWCL5nk5b2ZzMyLU86M9f1vr7S1vbO7V96vHBweHZ9UT886RmWa0JAornQvxoZyJmlomeW0l2qKRcxpN57eLfzuE9WGKfloZymNBB5LljCCrZPCeneo6sNqzW/4S6BNEhSkBgXaw+rXYKRIJqi0hGNj+oGf2ijH2jLC6bwyyAxNMZniMe07KrGgJsqXx87RlVNGKFHalbRoqf6eyLEwZiZi1ymwnZh1byH+5/Uzm9xGOZNpZqkkq0VJxpFVaPE5GjFNieUzRzDRzN2KyARrTKzLp+JCCNZf3iSdZiPwG8FDs9ZqFnGU4QIu4RoCuIEW3EMbQiDA4Ble4c2T3ov37n2sWkteMXMOf+B9/gDqmY4H</latexit>

ct <latexit sha1_base64="pirW76MgnYII8nutbf2Q83ixmBI=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5L0oicpePFYwbRCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl332yltbG5t75R3K3v7B4dH1eOTjkkyzbjPEpnox5AaLoXiPgqU/DHVnMah5N1wcjv3u09cG5GoB5ymPIjpSIlIMIpW8utsgPVBteY23AXIOvEKUoMC7UH1qz9MWBZzhUxSY3qem2KQU42CST6r9DPDU8omdMR7lioacxPki2Nn5MIqQxIl2pZCslB/T+Q0NmYah7Yzpjg2q95c/M/rZRhdB7lQaYZcseWiKJMEEzL/nAyF5gzl1BLKtLC3EjammjK0+VRsCN7qy+uk02x4bsO7b9ZaN0UcZTiDc7gED66gBXfQBh8YCHiGV3hzlPPivDsfy9aSU8ycwh84nz8IMY4k</latexit>
ht <latexit sha1_base64="3p2Sf4EV1fRYc738z6OONc502Do=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5L0oicpePFYwbRCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl332yltbG5t75R3K3v7B4dH1eOTjkkyzbjPEpnox5AaLoXiPgqU/DHVnMah5N1wcjv3u09cG5GoB5ymPIjpSIlIMIpW8uvjAdYH1ZrbcBcg68QrSA0KtAfVr/4wYVnMFTJJjel5bopBTjUKJvms0s8MTymb0BHvWapozE2QL46dkQurDEmUaFsKyUL9PZHT2JhpHNrOmOLYrHpz8T+vl2F0HeRCpRlyxZaLokwSTMj8czIUmjOUU0so08LeStiYasrQ5lOxIXirL6+TTrPhuQ3vvllr3RRxlOEMzuESPLiCFtxBG3xgIOAZXuHNUc6L8+58LFtLTjFzCn/gfP4AD9SOKQ==</latexit>

We extend our analysis to Minimal Gated Unit (MRU) and Long Short-Term Memory (LSTM)
RNNs. We show that these RNNs introduce extra decaying factors to further reduce the dependence on d and t in generalization.

The MGU RNNs are the simplest GRU RNNs, which compute output yt as follows,
rt = (Wrxt + Urht-1),

rt <latexit sha1_base64="oH0n51gWY1w44tvOQOSpqHm4/NY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glv66HWB9Wa27DXYJsEq8gNSjQGVa/BqOEZTFXyCQ1pu+5KQY51SiY5PPKIDM8pWxKx7xvqaIxN0G+PHZOrqwyIlGibSkkS/X3RE5jY2ZxaDtjihOz7i3E/7x+hlEryIVKM+SKrRZFmSSYkMXnZCQ0ZyhnllCmhb2VsAnVlKHNp2JD8NZf3iTdZsNzG959s9ZuFXGU4QIu4Ro8uIE23EEHfGAg4Ble4c1Rzovz7nysWktOMXMOf+B8/gAdTI4t</latexit>

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

Uh <latexit sha1_base64="WwGo1Rp5Q2OaeI9VfuNwpkrOtzo=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmFZoQ9lsJ+3SzSbsboQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MBVcG9f9dkobm1vbO+Xdyt7+weFR9fiko5NMMfRZIhL1GFKNgkv0DTcCH1OFNA4FdsPJ7dzvPqHSPJEPZppiENOR5BFn1FjJr/uDcX1QrbkNdwGyTryC1KBAe1D96g8TlsUoDRNU657npibIqTKcCZxV+pnGlLIJHWHPUklj1EG+OHZGLqwyJFGibElDFurviZzGWk/j0HbG1Iz1qjcX//N6mYlugpzLNDMo2XJRlAliEjL/nAy5QmbE1BLKFLe3EjamijJj86nYELzVl9dJp9nw3IZ336y1roo4ynAG53AJHlxDC+6gDT4w4PAMr/DmSOfFeXc+lq0lp5g5hT9wPn8A3YKOAA==</latexit>

<latexit sha1_base64="D0ousYKRBvlWYFmcQmQNhtQFSy0=">AAAB8XicbVA9SwNBEJ3zM8avqKXNYiJYhbs0WgZsLCOYD0yOsLeZS5bs7h27e0II+Rc2ForY+m/s/Ddukis08cHA470ZZuZFqeDG+v63t7G5tb2zW9gr7h8cHh2XTk5bJsk0wyZLRKI7ETUouMKm5VZgJ9VIZSSwHY1v5377CbXhiXqwkxRDSYeKx5xR66THSs/woaT9UaVfKvtVfwGyToKclCFHo1/66g0SlklUlglqTDfwUxtOqbacCZwVe5nBlLIxHWLXUUUlmnC6uHhGLp0yIHGiXSlLFurviSmVxkxk5DoltSOz6s3F/7xuZuObcMpVmllUbLkozgSxCZm/TwZcI7Ni4ghlmrtbCRtRTZl1IRVdCMHqy+ukVasGfjW4r5Xr9TyOApzDBVxBANdQhztoQBMYKHiGV3jzjPfivXsfy9YNL585gz/wPn8A03SQVg==</latexit>

h

h~ t <latexit sha1_base64="S3CdFfDx9HsepZJgWjBttnLljms=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4r2FZoQ9lsNu3SzSbuTgol9Hd48aCIV3+MN/+N24+Dtj4YeLw3w8y8IJXCoOt+O4WNza3tneJuaW//4PCofHzSNkmmGW+xRCb6MaCGS6F4CwVK/phqTuNA8k4wup35nTHXRiTqAScp92M6UCISjKKV/GoPhQx5Ppz2sdovV9yaOwdZJ96SVGCJZr/81QsTlsVcIZPUmK7npujnVKNgkk9LvczwlLIRHfCupYrG3Pj5/OgpubBKSKJE21JI5urviZzGxkziwHbGFIdm1ZuJ/3ndDKMbPxcqzZArtlgUZZJgQmYJkFBozlBOLKFMC3srYUOqKUObU8mG4K2+vE7a9Zrn1rz7eqVxtYyjCGdwDpfgwTU04A6a0AIGT/AMr/DmjJ0X5935WLQWnOXMKfyB8/kDdWmR1Q==</latexit>

Wh <latexit sha1_base64="3UtSFnvG9s5LzXvAasw77bxdiww=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmFZoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSMUmmGfdZIhP9GFLDpVDcR4GSP6aa0ziUvBtObud+94lrIxL1gNOUBzEdKREJRtFKfr07GNcH1ZrbcBcg68QrSA0KtAfVr/4wYVnMFTJJjel5bopBTjUKJvms0s8MTymb0BHvWapozE2QL46dkQurDEmUaFsKyUL9PZHT2JhpHNrOmOLYrHpz8T+vl2F0E+RCpRlyxZaLokwSTMj8czIUmjOUU0so08LeStiYasrQ5lOxIXirL6+TTrPhuQ3vvllrXRVxlOEMzuESPLiGFtxBG3xgIOAZXuHNUc6L8+58LFtLTjFzCn/gfP4A4JCOAg==</latexit>

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>
rt <latexit sha1_base64="oH0n51gWY1w44tvOQOSpqHm4/NY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glv66HWB9Wa27DXYJsEq8gNSjQGVa/BqOEZTFXyCQ1pu+5KQY51SiY5PPKIDM8pWxKx7xvqaIxN0G+PHZOrqwyIlGibSkkS/X3RE5jY2ZxaDtjihOz7i3E/7x+hlEryIVKM+SKrRZFmSSYkMXnZCQ0ZyhnllCmhb2VsAnVlKHNp2JD8NZf3iTdZsNzG959s9ZuFXGU4QIu4Ro8uIE23EEHfGAg4Ble4c1Rzovz7nysWktOMXMOf+B8/gAdTI4t</latexit>
1<latexitsha1_base64="+wPnq5mfo5j9NMc0te1uWUaQEUA=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4Irtc9Ej04hETeSSwIbPDLEyYxzoza0I2/IQXDxrj1d/x5t84wB4UrKSTSlV3uruihDNjff/bK2xsbm3vFHdLe/sHh0fl45O2UakmtEUUV7obYUM5k7RlmeW0m2iKRcRpJ5rczv3OE9WGKflgpwkNBR5JFjOCrZO61b5hI4Grg3LFr/kLoHUS5KQCOZqD8ld/qEgqqLSEY2N6gZ/YMMPaMsLprNRPDU0wmeAR7TkqsaAmzBb3ztCFU4YoVtqVtGih/p7IsDBmKiLXKbAdm1VvLv7n9VIbX4cZk0lqqSTLRXHKkVVo/jwaMk2J5VNHMNHM3YrIGGtMrIuo5EIIVl9eJ+16LfBrwX290rjJ4yjCGZzDJQRwBQ24gya0gACHZ3iFN+/Re/HevY9la8HLZ07hD7zPH1Y1j30=</latexit> <latexit sha1_base64="wQAvzFe7l0eLgjHF3NV0yfiWEZQ=">AAAB73icbVBNS8NAEJ34WetX1aOXxVbwYkmKoMeCF48V7Ae0oWy2m3bpZhN3J0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZekEhh0HW/nbX1jc2t7cJOcXdv/+CwdHTcMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb2d++4lrI2L1gJOE+xEdKhEKRtFKnYp3SXQfK/1S2a26c5BV4uWkDDka/dJXbxCzNOIKmaTGdD03QT+jGgWTfFrspYYnlI3pkHctVTTixs/m907JuVUGJIy1LYVkrv6eyGhkzCQKbGdEcWSWvZn4n9dNMbzxM6GSFLlii0VhKgnGZPY8GQjNGcqJJZRpYW8lbEQ1ZWgjKtoQvOWXV0mrVvXcqndfK9ev8jgKcApncAEeXEMd7qABTWAg4Rle4c15dF6cd+dj0brm5DMn8AfO5w9M3I7F</latexit>

rt

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

+ <latexit sha1_base64="RgQFrgPP09G9wiNkCBnzA2EsptM=">AAAB6nicbVBNS8NAEJ34WetX1aOXxVYQhJL0oseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/nbX1jc2t7cJOcXdv/+CwdHTcMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb2d++4lrI2L1iJOE+xEdKhEKRtFKD5XLSr9UdqvuHGSVeDkpQ45Gv/TVG8QsjbhCJqkxXc9N0M+oRsEknxZ7qeEJZWM65F1LFY248bP5qVNybpUBCWNtSyGZq78nMhoZM4kC2xlRHJllbyb+53VTDK/9TKgkRa7YYlGYSoIxmf1NBkJzhnJiCWVa2FsJG1FNGdp0ijYEb/nlVdKqVT236t3XyvWbPI4CnMIZXIAHV1CHO2hAExgM4Rle4c2Rzovz7nwsWtecfOYE/sD5/AEoUY0J</latexit>

ht = h (Whxt + Uh(rt ht-1)) ,
ht = (1 - rt) ht-1 + rt ht, yt = y(V ht),

ht <latexit sha1_base64="v4ZZ+3n27QM0FDeMDBfxAnlvQnY=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYiLYGO7SaCUBG8sI5kOSI+xt9pIlu3fH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzgkQKg6777aytb2xubRd2irt7+weHpaPjlolTzXiTxTLWnYAaLkXEmyhQ8k6iOVWB5O1gfDvz209cGxFHDzhJuK/oMBKhYBSt9FgZ9TO89KaVfqnsVt05yCrxclKGHI1+6as3iFmqeIRMUmO6npugn1GNgkk+LfZSwxPKxnTIu5ZGVHHjZ/ODp+TcKgMSxtpWhGSu/p7IqDJmogLbqSiOzLI3E//zuimG134moiRFHrHFojCVBGMy+54MhOYM5cQSyrSwtxI2opoytBkVbQje8surpFWrem7Vu6+V6zd5HAU4hTO4AA+uoA530IAmMFDwDK/w5mjnxXl3Phata04+cwJ/4Hz+ALLbj6c=</latexit>

1

xt
<latexit sha1_base64="wC5NfZJuEuzbwi8IACSSOv3lOAc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsarQyJjSUmHpDAhewte7Bhb++yO2ckhN9gY6Extv4gO/+NC1yh4EsmeXlvJjPzwlQKg6777RQ2Nre2d4q7pb39g8Oj8vFJyySZZtxniUx0J6SGS6G4jwIl76Sa0ziUvB2Ob+d++5FrIxL1gJOUBzEdKhEJRtFKfvWpj9V+ueLW3AXIOvFyUoEczX75qzdIWBZzhUxSY7qem2IwpRoFk3xW6mWGp5SN6ZB3LVU05iaYLo6dkQurDEiUaFsKyUL9PTGlsTGTOLSdMcWRWfXm4n9eN8PoOpgKlWbIFVsuijJJMCHzz8lAaM5QTiyhTAt7K2EjqilDm0/JhuCtvrxOWvWa59a8+3qlcZPHUYQzOIdL8OAKGnAHTfCBgYBneIU3RzkvzrvzsWwtOPnMKfyB8/kDKESOOQ==</latexit>

Ur <latexit sha1_base64="BW2T5sAmwcKzhZ8g1qGxQ2wjKug=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmFZoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSMUmmGfdZIhP9GFLDpVDcR4GSP6aa0ziUvBtObud+94lrIxL1gNOUBzEdKREJRtFKft0f6PqgWnMb7gJknXgFqUGB9qD61R8mLIu5QiapMT3PTTHIqUbBJJ9V+pnhKWUTOuI9SxWNuQnyxbEzcmGVIYkSbUshWai/J3IaGzONQ9sZUxybVW8u/uf1MoxuglyoNEOu2HJRlEmCCZl/ToZCc4ZyagllWthbCRtTTRnafCo2BG/15XXSaTY8t+HdN2utqyKOMpzBOVyCB9fQgjtogw8MBDzDK7w5ynlx3p2PZWvJKWZO4Q+czx/stI4K</latexit>

Wr <latexit sha1_base64="YH7coM1v251MW/+4mqAFEDNLMyM=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmFZoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSMUmmGfdZIhP9GFLDpVDcR4GSP6aa0ziUvBtObud+94lrIxL1gNOUBzEdKREJRtFKfr070PVBteY23AXIOvEKUoMC7UH1qz9MWBZzhUxSY3qem2KQU42CST6r9DPDU8omdMR7lioacxPki2Nn5MIqQxIl2pZCslB/T+Q0NmYah7Yzpjg2q95c/M/rZRjdBLlQaYZcseWiKJMEEzL/nAyF5gzl1BLKtLC3EjammjK0+VRsCN7qy+uk02x4bsO7b9ZaV0UcZTiDc7gED66hBXfQBh8YCHiGV3hzlPPivDsfy9aSU8ycwh84nz/vwo4M</latexit>

ht <latexit sha1_base64="3p2Sf4EV1fRYc738z6OONc502Do=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5L0oicpePFYwbRCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl332yltbG5t75R3K3v7B4dH1eOTjkkyzbjPEpnox5AaLoXiPgqU/DHVnMah5N1wcjv3u09cG5GoB5ymPIjpSIlIMIpW8uvjAdYH1ZrbcBcg68QrSA0KtAfVr/4wYVnMFTJJjel5bopBTjUKJvms0s8MTymb0BHvWapozE2QL46dkQurDEmUaFsKyUL9PZHT2JhpHNrOmOLYrHpz8T+vl2F0HeRCpRlyxZaLokwSTMj8czIUmjOUU0so08LeStiYasrQ5lOxIXirL6+TTrPhuQ3vvllr3RRxlOEMzuESPLiCFtxBG3xgIOAZXuHNUc6L8+58LFtLTjFzCn/gfP4AD9SOKQ==</latexit>

Figure 2: A basic building block of MGU RNNs

where Wr, Wh  Rdh×dx , Ur, Uh  Rdh×dh , V  Rdy×dh , and rt  Rdh . The notation denotes the Hadamard product (entrywise product) of vectors. Denote by Fg,t the class of mappings

from the first t inputs to the t-th output computed by gated (MGU or LSTM) RNNs. For simplicity,

we consider  being the sigmoid function, i.e., (x) = (1 + exp(-x))-1, h(·) = tanh(·), and

y being y-Lipschitz with y(0) = 0. Extensions to general Lipschitz activation operators as in

Assumption 3 are straightforward. Suppose we have h0 = 0 and the following assumption.

Assumption 6. All the weight matrices have bounded spectral norms respectively, i.e. Wr 2  BWr , Wh 2  BWh , Ur 2  BUr , Uh 2  BUh , and V 2  BV .

Using a similar argument for vanilla RNNs yields a generalization bound of MGU RNNs as follows.

Theorem 4. Let the activation operator y be given and Assumptions 1 and 6 hold. Then for (xt, zt)Tt=1 and S = (xi,t, zi,t)Tt=1, i = 1, . . . , m drawn i.i.d. from any underlying distribution over Rdx×T × {1, . . . , K}, with probability at least 1 -  over S, for every margin value  > 0 and
every ft  Fg,t for integer t  T , we have

P (zt = zt)  R (ft) + O

dyBV min

 d,BWh

Bx

 t -1 -1



m

log

t -1 -1

dm

+

log

1 

m

,

where d = 2dxdh + 2d2h + dhdy,  = maxjt 2BUr + BUr BUh .

1 - rj

 + BUh

rj

2 

, and 

= +

The detailed proof is provided in Appendix C.1. As can be seen, rt shrinks the magnitude of hidden
state to reduce the dependence on d and t in generalization. Specifically, the hidden state ht is entrywise bounded by 1. Then for any integer t  T , we have

rt





1

+

1 exp (-BWr Bx

-

Ur

1)

and

1 - rt





1

+

1 exp (-BWr Bx

-

Ur

1) ,

where Ur 1 denotes the maximal absolute row sum. By restricting BWr and Ur 1 sufficiently small, we can guarantee that  and  are strictly smaller than 1 when BUh = 1 or even BUh > 1. As a result, with proper normalization of weight matrices, the generalization bound of MGU RNNs
is less dependent on d and t than that of vanilla RNNs.

The LSTM RNNs are more complicated than MGU RNNs, which introduce more gates to control

the information flow in RNNs. LSTM RNNs

have two hidden states, and compute them as,

gt rt

= =

(Wg

xht xt+t 1<latexitsha1_base64="v4ZZ+3n27QM0FDeMDBfxAnlvQnY=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYiLYGO7SaCUBG8sI5kOSI+xt9pIlu3fH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzgkQKg6777aytb2xubRd2irt7+weHpaPjlolTzXiTxTLWnYAaLkXEmyhQ8k6iOVWB5O1gfDvz209cGxFHDzhJuK/oMBKhYBSt9FgZ9TO89KaVfqnsVt05yCrxclKGHI1+6as3iFmqeIRMUmO6npugn1GNgkk+LfZSwxPKxnTIu5ZGVHHjZ/ODp+TcKgMSxtpWhGSu/p7IqDJmogLbqSiOzLI3E//zuimG134moiRFHrHFojCVBGMy+54MhOYM5cQSyrSwtxI2opoytBkVbQje8surpFWrem7Vu6+V6zd5HAU4hTO4AA+uoA530IAmMFDwDK/w5mjnxXl3Phata04+cwJ/4Hz+ALLbj6c=</latexit> <latexit sha1_base64="wC5NfZJuEuzbwi8IACSSOv3lOAc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsarQyJjSUmHpDAhewte7Bhb++yO2ckhN9gY6Extv4gO/+NC1yh4EsmeXlvJjPzwlQKg6777RQ2Nre2d4q7pb39g8Oj8vFJyySZZtxniUx0J6SGS6G4jwIl76Sa0ziUvB2Ob+d++5FrIxL1gJOUBzEdKhEJRtFKfvWpj9V+ueLW3AXIOvFyUoEczX75qzdIWBZzhUxSY7qem2IwpRoFk3xW6mWGp5SN6ZB3LVU05iaYLo6dkQurDEiUaFsKyUL9PTGlsTGTOLSdMcWRWfXm4n9eN8PoOpgKlWbIFVsuijJJMCHzz8lAaM5QTiyhTAt7K2EjqilDm0/JhuCtvrxOWvWa59a8+3qlcZPHUYQzOIdL8OAKGnAHTfCBgYBneIU3RzkvzrvzsWwtOPnMKfyB8/kDKESOOQ==</latexit>

(Wrxt +

U <latexit sha1_base64="VLV1WfLffWbJvU05bja4sMBfihk=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmrbQhrLZbtqlm03YnQil9Cd48aCIV3+RN/+N2zYHbX0w8Hhvhpl5YSqFQdf9dgobm1vbO8Xd0t7+weFR+fikZZJMM+6zRCa6E1LDpVDcR4GSd1LNaRxK3g7Ht3O//cS1EYl6xEnKg5gOlYgEo2ilh6pf7Zcrbs1dgKwTLycVyNHsl796g4RlMVfIJDWm67kpBlOqUTDJZ6VeZnhK2ZgOeddSRWNuguni1Bm5sMqARIm2pZAs1N8TUxobM4lD2xlTHJlVby7+53UzjG6CqVBphlyx5aIokwQTMv+bDITmDOXEEsq0sLcSNqKaMrTplGwI3urL66RVr3luzbuvVxpXeRxFOINzuAQPrqEBd9AEHxgM4Rle4c2Rzovz7nwsWwtOPnMKf+B8/gBj7Y0l</latexit>

Ug

ht-1h<latexitsha1_base64="69P0MQb1+ipK1vhUuUqNxbRCjpA=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBhPBU9jNRU8S8OIxgnlgsoTZyWwyZB7LzKwQlvyFFw+KePVvvPk3TpI9aGJBQ1HVTXdXlHBmrO9/e4WNza3tneJuaW//4PCofHzSNirVhLaI4kp3I2woZ5K2LLOcdhNNsYg47UST27nfeaLaMCUf7DShocAjyWJGsHXSY7Vv2Ejgwbg6KFf8mr8AWidBTiqQozkof/WHiqSCSks4NqYX+IkNM6wtI5zOSv3U0ASTCR7RnqMSC2rCbHHxDF04ZYhipV1Jixbq74kMC2OmInKdAtuxWfXm4n9eL7XxdZgxmaSWSrJcFKccWYXm76Mh05RYPnUEE83crYiMscbEupBKLoRg9eV10q7XAr8W3NcrjZs8jiKcwTlcQgBX0IA7aEILCEh4hld484z34r17H8vWgpfPnMIfeJ8/0tqQVA==</latexit>

),

W

<latexit sha1_base64="2vPE/2toIup2Z/Lenq7o8M88OA0=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD9VOdVCuuDV3AbJOvJxUIEdzUP7qD2OWRlwhk9SYnucm6GdUo2CSz0r91PCEsgkd8Z6likbc+Nni1Bm5sMqQhLG2pZAs1N8TGY2MmUaB7Ywojs2qNxf/83ophjd+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmnXa55b8+7rlcZVHkcRzuAcLsGDa2jAHTShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwBm940n</latexit>
Ur

ht-1),

ht <latexit sha1_base64="3p2Sf4EV1fRYc738z6OONc502Do=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5L0oicpePFYwbRCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl332yltbG5t75R3K3v7B4dH1eOTjkkyzbjPEpnox5AaLoXiPgqU/DHVnMah5N1wcjv3u09cG5GoB5ymPIjpSIlIMIpW8uvjAdYH1ZrbcBcg68QrSA0KtAfVr/4wYVnMFTJJjel5bopBTjUKJvms0s8MTymb0BHvWapozE2QL46dkQurDEmUaFsKyUL9PZHT2JhpHNrOmOLYrHpz8T+vl2F0HeRCpRlyxZaLokwSTMj8czIUmjOUU0so08LeStiYasrQ5lOxIXirL6+TTrPhuQ3vvllr3RRxlOEMzuESPLiCFtxBG3xgIOAZXuHNUc6L8+58LFtLTjFzCn/gfP4AD9SOKQ==</latexit>

ot = (Woxt + Uoht-1),

ct = c (Wcxt + Ucht-1) ,

ct <latexit sha1_base64="qx063OU/We40do9XK5orrzTupSw=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYSLYGO7SaCUBG8sI5kOSI+xt9pIlu3vH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzwkRwg5737aytb2xubRd2irt7+weHpaPjlolTTVmTxiLWnZAYJrhiTeQoWCfRjMhQsHY4vp357SemDY/VA04SFkgyVDzilKCVHiu0n+GlP630S2Wv6s3hrhI/J2XI0eiXvnqDmKaSKaSCGNP1vQSDjGjkVLBpsZcalhA6JkPWtVQRyUyQzQ+euudWGbhRrG0pdOfq74mMSGMmMrSdkuDILHsz8T+vm2J0HWRcJSkyRReLolS4GLuz790B14yimFhCqOb2VpeOiCYUbUZFG4K//PIqadWqvlf172vl+k0eRwFO4QwuwIcrqMMdNKAJFCQ8wyu8Odp5cd6dj0XrmpPPnMAfOJ8/qySPog==</latexit>

1

ht <latexit sha1_base64="v4ZZ+3n27QM0FDeMDBfxAnlvQnY=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYiLYGO7SaCUBG8sI5kOSI+xt9pIlu3fH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzgkQKg6777aytb2xubRd2irt7+weHpaPjlolTzXiTxTLWnYAaLkXEmyhQ8k6iOVWB5O1gfDvz209cGxFHDzhJuK/oMBKhYBSt9FgZ9TO89KaVfqnsVt05yCrxclKGHI1+6as3iFmqeIRMUmO6npugn1GNgkk+LfZSwxPKxnTIu5ZGVHHjZ/ODp+TcKgMSxtpWhGSu/p7IqDJmogLbqSiOzLI3E//zuimG134moiRFHrHFojCVBGMy+54MhOYM5cQSyrSwtxI2opoytBkVbQje8surpFWrem7Vu6+V6zd5HAU4hTO4AA+uoA530IAmMFDwDK/w5mjnxXl3Phata04+cwJ/4Hz+ALLbj6c=</latexit>

1

xt
<latexit sha1_base64="wC5NfZJuEuzbwi8IACSSOv3lOAc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsarQyJjSUmHpDAhewte7Bhb++yO2ckhN9gY6Extv4gO/+NC1yh4EsmeXlvJjPzwlQKg6777RQ2Nre2d4q7pb39g8Oj8vFJyySZZtxniUx0J6SGS6G4jwIl76Sa0ziUvB2Ob+d++5FrIxL1gJOUBzEdKhEJRtFKfvWpj9V+ueLW3AXIOvFyUoEczX75qzdIWBZzhUxSY7qem2IwpRoFk3xW6mWGp5SN6ZB3LVU05iaYLo6dkQurDEiUaFsKyUL9PTGlsTGTOLSdMcWRWfXm4n9eN8PoOpgKlWbIFVsuijJJMCHzz8lAaM5QTiyhTAt7K2EjqilDm0/JhuCtvrxOWvWa59a8+3qlcZPHUYQzOIdL8OAKGnAHTfCBgYBneIU3RzkvzrvzsWwtOPnMKfyB8/kDKESOOQ==</latexit>

+

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

<latexit sha1_base64="RgQFrgPP09G9wiNkCBnzA2EsptM=">AAAB6nicbVBNS8NAEJ34WetX1aOXxVYQhJL0oseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/nbX1jc2t7cJOcXdv/+CwdHTcMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb2d++4lrI2L1iJOE+xEdKhEKRtFKD5XLSr9UdqvuHGSVeDkpQ45Gv/TVG8QsjbhCJqkxXc9N0M+oRsEknxZ7qeEJZWM65F1LFY248bP5qVNybpUBCWNtSyGZq78nMhoZM4kC2xlRHJllbyb+53VTDK/9TKgkRa7YYlGYSoIxmf1NBkJzhnJiCWVa2FsJG1FNGdp0ijYEb/nlVdKqVT236t3XyvWbPI4CnMIZXIAHV1CHO2hAExgM4Rle4c2Rzovz7nwsWtecfOYE/sD5/AEoUY0J</latexit>

gt <latexit sha1_base64="ofJKBrf7yurm8y6Rs4AdIQ4IycY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glvz4eYn1YrbkNdwmySbyC1KBAZ1j9GowSlsVcIZPUmL7nphjkVKNgks8rg8zwlLIpHfO+pYrG3AT58tg5ubLKiESJtqWQLNXfEzmNjZnFoe2MKU7MurcQ//P6GUatIBcqzZArtloUZZJgQhafk5HQnKGcWUKZFvZWwiZUU4Y2n4oNwVt/eZN0mw3PbXj3zVq7VcRRhgu4hGvw4AbacAcd8IGBgGd4hTdHOS/Ou/Oxai05xcw5/IHz+QMMf44i</latexit>
<latexit sha1_base64="+wPnq5mfo5j9NMc0te1uWUaQEUA=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4Irtc9Ej04hETeSSwIbPDLEyYxzoza0I2/IQXDxrj1d/x5t84wB4UrKSTSlV3uruihDNjff/bK2xsbm3vFHdLe/sHh0fl45O2UakmtEUUV7obYUM5k7RlmeW0m2iKRcRpJ5rczv3OE9WGKflgpwkNBR5JFjOCrZO61b5hI4Grg3LFr/kLoHUS5KQCOZqD8ld/qEgqqLSEY2N6gZ/YMMPaMsLprNRPDU0wmeAR7TkqsaAmzBb3ztCFU4YoVtqVtGih/p7IsDBmKiLXKbAdm1VvLv7n9VIbX4cZk0lqqSTLRXHKkVVo/jwaMk2J5VNHMNHM3YrIGGtMrIuo5EIIVl9eJ+16LfBrwX290rjJ4yjCGZzDJQRwBQ24gya0gACHZ3iFN+/Re/HevY9la8HLZ07hD7zPH1Y1j30=</latexit>
Ug
<latexit sha1_base64="hIJlFHarrACsHKkWbPIAATdJL/4=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmCTTjPsskYnuhdRwKRT3UaDkvVRzGoeSd8Pp3cLvPnFtRKIecZbyIKZjJSLBKFrJr/vDcX1YrbkNdwmySbyC1KBAe1j9GowSlsVcIZPUmL7nphjkVKNgks8rg8zwlLIpHfO+pYrG3AT58tg5ubLKiESJtqWQLNXfEzmNjZnFoe2MKU7MurcQ//P6GUa3QS5UmiFXbLUoyiTBhCw+JyOhOUM5s4QyLeythE2opgxtPhUbgrf+8ibpNBue2/AemrVWs4ijDBdwCdfgwQ204B7a4AMDAc/wCm+Ocl6cd+dj1Vpyiplz+APn8wfbY439</latexit>

rt <latexit sha1_base64="oH0n51gWY1w44tvOQOSpqHm4/NY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glv66HWB9Wa27DXYJsEq8gNSjQGVa/BqOEZTFXyCQ1pu+5KQY51SiY5PPKIDM8pWxKx7xvqaIxN0G+PHZOrqwyIlGibSkkS/X3RE5jY2ZxaDtjihOz7i3E/7x+hlEryIVKM+SKrRZFmSSYkMXnZCQ0ZyhnllCmhb2VsAnVlKHNp2JD8NZf3iTdZsNzG959s9ZuFXGU4QIu4Ro8uIE23EEHfGAg4Ble4c1Rzovz7nysWktOMXMOf+B8/gAdTI4t</latexit> <latexit sha1_base64="+wPnq5mfo5j9NMc0te1uWUaQEUA=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4Irtc9Ej04hETeSSwIbPDLEyYxzoza0I2/IQXDxrj1d/x5t84wB4UrKSTSlV3uruihDNjff/bK2xsbm3vFHdLe/sHh0fl45O2UakmtEUUV7obYUM5k7RlmeW0m2iKRcRpJ5rczv3OE9WGKflgpwkNBR5JFjOCrZO61b5hI4Grg3LFr/kLoHUS5KQCOZqD8ld/qEgqqLSEY2N6gZ/YMMPaMsLprNRPDU0wmeAR7TkqsaAmzBb3ztCFU4YoVtqVtGih/p7IsDBmKiLXKbAdm1VvLv7n9VIbX4cZk0lqqSTLRXHKkVVo/jwaMk2J5VNHMNHM3YrIGGtMrIuo5EIIVl9eJ+16LfBrwX290rjJ4yjCGZzDJQRwBQ24gya0gACHZ3iFN+/Re/HevY9la8HLZ07hD7zPH1Y1j30=</latexit>

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

c~t <latexit sha1_base64="7IYKm8XwRp93GEETYH7gLw+FdAI=">AAAB9HicbVA9TwJBEJ3zE/ELtbTZCCZW5I5GShIbS0zkI4EL2dtbYMPe3rk7R0Iu/A4bC42x9cfY+W9c4AoFXzLJy3szmZkXJFIYdN1vZ2t7Z3dvv3BQPDw6PjktnZ23TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wSTu4XfmXJtRKwecZZwP6IjJYaCUbSSX+mjkCHP2HyAlUGp7FbdJcgm8XJShhzNQemrH8YsjbhCJqkxPc9N0M+oRsEknxf7qeEJZRM64j1LFY248bPl0XNybZWQDGNtSyFZqr8nMhoZM4sC2xlRHJt1byH+5/VSHNb9TKgkRa7YatEwlQRjskiAhEJzhnJmCWVa2FsJG1NNGdqcijYEb/3lTdKuVT236j3Uyo16HkcBLuEKbsCDW2jAPTShBQye4Ble4c2ZOi/Ou/Oxat1y8pkL+APn8wdu9ZHU</latexit> <latexit sha1_base64="dB+er7u/0aPiyGEI3sSMpHypazw=">AAAB8XicbVA9SwNBEJ3zM8avqKXNYiJYhbs0WgZsLCOYD0yOsLeZS5bs7h27e0II+Rc2ForY+m/s/Ddukis08cHA470ZZuZFqeDG+v63t7G5tb2zW9gr7h8cHh2XTk5bJsk0wyZLRKI7ETUouMKm5VZgJ9VIZSSwHY1v5377CbXhiXqwkxRDSYeKx5xR66THSs/woaR9VumXyn7VX4CskyAnZcjR6Je+eoOEZRKVZYIa0w381IZTqi1nAmfFXmYwpWxMh9h1VFGJJpwuLp6RS6cMSJxoV8qShfp7YkqlMRMZuU5J7cisenPxP6+b2fgmnHKVZhYVWy6KM0FsQubvkwHXyKyYOEKZ5u5WwkZUU2ZdSEUXQrD68jpp1aqBXw3ua+V6PY+jAOdwAVcQwDXU4Q4a0AQGCp7hFd484714797HsnXDy2fO4A+8zx/L25BR</latexit>

c

Ur <latexit sha1_base64="bUgvZyrQPSR6O7BkYzmRNV/Fvuc=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO0yhQ3bbbO7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemAqujet+O6Wt7Z3dvfJ+5eDw6PikenrW0UmmGPosEYnqhVSj4BJ9w43AXqqQxqHAbji9W/jdJ1SaJ/LRzFIMYjqWPOKMGiv5dX+o6sNqzW24S5BN4hWkBgXaw+rXYJSwLEZpmKBa9z03NUFOleFM4LwyyDSmlE3pGPuWShqjDvLlsXNyZZURiRJlSxqyVH9P5DTWehaHtjOmZqLXvYX4n9fPTHQb5FymmUHJVouiTBCTkMXnZMQVMiNmllCmuL2VsAlVlBmbT8WG4K2/vEk6zYbnNryHZq3VLOIowwVcwjV4cAMtuIc2+MCAwzO8wpsjnRfn3flYtZacYuYc/sD5/AHsGo4I</latexit>
Wg
<latexit sha1_base64="CXq0WSt6oXSKT6+bmuVa67gCis0=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmCTTjPsskYnuhdRwKRT3UaDkvVRzGoeSd8Pp3cLvPnFtRKIecZbyIKZjJSLBKFrJr3eH4/qwWnMb7hJkk3gFqUGB9rD6NRglLIu5QiapMX3PTTHIqUbBJJ9XBpnhKWVTOuZ9SxWNuQny5bFzcmWVEYkSbUshWaq/J3IaGzOLQ9sZU5yYdW8h/uf1M4xug1yoNEOu2GpRlEmCCVl8TkZCc4ZyZgllWthbCZtQTRnafCo2BG/95U3SaTY8t+E9NGutZhFHGS7gEq7BgxtowT20wQcGAp7hFd4c5bw4787HqrXkFDPn8AfO5w/ecY3/</latexit>

Uc <latexit sha1_base64="nGb0ziqbMdgWBw2LRepOfBGsyT0=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO0yhQ3bbbO7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemAqujet+O6Wt7Z3dvfJ+5eDw6PikenrW0UmmGPosEYnqhVSj4BJ9w43AXqqQxqHAbji9W/jdJ1SaJ/LRzFIMYjqWPOKMGiv5dX/I6sNqzW24S5BN4hWkBgXaw+rXYJSwLEZpmKBa9z03NUFOleFM4LwyyDSmlE3pGPuWShqjDvLlsXNyZZURiRJlSxqyVH9P5DTWehaHtjOmZqLXvYX4n9fPTHQb5FymmUHJVouiTBCTkMXnZMQVMiNmllCmuL2VsAlVlBmbT8WG4K2/vEk6zYbnNryHZq3VLOIowwVcwjV4cAMtuIc2+MCAwzO8wpsjnRfn3flYtZacYuYc/sD5/AHVT435</latexit>
Wr <latexit sha1_base64="vz/mx7uwdW27AGvZy2UCSZZe0Fc=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO0yhQ3bbbO7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemAqujet+O6Wt7Z3dvfJ+5eDw6PikenrW0UmmGPosEYnqhVSj4BJ9w43AXqqQxqHAbji9W/jdJ1SaJ/LRzFIMYjqWPOKMGiv59e5Q1YfVmttwlyCbxCtIDQq0h9WvwShhWYzSMEG17ntuaoKcKsOZwHllkGlMKZvSMfYtlTRGHeTLY+fkyiojEiXKljRkqf6eyGms9SwObWdMzUSvewvxP6+fmeg2yLlMM4OSrRZFmSAmIYvPyYgrZEbMLKFMcXsrYROqKDM2n4oNwVt/eZN0mg3PbXgPzVqrWcRRhgu4hGvw4AZacA9t8IEBh2d4hTdHOi/Ou/Oxai05xcw5/IHz+QPvKI4K</latexit>

Wc <latexit sha1_base64="ICxsCyzHFX1UVaYy4kFhbj2nPGQ=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFoueiTx4hETCyTQkO0yhQ3bbbO7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemAqujet+O6Wt7Z3dvfJ+5eDw6PikenrW0UmmGPosEYnqhVSj4BJ9w43AXqqQxqHAbji9W/jdJ1SaJ/LRzFIMYjqWPOKMGiv59e6Q1YfVmttwlyCbxCtIDQq0h9WvwShhWYzSMEG17ntuaoKcKsOZwHllkGlMKZvSMfYtlTRGHeTLY+fkyiojEiXKljRkqf6eyGms9SwObWdMzUSvewvxP6+fmeg2yLlMM4OSrRZFmSAmIYvPyYgrZEbMLKFMcXsrYROqKDM2n4oNwVt/eZN0mg3PbXgPzVqrWcRRhgu4hGvw4AZacA9t8IEBh2d4hTdHOi/Ou/Oxai05xcw5/IHz+QPYXY37</latexit>

tanh
<latexit sha1_base64="6gN6AKKwFjC/Dz27jWrOkvIQpPI=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LLaCp5KUgh4LXjxWsB/QhrLZbtqlm03YnQgl9Ed48aCIV3+PN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpb2zu7e8X90sHh0fFJ+fSsY+JUM95msYx1L6CGS6F4GwVK3ks0p1EgeTeY3i387hPXRsTqEWcJ9yM6ViIUjKKVutUBUjWpDssVt+YuQTaJl5MK5GgNy1+DUczSiCtkkhrT99wE/YxqFEzyeWmQGp5QNqVj3rdU0YgbP1ueOydXVhmRMNa2FJKl+nsio5ExsyiwnRHFiVn3FuJ/Xj/F8NbPhEpS5IqtFoWpJBiTxe9kJDRnKGeWUKaFvZWwCdWUoU2oZEPw1l/eJJ16zXNr3kO90mzkcRThAi7hGjy4gSbcQwvawGAKz/AKb07ivDjvzseqteDkM+fwB87nD4/Mjv8=</latexit>

<latexit sha1_base64="+wPnq5mfo5j9NMc0te1uWUaQEUA=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4Irtc9Ej04hETeSSwIbPDLEyYxzoza0I2/IQXDxrj1d/x5t84wB4UrKSTSlV3uruihDNjff/bK2xsbm3vFHdLe/sHh0fl45O2UakmtEUUV7obYUM5k7RlmeW0m2iKRcRpJ5rczv3OE9WGKflgpwkNBR5JFjOCrZO61b5hI4Grg3LFr/kLoHUS5KQCOZqD8ld/qEgqqLSEY2N6gZ/YMMPaMsLprNRPDU0wmeAR7TkqsaAmzBb3ztCFU4YoVtqVtGih/p7IsDBmKiLXKbAdm1VvLv7n9VIbX4cZk0lqqSTLRXHKkVVo/jwaMk2J5VNHMNHM3YrIGGtMrIuo5EIIVl9eJ+16LfBrwX290rjJ4yjCGZzDJQRwBQ24gya0gACHZ3iFN+/Re/HevY9la8HLZ07hD7zPH1Y1j30=</latexit>
Uo <latexit sha1_base64="Kb0zcT93dOpcc1myT8ZFtY8k+Jc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLUlsLDHxgAQuZG/Zgw37cdndMyEXfoONhcbY+oPs/DcucIWCL5nk5b2ZzMyLU86M9f1vr7S1vbO7V96vHBweHZ9UT886RmWa0JAornQvxoZyJmlomeW0l2qKRcxpN57eLfzuE9WGKfloZymNBB5LljCCrZPCejhU9WG15jf8JdAmCQpSgwLtYfVrMFIkE1RawrEx/cBPbZRjbRnhdF4ZZIammEzxmPYdlVhQE+XLY+foyikjlCjtSlq0VH9P5FgYMxOx6xTYTsy6txD/8/qZTW6jnMk0s1SS1aIk48gqtPgcjZimxPKZI5ho5m5FZII1JtblU3EhBOsvb5JOsxH4jeChWWs1izjKcAGXcA0B3EAL7qENIRBg8Ayv8OZJ78V79z5WrSWvmDmHP/A+fwDni44F</latexit>

ot <latexit sha1_base64="mpFsBHdFvf28lLVlN5O4kpHBGFw=">AAAB7HicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XOZJ48YiJBRNoyHbZwobtbrM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZelApu0PO+ndLW9s7uXnm/cnB4dHxSPT3rGpVpygKqhNKPETFMcMkC5CjYY6oZSSLBetH0duH3npg2XMkHnKUsTMhY8phTglYK6mqI9WG15jW8JdxN4hekBgU6w+rXYKRoljCJVBBj+r6XYpgTjZwKNq8MMsNSQqdkzPqWSpIwE+bLY+fulVVGbqy0LYnuUv09kZPEmFkS2c6E4MSsewvxP6+fYdwKcy7TDJmkq0VxJlxU7uJzd8Q1oyhmlhCqub3VpROiCUWbT8WG4K+/vEm6zYbvNfz7Zq3dKuIowwVcwjX4cANtuIMOBECBwzO8wpsjnRfn3flYtZacYuYc/sD5/AEYt44q</latexit> <latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>
Wo <latexit sha1_base64="8dlnrEaEbzsGhOr739+jqrRqLDQ=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLUlsLDHxgAQuZG/Zgw37cdndMyEXfoONhcbY+oPs/DcucIWCL5nk5b2ZzMyLU86M9f1vr7S1vbO7V96vHBweHZ9UT886RmWa0JAornQvxoZyJmlomeW0l2qKRcxpN57eLfzuE9WGKfloZymNBB5LljCCrZPCeneo6sNqzW/4S6BNEhSkBgXaw+rXYKRIJqi0hGNj+oGf2ijH2jLC6bwyyAxNMZniMe07KrGgJsqXx87RlVNGKFHalbRoqf6eyLEwZiZi1ymwnZh1byH+5/Uzm9xGOZNpZqkkq0VJxpFVaPE5GjFNieUzRzDRzN2KyARrTKzLp+JCCNZf3iSdZiPwG8FDs9ZqFnGU4QIu4RoCuIEW3EMbQiDA4Ble4c2T3ov37n2sWkteMXMOf+B9/gDqmY4H</latexit>

ct <latexit sha1_base64="pirW76MgnYII8nutbf2Q83ixmBI=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5L0oicpePFYwbRCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl332yltbG5t75R3K3v7B4dH1eOTjkkyzbjPEpnox5AaLoXiPgqU/DHVnMah5N1wcjv3u09cG5GoB5ymPIjpSIlIMIpW8utsgPVBteY23AXIOvEKUoMC7UH1qz9MWBZzhUxSY3qem2KQU42CST6r9DPDU8omdMR7lioacxPki2Nn5MIqQxIl2pZCslB/T+Q0NmYah7Yzpjg2q95c/M/rZRhdB7lQaYZcseWiKJMEEzL/nAyF5gzl1BLKtLC3EjammjK0+VRsCN7qy+uk02x4bsO7b9ZaN0UcZTiDc7gED66gBXfQBh8YCHiGV3hzlPPivDsfy9aSU8ycwh84nz8IMY4k</latexit>
ht <latexit sha1_base64="3p2Sf4EV1fRYc738z6OONc502Do=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5L0oicpePFYwbRCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl332yltbG5t75R3K3v7B4dH1eOTjkkyzbjPEpnox5AaLoXiPgqU/DHVnMah5N1wcjv3u09cG5GoB5ymPIjpSIlIMIpW8uvjAdYH1ZrbcBcg68QrSA0KtAfVr/4wYVnMFTJJjel5bopBTjUKJvms0s8MTymb0BHvWapozE2QL46dkQurDEmUaFsKyUL9PZHT2JhpHNrOmOLYrHpz8T+vl2F0HeRCpRlyxZaLokwSTMj8czIUmjOUU0so08LeStiYasrQ5lOxIXirL6+TTrPhuQ3vvllr3RRxlOEMzuESPLiCFtxBG3xgIOAZXuHNUc6L8+58LFtLTjFzCn/gfP4AD9SOKQ==</latexit>

ct = gt ct-1 + rt ct, ht = ot tanh(ct),

Figure 3: A basic building block of LSTM RNNs

where Wg, Wr, Wo, Wc  plicity, we also consider 

Rdh×dx , Ug, Ur, Uo, Uc  Rdh×dh , and gt, rt, ot  Rdh . For sim-

being the sigmoid function, and c(·)

rt <latexit sha1_base64="oH0n51gWY1w44tvOQOSpqHm4/NY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glv66HWB9Wa27DXYJsEq8gNSjQGVa/BqOEZTFXyCQ1pu+5KQY51SiY5PPKIDM8pWxKx7xvqaIxN0G+PHZOrqwyIlGibSkkS/X3RE5jY2ZxaDtjihOz7i3E/7x+hlEryIVKM+SKrRZFmSSYkMXnZCQ0ZyhnllCmhb2VsAnVlKHNp2JD8NZf3iTdZsNzG959s9ZuFXGU4QIu4Ro8uIE23EEHfGAg4Ble4c1Rzovz7nysWktOMXMOf+B8/gAdTI4t</latexit>

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

7

=
Uh <latexit sha1_base64="WwGo1Rp5Q2OaeI9VfuNwpkrOtzo=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmFZoQ9lsJ+3SzSbsboQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MBVcG9f9dkobm1vbO+Xdyt7+weFR9fiko5NMMfRZIhL1GFKNgkv0DTcCH1OFNA4FdsPJ7dzvPqHSPJEPZppiENOR5BFn1FjJr/uDcX1QrbkNdwGyTryC1KBAe1D96g8TlsUoDRNU657npibIqTKcCZxV+pnGlLIJHWHPUklj1EG+OHZGLqwyJFGibElDFurviZzGWk/j0HbG1Iz1qjcX//N6mYlugpzLNDMo2XJRlAliEjL/nAy5QmbE1BLKFLe3EjamijJj86nYELzVl9dJp9nw3IZ336y1roo4ynAG53AJHlxDC+6gDT4w4PAMr/DmSOfFeXc+lq0lp5g5hT9wPn8A3YKOAA==</latexit>

tanh~ht (·).

h

<latexit sha1_base64="S3CdFfDx9HsepZJgWjBttnLljms=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4r2FZoQ9lsNu3SzSbuTgol9Hd48aCIV3+MN/+N24+Dtj4YeLw3w8y8IJXCoOt+O4WNza3tneJuaW//4PCofHzSNkmmGW+xRCb6MaCGS6F4CwVK/phqTuNA8k4wup35nTHXRiTqAScp92M6UCISjKKV/GoPhQx5Ppz2sdovV9yaOwdZJ96SVGCJZr/81QsTlsVcIZPUmK7npujnVKNgkk9LvczwlLIRHfCupYrG3Pj5/OgpubBKSKJE21JI5urviZzGxkziwHbGFIdm1ZuJ/3ndDKMbPxcqzZArtlgUZZJgQmYJkFBozlBOLKFMC3srYUOqKUObU8mG4K2+vE7a9Zrn1rz7eqVxtYyjCGdwDpfgwTU04A6a0AIGT/AMr/DmjJ0X5935WLQWnOXMKfyB8/kDdWmR1Q==</latexit>

<latexit sha1_base64="D0ousYKRBvlWYFmcQmQNhtQFSy0=">AAAB8XicbVA9SwNBEJ3zM8avqKXNYiJYhbs0WgZsLCOYD0yOsLeZS5bs7h27e0II+Rc2ForY+m/s/Ddukis08cHA470ZZuZFqeDG+v63t7G5tb2zW9gr7h8cHh2XTk5bJsk0wyZLRKI7ETUouMKm5VZgJ9VIZSSwHY1v5377CbXhiXqwkxRDSYeKx5xR66THSs/woaT9UaVfKvtVfwGyToKclCFHo1/66g0SlklUlglqTDfwUxtOqbacCZwVe5nBlLIxHWLXUUUlmnC6uHhGLp0yIHGiXSlLFurviSmVxkxk5DoltSOz6s3F/7xuZuObcMpVmllUbLkozgSxCZm/TwZcI7Ni4ghlmrtbCRtRTZl1IRVdCMHqy+ukVasGfjW4r5Xr9TyOApzDBVxBANdQhztoQBMYKHiGV3jzjPfivXsfy9YNL585gz/wPn8A03SQVg==</latexit>

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

Wh <latexit sha1_base64="3UtSFnvG9s5LzXvAasw77bxdiww=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmFZoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSMUmmGfdZIhP9GFLDpVDcR4GSP6aa0ziUvBtObud+94lrIxL1gNOUBzEdKREJRtFKfr07GNcH1ZrbcBcg68QrSA0KtAfVr/4wYVnMFTJJjel5bopBTjUKJvms0s8MTymb0BHvWapozE2QL46dkQurDEmUaFsKyUL9PZHT2JhpHNrOmOLYrHpz8T+vl2F0E+RCpRlyxZaLokwSTMj8czIUmjOUU0so08LeStiYasrQ5lOxIXirL6+TTrPhuQ3vvllrXRVxlOEMzuESPLiGFtxBG3xgIOAZXuHNUc6L8+58LFtLTjFzCn/gfP4A4JCOAg==</latexit>

rt <latexit sha1_base64="oH0n51gWY1w44tvOQOSpqHm4/NY=">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRjDxRFouciTx4hETCybQkO2yhQ3bbbM7NSENv8GLB43x6g/y5r9xgR4UfMkkL+/NZGZemEph0HW/ndLW9s7uXnm/cnB4dHxSPT3rmiTTjPsskYl+DKnhUijuo0DJH1PNaRxK3guntwu/98S1EYl6wFnKg5iOlYgEo2glv66HWB9Wa27DXYJsEq8gNSjQGVa/BqOEZTFXyCQ1pu+5KQY51SiY5PPKIDM8pWxKx7xvqaIxN0G+PHZOrqwyIlGibSkkS/X3RE5jY2ZxaDtjihOz7i3E/7x+hlEryIVKM+SKrRZFmSSYkMXnZCQ0ZyhnllCmhb2VsAnVlKHNp2JD8NZf3iTdZsNzG959s9ZuFXGU4QIu4Ro8uIE23EEHfGAg4Ble4c1Rzovz7nysWktOMXMOf+B8/gAdTI4t</latexit>

The

1 rt<latexitsha1_base64="+wPnq5mfo5j9NMc0te1uWUaQEUA=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4Irtc9Ej04hETeSSwIbPDLEyYxzoza0I2/IQXDxrj1d/x5t84wB4UrKSTSlV3uruihDNjff/bK2xsbm3vFHdLe/sHh0fl45O2UakmtEUUV7obYUM5k7RlmeW0m2iKRcRpJ5rczv3OE9WGKflgpwkNBR5JFjOCrZO61b5hI4Grg3LFr/kLoHUS5KQCOZqD8ld/qEgqqLSEY2N6gZ/YMMPaMsLprNRPDU0wmeAR7TkqsaAmzBb3ztCFU4YoVtqVtGih/p7IsDBmKiLXKbAdm1VvLv7n9VIbX4cZk0lqqSTLRXHKkVVo/jwaMk2J5VNHMNHM3YrIGGtMrIuo5EIIVl9eJ+16LfBrwX290rjJ4yjCGZzDJQRwBQ24gya0gACHZ3iFN+/Re/HevY9la8HLZ07hD7zPH1Y1j30=</latexit>

<latexit sha1_base64="AWIQUmfbHSBXDwD7ImzIoMzPydA=">AAAB7nicbVBNT8JAEJ3iF+IX6tFLI5h4Ii0XPRK9eMREwAQast1uYcN2t9mdmpCGH+HFg8Z49fd489+4QA8KvmSSl/dmMjMvTAU36HnfTmljc2t7p7xb2ds/ODyqHp90jco0ZR2qhNKPITFMcMk6yFGwx1QzkoSC9cLJ7dzvPTFtuJIPOE1ZkJCR5DGnBK3Uqw9UpLA+rNa8hreAu078gtSgQHtY/RpEimYJk0gFMabveykGOdHIqWCzyiAzLCV0Qkasb6kkCTNBvjh35l5YJXJjpW1JdBfq74mcJMZMk9B2JgTHZtWbi/95/Qzj6yDnMs2QSbpcFGfCReXOf3cjrhlFMbWEUM3trS4dE00o2oQqNgR/9eV10m02fK/h3zdrrZsijjKcwTlcgg9X0II7aEMHKEzgGV7hzUmdF+fd+Vi2lpxi5hT+wPn8AaSxjxg=</latexit>

<latexit sha1_base64="wQAvzFe7l0eLgjHF3NV0yfiWEZQ=">AAAB73icbVBNS8NAEJ34WetX1aOXxVbwYkmKoMeCF48V7Ae0oWy2m3bpZhN3J0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZekEhh0HW/nbX1jc2t7cJOcXdv/+CwdHTcMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb2d++4lrI2L1gJOE+xEdKhEKRtFKnYp3SXQfK/1S2a26c5BV4uWkDDka/dJXbxCzNOIKmaTGdD03QT+jGgWTfFrspYYnlI3pkHctVTTixs/m907JuVUGJIy1LYVkrv6eyGhkzCQKbGdEcWSWvZn4n9dNMbzxM6GSFLlii0VhKgnGZPY8GQjNGcqJJZRpYW8lbEQ1ZWgjKtoQvOWXV0mrVvXcqndfK9ev8jgKcApncAEeXEMd7qABTWAg4Rle4c15dF6cd+dj0brm5DMn8AfO5w9M3I7F</latexit>

t-th output +
<latexit sha1_base64="RgQFrgPP09G9wiNkCBnzA2EsptM=">AAAB6nicbVBNS8NAEJ34WetX1aOXxVYQhJL0oseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/nbX1jc2t7cJOcXdv/+CwdHTcMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb2d++4lrI2L1iJOE+xEdKhEKRtFKD5XLSr9UdqvuHGSVeDkpQ45Gv/TVG8QsjbhCJqkxXc9N0M+oRsEknxZ7qeEJZWM65F1LFY248bP5qVNybpUBCWNtSyGZq78nMhoZM4kC2xlRHJllbyb+53VTDK/9TKgkRa7YYlGYSoIxmf1NBkJzhnJiCWVa2FsJG1FNGdp0ijYEb/nlVdKqVT236t3XyvWbPI4CnMIZXIAHV1CHO2hAExgM4Rle4c2Rzovz7nwsWtecfOYE/sD5/AEoUY0J</latexit>

is

ht <latexit sha1_base64="v4ZZ+3n27QM0FDeMDBfxAnlvQnY=">AAAB8HicbVA9SwNBEJ3zM8avqKXNYiLYGO7SaCUBG8sI5kOSI+xt9pIlu3fH7pwQjvwKGwtFbP05dv4bN8kVmvhg4PHeDDPzgkQKg6777aytb2xubRd2irt7+weHpaPjlolTzXiTxTLWnYAaLkXEmyhQ8k6iOVWB5O1gfDvz209cGxFHDzhJuK/oMBKhYBSt9FgZ9TO89KaVfqnsVt05yCrxclKGHI1+6as3iFmqeIRMUmO6npugn1GNgkk+LfZSwxPKxnTIu5ZGVHHjZ/ODp+TcKgMSxtpWhGSu/p7IqDJmogLbqSiOzLI3E//zuimG134moiRFHrHFojCVBGMy+54MhOYM5cQSyrSwtxI2opoytBkVbQje8surpFWrem7Vu6+V6zd5HAU4hTO4AA+uoA530IAmMFDwDK/w5mjnxXl3Phata04+cwJ/4Hz+ALLbj6c=</latexit>

1

xt
<latexit sha1_base64="wC5NfZJuEuzbwi8IACSSOv3lOAc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsarQyJjSUmHpDAhewte7Bhb++yO2ckhN9gY6Extv4gO/+NC1yh4EsmeXlvJjPzwlQKg6777RQ2Nre2d4q7pb39g8Oj8vFJyySZZtxniUx0J6SGS6G4jwIl76Sa0ziUvB2Ob+d++5FrIxL1gJOUBzEdKhEJRtFKfvWpj9V+ueLW3AXIOvFyUoEczX75qzdIWBZzhUxSY7qem2IwpRoFk3xW6mWGp5SN6ZB3LVU05iaYLo6dkQurDEiUaFsKyUL9PTGlsTGTOLSdMcWRWfXm4n9eN8PoOpgKlWbIFVsuijJJMCHzz8lAaM5QTiyhTAt7K2EjqilDm0/JhuCtvrxOWvWa59a8+3qlcZPHUYQzOIdL8OAKGnAHTfCBgYBneIU3RzkvzrvzsWwtOPnMKfyB8/kDKESOOQ==</latexit>

Ur <latexit sha1_base64="BW2T5sAmwcKzhZ8g1qGxQ2wjKug=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmFZoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSMUmmGfdZIhP9GFLDpVDcR4GSP6aa0ziUvBtObud+94lrIxL1gNOUBzEdKREJRtFKft0f6PqgWnMb7gJknXgFqUGB9qD61R8mLIu5QiapMT3PTTHIqUbBJJ9V+pnhKWUTOuI9SxWNuQnyxbEzcmGVIYkSbUshWai/J3IaGzONQ9sZUxybVW8u/uf1MoxuglyoNEOu2HJRlEmCCZl/ToZCc4ZyagllWthbCRtTTRnafCo2BG/15XXSaTY8t+HdN2utqyKOMpzBOVyCB9fQgjtogw8MBDzDK7w5ynlx3p2PZWvJKWZO4Q+czx/stI4K</latexit>

Wr <latexit sha1_base64="YH7coM1v251MW/+4mqAFEDNLMyM=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FLx4rmFZoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSMUmmGfdZIhP9GFLDpVDcR4GSP6aa0ziUvBtObud+94lrIxL1gNOUBzEdKREJRtFKfr070PVBteY23AXIOvEKUoMC7UH1qz9MWBZzhUxSY3qem2KQU42CST6r9DPDU8omdMR7lioacxPki2Nn5MIqQxIl2pZCslB/T+Q0NmYah7Yzpjg2q95c/M/rZRjdBLlQaYZcseWiKJMEEzL/nAyF5gzl1BLKtLC3EjammjK0+VRsCN7qy+uk02x4bsO7b9ZaV0UcZTiDc7gED66hBXfQBh8YCHiGV3hzlPPivDsfy9aSU8ycwh84nz/vwo4M</latexit>

ht <latexit sha1_base64="3p2Sf4EV1fRYc738z6OONc502Do=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5L0oicpePFYwbRCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl332yltbG5t75R3K3v7B4dH1eOTjkkyzbjPEpnox5AaLoXiPgqU/DHVnMah5N1wcjv3u09cG5GoB5ymPIjpSIlIMIpW8uvjAdYH1ZrbcBcg68QrSA0KtAfVr/4wYVnMFTJJjel5bopBTjUKJvms0s8MTymb0BHvWapozE2QL46dkQurDEmUaFsKyUL9PZHT2JhpHNrOmOLYrHpz8T+vl2F0HeRCpRlyxZaLokwSTMj8czIUmjOUU0so08LeStiYasrQ5lOxIXirL6+TTrPhuQ3vvllr3RRxlOEMzuESPLiCFtxBG3xgIOAZXuHNUc6L8+58LFtLTjFzCn/gfP4AD9SOKQ==</latexit>

Under review as a conference paper at ICLR 2019

yt = y(V ht), where V  Rdy×dh , and y is y-Lipschitz with y(0) = 0. Suppose we have h0 = c0 = 0 and the following assumption.
Assumption 7. The spectral norms of weight matrices are bounded respectively, i.e. Wg 2  BWg , Wr 2  BWr , Wo 2  BWo , Wc 2  BWc , Ug 2  BUg , Ur 2  BUr , Uo 2  BUo , Uh 2  BUh , and V 2  BV .

For properly normalized weight matrices Wo and Uo, the generalization bound of LSTM RNNs is given in the following theorem.

Theorem 5. Let the activation operator y be given and Assumptions 1 and 7 hold. Then for (xt, zt)tT=1 and S = (xi,t, zi,t)Tt=1, i = 1, . . . , m drawn i.i.d. from any underlying distribution over Rdx×T × {1, . . . , K}, with probability at least 1 -  over S, for every margin value  > 0 and
every ft  Fg,t for integer t  T , we have

P (zt = zt)  R (ft) + O

dyBV min

 d,BWc

Bx

 t -1 -1



m

log

t -1 -1

dm

+

log

1 

m

,

where d = 4dxdh + 4dh2 + dhdy,  = maxjt gj  + BUc rj  oj  and  =  + BUg + BUr + BUo .
The detailed proof is provided in Appendix C.2. Similar to MGU RNNs, LSTM RNNs also introduce extra decaying factors to reduce the dependence on d and t in generalization. However, LSTM RNNs are more complicated, but more flexible than MGU RNNs, since three factors, rt, ot and gt are used to jointly control the spectrum of Uc. We further remark that LSTM RNNs also need the spectral norms of weight matrices, Wg, Wr, Wo, Ug, Ur, and Uo, to be properly controlled such that better generalization bounds can be obtained.
Remark 1. We also extend our analysis to convolutional RNNs (Conv RNNs, Pinheiro & Collobert (2014); Liang & Hu (2015); Xingjian et al. (2015)), and show that the convolutional filters in Conv RNNs can reduce the dependence on d through parameter sharing in generalization. Due to space limit, the detailed discussion is provided in Appendix D.

6 DISCUSSIONS AND OPEN PROBLEMS
(I) Tighter bounds: Our obtained generalization bounds depend on the spectral norms of weight matrices and the network size (the total number of parameters). Can we exploit other modeling structures to further reduce the dependence on the network size? Or can we find better choices of norms of weight matrices that yield better bounds?
(II) Margin value: Our generalization bounds depend on the margin value of the predictors. As can be seen, a larger margin value yields a better generalization bound. However, establishing a sharp characterization of the margin value is technically very challenging, because of its complicated dependence on the underlying data distribution and the training algorithm.
(III) Implicit bias of SGD: Numerous empirical evidences have already shown that RNNs trained by stochastic gradient descent (SGD) algorithms have superior generalization performance. There have been a few theoretical results showing that SGD tends to yield low complexity models, which can generalize (Neyshabur et al., 2014; 2015; Zhang et al., 2016; Soudry et al., 2017). Can we extend their argument to RNNs? For example, can SGD always yield weight matrices with well controlled spectra? As mentioned, this is crucial to the generalization of MGU and LSTM RNNs, since not well controlled spectra may even hurt generalization.
(IV) Adaptivity to the underlying distribution: The current PAC-Learning framework focuses on the worst case. Taking classification as an example, the theoretical analysis holds even when the input features and labels are completely independent. Therefore, this often yields very pessimistic results. For many real applications, however, data are not obtained adversarially. Some recent empirical evidences suggest that the generalization of neural networks seems very adaptive to the underlying distribution: Easier tasks lead to low complexity neural networks, while harder ones lead to highly complex neural networks. Unfortunately, none of the existing analysis can take the underlying distribution into consideration.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pp. 1120­1128, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241­6250, 2017.
Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng Sun. Doctor ai: Predicting clinical events via recurrent neural networks. In Machine Learning for Healthcare Conference, pp. 301­318, 2016a.
Edward Choi, Andy Schuetz, Walter F Stewart, and Jimeng Sun. Using recurrent neural network models for early detection of heart failure onset. Journal of the American Medical Informatics Association, 24(2):361­370, 2016b.
Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2625­2634, 2015.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. arXiv preprint arXiv:1712.06541, 2017.
Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711, 2012.
Alex Graves, Santiago Ferna´ndez, Faustino Gomez, and Ju¨rgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, pp. 369­376. ACM, 2006.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pp. 6645­6649. IEEE, 2013.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, and Bo Li. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. arXiv preprint arXiv:1709.06079, 2017.
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent network architectures. In International Conference on Machine Learning, pp. 2342­2350, 2015.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3128­3137, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Chao-Chee Ku and Kwang Y Lee. Diagonal recurrent neural networks for dynamic systems control. IEEE transactions on neural networks, 6(1):144­156, 1995.
9

Under review as a conference paper at ICLR 2019
Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
Ching-Hung Lee and Ching-Cheng Teng. Identification and control of dynamic systems using recurrent fuzzy neural networks. IEEE Transactions on fuzzy systems, 8(4):349­366, 2000.
Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, and Tuo Zhao. On tighter generalization bound for deep neural networks: Cnns, resnets, and beyond. arXiv preprint arXiv:1806.05159, 2018.
Ming Liang and Xiaolin Hu. Recurrent convolutional neural network for object recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3367­ 3375, 2015.
Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzel. Learning to diagnose with lstm recurrent neural networks. arXiv preprint arXiv:1511.03677, 2015.
Toma´s Mikolov, Martin Karafia´t, Luka´s Burget, Jan C ernocky`, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association, 2010.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2012.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2422­2430, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017.
Pedro HO Pinheiro and Ronan Collobert. Recurrent convolutional neural networks for scene labeling. In 31st International Conference on Machine Learning (ICML), number EPFL-CONF199822, 2014.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104­3112, 2014.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. arXiv preprint arXiv:1702.00071, 2017.
Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation. arXiv preprint arXiv:1703.01827, 2017.
SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In Advances in neural information processing systems, pp. 802­810, 2015.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pp. 2048­2057, 2015.
10

Under review as a conference paper at ICLR 2019 Sung Jin Yoo, Jin Bae Park, and Yoon Ho Choi. Adaptive dynamic surface control of flexible-joint
robots using self-recurrent wavelet neural networks. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 36(6):1342­1355, 2006. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. Jiong Zhang, Qi Lei, and Inderjit S Dhillon. Stabilizing gradients for deep neural networks via efficient svd parameterization. arXiv preprint arXiv:1803.09327, 2018. Guo-Bing Zhou, Jianxin Wu, Chen-Lin Zhang, and Zhi-Hua Zhou. Minimal gated unit for recurrent neural networks. International Journal of Automation and Computing, 13(3):226­234, 2016.
11

Under review as a conference paper at ICLR 2019

A PROOFS IN SECTION 2

A.1 LIPSCHITZ CONTINUITY OF M AND 

We show the Lipschitz continuity of the margin operator M and the loss function  in the following lemma.

Lemma 5. The margin operator M is 2-Lipschitz in its first argument with respect to vector Eu-

clidean norm, and



is

1 

-Lipschitz.

Proof. Let y, y and z be given, then

M(y, z) - M (y , z)

= yz - yz +

max
j=z

yj

-

max
j=z

yj



yz - yz

+

max
j=z

yj

-

yj

2 y-y   y-y 2.

For function

, it is a piecewise linear function.

Thus, it is straightforward to see that



is

1 

-

Lipschitz.

A.2 PROOF OF LEMMA 2

Proof. The Lemma is stated with matrix Frobenius norms. However, we can show a tighter bound only involving the spectral norms of weight matrices. Given weight matrices U, V, W and U , V , W , consider the t-th outputs yt and yt of vanilla RNNs,

yt - yt 2 = y(V ht) - y(V ht) 2

 y V ht - V ht + V ht - V ht 2

 y ( (V - V )ht 2 + V (ht - ht) 2)

 y ( ht 2 V - V 2 + BV ht - ht 2) .

(5)

We have to bound the norm of ht as in the following lemma. Lemma 6. Under Assumptions 1 to 3, for t  0, the norm of ht is bounded by

ht 2  min

 b d,

hBW

Bx

(hBU )t - hBU - 1

1

.

(6)

Proof. We prove by induction. Observe that for t  1, we have

ht 2 = h(W xt + U ht-1) 2  h W xt + U ht-1 2  h ( W xt 2 + U ht-1 2)  h (BW Bx + BU ht-1 2) .
Applying equation (7) recursively with h0 = 0, we arrive at,

(7)

ht

2



hBW Bx

t-1
(hBU )j
j=0

=

h

BW

Bx

(hBU )t - 1 hBU - 1

,

We min

alsohave b d, h

ht   b. Thus, combining

BW

Bx

(hBU )t-1 hBU -1

. Clearly, h0

with 2=0

the above upper bound, satisfies the upper bound.

we

get

ht 2 

When hBU = 1, the ratio is defined, by L'Hospital's rule, to be the limit,

lim
hBU 1

(hBU )t - 1 hBU - 1

=

t.

12

Under review as a conference paper at ICLR 2019

With Lemma 6 in hand, we plug the bound (6) into equation (5) and end up with

yt - yt

2



y h BW

Bx

(hBU )t - 1 hBU - 1

V -V

2 + yBV

ht - ht 2 .

(8)

The remaining task is to bound ht - ht 2 in terms of the spectral norms of the difference of weight matrices, W - W 2 and U - U 2. Lemma 7. Under Assumptions 1 to 3, for t  1, the difference of hidden states ht and ht satisfies

ht - ht 2  LW,t W - W 2 + LU,t U - U 2 ,

where

LW,t

=

h

Bx

(hBU )t-1 hBU -1

and

LU,t

=

h2

BW

Bx

t

(hBU )t2-1 (hBU )-1

.

Proof. Similar to the proof of Lemma 6, we use induction.

ht - ht 2 = h (W xt + U ht-1) - h W xt + U ht-1 2  h (W - W )xt + U ht-1 - U ht-1 2  h (W - W )xt 2 + U ht-1 - U ht-1 2  h Bx W - W 2 + U ht-1 - U ht-1 + U ht-1 - U ht-1 2  hBx W - W 2 + h ht-1 2 U - U 2 + BU ht-1 - ht-1 2 .
Repeat this derivation recursively, we have

ht - ht 2  hBx W - W 2 + h ht-1 2 U - U 2 + hBU ht-1 - ht-1 2  hBx (1 + hBU ) W - W 2 + h ( ht-1 2 + hBU ht-2 2) U - U + (hBU )2 ht-2 - ht-2 2  ......

2

t-1
 hBx (hBU )j W - W

j=0

+ (hBU )t h0 - h0 2



hBx

(hBU )t - 1 hBU - 1

W -W

t-1

2 + h

(hBU )t-1-j

j=0

t-1

2 + h

(hBU )t-1-j

j=0

hj 2 hj 2

U -U 2 U - U 2 . (9)

We now plug in the upper bound (6) to calculate the summation involving the Euclidean norms of the hidden state ht.

t-1 t-1

t-1

(hBU )t-1-j hj 2  (j + 1)(hBU )j hBW Bx  t (hBU )j hBW Bx

j=0

j=0

j=0



hBW

Bxt

(hBU )t - 1 hBU - 1

.

Plugging back into equation (9), we have as desired,

ht - ht

2



hBx

(hBU )t - 1 hBU - 1

W -W

2

+

h2 BW

Bxt

(hBU )t - 1 hBU - 1

U -U

2.

Combining equation (8) and Lemma 7, and W F  W 2, we immediately get Lemma 2. 13

Under review as a conference paper at ICLR 2019

A.3 PROOF OF LEMMA 3

Proof. Our goal is to construct a covering C(Ft, , dist(·, ·)), i.e., for any ft  Ft, there exists ft  Ft, for any input data (xt)Tt=1, satisfying
sup ft(Xt) - ft(Xt)  .
Xt 2
Note that f is determined by weight matrices U, V and W . By Lemma 2, we have

sup f (Xt) - ft(Xt)  LV,t V - V + LW,t W - W + LU,t U - U .

Xt

2F

FF

Then it is enough to construct three matrix coverings, C U, 3LU,t , · F , C V, 3LV,t , · F and

C W, 3LW,t , · F . Their Cartesian product gives us the covering C(Ft, , dist(·, ·)). The following

lemma gives an upper bound on the covering number of matrices with a bounded Frobenius norm.

Lemma 8. Let G = A  Rd1×d2 : A 2   be the set of matrices with bounded spectral norm

and > 0 be given. The covering number N (G, , · F) is upper bounded by

N (G, , · F) 



min 1+2

d1, d2 

d1 d2
.

Proof. For any matrix A  G, we define a mapping  : Rd1×d2  Rd1d2 , such that (A) =

[A:,1, A:,2, . . . , A:,h] , where A:,i denotes the i-th column of matrix A. Denote the vector space

induced by the mapping  by V(G) = {(A) : A  G}. Note that we have

A

2 F

=

h i=1

A:,iA:,i

=

(A)

2 2

and the mapping  is one-to-one and onto.

By definition, the square of Frobenius norm

equals the square of sum of singular values and the spectral norm is the largest singular value. Hence,

the equivalence of Frobenius norm and spectral norm is given by the following inequalities,

A 2  A F  min d1, d2 A 2.
Now, we see that if we construct a covering C(V(G), , · 2), then -1C(V(G), , · 2) = -1(v) : v  C(V(G), , · 2) is a covering of G at scale with respect to the matrix Frobe-
nius norm. Therefore, we get N (G, , · F)  N (V(G), , · 2). As a consequence, it is suffices to upper bound the covering number of V(G). In order to do so, we need another closely related concept, packing number.
Definition 2 (Packing). Let G be an arbitrary set and > 0 be given. We say P(G, , · ) is a packing of G at scale with respect to the norm · , if for any two elements A, B  P, we have
A-B > .
Denote by M(G, , · ) the maximal cardinality of P(G, , · ).

By the maximality, we can check that N (C, , · )  M(C, , · ). Indeed, let P(G, , · ) be a maximal packing. Suppose there exists A  G such that for any B  P(G, , · ), the inequality A - B > holds. Then we can add A to P(G, , · ), while still keeping it being a packing, which contradicts the maximality of P(G, , · ). Thus, we have N (G, , · )  M(G, , · ). Observe that V(G) is contained in an Euclidean ball B(0; R)  Rd1d2 of radius at most

R = max
AG

(A)

2  min

d1, d2 A 2  min

d1, d2 .

Additionally, the union of Euclidean balls B(v; /2)  Rd1d2 with radius /2 and center v  P(V(G), , ·2) isfurther contained in an Euclidean ball B(0; R ) of slightly enlarged radius R = min d1, d2  + /2. Those balls B(v; /2) are disjoint by the definition of packing, thus we have

N (V(C),

,

·

2)  P(V(C),

,

·

2)



vol(B(0, R )) vol(B(v; /2))

=

=

 1 + 2 min{ d1, d2}

d1 d2
,

R d1d2 /2

where vol(·) denotes the volume.

14

Under review as a conference paper at ICLR 2019

By Lemma 8, we can directly write out the upper bounds on the covering numbers of weight matri-

ces,

N

U, 3LU,t , · F



 1 + 6 dhBU LU,t

dh2
,

N

V, 3LV,t , · F



1 + 6 min{

 dy, dh}BV LV,t

dy dh
,

and

N

W, 3LW,t , · F



 1 + 6 min{ dx, dh}BW LW,t

dx dh
.

Then we immediately have,

N (Ft, , dist(·, ·))  N U, 3LU,t , · F × N V, 3LV,t , · F × N W, 3LW,t , · F



 1 + 6 dhBU LU,t

d2h

min{6 1+

 dy, dh}BV LV,t

dy dh

×

 1 + 6 min{ dx, dh}BW LW,t

dx dh
.

Substituting the coefficients LU,t, LV,t and LW,t from Lemma 2, we get

N (Ft, , dist(·, ·))



 1

+

 6 dyhBV

BW

Bx

(hBU )t-1 hBU -1

2d2 

 1

+

 6 dyh2 BU BV

BW

Bx

t

(hBU )t-1 hBU -1

d2 



 1

+

 6c dt

(hBU )t-1 hBU -1

3d2 

,

where c = yhBV BW Bx max {1, hBU }. For future usage, we also write down for small

such that

 6c dt

(hBU )t-1 hBU -1

> 1, the logarithm of covering number satisfies,

log N (Ft,

,

dist(·,

·))



3d2

log

 

 12c dt

(hBU )t-1 hBU -1

 

.

> 0,

A.4 PROOF OF LEMMA 4

Proof. Define FM,t = {(Xt, zt)  M(ft(Xt), zt) : ft  Ft}. By Lemma 5, we see that M is 2-Lipschitz in its first argument. In order to cover FM,t at scale , it suffices to cover Ft at scale 2 . This immediately gives us the covering number N (FM,t, , · )  N (Ft, /2, dist(·, ·)).

We then give the statement of Dudley's entropy integral.

Lemma 9. Let H be a real-valued function class taking values in [-r, r] for some constant r, and

assume that 0  H. Let S = (s1, . . . , sm) be given points, then

RS

(H)



inf
>0

4 m

+

12 m

2rm 

log N (H, , · )d

.

The proof can be found in Bartlett et al. (2017). FM,t takes values in [-r, r] with r = yBV ht 2

Taking H =  yBV min

FbMd,t,,whBe WcaBn xe(ashihlBByUUv)-te-1ri1fy

that and

15

Under review as a conference paper at ICLR 2019

0  FM. Thus, directly applying Lemma 9 yields the following bound,

RS (FM,t)



inf
>0



4m

+

12 m

2r 

m

log N (FM,t, , · )d

.

We bound the integral as follows,

2rm 

2rm
log N (FM,t, , · )d 


3d2 log

 24c dt

(hBU )t-1 hBU -1

d

  2r m

3d2 log

 24c dt

(hBU )t-1 hBU -1



.

Picking



=

1 m

is

enough

to

give

us

an

upper

bound

on

RS (FM,t),

RS (FM )



4 m

+

24 m

3d2r2 log

 24c dmt

(hBU )t - hBU - 1

1

.

Finally, by Talagrand's lemma (Mohri et al., 2012) and



being

1 

-Lipschitz,

we

have

RS (F,t)



1 

RS

(FM,t)



4 m

+

2m4

3d2r2 log

 24c dmt

(hBU )t - hBU - 1

1

.

B PROOF IN SECTION 4

B.1 PROOF OF THEOREM 3

Proof. Under additional Assumption 4, we only need to show that, with the additional matrix induced norm bound, we have a refined upper bound on the matrix covering number. The proof relies on the following lemma adapted from Bartlett et al. (2017) Lemma 3.2.

Lemma 10. Let G = A  Rd1×d2 : A 2,1   . We have the following matrix covering upper bound

log N (G, ,

·

2)



2
2

log(2d1d2).

The above Lemma is a direct consequence of Lemma 3.2 in Bartlett et al. (2017) with X being
identity, a = , b = 1, and m = d1, d = d2. We apply the same trick to split the overall covering accuracy into 3 parts, ,3LU,t ,3LV,t and ,3LW,t corresponding to U, V, W respectively. Then we derive a refined bound on the covering number of Ft:

9 log N (Ft, , dist(·, ·)) 

MU L2U,t + MV LV2 ,t + MW LW2 ,t
2

log(2d2),

(10)

where d = max {dx, dy, dh}. Substituting (10) into the Dudley integral as in the proof of Lemma 4 yields

RS (FM,t)



inf
>0

4 m

+

12 m

2rm 

log N (Ft, /2, · )d

.

We bound the integral as follows,

 2r m 

log N (Ft, /2, · )d 

 2r m
36

MU LU2 ,t + MV L2V,t + MW LW2 ,t

log(2d2)d


= 36

MU L2U,t + MV L2V,t + MW L2W,t



log(2d2)

log

2r 

m

.

16

Under review as a conference paper at ICLR 2019

Choosing



=

1 m

yields

RS (FM )



4 m

+

43m2

MU LU2 ,t + MV LV2 ,t + MW L2W,t

 log(2d2) log 2m d .

Finally, substituting the Lipschitz constant LU,t, LV,t, LW,t into the expression, we have

RS (F,t)



1 

RS

(FM,t)



4 m

+

432 m

MU L2U,t + MV L2V,t + MW LW2 ,t

log(2d2) log



O

 



max{MU

, MV , m

MW

}t

 t -1 -1

  log d log m d  .

 2m d

Combining with Lemma 1 completes the proof.

Under additional Assumption 5, our proof is based on the following result from Lemma 1 in

Neyshabur et al. (2017).

Lemma 11. Let f (x) : X  Rd be any predictor with parameter , and P be any distribution on the parameter that is independent of training data. Then, for any ,  > 0, with proba-

bility at least 1 -  over the training set of size m, for any  and any random perturbation  s.t.

P

maxxX

|f+

(x)

-

f

(x)|

<

 4



1 2

,

we

have

R0 (f) - R (f)  4

KL ( +  P) + log m-1

6m 

,

where KL ( +  P) is KL divergence of distributions  +  and P.

For convenience, we omit the superscript for sample index. Denote ht () and ht ( + ) as the hidden variables with parameters  and  +  respectively. Then we provide an upper bound of the
gap of hidden layers before and after the perturbation. Denote the parameters  = vec ({W, U, V })
and the perturbation  = vec ({W, U, V }).

For any t  {1, 2, . . . , T }, we have

ht ( + ) - ht () 2

(i)
 h (U + U ) ht-1 ( + ) + (W + W ) xt - U ht-1 () - W xt 2

(ii)
 hBU ht-1 ( + ) - ht-1 () 2 + hBU ht-1 ( + ) 2 + hBxBW
t t-1
 (hBU )t h0 ( + ) - h0 () 2 +  (hBU )i h(t-i) ( + ) 2 + hBxBW (hBU )i,
i=1 i=0
(11)

By Lemma 6, we have that for any t  T ,

ht () 2  min

bp,

hBxBW

(hBU )t - 1 hBU - 1

= ht .

(12)

Combining (11), (12), and h(0) = 0, we have

t t-1

h(t) ( + ) - h(t) () 2  ht (hBU )i + hBxBW (hBU )i

i=1 i=0





(ht hBU

+

hBxBW

)

(hBU )t - 1 hBU - 1

.

(13)

Denote yt () and yt ( + ) as the out with parameters  and  +  respectively. Then we have

(i)
y(t) ( + ) - y(t) () 2  y (1 + ) V ht ( + ) - V ht () 2

 yBV ht ( + ) - ht () 2 + yBV ht ( + ) 2

(ii)
 yBV

(ht hBU

+

hBxBW )

(hBU )t - 1 hBU - 1

+

 y BV

ht ,

(14)

17

Under review as a conference paper at ICLR 2019

where (i) is from Lipschitz continuity of y and (ii) is from (12) and (13).
Then choosing the prior distribution and the perturbation distribution as N 0, 2I , and from the concentration result for the spectral norm bounds, we have

PAN (0,2Id×d) [ A 2 > ]  2p exp

-2 2d2

.

This implies with probability at least 1/2, we have max {BU , BW , BV }   2d ln (12d).

Taking  =

/4y

(ht hBU

+

hBxBW )

(hBU )t-1 hBU -1

+

ht

2d ln (12d) and combining

with (14), with probability at least 1/2, we have

max
xXm

yt ( + ) - yt () 2



(ht hBU

+

hBxBW )

(hBU )t - 1 hBU - 1

+

ht

·

2d ln (12d)



 4

.

Finally, we calculate the KL divergence of P and  +  with respect to this choice of ,

KL ( + 

P) 



2 2

22

=O

2y 2

·

(ht hBU

+

hBxBW )

(hBU )t - 1 hBU - 1

+

ht

2
d ln (d) BU2 ,F + BW2 ,F + BV2,F

=O

y2 (ht hBU + hBxBW )2 (t - 1)2 p ln (p) BU2,F + BW2 ,F + BV2,F 2 ( - 1)2

.

We complete the proof by applying Lemma 11.

C PROOFS IN SECTION 5

C.1 PROOF OF THEOREM 4

Proof. We use the same argument from the analysis of vanilla RNNs to investigate the Lipschitz continuity of MGU RNNs. Consider ht and ht computed by different sets of weight matrices.

ht - ht 2 = (1 - rt) ht-1 + rt ht - (1 - rt) ht-1 - rt ht 2

 (rt - rt)

ht-1 2 + (1 - rt)

(ht-1 - ht-1) 2 + (rt - rt)

ht

+
2

rt

(ht - ht) 2



rt - rt 2

ht-1  +

1 - rt 

ht-1 - ht-1 2 +

rt - rt 2

ht

+


rt 

ht - ht 2

Expand the expression of ht. Note that rt is nonnegative, and rt   1. Then we have ht   1. Additionally tanh(·) is 1-Lipschitz. Thus we get

ht - ht


2

Uh(ht-1

rt) + Whxt - Uh(ht-1

rt) - Whxt 2

 Uh(ht-1 rt) - Uh(ht-1 rt) 2 + Bx Wh - Wh 2

 Uh - Uh 2 ht-1 rt 2 + BUh rt  ht-1 - ht-1 2 + BUh ht-1  rt - rt 2

+ Bx Wh - Wh 2

 ht 2 Uh - Uh 2 + BUh rt  ht-1 - ht-1 2 + BUh rt - rt 2 + Bx Wh - Wh 2 .

We have to expand rt - rt as follows,

rt - rt 2 = Wrxt + Urht-1 - Wrxt - Urht-1 2  Bx Wr - Wr 2 + BUr ht-1 - ht-1 2 + ht-1 2 Ur - Ur 2.

18

Under review as a conference paper at ICLR 2019

We also need to bound ht 2,

ht 2   =

1 - rt  ht-1 2 + rt  ht 2

1 - rt  ht-1 2 + rt  (BWh Bx + BUh rt 

1 - rt  + BUh

rt

2 

ht-1 2 + BWh Bx,

ht-1 2)

 max
jt

1 - rj  + BUh

rj

2 

ht-1 2 + BWh Bx.

Applying min d,

the above inequality

 t -1 -1

BWh

Bx

with 

recursively = maxjt

and 1

remember - rj  +

ht BUh


rj


2 

1, we get ht 2  . Put all the above in-

gredients together, we have

ht - ht

2



( + 

2BUr

+

BUr BUh )

ht-1 - ht-1

2

+ d Uh - Uh 2 + Bx Wh - Wh 2

+ (2 + BUh ) d Ur - Ur 2 + (2Bx + BUh Bx) Wr - Wr 2 .

Apply the above inequality recursively, denote by  =  + 2BUr + BUr BUh , we have

t

t

ht - ht 2  d j Uh - Uh 2 + Bx j Wh - Wh 2

j=1

j=1

 t

t

+ 2 d + BUh d

j Ur - Ur 2 + (2Bx + BUh Bx) j Wr - Wr 2 .

j=1

j=1

We then derive the Lipschitz continuity of yt 2,



yt - yt 2  yBV ht - ht 2 + y d V - V 2



y BV

 d

t 

-1 -1

Uh - Uh

2

+

y BV

Bx

t 

-1 -1

 Wh - Wh 2 + y d V - V

+ yBV

 2 d + BUh d

t - 1 -1

Ur - Ur

2

+

y BV

(2Bx

+

BUh Bx)

t - 1 -1

2
Wr - Wr 2 .

Following the same argument for proving the generalization bound of vanilla RNNs, we can get the generalization bound for MGU RNNs as



dyBV min

P

(zt

=

zt)



R (ft)

+

O

 



 d,

BWh

Bx

 t -1 -1



m

log

 dm

t -1 -1

+



log m

1 

  

.

C.2 PROOF OF THEOREM 5

Proof. We first bound the norm of ht as follows,

ht 2  ot  tanh(ct) 2  ot  ct 2  gt  ct-1 2 + rt  ct 2  gt  ct-1 2 + rt  (BWc Bx + BUc ht-1 2)  gt  ct-1 2 + rt  (BWc Bx + BUc ot  ct-1 2)  ( gt  + rt  ot BUc ) ct-1 2 + BWc Bx.

By applying the above inequality recursively, we have

ht 2 

ct2



BWc

Bx

t t

-1 -1

,

where

 = maxjt { gj  + rj  oj BUc }. We also have ht 2  d. Thus, put together, we

19

Under review as a conference paper at ICLR 2019

have

ht 2  min

 d,

BWc

Bx

 

t t

-1 -1

. Next, we investigate the Lipschitz continuity of ht.

ht - ht 2  ot tanh(ct) - ot tanh(ct) 2

 ot - ot 2 tanh(ct)  + ot  ct - ct 2

We have to expand ot - ot,

ot - ot 2  Bx Wo - Wo 2 + BUo ht-1 - ht-1 2 + ht-1 2 Uo - Uo 2.

Note that BUo 2 is usually small, ot and ot are close, and we have ht-1-ht-1 2  ot  ct-1-

ct-1 2  ct-1 - ct-1 2. Thus, we can derive



ot - ot 2  Bx Wo - Wo 2 + BUo ct-1 - ct-1 2 + d Uo - Uo 2.

We also expand ct - ct to get,

ct - ct 2  ct-1  gt - gt 2 + rt  ct-1 - ct-1 2 + ct  rt - rt 2 + rt  ct - ct 2.

We also have,

ct - ct 2  BUc

ht-1 - ht-1 2 +

ht-1

2

Uc - Uc

2

+ 

Bx

Wc - Wc

2,

gt - gt 2  Bx Wg - Wg 2 + BWg ht-1 - ht-1 2 +  d Ug - Ug 2, and

rt - rt 2  Bx Wr - Wr 2 + BWr ht-1 - ht-1 2 + d Ur - Ur 2.

Putting together, we get

ct - ct 2  Bx  Wc - Wc 2 + Wg - Wg 2 + Wr - Wr 2
+ d Uc - Uc 2 + Ug - Ug 2 + Ur - Ur 2 + gt  ct-1 - ct-1 2 + rt BUc + BUg + BUr ht-1 - ht-1 2  Bx  Wc - Wc 2 + Wg - Wg 2 + Wr - Wr 2 + (BUc + BUg + BUr ) Wo - Wo 2 + d Uc - Uc 2 + Ug - Ug 2 + Ur - Ur 2 + (BUc + BUg + BUr ) Uo - Uo 2 + ot  rt BUc + BUg + BUr + BUo ct-1 - ct-1 2. By induction, we have

ct - ct 2



Bx

t - 1 -1

Wc - Wc 2 + Wg - Wg 2 + Wr - Wr 2 + (BUc + BUg + BUr ) Wo - Wo 2

+

 d

t 

- -

1 1

Uc - Uc 2 + Ug - Ug 2 + Ur - Ur 2 + (BUc + BUg + BUr ) Uo - Uo 2 ,

where  =  + BUg + BUr + BUo . Now we immediately have

ht - ht 2



Bx

t - 1 -1

Wc - Wc 2 + Wg - Wg 2 + Wr - Wr 2 + (BUc + BUg + BUr ) Wo - Wo 2

+

 d

t 

- -

1 1

Uc - Uc 2 + Ug - Ug 2 + Ur - Ur 2 + (BUc + BUg + BUr ) Uo - Uo 2 .

Then the Lipschitz continuity of yt can be written as



yt - yt 2  yBV ht - ht 2 + y d V - V 2.

Following the same argument for proving the generalization bound of vanilla RNNs, we can get the generalization bound for LSTM RNNs as



dyBV min

P

(zt

=

zt)



R (ft)

+

O

 



 d,

BWc

Bx

 t -1 -1



m

log

 dm

t -1 -1

+



log m

1 

  

.

20

Under review as a conference paper at ICLR 2019

D EXTENSION TO CONVOLUTIONAL RNNS
We further extend our analysis to Convolutional RNNs (Conv RNNs). Conv RNNs integrate convolutional filters and recurrent neural networks. Specifically, we consider input x  Rd and k-channel k-dimensional convolutional filters I1, . . . , Ik  Rk followed by an average pooling layer over the k channels for reducing dimensionality. Extensions to convolution with strides and other kinds of average pooling layers (e.g., blockwise pooling) are straightforward.

Here we denote the circulant-like matrix gen-

erated by Ii as

Ii 0 . . . . . . . . . 0

 d-k 

0 Ii 0 . . . . . . 0 

 d-k-1 

Ci

=

  

...

...

...

  



R(d-k+1)×d,

0 . . . . . . 0 Ii 0 

 d-k-1



0 . . . . . . . . . 0

Ii

 

C1 <latexit sha1_base64="q9TSbQOxnCzoo9kGxpodj5+G2Vc=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk0lph4QAIXsrfMwYa9vcvungkh/AYbC42x9QfZ+W9c4AoFXzLJy3szmZkXpoJr47rfTmFre2d3r7hfOjg8Oj4pn561dZIphj5LRKK6IdUouETfcCOwmyqkcSiwE06aC7/zhErzRD6aaYpBTEeSR5xRYyW/2hx41UG54tbcJcgm8XJSgRytQfmrP0xYFqM0TFCte56bmmBGleFM4LzUzzSmlE3oCHuWShqjDmbLY+fkyipDEiXKljRkqf6emNFY62kc2s6YmrFe9xbif14vM9FtMOMyzQxKtloUZYKYhCw+J0OukBkxtYQyxe2thI2poszYfEo2BG/95U3Srtc8t+Y91CuNuzyOIlzAJVyDBzfQgHtogQ8MODzDK7w50nlx3p2PVWvByWfO4Q+czx9yp43F</latexit>
C2 <latexit sha1_base64="0uzefC9E+MdsMHfxoy06huLMd9s=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk0lph4QAIXsrfMwYa9vcvungkh/AYbC42x9QfZ+W9c4AoFXzLJy3szmZkXpoJr47rfTmFre2d3r7hfOjg8Oj4pn561dZIphj5LRKK6IdUouETfcCOwmyqkcSiwE06aC7/zhErzRD6aaYpBTEeSR5xRYyW/2hzUq4Nyxa25S5BN4uWkAjlag/JXf5iwLEZpmKBa9zw3NcGMKsOZwHmpn2lMKZvQEfYslTRGHcyWx87JlVWGJEqULWnIUv09MaOx1tM4tJ0xNWO97i3E/7xeZqLbYMZlmhmUbLUoygQxCVl8ToZcITNiagllittbCRtTRZmx+ZRsCN76y5ukXa95bs17qFcad3kcRbiAS7gGD26gAffQAh8YcHiGV3hzpPPivDsfq9aCk8+cwx84nz90LI3G</latexit>
C3 <latexit sha1_base64="8pnI62g/OWE2R3yzlbmXzg5QRnk=">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsstCTSWGLiIQlcyN4yBxv29i67eyaE8BtsLDTG1h9k579xgSsUfMkkL+/NZGZemAqujet+O4WNza3tneJuaW//4PCofHzS1kmmGPosEYnqhFSj4BJ9w43ATqqQxqHAx3DcnPuPT6g0T+SDmaQYxHQoecQZNVbyq83+VbVfrrg1dwGyTrycVCBHq1/+6g0SlsUoDRNU667npiaYUmU4Ezgr9TKNKWVjOsSupZLGqIPp4tgZubDKgESJsiUNWai/J6Y01noSh7YzpmakV725+J/XzUx0E0y5TDODki0XRZkgJiHzz8mAK2RGTCyhTHF7K2EjqigzNp+SDcFbfXmdtOs1z6159/VK4zaPowhncA6X4ME1NOAOWuADAw7P8ApvjnRenHfnY9lacPKZU/gD5/MHdbGNxw==</latexit>

I1>
<latexit sha1_base64="693YD64SqUxTnbmUycuzhnNE96A=">AAAB/XicbVDLSgMxFM3UV62v8bFzE2wFV2WmCLosuNFdBfuAzjhk0kwbmkmGJCPUofgrblwo4tb/cOffmGlnoa0HAodz7uWenDBhVGnH+bZKK6tr6xvlzcrW9s7unr1/0FEilZi0sWBC9kKkCKOctDXVjPQSSVAcMtINx1e5330gUlHB7/QkIX6MhpxGFCNtpMA+qnkx0iOMWHYzDdx7T4ukFthVp+7MAJeJW5AqKNAK7C9vIHAaE64xQ0r1XSfRfoakppiRacVLFUkQHqMh6RvKUUyUn83ST+GpUQYwEtI8ruFM/b2RoVipSRyayTyqWvRy8T+vn+ro0s8oT1JNOJ4filIGtYB5FXBAJcGaTQxBWFKTFeIRkghrU1jFlOAufnmZdBp116m7t41q87yoowyOwQk4Ay64AE1wDVqgDTB4BM/gFbxZT9aL9W59zEdLVrFzCP7A+vwB0HGUvw==</latexit>

I1>
<latexit sha1_base64="693YD64SqUxTnbmUycuzhnNE96A=">AAAB/XicbVDLSgMxFM3UV62v8bFzE2wFV2WmCLosuNFdBfuAzjhk0kwbmkmGJCPUofgrblwo4tb/cOffmGlnoa0HAodz7uWenDBhVGnH+bZKK6tr6xvlzcrW9s7unr1/0FEilZi0sWBC9kKkCKOctDXVjPQSSVAcMtINx1e5330gUlHB7/QkIX6MhpxGFCNtpMA+qnkx0iOMWHYzDdx7T4ukFthVp+7MAJeJW5AqKNAK7C9vIHAaE64xQ0r1XSfRfoakppiRacVLFUkQHqMh6RvKUUyUn83ST+GpUQYwEtI8ruFM/b2RoVipSRyayTyqWvRy8T+vn+ro0s8oT1JNOJ4filIGtYB5FXBAJcGaTQxBWFKTFeIRkghrU1jFlOAufnmZdBp116m7t41q87yoowyOwQk4Ay64AE1wDVqgDTB4BM/gFbxZT9aL9W59zEdLVrFzCP7A+vwB0HGUvw==</latexit>

I1>
<latexit sha1_base64="693YD64SqUxTnbmUycuzhnNE96A=">AAAB/XicbVDLSgMxFM3UV62v8bFzE2wFV2WmCLosuNFdBfuAzjhk0kwbmkmGJCPUofgrblwo4tb/cOffmGlnoa0HAodz7uWenDBhVGnH+bZKK6tr6xvlzcrW9s7unr1/0FEilZi0sWBC9kKkCKOctDXVjPQSSVAcMtINx1e5330gUlHB7/QkIX6MhpxGFCNtpMA+qnkx0iOMWHYzDdx7T4ukFthVp+7MAJeJW5AqKNAK7C9vIHAaE64xQ0r1XSfRfoakppiRacVLFUkQHqMh6RvKUUyUn83ST+GpUQYwEtI8ruFM/b2RoVipSRyayTyqWvRy8T+vn+ro0s8oT1JNOJ4filIGtYB5FXBAJcGaTQxBWFKTFeIRkghrU1jFlOAufnmZdBp116m7t41q87yoowyOwQk4Ay64AE1wDVqgDTB4BM/gFbxZT9aL9W59zEdLVrFzCP7A+vwB0HGUvw==</latexit>

I1>
<latexit sha1_base64="693YD64SqUxTnbmUycuzhnNE96A=">AAAB/XicbVDLSgMxFM3UV62v8bFzE2wFV2WmCLosuNFdBfuAzjhk0kwbmkmGJCPUofgrblwo4tb/cOffmGlnoa0HAodz7uWenDBhVGnH+bZKK6tr6xvlzcrW9s7unr1/0FEilZi0sWBC9kKkCKOctDXVjPQSSVAcMtINx1e5330gUlHB7/QkIX6MhpxGFCNtpMA+qnkx0iOMWHYzDdx7T4ukFthVp+7MAJeJW5AqKNAK7C9vIHAaE64xQ0r1XSfRfoakppiRacVLFUkQHqMh6RvKUUyUn83ST+GpUQYwEtI8ruFM/b2RoVipSRyayTyqWvRy8T+vn+ro0s8oT1JNOJ4filIGtYB5FXBAJcGaTQxBWFKTFeIRkghrU1jFlOAufnmZdBp116m7t41q87yoowyOwQk4Ay64AE1wDVqgDTB4BM/gFbxZT9aL9W59zEdLVrFzCP7A+vwB0HGUvw==</latexit>

I2>
<latexit sha1_base64="M1cGSsexzoIgE8kzyxrhnzzW6lQ=">AAAB/XicbVC7TsMwFHXKq5RXeGwsFi0SU5VkgbESC2xFoi1SEyLHdVqrjh3ZDlKJKn6FhQGEWPkPNv4Gp80ALUeydHTOvbrHJ0oZVdpxvq3Kyura+kZ1s7a1vbO7Z+8fdJXIJCYdLJiQdxFShFFOOppqRu5SSVASMdKLxpeF33sgUlHBb/UkJUGChpzGFCNtpNA+avgJ0iOMWH49Db17X4u0Edp1p+nMAJeJW5I6KNEO7S9/IHCWEK4xQ0r1XSfVQY6kppiRac3PFEkRHqMh6RvKUUJUkM/ST+GpUQYwFtI8ruFM/b2Ro0SpSRKZySKqWvQK8T+vn+n4IsgpTzNNOJ4fijMGtYBFFXBAJcGaTQxBWFKTFeIRkghrU1jNlOAufnmZdL2m6zTdG6/e8so6quAYnIAz4IJz0AJXoA06AINH8AxewZv1ZL1Y79bHfLRilTuH4A+szx/RYZS+</latexit>
I2>
<latexit sha1_base64="M1cGSsexzoIgE8kzyxrhnzzW6lQ=">AAAB/XicbVC7TsMwFHXKq5RXeGwsFi0SU5VkgbESC2xFoi1SEyLHdVqrjh3ZDlKJKn6FhQGEWPkPNv4Gp80ALUeydHTOvbrHJ0oZVdpxvq3Kyura+kZ1s7a1vbO7Z+8fdJXIJCYdLJiQdxFShFFOOppqRu5SSVASMdKLxpeF33sgUlHBb/UkJUGChpzGFCNtpNA+avgJ0iOMWH49Db17X4u0Edp1p+nMAJeJW5I6KNEO7S9/IHCWEK4xQ0r1XSfVQY6kppiRac3PFEkRHqMh6RvKUUJUkM/ST+GpUQYwFtI8ruFM/b2Ro0SpSRKZySKqWvQK8T+vn+n4IsgpTzNNOJ4fijMGtYBFFXBAJcGaTQxBWFKTFeIRkghrU1jNlOAufnmZdL2m6zTdG6/e8so6quAYnIAz4IJz0AJXoA06AINH8AxewZv1ZL1Y79bHfLRilTuH4A+szx/RYZS+</latexit>
I2>
<latexit sha1_base64="M1cGSsexzoIgE8kzyxrhnzzW6lQ=">AAAB/XicbVC7TsMwFHXKq5RXeGwsFi0SU5VkgbESC2xFoi1SEyLHdVqrjh3ZDlKJKn6FhQGEWPkPNv4Gp80ALUeydHTOvbrHJ0oZVdpxvq3Kyura+kZ1s7a1vbO7Z+8fdJXIJCYdLJiQdxFShFFOOppqRu5SSVASMdKLxpeF33sgUlHBb/UkJUGChpzGFCNtpNA+avgJ0iOMWH49Db17X4u0Edp1p+nMAJeJW5I6KNEO7S9/IHCWEK4xQ0r1XSfVQY6kppiRac3PFEkRHqMh6RvKUUJUkM/ST+GpUQYwFtI8ruFM/b2Ro0SpSRKZySKqWvQK8T+vn+n4IsgpTzNNOJ4fijMGtYBFFXBAJcGaTQxBWFKTFeIRkghrU1jNlOAufnmZdL2m6zTdG6/e8so6quAYnIAz4IJz0AJXoA06AINH8AxewZv1ZL1Y79bHfLRilTuH4A+szx/RYZS+</latexit>
I2>
<latexit sha1_base64="M1cGSsexzoIgE8kzyxrhnzzW6lQ=">AAAB/XicbVC7TsMwFHXKq5RXeGwsFi0SU5VkgbESC2xFoi1SEyLHdVqrjh3ZDlKJKn6FhQGEWPkPNv4Gp80ALUeydHTOvbrHJ0oZVdpxvq3Kyura+kZ1s7a1vbO7Z+8fdJXIJCYdLJiQdxFShFFOOppqRu5SSVASMdKLxpeF33sgUlHBb/UkJUGChpzGFCNtpNA+avgJ0iOMWH49Db17X4u0Edp1p+nMAJeJW5I6KNEO7S9/IHCWEK4xQ0r1XSfVQY6kppiRac3PFEkRHqMh6RvKUUJUkM/ST+GpUQYwFtI8ruFM/b2Ro0SpSRKZySKqWvQK8T+vn+n4IsgpTzNNOJ4fijMGtYBFFXBAJcGaTQxBWFKTFeIRkghrU1jNlOAufnmZdL2m6zTdG6/e8so6quAYnIAz4IJz0AJXoA06AINH8AxewZv1ZL1Y79bHfLRilTuH4A+szx/RYZS+</latexit>

I3>
<latexit sha1_base64="efrsSXsPytnSPSOBBkGXX4gKhuI=">AAAB/XicbVC7TsMwFHXKq5RXeGwsFi0SU5WUAcZKLLAViT6kJkSO67RWHTuyHaQSVfwKCwMIsfIfbPwNTpsBWo5k6eice3WPT5gwqrTjfFulldW19Y3yZmVre2d3z94/6CiRSkzaWDAheyFShFFO2ppqRnqJJCgOGemG46vc7z4Qqajgd3qSED9GQ04jipE2UmAf1bwY6RFGLLuZBuf3nhZJLbCrTt2ZAS4TtyBVUKAV2F/eQOA0JlxjhpTqu06i/QxJTTEj04qXKpIgPEZD0jeUo5goP5uln8JTowxgJKR5XMOZ+nsjQ7FSkzg0k3lUtejl4n9eP9XRpZ9RnqSacDw/FKUMagHzKuCASoI1mxiCsKQmK8QjJBHWprCKKcFd/PIy6TTqrlN3bxvVZqOoowyOwQk4Ay64AE1wDVqgDTB4BM/gFbxZT9aL9W59zEdLVrFzCP7A+vwB0uuUvw==</latexit>
I3>
<latexit sha1_base64="efrsSXsPytnSPSOBBkGXX4gKhuI=">AAAB/XicbVC7TsMwFHXKq5RXeGwsFi0SU5WUAcZKLLAViT6kJkSO67RWHTuyHaQSVfwKCwMIsfIfbPwNTpsBWo5k6eice3WPT5gwqrTjfFulldW19Y3yZmVre2d3z94/6CiRSkzaWDAheyFShFFO2ppqRnqJJCgOGemG46vc7z4Qqajgd3qSED9GQ04jipE2UmAf1bwY6RFGLLuZBuf3nhZJLbCrTt2ZAS4TtyBVUKAV2F/eQOA0JlxjhpTqu06i/QxJTTEj04qXKpIgPEZD0jeUo5goP5uln8JTowxgJKR5XMOZ+nsjQ7FSkzg0k3lUtejl4n9eP9XRpZ9RnqSacDw/FKUMagHzKuCASoI1mxiCsKQmK8QjJBHWprCKKcFd/PIy6TTqrlN3bxvVZqOoowyOwQk4Ay64AE1wDVqgDTB4BM/gFbxZT9aL9W59zEdLVrFzCP7A+vwB0uuUvw==</latexit>
I3>
<latexit sha1_base64="efrsSXsPytnSPSOBBkGXX4gKhuI=">AAAB/XicbVC7TsMwFHXKq5RXeGwsFi0SU5WUAcZKLLAViT6kJkSO67RWHTuyHaQSVfwKCwMIsfIfbPwNTpsBWo5k6eice3WPT5gwqrTjfFulldW19Y3yZmVre2d3z94/6CiRSkzaWDAheyFShFFO2ppqRnqJJCgOGemG46vc7z4Qqajgd3qSED9GQ04jipE2UmAf1bwY6RFGLLuZBuf3nhZJLbCrTt2ZAS4TtyBVUKAV2F/eQOA0JlxjhpTqu06i/QxJTTEj04qXKpIgPEZD0jeUo5goP5uln8JTowxgJKR5XMOZ+nsjQ7FSkzg0k3lUtejl4n9eP9XRpZ9RnqSacDw/FKUMagHzKuCASoI1mxiCsKQmK8QjJBHWprCKKcFd/PIy6TTqrlN3bxvVZqOoowyOwQk4Ay64AE1wDVqgDTB4BM/gFbxZT9aL9W59zEdLVrFzCP7A+vwB0uuUvw==</latexit>
I3>
<latexit sha1_base64="efrsSXsPytnSPSOBBkGXX4gKhuI=">AAAB/XicbVC7TsMwFHXKq5RXeGwsFi0SU5WUAcZKLLAViT6kJkSO67RWHTuyHaQSVfwKCwMIsfIfbPwNTpsBWo5k6eice3WPT5gwqrTjfFulldW19Y3yZmVre2d3z94/6CiRSkzaWDAheyFShFFO2ppqRnqJJCgOGemG46vc7z4Qqajgd3qSED9GQ04jipE2UmAf1bwY6RFGLLuZBuf3nhZJLbCrTt2ZAS4TtyBVUKAV2F/eQOA0JlxjhpTqu06i/QxJTTEj04qXKpIgPEZD0jeUo5goP5uln8JTowxgJKR5XMOZ+nsjQ7FSkzg0k3lUtejl4n9eP9XRpZ9RnqSacDw/FKUMagHzKuCASoI1mxiCsKQmK8QjJBHWprCKKcFd/PIy6TTqrlN3bxvVZqOoowyOwQk4Ay64AE1wDVqgDTB4BM/gFbxZT9aL9W59zEdLVrFzCP7A+vwB0uuUvw==</latexit>
WI <latexit sha1_base64="JF/qhyL3o+tFX1EVFQztaZFMubM=">AAAB+nicbVA9T8MwFHwpX6V8pTCyWLRITFXSBcYKFtiKRFukNooc12mtOk5kO6Aq9KewMIAQK7+EjX+D02aAlpMsne7e0ztfkHCmtON8W6W19Y3NrfJ2ZWd3b//Arh52VZxKQjsk5rG8D7CinAna0Uxzep9IiqOA014wucr93gOVisXiTk8T6kV4JFjICNZG8u1qvedngwjrMcE8u5nN6r5dcxrOHGiVuAWpQYG2b38NhjFJIyo04Vipvusk2suw1IxwOqsMUkUTTCZ4RPuGChxR5WXz6DN0apQhCmNpntBorv7eyHCk1DQKzGQeUi17ufif1091eOFlTCSppoIsDoUpRzpGeQ9oyCQlmk8NwUQykxWRMZaYaNNWxZTgLn95lXSbDddpuLfNWuuyqKMMx3ACZ+DCObTgGtrQAQKP8Ayv8GY9WS/Wu/WxGC1Zxc4R/IH1+QPnAJPA</latexit>

 <latexit sha1_base64="o/TztkgjxWN4eKNCgAhptEPqjkA=">AAAB73icbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lpjIRwIXsrfswYa9vXN3zoRc+BM2Fhpj69+x89+4wBUKvmSSl/dmMjMvSKQw6LrfTmFjc2t7p7hb2ts/ODwqH5+0TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST27nfeeLaiFg94DThfkRHSoSCUbRSt9pHEXFTHZQrbs1dgKwTLycVyNEclL/6w5ilEVfIJDWm57kJ+hnVKJjks1I/NTyhbEJHvGeponaLny3unZELqwxJGGtbCslC/T2R0ciYaRTYzoji2Kx6c/E/r5dieO1nQiUpcsWWi8JUEozJ/HkyFJozlFNLKNPC3krYmGrK0EZUsiF4qy+vk3a95rk1775eadzkcRThDM7hEjy4ggbcQRNawEDCM7zCm/PovDjvzseyteDkM6fwB87nD3ASj44=</latexit>

=) <latexit sha1_base64="vCKc/6hBHkQXgfn2Bg3RplKlHzA=">AAAB+nicbVBNT8JAEJ3iF+JX0aOXRjDxRFoueiR68eABE/lIoCHbZVs2bHeb3a2EVH6KFw8a49Vf4s1/4wI9KPiSSV7em8nMvCBhVGnX/bYKG5tb2zvF3dLe/sHhkV0+biuRSkxaWDAhuwFShFFOWppqRrqJJCgOGOkE45u533kkUlHBH/Q0IX6MIk5DipE20sAuV/t3gkeSRiONpBST6sCuuDV3AWedeDmpQI7mwP7qDwVOY8I1Zkipnucm2s+Q1BQzMiv1U0UShMcoIj1DOYqJ8rPF6TPn3ChDJxTSFNfOQv09kaFYqWkcmM4Y6ZFa9ebif14v1eGVn1GepJpwvFwUpszRwpnn4AypJFizqSEIS2pudfAISYS1SatkQvBWX14n7XrNc2vefb3SuM7jKMIpnMEFeHAJDbiFJrQAwwSe4RXerCfrxXq3PpatBSufOYE/sD5/AP9Nk9A=</latexit>

x <latexit sha1_base64="G49X2kJVehBS/VicdZIWw1N1guU=">AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lhgFTOBC9pY92LC3d9mdM5ILP8HGQmNs/UV2/hsXuELBl0zy8t5MZuYFiRQGXffbKaytb2xuFbdLO7t7+wflw6O2iVPNeIvFMtYPATVcCsVbKFDyh0RzGgWSd4Lx9czvPHJtRKzucZJwP6JDJULBKFrprvpU7Zcrbs2dg6wSLycVyNHsl796g5ilEVfIJDWm67kJ+hnVKJjk01IvNTyhbEyHvGupohE3fjY/dUrOrDIgYaxtKSRz9fdERiNjJlFgOyOKI7PszcT/vG6K4aWfCZWkyBVbLApTSTAms7/JQGjOUE4soUwLeythI6opQ5tOyYbgLb+8Str1mufWvNt6pXGVx1GEEziFc/DgAhpwA01oAYMhPMMrvDnSeXHenY9Fa8HJZ47hD5zPH51SjVY=</latexit>

Average
<latexit sha1_base64="jkkUwNyz9zz41O8vlB3HO6EJHTc=">AAAB+HicbVDLSgMxFL1TX7U+OurSTbAIrspMEXRZceOygn1AO5RMmmlDM8mQZIQ69EvcuFDErZ/izr8xnc5CWw8EDufcV06YcKaN5307pY3Nre2d8m5lb//gsOoeHXe0TBWhbSK5VL0Qa8qZoG3DDKe9RFEch5x2w+ntwu8+UqWZFA9mltAgxmPBIkawsdLQrd5YF48paklpZ4yHbs2reznQOvELUoMCraH7NRhJksZUGMKx1n3fS0yQYWUY4XReGaSaJphM7Y6+pQLHVAdZfvgcnVtlhCKp7BMG5ervjgzHWs/i0FbG2Ez0qrcQ//P6qYmug4yJJDVUkOWiKOXISLRIAY2YosTwmSWYKGZvRWSCFSbGZlWxIfirX14nnUbd9+r+faPWvCziKMMpnMEF+HAFTbiDFrSBQArP8ApvzpPz4rw7H8vSklP0nMAfOJ8/c0GS5A==</latexit>

Pooling

=) P<latexitsha1_base64="vCKc/6hBHkQXgfn2Bg3RplKlHzA=">AAAB+nicbVBNT8JAEJ3iF+JX0aOXRjDxRFoueiR68eABE/lIoCHbZVs2bHeb3a2EVH6KFw8a49Vf4s1/4wI9KPiSSV7em8nMvCBhVGnX/bYKG5tb2zvF3dLe/sHhkV0+biuRSkxaWDAhuwFShFFOWppqRrqJJCgOGOkE45u533kkUlHBH/Q0IX6MIk5DipE20sAuV/t3gkeSRiONpBST6sCuuDV3AWedeDmpQI7mwP7qDwVOY8I1Zkipnucm2s+Q1BQzMiv1U0UShMcoIj1DOYqJ8rPF6TPn3ChDJxTSFNfOQv09kaFYqWkcmM4Y6ZFa9ebif14v1eGVn1GepJpwvFwUpszRwpnn4AypJFizqSEIS2pudfAISYS1SatkQvBWX14n7XrNc2vefb3SuM7jKMIpnMEFeHAJDbiFJrQAwwSe4RXerCfrxXq3PpatBSufOYE/sD5/AP9Nk9A=</latexit>
<latexit sha1_base64="6oM9LZWw0N5BODND25NmHoPcwj8=">AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsaLYk2lhjlI4EL2Vv2YMPe3mV3zoRc+Ak2Fhpj6y+y89+4wBUKvmSSl/dmMjMvSKQw6LrfTmFjc2t7p7hb2ts/ODwqH5+0TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST27nfeeLaiFg94jThfkRHSoSCUbTSQ7VZHZQrbs1dgKwTLycVyNEclL/6w5ilEVfIJDWm57kJ+hnVKJjks1I/NTyhbEJHvGepohE3frY4dUYurDIkYaxtKSQL9fdERiNjplFgOyOKY7PqzcX/vF6K4bWfCZWkyBVbLgpTSTAm87/JUGjOUE4toUwLeythY6opQ5tOyYbgrb68Ttr1mufWvPt6pXGTx1GEMziHS/DgChpwB01oAYMRPMMrvDnSeXHenY9la8HJZ07hD5zPH2CKjS4=</latexit>

I
<latexit sha1_base64="z618fGDmaxgE3Mb7zVMaLkRPKEQ=">AAAB/XicbVC7TsMwFHXKq5RXeGwsFi0SU5V0gbGCBbYi0YfURtWN67RWHSeyHUSJKn6FhQGEWPkPNv4Gp80ALUeydHTOvbrHx485U9pxvq3Cyura+kZxs7S1vbO7Z+8ftFSUSEKbJOKR7PigKGeCNjXTnHZiSSH0OW3746vMb99TqVgk7vQkpl4IQ8ECRkAbqW8fVXoh6BEBnt5McQ+Uxg+Vvl12qs4MeJm4OSmjHI2+/dUbRCQJqdCEg1Jd14m1l4LUjHA6LfUSRWMgYxjSrqECQqq8dJZ+ik+NMsBBJM0TGs/U3xsphEpNQt9MZlHVopeJ/3ndRAcXXspEnGgqyPxQkHCsI5xVgQdMUqL5xBAgkpmsmIxAAtGmsJIpwV388jJp1aquU3Vva+X6ZV5HER2jE3SGXHSO6ugaNVATEfSIntErerOerBfr3fqYjxasfOcQ/YH1+QNxQZSM</latexit>

x

Figure 4: Illustration of input x  R6 convolving with 3-channel 3-dimensional convolutional filters

d-k I1, I2, and I3, followed by an average pooling.

and write WI

= [C1 , . . . , Ck ]

. We further denote P

=

1 k

[Id-k+1

Id-k+1

···

Id-k+1],

where

totally k identity matrices
Id denotes the d-dimensional identity matrix. Define I = [I1, . . . , Ik], and I  x = P WIx. Given a sample (xt, zt)Tt=1, the Conv RNNs compute ht and yt as follows,

ht = h (U  ht-1 + W  xt) , and yt = y (V  ht) ,

where ht, xt  Rd, and U , V, W  Rk×k are matrices with column vectors being k-dimensional convolutional filters. We use zero-padding to ensure the output dimension of convolutional filters
matches the input (Krizhevsky et al., 2012). To get yt, we convolve ht with V followed by an average pooling to reduce the dimension to K. Since we aim to show that Conv RNNs reduce the dependence on d in generalization through parameter sharing, we simplify the notations to assume h0 = 0, and impose the following assumption. Extensions to general settings are straightforward.

Assumption 8. The activation operators h and y are 1-Lipschitz with h(0) = y(0) = 0. h

is entrywise bounded by 1. The convolutional filters U, V, and W are orthogonal with normalized

columns, i.e., U U = UU

=

1 k

Ik

,

V

V = VV

=

1 k

Ik

,

and

W

W = WW

=

1 k

Ik

.

We remark that the orthogonality constraints enhance the diversity among convolutional filters (Xie

et

al.,

2017;

Huang

et

al.,

2017).

Additionally,

the

normalization

factor

1 k

is

to

control

the

spectral

norms of WU , WV , and WW , which prevents the blowup of hidden state. Denote by Fc,t the class of

mappings from the first t inputs to the t-th output computed by Conv RNNs. Then the generalization

bound is given in the following theorem.

Theorem 6. Let activation operators h and y be given, and Assumptions 1 and 8 hold. Then for (xt, zt)tT=1 and S = (xi,t, zi,t)Tt=1, i = 1, . . . , m drawn i.i.d. from any underlying distribution over Rd×T × {1, . . . , K}, with probability at least 1 -  over S, for every margin value  > 0 and
every ft  Fc,t for integer t  T , we have



P (zt = zt)  R(ft) + O  Bxkt

log (dt m

m) +



log m

1 



.

The detailed proof is provided in D.1. Similar to the analysis of vanilla RNNs, our proof is based
on the Lipschitz continuity of Conv RNNs with respect to its model parameters in the convolutional filters. Specifically, by Assumption 8, the spectral norms of WU , WV , and WW are all bounded by 1. Combining with the inequality, WU F  d U F, we have yt - yt 2  LV,t V - V F + LU,t U - U F + LW,t W - W F, where LU,t, LV,t, and LW,t are polynomials in d and t. Additionally, observe that the total number of parameters in a Conv RNN is at most 3k2, which is independent of input dimension d. As a consequence, the generalization bound of Conv RNNs only has a lieanr dependence on k and t.

21

Under review as a conference paper at ICLR 2019

D.1 PROOF OF THEOREM 6

Proof. We first characterize the Lipschitz continuity of yt 2 with respect to model parameters U , W and V. We have

yt - yt 2  y ht 2 WV - WV 2 + y WV 2 ht - ht 2. 
Since ht   1, we have ht 2  d. Then we expand ht - ht,

ht - ht 2  h U  ht-1 + W  xt - U  ht-1 - W  xt 2

= h P WU ht-1 + P WW xt - P WU ht-1 - P WW xt 2

 h

P

2

WU ht-1

+ WW xt

-

WU 

ht-1

-

WW

xt

2

 h P 2 Bx WW - WW 2 + d WU - WU 2 + WU 2 ht-1 - ht-1 2

Observe that we have by the definition of circulant matrix,

WU - WU

2 2



WU - WU

2 F

= (d - k)

U -U

2 F

d

U

-U

F2.

The same holds for WW - WW and WV - WV . We also have P 2 = 1. The remaining task

is to bound the spectral norm of WU and WV . Consider the matrix product WU WU . We claim

that the diagonal elements of WU WU is bounded by

k i=1

Ui

22,

and the off-diagonal elements

are zero. To see this, denote by CUi the circulant like matrix generated by Ui. Then we have

WU = [CU1 , . . . , CUk ] . The diagonal elements of WU WU are

k
WU WU ii =
j=1

CUj CUj

k

ii i=1

Ui

2 2

.

By the orthogonality of U, the off-diagonal elements are

k
WU WU pq =
j=1

CUj CUj

k
=
pq j=1

CUj :p

CUj :q = 0.

Thus, the spectral norm WU 2 

k i=1

Ui

2 2

 1, and

WV

2,

WW

2  1 also hold. Then

we can derive



ht - ht 2  hBx d W - W F + hd U - U F + h ht-1 - ht-1 2.

Apply the above inequality recursively, we get

ht - ht

2



h

Bx 

 d

ht h

- -

1 1

W -W

F

+

hd

th h

- -

1 1

U -U

 Bx dt W - W F + dt U - U F.

F

Thus, we have the following Lipschitz continuity of yt 2, 
yt - yt 2  d V - V F + Bx dt W - W F + dt U - U F.

We also bound the norm of ht by induction. Specifically, we have

ht 2  h P WU ht-1 + P WW xt 2  h WU ht-1 2 + h WW xt 2  ht-1 2 + Bx. 
Applying the above expression recursively, we have ht 2  min{ d, Bxt}  Bxt. Then following the same argument for proving the generalization bound of vanilla RNNs, we can get the
generalization bound for Conv RNNs as



P (zt = zt)  R(ft) + O  Bxkt

log (dt m

m) +



log m

1 



.

22

