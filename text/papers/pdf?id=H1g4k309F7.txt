Under review as a conference paper at ICLR 2019
WASSERSTEIN BARYCENTER MODEL ENSEMBLING
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper we propose to perform model ensembling in a multiclass or a multilabel learning setting using Wasserstein barycenters. Optimal transport metrics, such as the Wasserstein distance, allow incorporating semantic side information such as word embeddings. Using Wass. barycenters to find the consensus between models allows us to balance confidence and semantics in finding the agreement between the models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results show that the Wass. ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling.
1 INTRODUCTION
Model ensembling consists in combining many models into a stronger, more robust and more accurate model. Ensembling is ubiquitous in machine learning and yields improved accuracies across multiple prediction tasks such as multi-class or multi-label classification. For instance in deep learning, output layers of DNNs, such as softmaxes or sigmoids, are usually combined using a simple arithmetic or geometric mean. The arithmetic mean rewards confidence of the models while the geometric means seeks the consensus across models.
What is missing in the current approaches to models ensembling, is the ability to incorporate side information such as class relationships represented by a graph or via an embedding space. Often each semantic class can be represented with a finite dimensional vector in a pretrained word embedding space such as GloVe (Pennington et al., 2014). The models' predictions can be seen as defining a distribution in this label space defined by word embeddings: if we denote pi to be the confidence of a model on a bin corresponding to a word having an embedding xi, the distribution on the label space is therefore p = i pixi . In order to find the consensus between many models predictions, we propose to achieve this consensus within this representation in the label space. In contrast to arithmetic and geometric averaging, which are limited to the independent bins' confidence, this has the advantage of carrying the semantics to model averaging via the word embeddings.
To achieve this goal, we propose to combine model predictions via Wasserstein (Wass.) barycenters (Agueh & Carlier, 2011), which enables us to balance the confidence of the models and the semantic side information in finding a consensus between the models. Wasserstein distances are a naturally good fit for such a task, since they are defined with respect to a ground metric in the label space of the models, which carry such semantic information. Moreover they enable the possiblity of ensembling predictions defined on different label sets, since the Wasserstein distance allows to align and compare those different predictions. Since their introduction in (Agueh & Carlier, 2011) Wass. barycenter computations were facilitated by entropic regularization (Cuturi, 2013) and iterative algorithms that rely on iterative Bregman projections (Benamou et al., 2015). Many applications have used Wass. barycenters in NLP, clustering and graphics. We show in this paper that Wass. barycenters are effective in model ensembling and in finding a semantic consensus, and can be applied to a wide range of problems in the machine learning pipeline.
The paper is organized as follows: In Section 2 we revisit geometric and arithmetic means from a geometric viewpoint, showing that they are 2 and KL (extended KL divergence) barycenters respectively. We give a brief overview of optimal transport metric and Wass. barycenters in Section 3. We highlight the advantages of Wass. barycenter ensembling in terms of semantic smoothness and diversity in Section 4. Related work on Wass. barycenters in Machine learning are presented in Section 5. Finally we show applications of Wasserstein ensembling on attribute based classification, multi-label learning and image captioning in Section 6.
1

Under review as a conference paper at ICLR 2019

2 WASSERSTEIN BARYCENTERS FOR MODEL ENSEMBLING

Normalized and Unnormalized predictions Ensembling. In deep learning, predictions on a label space of fixed size M are usually in one of two forms: a) normalized probabilities: in a multiclass setting, the neural network outputs a probability vector (normalized through softmax), where each bin corresponds to a semantic class; b) unnormalized positive scores: in a multi-label setting, the outputs of M independent logistic units are unnormalized positive scores, where each unit corresponds to the presence or the absence of a semantic class.
Model ensembling in those two scenarios has long history in deep learning and more generally in machine learning (Breiman, 1996; Freund & Schapire, 1999; Wolpert, 1992) as they lead to more robust and accurate models. As discussed in the introduction, two methods have been prominent in model ensembling due to their simplicity: majority vote using the arithmetic mean of predictions, or consensus based using the geometric mean.
Note that in both cases (multi-class or multi-label), each semantic class can be represented with a finite dimensional vector in a pretrained word embedding space such as GloVe or word2vec. Arithmetic and geometric means do not use this semantic information in finding the consensus, and rely only on the confidence of the models.

Revisiting Arithmetic and Geometric Means from a geometric viewpoint. Given m predictions

µ , and weights   0 such that

m =1



= 1, the weighted arithmetic mean is given by µ¯a =

m =1



µ

,

and

the

weighted

geometric

mean

by

µ¯g

=

m=1(µ

).

It is instructive to reinterpret the arithmetic and geometric mean as weighted Frechet means (Definition 1) (Zemel & Panaretos, 2017).

Definition 1. [Weighted Frechet Mean] Given a distance d and {( , µ ),  > 0, µ 

R+M } =1...m, the Frechet mean is defined as follows: µ¯ = arg min

m =1



d(, µ

).

It is easy to prove (Appendix C) that the arithmetic mean corresponds to a Frechet mean for d =

.

2 2

(the

2 Euclidean distance).

A less known fact is that the geometric mean corresponds to a

Frechet Mean for d = KL, where KL is the extended KL divergence to unnormalized measures:

KL(p, q) =

i pi log

pi qi

- pi + qi. We give proofs and properties of arithmetic and geometric

mean in Appendix C.

Following this geometric viewpoint, in order to incorporate the semantics of the target space in
model ensembling, we need to use a distance d that takes advantage of the underlying geometry
of the word embedding label space when comparing positive measures. Optimal transport (OT)
metrics such as Wasserstein-2 have this property since they are built on an explicit cost matrix
defining pairwise distance between the semantic classes. In this paper we propose to use the Frechet means with Wasserstein distance (d = W22) for model ensembling, i.e. use Wasserstein barycenters (Agueh & Carlier, 2011) for model ensembling:

m

µ¯w

=

arg

min


 W22(, µ ).

=1

Intuitively, the barycenter looks for a distribution  that is close to all the base distributions µ in the Wasserstein sense. In our context transporting the consensus  to each individual model µ should have a minimal cost, where the cost is defined by the distance in the word embedding space.

3 WASSERSTEIN BARYCENTERS
Wasserstein distances were originally defined between normalized probability vectors (Balanced OT) (Villani, 2008; Peyre´ & Cuturi, 2017), but they have been extended to deal with unnormalized measures and this problem is referred to as unbalanced OT (Chizat et al., 2018; Frogner et al., 2015). Motivated by the multi-class and the multi-label ensembling applications, in the following we present a brief overview of Wass. barycenters in the balanced and unbalanced cases.

2

Under review as a conference paper at ICLR 2019

3.1 OPTIMAL TRANSPORT METRICS

Balanced OT. Given p  N , where N = {p  RN , pk  0,

N k=1

pk

=

1},

p

represents

histograms on source label space S = {xi  Rd, i = 1 . . . N }, in our case this corresponds to

words embeddings. Consider similarly q  M representing histograms whose bins are defined on a target label space T = {yj  Rd, j = 1 . . . M }. Consider a cost function c(x, y), (for example

c(x, y) = x - y 2). Let C be the matrix in  RN×M such that Cij = c(xi, yj). 1N denotes a

vector with all ones. Let   RN×M be a coupling matrix whose marginals are p and q such that:   (p, q) = {  RN×M , 1M = p,  1N = q}. The optimal transport metric is defined as

follows:

W (p, q) = min { C,  = Cijij.}
(p,q) ij

(1)

When c(x, y) = x - y 22, this distance corresponds to the so called Wasserstein-2 distance W22.

Unbalanced OT. When p and q are unnormalized and have different total masses, optimal transport metrics have been extended to deal with this unbalanced case. The main idea is in relaxing the
set (p, q) using a divergence such as the extended KL divergence. (Chizat et al., 2018) define for  > 0 the following Generalized Wasserstein distance between unnormalized measures:

GW (p, q) = min


C, 

+ KL(1M , p) + KL(

1N , q).

(2)

3.2 BALANCED AND UNBALANCED WASSERSTEIN IN MODELS ENSEMBLING

Throughout the paper we consider m discrete prediction vectors µ



N
R+

,

= 1 . . . m defined on

a discrete space (word embeddings for instance) S = {xi  Rd, i = 1 . . . N }. We refer to S

as source spaces. Our goal is to find a consensus prediction µ¯w  RM+ defined on a target discrete

space T = {yj  Rd, j = 1 . . . M }. Let C  RN ×M be the cost matrices, C ,i,j = c(xi , yj).

Balanced Wass. Barycenters: Normalized predictions. The Wass. barycenter (Agueh & Car-

lier, 2011) of normalized predictions is defined as follows: µ¯w = arg min

m =1



W (, µ

),

for

the Wasserstein distance W defined in equation (1). Hence one needs to solve the following prob-

lem, for m coupling matrices  , = 1 . . . m:

m

min min



  (µ ,), =1...m

=1

C ,

.

(3)

Unbalanced Wass. Barycenters: Unnormalized predictions. Similarly the Wass. barycenter of

unnormalized predictions is defined as follows: µ¯w = arg min

m =1



GW (, µ

),

for

the

Gen-

eralized Wasserstein distance GW defined in equation (2). Hence the unbalanced Wass. barycenter

problem (Chizat et al., 2018) amounts to solving , for m coupling matrices  , = 1 . . . m:

m

min min



  , =1...m

=1

C ,  + KL( 1M , µ ) + KL( 1N , ) .

(4)

3.3 COMPUTATION VIA ENTROPIC REGULARIZATION AND PRACTICAL ADVANTAGES

Entropic Regularized Wasserstein Barycenters Algorithms. The computation of the Wasser-
stein distance grows super-cubicly in the number of points. This issue was alleviated by the intro-
duction of the entropic regularization (Cuturi, 2013) to the optimization problem making it strongly
convex. Its solution can be found using scaling algorithms such as the so called Sinkhorn algorithm. For any positive matrix , the entropy is defined as follows: H() = - ij ij(log(ij) - 1). The entropic regularized OT distances in the balanced and unbalanced case become, for a hyperparameter  > 0:
W(p, q) = min C,  - H(),
(p,q)

GW(p,

q)

=

min


C, 

+ KL(1M , p) + KL(

1N , q) - H()

3

Under review as a conference paper at ICLR 2019

for   0, W and GW converge to the original OT distance, and for higher value of  we obtain the so called Sinkhorn divergence that allows for more diffuse transport between p and q. Balanced and

unbalanced Wass. barycenters can be naturally defined with the entropic regularized OT distance as

follows: min

m =1



W(, µ

)

and

min

m =1



GW(, µ

)

respectively.

This

regularization

leads to simple iterative algorithms (Benamou et al., 2015; Chizat et al., 2018) (for more details

we refer the interested reader to (Chizat et al., 2018) and references therein) for computing Wass.

barycenters that are given in Algorithms 1 and 2.

Algorithm 1: Balanced Barycenter for Multi- Algorithm 2: Unbalanced Barycenter for Multi-

class Ensembling (Benamou et al., 2015)

label Ensembling (Chizat et al., 2018)

Inputs: , C (|source| × |target|),  , µ

Initialize

K = exp(-C /), v  1M ,  = 1 . . . m

for u

i

= 

1

µ. .

.

Maxiter do , = 1...

m

Kv

p  exp

m =1



log

K

u

m=1(K v

u p

) =

1...m

Ku

=

end for

Output: p

Inputs: , C (|source| × |target|),  , , µ

Initialize

K = exp(-C /), v  1,  = 1 . . . m

for i = 1 . . . Maxiter do



u

µ Kv

+ ,  = 1 . . . m

p

+

m =1



Ku

 +

,



v

p Ku

+

= 1...m

end for

Output: p

We see that the output of Algorithm 1 is the geometric mean of K u , = 1 . . . m, where K is a Gaussian kernel with bandwidth  the entropic regularization parameter. Note v, = 1 . . . m

the values of v at convergence of Algorithm 1. The entropic regularized Wass. barycenter can be

written as follows:

M

exp  (log(K µ ) - log(K v)) .

=1
We see from this that K appears as matrix product multiplying individual models probability µ and the quantities v related to Lagrange multipliers. This matrix vector product with K ensures probability mass transfer between semantically related classes i.e between items that has entries K ,ij with high values.

Remark 1 (The case K = K = I). As the kernel K in Algorithm 1 approaches I (identity) (this happens when   0), the alternating Bregman projection of (Benamou et al., 2015) for balanced Wass. barycenter converges to the geometric mean µ¯g = m=1(µ ) . This is easy to see, when K = I, for all iterations we have v = 1M , and u = µ . When K = I the fixed point of Algorithm 1 reduces to geometric mean, and hence diverges from the Wass. barycenter. Note that
K approaches identity as   0, and in this case we don't exploit any semantics.

Practical Advantages of Wasserstein Barycenters in Models Ensembling. In the simplest case S = T and N = M for all , this corresponds to the case we discussed in multi-class and multi-labels ensemble learning, Wass. barycenters allows to balance semantics and confidence in finding the consensus. The case where source and target spaces are different is also of interest, we give here an application example in attribute based classification : µ corresponds to prediction on a set of attributes and we wish to make predictions through the Wass. barycenter on a set of labels defined with those attributes (See section 6.1).

4 THEORETICAL ADVANTAGES OF WASSERSTEIN BARYCENTERS IN MODELS ENSEMBLING
Smoothness of the Wasserstein Barycenter within Semantically Coherent Clusters. In this section we consider S = T = , i.e the Wass. barycenters and all individual models are defined on the same label space. When we are ensembling models, one desiderata is to have an accurate aggregate model. Smoothness and Diversity of the predictions of the ensemble is another desiderata

4

Under review as a conference paper at ICLR 2019

as we often want to supply many diverse hypotheses. In the context of sequence generation in language modeling such as image captioning, machine translation or dialog, this is very important as we use beam search on the predictions, diversity and smoothness of the predictions become key to the creativity and the composition of the sequence generator in order to go beyond "baby talk" and vanilla language based on high count words in the training set. Hence we need to increase the entropy of the prediction by finding a semantic consensus whose predictions are diverse and smooth on semantically coherent concepts without compromising accuracy. We will show in the following proposition that the Wass. barycenter allows such aggregation:

Proposition 1 (Properties of Wasserstein Barycenters). Let  be the target distribution (an oracle)

defined on a discrete space  = {x1, . . . xK , xj  Rd} (word embedding space) and µ , = 1 . . . m

be m estimates of . Assume W22(µ , )   . The Wass. barycenter µ¯w of {µ } satisfies the fol-

lowing:

1) Semantic Accuracy (Distance to an oracle). We have: W22(µ¯w, )  4

sume that W22(µ , )   , then we have: W22(µ¯w, )  4

m =1





.

m =1



W22(µ

, ).

As-

2) Diversity. The diversity of the Wass. barycenter depends on the diversity of the models with re-

spect to the Wasserstein distance (pairwise

m =k



W22(µ

, µk),



k

=

1, . . . m.

Wasserstein

distance

between

models):

W22 (µ¯w ,

µk )



3) Smoothness in the embedding space. Define the smoothness energy E () =

ij xi - xj 2 ij. We have: E (µ¯w) 

m =1



E (µ

). The Wass.

barycenter is smoother in

the embedding space than the individual models.

4) Entropy . Let H() = -

K i=1

i

log(i),

we

have:

H

(µ¯w )



m =1



H (µ

).

Proof. The proof is given in Appendix C.
We see from Proposition 1 that the Wass. barycenter preserves accuracy, but has a higher entropy than the individual models. This entropy increase is due to an improved smoothness on the embedding space: words that have similar semantics will have similar probability mass assigned in the barycenter. The diversity of the barycenter depends on the Wasserstein pairwise distance between the models: the Wass. barycenter output will be less diverse if the models have similar semantics as measured by the Wasserstein distance. The proof of proposition 1 relies on the notion of convexity along generalized geodesics of the Wasserstein 2 distance (Agueh & Carlier, 2011). Propositions 2 and 3 in Appendix C give similar results for geometric and arithmetic mean, note that the main
difference is that the guarantees are given in terms of KL and 2 respectively, instead of W2.
In order to illustrate the diversity and smoothness of the Wass. barycenter, we give here a few examples of the Wass. barycenter on a vocabulary of size 10000 words, where the cost matrix is constructed from word synonyms ratings, defined using https://www.powerthesaurus. org or using GloVe word embeddings (Pennington et al., 2014). We compute the Wass. barycenter (using Algorithm 1) between softmax outputs of 4 image captioners trained with different random seeds and objective functions. Figure 4 shows the Wass. barycenter as well as the arithmetic and geometric mean. It can be seen that the Wass. barycenter has higher entropy and is smooth along semantics (synonyms or semantics in the GloVe space) and hence more diverse than individual models. Table 1 shows top 20 words of barycenter, arithmetic and geometric means, from which we see that indeed the Wass. barycenter outputs clusters according to semantics. In order to map back the words xj that have high probability in the Wass. barycenter to an individual model , we can use the couplings  as follows: ij is the coupling between word j in the barycenter and word i in model . Examples are given in supplement in Table 8.

Controllable Entropy via Regularization. As the entropic regularization parameter  increases the distance of the kernel K from identity increases and the entropy of the optimal couplings  , (H( )) increases as well. Hence the entropy of entropic regularized Wass. Barycenter is controllable via the entropic regularization parameter . In fact since the barycenter can be written as µ¯w =  1N , one can show that (Lemma 1 in Appendix):
H(µ¯w) + H(µ )  - i,j log(ij),  = 1 . . . m,
i,j
As epsilon increases the right-hand side of the inequality increases and so does H(µ¯w). This is illustrated in Tables 2 and 7, we see that the entropy of the (entropic regularized) Wass. barycenter

5

Under review as a conference paper at ICLR 2019

increases as the distance of the kernel K to identity increases ( K - I F increases as  increases ) and the output of the Wass. barycenter remains smooth within semantically coherent clusters.

Rank 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

Wass. Barycenter

car 03.73

van 03.50

truck

03.49

vehicle 03.46

wagon 03.32

automob 03.32

coach 02.99

auto 02.98

bus 02.85

sedan 02.71

cab 02.70

wheels 02.70

buggy 02.70

motor 02.39

jeep 02.31

machine 02.30

limousi 02.27

black 01.67

white 00.85

red 00.54

Arithmetic car 45.11 fashion 04.37 truck 02.92 buildin 02.10 bus 02.00 black 01.79 train 01.73 parking 01.55 vehicle 01.49 cars 01.41 photo 01.29 red 01.26 van 01.18 white 01.04 passeng 00.92 model 00.81 city 00.73 silver 00.55 picture 00.54 style 00.50

Geometric car 41.94 truck 02.23 black 01.67 train 01.51 fashion 01.49 bus 01.30 vehicle 01.14 photo 01.01 van 01.01 red 01.01 parking 00.94 buildin 00.88 cars 00.81 passeng 00.71 white 00.67 model 00.60 picture 00.49 silver 00.47 style 00.43 city 00.38

Model 1 car cars parking vehicle model train truck buildin black van fashion suv automob parked picture bus photo suitcas broken passeng

61.37 02.79 02.62 01.93 01.75 01.26 01.22 01.17 01.04 01.04 00.82 00.69 00.67 00.57 00.55 00.48 00.47 00.44 00.40 00.39

Model 2 car cars white black train passeng model photo truck red silver vehicle van buildin bus yellow style picture blue photogr

62.25 03.16 02.22 01.95 01.68 01.33 01.24 01.21 01.15 01.15 01.03 00.78 00.75 00.71 00.70 00.69 00.67 00.67 00.59 00.44

Model 3 car fashion black truck red photo parking city train buildin fashion bus style time old picture traffic light vehicle photogr

33.25 18.15 03.08 02.29 01.88 01.57 01.52 01.41 01.30 00.74 00.72 00.71 00.69 00.67 00.58 00.49 00.47 00.45 00.45 00.43

Model 4 car truck bus vehicle red van fashion passeng pickup black train style model fire white silver classic cars photo colored

46.88 07.74 04.78 03.46 02.20 01.93 01.74 01.56 01.37 01.29 00.79 00.68 00.59 00.57 00.52 00.46 00.44 00.44 00.43 00.42

Table 1: Sample output (top 20 words) of Wass. barycenter (Algorithm 1), arithmetic and geometric means for softmaxes generated from four captioner models. Each column shows a word and a corresponding probability over the vocabulary (shown as a percentage).

Rank 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

109.7 car 10.51 truck 10.30 vehicle 09.73 auto 08.46 machine 08.17 black 01.67 fashion 01.49 red 01.06 white 00.98 parking 00.94 van 00.91 cars 00.81 coach 00.73 photogr 00.64 photo 00.57 bus 00.52 traffic 00.46 exchang 00.44 pickup 00.37 parked 00.33

79.8 car 12.79 vehicle 10.24 truck 09.16 auto 08.82 machine 06.17 black 01.67 fashion 01.49 red 01.05 van 00.99 parking 00.94 white 00.91 cars 00.81 bus 00.69 coach 00.67 photo 00.63 photogr 00.59 traffic 00.42 pickup 00.37 exchang 00.34 parked 00.33

59.4 car 15.52 vehicle 10.48 auto 08.87 truck 07.96 machine 04.33 black 01.67 fashion 01.49 van 01.06 red 01.04 parking 00.94 bus 00.88 white 00.85 cars 00.81 photo 00.69 coach 00.61 photogr 00.55 traffic 00.38 passeng 00.37 pickup 00.37 silver 00.35

43.3 car 19.22 vehicle 10.26 auto 08.42 truck 06.59 machine 02.55 black 01.67 fashion 01.49 van 01.12 bus 01.11 red 01.03 parking 00.94 cars 00.81 white 00.79 photo 00.77 coach 00.55 photogr 00.49 passeng 00.45 buildin 00.41 silver 00.40 pickup 00.37

15.8 car 33.60 vehicle 05.64 auto 03.45 truck 03.31 black 01.67 bus 01.54 fashion 01.49 van 01.08 red 01.01 photo 00.96 parking 00.94 cars 00.81 train 00.81 buildin 00.72 white 00.68 passeng 00.67 silver 00.47 pickup 00.37 photogr 00.36 old 00.33

0.25 car 41.94 truck 02.23 black 01.67 train 01.51 fashion 01.49 bus 01.30 vehicle 01.14 photo 01.01 van 01.01 red 01.01 parking 00.94 buildin 00.88 cars 00.81 passeng 00.71 white 00.67 model 00.60 picture 00.49 silver 00.47 style 00.43 city 00.38

Table 2: Controllable Entropy of regularized Wasserstein Barycenter (Algorithm 1). Output (top 20
words) for a synonyms-based similarity matrix K under different regularization , which controls the distance of K to identity K - I F . Each column shows a word and a corresponding probability over the vocabulary. As  decreases, K - I F also decreases, and the entropy of the output of Algorithm 1 decreases. Note that the last column coincides with the output from geometric mean
(for K = I, the Algorithm 1 coincides with geometric mean).

5 RELATED WORK

Wasserstein Barycenters in Machine Learning. Optimal transport is a relatively new comer to the machine learning community. The entropic regularization introduced in (Cuturi, 2013) fostered many applications and computational developments. Learning with a Wasserstein loss in a multilabel setting was introduced in (Frogner et al., 2015), representation learning via the Wasserstein discriminant analysis followed in (Flamary et al., 2016). More recently a new angle on generative adversarial networks learning with the Wasserstein distance was introduced in (Arjovsky et al., 2017; Genevay et al., 2017; Salimans et al., 2018). Applications in NLP were pioneered by the work on Word Mover Distance (WMD) on word embeddings of (Kusner et al., 2015). Thanks to new algorithmic developments (Cuturi & Doucet, 2014; Benamou et al., 2015) Wass. barycenters have been applied to various problems : in graphics (Solomon et al., 2015), in clustering (Ye et al., 2017), in dictionary learning (Schmitz et al., 2018), in topic modeling (Xu et al., 2018), in bayesian averaging (Rios et al., 2018), and in learning word and sentences embeddings (Muzellec & Cuturi, 2018; Pal Singh et al., 2018) etc. Most of these applications of Wass. barycenter focus on learning balanced barycenters in the embedding space (like learning the means of the clusters in clustering), in our ensembling application we assume the embeddings given to us (such as GloVe word embedding ) and compute the barycenter at the predictions level.

6

Under review as a conference paper at ICLR 2019

Incorporating Side Information in Classification. Incorporating side information such as knowledge graphs or word embeddings in classification is not new and has been exploited in diverse ways at the level of individual model training via graph neural networks (Marino et al., 2017; Deng et al., 2014), in the framework of Wass. barycenter we use this side information at the ensemble level.

6 APPLICATIONS

In this Section we evaluate Wass. barycenter ensembling in the problems of attribute-based classification, multi-label prediction and in natural language generation in image captioning.

6.1 ATTRIBUTE BASED CLASSIFICATION

As a first simple problem we study object classification based on attribute predictions. We use Animals with Attributes (Xian et al., 2017) which has 85 attributes and 50 classes. We have in our experiments 2 attributes classifiers to predict the absence/presence of each of the 85 attributes independently, based on (1) resnet18 and (2) resnet34 input features while training only the linear output layer (following the details in Section 6.2). We split the data randomly in 30322 / 3500 / 3500 images for train / validation / test respectively. We train the attribute classifiers on the train split.
Based on those two attributes detectors we would like to predict the 50 categories using unbalanced Wass. barycenters using Algorithm 2. Note that in this case the source domain is the set of the 85 attributes and the target domain is the set of 50 animal categories. For Algorithm 2 we use a column-normalized version of the binary animal/attribute matrix as K matrix (85 × 50), such that per animal the attribute indicators sum to 1. We selected the hyperparameters  = 0.3 and  = 2 on the validation split and report here the accuracies on the test split.
As a baseline for comparison, we use arithmetic mean (µ¯a) and geometric mean (µ¯g) ensembling of the two attribute classifiers resnet18 and resnet34. Then, using the same matrix K as above, we define the probability of category c (animal) as p(c|µ) = K µ¯ (for µ¯ = µ¯a and µ¯g resp.). We see from Table 3 that Wass. barycenter outperforms arithmetic and geometric mean on this task and shows its potential in attribute based classification.

resnet18 alone resnet34 alone Arithmetic Geometric Wass. Barycenter

Validation Accuracy 0.7771 0.8280 0.8129 0.8123 0.8803

Test Accuracy 0.7714 0.8171 0.8071 0.8060 0.8680

Table 3: Attribute based classifcation using Wass. barycenter ensembling.

6.2 MULTI-LABEL PREDICTION
For investigating Wass. barycenters on a multi-label prediction task, we use MS-COCO with 80 objects categories. MS-COCO is split into training (82K images), test (35K), and validation (5K) sets, following the Karpathy splits used in the community. From the training data, we build a set of 8 models using `resnet18' and `resnet50' architectures. To ensure some diversity, we start from pretrained models from either ImageNet or Places365. Each model has its last fully-connected (`fc') linear layer replaced by a linear layer allowing for 80 output categories. All these pretrained models are fine-tuned with some variations: The `fc' layer is trained for all models, some also fine-tune the rest of the model, while some fine-tune only the `layer4' of the ResNet architecture. These variations are summarized in Table 4. Training of the `fc' layer uses a 10-3 learning rate, while all fine-tunings use 10-6 learning rate. All multi-label trainings use ADAM with (1 = 0.9, 2 = 0.999) for learning rate management and are stopped at 40 epochs. Only the center crop of 224224 of an input image is used once its largest dimension is resized to 256.
Evaluation Metric. We use the mean Average Precision (mAP) which gives the area under the curve of P = f (R) for precision P and recall R, averaged over each class. mAP performs a sweep of

7

Under review as a conference paper at ICLR 2019

Architecture resnet18 resnet50

Pretraining
ImageNet Places365 ImageNet Places365

fc only r18.img.fc r18.plc.fc r50.img.fc r50.plc.fc

Training fc + fine-tuning fc + layer4 fine-tuning r18.img.fc+ft r18.plc.fc+ft -r50.plc.fc+ft r50.plc.fc+ft4

Table 4: Description of our 8 models built on MS-COCO

the threshold used for detecting a positive class and captures a broad view of a multi-label predictor performance. Performances for our 8 models are reported in Table 5. Precision, Recall and F1 for micro/macro are given in Table 9. Our individual models have reasonable performances overall.

rrrrRRRRR1555eeeee0080sssss....ppppNNNNNlllleeeeeccccttttt....-----ffff111ccccSS000+++RRfff711ttt-NN[[s4-[11ea]]t2t]m [[22]] r18.plc.fc rr5108..iimmgg..ffcc+ft r18.img.fc AGWreaitossh.membteratiirccycmemeentaaennr

Model mAP 59.8 59.5 60.1 61.8 62.0 61.4 61.6 58.3 52.3 49.6 64.1 63.3 58.1 64.5 63.9 65.1
Table 5: Our multi-label models performances compared to published results on MS-COCO test set. Arithmetic, geometric means and Wass. barycenter performances are reported as well. [1] (He et al., 2015) [2] (Zhu et al., 2017)

Arithmetic and geometric means offer direct improvements over our 8 individual models in mAP.

For unbalanced Wass. barycenter, the transport of probability mass is completely defined by its

matrix K = K in Algorithm 2. We investigated multiple K matrix candidates by defining K(i, j)

as (i) the pairwise GloVe distance between categories, (ii) pairwise visual word2vec embeddings

distance, (iii) pairwise co-occurence counts from training data. In our experience, it is challenging

to find a generic K that works well overall. Indeed, Wass. barycenter will move mass exactly as

directed by K. A generic K from prior knowledge may assign mass to a category that may not

be present in some images at test time, and get harshly penalized by our metrics. A successful

approach is to build a diagonal K for each test sample based on the top-N scoring categories from

each model and assign the

average

of

model posteriors

scores

K(i, i)

=

1 M

m pm(i|x) for image x

and category i. If a category is not top scoring, a low K(i, i) =  value is assigned to it, diminishing

its contribution. It gives Wass. barycenter the ability to suppress categories not deemed likely to be

present, and reinforce the contributions of categories likely to be. This simple diagonal K gives our

best results when using the top-2 scoring categories per model (the median number of active class

in our training data is about 2) and outperforms arithmetic and geometric means as seen in Table 5.

In all our experiments, Wass. barycenters parameters {, } in Algorithm 2 and  defined above

were tuned on validation set (5K). We report results on MS-COCO test set (35K). In this task of

improving our 8 models, Wass. barycenter offers a solid alternative to commonly used arithmetic

and geometric means.

6.3 IMAGE CAPTIONING
In this task the objective is to find a semantic consensus by ensembling 5 image captioner models. The base model is an LSTM-based architecture augmented with the attention mechanism over the image. In this evaluation we selected captioners trained with cross entropy objective as well as GAN-trained models (Dognin et al., 2018). The training was done on COCO dataset (Lin et al., 2014) using data splits from (Karpathy & Li, 2015): training set of 113k images with 5 captions each, 5k validation set, and 5k test set. The size of the vocabulary size is 10096 after pruning words with counts less than 5.
The matrix K = K in Algorithm 1 was constructed using word similarities, defined based on (i) GloVe word embeddings and (ii) synonym relationships. The model prediction µ , for = 1, . . . , 5 was selected as the softmax output of the captioner's LSTM at the current time step, and each model's input was weighted equally:  = 1/m. Once the barycenter p was computed, the result was fed into a beam search (beam size B = 5), whose output, in turn, was then given to the captioner's

8

Under review as a conference paper at ICLR 2019
LSTM and the process continued until a stop symbol (EOS) was generated. In order to exploit the controllable entropy of Wass. barycenter via the entropic regualrization parameter , we also decode using randomized Beam search of (Shao et al., 2017), where instead of maintaining the top k values, we sample D candidates in each beam. The smoothness of the barycenter in semantic clusters and its controllable entropy promotes diversity in the resulting captions. We baseline the Wass. barycenter ensembling with arithmetic and geometric means.
Controllable entropy and diversity. Figures 1 and 2 show the comparison of the ensembling methods on the validation set using topK and randomized beam search. The x-axis shows K -I F , which corresponds to a different regularization  (varied form 1 to 50). We report two n-gram based metrics: CIDEr and SPICE scores,as well as the WMD (Word Mover Distance) similarity (Kusner et al., 2015), which computes the earth mover distance (the Wasserstein distance) between the generated and the ground truth captions using the GloVe word embedding vectors.
In topK beam search, as  increases, causing the entropy to go up, the exact n-grams matching metrics, i.e., CIDEr and SPICE, deteriorate while WMD remains stable. This indicates that while the barycenter-based generated sentences do not match exactly the ground truth, they still remain semantically close to it (by paraphrasing), as indicated by the stability of WMD similarity. The results of the GloVe-based barycenter on the test split of COCO dataset are shown in Table 6.
In randomized beam search, the increase in entropy of the barycenter leads to a similar effect of paraphrasing but this works only up to a smaller value of , beyond which we observe a significant deterioration of the results. At that point all the words become neighbors and result in a very diffused barycenter, close to a uniform distribution. This diffusion effect is smaller for the synonyms-based K since there are only a certain number of synonyms for each word, thus the maximum neighborhood is limited.
Robustness of Wass. Barycenter to Semantic Perturbations Finally, the right panel of Figure 2, shows the robustness of the Wass. barycenter to random shuffling of the µ values, within semantically coherent clusters. Note that the size of those clusters increases as K moves away from identity. The results show that barycenter is able to recover from those perturbations, employing the side information from K, while both the arithmetic and geometric means (devoid of such information) are confused by this shuffling, displaying a significant drop in the evaluation metrics.

Figure 1: Comparison of the ensembling methods on COCO validation set using GloVe-based similarity matrix K for 2 versions of beam search: topK (left panel) and randomized (right panel).

CIDEr SPICE Entropy WMD

Barycenter K - I F = 48.5 ( = 8) 1.091 0.209 6.94 0.198

Geometric 1.119 0.203 1.33 0.200

Arithmetic 1.117 0.205 1.87 0.200

Table 6: Performance of GloVe-based Wass. barycenter on test split of COCO dataset using topK beam search versus Geometric and Arithmetic means.

Human Evaluation We performed human evaluation on Amazon MTurk on a challenging set of images out of context of MS-COCO (Dognin et al., 2018). We compared three ensembling

9

Under review as a conference paper at ICLR 2019

Figure 2: Left: Comparison of the ensembling methods on COCO validation set using synonymsbased similarity matrix and topK beam search; Center: Ensembling using synonyms and randomized beam search; Right: Comparison of ensembling methods when the predictions of input models are shuffled.

Percentage Scores

Best Caption Choice

100 80 29

33

60 34
40

32

20 37

35

0 correctness detailedness

Barycenter Geometric Arithmetic

Mean Opinion Scores

3.29 3 2.95

3.24 2.9

3.29 2.8

2
Barycenter Geometric Arithmetic correctness detailedness

Figure 3: Human evaluation results. Left: Percentage of human picking best captions in terms of correctness and detail. Right: Mean Opinion Score on a scale 1 to 5.

techniques: arithmetic, geometric and Wass. barycenter. For Wass. barycenter we used the similarity matrix K defined by visual word2vec (Kottur et al., 2016). For the three models we use randomized beam search. We asked MTurkers to give a score for each caption on a scale 1-5 and choose the best captions based on correctness and detailedness. Examples of those captions are given in supplement in Figure 5. We see in Figure 3 that Wass. barycenter has an advantage over the basic competing ensembling techniques.
7 CONCLUSION
We showed in this paper that Wass. barycenters are effective in model ensembling in machine learning. In the unbalanced case we showed their effectiveness in attribute based classification, as well as in improving the accuracy of multi-label classification. In the balanced case, we showed that they promote diversity and improve natural language generation by incorporating the knowledge of synonyms or word embeddings.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. SIAM J. Math. Analysis, 43, 2011.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. Arxiv, 2017.
Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyre´. Iterative bregman projections for regularized transportation problems. SIAM J. Scientific Computing, 2015.
Leo Breiman. Bagging predictors. Machine learning, 24(2):123­140, 1996.
Lena¨ic Chizat, Gabriel Peyre´, Bernhard Schmitzer, and Franc¸ois-Xavier Vialard. Scaling algorithms for unbalanced optimal transport problems. Math. Comput., 2018.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, pp. 2292­2300, 2013.
Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In Proceedings of the 31st International Conference on Machine Learning, 2014.
Jia Deng, Nan Ding, Yangqing Jia, Andrea Frome, Kevin Murphy, Samy Bengio, Yuan Li, Hartmut Neven, and Hartwig Adam. Large-scale object classification using label relation graphs. In European Conference on Computer Vision, 2014.
Pierre L Dognin, Igor Melnyk, Youssef Mroueh, Jarret Ross, and Tom Sercu. Improved image captioning with adversarial semantic alignment. arXiv preprint arXiv:1805.00063, 2018.
Re´mi Flamary, Marco Cuturi, Nicolas Courty, and Alain Rakotomamonjy. Wasserstein discriminant analysis. arXiv preprint arXiv:1608.08063, 2016.
Yoav Freund and Robert Schapire. A short introduction to boosting. Journal-Japanese Society For Artificial Intelligence, 14(771-780):1612, 1999.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learning with a wasserstein loss. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28. 2015.
Aude Genevay, Gabriel Peyre´, and Marco Cuturi. Learning generative models with sinkhorn divergences. Technical report, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.
Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015.
Satwik Kottur, Ramakrishna Vedantam, Jose´ M. F. Moura, and Devi Parikh. Visualword2vec (visw2v): Learning visually grounded word embeddings using abstract scenes. In CVPR, pp. 4985­ 4994. IEEE Computer Society, 2016.
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In Proceedings of the 32nd International Conference on Machine Learning, 2015.
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. EECV, 2014.
Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using knowledge graphs for image classification. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 20­28. IEEE Computer Society, 2017.
B. Muzellec and M. Cuturi. Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions. ArXiv e-prints, 2018.
11

Under review as a conference paper at ICLR 2019
S. Pal Singh, A. Hug, A. Dieuleveut, and M. Jaggi. Wasserstein is all you need. ArXiv e-prints, 2018.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, 2014.
Gabriel Peyre´ and Marco Cuturi. Computational optimal transport. Technical report, 2017. G. Rios, J. Backhoff-Veraguas, J. Fontbona, and F. Tobar. Bayesian Learning with Wasserstein
Barycenters. ArXiv e-prints, 2018. Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving gans using optimal
transport. 2018. Filippo Santambrogio. Optimal transport for applied mathematicians. May 2015. Morgan A. Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Maurice Ngole` Mboula, David Coeur-
jolly, Marco Cuturi, Gabriel Peyre´, and Jean-Luc Starck. Wasserstein dictionary learning: Optimal transport-based unsupervised nonlinear dictionary learning. SIAM J. Imaging Sciences, 2018. Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. Generating high-quality and informative conversation responses with sequence-to-sequence models. arXiv preprint arXiv:1701.03185, 2017. Justin Solomon, Fernando De Goes, Gabriel Peyre´, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric domains. ACM Transactions on Graphics (TOG), 34(4):66, 2015. Ce´dric Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften. Springer, 2008. David H Wolpert. Stacked generalization. Neural networks, 5(2):241­259, 1992. Xi-Zhu Wu and Zhi-Hua Zhou. A unified view of multi-label performance measures. CoRR, abs/1609.00288, 2016. Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly. CVPR, 2017. H. Xu, W. Wang, W. Liu, and L. Carin. Distilled Wasserstein Learning for Word Embedding and Topic Modeling. ArXiv e-prints, 2018. Jianbo Ye, Panruo Wu, James Z. Wang, and Jia Li. Fast discrete distribution clustering using wasserstein barycenter with sparse support. Trans. Sig. Proc., 2017. Yoav Zemel and Victor Panaretos. Frechet means in wasserstein space theory and algorithms. 2017. Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and Xiaogang Wang. Learning spatial regularization with image-level supervisions for multi-label image classification. CoRR, abs/1702.05891, 2017.
12

Under review as a conference paper at ICLR 2019

A IMAGE CAPTIONING
In this Section we provide additional results for evaluating the Wass. barycenter on image captioning task. Figure 4 (which corresponds to Table 1 of the main paper), visualizes the word distribution of the considered ensembling methods for the input of 4 models. It is clear that with a proper choice of similarity matrix K, Wass. barycenter can create diverse, high entropy outputs.

Model 1
ttbbrrlcpuuaiailrccndskkkii5n1n68gg72 car 0 vehicle 3 van 9 model 4

Model 2
trtprbrcewlhuadhaioicrcttnsk9koe418372 car 0 passenger 5 model 6

Model 3
tbrctpprirbuetalhaiuilrdyaockndtcii4k7oknn83gg5269 car 0 fashioned 1

Model 4
rblebatducrsk4uc29k 1 car 0

0.5
0.0 0
0.5
0.0 0
0.5
0.0 0
0.5
0.0 0 0.04 0.02 0.00
0 0.50 0.25 0.00 0 0.50 0.25 0.00 0

vehicle 8 fashioned 1

vehicle 6 van 8 fashioned 4

motor 1v3ehicle 3 van 1
machine 15 wheelwsag1o1n 4

passenger 7 vehicle 3 van 5 fashioned 6 pickup 8

coach 6

500 1000 1500 2000 2500 3000 500 1000 1500 2000 2500 3000 500 1000 1500 2000 2500 3000 500 1000 1500 2000 2500 3000 500 1000 1500 2000 2500 3000 500 1000 1500 2000 2500 3000 500 1000 W1o5rd00ID 2000 2500 3000

cbaubgg1y012 jeep 14

automobile 5

Barycenter
redwh1it9eb1l8ack 17bus t8rcucark02

Geometric
rtptbbrelrhudauisoctcn95kko3217 car 0

Arithmetic
tcpbtbrlrauariiuslsckcnidk4ik9n6n5g2g73 car 0

Figure 4: Visualization of the word distributions of Wass. barycenter, arithmetic and geometric means for the input from four models.

Table 7 shows the effect of entropic regularization  on the resulting distribution of the words of Wass. barycenter using GloVe embedding matrix. As K moves closer to the identity matrix, the entropy of barycenter decreases, leading to outputs that are close/identical to the geometric mean.
In Table 8 we show a mapping from a few top words in the barycenter output (for similarity matrix K based on synonyms) to the input models. In other words, each column defines the words in the input models which have the greatest influence on each of top 3 words in the barycenter output.
In Figure 5 we present a few captioning examples showing qualitative difference between the considered ensembling techniques.
13

Under review as a conference paper at ICLR 2019

Rank 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

48.5 ( = 10)

car 08.09

cars 00.99

vehicle 00.63

truck

00.60

van 00.40

automob 00.38

black 00.26

bus 00.26

parking 00.24

vehicle 00.23

passeng 00.21

train 00.20

auto 00.19

driving 00.19

photo 00.17

suv 00.16

red 00.14

white 00.14

taxi 00.13

pickup 00.11

39.2 ( = 9)

car 13.63

cars 01.36

truck

00.88

vehicle 00.86

van 00.59

automob 00.45

black 00.44

bus 00.39

parking 00.37

train 00.32

passeng 00.32

vehicle 00.27

photo 00.27

red 00.21

white 00.21

auto 00.21

suv 00.21

driving 00.21

model 00.17

pickup 00.17

27.8 ( = 7)

car 30.39

cars 01.85

truck

01.81

vehicle 01.30

black 01.12

van 00.94

train 00.88

bus 00.83

parking 00.79

photo 00.65

passeng 00.59

red 00.53

white 00.49

automob 00.44

model 00.35

buildin 00.35

silver 00.32

pickup 00.30

vehicle 00.29

suv 00.29

21.7 ( = 5) car 40.43 truck 02.30 black 01.60 cars 01.52 train 01.46 vehicle 01.31 bus 01.20 van 01.01 parking 00.98 photo 00.96 red 00.90 fashion 00.85 white 00.75 buildin 00.72 passeng 00.70 model 00.54 silver 00.45 city 00.44 picture 00.38 style 00.37

15.9 ( = 3) car 41.87 truck 02.22 black 01.67 train 01.51 fashion 01.44 bus 01.30 vehicle 01.14 photo 01.01 van 01.01 red 01.00 cars 00.95 parking 00.94 buildin 00.88 passeng 00.71 white 00.70 model 00.59 picture 00.48 silver 00.47 style 00.43 city 00.38

4.2 ( = 1) car 41.94 truck 02.23 black 01.67 train 01.51 fashion 01.49 bus 01.30 vehicle 01.14 photo 01.01 van 01.01 red 01.01 parking 00.94 buildin 00.88 cars 00.81 passeng 00.71 white 00.67 model 00.60 picture 00.49 silver 00.47 style 00.43 city 00.38

Table 7: Sample output (top 20 words) of barycenter for different similarity matrices K based on GloVe (columns titles denote the distance of K from identity K - I F and corresponding .). Each column shows a word and its corresponding probability over the vocabulary. Note that the last
column coincides with the output from geometric mean.

Word car jeep white

Model 1 car vehicle van automobile bus car automobile jeep motorcycle limousine silver white snowy pale blank

90.00 5.16 1.62 0.92 0.85 97.89 1.30 0.51 0.27 0.02 53.11 46.49 0.30 0.06 0.04

Model 2 car vehicle bus van truck car automobile jeep motorcycle cab white silver snowy pale blank

95.28 1.33 1.26 0.93 0.75 99.46 0.38 0.08 0.07 0 95.61 4.37 0.02 0 0

Model 3 car truck bus vehicle van car motorcycle jeep cab automobile white snow silver pale blank

84.96 9.08 2.74 1.41 0.88 97.60 1.72 0.28 0.23 0.16 88.27 6.63 4.66 0.24 0.2

Model 4 car truck vehicle bus van car motorcycle jeep cab automobile white silver snowy pale ivory

53.93 20.67 10.75 10.01 3.86 97.46 1.28 0.64 0.46 0.16 82.68 17.18 0.12 0.01 0.01

Table 8: Mapping from a few top words in the barycenter output (for similarity matrix K based on synonyms) to the input models. For each word in the left columns, the remaining columns show the contributing words and the percent of contribution.

B MULTI-LABEL PREDICTION

We evaluate our models using micro and macro versions of precision, recall, and F1-measure as

covered in multi-label prediction metrics study from (Wu & Zhou, 2016). For these measures,

a threshold of 0.5 is commonly used to predict a label as positive in the community's published

results. Macro precision is an average of per-class precisions while micro precision is computed by

computing the ratio of all true positives across all image samples over the number of all positive

classes

in

a

dataset.

Therefore

a

macro

(or

per-class)

precision

`P-C'

is

defined

as

1 C

i Pi while a

micro (or overall precision) `P-O' is defined as

i T Pi i T Pi+F Pi

where

T Pi

and

F Pi

are

true and

false

positives respectively. Per-class and overall versions for R and F1 are defined similarly. We also

employ mean Average Precision (mAP) which gives the area under the curve of P = f (R) averaged

over each class. Unlike P,R and F1, mAP inherently performs a sweep of the threshold used for

detecting a positive class and captures a broader view of a multi-label predictor's performance.

Performances for our 8 models and previously published results are reported in Table 9 and in

Table 5 in the paper. Our models have reasonable performances overall.

14

Under review as a conference paper at ICLR 2019

BA: a television is placed on the curb of the road AM: a TV sits on the side of a street GM: a television sitting on the side of a street GT: an empty sidewalk with an abandoned television sitting alone

BA: a car that is parked at the station AM: a car that has been shown in a subway GM: a car that is sitting on the side of a road GT: a car at the bottom of the stair well

BA: a person is sitting on the sidewalk with a tent

BA: a sheep sitting in a car looking out the window

AM: a couple of people sitting on benches next to a building

AM: a white sheep is sitting in a vehicle

GM: a couple of people sitting on the side of a street

GM: a close up of a sheep in a car

GT: a woman is sitting with a guitar near a man that is sitting on the GT: a sheep sitting at the steering wheel of a car with its hooves on the

ground in front of a tent

wheels

Figure 5: Examples of captions for several images. BA: Wasserstein Barycenter, AM: Arithmetic mean, GM: Geometric mean, GT: Ground truth.

Model

mAP F1-C P-C R-C F1-O P-O R-O

ResNet-101 [1]

59.8 55.7 65.8 51.9 72.5 75.9 69.5

ResNet-107 [2]

59.5 55.6 65.4 52.2 72.6 75.5 70.0

ResNet-101-sem [2] 60.1 54.9 69.3 48.6 72.6 76.9 68.8

ResNet-SRN-att [2] 61.8 56.9 67.5 52.5 73.2 76.5 70.1

ResNet-SRN [2]

62.0 58.5 65.2 55.8 73.4 75.5 71.5

r50.plc.fc+ft4

61.4 54.6 74.9 43.0 63.4 81.3 51.9

r50.plc.fc+ft

61.6 55.5 74.2 44.4 64.1 80.7 53.2

r18.plc.fc+ft

58.3 51.0 71.9 39.5 61.2 80.0 49.6

r50.plc.fc

52.3 43.8 71.3 31.6 55.6 79.8 42.7

r18.plc.fc

49.6 40.7 68.9 28.9 54.1 78.8 41.2

r18.img.fc+ft

64.1 57.9 76.7 46.4 64.3 80.9 53.4

r50.img.fc

63.3 55.9 78.7 43.4 62.2 83.1 49.7

r18.img.fc

58.1 50.7 75.7 38.1 58.4 80.5 45.8

Table 9: Our multi-label models performances compared to published results on MS-COCO test set. Arithmetic, geometric means and Wass. barycenter performances are reported as well. [1] (He et al., 2015) [2] (Zhu et al., 2017)

15

Under review as a conference paper at ICLR 2019

C ADDITIONAL THEORECTICAL RESULTS AND PROOFS
Proposition 2 (propreties of Geometric Mean). The following properties hold for geometric mean:
1. Geometric mean is the Frechet mean of KL. The geometric mean is the Frechet mean with respect to the extended KL divergence:
m
µ¯g = m=1(µ ) = arg min L() :=  KL(, µ )
 =1

2. Correctness guarantee of Geometric mean in KL. Let  be an oracle the geometric mean satisfies:
m
KL(, µ¯g)   KL(, µ ),
=1

Proposition 3 (properties of Arithmetic Mean). The following properties hold for geometric mean:

1. Arithmetic mean is the Frechet mean of 2.

mm

µ¯a =  µ = arg min L() :=   - µ 2

=1 

=1

2. Correctness guarantee of Arithmetic mean in 2. Let  be an oracle the arithmetic mean satisfies:
m
|| - µ¯g||22  2   - µ 2 .
=1

3. Entropy : Strong convexity of negative entropy with respect to 1 of arithmetic mean:

H(

 µ )

m

1  H(µ ) +

2

 k

µ

- µk

2 1

.

=1 =k

Proof of Proposition 2. 1)

m Nm

min L() :=  KL(, µ ) =





=1 i=1 =1

i log

i µ ,i

- i + µ ,i

First order optimality condition:

L m =  log

i

= 0,

i =1

µ ,i

This gives us the result:

i = m=1(µ ,i) .

16

Under review as a conference paper at ICLR 2019

2)

KL(, µ¯g) = = = = = 

(i log
i

i m=1(µ ,i)

- i + m=1(µ ,i) )

(i log
i

m=1

(

i µ ,i

)

m
- i + m=1(µ ,i) ) using  = 1
=1

m

=1

i

i

log(

i µ ,i

)

-

i

+ m=1(µ ,i) )

m

=1

i

i

log( i µ ,i

)

-

i

+

µ

,i

+
i

m
m=1(µ ,i) -  µ ,i
=1

m
 KL(, µ ) +

m
m=1(µ ,i) -  µ ,i

=1 i

=1

m

 KL(, µ ),

=1

The last inequality follows from the arithmetic-geometric mean inequality (Jensen inequality):

m
m=1(µ ,i) -  µ ,i  0.
=1

3)
m
KL(, µ¯g) -  KL(, µ )  µ¯g - µ¯a 1
=1

 - µ¯g - µ¯a 1  KL(, µ¯g)  u + µ¯g - µ¯a 1

Proof of Proposition 1. 1) By the triangle inequality we have for all :

W2(µ¯w, )  W2(µ¯w, µ ) + W2(µ , )

Raising to the power 2 we have:

W22(µ¯w, )  (W2(µ¯w, µ ) + W2(µ , ))2  2(W22(µ¯w, µ ) + W22(µ , )),

where we used (a + b)2  2(a2 + b2). Summing over all , and using

m =1



= 1 we have:

W22(µ¯w, )  2

mm
 W22(µ¯w, µ ) +  W22(µ , )
=1 =1

mm
 2  W22(, µ ) +  W22(µ , )
=1 =1 mm
( By Barycenter definition  W22(µ¯w, µ )   W22(, µ ).)
=1 =1 m
= 4  W22(, µ )(Using the symmetry of W2).
=1

2) For a fixed base distribution µ the functional W22(., µ) is convex along generalized geodesics (Santambrogio, May 2015). Given two distributions 1, 2, and T1 and T2, such that T1,#µ = 1 and T2,#µ = 2, we have:
W22((tT1 + (1 - t)T2)µ, µ)  tW22(1, µ) + (1 - t)W22(2, µ).

17

Under review as a conference paper at ICLR 2019

Fix k  [[1, m]]. Note that there exists T k, = 1 . . . m, such that µ¯w = T k,#µk = µ . Hence we have:

 T k # µk , where

 W22(µ¯w, µk) = W22 


m

 T k µk, µk   W22(T k,#µk, µk) =  W22(µ , µk).

# =1

=k

3) The smoothness energy is a valid interaction energy, by Proposition 7.7 Point 3 (for W = 2)

(Agueh & Carlier, 2011), it is convex along generalized geodesics and hence by Proposition 7.6 it is

barycenter convex:

m

E (µ¯w)   E (µ ).

=1

4) The negative entropy (-H()) is a valid internal energy, by Proposition 7.7 Point 1 (F (t) = -t log(t)) (Agueh & Carlier, 2011), it is convex along generalized geodesics and hence by Proposition 7.6 it is barycenter convex:
m
-H(µ¯w)  -  H(µ ).
=1

Remark 2 (Arithmetic mean and negative entropy). The negative entropy on discrete space is convex and moreover it is strongly convex with respect to the 1 norm hence we have:

-H (

 µ )  - m  H(µ ) - 1 2

 k

µ

- µk

2 1

,

=1 =k

hence the entropy of the arithmetic mean satisfies:

H(

 µ )

m

1  H(µ ) +

2

 k

µ

- µk

2 1

.

=1 =k

hence the diversity of the arithmetic mean depends on the 1 distance between the individual models, that does not take in count any semantics.
Remark 3. Recall strong convexity of f in Rd, there exists  > 0:

f (tx + (1 - t)y)  tf (x) + (1 - t)f (y) - (t)(t - 1)

x-y

2 2

To show something about how the entropy of barycenter depends on pairwise distances we need something on "strong geodesic convexity" or "strong barycenter convexity" of negative entropy (Conjectured here):

m
-H(µ¯w)  -  H(µ ) -   kW22(µ , µk)
=1 =k

and equivalently:

m
H(µ¯w)   H(µ ) +   kW22(µ , µk).
=1 =k

Note that this statement is true for a weaker notion of convexity that is displacement convexity and  depends on Ricci curvature.

Lemma 1 (Entropic Regularized Wass. Barycenters: Controllable entropy via ). Assume that µ are such that µ ,i > 0, for = 1 . . . m, i = 1 . . . M , we have:

NM

H(µ¯w) + H(µ )  -

ij log(ji),  = 1 . . . m

j=1 i=1

18

Under review as a conference paper at ICLR 2019

Proof.

Let 



N ×M
R+

be a coupling

between

p



M

and q



N , qj

>

0

we

have:



1N =

p and 1M = q, we have:

pj

=

N i=1

ij

=

N i=1

qi

ij qi

,

j

=

1...M

Let

ai

=

( ij
qi

)j=1...M .

Note

that

M j=1

ij qi

=

qi qi

= 1, hence ai

 M .

Hence p can be written as a

convex combination of probabilities:

N
p = qiai
i=1
Now the entropy of the convex combination is higher than convex combination of entropies (the entropy is concave):
N
H(p)  qiH(ai)
i=1

H (ai )

=

- M ij log( ij )

J=1 qi

qi



=

-1 qi

M

j=1

ij

log(ij )

-

M
log(qi)(
i=1

ij )

= -1 qi

M
ij log(ij) - log(qi)qi
i=1

.

Hence :

NM

N

NM

H(p)  -

ij log(ji) + qi log(qi) = -

ij log(ij) - H(q)

j=1 i=1

i=1

j=1 i=1

Hence :

NM

H(p) + H(q)  -

ij log(ij )

i=1 j=1

19

