Under review as a conference paper at ICLR 2019

TRAINING VARIATIONAL AUTO ENCODERS WITH DISCRETE LATENT REPRESENTATIONS USING IMPORTANCE SAMPLING
Anonymous authors Paper under double-blind review

ABSTRACT
The Variational Auto Encoder (VAE) is a popular generative latent variable model that is often applied for representation learning. Standard VAEs assume continuous valued latent variables and are trained by maximization of the evidence lower bound (ELBO). Conventional methods obtain a differentiable estimate of the ELBO with reparametrized sampling and optimize it with Stochastic Gradient Descend (SGD). However, this is not possible if we want to train VAEs with discrete valued latent variables, since reparametrized sampling is not possible. Till now, there exist no simple solutions to circumvent this problem. In this paper, we propose an easy method to train VAEs with binary or categorically valued latent representations. Therefore, we use a differentiable estimator for the ELBO which is based on importance sampling. In experiments, we verify the approach and train two different VAEs architectures with Bernoulli and Categorically distributed latent representations on two different benchmark datasets.

1 THE VARIATIONAL AUTO ENCODER
The variational auto encoder (VAE) is a generative model which it is trained to approximate the true data generating distribution p(x) of an observed random vector x from a given training set D = {x1, ..., xN } (Kingma & Welling (2013); Kingma et al. (2016)). It is an especially suited model if x is high dimensional or has highly nonlinear dependent elements. Therefore, the VAE is oftenly used for tasks like density estimation, data generation, data interpolation (White (2016)), outlier and anomaly detection (An & Cho (2015); Xu et al. (2018)) or clustering (Jiang et al. (2016); Dilokthanakul et al. (2016)).
As shown in Fig. 1, the VAE is an easy latent variable model, where the observations x  p(x|z) are dependent on latent variables z  p(z).

zx

Figure 1: The latent variable model of a VAE with latent variables z and observations x.

During training, the VAE maximizes the probability p(x) to observe the data x. Therefore, the negative evidence lower bound (ELBO)

L() = -Eq(z|x) [ln p(x|z)] + DKL(q(z|x)||p(z))  - ln p(x) + DKL(q(z|x)||p(z|x))

(1) (2)

is minimized, where p(z|x) = p(x|z)p(z)/ p(x|z)p(z)dz is the true but intractable posterior dis-
tribution the model assigns to z, q(z|x) is the corresponding tractable variational approximation
and DKL(q(z|x)||p(z|x)) is the Kullback-Leibler (KL) divergence between p(z|x) and q(z|x). Because DKL(q(z|x)||p(z|x)) > 0, minimizing L() means to maximize the probability p(x) the model assigns to observations x. Therefore, DKL(q(z|x)||p(z|x)) must be as as close as possible to 0, meaning that after training q(z|x) is a very good approximation of the true posterior p(z|x).

1

Under review as a conference paper at ICLR 2019

Kingma & Welling (2013) proposed to minimize L(), using stochastic gradient descent on a training data set, which they called Stochastic Gradient Variational Bayes (SGVB).

The VAE uses parametric distributions that are parametrized by an encoder network with parameters

E and a decoder network with parameters D for both q(z|x) and p(x|z), respectively. This leads to the well known encoder-decoder structure in Fig. 2. The data likelihood is a distribution with mean

x^, that is the output of the decoder network. Further, we assume in this paper, that the variational

posterior q(z|x) is a distribution from the exponential family

q(z|x) = exp(T (x; E)T (z) - A((x; E)))

(3)

with natural parameters (x; E), sufficient statistic T (z) and log partition function A((x; E)). This gives us the flexibility to study training with different q(z|x) in the same mathematical frame-

work. As shown in Fig. 2, the natural parameters  are the output of the encoder network, where we

drop the arguments x, E for shorter notations in the remainder of the paper.

x

 E

z D

x^

Encoder

Decoder

Figure 2: The encoder-decoder structure of a VAE. The encoder parametrizes q(z|x) as an exponential family distribution with natural parameters  and the decoder parametrizes p(x|z) with mean x^.

The conventional VAE proposed in (Kingma & Welling (2013); Kingma et al. (2016)) learns continuous latent representations z  Rc. It uses i.i.d. Gaussian distributed z, meaning that  = [µ1/12, -1/(212), ..., µc/c2, -1/(2c2)]T , T (z) = [z1, z12, ..., zc, zc2]T and A() is chosen such that q(z|x) integrates to one. The likelihood is also Gaussian, with p(x|z)  N (x^, 1). But in
many applications learning discrete rather than continuous representations is advantageous. Binary representations z  {0, 1}c can for example be used very efficiently for hashing, what is a power-
ful method for large-scale visual search (Liong et al. (2015)). Learning Categorical representations z  {e1, ..., ec} is interesting, because this naturally lead to clustering of the data x, as shown in the experiments. Further, for both binary and categorical z it is easy to find entropy based heuristics to
choose the size of the latent space, because the entropy is bounded for discrete z.

However, training VAEs with discrete latent representations is problematic, since standard SGVB

can not be applied for optimization. Because SGVB is a gradient based method, we need to calculate

the derivative of the two cost terms with respect to the encoder and decoder parameters

 

LKL()

=

 

DK

L

(q(z|x)||p(z))

(4)

 

LL()

=

 

Eq(z|x)

[ln

p(x|z)]

,

(5)

where LKL() only depends on the encoder parameters and the expected log likelihood term LL()

depends on both encoder and decoder parameters. For a suited choice of p(z) and q(z|x), LKL()

can be calculated in closed form. However, LL() contains an expectation over z  q(z|x) that has

to be estimated during training. A good estimator L^L() for LD() that is unbiased, differentiable with respect to  and that has low variance is the key to train VAEs. SGVB uses an estimator L^LR() that is based on reparametrization of q(z|x) and sampling (Kingma & Welling (2013)). However, as

described in section 2, this method places many restrictions on the form of q(z|x) and fails if q(z|x)

can not be reparametrized. This is the case if z is discrete, for example.

In this paper, we propose a simple and differentiable estimator L^LI () for LL() that is based on importance sampling. Because no reparametrization is needed, it can be used to train VAEs with bi-
nary or categorical latent representations. Compared to previously proposed methods like the Vector
Quantised-Variational Auto Encoder (VQ-VAE) (van den Oord et al. (2017)), which is based on a straight-through estimator for the gradient of LL() (Bengio et al. (2013)), our proposed estimator has two advantages. It is unbiased and its variance approaches zero the closer we are to the optimum.

2

Under review as a conference paper at ICLR 2019

2 ESTIMATING THE EXPECTED LOG-LIKELIHOOD WITH REPARAMETRIZED
SAMPLING
The standard estimator L^RL () proposed in Kingma & Welling (2013) is based on reparametrized sampling

  LL() =  Eq(z|x) [ln p(x|z)]

=

  Ep(

)

[ln p(x|z

=

f(

, ))]



1  M

M

ln p(x|z = f ( m, ))

m=1

=

 

L^LR

()

(6) (7) (8) (9)

where is a random variable with the distribution p( ), m are samples from this distribution and f ( , ) is a reparametrization function, such that z = f ( , )  q(z|x). This estimator can be used
to train VAEs with SGVB if two conditions are fulfilled: I) There exists a distribution p( ) and a reparametrization function f ( , ), such that z = f ( , )  q(z|x). II) The derivative of Eq. 5 must
exist. With Eq. 8, we obtain

 

L^LR

()

=

1 M

M

 z

ln p(x|z

=

f(

 m, ))  z,

m=1

(10)

meaning that both the reparametrization function f ( , ) and ln p(x|z) must be differentiable with respect to z and , respectively, to allow direct backpropagation of the gradient through the reparametrized sampling operator. If these conditions are fulfilled, the gradient can flow directly from the output to the input layer of the VAE, as shown in Fig. 3. Distributions over discrete latent representations z can not be reparametrized this way. Therefore, this estimator can not be used to train VAEs with such representations.

 z

L^RL ()

 

z

x

 E

z D

x^

Encoder

Decoder

Figure 3: The gradient flow through the VAE, using L^RL () based on reparametrized sampling. The gradient is propagated directly through the reparametrized sampling operator.

3

Under review as a conference paper at ICLR 2019

3 ESTIMATING THE EXPECTED LOG-LIKELIHOOD WITH IMPORTANCE
SAMPLING

We propose an estimator L^IL() which is based on importance sampling and can also be used to train VAEs with binary or categorical latent representations z. Expanding Eq. 4 leads to

 

LL()

=

 

ln p(x|z)q(z|x)dz

 q(z|x)

= 

ln p(x|z) qI (z) qI (z)dz



1  M

M m=1

ln

p(x|zm)

q(zm|x) qI (zm)

=

 

L^IL(),

(11) (12) (13) (14)

where qI (z) is an arbitrary distribution that is of the same form as q(z|x) which is independent from the parameters . zm  qI (z) are samples from this distribution. The estimator computes a weighted sum of the log likelihood ln p(x|zm) with the weighting q(zm|x)/qI (z).
The benefit is that the log likelihood ln p(x|zm) depends on the decoder parameters D only and not on the encoder parameters E whereas the weighting q(zm|x)/qI (z) depends only on D and not on E. Therefore, calculation of the gradient of L^LI () can be separated

 

L^LI ()

=

 E

L^LI (),

 D

L^IL()

,

(15)

with

 E

L^LI ()

=

1 M

M m=1

ln

p(x|zm

)



 E

q(zm|x) qI (zm)



 D

L^LI ()

=

1 M

M m=1

q(zm|x) qI (zm)

 D

ln p(x|zm).

(16) (17)

As shown in Fig.

4, gradient backpropagation is split into two separate parts.

 D

L^LI ()

back-

propagates

the

error

ln p(x|z)

from

the

output

of

the

VAE

to

the

sampling

operator

and

 E

L^LI ()

backpropagates

the

error

q(z|x) qI (z)

from

the

sampling

operator

to

the

input

layer

of

the

VAE.

ln

p(x|z)



 E

q(z|x) qI (z)

q(z|x) qI (z)

 D

ln p(x|z)

x

 E

z D

x^

Encoder

Decoder

Figure 4: Gradient flow through the VAE when using L^IL(), based on importance sampling.

Compared to L^RL (), we do not need to find a differentiable reparametrization for q(z|x), because we do not propagate the gradient through the sampling operator. Therefore, L^IL() can also be used if no reparametrization function exists for q(z|x), e.g. if it is a Bernoulli or a Categorical distribution.

4

Under review as a conference paper at ICLR 2019

4 VAE WITH BERNOULLI DISTRIBUTED z (BVAE)

Assume the latent representation z has i.i.d. components that are Bernoulli distributed, i.e. both the variational posterior distribution q(z|x) and qI (z) have the form

q(z|x) = exp(T z - A()) qI (z) = exp(T z - A()),

(18) (19)

where z  {0, 1}c,  = [ln(q1/(1 - q1)), ..., ln(qc/(1 - qc))] is the output vector of the encoder that contains the logits of the independent Bernoulli distributions and A() = 1T ln(1 + e) are the corresponding log-partition functions.

Hence, Eq. 17 is



 E

L^IL()

=

1 M

M

ln

p(x|zm

)

 

exp

( - )T zm - (A() - A())

m=1

 
E

=

1 M

M

ln p(x|zm) exp ( - )T zm - (A() - A()) (z - q)T

m=1

 ,
E

where q

=

[q1, ..., qc]T

=

1 1+e-

contains the probabilities q(zi

=

1|x).

The variance of the

estimator L^IL() heavily depends on the choice of the natural parameters  of the distribution qI (z).

We choose  = , leading to a gradient of the very simple form

 E

L^IL()

=

1 M

M
ln p(x|zm) (zm
m=1

-

q)T

 .
E

(20)

This estimator of the gradient has two desirable properties for training, which can be easily seen in the one dimensional case with z  {0, 1}. The mean of the estimator is

EqI (z)

 E

L^LI

()

=

EqI (z)

1 M

M

ln p(x|z) (zm - q)

m=1

 
E

= q(1 - q)(ln p(x|z = 1) - ln(p(x|z = 0))

 

E

=



 E

LL(),

(21) (22) (23)

meaning that the estimator is unbiased.

Further, the variance of

 E

L^LI ()

reduces

to

zero,

the

closer q is to 0 or 1, because q(1 - q)  0 and hence the variance of the estimator approaches 0.

That is desirable, since there are only three interesting cases during training that are shown in Fig. 5:

(a) ln p(x|z = 0) = ln p(x|z = 1): In this case the expected log likelihood Eq(z|x) [ln p(x|z = 0)] = ln p(x|z = 0) we want to maximize is independent of the choice of q.
(b) ln p(x|z = 0) > ln p(x|z = 1): In this case the expected log likelihood Eq(z|x) [ln p(x|z = 0)] = (1 - q) ln p(x|z = 0) + q ln p(x|z = 1) is maximized for q = 0.
(c) ln p(x|z = 0) < ln p(x|z = 1): In this case the expected log likelihood Eq(z|x) [ln p(x|z = 0)] = (1 - q) ln p(x|z = 0) + q ln p(x|z = 1) is maximized for q = 1.

This means, that the only candidate points that maximize the log likelihood are q = 0 or q = 1 lie near q(z = 1|x) = 0/1. Therefore, the longer we train, the more accurate the gradient estimate will
be.

5

Under review as a conference paper at ICLR 2019

ln p(x|z)

ln p(x|z)

ln p(x|z)

z=0

z=1 z z=0

z=1 z z=0

z=1 z

(a) (b) (c)
Figure 5: Three different cases how the log-likelihood can vary over z. The decoder network defines ln(x|z) over z  R. However, only the points z = 0 and z = 1 are of interest. (a) ln(x|z = 0) = ln(x|z = 1), meaning Eq(z|x) [ln(x|z)] is independent of q. (b) ln(x|z = 0) > ln(x|z = 1), meaning Eq(z|x) [ln(x|z)] is maximized for q = 0. (c) ln(x|z = 0) < ln(x|z = 1), meaning Eq(z|x) [ln(x|z)] is maximized for q = 1.

5 VAE WITH CATEGORICALLY DISTRIBUTED z (CVAE)

For Categorically distributed z both the variational posterior distribution q(z|x) and qI (z), again have the form

q(z|x) = exp(T z - A()) qI (z) = exp(T z - A()),

(24) (25)

but now z  {e1, ..., ec} can assume only c different values. The vector of natural parameters is  = [ln(p1/pc), ..., ln(pc-1/pc), 0)] and the log partition function is A() = ln 1T e.

With the formulas above, we arrive at the same easy form of the expected gradient of the log likeli-

hood

 E

L^LI ()

=

1 M

M
ln p(x|zm) (zm
m=1

-

q)T

 ,
E

(26)

but now with q = softmax() that consists of the probabilities qi = q(z = ei), where

c i=1

qi

=

1.

6 EXPERIMENTS

In the following section, we show our preliminary experiments on the MNIST and Fashion MNIST datasets LeCun & Cortes (2010); Xiao et al. (2017). Two different kinds of VAEs have been evaluated:
1. The BVAE with Bernoulli distributed z  {0, 1}c. 2. The CVAE with Categorically distributed z  {e1, ..., ec}.
To train both architectures, the estimator L^LI () derived in Sec. 5 is used.
Both BVAE and CVAE are tested with two different architectures given in Tab. 1. The fully connected architecture has 2 dense encoder and decoder layers. The encoder and decoder networks of the convolutional architecture consist of 4 convolutional layers and one dense layer each.
In our first experiment we train a FC BVAE with c = 50, i.e. z  {0, 1}50 and a FC CVAE with c = 100, i.e. z  {z1, ..., z100}. We train them for 300 epochs on the MNIST dataset, using

6

Under review as a conference paper at ICLR 2019

Table 1: The architectural details of the trained VAEs. FC, Conv and Conv-1 are the fully con-
nected, convolutional and deconvolutional layer, respectively. The shape of the convolutional layers is given in the form DimA × DimB × Channel/Stride/Activation.

Architecture FC BVAE or FC CVAE CNN BVAE or CNN CVAE

In/Out 784
28x28x1

Encoder
FC 1024/ReLu FC c/linear
Conv 3x3x32/2/ReLu Conv 3x3x64/2/ReLu Conv 3x3x64/2/ReLu Conv 3x3x128/2/ReLu
flatten FC c/linear

Latent

z



Rc



Ber(

1 1+e-

)

or

z



Rc



Cat(

e e1T



)

z or

 z

RcRc BeCra( t1(+e1e1e-T))

Decoder
FC 1024/ReLu
FC 784/sigmoid
FC c/ReLu
reshape Conv-1 3x3x64/2/ReLu Conv-1 3x3x64/2/ReLu Conv-1 3x3x32/2/ReLu Conv-1 3x3x1/2/sigmoid

SGVB with our proposed estimator L^IL(), to estimate the expected log likelihood, and ADAM as optimizer. Fig. 6 shows the convergence of the loss, the log likelihood the VAEs assign to the training data ln p(x|z) and the variance of the estimator L^LI (), for a learning rate of 1e - 3 and a batch size of 2048. During training, the loss decreases steadily without oscillation. We observe that the variance of the estimator L^IL() decreases the longer we train and the closer we get to the optimum. This is consistent with our theoretically considerations in Sec. 4. The results of the
corresponding simulations with the CNN BVAE and the CNN CVAE are shown in Fig. 9, in the
appendix.

loss

200 L() 180 160 140

epoch

-50 -100

log-likelihood Eq(z|x) [ln p(x|z)]

gradient variance

Var

 

L^LI

0.1

-150

5 · 10-2

epoch

100 200 300

0 100 200 300

FC BVAE

FC CVAE

100 200 300

Figure 6: Convergence of the loss, log-likelihood and gradient variance over 300 epochs of training on the MNIST dataset.

The performance of the FC CVAE is worse than the performance of the FC BVAE. Training converges to a lower log likelihood ln p(x|z), because the maximal information content HCV AE(z)  ln(100) of the latent variables of the FC CVAE is much less than the maximal information content HBV AE(z)  c ln(2) of the latent variables of the FC BVAE. The FC CVAE can at maximum learn to generate 100 different handwritten digits, what is a small number compared to the 250 different images that the FC CVAE can learn to generate.
Fig. 7 shows handwritten digits that are generated by the FC BVAE and the FC CVAE if we sample z from the variational posterior q(z|x). To draw samples from q(z|x), we feed test data which has not been seen during training to the encoders. The test data is shown in Fig. 7a and Fig. 7c. The corresponding reconstructions generated by the decoders are shown in Fig. 7b and Fig. 7d. Both input and reconstructed images are very similar in case of the FC BVAE, meaning that it can approximate the data generating distribution p(x) well. However, in case of the FC CVAE, the generated are blurry and look very different than the input of the encoder. In some cases, the class of the generated digit is even flipped. This happens because of the the very limited model capacity. Similar results for the CNN BVAE and CNN CVAE are shown in Fig. 10 in the appendix.
Fig. 8a shows generated images of the FC BVAE if we sample z  p(z) from the prior distribution. A few generated images look like templates of handwritten digits and the remaining generated images seem to resemble mixtures of different digits. This is similar to the behaviour of a VAE with continuous latent variables, where we can interpolate between or generate mixtures of different

7

Under review as a conference paper at ICLR 2019
(a) input FC BVAE (b) generated FC BVAE (c) input FC CVAE (d) generated FC CVAE Figure 7: Test input images and generated handwritten images of the FC BVAE and FC CVAE
digits by traveling through the latent space Kingma & Welling (2013). However, in comparison to VAEs with continuous latent variables, we can only generate discrete mixtures for VAEs if the latent variables z are Bernoulli. Fig. 8b shows generated images of the FC CVAE if we sample z  p(z) from the prior distribution. Since the FC CVAE can only learn to generate 100 different images, its decoder learns to generate template images that fit well to all the training images. We observe that some latent representations are decoded to meaningless patterns that just fit well to the data in avarage. However, the decoder also learned to generate at least one template image for each class of handwritten digits. Hence, the categorical latent representation can be interpreted as the cluster affiliation and the encoder of the FC CVAE automatically learns to cluster the data. Similar results for the CNN BVAE and CNN CVAE are shown in Fig. 11 in the appendix. A major drawback of the FC CVAE is, that the latent space of the FC CVAE can encode only very little information and thus its generative capabilites are poor. However, we think that they can be increased considerably if we allow a hybrid latent space with some continuous latent variables, as proposed in Chen et al. (2016). This could lead to a powerfull model for nonlinear clustering.
(a) samples FC BVAE (b) samples FC CVAE Figure 8: Handwritten digits generated by the FC BVAE and FC CVAE, if z is sampled from the prior distribution p(z).
7 CONCLUSION
In this paper, we derived an easy estimator for the ELBO, which does not rely on reparametrized sampling and therefore can be used to obtain differentiable estimates, even if reparametrization is not possible, e.g. if the latent variables z are Bernoulli or Categorically distributed. We have shown theoretically and in experiments, close to the optimal parameter configuration, the variance of the estimator approaches zero. This is a very desirable property for training.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using reconstruction probability. 2015.
Yoshua Bengio, Nicholas Le´onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL http://arxiv.org/abs/1308.3432.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. CoRR, abs/1606.03657, 2016. URL http://arxiv.org/abs/1606.03657.
Nat Dilokthanakul, Pedro A. M. Mediano, Marta Garnelo, Matthew C. H. Lee, Hugh Salimbeni, Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders. CoRR, abs/1611.02648, 2016. URL http://arxiv.org/abs/ 1611.02648.
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep embedding: A generative approach to clustering. CoRR, abs/1611.05148, 2016. URL http: //arxiv.org/abs/1611.05148.
D. P Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints, December 2013.
Diederik P. Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse autoregressive flow. CoRR, abs/1606.04934, 2016. URL http://arxiv.org/abs/1606. 04934.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann. lecun.com/exdb/mnist/.
V. E. Liong, Jiwen Lu, Gang Wang, P. Moulin, and Jie Zhou. Deep hashing for compact binary codes learning. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2475­2483, June 2015. doi: 10.1109/CVPR.2015.7298862.
Aa¨ron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. CoRR, abs/1711.00937, 2017. URL http://arxiv.org/abs/1711.00937.
Tom White. Sampling generative networks: Notes on a few effective techniques. CoRR, abs/1609.04468, 2016. URL http://arxiv.org/abs/1609.04468.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/ abs/1708.07747.
Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, Jie Chen, Zhaogang Wang, and Honglin Qiao. Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications. CoRR, abs/1802.03903, 2018. URL http://arxiv.org/abs/1802.03903.
9

Under review as a conference paper at ICLR 2019

A RESULTS FOR CNN BVAE AND CNN CVAE ON MNIST

loss 250
L()
200

-50 -100

log-likelihood Eq(z|x) [ln p(x|z)]

-150 150 epoch

100 200 300

0 100 200 300

CNN BVAE

CNN CVAE

Figure 9: Convergence of the loss and the log-likelihood over 300 epochs, while training on the MNIST dataset.

(a) input CNN BVAE (b) generated CNN BVAE (c) input CNN CVAE (d) generated CNN CVAE Figure 10: Test input images and generated handwritten images of the CNN BVAE and CNN CVAE

(a) samples CNN CVAE (b) samples CNN BVAE
Figure 11: Handwritten digits generated by the CNN BVAE and CNN CVAE, if z is sampled from the prior distribution p(z).

10

Under review as a conference paper at ICLR 2019

B RESULTS FOR CNN BVAE ON FASHION MNIST

As shown in Fig. 12, the gradient variance approaches 0, the closer we get to the optimum. This
is the same behaviour as for the MNIST dataset. As shown in Fig. 13, the FC BVAE can correctly
reconstruct the shape of the given clothes with high accuracy. However, details like texture are lost. This is due to the limited model capacity, i.e. the latent representation z  {0, 1}50 of the given VAE can at most encode 50Bits of information.

·105 2 L()
1.5
1

loss epoch

gradient variance ·10-2

8

Var

 

L^LI

6

4

2 epoch

200 400 600 800 1,000

200 400 600 800 1,000

FC BVAE

Figure 12: Convergence of the loss and gradient variance over 300 epochs of training on the fashion MNIST dataset.

(a) input FC BVAE

(b) generated FC BVAE

Figure 13: Test input images and generated images of the FC BVAE on the fashion MNIST dataset.

11

