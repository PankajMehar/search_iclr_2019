Under review as a conference paper at ICLR 2019

LAGGING INFERENCE NETWORKS AND POSTERIOR COLLAPSE IN VARIATIONAL AUTOENCODERS
Anonymous authors Paper under double-blind review

ABSTRACT
The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as "posterior collapse" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate to the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse results. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse, while being substantially faster.

1 INTRODUCTION

Variational autoencoders (VAEs) (Kingma & Welling, 2014) represent a popular combination of a
deep latent variable model (shown in Figure 1(a)) and an accompanying variational learning technique. The generative model in VAE defines a marginal distribution on observations, x  X , as:

p(x) = p(x|z)p(z)dz.

(1)

The model's generator defines p(x|z) and is typically parameterized as a complex neural network. Standard training involves optimizing an evidence lower bound (ELBO) on the intractable marginal data likelihood (Eq.1), where an auxiliary variational distribution q(z|x) is introduced to approximate the model posterior distribution p(z|x). VAEs make this learning procedure highly scalable to large datasets by sharing parameters in the inference network to amortize the cost. This amortization is in contrast to traditional variational inference that has separate local variational parameters for every data point (Hoffman et al., 2013).
While successful on some datasets, prior works has found that VAE training often suffers from "posterior collapse", in which the model ignores the latent variable z when the generator p(x|z) is parametrized using a strong autoregressive neural network such as LSTMs for text and PixelCNNs (van den Oord et al., 2016) for images (Bowman et al., 2016; Chen et al., 2017; Kingma et al., 2016; Yang et al., 2017; Dieng et al., 2018). Posterior collapse is especially evident when modeling discrete data, which hinders the usage of VAEs in important applications like natural language processing. Existing work analyze this problem from a static optimization perspective noting that the collapsed solution is often a reasonably good local optimum in terms of ELBO (Chen et al., 2017;

1

Under review as a conference paper at ICLR 2019

Approx. Posterior Params


p(z)
<latexit sha1_base64="BMlcnxo7sQRJA5Gg0pI6oINxqOE=">AAACEnicZVDLSgMxFM3UV62vqks3g0WoUMpUBHVXdOOygmML7VAyaaYNTSZjckeoQz9D3Op3uBK3/oCf4R+Ymc7CthfCPTn33HBy/IgzDY7zYxVWVtfWN4qbpa3tnd298v7Bg5axItQlkkvV8bGmnIXUBQacdiJFsfA5bfvjm3TefqJKMxnewySinsDDkAWMYDCUF1V7AsPID5Ln6Wm/XHHqTlb2MmjkoILyavXLv72BJLGgIRCOte42nAi8BCtghNNpqRdrGmEyxkPaNTDEgmovyUxP7RPDDOxAKnNCsDP2/0aChU69GWXa9NwsZUBKrmtGBSORtvSZ7K4nwq/5opaKlA70ghEILr2EhVEMNCQzH0HMbZB2GpA9YIoS4BMDMFHMfMUmI6wwARNjyWTUWExkGbhn9au6c3deaV7nYRXRETpGVdRAF6iJblELuYigR/SK3tC79WJ9WJ/W10xasPKdQzRX1vcf9sOfCw==</latexit>
z <latexit sha1_base64="pLnqAsmTFWJQSVA69c6xb03wv3M=">AAACD3icZVDLTgIxFO3gC/GFunQzkZi4IGQwJuqO6MYlJo4QgZBO6UBDH5P2jglO+AnjVr/DlXHrJ/gZ/oGdYRYCN2nu6bnnNqcniDgz4Hk/TmFldW19o7hZ2tre2d0r7x88GBVrQn2iuNLtABvKmaQ+MOC0HWmKRcBpKxjfpPPWE9WGKXkPk4j2BB5KFjKCwVKPXYFhFITJ87Rfrng1Lyt3GdRzUEF5Nfvl3+5AkVhQCYRjYzp1L4JegjUwwum01I0NjTAZ4yHtWCixoKaXZI6n7ollBm6otD0S3Iz9v5FgYVJrVpk2MzdLGVCKm6pVwUikLX0mu5uJCKqBqKYibUKzYATCy17CZBQDlWTmI4y5C8pN03EHTFMCfGIBJprZr7hkhDUmYDMs2Yzqi4ksA/+sdlXz7s4rjes8rCI6QsfoFNXRBWqgW9REPiJIolf0ht6dF+fD+XS+ZtKCk+8corlyvv8AQRieLA==</latexit>

Model Params


µx, <latexit sha1_base64="JrR8bpJT9WIDXN5JEdhmmw5BeVU=">AAACInicZVBNSwMxEM36WetX1WMvwSJ4KGUrgnorevFYwbWFbinZNNuGJpslmRXLsgd/jHjV3+FJPAn+Cf+B2bYH2w6EeXnzZph5QSy4Adf9dlZW19Y3Ngtbxe2d3b390sHhg1GJpsyjSijdDohhgkfMAw6CtWPNiAwEawWjm7zeemTacBXdwzhmXUkGEQ85JWCpXqnsy6SX+pLAMAjTp6yK/UCmfjzkWdYrVdyaOwm8DOozUEGzaPZKv35f0USyCKggxnTqbgzdlGjgVLCs6CeGxYSOyIB1LIyIZKabTo7I8Ill+jhU2r4I8IT935ESafI1rTJPZq6WM6CUMFWrgqHMUz5m8jdjGVQDWc1F2oRmYREIL7spj+IEWESne4SJwKBwbhjuc80oiLEFhGpuT8F0SDShYG0tWo/qi44sA++sdlVz784rjeuZWQVURsfoFNXRBWqgW9REHqLoGb2iN/TuvDgfzqfzNZWuOLOeIzQXzs8fgA2lpg==</latexit>

desired local
optimum O

mean of approximate posterior

q
<latexit sha1_base64="w6BvjHSyPIfpojRQkdLH6p+afXc=">AAACLXicZVBNSwMxEM36WetX1aOXYFEqlLIVQb0VvXisYFXolpJNs21oslmTWbGu+w/8MeJVf4cgiFfP/gOztYK2A2HevHkTZp4fCW7Add+cqemZ2bn53EJ+cWl5ZbWwtn5hVKwpa1AllL7yiWGCh6wBHAS7ijQj0hfs0u+fZP3LG6YNV+E5DCLWkqQb8oBTApZqF3au24nny8SLejxNS54k0POD5C7F9/i3uE1324WiW3GHgSdBdQSKaBT1duHL6ygaSxYCFcSYZtWNoJUQDZwKlua92LCI0D7psqaFIZHMtJLhPSnetkwHB0rbFwIesn8nEiJNtptVZsn862UMKCVM2aqgJ7OUfTOszUD6ZV+WM5E2gRlbBILDVsLDKAYW0p89glhgUDjzDne4ZhTEwAJCNbenYNojmlCwDuetR9VxRyZBY69yVHHP9ou145FZObSJtlAJVdEBqqFTVEcNRNEDekLP6MV5dF6dd+fjRzrljGY20L9wPr8Bh2+qSA==</latexit>

(z|x)

Inference (Encoder)

p (x|z)
<latexit sha1_base64="Wbl6/d1+NSe4D7I5NZ+LiaVjjT4=">AAACL3icZVBNSwMxEM36bf1a9eglWIQKpWxFUG+iF48VrC10S8mmWRtMNksyK9Z1/4I/Rrzq79CLePXoPzC7rWB1IMybN2/CzAtiwQ143pszNT0zOze/sFhaWl5ZXXPXNy6NSjRlTaqE0u2AGCZ4xJrAQbB2rBmRgWCt4Po077dumDZcRRcwjFlXkquIh5wSsFTPrcS91A9k6sOAAcmyii8JDIIwvc3wPf4p7rLdnlv2al4R+D+oj0EZjaPRc7/8vqKJZBFQQYzp1L0YuinRwKlgWclPDIsJvSZXrGNhRCQz3bS4KMM7lunjUGn7IsAF+3siJdLku1llnsxEL2dAKWGqVgUDmaf8m6I2QxlUA1nNRdqE5s8iEB52Ux7FCbCIjvYIE4FB4dw93OeaURBDCwjV3J6C6YBoQsF6XLIe1f868h8092pHNe98v3x8MjZrAW2hbVRBdXSAjtEZaqAmougBPaFn9OI8Oq/Ou/Mxkk4545lNNBHO5zdMW6sw</latexit>
Generator (Decoder)

x
<latexit sha1_base64="gXlW4ofy6WTdDobdCq1lEt46OQ0=">AAACD3icZVDLTgIxFO3gC/GFunQzkZi4IGTGmKg7ohuXmDhChAnplA409DFp7xjJhJ8wbvU7XBm3foKf4R/YARYCN2nu6bnnNqcnSjgz4Hk/TmFldW19o7hZ2tre2d0r7x88GJVqQgOiuNKtCBvKmaQBMOC0lWiKRcRpMxre5PPmE9WGKXkPo4SGAvclixnBYKnHjsAwiOLsedwtV7yaNyl3GfgzUEGzanTLv52eIqmgEgjHxrR9L4EwwxoY4XRc6qSGJpgMcZ+2LZRYUBNmE8dj98QyPTdW2h4J7oT9v5FhYXJrVpk3MzfLGVCKm6pVwUDkLX9mcjcjEVUjUc1F2sRmwQjEl2HGZJIClWTqI065C8rN03F7TFMCfGQBJprZr7hkgDUmYDMs2Yz8xUSWQXBWu6p5d+eV+vUsrCI6QsfoFPnoAtXRLWqgABEk0St6Q+/Oi/PhfDpfU2nBme0corlyvv8APdKeKg==</latexit>
(a) Variational autoencoders

collapsed local

0 optimum X

X

inference

collapse

µx, <latexit sha1_base64="8vnud1+psgy2EO9EZdPkDcMDhnQ=">AAACQnicbVBNS8NAEN34bf2KevSyWAQPpaQiqIhQ9OKxglWhKWWz3ZjF3WzYnYgl5Mf4Y8SrHv0NnsSrBzdpDlodWObtmzfDzAsSwQ143pszNT0zOze/sFhbWl5ZXXPXN66MSjVlXaqE0jcBMUzwmHWBg2A3iWZEBoJdB3dnRf36nmnDVXwJo4T1JbmNecgpAUsN3GNfpoPMlwSiIMwe8gb2A5n5ScTzHJ/g/6sQMSB5PnDrXtMrA/8FrQrUURWdgfvhDxVNJYuBCmJMr+Ul0M+IBk4Fy2t+alhC6B25ZT0LYyKZ6WflkTnescwQh0rbFwMu2Z8dGZGmWNQqi2R+1QoGlBKmYVUQySIVY8q/GcmgEchGIdImNBOLQHjYz3icpMBiOt4jTAUGhQtD8ZBrRkGMLCBUc3sKphHRhIK1vWY9ak068hd095pHTe9iv94+rcxaQFtoG+2iFjpAbXSOOqiLKHpEz+gFvTpPzrvz4XyOpVNO1bOJfoXz9Q0lJbMX</latexit>

= µx,

Xcmololadpesle

0
mean of true model posterior

(b) Posterior mean space

µx, <latexit sha1_base64="XWKglMNT3flz+HvE5Pzzkxay/dQ=">AAACJHicZVBNSwMxEM3Wr1q/qh5FCBbBQylbEdRb0YvHCtYWuqVk02wbmmyWZFYsy578MeJVf4cn8eDF/+A/MLvtwdaBMC9v3gwzz48EN+C6X05haXllda24XtrY3NreKe/u3RsVa8paVAmlOz4xTPCQtYCDYJ1IMyJ9wdr++Dqrtx+YNlyFdzCJWE+SYcgDTglYql8+9GTcTzxJYOQHyWNaxZ4vEw9GDEia9ssVt+bmgf+D+gxU0Cya/fKPN1A0liwEKogx3bobQS8hGjgVLC15sWERoWMyZF0LQyKZ6SX5GSk+tswAB0rbFwLO2b8dCZEmW9Qqs2TmahkDSglTtSoYySxlY/K/mUi/6stqJtImMAuLQHDRS3gYxcBCOt0jiAUGhTPL8IBrRkFMLCBUc3sKpiOiCQVrbMl6VF905D9ondYua+7tWaVxNTOriA7QETpBdXSOGugGNVELUfSEXtArenOenXfnw/mcSgvOrGcfzYXz/Qs8kaaP</latexit>

Figure 1: Left: Depiction of generative model p(z)p(x|z) and inference network q(z|x) in VAEs. Right: A toy posterior mean space (µx,, µx,) with scalar z. The horizontal axis represents the mean of the model posterior p(z|x), and the vertical axis represents the mean of the approximate posterior q(z|x). The dashed diagonal line represents when the approximate posterior matches the true model posterior in terms of mean.
Zhao et al., 2017; Alemi et al., 2018). Thus, many proposed solutions to posterior collapse focus on weakening the generator by replacing it with a non-recurrent alternative (Yang et al., 2017; Semeniuta et al., 2017) or modifying the training objective (Zhao et al., 2017; Tolstikhin et al., 2018). In this paper, we try to analyze the problem from perspective of training dynamics, and propose a novel training procedure for VAEs that optimizes the standard ELBO objective without changing the model structure.
Recently Kim et al. (2018) proposed a new approach to training VAEs by composing the standard inference network with additional mean-field updates. The resulting semi-amortized approach empirically avoided collapse and obtained better ELBO. However, because of the costly instancespecific local inference steps, the new method is more than 10x slower than basic VAE training in practice. It is also unclear why the basic VAE method fails to find better local optima that make use of latents. We consider two questions in this paper: (1) Why does basic VAE training often fall into undesirable collapsed local optima? (2) Is there a simpler way to change the training trajectory to find a better non-trivial local optimum?
To this end, we first study the posterior collapse problem from the perspective of training dynamics, and empirically find that the approximate posterior is lagging far behind the true model posterior in the initial stages of training (Section 3). We then demonstrate empirically how such lagging behavior can drive the generative model towards a collapsed local optimum, and propose a novel training procedure for VAEs that aggressively optimizes the inference network with more updates to mitigate the lagging issue (Section 4). Without introducing new modeling components over basic VAEs or additional complexity, our approach is surprisingly simple yet effective in circumventing posterior collapse. As a density estimator it outperforms neural autoregressive baselines on both text (Yahoo and Yelp) and image (OMNIGLOT) benchmarks, leading to comparable performance with more complicated previous state-of-the-art methods at a fraction of the training cost (Section 6).1

2 BACKGROUND

2.1 VARIATIONAL AUTOENCODERS

VAEs learn deep generative models defined by a prior p(z) and a conditional distribution p(x|z) as shown in Figure 1(a). In most cases the marginal data likelihood is intractable, so VAEs (Kingma & Welling, 2014) instead optimize a tractable variational lower bound (ELBO) of log p(x),

L(x; , ) = Ezq(z|x)[log p(x|z)] - DKL(q(z|x) p(z)),

(2)

Reconstruction Loss

KL Regularizer

1The code and data used in this paper will be released after the review period.

2

Under review as a conference paper at ICLR 2019
where q(z|x) is a variational distribution parameterized by an inference network with parameters , and p(x|z) denotes the generator network with parameters . q(z|x) is optimized to approximate the model posterior p(z|x). This lower bound is composed of a reconstruction loss term that encourages the inference network to encode information necessary to generate the data and a KL regularizer to push q(z|x) towards the prior p(z). Below, we consider p(z) := N (0, I) unless otherwise specified. A key advantage of using inference networks (also called amortized inference) to train deep generative models over traditional locally stochastic variational inference (Hoffman et al., 2013) is that they share parameters over all data samples, amortizing the computational cost and allowing for efficient training.
The term VAE is often used both to denote the class of generative models and the amortized inference procedure used in training. In this paper, it is important to distinguish the two and throughout we will refer to the generative model as the VAE-model, and the training procedure as VAE-training.
2.2 POSTERIOR COLLAPSE
Despite VAE's appeal as a tool to learn unsupervised representations through the use of latent variables, as mentioned VAE-models are often found to ignore latent variable when using a flexible generator like in the introduction LSTMs (Bowman et al., 2016). This problem of "posterior collapse", occurs when the training procedure falls into the trivial local optimum of the ELBO objective in which both the variational posterior and true model posterior collapse to the prior. This is undesirable because an important goal of VAEs is to learn meaningful latent features for inputs. Mathematically, posterior collapse represents a local optimum of VAEs where q(z|x) = p(z|x) = p(z) for all x. To facilitate our analysis about the cause leading up to collapse, we further define two partial collapse states for each x: model collapse, when p(z|x) = p(z), and inference collapse, when q(z|x) = p(z). Note that in this paper we use these two terms to denote the posterior states in the middle of training instead of local optima at the end. These two partial collapse states may not happen at the same time, which we will discuss later.
2.3 VISUALIZATION OF POSTERIOR DISTRIBUTION
Posterior collapse is closely related to true model posterior p(z|x) and approximate posterior q(z|x) as it is defined. Thus, in order to observe how posterior collapse happens, we track the state of p(z|x) and q(z|x) over the course of training, we will analyze the training trajectory in terms of the posterior mean space U = {µ : µ = (µTx,, µxT,)}, where µx, and µx, are the means of p(z|x)2 and q(z|x), respectively. We can then roughly consider µx, = 0 as model collapse and µx, = 0 as inference collapse as we defined before. Each x will be projected to a point in this space under the current model and inference network parameters. If z is a scalar we can efficiently compute µx, and visualize the posterior mean space as shown in Figure 1(b). The diagonal line µx, = µx, represents parameter settings where q(z|x) is equal to p(z|x) in terms of mean, indicating a well-trained inference network. The posterior collapsed local optimum is located at the origin,3 while more desirable local optima may be located on the diagonal. In this paper we will utilize this posterior mean space multiple times to analyze the posterior dynamics.
3 A LAGGING INFERENCE NETWORK PREVENTS USING LATENT CODES
In this section we analyze posterior collapse from a perspective of training dynamics. We will answer the question of why the basic VAE-training with strong decoders tends to hit a collapsed local optimum and provide intuition for the simple solution we propose in Section 4.
3.1 INTUITIONS FROM ELBO
Since posterior collapse is directly relevant to the approximate posterior q(z|x) and true model posterior p(z|x), we aim to analyze their training dynamics to study how posterior collapse happens.
2µx, can be approximated through discretization of the model posterior, which we will show in Section 3.2. 3Note that the converse is not true: a point located at the origin may not be a local optimum. For example when a model is initialized at the origin as we show in Section 3.2.
3

UnUdenrderervrieevwiewas aas caocnofenrfeenrecnecpeappaepreartaItCILCRLR20210919
µx, <latexit sha1_base64="JrR8bpJT9WIDXN5JEdhmmw5BeVU=">AAACInicZVBNSwMxEM36WetX1WMvwSJ4KGUrgnorevFYwbWFbinZNNuGJpslmRXLsgd/jHjV3+FJPAn+Cf+B2bYH2w6EeXnzZph5QSy4Adf9dlZW19Y3Ngtbxe2d3b390sHhg1GJpsyjSijdDohhgkfMAw6CtWPNiAwEawWjm7zeemTacBXdwzhmXUkGEQ85JWCpXqnsy6SX+pLAMAjTp6yK/UCmfjzkWdYrVdyaOwm8DOozUEGzaPZKv35f0USyCKggxnTqbgzdlGjgVLCs6CeGxYSOyIB1LIyIZKabTo7I8Ill+jhU2r4I8IT935ESafI1rTJPZq6WM6CUMFWrgqHMUz5m8jdjGVQDWc1F2oRmYREIL7spj+IEWESne4SJwKBwbhjuc80oiLEFhGpuT8F0SDShYG0tWo/qi44sA++sdlVz784rjeuZWQVURsfoFNXRBWqgW9REHqLoGb2iN/TuvDgfzqfzNZWuOLOeIzQXzs8fgA2lpg==</latexit> µx, <latexit sha1_base64="XWKglMNT3flz+HvE5Pzzkxay/dQ=">AAACJHicZVBNSwMxEM3Wr1q/qh5FCBbBQylbEdRb0YvHCtYWuqVk02wbmmyWZFYsy578MeJVf4cn8eDF/+A/MLvtwdaBMC9v3gwzz48EN+C6X05haXllda24XtrY3NreKe/u3RsVa8paVAmlOz4xTPCQtYCDYJ1IMyJ9wdr++Dqrtx+YNlyFdzCJWE+SYcgDTglYql8+9GTcTzxJYOQHyWNaxZ4vEw9GDEia9ssVt+bmgf+D+gxU0Cya/fKPN1A0liwEKogx3bobQS8hGjgVLC15sWERoWMyZF0LQyKZ6SX5GSk+tswAB0rbFwLO2b8dCZEmW9Qqs2TmahkDSglTtSoYySxlY/K/mUi/6stqJtImMAuLQHDRS3gYxcBCOt0jiAUGhTPL8IBrRkFMLCBUc3sKpiOiCQVrbMl6VF905D9ondYua+7tWaVxNTOriA7QETpBdXSOGugGNVELUfSEXtArenOenXfnw/mcSgvOrGcfzYXz/Qs8kaaP</latexit>

Basic

Aggressive

iter = 0

iter = 200

iter = 2000

convergence

ecFonigcuFactuerrhdiasrgaeiednnuneoir2gltneawf:egb2too,Tre:rfatlhhksitTenehithienrbpnesadorgeioptin.tcpjrioeaoTonmctijghinet.niecrtosogtnTtiwaoosthtpnhiefosseorfrofuoferw5orfos0udum50iril0sftfso0tdferurasrdaorethaiamnaontptawsinpttaishrmgmatoehmpaseabtclptsaeahtlssgwae.iemsc[hfsCriopoVl(anesm"AniSttnEhhtysaeetyothgrnupaeaetopihmcn1speoti"artuenoi,krcrgxise",oeidSmrttaohhtmtaafeeatgtesefbreeaoaoptnni2tnoott"siosnn,sptmlega.it.rgc..hi,rehToeo.otrhwlrpyeFioseirtbssoovltimapeefggrnrgroigoloembeiwrrnfe?tgmtoitts­ueoefGfrraarrrnoaNi"ggmbX]shgep[ttrhCEahewicesnpaesednobivcaostyehshhvoioseeciu"wnrVt,farteAhut"lhrseeX-Eoe dmiaonEgdaoepmnloapcelohylssito"neu,er.i.cao.o)nr.mdi­naeGbwuNaaps]yicw[fArViotdAhmd.E)li-a­ntbrGfaeeiNrlnse]intnocget,hcoeoultrloaappgsogefr.e"sVsiavneiltlraaiVnAinEg"apapnrdo"acOhusruAccpepsrsofualclhy"m(oovrewshthaetepvoeirnatsltoenrntoatthivee

Topth(isz|exn)di,sitloisgupse(fxu)l.toHaonwaelvyezre, athneadlteeprennadteenfocerm(ifoafnEyL) BmOay: be lost if q (z|x) is still close to p(z)

because pL((zx|;x),wo)u=ld
clear. Could you be more

bexepplircleoitsg?sup­rGe(dNx]t).o

agre-e with

q

D(zK|xL)(q[T(hzis|xis)|ipmp(ozr|txa)n)t,

but

I

t,hink

not

v(e3r)y

marginal log data likelihood agreement between approximate and model posteriors

pWjeci(pssTttipzihomva|(exczptehe)x|mlx,eiabsa)mtsyrh,yvgiteannirinenateohwdianpelt,pitthnidirecmtigahodtiedarazayapbtlonait(akinsazsoeemli)nytcl.iidhVcougoAfroroieaEpndllga,w(toabizionfat|hndsxaisa)ctphhpoiViepsnrAooeibxE-tnehdiflmteitwmurraaeeeiitnensencnisDnepigdoaoK,npsLbwapte(ylreqro[i"txwowsi(rcmizolal|qlaxafntore)e("r|xzpc?pt|eox­vs(sG,)iztseN|ouixrnsai])oleli)ratz,otoeeqwfntmhhtw(ieavzchta|phcixrcohid)has,rbtmieilvmsreoeiooodtshrdnepeelmali(perdpzaeoeon|lsaaxs-tltetv)iervaoritelioobuolr--yer wards q(z|x). Ideally if the approximate posterior is perfect, the second force will vanish, with

nea3Dr.l2yKLin(OdqeBpS(ezEn|RxdVe)A|npTtIuO(nzNd|Sxe)rO)Nbo=SthY0NqTwH(hzEe|TnxI)CqaDn(zdA|TxpA)

= p(z|x). (z|x) as we

At the show in

start of Section

training, 3.2, i.e.

z and all x

x are suffer

ftHdhriooveAsdamweetarGsrvetgaaebvaaeironeuf.esrrtisyo,hnWisnnmgitahtmethnoieahesobxmtyddjtipecpeeimrcxeloedtptotsicauhvsdogtreueaeelesrsllntiieaaehnzepstaegmrswataettatphaepiyvasoarukenitsbo,sssdmeer.iib?niTodo]lno]ydipvfs]ereecmcalarrra,cweneiuwmtntdihescciieeateceshsln,emtcdqahtLoreeue[aSldpe[lidT[aennibfpnvcMioynsedeugerertgdsh-ndiieametcnntryaeioeKvt,odrhbeaaLfeeesstttomhwtpbpree.eoierxg(rmsMeetztinuena|orwrxlnzreidoie)hnaoaredangftncaned.GotddpalxaTiliqsaulhs(utpsrzenss(ain|biezdbxau|eonhtx)trhuiap)oeaptsrnnttibooh,do(eniwrzsteqlh|--yensxeys(f)canpozfoiomtru|shmixunopelr)dporlt-eimogsictnnhtdpoadeeidorsantmc(ertstadxrsoestie)oontesr.eftt to abnridnegxtpheermimiennttodaeltiaginlsmceannt,bewfhoiulendloicnkAinpgpienntdoixmBo.d1e.l parameters that capture the distribution of x while ignoring z. Critically, posterior collapse is a local optimum; once a set of parameters that achWieevetrsaitnheasebagsoicalVs AarEe wreiatchhaedo,nger-addimieenntsoiopntiaml ilzaatetinotnvfaariialsblteo,mLaSkTeMfuernthceordperr,oagnredsLs,SeTvMen difecboedtteerr2 oveornalolumr osdyenltshethtiact dmaatakseetu.seWoef szamtopdlees5c0ri0bedaxtaexpiositn. ts and show them on the posterior mean space
plots at four different training stages from beginning to convergence in Figure 2. The mean value of

aNretaexhpltaepwtraivopeexpvlirymiossxauiitammeldipazlbteeeytsp:hyoenspttehoresitotiecrrdidoiasrttamrisbeeuattntioosnepxaµacxme, binyiestofrrauoirnmXhinytghpeoatobhuaetsspiiucst.VoAf tEhewiinthfearesnccaelanreltawteonrkt,vaanridabµlxe,onis

E [z] = [zip(zi|x)],

(5)

3.2 OBSERVATIONS ON SYNTHETzICpD(zA|xTA)

zi 2C

sAesvoweauhrtesesyriidnenetChtetehxitsiitscamgdgroairddtiadesslseienetttgiwswteazitesuhkrsosse..mdpDaisel(lctzars|eitxlrtsie)doesinseaqatnhuldsieosnssuacypeffinpdctrhaoieetxantii,tmclayasdtlapeatdroagssoeteentcratoihonvirdesrcegaoxrglipleda.e.prWsimeeehanastssaubrmeeeeinnthfAoeupdnpedennstdhitieyxmvBao.l1ust.e

msmWµyxeeenedtsmA,aahtesannhtseacctrceataoaaosiatnpinlpqfipnnlscedauothbcs(zimadsneteeztarato|abaptaasxgadntaplpeep)soedsppd,elitfrrcsttx(acoo.hizaionxxVel|aWtlsiixAlprmFmfaet)oeoiEpoaaguimsbsntturceeeawotora.sddmtepvtihiHsctoebfpht2hfssaaoyle,terlwafuremtdaaesrp5eirlsniosv0tlocttoacse0orapwtrtrslredo,adpipaatiniariaiyrsnzndlett(ltlaarfeiaszainrotpdbt|oipgfaexeoumaornnnt)stlehitidotnotoaeaenhlvtnsngfosngeaedecttµrfrtspapirhuaxtartohofenee,ibrmoeiddolnµmsremtiex,azsvpso,tea(pdLfairzlnrooreoiSao)eidirltx,nTmiaimpsgiataMtstsoiinleit.noilhsadazlTtnenerae[cche[ntrolso[iaiioceousasontsottfntrfipietptdlaehhprtluetnormeeorti--dn,n(csobtozohasfueatm|hhnnepgntxhvoadoieµ)nteewinnnxnrL(lo]igofs,in]tSneneeg]hfrTegneieepaimcrMmnnAxoeeicpi(nftpsexiilocd,painten)ecrelweisaFocnizntihsilothadneglhidtaaeiictuainxwbpehotrpgrlsneAofesoe.o,rou2ds)tkrng.otwI.,esnsgThohartoeainetmihousthcldeetrrphees. As illustrated in Figure 2, all points are located at the origin upon initialization, which means z and x are 2bTohtihs LaSlmTMostdeincoddeeprehnadselnestsacnadpazceitryothmaneatnheaotntehuesbedegfoinrncirneagtiongf tthraeidnaintags.etIsnintcheeinserecaolnwdosrltdagmeodoefl basciacpaVcAityEi,stuhseuaplolyinintssusftfiacritentot tosperxeaacdtlyalmoondgelththeeµemx,piraicxailsd. isTtrhibisutpiohne.nomenon implies that for some data points p(z|x) moves far away from the prior p(z), and confirms that log p(x) is able to help

4

4

Under review as a conference paper at ICLR 2019

Algorithm 1 VAE training with controlled aggressive inference network optimization.

1: ,   Initialize parameters

2: aggressive  TRUE

3: repeat

4: if aggressive then

5: repeat

[aggressive updates]

6: X  Random data minibatch

7: Compute gradients g  L(X; , )

8: Update  using gradients g

9: until convergence

10: X  Random data minibatch

11: Compute gradients g  L(X; , )

12: Update  using gradients g

13: else

[basic VAE training]

14: X  Random data minibatch

15: Compute gradients g  ,L(X; , ) 16: Update ,  using g

17: end if

18: Update aggressive as discussed in Section 4.2

19: until convergence

One step of generator update
Inner loop of inference network update
Figure 3: Trajectory of one data instance on the posterior mean space with our aggressive training procedure. Horizontal arrow denotes one step of generator update, and vertical arrow denotes the inner loop of inference network update. We note that the approximate posterior q(z|x) takes an aggressive step to catch up to the model posterior p(z|x).

move away from model collapse. However, all of these points are still close to the µx, axis, which suggests that q(z|x) fails to catch up to p(z|x) and these points are still in a state of inference collapse. As expected, the dependence between z and x under p(z|x) is gradually lost and finally
the model converges to the collapsed local optimum.

4 METHOD

4.1 AGGRESSIVE TRAINING OF THE INFERENCE NETWORK

The problem reflected in Figure 2 implies that the inference network is lagging far behind p(z|x), and might suggest more "aggressive" inference network updates are needed. Instead of blaming the poor approximation on the limitation of the inference network's amortization, we hypothesize that the optimization of the inference and generation networks are imbalanced, and propose to separate the optimization of the two. Specifically, we change the training procedure to:

 = arg max L(X; , ), where  = arg max L(X; , ),


(4)

where optimizing the inference network q(z|x) is an inner loop in the entire training process as shown in Algorithm 1. This training procedure shares the same spirit with traditional stochastic

variational inference (SVI) (Hoffman et al., 2013) that performs iterative inference for each data

point separately and suffers from very lengthy iterative estimation. Compared with recent work that

try to combine amortized variational inference and SVI (Hjelm et al., 2016; Krishnan et al., 2018;

Kim et al., 2018; Marino et al., 2018) where the inference network is learned to be a component to

help perform instance-specific variational inference, our approach keeps variational inference fully

amortized, allowing for reverting back to efficient basic VAE training as discussed in Section 4.2. Also, this aggressive inference network optimization algorithm is as simple as basic VAE training

without introducing additional SVI steps, yet attains comparable performance to more sophisticated

approaches as we will show in Section 6.

4.2 STOPPING CRITERION
Always training with Eq.4 would be inefficient and neglects the benefit of the amortized inference network. Following our previous analysis, the term DKL(q(z|x)|p(z|x)) tends to pressure q(z|x) or p(z|x) to p(z) only if at least one of them is close to p(z), and thus we posit that if we can confirm that we haven't reached this degenerate condition, we can continue with standard VAE training. Since q(z|x) is the one lagging behind, we use the mutual information Iq between z and

5

Under review as a conference paper at ICLR 2019

x under q(z|x) to control our stopping criterion. In practice, we compute the mutual information on the validation set every epoch, and stop the aggressive updates when Iq stops climbing. In all our experiments in this paper we found that the aggressive algorithm usually reverts back to basic
VAE-training within 5 epochs. Mutual information, Iq is computed by:

Iq = Expd(x)[DKL(q(z|x)||p(z))] - DKL(q(z) p(z)),

(5)

where pd(x) is the empirical distribution. The aggregated posterior, q(z) = Expd(x)[q(z|x)],
can be approximated with a Monte Carlo estimate. DKL(q(z) p(z)) is also approximated by Monte Carlo, where samples from q(z) can be easily obtained by ancestral sampling (i.e. sample x from dataset and sample z  q(z|x)). The complete algorithm is shown in Algorithm 1.

4.3 OBSERVATIONS ON SYNTHETIC DATASET
By training the VAE model with our approach on synthetic data, we visualize the 500 data samples in the posterior mean space in Figure 2. From this, it is evident that the points move towards µx, = µx, and are roughly distributed along the diagonal in the end. This is in striking contrast to the basic VAE and confirms our hypothesis that the inference and generator optimization can be rebalanced by simply performing more updates of the inference network. In Figure 3 we show the training trajectory of one single data instance for the first several optimization iterations and observe how the aggressive updates help escape inference collapse.

5 RELATION TO RELATED WORK
Posterior collapse in VAEs is first detailed in (Bowman et al., 2016) where they combine a LSTM decoder with VAE for text modeling. They interpret this problem from a regularization perspective, and propose the "KL cost annealing" method to address this issue, whereby the weight of KL term between approximate posterior and prior increases from a small value to one in a "warm-up" period. This method has been shown to be unable to deal with collapse on complex text datasets with very large LSTM decoders (Yang et al., 2017; Kim et al., 2018). Many works follow this line to lessen the effect of KL term such as -VAE (Higgins et al., 2017) that treats the KL weight as a hyperparameter or "free bits" method that constrains the minimum value of the KL term. Our approach differs from these methods in that we do not change ELBO objective during training and in principle maximum likelihood estimation. While these methods explicitly encourages the use of latent variable, they may implicitly sacrifice density estimation performance at the same time, as we will discuss in Section 6.
Another thread of research focuses on a different problem called the "amortization gap" (Cremer et al., 2018) that refers to the difference of ELBO caused by parameter sharing of the inference network. Some approaches try to combine traditional instance-specific variational inference with amortized variational inference to narrow this gap (Hjelm et al., 2016; Krishnan et al., 2018; Kim et al., 2018; Marino et al., 2018). The most related example is SA-VAE (Kim et al., 2018), which mixes instance-specific variational inference and empirically avoids posterior collapse. Our approach is much simpler without sacrificing performance, yet achieves an average of 5x training speedup.
Other attempts to address posterior collapse include proposing new regularizers (Zhao et al., 2017; Tolstikhin et al., 2018; Phuong et al., 2018), deploying less powerful decoders (Yang et al., 2017; Semeniuta et al., 2017), using lossy input (Chen et al., 2017), utilizing skip connections (Dieng et al., 2017; 2018), or changing the prior (Tomczak & Welling, 2018; Xu & Durrett, 2018).

6 EXPERIMENTS
Our experiments below are designed to (1) examine whether the proposed method indeed prevents posterior collapse, (2) test its efficacy with respect to maximizing predictive log-likelihood compared to other existing approaches, and (3) test its training efficiency.
6.1 SETUP
For all experiments we use a Gaussian prior N (0, I) and the inference network parametrizes a diagonal Gaussian. We evaluate with approximate negative log likelihood (NLL) as estimated by 500

6

Under review as a conference paper at ICLR 2019

Table 1: Results on all datasets. For LSTM-LM and PixelCNN we report the exact negative log likelihood.

Model

Yahoo

Yelp15

NLL KL MI NLL KL

VLAE (Chen et al., 2017) VampPrior (Tomczak & Welling, 2018) CNN-VAE (Yang et al., 2017) SA-VAE + anneal (Kim et al., 2018)

Previous Reports

­

­­

­

­

­­

­

332.1 10.0 ­ 359.1

327.5 7.19 ­

­

­ ­ 7.6 ­

OMNIGLOT MI NLL KL MI

­ 89.83 ­ ­

­ 89.76 ­ ­

­­

­­

­ 90.05 2.78 ­

VAE + anneal -VAE ( = 0.2) -VAE ( = 0.4) -VAE ( = 0.6) -VAE ( = 0.8) SA-VAE + anneal Ours + anneal
LSTM-LM PixelCNN VAE SA-VAE Ours

Modified VAE Objective

328.6 333.1 328.5 328.6 328.8 327.4 326.6

0.0 0.0 18.7 3.4 3.8 2.2 0.1 0.1 0.0 0.0 3.5 1.9 6.7 3.2

358.1 360.4 358.8 358.1 357.9 355.9 355.9

Standard VAE Objective

328.1 ­
328.8 329.1 328.0

­­ ­­ 0.0 0.0 0.1 0.0 5.4 3.0

357.3 ­
358.2 357.7 357.0

0.0 0.0 89.20 2.1 1.9 11.3 3.3 105.19 69.1 3.9 4.0 1.8 95.89 28.0 3.9 0.1 0.1 92.16 20.4 3.9 0.0 0.0 89.27 9.6 3.9 2.3 1.3 89.01 3.4 2.6 3.7 2.3 89.12 2.5 2.2

­­ ­­ 0.0 0.0 0.4 0.3 3.8 2.6

­ 89.76 89.41 89.30 89.03

­­ ­­ 1.5 1.4 2.6 2.2 2.5 2.2

importance weighted samples (Burda et al., 2016) since it produces a tighter lower bound to marginal data log likelihood than ELBO, and should be more accurate. We also report DKL(q(z|x) p(z)) (KL), mutual information Iq (MI) and include ELBO in Appendix C.
As baselines, we compare with strong neural autoregressive models (LSTM-LM for text and PixelCNN (van den Oord et al., 2016) for images), basic VAE, the "KL cost annealing" method (Bowman et al., 2016; Sønderby et al., 2016), -VAE (Higgins et al., 2017), and SA-VAE (Kim et al., 2018) which holds the previous state-of-the-art performance on text modeling benchmarks. For -VAE we vary  between 0.2, 0.4, 0.6, and 0.8. SA-VAE is ran with 10 refinement steps. We also examine the effect of KL cost annealing on both SA-VAE and our approach. To facilitate our analysis later, we report the results in two categories: "Standard VAE objectives", and "Modified VAE objectives".4
We evaluate our method on density estimation for text on the Yahoo and Yelp corpora (Yang et al., 2017) and images on OMNIGLOT (Lake et al., 2015). Following the same configuration as in Kim et al. (2018), we use a single layer LSTM as encoder and decoder for text. For images, we use a ResNet (He et al., 2016) encoder and a 13-layer Gated PixelCNN (van den Oord et al., 2016) decoder. We use 32-dimensional z and optimize ELBO objective with SGD for text and Adam (Kingma & Ba, 2015) for images. We concatenate z to the input for the decoders. For text, z also predicts the initial hidden state of the LSTM decoder. We dynamically binarize images during training and test on fixed binarized test data. Full details for setup are in Appendix B.2 and B.3.
6.2 RESULTS
In Table 1 we show the results on all three datasets. Our method outperforms all baselines for the three datasets in the "standard" as well as the "modified" section on text. We observe that SA-VAE suffers from posterior collapse on both text datasets without annealing. However, we demonstrate that our algorithm does not experience posterior collapse without annealing.
Note that to examine posterior collapse issue for images we use a larger PixelCNN decoder than previous work, thus our approach is not directly comparable to them and included at the top of Table 1 as reference points. On OMNIGLOT our method without annealing achieves comparable performance to SA-VAE combined with annealing.
4While annealing reverts back to ELBO objective after the warm-up period, we consider part of "Modified VAE objectives" since it might produce undesired behavior in the warm-up period, as we will discuss soon.
7

Under review as a conference paper at ICLR 2019

12 12

10

DKL(q(z|x)||p(z))

10

8 Iq

8

12 10 8

6 DKL(q(z) p(z)) 6
44

6 4

222

000

0 20 40 60 0 20 40 60 0

epoch

epoch

20
epoch

40

Figure 4: Training behavior on Yelp. Left: VAE + annealing. Middle: Our method. Right: -VAE ( = 0.2).

Table 2: Comparison of total training time, in terms of relative speed and absolute hours.

VAE SA-VAE Ours

Yahoo Relative Hours
1.00 5.35 9.91 52.99 2.20 11.76

Yelp15 Relative Hours
1.00 5.75 10.33 59.37 3.73 21.44

Omniglot Relative Hours
1.00 4.30 15.15 65.07 2.19 9.42

6.3 ANALYSIS
We analyze the difference between our approach and the methods that weaken the KL regularizer term in ELBO, and explain the unwanted behavior produced by breaking maximum likelihood estimation. As illustrative examples, we compare with the KL cost annealing method and -VAE. Decreasing the weight of the KL regularizer term in ELBO is equivalent to adding an additional regularizer to push q(z|x) far from p(z). We set  = 0.2 in order to better observe this phenomenon.
We investigate the training procedure on the Yelp dataset based on: (1) the mutual information between z and x, Iq, (2) the KL regularizer, Expd(x)[DKL(q(z|x) p(z))], and (3) the distance between the aggregated posterior and the prior, DKL(q(z) p(z)). Note that the KL regularizer is equal to the sum of the other two as stated in Eq.5. We plot these values over the course of training in Figure 4. In the initial training stage we observe that the KL regularizer increases with all three approaches, however, the mutual information, Iq, in the annealing remains small, thus a large KL regularizer term does not imply that the latent variable is being used. Finally the annealing method suffers from posterior collapse. For -VAE, the mutual information increases, but DKL(q(z) p(z)) also reaches a very large value. Intuitively, DKL(q(z) p(z)) should be kept small for learning the generative model well since in the objective the generator p(x|z) is learned with latent variables sampled from the variational distribution. If the setting of z that best explains the data has a lower likelihood under the model prior, then the overall model would fit the data poorly. The same intuition has been discussed in Zhao et al. (2017) and Tolstikhin et al. (2018). This also explains why -VAE generalizes poorly when it has large mutual information. In contrast, our approach is able to obtain high mutual information, and at the same time maintain a small DKL(q(z) p(z)) as a result of optimizing standard ELBO where the KL regularizer upper-bounds DKL(q(z) p(z)).
6.4 TIMING
In Table 2 we report the total training time of our approach, SA-VAE and basic VAE-training across the three datasets. We find that the training time for our algorithm is only 2-3 times slower than a regular VAE whilst being 3-7 times faster than SA-VAE.

7 CONCLUSION
In this paper we study the "posterior collapse" problem that variational autoencoders experience when the model is parameterized by a strong autoregressive neural network. In our synthetic experiment we identify that the problem lies with the lagging inference network in the initial stages of training. To remedy this, we propose a simple yet effective training algorithm that aggressively optimizes the inference network with more updates before reverting back to basic VAE-training. Experiments on text and image modeling demonstrates effectiveness of our approach.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing a broken elbo. In Proceedings of ICML, 2018.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In Proceedings of SIGNLL, 2016.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In Proceedings of ICLR, 2016.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder. In Proceedings of ICLR, 2017.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoencoders. In Proceedings of ICML, 2018.
Adji B Dieng, Chong Wang, Jianfeng Gao, and John Paisley. Topicrnn: A recurrent neural network with long-range semantic dependency. In Proceedings of ICLR, 2017.
Adji B Dieng, Yoon Kim, Alexander M Rush, and David M Blei. Avoiding latent variable collapse with generative skip models. In Proceedings of ICML workshop on Theoretical Foundations and Applications of Deep Generative Models, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of CVPR, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In Proceedings of ICLR, 2017.
Devon Hjelm, Ruslan R Salakhutdinov, Kyunghyun Cho, Nebojsa Jojic, Vince Calhoun, and Junyoung Chung. Iterative refinement of the approximate posterior for directed belief networks. In Proceedings of NIPS, 2016.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303­1347, 2013.
Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag, and Alexander M Rush. Semi-amortized variational autoencoders. In Proceedings of ICML, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of ICLR, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of ICLR, 2014.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Proceedings of NIPS, 2016.
Rahul Krishnan, Dawen Liang, and Matthew Hoffman. On the challenges of learning with inference networks on sparse, high-dimensional data. In Proceedings of AISTATS, 2018.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332­1338, 2015.
Joseph Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. In Proceedings of ICML, 2018.
Mary Phuong, Max Welling, Nate Kushman, Ryota Tomioka, and Sebastian Nowozin. The mutual autoencoder: Controlling information in latent code representations, 2018. URL https:// openreview.net/forum?id=HkbmWqxCZ.
Stanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. A hybrid convolutional variational autoencoder for text generation. In Proceedings of EMNLP, 2017.
9

Under review as a conference paper at ICLR 2019 Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder
variational autoencoders. In Proceedings of NIPS, 2016. Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. In Proceedings of ICLR, 2018. Jakub M. Tomczak and Max Welling. Vae with a vampprior. In Proceedings of AISTATS, 2018. Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Condi-
tional image generation with pixelcnn decoders. In Proceedings of NIPS, 2016. Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. In Pro-
ceedings of EMNLP, 2018. Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational
autoencoders for text modeling using dilated convolutions. In Proceedings of ICML, 2017. Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational
autoencoders. arXiv preprint arXiv:1706.02262, 2017.
10

Under review as a conference paper at ICLR 2019

A APPROXIMATION OF THE MEAN OF THE TRUE MODEL POSTERIOR

We approximate the mean of true model posterior p(z|x) by discretization of the density distribution (Riemann integral):

Ezp(z|x)[z] = [zip(zi|x)],
zi C

(6)

where C is a partition of an interval with small stride and sufficiently large coverage. We assume the density value outside this interval is zero. The model posterior, p(z|x), needs to be first approximated on this partition of interval. In practice, for the synthetic data we choose the interval
[-20.0, 20.0] and stride equal to 0.01. This interval should have sufficient coverage since we found all samples from true model posterior p(z|x) lies within [-5.0, 5.0] by performing MH sampling.

B EXPERIMENTAL DETAILS
In general, for annealing we increase the KL weight linearly from 0.1 to 1.0 in the first 10 epochs, as in Kim et al. (2018).
B.1 SYNTHETIC EXPERIMENT FOR SECTION 3 AND 4
To generate synthetic data points, we first sample a two-dimensional latent variable z from a mixture of Gaussian distributions that have four mixture components. We choose dimension two because we want the synthetic data distribution is relatively simple but in the meanwhile complex enough for an one-dimensional latent variable model to fit. We choose mixture of Gaussian to make sure the diversity of synthetic data. The mean of these Gaussians are (-2.0, -2.0), (-2.0, 2.0), (2.0, -2.0), (2.0, 2.0), respectively. All of them have a unit variance. Then we follow the synthetic data generation in Kim et al. (2018), where we sample data points from an one-layer LSTM conditioned on latent variables. The LSTM has 100 hidden units and 100-dimensional input embeddings. An affine transformation of z is used as the initial hidden state of LSTM decoder, z is also concatenated with output of LSTM at each time stamp to be directly mapped to vocabulary space. LSTM parameters are initialized with U(-1, 1), and the part of MLP that maps z to vocabulary space is initialized with U(-5, 5), this is done to make sure that the latent variables have more influence in generating data. We generated a dataset with 20,000 examples (train/val/test is 16000/2000/2000) each of length 10 from a vocabulary of size 1000.
In the synthetic experiment we use a LSTM encoder and LSTM decoder, both of which have 50 hidden units and 50 latent embeddings. This LSTM decoder has less capacity than the one used for creating the dataset since in real world model capacity is usually insufficient to exactly model the empirical distribution. Parameters of LSTM decoders are initialized with U(-0.1, 0.1), except the embedding weight matrix that is initialized with U(-0.01, 0.01). Dropout layers with probability 0.5 are applied to both input embeddings and output hidden embeddings of decoder. We use SGD optimizer and start with the learning rate of 1.0 and decay it by a factor of 2 if the validation loss has not improved in 2 epochs and terminate training once the learning rate has decayed a total of 5 times.
B.2 TEXT
Following (Kim et al., 2018), we use a single-layer LSTM with 1024 hidden units and 512dimensional word embeddings as the encoder and decoder for all of text models. The LSTM parameters are initialized from U(-0.01, 0.01), and embedding parameters are initialized from U(-0.1, 0.1). We use the final hidden state of the encoder to predict (via a linear transformation) the latent variable. We use SGD optimizer and start with the learning rate of 1.0 and decay it by a factor of 2 if the validation loss has not improved in 2 epochs and terminate training once the learning rate has decayed a total of 5 times. We don't perform any text preprocessing and use the datasets as provided. We follow Kim et al. (2018) and use dropout of 0.5 on the decoder for both the input embeddings of the decoder and on the output of the decoder before the linear transformation to vocabulary space.

11

Under review as a conference paper at ICLR 2019

B.3 IMAGES
We use the same train/val/test splits as provided by Kim et al. (2018). We use Adam optimizer and start with the learning rate of 0.001 and decay it by a factor of 2 if the validation loss has not improved in 20 epochs and terminate training once the learning rate has decayed a total of 5 times. Inputs were dynamically binarized throughout training by viewing the input as Bernoulli random variables that are sampled from pixel values. We validate and test on a fixed binarization and our decoder uses binary likelihood. Our ResNet is the same as used by Chen et al. (2017). Our 13-layer PixelCNN architecture is a larger variant based on what was used in Kim et al. (2018) and described in their Appendix B.3 section. The PixelCNN has five 7 x 7 layers, followed by, four 5 x 5 layers, and then four 3 x 3 layers. Each layer has 64 feature maps. We use batch normalization followed by an ELU activation before our final 1 x 1 convolutional layer and sigmoid nonlinearity.

C ADDITIONAL RESULTS CONTAINING ELBO

Table 3: Results on Yahoo, Yelp and OMNIGLOT. For LSTM language model () and PixelCNN () we report the exact negative log likelihood. Yang17: (Yang et al., 2017), Dieng18: (Dieng et al., 2018), Kim18: (Kim et al., 2018), Chen16: (Chen et al., 2017), Tomczak17: (Tomczak & Welling, 2018)

Model

Yahoo

Yelp15

Omniglot

IW -ELBO KL MI IW -ELBO KL MI IW -ELBO KL MI

VLAE (Chen16) VampPrior (Tomczak17) CNN-VAE (Yang17) SKIP-VAE (Dieng18) SA-VAE + anneal (Kim18)

­ ­ ­ ­ ­

­ ­ 332.1 330.5 327.5

­­ ­­ 10.0 ­ 0.34 0.31 7.19 ­

Previous Reports
­ ­­ ­ ­­ ­ 359.1 7.6 ­ ­­ ­ ­­

­ ­ ­ ­ ­

89.83 89.76
­ ­ ­

­ ­­ ­ ­­ ­ ­­ ­ ­­ 90.05 2.78 ­

VAE + anneal -VAE ( = 0.2) -VAE ( = 0.4) -VAE ( = 0.6) -VAE ( = 0.8) SA-VAE + anneal Ours + anneal
LSTM-LM PixelCNN VAE SA-VAE Ours

328.6 333.1 328.5 328.6 328.8 327.4 326.6

328.8 336.8 329.4 328.9 329.0 327.7 328.5

0.0 0.0 18.7 3.4 3.8 2.2 0.1 0.1 0.0 0.0 3.5 1.9 6.7 3.2

Modified VAE Objective

358.1 360.4 358.8 358.1 357.9 355.9 355.9

358.3 363.1 359.8 358.4 358.1 356.1 357.1

0.0 0.0 89.20 89.53 2.1 1.9 11.3 3.3 105.19 112.37 69.1 3.9 4.0 1.8 95.89 100.67 28.0 3.9 0.1 0.1 92.16 94.66 20.4 3.9 0.0 0.0 89.27 90.34 9.6 3.9 2.3 1.3 89.01 89.40 3.4 2.6 3.7 2.3 89.12 89.55 2.5 2.2

Standard VAE Objective

328.1 328.1 ­ ­ 357.3 357.3 ­ ­

­

­ ­­

­

­

­­

­

­ ­ ­ 89.76 89.76 ­ ­

328.8 329.1 0.0 0.0 358.2 358.4 0.0 0.0 89.41 89.65 1.5 1.4

329.1 329.1 0.1 0.0 357.7 357.8 0.4 0.3 89.30 89.56 2.6 2.2

328.0 329.5 5.4 3.0 357.0 358.2 3.8 2.6 89.03 89.48 2.5 2.2

12

