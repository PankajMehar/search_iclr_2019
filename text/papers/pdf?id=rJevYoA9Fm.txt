Under review as a conference paper at ICLR 2019
The Singular Values of Convolutional Layers
Anonymous authors Paper under double-blind review
A
We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2% to 5.3%.
1I
Exploding and vanishing gradients (Hochreiter, 1991; Hochreiter et al., 2001; Goodfellow et al., 2016) are fundamental obstacles to effective training of deep neural networks. Many deep networks used in practice are layered. We can think of such networks as the composition of a number of feature transformations, followed by a linear classifier on the final layer of features. The singular values of the Jacobian of a layer bound the factor by which it increases or decreases the norm of the backpropagated signal. If these singular values are all close to 1, then gradients neither explode nor vanish. These singular values also bound these factors in the forward direction, which affects the stability of computations, including whether the network produces the dreaded "Nan". Moreover, it has been proven (Bartlett et al., 2017) that the generalization error for a network is bounded by the Lipschitz constant of the network, i.e., product of the operator norm for each layer. The operator norm is the largest singular value of a layer. Cisse et al. (2017) discussed robustness to adversarial examples as a result of bounding the operator norm.
These considerations have led authors to regularize networks by driving down the operator norms of network layers (Drucker and Le Cun, 1992; Hein and Andriushchenko, 2017; Yoshida and Miyato, 2017; Miyato et al., 2018). Orthogonal initialization (Saxe et al., 2013; Pennington et al., 2017) and Parseval networks (Cisse et al., 2017) are motivated by similar considerations.
Convolutional layers (LeCun et al., 1998) are key components of modern deep networks. They compute linear transformations of their inputs. The Jacobian of a linear transformation is always equal to the linear transformation itself. Because of the central importance of convolutional layers to the practice of deep learning, and the fact that the singular values of the linear transformation computed by a convolutional layer are the key to its contribution to exploding and vanishing gradients, we study these singular values. Up until now, authors seeking to control the operator norm of convolutional layers have resorted to approximations (Yoshida and Miyato, 2017; Miyato et al., 2018; Gouk et al., 2018a). In this paper, we provide an efficient way to compute the singular values exactly -- this opens the door to various regularizers.
We consider the convolutional layers commonly applied to image analysis tasks. The input to a typical layer is a feature map, with multiple channels for each position in an n × n field. If there are m channels, then the input as a whole is a m × n × n tensor. The output is also an n × n field with multiple channels per position1. Each channel of the output is obtained by taking a linear combination of the values of the features in all channels in a local neighborhood centered at the corresponding position in the input feature map. Crucially, the same linear combination is used for all positions in the feature map. The coefficients are compiled in the kernel of the convolution. If the neighborhood is a k × k region, a kernel K is a m × m × k × k tensor. The projection Kc,:,:,: gives the coefficients that determine the cth channel of the output, in terms of the values found in all of the channels of all positions in it neighborhood; Kc,d,:,: gives the coefficients to apply to the dth input channel, and
1Here, to keep things simple, we are concentrating on the case that the stride is 1.
1

Under review as a conference paper at ICLR 2019

Kc,d,p,q are the coefficients to apply to this input at in the position in the field offset horizontally by p and vertically by q. For ease of exposition, we assume that feature maps and local neighborhoods are square and that the number of channels in the output is equal to the number of channels in the input the extension to the general case is completely straightforward.
To handle edge cases in which the offsets call for inputs that are off the feature maps, practical convolutional layers either do not compute outputs (reducing the size of the feature map), or pad the input with zeros. The behavior of these layers can be approximated by a layer that treats the input as if it were a torus; when the offset calls for a pixel that is off the right end of the image, the layer "wraps around" to take it from the left edge, and similarly for the other edges. The quality of this approximation has been heavily analyzed in the case of one-dimensional signals (Gray, 2006). Consequently, theoretical analysis of convolutions that wrap around has been become standard. This is the case analyzed in this paper.

Summary of Results: Our main result is a characterization of the singular values of a convolutional layer in terms of the kernel tensor K. Our characterization enables these singular values to be computed exactly in a simple and practically fast way, using O(n2m2(m + log n)) time. For comparison, the brute force solution that performs SVD on the matrix that encodes the convolutional layer's linear transformation would take O((n2m)3) = O(n6m3) time, and is impractical for commonly used network sizes. As another point of comparison, simply to compute the convolution takes O(n2m2k2) time. We prove that the following two lines of NumPy correctly compute the singular values.
def SingularValues(kernel, input_shape): transform_coefficients = np.fft.fft2(kernel, input_shape, axes=[0, 1]) return np.linalg.svd(transform_coefficients, compute_uv=False)
Here kernel is any 4D tensor, and input_shape is the shape of the feature map to be convolved. A TensorFlow implementation is similarly simple.
Timing tests, reported in Section 4.1, confirm that this characterization speeds up the computation of singular values by multiple orders of magnitude ­ making it usable in practice. The algorithm first performs m2 FFTs, and then it performs n2 SVDs. The FFTs, and then the SVDs, may be executed in parallel. Our TensorFlow implementation runs a lot faster than the NumPy implementation (see Figure 1); we think that this parallelism is the cause. We used our code to compute the singular values of the convolutional layers of the official ResNet-v2 model released with TensorFlow (He et al., 2016). The results are described in Section 4.4.
Exposing the singular values of a convolutional layer opens the door to a variety of regularizers for these layers, including operator-norm regularizers.
In Section 4.2, we evaluate an algorithm that periodically projects each convolutional layer onto a operator-norm ball. Using the projections improves the test error from 6.2% to 5.3% on CIFAR-10. We evaluate bounding the operator norm with and without batchnorm and we see that, regularizing the operator norm helps, even in the presence of batch normalization. Moreover, operator-norm regularization and batch normalization are not redundant, and neither dominates the other. They complement each other.

Related work: Prior to our work, authors have responded to the difficulty of computing the singular

values of convolutional layers in various ways. Cisse et al. (2017) constrained the matrix to have

orthogonal

rows

and

scale

the

output

of

each

layer

by

a

factor

of

(2k

+

1)-

1 2

,

for

k

×

k

kernels.

Yoshida and Miyato (2017) used the operator norm of a reshaping of K in place of the operator

norm for the linear transformation associated with K in their experiments. The two can be seen

to be different by considering the 1 × 1 × 2 × 2 kernel with every entry equal to 1. Gouk et al.

(2018a;b) proposed regularizing using a per-mini-batch approximation to the operator norm. They

find the largest ratio between the input and output of a layer in the minibatch, and then scale down the

transformation (thereby scaling down all of the singular values, not just the maximizers) so that the

new value of this ratio obeys a constraint. On the contrary, our analysis leads to an efficient method to

access the whole spectrum. Having access to the whole spectrum, we can clip the singular values and

compute a projection using the exact value of the operator norm which leads to a larger improvement

on a stronger baseline.

2

Under review as a conference paper at ICLR 2019
Overview of the Analysis: If the signal is 1D and there is a single input and output channel, then the linear transformation associated with a convolution is encoded by a circulant matrix, i.e., a matrix whose rows are circular shifts of a single row (Gray, 2006). For example, for a row a = (a1, a2, a3),
a0 a1 a2 the circulant matrix circ(a) generated by a is a2 a0 a1 . In the special case of a 2D signal
a1 a2 a0 with a single input channel and single output channel, the linear transformation is doubly block circulant (see (Goodfellow et al., 2016)). Such a matrix is made up of a circulant matrix of blocks, each of which in turn is itself circulant. Finally, when there are m input channels and m output channels, there are three levels to the hierarchy: there is a m × m matrix of blocks, each of which is doubly block circulant. Our analysis extends tools from the literature built for circulant (Horn and Johnson, 2012) and doubly circulant (Chao, 1974) matrices to analyze the matrices with a third level in the hierarchy arising from the convolutional layers used in deep learning. One key point is that the eigenvectors of a circulant matrix are Fourier basis vectors: in the 2D, one-channel case, the matrix whose columns are the eigenvectors is F  F , for the matrix F whose columns form the Fourier basis. Multiplying by this matrix is a 2D Fourier transform. In the multi-channel case, we show that the singular values can be computed by (a) finding the eigenvalues of each of the m2 doubly circulant matrices (of dimensions n2 × n2) using a 2D Fourier transform, (b) by forming multiple m × m matrices, one for each eigenvalue, by picking out the i-th eigenvalue of each of the n2 × n2 blocks, for i  [1..n2]. The union of all of the singular values of all of those m × m matrices is the multiset of singular values of the layer. Notation: We use upper case letters for matrices, lower case for vectors. For matrix M , Mi,: represents the i-th row and M:,j represents the j-th column; we will also use the analogous notation for higher-order tensors. The operator norm of M is denoted by ||M ||2. For n  N, we use [n] to denote the set {0, 1, . . . , n - 1} (instead of usual {1, . . . , n}). We will index the rows and columns of matrices using elements of [n], i.e. numbering from 0. Addition of row and column indices will be done mod n unless otherwise indicated. (Tensors will be treated analogously.) Let (·) be the mapping from a matrix to (the multiset of) its singular values. 2
 Let  = exp(2i/n), where i = -1. (Because we need a lot of indices in this paper, our use of i to define  is the only place in the paper where we will use i to denote -1.) Let F be the n × n matrix that represents the discrete Fourier transform: Fij = ij. We use In to denote the identity matrix of size n × n. We use ei, i  [n] to represent the basis vectors in Rn. We use  to represent the Kronecker product between two matrices (which also refers to the outer product of two vectors).
2A
2.1 O
As a warmup, we focus on the case that the number m of input channels and output channels is 1. In this case, the filter coefficients are simply a k × k matrix. It will simplify notation, however, if we embed this k × k matrix in an n × n matrix, by padding with zeroes (which corresponds to the fact that the offsets with those indices are not used). Let us refer to this n × n matrix also as K. An n2 × n2 matrix A is doubly block circulant if A is a circulant matrix of n × n blocks that are in turn circulant. For a matrix X, let vec(X) be the vector obtained by stacking the columns of X.
2 For two multisets S and T , we use S  T to denote the multiset obtained by summing the multiplicities of members of S and T .
3

Under review as a conference paper at ICLR 2019

Lemma 1 (see (Jain, 1989; Goodfellow et al., 2016)) For any filter coefficients K, the linear transform for the convolution by K is represented by the following doubly block circulant matrix:

 circ(K0,:) circ(K1,:) . . . circ(Kn-1,:) 

A

=

 



circ(Kn-1,:) ...

circ(K0,:) ...

... ...

circ(Kn-2,:) ...

 

.



circ(K1,:) circ(K2,:) . . . circ(K0,:)

That is, if X is an n × n matrix, and

(1)

then vec(Y ) = A vec(X).

ij, Yij =

Xi+p,j+q Kp,q

p[n] q[n]

(2)

So now we want to determine the singular values of a doubly block circulant matrix.

We will make use of the characterization of the eigenvalues and eigenvectors of doubly block circulant

matrices,

which

uses

the

following

definition:

Q

d=ef

1 n

(F



F).

Theorem 2 ((Jain, 1989)) For any n2 × n2 doubly block circulant matrix A, the eigenvectors of A are the columns of Q.

To get singular values in addition to eigenvalues, we need the following two lemmas. Lemma 3 ((Jain, 1989)) Q is unitary. Using Theorem 2 and Lemma 3, we can get the eigenvalues through the diagonal elements of QAQ. Lemma 4 The matrix A defined in equation 1 is normal, i.e., AT A = AAT .
Proof: AAT = AA = QDQQDQ = QDDQ = QDDQ = QDQQDQ = AA = AT A.

The following theorem characterizes the singular values of A as a simple function of K. As we will see, a characterization of the eigenvalues plays a major role. Chao (1974) provided a more technical characterization of the eigenvalues which may be regarded as making partial progress toward Theorem 5. However, we provide a proof from first principles, since it is the cleanest way we know to prove the theorem.

Theorem 5 For the matrix A defined in equation 1, the eigenvalues of A are the entries of F T KF , and its singular values are their magnitudes. That is, the singular values of A are

(F T KF )u,v : u, v  [n] .

(3)

Proof: By Theorems 2 and Lemma 3, the eigenvalues of A are the diagonal elements of QAQ =

1 n2

(F





F )A(F



F ).

If

we

view

(F 



F )A(F



F)

as

a

compound

n

×

n

matrix

of

n

×

n

blocks, for u, v  [n], the (un + v)th diagonal element is the vth element of the uth diagonal block.

Let us first evaluate the uth diagonal block. Using i, j to index blocks, we have

(QAQ)uu

=

1 n2

(F 



F )uiAij (F



F )ju

=

1 n2

-uiF circ(Kj-i,:)juF

i,j[n]

i,j[n]

1 = n2

u(j-i)F circ(Kj-i,:)F.

i,j[n]

(4)

To get the vth element of the diagonal of (4), we may sum the vth elements of the diagonals of each of its terms. Toward this end, we have

(F circ(Kj-i,:)F )vv =

-vrcirc(Kj-i,:)rssv =

v(s-r) Kj -i,s-r .

r,s[n]

r,s[n]

4

Under review as a conference paper at ICLR 2019

Substituting

into

(4),

we

get

1 n2

i,j,r,s[n] u(j-i)v(s-r)Kj-i,s-r. Collecting terms where j - i =

p and s - r = q, this is p,q[n] upvqKp,q = (F T KF )uv.

Since the singular values of any normal matrix are the magnitudes of its eigenvalues (Horn and Johnson, 2012), applying Lemma 4 completes the proof.
Note that F T KF is the 2D Fourier transform of K, and recall that ||A||2 is the largest singular value of A.

2.2 M -
Now, we consider case where the number m of channels may be more than one. We follow the notation of (Goodfellow et al., 2016). Assume we have a 4D kernel tensor K with element Kc,d,p,q giving the connection strength between a unit in channel d of the input and a unit in channel c of the output, with an offset of p rows and q columns between the input unit and the output unit. The input X  Rm×n×n; element Xd,i,j is the value of the input unit within channel d at row i and column j. The output Y  Rm×n×n has the same format as X, and is produced by

Ycrs =

Xd,r+p,s+q Kc,d,p,q .

d[m] p[n] q[n]

(5)

By inspection, vec(Y ) = M vec(X), where M is as follows

 B00

B01 . . . B0(m-1) 

M

=

 



B10 ...

B11 ...

... ...

B1(m-1) ...

 

,



B(m-1)0 B(m-1)1 . . . B(m-1)(m-1)

(6)

and each Bcd is a doubly block circulant matrix from Lemma 1 corresponding to the portion Kc,d,:,: of K that concerns the effect of the d-th input channel on the c-th output channel. (We can think of each output in the multichannel case as being a sum of single channel filters parameterized by one of the Kc,d,:,:'s.)
The following is our main result.

Theorem 6 For any K  Rm×m×n×n, if M is the matrix encoding the linear transformation computed by a convolutional layer parameterized by K, defined as in (6), then

(M ) =

 (F T Kc,d,:,:F )u,v cd .

u[n],v[n]

(7)

Before proving Theorem 6, let us take a moment to parse the RHS of (7). For each u  [n], v  [n], we use (F T Kc,d,:,:F )u,v cd to denote the m × m matrix whose cd entry is (F T Kc,d,:,:F )u,v. Then (M ) is the union of the singular values of all of these matrices.
The rest of this section is devoted to proving Theorem 6 through a series of lemmas.
The analysis of Section 2.1 implies that for all c, d  [m], Dcd d=ef QBcdQ is diagonal. Define

 D00

D01 . . . D0(m-1) 

L

d=ef

 



D10 ...

D11 ...

... ...

D1(m-1) ...

 

.



D(m-1)0 D(m-1)1 . . . D(m-1)(m-1)

(8)

Lemma 7 M and L have the same singular values.

5

Under review as a conference paper at ICLR 2019

Proof: We have

 B00 . . . B0(m-1)   QD00Q . . . QD0(m-1)Q 

M = 

...

...

...

= 

...

...

...

 

B(m-1)0 . . . B(m-1)(m-1)

QD(m-1)0Q . . . QD(m-1)(m-1)Q

 D00 . . . D0(m-1) 

= R 

...

...

...

 R = RLR, 

D(m-1)0 . . . D(m-1)(m-1)

where R d=ef Im  Q. Note that R is unitary because RR = (Im  Q)(Im  Q) = (ImIm)  (QQ) = Imn2 ;
this implies that M and L have the same singular values.

So now we have as a subproblem characterizing the singular values of a block matrix whose blocks
are diagonal. To express the the characterization, it helps to reshape the nonzero elements of L into a m × m × n2 tensor G as follows: Gcdw = (Dcd)ww.

Theorem 8 (L) =

 (G:,:,w) .

w[n2 ]

Proof: Choose an arbitrary w  [n2], and a (scalar) singular value  of G:,:,w whose left singular vector is x and whose right singular vector is y, so that G:,:,wy = x. Recall that ew  Rn2 is a
standard basis vector.

We claim that L(y  ew) = (x  ew). Since Dcd is diagonal, Dcdew = (Dcd)wwew = Gcdwew. Thus we have (L(y  ew))c = d[m] Dcdydew = ( d[m] Gcdwyd)ew = (G:,:,wy)cew = xcew,

which shows that

 D00

L(y



ew )

=

 



D10 ...

D(m-1)0

... ...
... ...

D0(m-1) D1(m-1)
... D(m-1)(m-1)

 
 
 


y0ew
... ym-1ew

 = 

x0ew
... xm-1ew



 

=

(x



ew ).

If ~ is another singular value of G:,:,w with a left singular vector x~ and a right singular vector y~, then (x  ew), (x~  ew) = x, x~ = 0 and, similarly (y  ew), (y~  ew) = 0. Also, (x  ew), (x  ew) = 1 and (y  ew), (y  ew) = 1.

For any x and x~, whether they are equal or not, if w = w~, then (x  ew), (x~  ew~) = 0, simply because their non-zero components do not overlap.

Thus, by taking the Kronecker product of each singular vector of G:,:,w with ew and assembling the results for various w, we may form a singular value decomposition of L whose singular values are w[n2](G:,:,w). This completes the proof.

Using Lemmas 7 and Theorem 8, we are now ready to prove Theorem 6.

Proof (of Theorem 6). Recall that, for each input channel c and output channel d, the diagonal

elements of Dc,d are the eigenvalues of Bc,d. By Theorem 5, this means that the diagonal elements

of Dc,d are

{(F T Kc,d,:,:F )u,v : u, v  [n]}.

(9)

The elements of (9) map to the diagonal elements of Dc,d as follows:

Gcdw = (Dc,d)ww = (F T Kc,d,:,:F ) w/m ,w mod m

and thus which in turn implies

G:,:,w = (F T Kc,d,:,:F ) w/m ,w mod m cd

(G:,:,w) = u[n],v[n] (F T Kc,d,:,:F )u,v cd .
w[n2 ]

Applying Lemmas 7 and 8 completes the proof.

6

Under review as a conference paper at ICLR 2019
3R
We now show how to use the spectrum computed above to project a convolution onto the set of convolutions with bounded operator norm. We exploit the following key fact.
Proposition 9 ((Lefkimmiatis et al., 2013)) Let A  Rn×n, and let A = U DV be its singular value decomposition. Let A~ = U D~ V , where, for all i  [n], D~ii = min(Dii, c) and B = {X | ||X||2  c}. Then A~ is the projection of A onto B; i.e. A~ = arg min ||A - X||F .
X B
This implies that the desired projection can be obtained by clipping the singular values of linear transformation associated with a convolutional layer to the interval [0, c]. Note that the eigenvectors remained the same in the proposition, hence the projected matrix is still generated by a convolution. However, after the projection, the resulting convolution neighborhood may become as large as n × n. On the other hand, we can project this convolution onto the set of convolutions with k × k neighborhoods, by zeroing out all other coefficients. NumPy code for this is in Appendix A.
Repeatedly alternating the two projections would give a point in the intersection of the two sets, i.e., a k × k convolution with bounded operator norm (Cheney and Goldstein, 1959; Boyd and Dattorro, 2003), and the projection onto that intersection could be found using the more complicated Dykstra's projection algorithm (Boyle and Dykstra, 1986).
When we wish to control the operator norm during an iterative optimization process, however, repeating the alternating projections does not seem to be worth it ­ we found that the first two projections already often produced a convolutional layer with an operator norm close to the desired value. Furthermore, because SGD does not change the parameters very fast, we can think of a given pair of projections as providing a warm start for the next pair.
In practice, we run the two projections once every few steps, thus letting the projection alternate with the training.
4E
First, we validated Theorem 6 with unit tests in which the output of the code given in the introduction is compared with evaluating the singular values by constructing the full matrix encoding the linear transformation corresponding to the convolutional layer and computing its SVD.
4.1 T
We generated 4D tensors of various shapes with random standard normal values, and computed their singular values using the full matrix method, the NumPy code given above and the equivalent TensorFlow code. For small tensors, the NumPy code was faster than TensorFlow, but for larger tensors, the TensorFlow code was able to exploit the parallelism in the algorithm and run much faster on a GPU. The timing results are shown in Figure 1.
4.2 R
We next explored the effect of regularizing the convolutional layers by clipping their operator norms as described in Section 3. We ran the CIFAR-10 benchmark with a standard 32 layer residual network with 2.4M training parameters; (He et al., 2016). This network reached a test error rate of 6.2% after 250 epochs, using a learning rate schedule determined by a grid search (shown by the gray plot in Figure 2). We then evaluated an algorithm that, every 100 steps, clipped the norms of the convolutional layers to various different values between 0.1 and 3.0. As expected, clipping to 2.5 and 3.0 had little impact on the performance, since the norms of the convolutional layers were between 2.5 and 2.8. Clipping to 0.1 yielded a surprising 6.7% test error, whereas clipping to 0.5 and 1.0 yielded test errors of 5.3% and 5.5% respectively (shown in Figure 2). A plot of test error against training time is provided in Figure 5 in Appendix B, showing that the projections did not slow down the training very much.
7

Under review as a conference paper at ICLR 2019

1,000 800

full matrix method NumPy implementation TensorFlow implementation

100 80

NumPy implementation TensorFlow implementation

Time (sec) Time (sec)

600 60

400 40

200 20

0 200 400 600 800 1,000 # Input/Output Channels

0 50 100 150 200 250 # Input/Output Channels

Figure 1: Time used to compute singular values. The left graph is for a 3 × 3 convolution on a 16 × 16 image with the number of input/output channels on the x-axis. The right graph is for a 11 × 11 convolution on a 64 × 64 image (no curve for full matrix method is shown as this method
could not complete in a reasonable time for these inputs).

Loss Test Error

0.4

no clipping

no clipping

1.5 clip at 0.5

clip at 0.5

clip at 1.0

0.3

clip at 1.0

1 0.2

0.5 0.1

0 50 100 150 200 250
# Epochs

0 50 100 150 200 # Epochs

Figure 2: Training loss and test error for ResNet model (He et al., 2016) for CIFAR-10.

4.3 R
The baseline algorithm studied in the previous subsection used batch normalization. Batch normalization tends to make the network less sensitive to linear transformations with large operator norms. However, batch normalization includes trainable scaling parameters (called  in the original paper) that are applied after the normalization step. The existence of these parameters lead to a complicated interaction between batch normalization and methods like ours that act to control the norm of the linear transformation applied before batch normalization.
Because the effect of regularizing the operator norm is more easily understood in the absence of batch normalization, we also performed experiments with a baseline that did not use batch normalization.
Another possibility that we wanted to study was that using a regularizer may make the process overall more stable, enabling a larger learning rate. We were generally interested in whether operator-norm regularization made the training process more robust to the choice of hyperparameters.
In one experiment, we started with the same baseline as the previous subsection, but disabled batch normalization. This baseline started with a learning rate of 0.1, which was multiplied by a factor 0.95 after every epoch. We tried all combinations of the following hyperparameters: (a) the norm of the ball projected onto (no projection, 0.5, 1.0, 1.5, 2.0); (b) the initial learning rate (0.001, 0.003, 0.01, 0.03, 0.1); (c) the minibatch size (32, 64); (d) the number of epochs per decay of the learning
8

Under review as a conference paper at ICLR 2019
rate (1,2,3). We tried each of the 150 combinations of the hyperparameters, trained for 100 epochs, and measured the test error. The results are plotted in Figure 3a. The operator norm regularization

(a) Without batch normalization

(b) With batch normalization

Figure 3: A scatterplot of the test errors obtained with different hyperparameter combinations, and different operator-norm regularizers.

improved the best result, and also made the process more robust to the choice of hyperparameters.
We conducted a similar experiment in the presence of batch normalization, except using learning rates 0.01, 0.03, 0.1, 0.2, and 0.3. Those results are shown in Figure 3b. Regularizing the operator norm helps, even in the presence of batch normalization.
It appears that operator-norm regularization and batch normalization are not redundant, and neither dominates the other. We were surprised by this.

4.4 T

- RN

We computed the singular values of the con-

volutional layers from the official "Resnet V2"

pre-trained model (He et al., 2016). These are

plotted in Figure 4. The singular values are

ordered by value. Only layers with kernels larger than 1 × 1 are plotted. The curves are plotted

with a mixture of red and green; layers closer to

the input are plotted with colors with a greater

share of red. The transformations with the largest

operator norms are closest to the inputs. As the

data has undergone more rounds of processing, Figure 4: Plot of the singular values of the linear

as we proceed through the layers, the number operators associated with the convolutional layers

of non-negligible singular values increases for a of the pretrained "ResNet V2" from the TensorFlow

while, but at the end, it tapers off.

website.

R

Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6240­6249, 2017.

Stephen Boyd and Jon Dattorro.

Alternating

https://web.stanford.edu/class/ee392o/alt_proj.pdf.

projections,

2003.

James P Boyle and Richard L Dykstra. A method for finding projections onto the intersection of convex sets in hilbert spaces. In Advances in order restricted statistical inference, pages 28­47. Springer, 1986.
Chong-Yun Chao. A note on block circulant matrices. Kyungpook Mathematical Journal, 14:97­100, 1974.
Ward Cheney and Allen A. Goldstein. Proximity maps for convex sets. Proceedings of the American Mathematical Society, 10(3):448­450, 1959. ISSN 00029939, 10886826. URL http://www.jstor.org/ stable/2032864.

9

Under review as a conference paper at ICLR 2019
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. ICML, 2017.
Harris Drucker and Yann Le Cun. Improving generalization performance using double backpropagation. IEEE Transactions on Neural Networks, 3(6):991­997, 1992.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www. deeplearningbook.org.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018a.
Henry Gouk, Bernhard Pfahringer, Eibe Frank, and Michael Cree. MaxGain: Regularisation of neural networks by constraining activation magnitudes. arXiv preprint arXiv:1804.05965, 2018b.
Robert M Gray. Toeplitz and circulant matrices: A review. Foundations and Trends® in Communications and Information Theory, 2(3):155­239, 2006.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pages 630­645. Springer, 2016. http://download.tensorflow.org/models/official/resnet_v2_imagenet_checkpoint.tar.gz; downloaded on on 5/1/18.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In Advances in Neural Information Processing Systems, pages 2266­2276, 2017.
Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universität München, 91:1, 1991.
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jürgen Schmidhuber, et al. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.
Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, New York, NY, USA, 2nd edition, 2012. ISBN 0521548233, 9780521548236.
Anil K Jain. Fundamentals of digital image processing. Englewood Cliffs, NJ: Prentice Hall,, 1989.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Stamatios Lefkimmiatis, John Paul Ward, and Michael Unser. Hessian schatten-norm regularization for linear inverse problems. IEEE transactions on image processing, 22(5):1873­1888, 2013.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. ICLR, 2018.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pages 4788­4798, 2017.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.
William Henry Young. On the multiplication of successions of fourier constants. Proc. R. Soc. Lond. A, 87(596): 331­339, 1912.
10

Under review as a conference paper at ICLR 2019

AN P
def Clip_OperatorNorm(kernel, input_shape, clip_to): transform_coefficients = np.fft.fft2(kernel, input_shape, axes=[0, 1]) U, D, V = np.linalg.svd(transform_coefficients, compute_uv=True, full_matrices=False) D_clipped = np.minimum(D, clip_to) if kernel.shape[2] > kernel.shape[3]: clipped_transform_coefficients = np.matmul(U, D_clipped[..., None] * V) else: clipped_transform_coefficients = np.matmul(U * D_clipped[..., None, :], V) clipped_kernel = np.fft.ifft2(clipped_transform_coefficients, axes=[0, 1]).real return clipped_kernel[np.ix_(*[range(d) for d in kernel.shape])]

BT

.

Figure 5 shows the plots of test error vs. training time in our CIFAR-10 experiment.

0.4 no clipping clip at 0.5
0.3 clip at 1.0

Test Error

0.2

0.1

0 1

234 Time(seconds)

5 ·104

Figure 5: Test error vs. training time for ResNet model (He et al., 2016) for CIFAR-10.

11

