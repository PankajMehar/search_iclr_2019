Under review as a conference paper at ICLR 2019
HIERARCHICALLY CLUSTERED REPRESENTATION LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years. In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations. To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space. Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model. Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components. This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy. In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features. We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.
1 INTRODUCTION
Clustering is one of the most traditional and frequently used machine learning tasks. Clustering models are designed to represent intrinsic data structures, such as latent Dirichlet allocation (Blei et al., 2003). The recent development of representation learning has contributed to generalizing model feature engineering, which also enhances data representation (Bengio et al., 2013). Therefore, representation learning has been merged into the clustering models, e.g., variational deep embedding (VaDE) (Jiang et al., 2017). Besides merging representation learning and clustering, another critical line of research is structuring the clustering result, e.g., hierarchical clustering. This paper introduces a unified model enabling nonparametric Bayesian hierarchical clustering with neural-network-based representation learning.
Autoencoder (Rumelhart et al., 1985) is a typical neural network for unsupervised representation learning and achieves a non-linear mapping from a high-dimensional input space to a lowdimensional embedding space by minimizing reconstruction errors. To turn the low-dimensional embeddings into random variables, a variational autoencoder (VAE) (Kingma & Welling, 2014) places a Gaussian prior on the embeddings. The autoencoder, whether it is probabilistic or not, has a limitation in reflecting the intrinsic hierarchical structure of data. For instance, VAE assuming a single Gaussian prior needs to be expanded to suggest an elaborate clustering structure.
Due to the limitations of modeling the cluster structure with autoencoders, prior works combine the autoencoder and the clustering algorithm. While some early cases pipeline just two models, e.g., Huang et al. (2014), a typical merging approach is to model an additional loss, such as a clustering loss, in the autoencoders (Xie et al., 2016; Guo et al., 2017; Yang et al., 2017; Nalisnick et al., 2016; Chu & Cai, 2017; Jiang et al., 2017). These suggestions exhibit gains from unifying the encoding and the clustering, yet they remain at the parametric and flat-structured clustering. A more recent development releases the previous constraints by using the nonparametric Bayesian approach.
1

Under review as a conference paper at ICLR 2019

(a) Hierarchically clustered embeddings

(b) Reconstructed images

(c) Level proportion features of real images

Figure 1: Example of hierarchically clustered embeddings on MNIST with three levels of hierarchy, the reconstructed digits from the hierarchical Gaussian mixture components, and the extracted level proportion features. We marked the mean of a Gaussian mixture component with the colored square, and the digit written inside the square refers to the unique index of the mixture component.

For example, the infinite mixture of VAEs (IMVAE) (Abbasnejad et al., 2017) explores the infinite space for VAE mixtures by looking for an adequate embedding space through sampling, such as the Chinese restaurant process (CRP). Whereas IMVAE remains at the flat-structured clustering, VAEnested CRP (VAE-nCRP) (Goyal et al., 2017) captures a more complex structure, i.e., a hierarchical structure of the data, by adopting the nested Chinese restaurant process (nCRP) prior (Griffiths et al., 2004) into the cluster assignment of the Gaussian mixture model.
This paper proposes hierarchically clustered representation learning (HCRL) that is a joint model of 1) nonparametric Bayesian hierarchical clustering, and 2) representation learning with neural networks. HCRL extends a previous work on merging flat clustering and representation learning, i.e., VaDE, by incorporating inter-cluster relation modelings. Unlike a previous work of VAE-nCRP, HCRL learns the full spectrum of hierarchical clusterings, such as the level assignment and the level proportion of generating a component hierarchy. These level assignments and proportions were not modeled in VAE-nCRP, so each data instance cannot be analyzed from the perspective of generalization and specialization in a hierarchy. On the contrary, by adding level assignment and proportion modeling, a data instance can be generated from an internal component of the hierarchy, which is limited to the leaf component in VAE-nCRP. Hierarchical mixture density estimation (Vasconcelos & Lippman, 1999), where all internal and leaf components are directly modeled to generate data, is a flexible framework for hierarchical mixture modeling, such as hierarchical topic modeling (Mimno et al., 2007; Griffiths et al., 2004), with regard to the learning of the internal components.
Specifically, HCRL jointly optimizes soft-divisive hierarchical clustering in an embedding space from VAE via two mechanisms. First, HCRL includes a hierarchical-versioned Gaussian mixture model (HGMM) with a mixture of hierarchically organized Gaussian distributions. Then, HCRL sets the prior of embeddings by adopting the generative processes of HGMM. Second, to handle a dynamic hierarchy structure dealing with the clusters of unequal sizes, we explore the infinite hierarchy space by exploiting an nCRP prior. These mechanisms are fused as a unified objective function; this is done rather than concatenating the two distinct models of clustering and autoencoding. The quantitative evaluations focus on density estimation quality and hierarchical clustering accuracy, which shows that HCRL has competent likelihoods and the best accuracies compared with the baselines. When we observe our results qualitatively, we visualize 1) the hierarchical clusterings, 2) the embeddings under the hierarchy modeling, and 3) the reconstructed images from each Gaussian mixture component, as shown in Figure 1. These experiments were conducted by crossing the data domains of texts and images, so our benchmark datasets include MNIST, CIFAR-100, RCV1 v2, and 20Newsgroups.
2 PRELIMINARIES
2.1 VARIATIONAL DEEP EMBEDDING
Figure 2 presents a graphical representation and a neural architecture of VaDE (Jiang et al., 2017). The model parameters of , µ1:K , and 12:K , which are a proportion, means, and covariances of
2

Under review as a conference paper at ICLR 2019

Figure 2: Graphical representation of VaDE (Jiang et al., 2017) (left), VAE-nCRP (Goyal et al., 2017) (center), and neural architecture of both models (right). In the graphical representation, the white/shaded circles represent latent/observed variables. The black dots indicate hyper or variational parameters. The solid lines represent a generative model, and dashed lines represent a variational approximation. A rectangle box means a repetition for the number of times denoted by the bottom right of the box.

mixture components, respectively, are declared outside of the neural network1. VaDE trains model parameters to maximize the lower bound of marginal log likelihoods via the mean-field variational inference (Jordan et al., 1999). VaDE uses the Gaussian mixture model (GMM) as the prior, whereas VAE assumes a single standard Gaussian distribution on embeddings. Following the generative process of GMM, VaDE assumes that 1) the embedding draws a cluster assignment, and 2) the embedding is generated from the selected Gaussian mixture component.

VaDE uses an amortized inference as VAE, with a generative and inference networks; L(x) in Equa-

tion 1 denotes the evidence lower bound (ELBO), which is the lower bound on the log likelihood. It

should be noted that VaDE merges the ELBO of VAE with the likelihood of GMM.

log p(x)  L(x) = Eq

p(c, z, x) log
q(c, z|x)

= Eq

log

K c=1

cN (z|µc, c2IJ ) p(c|z)N (z|µ, 2IJ )

+

log

p(x|z)

(1)

2.2 VARIATIONAL AUTOENCODER NESTED CHINESE RESTAURANT PROCESS

VAE-nCRP uses the nonparametric Bayesian prior for learning tree-based hierarchies, the nCRP

(Griffiths et al., 2004), so the representation could be hierarchically organized. The nCRP prior

defines the distributions over children components for each parent component, recursively in a top-

down way. The variational inference of the nCRP can be formalized by the nested stick-breaking

construction (Wang & Blei, 2009), which is also kept in the VAE setting. The distribution over paths

on the hierarchy is defined as being proportional to the product of weights corresponding to the nodes

lying in each path. The weight, i, for the i-th node follows the Griffiths-Engen-McCloskey (GEM)

distribution (Pitman et al., 2002), where i is constructed as i = vi

i-1 j=1

(1

-

vj

),

vi



Beta(1, )

by a stick-breaking process. Since the nCRP provides the ELBO with the nested stick-breaking

process, VAE-nCRP has a unified ELBO of VAE and the nCRP in Equation 2.

L(x) = Eq

log

p(v) q(v|x)

+ log

p(par(p)|)p(p|par(p), N2 ) q(p, par(p)|x)

p(|v) q(|x)

p(z|p, , D2 ) q(z|x)

+ log p(x|z)

(3.1)

(3.2)

(2)

Given the ELBO of VAE-nCRP, we recognized a number of potential improvements. First, term (3.1) is for modeling the hierarchical relationship among clusters, i.e., each child is generated from its parent. VAE-nCRP trade-off is the direct dependency modeling among clusters against the meanfield variational approach. This modeling may reveal that the higher clusters in the hierarchy are more difficult to train. Second, in term (3.2), leaf mixture components generate embeddings, which implies that only leaf clusters have direct summarization ability for sub-populations. Additionally, in term (3.2), variance parameter D2 is modeled as the hyperparameter shared by all clusters. In other words, only with J-dimensional parameters, , for the leaf mixture components, the local density modeling without variance parameters has a critical disadvantage.

For all of these weaknesses, we were able to compensate with the level proportion modeling and HGMM prior. The level assignment generated from the level proportion allows a data instance to

1Appendix D enumerates the symbols in this paper.

3

Under review as a conference paper at ICLR 2019

Figure 3: A simple depiction (left) of the key notations, where each numbered circle refers to the corresponding Gaussian mixture component. The graphical representation (center) and the neural architecture (right) of our proposed model, HCRL. The neural architecture of HCRL consists of two probabilistic encoder networks, g and gz , and one probabilistic decoder network, f.

select among all mixture components. We do not need direct dependency modeling between the parents and their children because all internal mixture components also generate embeddings.

3 METHODOLOGY

3.1 GENERATIVE PROCESS

The generative process of HCRL resembles the generative process of hierarchical clusterings, such as the hierarchical latent Dirichlet allocation (Griffiths et al., 2004). In detail, the generative process departs from selecting a path , from the nCRP prior (phase 1). Then, we sample a level proportion (phase 2) and a level, l (phase 3), from the sampled level proportion to find the mixture component in the path, and this component of l provides the Gaussian distribution for the latent representation (phase 4). Finally, the latent representation is exploited to generate an observed datapoint (phase 5). The below formulas are the generative process with its density functions. In addition, Figure 3 illustrates a graphical representation corresponding to the described generative process. The generative process also presents our formalization of corresponding prior distributions, denoted as p(·), and variational distributions, denoted as q(·), by generation phases. The variational distributions are used in our inference methods called mean-field variational inference (MFVI) (Jordan et al., 1999) as detailed in Section 3.3.

1. Choose a path   nCRP(|)

· p() =

L l=1

1,2 ,...,l

where

1,2 ,...,l

=

l l

=1 {v1,2 ,...,l

(

l -1 j=1

(1

-

v1,2 ,...,j

))},

q(|x)  S

child() S

2. Choose a level proportion   Dirichlet(|)

· p() = Dir(|), q (|x) = Dirichlet(|)  LogisticNormal(|µ, 2 IL)

where

[µ; log 2 ]

=

g (x),

l

=

1 2l

(1

-

2 L

+

e-µl L2

l e-µl )

3. Choose a level l  Multinomial(l|)

· p(l) = Multinomial(), q(l|x) = Multinomial(l|)

where l  exp (l) - (0) +  S

J j=1

-

1 2

log(22l,j )

-

z2j 22l ,j

-

(µzj -µl,j )2 22l ,j

4. Choose a latent representation z  N (z|µl , 2l IJ )

· p(z) = ,l p(|) · l · N (z|µl , 2l IJ ),

qz (z|x) = N (z|µz, z2IJ ) where [µz; log z2] = gz (x)

5. Choose an observed datapoint x  N x|µx, x2 ID where [µx; log x2 ] = f(z)2

3.2 NEURAL ARCHITECTURE
The neural architecture of HCRL consists of two probabilistic encoders on z and , and one probabilistic decoder on z as shown in the right part of Figure 3. This unbalanced architecture originates
2We introduce the sample distribution for the real-valued data instances, and Appendix F provides the binary case as well, which we use for MNIST.

4

Under review as a conference paper at ICLR 2019

from our modeling assumption of p(x|z), not p(x|z, ). The reconstruction design of x depending on the two stochastic variables of z and  may lead to a large variance of the reconstruction on x. Additionally, we cannot guarantee that both z and  contribute to the the reconstruction on x (Chen et al., 2016). Although the decoding structure of  is not included explicitly in the neural network architecture of HCRL, we provide the formalization of p(|z) in Table 1 according to our generative assumptions. We call this reconstruction process, which is inherently a generative process of the traditional probabilistic graphical model (PGM), PGM reconstruction (see the decoding neural network part of Figure 3).
Table 1: Encoding and decoding structure on z and  in HCRL. (s) indicates the s-th sample.

Encoding

Decoding

z z  qz (z|x), z(s) = gz ( (s), x)

x  p(x|z), x(s) = f(z(s))

   q (|x), (s) = g ( (s), x) p(x|)  v,z ,l p(x|z)p(z|, l)p(l|)p(|v)p(v)

3.3 MEAN-FIELD VARIATIONAL INFERENCE

The formal specification can be a factorized probabilistic model as Equation 3, where  =

{v, , , l, z} denotes the set of latent variables, and MT denotes the set of all nodes in tree T .
N

p(, x) =

p(vj | )

p(vi|) p(n|v)p(n|)p(ln|n)p(zn|n, ln)p(xn|zn) (3)

j/MT

iMT

n=1

The proportion and assignment on the mixture components for the n-th data instance are modeled by

n as a path assignment; n as a level proportion; and ln as a level assignment. v is a Beta draw used in the stick-breaking construction. The latent variables are inferred through MFVI, and therefore we

assume the variational distributions are as Equation 4:

N

q(|x) =

p(vj | )

q(vi|ai, bi) q(n|xn)q (n|xn)q(ln|n, xn)qz (zn|xn) (4)

j/MT

iMT

n=1

where q (n|xn) and qz (zn|xn) should be noted because these two variational distributions

follow the amortized inference of VAE. q(|x)  S

child() S is the variational distribution

over path , where child() means the set of all full paths that are not in T but include  as a sub

path. Because we specified both generative and variational distributions, we define the ELBO of

HCRL, L = Eq

log

p(,x) q(|x)

, in Equation 5. Appendix F enumerates the full derivation in detail.

We report that the Laplace approximation with the logistic normal distribution is applied to model

the prior, , of the level proportion, . We choose a conjugate prior of a multinomial, so p(n|) follows the Dirichlet distribution. To configure the inference network on the Dirichlet prior, the

Laplace approximation is used (MacKay, 1998; Srivastava & Sutton, 2017; Hennig et al., 2012).

L(x) = Eq

p(v) p() log q(v|x) + log q(|x) + log

p(|v) p(l|) p(z|µl , 2l ) + log p(x|z) q(|x) q(l|x) q(z|x)

(5)

,l

3.4 TRAINING ALGORITHM OF CLUSTERING HIERARCHY
This model is formalized according to the stick-breaking process scheme. Unlike the CRP, the stickbreaking process does not represent the direct sampling of the mixture component at the data instance level. Therefore, it is necessary to devise a heuristic algorithm for operations, such as GROW, PRUNE, and MERGE, to refine the hierarchy structure. Appendix C provides details about each operation, together with the overall training algorithm of HCRL. In the below description, an inner path and a full path refer to the path ending with an internal node and a leaf node, respectively.
· GROW expands the hierarchy by creating a new branch under the heavily weighted internal node. Compared with the work of Wang & Blei (2009), we modified GROW to first sample a path, ,

proportional to n q(n =  ), and then to grow the path if the sampled path is an inner path.

5

Under review as a conference paper at ICLR 2019
· PRUNE cuts a randomly sampled minor full path, , satisfying n q(n=) < , where  is
n, q(n=)
the pre-defined threshold. If the removed leaf node of the full path is the last child of the parent node, we also recursively remove the parent node. · MERGE combines two full paths, (i) and (j), with similar posterior probabilities, measured by J ((i), (j)) = qiqjT /|qi||qj|, where qi = [q(1 = (i)), · · · , q(N = (i))].
4 EXPERIMENTS
4.1 DATASETS AND BASELINES
Datasets: We used various hierarchically organized benchmark datasets as well as MNIST.
· MNIST (LeCun et al., 1998): 28x28x1 handwritten image data, with 60,000 train images and 10,000 test images. We reshaped the data to 784-d in one dimension.
· CIFAR-100 (Krizhevsky & Hinton, 2009): 32x32x3 colored images with 20 coarse and 100 fine classes. We used 3,072-d flattened data with 50,000 training and 10,000 testing.
· RCV1 v2 (Lewis et al., 2004): The preprocessed text of the Reuters Corpus Volume. We preprocessed the text by selecting the top 2,000 tf-idf words. We used the hierarchical labels up to the 4-level, and the multi-labeled documents were removed. The final preprocessed corpus consists of 11,370 training and 10,000 testing documents randomly sampled from the original test corpus.
· 20Newsgroups (Lang, 1995): The benchmark text data extracted from 20 newsgroups, consisting 11,314 training and 7,532 testing documents. We also labeled by 4-level following the annotated hierarchical structure. We preprocessed the data through the same process as that of RCV1 v2.
Baselines: We completed our evaluation in two aspects: 1) optimizing the density estimation, and 2) clustering the hierarchical categories. First, we evaluated HCRL from the density estimation perspective by comparing it with diverse flat clustered representation learning models, and VAE-nCRP. Second, we tested HCRL from the accuracy perspective by comparing it with multiple divisive hierarchical clusterings. The below is the list of baselines. We also added the two-stage pipeline approaches, where we trained features from VaDE first and then applied the hierarchical clusterings. We reused the open source codes3 provided by the authors for several baselines, such as IDEC, DCN, VAE-nCRP, and SSC-OMP.
1. Variational Autoencoder (VAE) (Kingma & Welling, 2014) 2. Variational Deep Embedding (VaDE) (Jiang et al., 2017) 3. Improved Deep Embedded Clustering (IDEC) (Guo et al., 2017): improves DEC (Xie et al.,
2016) by attatching decoder structure. We use the code by the authors. 4. Deep Clustering Network (DCN) (Yang et al., 2017): optimizes the K-means-related cost de-
fined on the embedding space. We used the open source code provided by the authors. 5. Infinite Mixture of Variational Autoencoders (IMVAE) (Abbasnejad et al., 2017): searches
for the infinite embedding space by using a Bayesian nonparametric prior. 6. Variational Autoencoder - nested Chinese Restaurant Process (VAE-nCRP) (Goyal et al.,
2017): We used the open source code provided by the authors. 7. Hierarchical K-means (HKM) (Nister & Stewenius, 2006): performs K-means (Lloyd, 1982)
recursive in a top-down way. 8. Mixture of Hierarchical Gaussians (MOHG) (Vasconcelos & Lippman, 1999): infers the
level-specific mixture of Gaussians. 9. Recursive Gaussian Mixture Model (RGMM): runs GMM recursively in a top-down manner. 10. Recursive Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit (RSS-
COMP): performs SSC-OMP (You et al., 2016) recursively for hierarchical clustering. SSCOMP is a well-known methods for image clustering, and we used the open source code.
4.2 QUANTITATIVE ANALYSIS
We used two measures to evaluate the learned representations in terms of the density estimations: 1) negative log likelihood (NLL), and 2) reconstruction errors (REs). Autoencoder models, such as
3https://github.com/XifengGuo/IDEC (IDEC); https://github.com/boyangumn/DCN (DCN); https://github.com/prasoongoyal/bnp-vae (VAE-nCRP); http://vision.jhu.edu/code/ (SSC-OMP)
6

Under review as a conference paper at ICLR 2019

IDEC and DCN, were tested only for the REs. The NLL is estimated with 100 samples. Table 2 indicates that HCRL is best in the NLL and is competent in the REs which means that the hierarchically clustered embeddings preserve the intrinsic raw data structure.
Table 2: Test set performance of the negative log likelihood (NLL) and the reconstruction errors (REs). Replicated ten times, and the best in bold. P  < 0.05 (Student's t-test). Model-L# means that the model trained with the #-depth hierarchy.

Model VAE VaDE
IDEC DCN
IMVAE VAE-nCRP-L3 VAE-nCRP-L4
HCRL-L3 HCRL-L4

MNIST NLL REs 230.71 10.46 217.20 10.35
N/A 12.75 N/A 11.30
296.57 10.69 718.78 32.67 721.00 32.53 203.24 8.70 203.91 8.16

CIFAR-100

NLL

REs

1960.06 57.54

1921.85 53.60

N/A 64.09

N/A 44.26 1992.83 40.45

2969.62 198.66

2950.73 1843.40 1849.13

198.97 50.44 50.47

RCV1 v2

NLL

REs

2559.46 1434.59

2558.32 1426.38

N/A 1376.26

N/A 1361.98

2566.01 1387.02

2642.88 1538.42

2646.48 1542.81 2554.50 1395.05 2535.43 1353.34

20Newsgroups

NLL

REs

2735.80 1788.22

2733.46 1782.86 N/A 1660.61

N/A 1691.17

2722.81 1718.08

2712.28 1680.56

2713.58 1680.71

2726.75 1828.71

2702.88 1711.30

VaDE generally performed better than VAE did, whereas other flat clustered representation learning models tended to be slightly different for each dataset. HCRL showed overall competent performance and better results with a deeper hierarchy of level four than of level three, which implies that capturing the deeper hierarchical structure is likely to be useful for the density estimation.
Additionally, we evaluated hierarchical clustering accuracies by following Xie et al. (2016), except for MNIST that is flat structured. Table 3 points out that HCRL has significantly better microaveraged F-scores compared with every baseline. HCRL is able to reproduce the ground truth hierarchical structure of the data, and this trend is consistent when HCRL compared with the pipelined model, such as VaDE with a clustering model. The result of the comparisons with the clustering models, such as HKM, MOHG, RGMM, and RSSCOMP, is interesting because it experimentally proves that the joint optimization of hierarchical clustering in the embedding space improves hierarchical clustering accuracies. HCRL also presented better hierarchical accuracies than VAE-nCRP. We conjecture the reasons for the modeling aspect of VAE-nCRP: 1) the simplified prior modeling on the variance of the mixture component as just constants, and 2) the non-flexible learning of the internal components.
Table 3: Hierarchical clustering accuracies with F-scores, on CIFAR-100 with a depth of three, RCV1 v2 with a depth of four, and 20Newsgroups with a depth of four. Replicated ten times, and a confidence interval with 95%. Best in bold.

Model HKM MOHG RGMM RSSCOMP VAE-nCRP VaDE+HKM VaDE+MOHG VaDE+RGMM VaDE+RSSCOMP HCRL

CIFAR-100
0.1620±0.0077 0.0846±0.0378 0.1686±0.0115 0.1461±0.0228 0.2011±0.0076 0.1637±0.0116 0.1659±0.0155 0.1806±0.0132 0.1923±0.0211 0.2245±0.0137

RCV1 v2
0.2564±0.0679 0.1026±0.0135 0.2743±0.0521 0.2657±0.0545 0.4128±0.0242 0.3308±0.0664 0.4227±0.0927 0.3858±0.0615 0.2718±0.0444 0.4553±0.0295

20Newsgroups
0.4088±0.0426 0.0402±0.0119 0.4351±0.0369 0.2953±0.0474 0.5584±0.0267 0.4850±0.0558 0.4915±0.0713 0.4095±0.0651 0.2905±0.0431 0.6008±0.0973

4.3 QUALITATIVE ANALYSIS
MNIST: In Figure 1, the digits {4, 7, 9} and the digits {3, 8} are grouped together with a clear hierarchy, which was consistent between HCRL and VaDE. Also, some digits {0, 4, 2} in a round form are grouped, together, in HCRL. In addition, among the reconstructed digits from the hierarchical mixture components, the digits generated from the root have blended shapes from 0 to 9, which is natural considering the root position.

7

Under review as a conference paper at ICLR 2019
CIFAR-100: Figure 4 shows the hierarchical clustering results on CIFAR-100. Given that there were no semantic inputs from the data, the color was dominantly reflected in the clustering criteria. However, if one observes the second hierarchy, the scene images of the same sub-hierarchy are semantically consistent, although the background colors are slightly different.
Figure 4: Example extracted sub-hierarchies on CIFAR-100 RCV1 v2: Figure 5 shows the embedding of RCV1 v2. VAE and VaDE show no hierarchy, and close sub-hierarchies are distantly embedded. VAE-nCRP guides the internal mixture components to be agglomerated at the center, and the cause of agglomeration is the generative process of VAEnCRP, where the parameter of the internal components are inferred without direct information from data. HCRL shows a clear separation between the sub-hierarchy without the agglomeration.

(a) VAE (Kingma & Welling, 2014)

(b) VaDE (Jiang et al., 2017)

(c) VAE-nCRP (Goyal et al., 2017)

(d) HCRL

Figure 5: Comparison of embeddings on RCV1 v2, plotted using t-SNE (Maaten & Hinton, 2008). We mark the mean of a mixture component with a numbered square, colored in {red} for VaDE, {red (root), green (internal), blue (leaf)} for VAE-nCRP and HCRL. The first-level sub-hierarchies
are indicated with four colors.

20Newsgroups: Figure 6 shows the example sub-hierarchies on 20Newsgroups. We enumerated topic words from documents with top-five likelihoods for each cluster, and we filtered the words by tf-idf values. We observe relatively more general contents in the internal clusters than in the leaf clusters of each internal cluster.

Figure 6: Example extracted sub-hierarchies on 20Newsgroups
5 CONCLUSION
In this paper, we have introduced a hierarchically clustered representation learning framework for the hierarchical mixture density estimation on deep embeddings. HCRL aims at encoding the relations among clusters as well as among instances to preserve the internal hierarchical structure of data. The main differentiated features of HCRL are 1) the crucial assumption regarding the internal mixture components for having the ability to generate data directly, and 2) the unbalanced autoencoding neural architecture for the level proportion modeling as the encoding structure, and the probabilistic model as the decoding structure. From the modeling and the evaluation, we found that HCRL enables the improvements due to the high flexibility modeling compared with the baselines.
8

Under review as a conference paper at ICLR 2019
REFERENCES
M Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel. Infinite variational autoencoder for semi-supervised learning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 781­790. IEEE, 2017.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798­1828, 2013.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993­1022, 2003.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems, pp. 2172­2180, 2016.
Wenqing Chu and Deng Cai. Stacked similarity-aware autoencoders. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pp. 1561­1567. AAAI Press, 2017.
Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, and Eric Xing. Nonparametric variational auto-encoders for hierarchical representation learning. arXiv preprint arXiv:1703.07027, 2017.
Thomas L Griffiths, Michael I Jordan, Joshua B Tenenbaum, and David M Blei. Hierarchical topic models and the nested chinese restaurant process. In Advances in neural information processing systems, pp. 17­24, 2004.
Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with local structure preservation. In International Joint Conference on Artificial Intelligence (IJCAI17), pp. 1753­1759, 2017.
Philipp Hennig, David Stern, Ralf Herbrich, and Thore Graepel. Kernel topic models. In Artificial Intelligence and Statistics, pp. 511­519, 2012.
Peihao Huang, Yan Huang, Wei Wang, and Liang Wang. Deep embedding network for clustering. In Pattern Recognition (ICPR), 2014 22nd International Conference on, pp. 1532­1537. IEEE, 2014.
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep embedding: An unsupervised and generative approach to clustering. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pp. 1965­1972. AAAI Press, 2017.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183­233, 1999.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the Second International Conference on Learning Representations (ICLR 2014), April 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Ken Lang. Newsweeder: Learning to filter netnews. In Machine Learning Proceedings 1995, pp. 331­339. Elsevier, 1995.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection for text categorization research. Journal of machine learning research, 5(Apr):361­397, 2004.
Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2): 129­137, 1982.
9

Under review as a conference paper at ICLR 2019
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579­2605, 2008.
David JC MacKay. Choice of basis for laplace approximation. Machine learning, 33(1):77­86, 1998.
David Mimno, Wei Li, and Andrew McCallum. Mixtures of hierarchical topics with pachinko allocation. In Proceedings of the 24th international conference on Machine learning, pp. 633­640. ACM, 2007.
Eric Nalisnick, Lars Hertel, and Padhraic Smyth. Approximate inference for deep latent gaussian mixtures. In NIPS Workshop on Bayesian Deep Learning, volume 2, 2016.
David Nister and Henrik Stewenius. Scalable recognition with a vocabulary tree. In Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pp. 2161­2168. Ieee, 2006.
Jim Pitman et al. Combinatorial stochastic processes. Technical report, Technical Report 621, Dept. Statistics, UC Berkeley, 2002. Lecture notes for St. Flour course, 2002.
Abel Rodriguez, David B Dunson, and Alan E Gelfand. The nested dirichlet process. Journal of the American Statistical Association, 103(483):1131­1154, 2008.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.
Akash Srivastava and Charles Sutton. Autoencoding variational inference for topic models. arXiv preprint arXiv:1703.01488, 2017.
Naonori Ueda, Ryohei Nakano, Zoubin Ghahramani, and Geoffrey E Hinton. Smem algorithm for mixture models. In Advances in neural information processing systems, pp. 599­605, 1999.
Nuno Vasconcelos and Andrew Lippman. Learning mixture hierarchies. In Advances in Neural Information Processing Systems, pp. 606­612, 1999.
Chong Wang and David M Blei. Variational inference for the nested chinese restaurant process. In Advances in Neural Information Processing Systems, pp. 1990­1998, 2009.
Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In International conference on machine learning, pp. 478­487, 2016.
Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces: Simultaneous deep learning and clustering. In International Conference on Machine Learning, pp. 3861­3870, 2017.
Chong You, Daniel Robinson, and Rene´ Vidal. Scalable sparse subspace clustering by orthogonal matching pursuit. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3918­3927, 2016.
10

Under review as a conference paper at ICLR 2019
A SYNTHETIC DEMO
We created a synthetic dataset that has a hierarchical structure and is sampled from the 50dimensional Gaussian distributions, presented in Figure 7. The hierarchy, which has a branch factor of two and a depth of four, has a total of eight leaf clusters. Figure 7a shows the raw synthetic dataset in the input space of R50, and after running HCRL, we plot the hierarchically clustered embeddings in the latent space in Figure 7b. In addition to the embeddings, we also present a confidence ellipse with dashed lines for each learned Gaussian mixture component. Because the root component is involved in generating all of the data, it forms a large ellipse, while the leaf component summarizes the local density, so the small ellipse is learned.

(a) Raw synthetic data  R50

(b)

Hierarchically

clustered

embeddings



2
R

Figure 7: Synthetic data in the input space of R50 (left), which is visualized via t-SNE (Maaten & Hinton, 2008), and hierarchically clustered embeddings in the latent space of R2 (right). We
additionally show a 95% confidence ellipse with a dashed line for each Gaussian mixture component.

We show how the above embeddings learned to be hierarchically clustered in the latent space during training in Figure 8. In the learning mechanism of HCRL, we can observe the hierarchically clustered embeddings from a major deviation to a minor deviation in the data over iterations.

(a) Epoch 0

(b) Epoch 10

(c) Epoch 20

(d) Epoch 25

(e) Epoch 30

(f) Epoch 50

Figure 8: The process by which the embeddings of the synthetic data are learned. The dashed ellipse corresponds to the 95% contour of the learned Gaussian mixture component, whose mean is marked as the gray circle.

11

Under review as a conference paper at ICLR 2019

B EXPERIMENTAL SETTINGS
We conducted experiments for all autoencoder-based models with a neural architecture whose encoder network was set as fully connected layers with dimensions D-2000-2000-500-J for z, and D-10-10-L for , and the decoder network is a mirror of the encoder network for z. The hyperparameters of HCRL given by users,  and , was set to 1.0, and a vector of all entries 1 sized of L, respectively. We used the Adam optimizer (Kingma & Ba, 2014) with an init learning rate of 0.001 for MNIST dataset and 0.0001 for other datasets. Meanwhile, VAE-nCRP is targeted for grouped data. For experiments with our non-grouped datasets, we treated the group instance as a group instance having a single data instance. For parametric hierarchical clustering models, we gave the branch factor as the input parameter, [1, 20, 5], [1, 4, 7, 9], and [1, 6, 4, 3], for CIFAR-100, RCV1 v2, and 20Newsgroups, respectively. For VaDE, we set the number of clusters to the number of leaf clusters; 100 for CIFAR-100, 252 for RCV1 v2, and 72 for 20Newsgroups.

C ALGORITHMS

C.1 TRAINING ALGORITHM
Algorithm 1 summarizes the overall algorithm for HCRL. The tree-based hierarhcy T is defined as (N, P), where N and P denote a set of nodes and paths, respectively. We refer to the node at level l lying on path , as N(1:l)  N. The defined paths, P, consist of full paths (ending at a leaf node), Pfull, and inner paths (ending at an internal node), Pinner, as a union set.
Algorithm 1 selects an operation out of three operations: GROW, PRUNE, and MERGE. The GROW algorithm is executed for every specific iteration period, tG. After ellapsing tb iterations since performing the GROW operation, we begin to check whether the PRUNE or MERGE operation should be performed. We prioritize the PRUNE operation first, and if the condition of performing PRUNE is not satisfied, we check for the MERGE operation next. After performing any operation, we initialize nb to 0, which is for locking the changed hierarchy during minimum tb iterations to be fitted to the training data.

Algorithm 1 Training for Hierarchically Clustered Representation Learning

Input: Training examples x; the tree-based hierarchy depth, L; period of performing GROW, tgrow;

minimum number of epochs locking the hierarchy, tlock; operation-related thresholds prune,

merge; a queue whose element is the set of changed paths, Q; the number of training epochs, E;

maximum length of Q, Qmax; grow scale, sgrow

Output:

T

(E)

,

z

,



,

,

,

{ai

,

bi

,

µi

,

i2

}iM T

(E)

1:

µ1:L

,

2
1:L

 Initialize L Gaussian mixture components

2:

T (0)



Initialize

the

tree-based

hierarchy

having

a

single

path

with

µ1:L

,

2
1:L

3: nlock  0 // for counting the number of epochs, where the hierarchy has not changed

4: for each epoch e = 1, · · · , E do

5: z, ,   Update the network weight parameters using gradients z,,L(x)
6: {ai, bi, µi, i2}iMT(e-1)  Update node-specific params. using gradients a,b,µ,2 L(x) 7: Update other variational parameters using gradients L(x)

8: if mod(e, tgrow) = 0 then

9: T (e), Q  GROW(T (e-1), Q, sgrow, Qmax) // See Algorithm 2

10: end if

11: if T (e) = T (e-1) and nlock  tlock then 12: T (e), Q  PRUNE(T (e-1), Q, prune) // See Algorithm 3

13: if T (e) = T (e-1) then T (e), Q  MERGE(T (e-1), Q, merge, Qmax) // See Algorithm 4

14: end if

15: if T (e) = T (e-1) then nlock  0 else nlock  nlock + 1

16: end for

12

Under review as a conference paper at ICLR 2019 C.2 ALGORITHM FOR GROW OPERATION

Figure 9: The illustration of GROW operation

The GROW operation expands the hierarchy by creating a new branch under the heavily weighted

internal node. Compared with the work from Wang & Blei (2009), we modify GROW to firstly

sample a path according to

N n=1

q(n

=

),

and

then

grow

the

path

if

the

sampled

path

is

an

inner path. When we create the new Gaussian mixture component, we initialize the parameters of a

corresponding Gaussian distribution depending on the mean and the variance of the parent node, as

shown in line 10 of Algorithm 2.

Algorithm 2 GROW Operation

1: function GROW(T, Q, sgrow, Qmax)

2:

J() 

N n=1

q(n

=

)

for





P

//

Calculate

the

measure

3:


Sample a path  with probability

J ()

 J ()

4: Q   // Temporary set of changed paths in this epoch

5: 6:

if



l0Pi|nner|

and




/ Q s.t. Q  Q

then

7: 8:

for

l = l0, · · · , L - 1 do j0  Maximum index

for

the

child

node

whose

parent

path

is



 1:l

9:



 1:l

+1





 1:l

, j0

+

1

10: 11:

N(1:l +1)  N (µ1:l Q  Q  {1:l +1}

+

, diag(21:l )) where

 N (0, ngIJ )

12: 13:

if

l

< L - 1 then Pinner  Pinner 

{

 1:l

+1}

14: 15:

else Pfull



Pfull



{ 1:L }

16: end if

17: end for

18: end if

19: enqueue Q to Q

20: while Qmax < |Q| do dequeue Q 21: P  Pfull  Pinner 22: T  (N, P)

23: return T, Q

24: end function

13

Under review as a conference paper at ICLR 2019 C.3 ALGORITHM FOR PRUNE OPERATION

Figure 10: The illustration of PRUNE operation

The PRUNE operation cuts a minor path, which is sampled according to nN=1q(n = ) among

the full paths satisfying

N n=1

q(n

=

)

<

,

where



is

the

pre-defined

threshold

parameter.

If

the removed leaf node of the full path is the last child of the parent node, we also recursively remove

the parent node as shown in the upper case of Figure 10.

Algorithm 3 PRUNE Operation

1: function PRUNE(T, Q, prune)

2:

J() 

N n=1

q(n

=

),





P

//

Calculate

the

measure

3:

  { |   Pfull,

J ( )  J (

)

<

prune}

4: Randomly sample a full path   

5:

if

|Pfull|

>

1

and




/

Q

s.t.

Q



Q

then

6: Pfull  Pfull\{1:L}

7: for l = L - 1, · · · , 1 do

8: N(1:l +1)  

9: 10:

if

l

<L Pinner

- 1 then  Pinner\{

 1:l

+1}

11: 12:

end if nc 

Number

of

the

children

nodes

whose

parent

path

is



 1:l

13: if nc > 0 then

14: break

15: end if

16: end for

17: end if

18: P  Pfull  Pinner 19: T  (N, P)

20: return T, Q

21: end function

14

Under review as a conference paper at ICLR 2019 C.4 ALGORITHM FOR MERGE OPERATION

Figure 11: The illustration of MERGE operation

The MERGE operation combines two full paths with similar posterior probabilities, measured by J ((i), (j)) = qiqjT /|qi||qj|, where qi = [q(1 = (i)), ..., q(N = (i))]. We merged two Gaussian components by following Ueda et al. (1999). The specific meaning of combining the two
paths is merging the paired two Gaussian distributions lying on the two paths by level, if the two
Gaussian distribtions are different. The estimation of merged Gaussian parameters, µ and , is the
weighted summation of two subject Gaussian parameters. The propbability of the node at level l lying on a path  given x, p(l|x), is proportional to n{q(ln = l) ·  q(n = )}, where  = { |l = l and   Pfull}.

Algorithm 4 MERGE Operation

1: function MERGE(T, Q, merge, Qmax)

2:

J ((i), (j))



qi qjT |qi||qj |

s.t.

qi

=

[q(1

=

(i)), ..., q(N

=

 (i) )]

//

Calculate

the

measure

3:   {((i), (j)) | J ((i), (j))  merge, {(i), (j)}  Pfull}

4: Randomly sample a pair of paths ((1), (2))  

5: Q   // Temporary set of changed paths in this epoch

6: if {(1), (2)} Q s.t. Q  Q then

7: l  Maximum level of nodes shared by (1), (2)

8:


 1:l





(1) 1:l

9: for l = l, · · · , L - 1 do

10:

µ(1)



µ ,

(1) 1:l

+1

(21)



 ,2



(1) 1:l

+1

µ(2)



µ ,

(2) 1:l

+1

(22)



2



(2) 1:l

+1

11:

w(1)



p(l(1+) 1|x),

w(2)



p(

(2) l +1

|x)

µ  ,  12:

w(1)µ(1)+w(2)µ(2) 2 w(1)(21)+w(2)(22)



w(1) +w(2)



w(1) +w(2)

13:

j0



Maximum

index

for

the

child

node

whose

parent

path

is



 1:l

14:



 1:l

+1





 1:l

, j0

+

1

15: N(1:l +1)  N (µ, diag(2))

16:

N((11:l) +1)





,

N(

(2) 1:l

+1)





17: if l < L - 1 then

18:

Pinner



Pinner



{

 1:l

+1 }\{ 1(1:l)

+1

,



(2) 1:l

+1}

19: else

20: Pfull  Pfull  {1:L}\{1(1:L) , 1(2:L) }

21: 22:

end if Q Q

 {1:l +1}

23: end for

24: end if

25: enqueue Q to Q

26: while Qmax < |Q| do dequeue Q 27: P  Pfull  Pinner 28: T  (N, P)

29: return T, Q

30: end function

15

Under review as a conference paper at ICLR 2019

D NOTATIONS
The following Table 4 lists the notations used throughout this paper. Table 4: Table of symbols

Models
All VaDE & VAE-nCRP VaDE & HCRL VAE-nCRP & HCRL
VaDE
VAE-nCRP
HCRL

Symbol x/x z D/J g(x) f (z )  µ~z, ~z2 µx, x2  µ~, ~ 2 N
xn=1,··· ,N zn=1,··· ,N
L
K cn=1,··· ,N
 µc, c2
M Nm=1,··· ,M xm,n=1,··· ,N zm,n=1,··· ,N
vmp
 m(0p) , m(1p)
mn Sm n par(p)  µpar(p), p2ar(p) p N2 µp, p2 D2 z  µ~z, ~z2 µ~, ~2
 vi  ai, bi n Sn n  ln n µi, i2

Definition
An observed / reconstructed datapoint A latent representation The input / latent dimensionality A encoder network parametrized by , whose input is x A decoder network parametrized by , whose input is z The variational parameters and weights of the decoder network f The variational mean and variance for Gaussian distribution qz (z|x) The prior parameters, mean and variance, for Gaussian distribution p(x|z) The variational parameters and weights of the encoder network g The variational mean and variance for Gaussian distribution q(z|x) The number of datapoints n-th observed datapoint n-th latent representation corresponding to xn The height of the tree-based hierarchy
The number of (finite) clusters The cluster assignment of zn,  {1, ..., K} The prior parameter for multinomial distribution p(c)
The prior parameters, mean and variance, for Gaussian distribution of c-th cluster, p(z)
The number of sequences The number datapoints in m-th sequence n-th observed datapoint in m-th sequence n-th latent representation corresponding to xmn The Beta draws of m-th sequence on node p, for the tree-based stickbreaking construction The prior parameter for Beta distribution p(vmp)
The variational parameters, for Beta distribution q(vmp|xm) The path assignment of zmn The variational parameter for multinomial distribution q(mn|xmn) The J-dimensional parameter vector for the parent node of p The prior parameter for Gaussian distribution p(p) for the root node The variational mean and variance for Gaussian distribution q(par(p)|x) The J-dimensional parameter vector for node p The prior parameter, variance, for Gaussian distribution p(p|par(p)) The variational mean and variance for Gaussian distribution q(p|x) The prior parameter, variance, for Gaussian distribution p(zmn|mn, p) The variational parameters and weights of the encoder network gz The variational parameters and weights of the encoder network g The variational mean and variance for Gaussian distribution qz (z|x) The variational mean and variance for logistic normal distribution q (|x) The variational parameter for Dirichlet distribution q (|x) The Beta draws for the tree-based stick-breaking construction of node i The prior parameter for Beta distribution p(vi) The variational parameters, for Beta distribution q(vi|x) The path assignment of zn The variational parameter for multinomial distribution q(n|xn) The level proportion of zn The prior parameter for Dirichlet distribution p(n) The level assignment of zn,  {1, ..., L} The variational parameter for multinomial distribution q(ln|xn)
The prior parameters, mean and variance, for Gaussian distribution of node i, p(zn|n, n)

16

Under review as a conference paper at ICLR 2019

E GENERATIVE AND INFERENCE MODEL FOR HCRL
HCRL assumes the generative process as described in Section 3.1. Section E.1 describes the joint probability distribution, and Section E.2 presents the corresponding variational distributions. We adopt the much notation-related conventions from Wang & Blei (2009), especially on paths.

E.1 GENERATIVE MODEL

N
p(v, , , l, z, x) = p(v|) p(n|v)p(n|)p(ln|n)p(zn|n, ln, µ1:, 12:)p(xn|zn)

n=1

N

= p(vj|) p(vi|) p(n|v)p(n|)p(ln|n)p(zn|n, ln)p(xn|zn)

j/MT

iMT

n=1

· MT : Set of all nodes in truncated tree T

· For j / MT , p(vj|) = Beta(vj|1, )

· For i  MT , p(vi|) = Beta(vi|1, )

· p(n = [1, 2, ..., L]|v)
L
p(n = [1, 2, ..., L]|v) = 1,2,...,l
l=1

L l-1

= 1,2,...,l-1 v1,2,...,l

(1 - v1,2,...,j )

l=1 j=1

Ll
=
l=1 l =1

v1,2 ,...,l

l -1
(1 - v1,2,...,j )
j=1

­ 1,2,...,l =

l l

=1 {v1,2 ,...,l

(

l -1 j=1

(1

-

v1,2 ,...,j

))}

· p(n|) = Dirichlet(n|)

· p(ln|n) = Multinomial(n) · p(zn|n = , ln = l) = N (zn|µl , 2l IJ ) · p(xn|zn) : Probabilistic decoding of xn parametrized by , whose input is zn

· Tree-based stick-breaking construction

­ We will denote all Beta draws as v, each of which is an independent draw from Beta(v|1, )

(except for root v1 = 1)

 vi  Beta(vi|1, )

­ The root nodes stick length: 1 = v1  1

­ Stick length at second level: 1i = 1v1i

i-1 j=1

(1

-

v1j

),

 i=1

1i

=

1

=

1

­ For the segment 1k, the stick lengths of its children are 1ki = 1kv1ki

i-1 j=1

(1

-

v1kj ),

for

i = 1, 2, ..., ,

 i=1

1ki

=

1k

E.2 INFERENCE MODEL

As VAE, we infer the random variables via the mean-field approximation, where the variational
distribution, q,z (v, , , l, z|x), approximates the intractable posterior. We model the variational distributions as follows:

N

q,z (v, , , l, z|x) = q(v|a, b, x) q(n|xn)q (n|xn)q(ln|n, xn)qz (zn|xn)
n=1

N

=

p(vj | )

q(vi|ai, bi) q(n|xn)q (n|xn)q(ln|n, xn)qz (zn|xn)

j/MT

iMT

n=1

17

Under review as a conference paper at ICLR 2019

· For j / MT , p(vj|) = Beta(vj|1, )

· For i  MT , q(vi|ai, bi)  viai-1(1 - vi)bi-1 = Beta(vi|ai, bi)

­ ai = 1 + (L - li + 1)

N n=1

l0+1,...,L q(n = [1, 2, ..., l0 , l0+1, ..., L])

­ bi =  + (L - li + 1)

N n=1

j,l0+1,...,L:j>l0 q(n = [1, 2, ..., l0-1, j, l0+1, ..., L])

 li : The level of the mixture component i

· q(n|xn)  Sn

child() Sn

­ : a path in the truncated tree T , either an inner path (a path ending at an internal node) or a full path (a path ending at a leaf node)
­ child(): the set of all full paths that are not in T but include  as a sub path
 As a special case, if  is a full path, child() just contains itself ­ In the case of a full path,

Sn = Sn

Ll

l -1

= exp Eq

log v1,2,...,l +

log(1 - v1,2,...,j )

l=1 l =1

j=1

+ Z0

L
= exp Eq
l=1

l l =1

l -1

log v1,2,...,l +

log(1 - v1,2,...,j )

j=1

+ log N (zn|µnl , 2nl IJ )

L l-1

= exp

(L - l + 1) Eq[log v1,2,...,l ] +

Eq[log(1 - v1,2,...,j )] + log N (zn|µnl , 2nl IJ )

l=1 j=1

­ In the case of an inner path,  [1, 2, ..., l0 ]  MT
 child() {[, l0+1, ..., L] : l0+1 > j0}  j0 : maximum index for the child node whose parent path is 

Sn =

Sn

child()

L

= exp Eq

child()

l=1

l l =1

l -1

log v1,2,...,l +

log(1 - v1,2,...,j )

j=1

+ log N (zn|µnl , 2nl IJ )

exp =

(Eq [

L l=l0 +1

log

N

(zn|µnl

,

2nl

IJ

)]

+

L l=l0 +1

log(L

-

l

+

1)

+

(L

-

l0)((1)

-

(1

+



)))}

(1 - exp{() - (1 + )})L-l0

l0 l-1
exp (L - l + 1) Eq[log v1,2,...,l ] + Eq[log(1 - v1,2,...,j )] + log N (zn|µnl , 2nl IJ )
l=1 j=1

j0
exp Eq (L - l0) log(1 - v1,2,...,l0 ,j )
j=1

­ q (n|xn) = Dirichlet(n|n)



nl =

(1 - +1
2nl

2 e-µnl L L2

L l =1

e-µnl

)

­ q(ln|n, xn) = Multinomial(ln|n)

 nl  exp{(nl) - (n0) +

· n0 =

L i=1

ni

 Sn (

J j=1

-

1 2

log(22nl,j ) -

z2nj 22nl ,j

-

)}(µznj -µnl,j )2
22nl ,j

18

Under review as a conference paper at ICLR 2019

 Derivation for nl

Lnl = nl((nl) - (n0)) - nl log nl
ll

+ Sn


l

nl

J -1 2
j=1

log(22nl,j ) -

z2nj 22nl,j

-

(µznj - µnl,j )2 22nl,j

= nl((nl) - (n0)) +


Sn nl

J1 - 2
j=1

log(22nl,j ) -

z2nj 22nl,j

-

(µznj - µnl,j )2 22nl,j

- nl log nl + ( nl - 1)
l

 Lnl nl

= ((nl) - (n0)) +



- (log nl + 1) + 

Sn

nl = exp (nl) - (n0) + Sn


nl  exp (nl) - (n0) + Sn


J -1 2
j=1

log(22nl,j ) -

z2nj 22nl,j

-

(µznj - µnl,j )2 22nl,j

J -1 2
j=1

log(22nl,j ) -

z2nj 22nl,j

-

(µznj - µnl,j )2 22nl,j

J1 - 2
j=1

log(22nl,j ) -

z2nj 22nl,j

-

(µznj - µnl,j )2 22nl,j

-1+

­ qz (zn|xn) = N (zn|µzn , z2n IJ )

F EVIDENCE LOWER BOUND

In this section, we present the detailed derivation of the ELBO in Equation 6, which is the objective function for learning HCRL.

log p(x)  LELBO(x) = Eq

p(v, , , l, z, x) log q(v, , , l, z|x)

= Eq log

iMT p(vi|)

N n=1

p(n

|v)p(n|)p(ln

|n)p(zn|n,

ln)p

(xn

|zn)

iMT q(vi|ai, bi)

N n=1

q(n|xn)q

(n|xn)q(ln|n,

xn)qz

(zn|xn)

N

= Eq[log p(vi|)] + Eq[log p(n|v) + log p(n|) + log p(ln|n) + log p(zn|n, ln)

iMT

n=1

N

+ log p(xn|zn)] -

Eq[log q(vi|ai, bi)] - Eq[log q(n|xn) + log q (n|xn)

iMT

n=1

+ log q(ln|n, xn) + log qz (zn|xn)]

(6)

F.1 DETAILED DERIVATION FOR ELBO

The followings are additional notations used for the detailed derivation:

·  : The digamma function

· n0 =

L i=1

ni,

0

=

L i=1

i

19

Under review as a conference paper at ICLR 2019

N

Eq[log p(vi|)] =

p(vj = v )

q(vi = v ) q(n =  )q(n =  )

v z  l  j/MT

iMT

n=1

q(ln = l )q(zn = z ) log p(vi = v )d dz dv

=

p(vj = v )

q(vi = v ) log p(vi = v )dv

v j/MT

iMT

= q(vi = v ) log p(vi = v )dv
v
= Beta(v |ai, bi) · log Beta(v |1, )dv
v
= log (1 + ) - log (1) - log () + (1 - 1)(ai) + ( - 1)(bi) +(-1 -  + 2)(ai + bi)
= log (1 + ) - log () + ( - 1)(bi) + (1 - )(ai + bi) = log (1 + ) - log () + ( - 1)((bi) - (ai + bi)) = log(()) - log () + ( - 1)((bi) - (ai + bi)) = log  + log ()) - log () + ( - 1)((bi) - (ai + bi)) = log  + ( - 1)((bi) - (ai + bi))

Eq[log p(n|v)]

=

exp{log (L - l0 + 1) + (L - l0)((1) - (1 + ))} (1 - exp{() - (1 + )})L-l0

l0 l-1

× exp

(L - l + 1) Eq[log v1,2,...,l ] +

Eq[log(1 - v1,2,...,j )]

l=1 j=1

j0
× exp Eq (L - l0) log(1 - v1,2,...,l0 ,j )
j=1

Eq[log p(n|)] =

p(vj = v )

q(vi = v )q(n =  )q(n =  )

v z  l  j/MT

iMT

q(ln = l )q(zn = z) log p(n =  |)d dz dv

= q(n =  ) log p(n =  |)d = Dir( |n) · log Dir( |)d

LL
= log (0) - log (i) + (i - 1)((ni) - (n0))
i=1 i=1

Eq[log p(ln|n)] =

p(vj = v )

q(vi = v )q(n = )q(n =  )

v z  l  j/MT

iMT

q(ln = l )q(zn = z ) log p(ln = l |n)ddz dv

= q(n =  )q(ln = l) log p(ln = l|n =  )d
l

= q(ln = l ) q(n =  ) log Mult(l | )d
l

= nl Dir( |n) log l d
l
= nl ((nl ) - (n0))
l

20

Under review as a conference paper at ICLR 2019

Eq[log p(zn|n, ln)] =

p(vj = v )

q(vi = v )q(n = )q(n =  )

v z  l  j/MT

iMT

q(ln = l )q(zn = z) log p(zn = z|n = , ln = l )d dz dv

= q(n = )q(ln = l )q(zn = z) log p(zn = z|n = , ln = l )dz
zl 

= q(n = )q(ln = l ) q(zn = z) log p(zn = z|n = , ln = l )dz

l

z

=

l



q(n =  )q(ln = l )

z

N (z|µz

, z2

IJ )

·

log N (z|µ nl

, 2 nl

IJ )dz

= Sn


nl
l

J -1 2
j=1

log(22 nl

,j ) -

z2nj 22 ,j
nl

-

(µznj

-

µ nl

22 ,j nl

,j )2

Eq[log p(xn|zn)] =

p(vj = v )

q(vi = v )q(n = )q(n =  )

v z  l  j/MT

iMT

q(ln = l )q(zn = z) log p(xn|zn = z )d dz dv

= q(zn = z) log p(xn|zn = z )dz
z



1 S

S

log p(xn|zn(s)), where z(i,s) = µx(i) + x(i)

s=1

(s) and (s)  N (0, IJ )

1 S

S s=1

D d=1

xnd

log

µ(xsn)d

+

(1

-

xnd) log(1

-

µ(xsn)d )

if xn is binary

=1 S

S s=1

D d=1

-

1 2

log(2x(sn)d2 )

-

(xnd-µ(xsn)d )2 2x(sn)d2

if xn is real-valued

Eq[log q(vi|ai, bi)] =

p(vj = v )

q(vi = v )q(n = )q(n =  )

v z  l  j/MT

iMT

q(ln = l )q(zn = z) log q(vi = v )d dz dv

=

p(vj = v )

q(vi = v ) log q(vi = v )dv

v j/MT

iMT

= q(vi = v ) log q(vi = v )dv = Beta(v |ai, bi) · log Beta(v |ai, bi)dv
vv
= log (ai + bi) - log (ai) - log (bi) + (ai - 1)(ai) + (bi - 1)(bi)
+(-ai - bi + 2)(ai + bi)

Eq[log q(n|xn)] =

p(vj = v )

q(vi = v )q(n = )q(n =  )

v z  l  j/MT

iMT

q(ln = l )q(zn = z) log q(n = )d dz dv

= q(n = ) log q(n = )



=


Sn log  Sn

Sn  Sn

21

Under review as a conference paper at ICLR 2019

Eq[log q (n|xn)] =

p(vj = v )

q(vi = v )q(n = )q(n =  )

v z  l  j/MT

iMT

q(ln = l )q(zn = z) log q(n =  )d dz dv

= q(n =  ) log q(n =  )d = Dir( |n) · log Dir( |n)d

LL
= log (n0) - log (ni) + (ni - 1)((ni) - (n0))
i=1 i=1

Eq[log q(ln|n, xn)] =

p(vj = v )

q(vi = v )q(n = )q(n =  )

v z  l  j/MT

iMT

q(ln = l )q(zn = z) log q(ln = l )d dz dv

= q(ln = l ) log q(ln = l ) = Mult(l |n) log Mult(l |n)
ll

= nl · log nl
l

Eq[log qz (zn|xn)] =

p(vj = v )

q(vi = v )q(n = )q(n =  )

v z  l  j/MT

iMT

q(ln = l )q(zn = z) log q(zn = z)d dz dv

= q(zn = z) log q(zn = z)d dz
z

= q(zn = z) log q(zn = z)dz = N (z|µz , z2 IJ ) · log N (z|µz , z2 IJ )dz
zz

=

- J log(2) - 1 22

J
(1 + log z2nj )

j=1

22

