Under review as a conference paper at ICLR 2019

ON COMPUTATION AND GENERALIZATION OF GENERATIVE ADVERSARIAL NETWORKS UNDER SPECTRUM CONTROL

Anonymous authors Paper under double-blind review
ABSTRACT
Generative Adversarial Networks (GANs), though powerful, suffer from training instability. Several recent works (Brock et al., 2016; Miyato et al., 2018) suggest that controlling the spectra of weight matrices in the discriminator can significantly improve the training of GANs. Motivated by their discovery, we propose a new framework for training GANs, which allows more flexible spectrum control (e.g., making the weight matrices of the discriminator have slow singular value decays). Specifically, we propose a new reparameterization approach for the weight matrices of the discriminator in GANs, which allows us to directly manipulate the spectra of the weight matrices through various regularizers and constraints, without intensively computing singular value decompositions. Theoretically, we further show that the spectrum control improves the generalization ability of GANs. Our experiments on CIFAR-10, STL-10, and ImgaeNet datasets confirm that compared to other methods, our proposed method is capable of generating images with competitive quality by utilizing spectral normalization and encouraging the slow singular value decay.

1 INTRODUCTION

Many efforts have been recently devoted to studying Generative Adversarial Networks (GANs, Goodfellow et al. (2014)). GANs provide a general unsupervised framework to learn a generative model from unlabeled real data. Successful applications of GANs include many unsupervised learning tasks, such as image generation, dialogue generation, and image inpainting (Abadi & Andersen, 2016; Goodfellow, 2016; Ho & Ermon, 2016; Li et al., 2017; Yu et al., 2018). Different from other unsupervised learning methods, which directly maximize the likelihood of deep generative models (e.g., Variational Auto-encoder, Nonlinear ICA, and Restricted Boltzmann Machine), GANs introduce a competition between two neural networks. Specifically, one neural network serves as the generator that yields artificial samples, and the other serves as the discriminator that distinguishes the artificial samples from the real data.

Mathematically, GANs can be formulated as the following min-max optimization problem:

min max f (, W) := 1

W

n

n

 (A(DW (xi))) + ExDG [ (1 - A(DW (x)))],

(1)

i=1

where {xi}in=1 are n real data points, G denotes the generative deep neural network parameterized by

, DW denotes the discriminative neural network parameterized by W, DG denotes the distribution

generated by G, (·) : [0, 1]  R is a properly chosen monotone function, and A(·) denotes a

monotone function related to the function (·). There have been many options for (·) and A(·) in

existing literature. For example, the original GAN proposed in Goodfellow et al. (2014) chooses

(x)

=

log(x),

A

=

1 1+exp(-x)

;

Arjovsky

et

al.

(2017)

use

(x)

=

x,

A(x)

=

x,

and

(1)

becomes

the Wasserstein GAN. Min-max problem (1) has a natural interpretation: The minimization problem

aims to find a discriminator DW , which can distinguish between the real data and the artificial

samples generated by G, while the maximization problem aims to find a generator G, which can fool the discriminator DW . From the perspective of game theory, the generator and discriminator are

essentially two players competing with each other and eventually achieving some equilibrium.

From an optimization perspective, problem (1) is a nonconvex-nonconcave min-max problem, that is, f (, W) is nonconvex in  given a fixed W and nonconcave in W given a fixed . Unlike convex-

1

Under review as a conference paper at ICLR 2019

concave min-max problems, which have been well studied in existing optimization literature, there is very limited understanding of general nonconvex-nonconcave min-max problems. Thus, most of existing algorithms for training GANs are heuristics. Although some theoretical guarantees have been established for a few algorithms, they all require very strong assumptions, which are not satisfied in practice (Heusel et al., 2017).

Despite of the lack of theoretical justifications, significant progress has been made in empirical studies of training GANs. Numerous empirical evidence has suggested several approaches for stabilizing the training of the discriminator, which can eventually improve the training of the generator. For example, Goodfellow et al. (2014) adopt a simple algorithmic trick that updates W for multiple iterations after updating  for one iteration, i.e., training the discriminator more frequently than the generator. Besides, Xiang & Li (2017) suggest that the weight normalization approach proposed in Salimans & Kingma (2016) can also stabilize the training of the discriminator. More recently, Miyato et al. (2018) propose a spectral normalization approach to control the spectral norm of the weight matrix in each layer. Specifically, in each forward step, they normalize the weight matrix by the approximation of its spectral norm, which is obtained by the one-step power method. They further show that spectral normalization essentially controls the Lipschitz constant of the discriminator with respect to the input. Compared to other methods for controlling the Lipschitz constant of the discriminator, e.g., gradient penalty (Gulrajani et al., 2017; Gao et al., 2017), the experiments in Miyato et al. (2018) show that the spectral normalization approach achieves better performance with fairly low computational cost. Moreover, Miyato et al. (2018) show that spectral normalization suffers less from the mode collapse, that is, the generator outputs only over a fairly small support. Such a phenomenon, though not well understood, suggests that the spectral normalization will balance the discrimination and representation well.

Besides the aforementioned algorithmic tricks and normalization approaches, regularization can also

stabilize the training of the discriminator (Brock et al., 2016; Roth et al., 2017; Nagarajan & Kolter,

2017). For instance, orthogonal regularization, proposed by Brock et al. (2016), forces the columns of

weight matrices in the discriminator to be orthonormal by augmenting the objective function f (, W)

with 

L i=1

Wi

Wi - I

2, where  > 0 is the regularization parameter, Wi denotes the weight

matrix of the i-th layer in the discriminator, I denotes the identity matrix, and L is the depth of the

discriminator. The experimental results in Brock et al. (2016) show that the orthogonal regularization

improves the performance and generalization ability of GANs. However, the empirical evidence in

Miyato et al. (2018) shows that the orthogonal regularization is still less competitive than the spectral

normalization approach. One possible explanation is that the orthogonal normalization, forcing all

non-zero singular values to be 1, is more restrictive than the spectral normalization, which only forces

the largest singular value of each weight matrix to be 1.

Motivated by the spectral normalization, we propose a novel training framework, which provides more flexible and precise control over the spectra of weight matrices in the discriminator. Specifically, we reparameterize each weight matrix Wi  Rdi×di+1 as Wi = UiEiVi , where Ui and Vi are required to have orthonormal columns, Ei = diag(ei1, ..., erii ) denotes a diagonal matrix with ri = min(di, di+1), and e1i  · · ·  eiri  0 are singular values of Wi. With such a reparameterization, an L-layer discriminator becomes
D(x; U , E, V) = ULELVL L-1(· · · 1(U1E1V1 x) · · · ),
where i(·) is the entry-wise activation operator of the i-th layer, U := {U1, ..., UL}, E := {E1, ..., EL}, and V := {V1, ..., VL} denote the parameters of the discriminator D, and x denotes the input vector. This reparameterization allows us to control the spectra of the original weight matrix Wi by manipulating Ei. For example, we can rescale Ei by its largest diagonal element, which essentially is the spectral normalization. Besides, we can also manipulate the diagonal entries of Ei to control the decays in singular values (e.g., fast or slow decays). Recall that our reparameterization requires Ui and Vi to have orthonormal columns. This requirement can be achieved by several methods in the existing literature, such as the stiefel manifold gradient method. However, Huang et al. (2017) show that the stochastic stiefel manifold gradient method is unstable. Moreover, other methods, such as cayley transformation and householder transformation, suffer from several disadvantages: (I). High computational cost1; (II). Sophisticated implementation (Shepard et al., 2015). Different from the methods mentioned above, our framework applies the orthogonal regularization to all Ui's and

1Without a sparse matrix implementation, these methods are highly unscalable and inefficient (not supported by the existing deep learning libraries such as TensorFlow and PyTorch in GPU).

2

Under review as a conference paper at ICLR 2019

Vi's. Such a regularization suffices to guarantee the approximate orthogonality of Ui's and Vi's in practice, which is supported by our experiments. Moreover, our experimental results on CIFAR-10, STL-10 and ImageNet datasets show that our proposed method achieves competitive performance on CIFAR-10 and better performance than the spectral normalization and other competing approaches on STL-10 and ImageNet. Besides the empirical studies, we provide theoretical analysis, which characterizes how the spectrum control benefits the generalization ability of GANs. Specifically, denote µ as the underlying data distribution and n as the distribution given by the well trained generator. We establish a generalization bound under spectrum control as follows (informal):

dF,(µ, n)  inf dF,(µ, ) + O
 DG

d2L ,
n

where d = max{d1, . . . , dL}, dF,(·, ·) is the F -distance, and DG denotes the class of distributions generated by generators. Compared to the results in Zhang et al. (2017), our result improves the generalization bound up to an exponential factor of the depth of the discriminator. More details will be discussed in Section 3.

The rest of the paper is organized as follows: Section 2 introduces our proposed training framework in detail; Section 3 presents the generation bound for GANs under spectrum control; Section 4 presents numerical experiments on CIFAR-10, STL-10, and ImageNet datasets.

Notations: Given an integer d > 0, we denote [d] = {1, 2, ..., d}. Given a vector v  Rd, we denote

v

2 2

=

d i=1

|vi|2

as

its

Euclidean

norm.

Given

a

matrix

M



Rm×n,

we

denote

the

spectral

norm

by M 2 as the largest singular value of M . We adopt the standard O(·) notation, which is defined

as f (x) = O (g(x)) as x  , if and only if there exists M > 0 and x0, such that |f (x)|  M g(x)

for x  x0. We use O(·) to denote O(·) with hidden logarithmic factors.

2 METHODOLOGY

We present a new framework for flexibly controlling the spectra of weight matrices. We first consider an L-layer discriminator D as follows:

D(x; W) = WLL-1(WL-1 · · · 1(W1x) · · · ),

(2)

where i(·) denotes the entry-wise activation operator of the i-th layer, Wi  Rdi+1×di denotes the weight matrix of the i-th layer, x  Rd1 denotes the input feature, W := {W1, ..., WL} denotes the parameters of the discriminator D, and dL+1 = 1.

2.1 SVD REPARAMETERIZATION

Our framework directly applies an SVD reparameterization to each weight matrix Wi in the discriminator D, i.e., Wi = UiEiVi , where ri = min(di, di+1), Ui  Rdi+1×ri and Vi  Rdi×ri denote two matrices with orthonormal columns, Ei = diag(ei1, · · · , eiri ) denotes a diagonal matrix, and e1i  · · ·  erii  0 are the singular values of Wi. The discriminator can be rewritten as follows:

D(x; U , E, V) = ULELVL L-1(UL-1EL-1VL-1 · · · 1(U1E1V1 x) · · · )),

(3)

where U := {U1, ..., UL}, E := {E1, ..., EL}, and V := {V1, ..., VL}2 denote the parameters of the discriminator D. Throughout the rest of the paper, if not clear specified, we denote D(x; U, E, V)

by D(x) for notational simplicity. The motivation behind this reparameterization is to control the

singular values of each weight matrix Wi by explicitly manipulating Ei. We then consider a new min-max problem as follows:

min max
 E,U ,V

1 n

n

 (A(D(xi))) + ExDG [ (1 - A(D(x)))] -R(E)

,

i=1

f (,E,U ,V)

subject to E  , Ui Ui = Ii, and Vi Vi = Ii  i  [L],

(4)

where Ii denotes the identity matrix of size ri, R(E) is the regularizer with a regularization parameter

 > 0, and  denotes a feasible set. By choosing different  and R(E), (4) can control the spectrum

2WL essentially is a vector. To be consistent, we still use ULELVL to reparametrize WL. Actually, it is not necessary. We can directly control the norm of WL in practice.

3

Under review as a conference paper at ICLR 2019

of the weight matrix Wi flexibly. For example, if we take the feasible set  = {E : eij = 1 eij 
Ei, Ei  E} and R(E) = 0, then our method essentially is the orthogonal regularization. We will discuss some options of  and R(E) later in detail.

As mentioned earlier, the orthogonal constraints in (4) suffer from the high computational cost

and sophisticated implementation. To address these drawbacks, we directly apply the orthogonal

regularization to all Ui's and Vi's. Therefore, probelm (4) becomes

L

min max f (, E, U, V) - 
 E,U ,V

( Ui Ui - Ii

2 F

+

Vi

Vi - Ii

2 F

)

-

R(E

),

s.t.

E



,

(5)

i=1

where  > 0 is a regularization parameter. A relative large  (e.g.,  = 1), ensures the orthogonality

of Ui and Vi. See more details in Section 4.1. Moreover, (5) can be efficiently solved by stochastic gradient algorithms. Projection may be needed to handle the constraint . See more details later.

2.2 SPECTRUM CONTROL

We provide a few options of  and R(E) for controlling the spectra of weight matrices in the

discriminator, which is motivated by Miyato et al. (2018). Miyato et al. (2018) have shown that for

an L-layer discriminator D, we have:

L-1

L-1

|D(x) - D(y)|  WL 2

Wi 2 · i x - y 2 = eL1

ei1 · i x - y 2, (6)

i=1 i=1

where i is the Lipschitz constant of i(·). The last equation holds for our proposed reparameterization. For commonly used activation operators, such as the sigmoid, ReLU, and leak-ReLU functions,

i  1. Therefore,

L i=1

e1i

is

essentially

an

upper

bound

for

the

Lipschitz

constant,

which

can

be

controlled by our proposed  and R(E).

· Spectrum Constraint: A strightforword choice of  is

 = E : 0  ei1  1 i  [L] .

(7)

This essentially controls

L i=1

e1i

by

forcing

each

e1i



1.

To

maintain

the

feasibility

of

E,

we

only

need a simple projection for each Ei in the back propagation, which can be implemented by a simple

entry-wise clipping operator defined as

g(t) := 0 · 1{t0} + t · 1{0<t<1} + 1 · 1{t1}, where 1tA = 1 if t  A, and 0 otherwise.

(8)

Note that the spectral normalization is an alternative way to reparameterize the  defined in (7). Specifically, the spectral normalization rescales the weight matrix Ei by its spectral norm e1i , which
is equivalent to solving the following problem:

L

min max f (, Q(E), U, V) - 
 Q(E),U ,V

(

Ui

Ui - Ii

2 F

+

Vi

Vi - Ii

2F),

i=1

where

Q

:=

{

E1 e11

,

.

.

.

,

EL eL1

}.

These

two

methods

are

essentially

solving

the

same

problem,

but

in

different formulations. Therefore, different algorithms are adopted. Due to the nonconvex-nonconcave

structure of (5), different solutions are obtained.

· Lipschitz Regularizer: We can also directly penalize

L i=1

ei1

to

control

the

Lipschitz

constant

of

the discriminator D. Specifically, we define the Lipschitz regularizer as:

LL

R(E) := max log e1i , 0 = max

log ei1, 0 .

i=1 i=1
Compared to the spectral constraint, which enforces all ei1  1, the Lipschitz regularizer is more flexible since it allows e1i > 1 for some i  [L].

From the experiments in Miyato et al. (2018), we observe that the spectral normalization leads to
a slow singular value decay as shown in Figure 1, which is in contrast to the widely discussed fast
decay phenomenon in the existing literature. Then we conjecture that the orthogonal regularization is too stringent since it enforces all singular values to be 1. This further motivates us to propose the following regularizers to encourage such a slow decay. Note that WL is a vector with only one singular value. For simplicity, we omit the last layer in the following analysis.

4

Under review as a conference paper at ICLR 2019

· D-Optimal Regularizer: Motivated by the decay shown in Figure 1 and D-optimal design (Wu & Hamada, 2011), we define the following regularizer:

R(E )

=

1 2

L-1
log(|(EiEi)-1|)

=

L-1
- log

ri
eik .

i=1 i=1 k=1

Note

that

the

derivative

of

log(t)

is

1 t

,

a

monotone

decreas-

ing function. Then log(t) has a significant impact when

t is small. Thus, D-optimal regularizer R(E) encourages

a slow singular value decay.

· Divergence Regularizer: We propose a divergence reg-

ularization to precisely control the slow decay as shown in Figure 1: An illustration of smooth sin-

Figure 1. To mimic such a decay, we consider a reference gular value decays with the spectral nor-

distribution, y = 1 - min(|z|, 1), where z  N (0, a2). malization. The vertical axis denotes the

Figure 2 shows the decays of 10K order statistics sampled value and the horizontal axis denotes the

from y. We then denote the density function of y as p normalized rank.

and the probability mass function of a uniform discrete

distribution

over

{eij }rj=i-11

as

Qi(eij )

=

1 ri -1

j



[ri

-

1].

Note

that

the

K-L

divergence

between

a discrete distribution and a continuous distribution is . To address this issue, we discretize p.

Specifically, given {eji }rj=i 1, we construct a discrete distribution over {eij}jr=i-11 with a probability

mass function Pi, defined as follows:

Pi(eji ) =

p(eji )(eji +1 - eij )

ri -1 k=1

p(eik)(eki +1

-

eki )

j  [ri - 1].

Ignoring the normalization term in the denominator, we then define the regularizer as follows:

R(E )

=

L-1 i=1

ri

1 -

1

ri -1 k=1

log

(ri - 1)-1 (eik+1 - eik)p(eik)

.

Note that the divergence regularizer requires the singular values in the interval [0, 1] and D-optimal regularizer

1.0

cannot control the Lipschitz constant of the discriminator D. Therefore, we incorporate the divergence regularizer with the spectrum constraint and combine the D-optimal regularizer with the spectral normalization to further improve the training of GANs. Our experimental results show that both combinations achieve an impressive results on CIFAR10 and STL-10 datasets.

0.8 0.6 0.4 0.2

a = 0.01 a = 0.10 a = 0.20 a = 0.30 a = 0.40 a = 0.50 fast decay

3 THEORY

0.0 0.0 0.2 0.4 0.6 0.8 1.0
Figure 2: The plot of normalized ranks

We show how the spectrum control benefits the generalization of GANs. Before proceed, we define F-distance
as follows.

versus values of 10K order statistics sampled from reference distributions. The vertical axis denotes the value; the horizontal axis denotes the normalized rank.

Definition 1 (F-distance). Let F be a class of functions

from Rd to [0, 1] such that if f  F , 1 - f  F . Let  be a concave function. Then given two distributions µ and  supported on Rd, the F -distance dF,(µ, ) with respect to  is defined as

dF,(µ, ) = sup Exµ[(f (x))] + Ex [(1 - f (x))] - 2(1/2).
f F

Note that F-distance unifies Jensen-Shannon distance, Wasserstein distance and neural dis-
tance as proposed in Arora et al. (2017). For example, when taking (x) = x and F = {all 1-Lipschitz functions from Rd to [0, 1]}, the F-distance is the Wasserstein distance. Recall that by (1), the training of GANs is essentially minimizing the F-distance with F being the collection
of composite functions A(D(·)), where D(·) is the L-layer discriminator network defined by (2). To
establish the generalization bound, we impose the following assumption.

5

Under review as a conference paper at ICLR 2019

Assumption 1. The activation operator i is 1-Lipschitz with i(0) = 0 for any i  [L - 1]. A is 1-Lipschitz such that if A(D(·))  F, 1 - A(D(·))  F.  is -Lipschitz. The spectral norms of weight matrices are bounded respectively, i.e., Wi 2  BWi for any i  [L].

Note that commonly used functions A, such as the sigmoid function, satisfy the assumption. We
denote by µ the underlying data distribution, and by µn the empirical data distribution. We further denote n as the distribution given by the generator that minimizes the loss (1) up to accuracy , i.e.,

dF,(µn, n)  inf dF,(µn, ) + ,
 DG

where DG is the class of distributions generated by generators. Then we give the generalization bound based on the PAC-learning framework as follows.

Theorem 2. Under Assumption 1, assume that the input data xi  Rd1 is bounded, i.e., xi 2  Bx for i  [n]. Then given activation operators 1, . . . , L-1, A, and , with probability at least 1 - 
over the joint distribution of x1, . . . , xn, we have





dF,(µ, n) 

inf

dF

,

(µ,



)

+

O

 

 DG



 d2L log dnL
 n

+ 



log

1 

 

+

,

n

where  = Bx

L i=1

BWi

and

d

=

max(d1,

...

,

dL).

The detailed proof is provided in Appendix A.1. By constraining each BWi = 1, the generalization bound is reduced to of the order O d2L/n , which is polynomial in d and L. On the contrary,
without such spectrum constraints, the bound can be exponentially dependent on L. For example, if BWi  1 + r with some constant r > 0 for any i = 1, . . . , L, we have   Bx(1 + r)L, which implies that GANs cannot generalize with polynomial number of samples.
Remark 3. Empirical Rademacher complexity (ERC) is adopted to derive our generalization bound,
which is of the order O  d2L/n . Directly applying the ERC based generalization bound in

Bartlett et al. (2017) yields a bound of the order O  d2L3/n . Our bound is tighter, and is
derived by exploiting the Lipschitz continuity of the discriminator with respect to its model parameters (weight matrices). Similar idea is used in Zhang et al. (2017), however, we derive sharper Lipschitz constants 3 by the key step of decoupling the spectral norms of weight matrices and the number of parameters, i.e., separating  and d2L.
Remark 4. Theorem 2 shows the advantage of spectrum control in generalization by constraining the class of discriminators. However, as suggested in Arora et al. (2017), the class of discriminators needs to be large enough to detect lack of diversity. Despite of a lack of theoretical justifications, empirical results in Miyato et al. (2018) show that discriminators with spectral normalization are powerful in distinguishing n from µ, and suffer less from the mode collapse. We conjecture that the observed singular value decay (as illustrated in Figure 1) contributes to preventing mode collapse. We leave this for future theoretical investigation.

4 EXPERIMENT
To demonstrate our proposed new methods, we conduct experiments on CIFAR-10 (Torralba et al., 2008), STL-10 (Coates et al., 2011), and ImageNet (Russakovsky et al., 2015). We illustrate the importance of spectrum control in GANs training by revealing a close relation between the performance and the singular value decays.
All implementations are done in Chainer as the official implementation of the SN-GAN (Miyato et al., 2018). For quantitative assessment of generated examples, we use inception score (Salimans et al., 2016) and Fréchet inception distance (FID, Heusel et al. (2017)). All reported results correspond to 10 runs of the GAN training with different random initializations. See more implementation details in Appendix C.1.
3The Lipschitz constant in Zhang et al. (2017) can be of the order dL.

6

Under review as a conference paper at ICLR 2019

4.1 DC-GAN

We test our methods on DC-GANs with two datasets, CIFAR-10 and STL-10. Specifically, we adopt

a 5-layer CNN as the generator and a 7-layer CNN as the discriminator. Recall that our proposed

training framework tries to solve the equilibrium for equation (5). We set (·) = log(·) and A being

the sigmoid function. Denote fD(E, U , V) = f (, E, U , V) - Lorth - R(E) for a fixed  and

fG() = -ExDG [A(D(x))] for fixed U , V, and E, where Lorth(U , V) =

L i=1

(

Ui

Ui - Ii

2 F

+

Vi

Vi - Ii

2 F

).

We

maximize

fD(E, U , V)

for

ndis

iterations

(ndis



1)

followed

by

minimizing

fG() for one iteration. Note that we use a log D trick (Goodfellow et al., 2014) to ease the

computation of minimizing fG(). Detailed implementations are provided in Appendices B and C.2.

We choose tuning parameters  = 10 and  = 1 in all the experiments except for the Divergence

regularizer, where we pick  = 10 and  = 0.054. We take 100K iterations in all the experiments on

CIFAR-10 and 200K iterations on STL-10 as suggested in Miyato et al. (2018).

To solve (5), we adopt the setting in Radford et al. (2015), which has been shown to be robust for
different GANs by Miyato et al. (2018). Specifically, we use the Adam optimizer (Kingma & Ba, 2014) with the following hyperparameters: (1) ndis = 1; (2)  = 0.0002, the initial learning rate; (3) 1 = 0.5, 2 = 0.999, the first and second order momentum parameters of Adam respectively.

Before we present our results, we show the effectiveness of our proposed reparameterization,

which aims to approximate the singular values of weight matrices while avoiding direct SVDs.

As can be seen, in Table 1, Ui and Vi have nearly orthonormal columns respectively, i.e.,

Ui

Ui - Ii

2F ,

Vi

Vi - Ii

2 F



10-4.

Although

the

reparameterization

introduces

more

model

parameters, it maintains comparable computational efficiency. See more details in Appendix D.2.

Table 1: The sub-orthogonality of Ui's and Vi's in the discriminator with the divergence regularizer on CIFAR-10 after 100K iterations. For other settings, we also observe that all Ui's and Vi's have nearly orthonormal columns.

Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6

U

U -I

2 F

2.3e-5

V

V

-I

2 F

7.9e-5

1.2e-5 1.0e-5

1.5e-5 1.7e-5

1.6e-5 2.5e-5

2.7e-5 4.1e-5

2.5e-5 7.1e-5

2.1e-5 3.9e-5

Figure 4 shows that the singular value decays of weight matrices with two different methods: SN-GAN and D-
optimal regularizer with spectral normalization. As can
be seen, our method achieves a slower decay in singular
values than that of SN-GAN. See more results of other
methods in Appendix D.3. Such a slower decay improves
the performance of GANs. Specifically, Table 2 presents
the inception scores and FIDs of our proposed methods
as well as other methods on CIFAR-10 and STL-10. As
can be seen, under CNN architecture, our methods achieve significant improvements on STL-10. Compared with STL-10, CIFAR-10 is easy to learn, and thus GAN training can only limitedly benefits from encouraging the slow singular value decay. As a result, on CIFAR-10, our meth- Figure 3: Inception scores on ImageNet. ods slightly improve the result of SN-GAN. Moreover, We can see that our method outperforms as shown in Figure 5, our method, D-optimal regularizer SN-GAN by a significant margin.
with spectral normalization, achieves better performance.
Moreover, it converge faster than SN-GAN.

4.2 RESNET-GAN

We also test our proposed method on ResNet, a more advanced structure, on both discriminator and generator (Appendix C.2). For these experiments, we adopt the hinge loss for adversarial training on

4In fact, the performance is not sensitive to these hyperparameters, since we only observe negligible difference by fine tuning these parameters.

7

Under review as a conference paper at ICLR 2019

5K <latexit sha1_base64="fQNjsXvv77ZFTKNs46ptPdo5j4A=">AAAB63icbVBNSwMxEJ2tX7V+VT16CbaCp7JbKHosehG8VLAf0C4lm2bb0CS7JFmhLP0LXjwo4tU/5M1/Y7bdg7Y+GHi8N8PMvCDmTBvX/XYKG5tb2zvF3dLe/sHhUfn4pKOjRBHaJhGPVC/AmnImadsww2kvVhSLgNNuML3N/O4TVZpF8tHMYuoLPJYsZASbTKo2qvfDcsWtuQugdeLlpAI5WsPy12AUkURQaQjHWvc9NzZ+ipVhhNN5aZBoGmMyxWPat1RiQbWfLm6dowurjFAYKVvSoIX6eyLFQuuZCGynwGaiV71M/M/rJya89lMm48RQSZaLwoQjE6HscTRiihLDZ5Zgopi9FZEJVpgYG0/JhuCtvrxOOvWa59a8h3qleZPHUYQzOIdL8OAKmnAHLWgDgQk8wyu8OcJ5cd6dj2VrwclnTuEPnM8fzpSNaA==</latexit>

50K <latexit sha1_base64="SCdrPHwveBnAmuaOhBkdFRTwVTQ=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LLaCp5IURI9FL4KXCrYW2lA220m7dLMJuxuhhP4GLx4U8eoP8ua/cdvmoK0PBh7vzTAzL0gE18Z1v53C2vrG5lZxu7Szu7d/UD48aus4VQxbLBax6gRUo+ASW4YbgZ1EIY0CgY/B+GbmPz6h0jyWD2aSoB/RoeQhZ9RYqVW9cKt3/XLFrblzkFXi5aQCOZr98ldvELM0QmmYoFp3PTcxfkaV4UzgtNRLNSaUjekQu5ZKGqH2s/mxU3JmlQEJY2VLGjJXf09kNNJ6EgW2M6JmpJe9mfif101NeOVnXCapQckWi8JUEBOT2edkwBUyIyaWUKa4vZWwEVWUGZtPyYbgLb+8Str1mufWvPt6pXGdx1GEEziFc/DgEhpwC01oAQMOz/AKb450Xpx352PRWnDymWP4A+fzBz0GjaI=</latexit>

100K <latexit sha1_base64="gP3C5WM2J0AUjQpgm1tkETwMT00=">AAAB7XicbVBNSwMxEJ31s9avqkcvwVbwVLK96LHoRfBSwX5Au5Rsmm1js8mSZIWy9D948aCIV/+PN/+NabsHbX0w8Hhvhpl5YSK4sRh/e2vrG5tb24Wd4u7e/sFh6ei4ZVSqKWtSJZTuhMQwwSVrWm4F6ySakTgUrB2Ob2Z++4lpw5V8sJOEBTEZSh5xSqyTWhUf48pdv1TGVTwHWiV+TsqQo9EvffUGiqYxk5YKYkzXx4kNMqItp4JNi73UsITQMRmyrqOSxMwE2fzaKTp3ygBFSruSFs3V3xMZiY2ZxKHrjIkdmWVvJv7ndVMbXQUZl0lqmaSLRVEqkFVo9joacM2oFRNHCNXc3YroiGhCrQuo6ELwl19eJa1a1cdV/75Wrl/ncRTgFM7gAny4hDrcQgOaQOERnuEV3jzlvXjv3seidc3LZ07gD7zPH6V7jdg=</latexit>

200K <latexit sha1_base64="Fh60DOMzeQOrdrabVUbEnN0Dfbw=">AAAB7XicbVBNSwMxEJ31s9avqkcvwVbwVLK96LHoRfBSwX5Au5Rsmm1js8mSZIWy9D948aCIV/+PN/+NabsHbX0w8Hhvhpl5YSK4sRh/e2vrG5tb24Wd4u7e/sFh6ei4ZVSqKWtSJZTuhMQwwSVrWm4F6ySakTgUrB2Ob2Z++4lpw5V8sJOEBTEZSh5xSqyTWpUaxpW7fqmMq3gOtEr8nJQhR6Nf+uoNFE1jJi0VxJiujxMbZERbTgWbFnupYQmhYzJkXUcliZkJsvm1U3TulAGKlHYlLZqrvycyEhsziUPXGRM7MsveTPzP66Y2ugoyLpPUMkkXi6JUIKvQ7HU04JpRKyaOEKq5uxXREdGEWhdQ0YXgL7+8Slq1qo+r/n2tXL/O4yjAKZzBBfhwCXW4hQY0gcIjPMMrvHnKe/HevY9F65qXz5zAH3ifP6cDjdk=</latexit>

Figure 4: Illustrations of singular value decay in 7 layers at 5K-th, 50K-th, 100K-th, and 200K-th iteration. The above figures are for the SN-GANs; the below are for D-optimal regularizer with SN.

CIFAR: FID

CIFAR: Inception Score

STL: FID

STL: Inception Score

Figure 5: The inception scores and FID's with error bar over 10 runs. Due to the space limit we only present the comparisaon between SN-GAN and D-optimial regularizer with SN, which is the best among our proposed methods. The full comparison with all proposed methods is in Appendix D.4.

discriminators:

fD (E ,

U, V)

=

ExDG [min

(0,

-1

-

D(x))]

+

1 n

n

min (0, -1 + D(xi)) .

i=1

We also adopt the commonly used hyperparameter settings for the Adam optimizer on ResNet:

ndis = 5,  = 0.0002, 1 = 0, and 2 = 0.9 (Gulrajani et al., 2017). Due to our computational

resource limit, we only test the method of spectral normalization (our version) with D-optimal

regularizer, which achieves the best performance on CNN experiments. We also test on the official

subsampled 64 × 64 ImageNet data using the conditional GAN with a projected discriminator Miyato

& Koyama (2018).

The results of our experiments on CIFAR-10 and STL-10 are listed in Table 2, and results on ImageNet are shown in Figure 3. We see that our method is much better than the other methods
on STL-10 and ImageNet. As for CIFAR-10, our method is better than orthogonal regularizer but
slightly worse than SN-GAN. We believe the reason behind is that CIFAR-10 is relatively easy. As can be seen, for CIFAR-10, the inception scores of all methods are around 8, while the inception score of real data is around 11. In contrast, for STL-10, the inception score of real data is around 26, while inception scores of all methods are less than 10. As a result, when the dataset is complicated and network needs high capacity, our method performs better than SN-GAN.

5 CONCLUSION
In this paper, we propose a new SVD-type reparameterization for weight matrices of the discriminator in GANs, allowing us to efficiently manipulate the spectra of weight matrices. We than establish a new generalization bound of GAN to justify the importance of spectrum control on weight matrices. Moreover, we propose new regularizers to encourage the slow singular value decay. Our experiments on CIFAR-10, STL-10, and ImageNet datasets support our proposed methods, theory, and discoveries.

8

Under review as a conference paper at ICLR 2019

Table 2: The inception scores and FIDs on CIFAR-10 and STL-10. For consistency, we reimplement baselines under our Chainer environment.

Method
Real Data CNN Baseline WGAN-GP Orthogonal Reg. SN-GAN Ours CNN Spectral Norm. Spectral Constraint Lipschitz Reg. SC + Divergence Reg. SN + D-Optimal Reg. ResNet Orthogonal Reg. SN-GAN SN + D-Optimal Reg.

Inception Score CIFAR-10 STL-10 11.24 ± .12 26.08 ± .26

6.72 ± .11 7.31 ± .09 7.39 ± .05

8.42 ± .09 8.77 ± .07 8.83 ± .07

7.35 ± .05 7.43 ± .08 7.43 ± .08 7.44 ± .05 7.48 ± .06

8.69 ± .08 8.97 ± .05 8.99 ± .06 9.21 ± .09 9.25 ± .08

7.90 ± .05 8.21 ± .05 8.06 ± .06

8.83 ± .05 9.15 ± .06 9.65 ± .06

FID CIFAR-10 STL-10
7.8 7.9

39.0 ± .29 54.1 ± .35 25.7 ± .33 44.5 ± .30 24.7 ± .25 45.5 ± .34

25.2 ± .22 24.8 ± .30 24.1 ± .28 24.3 ± .21 23.0 ± .27

44.8 ± .39 44.0 ± .42 45.3 ± .38 41.9 ± .37 40.5 ± .41

22.3 ± .26 44.9 ± .35 19.5 ± .22 43.0 ± .44 20.5 ± .18 39.9 ± .33

REFERENCES
Martín Abadi and David G Andersen. Learning to protect communications with adversarial neural cryptography. arXiv preprint arXiv:1610.06918, 2016.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241­6250, 2017.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215­223, 2011.
DC Dowson and BV Landau. The fréchet distance between multivariate normal distributions. Journal of multivariate analysis, 12(3):450­455, 1982.
Rui Gao, Xi Chen, and Anton J. Kleywegt. Wasserstein distributional robustness and regularization in statistical learning. CoRR, abs/1712.06050, 2017. URL http://arxiv.org/abs/1712. 06050.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5769­5779, 2017.
9

Under review as a conference paper at ICLR 2019
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Günter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565­4573, 2016.
Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, and Bo Li. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. arXiv preprint arXiv:1709.06079, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547, 2017.
Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv preprint arXiv:1802.05637, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In Advances in Neural Information Processing Systems, pp. 5591­5600, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. In Advances in Neural Information Processing Systems, pp. 2015­2025, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211­252, 2015.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pp. 901­909, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
Ron Shepard, Scott R Brozell, and Gergely Gidofalvi. The representation and parametrization of orthogonal matrices. The Journal of Physical Chemistry A, 119(28):7924­7939, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. Cvpr, 2015.
Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence, 30(11):1958­1970, 2008.
CF Jeff Wu and Michael S Hamada. Experiments: planning, analysis, and optimization, volume 552. John Wiley & Sons, 2011.
Sitao Xiang and Hao Li. On the effects of batch and weight normalization in generative adversarial networks. stat, 1050:22, 2017.
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. arXiv preprint arXiv:1801.07892, 2018.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discriminationgeneralization tradeoff in gans. arXiv preprint arXiv:1711.02771, 2017.
10

Under review as a conference paper at ICLR 2019

A PROOF IN SECTION 3

A.1 PROOF OF THEOREM 2

Proof. We bound the output of D(·) as follows,

L

D(x)  WL 2 L-1(· · · 1(W1x) · · · ) 2  · · ·  Bx BWi .

i=1

Consider dF,(µ, n) - infDG dF,(µn, ). We have

dF,(µ, n) - inf dF,(µ, )
 DG

=dF,(µ, n) - dF,(µn, n) + dF,(µn, n) - inf dF,(µ, )
 DG

+ inf dF,(µ, ) - inf dF,(µn, )

 DG

 DG

2 sup Exµ[(A(D(x)))] - Exµn [(A(D(x)))] + .
AD(·)F
Note that given x1, . . . , xi, . . . , xn and x1, . . . , xi, . . . , xn, we have
sup Exµ[(A(D(x)))] + Exµn [(A(D(x)))]
AD(·)F

(9)

- sup Exµ[(A(D(x)))] + Exµn [(A(D(x)))]
AD(·)F

 (A(D(xi))) - (A(D(xi))) m



D(xi) - D(xi) m

L

2Bx BWi .

i=1

Then McDiarmid's inequality gives us, with probability at least 1 - /2,

sup Exµ[(A(D(x)))] - Exµn [(A(D(x)))]
AD(·)F

E sup Exµ[(A(D(x)))] - Exµn [(A(D(x)))]
AD(·)F
By the argument of symmetrization, we have

L
+ 2Bx BWi
i=1

log

2 

2m

.

(10)

E sup Exµ[(A(D(x)))] - Exµn [(A(D(x)))]
AD(·)F

2Exiµ,

1 sup m m AD(·)F i=1

i(A(D(x)))

,

(11)

where i's are i.i.d. Rademacher random variables, i.e., P( i = 1) = P( i = -1) = 1/2. McDiarmid's inequality again gives us, with probability at least 1 - /2, we have

Exiµ,

1 sup m m AD(·)F i=1

i(A(D(x)))

E

1 sup m m AD(·)F i=1

i(A(D(x)))

L
+ 2Bx BWi
i=1

log

2 

2m

.

(12)

Not that E

1 m

supAD(·)F

m i=1

i(A(D(x)))

is essentially the empiricial Rademacher com-

plexity of (A(D(·))). Since  and A are both Lipschitz, by Talagrand's lemma, we have

11

Under review as a conference paper at ICLR 2019

E

1 m

supAD(·)F

m i=1

i(A(D(x)))

 E

1 m

supD

m i=1

iD(x) . We then use the stan-

dard Dudley's entropy integral to bound E

1 m

supD

m i=1

iD(xi) .

We exploit the parametric

form of discriminators to find a tight covering number. We have to investigate the Lipschitz continuity

of D(·) with respect to the weight matrices W1, . . . , WL. We based our argument on telescoping.

Given two sets of weight matrices W1, . . . , WL and W1, . . . , WL and fix the activation operators and A, we have

D(x) - D (x) 

 WLL-1(· · · 1(W1x) · · · ) - WLL-1(· · · 1(W1x) · · · ) 2 = WLL-1(· · · 1(W1x) · · · ) - WLL-1(· · · 1(W1x) · · · ) 2
+ WLL-1(· · · 1(W1x) · · · )WLL-1(· · · 1(W1x) · · · ) 2  WL - WL 2 L-1(· · · 1(W1x) · · · ) 2
+ WL 2 L-1(· · · 1(W1x) · · · ) - L-1(· · · 1(W1x) · · · ) 2
L-1
 WL - WL 2Bx BWi + WL 2 L-1(· · · 1(W1x) · · · ) - L-1(· · · 1(W1x) · · · ) 2
i=1
 ······

 L Bx
i=1

L j=1

BWj

BWi

Wi - Wi 2.

For notational simplicity, we denote LWi = B

.L
j=1

BWi

BWi

When the activation operators and A

are given, function D has a one to one correspondence to weight matrices W1, . . . , WL. Thus, to

construct a covering of F , it is enough to construct matrix coverings of W1, . . . , WL, and their

Cartesian product gives us a covering of F. The standard argument of volume ratio gives us an upper

bound of the covering number of matrices with bounded spectral norms. Suppose M = {M 

Rd×h : M 2  }, the covering number N (M, , · 2) at scale with respect to spectral norm is

bounded by



dh

N (M, , · 2)  1 + min( d, h) .

Therefore, the covering number N (F , , · ) is bounded by

L

N (F , , · )  i=1 N (Wi, LLW -i , · 2)

L


 1 + LLWi min( di, di+1)BWi

di di+1
.

i=1
Take d = max{d1, . . . , dL}. We get
N (F , , · ) 

 1 + dLBx

L i=1

BWi

d2 L
.

Then Dudley's entropy integral gives us

E

1 sup m m D i=1

iD(xi)

 4 + 12 mm

Bx 

L i=1

BWi

 m

log N (F , , · )d



4 m

+

12 m Bx

L

 BWi m

i=1

d2L log

 1 + dLBx

L i=1

BWi



It

is

enough

to

pick



=

1 m

,

which

yields

E

1 sup m m D i=1

iD(xi)



4

12Bx +

m

L i=1

BWi

 d2L log 2 dmLBx
 m

L i=1

BWi

.

.

12

Under review as a conference paper at ICLR 2019

Thus, we immediately have,

E

1 sup m m AD(·)F i=1

i(A(D(xi)))

 4 + 12Bx m

L i=1

BWi

 d2L log 2 dmLBx
 m

L i=1

BWi

.

(13)

Now, combining equations (9), (10), (11), (12), and (13) together, we get

dF,(µ, n)



inf
 DG

dF,(µ, n)

+

16 m

+

48

 d2L log 2 dmL

 m

+ 12

log

1 

,

m

where  = Bx

L i=1

BWi

.

On

the

other

hand,

naively

applying

the

argument

from

Bartlett

et

al.

(2017) yields the generalization bound





dF,(µ, n) 

inf

dF

,

(µ,

n)

+

O

 

 DG



 dL3 log dmL
 m

+ 



log

1 

 

.

m

Combining the two generalization bound together, we get

dF,(µ, n)







inf

dF

,(µ,

n

)

+

O

 

 DG



 dL min(d, L2) log dmL
 m

+ 



log

1 

 

.

m

(14)

13

Under review as a conference paper at ICLR 2019

B ALGORITHM

Recall that we are maximizing the following objective function fD(E, U, V) for discriminator D in

Section 4.1:

fD(E, U , V) = f (, E, U , V) - Lorth(U , V) - R(E).

The detailed training algorithm is described in Algorithm 1:

Algorithm 1 Adversarial training with Spectrum Control of Discriminator, D
Initialization 1: for l = 1, .., L do 2: Determine the rank of l-layer: ri = min{di, di+1}. 3: Initialize Ui  Rdi×ri and Vi  Rdi+1×ri with orthonormal columns. 4: Initialize Ei = Iri . 5: end for
Forward pass Input: mini-batch input Hi  Rm×di from previous layer Output: mini-batch output Si+1  Rm×di+1 Parameters: Ui, Vi, and Ei 1: Perform Singluar value update on Ei 2: Calculate weight matrix: Wi = UiEiVi  Rdi×di+1 . 3: Calculate output: Si+1 = HiWi.
Backward pass Input: activation derivative Si+1 f Output: Hi f, Ui fD, Vi fD, Ei fD 1: Calculate: Hi f = Si+1 f Wi as standard linear module. 2: Calculate: Wi f = Hi Si+1 f as standard linear module. 3: Calculate: Ui f, Vi f, Ei f based on Wi f . 4: Calculate: Ui fD = Ui f - Ui Lorth, Vi fD = Vi f - Vi Lorth. 5: Calculate: Ei fD = Ei f - Ei R(E). 6: Update Ui, Vi, and Ei with the Adam Optimizer.
Singluar value update
Input: Ei Output: Ei with e1i  [0, 1], i.e. the largest singular value is bounded by 1 1: If use spectrum constraint: Ei = g(Ei), where g(·) is the clipping operator defined in (8). 2: If use spectrum normalization: Ei = Ei/e1i . 3: If use Lipschitz regularizer: do nothing, since we do not need to layer-wisely control the Lipschitz
constant in this case.

Note that we omit the bias term for simplicity.

Convolutional Layer: We use the same training algorithm for convolutional layers with only
one additional step, reshaping operation. Denote the weight matrix of a convolutional layer as W C  Rco,ci,kh,kw , where co, ci, (kh, kw) denotes the output channel, the input channel and the kernel size. We only need to reshape W C as W  Rco,ci×kh×kw (Huang et al., 2017), i.e., merging
the last three dimensions while preserving the first dimension.

14

Under review as a conference paper at ICLR 2019

C EXPERIMENT SETTING

C.1 PERFORMANCE MEASURE

Inception score is introduced by Salimans et al. (2016):

I({xi}in=1) := exp(E[DKL[p(y|x)||p(y)]]),

where

p(y)

is

estimated

by

1 n

n i=1

p(y|xi)

and

p(y|x)

is

estimated

by

a

pretrained

Inception

Net,

fincept Szegedy et al. (2015). Following the procedure in Salimans et al. (2016), we calculated the

score for randomly generated 5000 examples from generator for 10 times. The average and the

standard deviation of the inception scores are reported.

Fréchet inception distance (FID) is introduced by Heusel et al. (2017). FID uses 2nd order information

of the final layer of the inception model applied to the examples. To begin with, Fréchet distance (FD,

Dowson & Landau (1982)) is 2-Wasserstein distance between two Gaussian distribution p1 and p2:

F (p1, p2) =

µ1 - µ2

2 2

+

tr[1

+

2

-

2(12)1/2],

where {µ1, 1} and {µ2, 2} are the mean and covariance of p1 and p2 respectively. FID between two image distribution p1 and p2 is the FD between fincept(p1) and fincept(p2), i.e., the distribution
after the inception net transformation. The emperical FID is calculated by sampling 10000 true

images and 5000 images from generator, DG . Different from inception score, multiple repetition of the experiments did not exhibit any notable variations on this score.

Acknowledging that different realizations of Inception Net results in different inception scores (Barratt & Sharma, 2018), we test inception scores with the standard tensorflow inception net for consistency.

15

Under review as a conference paper at ICLR 2019

C.2 NETWORK ARCHITECTURE

Table 3: The standard CNN architecture for CIFAR-10 and STL-10. For CIFAR-10, M = 32, Mg = 4. While for STL-10, M = 48, Mg = 6. The slopes coefficient is 0.1 for all LeakyReLU activations.

(a) Generator
Input: z  R128  N (0, I ) Linear: 128  Mg × Mg× 512 Deconv: [4 × 4, 256, stride = 2] BN, ReLU Deconv: [4 × 4, 128, stride = 2] BN, ReLU Deconv: [4 × 4, 64, stride = 2] BN, ReLU Conv: [3 × 3, 3, stride = 1] Tanh

(b) Discriminator
Input: Image x  RM×M×3 Conv: [3 × 3, 64, stride = 1] LeakyReLU Conv: [4 × 4, 64, stride = 2] LeakyReLU Conv: [3 × 3, 128, stride = 1] LeakyReLU Conv: [4 × 4, 128, stride = 2] LeakyReLU Conv: [3 × 3, 256, stride = 1] LeakyReLU Conv: [4 × 4, 256, stride = 2] LeakyReLU Conv: [3 × 3, 512, stride = 1] LeakyReLU Linear: Mg × Mg× 512  1

Table 4: The ResNet architectures for CIFAR-10 and STL-10 datasets.

(a) CIFAR-10 Generator
Input: z  R128  N (0, I ) Linear: 128  4 × 4× 256 ResBlocks: [256, Up-sampling] ×3
BN,ReLU Conv: [3 × 3, 3, stride = 1], Tanh

(b) CIFAR-10 Discriminator

Input:

Image x  R32×32×3

ResBlocks: [128, Down-Sampling] ×2

ResBlocks: [128] ×2

ReLU, Global sum pooling

Linear: 4 × 4× 128  1

(c) STL-10 Generator
Input: z  R128  N (0, I ) Linear: 128  6 × 6× 512 ResBlock: [256, Up-sampling] ResBlock: [128, Up-sampling] ResBlock: [64, Up-sampling]
BN,ReLU Conv: [3 × 3, 3, stride = 1], Tanh

(d) STL-10 Discriminator
Input: Image x  R48×48×3 ResBlock: [64, Down-Sampling] ResBlock: [128, Down-Sampling] ResBlock: [256, Down-Sampling] ResBlock: [512, Down-Sampling] ResBlock: [1024]
ReLU, Global sum pooling Linear: 3 × 3× 128  1

16

Under review as a conference paper at ICLR 2019

Table 5: The ResNet architectures for ImageNet dataset. Recall that we adopts conditional GAN framework with projection discriminator. The ResBlock is implemented with the conditional batch normalization for the generator. Embed(y), h is the inner product of label embedding, Embed(y), and the hidden state, h, after the global sum pooling. (Miyato & Koyama, 2018). We use the same Residual Block as Gulrajani et al. (2017) describes.

(a) Generator
Input: z  R128  N (0, I ) Linear: 128  4 × 4× 1024 ResBlock: [1024, Up-sampling] ResBlock: [512, Up-sampling] ResBlock: [128, Up-sampling] ResBlock: [64, Up-sampling]
BN,ReLU Conv: [3 × 3, 3, stride = 1], Tanh

(b) Discriminator

Input:

Image x  R64×64×3 Label y  {1, 2, 3, ..., 1000}

ResBlock: [64, Down-Sampling]

ResBlock: [128, Down-Sampling]

ResBlock: [256, Down-Sampling]

ResBlock: [512, Down-Sampling]

ResBlock: [1024, Down-Sampling]

ReLU, Global sum pooling

PLrionejeacrtion+: Embed(y), h + [1024  1]

17

Under review as a conference paper at ICLR 2019

D SUPPLEMENTARY RESULTS
D.1 ACCURACY OF APPROXIMATING SINGULAR VALUE DECOMPOSITION
In this section we evaluate how accurate can Ei = {eij}jri=1 approximate the true singular value Ei = {eji }jri=1 of recovered weight matrix Wi = UiEiVi . In Table 6, we compare the maximum, the minimum, the mean and the variance of Ei and Ei.

Table 6: The accuracy of singular value estimation on CIFAR-10 experiment with divergence regularizer after 100K iterations. For other layers and other setting, we can also observe highly accurate singular value approximation.

Max Min Mean Var
Ei Ei Ei Ei Ei Ei Ei Ei 0-th Layer 1.0000 1.0000 0.8344 0.8344 0.9448 0.9448 0.0037 0.0037 1-th Layer 1.0000 0.9999 0.8128 0.8128 0.9441 0.9441 0.0036 0.0036 2-th Layer 1.0000 0.9999 0.7972 0.7971 0.9438 0.9438 0.0036 0.0036 3-th Layer 1.0000 1.0000 0.7969 0.7969 0.9438 0.9438 0.0036 0.0036 4-th Layer 1.0000 1.0000 0.7816 0.7815 0.9432 0.9432 0.0036 0.0036 5-th Layer 1.0000 1.0002 0.7817 0.7816 0.9434 0.9434 0.0037 0.0037 6-th Layer 1.0000 1.0001 0.7765 0.7765 0.9591 0.9591 0.0033 0.0033

U

U -I

2 F

2.3e-5 1.2e-5 1.5e-5 1.6e-5 2.7e-5 2.5e-5 2.1e-5

V

V

-I

2 F

7.9e-5 1.0e-5 1.7e-5 2.5e-5 4.1e-5 7.1e-5 3.9e-5

D.2 TIMING COMPARISON
We compare the computational time on CIFAR-10 for different methods in Figure 6. We can see Spectral Constraint with Divergence Regularizer is slightly slower than other methods. Note that the training time will be impacted by the hardware condition, i.e. how many tasks are simultaneously running on the same server. In order to leverage such fluctuation, we test timing for every 1000 iterations and then take their average.

Seconds for 1000 generator updates

100

80

60

40

20

0

Orth.

SN-GANSpect.

NorSmp.ect.

Con.Lip.

Reg SC

+

DivS. NRe+gD. -opt.

Reg.

Figure 6: Time cost per 1000 iterations.

D.3 SINGULAR VALUE DECAY PATTERN
Here, we present an illustration of the singular value decays of weight matrices in 7 layers after 5K, 50K, 100K, and 200K iterations on STL-10 Data for different methods in Table 7. On CIFAR-10, we also observe a very similar pattern, and thus we only present the STL-10 results.
18

Under review as a conference paper at ICLR 2019

Table 7: Singular value decays. The top-to-bottom diagrams are for: SN-GAN; SN + D-Optimal Reg.; SC + Divergence Reg.; Spectral Norm.; Spectral Constraint and Lipschitz Reg.

1.0

0-th layer 1-th layer

2-th layer

3-th layer

0.8

4-th layer 5-th layer

6-th layer

0.6

0.4

0.2 0.0
1.0

0.2

0.9

0.8

0.7

0.6 0.0
1.01
1.00
0.99
0.98
0.97
0.96 0.0
1.00 0.95 0.90 0.85 0.80 0.75 0.70 0.65 0.60
0.0
1.01 1.00 0.99 0.98 0.97 0.96 0.95
0.0

0.2 0.2 0.2 0.2

1.04 1.02 1.00 0.98 0.96 0.94 0.92 0.90
0.0

0.2

5K
0.4 0.6 0.4 0.6 0.4 0.6 0.4 0.6 0.4 0.6 0.4 0.6

1.0

0-th layer 1-th layer

2-th layer

3-th layer

0.8

4-th layer 5-th layer

6-th layer

0.6

50K

0.4

0.2

0.8 1.0

0.0 0.2 0.4 0.6

1.0

0.8

0.6

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.4 0.2

0.8 1.0

0.0 0.2 0.4 0.6

1.00

0.95

0.90

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.85 0.80

0.8 1.0

0.0 0.2 0.4 0.6

1.0 0-th layer

1-th layer

2-th layer

0.8

3-th layer 4-th layer

5-th layer

0.6 6-th layer

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.4 0.2 0.0

0.8 1.0

0.0 0.2 0.4 0.6

1.0

0.9

0.8

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.7 0.6 0.5

0.8 1.0

0.0 0.2 0.4 0.6

1.2

1.1

1.0

0-th layer 1-th layer

0.9

2-th layer

3-th layer 4-th layer

0.8

5-th layer

6-th layer

0.8 1.0 0.7 0.0 0.2 0.4 0.6

100K

1.0

0-th layer 1-th layer

2-th layer

3-th layer

0.8 4-th layer

5-th layer

6-th layer

0.6

0.4

0.2

0.8 1.0

0.0 0.2 0.4 0.6

1.0

0.9

0.8

0.7

0.6

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.5 0.4 0.3 0.2

0.8 1.0

0.0 0.2 0.4 0.6

1.00

0.95

0.90

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.85 0.80

0.8 1.0

0.0 0.2 0.4 0.6

1.0 0-th layer

1-th layer

2-th layer

0.8

3-th layer 4-th layer

5-th layer

0.6 6-th layer

0.4

0.2

0.0

0.8 1.0

0.0 0.2 0.4 0.6

1.0

0.9

0.8

0.7

0.6

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.5 0.4 0.3 0.2

0.8 1.0

0.0 0.2 0.4 0.6

1.4

1.3

1.2

1.1

1.0

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.9 0.8 0.7 0.6

0.8 1.0

0.0 0.2 0.4 0.6

200K

1.0

0.8

0.6

0.4

0.2

0.8 1.0

0.0 0.2 0.4 0.6

1.0

0.9

0.8

0.7

0.6

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.5 0.4 0.3 0.2

0.8 1.0

0.0 0.2 0.4 0.6

1.00

0.95

0.90

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.85 0.80

0.8 1.0

0.0 0.2 0.4 0.6

1.0 0-th layer

1-th layer

2-th layer

0.8

3-th layer 4-th layer

5-th layer

0.6 6-th layer

0.4

0.2

0.0

0.8 1.0

0.0 0.2 0.4 0.6

1.0

0.8

0.6

0-th layer 1-th layer

0.4

2-th layer

3-th layer 4-th layer

0.2

5-th layer

6-th layer

0.8 1.0 0.0 0.0 0.2 0.4 0.6

1.6

1.4

1.2

1.0

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer

0.8 0.6 0.4

0.8 1.0

0.0 0.2 0.4 0.6

0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer 0.8 1.0
0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer 0.8 1.0
0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer 0.8 1.0
0.8 1.0
0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer 0.8 1.0
0-th layer 1-th layer 2-th layer 3-th layer 4-th layer 5-th layer 6-th layer 0.8 1.0

D.4 IMAGE GENERATION
For CIFAR-10, the growth of inception score and FID over iterations is shown in Figure 7. While the image generation results are shown in Figure 9.
For STL-10, the growth of inception score and FID over iterations is shown in Figure 8. While the image generation results are shown in Figure 10. For ImageNet, the results are shown in Figure 11

19

Under review as a conference paper at ICLR 2019

Inception score

Fréchet inception distance

7

6
5 7.5 4 7.0
6.5 3
6.0
35 200
30 150 25

SN +D-optimal Reg. SN-GAN Spectrum Normalization Spectrum Constraint Lipschitz Reg. Orthonomal SC + Divergence Reg.

100 30000 40000 50000 60000 70000 80000 90000 100000

50

0 20000 40000 Iteration60s000 80000 100000

Figure 7: The growth of inception score and FID over iterations on CIFAR-10 dataset.

Inception score

Fréchet inception distance

9

8

7 6 9.0 5

4 8.5 3 2 8.0
250 55

SN +D-optimal Reg. SN-GAN Spectrum Normalization Spectrum Constraint Lipschitz Reg. SC + Divergence Reg.

200 50

150 45 40
100 60000 80000 100000 120000 140000 160000 180000 200000

50
0 25000 50000 75000 It1e0r0a0t0i0ons125000 150000 175000 200000

Figure 8: The inception scores and FID's of different methods on STL-10. The top part is for the inception score, and the bottom is for the FID. The inner graph zooms in the results after 55K iterations. The x-axis denotes the number of iterations.

20

Under review as a conference paper at ICLR 2019

(a) Orthonormal

(b) SN-GAN

(c) Spectrum Normalization

(d) Spectrum Constraint

(e) Lipschitz Regularization (f) SC+Divergence Regularizer

(g) SN+D-Optimal Regularizer Figure 9: Image generation on CIFAR-10 dataset.
21

Under review as a conference paper at ICLR 2019

(a) SN-GAN

(b) Spectrum Normalization

(c) Spectrum Constraint

(d) Lipschitz Regularization

(e) SC+Divergence Regularizer

(f) SN+D-Optimal Regularizer

Figure 10: Image generation on STL-10 dataset.

22

Under review as a conference paper at ICLR 2019

(a) SN-GAN

(b) SN+D-Optimal Regularizer

Figure 11: Image generation on ImageNet.

(a) Valley

(b) Jellyfish

(c) Pizza

(d) Sea Anemone

(e) Shoji

(f) Brain Coral

(g) Cardoon

(h) Altar

(i) Jack-o'-lantern

Figure 12: Conditional Image generation on ImageNet (SN+D-Optimal Regularizer)

23

