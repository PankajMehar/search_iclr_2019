Under review as a conference paper at ICLR 2019
LEARNING MORE INTERPRETABLE, BACKPROPAGATION-FREE DEEP ARCHITECTURES WITH KERNELS
Anonymous authors Paper under double-blind review
ABSTRACT
One can substitute each neuron in any neural network with a kernel machine and obtain a counterpart powered by kernel machines. The new network inherits the expressive power and architecture of the original but works in a more intuitive way since each node enjoys the simple interpretation as a hyperplane (in a reproducing kernel Hilbert space). Further, using the kernel multilayer perceptron as an example, we prove that in classification, an optimal representation that minimizes the risk of the network can be characterized for each hidden layer. This result removes the need of backpropagation in learning the model and can be generalized to any feedforward kernel network. Moreover, unlike backpropagation, which turns models into black boxes, the optimal hidden representation enjoys an intuitive geometric interpretation, making the dynamics of learning in a deep kernel network simple to understand. Empirical results are provided to validate our theory.
1 INTRODUCTION
Any neural network (NN) can be turned into a kernel network (KN) by replacing each artificial neuron (McCulloch & Pitts, 1943), i.e., learning machine of the form f (x) = (w x + b), with a kernel machine, i.e., learning machine of the form f (x) = w, (x) + b with kernel function k(x, y) = (x), (y) . This combination of connectionism and kernel method enables the learning of hierarchical, distributed representations with kernels.
In terms of training, similar to NN, KN can be trained with backpropagation (BP) (Rumelhart et al., 1986). In the context of supervised learning, the need for BP in learning a deep architecture is caused by the fact that there is no explicit target information to tune the hidden layers (Rumelhart et al., 1986). Moreover, BP is usually computationally intensive and can suffer from vanishing gradient. And most importantly, BP results in hidden representations that are notoriously difficult to interpret or assess, turning deep architectures into "black boxes".
The main theoretical contribution of this paper is the following: Emplying the simplest feedforward, fully-connected KN as an example, we prove that in classification, the optimal representation for each hidden layer that minimizes the risk of the network can be explicitly characterized. This result removes the need for BP and makes it possible to train the network in a completely feedforward, layer-wise fashion. And the same idea can be generalized to other feedforward KNs.
The layer-wise learning algorithm gives the same optimality guarantee as BP in the sense that it minimizes the risk. But the former is much faster and evidently less susceptible to vanishing gradient. Moreover, the quality of learning in the hidden layers can be directly assessed during or after training. Most importantly, the optimal representation for each hidden layer enjoys an intuitive geometric interpretation, making the dynamics of learning in a deep KN much more transparent and interpretable than that in a deep NN.
To further reduce computational complexity, we show that the optimal hidden representations have a "sparse" nature, which makes possible a natural approach to speed up the upper layers of a KN without performance loss, making it a very computationally efficient model in practice.
Empirical results of feedforward, fully-connected KNs trained with the layer-wise algorithm are provided to validate the competence of the models and the effectiveness of the learning method, in which we show that the KNs compare favorably with their NN equivalents trained with BP as well as other popular deep architectures trained with BP together with unsupervised greedy pre-training.
1

Under review as a conference paper at ICLR 2019

(a) (b)
Figure 1: (a) Any NN (left, presented in the usual weight-nonlinearity abstraction) can be abstracted as a "graph" (right) with each node representing a neuron and each edge the input-output relationship between neurons. If a node receives multiple inputs, we view its input as a vector in some Euclidean space, as indicated by the colored rectangles. Under this abstraction, each neuron (f (x) = (w x + b)) can be directly replaced by a kernel machine (f (x) = w, (x) + b with kernel k(x, y) = (x), (y) ) mapping from the same Euclidean space into the real line without altering the architecture and functionality of the model. (b) Illustration for layer-wise optimality drifting away from network-optimality. Consider a two-layer network and let T1, T2 be the target function of the first and second layer, respectively. If the first layer creates error, which is illustrated by F (1)(x) being far away from T1(x), the composed solution F (2)  F (1) on the right is better than that on the left and hence corresponds to the network-wise optimality. But the F (2) on the left is clearly a better estimate to the layer-wise optimality T2 if the quality of estimation is measured by the supremum distance.

2 FROM NEURAL TO KERNEL NETWORKS

In this section, we discuss how to build a KN using a given NN. First, the generic approach is described in Fig. 1a. Note that KN inherits the expressive power of the original NN since a kernel machine is a universal function approximator (Park & Sandberg, 1991; Micchelli et al., 2006) and the two models share the same architecture. However, KN works in a more intuitive way since each node is a simple linear model in a reproducing kernel Hilbert space (RKHS).

We now concretely define the KN equivalent of an l-layer Multilayer Perceptron (MLP), which we shall refer to as the kernel MLP (kMLP). Given a random sample (xn, yn)Nn=1, where (xn, yn)  X1 × Y  Rd0 × R, denote (xn)Nn=1 as S and (yn)nN=1 as YS for convenience. For i = 1, 2, . . . , l, consider kernel k(i) : Xi × Xi  R, Xi  Rdi-1 (for i > 1, di-1 is determined by the width of the
i - 1th layer). k(i)(x, y) = (i)(x), (i)(y) Hi , where (i) is the mapping into RKHS Hi.

The ith layer in a kMLP, denoted F (i), is an array of di kernel machines: F (i) : Xi 

Rdi , F (i) = (f1(i), f2(i), . . . , fd(ii)), a di-tuple. Each fj(i) : Xi  R is a hyperplane in Hi:

fj(i)(x) = wj(i), (i)(x) Hi + bj(i), wj(i)  Hi, b(ji)  R. In practice, fj(i)(x) is usually imple-

mented as

N n=1

n(ij)k(i)(x,

xn)

+

b(ji),

where

the

n(ij),

bj(i)



R

are

the

learnable

parameters.

The

set of mappings {F ( )  · · ·  F (1) : n(ij), bj(i)  R for all admissible n, j, i} defines an l-layer kMLP.

In the rest of this paper, we shall restrict our discussions to this kMLP.

3 ASSUMPTIONS AND NOTATIONS
We now specify the assumptions that we impose on all kernels considered in this paper. First, we consider real, continuous, symmetric kernels only and we call a kernel positive semidefinite (PSD) or positive definite (PD) if for any S, the kernel matrix defined as (G)mn = k(xm, xn) is PSD or PD, respectively. We shall always assume that any kernel considered is at least PSD and that k(i)(x, x) = c < + for all x  Xi and infx,yXi k(i)(x, y) = a > -. It is straightforward to check using Cauchy-Schwarz inequality that the first condition implies maxx,yXi k(i)(x, y) = c. For each fixed x  Xi, we assume that k(i)(x, y), as a function of y, is Lx(i)-Lipschitz with respect to the Euclidean metric on Xi. Let supxXi L(xi) = L(i).
2

Under review as a conference paper at ICLR 2019
The following notations will be used whenever convenient: We use the shorthand F (1)(S) for (F (1)(xn))Nn=1. For i = 2, 3, . . . , l, F (i)(S) := F (i)  F (i-1)  · · ·  F (1)(S). Throughout this paper, notations such as F (i) can either be used to denote a set of functions or a specific function in some set depending on the context. Also, when there is no confusion, we shall supress the dependency of any loss function on the example for brevity, i.e., for a loss function , instead of writing (f (x), y), we shall write (f ).
4 A LAYER-WISE LEARNING ALGORITHM
To simplify discussion, we shall restrict ourselves to binary classification (Y = {+1, -1}) and directly give the result on classification with more than two classes in the end. A generalization to regression is reserved for future work. Again, we only focus on kMLP although the idea can be directly generalized to all feedforward KNs. We now discuss the layer-wise learning algorithm, beginning by addressing the difficulties with training a deep architecture layer-by-layer.
4.1 FUNDAMENTAL DIFFICULTIES
There are two fundamental difficulties with learning a deep architecture layer-wise. First, the hidden layers do not have supervision (labels) to learn from. And it depends on BP to propagate supervision from the output backward (Rumelhart et al., 1986). We shall prove that for kMLP, one can characterize the optimal target representation for each hidden layer, which induces a risk for that layer. The target is optimal in the sense that it minimizes the risk of the subsequent layer and eventually that of the network if all layers succeed in learning their optimal representations. This optimal representation defines what we call "layer-wise optimality".
The other difficulty with layer-wise learning is that for any given hidden layer, when the upstream layers create error, layer-wise optimality may not coincide with "network-wise optimality", i.e., the solution of this layer that eventually leads to the composed solution that minimizes the risk of the network in this suboptimal case. Indeed, when a hidden layer creates error, the objective of any layer after it becomes learning a solution that is a compromise between one that is close to the layer-wise optimality and one that prevents the error from the "bad" layer before it from "getting through" easily. And the best compromise is the network-wise optimality. The two solutions may not coincide, as shown in the toy example in Fig. 1b.
Clearly, we would like to always learn the network-wise optimality at each layer, but the learner is blind to it if it is only allowed to work on one layer at a time. By decomposing the overall error of the network into error at each layer, we prove that in fact, network-wise optimality is learnable for each hidden layer even in a purely layer-wise fashion.
4.2 THE OPTIMAL HIDDEN REPRESENTATIONS
We now address the first difficulty in layer-wise learning. The basic idea is first described in Section 4.2.1. Then we provide technical results in Section 4.2.2 and Section 4.2.3 to fill in the details.
4.2.1 BASIC IDEA
Given a deep architecture F := F (l)  · · ·  F (1) and a loss function l defined for this network which induces a risk
Rl = E l(F)
that we wish to minimize, BP views this problem in the following way: Rl is a function of F. The learner tries to find the optimal F that minimizes Rl using the random sample S with labels YS according to some learning paradigm such as Empirical Risk Minimization (ERM) or Structural Risk Minimization (SRM) (Vapnik, 2000; Shalev-Shwartz & Ben-David, 2014). S is considered as fixed in the sense that it cannot be adjusted by the learner.
Alternatively, one can view Rl as a function of F (l) and the learner tries to find the optimal F (l) using random sample Sl-1 with labels YS according to some learning paradigm, where Sl-1 :=
3

Under review as a conference paper at ICLR 2019

F (l-1)  · · ·  F (1)(S)1. The advantage is that the learner has the freedom to learn both the function F (l) and the random sample Sl-1. And since Sl-1 determines the decision of the learning paradigm, which then determines Rl, Rl is now essentially a function of both F (l) and Sl-1:
Rl = E l(F (l), Sl-1).
The key result is that independently of the actual learning of F (l), one can characterize the sufficient condition on Sl-1 under which Rl, as a function of Sl-1, is minimized, as we shall prove. In other words, the "global minimum" of Rl w.r.t. Sl-1 can be explicitly identified prior to any training. This gives the optimal Sl-1, which we denote as Sl-1.
Moreover, the characterization of Sl-1 gives rise to a new loss function l-1 and thus also a new risk Rl-1 that is a function of F (l-1)  · · ·  F (1). Consequently, the same reasoning would allow us to deduce Sl-2 before the learner learns F (l-1). And this analysis can be applied to each layer, eventually leading to a greedy learning algorithm that sequentially learns F (1), F (2)F (1), . . . , F (l) F (l-1)· · ·F (1), in that order, where the asterisk on the superscript indicates that the corresponding layer has been learned and frozen.
The layer-wise learning algorithm provides a framework that enjoys great flexibility. To be specific, one could stop the above analysis at any layer i, then learn layers i + 1, . . . , l in a greedy fashion but still learn layers 1, . . . , i together with BP. Thus, it is easy to see that BP can be brought under this framework as a special case. Nevertheless, in later text, we shall stay on the one end of the spectrum where each layer is learned individually for clarity.
We now present the formal results that give the optimal hidden representations. By the reasoning above, the analysis starts from the last hidden layer (layer l - 1) and proceeds backward.

4.2.2 FORMAL RESULTS: Sl-1

To begin with, we need to approximate the true classification error Rl since it is not computable. To this end, we first review a well-known complexity measure.
Definition 4.1 (Gaussian complexity (Bartlett & Mendelson, 2002)). Let P be a probability distribution on a metric space X and suppose x1, . . . , xN are independent random elements distributed as P. Let F be a set of functions mapping from X into R. Define

G^N (F ) = E

sup
f F

2N N gnf (xi)
n=1

x1, . . . , xN

,

where g1, . . . , gN are independent standard normal random variables. The Gaussian complexity of F is GN (F ) = E G^N (F ).

Intuitively, Gaussian complexity quantifies how well a given function class can fit a sequence of N random variables sampled according to a standard normal distribution. Based on this complexity measure, we have the following bound on the expected classification error.
Theorem 4.2. (Bartlett & Mendelson, 2002) For each F mapping S into Xl-1, let Fl,A = {f : x  w, (l)(F (x)) Hl + b | w Hl  A, b  R}. Fix A,  > 0, with probability at least 1 -  and for any N  N, every function f (l) in Fl,A satisfies

P (yf (l)(x)  0)  R^l(f (l)) + 2GN (Fl,A) +

8 +1


log(4/) ,
2N

where

R^l(f (l))

=

1 N

N n=1

max(0,

1

-

ynf

(l)(xn)/)

is

the

empirical

hinge

loss.

Given the assumptions on k(l), for any F , we have

GN (Fl,A)  2A

c .
N

1This is in fact a set of random samples as F (l-1)  · · ·  F (1) is a set of functions.

4

Under review as a conference paper at ICLR 2019

Without loss of generality, we shall set hyperparameter  = 1. We now characterize Sl-1. Note that

for the following lemma only, we add the assumption that

N n=1

1{yn=+}

=

N n=1

1{yn=-}.

For

a

given f (l), let A be the smallest positive real number such that f (l)  Fl,A.

Lemma 4.3. Given any learning paradigm minimizing R^l(f (l)) +  GN (Fl,A) using representation

Sl-1 = F (S), where  is any positive constant satisfying  < N (c - a)/(8c), suppose the learning paradigm returns f (l) . If the representation F (S) satisfies

k(l)(F (x+), F (x-)) = a and k(l)(F (x), F (x )) = c

(1)

for all pairs of x+, x- from distinct classes in S and all pairs of x, x from the same class, then for any N  N, R^l(f (l) ) +  GN (Fl,A) is minimized over all representations to which the corresponding
f (l) achieves zero hinge loss on at least one example from each class.

The optimal representation Sl-1, characterized by Eq. 1, enjoys a straightforward geometric interpretation: Examples from distinct classes are as distant as possible in the RKHS whereas examples from the same class are as concentrated as possible (see proof (C) of Lemma 4.3 for a rigorous justification). Intuitively, it is easy to see that such a represnetation is the "easiest" for the classifier.
The conditions in Eq. 1 can be concisely summarized in an ideal kernel matrix G defined as
(G )mn = a, if ym = yn;
(G )mn = c, if ym = yn.
And to have the l - 1th layer learn Sl-1, it suffices to train it to minimize some dissimilarity measure between G and the kernel matrix computed from k(l) and F (l-1)(S), which we denote Gl-1. Empirical alignment (Cristianini et al., 2002), L1 and L2 distances between matrices can all serve as the dissimilarity measure. To simplify discussion, we let the dissimilarity measure be the L1 distance
l-1(F (l-1), (xm, ym), (xn, yn)) = |(G )mn - (Gl-1)mn|.
This specifies R^l-1(F (l-1)) as the sample mean of ( l-1(F (l-1), (xm, ym), (xn, yn)))Nm,n=1 and Rl-1 as the expectation of l-1 over (X1, Y )×(X1, Y ). Note that due to the boundedness assumption on k(l), l-1  2 max(|c|, |a|).

4.2.3 FORMAL RESULTS: Sl-2, . . . , S1

Similar to Section 4.2.2, we first need to approximate Rl-1.
Lemma 4.4. For j = 1, 2, . . . , dl-1, let fj(l-1)  Fl-1, where Fl-1 is a given hypothesis space. There exists an absolute constant C > 0 such that for any N  N, with probability at least 1 - ,

Rl-1(F (l-1))



R^l-1(F (l-1))

+

4L(l) C dl-1 max(|c|, |a|)

GN

(Fl-1)

+

8 log(2/) .
N

We are now in a position to characterize Sl-2. For the following lemma only, we further assume that k(l-1)(x, y), as a function of (x, y), depends only on and strictly decreases in x - y 2 for all x, y  Xl-1 with k(l-1)(x, y) > a and that the infimum infx,yXl-1 k(l-1)(x, y) = a is attained in Xl-1
at all x, y with x - y 2  . Suppose infx,yXl-1; x-y 2< k(l-1)(x, y)/ x - y 2 = (l-1) is defined and is positive.

Consider an F mapping S into Xl-1, let Fl-1,A = {f : x  w, (l-1)(F (x)) Hl-1 + b | w Hl-1  A, b  R}. For a given F (l-1) = (f1(l-1), . . . , fd(ll--11)), let A be the smallest positive real number such that fj(l-1)  Fl-1,A for all j.

Lemma 4.5. Given any learning paradigm minimizing R^l-1(F (l-1)) +  GN (Fl-1,A) us-

ing representation (l-1) dl-1(c - a)

Sl-2 = F (S),

N m,n=1

1{ym =yn }

where /( 2cN


3/2

is ),

any positive constant satisfying  < suppose the learning paradigm returns

F (l-1) = (f1(l-1) , . . . , fd(ll--11) ). If the representation F (S) satisfies

k(l-1)(F (x+), F (x-)) = a and k(l-1)(F (x), F (x )) = c

5

Under review as a conference paper at ICLR 2019

for all pairs of x+, x- from distinct classes in S and all pairs of x, x from the same class, then for any N  N, R^l-1(F (l-1) ) +  GN (Fl-1,A) is minimized over all representations to which the corresponding F (l-1) achieves zero loss on at least one pair of examples from distinct classes.
Applying this analysis to the rest of the hidden layers, it is evident that each of these layers should be trained to minimize the difference between G and its own kernel matrix. Generalizing to classification with more than two classes requires no change to the algorithm since the definition of G is agnostic to the number of classes involved in the classification task.
Now since the optimal representation is consistent across layers, the dynamics of layer-wise learning in a kMLP is clear: The network maps the random sample sequentially through layers, with each layer trying to map examples from distinct classes as far as possible in the RKHS while keeping examples from the same class in a cluster as concentrated as possible. In other words, each layer learns a more separable representation of the sample. Eventually, the output layer works as a classifier on the final representation and since the representation would be "simple" after the mappings of the lower layers, the learned decision boundary would generalize better to unseen data.

4.3 FINDING NETWORK-WISE OPTIMALITY VIA LAYER-WISE LEARNING

We now discuss how to learn the network-wise optimality for each layer by working with only one layer at a time. A rigorous description of the problem of layer-wise optimality drifting away from network-wise optimality and the search for a solution begins with the following bound on the total error of a two-layer kMLP.
Lemma 4.6. For any i = 2, . . . , l, let the target function and the approximation function be Ti, F (i) : Xi  Rdi , respectively. Let i = F (i) - Ti s := supxXi F (i)(x) - Ti(x) 2, we have

F (i)  F (i-1) - Ti  Ti-1



 i + i-1

s

di
2L(i)
j=1

fj(i)

2
.
Hi

(2)

By applying the above bound sequentially from the input layer to the output, we can decompose the error of an arbitrary kMLP into the error of the layers. This result gives a formal description of the problem: The hypothesis with the minimal norm minimizes the propagated error from upstream, but evidently, this hypothesis is not necessarily close to the layer-wise optimality Ti.

Moreover, this bound also provides the insight needed for learning the network-wise optimality

independently at each layer: Searching for the network-wise optimality amounts to sequentially

minimizing the r.h.s. of Eq. 2 from the input layer forward. And it is easy to see that the solution

that minimizes R^i(F (i)) +  maxj{1,2,...,di}

fj(i)

, where  > 0 is a hyperparameter, provides
Hi

a good approximate to the minimizer of the r.h.s. of Eq. 2 if, of course,  is chosen well. This

learning problem is solvable independently at each layer and the optimality of the optimal hidden

representations are still guaranteed by Lemma 4.3 and Lemma 4.5. Note that for BP, one usually also

needs to heuristically tune the regularization coefficient for weights as a hyperparameter.

4.4 ACCELERATING THE UPPER LAYERS

There is a natural method to accelerate the upper layers (all but the input layer): The optimal

representation F (S) is sparse in the sense that (F (xm)) = (F (xn)) if ym = yn and (F (xm)) = (F (xn)) if ym = yn (see the proof (C) of Lemma 4.3). Since a kernel machine built on this

representation of the given sample is a function in the RKHS that is contained in the span of the

image of the sample, retaining only one example from each class would result in exactly the same

hypothesis space because trivially, we have {

N n=1

n(F

(xn

))|n



R}

=

{+(F (x+)) +

-(F (x-))|+, -  R} for arbitrary x+, x- in S when the representation F (S) is optimal.

Thus, after training a given layer, depending on how close the actual kernel matrix is to the ideal one,

one can (even randomly) discard a large portion of centers for kernel machines of the next layer to

speed up the training of it without sacrificing performance. As we will later show in the experiments,

randomly keeping a fraction of the training sample as centers for upper layers produces performance

comparable to or better than that obtained with using the entire training set.

6

Under review as a conference paper at ICLR 2019
5 FURTHER ANALYSIS
Due to page limit, some further analysis is relegated to the Appendix. Namely, in Appendix B.1, we give a bound on the Gaussian complexity of an l-layer kMLP, which describes the intrinsic model complexity of kMLP. In particular, the bound describes the relationship between the depth/width of the model and the complexity of its hypothesis space, providing useful information for model selection. In Appendix B.2, we give a constructive result stating that the dissimilarity measure being optimized at each hidden layer will not increase as training proceeds from the input layer to the output. This also implies that a deeper kMLP performs at least as well as its shallower counterparts in minimizing any loss function they are trained on. In Appendix B.3, a result similar to Lemma 4.3 is provided, stating that the characterization for the optimal representation can be made much simpler if one uses a more restricted learning paradigm. In fact, in contrast to Lemma 4.3, both necessary and sufficient conditions can be determined under the more restricted setting. In Appendix B.4, we provide a standard, generic method to estimate the Lipschitz constant of a continuously differentiable kernel, as this quantity has been repeatedly used in many of our results in this paper. In Appendix B.5, we state some advantages of kMLP over classical kernel machines. In particular, empirical results are provided in Appendix B.5.1, in which a two-layer kMLP consistently outperforms the classical Support Vector Machine (SVM) (Cortes & Vapnik, 1995) as well as several SVMs enhanced by Multiple Kernel Learning (MKL) algorithms (Bach et al., 2004; Go¨nen & Alpaydin, 2011).
6 RELATED WORKS
The idea of combining connectionism with the classical kernel method was initiated by Cho & Saul (2009). In their work, an "arc cosine" kernel was so defined as to imitate the computations performed by a one-layer MLP. Zhuang et al. (2011) extended the idea to arbitrary kernels with a focus on MKL, using an architecture very similar to a two-layer MLP with neurons substituted by kernel machines. As a further generalization, Zhang et al. (2017) independently proposed kMLP and the KN equivalent of CNN. However, they did not extend the idea to any arbitrary NN.
There are other works aiming at building "deep" kernels using approaches that are rather different in spirit from the above ones. Wilson et al. (2016) proposed to learn the covariance matrix of a Gaussian process using an NN in order to make the kernel "adaptive". This idea also underlies the now standard approach of combining a deep NN with an SVM for classification, which was first explored by Huang & LeCun (2006) and Tang (2013). It is worth noting that such an interpretation can be given to KNs as well, as we point out in Appendix B.5. Mairal et al. (2014) proposed to learn hierarchical representations by learning the mappings of kernels designed to be invariant to irrelevant variations in images.
Much works have been done to improve or substitute BP in learning a deep architecture. Most aim at improving the classical method, working as add-ons for BP. The most notable ones are perhaps the popular unsupervised greedy pre-training techniques proposed by Hinton et al. (2006) and Bengio et al. (2007). To the best of our knowledge, among the works that try to completely substitute BP, none provides a comparable optimality guarantee in theory as that given by BP. Fahlman & Lebiere (1990) pioneered the idea of greedily learn the architecture of an NN. In their work, each new node is added to maximize the correlation between its output and the residual error signal. Several authors explored the idea of approximating the error signals propagated by BP locally at each layer or each node (Bengio, 2014; Carreira-Perpinan & Wang, 2014; Lee et al., 2015; Balduzzi et al., 2015; Jaderberg et al., 2016). Zhou & Feng (2017) proposed a BP-free deep architecture based on decision trees, but the idea is very different from our work. Raghu et al. (2017) attempted to quantify the quality of hidden representations toward learning more interpretable deep architectures, sharing a motivation similar to ours.
7 EXPERIMENTS
We now evaluate kMLP on five vision benchmarks (Larochelle et al., 2007). The data sets consist of 28 × 28 grayscale images. The first three are binary classifications, the fourth and fifth are variants of MNIST (LeCun et al., 2010). More details are provided in Appendix A. In these experiments, we compared kMLP with other popular deep architectures including MLP, Deep Belief Network
7

Under review as a conference paper at ICLR 2019

Table 1: Test error (%) and 95% confidence interval (%). When two results have overlapping confidence intervals, they are considered equivalent. Best results are marked in bold. For kMLP-1FAST and kMLP-2FAST, we also include (in parentheses) the ratio of the training set retained as centers, i.e.,
the number of training examples randomly chosen as centers for the first/second hidden layer divided
by the size of the entire training set.

MLP-1 MLP-2 DBN-1 DBN-3 SAE-3 KMLP-1 KMLP-2 KMLP-1FAST KMLP-2FAST

RECTANGLES
5.37± 0.20 4.36± 0.18 4.71± 0.19 2.60± 0.14 2.41± 0.13 2.24± 0.13 2.24± 0.13 2.36± 0.13 (0.05) 2.21± 0.13 (0.3/0.3)

RECTANGLES-IMAGE
28.82± 0.40 25.69± 0.38 23.69± 0.37 22.50± 0.37 24.05± 0.37 23.29± 0.37 23.30± 0.37 23.86± 0.37 (0.01) 23.24± 0.37 (0.01/0.3)

CONVEX
30.07± 0.40 25.68± 0.38 19.92± 0.35 18.63± 0.34 18.41± 0.34 19.15± 0.34 18.53± 0.34 20.34± 0.35 (0.17) 19.32± 0.35 (0.005/0.03)

MNIST(10K)
4.69± 0.19 4.42± 0.18 3.94± 0.17 3.11± 0.15 3.46± 0.16 3.10± 0.15 3.16± 0.15 2.95± 0.15 (0.1) 3.18± 0.15 (0.3/0.3)

MNIST(10K) ROTATED
18.11± 0.34 17.22± 0.33 14.69± 0.31 10.30± 0.27 10.30± 0.27 11.09± 0.28 10.53± 0.27 12.61± 0.29 (0.1) 10.94± 0.27 (0.1/0.7)

(DBN) (Hinton & Salakhutdinov, 2006) and Stacked Autoencoder (SAE) (Vincent et al., 2010). Note that we only focused on comparing with the standard, generic architectures because kMLP, as the KN equivalent of MLP, does not have a specialized architecture or features designed for certain application domains. These experiments also serve as proofs of concept for the learning approach, showing that the layer-wise algorithm is competent when compared to standard BP as well as BP combined with some popular greedy pre-training techniques. In particular, the DBNs and SAEs in the experiments had all been pre-trained using standard unsupervised greedy methods (Hinton et al., 2006; Bengio et al., 2007) before they were trained with BP.
The experimental setup is as follows, kMLP-1 corresponds to a one-hidden-layer, greedily-learned kMLP with the first layer consisting of 15 to 150 kernel machines using the same Gaussian kernel (k(x, y) = e- x-y 2/2 ) and the second layer being a single or ten (depending on the number of classes) kernel machines using another Gaussian kernel. Note that the Gaussian kernel does not satisfy the condition that the infimum a is attained (see the extra assumptions before Lemma 4.5), but for practical purposes, it suffices to set the corresponding entries of the ideal kernel matrix to some small value. For all of our experiments, we set (G )mn = 1 if ym = yn and 0 otherwise. Hyperparameters were selected using the validation set. The validation set was then used in final training only for early-stopping based on validation error. We also included results from the same kMLP for which we accelerated by randomly choosing a fraction of the training set as centers for the second layer after the first had been trained (kMLP-1FAST). The kMLP-2 and kMLP-2FAST are the two-hidden-layer kMLPs, the second hidden layers of which contained 15 to 150 kernel machines.2 We compared kMLP with a one/two-hidden-layer MLP (MLP-1/MLP-2), a one/three-hidden-layer DBN (DBN-1/DBN-3) and a three-hidden-layer SAE (SAE-3). Note that the sizes of the hidden layers and the total numbers of parameters of the kMLPs were significantly smaller than those of the corresponding SAEs and DBNs in these experiments, see Appendix A for details.
From Table 1, we see that the performance of kMLP is on par with some of the most popular and most mature deep architectures. Also note that the accelerated kMLPs were significantly faster than the standard ones and produced results comparable to the latter even though we only randomly retained a small portion of centers. This shows that kMLP can be of practical interest even when dealing with the massive data sets in today's machine learning. These results also validate our earlier theoretical results on the layer-wise learning algorithm, showing that it compares favorably not only with BP but also BP enhanced by unsupervised greedy pre-training. Nevertheless, we argue that what makes the layer-wise approach promising is not only the numbers in the table. Namely, this framework of learning makes deep architectures more transparent and intuitive, which can serve as a tentative step toward more interpretable models with strong expressive power. Also, new design paradigms are now possible under the layer-wise framework. For example, each layer can now be "debugged" individually. Moreover, since learning becomes increasingly simple for the upper layers as the representations become more and more well-behaved, these layers are usually very easy to set up and also converge very fast during training.
2A PyTorch-based (Paszke et al., 2017) library for implementing kMLP and the layer-wise learning algorithm as well as the code and data for all experiments in this paper are available at: anonymized URL.
8

Under review as a conference paper at ICLR 2019
REFERENCES
N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, 68(3):337­404, 1950.
F. R. Bach, G. R. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo algorithm. In Proceedings of the twenty-first international conference on Machine learning, pp. 6. ACM, 2004.
David Balduzzi, Hastagiri Vanchinathan, and Joachim M Buhmann. Kickback cuts backprop's red-tape: Biologically plausible credit assignment in neural networks. In AAAI, pp. 485­491, 2015.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Y. Bengio. Learning deep architectures for ai. Foundations and trends R in Machine Learning, 2(1): 1­127, 2009.
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In Advances in neural information processing systems, pp. 153­160, 2007.
Yoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target propagation. arXiv preprint arXiv:1407.7906, 2014.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798­1828, 2013.
Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In Artificial Intelligence and Statistics, pp. 10­19, 2014.
Y. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pp. 342­350, 2009.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273­297, 1995.
Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz S Kandola. On kernel-target alignment. In Advances in neural information processing systems, pp. 367­373, 2002.
Scott E Fahlman and Christian Lebiere. The cascade-correlation learning architecture. In Advances in neural information processing systems, pp. 524­532, 1990.
P. Gehler and S. Nowozin. Infinite kernel learning. 2008.
M. Go¨nen and E. Alpaydin. Multiple kernel learning algorithms. Journal of machine learning research, 12(Jul):2211­2268, 2011.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504­507, 2006.
G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527­1554, 2006.
Geoffrey E Hinton. Distributed representations. 1984.
Fu Jie Huang and Yann LeCun. Large-scale learning with svm and convolutional for generic object categorization. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, volume 1, pp. 284­291. IEEE, 2006.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. arXiv preprint arXiv:1608.05343, 2016.
M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. Lp-norm multiple kernel learning. Journal of Machine Learning Research, 12(Mar):953­997, 2011.
9

Under review as a conference paper at ICLR 2019
G. R. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning the kernel matrix with semidefinite programming. Journal of Machine learning research, 5(Jan):27­72, 2004.
H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pp. 473­480. ACM, 2007.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2, 2010.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation. In Joint european conference on machine learning and knowledge discovery in databases, pp. 498­515. Springer, 2015.
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks. In Advances in neural information processing systems, pp. 2627­2635, 2014.
Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115­133, 1943.
C. A. Micchelli, Y. Xu, and H. Zhang. Universal kernels. Journal of Machine Learning Research, 7 (Dec):2651­2667, 2006.
J. Park and I. W. Sandberg. Universal approximation using radial-basis-function networks. Neural computation, 3(2):246­257, 1991.
Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan. Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration, 2017.
Gilles Pisier. The volume of convex bodies and Banach space geometry, volume 94. Cambridge University Press, 1999.
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems, pp. 6076­6085, 2017.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533­538, 1986.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural networks: A theoretical view. In AAAI, pp. 2066­2072, 2016.
Yichuan Tang. Deep learning using linear support vector machines. arXiv preprint arXiv:1306.0239, 2013.
Vladimir Vapnik. The nature of statistical learning theory. 2000.
M. Varma and B. R. Babu. More generality in efficient multiple kernel learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 1065­1072. ACM, 2009.
P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371­3408, 2010.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artificial Intelligence and Statistics, pp. 370­378, 2016.
Z. Xu, R. Jin, I. King, and M. Lyu. An extended level method for efficient multiple kernel learning. In Advances in neural information processing systems, pp. 1825­1832, 2009.
10

Under review as a conference paper at ICLR 2019 Shuai Zhang, Jianxin Li, Pengtao Xie, Yingchun Zhang, Minglai Shao, Haoyi Zhou, and Mengyi
Yan. Stacked kernel network. arXiv preprint arXiv:1711.09219, 2017. Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an alternative to deep neural networks. arXiv
preprint arXiv:1702.08835, 2017. J. Zhuang, I. W. Tsang, and S. C. Hoi. Two-layer multiple kernel learning. In Proceedings of the
Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 909­917, 2011.
11

Under review as a conference paper at ICLR 2019
APPENDIX A EXPERIMENTAL SETUP
The first data set, known as rectangles, has 1000 training images, 200 validation images and 50000 test images. The learning machine is required to tell if a rectangle contained in an image has a larger width or length. The location of the rectangle is random. The border of the rectangle has pixel value 255 and pixels in the rest of an image all have value 0. The second data set, rectangles-image, is the same with rectangles except that the inside and outside of the rectangle are replaced by an image patch, respectively. rectangles-image has 10000 training images, 2000 validation images and 50000 test images. The third data set, convex, consists of images in which there are white regions (pixel value 255) on black (pixel value 0) background. The learning machine needs to distinguish if the region is convex. This data set has 6000 training images, 2000 validation images and 50000 test images. The fourth data set contains 10000 training images, 2000 validation images and 50000 test images taken from MNIST. The fifth is the same as the fourth except that the digits have been randomly rotated. Sample images from the data sets are given in Fig. 2. For actual training and testing, the pixel values were normalized to [0, 1]. For detailed descriptions of the data sets, see (Larochelle et al., 2007).

Figure 2: From left to right: example from rectangles, rectangles-image, convex, mnist(10k) and mnist(10k) rotated.

For the other models, hyperparameters were also selected using the validation set. For the MLPs, the sizes of the hidden layers were chosen from the interval [25, 700]; for DBN-3 and SAE-3, the sizes of the three hidden layers varied in intervals [500, 3000], [500, 4000] and [1000, 6000], respectively. DBN-1 used a much larger hidden layer than DBN-3 to obtain comparable performance. A simple calculation shows that the total numbers of parameters in the kMLPs were fewer than those in the corresponding DBNs and SAEs by orders of magnitude in all experiments. Like in the training for the kMLPs, the validation set were also reserved for early-stopping in final training. The DBNs and SAEs had been pre-trained unsupervisedly before the supervised training phase, following the algorithms described in (Hinton et al., 2006; Bengio et al., 2007). More detailed settings for these models were reported in (Larochelle et al., 2007).

APPENDIX B FURTHER ANALYSIS

In this section, we provide some further analysis on kMLP and the layer-wise learning algorithm.

B.1 GAUSSIAN COMPLEXITY OF KMLP

We first give a result on the Gaussian complexity of a two-layer kMLP.

Lemma B.1. Given kernel k : X2 × X2  R, where X2  Rd1 . Let F2 = {f : X2  R, f (x) =

m =1

 k(x ,

x)

+

b

|



=

(1,

.

.

.

,

m)



Rm,



1  A, b  R}, where the x are arbitrary

examples from X2.

Consider F1 = {(f1, . . . , fd1 ) : X1  X2 | fj  }, where  is a given hypothesis space

that is closed under negation, i.e., if f  , then -f  . Define F2  F1 = {h : x 

m =1



k(F

(x

),

F

(x))

+

b

|

 1  A, b  R, F  F1}.

If the range of some element in  contains 0, we have

GN (F2  F1)  ALd1GN ().

The above result can be easily generalized to kMLP with an arbitrary number of layers.

12

Under review as a conference paper at ICLR 2019

Lemma B.2. Given an l-layer kMLP, for each fj(i)(x) =

m =1

(ij)k(i)(F

(i-1)(x

),

F

(i-1)

(x))

+

b,

(ji) := (1(ij), . . . , m(i)j)  Rm, b  R, assume (ji) 1  Ai and let dl = 1. Denote the class of

functions implemented by this kMLP as Fl, we have

l
GN (Fl)  d1 AiL(i)diGN ().
i=2

Proof. It is trivial to check that the hypothesis space of each layer is closed under negation and that there exists a function in each of these hypothesis spaces whose range contains 0. Then the result follows from repeatedly applying Lemma B.1.

B.2 NONINCREASING LOSS ACROSS LAYERS

Lemma B.3. For i  2, assume k(i) is PD and fix layers 1, 2, . . . , i - 1 at arbitrary states F (1), F (2), . . . , F (i-1). Let the loss functions i, i-1 be the same up to their domains, and de-
note both i and i-1 as . Suppose layer i is trained with a gradient-based algorithm to minimize the loss (F (i)). Denote the state of layer i after training by F (i). If di-1 = di, there is an initialization
for F (i) such that

(F (i))  (F (i-1)).

(3)

Calculation for this initialization is specified in the proof.
For i = 1, under the further assumption that X1  Rd0 and d0 = d1, Eq. 3 becomes (F (1))  (F (0)), where F (0) is the identity map on X1.
Remark B.3.1. For the greedily-trained kMLP, Lemma B.3 applies to the hidden layers and implicitly requires that k(i+1) = k(i) since the loss function for layer i, when viewed as a function of F (i), takes the form (k(i+1)(F (i))) and can be rewritten as k(i+1) (F (i)). Similarly, (k(i)(F (i-1))) is k(i) (F (i-1)). Since Lemma B.3 assumes to be the same across layers (otherwise it does not make sense to compare between layers), this forces =k(i+1) .k(i) Then it follows that k(i+1) = k(i).
Further, if k(i+1) and k(i) have the property that k(x, y) = k(x¯, y¯), where x¯, y¯ denote the images of x, y under an embedding of Rp into Rq (p  q) defined by the identity map onto a p-dimensional subspace of Rq, then the condition di-1 = di can be relaxed to di-1  di.

This lemma states that for a given kMLP, when it has been trained upto and including the ith hidden layer, the i + 1th hidden layer can be initialized in such a way that the value of its loss function will be lower than or equal to that of the ith hidden layer after training. In particular, the actual hidden representation "converges" to the optimal represetation as training proceeds across layers. On the other hand, when comparing two kMLPs, this result implies that the deeper kMLP will not perform worse in minimizing the loss function than its shallower counterpart.
In deep learning literature, results analogous to Lemma B.3 generally state that in the hypothesis space of a NN with more layers, there exists a hypothesis that approximates the target function nontrivially better than any hypothesis in that of another shallower network (Sun et al., 2016). Such an existence result for kMLP can be easily deduced from the earlier bound on its Gaussian complexity (see Lemma B.2). However, these proofs of existence do not guarantee that such a hypothesis can always be found through learning in practice, whereas Lemma B.3 is constructive in this regard. Nevertheless, one should note that Lemma B.3 does not address the risk R = E . Instead, it serves as a handy result that guarantees fast convergence of upper layers during training in practice.

B.3 SIMPLER OPTIMAL REPRESENTATION UNDER MORE RESTRICTED LEARNING PARADIGMS
The following lemma states that if we are willing to settle with a more restricted learning paradigm, the necessary and sufficient condition that guarantees the optimality of a representation can be characterized and is simpler than that described in Lemma 4.3. The setup for this lemma is the same as that of Lemma 4.3 except that the assumption that the numbers of examples from the two classes are equal is not needed.

13

Under review as a conference paper at ICLR 2019
Lemma B.4. Consider a learning paradigm that minimizes R^l(f (l)) +  GN (Fl,A) using representation Sl-1 = F (S) under the constraint that it returns an f (l)  Fl,A with R^l(f (l) ) = 0, where  is any positive constant. For any N  N, R^l(f (l) ) +  GN (Fl,A) is minimized over all linearly separable representations if and only if the representation F (S) satisfies
k(l)(F (x+), F (x-)) = a
for all pairs of x+, x- from distinct classes in S.
B.4 LIPSCHITZ CONSTANT FOR CONTINUOUSLY DIFFERENTIABLE KERNELS
In general, for a continuously differentiable function f : R  R with derivative f and any a, b  R, a < b, we have
bb
|f (b) - f (a)| = f (x)dx  |f (x)|dx  max |f (x)|(b - a).
a a axb
This simple result can be used to bound the Lipschitz constant of a continuously differentiable kernel. For example, for Gaussian kernel k : X × X  R, X  R, k(x, y) = e-(x-y)2/2 , we have k(x, y)/y = 2(x - y)k(x, y)/2. Hence for each fixed x  X, k(x, y) is Lipschitz in y with Lipschitz constant bounded by supyX 2(x - y)k(x, y)/2 . In practice, X is always compact and can be a rather small subspace of some Euclidean space after normalization of data, hence this would provide a reasonable approximation to the Lipschitz constant of Gaussian kernel.
B.5 COMPARING KMLP WITH CLASSICAL KERNEL MACHINES
There are mainly two issues with classical kernel machines besides their usually high computational complexity. First, despite the fact that they are capable of universal function approximation (Park & Sandberg, 1991; Micchelli et al., 2006) and that they enjoy a very solid mathematical foundation (Aronszajn, 1950), kernel machines are unable to learn multiple levels of distributed representations (Bengio et al., 2013), yet learning representations of this nature is considered to be crucial for complicated artificial intelligence (AI) tasks such as computer vision, natural language processing, etc. (Bengio, 2009; LeCun et al., 2015). Second, in practice, performance of a kernel machine is usually highly dependent on the choice of kernel since it governs the quality of the accessible hypothesis space. But few rules or good heuristics exist for this topic due to its extremely taskdependent nature. Existing solutions such as MKL (Bach et al., 2004; Go¨nen & Alpaydin, 2011) view the task of learning an ideal kernel for the given problem to be separate from the problem itself, necessitating either designing an ad hoc kernel or fitting an extra trainable model on a set of generic base kernels, complicating training.
kMLP learns distributed, hierarchical representations because it inherits the architecture of MLP. To be specific, first, we see easily that the hidden activation of each layer, i.e., F (i)(x)  Rdi , is a distributed representation (Hinton, 1984; Bengio et al., 2013). Indeed, just like in an MLP, each layer of a kMLP consists of an array of identical computing units (kernel machines) that can be activated independently. Further, since each layer in a kMLP is built on top of the previous layer in exactly the same way as how the layers are composed in an MLP, the hidden representations are hierarchical (Bengio et al., 2013).
Second, kMLP naturally combines the problem of learning an ideal kernel for a given task and the problem of learning the parameters of its kernel machines to accomplish that task. To be specific, kMLP performs nonparametric kernel learning alongside learning to perform the given task. Indeed, for kMLP, to build the network one only needs generic kernels, but each layer F (i) can be viewed as a part of a kernel of the form k(i+1)(F (i)(x), F (i)(y)). The fact that each F (i) is learnable makes this kernel "adaptive", mitigating to some extent any limitation of the fixed generic kernel k(i+1). The training of layer i makes this adaptive kernel optimal as a constituent part of layer i + 1 for the task the network was trained for. And it is always a valid kernel if the generic kernel k(i+1) is. Note that this interpretation has been given in a different context by Huang & LeCun (2006) and Bengio et al. (2013), we include it here only for completeness.
14

Under review as a conference paper at ICLR 2019

Table 2: Average test error (%) and standard deviation (%) from 20 runs. Results with overlapping 95% confidence intervals (not shown) are considered equivalent. Best results are marked in bold. The average ranks (calculated using average test error) are provided in the bottom row. When computing confidence intervals, due to the limited sizes of the data sets, we pooled the 20 random samples.

Breast Diabetes Australian Iono Ringnorm Heart Thyroid Liver German Waveform Banana
Rank

Size/Dimension
683/10 768/8 690/14 351/33 400/20 270/13 140/5 345/6 1000/24 400/21 400/2
-

SVM
3.2± 1.0 23.3± 1.8 15.4± 1.4 7.2± 2.0 1.5± 0.7 17.9± 3.0 6.1± 2.9 29.5± 4.1 24.8± 1.9 11.0± 1.8 10.3± 1.5
4.2

MKLLEVEL
3.5± 0.8 24.2± 2.5 15.0± 1.5 8.3± 1.9 1.9± 0.8 17.0± 2.9 7.1± 2.9 37.7± 4.5 28.6± 2.8 11.8± 1.6 9.8± 2.0
6.3

LpMKL
3.8± 0.7 27.4± 2.5 15.5± 1.6 7.4± 1.4 3.3± 1.0 23.3± 3.8 6.9± 2.2 30.6± 2.9 25.7± 1.4 11.1± 2.0 12.5± 2.6
7.0

GMKL
3.0± 1.0 33.6± 2.5 20.0± 2.3 7.3± 1.8 2.5± 1.0 23.0± 3.6 5.4± 2.1 36.4± 2.6 29.6± 1.6 11.8± 1.8 16.6± 2.7
6.9

IKL
3.5± 0.7 24.0± 3.0 14.6± 1.2 6.3± 1.0 1.5± 0.7 16.7± 2.1 5.2± 2.0 40.0± 2.9 30.0± 1.5 10.3± 2.3 9.8± 1.8
4.3

MKM
2.9± 1.0 24.2± 2.5 14.7± 0.9 8.3± 2.7 2.3± 1.0 17.6± 2.5 7.4± 3.0 29.9± 3.6 24.3± 2.3 10.0± 1.6 19.5± 5.3
5.4

2LMKL
3.0± 1.0 23.4± 1.6 14.5± 1.6 7.7± 1.5 2.1± 0.8 16.9± 2.5 6.6± 3.1 34.0± 3.4 25.2± 1.8 11.3± 1.9 13.2± 2.1
5.0

2LMKLINF
3.1± 0.7 23.4± 1.9 14.3± 1.6 5.6± 0.9 1.5± 0.8 16.4± 2.1 5.2± 2.2 37.3± 3.1 25.8± 2.0 9.6± 1.6 9.8± 1.6
2.8

kMLP-1
2.4± 0.7 23.2± 1.9 13.8± 1.7 5.0± 1.4 1.5± 0.6 15.5± 2.7 3.8± 2.1 28.9± 2.9 24.0± 1.8 10.3± 1.9 11.5± 1.9
1.6

B.5.1 EMPIRICAL RESULTS
We now compare a single-hidden-layer kMLP using simple, generic kernels with SVMs enhanced by MKL algorithms that used significantly more kernels to demonstrate the ability of kMLP to automatically learn task-specific kernels out of standard ones. The standard SVM and seven other SVMs enhanced by popular MKL methods were compared (Zhuang et al., 2011), including the classical convex MKL (Lanckriet et al., 2004) with kernels learned using the extended level method proposed in (Xu et al., 2009) (MKLLEVEL); MKL with Lp norm regularization over kernel weights (Kloft et al., 2011) (LpMKL), for which the cutting plane algorithm with second order Taylor approximation of Lp was adopted; Generalized MKL in (Varma & Babu, 2009) (GMKL), for which the target kernel class was the Hadamard product of single Gaussian kernel defined on each dimension; Infinite Kernel Learning in (Gehler & Nowozin, 2008) (IKL) with MKLLEVEL as the embedded optimizer for kernel weights; 2-layer Multilayer Kernel Machine in (Cho & Saul, 2009) (MKM); 2-Layer MKL (2LMKL) and Infinite 2-Layer MKL in (Zhuang et al., 2011) (2LMKLINF).
Eleven binary classification data sets that have been widely used in MKL literature were split evenly for training and test and were all normalized to zero mean and unit variance prior to training. 20 runs with identical settings but random weight initializations were repeated for each model. For each repetition, a new training-test split was selected randomly.
For kMLP, all results were achieved using a greedily-trained, one-hidden-layer model with the number of kernel machines ranging from 3 to 10 on the first layer for different data sets. The second layer was a single kernel machine. All kernel machines within one layer used the same Gaussian kernel, and the two kernels on the two layers differed only in kernel width . All hyperparameters were chosen via 5-fold cross-validation. As for the other models compared, for each data set, SVM used a Gaussian kernel. For the MKL algorithms, the base kernels contained Gaussian kernels with 10 different widths on all features and on each single feature and polynomial kernels of degree 1 to 3 on all features and on each single feature. For 2LMKLINF, one Gaussian kernel was added to the base kernels at each iteration. Each base kernel matrix was normalized to unit trace. For LpMKL, p was selected from {2, 3, 4}. For MKM, the degree parameter was chosen from {0, 1, 2}. All hyperparameters were selected via 5-fold cross-validation. From Table 2, kMLP compares favorably with other models, which validates our claim that kMLP learns its own kernels nonparametrically hence can work well even without excessive kernel parameterization. Performance difference among models can be small for some data sets, which is expected since they are all rather small in size and not too challenging. Nevertheless, it is worth noting that only 2 Gaussian kernels were used for kMLP, whereas all other models except for SVM used significantly more kernels.

15

Under review as a conference paper at ICLR 2019

APPENDIX C PROOFS

Proof of Lemma 4.3. Throughout this proof we shall drop the layer index l for brevity. Given that the representation satisfies Eq. 1, the idea is to first collect enough information about the returned f = (w , b ) such that we can compute R^(f ) +  GN (FA) and then show that for any other F (S), suppose the learning paradigm returns f = (w , b )  FA , then R^(f ) +  GN (FA )  R^(f ) +  GN (FA). We now start the formal proof.
First note that for f = (w, b) with w H = A, R^(f ) +  GN (FA) can be rewritten as R^(f ) +  w H
by writing 2 c/N  as the new  . Also, in the optimal representation, i.e., an F (S) such that Eq. 1 holds, it is easy to see that (F (x-)) - (F (x+)) H is maximized over all representations for all x-, x+.

Moreover, note that given the representation is optimal, we have (F (x)) = (F (x )) if y = y and (F (x)) = (F (x )) if y = y : Indeed, by Cauchy-Schwarz inequality, for all x, x  S, k(F (x), F (x )) = (F (x)), (F (x )) H  (F (x)) H (F (x )) H and the equality holds if and only if (F (x)) = p(F (x )) for some real constant p. Using the assumption on k, namely, that (F (x)) H = c for all F (x), we further conclude that the equality holds if and only if p = 1. And the second half of the claim follows simply from c > a. Thus, all examples from the + and -
class can be viewed as one vector (F (x+)) and (F (x-)), respectively.

The returned hyperplane f cannot pass both F (x+) and F (x-), i.e., f (F (x+)) = 0 and f (F (x-)) = 0 cannot happen simultaneously since if so, first subtract b , rotate while keeping w H unchanged and add some suitable b to get a new f such that f (F (x-)) < 0 and f (F (x+)) > 0, then it is easy to see that R^(f ) +  w H < R^(f ) +  w H . But by construc-
tion of the learning paradigm, this is not possible.

Now suppose the learning paradigm returns an f such that

y+f (F (x+)) + y-f (F (x-)) = (F (x+)) - (F (x-)), w H = (F (x+)) - (F (x-)) H w H cos F,w := .

(4)

First note that for an arbitrary F,w ,  is less than or equal to 2 since one can always adjust b such that y+f (F (x+)) = y-f (F (x-)) without changing  and hence having a larger  will not further reduce R^(f ), which is 0 when  = 2, but will result in a larger w H according to Eq. 4. On the other hand, F,w must be 0 since this gives the largest  with the smallest w H . Indeed, if the returned f does not satisfy F,w = 0, one could always shift, rotate while keeping w H fixed and then shift the hyperplane back to produce another f with F,w = 0 and this f results in a larger  if  < 2 or the same  if  = 2 but a smaller w H by rescaling. Hence R^(f ) +  w H < R^(f ) +  w H but again, this is impossible.
Together with what we have shown earlier, we conclude that 2   > 0. Then for some t  R,

R^(f ) = 1 N

N

1{yn=+}

max(0,

1

-

t)

+

1 N

N

1{yn=-} max(0, 1 - ( - t))

n=1

n=1

1 - /2,

if 1  t   - 1



=

1 2

(1

+

t

-



),

if t > 1



1 2

(1

-

t),

if t <  - 1.

Evidently, the last two cases both result in R^(f ) > 1 - /2 hence f must produce a t in [ - 1, 1] and we have

R^(f ) +  w

H

=

1

-

 2

+

=1-  + 2

 (F (x+)) - (F (x-)) H

2(c - a)

=1+

 - 1 , 2(c - a) 2

16

Under review as a conference paper at ICLR 2019

which, by the assumption on the original  in R^(f ) +  GN (FA), strictly decreases in  over (0, 2]. Hence the returned f must satisfy  = 2, which implies R^(f ) = 0 and we have



R^(f ) +  w

H

=

 2 . c-a

Now, for any other F (S), suppose the learning paradigm returns f . Let xw+ , x-w be the pair of examples with the largest f (F (x+)) - f (F (x-)). We have

y+w f (F (xw+ )) + y-w f (F (x-w )) = (F (x+w )) - (F (xw- )) H w H cos F ,w :=  .

Then

R^(f ) +  w H   w H

 = (F (x+w )) - (F (x-w )) H cos F ,w



 | | (F (xw+ )) - (F (x-w )) H

 2 2(c - a)

= R^(f ) +  w H ,

where we have used the assumption that there exists x-, x+ with y-f (F (x-)), y+f (F (x+))  1. This proves the desired result.

Lemma C.1. Suppose f1  F1, . . . , fd  Fd are elements from sets of real-valued functions defined on all of X1, X2, . . . , Xm, where Xj  Rd for all j, F  F1 × · · · × Fd. For f  F , define   f :
X1 × · · · × Xm × Y  R as (x1, . . . , xm, y)  (f1(x1), . . . , fd(x1), f1(x2), . . . , fd(xm), y), where  : Rmd × Y  R+  {0} is bounded and L-Lipschitz for each y  Y with respect to the Euclidean metric on Rmd. Let   F = {  f : f  F }. Denote the Gaussian complexity of Fi on Xj as GNj (Fi), if the Fi are closed under negation, i.e., for all i, if f  Fi, then -f  Fi, we have

dm

GN (  F )  2L

GNj (Fi).

i=1 j=1

(5)

In particular, for all j, if the xnj upon which the Gaussian complexities of the Fi are evaluated are sets of i.i.d. random elements with the same distribution, we have GN1 (Fi) = · · · = GNm(Fi) := GN (Fi) for all i and Eq. 5 becomes
d
GN (  F )  2mL GN (Fi).
i=1

This lemma is a generalization of a result on the Gaussian complexity of Lipschitz functions on Rk from (Bartlett & Mendelson, 2002). And the technique used in the following proof is also adapted from there.

Proof. For the sake of brevity, we prove the case where m = 2. The general case uses exactly the same technique except that the notations would be more cumbersome.
Let F be indexed by A. Without loss of generality, assume |A| < . Define

N

X = (f,1(xn), . . . , f,d(xn), yn)gn;
n=1

Nd

Y = L

(f,i(xn)gn,i + f,i(xn)gN+n,i),

n=1 i=1

17

Under review as a conference paper at ICLR 2019

where   A, the (xn, xn) are a sample of size N from X1 × X2 and g1, . . . , gN , g1,1, . . . , g2N,d are i.i.d. standard normal random variables.

Let arbitrary ,   A be given, define

X - X

2 2

= E(X - X)2, where the expectation is

taken over the gn. Define

Y - Y

2 2

similarly

and

we

have

N2

X - X

2 2

=

(f,1(xn), . . . , f,d(xn), yn) - (f,1(xn), . . . , f,d(xn), yn)

n=1

Nd

 L2

(f,i(xn) - f,i(xn))2 + (f,i(xn) - f,i(xn))2

n=1 i=1

=

Y - Y

2 2

.

By Slepian's lemma (Pisier, 1999) and since the Fi are closed under negation,

N 2

G^N

(



F

)

=

Egn

sup
A

X

 2Egn,i,gN+n,i sup Y
A

 N 2L 2

d

(G^N1 (Fi) + G^2N (Fi)).

i=1

Taking the expectation of the xn and xn on both sides, we have

NN

2 GN (  F ) 

2L 2

d

(G1N (Fi) + GN2 (Fi)).

i=1

Proof of Lemma 4.4. Normalize l-1 to [0, 1] by dividing 2 max(|c|, |a|). Then the loss function becomes

l-1 F (l-1), (xm, ym), (xn, yn)

=

1 2 max(|c|, |a|)

k(l)

F (l-1)(xm), F (l-1)(xn)

- (G )mn .

For each fixed (G )mn,

l-1 F (l-1), (xm, ym), (xn, yn) - l-1 F (l-1), (xm, ym), (xn, yn)



1 2 max(|c|, |a|)

k(l)

F (l-1)(xm), F (l-1)(xn)

- k(l) F (l-1)(xm), F (l-1)(xn)



1 2 max(|c|,

|a|)

k(l) F (l-1)(xm), F (l-1)(xn) - k(l) F (l-1)(xm), F (l-1)(xn)

+ k(l) F (l-1)(xm), F (l-1)(xn) - k(l) F (l-1)(xm), F (l-1)(xn)

 L(l) 2 max(|c|, |a|)

F (l-1)(xn) - F (l-1)(xn)

+
2

F (l-1)(xm) - F (l-1)(xm) 2

L(l) 
max(|c|, |a|)

F (l-1)(xn) - F (l-1)(xn), F (l-1)(xm) - F (l-1)(xm)

.
2

Hence l-1 is L(l)/ max(|c|, |a|)-Lipschitz in (F (l-1)(xm), F (l-1)(xn)) with respect to the Euclidean metric on R2dl-1 for each (G )mn.

The result follows from Lemma C.1 with m = 2, d = dl-1 and Corollary 15 in (Bartlett & Mendelson, 2002).

Proof of Lemma 4.5. This proof uses essentially the same idea as that of Lemma 4.3. Due to the complete dependence of k(l)(x, y) on x - y 2, rewrite k(l) F (l-1)(F (xm)), F (l-1)(F (xn)) as

18

Under review as a conference paper at ICLR 2019

h(l) F (l-1)(F (xm)) - F (l-1)(F (xn)) 2 for some h(l), define µ(ml-n1) = (l-1)(F (xm)) - (l-1)(F (xn)) Hl-1 and we have

R^l-1(F (l-1))

1N = N2

h(l)

m,n=1

1N = N2

h(l)

m,n=1

1N = N2

h(l)

m,n=1

F (l-1)(F (xm)) - F (l-1)(F (xn)) 2 - (G )mn

dl-1
fj(l-1)(F (xm)) - fj(l-1)(F (xn)) 2 - (G )mn
j=1

dl-1 j=1

µm(l-n1)

wj(l-1)

cos Hl-1

F,wj(l-1)

2

- (G )mn .

Given that the representation is optimal and rewriting 2 c/N  as  , we have

R^l-1(F (l-1)) +  max
1jdl-1

wj(l-1) Hl-1

1 = N2
ym =yn

h(l)

dl-1 j=1

µ(ml-n1)

wj(l-1)

cos Hl-1

F,wj(l-1)

2

+  max
1jdl-1

wj(l-1) .Hl-1

-a

The returned F (l-1) must satisfy

dl-1 j=1

µm(l-n1)

wj(l-1)

cos Hl-1

F,wj(l-1)

w1(l-1) Hl-1 = · · · = wd(ll--11) Hl-1 :=

cos F,w = 1.

2  ; w ;Hl-1

The first observation is trivial since if otherwise, one can always reduce the largest wj(l-1) Hl-1 to obtain equality to , this gives the same R^l-1(F (l-1) ) with a smaller  max1jdl-1 wj(l-1) .Hl-1 Note that if during shrinking the largest wj(l-1) Hl-1 , this element ceases to be the largest among all j, we shall continue the process with the new largest instead. To see the rest of the claim, note that for the largest of the wj(l-1) ,Hl-1 we must have F,wj(l-1) = 0 since if not, one could shift and rotate the hyperplane and again obtain the same R^l-1(F (l-1) ) with a smaller  max1jdl-1 wj(l-1) .Hl-1 Reducing the largest wj(l-1) Hl-1 and increasing the second largest by scaling, one would get a smaller risk. It is immediate that the minimal risk (w.r.t. only the first and second largest wj(l-1) )Hl-1 is attained when the largest and the second largest wj(l-1) Hl-1 are equal. Then a similar argument as before gives F,wj(l-1) = 0 for both of them. The rest of the claim follows via repeatedly applying this argument to all the wj(l-1) .Hl-1
Define

|f (F (x+)) - f (F (x-))| = (l-1)(F (x+)) - (l-1)(F (x-)) Hl-1 w .Hl-1

19

Under review as a conference paper at ICLR 2019

Then we have

R^l-1(F (l-1) ) +  max
1jdl-1

wj(l-1)

Hl-1

=

N m,n=1

1{ym =yn }

N2

h(l)

dl-1|f (F (x+)) - f (F (x-))|

+

 |f (F (x+)) - f (F (x-))| (l-1)(F (x+)) - (l-1)(F (x-)) Hl-1

=

N m,n=1

1{ym =yn }

N2

h(l)

dl-1|f (F (x+)) - f (F (x-))|

 |f +

(F (x+)) - f

(F (x-))| .

2(c - a)

-a -a

As we have shown, dl-1|f (F (x+)) - f (F (x-))|  [0, ]. Let  = |f (F (x+)) - f (F (x-))| and differentiate the r.h.s. of the above equation w.r.t.  and using the assumption on the original  ,
we have

dl-1

N m,n=1
N2

1{ym =yn }

dh(l)(t) dt

+

 < 0.
2(c - a)

Hence the overall risk decreases in  over [0, / dl-1], which implies that the returned F (l-1) must have  = / dl-1 and

R^l-1(F (l-1) ) +  max
1jdl-1

wj(l-1)

=Hl-1

 .
2dl-1(c - a)

Now for any other F (S), suppose the learning paradigm returns F (l-1) . Assume without loss of generality that the largest fj(l-1) (F (x+)) - fj(l-1) (F (x-)) over all j, x+ and x- is attained at j = 1 and write w = w1(l-1) , f = f1(l-1) for convenience. Let xw+ , xw- be the pair with the largest f (F (x+)) - f (F (x-)). Note that the assumption on F implies that f (F (x+)) - f (F (x-)) 
/ dl-1. Then we have

R^l-1(F (l-1) ) +  max
1jdl-1

wj(l-1)

Hl-1

  w Hl-1

=

 |f (F (x+)) - f (F (x-))| (l-1)(F (x+)) - (l-1)(F (x-)) Hl-1 | cos F ,w |

 

2dl-1(c - a)

= R^l-1(F (l-1) ) +  max
1jdl-1

wj(l-1)

,Hl-1

proving the lemma.

20

Under review as a conference paper at ICLR 2019

Proof of Lemma 4.6. First, it is trivial that the so-defined s metric is indeed a metric. In particular, it satisfies the triangle inequality. For i = 2, . . . , l,

F (i)  F (i-1) - Ti  Ti-1
s
 F (i)  F (i-1) - F (i)  Ti-1 +
s

F (i)  Ti-1 - Ti  Ti-1
s

 sup
xXi

di

fj(i)  F (i-1)(x) - fj(i)  Ti-1(x)

2
+

i

j=1

= sup
xXi

di j=1

2
fj(i)(), k(i)(, F (i-1)(x)) - k(i)(, Ti-1(x)) Hi + i

 sup
xXi

di j=1

2
fj(i) Hi k(i)(, F (i-1)(x)) - k(i)(, Ti-1(x)) Hi + i

= sup k(i)(, F (i-1)(x)) - k(i)(, Ti-1(x))

xXi

Hi

di j=1

fj(i)

2
+
Hi

i

 sup
xXi

L + L(i)
F (i-1)(x)

(i) Ti-1 (x)

F (i-1)(x) - Ti-1(x)
2

di j=1

fj(i)

2
+
Hi

i



di
2L(i)
j=1

fj(i)

2 Hi

which proves the lemma.

F (i-1) - Ti-1 + i,
s

Proof of Lemma B.1. Since  and F2 are both closed under negation, we have

G^N (F2  F1) = E sup
,F

2 N

Nm
 k(F (x ), F (xn))gn
n=1 =1



E

sup
,F

2 N

mN

| | k(F (x ), F (xn))gn

=1

n=1

2N



N

AE

sup
F

max


n=1

k(F

(x

),

F

(xn

))gn

2N

= AE sup max

N

F

 n=1

k(F (x ), F (xn)) - k(F (x ), 0)

gn

2N

+

N

AE

sup
F

max


n=1

k(F (x ), 0)gn



2N

N

AE

sup
F

max


n=1

LF (x )

F (xn)

2gn



2 ALE sup

N

N F n=1

F (xn)

2gn



2 ALE sup

N

N F n=1

F (xn)

1gn

 ALd1G^N ().

21

Under review as a conference paper at ICLR 2019

Taking expectation with respect to the xn finishes the proof.

Proof of Lemma B.3. For i  1, given the assumption that di-1 = di, we first show that one can find an initialization for layer i, denoted F (i), such that F (i)(S) = F (i-1)(S).

Without loss of generality, assume i = 1, then it amounts to showing that a set of parameters can

be found for the kernel machines f1(1), . . . , fd(11) of F (1) such that when these kernel machines are initialized with this certain set of parameters, this layer agrees with the identity function when

restricted to S. Recall that fj(i)(x) =

N n=1

j(in)k(i)(xn,

x)

and

{j(in)



R

|

n

=

1,

2,

.

.

.

,

N}

is

the

set of trainable parameters for this kernel machine.

Solve equation AG = D for A, where A is the d0×N matrix of parameters defined as (A)nm = n(1m) ,
G is the N × N kernel matrix defined as (G)nm = k(1)(xn, xm), D is a d0 × N matrix whose nth column is the nth example xn = (xn1, xn2, . . . , xnd0 ) for n = 1, 2, . . . , N . It is straightforward that the set of parameters A initializes F (1) to the desired F (1). Moreover, A can always be solved
in closed-form by inverting G since k(1) being PD guarantees that G is invertible.

Combining the above result and the defining property of gradient of the loss function with respect to the parameters, i.e., that gradient points at the direction of the greatest rate of increase of the loss function, we then have
(F (0)) = (F (1))  (F (1)).

Proof of Lemma B.4. In this proof we shall again drop the layer index l for brevity.
Identify f  FA using w, b upto R-scalar multiplication. Fix an F such that (F (S)) is linearly separable (otherwise the learning paradigm is not applicable).
Let x+, x- be the closest pair of examples from distinct classes in H. For a given f = (w, b), let xw-, x+w be a pair of examples from different classes that are closest to f in H in the sense that f (F (xw+)), f (F (x-w)) are the smallest in absolute value among all x+, x-, respectively. Denote the canonical form of f with respect to (F (S)) with equality attained by at least one of xw+, xw- as fF = (wF , bF ), i.e., fF satisfies yfF (x)  1 for all examples in (xn, yn)Nn=1 and for at least one of x-w , x+w, the equality is attained. Define AfF := infbF R wF H .
Suppose the algorithm returns f = (w , b ) and recall that A is the smallest such that f  FA, then for all f = (w, b)  FA such that R^(f ) = 0,
A  w H  wF H  AfF .
And it is easy to see that the last equality holds if and only if bF is such that y-wfF (x-w ) = y+wfF (x+w). Then fF  FA. Since R^(fF ) = 0 and we have the freedom to choose bF such that the last equality holds, by construction of the learning paradigm and the minimality of A, we have

A=

min
f FA

AfF

= AfF .

R^(f )=0

In canonical form, for any fF  FA, choose bF such that wF H = AfF , then

21

=

AfF

AfF

1 
AfF

(F (x+w)) - (F (xw-)), wF H (F (x+)) - (F (x-)), wF H =

(F (x+)) - (F (x-)) H cos F,w.

By construction of the learning paradigm, f must make the equality hold and satisfy F,w = 0.

Hence we have

2

A = AfF =

, (F (x+)) - (F (x-)) H

22

Under review as a conference paper at ICLR 2019

which proves that, as a function of F , A achieves its minimum if and only if F maximizes (F (x+)) - (F (x-)) H . Since

arg max
F

(F (x+)) - (F (x-))

H

= arg max
F

(F (x+))

2 H

+

(F (x-))

2 H

-

2

(F (x+)), (F (x-))

H

= arg min k F (x+), F (x-) ,
F

where we have used the assumption on k, namely, that k(x, x) =

(x), (x) H =

(x)

2 H

=

c,

for all x. It immediately follows that any minimizer F of A must minimize k(F (x+), F (x-)) for all

pairs of examples from opposite classes. This proves the desired result.

23

