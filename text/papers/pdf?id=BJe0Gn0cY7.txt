Under review as a conference paper at ICLR 2019
PREVENTING POSTERIOR COLLAPSE WITH -VAES
Anonymous authors Paper under double-blind review
ABSTRACT
Due to the phenomenon of "posterior collapse", current latent variable generative models pose a challenging design choice which trades-off optimizing the ELBO but handicapping the decoder's capacity and expressivity, or changing the loss to something that is not directly minimizing the description length of the data. In this paper we propose an alternative that utilizes the best, most powerful generative models as decoders, whilst optimizing the proper variational lower bound all while ensuring that the latent variables preserve and encode useful information. -VAEs proposed here achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 × 32.
1 INTRODUCTION
Deep latent variable models trained with amortized variational inference have led to advances in representation learning on high-dimensional datasets (Kingma & Welling, 2013; Rezende et al., 2014). These latent variable models typically have simple decoders, where the mapping from the latent variable to the input space is unimodal. This typically results in representations that are good at capturing the global structure, but fail at capturing more complex local structure (e.g., texture (Larsen et al., 2016)). In parallel, advances in autoregressive models have led to drastic improvements in density modeling and sample quality without explicit latent variables (van den Oord et al., 2016b). While these models are good at capturing local statistics, they often fail to produce globally coherent structures (Ostrovski et al., 2018).
Combining the powerful tractable densities of autoregressive models with the representation learning capabilities of latent variable models could result in higher-quality generative models with useful latent representations. While much prior work has attempted to pair the advantages of these two model by using the autoregressive model as the decoder (mapping from latents to inputs), a common problem remains. The "posterior collapse" which stems from the autoregressive decoder being too expressive to model the data density, and causes the model to learn a trivial posterior collapsing to the prior. In this case, the decoder can ignore the latents (which are effectively pure noise). This phenomenon has been observed by numerous previous works and has been referred to as optimisation challenges of VAEs by Bowman et al. (2015), the information preference property by Chen et al. (2016), and the posterior collapse problems by several others (e.g., van den Oord et al. (2017); Kim et al. (2018)). We adopt the later name throughout the paper.
Ideally, an approach that mitigates posterior collapse would not alter the ELBO training objective, and would allow the practitioner to leverage the most recent advances in powerful autoregressive decoders to improve performance. To the best of our knowledge, no prior work has succeeded at this goal. Existing approaches change the objective (Higgins et al., 2017; Alemi et al., 2017; Zhao et al., 2017; Chen et al., 2016; Lucas & Verbeek, 2017), weaken the decoder (Bowman et al., 2015; Gulrajani et al., 2016), or place strong restrictions on the variational family (van den Oord et al., 2017; Xu & Durrett, 2018; Guu et al., 2017). Additionally, these approaches are often challenging to tune and highly sensitive to hyperparameters (Alemi et al., 2017; Chen et al., 2016).
In this paper, we propose -VAEs, a simple framework for choosing variational families that prevent posterior collapse without altering the ELBO training objective or weakening the decoder. By re-
1

Under review as a conference paper at ICLR 2019

stricting the parameters or family of the posterior, we ensure that there is a minimum KL divergence, , between the posterior and the prior. We demonstrate the effectiveness of this approach at learning latent-variable models with powerful decoders on images (CIFAR-10, and ImageNet 32 × 32), and text (LM1B). We achieve SOTA log-likelihood results with image models by additionally in-
troducing a sequential latent-variable model with an anti-causal encoder structure. Our experiments demonstrate the utility of -VAEs at learning useful representations for downstream tasks without sacrificing performance on density modeling.

2 MITIGATING POSTERIOR COLLAPSE WITH -VAES
Our proposed -VAE builds upon the framework of variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) for training latent-variable models with amortized variational inference. Our goal is to train a generative model p(x, z) to maximize the marginal likelihood log p(x) on a dataset. As the marginal likelihood requires computing an intractable integral over the unobserved latent variable z, VAEs introduce an encoder network q(z|x) and optimize a tractable lower bound (the ELBO): log p(x)  Ezq(z|x) [log p(x|z)] - KL(q(z|x) p(z)). The first term is the reconstruction term, while the second term (KL) is the rate term, as it measures how many nats on average are required to send through the latent variables from the encoder (q(z|x)) to the decoder (p(z|x)) (Matthew D. Hoffman, 2016; Alemi et al., 2017).
The problem of posterior collapse is when the rate term, KL(q(z|x) p(z)) reduces to 0. In this case, the approximate posterior q(z|x) is trivially equal to the prior p(z), thus the latent variables do not convey any information about the input x. A necessary condition if we want representations to be meaningful is to have the rate term be positive.
In this paper we propose to address the posterior collapse problem with structural constraints on the prior and posterior such that the divergence between them is lower bounded by design. This can be achieved by choosing families of distributions for the prior and approximate posterior, p(z) and q(z|x) such that min, DKL(q(z|x) p(z))  . We refer to  as the committed rate of the model.
Note that a trivial choice for p and q to have a non-zero committed rate is to set them to Gaussian distributions with fixed (but different) variance term. We study a variant of this case in the experiments, and provide more details of this setup in the Appendix D. In the following section we describe our choices for p and q , but others should also be explored in future work.

2.1 -VAE WITH SEQUENTIAL LATENT VARIABLES

Intuitively, data such as speech, natural images and text exhibit strong spatiotemporal continuity. Our aim is to model variations in such data through latent variables, so that we have control over not just the global characteristics of the generated samples (e.g., existence of an object), but also can influence their finer, often shifting attributes such as texture and pose in the case of natural images, tone, volume and accent in the case of speech, or style and sentiment in the case of natural language. Sequences of latent variables can be an effective modeling tool for expressing the occurrence and evolution of such features throughout the sequence.

To define the -VAE in this sequential setting, we model the posterior distribution of each timestep as

q(zt|x) = N (zt; µt(x), t(x)). Therefore zi and zj are conditionally independent given x. For the

prior, we use the first-order linear autoregressive process, known as AR(1). This means that at each

timestep, zt = zt-1+ t where t is a Gaussian noise with zero mean and 2 constant variance. The conditional probability for the latent variable can be expressed as p(zt|z<t) = N (zt; zt-1,  ).

This process is wide-sense stationary (that is, having constant sufficient statistics through its time

evolution) if ||<

1.

If so, then zt has zero mean and variance of

2 1-2

.

It is thus convenient

to choose  = 1 - 2. The mismatch in the correlation structure of the prior and the posterior

results in the following positive lower-bound on the KL-divergence between the two distributions

(see Appendix C for derivation):

2

Under review as a conference paper at ICLR 2019

DKL(q(z|x)

p(z)) 

1 2

d
(n - 2) ln(1 + k2) - ln(1 - k2)

k=1

(1)

where n is the length of the sequence and d is the dimension of the latent variable at each timestep.
The amount of committed rate between the prior and the posterior is easily controlled by equating the right hand side of the inequality in equation 1 to a given rate c and solving for . In Fig. 1, we show the scaling of the minimum rate as a function of  and the behavior of -VAE in 2d.

Rate (nats)

8

7 n=2

6 n=8

5 4

n = 16

3

2

1

0

 = 0.00 R = 0.00

 = 0.50 R = 0.14

 = 0.80 R = 0.51

 = 0.95 R = 1.17

Prior

Posterior

0.0 0.2 0.4 0.6 0.8 1.0 

Figure 1: Effect of  in a toy model. Fitting an uncorrelated Gaussian for the posterior, q(z), to a correlated Gaussian prior, p(z), by minimizing KL(q(z) p(z)) over . Left: committed rate () as a function of the prior squared correlation  and the dimensionality n. Right: contours of the
optimal posterior and prior in 2d. As the correlation increases, the minimum rate grows.

2.1.1 RELATION TO PROBABILISTIC SLOWNESS PRIOR
The linear autoregressive process we have chosen as our main prior over the latent variables specifies the degree of temporal correlation in the latent space. The transitions in the latent space are smoother the closer  is to one. On the other hand in the limit of  approaching 0, the prior becomes the same as the independent standard Gaussian prior where there are no correlations between timesteps. This exact prior form has indeed been suggested (Turner & Sahani, 2007) to be, in the limit of infinte sequence length, the probabilistic counterpart to Slow Feature Analysis (Wiskott & Sejnowski, 2002). SFA has been shown to be an effective method for learning invariant spatio-temporal features (Wiskott & Sejnowski, 2002). In our models, we infer latent variables with multiple dimensions per timestep, each with a different slowness filter imposed by a different value of , corresponding to features with different speed of variation.
2.2 ANTI-CAUSAL ENCODER NETWORK
Having a high capacity autoregressive network as the decoder implies that it can accurately estimate p(xt|x<t). Given this premise, what kind of complementary information can latent variables provide? Encoding information about the past seems wasteful as the autoregressive decoder has full access to past observations already, so encoding them in latent variables seems wasteful. On the other hand, if we impose conditional independence between observations and latent variables at other timesteps given the current one (i.e., p(xt|z) = p(xt|zt)), there will then be at best (by the data processing inequality (Cover & Thomas, 2006)) a break-even situation between the KL cost of encoding information in zt and the resulting improvement in the reconstruction loss. There is therefore no advantage for the model to utilize the latent variable even if it would transmit to the decoder the unobserved xt. The situation is, however, different when zt can inform the decoder at multiple timesteps, encoding mutual information between xt and x>t. In this setting, the decoder pays the KL cost for the mutual information once, but is able to leverage the transmitted information multiple times to reduce its entropy about future predictions.
We introduce an encoder structure motivated by this argument that explicitly restricts the latents at each timestep to encode information about the future. We refer to this structure as the anti-causal posterior structure as shown in Fig. 2. Alternatively, one can consider the non-causal structure that allows latents be inferred from all observations were the inference and generative graphs are densely connected. The non-causal setup thus becomes very similar to non-temporal latent variable models as there now is no temporal order in either the encoder or the decoder. The anti-causal structure is a subgraph of the non-causal structure, so it is possible that in the course of training the model may

3

Under review as a conference paper at ICLR 2019

learn to ignore the rest of the non-causal inference network. Introducing this additional restriction is thus giving the model an additional inductive bias which can be useful insofar as the underlying premises hold true for the dataset and the decoder model. We compare the performance of both approaches in different settings in Sect. 4.

Generative Model z1 x1,:

AntiCausal Inference Model z1 x1,:

z2 x2,:

z2 x2,:

z3 x3,:

z3 x3,:

Figure 2: Generative structures for the inference of sequential latent variables. The anti-causal structure introduces an inductive bias to encode in each latent variable information about the future

3 RELATED WORK
The main focus of our work is on representation learning and density modeling in latent variable models with powerful decoders. Earlier work has focused on this kind of architecture, but has addressed the problem of posterior collapse in different ways.
In terms of our architecture, the decoders for our image models build on advances in autoregressive modeling from van den Oord et al. (2016b); Salimans et al. (2017); Chen et al. (2017); Parmar et al. (2018). Unlike prior models, we use sequential latent variables to generate the image row by row. This differs from Gregor et al. (2016), where the latents are sequential but the entire image is generated at each timestep. Our sequential image generation model resembles latent variable models used for timeseries (Chung et al., 2015; Babaeizadeh et al., 2017; Denton & Fergus, 2018) but does not rely on KL annealing, and has an additional autoregressive dependence of the outputs over time (rows of the image). Another main difference between our work and previous sequential latent variable models is our proposed Anti-Causal structure for the inference network (see Sect. 2.2). We motivate this structure from a code-efficiency and representation learning standpoints and demonstrate its effectiveness empirically in Sect. 4. For textual data, we use the Transformer architecture from Vaswani et al. (2017) as our main blueprint for the decoder. As shown in Sect. 4, our method is able to preserve the performance of these models, in terms of likelihoods, whilst learning informative latent variables.
To prevent posterior collapse, most prior work has focused on modifying the training objective. Bowman et al. (2015); Yang et al. (2017); Kim et al. (2018) and Gulrajani et al. (2016) use an annealing strategy, where they anneal the weight on the rate from 0 to 1 over the course of training. This approach does not directly optimize a lower-bound on likelihood for most of training, and tuning the annealing schedule to prevent collapse can be challenging (see Sect. 4). Similarly, Higgins et al. (2017) proposes using a fixed coefficient > 1 on the rate term to learn disentangled representations. Zhao et al. (2017) adds a term to the objective to pick the model with maximal rate. Chen et al. (2016); Kingma et al. (2016) use free-bits to allow the model to hit a target minimum rate, but the objective is non-smooth which leads to optimization difficulties in our hands, and deviations from a lower bound on likelihood when the soft version is used with a coefficient less than 1. Lucas & Verbeek (2017) add an auxiliary objective that reconstructs a low-resolution version of the input to prevent posterior collapse. Alemi et al. (2017) argue that the ELBO is a defective objective function for representation learning as it does not distinguish between models with different rates, and advocate for model-selection based on downstream tasks. Their method for sweeping models was to use -VAE with different coefficients, which can be challenging as the mapping from  to rate is highly nonlinear, and model- and data-dependent. While we adopt the same perspective as Alemi et al. (2017), we present a new way of achieving a target rate while optimizing the vanilla ELBO objective.
4

Under review as a conference paper at ICLR 2019
Most similar to our approach is work on constraining the variational family to regularize the model. VQ-VAE (van den Oord et al., 2017) uses discrete latent variables obtained by vector quantization of the latent space. Using discrete latent variables with a uniform prior over the outcome yields a fixed KL divergence equal to log K, where K is the size of the codebook. A number of recent papers have also used von Mises-Fisher (vMF) distribution to obtain a fixed KL divergence and mitigate the posterior collapse problem. In particular, Guu et al. (2017); Xu & Durrett (2018); Davidson et al. (2018) use vMF(µ, ) distribution with a fixed as their posterior, and the uniform distribution (i.e. vMF(·, 0)) as the prior. The mismatching prior-posterior thus give a constant KL divergence. As such, this approach can be considered as the continuous analogue of VQ-VAE. Unlike the VQVAE and vMF approach which have a constant KL divergence for every datapoint, -VAE can allow higher KL for different datapoints. This allows the model to allocate more bits for more complicated inputs, which has been shown to be useful for detecting outliers in datasets (Alemi et al., 2018). As such, -VAE may be considered a generalisation of fixed-KL approaches.
4 EXPERIMENTS
4.1 NATURAL IMAGES
We applied our method to generative modeling of images on the CIFAR-10 (Krizhevsky et al.) and downsampled Imagenet (Deng et al., 2009) (32 × 32 as prepared in van den Oord et al. (2016a)) datasets. We describe the main components in the following. The details of our hyperparameters can be found in Appendix E.
Decoder: Our decoder network is closest to PixelSNAIL (Chen et al., 2017) but also incorporates elements from the original GatedPixelCNN (van den Oord et al., 2016b). In particular, as introduced by Salimans et al. (2017) and used in Chen et al. (2017), we use a single channel network to output the components of discretized mixture of logistics distributions for each channel, and linear dependencies between the RGB colour channels. As in PixelSNAIL, we use attention layers interleaved with masked gated convolution layers. We use the same architecture of gated convolution introduced in van den Oord et al. (2016b). We also use the multi-head attention module of Vaswani et al. (2017). To condition the decoder, similar to Transformer and unlike PixelCNN variants that use 1x1 convolution, we use attention over the output of the encoder. The decoder-encoder attention is causally masked to realize the anti-causal inference structure, and is unmasked for the non-causal structure.
Encoder. Our encoder also uses the same blueprint as the decoder. To introduce the anti-causal structure the input is reversed, shifted and cropped by one in order to obtain the desired future context. Using one latent variable for each pixel is too inefficient in terms of computation. We, therefore, encode each row of the image with a multi-dimensional latent variable.
Auxiliary Prior. Tomczak & Welling (2017); Matthew D. Hoffman (2016) show that VAEs performance can suffer when there is a significant mismatch between the prior and the aggregate posterior, q(z) = ExD [q(z|x)]. When such a gap exists, the decoder is likely to have never seen samples from regions of the support of the prior distribution to which the marginal posterior assigns small probability mass. This phenomenon can be exacerbated in -VAEs, where the systematic mismatch between the prior and the posterior might induce a large gap between the prior and aggregate posterior. Increasing the complexity of the variational family can reduce this gap (Rezende & Mohamed, 2015), but require changes in the objective to control the rate and prevent posterior collapse (Kingma et al., 2016). To address this limitation, we adopt the approaches of van den Oord et al. (2017); Roy et al. (2018) and train a rich auxiliary prior over the course of learning to match the aggregate posterior, but that does not influence the encoder or decoder. We used a simple autoregressive model for the auxiliary prior paux: a single-layer LSTM network with conditional-Gaussian outputs.
4.1.1 DENSITY ESTIMATION RESULTS
We begin by comparing our approach to prior work on CIFAR-10 and downsampled Imagenet 32x32 in Table 1. As expected, we found that the capacity of the employed autoregressive decoder had a large impact on the overall performance. Nevertheless, our models have a negligible gap compared to their powerful autoregressive counterparts, while also learning informative latent variables. In comparison, (Chen et al., 2016) had a 0.03 bits per dimension gap with respect to their employed
5

Under review as a conference paper at ICLR 2019

PixelCNN++ architecture1. On Imagenet 32x32, our latent variable model achieves on par performance with purely autoregressive Image Transformer (Parmar et al., 2018). On CIFAR-10 we achieve a new state of the art of 2.83 bits per dimension, again matching the performance of our autoregressive baseline. Note that the values for KL appear quite small as they are reported in bits per dimension (e.g. 0.02 bits/dim translates to 61 bits/image encoded in the latents).
CIFAR10 Test (KL) Imagenet 32x32 Valid (KL)

Latent Variable Models
ConvDraw (Gregor et al. (2016))
DenseNet VLAE (Chen et al. (2016)) -VAE + PixelSNAIL + AR(1) Prior -VAE + PixelSNAIL + Auxiliary Prior

 3.85  2.95  2.85 (0.02)  2.83 (0.01)

-
 3.78 (0.08)  3.77 (0.07)

Autoregressive Models Gated PixelCNN(van den Oord et al. (2016b)) PixelCNN++ (Salimans et al. (2017)) PixelRNN (van den Oord et al. (2016a)) ImageTransformer (Parmar et al. (2018)) PixelSNAIL (Chen et al. (2017)) Our Decoder baseline

3.03 2.92 3.00 2.90 2.85 2.83

3.83 3.77 3.80 3.77

Table 1: Estimated upper bound for negative log-likelihood along with KL-divergence (in parenthesis) in bits per dimension for CIFAR-10 and Downsampled Imagenet.

4.2 UTILIZATION OF LATENT VARIABLES
In this section, we aim to demonstrate that our models learn meaningful representations of the data in the latent variables. We first investigate the effect of z on the generated samples from the model. Fig. 3 depicts samples from an ImageNet model (see Appendix for CIFAR-10), where we sample from the decoder network multiple times conditioned on a fixed sample from the auxiliary prior. We see similar global structure (e.g. same color background, scale and structure of objects) but very different details. This indicates that the model is using the latent variable to capture global structure, while the autoregressive decoder is filling in local statistics and patterns.
For a more quantitative assessment of how useful the learned representations are for downstream tasks, we performed linear classification from the representation to the class labels on CIFAR-10. We also study the effect of the chosen rate of the model on classification accuracy as illustrated in the right graph of Fig. 4. We find that generally a model with higher rate gives better classification accuracy, with our highest rate model, encoding 0.1 bits per dimension, giving the best accuracy of 68%. However, contrasting classification accuracy with negative log-likelihood of the models we find that higher likelihood does not necessarily yield features for better linear classification results. We caution that an obviously important requirement for this task is the linear separability of the learned feature space, which does not necessarily align with the desire to learn highly compressed representations.
4.3 ABLATION STUDIES
Table 2 reports the results of our experiments comparing -VAE with popular mitigation techniques for the posterior collapse problem. We report results for models that encode a non-negligible amount information in latent variables. Unlike the committed information rate approach of -VAE, most alternative solutions required considerable amount of effort to get the training converge or prevent the KL from collapsing altogether. For example, with linear annealing of KL (Bowman et al., 2015), despite trying a wide range of values for the end step of the annealing schedule, we were not able to train a model with a significant usage of latent variables; the KL collapsed as soon as  approached 1.0. A practical advantage of our approach is its simple formula to choose the target minimum rate of the model. Targeting a desired rate in -VAE, on the other hand, proved to be difficult, as many of
1the exact results for the autoregressive baseline was not reported in Chen et al. (2016)

6

Under review as a conference paper at ICLR 2019

(a) Multiple decoding of the same z

(b) Random samples from the our Auxiliary prior

Figure 3: Random samples from our ImageNet 32 × 32 model. Each column in Fig. 3a shows
multiple samples from p(x|z) for a fixed z  paux(z). Each image in Fig. 3b is decoded using a different sample from paux(z).

Figure 4: Accuracy of linear classifier trained on latent variables for CIFAR-10 models with different rates. The colour of each datapoint encodes its ELBO value.
our attempts resulted in either collapsed KL, or very large KL values that led to inefficient inference. As reported in Chen et al. (2016), we also observed that optimising models with the free-bits loss was challenging and sensitive to hyperparameter values.
Next, we compare the performance of the anti-causal encoder structure with the non-causal structure discussed in Sect. 2.2. The results for several configurations of our model are reported in the Appendix Table 6. In models where the decoder is not powerful enough (such as our 6-layer PixelCNN that has no attention and consequently a receptive field smaller than the causal context for most pixels), the anti-causal structure does not perform as well as the non-causal structure. The performance gap is however closed as the decoder becomes more powerful and its receptive field grows by adding self-attention and more layers. We observed that the anti-causal structure outperforms the non-causal encoder for very high capacity decoders, as well as for medium size models with a high rate.
4.4 TEXT
For our experiments on natural language, we used the 1 Billion Words or LM1B (Chelba et al., 2013) dataset in its processed form in the Tensor2Tensor Vaswani et al. (2018) codebase 2. The dataset is
2https://github.com/tensorflow/tensor2tensor
7

Under review as a conference paper at ICLR 2019

Method -VAE N (0, I) prior -VAE AR(1) prior -VAE LSTM prior
Free-bits
KL Annealing N (0, I) prior

ELBO

Aux. ELBO

3.03 (0.04)

3.00 (0.02)

3.01 (0.02)

3.00 (0.01)

2.99 (0.01)

Accuracy
44% 38% 47%

3.03 (0.12)

3.09 (0.07)

64%

2.98 ( 1e-6) 2.98 ( 1e-9) 10%

Independent -VAE ( = 0.02) Independent -VAE ( = 0.14) Temporal -VAE ( = 0.02) Temporal -VAE ( = 0.21)

3.02 (0.02) 3.12 (0.14) 3.00 (0.03) 3.22 (0.22)

2.98 (0.01) 3.02 (0.03)

51% 59% 52% 68%

Table 2: Comparison of different methods to mitigate the posterior collapse problem.
All the models had two self-attention layers interspersed with 6 layers of gated convolution with hidden size of 128 in both the encoder and the decoder.  out of 9 free-bits runs only two did not diverge, one of which collapsed the KL to zero.  in all our experiments with different linear schedules KL collapsed as  approached 1.

tokenized in subword tokens with a vocabulary size of about 32000. We discarded sequences longer than 256 tokens. Our employed architecture for text closely follows the Transformer network of Vaswani et al. (2017). Our sequence of latent variables has the same number of elements as in the number of tokens in the input, each having two dimensions with  = 0.2 and 0.4. Our decoder uses causal self-attention as in Vaswani et al. (2017). For the anti-causal structure in the encoder, we use the inverted causality masks as in the decoder to only allow looking at the current timestep and the future.
The quantitative results for our experiments is listed in Table 3, indicating that our model achieves slightly worse log-likelihood compared to its auto-regressive baseline, but makes considerable use of latent variables. Please refer to Appendix H for samples from our text model demonstrating this.

AR(1) ELBO (KL) Aux prior ELBO (KL) AR baseline NLL

-VAE  3.61(0.2)

 3.58(0.17)

3.55

Table 3: The result of our text experiments on LM1B in nats / token.

5 DISCUSSION
In this work, we have demonstrated that our proposed -VAE model provides a simple, intuitive, and effective solution to posterior collapse in latent variable models, enabling them to be paired with powerful decoders. Unlike prior work, we do not require changes to the objective or weakening of the decoder, and we can learn useful representations as well as achieving state-of-the-art likelihoods. While our work presents two simple posterior-prior pairs, there are a number of other possibilities that could be explored in future work. Our work also points to at least two interesting challenges for latent-variable models: can they exceed the performance of a strong autoregressive baseline? and can the representations aid state-of-the-art models for downstream applications such as classification?
REFERENCES
Mart´in Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for largescale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems
8

Under review as a conference paper at ICLR 2019
Design and Implementation, OSDI'16, pp. 265­283, Berkeley, CA, USA, 2016. USENIX Association. ISBN 978-1-931971-33-1. URL http://dl.acm.org/citation.cfm?id= 3026877.3026899.
Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif A Saurous, and Kevin Murphy. Fixing a Broken ELBO. 2017. ISSN 1938-7228. URL http://arxiv.org/abs/1711. 00464.
Alexander A Alemi, Ian Fischer, and Joshua V Dillon. Uncertainty in the variational information bottleneck. arXiv preprint arXiv:1807.00906, 2018.
Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016.
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell, and Sergey Levine. Stochastic variational video prediction. CoRR, abs/1710.11252, 2017.
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. Generating Sentences from a Continuous Space. 2015. doi: 10.18653/v1/K16-1002. URL http://arxiv.org/abs/1511.06349.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational Lossy Autoencoder. In Iclr, pp. 1­14, nov 2016. URL http://arxiv.org/abs/1611.02731.
Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An Improved Autoregressive Generative Model. pp. 12­17, 2017. URL http://arxiv.org/abs/1712. 09763.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua Bengio. A Recurrent Latent Variable Model for Sequential Data. Advances in Neural Information Processing Systems 28 (NIPS 2015), pp. 8, 2015. ISSN 10495258. URL http: //arxiv.org/abs/1506.02216.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, New York, NY, USA, 2006. ISBN 0471241954.
Tim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspherical variational auto-encoders. arXiv preprint arXiv:1804.00891, 2018.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.
Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. CoRR, abs/1802.07687, 2018.
Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3549­3557. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/ 6542-towards-conceptual-compression.pdf.
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and Aaron Courville. PixelVAE: A Latent Variable Model for Natural Images. pp. 1­9, 2016. ISSN 0066-4308. doi: 10.1146/annurev.psych.53.100901.135239. URL http://arxiv. org/abs/1611.05013.
9

Under review as a conference paper at ICLR 2019
Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating Sentences by Editing Prototypes. 6:437­450, 2017. ISSN 1938-7228. URL http://arxiv.org/abs/ 1709.08878.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. -VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In ICLR, 2017.
Yoon Kim, Sam Wiseman, Andrew Miller, David Sontag, and Alexander Rush. Semi-amortized variational autoencoders. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2678­2687, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/kim18e.html.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://dblp.uni-trier.de/db/journals/corr/ corr1412.html#KingmaB14.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improving Variational Inference with Inverse Autoregressive Flow. (2011), 2016. ISSN 10495258. URL http://arxiv.org/abs/1606.04934.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/~kriz/cifar.html.
Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. In International Conference on Machine Learning, pp. 1558­1566, 2016.
Thomas Lucas and Jakob Verbeek. Auxiliary guided autoregressive variational autoencoders. arXiv preprint arXiv:1711.11479, 2017.
Matthew Johnson Matthew D. Hoffman. Elbo surgery: yet another way to carve up the variational evidence lower bound. In NIPS Workshop on Advances in Approximate Bayesian Inference, 2016.
Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3936­3945, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/ostrovski18a.html.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image Transformer. 2018. ISSN 1938-7228. URL http://arxiv.org/abs/ 1802.05751.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML'15, pp. 1530­1538. JMLR.org, 2015. URL http://dl.acm. org/citation.cfm?id=3045118.3045281.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. 32, 2014. ISSN 10495258. doi: 10.1051/ 0004-6361/201527329. URL http://arxiv.org/abs/1401.4082.
Aurko Roy, Ashish Vaswani, Arvind Neelakantan, and Niki Parmar. Theory and experiments on vector quantized autoencoders. arXiv preprint arXiv:1805.11063, 2018.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications. pp. 1­9, 2017. ISSN 9781467324755. URL http://arxiv.org/abs/1701.05517.
10

Under review as a conference paper at ICLR 2019
Jakub M. Tomczak and Max Welling. VAE with a vampprior. CoRR, abs/1705.07120, 2017. URL http://arxiv.org/abs/1705.07120.
R. E. Turner and M. Sahani. A maximum-likelihood interpretation for slow feature analysis. Neural Computation, 19(4):1022­1038, 2007. ISSN 0899-7667. doi: http://dx.doi.org/10.1162/neco. 2007.19.4.1022.
Aa¨ron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. CoRR, abs/1601.06759, 2016a. URL http://arxiv.org/abs/1601.06759.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks. In International Conference on Machine Learning, volume 48, pp. 1747­1756, 2016b. ISBN 9781510829008. URL http://arxiv.org/abs/1601.06759.
Aa¨ron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. CoRR, abs/1711.00937, 2017. URL http://arxiv.org/abs/1711.00937.
L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. 2008.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. (Nips), 2017. ISSN 0140-525X. doi: 10.1017/S0140525X16001837. URL http://arxiv.org/abs/1706.03762.
Ashish Vaswani, Samy Bengio, Eugene Brevdo, Franc¸ois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Lukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018.
Laurenz Wiskott and Terrence J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Comput., 14(4):715­770, April 2002. ISSN 0899-7667. doi: 10.1162/ 089976602317318938. URL http://dx.doi.org/10.1162/089976602317318938.
Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. Proc. Empirical Methods in Natural Language Processing, 2018.
Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved Variational Autoencoders for Text Modeling using Dilated Convolutions. 2017. URL http://arxiv. org/abs/1702.08139.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Balancing Learning and Inference in Variational Autoencoders. ICML Workshop, 2017. URL http://arxiv.org/abs/1706. 02262.
11

Under review as a conference paper at ICLR 2019

A DERIVATION OF THE KL DIVERGENCE FOR SEQUENTIAL LATENT
VARIABLES

DKL(q(z|x) p(z)) =

z

q(z|x)

log

q(z|x) p(z)

dz

nn

= q(zi|x)( (zi|x) - log p(z))dz

z i=1

i=1

nn

n

= q(zi|x)( log q(zi|x) - log p(z1) - log p(zi|zi-1))dz

z i=1

i=1

i=2

nn

= q(zi|x)( log q(zi|x))dz - q(z1|x) log p(z1)dz1

z i=1

i=1

z1

i=n

-[

q(zi-1|x) q(zi|x) log p(zi|zi-1)dzidzi-1]

i=2 zi-1

zi

= q(z1|x) log q(z1|x)dz1 - q(z1) log p(z1)dz1
z1 z1 n

+ [ q(i|x) log q(zi|x)dzi -

q(zi-1|x) q(zi|x) log p(zi|zi-1)dzidzi-1]

i=2 zi

zi-1

zi

n
= DKL(q(z1|x) p(z1)) + [
i=2

q(zi-1|x)
zi-1

zi

q(zi|x)

log

q(zi|x) p(zi|zi-1)

dzidzi-1]

n

= DKL(q(z1|x) p(z1)) + Ezi-1q(zi-1|x)[DKL(q(zi|x) p(zi|zi-1))]

i=2

B DERIVATION OF THE KL-DIVERGENCE BETWEEN AR(1) AND DIAGONAL GAUSSIAN, AND ITS LOWER-BOUND

zi  Rd, z0  {0}d p(z1) = N (0, 1)
p(zi|zi-1) = N (zi-1, 1 - 2) i > 1 q(zi|x) = N (µi (x), i(x))

Noting the analytic form for the KL-divergence for two uni-variate Gaussian distributions:

DKL(N (µq, q)

N (µp, p)) =

1 2

[ln((

p q

)2

)

+

q2 + (µp - µq)2 p2

- 1]

(2)

we now derive the lower-bound for KL-divergence. Without loss of generality and to avoid clutter, we have assume the mean vector µi has equal values in each dimension.?.

n

DKL(q(z|x) p(z)) = E [zi-1q(zi-1|x) DKL(N (µqi , qi ) N (µpi , pi ))]

i=1

=

1 2

Ezi-1

[-

ln(12

)

+

12

-

1

+

µ12

+

n i=2

ln( 1

- 2 i2

)

+

1

i2 - 2

-

1

+

(µi

- zi-1)2 1 - 2

]

=

1 2

(f1

(12)

+

µ12

+

n

f1( 1

i2 - 2

)

+

1

1 -

2

Ezi-1

[(µi

-

zi-1)2])

i=2

12

Under review as a conference paper at ICLR 2019

Where fa(x) = ax-ln(x)-1. Using the fact that i2 = E[(zi -µi)2] = E(z2i )-µ2i , the expectation inside the summation can be simplified as follows.
Ezi-1 [(µi - zi-1)2)]) = = Ezi-1 [µ2i - 2µizi-1 + 2zi2-1] = µi2 - 2µiEzi-1 [zi-1] + 2Ezi-1 [zi2-1] = µi2 - 2µiµi-1 + 2i2-1 + 2µi2-1 = (µi - µi-1)2 + 2i2-1
Plugging this back gives us the following analytic form for the KL-divergence for the sequential latent variable z.

DKL(q(z|x)

p(z)) =

1 2

(f1

(12)

+

µ21

+

n

[f1(

1

i2 - 2

)

+

(µi

-

µi-1)2 + 1 - 2

2i2-1

])

i=2

(3)

C DERIVATION OF THE LOWER-BOUND

Removing non-negative quadratic terms involving µi in equation 3 and expanding back f inside the summation yields

DKL(q(z|x)

p(z)) 

1 2

(f1

(12)

+

212 1 - 2

+

n-1
[

i2(1 + 2 1 - 2

)

-

ln(

1

i2 - 2

)

-

1]

+

f1(

1

n2 - 2

))

i=2

=

1 2

(f

1 1-2

(12)

+

n-1

f1+2

(

1

i2 - 2

)

+

f1(

1

n2 - 2

))

i=2

Consider

fa(x)

=

ax

- ln(x) - 1

and

its

first

and

second

order

derivatives,

fa(x)

=

a

-

1 x

and

fa (x)  0. Thus, fa is convex and obtains its minimum value of ln(a) at x = a-1. Substituting

12

=

1 - 2,

n2

=

1 - 2

and

i2

=

1-2 1+2

yields

the

following

lower-bound

for

the

KL:

DKL(q(z|x)

p(z)) 

1 2

[(n

-

2)

ln(1

+

2)

-

ln(1

-

2)]

When using multi-dimensional zi at each timestep, the committed rate is the sum of the KL for each individual dimension:

DKL(q(z|x)

p(z)) 

1 2

[

D

(n - 2) ln(1 + k2) - ln(1 - k2)]

k=1

D INDEPENDENT -VAES

The most common choice for variational families is to assume that the components of the pos-
terior are independent, for example using a multivariate Gaussian with a diagonal covariance: q(z|x) = N (z; µq(x), q(x)). When paired with a standard Gaussian prior, p(z) = N (z; 0, 1), we can guarantee a committed information rate  by constraining the mean and variance of the variational family (see Appendix C) :

µ2q  2 + 1 + ln(q2) - q2

(4)

We can, thus, numerically solve

ln(q2) - q2 + 2r + 1  0

13

Under review as a conference paper at ICLR 2019

to obtain the feasible interval [ql , qu] where the above equation has a solution for µq, and the committed rate . Posterior parameters can thus be parameterised as:

q

=

ql

+

(qu

-

ql ) 1

+

1 e- (x)

µq = 2 + 1 + ln(q2) - q2 + max(0, µ(x))

(5)
(6) (7)

Where  parameterizes the data-dependent part of µq ad q, which allow the rate to go above the designated lower-bound .

E ARCHITECTUER DETAILS

E.1 IMAGE MODELS
In this section we provide the details of our architecture used in our experiments. The overall architecture diagram is depicted in Fig. 5. To establish the anti-causal context for the inference network we first reverse the input image and pad each spatial dimension by one before feeding it to the encoder. The output of the encoder is cropped and reversed again. As show in Fig. 5, this gives each pixel the anti-causal context (i.e., pooling information from its own value and future values). We then apply average pooling to this representation to give us row-wise latent variables, on which the decoder network is conditioned.

crop and reverse PixelCNN Encoder

reverse

pad

12 34

43 21

43 21

{} {4} {4,3} {4,3} {4,3,2} {4,3,
2,1}

PixelCNN Decoder

z1

avg. pool

{4,3, 2,1}

{4,3,2}

rows

z2 {4,3} {4}

Figure 5: Architecture for images
The exact hyper-parameters of our network is detailed in Table 4. We used dropout only in our decoder and applied it the activations of the hidden units as well as the attention matrix. As in (Vaswani et al., 2017), we used rectified linear units and layer normalization (Ba et al., 2016) after the multi-head attention layers. We found layer normalization to be essential for stabilizing training. For optimization we used the Adam optimizer (Kingma & Ba, 2014). We used the learning rate schedule proposed in (Vaswani et al., 2017) with a few tweaks as in the formulae:
LRimagenet = 0.18 × hd-0.5 min(step num0.35, step num × 40001.5) LRcifar10 = 0.36 × hd-0.5 min(step num0.35, step num × 80001.5)
We developed our code using Tensorflow (Abadi et al., 2016). Our experiments on natural images were conducted on Google Cloud TPU accelerators. For Imagenet, we used 128 TPU cores with batch size of 1024. We used 8 TPU cores for CIFAR-10 with batch size of 64.
E.2 TEXT MODELS
The architecture of our model for text experiment is closely based on the Transformer network of Vaswani et al. (2017). The exact hyper-parameters are summarized in Table 5.

14

Under review as a conference paper at ICLR 2019

le/ld he/hd re/rd

ae/ad ah ndmol dod z 

CIFAR-10

20/30 128/256 1024/1024 11/16 8 32

Imagenet 32x32 6/20 128/512 1024/2048 2/5 8 32

0.5 8 [0.3, 0.99] 0.3 16 [0.5, 0.95]

Table 4: Hyperparameter values for the best models used for each dataset. The subscripts e and d respectively denote the encoder and the decoder. l is the number of layers, h is the hidden size of each layer, r is the size of the residual filter, a is the number of attention layers interspersed with gated convolution layers of PixelCNN, ndmol is the number of components in the discrete mixture of logistics distribution, dod is the probability of dropout applied to the decoder, z is the dimensionality of the latent variable used for each row, and the alpha column gives the range of of the AR(1) prior
hyper-parameter for each latent.

lh r

ah d z 

LM1B 4 512 2048 8 0.1 2 [0.2, 0.4]

Table 5: Hyperparameter values for our LM1B experiments. l is the number of layers, h is the hidden size of each layer, r is the size of the residual filters, do is the probability of dropout, z is the dimensionality of the latent variable, and the alpha column gives the range of of the AR(1) prior
hyper-parameter for each latent dimension.

F VISUALIZATION OF THE LATENT SPACE
It is generally expected that images from the same class are mapped to the same region of the latent space. Fig. 6 illustrates the t-SNE (van der Maaten & Hinton, 2008) plot of latent variables inferred from 3000 examples from the test set of CIFAR-10 colour coded based on class labels. As can also be seen on the right hand plot classes that are closest are also mostly the one that have close semantic and often visual relationships (e.g., cat and dog, or deer and horse).

Figure 6: t-SNE plot of the posterior mean for 3000 CIFAR-10 images. Note the adjacent groups and mixed regions of the plot: cats and dogs images are mostly interspersed as are automobiles and trucks.The highest concentration of horses are on top of the region right above where deer examples are.
G ENCODER ABLATIONS H ADDITIONAL SAMPLES
15

Under review as a conference paper at ICLR 2019
Figure 7: Additional Imagenet samples. Top left: Each column is interpolation in the latent space. Top right: "Day dream" samples where we alternate between sampling x  p(x|z) and z  q(z|x). Bottom left: Each half-column contains in order an original image from the validation set, occlusion of that image, and two reconstructions from different posterior samples. Bottom right: Each halfcolumn contains in order an original image from the validation set, followed by 3 reconstructions from different posterior samples.
16

Under review as a conference paper at ICLR 2019
Figure 8: Additional CIFAR-10 samples. Top left: Each column is interpolation in the latent space. Top right: "Day dream" samples where we alternate between sampling x  p(x|z) and z  q(z|x). Bottom left: Each half-column contains in order an original image from the test set, occlusion of that image, and two reconstructions from different posterior samples. Bottom right: Each half-column contains in order an original image from the test set, followed by 3 reconstructions from different posterior samples.
17

Under review as a conference paper at ICLR 2019
Figure 9: Random samples from the auxiliary (left) and AR(1) (right) priors of our high-rate (top) and low-rate(bottom) CIFAR-10 models. The high-rate (low-rate) model has -ELBO of 2.90 (2.83) and KL of 0.10 (0.01) bits/dim. Notice that in the high rate model that has a larger value of , samples from the AR(1) prior can turn out too smooth compared to natural images. This is because of the gap beween the prior and the marginal posterior, that is closed by the auxiliary prior.
18

Under review as a conference paper at ICLR 2019
Figure 10: Additional unconditional random samples from Imagenet 32x32. Each half-column in each block contains 4 decodings of the same sample z  paux(z)
19

Under review as a conference paper at ICLR 2019
==== Interpolating dimension 0 ==== The company's stock price is also up for a year-on-year rally, when the The company's shares are trading at a record high for the year, when they were trading at The company's shares were trading at $3.00, down from their 52-week low The company's shares fell $1.14, or 5.7 percent, to $ UNK The company, which is based in New York, said it would cut 1,000 jobs in the The two-day meeting, held at the White House, was a rare opportunity for the United States The company, which is based in New York, said it was looking to cut costs, but added The company is the only company to have a significant presence in China. The company is the only company to have a significant presence in the North American market. The two men, who were arrested, have been released.
==== Interpolating dimension 1 ==== In the meantime, however, the company is taking the necessary steps to keep the company in the UNK In the meantime, however, the company is expected to take some of the most aggressive steps in the In the meantime, the company is expected to report earnings of $2.15 to $4. The two men, who were both in their 20s , were arrested on suspicion of causing death by dangerous The company said it was "disappointed" by a decision by the U.S. Food and Drug The company said it would continue to provide financial support to its business and financial services clients. The new plan would also provide a new national security dimension to U.S.- led efforts to "I've always been a great customer and theres´ always theres´ a good chance "It's a great personal decision...
Figure 11: One at a time interpolation of latent dimensions of a sample from the AR(1) prior.
==== Interpolating dimension 0 ==== "I'll be in the process of making the case," he said. "I've got to take the best possible shot at the top," he said "I'm not going to take any chances," he said. "I'm not going to take any chances," he said. "I'm not going to take any chances," he said. We are not going to get any more information on the situation," said a spokesman for the U. N. mission in Afghanistan, which is to be formally We are not going to get the money back," he said. We are not going to get the money back," said one of the co - workers. "We are not going to get the money back," said the man. "We are not going to get a lot of money back," said the man.
==== Interpolating dimension 1 ==== The company said the company, which employs more than 400 people, did not respond to requests for comment, but did not respond to an email seeking comment, which The new rules, which are expected to take effect in the coming weeks, will allow the government to take steps to ensure that the current system does not take too "The only thing that could be so important is the fact that the government is not going to be able to get the money back, so the people are taking "I'm not sure if the government will be able to do that," he said. "We are not going to get any more information about the situation," said Mr. O'Brien. "It's a very important thing to have a president who has a strong and strong relationship with our country," said Mr. Obama, who has been the "It's a very important thing to have a president who has a great chance to make a great president," said Mr. Obama, a former senator from "It's a very important decision," said Mr. Obama. "It's a very difficult decision to make," said Mr. McCain.
Figure 12: One at a time interpolation of latent dimensions of a sample from the auxiliary prior.
20

Under review as a conference paper at ICLR 2019

Non-Causal AR(1) Non-Causal Aux Anti-Causal AR(1) Anti-Causal Aux

l=6 h = 128 a=0 low-rate
3.04 (0.02) 3.03 (0.01) 3.07 (0.02) 3.06 (0.01)

l=8 h = 128 a=2 low-rate
3.01 (0.03) 2.98 (0.004) 3.01 (0.03) 2.98 (0.006)

l=8 h = 128 a=2 high-rate
3.32 (0.22) 3.11 (0.02) 3.22 (0.22) 3.03 (0.03)

le = 20,he = 128 ld = 30,hd = 256 a=6 low-rate
2.88 (0.05) 2.85 (0.01) 2.87 (0.05) 2.84 (0.02)

Table 6: Ablation of anti-causal vs. non-causal structure. l: number of layers, h: hidden size, a: number of attention layers. Subscripts e and d respectively denote encoder and decoder sizes, when they were different. The low-rate (high-rate) models had latent dimension of 8 (64) with alpha linearly placed in [0.5, 0.95] ([0.5, 0.99]) which gives the total rate of 79.44 (666.6) bits per image.

The company is now the world's cheapest for consumers . The company is now the world's biggest producer of oil and gas, with an estimated annual revenue of $2.2 billion. The company is now the world's third-largest producer of the drug, after Pfizer and AstraZeneca, which is based in the UK. The company is now the world's biggest producer of the popular games console, with sales of more than $1bn (312m) in the US and about $3bn in the UK. The company is now the world's largest company, with over $7.5 billion in annual revenue in 2008, and has been in the past for more than two decades. The company is now the world's second-largest, after the cellphone company, which is dominated by the iPhone, which has the iPhone and the ability to store in - store, rather than having to buy, the product, said, because of the Apple-based device. The company is now the world's biggest manufacturer of the door-to-door design for cars and the auto industry. The company is now the world's third-largest maker of commercial aircraft, behind Boeing and Airbus. The company is now the world's largest producer of silicon, and one of the biggest producers of silicon in the world. The company is now the world's largest maker of computer -based software, with a market value of $4.2 billion (2.6 billion) and an annual turnover of $400 million (343 million).
Figure 13: Text samples.
21

