Under review as a conference paper at ICLR 2019
PROXY-LESS ARCHITECTURE SEARCH VIA BINARIZED PATH LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Neural architecture search (NAS) has great impact automatically designing efficient neural network architectures. However, the computation demand is prohibitive with conventional NAS algorithms (e.g. 104 GPU hours), making it difficult to directly search the architecture on large-scale tasks (e.g. ImageNet). As a result, NAS needs to utilize proxy tasks, such as training on a smaller dataset (e.g. CIFAR-10), or learning with only a few blocks rather than the full depth, or training only for a few epochs rather than till convergence. These architectures optimized on proxy tasks are not guaranteed to be optimal on target task. In this paper, we present Proxy-less Architecture Search that can directly learn the architectures on a large-scale target task. We reduce the computational cost (GPU hours and GPU memory) of architecture search to the same level of normal training. Additionally, by using the measured hardware latency (rather than FLOPs) as a direct objective, we can specialize neural network architectures for different hardware architectures. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters. On ImageNet, our model achieves 2.5% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency. Compared with previous architecture search algorithms, we saved the GPU hours by two orders of magnitude, GPU memory by one orders of magnitude, both on ImageNet. We also analyzed the insights of efficient CNN models specialized for different hardware platforms.
1 INTRODUCTION
Recently neural architecture search (NAS) has demonstrated much success in automating neural network architecture design for various deep learning tasks, such as image recognition (Zoph et al., 2018; Cai et al., 2018a; Liu et al., 2018a; Zhong et al., 2018) and language modeling (Zoph & Le, 2017). Despite the remarkable results, conventional NAS algorithms are prohibitively computationintensive, requiring thousands of models to be trained on the target task in a single experiment. Therefore, directly applying NAS to a large-scale task (e.g. ImageNet) is computationally difficult or impossible. As a trade-off, Zoph et al. (2018) propose to search for building blocks on proxy tasks, such as training for fewer epochs, starting with a smaller dataset (e.g. CIFAR-10), or learning with fewer blocks. Then top-performing blocks are stacked and transferred to the target large-scale task. This paradigm has been widely adopted in subsequent NAS algorithms (Liu et al., 2018a;b; Real et al., 2018; Cai et al., 2018b; Liu et al., 2018c; Tan et al., 2018).
However, these blocks optimized on proxy tasks are not guaranteed to be optimal on target task. Fo example, when proxy strategy "fewer epochs" is adopted, lower weight decay may be preferred because of faster convergence on early training stage. But it might lead to a worse final result due to overfitting. Also, competitive blocks on CIFAR-10 can have less compelling results on ImageNet (Liu et al., 2018c). Moreover, to enable transferability, such methods typically search for only a few architectural motifs and then repeatedly stack the same pattern, which restricts the block diversity and thereby harms performance (Tan et al., 2018).
In this work, we propose a simple and effective solution to aforementioned limitations, called Proxyless Architecture Search, which directly learns the architectures on the target task rather with proxy (Figure 1) while allowing blocks to have different structures. To achieve this, we reduce the computation cost (GPU hours and GPU memory) of architecture search to the same level of normal training in following ways.
1

Under review as a conference paper at ICLR 2019

(1) Previous proxy-based approach

(2) Our proxy-less approach

Learner

Architecture
Proxy Transfer Task
Updates

Target Task &
Hardware

Learner

Architecture Target Task &
Hardware
Updates

Figure 1: Conventional NAS vs our proxy-less NAS.

GPU hour-wise, inspired by recent works (Bender et al., 2018; Liu et al., 2018c), we directly train a cumbersome network that contains all candidate paths (Figure 2). And during training we explicitly introduce architecture parameters to learn which paths are redundant, while these redundant paths are pruned at the end of training to get a compact optimized architecture. In this way, we simplify architecture search to be a single training process of the cumbersome network without any metacontroller (or hypernetwork).
However, naively including all the candidate pathsGlPeUaHdosursto GPU mGePmU Moermyoreyxplosion (Liu et al., 2018c; Bender et al., 2018), as the memory consumption grows linearly w.r.t. the number of choices. Thus, GPU memory-wise, we binarize the architecture parameters (1 or 0) and force only one path to be active at run-time, which reduces the required memory to the same level of training a compact model. These binarized architecture parameters can be optimized via gradient decent, using the BinaryConnect technique (Courbariaux et al., 2015). Furthermore, to handle non-differentiable objectives (e.g. latency), we propose a REINFORCE-based (Williams, 1992) algorithm as an alternative strategy to update these binary parameters.

In our experiments on CIFAR-10 and ImageNet, benefit from the efficiency and directness, our

method can achieve strong empirical resulNtsor.malOTranin CIFARN-A1S 0, ouDrARmTSo&dOneel-shroet acPrhoxeylsess2(O.u0rs8) % test error

with only 5.7M parameters. On ImageNet, our modelNeaecd hMeiteavCoenstro7lle4r N.5o M%etatCoonptr-o1lleracNocMuertaaCcoyntrowllerhich is 2.5%

higher

than

MobileNetV2

(Sandler

et

al.,

2018)

while

bNeeiend gPro1xy.2×

Need Proxy
faster.

No Proxy

Beside strong empirical results, our method also allows specializing neural network architectures for a certain device. Conventionally, the same network architecture is employed for different hardware architectures. For example, ResNet-50 is popular among various platforms, CPU, GPU and TPUs1. However, the specialized network architecture is needed for different hardware architectures to achieve best accuracy-efficiency trade-offs. Our study shows that CPU and GPU have totally different preferences on efficient neural network architectures: CPU prefers slim and deep structures, while GPU prefers wide and shallow ones, which matches the degree of their parallelisms (Figure 4). To the best of our knowledge, this is the first work to study the preferences of different platforms.

2 RELATED WORK
The use of machine learning techniques, such as reinforcement learning or neuro-evolution, to replace human experts in designing neural network architectures, usually referred to as neural architecture search, has drawn an increasing interest (Zoph & Le, 2017; Liu et al., 2018a;b;c; Cai et al., 2018a;b; Pham et al., 2018; Brock et al., 2018; Bender et al., 2018; Elsken et al., 2017; 2018b). In NAS, architecture search is typically considered as a meta-learning process. And a meta-controller (e.g. a recurrent neural network (RNN)), is introduced to explore a given architecture space with training a network in the inner loop to get an evaluation for guiding exploration. Consequently, such methods are computationally expensive to run, especially on large-scale tasks, e.g. ImageNet.
Some recent works (Brock et al., 2018; Pham et al., 2018) try to improve the efficiency of this meta-learning process by reducing the cost of getting an evaluation. In Brock et al. (2018), a hypernetwork is utilized to generate weights for each sampled network and hence can evaluate the architecture without training it. Similarly, Pham et al. (2018) propose to share weights among all sampled networks under the standard NAS framework (Zoph & Le, 2017). These methods speed up architecture search by orders of magnitude, however require a hypernetwork or an RNN controller, thus remain complicated and hard to optimize. Moreover, they mainly focus on small-scale tasks (e.g. CIFAR) rather than large-scale tasks (e.g. ImageNet).
Our work is most closely related to One-Shot (Bender et al., 2018) and DARTS (Liu et al., 2018c), both of which get rid of the meta-controller (or hypernetwork) by modeling architecture search as a single training process of a cumbersome network that comprises all candidate paths. Specifically, One-Shot trains the cumbersome network with DropPath (Zoph et al., 2018) that drops out each path with some fixed probability. Then they use the pre-trained cumbersome network to evaluate
1https://dawn.cs.stanford.edu/benchmark/ https://mlperf.org

2

Under review as a conference paper at ICLR 2019

update
CONV 3x3

INPUT

CONV 5x5

Identity ...

POOL 3x3

Weight  Parameters

CONV 3x3

INPUT

CONV 5x5

Identity ...

POOL 3x3

  ...

update

Architecture Parameters

  ...

1

0

...

0

Binary Gate (0:prune, 1:keep)

0

1

...0

OUTPUT
(1) Update weight parameters

fmap in memory

OUTPUT

fmap not in memory (2) Update architecture parameters

Figure 2: Learning both weight parameters and binarized architecture parameters.

architectures, which are sampled by randomly zeroing out paths. DARTS additionally introduces a

real-valued architecture parameter for each path and jointly train weight parameters and architecture

parameters via standard gradient decent. In this work, we address the large memory issue in these

two methods through path binarizaGtPioUnH.ours

GPU Memory

Another relevant topic is network pruning (Han et al., 2016) that aims to improve the efficiency of neural networks by removing insignificant neurons (Han et al., 2015) or channels (Liu et al., 2017). Similar to these works, we start with an over-parameterized network and then prune the redundant parts to derive the optimized architecture. The distinction is that they focus on layer-level pruning that only modifies the filter (or units) number of a layer but fails to change the topology of the network, while we focus on learning effective network architectures through path-level pruning.

3 METHOD

In this section, we first describe the construction of the cumbersome network with all candidate paths, then introduce hoNwormwaleTralienverNaegedeMbeNitanASCaornitzroelledr DaNAroRcMTheSitat&eCOcontneutr-osrlheleortpaNProraoMmxeytlaeestCseo(rnOstruortlsloe) r reduce the memory consumption of training the cumbersomNeeendePtrwoxyork to Ntheeed Psraoxmy e levelNoaPsronxyormal training. Finally, we present two algorithms for learning the binarized architecture parameters, based on BinaryConnect (Courbariaux et al., 2015) and REINFORCE (Williams, 1992) respectively.

3.1 CONSTRUCTION OF CUMBERSOME NETWORK

A neural network can be expressed as a directed acyclic graph (DAG), where each edge is associated with an operation o(·) that transforms the feature map. As the lack of connection between two nodes
in DAG can be indicated using the zero operation, the task of learning both the connections and
operations that are applied at different positions within the network can thereby reduce to choosing
operations on its edges (Liu et al., 2018c). For simplicity, we omit nodes and denote a neural network as N (e, · · · , en) where ei represents a certain edge in DAG in following discussions.

Let O = {oi} be the set of N candidate primitive operations (e.g. convolution, pooling, identity, zero, etc). To construct the cumbersome network that includes any architecture in the search space, instead of setting each edge to be a definite primitive operation, we set each edge to be a mixed operation that has N parallel paths (Figure 2), denoted as mO. As such, the cumbersome network can be expressed as N (e = mO1 , · · · , en = mnO). Here, we assume that the candidate set keeps the same throughout the network for ease of discussion, while we can actually arbitrarily adjust the candidate set for each certain position.

Given input x, the output of a mixed operation mO is defined based on the outputs of its N paths. In One-Shot, mO(x) is the sum of {oi(x)}, while in DARTS, mO(x) is weighted sum of {oi(x)} where
the weights are calculated by applying softmax to N real-valued architecture parameters {i}:

N

mOOne-Shot(x) =

oi(x),

i=1

NN
mDOARTS(x) = pioi(x) =

i=1

i=1

exp(i) j exp(j

)

oi

(x).

(1)

To derive the compact optimized architecture from the cumbersome network, we firstly train the cumbersome network, then identify and prune N - 1 redundant paths for each mixed operation. However, straightforwardly training such a cumbersome network can be problematic. As shown in Eq. (1), the output feature maps of all N paths are calculated and stored in the memory, while training a compact model only involves one path. Therefore, One-Shot and DARTS roughly need N times

3

Under review as a conference paper at ICLR 2019

GPU memory and GPU hours compared to training a compact model. On large-scale dataset, this can easily exceed the memory limits of hardware with large design space. In the following section, we solve this memory issue based on the idea of path binarization.

3.2 LEARNING BINARIZED PATH

In this work, inspired by BinaryConnect (Courbariaux et al., 2015), we employ a binarization ap-
proach for reducing the cost of training the cumbersome network. Specifically, similar to DARTS, we also introduce N real-valued architecture parameters {i}, but instead of directly using the realvalued weights {pi} (referred to as path weights), we additionally incorporate a binarization process that transforms the real-valued path weights to binary gates:

[1, 0, · · · , 0] 

with probability p1,

g = binarize(p1, · · · , pN ) =

···

[0, 0, · · · , 1] with probability pN .

(2)

Based on the binary gates g, the output of the mixed operation is given as:

mOBinary(x) =

N

o1(x) gioi(x) = · · ·

with probability p1 .

i=1 oN (x) with probability pN .

(3)

And the expectation of mOBinary(x) equals to mDOARTS(x):

N
E[mBOinary(x)] = pioi(x) = mODARTS(x).
i=1

(4)

As illustrated in Eq. (3) and Figure 2, by using the binary gates rather than real-valued path weights (Liu et al., 2018c), only one path of activation is active in memory at run-time and the memory requirement of training the cumbersome network is thus reduced to the same level of training a compact model.

Notice that setting the binary gate value to 0 corresponds to dropping out the path while setting to 1 corresponds to keeping the path. Therefore, when training the weight parameters of the cumbersome network (Figure 2 left), we are actually training the network with DropPath regularization (Zoph et al., 2018), as is done in One-Shot (Bender et al., 2018). While in our case, the probability of dropping out the path is not regarded as a hyperparameter but a trainable parameter. Furthermore, we are also not aiming to make the cumbersome network to be a good reference for ranking sampled architectures but directly learn the architecture by learning architecture parameters. Additionally, we allow only one path to be activated during training, while in One-Shot most of the paths are activated, beginning with all paths activated.

3.3 TRAINING BINARIZED ARCHITECTURE PARAMETERS
An illustration of the training procedure of the weight parameters and binarized architecture parameters in the cumbersome network is provided in Figure 2. When training weight parameters, we freeze the architecture parameters and stochastically sample binary gates according to Eq. (2) for each batch of input data. Then the weight parameters of active paths are updated via standard gradient decent on the training set (Figure 2 left). When training architecture parameters, the weight parameters are fixed, then we reset the binary gates and update the architecture parameters on the validation set (Figure 2 right). These two update steps are performed in an alternating manner. Once the training of architecture parameters is finished, we can then derive optimized compact architecture. In this work, we simply keep the path with the highest path weight.
Unlike weight parameters, the architecture parameters are not directly involved in the computation graph and thereby cannot be updated using the standard gradient decent. In this section, we introduce two algorithms for learning the architecture parameters.

3.3.1 GRADIENT-BASED UPDATES
The gradient-based algorithm for learning binarized architecture parameters is build upon BinaryConnect (Courbariaux et al., 2015) that proposes to update the real-valued weight using the gradient w.r.t. its corresponding binary gate. In our case, analogously, the gradients w.r.t. architecture

4

Under review as a conference paper at ICLR 2019

parameters can be approximately estimated using L/gi in replace of L/pi:

L N =

L pj  N

L pj = N

L 

i j=1 pj i j=1 gj i j=1 gj

exp(j ) k exp(k )
i

=

N j=1

L gj

pj (ij

-

pi),

(5)

where ij = 1 if i = j and ij = 0 if i = j. Since the binary gates g are involved in the com-
putation graph, as shown in Eq. (3), L/gj can be calculated through backpropagation. However,
computing L/gj actually requires to calculate and store oj(x). Therefore, directly using Eq. (5) to update the architecture parameters would also require roughly N times GPU memory compared to training a compact model.

To address this issue, we consider to factorize the task of choosing one path out of N candidates into multiple binary selection tasks. The intuition is that if a path is the best choice at a certain position, it should be the better choice when solely compared to any other path.

Following this idea, within an update step of the architecture parameters, we first sample two paths according to the multinomial distribution (p1, · · · , pN ) and mask all the other paths as if they do not exist. As such the number of candidates temporarily decrease from N to 2, while the path weights {pi} and binary gates {gi} are reset accordingly. Then we update the architecture parameters of these two sampled paths using the gradients calculated via Eq. (5). Finally, as path weights are
computed by applying softmax to the architecture parameters, we need to rescale the value of these
two updated architecture parameters by multiplying a ratio to keep the path weights of unsampled
paths unchanged. As such, in each update step, one of the sampled path is enhanced (path weight
increases) and the other sampled path is attenuated (path weight decreases) while all other paths keep unchanged. In this way, regardless of the value of N , only two paths are involved in each
update step of the architecture parameters, and thereby the memory requirement is reduced to the
same level of training a compact model.

3.3.2 REINFORCE-BASED UPDATES FOR NON-DIFFERENTIABLE OBJECTIVES

The gradient-based algorithm relies on the gradients of the loss function w.r.t. binary gates. But not all objectives are differentiable, such as latency, energy and memory. In this work, we consider another solution for updating binary weights based on REINFORCE (Williams, 1992), as an alternative to BinaryConnect (Courbariaux et al., 2015).

Consider a network that has both weight parameters and binarized architecture parameters . Here we assume the network only has one mixed operation for ease of illustration. Since only one path will be retained when deriving the final optimized architecture, the goal of updating architecture parameters is actually to find the optimal binary gates g that maximizes a certain reward, denoted
as R(·). Therefore, according to REINFORCE (Williams, 1992), we have the following updates for architecture parameters:

N

J () = Eg[R(Ng)] = piR(N (e = oi)),

i=1

NN

J () = R(N (e = oi))pi = R(N (e = oi))pi log(pi),

i=1

i=1

=

Eg[R(Ng) log(p(g))]



1 M

M

R(Ngi ) log(p(gi)),

i=1

(6)

where gi denotes the ith sampled binary gates, p(gi) denotes the probability of sampling gi according to Eq. (2) and Ngi is the compact network according to the binary gates gi. Since Eq. (6) does not require R(Ng) to be differentiable w.r.t. g, it can thus handle non-differentiable objectives. An interesting observation is that Eq. (6) has a similar form to the standard NAS (Zoph & Le, 2017), while it is not a sequential decision making process and no RNN meta-controller is used in our case. Furthermore, since both gradient-based updates and REINFORCE-based updates are essentially two different update rules to the same binarized architecture parameters, it is possible to combine them to form a new update rule for the architecture parameters.

4 EXPERIMENTS AND RESULTS
We demonstrate the effectiveness of our proposed method on two benchmark datasets (CIFAR-10 and ImageNet) for the image classification task. Unlike previous NAS works (Zoph et al., 2018; Cai

5

Under review as a conference paper at ICLR 2019

Model DenseNet-BC (Huang et al., 2017) PyramidNet (Han et al., 2017) Shake-Shake + c/o (DeVries & Taylor, 2017) PyramidNet + SD (Yamada et al., 2018) ENAS + c/o (Pham et al., 2018) DARTS + c/o (Liu et al., 2018c) NASNet-A + c/o (Zoph et al., 2018) PathLevel EAS + c/o (Cai et al., 2018b) AmoebaNet-B + c/o (Real et al., 2018) Proxyless-R + c/o (ours) Proxyless-G + c/o (ours)

Params 25.6M 26.0M 26.2M 26.0M 4.6M 3.4M 27.6M 14.3M 34.9M 5.8M 5.7M

Test error
3.46 3.31 2.56 2.31
2.89 2.83 2.40 2.30 2.13
2.30 2.08

Table 1: Proxyless NAS achieves state-of-the-art performance on CIFAR-10.

Accuracy (%)

96.8

96.6

96.4

96.2

96.0

One-Shot * DARTS

95.8 Proxyless-R

95.6 Proxyless-G

1.0 1P.a5rams (M2.)0 2.5

Figure 3: Proxyless NAS outperforms DARTS and One-Shot on CIFAR-10.

et al., 2018b; Liu et al., 2018c) that first learn CNN blocks on CIFAR-10 under small-scale setting (e.g. fewer blocks), then transfer the learned block to ImageNet or CIFAR-10 under large-scale setting by repeatedly stacking it, we directly learn the architectures on the target task (either Cifar or ImageNet) and allow each block to be different.

4.1 EXPERIMENTS ON CIFAR-10

Architecture Space. For CIFAR-10 experiments, we use the tree-structured architecture space that is introduced by Cai et al. (2018b) with PyramidNet (Han et al., 2017) as the backbone. Specifically, we replace all 3 × 3 convolution layers in the residual blocks of a PyramidNet with tree-structured cells, each of which has a depth of 3 and the number of branches is set to be 2 at each node (except the leaf nodes). The allocation scheme and merge scheme of a non-leaf node are set to be replication and add respectively. For further details about the tree-structured architecture space, we refer to the original paper (Cai et al., 2018b). In our experiments, instead of using a meta-controller to predict which operation should be chosen at each edge in a tree-structured cell, we set each edge to be a mixed operation over the candidate set2 and use the proposed method to learn the cell structures. Moreover, by using distinct architecture parameters for different cells, they are free to be have different structures. Additionally, we use two hyperparameters to control the depth and width of a network in this architecture space, i.e. B and F , which respectively represents the number of blocks at each stage (totally 3 stages on CIFAR-10) and the number of output channels of the final block.
Training Details. CIFAR-10 consists of 50,000 training images and 10,000 test images. We randomly sample 5,000 images from the training set as a validation set for learning architecture parameters which are updated using the Adam optimizer with an initial learning rate of 0.006 for gradientbased algorithm (Section 3.3.1) and 0.01 for REINFORCE-based algorithm (Section 3.3.2). In the following discussions, we refer to these two algorithms as Proxyless-G (gradient) and Proxyless-R (REINFORCE) respectively.
After the training process of the cumbersome network completes, a compact network is derived according to the architecture parameters, as discussed in Section 3.3. Next we train the compact network using the same training settings except that the number of training epochs increases from 200 to 300. Additionally, when the DropPath regularization (Zoph et al., 2018; Huang et al., 2016) is adopted, we further increase the number of training epochs to 600 (Zoph et al., 2018).
Results. We first apply the proposed method to learn architectures in the tree-structured architecture space with B = 18 and F = 400. Since we do not force cells to share the same structure and each cell has 12 learnable edges, totally 12 × 18 × 3 = 648 decisions are required to fully determine the architecture. Furthermore, each edge has 7 possible options, so there are 7648  10547 possible architectures in the entire search space.
The test error rate results of our proposed method and other state-of-the-art architectures on CIFAR10 are summarized in Table 1, where "c/o" indicates the use of Cutout (DeVries & Taylor, 2017). Compared to these state-of-the-art architectures, our proposed method can achieve not only lower test error rate but also better parameter efficiency. Specifically, Proxyless-G reaches a test error rate of 2.08% which is slightly better than AmoebaNet-B (Real et al., 2018) (the previous best architecture on CIFAR-10). Notably, AmoebaNet-B uses 34.9M parameters while our model only uses 5.7M parameters which is 6× fewer than AmoebaNet-B. Furthermore, compared with PathLevel
2The list of operations in the candidate set is provided in the appendix.

6

Under review as a conference paper at ICLR 2019

Model MobileNetV2 (Sandler et al., 2018) ShuffleNetV2 (1.5) (Ma et al., 2018) ResNet-34 (He et al., 2016) NASNet-A Zoph et al. (2018) MnasNet (Tan et al., 2018) Proxyless (ours)

Top-1 72.0 72.6 73.3 74.0 74.0 74.5

Top-5 91.0
91.4 91.3 91.8 92.1

GPU latency 6.1ms 7.3ms 8.0ms 38.3ms 6.1ms 5.1ms

Table 2: Accuracy (%) and measured latency on ImageNet.

Method NASNet DARTS Mnas Ours

GPU hours 104 102 104 102

GPU memory 101 102 101 101

Table 3: Proxyless NAS save GPU hours and GPU memory (GB) by 1-2 orders of magnitude on ImageNet.3

EAS (Cai et al., 2018b) that also explores the tree-structured architecture space, both Proxyless-G and Proxyless-R achieves similar or lower test error rate results with half fewer parameters. The strong empirical results of our proposed method demonstrate the benefits of removing the constraint of forcing all cells to be the same and directly exploring a large architecture space instead of repeatedly stacking the same cell which is learned on a small architecture space. It also shows that our proposed method can effectively handle such a huge architecture space.
To further justify our proposed method, we directly compare our method to two most related works, i.e. One-Shot (Bender et al., 2018) and DARTS (Liu et al., 2018c), using the same architecture space under three different settings, i.e. (B = 4, F = 256), (B = 8, F = 256) and (B = 8, F = 400). Additionally, to save time, the DropPath regularization is not used when training all the derived architectures. So they are trained for 300 rather than 600 epochs under all three settings. For DARTS, due to its high GPU memory requirement (Section 3.1), we apply it to learn the cell structure on the first setting, i.e. (B = 4, F = 256), and then transfer the learned cell to the other two settings. For One-Shot, we implement a modified version, denoted as One-Shot. We set the probability of keeping a path to be 1/N , where N is the number of candidate operations, rather than a high value in the original paper. As such, One-Shot can be viewed as a reduced version of our proposed method, where the architecture parameter updates are disabled. Also by setting a low keep probability, which reduces the GPU memory consumption, One-Shot can be directly applied to all of the three settings.
The results are reported in Figure 3 where we can observe that: (i) Both Proxyless-G and ProxylessR perform better than One-Shot under all settings, which demonstrates the importance of architecture parameter updates; (ii) DARTS performs better than One-Shot on the first setting where DARTS is directly learning on. But on the other two settings, the transferred cell of DARTS performs worse than the directly learned cell of One-Shot; (iii) Proxyless-G performs worse than Proxyless-R on the first setting while performs better on the other two settings. Notice that the architecture space is larger in the last two settings, we suppose it might because that gradient-based algorithm is more efficient in handling large architecture space than REINFORCE-based algorithms.

4.2 EXPERIMENTS ON IMAGENET
Not all rewards are differentiable. For ImageNet experiments, we aim to learn efficient CNN architectures that not only have high accuracy but also low latency on specific devices (e.g. GPU, CPU, mobile phone, etc.). Therefore, it is a multi-objective NAS task (Hsu et al., 2018; Dong et al., 2018; Elsken et al., 2018a; He et al., 2018; Tan et al., 2018), where one of the objectives is nondifferentiable (i.e. latency). Thus, we employ Proxyless-R for ImageNet experiments. Same as Tan et al. (2018), we use ACC(m) × [LAT (m)/T ]w as the optimization goal, where ACC(m) denotes the accuracy of model m, LAT (m) denotes the latency of m, T is the target latency and w is a hyperparameter for controlling the trade-off between accuracy and latency. We use on GPU latency and CPU latency in our experiments as reward. If given mobile farm infrastructure, our algorithm can be easily transferred to the mobile setting. The GPU latency is measured on V100 GPU with a batch size of 8 (single batch will make GPU severely under-utilized). The CPU latency is measured on AWS p3.8 instance with a batch size of 1.
Architecture Space. We use MobileNetV2 (Sandler et al., 2018) as the backbone to build the architecture space. Specifically, rather than having 3 × 3 mobile inverted bottleneck convolution (MBConv) everywhere, we allow a set of MBConv layers with various kernel sizes {3, 5, 7} and expansion ratios {3, 6}. To enable a direct trade-off between width and depth, we initiate the cumbersome network to have more blocks than MobileNetV2 and allow a block with the residual connection to be skipped by adding the zero operation to the candidate set of its mixed operation. In this way, with a limited latency budget, the network can either choose to be shallower and wider by skipping more blocks and using larger MBConv layers or choose to be deeper and thinner by keeping more blocks and using smaller MBConv layers.
3Further details about the Table 3 is provided in appendix.

7

3x232x42x2242x4224 CoCnovn3vx33x3
40x4101x21x1121x2112 MBMCBoCnovn1v31x33x3
24x2141x21x1121x2112 MBMCBoCnovn6v36x33x3
32x3526xx5566x56 MBMCBoCnovn3v33x33x3
32x3526xx5566x56 MBMCBoCnovn3v33x33x3
32x3526xx5566x56 MBMCBoCnovn3v33x33x3
32x3526xx5566x56 MBMCBoCnovn6v36x33x3
48x4288xx2288x28 MBMCBoCnovn3v33x33x3
48x4288xx2288x28 MBMCBoCnovn3v33x33x3
48x4288xx2288x28 MBMCBoCnovn3v53x55x5
48x4288xx2288x28 MBMCBoCnovn6v36x33x3
88x8184xx1144x14 MBMCBoCnovn3v33x33x3
88x8184xx1144x14 MBMCBoCnovn6v56x55x5
1041x0144xx1144x14 MBMCBoCnovn3v33x33x3
1041x0144xx1144x14 MBMCBoCnovn3v33x33x3
1041x0144xx1144x14 MBMCBoCnovn3v33x33x3
1041x0144xx1144x14 MBMCBoCnovn6v56x55x5
2162x176xx77x7 MBMCBoCnovn3v53x55x5
2162x176xx77x7 MBMCBoCnovn3v53x55x5
2162x176xx77x7 MBMCBoCnovn3v33x33x3
2162x176xx77x7 MBMCBoCnovn6v56x55x5
3603x670xx77x7 PoPoloionlignFgCFC

Under review as a conference paper at ICLR 2019

3x224x224 Conv 3x33x224x224 Conv 3x3
40x112x112 MBConv1 34x03x112x112 MBConv1 3x3
24x112x112 MBConv6 32x43x112x112 MBConv6 3x3
32x56x56 MBConv3 3x332x56x56 MBConv3 3x3
32x56x56 MBConv3 3x332x56x56 MBConv3 3x3
32x56x56 MBConv3 3x332x56x56 MBConv3 3x3
32x56x56 MBConv6 3x332x56x56 MBConv6 3x3
48x28x28 MBConv3 3x438x28x28 MBConv3 3x3
48x28x28 MBConv3 3x438x28x28 MBConv3 3x3
48x28x28 MBConv3 5x458x28x28 MBConv3 5x5
48x28x28 MBConv6 3x438x28x28 MBConv6 3x3
88x14x14 MBConv3 3x838x14x14 MBConv3 3x3
88x14x14 MBConv6 5x858x14x14 MBConv6 5x5
104x14x14 MBConv3 3x1034x14x14 MBConv3 3x3
104x14x14 MBConv3 3x1034x14x14 MBConv3 3x3
104x14x14 MBConv3 3x1034x14x14 MBConv3 3x3
104x14x14 MBConv6 5x1054x14x14 MBConv6 5x5
216x7x7 MBConv3 5x2516x7x7 MBConv3 5x5
216x7x7 MBConv3 5x2516x7x7 MBConv3 5x5
216x7x7
MBConv3 3x2316x7x7 MBConv3 3x3
216x7x7
MBConv6 5x2516x7x7 MBConv6 5x5
360x7x7
Pooling FC360x7x7 Pooling FC

(a) Efficient CPU model found by Proxyless NAS (deep and thin, only 3x3 and 5x5, late pooling).

3x232x42x2242x4224 CoCnovn3vx33x3
40x4101x21x1121x2112 MBMCBoCnovn1v31x33x3
24x2141x21x1121x2112 MBMCBoCnovn3v53x55x5
32x3526xx5566x56 MBMCBoCnovn3v73x77x7
56x5268xx2288x28 MBMCBoCnovn3v33x33x3
56x5268xx2288x28 MBMCBoCnovn6v76x77x7
1121x1124xx1144x14 MBMCBoCnovn3v53x55x5
1121x1124xx1144x14 MBMCBoCnovn6v56x55x5
1281x2184xx1144x14 MBMCBoCnovn3v33x33x3
1281x2184xx1144x14 MBMCBoCnovn3v53x55x5
1281x2184xx1144x14 MBMCBoCnovn6v76x77x7
2562x576xx77x7 MBMCBoCnovn6v76x77x7
2562x576xx77x7 MBMCBoCnovn6v76x77x7
2562x576xx77x7 MBMCBoCnovn6v56x55x5
2562x576xx77x7 MBMCBoCnovn6v76x77x7
4324x372xx77x7 PoPoloionlignFgCFC

(b) Efficient GPU model found by Proxyless NAS (shallow and wide, many 7x7, early pooling).
Figure 4: Efficient models optimized for different hardware. "MBConv3" and "MBConv6" denote mobile inverted bottleneck convolution layer with an expansion ratio of 3 and 6 respectively.

Model

Top-1 (%) GPU latency CPU latency

Proxyless NAS searched on GPU 74.5

5.1ms

204.0ms

Proxyless NAS searched on CPU 74.6

7.4ms

134.8ms

Table 4: Hardware prefers specialized models. Models optimized for GPU does not run fast on CPU, vice versa. Proxyless NAS provide an efficient, automated way to design specialized models for different hardware.

Training Details. We randomly sample 50,000 images from the training set as a validation set during the architecture search. The settings for updating architecture parameters are the same as CIFAR-10 experiments except the initial learning rate is 0.001. The cumbersome network is trained for 100 epochs on the remaining training images with batch size 256.
ImageNet Classification Results. Table 2 reports the results of our searched architecture with accuracy and GPU latency as objectives. We use the GPU latency of MobileNetV2 as the target latency. Compared to MobileNetV2 and ResNet-34, our model improves the top-1 accuracy by 2.5% and 1.2% respectively while maintaining lower GPU latency. While compared with MnasNet, our model can achieves 0.5% higher top-1 accuracy with 1.2× fewer GPU latency. More importantly, we are much more sample efficient: the GPU-hour is 100× fewer than MnasNet (Table 3).
Discussion on Our Searched Models for Different Platforms. Beside GPU latency, we also apply our method to learn architectures with CPU latency as an objective. The results of our GPU model and CPU model are reported in Table 4, while their detailed architectures are provided in Figure 4. From Figure 4, we notice an interesting fact that the architecture of our GPU model is totally different from the architecture of our CPU model: (i) The GPU model is shallower than the CPU model, especially in early stages where the resolution of the feature map is high; (ii) In the GPU model, 7 × 7 MBConv operations are preferred , especially in the last stage while in the CPU model no 7 × 7 MBConv operation is chosen. This is because GPU has much higher parallelism than CPU so it's advantageous for large kernel size. The take-home is: to achieve better efficiency, we actually need to specialize neural network architectures for different hardware architecture, rather than using the same model for all hardware.

5 CONCLUSION
In this work, we introduce Proxy-less Architecture Search, which is able to directly learn architectures on the target task without any proxy. We also reduces the cost (GPU-hours and GPU memory) of NAS to the same level of normal training using path binarization. Benefit from the direct search, we achieve strong empirical results on CIFAR-10 and ImageNet. Furthermore, we allow specializing network architectures for different platforms by directly incorporating the measured hardware latency into optimization objectives. We compared the optimal model on CPU/GPU and raised the awareness that specialized neural network architecture is needed for different hardware architectures.
8

E3 E35xc5o5nxvc5onv

E3 E3

E6 E6

55xcxc5o5onnvv

E1 E1

33xcxc3o3onnvv

E

E6 E6

E1 E1con3vcxo3n3vx3

EE33

cco7onxnv7v

E6 E

E6 7x7

E6

E3 E3

Under review as a conference paper at ICLR 2019
REFERENCES
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understanding and simplifying one-shot architecture search. In ICML, 2018.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. In ICLR, 2018.
Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by network transformation. In AAAI, 2018a.
Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level network transformation for efficient architecture search. In ICML, 2018b.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In NIPS, 2015.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.
Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun. Ppp-net: Platform-aware progressive search for pareto-optimal neural architectures. 2018.
Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. Simple and efficient architecture search for convolutional neural networks. arXiv preprint arXiv:1711.04528, 2017.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Multi-objective architecture search for cnns. arXiv preprint arXiv:1804.09081, 2018a.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. arXiv preprint arXiv:1808.05377, 2018b.
Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. In CVPR, 2017.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In NIPS, 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In ECCV, 2018.
Chi-Hung Hsu, Shu-Huan Chang, Da-Cheng Juan, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and ShihChieh Chang. Monas: Multi-objective neural architecture search using reinforcement learning. arXiv preprint arXiv:1806.10332, 2018.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In ECCV, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017.
Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In ECCV, 2018a.
Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representations for efficient architecture search. In ICLR, 2018b.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018c.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In ICCV, 2017.
9

Under review as a conference paper at ICLR 2019
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In ECCV, 2018.
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. In ICML, 2018.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548, 2018.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V Le. Mnasnet: Platformaware neural architecture search for mobile. arXiv preprint arXiv:1807.11626, 2018.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning. 1992.
Yoshihiro Yamada, Masakazu Iwamura, and Koichi Kise. Shakedrop regularization. arXiv preprint arXiv:1802.02375, 2018.
Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. Practical block-wise neural network architecture generation. In CVPR, 2018.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In ICLR, 2017. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In CVPR, 2018.
10

Under review as a conference paper at ICLR 2019
A THE LIST OF CANDIDATE OPERATIONS USED ON CIFAR-10
Similar to previous works (Zoph et al., 2018; Cai et al., 2018b; Liu et al., 2018c), we adopt the following 7 operations in our CIFAR-10 experiments:
· 3 × 3 dilated depthwise-separable convolution · Identity · 3 × 3 depthwise-separable convolution · 5 × 5 depthwise-separable convolution · 7 × 7 depthwise-separable convolution · 3 × 3 average pooling · 3 × 3 max pooling
B DETAILS ABOUT TABLE 3
Mnas trains 8,000 mobile-sized models on ImageNet, each of which is trained for 5 epochs for learning architectures. And our method trains a single model for 100 epochs whose cost of training is at the same level as training a mobile-size model. Therefore we use roughly 400× fewer GPU hours than Mnas, while our GPU memory cost also keeps at the same level of training a mobile-sized model. The GPU memory of DARTS is inferred using our architecture space on ImageNet, as the original paper does not directly search for architectures on ImageNet. Additionally, when training a mobile-sized model on ImageNet with a batch size of 64, the GPU memory consumption is roughly 10GB.
11

