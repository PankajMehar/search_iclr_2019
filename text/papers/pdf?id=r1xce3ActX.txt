Under review as a conference paper at ICLR 2019
LEARNING DEEP EMBEDDINGS IN KREIN SPACES
Anonymous authors Paper under double-blind review
ABSTRACT
The non-linear embedding achieved by a Siamese network is indeed a realization of a Hilbert space, i.e., a metric space with a positive definite inner product. Krein spaces generalize the notion of Hilbert spaces to geometrical structures with indefinite inner products. As a result, distances and norms in a Krein space can become negative. The negative spectral of an inner product is usually attributed to observation noise, though such a claim has never been fully studied, nor proved. Seeking how Krein spaces can be constructed from data, we propose a simple and innocent-looking modification to Siamese networks, equipping them with the power to realize indefinite inner-products. This provides a data-driven technique to decide whether the negative spectrum of an inner-product is helpful or not. We empirically show that our Krein embeddings outperform Hilbert space embeddings on recognition tasks.
1 INTRODUCTION
In this work, we propose an extension of Siamese networks that is able to realize Krein spaces (Azizov & Iokhvidov, 1981; Bogna´r, 1974). Siamese networks are de-facto machines in learning nonlinear embeddings and their traces can be found in a wide range of applications including face verification (Schroff et al., 2015; Chopra et al., 2005), person re-identification (Varior et al., 2016), image retrieval (Gordo et al., 2017), metric learning (Oh Song et al., 2016; Song et al., 2017), clustering (Hsu & Kira, 2015), domain adaptation (Motiian et al., 2017) and one-shot learning (Koch, 2015).
The embedding learned by a Siamese network realizes a Hilbert space, i.e., a complete metric space with a positive definite inner product1. In this paper we extend Siamese networks to realize Krein space, i.e., complete spaces with indefinite inner products. Krein spaces, while being to some degree alienated in computer science, have found their ways in quantum physics, suggesting the importance of our developments in a bigger context. Having said this, one observation that motivates us to look into Siamese structures is the theory of kernel machines. A well-practiced approach while facing indefinite kernels is to attribute the negative spectrum to observational noise, followed by removing it by flipping or clipping the negative spectrum. However, some recent studies challenge this assumption and show that the information contained in the negative spectrum of the Kernel can be useful (Chen & Ye, 2008; Bonnet-Loosli et al., 2016).
To the best of our knowledge, the same trend can be seen in learning representations and embeddings, meaning that positive-definiteness is deemed, believing that the negative spectrum contributes to the noise. That is why projections onto the positive definite cone Koestinger et al. (2012) or having constraints on the model Goldberger et al. (2005); Schroff et al. (2015) or during optimization Davis et al. (2007); Harandi et al. (2017) are ubiquitous in metric learning. One may claim that this is indeed compromising the data and can lead to throwing away valuable information. Furthermore, studies in human cognition (Scheirer et al., 2014) suggest that humans do not follow the rules embraced by distances in Hilbert spaces.
Based on the aforementioned discussion, we propose an innocent looking modification to Siamese networks, by just adding p extra parameters with p being the dimensionality of the embedding, to break free from Hilbert spaces. Our proposal equips the Siamese network with the capacity to learn
1In majority of cases, where the dimensionality of representations is high, the Siamese structure indeed realizes a semi-definite metric. This means that the distances can become zero even for unequal inputs. This technicality is somehow neglected.
1

Under review as a conference paper at ICLR 2019

deep embeddings in Krein spaces. Intuitively, the distances we compute for learning are indefinite. Hence, we do not make any tampering to the negative spectrum of the learned distance model. Instead, we let the data decide what type of distances (i.e., positive definite or indefinite) fits its structure the best. Interestingly, our formulation does not require the use of any complex parameters or gradients.
Our contributions in this paper are as,
1. We propose to learn deep Krein embeddings in contrast to what conventional Siamese networks can achieve, i.e., Hilbert space embedding.
2. We develop a solution that is capable of modelling a Krein space, deciding the significance of the positive/negative spectral properties of the model in a data driven fashion.
3. With empirical evaluations, we show that our proposed solution outperforms Hilbert space embeddings.

2 PROPOSED SOLUTION

We start by introducing our notations. Throughout the paper, bold capital letters denote matrices (e.g., X) and bold lower-case letters denote column vectors (e.g., x). In is the n × n identity matrix. S+n+ denote the Symmetric Positive Definite (SPD) manifold. Given by Diag (1, 2, · · · , n) is a diagonal matrix with the real values 1, 2, · · · , n as diagonal elements. We use the notation, H  Rn to denote an Hilbert space of dimensionality n. We use the notation [·]+ to denote max(0, x). We use the notation ·, · H to represent the inner product of a Hilbert space, H. We use  to represent the direct sum2 of two spaces. Furthermore, our discussion often require referencing to
the Mahalanobis distance.

Definition 1 (The Mahalanobis Distace) The Mahalanobis distance between xi and xj  Rn is

defined as,

d2(xi, xj) = (xi - xj)T M (xi - xj),

(1)

with M  S+n+.

To learn a Hilbert embedding, a loss with pair-wise Mahalanobis distances is often minimized (Davis et al., 2007; Weinberger & Saul, 2009; Koestinger et al., 2012; Harandi et al., 2017). A more generalized framework for learning a Mahalanobis distance is to learn the metric along a low-dimensional mapping Harandi et al. (2017) which fits better for our discussions. Formally, given xi, xj  Rn as feature vectors from a non-linear deep model, the squared Mahalanobis distance between them in a lower-dimensional Hilbert space, H  Rp is,
d2(xi, xj) = (W xi - W xj)T M (W xi - W xj) = (xi - xj)T W T M W (xi - xj). (2)

Here, the space H is identified by the projection W  Rp×n and the distance is parameterized with M  S+p +. In the case of Siamese networks, the matrices M and W are combined into a fully-connected layer parameterized with L  Rp×n according to, L = M 0.5W 3. Hence, the embedding distance defined in equation 2 could be re-organized for Siamese networks as,

d2(xi, xj) = (xi - xj)T W T M W (xi - xj) =

M 0.5W (xi - xj )

2 2

=

Lxi - Lxj

22.

(3)

Definition 2 (Krein Spaces) An inner product space (K, ·, · K) is a Krein space if there are two Hilbert spaces, H+ and H- spanning K such that f = f+ f-, f  K with f+  H+, f-  H- and f , g K = f+, g+ H+ - f-, g- H- , f , g  K.

From the definition, it becomes clear that Krein embedding has the capacity to model indefinite distances. Hence, learning Krein embeddings avoids the prior assumption that the negative spectrum of the data/model is noise. Having said this, enforcing negative distances with the typical Siamese

2When U1, U2 are subspaces of V . Then V is said to be the direct sum of U1 and U2, V = U1  U2 , V if V = U1 + U2 and U1  U2 = {0}
3Note that the matrix L remains real for the positive definite M

2

Under review as a conference paper at ICLR 2019

model structure would require a complete modification to the model such as using complex valued parameters (i.e., non-real as in Trabelsi et al. (2017)) for L (see equation 3).
In contrast, we propose a modification to the Siamese network that only requires an additional number of parameters equal to the dimensionality of the embeddings space. More importantly our proposal do not require complications such as complexed valued parameters or gradients.

2.1 MODELLING INDEFINITE DISTANCES WITH DIAGONALIZATION

It is the current practice in embedding learning to constraint M (see equation 2) to be symmetric positive definite. Instead, we propose a solution that provides flexibility in spectral properties of M . Formally, we achieve this by decomposing M as,

M = Q × Diag (1, 2, · · · , p) × QT .

(4)

Here, Q  Rp×p is an orthogonal matrix and 1, 2, · · · , p  R are unconstrained diagonal entries. With this decomposition, we rewrite the original metric definition in equation 2 as,

d2in.(xi, xj) = (xi - xj)T W~ T × Diag (1, 2, · · · , p) × W~ (xi - xj).

(5)

Following the joint dimensionality reduction and metric learning approach, we obtain W~ = QT W and the diagonal entries, 1, 2, · · · , p. This diagonalized implementation provides us a way to directly model the eigen values of the matrix, M . In this way, we assure that the spectral properties of M are kept unconstrained while enforcing the symmetric property.
Furthermore, this parameterization enables us to seamlessly integrate our model into a deep learning framework using two fully-connected layers without bias parameters. More specifically, we use a fully connected layer of size n × p (i.e., input × output) to model W~ . We model the diagonal matrix with a fully-connected layer of p × p and we set its non-diagonal entries to be zero after every gradient update. However, since only p number of elements are non zero in the diagonal layer, our solution costs only a negligible amount of additional parameters from the Siamese networks. Shortly, we will provide a concise proof to show that our solution models a Krein space.
In our solution we do not make any assumptions about how important is the positive/negative spectrum component of the metric for learning embeddings. Note that in existing metric learning literature, the negative spectrum is pre-judged as noise and discarded.

2.2 TRAINING WITH PAIR-WISE DISTANCES

Here we present details about the pair-wise loss we use for training. First, we shed light on
the policy we use when comparing instance similarities with indefinite distances. Let, X = {x1, x2, · · · , xi · · · xN }, x  Rn be our labeled training data from a given mini-batch4. We use 
to represent all the learnable parameters in our full deep model.

Remark 1 (Comparing with Indefinite Distances) The indefinite distance could at times be negative. We consider that a negative distance with a high absolute value will reflect a high similarity between the compared instances. Furthermore, we consider that a negative distance always indicates better similarity than a positive or a "0" distance.

The purpose of a pair-wise distance learning is to 1. minimize the distances between similarly
labeled samples and 2. maximize the distances between dis-similarly labeled samples. Therefore we form labeled pairs, (xi, xj)k with a label yk  {-1, +1} for k = 1, 2, 3, · · · , Npairs. Here, yk = +1 if the two instances in a pair belong to the same class. Otherwise, it is given the label, yk = -1. The model training is performed by minimizing the loss,

Npairs

Lpair() =

[yk × (di2n(xi, xj ) - uk)]+.

p=1

(6)

4Since our final model is realized as a deep network we use mini-batch optimization.

3

Under review as a conference paper at ICLR 2019

Here, uk  R is a scalar hyper-parameter defining the margin for the contrastive loss. The usual practice to use a single non-negative value for this margin. However, since our implementations
support negative distances we could also use negative values for the margin. For instance, one set of our experiments show that using uk = -1 for labeled similar pairs and uk = +1 for labeled dissimilar pairs helps improving the test performance.

MODELLING A KREIN SPACE

Here, we show that our distance (see equation 5) can be partitioned into a difference between two inner products from two separate Hilbert spaces, H+ and H-, hence realizing a Krein space. Consider that our learned projection parameters are W~  Rp×n and D = Diag (1, 2, · · · , p)  Rp×p. Furthermore, consider that out of the p diagonal elements, the first p+ elements are positive. The remaining p- = p - p+ elements are non-positive. Our objective here is to show that our distance computation can be partitioned into a difference between two inner products in H+  Rp+ and H-  Rp- .
Having defined, p+, p-, let the diagonal matrices D+ = Diag 1, 2, · · · , p+ and
D- = Diag (p++1), (p++2), · · · , p separate the positive and negative elements of D. Furthermore, let W~ p+ and W~ p- represents the first p+ and the latter p- rows of W~ , respectively. If x~ = xi - xj is the difference between instance feature, we can rewrite the equation 5 according to,

d2in.(xi, xj ) = x~T

W~ p+ T W~ p-

Dp+ 0

0 Dp-

W~ p+ W~ p-

x~

= x~T (W~ pT+ Dp+ W~ p+ + W~ pT- Dp- W~ p- )x~

= Dp0+.5W~ p+ x~, Dp0+.5W~ p+ x~ - (-Dp- )0.5W~ p- x~, (-Dp- )0.5W~ p- x~ .

(7)

Note Dp- is negative definite. Therefore, (-Dp- )0.5 is a real diagonal matrix. The decomposition of di2n. provided in equation 7 is in fact a difference of an inner product as in a Krein space(see Definition 2). The difference is between the inner products of the two Hilbert spaces defined by the projections Dp0+.5W~ p+ , i.e., positive space and (-Dp- )0.5W~ p- , i.e., negative space. An important thing to notice here is we made no prior assumptions about the dimensionality or weight of the two
individual Hilbert spaces. In other words, the numbers p+ and p- are entirely decided by the data.

3 MODEL INITIALIZATION
It is the practice in deep learning to initialize the network model with pre-trained weights. Such initializations have shown to improve the convergence speed and the final performance. Hence, we initialize the weights of our deep feature extractor model with pre-trained weights with softmax cross-entropy loss. As long as the parameters W~ and Diag (1, 2, · · · , p) are considered, we propose an initialization using a closed form Mahalanobis-like estimation. We recall our notations, a pair (xi, xj)k is given the label yk = +1 if the two pair instances belong to the same class or -1 otherwise. We use the notation Wpca  Rp×n to denote the PCA projection matrix for the training data. Hence, let zi = Wpcaxi be the low dimensional PCA embeddng of an instance xi.
Assuming that pair wise difference space for the labeled similar/dissimilar pair features form two multi-variate Gaussian distributions, the likelihood that a given pair,(zi, zj)k is dissimilar is captured by the function,

(zi - zj ) = (zi - zj )T (-s 1 - -d 1)(zi - zj ).

(8)

Here, s

=

1 (Nsim -1)

Nsim k,yk =+1

(zi

-

zj

)(zi

-

zj

)T

and

d

=

1 (Ndis -1)

Ndis k,yk =-1

(zi

-

zj

)(zi

-

zj)T . Nsim and Ndis represents the number of labeled similar and dissimilar pairs, respectively.

Given that the function, (zi - zj) indicates a dissimilarity measure between between two vectors,

we propose using its parameters in our model initialization. Formally, when M0 = -s 1 - d-1 and

4

Under review as a conference paper at ICLR 2019

its eigen decomposition is M0 = U U T , our model parameters are initialized as, Diag (1, 2, · · · , p)   W~  U T Wpca.

(9) (10)

Note, that  is a diagonal matrix and could be directly used in initializing the diagonal parameters of our model. Koestinger et al. (2012) proposes to use the positive definite projection of M0  M0+ as a Mahalanobis metric.

4 FURTHER DISCUSSION : TWO-SPACE INDEFINITE METRIC REALIZATION

In this section we discuss another realization for indefinite distance learning. However, unlike
the our proposed solution this realization uses the prior assumption that the negative distances are
equally important as positive distances. The core elements behind this realization are two separate equal dimensional Hilbert space. We refer to them as the positive embedding space, H+  Rp and the negative embedding space, H-  Rp. For a given data instance, x  Rn , the idea here is to have two separate embeddings, W+x  H+ and W-x  H- in each of these spaces. The two spaces H+ and H- are parameterized interms of two separate projection matrices W+, W-  Rp×n , respectively. Although there are two separate embedding spaces we define a combined distance
according to,

d2in,2(xi, xj ) = (xi - xj )T W+T M+W+ - W-T M-W- (xi - xj ).

(11)

Here, xi, xj  Rn are the two compared input data instances. The matrices, M+, M-  S+p + parameterizes two Mahalanobis metrics in H+ and H-, respectively. The intuition behind having the additional negative space, H- is to provide freedom for the combined metric to attain negative values regardless of the positive definite metrics M+ and M-.

Simalry to our proposed method, we follow a joint embedding space and distance learning approach
in this implementation. As such, we parameterize the projection matrices and the metrics with a
11
joint linear transformation. Formally, we use the parameters L+ = M+2 W+ and L- = M-2 W- to model the two spaces. Thereafter, we could rewrite the indefinite distance in equation 11 as,

d2in,2(xi, xj ) =

L+(xi - xj )

2 2

-

L-(xi - xj ) 22.

(12)

This decomposition enables us to easily incorporate the two-space solution into a deep network. Since L+, L-  Rp×n are linear transformations, we realize them with two fully-connected layers without the bias parameter. Here, the inputs xi, xj  Rn are output features from the deep feature extractor.

Model initialization: Here, we use the positive definite projection, M0+ (see section 3). For the

positive

space

embedding

layer,

L+

we

use

the

initialization,

M0+

1 2

Wpca.

For

the

negative

embed-

ding

space

we

use

a

similar

initialization,

-M0+

1 2

Wpca

with

a

sign

reversal.

This

sign

reversal

is

used to enforce the difference when treating the similar/dissimilar pair distances from the positive

space.

5 RELATED WORK
In this section we first conceptually compare our proposed solution with the existing Mahalanobis distance learning solutions. Thereafter, we discuss literature on indefinite Kernels. This topic is relevant to our idea as they belong to the class of work which shed light on the negative spectral components of data. Finally, we touch upon other "non-metric" learning solutions in machine learning literature.
Mahalanobis Metric learning :The primary motive behind learning a Mahalanobis metric (Davis et al., 2007; Weinberger & Saul, 2009; Koestinger et al., 2012; Harandi et al., 2017) is to learn a positive semi-definite matrix, M to fit pair-wise distances as in equation 2. This is equivalent to learning a global linear transformation of the input space preceded by an Euclidean distance computation (Weinberger & Saul, 2009). The principle difference between our solution with existing

5

Under review as a conference paper at ICLR 2019
Mahalanobis metric learning solutions is the given awareness to information contained in the negative spectral component of the model. For instance, the iterative metric updates of ITML (Davis et al., 2007) and Riemannian optimization solution of Harandi et al. (2017) are constrained to be on the S+n+. The M = LT L parametrization of Weinberger & Saul (2009) guarantees that the model is constrained to learn a positive semi-definite metric. The closed form KISS-ME solution explicitly perform a projection to S+n+ to discard the negative spectral components. Contrastively, our proposed Diagonalized solution gives provisions to keep the negative spectral energy of the metric unharmed.
Indefinite Kernel Methods: Positive definite Kernels plays an important role in machine learning from classification algorithms (Kernel SVM) (Fine & Scheinberg, 2001) to statistical learning algorithms (Maximum Mean Discrepancy) (Gretton et al., 2012; Pan et al., 2011). The positive definite Kernels assures the existence of Reproducing Kernel Hilbert Space (RKHS) (Daume´ III, 2004) embeddings for the data. However, it is not always the case for the similarity matrix to be positive definite (Weinberger & Saul, 2009). In such situations, it is the common practice to discard the negative spectrum of the Kernel matrix (Chen & Ye, 2008) by spectral modification. The assumption behind such approaches is that the negative spectral component only carry noise irrelevant to the learning task. In contrast to this assumption, literature has formed to show that the negative spectral component contain useful information for learning (i.e., Indefinite Kernel solution). For instance Bonnet-Loosli et al. (2016); Loosli & Canu (2010) uses Krein Spaces (Azizov & Iokhvidov, 1981) to realize indefinite Kernels for SVMs. We emphasize that our proposed indefinite metric learning framework has a similar motive to such indefinite Kernel methods. The reason being, both these solution classes raises the importance of considering the negative spectral component of the data.
Non-metric Recognition: All in all, a metric which violates any of the metric properties is a nonmetric. For this reason it is fair to argue that our proposed indefinite distances are too non-metrics. Hence, we shed light on literature surveying non-metric learning solutions for recognition problems. Scheirer et al. (2014), performs experiments on human subjects to show that human recognition violates several confines of metrics (e.g., the symmetry, triangular inequality). They further presents the notion "metric problem". In other words, a problem is said to be metric iff the best performance for a problem is shown by using a non-metric entity. Laub & MA~ zller (2004), performs evaluations on distortions brought to data which are non-metric in nature but embedded on metric spaces.
6 EXPERIMENTS
In this section we report details of our experiments. Our comparisons are based on image data on objects and digits. Namely, we use Google's Street view house numbers dataset (SVHN), CIFAR10 and CIFAR100. We use a 4 layer CNN, CNN-Small for our experiments.We provide more details of the network CNN-Small in the Appendix 8.2. For the CIFAR100 experiments we use CNN-Small as well as the All-Convolutional-Network of Springenberg et al. (2015). In all our experiments we use 1-NNs when evaluating the performance. Prior to training the models on our distance learning framework, we pre-train them with softmax cross entropy loss. For pre-training we use SGD optimizer with 0.9 momentum and a learning rate of 0.001 and step wise learning rate decaying for 200 epochs. We observe this improves the training speed as well as the final generalization performance.
SVHN: The SVHN dataset is a digit image dataset formed using house numbers in Google Street View images. This dataset consists 73, 257 labeled images for training and 26, 032 images for testing. All images are cropped to center the digit portion and scalled to 32 × 32 images. However, unlike in MNIST most of the images contain distracting artifacts at the sides.
CIFAR10: The CIFAR10 dataset consists of 60, 000, 32 × 32 images from 10 image classes (i.e., airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). There are labeled 50, 000 training and 10, 000 test images.
CIFAR100: The CIFAR100 images are similar to CIFAR10 images. However, it contains images from 100 classes. The labeled training and testing set sizes are 50, 000 and 10, 000, respectively.
Evaluated methods: In this section we refer to our proposed solution by Krein. Our comparisons are against positive definite distance learning. We refer to this baseline solution as Pos. Def.
6

Under review as a conference paper at ICLR 2019

Table 1: Classification accuracy comparison with 1-NNs

Solution SVHN(S) CIFAR10(S) CIFAR100 (S) CIFAR100(A)

Pos. Def. 88.5

61.1

29.0

61.3

Krein

89.7

62.4

30.9

62.1

For our experiments on SVHN and CIFAR10 we do not use data augmentation. The image pixel values are normalized into [-1, 1] range. For all CIFAR100 experiments we use random crops and flips. The images are normalized using channel means and standard deviations. We fine-tune all our models with RMSProp optimizer and a learning rate of 0.0001. We fine-tuning our models with distance losses for SVHN and CIFAR10 for 100 epochs. For CIFAR100 on CNN-Small, we finetune our model for 300 epochs. For CIFAR100 experiments with All-Conv-Net we fine-tune our model for 500 epochs. The embedding spaces for SVHN, CIFAR10 and CIFAR100 experiments on CNN-Small are of 32, 64 and 64 dimensionality (i.e., parameter p in our discussion). For CIFAR100 experiments we use an embedding space of 128 dimensionality. For SVHN experiments our training mini-batches consists 4 images from each class. We observe that for CIFAR10 experiments a large batch size consisting 16 images from each class shows better convergence. For CIFAR100 we sample 32 classes per batch and 4 samples per class. We report our results in Table 1. Here, we use CIFAR100(S) to denote CIFAR100 experiments on the CNN-Small and CIFAR100(A) to denote the experiments using the All-Conv-Net.
In all our experiments we observe that the best performance is obtained by our proposed Krein solution. In all our experiments except for the CIFAR100(A) we observe using the margin uk = -1 for similar pairs and uk = +1 for the dissimilar pairs gives the best performance (see section 2.2). We observe that the discusses Two-Space method in section 4 shows better performance than the positive definite learning. However, it does not perform better than the proposed diagonalized soluton.
6.1 VISUALIZATIONS
In this section we provide visualizations of our learned embeddings and the model. For these experiments we select the SVHN dataset and the CNN-Small model.
6.1.1 EMBEDDING VISUALIZATION
Although our proposed solution computes real distances, the embeddings learned are infact complex. For this reason, we are unable to directly visualize the embedding space. However, our solution and the studied 2-Space solution both could provide a real pair-wise distance matrix. We use this computed distance matrix with Multidimensional Scalling in visualizing the embeddings. To have all positive distances we subtract the least distance (i.e., largest negative value) from the distance matrix and force the diagonal entries to zero. In Fig. 1 we show such embedding plots obtained for SVHN data. Here, we randomly sample 1000 instances from the SVHN training data and each class is represented by a different color.
It could be seen that all the Diagonalized model embeddings (Fig. 1 d,e,f, and g ) show better discrimination than the positive definite embeddings ( Fig. 1 a). The best discrimination could be seen with the experiment setups shown in Fig. 1 (d) and (g).
6.1.2 MODEL SPECTRUM ANALYSIS
Our primary concerns in learning Krein spaces against Hilbert space embeddings was the consideration given to the model spectrum. In this section we compared the spectrum of the distance model parameters (i.e., for Pos. Def. solution eigen values of LLT in equation 3, and for the proposed solution the diagonal entries in equation 5). We illustrate our observations in Fig. 2 for the our final models trained on SVHN data.
From Fig. 2 it could be observed that the eigen values are entirely positive for the Pos. Def. solution. This is intuitive as the learned embedding distances are positive semi-definite. All our Diagonalized model solutions have preserved negative diagonal entries. It is interesting to notice that, the magnitude of the negative diagonal entries are in fact comparable with the positive diagonal entries. More importantly, they are higher in magnitude than the lower eigen values in the Pos. Def. solution. We
7

Under review as a conference paper at ICLR 2019
Figure 1: Embedding Visualization for SVHN. We compare the embeddings for the evaluated solutions through Multidimensional Scaling. We refer to our diagonalizer Krein space realization as diag. We use the notation 2-Space to refer to the solution in section 4. (should be viewed in color). think this expresses an important message that the negative model spectrum in fact contain useful information rather than simply being noise.
7 CONCLUSIONS
We claimed that learning embeddings on Hilbert spaces could tamper useful information contained in the data. In this paper we proposed a solution that can learn deep Krein embeddings. Our proposal is presented as an innocent-looking modification to the Siamese network architecture (i.e., when the embedding space dimension is p we only require additional p number of parameters from the Siamese model). We show that our proposed solution is capable of deciding positive/negative spectral properties of the model in a data driven fashion. We further elaborated that our solution could in fact outperform models learning Krein spaces but with prior assumptions (i.e., the negative space should be equally rich as the positive embedding space.). Our empirical evaluations on recognition tasks using CNNs show that Krein embeddings consistently outperform the Hilbert space embeddings learned with positive semi-definite distances.
Figure 2: Model spectrum comparison for the trained models on SVHN. 8

Under review as a conference paper at ICLR 2019
REFERENCES
T Ya Azizov and IS Iokhvidov. Linear Operators in Spaces with Indefinite Metric and Their Applications. Journal of Soviet Mathematics, 15(4):438­490, 1981.
Ja´nos Bogna´r. The Geometry of Krein Spaces, pp. 100­119. Springer Berlin Heidelberg, Berlin, Heidelberg, 1974.
Gaelle Bonnet-Loosli, Ste´phane Canu, and Cheng Soon Ong. Learning svm in krein spaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(6):1204­1216, 2016.
Jianhui Chen and Jieping Ye. Training SVM with Indefinite Kernels. In Proc. Int. Conference on Machine Learning (ICML), pp. 136­143. ACM, 2008.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a Similarity Metric Discriminatively, with Application to Face Verification. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 539­546. IEEE, 2005.
Hal Daume´ III. From Zero to Reproducing Kernel Hilbert Spaces in Twelve Pages or Less, 2004.
Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S Dhillon. Information-theoretic metric learning. In Proc. Int. Conference on Machine Learning (ICML), pp. 209­216. ACM, 2007.
Shai Fine and Katya Scheinberg. Efficient svm training using low-rank kernel representations. Journal of Machine Learning Research, 2(Dec):243­264, 2001.
Jacob Goldberger, Geoffrey E Hinton, Sam T Roweis, and Ruslan R Salakhutdinov. Neighbourhood Components Analysis. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 513­520, 2005.
Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Larlus. End-to-End Learning of Deep Visual Representations for Image Retrieval. Int. Journal of Computer Vision, 124(2):237­254, 2017.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scho¨lkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723­773, 2012.
Mehrtash Harandi, Mathieu Salzmann, and Richard Hartley. Joint Dimensionality Reduction and Metric Learning: A Geometric Take. In Proc. Int. Conference on Machine Learning (ICML), pp. 1404­1413, 2017.
Yen-Chang Hsu and Zsolt Kira. Neural Network-Based Clustering using Pairwise Constraints. arXiv preprint arXiv:1511.06321, 2015.
Gregory Koch. Siamese Neural Networks for One-Shot Image Recognition. In In ICML Deep Learning Workshop, 2015.
Martin Koestinger, Martin Hirzer, Paul Wohlhart, Peter M Roth, and Horst Bischof. Large Scale Metric Learning from Equivalence Constraints. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2288­2295. IEEE, 2012.
Julian Laub and Klaus-Robert MA~ zller. Feature Discovery in Non-Metric Pairwise Data. Journal of Machine Learning Research, 5(Jul):801­818, 2004.
Gae¨lle Loosli and Stephane Canu. Non Positive SVM. In Proc. 4th Int. Workshop Optimization Mach. Learn, 2010.
Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified Deep Supervised Domain Adaptation and Generalization. In Proc. Int. Conference on Computer Vision (ICCV), pp. 5716­5726. IEEE, 2017.
Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep Metric Learning via Lifted Structured Feature Embedding. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4004­4012, 2016.
9

Under review as a conference paper at ICLR 2019
Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199­210, 2011.
Walter J Scheirer, Michael J Wilber, Michael Eckmann, and Terrance E Boult. Good Recognition is Non-Metric. Pattern Recognition, 47(8):2721­2731, 2014.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A Unified Embedding for Face Recognition and Clustering. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 815­823, 2015.
Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy. Deep Metric Learning via Facility Location. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2206­2214. IEEE, 2017.
J Springenberg, Alexey Dosovitskiy, Thomas Brox, and M Riedmiller. Striving for Simplicity: The All Convolutional Net. In ICLR (workshop track), 2015.
Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, Joa~o Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, and Christopher J Pal. Deep Complex Networks. arXiv preprint arXiv:1705.09792, 2017.
Rahul Rama Varior, Mrinal Haloi, and Gang Wang. Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification. In Proc. European Conference on Computer Vision (ECCV), pp. 791­808. Springer, 2016.
Kilian Q Weinberger and Lawrence K Saul. Distance Metric Learning for Large Margin Nearest Neighbor Classification. Journal of Machine Learning Research, 10(Feb):207­244, 2009.
10

Under review as a conference paper at ICLR 2019
8 APPENDIX
8.1 FURTHER ANALYSIS In this section we provide further experimental analysis on our indefinite distance learning proposal. For these experiments we select the SVHN dataset and the CNN-Small model. 8.1.1 ANALYSIS ON EMBEDDING DIMENSION In our reported experiments in the results section we use a fixed 32 dimensional embedding space for the SVHN data. In this analysis we change the embedding dimension (32, 64, 256, 512) and report the performance of the positive definite distance learning (i.e., Pos. Def.), 2-Space solution and the proposed Diagonalized solution (i.e., diag). The notations (-1, +1) or (+1, +1) denotes the margin used for the used pair-wise distance learning. It is important to mention that for this experiment we do not use the proposed model initialization in section 3. This is for the reason that, when embedding dimensions are higher than the feature dimension (i.e., 84) our proposed initialization is not applicable. We report our observations from this analysis in Fig. 3. It could be seen that, the improvement brought by our proposed diagonalization solution increases with the embedding space dimension. Furthermore, both the indefinite distance learning solutions with the (-1, +1) margin performs significantly better. However, it is worth mentioning that our reported results in Table ?? are better than the accuracy observed in this experiment. This is because we do not use the proposed model initialization with closed form Mahalanobis distances. Note, in both the sets of experiments we initialize the feature extractor weights with the pretrained model weights on cross-entropy loss. 8.2 ARCHITECTURE OF CNN-SMALL Table 2 provides the architectural details of the Small-CNN used for experiments performed on SVHN, CIFAR10 and CIFAR100 datasets. Conv(h × h, c) denotes a convolutional layer with kernel of size h×h and c represents the number of output channels. MaxPool(h, h) denotes the maxpooling operation with kernel and stride set to h × h. Flatten() is used to vectorize its input. FC(n, p) denotes a fully connected of size n × p.
Figure 3: Embedding dimensionality analysis for SVHN. We compare the recognition accuracy of our evaluated methods by changing the embedding dimension (should be viewed in color).
11

Under review as a conference paper at ICLR 2019

Table 2: Architecture Details of the CNN-Small.

Dataset

SVHN / CIFAR10 /CIFAR100

Input

32 × 32 × 3 RGB image

Layer1

Conv(5 × 5, 6) ReLU MaxPool(2, 2)

Layer2 Conv(5 × 5, 16) ReLU MaxPool(2, 2) Flatten()

Layer3

FC(16 × 5 × 5, 120)

Layer4

FC(120, 84)

12

