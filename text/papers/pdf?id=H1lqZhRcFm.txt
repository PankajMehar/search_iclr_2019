Under review as a conference paper at ICLR 2019
UNSUPERVISED LEARNING OF THE SET OF LOCAL MAXIMA
Anonymous authors Paper under double-blind review
ABSTRACT
This paper describes a new form of unsupervised learning, whose input is a set of unlabeled points that are assumed to be local maxima of an unknown value function in an unknown subset of the vector space. Two functions are learned: (i) a set indicator c, which is a binary classifier, and (ii) a comparator function h that given two nearby samples, predicts which sample has the higher value. Loss terms are used to ensure that all training samples x are a local maxima, according to h and satisfy c(x) = 1. Therefore, c and h provide training signals to each other: a point x in the vicinity of x satisfies c(x) = -1 or is deemed by h to be lower in value than x. We present an algorithm, show an example where it is more efficient to use local maxima as an indicator function than to employ conventional classification, and derive a suitable generalization bound. Our experiments show that the method is able to outperform one-class classification algorithms in the task of anomaly detection and also provide an additional signal that is extracted in a completely unsupervised way.
1 INTRODUCTION
...from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved. (Darwin, 1859)
When we observe the natural world, we see the "most wonderful" forms. We do not observe the even larger quantity of less spectacular forms and we cannot see those forms that are incompatible with existence. In other words, each sample we observe is the result of optimizing some fitness or value function under a set of constraints: the alternative, lower-value, samples are removed and the samples that do not satisfy the constraints are also missing. The same principle also holds at the sub-cellular level. For example, a gene can have many forms. Some of them are completely synonymous, while others are viable alternatives. The gene forms that become most frequent are those which are not only viable, but which also minimize the energetic cost of their expression (Farkas et al., 2018). For example, the genes that encode proteins comprised of amino acids of higher availability or that require lower expression levels to achieve the same outcome have an advantage. The same idea, of mixing constraints with optimality, also holds for man-made objects. Consider, for example, the set of houses in a given neighborhood. Each architect optimizes the final built form to cope with various aspects, such as the maximal residential floor area, site accessibility, parking considerations, the energy efficiency of the built product, etc. What architects find most challenging, is that this optimization process needs to correspond to a comprehensive set of state and city regulations that regard, for example, the proximity of the built mass of the house to the lot's boundaries, or the compliance of the egress sizes with current fire codes. In another instance, consider the weights of multiple networks trained to minimize the same loss on the same training data, each using a different random initialization. By the nature of the problem, the obtained weights are the local optimum of some loss optimization process. In addition, the weights are sometimes subject to constraints, e.g., by using weight normalization.
1

Under review as a conference paper at ICLR 2019
The task tackled in this paper is learning the value function and the constraints, by observing only the local maxima of the value function among points that satisfy the constraints. This is an unsupervised problem: no labels are given in addition to the samples.
Let S be the set of such samples from a space X. Every x  S satisfies c(x) = 1 for a classifier c : X  {±1} that models the adherence to the set of constraints (satisfies or not). Alternatively, we can think of c as a class membership function that specifies, if a given input is within the class or not. In addition, we also consider a value function v, and for every point x , such that x - x  , for a sufficiently small > 0, we have: v(x ) < v(x).
This structure leads to a co-training of v and c, such that every point x in the vicinity of x can be used either to apply the constraint v(x ) < v(x) on v, or as a negative training sample for c. Which constraint to apply, depends on the other function: if c(x ) = 1, then the first constraint applies; if v(x )  v(x), then x is a negative sample for c. Thus, the two functions provide a training signal to each other, in an unsupervised setting, somewhat similar to the way adversarial training is done in GANs (Goodfellow et al., 2014), although the situation between c and h is not adversarial. Instead, both work collaboratively to minimize similar loss functions.
2 RELATED WORK
The input to our method is a set of unlabeled points. The goal is to model this set. This form of input is shared with the family of methods called one-class classification (Moya et al., 1993). The main application of these methods is anomaly detection, i.e., identifying an outlier, given a set of mostly normal (the opposite of abnormal) samples (Chandola et al., 2009).
The literature on one class classification can be roughly divided into three parts. The first includes the classical methods, mostly kernel-base methods, which were applying regularization in order to model the in-class samples in a tight way (Scho¨lkopf et al., 2001). The second group of methods, which follow the advent of neural representation learning, employ classical one-class methods to representations that are learned in an unsupervised way (Hawkins et al., 2002; Sakurada & Yairi, 2014; Xia et al., 2015; Xu et al., 2015; Erfani et al., 2016), e.g., by using autoencoders. Lastly, a few methods have attempted to apply a suitable one-class loss, in order to learn a neural network-based representation from scratch (Ruff et al., 2018).
Despite having the same structure of the input (an unlabeled training set), our method stands out of the one-class classification and anomaly detection methods we are aware of, by optimizing a specific model that disentangles two aspects of the data: one aspect is captured by a class membership function, similar to many one-class approaches; the other aspect compares pairs of samples. This dual modeling captures the notion that the samples are not nearly random samples from some class, but also the local optimum in this class. While "the local optima of in-class points" is a class by itself, a classifier-based modeling of this class would require a higher complexity than a model that relies on the structure of the class as pertaining to local maxima, as is proved, for one example, in Sec. 4. In addition to the characterization as local maxima, the factorization between the constraints and the values also assists modeling. This is reminiscent of many other cases in machine learning, where a divide and conquer approach reduces complexity. For example, using prior knowledge on the structure of the problem, helps to reduce the complexity in hierarchical models, such as LDA (Blei et al., 2003).
While we use the term "value function", and this function is learned, we do not operate in a reinforcement learning setting, where the term value is often used. Specifically, our problem is not inverse reinforcement learning (Ng & Russell, 2000) and we do not have actions, rewards, or policies.
3 METHOD
Recall that S is the set of unlabeled training samples, and that we seek two functions c and v such that for all x  S it holds that: (i) c(x) = 1, and (ii) x is a local maxima of v.
For every monotonic function f , the setting we define cannot distinguish between v, and f  v. This ambiguity is eliminated, if we replace v by a binary function h that satisfies h(x, x ) = 1 if v(x)  v(x ) and h(x, x ) = -1 otherwise. We found that training h in lieu of v is considerably
2

Under review as a conference paper at ICLR 2019

more stable. Note that we do not enforce transitivity, when training h, and, therefore, h can be such that no underlying v exists.

3.1 TRAINING c AND h

When training c, the training samples in S are positive examples. Without additional constraints, the recovery of c is an ill-posed problem. For example, Ruff et al. (2018) add an additional constraint on
the compactness of the representation space. Here, we rely on the ability to generate hard negative points1. There are two generators Gc and Gh, each dedicated to generating negative training points to either c or h, as described in Sec. 3.2 below.

The two generators are conditioned on a positive point x  S and each generates one negative point per each x: x = Gc(x) and x = Gh(x). The constraints on the negative points are achieved by multiplying two losses: one pushing c(x ) to be negative, and the other pushing h(x , x) to be
negative.

Let

(p,

y)

:=

-

1 2

((y

+

1)

log(p)+(1-y)

log(1-p))

be

the

binary

cross

entropy

loss

for

y



{±1}.

c and h are implemented as neural networks trained to minimize the following losses, respectively:

LC := (c(x), 1) + (c(Gc(x)), -1) (h(Gc(x), x), -1)

xS

xS

LH := (h(x, x), 1) + (c(Gh(x)), -1) · (h(Gh(x), x), -1)

xS

xS

(1) (2)

The first sum in LC ensures that c classifies all positive points as positive. The second sum links the outcome of h and c for points generated by Gc. It is given as a multiplication of two losses. This multiplication encourages c to focus on the cases where h predicts with a higher probability that the point Gc(x) is more valued than x.
The loss LH is mostly similar. It ensures that h has positive values when the two inputs are the same, at least at the training points. In addition, it ensures that for the generated negative points x , h(x , x) is -1, especially when c(x ) is high.
One can alternatively use a symmetric LH , by including an additional term xS (c(Gh(x)), -1) (h(x, Gh(x)), 1). This, in our experiments, leads to very similar
results, and we opt for the slightly simpler version.

3.2 NEGATIVE POINT GENERATION
We train two generators, Gc and Gh, to produce hard negative samples for the training of c and h, respectively. The two generators both receive a point x  S as input, and generate another point in the same space X. They are constructed using an encoder-decoder architecture, see Sec. 3.4 for the exact specifications.
When training Gc, the loss -LC is minimized. In other words, Gc finds, in an adversarial way, points x , that maximize the error of c (the first term of LC does not involve Gc and does not contribute, when training Gc).
Gh minimizes during training the loss  x ||x - Gh(x)|| - LH , for some parameter . Here, in addition to the adversarial term, we add a term that encourages Gh(x) to be in the vicinity of x. This is added, since the purpose of h is to compare nearby points, allowing for the recovery of points that are local optima. In all our experiments we set  = 1.
The need for two generators, instead of just one, is verified in our ablation analysis, presented in Sec. 5. One may wonder why two are needed. One reason stems from the difference in the training loss: h is learned locally, while c can be applied anywhere. In addition, c and h are challenged by different points, depending on their current state during training. By the structure of the generators, they only produce one point per input x, which is not enough to challenge both c and h.
1"hard negative" is a terminology often used in the object detection and boosting literature, which means negative points that challenge the training process.

3

Under review as a conference paper at ICLR 2019
Algorithm 1 Training c and h Require: S: positive training points; : a trade-off parameter; T : number of epochs. 1: Initialize c, h, Gc and Gh randomly. 2: for i = 1, ..., T do 3: Train Gc for one epoch to minimize -LC 4: Train c for one epoch to minimize LC 5: Train Gh for one epoch to minimize  x ||x - Gh(x)|| - LH 6: Train h for one epoch to minimize LH 7: return c, h
3.3 TRAINING PROCEDURE
The training procedure follows the simple interleaving scheme presented in Alg. 1. We train the networks in turns: Gc and then c, followed by Gh and then h. Since the datasets in our experiments are relatively small, each turn is done using all mini-batches of the training dataset S. The ADAM optimization scheme is used with mini-batches of size 32.
The training procedure has self regularization properties. For example, assuming that Gh(x) = x, LH as a function of h, has a trivial global minima. This solution is to assign h(x , x) to 1 iff x = x. However, for this specific h, the only way for Gh to maximize LH is to rely on c and h being smooth and to select points x = Gh(x) that converge to x, at least for some points in x  S. In this case, both (c(Gh(x)), -1) and (h(Gh(x), x), -1) will become high, since c(x )  1 and h(x , x)  1.
3.4 ARCHITECTURE
In the image experiments (MNIST, CIFAR10 and GTSRB), the networks Gh and Gc employ the DCGAN architecture of Radford et al. (2015). This architecture consists of an encoder-decoder type structure, where both the encoder and the decoder have five blocks. Each encoder (resp. decoder) block consists of a 2-strided convolution (resp. deconvolution) followed by a batch norm layer, and a ReLU activation. The fifth decoder block consists of a 2-strided convolution followed by a tanh activation instead. c and h's architectures consist of four blocks of the same structure as for the encoder. This is followed by a sigmoid activation.
For the Cancer Genome Atlas experiment, each encoder (resp. decoder) block consists of a fully connected (FC) layer, a batch norm layer and a Leaky Relay activation (slope of 0.2). Two blocks are used for the encoder and decoder. The encoder's first FC layer reduces the dimension to 512 and the second to 256. The decoder is built to mirror this. c and h consist of two blocks, where the first FC layer reduces the dimension to 512 and the second to 1. This is followed by a sigmoid activation.
4 ANALYSIS
We show an example in which modeling using local-maxima-points is an efficient way to model, in comparison to the conventional classification-based approach. We then extend the framework of spectral-norm bounds, which were derived in the context of classification, to the case of unsupervised learning using local maxima.
4.1 MODELING USING arg max v IS BENEFICIAL
While modeling with a classifier c is commonplace, modeling a set S as the local maxima of a function is much less conventional. Next, we will argue that at least in some situations, it may be advantageous. We compare the complexity of a ReLU network W2(W1x + b) modeling a set of m real numbers. Here, W1 and W2 are linear transformations, b is a vector and (x1, . . . , xn) = (max(0, x1), . . . , max(0, xn)) is the ReLU activation function. We show that we can capture these points exactly as the only local maxima of a one hidden layered network with 2m neurons, while a classification network would require 3m neurons to achieve a good enough approximation.
4

Under review as a conference paper at ICLR 2019

Theorem 1. Let S = {xi}im=1  R be any set of points such that xi < xi+1 for all i  {1, . . . , m - 1}. We define cS : R  {±1} to be the function, such that cS(x) = 1 if and only if x  S. Then,
1. There is a ReLU neural network v : R  R of the form v(x) = W2(W1x + b) with 2m hidden neurons such that the set of local maximum points of v is S.
2. Let D = q · D0  (1 - q) · D1 be a distribution that samples at probability q from D0 and probability 1 - q from D1, where D0 is any distribution supported by S and D1 is any distribution supported by the segment [x1 - 1, xm + 1]. Then, for a small enough > 0, every ReLU neural network c : R  R of the form c(x) = W2(W1x + b), such that
ExD11[c(x) = cS(x)]  has at least 3m hidden neurons.
The proof can be found in Appendix A.

4.2 GENERALIZATION BOUND

The following lemma provides a generalization bound that expresses the generalization of learning
c along with h. See Appendix B for the exact formulation and the proof.
Lemma 1 (Informal). Let V = {v : Rd  R |   } be a class of value functions and C = {sign f : Rd  {-1, 1} |   } a class of classifiers. Assume that v and f are ReLU neural networks of fixed architectures, with parameters  and  (resp.). Let C(g) is the spectral complexity of the neural network g and N (x) := {u  Rd | u - x 2  } an -neighborhood of x. Let D be a distribution of positive examples. With probability at least 1 -  over the selection of
the data S = {xi}mi=1 i.i.d Dm, for every v  V and c  C, we have:

Px v(x) = max v(u) and c(x) = 1
uN (x)



1

m
11

m

i=1

v(xi) = max v(u) and c(xi) = 1
uN (xi)

+O



C(v) + C(f) + log

m 



m

(3)

The above lemma shows that the probability of x  D to be a local maxima of v and classified as a positive example by c, is at most the sum of the probability of x  S to be a local maxima
of v and classified as a positive example by c and a penalty term. The penalty in this case is

of the form O

C (v )+C (f )+log(m/) m

, where m is the number of examples in the dataset and

C(v) + C(f) is the sum of the spectral norms of v and f. This suggests a tradeoff between the sum of the spectral complexities of v and f and the ability to generalize. The bound is similar asymptotically to the bounds of Neyshabur et al. (2018) and Bartlett et al. for (multi-class) super-

vised classification. In their bound, the penalty term is of the form O

C

(f

)+log(

m 

)

m

, where

the (multi-class) classifier is of the form c(x) = arg maxi{1,...,t} f (x)i, for a neural network f : Rd  Rt.

Our analysis focused on the value v and not on the comparator h. However, the complexities of the two are expected to be similar, since a value function can be converted to a comparator by employing h(x1, x2) = sign(v(x1) - v(x2)).

5 EXPERIMENTS
Since we share the same form of input with one-class classification, we conduct experiments using one-class classification benchmarks. These experiments both help to understand the power of our model in capturing a given set of samples, as well as study the properties of the two underlying functions c and h.
Following acceptable benchmarks in the field, specifically the experiments done by Ruff et al. (2018), we consider single classes out of multiclass benchmarks, as the basis of one-class problems. For example, in MNIST, the set S is taken to be the set of all training images of a particular

5

Under review as a conference paper at ICLR 2019

Table 1: One class experiments on the MNIST and CIFAR-10 datasets. For MNIST, there is one experiment per digit, where the training samples are the training set of this digit. The reported numbers are the AUC for classifying one-vs-rest, using the test set of this digit vs. the test sets of all other digits. For CIFAR-10, the same experiment is run with a class label, instead of the digits. Reported numbers (in all tables) are averaged over 10 runs with random initializations.

Digit

KDE

AnoGAN

Deep SVDD Our c Our h

(Parzen, 1962) (Schlegl, 2017) (Ruff et al., 2018)

0

97.1 96.6

98.0 99.1 83.5

1

98.9 99.2

99.7 97.2 50.7

2

79.0 85.0

91.7 91.9 67.1

3

86.2 88.7

91.9 94.3 62.4

4

87.9 89.4

94.9 94.2 85.7

5

73.8 88.3

88.5 87.2 73.3

6

87.6 94.7

98.3 98.8 62.8

7

91.4 93.5

94.6 93.9 61.6

8

79.2 84.9

93.9 96.0 45.8

9

88.2 92.4

96.5 96.7 66.8

Airplane Automobile Bird Cat Deer Dog Frog Horse Ship Truck

61.2 64.0 50.1 56.4 66.2 62.4 74.9 62.6 75.1 76.0

67.1 54.1 52.9 54.5 65.1 60.3 58.5 62.5 75.8 66.5

61.7 74.0 48.9 65.9 74.7 64.6 50.8 62.8 53.2 59.1 57.2 51.4 60.9 67.8 55.0 65.7 60.2 58.9 67.7 75.3 60.7 67.3 68.5 58.1 75.9 78.1 66.9 73.1 79.5 70.3

digit. When applying our method, we train h and c on this set. To clarify: there are no negative samples during training.
Post training, we evaluate both c and h on the one class classification task: positive points are now the MNIST test images of the same digit used for training, and negative points are the test images of all other digits. This is repeated ten times, for digits 0­9. In order to evaluate h, which is a binary function, we provide it with two replicas of the test point.
The classification ability is evaluated as the AUC obtained on this classification task. The same experiment was conducted for CIFAR-10 where instead of digits we consider the ten different class labels. The results are reported in Tab. 1, which also states the literature baseline values reported by Ruff et al. (2018). As can be seen, for both CIFAR-10 and MNIST, c strongly captures classmembership, outperforming the baseline results in most cases. h is less correlated with class membership, resulting in much lower AUC values. However, it should not come as a surprise that h does contain such information.
Indeed, the difference in shape (single input vs. two inputs) between c and h makes them different but not independent. c, as a classifier, strongly captures class membership. We can expect h, which compares two samples, to capture relative properties. In addition, h, due to the way negative samples are collected, is expected to model local changes, at a finer resolution than c. Since it is natural to expect that the samples in the training set would provide images that locally maximize some clarity score, among all local perturbations, one can expect quality to be captured by h.
To test this hypothesis, we considered positive points to be test points of the relevant one-class, and negative points to be points with varying degree of Gaussian noise added to them. We then measure using AUC, the ability to distinguish between these two classes.
As can be seen in Fig. 1, h is much better at identifying noisy images than c, for all noise levels. This property is class independent, and in Fig. 2 (Appendix C), we repeat the experiment for all test images (not just from the one class used during training), observing the same phenomenon.
6

Under review as a conference paper at ICLR 2019

CIFAR-10

MNIST

(Airplane)

(Automobile)

(0)

(1)

(Bird)

(Cat)

(2)

(3)

(Deer)

(Dog)

(4)

(5)

(Frog)

(Horse)

(6)

(7)

(Ship)

(Truck)

(8)

(9)

Figure 1: The ability to differentiate between an in-class image and an in-class image with added noise for both c (yellow) and h (blue). The x-axis is the amount of noise (SD of the Gaussian noise). The y-axis is the AUC. As can be seen, for both CIFAR-10 and MNIST, h is much more attuned to the image quality.

Table 2: An ablation analysis on the ten CIFAR classes (shown in order, Airplane to Truck).

1 2 3 4 5 6 7 8 9 10

Baseline c

74.0 74.7 62.8 57.2 67.8 60.2 75.3 68.5 78.1 79.5

Baseline h

48.9 64.6 53.2 51.4 55.0 58.9 60.7 58.1 66.9 70.3

c only

73.0 63.8 59.1 59.6 60.4 60.7 62.8 62.1 77.2 73.3

h only

35.6 51.9 50.1 48.0 48.3 48.0 68.0 54.7 75.6 73.1

c with Gc only 73.4 74.3 61.2 58.8 66.4 59.0 72.7 70.3 77.1 75.1

h with Gc only 63.7 68.3 59.2 56.6 58.8 57.4 60.7 65.5 71.3 74.2

c with Gh only 73.2 71.2 59.6 51.7 65.4 60.9 68.3 68.9 76.7 77.2

h with Gh only 56.0 65.3 55.5 53.2 50.6 58.6 54.8 58.4 65.2 71.8

We employ CIFAR also to perform an ablation analysis comparing the baseline method's c and h with four alternatives: (i) training c without training h, employing only Gc; (ii) training h and Gh without training c nor Gc; (iii) training both h and c but using only the Gc generator for both; and (iv) training both h and c but using only the Gh generator for both. The results, which can be seen in Tab. 2, indicate that the complete method is superior to the variants, since it outperform these in the vast majority of the experiments.
Next, we evaluate our method on data from the German Traffic Sign Recognition (GTSRB) Benchmark of Houben et al. (2013). The dataset contains 43 classes, from which one class (stop signs, class #15) was used by Ruff et al. (2018) to demonstrate one-class classification where the negative class is the class of adversarial samples (presumably based on a classifier trained on all 43 classes). We were not able to obtain these samples by the time of the submission. Instead, We employ the sign data in order to evaluate three other one-class tasks: (i) the conventional task, in which a class is compared to images out of all other 42 classes; (ii) class image vs. noise image, as above, using
7

Under review as a conference paper at ICLR 2019
Table 3: Results obtained on the GTSRB dataset on three one-class tasks. Reported are AUC values in percents. DS denotes Deep-SVDD by Ruff et al. (2018).
Class (i) Multiclass (ii) Noise in-class (iii) Noise all images.
c h DS c h DS c h DS
1 92.6 77.8 86.2 61.1 62.3 61.8 55.1 58.9 44.7 2 78.0 75.4 71.9 75.6 96.3 74.7 71.4 92.3 51.4 3 78.3 79.5 65.8 71.0 95.0 66.1 79.0 98.5 50.0 4 79.7 81.7 63.9 89.1 97.0 66.3 71.0 82.0 53.2 5 79.7 79.3 73.2 90.1 95.6 48.7 72.3 84.5 56.3 6 73.8 66.4 81.8 91.1 85.3 88.1 75.3 75.2 62.0 7 91.0 90.2 73.6 93.0 94.1 84.1 58.1 72.4 55.2 8 82.1 75.4 74.6 93.7 93.9 51.6 71.0 82.1 56.7 9 80.2 84.7 73.4 92.4 93.7 54.3 70.5 81.0 53.8 10 85.8 74.9 79.2 82.0 93.4 88.7 71.0 84.0 57.7 11 81.9 81.7 82.7 93.4 93.9 65.0 78.2 78.4 68.3 12 86.9 84.6 54.3 78.3 92.6 89.8 70.3 89.1 64.5 13 88.1 82.1 60.0 84.0 91.2 74.6 78.2 79.1 60.5 14 93.5 93.7 57.6 82.3 85.4 78.9 76.0 77.4 63.4 15 98.2 93.7 71.9 67.3 81.2 65.0 54.0 64.0 49.2 16 87.6 90.5 71.8 59.0 78.3 90.0 55.3 63.2 55.6 17 92.5 96.8 76.7 73.1 83.4 83.1 58.3 67.2 55.6 18 99.3 85.4 64.4 73.0 92.1 77.7 87.3 97.2 50.7 19 79.5 79.7 52.2 68.1 81.2 90.4 62.0 78.3 57.8 20 92.9 92.9 52.1 76.3 78.2 81.6 52.3 63.0 74.0
Avg 86.1 83.3 69.4 79.7 88.2 74.0 68.3 78.4 57.0
Gaussian noise with a fixed noise level of  = 0.2; (iii) same as (ii) only that after training on one class, we evaluate on images from all classes.
The results are presented, for the first 20 classes of GTSRB, in Tab. 3. The reported results are an average over 10 random runs. On the conventional one-class task (i), both our c and h networks outperform the baseline Deep-SVDD method, with c performing better than h, as in the MNIST and CIFAR experiments. Also following the same pattern as before, the results indicate that h captures image noise better than both c and Deep-SVDD, for both the test images of the training class and the test images from all 43 classes.
In order to explore the possibility of using the method out of the context of one-class experiments and for scientific data analysis, we downloaded samples from the Cancer Genome Atlas (https: //cancergenome.nih.gov/). The data contains mRNA expression levels for over 22,000 genes, measured from the blood of 9,492 cancer patients. For most of the patients, there is also survival data in days. We split the data to 90% train and 10% test.
We run our method on the entire train data and try to measure whether the functions recovered are correlated with the survival data on the test data. While, as mentioned in Sec. 1, the gene expression optimizes a fitness function, and one can claim that gene expressions that are less fit, indicate an expected shortening in longevity, this argument is speculative. Nevertheless, since survival is the only regression signal we have, we focus on this experiment.
We compare five methods: (i) the h we recover, (ii) the c we recover, (iii) the h we recover, when learning only h and not c, (iv) the c we recover, when learning only c and not h, (v) the first PCA of the expression data, (vi) the classifier of DeepSVDD. The latter is used as baseline due to the shared form of input with our method. However, we do not perform an anomaly detection experiment.
In the simplest experiment, we treat h as a unary function by replicating the single input, as done above. We call this the standard correlation experiment. However, h was trained in order to compare two local points and we, therefore, design the local correlation protocol. First, we identify for each test datapoint, the closest test point. We then measure the difference in the target value (the patient's
8

Under review as a conference paper at ICLR 2019

Table 4: Correlation between the recovered functions and the patient's survival.

Local

Standard

Method

Pearson correlation P-value Pearson correlation P-value

Our h Our c Our h trained without c Our c trained without h First PCA of mRNA expression Deep-SVDD

0.076 0.020 0.033 0.029 0.047 0.021

0.021 0.520 0.405 0.444 0.308 0.510

0.041 0.029 0.017 0.031 0.006 0.032

0.384 0.444 0.716 0.420 0.903 0.410

survival) between the two datapoints, the difference in value for unary functions (e.g., for c or for the first PCA), or h computed for the two datapoints. This way vectors of the length of the number of test data points are obtained. We use the Pearson correlation between these vectors and the associated p-values as the test statistic.
The results are reported in Tab. 4. As can be seen, the standard correlation is low for all methods. However, for local correlation, which is what h is trained to recover, the h obtained when learning both h and c is considerably more correlated than the other options, obtaining a significant p-value of 0.021. Interestingly, the ability to carve out parts of the space with c, when learning h seems significant and learning h without c results in a much reduced correlation.
6 DISCUSSION
The current machine learning literature focuses on models that are smooth almost everywhere. The label of a sample is implicitly assumed as likely to be the same as those of the nearby samples. In contrast to this curve-based world view, we focus on the cusps. This novel world view could be beneficial also in supervised learning, e.g., in the modeling of sparse events.
Our model recovers two functions: c and h, which are different in form. This difference may be further utilized to allow them to play different roles post learning. Consider, e.g., the problem of drug design, in which one is given a library of drugs. The constraint function c can be used, post training, to filter a large collection of molecules, eliminating toxic or unstable ones. The value function h can be used as a local optimization score in order to search locally for a better molecule.
REFERENCES
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks with rectified linear units. In International Conference on Learning Representations, 2018.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In NIPS.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John Lafferty. Latent dirichlet allocation. Journal of Machine Learning Research, 3:2003, 2003.
Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):15, 2009.
Charles Darwin. On the Origin of Species by Means of Natural Selection. Murray, London, 1859. or the Preservation of Favored Races in the Struggle for Life.
Sarah M Erfani, Sutharshan Rajasegarar, Shanika Karunasekera, and Christopher Leckie. Highdimensional and large-scale anomaly detection using a linear one-class svm with deep learning. Pattern Recognition, 58:121­134, 2016.
9

Under review as a conference paper at ICLR 2019
Zolta´n Farkas, Dorottya Kalapis, Zolta´n Bo´di, Be´la Szamecz, Andreea Daraba, Karola Alma´si, Ka´roly Kova´cs, Ga´bor Boross, Ferenc Pa´l, Pe´ter Horva´th, et al. Hsp70-associated chaperones have a critical role in buffering protein production costs. eLife, 7:e29845, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672­2680, 2014.
Simon Hawkins, Hongxing He, Graham Williams, and Rohan Baxter. Outlier detection using replicator neural networks. In International Conference on Data Warehousing and Knowledge Discovery, pp. 170­180. Springer, 2002.
Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark. In International Joint Conference on Neural Networks, number 1288, 2013.
David Mcallester. Simplified pac-bayesian margin bounds. In In COLT, pp. 203­215, 2003.
M. M. Moya, M. W. Koch, and L. D. Hostetler. One-class classifier networks for target recognition applications. NASA STI/Recon Technical Report N, 93, 1993.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In ICLR, 2018.
Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In ICML, pp. 663­670, 2000.
Emanuel Parzen. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 33(3):1065­1076, 09 1962.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Mu¨ller, and Marius Kloft. Deep one-class classification. In ICML, 2018.
Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimensionality reduction. In Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis, pp. 4. ACM, 2014.
Seebock Schlegl. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. IPMI, pp. 146157, 2017.
Bernhard Scho¨lkopf, John C. Platt, John C. Shawe-Taylor, Alex J. Smola, and Robert C. Williamson. Estimating the support of a high-dimensional distribution. Neural Computing, 13(7):1443­1471, 2001.
Y. Xia, X. Cao, F. Wen, G. Hua, and J. Sun. Learning discriminative reconstructions for unsupervised outlier removal. In 2015 IEEE International Conference on Computer Vision (ICCV), 2015.
Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, and Nicu Sebe. Learning deep representations of appearance and motion for anomalous event detection. arXiv preprint arXiv:1510.01553, 2015.
A PROOF OF THM. 1
Theorem 1. Let S = {xi}im=1  R be any set of points such that xi < xi+1 for all i  {1, . . . , m - 1}. We define cS : R  {±1} to be the function, such that cS(x) = 1 if and only if x  S. Then,
1. There is a ReLU neural network v : R  R of the form v(x) = W2(W1x + b) with 2m hidden neurons such that the set of local maximum points of v is S.
10

Under review as a conference paper at ICLR 2019

2. Let D = q · D0  (1 - q) · D1 be a distribution that samples at probability q from D0
and probability 1 - q from D1, where D0 is any distribution supported by S and D1 is any
distribution supported by the segment [x1 - 1, xm + 1]. Then, for a small enough > 0,
every ReLU neural network c : R  R of the form c(x) = W2(W1x + b), such that
ExD11[c(x) = cS(x)]  has at least 3m hidden neurons.

Proof. We begin by proving (1). We construct v as follows:

· x  (-, x1]: v(x) = x - x1 + 1.

·

i



{1,

.

.

.

,

m

-

1}

:

x



[xi,

xi +xi+1 2

]:

v(x)

=

-2 xi+1 -xi

(x

-

xi)

+

1.

·

i



{1,

.

.

.

,

m

-

1}

:

x



[

xi +xi+1 2

,

xi+1]:

v(x)

=

1 xi+1 -xi

(x

-

xi+1)

+

1.

· x  [xm, ): v(x) = xm - x + 1.

we consider that v is a piece-wise linear function with 2m linear pieces and arg max v = S. By Thm. 2.2 in Arora et al. (2018), this function can be represented as a ReLU network of the form v(x) = W2(W1x + b), that has 2m hidden neurons.

Next, we prove (2). We denote x0 = x1 - 1 and xm+1 = xm + 1. Let PD0 [xi] be the probability of sampling xi from D0. Since D0 is supported by S, we have: q · PD0 [xi] > 0. We define
 := q mini{1,...,m} PD0 [xi]. In addition, D1 is a continuous distribution supported by the closed

segment [x1 - 1, xm + 1]. Thus, by Weierestrass' extreme value theorem, the probability density

function PD1 [x] of D1 that is a continuous function, obtains its extreme values within the segment.

In 1,

addition, xm + 1].

since D1 is supported by [x1 - 1, xm + 1], By combining the above two statements, we

we have: conclude

PD1 [x] > 0 that there is a

for all point

x x

 

[x1 [x1

- -

1, xm + 1] such that PD1 [x]  PD1 [x] > 0 for every x  [x1 - 1, xm + 1]. We denote by

 := (1 - q) mini{0,...,m} PD1 [x  (xi, xi+1)] > 0. Since we are interested in proving the claim

for a small enough > 0, we can simply assume that < min(, ) and c : R  R a ReLU neural

network of the form c(x) = W2(W1x + b).

We have:

ExD11[c(x) = cS(x)] = qExD0 11[c(x) = cS(x)] + (1 - q)ExD1 11[c(x) = cS(x)]
m
 qExD0 11[c(x) = cS(x)]   11[c(x) = cS(x)]
i=1

(4)

Assume by contradiction that: c(xi) = cS(xi). Then, ExD11[c(x) = cS(x)]   > in contradic-
tion. Therefore, c(xi) = cS(xi) = 1 for every xi  S.
We also have:

ExD11[c(x) = cS(x)] = qExD0 11[c(x) = cS(x)] + (1 - q)ExD1 11[c(x) = cS(x)]  (1 - q)ExD1 11[c(x) = cS(x)]

(5)

Assume by contradiction that there is i  {1, . . . , m - 1}, such that the set Ei = {x  (xi, xi+1)|c(x) = 0} is finite. Then,

ExD11[c(x) = cS(x)]  (1 - q)ExD1 11[c(x) = cS(x)]
 (1 - q)PD1 [x  (xi, xi+1)]   >

(6)

in contradiction. Let i  {1, . . . , m - 1}, ai and bi be two points such that xi < ai < bi < xi+1 and c(ai) = c(bi) = 0. Since c is a continuous function and piece-wise linear and the four points (xi, 1), (a, 0), (b, 0), (xi+1, 1) are not co-linear, we conclude that c has at least three linear pieces in the segment [xi, xi+1]. Similarly, c has at least two linear pieces in each of the segments [x1 - 1, x1] and [xm, xm + 1]. We conclude that c has at least 3m + 1 pieces. By Thm. 2.2 in Arora et al. (2018), c has at least 3m hidden neurons.

11

Under review as a conference paper at ICLR 2019

B GENERALIZATION BOUND

In this section, we build upon the theory presented by Neyshabur et al. (2018) and provide a generalization bound that expresses the guarantees of learning c, along with v for a specific setting.

Before we introduce the generalization bound, we introduce the necessary terminology and setup. We assume that the sampling space is a ball of radius B, i.e., X = XB,d := {x  Rd | ||x||2  B}. Each value function v  V is a ReLU neural network of the form v(x) = Wr(Wr-1(. . . (W1x)), where, Wi  Rdi×di+1 for i  {1, . . . , r} such that dr+1 := 1 and d1 := d. In addition, (x) = (max(0, x1), . . . , max(0, xn)) is the ReLU activation function extended to all n  N and x  Rn. We denote,  = (W1, . . . , Wr). The set C consists of classifiers c := sign f such that each function f : Rd  R is a ReLU neural network of the form f(x) = Us(Us-1(. . . (U1x)), where, Ui  Rdi×di+1 for i  {1, . . . , s} such that es+1 := 1 and d1 := d. We denote  = (U1, . . . , Us). Additionally, we denote, q1 := max{di}ri=+11 and q2 := max{di}si=+11.
The spectral complexity of a ReLU neural network g = Vk(Vk-1(. . . (V1x)) with parameters  = (V1, . . . , Vk) is defined as follows:

C (g )

:=

C ( )

:=

k i=1

||Wi||22

k i=1

||Wi||F2 ||Wi||22

(7)

For two distributions P and Q over a set X, we denote the KL-divergence between them by, DKL(Q||P ) := ExQ[log(Q(x)/P (x))]. For two functions function f, g : R  R, we denote the asymptotic symbols: g(x) = O(f (x)) to specify that g(x)  c · f (x), for some constant c > 0.
We denote by 11[x] the indicator, if a boolean x  {true, false} is true or false.
We define a margin loss 1,2 : X ×  ×   R of the form:

1,2 (x; , ) := 11

v(x)  max v(u) - 1 and sign(f(x) - 2) = 1
uN (x)

(8)

where, 1, 2 > 0 are fixed margins and N (x) := {u | ||u - x||2  } is the -neighborhood of x, for a fixed > 0. In this model, the margins serve as parameters that dictate the amount of tolerance in classifying an example as positive. Similar to the standard learning framework, for a fixed distribution D over X, the goal of a learning procedure is to return (given some input)  and  that minimize the following generalization risk function:

FD[, ] := ExD[ 0,0(x; , )]

(9)

The learning process has no direct access to the distribution D. Instead, it is provided with a set
of m i.i.d samples from D, S = {xi}im=1 i.i.d Dm. In order to estimate the generalization risk, the empirical risk function is used during training:

F^1,2 [, ] := 1 m Sm

1,2 (xi; , )

i=1

(10)

The following lemma provides a generalization bound that expresses the generalization of learning c along with h.

Lemma 2. Let X := XB,n, V and C be as above. Let D be a distribution of positive examples. With probability at least 1 -  over the selection of the data S = {xi}mi=1 i.i.d Dm, for every v  V and c  C, we have:



FD [,

]



F^ 1 ,2
S

[,

]

+

O

 





B2

r2q1

log(rq1

)

C (v 12

)

+

s2q2

log(sq2

)

C

(f 22

)

+ log

m 





m

(11)

12

Under review as a conference paper at ICLR 2019

B.1 PROOF OF LEM. 2

All over the proofs, we will make use of two generic classes of functions G = {g : X  R2 |   } and H = {h : X  R2 |   }. For simplicity, we denote the indices of g(x) and h(x) by -1 and 1 (instead of 1 and 2). Given a target function y : X  {±1} and two functions g : X  R2 and h : X  R2, we denote the loss of them with respect to a sample x by:

e1,2 (x; , ) :=11 g(x)[-y(xi)] - 1  g(x)[y(xi)]  11 h(x)[-y(xi)] - 2  h(x)[y(xi)]

(12)

The generalization risk: And the empirical risk:

L1,2 [, ] := ExD[e1,2 (x; , )]

L^1,2 [, ] := 1 m

m

e1,2 (xi; , )

i=1

(13) (14)

We modify the proof of Lem. 1 in Neyshabur et al. (2018), such that it will fit our purposes.

Lemma 3. Let y : X  {±1} be a target function. Let G = {g : X  R2 |   } and

H = {h : X  R2 |   } be two classes class of functions (not necessarily neural networks).

Let P1 and P2 be any two distributions on the parameters  and  (resp.) that are independent of the

training data. Then, for any 1, 2,  > 0, with probability  1- over the training set of size m, for

any two posterior distributions q and q over  and  (resp.), such that P , [|g (x)-g(x)| 

1 4

and

|h

(x) - h(x)|



2 4

]



1 2

,

we

have:

L0,0[, ]  L^1,2 [, ] + 4

DKL(q

||P1)

+

DKL

(q

||P2)

+

log(

6m 

)

m-1

(15)

Proof. Let S,1,2   ×  be a set with the following properties:

S,1,2 =

( ,  )   × 

x



X

:

|g

(x) - g(x)|

<

1 4

and

|h

(x) - h(x)|

<

2 4

(16)

We construct a distribution Q~ over  × , with probability density function:

1 q~( ,  ) =
Z

q( ) · q( ) if ( ,  )  S,1,2 0 otherwise

(17)

Here,

Z

is

a

normalizing

constant.

By

the

assumption

in

the

lemma,

Z

=

P[(

,

)



S,1,2 ]



1 2

.

By the definition of Q~, we have:

max |g
xX

(x) -

g (x)|

<

1 4

and

max |h
xX

(x) -

h (x)|

<

2 4

(18)

Therefore,

max
xX

|g (x)[-1] - g (x)[1]| - |g(x)[-1] - g(x)[1]|

< 1 2

and also,

max
xX

|h (x)[-1] - h (x)[1]| - |h(x)[-1] - h(x)[1]|

<

2 2

Since this equation holds uniformly for all x  X, we have:

(19) (20)

L0,0[, ]



L 1 2

,

2 2

[

,

]

L^ 1 2

,

2 2

[

,



]



L^1,2 [, ]

(21)

13

Under review as a conference paper at ICLR 2019

Now using the above inequalities together with Eq. 6 in Mcallester (2003), with probability 1 -  over the training set we have:

L0,0(, )



E

,

L 1 2

,

2 2

[

,

]



E

,

L^ 1 2

,

2 2

[

,

]

+

2

2(DKL(q~||P1

×

P2)

+

log(

2m 

))

m-1

 L^1,2 [, ] + 2

2(DKL(q~||P1

×

P2)

+

log(

2m 

))

m-1

(22)

 L^1,2 [, ] + 4

DKL(q

×

q ||P1

×

P2)

+

log(

6m 

)

m-1

where the last inequality follows from the following observation.

Let Sc denote the complement set of S,1,2 and q~c denote the density function q := q ×q restricted to Sc and normalized. In addition, we denote p := P1 × P2. Then,

DKL(q||p) = ZDKL(q~||p) + (1 - Z)DKL(q~c||p) - H(Z)

(23)

where H(Z) = -Z log Z - (1 - Z) log(1 - Z)  1 is the binary entropy function. Since the KL-divergence is always positive, we get,

DKL(q~||p)

=

1 Z [DKL(q||p)

+

H (Z )

-

(1

-

Z )DKL (q~c ||p)]



2(DKL(q||p)

+

1)

(24)

Since P1 × P2 are q × q are independent joint distributions, we have: DKL(q × q||P1 × P2) = DKL(q||P1) + DKL(q||P2).

Lemma 4. Let V = {v : X  [0, 1] |   } be a class of value functions v(x)  [0, 1] and C = {c = sign f | f : X  R,   } a class of classifiers (not necessarily neural networks). We define two classes of functions G = {g = (maxuN (x) v(u), v(x)) |   } and
H = {h = (0, f(x)) |   }. Then,

P ,

max
xX

|g

(x) -

g (x)|

<

1 4

and

max
xX

|h

(x) -

h (x)|

<

2 4

 P

max
xX

|v

(x)

-

v (x)|

<

1 4

· P

max
xX

|f

(x) -

f (x)|

<

2 4

(25)

where,   q and   q.

Proof.

1 4

and

We would like to prove |h (x) - h(x)| 

t4h2aitfm|faxx(xX)

|g (x)- - f(x)|

g 

(x)| 

2 4

.

Since

1 4


if maxxX |v (x)-v(x)|  and  are independent, it will

prove the desired inequality.

First, we consider that:

|g (x) - g(x)| = max  max

max v (x) - max v(x) , |v (x) - v(x)|

uN (x)

uN (x)

max v
uN (x)

(x)

-

max v(x)
uN (x)

,

1 4

(26)

With no loss of generality, we assume that maxuN (x) v (x)  maxuN (x) v(x) and denote x = arg maxuN (x) v (x). Therefore, we have:

max v (x) - max v(x) = max v (x) - max v(x)

uN (x)

uN (x)

uN (x)

uN (x)

= v (x) - max v(x)
uN (x)



v

(x) - v(x)



1 4

(27)

14

Under review as a conference paper at ICLR 2019

Next, we consider that:

|h (x) - h(x)| = max

|0 - 0|, |f (x) - f(x)|

 2 4

(28)

Lemma 5. Let V = {v : X  [0, 1] |   } be a class of value functions v(x)  [0, 1]

and C = {c = sign f | f : X  R,   } a class of classifiers (not necessarily neural

networks). Let P1 and P2 be any two distributions over the parameters  and  (resp.) that are

independent of the training data. Then, for any 1, 2,  > 0, with probability  1 -  over the

training set of size m, for any two posterior distributions q and q over  and  (resp.), such that

P

q [|v

(x)

-

v (x)|



1 4

]



1 2

and

P

q [|f

(x)

-

f (x)|



2 4

]



1 ,
2

we

have:

ExD11 max v(u)  v(x) and sign(f(x)) = 1 uN (x)

1 m

m

11

max v(u) - 1  v(xi) and sign(f(xi) - 2) = 1
uN (x)

i=1

+4

DKL(q

||P1)

+

DKL(q

||P2)

+

log(

6m 

)

m-1

(29)

Proof. Let G and H be as in Lem. 4. By Lem. 4 and our assumption,

P ,

max |g
xX

(x)

- g(x)|

<

1 4

and

max |h
xX

(x)

- h(x)|

<

2 4

1 2

(30)

We note that all of the samples in D are positive. Therefore, by Lem. 3, with probability at least 1 - , we have:

ExD11 g(x)[-1]  g(x)[1] and h(x)[-1]  h(x)[1]

1 m

m

11

g(xi)[-1] - 1  g(xi)[1] and h(xi)[-1] - 2  h(xi)[1]

i=1

+4

DKL(q

||P1)

+

DKL

(q

||P2)

+

log(

6m 

)

m-1

(31)

By the definition of g: g(x)[-1] = maxuN (x) v(u), g(x)[1] = v(x). In addition, by the definition of h: h(x)[-1] = 0 and h(x)[1] = f(x). Therefore, we can rephrase Eq. 31 as follows:

ExD11 max v(u)  v(x) and sign(f(x)) = 1 uN (x)

1 m

m

11

max v(u) - 1  v(xi) and sign(f(xi) - 2) = 1
uN (x)

i=1

(32)

+4

DKL(q

||P1)

+

DKL(q

||P2)

+

log(

6m 

)

m-1

Proof of Lem. 2. We apply Lem. 5 with priors P1, P2 and posteriors q, q, distributions simi-

lar the proof of Thm. 1 in (Neyshabur et al., 2018). In their proof, they show that for their

selection of prior and posterior distributions:

(1) P q

maxxX |v(x)

-

v (x)|

<

1 4



1 2

holds

and

(2)

DKL(q||P )

=

O(dr2B2q log(rq)C()/12).

By taking 1 to be half

of the value the use and therefore,  (from their proof) to be half of the value they use

as

well,

we

obtain

P q

maxxX |v(x) - v(x)|

<

1 /2 4



1 2

and

DKL(q||P )

=

15

Under review as a conference paper at ICLR 2019

O(dr2B2q log(rq)C()/12). We select P2 and q in a similar fashion. In particular, we can replace the penalty term in Lem. 5 as follows:

4

DKL(q||P1) + DKL(q||P2) + log

6m 

m-1



O 

B2(r2q1 log(rq1)C(v)/12 + s2q2 log(sq2)C(f)/22) + log

m 



m

(33)

C MORE FIGURES
CIFAR-10

MNIST

(Airplane)

(Automobile)

(0)

(1)

(Bird)

(Cat)

(2)

(3)

(Deer)

(Dog)

(4)

(5)

(Frog)

(Horse)

(6)

(7)

(Ship)

(Truck)

(8)

(9)

Figure 2: Same as Fig. 1, but where the images are taken from the test set of all classes, regardless of the single class used for training.

16

