Under review as a conference paper at ICLR 2019

TRAINING GENERATIVE LATENT MODELS BY VARIATIONAL f -DIVERGENCE MINIMIZATION
Anonymous authors Paper under double-blind review

ABSTRACT
Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific form of f -divergence between the model and data distribution. We derive an upper bound that holds for all f -divergences, showing the intuitive result that the divergence between two joint distributions is at least as great as the divergence between their corresponding marginals. Additionally, the f -divergence is not formally defined when two distributions have different supports. We thus propose a noisy version of f -divergence which is well defined in such situations. We demonstrate how the bound and the new version of f -divergence can be readily used to train complex probabilistic generative models of data and that the fitted model can depend significantly on the particular divergence used.

1 INTRODUCTION
Probabilistic modelling generally deals with the task of trying to fit a model p(x) parameterized by  to a given distribution p(x). To fit the model we often wish to minimize some measure of difference between p(x) and p(x). A popular choice is the class of f -divergences1 (see for example Sason & Verdú (2015)) which, for two distributions p(x) and q(x), is defined by

p(x)

Df (p(x)||q(x)) =

q(x)f

dx

q(x)

(1)

where f (x) is a convex function with f (1) = 0.

Many of the standard divergences correspond to simple choices of the function f , see table 1. For example, for f (u) = u log u we have the "forward" KL divergence KL(p(x)||q(x)); setting f (u) = - log u gives the "reverse" KL divergence KL(q(x)||p(x)). The divergence Df (p(x)||q(x)) is zero if and only if p(x) = q(x). However, for a constrained model p(x) fitted to a distribution p(x) by minimizing Df (p(x)||p(x)), the resulting optimal  can be heavily dependent on the choice of the divergence function f Minka (2005).

Whilst there is significant recent interest in using f -divergences to train complex probabilistic models Nowozin et al. (2016); Goodfellow et al. (2014), the f -divergence is generally computationally intractable for such complex models. The main contribution of our paper is the introduction of an upper bound on the f -divergence. We show how this bound can be readily applied to training complex latent variable generative models based on only a modest departure from the standard Variational Autoencoder Kingma & Welling (2013).

2 BACKGROUND
2.1 MAXIMUM LIKELIHOOD
For data x1, . . . , xN drawn independently and identically from some unknown distribution, fitting an approximating distribution p(x) by minimizing the forward KL between p(x) and the empirical distribution p^(x) of the data we have
1The definition extends naturally to distributions on discrete x.

1

Under review as a conference paper at ICLR 2019

Name "forward" Kullback-Leibler "reverse" Kullback-Leibler Jensen-Shannon
GAN

Df (p(x)||q(x))

p(x)

log

p(x) q(x)

dx

q(x)

log

q(x) p(x)

dx

1 2

p(x)

log

2p(x) p(x)+q(x)

+

q(x) log

2q(x) p(x)+q(x)

dx

p(x) log

2p(x) p(x)+q(x)

+

q(x) log

2q(x) p(x)+q(x)

dx

f (u)

u log u

- log u

-(u

+

1) log

1+u 2

+

u log

u

-(u + 1) log(1 + u) + u log u

Table 1: Some standard f -divergences. p(x) is given and q(x) is the model. From Nowozin et al. (2016).

p^(x)  1 N

N

 (x - xn)



N
KL(p^(x)||p(x)) = - log p(xn) + const.

n=1

n=1

(2)

Minimizing KL(p^(x)||p(x)) w.r.t.  is therefore equivalent to maximizing the likelihood of the data, p(x). Given the asymptotic guarantees of the efficiency of maximum likelihood Wolfowitz (1965), the forward KL is the standard divergence used in statistics and machine learning.

2.2 FORWARD VERSUS REVERSE KL
It is interesting to compare models trained by the forward and reverse KL divergences. For example, when p is Gaussian with parameters  = µ, 2 , then minimizing the forward KL gives

arg min KL(p(x)||p(x))  µ = p(x)xdx, 2 = p(x)(x - µ)2dx
µ,2

(3)

so that the optimal setting is for µ to be the mean of p(x) and 2 the variance. For an "under-powered"
model (a model which is not rich enough to have a small divergence) p(x) and multi-modal p(x) this could result in p(x) placing significant mass on low probability regions in p(x). This is the so-called "mean matching" behavior of KL(p(x)||p(x)) that has been suggested as a possible explanation for the poor fidelity of images generated by models p(x) trained by forward KL minimization Goodfellow (2016). Conversely, when using the reverse KL objective KL(p(x)||p(x)), for a Gaussian p(x) and multi-modal p(x) with well separated modes, optimally µ and 2 fit one of the local modes. This behavior is illustrated in figure 1 and is the so-called "mode matching" behavior of KL(p(x)||p(x)). For this reason, the reverse KL objective has been suggested to be useful if high quality data samples are preferable to coverage of the dataset Goodfellow (2016).

This highlights the potentially significant difference in the resulting model that is fitted to the data, depending on the choice of divergence Minka (2005). In this sense, it is of interest to explore fitting generative models p(x) to a data distribution p(x) using f -divergences other than the forward KL divergence (maximum likelihood).

2.3 LATENT GENERATIVE MODELS

For the model p(x) to have the power to generate complex datasets, we introduce a latent variable z, i.e. p(x) =
p(x|z)p(z)dz. Following standard practice we do not place any parameters on the prior for the latent z, though this would be straightforward.
A standard approach to fitting a latent generative model to data x1, . . . , xN is maximum likelihood.

Figure 1: Fitting a Gaussian to a mixture of Gaussians by minimizing the forward KL (red) and the reverse KL (blue).

NN

log p(xn) = log

n=1

n=1

p (xn |zn )p(zn )dzn

(4)

2

Under review as a conference paper at ICLR 2019

In all but simple cases the integral above over the latent z is intractable and a lower bound is used instead

NN

log p(xn) 

n=1

n=1

q(zn|xn) [log p(xn|zn)p(zn) - log q(zn|xn)] dzn  L(, )

(5)

where the so-called variational distribution q(z|x) is chosen such that the bound (and its gradient) is either computationally tractable or can be readily estimated by sampling Kingma & Welling (2013). The parameters  of the variational distribution q(z|x) and parameters  of the model p(x) are jointly optimized to increase the log-likelihood lower bound L(, ). This lower bound on the likelihood corresponds to an upper bound on the forward divergence KL(p^(x)||p(x)), where p^(x) is the empirical data distribution.
Our interest is to train latent generative models using a different divergence from the forward KL.
Whilst the above upper bound (5) on the forward divergence KL(p^(x)||p(x)) is well known, an upper bound on other f -divergences seems to be unfamiliar Sason & Verdú (2015) and we are unaware of any upper bound on general f -divergences that has been used within the machine learning community.
Recently a lower bound on the f -divergence was introduced in Nowozin et al. (2016) by the use of the Fenchel conjugate. The resulting training algorithm is a form of minimax in which the parameters  that tighten the bound are adjusted so as to push up the bound towards the true divergence, whilst the model parameters  are adjusted to lower the bound. In Nowozin et al. (2016) the authors were then able to relate the Generative Adversarial Network (GAN) Goodfellow (2016) training algorithm to the Fenchel conjugate lower bound on a corresponding f -divergence, see table 1. In contrast, if the interest is purely on minimizing an f -divergence, it is arguably preferable to have an upper bound on the divergence since then standard optimization methods can be applied, resulting in a stable optimization procedure, see figure 2.

2.4 f -DIVERGENCE BETWEEN DISTRIBUTIONS WITH DISJOINT SUPPORTS
The f -divergence between two distributions, Df (p(x)||p(x)), is well defined when p(x) and p(x) have the same support. Many datasets that we are interested in, e.g. images, are generally believed to lie on low dimension manifolds embedded in high-dimensional space (Narayanan & Mitter (2010)). Moreover, if our model is a mixture of delta functions, then the support of p(x) is contained in a countable union of manifolds of dimension at most dim Z. In this case, the supports of data distribution and model distribution are disjoint, so the f -divergence is not formally defined, we refer readers to Arjovsky & Bottou (2017) for details. To solve the problem, Sønderby et al. (2017) proposed the instance noise trick and applied it to stabilize the training of the discriminator in GANs. Barber et al. (2018) introduced and discussed the relevant properties of a divergence which is well defined on distributions with different supports. We extend this idea to construct a surrogate of f -divergence in section 3.1 and show how to train the new divergence using the auxiliary upper bound.

3 THE f-DIVERGENCE UPPER BOUND

A central contribution of our paper is the following upper bound on Df (p(x)||q(x)) between any two distributions p(x) and q(x)

Df (p(x, z)||q(x, z)) =

q(x, z)f

p(x, z) q(x, z)

dxdz  Df (p(x)||q(x))

(6)

where p(x, z) is a distribution with marginal p(x, z)dz = p(x) and similarly q(x, z)dz = q(x). The bound corresponds to a generalization of the auxiliary variational method Agakov & Barber

3

Under review as a conference paper at ICLR 2019

(2004) and follows from a straightforward application of Jensen's inequality:

Df (p(x, z)||q(x, z)) =

q(x)

q(z|x)f p(x, z) dzdx q(x, z)

 q(x)f

q(z|x)

p(x, z) q(z|x)q(x)

dz

dx

=

q(x)f

p(x) q(x)

dx = Df (p(x)||q(x))

The result states that the divergence between two joint distributions is no less than the divergence
between their marginals. Additional properties of the auxiliary f -divergence are given in section A of the supplementary material. We show that the bound is tight and reduces to Df (p(x)||q(x)) when performing a full unconstrained minimization of the bound with respect to p(z|x) (keeping q(x, z)
fixed).

3.1 SPREAD f-DIVERGENCE

Recently, Barber et al. (2018) proposed the spread f -divergence; we give a brief introduction and show how to apply the spread divergence to f -divergence.
For q(x) and p(x) which have disjoint supports, we define new distributions q(y) and p(y) that have the same support. We let

p(y) = p(y|x)p(x) q(y) = p(y|x)q(x)

(7)

xx

where p(y|x) is a "noise" process designed such that p(y) and q(y) have the same support. For

example, if we use a Gaussian p(y|x) = N y x, 2 , then p(y) and q(y) both have support R.

We thus define the spread f -divergence

Df (q(x)||p(x)) = Df (q(y)||p(y))

(8)

This satisfies the requirements of a divergence, that is Df (q(x)||p(x))  0 and Df (q(x)||p(x)) = 0 if and only if q(x) = p(x).

The auxiliary upper bound can be easily applied to the spread f -divergence Df (p(x)||q(x)) = Df (p(y)||q(y))  Df (p(y, z)||q(y, z))

(9)

3.2 TRAINING LATENT GENERATIVE MODELS

The bound (9) can be directly applied to form an upper bound on f -divergences for training latent

generative models. The key idea is that, in situations in which the divergence Df (p(x)||p(x)) is intractable, the spread divergence of the joint Df (p(y, z)||p(y, z)) may be tractable. We introduce a variational distribution q(z|y) to express the joint p(y, z) = q(z|y)p(y). Our generative model factorizes as p(y, z) = p(y|z)p(z) as before. The bound is then

Df (p(y)||p(y))  Df (q(z|y)p(y)||p(y|z)p(z))  U (, )

(10)

Once the model has been trained we can recover the model on x by inverting the noise process. We will use a Gaussian noise process, p(y|x) = N y x, 2 , and a Gaussian output for the generative

model, p(y|z) = N y µ(z), 2 , with both set to use the same fixed variance. This means we can invert the noise process by simply taking the mean of the model output p(x|z) = (x - µ(z)).

Similar to standard treatments in variational inference (see for example Kingma & Welling (2013); Rezende et al. (2014)), the variational distribution q(z|y) is only being used to tighten the resulting bound and is not a component of the generative model, although it can be used to infer structure in
the latent space.

As an example, the above provides the following upper bound on the reverse KL divergence2

KL(p(y)||p(y))  p(y|z)p(z) [log p(y|z)p(z) - log q(z|y)p(y)] dydz

(11)

2As for the general f -divergence, we write the bound for continuous x but there is a natural analogue for discrete x. One simply replaces integration with summation.

4

Under review as a conference paper at ICLR 2019

Df (p(x)||p(x))

U (, ) Df (p(x)||p(x))

Lf (, )



(a) The Auxiliary f -divergence upper bound.

(b) The Fenchel-conjugate f -divergence lower bound.

Figure 2: Upper and lower bounds on the divergence Df (p(x)||p(x)). In our upper bound, both the model parameters  and bound parameters  are adjusted to push down the upper bound, thereby driving down the divergence. In the Fenchel-conjugate approach Nowozin et al. (2016), the lower bound is made tighter adjusting the bound parameters  to push up the bound towards the true divergence, whilst then minimizing this with respect the model parameters .

The upper bound (11) can now be estimated through sampling and minimized with respect to  and  by using the reparameterization trick (Kingma & Welling (2013)) and taking gradients. This results in a tractable procedure to minimize KL(p(y)||p(y))3. Note that this will require the evaluation of log p(y), for which we use the estimator log N -1 n p(y|x(n)) with x(n)  p(x) being points from the dataset. Using minibatches of x(n) makes this calculation fast, but the estimator is biased. We discuss this in more detail, along with an unbiased alternative, in the supplementary material section B.
For f -divergences other than the reverse KL, since the joint f -divergence Df (p(y, z)||p(y, z)) is expressed as an expectation over p(y|z)p(z), we can use a similar procedure to optimize the upper bound (10). Namely we can generate (y, z) samples from these distributions and then estimate the bound and take gradients.
4 EXPERIMENTS
In the following experiments our interest is to demonstrate the applicability of the f -divergence upper bound. The main focus is on training with the reverse KL divergence since this provides a natural "opposite" to training with the forward KL divergence . Throughout, the data is continuous and we use a Gaussian noise process with width  for p(y|x). We take p(z) to be a standard zero mean unit covariance Gaussian (thus with no trainable parameters). Similar to standard VAE training Kingma & Welling (2013), we use deep networks to parameterize the Gaussian model p(y|z) = N y µ(z), 2 and Gaussian variational distribution q(z|y) = N (z µ(y), (y)) for diagonal (y). Experimentally, we found that running several optimizer steps on  whilst keeping  fixed is also useful to ensure that the bound is tight when adjusting . We therefore use this strategy throughout training. The result that optimizing the auxiliary bound with respect to only q(z|y) tightens the bound (towards the marginal divergence) is shown in the supplementary materials section A.
4.1 TOY PROBLEM : FOWARD KL, REVERSE KL AND JS TRAINING
The toy dataset, as described by Roth et al. (2017), is a mixture of seven two-dimensional Gaussians arranged in a circle and embedded in three dimensional space, see figures: figure 3, figure 6. We use 5 hidden layers of 400 units and relu activation function for the mean and variance parameterization in q(z|y) and mean parameterization in p(y|z) with a two dimensional latent space z  R2.
3Note that the optimization of the reverse KL in the standard x space is intractable since we cannot obtain an unbiased estimate for the entropy of p(x). Whilst a biased estimate could be attempted by sampling, this would require a nested sampling strategy, making this computationally unfeasible.
5

Under review as a conference paper at ICLR 2019 p(x) samples

(a) Forward KL

(b) JS divergence Latent space

(c) Reverse KL

(d) Forward KL divergencs

(e) JS divergence

(f) Reverse KL divergence

Figure 3: Toy problem. In (a), (b) and (c) we plot the samples of the model p(x) trained by three different f -divergences. In (d), (e) and (f) we plot the latent z by sampling from the trained q(z|x) for each datapoint x. Note that this is after inverting the noise process, to recover the model on the x
space. See also section D

We use the KL (moment matching) objective KL(p(y)||p(y)), reverse KL (mode seeking) objective KL(p(y)||p(y)) objective, and JS divergence (balance between KL and reverse KL) objective JS(p(y)||p(y)) to train the model. We minimize the corresponding auxiliary f -divergence bounds by gradient descent using the Adam optimizer Kingma & Ba (2014) with learning rate 10-3.
To evaluate the bound in each iteration we use a minibatch of size 100 to calculate p(B)(y). For each minibatch we draw 100 samples from p(z) and subsequently draw 10 samples from p(y|z) for each drawn z to generate (y, z) samples. To facilitate training, we anneal the width, , of the spread divergence throughout the optimization process. This enables the model to feel the presence of other distant modes (high mass regions of p(y)), allowing the method to overcome any poor initialization of (, ). We can see in figure 3 that the model trained by JS and reverse KL divergence converge to cover each mode in the true generating distribution, and exhibits good separation of the seven modes in the latent space. Even though the reverse KL tends to collapse a model to a single mode, provided the model p(y|z) is sufficiently powerful, it can correctly capture all the 7 modes.
4.2 MNIST : FORWARD AND REVERSE KL TRAINING
To model the standard MNIST handwritten character dataset (LeCun & Cortes (2010)), we parametrize the mean of p(y|z) by a neural network and the variance of the spread divergence is fixed to 1 for RKL training and 0.5 for forward KL training. For q(z|y) we use a Gaussian with mean and isotropic covariance parameterized by a neural network. Both networks contain 5 layers, each layer with 400 units and leaky-relu as activation function. The latent z has dimension 64. We train both forward KL and reverse KL divergence using the f -divergence upper bound. For the reverse KL experiment, we first initialize the model by training using the forward KL objective for 20 epochs to prevent getting a sub-optimal solution (e.g. mode collapsing). After that, we train the model using the reverse KL objective for additional 20 epochs. In practice, we find the biased Monte Carlo estimation of log p(y) (discussed in section 3.2) does adversely affect the gradients of  during mini-batch training. To reduce the effect of this, we clip the contribution to the gradient from log p(y) to have norm 0.1.
6

Under review as a conference paper at ICLR 2019

(a) Forward KL

(b) Reverse KL

Figure 4: MNIST experiment. (a) Samples from the models trained by forward KL (b) Samples from the models trained by reverse KL.

We also train the model by forward KL for 40 epochs to make a comparison. All the networks are trained by Gradient descent (Adam) with learning rate 10-4. In figure 4 we show the samples from two models. As we can see, model trained by reverse KL generates sharper images but suffering from the "mode collapse" effect; model trained by forward KL is less sharper but has more variations, which recover the properties of forward KL divergence.

4.3 CELEBA : MIXED FORWARD AND REVERSE KL TRAINING
We pre-process CelebA Liu et al. (2015) images by first taking 140x140 center crops and then resizing to 64x64. Pixel values were then rescaled to lie in [0, 1]. The architectures of the convolutional encoder q(z|y) and deconvolutional decoder p(y|z) (with fixed variance 0.25) are given in the supplementary material, section E. The standard deviation of the spread divergence is 0.5. We train a VAE for 3 epochs as initialization and then train for one additional epoch for both pure forward KL and mixed standard and reverse KL combination 0.5  KL(p(y)||p(y)) + 0.5  KL(p(y)||p(y)). In order to ensure that the bound remained tight, we interleave each  update (learning rate 10-6) with 20  updates (learning rate 10-4), with Adam training used in both cases.
In figure 5 we show samples from the trained models. This enables us to see the effect of additional training using the mixed forward-reverse KL objective compared to additional training with the pure forward KL. As we can see, the impact of including the reverse KL term in training is significant, resulting in less variability in pose, but sharper images. This is consistent with the "mode-seeking" behavior of the reverse KL objective.

4.4 f -GAN COMPARISON

As discussed in section 2.3, a different approach to minimizing the f -divergence is used in Nowozin et al. (2016), utilizing a variational lower bound to the f -divergence:

Df (p(x)||p(x))  sup Exp(x)[T (x)] - Exp(x)[f (T (x))]
T T

(12)

Here f  is the Fenchel conjugate and T is any class of functions that respects the domain of f . After parameterizing T = gf (V) (where gf : R  domf and V is an unconstrained parametric function) and p(x), the optimization scheme is then to alternately tighten (i.e. increase) the bound through changes to  and then lower the bound through changes to , see figure 2. This is of interest
because the GAN objective Goodfellow et al. (2014) can be seen as a specific instance of this scheme.
We acknowledge that Nowozin et al. (2016) principally grounds GANs in a wider class of techniques, and is not necessarily intended as a scheme for minimizing an f -divergence. However, it is natural to
ask whether our auxiliary upper bound or the Fenchel-conjugate lower bound give different results when used to minimize the f -divergence for a similar complexity of parameter space (, ).

7

Under review as a conference paper at ICLR 2019

(a) Forward KL

(b) Mixed Forward and reverse KL

Figure 5: CelebA experiment. Image samples from the trained models p(x). After VAE initialization, we continued training for an additional epoch with (a) pure forward KL and (b) the mixed
forward+reverse KL combination.

KL rev-KL J-S

Df (p(x)||p(x)) 0.21 0.18
Df (p(x)||pLB(x)) 0.32 0.25 Df (p(x)||pUB(x)) 0.21 0.23
µ 1.70 1.85

0.05 0.23 0.15
1.76

µ^LB 1.71 1.73 1.70 µ^UB 1.71 1.76 1.70
 0.62 0.57 0.60

^LB 0.46 0.45 0.24 ^UB 0.62 0.65 0.33

Table 2: Learned Gaussian parameters to fit a mixture of two Gaussians using forward KL, reverse KL and Jensen-Shannon divergence. p(x) is the optimal Gaussian fitted to minimize the exact divergence. pUB(x) is the optimal Gaussian fitted to minimize our auxiliary upper bound on the divergence. pLB(x) is the optimal Gaussian fitted to minimize the Fenchel-conjugate lower bound on the divergence.

To compare the two methods we fit a univariate Gaussian p(x) to data generated from a mixture of two Gaussians through the minimization of various f -divergences. See the supplementary material for details. For the f -GAN lower bound we use a network with two hidden layers of size 64 for V(x). For our upper bound we use a network with two hidden layers of size 50 to parameterize q(z|x) and set p(x, z) to be a bivariate Gaussian, so that it marginalizes to a univariate Gaussian as required. The upper and lower bound methods have a similar number of free parameters (q has fewer hidden units but more outputs than V). The two methods result in broadly similar Gaussian fits, see table 2. In general, minimizing the upper bound results in a slightly superior fit compared to the f -GAN method Nowozin et al. (2016) in terms of proximity to the true minimal f -divergence fit and proximity of the bound value to the true divergence. Additionally we find that minimizing our upper bound is computationally more stable than the optimization procedure required for f -GAN training (simultaneous tightening and lowering of the bound ­ see supplementary material F).
5 RELATED WORK
The Auxiliary Variational Method Agakov & Barber (2004) uses an auxiliary space to minimize the joint KL divergence in order to minimize the marginal KL divergence. We extend this method to the more general class of f -divergences.
Variational Auto-Encoders Kingma & Welling (2013) Our method is a way to train an identical class of generative and variational models, but with a class of different optimization objectives based on f -divergences. Since the VAE optimization scheme is a variational method of maximizing the likelihood, it is similar to our scheme with the choice of minimizing the forward KL divergence, which is also a variational form of maximum likelihood. Both methodologies use sampling to
8

Under review as a conference paper at ICLR 2019

estimate a variational bound which can be differentiated through the use of the reparameterization trick Kingma & Welling (2013).

In Rényi divergence variational inference Li & Turner (2016), a variational lower bound of loglikelihood is proposed based on the Rényi divergence. However, our joint upper bound is an estimator of f -divergence in marginal data space, it only relates to maximum likelihood learning when we use KL divergence.

In Auxiliary Deep Generative Models Maaløe et al. (2016), a VAE is extended with an auxiliary space. This allows a richer variational distribution to be learned, with the correlation between latent variables being pushed to the auxiliary space to keep the calculation tractable. This, similarly to our method, utilizes the general auxiliary variational method Agakov & Barber (2004), but is focused on making VAEs more powerful rather than providing different optimization schemes.

In the f -GAN Nowozin et al. (2016) methodology, an interesting connection is made between the GAN training objective and a lower bound on the f -divergence. The authors conclude that using different divergences leads to largely similar results, and that the divergence only has a large impact when the model is "under-powered". However, that conclusion is somewhat at odds with our own, in which we find that the (upper bound on) different divergences gives very different model fits. Indeed, others have reached a similar conclusion: the reverse KL divergence is optimized as a GAN objective in Sønderby et al. (2017), demonstrating that it is effective in the task of image super-resolution. A variety of different generator objectives for GANs are used in Poole et al. (2016), with some divergence objectives exhibiting the "mode-seeking" behavior we have observed.

In Mohamed & Lakshminarayanan (2016), the authors demonstrate an alternative approach to train

f -divergence Df (p(x)||q(x)) =

q(x)f

p(x) q(x)

dx

by

directly

estimating

the

density

ratio

p(x) q(x)

.

This method makes a connection to GANs: the discriminator is trained to approximate the ratio and

the generator loss is designed based upon different choices of f -divergence (see the supplementary

material section G for details). We thereby recognize there are three different tractable estimations

of the f -divergence: 1. ratio estimation in the marginal space 2. Fenchel conjugate lower bound

(f -GAN) and 3. the variational joint upper bound (introduced by our paper).

Ratio estimation by classification has also been extended to minimize the KL-divergence in the joint space Huszár (2017). Similarly, Bi-directional GAN Donahue et al. (2016) and Ali-GAN Dumoulin et al. (2016) augment the GAN generator with an additional inference network. Although these models focus on similar training objectives to our own, the purpose of using the joint space is different to that of our approach. Our method uses the joint distribution to create an upper bound in order to estimate the f -divergence in the marginal space; the latent representation is automatically achieved. In contrast, all three methods mentioned above expand the original space to a joint space just for learning the latent representation, the divergence is estimated by either ratio estimation or GAN approaches. Additionally, they only minimize the target divergence only at the limit of an optimal discriminator (or in the nonparametric limit, see Goodfellow et al. (2014) and Mescheder et al. (2017)), which may cause instability in the GAN training process Arjovsky & Bottou (2017).

6 CONCLUSION
We introduced an upper bound on f -divergences, based on an extension of the auxiliary variational method. The approach allows variational training of latent generative models in a much broader set of divergences than previously considered. We showed that the method requires only a modest change to the standard VAE training algorithm but can result in a qualitatively very different fitted model. For our low dimensional toy problems, both the forward KL and reverse KL can be effective in learning the model. However, for higher dimensional image generation, compared to standard forward KL training (VAE), training with the reverse KL tends to focus much more on ensuring that data is generated with high fidelity around a smaller number of modes. The central contribution of our work is to facilitate the application of more general f -divergences to training of probabilistic generative models with different divergences potentially giving rise to very different learned models.
9

Under review as a conference paper at ICLR 2019
REFERENCES
F. V. Agakov and D. Barber. An Auxiliary Variational Method. Neural Information Processing (ICONIP), 2004.
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.
D. Barber, M. Zhang, R. Habib, and T. Bird. Spread divergences. arXiv preprint, 2018.
J. Donahue, P. Krähenbühl, and T. Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.
V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and A. Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
I. Goodfellow. NIPS 2016 tutorial: Generative Adversarial Networks. arXiv preprint arXiv:1701.00160, 2016.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing Systems 27, pp. 2672­2680. 2014.
F. Huszár. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235, 2017.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013. URL http://dblp.uni-trier.de/db/journals/corr/corr1312.html# KingmaW13.
Y. LeCun and C. Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun. com/exdb/mnist/.
Y. Li and R. Turner. Rényi divergence variational inference. In Advances in Neural Information Processing Systems, pp. 1073­1081, 2016.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep Learning Face Attributes in the Wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.
L. Maaløe, C. K. Sønderby, S. K. Sønderby, and O. Winther. Auxiliary Deep Generative Models. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML'16, pp. 1445­1454. JMLR.org, 2016. URL http://dl.acm. org/citation.cfm?id=3045390.3045543.
L. Mescheder, S. Nowozin, and A. Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.
T. Minka. Divergence measures and message passing. Technical Report MSR-TR-2005-173, Microsoft Research Ltd, Cambridge, UK, 2005.
S. Mohamed and B. Lakshminarayanan. Learning in implicit generative models. arXiv preprint arXiv:1610.03483, 2016.
H. Narayanan and S. Mitter. Sample complexity of testing the manifold hypothesis. In Advances in Neural Information Processing Systems, pp. 1786­1794, 2010.
S. Nowozin, B. Cseke, and R. Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
10

Under review as a conference paper at ICLR 2019
B. Poole, A. A. Alemi, J. Sohl-Dickstein, and A. Angelova. Improved generator objectives for GANs. CoRR, abs/1612.02780, 2016. URL http://arxiv.org/abs/1612.02780.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In ICML, volume 32 of JMLR Workshop and Conference Proceedings, pp. 1278­1286. JMLR.org, 2014.
K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. Stabilizing training of generative adversarial networks through regularization. In Advances in Neural Information Processing Systems, pp. 2015­2025, 2017.
I. Sason and S. Verdú. Bounds among f -divergences. CoRR, abs/1508.00335, 2015. URL http: //arxiv.org/abs/1508.00335.
C. K. Sønderby, J. Caballero, L. Theis, W. Shi, and F. Huszár. Amortised map inference for image super-resolution. International Conference on Learning Representations, 2017.
J. Wolfowitz. Asymptotic Efficiency of the Maximum Likelihood Estimator. Theory of Probability & Its Applications, 10:247­260, 1965.
11

Under review as a conference paper at ICLR 2019

SUPPLEMENTARY MATERIAL
A PROPERTIES OF THE AUXILIARY VARIATIONAL METHOD
Here we give a property of the auxiliary bound for f -divergences with differentiable f ; this covers most f of interest, and the argument extends to those f which are piecewise differentiable. Then for the particular case of the reverse Kl divergence we give a simpler proof of this property as well as two additional properties (which do not hold for general f ).
A.1 Df (p(x)||q(x))
For differentiable f we claim that when we fully optimize the auxiliary f -divergence w.r.t p(z|x), this is the same as minimizing the f -divergence in the x space alone. Let's first fix q(x, z) and find the optimal p(z|x) by taking the functional derivative of the auxiliary f -divergence

 p(z|x)

Df

(P

||Q)

=

 p(z|x)

p(z |x )p(x ) q(x , z )f
q(x , z )

p(z|x)p(x) p(x) = q(x, z)f
q(x, z) q(x, z)

p(z|x)p(x) = p(x)f q(z|x)q(x)

dx dz

(13) (14) (15)

At the minimum this will be equal to 0 (plus a constant Lagrange multiplier that comes from the constraint that p(z|x) is normalized). Since f is not constant (if it is then the f -divergence is a
constant), this then implies that the argument of f must be constant in z. This implies that optimally p(z|x) = q(z|x). Plugging this back into the f -divergence, it reduces to simply Df (p(x)||q(x))

Hence, we have shown

min Df (p(x, z)||q(x, z)) = Df (p(x)||q(x))
p(z|x)

(16)

Since the assumption is that Df (p(x)||q(x)) is not computationally tractable, this means that, in practice, we need to use a suboptimal p(z|x), restricting p(z|x) to a family p(z|x) such that the
joint f -divergence is computationally tractable.

A.2 RELATION TO KL(q(x)||p(x))

For the particular case of the reverse KL divergence we also provide this more straightforward proof.
Again, the claim is that when we fully optimize the auxiliary KL divergence w.r.t p(z|x), this is the same as minimizing the KL in the x space alone.
Let's first fix q(x, z) and find the optimal p(z|x). The divergence is

KL(q(x, z)||p(x, z)) = - q(z|x)q(x) log p(z|x)dxdz + const. = q(x)KL(q(z|x)||p(z|x)) dx + const.

(17)

Since we are taking a positive combination of KL divergences, this means that, optimally, p(z|x) = q(z|x). Plugging this back into the KL divergence, the KL reduces to simply

KL(q(x)||p(x))

(18)

Hence, we have shown

min KL(q(x, z)||p(z|x)p(x)) = KL(q(x)||p(x))
p(z|x)

(19)

12

Under review as a conference paper at ICLR 2019

A.3 INDEPENDENCE p(z|x) = p(z)
Also for the particular case of the reverse KL divergence we can derive a result from the assumption that the auxiliary variables are independent of the observations and the prior p(z|x) = p(z). We have

KL(q(x, z)||p(x, z)) = q(x, z) log q(x, z)dxdz - q(x, z) log[p(z)p(x)]dxdz = KL(q(z)||p(z)) + q(z)KL(q(x|z)||p(x)) dz

(20)

Optimally, therefore, we set p(z) = q(z), which gives the resulting expression

q(z)KL(q(x|z)||p(x))
z

(21)

Since we are still free to set q(z), we should optimally set q(z) to place all its mass on the z that

minimizes

KL(q(x|z)||p(x))

(22)

In other words, the assumption of independence p(z|x) = p(z) implies that method is no better than computing each KL(q(x|z)||p(x)) and then choosing the single best model q(x|z).

A.4 FACTORIZING q(x, z) = q(x)q(z)

Again for the reverse KL divergence, under the independence assumption q(x, z) = q(x)q(z), it is straightforward to show that

KL(q(x, z)||p(x, z)) = KL(q(x)||p(x))

(23)

In the case that q(x) for example is a simple Gaussian distribution, this means that the independence assumption does not help enrich the complexity of the approximating distribution.

A.5 RELATION TO ELBO
The reverse KL divergence in joint space: KL(q(x, z)||p(x, z)) is equivalent to using ELBO to lower bound log p(x) in KL(q(x)||p(x)):

KL(q(x)||p(x)) = q(x)(log q(x) - log p(x))dx 



 q(x) log q(x) - q(z|x)(log p(x, z) - log q(z|x)) dz dx 
ELBO
= q(x)q(z|x)(log q(x)q(z|x) - log p(x, z))dxdz

= KL(q(x, z)||p(x, z))

(24)

B MINIBATCH

A potential computational bottleneck is computing

where, for example,

1 log p(y) = log
N

N

p(y|xn)

n=1

p(y|xn)

=

1 (22)D/2

exp

-

1 22

(y

-

xn)2

(25) (26)

13

Under review as a conference paper at ICLR 2019

If N is large, this means that we would have to sum over the full dataset to compute log p(x), which might be prohibitively expensive.

One way to deal with is simply to use a minibatch

log p(y)  log 1 M

p(y|xm)

mM

(27)

where M is a minibatch. However, this is a biased estimator of log p. There may be situations in which the bias could be problematic. As such we discuss below a potential approach that could be used to speed up evaluation of a new bound.

B.1 VARIATIONAL TREATMENT FOR REVERSE KL
The idea is to bound log p(x) in the standard variational way. In the auxiliary reverse KL divergence bound, p(x) occurs in the contribution

- p(x, z) log p(x)dxdz

(28)

with the understanding that we need an upper bound on this expression. In other words, we need a

lower bound on

N

log p(x) = - log(N ) + log p(x|n)

(29)

n=1

We define

N
Z(x)  p(x|n)

(30)

n=1

We consider the definition

p~(n|x) 

p(x|n)

p(x|n) =

n p(x|n) Z(x)

(31)

Then considering the standard KL bound

KL(q(n|x)||p~(n|x))  0

(32)

gives This gives the bound

q(n|x) log q(n|x) - q(n|x) log p~(n|x)  0
nn

(33)

log Z(x)  - q(n|x) log q(n|x) + q(n|x) log p(x|n)
nn
We can now plug this bound into the auxiliary KL expression

(34)

KL(p(x)||p^(x))  p(x, z) log p(x, z)dxdz - p(x, z) log (q(z|x)p(x)) dxdz (35)

to give the new bound

KL(p(x)||p^(x))  p(x, z) log p(x, z)dxdz - p(x, z) log q(z|x)dxdz

+ p(x, z)

q(n|x) log q(n|x) - q(n|x) log p(x|n) dxdz + log N (36)

nn

We can then draw datapoints n from q(n|x) to form the minibatch. By sampling minibatches, this gives an unbiased estimator of a new upper bound. In this case we also need to optimise the resulting new upper bound with respect to  as well.

14

Under review as a conference paper at ICLR 2019

C USING BOTH STANDARD AND REVERSE KL

The standard KL bound is

KL(p^(x)||p(x))  0

which gives the log likelihood bound

(37)

N

log p(x) 

- q(z|n) log q(z|n) + q(z|n) log (p(xn|z)p(z))

n=1

z

z

We can then consider learning the model paremeter  by minimising

(38)

KL(p(x)||p^(x)) - (1 - ) log p(x),

  [0, 1]

(39)

Both terms can be upper bounded. Note that there is no requirement that the parameters or variational distributions q used in the bounds for each term should match.

D TARGET DISTRIBUTION OF THE TOY PROBLEM
We train on the toy dataset described by Roth et al. (2017), which is a mixture of seven two-dimensional Gaussians arranged in a circle and embedded in three dimensional space, see figure 6. The standard deviation of the each Gaussian is 0.05.

E NETWORK ARCHITECTURE

Both encoder and decoder used fully convolutional architectures with 5x5 convolutional filters and used vertical and horizontal strides 2 except the last deconvolution layer we used stride 1. Here Convk stands for a convolution with k filters, DeConvk for a deconvolution with k filters, BN for the batch normalization Ioffe & Szegedy (2015), ReLU for the rectified linear units, and FCk for the fully connected layer mapping to Rk.
x  R64×64×3  Conv128  BN  Relu  Conv256  BN  Relu  Conv512  BN  Relu  Conv1024  BN  Relu  FC64

Figure 6: Target distribution of the toy problem, from Roth et al. (2017)

z  R64  FC8×8×1024  DeConv512  BN  Relu  DeConv256  BN  Relu  DeConv128  BN  Relu  DeConv64  BN  Relu  DeConv3
F f -GAN COMPARISON
The mixture of Gaussians we attempt to fit a univariate Gaussian to is plotted in Figure 7. We plot the lower and upper bounds during training in Figure 8. We can see the upper bound is generally faster to converge and less noisy. It also a consistently decreasing objective, whereas the variational lower bound fluctuates higher and lower in value throughout the training process.
15

Under review as a conference paper at ICLR 2019

Density

Bound Bound Bound Bound Bound Bound

10 1 2 3 4
Figure 7: Mixture of two Gaussians, 0.3N1 + 0.7N2 where N1 = N (µ1 = 1, 1 = 0.1) and N2 = N (µ2 = 2, 2 = 0.5)

1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.50
0 1000 2000 3000 Ite4r0a0t0ion 5000 6000 7000 8000
(a) Forward KL lower bound

2.50 2.25 2.00 1.75 1.50 1.25 1.00
0

1000 2000 Ite3r0a0t0ion 4000 5000 6000

(b) Forward KL upper bound

1.50 1.25 1.00 0.75 0.50 0.25 0.00
0

500 1000 1500 Ite2r0a0t0ion 2500 3000 3500 4000

(c) Reverse KL lower bound

8 6 4 2 0
0 200 400Iteration600 800 1000
(d) Reverse KL upper bound

0.4 0.3 0.2 0.1 0.0 0.1
0 2500 5000 7500 Ite10ra0t0i0on12500 15000 17500 20000

1.0 0.8 0.6 0.4 0.2
0

200 400Iteration600 800 1000

(e) Jensen-Shannon lower bound

(f) Jensen-Shannon upper bound

Figure 8: Training runs for fitting a univariate Gaussian to a mixture of two Gaussians by minimizing

a variety bound.

of

f -divergences.

On

the

left

we

train

u1s6ing

the

lower

bound,

on

the

right

with

our

upper

Under review as a conference paper at ICLR 2019

G CLASS PROBABILITY ESTIMATION

In Mohamed & Lakshminarayanan (2016), two ratio estimation techniques, class probability estimation and ratio matching, are discussed. We briefly show how to use the class probability estimation technique to estimate f -divergence, and refer readers to the original paper Mohamed & Lakshminarayanan (2016) for the ratio matching technique.

The density ratio can be computed by building a classifier to distinguish between training data and the

data

generated

by

the

model.

This

ratio

is

p(x) q (x)

=

p(x|y=1) p(x|y=0)

,

where

label

y

=

1

represents

samples

from p and y = 0 represents samples from q. By using Bayes rule and assuming that we have the same

number of samples from both p and q, we have

p(x) q (x)

=

p(x|y=1) p(x|y=0)

=

p(y=1|x)p(x) p(y=1)

/

p(y=0|x)p(x) p(y=0)

=

p(y=1|x) p(y=0|x)

.

We

can

then

set

the

discriminator

output

to

be

D(x)

=

p(y

=

1|x),

so

the

ratio

can

be

written as

p(x) q (x)

=

p(y=1|x) 1-p(y=1|x)

=

D (x) 1-D (x)

.

The generator loss corresponding to an f -divergence

can then be designed as Df (p(x)||q(x)) =

q (x)f

p(x) q (x)

dx =

q

(x)f

(

D (x) 1-D (x)

)

=

Eq(z)

[f

(

D(G (z)) 1-D(G (z))

)].

17

