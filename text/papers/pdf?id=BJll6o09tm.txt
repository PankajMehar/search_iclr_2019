Under review as a conference paper at ICLR 2019
PADAM: CLOSING THE GENERALIZATION GAP OF ADAPTIVE GRADIENT METHODS IN TRAINING DEEP NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Adaptive gradient methods, which adopt historical gradient information to automatically adjust the learning rate, despite the nice property of fast convergence, have been observed to generalize worse than stochastic gradient descent (SGD) with momentum in training deep neural networks. This leaves how to close the generalization gap of adaptive gradient methods an open problem. In this work, we show that adaptive gradient methods such as Adam, Amsgrad, are sometimes "over adapted". We design a new algorithm, called Partially adaptive momentum estimation method (Padam), which unifies the Adam/Amsgrad with SGD by introducing a partial adaptive parameter p, to achieve the best from both worlds. Experiments on standard benchmarks show that Padam can maintain fast convergence rate as Adam/Amsgrad while generalizing as well as SGD in training deep neural networks. These results would suggest practitioners pick up adaptive gradient methods once again for faster training of deep neural networks.
1 INTRODUCTION
Stochastic gradient descent (SGD) is now one of the most dominant approaches for training deep neural networks (Goodfellow et al., 2016). In each iteration, SGD only performs one parameter update on a mini-batch of training examples. SGD is simple and has been proved to be efficient, especially for tasks on large datasets. In recent years, adaptive variants of SGD have emerged and shown their successes for their convenient automatic learning rate adjustment mechanism. Adagrad (Duchi et al., 2011) is probably the first along this line of research, and significantly outperforms vanilla SGD in the sparse gradient scenario. Despite the first success, Adagrad was later found to demonstrate degraded performance especially in cases where the loss function is nonconvex or the gradient is dense. Many variants of Adagrad, such as RMSprop (Hinton et al., 2012), Adam (Kingma & Ba, 2015), Adadelta (Zeiler, 2012), Nadam (Dozat, 2016), were then proposed to address these challenges by adopting exponential moving average rather than the arithmetic average used in Adagrad. This change largely mitigates the rapid decay of learning rate in Adagrad and hence makes this family of algorithms, especially Adam, particularly popular on various tasks. Recently, it has also been observed (Reddi et al., 2018) that Adam does not converge in some settings where rarely encountered large gradient information quickly dies out due to the "short momery" problem of exponential moving average. To address this issue, Amsgrad (Reddi et al., 2018) has been proposed to keep an extra "long term memory" variable to preserve the past gradient information and to correct the potential convergence issue in Adam.
On the other hand, people recently found that for largely over-parameterized neural networks, e.g., more complex modern convolutional neural network (CNN) architectures such as VGGNet (He et al., 2016), ResNet (He et al., 2016), Wide ResNet (Zagoruyko & Komodakis, 2016), DenseNet (Huang et al., 2017), training with Adam or its variants typically generalizes worse than SGD with momentum, even when the training performance is better. In particular, people found that carefullytuned SGD, combined with proper momentum, weight decay and appropriate learning rate decay strategies, can significantly outperform adaptive gradient algorithms eventually (Wilson et al., 2017). As a result, even though adaptive gradient methods are relatively easy to tune and converge faster at the early stage, recent advances in designing neural network structures are all reporting their performances by training their models with SGD-momentum (He et al., 2016; Zagoruyko & Komodakis,
1

Under review as a conference paper at ICLR 2019

2016; Huang et al., 2017; Simonyan & Zisserman, 2014; Ren et al., 2015; Xie et al., 2017; Howard et al., 2017). Moreover, it is difficult to apply the same learning rate decay strategies that work well in SGD with momentum to adaptive gradient methods, since adaptive gradient methods usually require a much smaller base learning rate that will soon die out after several rounds of decay. We refer to it as the "small learning rate dilemma" (see more details in Section 3).

With all these observations, a natural question is:

Can we take the best from both Adam and SGD-momentum, i.e., design an algorithm that not only enjoys the fast convergence rate as Adam, but also generalizes as well as SGD-momentum?

In this paper, we answer this question affirmatively. We close the generalization gap of adaptive gradient methods by presenting a new algorithm, called partially adaptive momentum estimation (Padam) method, which unifies Adam/Amsgrad with SGD-momentum to achieve the best of both worlds.

In particular, we make the following contributions:
· We propose a new algorithm, Padam, which unifies Adam/Amsgrad and SGD with momentum by a partially adaptive parameter. We show that Adam/Amsgrad can be seen as a special fully adaptive instance of Padam. The intuition behind our algorithm is that, by controlling the degree of adaptiveness, the learning rate in Padam does not need to be as small as other adaptive gradient methods to prevent the gradient update from exploding. This resolves the "small learning rate dilemma" for adaptive gradient methods and allows for faster convergence, hence closing the gap of generalization.
· We show that the Padam's performance is also theoretically guaranteed. We provide a convergence analysis of Padam in the convex setting, based on the analysis of Kingma & Ba (2015); Reddi et al. (2018), and prove a data-dependent regret bound.
· We also provide thorough experiments about our proposed Padam method on training modern deep neural architectures. We empirically show that Padam achieves the fastest convergence speed while generalizing as well as SGD with momentum. These results suggest that practitioners should pick up adaptive gradient methods once again for faster training of deep neural networks.

Notation: Scalars are denoted by lower case letters, vectors by lower case bold face letters, and matrices by upper case bold face letters. For a vector   Rd, we denote the 2 norm of  by

 2=

d i=1

i2,

the

 norm of  by



 = maxdi=1 |i|. For a sequence of vectors {j}jt=1,

we denote by j,i the i-th element in j. We also denote 1:t,i = [1,i, 2,i, . . . , t,i] . With slightly abuse of notation, for any two vectors a and b, we denote a2 as the element-wise square, ap as the

element-wise power operation, a/b as the element-wise division and max(a, b) as the element-wise maximum. We denote by diag(a) a diagonal matrix with diagonal entries a1, . . . , ad. Let S+d + be the set of all positive definite d × d matrices. We denote by X ,A(b) the projection of b onto a
convex set X , i.e., argminaX A1/2(a - b) 2 for b  Rd, A  S+d +. Given two sequences {an} and {bn}, we write an = O(bn) if there exists a constant 0 < C < + such that an  C bn. We

use notation O(·) to hide logarithmic factors. We write an = o(bn) if an/bn  0 as n  .

2 REVIEW OF ADAPTIVE GRADIENT METHODS

Various adaptive gradient methods have been proposed in order to achieve better performance on
various stochastic optimization tasks. Adagrad (Duchi et al., 2011) is among the first methods
with adaptive learning rate for each individual dimension, which motivates the research on adaptive gradient methods in the machine learning community. In detail, Adagrad1 adopts the following
update form:

t+1

=

t

-

t

gt vt

,

where vt

=

1 t

t
gj2
j=1

(Adagrad)

1The formula here is equivalent to the one from the original paper (Duchi et al., 2011) after simple manipulations.

2

Under review as a conference paper at ICLR 2019

 Here gt stands for the stochastic gradient ft(t), and t = / t is the step size (a.k.a., learning rate). Adagrad is proved to enjoy a huge gain in terms of convergence especially in sparse gradient
situations. Empirical studies also show a performance gain even for non-sparse gradient settings.
RMSprop (Hinton et al., 2012) follows the idea of adaptive learning rate and it changes the arithmetic averages used for vt in Adagrad to exponential moving averages. Even though RMSprop is an empirical method with no theoretical guarantee, the outstanding empirical performance of RMSprop
raised people's interests in exponential moving average variants of Adagrad. Adam (Kingma & Ba, 2015)2 is the most popular exponential moving average variant of Adagrad. It combines the idea of
RMSprop and momentum acceleration, and takes the following update form:

t+1

=

t

-

t

mt vt

,

where

mt

=

1mt-1

+

(1

-

1)gt, vt

=

2vt-1

+

(1

-

2)gt2

(Adam)

 Adam also requires t = / t for the sake of convergence analysis. In practice, any decaying step

size or even constant step size works well for Adam. Note that if we choose 1 = 0, Adam basically

reduces to RMSprop. Reddi et al. (2018) identified a non-convergence issue in Adam, and proposed

a modified algorithm namely Amsgrad. More specifically, Amsgrad adopts an additional step to ensure the decay of the effective learning rate t/ vt, and its key update formula is as follows:

t+1

=

t

-

t

mt vt

,

where

vt

=

max(vt-1, vt)

(Amsgrad)

The definitions of mt and vt are the same as Adam. Note that the modified vt ensures the convergence of Amsgrad. Reddi et al. (2018) also corrected some mistakes in the original proof of Adam and proved an O(1/ T ) convergence rate of Amsgrad for convex optimization. Note that all the theoretical guarantees on adaptive gradient methods (Adagrad, Adam, Amsgrad) are only proved for convex functions.

3 THE PROPOSED ALGORITHM
 In this section, we propose a new algorithm that not only inherits the O(1/ T ) convergence rate from Adam/Amsgrad, but also has comparable or even better generalization performance than SGD with momentum. Specifically, we introduce the a partial adaptive parameter p to control the level of adaptivity of the optimization procedure. The proposed algorithm is displayed in in Algorithm 1.

Algorithm 1 Partially adaptive momentum estimation method (Padam)
input: initial point 1  X ; step sizes {t}; momentum parameters {1t}, 2; partially adaptive parameter p  (0, 1/2]
set m0 = 0, v0 = 0, v0 = 0 for t = 1, . . . , T do
gt = ft(t) mt = 1tmt-1 + (1 - 1t)gt vt = 2vt-1 + (1 - 2)gt2 vt = max(vt-1, vt) t+1 = X ,diag(vtp) t - t · mt/vtp end for

In Algorithm 1, gt denotes the stochastic gradient and vt can be seen as a moving average over the second order momentum of the stochastic gradients. As we can see from Algorithm 1, the key
difference between Padam and Amsgrad (Reddi et al., 2018) is that: while mt is still the first order momentum as in Adam/Amsgrad, it is now "partially adapted" by the second order momentum, i.e.,

t+1

=

t

-

t

mt vtp

,

where

vt

=

max(vt-1, vt)

(Padam)

We call p  (0, 1/2] the partially adaptive parameter. Note that 1/2 is the largest possible value for p and a larger p will result in non-convergence in the proof (see the details of the proof in the

2Here for simplicity and consistency, we ignore the bias correction step in the original paper of Adam. Yet adding the bias correction step will not affect the argument in the paper.

3

Under review as a conference paper at ICLR 2019

appendix). When p  0, Algorithm 1 reduces to SGD with momentum3 and when p = 1/2, Algorithm 1 is exactly Amsgrad. Therefore, Padam indeed unifies Amsgrad and SGD with momentum.
Now the question is, what value for p should we choose? Or in another way, is p = 1/2 the best choice? The answer is negative. The intuition behind this is simple: it is very likely that Adam/Amsgrad is "over-adaptive". One notable fact that people found when using adaptive gradient methods to train deep neural networks is that the learning rate needs to be much smaller than that of SGD-momentum (Keskar & Socher, 2017; Wilson et al., 2017). For many tasks, the base learning rate for SGD-momentum is usually set to be 0.1 while that of Adam is usually set to be 0.001. In fact, the key reason that prohibits Adam from adopting a more aggressive learning rate is the large adaptive term 1/ vt. The existence of such a large adaptive term makes the effective learning rate (t/ vt) easily explode with a larger t. Moreover, the learning rate decaying strategy used in modern deep neural network training makes things worse. More specifically, after several rounds of decaying, the learning rates of the adaptive gradient methods are too small to make any significant progress in the training process. We call it "small learning rate dilemma". This explains the relatively weak performances of adaptive gradient methods at the later stage of the training process, where the non-adaptive gradient methods like SGD start to outperform them.
The above discussion suggests that we should consider using Padam with a proper adaptive parameter p, which will enable us to adopt a larger learning rate to avoid the "small learning rate dilemma". And we will show in our experiments (Section 5) that Padam can adopt an equally large base learning rate as SGD.
Note that besides Algorithm 1, our partially adaptive idea can also be applied to other adaptive gradient methods such as Adagrad, Adadelta, RMSprop, AdaMax (Kingma & Ba, 2015). For the sake of conciseness, we do not list the partially adaptive versions for other adaptive gradient methods here. We also would like to comment that Padam is totally different from the p-norm generalized version of Adam in Kingma & Ba (2015), which induces AdaMax method when p  . In their case, p-norm is used to generalize 2-norm of their current and past gradients while keeping the scale of adaptation unchanged. In sharp contrast, we intentionally change (reduce) the scale of the adaptive term in Padam to get better generalization performance.
Finally, note that in Algorithm 1 we remove the bias correction step used in the original Adam paper following Reddi et al. (2018). Nevertheless, our arguments and theory are applicable to the bias correction version as well.

4 CONVERGENCE ANALYSIS OF THE PROPOSED ALGORITHM

In this section, we establish the theory of convergence for our proposed algorithm in the online optimization setting (Zinkevich, 2003), where we try to minimize the cumulative objective value of a sequence of loss functions: f1, f2, . . . , fT . In particular, at each time step t, the optimization algorithm picks a point t  X , where X  Rd is the feasible set. A loss function ft is then revealed, and the algorithm incurs loss ft(t). Let  be optimal solution to the cumulative objective function as follows
T
 = argmin ft(),
X t=1
where X is a feasible set for all steps. We evaluate our algorithm using the regret, which characterizes the sum of all previous loss function values ft(t) relative to the performance of the best fixed parameter  from a feasible set. Specifically, the regret is defined as

T

RT =

ft(t) - ft() ,

t=1

and our goal is to predict the unknown parameter t and minimize the overall regret RT . Our theory is established for convex loss functions, where the following assumption holds.

3The only difference between Padam with p = 0 and SGD with momentum is an extra constant factor (1 - 1), which can be moved into the learning rate such that the update rules for these two algorithms are identical.

4

Under review as a conference paper at ICLR 2019

Assumption 4.1 (Convex function). All ft() are convex functions on X for 1  t  T , i.e., for all x, y  X ,
ft(y)  ft(x) + ft(x) (y - x).

Assumption 4.1 is a standard assumption in online learning and the same assumption has been used in the analysis of Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2015) and Amsgrad (Reddi et al., 2018).

Next we provide the main convergence rate result for our proposed algorithm.

Theorem 4.2. Under Assumption 4.1, if the convex feasible set X has bounded diameters, i.e.,  -    D for all   X, and ft has bounded gradients, i.e., ft()   G for all   X , 1  t  T , let t = / t, 1t = 1t-1,   (0, 1), 0  1, 2 < 1, p  (0, 0.5], the regret of Algorithm 1 satisfies:

RT



D2 2(1 - 1)

d T
i=1

· vTp ,i +

G(1-2p)1 + log T (1 - 1)2(1 - )(1 - 2)p

d i=1

+

1dD2 G2p 2(1 - 1)(1 -

)2

,



where  = 1/ 2 < 1.

g1:T ,i 2

(4.1)

Remark 4.3. Theorem 4.2 suggests that, similar to Adam (Kingma & Ba, 2015) and Amsgrad

(Reddi et al., 2018), the

(which is known to have

2011) and

d i=1

vTp ,i

arergergerteot fboPuanddamofcOan(bedTco))nswidheernablyid=be1ttegr1:tTh,ain2online dgTrad(Dieuncthdieestceanl.t, d. Also, from the proof of Theorem 4.2 in Appendix A, we can see

that the regret bound can remain in the same order even with a more modest momentum decay

1t = 1/t.

 The following corollary demonstrates that our proposed algorithm enjoys a regret bound of O( T ), which is comparable to the best known bound for general convex online learning problems.

Corollary 4.4. Under the same conditions of Theorem 4.2, for all T  1, the regret of Algorithm 1 satisfies RT = O T .

Corollary 4.4 suggests that Padam attains RT = o(T ) for all situations (no matter whether the data features are sparse or not). This suggests that Algorithm 1 indeed converges to the optimal solution when the loss functions are convex, as shown by the fact that limT  RT /T  0.

5 EXPERIMENTS
In this section, we empirically evaluate our proposed algorithm for training various modern deep learning models and test them on several standard benchmarks4. We show that for nonconvex loss functions in deep learning, our proposed algorithm still enjoys a fast convergence rate, while its generalization performance is as good as SGD with momentum and much better than existing adaptive gradient methods such as Adam and Amsgrad.

5.1 ENVIRONMENTAL SETUP
All experiments are conducted on Amazon AWS p3.8xlarge servers which come with Intel Xeon E5 CPU and 4 NVIDIA Tesla V100 GPUs (16G RAM per GPU). All experiments are implemented in Pytorch platform version 0.4.1 within Python 3.6.4.

5.2 BASELINE ALGORITHMS
We compare our proposed algorithm against several state-of-the-art algorithms, including: (1) SGD with momentum (2) Adam (Kingma & Ba, 2015) (3) Amsgrad (Reddi et al., 2018).
4The code to reproduce the experimental results is available online. The URL is removed for double-blind review

5

Under review as a conference paper at ICLR 2019

Table 1: Test accuracy of VGGNet on CIFAR-10. Bold number indicates the best result.

Methods

25th Epoch

Test Accuracy (%)

50th Epoch

75th Epoch

100th Epoch

SGD Momentum Adam Amsgrad Padam

84.25 ± 0.59 87.55 ± 0.48 88.73 ± 0.41 87.73 ± 0.60

91.01 ± 0.09 90.74 ± 0.21 91.62 ± 0.12 92.11 ± 0.27

92.64 ± 0.14 91.43 ± 0.36 91.87 ± 0.07 92.85 ± 0.23

92.72 ± 0.08 91.41 ± 0.04 92.04 ± 0.06 92.86 ± 0.11

Note that we do not perform the projection step explicitly in all experiments as in Reddi et al. (2018). Also both Amsgrad and Padam will perform the bias correction step as in Adam for a fair comparison.
5.3 DATASETS / PARAMETER SETTINGS / CNN ARCHITECTURES
We use several popular datasets for image classifications: CIFAR-10 (Krizhevsky & Hinton, 2009), CIFAR-100 (Krizhevsky & Hinton, 2009), ImageNet dataset (ILSVRC2012) (Deng et al., 2009). We adopt three popular CNN architectures to evaluate the performance of our proposed algorithms: VGGNet (Simonyan & Zisserman, 2014), Residual Neural Network (ResNet) (He et al., 2016), Wide Residual Network(Zagoruyko & Komodakis, 2016). We test all tasks for 100 epochs. For all experiments, we use a fixed multi-stage learning rate decaying scheme: we decay the learning rate by 0.1 at the 30th, 60th and 80th epochs. We perform grid search on validation set to choose the best base learning rate for all algorithms, the partially adaptive parameter p and also the second order momentum parameter 2 for all the adaptive gradient methods.
Details about the datasets, CNN architectures and all the specific model parameters used in the following experiments can be found in the supplementary materials.

Train Loss

1.0 0.5 0.5

SGD-Momentum

SGD-Momentum

SGD-Momentum

0.8

Adam

0.4

Adam

0.4

Adam

Amsgrad

Amsgrad

Amsgrad

0.6

Padam

0.3

Padam

0.3

Padam

Train Loss

Train Loss

0.4 0.2 0.2

0.2 0.1 0.1

0.0 0 10 20 30 40Epo50chs60 70 80 90 100 (a) Train Loss for VGGNet

0.0 0 10 20 30 40Epo50chs60 70 80 90 100 (b) Train Loss for ResNet

0.0 0 10 20 30 40Epo50chs60 70 80 90 100 (c) Train Loss for Wide ResNet

0.30 SGD-Momentum 0.25 Adam
Amsgrad 0.20 Padam
0.15 0.10
0.05 0 10 20 30 40Epo50chs60 70 80 90 100
(d) Test Error for VGGNet

Test Error

0.20 0.18

SGD-Momentum Adam

0.16 Amsgrad

0.14 Padam

0.12

0.10

0.08

0.06
0 10 20 30 40Epo50chs60 70 80 90 100

(e) Test Error for ResNet

Test Error

0.20 0.18

SGD-Momentum Adam

0.16 Amsgrad

0.14 Padam

0.12

0.10

0.08

0.06
0 10 20 30 40Epo50chs60 70 80 90 100

(f) Test Error for Wide ResNet

Test Error

Figure 1: Train loss and test error (top-1 error) of three CNN architectures on Cifar-10. In all cases, Padam achieves the fastest training procedure among all methods and generalizes slightly better than SGD-momentum.
5.4 EXPERIMENTAL RESULTS
We compare our proposed algorithm with other baselines on training the aforementioned three modern CNN architectures for image classification on the CIFAR-10, CIFAR-100 and also ImageNet datasets. Due to the paper length limit, we leave all our experimental results regarding CIFAR-100 dataset in the supplementary materials. Figures 1 plots the train loss and test error (top-1 error) against training epochs on the CIFAR-10 dataset. As we can see from the figure, at the early stage

6

Under review as a conference paper at ICLR 2019

Test Error

0.9 0.8

SGD-Momentum Adam

0.7 Amsgrad

0.6 Padam

0.5

0.4

0.3

0.2 0 10 20 30 40Epo50chs60 70 80 90 100

(a) Top-1 Error for VGGNet

Test Error

0.6 SGD-Momentum 0.5 Adam
Amsgrad 0.4 Padam
0.3
0.2
0.1
0 10 20 30 40Epo50chs60 70 80 90 100 (b) Top-5 Error for VGGNet

0.70 0.65 0.60 0.55 0.50

SGD-Momentum Adam Amsgrad Padam

0.45

0.40

0.35

0.30

0.25 0 10 20 30 40Epo50chs60 70 80 90 100

(c) Top-1 Error for ResNet

Test Error

0.6
SGD-Momentum 0.5 Adam
Amsgrad 0.4 Padam
0.3
0.2
0.1
0 10 20 30 40Epo50chs60 70 80 90 100 (d) Top-5 Error for ResNet

Test Error

Figure 2: Top-1 and Top-5 error for VGGNet and ResNet on ImageNet dataset. In all cases, Padam achieves the fastest training speed and generalizes as well as SGD with momentum.

of the training process, (partially) adaptive gradient methods including Padam, make rapid progress lowing both the train loss and the test error, while SGD with momentum converges relatively slowly. After the first learning rate decaying at the 30-th epoch, different algorithms start to behave differently. SGD with momentum makes a huge drop while fully adaptive gradient methods (Adam and Amsgrad) start to generalize badly. Padam, on the other hand, maintains relatively good generalization performance and also holds the lead over other algorithms. After the second decaying at the 60-th epoch, Adam and Amsgrad basically lose all the power of traversing through the parameter space due to the "small learning dilemma", while the performance of SGD with momentum finally catches up with Padam. Overall we can see that Padam indeed achieves the best from both worlds(i.e., Adam and SGD with momentum): it maintains faster convergence rate while also generalizing as well as SGD with momentum in the end. Table 1 shows the test accuracy of VGGNet on CIFAR-10 dataset, from which we can also observe the fast convergence of Padam. Specifically, Padam achieves the best test accuracy at 50th, 75th and 100th epochs for CIFAR-10, except for the 25th, where fully adaptive methods like Adam and Amsgrad still enjoy their fast early stage convergence performances. This suggests that practitioners should once again, use fast to converge partially adaptive gradient methods for training deep neural networks, without worrying about the generalization performances.

(a) Sensitivity of p

|| t 0||2

70

60

50

40

30 SGD-Momentum

20 10

Adam Amsgrad Padam

0 0 10 20 30 40Epo50chs60 70 80 90 100

(b) t - 0 2 (p = 1/8)

Figure 3: Plots for sensitivity of p (Left) and t - 0 2 against training epochs (Right). Both experiments adopts ResNet on CIFAR-10 dataset.

7

Under review as a conference paper at ICLR 2019
Figure 2 plots the Top-1 and Top-5 error against training epochs on the ImageNet dataset for both VGGNet and ResNet5. We can see that on ImageNet, all methods behave similarly as in our CIFAR10 experiments. Padam method again obtains the best from both worlds by achieving the fastest convergence while generalizing as well as SGD with momentum. Even though Amsgrad has greatly improve the performance comparing with Adam, it still suffers from the generalization gap on ImageNet.
We also test the influence of different partial adaptive parameter p on the performance of our proposed method. Figure 3(a) shows the comparison of test error performances under the different partial adaptive parameter p for ResNet on CIFAR-10 dataset. We can observe that a larger p will lead to fast convergence at early stages and worse generalization performance later, while a smaller p behaves more like SGD with momentum: slow in early stages but finally catch up. While a proper choice of p (e.g., 1/8), could obtain the best of both worlds. As a side product of our experiments, we also examine the distance that t has traversed in the parameter space, i.e., t - 0 2, as shown in many other studies (Hoffer et al., 2017; Xing et al., 2018) discussing the generalization performance of various optimization algorithms for deep learning. Figure 3(b) shows the plot of the Euclidean distance of weight vector t from initialization 0, against training epochs for ResNet on CIFAR-10 dataset. First, we would like to emphasize that these are the plots only regrading the Euclidean distance, which does not contain any directional information. In other words, the cross in the plot does not mean that the weight vectors in different optimization algorithms actually meet somewhere during the training process. As we can see from the plots, SGD with momentum tends to overshoot a lot at the early stage of the entire training process. This could explain why the convergence of SGD with momentum is slower at early stages. And for Adam/Amsgrad, despite the quick start and less overshooting, the "small learning rate dilemma" (see Section 3) largely limits the weight vector's ability of exploring the parameter space in the middle and later stages, which could explain the bad generalization performance of Adam/Amsgrad. On the other hand, Padam behaves inbetween SGD with momentum and Adam/Amsgrad, with less severe overshooting compared with SGD with momentum, while maintaining a better capability of traversing through the parameter space compared with Adam/Amsgrad. This partly justifies the outstanding generalization performance of Padam.
6 RELATED WORK
We briefly review the related work in this section. There are only several studies closely related to improving the generalization performance of Adam. Zhang et al. (2017) proposed a normalized direction-preserving Adam (ND-Adam), which changes the adaptive terms from individual dimensions to the whole gradient vector. This makes it more like a SGD-momentum with learning rate adjusted in each iteration, rather than an adaptive gradient algorithm. The empirical result also shows that its performance resembles SGD with momentum. Keskar & Socher (2017) proposed to improve the generalization performance by switching from Adam to SGD. Yet the empirical result shows that it actually sacrifices some of the convergence speed for better generalization rather than achieving the best from both worlds. Also deciding the switching learning rate and the best switching point requires extra efforts on parameter tuning since they can be drastically different for different tasks according to the paper. Loshchilov & Hutter (2017) proposed to fix the weight decay regularization in Adam by decoupling the weight decay from the gradient update and this improves the generalization performance of Adam.
7 CONCLUSIONS AND FUTURE WORK
In this paper, we proposed Padam, which unifies Adam/Amsgrad with SGD-momentum. With an appropriate choice of the partially adaptive parameter, we show that Padam can achieve the best from both worlds, i.e., maintaining fast convergence rate while closing the generalization gap. We also provide a theoretical analysis towards the convergence rate of Padam and show a similar datadependent regret bound as in Duchi et al. (2011); Reddi et al. (2018).
It would be interesting to see how well Padam performs in other types of nerual networks, such as recurrent neural networks (RNNs) (Hochreiter & Schmidhuber, 1997) and generative adversarial networks (GANs) (Goodfellow et al., 2014). We leave it as a future work.
5We did not conduct WideResNet experiment on Imagenet dataset due to GPU memory limits.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. Ieee, 2009.
Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent, 2012.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, pp. 1729­1739, 2017.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 2017.
H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. COLT 2010, pp. 244, 2010.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91­99, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
9

Under review as a conference paper at ICLR 2019
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4151­4161, 2017.
Saining Xie, Ross Girshick, Piotr Dolla´r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pp. 5987­5995. IEEE, 2017.
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv preprint arXiv:1802.08770, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
Zijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu. Normalized direction-preserving adam. arXiv preprint arXiv:1709.04546, 2017.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 928­936, 2003.
10

Under review as a conference paper at ICLR 2019

A PROOF OF THE MAIN RESULTS

A.1 PROOF OF THEOREM 4.2

The proof of Theorem 4.2 needs the following lemmas.

Lemma A.1. Under the same conditions as in Theorem 4.2, we have

T t=1

d i=1

t · mt2,i vtp,i



G(1-2p)1 + log T (1 - 1)(1 - )(1 - 2)p

d i=1

g1:T ,i 2.

(A.1)

Lemma A.1 basically describes the bound of the key quantity (

T t=1

d i=1

t

·

m2t,i/vtp,i)

in

the

con-

vergence analysis of Algorithm 1. Its proof is inspired by Lemma 2 in Reddi et al. (2018). Note that

the original proof in Kingma & Ba (2015) for the corresponding quantity contains some mistakes,

which result in a bound without the logarithmic term in (A.1). Here we follow the corrected version

in Reddi et al. (2018).

Lemma A.2. For any V  S+d + and convex feasible set X  Rd, suppose a1 = X ,Vp (b1), a2 = X ,Vp (b2) for p  (0, 1/2], we have

Vp(a1 - a2) 2  Vp(b1 - b2) 2,

(A.2)

where p  (0, 1/2] is an absolute constant.

Lemma A.2 is an adaptation of Lemma 5 in McMahan & Streeter (2010) (or Lemma 4 in Reddi et al. (2018)).
Now we are ready to prove Theorem 4.2.

Proof of Theorem 4.2. By Assumption 4.1, all ft's are convex, we have

TT

ft(t) - ft(t) 

gt, t -  .

t=1 t=1

(A.3)

Consider the update rule for Algorithm 1, let Vt = diag(vt), then we have t+1 = X ,Vtp t - tVt-p · mt . Since X ,Vtp () =  for all   X , by Lemma A.2 we have

Vtp/2(t+1 - )

2
2

Vtp/2(t - tVt-p · mt - ) 22.

(A.4)

Now expand the square term on the right hand side of (A.4), we have

Vtp/2(t+1 - )

2 2



=

Vtp/2(t - )

2 2

+

t2

Vt-p/2mt

2 2

-

2t

mt, t - 

Vtp/2(t - )

2 2

+

t2

Vt-p/2mt

2 2

-

2t

1tmt-1 + (1 - 1t)gt, t -  ,

(A.5)

where the second equality follows from the definition for mt. Rearrange the items in (A.5), we have

gt, t - 

1 = 2t(1 - 1t)

Vtp/2(t+1 - )

2 2

-

Vtp/2(t - )

2 2

+

2(1

t - 1t)

·

Vt-p/2mt

2 2

- 1t 1 - 1t

mt-1, t - 

1 2t(1 - 1t)

Vtp/2(t+1 - )

2 2

-

Vtp/2(t - )

2 2

+ t · 2(1 - 1t)

Vt-p/2mt

2 2

+ 1tt-1 · 2(1 - 1t)

Vt--p1/2mt-1

2 2

+

1t 2t-1(1 -

1t)

Vtp-/21(t - ) 22,

(A.6)

11

Under review as a conference paper at ICLR 2019

where the inequality holds due to Young's inequality. Summing over (A.6) for all 1  t  T and submitting it back into (A.3), we obtain

T
ft(t) - ft(t)
t=1

T


1

t=1 2t(1 - 1t)

Vtp/2(t+1 - )

2-
2

Vtp/2(t - )

2 2

+

T t=2

1t 2t-1(1 -

1t)

Vtp-/21(t - )

2 2

+

T t=2

1tt-1 2(1 - 1t)

·

Vt--p1/2mt-1

2 2

+

T t=1

2(1

t - 1t)

·

Vt-p/2mt

2 2

Td


vtp,i

t=1 i=1 2t(1 - 1)

(t+1,i - i)2 - (t,i - i)2

T
+
t=1

d i=1

1t · vtp,i 2t(1 - 1

)

(t+1,i

-

i)2

I1

+

1 + 1 2(1 - 1)

T t=1

d i=1

t

· m2t,i vtp,i

,

I2
(A.7)

I3

where the last inequality holds due to the fact that 1t = 1t-1 is monotonically decreasing with t. For term I1, we have

Td

vtp,i

t=1 i=1 2t(1 - 1)

(t+1,i - i)2 - (t,i - i)2

1 =

d v1p,i · (1,i - i)2 +

1

Td

2(1 - 1) i=1

1

2(1 - 1) t=2 i=1

 D2 2(1 - 1)

d v1p,i + T d

i=1 1

t=2 i=1

vtp,i - vtp-1,i t t-1

= D2

d vTp ,i

2(1 - 1) i=1 T

=

D2 2(1 - 1)

d i=1

 T

·

vTp ,i,

vtp,i - vtp-1,i t t-1

(t,i - i)2

(A.8)

where the inequality follows from the bounded diameter condition and the definition of vt ensures that vtp,i/t - vtp-1,i/t-1 > 0, and the second equality holds due to simple telescope sum. For term I2, note that simple calculation yields that vtp,i  G2p, we have

T t=1

d i=1

1tvtp,i 2t(1 - 1)

(t+1,i

- i)2



1dD2 G2p 2(1 - 1)

T tt-1
t=1



1dD2 G2p 2(1 - 1)(1 -

)2

,

(A.9)

 where the last inequality follows from first relaxing t to t and then using the geometric series summation rule. For term I3, by Lemma A.1 we have

1 + 1 2(1 - 1)

T t=1

d i=1

t · m2t,i vtp,i



G(1-2p)

 1

+

log

T

(1 - 1)2(1 - )(1 - 2)p

d i=1

g1:T ,i 2.

(A.10)

Submitting (A.8), (A.9) and (A.10) back into (A.7) yields the desired result.

12

Under review as a conference paper at ICLR 2019

A.2 PROOF OF COROLLARY 4.4

Proof of Corollary 4.4. By Theorem 4.2, we have

RT



D2 2(1 - 1)

d T
i=1

· vTp ,i +

G(1-2p)1 + log T (1 - 1)2(1 - )(1 - 2)p

d i=1

+

1dD2 G2p 2(1 - 1)(1 -

)2

.

g1:T ,i 2

Note that we have

dd
g1:T ,i 2 
i=1 i=1

T gt2,i  dG T ,
t=1

and by definition we also have

vtp,i =

Tp
(1 - 2) 2T -j gj2,i  (1 - 2)p · G2p
j=1

T
2T -j
j=1

p
 G2p.

Combine the above results we can easily have
 RT = O T .

This completes the proof.

B PROOF OF TECHNICAL LEMMAS IN APPENDIX A

B.1 PROOF OF LEMMA A.1

Proof. Consider

T t=1

d i=1

t · m2t,i vtp,i

T -1 d
=
t=1 i=1

t · m2t,i vtp,i

+

d i=1

T · mT2 ,i vTp ,i



T -1 t=1

d i=1

t · mt2,i vtp,i

+

d i=1

T · mT2 ,i vTp ,i

T -1 d

t=1 i=1

t · m2t,i vtp,i

+  T

d i=1

jT=1(1 - 1j )1T -j gj,i

(1 - 2)

T j=1

2T

-j

gj2,i

2
p,

(B.1)

where the first inequality holds due to the definition of vT,i and the second inequality follows from the fact that 1t = 1t-1 and   (0, 1). (B.1) can be further bounded as:

Td t=1 i=1

t · m2t,i vtp,i

T -1 d

t=1 i=1

t · mt2,i vtp,i

+

 T (1 - 2)p

d i=1

T j=1

1T

-j

gj(,1i-2p)

T j=1

1T

-j

gj(,1i+2p)

T j=1

2T

-j

gj2,i

p



T -1 d t=1 i=1

t · mt2,i vtp,i

+

 G(1-2p) T (1 - 1)(1 - 2)p

d i=1

T j=1

1T

-j

gj(,1i+2p)

T j=1

2T

-j

gj2,i

p



T -1 t=1

d i=1

t · m2t,i vtp,i

+

 G(1-2p) T (1 - 1)(1 - 2)p

T j=1

d i=1

1T -j gj(,1i+2p) 2T -j gj2,i p



T -1 t=1

d i=1

t · mt2,i vtp,i

+

 G(1-2p) T (1 - 1)(1 - 2)p

T j=1

d i=1

T -j |gj,i|,

where the first inequality holds due to Cauchy-Schwarz inequality and the fact that 0  1j < 1,

the second inequality follows from the bounded gradient condition and

T j=1

1T

-j



1/(1 - 1),

13

Under review as a conference paper at ICLR 2019

and

the

last

inequality

is

due

to

the

fact

that

1/2p



 1/ 2

=

.

Now

repeat

the

above

process

for 1  t  T , we have

Td t=1 i=1

t · mt2,i vtp,i



G(1-2p) (1 - 1)(1 - 2)p

T t=1

1 t

td
t-j |gj,i|
j=1 i=1

=

G(1-2p) (1 - 1)(1 - 2)p

d i=1

TT
|gt,i|
t=1 j=t

j-t j



G(1-2p) (1 - 1)(1 - 2)p

d i=1

T t=1

|gt,i| t

T
 j -t
j=t



G(1-2p) (1 - 1)(1 - 2)p

d i=1

T t=1

|gt,i| , (1 - ) t

(B.2)

where the equality holds due to the change of the order of summation, the last inequality follows

from the fact that

T j=t

 j -t



1/(1 - ).

By

Cauchy-Schwarz

inequality

,

(B.2) can

be further

written as

T t=1

d i=1

t · mt2,i vtp,i



G(1-2p) (1 - 1)(1 - )(1 - 2)p

d i=1



G(1-2p)

 1

+

log

T

d

(1 - 1)(1 - )(1 - 2)p i=1

This completes the proof.

g1:T ,i 2 g1:T ,i 2.

T1 t
t=1

B.2 PROOF OF LEMMA A.2

Proof. The proof is inspired from Lemma 4 in Reddi et al. (2018) and Lemma 5 in McMahan & Streeter (2010). Since a1 = argminxX Vp(x - b1) 2 and a2 = argminxX Vp(x - b2) 2, by projection property we have

V2p(a1 - b1), a2 - a1  0,

V2p(a2 - b2), a1 - a2  0.

Combine the above inequalities we have

V2p(a1 - b1), a2 - a1 - V2p(a2 - b2), a2 - a1  0.

(B.3)

Rearrange (B.3) yields that

(b2 - b1) V2p(a2 - a1)  (a2 - a1) V2p(a2 - a1).

(B.4)

Also since V is positive definite, the fact that (b2 - b1 - (a2 - a1)) V2p(b2 - b1 - (a2 - a1))  0 immediately implies that

(b2 - b1) V2p(b2 - b1)  -(a2 - a1) V2p(a2 - a1) + 2(a2 - a1) V2p(b2 - b1). (B.5)

Combining (B.4) and (B.5) we have the desired result.

C EXPERIMENT DETAILS
C.1 DATASETS
We use several popular datasets for image classifications.
· CIFAR-10 (Krizhevsky & Hinton, 2009): it consists of a training set of 50, 000 32 × 32 color images from 10 classes, and also 10, 000 test images.
· CIFAR-100 (Krizhevsky & Hinton, 2009): it is similar to CIFAR-10 but contains 100 image classes with 600 images for each.
· ImageNet dataset (ILSVRC2012) (Deng et al., 2009): ILSVRC2012 contains 1.28 million training images, and 50k validation images over 1000 classes.

14

Under review as a conference paper at ICLR 2019

C.2 PARAMETER SETTINGS
We perform grid searches to choose the best base learning rate for all algorithms over {0.0001, 0.001, 0.01, 0.1, 1}, the partial adaptive parameter p from {1/4, 1/8, 1/16} and the second order momentum parameter 2 from {0.9, 0.99, 0.999}. For SGD with momentum, the base learning rate is set to be 0.1 with a momentum parameter of 0.9. For Adam and Amsgrad, we set the base learning rate as 0.001. For Padam, the base learning rate is set to be 0.1 and the partially adaptive parameter p is set to be 1/8 due to its best empirical performance. We also tune the momentum parameters for all the adaptive gradient methods. After tuning, for Adam, Amsgrad, the momentum parameters are set to be 1 = 0.9, 2 = 0.99. And for Padam, we set 1 = 0.9, 2 = 0.999. All the algorithms tested also come with a weight decay factor of 5 × 10-4. All experiments use cross-entropy as the loss function. The minibatch sizes for CIFAR-10 and CIFAR-100 are set to be 128 and for ImageNet dataset we set it to be 256.
C.3 CNN ARCHITECTURES
VGGNet (Simonyan & Zisserman, 2014): We use a modified VGG-16 architecture for this experiment. The VGG-16 network uses only 3 × 3 convolutional layers stacked on top of each other for increasing depth and adopts max pooling to reduce volume size. Finally, two fully-connected layers 6 are followed by a softmax classifier.
ResNet (He et al., 2016): Residual Neural Network (ResNet) introduces a novel architecture with "skip connections" and features a heavy use of batch normalization. Such skip connections are also known as gated units or gated recurrent units and have a strong similarity to recent successful elements applied in RNNs. We use ResNet-18 for this experiment, which contains 2 blocks for each type of basic convolutional building blocks in He et al. (2016).
Wide ResNet (Zagoruyko & Komodakis, 2016): Wide Residual Network further exploits the "skip connections" used in ResNet and in the meanwhile increases the width of residual networks. In detail, we use the 16 layer Wide ResNet with 4 multipliers (WRN-16-4) in our experiments.

D ADDITIONAL EXPERIMENTS
Figure 4 plots the train loss and test error (top-1 error) against training epochs on the CIFAR-10 dataset and the CIFAR-100 dataset respectively. We can see that Padam achieves the best from both worlds by maintaining faster convergence rate while also generalizing as well as SGD with momentum in the end.

Table 2: Test accuracy of VGGNet on CIFAR-100. Bold number indicates the best result.

Methods

25th Epoch

Test Accuracy (%)

50th Epoch

75th Epoch

100th Epoch

SGD Momentum Adam Amsgrad Padam

52.12 ± 0.65 52.35 ± 0.47 58.39 ± 0.36 60.28 ± 0.25

66.53 ± 0.33 61.73 ± 0.30 65.31 ± 0.31 69.69 ± 0.30

70.43 ± 0.24 62.18 ± 0.15 66.32 ± 0.25 71.05 ± 0.24

70.78 ± 0.11 62.20 ± 0.13 66.36 ± 0.14 71.10 ± 0.08

Table 2 shows the test accuracy of VGGNet on CIFAR-100 dataset. We can see that Padam achieves the best test accuracy at 25th, 50th, 75th and 100th epochs for CIFAR-100 dataset.
Figure 5(a) shows the comparison of test error performances under the different partial adaptive parameter p for ResNet on CIFAR-100 dataset. We can observe that a larger p will lead to fast convergence at early stages and worse generalization performance later, while a smaller p leads to slow convergence but finally good generalization performances. While a proper choice of p could obtain the best of both worlds.
6For CIFAR experiments, we change the ending two fully-connected layers from 2048 nodes to 512 nodes. For ImageNet experiments, we use batch normalized version (vgg16 bn) provided in Pytorch official package

15

Under review as a conference paper at ICLR 2019

Train Loss

3.0 SGD-Momentum

2.5 Adam

2.0

Amsgrad Padam

1.5

1.0

0.5

0.0 0 10 20 30 40Epo50chs60 70 80 90 100

(a) Train Loss for VGGNet

Train Loss

1.4 SGD-Momentum

1.2 Adam

1.0

Amsgrad Padam

0.8

0.6

0.4

0.2

0.0 0 10 20 30 40Epo50chs60 70 80 90 100

(b) Train Loss for ResNet

Train Loss

1.4 SGD-Momentum

1.2 Adam

1.0

Amsgrad Padam

0.8

0.6

0.4

0.2

0.0 0 10 20 30 40Epo50chs60 70 80 90 100

(c) Train Loss for Wide ResNet

0.8 0.7 0.6

SGD-Momentum Adam Amsgrad Padam

0.50 0.45 0.40

SGD-Momentum Adam Amsgrad Padam

0.50 0.45 0.40

SGD-Momentum Adam Amsgrad Padam

0.5 0.35 0.35

Test Error

Test Error

0.4 0.30 0.30

0.3 0.25 0.25

0 10 20 30 40Epo50chs60 70 80 90 100

0 10 20 30 40Epo50chs60 70 80 90 100

0 10 20 30 40Epo50chs60 70 80 90 100

(d) Test Error for VGGNet

(e) Test Error for ResNet

(f) Test Error for Wide ResNet

Test Error

Figure 4: Train loss and test error (top-1 error) of three CNN architectures on CIFAR-100. In all cases, Padam achieves the fastest training procedure among all methods and generalizes as well as SGD with momentum.

(a) Sensitivity of p

|| t 0||2

90 80 70 60 50
40 SGD-Momentum 30 Adam 20 Amsgrad 10 Padam 0 0 10 20 30 40Epo50chs60 70 80 90 100
(b) t - 0 2 (p = 1/8)

Figure 5: Plots for sensitivity of p (Left) and t - 0 2 against training epochs (Right). Both experiments adopts ResNet on CIFAR-100 dataset.

Figure 5(b) shows the plot of the Euclidean distance of weight vector t from initialization 0, against training epochs for ResNet on CIFAR-100 dataset. As we can see from the plots, SGD with momentum tends to overshoot a lot at the early stage of the entire training process. And for Adam/Amsgrad, despite the quick start and less overshooting, the "small learning rate dilemma" largely limits the weight vector's ability of exploring the parameter space in the middle and later stages. On the other hand, Padam behaves inbetween SGD with momentum and Adam/Amsgrad, with less severe overshooting compared with SGD with momentum, while maintaining a better capability of traversing through the parameter space compared with Adam/Amsgrad.

16

