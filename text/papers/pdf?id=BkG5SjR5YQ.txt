Under review as a conference paper at ICLR 2019
POST SELECTION INFERENCE WITH INCOMPLETE MAXIMUM MEAN DISCREPANCY ESTIMATOR
Anonymous authors Paper under double-blind review
ABSTRACT
Measuring divergence between two distributions is essential in machine learning and statistics and has various applications including binary classification, change point detection, and two-sample test. Furthermore, in the era of big data, designing divergence measure that is interpretable and can handle high-dimensional and complex data becomes extremely important. In this paper, we propose a post selection inference (PSI) framework for divergence measure, which can select a set of statistically significant features that discriminate two distributions. Specifically, we employ an additive variant of maximum mean discrepancy (MMD) for features and introduce a general hypothesis test for PSI. A novel MMD estimator using the incomplete U-statistics, which has an asymptotically normal distribution (under mild assumptions) and gives high detection power in PSI, is also proposed and analyzed theoretically. Through synthetic and real-world feature selection experiments, we show that the proposed framework can successfully detect statistically significant features. Last, we propose a sample selection framework for analyzing different members in the Generative Adversarial Networks (GANs) family.
1 INTRODUCTION
Computing the divergence between two probability distributions is fundamental to machine learning and has many important applications such as binary classification (Friedman et al., 2001), change point detection (Yamada et al., 2013a; Liu et al., 2013), two-sample test (Gretton et al., 2012; Yamada et al., 2013b), and generative models such as generative adversarial networks (GANs) (Goodfellow et al., 2014; Li et al., 2015b; Nowozin et al., 2016), to name a few. Recently, interpreting the difference between distributions has become an important task in applied machine learning (Mueller & Jaakkola, 2015; Jitkrittum et al., 2016) since it can facilitate scientific discovery. For instance, in biomedical binary classification tasks, it is common to analyze which variables or features are different between two different distributions (classes).
The simplest approach to measure divergence between two probability densities would be parametric methods. For example, t-test can be used if the distributions to be compared are known and well-defined (Anderson, 2001). However, in many real-world problems, the property of distribution is not known a priori, and therefore model assumptions are likely to be violated. In contrast, non-parametric methods can be applied to any distributions without prior assumptions. The maximum mean discrepancy (MMD) (Gretton et al., 2012) is an example of non-parametric discrepancy measures and is defined as the difference between the mean embeddings of two distributions in a reproducing kernel Hilbert space (RKHS). Due to the mean embeddings in RKHS, all moment information is stored. Thus in comparing the means of a characteristic kernel, MMD captures nonlinearity in data and can be computed in a closed-form. However, since MMD considers the entire d dimensional vector, it is hard to interpret how individual features contribute to the discrepancy.
To deal with the interpretability issue, divergence measure with feature selection has been actively studied (Yamada et al., 2013a; Mueller & Jaakkola, 2015; Jitkrittum et al., 2016). For instance, in Mueller & Jaakkola (2015), the Wasserstein divergence is employed as a divergence measure and 1 regularizer is used for feature selection. However, these approaches focus on detecting a set of features that discriminate two distributions. But for scientific discovery applications such as biomarker discovery, it might be preferable to test the significance of each selected feature (e.g., one biomarker). A naive approach would be to select features from one dataset and then test the selected
1

Under review as a conference paper at ICLR 2019

D 22 . I C I G IIED I B# (
D G I 3EGC B

BE A 22 . I C I 7 G C I B#

1 D G 22 . I C I
G IIED I B# 0D EC B I 22 . I C I
GE E

,ED I ED =EG C IEI BB 3EGC B
,ECCED , E
,EC B I

D

BE A  

BE A ) D



D% BE A

CB D#



C B ) GD

CB

Figure 1: Comparison between Unbiased, Block, Linear, and Incomplete MMD estimation.

features using the same data. However, in such case, selection bias is included and thus the false positive rate cannot be controlled. Therefore, it is crucial in hypothesis testing that the selection event should be taken into account to correct the bias. To the best of our knowledge, there is not yet an existing framework that tests the significance of selected features that distinguish between two different distributions on the same dataset.
In this paper, we propose mmdInf, a post selection inference (PSI) algorithm for distribution comparison, which finds a set of statistically significant features that discriminate between two distributions. mmdInf enjoys several compelling properties. First, it is a non-parametric method based on kernels, and thus it can detect features that distinguish various types of distributions. Second, the proposed framework is general and can be applied to not only feature selection but also other distribution comparison problems, such as dataset comparison. In mmdInf, we employ the recently developed PSI algorithm (Lee et al., 2016) and use MMD as a divergence measure. However, the standard empirical squared MMD estimator has a degenerated null distribution, which violates the requirement of a proper Normal distribution for the current PSI algorithm. To address this issue, we apply the block estimate (Zaremba et al., 2013) and the linear estimate (Gretton et al., 2012) of MMD. Furthermore, we propose a new empirical estimate of MMD based on the incomplete U-statistics (incomplete MMD estimator) (Blom, 1976; Janson, 1984; Lee, 1990) and show that it asymptotically follows the normal distribution, and has much greater detection power (compared to the existing estimators of MMD) in PSI. Finally, we propose a framework to analyze different members in the GAN (Goodfellow et al., 2014) family based on mmdInf. We elucidate the theoretical properties of the incomplete U-statistics estimate of MMD and show that mmdInf can successfully detect significant features through feature selection experiments.
The contributions of our paper are summarized as follows: (1) We propose a non-parametric PSI algorithm mmdInf for distribution comparison. (2) We propose the incomplete MMD estimator and investigate its theoretical properties (see Figure 1). (3) We propose a sample selection framework based on mmdInf that can be used for analyzing generative models.

2 RELATED WORK
There exist a number of divergence measures including the Kullback-Leibler divergence (Cover & Thomas, 2012), f -divergence (Ali & Silvey, 1966), -divergence (Re´nyi et al., 1961), K-nearest neighbor approaches (Po´czos & Schneider, 2011), density-ratio based approaches (Yamada et al., 2013b; Sugiyama et al., 2008; Kanamori et al., 2009), etc. Among these divergence measures, we are interested in Kernel Mean Embedding (KME) approaches. For instance, the maximum mean discrepancy (MMD) (Gretton et al., 2012) is a widely used kernel based divergence that compares the means of two distributions in a reproducing kernel Hilbert space (RKHS). Although these divergence measures can be used for testing the discrepancy between p(x) and q(x), it is hard to test the significance of one of k selected features from the entire d features, where the setup is useful for scientific discovery tasks such as biomarker discovery. Recently, a MMD-based change point detection algorithm, which can compute the p-value of the maximum MMD score, has been proposed (Li et al., 2015a). However, this method can only test the maximum MMD score (i.e., one-feature). Thus, it is not clear whether the approach can be extended to feature selection in general.

2

Under review as a conference paper at ICLR 2019

A novel testing framework for the post selection inference (PSI) has been recently proposed (Lee et al., 2016), in which statistical inference after feature selection with Lasso (Tibshirani, 1996) is investigated. This work shows that statistical inference conditioned on the selection event can be done for linear regression models with Gaussian noise if the selection event can be written as a set of linear constraints. However, the PSI algorithm needs to assume a Gaussian response, which is a relatively strong assumption, and consequently it cannot be directly applied for non-Gaussian output problems such as classification. To deal with this issue, a kernel based PSI algorithm (hsicInf) for independence test has been proposed (Yamada et al., 2018), in which the Hilbert-Schmidt Independence Criterion (HSIC) (Gretton et al., 2005) is employed to measure the independence between input and its output, and thus significance test can be performed on non-Gaussian data. In this paper, we propose an alternative kernel based inference algorithm for distribution comparison called mmdInf, which can be used for both feature selection and binary classification.
In Sections 3.4 and 5.3, we manifest how we analyze different members in the GANs (Goodfellow et al., 2014) family. Here, we discuss several evaluation metrics that have been proposed to compare generative models. For example, the inception scores (Salimans et al., 2016) and the mode scores (Che et al., 2016) measure the quality and diversity of the generated samples, but they were not able to detect overfitting and mode dropping / collapsing for generated samples. The Frechet inception distance (FID) (Heusel et al., 2017) defines a score using the first two moments of the real and generated distributions, whereas the classifier two-sample tests (Lopez-Paz & Oquab, 2016) considers the classification accuracy of a binary classifier as a statistic for two-sample test. Although the above metrics are reasonable in terms of discriminability, robustness, and efficiency, the distances between samples are required to be computed in a suitable feature space. We can also use the kernel density estimation (KDE); or more recently, Wu et al. (2016) proposed to apply the annealed importance sampling (AIS) to estimate the likelihood of the decoder-based generative models. Nevertheless, these approaches need the access to the generative model for computing the likelihood, which are less favorable comparing to the model agnostic approaches which rely only on a finite generated sample set. On the other hand, the maximum mean discrepancy (MMD) (Gretton et al., 2012) is preferred against its competitors (Sutherland et al., 2016; Huang et al., 2018). Therefore, we propose the mmdInf based GANs analysis framework.

3 PROPOSED METHOD (M M DIN F)

In this section, we introduce a PSI algorithm with MMD (Gretton et al., 2012).

3.1 POST SELECTION INFERENCE (PSI)

Suppose we are given independent and identically distributed (i.i.d.) samples X = {xi}im=1  Rd×m from a d-dimensional distribution p and i.i.d. samples Y = {yj}nj=1  Rd×n from another d-dimensional distribution q. Our goal is to find k < d features that differentiate between X and Y and test whether each selected feature is statistically significant.
Let S be a set of selected features, we consider the following hypothesis test:

· H0: · H1:

d s=1

s D(X (s) ,

Y

(s))

=

0

|

S

was

selected,

d s=1

s D(X (s) ,

Y

(s))

=0|S

was selected,

where D(X(s), Y (s)) is the estimated discrepancy measure for the selected feature s and  = [1, . . . , d]  Rd is an arbitrary pre-defined parameter. To test the s-th feature, we can set  as a
unit vector whose s-th position is 1 and zero otherwise.

We employ the post-selection inference (PSI) framework (Lee et al., 2016) to test the hypoth-

esis. Thanks to the cumulative distribution of Theorem 1, we can compute the p-value of

d s=1

s

D(X

(s)

,

Y

(s))

under

feature

selection.

Theorem 1 (Lee et al., 2016) Suppose that z  N (µ, ), and the feature selection event can be

expressed as Az  b for some matrix A and vector b, then for any given feature represented by

  Rn we have

F[V

- (A,b),V µ, 

+

(A,b)]

(

z)

|

Az  b  Unif(0, 1),

3

Under review as a conference paper at ICLR 2019

Algorithm 1 mmdInf (Feature Selection) H0 : 0 | S was selected

d s=1

s MMDi2nc (X (s) ,

Y

(s))

Input: Data matrices X = [Xsel Xc]  Rd×m and Y = [Ysel Yc]  Rd×n. Params:   Rd and k.
1: Compute z = [MMD2inc(Xs(e1l), Ys(e1l)), . . . , MMD2inc(Xs(edl), Ys(edl))]  Rd. 2: Select k features (i.e., k = |S|) and compute A using z and  using Xc and Yc. Set b = 0. 3: Compute p-value as p = 1 - F [V -(A,b),V +(A,b)]( z).
0, 

=

where Fµ[a,,b2](x) is the cumulative distribution function (CDF) of a truncated normal distribution

truncated at [a,b], and  is the CDF of standard normal distribution with mean µ and variance 2.

Given

that



=

A

  

,

the

lower

and

upper

truncation

points

are

given

by

V -(A, b) = max bj - (Az)j +  z, V +(A, b) = min bj - (Az)j +  z.

j:j <0

j

j:j >0

j

Feature Selection with Discrepancy Measure: Assume we have an estimate of a discrepancy measure for each feature: z = [D(X(1), Y (1)), . . . , D(X(d), Y (d))]  Rd  N (µ, ), where D(·, ·) has large positive value when two distribution are different. We select the top-k features with largest discrepancy scores. We denote the index set of the selected k features by S, and that of the unselected k¯ = d - k features by S¯. This feature selection event can be characterized by
D(X(s), Y (s))  D(X( ), Y ( )), for all (s, )  S × S¯.
Note that we have in total k · k¯ constraints.
The selection event can be rewritten as As, z  0, for all (s, )  S × S¯, where As, = [0 · · · 0 -1 0 · · · 0 1 0 · · · 0] and As,  Rd is a row vector of A  R(k·k¯)×d. Under such
s
construction, Az  b can be satisfied by setting b = 0.

3.2 MAXIMUM MEAN DISCREPANCY (MMD)

We employ the maximum mean discrepancy (MMD) (Gretton et al., 2012) as divergence measure.

Let F be the unit ball in a reproducing kernel Hilbert space (RKHS) and k(x, x ) the corresponding positive definite kernel, the squared population MMD is defined as
MMD2[F , p, q] = Ex,x [k(x, x )] - 2Ex,y[k(x, y)] + Ey,y [k(y, y )],
where Ex,y denotes the expectation over independent random variables x with distribution p and y with distribution q. It has been shown that if the kernel function k(x, x ) is characteristic, then MMD[F, X, Y ] = 0 if and only if p = q (Gretton et al., 2012). In the following, we introduce two existing MMD estimators and then propose the incomplete U-statistics MMD estimator.

(Complete) U-statistics estimator (Gretton et al., 2012): We use the Gaussian kernel: k(x, x ) =

exp

- x-x

2 2

2x2

, where x > 0 is the Gaussian width. When m = n, the complete U-statistics of

MMD is defined as

MMD2u[F , X, Y

]

=

1 m(m -

1)

h(ui, uj),

i=j

where h(u, u ) = k(x, x )+k(y, y )-k(x, y )-k(x , y) is the U-statistics kernel for MMD and u = [x y ]  R2d. However, since the complete U-statistics estimator of MMD is degenerated
under p = q and does not follow normal distribution, this estimator cannot be used in PSI.

Block estimator (Zaremba et al., 2013): The block estimate of MMD is given by

MMD2b [F , X, Y

]

=

B1 n

n/B1

MMD2u[F , Xi, Yi],

i=1

4

Under review as a conference paper at ICLR 2019

where X = [X1, . . . , Xm/B1 ], Y = [Y1, . . . , Yn/B2 ], and Xi  Rd×B1 and Yi  Rd×B2 are i-th partitioned data. Here, we assume that the number of blocks m/B1 = n/B2 is an integer and B = B1 = B2 in this paper. This estimator asymptotically follows the normal distribution when B1 and B2 are finite and m and n go to infinity. The block estimator can be used for PSI, but the variance and normality depends on the partition of X and Y . Specifically, when the total number
of samples is small, then a small block size would result in high variance, whereas larger block size
tends to result in non-Gaussian response.

Incomplete U-statistics estimator: The described problems of the block-estimator motivated us to design a new MMD estimator that is normally distributed and has smaller variance. We therefore propose an MMD estimator based on the incomplete U-statistics (Blom, 1976; Janson, 1984; Lee, 1990). The incomplete U-statistics estimator of MMD is given by

MMD2inc[F , X, Y ] = 1

h(ui, uj),

(i,j)D

where D is a subset of Sn,k = {(i, j)}i=j. D can be fixed design or random design. In particular, if we design D as D = {(1, 2), (3, 4), . . . , (n - 1, n)} and assume that n is an even number, then the
incomplete U-statistic corresponds to the linear-time MMD estimator (Gretton et al., 2012).

3.3 COVARIANCE MATRIX ESTIMATION
For PSI, we need to estimate the covariance matrix  from data.
Block estimator To estimate , we first compute Hb  Rd×(n/B) whose elements are the meansubtracted MMDs. Then, we regard Hb as the data matrix and use a standard covraiance estimator for estimating b. Note that for small n, the number of blocks n/B tends to be smaller than the dimension d, and thus, the estimation accuracy of covariance matrix is low and affects the detection accuracy. To handle this issue, we employ the POET algorithm Fan et al. (2013).
Incomplete U-statistics estimator To estimate , we first compute Hinc  Rd× whose elements are the mean-subtracted U-stat kernels. Then, we regard Hinc as the data matrix and can use a standard covraiance estimator to compute inc.
The estimation performance of the covariance estimation for the block estimate heavily depends on the block size B. That is, if B is large, we need to estimate b from a small number of samples. On the other hand, in the incomplete U-statistics estimator, we can set = rn n/B, where r > 0 is a constant (See the theoretical analysis section). Thus, in practice, the estimated covariance matrix tends to be more accurate than the block estimator counterpart. Figure 6 in the supplementary material shows the MSEs between the true and the estimated covariance matrices of the incomplete estimate and the block estimate, respectively. As can be seen, the error of the incomplete estimate is smaller than that of the block estimate, which is an advantage of the incomplete MMD over the block MMD in PSI.
3.4 ADDITIONAL APPLICATIONS (GANS ANALYSIS)
The proposed PSI framework is general and can be used for not only feature selection but also sample selection. In generative modeling, the generated distribution should match the data distribution; in other words, the discrepancy between the generated data and real data should be small. We can therefore apply the selective inference algorithm and use the significance value to evaluate the generation quality. In this paper we apply mmdInf to compare the performance of GANs. We first select the model whose generated samples has the smallest MMD score with the real data and then perform the hypothesis test.
Let xi(s)  Rp be a feature vector generated by s-th GAN model with random seed i and yj  Rp is a feature vector of an original image. Image features can be extracted by pre-trained Resnet (He et al., 2016) or auto-encoders. The hypothesis test can be written as
· H0: MMDi2nc[F , X(s), Y ] = 0 | s generates samples closest to the real distribution, · H1: MMD2inc[F , X(s), Y ] = 0| s generates samples closest to the real distribution.

5

Under review as a conference paper at ICLR 2019

(a) MMDu.

(b) MMDinc, r = 100. (c) MMDinc, r = 10. (d) MMDinc, r = 1.

Figure 2: Empirical distribution under p = q and p = q. (a) Complete U-statistics. (b)-(d): The incomplete MMD estimator with different sampling parameter r. For all plots, we fixed the number of samples as n = 200 and the dimensionality d = 1.

Since we want to test the best generator that minimizes the discrepancy between generated and true samples (e.g., low MMD score), this sample selection event can be characterized by
MMDi2nc[F , X(s), Y ]  MMDi2nc[F , X( ), Y ], (s, )  S × S¯.

4 THEORETICAL ANALYSIS OF INCOMPLETE MMD

We investigate the theoretical properties of the incomplete MMD estimator under the random design with replacement. For simplicity, we denote MMD2inc[F , X, Y ] = MMD2inc, MMD2[F , p, q] = MMD2, MMD2inc[F , X(s), Y (s)] = MMDi2n,(cs), and MMD2[F , p(s), q(s)] = MMD2,(s), respectively. See the supplementary material for proof.

Theorem 2 Let n and tend to infinity such that  = limn,  n-(c+1) , 0    . For sampling with replacement, we have

 

1 2

(MMDi2nc

-

MMD2)

-d

N

(0,

2),



n c+1 2

(MMDi2nc

-

MMD2)

-d

V,

 

1 2

(MMD2inc

-

MMD2

)

-d



1 2

V

+ T,

if  = 0. if  = . if 0 <  < .

where V

is

the

random

variable

of

the

limit

distribution

of

n

c+1 2

(MMDu2

-

MMD2

),

T

is the random

variable of N (0, 2), 2 = V ar(h(u, u )), and T and V are independent.

Corollary 3 Assume limn,  n-2 = 0 and 0 <  = limn,  n-1 < . For sampling with replacement,incomplete U-statistics estimator of MMD is asymptotically normally distributed as

1 2

MMD2inc

-d

N (0,

2),

if p = q.

1 2

(MMD2inc

-

MMD2)

-d

N

(0,

2

+

 u2 ),

if p = q.

where 2 = V ar(h(u, u )) and u2 = 4(Eu[(Eu [h(u, u )]] - Eu,u [h(u, u )])2).

Corollary 4 Assume limn,  n-1 = 0. For sampling with replacement, the incomplete Ustatistics estimator of MMD is asymptotically normally distributed as

1 2

(MMDi2nc

-

MMD2)

-d

N

(0,

2).

Thus, in practice, by setting n2, the incomplete estimator is asymptotically normal and therefore can be applied in PSI. More specifically, we can set = rn n2, where r is a small constant. In practice, we found that r = 10 works well in general.
Figure 2 shows the empirical distribution under p = q and p = q for the complete estimator, the block estimator and the incomplete estimator. As can be observed, the empirical distribution of the incomplete estimator is normal for small sampling parameter r, and becomes similar to its complete counterpart if r is large; this is supported by Theorem 2 ( = ).
Finally, the following theorem assure the joint normality of the z vector in Theorem 1.
Theorem 5 Suppose 0   < . Then,
1/2 MMD2in,(c1), . . . , MMDi2n,(cd) - MMD2,(1), . . . , MMD2,(d)
converges to multivariate normal distribution.

6

Under review as a conference paper at ICLR 2019

FPR FPR TPR TPR

0.6 0.6

mmdinf (Incomplete)

mmdinf (Incomplete)

0.5

mmdinf (Block) mmdinf (Linear)

0.5

mmdinf (Block) mmdinf (Linear)

mmd (Incomplete)

mmd (Incomplete)

0.4

mmd (Block)

0.4

mmd (Block)

mmd (Linear)

mmd (Linear)

0.3 0.3

0.2 0.2

0.1 0.1

0 500 1000 1500 2000 2500 3000

0 500 1000 1500 2000 2500 3000

number of instances

number of instances

(a) Mean shift (FPR). (b) Variance shift (FPR).

11

0.8 0.8

0.6 0.6

0.4 0.4

0.2 mmdinf (Incomplete) mmdinf (Block) mmdinf (Linear)
0 500 1000 1500 2000 2500 3000
number of instances
(c) Mean shift (TPR).

0.2 mmdinf (Incomplete) mmdinf (Block) mmdinf (Linear)
0 500 1000 1500 2000 2500 3000
number of instances
(d) Variance shift (TPR).

Figure 3: (a)(b): False positive rates at significant level  = 0.05 of the proposed incomplete estimator, block estimator and linear estimator with/without PSI. For incomplete MMD, we set = 10n. For block MMD, we set the block size B = n. The MMD without PSI computes the p-values without adjusting for the selection. (c)(d): True positive rate comparison of the following three empirical estimates for mmdInf.

5 EXPERIMENTS

We compared mmdInf with a naive testing baseline (mmd), which first selects features using MMD and estimates corresponding p-values with the same data of feature selection without adjustment for the selection event. For mmdInf, we used the three MMD estimators: the linear-time MMD (Gretton et al., 2012), the block MMD (Zaremba et al., 2013), and the incomplete MMD. We used 1/2 of data to calculate the covariance matrix of MMD and the rest to perform feature selection and inference. We fixed the number of selected features (prior to PSI) k to 30. In PSI the significance of each of the 30 selected features (from ranking MMD) is computed and features with p-value lower than the significance level  = 0.05 are selected as statistically significant features.

For block MMD, in each experiment we set the candidate of block size as B = {10, 20, 50} . For

incomplete MMD, in each experiment the ratio between number of pairs (i, j) sampled to compute

incomplete MMD score and sample size is fixed at r = n  {0.5, 5, 10}. We reported the true

kposiistivtheeratoteta(lTnPuRm)bkker

where k is the of true features

number of true features selected by mmdInf or mmd and in synthetic data. We further computed the false positive

rate

(FPR)

k k-k

where

k

is the number of non-true features reported as positives. We ran all

experiments over 200 times, and reported the average TPR and FPR.

5.1 SYNTHETIC EXPERIMENTS (PSI)
The number of features d is fixed to 50, and for each feature, data is randomly generated following a Gaussian distribution with set mean and variance. 10 out of the 50 features are set to be significantly different by shifting the distribution of one class away from the other (mean or variance). More specifically, we generate the synthetic data as
(a) Mean shift x  N (050, I50), y  N (µ, I), µ = [110 040]  R50,
(b) Variance shift x  N (050, I50), y  N (0, ),  = diag([1.5110 140] )
where N (µ, ) is a multivariate normal distribution with mean µ  Rd and covariance   Rd×d, 1p  Rd is a vector whose elements are all one, 0d  Rd is a vector whose elements are all zero, and diag(a)  Rd×d is a diagonal matrix whose diagonal elements are a  Rd.
Figure 3(a) and (b) show the FPRs of linear MMD, block MMD and incomplete MMD with or without PSI. As can be clearly seen, PSI successfully controls FPR with significance level  = 0.05 for all the three estimators, whereas the naive approach tends to have higher FPRs. Figures 3 shows the TPRs of the synthetic data. In both cases, the TPR of incomplete MMD converges to 1 significantly faster than the the other two empirical estimates.
5.2 REAL-WORLD DATA (BENCHMARK)
We compared the proposed algorithm by using real-world datasets. Since it is difficult to decide what is a "true feature" in real-world data, we choose a few datasets for binary classification with small amount of features, and regard all the original features as true. We then concatenated random features to the true features (the total number of features d = 100). Table 1 shows TPRs and FPRs of mmdInf with different MMD estimators. It can be observed that the incomplete estimator

7

Under review as a conference paper at ICLR 2019

Table 1: Post selection inference experimental results for real-world datasets. The average TPR and FPR over

200 trials are reported.

Datasets

Linear-Time

Block

Incomplete

dn

B = 10

B = 20

B = 50

r = 0.5

r=5

r = 10

TPR FPR TPR FPR TPR FPR TPR FPR TPR FPR TPR FPR TPR FPR

Diabetis

8 768 0.05 0.06 0.20 0.04 0.40 0.12 0.46 0.18 0.13 0.06 0.52 0.07 0.65 0.10

Wine (Red) 11 4898 0.13 0.06 0.26 0.01 0.56 0.04 0.68 0.09 0.32 0.06 0.71 0.06 0.79 0.06

Wine (White) 11 1599 0.08 0.06 0.26 0.04 0.37 0.09 0.49 0.15 0.15 0.06 0.51 0.07 0.61 0.07

Australia

7 690 0.08 0.06 0.36 0.06 0.47 0.11 0.65 0.21 0.16 0.05 0.66 0.07 0.79 0.09

300 10 -3 8

MMD 2

6 200
4

100 0 0

0.5 p-value

2 0
1
(a)

CDBRCEAGGMAAENNR MIWNDSIGTRBAADANT-GDGCFAAGHNNPM

(b)

Figure 4: (a) Histogram of p-values over 5000 runs. (b) Averaged incomplete MMD scores.

significantly outperforms the other empirical estimates. Note that a higher TPR can be achieved with higher r, while the FPR is still controlled at 0.05 with the highest r = 10 that we chose.

5.3 GANS ANALYSIS (SAMPLE SELECTION)
We also applied mmdInf for evaluating the generation quality of GANs. We trained BEGAN (Berthelot et al., 2017), DCGAN (Radford et al., 2015), STDGAN (Miyato et al., 2017), Cramer GAN (Bellemare et al., 2017), DFM (Warde-Farley & Bengio, 2016), DRAGAN (Kodali et al., 2017), Minibatch Discrimination GAN (Salimans et al., 2016), and WGAN-GP (Gulrajani et al., 2017), generated 5000 images (using Chainer GAN package 1 with CIFAR10 datasets), and extracted 512 dimensional features by pre-trained Resnet18 (He et al., 2016). For the true image sets, we subsampled 5000 images from CIFAR10 datasets and computed the 512 dimensional features using the same Resnet18. We then tested the difference between the generated images and the real images using mmdInf on extracted features (see Sec. 3.4).
We found that for all the members in the GAN family, the null hypothesis was rejected, i.e., the generated distribution and the real distribution are different. This result is consistent with the findings in (Sutherland et al., 2016), which demonstrate that optimized MMD has perfect discriminative power in GANs evaluation. As sanity check, we evaluated mmdInf by constructing an "oracle" generative model that generates real images from CIFAR10. Next, we randomly selected 5000 images (a disjoint set from the oracle generative images) from CIFAR10 in each trial, and set the sampling ratio to r = 5. Figure 4(a) shows the distribution of p-values computed by our algorithm. We can see that the p-values are distributed uniformly in the tests for the "oracle" generative model, which matches the theoretical result in Theorem 1. Thus the algorithm is able to detect the distribution difference and control the false positive rate. In other words, if the generated samples do not follow the original distribution, we can safely reject the null hypothesis with a given significance level .
Figure 4(b) shows the estimated MMD scores of each model. Based on the results, we could tell that DFM was the best model and DCGAN was the second best model to approximate the true distribution. However, the difference between various members is not obvious. Developing a validation pipeline based on mmdInf for GANs analysis would be one interesting line of future work.

6 CONCLUSION
In this paper, we proposed a novel statistical testing framework mmdInf, which can find a set of statistically significant features that can discriminate two distributions. Through synthetic and realworld experiments, we demonstrated that mmdInf can successfully find important features and/or datasets. We also proposed a method for sample selection based on mmdInf and applied it in the evaluation of generative models.
1https://github.com/pfnet-research/chainer-gan-lib

8

Under review as a conference paper at ICLR 2019
REFERENCES
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution from another. Journal of the Royal Statistical Society. Series B (Methodological), pp. 131­142, 1966.
Marti J Anderson. A new method for non-parametric multivariate analysis of variance. Austral ecology, 26(1):32­46, 2001.
Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan, Stephan Hoyer, and Re´mi Munos. The cramer distance as a solution to biased wasserstein gradients. arXiv preprint arXiv:1705.10743, 2017.
David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.
Patrick Billingsley. Probability and measure. John Wiley & Sons, 2008.
Gunnar Blom. Some properties of incomplete u-statistics. Biometrika, 63(3):573­580, 1976.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Jianqing Fan, Yuan Liao, and Martina Mincheva. Large covariance estimation by thresholding principal orthogonal complements. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75(4):603­680, 2013.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics New York, 2001.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical dependence with hilbert-schmidt norms. In ALT, 2005.
Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Scho¨lkopf, and Alex J Smola. A kernel method for the two-sample-problem. In NIPS, 2007.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scho¨lkopf, and Alexander Smola. A kernel two-sample test. JMLR, 13(Mar):723­773, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NIPS, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gu¨nter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.
Gao Huang, Yang Yuan, Qiantong Xu, Chuan Guo, Yu Sun, Felix Wu, and Kilian Weinberge. An empirical study on evaluation metrics of generative adversarial networks. arXiv preprint arXiv:1610.06545, 2018.
Svante Janson. The asymptotic distributions of incomplete u-statistics. Probability Theory and Related Fields, 66(4):495­505, 1984.
Wittawat Jitkrittum, Zolta´n Szabo´, Kacper P Chwialkowski, and Arthur Gretton. Interpretable distribution features with maximum testing power. In NIPS, 2016.
Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to direct importance estimation. JMLR, 10(Jul):1391­1445, 2009.
9

Under review as a conference paper at ICLR 2019
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans. arXiv preprint arXiv:1705.07215, 2017.
Jason D Lee, Dennis L Sun, Yuekai Sun, Jonathan E Taylor, et al. Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3):907­927, 2016.
Justin Lee. U-statistics: Theory and practice. 1990.
Shuang Li, Yao Xie, Hanjun Dai, and Le Song. M-statistic for kernel change-point detection. In NIPS, 2015a.
Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In NIPS, 2015b.
Song Liu, Makoto Yamada, Nigel Collier, and Masashi Sugiyama. Change-point detection in timeseries data by relative density-ratio estimation. Neural Networks, 43:72­83, 2013.
David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. arXiv preprint arXiv:1610.06545, 2016.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICML Implicit Models Workshop, 2017.
Jonas W Mueller and Tommi Jaakkola. Principal differences analysis: Interpretable characterization of differences between distributions. In NIPS, 2015.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Barnaba´s Po´czos and Jeff Schneider. On the estimation of alpha-divergences. In AISTATS, 2011.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Alfre´d Re´nyi et al. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics. The Regents of the University of California, 1961.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, 2016.
Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V Buenau, and Motoaki Kawanabe. Direct importance estimation with model selection and its application to covariate shift adaptation. In NIPS, 2008.
Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. arXiv preprint arXiv:1611.04488, 2016.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pp. 267­288, 1996.
David Warde-Farley and Yoshua Bengio. Improving generative adversarial networks with denoising feature matching. 2016.
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of decoder-based generative models. arXiv preprint arXiv:1611.04273, 2016.
Makoto Yamada, Akisato Kimura, Futoshi Naya, and Hiroshi Sawada. Change-point detection with feature selection in high-dimensional time-series data. In IJCAI, 2013a.
Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Masashi Sugiyama. Relative density-ratio estimation for robust distribution comparison. Neural computation, 25(5): 1324­1370, 2013b.
10

Under review as a conference paper at ICLR 2019 Makoto Yamada, Yuta Umezu, Kenji Fukumizu, and Ichiro Takeuchi. Post selection inference with
kernels. In AISTATS, 2018. Wojciech Zaremba, Arthur Gretton, and Matthew Blaschko. B-test: A non-parametric, low variance
kernel two-sample test. In NIPS, 2013.
11

Under review as a conference paper at ICLR 2019

SUPPLEMENTARY MATERIALS

THEORETICAL ANALYSIS OF INCOMPLETE MMD

We investigate the theoretical properties of the incomplete MMD estimator under the random design with replacement. For simplicity, we denote MMD2inc[F , X, Y ] = MMD2inc and MMD2[F , p, q] = MMD2, respectively.

Theorem 2 Let n and tend to infinity such that  = limn,  n-(c+1) , 0    . For sampling with replacement, we have

 

1 2

(MMDi2nc

-

MMD2)

-d

N

(0,

2),



n c+1 2

(MMD2inc

-

MMD2)

-d

V,

 

1 2

(MMDi2nc

-

MMD2

)

-d



1 2

V

+ T,

if  = 0. if  = . if 0 <  < .

where V

is

the

random

variable

of

the

limit

distribution

of

n

c+1 2

(MMDu2

-

MMD2

),

T

is the random

variable of N (0, 2), 2 = V ar(h(u, u )), and T and V are independent.

Proof: Use Corollary 1 of Janson (1984) (or Theorem 1 of Lee (1990), pp. 200). In MMD, c = 1 for p = q and c = 0 for p = q.

Corollary 3 Assume limn,  n-2 = 0 and 0 <  = limn,  n-1 < . For sampling with replacement, the incomplete U-statistics estimator of MMD is asymptotically normally distributed
as

1 2

MMD2inc

-d

N (0,

2),

if p = q.

1 2

(MMDi2nc

-

MMD2)

-d

N

(0,

2

+

 u2 ),

if p = q.

where 2 = V ar(h(u, u )) and u2 = 4(Eu[(Eu [h(u, u )]] - Eu,u [h(u, u )])2).

Proof: Under p = q (c = 1), since limn,  n-2 = 0 and MMD2 = 0, we can immediately obtain the limit distribution by Theorem 2. Under p = q (c = 0), MMDu converges in distribution to a Gaussian according to (Gretton et al., 2007)

n

1 2

(MMD2u

-

MMD2)

-d

N

(0,

u2 )

where u2 = 4(Eu[(Eu [h(u, u )]] - Eu,u [h(u, u )])2). Based on Theorem 2, under the given assumption, we can obtain the distribution of MMDi2nc since T and V are independent.

Corollary 4 Assume limn,  n-1 = 0. For sampling with replacement, the incomplete Ustatistics estimator of MMD is asymptotically normally distributed as

1 2

(MMDi2nc

-

MMD2)

-d

N

(0,

2).

Proof: Since limn,  n-1 = 0 and limn,  n-2 1/2(MMD2inc - MMD2) is N (0, 2) based on Theorem 2.

= 0, the limit distribution of

Thus, in practice, by setting n2, the incomplete estimator is asymptotically normal and therefore can be applied in PSI. More specifically, we can set = rn n2, where r is a small constant. In
practice, we found that r = 10 works well in general.

Theorem 5 Suppose 0   < . Then, 1/2 MMDi2n,(c1), . . . , MMDi2n,(cd) - MMD2,(1), . . . , MMD2,(d)
converges to multivariate normal distribution.

12

Under review as a conference paper at ICLR 2019

(a) MMDu.

(b) MMDb, B = 100. (c) MMDb, B = 20. (d) MMDb, B = 5.

(e) MMDinc, r = 100. (f) MMDinc, r = 50. (g) MMDinc, r = 10. (h) MMDinc, r = 1.
Figure 5: Empirical distribution under p = q and p = q. (a) Complete U-statistics. (b)-(d): The block MMD estimator with different block parameter B. (e)-(h): The incomplete MMD estimator with different sampling parameter r. For all plots, we fixed the number of samples as n = 200 and the dimensionality d = 1.

Proof: We use the fact that convergence in distribution of multivariate random variables results

in convergence in distribution of univariate random variables. From Theorem29.4 in Billings-

ley (2008)(Crame´r-Wold device), it is sufficient to prove that for any  = [1, . . . , d]  Rd,

d s=1

sMMD2inc[F ,

X (s) ,

Y

(s)]

-d

d s=1

s

Zs

,

where

Zs, s

=

1, 2, . . . , d

are

normal

distri-

butions. Since each incomplete U-statistic MMDi2nc[F , X(s), Y (s)] converges to normal distribu-

tion derived in Corollary 3 when 0   < , and from the continuous mapping theorem for

g(x) =  x, we obtain the desired result.

ILLUSTRATIVE EXPERIMENTS
Figure 5 shows the empirical distribution under p = q and p = q for the complete estimator, the block estimator and the incomplete estimator. As can be observed, the empirical distribution of the incomplete estimator is normal for small sampling parameter r, and becomes similar to its complete counterpart if r is large; this is supported by Theorem 2 ( = ). Moreover, compared to the block estimator, the incomplete estimator tends to have a better trade-off between variance and normality.
Figure 7(a) shows the Type II error comparison for two-sample test with one dimensional Gaussian mean shift data. The Type II error is computed when the Type I error is fixed at 0.05, and the incomplete MMD outperforms other estimators. Figure 7 (b) compares the computational time of the empirical estimates, and for small r the computational time of incomplete MMD is much less than that of the block MMD. Overall, the incomplete MMD has favorable properties in practice.

THE EFFECTIVENESS OF PSI

Here, we compared the PSI and non-PSI counterpart on benchmark data. As clearly see, if we do not use the PSI, we cannot control FPR, while the proposed algorithm can successfuly control FPR values.

Datasets

d

Diabetis Wine (Red) Wine (White) Australia

8 11 11 7

n
768 4898 1599 690

Linear-Time
TPR FPR 0.05 0.05 0.13 0.06 0.08 0.06 0.08 0.06

Incomplete (without PSI)

r = 0.5

r=5

r = 10

TPR FPR TPR FPR TPR FPR

0.41 0.21 0.74 0.27 0.83 0.31

0.58 0.23 0.81 0.25 0.86 0.26

0.39 0.24 0.65 0.26 0.71 0.29

0.52 0.21 0.85 0.25 0.91 0.30

Incomplete (with PSI)

r = 0.5

r=5

r = 10

TPR FPR TPR FPR TPR FPR

0.13 0.06 0.52 0.07 0.65 0.10

0.32 0.06 0.71 0.06 0.79 0.06

0.15 0.06 0.51 0.07 0.61 0.07

0.16 0.05 0.66 0.07 0.79 0.09

13

Under review as a conference paper at ICLR 2019

10 0 10 -2

Incomplete Block

MSE

10 -4 0

2000

4000

Number of Sample

Figure 6: Covariance estimation error with respect to the number of samples. We fixed B = 20 and = 5n.

0.5 0.5 r =0.5

Incomplete MMD

r =1

0.4

Block MMD Linear MMD

0.4 r =5 r =10

Complete MMD (unbiased)

r =20

block 0.3 0.3

Type II Error Time (sec)

0.2 0.2

0.1 0.1

00 300 600 900 1200 1500
number of instances
(a)

0.5 1 1.5 Number of samples
(b)

2 10 4

Figure 7: (a): Type II error comparison. We change the sample size n = [100, 200 . . . , 1500] and compute the type II error of the four empirical estimates of MMD when the type I error is controlled at 0.05. For incomplete MMD, we use r = 10. For the block MMD, we use B = n. (b): Computational time comparison. We change the sample size n = [2000, 4000, . . . , 20000] and compute the incomplete MMD and the block MMD, respectively. For incomplete MMD, we use r = [0.5, 1, 5, 10, 20]. For the block MMD, we use B = n. Incomplete MMD with r = 0.5 (i.e.,
= n/2) can be regarded as the linear-time MMD estimator (Gretton et al., 2012).

14

