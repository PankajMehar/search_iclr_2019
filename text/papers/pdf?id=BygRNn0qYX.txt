Under review as a conference paper at ICLR 2019
P2IR: UNIVERSAL DEEP NODE REPRESENTATION VIA PARTIAL PERMUTATION INVARIANT SET FUNCTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Graph node representation learning is a central problem in social network analysis, aiming to learn the vector representation for each node in a graph. The key problem is how to model the dependence of each node to its neighbor nodes since the neighborhood can uniquely characterize a graph. While most existing approaches rely on defining the specific neighborhood dependence as the computation mechanism of representations, which may exclude important subtle structures within the graph and dependence among neighbors, we propose a novel graph node embedding method (namely P2IR) via developing a novel notion, namely partial permutation invariant set function. Our method can 1) learn an arbitrary form of the representation function from the neighborhood, without losing any potential dependence structures, 2) automatically decide the significance of neighbors at different distances, and 3) be applicable to both homogeneous and heterogeneous graph embedding, which may contain multiple types of nodes. Theoretical guarantee for the representation capability of our method has been proved for general homogeneous and heterogeneous graphs. Evaluation results on benchmark data sets show that the proposed P2IR outperforms the state-of-the-art approaches on producing node vectors for classification tasks.
1 INTRODUCTION
Graph node representation learning (or graph embedding in some literature) is to learn the numerical representation of each node in a graph by vectors in a Euclidean space, where the geometric relationship reflects the structure of the original graph. Nodes that are "close" in the graph are embedded to have similar vector representations (Cai et al., 2017). The learned node vectors benefit a number of graph analysis tasks, such as node classification (Bhagat et al., 2011), link prediction (LibenNowell & Kleinberg, 2007), community detection (Fortunato, 2010), and many others (Hamilton et al., 2017b). In order to preserve the node geometric relations in an embedded space, the similarity/proximity/distance of a node to its neighborhood is generally taken as input to different graph embedding approaches. For example, matrix-factorization approaches work on pre-defined pairwise similarity measures (e.g., different order of adjacency matrix) (Zhang et al., 2018). Deepwalk (Perozzi et al., 2014), node2vec (Grover & Leskovec, 2016) and other recent approaches (Dong et al., 2017) consider flexible, stochastic measure of node similarity by the node co-occurrence on short random walks over the graph (Goyal & Ferrara, 2017). Neighborhood autoencoder methods compress the information about a node's local neighborhood that is described as a neighborhood vector containing the node's pairwise similarity to all other nodes in the graph (Wang et al., 2016; Cao et al., 2016; Bojchevski & Günnemann, 2018). Neural network based approaches such as graph convolutional networks (GCN) and GraphSAGE apply convolution like functions on its surrounding neighborhood for aggregating neighborhood information (Kipf & Welling, 2016a; Hamilton et al., 2017a; Tu et al., 2018).
However, most existing methods either explicitly or implicitly restrict the dependence form of each node to its neighbors and also the depth of neighbors. Therefore, some important topology structures within the graph and subtle dependence between neighbors may not be captured in the embedded space. For example, the methods in the family of stochastic walk lose the control of influence of further neighbors in a network since they usually utilize a fixed hyper-parameter as a window size, or need to specify the degree of neighborhood proximity; GCN losses the flexibility of neighborhood information since it fixes the two-layer aggregation function for the representation computation.
1

Under review as a conference paper at ICLR 2019
In this work, we propose a Partial Permutation Invariant Representation method (P2IR) by developing a new notion of partial permutation invariant set function, that can
· learn node representation via a universal graph embedding function f , without pre-defining pairwise similarity, specifying random walk parameters, or choosing aggregation functions among element-wise mean, a max-pooling neural network, or LSTMs;
· capture the arbitrary relationship of each node to its neighbors; · automatically decide the significance of nodes at different distances; · be generally applied to any graphs from simple homogeneous graphs to heterogeneous
graphs with complicated types of nodes.
Evaluation results on benchmark data sets show that the proposed P2IR outperforms the state-of-theart approaches on producing node vectors for classification tasks.
2 RELATED WORK
The main difference among various graph embedding methods lies in how they define the "closeness" between two nodes (Cai et al., 2017). First-order proximity, second-order proximity or even high-order proximity have been widely studied for capturing the structural relationship between nodes (Tang et al., 2015b; Yang et al., 2017; Zhu et al., 2018). Comprehensive reviews of graph embedding have been done by Cai et al. (2017); Hamilton et al. (2017b); Goyal & Ferrara (2017); Yang et al. (2017). In this section, we discuss the relevant graph embedding approaches in terms of how node closeness to neighboring nodes is measured, for highlighting our contribution on utilizing neighboring nodes in a most general manner.
Matrix Analysis on Graph Embedding As early as 2011, a spectral clustering method (Tang & Liu, 2011) took the eigenvalue decomposition of a normalized Laplacian matrix of a graph as an effective approach to obtain the embeddings of nodes. Other similar approaches work on different node similarity matrix by applying various similarity functions to make a trade-off between modeling the "first-order similarity" and "higher-order similarity" (Cao et al., 2015; Ou et al., 2016). Node content information can also be easily fused in the pairwise similarity measure, e.g., in TADW (Yang et al., 2015), as well as node label information, which resulting in semi-supervised graph embedding methods, e.g., MMDW (Tu et al., 2016). Recently, an arbitrary-order proximity preserved network embedding method is introduced in (Zhang et al., 2018) based on matrix eigen-decomposition, which is applied to a pre-defined high-order proximity matrix. For heterogeneous networks, Huang et al. (2017) proposed a label involved matrix analysis to learn the classification result of each vertex within a semi-supervised framework.
Random Walk on a Graph to Node Representation Learning Both deepwalk (Perozzi et al., 2014) and node2vec (Grover & Leskovec, 2016) are outstanding graph embedding methods to solve the node representation learning problem. They convert the graph structures into a sequential context format with random walk (Lovász, 1993). Thanks to the invention of Mikolov et al. (2013) for word representation learning of sentences, deepwalk inherited the learning framework for words representation learning in paragraphs to generate the representation of nodes in random walk context. And then node2vec evolved such the idea with additional hyper-parameter tuning for the trade-off between DFS and WFS to control the direction of random walk. Gao & Huang (2018) propose a self-paced network embedding by introducing a dynamic negative sampling method to select difficult negative context nodes in the training process. Planetoid (Yang et al., 2016) proposed a semi-supervised learning framework by guiding random walk with available node label information. The heterogeneity of graph nodes is often handled by a heterogeneous random walk procedure (Dong et al., 2017), or selected relation pairs (Chang et al., 2015). Tang et al. (2015a) considered the predictive text embedding problem on a large-scale heterogeneous text network and the proposed method is also based on pre-defined heterogeneous random walks.
Neighborhood Encoders to Graph Embedding There are also methods focusing on aggregating or encoding the neighbors' information to generate node embeddings. DNGR (Cao et al., 2016) and SDNE (Wang et al., 2016) introduce the autoencoder to construct the similarity function between the
2

Under review as a conference paper at ICLR 2019
neighborhood vectors and the embedding of the target node. DNGR defines neighborhood vectors based on random walks and SDNE introduces adjacency matrix and Laplacian eigenmaps to the definition of neighborhood vectors. Although the idea of autoencoder is a great improvement, these methods are painful when the scale of the graph is up to millions of nodes. Therefore, methods with neighborhood aggregation and convolutional encoders are involved to construct a local aggregation for node embedding, such as GCN (Kipf & Welling, 2016a;b; Schlichtkrull et al., 2018; van den Berg et al., 2017), FastGCN (Chen et al., 2018), column networks (Pham et al., 2017), the GraphSAGE algorithm (Hamilton et al., 2017a), and a recent DRNE (Tu et al., 2018) method using layer normalized LSTM to approximate the embedding of a target node by the aggregation of its neighbors' embeddings. The main idea of these methods is involving an iterative or recursive aggregation procedure e.g., convolutional kernels or pooling procedures to generate the embedding vectors for all nodes and such aggregation procedures are shared by all nodes in a graph.
The above-mentioned methods work differently on using neighboring nodes for node representation learning. They require on pre-defining pairwise similarity measure between nodes, or specifying random walk parameters, or choosing aggregation functions. In practice, it takes non-trivial effort to tune these parameters or try different measures, especially when graphs are complicated with nodes in multiple types, i.e., heterogeneous graphs. This work hence targets on making neighboring nodes play their roles in a most general manner such that their contributions are learned but not user-defined. The resultant embedding method has the flexibility to work on any types of homogeneous and heterogeneous graph.
Our proposed method P2IR has a natural advantage on avoiding any manual manipulation of random walking strategies or designs for the relationships between different types of nodes. To the invention of set functions by Zaheer et al. (2017), all existing valid mapping strategies from neighborhood to the target nodes can be represented by the set functions which are learnt by P2IR automatically.
3 THE PROPOSED P2IR METHOD
In this section, we first formally define the problem, followed by introducing a new notion - partial permutation invariant function set function. This section ends up with the overall P2IR method.
3.1 PROBLEM SETUP
We target on designing graph embedding models for the most general graph that may include K different types of nodes (k=1 for homogeneous graphs). Formally, a graph G = {V, E}, where the node set V = kK=1Vk, i.e., V is composed of K disjoint types of nodes. One instance of such a graph is the academic publication network, which includes different types of nodes for papers, publication venues, author names, author affiliations, research domains etc. Given such a graph G, our goal is to learn the embedding vector for each node in this graph.
3.2 THE PROPOSED UNIVERSAL GRAPH EMBEDDING MODEL
As we know, the position of a node in the embedded space is collaboratively determined by its neighboring nodes. Therefore, the key issue to address for graph embedding is how to model the dependence of each node to its neighbors. Most existing approaches need to pre-specify the neighbors (usually via the distance on the graph), and either explicitly or implicitly define a specific form to characterize the dependence between each node and its neighbors.
We propose a universal graph embedding model that neither requires to pre-define neighbors nor to specify the dependence form between each node and its neighbors. The embedding vector xv of node v  Vk can be represented by its neighbors' embedding vectors via a function f
xv = f (X1v, X2v, · · · , XKv ), v  Vk, k  {1, 2, . . . , K} where Xkv is a matrix with column vectors corresponding to the embedding of node v's neighbors in type k. Note that the neighbors can include step-1 (or immediate) neighbors, step-2 neighbors, and even further neighbors.
How to characterize an arbitrary dependence to neighbors? The key observation is that all neighboring nodes reachable from a target node at the same step are not distinguishable from the
3

Under review as a conference paper at ICLR 2019

view of the target node. Therefore, function f (·) should be a partially permutation invariant function.
That is, if we swap any columns in each Xkv, the function value remains the same. Unfortunately, the set function is not simply learn-able because the permutation property is hard to guarantee directly.

One straightforward idea to represent the partially permutation invariant function is to define it in the following form

f (X1v, · · · , XKv ) :=

· · · g(X¯1vP1, · · · , X¯Kv PK )

P1P|V1| P2P|V2|

PK P|VK |

(1)

where Pn denotes the set of n-dimensional permutation matrices, and X¯1v,

representation the columns in

matrix X¯1v. It

consisting of the vectors in X1v, is easy to verify that the function

· · · , XKv , defined in

respectively. equation 1 is

·X¯· ·1v,PX1¯Kivs

denote the to permute

partially permutation

invariant, but it is intractable because it involves

N k=1

(|Vk

|

!)

"sum"

items.

Our solution of learning function f is based on the following important theorem, which gives a neat way to represent any partial permutation invariant function. The proof is provided in the Appendix.

Theorem 3.1. [Partial permutation invariant function representation theorem] Let f be a continuous real-valued function defined on a compact set X with the following form

f (x1,1, x1,2, · · · , x1,N1 , x2,1, x2,2, · · · , x2,N2 , · · · , xK,1, xK,2, · · · , xK,NK ).

G1 G2

GK

If function f is partially permutation invariant, that is, any permutations of the values within the group Gk for any k does not change the function value, then there must exist functions h(·) and {gk(·)}Kk=1 to approximate f with arbitrary precision in the following form

N1 N2

NK

h g1(x1,n), g2(x2,n), · · · , gK (xK,n) .

n=1

n=1

n=1

(2)

The rigorous proof is provided in Appendix. This result essentially suggests a neat but universal way to represent any partially permutation invariant function. For instance, a popular permutation invariant function widely used in deep learning max(·) can be approximated in an arbitrary precision by
N

max(x1, x2, · · · , xN )  h

g(xi)

i=1

with g(z) = [exp(kz)z, exp(kz)], and h([z, z ]) = z/z , as long as k is large enough. This is because

max(x1,

x2,

·

··

,

xN )

=

lim
k

N -1 N

exp(kxi)

exp(kxi) · xi

i=1 i=1

Therefore, based on Theorem 3.1, we only need to parameterize h(·) and {gk(·)}kK=1 to learn the node embedding function, which makes the partial permutation function finally learn-able and tractable. To make equation 2 approximate an arbitrary partial permutation invariant function, we only need to ensure that functions h and all g's can represent (or approximate) arbitrary real functions. As we know, deep neural network provides an efficient way to approximate an arbitrary continuous real function, because 3-layer neural networks is able to approximate any function (Cybenko, 1989; Hornik, 1991; Haykin, 1994). We next formulate the embedding model when considering different order of neighborhood.

How to automatically learn the importance of neighbors at different distances?
(1-step neighbors) From Theorem 3.1, any mapping function of a node v  Vk can be characterized by appropriately defined functions 1, 2, · · · , K , and k:



xv = k 

1 (xu) ,

2 (xu) , · · · ,

K (xu) v  Vk, k  {1, . . . , K},

uv1,1

uv1,2

uv1,K

4

Under review as a conference paper at ICLR 2019

where nv,k := nv  Vk denotes the step-n neighbors of node v in node type k.
(Multi-step neighbors) High order proximity has been shown to be beneficial on generating high quality embedding vectors (Yang et al., 2017). Extending the 1-step neighbor model, we can have the more general model where the representation of each node could depend on immediate (1-step) neighbors, 2-step neighbors, 3-step neighbors, and even infinite-step neighbors.



xv = k  n



1 (xu) ,

n


2 (xu) , · · · , n

 K (xu)

n=0

uvn,1

n=0

unv ,2

n=0

unv ,K

v  Vk , k  {1, 2, . . . , K}.

(3)

where 1, 2, · · · ,  are the weights for neighbors at different steps. Let A  {0, 1}|V|×|V| be the

adjacent matrix indicating all edges by 1. If we define the polynomial matrix function B(·) on the

adjacent matrix A as B(A) =

 n=0

nAn,

we

can

cast

equation

3

into

its

matrix

form

xv = k ¯1(X1)[B(A)]V1,v, ¯2(X2)[B(A)]V2,v, · · · , ¯K (XK )[B(A)]VK,v

(4)

v  Vk , k  {1, 2, . . . , K},

where Xk denotes the representation matrix for nodes in type k, [B(A)]Vk,v denotes the sub-matrix of B(A) indexed by column v and rows in Vk, and function ¯ (with ¯· on the top of function (·)) is defined as the function extension
¯(X) := [(x1), (x2), · · · , (xN )].

Note that the embedding vectors for different type of nodes may be with different dimensions. Homogeneous graph is a special case of the heterogeneous graph with K = 1. The above proposed model is thus naturally usable on a homogeneous graph.

To avoid optimizing infinite number of coefficients, we propose to use a 1-dimensional NN function (·) : R  R to equivalently represent the function B(·) to reduce the number of parameters based on the following observations



B(A) = U diag

n1n, · · · , nNn U = U diag ((1), · · · , (N )) U ,

n=0

n=0

where A = U diag(1, · · · , N )U is the singular value decomposition. We parameterize  : R  R using 1-dimensional NN, which allows us easily controlling the number of variables to optimize in  by choosing the number of layers and the number of nodes in each layer.

3.3 THE OVERALL P2IR MODEL

For short, we denote the representation function for xv in equation 4 by
xv = R,k,{k}Kk=1 (v) v  Vk , k  {1, 2, . . . , K}.
To fulfill the requirement of a specific learning task, we propose the complete P2IR model by involving a supervised component

min
{xv }vV ,, {k }kK=1 ,{k }Kk=1

K1 k=1 k|Vk| uVk

xu - R,k,{k}kK=1 (u)

21 + |Vlabel| vVlabel

(xv, yv)

(5)

where Vlabel  V denotes the set of labeled nodes, and k > 0 balances the representation error and prediction error. The first unsupervised learning component restricts the representation error between the target node and its neighbors with L2 norm since it is allowed to have noise in a practical graph. And the supervised component is flexible to be replaced with any designed learning task on the nodes in a graph. For example, to a regression problem, a least square loss can be chosen to replace (x, y) and a cross entropy loss can be used to formulate a classification learning problem. To solve the problem in equation 5, we apply a stochastic gradient descent algorithm (SGD) to compute the effective solutions for the learning variables simultaneously.

5

Under review as a conference paper at ICLR 2019

4 EXPERIMENTS
This section reports experimental results to validate the proposed method, comparing to state-of-theart algorithms on benchmark datasets including both homogeneous and heterogeneous graphs, for evaluating the applicability of our proposed method on embedding problem of general graphs.

4.1 COMPARISON ON HOMOGENEOUS GRAPHS
We consider the multi-class classification problem over the homogeneous graphs. Given a graph with partially labeled nodes, the goal is to learn the representation for each node for predicting the class for unlabeled nodes.

Datasets We evaluate the performance of P2IR and methods for comparison on five datasets.
· Cora (McCallum et al., 2000) is a paper citation network. Each node is a paper. There are 2708 papers and 5429 citation links in total. Each paper is associated with one of 7 classes.
· CiteSeer (Giles et al., 1998) is another paper citation network. CiteSeer contains 3,312 papers and 4,732 citations in total. All these papers have been classified into 6 classes.
· Pubmed (Sen et al., 2008) is a larger and more complex citation networks compared to previous two datasets. There are 19,717 vertexes and 88,651 citation links. Papers are classified into 3 classes.
· Wikipedia (Sen et al., 2008) contains 2,405 online web-pages. The 17,981 links are undirected between pairs of them. All these pages are from 17 categories.
· Email-eu (Leskovec et al., 2007) is an Email communication network which illustrates the email communication relationships between researchers in a large European research institution. There are 1,005 researchers and 25,571 links between them. Department affiliations (42 in total) of the researchers are considered as labels to predict.

Baseline methods The compared baseline algorithms are listed below:
· Deepwalk (Perozzi et al., 2014) is an unsupervised graph embedding method which relies on the random walk and word2vec method. For each vertex, we take 80 random walks with length 40, and set window size as 10. Since deepwalk is unsupervised, we apply a logistic regression on the generated embeddings for node classification.
· Node2vec (Grover & Leskovec, 2016) is an improved graph embedding method based on deepwalk. We set the window size as 10, the walk length as 80 and the number of walks for each node is set to 100. Similarly, the node2vec is unsupervised as well. We apply the same evaluation procedure on the embeddings of node2vec as what we did for deepwalk.
· MMDW (Tu et al., 2016) is a semi-supervised learning framework of graph embedding which combines matrix decomposition and SVM classification. We tune the method multiple times and take 0.01 as the hyper-parameter  in the method which is recommended by the authors.
· Planetoid (Yang et al., 2016) is a semi-supervised learning framework. We set the batch size as 200, learning rate as 0.01, the batch size for label context loss as 200, and mute the node attributes as input while using softmax for the model output.
· GCN (Graph Convolutional Networks) (Kipf & Welling, 2016a) applies the convolutional neural networks into the semi-supervised embedding learning of graph. We eliminate the node attributes for fairness as well.

Experiment setup and results For fair comparison, the dimension of representation vectors is
chosen to be the same for all algorithms (the dimension is 64). The hyper-parameters are fine-tuned for all of them. The details of P2IR for multi-class case are as follows.

· Supervised Component: Softmax function is chosen to formulate our supervised com-

ponent in equation 5 which is defined as  : RK  {z  RK |

K i=1

zi

=

1, zi

>

0}.

6

Under review as a conference paper at ICLR 2019

Table 1: A table with rounded corners

For an arbitrary embedding x  Rd, we have the probability term as P(y = j|x) =

exp(wj x+bj )

K i

exp(wi

x+bi )

for such node to be predicted as class j, where (wj



Rd, bj



R)

is a classifier for class j. Therefore, the whole supervised component in equation 5 is

-

vVlabel

K j

yj

log

P(y

=

j |xv )

+

w R(w),

where

R(w)

is

an

L2

regularization

for

w and w is chosen to be 10-3.

· Unsupervised embedding mapping Component: We design a two-layer NN with hidden

dimension 64 to form the mapping from embeddings of neighbors to the target node and

we also form a two-layer 1-to-1 NN with a 3 dimensional hidden layer to construct the matrix function for the adjacency matrix. We pre-process the matrix A with an eigenvalue

decomposition where the procedure preserve the highest 1000 eigenvalues in default. The balance hyper-parameter 1 is set to be 10-3.

We take experiments on each data set and compare the performance among all baseline methods mentioned above. Since it is the multi-class classification scenario, we use Accuracy as the evaluation criterion. The percentage of labeled samples is chosen from 10% to 90% and the remaining samples are used for evaluation. All experiments are repeated for five times and we report the mean and standard deviation of their performance in the Table 2 . We highlight the best performance for each dataset with bold font style and the second best results with a "*". We can observe that in most cases, our method outperforms other methods.

4.2 COMPARISON ON HETEROGENEOUS GRAPHS
We next conduct evaluation on heterogeneous graphs, where learned node embedding vectors are used for multi-label classification.

Datasets The used datasets include
· BlogCatalog (Wang et al., 2010) is a social media network with 55,814 users and according to the interests of users, they are classified into multiple overlapped groups. We take the five largest groups to evaluate the performance of methods. Users and tags are two types of nodes. The 5,413 tags are generated by users with their blogs as keywords. Therefore, tags are shared with different users and also have connections since some tags are generated from the same blogs. The number of edges between users, between tags and between users and tags are about 1.4M, 619K and 343K respectively.
· DBLP (Ji et al., 2010) is an academic community network. Here we obtain a subset of the large network with two types of nodes, authors and key words from authors' publications. The generated subgraph includes 27K (authors) + 3.7K (key words) vertexes. The link between a pair of author indicates the coauthor relationships, and the link between an author and a word means the word belongs to at least one publication of this author. There are 66,832 edges between pairs of authors and 338,210 edges between authors and words. Each node can have multiple labels out of four in total.
Baseline Methods To illustrate the validation of the performance of P2IR on heterogeneous graphs, we conduct the experiments on two stages: (1) comparing P2IR with Deepwalk (Perozzi et al., 2014) and node2vec (Grover & Leskovec, 2016) on the graphs by treating all nodes as the same type (P2IR with K = 1 in a homogeneous setting); (2) comparing P2IR with the state-of-the-art heterogeneous graph embedding method, metapath2vec (Dong et al., 2017), in a heterogeneous setting. The hyper-parameters of the method are fine-tuned and metapath2vec++ is chosen as the option for the comparison.

Experiment Setup and Results For fair comparison, the dimension of representation vectors is
chosen to be the same for all algorithms (the dimension is 64). We fine-tune the hyper-parameter for all of them. The details of P2IR for multi-label case are as follows.

7

Under review as a conference paper at ICLR 2019

Table 2: Accuracy (%) of multi-class classification experiments in homogeneous graphs

Cora

CiteSeer

Pubmed

training% deepwalk node2vec MMDW planetoid
GCN P2 IR deepwalk node2vec MMDW planetoid GCN P2 IR deepwalk node2vec MMDW planetoid GCN P2 IR deepwalk node2vec MMDW planetoid GCN P2 IR deepwalk node2vec MMDW planetoid GCN P2 IR

10.00%
72.73 ±2.02 73.46 ±0.87 ±740..8283* 40.87 ±15.61 59.10 ±1.48 76.28 ±1.35
46.52 ±1.45 46.38 ±1.32 55.36 ±0.60 38.44 ±0.89 39.78 ±0.34 ±531..9684*
75.51 ±0.34 ±760..1276* 75.43 ±0.21 40.27 ±0.50 57.17 ±0.44 76.91 ±0.73
45.82 ±1.05 45.04 ±0.74 ±530..0554* 49.78 ±1.81 52.93 ±1.91 56.02 ±1.00
28.87 ±2.40 29.82 ±1.79 36.76 ±0.90 53.50 ±2.60 ±583..9547* 63.05 ±5.74

20.00%
75.50 ±1.12 75.70 ±1.33 ±790..1180* 52.63 ±0.44 67.08 ±1.98 80.10 ±1.36
49.18 ±0.49 47.65 ±0.88 60.98 ±0.56 41.12 ±0.98 47.89 ±1.21 ±600..3841*
75.84 ±0.20 76.46 ±0.26 ±760..7271* 40.48 ±0.23 60.92 ±0.70 78.50 ±0.40
51.22 ±1.05 49.70 ±0.82 ±590..4355* 51.44 ±0.93 59.12 ±1.16 63.96 ±0.62
42.59 ±2.95 43.93 ±3.72 40.72 ±2.54 61.26 ±1.13 ±642..9333* 67.19 ±3.39

30.00%
76.72 ±0.99 76.45 ±1.11 ±810..2101* 52.23 ±1.47 73.06 ±1.07 82.95 ±1.44
49.28 ±0.72 48.68 ±0.71 ±620..0107* 40.67 ±1.42 53.85 ±1.38 63.55 ±0.97
76.00 ±0.19 76.41 ±0.17 ±770..7157* 40.30 ±0.50 63.70 ±0.33 79.38 ±0.23
52.18 ±0.89 52.17 ±0.98 ±620..8555* 51.61 ±0.55 62.63 ±1.27 65.92 ±0.59
48.02 ±2.10 48.42 ±2.03 43.22 ±0.61 62.75 ±2.39 ±691..1338* 71.81 ±2.39

40.00%
77.34 ±0.52 78.07 ±1.03 ±820..1295* 52.07 ±0.55 75.61 ±1.02 83.99 ±1.14
50.41 ±0.87 48.12 ±0.64 ±630..8192* 38.59 ±1.50 57.92 ±1.21 66.34 ±0.63
76.15 ±0.22 76.67 ±0.31 ±780..8037* 40.33 ±0.23 65.76 ±0.35 80.26 ±0.27
54.05 ±1.13 52.45 ±1.80 62.42 ±0.07 50.35 ±1.38 ±660..2519* 68.99 ±0.80
52.07 ±0.71 52.90 ±0.78 43.01 ±1.52 65.19 ±1.09 ±702..6153* 73.23 ±2.73

50.00%
78.38 ±0.82 78.42 ±1.14 ±830..1401* 53.06 ±1.99 79.08 ±1.23 85.13 ±0.71
50.43 ±1.27 48.37 ±1.14 ±660..5297* 39.32 ±2.13 62.00 ±0.72 69.41 ±1.10
76.13 ±0.38 76.96 ±0.48 ±790..3084* 40.33 ±0.49 66.96 ±0.43 80.54 ±0.52
54.98 ±0.79 53.76 ±0.85 64.26 ±1.09 50.32 ±0.59 ±671..4067* 69.80 ±0.80
56.81 ±1.72 56.81 ±2.08 46.11 ±1.23 67.02 ±1.72 ±721..8575* 76.33 ±1.56

60.00%
78.41 ±1.47 78.65 ±1.46 ±840..6029* 51.86 ±1.90 80.11 ±1.15 85.95 ±0.81
50.79 ±0.99 48.13 ±0.98 ±690..0105* 37.96 ±1.60 64.74 ±1.09 71.63 ±0.92
76.22 ±0.26 76.82 ±0.42 ±790..8039* 40.79 ±0.55 67.80 ±0.65 81.28 ±0.46
54.45 ±1.46 53.41 ±1.26 66.46 ±0.85 49.89 ±2.02 ±691..0400* 71.27 ±0.89
57.26 ±1.10 56.87 ±1.87 44.94 ±0.38 66.41 ±2.46 ±742..1936* 75.57 ±0.96

70.00%
78.82 ±1.12 78.94 ±1.43 ±850..5444* 51.91 ±1.09 82.61 ±1.00 86.87 ±0.97
49.63 ±0.75 47.67 ±1.38 ±690..7822* 39.07 ±0.72 67.59 ±1.32 73.11 ±1.82
76.05 ±0.37 76.69 ±0.54 ±790..7037* 40.66 ±0.48 68.06 ±0.61 81.58 ±0.37
56.37 ±2.19 54.87 ±1.98 67.50 ±0.48 51.00 ±1.89 ±692..7248* 71.57 ±2.59
60.00 ±2.44 59.38 ±3.17 48.08 ±1.21 67.51 ±2.14 ±751..3857* 76.41 ±3.46

80.00%
79.15 ±1.04 79.08 ±1.44 ±850..2272* 51.29 ±2.76 84.29 ±1.88 87.84 ±0.35
50.06 ±1.89 48.55 ±1.11 ±700..4903* 36.98 ±1.59 68.04 ±1.56 72.99 ±1.47
76.70 ±0.83 77.41 ±0.96 ±790..3172* 40.43 ±0.31 69.39 ±0.30 81.99 ±0.71
56.34 ±2.11 55.93 ±1.64 67.37 ±0.56 50.50 ±3.01 ±710..8657* 73.81 ±1.81
59.80 ±2.55 59.50 ±2.32 53.62 ±0.83 68.44 ±2.90 ±761..1929* 77.81 ±2.35

90.00%
79.70 ±2.32 81.04 ±0.99 ±870..8327* 54.32 ±3.48 85.18 ±1.66 88.59 ±1.58
52.33 ±0.89 48.82 ±2.46 70.64 ±0.31 36.75 ±3.32 ±731..6053* 77.16 ±1.82
76.03 ±1.08 76.85 ±1.29 ±800..0145* 40.92 ±1.35 70.24 ±0.95 82.17 ±0.60
57.00 ±2.91 53.83 ±4.16 70.20 ±1.22 52.03 ±2.60 ±723..5789* 74.33 ±2.85
58.80 ±5.07 60.00 ±6.44 65.50 ±1.34 69.06 ±3.95 ±773..6501* 79.00 ±4.53

Wikipedia

Email-eu

· Supervised Component: Since it is a multi-label classification problem, each label can be treated as a binary classification problem. Therefore, we apply logistic regression for each label and for an arbitrary instance x and the i-th label yi, the supervised component is formulated as l(x, yi) = log(1+exp(wi x+bi))-yi(wi x+bi), where (wi  Rd, bi  R) is the classifier for the i-th label. Therefore, the supervised component in equation 5 is defined as vVlabel i l(x, yi) + wR(wi) and R(wi) is the regularization component for wi, where w is chosen to be 10-4.
8

Under review as a conference paper at ICLR 2019

BlogCatalog (macro)

BlogCatalog (micro)

Table 3: F1-score (macro, micro) (%) of multi-label classification in heterogeneous graphs

training%
deepwalk
node2vec
metapath2vec++ P2 IR
(Homogeneous) P2 IR
(Heterogeneous)
deepwalk
node2vec
metapath2vec++ P2 IR
(Homogeneous) P2 IR
(Heterogeneous)
Deepwalk
Node2vec
Metapath2vec++ P2 IR
(Homogeneous) P2 IR
(Heterogeneous)
deepwalk
node2vec
metapath2vec++ P2 IR
(Homogeneous) P2 IR
(Heterogeneous)

10.00%
45.13 ±0.68 45.78 ±0.68 37.46 ±0.61 ±473..6136*
49.65 ±0.63
47.93 ±0.48 48.52 ±0.50 40.90 ±0.34 ±502..7754*
53.06 ±0.45
±740..4384* 73.37 ±0.24 74.82 ±0.30 72.51 ±0.91
74.06 ±0.37
76.65 ±0.25 75.65 ±0.16 ±760..9281* 74.37 ±0.88
77.06 ±0.29

20.00%
44.64 ±0.21 45.42 ±0.30 36.72 ±0.36 ±500..9099*
51.47 ±0.40
47.36 ±0.15 48.20 ±0.18 40.06 ±0.21 ±540..2162*
54.77 ±0.21
74.87 ±0.14 73.94 ±0.11 75.27 ±0.08 ±760..8096*
78.43 ±0.61
77.03 ±0.19 76.21 ±0.12 77.38 ±0.10 ±780..5029*
80.67 ±0.45

30.00%
44.52 ±0.42 45.28 ±0.32 36.69 ±0.29 ±510..7109*
52.69 ±0.24
47.25 ±0.45 48.01 ±0.31 40.10 ±0.25 ±550..0088*
55.93 ±0.17
74.95 ±0.15 74.00 ±0.18 75.55 ±0.12 ±790..9019*
81.00 ±0.19
77.15 ±0.15 76.30 ±0.16 77.66 ±0.08 ±810..4171*
82.87 ±0.13

40.00%
44.64 ±0.23 45.41 ±0.18 36.58 ±0.28 ±501..0940*
53.37 ±0.45
47.30 ±0.19 48.18 ±0.17 39.97 ±0.22 ±531..4912*
56.41 ±0.31
75.10 ±0.22 74.25 ±0.23 75.63 ±0.27 ±820..1246*
83.18 ±0.16
77.21 ±0.15 76.48 ±0.18 77.70 ±0.21 ±830..5268*
84.75 ±0.16

50.00%
44.32 ±0.21 45.17 ±0.20 36.74 ±0.17 ±501..6703*
53.73 ±0.01
47.07 ±0.16 47.97 ±0.25 40.04 ±0.17 ±531..8780*
56.86 ±0.06
75.07 ±0.22 74.06 ±0.31 75.53 ±0.22 ±840..6402*
84.95 ±0.22
77.20 ±0.17 76.33 ±0.25 77.61 ±0.18 ±850..8412*
86.29 ±0.22

60.00%
44.36 ±0.46 45.19 ±0.36 36.90 ±0.33 ±500..3545*
53.97 ±0.43
47.09 ±0.48 48.04 ±0.39 40.22 ±0.29 ±530..5570*
57.28 ±0.38
75.44 ±0.22 74.52 ±0.21 75.92 ±0.24 ±860..3542*
86.91 ±0.54
77.60 ±0.20 76.82 ±0.22 78.04 ±0.18 ±870..5514*
88.09 ±0.47

70.00%
44.78 ±0.48 45.57 ±0.13 36.89 ±0.32 ±521..2101*
53.83 ±0.62
47.39 ±0.49 48.22 ±0.15 40.21 ±0.22 ±551..3360*
57.13 ±0.43
75.33 ±0.33 74.52 ±0.35 75.92 ±0.42 ±870..4391*
88.30 ±0.29
77.44 ±0.31 76.76 ±0.33 77.95 ±0.39 ±880..4247*
89.27 ±0.25

80.00%
44.33 ±0.62 45.04 ±0.31 36.42 ±0.41 ±511..8088*
54.07 ±0.40
47.02 ±0.72 47.83 ±0.32 39.82 ±0.44 ±541..9037*
57.43 ±0.14
74.75 ±0.31 74.32 ±0.26 75.56 ±0.36 ±880..2215*
89.18 ±0.17
76.87 ±0.35 76.44 ±0.34 77.54 ±0.36 ±890..1262*
90.11 ±0.13

90.00%
44.49 ±0.86 44.96 ±0.55 36.16 ±0.98 ±510..3266*
53.36 ±0.84
47.27 ±0.82 47.95 ±0.55 39.66 ±0.82 ±550..0532*
56.98 ±0.53
75.36 ±0.73 74.55 ±0.57 76.09 ±0.46 ±890..6518*
90.37 ±0.38
77.54 ±0.72 76.73 ±0.55 78.02 ±0.47 ±900..5540*
91.22 ±0.43

DBLP (macro)

DBLP (micro)

· Unsupervised Embedding Mapping Component: We design a two-layes NN with a 64dimensional hidden layer for each type of nodes with the types of nodes in its neighborhood to formulate the mapping from embedding of neighbors to the embedding of the target node. We also form a two-layer 1-to-1 NN wth a 3 dimensional hidden layer to construct the matrix function for the adjacency matrix A for the whole graph. We pre-process the matrix A with an eigenvalue decomposition by preserving the highest 1000 eigenvalues of magnitude in default. We denote the nodes to be classified as type 1 and the other type as type 2. The balance hyper-parameter [1, 2] is set to be [0.2, 200].
For the datasets DBLP and BlogCatalog, we carry out the experiments on each of them and compare the performance among all methods mentioned above. Since it is a multi-label classification task, we take f1-score(macro, micro) as the evaluation score for the comparison. The percentage of labeled samples is chosen from 10% to 90%, while the remaining samples are used for evaluation. We repeat all experiments for three times and report the mean and standard deviation of their performance in the Table 3. We can observe that in most cases, P2IR in heterogeneous setting has the best performance. P2IR in homogeneous setting is better than deepwalk and node2vec in the same homogeneous setting, and is even better than metapath2vec++ in heterogeneous setting (achieving the second best results). Overall, the superior performance of P2IR in Table 2 and 3 demonstrates the validity of our proposed universal graph embedding mechanism.
9

Under review as a conference paper at ICLR 2019
5 CONCLUSION AND FUTURE WORK
To summarize the whole work, we propose P2IR, a general graph embedding solution with the novel notion of partial permutation invariant set function, in principle that it can capture an arbitrary dependence between neighbors and automatically decide the significance of neighbor nodes at different distance for both homogeneous and heterogeneous graphs. We provide a theoretical guarantee for the effectiveness of the whole model. Through conducting extensive experimental evaluation, we show P2IR has better performance on both homogeneous and heterogeneous graphs, comparing to the stochastic trajectories based, matrix analytics based and graph neural network based state-of-the-art algorithms. For the future work, our model can be extended to more general cases, e.g., involving the rich content information out of graph neighborhood structures.
REFERENCES
Smriti Bhagat, Graham Cormode, and S Muthukrishnan. Node classification in social networks. In Social network data analytics, pp. 115­148. Springer, 2011.
Aleksandar Bojchevski and Stephan Günnemann. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. In International Conference on Learning Representations (ICLR), 2018.
HongYun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph embedding: Problems, techniques and applications. CoRR, abs/1709.07604, 2017.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM '15, pp. 891­900, 2015. ISBN 978-1-4503-3794-6.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph representations. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16, pp. 1145­ 1152, 2016.
Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and Thomas S Huang. Heterogeneous network embedding via deep architectures. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 119­128. ACM, 2015.
Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations (ICLR), 2018.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303­314, 1989.
Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In KDD '17, pp. 135­144. ACM, 2017.
Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75­174, 2010.
Hongchang Gao and Heng Huang. Self-paced network embedding. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '18, pp. 1406­1415, 2018.
Clyde Lee Giles, Kurt Dewitt Bollacker, and Steve Lawrence. Citeseer: An automatic citation indexing system. In Proceedings of the third ACM conference on Digital libraries, pp. 89­98. ACM, 1998.
Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications, and performance: A survey. CoRR, abs/1705.02801, 2017.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 855­864. ACM, 2016.
10

Under review as a conference paper at ICLR 2019
William Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NIPS, pp. 1024­1034. 2017a.
William Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. CoRR, abs/1709.05584, 2017b.
Simon Haykin. Neural networks: a comprehensive foundation. Prentice Hall PTR, 1994.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2): 251­257, 1991.
Xiao Huang, Jundong Li, and Xia Hu. Label informed attributed network embedding. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pp. 731­739. ACM, 2017.
Ming Ji, Yizhou Sun, Marina Danilevsky, Jiawei Han, and Jing Gao. Graph regularized transductive classification on heterogeneous information networks. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 570­586. Springer, 2010.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016a.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016b.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densification and shrinking diameters. ACM Transactions on Knowledge Discovery from Data (TKDD), 1(1):2, 2007.
David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. journal of the Association for Information Science and Technology, 58(7):1019­1031, 2007.
László Lovász. Random walks on graphs. Combinatorics, Paul erdos is eighty, 2(1-46):4, 1993.
Ian Grant Macdonald. Symmetric functions and Hall polynomials. Oxford university press, 1998.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. Information Retrieval, 3(2):127­163, 2000.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111­3119, 2013.
Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. Asymmetric transitivity preserving graph embedding. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pp. 1105­1114, 2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701­710. ACM, 2014.
Trang Pham, Truyen Tran, Dinh Q Phung, and Svetha Venkatesh. Column networks for collective classification. In AAAI, pp. 2485­2491, 2017.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pp. 593­607. Springer, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008.
Richard P Stanley. Enumerative combinatorics, volume 2. 2001.
Marshall H Stone. Applications of the theory of boolean rings to general topology. Transactions of the American Mathematical Society, 41(3):375­481, 1937.
11

Under review as a conference paper at ICLR 2019
Marshall H Stone. The generalized weierstrass approximation theorem. Mathematics Magazine, 21 (5):237­254, 1948.
Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1165­1174. ACM, 2015a.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pp. 1067­1077. International World Wide Web Conferences Steering Committee, 2015b.
Lei Tang and Huan Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447­478, 2011.
Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and Maosong Sun. Max-margin deepwalk: Discriminative learning of network representation. In IJCAI, pp. 3889­3895, 2016.
Ke Tu, Peng Cui, Xiao Wang, Philip S. Yu, and Wenwu Zhu. Deep recursive network embedding with regular equivalence. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '18, pp. 2357­2366, 2018.
Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix completion. stat, 1050:7, 2017.
Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pp. 1225­1234, 2016.
Xufei Wang, Lei Tang, Huiji Gao, and Huan Liu. Discovering overlapping groups in social media. In the 10th IEEE International Conference on Data Mining series (ICDM2010), Sydney, Australia, December 14 - 17 2010.
Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation learning with rich text information. In IJCAI, pp. 2111­2117, 2015.
Cheng Yang, Maosong Sun, Zhiyuan Liu, and Cunchao Tu. Fast network embedding enhancement via high order proximity approximation. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI, pp. 19­25, 2017.
Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML'16, pp. 40­48. JMLR.org, 2016.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp. 3394­3404, 2017.
Ziwei Zhang, Peng Cui, Xiao Wang, Jian Pei, Xuanrong Yao, and Wenwu Zhu. Arbitrary-order proximity preserved network embedding. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '18, pp. 2778­2786, 2018.
Dingyuan Zhu, Peng Cui, Daixin Wang, and Wenwu Zhu. Deep variational network embedding in wasserstein space. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '18, pp. 2827­2836, 2018.
12

Under review as a conference paper at ICLR 2019
Supplemental Material

We provide the proof to Theorem 3.1 in the supplemental material. The proof in our paper borrows some idea from the proof sketch for a special case of our theorem in (Zaheer et al., 2017). While (Zaheer et al., 2017) only provides a small paragraph to explain the raw idea on how to prove a special case, we provide complete and rigorous proof and nontrivial extension to the more general case.
Definition 5.1 (Power Sum Symmetric Polynomials). For every integer k  0, the k-th power sum symmetric polynomial in variables x1, x2, · · · , xn is defined as
n
pk(x1, x2, · · · , xn) = xki .
i=1
Definition 5.2 (Monomial Symmetric Polynomials). For  = [1, 2, . . . , n]  Rn where 1  2  · · ·  n  0, let  be the set of all permutations of the entries in . For , the monomial symmetric polynomials is defined as

m =

x11 x22 · · · xnn

[1,··· ,n] 

Lemma 5.3. If f : X  R is a continuous real-valued function defined on the compact set X  Rn, then  > 0, there exists a polynomial function p : X  R, such that supxX |f (x) - p(x)| < .

Proof. This lemma is a direct application of Stone-Weierstrass Theorem (Stone, 1937; 1948) for the compact Hausdorff space.

Corollary 5.3.1. For any continuous real-valued function f : X  R of the form defined in Theorem 3.1,

f (x1,1, x1,2, · · · , x1,N1 , x2,1, x2,2, · · · , x2,N2 , · · · , xK,1, xK,2, · · · , xK,NK ),

G1 G2

GK

there exist polynomial functions p : X  R also permutation invariant within each Gk to closely approximate f .

Proof. Let Sk be the set of all possible permutations of [1, · · · , Nk] for the group Gk, where k  [1, . . . , K]. Suppose k = [1k, 2k, . . . , Nk k ]  Sk is some permutation for Gk. Then we know that k  Sk with k  [1, . . . , K], we have

f (x1, x2, · · · , xK ) = f ([x1]1 , [x2]2 , · · · , [xK ]K )

where we let xk to denote [xk,1, xk,2, · · · , xk,Nk ] for Gk and [xk]k to denote the permuted xk for

simplicity. There are in total

K k=1

(Nk

!)

permutations.

By lemma 5.3, we can know that  > 0, there exists a polynomial function q : X  R such that sup |f (x1, x2, · · · , xK ) - q(x1, x2, · · · , xK )|  . This further implies that k  Sk with
k  [1, . . . , K], we have

sup |f ([x1]1 , [x2]2 , · · · , [xK ]K ) - q([x1]1 , [x2]2 , · · · , [xK ]K )| 

13

Under review as a conference paper at ICLR 2019

We let p(x1, x2, · · · , xK ) =

1

K k=1

(Nk

!)

1S1,··· ,K SK q([x1]1 , [x2]2 , · · · , [xK ]K ) and by

the property of permutation invariant within each Gk of the function f , we have that

sup |f (x1, x2, · · · , xK ) - p(x1, x2, · · · , xK )|

= sup f (x1, x2, · · · , xK ) -

1

K k=1

(Nk

!)

1S1,··· ,K SK

q([x1]1 , [x2]2 , · ·

·

,

[xK ]K )

= sup

1

K k=1

(Nk

!)

1S1,··· ,K SK

f ([x1]1 , · ·

·

, [xK ]K )

-

1

K k=1

(Nk

!)

1S1,··· ,K SK

q([x1]1 , · · ·

, [xK ]K )



1 Kk=1(Nk!) 1S1,··· ,K SK sup

f ([x1]1 , [x2]2 , · · ·

, [xK ]K ) - q([x1]1 , [x2]2 , · · · , [xK ]K )



1 Kk=1(Nk!) 1S1,··· ,K SK

=

Therefore, we can see that the polynomial function p(x1, x2, · · · , xK ) is permutation invariant within each group and can closely approximate the function f (x1, x2, · · · , xK ).

Lemma 5.4. Suppose p : X  R is a real-valued polynomial in the form

p(x1,1, x1,2, · · · , x1,N1 , · · · , xK,1, xK,2, · · · , xK,NK ),
G1 GK

which is permutation invariant within each group Gk, k  [1, . . . , K]. Then, p can be represented as

p(x1,1, x1,2, · · · , x1,N1 , · · · , xK,1, xK,2, · · · , xK,NK ) =

cim1 i1 m2 i2 . . . mKKi

i=1 G1 GK

where ci is the rational coefficient, mk iK denotes a monomial symmetric polynomial of variables xk,1, xk,2, · · · , xk,Nk within group Gk and Ki  Rn are some certain exponents for the monomial symmetric polynomial.

Proof. Suppose that the polynomial is expressed as a summation of monomials

p(x1,1, x1,2, · · · , x1,N1 , · · · , xK,1, xK,2, · · · , xK,NK ) =

cr

N1

x1r,j
1,j

N2

x2r,j
2,j

,

·

·

·

, xNK

Kr ,j K,j

r j=1

j=1

j=1

G1 GK

where kr,j is the exponent of xk,j for the r-th monomial.

We consider the term cr

xN1 1r,j
j=1 1,j

N2 j=1

xr2,j
2,j

·

·

·

xNK Kr ,j
j=1 K,j

for

a

certain

r

in

the

above

summation. Since p(·) is partially symmetric within the group G1, then there must exist

terms having exponents of permuted 1r,j, j  [1, . . . , N1] on x1,j while the other factors

cr

N2 j=1

x2r,j
2,j

···

xNK rK,j
j=1 K,j

remain the same.

(Otherwise, the polynomial is not permutation-

invariant w.r.t. group G1.) By summing up those terms together, we get a term with a factor of

monomial symmetric polynomial as crm1 1r

N2 j=1

xr2,j
2,j

···

NK j=1

xrK,j
K,j

.

Based on this term, we

consider that the polynomial p(·) is also permuted invariant within G2, which implies that there

must exist terms having exponents of permuted 2r,j, j  [1, . . . , N2] on x2,j while the other factors

cr m1r1

N3 j=1

x3r,j
3,j

·

·

·

NK j=1

xKr ,j
K,j

remain

the

same.

Therefore,

adding

up

all

those

terms

together,

we have crm11r m22r

N3 j=1

x3r,j
3,j

·

·

·

NK j=1

xrK,j
K,j

.

Carrying out the above procedures recursively,

we can eventually have crm11r m2r2 . . . mKrK . For all the remaining terms in the polynomial p(·),

performing the same steps will lead to completion of our proof.

14

Under review as a conference paper at ICLR 2019

Example 5.5. p(x11, x21, x21, x22) is a polynomial that is permutation invariant within each group, G1 = {x11, x12} and G2 = {x21, x22}. We let
p(x1,1, x1,2, x2,1, x2,2) =x1,1x12,2x2,1x22,2 + x12,1x1,2x22,1x2,2 + x1,1x21,2x22,1x2,2 + x21,1x1,2x2,1x22,2 + x12,1x31,2x23,1x42,2 + x31,1x21,2x42,1x32,2 + x21,1x31,2x24,1x23,2 + x31,1x21,2x23,1x42,2.
It is easy to observe that p(x1,1, x1,2, x2,1, x2,2) can be rewritten as
p(x1,1, x1,2, x2,1, x2,2) =(x1,1x12,2 + x21,1x1,2)(x2,1x22,2 + x22,1x2,2) + (x12,1x13,2 + x13,1x12,2)(x23,1x24,2 + x42,1x32,2) = m1(2,1)m(22,1) + m(13,2)m2(4,3).
Lemma 5.6. For a symmetric polynomial p(x1, . . . , xn) of n variables, it can be expressed by a polynomial in the power sum symmetric polynomials pk(x1, . . . , xn) for 1  k  n with rational coefficients.

Proof. This lemma is a direct result of the fact that p1, p2, . . . , pn are algebraically independent and the ring of symmetric polynomials with rational coefficients can be generated as a Q-algebra, i.e. Q[p1, p2, . . . , pn]. (Macdonald, 1998; Stanley, 2001) This lemma can also be easily proved by the combination of the fundamental theorem of symmetric polynomials and newton's identities.

Proof of Theorem 3.1

Proof. By Corollary 5.3.1, for any function f : X  R that are permutation-invariant w.r.t. the variables within each group Gk, k  [1, . . . , K], which is in the form

f (x1,1, x1,2, · · · , x1,N1 , x2,1, x2,2, · · · , x2,N2 , · · · , xK,1, xK,2, · · · , xK,NK ),

G1 G2

GK

we can always find a polynomial function p : X  R that are also permutation-invariant w.r.t. the variables within each group Gk to approximate f closely in a given small error tolerance .

Here we first define the function gk : R  RNk in the following form,

xk,n

xk2,n

gk (xk,n )

=

x3k,n

  

...

  

xNk,kn

which thus leads to



 Nk 

gk (xk,n )

=

 



n=1 



Nk n=1

xk,n

Nk n=1

x2k,n

 

 p1(xk,1, · · · , xk,Nk )   p2(xk,1, · · · , xk,Nk ) 

Nk
n=1
...

x3k,n  


=

   

p3(xk,1,

·

· ...

·

,

xk,Nk )

   

Nk n=1

xNk,kn

pNk (xk,1, · · · , xk,Nk )

Therefore, we generate a sequence of power sums basis by

Nk n=1

gk (xk,n ).

By Lemma 5.4, the polynomial function p(x1,1, x1,2, · · · , x1,N1 , · · · , xK,1, xK,2, · · · , xK,NK ) can be expressed as i=1 cim11i m2 2i . . . mKKi . Note that each mk ki is a symmetric polynomial, which thus can be rewritten as a polynomial expression of power sum basis,

p1(xk,1, · · · , xk,Nk ), p2(xk,1, · · · , xk,Nk ), · · · , pNk (xk,1, · · · , xk,Nk ).

which has been generated by

Nk n=1

gk

(xk,n).

15

Under review as a conference paper at ICLR 2019

Hence the polynomial p(x1,1, x1,2, · · · , x1,N1 , · · · , xK,1, xK,2, · · · , xK,NK ) is also a function of

Nk n=1

gk (xk,n ),

k



[1,

.

.

.

,

K ],

which

will

be

expressed

as

N1 N2

NK

p(x1,1, x1,2, · · · , x1,N1 , · · · , xK,1, xK,2, · · · , xK,NK ) = h( g1(x1,n), g2(x2,n), · · · , gK (xK,n)).

n=1

n=1

n=1

Thus, the function f could be approximate in any given error tolerance by a polynomial

h(

N1 n=1

g1(x1,n),

N2 n=1

g2(x2,n),

·

·

·

,

NK n=1

gK

(xK,n)),

which

finishes

our

proof.

16

