Under review as a conference paper at ICLR 2019

THE UNIVERSAL APPROXIMATION POWER OF FINITEWIDTH DEEP RELU NETWORKS
Anonymous authors Paper under double-blind review

ABSTRACT
We show that finite-width deep ReLU neural networks yield rate-distortion optimal approximation (Bo¨lcskei et al., 2018) of a wide class of functions, including polynomials, windowed sinusoidal functions, one-dimensional oscillatory textures, and the Weierstrass function, a fractal function which is continuous but nowhere differentiable. Together with the recently established universal approximation result for affine function systems (Bo¨lcskei et al., 2018), this demonstrates that deep neural networks approximate vastly different signal structures generated by the affine group, the Weyl-Heisenberg group, or through warping, and even certain fractals, all with approximation error decaying exponentially in the number of neurons. We also prove that in the approximation of sufficiently smooth functions finite-width deep networks require strictly fewer neurons than finite-depth wide networks.

1 INTRODUCTION
A theory establishing a link between the complexity of a neural network and the complexity of the function class to be approximated by the network was recently developed in Bo¨lcskei et al. (2018). Based on this framework, it was shown (Bo¨lcskei et al., 2018) that all affine function classes are optimally representable by neural networks in the sense of Kolmogorov rate-distortion theory (Donoho, 1993; Grohs, 2015). Equivalently, this means that the approximation error decays exponentially in the number of neurons employed in the approximation.
The present paper explores the question of whether the universality result established in Bo¨lcskei et al. (2018) extends beyond affine function classes and answers it in the affirmative. Specifically, we consider the approximation of polynomials, windowed sinusoidal functions (Gro¨chenig & Samarah, 2000; Gro¨chenig, 2001), one-dimensional oscillatory textures according to Demanet & Ying (2007), and the Weierstrass function, a fractal function which is continuous everywhere and differentiable nowhere. The central conclusion of this paper is that finite-width ReLU networks of depth scaling poly-logarithmically in the inverse of the approximation error lead to exponentially decaying approximation error for all these different signal structures. This result is established by building on a recent breakthrough in Yarotsky (2016) and recognizing that the width of networks approximating polynomials need not scale linearly in the degree of the polynomial, but can actually be finite independently of the degree. This insight will also allow a sharp statement on the benefit of depth; specifically, we prove that in the approximation of sufficiently smooth functions finite-width deep networks require strictly fewer neurons than finite-depth wide networks.
Notation. For the function f (x) : Rd  R, we define f L() := inf{C  0 : |f (x)|  C, for all x  }. For a vector b  Rd, we let b  := maxi=1,...,d |bi|, similarly we write A  := maxi,j |Ai,j| for the matrix A  Rm×n. We denote the identity matrix of size n × n by In. Throughout, log stands for the logarithm to base 2.

2 SETUP AND BASIC RELU CALCULUS
We start by defining ReLU neural networks. Definition 2.1. Let L, N0, N1, . . . , NL  N. A map  : RN0  RNL given by
(x) = WL( (WL-1( (. . .  (W1(x)))))),

(1)

1

Under review as a conference paper at ICLR 2019

with affine linear maps W : RN -1  RN ,  {1, 2, . . . , L}, and the ReLU activation function (x) = max(x, 0), x  R, acting component-wise (i.e., (x1, . . . , xN ) := ((x1), . . . , (xN ))) is called a (ReLU neural) network. The affine linear map W corresponding to layer is given by W (x) = A x + b , with A  RN ×N -1 and b  RN . We define the network connectivity as the total number of non-zero entries in the matrices A ,  {1, 2, . . . , L}, and the vectors b ,  {1, 2, . . . , L}. The depth of the network or, equivalently, the number of layers is L() := L, its width is given by W() := max =1,...,L N . We further denote by B() := max =1,...,L(max{ A , b }) the maximal absolute value of the weights of the
network.

We designate the class of ReLU networks  : Rd  RNL with no more than L layers, width
no more than M , input dimension d, and output dimension NL by N N L,M,d,NL . Note that the connectivity of  is upper-bounded by LM (M + 1).

For later use we record two technical results. The first one shows how to augment network depth while retaining the network's input-output relation and without increasing its width.1
Lemma 2.2. Let L, M, K, d  N, 1  N N L,M,d,1, and K > L. Then, there exists a corresponding network 2  N N K,max{2,M},d,1 such that 2(x) = 1(x) for all x  Rd. Moreover, the weights of 2 consist of the weights of 1 and ±1's.

The next result formalizes the concept of a linear combination of networks.

Lemma 2.3. Let N, Li, Mi, di  N, ai  R, i  N N Li,Mi,di,1, i = 1, 2, . . . , N , d =

N i=1

di

.

Then, there exist networks 1  N N L,M,d,N and 2  N N L,M,d,1 with L = maxi Li, and

M=

N i=1

Mi

satisfying

1(x) = (1(x1) 2(x2) . . . N (xN ))T and

N
2(x) = aii(xi),
i=1

for all x = (xT1 , x2T , . . . , xNT )T  Rd with xi  Rdi , i = 1, 2, . . . , N . Moreover, the weights of 1 consist of the weights of the networks i, i = 1, 2, . . . , N , and ±1's. The weights of 2 consist of the weights of 1 and {a1, a2, ..., aN }.

Remark 2.4. Note that if the networks i in Lemma 2.3 have shared inputs, the resulting networks

1 and 2 will have fewer than d =

N i=1

di

inputs.

3 APPROXIMATION OF POLYNOMIALS

This section shows how the multiplication operation and polynomials can be approximated to within error with ReLU networks of finite width and of depth poly-logarithmic in 1/ . We also note that the approximation results throughout the paper guarantee that the magnitude of the weights in the network does not grow faster than polynomially in the cardinality of the domain over which approximation takes place. Although not shown here for space constraints, the combination of finite width, depth scaling poly-logarithmically in 1/ , and weights growing no faster than polynomially guarantees rate-distortion optimality in the sense of Bo¨lcskei et al. (2018) and hence exponential error decay.

The proof ideas for the results in this section are inspired by Yarotsky (2016) and by the "sawtooth"
construction of Telgarsky (2015). In contrast to Yarotsky (2016), we consider networks without
"skip connections" and of finite and explicitly specified width. Before starting with the approximation of x2, we note that all our results apply to the multivariate case as well, but we restrict ourselves
to the univariate case for simplicity of exposition.

Proposition 3.1. There exists a constant C > 0 such that for all  (0, 1/2) there is a network   N N ,4,1,1 satisfying L( )  C log( -1), B( )  4,  (0) = 0, and

 (x) - x2 L([0,1])  .

(2)

1The proofs of the following two lemmata, along with all other proofs not provided in the paper, can be found in the supplement.

2

Under review as a conference paper at ICLR 2019

With Proposition 3.1 we are now ready to show how ReLU networks can approximate the multiplication operation, which will then lead us to the approximation of arbitrary powers of x.

Proposition 3.2. There exists a constant C > 0 such that for all D  R+ and  (0, 1/2) there is a network D,  N N ,12,2,1 satisfying L(D, )  C log( D 2 -1), B(D, )  max{4, 2 D 2},

D, (x, y) - xy L([-D,D]2)  ,

(3)

and D, (0, x) = D, (x, 0) = 0, for all x  R.

The next result establishes that arbitrary polynomials can be approximated by ReLU networks of finite and explicitly specified width and of depth growing logarithmically in the inverse of the approximation error. In particular, the width of the approximating network does not grow with the degree of the polynomial as is the case in Yarotsky (2016), Ding et al. (2018), Liang & Srikant (2017). This finite-width aspect is central to the approximation of sinusoidal functions by ReLU networks as described in the next section.

Proposition 3.3. There exists a constant C > 0 such that for all m  N, A  R+, pm(x) =

m i=0

aixi

with

maxi=0,...,m |ai|



A,

D



R+,

and

 (0, 1/2) there is a network pm,D, 

N N ,18,1,1 satisfying L(pm,D, )  Cm(log( A ) + log( -1) + m log( D ) + log(m)),

B(pm,D, )  max{A, 32 D 2m-2}, and

pm,D, - pm L([-D,D])  .

(4)

Proof. First note that for m = 1 the polynomial pm(x) is simply an affine transformation in which case the statement follows trivially. We hence need to treat the case m  2 only in the following.

The proof will be effected by observing that any monomial xk, k  2, can be approximated to

within error by iterative application of multiplication networks with the depth of the resulting composition scaling according to k(log( -1) + k log( D ) + log(k)), and combining this with a

construction that uses each monomial xk not only to compute the next degree monomial xk+1 but

also the partial sum

k i=0

aixi

in

parallel.

To this end set HDk , := D k + 

k-2 s=0

D

s, k



N, and let HDk ,,, D



R+,

k



N,

  (0, 1/2), be multiplication networks according to Proposition 3.2. For D  R+, k  N, and   (0, 1/2), we then recursively define2 kD, according to D0 ,(x) := 1, D1 ,(x) := x, and

kD,(x) = HDk-,1,(x, Dk-,1(x)), k  2. We first show by induction that for all   (0, 1/2), k  2,

k-2

kD,(x) - xk L([-D,D])  

D s.

(5)

s=0

The base case k = 2 follows from

D2 ,(x) - x2 L([-D,D]) = HD1 ,,(x, x) - x2 L([-D,D])  .

Next, we establish the induction step (k - 1)  k. The induction assumption is

k-3

Dk-,1(x) - xk-1 L([-D,D])  

D s.

s=0

(6)

Since kD-,1 L([-D,D])  xk-1 L([-D,D]) + Dk-,1(x) - xk-1 L([-D,D])  HDk-,1, Proposition 3.2 implies that

Dk ,(x) - xk L([-D,D])  HDk-,1,(Dk-,1(x), x) - xkD-,1(x) L([-D,D])

+ max |x|
[-D,D]

kD-,1(x) - xk-1

L ([-D,D])

k-3

k-2

  + D  D s =  D s,

s=0

s=0

2Note that these networks are well-defined in the sense of Definition 2.1, as for any L  N, there is a neural network of depth L realizing the function x  x.

3

Under review as a conference paper at ICLR 2019

which completes the induction. We are now ready to proceed to the construction of the net-

work pm,D, approximating the polynomial pm(x) =

m i=0

aixi

.

To this end, we note that

the identity mapping x  x and the linear combination x, y  x + ai-1y are affine trans-

formations and can hence be realized by a network of depth 1. By Lemma 2.3 there ex-

ists a constant C2 such that for every i  {2, 3, . . . , m},   (0, 1/2) there is a network

i
ai-1

,D,



N N ,18,3,3 with L(ai i-1,D,)



C2 log( HDi-,1 2-1) and B(ai i-1,D,)



max{4, 2 HDi-,1 2, maxi{1,...,m} |ai|}, realizing the map

(x s y)  x s + ai-1y HDi-,1,(x, y) .

The statements in the remainder of the proof apply to all m  N, A  R+, pm(x) =

m i=0

aixi

with maxi=0,...,m |ai| polynomial pm(x) =

 A, D  R+, and  (0, 1/2).

m i=0

aixi

is

constructed

as

The

network

pm,D,

approximating the

pm,D, (x) := (0

1

am

)

m
am-1

,D,

m-1
am-2

,D,

. . . 2a1,D,

10
0 x + a0 10

,

where  := ( A m2 D m)-1 . In particular, for all x  R, we have
m
pm,D, (x) = aiiD, (x).
i=0
Hence Equation 5 implies

m m i-2

pm,D, (x) - pm


L ([-D,D])

|ai| Di , (x) - xi L([-D,D]) 

|ai| 

Ds

i=0 i=2 s=0

m

  max |ai| (i - 1) D i-1  Am2 D m-1  .
i{1,...,m} i=2

Thanks to its compositional structure, the width of pm,D, equals the maximum width of the individual networks in the composition, i.e., W(pm,D, ) = 18. Since HDi-,1  2 D m-1, for i  m, we further have

mm

L(pm,D, ) 

L(pi m,D, ) 

C2 log( HDi-,1 2-1)

i=2 i=2

 C2m(log( A m2 D m-1 -1) + 2 log( 2 D m-1 ))

 C2m(log( A ) + log( -1) + 3(m - 1) log( D ) + 2 log(m) + 4)

 3C2m(log( A ) + log( -1) + m log( D ) + log(m))

and

B(pm,D,

)

=

max{1,

|a0

|,

|am

|,

max
i{2,3,...,m}

B(pi m

,D,

)}

 max{4, A, 2 2 D m-1 2}  max{A, 32 D 2m-2}.

This finalizes the proof.

We conclude this section with a result on ReLU networks approximating smooth functions with exponential accuracy. The proof of this statement, provided in the supplement, is based on the theory developed above.
Definition 3.4. For D  R+, let the set SD  C([-D, D], R) be given by

SD = f  C([-D, D], R) : f (n)(x) L([-D,D])  n!, for all n  N0 .

(7)

Lemma 3.5. There exist a constant C > 0 and a polynomial  such that for all D  R+, f  SD, and  (0, 1/2) there is a network f,  N N ,25,1,1 satisfying L(f, )  C D (log( -1))2, B(f, )  max{1/D, D ( -1)}, and

f, - f L([-D,D])  .

(8)

4

Under review as a conference paper at ICLR 2019

Note that for expositional simplicity Lemma 3.5 covers functions defined on symmetric intervals [-D, D]. Close inspection of the proof reveals, however, that for arbitrary intervals [a, b] and functions f  C([a, b], R) with f (n) L([a,b])  n!, for all n  N, the same statement (with D replaced by b - a in the bounds on L(f, ), B(f, )) holds.

4 APPROXIMATION OF SINUSOIDAL FUNCTIONS

We are now ready to proceed to the approximation of sinusoidal functions. Theorem 4.1. There exists a constant C such that for all a, D  R+,  (0, 1/2) there is a network a,D,  N N ,18,1,1 satisfying L(a,D, )  C((log(1/ ))2 + log( aD )), B(a,D, )  C, and
a,D, - cos(a · ) L([-D,D])  .

Proof. We start by approximating x  cos(2x) on [0, 1]. To this end note that the MacLaurin series representation of cos(x) is given by

cos(x) =  (-1)n x2n, (2n)!
n=0

x  R.

Thanks to the Taylor theorem with remainder in Lagrange form, we have for all x  [0, 1],

cos(2x) - N (-1)n (2x)2n  (2x)2N+1

sup

| cos(2N+1)(2t)| 

(2)4N +2 .

(9)

(2n)!
n=0

(2N + 1)! t[0,1]

(2N + 1)!

Next

observe

that

n!



(

n e

)ne,

for

all

n



N,

which

implies

that

for

all

N



N,

(2)4N +2 (2N + 1)!



(42 )2N +1

(

2N +1 e

)2N

+1e



42e 2N+1 .
2N + 1

(10)

Setting N := 22e log(2/ ) , we obtain for all  (0, 1/2),

42e 2N +1

42e

2 22e log(2/ ) +1

2N + 1

= 2 22e log(2/ ) + 1

(11)

 2- 22e log(2/ )  2- log(2/ ) = . 2

(12)

Note that C1 :=

maxnN0

(

(2)2n (2n)!

)

<  and N

 C2 log( -1), for all

 (0, 1/2), where

C2 := 82e. Therefore, application of Proposition 3.3 with m = N ,

N
pm(x) = pN (x) :=

(-1)n (2x)2n, (2n)!

n=0

and D = 1 establishes the existence of a constant C3 such that for all  (0, 1/2) there is a network  /2 satisfying

 /2(x) - pN (x)

, 2

L [-1,1]

(13)

with W( /2) = 18, B( /2)  C3, and

L( /2)  C3N (log(C1) + log(2/ ) + N log(1) + log(N ))  2C1C2(C3 + log(C2) + 4)(log( -1))2 = C4(log( -1))2,

where C4 := 2C1C2(C3 + log(C2) + 4). Combining Equation 9, Equation 11, Equation 10, and Equation 13 it follows that the network  /2 approximates the function x  cos(2x) on [0, 1] to
within accuracy , i.e., it holds, for all  (0, 1/2), that

 /2(x) - cos(2x) L[0,1]  .

(14)

5

Under review as a conference paper at ICLR 2019

We next extend this result to the approximation of x  cos(ax) on the interval [-1, 1] for arbitrary
a  R+. This will be accomplished by exploiting that x  cos(2x) is 1-periodic and even. First consider the function g : [0, 1]  [0, 1],

2x, g(x) =

if

x

<

1 2

,

2(1 - x),

if

x



1 2

,

along with the "sawtooth" function given by its s-fold composition

(15)

gs := g  g  · · ·  g, s  0,

(16)

s

where g0(x) := x and g1(x) := g(x). Note that it holds for every s  N that

gs(x) = 1,

for

x

=

2k-1 2s

with

k



{1, 2, . . . , 2s-1},

gs(x) = 0,

for

x

=

2k 2s

with

k



{0, 1, . . . , 2s-1},

and

gs

is

linear

on

[

k 2s

,

k+1 2s

],

for k  {0, 1, . . . , 2s}.

It is a bit tedious, albeit straightforward, to see that hence, for all s  N, x  [0, 1], it holds that

cos(22sx) = cos(2gs(x)).

Again using that cos(x) is even, one gets, for all s  N, x  [-1, 1], that cos(22sx) =
cos(2gs(|x|)). Moreover, note that for every a  R+ there exists a Ca  (1/2, 1] such that a/(2) = Ca2 log(a)-log(2) and thus we have for all a  R+, x  [-1, 1], that

cos(ax) = cos(22 log(a)-log(2) Cax) = cos 2g log(a)-log(2) (Cax) .

Since g log(a)-log(2) (Ca|x|)  [0, 1], for all a  R+, x  [-1, 1], it follows from Equation 14 that

 /2 g log(a)-log(2) (Ca|x|) - cos 2g log(a)-log(2) (Cax) L[-1,1]

=

 /2 g log(a)-log(2) (Ca|x|)

- cos(ax)

.

L [-1,1]

Next, note that x  |x| = (x) - (-x) = (1 -1) ((1 -1)T x) is a 2-layer network and take x  g log(a)-log(2) (Cax), a  R+, to be the network with architecture and weight assignment as described in the proof of Proposition 3.1. For every a  R+,  (0, 1/2), let

a, :=  /2(g log(a)-log(2) (Ca|x|))

and note that, for all a  R+, we have L(g log(a)-log(2) )  log(a) - log(2) + 1,
W(g log(a)-log(2) )  3, and B(g log(a)-log(2) )  4. Thus, there exists a constant C5 such that, for all a  R+,  (0, 1/2), it holds that L(a, )  C5((log(1/ ))2 + log( a )), W(a, ) = 18, and B(a, )  C5.

Finally, we extend the approximation of x  cos(ax) to intervals [-D, D], for arbitrary D  1. To

this end, we take for every a  R+, D  [1, ),



(0, 1/2)

the

network

a,D,

(x)

:=

aD,

(

x D

)

and observe that

sup |a,D, (x) - cos(ax)| = sup |a,D, (Dy) - cos(aDy)|

x[-D,D]

y[-1,1]

= sup |aD, (y) - cos(aDy)|  .
y[-1,1]

This concludes the proof.

An approximation result for sin(ax) can be obtained directly from Theorem 4.1 simply by noting that sin(x) = cos(x - /2), which can be realized by the concatenation of a neural network that performs an affine transformation and a network that approximates cos(x). The formal statement is

as follows.

Corollary 4.2. There exists a constant C > 0 such that for every a, D  R+, b  R,  (0, 1/2) there is a network a,b,D,  N N ,18,1,1 satisfying

a,b,D, - cos(a · - b) L([-D,D])  ,

(17)

with L(a,b,D, )  C((log( -1))2 + log( aD + |b| )) and B(a,b,D, )  C.

6

Under review as a conference paper at ICLR 2019

Figure 1: Left: A function in F1,100. Right: The function W 1 ,2. 2

5 OSCILLATORY TEXTURES AND THE WEIERSTRASS FUNCTION

Consider the following function class consisting of one-dimensional "oscillatory textures" according to Demanet & Ying (2007).

Definition 5.1. Let the sets FD,a, D, a  R+, be given by

FD,a = {cos(ag)h : g, h  SD} .

(18)

The efficient approximation of functions in FD,a with a large is a notoriously difficult problem due to the combination of the rapidly oscillating cosine term and the warping g. The best available approximation results in the literature (Demanet & Ying, 2007) are based on wave-atom dictionaries3
and yield low-order polynomial approximation rates. In what follows we show that finite-width deep
networks drastically improve these results to exponential approximation rates.

Proposition 5.2. There exist a constant C > 0 and a polynomial  such that for all D, a  R+, f  FD,a, and  (0, 1/2) there is a network f,  N N ,50,1,1 which satisfies

f, - f L([-D,D])  ,

(19)

with L(f, )  C( D (log( -1))2 + log( aD )) and B(f, )  max{1/D, ( -1, D , a )}.

We note that this result allows to show that local cosine bases (Gro¨chenig & Samarah, 2000) can be approximated by deep ReLU networks with exponential error decay.

Finally, we show how the Weierstrass function--a fractal function, which is continuous everywhere but differentiable nowhere--can be approximated with exponential accuracy by deep ReLU networks. Specifically, we consider


Wp,a(x) = pk cos(akx), for p  (0, 1/2], a  R+.
k=0

(20)

Let 

=

-

log(p) log(a)

.

It is well known (Zygmund, 2002) that Wp,a possesses Ho¨lder smoothness 

which may be arbitrarily small, depending on p and a, see Figure 1 right. While classical approxi-

mation methods, for instance sparse approximation in frames, are not suitable owing to the warping

operation, it turns out that deep finite-width networks achieve exponential approximation rate. The

corresponding formal statement is as follows.

Proposition 5.3. There exists a constant C > 0 such that, for all  (0, 1/2), p  (0, 1/2], a  R+, D  1, there is a network p,a,D,  N N ,22,1,1 satisfying

p,a,D, - Wp,a L([-D,D])  ,

(21)

with L(p,a,D, )  C((log(1/ ))3 + 2(log(1/ ))2 log( a ) + log(1/ ) log(D)) and B(p,a,D, )  C.

3To be precise, the results in Demanet & Ying (2007) are concerned with the two-dimensional case, whereas we focus on the one-dimensional case. Note, however, that all our results can be readily extended to the multivariate case.

7

Under review as a conference paper at ICLR 2019

6 FINITE DEPTH IS NOT ENOUGH

We next show that, in the approximation of periodic functions, finite-width deep networks require asymptotically smaller connectivity than finite-depth wide networks. This statement is then extended to sufficiently smooth non-periodic functions, thereby formalizing the benefit of deep networks over shallow networks in the approximation of a broad class of functions.
We start with preparatory material taken from Telgarsky (2015).
Definition 6.1 (Telgarsky (2015)). Let k  N. A function f : R  R is called k-sawtooth if it is piecewise linear with no more than k pieces, i.e., its domain R can be partitioned into k intervals such that f is linear on each of these intervals.
Lemma 6.2 (Telgarsky (2015)). Every   N N L,M,1,1 is (2M )L-sawtooth.
Definition 6.3. For a non-constant u-periodic function f  C(R), we define
(f ) := inf f (x) - (cx + d) L([,u+]).
[0,u), c,dR

The quantity (f ) measures the error incurred by the best linear approximation of f on any segment of length equal to the period of f ; It can hence be interpreted as quantifying the non-linearity of f . The next result states that finite-depth networks with width scaling poly-logarithmically in the highest frequency of the periodic function to be approximated can not achieve arbitrarily small approximation error.
Proposition 6.4. Let f  C(R) be a non-constant u-periodic function, let L  N and  a polynomial. Then there exists a  R+ such that for every network   N N L,M,1,1 with M  (log(a)) it holds that
f (ax) - (x) L([0,u])  (f ) > 0.

Application of Proposition 6.4 shows that finite-depth networks, owing to (cos) > 0, require faster than poly-logarithmic growth of connectivity in a to approximate x  cos(ax) with arbitrarily small error, whereas finite-width networks, thanks to Theorem 4.1, can accomplish this with polylogarithmic growth in connectivity. The next result, taken from (Frenzen et al., 2010), allows us to extend this conclusion to non-periodic sufficiently smooth functions.
Theorem 6.5 (Frenzen et al. (2010)). Let f  C3([a, b]) and consider a piecewise linear approximation of f on [a, b] that is accurate to within in the L([a, b])-norm. The minimal number of linear pieces required to accomplish this scales according to
s( )  c ,  0, where c = 1 b |f (x)|dx. 4a

Combining this with Lemma 6.2 yields the following statement on the depth-width tradeoff of networks approximating three-times continuously differentiable functions.

Theorem 6.6. Let f  C3([a, b]) with

b a

|f (x)|dx > 0, L  N, and let  be a polynomial.

Then there exists > 0 such that for every network   N N L,M,1,1 with M  (log( -1)) it

holds that

f -  L([a,b])  .

This shows that any function which is at least three times continuously differentiable cannot be approximated by finite-depth networks of connectivity scaling poly-logarithmically. In contrast, as Proposition 3.3 and Theorem 4.1 show, finite-width networks can approximate various interesting types of smooth functions such as polynomials and sinusoidal functions at poly-logarithmic connectivity growth rates. Further results on the limitations of finite-depth networks akin to Theorem 6.6 were reported recently in Petersen & Voigtlaender (2017).

8

Under review as a conference paper at ICLR 2019
REFERENCES
H. Bo¨lcskei, P. Grohs, G. Kutyniok, and P. Petersen. Optimal approximation with sparsely connected deep neural networks. arXiv:1705.01714, 2018.
L. Demanet and L. Ying. Wave atoms and sparsity of oscillatory patterns. Applied and Computational Harmonic Analysis, 23(3):368­387, 2007.
Y. Ding, J. Liu, and Y. Shi. On the universal approximability of quantized ReLU neural networks. arXiv:1802.03646, 2018.
D. L. Donoho. Unconditional bases are optimal bases for data compression and for statistical estimation. Applied and Computational Harmonic Analysis, 1:100­115, 1993.
C. L. Frenzen, T. Sasao, and J. T. Butler. On the number of segments needed in a piecewise linear approximation. Journal of Computational and Applied Mathematics, 234(2):437 ­ 446, 2010.
K. Gro¨chenig. Foundations of Time-Frequency Analysis. Birkha¨user Basel, 2001. K. Gro¨chenig and S. Samarah. Nonlinear approximation with local Fourier bases. Constructive
Approximation, 16(3):317­331, Jul 2000. P. Grohs. Optimally sparse data representations. volume 68, pp. 199­248. Birkha¨user, Cham, 2015. S. Liang and R. Srikant. Why deep neural networks for function approximation? In ICLR, 2017. P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep
ReLU neural networks. ArXiv e-prints, September 2017. M. Telgarsky. Representation benefits of deep feedforward networks. arXiv:1509.08101, 2015. D. Yarotsky. Error bounds for approximations with deep ReLU networks. arXiv:1610.01145, 2016. A. Zygmund. Trigonometric series. Cambridge University Press, 2002.
9

Under review as a conference paper at ICLR 2019

A PROOFS

A.1 PROOF OF LEMMA 2.2

Proof. The proof is based on the identity x = (x)-(-x). First, note that by definition Equation 1, we can write 1(x) = WL( . . . ). For K = L + 1, 2 is given by

2(x) = (1 -1) 

WL -WL

( . . . )

 N N L+1,max{2,M},d,1.

(22)

Now assume that K > L + 1 and consider the network

1(x) =

1 0

0 1



WL -WL

( . . . )

 N N L+1,max{2,M},d,2,

(23)

which satisfies 1(x) = ((1(x)) (-1(x))) , for all x  Rd. Next, we note that for every network of the form (x) = I2( . . . ), the network

 (x) := I2((x)),

(24)

satisfies  (x) = (x), for all x  Rd, L( ) = L() + 1.Moreover, the weights of  consist of the weights of  and {1}. Noting that 1 in Equation 23 is of the form I2( . . . ) and iteratively applying the operation in Equation 24 K - L - 2 times to 1, we obtain a network 1  N N K-1,max{2,M},d,2. The proof is concluded by noting that 2 = (1 - 1)(1 )  N N K,max{2,M},d,1 satisfies 2(x) = 1(x), for all x  Rd.

A.2 PROOF OF LEMMA 2.3
Proof. Apply Lemma 2.2 to the networks i to get corresponding networks ~ i of depth L and set 1(x) := ~ 1(x1), ~ 2(x2), . . . , ~ N (xN ) , 2(x) := (a1, a2, . . . , aN )1(x).

A.3 PROOF OF PROPOSITION 3.1

Proof. Consider the function g : [0, 1]  [0, 1],

2x, g(x) =

if

x

<

1 2

,

2(1 - x),

if

x



1 2

,

along with the "sawtooth" functions given by its s-fold composition

(25)

gs := g  g  · · ·  g, s  0
s

(26)

where g0(x) := x and g1(x) := g(x). We next briefly review a fundamental result from (Yarotsky, 2016) showing how the function f (x) := x2, x  [0, 1], can be approximated by linear combinations of "sawtooth" functions gs. Specifically, let fm be the piecewise linear interpolation of f with 2m+1
uniformly spaced "knots" according to

k fm 2m

=

k2 2m ,

k = 0, . . . , 2m,

m  N0.

The function fm approximates f with error m = 2-2m-2 in the sense of fm(x) - x2 L[0,1]  2-2m-2.

Next note that we can refine interpolations in the sense of going from fm-1 to fm by adjustment with a sawtooth function according to

This leads to the representation

fm(x)

=

fm-1(x)

-

gm(x) 22m

.

fm(x) = x -

m

gs(x) 22s

.

s=1

(27) (28)

10

Under review as a conference paper at ICLR 2019

While Yarotsky's construction (Yarotsky, 2016) is finalized by realizing Equation 28 through a deep

ReLU network with the help of skip connections, we proceed by constructing an equivalent network

without skip connections and of width 4. As g(x) = 2(x) - 4(x - 1/2) + 2(x - 1), it follows

that

gm = 2(gm-1) - 4(gm-1 - 1/2) + 2(gm-1 - 1),

(29)

and since fm = (fm), m  N0, Equation 27 can be rewritten as

fm = (fm-1) - 2-2m 2(gm-1) - 4(gm-1 - 1/2) + 2(gm-1 - 1) .

(30)

Equivalently Equation 29 and Equation 30 can be cast as a composition of affine linear maps and a ReLU nonlinearity according to

gm fm

= W1



W2

gm-1 fm-1

,

(31)

with

x1

1 0

0

W1(x) =

2 -2-2m+1

-4 2-2m+2

2 -2-2m+1

0 1

x2 x3

 

,

W2(x)

=

1  1

0  0

x1 x2

-11/2 .

x4

01

0

(32)

Applying Equation 31 iteratively initialized with g0(x) = x, f0(x) = x yields

gm fm

= W1 W2W1 . . . 

W2W1

W2

x x

,

(33)

and hence shows that fm can be realized through a network in N N m+1,4,1,1 with weights bounded (in magnitude) by 4. Since m = 2-2m-2 and hence log(1/ m) = 2m + 2, the statement follows.

A.4 PROOF OF PROPOSITION 3.2

Proof. The proof is based on the identity

xy

=

1 ((x

+

y)2

-

x2

-

y2),

2

(34)

which shows how multiplication can be implemented through the squaring operation. Let (x) be a neural network approximating x2 according to Proposition 3.1, i.e., (x) - x2 L[0,1]  ,
(0) = 0. We first extend this approximation result to the interval [-D, D], D > 1. Specifically, we need to find a ReLU network realization of x2 and y2 over [-D, D] and of (x + y)2 over [-D, D]2. This will be accomplished by first noting that

4 D 2

|x| 2D

- x2

 4 D 2,

L ([-D,D])

(35)

and likewise

4 D 2

|x + y| 2D

- (x + y)2

 4 D 2.

L ([-D,D])2

(36)

The network x  (|x|) has one layer more than the network x  (x) as it implements |x| = (x) + (-x) in its first layer. Next we define for every D  R+,   (0, 1/2) the network

D,(x, y) := 2 D 2



|x + y| 2D

- 

|x| 2D

- 

|y| 2D

,

(37)

and observe that Lemma 2.3 implies that there exists a constant C > 0 such that for all D  R+,   (0, 1/2), x  R it holds that L(D,)  C log(-1), W(D,) = 12, B(D,)  max{4, 2 D 2} and D,x, 0) = D ,(0, x) = 0. Using Equation 34 in combination with Equation 35 and Equation 36 yields

D, (x,

y)

-

1 2

(x + y)2 - x2 - y2

The proof is completed by taking for every D  R+, with D, := 6 D 2 .

 6 D 2.
L ([-D,D])2
 (0, 1/2) the network D,

:= D ,D,

11

Under review as a conference paper at ICLR 2019

A.5 PROOF OF LEMMA 3.5

Proof. We first consider the case D = 1. A fundamental result on Chebyshev interpolation, see e.g. (Liang & Srikant, 2017, Lemma 3), guarantees, for all f  S1, n  N, the existence of a polynomial Pf,n of degree n such that

f - Pf,n

L [-1,1]



1 2n (n+1)!

f (n+1)

L [-1,1]



1 2n

.

(38)

Writing the polynomials Pf,n as Pf,n =

n j=0

af,n,j xj ,

crude--but

sufficient

for

our

purposes--

estimates show that there exists a constant c > 0 such that for all f  S1, n  N it holds that

Af,n := max |af,n,j |  2cn.
j=0,...,n

Applying Proposition 3.3 to Pf,n yields the existence of a constant C1 > 0 such that for all f  S1, n  N,  (0, 1/2) there is a network Pf,n,1, /2  N N ,18,1,1 satisfying

Pf,n,1, /2 - Pf,n L[-1,1]  2 ,

(39)

B(Pf,n,1, /2)  max{Af,n, 8n2}  max{2cn, 8n2} and

L(Pf,n,1, /2)  C1n(cn + log(2/ ) + log(n)).

(40)

For s  S1,  (0, 1/2) we define n = log(2/ ) and f, = Pf,n ,1, /2. Combining Equation 38 and Equation 39 establishes that for all f  S1,  (0, 1/2) we have

f, - f L[-1,1]  f, - Pf,n L[-1,1] + Pf,n - f L[-1,1]



2

+

1 2n

2+2=

.

Note that for all  (0, 1/2), we have log(2/ )  2 log(2/ ) and log(2/ )  2 log(1/ ). Hence

Equation 40 implies the existence of a constant C2 and a polynomial 1 such that for all f  S1,  (0, 1/2) it holds that

L(f, ) = L(Pf,n ,1, /2)  C2(log( -1))2

(41)

and

B(f, ) = B(Pf,n ,1, /2)  max{2cn , 8n2}  max{4c -2c, 32(log -1)2}  1( -1). (42)

This completes the proof for the case D = 1.

We next prove the statement for D  (0, 1). To this end, we start by noting that for g  SD, with D  (0, 1), the function fg : [-1, 1]  R, x  g(Dx) is in S1. Hence there exists for every g  SD,  (0, 1/2) a network fg,  N N ,18,1,1 satisfying supx[-1,1] |fg, (x)-fg(x)|  , L(fg, )  C2(log(1/ ))2, and B(fg, )  1( -1). The claim is established by noting that

sup

|fg ,

(

x D

)

-

g(x)|

=

sup

|fg ,

(

x D

)

-

fg

(

x D

)|

x[-D,D]

x[-D,D]

= sup |fg, (x) - fg(x)|  .
x[-1,1]

It remains to prove the statement for the case D > 1. This will be accomplished by approximating f

on intervals of length 2 (or less) and stitching the resulting approximations together using a localized

partition of unity. To this end consider a, b  R such that 1  b - a  2, and let h  C([a, b], R)

with

h(n)

L ([a,b])

 n! for all n  N.

Next

note

that

the

function

x



h(

b-a 2

x

+

b+a 2

)

is

in

S1

since b - a  2. Hence there exists for every  (0, 1/2) a network h,  N N ,18,1,1 such that

supx[-1,1] |h,

(x)

-

h(

b-a 2

x

+

b+a 2

)|



, L(h, )  C2(log(1/ ))2, and B(h, )  1( -1).

Hence

the

networks

given

by

h,

(x)

:=

h,

(

2 b-a

x

-

b+a b-a

)

satisfy

sup |h, (x) - h(x)| =

sup

|h,

(y)

-

h(

b-a 2

y

+

b+a 2

)|



,

x[a,b]

y[-1,1]

(43)

L(h, )  C2(log(1/ ))2, W(h, ) = 18, and B(h, )  max{2, |b| + |a|, 1( -1)}. Now for

D

>

1 let ND



N such that 1



2D ND



2 and consider for k



{-ND, . . . , ND} the intervals

ID,k :=

(k-1)D ND

,

(k+1)D ND

.

12

Under review as a conference paper at ICLR 2019

By Equation 43 it follows that, for all D > 1, f  SD, k  {-ND, . . . , ND}, and  (0, 1/2), there exists a network f,k,  N N ,18,1,1 satisfying

sup |f,k, (x) - f (x)|  4 ,
xID,k

(44)

L(f,k, )  C2(log(4/ ))2, and B(f,k, )  max{2, 2|k|, 1( -1)}. We next build a partition

of unity through ReLU networks. Specifically, let (x) = (x + 1) - 2(x) + (x - 1) and

D,k



N N 2,3,1,1, D

>

1,

k



Z,

with D,k(x)

=

(

ND D

x

-

k).

Observe that for all D

>

1,

k  Z it holds that supp(D,k) = ID,k. Further note that for all x  R we have

D,k(x) = 1.
kZ
For D > 1, f  SD,  (0, 1/2) let f : R  R be the function given by

(45)

N

f (x) :=

2, /4(D,k(x), f,k, (x)),

k=-N

(46)

where 2, /4 is the multiplication network from Proposition 3.2. Note that |f (x)|, |k(x)|  1, for all x  [-D, D], k  {-N, . . . , N }. Observe further that, by definition, for each x  [-D, D]
there are no more than 2 indices k such that D,k(x) = 0. Thus Proposition 3.2 implies that the sum in Equation 46 has no more than 2 non-zero terms for each x  [-D, D]. Therefore, combining Equation 44, Equation 45, and Proposition 3.2 establishes that for all D > 1, f  SD,  (0, 1/2) holds

f - f L([-D,D])  .

It remains to be shown that the functions f can be realized by networks with the desired properties.
In particular we require finite-width, which means we cannot simply apply Lemma 2.3. To this end consider for every D > 1, f  SD, k  {1, . . . , 2N + 1},  (0, 1/2) the network f,k,  N N ,21,1,1 given by

f,k, (x) := 2, /4(k-(N+1)(x), f,k-(N+1), (x)), and network f,k,  N N ,25,3,3 given by

f,k, (x1, x2, x3) :=

x1 f,k, (x2)
x3

.

Further set 0(x) := (x, 0, 0)T and let A  R3×3 be such that A(y1, y2, y3)T = (y1, y1, y2 + y3)T ,for all y1, y2, y3  R. We can now define for every D > 1, f  SD,  (0, 1/2) the network f,  N N ,25,1,1 given by

f, (x) := (0 1 1) f,2N+1, (Af,2N, ( . . . (Af,1, (A0(x))))).

Direct calculation show that we indeed have f (x) = f, (x) for all D > 1, f  SD,  (0, 1/2), x  R. Furthermore, by construction there exists a constant C3 > 0 and a polynomial 3 such that for all D > 1, f  SD,  (0, 1/2) it holds that

W(f, ) = 2 +

max

W(f,k, ) + 2 = 25,

k{1,...,2ND +1}

that

2ND +1

2ND +1

L(f, ) 

L(f,k, ) =

L(2, /4) + max{L(k-(N+1)), L(f,k-(N+1), )}

k=1

k=1

 (2ND + 1)(log(16 -1) + max{2, C2(log(4 -1))2})  C3D(log( -1))2,

and that

B(f, ) =

max

B(f,k, )  max{8, 2|ND|, 1( -1)}  D 2( -1).

k{1,...,2ND +1}

(47)

This completes the proof.

13

Under review as a conference paper at ICLR 2019

A.6 PROOF OF COROLLARY 4.2

Proof. For every a, D  R+, b  R,  (0, 1/2) take the network given by a,b,D, (x) :=

a,D+

|b| a

,

(x -

b a

)

with

a,D+

|b| a

,

according to Theorem 4.1 and observe that

sup |a,b,D,
x[-D,D]

(x) - cos(ax - b)|

=

[-(D+

sup

|b| a

),D+

|b| a

]

|a,D+

|b| a

,

(y) - cos(ay)|



.

Applying Theorem 4.1 completes the proof.

A.7 PROOF OF PROPOSITION 5.2

Proof. For all D, a  R+, f  FD,a, let gf , hf  SD be functions such that f = cos(agf )hf
holds. Note that Lemma 3.5 guarantees the existence of a constant C1 > 0 and a polynomial 1 such that for all D, a  R+, f  FD,a,  (0, 1/2) there are networks hf , , gf ,  N N ,25,1,1 which satisfy L(hf , ), L(gf , )  C1 D (log([ 12 a ]-1))2, B(hf , ), B(gf , )  max{1/D, D 1([ 12 a ]-1)} and

gf , - gf L([-D,D]),

hf , - hf L([-D,D])  12 a .

(48)

Theorem 4.1 further ensures the existence of a constant C2 > 0 such that for all a, D  R+,  (0, 1/2) there is a neural network a,D,  N N ,18,1,1 which satisfies L(a,D, ) 
C2((log(1/ ))2 + log( aD )), B(a,D, )  C2, and

a,D, - cos(a · ) L([-D,D])  3 .

(49)

Further, thanks to Proposition 3.2, there exists a constant C3 > 0 such that for all  (0, 1/2) there is a network µ  N N ,12,2,1 which satisfies L(µ )  C3 log(1/ ), B(µ )  max{4, 2 D 2}, and

sup |µ (x, y) - xy|  3 .
x,y[-D,D]

(50)

For all D, a  R+, f  FD,a,  (0, 1/2) we define the neural networks

f, := µ (a,D, (gf , ), hf , ).

(51)

First,

observe

that

Equation

48,

Equation

49,

and

supxR

|

d dx

cos(ax)|

=

a

imply

for

all

x



[-D, D] that

|a,D, (gf , (x)) - cos(agf (x))|  |a,D, (gf , (x)) - cos(agf , (x))|

+ | cos(agf , (x)) - cos(agf (x))|

 3 + a 12 a



5 12

.

Combining this with Equation 48, Equation 50, and cos L[-D,D], f L[-D,D]  1 yields for all x  [-D, D] that

|f, (x) - f (x)| = |µ (a,D, (gf , (x)), hf , (x)) - cos(agf (x))hf (x)|

 |µ (a,D, (gf , (x)), hf , (x)) - a,D, (gf , (x))hf , (x)|

+ |a,D, (gf , (x))hf , (x) - cos(agf (x))hf (x)|



3

+

5 12

+

12

a

+

5 12

12

a



.

By construction there exists a constant C4 and a polynomial 2 such that for all D, a  R+, f  FD,a,  (0, 1/2) it holds that W(f, ) = 50,
L(f, )  L(µ ) + max{L(a,D, ) + L(gf , ), L(hf , )}  C4 D ((log( -1))2 + log( a )),
and B(f, )  max{B(µ ), B(a,D, ), B(gf , ), B(hf , )}  max{1/D, 2( -1, D , a )}.
This completes the proof.

14

Under review as a conference paper at ICLR 2019

A.8 PROOF OF PROPOSITION 5.3

Proof. For every N  N, p  (0, 1/2), a  R+, x  R, let SN,p,a(x) =

N k=0

pk

cos(akx).

The

geometric sum formula ensures that



|SN,p,a(x) - Wp,a(x)| 

|pk cos(akx)| 

pk

=

1 1-p

-

1-pN +1 1-p

 2-N .

k=N +1

k=N +1

(52)

Let N := log(2/ ) ,  (0, 1/2). Next note that Theorem 4.1 ensures the existence of a constant
C1 > 0 such that for all a, D  R+, k  N0,  (0, 1/2) there is a network ak,D,  N N ,18,1,1 which satisfies L(ak,D, )  C1((log( -1))2 + log( akD )), B(ak,D, )  C1, and

ak,D, - cos(ak · ) L([-D,D])  4 .

(53)

Thanks to x = (x) - (-x) and Lemma 2.2, there exists, for every L  N, a neural network
L  N N L,2,1,1 which satisfies for all x  R that L(x) = x. For all p  (0, 1/2), a, D  R+, k  N0,  (0, 1/2), define the neural networks

x Dp,,a,0(x) = p0a0,D, (x)
0

 x1  and Dp,,a,k(x1, x2, x3) = pkak,D, (x2) , k > 0,
x3

(54)

and let A  R3×3 be such that A(y1, y2, y3)T = (y1, y1, y2 + y3)T , for all y  R3. Consider now, for p  (0, 1/2), a, D  R+,  (0, 1/2), the network p,a,D, defined by

p,a,D, (x) := (0 1 1) Dp,,a,N (ADp,,a,N -1(. . . (ADp,,a,0(x)))).

(55)

Note Equation 53 combined with the geometric sum formula implies that for all p  (0, 1/2), a, D  R+,  (0, 1/2), x  [-D, D] we have

NN

|p,a,D, (x) - SN ,p,a(x)| =

pkak,D, (x) - pk cos(akx)

k=0

k=0

N

 pk|ak,D, (x) - cos(akx)|  4 2-k = 2 .

k=0

k=1

Combining this with Equation 52 establishes that for all p  (0, 1/2), a, D  R+,  (0, 1/2), x  [-D, D] it holds

|p,a,D, (x) - Wp,a(x)|  2- log( 2 ) + 2  2 + 2 = .
By construction there exists a constant C2 such that for all p  (0, 1/2), a, D  R+, we have W(p,a,D, ) = 22,

 (0, 1/2)

N
L(p,a,D, )  L(ak,D, )  (N + 1)C1((log( -1))2 + log( aN D ))
k=0
 C2((log( -1))3 + (log( -1))2 log( a ) + log( -1) log(D)),

and

B(p,a,D, ) = max B(ak,D, ) = C1.
k{0,...,N }

This completes the proof.

A.9 PROOF OF PROPOSITION 6.4
Proof. First note that for every polynomial ~ it holds that ~(log(t))  O(t), t  . Since x  (2(x))L is a polynomial, there exists a  N such that a > (2(log(a)))L. Lemma 6.2 now implies that any network   N N L,M,1,1 with M  (log(a)) is (2(log(a)))L-sawtooth and therefore
15

Under review as a conference paper at ICLR 2019

has less than a-many different linear pieces. Hence there exists an interval [u1, u2]  [0, u] with u2 - u1  (u/a) on which  is linear. Since u2 - u1  (u/a) the interval supports a full period of f (a · ) and we can therefore conclude that

f (a · ) -  L([0,u]) 

f (a·) - 

L ([u1 ,u2 ])



inf
[0,u),

f (x) - (cx + d) L([,u+]) = (f ).

c,dR

Finally note, that (f ) > 0 holds by assumption, since any continuous u-periodic function which is linear on an interval of length u must be constant.

A.10 PROOF OF THEOREM 6.6

Proof. The proof will be effected by contradiction. Assume that for every > 0 there exists a

network   N N L,M,1,1 with M  (log( -1)) and f -  L([a,b])  . Since every ReLU

network is piecewise linear we can now apply Theorem 6.5 to conclude that there exists a constant

C such that for all

> 0 the network 

must

have

at

least

C

-

1 2

many

different linear

pieces.

This leads to a contradiction as, by assumption combined with Lemma 6.2,  is (2(log( -1)))L-

sawtooth and it holds for every polynomial ~ that ~(log( -1))  o( -1/2),  0.

16

