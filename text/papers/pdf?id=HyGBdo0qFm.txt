Under review as a conference paper at ICLR 2019
ON THE TURING COMPLETENESS OF MODERN NEURAL NETWORK ARCHITECTURES
Anonymous authors Paper under double-blind review
ABSTRACT
Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.
1 INTRODUCTION
There is an increasing interest in designing neural network architectures capable of learning algorithms from examples (Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015; Kaiser & Sutskever, 2016; Kurach et al., 2016; Dehghani et al., 2018). A key requirement for any such an architecture is thus to have the capacity of implementing arbitrary algorithms, that is, to be Turing complete. Turing completeness often follows for these networks as they can be seen as a control unit with access to an unbounded memory; as such, they are capable of simulating any Turing machine.
On the other hand, the work by Siegelmann & Sontag (1995) has established a different way of looking at the Turing completeness of neural networks. In particular, their work establishes that recurrent neural networks (RNNs) are Turing complete even if only a bounded number of resources (i.e., neurons and weights) is allowed. This is based on two conditions: (1) the ability of RNNs to compute internal dense representations of the data, and (2) the mechanisms they use for accessing such representations. Hence, the view proposed by Siegelmann & Sontag shows that it is possible to release the full computational power of RNNs without arbitrarily increasing its model complexity.
Most of the early neural architectures proposed for learning algorithms correspond to extensions of RNNs ­ e.g., Neural Turing Machines (Graves et al., 2014) ­, and hence they are Turing complete in the sense of Siegelmann & Sontag. However, a recent trend has shown the benefits of designing networks that manipulate sequences but do not directly apply a recurrence to sequentially process their input symbols. Architectures based on attention or convolutions are two prominent examples of this approach. In this work we look at the problem of Turing completeness a` la Siegelmann & Sontag for two of the most paradigmatic models exemplifying these features: the Transformer architecture (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016).
The main contribution of our paper is to show that the Transformer and the Neural GPU are Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. We prove both results by simulating standard sequence-tosequence RNNs. Our study also reveals some minimal sets of elements needed to obtain these completeness results. This might be of help in understanding how such elements can be exploited to obtain full computational power at a minimal computational cost.
Background work The study of the computational power of neural networks can be traced back to McCulloch & Pitts (1943) which established an analogy between neurons with hard-threshold
1

Under review as a conference paper at ICLR 2019

activations and first order logic sentences, and Kleene (1956) that draw a connection between neural networks and finite automata. As mentioned earlier, the first work showing the Turing completeness of finite neural networks with linear connections was carried out by Siegelmann & Sontag (1992; 1995). Since being Turing complete does not ensure the ability to actually learn algorithms in practice, there has been an increasing interest in enhancing RNNs with mechanisms for supporting this task. One strategy has been the addition of inductive biases in the form of external memory, being the Neural Turing Machine (NTM) (Graves et al., 2014) a paradigmatic example. To ensure that NTMs are differentiable, their memory is accessed via a soft attention mechanism (Bahdanau et al., 2014). Other examples of architectures that extend RNNs with memory are the Stack-RNN (Joulin & Mikolov, 2015), and the (De)Queue-RNNs (Grefenstette et al., 2015). By Siegelmann & Sontag's results, all these architectures are Turing complete.
The Transformer architecture (Vaswani et al., 2017) is almost exclusively based on the attention mechanism, and it has achieved state of the art results on many language-processing tasks. While not initially designed to learn general algorithms, a parallel study has advocated the need for enriching its architecture with several new features as a way to learn general procedures in practice (Dehghani et al., 2018). This enrichment is motivated by the empirical observation that the original Transformer architecture struggles to generalize to input of lengths not seen during training. We, in contrast, show that the original Transformer architecture is Turing complete, based on different considerations. These results do not contradict each other, but show the differences that may arise between theory and practice. In fact, we think that both approaches can be complementary as our theoretical results can shed light on what are the intricacies of the original architecture, which aspects of it are candidates for change or improvement, and which others are strictly needed. For instance, our proof uses additive hard attention while the Transformer is often trained with multiplicative soft attention (Vaswani et al., 2017). See Section 3.3 for a discussion on these differences.
The Neural GPU is an architecture that mixes convolutions and gated recurrences over tridimensional tensors. It has been shown that NeuralGPUs are powerful enough to learn decimal multiplication from examples (Freivalds & Liepins, 2018), being the first neural architecture capable of solving this problem end-to-end. The similarity of Neural GPUs and cellular automata has been used as an argument to state the Turing completeness of the architecture (Kaiser & Sutskever, 2016; Price et al., 2016). Cellular automata are Turing complete (Smith III, 1971; Ollinger, 2012) and their completeness is established assuming an unbounded number of cells. In the Neural GPU architecture, in contrast, the number of cells that can be used during a computation is proportional to the size of the input sequence (Kaiser & Sutskever, 2016). One can cope with the need for more cells by padding the Neural GPU input with additional (dummy) symbols, as much as needed for a particular computation. Nevertheless, this is only a partial solution, as for a Turing-complete model of computation, one cannot decide a priori how much memory is needed to solve a particular problem. Our results in this paper are somehow orthogonal to the previous argument; we show that one can leverage the dense representations of the Neural GPU cells to obtain Turing completeness without requiring to add cells beyond the ones used to store the input.

2 PRELIMINARIES

We assume all weights to be rational numbers in Q. Moreover, we only allow the use of rational functions with rational coefficients. Most of our positive results make use of the piecewise-linear sigmoidal activation function  : Q  Q, which is defined as

(x) =

0 x < 0, x 0  x  1, 1 x > 1.

We are mostly interested in sequence-to-sequence (seq-to-seq) neural network architectures that we next formalize. A seq-to-seq network N receives as input a sequence X = (x1, . . . , xn) of vectors xi  Qd, for some d > 0, and produces as output a sequence Y = (y1, . . . , ym) of vectors yi  Qd. Most of these types of architectures require a seed vector s and some stopping criterion for determining the length of the output. The latter is usually based on the generation of a particular
output vector called an end of sequence mark. In our formalization instead, we allow a network to produce a fixed number r  0 of output vectors. Thus, for convenience we see a general seq-toseq network as a function N such that the value N (X, s, r) corresponds to an output sequence of

2

Under review as a conference paper at ICLR 2019

the form Y = (y1, y2, . . . , yr). With this definition, we can view every seq-to-seq network as a language recognizer of strings as follows.
Definition 2.1. A seq-to-seq language recognizer is a tuple A = (, f, N, s, F), where  is a finite alphabet, f :   Qd is an embedding function, N is a seq-to-seq network, s  Qd is a seed vector, and F  Qd is a set of final vectors. We say that A accepts the string w  , if there exists an integer r  N such that N (f (w), s, r) = (y1, . . . , yr) and yr  F. The language accepted by A, denoted by L(A), is the set of all strings accepted by A.

We impose two additional restrictions over recognizers. The embedding function f :   Qd should be computed by a Turing machine in time linear w.r.t. the size of . This covers the two most typical ways of computing input embeddings from symbols: the one-hot encoding, and embeddings computed by fixed feed-forward networks. Moreover, the set F should also be recognizable in lineartime; given a vector f , the membership f  F should be decided by a Turing machine working in linear time with respect to the size (in bits) of f . This covers the usual way of checking equality with a fixed end-of-sequence vector. We impose these restrictions to disallow the possibility of cheating by encoding arbitrary computations in the input embedding or the stop condition, while being permissive enough to construct meaningful embeddings and stoping criterions.
Finally, a class N of seq-to-seq neural network architectures defines the class LN composed of all the languages accepted by language recognizers that use networks in N . From these notions, the formalization of Turing completeness of a class N naturally follows.
Definition 2.2. A class N of seq-to-seq neural network architectures is Turing Complete if LN is exactly the class of languages recognized by Turing machines.

Given an input sequence X = (x1, . . . , xn), a seed vector y0, and r  N, an encoder-decoder RNN is given by the following two recursions

h0 = 0, hi = f1(xiW + hi-1V + b1) (with 1  i  n) g0 = hn, gt = f2(gt-1U + yt-1R + b2), yt = O(gt) (with 1  t  r)

(1) (2)

where V , W , U , R are matrices, b1 and b2 are vectors, O(·) is an output function, and f1 and f2 are activations functions. Equation (1) is called the RNN-encoder and (2) the RNN-decoder.

The next Theorem follows by inspection of the proof by Siegelmann & Sontag (1992; 1995) after adapting it to our formalization of encoder-decoder RNNs.
Theorem 2.3 (Siegelmann & Sontag (1992; 1995)). The class of encoder-decoder RNNs is Turing complete. Turing completeness holds even if we restrict to the class in which R is the zero matrix, b1 and b2 are the zero vector, O(·) is the identity function, and f1 and f2 are the piecewise-linear sigmoidal activation .

3 THE TRANSFORMER ARCHITECTURE

In this section we present a formalization of the Transformer architecture (Vaswani et al., 2017), abstracting away from specific choices of functions and parameters. Our formalization is not meant to produce an efficient implementation of the Transformer, but to provide a simple setting over which its mathematical properties can be established in a clean way.

The Transformer is heavily based on the attention mechanism introduced next. Consider a scoring
function score : Qd × Qd  Q and a normalization function  : Qn  Qn, for d, n > 0. Let us assume that q  Qd, and that K = (k1, . . . , kn) and V = (v1, . . . , vn) are tuples of elements in Qd. A q-attention over (K, V ), denoted by Att(q, K, V ), is a vector a  Qd defined as follows.

(s1, . . . , sn) = (score(q, k1), score(q, k2), . . . , score(q, kn)) a = s1v1 + s2v2 + · · · + snvn.

(3) (4)

Usually, q is called the query, K the keys, and V the values. We do not pose any restriction

on the scoring and normalization functions, as some of our results hold in general. We only re-

quire the normalization function to satisfy that there is a function f from Q to Q+ such that for

each x f(xi)/

=

nj(=x11f,.

.., (xj

xn)  ). Thus,

Qn it is the case a in Equation (4)

that the i-th component i(x) of (x) is equal is a convex combination of the vectors in V .

to

3

Under review as a conference paper at ICLR 2019

When proving possibility results, in particular Turing completeness, we will need to pick specific

scoring and normalization functions. A usual choice for the scoring function is a feed forward net-

work with input (q, ki) and a single neuron as output (Bahdanau et al., 2014), sometimes called additive attention. Another possibility is to use the dot product q, ki (Vaswani et al., 2017), called

multiplicative attention. For the normalization function, softmax is a standard choice. Neverthe-

less, in our possibility proofs we mostly use the hardmax function, which is obtained by setting

fhardmax(xi) = 1 if xi is the maximum value, and fhardmax(xi) = 0 otherwise. Thus, for a vector

x in which the maximum value

occurs

r

times, we

have that hardmaxi(x)

=

1 r

if

xi

is the maxi-

mum value of x, and hardmaxi(x) = 0 otherwise. We call it hard attention whenever hardmax is

used as normalization function. As customary, if F : Qd  Qd and X = (x1, x2, . . . , xn), with

xi  Qd, we write F (X) for the sequence (F (x1), . . . , F (xn)).

Transformer Encoder and Decoder A single-layer encoder of the Transformer is a parametric function Enc(X; ) receiving a sequence X = (x1, . . . , xn) of vectores in Qd and returning a sequence Z = (z1, . . . , zn) of the same length of vectors in Qd. In general, we consider the parameters in Enc(X; ) as functions Q(·), K(·), V (·), and O(·), all of them from Qd to Qd. The
single-layer encoder is then defined as follows.

ai = Att(Q(xi), K(X), V (X)) + xi zi = O(ai) + ai

(5) (6)

In practice Q(·), K(·), V (·) are typically matrix multiplications, and O(·) a feed-forward network.
The + xi and + ai summands are usually called residual connections (He et al., 2016a;b). When the particular functions used as parameters are not important, we simply write Z = Enc(X).

The Transformer encoder is defined simply as the repeated application of single-layer encoders (with independent parameters), plus two final transformation functions K(·) and V (·) applied to
every vector in the output sequence of the final layer. Thus the L-layer Transformer encoder is defined by the following recursion (with 1   L - 1 and X1 = X).

X +1 = Enc(X ;  ), K = K(XL), V = V (XL).

(7)

We use (K, V ) = TEncL(X) to denote an L-layer Transformer encoder over the sequence X.
A single-layer decoder is similar to a single-layer encoder but with additional attention to an external pair of key-value vectors (Ke, V e). The input for the single-layer decoder is a sequence Y = (y1, . . . , yk) plus the external pair (Ke, V e), and the output is a sequence Z = (z1, . . . , zk). This layer is also parameterized by four functions Q(·), K(·), V (·) and O(·) and is defined as follows.

pi = Att(Q(yi), K(Y ), V (Y )) + yi ai = Att(pi, Ke, V e) + pi
zi = O(ai) + ai

(8) (9) (10)

Notice that the first (self) attention over (K(Y ), V (Y )) is used to generate a query pi to attend the external pair (Ke, V e). We denote this single-decoder layer by Dec((Ke, V e), Y ; ).

The Transformer decoder is a repeated application of single-layer decoders, plus a transformation function F : Qd  Qd which is applied over the final vector of the decoded sequence. Thus, the output of the decoder is a single vector z  Qd, and not a sequence of them as in the case of the
encoder. Formally, the L-layer Transformer decoder is defined as

Y +1 = Dec((Ke, V e), Y ;  ), z = F (ykL) (1   L - 1 and Y 1 = Y ). (11)

We use z = TDecL((Ke, V e), Y ) to denote an L-layer Transformer decoder.

The complete Tansformer A Transformer network receives an input sequence X, a seed vector y0, and a value r  N. Its output is a sequence Y = (y1, . . . , yr) defined as

yt+1 = TDec(TEnc(X), (y0, y1, . . . , yt)),

for 0  t  r - 1.

(12)

We denote the output sequence of the transformer as Y = (y1, y2, . . . , yr) = Trans(X, y0, r).

4

Under review as a conference paper at ICLR 2019
3.1 INVARIANCE UNDER PROPORTIONS
The Transformer, as defined above, is order-invariant: two input sequences that are permutations of each other produce exactly the same output. This is a consequence of the following straightforward property of the attention function: if K = (k1, . . . , kn), V = (v1, . . . , vn), and  : {1, . . . , n}  {1, . . . , n} is a permutation, then Att(q, K, V ) = Att(q, (K), (V )) for every query q. This weakness has motivated the need for including information about the order of the input sequence by other means; in particular, this is often achieved by using the so-called positional encodings (Vaswani et al., 2017; Shaw et al., 2018), which we study below.
But before going into positional encodings, a natural question is what languages the Transformer can recognize without them. As a standard yardstick we use the well-studied class of regular languages, i.e., languages recognized by finite automata. Order-invariance implies that not every regular language can be recognized by a Transformer network. As an example, there is no Transformer network that can recognize the regular language (ab), as the latter is not order-invariant. A reasonable question then is whether the Transformer can express all regular languages which are order-invariant. It is possible to show that this is not the case by proving that the Transformer actually satisfies a stronger invariance property, which we call proportion invariance. For a string w   and a symbol a  , we use prop(a, w) to denote the ratio between the number of times that a appears in w and the length of w. Consider now the set PropInv(w) = {u   | prop(a, w) = prop(a, u) for every a  }. Then: Proposition 3.1. Let Trans be a Transformer, s a seed, r  N, and f :   Qd an embedding function. Then Trans(f (w), s, r) = Trans(f (u), s, r), for each u, w   with u  PropInv(w).
As an immediate corollary we obtain the following. Corollary 3.2. Consider the order-invariant regular language L = {w  {a, b} | w has an even number of a symbols}. Then L cannot be recognized by a Transformer network.
On the other hand, languages recognized by Transformer networks are not necessarily regular. Proposition 3.3. There is a Transformer network that recognizes the non-regular language S = {w  {a, b} | w has strictly more symbols a than symbols b}.
That is, the computational power of Transformer networks without positional encoding is both rather weak (they do not even contain order-invariant regular languages) and not so easy to capture (as they can express counting properties that go beyond regularity). As we show in the next section, the inclusion of positional encodings radically changes the picture. In fact, with such positional encodings the Tranformer is able to recognize all computable languages.
3.2 POSITIONAL ENCODINGS AND COMPLETENESS OF THE TRANSFORMER
Order-invariance states that Transformer networks lack information about how the input is ordered. Positional encodings come to remedy this issue by providing information about the absolute positions of the symbols in the input. A positional encoding is just a function pos : N  Qd. Function pos combined with an embedding function f :   Qd give rise to a new embedding function fpos :  × N  Qd such that fpos(a, i) = f (a) + pos(i). Thus, given an input string w = a1a2 · · · an  , the result of the embedding function fpos(w) provides a "new" input
fpos(a1, 1), fpos(a2, 2), . . . , fpos(an, n)
to the Transformer encoder. Similarly, the Transformer decoder instead of receiving the sequence Y = (y0, y1, . . . , yt) as input, it receives now the sequence
Y = y0 + pos(1), y1 + pos(2), . . . , yt + pos(t + 1)
As for the case of the embedding functions, we require the positional encoding pos(i) to be computable by a Turing machine working in linear time w.r.t. the size (in bits) of i.
The main result of this section is the completeness of Transformers with positional encodings. Theorem 3.4. The class of Transformer networks with positional encodings is Turing complete.
5

Under review as a conference paper at ICLR 2019
Proof Sketch. For the sake of space we only present a sketch of the proof. The full proof can be found in the appendix. Let A = (, f, N, s, F) be an RNN encoder-decoder language recognizer, such that N is of dimension d and its encoder and decoder are defined by the equations hi = (xiW + hi-1V ) and gt = (gt-1U ), respectively, where g0 = hn and n is the length of the input. We explain how to construct a Transformer network A = (, f , T, s , F ) such that L(A) = L(A ). By Theorem 2.3 this yields Turing-completeness for Transformer networks.
We use a simple positional encoding pos(i) = [0, 0, 0, i]  Q3d+1, where 0 is the vector composed exclusively by 0's. For each a   we define f (a) = [f (a), 0, 0, 0]. Therefore, if for an input w   it is the case that f (w) = (x1, . . . , xn), then fpos(w) = (x1, . . . , xn), where xi = [xi, 0, 0, i]. We construct a Tranformer encoder which consists of a single-layer that takes each input vector xi = [xi, 0, 0, i] and produces ki = [0, 0, 0, i] as keys and vi = [0, xi, 0, 0] as values.
The first n steps of the Transformer decoder emulate the work of the RNN encoder. We maintain the following invariant: for every i  n the i-th output of the decoder is yi = [0, hi, 0, 0]. We prove this by an inductive argument. Let us assume by induction hypothesis that yi = [0, hi, 0, 0], and thus that its positional encoding is yi = [0, hi, 0, i + 1]. We omit the first self attention (Equation (8)) so that pi = yi, and use pi to attend ­ exactly ­ at the (i + 1)-th pair (ki+1 = [0, 0, 0, i + 1], vi+1 = [0, xi+1, 0, 0]) of the Transformer encoder. This can be done with a feed-forward network with three layers plus hard attention. Hence, ai in Equation (9) will be vi+1 + pi = [xi+1, hi, 0, i + 1]. We then define the O(·) function in Equation (10) in such a way that O([xi+1, hi, 0, i + 1]) = [0, 0, (xi+1W + hiV ), 0] = [0, 0, hi+1, 0], and thus zi = O(ai) + ai = [xi+1, hi, hi+1, i + 1]. We produce yi+1 = [0, hi+1, 0, 0] as the output of another decoder layer and use function F (·) in Equation (11) as the identity to produce the final output.
Simulating the decoder of the RNN N , on the other hand, is a bit more cumbersome. We need to extend the previous construction so that the Transformer decoder can stop reading the input and begin implementing the recursion gt = (gt-1U ). For this, we maintain a state value bi in the decoder such that bi = 0 if i = n + 1, and bi = 1 otherwise. We do this by combining an attention over the key-value pairs (ki, vi) generated by the Transformer encoder together with a self-attention in the Transformer decoder as follows. The attention we use over the encoder satisfies that, whenever one tries to attend over an index bigger than n, it yields the same value, say v. On the other hand, the self-attention over the decoder decides if the value obtained from the encoder in this iteration is the same as in the previous one. In this way we can generate a sequence a1, a2, a3, . . . of the form 000 · · · 0111 · · · that changes from 0 to 1 exactly when i = n + 1. With a decoder layer we produce the desired bi simply as 1 - (ai - ai-1). We maintain another vector i and set i := (hi-1 - bi1). Since 0  hi  1 for every i, we have that i = hn exactly when i = n + 1, and i = 0 otherwise. Combining this with the idea developed in the previous paragraph for simulating the encoder of N , we can maintain yet another vector i defined as i := (i-1U ) + i. We obtain then that gi = n+1+i, which implies that we can also simulate the decoder part of N . Putting it all together, we have that the Transformer needs a vector in Q6d+8 to store the vectors that simulate hi and gi, the positional encoding, the values bi, ai, and some additional working space.
3.3 DIFFERENCES WITH VASWANI ET AL. (2017)'S FRAMEWORK
Although the general architecture that we presented closely follows that of Vaswani et al. (2017), some choices for functions and parameters in our positive results are different to the usual choices in practice. For instance, we use additive hard attention which allow us to attend directly to specific positions. In contrast, Vaswani et al. (2017) use dot-product for scores, softmax to attend, plus sin-cos functions as positional encodings. The softmax, sin and cos are not rational functions, and thus, are forbidden in our formalization. An interesting line for future work is to consider arbitrary functions but with additional restrictions, such as finite precision as done by Weiss et al. (2018). Another difference is that for the function O(·) in Equation (10) our proof uses a feed-forward network with various layers, while in Vaswani et al. (2017) only two layers are used. But perhaps the most important difference is the use of function F (·) in Equation (11). Vaswani et al. (2017) uses a function F from Qd to {0, 1}d; actually, a one-hot function produced by a softmax. This one-hot vector is then passed back as input for the next iteration of the decoder. Although the function we use is simpler (we use the identity), it allows us to pass more information to the next decoder step. We leave as an open question whether the Transformer remains Turing complete when a one-hot output function is used instead of the identity function that we use in our proof.
6

Under review as a conference paper at ICLR 2019

4 NEURAL GPUS

The Neural GPU (Kaiser & Sutskever, 2016) is an architecture that mixes convolutions and gated
recurrences over tridimensional tensors. It is parameterized by three functions U (·) (update function), R(·) (reset function), and F (·). Given a tensor S  Qh×w×d and a value r  N, it produces a sequence S1, S2, . . . , Sr given by the following recursive definition (with S0 = S).

Ut = U (St-1),

Rt = R(St-1),

St = Ut St-1 + (1 - U) F (Rt St-1).

where denotes the element-wise product, and 1 is a tensor with only 1's. Neural GPUs force

functions U (·) and R(·) to produce a tensor of the same shape as its input with all values in [0, 1].

Thus, a Neural GPU resembles a gated recurrent unit (Cho et al., 2014), with U working as the

update gate and R as the reset gate. Functions U (·), R(·), and F (·) are defined as a convolution of

its input with a 4-dimensional kernel bank with shape (kH , kW , d, d) plus a bias tensor, followed by

a point-wise transformation

f (K  S + B)

(13)

with different kernels and biases for U (·), R(·), and F (·).

To have an intuition on how the convolution K  S works, it is illustrative to think of S as an (h × w)-grid of (row) vectors and K as a (kH × kW )-grid of (d × d) matrices. More specifically, let sij = Si,j,:, and Kij = Ki,j,:,:, then K  S is a regular two-dimensional convolution in which scalar
multiplication has been replaced by vector-matrix multiplication as in the following expression

(K  S)i,j,: =

si+1(u),j+2(v) Kuv ,

uv

(14)

where 1(u) = u - kH /2 - 1 and 2(v) = v - kW /2 - 1. This intuition makes evident the similarity between Neural GPUs and cellular automata: S is a grid of cells, and in every itera-
tion each cell is updated considering the values of its neighbors according to a fixed rule given by K (Kaiser & Sutskever, 2016). As customary, we assume zero-padding when convolving outside S.

4.1 THE COMPUTATIONAL POWER OF NEURAL GPUS
To study the computational power of Neural GPUs, we cast them as a standard seq-to-seq architecture. Given an input sequence, we put every vector in the first column of the tensor S. We also need to pick a special cell of S as the output cell from which we read the output vector in every iteration. We pick the last cell of the first column of S. Formally, given a sequence X = (x1, . . . , xn) with xi  Qd, and a fixed value w  N, we construct the tensor S  Qn×w×d by leting Si,1,: = xi and Si,j,: = 0 for j > 1. The output of the Neural GPU, denoted by NGPU(X, r), is the sequence of vectors Y = (y1, y2, . . . , yr) such that yt = Snt ,1,:. Given this definition, we can naturally view the Neural GPUs as language recognizers (as formalized them in Section 2).
Since the bias tensor B in Equation (13) is of the same size than S, the number of parameters in a Neural GPU grows with the size of the input. Thus, a Neural GPU cannot be considered as a fixed architecture. To tackle this issue we introduce the notion of uniform Neural GPU, as one in which for every bias B there exists a matrix B  Qw×d such that Bi,:,: = B for each i. Thus, uniform Neural GPUs can be finitely specified (as they have a constant number of parameters, not depending on the length of the input). We now establish the completeness of this model.
Theorem 4.1. The class of uniform Neural GPUs is Turing complete.

Proof sketch. The proof is based on simulating a seq-to-seq RNN; thus, completeness follows from
Theorem 2.3. Consider an RNN encoder-decoder language recognizer, such that N is of dimension
d and its encoder and decoder are defined by the equations hi = (xiW + hi-1V ) and gt = (gt-1U ), respectively, where g0 = hn and n is the length of the input. We use a Neural GPU with input tensor S  Qn×1×3d+3. Let Ei = Si,1,1:d and Di = Si,1,d+1:2d. The idea is to use E for the encoder and D for the decoder. We use kernel banks of shape (2, 1, 3d + 3, 3d + 3) with uniform
bias tensors to simulate the following computation. In every step t, we first compute the value of (EtW + Et-1V ) and store it in Et, and then reset Et-1 to zero. Similarly, in step t we update the vector in position Dt-1 storing in it the value (Dt-1U + Et-1U ) (for the value of Et-1 before the reset). We use the gating mechanism to ensure a sequential update of the cells such that at time t

7

Under review as a conference paper at ICLR 2019
we update only positions Ei and Dj for i  t and j  t - 1. Thus the updates on the D are always one iteration behind the update of E. Since the vectors in D are never reset to zero, they keep being updated which allows us to simulate an arbitrary long computation. In particular we prove that at iteration t it holds that Et = ht, and at iteration n + t it holds that Dn = gt. We require 3d + 3 components, as we need to implement several gadgets for properly using the update and reset gates. In particular, we need to store the value of Et-1 before we reset it. The detailed construction and the correctness proof can be found in the appendix.
The proof above makes use of kernels of shape (2, 1, d, d) to obtain Turing completeness. This is, in a sense, optimal, as one can easily prove that Neural GPUs with kernels of shape (1, 1, d, d) are not Turing complete, regardless of the size of d. In fact, for kernels of this shape the value of a cell of S at time t depends only on the value of the same cell in time t - 1.
Zero padding vs circular convolution The proof of Theorem 4.1 requires the application of zero padding in convolution. This allows us to clearly differentiate internal cells from cells corresponding to the endpoints of the input sequence. Interestingly, Turing-completeness is lost if we replace zero padding with circular convolution. Formally, given S  Qh×w×d, a circular convolution is obtained by defining Sh+1,:,: = S1,:,: and S0,:,: = Sh,:,: One can prove that uniform Neural GPUs with circular convolutions cannot differentiate among periodic sequences of different length; in particular, they cannot check if a periodic input sequence is of even or odd length. This yields the following: Proposition 4.2. Uniform Neural GPUs with circular convolutions are not Turing complete.
Related to this last result is the empirical observation by Price et al. (2016) that Neural GPUs that learn to solve hard problems, e.g., binary multiplication, and which generalizes to most of the inputs, struggle with highly symmetric (and nearly periodic) inputs. Actually, Price et al. (2016) exhibit examples of the form 11111111 × 11111111 failing for all inputs with eight or more 1s. We leave as future work to explore the implications of our theoretical results on this practical observation.
Bidimensional tensors and piecewise linear activations Freivalds & Liepins (2018) simplified Neural GPUs and proved that, by considering piecewise linear activations and bidimensional input tensors instead of the original smooth activations and tridimensional tensors used by Kaiser & Sutskever (2016), it is possible to achieve substantially better results in terms of training time and generalization. Our Turing completeness proof also relies on a bidimensional tensor and uses piecewise linear activations, thus providing theoretical evidence that these simplifications actually retain the full expressiveness of Neural GPUs while simplifying its practical applicability.
5 FINAL REMARKS AND FUTURE WORK
We have presented an analysis of the Turing completeness of two popular neural architectures for sequence-processing tasks; namely, the Transformer, based on attention, and the Neural GPU, based on recurrent convolutions. We plan to further refine this analysis in the future. For example, our proof of Turing completeness for the Transformer requires the presence of residual connections, i.e., the +xi, +ai, +yi, and +pi summands in Equations (5-10), while our proof for Neural GPUs heavily relies on the gating mechanism. We will study whether these features are actually essential to obtain completeness.
Although our results are mostly of theoretical interest, they might lead to observations of practical interest. For example, Chen et al. (2018) have established the undecidability of several practical problems related to probabilistic language modeling with RNNs. This means that such problems can only be approached in practice via heuristics solutions. Many of the results in Chen et al. (2018) are, in fact, a consequence of the Turing completeness of RNNs as established by Siegelmann & Sontag (1995). We plan to study to what extent our analogous undecidability results for Transformers and Neural GPUs imply undecidability for language modeling problems based on these architectures.
Finally, our results rely on being able to compute internal representations of arbitrary precision. It would be interesting to perform a theoretical study of the main properties of both architectures in a setting in which only finite precision is allowed, as have been recently carried out for RNNs (Weiss et al., 2018). We also plan to tackle this problem in our future work.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/ abs/1409.0473.
Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks as weighted language recognizers. In NAACL-HLT 2018, pp. 2261­2271, 2018.
Kyunghyun Cho, Bart van Merrienboer, C¸ aglar Gu¨lc¸ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP, pp. 1724­1734, 2014.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. CoRR, abs/1807.03819, 2018. URL https://arxiv.org/abs/1807. 03819.
Karlis Freivalds and Renars Liepins. Improving the neural GPU architecture for algorithm learning. In Neural Abstract Machines & Program Induction (NAMPI), 2018.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv preprint arXiv:1410.5401, 2014.
Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded memory. In Advances in Neural Information Processing Systems, pp. 1828­1836, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770­778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, pp. 630­645, 2016b.
Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in neural information processing systems, pp. 190­198, 2015.
Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In ICLR, 2016.
S. C. Kleene. Representation of events in nerve nets and finite automata. In Claude Shannon and John McCarthy (eds.), Automata Studies, pp. 3­41. Princeton University Press, 1956.
Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. In ICLR, 2016.
Warren McCulloch and Walter Pitts. A logical calculus of ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5:127­147, 1943.
Nicolas Ollinger. Universalities in cellular automata. In Handbook of Natural Computing, pp. 189­ 229. 2012.
Eric Price, Wojciech Zaremba, and Ilya Sutskever. Extensions and limitations of the neural GPU. CoRR, abs/1611.00736, 2016. URL http://arxiv.org/abs/1611.00736.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In NAACL-HLT, pp. 464­468, 2018.
Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. In Proceedings of the Fifth Annual ACM Conference on Computational Learning Theory, COLT, pp. 440­449, 1992.
Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. J. Comput. Syst. Sci., 50(1):132­150, 1995.
9

Under review as a conference paper at ICLR 2019 Alvy Ray Smith III. Simple computation-universal cellular spaces. Journal of the ACM (JACM), 18
(3):339­353, 1971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998­6008, 2017. Gail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of finite preci-
sion RNNs for language recognition. In ACL 2018, pp. 740­745, 2018.
10

Under review as a conference paper at ICLR 2019
A PROOFS FOR SECTION 2
A.1 PROOF OF THEOREM 2.3
We first sketch the main idea of Siegelmann & Sontag's proof. We refer the reader to the original paper for details. Siegelmann & Sontag show how to simulate a two-stack machine M (and subsequently, a Turing machine) with a single RNN N with  as activation. They first construct a network N1 that, with 0 as initial state (hN0 1 = 0) and with a binary string w  {0, 1} as input sequence, produces a representation of w as a rational number and stores it as one of its internal values. Their internal representation of strings encodes every w as a rational number between 0 and 1. In particular, they use base 4 such that, for example, a string w = 100110 is encoded as (0.311331)4 that is, its encoding is
3 × 4-1 + 1 × 4-2 + 1 × 4-3 + 3 × 4-4 + 3 × 4-5 + 1 × 4-6.
This representation allows one to easily simulate stack operations as affine transformations plus  activations. For instance, if xw is the value representing string w = b1b2 · · · bn seen as a stack, then the top(w) operation can be defined as simply y = (4xw - 2), since y = 1 if and only if b1 = 1, and y = 0 if and only if b1 = 0. Other stack operations can de similarly simulated. Using this representation, they construct a second network N2 that simulates the two-stacks machine by using one neuron value to simulate each stack. The input w for the simulated machine M is assumed to be at an internal value given to N2 as an initial state (hN0 2 ). Thus, N2 expects only zeros as input. Actually, to make N2 work for r steps, an input of the form 0r should be provided. Finally, they combine N1 and N2 to construct a network N which expects an input of the following form: (b1, 1, 0)(b2, 1, 0) · · · (bn, 1, 0)(0, 0, 1)(0, 0, 0)(0, 0, 0) · · · (0, 0, 0). The idea is that the first component contains the input string w = b1b2 · · · bn, the second component states when the input is active, and the third component is 1 only when the input is inactive for the first time. Before the input vector (0, 0, 1) the network N1 is working. The input (0, 0, 1) is used to simulate a change from N1 to N2, and the rest of input vectors (0, 0, 0) are provided to continue with N2 for as many steps as needed. The number neurons that this construction needs to simulate a machine M with m states, is 10m + 30. 1
It is clear that Siegelmann & Sontag's proof resembles a modern encoder-decoder RNN architecture, where N1 is the encoder and N2 is the decoder, thus it is straightforward to use the same construction to provide an RNN encoder-decoder N and a language recognizer A that uses N and simulates the two-stacks machine M . There are some details that is important to notice. Assume that N is given by the formulas in Equations (1) and (2). First, since N2 in the above construction expects no input, we can safely assume that R in Equation (2) is the null matrix. Moreover, since A defines its own embedding function, we can ensure that every vector that we provide for the encoder part of N has a 1 in a fixed component, and thus we do not need the bias b1 in Equation (1) since it can be simulated with one row of matrix V . We can do a similar construction for the bias b2 (Equation (2)). Finally, Siegelmann & Sontag show that its construction can be modified such that a particular neuron of N2, say n , is always 0 except for the first time an accepting state of M is reached, in which case n = 1. Thus, one can consider O(·) (Equation (2)) as the identity function and add to A the stopping criterion that just checks if n is 1. This completes the proof sketch of Theorem 2.3.
1The idea presented above allows one to linearly simulate M , that is, each step of M is simulated with a constant number of steps of the corresponding RNN. Siegelmann & Sontag show that, with a refinement of the above encoding one can simulate M in real-time, that is, a single step of M is simulated with a single step of the recurrent network. The 10m + 30 is the bound given by a simulation with slow-down of two. See the original paper for details (Siegelmann & Sontag, 1995).
11

Under review as a conference paper at ICLR 2019

B PROOFS FOR SECTION 3

B.1 PROOF OF PROPOSITION 3.1

Proof. We extend the definition of the function PropInv to sequences of vectors. Given a sequence X = (x1, . . . , xn) we use vals(X) to denote the set of all vectors occurring in X. Similarly as for strings, we use prop(v, X) as the number of times that v occurs in X divided by the length of X. Now we are ready to extend PropInv with the following definition:

PropInv(X) = {X | vals(X ) = vals(X) and prop(v, X) = prop(v, X ) for all v  vals(X)}

Notice that for every embedding function f :   Qd and string w  , we have that if u  PropInv(w) then f (u)  PropInv(f (w)). Thus in order to prove that Trans(f (w), s, r) = Trans(f (u), s, r) for every u  PropInv(w), it is enough to prove that

Trans(X, s, r) = Trans(X , s, r) for every X  PropInv(X)

(15)

To further simplify the exposition of the proof we introduce another notation. We denote by pXv as the number of times that vector v occurs in X. Thus we have that X  PropInv(X) if and only if, there exists a value   Q+ such that for every v  vals(X) it holds that pXv = pvX .
We now have all the necessary to proceed with the proof of Proposition 3.1. We will prove it by proving the property in (15). Let X = (x1, . . . , xn) be an arbitrary sequence of vectors, and let X = (x1, . . . , xm)  PropInv(X). Moreover, let Z = (z1, . . . , zn) = Enc(X; ) and Z = (z1, . . . , zm) = Enc(X ; ). We first prove the following property:
For every pair of indices (i, j)  {1, . . . , n} × {1, . . . , m}, if xi = xj then zi = zj. (16)

Lets (i, j) be a pair of indices such that xi = xj. From Equations (5-6) we have that zi = O(ai)+ai
where ai = Att(Q(xi), K(X), V (X)) + xi. Thus, since xi = xj, in order to prove zi = zj it is
enough to prove that Att(Q(xi), K(X), V (X)) = Att(Q(xj), K(X ), V (X )). By equations (34) and the restriction over the form of normalization functions we have that

where  =

1n Att(Q(xi), K(X), V (X)) =  f(score(Q(xi), K(x )))V (x )
=1

n =1

f(score(Q(x

), K(x

))).

The

above

equation

can

be

rewritten

as

1 Att(Q(xi), K(X), V (X)) = 

pvX f(score(Q(xi), K(v)))V (v)

vvals(X )

with  = vvals(X) pXv f(score(Q(v), K(v))). By a similar reasoning we can write

1 Att(Q(xj), K(X ), V (X )) = 

pvX f(score(Q(xj), K(v)))V (v)

vvals(X )

with  = vvals(X ) pvX f(score(Q(v), K(v))). Now, since X  PropInv(X) we know that
vals(X) = vals(X ) and there exists a   Q+ such that pvX = pXv for every v  vals(X). Finally, from this last property, plus the fact that xi = xj we have

1 Att(Q(xj), K(X ), V (X )) = 

pXv f(score(Q(xj), K(v)))V (v)

vvals(X )

1 =


pXv f(score(Q(xi), K(v)))V (v)

vvals(X )

= Att(Q(xi), K(X), V (X))

Which completes the proof of Property (16) above.

Consider now the complete encoder TEnc. Let (K, V ) = TEnc(X) and (K , V ) = TEnc(X ), and let q be an arbitrary vector. We will prove now that Att(q, K, V ) = Att(q, K , V ). By

12

Under review as a conference paper at ICLR 2019

following a similar reasoning as for proving Property (16) (plus induction on the layers of TEnc)
we obtain that if xi = xj then ki = kj and vi = vj, for every i  {1, . . . , n} and j  {1, . . . , m}. Thus, there exists a mapping MK : vals(X)  vals(K) such that MK (xi) = ki and MK (xj) = kj and similarly a mapping MV : vals(X)  vals(V ) such that MV (xi) = vi and MV (xj) = vj, for every i  {1, . . . , n} and j  {1, . . . , m}. Lets focus now on Att(q, K, V ). We have:

with  =

1n

Att(q, K, V ) = 

f(score(q, ki))vi

i=1

n i=1

f(score(q,

ki

)).

Similarly

as

before,

we

can

rewrite

this

as

1n

Att(q, K, V ) = 

f(score(q, MK (xi)))MV (xi)

i=1

1 =


pvX f(score(q, MK (v)))MV (v)

vvals(X )

with  =

vvals(X) pXv f(score(q, MK (v))). Similarly for Att(q, K , V ) we have

1m

Att(q, K , V ) = 

f(score(q, MK (xj)))MV (xj)

j=1

1 =


pXv f(score(q, MK (v)))MV (v)

vvals(X )

And finally using that X  PropInv(X) we obtain

1 Att(q, K , V ) =


pvX f(score(q, MK (v)))MV (v)

vvals(X )

1 =


pvX f(score(q, MK (v)))MV (v)

vvals(X )

1 =


pvX f(score(q, K(v))V (v)

vvals(X )

= Att(q, K, V )

which is what we wanted.

To complete the rest proof, consider Trans(X, y0, r) which is defined by the recursion yk+1 = TDec(TEnc(X), (y0, y1, . . . , yk))

To prove that Trans(X, y0, r) = Trans(X , y0, r) we use an inductive argument. We know that y1 = TDec(TEnc(X), (y0)) = TDec((K, V ), (y0)).
Now TDec only access (K, V ) via attentions of the form Att(q, K, V ) and for the case of y1 the vector q can only depend on y0, thus, from Att(q, K, V ) = Att(q, K , V ) we have that
y1 = TDec((K, V ), (y0)) = TDec((K , V ), (y0)) = TDec(TEnc(X ), (y0)).
The rest of the steps follow by a simple induction on k.
B.2 PROOF OF COROLLARY 3.2
To obtain a contradiction, assume that there is a language recognizer A that uses a Transformer network and such that L = L(A). Now consider the strings w1 = aabb and w2 = aaabbb. Since w1  PropInv(w2) by Proposition 3.1 we have that w1  L(A) if and only if w2  L(A) which is a contradiction since w1  L but w2 / L. This completes the proof of the corollary.

13

Under review as a conference paper at ICLR 2019

B.3 PROOF OF PROPOSITION 3.3

We construct a language recognizer A = (, f, Trans, s, F) with Trans a very simple Transformer network with dimension d = 2 and using just one layer of encoder and one layer of decoder, such that L(A) = {w  {a, b} | w has strictly more symbols a than symbols b}. As embedding function, we use f (a) = [0, 1] and f (b) = [0, -1].

Assume that the output for the encoder part of the transformer is X = (x1, . . . , xn). First we use an encoder layer that implements the identity function. This can be trivially done using null
functions for the self attention and through the residual connections this encoder layer shall preserve the original xi values. For the final V (·) and K(·) functions of the Transformer encoder (Equation (7)), we use V (x) = x the identity function and K(x) = [0, 0], giving V e = X and Ke = ([0, 0], [0, 0], . . . , [0, 0]).

For the decoder we use a similar approach. We consider the identity in the self attention plus the
residual (which can be done by just using the null functions for the self attention). Considering the external attention, that is the attention over (Ke, V e), we let score and  be arbitrary scoring and normalization functions. And finally for the function O(·) (Equation (10)) we use a single layer neural network implementing the affine transformation O([x, y]) = [y - x, -y] such that O([x, y]) + [x, y] = [y, 0]. The final function F (·) is just the identity function.

In order to complete the proof we introduce some notation. Lets denote by #a(w) as the number of

a's We

in w, and similarly #b(w) for now prove that, for any string

the number of b's in w. Lets call cw w  {a, b} if we consider f (w) =

as the X=

value (x1, .

#a

(w)-#b n

(w)

.

. . , xn) as the

input sequence for Trans and we use initial value s = [0, 0] for the decoder, the complete network

shall compute a sequence y1, y2, . . . , yr such that:

[0, 0] i = 0 yi = [cw, 0] i > 0

We proceed by induction. The base case trivially holds since y0 = s = [0, 0]. Assume now that we are at step r and the input for the decoder is (y0, y1, . . . , yr). We will show that yr+1 = [cw, 0]. Since we consider the identity in the self attention (Equation (8)), we have that pi = yi for every i in {0, . . . , i}. Now considering the external attention, that is the attention over (Ke, V e), Since all key vectors in Ke are [0, 0], the external attention will produce the same score value for all
positions. That is, score(pi, kj1 ) = score(pi, kj2 ) for every j1, j2. Lets call this value s . Thus we have that

(score(pi, k1), . . . , score(pi, kn)) = (s , s , . . . , s )

11 1 = , ,..., .
nn n

Then, since V e = X we have that

Att(pi, Ke, V e)

=

1n x
n

=1

=

1 n

[0,

#a(w)

-

#b(w)]

for every i  {0, . . . , r}. The last equality holds since our embedding are f (a) = [0, 1] and f (b) = [0, -1], and so every a in w sums one and every b subtracts one. Thus, we have that

Att(pi, Ke, V e) = [0, cw].

for every i  {0, . . . , r}. In the next step, after the external attention plus the residual connection (Equation (9)) we have

ai = Att(pi, Ke, V e) + pi = Att(pi, Ke, V e) + yi = [0, cw] + [cw, 0] = [cw, cw]

14

Under review as a conference paper at ICLR 2019
Applying function O(·) plus the residual connection (Equation (10)) we have
zi = O(ai) + ai = O([cw, cw]) + [cw, cw] = [cw - cw, -cw] + [cw, cw] = [cw, 0]
Finally, yr+1 = F (zr) = zr = [cw, 0] which is exactly what we wanted to prove. To complete the proof, notice that #a(w) > #b(w) if and only if cw > 0. If we define F as Q+ × Q, the recognizer A = (, f, Trans, s, F) will accept the string w exactly when cw > 0, that is, w  L(A) if and only if #a(w) > #b(w). That is exactly the language S, and so the proof is complete.
B.4 PROOF OF THEOREM 3.4
The main idea is to simulate an RNN encoder-decoder architecture using a Transformer network. Let A = (, f, N, s, F) be a recognizer, with N an RNN encoder-decoder of dimension d and composed of the equations
hi = (xiV + hi-1V ) gt = (gt-1U ) with h0 = 0 (the vector in Qd with only zeros), and g0 = hn where n is the length of the input. We shall construct a recognizer A = (, f , Trans, s , F ) with Trans a Transformer network such that L(A) = L(A ). In our proof we will use vectors in Q6d+8. The idea of every component of this vector will be clear when we describe the details of the construction.
PRELIMINARIES
We first introduce the embedding and positional encoding functions. We use an embedding function f :   Q6d+8 such that
f (a) = [f (a), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] Our construction uses a very simple positional encoding pos : N  Q6d+8 such that pos(i) is a vector that only has the value i in its last position and 0 everywhere else. Thus if f (ai) = xi we have that
fpos(ai) = [xi, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, i] We denote this last vector by xi. That is, the input for Trans is the sequence (x1, x2, . . . , xn). We need a final notation to begin with the proof. We will usually have the last component of all our vectors with the positional encoding. Thus, given an arbitrary vector v  Q6d+8 we denote by e(v) the value of its last component. That is, for the vector xi above we have e(xi) = i.
THE ATTENTION MECHANISM USED
In what follows we will use a scoring function score : Q6d+8×Q6d+8  Q that essentially performs the following computation for a pair of vectors (q, k):
score(q, k) = -|e(q) - e(k)|
This function can be implemented with an additive attention as follows. We use a feed-forward network of three layers that receives as input a vector of the form [q, k]  Q2(6d+8). The first layer will be a function f1 : Q2(6d+8)  Q2 that just picks the values at positions 6d + 8 and at 2(6d + 8), thus, with input [q, k] it produces f1([q, k]) = [e(q), e(k)]. It is clear that this transformation can be performed with a single-layer neural network (with an identity activation) since this is just a linear transformation. The next two layers are defined by functions f2 : Q2  Q2, and f3 : Q2  Q defined as follows:
f2([x, y]) = relu([x - y, y - x]) f3([x, y)] = -x - y
15

Under review as a conference paper at ICLR 2019

Thus, score(q, k) will be score(q, k) = f3(f2(f1([q, k])))
It is not difficult to see that the result will be

score(q, k) = - relu(e(q) - e(k)) - relu(e(k) - e(q)) = -|e(q) - e(k)|

which is exactly what we wanted to obtain.

The most important property of this score function is that, when combined with hard attention, it
has the effect of attending to a specific position of the attended sequence of vectors. More formally, assume that q is a vector such that 1  e(q)  n, and k1, k2, . . . , kn are all such that e(ki) = i. Then we have that the following holds:

(score(q, k1), score(q, k2), . . . , score(q, kn)) = (-k, -k + 1, . . . , -1, 0, -1, . . . , -(n - k)) where the 0 appears exactly in the k-th position of the above vector. Thus we have that

hardmax(score(q, k1), score(q, k2), . . . , score(q, kn)) = (0, 0, . . . , 0, 1, 0, . . . , 0)
Where the 1 appears exactly in the k-th position. Moreover, if we assume that e(q) = k such that k > n then

(score(q, k1), score(q, k2), . . . , score(q, kn)) = (-k, -k + 1, . . . , -k + n) and thus the maximum value occurs in the last position which implies that

hardmax(score(q, k1), score(q, k2), . . . , score(q, kn)) = (0, 0, . . . , 0, 1) That is, it is the sequence with a 1 in its last position.

Putting all this in the Att function, we obtain the following. Assume that we have a sequence K = (k1, . . . , kn) of key vectors such that e(ki) = i for every i. Let V = (v1, . . . , vn) be a sequence of arbitrary value vectors, and assume that we have a query q such that 1  e(q)  n. Then we have
Att(q, K, V ) = ve(q).
Moreover, whenever we have a query vector q such that e(q) > n then

Att(q, K, V ) = vn.
Given this property and also given that all vectors that we use will have a positional encoding in its last component, to simplify the notation we can use the function Att(i, K, V ) = vi, and thus
Att(q, K, V ) = Att(e(q), K, V )

Notice that

Att(i, K, V ) = vi i  n vn i > n

We extensively use these properties of the attention function in our construction. We now have all the necessary to begin with the details of our proof.

CONSTRUCTION OF TEnc
The encoder part of Trans is very simple. For TEnc we use a single-layer encoder, such that TEnc(x1, . . . , xn) = (Ke, V e) where
Ke = (x1, . . . , xn)
that is, it implements the identity function for keys, and
V e = (v1, . . . , vn)
where vi = [xi, 0, 0, 0, 0, 0, i, 0, 0, 0, 0, 0, 0, 0]
That is, vi is just xi where the last component has been moved to the center component. It is straightforward that this can be done with a single layer encoder by just implementing the identity (that can be done by taking advantage of the residual connection in Equations (5) and (6)), and then using affine transformations for V (·) and K(·) in Equation (7). The most important part of the construction is in the decoder that we next describe.

16

Under review as a conference paper at ICLR 2019

OVERVIEW OF THE CONTRUCTION OF TDec

We base our proof in the construction the following sequences of vectors and values:

i =

xi 1  i  n xn i > n

i =

hi 0  i  n (W i-1 + V i) i > n

0  i = n

in i=n+1

(U i-1) i > n + 1

0 0in ai = 1 i > n

0 i=n+1 bi = 1 i = n + 1

i 0in ci = n i > n

It is easy to see that since 0 = h0 and i = xi for 1  i  n, then  follows the recurrence i = (i-1W + xiV ) with the same starting value and equations as the hi's , and so we have that i = hi for i  n. Then, since n+1 = n = hn = g0, and after that i = (U i-1), we conclude that n+1+i = gi. We will show that these sequences can be computed in a transformer network, and so we can simulate the RNN N .

In what follows we show an overview on how to compute all these sequences with the decoder part of Trans. In the decoder, we show how to produce the sequence of outputs (y1, y2, . . . , yr) where yi is given by:
yi = [0, i, i, ai, bi, ci, 0, 0, 0, 0, 0, 0, 0, 0]
We consider as the starting vector for the decoder the vector

y0 = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0].

Thus, what we are actually stating is that 0 = 0 = 0, a0 = c0 = 0 and b0 = 1, which coincide with the values for the sequences described above. Recall that we are using positional encodings, thus, when producing the output, the actual input for the first layer of the decoder will be the sequence
y0 + pos(1), y1 + pos(2), . . . , yr + pos(i + 1)
We denote by yi the vector yi plus its positional encoding (which is always at its last component).
We now roughly describe a two-layer decoder network that, with input Ke, V e and sequence (y0, . . . , yr) will produce yr+1 as output (the details are in the following section). Our construction will resemble a proof by induction; we show how to construct yr+1 from the previous vector yr.
The overview of the construction is very simple. With input Ke, V e and sequence (y0, . . . , yr) the first decoder layer produces a sequence of vectors z01, z11, . . . , zr1 such that

zi1 = [i+1, i, i, ai, bi, ci, ci+1, i+1, i+1, 0, ai+1, bi+1, ci+1, i + 1]

(17)

Notice that the first decoder layer will do almost all the heavy work. We actually use a feed-forward
network with 3 layers to obtain this output. After that, the second decoder layer, just do some
cleaning work by moving some parts of the vector and deleting others to produce the sequence of output vectors z02, z12, . . . , zr2 where

zi2 = [0, i+1, i+1, ai+1, bi+1, ci+1, 0, 0, 0, 0, 0, 0, 0, 0].

(18)

This is exactly what we want as output, and thus, we just consider the final output transformation F (·) as the identity to produce yr+1 = F (zr2) (Equation (11)) which completes our construction.

17

Under review as a conference paper at ICLR 2019

DETAILED CONSTRUCTION OF TDec

First, we have the following useful identities for i, i and i that can be obtained by simply inspecting the definitions above and the attention function that we defined:

i = Att(i, Ke, V e)1:d

i =

0 i=0 (i-1W + iV ) i > 0

i =

0 i=0 (i-1U ) + (1 - bi)i-1 i > 0

We also have some identities for ai, bi and ci:

ci =

0 i=0 Att(i, Ke, V e)3d+4 i > 0

ai = bi =

0 i=0 1 - (ci - ci-1) i > 0
1 i=0 1 - (ai - ai-1) i > 0

We also note that bi can be rewritten as:

bi =

1 i=0 ci - ci-1 + ai-1 i > 0

The first layer of the decoder, that we call Dec1, is defined by the functions Q1(·), V1(·), K1(·), and O1(·) that we now describe. We use the function V1 : Q6d+8  Q6d+8 such that V1(v) is the vector with only zeroes, thus, we just nullify the self-attention in the first decoder layer. This plus the residual connection in Equation (8) has the net effect of producing the identity and thus we obtain pi1 = yi for every i  {0, . . . , r}. Notice that e(p1i ) = e(yi) = i + 1, and thus, if i + 1  n we have that
Att(pi1, Ke, V e) = Att(i + 1, Ke, V e) = vi+1 = [xi+1, 0, 0, 0, 0, 0, i + 1, 0, 0, 0, 0, 0, 0, 0]
and, otherwise, if i + 1 > n we have
Att(p1i , Ke, V e) = Att(i + 1, Ke, V e) = vn = [xn, 0, 0, 0, 0, 0, n, 0, 0, 0, 0, 0, 0, 0].
Thus, combining both cases we actually have that
Att(pi1, Ke, V e) = [i+1, 0, 0, 0, 0, 0, ci+1, 0, 0, 0, 0, 0, 0, 0].
This last result is summed with pi1 = yi (the residual connection in Equation (9)) to obtain ai1 which is given by:
ai1 = Att(pi1, Ke, V e) + pi1 = [i+1, i, i, ai, bi, ci, ci+1, 0, 0, 0, 0, 0, 0, i + 1]
We now describe the function O1(·) which is going to be defined by a feed-forward neural network that we next describe. The first layer of the network will be given by a function f1 : Q6d+8  Q6d+8 such that with input a1i it will produce
f1(a1i ) = [i+1, i, i, 0, 0, 0, 0, 0, 0, 0, ai+1, bi+1, 0, 0].
It is clear that f1 can be implemented with a single neural-network layer as it only has to apply the affine transformation given by the equations ai+1 = 1 - (ci+1 - ci) and bi+1 = ci+1 - ci + ai, and making some positions equal to 0.
After f1 we apply another feed-forward layer f2 : Q6d+8  Q6d+8 given by
f2(f1(ai1)) = ([0, i, i, 0, 0, 0, 0, iW + i+1V , iU , i - bi+11, ai+1, bi+1, 0, 0])

18

Under review as a conference paper at ICLR 2019
where 1 is a vector in Qd with only ones. It is clear that the internal vector can be computed with an affine transformation. Since ai+1, bi+1 are all rationals between 0 and 1, and all the values in i and i are also between 0 and 1, when we apply the function  we obtain f2(f1(ai1)) = [0, i, i, 0, 0, 0, 0, (iW + i+1V ), (iU ), (i - bi+11), ai+1, bi+1, 0, 0]).
We note that (iW + i+1V ) is actually i+1. Moreover, as we next describe, it holds that (i - bi+11) = (1 - bi+1)i. To see this, notice that i has all its values between 0 and 1. Then, if bi+1 = 1 we have that i - bi+11 is a vector with all its components less than or equal to 0 which implies that (i - bi+11) = 0. On the other hand, if bi+1 = 0 then (i - bi+11) = (i) = i. Thus we can write the output of this layer as:
f2(f1(a1i )) = [0, i, i, 0, 0, 0, 0, i+1, (iU ), (1 - bi+1)i, ai+1, bi+1, 0, 0]). We need a final linear layer given by a function f3 : Q6d+8  Q6d+8 that does the following computation
f3(f2(f1(ai1))) = [0, 0, 0, 0, 0, 0, 0, i+1, (iU ) + (1 - bi+1)i, 0, ai+1, bi+1, 0, 0]). Notice that (iU ) + (1 - bi+1)i is exactly i+1 thus the output of f3 can be written as
f3(f2(f1(ai1))) = [0, 0, 0, 0, 0, 0, 0, i+1, i+1, 0, ai+1, bi+1, 0, 0]). This is the output of function O1(·). Thus. we have that
O1(a1i ) = f3(f2(f1(ai1))) = [0, 0, 0, 0, 0, 0, 0, i+1, i+1, 0, ai+1, bi+1, 0, 0]).
Now the output of this decoder layer is zi1 = O1(a1i ) + ai1 (Equation (10)) and thus we have zi1 = O1(a1i ) + a1i = [i+1, i, i, ai, bi, ci, ci+1, i+1, i+1, 0, ai+1, bi+1, 0, i + 1]).
For the second decoder layer Dec2, it is straightforward to construct a layer that just do some cleaning and thus from z01, z11, . . . , zr1 produces the sequence z02, z12, . . . , zr2 such that
zi2 = [0, i+1, i+1, ai+1, bi+1, ci+1, 0, 0, 0, 0, 0, 0, 0, 0]). We finally use F (·) as the identity function to produce our desired output from zr2, that is
yr+1 = F (zr2) = [0, r+1, r+1, ar+1, br+1, cr+1, 0, 0, 0, 0, 0, 0, 0, 0]). This completes the construction and the correctness of our construction.
FINAL STEP We now can use our Trans network to construct the recognizer A = (, f , Trans, s , F ). We use s as simple y0 in our above construction. Finally, notice that in Trans the vector i is actually the output of the RNN encoder-decoder that we are simulating but only for i  n+1. We take advantage of the value ai which is 1 if and only if i  n + 1. Thus we can define F = Q2d × F × {1} × Q3d+7. This completes the proof of the Theorem.
19

Under review as a conference paper at ICLR 2019

C PROOFS FOR SECTION 4

C.1 PROOF OF THEOREM 4.1

The formulas of the Neural GPU in detail are as follows (with S0 the initial input tensor):

Ut = U (St-1) Rt = R(St-1) St = Ut St-1 + (1 - U)

F (Rt

St-1)

With U (·), R(·), and F (·) defined as

U (X) = fU (KU  X + BU ) R(X) = fR(KR  X + BR) F (X) = fF (KF  X + BF )

Consider now an RNN encoder-decoder N of dimension d and composed of the equations
hi = (xiW + hi-1V ) gt = (gt-1U )
with h0 = 0 and g0 = hn where n is the length of the input.

CONSTRUCTING THE NEURAL GPU TO SIMULATE N

We construct a Neural GPU network NGPU that simulates N as follows. Assume that the input of N is X = (x1, . . . , xn). Then we first construct the sequence X = (x1, . . . , xn) such that xi = [xi, 0, 0, 1, 1, 0] with 0  Qd the vector with all values as 0. Notice that xi  Q3d+3, moreover it is straightforward that if xi was constructed from an embedding function f :   Qd applied to a symbol a  , then xi can also be constructed with an embedding function f :   Q3d+3 such that f (a) = [f (a), 0, 0, 1, 1, 0].
We consider an input tensor S  Qn×1×3d+3 such that for every i  {1, . . . , n} it holds that Si,1,: = xi = [xi, 0, 0, 1, 1, 0]. Notice that since we picked w = 1, our tensor S is actually a 2D grid. Our proof shows that a bi-dimensional tensor is enough for simulating an RNN.
We now describe how to construct the kernel banks KU , KR and KF of shape (2, 1, 3d + 3, 3d + 3). Notice that for each kernel KX we essentially have to define two matrices K1X,1,:,: and K2X,1,:,: each one of dimension (3d + 3) × (3d + 3). We begin by defining every matrix in KF as block matrices. When defining the matrices, all blank spaces are considered to be 0.



KF1,1,:,:

= 

V

V


  F1

W W



K2F,1,:,:

=

 

U 
U

F2

where F1 and F2 are 3 × 3 matrices defined by

100
F1 = 0 0 0 000

010
F2 = 0 0 0 000

20

Under review as a conference paper at ICLR 2019

Tensors KU and KR are considerable simpler. For the case of KU we have



K1U,1,:,:

=

 

AA

 
U1



KU2,1,:,:

=

 

where U1 and U2 are 3 × 3 matrices defined by
110 U1 = 0 0 0
000

  A U2
001 U2 = 0 0 0
000

and where A is the 3 × d matrix defined by
1 1 ··· 1 A = 0 0 ··· 0
0 0 ··· 0

Finally, we define KR as 

KR1,1,:,:

=

 

 



K2R,1,:,:

=

 

A

where R2 is the 3 × 3 matrix defined by

B

 
R2

100
R2 = 0 1 0 000

and where B is the 3 × d matrix defined by

0 0 ··· 0 B = 1 1 ··· 1
0 0 ··· 0

The bias tensors BU and BF are 0 (the tensor with all values 0). Finally, to construct tensor BR we consider the matrix D of dimension 1 × (3d + 3) such that
D=[ 0 0 1 0 0 1 ]
Then we let BiR,:,: = D for all i. Finally, we consider fU = fR = fF = . The constructed Neural GPU is a uniform Neural GPU.
Before continuing with the proof we note that for every kernel KX and tensor S we have that
(KX  S)i,1,: = Si-1,1,:KX1,1,:,: + Si,1,:KX2,1,:,:

CORRECTNESS OF THE CONSTRUCTION

We now prove that the following properties hold for every t  0:

Sti,1,:

=

 

[0, 0, it-i, 0, 0, 0]

[hi, hi, 0, 0, 1, 0]

 [xi, 0, 0, 1, 1, 0]

for i < t for i = t for i > t

(19)

21

Under review as a conference paper at ICLR 2019

where jk is given by the recurrence k0 = hk and jk = (kj-1U ). Notice that gj = nj . That is, we are going to prove that our construction actually simulates N . By (19) one can see that the intuition in our construction is to use the first d components to simulate the encoder part, the next d components to communicate data between the encoder and decoder simulation, and the next d components to simulate the decoder part. The last three components are needed as gadgets for the gates to actually simulate a sequencial read of the input, and to ensure that the hidden state of the encoder and decoder are updated properly.
We prove the above statement by induction in t. First notice that the property trivially holds for S0. Now assume that this holds for t - 1 and lets prove it for t. We know that Ut is computed as
Ut = (KU  St-1 + BU ) = (KU  St-1)

Thus we have that:

Uti,1,: = ((KU  St-1)i,1,:) = (Sit--11,1,:KU1,1,:,: + Sti,-1,1:KU2,1,:,:)

By the induction hypothesis we have

Sti,-1,1:

=

 

[0, 0, ti-1-i, 0, 0, 0]

[hi, hi, 0, 0, 1, 0]

 [xi, 0, 0, 1, 1, 0]

for i < t - 1 for i = t - 1 for i > t - 1

(20)

Now, notice that K1U,1,:,: and KU2,1,:,: are not zero only in its three last rows, thus we can focus on the three last components of the vectors in St-1, and then we can compute Uit,1,: as

Uit,1,:

=


   

([ ([

, ,

, ,

, 0, 0, 0]K1U,1,:,: + [ , 0, 0, 0]KU1,1,:,: + [

, ,

, ,

, 0, 0, 0]KU2,1,:,:) , 0, 1, 0]K2U,1,:,:)

   

([ ([

, ,

, ,

, 0, 1, 0]K1U,1,:,: + [ , 1, 1, 0]KU1,1,:,: + [

, ,

, ,

, 1, 1, 0]K2U,1,:,:) , 1, 1, 0]K2U,1,:,:)

 ([0, 0, 0, 0, 0, 0])

=

 

([0, 0, 0, 0, 0, 0])

([0, 0, 1, 0, 0, 1])

  ([1, 1, 1, 1, 1, 1])

for i < t - 1 for i = t - 1 for i = t for i > t

[0, 0, 0, 0, 0, 0] for i < t = [0, 0, 1, 0, 0, 1] for i = t
[1, 1, 1, 1, 1, 1] for i > t

for i < t - 1 for i = t - 1 for i = t for i > t

Now, for Rt we have

Rt = (KR  St-1 + BR)

and thus for Rit,1,: we have
Rti,1,: = ((KR  St-1)i,1,: + BRi,1,:) = (Sit--11,1,:KR1,1,:,: + Sit,-1,1:KR2,1,:,: + BRi,1,:) = (Sit,-1,1:KR2,1,:,: + BRi,1,:) = (Sti,-1,1:K2R,1,:,: + [0, 0, 1, 0, 0, 1])

22

Under review as a conference paper at ICLR 2019

where we deleted the term with K1R,1,:,: since it is the null matrix. Then by using the definition of Sit,-1,1: above (Equation (20)) we have

Rit,1,:

=


   

([ ([

, ,

, ,

, 0, 0, 0]K2R,1,:,: + [0, 0, 1, 0, 0, 1]) , 0, 1, 0]K2R,1,:,: + [0, 0, 1, 0, 0, 1])

 ([ , , , 1, 1, 0]K2R,1,:,: + [0, 0, 1, 0, 0, 1])

  

([

,

,

, 1, 1, 0]K2R,1,:,: + [0, 0, 1, 0, 0, 1])

 ([0, 0, 1, 0, 0, 1])

=

 

([0, 1, 1, 0, 1, 1])

([1, 1, 1, 1, 1, 1])

  ([1, 1, 1, 1, 1, 1])

for i < t - 1 for i = t - 1 for i = t for i > t

[0, 0, 1, 0, 0, 1] for i < t - 1 = [0, 1, 1, 0, 1, 1] for i = t - 1
[1, 1, 1, 1, 1, 1] for i  t

for i < t - 1 for i = t - 1 for i = t for i > t

We can now compute Sit,1,:. By the definition of St we have Sti,1,: = Uit,1,: Sti,-1,1: + (1i,1,: - Ui,1,:) ((KF  (Rt St-1))i,1,:)

where we dropped BF that has only zeros. First, by using what we already computed for Uti,1,: we have that

Sit,1,:

=

 

((KF  (Rt [0, 0, 1, 0, 0, 1]

St-1))i,1,:) Sit,-1,1: + [1, 1, 0, 1, 1, 0]

 Sti,-1,1:

((KF  (Rt

St-1))i,1,:)

for i < t for i = t for i > t

When i = t we have that Sit,-1,1: = [xi, 0, 0, 1, 0, 0] (Equation (20)), thus [0, 0, 1, 0, 0, 1] Sit,-1,1: = [0, 0, 0, 0, 0, 0]. Then

Sit,1,:

=

 ((KF  (Rt 

St-1))i,1,:)

[1, 1, 0, 1, 1, 0] ((KF  (Rt

 [xi, 0, 0, 1, 1, 0]

St-1))i,1,:)

for i < t
for i = t for i > t

(21)

We are almost done with the inductive step, we only need to compute ((KF  (Rt St-1))i,1,:). Given what we have for Rt and St-1 we have that Rt St-1 is

(Rt



St-1)i,1,:

=

 [0, 0, 1, 0, 0, 1] [0, 1, 1, 0, 1, 1]

[0, 0, ti-1-i, 0, 0, 0] [hi, hi, 0, 0, 1, 0]

 [1, 1, 1, 1, 1, 1] [xi, 0, 0, 1, 1, 0]

 

[0, 0, it-1-i, 0, 0, 0]

= [0, hi, 0, 0, 1, 0]

 [xi, 0, 0, 1, 1, 0]

for i < t - 1 for i = t - 1 for i  t

for i < t - 1 for i = t - 1 for i  t

23

Under review as a conference paper at ICLR 2019

Lets Tt = (KF  (Rt St-1)). Notice that from Equation (21) we actually need to know the values in Tti,1,: only for i  t. Now we have that

Tti,1,: = ((KF  (Rt St-1))i,1,:)

= ((Rt St-1))i,1,:K1F,1,:,: + (Rt St-1))i,1,:K2F,1,:,:)

 

([0, 0, ti--1i , 0, 0, 0]KF1,1,:,: + [0, 0, ti-1-i, 0, 0, 0]K2F,1,:,:)

= ([0, 0, ti--1i , 0, 0, 0]KF1,1,:,: + [0, hi, 0, 0, 1, 0]K2F,1,:,:)

 ([0, hi-1, 0, 0, 1, 0]KF1,1,:,: + [xi, 0, 0, 1, 1, 0]KF2,1,:,:)

for i < t - 1 for i = t - 1 for i = t

 

([0, 0, ti-1-iU , 0, 0, 0])

= ([0, 0, hiU , 0, 0, 0])

 ([xiW + hi-1V , xiW + hi-1V , 0, 0, 1, 0])

for i < t - 1 for i = t - 1 for i = t

 

[0, 0, it-i, 0, 0, 0]

= [0, 0, i1, 0, 0, 0]

 [hi, hi, 0, 0, 1, 0]

for i < t - 1 for i = t - 1 for i = t

=

[0, 0, it-i, 0, 0, 0] [hi, hi, 0, 0, 1, 0]

for i < t for i = t

Putting the value of Tit,1,: in Equation (21) we obtain

Sit,1,:

=

 

Tit,1,:

[1, 1, 0, 1, 1, 0]

Tti,1,:

 [xi, 0, 0, 1, 1, 0]

for i < t
for i = t for i > t

 

[0, 0, it-i, 0, 0, 0]

= [hi, hi, 0, 0, 1, 0]

 [xi, 0, 0, 1, 1, 0]

for i < t for i = t for i > t

which is exactly what we needed to prove (Equation (19)).

Now, lets focus on Snn,+1t,: for t  1. By what we have just proved, we obtain that
Snn,+1t,: = [0, 0, nt , 0, 0, 0] = [0, 0, gt, 0, 0, 0]
which is the decoder part of the RNN N . Thus, we can simulate the complete network N with a Neural GPU.

C.2 PROOF OF PROPOSITION 4.2

We first prove the following claim: Assume that S  Qh×w×d is a tensor that satisfies the following property: there exists a p  1 that divides h and such that, for every i  {1, 2, . . . , h - p} it holds
that Si,:,: = Si+p,:,:.
Given that we will be considering circular convolutions, then we have that for every  0 we have that S ,j,: = Sh+ ,j,: and for every > h we have that S ,j,: = Sh- ,j,:. With this we have that for every i  N it holds that Si,:,: = Si+p,:,: that is, we do not need to restrict to values in i  {1, 2, . . . , h - p}.

Now lets K be an arbitrary kernel bank of shape (kH , kW , d, d). Let T = K S where denotes the circular convolution. We prove next that

Ti,:,: = Ti+p,:,:

for every i. This is a simple fact that follows from the way in which the convolution is defined. We

use the notation in the body of the paper for T = K S, that is, we denote by sij the vector Si,j,: and Kij the matrix Ki,j,:,:. Notice that sij = si+p,j for every i  N. Now the circular convolution

is
kH kW

Ti,j,: = (K S)i,j,: =

si+1(u),j+2(v) Kuv

u=1 v=1

24

Under review as a conference paper at ICLR 2019

where 1(u) = u - kH /2 - 1 and 2(v) = v - kW /2 - 1. Then, given that sij = si+p,j for every i  N we have that

Ti,j,: = (K

kH kW

S)i,j,: =

si+1(u)+p,j+2(v) Kuv = (K

u=1 v=1

S)i+p,j,: = Ti+p,j,:

and then, Ti,:,: = Ti+p,:,:.
Consider now an arbitrary uniform Neural GPU that processes tensor S above, and assume that S1, S2, . . . , Sr is the sequence produced by it. Next we prove that for every t and for every i it holds that Sit,:,: = Sti+p,:,:. We prove it by induction in t. For the case S0 it holds by definition. Thus assume that St-1 satisfies the property. Let

Ut = fU (KU  St-1 + BU ) Rt = fR(KR  St-1 + BR) St = Ut St-1 + (1 - U)

fF (KF  (Rt

St-1) + BF )

Since we are considering uniform Neural GPUs, we know that there exist three matrices BU , BR
and BF such that for every i it holds that BUi,:,: = BU , BiR,:,: = BR, and BFi,:,: = BF . It is easy to prove that Uti,:,: = Uit+p,:,:. First note that by inductive hypothesis, we have that Sti,-:,1: = Sti+-p1,:,: and thus by the property proved above we have that (KU  St-1)i,:,: = (KU  St-1)i+p,:,:. Thus we
have that

Uit,:,: = fU ((KU  St-1)i,:,: + BU ) = fU ((KU  St-1)i+p,:,: + BU ) = Uit+p,:,:

With a similar argument we can prove that Rit,:,: = Rit+p,:,:. Moreover, notice that (Rt St-1)i,:,: = (Rt St-1)i+p,:,:, and thus (KF  (Rt St-1))i,:,: = (KF  (Rt St-1))i+p,:,:. With all this we
finally we have that

Sit,:,: = Uit,:,: Sit,-:,1: + (1i,:,: - Ui,:,:) fF ((KF  (Rt St-1))i,:,: + BF ) = Uit+p,:,: Sti+-p1,:,: + (1i+p,:,: - Ui+p,:,:) fF ((KF  (Rt St-1))i+p,:,: + BF ) = Sti+p,:,:

This completes the first part of the proof.

We have shown that if the input of a uniform neural GPU is periodic, then the output is also periodic. We make a final observation. Let N be a uniform Neural GPU, and S  Qkp×w×d be a tensor such that Si,:,: = Si+p,:,: for every i. Moreover, let T  Qk p×w×d be a tensor such that Ti,:,: = Ti+p,:,: for every i, and assume that S1:p,:,: = T1:p,:,:. Lets S1, S2, . . . and T1, T2, . . . be the sequences produced by N . Then with a similar argument as above it is easy to prove that for every t it holds that St1:p,:,: = T1t :p,:,:.
From this it is easy to prove that uniform Neural GPUs will no be able to recognize the length of periodic inputs. Thus assume that there is a language recognizer A defined by of a uniform neural GPU N such that L(A) contains all strings of even length. Assume that u is an arbitrary string in  such that |u| = p with p an odd number, and let w = uu and w = uuu. Notice that |w| = 2p and thus w  L(A), but |w | = 3p and thus w  L(A).

Let f :   Qd and let X = f (w) = (x1, x2, . . . , x2p) and X = f (w ) = (x1, x2, . . . , x3p). Consider now the tensor S  Q2p×w×d such that Si,1,: = xi for i  {1, . . . , 2p}, thus Si,:,: = Si+p,:,:. Similarly, consider T  Q3p×w×d such that such that Ti,1,: = xi for i  {1, . . . , 3p}, and thus Ti,:,: = Ti+p,:,:. Notice that S1:p,:,: = T1:p,:,: then by the property above we have that for every t it holds that St1:p,:,: = T1t :p,:,:. In particular, we have Stp,:,: = Tpt ,:,:. We also know that Spt ,:,: = S2t p,:,: and that Tpt ,:,: = Tt2p,:,: = T3t p,:,:. Thus we have that for every t it holds that St2p,1,: = Tt3p,1,:. From this we conclude that the outputs of N for both inputs X and X are the
same, and thus if A accepts w then A accepts w which is a contradiction.

25

