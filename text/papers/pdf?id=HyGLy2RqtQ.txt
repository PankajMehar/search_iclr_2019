Under review as a conference paper at ICLR 2019
OVER-PARAMETERIZATION IMPROVES GENERALIZATION IN THE XOR DETECTION PROBLEM
Anonymous authors Paper under double-blind review
ABSTRACT
Empirical evidence suggests that neural networks with ReLU activations generalize better with over-parameterization. However, there is currently no theoretical analysis that explains this observation. In this work, we study a simplified learning task with over-parameterized convolutional networks that empirically exhibits the same qualitative phenomenon. For this setting, we provide a theoretical analysis of the optimization and generalization performance of gradient descent. Specifically, we prove data-dependent sample complexity bounds which show that overparameterization improves the generalization performance of gradient descent.
1 INTRODUCTION
Most successful deep learning models use a number of parameters that is larger than the number of parameters that are needed to get zero-training error. This is typically referred to as overparameterization. Indeed, it can be argued that over-parameterization is one of the key techniques that has led to the remarkable success of neural networks. However, there is still no theoretical account for its effectiveness.
One very intriguing observation in this context is that over-parameterized networks with ReLU activations often exhibit better generalization error than smaller networks (Neyshabur et al., 2014; 2018; Novak et al., 2018). This somewhat counter-intuitive observation suggests that over-parameterized networks have an inductive bias towards solutions with better generalization performance. Understanding this inductive bias is a major theoretical challenge and is a necessary step towards a full understanding of neural networks in practice.
To better understand this phenomenon, it is crucial to be able to reason about optimization and generalization properties of over-parameterized networks with ReLU activations, which are trained with gradient based methods. However, in the current state of affairs, these are far from understood. In fact, there do not exist optimization or generalization guarantees for these networks even in very simple learning tasks such as the classic XOR problem.1
In order to make progress on this issue, we focus on a particular learning setting that captures key properties of the over-parameterization phenomenon. We consider a high-dimensional extension of the XOR problem, which we refer to as the "XOR Detection problem (XORD)". The XORD is a pattern recognition task where the goal is to learn a function which classifies binary vectors according to whether they contain a two-dimensional binary XOR pattern (i.e., (1, 1) or (-1, -1)). This problem contains the classic XOR problem as a special case when the vectors are two dimensional. We consider learning this function with gradient descent trained on an over-parameterized convolutional neural network (i.e., with multiple channels) with ReLU activations and three layers: convolutional, max pooling and fully connected. As can be seen in Fig. 1, over-parameterization improves generalization in this problem as well. Therefore it serves as a good test-bed for understanding the role of over-parameterization.
1We are referring to the problem of learning the XOR function given four two-dimensional points with binary entries, using a moderate size one-hidden layer neural network (e.g., with 50 hidden neurons). Note that there are no optimization guarantees for this setting. Variants of XOR have been studied in Lisboa & Perantonis (1991); Sprinkhuizen-Kuyper & Boers (1998) but these works only analyzed the optimization landscape and did not provide guarantees for optimization methods.
1

Under review as a conference paper at ICLR 2019
Figure 1: Over-parameterization improves generalization in the XORD problem. The network in Eq. 1 and Fig. 4 is trained on data from the XORD problem (see Sec. 3). The figure shows the test error obtained for different number of channels k. The blue curve shows test error when restricting to cases where training error was zero. It can be seen that increasing the number of channels improves the generalization performance.
.
In this work we provide an analysis of optimization and generalization of gradient descent for XORD. We show that for various ranges of accuracy and confidence parameters, sufficiently overparameterized networks have better sample complexity than a small network which can realize the ground truth classifier. To the best of our knowledge, this is the first example which shows that overparamaterization can provably improve generalization for a neural network with ReLU activations. Our analysis provides a clear distinction between the inductive bias of gradient descent for overparameterized and small networks. It reveals that over-parameterized networks are biased towards global minima that detect all possible patterns in the data.2 In contrast, a small network trained with gradient descent does not have this inductive bias. Indeed, we show that with a high constant probability, the small network will converge to a global minimum which does not contain detectors for all patterns. Therefore, even though it has succeeded in optimization, it will not learn the ground truth classifier. For further details see Sec. 5. In Sec. 6 we describe an empirical demonstration for this inductive bias, and show how it can be used to train smaller models that generalize better.
2 RELATED WORK
In recent years there have been many works on theoretical aspects of deep learning. We will refer to those that are most relevant to this work. First, we note that we are not aware of any work that shows that generalization performance provably improves with over-parameterization. This distinguishes our work from all previous works. Several works study convolutional networks with ReLU activations and their properties Du et al. (2017a;b); Brutzkus & Globerson (2017). All of these works consider convolutional networks with a single channel. Daniely (2017) and Li & Liang (2018) provide guarantees for SGD in general settings. However, their analysis holds for over-parameterized networks with an extremely large number of neurons that is not used in practice (e.g., the number of neurons is a very large polynomial of certain problem parameters). Furthermore, we consider a 3-layer convolutional network with max-pooling which is not studied in these works. Soltanolkotabi et al. (2018), Du & Lee (2018) and Li et al. (2017) study the role of overparameterization in the case of quadratic activation functions. Brutzkus et al. (2017) provide generalization guarantees for over-parameterized networks with Leaky ReLU activations on linearly separable data. Neyshabur et al. (2018) prove generalization bounds for neural networks. However, these bounds are empirically vacuous for over-parameterized networks and they do not prove that networks found by optimization algorithms give low generalization bounds.
2See Definition 5.1 for a formal definition of detection of a pattern.
2

Under review as a conference paper at ICLR 2019

3 PROBLEM FORMULATION

We begin with some notations and definitions. Let d  4 be an integer. We consider a classification problem in the space {±1}2d. Namely, the space of vectors of 2d coordinates where each coordinate can be +1 or -1. Given a vector x  {±1}2d, we consider its partition into d sets of two coordinates as follows x = (x1, ..., xd) where xi  {±1}2. We refer to each such xi as a pattern in x.

Neural Architecture: We consider learning with the following three-layer neural net model. The
first layer is a convolutional layer with non-overlapping filters and multiple channels, the second
layer is max pooling and the third layer is a fully connected layer with 2k hidden neurons and weights fixed to values ±1. Formally, for an input x = (x1, ..., xd)  R2d where xi  R2, the output of the network is given by:

k

NW (x) =

max  w(i) · xj

j

i=1

- max  u(i) · xj
j

(1)

where W  R2k×2 is the weight matrix whose rows are the w(i) vectors followed by the u(i) vectors, and (x) = max{0, x} is the ReLU activation applied element-wise. See Figure 4 for an
illustration of this architecture.

Remark 3.1. Because there are only 4 different patterns, the network is limited in terms of the number of different rules it can implement. Specifically, it is easy to show that its VC dimension is at most 15 (see Sec. 10). Despite this limited expressive power, there is a generalization gap between small and large networks in this setting, as can be seen in Figure 1, and in our analysis below.

Data Generating Distribution: Next we define the classification rule we will focus on. Let

PXOR correspond to the following two patterns: PXOR = {(1, 1), (-1, -1)}. Define the classification rule:

f (x) =

1 i  {1, . . . , d} : xi  PXOR -1 otherwise

(2)

Namely, f  detects whether a pattern in PXOR appears in the input. In what follows, we refer to PXOR as the set of positive patterns and {±1}2 \ PXOR as the set of negative patterns.
Let D be a distribution over X × {±1} such that for all (x, y)  D we have y = f (x). We say that a point (x, y) is positive if y = 1 and negative otherwise. Let D+ be the marginal distribution over {±1}2d of positive points and D- be the marginal distribution of negative points. In the following definition we introduce the notion of diverse points, which will play a key role in our analysis.

Definition 3.2 (Diverse Points). We say that a positive point (x, 1) is diverse if for all z  {±1}2 there exists 1  i  d such that xi = z. We say that a negative point (x, -1) is diverse if for all z  {±1}2 \ PXOR there exists 1  i  d such that xi = z.

For   {-, +} define p to be the probability that x is diverse with respect to D. For example,

if both D+ and D- are uniform, then by the inclusion-exclusion principle it follows that p+ =

1

-

4·3d -6·2d +4 4d

and

p-

=

1

-

.1
2d-1

Learning Setup: Our analysis will focus on the problem of learning f  from training data with a three layer neural net model. The learning algorithm will be gradient descent, randomly initialized. As in any learning task in practice, f  is unknown to the training algorithm. Our goal is to analyze the performance of gradient descent when given data that is labeled with f . We assume that we are given a training set S = S+  S-  {±1}2d × {±1}2 where S+ consists of m IID points drawn from D+ and S- consists of m IID points drawn from D-.3
Importantly, we note that the function f  can be realized by the above network with k = 2. Indeed, the network N defined by the filters w(1) = (3, 3), w(2) = (-3, -3), u(1) = (-1, 1), u(2) = (1, -1) satisfies sign (N (x)) = f (x) for all x  {±1}2d. It can be seen that for k = 1, f  cannot be realized. Therefore, any k > 2 is an over-parameterized setting.

3For simplicity, we consider this setting of equal number of positive and negative points in the training set.

3

Under review as a conference paper at ICLR 2019

Training Algorithm: We will use gradient descent to optimize the following hinge-loss function.

11

(W ) = m

max{ -NW (xi), 0}+ m

max{1+NW (xi), 0} (3)

(xi ,yi )S+ :yi =1

(xi ,yi )S- :yi =-1

for   1. 4We assume that gradient descent runs with a constant learning rate  and the weights
are randomly initiliazed with IID Gaussian weights with mean 0 and standard deviation g. Furthermore, only the weights of the first layer, the convolutional filters, are trained. 5

We will need the following notation. Let Wt be the weight matrix in iteration t of gradient descent.

For 1  i  k, denote by w(ti)  R2 the ith convolutional filter at iteration t. Similarly, for

1  i  k we define ut(i)  R2 to be the k + i convolutional filter at iteration t. We assume that each

w0(i) and u(0i) is initialized as a Gaussian random variable where the entries are IID and distributed

as

N (0, g2).

In

each

iteration,

gradient

descent

performs

the

update

Wt+1

=

Wt

-



 W

(Wt).

4 MAIN RESULT

In this section we state our main result that demonstrates the generalization gap between overparameterized networks and networks with k = 2. Define the generalization error to be the difference between the 0-1 test error and the 0-1 training error. For any ,  and training algorithm let m( , ) be the sample complexity of a training algorithm, namely, the number of minimal samples the algorithm needs to get at most generalization error with probability at least 1 - . We consider running gradient descent in two cases, when k  120 and k = 2. In the next section we exactly define under which set of parameters gradient descent runs, e.g., which constant learning rates.

Fix parameters p+ and p- of a distribution D and denote by c < 10-10 a negligible constant. Assume that gradient descent is given a sample of points drawn from D+ and D-. We denote the sample complexity of gradient descent in the cases k  120 and k = 2, by m1 and m2, respectively. The following result shows that there can be a distribution-dependent gap between these sample
complexities.

Theorem 4.1. For any p+, p- there exists D for which the following holds. Let   1 - p+p-(1 -

c - 16e-8) and

 min

1-p+ 4

,

1-p- 4

.

Then m1(

, )



2 whereas m2(

, )



( )2 log

48 33(1-c)

.log(p+ p- )

The proof follows from Theorem 5.2 and Theorem 5.3 which we state in the next section. The proof is given in Sec. 8.7. One surprising fact of this theorem is that m1(0, )  2. Indeed, our analysis shows that for an over-parameterized network and for sufficiently large p+ and p-, one diverse positive point and one diverse negative suffice for gradient descent to learn f  with high probability. We note that even in this case, the dynamics of gradient descent is highly complex. This is due to the randomness of the initialization and to the fact that there are multiple weight filters in the network, each with different dynamics. See Sec. 5 for further details.
We will illustrate the guarantee of Theorem 4.1 with several numerical examples. In the first example, we assume that p+ = p- = 0.98 and  = 1 - 0.982(1 - c - 16e-8)  0.05. In this case we get that for any 0   0.005, m1( , )  2 whereas m2( , )  129. For the second example consider the case where p+ = p- = 0.92. It follows that there exists distributions such that for  = 0.16 and any 0   0.02 it holds that m1( , )  2 and m2( , )  17.
For = 0 and any  > 0, by setting p+ and p- to be sufficiently close to 1, we can get an arbitrarily large gap between m1( , ) and m2( , ). In contrast, for sufficiently small p-, p+, e.g., in which p+, p-  0.5, our bound does not guarantee a generalization gap.

4In practice it is common to set  to 1. In our analyis we will need   8 to guarantee generalization. In Section 8.3 we show empirically, that for this task, setting  to be larger than 1 results in better test performance than setting  = 1.
5Note that Hoffer et al. (2018) show that fixing the last layer weights to ±1 does not degrade performance in various tasks. This assumption also appeared in other works (Brutzkus et al., 2017; Li & Yuan, 2017).

4

Under review as a conference paper at ICLR 2019

5 PROOF SKETCH AND INSIGHTS

In this section we sketch the proof of Theorem 4.1. The theorem follows from two theorems: Theorem 5.2 for over-parameterized networks and Theorem 5.3 for networks with k = 2. We formally show this in Sec. 8.7. In Sec. 5.1 we state Theorem 5.2 and outline its proof. In Sec. 5.2 we state Theorem 5.3 and shortly outline its proof. In what follows, we will need the following formal definition for a detection of a pattern by a network.

Definition 5.1. Let cd  0 be a constant. For each positive pattern vp define Dvp =

k i=1



w(i) · vp

and for each negative pattern vn define Dvn =

k i=1



u(i) · vn .

We say

that a pattern v (positive or negative) is detected by the network NW with confidence cd if Dv > cd.

Theorem 5.2 and Theorem 5.3 together imply a clear characterization of the different inductive biases of gradient descent in the case of small (k = 2) and over-parameterized networks. The characterization is that over-parameterized networks are biased towards global minima that detect all patterns in the data, whereas small networks with k = 2 are biased towards global minima that do not detect all patterns (see Definition 5.1). In Sec. 8.4 we show this empirically in the XORD problem and in a generalization of the XORD problem.
In the following sections we will need several notations. Define x1 = (1, 1), x2 = (1, -1), x3 = (-1, -1), x4 = (-1, 1) to be the four possible patterns in the data and the following sets:

Wt+(i) = j | arg max w(tj) · xl = i , Ut+(i) = j | arg max ut(j) · xl = i

1l4

1l4

Wt-(i) = j | arg max w(tj) · xl = i , Ut-(i) = j | arg max u(tj) · xl = i

l{2,4}

l{2,4}

(4)

We denote by x+ a positive diverse point and x- a negative diverse point. Define the following sum:

St+ =

max  w(j) · x+1 , ...,  w(j) · xd+

j Wt+ (1)Wt+ (3)

Finally, in all of the results in this section we will denote by c < 10-10 a negligible constant.

5.1 A SAMPLE COMPLEXITY UPPER BOUND FOR OVER-PARAMETERIZED NETWORKS

The main result in this section is given by the following theorem.

Theorem 5.2. Let with parameters 

S =

=
c k

S+  S- where c

be 

a training set as

1 410

,

g



c
3
16k 2

in ,k

Sec. 3.  120

Assume that gradient descent runs and   8. Then, with probability

at least (p+p-)m

1 - c - 16e-8

after running gradient descent for T



28( +1+8c ) c

iterations,

it converges to a global minimum which satisfies:

1. sign (NWT (x)) = f (x) for all x  {±1}2d.

2.

Let (k) =



.k
4

+2k

k 4

-2

k

Then

for

cd



1-

5c 4

(k)+1

,

all

patterns

are

detected

with

confidence

cd.

This result shows that given a small training set size, over-parameterized networks converge to a global minimum which realizes the classifier f  with high probability and in a constant number of iterations. Furthermore, this global minimum detects all patterns in the data with confidence that increases with over-parameterization. The full proof of Theorem 5.2 is given in Sec. 8.5.
We will now sketch its proof. With probability at least (p1p-1)m all training points are diverse and we will condition on this event. From Sec. 10 we can assume WLOG that the training set consists of one positive diverse point x+ and one negative diverse point x- (since the network will have the same output on all same-label diverse points). We note that empirically over-parameterization improves generalization even when the training set contains non-diverse points (see Fig. 1 and Sec. 8.2).

5

Under review as a conference paper at ICLR 2019

Now, to understand the dynamics of gradient descent it is crucial to understand the dynamics of the sets in Eq. 4. This follows since the gradient updates are expressed via these sets. Concretely, let j  Wt+(i1)  Wt-(i2) then the gradient update is given as follows: 6

1 1w(t+j)1 = wt(j) + xi1 NW (x+)< - xi2 NW (x-)<1

(5)

Similarly, for j  Ut+(i1)  Ut-(i2) the gradient update is given by:

1 1ut(+j)1 = u(tj) - xi1 NW (x+)< + xi2 NW (x-)<1

(6)

Furthermore, the values of NW (x+) and NW (x-) depend on these sets and their corresponding weight vectors, via sums of the form St+, defined above.

The proof consists of a careful analysis of the dynamics of the sets in Eq. 4 and their corresponding

weight vectors. For example, one result of this analysis isthat for all t  1 and i  {1, 3} we have

Wt+(i)

=

W0+(i)

and

the

size

of

Wt+(i)

is

at

least

k 2

-

2

k with high probability.

There are two key technical observations that we apply in this analysis. First, with a small initial-

ization and with high probability, for all 1  j  k and 1  i  4 it holds that

w(0j) · xi



 4

and

u0(j) · xi



 4

.

This allows us to keep track of the dynamics of the sets in Eq. 4 more easily.

For

example, by this observation it follows that if for some j  Wt+(2) it holds that j  Wt++1(4),

then for all j such that j  Wt+(2) it holds that j  Wt++1(4). Hence, we can reason about the

dynamics of several filters all at once, instead of each one separately. Second, by concentration of

measure we can estimate the sizes of the sets in Eq. 4 at iteration t = 0. Combining this with results of the kind Wt+(i) = W0+(i) for all t, we can understand the dynamics of these sets throughout the optimization process.

The theorem consists of optimization and generalization guarantees. For the optimization guarantee
we show that gradient descent converges to a global minimum. To show this, the idea is to characterize the dynamics of St+ using the characterization of the sets in Eq. 4 and their corresponding weight vectors. We show that as long as gradient descent did not converge to a global minimum, St+ cannot decrease in any iteration and it is upper bounded by a constant. Furthermore, we show that there cannot be too many consecutive iterations in which St+ does not increase. Therefore, after sufficiently many iterations gradient descent will converge to a global minimum.

We will now outline the proof of the generalization guarantee. Denote the network learned by
gradient descent by NWT . First, we show that the network classifies all positive points correctly. Define the following sums for all 1  i  4:

Xt+(i) =

max 
k

w(j) · x+k

j WT+ (i)

, Yt+(i) =

max 
k

u(j) · x+k

j UT+ (i)

(7)

First we notice that for all positive z we have NWT (z) min{XT+(1), XT+(3)} - YT+(2) - YT+(4).

Then by the fact that NWT (x+)   at the global minimum, we can show that XT+(1) + XT+(3)

is sufficiently large. As mentioned previously, by concentration of measure, W0+(i) is sufficiently

large. Then by a symmetry argument we show that this implies that both XT+(1) and XT+(3) are

sufficiently -NWT (x-

large. Finally, we show that YT+(2) + YT+(4) is not too large due to an upper bound ). Hence, we can show that each positive point is classified correctly. The proof that

on all

negative points are classified correctly is similar but slightly more technical. We refer the reader to

Sec. 8.5 for further details.

Finally, for completeness, in Sec. 9 we also provide a convergence guarantee for the XOR problem, where d = 1.

6Note that with probability 1,  (wt(j) · xi1 ) = 1,  (wt(j) · xi2 ) = 1 for all t, and therefore we omit these from the gradient update. This follows since  (wt(j) · xi1 ) = 0 for some t only if w0(j) · xi1 is an integer multiple of .

6

Under review as a conference paper at ICLR 2019

(a) (b) (c)
Figure 2: Inductive bias in XORD and OBD problems. (a) Over-parameterization improves generalization in the OBD problem (b) Pattern detection phenomenon in the XORD problem (c) Pattern detection phenomenon in the OBD problem. In both (b) and (c) we see that as the number of channels increase, gradient descent is biased towards %0 training error solutions with more detected patterns.

5.2 A SAMPLE COMPLEXITY LOWER BOUND FOR SMALL NETWORKS (k = 2)

The following theorem provides generalization lower bounds of global minima in the case that k = 2 and in a slightly more general setting than the one given in Theorem 5.2.

Theorem 5.3. Let S with parameters  =

=
c k

S+  S- where c

be 

a training set as in Sec. 3.

1 41

,

g



c
3
16k 2

,

k

=

2

and

Assume that   1. Then

gradient descent runs the following holds:

1.

With

probability

at

least

(p+p-)m

(1

-

c)

33 48

,

gradient

descent

converges

to

a

global

min-

imum that has non-zero test error. Furthermore, for cd  2c, there exists at least one

pattern which is not detected by the global minimum with confidence cd.

2. There exists distributions D such that the non-zero test error above is at least

min

1-p+ 4

,

1-p- 4

.

The theorem shows that given a small training set size, with high constant probability, gradient descent will converge to a global minimum that is not the classifier f . Furthermore, this global minimum does not detect at least one pattern. The proof of the theorem is given in Sec. 8.6.
We will now provide a short outline of the proof. Let wT(1), w(T2), u(T1) and u(T2) be the filters of the network at the iteration T in which gradient descent converges to a global minimum. The proof shows that gradient descent will not learn f  if one of the following conditions is met: a) WT+(1) = . b) WT+(3) = . c) u(1) · x2 > 0 and u(2) · x2 > 0. d) u(1) · x4 > 0 and u(2) · x4 > 0. Then by using a symmetry argument which is based on the symmetry of the initialization and the training data it can be shown that one of the above conditions is met with high constant probability.

6 EXPERIMENTS
We perform several experiments that corroborate our theoretical findings. In Sec. 8.4 we empirically demonstrate our insights on the inductive bias of gradient descent. In Sec. 6.2 we evaluate a model compression scheme implied by our results, and demonstrate its success on the MNIST dataset.
6.1 PATTERN DETECTION
In this section we perform experiments to examine the insights from our analysis on the inductive bias of gradient descent. Namely, that over-parameterized networks are biased towards global minima that detect more patterns in the data than global minima found by smaller networks. We check this both on the XORD problem which contains 4 possible patterns in the data and on an instance of an extension of the XORD problem, that we refer to as the Orthonormal Basis Detection (OBD) problem, which contains 60 patterns in the data. In Sec. 8.4 we provide details on the experimental setups.

7

Under review as a conference paper at ICLR 2019
Figure 3: Model compression in MNIST. The plot shows the test error of the small network (4 channels) with standard training (red), the small network that uses clusters from the large network (blue), and the large network (120 channels) with standard training (green). It can be seen that the large network is effectively compressed without losing much accuracy.
Due to space considerations, we will not formally define the OBD problem in this section. We refer the reader to Sec. 8.4 for a formal definition. Informally, The OBD problem is a natural extension of the XORD problem that contains more possible patterns in the data and allows the dimension of the filters of the convolutional network to be larger. The patterns correspond to a set of orthonormal vectors and their negations. The ground truth classifier in this problem can be realized by a convolutional network with 4 channels. In Fig. 2 we show experiments which confirm that in the OBD problem as well, overparameterization improves generalization. We further show the number of patterns detected in %0 training error solutions for different number of channels, in both the XORD and OBD problems. It can be clearly seen that for both problems, over-parameterized networks are biased towards %0 training error solutions that detect more patterns, as predicted by the theoretical results.
6.2 NETWORK COMPRESSION
By inspecting the proof of Theorem 5.2, one can see that the dynamics of the filters of an overparameterized network are such that they either have low norm, or they have large norm and they point to the direction of one of the patterns (see, e.g., Lemma 8.4 and Lemma 8.6). This suggests that by clustering the filters of a trained over-parameterized network to a small number of clusters, one can create a significantly smaller network which contains all of the detectors that are needed for good generalization performance. Then, by training the last layer of the network, it can converge to a good solution. Following this insight, we tested this procedure on the MNIST data set and a 3 layer convolutional network with convolutional layer with multiple channels and 3 × 3 kernels, max pooling layer and fully connected layer. We trained an over-parameterized network with 120 channels, clustered its filters with k-means into 4 clusters and used the cluster centers as initialization for a small network with 4 channels. Then we trained only the fully connected layer of the small network. In Fig. 3 we show that for various training set sizes, the performance of the small network improves significantly with the new initialization and nearly matches the performance of the overparameterized network.
7 CONCLUSION
In this paper we consider a simplified learning task on binary vectors and show that overparameterization can provably improve generalization performance of a 3-layer convolutional network trained with gradient descent. Several open problems remain. It would be interesting to extend our result to the case where the training set contains non-diverse points. Another direction for future work is to study other settings, such as the OBD problem introduced in the previous section, and prove similar results. Finally, it would be interesting to further study the implications of such results on model compression and on improving training algorithms.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.
Amit Daniely. Sgd learns the conjugate kernel class of the network. arXiv preprint arXiv:1702.08503, 2017.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017b.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your classifier: the marginal value of training the last weight layer. 2018.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix recovery. arXiv preprint arXiv:1712.09203, 2017.
PJG Lisboa and SJ Perantonis. Complete solution of the local minima in the xor problem. Network: Computation in Neural Systems, 2(1):119­124, 1991.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards understanding the role of over-parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076, 2018.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha SohlDickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory, 2018.
Ida G Sprinkhuizen-Kuyper and Egbert JW Boers. The error surface of the 2-2-1 xor network: The finite stationary points. Neural Networks, 11(4):683­690, 1998.
Roman Vershynin. High-dimensional probability. An Introduction with Applications, 2017.
9

Under review as a conference paper at ICLR 2019

Figure 4: Network architecture used for training in the XORD problem.

8 APPENDIX
8.1 NETWORK ARCHITECTURE IN THE XORD PROBLEM
8.2 EXPERIMENTAL SETUPS
8.2.1 EXPERIMENT IN FIGURE 1
We tested the generalization performance in the setup of Section3. We considered networks with number of channels 4,6,8,20,100 and 200. The distribution in this setting has p+ = 0.5 and p- = 0.9 and the training sets are of size 12 (6 positive, 6 negative). The ground truth network can be realized by a network with 4 channels. For each number of channels we trained a convolutional network 100 times and averaged the results. We plot both average test error over all 100 runs and average test error only over the runs that ended at 0% train error. In this case, for each number of channels 4,6,8,20,100,200 the number of runs in which gradient descent converged to a 0% train error solution is 62, 79, 94, 100, 100, 100, 100, respectively.

8.3 HINGE LOSS CONFIDENCE
Figure 5 shows that setting  = 5 gives better performance than setting  = 1 in the XORD problem. The setting is similar to the setting of Section 8.2.1. Each point is an average test error of 100 runs.

8.4 EXPERIMENTAL SETUP IN SECTION 6

We will first formally define the OBD problem. Fix an even dimension parameter d1  2. In this

problem, we assume there is an orthonormal basis B = {v1, ..., vd1 } of Rd1 . Divide B into two

equally sized sets B1

and B2, each of size

d1 2

.

Now define the set of positive patterns to be P

=

{v | v  B1}  {-v | v  B1} and negative patterns to be N = {v | v  B2}  {-v | v  B2}.

Let POBD = P  N . For d2 > 0, we assume the input domain is X  Rd1d2 and each x  X is

a vector such that x = (x1, ..., xd2 ) where each xi  POBD. We define the ground truth classifier fOBD : X  {±1} such that fOBD(x) = 1 if and only there exists at least one xi such that

xi  P . Notice that for d1 = 2 and by normalizing the four vectors in {±1}2 to have unit norm, we

get the XORD problem. We note that the positive patterns in the XORD problem are defined to be

PXOR and the negative patterns are {±1}2 \ PXOR.

10

Under review as a conference paper at ICLR 2019

Figure 5: Higher confidence of hinge-loss results in better performance in the XORD problem.

Let D be a distribution over X 2d × {±1} such that for all (x, y)  D, y = fOBD(x). As in the XORD problem we define the distributions D+ and D-. We consider the following learning task
which is the same as the task for the XORD problem. We assume that we are given a training set S = S+  S-  {±1}d1d2 × {±1} where S+ consists of m IID points drawn from D+ and S- consists of m IID points drawn from D-. The goal is to train a neural network with randomly initialized gradient descent on S and obtain a network N : Rd1d2  R such that sign (N (x)) = fOBD(x) for all x  {±1}d1d2 .

We consider the same network as in the XORD problem (Eq. 1), but now the filters of the convolution layer are d1-dimensional. Formally, for an input x = (x1, ..., xd)  X the output of the network is given by

k

NW (x) =

max  w(i) · x1 , ...,  w(i) · xd

i=1

- max  u(i) · x1 , ...,  u(i) · xd

(8)

where W  R2k×d1 is the weight matrix which contains in the first k rows the vectors w(i)  Rd1 , in the next k rows the vectors u(i)  Rd1 and (x) = max{0, x} is the ReLU activation applied element-wise. We performed experiments in the case that d1 = 30, i.e., in which there are 60 possible patterns.

In Figure 2a, for each number of channels we trained a convolutional network given in Eq. 8 with gradient descent for 100 runs and averaged the results. The we sampled 25 positive points and 25 negative points in the following manner. For each positive point we sampled with probability 0.25 one of the numbers [4,6,8,10] twice with replacement. Denote these numbers by m1 and m2. Then we sampled m1 different positive patterns and m2 different negative patterns. Then we filled a 60d1dimensional vectors with all of these patterns. A similar procedure was used to sample a negative point. We plot both average test error over all 100 runs and average test error only over the runs that ended at 0% train error. For each number of channels 4,6,8,20,100,200,500 the number of runs in which gradient descent converged to a 0% train error solution is 96, 99, 100, 100, 100, 100, 100, respectively. For each 0% train error solution we recorded the number of patterns detected according to the Definition 5.1 (generalized to the OBD problem).

8.5 PROOF OF THEOREM 5.2

We will first need a few notations. Define x1 = (1, 1), x2 = (1, -1), x3 = (-1, -1), x4 = (-1, 1) and the following sets:

Wt+(i) = j | arg max w(tj) · xl = i , Ut+(i) = j | arg max ut(j) · xl = i

1l4

1l4

Wt-(i) = j | arg max w(tj) · xl = i , Ut-(i) = j | arg max u(tj) · xl = i

l{2,4}

l{2,4}

11

Under review as a conference paper at ICLR 2019

We can use these definitions to express more easily the gradient updates. Concretely, let j  Wt+(i1)  Wt-(i2) then the gradient update is given as follows:7

1 1w(t+j)1 = wt(j) + xi1 NW (x+)< - xi2 NW (x-)<1

(9)

Similarly, for j  Ut+(i1)  Ut-(i2) the gradient update is given by:

1 1u(t+j)1 = u(tj) - xi1 NW (x+)< + xi2 NW (x-)<1

(10)

We denote by x+ a positive diverse point and x- a negative diverse point. Define the following sums for   {+, -}:

St =

max  w(j) · x1 , ...,  w(j) · xd

j Wt+ (1)Wt+ (3)

Pt =

max  u(j) · x1 , ...,  u(j) · xd

j Ut+ (1)Ut+ (3)

Rt =

max  w(j) · x1 , ...,  w(j) · xd

j Wt+ (2)Wt+ (4)

- max  u(i) · x1 , ...,  u(i) · xd
j Ut+ (2)Ut+ (4)

Note that Rt+ = Rt- since for z  {x+, x-} there exists i1, i2 such that zi1 = x2, zi2 = x4.
By the conditions of the theorem, with probability at least (p+p-)m all the points in the training set are diverse. From now on we will condition on this event. Furthermore, without loss of generality, we can assume that the training set consists of one diverse point x+ and one negative points x-. This follows since the network and its gradient have the same value for two different positive diverse points and two different negative points. Therefore, this holds for the loss function defined in Eq. 3 as well.

We will now proceed to prove the theorem. In Section 8.5.1 we prove results on the filters at ini-

tialization. In Section 8.5.2 we prove several auxiliary lemmas. In Section 8.5.3 we prove upper

bounds on St-, Pt+ and Pt- for all iterations t. In Section 8.5.4 we characterize the dynamics of St+

and in Section 8.5.5 we prove -NWt (x-) for all iterations t.

an

upper

bound

on

it

together

with

upper

bounds

on

NWt (x+)

and

We provide an optimization guarantee for gradient descent in Section 8.5.6. We prove generalization guarantees for the points in the positive class and negative class in Section 8.5.7 and Section 8.5.8, respectively. We complete the proof of the theorem in Section 8.5.9.

8.5.1 INITIALIZATION GUARANTEES

Lemma 8.1. With probability at least 1 - 4e-8, it holds that

W0+(1)  W0+(3)

-k 2

 2 k

and

U0+(1)  U0+(3)

-k 2

 2 k

7Note that with probability 1,  (wt(j) · xi1 ) = 1,  (wt(j) · xi2 ) = 1 for all t, and therefore we omit these from the gradient update. This follows since  (w(tj) · xi1 ) = 0 for some t if and only if w0(j) · xi1 is an integer multiple of .

12

Under review as a conference paper at ICLR 2019

Proof. Without loss of generality consider W0+(1)  W0+(3) . Since P j  W0+(1)  W0+(3) =

1 2

,

we

get

by

Hoeffding's

inequality

P

W0+(1)  W0+(3)

-k 2

 <2 k

 2e-

2(22 k

k)

= 2e-8

The result now follows by the union bound.



Lemma 8.2.

With probability 

1-

 2k e8k

,

for all 1



j



k

and 1



i



4 it holds that

w(0j) · xi



 4

and

u(0j) · xi



 4

.

Proof. Let Z be a random variable distributed as N (0, 2). Then by Proposition 2.1.2 in Vershynin

(2017), we have

P [|Z|  t]  2

e-

t2 22

2t

Therefore, for all 1  j  k and 1  i  4,

P

w(0j) · xi

 4

  1 e-8k 32k

and

P

u(0j) · xi

 4

  1 e-8k 32k

The result follows by applying a union bound over all 2k weight vectors and the four points xi, 1  i  4.

From now on we assume that the highly probable event in Lemma 8.2 holds. Lemma 8.3. NWt (x+) < 1 and -NWt (x-) < 1 for 0  t  2.

Proof. By Lemma 8.2 we have

k

NW0 (x+) =

max  w(0i) · x1+ , ...,  w0(i) · x+d - max 

i=1

 k <  4

and similarly -NW0 (x-) < 1. Therefore, by Eq. 9 and Eq. 10 we get:

u0(i) · x1+

, ..., 

u0(i) · x+d

1. For i  {1, 3}, l  {2, 4}, j  W0+(i)  W0-(l), it holds that w1(j) = w0(j) - xl + xi. 2. For i  {2, 4} and j  W0+(i), it holds that w1(j) = w(0j). 3. For i  {1, 3}, l  {2, 4}, j  U0+(i)  U0-(l), it holds that u(1j) = u0(j) - xi + xl. 4. For i  {2, 4} and j  U0+(i), it holds that u(2j) = u(0j).

Applying Lemma 8.2 again and using the fact that 



1 8k

we

have

NW1 (x+)

<

 and

-NW1 (x-) < 1. Therefore we get,

1. For i  {1, 3}, l  {2, 4}, j  W0+(i)  W0-(l), it holds that w(2j) = w(0j) + 2xi. 2. For i  {2, 4} and j  W0+(i), it holds that w(2j) = w(0j). 3. For i  {1, 3}, l  {2, 4}, j  U0+(i)  U0-(l), it holds that u(2j) = u0(j) - xi + xl. 4. For i  {2, 4} and j  U0+(i), it holds that u2(j) = u0(j).
As before, by Lemma 8.2 we have NW2 (x+) <  and -NW2 (x-) < 1.

13

Under review as a conference paper at ICLR 2019

8.5.2 AUXILIARY LEMMAS Lemma 8.4. For all t  1 we have Wt+(i) = W0+(i) for i  {1, 3}.

Proof. We will first prove that W0+(i)  Wt+(i) for all t  1. To prove this, we will show by induction on t  1, that for all j  W0+(i)  W0+(l), where l  {2, 4} the following holds:

1. j  Wt+(i). 2. wt(j) · xl = w0(j) · xl -  or wt(j) · xl = w(t0) · xl. 3. wt(j) · xi > .

The claim holds for t = 1 by the proof of Lemma 8.3. Assume it holds for t = T . By the induction hypothesis there exists an l  {2, 4} such that j  WT+(i)  WT-(l ). By Eq. 9 we have,

w(Tj+) 1 = wT(j) + axi + bxl

(11)

where a  {0, 1} and b  {-1, 0}.

If w(Tj)·xl = w(0j)·xl then l = l and either w(Tj+) 1·xl = w(0j)·xl if b = 0 or wT(j+) 1·xl = w(0j)·xl-

if b = -1. Otherwise, assume that w(Tj) · xl = w0(j) · xl - . By Lemma 8.2 we have 0 < w(0j) · xl <

 4

.

Therefore

-

<

w(Tj)

·

xl

<

0

and

l

= l. It follows that either w(Tj+) 1 · xl = w0(j) · xl -  if

b = 0 or w(Tj+) 1 · xl = w(0j) · xl if b = -1. In both cases, we have w(Tj+) 1 · xl < . Furthermore, by

Eq. 11, w(Tj+) 1 ·xi  wT(j) ·xi > . Hence, arg max1l4 w(Tj+) 1 ·xl = i which by definition implies that j  WT++1(i). This concludes the proof by induction which shows that W0+(i)  Wt+(i) for all t  1.

In order to prove the lemma, it suffices to show that W0+(2)  W0+(4)  Wt+(2)  Wt+(4). This

follows since

4 i=1

Wt+(i)

=

{1, 2, ..., k}.

We will show by induction on t



1, that for all j



W0+(2)  W0+(4), the following holds:

1. j  Wt+(2)  Wt+(4). 2. w(tj) = w0(j) + mx2 for m  Z.

The claim holds for t = 1 by the proof of Lemma 8.3. Assume it holds for t = T . By the induction hypothesis j  WT+(2)  WT+(4). Assume without loss of generality that j  WT+(2). This implies that j  WT-(2) as well. Therefore, by Eq. 9 we have

w(Tj+) 1 = wT(j) + ax2 + bx2

(12)

where a  {0, 1} and b  {0, -1}. By the induction hypothesis, wT(j+) 1 = w0(j) + mx2 for m  Z. If a = 1 or b = 0 we have for i  {1, 3},

wT(j+) 1 · x2  w(Tj) · x2 > wT(j) · xi = wT(j+) 1 · xi where the first inequality follows since j  WT+(2) and the second by Eq. 12. This implies that j  WT++1(2)  WT++1(4).

Otherwise, assume that a

=

0 and b

=

-1.

By Lemma 8.2 we have w(0j) · x2

<

 4

.

Since

j  WT+(2), it follows by the induction hypothesis that w(Tj) = w(0j) + mx2, where m  Z and

m  0. To see this, note that if m < 0, then w(Tj) · x2 < 0 and j / WT+(2), which is a contradiction.

Let

i



{1, 3}.

If m

=

0,

then

wT(j+) 1

=

w0(j) - x2,

wT(j+) 1 · x4

>

 2

and

wT(j+) 1 · xi

=

w0(j) · xi

<

 4

by Lemma 8.2. Therefore, j  WT++1(4).

Otherwise, if m > 0, then wT(j+) 1 · x2  w(0j) · x2 > w0(j) · xi = w(Tj+) 1 · xi. Hence, j  WT++1(2), which concludes the proof.

14

Under review as a conference paper at ICLR 2019

Lemma 8.5. For all t  0 we have U0+(2)  U0+(4)  Ut+(2)  Ut+(4).

Proof. Let j  U0+(2)  U0+(4). It suffices to prove that ut(j) = u0(j) + tx2 for t  Z. This

follows since the inequalities

u0(j) · x1

<

u0(j) · x2



 4

imply that in this case j



Ut+(2) 

Ut+(4). Assume by contradiction that there exist an iteration t for which ut(j) = u0(j) + tx2 + txi where t  {-1, 1}, t  Z, i  {1, 3} and u(t-j)1 = u0(j) + t-1x2 where t-1  Z. 8 Since the coefficient of xi changed in iteration t, we have j  Ut+-1(1)  Ut+-1(3). However,
this contradicts the claim above which shows that if ut(-j)1 = u0(j) + t-1x2, then j  Ut+-1(2)  Ut+-1(4).

Lemma 8.6. Let i  {1, 3} and l  {2, 4}. For all t  0, if j  U0+(i)  U0-(l), then there exists at  {0, -1}, bt  N such that ut(j) = u(0j) + atxi + btxl.

Proof. First note that by Eq. 10 we generally have u(tj) = u0(j) + xi + xl where ,   Z.

Since

u(0j) · x1



 4

,

by the

gradient update in Eq.

10

it

holds

that

at



{0, -1}.

Indeed,

a0

=

0

and by the gradient update if at-1 = 0 or at-1 = -1 then at  {-1, 0}.

Assume by contradiction that there exists an iteration t > 0 such that bt = -1 and bt-1 = 0. Note that by Eq. 10 this can only occur if j  Ut+-1(l). We have u(t-j)1 = u(0j) + at-1xi where

at-1  {0, -1}. Observe that

u(t-j)1 · xi



u(0j) · xi

by the fact that

u0(j) · xi



 4

.

Since

u0(j) · xi > u0(j) · xl = u(t-j)1 · xl we have j  Ut+-1(1)  Ut+-1(3), a contradiction.

Lemma 8.7. Let

Xt+ =

max  w(i) · x1+ , ...,  w(i) · x+d

j Wt+ (1)

and

Yt+ =

max  w(i) · x+1 , ...,  w(i) · xd+

j Wt+ (3)

Then for all t,

Xt+ -X0+
|Wt+(1)|

=

Yt+ -Y0+
|Wt+(3)|

.

Proof. We will prove the claim by induction on t. For t = 0 this clearly holds. Assume it holds for t = T . Let j1  WT+(1) and j2  WT+(3). By Eq. 9, the gradient updates of the corresponding weight vector are given as follows:

wT(j+1)1 = wT(j1) + ax1 + b1x2
and w(Tj+2)1 = wT(j2) + ax3 + b2x2
where a  {0, 1} and b1, b2  {-1, 0, 1}. By Lemma 8.4, j1  WT++1(1) and j2  WT++1(3). Therefore,

max



w(Tj+1)1 · x+1

, ..., 

w

(j1 ) T +1

·

xd+

= max  wT(j1) · x+1 , ...,  wT(j1) · xd+

+ a

and

max



w(Tj+2)1 · x1+

, ..., 

w

(j2 ) T +1

·

xd+

= max  wT(j2) · x1+ , ...,  w(Tj2) · x+d

+ a

8Note that in each iteration t changes by at most .

15

Under review as a conference paper at ICLR 2019

By Lemma 8.4 we have Wt+(1) = W0+(1) and Wt+(3) = W0+(3) for all t. It follows that

XT++1 - X0+ WT++1(1)

=

a

W0+(1) + XT+ - X0+ W0+(1)

=

a

+

YT+ - Y0+ W0+(3)

=

a

W0+(3) + YT+ - Y0+ W0+(3)

=

YT++1 - Y0+ WT++1(3)

where the second equality follows by the induction hypothesis. This proves the claim.

8.5.3 BOUNDING Pt+, Pt- AND St- Lemma 8.8. The following holds
1. St-  Wt+(1)  Wt+(3)  for all t  1. 2. Pt+  Ut+(1)  Ut+(3)  for all t  1. 3. Pt-  Ut+(1)  Ut+(3)  for all t  1.

Proof. In Lemma 8.4 we showed that for all t  0 and j  Wt+(1)  Wt+(3) it holds that wt(j) · x2   . This proves the first claim. The second claim follows similarly. Without loss of generality, let j  Ut+(1). By Lemma 8.5 it holds that Ut+(1)  U0+(1)  U0+(3) for all t  t. Therefore, by Lemma 8.6 we have ut(j)x1 < , from which the claim follows.
For the third claim, without loss of generality, assume by contradiction that for j  Ut+(1) it holds that u(tj) · x2 > . Since ut(j) · x1 <  by Lemma 8.6, it follows that j  Ut+(2)  Ut+(4),
a contradiction. Therefore, u(tj) · x2   for all j  Ut+(1)  Ut+(3), from which the claim follows.

8.5.4 DYNAMICS OF St+ Lemma 8.9. The following holds:
1. If NWt (x+) <  and -NWt (x-) < 1, then St++1 = St+ +  Wt+(1)  Wt+(3) . 2. If NWt (x+)   and -NWt (x-) < 1, then St++1 = St+. 3. If NWt (x+) <  and -NWt (x-)  1, then St++1 = St+ +  Wt+(1)  Wt+(3) .

Proof.

1. The equality follows since for each i  {1, 3}, l  {2, 4} and j  Wt+(i)  Wt-(l) we have wt(+j)1 = w(tj) + xi - xl and Wt++1(1)  Wt++1(3) = Wt+(1)  Wt+(3) by
Lemma 8.4.

2. In this case for each i  {1, 3}, l  {2, 4} and j  Wt+(i)  Wt-(l) we have wt(+j)1 = wt(j) - xl and Wt++1(1)  Wt++1(3) = Wt+(1)  Wt+(3) by Lemma 8.4.

3. This equality follows since for each i  {1, 3}, l  {2, 4} and j  Wt+(i)  Wt-(l) we have w(t+j)1 = w(tj) + xi and Wt++1(1)  Wt++1(3) = Wt+(1)  Wt+(3) by Lemma 8.4.

16

Under review as a conference paper at ICLR 2019

8.5.5 UPPER BOUNDS ON NWt (x+), -NWt (x-) AND St+ Lemma 8.10. Assume that NWt (x+)   and -NWt (x-) < 1 for T  t < T + b where b  2. Then NWT+b (x+)  NWT (x+) - (b - 1)c +  W0+(2)  W0+(4) .

Proof. Define Rt+ = Yt+ - Zt+ where

Yt+ =

max  w(i) · x+1 , ...,  w(i) · xd+

j Wt+ (2)Wt+ (4)

and

Zt+ =

max  u(i) · x+1 , ...,  u(i) · xd+

j Ut+ (2)Ut+ (4)

Let l  {2, 4}, t = T and j  Ut++1(l). Then, either j  Ut+(2)  Ut+(4) or j  Ut+(1)  Ut+(3). In
the first case, ut(+j)1 = u(tj) + xl. Note that this implies that Ut+(2)  Ut+(4)  Ut++1(2)  Ut++1(4) (since xl will remain the maximal direction). Therefore,

max  u(t+j)1 · x1+ , ...,  u(t+j)1 · xd+

( ) ( )j Ut++1(2)Ut++1(4)

Ut+ (2)Ut+ (4)

- max  u(tj) · x+1 , ...,  u(t+j)1 · x+d
j Ut+ (2)Ut+ (4)

=  Ut++1(2)  Ut++1(4)

Ut+(2)  Ut+(4)

=  Ut+(2)  Ut+(4)

(13)

In the second case, where we have j  Ut+(1)  Ut+(3), it holds that ut(+j)1 = ut(j) + xl, j  Ut-(l) and ut(+j)1 · xl > . Furthermore, by Lemma 8.6, ut(j) · xi <  for i  {1, 3}. Note that by Lemma 8.6, any j1  Ut+(1)  Ut+(3) satisfies j1  Ut++1(2)  Ut++1(4). By all these observations, we have

max  u(t+j)1 · x1+ , ...,  ut(+j)1 · x+d

( ) ( )j Ut++1(2)Ut++1(4)

Ut+ (1)Ut+ (3)

- max  u(tj) · x1+ , ...,  ut(+j)1 · x+d
j Ut+ (1)Ut+ (3)
0

(14)

By Eq. 13 and Eq. 14, it follows that, Zt++1 + Pt++1  Zt++1  Zt+ + Pt+ +  Ut+(2)  Ut+(4) .

By induction we have Zt++b + Pt++b  Zt+ + Pt+ +

b-1 i=0



Ut++i(2)  Ut++i(4)

.

By Lemma 8.6

for any 1  i  b - 1 we have Ut++i(2)  Ut++i(4) = {1, ..., k}. Therefore, Zt++b + Pt++b 

Zt+ + Pt+ + (b - 1)c.

Now, assume that j  WT+(l) for l  {2, 4}. Then wT(j+) 1 = w(Tj) - xl. Thus either

max  wT(j+) 1 · x1+ , ...,  w(Tj+) 1 · xd+ - max  w(Tj) · x+1 , ...,  w(Tj) · x+d = -

in the case that j  WT++1(l), or

max  w(Tj+) 1 · x1+ , ...,  wT(j+) 1 · xd+  

if j / WT++1(l).

17

Under review as a conference paper at ICLR 2019

Applying these observations b times, we see that YT++b - YT+ is at most  WT++b(2)  WT++b(4) =  W0+(2)  W0+(4) where the equality follows by Lemma 8.4. By Lemma 8.9, we have ST++b = ST+.
Hence, we can conclude that NWT +b (x+) - NWT (x+) = ST++b + RT++b - PT++b - ST- - RT+ + PT+ = YT++b - ZT++b - PT++b - YT+ + ZT+ + PT+  -(b - 1)c +  W0+(2)  W0+(4)

Lemma 8.11. Assume that NWt (x+) <  and -NWt (x-)  1 for T  t < T + b where b  1. Then -NWT+b (x-)  -NWT (x-) - b W0+(2)  W0+(4) + c.

Proof. Define

Yt- =

max  w(i) · x1+ , ...,  w(i) · x+d

j Wt+ (2)Wt+ (4)

and
k

Zt- =

max  u(j) · x1+ , ...,  u(j) · xd+

j=1

First note that by Lemma 8.4 we have Wt++1(2)  Wt++1(4) = Wt+(2)  Wt+(4). Next, for any l  {2, 4} and j  Wt+(l) we have w(t+j)1 = wt(j) + xl. Therefore,
YT-+b  YT- + b WT+(2)  WT+(4) = YT- + b W0+(2)  W0+(4)
where the second equality follows by Lemma 8.4.

Assume that j  UT+(l) for l  {1, 3}. Then uT(j+) 1 = uT(j) - xl and

max  u(Tj+) 1 · x-1 , ...,  u(Tj+) 1 · x-d - max  uT(j) · x-1 , ...,  uT(j) · x-d = 0

(15)

To see this, note that by Lemma 8.6 and Lemma 8.5 it holds that uT(j) = u0(j) + aT xl where

aT  {-1, 0}. Hence, u(Tj+) 1 = u(0j) + aT +1xl where aT +1  {-1, 0}. Since

u0(j) · x2

<

 4

it

follows that u(Tj+) 1 · x2 = u(Tj) · x2 = u(0j) · x2 and thus Eq. 15 holds.

Now assume that j  UT+(l) for l  {2, 4}. Then max  u(Tj+) 1 · x-1 , ...,  uT(j+) 1 · x-d - max  uT(j) · x-1 , ...,  u(Tj) · x-d

= -

if l  {2, 4} and j  UT++1(l), or

max  uT(j+) 1 · x1- , ...,  uT(j+) 1 · xd-  

if l  {2, 4} and j / UT++1(l).

Applying these observations b times, we see that ZT-+b - ZT- is at most  UT++b(2)  UT++b(4) . Furthermore, for j  WT+(l), l  {1, 3}, it holds that wT(j+) 1 = wT(j) + xl. Therefore

max  wT(j+) 1 · x-1 , ...,  w(Tj+) 1 · xd- = max  wT(j) · x1- , ...,  w(Tj) · xd-

and since WT++1(1)  WT++1(3) = WT+(1)  WT+(3) by Lemma 8.4, we get ST-+b = ST-. Hence, we can conclude that
-NWT +b (x-) + NWT (x-) = -ST-+b - YT-+b + ZT-+b + ST- + YT- - ZT-  -b W0+(2)  W0+(4) +  UT++b(2)  UT++b(4)  -b W0+(2)  W0+(4) + c

18

Under review as a conference paper at ICLR 2019

Lemma 8.12. For all t, NWt (x+)   + 3c, -NWt (x-)  1 + 3c and St+   + 1 + 8c.
Proof. The claim holds for t = 0. Consider an iteration T . If NWT (x+) <  then NWT+1 (x+)  NWT (x+)+2k   +2c. Now assume that NWt (x+)   for T  t  T +b and NWT-1 (x+) < . By Lemma 8.10, it holds that NWT+b (x+)  NWT (x+) + k  NWT (x+) + c   + 3c, where the last inequality follows from the previous observation. Hence, NWt (x+)   + 3c for all t.
The proof of the second claim follows similarly. It holds that -NWT+1 (x-) < 1 + 2c if -NWT (x-) < 1. Otherwise if -NWt (x-)  1 for T  t  T + b and -NWT-1 (x-) < 1 then -NWT+b (x-)  1 + 3c by Lemma 8.11.
The third claim holds by the following identities and bounds NWT (x+) - NWT (x-) = ST+ - PT+ + PT- - ST-, PT-  0, PT+  c, ST-  c and NWT (x+) - NWT (x-)   + 1 + 6c by the previous claims.

8.5.6 OPTIMIZATION

We are now ready to prove a global optimality guarantee for gradient descent.



Proposition 8.13.

Let k

>

16 and 



1.

With

probabaility

at

least

1

-

 2k e8k

- 4e-8, after

T

=

7( +1+8c )

(

k 2

-2

k)

iterations,

gradient

descent

converges

to

a

global

minimum.



Proof.

First

note

that

with

probability

at

least

1-

 2k e8k

- 4e-8

the

claims

of

Lemma

8.1

and

Lemma 8.2 hold. Now, if gradient descent has not reached a global minimum at iteration t then

either NWt (x+) <  or -NWt (x-) < 1. If -NWt (x+) <  then by Lemma 8.9 it holds that

St++1  St+ +  W0+(1)  W0+(3)  St+ +

k

 -2 k

2



(16)

where the last inequality follows by Lemma 8.1. If NWt (x+)   and -NWt (x-) < 1 we have St++1 = St+ by Lemma 8.9. However, by Lemma 8.10, it follows that after 5 consecutive iterations t < t < t + 6 in which NWt (x+)   and -NWt (x-) < 1, we have NWt+6 (x+) < . To see this, first note that for all t, NWt (x+)  +3c by Lemma 8.12. Then, by Lemma 8.10 we have
NWt+6 (x+)  NWt (x+) - 5c +  W0+(2)  W0+(4)   + 3c - 5c + c <

where the second inequality follows by Lemma 8.1 and the last inequality by the assumption on k.

Assume by contradiction that GD has not converged to a global minimum after T

=

7( +1+8c )

(

k 2

-2

k)

iterations. Then, by the above observations, and the fact that S0+ > 0 with probability 1, we have

ST+  S0+ +

k

-

 2k

2

T 
7

>  + 1 + 8c

However, this contradicts Lemma 8.12.

8.5.7 GENERALIZATION ON POSITIVE CLASS

We will first need the following three lemmas.

Lemma 8.14. With probability at least 1 - 4e-8, it holds that

W0+(1)

-k 4

 2 k

19

Under review as a conference paper at ICLR 2019

and

W0+(3)

-k 4

 2 k

Proof. The proof is similar to the proof of Lemma 8.1.
Lemma 8.15. Assume that gradient descent converged to a global minimum at iteration T . Then there exists an iteration T2 < T for which St+   + 1 - 3c for all t  T2 and for all t < T2, -NWt (x-) < 1.
Proof. Assume that for all 0  t  T1 it holds that NWt (x+) <  and -NWt (x-) < 1. By continuing the calculation of Lemma 8.3 we have the following:

1. For i  {1, 3}, l  {2, 4}, j  W0+(i)  W0-(l), it holds that wT(j1) = w(0j) + T1xi -

1 2

(1

-

(-1)T1 )xl

.

2. For i  {2, 4} and j  W0+(i), it holds that wT(j1) = w(0j).

3. For i  {1, 3}, l  {2, 4}, j  U0+(i)  U0-(l), it holds that uT(j1) = u(0j) - xi + xl.

4. For i  {2, 4} and j  U0+(i), it holds that u(Tj1) = u0(j).

Therefore, there exists an iteration T1 such that NWT1 (x+)   and -NWT1 (x-) < 1 and for

a-shllNowtW<Tth2a(Ttxf1-o, r)NalWl 1Tt (.1xW+e)tc<<laimT2tahthnaedt

-NWt (x-) < for all T1  t 
following holds:

1. T2

Let T2 we have

 T be the first iteration such that NWT1 (x+)   - 2c. It suffices to

1. If NWt (x+)   then NWt+1 (x+)   - 2c. 2. If NWt (x+) <  then NWt+1 (x+)  NWt (x+).

The first claim follows since at any iteration NWt (x+) can decrease by at most 2k = 2c. For the second claim, let t < t be the latest iteration such that NWt (x+)  . Then at iteration t it holds that -NWt (x-) < 1 and NWt (x+)  . Therefore, for all i  {1, 3}, l  {2, 4} and

j  U0+(i)  U0+(l) it holds that u(tj+) 1 = u(tj) + xl. Hence, by Lemma 8.5 and Lemma 8.6 it holds

that Ut++1(1)  Ut++1(3) = . Therefore, by the gradient update in Eq. 10, for all 1  j  k, and

all t < t

 t we have ut(j)+1 = u(tj), which implies that NWt

(x+)
+1



NWt

(x+). For t

=t

we get NWt+1 (x+)  NWt (x+).

The above argument shows that NWT2 (x+)   - 2c and -NWT2 (x-)  1. Since NWT2 (x+) - NWT2 (x-) = ST+2 -PT+2 +PT-2 -ST-2 , PT-2 , ST-2  0 and PT-2  c it follows that ST+2  +1-3c. Finally, by Lemma 8.9 we have St+   + 1 - 3c for all t  T2.

Lemma 8.16. Let

Xt+ =

max  w(j) · x1+ , ...,  w(j) · x+d

j Wt+ (2)Wt+ (4)

and

Yt+ =

max  u(j) · x+1 , ...,  u(j) · xd+

j Ut+ (2)Ut+ (4)

Assume that k  64 and gradient descent converged to a global minimum at iteration T . Then, XT+  34c and YT+  1 + 38c.

20

Under review as a conference paper at ICLR 2019

Proof. Notice that by the gradient update in Eq. 9 and Lemma 8.2, Xt+ can be strictly larger than

max Xt+-1,  Wt+(2)  Wt+(4) only if NWt-1 (x+) <  and -NWt-1 (x-)  1. Furthermore,

in this case Xt+ - Xt+-1 =  Wt+(2)  Wt+(4) . By Lemma 8.9, St+ increases in this case by

 Wt+(1)  Wt+(3) . We know by Lemma 8.15 that there exists T2 < T such that ST+2  +1-3c

and that NWt (x+) <  and -NWt (x-)  1 only for t > T2. Since St+   + 1 + 8c for all t

by

Lemma

8.12,

there

can

only

be

at

most

11c
 |WT+ (1)WT+ (3)|

iterations

in

which

NWt (x+)

<



and

-NWt (x-)  1. It follows that

Xt+  

WT+(2)  WT+(4)

+

11c WT+(2)  WT+(1) 

 WT+(4) WT+(3)



 c + 11c

k 2

+

2

k



k 2

-

2

k

 34c

where the second inequality follows by Lemma 8.1 and the third inequality by the assumption on k.

At convergence we have NWT (x-) = ST- + XT+ - YT+ - PT-  -1 - 3c by Lemma 8.12 (recall that Rt- = Rt+ = Xt+ - Yt+). Furthermore, PT-  0 and by Lemma 8.8 we have ST-  c. Therefore, we get YT+  1 + 38c.

We are now ready to prove the main result of this section.

Proposition 8.17.

Define ()


=

.

-40

1 4

c

39c +1

Assume that 



2 and k



64

()+1 ()-1

2
.

Then

with

probability

at

least

1

-

 2k e8k

-

8e-8,

gradient

descent

converges

to

a

global

minimum

which

classifies all positive points correctly.



Proof.

With probability at least 1 -

128k k

- 8e-8

Proposition 8.13, and Lemma 8.14 hold.

It suf-

e 2

fices to show generalization on positive points. Assume that gradient descent converged to a global

minimum at iteration T . Let (z, 1) be a positive point. Then there exists zi  {(1, 1), (-1, -1)}.

Assume without loss of generality that zi = (-1, -1) = x3. Define

Xt+(i) =

max  w(j) · x+1 , ...,  w(j) · x+d

j WT+ (i)

for i  [4].

Yt+(i) =

max  u(j) · x1+ , ...,  u(j) · xd+

j UT+ (i)

Notice that

NWT (x+) = XT+(1) + XT+(3) - PT+ + RT+ = XT+(1) + XT+(3) - PT+ + RT- = XT+(1) + XT+(3) - PT+ + NWT (x-) - ST- + PT-
Since NWT (x+)  , -NWT (x-)  1, PT-  c by Lemma 8.8 and PT+, ST-  0 , we obtain

XT+(1) + XT+(3)   + 1 - c

(17)

Furthermore, by Lemma 8.7 we have

XT+(1) - X0+(1) WT+(1)

=

XT+(3) - X0+(3) WT+(3)

(18)

and by Lemma 8.14,



k 4

-2 k 



k 4

+

2

k

WT+(1) WT+(3)



k 4

+2 k 

k 4

-2

k

(19)

21

Under review as a conference paper at ICLR 2019



Let (k) =

k 4

+2k

k 4

-2

k

.

By

Lemma

8.2

we

have

X0+(1)



k 4



c 4

.

Combining this

fact

with Eq. 18

and Eq. 19 we get

XT+(1)



(k)XT+(3)

+

X0+(1)



(k)XT+(3)

+

c 4

which

implies

together

with

Eq.

17

that

XT+(3)





+1-

5c 4

1+(k)

.

Therefore,

NWT (z)  XT+(3) - PT+ - YT+(2) - YT+(4)





+

1

-

5c 4

1 + (k)

- c - 1 - 3(8c) - 14c

=



+

1

-

5c 4

1 + (k)

- 39c - 1 > 0

(20)

where the first inequality is true because

k
max  u(j) · z1 , ...,  u(j) · zd
j=1

k
 max  u(j) · x+1
j=1
= PT+ + YT+(2) + YT+(4)

, ..., 

u(j) · x+d
(21) (22)

The second inequality in Eq. 20 follows since PT+  c and by appyling Lemma 8.16. Finally, the last inequality in Eq. 20 follows by the assumption on k. 9 Hence, z is classified correctly.

8.5.8 GENERALIZATION ON NEGATIVE CLASS

We will need the following lemmas. Lemma 8.18. With probability at least 1 - 8e-8, it holds that

U0+(2)

-k 4

 2 k

U0+(4)

-k 4

 2 k

U0+(1)  U0+(3)

 U0-(2)

-k 4

 2 k

U0+(1)  U0+(3)

 U0-(4)

-k 4

 2 k

Proof. The proof is similar to the proof of Lemma 8.1 and follows from the fact that
P j  U0+(2) = P j  U0+(4) = P j  U0+(1)  U0+(3)  U0-(2) = P j  U0+(1)  U0+(3)  U0-(4) 1 = 4

9The inequality



+1-

5c 4

1+(k)

- 39c - 1

>

0 is equivalent to (k)

<

() which is equivalent to k

>

64

()+1 ()-1

2
.

22

Under review as a conference paper at ICLR 2019

Lemma 8.19. Let

Xt- =

max  ut(j) · x-1 , ...,  ut(j) · xd-

j U0+ (2)

and

Yt- =

max  u(tj) · x-1 , ...,  ut(j) · x-d

j U0+ (4)

Then for all t, there exists X, Y

 0 such that |X|  

U0+(2) , |Y |  

U0+(4)

and

Xt- -X
|U0+(2)|

=

Yt- -Y
|U0+(4)|

.

Proof. First, we will prove that for all t there exists at  Z such that for j1  U0-(2) and j2  U0-(4) it holds that u(tj1) = u0(j1) + atx2 and u(tj2) = u(0j2) - atx2. 10 We will prove this by induction on t.

For t = 0 this clearly holds. Assume it holds for an iteration t. Let j1  U0-(2) and j2  U0-(4).

By the induction hypothesis, there exists aT  Z such that ut(j1) = u0(j1) + atx2 and u(tj2) =

u0(j2) - atx2. Since for all 1  j  k it holds that u(0j) · x2

<

 4

,

it

follows

that

either

U0-(2)  Ut-(2) and U0-(4)  Ut-(4) or U0-(2)  Ut-(4) and U0-(4)  Ut-(2). In either case, by Eq. 10, we have the following update at iteration t + 1:

u(t+j11) = ut(j1) + ax2
and u(t+j21) = u(tj2) - ax2
where a  {-1, 0, 1}. Hence, ut(+j11) = u0(j1) + (at + a)x2 and ut(j2) = u(0j2) - (at + a)x2. This concludes the proof by induction.
Now, consider an iteration t, j1  U0+(2), j2  U0+(4) and the integer at defined above. If at  0 then

max  u(tj1) · x-1 , ...,  u(tj1) · x-d - max  u0(j1) · x-1 , ...,  u(0j1) · x-d = at

and

max  ut(j2) · x1- , ...,  ut(j2) · x-d - max  u(0j2) · x-1 , ...,  u0(j2) · x-d = at

Define X = X0- and Y = Y0- then |X|   U0-(2) , |Y |   U0-(4) and

Xt- - X U0-(2)

=

U0-(2) at U0-(2)

=

at

=

U0-(4) at U0-(4)

=

Yt- - Y U0-(4)

which proves the claim in the case that at  0.

If at < 0 it holds that

max  u(tj1) · x1- , ...,  u(tj1) · xd- and max  ut(j2) · x1- , ...,  ut(j2) · x-d

- max  u(0j1) - x2 · x1- , ...,  u(0j1) - x2 · xd- - max  u(0j2) + x2 · x-1 , ...,  u(0j2) + x2 · xd-

= (-at - 1) = (-at - 1)

Define 10Recall that by Lemma 8.5 we know that U0+(2)  U0+(4)  Ut+(2)  Ut+(4).

23

Under review as a conference paper at ICLR 2019

X = max 
j U0+ (2)

u(0j) - x2 · x-1 , ..., 

u0(j) - x2 · xd-

and

Y=

max  u(0j) + x2 · x1- , ...,  u0(j) + x2 · x-d

j U0+ (4)

Since for all 1  j  k it holds that

u(0j) · x2

<

 4

,

we

have

|X |





U0-(2) , |Y |  

U0-(4) .

Furthermore,

Xt- - X U0-(2)

=

U0-(2) (-at U0-(2)

-

1)

=

(-at

-

1)

=

U0-(4) (-at U0-(4)

- 1)

=

Yt- - Y U0-(4)

which concludes the proof.

Lemma 8.20. Let

Xt- =

max  ut(j) · x-1 , ...,  u(tj) · x-d

j (U0+ (1)U0+ (3))U0- (2)

and

Yt- =

max  ut(j) · x1- , ..., 

j (U0+ (1)U0+ (3))U0- (4)

Then for all t,

Xt- -X0-
|(U0+ (1)U0+ (3))U0- (2)|

=

|(U0+

Yt- -Yt-
)(1)U0+(3) U0-

(4)|

.

ut(j) · x-d

Proof. We will first prove that for all t there exists an integer at  0 such that for j1  U0+(1)  U0+(3)  U0-(2) and j2  U0+(1)  U0+(3)  U0-(4) it holds that u(tj1) · x2 = u0(j1) · x2 + at and u(tj2) · x4 = u0(j2) · x4 + at. We will prove this by induction on t.

For t = 0 this clearly holds. Assume it holds for an iteration t. Let j1  U0+(1)  U0+(3)  U0-(2)

and j2  U0+(1)  U0+(3)  U0-(4). By the induction hypothesis, there exists an integer at  0

such that u(tj1) · x2 = u(0j1) · x2 + at and u(tj2) · x4 = u(0j2) · x4 + at. Since for all 1  j  k it

holds that

u(0j) · x1

<

 4

,

it

follows

that

if

at



1

we

have

the

following

update

at

iteration

T

+

1:

ut(+j11) = u(tj1) + ax2

and ut(+j21) = ut(j2) + ax4
where a  {-1, 0, 1}. Hence, u(t+j11)·x2 = u0(j1)·x2+(at+a) and u(t+j21)·x4 = u(0j2)·x4+(at+a).

Otherwise, if at = 0 then

ut(+j11) = ut(j1) + ax2 + b1x1

and u(t+j21) = ut(j2) + ax4 + b2x1

such that a  {0, 1} and b1, b2  {-1, 0, 1}. Hence, u(t+j11) · x2 = u0(j1) · x2 + (at + a) and u(t+j21) · x4 = u0(j2) · x4 + (at + a). This concludes the proof by induction.

Now, consider an iteration t, j1  U0+(1)  U0+(3)  U0-(2) and j2  U0+(1)  U0+(3)  U0-(4) and the integer at defined above. We have,

max  ut(j1) · x1- , ...,  ut(j1) · xd- - max  u0(j1) · x1- , ...,  u(0j1) · x-d = at

24

Under review as a conference paper at ICLR 2019

and

max  u(tj2) · x1- , ...,  ut(j2) · x-d - max  u(0j2) · x-1 , ...,  u0(j2) · x-d

It follows that

Xt- - X0- U0+(1)  U0+(3)  U0-(2)

=

U0+(1)  U0+(3)  U0-(2) at U0+(1)  U0+(3)  U0-(2)

= at

=

U0+(1)  U0+(3)  U0-(4) at U0+(1)  U0+(3)  U0-(4)

=

Yt- - Y0- U0+(1)  U0+(3)  U0-(4)

= at

which concludes the proof.

We are now ready to prove the main result of this section.

Proposition 8.21.


Define 

=

.1-36

1 4

c

35c

Assume that k

>

64

+1 -1

2
. Then with probability at

least

1

-

 2k e8k

-

8e-8

,

gradient

descent

converges

to

a

global

minimum

which

classifies

all

negative

points correctly.



Proof.

With

probability

at

least

1

-

 2k e8k

- 16e-8 Proposition 8.13 and Lemma 8.18 hold.

It

suffices to show generalization on negative points. Assume that gradient descent converged to a

global minimum at iteration T . Let (z, -1) be a negative point. Assume without loss of generality

that zi = x2 for all 1  i  d. Define the following sums for l  {2, 4},

Xt- =

max  w(j) · x-1 , ...,  w(j) · x-d

j Wt+ (2)Wt+ (4)

Yt-(l) =

max  ut(j) · x1- , ...,  u(tj) · x-d

j U0+ (l)

Zt-(l) =

max  u(j) · x1- , ...,  u(j) · xd-

j (U0+ (1)U0+ (3))U0- (l)

First, we notice that

NWT (x-) = ST- + XT- - YT-(2) - YT-(4) - ZT-(2) - ZT-(4)

and imply that

XT-, ST-  0 NWT (x-)  -1 YT-(2) + YT-(4) + ZT-(2) + ZT-(4)  1

(23)

We note that by the analysis in Lemma 8.18, it holds that for any t, j1  U0+(2) and j2  U0+(4), either j1  Ut+(2) and j2  Ut+(4), or j1  Ut+(4) and j2  Ut+(2). We assume without loss of generality that j1  UT+(2) and j2  UT+(4). It follows that in this case NWT (z)  ST- + XT- - ZT-(2) - YT-(2). 11Otherwise we would replace YT-(2) with YT-(4) and vice versa and continue with the same proof.

11The fact that we can omit the term -ZT-(4) from the latter inequality follows from Lemma 8.6.

25

Under review as a conference paper at ICLR 2019



Let (k) =

.k
4

+2k

k 4

-2

k

By Lemma 8.20 and Lemma 8.18

ZT-(4)



(k)ZT-(2)

+

Z0-(2)



(k)ZT-(2)

+

c 4

and by Lemma 8.19 and Lemma 8.18 there exists Y  c such that:

YT-(4)  (k)YT-(2) + Y  (k)YT-(2) + c

Plugging these inequalities in Eq. 23 we get:

(k)ZT-(2) +

c 4

+ (k)YT-(2)

+ c

+ YT-(2) + ZT-(2)



1

which implies that

YT-(2)

+

ZT-(2)



1

-

5c 4

(k) + 1

By Lemma 8.16 we have XT-  34c. Hence, by using the inequality ST-  c we conclude that

NWT (z)



ST-

+

XT-

-

ZT-(2) - YT-(2)



35c

-

1

-

5c 4

(k) + 1

<

0

2

where the last inequality holds for k > 64

+1 -1

. 12 Therefore, z is classified correctly.

8.5.9 FINISHING THE PROOF



First,

for

k



120,

with

probability

at

least

1

-

 2k e8k

- 16e-8,

Proposition

8.13,

Lemma

8.14

and

Lemma

8.18

hold.

Also,

for

the

bound

on

T,

note

that

in

this

case

28( +1+8c ) c



7(
(k 2

+1+8c )
-2 k)

.

Define 1

=

-40

1 4

c

39c +1

and 2

=

1-36

1 4

c

35c

and let 

=

max{1, 2}.

For 



8 and c



1 410

it

2

holds that 64

+1 -1

< 120. By Proposition 8.17 and Proposition 8.21, it follows that for k  120

gradient descent converges to a global minimum which classifies all points correctly.

We will now prove pattern detection results. In the case of over-paramterized networks, in Propo-

sition 8.17 we proved that X+(1), X+(3) 

.

+1-

5c 4

1+(k)

Since for i  {1, 3} it holds that

Dxi  X+(i), it follows that patterns x1 and x3 are detected. Similarly, in Proposition 8.21

our

analysis

implies

that,

without

loss

of

generality,

YT-(2) + ZT-(2), YT-(4) + ZT-(4)



.1-

5c 4

(k)+1

Since, for l  {2, 4}, Dxl  YT-(l) + ZT-(l) (under the assumption that we assumed without loss

of generality), it follows that patterns x2 and x4 are detected. The confidence of the detection is at

least

.1-

5c 4

(k)+1

8.6 PROOF OF THEOREM 5.3

1. We refer to Eq. 16 in the proof of Proposition 8.13. To show convergence and provide
convergence rates of gradient descent, the proof uses Lemma 8.1. However, to only show convergence, it suffices to bound the probability that W0+(1)  W0+(3) =  and that the initialization satisfies Lemma 8.2. Given that Lemma 8.2 holds (with probability at least

1-

8 

e-32

),

then

W0+(1)



W0+(3)

=



holds

with

probability

3 4

.

The

first

part

of

the

theorem follows.

12It

holds

that

35c

-

1-

5c 4

(k)+1

<

0

if

and

only

if

(k)

<



which

holds

if

and

only

if

k

>

64

+1 -1

2
.

26

Under review as a conference paper at ICLR 2019

2. By the previous section, with probability at least (p+p-)m 1 -

8 

e-32

3 4

all

training

points are diverse, Lemma 8.2 holds with k = 2 and W0+(1)  W0+(3) =  which implies that gradient descent converges to a global minimum. For the rest of the proof we will

condition on the corresponding event. Let T be the iteration in which gradient descent

converges to a global minimum. Note that T is a random variable. Denote the network at

iteration T by N . For all z  R2d denote

2

N (z) =

max  w(j) · z1 , ...,  w(j) · zd

j=1

- max  u(j) · z1 , ...,  u(j) · zd

Let E denote the event for which at least one of the following holds:

(a) WT+(1) = . (b) WT+(3) = .
(c) u(1) · x2 > 0 and u(2) · x2 > 0. (d) u(1) · x4 > 0 and u(2) · x4 > 0.

Our proof will proceed as follows. We will first show that if E occurs then gradient descent

does not learn f , i.e., the network N does not satisfy sign (N (x)) = f (x) for all x 

{±1}2d.

Then,

we

will

show

that

P [E]



11 12

.

This

will

conclude

the

proof.

Assume that one of the first two items in the definition of the event E occurs. Without

loss of generality assume that WT+(1) =  and recall that x- denotes a negative vector

which only contains the patterns x2, x4 and let z+  R2d be a positive vector which only contains the patterns x1, x2, x4. By the assumption WT+(1) =  and the fact that x1 = -x3 it follows that for all j = 1, 2,

max  w(j) · z+1 , ...,  w(j) · z+d = max  w(j) · x-1 , ...,  w(j) · x-d

Furthermore, since z+ contains more distinct patterns than x-, it follows that for all j = 1, 2,

max  u(j) · z1+ , ...,  u(j) · z+d  max  u(j) · x-1 , ...,  u(j) · x-d

Hence, N (z+)  N (x-). Since at a global minimum N (x-)  -1, we have N (z+)  -1 and z2 is not classified correctly.

Now assume without loss of generality that the third item in the definition of E occurs. Let

z- be the negative vector with all of its patterns equal to x4. It is clear that N (z-)  0 and therefore z- is not classified correctly. This concludes the first part of the proof. We

will

now

proceed

to

show

that

P [E]



11 12

.

Denote by Ai the event that item i in the definition of E occurs and for an event A denote by Ac its complement. Thus Ec = i4=1Aic and P [Ec] = P [Ac3  Ac4 | Ac1  A2c ] P [A1c  Ac2].

We will first calculate P [A1c  A2c]. By Lemma 8.4, we know that for i  {1, 3}, W0+(i) =

WT+(i). Therefore, it suffices to calculate the probabilty that W0+(1) =  and W0+(3) = ,

provided that W0+(1)  W0+(3) = . Without conditioning on W0+(1)  W0+(3) = , for

each

1



i



4

and

1



j



2

the

event

that

j



W0+(i)

holds

with

probability

1 4

.

Since

the

initializations

of

the

filters

are

independent,

we

have

P [A1c



A2c ]

=

1 6

.

13

We will show that P [Ac3  Ac4 | A1c  Ac2]

=

1 2

by

a

symmetry

argument.

This

will finish the proof of the theorem. For the proof, it will be more convenient

to denote the matrix of weights at iteration t as a tuple of 4 vectors, i.e., Wt =

w(01), w0(2), u0(1), u(02) . Consider two initializations W0(1) = w(01), w0(2), u(01), u0(2)

and W0(2) = w(01), w(02), -u0(1), u(02) and let Wt(1) and Wt(2) be the corresponding

weight values at iteration t. We will prove the following lemma:

13Note that this holds after conditioning on the corresponding event of Lemma 8.2.

27

Under review as a conference paper at ICLR 2019

Lemma 8.22. For all t  0, if Wt(1) = w(t1), wt(2), -u(t1), ut(2) .

w(t1), w(t2), u(t1), u(t2) then Wt(2) =

Proof. We will show this by induction on t. 14This holds by definition for t = 0. Assume it holds for an iteration t. Denote Wt(+2)1 = (z1, z2, v1, v2). We need to show that z1 = wt(+1)1, z2 = wt(+2)1, v1 = -u(t+1)1 and v2 = ut(+2)1. By the induction hypothesis it holds that NWt(1) (x+) = NWt(2) (x+) and NWt(1) (x-) = NWt(2) (x-). This follows since for diverse points (either positive or negative), negating a neuron does not change the function value. Thus, according to Eq. 9 and Eq. 10 we have z1 = w(t+1)1, z2 = wt(+2)1 and v2 = ut(+2)1. We are left to show that v1 = -ut(+1)1. This follows from Eq. 10 and the following facts:
(a) x3 = -x1.
(b) x2 = -x4.
(c) arg max1l4 u · xl = 1 if and only if arg max1l4 -u · xl = 3. (d) arg max1l4 u · xl = 2 if and only if arg max1l4 -u · xl = 4. (e) arg maxl{2,4} u · xl = 2 if and only if arg maxl{2,4} -u · xl = 4.
To see this, we will illustrate this through one case, the other cases are similar. Assume, for example, that arg max1l4 ut(1) · xl = 3 and arg maxl{2,4} u(t1) · xl = 2 and assume without loss of generality that NWt(1) (x+) = NWt(2) (x+) <  and NWt(1) (x-) = NWt(2) (x-) > -1. Then, by Eq. 10, u(t+1)1 = u(t1) - x3 + x2. By the induction hypothesis and the above facts it follows that v1 = -u(t1) - x1 + x4 = -ut(1) + x3 - x2 = -u(t+1)1. This concludes the proof.

Consider an initialization of gradient descent where w(01) and w(02) are fixed and the event that we conditioned on in the beginning of the proof and A1c  Ac2 hold. Define the set B1 to be the set of all pair of vectors (v1, v2) such that if u(01) = v1 and u0(1) = v2 then at iteration T , u(1) · x2 > 0 and u(2) · x2 > 0. Note that this definition implicitly implies
that this initialization satisfies the condition in Lemma 8.2 and leads to a global minimum.

Similarly, let B2 be the set of all pair of vectors (v1, v2) such that if u(01) = v1 and

u0(1) = v2 then at iteration T , u(1) · x4 > 0 and u(2) · x2 > 0. First, if (v1, v2)  B1 then

(-v1, v2) satisfies the conditions of Lemma 8.2. Second, by Lemma 8.22, it follows that

if (v1, v2)  and NWt (x-

B1 ) in

then initializating with (-v all iterations 0  t  T .

1, v2), leads to the same values of NWt Therefore, initializing with (-v1, v2)

(x+) leads

to a convergence to a global minimum with the same value of T as the initialization with

(v1, v2). Furthermore, if (v1, v2)  B1, then by Lemma 8.22, initializing with u(01) = -v1 and u(01) = v2 results in u(1) · x2 < 0 and u(2) · x2 > 0. It follows that (v1, v2)  B1 if and only if (-v1, v2)  B2.

For l1, l2  {2, 4} define Pl1,l2 = P u(1) · xl1 > 0  u(2) · xl2 > 0 | Ac1  A2c , w(01), w(02)

Then, by symmetry of the initialization and the latter arguments it follows that P2,2 = P4,2.

By similar arguments we can obtain the equalities P2,2 = P4,2 = P4,4 = P2,4.

Since

all

of

these

four

probabilities

sum

to

1,

each

is

equal

to

1 4

.

15Taking

expectations

of

these probabilities with respect to the values of w(01) and w0(2) (given that Lemma 8.2 and

A1c  A2c hold) and using the law of total expectation, we conclude that

P [A3c  A4c | A1c  A2c ] = P u(1) · x4 > 0  u(2) · x2 > 0 | Ac1  A2c

+P

u(1) · x2 > 0  u(2) · x4 > 0 | A1c  Ac2

1 =
2

14Recall that we condition on the event corresponding to Lemma 8.2. By negating a weight vector we still satisfy the bounds in the lemma and therefore the claim that will follow will hold under this conditioning.
15Note that the probablity that u(i) · xj = 0 is 0 for all possible i and j.

28

Under review as a conference paper at ICLR 2019

Finally, we show results for detection of a pattern. To see this, we will show that if one of the four conditions of the event E defined above is met, then for cd > 2c, the network does not detect all patterns. If the one of the last two conditions hold, then this is true even for cd  0. Now, assume without loss of generality that WT+(1) = ). In this case by Lemma 8.4 and Lemma 8.2, it follows that

Dx1 =

 w(Ti) · x1

j WT+ (2)WT+ (4)

 2c

and therefore, x1 cannot be detected with confidence greater than 2c.

3. Let z1 be a positive point which contains only the patterns x1, x2, x4, z2 be a positive

point which contains only the patterns x3, x2, x4. Let z3 be a negative point with all

patterns equal to x2 and z4 be a negative point with all patterns equal to x4. In the proof

of the previous section, if the event E holds, then gradient descent converges to a solution

at iteration T which errs on one of the points zi, 1  i  4. Let D be a probability

distribution where the probability for a positive point is 0.5. Furthermore, the probability

for

each

z1

and

z2

is

1-p+ 4

and

the

probability

for

each

z3

and

z4

is

1-p- 4

.

In

this

case,

if the event E holds, then gradient descent will have test error at least min

1-p+ 4

,

1-p- 4

,

which concludes the proof.

8.7 PROOF OF THEOREM 4.1

Let   1 - p+p-(1 - c - 16e-8). By Theorem 5.2, given 2 samples, one positive and one negative, with probability at least 1 -   p+p-(1 - c - 16e-8), gradient descent will converge to a global minimum that has 0 test error. Therefore, for all  0, m( , )  2. On the other hand, by Theorem

5.3,

there

exists

distributions

such

that

if

m

<

( )2 log

48 33(1-c)

log(p+ p- )

then

with

probability

greater

than

( )log

48 33(1-c)

33

(p p )+ - log(p+p-)

(1 - c) =  48

gradient descent converges to a global minimum with test error at least min

follows that for 0 

< min

1-p+ 4

,

1-p- 4

, m(

, )



( )2 log

48 33(1-c)

.log(p+ p- )

1-p+ 4

,

1-p- 4

. It

9 XOR

In this section we assume that we are given a training set S  {±1}2 × {±1}2 consisting of
points (x1, 1), (x2, -1), (x3, 1), (x4, -1), where x1 = (1, 1), x2 = (-1, 1), x3 = (-1, -1) and x4 = (1, -1). Our goal is to learn the XOR function with gradient descent.

Note that in the case of two dimensions, the convolutional network introduced in Section 3 reduces to the following two-layer fully connected network.

k

NWt (x) =

 wt(i) · x -  ut(i) · x

i=1

We consider initialization

running gradient descent with a constant with mean 0 and standard deviation g

le=arn1i6nckg3/r2a.teW eascksu,mce

1 that

and IID gaussian gradient descent

minimizes the hinge loss

(W ) =

max{1 - yNW (x), 0}

(x,y)S

where optimization is only over the first layer. We will show that gradient descent converges to the global minimum in a constant number of iterations.

For each point xi  S define the following sets of neurons: Wt+(i) = j | w(tj) · xi > 0

29

Under review as a conference paper at ICLR 2019

Wt-(i) = j | w(tj) · xi < 0 Ut+(i) = j | u(tj) · xi > 0 Ut+(i) = j | ut(j) · xi < 0 Lemma 9.1. For i  {1, 3}, if j  W0+(i) then j  Wt+(i) for all t > 0 and if j  W0-(i) then j  Wt-(i) for all t > 0. Similarly, for i  {2, 4}, if j  Ut+(i) then j  Ut+(i) for all t > 0 and if j  Wt-(i) then j  Ut-(i) for all t > 0.

Proof. Without loss of generality it suffices to prove the claim for Wt+(1). This follows by symmetry and the fact that j  Wt+(1) if and only if j  Wt-(3). The proof is by induc-

tion. Assume that j  Wti,+. The gradient of w(i) with respect to a point (x, y) is given

1by

(W ) (x,y)
 w(i)

=

-y (w(i) · x)x

yNW (x)<1.

Therefore, by the facts w(tj) · x3 < 0 and

x1 · x2, x1 · x4 = 0 it follows that wt(+j)1 · x1 > 0, which concludes the proof.

Lemma 9.2. With probability at least 1 - 8e-8, for all 1  j  4

k

-

 2k



2

W0+(j)

,

U0+(j)



k +2 k
2

Proof. Without loss of generality consider W0+(1) . Since the sign of a one dimensional Gaussian random variable is a Bernoulli random variable, we get by Hoeffding's inequality

P

W0+(1)

-k 2

 <2 k

 2e-

2(22 k

k)

= 2e-8

Since W0+(1) + W0+(3) = k with probability 1, we get that if

W0+(1)

-

k 2

W0+(3)

-

k 2

<2

k. The result now follows by symmetry and the union bound.

 < 2 k then

For each point xi, define the following sums:

St+(i) =

 w(tj) · xi

j Wt+ (i)

St-(i) =

 w(tj) · xi

j Wt- (i)

Rt+(i) =

 u(tj) · xi

j Ut+ (i)

Rt-(i) =

 ut(j) · xi

j Ut- (i)

We will prove the following lemma regarding St+(1), St-(1), Rt+(1), Rt-(1) for i = 1. By symme-

try, analogous lemmas follow for i = 1.



Lemma 9.3.

The following holds with probability  1 -

 2k e8k

:

1. For all t  0, Rt+(1) + Rt-(1)  k.

2. For all t  0, St-(1) = 0.

3.

Let t



0.

If -yNWt (x1)

<

1, then St++1(1)



St+(1) +

Wt+(1)

 4

.

Otherwise, if

-yNWt (x1)  1 then St++1(1) = St+(1).

30

Under review as a conference paper at ICLR 2019

Proof.

1. For t = 0 the claim holds by Lemma 8.2. Assume by contradiction that there exists t > 0, such that Rt+(1) + Rt-(1) > k. It follows that, without loss of generality, there exists j  Ut+(1) such that  ut(j) · x1 > . In each iteration u(tj) · x1 can increase by
at most , therefore by Lemma 8.2 there exists 0  t < t such that 0 <  u(tj) · x1  
and  ut(j+) 1 · x1 > . However, in this case ut(j+) 1 = ut(j) + x1 + x2 + x4 where   0. Therefore, ut(j+) 1 · x1 < u(tj) · x1  , a contradiction.

2. This is a direct consequence of Lemma 9.1.

3.

By Lemma 9.1, if j

 Wt+(1) and -yNWt (x1) < 1 then w(t+j)1

=

w(tj ) +

 4

x1 + x2 + x4 ,

from which the first part of the claim follows. The second claim follows similarly.



Proposition 9.4.

Assume that k



25.

With probability 

1-

 2k e8k

- 8e-8, for all i, if until

iteration

T

there

were

at

least

l



k k-2 k

+ 10

iterations,

in

which

-yNWt (xi)

<

1,

then

it

holds

that -yNWt (xi)  1 for all t  T .

Proof. Without loss of generality assume that i = 1. By Lemma 9.3 and Lemma 8.2, with probabil-

ity



1

-

 2k e8k

-

8e-8,

if

-yNWt (x1)

<

1

then

St++1(1)



St+(1)

+

k 2

-

2

k

. Therefore,

by Lemma 9.3, for all t  T

NWt (x) = St+(1) + St-(1) - Rt+(1) + Rt-(1)



k

-

 2k

l - k

2

1

where the last ineqaulity follows by the assumption on l.



Theorem 9.5.

Assume that k



25.

With probability 

1

-

 2k e8k

- 8e-8, after at most

4k k-2 k

+ 40

iterations, gradient descent converges to a global minimum.

Proof.

Proposition

9.4

implies

that

there

are

at

most

4k k-2 k

+ 40

iterations

in

which

there

exists

(xi, yi) such that -yiNWt (xi) < 1. After at most that many iterations, gradient descent converges

to a global minimum.

10 VC DIMENSION

As noted in Remark 3.1, the VC dimension of the model we consider is at most 15. To see this, we

first define for any z  {±1}2d the set Pz  {±1}2 which contains all the distinct two dimensional

binary patterns that z has. For example, for a positive diverse point z it holds that Pz = {±1}2.

Now, for any points z(1), z(2)  {±1}2d such that Pz(1) = Pz(2) and for any filter w  R2 it holds

that maxj  w · zj(1)

= maxj 

w

·

z

(2) j

.

Therefore, for any W , NW (z(1)) = NW (z(2)).

Specifically, this implies that if both z(1) and z(2) are diverse then NW (z(1)) = NW (z(2)). Since there are 15 non-empty subsets of {±1}2, it follows that for any k the network can shatter a set of

at most 15 points, or equivalently, its VC dimension is at most 15. Despite these expressive power

limitations, there is a generalization gap between small and large networks in this setting, as can be

seen in Figure 1.

31

