Under review as a conference paper at ICLR 2019
A PRIORI ESTIMATES OF THE GENERALIZATION ERROR FOR TWO-LAYER NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
New estimates for the generalization error are established for a nonlinear regression problem using a two-layer neural network model. These new estimates are a priori in nature in the sense that the bounds depend only on some norms of the underlying functions to be fitted, not the parameters in the model. In contrast, most existing results for neural networks are a posteriori in nature in the sense that the bounds depend on some norms of the model parameters. The error rates are comparable to that of the Monte Carlo method in terms of the size of the dataset. Moreover, these bounds are equally effective in the over-parametrized regime when the network size is much larger than the size of the dataset.
1 INTRODUCTION
One of the most important theoretical challenges in machine learning comes from the fact that classical learning theory cannot explain the effectiveness of over-parametrized models in which the number of parameters is much larger than the size of the training set. This is especially the case for neural network models, which have achieved remarkable performance for a wide variety of problems (Bahdanau et al., 2015; Krizhevsky et al., 2012; Silver et al., 2016). Therefore, understanding the mechanism behind these successes is critical, which requires developing new analytical tools that can work effectively in the over-parametrized regime (Zhang et al., 2017). Our work is partly motivated by the situation in classical approximation theory and finite element analysis. There are two kinds of error bounds in finite element analysis depending on whether the target solution (the ground truth) or the numerical solution enters into the bounds. Let f  and f^n be the true solution and the "numerical solution", respectively. In "a priori" error estimates, only norms of the true solution enter into the bounds, namely
f^n - f  1  C f  2.
In "a posteriori" error estimates, the norms of the numerical solution enter into the bounds:
f^n - f  1  C f^n 3.
Here · 1, · 2, · 3 denote various norms. In this language, most recent theoretical efforts (Neyshabur et al., 2015; Bartlett et al., 2017; Golowich et al., 2018; Neyshabur et al., 2017; 2018a;b) on estimating the generalization error of neural networks should be viewed as "a posteriori" analysis, since all the bounds depend on some norms of the solutions. Unfortunately, as observed in Arora et al. (2018) and Neyshabur et al. (2018b), the numerical values of these norms are always huge, yielding vacuous estimates. In this paper we pursue a different line of attack by providing "a priori" analysis. For this purpose, a suitably regularized two-layer network is considered. It is proved that the generalization error of regularized solutions is asymptotically sharp with constants depending only on the properties of the target function. Numerical experiments show that these a priori bounds are non-vacuous (Dziugaite & Roy, 2017) for datasets of practical interests, such as MNIST and CIFAR-10. In addition, our experimental results also suggest that such regularization terms are necessary in order for the model to be "well-posed" (see Section 6 for the precise meaning).
1

Under review as a conference paper at ICLR 2019

1.1 SETUP

We will focus on the regression problem. Let f  :   R be the target function, with  = [-1, 1]d,

and S = {(xi, yi)}in=1 denotes the training set. underlying distribution  with supp()  , and yi

H=erfe{(xxii)}+ni=1i

are i.i.d , with i

samples drawn being the noise.

from an Our aim

is to recover f  by fitting S using a two-layer fully connected neural network with ReLU (rectified

linear units) activation:

m

f (x; ) = ak(bk · x + ck),

(1)

k=1

where (t) = max(0, t) is the ReLU function, bk  Rd, and  = {(ak, bk, ck)}mk=1 represents all the parameters to be learned from the training data. m denotes the network width. To control the

complexity of networks, we use the following scale-invariant norm.

Definition 1 (Path norm). For a two-layer ReLU network (1), the path norm is defined as

m
 P = |ak|( bk 1 + |ck|).
k=1

Definition 2 (Spectral norm). Given f  L2(), denote F  L2(Rd) as an extension of f to Rd. Let F^ denote the Fourier transform of F , then f (x) = Rd ei x, F^()d  x  . We define the spectral norm of f as follows

(f ) =

inf

F L2(Rd),F |=f |

Rd

 12|F^()| d.

(2)

Assumption 1 (Target function). Following Breiman (1993) and Klusowski & Barron (2016), we

consider target functions that have finite spectral norm. By defining

Fs := L2()  f (x) :   R (f ) < , f   1 ,

(3)

We assume that f   Fs.

Assumption 2 (Noise). We assume the noise has zero mean, and its probability distribution has an

exponentially decaying tail, i.e.,

E[] = 0,

P[||

>

t]



c0

e-

t2 

 t  0.

(4)

Here c0, 0 and  are constants.

The ultimate aim is to minimize the generalization error (expected risk) L() = Ex,y[(f (x; )-y)2].

In

practice,

we only

have at

our disposal

the empirical

risk L^()

=

1 n

ni=1(f (xi; ) - yi)2. The

generalization gap is defined as the difference between expected and empirical risk. We also define

the

truncated

risks

by

LB ()

=

Ex,y[(f (x; ) - y)2

 B2], L^B()

=

1 n

n i=1

(f

(xi;

)

-

yi)2



B2.

2 PRELIMINARY

In this section, we summarize some results on the approximation error and generalization bound for two-layer ReLU networks, whose proofs are deferred to Appendix A and B. These results are required by our subsequent a priori analysis.

2.1 APPROXIMATION PROPERTIES

Most of the content is adapted from Barron (1993); Breiman (1993) and Klusowski & Barron (2016). Proposition 1. For any f  Fs, it has an integral representation as follows

f (x) - f (0) - x · f (0) = v

h(x; z, t, )dp(z, t, ),

{-1,1}×[0,1]×Rd

where v < 2(f ) and

s(z, t, ) = - sign cos(  1t - zb()) h(x; z, t, ) = s(z, t, ) (zx · /  1 - t)+ .

2

Under review as a conference paper at ICLR 2019

For simplicity, in the rest of this paper, we assume f (0) = 0, f (0) = 0. We take m samples Tm =

{(z1, t1, 1), . . . , (zm, tm, m)} with (zi, ti, i) randomly drawn from p(z, t, ), and consider the

empirical average f^m(x)

=

v m

m k=1

h(x;

zi, ti,

i),

which

is

exactly

a

two-layer

ReLU

network

of width m. The central limit theorem (CLT) tells us that the approximation error is roughly

E(z,t,)

[h(x; z,

t,

)]

-

1 m

m

h(x; zk, tk, k) 

k=1

Var(z,t,)[h(x; z, t, )] . m

So as long as we can bound the variance at the right-hand side, we will have an estimate of the approximation error. The following result formalizes this intuition.

Theorem 2. For any distribution  with supp()   and any f  Fs, there exists a two-layer network f (x; ~) of width m such that

Ex|f (x) -

f (x; ~)|2



162(f ) .
m

Furthermore ~ P  4(f ), which means that the path norm of ~ can be bounded by the spectral norm of the target function.

2.2 ESTIMATING THE GENERALIZATION GAP

Definition 3 (Rademacher complexity). Let H be a hypothesis space, i.e. a set of functions.

The Rademacher complexity of H with respect to samples S = (z1, z2, . . . , zn) is defined as

R^(H)

=

1 n

E

[suphH

n i=1

h(zi

)i

],

where

{i}ni=1

are

independently

random

variables

with

P(i

=

+1)

=

P(i

=

-1)

=

1 2

.

The generalization gap can be estimated via Rademacher complexity by the following theorem (see Bartlett & Mendelson (2002) and Shalev-Shwartz & Ben-David (2014) ).

Theorem 3. Fix a hypothesis space H, and suppose that for any h  H and z, |h(z)|  c. Then for any  > 0, with probability at least 1 -  over the choice of S = (z1, z2, . . . , zn), we have

sup
hH

|1 n

n i=1

h(zi)

-

Ez [h(z )]|



2ES [R^ (H)]

+

c

2 log(2/) .
n

About the Rademacher complexity of two-layer networks (1), we have the following result.
Lemma 1. Let z = (x, y) and h(z; ) = (f (x; ), y). Consider all the two-layer networks with path norm bounded by Q, i.e. HQ := {h(z; ) |  P  Q}. If loss function (y, y^) is L-Lipschitz continuous with respect to y, then we have

R^(HQ)  QL

2 log(2d) n

Applying Theorem 3 and Lemma 1 gives us the following generalization bound.

Theorem 4 (A posterior generalization bound). For any  > 0, with probability at least 1 -  over the choice of the training set S, we have for any two-layer network f (x; ), the following result
holds:

|LB() - L^B()|  4B(  P + 1)

where c =

 k=1

1/k2.

2 log(2d) + B2 n

2 log(2c(1 +  P )2/) , n

(5)

3 MAIN RESULTS
We see that the path norm of the special solution ~ which achieves the optimal approximation error is independent of the network size, and this norm can also be used to bound the generalization gap (Theorem 4). Therefore, if the path norm is suitably penalized during training, we should be able to control the generalization gap without harming the approximation accuracy. One possible implementation of this idea is through the structural empirical risk minimization (Vapnik, 1998) as follows.

3

Under review as a conference paper at ICLR 2019

Definition 4 (Path-norm regularized estimator). Let the path-norm regularized risk defined as

J() := L^Bn () + Bn

2 log(2d) n (1 +  P ),

(6)

where Bn = 2 + max{0,  log n} and  is a positive constant. The condition on  will be given below. The path-norm regularized estimator is defined as

^n = argmin J().

(7)

It is worth noting that the minimizer is not necessarily unique, and ^n should be understood as any of the minimizers. About this estimator, we have the following result.

Theorem 5 (Main Result). Under Assumption 1 and 2, there exists a constant C depending only on , c0 such that for any  > 0 and   4, with probability at least 1 -  over the choice of the training set S, the generalization error of estimator (7) satisfies

E|f (x;

^n)

-

f (x)|2



C

2(f ) m

+

C

Bn2 n

^(f )

log(2d) +

log(nc/) .

(8)

Here ^(f ) = max{(f ), 1}.

Since Bn depends on the magnitude of noise, we actually prove



E|f (x; ^n)

-

f (x)|2

=

O( 

1 m

)

+

O

O( 

1 m

)

+

O

log d+log n n

log2(n)

log d+log n n

if 



C log n

if 

>

C log

n

,

where C is a constant. This means that the noise introduces at most an extra logarithmic term. Moreover, if the probability distribution function of the noise decays sufficiently fast (for example, bounded noise  = 0), the logarithmic term can even be eliminated.
Remark 1. It should be noted that both terms at the right hand side of the above result has a Monte Carlo nature, as can be seen later in the proof. From this viewpoint, the result is quite sharp. The dimensional dependence is mainly reflected in the norm (f ) (see Barron (1993)).

Comparison with existing results Klusowski & Barron (2016) analyzed a similar problem. However they require the network width m to be the orders of poly(n). In contrast, our results allow the network width to be arbitrarily large. See Table 1 for the detailed comparison between our results and theirs.

noise Our results Results of Klusowski & Barron (2016)

zero

1 m

+

log d+log n n
1/3 log d
n

1/2

sub-Gaussian

1 m

+

log2(n)

log d+log n n

1/4 log d

n

1/2

Table 1: Comparison between our work and Klusowski & Barron (2016).

4 PROOF OF MAIN RESULTS

4.1 NOISELESS CASE

For this case, we provide a rather short and intuitive sketch of proof, which helps to clarify the main

idea of the complete proof in the next section. In the noiseless case,  = 0, 0 = 0, thus Bn = 2. The solution ~ constructed in Theorem 2 satisfies L(~) = O(m-1) and ~ P = O(1). According
to Theorem 4, we have L^(~) = O(m-1) + O~(n-1/2) 1. Hence the corresponding regularized risk

satisfies

J(~) = O(m-1) + O~(n-1/2).

1Asymptotic notation O~(·) is similar to O(·) but with logarithmic terms ignored.

4

Under review as a conference paper at ICLR 2019

By comparing the minimizer ^n with ~, we have J(^n)  J(~) = O(m-1) + O~(n-1/2).
Furthermore ^n P  O~(n1/2m-1). By Theorem 4, we have

L(^n)  L^(^n) + 4 As long as   4, we have

2 log(2d) (
n

^n

P + 1) + O~

n-1/2

log( ^n P ) .

(9)

L(^n)  J(^n) + O~ n-1/2 log( ^n P ) = O(m-1) + O~(n-1/2).

We thus complete the proof.
This analysis highly relies on the fact that the approximation error and generalization gap can be controlled by the path norm simultaneously.

4.2 NOISY CASE

In the presence of noise, the expected risk can be decomposed into three terms Ex,y|f (x; ) - y|2 = Ex,|f (x; ) - f (x) - |2 = Ex,|f (x; ) - f (x)|2 + 2Ex,[(f (x; ) - f (x))] + E[2].
Since  is independent of x and E[] = 0, we have L() = Ex|f (x; ) - f (x)|2 + E[2].
This suggests that, in spite of noise, we still have argmin L() = argmin Ex|f (x; ) - f (x)|2,
and the latter is what we really want to minimize.

(10)

We first need to address the issue that L^() - L() can be arbitrarily large, due to the presence of the noise. Let us consider the truncated risk LB() = E[(f (x; ) - y)2  B2], which has the following
property, whose proof is deferred to Appendix C.

Lemma 2. Under Assumption 2, we have

sup |L()


-

LBn ()|



2c02 , n

By triangle inequality, we have |L()|



|L()-LBn ()|+|LBn ()|

=

2c0  2 n

+|L^ Bn

()|.

Therefore

this lemma tell us that as long as we can control the truncated risk, then the original risk will be

controlled accordingly.

Proposition 6. Let ~ be the solution constructed in Theorem 2. Then with probability at least 1 - ,

we have

J(~)



L(~)

+

2c02 n

+

Bn2 n

^(f ) 3 + 5( + 4)

log(2d)

+

log(2c/) ,

where ^(f ) = max{(f ), 1}.

Proof. According to Definition 4 and the property that ~ P  4(f ), the regularized cost of ~ must satisfies

J(~) = L^Bn (~) + Bn

2 log(2d) (
n

~

P

+ 1)

(1)
 LBn (~) + (4 + )Bn

2 log(2d) (
n

~

P

+ 1) + Bn2

2 log(2c(1 + ~ P )2/) n

(2)


L(~)

+

2c02 n

+

(

+

4)Bn

2

log(2d) n

(4

(f

)

+

1)

+

Bn2

2 log(2c(1 + 4(f ))2/) ,
n (11)

5

Under review as a conference paper at ICLR 2019

uwshinerge(1a),+(2b)folloaw+the bThaneodrleomg(41

and Lemma 2, respectively. + a)  a for a  0, b  0.

The last term So we have

can

be

simplified

by

2 log(2c(1 + 4(f ))2/)  2 log(2c/) + 4 log(1 + 4(f ))

 2 log(2c/) + 4 (f )

 2 log(2c/) + 4^(f ),

where ^(f ) = max((f ), 1). By plugging it into Equation (11), and using ^(f )  1, Bn  1, we have

J(~)



L(~)

+

2c02 n

+

Bn2 n

^(f ) 3 + 5( + 4)

log(2d)

+

log(2c/) .

Proposition 7 (Properties of the regularized estimator). The path-norm regularized estimator ^n satisfies:

J(^n)  J(~)

^n P  -1Bn-1

2

n log(2d)

J(~)

Proof. The first claim follows from the definition of ^n. For the second claim, we have



log(2d) n

^n P  J(^n)  J(~), so

^n P  -1Bn-1

2

n log(2d)

J(~).

Remark 2. The above proposition establishes the connection between the regularized solution and

the special solution ~ constructed in Theorem 2. According to Proposition 6, we can conclude

that the upper bound of the generalization gap satisfies

^n P n

= O(L(~)) + O~((f )n-1/2) 

O(L(~)), as n  . It suggests that our regularization term is added appropriately, which forces

the generalization gap to be roughly in the same order of approximation error.

Proof of Theorem 5. We are now ready to prove our main theorem. Let C1 = 2c02. Lemma 2

implies

that

L(^n)



LBn (^n)

+

C1 n

.

Then

we

have

(1)
L(^n)  L^Bn (^n) + 4Bn( ^n P + 1)

2

log(2d) n

+

Bn2

log(2c(1 + ^n P )2/) + C1 nn

(2)
 J(^n) + Bn2

log(2c(1 + ^n P )2/) + C1 nn

Where (1) follows from the a posteriori generalization bound in Theorem 4, and (3) is due to   4. Furthermore,

log(2c(1 + ^n P )2/)  log(2nc/) + 2 log(1 + n-1/2 ^n P )

 log(2nc/) + 2n-1/2 ^n P .

By plugging it back and simplifying the right hand side according to Proposition 6 and Proposition 7,

we conclude that there exists a constant C2 such that

L(^n)



L(~)

+

C2

Bn2 n

^(f )

log(2d) +

log(nc/) .

By applying the decomposition that L() = E|f (x; )-f (x)|2+E[2], and the result of Theorem 2,

we obtain

E|f (x;

^m)

-

f (x)|2



C

2(f ) m

+

C

Bn2 n

^(f )

log(2d) +

log(nc/) .

Here the constant C depends only on , c0.
Remark 3. From the proof, we can see that the requirement of   4 is due to constant 4 appears in the upper bound of the generalization gap. If we have a sharper generalization bound, then  could be set smaller.

6

Under review as a conference paper at ICLR 2019

5 NUMERICAL EXPERIMENTS

We evaluate the properties of the regularized estimator on both MNIST2 (LeCun et al., 1998) and CIFAR-103 (Krizhevsky & Hinton, 2009) datasets. Each example in MNIST is a 28 × 28 grayscale image, while each example in CIFAR-10 is a 32 × 32 × 3 color image. To be consistent with our
setup in theoretical analysis, we restrict ourselves to a binary classification problem. For MNIST, we map numbers {0, 1, 2, 3, 4} to label 0 and {5, 6, 7, 8, 9} to 1. For CIFAR-10, we select the examples with labels 0 and 1 to construct our new training and validation sets. Thus, our new MNIST has
60, 000 training examples, and CIFAR-10 has 10, 000 training examples. The mean squared error
rather than cross entropy is used as our loss function.

Following the standard strategy (He et al., 2015), the two-layer ReLU network is initialized using

ai



N (0,

2 m

),

bi,j



N (0, 2/d),

ci

=

0.

We

use



=

1

and

train

the

models

using

the

Adam

optimizer (Kingma & Ba, 2015) for T = 10, 000 steps, unless it is specified otherwise. The initial

learning rate is set to be 0.001, and it is then multiplied by a decay factor of 0.1 at 0.7T and again

at 0.9T . We set the trade-off parameter  = 0.1 for regularized models. Although the theoretical

results suggest   4, we find in practice usually a smaller  can achieve better test performance.

5.1 THE NON-VACUOUS UPPER BOUND OF THE GENERALIZATION GAP

Theorem 4 shows that the generalization gap can be bounded by

 P n

up

to some constants.

To see

how this works in practice, we trained both regularized models with  = 0.1 and un-regularized

models ( = 0) for fixed network width m =10,000. To cover the over-parameterization regime,

we also consider n = 100 where m/n = 100 1. The results are summarized in Table 2.

Table 2: Comparison of regularized ( = 0.1) and un-regularized ( = 0) models. The experiments are repeated for 5 times, and the mean values are reported.

dataset 

n

training accuracy test accuracy

 P n

0 CIFAR-10
0.1

104
100 104
100

100% 100% 87.4% 91.0%

84.5% 70.5% 86.9% 72.0%

58 507 0.14 0.43

MNIST

0

6 × 104 100

0.1

6 × 104 100

100% 100% 98.1% 100%

98.8% 78.7% 97.8% 74.9%

58 162 0.27 0.41

MNIST
60  = 0.1 =0
40

CIFAR-10 60
=0
 = 0.1
40

  P/ n
  P/ n

20 20

0 102

103 104
network width: m

0 102

103
network width: m

104

Figure 1: Comparison of path norms between regularized and un-regularized solutions for varying widths.

As we can see, the test accuracies of regularized and un-regularized solutions are generally compa-

rable, but the upper bounds of generalization gap

 P n

are dramatically different.

Specifically, for

un-regularized models, the bounds are always vacuous, since they are several orders of magnitude

larger than the naively upper bound 1. This is consistent with the observations in Arora et al. (2018)

2http://yann.lecun.com/exdb/mnist/ 3https://www.cs.toronto.edu/~kriz/cifar.html

7

Under review as a conference paper at ICLR 2019

and Neyshabur et al. (2018b). However, for regularized models, the bounds are non-vacuous, although they are still far from the true values. These numerical observations are consistent with our theoretical prediction in Proposition 7.

To further explore the impact of over-parameterization, we trained various models with different

widths. For both datasets, all the training examples are used. In Figure 1, we display how the upper

bound

 P n

of the learned solution varies with the network width.

We find that this quantity for

the regularized model is almost constant, whereas for the original model it increases with network

width. This provides numerical evidence that our theoretical results hold for the network with an

arbitrary width.

5.2 DEPENDENCE ON THE INITIALIZATION
Since the neural network model is non-convex, it is interesting to see how the initialization affects the performance of the solutions, especially in the over-parametrized regime. To this end, we fix m = 10000, n = 100 and vary the variance of random initialization . The results are reported in Figure 2. In general, we find that regularized models are much more stable than the un-regularized models. For large initialization, the regularized model always performs significantly better.

Test accuracy(%) Test accuracy(%)

MNIST 80

70

60
50 10-2

=0  = 0.1

10-1

100

101

initialization: 

102

70 65 60 55 50
10-2

CIFAR-10

=0  = 0.1

10-1

100

101

initialization: 

Figure 2: Test accuracies of solutions obtained from different initializations. Each experiment is repeated for 5 times, and we report the mean and standard deviation.

6 CONCLUDING REMARKS
The most unsatisfactory aspect of our result is that it is proved for the regularized model. In practice it is uncommon to add explicit regularizations. Instead, practitioners rely on the so-called "implicit regularization" (Zhang et al., 2017; Neyshabur, 2017). At the moment it is unclear where the "implicit regularization" comes from and how it actually works. But there are overwhelming evidence that by tuning extensively the details of the optimization procedure, including the algorithm, the initialization, the hyper-parameters, etc., one can find solutions with superior performance on the test data. The disadvantage is that excessive tuning and serious experience is required to find good solutions. Until we have a good understanding about the mysteries surrounding implicit regularization, the business of parameter tuning will remain an art. In contrast, the regularized model is rather robust and much more fool-proof. Borrowing the terminology from mathematical physics, we consider the regularized model to be "well-posed" and the original model to be "ill-posed" .
There are two clear paths moving forward. One is to study other regularized models. In fact to avoid the slight loss of test accuracy shown for the MNIST dataset in Figure 1, one can consider regularizations that vanish for small values of the path norm. Our main results should hold for this kind of regularizations. The other is to study the so-called "implicit regularization". Recently, assuming that the data is well-separated, Brutzkus et al. (2018); Li & Liang (2018) proved that for two-layer networks, the number of iterations required for SGD to achieve certain accuracy for the classification problem is independent of the network size. Implicit regularization has also been studied in other problems, such as logistic regression (Soudry et al., 2018) and matrix factorization (Li et al., 2018; Gunasekar et al., 2017).
8

Under review as a conference paper at ICLR 2019
REFERENCES
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 254­263. PMLR, 10­15 Jul 2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015.
Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930­945, 1993.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems 30, pp. 6240­6249. 2017.
Leo Breiman. Hinging hyperplanes for regression, classification, and function approximation. IEEE Transactions on Information Theory, 39(3):999­1013, 1993.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. In International Conference on Learning Representations, 2018.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In Proceedings of the 31st Conference On Learning Theory, volume 75, pp. 297­299. PMLR, 2018.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, pp. 6151­6159, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1026­1034, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
Jason M Klusowski and Andrew R Barron. Risk bounds for high-dimensional ridge function combinations including neural networks. arXiv preprint arXiv:1607.01434, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, 2018.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pp. 2­47, 2018.
9

Under review as a conference paper at ICLR 2019
Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953, 2017.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pp. 1376­1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems 30, pp. 5949­5958. 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018a.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards understanding the role of over-parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076, 2018b.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In International Conference on Learning Representations, 2018.
Vladimir N. Vapnik. Statistical learning theory, volume 1. Wiley New York, 1998. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.
10

Under review as a conference paper at ICLR 2019

A PROOFS FOR APPROXIMATION PROPERTIES

Proof of Proposition 1 By an abuse of notation, let f be its own L2 extension in Rd. Since f  L2(Rd), f (x) - x · f (0) - f (0) can be written as

ei·x - i · x - 1 f^()d.
Rd
Note that the following identity

(12)

c
- (z - s)+eis + (-z - s)+e-is ds = eiz - iz - 1
0
holds when |z|  c. Choosing c =  1, z =  · x, we have

|z|   1 x   c.

Let s =  1t, 0  t  1, and ^ = /  1, we have

1

-



2 1

(^ · x - t)+ei  1t + (-^ · x - t)+e-i  1t dt = ei·x - i · x - 1.

0

Let f^() = eib()|f^()|, inserting (13) into (12) yields

(13)

where

1

f (x) - x · f (0) - f (0) =

g(t, )dtd,

Rd 0

g(t, ) = -



2 1

||f^()|

(^ · x - t)+ cos(



1t + b()) + (-^ · x - t)+ cos(



1t - b())

.

Consider a density on {0, 1} × [0, 1] × Rd defined by

p(z, t, ) = |f^()|



2 1

|

cos

(



1t - zb()) |/v

where the normalized constant v is given by

1

v=

|f^()|



2 1

| cos (



1t + b()) | + | cos (



1t - b()) |

ddt.

Rd 0

Since f belongs to Fs, so we have

v  2(f ) < +,

therefore the density p(z, t, ) is well-defined. To simplify the notations, we let

Then we have

s(z, t, ) = -sign cos(  1t - zb()) h(x; z, t, ) = s(z, t, ) (z^ · x - t)+ .

(14)
(15) (16) (17) (18)

f (x) - x · f (0) - f (0) = v

h(x; z, t, )dp(z, t, ).

{-1,1}×[0,B]×Rd

Since x = (x)+ - (-x)+, we obtain

(19)

f (x) = f (0) + (x · f (0))+ - (-x · f (0))+ + v

h(x; z, t, )dp(z, t, ).

{-1,1}×[0,B]×Rd

Therefore f  H¯.

11

Under review as a conference paper at ICLR 2019

Proof of Theorem 2

Let f^m(x)

=

v m

m k=1

h(x;

zi,

ti

,

i)

be

the

Monte-Carlo

estimator,

we

have

ETm Ex|f (x) - f^m(x)|2 = ExETm |f (x) - f^m(x)|2

v2 = m Ex

E(z,t,)[h2(x; z, t, )] - f 2(x)



v2 m

Ex

E(z,t,)[h2(x;

z,

t,

)]

Furthermore, for any fixed x, the variance can be upper bounded since

E(z,t,)[h2(x; z, t, )]  E(z,t,) (z^ · x - t)+2  E(z,t,) (|^ · x| + t)2  4.

Hence we have

ETm Ex|f (x) - f^m(x)|2



4v2 m



162(f ) m

Therefore there must exist a set of Tm, such that the corresponding empirical average satisfies

Ex|f

-

fm|2



162(f ) .
m

Due to the special structure of the Monte-Carlo estimator, we have |ak| = v/m, bk 1 = 1, |ck|  1. It follows Equation (16) that ~ P  2v  4(f ).

B PROOFS FOR GENERALIZATION BOUNDS

Before to provide the upper bound for the Rademacher complexity of two-layer networks, we first need the following two lemmas.

Lemma 3 (Lemma 26.11 of Shalev-Shwartz & Ben-David (2014)). Let S = (x1, . . . , xn) be n vectors in Rd. Then the Rademacher complexity of H1 = {x  u · x | u 1  1} has the following upper bound,

R^(H1)  max xi 
i

2 log(2d) n

The above lemma characterizes the Rademacher complexity of a linear predictor with 1 norm bounded by 1. To handle the influence of nonlinear activation function, we need the following contraction lemma.
Lemma 4 (Lemma 26.9 of Shalev-Shwartz & Ben-David (2014)). Let i : R  R be a -Lipschitz function, i.e. for all ,   R we have |i() - i()|  | - |. For any a  Rn, let (a) = (1(a1), . . . , n(an)), then we have
R^(  H)  R^(H)

We are now ready to characterize the Rademacher complexity of two-layer networks. We use the path norm to control the complexity of the network.
Lemma 5. Let FQ = {fm(x; ) |  P  Q} be the set of two-layer networks with path norm bounded by Q, then we have

R^(FQ)  Q

2 log(2d) n

12

Under review as a conference paper at ICLR 2019

Proof. To simplify the proof, we let ck = 0, otherwise we can define bk = (bTk , ck)T and x = (xT , 1)T .

nm

nR^(FQ) = E sup

i ak bk 1(b^kT xi)

 P Q i=1 k=1

nm

 E

sup

i ak bk 1(ukT xi)

 P Q, uk 1=1 i=1 k=1

mn

= E

sup

ak bk 1 i(uTk xi)

 P Q, uk 1=1 k=1

i=1

mn

 E sup

|ak bk 1| sup | i(uT xi)|

 P Q k=1

u 1=1 i=1

nn

 QE sup | i(uT xi)|  QE sup | i(uT xi)|

u 1=1 i=1

u 11 i=1

n

= QE sup

i(uT xi)

u 11 i=1

Since  is a 1-Lipschitz continuous, by applying Lemma 4 and Lemma 3, we obtain

R^(FQ)  Q

2 log(2d) .
n

Proof of Lemma 1 Since for any yi, (y, yi) = (y - yi)2  B2 is 2B-Lipschitz continuous, by applying the contraction property of Rademacher complexity and Lemma 5, for HQ = {  f | f  FQ} we have

R^(HQ)  2BQ

2 log(2d) .
n

Directly applying Theorem 3 yields the result.

Proposition 8. For the truncated risk, we have, with probability at least 1 - ,

sup |G^B()|  4BQ
 P Q

2 log(2d) + B2 n

2 log(2/) n

(20)

Proof of Theorem 4 Consider the decomposition F = l=1Fl, where Fl = {fm(x; ) |  P 

l}.

Let

l

=

 c l2

where

c

=

 l=1

1 l2

.

According

to

Theorem

8,

if

we

fixed

l

in

advance,

then

with

probability at least 1 - l over the choice of S,

sup |Gn()|  4Bl
 P l

2 log(2d) + B2 n

2 log(2/l) . n

So the probability that there exists at least one l such that (B) fails is at most

 l=1

l

=

.

In

other

words, with probability at least 1 - , the inequality (B) holds for all l.

Given an arbitrary set of parameters , denote l0 = min{l |  P  l}, then l0   P + 1. Equation (B) implies that

|Gn()|  4Bl0

2 log(2d) + B2 n

2 log(2cl02/) n

 4B(  P + 1)

2 log(2d) + B2 n

2 log(2c(1 +  P )2/) . n

13

Under review as a conference paper at ICLR 2019

C THE POOF OF LEMMA 2

Proof. Let Z = fm(x; ) - f (x) - , then for any B  2 + 0, we have

|L() - LB()| = E (Z2 - B2)1|Z|B



= P{Z2 - B2  t2}dt2  P{|Z| 

00 

 P{||  B2 + t2 - 2}dt2

0
= c0



e-

s2 22

ds2

=

2c02



e-sds

B B2/22

= 2c02e-B2/22

B2 + t2}dt2

Since Bn



2

+

max{0,

2

log

n},

we

have

2c02e-

Bn2 22

 2c02n-1/2. Therefore,

sup |L()


-

LBn ()|]



2c02 . n

14

