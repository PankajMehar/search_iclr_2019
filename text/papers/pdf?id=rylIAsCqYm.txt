Under review as a conference paper at ICLR 2019

A2BCD: Asynchronous Acceleration with Optimal Complexity
Anonymous authors Paper under double-blind review

Abstract
In this paper, we propose the Asynchronous Accelerated Nonuniform Randomized Block Coordinate Descent algorithm (A2BCD). We prove A2BCD converges linearly to a solution of the convex minimization problem at the same rate as NU_ACDM, so long as the maximum delay is not too large. This is the first asynchronous Nesterov-accelerated algorithm that attains any provable speedup. Moreover, we then prove that these algorithms both have optimal complexity. Asynchronous algorithms complete much faster iterations, and A2BCD has optimal complexity. Hence we observe in experiments that A2BCD is the top-performing coordinate descent algorithm, converging up to 4 - 5× faster than NU_ACDM on some data sets in terms of wall-clock time. To motivate our theory and proof techniques, we also derive and analyze a continuous-time analogue of our algorithm and prove it converges at the same rate.

1 Introduction

In this paper, we propose and prove the convergence of the Asynchronous Accelerated Nonuniform Randomized Block Coordinate Descent algorithm (A2BCD), the first asynchronous Nesterovaccelerated algorithm that achieves optimal complexity. No previous attempts have been able to prove a speedup for asynchronous Nesterov acceleration. We aim to find the minimizer x of the unconstrained minimization problem:

min f (x) = f x(1), . . . , x(n)
xRd

(1.1)

where f is -strongly convex for  > 0 with L-Lipschitz gradient f = (1f, . . . , nf ). x  Rd is composed of coordinate blocks x(1), . . . , x(n). The coordinate blocks of the gradient if are assumed
Li-Lipschitz with respect to the ith block. That is, x, h  Rd:

if (x + Pih) - if (x)  Li h

(1.2)

where Pi is the projection onto the ith block of Rd. Let L¯

1 n

n i=1

Li

be

the

average

block

Lipschitz constant. These conditions on f are assumed throughout this whole paper. Our algorithm

can also be applied to non-strongly convex objectives ( = 0) or non-smooth objectives using the

black box reduction techniques proposed in Allen-Zhu & Hazan (2016). Hence we consider only

the coordinate smooth, strongly-convex case. Our algorithm can also be applied to the convex

regularized ERM problem via the standard dual transformation (see for instance Lin et al. (2014)):

1 f (x) =
n

n

 fi( ai, x ) + 2

x2

i=1

(1.3)

1

Under review as a conference paper at ICLR 2019

Hence A2BCD can be used as an asynchronous Nesterov-accelerated finite-sum algorithm.

Coordinate descent methods, in which a chosen coordinate block ik is updated at every iteration, are a popular way to solve equation 1.1. Randomized block coordinate descent (RBCD, Nesterov
(2012)) updates a uniformly randomly chosen coordinate block ik with a gradient-descent-like step: xk+1 = xk - (1/Lik )ik f (xk). This algorithm decreases the error E(f (xk) - f (x)) to (f (x0) - f (x)) in K( ) = O(n(L¯/) ln(1/ )) iterations.

Using a series of averaging and extrapolation steps, accelerated RBCD Nesterov (2012) improves

RBCD's iteration complexity K( ) to O(n L¯/ ln(1/ )), which leads to much faster convergence

when

L¯ 

is

large.

This

rate

is

optimal

when

all

Li

are equal Lan

&

Zhou

(2015).

Finally,

using

a special probability distribution for the random block index ik, the non-uniform accelerated

coordinate

to O(

n i=1

descent Li/

method ln(1/ )),

Allen-Zhu which can

et al. (2015) be up to n

(NU_ACDM) can further decrease the times faster than accelerated RBCD,

complexity since some

Li can be significantly smaller than L. NU_ACDM is the current state-of-the-art coordinate descent

algorithm for solving equation 1.1.

Our A2BCD algorithm generalizes NU_ACDM to the asynchronous-parallel case. We solve equation 1.1 with a collection of p computing nodes that continually read a shared-access solution vector y into local memory then compute a block gradient if , which is used to update shared solution vectors (x, y, v). Proving convergence in the asynchronous case requires extensive new technical machinery.

A traditional synchronous-parallel implementation is organized into rounds of computation: Every computing node must complete an update in order for the next iteration to begin. However, this synchronization process can be extremely costly, since the lateness of a single node can halt the entire system. This becomes increasingly problematic with scale, as differences in node computing speeds, load balancing, random network delays, and bandwidth constraints mean that a synchronous-parallel solver may spend more time waiting than computing a solution.

Computing nodes in an asynchronous solver do not wait for others to complete and share their updates before starting the next iteration. They simply continue to update the solution vectors with the most recent information available, without any central coordination. This eliminates costly idle time, meaning that asynchronous algorithms can be much faster than traditional ones, since they have much faster iterations. For instance, random network delays cause asynchronous algorithms to complete iterations (ln(p)) time faster than synchronous algorithms at scale. This and other factors that influence the speed of iterations are discussed in Hannah & Yin (2017a). However, since many iterations may occur between the time that a node reads the solution vector, and the time that its computed update is applied, effectively the solution vector is being updated with outdated information. At iteration k, the block gradient ik f is computed at a delayed iterate y^k defined as1:

y^k = y(k-j(k,1)), . . . , y(k-j(k,n))

(1.4)

for delay parameters j(k, 1), . . . , j(k, n)  N. Here j(k, i) denotes how many iterations out of date
coordinate block i is at iteration k. Different blocks may be out of date by different amounts, which is known as an inconsistent read. We assume2 that j(k, i)   for some constant  < .

Asynchronous algorithms were proposed in Chazan & Miranker (1969) to solve linear systems. General convergence results and theory were developed later in Bertsekas (1983); Bertsekas & Tsitsiklis (1997); Tseng et al. (1990); Luo & Tseng (1992; 1993); Tseng (1991) for partially and totally

1Every coordinate can be outdated by a different amount without significantly changing the proofs. 2This condition can be relaxed however by techniques in Hannah & Yin (2017b); Sun et al. (2017); Peng
et al. (2016c); Hannah & Yin (2017a)

2

Under review as a conference paper at ICLR 2019

1.1 Summary of Contributions

asynchronous systems, with essentially-cyclic block sequence ik. More recently, there has been renewed interest in asynchronous algorithms with random block coordinate updates. Linear and sublinear convergence results were proven for asynchronous RBCD Liu & Wright (2015); Avron et al. (2014), and similar was proven for asynchronous SGD Recht et al. (2011), and variance reduction algorithms Reddi et al. (2015); Leblond et al. (2017); Mania et al. (2015); Huo & Huang (2016), and primal-dual algorithms Combettes & Eckstein (2018). Further related work is discussed in Section 4.

1.1 Summary of Contributions

In this paper, we prove that A2BCD attains NU_ACDM's state-of-the-art iteration complexity to highest order for solving equation 1.1, so long as delays are not too large (see Section 2). The proof is very different from that of Allen-Zhu et al. (2015), and involves significant technical innovations and complexity related to the analysis of asynchronicity.

We also prove that A2BCD (and hence NU_ACDM) has optimal complexity to within a constant factor over a fairly general class of randomized block coordinate descent algorithms (see Section 2.1). This extends results in Lan & Zhou (2015) to asynchronous algorithms with Li not all equal. Since asynchronous algorithms complete faster iterations, and A2BCD has optimal complexity, we expect A2BCD to be faster than all existing coordinate descent algorithms. We confirm with numerical experiments that A2BCD is the current fastest coordinate descent algorithm (see Section 5).

We are only aware of one previous and one contemporaneous attempt at proving convergence results
for asynchronous Nesterov-accelerated algorithms. However, the first is not accelerated and relies
on extreme assumptions, and the second obtains no speedup. Therefore, we claim that our results
are the first-ever analysis of asynchronous Nesterov-accelerated algorithms that attains a speedup. Moreover, our speedup is optimal for delays not too large3.

The work of Meng et al. claims to obtain square-root speedup for an asynchronous accelerated SVRG.

In the case where all component functions have the same Lipschitz constant L, the complexity they

obtain reduces to (n + ) ln(1/ ) for  = O  n2 (Corollary 4.4). Hence authors do not even obtain

accelerated

rates.

Their

convergence

condition

is



<

1 41/8

for

sparsity

parameter

.

Since

the

dimension

d

satisfies

d



1 

,

they

require

d



216 8.

So



=

20

requires

dimension

d

>

1015.

In a contemporaneous preprint, authors in Fang et al. (2018) skillfully devised accelerated schemes for asynchronous coordinate descent and SVRG using momentum compensation techniques. Although their complexity results have the improved  dependence on the condition number, they do not prove any speedup. Their complexity is  times larger than the serial complexity. Since  is necessarily greater than p, their results imply that adding more computing nodes will increase running time. The authors claim that they can extend their results to linear speedup for asynchronous, accelerated SVRG under sparsity assumptions. And while we think this is quite likely, they have not yet provided proof.

We also derive a second-order ordinary differential equation (ODE), which is the continuous-time limit of A2BCD (see Section 3). This extends the ODE found in Su et al. (2014) to an asynchronous accelerated algorithm minimizing a strongly convex function. We prove this ODE linearly converges to a solution with the same rate as A2BCD's, without needing to resort to the restarting techniques. The ODE analysis motivates and clarifies the our proof strategy of the main result.

3Speedup is defined precisely in Section 2
3

Under review as a conference paper at ICLR 2019

2 Main results

We should consider functions f where it is efficient to calculate blocks of the gradient, so that
coordinate-wise parallelization is efficient. That is, the function should be "coordinate friendly"
Peng et al. (2016b). This is a very wide class that includes regularized linear regression, logistic regression, etc. The L2-regularized empirical risk minimization problem is not coordinate friendly in
general, however the equivalent dual problem is, and hence can be solved efficiently by A2BCD (see Lin et al. (2014), and Section 5).

To calculate the k + 1'th iteration of the algorithm from iteration k, we use only one block of the gradient ik f . We assume that the delays j(k, i) are independent of the block sequence ik, but otherwise arbitrary (This is a standard assumption found in the vast majority of papers, but can be
relaxed Sun et al. (2017); Leblond et al. (2017); Cannelli et al. (2017)).

Definition 1. Asynchronous Accelerated Randomized Block Coordinate Descent (A2BCD). Let f be -strongly convex, and let its gradient f be L-Lipschitz with block coor-
dinate Lipschitz parameters Li as in equation 1.2. We define the condition number  = L/, and let L = mini Li. Using these parameters, we sample ik in an independent and identically distributed (IID) fashion according to

P[ik = j] = L1j/2/S, j  {1, . . . , n}, for S =

n i=1

L1i /2

.

(2.1)

Let  be the maximum asynchronous delay. We define the dimensionless asynchronicity parameter , which is proportional to  , and quantifies how strongly asynchronicity will affect convergence:

 = 9 S-1/2L-1/2L3/41/4 × 

(2.2)

We use the above system parameters and  to define the coefficients , , and  via eqs. (2.3) to (2.5). Hence A2BCD algorithm is defined via the iterations: eqs. (2.6) to (2.8).

 1 + (1 + )-1/2S -1  1 - (1 - )1/2S-1 h 1 - 1 1/2L-1/2.
2

(2.3) (2.4) (2.5)

yk = vk + (1 - )xk, xk+1 = yk - hLi-k1ik f (y^k), vk+1 = vk + (1 - )yk - -1/2L-ik1/2ik f (y^k).

(2.6) (2.7) (2.8)

See Section A for a discussion of why it is practical and natural to have the gradient ik f (y^k) to be outdated, while the actual variables xk, yk, vk can be efficiently kept up to date. Essentially it is because most of the computation lies in computing ik f (y^k). After this is computed, xk, yk, vk can be updated more-or-less atomically with minimal overhead, meaning that they will always be up to
date. However our main results still hold for more general asynchronicity.

A natural quantity to consider in asynchronous convergence analysis is the asynchronicity error, a powerful tool for analyzing asynchronous algorithms used in several recent works Peng et al. (2016a); Hannah & Yin (2017b); Sun et al. (2017); Hannah & Yin (2017a). We adapt it and use a weighted sum of the history of the algorithm with decreasing weight as you go further back in time.

Definition 2. Asynchronicity error. Using the above parameters, we define:



Ak =

cj yk+1-j - yk-j 2

j=1

(2.9)

for

ci

=

6 S

L1/23/2



1 - 1/2S-1 i-j-1-1. (2.10)

j=i

Here we define yk = y0 for all k < 0. The determination of the coefficients ci is in general a very involved process of trial and error, intuition, and balancing competing requirements. The algorithm
doesn't depend on the coefficients, however; they are only an analytical tool.

4

Under review as a conference paper at ICLR 2019

We define Ek[X] as the expectation of X conditioned on (x0, . . . , xk), (y0, . . . , yk), (v0, . . . , vk), and (i0, . . . , ik-1). To simplify notation4, we assume that the minimizer x = 0, and that f (x) = 0 with no loss in generality. We define the Lyapunov function:

k = vk 2 + Ak + cf (xk)

(2.11)

for c = 2-1/2S-1 -1(1 - ) + 1 .

(2.12)

We now present this paper's first main contribution.

Theorem 1. Let f be -strongly convex with a gradient f that is L-Lipschitz with block Lipschitz

constants

{Li}ni=1.

Let



defined

in

equation

2.2

satisfy





3 7

(i.e.





1 21

S 1/2 L1/2 L-3/4 -1/4 ).

Then for A2BCD we have:

Ek[k+1]  1 - (1 - )1/2S-1 k.

To obtain E[k]  0, it takes KA2BCD( ) iterations for:

KA2BCD( ) =

-1/2S + O(1)

ln(1/ ) 1-

,

where O(·) is asymptotic with respect to -1/2S  , and uniformly bounded.

(2.13)

This result is proven in Section B. A stronger result for Li  L can be proven, but this adds to the complexity of the proof; see Section E for a discussion. In practice, asynchronous algorithms are far more resilient to delays than the theory predicts.  can be much larger without negatively affecting the convergence rate and complexity. This is perhaps because we are limited to a worst-case analysis, which is not representative of the average-case performance.
Allen-Zhu et al. (2015) (Theorem 5.1) shows a linear convergence rate of 1 - 2/ 1 + 2-1/2S for NU_ACDM, which leads to the corresponding iteration complexity of KNU_ACDM( ) = -1/2S + O(1) ln(1/ ). Hence, we have:
1 KA2BCD( ) = 1 -  (1 + o(1))KNU_ACDM( )
When 0   1, or equivalently, when  S1/2L1/2L-3/4-1/4, the complexity of A2BCD asymptotically matches that of NU_ACDM. Hence A2BCD combines state-of-the-art complexity with the faster iterations and superior scaling that asynchronous iterations allow. We now present some special cases of the conditions on the maximum delay  required for good complexity.
Corollary 3. Let the conditions of Theorem 1 hold. If all coordinate-wise Lipschitz constants Li are equal (i.e. Li = L1, i), then we have KA2BCD( )  KNU_ACDM( ) when  n1/2-1/4(L1/L)3/4. If we further assume all coordinate-wise Lipschitz constants Li equal L. Then KA2BCD( )  KNU_ACDM( ) = KACDM( ), when  n1/2-1/4.
Remark 1. Reduction to synchronous case. Notice that when  = 0, we have  = 0, ci  0 and hence Ak  0. Thus A2BCD becomes equivalent to NU_ACDM, the Lyapunov function5 k becomes equivalent to one found in Allen-Zhu et al. (2015)(pg. 9), and Theorem 1 yields the same complexity.

The maximum delay  will be a function  (p) of p, number of computing nodes. Clearly   p, and experimentally it has been observed that  = O(p) Leblond et al. (2017). Let gradient complexity
4We can assume x = 0 with no loss in generality since we may translate the coordinate system so that x is at the origin. We can assume f (x) = 0 with no loss in generality, since we can replace f (x) with f (x)-f (x). Without this assumption, the Lyapunov function simply becomes: vk - x 2 + Ak + c(f (xk) - f (x)).
5Their Lyapunov function is in fact a generalization of the one found in Nesterov (2012).

5

Under review as a conference paper at ICLR 2019

2.1 Optimality

K( ,  ) be the number of gradients required for an asynchronous algorithm with maximum delay 

to attain suboptimality .  (1) = 0, since with only 1 computing node there can be no delay. This

corresponds to the serial complexity. We say that an asynchronous algorithm attains a complexity

speedup

if

pK ( K(

, (0)) , (p)

is

increasing

in

p.

We

say

it

attains

linear

complexity

speedup

if

pK ( K(

, (0)) , (p)

=

(p).

In Theorem 1, we obtain a linear complexity speedup (for p not too large), whereas no other prior

attempt can attain even a complexity speedup with Nesterov acceleration.

In the ideal scenario where the rate at which gradients are calculated increases linearly with p, algorithms that have linear complexity speedup will have a linear decrease in wall-clock time. However in practice, when the number of computing nodes is sufficiently large, the rate at which gradients are calculated will no longer be linear. This is due to many parallel overhead factors including too many nodes sharing the same memory read/write bandwidth, and network bandwidth. However we note that even with these issues, we obtain much faster convergence than the synchronous counterpart experimentally.

2.1 Optimality

NU_ACDM and hence A2BCD are in fact optimal in some sense. That is, among a fairly wide class of coordinate descent algorithms A, they have the best-possible worst-case complexity to highest order. We extend the work in Lan & Zhou (2015) to encompass algorithms are asynchronous and have unequal Li. For a subset S  Rd, we let IC(S) (inconsistent read) denote the set of vectors v whose components are a combination of components of vectors in the set S. That is, v = (v1,1, v2,2, . . . , vd,d) for some vectors v1, v2, . . . , vd  S. Here vi,j denotes the jth component of vector vi.
Definition 4. Asynchronous Randomized Incremental Algorithms. Consider the unconstrained minimization problem equation 1.1 for function f satisfying the conditions stated in Section 1. We define the class A as algorithms G on this problem such that:

1. For each parameter fixed distribution P[ik

set ]=

(, L1, pi for

.

.in.=,1Lpni,=n)1, .G

has

an

associated

IID

random

variable

ik

with

some

2. The iterates of A satisfy: xk+1  span{IC(Xk), i0 f (IC(X0)), i1 f (IC(X1)), . . . , ik f (IC(Xk))}

This is a rather general class: xk+1 can be constructed from any inconsistent reading of past iterates IC(Xk), and any past gradient of an inconsistent read ij f (IC(Xj)).
Theorem 2. For any algorithm G  A that solves eq. (1.1), and parameter set (, L1, . . . , Ln, n), there is a dimension d, a corresponding function f on Rd, and a starting point x0, such that

E

xk - x

2/

x0 - x

2

1 2

1 - 4/

n j=1

Li/ + 2n k

Hence

A

has

a

complexity

lower

bound:

K(

)



1 4

(1

+

o(1))

n j=1

Li/ + 2n ln(1/2 )

Our proof in Section D follows very similar lines to Lan & Zhou (2015); Nesterov (2013).

3 ODE Analysis

In this section we present and analyze an ODE which is the continuous-time limit of A2BCD. This

ODE is a strongly convex, and asynchronous version of the ODE found in Su et al. (2014). For

simplicity, assume

Li

= L, i.

We rescale (I.e.

we replace f (x)

with

1 

f

.)

f

so

that



= 1, and

hence  = L/ = L. Taking the discrete limit of synchronous A2BCD (i.e. accelerated RBCD), we can

6

Under review as a conference paper at ICLR 2019

derive the following ODE6 (see Section equation C.1):

Y¨ + 2n-1-1/2Y + 2n-2-1f (Y ) = 0

(3.1)

We define the parameter 

n1/2,

and

the

energy:

E(t) =

en-1-1/2t(f (Y ) +

1 4

Y + Y

2). This

is

very

similar

to

the

Lyapunov

function

discussed

in

equation

2.11,

with

1 4

Y (t) + Y (t)

2 fulfilling

the role of vk 2, and Ak = 0 (since there is no delay yet). Much like the traditional analysis in the

proof of Theorem 1, we can derive a linear convergence result with a similar rate. See Section C.2.

Lemma 5. If Y satisfies equation 3.1, the energy satisfies E (t)  0, E(t)  E(0), and hence:

f (Y (t)) + 1

Y (t) + n1/2Y (t)

2


f (Y (0)) + 1

Y (0) + Y (0) 2

e-n-1 -1/2 t

44

We may also analyze an asynchronous version of equation 3.1 to motivate the proof of our main theorem. Here Y^ (t) is a delayed version of Y (t) with the delay bounded by  .

Y¨ + 2n-1-1/2Y + 2n-2-1f Y^ = 0,

(3.2)

Unfortunately, this energy satisfies (see Section equation C.4, equation C.7):

e--1tE (t)  - 1  Y 2 + 32-1 D(t), for D(t) 8

t
Y (s) 2ds.
t-

Hence this energy E(t) may not be decreasing in general. But, we may add a continuous-time

asynchronicity error (see Sun et al. (2017)), much like in Definition 2, to create a decreasing

energy. Let c0  0 and r > 0 be arbitrary constants that will be set later. Define:

t

A(t) =

c(t - s) Y (s) 2ds, for c(t)

t-

c0

e-rt

+

1

e-r - e-r

e-rt - 1

.

Lemma 6.

When

r



1 2

,

the

asynchronicity

error

A(t)

satisfies:

e-rt d dt

ertA(t)

 c0

Y (t)

2

-

1 2



-1

c0

D(t).

See Section C.3 for the proof. Adding this error to the Lyapunov function serves a similar purpose

in the continuous-time case as in the proof of Theorem 1 (see Lemma 11). It allows us to negate

1 2



-1c0

units

of

D(t)

for

the

cost

of

creating

c0

units

of

Y (t) 2. This restores monotonicity.

Theorem 3.

Let

c0

=

62-1 2,

and

r

=

-1.

If





1 n-1/2
48

then

we

have:

e--1t d E(t) + e-1tA(t)  0. dt

(3.3)

Hence f (Y (t)) convergence linearly to f (x) with rate O exp -t/(n1/2)

Notice how this convergence condition is similar to Corollary 3, but a little looser. The convergence

condition in Theorem 1 can actually be improved to approximately match this (see Section E).

Proof.

e--1t d E(t) + e-1tA(t) dt



c0

-

1 
8

Y

2+

32-1

-

1 2



-1c0

D(t)

= 6-12  2 - 1 n2-1 Y 2  0 48

The preceding should hopefully elucidate the logic and general strategy of the proof of Theorem 1. 6For compactness, we have omitted the (t) from time-varying functions Y (t), Y (t), Y (t), etc.

7

Under review as a conference paper at ICLR 2019

4 Related work

We now discuss related work that was not addressed in Section 1. Nesterov acceleration is a

method for improving an algorithm's iteration complexity's dependence the condition number .

Nesterov-accelerated methods have been proposed and discovered in many settings Nesterov (1983);

Tseng (2008); Nesterov (2012); Lin et al. (2014); Lu & Xiao (2014); Shalev-Shwartz & Zhang (2016);

Allen-Zhu (2017), including for coordinate descent algorithms (algorithms that use 1 gradient block

(aligforoitrhmmisnfiomrizfieniwteitshumresppreocbtletmos1n1cooinr=d1infai(txe)btlhoactk

per iteration), and incremental algorithms use 1 function gradient fi(x) per iteration).

Such algorithms can often be augmented to solve composite minimization problems (minimization

for objective of the form f (x) + g(x), especially for nonsomooth g), or include constraints.

In Peng et al. (2016a), authors proposed and analyzed an asynchronous fixed-point algorithm called ARock, that takes proximal algorithms, forward-backward, ADMM, etc. as special cases. Work has also been done on asynchronous algorithms for finite sums in the operator setting Davis (2016); Johnstone & Eckstein (2018). In Hannah & Yin (2017b); Sun et al. (2017); Peng et al. (2016c); Cannelli et al. (2017) showed that many of the assumptions used in prior work (such as bounded delay  < ) were unrealistic and unnecessary in general. In Hannah & Yin (2017a) the authors showed that asynchronous iterations will complete far more iterations per second, and that a wide class of asynchronous algorithms, including asynchronous RBCD, have the same iteration complexity as their synchronous counterparts. Hence certain asynchronous algorithms can be expected to significantly outperform traditional ones.

In Xiao et al. (2017) authors propose a novel asynchronous catalyst-accelerated Lin et al. (2015) primal-dual algorithmic framework to solve regularized ERM problems. They structure the parallel updates so that the data that an update depends on is up to date (though the rest of the data may not be). However catalyst acceleration incurs a log() penalty over Nesterov acceleration in general. In Allen-Zhu (2017), the author argues that the inner iterations of catalyst acceleration are hard to tune, making it less practical than Nesterov acceleration.

5 Numerical experiments

To investigate the performance of A2BCD, we solve the ridge regression problem. Consider the following primal and corresponding dual objective (see for instance Lin et al. (2014)):

1

min P (w) =

wRd

2n

AT w - l

2+  2

w

2, min D()
Rn

=

1 2d2

A

2+ 1 2d

+l

2

(5.1)

where A  Rd×n is a matrix of n samples and d features, and l is a label vector. We let A = [A1, . . . , Am] where Ai are the column blocks of A. We compare A2BCD (which is asynchronous accelerated), synchronous NU_ACDM (which is synchronous accelerated), and asynchronous RBCD (which is asynchronous non-accelerated). Nodes randomly select a coordinate block according to equation 2.1, calculate the corresponding block gradient, and use it to apply an update to the shared solution vectors. synchronous NU_ACDM is implemented in a batch fashion, with batch size p (1 block per computing node). Nodes in synchronous NU_ACDM implementation must wait until all nodes apply an update before they can start the next iteration, but the asynchronous algorithms simply compute with the most up-to-date information available.

We use the datasets w1a (47272 samples, 300 features), a congregate file (wxa) from w1a to w8a (293201 samples, 300 features), and aloi (108000 samples, 128 features) from LIBSVM Chang & Lin (2011). The algorithm is implemented in a multi-threaded fashion using C++11 and GNU Scientific Library with a shared memory architecture. We use 40 threads on two 2.5GHz 10-core Intel Xeon

8

Under review as a conference paper at ICLR 2019
E5-2670v2 processors. See Section A.1 for a discussion of parameter tuning and estimation. The parameters for each algorithm are tuned to give the fastest performance, so that a fair comparison is possible. A critical ingredient in the efficient implementation of A2BCD and NU_ACDM for this problem is the efficient update scheme discussed in Lee & Sidford (2013b;a). In linear regression applications such as this, it is essential to be able to efficiently maintain or recover Ay. This is because calculating block gradients requires the vector AiT Ay, and without an efficient way to recover Ay, block gradient evaluations are essentially 50% as expensive as full-gradient calculations. Unfortunately, every accelerated iteration results in dense updates to yk because of the averaging step in equation 2.6. Hence Ay must be recalculated from scratch. However Lee & Sidford (2013a) introduces a linear transformation that allows for an equivalent iteration that results in sparse updates to new iteration variables p and q. The original purpose of this transformation was to ensure that the averaging steps (e.g. equation 2.6) do not dominate the computational cost for sparse problems. However we find a more important secondary use which applies to both sparse and dense problems. Since the updates to p and q are sparse coordinate-block updates, the vectors Ap, and Aq can be efficiently maintained, and therefore block gradients can be efficiently calculated. The specifics of this efficient implementation are discussed in Section A.2. In Table 5, we plot the sub-optimality vs. time for decreasing values of , which corresponds to increasingly large condition numbers . When  is small, acceleration doesn't result in a significantly better convergence rate, and hence A2BCD and async-RBCD both outperform sync-NU_ACDM since they complete faster iterations at similar complexity. Acceleration for low  has unnecessary overhead, which means async-RBCD can be quite competitive. When  becomes large, async-RBCD is no longer competitive, since it has a poor convergence rate. We observe that A2BCD and sync-NU_ACDM have essentially the same convergence rate, but A2BCD is up to 4 - 5× faster than sync-NU_ACDM because it completes much faster iterations. We observe this advantage despite the fact that we are in an ideal environment for synchronous computation: A small, homogeneous, high-bandwidth, low-latency cluster. In large-scale heterogeneous systems with greater synchronization overhead, bandwidth constraints, and latency, we expect A2BCD's advantage to be much larger.
9

Under review as a conference paper at ICLR 2019

REFERENCES

Table 1: Sub-optimality f (yk) - f (x) (y-axis) vs time in seconds (x-axis) for A2BCD, synchronous NU_ACDM, and asynchronous RBCD for data sets w1a and rcv1_train for various values of .
References
Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pp. 1200­1205, New York, NY, USA, 2017. ACM.
Zeyuan Allen-Zhu and Elad Hazan. Optimal Black-Box Reductions Between Optimization Objectives. arXiv:1603.05642, March 2016.
Zeyuan Allen-Zhu, Zheng Qu, Peter Richtárik, and Yang Yuan. Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling. arXiv:1512.09103, December 2015.
Yossi Arjevani. Limitations on Variance-Reduction and Acceleration Schemes for Finite Sums Optimization. In Advances in Neural Information Processing Systems 30, pp. 3540­3549. Curran Associates, Inc., 2017.
H. Avron, A. Druinsky, and A. Gupta. Revisiting asynchronous linear solvers: Provable convergence rate through randomization. In Parallel and Distributed Processing Symposium, 2014 IEEE 28th International, pp. 198­207, May 2014.
Dimitri P. Bertsekas. Distributed asynchronous computation of fixed points. Mathematical Programming, 27(1):107­120, 1983.
Dimitri P. Bertsekas and John N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Athena Scientific, 1997.
Loris Cannelli, Francisco Facchinei, Vyacheslav Kungurtsev, and Gesualdo Scutari. Asynchronous Parallel Algorithms for Nonconvex Big-Data Optimization. Part II: Complexity and Numerical Results. arXiv:1701.04900, January 2017.
10

Under review as a conference paper at ICLR 2019

REFERENCES

Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A Library for Support Vector Machines. ACM Trans. Intell. Syst. Technol., 2(3):27:1­27:27, May 2011.
D. Chazan and W. Miranker. Chaotic relaxation. Linear Algebra and its Applications, 2(2):199­222, April 1969.
Patrick L. Combettes and Jonathan Eckstein. Asynchronous block-iterative primal-dual decomposition methods for monotone inclusions. Mathematical Programming, 168(1-2):645­672, March 2018.
Damek Davis. SMART: The stochastic monotone aggregated root-finding algorithm. arXiv:1601.00698, January 2016.
Cong Fang, Yameng Huang, and Zhouchen Lin. Accelerating Asynchronous Algorithms for Convex Optimization by Momentum Compensation. arXiv:1802.09747 [cs, math], February 2018.
Robert Hannah and Wotao Yin. More Iterations per Second, Same Quality ­ Why Asynchronous Algorithms may Drastically Outperform Traditional Ones. arXiv:1708.05136, August 2017a.
Robert Hannah and Wotao Yin. On Unbounded Delays in Asynchronous Parallel Fixed-Point Algorithms. Journal of Scientific Computing, pp. 1­28, December 2017b.
Zhouyuan Huo and Heng Huang. Asynchronous Stochastic Gradient Descent with Variance Reduction for Non-Convex Optimization. arXiv:1604.03584, April 2016.
Patrick R. Johnstone and Jonathan Eckstein. Projective Splitting with Forward Steps: Asynchronous and Block-Iterative Operator Splitting. arXiv:1803.07043 [cs, math], March 2018.
Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv:1507.02000, July 2015.
Rémi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. ASAGA: Asynchronous Parallel SAGA. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pp. 46­54, April 2017.
Y. T. Lee and A. Sidford. Efficient Accelerated Coordinate Descent Methods and Faster Algorithms for Solving Linear Systems. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pp. 147­156, October 2013a.
Yin Tat Lee and Aaron Sidford. Efficient Accelerated Coordinate Descent Methods and Faster Algorithms for Solving Linear Systems. arXiv:1305.1922, May 2013b.
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A Universal Catalyst for First-Order Optimization. arXiv:1506.02186, June 2015.
Qihang Lin, Zhaosong Lu, and Lin Xiao. An Accelerated Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization. arXiv:1407.1296, July 2014.
J. Liu and S. Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence properties. SIAM Journal on Optimization, 25(1):351­376, January 2015.
Zhaosong Lu and Lin Xiao. On the complexity analysis of randomized block-coordinate descent methods. Mathematical Programming, 152(1-2):615­642, August 2014.
Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72(1):7­35, January 1992.
11

Under review as a conference paper at ICLR 2019

REFERENCES

Zhi-Quan Luo and Paul Tseng. On the convergence rate of dual ascent methods for linearly constrained convex minimization. Mathematics of Operations Research, 18(4):846­867, November 1993.
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael I. Jordan. Perturbed Iterate Analysis for Asynchronous Stochastic Optimization. arXiv:1507.06970, July 2015.
Qi Meng, Wei Chen, Jingcheng Yu, Taifeng Wang, Zhi-Ming Ma, and Tie-Yan Liu. Asynchronous Accelerated Stochastic Gradient Descent.
Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341­362, January 2012.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate O (1/k2). In Soviet Mathematics Doklady, volume 27, pp. 372­376, 1983.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Science & Business Media, December 2013.
Z. Peng, Y. Xu, M. Yan, and W. Yin. ARock: An Algorithmic Framework for Asynchronous Parallel Coordinate Updates. SIAM Journal on Scientific Computing, 38(5):A2851­A2879, January 2016a.
Zhimin Peng, Tianyu Wu, Yangyang Xu, Ming Yan, and Wotao Yin. Coordinate friendly structures, algorithms and applications. Annals of Mathematical Sciences and Applications, 1(1):57­119, 2016b.
Zhimin Peng, Yangyang Xu, Ming Yan, and Wotao Yin. On the Convergence of Asynchronous Parallel Iteration with Unbounded Delays. arXiv:1612.04425 [cs, math, stat], December 2016c.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems 24, pp. 693­701, 2011.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabás Póczos, and Alex Smola. On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants. arXiv:1506.06840, June 2015.
Nicolas Le Roux, Mark Schmidt, and Francis Bach. A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets. arXiv:1202.6258 [cs, math], February 2012.
Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. Mathematical Programming, 155(1-2):105­145, January 2016.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights. In Advances in Neural Information Processing Systems 27, pp. 2510­2518. 2014.
Tao Sun, Robert Hannah, and Wotao Yin. Asynchronous Coordinate Descent under More Realistic Assumptions. In Advances in Neural Information Processing Systems 30, pp. 6183­6191. 2017.
P. Tseng. On the rate of convergence of a partially asynchronous gradient projection algorithm. SIAM Journal on Optimization, 1(4):603­619, November 1991.
12

Under review as a conference paper at ICLR 2019

REFERENCES

P. Tseng, D. Bertsekas, and J. Tsitsiklis. Partially asynchronous, parallel algorithms for network flow and other problems. SIAM Journal on Control and Optimization, 28(3):678­710, March 1990.

Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. Department of Mathematics, University of Washington, Tech. Rep., 2008.

Lin Xiao, Adams Wei Yu, Qihang Lin, and Weizhu Chen. DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization. arXiv:1710.05080, October 2017.
13

Under review as a conference paper at ICLR 2019

A Efficient Implementation
An efficient implementation will have coordinate blocks of size greater than 1. This to ensure the efficiency of linear algebra subroutines. Especially because of this, the bulk of the computation for each iteration is computing ik f (y^k), and not the averaging steps. Hence the computing nodes only need a local copy of yk in order to do the bulk of an iteration's computation. Given this gradient ik f (y^k), updating yk and vk is extremely fast (xk can simply be eliminated). Hence it is natural to simply store yk and vk centrally, and update them when the delayed gradients ik f (y^k). Given the above, a write mutex over (y, v) has minuscule overhead (which we confirm with experiments), and makes the labeling of iterates unambiguous. This also ensures that vk and yk are always up to date when (y, v) are being updated. Whereas the gradient ik f (y^k) may at the same time be out of date, since it has been calculated with an outdated version of yk. However a write mutex is not necessary in practice, and does not appear to affect convergence rates or computation time. Also it is possible to prove convergence under more general asynchronicity.

A.1 Parameter selection and tuning
When defining the coefficients,  may be underestimated, and L, L1, . . . , Ln may be overestimated if exact values are unavailable. Notice that xk can be eliminated from the above iteration, and the block gradient ik f (y^k) only needs to be calculated once per iteration. A larger (or overestimated) maximum delay  will cause a larger asynchronicity parameter , which leads to more conservative step sizes to compensate.
To estimate , one can first performed a dry run with all coefficient set to 0 to estimate  . All function parameters can be calculated exactly for this problem in terms of the data matrix and . We can then use these parameters and this tau to calculate .  and  merely change the parameters, and do not change execution patterns of the processors. Hence their parameter specification doesn't affect the observed delay. Through simple tuning though, we found that  = 0.25 resulted in good performance.
In tuning for general problems, there are theoretical reasons why it is difficult to attain acceleration without some prior knowledge of , the strong convexity modulus Arjevani (2017). Ideally  is pre-specified for instance in a regularization term. If the Lipschitz constants Li cannot be calculated directly (which is rarely the case for the classic dual problem of empirical risk minimization objectives), the line-search method discussed in Roux et al. (2012) Section 4 can be used.

A.2 Sparse update formulation

As mentioned in Section 5, authors in Lee & Sidford (2013a) proposed a linear transformation of an accelerated RBCD scheme that results in sparse coordinate updates. Our proposed algorithm can be given a similar efficient implementation. We may eliminate xk from A2BCD, and derive the equivalent iteration below:

yk+1 vk+1

=

1 - ,  1 - , 

C

yk vk

- Qk

yk vk

 -

-1/2L-ik1/2 + h(1 - )L-ik1 ik f y^k -1/2Li-k1/2 ik f y^k

 

14

Under review as a conference paper at ICLR 2019

A.2 Sparse update formulation

where C and Qk are defined in the obvious way. Hence we define auxiliary variables pk, qk defined via:

yk vk

= Ck

pk qk

(A.1)

These clearly follow the iteration:

pk+1 qk+1

=

pk qk

- C-(k+1)Qk

(A.2)

Since the vector Qk is sparse, we can evolve variables pk, and qk in a sparse manner, and recover the original iteration variables at the end of the algorithm via A.1.
The gradient of the dual function is given by:

D(y) = 1 1 AT Ay + (y + l) d d

As mentioned before, it is necessary to maintain or recover Ayk to calculate block gradients. Since Ayk can be recovered via the linear relation in equation A.1, and the gradient is an affine function, we maintain the auxiliary vectors Apk and Aqk instead.
Hence we propose the following efficient implementation in Algorithm 1. We used this to generate the results in Table 5. We also note also that it can improve performance to periodically recover vk and yk, reset the values of pk, qk, and C to vk, yk, and I respectively, and restarting the scheme (which can be done cheaply in time O(d)).
We let B  R2×2 represent Ck, and b represent B-1.  is the Kronecker product. Each computing node has local outdated versions of p, q, Ap, Aq which we denote p^, q^, A^p, A^q respectively. We also find it convenient to define:

D1k D2k

=

-1/2Li-k1/2 + h(1 - )L-ik1 -1/2L-ik1/2

15

(A.3)

Under review as a conference paper at ICLR 2019

Algorithm 1 Shared-memory implementation of A2BCD

1: Inputs: Function parameters A, , L, {Li}in=1, n, d. Delay  (obtained in dry run). Starting vectors y, v.

2: Shared data: Solution vectors p, q; auxiliary vectors Ap, Aq; sparsifying matrix B 3: Node local data: Solution vectors p^, q^, auxiliary vectors A^p, A^q, sparsifying matrix B^.

4: Calculate parameters , , , h via 1. Set k = 0. 5: Initializations: p  y, q  v, Ap  Ay, Aq  Av, B  I.

6: while not converged, each computing node asynchronous do

7: Randomly select block i via equation 2.1. 8: Read shared data into local memory: p^  p, q^  q, A^p  Ap, A^q  Aq, B^  B.

9:

Compute

block

gradient:

if (y^)

=

1 n

1 n

AiT

B^1,1A^p + B^1,2A^q

+  B^1,1p^ + B^1,2q^

10: Compute quantity gi = ATi if (y^) Shared memory updates:

11:

Update B 

1 -  1-

 

× B, calculate inverse b  B-1.

12:

p q

-= b

D1k D2k

 if (y^) ,

Ap Aq

-= b

D1k D2k

 gi

13: Increase iteration count: k  k + 1

14: end while

15: Recover original iteration variables:

y v

B

p q

. Output y.

B Proof of the main result

We first recall a couple of inequalities for convex functions. Lemma 7. Let f be -strongly convex with L-Lipschitz gradient. Then we have:

f (y)  f (x) +

y - x, f (x)

1 +L

y-x

2, x, y

2

f (y)  f (x) +

y - x, f (x)

1 +

y-x

2, x, y

2

We also find it convenient to define the norm:

s =

n
Li-1/2 si 2
i=1

16

(B.1) (B.2)
(B.3)

Under review as a conference paper at ICLR 2019

B.1 Starting point

B.1 Starting point

First notice that using the definition equation 2.8 of vk+1 we have:

vk+1 2 = vk + (1 - )yk 2 - 2-1/2L-ik1/2 vk + (1 - )yk, ik f (y^k) + -1L-ik1 ik f (y^k) 2

Ek vk+1 2 = vk + (1 - )yk 2 -2-1/2S-1 vk + (1 - )yk, f (y^k)

(B.4)

A

n

+ S-1-1

L-i 1/2 if (y^k) 2

i=1

B

C

We have the following general identity:

x + (1 - )y 2 =  x 2 + (1 - ) y 2 - (1 - ) x - y 2, x, y

(B.5)

It can also easily be verified from equation 2.6 that we have:

vk = yk + -1(1 - )(yk - xk)

(B.6)

Using equation B.5 on term A, equation B.6 on term B, and recalling the definition equation B.3 on term C, we have from equation B.4:

Ek

vk+1

2=

vk

2 + (1 - )

yk

2 - (1 - )

vk - yk

2 + S-1-1/2

f (y^k)

2 

- 2-1/2S-1-1(1 - ) yk - xk, f (y^k) - 2-1/2S-1 yk, f (y^k)

(B.7)

This inequality is our starting point. We analyze the terms on the second line in the next section.

B.2 The Cross Term

To analyze these terms, we need a small lemma. This lemma is fundamental in allowing us to deal with asynchronicity.

Lemma 8. Let , A > 0. Let the delay be bounded by  . Then:

A y^k - yk



1 -1A2

+

1 

22



yk+1-j - yk-j 2

j=1

Proof. See Hannah & Yin (2017a).

Lemma 9. We have:

- f (y^k), yk

1



-f (yk)

-

(1 2

-

)

yk

2 + 1 L-1 2



yk+1-j - yk-j 2

j=1

(B.8)

f (y^k), xk - yk  f (xk) - f (yk)

(B.9)



+

1 L(1

-

)-1-1

2

vk - yk

2 + -1-1



yk+1-j - yk-j

2


j=1

The terms in bold in equation B.8 and equation B.9 are a result of the asynchronicity, and are identically 0 in its absence.

17

Under review as a conference paper at ICLR 2019

B.3 Function-value term

Proof. Our strategy is to separately analyze terms that appear in the traditional analysis of Nesterov (2012), and the terms that result from asynchronicity. We first prove equation B.8:

- f (y^k), yk = - f (yk), yk - f (y^k) - f (yk), yk



-f (yk)

-

1 
2

yk

2+L

y^k - yk

yk

(B.10)

equation B.10 follows from strong convexity (equation B.2 with x = yk and y = x), and the fact that f is L-Lipschitz. The term due to asynchronicity becomes:

L y^k - yk

yk



1 L-1 2

yk

2 + 1 L-1 2



yk+1-j - yk-j 2

j=1

using Lemma 8 with  = -1, A = yk . Combining this with equation B.10 completes the proof of equation B.8.
We now prove equation B.9:

f (y^k), xk - yk = f (yk), xk - yk + f (y^k) - f (yk), xk - yk

 f (xk) - f (yk) + L y^k - yk xk - yk

 f (xk) - f (yk)



+

1 2

L-1



-1(1

-

)

xk - yk

2 + -1-1(1 - )-1



yk+1-j - yk-j

2


j=1

Here the last line follows from Lemma 8 with  = -1-1(1 - )-1, A = nxk - yk. We can complete the proof using the following identity that can be easily obtained from equation 2.6:

yk - xk = (1 - )-1(vk - yk)

B.3 Function-value term

Much like Nesterov (2012), we need a f (xk) term in the Lyapunov function (see the middle of page 357). However we additionally need to consider asynchronicity when analyzing the growth of this term. Again terms due to asynchronicity are emboldened.
Lemma 10. We have:

Ekf (xk+1)



f (yk)

-

1 h
2

2-h

1 + 1 1/2L-1/2 2

S -1

f (y^k)

2 



+ S-1L1/2-1

yk+1-j - yk-j 2

j=1

Proof. From the definition equation 2.7 of xk+1, we can see that xk+1 - yk is supported on block ik. Since each gradient block if is Li Lipschitz with respect to changes to block i, we can use
18

Under review as a conference paper at ICLR 2019

B.4 Asynchronicity error

equation B.1 to obtain:

f (xk+1)  f (yk) +

f (yk), xk+1 - yk

1 + 2 Lik

xk+1 - yk

2

(from equation 2.7) = f (yk) - hL-ik1 ik f (yk), ik f (y^k)

+

1 2

h2

L-ik1

ik f (y^k)

2

= f (yk) - hL-ik1 ik f (yk) - ik f (y^k), ik f (y^k)

-

1 h(2
2

-

h)L-ik1

ik f (y^k)

2

Ekf (xk+1)  f (yk) - hS-1

n

L-i 1/2 if (yk) - if (y^k), if (y^k)

-

1 h(2

-

h)S-1

2

f (y^k)

2 

i=1

(B.11)

Here the last line followed from the definition equation B.3 of the norm · 1/2. We now analyze the middle term:

n
- Li-1/2 if (yk) - if (y^k), if (y^k)

i=1

nn

=-

Li-1/4(if (yk) - if (y^k)), Li-1/4if (y^k)

i=1 i=1

nn

(Cauchy Schwarz) 

L-i 1/4(if (yk) - if (y^k))

Li-1/4if (y^k)

i=1 i=1

n

1/2 n

1/2

= Li-1/2 if (yk) - if (y^k) 2

L-i 1/2 if (y^k) 2

i=1 i=1

(L  Li, i and equation B.3)  L-1/4 f (yk) - f (y^k) f (y^k) 

(f is L-Lipschitz)  L-1/4L yk - y^k f (y^k) 

We then apply Lemma 8 to this with  = 2h-11/2L1/4-1, A = f (y^k)  to yield:

n

- L-i 1/2 if (yk) - if (y^k), if (y^k)  h-1L1/2-1

yk+1-j - yk-j 2

(B.12)

i=1 j=1

+ 1 h1/2L-1/2 4

f (y^k)

2 

Finally to complete the proof, we combine equation B.11, with equation B.12.

B.4 Asynchronicity error

The previous inequalities produced difference terms of the form yk+1-j - yk-j 2. The following lemma shows how these errors can be incorporated into a Lyapunov function.

Lemma 11. Let 0 < r < 1 and consider the asynchronicity error and corresponding coefficients:


Ak = cj yk+1-j - yk-j 2

j=1



ci =

ri-j-1sj

j=i

19

Under review as a conference paper at ICLR 2019

B.4 Asynchronicity error

This sum satisfies:


Ek[Ak+1 - rAk] = c1Ek yk+1 - yk 2 - sj yk+1-j - yk-j 2
j=1

Remark 2. Interpretation. This result means that an asynchronicity error term Ak can negate

a series of difference terms -

 j=1

sj

yk+1-j - yk-j

2 at the cost of producing an additional error

c1Ek yk+1 - yk 2, while maintaining a convergence rate of r. This essentially converts difference

terms, which are hard to deal with, into a yk+1 - yk 2 term which can be negated by other terms

in the Lyapunov function. The proof is straightforward.

Proof.


Ek[Ak+1 - rAk] = Ek cj+1 yk+1-j - yk-j 2 - rEk cj yk+1-j - yk-j 2

j=0

j=1


= c1Ek yk+1 - yk 2 + Ek (cj+1 - rcj ) yk+1-j - yk-j 2

j=1

Noting the following completes the proof:



ci+1 - rci =

ri+1-j-1sj - r ri-j-1sj = -si

j=i+1

j=i

Given that Ak allows us to negate difference terms, we now analyze the cost c1Ek yk+1 - yk 2 of this negation.
Lemma 12. We have:
Ek yk+1 - yk 2  222 vk - yk 2 + 2S-1L-1 f (y^k) 2

Proof.

yk+1 - yk = (vk+1 + (1 - )xk+1) - yk

=  vk + (1 - )yk - -1/2L-ik1/2ik f (y^k) + (1 - ) yk - hL-ik1ik f (y^k) - yk (B.13)

= vk + (1 - )yk - -1/2L-ik1/2ik f (y^k) - yk - (1 - )hLi-k1ik f (y^k) = (vk - yk) - -1/2Li-k1/2 + h(1 - )L-ik1 ik f (y^k)

yk+1 - yk 2  222 vk - yk 2 + 2 -1/2L-ik1/2 + h(1 - )L-ik1 2 ik f (y^k) 2

(B.14)

20

Under review as a conference paper at ICLR 2019

B.5 Master inequality

Here equation B.13 following from equation 2.8, the definition of vk+1. equation B.14 follows from the inequality x + y 2  2 x 2 + 2 y 2. The rest is simple algebraic manipulation.

yk+1 - yk 2  222 vk - yk 2 + 2L-ik1 -1/2 + h(1 - )L-ik1/2 2 ik f (y^k) 2

(L  Li, i)  222 vk - yk 2 + 2L-ik1 -1/2 + h(1 - )L-1/2 2 ik f (y^k) 2

= 222 vk - yk 2 + 2L-ik1L-1 L1/2-1/2 + h(1 - ) 2 ik f (y^k) 2

E

yk+1 - yk

2  222

vk - yk

2 + 2S-1L-1

L1/2-1/2 + h(1 - )

2

f (y^k)

2 

Finally, to complete the proof, we prove L1/2-1/2 + h(1 - )  1.

L1/2-1/2 + h(1 - ) = h +  L1/2-1/2 - h
(definitions of h and : equation 2.3, and equation 2.5) = 1 - 1 1/2L-1/2 + 1/2S-1 L1/2-1/2 2
 1 - 1/2L-1/2 1  - -1/2S-1L1 2 (B.15)

Rearranging the definition of , we have:

S -1

=

1 92

2L1

L-3/2-1/2

-2

(

1

and





1 )
2



1 182

L1

L-3/2-1/2

Using this on equation B.15, we have:

L1/2-1/2 + h(1 - )  1 - 1/2L-1/2

= 1 - 1/2L-1/2

(



1 )

=

1

-

1/2L-1/2

2

1 
2

-

1 182

L1L-3/2-1/2

-1/2

L1

1 
2

-

1 182

(L/L)2

1 24

-

1 182

 1.

This completes the proof.

B.5 Master inequality
We are finally in a position to bring together all the all the previous results together into a master inequality for the Lyapunov function k (defined in equation 2.11). After this lemma is proven, we will prove that the right hand size is negative, which will imply that k linearly converges to 0 with rate .

21

Under review as a conference paper at ICLR 2019

B.5 Master inequality

Lemma 13. Master inequality. We have:

Ek[k+1 - k]  + yk 2
+ vk - yk 2

+ f (yk)

+ f (xk)

+ yk+1-j - yk-j 2
j=1

+

f (y^k)

2 

× 1 -  - -1/2S-1(1 - ) × 22c1 + S-1L1/2-1/2 - (1 - ) × c - 2-1/2S-1 -1(1 - ) + 1 × 2-1/2S-1-1(1 - ) - c

(B.16)

×S-1L-1 1/2 2-1 + c - s

×S-1

-1

+

2L-1c1

-

1 ch
2

2-h

1 + 1 1/2L-1/2 2

Proof.

Ek vk+1 2 -  vk 2

(B.7) = (1 - )

yk

2 - (1 - )

vk - yk

2 + S-1-1

f (y^k)

2 

- 2-1/2S-1 yk, f (y^k)

- 2-1/2S-1-1(1 - ) yk - xk, f (y^k)

 (1 - )

yk

2 - (1 - )

vk - yk

2 + S-1-1

f (y^k)

2 

(B.17)



(B.8)

+

2-1/2S-1-f (yk)

-

1 (1
2

-

)

yk

2 + 1 L-1 2



yk+1-j - yk-j

2


j=1

(B.9) - 2-1/2S-1-1(1 - )(f (xk) - f (yk))




+ -1/2S-1L-1 vk - yk 2 + -1-1

yk+1-j - yk-j

2


j=1

We now collect and organize the similar terms of this inequality.

 + yk 2

+ vk - yk 2

- f (yk)

+ f (xk)


+ yk+1-j - yk-j 2

j=1

+

f (y^k)

2 

× 1 -  - -1/2S-1(1 - ) × -1/2S-1L-1 - (1 - ) ×2-1/2S-1 -1(1 - ) + 1 ×2-1/2S-1-1(1 - ) ×2-1/2S-1L-1
×-1S-1

22

Under review as a conference paper at ICLR 2019

B.6 Proof of main theorem

Now finally, we add the function-value and asynchronicity terms to our analysis. We use Lemma 11 is with r = 1 - 1/2S-1, and

s = 6S-1L1/23/2-1, 1  i  

si = 0,

i>

(B.18)

Notice that this choice of si will recover the coefficient formula given in equation 2.9. Hence we have:

Ek[cf (xk+1) + Ak+1 - (cf (xk) + Ak)]

1

(Lemma

10)



cf (yk)

-

ch 2

2-h

1 + 1 1/2L-1/2 2

S -1

f (y^k)

2 

-

cf

(xk )

(B.19)



+ S-1L1/2-1

yk+1-j - yk-j 2

j=1

(Lemmas 11 and 12) + c1 222 vk - yk 2 + 2S-1L-1 f (y^k) 2

(B.20)


- sj yk+1-j - yk-j 2 + Ak(r - )
j=1

Notice Ak(r - )  0. Finally, combining equation B.17 and equation B.19 completes the proof.

In the next section, we will prove that every coefficient on the right hand side of equation B.16 is 0 or less, which will complete the proof of Theorem 1.

B.6 Proof of main theorem
Lemma 14. The coefficients of yk 2, f (yk), and non-positive.

 j=1

yk+1-j - yk-j

2

in

Lemma

13

are

Proof. The coefficient 1 - (1 - )1/2S-1 -  of yk 2 is identically 0 via the definition equation 2.4 of . The coefficient c - 2-1/2S-1 -1(1 - ) + 1 of f (yk) is identically 0 via the definition equation 2.12 of c.
First notice from the definition equation 2.12 of c:

c = 2-1/2S-1 -1(1 - ) + 1 (definitions of , ) = 2-1/2S-1 1 - 1/2S-1(1 - ) (1 + )-1/2S + 1

= 2-1/2S-1 (1 + )-1/2S + 2

= 2-1 (1 + ) + 21/2S-1 c  4-1

(B.21) (B.22)

23

Under review as a conference paper at ICLR 2019

B.6 Proof of main theorem

Here

the

last

line

followed

since





1 2

and

 1/2 S -1



1.

We now analyze the coefficient of

 j=1

yk+1-j - yk-j

2.

S-1L-1 1/2 2-1 + c - s (B.22)  6L1/23/2-1 - s (definition equation B.18 of s)  0

Lemma 15. The coefficient  2-1/2S-1-1(1 - ) - c of f (xk) in Lemma 13 is non-positive.

Proof.

2-1/2S-1-1(1 - ) - c (B.21) = 2-1/2S-1(1 + )-1/2S - 2-1 (1 + ) + 21/2S-1
= 2-1 (1 + ) - (1 + ) + 21/2S-1 = -22-1/2S-1  0

Lemma 16.

The coefficient S-1

-1

+

2L-1c1

-

1 2

ch

2-h

1

+

1 2

1/2

L-1/2



in Lemma 13 is non-positive.

of

f (y^k)

2 

Proof. We first need to bound c1.


(equation B.18 and equation 2.9) c1 = s

1 - 1/2S-1 -j

j=1


equation B.18  6S-1L1/23/2-1

1 - 1/2S-1 -j

j=1 -
 6S-1L1/23/2-1 2 1 - 1/2S-1

It

can

be

easily

verified

that

if

x



1 2

and

y



0,

then

(1 - x)-y



exp(2xy).

Using

this

fact

with

x = 1/2S-1 and y =  , we have:

 6S-1L1/23/2-1 2 exp  1/2S-1

(since





3/7

and

hence

 1/2S-1



1 )



S-1L1/23/2-1 2

× 6 exp

7

c1  7S-1L1/23/2-1 2

1 7

(B.23)

24

Under review as a conference paper at ICLR 2019

B.6 Proof of main theorem

We now analyze the coefficient of

f (y^k)

2 

-1

+

2L-1c1

-

1 ch
2

2-h

1 + 1 1/2L-1/2 2

(B.23

and

2.5)



-1

+

14S-1L-1L1/23/2-1 2

-

1 ch

1 + 1 1L-12

24



-1

+

14S-1L-1L1/23/2-1 2

-

1 ch

2

(definition

2.2

of

)

=

-1

+

14 -1

-

1 ch

81 2

(B.21, definition 2.5 of h) = -1

1

+

14 

-

(1 + ) + 21/2S-1

81

1 - 1 1/2L-1/2 2

(1/2L-1/2  0 and 1/2S-1  1)  -1

1

+

14 

-

(1

+

)

1

-

1 

81 2

= -1

14

+

1 

-

1

81 2 2

Lemma 17. positive.

(



1 )



0

2

The coefficient  22c1 + S-1L1/2-1/2 - (1 - ) of vk - yk 2 in 13 is non-

Proof.

22c1 + 1/2S-1 - (1 - )1/2S-1 (B.23)  142S-1L1/23/2-1 2 + 1/2S-1 - (1 - )1/2S-1

 14S-3L1/23/2-1 2 + 1/2S-1 - (1 - )1/2S-1

= 1/2S-1 14S-2L 2-1 + 2 - 1

Here the last inequality follows since   1 and   1/2S-1. We now rearrange the definition of 

to yield the identity:

S-2

=

1 94

L2L-3

-4

4

Using this, we have:

14S-2L 2-1 + 2 - 1

=

14 94

L2L-23



-2

+ 2

-1

 14 94

3 7

3
1-2

+

6

-

1



0

7

Here

the

last

line

followed

since

L



L,





3 7

,

and





1.

Hence

the

proof

is

complete.

Proof of Theorem 1. Using the master inequality 13 in combination with the previous Lemmas 14, 15, 16, and 17, we have:
Ek[k+1]  k = 1 - (1 - )1/2S-1 k

25

Under review as a conference paper at ICLR 2019

When we have:

1 - (1 - )1/2S-1

k


then the Lyapunov function k has decreased below 0 in expectation. Hence the complexity K( ) satisfies:
K( ) ln 1 - (1 - )1/2S-1 = ln( )

-1

K( ) =

ln(1/ )

ln 1 - (1 - )1/2S-1

Now

it

can

be

shown

that

for

0

<

x



1 2

,

we

have:

1 x

-

1



-1 ln(1 -

x)



1 x

-

1 2

-1 ln(1 -

x)

=

1 x

+

O(1)

Since

n



2,

we

have

 1/2 S -1



1 2

.

Hence:

1 K( ) = 1 - 

-1/2S + O(1)

ln(1/ )

An expression for KNU_ACDM( ), the complexity of NU_ACDM follows by similar reasoning.

KNU_ACDM( ) = -1/2S + O(1) ln(1/ )

(B.24)

Finally we have: 1 -1/2S + O(1)
K( ) = 1 -  -1/2S + O(1) KNU_ACDM( ) 1
= 1 -  (1 + o(1))KNU_ACDM( ) which completes the proof.

C Ordinary Differential Equation Analysis
C.1 Derivation of ODE for synchronous A2BCD
If we take expectations with respect to Ek, then synchronous (no delay) A2BCD becomes: yk = vk + (1 - )xk
Ekxk+1 = yk - n-1-1f (yk) Ekvk+1 = vk + (1 - )yk - n-1-1/2f (yk) We find it convenient to define  = n1/2. Inspired by this, we consider the following iteration:

yk = vk + (1 - )xk xk+1 = yk - s1/2-1/2-1f (yk) vk+1 = vk + (1 - )yk - s1/2-1f (yk)

(C.1) (C.2) (C.3)

26

Under review as a conference paper at ICLR 2019

C.1 Derivation of ODE for synchronous A2BCD

for coefficients:

 = 1 + s-1/2 -1  = 1 - s1/2-1

s is a discretization scale parameter that will be sent to 0 to obtain an ODE analogue of synchronous A2BCD. We first use equation B.6 to eliminate vk from from equation C.3.
0 = -vk+1 + vk + (1 - )yk - s1/2-1f (yk) 0 = --1yk+1 + -1(1 - )xk+1
+  -1yk - -1(1 - )xk + (1 - )yk - s1/2-1f (yk) (times by ) 0 = -yk+1 + (1 - )xk+1
+ (yk - (1 - )xk) + (1 - )yk - s1/2-1f (yk) = -yk+1 + yk( + (1 - )) + (1 - )xk+1 - xk(1 - ) - s1/2-1f (yk)

We now eliminate xk using equation C.1:
0 = -yk+1 + yk( + (1 - )) + (1 - ) yk - s1/2-1-1/2f (yk) - yk-1 - s1/2-1-1/2f (yk-1) (1 - ) - s1/2-1f (yk) = -yk+1 + yk( + (1 - ) + (1 - )) - (1 - )yk-1 + s1/2-1f (yk-1)( - 1)(1 - ) - s1/2-1f (yk) = (yk - yk+1) + (1 - )(yk - yk-1) + s1/2-1(f (yk-1)( - 1)(1 - ) - f (yk))

Now to derive an ODE, we let yk = Y ks1/2 . Then f (yk-1) = f (yk) + O s1/2 . Hence the above becomes:

0 = (yk - yk+1) + (1 - )(yk - yk-1) + s1/2-1(( - 1)(1 - ) - )f (yk) + O s3/2
0 = -s1/2Y - 1 sY¨ + (1 - ) s1/2Y - 1 sY¨ 22
+ s1/2-1(( - 1)(1 - ) - )f (yk) + O s3/2

(C.4)

27

Under review as a conference paper at ICLR 2019

C.2 Convergence proof for synchronous ODE

We now look at some of the terms in this equation to find the highest-order dependence on s.

(1 - ) = 1 - s1/2-1

1 1 - 1 + s-1/2

=

1 - s1/2-1

s-1/2 1 + s-1/2

s-1/2 - 1 = s-1/2 + 1

1 - s1/2-1 = 1 + s1/2-1

= 1 - 2s1/2-1 + O(s)

We also have:

( - 1)(1 - ) -  = (1 - ) - 1 = -2s1/2-1 + O(s)

Hence using these facts on equation C.4, we have:

0 = -s1/2Y - 1 sY¨ + 1 - 2s1/2-1 + O(s) s1/2Y - 1 sY¨ 22
+ s1/2-1 -2s1/2-1 + O(s) f (yk) + O s3/2 0 = -s1/2Y - 1 sY¨ + s1/2Y - 1 sY¨ - 2s1-1Y + O s3/2
22 -2s1-2 + O s3/2 f (yk) + O s3/2 0 = -sY¨ - 2s-1Y - 2s-2f (yk) + O s3/2 0 = -Y¨ - 2-1Y - 2-2f (yk) + O s1/2

Taking the limit as s  0, we obtain the ODE: Y¨ (t) + 2-1Y + 2-2f (Y ) = 0

C.2 Convergence proof for synchronous ODE

e--1tE (t) = f (Y (t)), Y (t) + -1f (Y (t))

1 +

Y (t) + Y (t), Y (t) + Y¨ (t)

+ -1 1

Y (t) + Y (t)

2

24

(strong convexity equation B.2)  f (Y ), Y + -1 f (Y ), Y - 1 -1 Y 2 2

1 +

Y + Y , -Y - 2-1f (Y )

+ -1 1

Y (t) + Y (t)

2

24

= - 1 -1 Y

2

-

1 

Y

20

44

28

Under review as a conference paper at ICLR 2019

C.3 Asynchronicity error lemma

Hence we have E (t)  0. Therefore E(t)  E(0). That is:

E(t) = en-1-1/2t

1 f (Y ) +

Y + Y

2

 E(0) = f (Y (0)) + 1 Y (0) + Y (0) 2

44

which implies:

1 f (Y (t)) +

Y (t) + Y (t)  e2 -n-1-1/2t

1 f (Y (0)) +

Y (0) + Y (0) 2

44

(C.5) (C.6)

C.3 Asynchronicity error lemma

This result is the continuous-time analogue of Lemma 11. First notice that c(0) = c0 and c( ) = 0.

We also have:

c

(t)/c0

=

-re-rt

-

re-rt 1

e-r - e-r

= -r

e-rt

+

e-rt

1

e-r - e-r

= -r

e-rt +

e-rt - 1

e-r 1 - e-r

e-r + 1 - e-r

c

(t)

=

-rc(t)

-

rc0

1

e-r - e-r

Hence using c( ) = 0:

t

A (t) = c0 Y (t) 2 +

c (t - s) Y (s) 2ds

t-

= c0

Y (t)

2

-

rA(t)

-

rc0

1

e-r - e-r

D(t)

Now

when

x



1 2

,

we

have

e-x 1-e-x



1 2

x-1.

Hence

when

r



1 2

,

we

have:

A (t)  c0

Y (t)

2

-

rA(t)

-

1 2



-1c0D(t)

and the result easily follows.

C.4 Convergence analysis for the asynchronous ODE

We consider the same energy as in the synchronous case (that is, the ODE in equation 3.1). Similar to before, we have:

e--1tE (t)  f (Y ), Y + -1 f (Y ), Y - 1 -1 Y 2 2

1 +

Y + Y , -Y - 2-1f

Y^

+ -1 1 Y (t) + Y (t) 2

24

= f (Y ), Y + -1 f (Y ), Y - 1 -1 Y 2 2

1 +

Y + Y , -Y - 2-1f (Y )

+ -1 1

Y (t) + Y (t)

2

24

- -1 Y + Y , f Y^ - f (Y )

= - 1 -1

Y

2

-

1 

Y

2 - -1 Y + Y , f Y^

- f (Y )

44

29

Under review as a conference paper at ICLR 2019

where the final equality follows from the proof in Section C.2. Hence

e--1tE (t)  - 1 -1 Y

2

-

1 

Y

2 + L-1 Y

Y^ - Y + L Y

44

Now we present an inequality that is similar to equation 8.

Lemma 18. Let A,  > 0. Then:

Y (t) - Y^ (t)

A

1  D(t)

+

1 -1A2

22

Y^ - Y

(C.7)

Proof. Since Y^ (t) is a delayed version of Y (t), we have: Y^ (t) = Y (t - j(t)) for some function j(t)  0

(though this can be easily generalized to an inconsistent read). Recall that for  > 0, we have

ab 

1 2

a2 + -1b2

.

Hence

t
X(t) - X^ (t) =

X (s)ds

s=t-j(t)

t
X(t) - X^ (t) A =

X (s)ds A

s=t-j(t)



1 

2

t

2
X (s)ds + 1 -1A2

s=t-j(t)

2

1t (Holder's inequality)  

X (s) 2ds

2 s=t-j(t)

t 1ds + 1 -1A2

s=t-j(t)

2



1 

2

t X (s) 2ds + 1 -1A2

s=t-j(t)

2

We use this lemma twice on Y Y^ - Y and Y Y^ - Y in equation C.7 with  = 2L, A = Y and  = 4L-1, A = Y respectively, to yield:

e--1tE (t)  - 1 -1 Y

2

-

1 

Y

2

44

+ L-1 L D(t) + 1 L-1 Y 2 4

=

-

1 

Y

2 + 3L2-1 D(t)

8

+ L 2L-1 D(t) + 1 L-1 Y 2 8

The proof of convergence is completed in Section 3.

D Optimality proof
For parameter set , L1, . . . , Ln, n, we construct a block-separable function f on the space Rbn (separated into n blocks of size b), which will imply this lower bound. Define i = Li/. We define

30

Under review as a conference paper at ICLR 2019

the matrix Ai  Rb×b via:

 2 -1 0



Ai

 

-1



 

0







2 ... ...

... ...
-1

... -1 2



0

  ,  

for

i

=

i1/2 i1/2

+ +

3 .
1

-1

 

0 -1 i

Hence we define fi on Rb via:

fi

=

Li - 4



1 2

x, Aix

-

e1, x

 +

x

2

2

which is clearly -strongly convex and Li-Lipschitz on Rb. From Lemma 8 of Lan & Zhou (2015), we know that this function has unique minimizer

Finally, we define f via:

x,(i)

qi, qi2, . . . , qib

,

for q =

1i /2 1i /2

- +

1 1

.

f (x)

n
fi x(i) .
i=1

Now let e(i, j) be the jth unit vector of the ith block of size b in Rbn. For I1, . . . , In  N, we define the subspaces

Vi(I) = span{e(i, 1), . . . , e(i, I)}, V (I1, . . . , In) = V1(I1)  . . .  Vn(In).

V (I1, . . . , In) is the subspace with the first I1 components of block 1 nonzero, the first I2 components of block 2 nonzero, etc. First notice that IC(V (I1, . . . , In)) = V (I1, . . . , In). Also, clearly, we have:

if (V (I1, . . . , In))  V (0, . . . , 0, min{Ii + 1, b}, 0, . . . , 0).

(D.1)

if is supported on the ith block, hence why all the other indices are 0. The patten of nonzeros in A means that the gradient will have at most 1 more nonzero on the ith block (see Nesterov (2013)).

Let the initial point x0 belong to V I¯1, . . . , I¯n . Let IK,i be the number of times we have had ik = i for k = 0, . . . , K - 1. By induction on condition 2 of Definition 4 using equation D.1, we have:

xk  V min I¯1 + Ik,1, b , . . . , min I¯n + Ik,m, b

Hence if x0,(i)  Vi(0) and k  b, then

b

xk,(i) - x,(i)

2  min
xVi (Ik,i )

x - x,(i)

2 = qi2j =
j =Ik,i +1

q 2Ik,i +2
i

-

qi2b+2

/

1 - qi2

Therefore for all i, we have:

E xk - x 2  E xk,(i) - x,(i) 2  E

q 2Ik,i +2
i

-

qi2b+2

/ 1 - qi2

31

Under review as a conference paper at ICLR 2019

To evaluate this expectation, we note:

Hence

k
Eqi2Ik,i =

k j

pji (1 - pi)k-j qi2j

j=0

k
= (1 - pi)k
j=0

k j

qi2pi(1 - pi)-1 j

= (1 - pi)k 1 + qi2pi(1 - pi)-1 k

= 1 - 1 - qi2 pi k

E xk - x 2  1 - 1 - qi2 pi k - qi2b qi2/ 1 - qi2 .

For any i, we may select the starting iterate x0 by defining its block j = 1, . . . , n via:

x0,(j) = (1 - ij )x,(j)

For such a choice of x0, we have

x0 - x 2 =

x,(i)

2

=

qi2

+

..

.

+

qi2b

=

qi2

1 - qi2b 1 - qi2

Hence for this choice of x0:

E xk - x 2/ x0 - x 2  1 - 1 - qi2 pi k - qi2b / 1 - qi2b

Now notice: 1 - 1 - qi2 pi k = qi-2 - qi-2 - 1 pi kqi2k  qi2k
Hence E xk - x 2/ x0 - x 2  1 - 1 - qi2 pi k 1 - qi2b-2k / 1 - qi2b
Now if we let b = 2k, then we have:

E xk - x 2/ x0 - x 2  1 - 1 - qi2 pi k 1 - qi2k / 1 - qi4k

= 1 - 1 - qi2 pi k/ 1 + qi2k

E

xk - x

2/ x0 - x

2



1 max

1-

2i

1 - qi2

pi

k

Now let us take the minimum of the right-hand side over the parameters pi, subject to The solution to this minimization is clearly:



n

pi = 1 - qi2 -1/

1 - qj2

-1


j=1

n i=1

pi

=

1.

32

Under review as a conference paper at ICLR 2019

Hence

 

-1k

E

xk - x

2/

x0 - x

2

1 1 -  2

n

1 - qj2

-1


 

j=1

n

1 - qj2

-1

=

1 4

n

i1/2 + 2 + -i 1/2

j=1

j=1





1 4

n

i1/2 + 2n

j=1

E

xk - x

2/

x0 - x

2

1 2

1-

Hence the complexity I( ) satisfies:

4

n j=1

1i /2

+

2n

k

 1 1- 2

4

n j=1

1i /2

+

2n

I( )

I( )  - ln 1 -

4

n j=1

i1/2

+

2n

-1
ln(1/2 )



1

=

(1 4

+

o(1))n

+

n

i1/2 ln(1/2 )

j=1

E Extensions

As mentioned, a stronger result than Theorem 1 is possible. In the case when Li = L for all i, we can consider a slight modification of the coefficients:

 1 + (1 + )-1/2S -1

(E.1)

 1 - (1 + )-11/2S-1

(E.2)

h

1 + 1 1/2L-1/2

-1
.

(E.3)

2

for the asynchronicity parameter:

yk = vk + (1 - )xk, xk+1 = yk - hL-1ik f (y^k), vk+1 = vk + (1 - )yk - -1/2L-1/2ik f (y^k).

 = 61/2n-1 × 

(E.4) (E.5) (E.6)
(E.7)

This leads to complexity:

K( ) = (1 + )n1/2 ln(1/ )

(E.8)

Here there is no restriction on  as in Theorem 1, and hence there is no restriction on  . Assuming

  1 gives optimal complexity to within a constant factor. Notice then that the resulting condition

of 

  1 n-1/2 6

(E.9)

33

Under review as a conference paper at ICLR 2019 now essentially matches the one in Theorem 3 in Section 3. While this result is stronger, it increases the complexity of the proof substantially. So in the interests of space and simplicity, we do not prove this stronger result.
34

