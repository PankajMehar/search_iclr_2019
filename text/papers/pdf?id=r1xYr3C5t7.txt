Under review as a conference paper at ICLR 2019
GRAPH2GRAPH NETWORKS FOR MULTI-LABEL CLAS-
SIFICATION
Anonymous authors Paper under double-blind review
ABSTRACT
Multi-label classification (MLC) is the task of assigning a set of target labels for a given sample. Modeling the combinatorial label interactions in MLC has been a long-haul challenge. Recurrent neural network (RNN) based encoder-decoder models have recently shown state-of-the-art performance for solving MLC. However, the sequential nature of modeling label dependencies through an RNN limits its ability in parallel computation, predicting dense labels, and providing interpretable results. In this paper, we propose Graph2Graph Networks, graph neural network models aiming to provide fast, accurate, and interpretable MLC. Graph2Graph networks replace all RNNs in the encoder-decoder architecture with graph attention mechanisms and dispense with autoregressive inference entirely. Our Graph2Graph decoder for MLC uses a modified graph attention network on an unknown and conditional graph to estimate how the labels and label interactions attend to the components of an input. Similarly, the Graph2Graph encoder adapts another graph attention network to learn representations about inputs via an unknown graph. This enables it to freely model interactions among input components and is not limited to sequential inputs. The proposed models are simple, fast, accurate, interpretable, and structure-agnostic (as long as input and output structures can each be described as graphs). Experiments on six real-world MLC datasets show the proposed models outperform autoregressive RNN models across five different metrics with a significant speedup during training and testing time.
1 INTRODUCTION
Multi-label classification (MLC) is receiving increasing attention in tasks such as text categorization and image classification. Scalable and accurate MLC methods are in urgent need for applications like automatically assigning topics to web articles, identifying objects in a web image, or finding binding locations of proteins on DNA. The most common and straightforward method for MLC is the Binary Relevance (BR) approach that considers multiple target labels independently (Tsoumakas and Katakis, 2006). However, in many MLC tasks there is a clear dependency structure among labels, which is excluded in BR methods.
Accordingly, probabilistic classifier chain (PCC) models were proposed to model label dependencies and formulate MLC in an autoregressive sequential prediction manner (Read et al., 2009; Dembczynski et al.). Recently Nam et al. (2017) replaced the classifier chain in PCC models with a recurrent neural network (RNN) sequence to sequence (Seq2Seq) method. However, in cases where the number of positive labels is large (i.e., dense labels), classifier chains struggle to accurately capture the long-range dependency among labels. Secondly, the inherently sequential nature of classifier chain models precludes parallelization within training and testing. Lastly, many real-world applications prefer interpretable predictors. For instance, in the task of DNA sequence based binding site prediction towards multiple transcription factors (labels), users of MLC care about how the prediction is created and how the interactions among labels influence the predictions.
In this work, we propose Graph2Graph Networks aiming to provide fast, accurate, and interpretable multi-label predictions. The key idea is to replace RNNs and to rely on attention mechanisms entirely to draw global dependencies among the labels and between labels and positions in an input. The proposed Graph2Graph networks allow for significantly more parallelization in training. Furthermore, to reduce the sequential inference, we propose to abandon the autoregressive prediction method for modelling label dependencies by relying solely on graph attention mechanisms and predict all MLC
1

Under review as a conference paper at ICLR 2019

c c1 31 2 c1 1
c c0 30 2 c0 1
GENC

y1

y 2

y 3

y4

y5

h2 3
h2 2

h2 4

h h2 2 15

h1 4
h1 5

h1 3

h h1 1 12

h0 4
h0 5

h0 3

Figure 1: Graph2Graph Net-
works. Given input x, we can
encode its components as initial input nodes {ci0} of encoder graph GENC. One hop
of GENC is shown on the left
side where its final node states {c1i } are passed to the decoder graph GDEC on the right side.
One hop of GDEC is shown
where its initial label nodes {hi0} first attend to the input nodes {ci1} to produce {hi1}, and then they attend to all
other label nodes to produce {h2i }. The final GDEC node states are then passed to a read-
out function to produce the
predicted labels {y^i}.

x1 x2 x3

hGDEC

0 1

h0 2

labels simultaneously. A more in depth overview of MLC and how Graph2Graph networks connect to previous works is shown in the Appendix. The main contributions of this paper are:

· Novel approach. Graph2Graph presents a new method for MLC using conditional graph attention. · Accurate MLC. Our model achieves similar, or better performance compared to the previous state
of the art across five different MLC metrics. We validate our model on six MLC datasets. · Fast. Empirically our model achieves an average 1.5x speedup over the autoregressive seq2seq
MLC at training time and an average 2.4x speedup over its testing time. · Interpretable. Although deep-learning based systems have widely been viewed as "black boxes"
due to their complexity, our attention based Graph2Graph models provide a straightforward way to
explain label to label and input to label dependencies.

In this paper we only show Graph2Graph networks in one task (text MLC). However, Graph2Graph networks are not limited to the datasets we chose. For example, they could be used on image inputs, or for different types of graph outputs. Our usage of the word "Graph2Graph" differs slightly from conventional phrasing such as Seq2Seq in which the decoder output is a sequence. Although our output is not a graph for our experiments, we assume a fully connected graph and learn a node specific weighting scheme on the edges to classify the nodes of the graph. It is straightforward to extend our model to the cases where the decoder generates a graph, such as drug molecule generation (Kadurin et al., 2017). Our emphasis of the term Graph2Graph is about representing the input and output space as two separate graphs, where the edges of both graphs are learned implicitly.

2 METHOD: GRAPH2GRAPH NETWORKS
2.1 GRAPH ENCODER-DECODER ARCHITECTURE FOR MLC
Given a set of input/output pairs D = {(xn, yn)}Nn=1 with inputs x  X and outputs y  Y , MLC involves the binary classification of L labels y1, y2, ..., yL where yi  {0, 1}. We assume that our inputs xn can be decomposed into a set of M components {x1, x2, ..., xM }. In our experiments, each component xi is a word from the input sentence. However, we want to emphasize that these can be any such component of a particular input (e.g. patches of an image, or tabular features). We represent each input component as a vector ci0 in a vector space of dimension d, where ·0 represents its initial state. For word inputs, we can represent each word in this space

2

Under review as a conference paper at ICLR 2019

by using a word embedding matrix Wx  R×d, where  is the dictionary size. Thus, each xn = {c10, c02, ..., c0M }, ci0  Rd. We emphasize that this assumption of component decomposition is not a drawback, but rather a way to learn and leverage the structure of the input space.
Similarly, the outputs yn are a set of L labels {y1, y2, ..., yL}. We can represent each label in the same vector space of dimension d using a label embedding matrix Wy  RL×d to produce initial label embeddings hi0. Each yn is then represented in this space as yn = {h10, h20, ..., h0L}, hi0  Rd.
Given these assumptions, we formulate MLC as a graph encoder-decoder architecture. The encoder is a graph GENC where input components {c0i } are nodes in the graph. Similarly, the decoder is a graph GDEC with label embeddings {h0i } as nodes, and GDEC is conditioned on GENC.
We propose the Graph2Graph model, which makes node level predictions on a learned label graph GDEC given the learned input graph GENC. The decoder graph GDEC updates its initial label representation nodes {hi0} conditioned on GENC in order to make binary classification predictions {y^1, y^2, ..., y^L}, y^i  {0, 1}. We first explain the decoder graph attention network assuming we have an encoder graph GENC and its processed nodes, {ci1}. An overview of our model is shown in fig. 1.

2.2 DECODER: GRAPH ATTENTION NETWORK (ON AN UNKNOWN AND CONDITIONAL GRAPH ABOUT LABELS)

The core of the Graph2Graph model lies within the decoder module. We assume that each label is a node in an unknown graph GDEC, conditioned on an encoder graph GENC. That is, each node corresponds to a label, encoded as a hidden vector representation. Initially, all label nodes are connected through a fully connected graph. Using attention mechanisms, we refine the graph edges to update the nodes.

Starting from an initial state of GDEC label nodes {h10, h20, ..., hL0 }, the decoder updates each label

node by attending first to the input nodes from GENC, and then to the other label nodes. One update

produces the decoder uses

new label a readout

node states function on

{h12, h22, ..., hL2 }. After T the final label node states

label {h2i }

node update steps or `hops', to predict each label as 0/1.

the

2.2.1 INPUT TO LABEL NODE ATTENTION

In order to update the label nodes given a particular input x, the label graph must condition on x. To do this, the decoder uses a weighted sum of each processed x embedding component cj1, which are nodes in graph GENC (method to produce processed nodes {c1j } explained in section 2.3). The weights of each input node cj1 for a particular label node, initially represented by h0i , are obtained via attention (Bahdanau et al., 2014). Attention coefficients for pairs of (h0i , cj1) nodes are computed using some attention function a(·), where we first apply learned linear transformations Wq and Wu
on the label and input nodes, respectively:

eij = a(Wqh0i , Wucj1),

(1)

where eij represents the importance ofinput node j for label node i. For our attention function, we used a scaled dot product, dividing by d to mitigate training issues (Vaswani et al., 2017),

a = (Wqhi0) (Wuc1j ) . d

(2)

Attention coefficients are then normalized across all words using a softmax function:

ij = softmaxj(eij) =

exp(eij )

M m=1

exp(eim)

.

(3)

Now the label nodes are updated by a linear combination of the input nodes using their attention weights and a learned linear transformation matrix Wv, with a residual connection on the current hi0,

M
h~ 1i = hi0 + ij Wjvcj1.

(4)

j=1

Lastly, we apply a feedforward layer to learn nonlinear node updates, with a residual connection,

h1i = h~ 1i + ReLU(Wif1h~ i1 + b1)Wif2 + b2.

(5)

3

Under review as a conference paper at ICLR 2019

2.2.2 LABEL TO LABEL NODE ATTENTION

After the first layer of the Graph2Graph decoder, the initial label nodes {hi0} have attended to the input nodes {ci1} to produce {hi1}. At this point, the decoder can make an independent prediction for each label conditioned on x. However, in order to make more accurate predictions, we model
interactions among the current label nodes and update the label nodes accordingly. To do this we use a second GDEC update layer, this time only on the current label node states {hi1}.
This layer is identical to the input-to-label node layer, except that it replaces the input nodes {ci1} with all other label nodes {hj1} for a particular h1i , j = i. That is, we update each h1i by a weighted sum of all other {h1j } based on the attention weights.

We compute the label-to-label attention weights using the same scaled dot product, and normalize

them across all nodes using a softmax. The label nodes are then updated by a linear combination of

the other labels using their normalized attention weights and then a feedforward layer is applied, with

residual connections on both,

 eij = (Wqhi1) (Wuh1j )/ d

(6)

ij =

exp(eij ) k=i exp(eik)

(7)

h~ 2i = h1i + ij Wvhj1

(8)

j=i

h2i = h~ i2 + ReLU(Wif1h~ i2 + b1)Wif2 + b2

(9)

Another way to perform label-to-label updates is with a graph neural network conditioned on a single embedding representation of x, such as the mean across all {c1i }. The advantage of the input-to-label attention in our model is that each label node can attend to different input nodes (e.g. different
words in the sentence). Regardless of the specific representation of x, the main difference between
Graph2Graphs and Graph Attention (GAT) models (Velickovic´ et al., 2017) is that the label-to-label
node updates are conditioned on an external input variable x.

Readout Layer (MLC Predictions from the Decoder) The last layer of the decoder predicts each label {y^1, ...y^L}. The outputs are L transformed label vectors h2i  Rd. Each of these are projected to a pre-sigmoid scalar for their respective label via a readout projection matrix Wo  RL×d. Each
scalar is then fed through a sigmoid function to produce probabilities of each label being positive:

y^i = sigmoid(Wiohi2).

(10)

The label embedding matrix weights Wy are shared with the readout projection matrix. In other words, the embedding matrix Wy is used to produce the initial node vectors for GDEC, and then is used again to produce the pre-sigmoid output values for each label, so Wo  Wy. Sharing the input and output weights using Wy further forces each label's embedding to correspond to that particular
label. This was shown beneficial in seq2seq models for machine translation (Press and Wolf, 2016).

In Graph2Graph networks we use binary cross entropy on the individual label predictions to train the

movoedrecl,11:bMutapn(dyio|v{e\ryie}m, bce11d:Mdi;nWg o)fiaslal pthperooxtihmeratleadbeblys

jointly {\yi}.

predicting

each

label

yi

using

attention

2.3 ENCODER: GRAPH ATTENTION NETWORK (ON AN UNKNOWN GRAPH ABOUT INPUT COMPONENTS)

So far, we have assumed that the input nodes from graph GENC have already been processed, and we have shown how to change the GDEC nodes conditioned on GENC. In theory, we could use any representation for the nodes in GENC, such as the outputs of a recurrent neural network model. However, we treat the encoder as a similar graph as the decoder, which updates the input nodes based on the other input nodes using attention. This is beneficial for several reasons. First, fully attentive models have shown superior performance and speed over recurrent models in NLP tasks (Vaswani et al., 2017). Second, not all inputs can be represented as a sequence, making recurrent models ineffective.For example, three of our MLC datasets use bag-of-word input representations (Bibtex, Delicious, Bookmarks). This makes using a graph representation, such as self attention suitable. Thus, we relax the constraint of a sequential input ordering by using a graph encoder, making our

4

Under review as a conference paper at ICLR 2019

model more flexible. For datasets which do have sequential ordering of the input components, we use add a positional encoding to the word embedding as used in Vaswani et al. (2017) (sine and cosine functions of different frequencies) to encode the location of each word in the sentence.

Starting with initial x component representations {ci0}, we update each c0i using attention across component node assuming a fully connected graph to produce c1i . We compute multiple update steps by using {c1i } as {c0i } for the next update. After T such update hops, the final input nodes {c1i } are used as the conditioning nodes for GDEC.

Similar to the decoder modules, we compute input node to node attention weights, and then update

each input node via a weighted sum over all other inputs:

 eij = (Wqci0) (Wucj0)/ d

(11)

ij =

exp(eij ) k=i exp(eik)

(12)

c~i = ci0 + ij Wvc0j
j=i

(13)

c1i = c~i + ReLU(Wif1c~i + b1)Wif2 + b2

(14)

2.4 MODEL DETAILS

Multi-head Attention. In order to allow a particular node to attend to multiple other nodes at once,
multiple attention heads are required. Inspired by Vaswani et al. (2017), we use K independent attention heads for each weight in the attention modules, where each weight matrix column Wj·,k is of dimension d/K. The generated representations are concatenated (denoted by ) and linearly transformed by learned matrix Wz  Rd×d. For example, multi-head attention in the encoder GENC:

ekij

=

(Wq,k ci0

) 

(Wu,k

cj0

)

d

c~i = ci0 +

K k=1

ij Wjv,kcj0
j=i

Wz

(15) (16)

Graph Hops. In order to learn more complex graph relations, we compute T "hops" of graph updates. This is essentially a stack of T updates of GENC and GDEC. For the decoder, we compute the input-to-label node and label-to-label node attention (equations 1-9) for T message passing hops, where h2i is used as h0i for each successive hop after T = 1.
Model Parameters. Matrices Wq, Wu, Wv, Wf1, Wf2, Wz, are not shared between input-toinput, input-to-label, and label-to-label attention, or across hops. Input embedding Wx and label embedding Wy are learned jointly with all other parameters during training.

2.5 ADVANTAGES OF GRAPH2GRAPH MODELS
Speed. Autoregressive models have been proven effective for machine translation and MLC (Sutskever et al.; Bahdanau et al., 2014; Nam et al., 2017). However, predictions must be made sequentially, eliminating parallelization and increasing translation time. Also, beam search is typically used at test time to find optimal translations. But beam search is limited by the time cost of large beams sizes, making it difficult to optimally translate long sequences (Koehn and Knowles, 2017).
In Graph2Graph models, the joint probability of labels isn't explicitly estimated using the chain rule, but making predictions in parallel decreases test time drastically, especially when the number of labels is large. We instead model the joint probability implicitly using the Graph2Graph decoder, at the benefit of a substantial speedup. The time complexities of different types of models are compared in Table 2. The biggest advantage of Graph2Graph networks is constant training and testing times.
Handling dense label predictions. Autoregressive models are well suited for machine translation because these models mimic the sequential decoding process of real translation. However, for MLC, the output labels have no intrinsic ordering. While the joint probability of the output labels is

5

Under review as a conference paper at ICLR 2019

independent of the label ordering (equations 18 and 19), the chosen ordering can make a difference in practice (Vinyals et al., 2015; Nam et al., 2017). Some ordering of labels must be used during training, and this chosen ordering can lead to unstable predictions at test time.
In addition to speed constraints, beam search for autoregressive inference introduces a second drawback: initial wrong predictions will propagate when using a modest beam size (e.g. most models use a beam size of 5). This can lead to significant decreases in performance when the number of positive labels is large. For example, the Delicious dataset has a median of 19 positive labels per sample, and it can be very difficult to correctly predict the labels at the end of the prediction chain.
The Graph2Graph model removes the dependency on a training set label ordering and on beam search to predict outputs. This is particularly beneficial when the number of positive output labels is large (i.e. dense). In addition, the Graph2Graph model predicts the output set of labels all at once, which is made possible by the fact that inference doesn't use a probabilistic chain, but there is still a representation of label dependencies via label to label attention. As an additional benefit, as noted by Belanger and McCallum (2016), it may be useful to maintain `soft' predictions for each label, for example in detection problems. This is a major drawback of the PCC models which make `hard' predictions of the positive labels, defaulting all other labels to 0.
Interpretability. One advantage of Graph2Graph models is that interpretability is "built in" via graph attention. Specifically, we can visualize 3 aspects: input-to-input (word dependencies), input-to-label (label/word dependencies), and label-to-label attention (label dependencies).

3 CONNECTING TO RELATED TOPICS

Structured Output Predictions The use of graph attention in Graph2Graph models is closely

connected to the literature of structured output prediction for MLC. Ghamrawi and McCallum

(2005) used conditional random fields (Lafferty et al., 2001) to model dependencies among labels

and features for MLC. Their model learns the following distribution:

Z

1 (x)

exp

k kfk(x, y) +

k k fk (x, y) , where k is an index over features corresponding to a label and input component pair, and k is an index over features corresponding to a triple of an input component and label-label

pairs.  are learned parameters for each feature pair or triple, and f are the features, and Z(x) is

the normalizing constant. Graph2Graph also models the dependencies between label-label pairs to

features through deep-learning based attention and embedding mechanisms.

In another research direction, recently proposed SPENs (structured prediction energy network (Belanger and McCallum, 2016; Tu and Gimpel, 2018)) and Deep Value Networks (Gygli et al., 2017) tackled MLC by optimizing different variants of structured loss formulations. As a future direction, we plan to improve Graph2Graph models by integrating a similar structured loss in our formulation.

Seq2Seq Models In machine translation (MT), sequence-to-sequence (Seq2Seq) models have proven to be the superior method, where an encoder reads the source language sentence into the encoder hidden state, and a decoder translates the encoder hidden state into the target sentence. Sutskever et al. introduced the neural Seq2Seq model where an encoder RNN produced the encoder hidden state vector which was used as the first hidden state of the decoder RNN to predict each word in an autoregressive manner. Seq2Seq models approximate equation 19 (in Section 6.1) using an RNN. Bahdanau et al. (2014) improved this model by introducing "neural attention" which allows the decoder RNN to "attend" to every encoder word at each step of the autoregressive translation.

RNN Seq2Seq autoregressive models have shown to be useful for MLC, where RNNs are used for PCC prediction (Wang et al., 2016; Nam et al., 2017). Since MLC outputs are a set, Seq2Seq autoregressive models use some pre-defined ordering of the labels during training. An RNN Seq2Seq model with attention showed state of the art subset accuracy on MLC for text Nam et al. (2017).

Vaswani et al. (2017) eliminated the need for the recurrent network in MT by introducing the Transformer. Instead of using an RNN to model dependencies, the Transformer explicitly models pairwise dependencies among all of the features by using attention (Bahdanau et al., 2014; Xu et al.) between signals. This speeds up training time because RNNs can't be fully parallelized. However, the transformer still uses an autoregressive decoder, which is slow at test time. The node update steps in our Graph2Graph model are similar to the hidden state updates in the Transformer, except that we model the output labels as a graph, rather than a sequence, allowing us to make all decoder node update steps simultaneously. Our Graph2Graph Autoregressive model follows the Transformer more closely where we predict each positive label in an autoregressive manner.

6

Under review as a conference paper at ICLR 2019
Gu et al. (2017) removed the autoregressive decoder in MT with the Non-Autoregressive Transformer. In this model, the encoder makes a proxy prediction, called "fertilities", which are used to predict all translated words at once. The difference between their model and ours is that we have a constant label at each position, so we don't need to marginalize over all possible labels at each position.
Graph Neural Networks and Graph Attention Graph neural networks (GNNs) were first introduced to process data in known, directed acyclic graphs (Gori et al.; Scarselli et al., 2009). GNNs update node states until equilibrium and then a neural network predicts an output for each node.
There have since been many works using the basic GNN framework to update nodes using message passing with various readout functions on the graph (Kipf and Welling, 2016; Hamilton et al., 2017; Duvenaud et al., 2015; Li et al., 2015; Gilmer et al., 2017; Battaglia et al., 2016; Kearnes et al., 2016). However, none of these have formulated GNNs using an encoder/decoder framework where the GNN used for output prediction is conditioned on another GNN (GENC). Velickovic´ et al. (2017) introduced the Graph Attention Network for node classification that computes node updates by self attention on neighborhood nodes in a pre-specified graph structure. The main difference between the Graph2Graph node self attention and the Graph Attention Network, is that the label to label (i.e. node to node) updates are conditioned on the input x, also encoded in a GNN layer. Additionally, our label interaction graph is completely unknown during training, therefore we allow every node to attend on every other node. Thus, Graph2Graph needs only a minimum of 2 hops of the node update steps to incorporate information from the entire graph using only pairwise terms.
4 EXPERIMENTS
In the appendix, we have added the details of our datasets (6.3), evaluation metrics (6.4), and previous work baseline models (6.5).
4.1 EXPERIMENTAL DETAILS
For all 6 datasets, we use the same Graph2Graph model with T =3 hops, d = 512, K=4 attention heads. We trained our model on an NVIDIA TITAN X Pascal with a batch size of 32. We used Adam (Kingma and Ba, 2014) with betas= (0.9, 0.999), eps=1e-08, and a learning rate of 0.0002 for each dataset. We used dropout of p = 0.2 for all models. The Graph2Graph models also use layer normalization (Ba et al., 2016) around each of the the attention and feedforward layers.
We explain our own Graph2Graph variations and baselines here, and previous work baselines in section 6.5. The BR models are trained with binary cross-entropy on each label and the autoregressive models with cross entropy across all possible labels at each position. All of our autoregressive models predict only the positive labels before outputting a stop signal. This is a special case of PCC models (explained as PCC+ in section 6.1), which have been shown to outperform the binary prediction of each label in terms of performance and speed. These models use beam search at inference time with a beam size of 5. For the non-autoregressive models, to convert the labels to {0, 1} we chose the best threshold on the validation set from the same set of thresholds used in Tu and Gimpel (2018).
Graph2Graph: In the full Graph2Graph model, we use 3 encoder hops and 3 decoder hops with label to label attention in both the encoder and decoder.
Graph2Graph Edgeless GDEC: The same as Graph2Graph except that there are no label-to-label attention updates. That is, GDEC contains no edges (edgeless), which assumes labels are independent. Graph2Graph Autoregressive: Instead of predicting all labels simultaneously and modelling label dependencies using graph attention, Graph2Graph Autoregressive networks model p(yi|y0:i-1, x1:M ; W ) by predicting each positive label yi sequentially and using attention on {ci1} and the previously predicted labels (y^0, ..., y^i-1) until a stop signal is predicted (see PCC+ in Appendix). While it is possible to predict the probability for each label (i.e. regular PCC), this is extremely time consuming when the number of possible labels is large (and in general, doesn't increase performance (Nam et al., 2017)).
Graph2MLP: The mean node output of a 3 hop GENC is used as the input to a 2 layer multilayer perceptron (MLP) readout module. This model assumes no dependencies among the outputs.
7

Under review as a conference paper at ICLR 2019

Dataset Reuters
Bibtex
Delicious
Bookmarks
RCV1 TFBS

Model
Graph2Graph Graph2Graph Edgeless GDEC Graph2Graph Autoregressive Graph2MLP Seq2Seq Autoregressive Seq2Seq Autoregressive (Nam et al) RNN + RNN_b (Nam et al) MLP (Nam et al)
Graph2Graph Graph2Graph Edgeless GDEC Graph2Graph Autoregressive Graph2MLP Seq2Seq Autoregressive InfNet SPEN (Tu & Gimpel) SPEN (Belanger & McCallum) MLP (Belanger & McCallum)
Graph2Graph Graph2Graph Edgeless GDEC Graph2Graph Autoregressive Graph2MLP Seq2Seq Autoregressive InfNet SPEN (Tu & Gimpel) SPEN (Belanger & McCallum) MLP (Belanger & McCallum)
Graph2Graph Graph2Graph Edgeless GDEC Graph2Graph Autoregressive Graph2MLP Seq2Seq Autoregressive InfNet SPEN (Tu & Gimpel) SPEN (Belanger & McCallum) MLP (Belanger & McCallum)
Graph2Graph Graph2Graph Edgeless GDEC Graph2Graph Autoregressive Graph2MLP Seq2Seq Autoregressive Seq2Seq Autoregressive (Nam et al)1 MLP (Nam et al)
Graph2Graph Graph2Graph Edgeless GDEC Graph2Graph Autoregressive Graph2MLP Seq2Seq Autoregressive

ACC 0.834 0.828 0.835 0.824 0.837 0.828 0.676 0.750 0.178 0.176 0.197 0.168 0.195 0.007 0.007 0.014 0.005 0.008 0.247 0.245 0.252 0.241 0.273 0.655 0.650 0.660 0.640 0.655
0.680 0.584 0.038 0.027 0.057 0.035 0.114

HA 0.997 0.997 0.997 0.997 0.996 0.996 0.993 0.995 0.987 0.987 0.984 0.987 0.985 0.982 0.982 0.973 0.982 0.980 0.991 0.991 0.988 0.991 0.990 0.993 0.993 0.992 0.992 0.992
0.993 0.991 0.961 0.962 0.963 0.961 0.961

ebF1 0.902 0.898 0.899 0.891 0.900 0.894 0.718 0.840 0.443 0.416 0.432 0.410 0.393 0.422 0.422 0.389 0.368 0.332 0.322 0.354 0.320 0.375 0.375 0.378 0.394 0.350 0.347 0.382 0.362 0.376 0.344 0.338 0.881 0.883 0.881 0.870 0.880
0.890 0.844 0.209 0.077 0.201 0.203 0.249

miF1 0.871 0.873 0.868 0.870 0.861 0.858 0.714 0.818 0.444 0.449 0.419 0.430 0.384 0.380 0.358 0.325 0.378 0.329 0.370 0.354 0.304 0.364 0.329 0.871 0.875 0.868 0.864 0.865
0.884 0.840 0.325 0.238 0.354 0.315 0.311

maF1 0.503 0.495 0.496 0.463 0.496 0.457 0.090 0.308 0.362 0.329 0.315 0.355 0.282 0.200 0.187 0.167 0.195 0.166 0.277 0.256 0.209 0.275 0.237 0.705 0.721 0.715 0.642 0.698
0.738 0.657 0.238 0.139 0.245 0.147 0.199

Average 0.822 0.818 0.819 0.809 0.818 0.807 0.638 0.742 0.483 0.472 0.470 0.470 0.448 0.388 0.373 0.360 0.383 0.361 0.456 0.439 0.420 0.451 0.438 0.821 0.824 0.823 0.802 0.818
0.837 0.783 0.354 0.288 0.364 0.332 0.367

Train Time 0.788 (1.5x) 0.738 0.618 0.595 1.187 0.376 (2.1x) 0.31 0.206 0.193 0.778 3.172 (1.1x) 2.965 1.158 0.502 3.349 9.664 (1.2x) 8.501 7.146 8.022 11.863 98.346 90.142 40.366 69.957 119.82
187.14 (2.5x) 126.83 163.04 111.33 459.41

Test Time 0.116 (2.1x) 0.111 0.374 0.102 0.242 0.08 (2.1x) 0.013 0.208 0.011 0.165 0.473 (3.2x) 0.316 5.129 0.043 1.497 1.849 (1.3x) 1.5345 5.74 1.312 2.411 1.003 0.884 1.928 0.593 1.727
13.048 (4.2x) 12.295 114.87 7.715 54.651

Table 1: Results. Across all 6 datasets, Graph2Graph produces similar or better average metric scores to autoregressive models, at a fraction of the training and testing time. Graph2Graph results in a 1.5x and 2.4x training and testing speedup over the previous state-of-the-art probabilistic MLC method which uses recurrent networks. Speedups over Seq2Seq Autoregressive model are shown in parentheses for the Graph2Graph model. Train and Test Times shown in minutes per epoch.

Seq2Seq Autoregressive: A 3-layer Seq2Seq Autoregressive model with attention. This is similar to Seq2Seq Autoregressive from Nam et al. (2017) (RNNm in their paper), except with 3 layers.

4.2 RESULTS

Across all datasets, Graph2Graph outperforms or achieves similar results as the baseline models. Most importantly, we show that autoregressive models are not crucial in MLC for most metrics, and non-autoregressive models result in a significant speedup at test time.
Accuracy Table 1 shows the performance of different models across the 6 datasets. For subset accuracy, autoregressive models perform the best, but at a small margin of increase. More importantly, autoregressive models that predict only positive labels are targeted at maximizing subset accuracy, but they perform poorly on other metrics. For all other metrics, autoregressive models are not essential.
One important observation is that for most datasets, Graph2Graph outperforms the autoregressive models in both miF1 (frequent labels) and more importantly, maF1 (rare labels). Since maF1 favors models which can predict the rare labels, this shows that autoregressive models with beam search often make the wrong predictions on the rare labels (which are ordered last in the sequence during
1Nam et al. (2017) pretrain the label embeddings using word2vec, and we start from scratch. Additionally, our RCV1-V2 datasets may be pre-processed slightly differently as theirs is not publicly available

8

Under review as a conference paper at ICLR 2019
training). Graph2Graph is a solid choice across all metrics as it comes the closest to subset accuracy as the autoregressive models, but also performs well in other metrics. While Graph2Graph does not explicitly model label dependencies as autoregressive or structured prediction models do, it seems as though the attention weights do learn some dependencies among labels (as shown in the visualizations). This is indicated by the fact that Graph2Graph, which use label-to-label attention, mostly outperforms the ones which don't, indicating that it is learning label dependencies. Table 1 shows 3 hop models results, but a comparison of hops is shown in figure 2. Larger datasets (RCV1-V2 and TFBS) are difficult to make accurate predictions on the joint labels without an autoregressive model, but Graph2Graph outperforms the MLP models, while achieving similar speed. Speed Table 1 shows the per epoch train and test times for each model. All models are trained and tested on the same GPU using a batch size of 32. At test time, since the autoregressive model cannot be parallelized, Graph2Graph and other non-autoregressive models are significantly faster. During training, the autoregressive model can be parallelized because the true labels are fed as the previous label. Since the autoregressive model only predicts the  positive labels, it is faster at training time, whereas the Graph2Graph model is predicting the probability for all labels. Visualizations We present visualizations of the input-to-label and label-to-label attention weights (averaged across the 4 attention heads) in the Appendix. In the visualizations, we show the positive labels only, and the darker lines show higher attention weights to the corresponding label or word. The attention weights clearly learn certain relationships between input-label pairs as well as the label-label pairs, which is all done in an unsupervised manner. In future work, we plan to add a structured prediction loss function which will likely improve the attention mechanisms and the model's ability to estimate the joint probability.
5 CONCLUSION
In this work we present Graph2Graph networks which achieve a significant speedup at close to the same performance as autoregressive models for MLC. We open a new avenue of using graph attention to model label dependencies in MLC tasks, showing that graph attention on labels is almost all you need for modelling output dependencies. We plan to explore further modelling of the joint probability, while still using the Graph2Graph framework. It is important to point out that in the current paper, Graph2Graph does not aim to predict a graph as output. Instead, we use an unseen output graph to model the interactions among labels. One of our future extensions is to adapt the current model to predict more structured outputs.
9

Under review as a conference paper at ICLR 2019
REFERENCES
Grigorios Tsoumakas and Ioannis Katakis. Multi-label classification: An overview. International Journal of Data Warehousing and Mining, 3(3), 2006.
Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. Classifier chains for multi-label classification. Machine Learning and Knowledge Discovery in Databases, pages 254­269, 2009.
Krzysztof Dembczynski, Weiwei Cheng, and Eyke Hüllermeier. Bayes optimal multilabel classification via probabilistic classifier chains.
Jinseok Nam, Eneldo Loza Mencía, Hyunwoo J Kim, and Johannes Fürnkranz. Maximizing subset accuracy with recurrent neural networks in multi-label classification. In Advances in Neural Information Processing Systems, pages 5419­5429, 2017.
Artur Kadurin, Sergey Nikolenko, Kuzma Khrabrov, Alex Aliper, and Alex Zhavoronkov. drugan: an advanced generative adversarial autoencoder model for de novo generation of new molecules with desired molecular properties in silico. Molecular pharmaceutics, 14(9):3098­3104, 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000­6010, 2017.
Petar Velickovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104­3112. URL http://papers.nips.cc/paper/ 5346-information-based-learning-by-agents-in-unbounded-state-spaces. pdf.
Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. arXiv preprint arXiv:1706.03872, 2017.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015.
David Belanger and Andrew McCallum. Structured prediction energy networks. In International Conference on Machine Learning, pages 983­992, 2016.
Nadia Ghamrawi and Andrew McCallum. Collective multi-label classification. In Proceedings of the 14th ACM international conference on Information and knowledge management, pages 195­200. ACM, 2005.
John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. 2001.
Lifu Tu and Kevin Gimpel. Learning approximate inference networks for structured prediction. arXiv preprint arXiv:1803.03376, 2018.
Michael Gygli, Mohammad Norouzi, and Anelia Angelova. Deep value networks learn to evaluate and iteratively refine structured outputs. arXiv preprint arXiv:1703.04363, 2017.
Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, and Wei Xu. Cnn-rnn: A unified framework for multi-label image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2285­2294, 2016.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pages 2048­2057. URL http://www.jmlr.org/proceedings/ papers/v37/xuc15.pdf.
Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.
10

Under review as a conference paper at ICLR 2019
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Neural Networks, 2005. IJCNN'05. Proceedings. 2005 IEEE International Joint Conference on, volume 2, pages 729­734. IEEE.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61­80, 2009.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.
William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584, 2017.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán AspuruGuzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pages 2224­2232, 2015.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pages 4502­4510, 2016.
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):595­608, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
Andrew McCallum. Multi-label text classification with a mixture model trained by em.
Naonori Ueda and Kazumi Saito. Parametric mixture models for multi-labeled text. In Advances in neural information processing systems, pages 737­744, 2003.
André Elisseeff and Jason Weston. A kernel method for multi-labelled classification. In Advances in neural information processing systems, pages 681­687, 2002.
Grigorios Tsoumakas and Ioannis Vlahavas. Random k-labelsets: An ensemble method for multilabel classification. In European conference on machine learning, pages 406­417. Springer, 2007.
Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. Classifier chains for multi-label classification. Machine learning, 85(3):333, 2011.
Min-Ling Zhang and Zhi-Hua Zhou. A k-nearest neighbor based algorithm for multi-label classification. In Granular Computing, 2005 IEEE International Conference on, volume 2, pages 718­721. IEEE, 2005.
Shantanu Godbole and Sunita Sarawagi. Discriminative methods for multi-labeled classification. In Pacific-Asia conference on knowledge discovery and data mining, pages 22­30. Springer, 2004.
Elena Montañes, Robin Senge, Jose Barranquero, José Ramón Quevedo, Juan José del Coz, and Eyke Hüllermeier. Dependent binary relevance models for multi-label classification. Pattern Recognition, 47(3):1494­1508, 2014.
Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured data. In International Conference on Machine Learning, pages 2702­2711, 2016.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. Mining multi-label data. In Data mining and knowledge discovery handbook, pages 667­685. Springer, 2009.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. Effective and efficient multilabel classification in domains with large number of labels.
11

Under review as a conference paper at ICLR 2019 Ioannis Katakis, Grigorios Tsoumakas, and Ioannis Vlahavas. Multilabel text classification for automated tag
suggestion. David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection for text
categorization research. Journal of machine learning research, 5(Apr):361­397, 2004. ENCODE Project Consortium et al. An integrated encyclopedia of dna elements in the human genome. Nature,
489(7414):57, 2012.
12

Under review as a conference paper at ICLR 2019

6 APPENDIX:

6.1 BACKGROUND OF MULTI-LABEL CLASSIFICATION:

MLC has a rich history in text (McCallum; Ueda and Saito, 2003), images (Tsoumakas and Katakis, 2006; Elisseeff and Weston, 2002), bioinformatics (Tsoumakas and Katakis, 2006; Elisseeff and Weston, 2002), and many other domains. MLC methods can roughly be broken into several groups, which are explained as follows.

Label powerset models (LP) (Tsoumakas and Vlahavas, 2007; Read et al., 2011), classify each input into one label combination from the set of all possible combinations Y = {{1}, {2}, ..., {1, 2, ..., L}}. LP explicitly
models the joint distribution by predicting the one subset of all positive labels. Since the label set Y grows exponentially in the number of total labels (2L), classifying each possible label set is intractable for a modest L.
In addition, even in small L tasks, LP suffers from the "subset scarcity problem" where only a small amount of
the label subsets are seen during training, leading to bad generalization.

Binary relevance (BR) methods predict each label separately as a logistic regression classfier for each label (Zhang and Zhou, 2005; Godbole and Sarawagi, 2004). The naïve approach to BR prediction is to predict all labels independently of one another, assuming no dependencies among labels. That is, BR uses the following conditional probability parameterized by learned weights W :

L
PBR(Y |X; W ) = p(Yi|X1:M ; W )
i=1

(17)

Probabilistic classifier chain (PCC) methods (Dembczynski et al.; Read et al., 2009) are autoregressive models that estimate the true joint probability of output labels given the input by using the chain rule, predicting one label at a time:

L
PP CC (Y |X; W ) = p(Yi|Y1:i-1, X1:M ; W )
i=1

(18)

Two issues with PCC models are that inference is very slow if L is large, and the errors propagate as L increases (Montañes et al., 2014). To mitigate the problems with both LP and PCC methods, one solution is to only predict the true labels in the LP subset. In other words, only predicting the positive labels (total of  for a particular sample) and ignoring all other labels, which we call PCC+. Similar to PCC, the joint probability of PCC+ can be computed as product of conditional probabilities, but unlike PCC, only  < L terms are predicted as positive:



PP CC+(Y |X; W ) = p(Ypi |Yp1:i-1 , x1:M ; W )
i=1

(19)

This can be beneficial when the number of possible labels L is large, reducing the total number of prediction

steps. However, in both PCC and PCC+, inference is done using beam search, which is a costly dynamic

programming step to find the optimal prediction.

Recently, Nam et al. (2017) showed that, across several metrics, state-of-the-art MLC results could be achieved by using a recurrent neural network (RNN) based encoder-to-decoder framework for Equation 19 (PCC+). They use a Seq2Seq RNN model (Seq2Seq Autoregressive) which uses one RNN to encode x, and a second RNN to predict each positive label sequentially, until it predicts a `stop' signal. This type of model seeks to maximize the `subset accuracy', or correctly predict every label as its exact 0/1 value. But, this model suffers from two important drawbacks: (1) test time is slow because the outputs cannot be parrallelized due to the autoregressive nature, (2) correct modelling of the joint output space using beam search is both slow and prone to errors which propagate.

Our goal is to achieve near the performance of PCC methods (Equations 18 and 19), at the test speed of BR methods (Equation 17). Graph2Graph models provide an approximation of the following factored formulation, where N (Yi) denotes the neighboring nodes of label node yi:

L
PG2G(Y |X; W ) = p(Yi|{YN (Yi)}, x1:M ; W )
i=1

(20)

Using graph attention to pass embedding from node to neighbor node connects to a large body of literature researching on graph neural networks and embedding models for structures. For instance Dai et al. (2016) proposed a so called structure2vec framework, using the idea of embedding latent variable models into feature spaces. The paper provides proofs that the proposed structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. The key idea is that a graphical model is built for each individual data point and the method uses the graph structure of the input data directly as the conditional independence structure of an undirected graphical

13

Under review as a conference paper at ICLR 2019

Figure 2: Average Across Metrics for T =1, T =2, and T =3 GDEC Hops. In these experiments, the encoder GENC is processed with a fixed T =3 hops, and the decoder hops are varied.
model. Then instead of conducting probabilistic operations (such as sum, product or re-normalization), the proposed model performs nonlinear function mappings in each step to learn feature representations of structured components. Recent neural message passing (Gilmer et al., 2017), graph attention networks (Velickovic´ et al., 2017) and neural relation models (Battaglia et al., 2016) follow the similar idea to pass the embedding from node to neighbor nodes or neighbor edges.
6.2 COMPLEXITY COMPARISON

Model
Seq2Seq Autoregressive Graph2Graph Edgeless GDEC Graph2Graph

Sequential Operations (Training)
O(M + ) O(1) O(1)

Autoregressive Steps (Testing)
O() O() O(1)

Layer
Complexity
(Encoder) O(M · d2) O(M 2 · d) O(M 2 · d)

Layer
Complexity
(Decoder) O( · d2) O( · d2) O(L2 · d)

Table 2: Model Complexities. M represents the input sequence length,  represents the number of positive labels, L represents total number of labels, and d is the size of the model's hidden state.

6.3 DATASETS
We test our method against baseline methods on 6 different multi-label sequence classification datasets. The datasets are summarized in Table 3 in the Appendix. We use Reuters-21578, Bibtex (Tsoumakas et al., 2009), Delicious (Tsoumakas et al.), Bookmarks (Katakis et al.), RCV1-V2 (Lewis et al., 2004), and our own DNA protein binding dataset (TFBS) from Consortium et al. (2012). As shown in the table, each dataset has a varying number of samples, number of labels, positive labels per sample, and samples per label. For BibTex and Delicious, we use 10% of the provided training set for validation. For the TFBS dataset, we use 1 layer of convolution at the first layer to extract "words" from the DNA characters (A,C,G,T), as commonly done in deep learning models for DNA.

6.4 EVALUATION METRICS

Multi-label classification methods can be evaluated with many different metrics which each evaluate different strengths or weaknesses. We use the same 5 evaluation metrics from Nam et al. (2017).

Example-based measures are defined by comparing the target vector y to the prediction vector y^. Subset

Accuracy (ACC) requires an exact match of the predicted labels and the true labels: ACC(y, y^) = I[y = y^].

Hamming

Accuracy

(HA) evaluates

how

many labels

are

correctly

predicted

in y^:

HA(y, y^)

=

1 L

L j=1

I[yj

=

y^j]. Example-based F1 (ebF1) measures the ratio of correctly predicted labels to the the sum of the total true and

predicted labels:

.2

L j=1

yj

y^j

L j=1

yj

+

L j=1

y^j

Label-based measures treat each label yj as a separate two-class prediction problem, and compute the number of true positives (tpj), false positives (f pj), and false negatives (f nj) for a label. Macro-averaged F1 (maF1)

14

Under review as a conference paper at ICLR 2019

measures the label-based F1 averaged over all labels:

1 L

.L 2tpj
j=1 2tpj +f pj +f nj

Micro-averaged F1 (miF1)

measures the label-based F1 averaged over each sample:

.L
j=1

2tpj

L j=1

2tpj

+f

pj

+f

nj

High maF1 scores usually

indicate high performance on less frequent labels. High miF1 scores usually indicate high performance on more

frequent labels.

6.5 BASELINE COMPARISONS
For Reuters and RCV1-V2, we compare against the MLP and autoregressive model used in (Nam et al., 2017), which use these two datasets. MLP: standard MLP without the self attention input representation. Seq2Seq Autoregressive: RNN PCC+ autoregressive model from (Nam et al., 2017), which is a GRU encoder/decoder model with pretrained word embeddings. We omit the binary relevance RNN since it was shown that it doesn't perform well and is extremely slow.
For BibTex, Delicious, Bookmarks, we compare against the models from Belanger and McCallum (2016) and Tu and Gimpel (2018), which use these three datasets. They only evaluate using ebF1, so this is the only metric for which we compare our model with theirs. MLP (Belanger and McCallum, 2016): Standard 2-layer MLP. SPEN (Belanger and McCallum, 2016): original structured prediction energy network. InfNet SPEN (Tu and Gimpel, 2018): improved SPEN model, which uses an inference network to estimate y^.

Dataset
Reuters RCV1-V2 BibTex Delicious Bookmarks TFBS

#Train
6,993 703,135 4,377 11,597 48,000 1,671,873

#Val
777 78,126 487 1,289 12,000 301,823

#Test
3,019 23,149 2,515 3,185 27,856 323,796

Labels (L)
90 103 159 983 208 179

Mean Labels /Sample
1.23 3.21 2.38 19.06 2.03 7.62

Median Labels /Sample 1 3 2 20 1 2

Max Labels /Sample 15 17 28 25 44 178

Mean Samples /Label 106.50 24,362.15 72.79 250.15 584.67 84,047.43

Median Samples /Label 18 7,250 54 85 381 45,389

Max Samples /Label 2,877 363,991 689 5,189 4,642 466,876

Table 3: Dataset Statistics

6.6 VISUALIZATIONS

15

Under review as a conference paper at ICLR 2019

wheat soybean oilseed
grain corn
Label1

zinc yen wpi *wheat veg-oil trade tin tea sunseed sun-oil sun-meal sugar strategic-metal soy-oil soy-meal *soybean sorghum silver ship rye
rubber rice retail
reserves rapeseed rape-oil
rand propane potato platinum pet-chem palm-oil palmkernel palladium orange
*oilseed oat nzdlr nkr
nickel nat-gas naphtha money-supply money-fx meal-feed lumber livestock
lin-oil lei lead
l-cattle jobs jet
iron-steel ipi
interest instal-debt
income housing
hog heat groundnut-oil groundnut *grain gold gnp gas
fuel earn dmk dlr dfl crude cpu cpi cotton-oil cotton *corn copra-cake copper
coffee coconut-oil
coconut cocoa castor-oil carcass bop barley alum
acq
Label2

Figure 3: Label-To-Label Attention Weights. On the left are the positive labels, and on the right are all labels. These are taken from the layer 2 Label-To-Label attentions on a sample from Reuters-21578 (same input sample as shown in 4.

16

Under review as a conference paper at ICLR 2019

wheat soybean oilseed
grain corn

wheat week usda u.s. transactions total tonnes
to the soybeans september season sales said reuter report purchases outstanding of num june its in has for february export ended department delivery covering corn commitments china cancels cancelled begins began &apos;s and agriculture adds added according
. / -

Label

Word

Figure 4: Input to Label Attention Weights. On the left are the positive labels, and on the right are all input components (words). These are taken from the layer 1 Input to Label attentions on a sample from Reuters-21578.

17

