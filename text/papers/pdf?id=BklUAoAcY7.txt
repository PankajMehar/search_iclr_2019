Under review as a conference paper at ICLR 2019
UNSUPERVISED LEARNING OF SENTENCE REPRESENTATIONS USING SEQUENCE CONSISTENCY
Anonymous authors Paper under double-blind review
ABSTRACT
Computing universal distributed representations of sentences is a fundamental task in natural language processing. We propose a simple, yet surprisingly powerful unsupervised method to learn such representations by enforcing consistency constraints on sequences of tokens. We consider two classes of such constraints ­ sequences that form a sentence and between two sequences that form a sentence when merged. We learn a sentence encoder by training it to distinguish between consistent and inconsistent examples. Extensive evaluation on several transfer learning and linguistic probing tasks shows improved performance over strong unsupervised and supervised baselines, substantially surpassing them in several cases.
1 INTRODUCTION
In natural language processing, the use of distributed representations has become standard through the effective use of word embeddings. In a wide range of NLP tasks, it is beneficial to initialize the word embeddings with ones learnt from large text corpora like word2vec Mikolov et al. (2013) or GLoVe Pennington et al. (2014) and tune them as a part of a target task e.g. text classification. It is therefore a natural question to ask whether such standardized representations of whole sentences that can be widely used in downstream tasks, is possible.
There are two classes of approaches to this problem. Taking cue from word2vec, an unsupervised learning approach is taken by SkipThought (Kiros et al. (2015)), FastSent (Hill et al. (2016)) and QuickThoughts Logeswaran & Lee (2018) exploiting the closeness of adjacent sentences in a text corpus. More recently, the work of Conneau et al. (2017) takes a supervised learning approach. They train a sentence encoder on large scale natural language inference datasets (Bowman et al. (2015); Williams et al. (2018)) and show that the learned encoding transfers well to to a set of transfer tasks. This is reminiscent of the approach taken by ImageNet Deng et al. (2009) in the computer vision community. In (Subramanian et al. (2018)), the authors train a sentence encoder on multiple tasks to get improved performance.
In this paper, we take a slightly different unsupervised approach to learning sentence representations. We define a sequence of tokens to be consistent if they form a valid sentence. While defining consistency precisely is difficult, we generate approximately inconsistent sequences of tokens by slightly perturbing consistent ones. We extend the notion of consistency to pairs of sequences ­ two sequences are defined to be consistent if they can me merged to form a consistent sequence. We then train a sentence representation encoder to discriminate between consistent and inconsistent sequences or pairs of sequences. Note that external supervised labels are not required for training as we generate our own labels using the notion of consistency.
2 RELATED WORK
Learning general sentence encoders is a fundamental problem in NLP and there is a long list of works that addresses this problem. To begin with, an untrained BiLSTM model with max pooling of the intermediate states performs fairly well on several transfer and linguistic probing tasks Conneau et al. (2018). Evidently the structure of such a model incorporates certain biases that are beneficial. The next set of simple models consists of a bag-of-words approach using learned word embeddings
1

Under review as a conference paper at ICLR 2019
like GloVe (Pennington et al. (2014)) or FastText (Joulin et al. (2017)), where a simple average of the word embeddings in a sentence define the sentence embedding. Although very fast, these approaches are limited by the dimensions of the word embeddings and do not take into account the order of the words.
Due to the availability of practically unlimited textual data, learning sentence encoders using unsupervised learning is an attractive proposition. The SkipThought model of Kiros et al. (2015) learns sentence encoders by using an encoder-decoder architecture. Exploiting the relatedness inherent in adjacent sentences, the model is trained by using the encoder to encode a particular sentence and then using the decoder to decode words in adjacent sentences. This approach is directly inspired by a similar objective for learning word embeddings like word2vec (Mikolov et al. (2013)). The Bag-of-Words approach is developed further in the FastSent model (Hill et al. (2016)) which uses a bag-of-words representation of a sentence to predict words in adjacent sentences. In work by Arora et al. (2017) it is shown that a simple post processing of the average word-embeddings can perform comparably or better than skip-thought like objectives. In more recent work, Logeswaran & Lee (2018) propose QuickThoughts which use a form of discriminative training on encodings of sentences, by biasing the encodings of adjacent sentences to be closer to each other than non-adjacent sentences, where closeness is defined by the dot product. The work of Pagliardini et al. (2018) where the authors use n-gram features for unsupervised learning is also relevant. The other notable unsupervised approach is that of using a denoising autoencoder (Hill et al. (2016)).
More recently, supervised learning has been used to learn better sentence encoders. Conneau et al. (2017) use the SNLI (Bowman et al. (2015)) and MultiNLI (Williams et al. (2018)) corpus to train a sentence encoder on the natural language inference task that performs well on several transfer tasks. Another domain where large datasets for supervised training is available is machine translation and the work of McCann et al. (2017) exploits this in learning sentence encoders by training it on the machine translation task. Finally, Subramanian et al. (2018) combine several supervised and unsupervised objectives in a multi-task framework to obtain some of the best results on learning general sentence representations.
Our approach is based on automatically generating (possibly noisy) training data by perturbing sentences. Such an approach was used by Wagner et al. (2009) to train a classifier to judge the grammaticality of sentences. The ungrammatical sentences were generated by, among other things, dropping and inserting words. Recent work by Warstadt et al. (2018) extend this approach by using neural network classifiers. Finally, in parallel to our work, a recent report Ranjan et al. (2018) uses word dropping and word permutation to generate fake sentences and learn sentence encoders. Our work is substantially more general and exhaustive.
3 CONSISTENT SEQUENCES AND DISCRIMINATIVE TRAINING
Consider S = {w1, w2, · · · , wn}, an ordered sequence of n tokens. We define this sequence to be consistent if the tokens form a valid sentence. Let E be an encoder that encodes any sequence of tokens into a fixed length distributed representation. Our goal is to train E to discriminate between consistent sequences and inconsistent ones. We argue that such an encoder will have to encapsulate many structural properties of sentences, thereby resulting in a good sentence representation.
Whether a sequence of tokens S is consistent is a notoriously hard problem to solve. We take a slightly different approach. We start from a consistent sequence (e.g. from some corpus) and introduce perturbations to generate sequences S that are not consistent. We take inspiration from the standard operations in string edit distance and consider the following variations for generating S.
· ConsSent-D(k): We pick k random tokens in S and delete them.
· ConsSent-P(k): We pick k random tokens in S and permute them randomly (avoiding the identity permutation).
· ConsSent-I(k): We pick k random tokens not from S and insert them at random positions in S.
· ConsSent-R(k): We pick k random tokens in S and replace them with other random tokens not in S.
2

Under review as a conference paper at ICLR 2019

Model ConsSent-D(1) ConsSent-P(2) ConsSent-I(1) ConsSent-R(1) ConsSent-C(2)
ConsSent-N(2)

Positive Example Maya goes to school . Maya goes to school . Maya goes to school . Maya goes to school . Maya goes to school . She loves it . Maya goes to school . She loves it .

Negative Example Maya goes to . Maya to goes school . Maya goes are to school. Maya doesn't to school . Maya it . She loves goes to school . Maya it school . She loves goes to .

Table 1: Example sequences for the six ConsSent models.

It is important to note that it is possible that in some cases S will itself form a valid sentence and hence violate the definition of consistency. We do not address this issue and assume that such cases will be a relatively few and will not influence the encoder in a substantial manner. Also, with larger values of k, the chance of this happening goes down. We train E to distinguish between S and S using a binary classifier.
We extend the definition of consistency to pairs of sequences. Given two sequences, we define them to be consistent if they can be merged into a consistent sequence without changing the order of the tokens. Similar to the above definitions, we generate consistent and inconsistent pairs by starting from a consistent sequence and splitting them into two parts. We consider the following variations.
· ConsSent-N(k): If n is the number of tokens in a sequence S1, let S11 be a random subsequence of S1, and let S21 = S1 \S11 be the complementary subsequence. For a consistent sequence S1, S11 and S12 form a consistent pairs of sequences. Let S21 and S22 be a partition for a different consistent sequence S2, such that S21 = S22. Then S11 and S22 form an inconsistent pair of sequences, by virtue of the fact they belong to two different consistent sequences. We can vary the complexity of the encoder E by training it to discriminate between a consistent pair (S11, S12) and k - 1 other inconsistent pairs (S11, S22), (S11, S32), · · · , (S11, Sk2) for different values of k. It is possible to pose the task of discriminating the consistent pair (S11, S12) from the k - 1 inconsistent pairs as a classification problem with a classification layer applied to encodings of the pairs. But this introduces additional parameters which is avoidable for sentence pairs. Instead, we train E by enforcing the constraint that
E(S11) · E(S21)  E(S11) · E(Sj2) j  {2, k}
In other words, we train the encoder to place the representations of consistent pairs of sequences closer in terms of dot product than inconsistent pairs. A similar procedure was also used in training sentence representations in Logeswaran & Lee (2018), but with whole sentences appearing adjacent to each other in a larger body of text. Note that such kind of training is not possible for classifying single sequences. · ConsSent-C(k): We generate S11 and S12 from S1 by partitioning it at a random point. Thus both S11 and S12 are contiguous subsequences of S1. If S1 is consistent, then the two partitioned sequences form a consistent pair. We generate inconsistent pairs by pairing S11 with k - 1 other Sj2 originating from the partition of different consistent sequences Sj.
In Table1 we show toy positive and negative training examples for each of these methods. The choice of the encoder E is important to generate good sentence representations. Following Conneau et al. (2017), we use a bidirectional LSTM to process a sequence of tokens and take a max-pool of the intermediate hidden states to compute a distributed representation.
4 EXPERIMENTS
We use the Billionword corpus (Chelba et al. (2014)) to train our models. We use the first 50 shards of the training set (approximately 15 million sentences) for training and 50000 sentences from the
3

Under review as a conference paper at ICLR 2019
validation set for validation. For ConsSent-D(k), ConsSent-P(k), ConsSent-I(k) and ConsSent-R(k) for each sentence we delete, permute, insert or replace k tokens with a probability of 0.5. This produces roughly an equal number of consistent and inconsistent sequences. For ConsSent-{D,I,K}(k) we sweep over k  {1, 2, 3, 4, 5} and for ConsSent-P(k) we sweep over k  {2, 3, 4, 5, 6} to train a total of 20 encoders.
In the case of ConsSent-N(k), for each consistent sequence S1, we partition it into two subsequences by randomly picking a token to be in the first part S11 with probability 0.5. The remaining tokens go into S21. We pick (k - 1) other random consistent sequences S2, · · · Sk and do the same. For ConsSent-C(k), for each consistent sequence S1, we pick i  {2, · · · , n - 1} uniformly at random and partition it at i to produce S11 and S12. The remainder of the training procedure is the same as ConsSent-N(k). For both these models, we sweep over k  {2, 3, 4, 5, 6} to train a total of 10 encoders. Overall, we train 30 encoders for the six methods.
We train the BiLSTM-Max encoder E with a hidden dimension of 2048, resulting in 4096 dimensional sentence representations. For ConsSent-D(k), ConsSent-P(k), ConsSent-I(k) and ConsSentR(k), the sentence representations are passed through two linear layers of size 512 before the classification Softmax.
For ConsSent-N(k) and ConsSent-C(k), we pair S11 with (k - 1) random S2j from within the same minibatch to generate the inconsistent pairs. For optimization, we use SGD with an initial learning rate of 0.1 which is decayed by 0.99 after every epoch or by 0.2 if there is a drop in the validation accuracy. Gradients are clipped to a maximum norm of 5.0 and we train for a maximum of 20 epochs.
We evaluate the sentence encodings using the SentEval benchmark Conneau & Kiela (2018). This benchmark consists of two sets of tasks related to transfer learning and predicting linguistic properties of sentences. In the first set, there are 6 text classification tasks (MR, CR, SUBJ, MPQA, SST, TREC), one task on paraphrase detection (MRPC) and one on entailment classification (SICK-E). All these 8 tasks have accuracy as their performance measure. There are two other tasks on estimating the semantic relatedness of two sentences (SICK-R and STSB) for which the performance measure is Pearson correlation (expressed as percentage) between the estimated similarity scores and ground truth scores. For each of these datasets, the learned ConsSent sequence encoders are used to produce representations of sentences. These representations are then used for classification or score estimation using a logistic regression layer. We also use a L2 regularizer on the weights of the logistic layer whose coefficient is tuned using the validation sets. The goal of testing ConsSent on these tasks is to evaluate the quality of the encoders as general sentence representation generators which can be used in a wide variety of downstream tasks with limited training data.
The second set of tasks probes for 10 difference linguistic properties of sentences. These include tasks like predicting which of a set of target words appears in a sentence (WordContent), the number of the subject in the main clause i.e. whether the subject is singular or plural (SubjNum), depth of the syntactic tree (TreeDepth) and length of the sentence quantized into a few bins (SentLen). Some of these properties are syntactic in nature, while some require deeper understanding of the semantics of a sentence. The goal of testing ConsSent on these tasks is to evaluate how much linguistic information is captured by the encoders. For each of the tasks, the representations produced by the ConsSent encoders are input to a classifier with a linear layer followed by Sigmoid followed by a classification layer. We tune the classifier on the validation sets by varying the dimension of the linear layer in [50, 100, 200] and the dropout before the classification layer in [0, 0.1, 0.2]. For more details on these tasks, please refer to (Chelba et al. (2014); Conneau et al. (2018)).
5 RESULTS ON TRANSFER TASKS
In Table 2 we present results on each of the transfer tasks in SentEval. In addition to the accuracy and correlation figures, we also report an average of all the 10 scores in the last column. We only show two of the best performing models (out of 5) for each of the six methods. Certain trends can be established from these numbers. ConsSent-N(3), which discriminate between pairs of sequences, performs the best on an average. Among the methods that classify single sequences, ConsSentR(2) and ConsSent-R(3) perform the best, second only to ConsSent-N. ConsSent-C is dominated by ConsSent-N in most cases, while ConsSent-D and ConsSent-P perform the worst. Notably, all
4

Under review as a conference paper at ICLR 2019

Model

MR CR SUBJ MPQA SST TREC MRPC SK-E SK-R STSB AVG

Unsupervised Methods

Untrained LSTM 77.1 79.3 91.2 89.1 81.8 82.8 71.6 85.3 82.0 71.0 81.1

FastText BoW

78.2 80.2 91.8 88.0 82.3 83.4 74.4 82.0 78.9 70.2 80.9

SkipThought-LN 79.4 83.1 93.7 89.3 82.9 88.4 72.4 85.8 79.5 72.1 82.7

QuickThoughts(BC) 80.4 85.2 93.9 89.4 - 92.8 76.9 86.8 - - -

QuickThoughts 82.4 86.0 94.8 90.2 87.6 92.4 76.9 - 87.4 - -

Our Methods

ConsSent-C(4)

80.1 83.7 93.6 89.5 83.1 90.0 75.9 86.0 83.2 74.4 84.0

ConsSent-C(5)

80.0 83.9 93.7 89.6 82.9 89.0 76.2 86.2 83.0 74.7 83.9

ConsSent-N(3)

80.1 84.2 93.8 89.5 83.4 90.8 77.3 86.1 83.8 75.8 84.4

ConsSent-N(6) 80.3 83.7 93.7 89.4 83.0 91.0 76.6 86.2 83.9 75.1 84.3

ConsSent-D(2)

80.1 83.8 93.2 90.3 82.0 92.4 74.2 84.0 82.8 69.3 83.2

ConsSent-D(3)

80.6 83.2 93.3 90.1 82.8 91.2 74.6 84.1 82.7 68.0 83.0

ConsSent-P(3)

80.0 83.2 93.4 89.9 82.8 92.2 75.4 84.0 83.1 68.6 83.3

ConsSent-P(4)

80.2 82.8 93.2 90.0 83.1 91.8 74.8 83.9 82.9 68.6 83.1

ConsSent-I(2)

79.2 83.4 93.6 90.3 82.9 92.8 75.0 83.4 84.9 71.1 83.6

ConsSent-I(3)

80.4 83.4 93.4 90.1 83.0 92.2 75.5 83.4 85.0 70.9 83.7

ConsSent-R(2)

79.9 84.3 93.5 90.2 83.4 92.6 75.9 84.2 85.2 72.0 84.1

ConsSent-R(3)

80.3 83.5 93.3 90.2 83.8 92.2 75.8 84.1 85.0 72.3 84.1

Supervised and MultiTask Methods

InferSent

81.1 86.3 92.4 90.2 84.6 88.2 76.2 86.3 88.4 75.8 85.0

MultiTask

82.5 87.7 94.0 90.9 83.2 93.0 78.6 87.8 88.8 78.9 86.5

Table 2: Performance of ConsSent on the transfer tasks in the SentEval benchmark. SkipThought is described in (Kiros et al., 2015), QuickThoughts in (Logeswaran & Lee, 2018) and MultiTask in Subramanian et al. (2018) and InferSent in Conneau et al. (2017). SK-R and SK-E stand for SICKR and SICK-E respectively. AVG is a simple average over all the tasks. Bold indicates best result among our models and underline indicates best overall for unsupervised tasks.

Accuracy Accuracy Pearson Correlation

CS-C CS-N CS-D CS-P CS-I CS-R 93
92
91
90
89
88 012345 k or k - 1
(a) TREC

CS-C CS-N CS-D CS-P CS-I CS-R 77
76
75
74
012345 k or k - 1
(b) MRPC

CS-C CS-N CS-D CS-P CS-I CS-R 76
74
72
70
68
66 012345 k or k - 1
(c) STSB

Figure 1: Performance of ConsSent models on the TREC, MRPC and STSB tasks. The y-values are in percentages. The x-values are k for ConSent-{D,I,R}(k) and k - 1 for ConSent-{C,N,P}(k).

5

Under review as a conference paper at ICLR 2019

Model

SentLen WC TDepth TConst BShift Tense SNum ONum SOMO CInv AVG

Unsupervised Methods

Untrained

73.3 88.8 46.2 71.8 70.6 89.2 85.8 81.9 73.3 68.3 74.9

BoV-fastText 66.6 91.6 37.1 68.1 50.8 89.1 82.1 79.8 54.2 54.8 67.4

AutoEncoder 99.4 16.8 46.3 75.2 71.9 87.7 88.5 86.5 73.5 72.4 71.8

SkipThought 79.1 48.4 45.7 79.2 73.4 90.7 86.6 81.7 72.4 72.3 73.0

Our Methods

ConsSent-C(3) 84.0 85.4 45.2 80.9 66.7 89.3 87.6 80.3 63.0 71.1 75.4

ConsSent-C(4) 82.6 87.2 44.6 80.9 67.3 89.2 87.7 80.4 63.6 71.0 75.4

ConsSent-N(3) 87.0 85.9 46.6 82.2 79.5 88.6 88.9 84.7 63.2 71.5 77.8

ConsSent-N(6) 86.7 86.1 46.9 82.2 80.5 89.1 88.7 84.9 63.3 71.8 78.0

ConsSent-D(3) 87.5 91.0 54.0 84.7 84.3 89.9 89.3 87.4 62.1 72.9 80.3

ConsSent-D(5) 89.5 88.2 54.9 84.9 84.5 90.0 90.0 88.5 61.5 73.2 80.5

ConsSent-P(3) 86.9 91.5 53.2 84.3 87.6 90.1 89.4 88.4 61.7 72.3 80.5

ConsSent-P(4) 86.8 90.2 53.8 83.9 86.4 89.8 89.5 87.6 61.7 72.6 80.2

ConsSent-I(2) 85.7 91.6 52.3 84.0 87.0 89.7 88.6 87.2 62.2 71.4 80.0

ConsSent-I(3) 85.7 92.3 52.4 83.5 85.6 89.7 88.8 87.2 62.1 71.5 79.9

ConsSent-R(2) 86.9 92.0 51.3 82.8 83.8 90.1 89.0 87.6 61.9 71.2 79.7

ConsSent-R(3) 86.3 92.1 51.8 82.6 82.9 90.0 88.9 87.5 63.1 71.5 79.7

Supervised Methods

Seq2Tree

96.5 8.7 62.0 88.9 83.6 91.5 94.5 94.3 73.5 73.8 76.7

NMTEn-Fr

80.1 58.3 51.7 81.9 73.7 89.5 90.3 89.1 73.2 75.4 76.3

NLI 71.7 87.3 41.6 70.5 65.1 86.7 80.7 80.3 62.1 66.8 71.3

Table 3: Performance of ConsSent on the linguistic probing tasks in the SentEval benchmark. The other results have been taken from (Conneau et al. (2018)).

Average Measure Average Measure Average Measure

CS-C CS-N CS-D CS-P CS-I CS-R
92

90

88

86

84
01234 k or k - 1
(a) WordContent

5

CS-C CS-N CS-D CS-P CS-I CS-R

85

80

75

70

65 01234 k or k - 1
(b) BigramShift

5

CS-C CS-N CS-D CS-P CS-I CS-R 90
89
88
87 012345 k or k - 1
(c) SubjNum

Figure 2: Performance of ConsSent models on the WordContent, BigramShift and SubjNum tasks. The y-values are in percentages. The x-values are k for ConSent-{D,I,R}(k) and k - 1 for ConSent{C,N,P}(k).

the methods perform better than SkipThought-LN (Kiros et al. (2015)) on an average and on most individual tasks. Note that the different methods use sentence representations of varying length, which may be smaller than our 4096 length representations.
Looking at the individual tasks, both ConsSent-D(2) and ConsSent-I(2) achieve an accuracy of 90.3% on MPQA, which is better that other strongly performing unsupervised representation learning algorithms including QuickThoughts (Logeswaran & Lee (2018)) which uses an order of magnitude more data to train. Similarly, ConsSent-N(3) achieves an accuracy of 77.3% on the MRPC dataset, substantially better than QuickThoughts. This model also performs very well on STSB, achieving the best accuracy of 75.8% among unsupervised methods.
6

Under review as a conference paper at ICLR 2019

Percentage

84

83

ConsSent-C ConsSent-N ConsSent-D ConsSent-P

ConsSent-I

ConsSent-R

Figure 3: Average performance of all models for the transfer tasks. The bars in a group represent increasing values of k.

80

Percentage

78

76

74 ConsSent-C

ConsSent-N ConsSent-D

ConsSent-P

ConsSent-I

ConsSent-R

Figure 4: Average performance of all models for the linguistic probing tasks. The bars in a group represent increasing values of k.

We compare the relative performance of the encoders with varying values of k for three of the transfer tasks - TREC, MRPC and STSB in Fig. 1. For TREC and MRPC, which are classification tasks, there is roughly an inverted V shaped trend with some intermediate value of k giving the best results for ConSent-D,P,I,R. Note that for smaller values of k, the encoders are exposed to negative examples that are are relatively similar to the positive ones and hence the discriminative training can be noisy. On the other hand, as k increases, the encoders may latch onto superficial patterns and hence not generalize well. For ConsSent-C,N the trends are less clear for TREC but are closer to an inverted V for MRPC. For the semantic scoring task of STSB, the trend lines show no clear pattern.
6 RESULTS ON LINGUISTIC PROBING TASKS
We present results on the 10 linguistic probing tasks in Table 3 where we also compare with other unsupervised methods like a sequence autoencoder and SkipThought. All the encoders perform surprisingly well on most of the tasks, with the best ones ConsSent-D(5) and ConsSent-P(3) attaining an average score of 80.5%, which is 7.5% more than the score achieved by SkipThought. For these tasks, encoders trained on single seqeunces perform better than the ones trained using pairs of sequences. Notably, the performance is significantly better than even the supervised baseline results trained on machine translation and natural language entailment in Conneau et al. (2018). The performance of a third method Seq2Tree using a gated convolutional network (GCN) is however significantly better than the ConsSent encoders (except on the WordContent task). We have not experimented with a GCN encoder and it is possible that such an encoder may give better results.
7

Average Measure MR CR MSPUQBJA SSIITMCCRRSKK-EP-SERTCC WBoirTgrTdreSoaCeepomSnDCtnSTteoLhepSnietnsfBnttht SCOuoSbbojjrTOedNNInMuunsvOemm

Under review as a conference paper at ICLR 2019
ConsSent-N(3) ConsSent-D(5) ConsSent-R(2)
80
60
Figure 5: Performance of the best models on transfer tasks (ConsSent-N(3)), linguistic probing tasks (ConsSent-D(5)) and overall (ConsSent-R(2)).
7 COMPARISON ACROSS TRANSFER AND LINGUISTIC PROBING TASKS
In this section, we compare the ConsSent models across the two sets of tasks. In Fig.3 we plot the average performance of all the models in the transfer tasks and in Fig.4 we plot the average performance of all the models on the linguistic probing tasks. From these two plots, it is clear that ConsSent-C and ConsSent-N are significantly better at transfer tasks than encoding linguistic knowledge. On the other hand, ConsSent-D and ConsSent-P are better at encoding linguistic information than doing well on transfer tasks. The models that perform best for all the tasks on an average come from ConsSent-I and ConsSent-R. In fact, we take an average over all the 20 tasks, the best model is ConsSent-R(2). We show the performance of the best model in the transfer tasks ConsSent-N(3), the best model on the linguistic tasks ConsSent-D(5) and the best model over all the tasks ConsSent-R(2) in Fig.5.
REFERENCES
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In ICLR, volume abs/1607.03474, 2017. URL http://arxiv.org/abs/1607. 03474.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In EMNLP, 2015. URL https: //arxiv.org/abs/1508.05326.
Siddhartha Brahma. Suffix bidirectional long short-term memory. CoRR, abs/1805.07340, 2018. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. One
billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH, 2014. Alexis Conneau and Douwe Kiela. Senteval: An evaluation toolkit for universal sentence representations. CoRR, abs/1803.05449, 2018. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ic Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In EMNLP, 2017. URL http://aclweb.org/anthology/D17-1070. Alexis Conneau, Germa´n Kruszewski, Guillaume Lample, Lo¨ic Barrault, and Marco Baroni. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In ACL, 2018.
8

Under review as a conference paper at ICLR 2019
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. In HLT-NAACL, 2016. URL http://www.aclweb.org/ anthology/N16-1162.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 427­431. Association for Computational Linguistics, April 2017.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In NIPS, 2015. URL https://arxiv.org/ abs/1506.06726.
Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representations. In ICLR, 2018. URL http://arxiv.org/abs/1803.02893.
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In NIPS, 2017. URL http://arxiv.org/abs/1708.00107.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013.
Allen Nie, Erin D. Bennett, and Noah D. Goodman. DisSent: Sentence representation learning from explicit discourse relations. 2018. URL http://arxiv.org/abs/1710.04334.
Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embeddings using compositional n-gram features. In NAACL, 2018. URL https://arxiv.org/ abs/1703.02507.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, 2014. URL https://www.aclweb.org/anthology/ D14-1162.
Christian S. Perone, Roberto Silveira, and Thomas S. Paula. Evaluation of sentence embeddings in downstream and linguistic probing tasks. CoRR, abs/1806.06259, 2018.
Viresh Ranjan, Heeyoung Kwon, Niranjan Balasubramanian, and Minh Hoai. Fake sentence detection as a training task for sentence encoding. arXiv preprint arXiv:1808.03840, 2018.
Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. Learning general purpose distributed sentence representations via large scale multi-task learning. In ICLR, 2018. URL http://arxiv.org/abs/1804.00079.
Joachim Wagner, Jennifer Foster, and Josef van Genabith. Judging grammaticality: Experiments in sentence classification. CALICO Journal, 26(3):474­490, 2009.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. CoRR, abs/1805.12471, 2018.
Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. NAACL, 2018. URL http://arxiv.org/abs/ 1704.05426.
9

