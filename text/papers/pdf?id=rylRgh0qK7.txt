Under review as a conference paper at ICLR 2019
DETERMINISTIC POLICY GRADIENTS WITH GENERAL STATE TRANSITIONS
Anonymous authors Paper under double-blind review
ABSTRACT
We study a reinforcement learning setting, where the state transition function is a convex combination of a stochastic continuous function and a deterministic function. Such a setting generalizes the widely-studied stochastic state transition setting, namely the setting of deterministic policy gradient (DPG). We firstly give a simple example to illustrate that the deterministic policy gradient may be infinite under deterministic state transitions, and introduce a theoretical technique to prove the existence of the policy gradient in this generalized setting. Using this technique, we prove that the deterministic policy gradient indeed exists for a certain set of discount factors, and further prove two conditions that guarantee the existence for all discount factors. We then derive a closed form of the policy gradient whenever exists. Furthermore, to overcome the challenge of high sample complexity of DPG in this setting, we propose the Generalized Deterministic Policy Gradient (GDPG) algorithm. The main innovation of the algorithm is a new method of applying model-based techniques to the model-free algorithm, the deep deterministic policy gradient algorithm (DDPG). GDPG optimize the long-term rewards of the model-based augmented MDP subject to a constraint that the long-rewards of the MDP is less than the original one. We finally conduct extensive experiments comparing GDPG with state-of-the-art methods and the direct model-based extension method of DDPG on several standard continuous control benchmarks. Results demonstrate that GDPG substantially outperforms DDPG, the model-based extension of DDPG and other baselines in terms of both convergence and long-term rewards in most environments.
1 INTRODUCTION
Reinforcement learning has been one of the most successful computational tools for solving complex decision making problems (Sutton & Barto (1998)), with extensive applications in both discrete tasks such as general game playing (Mnih et al. (2013; 2015)) and continuous control tasks such as robotics (Kober et al. (2013)). In contrast to the traditional value-based methods (Tesauro (1995); Watkins & Dayan (1992); Mnih et al. (2013; 2015)) that are meant for solving problems with discrete and low-dimensional action space, policy gradient methods (Peters & Bagnell (2011); Sutton et al. (2000)) aim to tackle these limitations, by optimizing a parameterized policy via estimating the gradient of the expected long-term reward, using gradient ascent.
Silver et al. (2014) propose the deterministic policy gradient (DPG) algorithm that aims to find an optimal deterministic policy, which lowers the variance when estimating the policy gradient (Zhao et al. (2011)), compared to stochastic policies (Sutton et al. (2000)). It is shown that the algorithm can be applied to domains with continuous and high-dimensional action spaces. Lillicrap et al. (2015) further propose the deep deterministic policy gradient (DDPG) algorithm, by combining deep neural networks to improve convergence. It is recognized that DDPG has been successful in robotic control tasks such as locomotion (Song et al. (2017)) and manipulation (Gu et al. (2017)).
Despite the effectiveness of DDPG in these tasks, it is limited for problems with stochastic continuous state transitions. Here, the continuity means that the probability density of the next state is continuous in the action taken at the current state. In fact, many important control problems, such as MountainCar, Pendulum (Brockman et al. (2016)), and autonomous driving, include both stochastic and deterministic state transitions. For example, in most autonomous driving tasks, state transitions
1

Under review as a conference paper at ICLR 2019
are deterministic under normal driving conditions, yet are still stochastic due to sudden disturbances. As a result, DDPG, which assumes stochastic state transitions, does not generalize well in practice.
Tasks with deterministic state transitions pose serious technical challenges due to the discontinuity of the transition function, where the gradient of the transition probability density function over actions does not always exist. (Werbos (1990); Fairbank (2008); Heess et al. (2015)) consider the gradient of the value function over states and the deterministic policy gradient in the setting of deterministic state transitions, but the existence of the value function's gradient over states and the deterministic policy gradient is not studied. Lacking of theoretical guarantees for the existence of the gradient limits the applicability of deterministic policy gradient algorithms. As a result, an important question for policy gradient based methods is, Does the gradient exist in settings with deterministic state transitions? If yes, can one solve the problem efficiently by its gradient?
In this paper, we study a generalized setting, where the state transition is a convex combination of a stochastic continuous transition function and a deterministic discontinuous transition function. As a result, it includes both the stochastic case and the deterministic case as special cases. Our setting is arguably closer to the mixed control problems mentioned above than those stochastic settings. We first give a simple example to illustrate that the deterministic policy gradient may be infinite under deterministic state transitions. Then we introduce a new theoretical technique to prove the existence of the policy gradient in this generalized setting. Using this technique, we prove that the deterministic policy gradient indeed exists for a certain set of discount factors. We further present two conditions that guarantee the existence for all discount factors. We then derive a closed form of the policy gradient.
However, the estimation of the deterministic policy gradient is much more challenging due to the sample complexity of model-free algorithms (Schulman et al. (2015)) and complex state transitions. As for the state transition, the difficulty of the computation of the gradient mainly comes from the dependency of the policy gradient and the gradient of the value function over the state. Such computation may involve infinite times of sampling the whole state space. Thus applying DDPG directly in a general setting even with low-dimensional state space may incur high sample complexity.
To overcome these challenges, we approximate the original Markov decision process (MDP) by a model-based augmented MDP with the same reward function and the transition function being the expectation of original MDP. By the form of the deterministic policy gradient with deterministic state transitions, we get that the model-based augmented MDP has a simple structure, which allows for more efficient computations and faster convergence than model-free methods (Li & Todorov (2004); Levine & Koltun (2013); Watter et al. (2015)). Unfortunately, applying this mode-based technique directly does not help to solve environments with large continuous state space as it is hard to represent the transition dynamics (Wahlström et al. (2015)). This leads to an essential question: How to apply the model-based technique to deterministic policy gradient algorithms effectively?
We then consider a program that maximizes the long-term rewards of the augmented MDP with the constraint that its long-term rewards is less than that of the original MDP. The intuition is that we choose a objective with less sample complexity to optimize, and it serves as a lower bound of the original objective. Note that the improvement of the new objective, guarantees the improvement of the original objective. As the constrainted problem is hard to optimize, we choose to optimize the Lagrangian dual function of the program, which can be interpreted as a weighted objective between the long-term reward of the original MDP and the augmented MDP. Based on this dual function, we propose the Generalized Deterministic Policy Gradient (GDPG) algorithm. The algorithm updates the policy by stochastic gradient ascent with the gradient of the weighted objective over the parameter of the policy, and the weight maintains a trade-off between fast convergence and performance.
To sum up, the main contribution of the paper is as follows:
· First of all, we provide a theoretical guarantee for the existence of the gradient in settings with deterministic state transitions.
· Secondly, we propose a novel policy gradient algorithm, called Generalized Deterministic Policy Gradient (GDPG), which combines the model-free and model-based methods. GDPG reduces sample complexity, enables faster convergence and performance improvement.
2

Under review as a conference paper at ICLR 2019

· Finally, we conduct extensive experiments on standard benchmarks comparing with stateof-the-art stochastic policy gradient methods including TRPO (Schulman et al. (2015)), ACKTR (Wu et al. (2017)) and the direct model-based extension of DDPG, called MDPG. Results confirm that GDPG significantly outperforms other algorithms in terms of both convergence and performance.

2 PRELIMINARIES

A Markov decision process (MDP) is a tuple (S, A, p, r, , p1), where S and A denote the set of states and actions respectively. Let p(st+1|st, at) represent the conditional density from state st to state st+1 under action at, which satisfies the Markov property, i.e., p(st+1|s0, a0, ..., st, at) = p(st+1|st, at).
The density of the initial state distribution is denoted by p0(s). At each time step t, the agent interacts
with the environment with a deterministic policy µ, which is parameterized by . We use r(st, at)

to represent the corresponding immediate reward, contributing to the discounted overall rewards

from state s0 following µ, denoted by J(µ) = E[

 k=0



k

r(ak

,

sk

)|µ

,

s0].

Here, 



[0, 1]

is the discount factor.

Qµ (st, at) = E[

 k=t

The Q-function of state k-tr(ak, sk)|µ, st, at].

st and action at under The corresponding value

policy µ is denoted by function of state st under

policy µ is denoted by V µ (st) = Qµ (st, µ(st)). We denote the density at state s after t time

steps from state s by p(s, s , t, µ) following the policy µ. We denote the (improper) discounted state

distribution by µ (s ) = S

 t=1

 t-1

p0

(s)p(s,

s

, t, µ)ds.

The

agent

aims

to

find

an

optimal

policy that maximizes J(µ).

2.1 WHY IS THE DPG THEOREM NOT APPLICABLE FOR DETERMINISTIC STATE TRANSITIONS?

An important property of the DPG algorithms is the Deterministic Policy Gradient Theorem (Silver et al. (2014)), J (µ) = S µ (s)( µ(s) a Qµ (s, a)|a=µ(s))ds, which proves the existence of the deterministic policy gradient. The DPG theorem holds under the regular condition presented
by (Silver et al. (2014)), i.e., p(s |s, a) is continuous in a. The arguments in the proof of the DPG theorem do not work without this condition1.

Now we give a simple example to show the policy gradient is infinite for some discount factors.

Example 2.1. Given a MDP with two dimensional state spaces and action spaces, whose transition and reward functions are defined by T (s, a) = (2s1 + 2s2 + a1, 2s1 + 2s2 + a2)T , r(s, a) = -sT a.

Consider a deterministic policy µ(s) = , then

sT (s, µ(s)) =

2 2

2 2

,

and

sV µ (s) =

-(I +

 n=1



n

22n-1 22n-1

22n-1 22n-1

). Then

sV µ (s) converges if and only if  < 1/4.

One must need a new technique to determine the existence of the gradient of J(µ) over  in irregular cases.

3 DETERMINISTIC STATE TRANSITIONS

In this section we study a simple setting where the state transition is a deterministic function. As discussed before, the DPG theorem does not apply to this setting. To analyze the gradient of a deterministic policy, we let T (s, a) denote the next state given the current state s and the action a. Without loss of generality, we assume that T (s, a), aT (s, a), sT (s, a), r(s, a), sr(s, a), ar(s, a) are all continuous in s and a and bounded. By definition, V µ (s) = (r(s, µ(s)) + V µ (s )|s =T (s,µ(s))). Thus the key of the existence of the gradient of V µ (s) over  is the existence of sV µ (s). Now we give a sufficient condition of the existence of sV µ (s).

Lemma 1. For any policy µ, let n denote the dimension of the state, and c be the maximum of the

max norm of all Jacobain matrices, maxs || sV µ (s) exists.

s

T

(s,

µ (s))||max ,

for

any

discount

factor



in

[0,

1 nc

),

1Readers can refer to http://proceedings.mlr.press/v32/silver14-supp.pdf

3

Under review as a conference paper at ICLR 2019

Proof. By definition, V µ (s) = Qµ (s, µ(s)) = r(s, µ(s)) + V µ (s )|s =T (s,µ(s))). Then

sV µ (s) = sr(s, µ(s)) +  s T (s, µ(s)) s V µ (s )|s =T (s,µ(s)).

(1)

By unrolling (1) with infinite steps, we get sV µ (s) =

 t=0

S tg(s, t, µ)I(s, s , t, µ)

s

r(s , µ(s ))ds , where I(s, s , t, µ) is an indicator function that indicates whether s is obtained

after t steps from the state s following the policy µ. Here, g(s, t, µ) =

t-1 i=0

si T (si, µ(si)),

where s0 = s and si is the state after i steps following policy µ. The state transitions

and policies are both deterministic.

We now prove that for any µ, s, s

and 



[0,

1 nc

),

A(s) =

 t=0

tg(s, t, µ)I(s, s

, t, µ)

converges.

We describe the proof sketch here and the

complete proof is referred to Appendix A. For each state s , which is reached from the initial

state s with infinite steps, there are three cases due to deterministic state transitions: never vis-

ited, visited once, and visited infinite times. It is easy to see that A(s) converges in the first two cases. In the last case, as A(s) is the sum of the power of the matrix t2 g(s, t2, µ), then
we get a upper bound of  such that A(s) converges. By Lebesgue's Dominated Convergence

Theorem (Royden & Fitzpatrick (1968)), we exchange the order of the limit and the integration,

sV µ (s) = S

 t=0

tg(s,

t,

µ )I (s,

s

,

t,

µ )

s r(s , µ(s ))ds . By the continuity of T , r

and µ, the gradient of V µ (s) over s exists.

Note that the condition proposed in Lemma 1 is indeed necessary in Example 2.1, where n = 2, c = 2 and the gradient exists if and only if the discount factor  < 1/4. By Lemma 1, we show that the deterministic policy gradient exists and obtain the closed form. The proof is referred to Appendix B.

Theorem 1. For any policy µ and MDP with deterministic state transitions, for any discount factor



in

[0,

1 nc

),

the

policy

gradient

exists,

and

J (µ) = µ (s) µ(s)( ar(s, a)|a=µ(s)+ aT (s, a)|a=µ(s) s V µ (s )|s =T (s,a))ds.
S

4 DETERMINISTIC POLICY GRADIENTS WITH GENERAL STATE TRANSITIONS

In this section we consider a general setting where the state transition for any state s and any action a is a convex combination of a deterministic transition function T (s, a) with probability f (s, a), and a
stochastic probability transition density function p(s |s, a) with probability 1 - f (s, a). Note that this setting generalizes that of DPG. Here, T also satisfies the same condition as in Section 3. We assume that f (s, a), sf (s, a) and af (s, a) are continuous and bounded. By the similar technique to the setting with deterministic state transitions, we get the main theorem which proves the existence of the gradient of J(µ) over  for a set of discount factors and proposes two conditions such that for all discount factors the policy gradient exists:

Condition

A.1:

maxs

f (s, µ(s))



1 nc

.

Condition A.2: For any sequence of states (s0, ..., st-1) and any timestep t, the eigenvalues of

t-1 i=0

f

(si,

µ

(si))

si T (si, µ(si)) are in [-1, 1].

Theorem 2. The GDPG Theorem

For any MDP in the general cases and any policy µ, for any discount factor  in

[0,

nc maxs

1 f (s,µ

(s))

),

the

policy

gradient

exists.

If

the

MDP

satisfies

Condition

A.1

or

Condition

A.2,

for any discount factor and any policy µ, the policy gradient exists. The form is

J (µ) = µ (s)( µ(s) a r(s, a)|a=µ(s) + f (s, µ(s))  µ(s) a T (s, a)|a=µ(s)
S

s V µ (s )|s =T (s,a) + (1 - f (s, µ(s)))

 µ (s)

S

V µ (s )ds +   f (s, µ(s))V µ (s )|s =T (s,µ(s)) - 

a p(s |s, a)|a=µ(s)  f (s, µ(s))

p(s |s, a)|a=µ(s)V µ (s )ds )ds = µ (s)( µ(s) a Qµ (s, a)|a=µ(s))ds.
SS
(2)

4

Under review as a conference paper at ICLR 2019

The proof is referred to Appendix C. It is interesting to note that the form is the same as the form of gradient of DPG. In fact, the assumption of the condition A.1 and A.2 would become weaker when the probability of the deterministic state transition becomes lower. In the extreme case, i.e., the stochastic case, where the probability is zero, the policy gradient exists without any assumption as in (Silver et al. (2014)).
In fact, the form of the policy gradient is the same in settings of the deterministic state transition and the general case. However, given an estimator of the value function, the complexity of calculating the gradient of these two cases is different. By comparing (1) with (2), we get that it is the more computationally expensive for the gradient of the general case than the deterministic case. The gradient of deterministic state transitions only involves r(s, µ(s)) and s V µ (s ), while the gradient of the general case introduces additional integration on the state space.
4.1 DIRECT MODEL-BASED EXTENSION OF DDPG
As discussed before, even for the environment with low-dimensional state space, the sample complexity of DDPG is significantly high for the general case, which may limit the capability of the model-free algorithms due to slow convergence. Thus, we consider a model-based augmented MDP M of the original MDP M with the same reward function, while the state transition function is defined as the expectation of the distribution of the next state of the original MDP, i.e., T(s, a) = E[s |s, a]. M is easier to solve as the state transition of M is deterministic. Note that if the environment is indeed deterministic, M = M. Now we define a direct model-based extension of DDPG, called MDPG. MDPG directly uses the gradient of the long-term rewards of M with policy µ to improve the policy instead of the deterministic policy gradient, i.e., J(µ) = µ(s) a Qµ (s, a), where Qµ (s, a) denotes the action value function of the augmented MDP. However, it is hard to represent the transition dynamics in complex environments, and it may cause the policy to move to a wrong direction as shown in Section 5.2 on problems with large state space.
4.2 THE GDPG ALGORITHM
On the one hand, only solving the model-based augmented MDP may be too myopic. On the other hand, the model-free algorithm suffers from high sample complexity as mentioned. Consequently, we consider a program that maximizes the long-term rewards of the augmented MDP, with the constraint being that the long-term rewards of the augmented MDP is less than the original MDP, i.e.,

max J(µ), s.t.J(µ)  J (µ).


(3)

It is easy to check that the optimum of this program is less than max J(µ), and it serves as a lower bound of the long-term rewards of the original MDP. The intuition of this program is to optimize a model-based objective which is easier to solve and the improvement of the new objective guarantees the improvement of the original objective.
If the value function is convex in states 2, the long-term rewards of M with policy µ, J(µ) is no larger than the long-term rewards of M, as illustrated in Theorem 3. That is, the program turns into a problem that maximizes the model-based objective. The proof is referred to Appendix D.
Theorem 3. If V µ (s) is convex in s, J (µ)  J(µ).

In the other case that the value function is not convex, it is hard to solve the program directly. Therefore, we choose to optimize its Lagrangian dual program,

min
0

max


J

(µ

)

+

(J

(µ

)

-

J(µ

)).

(4)

Then for each choice of , we use the gradient of J(µ) + (J(µ) - J(µ)), i.e.,
2The value functions of Linear Quadratic Regulation (Bradtke (1993)) and Linearly-solvable Markov Decision Process (Todorov (2007)) are indeed convex.

5

Under review as a conference paper at ICLR 2019

Algorithm 1: GDPG algorithm

1 Initialize a positive weight  2 Initialize the transition network T (s, a|T ) with random weights T 3 Initialize the original and augmented critic networks Q(s, a|Q), Q(s, a|Q ) with random weights Q, Q 4 Initialize the actor network µ(s|µ) with random weights µ

5 Initialize the target networks Q , Q and µ with weights Q = Q, Q = Q , µ = µ 6 Initialize Experience Replay buffer B 7 for episode= 0, ..., N - 1 do
8 Initialize a random process N for action exploration
9 Receive initial observation state s0.
10 for t = 1, ..., T do 11 Select action at = µ(st|µ) + Nt according to the current policy and exploration noise 12 Execute action at, observe reward rt and new state st+1, and store transition (st, at, rt, st+1) in B 13 Sample a random minibatch of N transitions (si, ai, ri, si+1) from B

14 Set yi = ri + Q (si+1, µ (si+1|µ )|Q )

15

Update

the

critic

Q

by minimizing the loss:

L1

=

1 N

i (yi - Q(si, ai|Q))2

16 Set yi = ri + Q(T (si, ai|T ), µ (T (si, ai|T )|µ )|Q )

17

Update

the

augmented

critic

Q

by

minimizing

the

loss:

L2

=

1 N

i

(yi

-

Q(si,

ai

|Q

2
))

18

Upate

the

transition

T

by

minimizing

the

loss:

L3

=

1 N

i (si+1 - T (si, ai|T ))2

19 Update the actor by the sampled policy gradient and target networks:

µ

J

(µ)

=

1 N

(1 - ) µ µ(s|µ) a Q(s, a|Q ) +  µ µ(s|µ) a Q(s, a|Q)

i

20
Q =  Q + (1 -  )Q ; Q =  Q + (1 -  )Q ; µ =  µ + (1 -  )µ

(1 - )µ(s) a Qµ (s, a) + µ(s) a Qµ (s, a),

(5)

which generalizes the gradient of the DDPG algorithm, to improve the policy by stochastic gradient ascent, where Qµ (s, a) denotes the action value function of the augmented MDP. However, the estimation of the value function of the augmented MDP relies on the expectation of the distribution of the next state, which is unknown. To overcome this challenge, we follow the idea of (Nagabandi et al. (2017)), where neural networks are applied to predict the next state. Different from (Nagabandi et al. (2017)) where they take model predictive control as the control policy, we apply the estimators of state transitions to estimate the action-value function of the augmented MDP. We now propose the Generalized Deterministic Policy Gradient (GDPG) algorithm, as shown in Algorithm 1. Apart from training the actor and the critic, we also train a transition network T which predicts the next state.

5 EXPERIMENTS
In this section, we design a series of experiments to evaluate GDPG. We aim to investigate the following questions: (1) How does the value of  affect the performance on a toy problem with general state transitions? (2) How does GDPG compare with DDPG, MDPG, and other state-of-the-art methods on continuous control benchmarks?
We first illustrate the influence of the weight  in a toy environment, ComplexPoint-v0 with general state transitions. Then we evaluate GDPG in a number of continuous control benchmark tasks in OpenAI Gym (Brockman et al. (2016)), including a classic control problem (Moore (1990)) and a task in the Box2D and MuJoCo (Todorov et al. (2012)) simulator. The details of our benchmarks are referred to Appendix E. We compare GDPG with the following baselines: (a) DDPG, (b) MDPG, (c) TRPO, (d) ACKTR. For the experiments, we run each algorithm 1M steps on each environment over 5 random seeds. Note that the configuration of GDPG is the same as that of DDPG of except for

6

Under review as a conference paper at ICLR 2019

Return Return Return

(a) The ComplexPoint environment.

20 ComplexPoint-v0

100 Stage 1

40 105

60 110

80 115

100 120

120

=0 =0.25

125

=0

=0.5 =0.25

140

=0.75 =1

130

=0.5 =0.75

=2 =1

160 0 200000 400000 600000 800000 1000000 135 15000 20000 25000 30000 35000 40000

Steps Steps

(b) Effect of .

(c) Earlier stage.

24 26 28 30 32 34 36 38 49050000

960000

Stage 2 970000 980000
Steps

=0 =0.25 =0.5 =0.75 =1 990000 1000000

(d) Convergence stage.

Figure 1: Return/steps of training on algorithms

the transition network. Full configuration is referred to Appendix E. We use the averaged return of previous 100 episodes as the performance metric.
5.1 THE ABLATION STUDY OF GDPG
To better understand the effect of  in the dual function, we evaluate GDPG with five different choices of the weight  = 0, 0.25, 0.5, 0.75, 1, 2 in ComplexPoint-v0. Figure 1(a) shows a snapshot of this environment, where the state is the coordinates of the agent in the 5D space while the feasible action set is [-0.1, 0.1]5. The state transition is a convex combination of the deterministic transition T (s, a) = s + a with probability f (s, a), and uniform distribution [-1, 1]5 with probability 1 - f (s, a), where f (s, a) = ||a||22/0.05. The reward function is r(s, a) = -||s + a||2, i.e., the distance between the agent and the origin. The task is terminated either when the agent enters the termination area or the number of steps exceeds a threshold of 100 steps.
Figure 1(b) shows the performance comparison, and Figure 1(c) and Figure 1(d) correspond to its earlier stage and convergence stage, which illustrates convergence and performance more clearly. As shown, for  = 1, which indeed corresponds to DDPG, results in a bad performance and slow convergence. The slow convergence attributes to the computation complexity of gradient in this environment. For  = 0, the goal corresponds to optimize the augmented MDP, which performs better than DDPG as it efficiently reduces sample complexity. However, it is too myopic as it solely focuses on the augmented MDP, which may deviate from the original objective and limit its performance. We observe that the best performance is achieved when  = 0.5. We can view the weighted objective as a convex combination of the model-free objective and the model-based objective when   [0, 1].  trades-off between the convergence and the performance. A large  may introduce bias while a small  may suffer from sample complexity. Note that the choice of 2 for the value of  achieves the worst performance. Recall (5), the reason is that setting a value of  larger than 1 may lead the gradient of the policy to a totally opposite direction and induce large variance of the policy gradient.
5.2 PERFORMANCE COMPARISON WITH BASELINES ON CONTINUOUS CONTROL BENCHMARKS
We now present and discuss the findings from our experiments on several continuous control tasks, all of which are standard benchmark defined in OpenAI Gym (Brockman et al. (2016)). Tasks range from low-dimensional input space to high-dimensional input space. For the baselines algorithms, we use the implementation from OpenAI Baselines (Dhariwal et al. (2017)). Figure 2 shows the sample mean and the standard deviation of the averaged returns in each environment. As shown in Figure 2, GDPG outperforms other baselines in tasks with low-dimensional input space including a classic continuous control task and a task simulated by Box2D. From Figure 2, we observe that GDPG outperforms high-dimensional tasks simulated by MuJoCo by a large margin, especially in Swimmer-v2, HalfCheetah-v2, and Humanoid-v2. This demonstrates that GDPG combines the model-based augmented MDP and the original MDP efficiently. Note that the direct model-based extension of DDPG, MDPG performs the worst in all environments except Swimmer-v2. It shows that the model-based technique can not solve complex settings like MuJoCo as it is hard to represent the transition dynamics.
7

Under review as a conference paper at ICLR 2019

Return

200 400 600 800 1000 1200 1400
0

Pendulum-v0

200000

400000 600000 Steps

800000

GDPG DDPG MDPG TRPO ACKTR 1000000

(a) Pendulum-v0.

Return

LunarLanderContinuous-v2 200 GDPG

0

200

400
600 0

200000

400000 600000 Steps

800000

GDPG DDPG MDPG TRPO ACKTR 1000000

(b) LunarLander-v2.

Return

250 200 150 100 50
0 50
0

Swimmer-v2

200000

400000 600000 Steps

800000

GDPG DDPG MDPG TRPO ACKTR 1000000

(c) Swimmer-v2.

5000 4000 3000 2000 1000
0 1000
0

HalfCheetah-v2

120000

100000

Return

80000

200000

400000 600000 Steps

800000

GDPG DDPG MDPG TRPO ACKTR 1000000

60000 40000
0

HumanoidStandup-v2

200000

400000 600000 Steps

800000

GDPG DDPG MDPG TRPO ACKTR 1000000

(d) HalfCheetah-v2.

(e) HumanoidStandup-v2.

Return

1600 1400 1200 1000 800 600 400 200
0 0

Humanoid-v2

200000

400000 600000 Steps

800000

GDPG DDPG MDPG TRPO ACKTR 1000000

(f) Humanoid-v2.

Figure 2: Return/steps of training on environments from the MuJoCo simulator.

Return

6 RELATED WORK
Model-based algorithms has been widely studied (Koutník et al. (2013); Lioutikov et al. (2014); Moldovan et al. (2015); Montgomery & Levine (2016)) in recent years. Iterative LQG (Li & Todorov (2004)) applies model-based methods and assumes a specific form of both transition dynamics and the value function while (Sutton (1990); Gu et al. (2016); Kurutach et al. (2018)) generate synthetic samples by the learned model. Different from traditional model-based methods, we optimize the dual function that involves the model-based augmented MDP and the original MDP. Perhaps the most related model-based approach to our work is PILCO (Deisenroth & Rasmussen (2011)), which learns the transition model by Gaussian processes. With the non-parametric transition model, (Deisenroth & Rasmussen (2011)) applies policy improvement on analytic policy gradients. However this method does not scale well to nonlinear transition dynamics or high-dimensional state spaces. Different from (Deisenroth & Rasmussen (2011)), we do not rely on assumptions of the transition model.
7 CONCLUSION
Most existing works on policy gradient assume stochastic state transitions, while most realistic settings often involve deterministic state transitions. In this paper, we study a setting with a general state transition that is a convex combination of a stochastic continuous function and a deterministic discontinuous function. We prove the existence of the deterministic policy gradient for a certain set of discount factors. We propose the GDPG algorithm to reduce the sample complexity of the deterministic policy gradient. GDPG solves a program that maximizes the long-terms rewards of the model-based augmented MDP with the constraint that the objective serves as the lower bound of the original MDP. We compare GDPG with MDPG and state-of-the-art algorithms on several continuous control benchmarks. Results show that GDPG substantially outperforms other baselines in terms of convergence and long-term rewards. For future work, how to address the optimal weight in the dual program remains to be studied. It is worth studying whether the deterministic policy gradient exists in more general settings that involve multiple deterministic state transitions. Last but not least, it is promising to apply the model-based technique presented in this paper to other model-free algorithms.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Steven J Bradtke. Reinforcement learning applied to linear quadratic regulation. In Advances in neural information processing systems, pp. 295­302, 1993.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp. 465­472, 2011.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/ openai/baselines, 2017.
Michael Fairbank. Reinforcement learning by value gradients. arXiv preprint arXiv:0803.3539, 2008.
AB Farnell. Limits for the characteristic roots of a matrix. Bulletin of the American Mathematical Society, 50(10):789­794, 1944.
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with model-based acceleration. In International Conference on Machine Learning, pp. 2829­2838, 2016.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 3389­3396. IEEE, 2017.
Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944­2952, 2015.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238­1274, 2013.
Jan Koutník, Giuseppe Cuccu, Jürgen Schmidhuber, and Faustino Gomez. Evolving large-scale neural networks for vision-based reinforcement learning. In Proceedings of the 15th annual conference on Genetic and evolutionary computation, pp. 1061­1068. ACM, 2013.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.
Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning, pp. 1­9, 2013.
Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biological movement systems. In ICINCO (1), pp. 222­229, 2004.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Rudolf Lioutikov, Alexandros Paraschos, Jan Peters, and Gerhard Neumann. Sample-based informationl-theoretic stochastic optimal control. In Robotics and Automation (ICRA), 2014 IEEE International Conference on, pp. 3896­3902. IEEE, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
9

Under review as a conference paper at ICLR 2019
Teodor Mihai Moldovan, Sergey Levine, Michael I Jordan, and Pieter Abbeel. Optimism-driven exploration for nonlinear systems. In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pp. 3239­3246. IEEE, 2015.
William Montgomery and Sergey Levine. Guided policy search as approximate mirror descent. arXiv preprint arXiv:1607.04614, 2016.
Andrew William Moore. Efficient memory-based learning for robot control. 1990.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. arXiv preprint arXiv:1708.02596, 2017.
Jan Peters and J Andrew Bagnell. Policy gradient methods. In Encyclopedia of Machine Learning, pp. 774­776. Springer, 2011.
Halsey Lawrence Royden and Patrick Fitzpatrick. Real analysis, volume 2. Macmillan New York, 1968.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889­1897, 2015.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International Conference on Machine Learning (ICML, 2014.
Doo Re Song, Chuanyu Yang, Christopher McGreavy, and Zhibin Li. Recurrent network-based deterministic policy gradient for solving bipedal walking challenge on rugged terrains. arXiv preprint arXiv:1710.02896, 2017.
Richard S Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine Learning Proceedings 1990, pp. 216­224. Elsevier, 1990.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000.
Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3): 58­68, 1995.
Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information processing systems, pp. 1369­1376, 2007.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012.
Niklas Wahlström, Thomas B Schön, and Marc Peter Deisenroth. From pixels to torques: Policy learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pp. 2746­2754, 2015.
Paul J Werbos. A menu of designs for reinforcement learning over time. Neural networks for control, pp. 67­95, 1990.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems, pp. 5285­5294, 2017.
10

Under review as a conference paper at ICLR 2019

Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement of policy gradient estimation. In Advances in Neural Information Processing Systems, pp. 262­270, 2011.

A PROOF OF LEMMA 1

Proof. Recall the definition of V µ (s), we have

V µ (s) = Qµ (s, µ(s)) = r(s, µ(s)) + V µ (s )|s =T (s,µ(s))).

(6)

sV µ (s) = sr(s, µ(s)) +  s T (s, µ(s)) s V µ (s )|s =T (s,µ(s)). By unrolling (7) with infinite steps, we get



s V µ (s) =

tg(s, t, µ)I(s, s , t, µ) s r(s , µ(s ))ds ,

t=0 S

(7) (8)

where I(s, s , t, µ) is an indicator function that indicates whether s is obtained after t steps from

the state s following the policy µ. Here, g(s, t, µ) =

t-1 i=0

si T (si, µ(si)),where s0 = s

and si is the state after i steps following policy µ. The state transitions and policies are both

deterministic.

We now prove that for any µ, s, s

and any discount factor 



[0,

1 nc

)

such

that

 t=0

tg(s,

t,

µ )I (s,

s

,

t,

µ )

converges.

For each state s , which is reached from the initial state s with infinite steps, there are three cases due to deterministic state transitions, as analyzed below:

1. Never visited:

 t=0

tg(s,

t,

µ )I (s,

s

,

t,

µ )

=

0.

2. Visited once: Let ts denote the number of steps that it takes to reach the state s , then

 t=0

tg(s,

t,

µ

)I

(s,

s

, t, µ)

=



t
s

g(s, ts

, µ).

3. Visited infinite times: Let t1 denote the number of steps it takes to reach s for the first time. The state s will be revisited every t2 steps after the previous visit.


tg(s, t, µ)I(s, s , t, µ) = t1+kt2 g(s, t1 + kt2, µ).
t=0 k=0
By the definition of g, g(s, t1 + kt2, µ) = g(s, t1, µ)(g(s, t2, µ))k, we have

tg(s, t, µ)I(s, s , t, µ) = t1 g(s, t1, µ) (t2 g(s, t2, µ))k.
t=0 k=0

(9) (10)

We get the sum of the absolute value of a row or a column of the matrix g(s, t2, µ)) is no

larger

than

(nc)t2 .

If

we

choose



such

that



<

1 nc

,

by

(Farnell

(1944)),

the

absolute

value

of any eigenvalue of t2 g(s, t2, µ) is strictly less than t2 (nc)t2 = 1.

By representing t2 g(s, t2, µ) with Jordan normal form, i.e., t2 g(s, t2, µ) = M J M -1,


t1 g(s, t1, µ) (t2 g(s, t2, µ))k = t1 g(s, t1, µ)M J kM -1.

k=0

k=0

(11)

As the absolute value of any eigenvalue of t2 g(s, t2, µ) is strictly less than 1,

 k=0

J

k

converges, then

 k=0

(t2 g(s,

t2,

µ ))k

and

 t=0

tg(s,

t,

µ )I (s,

s

,

t,

µ )

converge.

11

Under review as a conference paper at ICLR 2019

By the Lebesgue's Dominated Convergence Theorem (Royden & Fitzpatrick (1968)), we exchange

the order of the limit and the integration.



s V µ (s) =

tg(s, t, µ)I(s, s , t, µ) s r(s , µ(s ))ds .

S t=0

(12)

By the continuity of T , r and µ, the gradient of V µ (s) over s exists.

B PROOF OF THEOREM 1

Proof. By the definition, V µ (s) =  Qµ (s, µ(s)) =  (r(s, µ(s)) + V µ (s )|s =T (s,µ(s))) =  µ(s) a r(s, a)|a=µ(s) +   V µ (s )|s =T (s,µ(s)) +  µ(s) a T (s, a)|a=µ(s) s V µ (s )|s =T (s,a).

(13)

With the indicator function I(s, s , t, µ), we rewrite the equation (13): V µ (s) =  µ(s)( ar(s, a)|a=µ(s) +  a T (s, a)|a=µ(s) s V µ (s )|s =T (s,a))

+ I(s, s , 1, µ)  V µ (s )ds .
S
=  µ(s)( ar(s, a)|a=µ(s) +  a T (s, a)|a=µ(s)

s V µ (s )|s =T (s,a))

+ I(s, s , 1, µ)  µ(s )( a r(s , a )|a =µ(s ) +  a T (s , a )|a =µ(s )
S
s V µ (s )|s =T (s ,a ))ds + I(s, s , 1, µ) I(s , s , 1, µ)  V µ (s )ds ds .
SS
=  µ(s)( ar(s, a)|a=µ(s) +  a T (s, a)|a=µ(s) s V µ (s )|s =T (s,a))

+ I(s, s , 1, µ)  µ(s )( a r(s , a )|a =µ(s ) +  a T (s , a )|a =µ(s )
S

s V µ (s )|s =T (s ,a ))ds + 2I(s, s , 2, µ)  V µ (s )ds .
S

(14)

By unrolling (14) with infinite steps, we get



V µ (s) =

tI(s, s , t, µ)  µ(s )(

S t=0

s V µ (s )|s =T (s ,a ))ds .

a r(s , a )|a =µ(s ) + 

a T (s , a )|a =µ(s ) (15)

By the definition of J(µ),

J (µ) =  p0(s)V µ (s)ds
S

= p0(s)  V µ (s)ds.
S

As


µ (s ) =

tp0(s)I(s, s , t, µ)ds.

S t=0

(16) (17)

12

Under review as a conference paper at ICLR 2019

By exchanging the order of the integration, we get J (µ) = µ (s) µ(s)( ar(s, a)|a=µ(s)+ aT (s, a)|a=µ(s) s V µ (s )|s =T (s,a))ds.
S
(18)

C PROOF OF THEOREM 2
Proof. We first prove a fact that for any continuous policy µ, there exists a discount fator  such that the gradient of V µ (s) over s exists. Recall the definition of V µ (s), we have

V µ (s) = Qµ (s, µ(s)) = r(s, µ(s)) + f (s, µ(s))V µ (s )|s =T (s,µ(s)) + (1 - f (s, µ(s)))
p(s |s, a)|a=µ(s)V µ (s )ds .
S

(19)

Then

sV µ (s) = s r(s, µ(s)) +  s f (s, µ(s))V µ (s )|s =T (s,µ(s)) + f (s, µ(s))

s T (s, µ(s)) s V µ (s )|s =T (s,µ(s)) + (1 - f (s, µ(s)))

sp(s |s, µ(s))

S

V µ (s )ds -  s f (s, µ(s)) p(s |s, a)|a=µ(s)V µ (s )ds .
S

(20)

By unrolling (20) with infinite steps, we get



sV µ (s) =

tg(s, t, µ)I(s, s , t, µ)( s r(s , µ(s )) +  s f (s , µ(s ))V µ (s )+

t=0 S

(1 - f (s , µ(s ))

s p(s | s , µ(s ))V µ (s )ds -  s f (s , µ(s ))

S

p(s |s , a )|a =µ(s )V µ (s )ds )ds,
S

(21)

where I(s, s , t, µ) is an indicator function that indicates whether s is obtained after t steps from the

state s following the policy µ and the deterministic transition. g(s, t, µ) =

t-1 i=0

f

(si,

µ (si ))

si

T (si, µ(si)), where s0 = s. Here, as the policy is deterministic and the calculation of the gradient

with  only involves the deterministic state transitions, si is the state after i steps following policy

µ. By the same technique of the proof of Lemma 1, we get that there exists a discount factor

(0 <  < 1) such that



tg(s, t, µ)I(s, s , t, µ)

(22)

t=0

converges.

In fact, we can choose  such that  × maxs f (s, µ(s))

<

1 nc

,

where

n

denotes

the dimension of the state, and c be the maximum absolute value of elements of all matrices

sT (s, µ(s)).

If

the

condition

A.1

holds,

i.e.,

for

any

state

s,

maxs f (s, µ(s))



1 nc

,

by

the

proof

of

Lemma

1,

for

any discount factor,

 t=0

tg(s,

t,

µ )I (s,

s

,

t,

µ )

converges.

13

Under review as a conference paper at ICLR 2019

If the condition A.2 holds, we have

t2 max|(g(s, t2, µ))| < 1.

Thus for any discount factor

 t=0

tg(s,

t,

µ )I (s,

s

,

t, µ)

converges.

By the Lebesgue's Dominated Convergence Theorem, we exchange the order of the limit and the intergation:



sV µ (s) =

tg(s, t, µ)I(s, s , t, µ)( s r(s , µ(s )) +  s f (s , µ(s ))V µ (s )+

S t=0

(1 - f (s , µ(s ))

s p(s | s , µ(s ))V µ (s )ds -  s f (s , µ(s ))

S

p(s |s , a )|a =µ(s )V µ (s )ds )ds,
S

(23)

By the continuity of T , r, f and µ, the gradient of V µ (s) over s exists. Now we derive the form of the policy gradient. By definition,

V µ (s) =  r(s, µ(s)) +   f (s, µ(s))V µ (s )|s =T (s,µ(s)) + f (s, µ(s))

 T (s, µ(s)) s V µ (s )|s =T (s,µ(s)) + (1 - f (s, µ(s)))

p(s |s, µ(s))

S

V µ (s )ds -   f (s, µ(s)) p(s |s, a)|a=µ(s)V µ (s )ds +
S
f (s, µ(s))  V µ (s )|s =T (s,µ(s)) + (1 - f (s, µ(s)))

p(s |s, a)|a=µ(s)  V µ (s )ds .
S

(24)

By unrolling (24) with infinite steps, we get



V µ (s) =

tp(s, s , t, µ)( r(s , µ(s )) + 

S t=0

 f (s , µ(s ))V µ (s )|s =T (s,µ(s ))

+ f (s , µ(s ))  T (s , µ(s )) s V µ (s )|s =T (s,µ(s )) + (1 - f (s , µ(s )))

p(s |s , µ(s ))V µ (s )ds -   f (s , µ(s ))
S

p(s |s , a)|a=µ(s )V µ (s )ds )ds ,
S

(25)

where p(s, s , t, µ) denotes the probability density of the state s after t steps following the policy µ. By the definition of J(µ) and the same technique as the proof of Theorem 1, we get (2). By definition,

Qµ (s, a) = r(s, a) + f (s, a)V µ (s )|s =T (s,a) + (1 - f (s, a)) p(s |s, a)V µ (s )ds . (26)
S
14

Under review as a conference paper at ICLR 2019

Then  µ (s)

a Qµ (s, a)|a=µ(s) =

 µ(s) a r(s, a)|a=µ(s) + f (s, µ(s))  µ(s) a T (s, a)|a=µ(s) s V µ (s )|s =T (s,a) + (1 - f (s, µ(s)))

µ(s) a p(s |s, a)|a=µ(s)V µ (s )ds +   f (s, µ(s))
S

V µ (s )|s =T (s,µ(s)) -   f (s, µ(s)) p(s |s, a)V µ (s )ds .
S
(27)

Thus, we get that the policy gradient of (2) is equivalent to the form of the DPG theorem.

D PROOF OF THEOREM 3

Proof. By definition, we have

s, V µ (s) = r(s, µ(s)) + 

V µ (s )ds ,

sD(s,µ (s))

(28)

where D(s, µ(s)) denotes the distribution of the next state. As the value function is convex, we get

By definition, Thus

s, V µ (s)  r(s, µ(s)) + V µ (s )|s =T (s,µ(s)). s, Vµ (s) = r(s, µ(s)) + Vµ (s )|s =T (s,µ(s)). s, V µ (s) - Vµ (s)  (V µ (s ) - Vµ (s ))|s =T (s,µ(s)).

(29) (30) (31)

As these two value functions are bounded, there is a lower bound C such that s, V µ (s) - Vµ (s)  C.

(32)

Combining (31) with (32) repeatedly, we obtain s, V µ (s)  Vµ (s).

(33)

Note that and Thus J (µ)  J(µ).

J (µ) = p0(s)V µ (s)ds.
S
J(µ) = p0(s)Vµ (s)ds.
S

(34) (35)

E IMPLEMENTATION DETAILS
In this section we describle the details of the implementation of GDPG. The configuration of the actor network and the augmented critic network is the same as the implementation of OpenAI Baslines. Each network has two fully connected layers, where each layer has 64 units. The activation function is RelU, the batch size is 128, the learning rate of the actor is 10-4, and the learning rate of the critic is 10-3.
We exploit the model-based technique by estimating the state transition function using deep neural networks. For problems with low-diemensional input space including ComplexPoint-v0, Pendulumv0, HalfCheetah-v2, LunarLanderContinuous-v2, we use the two layers fully connected structure for the transition network. For problems which are more complex, including Humanoid-v2,

15

Under review as a conference paper at ICLR 2019

Paramter Filters for Layer 1 Filters for Layer 2
Kernel Size Paxdding Mode
Pooling Size Strides
Activation Function

Value 32 64 5
Same 2 2
ReLU

Table 1: Configurations.

Environment ComplexPoint-v0 Pendulum-v0 LunarLanderContinuous-v2 Swimmer-v2 HalfCheetah-v2 HumanoidStandup-v2 Humanoid-v2

||S || 5 3 8 8 17
376 376

||A|| 5 1 2 2 6 17 17

Table 2: List of environments.

HumanoidStandup-v2, we apply the Convolutional Neural Networks (CNN). In particular, the network contains two layers of CNN followed by a fully connected layer. The configuration for the CNN layer is as listed in Table 1. The learning rate of the transition network is 10-3. We also add L2 norm regularizer to the loss and the batch size is 128.
Note that the weight of our objective affects the performance of GDPG as discussed in Section 5.3, we test different value of  on all environments, and we get that the value of  = 0.9 achieves the best performance in all environments.

16

