Under review as a conference paper at ICLR 2019
PROXQUANT: QUANTIZED NEURAL NETWORKS VIA PROXIMAL OPERATORS
Anonymous authors Paper under double-blind review
ABSTRACT
To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straightthrough gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works. Building upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov's dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called PROXQUANT, that formulates quantized network training as a regularized learning problem instead and optimizes it via the proxgradient method. PROXQUANT does back-propagation on the underlying fullprecision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, PROXQUANT outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. For binary quantization, our analysis shows both theoretically and experimentally that PROXQUANT is more stable than the straight-through gradient method (i.e. BinaryConnect), challenging the indispensability of the straight-through gradient method and providing a powerful alternative.
1 INTRODUCTION
Deep neural networks (DNNs) have achieved impressive results in various machine learning tasks (Goodfellow et al., 2016). High-performance DNNs typically have over tens of layers and millions of parameters, resulting in a high memory usage and a high computational cost at inference time. However, these networks are often desired in environments with limited memory and computational power (such as mobile devices), in which case we would like to compress the network into a smaller, faster network with comparable performance.
A popular way of achieving such compression is through quantization ­ training networks with lowprecision weights and/or activation functions. In a quantized neural network, each weight and/or activation can be representable in k bits, with a possible codebook of negligible additional size compared to the network itself. For example, in a binary neural network (k = 1), the weights are restricted to be in {±1}. Compared with a 32-bit single precision float, a quantized net reduces the memory usage to k/32 of a full-precision net with the same architecture (Han et al., 2015; Courbariaux et al., 2015; Rastegari et al., 2016; Hubara et al., 2017; Zhou et al., 2016; Zhu et al., 2016). In addition, the structuredness of the quantized weight matrix can often enable faster matrixvector product, thereby also accelerating inference (Hubara et al., 2017; Han et al., 2016).
Typically, training a quantized network involves (1) the design of a quantizer q that maps a full-precision parameter to a k-bit quantized parameter, and (2) the straight-through gradient method (Courbariaux et al., 2015) that enables back-propagation from the quantized parameter back onto the original full-precision parameter, which is critical to the success of quantized network training. With quantizer q, an iterate of the straight-through gradient method (see Figure 1a) proceeds as t+1 = t - tL()|=q(t), and q() (for the converged ) is taken as the output model. For
1

Under review as a conference paper at ICLR 2019

training binary networks, choosing q(·) = sign(·) gives the BinaryConnect method (Courbariaux et al., 2015).
Though appealingly simple and empirically effective, it is information-theoretically rather mysterious why the straight-through gradient method works well, at least in the binary case: while the goal is to find a parameter   {±1}d with low loss, the algorithm only has access to stochastic gradients at {±1}d. As this is a discrete set, a priori, gradients in this set do not necessarily contain any information about the function values. Indeed, a simple one-dimensional example (Figure 1b) shows that BinaryConnect fails to find the minimizer of fairly simple convex Lipschitz functions in {±1}, due to a lack of gradient information in between.

rL(q(t))
<latexit sha1_base64="1xQczOeLSZHOFvKSuvor2HX666s=">AAACFXicdVBNaxsxENW6TZo6aeK2x15ETcCGxGjNtrZvgV56yMGF+gO8xszKWltYq91IswWz+E/00r/SSw8NoddAbvk3lT8KbWgGhB7vzTBvXpQpaZGxe6/05One/rOD5+XDoxfHJ5WXr/o2zQ0XPZ6q1AwjsEJJLXooUYlhZgQkkRKDaPFhrQ++CGNlqj/jMhPjBGZaxpIDOmpSOQujVE3tMnFfcR5qiBTQy1qYAM5tXFytaiHOBcIE6/XVpFJljQ4LWCegrBGwdz5rOeAHbb/1nvoNtqkq2VV3UrkLpynPE6GRK7B25LMMxwUYlFyJVTnMrciAL2AmRg5qSIQdF5urVvTUMVMap8Y9jXTD/j1RQGLXxl3nxu1DbU3+TxvlGLfHhdRZjkLz7aI4VxRTuo6ITqURHNXSAeBGOq+Uz8EARxdk2YXw51L6OOg3G75L5lNQvWju4jggb8hbUiM+aZEL8pF0SY9w8pV8Jz/JtffN++HdeL+2rSVvN/Oa/FPe7W/6r59G</latexit>
Straight-through

q(t)
<latexit sha1_base64="lFh5oPMR4/y+f66EHCOLQ6e7ruCrvVGUjGfJ1oHl25d9SELV3STfltDnk9gZzFOvhahvjlwCdSef/zN9qHaGvob04PQRCcQ8P1vEor9RSEdKW6nJNM8GIKwXz6gyMc40w8=">AAACB2B/3HXicbdZVDBJLNSg8NMBxAEFIOMJ23bXvJ4PW1re4PLWz8VvfebV6qdVR6ErqSFd1elWBQIrCGsBNxAo8ISMtELhQ1iULuNSU02C8EOoZmmz8aMM62pbdkFnRQ5dsHwt3p4BguMTHrhVGucFjZCXuKBF/wL9WbYNwgZCAHmCVOJdlIoq5MaVS1ReS2kMT5SM0kThy/4Nk7tSdbzmpabGOFsekLxZMxhuRuyzc0gCRkcYhi2oIhlQ6DN32MyH65/d0Nu+eBrF/SB+2U58LvEeG3FHLhfhSPCTF9IRx+3v6ehizoyPP9v3+4pf8uxz20GvJl/9Qs7ibMHLD0qIw4NOKYQiBK+HzPykHmJgkzjnzg/6/Ie58znvm2VyHS4FMcHVnuvV9lXfEf9LphXEQ7cjmKUSxBhYHNdYUAbDI9XFrezl/MvbvWbMpdVD20h31Pmp5Y9bFh2/YZcV3+Wg1N1blfr9Xe5u1ZYg33XfSj0TNMVpf7knumOztZjV3rkr3s6rxe9d2Nud7dZb/m3Wo4TBd/215vbfWz2P/eHW06gkRiyCjbesnl2JwVuiNlZOWhzRKNlS8NIz1ly5qsjVlShnoKh0mEZCEFdqGpq0XJuHHqUsAJeu2aYkCiBo5SQVJKFxwHNCr1UtpAGPs8omLgWcAUg5gPBJzCK61ysrLUCTEdFc0fxWHlqbPEGjHaJf6kBS5FOfrfeJ0AwDJift7B81j7vUeuH+3+nghGmFs4tPTzx/nzLO+t/y5XWcjNTziy8YJQsuVtkrR90D4M4MluzNFrhQ4JCrmWDehESxMZtS+OnmsUYv5xCuQ5fykQ7NUQEh6ZOiwEh6DZSto1hFyzOgnaFCrlqWIeai3CA+pdeVWb+tBLxMrG/kEW4fT7i28BbIxiJ4V2gvWYxGSEDhaH2CQE9us5LYb0rCamnLKg/Ng0YabkWp+ncIpGfRw5Soe3S7+7GxQRb2W8KaM4VDcIDyP6F0UgjBxJ47xEMwQaGJePH0JS5Ad2ht8+7XEOnNFD9A+pUBW73tvU8cebxoUfuayNu3e4AZSbx1tat1FkEqQVwlr/7lmJgeo3YJW29tXULqbuYiOcEYwW6FU11XiLG3puUVza/VvJVK70dqKG2KgqDBmjUSghPVnSLNqrMYqxl7n8qW61XRrQbRm5tSr/tRWZqahxp2Mz4x8zV1jTi9j/Y6duTcdnSpkM7WybOKs7MRVyAE0qlUoN1m2+bv+3Cf5+SofcOY/uT84TzkOvBtQ3IlBxea9Zc4jnBK4wuhKwLTgZ2EA9JKZqgbJigqO3VesSNfdvtUedjXcs2dSMzIRDuDHWwghYsNlF3NuR7JVeRASGvnKaiInY24sgM5FjkWiKP0kOy/za5LSTsBndiq28RxWuLqYFelARUtYOIk1XLEiZmYDl2HDdZTrFV3MylEhGnX6ZSy0mqa4oIrbD8HTY4n6kGTcYXKPhMCWpe0qVSKbN+o5YyR5l314XwdTtjbMfd8cGKmVZqlaMCVcWJmAkBNWfCtEZWhiSCvpPIqMzEpVPHiR84/xdABhwzeM4uYnKaLh5nlGS+bG6/7Vvdl0SYx7i1MQkI0Jclr1Ngq2ZD1uqayRNrRQDt6qrazkt8ciqjsZufD38Yw4HAoiEYykjiSPD/T19bB9G2zBms25buN9imDSJ7hfXE137Ikom439GdBxQq6unH/3OjzaGbwZMSKTc3YX0mgkASIQSJ6Yx5ZJnc8MVArgBMTCFYXuNhvEQjk0EkoIbNxUJyQ3iUwXJ0ADBZHihl2Du9GIeCjyhBK4LNkAFgGhARXIo/j4z+IleBXKRAYVeOa0jvEYraKnBqoM35C+Hn1TpM27XrsduTvYEQ/qdpOqEg5E6BKgEM9odLO6wDOuCr4ojmIqwQ+XOwg84fzeRxGtK81S4Ku/4xDIRhe9oYpa4dF<Il0f4A/8KQlqc2GaWzTztrueM/r5AxqdKCibPztWFI>D/dsFDvuKdL2PyDOIzNgcuKIyveo6zyBNsNicC0dA8gEronzgjLVKiIynDRkUFhrdASVs0U1BRwR0/C5TIeH0Vo8qzgw+oZjhQqOaPI8ivo6WLjAJv4/h0WE9gok</JC7Ufl0Qa6sbtxT3NeVFxwvnbiPjtzBb>8ojavLtxw8ubIu+oioyN3XGTjU4GsBWPebwUK7KfRYoBDnx4ghy=v3=nT<w/bWlRamtgfeAxwi=<t>/latexit>

tp+q1 <latexit sha1_base64="r54nLuiBVj6jJQBHCcoRh2wIVfU=">AAACC3icdVDLSgMxFM34tr6qLt2EFkEQhqS2tt0JblxWsFro1JJJUxvMPEzuCGWYvRt/xY0LRdz6A+78G9OHoKIHQg7n3Mu99/ixkgYI+XBmZufmFxaXlnMrq2vrG/nNrXMTJZqLJo9UpFs+M0LJUDRBghKtWAsW+Epc+NfHI//iVmgjo/AMhrHoBOwqlH3JGVipmy94fqR6ZhjYL/VgIIBdpp4OcHyTdVPYp1nWzReJWyJ1cljFxD2oVOq0ZkmVViu0jKlLxiiiKRrd/LvXi3gSiBC4Ysa0KYmhkzINkiuR5bzEiJjxa3Yl2paGLBCmk45vyfCuVXq4H2n7QsBj9XtHygIzWtdWBgwG5rc3Ev/y2gn0a51UhnECIuSTQf1EYYjwKBjck1pwUENLGNfS7or5gGnGwcaXsyF8XYr/J+cllxKXnpaLR6VpHEtoBxXQHqKoio7QCWqgJuLoDj2gJ/Ts3DuPzovzOimdcaY92+gHnLdPjnGb9Q==</latexit>

ts+t 1 <latexit sha1_base64="FaWdjcVKlJ0n8LbrhKDBofzT+80=">AAACC3icdVDLSgMxFM34rPVVdekmtAiCUGZa2053BTcuK9gHdMaSSdM2NPMguSOUYfZu/BU3LhRx6w+4829MH4KKHgg5nHMv997jRYIrMM0PY2V1bX1jM7OV3d7Z3dvPHRy2VRhLylo0FKHsekQxwQPWAg6CdSPJiO8J1vEmFzO/c8uk4mFwDdOIuT4ZBXzIKQEt9XN5xwvFQE19/SUOjBmQfgJnVnqTONLHCtK0nyuYRatUtqsmNovlStmq1DSp1m27WsdW0ZyjgJZo9nPvziCksc8CoIIo1bPMCNyESOBUsDTrxIpFhE7IiPU0DYjPlJvMb0nxiVYGeBhK/QLAc/V7R0J8NVtXV/oExuq3NxP/8noxDG034UEUAwvoYtAwFhhCPAsGD7hkFMRUE0Il17tiOiaSUNDxZXUIX5fi/0m7VLR0VlfnhUZpGUcGHaM8OkUWqqEGukRN1EIU3aEH9ISejXvj0XgxXhelK8ay5wj9gPH2CaMinAU=</latexit>

ProxQuant

prox

t <latexit sha1_base64="k8oWYEM4F9JUhgq5441TNQW4XmE=">AAAB/nicdVDLSgMxFM3UV62vUXHlJlgEV2Wmte10V3DjsoJ9QKeUTCZtQzMPkjtCGQr+ihsXirj1O9z5N2baCip6IORwzr3k5Hix4Aos68PIra1vbG7ltws7u3v7B+bhUUdFiaSsTSMRyZ5HFBM8ZG3gIFgvlowEnmBdb3qV+d07JhWPwluYxWwQkHHIR5wS0NLQPHG9SPhqFugrdWHCgAxhPjSLVskuV5yaha1SpVqxq3VNag3HqTWwXbIWKKIVWkPz3fUjmgQsBCqIUn3bimGQEgmcCjYvuIliMaFTMmZ9TUMSMDVIF/Hn+FwrPh5FUp8Q8EL9vpGSQGUJ9WRAYKJ+e5n4l9dPYOQMUh7GCbCQLh8aJQJDhLMusM8loyBmmhAquc6K6YRIQkE3VtAlfP0U/0865ZKtu7q5LDbLqzry6BSdoQtkozpqomvUQm1EUYoe0BN6Nu6NR+PFeF2O5ozVzjH6AePtE7velpc=</latexit>

<latexit sha1_base64="kh0E+M6CPmy4C+q6gnLxTwor+7Q08UkKXft7P37d2yLwdA3Q+AMktPs+KuZAjaAdXWPzluSaRd/OyqvSias7mQlPSx3tQov1RAE3Z6TKGVkCM7rXBHoyGN4Mcowg=">AAACB/C2XnicdbVZADB9NSwgNMBxEFINJ3XZzvn2M1/8Lb/agZ86v+VoqV0tKaqdbXZ1R9NmeZrNYMgDhQEi8CIEiLsx0oiMwuDOCNHZyzKsoZFzhV0abGn2CuQg0tnXpNYEus8WGOFLwHFGLshEwLZBBUwMgaDbI4AJOZmQqVCB5oXOCEV2A5hKFvRYMzuK2s5mb2J0kzr42kMaWyb49mYZtDG/s7/kaev7MOs23yJTLRJsNk02nhCThWDHwpCXHGOrl0Ht1BWj/F4xCX2lr/s95FExi2xY4fFk6CorIG9ERI34WfrMv/+o4f+z3/YB3A8pN/nz2f40n4/J1Gjbp+TDCJVVkYwyQ1DZg8QR+zo0gsgZkMQ+zfnCWjIXLfvP9btSR8w2c6l3Q2ubmwYl5LS8kWZyfQW8UeOpmKkBtNFCHpXhLZ9SepE8I+/kbmVWb9PtFsdwv3b3NbWsj2/VTp+41kxg6bZ13fOunPT9TZUgsM9fubMNfG1fmBGzWuEaxuj3OmtkS9L1z7N+9SuwY7m8suoeX+Lr2CXRofqWc9zvUWzX0lgt95+kjM9s74mmtNis0eLlnlFdGzimCqlwu5NGKRkjjnOSwmqxVmRWFUCPp3MXsWmIZU2MoHr5CgVoINBVrVSaJlSiCGwNmpdSkZp8iLRgJgQURRVu4zKyIpl+VLeKuZ1FNEfBlQyLZMbjNigSI6EDfImAt0qhlSi5VWu497EiA7VHKA+786g7f1sPD8JvhTyVLzuPO6LPX/F+GwTzlk2fYmQiCZrdYWSKM6JsDM20nrsNa9o4ZMpiW9+MaYTyNItPnZTEUoSNC4h0kvCXPD7DoLKOlq6vauZtoKZmR4ydaIRSyrTSHQuNaG5qBAdqWJ1rWlLCR1MT7kwgWR2UlNMIqpw7ElVmdprxi1UDjH+xsC7/S9oZBmvYyj/5a9NiXaa+ub7Ug3Y+G7IXxaoUSYYSAsKZgh7yIKVuRA0qHDWTDLi3NEU9eGJNiBCxg1WraJD0EOG2nEXdBEThb0NsdMEqboFL4daBPcYUtWScsMivUtWXfrvNySYv+qVKLaxLOFUywjPwqVZ73gARmyJ9LF2H/LMEiSyFFcnwQBipXUWmXlKUCFqyndHzbibdf7uvGLDszgX26UvMqEPoOvNji+fM4rMN7uVXGbkRs+DRQXrNt29RhxHuG4x5jzQFLf1vKzo8r3iW6YTxNUdGDU6KkPCJ/76BZVAjZOJ0nJjzNL+lo3RvY5L5koybVYta29k6ERvJAnd3cJ498RDVZ4zDkMQ5usrISKZNGQUH9ur0IbkQKFjPCOc1Oasj8BxtZsBj29dQ+tlzSZRmz7D9IXhQCZrGNqBg07w4UJGnbOIaHS2yJK6lMWzPSRgj5/EVKomjLhUBdgpituekySW5kKVUlKBHthfu1HBgEsIrnalfurdxVBqj2EJL1TNShTRaSqbzrKI4XZrOHE6J+IwdK2nKGrsCbY0g1yi2xV0ovtSpM5mUmNwRADIltbudXmuWX5mieazUnMJSDpNwuCL7p0Sh3qmLpIYx3z9ujKRckxT2ceNR8QwjdGzYKat72RtnXS+fnvbH71Ol8wRYLpgkl/hEJoMtd3N5srF1HNKBSylQYnsX/aGq8W4zQUwJnZM3ABN2H6ZeY/LrxOna/S/f1b3vM2NGxa28dH9LDU48D/ol7Y8K297ZhNfovMYmdpsd1WBLgKuE2v638fw3abyMCwmqYUdmRVAmWOy6JNMinHxEkKMLSFQMzaFRuHfpzXwENx07E9GuVqId1ZNcV8oFS83Gp0AEiMoHaQ5ZD6oS9OUegMTCxWsBC20KLrIKg7p4OhZoI0QGsg4FcqKBFsPTQWX7qeTDROXlvw0Y4qnLCAcw3zbS5r6ysXLnBcab2/r4lKs5vtuRTAqe80cDpw5tGWhw6OCNwv9LGLrIFwOc4VcSwi3hIQF76+hEU8C1rIfzxF06Y1vbB8Kzs4p/fuxy3I+fSRoGn4qr+1U<XqYw/EPrkJlMDv9adkmMOthjVeDkezxMNBOisyctMC7vF>XzAyqeBKOauxI/7LF4fACsAJWIvtdSsXTcgob2fgZKAsIR7vhK74WfgBpvVBEMHblkogqxu54BOI/BSibAXcuHnkp34uREAbcGYPh4el5ugJSr1bv2XcYg3Iki50JXwjEf=y+8=5ew<N/XD1Ufl5eWaDtgf9He/6Jfxszib9vtg/>XRJy8ZWVOTvunLGmwNTD+enk9Yb3+IYyD0A3Q9i=7v=rH<B5/WlRpamtjeUxE=i<t/>latexit>
et+1 <latexit sha1_base64="ONl/Lpn4RKqUA48Yp1vZfJuuwTw=">AAACDnicdVBNaxsxENW6aeu4aeu2x1xEjKEQWCTXru1bIJceE4g/wGuMVju2hbUfSLMtZtlf0Ev/Si89NJRee+4t/ybyRyAJ7QOhx3szzMwLM60sMnbjVZ4cPH32vHpYe3H08tXr+pu3Q5vmRsJApjo141BY0CqBASrUMM4MiDjUMApX5xt/9BmMVWlyhesMprFYJGqupEAnzerNIEx1ZNex+4rgi4oAlY6gCHAJKMpZgae8LGf1BvNbrM8+dinzP3Q6fd5zpMu7Hd6m3GdbNMgeF7P63yBKZR5DglILayecZTgthEElNZS1ILeQCbkSC5g4mogY7LTYnlPSplMiOk+NewnSrXq/oxCx3WzsKmOBS/vY24j/8iY5znvTQiVZjpDI3aB5rimmdJMNjZQBiXrtiJBGuV2pXAojJLoEay6Eu0vp/8mw5XPm88t246y1j6NKjskJeU846ZIz8olckAGR5Cv5Tn6Sa++b98P75f3elVa8fc878gDen1tuWJ2Q</latexit>

rL(t)

quantize

(a) (b)
Figure 1: (a) Comparison of the straight-through gradient method and our PROXQUANT method. The straightthrough method computes the gradient at the quantized vector and performs the update at the original real vector; PROXQUANT performs a gradient update at the current real vector followed by a prox step which encourages quantizedness. (b) A two-function toy failure case for BinaryConnect. The two functions are f1(x) = |x + 0.5| - 0.5 (blue) and f-1(x) = |x - 0.5| - 0.5 (orange). The derivatives of f1 and f-1 coincide at {-1, 1}, so any algorithm that only uses this information will have identical behaviors on these two functions. However, the minimizers in {±1} are x1 = -1 and x-1 = 1, so the algorithm must fail on one of them.
In this paper, we formulate the problem of model quantization as a regularized learning problem and propose to solve it with a proximal gradient method. Our contributions are summarized as follows.
· We present a unified framework for defining regularization functionals that encourage binary, ternary, and multi-bit quantized parameters, through penalizing the distance to quantized sets (see Section 3.1). For binary quantization, the resulting regularizer is a W -shaped non-smooth regularizer, which shrinks parameters towards either -1 or 1 in the same way that the L1 norm regularization shrinks parameters towards 0. We demonstrate that the prox-operators for regularizers that come out of our framework often admit linear-time solutions (or linear time approximation heuristics) which result in numerically exact quantized parameters.
· We propose training quantized networks using PROXQUANT (Algorithm 1) -- a stochastic proximal gradient method with a homotopy scheme. Compared with the straightthrough gradient method, PROXQUANT has access to additional gradient information at non-quantized points, which avoids the problem in Figure 1b and its homotopy scheme prevents potential overshoot early in the training (Section 3.2). Algorithmically, PROXQUANT involves just adding a simple proximal step with respect to a quantization-inducing regularizer after each stochastic gradient step (Figure 1a), thus can be efficiently implemented under any major deep learning frameworks without incurring significant system overhead and be used as a modular component to add to the training pipeline of any deep networks to result in a quantized network.
· We demonstrate the effectiveness and flexibility of PROXQUANT through systematic experiments on (1) image classification with ResNets (Section 4.1); (2) language modeling with LSTMs (Section 4.2). The PROXQUANT method outperforms the state-of-the-art results
2

Under review as a conference paper at ICLR 2019
on binary quantization and is comparable with the state-of-the-art on ternary and multi-bit quantization.
· For binary nets, we show that BinaryConnect suffers from more optimization instability than PROXQUANT through (1) a theoretical characterization of convergence for BinaryConnect (Section 5.1) and (2) a sign change experiment on CIFAR-10 (Section 5.2). Experimentally, PROXQUANT finds better binary nets that is also closer to the initialization in the sign change metric.
1.1 PRIOR WORK
Methodologies Han et al. (2015) propose Deep Compression, which compresses a DNN via sparsification, nearest-neighbor clustering, and Huffman coding. This architecture is then made into a specially designed hardware for efficient inference (Han et al., 2016). In a parallel line of work, Courbariaux et al. (2015) propose BinaryConnect that enables the training of binary neural networks, and Li & Liu (2016); Zhu et al. (2016) extend this method into ternary quantization. Training and inference on quantized nets can be made more efficient by also quantizing the activation (Hubara et al., 2017; Rastegari et al., 2016; Zhou et al., 2016), and such networks have achieved impressive performance on large-scale tasks such as ImageNet classification (Rastegari et al., 2016; Zhu et al., 2016). In the NLP land, quantized language models have been successfully trained using alternating multi-bit quantization (Xu et al., 2018).
Theories Li et al. (2017) prove the convergence rate of stochastic rounding and BinaryConnect on convex problems and demonstrate the advantage of BinaryConnect over stochastic rounding on non-convex problems. Anderson & Berg (2017) demonstrate the effectiveness of binary networks through the observation that the angles between high-dimensional vectors are approximately preserved when binarized, and thus high-quality feature extraction with binary weights is possible. Ding et al. (2018) show a universal approximation theorem for quantized ReLU networks.
Principled methods Sun & Sun (2018) perform model quantization through a Wasserstein regularization term and minimize via the adversarial representation, similar as in Wasserstein GANs (Arjovsky et al., 2017). Their method has the potential of generalizing to other generic requirements on the parameter, but might be hard to tune due to the instability of the inner maximization problem.
While preparing this manuscript, we discovered the independent work of Carreira-Perpina´n (2017); Carreira-Perpina´n & Idelbayev (2017). They formulate quantized network training as a constrained optimization problem and propose to solve them via augmented Lagrangian methods. From an optimization perspective, our views are largely complementary: they treat the quantization as a constraint, whereas we encourage quantization through a regularizer. Due to time constraints, we did not do experimental comparison (they only reported results on VGG whereas we focus on ResNets) ­ as they solve a full augmented Lagrangian minimization in between each compression step, successful training of their LC algorithm will at least require a careful tuning of this inner optimization procedure.
2 PRELIMINARIES
The optimization difficulty of training quantized models is that they involve a discrete parameter space and hence efficient local-search methods are often prohibitive. For example, the problem of training a binary neural network is to minimize L() for   {±1}d. Projected SGD on this set will not move unless with an unreasonably large stepsize (Li et al., 2017), whereas greedy nearestneighbor search requires d forward passes which is intractable for neural networks where d is on the order of millions. Alternatively, quantized training can also be cast as minimizing L(q()) for   Rd and an appropriate quantizer q that maps a real vector to a nearby quantized vector, but   q() is often non-differentiable and piecewise constant (such as the binary case q(·) = sign(·)), and thus back-propagation through q does not work.
3

Under review as a conference paper at ICLR 2019

2.1 THE STRAIGHT-THROUGH GRADIENT METHOD

The pioneering work of BinaryConnect (Courbariaux et al., 2015) proposes to solve this problem via

the straight-through gradient method, that is, propagate the gradient with respect to q() unaltered

to , i.e.

to let

L 

:=

L q()

.

One iterate of the straight-through gradient method (with the SGD

optimizer) is

t+1 = t - tL()|=q(t).

This enables the real vector  to move in the entire Euclidean space, and taking q() at the end of training gives a valid quantized model. Such a customized back-propagation rule yields good empirical performance in training quantized nets and has thus become a standard practice (Courbariaux et al., 2015; Zhu et al., 2016; Xu et al., 2018). However, as we have discussed, it is information theoretically unclear how the straight-through method works, and it does fail on very simple convex Lipschitz functions (Figure 1b).

2.2 STRAIGHT-THROUGH GRADIENT AS LAZY PROJECTION

Our first observation is that the straight-through gradient method is equivalent to a dual-averaging method, or a lazy projected SGD (Xiao, 2010). In the binary case, we wish to minimize L() over Q = {±1}d, and the lazy projected SGD proceeds as

t = ProjQ(t) = sign(t) = q(t), t+1 = t - tL(t).

(1)

Written compactly, this is t+1 = t - tL()|=q(t), which is exactly the straight-through gradient method: take the gradient at the quantized vector and perform the update on the original
real vector.

2.3 PROJECTION AS A LIMITING PROXIMAL OPERATOR

We take a broader point of view that a projection is also a limiting proximal operator with a suitable
regularizer, to allow more generality and to motivate our proposed algorithm. Given any set Q, one could identify a regularizer R : Rd  R0 such that the following hold:

R() = 0,   Q and R() > 0,  / Q.

(2)

In the case Q = {±1}d for example, one could take

d
R() = Rbin() = min {|j - 1|, |j + 1|}.
j=1

(3)

The proximal operator (or prox operator) (Parikh & Boyd, 2014) with respect to R and strength

 > 0 is

proxR() := arg min
Rd

1

2
 -  + R()

22

.

In the limiting case  = , the argmin has to satisfy R() = 0, i.e.   Q, and the prox operator is

to minimize

 - 0

2 2

over



 Q, which is the Euclidean projection onto Q.

Hence, projection is

also a prox operator with  = , and the straight-through gradient estimate is equivalent to a lazy

proximal gradient descent with and  = .

While the prox operator with  =  correponds to "hard" projection onto the discrete set Q, when  <  it becomes a "soft" projection that moves towards Q. Compared with the hard projection, a finite  is less aggressive and has the potential advantage of avoiding overshoot early in training. Further, as the prox operator does not strictly enforce quantizedness, it is in principle able to query the gradients at every point in the space, and therefore has access to more information than the straight-through gradient method.

4

Under review as a conference paper at ICLR 2019

3 QUANTIZED NET TRAINING VIA REGULARIZED LEARNING

We propose the PROXQUANT algorithm, which adds a quantization-inducing regularizer onto the loss and optimizes via the (non-lazy) prox-gradient method with a finite . The prototypical version of PROXQUANT is described in Algorithm 1.

Algorithm 1 PROXQUANT: Prox-gradient method for quantized net training
Require: Regularizer R that induces desired quantizedness, initialization 0, learning rates {t}t0, regularization strengths {t}t0 while not converged do Perform the prox-gradient step

t+1 = arg min
Rd

L(t) +

 - t, L(t)

1 +
2t

 - t

2 2

+

tR()

= proxttR t - tL(t) .

(4) (5)

The inner SGD step in eq. (5) can be replaced by any preferred stochastic optimization method such as Momentum SGD or Adam (Kingma & Ba, 2014). end while

Compared to usual full-precision training, PROXQUANT only adds a prox step after each stochastic gradient step, hence can be implemented straightforwardly upon existing full-precision training. As the prox step does not need to know how the gradient step is performed, our method adapts to other stochastic optimizers as well such as Adam. Further, each iteration is a prox-gradient step over the objective L() + tR() with learning rates t, and by choosing (t, t) we obtain a joint control over the speed of training and falling onto the quantized set.
In the remainder of this section, we define a flexible class of quantization-inducing regularizers through "distance to the quantized set", derive efficient algorithms of their corresponding prox operator, and propose a homotopy method for choosing the regularization strengths. Our regularization perspective subsumes most existing algorithms for model-quantization (e.g.,(Courbariaux et al., 2015; Han et al., 2015; Xu et al., 2018)) as limits of certain regularizers with strength   . Our proposed method can be viewed as a principled generalization of these methods to  < .

3.1 REGULARIZATION FOR MODEL QUANTIZATION

Let Q  Rd be a set of quantized parameter vectors. An ideal regularizer for quantization would be to vanish on Q and reflect some type of distance to Q when  / Q. To achieve this, we propose L1 and L2 regularizers of the form

R() = inf
0 Q

 - 0 1

or R() = inf
0 Q

 - 0

2 2

.

(6)

This is a highly flexible framework for designing regularizers, as one could specify any Q and choose between L1 and L2. Specifically, Q encodes certain desired quantization structure. By appropriately choosing Q, we can specify which part of the parameter vector to quantize1, the number of bits to

quantize to, whether we allow adaptively-chosen quantization levels and so on.

The choice of distance metrics will result in distinct properties in the regularized solutions. For example, choosing the L1 version leads to non-smooth regularizers that induce exact quantizedness in the same way that L1 norm regularization induces sparsity (Tibshirani, 1996), whereas choosing the squared L2 version leads to smooth regularizers that induce quantizedness "softly".
In the following, we present a few examples of regularizers under our framework eq. (6) which induce binary weights, ternary weights and multi-bit quantization. We will also derive efficient algorithms (or approximation heuristics) for solving the prox operators corresponding to these regularizers, which generalize the projection operators used in the straight-through gradient algorithms.

1Empirically, it is advantageous to keepthe biases of each layers and the BatchNorm layers at full-precision, which is often a negligible fraction, say 1/ d of the total number of parameters

5

Under review as a conference paper at ICLR 2019

Binary neural nets In a binary neural net, the entries of  are in {±1}. A natural choice would be taking Q = {-1, 1}d. The resulting L1 regularizer is

d

R() = inf
0 {±1}d

 - 0

1

=

j=1

inf
[0]j {±1}

|j

-

[0]j |

d
= min {|j - 1|, |j + 1|} =  - sign() 1 .
j=1

(7)

This is exactly the binary regularizer Rbin that we discussed earlier in eq. (3). Figure 2 plots the W-shaped one-dimensional component of Rbin from which we see its effect for inducing {±1}
quantization in analog to L1 regularization for inducing exact sparsity.

The prox operator with respect to Rbin, despite being a non-convex optimization problem, admits a simple analytical solution:

2.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00
3 2 10 1 2 3

proxRbin () = SoftThreshold(, sign(), )
= sign() + sign( - sign()) [| - sign()| - ]+. (8)
We note that the choice of the L1 version is not unique: the squared L2 version works as well, whose prox operator is given by ( +  sign())/(1 + ). See Appendix A.1 for the derivation of these

Figure 2: W-shaped regularizer for binary quantization.

prox operators and the definition of the soft thresholding operator.
Multi-bit quantization with adaptive levels. Following (Xu et al., 2018), we consider k-bit quantized parameters with a struc-

tured adaptively-chosen set of quantization levels, which translates

into

k

Q=

ibi : {1, . . . , k}  R, bi  {±1}d = 0 = B :   Rk, B  {±1}d×k .

i=1
The squared L2 regularizer for this structure is

(9)

Rk-bit() =

inf

Rk ,B {±1}d×k

 - B

2 2

,

(10)

which is also the alternating minimization objective in (Xu et al., 2018).

We now derive the prox operator for the regularizer eq. (10). For any , we have

proxRk-bit () = arg min


= arg min

inf

 Rk,B{±1}d×k

12

- +

inf

2 2 Rk,B{±1}d×k

1

2
- +

2
 - B

.

22

2

2
 - B
2

(11)

This is a joint minimization problem in (, B, ), and we adopt an alternating minimization schedule to solve it:

(1)

Minimize over  given (B, ), which has a closed-form solution  =

+2B 1+2

.

(2) Minimize over (B, ) given , which does not depend on 0, and can be done via calling the alternating quantizer of (Xu et al., 2018): B = qalt().

Together, the prox operator generalizes the alternating minimization procedure in (Xu et al., 2018), as  governs a trade-off between quantization and closeness to . To see that this is a strict generalization, note that for any  the solution of eq. (11) will be an interpolation between the input  and its Euclidean projection to Q. As   +, the prox operator collapses to the projection.

Ternary quantization Ternary quantization is a variant of 2-bit quantization, in which weights are constrained to be in {-, 0, } for real values ,  > 0. We defer the derivation of the ternary
prox operator into Appendix A.2.

6

Under review as a conference paper at ICLR 2019

3.2 HOMOTOPY METHOD FOR REGULARIZATION STRENGTH
Recall that the larger t is, the more aggressive t+1 will move towards the quantized set. An ideal choice would be to (1) force the net to be exactly quantized upon convergence, and (2) not be too aggressive such that the quantized net at convergence is sub-optimal.
We let t be a linearly increasing sequence, i.e. t :=  · t for some hyper-parameter  > 0 which we term as the regularization rate. With this choice, the stochastic gradient steps will start off close to full-precision training and gradually move towards exact quantizedness, hence the name "homotopy method". The parameter  can be tuned by minimizing the validation loss, and controls the aggressiveness of falling onto the quantization constraint. There is nothing special about the linear increasing scheme, but it is simple enough and works well as we shall see in the experiments.

4 EXPERIMENTS

We evaluate the performance of PROXQUANT on two tasks: image classification with ResNets, and language modeling with LSTMs. On both tasks, we show that the default straight-through gradient method is not the only choice, and our PROXQUANT can achieve the same and often better results.

4.1 IMAGE CLASSIFICATION ON CIFAR-10
Problem setup We perform image classification on the CIFAR-10 dataset, which contains 50000 training images and 10000 test images of size 32x32. We apply a commonly used data augmentation strategy (pad by 4 pixels on each side, randomly crop to 32x32, do a horizontal flip with probability 0.5, and normalize). Our models are ResNets (He et al., 2016) of depth 20, 32, and 44 with weights quantized to binary or ternary.

Method We use PROXQUANT with regularizer eq. (3) in the binary case and eqs. (13) and (14) in the ternary case, which we respectively denote as PQ-B and PQ-T. The training is initialized at pre-trained full-precision nets (warm-start). For the regularization strength we use the homotopy method t =  · t with  = 10-4. We initialize at pre-trained full-precision networks and use the Adam optimizer with constant learning rate 0.01. To accelerate training in the final stage, we do a hard quantization   q() at epoch 400 and keeps training till the 600-th epoch to stabilize the BatchNorm layers.
We compare with BinaryConnect (BC) for binary nets and Trained Ternary Quantization (TTQ) (Zhu et al., 2016) for ternary nets. For BinaryConnect, we haven't found reported results with ResNets on CIFAR-10, and we train with the recommended Adam optimizer with learning rate decay (Courbariaux et al., 2015) (initial learning rate 0.01, multiply by 0.1 at epoch 81 and 122, hard-quantize at epoch 400), which we find leads to the best result for BinaryConnect.

Result The top-1 classification errors are reported in Table 1. For binary nets, our PROXQUANT-
Binary consistently yields better results than BinaryConnect. For ternary nets, our results are comparable with the reported results of TTQ,2 and the best performance of our method over 4 runs (from
the same initialization) is slightly better than TTQ.

Table 1: Top-1 classification error of quantized ResNets on CIFAR-10. Performance is reported in mean(std) over 4 runs, where for PQ-T we report in addition the best of 4 (Bo4).

Model (Bits) ResNet-20 ResNet-32 ResNet-44

Full-Precision (32) 8.06 7.25 6.96

BC (1) 9.49 (0.22) 8.66 (0.36) 8.26 (0.24)

PQ-B (ours) (1)
9.15 (0.21) 8.40 (0.23) 7.79 (0.06)

TTQ (2) 8.87 7.63 7.02

PQ-T (ours) (2)
8.40 (0.13) 7.65 (0.15) 7.05 (0.08)

PQ-T (Bo4) (2) 8.22 7.53 6.98

2We note that our PROXQUANT-Ternary and TTQ are not strictly comparable: we have the advantage of using better initializations; TTQ has the advantage of a stronger quantizer: they train the quantization levels (+, -) whereas our quantizer eq. (14) pre-computes them from the current full-precision parameter.

7

Under review as a conference paper at ICLR 2019

4.2 LANGUAGE MODELING WITH LSTMS
Problem setup We perform language modeling with LSTMs Hochreiter & Schmidhuber (1997) on the Penn Treebank (PTB) dataset (Marcus et al., 1993), which contains 929K training tokens, 73K validation tokens, and 82K test tokens. Our model is a standard one-hidden-layer LSTM with embedding dimension 300 and hidden dimension 300. We train quantized LSTMs with the encoder, transition matrix, and the decoder quantized to k-bits for k  {1, 2, 3}. The quantization is performed in a row-wise fashion, so that each row of the matrix has its own codebook {1, . . . , k}.

Method We compare our multi-bit PROXQUANT (eq. (11)) to the state-of-the-art alternating minimization algorithm with straight-through gradients (Xu et al., 2018). Training is initialized at a pre-trained full-precision LSTM. We use the SGD optimizer with initial learning rate 20.0 and decay by a factor of 1.2 when the validation error does not improve over an epoch. We train for 80 epochs with batch size 20, BPTT 30, dropout with probability 0.5, and clip the gradient norms to 0.25. The regularization rate  is tuned by finding the best performance on the validation set. In addition to multi-bit quantization, we also report the results for binary LSTMs (weights in {±1}), comparing BinaryConnect and our PROXQUANT-Binary.

Result We report the perplexity-per-word (PPW, lower is better) in Table 2. The performance of PROXQUANT is comparable with the Straight-through gradient method. On Binary LSTMs, PROXQUANT-Binary beats BinaryConnect by a large margin. These results demonstrate that PROXQUANT offers a powerful alternative for training recurrent networks.

Table 2: PPW of quantized LSTM on Penn Treebank.

Method / Number of Bits 1 2 3 FP (32)

BinaryConnect

419.1 - -

PROXQUANT-Binary (ours) 321.8 -

-

ALT Straight-through3 104.7 90.2 86.1

88.5

ALT-PROXQUANT (ours) 106.2 90.0 87.2

5 STABILITY ANALYSIS OF BINARY QUANTIZATION

5.1 CONVERGENCE CHARACTERIZATION FOR BINARYCONNECT

We now show that BinaryConnect has a very stringent convergence condition. Consider the BinaryConnect method with batch gradients:

st = sign(t), t+1 = t - tL(st).

(12)

Definition 5.1 (Fixed point and convergence). We say that s  {±1}d is a fixed point of the Bina-

ryConnect algorithm, if s0 = s in eq. (12) implies that st = s for all t = 1, 2, .... We say that the BinaryConnect algorithm converges if there exists t <  such that st is a fixed point.

Theorem 5.1. Assume that the learning rates satisfy

 t=0

t

=

,

then

s



{±1}d

is

a

fixed

point for BinaryConnect eq. (12) if and only if sign(L(s)[i]) = -s[i] for all i  [d] such that

L()[i] = 0. Such a point may not exist, in which case BinaryConnect does not converge for any

initialization 0  Rd.

We have already seen that such a fixed point s might not exist in the toy example in Figure 1b. In the following sign change experiment on CIFAR-10, we are going to see that BinaryConnect indeed fails to converge to a fixed sign pattern, corroborating Theorem 5.1.

3We thank Xu et al. (2018) for sharing the implementation of this method through a personal communi-
cation. There is a very clever trick not mentioned in their paper: after computing the alternating quantization qalt(), they multiply by a constant 0.3 before taking the gradient; in other words, their quantizer is a rescaled alternating quantizer:   0.3qalt(). This scaling step gives a significant gain in performance ­ without scaling the PPW is {116.7, 94.3, 87.3} for {1, 2, 3} bits. In contrast, our PROXQUANT does not involve a scaling
step and achieves better PPW than this unscaled ALT straight-through method.

8

Under review as a conference paper at ICLR 2019

5.2 SIGN CHANGE EXPERIMENT

We experimentally compare the training dynamics of PROXQUANT-Binary and BinaryConnect through the sign change metric. The sign change metric between any 1 and 2 is the proportion of their different signs, i.e. the (rescaled) Hamming distance:

SignChange(1, 2) =

sign(1) - sign(2) 1  [0, 1]. 2d

In Rd, the space of all full-precision parameters, the sign change is a natural distance metric that represents the closeness of the binarization of two parameters.

Recall in our CIFAR-10 experiments (Section 4.1), for both BinaryConnect and PROXQUANT, we initialize at a good full-precision net 0 and stop at a converged binary network   {±1}d. We
are interested in SignChange(0, t) along the training path, as well as SignChange(0, ), i.e. the distance of the final output model to the initialization.

As PROXQUANT converges to higher-performance solutions than BinaryConnect, we expect that if we run both methods from a same warm start, the sign change of PROXQUANT should be higher than that of BinaryConnect, as in general one needs to travel farther to find a better net.

0.25 sign_change_conv1.weight

0.20

0.15

0.10

0.05 0

ProxQuant BinaryConnect 100 200 300 400 500 600

(a)

sign_change_layer3.2.conv2.weight
0.450

0.425

0.400

0.375

0.350

0.325

0.300

0.275 0.250
0

ProxQuant BinaryConnect 100 200 300 400 500 600

(b)

0.16 0.14 0.12 0.10 0.08 0.06 0.04 0.02
0

sign_change_fc.weight
ProxQuant BinaryConnect 100 200 300 400 500 600

(c)

16 15 14 13 12 11 10 9
350

val_error1_bin
ProxQuant BinaryConnect
400 450 500 550 600

(d)

Figure 3: SignChange(0, t) against t (epoch) for BinaryConnect and PROXQUANT, over 4 runs starting from the same full-precision ResNet-20. PROXQUANT has significantly lower sign changes than BinaryConnect while converging to better models. (a) The first conv layer of size 16 × 3 × 3 × 3; (b) The last conv layer of size 64 × 64 × 3 × 3; (c) The fully connected layer of size 64 × 10; (d) The validation top-1 error of the
binarized nets (with moving average smoothing).

However, we find that this is not the case: PROXQUANT produces binary nets with both lower sign changes and higher performances, compared with BinaryConnect. This finding is consistent in all layers, across different warm starts, and across differnent runs from each same warm start (see Figure 3 and Table 3 in Appendix B). This shows that for every warm start position, there is a good binary net nearby which can be found by PROXQUANT but not BinaryConnect, suggesting that BinaryConnect, and in general the straight-through gradient method, suffers from higher optimization instability than PROXQUANT. This result here is also consistent with Theorem 5.1: the signs in BinaryConnect never stop changing until we manually freeze the signs at epoch 400.

6 CONCLUSION
In this paper, we propose and experiment with the PROXQUANT method for training quantized networks. Our results demonstrate that PROXQUANT offers a powerful alternative to the straightthrough gradient method and suffers from less optimization instability. For future work, it would be of interest to propose alternative regularizers for ternary and multi-bit PROXQUANT and experiment with our method on larger tasks.

REFERENCES
Alexander G Anderson and Cory P Berg. The high-dimensional geometry of binary neural networks. arXiv preprint arXiv:1705.07199, 2017.

9

Under review as a conference paper at ICLR 2019
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Miguel A Carreira-Perpina´n. Model compression as constrained optimization, with application to neural nets. part i: General framework. arXiv preprint arXiv:1707.01209, 2017.
Miguel A Carreira-Perpina´n and Yerlan Idelbayev. Model compression as constrained optimization, with application to neural nets. part ii: Quantization. arXiv preprint arXiv:1707.04319, 2017.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. BinaryConnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123­3131, 2015.
Yukun Ding, Jinglan Liu, and Yiyu Shi. On the universal approximability of quantized relu neural networks. arXiv preprint arXiv:1802.03646, 2018.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J Dally. EIE: Efficient inference engine on compressed deep neural network. In Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International Symposium on, pp. 243­254. IEEE, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. Journal of Machine Learning Research, 18:187­1, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Fengfu Li and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016.
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems, pp. 5811­5821, 2017.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313­330, 1993.
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends R in Optimization, 1 (3):127­239, 2014.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pp. 525­542. Springer, 2016.
Ju Sun and Xiaoxia Sun. Adversarial probabilistic regularization. Unpublished draft, 2018.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pp. 267­288, 1996.
Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11(Oct):2543­2596, 2010.
10

Under review as a conference paper at ICLR 2019

Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= S19dR9x0b.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv preprint arXiv:1612.01064, 2016.

A ADDITIONAL RESULTS ON REGULARIZATION

A.1 PROX OPERATORS FOR BINARY NETS

Here we derive the prox operators for the binary regularizer eq. (7) and its squared L2 variant. Recall that
d

Rbin() = min {|j - 1|, |j + 1|}.

j=1

By definition of the prox operator, we have for any   Rd that



1 2 d



proxRbin ()

=

arg min
Rd

2

 -  +  min
2 j=1

|j - 1|, |j + 1| 



=

arg min
Rd

d 
j=1

1 2 (j

-

j )2

+



min

|j - 1|, |j + 1|

 .

This minimization problem is coordinate-wise separable. For each j, the penalty term remains the
same upon flipping the sign, but the quadratic term is smaller when sign(j) = sign(j). Hence, the solution  to the prox satisfies that sign(j ) = sign(j), and the absolute value satisfies

|j | = arg min
t0

1 2

(t

-

|j

|)2

+

|t

-

1|

= SoftThreshold(|j|, 1, ) = 1+sign(|j|-1)[||j|-1|-]+.

Multiplying by sign(j ) = sign(j), we have

which gives eq. (8).

j = SoftThreshold(j, sign(j), ),

For the squared L2 version, by a similar argument, the corresponding regularizer is
d
Rbin() = min (j - 1)2, (j + 1)2 .
j=1

For this regularizer we have



proxRbin ()

=

arg min
Rd

d 
j=1

1 2 (j

-

j )2

+



min

(j - 1)2, (j + 1)2

  .

Using the same argument as in the L1 case, the solution  satisfies sign(j ) = sign(j), and

|j | = arg min
t0

1 2

(t

-

|j

|)2

+

(t

-

1)2

=

|j |

+

 .

1+

Multiplying by sign(j ) = sign(j) gives

j

=

j

+  sign(j) , 1+

or, in vector form,  = ( +  sign())/(1 + ).

11

Under review as a conference paper at ICLR 2019

A.2 PROX OPERATOR FOR TERNARY QUANTIZATION

For ternary quantization, we use an approximate version of the alternating prox operator eq. (11): compute  = proxR() by initializing at  =  and repeating

 + 2

 = q() and  =

,

1 + 2

where q is the ternary quantizer defined as

(13)

q() = +1{  } + -1{  -},

0.7 =
d

 1,

+ = |i:i,

- = |i:i-. (14)

This is a straightforward extension of the TWN quantizer (Li & Liu, 2016) that allows different levels for positives and negatives. We find that two rounds of alternating computation in eq. (13) achieves a good performance, which we use in our experiments.

12

Under review as a conference paper at ICLR 2019

B DETAILED SIGN CHANGE RESULTS ON RESNET-20

Table 3: Performances and sign changes on ResNet-20 in mean(std) over 3 full-precision initializations and 4 runs per (initialization x method). Sign changes are computed over all quantized parameters in the net.

Initialization FP-Net 1 (8.06) FP-Net 2 (8.31) FP-Net 3 (7.73)

Method BC PQ-B BC PQ-B BC PQ-B

Top-1 Error(%) 9.489 (0.223) 9.146 (0.212) 9.745 (0.422) 9.444 (0.067) 9.383 (0.211) 9.084 (0.241)

Sign change 0.383 (0.006) 0.276 (0.020) 0.381 (0.004) 0.288 (0.002) 0.359 (0.001) 0.275 (0.001)

Table 4: Performances and sign changes on ResNet-20 in raw data over 3 full-precision initializations and 4 runs per (initialization x method). Sign changes are computed over all quantized parameters in the net.

Initialization FP-Net 1 (8.06) FP-Net 2 (8.31) FP-Net 3 (7.73)

Method BC PQ-B BC PQ-B BC PQ-B

Top-1 Error(%) 9.664, 9.430, 9.198, 9.663 9.058, 8.901, 9.388, 9.237 9.456, 9.530, 9.623, 10.370 9.522, 9.474, 9.410, 9.370 9.107, 9.558, 9.538, 9.328 9.284, 8.866, 9.301, 8.884

Sign change 0.386, 0.377, 0.390, 0.381 0.288, 0.247, 0.284, 0.285 0.376, 0.379, 0.382, 0.386 0.291, 0.287, 0.289, 0.287 0.360, 0.357, 0.359, 0.360 0.275, 0.276, 0.276, 0.275

C PROOF OF THEOREM 5.1

We start with the "" direction. If s is a fixed point, then by definition there exists 0  Rd such that t =  for all t = 0, 1, 2, .... By the iterates eq. (12)

T
T = 0 - tL(st).
t=0

Take signs on both sides and apply st = s for all t on both sides, we get that

T
s = sT = sign(T ) = sign 0 - L(s) t
t=0

Take the limit T   and apply the assumption that t t = , we get that for all i  [d] such that [L()]i = 0,

T

s[i] = lim sign
T 

0 - L(s)

t [i] = - sign(L(s))[i].

t=0

Now we prove the "" direction. If  obeys that sign(L(s)[i]) = -s[i] for all i  [d] such
that L(s)[i] = 0, then if we take any 0 such that sign(0) = s, t will move in a straight line towards the direction of -L(s), which does not change the sign of h0. In other words, st = sign(t) = sign(0) = s for all t = 0, 1, 2, .... Therefore, by definition, s is a fixed point.

13

