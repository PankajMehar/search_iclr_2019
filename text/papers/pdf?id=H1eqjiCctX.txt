Under review as a conference paper at ICLR 2019
UNDERSTANDING COMPOSITION OF WORD EMBEDDINGS VIA TENSOR DECOMPOSITION
Anonymous authors Paper under double-blind review
ABSTRACT
Word embedding is a powerful tool in natural language processing. In this paper we consider the problem of word embedding composition ­ given vector representations of two words, compute a vector for the entire phrase. We give a generative model that can capture specific syntactic relations between words. Under our model, we prove that the correlations between three words (measured by their PMI) form a tensor that has an approximate low rank Tucker decomposition. The result of the Tucker decomposition gives the word embeddings as well as a core tensor, which can be used to produce better compositions of the word embeddings. We also complement our theoretical results with experiments that verify our assumptions, and demonstrate the effectiveness of the new composition method.
1 INTRODUCTION
Word embeddings have become one of the most popular techniques in natural language processing. A word embedding maps each word in the vocabulary to a low dimensional vector. Several algorithms (e.g., Mikolov et al. (2013); Pennington et al. (2014)) can produce word embedding vectors whose distances or inner-products capture semantic relationships between words. The vector representations are useful for solving many NLP tasks, such as analogy tasks(Mikolov et al., 2013) or serving as features for supervised learning problems (Maas et al., 2011).
While word embeddings are good at capturing the semantic information of a single word, a key challenge is the problem of composition: how to combine the embeddings of two co-occurring, syntactically related words to an embedding of the entire phrase. In practice composition is often done by simply adding the embeddings of the two words, but this may not be appropriate when the combined meaning of the two words differ significantly from the meaning of individual words (e.g., "complex number" should not just be "complex"+"number").
In this paper, we try to learn a model for word embeddings that incorporates syntactic information and naturally leads to better compositions for syntactically related word pairs. Our model is motivated by the principled approach for understanding word embeddings initiated by Arora et al. (2015), and models for composition similar to Coecke et al. (2010).
Arora et al. (2015) gave a generative model (RAND-WALK) for word embeddings, and showed several previous algorithms can be interpreted as finding the hidden parameters of this model. However, the RAND-WALK model does not capture any syntactic information. We give a generative model called syntactic RAND-WALK (see Section 3) that is capable of capturing specific syntactic relations (e.g., adjective-noun pairs). Taking adjective-noun pairs as an example, previous works (Socher et al., 2012; Baroni & Zamparelli, 2010; Maillard & Clark, 2015) have tried to model the adjective as a linear operator (a matrix) that can act on the embedding of the noun. However, this would require learning a d × d matrix for each adjective while the normal embedding only has dimension d. In our model, we use a core tensor T  Rd×d×d to capture the relations between a pair of words and its context. In particular, using the tensor T and the word embedding for the adjective, it is possible to define a matrix for the adjective that can be used as an operator on the embedding of the noun. Therefore our model allows the same interpretations as many previous models while having much fewer parameters to train.
One salient feature of our model is that it makes good use of high order statistics. Standard word embeddings are based on the observation that the semantic information of a word can be captured by
1

Under review as a conference paper at ICLR 2019
words that appear close to it. Hence most algorithms use pairwise co-occurrence between words to learn the embeddings. However, for the composition problem, the phrase of interest already has two words, so it would be natural to consider co-occurrences between at least three words (the two words in the phrase and their neighbors).
Based on the model, we can prove an elegant relationship between high order co-occurrences of words and the model parameters. In particular, we show that if we measure the Pointwise Mutual Information (PMI) between three words, and form an n × n × n tensor that is indexed by three words a, b, w, then the tensor has a Tucker decomposition that exactly matches our core tensor T and the word embeddings (see Section 2, Theorem 1, and Corollary 1). This suggests a natural way of learning our model using a tensor decomposition algorithm.
Our model also allows us to approach the composition problem with more theoretical insights. Based on our model, if words a, b have the particular syntactic relationships we are modeling, their composition will be a vector va + vb + T (va, vb, ·). Here va, vb are the embeddings for word a and b, and the tensor gives an additional correction term. By choosing different core tensors it is possible to recover many previous composition methods. We discuss this further in Section 3.
Finally, we train our new model on a large corpus and give experimental evaluations. In the experiments, we show that the model learned satisfies the new assumptions that we need. We also give both qualitative and quantitative results for the new embeddings. Our embeddings and the novel composition method can capture the specific meaning of adjective-noun phrases in a way that is impossible by simply "adding" the meaning of the individual words. Quantitative experiment also shows that our composition vector are better correlated with humans on a phrase similarity task.
1.1 RELATED WORK
Syntax and word embeddings Many well-known word embedding methods (e.g., Pennington et al. (2014); Mikolov et al. (2013)) don't explicitly utilize or model syntactic structure within text. Andreas & Klein (2014) find that such syntax-blind word embeddings fail to capture syntactic information above and beyond what a statistical parser can obtain, suggesting that more work is required to build syntax into word embeddings.
Several syntax-aware embedding algorithms have been proposed to address this. Levy & Goldberg (2014a) propose a syntax-oriented variant of the well-known skip-gram algorithm of Mikolov et al. (2013), using contexts generated from syntactic dependency-based contexts obtained with a parser. Cheng & Kartsaklis (2015) build syntax-awareness into a neural network model for word embeddings by indroducing a negative set of samples in which the order of the context words is shuffled, in hopes that the syntactic elements which are sensitive to word order will be captured.
Word embedding composition Several works have addressed the problem of composition for word embeddings. Coecke et al. (2010) present a mathematical framework for reasoning about word embedding composition that motivated our syntactic RAND-WALK model. Our new contribution is a concrete and practical learning algorithm with theoretical guarantees. Mitchell & Lapata (2008; 2010) explore various composition methods that involve both additive and multiplicative interactions between the component embeddings, but some of these are limited by the need to learn additional parameters post-hoc in a supervised fashion.
Guevara (2010) get around this drawback by first training word embeddings for each word and also for tokenized adjective-noun pairs. Then, the composition model is trained by using the constituent adjective and noun embeddings as input and the adjective-noun token embedding as the predictive target. Maillard & Clark (2015) treat adjectives as matrices and nouns as vectors, so that the composition of an adjective and noun is just matrix-vector multiplication. The matrices and vectors are learned through an extension of the skip-gram model with negative sampling. In contrast to these approaches, our model gives rise to a syntax-aware composition function, which can be learned along with the word embeddings in an unsupervised fashion, and which generalizes many previous composition methods (see Section 3.3 for more discussion).
Tensor factorization for word embeddings As Levy & Goldberg (2014b) and Li et al. (2015) point out, some popular word embedding methods are closely connected matrix factorization problems involving pointwise mutual information (PMI) and word-word co-occurrences. It is natural to consider
2

Under review as a conference paper at ICLR 2019

generalizing this basic approach to tensor decomposition. Sharan & Valiant (2017) demonstrate this technique by performing a CP decomposition on triple word co-occurrence counts. Bailey & Aeron (2017) explore this idea further by defining a third-order generalization of PMI, and then performing a symmetric CP decomposition on the resulting tensor. In contrast to these recent works, our approach arives naturally at the more general Tucker decomposition due to the syntactic structure in our model. Our model also suggests a different (yet still common) definition of third-order PMI.

2 PRELIMINARIES

Notation For a vector v, we use v to denote its Euclidean norm. For vectors u, v we use u, v to denote their inner-product. For a matrix M , we use M to denote its spectral norm,

M F=

i,j Mi2,j to denote its Frobenius norm, and Mi,: to denote it's i-th row. In this paper,

we will also often deal with 3rd order tensors, which are just three-way indexed arrays. We use  to

denote the tensor product: if u, v, w  Rd are d-dimensional vectors, T = u  v  w is a d × d × d

tensor whose entries are Ti,j,k = uivjwk.

Tensor basics Just as matrices are often viewed as bilinear functions, third order tensors can be
interpreted as trilinear functions over three vectors. Concretely, let T be a d × d × d tensor, and let x, y, z  Rd. We define the scalar T (x, y, z)  R as follows

d

T (x, y, z) =

Ti,j,k x(i)y (j )z (k).

i,j,k=1

This operation is linear in x, y and z. Analogous to applying a matrix M to a vector v (with the result vector M v), we can also apply a tensor T to one or two vectors, resulting in a matrix and a vector, respectively:

dd

T (x, y, ·)(k) =

Ti,j,kx(i)y(j), T (x, ·, ·)j,k = Ti,j,kx(i)

i,j=1

i=1

We will make use of the simple facts that z, T (x, y, ·) = T (x, y, z) and [T (x, ·, ·)] y = T (x, y, ·).

Tensor decompositions Unlike matrices, there are several different definitions for the rank of a tensor. In this paper we mostly use the notion of Tucker rank Tucker (1966). A tensor T  Rn×n×n has Tucker rank d, if there exists a core tensor S  Rd×d×d and matrices A, B, C  Rn×d such that

d

Ti,j,k =

Si ,j ,k Ai,i Bj,j Ck,k = S(Ai,:, Bj,:, Ck,:),

i ,j ,k =1

The equation above is also called a Tucker decomposition of the tensor T . The Tucker decomposition for a tensor can be computed efficiently.

When the core tensor S is restricted to a diagonal tensor (only nonzero at entries Si,i,i), the decompo-

sition is called a CP decomposition Carroll & Chang (1970); Harshman (1970) which can also be

written as T =

d i=1

Si,i,iAi,:



Bi,:



Ci,:.

In

this

case,

the

tensor

T

is

the

sum

of

d

rank-1

tensors

(Ai,:  Bi,:  Ci,:). However, unlike matrix factorizations and the Tucker decomposition, the CP

decomposition of a tensor is hard to compute in the general case (Håstad, 1990; Hillar & Lim, 2013).

Later in Section 4 we will also see why our model for syntactic word embeddings naturally leads to a

Tucker decomposition.

3 SYNTACTIC RAND-WALK MODEL
In this section, we introduce our syntactic RAND-WALK model and present formulas for inference in the model. We also derive a novel composition technique that emerges from the model.

3

Under review as a conference paper at ICLR 2019

(a) RAND-WALK

(b) Syntactic RAND-WALK

Figure 1: Graphical models of RAND-WALK (left) and our new model (right), depicting a syntactic word pair (wt, wt). Green nodes correspond to observed variables, white nodes to latent variables.

RAND-WALK model We first briefly review the RAND-WALK model (Arora et al., 2015). In this
model, a corpus of text is considered as a sequence of random variables w1, w2, w3, . . ., where wt takes values in a vocabulary V of n words. Each word w  V has a word embedding vw  Rd. The prior for the word embeddings is vw = s · v^, where s is a positive bounded scalar random variable with constant expectation  and upper bound , and v^  N (0, I).

The distribution of each wt is determined in part by a random walk {ct  Rd | t = 1, 2, 3 . . .}, where
ct ­ called a discourse vector ­ represents the topic of the text at position t. This random walk is slow-moving in the sense that ct+1 - ct is small, but mixes quickly to a stationary distribution that is uniform on the unit sphere, which we denote by C.

Let C denote the sequence of discourse vectors, and let V denote the set of word embeddings. Given these latent variables, the model specifies the following conditional probability distribution:

Pr[wt = w |ct]  exp( vw, ct ).

(1)

The graphical model depiction of RAND-WALK is shown in Figure 1a.

3.1 SYNTACTIC RAND-WALK

One limitation of RAND-WALK is that it can't deal with syntactic relationships between words. Observe that conditioned on ct and V , wt is independent of the other words in the text. However, in natural language, words can exhibit more complex dependencies, e.g. adjective-noun pairs,
subject-verb-object triples, and other syntactic or grammatical structures.

In our syntactic RAND-WALK model, we start to address this issue by introducing direct pairwise word dependencies in the model. When there is a direct dependence between two words, we call the two words a syntactic word pair. In RAND-WALK, the interaction between a word embedding v and a discourse vector c is mediated by their inner product v, c . When modeling a syntactic word pair, we need to mediate the interaction between three quantities, namely a discourse vector c and the word embeddings v and v of the two relevant words. A natural generalization is to use a trilinear form defined by a tensor T , i.e.

d

T (v, v , c) =

Ti,j,kv(i)v (j)c(k).

i,j,k=1

Here, T  Rd×d×d is also a latent random variable, which we call the composition tensor.

We model a syntactic word pair as a single semantic unit within the text (e.g. in the case of adjective-

noun phrases). We realize this choice by allowing each discourse vector ct to generate a pair of words
wt, wt with some small probability psyn. To generate a syntactic word pair wt, wt, we first generate a root word wt conditioned on ct with probability proportional to exp( ct, wt ), and then we draw
wt from a conditional distribution defined as follows:

Pr[wt = b | wt = a, C , V ]  exp( ct, vb + T (va, vb, ct)).

(2)

4

Under review as a conference paper at ICLR 2019

Here exp( ct, vb ) would be proportional to the probability of generating word b in the original RAND-WALK model, without considering the syntactic relationship. The additional term T (va, vb, ct) can be viewed as an adjustment based on the syntactic relationship.
We call this extended model Syntactic RAND-WALK. Figure 1b gives the graphical model depiction for a syntactic word pair, and we summarize the model below.
Definition 1 (Syntactic RAND-WALK model). The model consists of the following:

1. Each word w in vocabulary has a corresponding embedding vw  s · v^w, where s  R0 is bounded by  and E[s] =  ; v^w  N (0, Id×d).

2. The sequence of discoursevectors c1, ..., ct are generated by a random walk on the unit sphere, ct - ct+1  w/ d and the stationary distribution is uniform.

3. For each ct, with probability 1-psyn, it generates one word wt with probability proportional to exp( ct, vwt ).

4. For each ct, with probability psyn, it generates a syntactic pair wt, wt with probability

proportional to exp( ct, vwt ) and is a d × d × d composition tensor.

exp(

ct, vwt

+ T (vwt , vwt , ct)) respectively, where T

3.2 INFERENCE IN THE MODEL

We now calculate the marginal probabilities of observing pairs and triples of words under the syntactic RAND-WALK model. We will show that these marginal probabilities are closely related to the model parameters (word embeddings and the composition tensor). All proofs in this section are deferred to supplementary material.

Throughout this section, we consider two adjacent context vectors ct and ct+1, and condition on the event that ct generated a single word and ct+1 generated a syntactic pair1. The main bottleneck in computing the marginal probabilities is that the conditional probailities specified in equations
(1) and (2) are not normalized. Indeed, for these equations to be exact, we would need to divide by the appropriate partition functions, namely Zct := wV exp( vw, ct ) for the former and Zct,a := wV exp( ct, vw + T (va, vw, ct)) for the latter. Fortunately, we show that under mild assumptions these quantities are highly concentrated. To do that we need to control the norm of the
composition tensor.

Definition 2. The composition tensor T is (K, )-bounded, if for any word embedding va, vb, we have

T (va, ·, ·) + I

2

Kd 2 log2 n ;

T (va, ·, ·) + I

2 F

 Kd;

T (va, vb, ·) 2  Kd.

To make sure exp( ct, vw +T (va, vw, ct)) are within reasonable ranges, the value K in this definition should be interpreted as an absolute constant (like 5, similar to previous constants  and  ). Intuitively these conditions make sure that the effect of the tensor cannot be too large, while still making sure the tensor component T (va, vb, c) can be comparable (or even larger than) vb, c . We have not tried to optimize the log factors in the constraint for T (va, ·, ·) + I 2.

Note that if the tensor component T (va, ·, ·) has constant singular values (hence comparable to I), we know these conditions will be satisfied with K = O(1) and = O( log n ). Later in Section 5
d
we verify that the tensors we learned indeed satisfy this condition. Now we are ready to state the
concentration of partition functions:

Lemma 1 (Concentration of partition functions). For the syntactic RAND-WALK model, there exists a constant Z such that

Pr [(1 -
cC

z)Z  Zc  (1 +

z)Z]  1 - ,

for z = O~(1/n) and  = exp(-(log2 n)).

1As we will see in Section 5, in practice it is easy to identify which words form a syntactic pair, so it is possible to condition on this event in training.

5

Under review as a conference paper at ICLR 2019

Furthermore, if the tensor T is (K, )-bounded, then for any fixed word a  V , there exists a constant Za such that
Pr [(1 - z,a)Za  Zc,a  (1 + z,a)Za]  1 - ,
cC
for z,a = O( ) + O~(1/n) and  = exp(-(log2 n)).
Using this lemma, we can obtain simple expressions for co-occurrence probabilities. In particular, for any fixed w, a, b  V , we adopt the following notation:
p(a) := Pr[wt+1 = a] p(w, a) := Pr[wt = w, wt+1 = a] p([a, b]) := Pr[wt+1 = a, wt+1 = b] p(w, [a, b]) := Pr[wt = w, wt+1 = a, wt+1 = b].

Here in particular we use [a, b] to highlight the fact that a and b form a syntactic pair. Note p(w, a) is the same as the co-occurrence probability of words w and a if both of them are the only word generated by the discourse vector. Later we will also use p(w, b) to denote Pr[wt = w, wt+1 = b] (not Pr[wt = w, wt+1 = b]).
We also require two additional properties of the word embeddings, namely that they are norm-bounded above by some constant times d, and that all partition functions are bounded below by a positive constant. Both of these properties hold with high probability over the word embeddings provided n d log d and d log n, as shown in the following lemma:
Lemma 2. Assume that the composition tensor T is (K, )-bounded, where K is a constant. With probability at least 1 - 1 - 2 over the word vectors, where 1 = exp((d log d) - (n)) and 2 = exp((log n) - (d)), there exist positive absolute constants  and  such that vi   for each i  V and Zc   and Zc,a   for any unit vector c  Rd and any word a  V .

We can now state the main result.

Theorem 1. Suppose that the events referred to in Lemma 1 hold. Then

log p(a) =

va 2 - log Z ± 2d

p

(3)

log p(w, a) =

vw + va 2d

2
- 2 log Z ±

p

(4)

log p([a, b]) =

va + vb + T (va, vb, ·) 2d

2
- log Z - log Za ±

p

(5)

log p(w, [a, b]) =

vw + va + vb + T (va, vb, ·) 2d

2
- 2 log Z - log Za ±

p

(6)

Here p = O( + w) + O~(1/n + 1/d), where is from the (K, )-boundedness of T and w is

from Definition 1.

3.3 COMPOSITION

Our model suggests that the latent discourse vectors contain the meaning of the text at each location. It is therefore reasonable to view the discourse vector c corresponding to a syntactic word pair (a, b) as a suitable representation for the phrase as a whole. The posterior distribution of c given (a, b) satisfies
1 Pr[ct = c | wt = a, wt = b]  ZcZc,a exp ( va + vb + T (va, vb, ·), c ) Pr[ct = c].

Since Pr[ct = c] is constant, and since Zc and Zc,a concentrate on values that don't depend on c, the MAP estimate of c given [a, b], which we denote by c^, satisfies

c^  arg max exp ( va + vb + T (va, vb, ·), c ) =
c =1

va + vb + T (va, vb, ·) va + vb + T (va, vb, ·)

.

Hence, we arrive at our basic tensor composition: for a syntactic word pair (a, b), the composite embedding for the phrase is va + vb + T (va, vb, ·).

6

Under review as a conference paper at ICLR 2019

Note that our composition involves the traditional additive composition va + vb, plus a correction term T (va, vb, ·). We can view T (va, vb, ·) as a matrix-vector multiplication [T (va, ·, ·)] vb, i.e. the composition tensor allows us to compactly associate a matrix with each word in the same vein as Maillard & Clark (2015). Depending on the actual value of T , the term T (va, vb, ·) can also recover any manner of linear or multiplicative interactions between va and vb, such as those proposed in Mitchell & Lapata (2010).

4 LEARNING

In this section we discuss how to learn the parameters of the syntactic RAND-WALK model. Theorem 1 provides key insights into the learning problem, since it relates joint probabilities between words (which can be estimated via co-occurrence counts) to the word embeddings and composition tensor. By examining these equations, we can derive a particularly simple formula that captures these relationships. To state this equation, we define the PMI for 3 words as

p(w, [a, b])p(a)p(b)p(w)

P M I3(a, b, w) := log

.

p(w, a)p(w, b)p([a, b])

(7)

We note that this is just one possible generalization of pointwise mutual information (PMI) to several random variables, but in the context of our model, it is a very natural definition as all the partition numbers will be canceled out. Indeed, as an immediate corollary of Theorem 1, we have
Corollary 1. Suppose that the events referred to in Lemma 1 hold. Then for p same as Theorem 1

P M I3(a, b, w)

=

1 d

T

(va

,

vb

,

vw

)

±

O(

p).

(8)

That is, if we consider P M I3(a, b, w) as a n × n × n tensor, Equation equation 8 is exactly a Tucker decomposition of this tensor of Tucker rank d. Therefore, all the parameters of the syntactic RAND-WALK model can be obtained by finding the Tucker decomposition of the PMI3 tensor. This equation also provides a theoretical motivation for using third-order pointwise mutual information in learning word embeddings.

4.1 IMPLEMENTATION We now discuss concrete details about our implementation of the learning algorithm.

Corpus. We train our model using a February 2018 dump of the English Wikipedia. The text is pre-processed to remove non-textual elements, stopwords, and rare words (words that appear less than 1000 within the corpus), resulting in a vocabulary of size 68,279. We generate a matrix of word-word co-occurrence counts using a window size of 5. To generate the tensor of adjective-noun-word co-occurrence counts, we first run the Stanford Dependency Parser (Chen & Manning, 2014) on the corpus in order to identify all adjective-noun word pairs, and then use context windows that don't cross sentence boundaries to populate the adjective-noun-word co-occurrence counts.

Training. We first train the word embeddings according to the RAND-WALK model, following Arora et al. (2015). Using the learned word embeddings, we next train the composition tensor T via the following optimization problem
min f (X(a,b),w) log(X(a,b),w) - vw + va + vb + T (va, vb, ·) 2 - Ca - C 2 ,
T,{Cw},C (a,b),w
where X(a,b),w denotes the number of co-occurrences of word w with the adjective-noun pair (a, b) (a denotes the noun) and f (x) = min(x, 100). This objective function isn't precisely targeting the Tucker decomposition of the PMI3 tensor, but it is analogous to the training criterion used in Arora et al. (2015), and can be viewed as a negative log-likelihood for the model. To reduce the number of parameters, we constrain T to have CP rank 1000. We also trained the embeddings and tensor jointly, but found that this approach yields very similar results. In all cases, we utilize the Tensorflow framework (Abadi et al., 2016) with the Adam optimizer (Kingma & Ba, 2014) (using default parameters), and train for 1-5 epochs.

7

Under review as a conference paper at ICLR 2019

density

4664 44
2222 0 0.5 1.0 1.5 2.0 0 0.5 1.0 1.5 2.0 0 0.5 1.0 1.5 2.0 0 0.5 1.0 1.5 2.0
Figure 2: Histograms of partition functions Zc,a (x-axis is Zc,a/E[Zc,a])

5 EXPERIMENTAL VERIFICATION

In this section, we verify and evaluate our model empirically on select qualitative and quantitative tasks. In all of our experiments, we focus solely on syntactic word pairs formed by adjective-noun phrases, where the noun is considered the root word.

5.1 MODEL VERIFICATION
Arora et al. (2015) empirically verify the model assumptions of RAND-WALK, and since we trained our embeddings in the same way, we don't repeat their verifications here. Instead, we verify two key properties of syntactic RAND-WALK.

Norm of composition tensor We check the assumptions that the tensor T is (K, )-bounded.

Ranging

over

all

adjective-noun

pairs

in

the

corpus,

we

find

that

1 d

T (va, ·, ·) + I

2 has mean 0.052

and

maximum

0.248,

1 d

T (va, ·, ·) + I

2 F

has

mean

1.61

and

maximum

3.23,

and

1 d

T (va, vb, ·)

2

has mean 0.016 and maximum 0.25. Each of these three quantities has a well-bounded mean, but

T (va, ·, ·) + I 2 has some larger outliers. If we ignore the log factors (which are likely due to

artifacts in the proof) in Definition 2, the tensor is (K, ) bounded for K = 4 and = 0.25.

Concentration of partition functions In addition to Definition 2, we also directly check its implications: our model predicts that the partition functions Zc,a concentrate around their means. To check this, given a noun a, we draw 1000 random vectors c from the unit sphere, and plot the histogram of Zc,a.Results for a few randomly selected words a are given in Figure 2. All partition functions that we inspected exhibited good concentration.

5.2 QUALITATIVE ANALYSIS OF COMPOSITION
We test the performance of our new composition for adjective-noun pairs by looking for the words with closest embedding to the composed vector. For an adjective-noun phrase (a, b), we compute c = va + vb + T (va, vb, ·), and then retrieve the words w whose embeddings vw have the largest cosine similarity to c. We compare our results to the additive composition method. Results for three adjective-noun phrases are listed in Table 1. In each case, the tensor composition is able to retrieve some words that are more specifically related to the adjective-noun pair. However, the tensor composition also sometimes retrieves words that seem completely unrelated to either word in the phrase. We conjecture that this might be due to the sparseness of co-occurrence of three words. We also observed cases where the tensor composition method was about on par with or inferior to the additive composition method for retrieving relevant words, particularly in the case of low-frequency adjective-noun phrases. More results can be found in supplementary material.

5.3 PHRASE SIMILARITY
We also test our tensor composition method on a adjective-noun phrase similarity task using the dataset introduced by Mitchell & Lapata (2010). The data consists of 108 pairs of adjective-noun phrases that have been given similarity ratings by a group of 54 humans. The task is to use the word embeddings to produce similarity scores that correlate well with the human scores; we use both the Spearman rank correlation and the Pearson correlation as evaluation metrics for this task. We note that the human similarity judgments are somewhat noisy; intersubject agreement for the task is 0.52 as reported in Mitchell & Lapata (2010).

8

Under review as a conference paper at ICLR 2019

Table 1: Top 10 words relating to various adjective-noun phrases

civil war

additive

tensor

complex numbers

additive

tensor

national park additive tensor

war civil military army conflict wars fought revolutionary forces outbreak

civil somalian eicher crimean laotian francoist ulysses liberian confederate midst

complex numbers number function complexes functions integers multiplication algebraic integer

complex eigenvalues numbers hermitian quaternions marginalia azadi rationals holomorphic rhythmically

national park parks recreation forest historic heritage wildlife memorial south

yosemite denali gunung kenai nps teton refuges tilden snowdonia jigme

Given an adjective a and noun n with embeddings va, vn, respectively, we found that the tensor composition va + vn + T (va, vn, ·) yields worse performance than the simple additive composition va + vn. For this reason, we consider a weighted tensor composition va + vn + T (va, vn, ·) with   0. Following Mitchell & Lapata (2010), we split the data into a development set of 18 humans and a test set of the remaining 36 humans. We use the development set to select the optimal scalar weight for the weighted tensor composition, and using this fixed parameter, we report the results using the test set. We repeat this three times, rotating over folds of 18 subjects, and report the average results. As a baseline, we also report the average results using just the additive composition, as well as a weighted additive composition va + vn, where   0. We select  using the development set ("weighted additive 1") and the test set ("weighted additive 2"). We allow weighted additive 2 to cheat in this way because it provides an upper bound on the best possible weighted additive composition; if the tensor composition outperforms this method, then we can conclude that it outperforms all additive methods.
We also perform this same experiment using two other standard sets of pre-computed word embeddings, namely GloVe2 and carefully optimized cbow vectors3 (Mikolov et al., 2017). We re-trained the composition tensor using the same corpus and technique as before, but substituting these precomputed embeddings in place of the RAND-WALK (rw) embeddings. However, a bit of care must be taken here, since our syntactic RAND-WALK model constrains the norm of the word embeddings to be related to the frequency of the words, whereas this is not the case with the pre-computed embeddings. To deal with this, we rescaled the pre-computed embeddings sets to have the same norms as their counterparts in the rw embeddings, and then trained the composition tensor using these rescaled embeddings. At test time, we use the original embeddings to compute the additive components of our compositions, but use the rescaled versions when computing the tensor components.
The results are given in Table 2. The tensor composition outperforms all other compositions on all embedding sets apart from the Spearman correlation on the cbow vectors, where the weighted additive 2 method has a slight edge. This suggests that the composition tensor captures additional information beyond the individual word embeddings that is useful for this task. There was high consistency across the folds for the optimal weight parameter , with  = 0.4 for the rw embeddings,  = .2, .3 for the glove embeddings, and  = .3 for the cbow embeddings.
REFERENCES
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In OSDI, volume 16, pp. 265­283, 2016.
Jacob Andreas and Dan Klein. How much do word embeddings encode about syntax? In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
2obtained from https://nlp.stanford.edu/projects/glove/ 3obtained from https://fasttext.cc/docs/en/english-vectors.html
9

Under review as a conference paper at ICLR 2019

Table 2: Correlation measures between human judgments and embedding-based similarity scores (Spearman, Pearson)

additive weighted additive 1 weighted additive 2 tensor

rw .446, .438 glove .357, .336 cbow .471, .452

.444, .448 .351, .334 .469, .451

.452, .453 .358, .345 .476, .456

.460, .465 .368, .347 .474, .471

Papers), volume 2, pp. 822­827, 2014.
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Rand-walk: A latent variable model approach to word embeddings. arXiv preprint arXiv:1502.03520, 2015.
Eric Bailey and Shuchin Aeron. Word embeddings via tensor factorization. arXiv preprint arXiv:1704.02686, 2017.
Marco Baroni and Roberto Zamparelli. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 1183­1193. Association for Computational Linguistics, 2010.
J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling via an n-way generalization of "eckart-young" decomposition. Psychometrika, 35(3):283­319, 1970.
Danqi Chen and Christopher Manning. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 740­750, 2014.
Jianpeng Cheng and Dimitri Kartsaklis. Syntax-aware multi-sense word embeddings for deep compositional models of meaning. arXiv preprint arXiv:1508.02354, 2015.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical foundations for a compositional distributional model of meaning. arXiv preprint arXiv:1003.4394, 2010.
Emiliano Guevara. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, pp. 33­37. Association for Computational Linguistics, 2010.
Richard A Harshman. Foundations of the parafac procedure: Models and conditions for an" explanatory" multimodal factor analysis. 1970.
Johan Håstad. Tensor rank is np-complete. Journal of Algorithms, 11(4):644­654, 1990.
Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM (JACM), 60(6):45, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pp. 1302­1338, 2000.
Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 302­308, 2014a.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, pp. 2177­2185, 2014b.
Yitan Li, Linli Xu, Fei Tian, Liang Jiang, Xiaowei Zhong, and Enhong Chen. Word embedding revisited: A new representation learning and explicit matrix factorization perspective. In IJCAI, pp. 3650­3656, 2015.
10

Under review as a conference paper at ICLR 2019
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142­150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/P11-1015.
Jean Maillard and Stephen Clark. Learning adjective meanings with a tensor-based skip-gram model. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning, pp. 327­331, 2015.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111­3119, 2013.
Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Advances in pre-training distributed word representations. arXiv preprint arXiv:1712.09405, 2017.
Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. proceedings of ACL-08: HLT, pp. 236­244, 2008.
Jeff Mitchell and Mirella Lapata. Composition in distributional models of semantics. Cognitive science, 34(8):1388­1429, 2010.
Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In "Proceedings of the ACL", 2004.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825­2830, 2011.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pp. 1532­1543, 2014.
Vatsal Sharan and Gregory Valiant. Orthogonalized als: A theoretically principled tensor decomposition algorithm for practical use. arXiv preprint arXiv:1703.01804, 2017.
Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pp. 1201­ 1211. Association for Computational Linguistics, 2012.
Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3): 279­311, 1966.
11

Under review as a conference paper at ICLR 2019

Table 3: Top 10 words relating to various adjective-noun phrases

united states

additive tensor

united states us canada countries california usa america kingdom nations

united states emigrating emirates immigrated cartographic extradited senate lighthouses stateside

soviet union

additive tensor

union soviet ussr russian communist russia soviets moscow sfsr ukraine

union soviet sfsr disintegration lyudmila dismemberment brezhnev ussr perestroika zhukov

european union

additive

tensor

european union europe countries federation nations soviet organisations socialist eu

eec ebu dismemberment retort detracts arguable kely eea geosciences bugzilla

Table 4: Top 10 words relating to various adjective-noun phrases

expensive taste

additive

tensor

taste expensive cheaper flavor tastes unpleasant inexpensive smell costly ingredients

expensive taste costly prohibitively computationally cheaper luxurious sweeter inexpensive afford

awful taste

additive tensor

taste awful smell unpleasant flavor refreshing something things really odor

taste awful smell disagreeable fruity aroma fishy pungent odor becuase

refined taste

additive tensor

taste refined flavor tastes smell flavour aroma sour ingredients qualities

refined taste sweeter sensuous elegant disagreeable elegance neoclassicism refinement perfected

A ADDITIONAL QUALITIATIVE RESULTS
In this section we present additional qualitiative results demonstrating the use of the composition tensor for the retrieval of words related to adjective-noun phrases.
In Table 3, we show results for the phrases "United States", "Soviet Union", and "European Union". These phrases, which all occur with comparatively high frequency in the corpos, were identified as adjective-noun phrases by the tagger, but they function more as compound proper nouns. In each case, the additive composition retrieves reasonably relevant words, while the tensor composition is more of a mixed bag. In the case of "European Union", the tensor composition does retrieve the highly relevant words eec (European Economic Community) and eea (European Economic Area), which the additive composition misses, but the tensor composition also produces several false positives. It seems that for these types of phrases, the additive composition is sufficient to capture the meaning.
In Table 4, we fix the noun "taste" and vary the modifying adjective to highlight different senses of the noun. In the case of "expensive taste", both compositions retrieve words that seem to be either related to "expensive" or "taste", but there don't seem to be words that are intrinsically related to the phrase as a whole (with the exception, perhaps, of "luxurious", which the tensor composition retrieves). In the case of "awful taste", both compositions retrieve fairly similar words, which mostly relate to the physical sense of taste (rather than the more abstract sense of the word). For the phrase "refined taste", the additive composition fails to capture the sense of the phrase and retrieves many words related to food taste (which are irrelevant in this context), whereas the tensor composition retrieves more relevant words.
In Table 5, we fix the noun "friend" and vary the modifying adjective, but in all three cases, the adjective-noun phrase has basically the same meaning. In the case of "close friend" and "dear
12

Under review as a conference paper at ICLR 2019

Table 5: Top 10 words relating to various adjective-noun phrases

close friend

additive

tensor

close friend friends confidant colleague closest collaborator confidante classmate brother

confidante confidant coworker close friend confided schoolmates classmate protege cuz

best friend

additive tensor

best friend actor awards actress award nominated friends girlfriend writer

confidante confides misinterpreting coworker memoirists protege presumes helpfully matth regretfully

dear friend

additive tensor

friend dear colleague lover friends girlfriend beloved boyfriend classmate roommate

friend confidante coworker colleague dear confidant dearest protege confided collaborator

Table 6: Test accuracy for sentiment analysis task (standard deviation reported in parentheses)

Dataset

Additive

Tensor

Pang and Lee

0.741 (0.018) 0.759 (0.025)

Large Movie Review 0.793

0.794

friend", both compositions retrieve fairly relevant and similar words. In the case of "best friend", both compositions retrieve false positives: the additive composition seems to find words related to movie awards, while the tensor composition finds unintuitive false positives. We note that in all three phrases, the tensor composition consistently retrieves the words "confidante", "confided" or "confides", "coworker", and "protoge", all of which are fairly relevant.
A.1 SENTIMENT ANALYSIS
We test the effect of using the composition tensor for a sentiment analysis task. We use the movie review dataset of Pang and Lee (Pang & Lee, 2004) as well as the Large Movie Review dataset (Maas et al., 2011), which consist of 2,000 movie reviews and 50,000 movie reviews, respectively. For a fixed review, we identify each adjective-noun pair (a, b) and compute T (va, vb, ·). We add these compositions together with the word embeddings for all of the words in the review, and then normalize the resulting sum. This vector is used as the input to a regularized logistic regression classifier, which we train using scikit-learn (Pedregosa et al., 2011) with the default parameters. We also consider a baseline method where we simply add together all of the word embeddings in the movie review, and then normalize the sum. We evaluate the test accuracy of each method using 5-fold cross-validation on the smaller dataset and the training-test set split provided in the larger dataset. Results are shown in Table 6. Although the tensor method seems to have a slight edge over the baseline, the differences are not significant.
B OMITTED PROOFS FOR SECTION 3
In this section we will prove the main Theorem 1, which establishes the connection between the model parameters and the correlations of pairs/triples of words. As we explained in Section 3, a crucial step is to analyze the partition function of the model and show that the partition functions are concentrated. We will do that in Section B.1. We then prove the main theorem in Section B.2. More details and some technical lemmas are deferred to Section B.3
B.1 CONCENTRATION OF PARTITION FUNCTION
In this section we will prove concentrations of partition functions (Lemma 1). Recall that we need the tensor to be K-bounded (where K is a constant) for this to work.
13

Under review as a conference paper at ICLR 2019

Definition 3. (Definition 2 restated) The composition tensor T is (K, )-bounded, if for any word

embedding va, vb, we have

T (va, ·, ·) + I

2



Kd log2

2
; n

T (va, ·, ·) + I

2 F



K d;

T (va, vb, ·)

2  Kd.

Note that K here should be considered as an absolute constant (like 5, in fact in Section 5 we show K is less than 4). We first restate Lemma 1 here:

Lemma 3 (Lemma 1 restated). For the syntactic RAND-WALK model, there exists a constant Z such

that

for

z

=

O~(1/n)

and



Pr [(1 -
cC

z )Z

= exp(-(log2

 Zc n)).



(1

+

z)Z]  1 - ,

Furthermore, if the tensor T is (K, )-bounded, then for any fixed word a  V , there exists a constant

Za such that

for

z,a

=

O(

)

Pr [(1 - + O~(1/cnC ) and

z,a)Za  Zc,a  (1 +  = exp(-(log2 n)).

z,a)Za]  1 - ,

In fact, the first part of this Lemma is exactly Lemma 2.1 in Arora et al. (2015). Therefore we will focus on the proof of the second part.
For the second part, we know the probability of choosing a word b is proportional to exp(T (va, vb, c)+ c, vb ) = exp( T (va, ·, c) + c, vb ).
If the probability of choosing word w is proportional to exp( r, vw ) for some vector r (think of r = T (va, ·, c)+c), then in expectation the partition function should be equal to nEvDV [exp( r, v )] (here DV is the distribution of word embedding). When the number of words is large enough, we hope that with high probability the partition function is close to its expectation. Since the Gaussian distribution is spherical, we also know that the expected partition function nEvDV [exp( r, v )] should only depend on the norm of r. Therefore as long as we can prove the norm of r = T (va, ·, c)+c remain similar for most c, we will be able to prove the desired result in the lemma.
We will first show the norm of r = T (va, ·, c) + c is concentrated if the tensor T is (K, )-bounded. Throughout all subsequent proofs, we assume that < 1 and d  log2 n/ 2.
Lemma 4. Let va be a fixed word vector, and let c be a random discourse vector. If T is (K, )bounded with d  log2 n/ 2, we have
Pr[ T (va, ·, c) + c 2  L ± O( )]  1 - ,
where 0  L  K is a constant that depends on va, and  = exp(-(log2 n)).

Proof. Since c is a uniform random vector on the unit sphere, we can represent c as c = z/ z , where z  N (0, I) is a standard spherical Gaussian vector. For ease of notation, let M = T (va, ·, ·)+I, and write the singular value decomposition of M as M = U V T . Note that  = diag(1, . . . , d) and U and V are orthogonal matrices, so that in particular, the random variable y = V T z has the same distribution as z, i.e. its entries are i.i.d. standard normal random variables. Further, U x 2 = x 2
for any vector x, since U is orthogonal. Hence, we have

T (va, ·, c) + c 2 =

1 z2

Mz

2=

1 z2

U y

2=

d i=1

2i yi2

d i=1

zi2

.

Since both the numerator and denominator of this quantity are generalized 2 random variables,

we can apply Lemma 7 to get tail bounds on both. Observe that by assumption, we have i2 

Kd 2/ log2 n for all i, and

d i=1

2i



K d.

Set

A

=

d i=1

i2yi2

and

B

=

d i=1

zi2.

Let

m2 ax

=

1miaxd2i . Note that E[A] =

d i=1

2i



Kd

and

E[B]

=

d.

We will apply Lemma 7 to prove concentration bounds for A, in this case we have



Pr |A - E[A]|  2

d

 4i x

+

2m2 axx



2

exp(-x).

i=1

14

Under review as a conference paper at ICLR 2019

Under our assumptions, we know 2max  Kd 2/ log2 n and

d i=1

4i



2max

d i=1

i2



Kd

/ log n.

Take

x

=

1 16

log2 n,

we

know

2

d i=1

i4x

+

2m2 axx]



Kd

.

Therefore

Pr[|A - E[A]|  Kd ]  2 exp(-(log2 n)).

Similarly, we can apply Lemma 7 to B (in fact we can apply simpler concentration bounds for standard 2 distribution), and we get

 Pr[|B - E[B]|  2 d x + 2x]  2 exp(-x).

If

we

take

x

=

1 16

log2

n,

we

know

 2dx

+

2x



d. This implies

Pr[|B - E[B]|  d ]  2 exp(-(log2 n)).

When

both

events

happen

we

know

|

A B

-

E[A] E[B]

|



4K

= O( ) (here K is considered as a constant).

This finishes the proof.

Using this lemma, we will show that the expected condition number nEvDV [exp( r, v )] (where r = T (va, ·, c) + c) is concentrated Lemma 5. Let va be a fixed word vector, and let c be a random discourse vector. If T is (K, )bounded, there exists Za such that we have
Pr[nEvDV [exp( T (va, ·, c) + c, v )]  Za(1 ± O( )  1 - ,
where Za = (n) depends on va, and  = exp(-(log2 n)).
Proof. We know v = s · v^ where v^  N (0, I) and s is a (random) scaling. Let r = T (va, ·, c) + c. Conditioned on s we know r, v is equivalent to a Gaussian random variable with standard deviation  = r s. For this random variable we know

E[exp( r, v )|s] =

1 exp

x  2

= 1 exp x  2

= exp(2/2).

-

x2 22

exp(x)dx

- (x

- 2)2 22

+

2/2

dx

Hence,

E[exp( r, v )|s] = exp(s2 r 2/2).

Let g(x) = Es[exp(s2x/2)], we know g (x) = Es[exp(s2x/2) · (s2/2)]  2/2 · g(x). In particular, this implies g(x + )  exp(2/2)g(x) (for small ).
By Lemma 4, we know with probability at least 1 - (log2 n), r 2  L ± O( ). Therefore, when this holds, we have
nEvDV [exp( r, v )]  ng(L - O( )) · [1, exp(O( 2/2)].
The multiplicative factor on the RHS is bounded by 1 + O( ) when is small enough (and  is a constant). This finishes the proof.

Now we know the expected partition function is concentrated (for almost all discourse vectors c), it remains to show when we have finitely many words the partition function is concentrated around its expectation. This was already proved in Arora et al. (2015), we use their lemma below:

15

Under review as a conference paper at ICLR 2019

Lemma 6. For any fixed vector r (whose norm is bounded by a constant), with probability at least 1 - exp(-(log2 n)) over the choices of the words, we have

where z = O~(1/n).

n
exp( r, vi )  nEvDV [exp( r, v )](1 ± z),
i=1

This is essentially Lemma 2.1 in Arora et al. (2015) (see Equation A.32). The version we stated is a bit different because we allow r to have an arbitrary constant norm (while in their proof vector r is the discourse vector c and has norm 1). This is a trivial corollary as we can move the norm of r into the distribution of the scaling factor s for the word embedding.
Finally we are ready to prove Lemma 1.

Proof of Lemma 1. The first part is exactly Lemma 2.1 in Arora et al. (2015).

For the second part, note that the partition function Zc,a =

n i=1

T (va, ·, c) + c, vi

.

We will use

E[Zc,a] to denote its expectation over the randomness of the word embedding {vi}. By Lemma 5,

we know for at least 1 - exp(-(log2 n)) fraction of discourse vectors c, the expected partition

function is concentrated (E[Zc,a]  (1 ± O( ))Za). Let S denote the set of c such that Lemma 5

holds. Now by Lemma 6 we know for any x  S, with probability at least 1 - exp(-(log2 n)

Zc,a  (1 ± z)E[Zc,a].

Therefore we know if we consider both c and the embedding as random variables, Pr[Zc,a  (1 ± O( + z))Za]  1 -  where  = exp(-(log2 n)). Let S be the set of word embedding such that there is at least  fraction of c that does not satisfy Zc,a  (1 ± O( + z))Za, we must have Pr[S] ·    . Therefore
 Pr[S]   .

 That is, with probability at least 1 -  (over the word embeddings), there is at least 1 -  fraction of c such that Zc,a  (1 ± O( + z))Za.

B.2 ESTIMATING THE CORRELATIONS
In this section we prove Theorem 1 and Corollary 1. The proof is very similar to the proof of Theorem 2.2 in Arora et al. (2015). We use several lemmas in that proof, and these lemmas are deferred to Section B.3.

Proof of Theorem 1. Throughout this proof we consider two adjacent discourse vectors c, c , where c generated a single word w and c generated a syntactic pair (a, b).
The first two results in Theorem 1 are exactly the same as Theorem 2.2 in Arora et al. (2015). Therefore we only need to prove the result for p([a, b]) and p(w, [a, b]).
For p([a, b]), by definition of the model we know

11

p([a,

b])

=

Ec

[ Zc

Zc ,a exp( c , va

+

c , vb

+ T (va, vb, c )].

Here Zc is the partition function

n i=1

exp(

c

, vi

+ T (va, vi, c ).

n i=1

exp(

c

, vi

),

and

Zc ,a

is

the

partition

function

Let F be the event that c satisfies the equations in Lemma 1. Let F¯ be its negation. By Lemma 1 we know Pr[F]  1 - exp(-(log2 n)). Using this event, we can write

16

Under review as a conference paper at ICLR 2019

11

p([a,

b])

=Ec

[ Zc

Zc ,a exp( c , va

+

c , vb

+ T (va, vb, c ))1F ]

11

+

Ec

[ Zc

Zc ,a exp( c , va

+

c , vb

+ T (va, vb, c ))1F¯].

The second term can be bounded by Lemma 8 and the fact that Zc Zc ,a   from Lemma 2. We know

1 Ec [ Zc

1 Zc ,a exp( c , va

+

c , vb

+ T (va, vb, c )1F¯]  exp(-(log1.8 n)).

For the first term, we know by Lemma 1 that there exists Z, Za that are close to Zc and Zc ,a. Therefore

11 p([a, b]) = Ec [ Zc Zc ,a exp( c , va + c , vb + T (va, vb, c )1F ]

11

+

Ec

[ Zc

Zc ,a exp( c , va

+

c , vb

+ T (va, vb, c )1F¯].

 (1 +

z)(1 +

11

z,a)Ec

[ Z

Za

exp(

c

, va

+

c , vb

+ T (va, vb, c )1F ] + exp(-(log1.8 n))

 (1 +

z)(1 + Z Za

z,a) Ec

[exp(

c

, va

+

c , vb

+ T (va, vb, c )] + exp(-(log1.8 n))

 (1 +

z)(1 +

z,a)(1 + O~(1/d)) exp(

va + vb + T (va, vb, ·)

2
) + exp(-(log1.8 n)).

Z Za

2d

Here the last step
va+vb+T (va,vb,·) 2 2d

used Lemma 10. is bounded by (4

Since both Z + 2K)2, we

and Za can be bounded by O(n), and know the first term is of order (1/n2),

and the second term is negligible.

For the lowerbound, we can have

11 p([a, b]) = Ec [ Zc Zc ,a exp( c , va + c , vb + T (va, vb, c )1F ]

11

+

Ec

[ Zc

Zc ,a exp( c , va

+

c , vb

+ T (va, vb, c )1F¯].

 (1 -

z)(1 -

11

z,a)Ec

[ Z

Za

exp(

c

, va

+

c , vb

+ T (va, vb, c )1F ]

 (1 -

z)(1 - Z Za

z,a) {Ec [exp( c , va

+

c , vb

+ T (va, vb, c ))]

-Ec [exp( c , va + c , vb + T (va, vb, c ))1F¯]}

 (1 - z)(1 - z,a) Z Za

Ec [exp( c , va + c , vb + T (va, vb, c ))] - exp(-(log1.8 n))

 (1 - z)(1 - z,a)(1 - O~(1/d))

exp(

va + vb + T (va, vb, ·)

2
) - exp(-(log1.8 n))

Z Za

2d

.

Again the last step is using Lemma 10 and the term exp(-(log1.8 n) is negligible. Combining the upper and lower bound, we know

log p([a, b]) =

va + vb + T (va, vb, ·) 2d

2
- log Z - log Za ±

p,

where p = O( z + z,a) + O~(1/d).

17

Under review as a conference paper at ICLR 2019

Now we turn to the most complicated term log p(w, [a, b]). By definition we know

1 11

p(w, [a, b])

=

Ec,c

[ Zc

exp(

c, vw

) Zc

Zc ,a exp( c , va

+

c , vb

+ T (va, vb, c )].

We will follow similar idea as before. Let F be the event that both c, c satisfy the equations in Lemma 1 and F¯ be its negation. By Lemma 1 and union bound we know Pr[F]  1 - exp(-(log2 n)).

We again separate the co-occurrence probability based on the event F:

1 11

p(w, [a, b])

=Ec,c

[ Zc

exp(

c, vw

) Zc

Zc ,a exp( c , va

+

c , vb

+ T (va, vb, c )1F ]

1 11

+ Ec,c

[ Zc

exp(

c, vw

) Zc

Zc ,a exp( c , va

+

c , vb

+ T (va, vb, c )1F¯].

For the second term, we can again use Lemma 8 to show that it is bounded by exp(-(log1.8 n)). Now, using techniques similar as before, we can prove

p(w, [a, b]) = (1 ± O( z +

1 z,a)) Z2Za Ec,c [exp( c, vw ) exp( c , va

+

c , vb

+ T (va, vb, c ))].

(9)

Now the final step is to use the fact that c and c are close to simplify the final formula. Let A(c ) = Ec|c [exp( c, vw )], by Lemma 9 we know A(c )  (1 ± w) exp( vw, c ). Therefore

Ec,c [exp( c, vw ) exp( c , va + c , vb + T (va, vb, c ))]

=Ec [exp( c , va + c , vb + T (va, vb, c ))Ec|c [exp( c, vw )]]

=Ec [exp( c , va + c , vb + T (va, vb, c ))A(c )]

=(1 ± w)Ec [exp( c , va + c , vb + T (va, vb, c ) + c , vw )]

=(1 ±

w)(1 ± O~(1/d)) exp(

vw + va + vb + T (va, vb, ·) 2d

2
).

Here the last step is again by Lemma 10. Combining this with Equation equation 9 gives the result.

Finally we prove Corollary 1, which is just a simple calculation based on Theorem 1:

Proof of Corollary 1. By the definition of PMI3, we know

P M I3 = log p(w, [a, b]) + log p(a) + log p(b) + log p(w) - log p(w, a) - log p(w, b) - log p([a, b]).

=(

vw + va + vb + T (va, vb, ·) 2d

2
- 2 log Z - log Za) + (

va

2
+

2d

vb

2
+

2d

vw 2 - 3 log Z) 2d

-(

vw + va

2
+

2d

vw + vb 2d

2
- 4 log Z) - (

va + vb + T (va, vb, ·) 2d

2
- log Z - log Za) ± 7

= T (va, vb, vw) ± 7 . d

B.3 AUXILIARY LEMMAS

Tail bound for 2 distribution We will use the following tail bounds for the generalized 2squared distribution.

Lemma 7. Laurent & Massart (2000) Let y1, . . . , yd be i.i.d. standard normal random variables,

and let a1, . . . , ad be nonnegative real numbers. Set Y =

d i=1

aiyi2

and

a

=

(a1,

a2,

.

.

.

,

ad).

Then

the following hold for any positive real number x:



P (Y - E[Y ]  2 a 2 x + 2 a x)  exp(-x)

P (Y - E[Y ]  -2 a 2 x)  exp(-x).

18

Under review as a conference paper at ICLR 2019

Additional Lemmas We will use several tools developed in Arora et al. (2015). The first lemma allows us to bound the probabilities the discourse vector c does not satisfy the results of Lemma 1.

Lemma 8. Let F be any event 1 - exp(-(log2 n)), and F¯ be

that depends on the discourse vector c with probability its negation. Suppose r is a vector of norm O( d), then

at

least

Ec[exp( r, c )1F¯]  exp(-(log1.8 n)).
Further, if we consider two consecutive discourse vectors c, c , redefine F to be an event that can depend on both discourse vectors, again with probability at least 1 - exp(-(log2 n)). If r, r are two vectors of norm O( d) we have

Ec,c [exp( r, c ) exp( r , c )1F¯]  exp(-(log1.8 n)).

Proof. The proof of this lemma appears on page 20 in Arora et al. (2015), as a step in the proof of their Theorem 2.2. For completeness, we reproduce (and slightly adapt) their argument here.

Observe that

Ec[exp( r, c )1F¯] = Ec[exp( r, c )1 r,c >01F¯] + Ec[exp( r, c )1 r,c <01F¯]. The second term of B.3 is upper bounded by

Ec[1F¯]  exp(-(log2 n)). Note that the first term of B.3 can be bounded as follows:

Ec[exp( r, c )1 r,c >01F¯]  Ec[exp( r, c )1 r,c >01F¯]  Ec[exp( r, c )1F¯]

for  > 1. Therefore, to obtain a bound on Ec[exp( r, c )1 r,c >01F¯] is suffices to bound

 when r = ( d).

Ec[exp( r, c )1F¯]

Let z denote the random variable r, c , and let r(z) = 1F¯. Using Lemma A.4 in Arora et al. (2015), we have
Ec[exp(z)r(z)]  Ec[exp(z)1[t,](z)],
where t satisfies that Ec[1[t,](z)] = Pr[z  t] = Ec[r(z)]  exp(-(log2 n)). Then by Lemma A.1 of Arora et al. (2015), we have that t  (log.9 n). Finally, applying Corollary A.3 of Arora et al. (2015), we have
Ec[exp(z)r(z)]  Ec[exp(z)1[t,](z)] = exp(-(log1.8 n)),
which completes the proof for the first part of this lemma.

The second part of this lemma can be proved in much the same fashion. By Cauchy-Schwarz,

(Ec,c [exp( r, c ) exp( r , c )1F¯])2  Ec,c [exp( r, c )21F¯] Ec,c [exp( r , c )21F¯]  Ec[exp( 2r, c )Ec |c[1F¯]] Ec [exp( 2r , c )Ec|c [1F¯]] .

Now we bound Ec[exp( 2r, c )Ec |c[1F¯]] using the same argument as above in the first part of this
proof, replacing 1F¯ with Ec |c[1F¯], r with 2r, and r(z) = 1F¯ with r(z) = Ec |z[1F¯]. In particular, we have Ec[exp( 2r, c )Ec |c[1F¯]]  exp(-(log1.8 n)). Likewise, we have the same bound for
Ec [exp( 2r , c )Ec|c [1F¯]]. Putting these two together, we conclude that

Ec,c [exp( r, c ) exp( r , c )1F¯]  Ec[exp( 2r, c )Ec |c[1F¯]] 1/2 Ec [exp( 2r , c )Ec|c [1F¯]] 1/2  exp(-(log1.8 n)),

as desired.

The next lemma allows us to handle the difference between two consecutive discourse vectors: 19

Under review as a conference paper at ICLR 2019

Lemma 9. Let c, c be two discourse vectors that are adjacent, let vw be a word embedding satisfying vw  K d, and let A(c) := Ec |c[exp( vw, c )], then we have
A(c)  (1 ± w) exp( vw, c ).
Proof. The proof of this lemma appears on page 21 in Arora et al. (2015), again as a step in the proof of their Theorem 2.2. For completeness, we reproduce the argument here.
 Since vw  K d for some constant K , we have that vw, c - c  vw c - c  K d c - c . Hence,
A(c) = Ec |c[exp( vw, c )] = exp( vw, c )Ec |c[exp( vw, c - c )]  exp( vw, c )Ec |c[K d c - c )]  (1 + w) exp( vw, c ),
where the last inequality follows from our model assumptions.
To get the lower bound, observe that 
Ec |c[exp(K d c - c )] + Ec |c[exp(-K d c - c )]  2.
Therefore, the model assumptions imply that 
Ec |c[exp(-K d c - c )]  1 - w.
Hence,
A(c) = exp( vw, c )Ec |c[exp( vw, c - c )]  exp( vw, c )Ec |c[exp(K d c - c )]  (1 - w) exp( vw, c ).

The next lemma we use gives bound on E[exp( v, c )] where c is a uniform vector on the unit sphere. Lemma 10. [Lemma A.5 in Arora et al. (2015)] Let v  Rd be a fixed vector with norm v = O( d). For random variable c with uniform distribution over the sphere, we have that

where c = O~(1/d).

log E[exp( v, c )] =

v 2± 2d

c,

We end with the proof of Lemma 2.

Proof of Lemma 2. Just for this proof, we use the following notation. Let Id×d be the d-dimensional

identity matrix, and let x1, x2, . . . , xn be i.i.d. draws from N (0, Id×d). Let yi = xi 2, and note that

yi2 is a standard -squared random variable with d degrees of freedom. Let  be a positive constant,

and let Zc =

s1ni=, s12e,x.p. (.

, sn vi,

be i.i.d. c ), and

draws define

from Zc,a

a distribution supported on [0, ]. Let

=

n i=1

exp(

vi, c

+ T (va, vi, c)).

vi

=

si

·

xi.

Define

We first cover the unit sphere by a finite number of metric balls of small radius. Then we show that with high probability, the partition function at the center of these balls is indeed bounded below by a constant. Finally, we show that the partition function evaluated at an arbitrary point on the unit sphere can't be too far from the partition function at one of the ball centers provided the norms of the vi are not too large. We finish by appropriately controlling the norms of the vi.

For > d-1, cover the unit sphere in Rd with N = ( 2 + 1)d balls of radius . Let c1, c2, . . . , cN be the centers of these balls (so that each ci is a unit vector). Let   0 be a constant. Note that vj, ci = cj, sj · ci and vk, ci + T (vl, vk, ci) = xk, sk(I + T (vl, ·, ·))T ci are Gaussian random
variables with mean 0.

20

Under review as a conference paper at ICLR 2019

Let Fi be the event that there exists some j, k  [n] such that vj, ci  0 and vk +T (va, vk, ·), ci  0 . Note that

Pr[F¯i]  Pr[j  [n], vj, ci  0] + Pr[k  [n], vk + T (va, vk, ·), ci  0]

nn

= Pr[ vj, ci  0] + Pr[ vk + T (va, vk, ·), ci  0]

j=1 k



1 2n

+

1 2n

 exp(-(n)).



Let  > 0. Let Gi be the event that yi < 

d. Set t = ( 1
2

2

-

1 2

-

1 2

)2d,

so

that

d+2

dt+2t =

2d. Then by Lemma 7,

Pr[G¯i]  exp(-t).

Let E =

N i=1

Fi

n i=1

Gi.

Assume

that

the

word

embeddings

satisfy

the

event

E.

Let

ci

be

a

center

of one of the covering balls such that c - ci 2 < . Let vj, vk be vectors that satisfies xj, ci  -

and vk + T (va, vk, ·), ci  -. By Cauchy-Schwarz and the definition of E, we have

vj, c = vj, ci + vj, c - ci

-

vj

c 

-

ci

 -  d

= -d-1/2 

for some appropriate universal constant . Likewise, using the boundedness property of T , we have



vk + T (va, vk, ·), c

- K 

d

= - Kd-1/2

.

Hence, and

n
Zc = exp( vi, c )  exp( vj, c )  exp(- )
i=1
n
Zc,a = exp( vi, c + T (va, vi, c))  exp( vk + T (va, vk, ·), c )  exp(- ).
i=1

It remains to analyze the probability of E. By the union bound, we have
Pr[E]  1 - N exp - n2 - n exp(-t) 2
= 1 - exp(O(d log d) - (n)) - exp(log n - ( 1 2 - 1 - 1 )2d) 2 22
= 1 - exp((d log d) - (n)) - exp((log n) - (d)).

Note that this is a high probability if n d log d and d log n.

21

