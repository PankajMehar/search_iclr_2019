Under review as a conference paper at ICLR 2019
ADVERSARIAL VULNERABILITY OF NEURAL NETWORKS INCREASES WITH INPUT DIMENSION
Anonymous authors Paper under double-blind review
ABSTRACT
Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the 1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network's weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after training.
1 INTRODUCTION
Following the work of Goodfellow et al. (2014), Convolutional Neural Networks (CNNs) have been found vulnerable to adversarial examples: an adversary can drive the performance of state-of-the art CNNs down to chance level with imperceptible changes of the inputs. A number of studies have tried to address this issue, but only few have stressed that, because adversarial examples are essentially small input changes that create large output variations, they are inherently caused by large gradients of the neural network with respect to its inputs. Of course, this view, which we will focus on here, assumes that the network and loss are differentiable. It has the advantage to yield a large body of specific mathematical tools, but might not be easily extendable to masked gradients, non-smooth models or the 0-1-loss. Nevertheless, our conclusions might even hold for non-smooth models, given that the latter can often be viewed as smooth at a coarser level. Contributions. More specifically, we provide theoretical and empirical arguments supporting the existence of a monotonic relationship between the gradient norm of the training objective (of a differentiable classifier) and its adversarial vulnerability. Evaluating this norm based on the weight statistics at initialization, we show that CNNs and most feed-forward networks, by design, exhibit increasingly large gradients with input dimension d, almost independently of their architecture. That leaves them increasingly vulnerable to adversarial noise. We corroborate our theoretical results by extensive experiments. Although some of those experiments involve adversarial regularization schemes, our goal is not to advocate a new adversarial defense (these schemes are already known), but to show how their effect can be explained by our first order analysis. We do not claim to explain all aspects of adversarial vulnerability, but we claim that our first order argument suffices to explain a significant part of the empirical findings on adversarial vulnerability. This calls for researching the design of neural network architectures with inherently smaller gradients and provides useful guidelines to practitioners and network designers.
2 FROM ADVERSARIAL EXAMPLES TO LARGE GRADIENTS
Suppose that a given classifier  classifies an image x as being in category (x). An adversarial image is a small modification of x, barely noticeable to the human eye, that suffices to fool the classifier into predicting a class different from (x). It is a small perturbation of the inputs, that creates a large variation of outputs. Adversarial examples thus seem inherently related to large gradients of the network. A connection, that we will now clarify. Note that visible adversarial examples sometimes appear in the literature, but we deliberately focus on imperceptible ones.
1

Under review as a conference paper at ICLR 2019

Adversarial vulnerability and adversarial damage. In practice, an adversarial image is constructed by adding a perturbation  to the original image x such that   for some (small) number and a given norm · over the input space. We call the perturbed input x +  an -sized
· -attack and say that the attack was successful when (x + ) = (x). This motivates

Definition 1. Given a distribution P over the input-space, we call adversarial vulnerability of a classifier  to an -sized · -attack the probability that there exists a perturbation  of x such that

  and (x) = (x + ) .

(1)

We call the average increase-after-attack ExP [L] of a loss L the (L-) adversarial damage (of the classifier  to an -sized · -attack).

When L is the 0-1-loss L0/1, adversarial damage is the accuracy-drop after attack. The 0-1-loss damage is always smaller than adversarial vulnerability, because vulnerability counts all class-changes of (x), whereas some of them may be neutral to adversarial damage (e.g. a change between two wrong classes). The L0/1-adversarial damage thus lower bounds adversarial vulnerability. Both are even equal when the classifier is perfect (before attack), because then every change of label introduces an error. It is hence tempting to evaluate adversarial vulnerability with L0/1-adversarial damage.

From L0/1 to L and to xL. In practice however, we do not train our classifiers with the non-differentiable 0-1-loss but use a smoother loss L, such as the cross-entropy loss. For similar reasons, we will now investigate the adversarial damage Ex [L(x, c)] with loss L rather than L0/1. Like for Goodfellow et al. (2014); Lyu et al. (2015); Sinha et al. (2018) and many others, a classifier  will hence be robust if, on average over x, a small adversarial perturbation  of x creates only a small variation L of the loss. Now, if   , then a first order Taylor expansion in shows that

L = max |L(x + , c) - L(x, c)|  max |xL · | = |||xL|||,

:  

:  

(2)

where xL denotes the gradient of L with respect to x, and where the last equality stems from the definition of the dual norm |||·||| of · . Now two remarks. First: the dual norm only kicks in because we let the input noise  optimally adjust to the coordinates of xL within its -constraint. This is the brand mark of adversarial noise: the different coordinates add up, instead of statistically canceling weaxiclLlhs. toSrtiehcctelorynoadulirtgeanmswathrikteh:ywwxhoLilue.ldIthfweiniTstahteyarlaodnr deoxmpannsoiiosne,.itnhFeo(n2r)ebxweacmiollmpaleleis,gienfxwwacietthfimothrpeionssfiiegnnittheoasftitmhealcp2oeorrtduir,nbatahtteeiosnnosf, for finite ones it may actually be dominated by higher-order terms. Our experiments (Figures 1 & 2) however strongly suggest that in practice the first order term dominates the others. Now, remembering that the dual norm of an p-norm is the corresponding q-norm, and summarizing, we have proven

Lemma 2. At first order approximation in , an -sized adversarial attack generated with norm ·

increases the loss L at point x by |||xL|||, where |||·||| is the dual norm of · . In particular, an

-sized p-attack increases the loss by

xL

q

where 1  p   and

1 p

+

1 q

= 1.

Consequently, the adversarial damage of a classifier with loss L to -sized attacks generated with norm · is Ex|||xL|||. This is valid only at first order, but it proves that at least this kind of first-order
vulnerability is present. We will see that the first-order predictions closely match the experiments, and that this insight helps protecting even against iterative (non-first-order) attack methods (Figure 1).

Calibrating the threshold to the attack-norm · . Lemma 2 shows that adversarial vulnerability

depends on three main factors: (i) · , the norm chosen for the attack (ii) , the size of the attack,

and (iii) Ex|||xL||| , the expected dual norm of xL. We could see Point (i) as a measure of our sensibility to image perturbations, (ii) as our sensibility threshold, and (iii) as the classifier's expected

marginal sensibility to a unit perturbation. Ex|||xL||| hence intuitively captures the discrepancy between our perception (as modeled by · ) and the classifier's perception for an input-perturbation

of small size . Of course, this viewpoint supposes that we actually found a norm · (or more

generally a metric) that faithfully reflects human perception ­ a project in its own right, far beyond

the scope of this paper. However, it is clear that the threshold that we choose should depend on the

norm · and hence on the input-dimension d. In particular, for a given pixel-wise order of magnitude

of the perturbations , the p-norm of the perturbation will scale like d1/p. This suggests to write the threshold p used with p-attacks as:

p =  d1/p ,

(3)

2

Under review as a conference paper at ICLR 2019

where  denotes a dimension-independent constant. In Appendix D we show that this scaling also

preserves the average signal-to-noise ratio p could correspond to a constant human

pxerc2ep/tion-2t,hbreosthhoalcdr.oWss inthortmhissainndmdiinmde,ntshieonims, psoatitehnatt

reader may already jump to Section 3, which contains our main contributions: the estimation of

Ex xL q for standard feed-forward nets. Meanwhile, the rest of this section shortly discusses two straightforward defenses that we will use later and that further illustrate the role of gradients.

A new old regularizer. Lemma 2 shows that the loss of the network after an 2 -sized · -attack is

L ,|||·|||(x, c) := L(x, c) + 2 |||xL||| .

(4)

It is thus natural to take this loss-after-attack as a new training objective. Here we introduced a factor 2

for reasons that will become clear in a moment. Incidentally, for · an old regularization-scheme proposed by Drucker & LeCun (1991)

=call·ed2 ,dtohuibs lnee-bwalcokspsrorepdaugcaetsiotno.

At the time, the authors argued that slightly decreasing a function's or a classifier's sensitivity to input

perturbations should improve generalization. In a sense, this is exactly our motivation when defending

against adversarial examples. It is thus not surprising to end up with the same regularization term.

Note that our reasoning only shows that training with one specific norm |||·||| in (4) helps to protect

against adversarial examples generated from · . A priori, we do not know what will happen for

attacks generated with other norms; but our experiments suggest that training with one norm also

protects against other attacks (see Figure 2 and Section 4.1).

Link to adversarially-augmented training. In (1), designates an attack-size threshold, while in (4), it is a regularization-strength. Rather than a notation conflict, this reflects an intrinsic duality between two complementary interpretations of , which we now investigate further. Suppose that, instead of using the loss-after-attack, we augment our training set with -sized · -attacks x + , where for each training point x, the perturbation  is generated on the fly to locally maximize the loss-increase. Then we are effectively training with

L~

,

·

(x, c)

:=

1 (L(x, c) + L(x + 2

, c)) ,

(5)

where by construction  satisfies (2). We will refer to this technique as adversarially augmented

training. It was first introduced by FGSM1-augmented training. Using

Goodfellow et al. (2014) with the first order Taylor expansion

· in

= of

· (2),

under the name of this `old-plus-post-

attack' loss of (5) simply reduces to our loss-after-attack, which proves

Proposition 3. Up to first-order approximations in , L~ , · = L ,|||·||| . Said differently, for small enough , adversarially-augmented training with -sized · -attacks amounts to penalizing the dual norm |||·||| of xL with weight /2. In particular, double-backpropagation corresponds to training
with 2-attacks, while FGSM-augmented training corresponds to an 1-penalty on xL.

This correspondence between training with perturbations and using a regularizer can be compared to Tikhonov regularization: Tikhonov regularization amounts to training with random noise Bishop (1995), while training with adversarial noise amounts to penalizing xL. Section 4.1 verifies the correspondence between adversarial augmentation and gradient regularization empirically, which also strongly suggests the empirical validity of the first-order Taylor expansion in (2).

3 ESTIMATING xL q TO EVALUATE ADVERSARIAL VULNERABILITY

In this section, we evaluate the size of xL q for standard neural network architectures. We start with fully-connected networks, and finish with a much more general theorem that, not only encompasses CNNs (with or without strided convolutions), but also shows that the gradient-norms are essentially independent of the network topology. We start our analysis by showing how changing q affects the size of xL q. Suppose for a moment that the coordinates of xL have typical magnitude |xL|. Then xL q scales like d1/q|xL|. Consequently

p xL q  p d1/q |xL|  d |xL| .

(6)

1FGSM = Fast Gradient Sign Method

3

Under review as a conference paper at ICLR 2019

This equation carries two important messages. First, we see how xL q depends on d and q. The dependence seems highest for q = 1. But once we account for the varying perceptibility threshold p  d1/p, we see that adversarial vulnerability scales like d · |xL|, whatever p-norm we use. Second, (6) shows that to be robust against any type of p-attack at any input-dimension d, the average absolute value of the coefficients of xL must grow slower than 1/d. Now, here is the catch, which brings us to our core insight.

3.1 CORE IDEA: ONE NEURON WITH MANY INPUTS

In order to preserve the activation variance of the neurons from layer to layer, the neural weights are

usually initialized with a variance that is inversely proportional to the number of inputs per neuron.

Imagine for a moment that the network consisted only of one output neuron o linearly connected to

all input pixels. For the weights with a variance

purpose of this example, we of 1/d, their average absolute

assimilate o value |xo|

and L. Because we  |xL| grows like

1in/itiadl,izraeththeer

tlhikaendth/erdeq=uiredd.1/d. By (6), the adversarial vulnerability xo q  xL q therefore increases

This toy example shows that the standard initialization scheme, which preserves the variance from layer to layer, causes the average coordinate-size |xL| to grow like 1/ d instead of 1/d. When an -attack tweaks its -sized input-perturbations to align with the coordinate-signs of xL,all coordinates of xL add up in absolute value, resulting in an output-perturbation that scales like d
and leaves the network increasingly vulnerable with growing input-dimension.

3.2 GENERALIZATION TO DEEP NETWORKS

Our next theorems generalize the previous toy example to a very wide class of feedforward nets with ReLU activation functions. For illustration purposes, we start with fully connected nets and only then proceed to the broader class, which includes any succession of (possibly strided) convolutional layers. In essence, the proofs iterate our insight on one layer over a sequence of layers. They all rely on the following set (H) of hypotheses:

H1 Non-input neurons are followed by a ReLU killing half of its inputs, independently of the weights.
H2 Neurons are partitioned into layers, meaning groups that each path traverses at most once. H3 All weights have 0 expectation and variance 2/(in-degree) (`He-initialization'). H4 The weights from different layers are independent. H5 Two distinct weights w, w from a same node satisfy E [w w ] = 0. If we follow common practice and initialize our nets as proposed by He et al. (2015), then H3-H5 are satisfied at initialization by design, while H1 is usually a very good approximation (Balduzzi et al., 2016). Note that such i.i.d. weight assumptions have been widely used to analyze neural nets and are at the heart of very influential and successful prior work (e.g., equivalence between neural nets and Gaussian processes as pioneered by Neal 1996). Nevertheless, they do not hold after training. That is why all our statements in this section are to be understood as orders of magnitudes that are very well satisfied at initialization in theory and in practice, and that we will confirm experimentally after training in Section 4. Said differently, while our theorems rely on the statistics of neural nets at initialization, our experiments confirm their conclusions after training.

Theorem 4 (Vulnerability of Fully Connected Nets). Consider a succession of fully connected

layers with ReLU activations which takes inputs x of dimension d, satisfies assumptions (H), and

outputs logits fk(x) that get fed to a final cross-entropy-loss layer L. Then the coordinates of xfk

grow like 1/ d, and

xL

q



d1 q

-

1 2

and

 p xL q  d .

(7)

These networks are thus increasingly vulnerable to p-attacks with growing input-dimension.

Theorem 4 is a special case of the next theorem, which will show that the previous conclusions are essentially independent of the network-topology. We will use the following symmetry assumption on the neural connections. For a given path p, let the path-degree dp be the multiset of encountered in-degrees along path p. For a fully connected network, this is the unordered sequence of layer-sizes

4

Under review as a conference paper at ICLR 2019

preceding the last path-node, including the input-layer. Now consider the multiset {dp}pP(x,o) of all path-degrees when p varies among all paths from input x to output o. The symmetry assumption (relatively to o) is

(S) All input nodes x have the same multiset {dp}pP(x,o) of path-degrees from x to o. Intuitively, this means that the statistics of degrees encountered along paths to the output are the same for all input nodes. This symmetry assumption is exactly satisfied by fully connected nets, almost satisfied by CNNs (up to boundary effects, which can be alleviated via periodic or mirror padding) and exactly satisfied by strided layers, if the layer-size is a multiple of the stride.

Theorem 5 (Vulnerability of Feedforward Nets). Consider any feed-forward network with linear

connections and ReLU activation functions. Assume the net satisfies assumptions (H) and outputs

logits fk(x) that get fed to thecross-entropy-loss L. Then xfk 2 is independent of the input

dimension dand 2 |xfk|  1/ d and

xL 2 (7) still

 d. holds:

Moreover, xL q 

if d

the net satisfies

1 q

-

1 2

and

p

x

the symmetry L q  d.

assumption

(S ),

then

Theorems 4 and 5 are proven in Appendix B. The main proof idea is that in the gradient norm computation, the He-initialization exactly compensates the combinatorics of the number of paths in the network, so that this norm becomes independent of the network topology. In particular, we get

Corollary 6 (Vulnerability of CNNs). In any succession of convolution and dense layers, strided

or not, with ReLU cross-entropy-loss

activations, that satisfies assumptions (H) and outputs L, the gradient of the logit-coordinates scale like 1/ d

logits that and (7) is

get fed to the satisfied. It is

hence increasingly vulnerable with growing input-resolution to attacks generated with any p-norm.

Appendix A shows that the network gradient are dampened when replacing strided layers by average poolings, essentially because average-pooling weights do not follow the He-init assumption H3.

4 EMPIRICAL RESULTS
In Section 4.1, we empirically verify the validity of the first-order Taylor approximation made in (2) (Fig.1), for example by checking the correspondence between loss-gradient regularization aavnedraagdever1s-anroiarmllyo-af ugxmLeannteddthtreaaindivnegrs(aFriiagl.2v)u.lnSeercatbioilnity4.g2rotwhenlikeempdiriacsalplryedviecrtiefidesbythCaot rboolltahryth6e. For all experiments, we approximate adversarial vulnerability using various attacks of the Foolboxpackage (Rauber et al., 2017). We use an  attack-threshold of size  = 0.005 which, for pixel-values ranging from 0 to 1, is completely imperceptible but suffices to fool the classifiers on a significant proportion of examples. This -threshold should not be confused with the regularizationstrengths appearing in (4) and (5), which will be varied in some experiments. 4.1 FIRST-ORDER APPROXIMATION, GRADIENT PENALTY, ADVERSARIAL AUGMENTATION

% fooled ( adv. vuln.)

60 -attacks
2-attacks

deep-fool

40

iterative-  iterative- 2

20

-4.0

-3.5 (a-)3.0log10 -2.5

-2.0

50 100 150 200 (b) Ex xL 1

Figure 1: Adversarial vulnerability approximated by different attack-types for 10 trained networks as a function of (a) the 1 gradient regularization-strength used to train the nets and (b) the average gradient-norm. These curves confirm that the first-order expansion term in (2) is a crucial component of adversarial vulnerability.

5

Under review as a conference paper at ICLR 2019

accuracy

adversarial vulnerability

85 200 60
50 80 150

Ex xL 1

40 75 100
30 50 70
20

-4.0 -3.5 -3.0 -2.5 -2.0
(a) log10 ( d1/p)
60

-4.0 -3.5 -3.0 -2.5 -2.0
(b) log10 ( d1/p)

-4.0 85

-3.5 -3.0 -2.5 -2.0
(c) log10 ( d1/p)

adversarial vulnerability

accuray

Ex xL 2

50

80 4

40

75

Grad Regu q = 1 Grad Regu q = 2

30

2

Adv Train p = 
70 Adv Train p = 2

20 50 100 150 200
(d) Ex xL 1

50 100 150 200
(e) Ex xL 1

65 20
(f)

Cross-Lipschitz
30 40 50 60
adversarial vulnerability

Figure 2: Average norm Ex xL of the loss-gradients, adversarial vulnerability and accuracy (before attack) of various networks trained with different adversarial regularization methods and

regularization strengths . Each point represents a trained network, and each curve a training-method.

Upper row: A priori, the regularization-strengths have different meanings for each method. The near

superposition of all upper-row curves illustrates (i) the duality between adversarial augmentation

and gradient-regularization (Prop. 3) and (ii) confirms the rescaling of proposed in (3). (d): near

functional relation between adversarial vulnerability and average loss-gradient norms. (e): the

near-perfect linear given attack-norm

relation between the also protects against

E xL others.

(1fa)n: dMEergixnLg

22bsaungdg2ecstsshtohwatsptrhoattecatlilnagdavgearsinasritaal

augmentation and gradient-regularization methods achieve similar accuracy-vulnerability trade-offs.

We train several CNNs with same architecture to classify CIFAR-10 images (Krizhevsky, 2009). For each net, we use a specific training method with a specific regularization value . The training methods used were 1- and 2-penalization of xL (Eq. 4), adversarial augmentation with - and 2- attacks (Eq. 5) and the cross-Lipschitz regularizer (Eq. 17 in Appendix C). All networks have 6 `strided convolution  batchnorm  ReLU' layers with strides [1, 2, 2, 2, 2, 2] respectively and 64 output-channels each, followed by a final fully-connected linear layer. Results are summarized in Figures 1 and 2. Figure 1 fixes the training method ­ gradient 1-regularization ­ and plots the obtained adversarial vulnerabilities for various attacks types. Figure 2 fixes the attack type ­ iterative 1-attack ­ but plots the curves obtained for various training methods. Note that our goal here is not to advocate one defense over another, but rather to check the validity of the Taylor expansion, and empirically verify that first order terms (i.e., gradients) suffice to explain much of the observed adversarial vulnerability. Similarly, our goal in testing several attacks (Figure 1) is not to present a specifically strong one, but rather to verify that for all attacks, the trends are the same: the vulnerability grows with increasing gradients.

Validity of first order expansion. The efficiency of the first-order defense against iterative (non-

firs-order) attacks (Fig.1a) strongly suggest that the first-order Taylor expansion in (2) is indeed

a crucial component of adversarial vulnerability. This is further confirmed by the functional-like

dependence between independence on the

tarnayinainpgprmoxeitmhoadtio(Fnigo.f2add)v. eSrasiadridailfvfeurlennetrlayb, ialditvyearsnadriEalxexaxmLpl1es(Fseigem.1bi)n,daenedd

its to

be primarily caused by large gradients of the classifier as captured via the induced loss. 2

Illustration of Proposition 3. The upper row of Figure 2 plots Ex xL1 , adversarial vulnerability and accuracy as a function of d1/p. The excellent match between the adversarial augmentation curve with p =  (p = 2) and its gradient-regularization dual counterpart with q = 1 (resp. q = 2) illustrates the duality between as a threshold for adversarially-augmented training and as a regularization constant in the regularized loss (Proposition 3). It also supports the validity of the first-order Taylor expansion in (2).

2On Figure 1, the two -attacks seem more efficient than the others, because we chose an  perturbation threshold ( ). With an 2-threshold it is the opposite (see Figure 5, Appendix E).

6

Under review as a conference paper at ICLR 2019

Confirmation of (3). Still on the upper row, the curves for p = , q = 1 have no reason to match those for p = q = 2 when plotted against , because -threshold is relative to a specific attack-norm. However, (3) suggested that the rescaled thresholds d1/p may approximately correspond to a same `threshold-unit' across p-norms and across dimension. This is well confirmed by the upper row plots: by rescaling the x-axis, the p = q = 2 and q = 1, p =  curves get almost super-imposed.

Accuracy-vs-Vulnerability Trade-Off. Merging Figures 2b and 2c by taking out , Figure 2f shows that all gradient regularization and adversarial training methods yield equivalent accuracyvulnerability trade-offs. Incidentally, for higher penalization values, these trade-offs appear to be much better than those given by cross Lipschitz regularization.

The penalty-norm does not matter. We were surprised to see that on Figures 2d and 2f, the L ,q curves are almost identical for q = 1 and 2. This indicates that both norms can be used interchangeably

in (4) (modulo proper rescaling of via (3)), and suggests that protecting against a specific attack-

norm also protects against others. (6) may provide an explanation: if the coordinates of xL behave like centered, uncorrelated variables with equal variance ­which follows from assumptions (H) ­, then

the 1- and in Figure 2e

2-norms of xL are simply proportional. Plotting Ex confirms this explanation. The slope is independent

of xthLe(xtra)in2inaggaminestthEodx.

ThxLer(exfo)re1,

pbuent aalliszoindgrivedxoLw(xn)Ex1

during training will not xL 2 and vice-versa.

only

decrease

Ex

xL

1 (as shown in Figure 2a),

4.2 VULNERABILITY GROWS WITH INPUT RESOLUTION

Theorems 4-5 and Corollary 6 predict a linear growth of the average 1-norm of xL with the square root of the input dimension d, and therefore also of adversarial vulner-

ability (Lemma 2). To test these predictions, we created a 12-class dataset of approxi-

mately 80, 000 256 × 256 × 3-sized RGB-images by merging similar ImageNet-classes, re-

sizing the smallest image-edge to 256 pixels and center-cropping the result. We then

downsized the images to 32, 64, 128 and 256 pixels per edge, and trained 10 CNNs

on each of these downsized datasets.

vulnerability Ex xL 1

We then computed adversarial vulnerability (with iterative -attacks) and aavsearmageeheldx-Lout1tefostr-deaactahsente.tFwiogrukreon3

30 20

100 80 60

summarizes the results. The dashed-

line follows the median of each group

40

of 10 networks; the errorbars show the 10th and 90th quantiles. As preadpnricodtxeaidmdvbaeytresolayurrilaitnlheveauorlrlneyemwrasi,bthbiloittyhdg.rAowxsLtahpe1gradients get much larger at higher dimensions, the first order approximation in (2) becomes less and less valid, which explains the little inflection of

10

32 64 128
image-width



 256 d

32 64 128
image-width



 256 d

Figure (right)

3: Both adversarial vulnerability (left) and increase linearly with the square-root of

Ex the

imxaLge1-

resolution d, as predicted by Corollary 6. Adversarial vulner-

ability gets slightly dampened at higher dimension, probably

because the first-order approximation made in (2) becomes

less and less valid.

the adversarial vulnerability curve. For smaller -thresholds, we verified that the inflection disappears.

All networks had exactly the same amount of parameters and very similar structure across the various input-resolutions. The CNNs were a succession of 8 `convolution  batchnorm  ReLU' layers with 64 output channels, followed by a final full-connection to the 12 logit-outputs. We used 2 × 2max-poolings after layers 2,4 and 6, and a final max-pooling after layer 8 that fed only 1 neuron per channel to the fully-connected layer. To ensure that the convolution-kernels cover similar ranges of the images across each of the 32, 64, 128 and 256 input-resolutions, we respectively dilated all convolutions (`à trous') by a factor 1, 2, 4 and 8.

5 RELATED LITERATURE
Goodfellow et al. (2014) already stressed that adversarial vulnerability increases with growing dimension d. But their argument only relied on a linear `one-output-to-many-inputs'-model with

7

Under review as a conference paper at ICLR 2019

dimension-independent weights. They therefore concluded on a linear growth of adversarial vul-

nerability with d. In contrast, our theory applies to almost any standard feed-forward architecture

(not just linear), and shows vulnerability increases like

thadt,(onnoct ed)w, ealamdojussttinfodretpheenwdeenigtlhyt'osf

dimension-dependence, adversarial the architecture. Nevertheless, our

experiments confirm Goodfellow et al.'s idea that our networks are "too linear-like", in the sense

that a first-order Taylor expansion is indeed sufficient to explain the adversarial vulnerability of

neural networks. As suggested by the one-output-to-many-inputs model, the culprit is that growing

dimensionality gives the adversary more and more room to `wriggle around' with the noise and adjust

to the gradient of the output neuron. This wriggling, we show, is still possible when the output is

connected to all inputs only indirectly, even when no neuron is directly connected to all inputs, like in

CNNs. This explanation of adversarial vulnerability is independent of the intrinsic dimensionality or

geometry of the data (compare to Amsaleg et al. 2017; Gilmer et al. 2018).

Incidentally, Goodfellow et al. (2014) also already relate adversarial vulnerability to large gradients of the loss L, an insight at the very heart of their FGSM-algorithm. They however do not propose any explicit penalizer on the gradient of L other than indirectly through adversarially-augmented training. Conversely, Ross & Doshi-Velez (2018) propose the old double-backpropagation to robustify networks but make no connection to FGSM and adversarial augmentation. Lyu et al. (2015) discuss and use the connection between gradient-penalties and adversarial augmentation, but never actually compare both in experiments. This comparison however is essential to test the validity of the firstorder Taylor expansion in (2), as confirmed by the similarity between the gradient-regularization and adversarial-augmentation curves in Figure 2. Hein & Andriushchenko (2017) derived yet another gradient-based penalty ­the cross-Lipschitz-penalty­ by considering (and proving) formal guarantees on adversarial vulnerability itself, rather than adversarial damage. While both penalties are similar in spirit, focusing on the adversarial damage rather than vulnerability has two main advantages. First, it achieves better accuracy-to-vulnerability ratios, both in theory and practice, because it ignores classswitches between misclassified examples and penalizes only those that reduce the accuracy. Second, it allows to deal with one number only, L, whereas Hein & Andriushchenko's cross-Lipschitz regularizer and theoretical guarantees explicitly involve all K logit-functions (and their gradients). See Appendix C. Penalizing network-gradients is also at the heart of contractive auto-encoders as proposed by Rifai et al. (2011), where it is used to regularize the encoder-features. Seeing adversarial training as a generalization method, let us also mention Hochreiter & Schmidhuber (1995), who propose to enhance generalization by searching for parameters in a "flat minimum region" of the loss. This leads to a penalty involving the gradient of the loss, but taken with respect to the weights, rather than the inputs. In the same vein, a gradient-regularization of the loss of generative models also appears in Proposition 6 of Ollivier (2014), where it stems from a code-length bound on the data (minimum description length). More generally, the gradient regularized objective (4) is essentially the first-order approximation of the robust training objective max   L(x + , c) which has a long history in math (Wald, 1945), machine learning (Xu et al., 2009) and now adversarial vulnerability (Sinha et al., 2018). Finally, Cisse et al. (2017) propose new network-architectures that have small gradients by design, rather than by special training: an approach that makes all the more sense, considering the conclusion of Theorems 4 and 5. For further details and references on adversarial attacks and defenses, we refer to Yuan et al. (2017).

6 CONCLUSION
For differentiable classifiers and losses, we showed that adversarial vulnerability increases with the gradients xL of the loss, which is confirmed by the near-perfect functional relationship between gradient norms and vulnerability (Figures 1&2d). We then evaluated the size of xL q and showed that usual feed-forward nets (convolutional or fully connected) are increasingly vulnerable to pattacks with growing input dimension d (the image-size), almost independently of their architecture. Our theorems rely on the statistical weight distribution at initialization, but our experiments confirm the conclusions also for the tested networks after training. Our results rely on a first order analysis that assumes a differentiable loss and architecture: they may not cover every aspect of adversarial vulnerability nor easily extend to non-differentiable structures (even though such structures are often smooth at a coarser scale). Nevertheless, they show that at least this type of first-order vulnerability is present, common, and firmly rooted in our current network architectures. They hence suggest to
8

Under review as a conference paper at ICLR 2019
tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques.
REFERENCES
Laurent Amsaleg, James E. Bailey, Dominique Barbe, Sarah Erfani, Michael E Houle, Vinh Nguyen, and Milos Radovanovic. The Vulnerability of Learning to Adversarial Perturbation Increases with Intrinsic Dimensionality. In WIFS, Rennes, France, 2017.
David Balduzzi, Brian McWilliams, and Tony Butler-Yeoman. Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks. arXiv:1611.02345, 2016.
Chris M. Bishop. Training with noise is equivalent to Tikhonov regularization. Neural computation, 7(1): 108­116, 1995.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In ICML, 2017.
Harris Drucker and Yann LeCun. Double backpropagation increasing generalization performance. In International Joint Conference on Neural Networks, 1991.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. Adversarial Spheres. arXiv:1801.02774, 2018.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples. arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectifiers: Surpassing HumanLevel Performance on ImageNet Classification. arXiv:1502.01852, 2015.
Matthias Hein and Maksym Andriushchenko. Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation. arXiv:1705.08475, 2017.
Sepp Hochreiter and Juergen Schmidhuber. Simplifying neural nets by discovering flat minima. In NIPS, 1995. Jinggang Huang. Statistics of Natural Images and Models. PhD thesis, Brown University, Providence, RI, 2000. Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Technical report, 2009. Chunchuan Lyu, Kaizhu Huang, and Hai-Ning Liang. A Unified Gradient Regularization Family for Adversarial
Examples. In ICDM, 2015. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: A Simple and Accurate
Method to Fool Deep Neural Networks. In CVPR, 2016. Radford M. Neal. Bayesian Learning for Neural Networks, volume 118 of Lecture Notes in Statistics. Springer
New York, New York, NY, 1996. Yann Ollivier. Auto-encoders: reconstruction versus compression. arXiv:1403.7752, 2014. Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox v0.8.0: A Python toolbox to benchmark the
robustness of machine learning models. arXiv:1707.04131, 2017. Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive Auto-encoders:
Explicit Invariance During Feature Extraction. In ICML, 2011. Andrew Slavin Ross and Finale Doshi-Velez. Improving the Adversarial Robustness and Interpretability of Deep
Neural Networks by Regularizing Their Input Gradients. In AAAI, 2018. Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled
adversarial training. In ICLR, 2018. Abraham Wald. Statistical decision functions which minimize the maximum risk. Annals of Mathematics, 46(2):
265­280, 1945. Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines.
JMLR, 10:1485­1510, 2009. Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, and Xiaolin Li. Adversarial Examples: Attacks and
Defenses for Deep Learning. arXiv:1712.07107, 2017.
9

Under review as a conference paper at ICLR 2019

A EFFECTS OF STRIDED AND AVERAGE-POOLING LAYERS ON ADVERSARIAL VULNERABILITY

It is common practice in CNNs to use average-pooling layers or strided convolutions to progressively

decrease the number of pixels per channel. Corollary 6 shows that using strided convolutions

does not protect against adversarial examples. However, what if we replace strided convolutions

by convolutions with stride 1 initialized weights with typical

plus size

1a/navine-rdaegger-epeo.oAlivnegralgaey-epro?oTlihnegosrheomwe5vceor ninstirdoedruscoendlyeterarmndinoimstliyc

weights of size 1/(in-degree). These are smaller and may therefore dampen the input-to-output

gradients and protect against adversarial examples. We confirm this in our next theorem, which uses

a slightly modified version (H ) of (H) to allow average pooling layers. (H ) is (H), but where the

He-init H3 applies to all weights except the (deterministic) average pooling weights, and where H1

places a ReLU on every non-input and non-average-pooling neuron.

Theorem 7 (Effect of Average-Poolings). Consider a succession of convolution layers, dense layers

and n average-pooling layers, in any order, that satisfies (H ) and outputs logits fk(x). Assume

the ...,

annanvoerdaegserepsopoelcintigvelalyy.erTshheanveaxsftkrid2eaenqdua|l xtofkt|hesciralmealsikkesi1z/eanad1

perform averages over · · · an and 1/ d a1 · · ·

a1, an

respectively.

Proof in Appendix B.4. Theorem 7 suggest to try and replace any strided convolution by its

non-strided counterpart, followed by an average-pooling layer. It also shows that if we system-

atically reduce the number of pixels per channel down to 1 by using only non-strided convolu-

tions and average-pooling layers become independent of d, thereby

(i.e. d = making the

nine=tw1 aori)k,

then all input-to-output gradients should completely robust to adversarial examples.

Our following experiments (Figure 4) show that

after training, the networks get indeed robusti-

fied to adversarial examples, but remain more 88.5 vulnerable than suggested by Theorem 7.

30

88.0

accuracy vulnerability

Experimental setup. Theorem 7 shows that, 87.5

25

contrary to strided layers, average-poolings should decrease adversarial vulnerability. We tested this hypothesis on CNNs trained on

87.0 average strided max

20
average strided max

CIFAR-10, with 6 blocks of `convolution  Figure 4: As predicted by Theorem 7, average-

BatchNorm ReLU' with 64 output-channels, pooling layers make networks more robust to ad-

followed by a final average pooling feeding one versarial examples, contrary to strided (and max-

neuron per channel to the last fully-connected pooling) ones. But the vulnerability with average-

linear layer. Additionally, after every second poolings remains higher than anticipated.

convolution, we placed a pooling layer with

stride and mask-size (2, 2) (thus acting on 2 × 2

neurons at a time, without overlap). We tested average-pooling, strided and max-pooling layers and

trained 20 networks per architecture. Results are shown in Figure 4. All accuracies are very close,

but, as predicted, the networks with average pooling layers are more robust to adversarial images

than the others. However, they remain more vulnerable than what would follow from Theorem 7. We

also noticed that, contrary to the strided architectures, their gradients after training are an order of

magnitude higher than at initialization and than predicted. This suggests that assumptions (H) get

more violated when using average-poolings instead of strided layers. Understanding why will need

further investigations.

B PROOFS
B.1 PROOF OF PROPOSITION 3 Proof. Let  be an adversarial perturbation with  = 1 that locally maximizes the loss increase at point x, meaning that  = arg max  1xL ·  . Then, by definition of the dual norm of xL we have: xL · ( ) = |||xL|||. Thus

10

Under review as a conference paper at ICLR 2019

L~

,

·

(x, c)

=

1 (L(x, c) + L(x + 2

, c)) = 1 (2L(x, c) + 2

|xL · | + o(  )) =

= L(x, c) + 2 |||xL||| + o( ) = L ,|||·|||(x, c) + o( ) .

B.2 PROOF OF THEOREM 4

Proof. Let x designate a generic coordinate of x. To evaluate the size of xL q, we will evaluate the size of the coordinates xL of xL by decomposing them into

xL =

K L k=1 fk

fk x

=:

K
kL xfk,
k=1

where fk(x) denotes the logit-probability of x belonging to class k. We now investigate the statistical properties of the logit gradients xfk, and then see how they shape xL.

Step 1: Statistical properties of xfk. Let P(x, k) be the set of paths p from input neuron x to output-logit k. Let p - 1 and p be two successive neurons on path p, and p~ be the same path p but without its input neuron. Let wp designate the weight from p - 1 to p and p be the path-product p := pp~ wp. Finally, let p (resp. p) be equal to 1 if the ReLU of node p (resp. if path p) is active for input x, and 0 otherwise. As previously noticed by Balduzzi et al. (2016) using the chain rule, we see that xfk is the sum of all p whose path is active, i.e. xfk(x) = pP(x,k) pp. Consequently:

EW, xfk(x)2 =

EW wp2 E p2

pP(x,k) pp~

=

|P(x, k)|

2

pp~ dp-1

1 2

=

pp~

dp

·

pp~

1 dp-1

=

1 d

.

(8)

The first equality uses H1 to decouple the expectations over weights and ReLUs, and then applies Lemma 10 of Appendix B.3, which uses H3-H5 to kill all cross-terms and take the expectation over weights inside the product. The second equality uses H3 and the fact that the resulting product is the same for all active paths. The third equality counts the number of paths from x to k and we conclude by noting that all terms cancel out, except dp-1 from the input layer which is d. Equation 8 shows that |xfk|  1/ d.

Step 2: Statistical properties of image x belonging to class k

of kL and according to

xL. Defining qk(x) the network), we have,

b:y=defiKhne=ift1kioe(xfnh) o(xf)

(the probability the cross-entropy

loss, L(x, c) := - log qc(x), where c is the label of the target class. Thus:

kL(x) =

-qk (x) 1 - qc(x)

if k = c otherwise,

and

xL(x) = (1 - qc) xfc(x) + qk (-xfk(x)).

(9)

k=c

Using again Lemma 10, we see that the xfk(x) are K centered and uncorrelated variables. So

x is

L(x) given

is approximately by (1 - qc)2 +

the sum of k=c qk2

K /d.

uHnceonrcreeltahteedmvaagrinaibtuledsewoifthxzLer(ox-m) eisan1,/andd

its total variance for all x, so the

q-norm of the full input gradient is d1/q-1/2. (6) concludes.

Remark 1. Equation 9 can be rewritten as

K
xL(x) = qk(x) xfc(x) - xfk(x) .

(10)

k=1

As the term k = c disappears, the norm of the gradients xL(x) appears to be controlled by the total error probability. This suggests that, even without regularization, trying to decrease the ordinary

11

Under review as a conference paper at ICLR 2019

classification error is still a valid strategy against adversarial examples. It reflects the fact that when increasing the classification margin, larger gradients of the classifier's logits are needed to push images from one side of the classification boundary to the other. This is confirmed by Theorem 2.1 of Hein & Andriushchenko (2017). See also (16) in Appendix C.

B.3 PROOF OF THEOREM 5

The proof of Theorem 5 is very similar to the one of Theorem 4, but we will need to first generalize the equalities appearing in (8). To do so, we identify the computational graph of a neural network to an abstract Directed Acyclic Graph (DAG) which we use to prove the needed algebraic equalities. We then concentrate on the statistical weight-interactions implied by assumption (H), and finally throw these results together to prove the theorem. In all the proof, o will designate one of the output-logits fk (x).
Lemma 8. Let x be the vector of inputs to a given DAG, o be any leaf-node of the DAG, x a generic coordinate of x. Let p be a path from the set of paths P(x, o) from x to o, p~ the same path without node x, p a generic node in p~, and dp be its input-degree. Then:

1 =1
xx p~P(x,o) pp~ dp

(11)

Proof. We will reason on a random walk starting at o and going up the DAG by choosing any

incoming node with equal probability. The DAG being finite, this walk will end up at an input-node

x with probability 1. Each path p is taken with probability

pp~

1 dp

.

And

the

probability

to

end

up at an input-node is the sum of all these probabilities, i.e. xx pP(x,o) pp dp-1, which concludes.

The sum over all inputs x in (11) being 1, on average it is 1/d for each x, where d is the total number of inputs (i.e. the length of x). It becomes an equality under assumption (S):

Lemma 9. Under the symmetry assumption (S), and with the previous notations, for any input

x  x:

11

=.

pP(x,o) pp~ dp

d

(12)

Proof. Let us denote D(x, o) := {dp}xP(x,o). Each path p in P(x, o) corresponds to exactly one element dp in D(x, o) and vice-versa. And the elements dp of dp completely determine the product
pp~ dp-1. By using (11) and the fact that, by (S), the multiset D(x, o) is independent of x, we hence conclude

11

=

xx pP(x,o) pp~ dp

xx dpD(x,o) dpdp dp

1 = d = 1.
dpD(x,o) dpdp dp

Now, let us relate these considerations on graphs to gradients and use assumptions (H). We remind that path-product p is the product pp~ wp.
Lemma 10. Under assumptions (H), the path-products p, p of two distinct paths p and p starting from a same input node x, satisfy:

EW [p p ] = 0 and EW p2 = EW wp2 .
pp~
Furthermore, if there is at least one non-average-pooling weight on path p, then EW [p] = 0.

12

Under review as a conference paper at ICLR 2019

Proof. Hypothesis H4 yields



EW p2 = EW  wp2 = EW wp2 .

pp~

pp~

Now, take two different paths p and p that start at a same node x. Starting from x, consider the first node after which p and p part and call p and p the next nodes on p and p respectively. Then the weights wp and wp are two weights of a same node. Applying H4 and H5 hence gives

EW [p p ] = EW p\p p \p EW [wp wp ] = 0 . Finally, if p has at least one non-average-pooling node p, then successively applying H4 and H3 yields: EW [p] = EW p\p EW [wp] = 0.

We now have all elements to prove Theorem 5.

Proof. (of Theorem 5) For a given neuron p in p~, let p - 1 designate the previous node in p of p. Let p (resp. p) be a variable equal to 0 if neuron p gets killed by its ReLU (resp. path p is inactive), and 1 otherwise. Then:

Consequently:

xo =

p-1 p =

p p

pP(x,o) pp~

pP (x,o)

EW, (xo)2 =

EW [p p ] E [pp ]

p,p P(x,o)

= EW p2 E p2
pP(x,o) pp~

(13)

21 1 = =,
pP(x,o) pp~ dp 2 d

where the firs line uses the independence between the ReLU killings and the weights (H1), the second

uses Lemma expectations

10 and the scale like

last uses Lemma 1/d. Thus each

c9o. oTrhdeingartaedsiecnaltesxloiktehu1s/hads

coordinates whose squared and xo q like d1/2-1/q.

Conclude on xL q and p xL q by using Step 2 of the proof of Theorem 4.

Finally, note that, even without the symmetry assumption (S), using Lemma 8 shows that

EW

xo

2 2

=

EW (xo)2

xx

21 = = 1.
xx pP(x,o) pp~ dp 2

Thus, with or without (S), xo 2 is independent of the input-dimension d.

B.4 PROOF OF THEOREM 7

To prove Theorem 7, we will actually prove the following more general theorem, which generalizes Theorem 5. Theorem 7 is a straightforward corollary of it.

Theorem 11. Consider any feed-forward network with linear connections and ReLU activation
functions that outputs logits fk(x) and satisfies assumptions (H). Suppose that there is a fixed multiset of integers {a1, . . . , an} such that each path from input to output traverses exactly n average pooling nodes with degrees {a1, . . . , an}. Then:

xfk 2 

1

n i=1

 ai

.

(14)

Furthermore,

if

the

net

satisfies

the

symmetry

assumption

(S ),

then:

|xfk|




d

1.

n i=1

ai

13

Under review as a conference paper at ICLR 2019

Two remarks. First, in all this proof, "weight" encompasses both the standard random weights, and the constant (deterministic) weights equal to 1/(in-degree) of the average-poolings. Second, assumption H5 implies that the average-pooling nodes have disjoint input nodes: otherwise, there would be two non-zero deterministic weights w, w from a same neuron that would hence satisfy: EW [w w ] = 0.

Proof. As previously, let o designate any fixed output-logit fk(x). For any path p, let a be the set of

average-pooling nodes of p and let q be the set of remaining nodes. Each path-product p satisfies:

p = qa, where a is a same fixed constant. For two distinct paths p, p , Lemma 10 therefore

yields: EW assumption

(S)p2,

= we

gea2t

sEimWilarlq2y

and EW to (13):

[pp

]

=

0.

Combining

this

with

Lemma

9

and

under

EW,

(xo)2

= aa EW [q q ] E [qq ]

p,p P(x,o)

n1

= pP(x,o) i=1 a2i

EW
qq~

q2

E

q2

n1

n1

21

=

i=1 ai pP(x,o) i=1 ai qq~ dq 2

(15)

Therefore, |xo| = |xfk|  1/ (15) and Lemma 8 shows that

safmoreavllaplue

1 pp~ dp

1n 1 =.
d i=1 ai

=

1 d

(Lemma 9)

d

n i=1

ai.

Again,

note

that,

even

without

assumption

(S ),

using

EW

xo

2 2

=

EW, (xo)2

xx

(=15) n 1

n1

21

xx i=1 ai pP(x,o) i=1 ai pp~ dp 2

n1

1 n1

= =,

i=1 ai xx pP(x,o) pp~ dp i=1 ai

which proves (14).

=1 (Lemma 8)

C COMPARISON TO THE CROSS-LIPSCHITZ REGULARIZER

In their Theorem 2.1, Hein & Andriushchenko (2017) show that the minimal =  p perturbation to fool the classifier must be bigger than:

min
k=c

fc(x) - fk(x) maxyB(x, ) xfc(y) - xfk(y)

q

.

(16)

They argue that the training procedure typically already tries to maximize fc(x) - fk(x), thus one only needs to additionally ensure that xfc(x) - xfk(x) q is small. They then introduce what they call a Cross-Lipschitz Regularization, which corresponds to the case p = 2 and involves the gradient differences between all classes:

RxLip

:=

1 K2

K

xfh(x) - xfk(x)

2 2

k,h=1

(17)

In contrast, using (10), (the square of) our proposed regularizer xL q from (4) can be rewritten, for p = q = 2 as:

14

Under review as a conference paper at ICLR 2019

K

R

·

(f )
2

=

qk(x)qh(x) xfc(x) - xfk(x) ·

k,h=1

· xfc(x) - xfh(x) (18)

Although both (17) and (18) consist in K2 terms, corresponding to the K2 cross-interaction between the K classes, the big difference is that while in (17) all classes play exactly the same role, in (18) the summands all refer to the target class c in at least two different ways. First, all gradient differences are always taken with respect to xfc. Second, each summand is weighted by the probabilities qk(x) and qh(x) of the two involved classes, meaning that only the classes with a non-negligible probability get their gradient regularized. This reflects the idea that only points near the margin need a gradient regularization, which incidentally will make the margin sharper.

D PERCEPTION THRESHOLD

To keep the average pixel-wise variation constant across dimensions d, we saw in (3) that the threshold p of an p-attack should scale like d1/p. We will now see another justification for this scaling. Contrary to the rest of this work, where we use a fixed p for all images x, here we will let p depend on the 2-norm of x. If, as usual, the dataset is normalized such that the pixels have on average variance 1, both approaches are almost equivalent.

Suppose that given an p-attack norm, we want to choose p such that the signal-to-noise ratio (SNR)

x For

2
p

/ =

yields

 2

t2hiosfiamppeorsteusrba2ti=on



with x 2.

p-norm  p is never greater More generally, studying the

than a given inclusion of

SNR threshold 1/ . p-balls in 2-balls

p = x 2 d1/p-1/2 .

(19)

Note that this gives again p = d1/p. This explains how to adjust the threshold with varying p-attack norm.

Now, let us see how to adjust the threshold of a given p-norm when the dimension d varies. Suppose that x is a natural image and that decreasing its dimension means either decreasing its resolution or

cropping it. Because the statistics of natural images are approximately resolution and scale invariant

(Huang, 2000), in which implies that

exithe2rsccaasleestlhikeeavedra. gPeasstqinugartehdisvbaalcuke

of the image pixels remains into (19), we again get:

unchanged,

p =  d1/p . In particular,   is a dimension-free number, exactly like in (3) of the main part. Now, why did we choose the SNR as our invariant reference quantity and not anything else? One reason is that it corresponds to a physical power ratio between the image and the perturbation, which we think the human eye is sensible to. Of course, the eye's sensitivity also depends on the spectral frequency of the signals involved, but we are only interested in orders of magnitude here.

Another 1/ . For

point: any image x yields 2-attacks, this inequality

an adversarial perturbation x, where by constraint x is actually an equality. But what about other p-attacks:

(2o/n

x  average

over x,) how far is the signal-to-noise ratio from its imposed upper bound 1/ ? For p  {1, 2, },

the answer unfortunately depends on the pixel-statistics of the images. But when p is 1 or , then

the situation is locally the same as for p = 2. Specifically:

Lemma 12. Let x be a given input and > 0. Let p be the greatest threshold such that for any  with  p  p, the SNR x 2 /  2 is  1/ . Then p = x 2 d1/p-1/2.

Moreover, for p  {1, 2, }, if x is the p-sized p-attack that locally maximizes the loss-increase i.e. x = arg max  p p |xL · |, then:

SNR(x) :=

x

2

1 =

x 2

and

1 Ex [SNR(x)] = .

Proof. The first paragraph follows from the fact that the greatest p-ball included in an 2-ball of radius x 2 has radius x 2 d1/p-1/2.

15

Under review as a conference paper at ICLR 2019

wwThhheiiccshhecssoaatntiidssfifipeeassr::agraxpxh22is==cle2a/rfoddr =p==

2. For x 2. x 2.

p= For

, it p=

follows from the fact that x =  1, it is because x = 1 maxi=1..d

sign xL |(xL)i|,

Intuitively, this means that for p  {1, 2, }, the SNR of p-sized p-attacks on any input x will be exactly equal to its fixed upper limit 1/ . And in particular, the mean SNR over samples x is the same (1/ ) in all three cases.

E FIGURES WITH AN 2 PERTURBATION-THRESHOLD AND DEEP-FOOL ATTACKS

Here we 0.005 d

plot the same instead of the

curves as in the main part, -threshold and deep-fool

but using an 2-attack threshold of size 2 = attacks (Moosavi-Dezfooli et al., 2016) instead

of iterative rescaled by

d-otonesstaiyn

Figs. 6 and 7. Note that contrary consistent across dimensions (see

to -thresholds, 2-thresholds must be Eq.3 and Appendix D). All curves look

essentially the same as their counterparts in the main text.

% fooled ( adv. vuln.)

80 -attacks
2-attacks deep-fool
60 iterative- 
iterative- 2
40

20 -4.0

-3.5 (a-)3.0log10 -2.5

-2.0

50 100 150 200 (b) Ex xL 1

Figure 5: Same as Figure 1 but using an 2 threshold instead of a  one. Now the 2-based methods (deep-fool, and single-step and iterative 2-attacks) seem more effective than the  ones.

Ex xL 1

200 150 100 50
-4.0 -3.5 -3.0 -2.5 -2.0
(a) log10 ( d1/p)

adversarial vulnerability

60 40
-4.0 -3.5 -3.0 -2.5 -2.0
(b) log10 ( d1/p)

accuracy

85

80

75

70

-4.0 85

-3.5 -3.0 -2.5 -2.0
(c) log10 ( d1/p)

adversarial vulnerability

accuray

Ex xL 2

80 60 4

75

Grad Regu q = 1 Grad Regu q = 2

40

2

Adv Train p = 
70 Adv Train p = 2

Cross-Lipschitz

50 100 150 200
(d) Ex xL 1

50 100 150 200
(e) Ex xL 1

65 40

60

(f) adversarial vulnerability

Figure 6: Same as Figure 2, but with an 2- perturbation-threshold (instead of ) and deep-fool attacks (Moosavi-Dezfooli et al., 2016) instead of iterative  ones. All curves look essentially the same than in Fig. 2.

F A VARIANT OF ADVERSARIALLY-AUGMENTED TRAINING
In usual adversarially-augmented training, the adversarial image x +  is generated on the fly, but is nevertheless treated as a fixed input of the neural net, which means that the gradient does not

16

Under review as a conference paper at ICLR 2019

vulnerability Ex xL 1

100 30 80

20 60

10

32 64 128
image-width



 256 d

40

32 64 128
image-width



 256 d

Figure 7: Same as Figure 3 but with an 2 perturbation-threshold (instead of an  one) and using deep-fool (instead of iterative- ) attacks to approximate adversarial vulnerability.

get backpropagated through . This need not be. As  is itself a function of x, the gradients could actually also be backpropagated through . As it was only a one-line change of our code, we used this opportunity to test this variant of adversarial training (FGSM-variant in Figure 2) and thank Martín Arjovsky for suggesting it. But except for an increased computation time, we found no significant difference compared to usual augmented training.

17

