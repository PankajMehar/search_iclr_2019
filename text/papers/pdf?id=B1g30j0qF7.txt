Under review as a conference paper at ICLR 2019
BAYESIAN CONVOLUTIONAL NEURAL NETWORKS WITH MANY CHANNELS ARE GAUSSIAN PROCESSES
Anonymous authors Paper under double-blind review
ABSTRACT
There is a previously identified equivalence between wide fully connected neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for instance, test set predictions that would have resulted from a fully Bayesian, infinitely wide trained FCN to be computed without ever instantiating an FCN, but by instead evaluating the corresponding GP. In this work, we derive an analogous equivalence for multi-layer convolutional neural networks (CNNs) both with and without pooling layers. Surprisingly, in the absence of pooling layers, the corresponding GP is identical for CNNs with and without weight sharing. This means that translation equivariance in SGD-trained finite CNNs has no corresponding property in the Bayesian treatment of the infinite width limit ­ a qualitative difference between the two regimes that is not present in the FCN case. We confirm experimentally that in some scenarios, while the performance of trained finite CNNs becomes similar to that of the corresponding GP with increasing channel count, with careful tuning SGD-trained CNNs can significantly outperform their corresponding GPs. Finally, we introduce a Monte Carlo method to estimate the GP corresponding to a NN architecture, even in cases where the analytic form has too many terms to be computationally feasible.
1 INTRODUCTION
Neural networks (NNs) demonstrate remarkable performance (He et al., 2016; Oord et al., 2016; Silver et al., 2017; Vaswani et al., 2017), but are still only poorly understood from a theoretical perspective (Goodfellow et al., 2014; Choromanska et al., 2015; Pascanu et al., 2014; Zhang et al., 2017). NN performance is often motivated in terms of model architectures, initializations, and training procedures together specifying biases, constraints, or implicit priors over the class of functions learned by a network. This induced structure in learned functions is believed to be well matched to structure inherent in many practical machine learning tasks, and in many real-world datasets. For instance, properties of NNs which are believed to make them well suited to modeling the world include: hierarchy and compositionality (Lin et al., 2017; Poggio et al., 2017), Markovian dynamics (Tino et al., 2004; 2007) , and equivariances in time and space for RNNs (Werbos, 1988) and CNNs (Fukushima & Miyake, 1982; Rumelhart et al., 1985) respectively.
The recent discovery of an equivalence between deep neural networks and GPs (Lee et al., 2018; de G. Matthews et al., 2018) allow us to express an analytic form for the prior over functions encoded by deep NN architectures and initializations. This transforms an implicit prior over functions into an explicit prior, which can be analytically interrogated and easily reasoned about. Previous work studying these Neural Network-equivalent Gaussian Processes (NN-GPs) has established the correspondence only for fully connected networks (FCNs). Additionally, previous work has not used analysis of NN-GPs to gain specific insights into the equivalent NNs.
In the present work, we extend the equivalence between NNs and NN-GPs to deep Convolutional Neural Networks (CNNs), both with and without pooling. CNNs are a particularly interesting architecture for study, since they are frequently held forth as a success of motivating NN design based on invariances and equivariances of the physical world (Cohen & Welling, 2016) ­ specifically, designing a NN to respect translation equivariance (Fukushima & Miyake, 1982; Rumelhart et al., 1985). As we will see in this work, absent pooling, this quality can vanish in the Bayesian treatment of the infinite width limit.
1

Under review as a conference paper at ICLR 2019

The specific novel contributions of the present work are:
1. We show analytically that CNNs with many channels, both with and without average pooling, trained in a fully Bayesian fashion, correspond to an NN-GP (§2, §3). We prove convergence as the number of channels in hidden layers go to infinity uniformly (§A.4.3), strengthening and extending the result of de G. Matthews et al. (2018).
2. We show that in the absence of pooling, the NN-GP for a CNN and a Locally Connected Network (LCN) are identical (§5.1). An LCN has the same local connectivity pattern as a CNN, but without weight sharing or translation equivariance.
3. We experimentally compare trained CNNs and LCNs and find that under certain conditions both often perform similarly to the respective NN-GP (Figures 5, 6). Moreover, both architectures tend to perform better with increased channel count, suggesting that similarly to FCNs (Neyshabur et al., 2015; Novak et al., 2018) CNNs benefit from overparameterization (Figures 1, 5).

Validation Accuraccy

CNN with pooling

0.85 0.8
0.75 0.7

0.65

6 7 89
100

2 3 4 5 6 7 89
1000

Number of Filters

Figure 1: SGD-trained CNNs with pooling generally benefit from overparameterization. Each line corresponds to a particular setting of hyper-parameters, with best learning rate and weight decay selected independently for each number of filters (x-axis). All networks have reached 100% training accuracy on CIFAR10. Best achieved generalization (y-axis) increases as the number of free parameters grows. See §A.6.5 for experimental details.

4. However, we also show that careful tuning of hyper-parameters allows finite CNNs trained with SGD to outperform their corresponding NN-GP by a significant margin. We experimentally disentangle and quantify the contributions stemming from local connectivity, equivariance, and invariance in a convolutional model in one such setting (Table 2), and suggest a potential explanation for the observed difference in performance (§5.3).
5. We introduce a Monte Carlo method to compute NN-GP kernels for situations (such as CNNs with pooling) where evaluating the NN-GP is otherwise computationally infeasible (§4).
1.1 RELATED WORK
In early work on neural network priors, Neal (1994) demonstrated that, in a fully connected network with a single hidden-layer, certain natural priors over network parameters give rise to a Gaussian process prior over functions when the number of hidden units is taken to be infinite. Followup work extended the conditions under which this correspondence applied (Williams, 1997; Le Roux & Bengio, 2007; Hazan & Jaakkola, 2015). An exactly analogous correspondence for infinite width, finite depth deep fully connected networks was developed recently (Lee et al., 2018; de G. Matthews et al., 2018).
The line of work examining signal propagation in random deep networks (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Xiao et al., 2018; Chen et al., 2018) is related to the construction of the GPs we consider. They apply a mean-field approximation in which the preactivation signal is replaced with a Gaussian, and the derivation of the correlation function with depth gives the kernel function of the corresponding GP. Recently, Xiao et al. (2018) extended this to convolutional architectures without pooling. Xiao et al. (2018) also analyzed properties of the convolutional kernel at large depths to construct a phase diagram which will be relevant to NN-GP performance, as discussed in §A.3. Compositional kernels coming from convolutional and fully connected layers also appeared outside of the GP context in (Daniely et al., 2016). In this work, they prove approximation guarantees between a network and its corresponding kernel, and show that empirical kernels will converge as the number of channels increases.

2

Under review as a conference paper at ICLR 2019

There is a line of work considering stacking of GPs, such as deep GPs (Lawrence & Moore, 2007; Damianou & Lawrence, 2013; Kumar et al., 2018). These no longer correspond to GPs, though they can describe a rich class of probabilistic models beyond GPs. Alternatively, deep kernel learning (Wilson et al., 2016b;a; Bradshaw et al., 2017) utilizes GPs with base kernels which take in features produced by a deep neural network (often a CNN), and train the resulting model end-toend. Similarly, van der Wilk et al. (2017) incorporates convolutional structure into GP kernels. Our work differs from all of these in that our GP corresponds exactly to a fully Bayesian CNN in the many channel limit.
Borovykh (2018) also studies CNNs converging to GP behavior. However this work considers the infinite filter size limit, and works with CNNs over temporal data. They show that network outputs at different time points are described by a GP after marginalization over an input distribution. Thus they do not address the distribution over network outputs for specific inputs.
In concurrent work Garriga-Alonso et al. (2018) derive an NN-GP kernel equivalent to one of the kernels considered in our work. In addition to explicitly specifying kernels corresponding to pooling and vectorizing, we also compare the NN-GP performance to finite-width SGD-trained CNNs and analyze the differences between the two models.

2 WIDE BAYESIAN CNNS ARE GAUSSIAN PROCESSES

2.1 PRELIMINARIES

Consider a series of L convolutional hidden layers, l = 0, ..., L - 1. The parameters of the network are the convolutional filters and biases, ilj, and bli, respectively, with outgoing (incoming) channel index i (j) and filter relative spatial location  = -k, ..., k.1 Assume a Gaussian prior on both the
filter weights and biases,

p ilj, = N

0,

v

w2 nl

,

p bil = N 0, b2 .

(1)

The weight and bias variance are w2 , b2, respectively. nl is the number of channels (filters) in layer l, 2k + 1 is the filter size, and v is the fraction of the receptive field variance at location  (with  v = 1). Typically in practice we utilize uniform v = 1/(2k + 1), but nonuniform v = 1/(2k + 1) enables kernel properties that are better suited for ultra-deep networks, as in (Xiao et al., 2018).

Let X denote a set of input images (training set or validation set or both). The network has acti-
vations xl(x) and pre-activations zl(x) for each input image x  X  Rn0×d, with input channel count n0, number of pixels d, and where

xil,(x) =

xi,  zil,-1(x)

l=0 l>0 ,

nl k

zil,(x) =

ilj, xlj,+ (x) + bli.

j=1 =-k

(2)

We emphasize the dependence of the quantities on the input x.  is a pointwise nonlinearity . xl is assumed to be zero padded.

A recurring quantity in this work will be the empirical covariance functions of the activations xl, defined as

Kl ,

(x, x

)



1 nl

nl

xil,(x)xil, (x ).

i=1

(3)

K is therefore a 4-dimensional random variable indexed by two inputs x, x and two top-layer spatial locations ,  (the dependence on layer widths n1, . . . , nl and their weights and biases is implied and by default not stated explicitly). K0, the empirical covariance of inputs, is deterministic. In
obtaining Bayesian predictions from the GP, x and x will be examples from the training or test set.

1We will use Roman letters to index channels and Greek letters for spatial location. We use letters i, j, i , j , etc to denote channel indices, ,  , etc to denote spatial indices and ,  , etc the filter indices. For notational simplicity, we treat the 1D case with spatial dimension d in the text, but the single spatial index can be extended to higher dimensions by replacing with tuples.

3

Under review as a conference paper at ICLR 2019

2.2 CORRESPONDENCE BETWEEN GAUSSIAN PROCESSES AND PRIORS OVER DEEP CNNS
WITH INFINITELY MANY CHANNELS
We next consider the prior over functions computed by a CNN in the limit of infinitely many channels in the hidden (excluding input and output) layers, nl   for 1 l L, and derive its equivalence to a GP with a compositional kernel. The following section gives a proof which uses the empirical covariance functions K to characterize finite width intermediate layers and relies on explicit marginalization over hidden layers. In Appendix A.4 we give several alternative derivations of the correspondence.

2.2.1 A SINGLE CONVOLUTIONAL LAYER IS A GP CONDITIONED ON THE SECOND MOMENT TENSOR OF THE PREVIOUS LAYER'S ACTIVATIONS

As can be seen in Equation 2, the pre-activations zl are weighted sums of the Gaussian entries in l and bl, where the previous layer activations xl-1 provide the weighting factors. A weighted sum of Gaussian random variables is itself Gaussian. Specifically,

p zl|xl = p zil|xl = N zil; 0, A Kl ,
ii

(4)

where the first equality in Equation 4 follows from the independence of the weights and biases for each channel i. The covariance matrix A Kl for the pre-activations zl is derived in (Xiao et al., 2018), where A is an affine transform (a cross-correlation operator followed by a shifting operator)
and has functional form:

[A (K)], (x, x )  v(w2 [K]+, + (x, x ) + b2).


(5)

This result is exact due to the choice of a Gaussian prior over weights and biases.

2.2.2 SECOND MOMENT MATRIX BECOMES DETERMINISTIC WITH INCREASING CHANNEL
COUNT

The summands in Equation 3 are i.i.d., due to the independence of the weights and biases for each channel i. Subject to weak restrictions on the nonlinearity , we can apply the law of large number and conclude that,

Kl-1 lim p Kl|Kl-1 =  Kl - (C  A) Kl-1
nl 

in probability,

(6)

where C is defined as
E[C (K)], (x, x )  uN (0,K) [ (u(x))  (u (x ))] .

(7)

For nonlinearities such as ReLU (Nair & Hinton, 2010) and the error function (erf) C can be computed in closed form as derived in (Cho & Saul, 2009) and (Williams, 1997) respectively.

2.2.3 BAYESIAN MARGINALIZATION OVER ALL HIDDEN LAYERS
The distribution over the CNN outputs zL can be evaluated by marginalizing over all intermediate layer covariances in the network (see Figure 2):

p zL|x0 = dK0 · · · dKLp zL, K0 . . . KL|x0

(8)

L
= dK0 · · · dKLp K0|x0 p Kl|Kl-1 p zL|KL .
l=1

(9)

x0

K0

z0

K1

···

zL-2

K L-1

zL-1

KL

zL

Figure 2: Graphical model for computation performed by neural network (Lee et al., 2018). Per Equation 4 p zl|xl only depend on the empirical covariance Kl.

4

Under review as a conference paper at ICLR 2019

In the limit of infinitely many channels in the hidden layers, min{n1, . . . , nL}  2 , all the conditional distributions except for p zL|KL converge weakly to delta functions and can be integrated
out. Then, as elaborated in Appendix A.4 Theorem A.2, Equation 9 reduces to

lim p zL|x0 =
min{n1 ,...,nL }

N ziL; 0, A KL , where KL  (C  A)L K0

i

(10)

i.e. (C  A) composed with itself l times and applied to K0. In other words, KL is the (deterministic) covariance of the CNN activations in the limit of infinitely-many (hence  subscript) channels in each of the convolutional layers from 0 to L - 1.

Equation 10 states that the outputs for any set of input examples and pixel indices are jointly Gaus-
sian distributed ­ i.e. the output of a CNN with infinitely many channels in its L hidden layers is described by a GP with a covariance function A KL .

3 TRANSFORMING A GP OVER SPATIAL LOCATIONS INTO A GP OVER
CLASSES

In §2.2 we have shown that a deep CNN is a GP indexed by input samples and spatial locations of the top layer. In the infinite channel limit its covariance tensor KL can be computed in closed form.
Here we show that transformations to obtain class predictions that are common in CNN classifiers can be represented as either vectorization or projection (as long as we treat classification as regression, similarly to (Lee et al., 2018)). Both of these operations preserve the GP equivalence and allow the computation of the covariance tensor K of the respective GP (now indexed by input samples and target classes) as a simple transformation of KL .

3.1 VECTORIZATION

One popular way is to vectorize (flatten) the output of the last convolutional layer into a vector vec zL  RnL+1d and stack a fully connected layer on top:

nL+1 d

ziL+1 =

iLj+1 vec zL

j=1

j + biL+1,

(11)

where the weights L+1  Rc×nL+1d and biases bL+1  Rc are i.i.d. Gaussian, iLj+1 

N (0, 2 /(nL+1d)), biL+1  class i) of this particular GP,

N (0, b2). denoted by

The sample-sample GPvec, is

kernel

of

the

output

(identical

for

each

EKvec KL+1 =

ziL+1T ziL+1 KL+1

= 2 d

KL+1 , + b2,



And the limit of infinite width is derived identically to §2.2:

(12)

Kvec



lim
min{n1 ,...,nL }

Kvec x0

= 2 d



KL+1 , + b2.

(13)

As observed in Xiao et al. (2018), to compute any diagonal terms of KL+1 (x, x ), one needs only the corresponding diagonal terms of Kl (x, x ). Consequently, we only need to store

Kl

, (x, x ) : x, x

 X 2,   {1 . . . d}

and the memory cost is O
l=0,...,L

|X |2d .

Note

that this approach ignores pixel-pixel covariances and produces a GP corresponding to a locally-

connected network (see §5.1).

2Unlike de G. Matthews et al. (2018), we do not require each nl to be strictly increasing.

5

Under review as a conference paper at ICLR 2019

3.2 PROJECTION

Another approach is to project a multidimensional Gaussian into a 1-dimensional one. Let v  Rd
be a deterministic vector, iLj+1  N (0, 2 /c) for 1  j  c, bL+1 be the same as above. Define the output to be

nL+1

ziL+1 =

iLj+1  zL v j + bLi +1.

(14)

j=1

Any choice of v induces a GP whose sample-sample kernel is

Kv  2

vv KL+1 , + b2,

,

(15)

where the limiting behavior is derived identically to Equations 12 and 13. Examples of this approach include

1.

Average pooling:

take v

=

1 d

1d

and denote this particular GP as GPpool.

Then

Kpool



2 d2

KL+1 , + b2.

,

(16)

This approach corresponds to using a global average pooling right after the last convolution
layer. Spatially local average pooling layers can be constructed in a similar fashion. This
approach takes all pixel-pixel covariance into consideration and makes the kernel translation invariant. However, it requires O |X |2d2 memory to compute the sample-sample covariance of the GP. It is impractical to use this method to evaluate the GP, and we propose to use a Monte Carlo approach (see §4).

2. Subsampling one particular pixel: take v = e,

Ke  2 KL+1 , + b2.

(17)

This approach (denoted GPe ) makes use of only one pixel-pixel covariance, and requires

the same amount of memory as GPvec to compute.

We compare the performance of presented strategies in Figure 3. Note that all described strategies admit stacking additional FC layers on top while retaining the GP equivalence, using the recursion relationship in (Lee et al., 2018; de G. Matthews et al., 2018).

0.5 MC-CNN-GP with pooling

Accuracy

CNN-GP
0.4 CNN-GP (center pixel)

0.3 CNN-GP (without padding)
FCN-GP 0.2

12

5 10 2

5 100

Depth

Figure 3: Different dimensionality collapsing strategies described in §3. Validation accuracy of an MC-CNN-GP with pooling (item 3.2.1) is consistently better than other models due to translation invariance of the kernel. CNN-GP with zero padding (§3.1) outperforms an analogous CNN-GP without padding as depth increases. At depth of 15 the spatial dimension of the output without padding is reduced 1 × 1, making the CNN-GP without padding equivalent to the center pixel selection strategy (item 3.2.2) ­ which performs worse than CNN-GP (we conjecture, due to over-
fitting to centrally-located features) but approaches the latter (right) in the limit of large depth, as
information becomes more uniformly spatially distributed (Xiao et al., 2018). CNN-GPs generally
outperform FCN-GP, presumably due to the local connectivity prior, but can fail to capture nonlin-
ear interactions between spatially-distant pixels at shallow depths (left). Values are reported on a 2K/4K train/validation subsets of CIFAR10. See §A.6.2 for experimental details.

6

Under review as a conference paper at ICLR 2019

4 MONTE CARLO EVALUATION OF INTRACTABLE GP KERNELS

We introduce a Monte Carlo estimation method for NN-GP kernels which are computationally impractical to compute analytically, or for which we do not know the analytic form. Similar in spirit to traditional random feature methods (Rahimi & Recht, 2007), the core idea is to instantiate many random finite width networks and use the empirical second moments of activations to estimate the Monte Carlo-GP (MC-GP) kernel,

Knl ,M

,

(x, x ) 

1 Mn

M

n
xlc (x; m) xlc (x ; m)

m=1 c=1

(18)

where  consists of M draws of the weights and biases from their prior distribution, m  p (), and
n is the width or number of channels in hidden layers. The MC-GP kernel converges to the analytic kernel with increasing width, limn Knl ,M = Kl in probability.

For finite width networks, the uncertainty in Knl ,M is Var[Knl ,M ] = Var Knl () /M . From

Daniely et al. (2016), we know that Var

Knl ()



1 n

,

which

leads

to

Var[Knl ,M ]



1 Mn

.

For

finite n, Knl ,M is also a biased estimate of Kl , where the bias depends solely on network width.

We do not currently have an analytic form for this bias, but we can see in Figures 4 and 9 that for

the hyperparameters we probe it is small relative to the variance. In particular,

Knl ,M () - KL

2 F

is nearly constant for constant M n. We thus treat M n as the effective sample size for the Monte

Carlo kernel estimate. Increasing M and reducing n can reduce memory cost, though potentially at

the expense of increased compute time and bias.

The MC-GP reduces the memory requirements to compute GPpool from O |X |2 d2 to

O |X |2 + n2 + nd , making the evaluation of CNN-GPs with pooling practical.

10

0.42 10

0.5

MC-CNN-GP log2(Filters) Validation Accuracy log2(Filters) log10(Kernel Distance)

0 0

log2(Samples)

10

0.1 0
0

log2(Samples)

10

-6.5

Figure 4: Validation accuracy (left) of an MC-CNN-GP increases with n × M (i.e. filter count
times number of samples) and approaches that of the exact CNN-GP (not shown), while the distance (right) to the exact kernel decreases. The dark band corresponds to ill-conditioning of KnL,M when the number of outer products contributing to KnL,M approximately equals its rank. Values reported for a 3-layer model on a 2K/4K train/validation subset of CIFAR10 downsampled to 8 × 8. See Figure 9 for similar results with other architectures and §A.6.3 for experimental details.

5 DISCUSSION
5.1 BAYESIAN CNNS WITH MANY CHANNELS ARE IDENTICAL TO LOCALLY CONNECTED NETWORKS, IN THE ABSENCE OF POOLING
Locally Connected Networks (LCNs) (Fukushima, 1975; Lecun, 1989) are CNNs without weight sharing between spatial locations. LCNs preserve the connectivity pattern, and thus topology, of a CNN. However, they do not possess the equivariance property of a CNN ­ if an input is translated, the latent representation in an LCN will be completely different, rather than also being translated.
7

Under review as a conference paper at ICLR 2019

The CNN-GP predictions without spatial pooling in §3.1 and §3.2 depend only on sample-sample

covariances, and do not depend on pixel-pixel covariances. LCNs destroy pixel-pixel covariances:

KL

LCN ,

(x, x

)

=

0, for 

=



and all (x, x )



X 2 and L

>

0.

However, LCNs preserve

the correlations between input examples at every pixel:

KL

LCN ,

(x,

x

)

=

KL

CNN ,

(x,

x

).

As

a result, in the absence of pooling, LCN-GPs and CNN-GPs are identical. Moreover, LCN-GPs

with pooling are identical to CNN-GPs with vectorization of the top layer (under suitable scaling of xL+1). We confirm these findings experimentally in trained networks in the limit of large width in

Figure 5, as well as by demonstrating convergence of MC-GPs of the respective architectures to the

same CNN-GP (modulo scaling of xL+1) in Figures 4 and 9.

LCN

No Pooling

0.35 0.3
0.25 0.2
0.15 5 10 2

5 100 2

5 1000 2

5

Accuracy

Global Average Pooling

0.35 0.3
0.25 0.2
0.15 5 10 2

5 100 2

5 1000 2

5

Validation Accuracy
0.7 0.6

CNN-GP

CNN

0.35 0.3
0.25 0.2
0.15 5 10 2

5 100 2

5 1000 2

5

Accuracy

0.35 0.3
0.25 0.2
0.15 5 10 2

5 100 2

5 1000 2

5

0.5 0.4 0.3

Number of Filters

0.4 0.6

Figure 5: Best validation accuracy of trained neural networks

Best CNN

fitting the whole training set approaches that of a respective Figure 6: CNN-GP can often out-

CNN-GP as the number of channels increases. Notice that un- perform the best trained CNN of

less a network has both weight sharing and pooling (bottom- the same architecture that fit the

right), performance of all models converges to the same CNN- training set. Each point corre-

GP kernel (modulo a scaling term on the final layer activations sponds to the validation accuracy xL+1) performance. Translation invariance is only enabled of: (y-axis) a specific CNN-GP;

with both weight sharing and pooling (bottom-right), allowing (x-axis) the best CNN with the

for an improved performance of both CNNs and the respective same parameters selected among

CNN-GP. Values are reported for 3-layer models on a 0.5K/4K the 100%-accurate models on the train/validation subsets of CIFAR10 downsampled to 8×8. All full training CIFAR10 dataset with

networks have a 100% training accuracy. See Figures 8 and 7 different learning rates, weight de-

for training and validation loss, and §A.6.1 for experimental cay and number of filters. See

details.

§A.6.5 for experimental details.

5.2 POOLING LEVERAGES EQUIVARIANCE TO PROVIDE INVARIANCE

The only kernel leveraging pixel-pixel covariances is that of the CNN-GP with pooling. This enables the predictions of this GP and the corresponding CNN to be invariant to translations (modulo edge effects) ­ a clearly beneficial quality for an image classifier. We observe strong experimental evidence supporting the benefits of invariance throughout this work (Figures 3, 4, 5; Tables 1 and 2), in both CNNs and CNN-GPs.
5.3 FINITE-CHANNEL SGD-TRAINED CNNS CAN OUTPERFORM INFINITE-CHANNEL BAYESIAN CNNS, IN THE ABSENCE OF POOLING

Absent pooling benefits of equivariance and weight sharing are more challenging to explain in terms of priors on predictions made with image classifiers (since it is not a property of outputs, but of the intermediary representations). Indeed, in this work we find that CNN-GPs can often perform competitively to their finite-width SGD-trained counterparts (Figures 5, 6)3.
However, as can be seen in Table 1 and Figure 6, the best CNN overall outperforms the best CNNGP by a significant margin - an observation specific to CNNs and not FCNs or LCNs4. In Table 2 we
3This observation is conditioned on the respective NN fitting the training set to 100%. Underfitting breaks the correspondance to an NN-GP, since train set predictions of such a network no longer correspond to the true training labels.
4While we did not perform as extensive an evaluation of these models as of CNNs in this work, we have not observed these architectures to outperform their Bayesian infinite width limits by a substantial margin.

8

Under review as a conference paper at ICLR 2019

demonstrate this large gap in performance by evaluating different models with equivalent settings that are chosen for good SGD-trained CNN performance.
We conjecture that equivariance, a property lacking in LCNs and the Bayesian treatment of the infinite width CNN limit, contributes to the performance of SGD-trained finite-width CNNs with the correct settings of hyper-parameters. Indeed, notice that in the context of gradient optimization, weight sharing is easier to relate to the outputs of the classifier. It is straightforward to demonstrate that through equivariance of internal representations, weight sharing enforces equivariance of gradients5 (modulo edge effects). There is no corresponding qualitative gradient property of an FCN or an LCN (with or without pooling), which is consistent with best SGD-trained networks of these architectures performing similarly to NN-GPs.
Nonetheless, more work is needed to disentangle and quantify the separate contributions of stochastic optimization and infinite width limit to provide a comprehensive explanation.

Model
CNN with pooling
CNN with ReLU and large learning rate CNN-GP CNN with small learning rate CNN with erf (any learning rate)
Convolutional GP (van der Wilk et al., 2017) Deep Convolutional GP (Kumar et al., 2018) ResNet GP (Garriga-Alonso et al., 2018) Residual CNN-GP (Garriga-Alonso et al., 2018) CNN-GP (Garriga-Alonso et al., 2018)
FCN-GP FCN-GP (Lee et al., 2018) FCN

CIFAR10
14.85 (15.65)
24.76 (17.64) 32.86 33.31(22.89) 33.31(22.17)
35.40 45.00 - - -
41.06 44.34 45.52 (44.73)

MNIST
-
- 0.88 - -
1.17 1.34 0.84 0.96 1.03
1.22 1.21 -

Fashion-MNIST
-
- 7.40 - -
- - - - -
8.22 - -

Table 1: Test error (%) of the different models. All networks have reached 100% training accuracy. Note that without such condition, higher test performance is achieved (values reported in the parentheses), though this breaks the correspondence to a NN-GP, since train set predictions of an underfit CNN no longer correspond to the true training labels. See §A.6.5 for experimental details.

Quality: Model: Error:

Compositionality FCN FCN-GP 46.26 41.45

Local connectivity

LCN (w/ pooling) CNN-GP

36.52 (36.23)

36.71

Equivariance CNN 19.93

Invariance CNN w/ pooling 16.54

Table 2: Test error (%) of different models of the same architecture and a conjectured explanation for the observed difference. Values reported for 8-layer ReLU models on CIFAR10. See §A.6.6 for
experimental details.

6 CONCLUSION
In this work we have derived a Gaussian process that corresponds to a fully Bayesian, infinitely wide CNN. The covariance of this GP can be efficiently computed either in closed form or by using Monte Carlo sampling, depending on the architecture.
CNN-GP can often perform competitively to the best CNNs (that fit the training set) of equivalent architecture and weight priors, which makes it an appealing choice for small datasets, as it eliminates all training-related hyper-parameters. However, we found that the best overall performance is achieved by finite SGD-trained CNNs and not by their infinitely wide Bayesian counterparts. We hope our work stimulates future research into disentangling the contributions of the two qualities (Bayesian treatment and infinite width) to the performance gap observed.
5An addition of global average pooling guarantees gradient invariance.
9

Under review as a conference paper at ICLR 2019
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Anastasia Borovykh. A gaussian process perspective on convolutional neural networks. 05 2018.
John Bradshaw, Alexander G de G Matthews, and Zoubin Ghahramani. Adversarial examples, uncertainty, and transfer testing robustness in gaussian process hybrid deep networks. arXiv preprint arXiv:1707.02476, 2017.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 873­882, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/chen18i.html.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pp. 342­350, 2009.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Ge´rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192­204, 2015.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on machine learning, pp. 2990­2999, 2016.
Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artificial Intelligence and Statistics, pp. 207­215, 2013.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In Advances In Neural Information Processing Systems, pp. 2253­2261, 2016.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= H1-nGgWC-.
Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biological cybernetics, 20(3-4):121­136, 1975.
Kunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets, pp. 267­285. Springer, 1982.
Adria` Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional networks as shallow Gaussian processes. arXiv preprint arXiv:1808.05587, aug 2018. URL https://arxiv.org/abs/1808.05587.
Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1487­1495. ACM, 2017.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544, 2014.
Tamir Hazan and Tommi Jaakkola. Steps toward deep kernel methods from infinite neural networks. arXiv preprint arXiv:1508.05133, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
10

Under review as a conference paper at ICLR 2019
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Vinayak Kumar, Vaibhav Singh, PK Srijith, and Andreas Damianou. Deep gaussian processes with convolutional kernels. arXiv preprint arXiv:1806.01655, 2018.
Neil D Lawrence and Andrew J Moore. Hierarchical gaussian process latent variable models. In Proceedings of the 24th international conference on Machine learning, pp. 481­488. ACM, 2007.
Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. In Artificial Intelligence and Statistics, pp. 404­411, 2007.
Yann Lecun. Generalization and network design strategies. In Connectionism in perspective. Elsevier, 1989.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohldickstein. Deep neural networks as gaussian processes. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z.
Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well? Journal of Statistical Physics, 168(6):1223­1247, 2017.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807­814, 2010.
Radford M. Neal. Priors for infinite networks (tech. rep. no. crg-tr-94-1). University of Toronto, 1994.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. Proceeding of the international Conference on Learning Representations workshop track, abs/1412.6614, 2015.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= HJC2SzZCW.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.
Razvan Pascanu, Yann N Dauphin, Surya Ganguli, and Yoshua Bengio. On the saddle point problem for non-convex optimization. arXiv preprint arXiv:1405.4604, 2014.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5):503­519, Oct 2017. ISSN 1751-8520. doi: 10.1007/s11633-017-1054-2. URL https://doi.org/10.1007/ s11633-017-1054-2.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances In Neural Information Processing Systems, pp. 3360­3368, 2016.
Joaquin Quin~onero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate gaussian process regression. Journal of Machine Learning Research, 6(Dec):1939­1959, 2005.
Ali Rahimi and Ben Recht. Random features for large-scale kernel machines. In In Neural Infomration Processing Systems, 2007.
Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning, volume 1. MIT press Cambridge, 2006.
11

Under review as a conference paper at ICLR 2019
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. ICLR, 2017.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.
Peter Tino, Michal Cernansky, and Lubica Benuskova. Markovian architectural bias of recurrent neural networks. IEEE Transactions on Neural Networks, 15(1):6­15, 2004.
Peter Tino, Barbara Hammer, and Mikael Bode´n. Markovian bias of neural-based architectures with feedback connections. In Perspectives of neural-symbolic integration, pp. 95­133. Springer, 2007.
Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional gaussian processes. In Advances in Neural Information Processing Systems 30, pp. 2849­2858, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998­6008, 2017.
Paul J Werbos. Generalization of backpropagation with application to a recurrent gas market model. Neural networks, 1(4):339­356, 1988.
Christopher KI Williams. Computing with infinite networks. In Advances in neural information processing systems, pp. 295­301, 1997.
Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic variational deep kernel learning. In Advances in Neural Information Processing Systems, pp. 2586­2594, 2016a.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artificial Intelligence and Statistics, pp. 370­378, 2016b.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5393­5402, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http://proceedings. mlr.press/v80/xiao18a.html.
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances in neural information processing systems, pp. 7103­7114, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.
12

Under review as a conference paper at ICLR 2019

A APPENDIX
A.1 ADDITIONAL FIGURES

No Pooling

0.14 0.13 0.12 0.11
0.1
0.09
0.08 5 10 2

5 100 2

5 1000 2

5

Validation Loss

Global Average Pooling

0.14 0.13 0.12 0.11 0.1
0.09
0.08 5 10 2

5 100 2

5 1000 2

5

LCN

CNN

0.14 0.13 0.12 0.11
0.1
0.09
0.08 5 10 2

5 100 2

5 1000 2

5

Validation Loss

0.14 0.13 0.12 0.11 0.1
0.09
0.08 5 10 2

5 100 2

5 1000 2

5

Number of Filters
Figure 7: Best validation loss (vertical axis) of trained neural networks (dashed line) as the number of channels increases (horizontal axis) approaches that of a respective (MC-)CNN-GP (solid horizontal line). See Figure 5 for validation accuracy, Figure 8 for training loss and §A.6.1 for experimental details.

No Pooling

0.1 0.01 0.001 100 10
1 5 10 2

5 100 2

5 1000 2

5

Training Loss

Global Average Pooling

0.1 0.01 0.001 100 10
1 5 10 2

5 100 2

5 1000 2

5

LCN

CNN

0.1 0.01 0.001 100 10
1 5 10 2

5 100 2

5 1000 2

5

Training Loss

0.1 0.01 0.001 100 10
1 5 10 2

5 100 2

5 1000 2

5

Number of Filters
Figure 8: Training loss (vertical axis) of best (in terms of validation loss) neural networks as the number of channels increases (horizontal axis). While perfect 0 loss is not achieved (but 100% accuracy is), we observe no consistent improvement when increasing the capacity of the network (left to right). This eliminates underfitting as a possible explanation for why small models perform worse in Figure 5. See Figure 7 for validation loss and §A.6.1 for experimental details.

13

Under review as a conference paper at ICLR 2019
10 0.42 10

0.5

log10(Kernel Distance)

Validation Accuracy log2(Filters)

MC-CNN-GP with pooling log2(Filters)

0 0
10

log2(Samples)

10

0.1 0 0
0.42 10

log2(Samples)

10

-6.5 0.5

log10(Kernel Distance)

Validation Accuracy log2(Filters)

MC-LCN-GP log2(Filters)

0 0
10

log2(Samples)

10

0.1 0 0
0.42 10

log2(Samples)

10

-6.5 0.5

log10(Kernel Distance)

Validation Accuracy log2(Filters)

MC-LCN-GP with Pooling log2(Filters)

0 0
10

log2(Samples)

10

0.1 0 0
0.42 10

log2(Samples)

10

-6.5 0.5

log10(Kernel Distance)

Validation Accuracy log2(Filters)

MC-FCN-GP log2(Filters)

0

0.1 0

-6.5

0

log2(Samples)

10

0

log2(Samples)

10

Figure 9: As in Figure 4, validation accuracy (left) of MC-GPs increases with n × M (i.e. width

times number of samples), while the distance (right) to the the respective exact GP (or the best avail-

able estimate in the case of CNN-GP with pooling, top row) decreases. We remark that when using

shared weights, convergence is slower as smaller number of independent random parameters are be-

ing used. Var[KCl NN

For example ]/Var[KLl CN]

a single-layer  # LCN

MC-LCN-GP kernel is params/# CNN params

expected to converge approximately = spatial size of the output layer

times faster than MC-CNN-GP, which is in agreement with our results obtained in the two top rows.

I.e. the geometric mean of the ratios of the kernel distance from (3-layer) MC-CNN-GP and MCLCN-GP to the respective CNN-GP is  2.2 spatial size of the output layer = 2 × 2 = 2).

See §A.6.3 for experimental details.

14

Under review as a conference paper at ICLR 2019

A.2 REVIEW OF EXACT BAYESIAN REGRESSION WITH GPS

Our discussion in the paper has focused on model priors. A crucial benefit we derive by mapping
to a GP is that Bayesian inference is straightforward to implement and can be done exactly for regression (Rasmussen & Williams, 2006, chapter 2), requiring only simple linear algebra. Let X denote training inputs x1, ..., x|X |, tT = (t1, ..., t|X |) the training targets, and collectively D for the training set. The integral over the posterior can be evaluated analytically to give a posterior predictive distribution on a test point x which is Normal, z|D, x  N (µ, 2), with

µ = K(x, X )(K(X , X ) + 2I|X |)-1t, 2 = K(x, x) - K(x, X )(K(X , X ) + 2I|X |)-1K(X , x).

(19) (20)

We use the shorthand K(X , X ) to denote the |X | × |X | matrix formed by evaluating the GP covariance on the training inputs, and likewise K(x, X ) is a |X |-length vector formed from the covariance between the test input and training inputs. Computationally, the costly step in GP posterior
predictions comes from the matrix inversion, which in all experiments were carried out exactly, and typically scales as O(|X |3) (though algorithms scaling as O(|X |2.4) exist for sufficiently large matrices). Nonetheless, there is a broad literature on approximate Bayesian inference with GPs which
can be utilized for efficient implementation (Rasmussen & Williams, 2006, chapter 8); (Quin~onero-
Candela & Rasmussen, 2005).

A.3 RELATIONSHIP TO DEEP SIGNAL PROPAGATION

The recurrence relation linking the GP kernel at layer l + 1 to that of layer l following from Equation

10 (i.e. Kl+1 = (C  A) Kl ) is precisely the covariance map examined in a series of related papers on signal propagation (Xiao et al., 2018; Poole et al., 2016; Schoenholz et al., 2017; Lee et al.,

2018) (modulo notational differences; denoted as F , C or A C in (Xiao et al., 2018)). In those works,

the action of this map on hidden-state covariance matrices was interpreted as defining a dynamical

system whose (C  A) Kl

large-depth  Kl 

behavior K , i.e.

informs aspects of trainability. In particular, as the covariance approaches a fixed point K .

l The

, Kl+1 = convergence

to a fixed point is problematic for learning because the hidden states no longer contain information

that can distinguish different pairs of inputs. It is similarly problematic for GPs, as the kernel

becomes pathological as it approaches a fixed point. Precisely, in the chaotic regime outputs of

the GP become asymptotically decorrelated and therefore independent, while in the ordered regime

they approach perfect correlation of 1. Either of these scenarios captures no information about

the training data in the kernel and makes learning infeasible. This problem can be ameliorated by

judicious hyperparameter selection, which can reduce the rate of exponential convergence to the

fixed point. For hyperpameters chosen on a critical line separating two untrainable phases, the

convergence rates slow to polynomial, and very deep networks can be trained, and inference with

deep NN-GP kernels can be performed ­ see Table 3.

Depth: 1

10 100 1000

Phase boundary
2

0.378

FCN-GP CNN-GP b2 Validation Accuracy

00.1 w2

0.1 5

Table 3: Validation accuracy of CNN- and FCN- GPs as a function of weight (w2 , horizontal axis) and bias (b2, vertical axis). As expected, the regions of good performance concentrate around the critical line (phase boundary, right) as the depth increases (left to right). All plots share common
axes ranges. See §A.6.3 for experimental details.

15

Under review as a conference paper at ICLR 2019

A.4 KERNEL CONVERGENCE PROOF

In this section, we present three different approaches to illustrate the weak convergence of neural networks to Gaussian process as the number of channels goes to infinity. Although the first A.4.1 and second approaches A.4.2 (taking iterated limits) are less formal, they provide some intuitions to the convergence of neural network to Gaussian process. The approach in Section A.4.3 is more standard and the proof is more involved. We only provide the arguments for convolution neural networks. It is straightforward to extend them to locally- or fully connected networks.
We will use the following well-known theorem.
Theorem A.1 (Portmanteau Theorem). Let Xn be a sequence of real-valued random variables. The following are equivalent:

1. Xn  X in distribution, 2. for all bounded continuous function f ,

lim
n

E[f

(Xn)]

=

E[f

(X

)],

(21)

3. The characteristic functions of Xn, i.e. EeitXn converges to that of X point-wisely, i.e., for all t,

lim E eitXn = E eitX .
n

(22)

A.4.1 FORWARD MODE
We show that when taking n1  , . . . , nL   sequentially, a CNN converges to a Gaussian Process in the following sense: preactivations of each layers (l  1) converge to a Gaussian in distribution. We will proceed by induction. Let n1  . It is not difficult to see that zj0 are pair-wisely independent Gaussian with identical distribution and thus iid Gaussian. Assume zjl are i.i.d. Gaussian (unconditionally). We claim that so are zjl+1 1j. zjl+1 . Indeed, since both the connection weights from layer l to layer l + 1 and the biases from different channels are independent, zjl+1 are pair-wisely independent and have the same distribution. To prove that they are mutually independent, we only need to show that for each j, zjl+1 converges to a Gaussian in distribution as nl  . Since xlj+1 =  zjl are i.i.d., thus the outcomes of the inner sum of Equation 2 are i.i.d. We can then apply a multivariate central limit theorem6 to conclude that zjl+1 converges to a Gaussian in distribution (note that we have applied the fact that bjl+1 is a Gaussian).

A.4.2 REVERSE MODE

Conditioning on Kl-1, Kl is a random variable that converges to (C  A) Kl-1 in probability as the number of channels nl   (the law of large number, see Equation 6).
It is clear that different channels of zL are uncorrelated and have the same distribution. We will show that for any channel index j, the random variable zjL "converges" to the Gaussian

N 0, A  (C  A)L K0

(23)

in the sense that its characteristic function converges point-wisely to that of N 0, A  (C  A)L K0 , i.e. for each j and for all vectors t

lim . . . lim
n1 nL

Gn1,...,nL (t)  E

eiz0L ·t

=

e-

1 2

tT

A(CA)L(K0)t.

6Assuming the covariance of  zjl is finite.

(24)

16

Under review as a conference paper at ICLR 2019

Proof. Applying Fubini's Theorem and the formula of the characteristic function of multivariate Gaussian

Gn1,...,nL (t) = eiz0L·tp z0L|KL p KL|KL-1 · · · p K1|K0 dKL · · · dK0dz0L

(25)

= eiz0L·tp z0L|KL dz0Lp KL|KL-1 · · · p K1|K0 dKL · · · dK0

(26)

=

e-

1 2

tT

K

L

t

p

K L |K L-1

···p

K 1 |K 0

dKL · · · dK0

(27)

=

e-

1 2

tT

K

L

t

p

K L |K L-1

dK L

·

p KL-1|KL-2 · · · p K1|K0 dKL-1 · · · dK0

(28) (29)

We now apply limnL and switch the order of it with the outer integral. The Lebesgue dominant theorem allows us to do so because the inner integral is bounded above by the constant function

g = 1 which is absolutely integrable w.r.t. the outer integral. We then apply Theorem A.1, since

e-

1 2

tT

K

L

t

is

bounded

and

continuous

in

KL

and

KL



A



(C



A)

K L-1

.

lim Gn1,...,nL (t) =
nL 

e-

1 2

tT

A(C

A)(K

L-1

)t

p

K L-1 |K L-2

···p

K 1 |K 0

dKL-1 · · · dK0.

(30)

Repeatedly applying the same argument7 gives

lim
n1 

.

.

.

lim
nL 

Gn1,...,nL (t)

=

e-

1 2

tT

A(CA)L(K0)t.

(31)

Note that the addition of various layers on top (as discussed in §3) does not change the proof in a qualitative way.

A.4.3 MORE GENERAL SETTING

In this section, we present a sufficient condition on the activation function  so that the neural
networks will converge to a Gaussian process as all the widths approach to infinity simultaneously. More precisely, we are interested in the case nl = nl(t)   as t  , i.e.,

lim min n1(t), . . . , nL(t)  .
t

(32)

Using Theorem A.1 and the arguments in the above section, it is not difficult to see that a sufficient condition is that the empirical covariance converges in probability to the analytic covariance.
Corollary A.1.1. If KL converges to KL in probability as t  , then

ziL -D N 0, KL = N 0, A  (C  A)L K0 .

(33)

In the remaining section, we provide a sufficient condition for Corollary A.1.1, borrowing some
ideas from Daniely et al. (2016). Let PSDm denote the set of m × m positive semi-definite matrices and for R  1, let

PSDm(R) = {  PSDm : 1/R  ,  R for 1    m} .

(34)

Let T and Tn : PSD2  R be a function and a random variable (induced by the activation ) given by

T() = E(x,y)N (0,)[(x)(y)

(35)

and

Tn()

=

1 n

n

(xi)(yi),

i=1

(36)

7Here we need C to be continuous.

17

Under review as a conference paper at ICLR 2019

where {(xi, yi)}in=1 are drawn independently from N (0, ). We assume  is uniformly squared integrable with respect to the Gaussian measure N (0, r) for 1/R  r  R, i.e.,

sup  L2(N (0,r)) < .
r[1/R,R]

(37)

By the law of large numbers, Tn()  T() in probability for all   PSD2(R), but this is not enough for our purpose and we need a stronger type of convergence: uniform convergence in probability. Note that the function T is closely related to the mapping C: if  is the 2×2 symmetric matrix with , = [K], (x, x ), , = [Kl],(x, x) and  , = [K] , (x , x ), then

[C Kl ], (x, x ) = T()

(38)

We choose R large enough so that for l = 0, . . . , L, all x  X and ,

1/R < Kl , (x, x), A(Kl ) , (x, x) < R,

(39)

which implies for 0  l  L, Kl and A(Kl ) are interior points of PSD|X |d(R). Let (R) denote the space of measurable functions with the following properties:

1. Uniformly Squared Integrable:
sup  L2(N (0,r)) < ;
r[1/R,R]

(40)

2. Lipschitz Continuity: there exists  = (, R) > 0 such that for all ,   PSD2(R),

|T() - T( )|    -   ;

(41)

3. Uniform Convergence in Probability: for every > 0,

sup P (|Tn() - T()| > )  0 as n  .
PSD2 (R)

(42)

It is easy to see that (R) is a vector space since it is closed under scalar multiplication and addition. The following is the main result of this section.
Theorem A.2. For   (R), KL  KL in probability and ziL  N 0, A  (C  A)L K0 in distribution.

It is not difficult to see that (R) contains the space of Schwartz functions. Indeed, Daniely et al. (2016) prove (Lemmas 12 and 13) that (R) contains all C2-functions, i.e. twice continuously differentiable functions with  ,  ,   <  and the ReLu function. Therefore it contains all finite linear combination of such functions. Characterizing the space (R) and its topology is
left for future work.

Proof. Let   (R). Note that the affine transform A is 2 -Lipschitz and the second property of (R) implies that the C operator is |X |2 d2-Lipschitz. Thus C  A is ¯-Lipschitz with ¯ =

2 |X |2 d2. We loss of generality,

proceed by we assume

induction. ¯  1. Let

Assume Kl  Kl > 0 be sufficiently

in probability as small so that the

t  . Without ¯-neighborhood of

A(Kl ) is contained in PSD|X |d(R), where ¯ = max , 2 /¯ . Since

Kl+1 - Kl+1   Kl+1 - C  A Kl  + C  A Kl - Kl+1   C  A Kl - C  A Kl  + C  A Kl - Kl+1  ,

to prove Kl+1 for all t > t,



Kl+1

in

probability,

it

suffices

to

show

that

for

every



>

0,

there

is

a

t

so

that

P ( C  A Kl - C  A Kl  > /2) + P C  A Kl - Kl+1  > /2 <  . (43) By our induction assumption, there is a t0 so that for all t > t0

p |Kl - Kl| > /2¯ < /3 .

(44)

18

Under review as a conference paper at ICLR 2019

Since C  A is ¯-Lipschitz, then P C  A Kl - C  A Kl  > /2 < /3 .
To bound the second term in Equation 43, let U (t) denote the event U (t) = A Kl  PSD|X |d(R)
and U (t)c denote its complement and

(45) (46)

[V (t)], (x, x ) = |[C  A Kl ], (x, x ) - Kl+1 , (x, x )| > /2 .

Our assumption implies for all t > t0 P (U (t)c) < /3.

The fact



C  A Kl - Kl+1  > /2  U (t)c 

[V (t)], (x, x ) U (t)

x,x ,,

(47) (48)

implies

P C  A Kl - Kl+1  > /2  /3 + |X |2d2 sup P ([V (t)], (x, x )  U (t)) .
x,x ,,

If we use  to denote the 2×2 matrix with , = [A Kl ], (x, x ), , = [A Kl ],(x, x) and  , = [A Kl ] , (x , x ), then

[C  A Kl ], (x, x ) = T()

(49)

and Kl+1 , (x, x ) and Tnl+1 () have the same distribution. Applying the third property of (R), we conclude that there is a n so that for all nl+1  n,

sup
x,x ,,

P

([V (t)],

(x, x

)  U (t)) 

 3|X |2d2

and

P C  A Kl - Kl+1  > /2  U (t) < 2/3. Therefore we just need to choose t > t0 so that nl+1(t) > n for all t > t.

A.5 GLOSSARY
We use the following shorthands in this work:
1. NN - neural network; 2. CNN - convolutional neural network; 3. LCN - locally-connected network, a.k.a. convolutional network without weight sharing; 4. FCN - fully connected network, a.k.a. multilayer perceptron (MLP); 5. GP - Gaussian process; 6. X-GP - a GP equivalent to a Bayesian infinitely wide neural network of architecture X (§2). 7. MC-(X-)-GP - a Monte Carlo estimate (§4) of the X-GP. 8. Width, number of filters, number of channels represent the same property for CNNs and
LCNs. 9. Pooling - referring to architectures as "with" or "without pooling" means having a single
global average pooling layer (collapsing the spatial dimensions of the activations xL+1) before the final linear FC layer outputting the regression outputs zL+1. 10. Invariance and equivariance are always discussed w.r.t. translations in the spatial dimensions.

19

Under review as a conference paper at ICLR 2019

A.6 EXPERIMENTAL SETUP
Throughout this work we only consider 3 × 3 (possibly unshared) convolutional filters with stride 1 and no dilation.
All inputsare normalized to have zero mean and unit variance, i.e. lie on the d-dimensional sphere of radius d, where d is the total dimensionality of the input.
All labels are treated as regression targets with zero mean. I.e. for a single-class classification problem with C classes targets are C-dimensional vectors with -1/C and (C - 1)/C entries in incorrect and correct class indices respectively.
If a subset of a full dataset is considered for computational reasons, it is randomly selected in a balanced fashion. No data augmentation is used.
All experiments were implemented in Tensorflow (Abadi et al., 2016) and executed with the help of Vizier (Golovin et al., 2017).
All neural networks are trained using Adam (Kingma & Ba, 2014) minimizing the mean squared error loss.

A.6.1 WIDE CNNS AND LCNS

Relevant figures: 5, 7, 8.

We use a training and validation subsets of CIFAR10 of sizes 500 and 4000 respectively. All images are bilinearly downsampled to 8 × 8 pixels.

All models have 3 hidden layers with an erf nonlinearity. No ("valid") padding is used.

Weight and bias variances are tion variance fixed point q =

set to w2 1 (Poole

 et

1.7562 and b2  0.1841, corresponding al., 2016) for the erf nonlinearity.

to

the

preactiva-

NN training proceeds for 219 gradient updates, but aborts if no progress on training loss is observed for the last 100 epochs. If the training loss does not reduce by at least 10-4 for 20 epochs, the learning rate is divided by 10.

All computations are done with 32-bit precision.

The following NN parameters are considered8:

1. Architecture: CNN or LCN.
2. Pooling: no pooling or a single global average pooling (averaging over spatial dimensions) before the final FC layer.
3. Number of filters: 2k for k from 0 to 12. 4. Initial learning rate: 10-k for k from 0 to 15. 5. Weight decay: 0 and 10-k for k from 0 to 8.
6. Batch size: 10, 25, 50, 100, 200.

For NNs, all models are filtered to only 100%-accurate ones on the training set and then for each configuration of {architecture, pooling, number of filters} the model with the lowest validation loss is selected among the configurations of {learning rate, weight decay, batch size}.
For GPs, the same CNN-GP is plotted against CNN and LCN networks without pooling. For LCN with pooling, inference was done with an appropriately rescaled CNN-GP kernel, i.e. Kvec - b2 /d + b2, where d is the spatial size of the penultimate layer. For CNNs with pooling, a Monte Carlo estimate was computed (see §4) with n = 212 filters and M = 26 samples.

A.6.2 TRANSFORMING A GP OVER SPATIAL LOCATIONS INTO A GP OVER CLASSES
Relevant figure: 3. 8Due to time and memory limitations certain large configurations could not be evaluated. We believe this
did not impact the results of this work in a qualitative way.

20

Under review as a conference paper at ICLR 2019

We use the same setup as in §A.6.3, but rescale the input images to size of 31 × 31, so that at depth 15 the spatial dimension collapses to a 1 × 1 patch if no padding is used (hence the curve of the CNN-GP without padding halting at that depth).
For MC-CNN-GP with pooling, we use samples of networks with n = 16 filters. Due to computational complexity we only consider depths up to 31 for this architecture. The number of samples M was selected independently for each depth among 2k for k from 0 to 15 to maximize the validation accuracy on a separate 500-points validation set. This allowed us to avoid the poor conditioning of the kernel.

A.6.3 MONTE CARLO EVALUATION OF INTRACTABLE GP KERNELS

Relevant figures: 4, 9.

We use the same setup as in §A.6.1, but training and validation sets of sizes 2000 and 4000 respectively.

For MC-GPs we consider the number of channels n (width in FCN setting) and number of NN instantiations M to accept values of 2k for k from 0 to 10.

Kernel distance is computed as:

K - Kn,M

K

2 F

2
F,

(50)

where K is substituted with K210,210 for the CNN-GP pooling case (due to impracticality of computing the exact Kpool).

A.6.4 RELATIONSHIP TO DEEP SIGNAL PROPAGATION
Relevant Table: 3.
We use a training and validation subsets of CIFAR10 of sizes 500 and 1000 respectively.
We use the erf nonlinearity. For CNN-GP, images are zero-padded ("same" padding) to maintain the spatial shape of the activations as they are propagated through the network. Weight and bias variances (horizontal axis w2 and vertical axis b2 respectively) are sampled from a uniform grid of size 50 × 50 on the range [0.1, 5] × [0, 2] including the endpoints. If an experiment fails due to a numerical reason it is assigned the 0.1 (random chance) validation accuracy.
All computations are done with 64-bit precision.

A.6.5 CNN-GP ON FULL DATASETS
Relevant Table: 1, figures 1, 6.
We use full training, validation, and test sets of sizes 50000, 10000, and 10000 respectively for MNIST and Fashion-MNIST, 45000, 5000, and 10000 for CIFAR10. We use validation accuracy to select the best configuration for each model (we do not retrain on valdiation sets).
GPs are computed with 64-bit precision, and NNs are trained with 32-bit precision.
Zero-padding ("same") is used.
The following parameters are considered:
1. Architecture: CNN of FCN. 2. Nonlinearity: erf or ReLU. 3. Depth: 2k for k from 0 to 4 (and up to 25 for MNIST and Fashion-MNIST datasets). 4. Weight and bias variances. For erf: q from {0.1, 1, 2, . . . , 8}. For ReLU: a fixed weight
variance 2 = 2 and bias variance b2 from {0.1, 1, 2, . . . , 8}.

21

Under review as a conference paper at ICLR 2019

On CIFAR10, we additionally train NNs for 218 gradient updates with a batch size of 128 with corresponding parameters in addition to9
1. Pooling: no pooling or a single global average pooling (averaging over spatial dimensions) before the final FC layer (only for CNNs).
2. Number of filters or width: 2k for k from 1 to 9 (and up to 210 for CNNs with pooling in Figure 1).
3. Learning rate: 10-k × 216/ (width × q) for k from 5 to 9, where width is substituted with the number of filters for CNNs and q is substituted with b2 for ReLU networks. "Small learning rate" in Table 1 refers to k  {8, 9}.
4. Weight decay: 0 and 10-k for k from 0 to 5.
For NNs, all models are filtered to only 100%-accurate ones on the training set (expect for when the value in parentheses in Table 1) is reported). The reported values are then reported for models that achieve the best validation accuracy.

A.6.6 MODEL COMPARISON ON CIFAR10

Relevant Table 2.

We use the complete CIFAR10 dataset as described in §A.6.5 and consider 8-layer ReLU models

with weight 210 and 212

and bias variances of w2 for LCN, CNN, and FCN

= 2 and b2 = respectively.

0.01.

The

number

of

filters

/

width

is

set

to

25,

GPs are computed with 64-bit precision, and NNs are trained with 32-bit precision.

No padding ("valid") is used.

NN training proceeds for 218 gradient updates with batch size 64, but aborts if no progress on training loss is observed for the last 10 epochs. If the training loss does not reduce by at least 10-4 for 2 epochs, the learning rate is divided by 10.
Values for NNs are reported for the best validation accuracy over different learning rates (10-k for k from 2 to 12) and weight decay values (0 and 10-k for k from 2 to 7).For GPs, validation accuracy is maximized over initial diagonal regularization terms applied to the training convariance matrix: 10-k × [mean of the diagonal] for k among 2, 4 and 9 (if the cholesky decompisition fails, the regularization term is increased by a factor of 10 until it succeeeds).

9Due to time and compute limitations certain large configurations could not be evaluated. We believe this did not impact the results of this work in a qualitative way.
22

