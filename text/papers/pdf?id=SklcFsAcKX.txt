Under review as a conference paper at ICLR 2019

DEEP DENOISING: RATE-OPTIMAL RECOVERY OF STRUCTURED SIGNALS WITH A DEEP PRIOR
Anonymous authors Paper under double-blind review

ABSTRACT
Deep neural networks provide state-of-the-art performance for image denoising, where the goal is to recover a near noise-free image from a noisy image. The underlying principle is that neural networks trained on large datasets have empirically been shown to be able to generate natural images well from a lowdimensional latent representation of the image. Given such a generator network, or prior, a noisy image can be denoised by finding the closest image in the range of the prior. However, there is little theory to justify this success, let alone to predict the denoising performance as a function of the networks parameters. In this paper we consider the problem of denoising an image from additive Gaussian noise, assuming the image is well described by a deep neural network with ReLu activations functions, mapping a k-dimensional latent space to an n-dimensional image. We state and analyze a simple gradient-descent-like iterative algorithm that minimizes a non-convex loss function, and provably removes a fraction of p1 ´ Opk{nqq of the noise energy. We also demonstrate in numerical experiments that this denoising performance is, indeed, achieved by generative priors learned from data.

1 INTRODUCTION

We consider the image or signal denoising problem, where the goal is to remove noise from an unknown image or signal. In more detail, our goal is to obtain an estimate of an image or signal y° P Rn from
y " y° ` ,
where  is unknown noise, often modeled as a zero-mean white Gaussian random variable with covariance matrix 2{nI.

Image denoising relies on modeling or prior assumptions on the image y°. For example, suppose that the image y° lies in a k-dimensional subspace of Rn denoted by Y. Then we can estimate the original image by finding the closest point in 2-distance to the noisy observation y on the subspace Y. The corresponding estimate, denoted by y^, obeys

}y^

´

y°}2

À

2

k ,
n

(1)

with high probability (throughout, }¨} denotes the 2-norm). Thus, the noise energy is reduced by a factor of k{n over the trivial estimate y^ " y which does not use any prior knowledge of the signal. The denoising rate (1) shows that the more concise the image prior or image representation (i.e., the smaller k), the more noise can be removed. If on the other hand the prior (the subspace, in this example) does not include the original image y°, then the error bound (1) increases as we would remove a significant part of the signal along with noise when projecting onto the range of the signal
prior. Thus a concise and accurate prior is crucial for denoising.

Real world signals rarely lie in a priori known subspaces, and the last few decades of image denoising research have developed sophisticated and accurate image models or priors and algorithms. Examples include models based on sparse representations in overcomplete dictionaries such as wavelets (Donoho, 1995) and curvelets (Starck et al., 2002), and algorithms based on exploiting

1

Under review as a conference paper at ICLR 2019

self-similarity within images (Dabov et al., 2007). A prominent example of the former class of algorithms is the BM3D (Dabov et al., 2007) algorithm, which achieves state-of-the-art performance for certain denoising problems. However, the nuances of real world images are difficult to describe with handcrafted models. Thus, starting with the paper (Elad & Aharon, 2006) that proposes to learn sparse representation based on training data, it has become common to learn concise representation for denoising (and other inverse problems) from a set of training images.
In 2012, Burger et al. (Burger et al., 2012) applied deep networks to the denoising problem, by training a deep network on a large set of images. Since then, deep learning based denoisers (Zhang et al., 2017) have set the standard for denoising. The success of deep network priors can be attributed to their ability to efficiently represent and learn realistic image priors, for example via autodecoders (Hinton & Salakhutdinov, 2006) and generative adversarial models (Goodfellow et al., 2014). Over the last few years, the quality of deep priors has significantly improved (Karras et al., 2017; Ulyanov et al., 2017). As this field matures, priors will be developed with even smaller latent code dimensionality and more accurate approximation of natural signal manifolds. Consequently, the representation error from deep priors will decrease, and thereby enable even more powerful denoisers.
While deep networks can model complex priors through many parameters and non-linearities, those non-linearities also make their analysis inherently difficult, increasing the gap between theory and practice. As a consequence, there is a need for theory explaining the success of deep network based priors.

Contributions: The goal of this paper is to analytically quantify the denoising performance of

deep-prior based denoisers. Specifically, we characterize the denoising performance of a simple

and efficient algorithm for denoising based on a d-layer generative neural network G : Rk Ñ Rn,

with k  n, and random weights. In more detail, we propose a gradient method with a tweak that

attempts

to

minimize

the

least-squares

loss

f pxq

"

1 2

}Gpxq

´

y}

between

the

noisy

image

y

and

an

image in the range of the prior, Gpxq. Albeit f is non-convex, we show that the gradient method

with a tweak yields an estimate x^ obeying

}Gpx^q

´

y°}2

À

2

k ,
n

with high probability, where the notation À absorbs a constant factor depending on the number

of layers of the network, and its expansitivity, discussed in more detail later. Our result shows

that the denoising rate of a deep prior based denoiser is determined by the dimension of the latent

representation.

We also show in numerical experiments, that this rate--shown to be analytically achieved for random priors--is also experimentally achieved for priors learned from real imaging data.

2 PROBLEM FORMULATION
We consider the problem of estimating a vector y° P Rn from a noisy observation y " y° ` . We assume that the vector y° belongs to the range of a d-layer generative neural network G : Rk Ñ Rn, with k  n. That is, y° " Gpx°q for some x° P Rk. We consider a generative network of the form
Gpxq " relupWd . . . relupW2 relupW1x°qq . . .q, where relupxq " maxpx, 0q applies entrywise, Wi P Rni^ni´1 , are the weights in the i-th layer, ni is the number of neurons in the ith layer, and the network is expansive in the sense that k " n0  n1  ¨ ¨ ¨  nd " n. The problem at hand is: Given the weights of the network W1 . . . Wd and a noisy observation y, obtain an estimate y^ of the original image y° such that }y^ ´ y°} is small and y^ is in the range of G.

3 DENOISING VIA EMPIRICAL RISK MINIMIZATION

As a way to solve the above problem, we first obtain an estimate of x°, denoted by x^, and then

estimate y° as Gpx^q. In order to estimate x°, we minimize the empirical risk objective

f pxq

:"

1 2 }Gpxq

´

y}2.

2

Under review as a conference paper at ICLR 2019

¨10´2 4

f pxq

2

0 ´2 ´1

0

x1

1

0 ´1 x2

Figure 1: Loss surface f pxq " }Gpxq ´ Gpx°q}, x° " r1, 0s, of an expansive network G with ReLu activation functions with k " 2 nodes in the input layer and n2 " 300 and n3 " 784 nodes in the hidden and output layers, respectively, with random Gaussian weights in each layer. The surface
has a critical point near ´x°, a global minimum at x°, and a local maximum at 0.

Since this objective is nonconvex, there is no a priori guarantee of efficiently finding the global minimum. Approaches such as gradient methods could in principle get stuck in local minima, instead of finding a global minimizer that is close to x°.
However, as we show in this paper, under appropriate conditions, a gradient method with a tweak-- introduced next--finds a point that is very close to the original latent parameter x°, with the distance to the parameter x° controlled by the noise. In order to state the algorithm, we first introduce a useful quantity. For analyzing which rows of a matrix W are active when computing relupW xq, we let
W`,x " diagpW x  0qW.
For a fixed weight matrix W , the matrix W`,x zeros out the rows of W that do not have a positive dot product with x. Alternatively put, W`,x contains weights from only the neurons that are active for the input x. We also define W1,`,x " pW1q`,x " diagpW1x  0qW1 and
Wi,`,x " diagpWiWi´1,`,x ¨ ¨ ¨ W2,`,xW1,`,xx  0qWi.
The matrix Wi,`,x consists only of the weights of the neurons in the ith layer that are active if the input to the first layer is x.
We are now ready to state our algorithm: a gradient method with a tweak informed by the loss surface of the function to be minimized. Given a noisy observation y, the algorithm starts with an arbitrary initial point x0  0. At each iteration i " 0, 1, . . ., the algorithm computes the step direction
v~xi " pi1"dWi,`,xi qtpGpxiq ´ yq, which is equal to the gradient of f if f is differentiable at xi. It then takes a small step opposite to v~xi . The tweak is that before each iteration, the algorithm checks whether f p´xiq is smaller than f pxiq, and if so, negates the sign of the current iterate xi.
This tweak is informed by the loss surface. To understand this step, it is instructive to examine the loss surface for the noiseless case in Figure 1. It can be seen that while the loss function has a global minimum at x°, it is relatively flat close to ´x°. In expectation, there is a critical point that is a negative multiple of x° with the property that the curvature in the x° direction is positive, and the curvature in the orthogonal directions is zero. Further, around approximately ´x°, the loss function is larger than around the optimum x°. As a simple gradient descent method (without the tweak) could potentially get stuck in this region, the negation check provides a way to avoid converging to this region. Our algorithm is formally summarized as Algorithm 1 below.
Other variations of the tweak are also possible. For example, the negation check in Step 2 could be performed after a convergence criterion is satisfied, and if a lower objective is achieved by negating the latent code, then the gradient descent can be continued again until a convergence criterion is again satisfied.

3

Under review as a conference paper at ICLR 2019

Algorithm 1 Gradient method
Require: Weights of the network Wi, noisy observation y, and step size   0 1: Choose an arbitrary initial point x0 P Rkzt0u 2: for i " 0, 1, . . . do 3: if f p´xiq  f pxiq then 4: xi Ð ´xi; 5: end if 6: Compute v~xi " p1i"dWi,`,xi qtpGpxiq ´ yq 7: xi`1 " xi ´ v~xi 8: end for

4 MAIN RESULTS

For our analysis, we consider a fully-connected generative network G : Rk Ñ Rn with Gaussian weights and no bias terms. Specifically, we assume that the weights Wi are independently and identically distributed as N p0, 2{niq, but do not require them to be independent across layers. Moreover, we assume that the network is sufficiently expansive:
Expansivity condition. We say that the expansivity condition with constant  0 holds if
ni  c ´2 logp1{ qni´1 log ni´1, for all i,
where c is a particular numerical constant.

In a real-world generative network the weights are learned from training data, and are not drawn from a Gaussian distribution. Nonetheless, the motivation for selecting Gaussian weights for our analysis is as follows:

1. The empirical distribution of weights from deep neural networks often have statistics consistent with Gaussians. AlexNet is a concrete example (Arora et al., 2015).
2. The field of theoretical analysis of recovery guarantees for deep learning is nascent, and Gaussian networks can permit theoretical results because of well developed theories for random matrices.

We are now ready to state our main result.
Theorem 1. Consider a network with the weights in the i-th layer, Wi P Rni^ni´1 , i.i.d. N p0, 2{niq distributed, and suppose that the network satisfies the expansivity condition for some  K{d90. Also, suppose that the noise variance obeys

c





}x°}K1 d16

,

 :"

182

k n

logpn1dn2d´1

.

.

.

ndq.

Consider

the

iterates

of

Algorithm

1

with

stepsize



"

K4

1 d2

.

Then,

there

exists

a

number

of

steps

N upper bounded by

N



K2 d4

f px0q }x°}

such that after N steps, the iterates of Algorithm 1 obey

}xi ´ x°}  K5d9}x°}? ` K6d6, for all i  N ,

(2)

with

probability

at

least

1

´

2e´2k

log

n

´

d
i"2

8ni e´K7 ni´2

´

8n1e´K7

2 logp1{

qk .

Here, K1, K2, ..

are numerical constants, and x0 is the initial point in the optimization.

The error term in the bound (2) consists of two terms--the first is controlled by , and the second depends on the noise. The first term is negligible if is chosen sufficiently small, but that comes at the expense of the expansivity condition being more stringent. The second term in the bound (2) is more interesting and controls the effect of noise. Specifically, for sufficiently small, our result guarantees that after sufficiently many iterations,

}xi

´

x°}2

À

2

k n

,

4

Under review as a conference paper at ICLR 2019

where the notation À absorbs a factor logarithmic in n and polynomial in d. One can show that G is Lipschitz in a region around x°1,

}Gpxiq

´

Gpx°q}2

À

2

k .
n

Thus, the theorem guarantees that our algorithm yields the denoising rate of 2k{n, and, as a consequence, denoising based on a generative deep prior provably reduces the energy of the noise in the original image by a factor of k{n. We note that the intention of this paper is to show rate-optimality of recovery with respect to the noise power, the latent code dimensionality, and the signal dimensionality. As a result, no attempt was made to establish optimal bounds with respect to the scaling of constants or to powers of d. The bounds provided in the theorem are highly conservative in the constants and dependency on the number of layers, d, in order to keep the proof as simple as possible. Numerical experiments shown later reveal that the parameter range for successful denoising are much broader than the constants suggest. As this result is the first of its kind for rigorous analysis of denoising performance by deep generative networks, we anticipate the results can be improved in future research, as has happened for other problems, such as sparsity-based compressed sensing and phase retrieval.

4.1 THE WEIGHT DISTRIBUTION CONDITION (WDC)

To prove our main result, we make use of a deterministic condition on G, called the Weight Distribution Condition (?WDC), and then show that Gaussian Wi, as given by the statement of Theorem 1 are such that Wi{ 2 satisfies the WDC with the appropriate probability for all i, provided the expansivity cond?ition holds. Our main result, Theorem 1, continues to hold for any weight matrices such that Wi{ 2 satisfy the WDC.

The condition is on the spatial arrangement of the network weights within each layer. We say that
the matrix W P Rn^k satisfies the Weight Distribution Condition with constant if for all nonzero x, y P Rk,

n

>ÿ >
>

1

wi ,x

01

wi ,y

0

¨

wiwit

´

>

Qx,y

> >



,

with

Qx,y

"



´ 0 2

Ik

`

sin 0 2

Mx^Øy^,

i"1

(3)

where wi P Rk is the ith row of W ; Mx^Øy^ P Rk^k is the matrix2 such that x^ ÑÞ y^, y^ ÞÑ x^, and z ÑÞ 0 for all z P spanptx, yuqK; x^ " x{}x}2 and y^ " y{}y}2; 0 " =px, yq; and 1S is the indicator function on S. The norm in the left hand side of (3) is the spectral norm. Note that an elementary calculation3 gives that Qx,y " Erin"1 1 wi,x 01 wi,y 0 ¨wiwits for wi ,, N p0, Ik{nq. As the rows wi correspond to the neural network weights of the ith neuron in a layer given by W , the WDC provides a deterministic property under which the set of neuron weights within the layer
given by W are distributed approximately like a Gaussian. The WDC could also be interpreted as a
deterministic property under which the neuron weights are distributed approximately like a uniform
random variable on a sphere of a particular radius. Note that if x " y, Qx,y is an isometry up to a factor of 1{2.

5 APPLICATIONS TO COMPRESSED SENSING

In this section we briefly discuss another important scenario to which our results apply to, namely regularizing inverse problems using deep generative priors. Approaches that regularize inverse problems using deep generative models (Bora et al., 2017) have empirically been shown to improve over sparsity-based approaches, see (Lucas et al., 2018) for a review for applications in imaging,

1The proof of Lipschitzness follows from applying the Weight Distribution Condition in Section 4.1.

2A formula for Mx^Øy^ is as follows. If 0 " =px^, y^q P p0, q and R is a rotation matrix such that x^ and y^

¨

cos 0 sin 0

0

map to e1 and cos 0 ¨ e1 ` sin 0 ¨ e2 respectively, then Mx^Øy^ " Rt sin 0 ´ cos 0 0 ,R, where

0 0 0k´2

0k´2 is a k ´ 2 ^ k ´ 2 matrix of zeros. If 0 " 0 or , then Mx^Øy^ " x^x^t or ´x^x^t, respectively.

3To do this calculation, take x " e1 and y " cos 0 ¨ e1 ` sin 0 ¨ e2 without loss of generality. Then each

entry of the matrix can be determined analytically by an integral that factors in polar coordinates.

5

Under review as a conference paper at ICLR 2019

and (Mardani et al., 2017) for an application in Magnetic Resonance Imaging showing a significant performance improvement over conventional methods.

Consider an inverse problem, where the goal is to reconstruct an unknown vector y° P Rn from m  n noisy linear measurements:

z " Ay° `  P Rm,

where A P Rm^n is called the measurement matrix and  is zero mean Gaussian noise with

covariance matrix 2{nI, as before. As before, assume that y° lies in the range of a genera-

tive prior G, i.e., y° " Gpx°q for some x°. As a way to recover x°, consider minimizing the

empirical risk objective f pxq

"

1 2

}AGpxq

´

z},

using

Algorithm

1,

with

Step

6

substituted

by

v~xi " pAi1"dWi,`,xi qtpAGpxiq ´ yq, to account for the fact that measurements were taken with

the matrix A.

Suppose that A is a random projection matrix, for concreteness assume that A has i.i.d. Gaussian

entries with variance 1{m. One could prove an analogous result as Theorem 1, but with  "

b

182

k m

logpnd1 nd2´1

. . . ndq,

(note

that

n

has

been

replaced

by

m).

This extension shows that,

provided is chosen sufficiently small, that our algorithm yields an iterate xi obeying

}Gpxiq

´

Gpx°q}2

À

2

k ,
m

where again À absorbs factors logarithmic in the ni's, and polynomial in d. Proving this result would be analogous to the proof of Theorem 1, but with the additional assumption that the sensing matrix A acts like an isometry on the union of the ranges of i1"dWi,`,xi , analogous to the proof in (Hand & Voroninski, 2018). This extension of our result shows that Algorithm 1 enables solving
inverse problems under noise efficiently, and quantifies the effect of the noise.

We hasten to add that the paper (Bora et al., 2017) also derived an error bound for minimizing empirical loss. However, the corresponding result (for example Lemma 4.3) differs in two important aspects to our result. First, the result in (Bora et al., 2017) only makes a statement about the minimizer of the empirical loss and does not provide justification that an algorithm can efficiently find a point near the global minimizer. As the program is non-convex, and as non-convex optimization is NP-hard in general, the empirical loss could have local minima at which algorithms get stuck. In contrast, the present paper presents a specific practical algorithm and proves that it finds a solution near the global optimizer regardless of initialization. Second, the result in (Bora et al., 2017) considers arbitrary noise  and thus can not assert denoising performance. In contrast, we consider a random model for the noise, and show the denoising behavior that the resulting error is no more than Opk{nq, as opposed to }}2 « Op1q, which is what we would get from direct application of the result in (Bora et al., 2017).

6 EXPERIMENTAL RESULTS
In this section we provide experimental evidence that corroborates our theoretical claims that denoising with deep priors achieves a denoising rate proportional to 2k{n. We consider both a synthetic, random prior, as studied theoretically in the paper, as well as a prior learned from data. All our results are reproducible with the code provided in the supplement.
6.1 DENOISING WITH A SYNTHETIC PRIOR
We start with a synthetic generative network prior with ReLu-activation functions, and draw its weights independently from a Gaussian distribution. We consider a two-layer network with n " 1500 neurons in the output layer, 500 in the middle layer, and vary the number of input neurons, k, and the noise level, . We next present simulations showing that if k is sufficiently small, our algorithm achieves a denoising rate proportional to k{n as guaranteed by our theory.
Towards this goal, we generate Gaussian inputs x° to the network and observe the noisy image y " Gpx°q ` ,  ,, N p0, 2{nIq. From the noisy image, we first obtain an estimate x^ of the latent representation by running Algorithm 1 until convergence, and second we obtain an estimate of the image as y^ " Gpx^q. In the left and middle panel of Figure 3, we depict the normalized mean

6

Under review as a conference paper at ICLR 2019

noise variance noisy
denoised

0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7

Figure 2: Denosing with a learned generative prior: Even when the number is barely visible, the denoiser recovers a sharp image.

squared error of the latent representation, MSEpx^, x°q, and the mean squared error in the image domain, MSEpGpx^q, Gpx°qq, where we defined MSEpz, z1q " }z ´ z1}2. For the left panel, we fix the noise variance to 2 " 0.25, and vary k, and for the middle panel we fix k " 50 and vary the noise variance. The results show that, if the network is sufficiently expansive, guaranteed by k being sufficiently small, then in the noiseless case (2 " 0), the latent representation and image are perfectly recovered. In the noisy case, we achieve a MSE proportional to 2k{n, both in the representation and image domains.
We also observed that for the problem instances considered here, the negation trick in step 3-4 of Algorithm 1 is often not necessary, in that even without that step the algorithm typically converges to the global minimum. Having said this, in general the negation step is necessary, since there exist problem instances that have a local minimum opposite of x°.
6.2 DENOISING WITH A LEARNED PRIOR
We next consider a prior learned from data. Technically, for such a prior our theory does not apply since we assume the weights to be chosen at random. However, the numerical results presented in this section show that even for the learned prior we achieve the rate predicted by our theory pertaining to a random prior. Towards this goal, we consider a fully-connected autoencoder parameterized by k, consisting of an decoder and encoder with ReLu activation functions and fully connected layers. We choose the number of neurons in the three layers of the encoder as 784, 400, k, and those of the decoder as k, 400, 784. We set k " 10 and k " 20 to obtain two different autoencoders. We train both autoencoders on the MNIST (Lecun et al., 1998) training set.
We then take an image y° from the MNIST test set, add Gaussian noise to it, and denoise it using our method based on the learned decoder-network G for k " 10 and k " 20. Specifically, we estimate the latent representation x^ by running Algorithm 1, and then set y^ " Gpx^q. See Figure 2 for a few examples demonstrating the performance of our approach for different noise levels.
We next show that this achieves a mean squared error (MSE) proportional to 2k{n, as suggested by our theory which applies for decoders with random weights. We add noise to the images with noise variance ranging from 2 " 0 to 2 " 6. In the right panel of Figure 3 we show the MSE in the image domain, MSEpGpx^q, Gpx°qq, averaged over a number of images for the learned decoders with k " 10 and k " 20. We observe an interesting tradeoff: The decoder with k " 10 has fewer parameters, and thus does not represent the digits as well, therefore the MSE is larger than that for k " 20 for the noiseless case (i.e., for  " 0). On the other hand, the smaller number of parameters results in a better denoising rate (by about a factor of two), corresponding to the steeper slope of the MSE as a function of the noise variance, 2.
REFERENCES
S. Arora, Y. Liang, and T. Ma. Why are deep nets reversible: A simple theory, with implications for training. arXiv:1511.05653, 2015.
A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models. arXiv:1703.03208, 2017.
H. C. Burger, C. J. Schuler, and S. Harmeling. Image denoising: Can plain neural networks compete with BM3d? In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2392­ 2399, 2012.

7

Under review as a conference paper at ICLR 2019

mean squared error

0.03
0.02
0.01
0 0

noisy rep. noisy img.
20 40

0.6 0.4 0.2
0 60 0

noisy rep. noisy img.
10

0.6
k " 10 k " 20
0.4
0.2
20 0 2 4

6

k 2 2

Figure 3: Mean square error in the image domain, MSEpGpx^q, x°q, and in the latent representation, MSEpx^, x°q, as a function of the dimension of the latent representation, k, with 2 " 0.25 (left panel), and the noise variance, 2 with k " 50 (middle panel). As suggested by the theory pertaining to decoders with random weights, if k is sufficiently small, and thus the network is sufficiently expansive, the denoising rate is proportional to 2k{n. Right panel: Denoising of handwritten digits based on a learned decoder with k " 10 and k " 20, along with the least-squares fit as dotted lines. The learned decoder with k " 20 has more parameters and thus represents the images with a smaller error; therefore the MSE at  " 0 is smaller. However, the denoising rate for the decoder with k " 20, which is the slope of the curve is larger as well, as suggested by our theory.

C. Clason. Nonsmooth analysis and optimization. arXiv:1708.04180, 2017.
K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-D transformdomain collaborative filtering. IEEE Transactions on Image Processing, 16(8):2080­2095, 2007.
D. L. Donoho. De-noising by soft-thresholding. IEEE Transactions on Information Theory, 41(3): 613­627, 1995.
M. Elad and M. Aharon. Image denoising via sparse and redundantd representations over learned dictionaries. IEEE Transactions on Image Processing, 15(12):3736­3745, 2006.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pp. 2672­2680. 2014.
P. Hand and V. Voroninski. Global guarantees for enforcing deep generative priors by empirical risk. In Conference on Learning Theory, 2018. arXiv:1705.07576.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504­507, 2006.
T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. arXiv: 1710.10196, October 2017.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
A. Lucas, M. Iliadis, R. Molina, and A. K. Katsaggelos. Using deep neural networks for inverse problems in imaging: Beyond analytical methods. IEEE Signal Processing Magazine, 35(1): 20­36, 2018.
M. Mardani, H. Monajemi, V. Papyan, S. Vasanawala, D. Donoho, and J. Pauly. Recurrent generative adversarial networks for proximal learning and automated compressive image recovery. arXiv:1711.10046, 2017.
Jean-Luc Starck, E. J. Candes, and D. L. Donoho. The curvelet transform for image denoising. IEEE Transactions on Image Processing, 11(6):670­684, 2002.
D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep Image Prior. arXiv:1711.10925, 2017.
K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising. IEEE Transactions on Image Processing, 26(7):3142­3155, 2017.

8

Under review as a conference paper at ICLR 2019

A PROOFS

In this section we prove our main result, Theorem 1. Instead of proving Theorem 1 as stated, we will prove the following equivalent rescaled statement for when Wi have i.i.d. N p0, 1{niq entries. Because of this rescaling, Gpxq scales like 2´d{2}x}, the noise  is assumed to scale like 2´d{2, f scales like 2d, and  scales like 2d.
Theorem 2. Consider a network with the weights in the i-th layer, Wi P Rni^ni´1 , i.i.d. N p0, 1{niq distributed, and suppose that the network satisfies the expansivity condition for some  K{d90. Also, suppose that the noise variance obeys





}x°}K12´d{2 d16

,

c

 :"

182

k n

logpn1dn2d´1

.

.

.

ndq.

Consider

the

iterates

of

Algorithm

1

with

stepsize



"

K4

2d d2

.

Then,

there

exists

a

number

of

steps

N upper bounded by

N



K2 d4

f px0q2d }x°}

such that after N steps, the iterates of Algorithm 1 obey

}xi ´ x°}  K5d9}x°}? ` K6d62d{2, for all i  N ,

(4)

with

probability

at

least

1

´

2e´2k

log

n

´

d
i"2

8ni e´K7 ni´2

´

8n1e´K7

2 logp1{

qk .

Here, K1, K2, ..

are numerical constants, and x0 is the initial point in the optimization.

As mentioned in Section 4.1, our proof makes use of a deterministic condition, called the Weight Distribution Condition (WDC), formally defined in Section 4.1. The following proposition establishes that the expansivity condition ensures that the WDC holds:
Lemma 3 (Lemma 9 in (Hand & Voroninski, 2018)). Fix P p0, 1q. If the entires of Wi P Rni^ni´1 are i.i.d. N p0, 1{niq and the expansivity condition ni  c ´2 logp1{ qni´1 log ni´1 holds, then Wi satisfies the WDC with constant with probability at least 1 ´ 8nie´K .2ni´1 Here, c and K are numerical constants.

We note that the form of dependence of ni on can be read off the proofs of Lemma 10 in (Hand

& Voroninski, 2018). It follows from Lemma 3, that the WDC holds for all Wi with probability at

least

1

´

d
i"2

8ni e´K7 ni´2

´ 8n1e´K7

2 logp1{

qk .

In the remainder of the proof we work on the event that the WDC holds for all Wi.

A.1 PRELIMINARIES

Recall that the goal of our algorithm is to minimize the empirical risk objective

f pxq

"

1 2 }Gpxq

´

y}2,

where y :" Gpx°q ` , with  ,, N p0, 2{nIq.

Our results rely on the fact that outside of two balls around x " x° and x " ´dx°, with d a constant defined below, the direction chosen by the algorithm is a descent direction, with high
probability. Towards this goal, we use a concentration argument, similar to the arguments used in (Hand & Voroninski, 2018). First, define x :" i1"dWi,`,x (with Wi,`,x defined in Section 3) for notational convenience, and note that the step direction of our algorithm can be written as

v~x " vx ` q¯x, with vx :" txxx ´ pxqtpx° qx°, and q¯x :" tx. Note that at points x where G (and hence f ) is differentiable, we have that v~x " f pxq.

(5)

The proof is based on showing that v~x concentrates around a particular hx P Rk, defined below, that is a continuous function of nonzero x, x° and is zero only at x " x° and x " ´dx°. The

9

Under review as a conference paper at ICLR 2019

definition of hx depends on a function that is helpful for controlling how the operator x ÑÞ W`,xx distorts angles, defined as:

gpq

:"

´ cos´1

p

´

q

cos 



`

sin



¯ .

With this notation, we define

hx

:"

´

1 2d

´d´1
i"0



´ 

i

¯ x°

`

1 2d

« x

´

d´1
ÿ
i"0

sin i 

´

d´1


j"i`1



´ 

j

¯

}x°}2 }x}2

ff x

,

(6)

where 0 " =px, x°q and i " gpi´1q. Note that hx is deterministic and only depends on x, x°, and the number of layers, d.

In order to bound the deviation of v~x from hx we use the following two lemmas, bounding the deviation controlled by the WDC and the deviation from the noise:

Lemma 4 (Lemma 6 in (Hand & Voroninski, 2018)). Suppose that the WDC holds with

1{p16d2q2. Then, for all nonzero x, x° P Rk,

d3? }vx ´ hx}2  K 2d maxp}x}2, }x°}2q, and

@pi1"dWi,`,xqx, pi1"dWi,`,x° qx°D



1 4

1 2d }x}2}x°}2,

and

>>1i"dWi,`,x>>2



1 2d p1 `

2

qd



13 2´d. 12


(7) (8) (9)

Proof. Equation (7) and (8) are Lemma 6 in (Hand & Voroninski, 2018). Regarding (9), note that the WDC implies that }Wi,`,x}2  1{2 ` . It follows that

>>i1"dWi,`,x

>2 >



1 2d p1 ` 2

qd

"

1 2d

ed

logp1`2

q



1`4 2d

d



13 2´d, 12

where the last inequalities follow by our assumption on .

Lemma 5. Suppose the WDC holds with  1{p16d2q2, that any subset of ni´1 rows of Wi are linearly independent for each i, and that  ,, N p0, 2{nIq. Then the event

c

Enoise

:"

!>>pi1"dWi,`,xqt>>



 2d{2 ,

) for all x ,

 :"

16

k n

logpn1dnd2´1

.

.

.

ndq

(10)

holds with probability at least 1 ´ 2e´2k log n.

As the cost function f is not differentiable everywhere, we will make use of the generalized sub-

differential in order to reference the subgradients at nondifferentiable points. For a Lipschitz

function f~ defined from a Hilbert space X to R, the Clarke generalized directional derivative of f~ at the point x P X in the direction u, denoted by f~opx; uq, is defined by f~opx; uq "

lim supyÑx,tÓ0

f~py`tuq´f~pyq t

,

and

the

generalized

subdifferential

of

f~

at

x,

denoted

by

Bf~pxq,

is

defined by

Bf~pxq " tv P Rk | xv, uy  f~opx; uq, for all u P X u.

Since f pxq is a piecewise quadratic function, we have

Bf pxq " convpv1, v2, . . . , vtq,

(11)

where conv denotes the convex hull of the vectors v1, . . . , vt, t is the number of quadratic functions adjoint to x, and vi is the gradient of the i-th quadratic function at x.

Lemma 6. Under the assumption of Lemma 5, and assuming that Enoise holds, we have that, for any

x  0 and any vx P Bf pxq,

d3?



}vx ´ hx}  K 2d maxp}x}2, }x°}2q ` 2d{2 .

In particular, this holds for the subgradient vx " v~x.

10

Under review as a conference paper at ICLR 2019

Proof. By (11), Bf pxq " convpv1, . . . vtq for some finite t, and thus vx " a1v1 ` . . . atvt for some

a1, . . . , at



0,


i

ai

"

1.

For

each

vi,

there

exists

a

w

such

that

vi

"

limtÓ0

v~x`tw .

On

the

event

Enoise, we have that for any x  0, for any v~x P Bf pxq

}v~x ´ hx} " }vx ` q¯x ´ hx}

 }vx ´ hx} ` }q¯x}

d3?



 K 2d maxp}x}2, }x°}2q ` 2d{2 ,

where the last inequality follows from Lemmas 4 and 5 above. The proof is concluded by appealing

to the continuity of hx with respect to nonzero x, and by noting that

ÿ d3?



}vx ´ hx}  ai}vi ´ hx}  K
i

2d

maxp}x}2, }x°}2q ` 2d{2 ,

where

we

used

the

inequality

above

and

that


i

ai

"

1.

We will also need an upper bound on the norm of the step direction of our algorithm:

Lemma 7. Suppose that the WDC holds with  1{p16d2q2 and that the event Enoise holds with





.2´d{2 }x° }
8

Then,

for

all

x,

dK }v~x}  2d maxp}x}, }x°}q,

(12)

where K is a numerical constant.

Proof.

Define for convenience j

"

d´1
i"j

´¯j,x,x° 

.

We

have

}v~x} }hx} ` }hx ´ v~x}



>

>1

>

> >

2d

x

´

1 2d

0x°

´

1 2d

d´1
ÿ
i"0

sin ¯i,x 

i`1

}x°} }x}

> > x>> >

`

K1

d3? 2d

 maxp}x}2, }x°}2q ` 2d{2

1 ^1 d 

d3?



 2d }x} ` 2d ` 2d }x°} ` K1 2d maxp}x}, }x°}q ` 2d{2

dK  2d maxp}x}, }x°}q,

where the second inequality follows from the definition of hx and Lemma 6, the third inequality

uses

|j |



1,

and

the

last

inequality

uses

the

assumption





.2´d{2 }x° }
8

A.2 PROOF OF THEOREM 2

We are now ready to prove Theorem 2. The logic of the proof is illustrated in Figure 4. Recall that xi is the ith iterate of x as per Algorithm 1. We first ensure that we can assume throughout that xi is bounded away from zero:

Lemma 8. Suppose that WDC holds with  1{p16d2q2 and that Enoise holds with  in (10)

obeying





.2´d{2 }x° }
8

Moreover,

suppose

that

the

step

size

in

Algorithm

1

satisfies

0







K 2d d2

,

where

K

is

a

numerical

constant.

Then,

after

at

most

N

"

p

38K0 

2d

q2

steps,

we

have

that

for

all

i



N

that

xi

R

Bp0, K0}x°}q,

K0

"

1 32

.

In particular, if  " K2d{d2, then N is bounded by a constant times d4.

We can therefore assume throughout this proof that xi

R

Bp0, K0}x°}q, K0

"

1 32

.

We prove

Theorem 2 by showing that if }hx} is sufficiently large, i.e., if the iterate xi is outside of set

S

"

! x

P

Rk

|

}hx}



1 2d 

) maxp}x}, }x°}q ,

11

Under review as a conference paper at ICLR 2019

S´ ´dx°

0

S` x°

Figure 4: Logic of the proof: Starting at an arbitrary point, Algorithm 1 moves away from 0,
at least till its iterates are outside the gray ring, as 0 is a local maximum; and once an iterate xi leaves the gray ring around 0, all subsequent iterates will never be in the white circle around 0 again
(see Lemma 8). Then the algorithm might move towards ´dx°, but once it enters the dashed ball around ´dx°, it enters a region where the function value is strictly larger than that of the dashed ball around x°, by Lemma 10. Thus steps 3-5 of the algorithm will ensure that the next iterate xi is in the dashed ball around x°. From there, the iterates will move into the region S`, since outside of S` Y S´ the algorithm chooses a descent direction in each step (see the argument around equation (16)). The region S` is covered by a ball of radius r, by Lemma 9, determined by the noise and .

with

 " 4Kd3? ` 132d{2{}x°},

(13)

then the algorithm makes progress in the sense that f pxi`1q´f pxiq is smaller than a certain negative value. The set S is contained in two balls around x° and ´x°, whose radius is controlled by :

Lemma 9.

For any 



,1
642 d12

S  Bpx°, 5000d6}x°}2q Y Bp´dx°, 500d11a}x°}2q.

(14)

Here, d  0 is defined in the proof and obeys d Ñ 1 as d Ñ 8.

Note that by the assumption 



}x° } K1 2´d{2 d16

and Kd45?

 1, our choice of  in (13) obeys





1 642 d12

for

sufficiently

small

K1, K,

and

thus

Lemma

9

yields:

b S  Bpx°, rq Y Bp´dx°, r}x°}d8q.

were we define the radius r " K2d9? }x°} ` K3d62d{2, where K2, K3 are numerical constants. Note that hat the radius r is equal to the right hand side in the error bound (4) in our theorem. In
order to guarantee that the algorithm converges to a ball around x°, and not to that around ´dx°, we use the following lemma:

Lemma 10. Suppose that the WDC holds with  1{p16d2q2. Moreover suppose that Enoise holds,

and that  in the event Enoise obeys

 2´d{2 }x° }2



K9{d2, where K9



1 is a universal constant.

Then for any d P rd, 1s, it holds that

f pxq  f pyq

(15)

for all x P Bpdx°, K3d´10}x°}q and y P Bp´dx°, K3d´10}x°}q, where K3  1 is a universal constant.

In order to apply Lemma 10, define for convenience the two sets:
S` :"S X Bpx°, rq, and b
S´ :"S X Bp´dx°, r}x°}d8q.

12

Under review as a conference paper at ICLR 2019

By the assumption that Kd45?  1 and   K1d´162´d{2}x°}, we have that for sufficiently small K1, K,
S`  Bpx°, K3d´10}x°}q and S´  Bp´dx°, K3d´10}x°}q.

Thus, the assumptions of Lemma 10 are met, and the lemma implies that for any x P S´ and y P S`, it holds that f pxq  f pyq. We now show that the algorithm converges to a point in S`. This fact and the negation step in our algorithm (line 3-5) establish that the algorithm converges to a point in S` if we prove that the objective is nonincreasing with iteration number, which will form the remainder of this proof.
Consider i such that xi R S. By the mean value theorem (Clason, 2017, Theorem 8.13), there is a t P r0, 1s such that for x^i " xi ´ tv~xi there is a vx^i P Bf px^iq, where Bf is the generalized subdifferential of f , obeying

f pxi ´ v~xi q ´ f pxiq "xvx^i , ´v~xi y "xv~xi , ´v~xi y ` xvx^i ´ v~xi , ´v~xi y  ´ }v~xi }2 ` }vx^i ´ v~xi }}v~xi } " ´ }v~xi }p}v~xi } ´ }vx^i ´ v~xi }q.

(16)

In the next subsection, we guarantee that for any t P r0, 1s, vx^i with x^i " xi ´ tv~xi is close to v~xi :

^ 5 d2  }vx^i ´ v~xi }  6 ` K7 2d }v~xi }, for all vx^i P Bf px^iq.

(17)

Applying (17) to (16) yields

f

pxi

´

v~xi

q

´

f

pxiq



´

1 12

}v~xi

}22,

where

we

used

that

K7

d2 2d



1 12

,

by

our

assumption

on

the

stepsize



being

sufficiently

small.

Thus, the maximum number of iterations for which xi R S is f px0q12{p mini }v~xi }2q. We next lower-bound }v~xi }. We have that on Enoise, for all x R S, with  given by (13).

}v~x}2  }hx} ´ }hx ´ v~x}



2´d

maxp}x},

´ }x°}q 

´

K1d3?

2d{2 ¯ ´  }x°}



2´d

maxp}x},

^ }x°}q 3K

d3?

2d{2  ` 12 }x°}

 2´d}x°}3Kd3?

(18)

where the second inequality follows by the definition of S and Lemma 6, and the third inequality follows from our definition of  in (13). Thus,

f pxi ´ v~xi q ´ f pxiq  ´K52´2dd6 }x°}2  ´2´dd4K6 }x°}2

where

we

used



"

K4

2d d2

.

Hence,

there

can

be

at

most

f px0q2d K6d4 }x°}2

iterations

for

which

xi

R

S .

In order to conclude our proof, we remark that once xi is inside a ball of radius r around x°, the iterates do not leave a ball of radius 2r around x°. To see this, note that by (12) and our choice of stepsize,
K }v~xi }  d maxp}xi}, }x°}q.

This concludes our proof.

The remainder of the proof is devoted to prove the lemmas used in this section.

13

Under review as a conference paper at ICLR 2019

A.3 PROOF OF EQUATION (17)

Our proof relies on hx being Lipschitz, as formalized by the lemma below, which is proven in Section A.9:
Lemma 11. For any x, y R Bp0, K0}x°}q, where K0 and K4 are numerical constants,

}hx

´

hy }



K4d2 2d

}x

´

y}.

By Lemma 11, for all t P r0, 1s and i  N (recall that by Lemma 8, after at most N steps,

xi  Bp0, K0}x°}q):

}hx^i

´ hxi }



K4d2 2d

}x^i

´ xi},

(19)

where x^i " xi ´ tv~xi . Thus, we have that on Enoise, for any vx^i P Bf px^iq by Lemma 6,

}vx^i ´ v~xi } }vx^i ´ hx^i } ` }hx^i ´ hxi } ` }hxi ´ v~xi }

d3? K1 d32?d K1 2d
d3? K1 2d

maxp}x^i maxp}xi

 }, }x°}q ` 2d{2 } ` }v~xi }, }x°

` }q

K4d2 2d

}x^i

´

xi}

`

K1

d3? 2d

`

K4d2 2d

}v~xi }

`

K1

d3? 2d

 maxp}xi}, }x°}q ` 2d{2
 maxp}xi}, }x°}q ` 2 2d{2

^ 2

`

dK 2d



maxp}xi},

}x°}q

`

K4d2 2d

}v~xi

}

`

2

K9{d2 2d

}x°}

(20)

where the second inequality is from Lemma 6 and Equation 19, and the fourth inequality is from (12)

and the assumption

 2´d{2 }x° }2

 K9{d2.

Combining (20) and (18), we get that

^ 5 d2  }vx^i ´ v~xi }  6 ` K7 2d }v~xi },

with the appropriate constants chosen sufficiently small. This concludes the proof of Equation (17).

A.4 PROOF OF LEMMA 8

First suppose that xi P Bp0, 2K0}x°}q. We show that after a polynomial number of iterations N , we have that xi`N R Bp0, 2K0}x°}q. Below, we prove that

x, v~x



0

and

}v~x}



1 2d16 }x°}

for

all

x

P

Bp0, 2K0}x°}q.

(21)

It follows that for any xi P Bp0, 2K0}x°}q, xi and the next iterate produced by the algorithm, xi`1 " xi ´ v~xi , form an obtruse triangle. As a consequence,

}xi`1}2  }xi}2 ` 2}v~xi }2



}xi}2

`

2

1 p2d16q2

}x°}2,

where the last inequality follows from (21). Thus, the norm of the iterates xi will increase until after

` 2K02d16 2


iterations,

we

have

xi`N

R

Bp0, 2K0}x°}q.

The proof of the lemma is concluded by showing that

xi R Bp0, 2K0}x°}q implies xi`1 R Bp0, K0}x°}q

(22)

As a consequence, in a polynomial number N of steps, for each iterate, we have that xi R Bp0, K0}x°}q, for all i  N , as claimed.

14

Under review as a conference paper at ICLR 2019

We next prove the implication (22). Consider xi R Bp0, 2K0}x°}q, and note that

}xi`1} " }xi ´ v~xi }  }xi} ´ }v~xi }

dK  }xi} ´  2d maxp}xi}, }x°}q



}xi}

´

dK  2d

}xi} 2K0

1  }xi} ´ 2 }xi}

where the second inequality follows from (12), the third inequality from }xi}  2K0}x°}, and finally the last inequality from our assumption on the stepsize . This concludes the proof of (22).

Proof of (21): It remains to prove (21). We start with proving x, v~x  0. For brevity of notation,

let

z

"

1
i"d

Wi,`,z .

We

have

xT v~x " Tx xx ´ xT x° x° ` xT , x



13 12

2´d

}x}2

´

1 4

1 2d

}x}}x°}

`

}x}

 2d{2

}x}

^

13 12

2´d

}x}

`

1{p8q 2d

}x°}

´

1 4

1 2d

 }x°}

1^ 1  }x} 2d 2}x} ´ 8 }x°} .

The first inequality follows from (8) and (9), and the second inequality follows from our assumption

on

.

Therefore,

for

any

x

P

Bp0,

1 16

}x°}q,

x, v~x

 0, as desired.

We

next

show

that,

for

any

x

P

Bp0,

1 16

}x°}q

}v~x} "}xT xx ´ Tx x° x° ` Tx }  }xT x° x°} ´ }Tx xx} ´ }xT }

1 1 13 1 w  4 2d }x°} ´ 12 2d }x} ´ 2d{2

1^1 1   2d 8 ´ 16 }x°}.

where the second inequality is from (8) and (9). This concludes the proof of (21).

A.5 PROOF OF LEMMA 5

Let x " i1"dWi,`,x. We have that }q¯x}2 " >>tx>>2  }x}2}Px }2,

where Px is a projector onto the span of x. As a consequence, }Px }2 is 2-distributed random variable with k-degrees of freedom scaled by {n. A standard tail bound (see (?, p. 43)) yields that,

for any   k,

P

" }Px

}2



i 4



2e´ .

Next, we note that by applying Lemmas 13-14 from (Hand & Voroninski, 2018, Proof of Lem. 15))4, with probability one, that the number of different matrices x can be bounded as

| tx|x  0u | " | 1i"dWi,`,x|x  0( |  10d2 pn1dnd2´1 . . . ndqk  pnd1n2d´1 . . . ndq2k,

where the second inequality holds for logp10q  k{4 logpn1q. To see this, note that pnd1nd2´1 . . . ndqk  10d2 is implied by kpd logpn1q ` pd ´ 1q logpn2q ` . . . logpndqq  kd2{4 logpn1q  d2 logp10q. Thus, by the union bound,

P

" }Px



}2



16k logpnd1n2d´1 . . . ndq,

for

all

i x



1 ´ 2e´2k logpnq,

4The proof in that argument only uses the assumption of independence of subsets of rows of the weight matrices.

15

Under review as a conference paper at ICLR 2019

where n " nd.

Recall from (9) that }x} 

13 12

.

Combining this inequality with }q¯x}2



}x}2}Px }2 concludes the proof.

A.6 PROOF OF LEMMA 9

We now show that hx is away from zero outside of a neighborhood of x° and ´dx°. We prove

Lemma 9 by establishing the following:

Lemma 12.

Suppose

64d6

? 

 1. Define

d

:"

d´1
ÿ

sin qi 

~

d´1


¸  ´ qj ,


i"0 j"i`1

where q0 "  and qi " gpqi´1q. If x P S, then we have that either |0|  32d4 and |}x}2 ´ }x°}2|  132d6}x°}2
or |0 ´ |  8d4a and |}x}2 ´ }x°}2d|  200d7? }x°}2. In particular, we have
S  Bpx°, 5000d6}x°}2q Y Bp´dx°, 500d11a}x°}2q. Additionally, d Ñ 1 as d Ñ 8.

(23)

Proof. Without loss of generality, let }x°} " 1, x° " e1 and x^ " r cos 0 ¨ e1 ` r sin 0 ¨ e2 for 0 P r0, s. Let x P S.

First we introduce some notation for convenience. Let



"

d´1




´ 

i

,



"

d´1
ÿ

sin i 

d´1


 ´ j , 

r " }x}2,

M " maxpr, 1q.

i"0 i"0 j"i`1

Thus, hx

"

´

1 2d

x^0

`

1 2d

pr

´

 qx^.

By inspecting the components of hx,

we have that x

P

S

implies

| ´  ` cos 0pr ´ q|  M | sin 0pr ´ q|  M

(24) (25)

Now, we record several properties. We have:

i P r0, {2s for i  1

i  i´1 for i  1 ||  1

d ||   sin 0

3 qi  i ` 3 for i  0
 qi  i ` 1 for i  0



"

d´1




´ i 





´ 0 d´3 

i"0

0 "  ` O1pq ñ i " qi ` O1piq

 0 "  ` O1pq ñ ||  

0

"



` O1pq

ñ



"

d

`

O1p3d3q

if

d2 



1

(26) (27) (28) (29)
(30) (31) (32) (33)

16

Under review as a conference paper at ICLR 2019

We

now

establish

(28).

Observe

0



gpq



`1
3

`

1 ´1


":

g~pq

for



P

p0, s.

As

g

and

g~

are

monotonic

increasing,

we

have

qi

"

gipq0q

"

gipq



g~ipq

"

`i
3

`

1 ´1


"

3 i`3

.

Similarly,

gpq



p

1 

`

1 

q´1

implies

that

qi



 i`1

,

establishing

(29).

We now establish (30). Using (28) and i  qi, we have

d´1´ 1
i"1

´

i ¯ 



d´1´ 1
i"1

´

i

3 `

¯ 3



d´3,

where the last inequality can be established by showing that the ratio of consecutive terms with respect to d is greater for the product in the middle expression than for d´3.

We establish (31) by using the fact that |g1pq|  1 for all  P r0, s and using the same logic as for (Hand & Voroninski, 2018, Eq. 17).

We

now

establish

(33).

As

0

"



` O1pq,

we

have

i

"

qi

` O1piq.

Thus,

if

d2  



1,

d´1


 ´ j 

"

d´1


´

´ qj 

i ¯ ` O1p 2 q

"

´

d´1




´ 

qj

¯

`

O1pd2



q

j"i`1

j"i`1

j"i`1

So



"

dÿ´1´ sin qi 

i ¯"´ ` O1p  q

d´1




´ 

qj

¯

`

O1pd2

i q

i"0 j"i`1

´¯ " d ` O1 d2{ ` d3{ ` d42{

(34) (35)

" d ` O1p3d3q.

(36)

Thus (33) holds.

Next, we establish that x P S ?ñ r  4d, and thus?M  4d. Suppose r? 1. At least one of the

following h?olds: | sin 0| |r ´ |  2r. Using
(24) implies that |r ´ |
?

 1{ 2 or | cos 0|  1{ 2. If | sin 0|

(27), we get r ?



d?{ 1´ 2



d{2 if 



 2pr ` ||q. Using (26), (27), and 

 1{ 2 then (25) imp?lies that

1{4. If | cos 0|  1{ 2, then

?

 1{4, we get r 

2|?|` 1´ 2



d`? 2 1´ 2



4d.

Thus,

we

have

x

P

S

ñ

r



4d

ñ

M



4d.

Next, we establish that we only need to consider the small angle case (0 « 0) and the large angle case (0 « ), by considering the following three cases:

(Case I) sin 0  16d4: We have 0 " O1p32d4q or 0 " `O1p32d4q, as 32d4  1.

??

(Case II) |r ´ |  (30), we get 0 " 

`OM1p:2Apdp3l?yinMg cqa.se

II

to

inequality

(24)

yields

||



2

M . Using

(Case

III)

sin 0



16d4

and

|r

´

|



? M:

Finally,

consider

Case

III.

By

(25),

we

have |r ´ |



M .
sin 0

Using this inequality in (24), we have ||



M

`

M sin 0



2M sin 0



1 8

d´4

M



1 2

d´3,

where the second to last inequality uses sin 0



16d4

and the last

inequality uses M



4d.

By (30), we have ?

´0 

d´3







1 2

d´3,

which implies ?

that

0  {2. No?w, as |r ´ |  M , then by (25), we have | sin 0|  . Hence,

0 "  ` O1p2 q, as 0  {2 and as   1.

At least one of the Cases I,II, or III hold. Thus, we case 0 " O1p32d4q or the large angle case 0 " 

s`eeOth1pa8t itds4u?fficqe.s

to

consider

the

small

angle

Small Angle Case. Assume 0 " O1pq with  " 32d4. As i  0   for all i, we have

1







p1

´

 

qd

"

1

`

O1

p

2d 

q

provided

d{



1{2

(which

holds

by

our

choice



"

32d4

by

17

Under review as a conference paper at ICLR 2019

assumption

64d6

? 



1).

By

(27),

we

also

have



"

O1

p

d 

q.

By

(24),

we

have

| ´  ` cos 0pr ´ q|  M.

Thus, as cos 0 " 1 ` O1p20{2q " 1 ` O1p2{2q,

´ 2d ¯

2d d

´ 1 ` O1p  q ` p1 ` O1p  qqpr ` O1p  qq " O1p4dq,

and r  M  4d (shown above) provides,

2d d 2d

22d2

r ´ 1 " O1p4d `  `  `  4d ` 2 q

" O1p4d ` 4d2q.

(37) (38)

By plugging in that  " 32d4, we have that r ´ 1 " O1p132d6q, where we have used that

32d5  

 1{2.

Large Angle  " O1p{q,

Case. Assume 0 "  ` O1pq and we have  " d ` O1p3d3q

iwf 8hde6re?

" 

8d4?. By (32) and 1. By (24), we have

(33),

we

have

| ´  ` cos 0pr ´ q|  M,

so, as cos 0 " 1 ´ O1p02{2q,

O1p{q ` p1 ` O1p2{2qqpr ´ d ` O1p3d3qq " O1pM q,

and

thus,

using

r



4d,

d



d,

and



"

8d4

? 



1,

r

´

d

"

O1pM

`

{

`

3d3

`

5 2d 2

`

3 2

d3

3q

"

´ O1 4d

`

1 p 

`

3d3

`

5 d
2

`

3 2

¯ d3q

" O1p200d7aq

(39) (40) (41)

To conclude the proof of (23), we use the fact that

}x

´

x°}2



}x}2

´

}x°

 }2

`

p}x°}2

`

}x}2

´

}x°}2q0.

This fact simply says that if a 2d point is known to have magnitude within r of some r and is known to be within angle  from 0, then its Euclidean distance to the point of polar coordinates pr, 0q is no more than r ` pr ` rq.

Finally,

we

establish

that

d

Ñ

1

as

d

Ñ

8.

Note

that

d`1

"

p1 ´

qd 

qd

`

sin qd 

and

0

"

0.

It suffices to show ~d Ñ 0, where ~d :" 1 ´ d. The following recurrence relation holds: ~d "

p1

´

qd´1 

q~d´1

`

,qd´1´sin qd´1


with

~0

"

1.

Using

the

recurrence

formula

(Hand

&

Voroninski,

2018, Eq. (15)) and the fact that q0 " , we get that

~d

"

d
ÿ

qi´1

´ sin qi´1 

d


`1 ´

qj´1  

i"1 j"i`1

(42)

using (29), we have that

d
´ 1
j"i`1

´

qj´1 ¯ 



d
´ 1
j"i`1

´

1¯ j

"

d
´ÿ exp ´
j"i`1

1¯ j



´  d`1 exp ´
i`1

1¯ ds
s

"

i`1 d`1

Using

(28)

and

the

fact

that

qi´1

´

sin qi´1



qi3´1{6,

we

have

that

~d



d
i"1

qi3´1 6

¨

i`1 d`1

Ñ

0

as

d Ñ 8.

18

Under review as a conference paper at ICLR 2019

A.7 PROOF OF LEMMA 10

Consider the function

fpxq " f0pxq ´ xGpxq ´ Gpx°q, y,

and note that f pxq " fpxq ` }}2. Consider x P Bpdx°, }x°}q, for a  that will be specified later. Note that

| Gpxq ´ Gpx°q,  |  | 1i"dWi,`,xx,  | ` | i1"dWi,`,x° x°,  |

" | x, p1i"dWi,`,xqt | ` | x°, p1i"dWi,`,x° qt | 
 p}x} ` }x°}q 2d{2 
 p}x°} ` }x°}q 2d{2 ,

where the second inequality holds on the event Enoise, by Lemma 5, and the last inequality holds by our assumption on x. Thus, for x P Bpdx°, }x°}q

fpxq Ef0pxq ` |f0pxq ´ Ef0pxq| ` | Gpxq ´ Gpx°q,  |



1 2d`1

^ d2

p1 ` 4 ` 2d

´

2d

`

10 K23

 d

}x°}2

dq }x}2 `

p1 ` 4 dq ` 2d`1

1 ` 2d`1 48d3?

}x°}2 }x}}x°}

`

p1

`4 2d

dq }x°}2



` p}x°} ` }x°}q 2d{2



1 2d`1

^ 2d

p1 ` 4 ` 2d

´ 2d ` dq pd `

10  K23 d }x° q2}x°}2 `

}2

`

1 2d`1

}x°}2

p1 ` 4 dq ` 48d3

2d`1

?

pd ` q}x°}2 `

p1

`4 2d

dq }x°}2



` p}x°} ` }x°}q 2d{2



}x°}2 2d`1

^ 1

`

2d

´

2d

`

10 K23

d

` 68d2?

 `

p}x°}

 ` }x°}q 2d{2

?

where the last inequality follows from  , d  1, 4 d  1,   1 and assuming  " .

(43)

Similarly, we have that for any y P Bp´dx°, }x°}q

fpyq Erf pyqs ´ |f pyq ´ Erf pyqs| ´ | Gpxq ´ Gpx°q,  |

1  2d`1
^
´

`d2 ´ 2dd ´ 10d3

p1

`4 2d

dq }y}2

`

p1 `

}4x2d°dq}`2`1`482dd13`?1 }x}°y}}2}x°}

`

p1 ` 4 2d

dq

}x°

 }2

 ´ p}x°} ` }x°}q 2d{2

 ?

}x°}2 2d`1

`1

` 2d

´ 2dd

´ 10d3 ´ 68d2?

 ´

p}x°}

 ` }x°}q 2d{2

(44)

Using  , d  1, 4 d  1,   1 and assuming  " , the right side of (43) is smaller than

the right side of (44) if

¨ 2

We can establish that:

"



d ´ dd ´

´

13}}2 ¯,

.

125

`

5 K23

d3

(45)

Lemma 13. For all d  2, that 1{ `K1pd ` 2q2  1 ´ d  250{pd ` 1q.

Thus, it suffices to have  "

"

K3 d10

and 13}}2



K9 d2



1 K2 2 K1pd`2q2

for an appropriate universal

constant K9, and for an appropriate universal constant K3.

19

Under review as a conference paper at ICLR 2019

A.8 PROOF OF LEMMA 13

It holds that

}x ´ y}  2 sinpx,y{2q minp}x}, }y}q,

@x, y

(46)

sinp{2q  {4,

@ P r0, s

(47)

d d gpq P r0, 1s logp1 ` xq  x

@ P r0, s @x P r´0.5, 1s

(48) (49)

logp1 ´ xq  ´2x

@x P r0, 0.75s

(50)

where x,y " =px, yq. We recall the results (36), (37), and (50) in (Hand & Voroninski, 2018):

i



3 i`3

and

i



i

 `1

@i  0

1

´

d

"

d´1


^ 1

´

i 



`

d´1
ÿ

i

´ sin 

i

d´1


^ 1

´

j 



.

i"1 i"1 j"i`1

Therefore, we have for all 0  i  d ´ 2,

d´1


^ 1

´

j





d´1


^ 1´

1

 i ` 2d´1 " e p q  e  e " ,j"i`1

log

1´

j

1 `1

´

d´1
j"i`1

1 j`1

´

d
i`1

1 s`1

ds

j"i`1


j"i`1

j`1

d`1

d´1


^ 1

´

j





d´1


^ 1´

3



" e p qd´1 j"i`1

log

1´

j

3 `3

 e´

d´1
j"i`1

6 j`3

 e´

d´1
i

6 s`3

ds

" ^ i ` 3 6 ,

j"i`1


j"i`1

j`3

d`2

where the second and the fifth inequalities follow from (49) and (50) respectively. Since 3{p12pi ` 1q3q  i3{12  i ´ sin i  i3{6  273{p6pi ` 3q3q, we have that for all d  3

1´

d

2 d ` 1

`

d´1
ÿ
i"1

273 6pi ` 3q3

i`2 d`1



2 d`

1

`

35 4pd ` 1q



250 d`1

and

1

´

d

^  pd

3 `

6 2q

`

d´1
ÿ
i"1

3 12pi `

3q3

^ i ` 3 6 d`2



1 K1pd `

2q2 ,

where

we

use

8
i"4

1 i2



2 6

and

n
i"1

i3

"

Opn4q.

A.9 PROOF OF LEMMA 11

To establish Lemma 11, we prove the following:

Lemma 14. For all x, y  0,

}hx

´

hy }



^1 2d

`

6d ` 4d2 2d

^1 max ,
}x}

1  }y} }x°} }x

´

y}

Lemma

11

follows

by

noting

that

if

x, y

R

Bp0, r}x°}q,

then

}hx

´

hy }



´
1 2d

`

6d`4d2 r2d

¯

}x

´

y}.

Proof of Lemma 14.

For brevity of notation, let j,z

"

d´1
i"j

.´¯i,z


Combining (46) and (47)

´¯

gives |¯0,x ´ ¯0,y|  4 max

1 }x}

,

1 }y}

}x ´ y}. Inequality (48) implies |¯i,x ´ ¯i,y|  |¯j,x ´ ¯j,y|

for all i  j. It follows that

11

}hx

´

hy }

 2d

}x

´

y}

`

2d

|0,x ´ 0,y| l jh n

}x°}

T1

1 ` 2d

dÿ´1 

 i"0

sin ¯i,x 

i`1,xx^

´

d´1
ÿ
i"0

sin ¯i,y 





i`1,y

y^ 

}x°

}.



l jh n

(51)

T2

20

Under review as a conference paper at ICLR 2019

By Lemma 15, we have

T1



d 

|¯0,x

´ ¯0,y|



4d 

^1 max ,
}x}

1 }y} }x ´

y}.

(52)

Additionally, it holds that

T2

"

dÿ´1 

 i"0

sin ¯i,x 

i`1,xx^

´

sin ¯i,x 

i`1,xy^

`

sin ¯i,x 

i`1,xy^

´

d´1
ÿ
i"0

sin ¯i,y 

  i`1,y y^ 

d 

}x^

´

y^}

`

dÿ´1 

 i"0

sin ¯i,x 

i`1,x

´

d´1
ÿ
i"0

sin ¯i,y 





i`1,y

 



.

l jh n

T3

We have

(53)

T3

d´1
ÿ 

,, 

sin

¯i,x





i`1,x

´

sin ¯i,x 



i`1,y

 



`

 

sin

¯i,x





i`1,y

´

sin ¯i,y 



i`1,y

 



i"0

d´1
ÿ 

,,1 

^d

´i´ 

1

¯i´1,x

´



¯i´1,y

 

`

1 |

sin

¯i,x

´

 sin ¯i,y|

i"0



d2 

|¯0,x

´

¯0,y |



4d2 

^1 max ,
}x}

1 }y} }x

´

y}.

(54)

Using (46) and (47) and noting }x^ ´ y^}  x,y yield

^1 1

}x^ ´ y^}  x,y  2 max

, }x} }y}

}x ´ y}.

(55)

Finally, combining (51), (52), (53), (54) and (55) yields the result.

Lemma 15. Suppose ai, bi P r0, s for i " 1, . . . , k, and |ai ´ bi|  |aj ´ bj|, @i  j. Then it holds

that

k  

i"1



´ 

ai

´

k

i"1



´ 



bi

 







k  |a1

´

b1|.

Proof. Prove by induction. It is easy to verify that the inequality holds if k " 1. Suppose the inequality holds with k " t ´ 1. Then

t 

i"1



´ ai 

t
 ´
i"1



´ 



bi

 





t



 

i"1



´ ai 

´



´ at 

t´1

i"1





´

bi

 



 

`

  



´ 

at

t´1

i"1



´ 

bi

´

t

i"1



´ 



bi

 





t 

´ 

1 |a1

´

b1|

`

1  |at

´

bt|



t  |a1

´

b1|.

21

