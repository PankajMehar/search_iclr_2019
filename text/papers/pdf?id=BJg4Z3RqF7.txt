Under review as a conference paper at ICLR 2019
UNSUPERVISED ADVERSARIAL IMAGE RECONSTRUC-
TION
Anonymous authors Paper under double-blind review
ABSTRACT
We address the problem of recovering an underlying signal from lossy and inaccurate measurements in an unsupervised fashion. Typically, we consider situations where there is no background knowledge on the structure of the unknown signal and where we do not have access to signal-measurement pairs, nor even unpaired signal data. We introduce a general framework, where a neural network is trained to recover plausible signals from the measurements in the data, by introducing an adversarial and a reconstruction loss. We evaluate our framework on different corruption instances, and show that our approach yields comparable results to model variants trained with stronger supervision.
1 INTRODUCTION
Many learning problems require acquiring information of the true state of some physical system from incomplete and noisy measurements. For example, weather forecasting relies on measurements acquired by different types of sensors. Data from these sensors are often incomplete and may be contaminated by external factors. For example, satellite sensors have to deal with the presence of clouds or with inherent errors relative to acquisition instruments, hindering the overall measurement process. In such cases, one needs to remove the effect of unwanted external factors and retrieve the true signal.
Recovering the signal and eliminating external factors from the measurements does not usually yield a unique solution, which means that multiple signals reconstructions could explain the measurements. For instance, if clouds are blocking the temperature acquisition, we could replace the missing temperature values with absurd ones, and this would explain the measurements just as accurately. One usually relies on some prior on the true signal in order to constrain the reconstruction to plausible solutions. A common approach is to hand-craft priors on the structure of the signal (Candès et al. (2005), Mota et al. (2017)). This approach is limited to model idealized situations in which the structure can be easily described, and which are rarely observed in the wild.
With the development of powerful generative models parameterized by neural networks (Goodfellow et al. (2014), Kingma & Welling (2013), Dinh et al. (2016)), a promising statistical approach to signal recovery is emerging, where priors are not handcrafted, but learned from large amounts of data. Despite exhibiting interesting results (Bora et al. (2017), Mardani et al. (2017), Ledig et al. (2016)), these methods all require some form of supervision, either measurement-signal pairs, or fully observed signal samples. For many practical problems, e.g. scientific data acquisition mentioned above, obtaining these samples is extremely expensive and/or impractical, which makes these approaches unrealistic for such situations.
In this work, we propose a general framework allowing to recover signals from noisy and lossy data, using only a dataset of lossy measurements. We compare our approach with baselines and variants of our model that have access to fully observed signal data, or to measurement-signal pairs.
2 PRELIMINARIES
Notations. We use capital letters (e.g. X) for random variables, and lower-case letters (e.g. x) for their values. pX (x) denotes the distribution (or the density in the appropriate context) of X evaluated at x.
1

Under review as a conference paper at ICLR 2019

2.1 PROBLEM SETTING. We suppose that a measurement Y  pY is generated jointly from a signal X  pX and from additional known external factors   p combined with a measurement operator F : Rn × Rd  Rm. Additional sources of uncertainty due to unknown factors, or due to the fact that F is imperfect can be modeled with an additive Gaussian noise E  N (0, 2I):

Y = F (X; ) + E

(1)

We assume F to be differentiable w.r.t. its first argument X, and p to be easy to sample from, and  and X to be independent. If F (X; ) adequately explains the measurements, we can choose E such that 2 is close to zero. For example,  could correspond to the position or the size of the clouds hindering the measurements, and E to instrumental noise or other factors that are not modelled with F . We wish to find a general method that recovers the signal X from the lossy, inaccurate measurements Y .

2.2 APPROACH
The posterior distribution pX|Y (·|y) describes the current state of knowledge we have about the signal when measurement y is observed. Maximum a posteriori (MAP) estimation consists in retrieving the most probable signal x^ under this distribution:

x^ = arg max log pX|Y (x|y)
x

(2)

In order to recover signal x from measurement y, we want to learn a function G : Y  X such that each measurement y is associated with its MAP estimate x^. Therefore, plugging estimate G(y) in place of the true signal valuex, it is natural to maximize the following average log-posterior:

arg max
G

EpY

log pX|Y

(G(y)|y)

(3)

Applying Bayes' theorem to the posterior and ignoring the normalizing constant, objective 3 is equivalent to:

arg max
G

EpY

log pY |X (y|G(y)) + log pX (G(y))

(4)

where pY |X is the likelihood function and pX is the prior. Therefore, the problem amounts at finding a reconstruction function G(y) that is: (1) likely to have generated the data, i.e. yielding high probability under the likelihood pY |X and (2) is plausible, i.e. yielding high probability under the prior pX . Unfortunately, these two distributions are difficult to evaluate: computing the likelihood requires computing an intractable integral, and the prior distribution is unknown overall. In the
following sections, we will see how to handle these two terms.

3 METHOD
We wish to learn a reconstruction network G that takes as input corrupted measurements and produces reconstructions that yield high probability for the likelihood and the prior. The model will be trained using a combination of two losses, one for each requirement, as is shown in Figure 1.
3.1 HANDLING THE LIKELIHOOD TERM
In the general case, evaluating the likelihood pY |X (y|x) of a signal with value x given measurement y requires marginalizing on the unobserved noise variable : pY |X (y|x) = Ep pY |X,(y|x, ) , which involves computing an intractable integral. Many probabilistic approaches for image denoising design the measurement model in order to obtain a

2

Under review as a conference paper at ICLR 2019

GReconstruction Network FMeasurement Process



y x^ y^ x~ y~

Prior Loss (GAN)

Likelihood Loss (MSE)

Figure 1: Schematic representation of the proposed approach. We wish to train G to recover a plau-

sible signal from lossy measurements. As is shown in Section 2.2, this requires the reconstructions

x^ := G(y) to have high probability under the likelihood (Red frame) and the prior (Blue frame).

For simplicity, variable E has been omitted. Blue frame: we sample a measurement y from the

data, produce a reconstruction x^, and sample a perturbation parameter . We enforce the simulated

measurement y^ := F (x^; ) to be similar to measurements in the data using an adversarial penalty.

Intuitively, this requires the network to remove the corruption. Red frame: to enforce G to produce

reconstructions with high likelihood, it is not possible to add a penalty to constrain the mean square

error (MSE) between y and y^ to be small. This is because the underlying perturbation that caused y

is unknown, and may be different from . However, if y^ is similar to the measurements in the data,

we can use it as a proxy for a measurement of the data where its associated  is known. We can now

constrain

y^ - F (G(y^); )

2 2

to

be

small.

known analytic form (Boyat & Joshi (2015), Alkinani & El-Sakka (2017)). However, we wish to attack the general problem and therefore choose not to make this assumption.
We will show that thanks to the independence assumption between  and X, the log-likelihood can be transformed into an amenable form for us. Making use of this independence assumption, we can decompose the log-likelihood as:

log pY |X (y|x) = log pY,|X (y, |x) - log p|X,Y (|x, y) X= log pY |X,(y|x, ) + log p() - log p|Y (|y)
Applying the expectation w.r.t to |Y on both sides, we obtain

(5)

log pY |X (y|x) = Ep|Y log pY |X,(y|x, ) + log p() - log p|Y (|y)

(6)

Since the last two terms do not depend on the signal, this means that in equation 4, where x is substituted with G(y), this term has no impact and can be ignored. Thus, handling the likelihood term
in equation 4 is equivalent to handling EpY p|Y log pY |X,(y|G(y), ) . This form is convenient since log pY |X,(y|x, ) yields a simple analytic expression:

1 log p(y|x, ) = - 22

y - F (x; )

2 2

+

c

where c is a constant. A natural penalty to enforce high likelihood is thus:

(7)

Llikeli(G) := EppY |

y - F (G(y); )

2 2

= EppX pY |X,

y - F (G(y); )

2 2

(8)

3

Under review as a conference paper at ICLR 2019

We have taken the expectation w.r.t ppY | instead of pY p|Y , which from a theoretical viewpoint is equivalent. In practice, this allows us to sample from p, and then sample from pY |. If the
signal distribution pX were known, given that pY |(y|) = EpX pY |X,(y|x, ) the expectation in equation 8 could be computed. However, pX is unknown. In the following sections, we will see how we work around this problem. This process is associated to the red frame in Figure 1.

3.2 HANDLING THE PRIOR TERM
How can we generate reconstructed signals that are probable under the prior if the latter is unknown? If we had access to fully-observed signal data, it could be possible to enforce our reconstruction network to produce signals that are indistinguishable from the ones in the data by leveraging adversarial training, similarly to what is done in the CycleGAN model (Zhu et al. (2017)). However, as stated above, we do not have access to the true signal.
We will build on an idea introduced in the AmbientGAN model by Bora et al. (2018). AmbientGAN aims at learning an unconditional generative model of the true signal distribution, when only lossy measurements of the signal are available. They train a generator to produce signal samples from a latent code, which when corrupted are indistinguishable from the measurements in the data w.r.t. a discriminating network. Intuitively, this requires the network to produce signal that are uncorrupted. Bora et al. (2018) shows that given a fixed noise distribution p, if the distribution of measurements uniquely determines the distribution of signals, Nash Equilibrium for the adversarial game must imply that the generator's induced distribution matches the signal's true distribution (for more details on the measurements for which there exists provable guarantees, see Section 4.2, or Bora et al. (2018)). Note that this is different from our problem: we want to reconstruct corrupted signals and not to learn a distribution of these signals. In AmbientGAN, the generator only takes as input a latent code, and reconstruction from a specific measurement is not possible.
Taking inspiration from this approach, we add an adversarial penalty to enforce our conditioned generator G to produce reconstructions, such that their associated measurements are indistinguishable from the measurements in the data w.r.t. a discriminator D. We introduce the following penalty:

Lprior(G)

:=

max
D

EY pY ,Y^ pGY

log D(y) + log 1 - D(y^)

(9)

where pYG corresponds to the distribution induced by G's corrupted outputs, i.e. pGY (y) := EppGX p(y|x, ) and pGX denotes the marginal distribution induced by G's outputs: pXG (x) := EpY pXG|Y (x|y) = EpY  x - G(y) 1. This penalty enforces the marginal pXG to be close to the true prior distribution pX , and thus enforces G to map its input measurements onto pX . This process is associated to the blue frame in Figure 1.

3.3 IN PRACTICE
In Section 3.1, we have shown that it possible to maximize the average log-likelihood, given that we can sample from the unknown prior distribution pX . In Section 3.2, we have shown how it is possible to enforce the generator to produce signals from pX without ever having access to uncorrupted samples. We can then use the distribution induced by the generator's output pXG as a proxy for pX to compute an approximate value of the expectation in equation 8. Taking the expectation w.r.t. the distribution pXG induced by the generator instead of pX gives us the approximate penalty term:

Llikeli(G) := EppX ,Y^ pYG|X,

y^ - F (G(y^); )

2 2

The full objective is a linear combination of both penalties:

(10)

arg min Lprior(G) +  · Llikeli(G)
G
1(x) is the Dirac delta function, which is equal to zero everywhere except in x.

(11)

4

Under review as a conference paper at ICLR 2019

Under appropriate conditions introduced in Section 3.2, if the Nash Equilibrium for the adversarial game is reached, pXG = pX is recovered, which then implies that the approximate likelihood penalty Llikeli(G) reduces to the likelihood penalty Llikeli(G).
In practice, we use a Monte-Carlo estimation to approximate the expectations for both terms in 11, as described in Figure 1 and Algorithm 1.

Algorithm 1 Training Procedure. Require: Initialize parameters of the the generator G and the discriminator D.
while (G, D) not converged do
Sample {yi}1in from data distribution pY Sample {i}1in from P Sample {i}1in from PE Set y^i to F (G(yi), i) + i for 1  i  n Update D by ascending:
1n n log D(yi) + log(1 - D(y^i))
i=1

Update G by descending:

1 n

n

·

y^i - F (G(y^i); i)

2 2

-

log

D(y^i

)

i=1

end while

4 EXPERIMENTS
4.1 ARCHITECTURE DETAILS
Network. For all are experiments, we use the Resnet architecture He et al. (2016) for our reconstruction network G. For the measurement instance Patch Band (see Section 4.2), we use G(y) = y + Resnet(y) and use small initial parameters in order to initialize our network to the identity function. This allows our model to learn the distribution of the difference of the signal and measurements, which is simpler for the model when the measurements are similar to the associated signal. For the discriminator D, we used the Patch-GAN architecture (Zhu et al. (2017); Isola et al. (2016)).
The hyper-parameters of the models, e.g. the number of layers or the dropout rate are chosen using the validation set. Due to the large number of combinations of hyper-parameters, we made a random search ( Bergstra & Bengio (2012)) and selected the model with the lowest mean square error between the reconstructions x^ and the image x.
Training. We use the Adam solver (Kingma & Ba (2014)) with a batch size of 32. All networks were trained from scratch with a cross validated learning rate. We linearly decay the learning rate between epochs 20 and 100.
We evaluate our approach on the CelebA dataset (Liu et al. (2015)), which consists in 200 000 celebrity photos 2. We use the same preprocessing of the data as Bora et al. (2018): the images are cropped and resized to 64 × 64. The images have all been corrupted once, i.e. there is no two measurements in the data generated with the same uncorrupted image.
4.2 CORRUPTIONS
As for the corruption function, we used the following measurement model :
2The dataset is available at http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html.
5

Under review as a conference paper at ICLR 2019

Remove-Pixel In this measurement process, 95% of pixels are randomly selected and set to 0.
Remove-Pixel-Channel In this measurement process 95% of channel values are randomly selected and set to 0.
Convolve-Noise The image is convoluted with a Gaussian kernel of size k, and a Gaussian noise of variance  = 0.15.
Patch-Band For this function, a horizontal band of height h is randomly set to zero. It can be similar to cloud weather data completion.
Influence of the corruption function can be seen in the figures in section 4. For measurement models Remove-Pixel, Convolve-Noise and Remove-Pixel-Channel, Bora et al. (2018) show that the distribution of measurements induces a unique distribution of signals.

4.3 BASELINES

Conditional AmbientGan. An unconditional generator is trained using the AmbientGan frame-

work (Bora et al. (2018)) to produce samples from the uncorrupted signal distribution, for each type

of corruption (see Section 4.2). We use the same architecture and hyperparameters as those provided

by the authors. For each measurement y, we look for the latent code z that produced y, and selects

the reconstruction x^ that solves x^ = arg minz

y - G(z)

2 2

,

as

is

proposed

in Bora et

al.

(2017).

This approach is the most similar to ours, and does not require having access to samples from the

signal distribution. Because this approach is sensitive to the initial latent code, we reiterate this

approach three times and select the best resulting image.

Unpaired Supervision This is a variant of our model that has access to the fully-observed im-
ages. This model is identical to ours, apart from the fact that instead of discriminating between measurements y and generated measurements y^, we discriminate between fully-observed images x and generated images x^.

Paired supervision This is a variant of our model that has access to pairs of measurements and fully-observed images (y, x). We discriminate between fully-observed images x and generated images x^, and regress to the associated fully-observed image x, like in the Pix2Pix model (Isola et al.
(2016)).

Other. We also compare our model to different baselines, including Deep Image Prior (Ulyanov et al. (2017)), Damelin & Hoang (2018), and Total Variation Denoising3. These results are provided
in the appendix.

4.4 QUANTITATIVE RESULTS
We now compare our model with those presented in the previous section. Each model is evaluated with a mean square error metric between the reconstructed x^ and the x used to generate the input y. The hyper-parameters are tuned on the validation set. In Table 1, we report the performance on the test set, which is a random subset of the CelebA dataset of size 40000. Because the Conditional AmbientGan model is computationally expensive, we only report the MSE on 40 randomly chosen samples of the test set.
Table 1: : Average MSE on the test set. Each column corresponds to a different corruption process.

Conditional AmbientGan Ours
Unpaired Supervision Paired Supervision

RemovePixel
0.292 0.0414 0.037 0.0383

RemovePixelChannel
0.2829 0.0409 0.0336 0.0401

Patch-Band
0.1421 0.0165 0.034 0.0147

ConvolveNoise
0.0814 0.0088 0.0103 0.0084

3http://scikit-image.org/docs/0.12.x/auto_examples/filters/plot_ denoise.html

6

Under review as a conference paper at ICLR 2019
Quantitatively, our model performs well. Except for the Conditional Ambiant GAN, all the methods are quite similar in MSE. This means that our unsupervised model reaches performance very close - sometimes better - than the two supervised models. We remark that even if we do not have access to aligned pair, computing the prior loss directly on the uncorrupted signal yield results comparable, and sometimes better than the paired supervision model. This empirically suggests that the likelihood loss is sufficient to efficiently condition the reconstruction on the input signal.
4.5 QUALITATIVE RESULTS Besides MSE, we need to analyze the reconstructed samples qualitatively. Figure 2 shows the reconstruction obtained by the different model. Each row corresponds to a kind of measurement, and each column to a different model. As we can see, the measurement function we used, induce a large loss of information, and is difficult to reconstruct, even for a human.
Remove-Pixel

Convolve-Noise
Remove-PixelChannel

Patch Band

Measurement

Cond AG, Unsupervised, no access to
uncorrupted samples

Ours, Unsupervised, no access to
uncorrupted samples

Variant, Unsupervised,
access to uncorrupted
samples

Variant, Supervised, access to pairs

Figure 2: Reconstructions for each of the models. Each row corresponds to a specific corruptions, and each columns to a specific model.

The visual reconstruction is coherent with the quantitative results. In figure 3 and 4 we show randomly sampled reconstruction from our model.

5 RELATED WORK
Up to our knowledge, there is no other Deep Learning approach attempting to solve the unsupervised signal reconstruction problem. However, some of the ideas developed here are close or even inspired as mentioned in the text to recent existing work.
The idea of cycle consistency has been proposed or used in several contexts. (Zhu et al. (2017), Lample et al. (2017), Almahairi et al. (2018)) for example all exploit this constraint in order to learn from unpaired data sets. To enforce high likelihood, we recover a penalty loss that is close to

7

Under review as a conference paper at ICLR 2019
Figure 3: Random samples from our model, trained to reconstruct the images from Patch-Band measurements.
Figure 4: Random samples from our model, trained to reconstruct the images from Remove-Pixel measurements.
the Cycle consistency loss. Moreover, they too use, as we did, adversarial training to constrain the marginal distribution induced by the generator. In the context of image super resolution, (Ledig et al. (2016), Sønderby et al. (2016), Mardani et al. (2017)) attempt to retrieve map estimates of the super resolution image conditioned on an input image. They too use a generative model of the signal trained in an adversarial fashion using samples from signal distribution to constrain their reconstructions. Their approach is fully supervised. Other works attempt to solve ill-posed inverse problems using generative models (Bora et al. (2017), Ulyanov et al. (2017), Asim et al. (2018), Tripathi et al. (2018), Van Veen et al. (2018)). The general approach in all these contributions consists in inverting the generative model by finding a latent code close to the measurement by directly minimizing the mean square error between the corrupted reconstruction and the measurement. This requires solving an optimization problem for each image, which takes several minutes (Ulyanov et al. (2017)) on GPU, and requires random restarts to avoid falling a bad local minima. Again, the setting is fully supervised. Finally Lehtinen et al. (2018) propose a method for denoising images without direct supervision. They train a network to regress a corrupted image using the same image with a different corruption value as input. Assuming the corruption has zero-mean, their network learns to remove the corruption which is present in the input image. This setting implies that they have access to the original (uncorrupted) image for generating different noisy versions of the same image, which is not our case. Our setting is then more complex and probably more realistic.
6 CONCLUSION
We have proposed a general formulation to recover a signal from lossy measurements using a neural network, without having access to uncorrupted signal data. From the maximum a posteriori approach, we derive two penalties for our neural network. By minimizing a linear combination of the previous penalties, we show that our approach yields results that are competitive with other models that have access to higher forms of supervision. For future work, we plan to apply our framework to different corruption processes, and test if our model holds in the wild, specifically for retrieving uncorrupted scientific data. Lastly, it would be interesting to approximate the true posterior by making our model stochastic.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Monagi H. Alkinani and Mahmoud R. El-Sakka. Patch-based models and algorithms for image denoising: a comparative review between patch-based images denoising methods for additive noise reduction. EURASIP Journal on Image and Video Processing, 2017(1):58, Aug 2017. ISSN 1687-5281. doi: 10.1186/s13640-017-0203-4. URL https://doi.org/10.1186/ s13640-017-0203-4.
Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip Bachman, and Aaron Courville. Augmented cyclegan: Learning many-to-many mappings from unpaired data. arXiv preprint arXiv:1802.10151, 2018.
Muhammad Asim, Fahad Shamshad, and Ali Ahmed. Solving bilinear inverse problems using deep generative priors. CoRR, abs/1802.04073, 2018. URL http://arxiv.org/abs/1802. 04073.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281­305, 2012.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed Sensing using Generative Models. arXiv:1703.03208 [cs, math, stat], March 2017. URL http://arxiv.org/ abs/1703.03208. arXiv: 1703.03208.
Ashish Bora, Eric Price, and Alexandros G. Dimakis. AmbientGAN: Generative models from lossy measurements. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=Hy7fDog0b.
Ajay Kumar Boyat and Brijendra Kumar Joshi. A review paper: Noise models in digital image processing. CoRR, abs/1505.03489, 2015.
Emmanuel J. Candès, Justin K. Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics, 59(8):1207­ 1223, 2005. doi: 10.1002/cpa.20124. URL https://onlinelibrary.wiley.com/doi/ abs/10.1002/cpa.20124.
SB Damelin and NS Hoang. On surface completion and image inpainting by biharmonic functions: Numerical aspects. International Journal of Mathematics and Mathematical Sciences, 2018, 2018.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. CoRR, abs/1605.08803, 2016. URL http://arxiv.org/abs/1605.08803.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS'14, pp. 2672­2680, Cambridge, MA, USA, 2014. MIT Press. URL http://dl.acm.org/ citation.cfm?id=2969033.2969125.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770­778, 2016. URL https://doi.org/10.1109/ CVPR.2016.90.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. CoRR, abs/1611.07004, 2016. URL http://arxiv.org/ abs/1611.07004.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013. URL http://arxiv.org/abs/1312.6114.
9

Under review as a conference paper at ICLR 2019
Guillaume Lample, Ludovic Denoyer, and Marc'Aurelio Ranzato. Unsupervised machine translation using monolingual corpora only. CoRR, abs/1711.00043, 2017. URL http://arxiv. org/abs/1711.00043.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew P. Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. CoRR, abs/1609.04802, 2016. URL http://arxiv.org/ abs/1609.04802.
Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning Image Restoration without Clean Data. arXiv:1803.04189 [cs, stat], March 2018. URL http://arxiv.org/abs/1803.04189. arXiv: 1803.04189.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Morteza Mardani, Enhao Gong, Joseph Y. Cheng, Shreyas Vasanawala, Greg Zaharchuk, Marcus T. Alley, Neil Thakur, Song Han, William J. Dally, John M. Pauly, and Lei Xing. Deep generative adversarial networks for compressed sensing automates MRI. CoRR, abs/1706.00051, 2017. URL http://arxiv.org/abs/1706.00051.
João FC Mota, Nikos Deligiannis, and Miguel RD Rodrigues. Compressed sensing with prior information: Strategies, geometry, and bounds. IEEE Transactions on Information Theory, 63(7): 4472­4496, 2017.
Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszár. Amortised MAP Inference for Image Super-resolution. arXiv:1610.04490 [cs, stat], October 2016. URL http://arxiv.org/abs/1610.04490. arXiv: 1610.04490.
Subarna Tripathi, Zachary C. Lipton, and Truong Q. Nguyen. Correction by projection: Denoising images with generative adversarial networks. CoRR, abs/1803.04477, 2018. URL http:// arxiv.org/abs/1803.04477.
Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Deep image prior. CoRR, abs/1711.10925, 2017. URL http://arxiv.org/abs/1711.10925.
David Van Veen, Ajil Jalal, Eric Price, Sriram Vishwanath, and Alexandros G. Dimakis. Compressed Sensing with Deep Image Prior and Learned Regularization. arXiv:1806.06438 [cs, math, stat], June 2018. URL http://arxiv.org/abs/1806.06438. arXiv: 1806.06438.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. arXiv:1703.10593 [cs], March 2017. URL http://arxiv.org/abs/1703.10593. arXiv: 1703.10593.
10

Under review as a conference paper at ICLR 2019
Measurement Deep Image Prior
Biharmonic Inpaiting
Cond AG
Figure 5: Baseline comparison for the remove-pixel measurement
Measurement Deep Image Prior
Biharmonic Inpaiting
Cond AG
Figure 6: Baseline comparison for the remove-pixel-channel measurement
Measurement Deep Image Prior
TV Denoising Cond AG
Figure 7: Baseline comparison for the convolve-noise measurement
11

Under review as a conference paper at ICLR 2019
Measurement Deep Image Prior
Biharmonic Inpaiting
Cond AG
Figure 8: Baseline comparison for the Patch-Band measurement
Figure 9: Random samples from our model, trained to reconstruct the images from Convolve-Noise measurements with a noise variance of 0.15 .
Figure 10: Random samples from our model, trained to reconstruct the images from Convolve-Noise measurements with a noise variance of 0.5 .
Figure 11: Random samples from our model, trained to reconstruct the images from keep-patch measurements.
12

Under review as a conference paper at ICLR 2019
Figure 12: Random samples from our model, trained to reconstruct the images from remove-pixel measurements with p = 0.80.
Figure 13: Random samples from our model, trained to reconstruct the images from remove-pixel measurements with p = 0.95.
Figure 14: Random samples from our model, trained to reconstruct the images from remove-pixelchannel measurements with p = 0.95.
13

