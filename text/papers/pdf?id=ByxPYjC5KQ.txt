Under review as a conference paper at ICLR 2019
IMPROVING GENERALIZATION AND STABILITY OF GENERATIVE ADVERSARIAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.
1 INTRODUCTION
GANs (Goodfellow et al., 2014) are one of the most popular tools for modeling high dimensional data. The original GAN is, however, highly unstable and often suffers from mode collapse. Much of recent researches has focused on improving the stability of GANs (Radford et al., 2015; Arjovsky et al., 2017; Heusel et al., 2017; Miyato et al., 2018; Karras et al., 2018). On the theoretical aspect, Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable. Heusel et al. (2017) further proved that GANs trained with Two Timescale Update Rule (TTUR) converge to local equilibria. However, the generalization of GANs at local equilibira is not discussed in depth in these papers.
Arora et al. (2017) showed that the generator can win by remembering a polynomial number of training examples. The result implies that a low capacity discriminator cannot detect the lack of diversity. Therefore, it cannot teach the generator to approximate the target distribution. In section 2, we discuss the generalization capability of high capacity discriminators which are commonly used in practice. We show that high capacity discriminators trained with the original GAN loss tends to overfit to the mislabeled samples in training dataset, guiding the generator toward collapsed equilibria (i.e. equilibria where the generator has mode collapse).
Arora et al. (2018) proposed to measure the generalization capability of GAN by estimating the number of modes in the model distribution using the birthday paradox. Experiments on several datasets showed the number of modes in the model distribution is several times greater than the number of training examples. The author concluded that although GANs might not be able to learn distributions, they do exhibit some level of generalization. Our analysis shows that poor generalization comes from the mismatch between discriminators trained on discrete finite datasets and the theoretically optimal discriminator. We propose zero-centered gradient penalty for improving the generalization capability of (high capacity) discriminators by pushing them toward the optimal one.
Our contributions are as follow:
1. We show that discriminators trained with the original GAN loss has poor generalization capability. Poor generalization in the discriminator prevents the generator from learning the target distribution.
2. We show that the original GAN objective encourages gradient exploding in the discriminator. Gradient exploding in the discriminator can lead to mode collapse in the generator.
1

Under review as a conference paper at ICLR 2019

3. We propose a zero-centered gradient penalty (0-GP) for improving the generalization capability of the discriminator. We show that non-zero centered GP and the zero-centered GP proposed in Mescheder et al. (2018) cannot make the discriminator generalize. Our zero-centered GP helps GANs to converge to generalizable equilibria. Theoretical results are verified on real world datasets.
4. We show that 0-GP helps the discriminator to distribute its capacity more equally between regions of the space, effectively preventing mode collapse. Experiments on synthetic and real world datasets verify that 0-GP can prevent mode collapse. GANs with 0-GP is much more robust to changes in hyper parameters, optimizers, and network architectures than the original GAN and GANs with other gradient penalties.
NOTATIONS
In this paper, we use the following notations:

pr pg pz dx dz D(·) D(·; D) G(·) G(·; G) supp(p) x  pr z  pz y = G(z) Dr = {x1, ..., xn}
Dg(t) = y1(t), ..., ym(t)
D(t) = Dr  Dg(t) v

the target distribution the model distribution the noise distribution the dimensionality of a data sample (real or fake) the dimensionality of a noise sample a discriminator, a function from Rdx to [0, 1] a discriminator with parameter D a generator, a function from Rdz to Rdx a generator with parameter G the support of distribution p a real sample a noise vector drawn from the noise distribution pz a generated sample the set of n real samples
the set of m generated samples at step t
the training dataset at step t the Euclidean norm of v

2 GENERALIZATION CAPABILITY OF DISCRIMINATORS

2.1 THE EMPIRICALLY OPTIMAL DISCRIMINATOR DOES NOT APPROXIMATE THE
THEORETICALLY OPTIMAL DISCRIMINATOR

In the original GAN, the discriminator D maximizes the following objective

L = Expr [log(D(x))] + Ezpz [log(1 - D(G(z)))]

(1)

Goodfellow et al. (2014) showed that if the density functions pg and pr are known, then for a fixed generator G the optimal discriminator is

D(v)

=

pr(v) , v pr(v) + pg(v)



supp(pr )



supp(pg )

(2)

In the beginning of the training, pg is very different from pr so we have pr(x) pg(x), for x  pr

and pg(y) pr(y), for y  pg. Therefore, in the beginning of the training D(x)  1, for x  pr

and D(y)  0, for y  pg. As the training progresses, the generator will bring pg closer to pr. The

game

reaches

the global equilibrium when

pr

=

pg .

At the

global

equilibrium

D(v)

=

1 2

,

v



supp(pr)supp(pg). One important result of the original paper is that if the discriminator is optimal

at every step of the GAN algorithm, then pg converges to pr.

In practice, density functions are not known and the optimal discriminator is approximated by op-
timizing the classification performance of a parametric discriminator D(·; D) on a discrete finite dataset D = Dr  Dg. We call a discriminator trained on a discrete finite dataset an empirical

2

Under review as a conference paper at ICLR 2019

discriminator. The empirically optimal discriminator is denoted by D^ . It has been observed that
if the discriminator is too good at discriminating real and fake samples, the generator cannot learn effectively (Goodfellow et al., 2014; Arjovsky & Bottou, 2017). The phenomenon suggests that D^  does not well approximate D, and does not guarantee the convergence of pg to pr. In the following, we clarify the mismatch between D^  and D, and the implications of this phenomenon.

For continuous random variable V , P(V = v) = 0 for any v. The probability of finding a noise vector z such that G(z) is exactly equal to a real datapoint x  Dr via random sampling is 0 1. Therefore, the probability of a real datapoint xi being in the fake dataset Dg is 0. Similarly, the
probability of any fake datapoint being in the real dataset is 0.

P(x  Dg(t)) = 0, x  Dr, t  N P(y  Dr) = 0, y  Dg(t), t  N
P(Dr  Dg(t) = ) = 1, t  N

(3)

Dr and Dg(t) are disjoint with probability 1 even when pg and pr are exactly the same. The empirically optimal discriminator D^  will perfectly classify the real and the fake datasets, and

D^ (x) = 1, x  Dr D^ (y) = 0, y  Dg(t)

The value of D^  on D(t) does not depends on how close the two distributions are and does not reflect the learning progress. The value of D^  on the training dataset approximates that of D in the
beginning of the learning process but not when the two distributions are close. When trained using gradient descent on a discrete finite dataset with the loss in Eqn. 1, the discriminator D is pushed toward D^ , not D. This behavior does not depend on the size of training set (see Fig. 1a, 1b),
implying that the original GAN is not guaranteed to converge to the target distribution even when
given enough data.

2.2 EMPIRICAL DISCRIMINATORS HAVE POOR GENERALIZATION CAPABILITY
When the generator gets better, generated samples are more similar to samples from the target distribution. However, regardless of their quality, generated samples are still labeled as fake in Eqn. 1. The training dataset D is a bad dataset as it contains many mislabeled examples. A discriminator trained on such dataset will overfit to the noise (the mislabeled examples) in the data and have poor generalization capability, i.e. it will misclassify unseen samples and cannot teach the generator to generate these samples.
Figure 1a and 1b demonstrate the problem on a synthetic dataset consisting of samples from two Gaussian distributions. The discriminator in Fig. 1a overfits to the small dataset and does not generalize to new samples in Fig. 1b. Although the discriminator in Fig. 1b was trained on a larger dataset which is sufficient to characterize the two distributions, it still overfits to the data and its value surface is very different from that of the theoretically optimal discriminator in Fig. 1f.
An overfitted discriminator does not guide the model distribution toward target distribution but toward the real samples in the dataset. This explains why the original GAN usually exhibits mode collapse behavior. Finding the empirically optimal discriminator using gradient descent usually requires many iterations. Heuristically, overfitting can be alleviated by limiting the number of discriminator updates per generator update. Goodfellow et al. (2014) recommended to update the discriminator once every generator update. In the next subsection, we show that limiting the number of discriminator updates per generator update prevents the discriminator from overfitting to the dataset.

2.2.1 -OPTIMAL DISCRIMINATORS
The empirically optimal discriminator D^  is costly to find and maintain. We consider here a weaker notion of optimality which can be achieved in practical settings.
1Due to the curse of dimensionality, the probability of sampling a datapoint which is close to another datapoint in high dimensional space also decrease exponentially. The distances between datapoints are larger in higher dimensional space. That suggests that it is easier to separate Dr and Dg(t) in higher dimensional space.

3

Under review as a conference paper at ICLR 2019

4 3 2 1 0 1 2
2 10 1 2 3 4
Powered by TCPDF (www.tcpdf.org)
(a)

4 3 2 1 0 1 2
2 10 1 2 3 4
(b)

4 3 2 1 0 1 2
2 10 1 2 3 4
(c)

4 3 2 1 0 1 2
2 10 1 2 3 4
(d)

4 3 2 1 0 1 2
2 10 1 2 3 4
(e)

4 3 2 1 0 1 2
2 10 1 2 3 4
(f)

1.0 0.8 0.6 0.4 0.2 0.0

Figure 1: Value surfaces of discriminators trained for 10,000 iterations with different gradient penalties, on samples from two Gaussian distributions. The discriminator is a 2 hidden layer MLP with 64 hidden neurons, tanh activation function at hidden layers, sigmoid activation function at output layer. (a) No GP. (b) No GP with more samples. (c) One-centered GP as proposed in Gulrajani et al. (2017) with  = 1.(d) Zero-centered GP on real/fake samples only (0-GP-sample) as proposed in Mescheder et al. (2018) with  = 1. (e) Our zero-centered GP with  = 1. (f) Theoretically optimal discriminator computed using equation 2.

Definition 1 ( -optimal discriminator). Given two disjoint datasets Dr and Dg, and a number > 0, a discriminator D is -optimal if

D(x)



1 2

+

, x 2



Dr

D(y)



1 2

-

, y 2



Dg

As pointed out in (Goodfellow et al., 2014; Arjovsky & Bottou, 2017), D^  does not generate usable gradient for the generator. Goodfellow et al. (2014) proposed the non-saturating loss for the generator to circumvent this vanishing gradient problem. For an -optimal discriminator, if is relatively small, then the gradient of the discriminator w.r.t. fake datapoints will not vanish and can be used to guide the model distribution toward the target distribution.
Proposition 1. Given two disjoint datasets Dr and Dg, and a number > 0, an -optimal discriminator D exists and can be constructed as a one hidden layer MLP with O(dx(m + n)) parameters.
Proof. See appendix A.

Because deep networks are more powerful than shallow ones, the size of a deep -optimal discriminator can be much smaller than O(dx(m + n)). Real world datasets usually contains hundreds of thousands to a few millions datapoints whose dimensionality ranges from hundreds to thousands. The size of a -optimal discriminator for these datasets ranges from a few to hundreds of millions parameters. That is comparable to the size of discriminators used in practice. Arjovsky & Bottou (2017) showed that even when the generator can generate realistic samples, a discriminator that can perfectly classify real and fake samples can be found easily by training the discriminator with gradient descent. The experiment verified that -optimal discriminator can be found using gradient descent in practical settings.
We observe that the norm of the gradient w.r.t. the discriminator's parameters decreases as fakes samples approach real samples. If the discriminator's learning rate is fixed, then the number of gradient descent steps that the discriminator has to take to reach -optimality increases.

4

Under review as a conference paper at ICLR 2019

Proposition 2. Alternating gradient descent with the same learning rate for discriminator and generator, and fixed number of discriminator updates per generator update (Fixed-Alt-GD) cannot maintain the optimality of the discriminator.
Fixed-Alt-GD decreases the discriminative power of the discriminator to improve its generalization capability. The proof for linear case is given in appendix B.
In GANs trained with Two Timescale Update Rule (TTUR) (Heusel et al., 2017), the ratio between the learning rate of the discriminator and that of the generator goes to infinity as the iteration goes to infinity. Therefore, the discriminator can learn much faster than the generator and might be able to maintain its optimality throughout the learning process.

2.2.2 GRADIENT EXPLODING IN -OPTIMAL DISCRIMINATORS

Let's consider a simplified scenario where the real and the fake dataset each contains a single datapoint: Dr = {x}, Dg(t) = y(t) . Updating the generator according to the gradient from the
discriminator will push y(t) toward x. The absolute value of directional derivative of D in the direction u = x - y(t), at x is

D(x) - D(y(t))

|(uD)x| = lim
y (t) x

x - y(t)

If D is always -optimal, then D(x) - D(y(t))  , t  N, thus

|(uD)x|  lim
y (t) x

x - y(t)

=

The directional derivate of the -optimal discriminator explodes as the fake datapoint approaches the

real datapoint.

|D(x)-D(y (t) )|
x-y(t)

can be interpreted as the average value of the directional derivatives

w.r.t. datapoints on the line segment connecting x and y(t). There must be some datapoint v in the

line segment from y(t) to x whose directional derivative is greater than or equal to

. Di-

x-y(t)

rectional derivative exploding implies gradient exploding on datapoints on line segment connecting

x and y(t).

Let's consider the following line integral

(D)v · ds = D(x) - D(y(t))
C

(4)

where C is the line segment from y(t) to x. As the model distribution gets closer to the target distribution, the length of C should be non increasing. Therefore, maximizing D(x) - D(y(t)), or the discriminative power of D, leads to the maximization of the directional derivative of D in the direction ds. The original GAN loss makes D to maximize its discriminative power, encouraging gradient exploding to occur.

Gradient exploding happens in the discriminator trained with TTUR in Fig. 2c and 2d. Note that, because of the saturated regions in the sigmoid function used in neural network based discriminators, the gradient w.r.t. datapoints in the training set could vanishes. However, gradient exploding must happen at some datapoints on the path connecting a pair of real and fake datapoints. In Fig. 1a, gradient exploding happens for datapoints near the decision boundary. Because TTUR can help the discriminator maintain its optimality, gradient exploding happens and persists throughout the training process. Without TTUR, the discriminator cannot maintain its optimality so gradient exploding can happen sometimes during the training but does not persist (Fig. 2a and 2b).

In practice, Dr and Dg contain many datapoints and the generator is updated using the average of
gradients of the discriminator w.r.t. fake datapoints in the mini-batch. If a fake datapoint y0 is very
close to a real datapoint x0, the gradient (D)y0 might explode. When the average gradient is computed over the mini-batch, (D)y0 outweighs other gradients. The generator updated with this average gradient will move many fake datapoints in the direction of (D)y0 , toward x0, making
mode collapse visible.

5

Under review as a conference paper at ICLR 2019

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10 5 0

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 5 10 10.0 10 5 0

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 5 10 10.0 10 5 0

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 5 10 10.0 10 5 0

5 10

(a) (b) (c) (d)

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10 5 0

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 5 10 10.0 10 5 0

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 5 10 10.0 10 5 0

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 5 10 10.0 10 5 0

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 5 10 10.0 10 5 0

5 10

(e) (f) (g) (h) (i)

Figure 2: Gradient w.r.t. the input of the discriminator of a GAN trained with different gradient penalties. The vector associated with a datapoint v points in the direction that increases the value of log (D(v)) the fastest. The discriminator is a 2 hidden layer MLP with 512 hidden neurons. The discriminator is updated once every generator update. SGD is used for optimization. (a) No GP, iteration 1000. (b) No GP, iteration 10,000. (c) No GP with TTUR, iteration 1000. (d) No GP with TTUR, iteration 10,000. (e) Our zero-centered GP with  = 10, iteration 10,000. (f) Our zerocentered GP with TTUR and  = 10, iteration 10,000. (g) Our zero-centered GP with TTUR and  = 10, iteration 20,000. (h) One-centered GP with  = 10, iteration 10,000. (i) 0-GP-sample with  = 10, iteration 10,000.

3 IMPROVING GENERALIZATION CAPABILITY OF EMPIRICAL
DISCRIMINATORS

Although the theoretically optimal discriminator D is generalizable, the original GAN loss does not push empirical discriminators toward D. We aim to improve the generalization capability of empirical discriminators by pushing them toward D.

3.1 PUSHING EMPIRICAL DISCRIMINATORS TOWARD D

For any input v  supp(pr)  supp(pg), to 0 as pg approaches pr. Consider again

the the

value of D(v) goes line integral in Eqn.

to 4.

A12 saDndt(hxe)garnaddiDent((y)Dap)pvrogaocehs

1 2

for

all

x



supp(pr )

and

y



supp(pg ),

we

have

D(x) - D(y) = (D)v · ds  0
C

(5)

for all pairs of x and y and all paths C from y to x. That means, the discriminative power of D must decrease as the two distributions become more similar.
To push an empirical discriminator D toward D, we force D to satisfy two requirements:

1. (D)v  0,  v  supp(pr)  supp(pg) 2. D(x) - D(y) = C (D)v · ds  0,  x  pr, y  pg, C from y to x

3.2 ZERO-CENTERED GRADIENT PENALTY
The first requirement can be implemented by sampling some datapoints v  supp(pr)  supp(pg) and force the gradient w.r.t. these datapoints to be 0. The second requirement can be implemented by sampling pairs of real and fake datapoints (x, y) and force D(x) - D(y) to be 0. The two

6

Under review as a conference paper at ICLR 2019

requirements can be added to the discriminator's objective as follows

L^ = L - 1Ev[ (D)v 2] - 2Ex,y[(D(x) - D(y))2]

where L is the objective in Eqn. 1. However, as discussed in section 2.2.2, an -optimal discriminator can have zero gradient on the training dataset and have gradient exploding outside of the training dataset. The gradient norm could go to infinity even when D(x) - D(y) is small. Regulating the difference between D(x) and D(y) is not an efficient way to prevent gradient exploding.

We want to prevent gradient exploding on every path in supp(pr)supp(pg). Because (D)v  0 for all v  supp(pr)supp(pg) as pg approach pr, we could push the gradient w.r.t. every datapoint on every path C  supp(pr)  supp(pg) toward 0. We note that, if (D)v  0,  v  C then C (D)v · ds  0. Therefore, the two requirements can be enforced by a single zero-centered gradient penalty of the form
EvC[ (D)v 2]
The remaining problem is how to find the path C from a fake to a real sample which lies inside supp(pr)  supp(pg). Because we do not have access to the full supports of pr and pg, and the supports of two distributions could be disjoint in the beginning of the training process, finding a
line which lies completely inside the support is infeasible. In the current implementation, we approximate C by the straight line connecting a pair of samples, although there is no guarantee that all datapoints on that straight line are in supp(pr)  supp(pg). That results in the following objective

L0-GP = L - Ex~[ (D)x~ 2]

(6)

where x~ = x + (1 - )y, x  pr, y  pg, and   U (0, 1). The larger  is, the stronger (D)x~ is pushed toward 0. If  is 0, then the discriminator will only focus on maximizing its discriminative power. If  approaches infinity, then the discriminator has maximum generalization capability and no discriminative power.  controls the tradeoff between discrimination and generalization in the discriminator.

3.3 GENERALIZATION CAPABILITY OF DIFFERENT GRADIENT PENALTIES

Mescheder et al. (2018) proposed to force the gradient w.r.t. datapoints in the real and/or fake distribution to be 0 to make the training of GANs convergent. While the penalty helps GANs to converge, it does not help to push D toward D. Arjovsky & Bottou (2017) proved that if supp(pr)  supp(pg) = , then a perfect discriminator D~  exists and satisfies
D~ (x) = 1, x  pr
D~ (y) = 0, y  pg
(D~ )v = 0, v  supp(pr)  supp(pg)
In section 2, we showed that for discrete training dataset, an empirically optimal discriminator D^  always exists and could be found by gradient descent. Although D~  and D^  have 0 gradient on the training dataset, they do not satisfy the requirement in Eqn. 5 and have gradient exploding when pg approaches pr. The discriminators in Fig. 1a, 1b, 1d, 2c and 2d have vanishingly small gradients on datapoints in the training dataset and very large gradients outside. They have poor generalization capability and cannot teach the generator to generate unseen real datapoints.

Therefore, zero-centered gradient penalty on samples from pr and pg only cannot help improving the generalization of the discriminator.

Non-zero centered GP will not push an empirical discriminator toward D because the gradient does not converge to 0. A commonly used non-zero centered GP is the one-centered GP (1-GP) (Gulrajani et al., 2017) which has the following form

Ex~[( (D)x~ - 1)2]

(7)

where x~ = x + (1 - )y, x  pr, y  pg, and   U (0, 1). Although, the initial goal of 1-GP was to enforce Lipschitz constraint on the discriminator 2, Fedus et al. (2018) found that 1-GP
prevents gradient exploding, making the original GAN more stable.

2Petzka et al. (2018) pointed out that 1-GP is based on the wrong intuition that the gradient of the optimal critic must be 1 everywhere under pr and pg. The corrected GP is based on the definition of Lipschitzness.

7

Under review as a conference paper at ICLR 2019
1-GP forces the norm of gradients w.r.t. datapoints on the line segment connecting x and y to be 1. If all gradients on the line segment have norm 1, then the line integral in Eqn. 4 could be as large as the length of line segment which is x - y . Because the distance between random samples grows with the dimensionality, in high dimensional space x - y is greater than 1 with high probability. The discriminator could maximize the value of the line integral without violating the Lipschitz constraint. The discriminator trained with 1-GP, therefore, can overfit to the training data and have poor generalization capability.
3.4 CONVERGENCE ANALYSIS FOR ZERO-CENTERED GRADIENT PENALTY
Mescheder et al. (2018) showed that zero-centered GP on real and/or fake samples (0-GP-sample) makes GANs convergent. The penalty is based on the convergence analysis for the Dirac GAN, an 1-dimensional linear GAN which learns the Dirac distribution. The intuition is that when the model distribution is the same as the target distribution, the gradient of the discriminator w.r.t. the fake datapoints (which are also real datapoints) should be 0 so that generator will not move away when being updated using this gradient. If the gradient of the discriminator is not 0, then the generator will oscillate around the equilibrium.
Our GP forces the gradient w.r.t. all datapoints on the line segment between a pair of samples (including the two endpoints) to be 0. As a result, our GP also prevents the generator from oscillating. Therefore, our GP has the same convergence guarantee as the 0-GP-sample.
3.5 ZERO-CENTERED GRADIENT PENALTY IMPROVES CAPACITY DISTRIBUTION
Discriminators trained with the original GAN loss tends to focus on the region of the where fake samples are close to real samples, ignoring other regions. The phenomenon can be seen in Fig. 2a, 2b, 2c, 2d, 2h and 2i. Gradients in the region where fake samples are concentrated are large while gradients in other regions, including regions where real samples are located, are very small. The generator cannot discover and generate real datapoints in regions where the gradient vanishes.
When trained with the objective in Eqn. 6, the discriminator will have to balance between maximizing L and minimizing the GP. For finite , the GP term will not be exactly 0. Let  = Ex~[ (D)x~ 2]. Among discriminators with the same value of , gradient descent will find the discriminator that maximizes L. As discussed in section 2.2.2, maximizing L leads to the maximization of norms of gradients on the path from y to x. The discriminator should maximize the following value
 = Ex~[ (D)x~ ] If  is fixed then  is maximized when D =x~(i) D ,x~(j)  i, j (Cauchy-Schwarz inequality). Therefore, our zero-centered GP encourages the gradients at different regions of the real data space to have the same norm. The capacity of D is distributed more equally between regions of the real data space. Mode collapse problem is effectively reduced. The effect can be seen in Fig. 2e and 2f. 1-GP encourages | D x~(i) - 1| = | D x~(j) - 1|,  i, j. That allows gradient norms to be smaller than 1 in some regions and larger than 1 in some other regions. The problem can be seen in Fig. 2h.
4 EXPERIMENTS
The code for all experiments will be released after the review process.
4.1 ZERO-CENTERED GRADIENT PENALTY PREVENTS OVERFITTING
We want to test the effectiveness of gradient penalties in preventing overfitting. We designed a dataset with real and fake samples coming from two Gaussian distributions and trained a MLP based discriminator on that dataset. The result is shown in Fig. 1. As predicted in section 3.3, 0-GPsample does not help to improve generalization. One-centered GP helps to improve generalization. The value surface in Fig. 1c is smoother than that in Fig. 1a. However, as discussed in section 3.3, 1-GP cannot help much in higher dimensional space where the distances between datapoints are
8

Under review as a conference paper at ICLR 2019
(a) (b) (c) (d) (e)
Figure 3: Result on MNIST. The networks has the same architectures with networks used in synthetic experiment. Batch normalization (Ioffe & Szegedy, 2015) was not used. Adam optimizer (Kingma & Ba, 2014) with 1 = 0.5, 2 = 0.9 was used in training. (a) No GP, iteration 1,000. (b) Zero-centered GP on real samples only with  = 100, iteration 1,000. (c) One-centered GP with  = 100, iteration 1,000. (d) Our zero-centered GP with  = 100, iteration 1,000. (d) Our zero-centered GP with  = 100, iteration 10,000.
large. The discriminator trained with our zero-centered GP has the best generalization capability. Its value surface is the most similar to that of the theoretically optimal one.
4.2 ZERO-CENTERED GRADIENT PENALTY IMPROVES THE ROBUSTNESS OF GANS
SYNTHETIC DATA We tested different gradient penalties on a number of synthetic datasets to compare their effectiveness. The first dataset is a mixture of 8 Gaussians. We scale the dataset up by a factor of 10 to simulate the situation in high dimensional space where random samples are far from each other. The result is shown in Fig. 2. GANs with other gradient penalties all fail to learn the distribution and exhibit mode collapse problem to different extents. GAN with our 0-GP (GAN-0-GP) can successfully learn the distribution. We observe that GAN-0-GP behaves similar to Wasserstein GAN as it first learns the overall structure of the distribution and then focuses on the modes. An evolution sequence of GAN-0-GP is shown in Fig. 5 in appendix C. Results on other synthetic datasets are shown in appendix C.
MNIST DATASET The result on MNIST dataset is shown in Fig. 3. After 1,000 iterations, all other GANs exhibits mode collapse or cannot learn anything. GAN-0-GP is robust to changes in hyper parameters such as learning rate and optimizers. When Adam is initialized with large 1, for example 0.9, GAN with other GP cannot learn anything after many iterations. More samples are given in appendix C. We observe that higher value of  improves the diversity of generated samples. For  = 50, we observe some similar looking samples in the generated data. This is consistent with our conjecture that larger  leads to better generalization.
IMAGENET When trained on ImangeNet, GAN-0-GP can produce high quality samples from all 1,000 classes. We compared our method with GAN with 0-GP-sample (Mescheder et al., 2018) which is able to produce samples of state of the art quality without using progressive growing trick (Karras et al., 2018). The result in Fig. 4 shows that our method consistently outperforms GAN with 0-GP-sample. Image samples are given in appendix C.
5 RELATED WORKS
Gradient penalties are widely used in GANs literature. There are a plethora of works on using gradient penalty to improve the stability of GANs (Mescheder et al., 2018; Gulrajani et al., 2017; Petzka et al., 2018; Roth et al., 2017; Qi, 2017). However, these works mostly focused on making
9

Under review as a conference paper at ICLR 2019

Inception score

12

GAN-0GP-TTUR GAN-0-GP-sample-TTUR

11

10

9

8

7

200000

400000 Iteratio6n00000

800000

1000000

Figure 4: Inception score of GAN with our gradient penalty (GAN-0-GP) and GAN with the gradient penalty proposed in Mescheder et al. (2018) (GAN-0-GP-sample). To improve convergence we used TTUR with learning rate 0.0001 and 0.0003 for the generator and discriminator, respectively.

the training of GANs stable and convergent. Our work aims to improve the generalization capability of GANs via gradient regularization.
Arora et al. (2018) attempted to measure generalization capability of GANs using the birthday paradox. They showed that the number of modes in the model distribution grows linearly with the size of the discriminator. The result implies that higher capacity discriminators are needed for better approximation of the target distribution. Zhang et al. (2018) studied the tradeoff between generalization and discrimination in GANs. The authors showed that generalization is guaranteed if the discriminator set is small enough. In practice, rich discriminators are usually used for better discriminative power. Our proposed makes rich discriminators generalizable while remaining discriminative.
6 CONCLUSION
In this paper, we clarify the reason behind the poor generalization capability of GAN. We show that the original GAN loss does not guide the discriminator and the generator toward a generalizable equilibrium. We propose a zero-centered gradient penalty which pushes empirical discriminators toward the optimal discriminator with good generalization capability. Our gradient penalty provides better generalization and convergence guarantee than other gradient penalties. Experiments on diverse datasets verify that our method significantly improves the generalization and stability of GANs.
REFERENCES
M. Arjovsky and L. Bottou. Towards Principled Methods for Training Generative Adversarial Networks. ArXiv e-prints, January 2017.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein generative adversarial networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214­223, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (GANs). In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 224­232, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=BJehNfW0-.
10

Under review as a conference paper at ICLR 2019
William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=ByQpn1ZA-.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672­2680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5767­5777. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7159-improved-training-of-wasserstein-gans.pdf.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6626­6637. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7240-gans-trained-by-a-twotime-scale-update-rule-converge-to-a-local-nash-equilibrium.pdf.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 448­456, Lille, France, 07­09 Jul 2015. PMLR. URL http: //proceedings.mlr.press/v37/ioffe15.html.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hk99zCeAb.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3478­3487, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/mescheder18a.html.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1QRgziT-.
Vaishnavh Nagarajan and J. Zico Kolter. Gradient descent gan optimization is locally stable. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5585­5595. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7142-gradientdescent-gan-optimization-is-locally-stable.pdf.
Henning Petzka, Asja Fischer, and Denis Lukovnikov. On the regularization of wasserstein GANs. In International Conference on Learning Representations, 2018. URL https:// openreview.net/forum?id=B1hYRMbCW.
G.-J. Qi. Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities. ArXiv e-prints, January 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015. URL http:// arxiv.org/abs/1511.06434.
11

Under review as a conference paper at ICLR 2019

Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2018­2028. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/6797-stabilizing-training-ofgenerative-adversarial-networks-through-regularization.pdf.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discriminationgeneralization tradeoff in GANs. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hk9Xc lR-.

A CONSTRUCTING -OPTIMAL DISCRIMINATORS

To make the construction process simpler, let's assume that samples are normalized:

xi = yj = 1, xi  Dr, yj  Dg

Let's use the following new notations for real and fake samples:

D = Dr  Dg = {v1, ..., vm+n} vi = xi, for i = 1, ..., n vi = yi-n, for i = n + 1, ..., n + m
We construct the -optimal discriminator D as a MLP with 1 hidden layer. Let W1  R(m+n)×dx and W2  Rm+n be the weight matrices of D. The total number of parameters in D is dx(m + n) + (m + n) = O(dx(m + n)). We set the value of W1 as

and W2 as

 v1 

W1

=

k 

...

 

vn+m

1 W2,i = 2 + 2 + , for i = 1, ..., n

W2,i

=

1 - - , for i = n + 1, ..., n + m 22

>0

Given an input v  D, the output is computed as:

D(v) = W2 (W1v) where  is the softmax function. Let a = W1v, we have

ai = kvi v =

k, < k,

if v = vi if v = vi

As k  , (W1vi) becomes a one-hot vector with the i-th element being 1, all other elements being 0. Thus, for large enough k, for any vj  D, the output of the network is

D(vj ) = W2 (W1vj )  W2,j =

>

1 2

+

2,

for

j

=

1, ..., n

<

1 2

-

2,

for

j

=

n+

1, ..., n

+m

D is a -optimal discriminator for dataset D.

12

Under review as a conference paper at ICLR 2019

B FIXED-ALT-GD CANNOT MAINTAIN THE OPTIMALITY OF -DISCRIMINATORS

Let's consider the case where the real and the fake dataset each contain a single datapoint Dr = {x}, Dg(t) = y(t), and the discriminator and the generator are linear:
D(v) = Dv G(z) = Gz
and the objective is also linear (Wasserstein GAN's objective):
L = ExDr [D(x)] - EyDg(t) [D(y)] = D(x) - D(y(t))
The same learning rate  is used for D and G.
At step t, the discriminator is -optimal

D(x) - D(y(t)) = D x - y(t) 

(8)

D  x - y(t)

(9)

The gradients w.r.t. D and G are

L = x - y(t) D

L =

L × z

G

y(t)

= D × z

(10) (11)

If the learning rate  is small enough, x - y(t) should decrease as t increases. As the empirical
fake distribution converges to the empirical real distribution, x - y(t)  0. The gradient w.r.t. D, therefore, decreases as t increases and vanishes when the two empirical distributions are the same. From Eqn. 9, we see that, in order to maintain D's -optimality when x - y(t) decreases,
D has to increase. From Eqn. 9 and 11, we see that the gradient w.r.t. G grows as the two empirical distributions are more similar. As x - y(t)  0,

L

D L

0

G

(12)

Because the same learning rate  is used for both G and D, G will learn much faster than D. Furthermore, because x - y(t) decreases as t increases, the difference

-

x - y(t+1)

x - y(t)

also increases. The number of gradient steps that D has to take to reach the next -optimal state increases, and goes to infinity as x - y(t)  0. Therefore, gradient descent with fixed number of updates to D cannot maintain the optimality of D.
The derivation for the objective in Eqn. 1 is similar.

C RESULTS ON DIFFERENT DATASETS

13

Under review as a conference paper at ICLR 2019

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10 5 0

5

(a) Iter. 0

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10 10.0 10 5 0

5

(b) Iter. 1,000

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10 10.0 10 5 0

5

(c) Iter. 2,000

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10 10.0 10 5 0

5

(d) Iter. 5,000

10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10 10.0 10 5 0

5 10

(e) Iter. 20,000

Figure 5: Evolution of GAN-0-GP with  = 100 on 8 Gaussians dataset.

2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2 1 0 1
(a) GAN-0-GP

2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2 2.0 2

10

1

(b) GAN-1-GP

2

Figure 6: GANs trained with different gradient penalty on swissroll dataset. Although GAN-1-GP is able to learn the distribution, the gradient field has bad pattern. GAN-1-GP is more sensitive to change in hyper parameters and optimizers. GAN-1-GP fails to learn the scaled up version of the distribution.

(a) (b) (c) (d)
Figure 7: Result on MNIST. Adam was initialized with 1 = 0.5, 2 = 0.9. (a) No GP, iteration 10,000. (b) Zero-centered GP on real samples only with  = 100, iteration 10,000. (c) One-centered GP with  = 100, iteration 10,000. (d) Our zero-centered GP with  = 100, iteration 10,000.

(a) (b) Figure 8: Result on MNIST. Adam was initialized with 1 = 0.9, 2 = 0.99. (a) Zero-centered GP on real samples only with  = 100, iteration 10,000. (b) Our zero-centered GP with  = 100, iteration 10,000.
14

Under review as a conference paper at ICLR 2019
(a) All categories
(b) Dogs Figure 9: Samples from GAN-0-GP trained on ImageNet.
15

