Under review as a conference paper at ICLR 2019
LEARNING TO SEARCH EFFICIENT DENSENET WITH LAYER-WISE PRUNING
Anonymous authors Paper under double-blind review
ABSTRACT
Deep neural networks have achieved outstanding performance in many real-world applications with the expense of huge computational resources. The DenseNet, one of the recently proposed neural network architecture, has achieved the state-of-the-art performance in many visual tasks. However, it has great redundancy due to the dense connections of the internal structure, which leads to high computational costs in training such dense networks. To address this issue, we design a reinforcement learning framework to search for efficient DenseNet architectures with layer-wise pruning (LWP) for different tasks, while retaining the original advantages of DenseNet, such as feature reuse, short paths, etc. In this framework, an agent evaluates the importance of each connection between any two block layers, and prunes the redundant connections. In addition, a novel reward-shaping trick is introduced to make DenseNet reach a better trade-off between accuracy and float point operations (FLOPs). Our experiments show that DenseNet with LWP is more compact and efficient than existing alternatives.
1 INTRODUCTION
Deep neural networks are increasingly used on mobile devices, where computational resources are quite limited(Chollet, 2017; Sandler et al., 2018; Zhang et al., 2017; Ma et al., 2018). Despite the success of deep neural networks, it is very difficult to make efficient or even real-time inference on low-end devices, due to the intensive computational costs of deep neural networks. Thus, the deep learning community has paid much attention to compressing and accelerating different types of deep neural networks(Gray et al., 2017).
Among recently proposed neural network architectures, DenseNet (Huang et al., 2017b) is one of the most dazzling structures which introduces direct connections between any two layers with the same featuremap size. It can scale naturally to hundreds of layers, while exhibiting no optimization difficulties. In addition, it achieved state-of-the-art results across several highly competitive datasets. However, recent extensions of Densenet with careful expert design, such as Multi-scale DenseNet(Huang et al., 2017a) and CondenseNet(Huang et al., 2018), have shown that there exists high redundancy in DenseNet. Our paper mainly focuses on how to compress and accelerate the DenseNet with less expert knowledge on network design.
A number of approaches have been proposed to compress deep networks. Generally, most approaches can be classified into four categories: parameter pruning and sharing, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation(Gray et al., 2017). Unlike these approaches requiring intensive expert experience, automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Neural architecture search (NAS) has been successfully applied to design model architectures for image classification and language models (Liu et al., 2018; Zoph & Le, 2016; Pham et al., 2018; Liu et al., 2017a; Brock et al., 2017).
1

Under review as a conference paper at ICLR 2019
However, none of these NAS methods are efficient for DenseNet due to the dense connectivity between layers. It is thus interesting and important to develop an adaptive strategy for searching an on-demand neural network structure for DenseNet such that it can satisfy both computational budget and inference accuracy requirement. To this end, we propose a layer-wise pruning method for DenseNet based on reinforcement learning. Our scheme is that an agent learns to prune as many as possible weights and connections while maintaining good accuracy on validation dataset. As illustrated in Figure 1, our agent learns to output a sequence of actions and receives reward according to the generated network structure on validation datasets. Additionally, our agent automatically generates a curriculum of exploration, enabling effective pruning of neural networks. Extensive experiments on several highly competitive datasets show that our method largely reduces the number of parameters as well as flops, while maintaining or slightly degrading the prediction performance, such that the corresponding network architecture can adaptively achieve a balance between inference accuracy and computational resources.

(a) Vanilla DenseNet

(b) DenseNet with layer-wise pruning
Figure 1: An illustration of layer-wise pruning method based on vanilla DenseNet. For one layer, not all connections are required and each layer has its unique connections. After being pruned some dense connections, the network can still predict correctly.

2 BACKGROUND

We first introduce reinforcement learning and policy gradient in Section 2.1, and DenseNet in Section 2.2, and finally neural architecture search in Section 2.3.

2.1 REINFORCEMENT LEARNING AND POLICY GRADIENT

Reinforcement learning considers the problem of finding an optimal policy for an agent that interacts with
an uncertain environment and collects reward per action(Sutton et al., 1998). The goal of the agent is to
maximize the long-term cumulative reward. Formally, this problem can be formulated as a Markov decision process over the environment states s  S and agent actions a  A, under an unknown environmental dynamic defined by a transition probability T (s |s, a) and a reward signal r(s, a) immediately following the action a performed at state s. The agent's action a is selected by a conditional probability distribution (a|s) called policy or actor. In policy gradient methods, we consider a set of candidate policies (a|s) parameterized by  and obtain the optimal policy by maximizing the expected cumulative reward or return

J () = Es,a(a|s) [r(s, a)] ,

(1)

2

Under review as a conference paper at ICLR 2019

where (s) =

 t=1

t-1Pr(st

=

s)

is

the

normalized

discounted

state

visit

distribution

with

a

discount

factor   [0, 1). To simplify the notation, we denote Es,a(a|s)[·] by simply E[·] in the rest of paper.

According to the policy gradient theorem(Sutton et al., 1998), the gradient of J() can be written as

J () = E [ log (a|s)Q(s, a)] ,

(2)

where Q(s, a) = E

 t=1

t-1r(st,

at)|s1

=

s, a1

=

a

denotes the expected return under policy  after

taking an action a in state s, which is also called critic.

Since the expectation in Eq (3) is over action, it is helpful to estimate a value function V (s) and subtract it

from Q(s, a) to reduce variance while keeping unbiased.

J() = E [ log (a|s) (Q(s, a) - V (s))] .

(3)

The most straightforward way is to simulate the environment with the current policy  to obtain a trajectory {(st, at, rt)}tn=1 and estimate J () using the Monte Carlo estimation:

^ J ()

=

1 n

n

t-1 log (at|st)

Q^(st, at) - V^ (st)

,

t=1

(4)

where Q^(st, at) is an empirical estimate of Q(st, at), e.g., Q^(st, at) = jt j-trj, and V^ (st) is an empirical estimate of V (s).

2.2 DENSENET
Densely connected networks(Huang et al., 2017b) consist of multiple dense blocks, each of which also consists of multiple layers. Each layer produces k features maps, where k is referred to the growth rate of the network. The distinguishing property of DenseNets is that the input of each layer is a concatenation of all feature maps generated by all preceding layers within the same dense block.
Inside every dense block, the first transformation is a composition of batch normalization(BN) and rectified linear units(RELU), followed by the first convolutional layer in the sequence which reduces the number of channels to save computational cost by using the 1 × 1 filters. The output is then followed by another BN-ReLU combination transformation and is then reduced to the final k output features through a 3 × 3 convolution.

2.3 NEURAL ARCHITECTURE SEARCH
Neural Architecture Search(NAS) is a method for automated design of neural network structures, with the aid of either evolutionary algorithms(Xie & Yuille, 2017; Real et al., 2017) or reinforcement learning (Baker et al., 2016; Cai et al., 2018; Zhong et al., 2017; Zoph & Le, 2016; Zoph et al., 2017). When using reinforcement learning, the agent performs a sequence of actions, which specifies a network structure; this network is then trained and its corresponding validation performance is returned as the reward to update the agent.

3 METHOD
We analyze the dense connections of DenseNet in Section 3.1, then we model the layer-wise pruning as a Markov decision process (MDP)(Puterman, 2014) and design a Long-short term memory( LSTM)(Hochreiter & Schmidhuber, 1997) controller to generate inference paths in Section 3.2. The interaction between the agent (i.e., the LSTM controller) and the environment (i.e., the DenseNet) is described in Figure 2. The reward shaping technique in our method is introduced in Section 3.3. Finally, we show the complete training process of LWP in Section 3.4.

3

Under review as a conference paper at ICLR 2019

3.1 PRETRAINED DENSE CONVOLUTIONAL NETWORKS
Vanilla DenseNet consists of four parts: the first convolution layer, multiple dense blocks, transition layers and finally the fully-connected layer. The first convolution layer is only for feature extraction from raw data. As for the multiple dense blocks, each dense block consists of multiple layers. The transition layers are used as down-sampling layers to change the size of feature maps and the last full-connected layer is used for image classification. Obviously, the dense connections are mainly reflected on the dense blocks. Therefore, we study the connection policy for dense layers in this paper.

3.2 GENERATE INFERENCE PATHS WITH AN LSTM CONTROLLER

Suppose the DenseNet has L layers, the controller needs to make K (equal to the number of layers in

dense blocks) decisions. For layer i, we specify the number of previous layers to be connected in the

range between 0 and ni (ni = i). All possible connections among the DenseNet constitute the action

space of the agent. However, the time complexity of traversing the action space is O(

K i=1

2ni ),

which

is

NP-hard and unacceptable for DenseNet(Huang et al., 2017b). Fortunately, reinforcement learning is good at

solving sequential decision optimization problems and we model the network pruning as a Markov Decision

Process(MDP). Since these hierarchical connections have time-series dependencies, it is natural to train

LSTM as the controller to simply solve the above-mentioned issue.

Figure 2: Illustration of our proposed framework. In each iteration, the output of the i-th time step makes keeping or dropping decisions for the i-th layer. All outputs of the LSTM controller generate a child network by sampling from K × K-dimensional Bernoulli distribution. Then, the child network forwards propagation with mini-batch samples and
the reward function can be evaluated with the predictions and FLOPs. The controller is optimized with policy gradient.

At the first time step, the LSTM controller receives an empty embedding vector as the input that is regarded

as the fixed state s of the agent, and the output of the previous time step is the input for the next time step.

Each

output

neuron

in

the

LSTM

is

equipped

with

(x)

=

1 1+e-x

,

so

that

the

output

oi

defines

a

policy

pi,ai

of keeping or dropping connections between the current layer and its previous layers as an ni-dimensional

Bernoulli distribution:

oi = f (s; c),

pi,ai =

ni j=1

oiajij

(1

-

oij )(1-aij ),

(5)

where f denotes the controller parameterized with c. The j-th entry of the output vector oi, denoted by oij  [0, 1], represents the likelihood probability of the corresponding connection between the i-th layer and

4

Under review as a conference paper at ICLR 2019

the j-th layer being kept. The action ai  {0, 1}ni is sampled from Bernoulli(oi). aij = 1 means keeping the
connection, otherwise dropping it. There are total ni connections for the i-th layer, but the output dimension
of LSTM at each time step is K. To unify the action space dimension and LSTM output dimension, we set both to K and the output of each time step take a mask  {0, 1}K operation, where the mask numbers from

1-th to ni-th element are 1 and others are 0. Finally, the probability distribution of the whole neural network architecture is formed as:

(a1:K |s; c) =

K
i=1 pi,ai

(6)

3.3 REWARD SHAPING

Reward shaping is introduced to help the controller make progress to an optimal solution. The reward function

is designed for each sample and not only considers the prediction correct or not, but also encourages less

computation:

1 -  if predict correctly

R(a) = -

otherwise.

(7)

where



=

SU BF LOP s F LOP s

measures

the

percentage

of

float

operations

utilized.

SUBFLOPs,

FLOPs

represent

the float point operations of the child network and vanilla DenseNet, respectively. In order to maximize

the reward, the prediction needs to be correct and SUBFLOPs should be reduced as much as possible. The

trade-off between performance and complexity is mainly controlled by  and  and more details will be

discussed in the Section 7.3 of the appendix.

3.4 TRAINING WITH ADVANTAGE ACTOR-CRITIC

After obtaining the feedback from the child network, we modify the Eq (1) as the following expected reward:

J (c) = Eac [r(s, a)]

(8)

To maximize Eq (8) and accelerate policy gradient training over c, we utilize the advantage actor-critic(A2C) with an estimation of state value function V (s; v) to derive the gradients of J(c) as:

c J (c) = (r(s, a) - V (s; v)) (a|s, c)c log (a|s, c)
a

The Eq (9) can be approximated by using the Monte Carlo sampling method:

1n c J (c) = n

r(t)(s, a) - V (s; v) c log (a|s, c)

t=1

(9) (10)

where n is the batch size. The mini-batch samples share the same child network and perform forward propagation in parallel. Therefore, they have the same policy distribution (a|s, c) but different r(s, a). We further improve exploration to prevent the policy from converging to suboptimal deterministic policy by adding the entropy of the policy (a|s, c),H((a|s, c)) to the objective function. The gradient of the full objective function takes the form:

1n c J (c) = n
t=1

r(t)(s, a) - V (s, v) c log (a|s, c) + c H((a|s, c))

(11)

As for the value network, we define the loss function as Lv and utilize gradient descent methods to update v:

1n Lv = n

2
r(t)(s, a) - V (s; v) ,

t=1

v Lv

=

2 n

n

t=1

r(t)(s, a) - V (s; v)

V (s; v) v

(12)

5

Under review as a conference paper at ICLR 2019

The entire training procedure is divided into three stages: curriculum learning, joint training and training from scratch. Algorithm 1 shows the complete recipe for layer-wise pruning.

Curriculum learning. It is easy to note that the search space scales exponentially with the block layers of

DenseNet and there are total

K i=1

2ni

keeping/dropping

configurations.

We

use

curriculum

learning(Bengio,

2013) like BlockDrop(Wu et al., 2018) to solve the problem that policy gradient is sensitive to initialization.

For epoch t (1  t < K), the LSTM controller only learns the policy of the last t layers and keeps the policy

of the remaining K - t layers consistent with the vanilla DenseNet. As t  K, all block layers are involved

in the decision making process.

Joint training. The previous stage just updates parameters c and v. The controller learns to identify connections between two block layers to be kept or dropped. However, it prevents the agent from learning the optimal architecture. Jointly training the DenseNet and controller can be employed as the next stage so that the controller guides the gradients of v to the direction of dropping more connections.

Training from scratch. After joint training, several child networks can be sampled from the policy distribution (a|s, c) and we select the child network with the highest reward to train from scratch, and thus better experimental results have been produced.
We summarize the entire process in Algorithm 1.

4 RELATED WORK
Huang et al. (2018) proposed group convolution to remove connections between layers in DenseNet for which this feature reuse is superfluous; Huang et al. (2017a) also suggested progressively update prediction for every test sample to unevenly adapt the amount of computational resource at inference time. The most related work is BlockDrop (Wu et al., 2018), which used reinforcement learning to prune weight dynamically at inference time but can only be applied to ResNet or its variants. In contrast, our approach is based on DenseNet, aiming to find efficient network structure based the densely connected features of DenseNet.

5 EXPERIMENT
We evaluate the LWP method on three benchmarks: CIFAR-10, CIFAR-100 (Krizhevsky & Hinton, 2009) and ImageNet 2012 (Deng et al., 2009) and these three datasets are used for image classification. Details of experiments and hyperparameters setting in Appendix 7.3.
5.1 RESULTS ON CIFAR
Pretrained DenseNet. For CIFAR datasets, DenseNet-40-12 and DenseNet-100-12 are selected as the backbone CNN. During the training time, the backbone CNN needs to make predictions with dynamic computation paths. In order to make the backbone CNN adjust to our algorithm strategy, we reproduced the DenseNet-40-12 and DenseNet-100-12 on CIFAR based on Pytorch (Paszke et al., 2017) and the results are shown in Table 1.
Comparisons and analysis. The results on CIFAR are reported in Table 1. For CIFAR-10 dataset and the vanilla DenseNet-40-12, our method has reduced the amounts of FLOPs, parameters by nearly 81.4%, 78.2%, respectively and the test error only increase 1.58%. The exponential power  and penalty  can be tuned to improve the performance. In this experiment, we just modify hyperparameter  from 2 to 3 so that the model

6

Under review as a conference paper at ICLR 2019

complexity(105M vs 173M FLOPs) is increased while test error rate is reduced to 6.00%.The same law can be observed on the DenseNet-100-12 with LWP. Our algorithm also has advantages on Condensenet (Huang et al., 2018) which needs more expert knowledge and NAS (Zoph & Le, 2016) which takes much search time complexity and needs more parameters but gets higher test error.
We can also observe the results on CIFAR-100 from the Table 1 that the amounts of FLOPs in DenseNet with LWP are just nearly 46.5%, 66.3% of the DenseNet-40-12 and DenseNet-100-12. The compression rates are worse than that for CIFAR-10. This may be caused by the complexity of the CIFAR-100 classification task. The more hard task, the more computation is needed.

Model
DenseNet-40-12 (Huang et al., 2017b)(our impl.) DenseNet-100-12 (Huang et al., 2017b)(our impl.)
VGG-16-Pruned (Li et al., 2016) VGG-19-pruned (Liu et al., 2017b) VGG-19-pruned (Liu et al., 2017b)
ResNet-110-pruned (Li et al., 2016) DenseNet-40-pruned (Liu et al., 2017b) CondenseNetlight-94 (Huang et al., 2018) CondenseNet-86 (Huang et al., 2018)
NAS v2 predicting strides (Zoph & Le, 2016) DenseNet-40-12-LWP ( = 2,  = -0.5) DenseNet-40-12-LWP ( = 2,  = -0.5) DenseNet-40-12-LWP ( = 3,  = -0.5) DenseNet-100-12-LWP ( = 2,  = -0.5) DenseNet-100-12-LWP ( = 2,  = -0.5)

FLOPs 566M 3.63G 206M 195M 250M 213M 190M
122M 65M
105M 263M 173M 716M 2.42G

Params 1.10M 7.19M 5.40M 2.30M 5.00M 1.68M 0.66M
0.33M 0.52M 2.5M 0.24M 0.66M 0.40M 1.43M 5.15M

CIFAR-10 5.24 4.34 6.60 6.20 6.45 5.19
5.00 5.00 6.01 6.82
6.00 5.12
-

CIFAR-100 25.09 20.88 25.28 26.52 25.28
24.08 23.64
26.99 21.14

Table 1: Results on CIFAR. DenseNet-40-12 and DenseNet-100-12 are selected as the backbone CNN on CIFAR dataset and our algorithm is applied to the two models. The FLOPs, parameters and test error of the DenseNet with LWP are compered with the vanilla DenseNet and the neural network architecture with other pruned methods.

5.2 RESULTS ON IMAGENET
Pretrained DenseNet. We compress the DenseNet-121-32 which has four dense blocks([6, 12, 24, 16]) on ImageNet. The growth rate of DenseNet-121-32 is 32 and this neural network architecture is equipped with bottleneck layers and compression ratio fixed at 0.5 that are designed to improve the model compactness. In the following section, we prove that the model can be further compressed. This model is initialized by loading the checkpoint file of pretrained model from Pytorch.

Make comparisons and analysis. Although the bottleneck layer and compression ratio are introduced in DenseNet-121-32, the result shows that there is still much redundancy. As observed from Table 2, we can still reduce 54.7% FLOPs and 35.2% parameters of the vanilla DenseNet-121-32 with 1.84% top-1 and 1.28% top-5 test error increasing.

Model

FLOPs Params Top-1 Top-5

DenseNet-121-32-BC (Huang et al., 2017b) 5.67G 7.98M 25.35 7.83

DenseNet-121-32-BC-LWP

2.57G 5.17M 27.19 9.11

Table 2: Results on ImageNet. DenseNet-121-32 is selected as the backbone CNN on ImageNet. It can be further compressed even if its parameters are already quite efficient.

7

Under review as a conference paper at ICLR 2019

Number of input channel Source layer(s)

450

400

350

D40-12-LWP D40-12

300

250

200

150

100

50

0 0 5 10 15 20 25 30 35

0 2 4 6 8 10 12 14 16 18 20 22 24
1 3 5 7 9 11 13 15 17 19 21 23 25

1.0 0.8 0.6 0.4 0.2 0.0

Layer index

Target layer(t)

Figure 3: Quantitative results on DenseNet-40-12 wth LWP. Lef t: the number of input channel in vanilla DenseNet40-12 and the learned child network. Right: the connection dependency between any two layers is represented as the average absolute wights of convolution layer.

5.3 QUANTITATIVE RESULTS
In this section, we argue that our proposed methods can learn more compact neural network architecture by analyzing the number of input channel in DenseNet layer and the connection dependency between a convolution layer with its preceding layers.
In Figure 3 lef t, the red bar represent the number of input channel in DenseNet-40-12-LWP (D40-12-LWP) and the blue bar represent the number of input channel in vanilla DenseNet. We can observe that the number of input channels grows linearly with the layer index because of the concatenation operation and D40-12-LWP has layer-wise input channels identified by the controller automatically. The input channel is 0 means this layer is dropped so that the block layers is reduced from 36 to 26. The number of connections between a layer with its preceding layers can be obtained from the right panel of Figure 3. In Figure 3 right, the x, y axis define the target layer t and source layer s. The small square at position (s, t) represents the connection dependency of target layer t on source layer s. The pixel value of position (s, t) is evaluated with the average absolute filter weights of convolution layers in D40-12-LWP. One small square means one connection and the number of small squares in the vertical direction indicates the number of connections to target layer t.
As reported by the paper DenseNet(Huang et al., 2017b), there are redundant connections because of the low kernel weights on average between some layers. The right panel of Figure 3 obviously shows that the values of these small square connecting the same target layer t are almost equal which means the layer t almost has the same dependency on different preceding layers. Naturally, we can prove that the child network learned from vanilla DenseNet is quite compact and efficient.
6 CONCLUSION
We propose an algorithm strategy to search efficient child network of DenseNet with reinforcement learning agent. The LSTM is used as the controller to layer-wise prune the redundancy connections. The whole process is divided into three stages: curriculum learning, joint training and training from scratch. The extensive experiments based on CIFAR and ImageNet show the effectiveness of our method. Analyzing the child network and the filter parameters in every convolution layer prove that our proposed method can learn to search compact and efficient neural network architecture.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.
Yoshua Bengio. Deep learning of representations: Looking forward. In International Conference on Statistical Language and Speech Processing, pp. 1­37. Springer, 2013.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.
Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by network transformation. AAAI, 2018.
Franc¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint, pp. 1610­02357, 2017.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. Ieee, 2009.
Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. Technical report, Technical report, OpenAI, 2017.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735­1780, 1997.
Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q Weinberger. Multi-scale dense networks for resource efficient image classification. arXiv preprint arXiv:1703.09844, 2017a.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017b.
Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: An efficient densenet using learned group convolutions. group, 3(12):11, 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. arXiv preprint arXiv:1712.00559, 2017a.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 2755­2763. IEEE, 2017b.
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. arXiv preprint arXiv:1807.11164, 2018.
9

Under review as a conference paper at ICLR 2019
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. arXiv preprint arXiv:1802.03268, 2018.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, and Alex Kurakin. Large-scale evolution of image classifiers. arXiv preprint arXiv:1703.01041, 2017.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. arXiv preprint arXiv:1801.04381, 2018.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998. Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen Grauman, and
Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks. 2018. Lingxi Xie and Alan L Yuille. Genetic cnn. In ICCV, pp. 1388­1397, 2017. X Zhang, X Zhou, M Lin, and J Sun. Shufflenet: An extremely efficient convolutional neural network for
mobile devices. arxiv 2017. arXiv preprint arXiv:1707.01083, 2017. Zhao Zhong, Junjie Yan, and Cheng-Lin Liu. Practical network blocks design with q-learning. arXiv preprint
arXiv:1708.05552, 2017. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for
scalable image recognition. arXiv preprint arXiv:1707.07012, 2(6), 2017.
10

Under review as a conference paper at ICLR 2019
7 APPENDIX
7.1 DATASETS AND EVALUATION METRICS
CIFAR-10 and CIFAR-100 consists of 10 and 100 classes images with 32 × 32 RGB pixels. Both datasets contain 60, 000 images, of which 50, 000 images for training sets and 10, 000 images for test sets. We use a standard data pre-processing and augmentation techniques and the complete procedure is: normalize the data by using the channel means and standard deviations, centrally pad the training images with size 4, randomly crop to restore 32 × 32 images and randomly flip with probability 0.5 horizontally. The evaluation metric in CIFAR is the prediction accuracy. There are total 1.33 million colored images with 1000 visual classes in ImageNet, 1.28 million for training images and 50k for validation images. We also adopt the data-augmentation scheme for pre-processing, ie: resize the images to 256 × 256, normalize the images using channel means and standard derivations, randomly crop to 224 × 224 and flip horizontally at training time but apply a center crop with size 224 × 224 at test time. The performance in ImageNet is measured by both top-1 and top-5 prediction accuracy.
7.2 TRAINING CONFIGURATIONS
Training configurations for CIFAR. Based on the pretrained DenseNet, the LSTM controller is trained with batch size 128 for 1000 epochs during the curriculum learning procedure and ADAM optimizer without weight decay is adopt. The learning rate starts from 10-3 and it is lowered by 10 times at epoch 500 and 750. For the joint training, we fix the learning rate at 10-4 and finetune the model for 1000 epochs. Then we select the optimal child network with highest reward to train from scratch. In the last stage, the SGD optimizer with a weight decay of 10-4 and a Nesterov momentum of 0.9 without dampening is adopt. We train the optimal child network with mini-batch size 64 and a cosine shape learning rate from 0.1 to 0 for 300 epochs.
Training configurations for ImageNet. For curriculum learning and joint training, we set the epochs 90, 50 respectively and batch size 1024. In curriculum learning procedure, the learning rate is set to 1e-3 and is lowered 10 times in epoch 45 and 75. The learning rate is fixed at 1e-4 for joint training procedure. We use the same optimizer parameters as CIFAR experiments. At last, the learned optimal child network is optimized like DenseNet(Huang et al., 2017b).
7.3 HYPERPARAMETERS SEARCH
We use reward shaping technique in our model and the detailed reward formulation is defined in Eq (7). The trade-off between the model performance and complexity can be controlled by adjusting different reward functions. As shown in Eq (7), the reward function mainly depends on the exponential power  of FLOPs ratio and the penalty -. We mainly analyzed and explored these two factors of the child network (D40-12-LWP) based on DenseNet-40-12 (D40-12) and CIFAR-10 dataset in the following section.
Exponential power. Given one policy, pass a image to the child network and we hope to get higher reward if the prediction is correct. The lower the FLOPs of the child network, the larger the reward value if fix . On the contrary, in order to get the same reward value, the exponential power  is bigger and the model complexity is larger. As shown in Figure 4 (a) (b), with setting exponential power  = 1/3, 1/2, 1, 2, 3 and fixing  = 0.5, the classification accuracy and FLOPs of the child network also increase gradually.
Penalty. Considering the incorrect prediction, the penalty - is given as the feedback. The bigger  means the controller emphasis on the model performance rather than the reduced model complexity.  is set to
11

Under review as a conference paper at ICLR 2019

ACC(%) FLOPs(x1e7 ) ACC(%) FLOPs(x1e7 )

95 90 85 D40-12-ACC
D40-12-LWP-ACC 80 75 700.0 0.5 1.0 1.5 2.0 2.5 3.0

7 6 5 4 D40-12-FLOPs
D40-12-LWC-FLOPs 3 2 1 00.0 0.5 1.0 1.5 2.0 2.5 3.0

95 90 85 D40-12-FLOPs
D40-12-LWC-FLOPs 80 75 700.0 0.2 0.4 0.6 0.8 1.0

7 6 5 4 D40-12-FLOPs
D40-12-LWC-FLOPs 3 2 1 00.0 0.2 0.4 0.6 0.8 1.0

Exponential power()

Penalty()

Exponential power()

Penalty()

(a) (b) (c) (d)
Figure 4: Hyperparameters search on CIFAR-10 to explore the trade off between FLOPs and accuracy by changing the exponential power of FLOPs ratio and penalty -.

0.25, 0.5, 0.75, 1 and exponential power  is fixed at 2, respectively. The results is shown in Figure 4 (c) (d) and we can observe that both curves are increased first and then decreased.
7.4 ALGORITHM
Algorithm 1 The pseudo-code for layer-wise pruning.
Input: Training dataset Dt; Validation dataset Dv; Pretrained DenseNet. Initialize the parameters c of the LSTM controller and v of the value network randomly. Set epochs for curriculum learning, joint training and training from scratch to M cl, M jt and M fs respectively and sample Z child networks.
Output: The optimal child network 1: //Curriculum learning 2: for t = 1 to M cl do 3: o = f (s; c) 4: if t < K - t then 5: for i = 1 to K - t do 6: o[i, 0 : i] = 1 7: o[i, i :] = 0 8: end for 9: end if 10: Sample a from Bernoulli(o) 11: DenseNet with policy makes predictions on the training dataset Dt 12: Calculate feedback R(a) with Eq (7) 13: Update parameters c and v with Eq (11) and Eq (12) respectively 14: end for 15: //Joint training 16: for t = 1 to M jt do 17: Simultaneously train DenseNet and the controller 18: end for 19: for t = 1 to Z do 20: Sample a child network from (a|s, c) 21: Execute the child network on the validation dataset Dv 22: Obtain feedback R(t)(a) with Eq (7) 23: end for 24: Select the child network N with highest reward 25: //Training from scratch 26: for t = 1 to M fs do 27: Train the child network N from scratch 28: end for 29: return The optimal child network N

12

