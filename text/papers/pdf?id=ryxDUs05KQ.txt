Under review as a conference paper at ICLR 2019
DIFFERENCE-SEEKING GENERATIVE ADVERSARIAL NETWORK
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, pt, are difficult to collect. Suppose there are two distributions pd¯ and pd such that the density of the target distribution can be the differences between the densities of pd¯ and pd. We show how to learn the target distribution pt only via samples from pd and pd¯ (relatively easy to obtain). DSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.
1 INTRODUCTION
In machine learning, how to learn a probability distribution is usually conducted in a unsupervised learning manner. Generative approaches are developed for learning data distribution from its samples and thereafter produce novel and high-dimensional samples from learned distributions, such as image and speech synthesis (Saito et al. (2018)). The state-of-the-art approaches include variational autoencoders (VAE)(Kingma & Welling (2014)), Pixel-CNN (Oord et al. (2016)), and Generative Adversarial Networks (GAN)(Goodfellow et al. (2014)). VAE is relatively easy to implement and robust to hyperparameter choices with useful latent representation. But, it might have the "blurring" effect on images. PixelCNN produces sharp images and obtains explicit likelihood of training data along with good evaluation metric, but is slow due to sequential generation and does not have a latent representation. GAN also produces sharp images based on a game-theoretic framework, but can be tricky and unstable to train due to multiple interacting losses. Correct hyperparameter selection is critical in GAN. In addition, hybrid methods (Larsen et al. (2016), Tolstikhin et al. (2018)) try to take various advantages of VAE, PixelCNN, and GAN, but with image quality being not better than that obtained from GAN. In this paper, our study focuses on GAN.
GAN consists of two functions: generator and discriminator. Both functions are represented as parameterized neural networks. The discriminator network is trained to classify whether or not inputs belong to real data or fake data created by the generator. The generator learns to map a sample from a latent space to some distribution to increase the classification errors of discriminator. GAN corresponds to a minimax two-player game, which ends if the generator actually learns the real data distribution. The generator is of main interest because the discriminator will be unable to differentiate between both distributions once the generator has been trained well.
One of requirements in GAN is to prepare training samples drawn from the the target distribution that the generator captures. However, it is sometimes difficult to collect training samples, including, for example, the out-of-distribution examples in MNIST. To overcome this difficulty, the out-ofdistribution examples, from another point of view, are found to belong to the difference of the set of examples in MNIST and the universal set. Examples in both sets are relatively easy to obtain. Thus, if out-of-distribution examples can be produced by GAN, which is trained only by examples in both sets (e.g., MNIST and its universal set), this problem can be solved. Nevertheless, the existing GAN methods are not found to utilize such an idea. It should be noted that the target distribution is equal to the training data distribution in traditional GANs; nevertheless, both distributions are considered different in DSGAN.
1

Under review as a conference paper at ICLR 2019

In this paper, we make the following contributions:
(1) We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), where the density of target distribution pt is the difference between those of any two distributions, pd¯ and pd. Nevertheless, the differences of two densities being negative are not well-defined. Thus, instead of learning the target distribution pt directly, the generator distribution approximates pt by minimizing the statistical distance between the mixture distribution of pd and the generator distribution, and pd¯.
(2) Theoretical results based on traditional GAN are extended to the case of mixture distribution considered in this paper. With enough capacity of the generator and the discriminator, we show DSGAN converges when the optimum of the Jensen-Shannon divergence is attained such that the generator distribution is equal to the target distribution.
(3) We show that DSGAN possesses the flexibility to learn different target distributions in two key applications: semi-supervised learning and adversarial training. Samples from target distribution in semi-supervised learning must satisfy two conditions: i) are linear combination of any label data and unlabel data; ii) do not belong to neither label data nor unlabel data. For adversarial training, samples from the target distribution are assigned as out-of-distribution examples with bounded distortion. Experiments validate that DSGAN can learn these two kinds of distributions well in various datasets.
The paper is structured as follows. In Sec. 2, we introduce DSGAN, including the algorithm in the training procedure. Theoretical results are described in Sec. 3 along with a trick to stabilize the training procedure. In Sec. 4, two applications are taken as examples to show the effectiveness of DSGAN. Finally, we present the experimental results in Sec. 5 and conclude by pointing out some promising directions for future work in Sec. 6.

2 PROPOSED METHOD-DSGAN

We denote the generator distribution as pg and training data distribution as pd, both in a N -
dimensional space. Let pd¯ be the distribution decided by user. For example, pd¯ can be the convolution of pd and normal distribution. Let pt be the target distribution such that

(1 - )pt(x) + pd(x) = pd¯(x),

(1)

where   [0, 1]. Our method, DSGAN, aims to learn pg such that pg = pt. In other words,
the generator is prone to output samples located in high-density areas of pd¯ and low-density areas of pd. Fig. (1)(a) illustrates an example to generate boundary data given training data in one-
dimensional space. We design pd¯ as the convolution of pd and the normal distribution. When x belongs to boundary data, the density of pd¯(x) is higher than that of pd(x), meaning that G generates boundary points with probability being larger than 0. Thus, DSGAN achieves the goal.
More examples are illustrated in a two-dimensional space in Figs. (1)(b) and (c).

Intuitively,

our method tries to learn pg

such that pg(x)



pd¯(x) - pd(x) 1-

.

Moreover, we adopt

the MNIST dataset to illustrate another example in Fig. (1)(d), where training data are images of

digits "1" and "7". Let the examples from images of digit "1" be assigned as samples from pd and

let the samples from pd¯ be examples from images of digits "1" and "7". Since the density pd(x) is high when x is digit "1," the generator outputs digit "7" with high probability.

The inputs z of the generator are drawn from pz (z) in an M -dimensional space. The generator function G(z; g) : RM  RN represents a mapping to data space, where G is a differentiable function with parameters g. The discriminator is defined as D (x; d) : RN  [0, 1] that outputs a single scalar. D (x) can be considered as the probability that x belongs to a class of real data.

Similar to traditional GAN, we train D to distinguish the real data and the fake data sampled from
G. Meanwhile, G is trained to produce realistic data as possible to mislead D. But, in DSGAN,
the definitions of "real data" and "fake data" are different from those in traditional GAN. The sam-
ples from pd¯ are considered as real but those from the mixture distribution between pd and pg are considered as fake.

2

Under review as a conference paper at ICLR 2019

(a)

(b) (c) (d)

Figure 1: Illustrations of DSGAN. (a) shows a simple illustration about generating boundary data
around training data. First, the convolution of pd and normal distribution makes the density on
boundary data be no longer zero. Second, we seek pg such that Eq. (1) holds, where the support set of
pg is approximated by the difference of those between pd¯ and of pd. For (b) and (c), the orange points are samples drawn from pd. By assigning pd¯ to the convolution of pd and the normal distribution, pg generates boundary points (green points) around pd. For (d), pd denotes the distribution of digit
"1" in the MNIST dataset, the samples drawn from pd¯ are digits "1" and "7", and pg generates digit "7" with high probability. From figures (b)(c)(d), we can observe two properties of pg: i) the higher
density of pd(x), the lower density of pg(x); ii) pg prefers to output samples from high-density areas
of pd¯.  = 0.8 in Eq. (1) is used.

The objective function is defined as follows:

min max V
GD

(G, D)

=

Expd¯(x)

[log D(x)] + (1

-

)Ezpz(z) [log (1

-

D (G (z)))]

+ Expd(x) [log (1 - D(x))] .

(2)

During the training procedure, an iterative approach like traditional GAN is to alternate between k steps of training D and one step of training G. In practice, minibatch stochastic gradient descent via back propagation is used to update d and g. In other words, for each of pg, pd and pd¯, m sample are required for computing gradients, where m is the number of samples in a minibatch. Algorithm 1 illustrates the training procedure in detail. DSGAN suffers from the same drawbacks with traditional GAN (e.g., mode collapse, overfitting, and strong discriminator such that the generator gradient vanishes). There are literatures (Salimans et al. (2016), Arjovsky & Bottou (2017), and Miyato et al. (2018)) focusing on improving the above problems, and their ideas can be combined into DSGAN.

Li et al. (2017) and Reed et al. (2016) proposed the similar objective function like (2). Their goal is to learn the conditional distribution of training data. Nevertheless, we aim to learn the target distribution pt in Eq. (1), not the training data distribution.

3

Under review as a conference paper at ICLR 2019

In the next section, we will show that the objective function is equivalent to minimizing the Jensen-
Shannon divergence between the mixture distribution (pd and pg) and pd¯ as G and D are given enough capacity. Furthermore, we provide a trick by reformulating the objective function (2) such
that it is more stable to train DSGAN.

Algorithm 1 The training procedure of DSGAN using minibatch stochastic gradient descent. k is the number of steps to apply to the discriminator.  is the ratio between pg and pd in the mixture distribution. We used k = 1 and  = 0.8 in experiments.
01. for number of training iterations do 02. for k steps do 03. Sample minibatch of m noise samples z(1), ..., z(m) from pg(z). 04. Sample minibatch of m samples x(d1), ..., x(dm) from pd(x). 05. Sample minibatch of m samples xd(¯1), ..., xd(¯m) from pd¯(x). 06. Update the discriminator by ascending its stochastic gradient:

d

1m log D
m

x(di)

+ log 1 - D

G z(i)

i=1

+ log 1 - D xd(¯i)

07. end for 08. Sample minibatch of m noise samples z(1), ..., z(m) from pg(z). 09. Update the generator by descending its stochastic gradient:

1m g m

log 1 - D G z(i)

i=1

10. end for

3 THEORETICAL RESULTS

There are two assumptions for subsequent proofs. First, in a nonparametric setting, we assume both generator and discriminator have infinite capacity. Second, pg is defined as the distribution of the samples drawn from G(z) under z  pz. We will first show the optimal discriminator given G and then show that minimizing V (G, D) via G given the optimal discriminator is equivalent to minimizing the Jensen-Shannon divergence between (1 - )pg + pd and pd¯.
Proposition 1. For G being fixed, the optimal discriminator D is

DG (x)

=

pd(x)

+

(1

pd(x) - )pg(x)

+

. pd(x)

Proof. See Appendix A.1 in details.

Moreover, D can be considered to discriminate between samples from pd¯ and those from ((1 - )pg(x) + pd(x)). By replacing the optimal discriminator into V (G, D), we obtain

C(G) = max V (G, D)
D

= Expd¯(x) [log DG (x)] + (1 - )Ezpz(z) [log (1 - DG (G (z)))] + Expd(x) [log (1 - DG (x))]
= Expd¯(x) [log DG (x)] + Ex(1-)pg(x)+pd(x) [log (1 - DG (x))]

= Expd¯(x)

log pd¯(x) pd¯(x) + (1 - )pg(x) + pd(x)

+ Ex(1-)pg(x)+pd(x)

log

(1 - )pg(x) + pd(x) pd¯(x) + (1 - )pg(x) + pd(x)

,

(3)

4

Under review as a conference paper at ICLR 2019

where the third equality holds because of the linearity of expectation.
Actually, the previous results show the optimal solution of D given G being fixed in (3). Now, the next step is to find the optimal G with DG being fixed. Theorem 1. Suppose pd(x)  pd¯(x) for all x's. The global minimum of the virtual training criterion C(G) is achieved if and only if (1 - )pg(x) + pd(x) = pd¯(x) for all x's. At that point, C(G) achieves the value - log 4.

Proof. See Appendix A.2 in details.

The assumption in Theorem 1 may be impractical in real applications. We discuss that DSGAN still works well even though the assumption does not hold. There are two facts: i) given D, V (G, D) is
a convex function in pg and ii) Due to pg(x)dx = 1, the set collecting all feasible solutions of
x
pg is a convex set. In other words, there always exists a global minimum of V (G, D) given D, but it may not be - log(4). In this following, without the assumption in Theorem 1, we show that the support set of pg is contained within that of pd¯ while achieving the global minimum such that we can generate the desired pg by designing appropriate pd¯. Proposition 2. Suppose pd(x)  pd¯ for x  Supp(pd). If the global minimum of the virtual training criterion C(G) is achieved, then
Supp (pg)  Supp (pd¯) - Supp(pd).
Proof. See Appendix A.3 in details.

In sum, the generator is prone to output samples located in high-density areas of pd¯ and low-density areas of pd.
Another concern is the convergence of Algorithm 1.
Proposition 3. The discriminator reaches its optimal value given G in Algorithm 1, and pg is updated by minimizing
Expd¯(x) [log DG (x)] + Ex(1-)pg(x)+pd(x) [log (1 - DG (x))] .
If G and D have enough capacity, then pg converges to argmin JSD (pd¯ (1 - )pg + pd).
pg

Proof. See Appendix A.4 in details.

3.1 TRICKS FOR STABLE TRAINING
We provide a trick to stabilize the training procedure by reformulating the objective function. Specifically, V (G, D) in (2) is reformulated as:

V (G, D) = pd¯(x) log (D (x)) + ((1 - )pg(x) + pd(x)) log (1 - D (x)) dx
x
= Expd¯(x) [log D(x)] + Ex(1-)pg(x)+pd(x) [log (1 - D (x))] .

(4)

Instead of sampling a mini-batch of m samples from pz and pd in Algorithm 1, (1 - )m and m samples from both distributions are required, respectively. The computation cost in training can be reduced due to fewer samples. Furthermore, although (4) is equivalent to (2) in theory, we find that the training using (4) achieves better performance than using (2) via empirical validation in Table 1. We conjecture that the equivalence between (4) and (2) is based on the linearity of expectation, but mini-batch stochastic gradient descent in practical training may lead to the different outcomes.

5

Under review as a conference paper at ICLR 2019

Table 1: Comparing the semi-supervised learning results on MNIST whether to use the sampling tricks.

Methods

MNIST (# errors)

Our method w/o tricks

91.0(±7.0)

Our method w/ tricks

82.7(±4.6)

4 APPLICATIONS

DSGAN is applied to two problems: semi-supervised learning and adversarial training. Specifically, DSGAN acts as a "bad generator," which creates complement samples versus label data and unlabel data in semi-supervised learning. As for adversarial training, DSGAN generates adversarial examples located in the low-density areas of training data.

4.1 SEMI-SUPERVISED LEARNING

Semi-supervised learning (SSL) is a kind of learning model with the use of a small number of labeled data and a large amount of unlabeled data. The existing SSL works based on generative model (e.g., VAE (Kingma et al. (2014)) and GAN (Salimans et al. (2016))) obtain good empirical results. Dai et al. (2017) theoretically show that good semi-supervised learning requires a bad GAN with the objective function:

max
D

Ex,yL

log

PD

(y

|

x, y



K)

+

Expd (x)

log

PD

(y



K

|

x)

+ Expg(x) log Pg (K + 1 | x) ,

(5)

where (x, y) denotes a pair of data and its corresponding label, {1, 2, . . . , K} denotes the label space for classification, and L = {(x, y)} is the label set. Note that the discriminator D in GAN also plays
the role of classifier. If the generator distribution exactly matches the real data distribution (i.e., pg = pd), then the classifier trained by the objective function (5) with the unlabeled data cannot have better performance than that trained by supervised learning with the objective function:

max Ex,yL log PD (y | x, y  K) .
D

(6)

On the contrary, the generator is preferred to generate complement examples, which lie on lowdensity area of pd. Under some mild assumptions, those complement examples help D to learn correct decision boundaries in low-density area because the probabilities of the true classes are forced to be low on out-of-distribution areas.
The complement examples in Dai et al. (2017) are complicate to produce. We will demonstrate that DSGAN is easy to generate complement examples in Sec. 5.

4.2 ADVERSARIAL TRAINING
Deep neural networks have impacted on our daily life. Neural networks, however, are vulnerable to adversarial examples, as evidenced in recent studies (Papernot et al. (2016))(Carlini & Wagner (2017)). Thus, there has been significant interest in how to enhance the robustness of neural networks. Unfortunately, if the adversary has full access to the network, namely white-box attack, a complete defense strategy has not yet been found.
Athalye et al. (2018) surveyed the state-of-the-art defense strategies and showed that adversarial training (Madry et al. (2018)) is more robust than other strategies. Given a trained classifier C parameterized by  and a loss function (x; y; C), adversarial training solves a min-max game, where the first step is to find adversarial examples within -ball for maximizing the loss, and the second step is to train the model for minimizing the loss, given adversarial examples. Specifically, the objective (Madry et al. (2018)) is

argmin E(x,y)L max (x; y; C) .
 [- , ]N

(7)

6

Under review as a conference paper at ICLR 2019

The authors used projected gradient descent (PGD) to find adversarial examples by maximizing the inner optimization.
Instead of relying on PGD, our DSGAN generates adversarial examples directly, which are combined into real training data to fine-tune C. -ball in terms of 2 or inf can be intuitively incorporated into the generation of adversarial examples.

5 EXPERIMENTS

We first demonstrate the empirical results about semi-supervised learning, where DSGAN generates complement samples, in Sec. 5.1. Then, DSGAN is used to generate adversarial examples for adversarial training in Sec. 5.2.
Note that, the training procedure of DSGAN can be improved by other extensions of GANs such as WGAN (Arjovsky et al. (2017)), WGAN-GP (Gulrajani et al. (2017)), EBGAN (Zhao et al. (2017)), LSGAN (Mao et al. (2017)) and etc. We use the idea of WGAN-GP in our method such that DSGAN is stable in training and suffers less mode collapse.

5.1 DSGAN IN SEMI-SUPERVISED LEARNING

Following the previous works, we apply the proposed DSGAN in semi-supervised learning on three benchmark datasets, including MNIST (LeCun et al. (1998)), SVHN (Netzer et al. (2011)), and CIFAR-10 (Krizhevsky (2009)).

We first introduce how DSGAN generates complement examples in the semi-supervised learning. Specifically, Dai et al. (2017) proved that if complement examples generated by G can satisfy the following two assumptions in (8) and (9):

x



pg(x), 0

>

max
1iK

wiT

f

(x),

and

x



pd(x), 0

<

max
1iK

wiT

f

(x)

(8)

where f is the feature extractor and wi is the linear classifier for the ith class and

x1  L, x2  pd(x), xg  pg(x) s.t. xg = x1 + (1 - )x2 with   [0, 1],

(9)

then all unlabeled data will be classified correctly via the objective function (5).

The assumption in (8) implies Supp (pg) Supp (pd) = {}. Furthermore, in (9), let each sam-

ple from pd¯ in DSGAN be the linear combination of those from L and pd. Since pg(x) 

pd¯(x) - pd 1-

(x)

,

the

term

-pd

ensures

that

samples

from

pg

do

not

belong

to

Supp (pd).

In

sum, pg(x) approximates pd¯(x) for x  Supp (pd). Thus, pg satisfies both assumptions in (8) and

(9).

The details of the experiments, including the network models, can be found in Appendix B.

5.1.1 MNIST, SVHN, AND CIFAR-10
For evaluating the semi-supervised learning task, we used 60000/ 73257/ 50000 samples and 10000/ 26032/ 10000 samples from the MNIST/ SVHN/ CIFAR-10 dataset for training and testing, respectively. Due to the semi-supervised setting, we randomly chose 100/ 1000/ 4000 samples from the training samples as the MNIST/ SVHN/ CIFAR-10 labeled dataset, and the amount of labeled data for all classes are equal.
The hyperparameters were chosen to make our generated samples consistent with the assumptions in (8) and (9). However, in practice, if we make all the samples produced by the generator following the assumption in (9), then the generated distribution is not close to the true distribution, even a large margin between them exists in most of the time, which is not what we desire. So, in our experiments, we make a concession that the percentage of generated samples, which accords with the assumption, is around 90%. To meet this objective, we tune the hyperparameters.
Finally, we perform testing with 10/ 5/ 5 runs on MNIST/ SVHN/ CIFAR-10 based on the selected hyperparameters and randomly selected labeled dataset. Following Dai et al. (2017), the results are recorded as the mean and standard deviation of number of errors from each run.

7

Under review as a conference paper at ICLR 2019

5.1.2 MAIN RESULTS
First, the hyperparameters we chose is depicted in Table 2, where  is defined in (9). Second, the results obtained from our DSGAN and the state-of-the-art methods on three benchmark datasets are depicted in Table 3.

Table 2: Hyperparameters in semi-supervised learning.

Hyperparameters MNIST SVHN CIFAR-10



0.8 0.8

0.5



0.3 0.3

0.1

It can be observed that our results can compete with state-of-the-art methods on the three datasets. Moreover, in comparison with Dai et al. (2017), our methods don't need to rely on an additional density estimation network PixelCNN++ (Salimans et al. (2017)). Although PixelCNN++ is one of the best density estimation network, it cannot estimate the density in the feature space, which is dynamic during training. This drawback make the models in Dai et al. (2017) cannot completely fulfill the assumptions in their paper.

Table 3: Comparison of semi-supervised learning between our DSGAN and other state-of-the-art results. For fair comparison, we only consider the GAN-based methods.  indicates the use of the same architecture of classifier.  indicates a larger architecture of classifier.  indicates the use of
data augmentation.

Methods
CatGAN (Springenberg (2016)) TripleGAN (Li et al. (2017)) FM (Salimans et al. (2016)) badGAN (Dai et al. (2017)) CT-GAN (Wei et al. (2018))
Our method

MNIST (# errors) 191(±10) 91(±58) 93(±6.5) 79.5(±9.8)
82.7(±4.6)

SVHN (% of errors) -
5.77(±0.17) 8.11(±1.3) 4.25(±0.03)
5.01(±0.14)

CIFAR-10 (% of errors) 19.58(±0.46)
16.99(±0.36) 18.63(±1.32) 14.41(±0.30)
9.98(±0.21) 15.08(±0.24)

5.1.3 REMARKS
Our result on MNIST in Table 3 is slightly inferior to the best record of badGAN (Dai et al. (2017)) but outperforms other approaches. This indicates that the samples generated from DSGAN can satisfy the assumptions in (8) and (9) and really help the semi-supervised learning through the proof of badGAN. In SVHN and CIFAR-10, we still attain the same conclusions as in MNIST. In comparison with badGAN, there is a probable reason to explain the slightly inferior performance in the following. Since the patterns of images are complicated, the generator without enough capacity is not able to learn our desired distribution. However, this problem will be attenuated with the improvements of GAN, and our models benefit from them.
5.2 DSGAN IN ADVERSARIAL TRAINING
Our proposed DSGAN is capable to be used to improve the robustness of the classifier against adversarial examples. In the experiments, we mainly validate DSGAN on CIFAR-10, which is widely used in adversarial training.
Recall that the objective function (7) requires finding adversarial examples to maximize the classification error (·). Adversarial examples usually locate on the low-density area of pd and are generated from labeled data via gradient descent. Instead of using gradient descent, we aim to generate adversarial examples via GAN. By assigning pd¯ as the convolution of pd and uniform distribution, samples from pg will locate on the low-density area of pd. Furthermore, the distortion is directly related to the range of uniform distribution. It, however, may be impractical for training the
8

Under review as a conference paper at ICLR 2019

generator for each class. Thus, we propose a novel semi-supervised adversarial learning approach here.
Three stages are required to train our model: First, we train a baseline classifier on all the training data. All the training data are labeled and represent samples from L in (10). Second, we train a generator to generate adversarial samples and treat these adversarial samples as additional unlabeled training data (x  pg in (10)). Third, we fine-tune the classifier C with all training data and the data produced by the generator via minimizing the following objective:

argmin E(x,y)L [ (x; y; C)] + Exgpg(x) [H (C(xg))] ,


(10)

where the first term is a typical supervised loss such as cross-entropy loss and the second term is the entropy loss H of generated unlabeled samples corresponding to the classifier, meaning that we would like the classifier to confidently classify the generated samples. In other words, if an adversarial example xg is the closest to one of labeled data x, it should be classified into the class of x. Thus, the additional entropy loss will prevent our model from the attack by adversarial examples.
In order to determine how many iterations are required to train the classifier in the first and third stages, we use the classification accuracy on the validation set as the criterion to choose the best number of training iterations. The size of validation data is 5000. After determining the best number of iterations, we train the model with the whole training set. As for the second stage, we train DSGAN for 50 iterations in Algorithm 1.

5.2.1 MAIN RESULTS
The size of labeled data for CIFAR-10 is 4000 and we balance the number of data for each class. The experimental detail can be found in Appendix C. We evaluate the trained models against a range of adversaries, where the distortion is evaluated by 2-norm or inf -norm. The adversaries include:
· White-box attacks with Fast Gradient Sign Method (FGSM) (Goodfellow et al. (2015)) using inf -norm.
· White-box attacks with PGD (Kurakin et al. (2017)) using inf -norm.
· White-box attacks with Deepfool (Moosavi-Dezfooli et al. (2016)) using 2-norm and inf norm.
According to different adversaries, we first generate 3000 adversarial examples with minimal distortions and then calculate the average of 10000 distortions. We also train our models with different variances of uniform distribution. Table 4 shows the results about the adversarial training. The accuracy, which is the probability that adversarial examples fail to attack, is illustrated in Table 5. was set to be 0.01 and 0.2 for 2-norm and inf -norm, respectively. Both experiments demonstrate that our models exhibit stronger robustness in that adversarial examples require larger distortions to defeat our models.

Table 4: Comparison of adversarial training between the baseline model and our model. Our model is trained with different variances (0.1, 0.3, and 0.5) of uniform distribution. We evaluate the average distortion based on 2-norm or inf -norm.

Methods Baseline Our method-0.1 Our method-0.3 Our method-0.5

FGSM- inf 0.0113 0.0137 0.0151 0.0143

Deepfool- inf 0.0062 0.0069 0.0078 0.0075

PGD- inf 0.0057 0.0063 0.0071 0.0066

Deepfool- 2 0.248 0.259 0.273 0.263

9

Under review as a conference paper at ICLR 2019

Table 5: Comparison of accuracy between the baseline model and our model. was set to be 0.008 and 0.2 for 2-norm and inf -norm, respectively.

Methods Baseline Our method-0.1 Our method-0.3 Our method-0.5

FGSM- inf 56% 63% 78% 69%

Deepfool- inf 48% 54% 69% 61%

PGD- inf 41% 45% 63% 56%

Deepfool- 2 47% 51% 66% 57%

6 CONCLUSIONS
In this paper, we propose DSGAN that can produce samples from the target distribution based on the assumption that the density of target distribution can be the difference between the densities of any two distributions. DSGAN is useful in the environment when the samples from the target distribution are more difficult to collect than those from the two known distributions. We demonstrate that DSGAN is really applicable to, for example, semi-supervised learning and adversarial training. Empirical and theoretical results are provided to validate the effectiveness of DSGAN. Finally, because DSGAN is developed based on traditional GAN, it is easy to extend any improvements of traditional GAN to DSGAN.
REFERENCES
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. In ICLR. 2017.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML, volume 70, pp. 214­223, 2017.
A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In ICML. 2018.
N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (SP), pp. 39­57, 2017.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semisupervised learning that requires a bad gan. In NIPS, pp. 6510­6520. 2017.
I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672­2680. 2014.
Ishaan Gulrajani, Faruk Ahmed, Mart´in Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. In NIPS, 2017.
D. P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR. 2014.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In NIPS, pp. 3581­3589. 2014.
A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial machine learning at scale. In ICLR, 2017.
A. B. L. Larsen, S. K. Snderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels using a learned similarity metric. In ICML, pp. 1558­1566. 2016.
Y. LeCun, C. Cortes, and C. J. C. Burges. The mnist database of handwritten digits. 1998.
10

Under review as a conference paper at ICLR 2019
Chongxuan Li, Kun Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In NIPS, 2017.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR. 2018.
X. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang, and S. P. Smolley. Least squares generative adversarial networks. In IEEE ICCV, pp. 2813­2821, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR. 2018.
S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In IEEE CVPR, 2016.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop, 2011.
A.V. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. In ICML, pp. 1747­1756. 2016.
N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In IEEE European Symposium on Security and Privacy (EuroS P), pp. 372­387, 2016.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semisupervised learning with ladder networks. In NIPS, 2015.
Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In ICML, 2016.
Y. Saito, S. Takamichi, and H. Saruwatari. Statistical parametric speech synthesis incorporating generative adversarial networks. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(1):84­96, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In NIPS, pp. 2234­2242. 2016.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. In ICLR, 2017.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. In ICLR, 2016.
I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. In ICLR. 2018.
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training of wasserstein gans: A consistency term and its dual effect. In ICLR, 2018.
J. J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. In ICLR, 2017.
11

Under review as a conference paper at ICLR 2019

APPENDIX

A PROOFS

A.1 PROOF OF THE PROPSITION 1
Proof. Given any generator G, the training criterion for the discriminator D is to maximize the quantity V (G, D):

V (G, D) = pd¯(x) log (D (x)) dx + (1 - ) pz(z) log (1 - D (G (z))) dz
xz

+  pd(x) log (1 - D (x)) dx
x

= pd¯(x) log (D (x)) dx + (1 - ) pg(x) log (1 - D (x)) dz
xx

+  pd(x) log (1 - D (x)) dx
x

= pd¯(x) log (D (x)) + ((1 - )pg(x) + pd(x)) log (1 - D (x)) dx.
x

For any (a, b)  R2\{0, 0}, the function a log (y) + b log (1 - y) achieves its maximum in [0, 1]

at

y

=

a a+b

.

The

discriminator

only

needs

to

be

defined

within

Supp(pd¯)

Supp(pd)

Supp(pg ).

We complete this proof.

A.2 PROOF OF THEOREM 1

Proof. We start from

(3) = - log(4) + Expd¯(x)

log

pd¯(x)

+

(1

2pd¯(x) - )pg(x)

+

pd(x)

+ Ex(1-)pg(x)+pd(x)

log 2 ((1 - )pg(x) + pd(x)) pd¯(x) + (1 - )pg(x) + pd(x)

= - log(4) + KL

pd¯

pd¯ + (1 - )pg + pd 2

+ KL (1 - )pg(x) + pd

pd¯ + (1 - )pg + pd 2

= - log(4) + 2 JSD (pd¯ (1 - )pg + pd) ,

where KL is the Kullback-Leibler divergence and JSD is the Jensen-Shannon divergence. The JSD
returns the minimal value, which is 0, iff both distributions are the same, namely pd¯ = (1 - )pg + pd. Because pg(x)'s are always non-negative, it should be noted both distributions are the same only if pd(x)  pd¯(x) for all x's. We complete this proof.

A.3 PROOF OF PROPOSITION 2

Proof. Recall

C(G) = pd¯(x) log
x

pd¯(x) pd¯(x) + (1 - )pg(x) + pd(x)

+ ((1 - )pg(x) + pd(x)) log

(1 - )pg(x) + pd(x) pd¯(x) + (1 - )pg(x) + pd(x)

dx

= S(pg; x)dx
x

=

S(pg; x)dx +

S(pg; x)dx

xSupp(pd¯)-Supp(pd )

xSupp(pd )

12

Under review as a conference paper at ICLR 2019

S(pg; x) is to simplify the notations inside the integral. For any x, S(pg; x) in pg(x) is non-
increasing. Specifically, S(pg; x) is decreasing when pg(x)  0 and pd¯(x) > 0; S(pg; x) is a constant for any pg(x) if pd¯(x) = 0. Since DSGAN aims to minimize C(G) with the

constraint pg(d)dx = 1, the solution attaining the global minima satisfies pg(x) = 0 if

x

pd¯(x) = 0; otherwise, there exists another solution with smaller value of C(G). By the contra-

positive of the previous statement, it implies Supp (pg)  Supp (pd¯). Furthermore, T (pg; x) =

S(pg; x) = log  pg (x)

(1 - )pg(x) + pd(x) pd¯(x) + (1 - )pg(x) + pd(x)

is increasing on pg(x) and converges to 0.

When

x



Supp(pd),

T (pg; x)



log

1 2

always

holds

due

to

the

assumption

pd(x)



pd¯(x).

In

other pg (x)

words, given pd¯(x) being fixed, being larger than zero with T (pg

the ; x)

strategy

<

log

1 2

to minimize C(G) for x  Supp(pd)

is -

to assign the density of Supp(pd). We complete

this proof.

A.4 PROOF OF PROPOSITION 3
Proof. Consider V (G, D) = U (pg, D) as a function of pg. By the proof idea of Proposition 2 in Goodfellow et al. (2014), if f (x) = supA f(x) and f(x) is convex in x for every , then f(x)  f if  = argsupA f(x). In other words, if supD V (G, D) is convex in pg, the subderivatives of supD V (G, D) includes the derivative of the function at the point, where the maximum is attained, implying the convergence with sufficiently small updates of pg. We complete this proof.

B EXPERIMENTAL DETAILS FOR SEMI-SUPERVISED LEARNING
In order to fairly compare with other methods, our generators and classifiers for MNIST, SVHN, and CIFAR-10 are same as in Salimans et al. (2016) and Dai et al. (2017). However, different from previous works that have only a generator and a discriminator, we design an additional discriminator in the feature space, and it's architecture is similar across all datasets with only the difference in the input dimensions. Following Dai et al. (2017), we also define the feature space as the input space of the output layer of discriminators.
Compared to SVHN and CIFAR-10, MNIST is a simple dataset as it is only composed of fully connected layers. Batch normalization (BN) or weight normalization (WN) is used to every layer to stable training. Moreover, Gaussian noise is added before each layer in the classifier, as proposed in Rasmus et al. (2015). We find that the added Gaussian noise exhibits positive effect for semisupervised learning and keep to use it. The architecture is shown in Table 6.
Table 7 and Table 8 are models for SVHN and CIFAR-10, respectively, and these models are almost the same except for some implicit differences, e.g., the number of convolutional filters and types of dropout. In these tables, given a dropping rate, "Dropout" is a normal dropout in that the elements of input tensor are randomly set to zero while Dropout2d is a dropout only applied on the channels to randomly zero all the elements.
13

Under review as a conference paper at ICLR 2019

Table 6: Network architectures for semi-supervised learning on MNIST.

Generator G Input z  R100 from unif(0, 1)
100 × 500 FC layer with BN Softplus 500 × 500 FC layer with BN Softplus 500 × 784 FC layer with WN Sigmoid

Discriminator D Input 28 × 28 gray image
250 × 400 FC layer ReLU 400 × 200 FC layer ReLU 200 × 100 FC layer ReLU 100 × 1 FC layer

Classifier C Input 28 × 28 gray image
Gaussian noise, std = 0.3 784 × 1000 FC layer with WN ReLU Gaussian noise, std = 0.5 1000 × 500 FC layer with WN ReLU Gaussian noise, std = 0.5 500 × 250 FC layer with WN ReLU Gaussian noise, std = 0.5 250 × 250 FC layer with WN ReLU Gaussian noise, std = 0.5 250 × 250 FC layer with WN ReLU

250 × 10 FC layer with WN

Table 7: Architectures of generator and discriminator for semi-supervised learning on SVHN and CIFAR-10. N was set to 128 and 192 for SVHN and CIFAR-10, respectively.

Generator G Input z  R100 from unif(0, 1)
100 × 8192 FC layer with BN ReLU Reshape to 4 × 4 × 512 5 × 5 conv. transpose 256 stride = 2 with BN ReLU 5 × 5 conv. transpose 128 stride = 2 with BN ReLU 5 × 5 conv. transpose 3 stride = 2 with WN Tanh

Discriminator D Input 32 × 32 RGB image
N × 400 FC layer ReLU 400 × 200 FC layer ReLU 200 × 100 FC layer ReLU 100 × 1 FC layer

14

Under review as a conference paper at ICLR 2019

Table 8: The architecture of classifiers for semi-supervised learning on SVHN and CIFAR-10.

Classifier C for SVHN Input 32 × 32 RGB image

Classifier C for CIFAR-10 Input 32 × 32 RGB image

Gaussian noise, std = 0.05 Dropout2d, dropping rate = 0.15 3 × 3 conv. 64 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 64 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 64 stride = 2 with WN LeakyReLU, leak rate = 0.2 Dropout2d, dropping rate = 0.5 3 × 3 conv. 128 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 128 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 128 stride = 2 with WN LeakyReLU, leak rate = 0.2 Dropout2d, dropping rate = 0.5 3 × 3 conv. 128 stride = 1 with WN LeakyReLU, leak rate = 0.2 1 × 1 conv. 128 stride = 1 with WN LeakyReLU, leak rate = 0.2 1 × 1 conv. 128 stride = 1 with WN LeakyReLU, leak rate = 0.2
Global average Pooling

Gaussian noise, std = 0.05 Dropout2d, dropping rate = 0.2 3 × 3 conv. 96 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 96 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 96 stride = 2 with WN LeakyReLU, leak rate = 0.2 Dropout, dropping rate = 0.5 3 × 3 conv. 192 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 192 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 192 stride = 2 with WN LeakyReLU, leak rate = 0.2 Dropout, dropping rate = 0.5 3 × 3 conv. 192 stride = 1 with WN LeakyReLU, leak rate = 0.2 1 × 1 conv. 192 stride = 1 with WN LeakyReLU, leak rate = 0.2 1 × 1 conv. 192 stride = 1 with WN LeakyReLU, leak rate = 0.2
Global average Pooling

128 × 10 FC layer with WN

192 × 10 FC layer with WN

Furthermore, the training procedure alternates between k steps of optimizing D and one step of optimizing G. We find that k in Algorithm 1 is a key role to the problem of mode collapse for different applications. For semi-supervised learning, we set k = 1 for all datasets.
C EXPERIMENTAL DETAILS FOR ADVERSARIAL TRAINING
In the experiments for adversarial training on CIFAR-10, the generator and discriminator are the same as those in semi-supervised learning. The architecture is described in Table 7 and the classifier is modified from the one shown in Table 8. First, we get rid of all the dropouts and Gaussian noise so that we can compare among different models with less randomness. Moreover, we decrease the number of layers in the original model, simply intending to accelerate training. The number of layers following the feature space is increased to 3. Because we apply our method in the feature space, the sub-network after feature space should be non-linear so that it can correctly classify generated data. The architecture is described in Table 9. Furthermore, k is assigned to 5 in all experiments.
15

Under review as a conference paper at ICLR 2019
Table 9: The architecture of classifier for adversarial training on CIFAR-10. Classifier C for CIFAR-10 Input 32 × 32 RGB image 3 × 3 conv. 96 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 96 stride = 2 with WN LeakyReLU, leak rate = 0.2 Dropout, dropping rate = 0.5 3 × 3 conv. 192 stride = 1 with WN LeakyReLU, leak rate = 0.2 3 × 3 conv. 192 stride = 2 with WN LeakyReLU, leak rate = 0.2 Dropout, dropping rate = 0.5 3 × 3 conv. 192 stride = 1 with WN LeakyReLU, leak rate = 0.2 1 × 1 conv. 192 stride = 1 with WN LeakyReLU, leak rate = 0.2 Global average Pooling 192 × 192 FC layer with WN LeakyReLU, leak rate = 0.2 192 × 192 FC layer with WN LeakyReLU, leak rate = 0.2 192 × 192 FC layer with WN LeakyReLU, leak rate = 0.2 192 × 10 FC layer with WN
16

