Under review as a conference paper at ICLR 2019
HEATED-UP SOFTMAX EMBEDDING
Anonymous authors Paper under double-blind review
ABSTRACT
Metric learning aims at learning a distance which is consistent with the semantic meaning of the samples. The problem is generally solved by learning an embedding, such that the samples of the same category are close (compact) while samples from different categories are far away (spread-out) in the embedding space. One popular way of generating such embeddings is to use the second-to-last layer of a deep neural network trained as a classifier with the softmax cross-entropy loss. In this paper, we show that training classifiers with different temperatures of the softmax function lead to different distributions of the embedding space. And finding a balance between the compactness, "spread-out" and the generalization ability of the feature is critical in metric learning. Leveraging these insights, we propose a "heating-up" strategy to train a classifier with increasing temperatures. Extensive experiments show that the proposed method achieves state-of-the-art performances on a variety of metric learning benchmarks.
1 INTRODUCTION
Metric learning is a fundamental research topic in machine learning and has been widely explored in a variety of computer vision applications such as clustering (Xing et al., 2003; Ye et al., 2007), image retrieval (Lee et al., 2008; Zhang et al., 2017), face recognition (Guillaumin et al., 2009; Schroff et al., 2015), and person re-identification (Koestinger et al., 2012; Lisanti et al., 2017).
The objective of metric learning is usually formulated (Chopra et al., 2005; Hoffer & Ailon, 2015; Schroff et al., 2015; Zhang et al., 2017) as learning a metric space which is "compact", where samples from the same classes are close to each other, and "spread-out", where two randomly sampled non-matching features are far away to each other. The learned embedding should also be able to generalize well to unseen classes and samples of the same domain.
One solution for this problem is to define a loss function that tries to explicitly enforce the properties of compactness and spread in the metric space. Two of the most popular loss functions are the contrastive loss (Chopra et al., 2005) and the triplet loss (Hoffer & Ailon, 2015). However, such losses face challenges in sampling, as usually there are a very large number of constraints (possible pairs or triplets) in one dataset and most of them become quickly non-informative in the training process.
Using features from the second-to-last (a.k.a. bottleneck) layer of a deep network trained as a classifier with the softmax function and the cross-entropy loss works well for many metric learning based applications (Razavian et al., 2014) such as image retrieval (Babenko & Lempitsky, 2015) and face verification (Liu et al., 2017). However, the goals of classifier training and metric learning are different, i.e. finding the best decision function vs. learning a "compact" and "spread-out" embedding. This motivates us to investigate the relation between metric learning and classifier training.
In this paper, we study the gradient of the embeddings and show how the temperature parameter in the softmax function (defined by Hinton et al. (2015) for knowledge transfer) plays a crucial role in determining the distribution of the embeddings. This motivates a "heating-up" method that learns a classifier with normalized features and weights using first an intermediate temperature and increasing it during training. The proposed "heating-up" strategy produces compact and spread-out embeddings achieving state-of-the-art performance in deep metric learning.
1

Under review as a conference paper at ICLR 2019

2 RELATED WORKS

Siamese networks with contrastive loss (Chopra et al., 2005) was one of the earliest attempts to solve the metric learning problem. By sampling either two data samples from the same category (positive pair) or two different categories (negative pair), contrastive loss tries to pull two points from positive pair together and push away the points from negative pair. Triplet loss (Hoffer & Ailon, 2015) further requires a margin between the positive and negative pairs distances. One of the main issues of these losses is that the number of possible pairs or triplets is extremely large for a large dataset.
A reasonable solution to address the sampling issue is mining samples that are the most informative for training, also known as "hard mining". There is a large body of works addressing this problem (Schroff et al., 2015; Mishchuk et al., 2017; Harwood et al., 2017; Yuan et al., 2017; Wu et al., 2017). Semihard mining (Schroff et al., 2015) tries to find triplets in a training batch, for which the distance of the positive pair and the distance of the negative pair are within a certain margin. HardNet (Mishchuk et al., 2017) is designed to mine some of the hardest triplets within one training batch.
Designing structured losses to consider all the possible training pairs or triplets within one training batch and perform "soft" hard mining can be an alternative solution for hard mining (Song et al., 2017; 2016; Ustinova & Lempitsky, 2016). Lifted structured loss (Song et al., 2016) exploits all triplets in a training batch and provides a smooth loss function for hard mining. A few deep clustering based losses (Law et al., 2017; Song et al., 2017) have also been proposed to solve the problem. Proxy NCA (Movshovitz-Attias et al., 2017) proposes to learn semantic proxies for training data and use a NCA loss for training. Applying hard mining with proxies is more efficient than with samples.
In face verification, training a classifier and using the output of the bottleneck layer as embedding performs reasonably well (Wang et al., 2017b). For this task, the normalization of features (Ranjan et al., 2017), weights (Liu et al., 2017) or both (Wang et al., 2017a) have been explored. In order to achieve promising results, a learnable or fixed scalar is required to be multiplied to the final logits in the softmax function. There are preliminary discussions (Wang et al., 2017a; Ranjan et al., 2017) about the influence of this scalar on the embedding.
This paper shows that the scalar can be seen as the temperature parameter of the softmax function in Hinton et al. (2015), where its influence on the logits for the task of knowldege transfer was studied. The temperature was also explored as a post-processing step to calibrate a pre-trained model (Guo et al., 2017) and produce probability estimates aligned with the expected accuracy of the model. Differently from the above works, which focus on the output probability, the goal of this paper is to study the features from the bottleneck layer.
We analyze how the temperature parameter controls the distribution of the embedding by assigning different gradients to different samples and weights. Inspired by these findings, we propose a "heating-up" strategy for training the embedding, which uses increasing temperature while training the classifier. The proposed method makes the embedding trained simply with the softmax cross-entropy loss achieve comparable or better performance than state-of-the-art deep metric learning methods.

3 THE HEATED-UP EMBEDDING

3.1 REVISITING SOFTMAX EMBEDDING WITH TEMPERATURE

Given a set of n labelled training samples {(x1, y1), . . . , (xn, yn)}, where xi  Rd is the feature of the i-th sample, d is the number of dimension for the training data, yi  {1, . . . , M } is the category
label of sample xi, and M is the number of categories for training samples, we try to learn an embedding function f (·) : Rd  Rk, which maps a data sample to a vector in Rk, such that for all i, j, p with yi = yj = yp, l(f (xi), f (xj)) < l(f (xi), f (xp)), where l(·, ·) : Rk × Rk  R is a
distance function.

We call f (x)  Rk the embedding of the data sample x and use f as f (x) to simplify the notation. Considering training a linear classifier W = [w1, . . . , wM ]  Rk×M and b = [b1, . . . , bM ]T  RM , with wm and bm being respectively the weight vector and bias of the mth category classifier, z = [z1, . . . , zM ]T = WT f + b  RM is called the logits. The probability that sample x belongs to category m  {1, . . . , M } can be predicted by the softmax function as:

p(m|x) =

exp(zm/T )

M j=1

exp(zj

/T

)

=

exp(zm)

M j=1

exp(zj

)

.

(1)

2

Under review as a conference paper at ICLR 2019

T , which is normally set to 1, is the temperature as mentioned in Hinton et al. (2015). We set  = 1/T as the reciprocal of the temperature to simplify the notations in the paper.

Assuming the ground-truth distribution of the training sample is q(m|x), generally q(m|x) is a Dirac delta function, which equals 1 if m = y and 0 otherwise, where y is the ground-truth label of x, the cross entropy loss with respect to x, and its gradient with respect to zm are defined as:

M
(x, ) = - log(p(m|x, ))q(m|x) and

 = (p(m|x, ) - q(m|x)).

m=1

zm

(2)

Considering zm = wmT f + bm, the gradient with respect to the feature f is:

 f

M
=
m=1

 zm zm f

M
=  (p(m|x, ) - q(m|x))wm.
m=1

(3)

And the gradient with respect to the weight wm is:



 =

zm = (p(m|x, ) - q(m|x))f .

wm zm wm

(4)

The following part of this section (Sec. 3.2) will show how  changes the magnitude of the gradient

assignment to different features and weights. To show this, we 2-normalize the classifier weights

wm and the feature f (as in Wang et al. (2017a)). It helps 1) separate the impact of  from the norm

of the embedding and the norm of the classifier weights to the softmax function; 2) achieve better

performance than the unnormalized model (Wang et al., 2017a) (a detailed comparison between the

normalized model and unnormalized model will be given in Sec. 4). Sec. 3.3 will study the effect

of the normalization . Exploiting these findings to derive an effective strategy to learn compact and

spread-out embeddings, we finally detail our proposed "heating-up" idea in Sec. 3.4.

3.2 GRADIENT ASSIGNMENT BY 

In this subsection, we will show how training a deep classification network with different  values affects the gradients of different training samples and class weights. From Eq. (1), p(m|x, ) satisfies:

lim p(m|x, ) = 1/K zm = max(z1, ..., zM ) , and lim p(m|x, ) = 1/M

+

0 otherwise

0

(5)

where K is the number of logits whose value equals the maximum logits value. In other words, as 

increases, the predicted probability will become more "spiky" at the logits that have the largest value.

On the other hand, if  approaches 0, the predicted probability will approach the uniform distribution.

The training samples can be divided into: (i) "incorrect" samples: samples that are not classified as the correct category ({x : m = y, zm  zy}); (ii) "correct" samples: samples being correctly classified by the classifier ({x : m = y, zy > zm}). We further define two subtypes of samples in the "correct" category: "boundary" samples are samples close to the decision boundary; "centroid"
samples are samples laying close to the center of the region that belongs to the category.

We denote the normalized weights and features as w^ m and ^f respectively. The gradient of the loss w.r.t. the normalized embedding and the normalized weight (i.e. Eq. (3)-(4) with ^f instead of f and w^m instead of wm) exhibits three important properties for gradient assignment for different  values.
Property 1. The gradient of the cross entropy loss with respect to the feature and the weights satisfy:

lim
0

||

 ^f

||2

=

0,

and

lim
0

||

 w^ m

||2

=

0.

(6)

It's because -1  p(m|x, ) - q(m|x)  1. Therefore, with very small  (e.g. to the left of the

green dashed line in Fig. 1, close to 0), all the samples will get small gradients. Similarly, all the weights will get small gradients.

Property 2. If ^f is the normalized feature of a "correct" sample, the gradient of the cross entropy

loss with respect to the feature satisfies:

lim
+

||

 ^f

||2

=

0.

(7)

3

Under review as a conference paper at ICLR 2019

||l/f^||2 ||l/f^||2 ||l/f^||2

6
4
2
0 
(a) Correct centroid sample

6
4
2
0 
(b) Correct boundary sample

60
40
20
0 
(c) Incorrect sample

(d) Illustration of gradient assignment by the proposed "heating-up" strategy for different samples.

Figure 1: (a-c) Relation between  and the magnitude of the gradient with respect to the embeddings of "correct" (centroid and boundary) and "incorrect" samples. The gradients are sampled from models trained on the Car196 dataset (Krause et al., 2013). (d) Illustration of our `heating-up" strategy.

If ^f is the normalized feature of an "incorrect" sample, the gradient, in most cases1, satisfies:



lim
+

||

^f

||2

=

+

(8)

We give the detailed proof of property 2 in the Appendix A.1. Overall, with large  (e.g. the blue dashed line in Fig. 1) the magnitudes of the gradients for "incorrect" samples will become very large, while the magnitudes of gradients for "correct" samples will become very small.

Property 3. If ^f is the normalized feature of a "correct" sample, the gradient of the cross entropy loss with respect to the weight satisfies:

lim
+

||

 w^ m

||2

=

0.

(9)

If ^f is the normalized feature of an "incorrect" sample, the gradient satisfies:

lim
+

||  w^ m

||2

=

+ 0

zm = max(z1, ..., zM ) or m = y; otherwise.

(10)

A detailed proof will be given in Appendix. A.2. Overall, with large , the network will
only consider few weights whose predicted probabilities violate the ground-truth distribution (lim+ p(m|x, ) = q(m|x)). We named those weights as "hard" weights.

3.3 INFLUENCE OF THE NORMALIZATION

We here discuss the influence of the normalization on the gradient of the loss w.r.t. the embedding. The Jacobian matrix of the 2-normalized embedding ^f with respect to the original embedding f is:

J^f (f )

=

1 (I
||f ||2

- ^f^f T ),

(11)

where I is the identity matrix. Considering Eq. (3) and the chain rule, we have:

 f

=

(

 ^f

)T

J^f (f

)

(12)

1The equation may not be satisfied in some special cases. For example, if one positive weight and one negative weight have exactly the same magnitude and direction, then the gradient would be 0.

4

Under review as a conference paper at ICLR 2019

Considering the norm ||f ||2 in the denominator, the magnitude of the gradient is inversely proportional to the norm of the embedding. Therefore, even if the normalized embeddings are the same, the gradients w.r.t. the embeddings would still different for embeddings with different norms. Specifically, the embedding with larger norm will have smaller gradient. This may seem as a problem, and one possible solution is to remove the norm term ||f ||2 in the denominator. We tried this idea for the experiments in Sec. 5, but it did not give significant improvement. The reason for that is likely that since  /f and f are always orthogonal (Wang et al., 2017a), updating the feature along the direction of gradient cannot change much the norm of the feature. We did observe that in practice. When applying 2-normalization to the feature during training, the norms of features before normalization are very similar. On the contrary, when training without normalization, the norm of the features may have large variations as it is the case for off-the-shelf classifier as detailed in Sec. 4. For numerical stability and ease of implementation we use Eq. (12) to calculate the gradient, but the gradient analysis in Sec. 3.2 still holds for unnormalized features before normalization.

We found out that batch normalization (Ioffe & Szegedy, 2015) without the learned scale2, B^N (·),

can work slightly better than 2-normalization. We define the batch normalized embedding as:



^fBN = B^N (f )/ k

(13)

where k is the number of dimensions of f . Batch normalization tries to make each dimension of

the embedding have of the embedding is

zero mean and unit variance. Therefore, roughly k, and the normalized feature

after ^fBN

batch normalization, has norm close to 1,

the norm which is

similar to 2 normalization. Batch normalization may work better than 2-normalization because in

fine-grained recognition problem, many embeddings from different categories can be very similar.

Batch normalization removes the mean and scales the embedding thus creating more variance. For

classifier weights, 2-normalization always gives us promising result.

3.4 THE "HEATING-UP" STRATEGY
In order to get an intra-category compact, inter-category "spread-out" and well generalized feature, we propose a two-step "heating-up" strategy, which 1) uses an intermediate  (temperature) to start training, and then 2) increases the "temperature" (decreasing  value) and decreases the learning rate.
At the beginning of the training, the network should focus more on the "incorrect" samples and "boundary" samples and quickly moves them to the class center (producing compactness, Fig. 1(d) left). It should also focus more on the "hard" weights, and push them away (producing spread-out). Using an intermediate  for training (e.g. the red dashed line in Fig. 1) satisfies these conditions.
If the training started with a small  (i.e. high temperature), the network would assign similar (and potentially very small, if  is very small) gradients to all samples (left of the green dashed line in Fig. 1(a)-1(c)) and all weights. There are two main issues with this, 1) assigning gradient to the centroid samples at the beginning of the training will move them to a very small region close to the class center. However, in metric learning problem, as the training categories are different from the test categories, this can make the embedding overfit the training categories and not generalize well to test categories; 2) "incorrect" samples and "hard" weights affect the accuracy the most. Failing in giving larger gradients (i.e. higher priority) to those will make the training process inefficient.
On the contrary, choosing a large  (i.e. very low temperature) for the initial training would assign large gradients to "incorrect" samples and very small gradients to all "correct" ("centroid" and "boundary") samples (right of the red dashed line in Fig. 1(a)-1(c)). Since the "boundary" samples will not get enough update, they will stay near the decision boundary. Therefore, the embedding of the samples of the same category will not be compact (as shown in Appendix C, features of the same category are more compact for the model trained with smaller  values). Also, the network will only focus on the "hard" weights and not pushing away other weights, which makes the weights vector not spread-out. The non spread-out weight vectors results in the features not being spread-out, since the features are moved towards corresponding weights.
After the first step of training with an intermediate , most of the "incorrect" samples become "boundary" samples and most of the "boundary" samples become "centroid" samples (Fig. 1(d), right), in order to make the feature more compact, we increase the temperature (decrease  value). It helps
2in this paper, batch normalization always refers to batch normalization without learned scale

5

Under review as a conference paper at ICLR 2019
(a) (b) (c)
Figure 2: 2(a)-2(b): Embedding and normalized embedding of a classifier trained without 2 normalization. 2(c): Embedding obtained with a classifier trained with 2 normalization and  = 4.
assign sufficient gradient to all "correct" samples and push them towards the class center. In practice, we also reduce the learning rate to avoid overfitting. Multiple strategies could be defined to increase the temperature during training: (i) gently increasing the temperature; (ii) training with the starting temperature until convergence and using a higher temperature to fine-tune the trained network. Both methods lead to similar performance. However, since the former method would introduce an additional parameter to control the speed of increase of the temperature, we used the latter in our experiments in Sec. 5. The visualization of the feature learned with different models on Car196 dataset (Krause et al., 2013) is provided in Appendix C.
4 COMPARING WITH OFF-THE-SHELF CLASSIFIER
It is interesting to compare the embedding from an off-the-shelf classifier, i.e. trained with unnormalized features and weights and  = 1, with the embeddings obtained by a model trained with normalized features and weight and some fixed  value. We train a LeNet (LeCun et al., 2015) model on the MNIST dataset, and set the number of nodes in the bottleneck layer to 2 for visualization. 50,000 samples are used for training, and 10,000 different test samples are used to draw the figures. In Fig. 2, different colors represent different digits, and each diamond corresponds to the weight of the classifier of the corresponding digit (For better visualization, the weight are slightly moved towards origin). We can see in Fig.2(a), as observed in other works (Wang et al., 2017a; Ranjan et al., 2017), that for an off-the-shelf classifier (i) the magnitude of the embedding can be extremely large, since "correct" samples with larger norm will produce smaller loss (Wang et al., 2017a); (ii) the embedding is not "compact". Even if the feature is 2-normalized (Fig. 2(b)), the embedding is still not as "compact" as the feature trained with normalization and proper  values (Fig. 2(c), see detailed analysis in Appendix B.1). Training with 2-normalization for the feature is different from simply applying normalization to the final feature in the test phase. As discussed and shown in Sec. 3.3 in Eq.(11) and (12), 2normalization will change the gradient of each sample during training.
5 EXPERIMENTS ON METRIC LEARNING
We conduct experiment on the following fine-grained datasets, using the training/test splits of Movshovitz-Attias et al. (2017). In all these datasets, the categories in the training and test splits do not overlap. For the sake of completeness, we also report fine-grained classification results on Car196 and CUB200 in Appendix D.
· Cars (Car196) (Krause et al., 2013) is a fine-grained car category dataset, which contains 16,185 images of 196 car models. 8,054 images of the first 98 categories are used for training, while the 8,131 images of the other 98 categories are used for test.
· Caltech-UCSD Birds-200-2011 (CUB200) (Welinder et al., 2010) is a fine-grained dataset which contains 11,788 images of 200 bird species. 5,864 images of the first 100 species are used for training, while the 5,924 images of the other 100 species are used for test.
· Stanford Online Product (Product) dataset (Song et al., 2016) contains 120,053 images of 22,634 products categories. 59,551 images of 11,318 categories are used for training, while the other 60,502 images from 11,316 categories are kept for test.
6

Under review as a conference paper at ICLR 2019
· In-shop Clothes Retrieval (Fashion) dataset (Liu et al., 2016) contains 54,642 images of 11,735 categories of fine-grained clothes. The dataset is split into 3 subsets. 52,712 images of 7,982 categories are used for training. The other 28,760 images of 3985 categories are kept for test, split into a gallery set (12,612 images) and a query set (14,218 images).
5.1 IMPLEMENTATION DETAILS
We use the TensorFlow Deep Learning framework (Abadi et al., 2016) to implement the proposed method. For fair comparison, we exactly follow the details in Movshovitz-Attias et al. (2017). We use a GoogLeNet V1 (Szegedy et al., 2015) network pre-trained on ILSVRC 2012-CLS data (Russakovsky et al., 2015) as base network. The input images are all resized to 256 × 256. For training, the resized images are randomly crop to 224 × 224 with random horizontal flipping. In test phase, we use one single center crop as in Movshovitz-Attias et al. (2017). For Car196, CUB200 and fashion datasets the network is fine-tuned by SGD optimizer with 0.9 momentum. The learning rate is set to 0.004. For product dataset, the optimizer is ADAM with learning rate of 0.01. The embedding size is set to 64 and the batch size is set to 32. We choose  = 16 for all the datasets, as an "intermediate" temperature, it works well for different embedding sizes (see Sec. 5.3). For "heating-up",  will decrease (temperature increases on the other hand) from 16 to 4 and the learning rate will decrease to 1/10 of the original learning rate. The training process usually converges within 50 training epochs, which is similar to the fastest state-of-the-art method, ProxyNCA (Movshovitz-Attias et al., 2017).
5.2 EVALUATION
Following previous works on metric learning, we evaluate the clustering quality and the retrieval performance on the images of the test set. All the features are 2-normalized before calculating the evaluation metric, as in Song et al. (2017). The normalized features performs slightly better than the unnormalized feature. For clustering, the K-Means algorithm is run on all the embeddings of the test samples. The number of cluster is chosen to be the number of categories in the test set. Each test sample will be assigned a cluster index according to which cluster it belongs to. Normalized Mutual Information (NMI) (Schütze et al., 2008) between the clustering index and the ground-truth label is used as the metric for clustering.For retrieval, the performance is evaluated by Recall@K, which is also a widely used metric for this problem. Given a query sample from the test set, K samples from the rest of the test set (or the gallery set for the fashion dataset) with the smallest distance are retrieved. If any retrieved sample is from the same category as the query sample, the recall for this query is 1, otherwise, 0. The reported Recall@K is the average recall on the whole test set.
We train a classifier on the training dataset with the softmax function and cross-entropy as baseline (SM). For the baseline classifier, in training, the features and the weights are not normalized and  is set to 1. 4 different versions of classifiers trained with the proposed methods are used for evaluation:
· LN: softmax with 2-normalized embedding, 2-normalized weights and  = 16. · BN: softmax with batch normalized embedding, 2-normalized weights and  = 16. · HLN: Heated-up model using  = 4 to fine-tune LN. · HBN: Heated-up model using  = 4 to fine-tune BN.
We also compare the proposed method with state-of-the-art metric learning methods. Existing literatures are using different base networks and different evaluation protocols. For fair comparison, only the methods using GoogleNetV1 as base network and Euclidean distance as the final evaluation metric are listed: [1] Triplet learning with semi-hard negative mining (Schroff et al., 2015), [2] Lifted structured loss (Song et al., 2016), [3] Learnable Structured Clustering (Song et al., 2017); [4] Deep Clustering Learning without spectral learning (Law et al., 2017); [5] Deep Metric Learning with Smart Mining (Harwood et al., 2017) and [6] ProxyNCA (Movshovitz-Attias et al., 2017). For the Fashion dataset, we compared with [7] FashionNet (Liu et al., 2016) and [8] Hard-Aware Deeply Cascaded Embedding (Yuan et al., 2017).
5.3 METRIC LEARNING
The performances of all the methods on all datasets are listed in Tables 1 and 2. The softmax baseline already shows comparable result with many other triplet loss based methods. The embeddings trained with either 2-normalization or batch normalization improve the performance of the softmax baseline.
7

Under review as a conference paper at ICLR 2019

Table 1: NMI and Recall(%) for the Car196, CUB200 and Stanford datasets

[1]

NMI R@1 R@2 R@4

53.35 51.54 63.78 73.52

NMI R@1 R@2 R@4

55.38 42.59 55.03 66.44

NMI 89.46 R@1 66.67 R@10 82.39 R@100 91.85

[2]
56.88 52.98 66.70 76.01
56.50 43.57 56.55 68.59
88.65 62.46 80.81 91.93

[3]
54.44 58.11 70.64 80.27
59.23 48.18 61.44 71.83
89.48 67.02 83.65 93.23

[4] [5] [6] SM

CAR196 DATASET

61.12 67.54 77.77 85.74

59.50 64.65 76.20 84.23

64.90 73.22 82.42 86.36

59.52 60.76 73.58 82.50

CUB200 DATASET

56.87 50.08 62.24 73.38

59.90 49.78 62.34 74.05

59.53 49.21 61.90 67.90

57.19 44.02 55.86 68.18

STANFORD PRODUCT DATASET

88.70 64.52 82.53 92.35

-

90.60 73.70 -

88.66 63.94 80.07 90.28

LN
62.40 68.59 78.55 86.18
59.23 46.86 59.79 71.56
90.11 69.51 84.69 92.97

BN
65.81 71.12 80.62 87.82
59.20 47.27 59.67 71.89
90.45 71.19 85.89 93.75

HLN
66.87 71.93 81.68 88.34
60.34 49.68 61.85 73.08
90.39 70.36 85.41 93.70

HBN
68.10 74.70 83.90 89.77
60.75 50.68 62.58 73.82
90.61 72.04 86.25 93.80

Table 2: Recall(%) for the In-shop Clothes Retrieval Dataset
[7] [8] SM LN BN HLN HBN
R@1 53.0 62.1 78.6 79.6 80.7 80.5 81.1 R@10 73.0 84.9 93.7 94.2 94.4 94.2 94.2 R@20 76.0 91.2 95.4 96.0 96.1 96.1 95.9 R@30 77.0 92.3 96.3 96.8 96.9 96.7 96.9

Since in test phase, all the features are 2 normalized before calculating the metric, the performance gain is not coming from the 2-normalization of the final features. Batch normalization works slightly better than 2-normalization. The "heated-up" models (HLN and HBN) show better performance in almost all the metrics compared to the embedding trained with a fixed temperature.
We further study how different embedding sizes and  values affect the retrieval performance. The size of the embedding is chosen in [64, 128, 256], and the  value in [4, 8, 16, 32, 64]. The R@1 metric on the test set with different embedding sizes and  values is reported in Tab. 3. Performances of the feature learned by softmax function without normalization and feature learned from "heated-up" model are also given. The "heated-up" model outperforms all the other models by a significant margin. Between models trained with fixed  values, the model using  = 16 outperforms others.

Table 3: R@1(%) for Car196 with Different  Values and Embedding Sizes

#DIM

MODEL SM  = 4  = 8  = 16  = 32  = 64 HBN (16  4)

64

60.8 67.4 68.7 71.1 69.5 62.5

74.0

128

65.2 71.6 71.0 74.2 73.0 66.6

77.5

256

67.3 72.2 69.7 78.0 75.2 70.1

80.1

6 DISCUSSION
We have discussed how the temperature parameter in the softmax function affects the distribution of the embedding in the second last layer of a deep classification model. Training with an intermediate temperature will lead to an intra-category compact and inter-category "spread-out" embedding which is beneficial for both clustering and retrieval. A "heating-up" method is also proposed to further improve the clustering and retrieval performance of the embedding by fine-tuning with a higher temperature. Our classifier based approach achieves good performance in metric learning problems with a simple and efficient training process.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A System for Large-Scale Machine Learning. OSDI, 2016.
Artem Babenko and Victor Lempitsky. Aggregating Deep Convolutional Features for Image Retrieval. In ICCV, 2015.
S. Chopra, R. Hadsell, and Y. LeCun. Learning a Similarity Metric Discriminatively, with Application to Face Verification. In CVPR, 2005.
Matthieu Guillaumin, Jakob Verbeek, and Cordelia Schmid. Is that you? metric learning approaches for face identification. In CVPR, 2009.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017.
Ben Harwood, Vijay Kumar B G, Gustavo Carneiro, Ian Reid, and Tom Drummond. Smart Mining for Deep Metric Learning. In ICCV, 2017.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. In NIPS Workshop, 2015.
Elad Hoffer and Nir Ailon. Deep Metric Learning Using Triplet Network. In International Workshop on Similarity-Based Pattern Recognition, 2015. Springer, 2015.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In ICML, 2015.
Martin Koestinger, Martin Hirzer, Paul Wohlhart, Peter M Roth, and Horst Bischof. Large scale metric learning from equivalence constraints. In CVPR, 2012.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D Object Representations for FineGrained Categorization. In 4th International IEEE Workshop on 3D Representation and Recognition, 2013.
Marc T. Law, Raquel Urtasun, and Richard S. Zemel. Deep Spectral Clustering Learning. In ICML, 2017.
Yann LeCun et al. Lenet-5, Convolutional Neural Networks. url: http://yann.lecun.com/exdb/lenet, 2015.
Jung-Eun Lee, Rong Jin, and Anil K Jain. Rank-based distance metric learning: An application to image retrieval. In CVPR, 2008.
Giuseppe Lisanti, Svebor Karaman, and Iacopo Masi. Multichannel-kernel canonical correlation analysis for cross-view person reidentification. ACM TOMM, 2017.
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. SphereFace: Deep Hypersphere Embedding for Face Recognition. In CVPR, 2017.
Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations. In CVPR, 2016.
Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Working Hard to Know Your Neighbor's Margins: Local Descriptor Learning Loss. In NIPS, 2017.
Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and Saurabh Singh. No Fuss Distance Metric Learning Using Proxies. In ICCV, 2017.
Rajeev Ranjan, Carlos D. Castillo, and Rama Chellappa. L2-constrained Softmax Loss for Discriminative Face Verification. arXiv:1703.09507 [cs], 2017.
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-Shelf: an Astounding Baseline for Recognition. In CVPRW, 2014.
9

Under review as a conference paper at ICLR 2019
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Unified Embedding for Face Recognition and Clustering. In CVPR, 2015.
Hinrich Schütze, Christopher D Manning, and Prabhakar Raghavan. Introduction to Information Retrieval. Cambridge University Press, 39, 2008.
Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep Metric Learning via Lifted Structured Feature Embedding. In CVPR, 2016.
Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy. Deep Metric Learning via Facility Location. In CVPR, 2017.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going Deeper With Convolutions. In CVPR, 2015.
Evgeniya Ustinova and Victor Lempitsky. Learning Deep Embeddings with Histogram Loss. In NIPS, 2016.
Feng Wang, Xiang Xiang, Jian Cheng, and Alan L. Yuille. NormFace: L2 Hypersphere Embedding for Face Verification. In ACM MM 2017, 2017a.
Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing Lin. Deep Metric Learning with Angular Loss. In ICCV, 2017b.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical report, 2010.
Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and Philipp Krähenbühl. Sampling Matters in Deep Embedding Learning. In ICCV, 2017.
Eric P Xing, Michael I Jordan, Stuart J Russell, and Andrew Y Ng. Distance metric learning with application to clustering with side-information. In NIPS, 2003.
Jieping Ye, Zheng Zhao, and Huan Liu. Adaptive distance metric learning for clustering. In CVPR, 2007.
Yuhui Yuan, Kuiyuan Yang, and Chao Zhang. Hard-Aware Deeply Cascaded Embedding. In ICCV, 2017.
Xu Zhang, Felix X. Yu, Sanjiv Kumar, and Shih-Fu Chang. Learning Spread-Out Local Feature Descriptors. In ICCV, 2017.
10

Under review as a conference paper at ICLR 2019

APPENDIX
A GRADIENT ANALYSIS FOR CLASSIFIER WITH NORMALIZATION AND TEMPERATURE
A.1 PROOF OF PROPERTY 2
Eq (3) contains M terms in the sum, one term for each of the M categories. There is 3 types of terms: type 1, term with respect to the ground-truth category; type 2, term with respect to the logits that has the largest value and does not belong to the ground-truth category; type 3, other terms.
For   +, considering the term of type 1 for "correct" samples first, since lim+ p(y|x, ) = 1 and q(y|x) = 1, according to the property of exponential function, lim+ (p(y|x, ) - q(y|x)) = 0 and the magnitude for this term will approach 0. For other terms, the magnitude will also approach 0, due to lim+ p(m|x, ) = 0, and q(m|x) = 0, m = y. Therefore, the magnitude of the gradient will always approach to 0.
Considering "incorrect" samples, the magnitude of the type 1 term ((p(y|x, ) - q(y|x))w^ y) will approach + as   +, because p(y|x, ) will approach either 0 or 1/K (K  2)3 and q(y|x) = 1. Similarly, the magnitude of the term of type 2 will also approach +. For other terms, due to the property of the exponential function, that if zm = max(z1, ..., zM ), lim+ p(m|x, ) = 0, the magnitude of any term of type 3 will decrease to 0. Therefore, for "incorrect" samples, as   +, unless in some special cases4, the magnitude of the gradient with respect to the normalized embedding will approach infinity.
A.2 PROOF OF PROPERTY 3
From Eq. (1) and Eq. (5), similar to the discussion in Appendix A.1, for the normalized feature ^f of a "correct" sample, lim+ (p(y|x, ) - 1) = 0 and lim+ p(m|x, ) = 0, m = y. Applying these to Eq. (4), Eq. (9) is proved.
For the normalized feature ^f of an "incorrect" sample, similar to the discussion in Appendix. A.1, when lim+(p(m|x, ) - q(m|x)) = 0, which means zm = max(z1, ..., zM ) or m = y, the magnitude of the gradient will approach to +. Otherwise, the magnitude of the gradient will approach to 0.

B THE GRADIENT ANALYSIS FOR OFF-THE-SHELF CLASSIFIER
To understand the feature learned with the off-the-shelf classifier, we need to study the gradient of the loss with respect to the feature and the gradient of the loss with respect to the classifier weights.

B.1 GRADIENT WITH RESPECT TO THE FEATURE

By setting  = 1, considering zj = ||f ||2wjT ^f , Eq. (1) becomes:

p(m|x) =

exp(||f ||2wmT ^f )

M j=1

exp(||f

||2

wjT

^f )

(14)

p(m|x) satisfies:

lim p(m|x) = 1/K zm = max(z1, ..., zM )

||f ||2+

0 otherwise,

(15)

where K is the number of logits whose value equals the maximum logits value. In other words, as ||f ||2 increases, the predicted probability will be spikier at the logits that have the largest value.

3K can't be 1, otherwise x is a "correct" sample. 4For example, two terms having exactly the same magnitude but opposite direction.

11

Under review as a conference paper at ICLR 2019

||l/f ||2 ||l/f ||2 ||l/f ||2

1.5 1
0.5 0 ||f ||2
(a) Correct centroid sample

1.5 1
0.5 0 ||f ||2
(b) Correct boundary sample

1.5 1
0.5 0 ||f ||2
(c) Incorrect sample

Figure 3: Relation between the norm of the feature and the magnitude of the gradient with respect to "correct" (centroid and boundary) and "incorrect" samples for SM model. The green dashed lined correspond to a small feature norm, the red one to an intermediate feature norm and the blue one to a large feature norm. The gradients are sampled from models trained with Car196 dataset.

By setting  = 1, Eq. (3) becomes,

 f

M
=



zm

m=1 zm f

M
= (p(m|x) - q(m|x))wm
m=1

(16)

We also consider the 3 types of terms in Eq. (16) as defined in Sec. 3.2. When ||f ||2  +, for "correct" samples, the predicted probability will become a one-hot vector, and all the terms in Eq. 16 will approach to 0. Therefore, the overall gradient will approach to 0 and the magnitude of the gradient is 0.
For "incorrect" samples, when ||f ||2  +, since p(y|x) will approach to either 0 or 1/K and q(y|x) = 1, type 1 term will become either -wy or (1/K - 1)wy, both are fixed vector. Similarly, all type 2 terms will become fixed vectors (1/Kwm). Type 3 terms will approach to 0. Therefore, Eq. 16 will approach to a fixed vector (the vector sum of type 1 term and type 2 terms) and the magnitude of the gradient will approach to a constant. Unless some special cases, the constant is not 0.
Thus, the properties used in Sec. 4 have been proved. Similarly to Figure 1, the relation between the magnitude of the gradient w.r.t. the norm of the feature for "incorrect" and "correct" samples is shown in Fig. 3. As boundary features trained with large , boundary features (Fig. 3(b)) with large norm (blue dashed line in Fig. 3) will not get enough update thus leading to a not compact embedding.

B.2 GRADIENT WITH RESPECT TO THE WEIGHTS

Considering the gradient with respect to the weights, from Eq. 2, we have

 wm

=

 zm

zm wm

=

(p(m|x) - q(m|x))f

=

||f ||2(p(m|x) - q(m|x))^f

(17)

From Eq. (14), Eq. (15) and the property of the exponential function, similar to the discussion in
Appendix. A.2, we have, for a "correct" sample feature f , lim||f||2+ ||f ||2(p(y|x) - 1) = 0 and lim||f||2+ ||f ||2p(m|x) = 0, m = y, and therefore

 lim = 0. ||f ||2+ wm

(18)

For "incorrect" samples,

lim  = +  zm = max(z1, ..., zM ) or m = y;

||f ||2+ wm

0

otherwise.

(19)

Therefore, for a "correct" sample, whose feature has large norm, the network will failed to push the weights of other categories away from the feature, even if the weights are close to the feature. Also, for an "incorrect" sample, whose feature has large norm, the network will only consider the weights of the categories with largest logits and the weight of the ground-truth category of the sample.

12

Under review as a conference paper at ICLR 2019
Even if the other weights are close to the feature (but not close enough to be the weight of the maximum logits), the network will not push the weights away. Therefore, the weights vector may not be spread-out. Since the network will move the feature towards the corresponding weight in order to increase the logits and decrease the loss, if the weights are not spread-out, the features will not be spread-out.
C VISUALIZATION OF THE DISTRIBUTION OF THE FEATURE
The normalized histograms of the cosine similarity between the embeddings (learned with different models on the training set of the Car196 dataset) of samples of the same class (red) and samples of different class (blue) are shown in Fig. 4, for both the training (left) and test (right) sets. We show features trained with a total of 6 models: 1) Softmax, 2) BN (Softmax+BN),  = 4, 3) BN,  = 8, 4) BN,  = 16, and 5) BN,  = 32, and 6) HBN,  = 16  4, A good embedding should exhibit the following properties:
· the features from the same category should be compact, which would transcribe as the red histogram being concentrated at the location where the cosine similarity is closed to 1;
· the features from different categories are spread-out, which would be observed as concentration of the blue histogram at the location where the cosine similarity is close to 0. In other words, two randomly sampled samples from different categories are close to orthogonal (Zhang et al., 2017).
· the embedding should be able to generalize to unseen classes of the same domain, i.e. the distribution on the test set should also exhibit compactness and spread-out properties.
Fig. 4(a) and 4(b) are the histograms for the model trained with the softmax function without any normalization. The main drawback of the feature is that the histogram of the feature of different categories is not concentrated (both for the training and test set), which means the samples from different categories are not spread-out (as detailed in Appendix B.2). Therefore the margin between different categories is not enough, or the features are not discriminative enough. For BN models learned with different  values (Fig. 4(c)-4(j)), in training set (left hand side), we clearly see the trend that a smaller  value will lead to more compact features. Unfortunately, this trend doesn't hold for test set (right hand side). For the model trained with  = 4, 8, although in the training set the histograms show nice compactness and spread-out properties, however the features are not compact at all on the test set. The reason is that, in metric learning problem, the training categories and the test categories are not overlapped, pushing the training samples of each category too close to each other can lead to overfitting. We are especially interested in comparing the histograms of the features learned without heating-up (Fig. 4(g)-4(h)) and with heating-up (Fig. 4(k)-4(l)). Applying the proposed "heating-up" strategy, i.e. fine-tuning the network with smaller  and learning rate, makes the positive pairs more compact while keeping the negative pairs spread-out. It improves the clustering and retrieval performance over model trained without "heating-up" as shown in Sec. 5.3.
13

Under review as a conference paper at ICLR 2019

Positive

Negative

4 2 01.00 0.75 0.50 0.25 0.00

0.25 0.50

(a) Softmax, Training

Positive

Negative

Positive 2

Negative

1

01.00 0.75 0.50 0.25 0.00 0.25 0.50

(b) Softmax, Test

Positive

Negative

20 2

01.00 0.75 0.50 0.25 0.00 0.25 0.50 01.00 0.75 0.50 0.25 0.00 0.25 0.50

(c)  = 4, Training

Positive

Negative

(d)  = 4, Test

Positive

Negative

10 01.00 0.75 0.50 0.25 0.00 0.25 0.50

2 1 01.00 0.75 0.50 0.25 0.00

0.25 0.50

(e)  = 8, Training

Positive

Negative

4 2 01.00 0.75 0.50 0.25 0.00

0.25 0.50

(f)  = 8, Test

Positive 2

Negative

1

01.00 0.75 0.50 0.25 0.00 0.25 0.50

(g)  = 16, Training

Positive

Negative

2

(h)  = 16, Test

Positive 2

Negative

1

01.00 0.75 0.50 0.25 0.00 0.25 0.50 01.00 0.75 0.50 0.25 0.00 0.25 0.50

(i)  = 32, Training

Positive

Negative

5.0

2.5

0.01.00 0.75 0.50 0.25 0.00 0.25 0.50

(j)  = 32, Test

Positive 2

Negative

1

01.00 0.75 0.50 0.25 0.00 0.25 0.50

(k) Heated-Up:  = 16  4, Training

(l) Heated-Up:  = 16  4, Test

Figure 4: Embeddings trained with 2-normalization and different  values on Car196 dataset.

14

Under review as a conference paper at ICLR 2019
D FINE-GRAINED CLASSIFICATION
Metric learning focuses on learning a compact and spread-out embedding. The goal of fine-grained classification focuses on learning a decision function to better classify the samples of different categories. Although the fine grained classification task doesn't require a compact and spread-out embedding, it's still interesting to see whether the proposed heating-up strategy helps fine-grained classification or not. We report in Table 4 the results of the fine-grained classification performance of different models on the Car196 and CUB200 datasets. The fine-grained classification model is trained with the predefined training/test split on all categories. Different from the training setting for metric learning in Sec. 5.3, here the training and test categories are the same. Except this, all other settings are the same. The classification accuracy over all categories is used for evaluation. The models trained with heating-up strategy outperforms the baseline model by a clear margin. The 2 normalization model seems to work better than batch normalization model in fine-grained classification problem.
Table 4: Fine-grained Classification Performance for Car196 and CUB200 Datasets SM LN BN HLN HBN
CAR196 83.6 84.8 85.5 87.0 87.0 CUB200 68.7 69.1 68.8 75.4 70.9
15

