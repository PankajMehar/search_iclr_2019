Under review as a conference paper at ICLR 2019

CRAMER-WOLD AUTOENCODER
Anonymous authors Paper under double-blind review

ABSTRACT
Assessing distance betweeen the true and the sample distribution is a key component of many state of the art generative models, such as Wasserstein Autoencoder (WAE). Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and kernel smoothing we construct a new generative model ­ Cramer-Wold AutoEncoder (CWAE). CWAE cost function, based on introduced Cramer-Wold distance between samples, has a simple closed-form in the case of normal prior. As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of WAE-MMD (WAE using maximum mean discrepancy based distance function) and often improves upon SWAE.

1 INTRODUCTION

One of the crucial aspect in construction of generative models is devising effective method for computing and minimizing distance between the true and the model distribution. Originally in Variational Autencoder (VAE) (Kingma & Welling, 2014) this computation was carried out using variational methods. An important improvement was brought by the introduction of Wasserstein metric (Tolstikhin et al., 2017), which relaxes the need for variational methods. However, the computation of Wasserstein distance is still nontrivial and requires a separate optimization problem to be solved to approximate the optimal transport function. Most recent contribution to this trend of simplifying the construction of generative models is Sliced-Wasserstein Autoencoder (SWAE, Kolouri et al. (2018)), where a significantly simpler AutoEncoder-based model is proposed.

The main innovation of SWAE was the introduction of the sliced-Wasserstein distance ­ a fast to estimate metric for comparing two distributions, based on the mean Wasserstein distance of one-dimensional projections. However, even in SWAE there is no close analytic formula that would enable computing the distance of the sample from the standard normal distribution. Consequently in SWAE two types of sampling are needed: (i) sampling from the prior distribution and (ii) sampling over one-dimensional projections.

Our main contribution is introduction of the Cramer-

Wold distance between distributions, which has a

closed-form for the distance of a sample from stan-

dard multivariate normal distribution. We use it to

construct an AutoEncoder based generative model, called Cramer-Wold AutoEncoder (CWAE), in which the cost function, for a normal prior distribution, has a closed analytic formula. We benchmark CWAE against generative models using a non-parametrized distance

Figure 1: Cramer-Wold distance of two sets is obtained as the mean squared L2-distance of their smoothed projections on all onedimensional lines.

functions: WAE-MMD (WAE variant from Tolstikhin

et al. (2017) based on the classical MMD distance) and

SWAE. Our results show that, while simplifying the optimization procedure by forgoing the need to

sample data projections or from the prior distribution we obtain an AutoEncoder based generative

1

Under review as a conference paper at ICLR 2019

model which obtains quantitatively and qualitatively similar results to WAE-MMD and SWAE models, see Section 5.
Let us now briefly describe the outline of the paper. In the following two sections we introduce and theoretically investigate the Cramer-Wold distance. However, the reader interested mainly in the construction of our generative AutoEncoder model can proceed directly to Section 4. Section 5 contains experiments. Conclusions are given in Section 6.

2 CRAMER-WOLD DISTANCE: CONSTRUCTION

Motivated by the prevalent use of normal distribution as prior in modern generative models, we investigate whether it is possible to simplify optimization of such models. As the first step towards this, in this section we introduce Cramer-Wold distance, which has a simple analytical formula for computing normality of a high-dimensional sample. On a high level our approach uses the traditional L2 distance of kernel-based density estimation, computed across multiple single-dimensional projections of the true data and the output distribution of the model. We base our construction on the following two popular tricks of the trade:

Sliced-based decomposition of a distribution: Following the footsteps of Kolouri et al. (2018);
Deshpande et al. (2018), the first idea is to leverage the Cramer-Wold Theorem (Cramér & Wold,
1936) and Radon Transform (Deans, 1983) to reduce computing distance between two distributions to one dimensional calculations. For v in the unit sphere SD  RD, the projection of the set X  RD onto the space spanned by v is given by vT X and the projection of N (m, I) is N (vT m, ).
Cramer-Wold theorem states that two multivariate distributions can be uniquely identified by their all
one-dimensional projections. For example, to obtain the key component of SWAE model, i.e. the sliced-Wasserstein distance between two samples X, Y  RD, we compute the mean Wasserstein distance between all one-dimensional projections:

dW (X, Y ) = dW (vT X, vT Y ) dD(v),
SD

(1)

where SD denotes the unit sphere in RD and D is the normalized surface measure on SD. This approach is effective since the one-dimensional Wasserstein distance between samples has the closed

form, and therefore to estimate (1) one has to sample only over the projections.

Smoothing distributions: Using the sliced-based decomposition requires us to define distance
between two sets of samples, in a single dimensional space. To this end we will use a trick-of-trade
applied commonly in statistics in order to compare samples or distributions which is to first smoothen (sample) distribution with a Gaussian kernel. For the sample R = (ri)i=1..n  R by its smoothing with Gaussian kernel N (0, ) we understand

sm (R)

=

1 n

N (ri, ),

i

where by N (m, S) we denote the one-dimensional normal density with mean m and variance S.

This produces a distribution with regular density, and is commonly used in kernel density estimation.

If R has standard deviation close to one, the asymptotically optimal choice of  is given by the

Silverman's rule of thumb 

=

(

4 3n

)2/5,

see

Silverman

(1986).

For continuous density f , its

smoothing sm(f ) is given by the convolution with N (0, ), and in the special case of Gaussians

we have sm(N (m, S)) = N (m, S + ). While in general kernel density estimations works well

only in low-dimensional spaces, this fits the bill for us, as we will only compute distances on single

dimensional projections of the data.

Cramer-Wold distance. We are now ready to introduce the Cramer-Wold distance. In a nutshell,

we propose to compute the squared distance between two samples by considering the mean squared

L2 distance between their smoothed projections over all single dimensional subspaces. By the squared

L2 distance between functions f, g : R  R we refer to

f -g

2 2

=

|f (x) - g(x)|2dx. A key

feature of this distance is that it permits a closed-form in the case of normal distribution.

More precisely, the following algorithm fully defines the Cramer-Wold distance between two samples X = (xi)i=1..n, Y = (yj)j=1..k  RD (for illustration of Steps 1 and 2 see Figure 1):

2

Under review as a conference paper at ICLR 2019

1. given v in the unit sphere S(0, 1)  RD consider the projections vT X = (vT xi)i=1..n and vT Y = (vT yj )j=1..k,
2. compute the squared L2 distance of the densities sm(vT X) and sm(vT X):
sm (vT X) - sm (vT Y ) 22,
3. to obtain squared Cramer-Wold distance average (integrate) the above formula over all possible v  SD.

3 CRAMER-WOLD DISTANCE: THEORY

The key theoretical outcome of this paper is that the result of the computation of the Cramer-Wold distance from the previous section can be simplified to a closed form solution. Consequently, to compute the distance of two samples there is no need of finding the optimal transport like in WAE or the necessity to sample over the projections like in SWAE.
Theorem 3.1. Let X = (xi)i=1..n, Y = (yj)j=1..n  RD be given1. We formally define the squared Cramer-Wold distance by the formula

d2cw(X, Y ) :=

sm (vT X) - sm (vT Y )

2 2

dD

(v).

SD

Then

dc2w(X, Y ) = 2n21

D (

xi -xi 4

2
)+

D (

yj -yj 4

2
)-2

D (

xi -yj 4

2
)

,

ii jj

ij

(2)

where

D (s)

=

1F1(

1 2

;

D 2

;

-s)

and

1F1

is

the

Kummer's

confluent

hypergeometric

function

(see,

e.g.,

Barnard et al. (1998)). Moreover, D(s) has the following asymptotic formula valid for D  20:

D (s)



(1

+

4s 2D-3

)-1/2

.

(3)

To prove the Theorem 3.1 we will need the following crucial technical proposition. Proposition 3.1. Let z  RD and  > 0 be given. Then

N (vT z, )(0) dD(v)

=

1 2

D

z2 2

.

SD

(4)

Proof. By applying orthonormal change of coordinates without loss of generality we may assume that z = (z1, 0, . . . , 0), and then vT z = z1v1 for v = (v1, . . . , vD). Consequently we get

SD N (vT z, )(0) dD(v) = SD N (z1v1, )(0) dD(v).

Making use of the formula for slice integration of functions on spheres (Axler et al., 1992, Corol-

lary A.6) we get:

SD

f

dD

=

VD-1 VD

-11(1 - x2)(D-3)/2

 SD-1 f (x, 1 - x2 ) dD-1() dx,

where VK denotes the surface volume of a sphere SK  RK . Applying the above equality for the function f (v1, . . . , vD) = N (z1v1, )(0) and s = z12/(2) = z 2/(2) we consequently get that the LHS of (4) simplifies to

VD-1  1 VD 2

-11(1 - x2)(D-3)/2 exp(-sx2) dx,

which completes the proof since VK

=

K

2· 2

(

K 2

)

and

1 -1

exp(-sx2

)(1

-

x2)(D-3)/2 dx

 

(

D-1 2

(

D 2

)

)

1F1

1 2

;

D 2

;

-s

.

1For clarity of presentation we provide here the formula for the case of samples of equal size.

=

3

Under review as a conference paper at ICLR 2019

Proof of Theorem 3.1. Directly from the definition of smoothing we obtain that

dc2w(X, Y ) =

1 n

N (vT

xi, )

-

1 n

N (vT yj, )

2 2

dD

(v).

SD i

j

(5)

Now applying the one-dimensional formula the the L2-scalar product of two Gaussians:

N (r1, 1), N (r2, 2) 2 = N (r1 - r2, 1 + 2)(0)

and the equality

f

-g

2 2

=

f, f 2 +

g, g 2 - 2 f, g 2 (where

f, g 2 =

f (x)g(x)dx), we

simplify the squared-L2 norm in the integral of RHS of (5) to

1 n

N (vT

xi,

)

-

1 n

N (vT yj, )

2 2

ii

=

1 n2

N (vT xi, ),

N (vT xi, )

2

+

1 n2

N (vT yj, ), N (vT yj, ) 2

ii

jj

-

2 n2

N (vT xi, ), N (vT yj, ) 2

ij

=

1 n2

N (vT

(xi

-

xi

),

2)(0)

+

1 n2

N (vT (yj

-

yj

),

2)(0)

-

2 n2

N (vT (xi - yj), 2)(0).

ii jj ij

Applying directly Proposition 3.1 we obtain formula (2). Proof of the formula for the asymptotics of the function D is provided in the Appendix.

Thus to estimate the distance of a given sample X to some prior distribution f , one can follow the common approach and take the distance between X and a sample from f . As the main theoretical result of the paper we view the following theorem, which says that in the case of standard Gaussian multivariate prior, we can completely reduce the need for sampling (we omit the proof since it is similar to that of Theorem 3.1).
Theorem 3.2. Let X = (xi)i=1..n  RD be a given sample. We formally define

dc2w(X, N (0, I)) :=

sm(vT X) - sm(N (0, 1)) 22dD(v).

SD

Then

dc2w(X, N (0, I))

=

1 2n2 

1

i,j

D (

xi -xj 4

2
)+

n2 1+

-

2n



+

1 2

D

(

xi 2 2+4

)

.

i

(6)

One can easily obtain the general formula for the distance between mixtures of radial distributions. This follows from the fact that the Cramer-Wold distance is given by a scalar product ·, · cw which has a closed-form for the product of two radial Gaussians:

N (x, I), N (y, I)

cw

=

1
2(++2)

D

x-y 2 2(++2)

.

4 CRAMER-WOLD AUTOENCODER MODEL (CWAE)

This section is devoted to the construction of CWAE. Since we base our construction on the AutoEncoder, to establish notation let us formalize it here.

AutoEncoder. Let X = (xi)i=1..n  RN be a given data set. The basic aim of AE is to transport the data to a typically, but not necessarily, less dimensional latent space Z = RD with reconstruction error as small as possible. Thus, we search for an encoder E : Rn  Z and decoder D : Z  Rn
functions, which minimize the reconstruction error on the data set X:

M SE(X; E, D) = 1 n n

xi - D(Exi) 2.

i=1

4

Under review as a conference paper at ICLR 2019

AutoEncoder based generative model. CWAE, similarly to WAE, is a classical AutoEncoder model with modified cost function which forces the model to be generative, i.e. ensures that the data transported to the latent space comes from the (typically Gaussian) prior f . This statement is formalized by the following important remark, see also (Tolstikhin et al., 2017). Remark 4.1. Let X be an N -dimensional random vector from which our data set was drawn, and let Y be a random vector with a density f on latent Z.
Suppose that we have constructed functions E : RN  Z and D : Z  RN (representing the encoder and the decoder) such that
1. D(Ex) = x for x  image(X),
2. random vector EX has the distribution f .
Then by the point 1 we obtain that D(EX) = X, and therefore
DY has the same distribution as D(EX) = X.
This means that to produce samples from X we can instead produce samples from Y and map them by the decoder D.
Since an estimator of the image of the random vector X is given by its sample X, we conclude that a generative model is correct if it has small reconstruction error and resembles the prior distribution in the latent. Thus, to construct a generative AutoEncoder model (with Gaussian prior), we add to its cost function a measure of distance of a given sample from normal distribution.

CWAE cost function. Once the crucial ingredient of CWAE is ready, we can describe its cost
function. To ensure that the data transported to the latent space Z are distributed according to the standard normal density, we add to the cost function logarithm2 of the Cramer-Wold distance from standard multivariate normal density dc2w(X, N (0, I)):

cost(X; E, D) = log d2cw(EX, N (0, I)) + M SE(X; E, D).

(7)

Since the use of special functions involved in the formula for Cramer-Wold distance might be

cumbersome, we apply in all experiments (except for the illustrative 2D case) the asymptotic form (9)

of function D:

2d2cw (X )



1 n2

(n +

xi -xj 2D-3

2 )-1/2

+

(1

+

n)-1/2

-

2 n

(n

+

1 2

+

xi 2 2D-3

)-1/2

,

ij i

where

n

=

(

4 3n

)2/5

is

chosen

by

the

Silverman's

rule

of

thumb

(Silverman,

1986).

Comparison with WAE and SWAE models. Finally, let us briefly recapitulate differences between the introduced CWAE, WAE variants of (Tolstikhin et al., 2017) and SWAE (Kolouri et al., 2018). In contrast to WAE-MMD and SWAE, CWAE model does not require sampling from normal distribution (as in WAE-MMD) or over slices (as in SWAE) to evaluate its cost function, and in this sense uses a closed formula cost function. In contrast to WAE-GAN, our objective does not require a separately trained neural network to approximate the optimal transport function, thus avoiding pitfalls of adversarial training. In this paper we are interested in WAE-MMD and SWAE models, which do not use parametrized distance functions, e.g. trained adversarially like in WAE-GAN. However, in future work we plan to introduce an adversarial version of CWAE and compare it with WAE-GAN.

5 EXPERIMENTS
In this section we empirically validate the proposed CWAE model on standard benchmarks for generative models: CelebA, Cifar-10 and MNIST. We will compare CWAE model with WAEMMD (Tolstikhin et al., 2017) and SWAE (Kolouri et al., 2018). As we will see, our results match those of WAE-MMD, and in some cases improve upon SWAE, while using a simpler to optimize
2We take the logarithm of the Cramer-Wold distance to improve balance between the two terms in the objective function.

5

Under review as a conference paper at ICLR 2019

cost function (see the previous section for a more detailed discussion). The rest of this section is structured as follows. In Section 5.2 we report results on standard qualitative tests, as well as a visual investigations of the latent space. In Section 5.3 we will turn our attention to quantitative tests using Fréchet Inception Distance and other metrics.

5.1 EXPERIMENTATION SETUP
In the experiment we have used two basic architecture types. Experiments on MNIST were performed using a feedforward network for both encoder and decoder, and a 20 neuron latent layer, all with ReLU activations. In case of CIFAR-10 and CelebA data sets we used convolution-deconvolution architectures. Please refer to Appendix C for full details.

5.2 QUALITATIVE TESTS

The quality of a generative model is typically evaluated by

examining samples or interpolations. We present such a com-

parison between CWAE with WAE-MMD in Figure 2. We Table 1: CWAE achieves similar

follow the same procedure as in (Tolstikhin et al., 2017). In FID (lower is better) and sharpness

particular, we use the same base neural architecture for both (higher is better) to WAE-MMD on

CWAE and WAE-MMD. We consider for each model (i) in- the original WAE architecture (see

terpolation between two random examples from the test set Appendix C for details).

(leftmost in Figure 2), (ii) reconstruction of a random example from the test set (middle column in Figure 2), and finally a sample reconstructed from a random point sampled from the prior distribution (right column in Figure 2). The experiment shows that there are no perceptual differences between CWAE and WAE-MMD generative distribution.

Algorithm FID Sharpness

SWAE VAE WAE-MMD CWAE

72 63 55 54

0.008 0.003 0.006 0.006

In the next experiment we qualitatively assess normality of WAE-GAN the latent space. This will allow us to ensure that CWAE does True data

42 2

0.006 0.020

not compromise on the normality of its latent distribution,

which recall is part of the cost function for all the models

except AE. We compare CWAE3 with AE, VAE, WAE and SWAE on the MNIST data with using

2-dimensional latent space and a two dimensional Gaussian prior distribution. Results are reported in

Figure 3. As is readily visible, the latent distribution of CWAE is as close, or perhaps even closer, to

the normal distribution than that of the other models. Furthermore, the AutoEncoder presented in the

second figure is noticeably different from a Gaussian distribution, which is to be expected because it

does not optimize for normality in contrast to the other models.

To summarize, both in terms of perceptual quality and satisfying normality objective, CWAE matches WAE-MMD. The next section will provide more quantitative studies.

5.3 QUANTITATIVE TESTS

In order to quantitatively compare CWAE with other models, in the first experiment we follow the common methodology and use the Fréchet Inception Distance (FID) introduced by Heusel et al. (2017). Further, we evaluate the sharpness of generated samples using the Laplace filter following Tolstikhin et al. (2017). Results for CWAE and WAE are summarized in Tab. 1. In agreement with the qualitative studies, we observe FID and sharpness scores of CWAE to be similar to WAE-MMD.
Next, by comparing training time between CWAE and other models, we found that for batch-sizes up to 1024, which covers the range of batch-sizes used typically for training autoencoders, CWAE is faster (in terms of time spent per batch) than other models. More precisely, CWAE is approximately 2× faster up to 256 batch-size. Details are relegated to the Appendix B.

3Since (3) is valid for dimensions D  20, to implement CWAE in 2-dimensional latent space we apply

equality

1F1(1/2, 1, -s)

=

e-

s 2

I0

s 2

jointly with the approximate formula (Abramowitz & Stegun, 1964,

page 378) for the Bessel function of the first kind I0, for more details see Appendix A.

6

Under review as a conference paper at ICLR 2019

Test interpolation

Test reconstruction

Random sample

WAE-MMD

CWAE

Figure 2: CWAE achieves perceptually similar results to WAE-MMD. Results of WAE-MMD (first row) and CWAE (second row) on CelebA data set of original WAE-MMD architecture. Left: Interpolations between two examples from the test distribution (left to right, in each row). Middle: Reconstruction of examples from the test distribution; odd rows correspond to the real test points. Right: Reconstructed examples from a random samples from the prior distribution.

Figure 3: Latent distribution of CWAE is close to the normal distribution. Each subfigure presents points sampled from two-dimensional latent spaces, AE, VAE, WAE, SWAE, and CWAE (left to right). All trained on the MNIST data set.

Finally, motivated by Remark 4.1 we propose a novel method for quantitative assessment of the
models based on their comparison to standard normal distribution in the latent. To achieve this we
have decided to use one of the most popular statistical normality tests, i.e. Mardia tests (Henze, 2002). Mardia's normality tests are based on verifying whether the skewness b1,D(·) and kurtosis b2,D(·) of a sample X = (xi)i=1..n  RD:

b1,D (X )

=

1 n2

(xjT xk)3,

and

b2,D (X )

=

1 n

xj 4,

j,k j

are close to that of standard normal density. The expected Mardia's skewness and kurtosis for standard
multivariate normal distribution is 0 and D(D + 2), respectively. To enable easier comparison in experiments we consider also the value of the normalized Mardia's kurtosis given by b2,D(X) - D(D + 2), which equals zero for the standard normal density.

Results are presented in Figure 4 and Table 2. In Figure 4 we report for CelebA data set the value of reconstruction error, Mardia's skewness and kurtosis during learning process of AE, VAE, WAE, SWAE and CWAE (measured on the validation data set). WAE, SWAE and CWAE models obtain the best reconstruction error, comparable to AE. VAE model exhibits a sightly worse reconstruction error, but values of kurtosis and skewness indicating their output is closer to normal distribution. As

7

Under review as a conference paper at ICLR 2019

Table 2: Comparison between different models output distributions and the normal distribution, together with reconstruction error. All models outputs except AE are similarly close to the normal distribution. Normality is assesed by comparing Mardia's skewness, kurtosis, and the reconstruction error. For reference FID scores are provided as well (except for MNIST, where it is not defined).

Data set MNIST CIFAR10
CelebA

Method
Skewness Kurtosis (normalized) Reconstruction error
Skewness Kurtosis (normalized) Reconstruction error FID score error
Skewness Kurtosis (normalized) Reconstruction error FID score error

AE
659.67 749.58
2.10
11444.35 -2219.50
49.67 400.14
59770025.50 1363931.65 139.30 307.70

VAE
0.49 -410.69
4.12
3.07 -4158.33
82.82 218.43
22.07 53.09 142.46 95.35

WAE
82.12 35.61
2.11
1893.10 2346.49
45.80 146.34
301.71 942.68 139.10 96.30

SWAE
55.59 -37.43
2.11
996.01 193.15
44.84 145.04
196.64 507.39 138.23 100.56

CWAE
52.83 95.77
2.13
171.67 1943.35
45.52 121.16
59.22 307.29 138.54 97.22

Figure 4: Metrics assessing normality of the model output distributions, during training. We plot values of reconstruction error, Mardia's skewness and normalized kurtosis for AE, VAE, WAE, SWAE and CWAE, on the validation data set and the CelebA data sets. The optimal value of kurtosis (given by the kurtosis for normal distribution) is denoted by the dotted line.
expected, the output of AE is far from normal distribution; its kurtosis and skewness grow during learning. This arguably less standard evaluation, which we hope will find adoption in the community, serves as yet another evidence that CWAE has strong generative capabilities which at least match performance of WAE-MMD. Moreover we observe that VAE model's output distribution is closest to the normal distribution, at the expense of the reconstruction error, which is reflected by the blurred reconstructions typically associated with VAE model.
On the whole, WAE-MMD and CWAE achieve, practically speaking, the same level of performance in terms of FID score, sharpness, and our newly introduced normality test. Additionally, CWAE fares better in many of these metrics than SWAE.
6 CONCLUSIONS
In the paper we have presented a new autoencoder based generative model CWAE, which matches results of WAE-MMD, while using a cost function given by a simple closed analytic formula. We hope this result will encourage future work in developing simpler to optimize analogs of strong neural models.
Crucial in the construction of CWAE is the use of the developed Cramer-Wold metric between samples and distributions, which can be effectively computed for Gaussian mixtures. As a consequence we obtain a reliable measure of the divergence from normality. Future work could explore use of the Cramer-Wold distance in other settings, in particular in adversarial models.
8

Under review as a conference paper at ICLR 2019

REFERENCES
M. Abramowitz and I.A. Stegun. Handbook of mathematical functions with formulas, graphs, and mathematical tables, volume 55 of National Bureau of Standards Applied Mathematics Series. U.S. Government Printing Office, Washington, D.C., 1964.
S. Axler, P. Bourdon, and R. Wade. Harmonic function theory, volume 137 of Graduate Texts in Mathematics. Springer, New York, 1992.
R.W. Barnard, G. Dahlquist, K. Pearce, L. Reichel, and K.C. Richards. Gram polynomials and the kummer function. J. Approx. Theory, 94(1):128­143, 1998.
H. Cramér and H. Wold. Some theorems on distribution functions. J. London Math. Soc., 11(4): 290­294, 1936.
S.R. Deans. The Radon Transform and Some of Its Applications. Wiley, New York, 1983.
I. Deshpande, Z. Zhang, and A. Schwing. Generative modeling using the sliced wasserstein distance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3483­ 3491, 2018.
I.S. Gradshteyn and I.M. Ryzhik. Table of integrals, series, and products. Elsevier/Academic Press, Amsterdam, 2015.
N. Henze. Invariant tests for multivariate normality: a critical review. Statist. Papers, 43(4):467­506, 2002.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. arXiv:1706.08500, 2017.
D.P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.
D.P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2014.
S. Kolouri, Ch.E. Martin, and G.K. Rohde. Sliced-wasserstein autoencoder: an embarrassingly simple generative model. arXiv:1804.01947, 2018.
B.W. Silverman. Density estimation for statistics and data analysis. Monographs on Statistics and Applied Probability. Chapman & Hall, London, 1986.
I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. arXiv:1711.01558, 2017.
F. Tricomi and A. Erdélyi. The asymptotic expansion of a ratio of gamma functions. Pacific J. Math., 1:133­142, 1951.

APPENDICES

A COMPUTATION OF D

In this section we consider the estimation of values of the function

D (s)

=

1F1(

1 2

;

D 2

;

-s)

for

s



0,

which is crucial in the formulation for the Cramer-Wold distance. First we will provide its approximate asymptotic formula valid for dimensions D  20, and then we shall consider the special case of D = 2 (see Figure 5)

To do so, let us first recall (Abramowitz & Stegun, 1964, Chapter 13) that the Kummer's confluent hypergeometric function 1F1 (denoted also by M ) has the following integral representation

(b) 1F1(a, b, z) = (a)(b - a)

1
ezuua-1(1 - u)b-a-1 du,
0

9

Under review as a conference paper at ICLR 2019

valid for a, b > 0 such that b > a. Since we consider that latent is at least of dimension D  2, it

follows that

D (s)

=

(

D 2

)

(

1 2

)(

D 2

-

1 2

)

1
e-suu-1/2(1 - u)D/2-3/2 du.
0

By making a substitution u = x2, du = 2xdx, we consequently get

1

D (s)

=

2

·

(D/2) (1/2)(D/2-1/2)

e-sx2 (1 - x2)(D-3)/2 dx

0

1

=

(D/2) (1/2)(D/2-1/2)

e-sx2 (1 - x2)(D-3)/2 dx.

-1

(8)

Figure 5: Comparison of D value (red line) with the approximation given by equation 9 (green line) in the case of dimensions D = 2, 5, 20. Observe that for D = 20, the functions practically coincide.

Proposition A.1. For large4 D we have

D (s)



(1

+

4s 2D-3

)-1/2

for

all

s



0.

(9)

Proof. By (8) we have to estimate asymptotics of

D (s)

=

(

D 2

)

(

1 2

)(

D 2

-

1 2

)

1
e-sx2 (1 - x2)(D-3)/2 dx.
-1

Since for large D, for all x  [-1, 1] we have

(1 - x2)(D-3)/2e-sx2  (1 - x2)(D-3)/2 · (1 - x2)s = (1 - x2)s+(D-3)/2,

we get

D(s)  ( D2-( D12)) ·

1
(1 - x2)s+(D-3)/2 dx
-1

=

( D2-( D12))

·

 

(s

+

D 2

(s +

-

1 2

D 2

)

)

.

To simplify the above we apply the formula (1) from (Tricomi & Erdélyi, 1951):

(z + ) = z-(1 + ( - )( +  - 1) + O(|z|-2)),

(z + )

2z

with ,  fixed so that  +  = 1 (so only the error term of order O(|z|-2) remains), and get

(

D 2

)

(

D-1 2

)

=

((

D 2

((

D 2

- -

3 4

)

+

3 4

)

+

3 4

)

1 4

)



D-3 24

1 2

and

(s

+

D 2

(s +

-

1 2

)

D 2

)



D

3

-

1 2

s+ -

.

24

(10)

Summarizing,

D(s) 

(

D 2

-

3 4

)1/2

(s

+

D 2

-

3 4

)1/2

= (1 +

4s 2D-3

)-1/2.

4In practice we can take D  20.

10

Under review as a conference paper at ICLR 2019

In general one can obtain the iterative direct formulas for function D with the use of erf and modified Bessel functions of the first kind I0 and I1, but for large D they are of little numerical value. We consider here only the special case D = 2 since it is used in the paper for illustrative reasons in the

latent for the MNIST data set. Since we have the equality (Gradshteyn & Ryzhik, 2015, (8.406.3)

and (9.215.3)):

2(s)

=

1F1(

1 2

,

1,

-s)

=

e-

s 2

I0

s 2

,

to practically implement 2 we apply the approximation of I0 from (Abramowitz & Stegun, 1964, page 378) given in the following remark.

Remark A.1. Let s  0 be arbitrary and let t = s/7.5. Then

2(s) 

 

e-

s 2

·

(1+3.5156229t2 +3.0899424t4 +1.2067492t6 +.2659732t8 +.0360768t10 +.0045813t12)





  

for s  [0, 7.5],







2 s

·

(.39894228+.01328592t-1 +.00225319t-2 -.00157565t-3 +.0091628t-4 -.02057706t-5

   

+.02635537t-6 -.01647633t-7 +.00392377t-8)



 

for s  7.5.

B COMPARISON OF LEARNING TIMES
Figure 6 gives comparison of mean learning time for different most frequently used batch-sizes. Time spent on processing a batch is actually smaller for CWAE for a practical range of batch-sizes [32, 512]. For batch-sizes larger than 1024, CWAE is slower due to its quadratic complexity with respect to the batch-size. However, we note batch-sizes larger even than 512 are relatively rarely used in practice for training autoencoders.

Figure 6: Comparison of mean batch learning time (times are in log-scale) for different algorithms in seconds, all for the same architecture like the one in Tolstikhin et al. (2017) and all requiring similar number of epochs to train the full model. This times may differ for computer architectures with more/less memory on a GPU card.

AE

VAE

10 1

WAE SWAE

CWAE

Average batch processing time

10 2 32

64 128 Batc2h56size 512 1024 2048

C ARCHITECTURE DETAILS
MNIST (28 × 28 images) an encoder-decoder feedforward architecture: encoder three feed-forward ReLU layers, 200 neurons each
latent 20-dimensional, decoder three feed-forward ReLU layers, 200 neurons each. CelebA (with images centered and cropped to 64×64 with 3 color layers) a convolution-deconvolution network:
11

Under review as a conference paper at ICLR 2019

encoder
latent decoder

four convolution layers with 4 × 4 filters and 2 × 2 strides (consecutively 32, 32, 64, and 64 output channels), all ReLU activations, two dense layers (1024 and 256 ReLU neurons)
64-dimensional,
first two dense layers (256 and 1024 ReLU neurons), three transposed-convolution layers with 4 × 4 filters with 2 × 2 strides (consecutively 64, 32, 32 channels) with ReLU activation, transposed-convolution 4 × 4 with 2 × 2 stride, 3 channels, and sigmoid activation.

CIFAR-10 dataset (32× images with three color layers): a convolution-deconvolution network

encoder
latent decoder

four convolution layers with 2 × 2 filters, the second one with 2 × 2 strides, other non-strided (3, 32, 32, and 32 channels) with ReLU activation, 128 ReLU neurons dense layer,
with 64 neurons,
two dense ReLU layers with 128 and 8192 neurons, two transposed-convolution layers with 2 × 2 filters (32 and 32 channels) and ReLU activation, a transposed convolution layer with 3 × 3 filter and 2 × 2 strides (32 channels) and ReLU activation, a transposed convolution layer with 2 × 2 filter (3 channels) and sigmoid activation.

The last layer returns the reconstructed image. The results for all above architectures are given in Table 2. All networks were trained with the Adam optimizer (Kingma & Ba, 2014). The parameters used were learning rate = 0.001, 1 = 0.9, 2 = 0.999, = 1e - 8. MNIST models were trained for 500 epochs, both CIFAR-10 and CelebA for 200.
Additionally, to have a direct comparison to WAE-MMD model on CelebA, an identical architecture was used as that in Tolstikhin et al. (2017) utilized for the WAE-MMD model (WAE-GAN architecture is, naturally, different):

encoder
latent decoder

four convolution layers with 5 × 5 filters, each layer followed by a batch normalization (consecutively 128, 256, 512, and 1024 channels) and ReLU activation,
64-dimensional,
dense 1024 neuron layer, three transposed-convolution layers with 5 × 5 filters, and each layer followed by a batch normalization with ReLU activation (consecutively 512, 256, and 128 channels), transposed-convolution layer with 5 × 5 filter and 3 channels, clipped output value.

The results for this architecture for CWAE compared to VAE and WAE-MMD models are given in Table 1.
Similarly to Tolstikhin et al. (2017), models were trained using Adam with for 55 epochs, with the same optimizer parameters.

D ADDITIONAL FIGURES

12

Under review as a conference paper at ICLR 2019

Test interpolation

Test reconstruction

Random sample

WAE-MMD VAE

SWAE

CWAE

Figure 7: Results of VAE, WAE-MMD, SWAE, and CWAE models trained on CelebA dataset using the WAE architecture from Tolstikhin et al. (2017). In "test reconstructions" odd rows correspond to the real test points.
13

