Under review as a conference paper at ICLR 2019
CONVERGENT REINFORCEMENT LEARNING WITH FUNCTION APPROXIMATION: A BILEVEL OPTIMIZATION PERSPECTIVE
Anonymous authors Paper under double-blind review
ABSTRACT
We study reinforcement learning algorithms with nonlinear function approximation in the online setting. By formulating both the problems of value function estimation and policy learning as bilevel optimization problems, we propose online Q-learning and actor-critic algorithms for these two problems respectively. Our algorithms are gradient-based methods and thus are computationally efficient. Moreover, by approximating the iterates using differential equations, we establish convergence guarantees for the proposed algorithms. Thorough numerical experiments are conducted to back up our theory.
1 INTRODUCTION
In reinforcement learning (Sutton & Barto, 1998), the agent aims to make optimal decisions by interacting with the environment and learning from the experiences, where the environment is modeled as a Markov Decision Process (MDP). With the recent advancement of deep learning, reinforcement learning has achieved extraordinary empirical success in solving complicated decision making problems, such as the game of Go (Silver et al., 2016; 2017), navigation (Banino et al., 2018), and dialogue systems (Li et al., 2016).
Despite its great empirical success, there exists a gap between the theory and practice of reinforcement learning. Specifically, in terms of theory, most existing works focus on either the tabular case or the case with linear function approximation. In the former case, both the state and action spaces are finite, while in the latter the value function is assumed to be linear in a given feature mapping. Using tools for convex optimization and linear regression, the statistical and computational properties of the reinforcement learning algorithms are well-understood under these restrictive settings. Moreover, when it comes to nonlinear function approximation, theoretical analysis becomes intractable as it involves solving a highly nonconvex statistical optimization problems. Moreover, it is shown in Tsitsiklis & Van Roy (1997) that simple method such as TD(0) might fail to converge when nonlinear value functions are applied. However, using deep learning techniques, methods such as deep Q-network (DQN) (Mnih et al., 2015) and asynchronous advantage actor-critic (A3C) (Mnih et al., 2016) become baseline algorithms for artificial intelligence for practical applications.
To bridge such a gap in DRL, we make the first attempt to study the convergence of online reinforcement learning algorithms with nonlinear function approximation in general. In particular, we propose online Q-learning (Watkins & Dayan, 1992) and actor-critic algorithms with two-timescale updates Konda & Tsitsiklis (2000), i.e., the iterative algorithm consists of two parameters that updates with different learning rates. Although the statistical and computational properties of these methods with linear function approximation are well-studied, convergence of reinforcement learning methods with nonlinear function approximation is fundamentally more challenging due to the lack of convexity. In this case, we can only expect convergence to local minima or stationary points.
Furthermore, we bring both the problems of value function and policy learning under the unified framework of bilevel optimization, which motivates two-timescale updating rules in our algorithms.
1

Under review as a conference paper at ICLR 2019
Specifically, bilevel optimization problems consist of two subproblems that are intertwined with each other. Such phenomenon appears in both Q-learning and actor-critic algorithms. For Q-learning with function approximation, the updates of the target function (also known as target network for DQN) and the value function are coupled together; the same is the updates of the policy and value function in the actor-critic algorithm. Since the two variables in bilevel optimization have different roles, one servers as the constraint of the other, two-timescale updates appears naturally for these problems. In particular, the parameter of the lower optimization problem needs to be updated using a larger stepsize. In addition, as a byproduct, from the view of bilevel optimization, the target network in DQN appears to be the parameter of the upper level optimization, which justifies the trick of using a target network in DQN.
The analysis of our algorithms utilizes stochastic approximation (Borkar, 2008; Kushner & Yin, 2003), which approximates the asymptotic behavior of the iterates using ordinary differential equations. Specifically, under certain assumptions, we show that the online Q-learning algorithm converges almost surely to a local minimizer of the mean squared Bellman error. In addition, for the actor-critic algorithm, we show that the limit points of the actor and critic updates can be characterized by a set of equations.
Our contributions are three-fold. First, we unify the problems of value function estimation and policy learning using the framework of bilevel optimization, which leads to the online Q-learning and actor-critic algorithms using two-timescale updates. Second, using stochastic approximation, the convergence of the proposed algorithms are established with nonlinear function approximation. Third, as a byproduct, the formulation of Q-learning via bilevel optimization justifies the techniques of target network used in DQN, which can be viewed as the parameter of the upper level optimization subproblem.
Related Work. Our work is closely related to the literature on the convergence of online reinforcement learning algorithms with function approximation. For policy evaluation, most existing works focus on algorithms with linear function approximation. Tsitsiklis & Van Roy (1997) study the convergence of the on-policy TD() algorithm based on temporal-difference (TD) error. To handle off-policy sampling, Maei et al. (2010); Sutton et al. (2009; 2016); Yu (2015); Hallak & Mannor (2017) propose various TD-learning methods with convergence guarantees. Utilizing two-timescale stochastic approximation in Borkar (2008) and strong convexity, they establish global convergence results for the proposed methods. The finite-sample analysis of these methods are recently established in Dalal et al. (2017b;a). Moreover, using a duality formulation, Liu et al. (2015); Du et al. (2017) study the finite-sample performance of TD-learning algorithms with primal-dual updates. As for nonlinear function approximation, to the best of our knowledge, the only convergent algorithm is the nonlinear-GTD algorithm proposed in Bhatnagar et al. (2009a), which is more pertinent to our results. Their analysis also depends on two-timescale stochastic approximation, where the variable in the faster timescale essentially solves a linear equation, thus converges to the global optimum. However, in our problem, both the two variables solves nonconvex problems, which adds more challenge in the analysis. In addition, we mainly consider the problem of Q-learning and actor-critic, whereas their results only focus on policy evaluation. Moreover, their algorithm involves the Hessian of the value function and their theory requires the value function to be three times differentiable, whereas our algorithm only requires the gradient of the value function with less stringent assumptions. Thus, our results are not directly comparable with this work.
Furthermore, for Q-learning and the actor-critic algorithm, existing results all consider linear function approximation. For Q-learning, Melo et al. (2008); Prashanth et al. (2014) study the convergence using stochastic approximation. In addition, the actor-critic algorithm is introduced in Sutton et al. (2000); Konda & Tsitsiklis (2000), which also study its convergence with linear function approximation. Later, Kakade (2002); Peters & Schaal (2008) propose the natural actor-critic algorithm which updates the policy function using natural gradient descent (Amari, 1998). Convergence of actor-critic and natural actor-critic algorithms with linear function approximation are studied in Bhatnagar et al. (2009b; 2008); Castro & Meir (2010); Bhatnagar (2010); Maei (2018).
2

Under review as a conference paper at ICLR 2019

2 NOTATION AND BACKGROUND

In this section, we first lay out the notation that will be followed throughout this paper and then introduce some background knowledge on reinforcement learning.
We stick to the following notation. For a measurable space with domain S, we denote by B(S) the set of bounded measurable functions on S. Let Lip(S, L) be the set of Lipschitz continuous functions on S with Lipschitz constant L. Let P(S) be the set of all probability measures over S. For any   M(S) and any measurable function f : S  R, we denote by f ,p the p-norm of f with respect to measure . For an integer k, we use k to denote the probability simplex in Rk, and let [k] = {1, 2, ..., k}.
In reinforcement learning, the problem is modeled by a Markov decision process. A discounted MDP is defined by a tuple (S, A, P, R, p0, ), where S is the set of states, A is the set of all possible actions, P : S × A  P(S) is the Markov transition kernel, R : S × A  R is the reward function, p0  P(S) is the distribution of the initial state s0, and   (0, 1) is the discount factor. At any state s  S, suppose action a  A is executed, we use P (· |s, a)  P(S) to denote the probability distribution of the next state, and let R(s, a) be the the immediate reward. For regularity, we assume that the rewards are uniformly bounded by Rmax. By taking action at at state st for each time step t  0, the agent receives the discounted cumulative reward R = t0 t · R(st, at).
Moreover, a policy  : S  P(A) is a action selection rule. Specifically, (a | s) is the probability of selecting action a at state s. We define the state value function and the action value function of policy  as V (s) = E(R | s0 = s, ) and Q(s, a) = E(R | s0 = s, a0 = a), respectively. Here we use E to indicate that the actions {at}t1 follow policy . We define Bellman operator of T  : B(S)  B(S) by T V (s) = E[r(st, at) +  · V (st+1) | st = s], where at  (· | st). By definition, we have V  = T V  for any policy . Since T  is -contractive, V  is the unique fixed point of T . The problem of estimating V  for a fixed policy  is called policy evaluation.
Furthermore, in reinforcement learning, the goal is to obtain the policy that yields the maximal cumulative reward in expectation. To this end, we define the optimal value function Q : S ×A  R by Q(s, a) = sup Q(s, a), (s, a)  S × A, where the supremum is taken over all possible policies. Then it is known that the greedy policy with respect to Q is the optimal policy. To estimate Q with accuracy, most existing methods utilize the fact that Q is the unique fixed point of the Bellman optimality operator T , which is defined by

(T Q)(s, a) =r(s, a) +  · E max Q(st+1, a) st = s, at = a .
aA

(2.1)

The most well-known method for estimating Q is the Q-learning algorithm (Watkins & Dayan, 1992), based upon a large family of algorithms are derived.
In addition to methods that focus on estimating the value functions, another class of algorithms in reinforcement learning directly search for a good policy over a parametrized policy class { :   Rp} directly, where  is the parameter. Specifically, the goal is to maximize J() = E (R) = Es0p0 [V  (s0)]. The basis of these methods are provided by the policy gradient theorem (Baxter & Bartlett, 2000), which states that

(1 - ) · J () = Es,a(· | s)[ log (a | s) · Q (s, a)] = Es,a(· | s)  log (a | s) · [R(s, a) +  · V  (s ) - V  (s)] ,

(2.2)

where s  P (· | s, a) is the next state of the MDP given (s, a)  S × A, and   P(S) is the state occupancy measure of , i.e., (s) = (1 - ) · t0 t · P (st = s). We note that (2.2) brings the value function and the policy together. By updating  using (2.2) with the value function Q or V  estimated using temporal difference learning methods, we obtain the actor-critic algorithms
(Konda & Tsitsiklis, 2000).

3

Under review as a conference paper at ICLR 2019

3 REINFORCEMENT LEARNING AS BILEVEL OPTIMIZATION

As we introduced in the previous section, estimation of the value functions and policy learning are crucial problems of reinforcement learning. In this section, we show that both these two problems can be cast into bilevel optimization. From this view, we establish the Q-learning and actor-critic algorithms with two-timescale updates.

In the unconstrained form, bilevel optimization is formulated as an optimization problem that contain

another optimization problem as a constraint. Specifically, it can be written as

x = argmin F [x, y(x)],
xX

where y(x) = argmin G(x, y),
yY

(3.1)

where F, G : X ×Y  R are two differentiable functions. Here the challenges in (3.1) are threefold. First, to evaluate the objective function F [x, y(x)], one needs to achieve the global minimizer of

G(x, y) for each x, which might not be feasible when G(x, y) is not a convex function of y. Second, even if we are able to obtain y(x) and evaluate F [x, y(x)] for any x  X , since it is impossible to compute the gradient of y(·), minimize F [x, y(x)] via gradient-based methods is impossible. In

practice, we can only hope for obtaining (x , y )  X × Y satisfying

y = y(x ) and x = argmin F (x, y) | y=y .
xX

(3.2)

Third, in machine learning applications, usually both F and G in (3.1) are computed using large-

scale datasets. In this situation, it can be highly costly to solve the lower level optimization problem

to the exact minimizer. Thus, efficient algorithms have to be robust to the error incurred in solving for y(·).

An efficient algorithm that achieves the condition in (3.2) is two-timescale gradient descent, where we simultaneously perform gradient update with respect to both x and y. Specifically, we have

xt+1  xt - t · xF (xt, yt), yt+1  yt - t · yG(xt, yt),

(3.3)

where the stepsizes {t, t}t0 satisfy the following two-timescale assumption. Assumption 3.1. We assume that the stepsizes {t, t}t0 satisfy

t = t = ,
t0 t0

t2 + t2 < ,
t0

lim
t

t/t

=

0.

(3.4)

That is, in addition to the standard Robbins-Monro condition (Robbins et al., 1951), we have t/t  0, which indicates that {yt}t0 updates in a faster pace than {xt}t0. This condition gives the name of two-timescale update, and plays a pivotal role in the theoretical analysis.

More rigorously, measured in the faster timescale using {t}t0, the updates in (3.3) converge to the asymptotically stable equilibria of the ODE system {x = 0, y = -yG(x, y)}. Thus, {xt, yt}t0 converges to some (x , y ) where y is a local minimizer of G(x , ·). When G(x , ·) has a unique minimizer, we have y = y(x ). Thus, under the faster timescale, the updates in (3.3) essentially fix the x variable and let {yt}t0 converge to y(x) using gradient descent.
Furthermore, to determine the convergence of {xt}t0, we need to look into the slower timescale. Since {yt}t0 updates in a faster speed under Assumption 3.1, we could safely assume that {yt}t0 already converge to y(x) for some x  X . In this scenario, the asymptotic behavior of {xt}t0 is captured by the ODE x = -xF (x, y) | y=y(x). Suppose minx F (x, y) has a unique minimizer, denoted by x(y), then {xt, yt}t0 converges to some (x , y ) satisfying x = x(y ), y = y(x ). This implies that the two-timescale gradient update in (3.3) achieves the condition in (3.2) asymptotically. Moreover, when the minimizers of F (·, y) and G(x, ·) are not unique, using similar analysis,
it can be shown that x is a local minimizer of minx F (x, y ), and simultaneously y is a local minimizer of miny G(x , y).

In the rest of this section, we apply the above argument to problems in reinforcement learning. In particular, we formulate both value function estimation and policy learning as bilevel optimization problems, which naturally motivates us to propose two-timescale algorithms with convergence guarantees.

4

Under review as a conference paper at ICLR 2019

3.1 VALUE FUNCTION ESTIMATION

In value function estimation, the goal is to estimate either V  or Q defined in §2 using function approximation. Note that V  and Q are the unique fixed points of Bellman operators T  and T , respectively. Here we mainly focus on Q, the results for V  can be similarly obtained by replacing T  in (2.1) by T .

Let F = {Q : S × A  Rd,   Rd} be a parametrized function class, where  is the parameter. Our goal is to find Q such that the mean-squared Bellman error (MSBE)

() =

Q - T Q

2 

=

E(s,a)

Q(s, a)

- R(s, a) -



·

E

max
a A

Q

(s

,a

)

s, a

2

(3.5)

is minimized, where   P(S × A) is probability distribution, and s  P (· | s, a) is the next state of the MDP. Note that Q-learning is an off-policy method. To learn Q, it is common to obtain

sample from the MDP using a behavioral policy b, which induces a Markov chain on S × A. Then  can be viewed as the stationary distribution of this Markov chain. Furthermore, for simplicity, we

assume that drawing i.i.d. samples from  is possible, which approximately holds when the Markov

chain enjoys rapid mixing properties.

Moreover, notice that in (3.5), the conditional expectation is inside the quadratic function, which is

known to cause the issue of "double sampling" (Baird et al., 1995). That is, to obtain an unbiased

estimate of the MSBE, given (s, a)  , we need two conditionally independent samples from

P(· | s, a), which cannot be satisfied in practice. To avoid this issue, we write MSBE minimization

as a bilevel optimization problem

 = min


 - ()

2 2

subject to () = argmin

L(, ) =

Q - T Q

2 

.

(3.6)

Rd

We note that if there exists a  such that Q = Q, then we have () = , which implies that (, ) = (, ) is the solution of (3.6). Intuitively, with nonlinear function approximation, the

lower level optimization problem min L(, ) solves a nonlinear least square regression problem using function class F , where T Q can be viewed as the response variable. After obtaining (),
then for the upper level optimization problem, we directly replace by (). Thus, in the batch

setting, solving the bilevel optimization in (3.6) yields the fitted Q-iteration algorithm (Riedmiller,

2005; Munos & Szepesva´ri, 2008). Specifically, When deep neural networks are used for function

approximation, we recover the well-known deep Q-network (DQN) (Mnih et al., 2015), where Q in (3.6) is the target network. We note that the usage of the target network is a critical trick in

DQN, whose mechanism is not fully understood. Therefore, the target network in DQN can be

viewed as the parameter of the upper level optimization of the bilevel formulation in (3.6). From

the perspective of bilevel optimization, it is natural that the target network needs to be updated in a

slower rate, which justifies the empirical practice of DQN that the target network is updated every

Ttarget iterations while the Q-network is updated in each iteration.

To obtain an online algorithm, we solve (3.6) via two-timescale gradient update given in (3.3). The update rules are given by t+1  (1 - t) · t + t · t, and t+1  t - t · L(t, t). In addition, we note that L(, ) has an unbiased estimate
[Q(s, a) - R(s, a) -  · max Q(s , a )] · Q(s, a).
a A
Thus, replacing L(, ) by its estimate, we obtain our online Q-learning algorithm, which is stated in Algorithm 1. Here in line 6 we project the iterate to a compact set X = {x  Rd : x 2  RX } for stability, where RX is a sufficiently large constant. It is clear that our algorithm is free from the double sampling issue. Moreover, as we see in §4, this algorithm converges almost surely to , (), where () is a local minimizer of min L(, ), and  is close to () up to some error caused by projection.

Finally, we note that Algorithm 1 can be similarly applied to policy evaluation, where we estimate V  using function class {V : S  R,   Rd}. In this case, by the definition of T , we only need
to replace t in line 5 by t = Vt (st) - R(st, at) -  · Vt (st). Moreover, here  is set to be the stationary distribution induced by policy . We defer the algorithm for policy evaluation to §A.

5

Under review as a conference paper at ICLR 2019

Algorithm 1 Online Q-learning with nonlinear function approximation.
1: Input: Initialization 0 = 0  , stepsizes {t, t}t0. 2: Output: The sequences {t}t0 and {t}t0 . 3: for t = 0, 1, 2, . . . do 4: Sample (st, at)  , observe reward R(st, at) and next state st. 5: Compute the TD-error t = Qt (st, at) - R(st, at) -  · maxaA Qt (st, a). 6: Update the parameters by

t+1 = X [(1 - t) · t + t · t], 7: end for

t+1 = t - t · t · Qt (st, at).

(3.7)

3.2 THE ACTOR-CRITIC ALGORITHM

Now we formulate policy learning as bilevel optimization and obtain the actor-critic algorithm. Let

{ :   Rp} be the family of policies and let {V :   Rd} be a parametrized family of value functions. Note that the objective in policy learning can be written as J() = Es0p0 [V  (s0)].
We define F (, ) = Es0p0 [V(s0)] and consider the bilevel optimization problem

max F [(), ]


subject to

() = argmin G(, ) =

V - T  V

2 

,



(3.8)

where  is the state-occupancy measure of policy . To see the correctness of such a formulation, notice that if V   {V,   Rd}, then the solution of the lower level optimization problem () satisfies that V() = V  , which leads to F [(), ] = J() for all   Rp. Thus, we recover the

original policy learning problem.

Note that min G(, ) is an policy evaluation problem. Directly solving this problem suffers the issue of double sampling. To resolve this issue, as stated above, one need to formulate itself as a bilevel optimization problem. This will result in a three-timescale algorithm for policy learning. However, due to its complexity, instead we consider the TD(0) method, which is a semi-gradient method for policy evaluation (Sutton & Barto, 1998). Specifically, we define

g(, ) = Es,a(· | s) [R(s, a) +  · V)(s ) - V(s)] · V(s) ,

(3.9)

which can be viewed as a biased estimate of -G(, ). Similar to (3.3), we approximately solve (3.8) via t+1  t + F (t, t), and t+1  t + t · g(t, t). Moreover, by policy gradient

theorem, we have

F (, ) = Es,a(· | s) [R(s, a) +  · V)(s ) - V(s)] ·  log (a | s) . (3.10)
Thus, replacing (3.9) and (3.10) by their sample-based counterparts, we recover the actor-critic algorithm with TD(0) updates,whose details are deferred to §A. Moreover, for algorithmic stability, we project {t}t0 to  = {y  Rd : y 2  R} with R sufficiently large.

Furthermore, we note that TD(0) with nonlinear function approximation can diverge in some situations (Tsitsiklis & Van Roy, 1997). In the next section, we given sufficient conditions for the convergence of actor-critic with nonlinear function approximation.

4 MAIN RESULTS
In this section, we establish the convergence results for the algorithm presented in §3. We first consider the online Q-learning algorithm with nonlinear function approximation. Theoretical results for policy evaluation can be similarly obtained, which are omitted for simplicity. We make the following assumption for the online Q-learning. Assumption 4.1. For the online Q-learning algorithm, we assume the following conditions holds.
(i). For any (s, a)  S × A and any   Rd, we have |Q(s, a)|  Qmax, Q(s, a) 2  Gmax, and Q(s, a) is Lipschitz continuous in . Here Qmax  Rmax/(1 - ) and Gmax are two positive constants.

6

Under review as a conference paper at ICLR 2019
(ii). Let L(, ) be defined in (3.6). For any   X , we assume that any local minimizer  of min L(, ) satisfies that 2L(, ) 0.
Here condition (i) in Assumption 4.1 requires that Q(s, a), as a function of , satisfies certain smoothness condition. Condition (ii) postulates that any local minimizer of L() = L(, ) can be determined using second order optimality condition. This assumption is strictly weaker than the strict saddle property (Ge et al., 2015), which is known to hold for a family of nonconvex functions. Theorem 4.2. Under Assumptions 3.1 and 4.1, if supt0 t 2 < , then {t, t}t0 converges almost surely to some (, ), where   X and  is a local minimizer of the function L(·, ). In addition, if  2  RX , we have  = . Whereas if  2 = RX , we have  =  ·  for some   1.
To see the intuition of Theorem 4.2, consider the two-timescale gradient updates for the population problem given in (3.6). In the faster timescale, we could fix {t}t0 at . Then {t}t0 converges to a local minimizer of L(·, ), which is denoted by (). For the slower timescale, due to the projection X in (3.7), the dynamics of {t}t0 is characterized by a projected ODE  = () -  + , where (t) is the additional term introduced by projection. Consider any asymptotically stable equilibrium  of this projected ODE. If  is in the interior of X , then projection is not activated, and we have () = . If  is on the boundary of X , since X is a Euclidean ball, () -  must be in the same direction as , which implies that () =  ·  with   1.
In the following, we lay out the assumption for the actor-critic algorithm. Assumption 4.3. For the actor-critic algorithm, we assume that the following conditions holds.
(i). For any s  S and any   Rd, we assume that |V(s)|  Qmax, V(s) 2  Gmax, and V(s) is Lipschitz continuous. In addition, there exists a constant max > 0 such that  log (a | s) 2  max for all (s, a)  S × A and   .
(ii). For function g(, ) defined in (3.9), we assume that for each   , the ODE (t) = g[(t), ] has a local asymptotically stable equilibrium (). Moreover, (·) is Lipschitz continuous in a neighborhood of .
The first condition in Assumption 4.3 is parallel to that in Assumption 4.1. This condition ensures that both the value function and the policy function are regular. More importantly, as shown in §C.2, under the faster timescale, we could fix {t}t0 at some   , and {t}t0 converges to a stable equilibrium of ODE  = g(, ). Thus, condition (ii) ensures that this equilibrium, as a function of , is locally Lipschitz. We note that condition (ii) is more restrictive compared with the second condition in the previous assumption. The reason is that TD(0) update for the critic is not a gradient step, i.e., g(, ) is not equal to G(, ). Thus, {t}t0 cannot converge to a local minimizer of G(·, ). Now we present the convergence result for the actor-critic algorithm. Theorem 4.4. Under Assumptions 3.1 and 4.3, if supt0 t 2 < , {t, t} converges to some {(), }, where () is an asymptotically stable equilibrium of ODE  = g(, ). Moreover, if  2 < R, we have F [(), ] = 0. Whereas if  2 = R, then there exists   0 such that F [(), ] =  · .
Similar to Theorem 4.2, to determine the convergence of {t}t0, we consider the slower timescale, under which we have the projected ODE  = F [(), ] + . By studying the equilibrium of this ODE, we obtain the above theorem. An interesting case is that when  2 < R, we have g(, ) = 0 and F (, ) = 0, where  = (). Intuitively, this implies that actor-critic converges to a local Nash equilibrium.
5 NUMERICAL EXPERIMENTS
In this section, we evaluate the performance of our online Q-learning and actor-critic algorithms using OpenAI Gym (Brockman et al., 2016), which is a standard testbed for reinforcement learning.
7

Under review as a conference paper at ICLR 2019

reward reward

Online Q-learning. First, we present the results for Algorithm 1 on Acrobot and MountainCar, which are two classical control tasks. The value functions for Acrobot and MountainCar are neural networks with one and two hidden layers, respectively, where all hidden layers have 64 neurons. To corroborate with our theory, in the experiments, we fix t = 10-3 for all t  0 and let t be various constants. We train the neural networks using minibatch stochastic gradient descent, where the batch size is 32. The results are plotted in Figure 1, where three independent experiments are performed for each setting. Here each curve is the mean value µ of the results in the same setting, together with the shadow representing the interval [µ - /2, µ + /2], where  is the standard deviation. Also, in order to have a better visualization, we use a uniform moving average method with window size 20000 frames to smooth out the curves.
As shown in Figure 1, for both Acrobot and MountainCar, when t < t (green and blue lines), the reward grows gradually and approaches the maximum reward. Whereas if t > t (dark yellow and black lines), the reward finally approaches the minimum. This sharp transition justifies the validity of using two-timescale updates with t t in the online Q-learning.

Acrobot

MountainCar

100 140

150 200 250

tttt====0501e.e.75--44

150 160

300 170

350 180

400 190

tttt====0150..ee75--44

450 200

0.0 0.5 1.0 fra1m.5e 2.0 2.5 1e6

0 1 2 3 fram4e 5 6 17e5

Figure 1: Training Acrobot and MountainCar using online Q-learning. Here we set t = 10-3 for both problems. In addition, for Acrobot, we let t be one of {10-2, 8 · 10-3, 3 · 10-4, 10-4}. For or MountainCar we set t to be one of {0.7, 0.5, 5 · 10-4, 10-4}.
Two-timescale actor-critic algorithm. We also test the actor-critic algorithm on two Atari 2600 games: Pong and Breakout. Here we implement our algorithm based on the A3C framework (Mnih et al., 2016) with 32 asynchronous agents in parallel. In these experiments, we fix stepsize of the actor to t = 10-4 and let the stepsize of the actor t be various constants. Similar to the previous experiment, we plot the results in Figure 2 based on three independent trials, which is deferred to the appendix.
As shown in Figure 2, for those settings with a larger critic stepsize t (green and blue lines), the reward gradually converges to its maximum, while for others with a smaller critic stepsize (dark yellow and black lines), the rewards stay at a very low level. This agrees with our claim in Theorem 4.4 that the critic proceeds in a faster speed than the actor.
6 CONCLUSION
We study online reinforcement learning with nonlinear function approximation in general. Using the unified framework of bilevel optimization, we propose online first-order algorithms for both value function estimation and policy learning. Moreover, using stochastic approximation results, we show that the asymptotic behavior of the algorithms are captured by ordinary differential equations. Finally, we perform thorough empirical studies in support of our theory.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251­ 276, 1998.
Leemon Baird et al. Residual algorithms: Reinforcement learning with function approximation. In International Conference on Machine Learning, pp. 30­37, 1995.
Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based navigation using grid-like representations in artificial agents. Nature, pp. 1, 2018.
Jonathan Baxter and Peter L Bartlett. Direct gradient-based reinforcement learning. In International Symposium on Circuits and Systems, pp. 271­274, 2000.
Shalabh Bhatnagar. An actor­critic algorithm with function approximation for discounted cost constrained Markov Decision Processes. Systems & Control Letters, 59(12):760­766, 2010.
Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton. Incremental natural actor-critic algorithms. In Advances in Neural Information Processing Systems, pp. 105­112, 2008.
Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba Szepesva´ri. Convergent temporal-difference learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems, pp. 1204­1212, 2009a.
Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic algorithms. Automatica, 45(11):2471­2482, 2009b.
Vivek S Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University Press, 2008.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Dotan Di Castro and Ron Meir. A convergent online single-time-scale actor-critic algorithm. Journal of Machine Learning Research, 11(Jan):367­410, 2010.
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Concentration bounds for two timescale stochastic approximation with applications to reinforcement learning. arXiv preprint arXiv:1703.05376, 2017a.
Gal Dalal, Bala´zs Szo¨re´nyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis for td (0) with linear function approximation. arXiv preprint arXiv:1704.01161, 2017b.
Asen L Dontchev and William W Hager. Implicit functions, lipschitz maps, and stability in optimization. Mathematics of Operations Research, 19(3):753­768, 1994.
Joseph L Doob. Stochastic processes, volume 7. Wiley New York, 1953.
Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction methods for policy evaluation. In International Conference on Machine Learning, pp. 1049­1058, 2017.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pp. 797­842, 2015.
Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. arXiv preprint arXiv:1702.07121, 2017.
9

Under review as a conference paper at ICLR 2019
Sham M Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems, pp. 1531­1538, 2002.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information Processing Systems, pp. 1008­1014, 2000.
Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and Applications. Springer, New York, NY, 2003.
Harold Joseph Kushner and Dean S Clark. Stochastic Approximation Methods for Constrained and Unconstrained Systems. Springer Science & Business Media, 1978.
Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample analysis of proximal gradient td algorithms. In Conference on Uncertainty in Artificial Intelligence, pp. 504­513. AUAI Press, 2015.
Hamid Reza Maei. Convergent actor-critic algorithms under off-policy training and function approximation. arXiv preprint arxiv:1802.07842, 2018.
Hamid Reza Maei, Csaba Szepesva´ri, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy learning control with function approximation. In nternational Conference on International Conference on Machine Learning, pp. 719­726, 2010.
Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning with function approximation. In International Conference on Machine Learning, pp. 664­671. ACM, 2008.
Michel Metivier and Pierre Priouret. Applications of a Kushner and Clark lemma to general classes of stochastic algorithms. IEEE Transactions on Information Theory, 30(2):140­151, 1984.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928­1937, 2016.
Re´mi Munos and Csaba Szepesva´ri. Finite-time bounds for fitted value iteration. Journal of Machine Learning Research, 9(May):815­857, 2008.
Jacques Neveu. Discrete-parameter martingales. Elsevier, 1975.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7):1180­1190, 2008.
HL Prasad, LA Prashanth, and Shalabh Bhatnagar. Actor-critic algorithms for learning Nash equilibria in n-player general-sum games. arXiv preprint arXiv:1401.2086, 2014.
LA Prashanth, Abhranil Chatterjee, and Shalabh Bhatnagar. Two timescale convergent q-learning for sleep-scheduling in wireless sensor networks. Wireless networks, 20(8):2589­2604, 2014.
Martin Riedmiller. Neural fitted q iteration­first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning, pp. 317­328. Springer, 2005.
Herbert Robbins, Sutton Monro, et al. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400­407, 1951.
10

Under review as a conference paper at ICLR 2019
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550(7676):354­359, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. Cambridge: MIT press, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, pp. 1057­1063, 2000.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesva´ri, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In International Conference on Machine Learning, pp. 993­1000. ACM, 2009.
Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1): 2603­2631, 2016.
John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation. In Advances in Neural Information Processing Systems, pp. 1075­1081, 1997.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992. Huizhen Yu. On convergence of emphatic temporal-difference learning. In Conference on Learning
Theory, pp. 1724­1751, 2015.
11

Under review as a conference paper at ICLR 2019

A THE POLICY EVALUATION AND ACTOR-CRITIC ALGORITHMS

Algorithm 2 Online policy evaluation with nonlinear function approximation.
1: Input: Initialization 0 = 0  , stepsizes {k}, {k}, and sampling distribution   (S × A).
2: Output: The sequences {t}t0 and {t}t0 . 3: for t = 0, 1, 2, . . . do 4: Sample (st, at)  , observe reward R(st, at) and next state st. 5: Compute the TD error t = Qt - Rt -  · maxaA Qt (St, a). 6: Update the function estimates by

t+1 = [(1 - t) · t + t · t], 7: end for

t+1 = t - t · t · Qt (St, At). (A.1)

Algorithm 3 The actor-critic algorithm with nonlinear function approximation.
1: Input: Initialization 0  Rd and 0  Rp, stepsizes {t, t}t0. 2: Output: The sequences {t}t0 and {t}t0 . 3: for t = 0, 1, 2, . . . do 4: Sample st  t , take action at  t (· | st), observe reward R(st, at) and the next state st.

5: Compute the TD-error t = R(st, at) +  · Vt (st) - Vt (st). 6: Update the parameters by

t+1 = [t + t ·  log t (at | st) · t], 7: end for

t+1 = t + t · t · Vt (st). (A.2)

B ADDITIONAL FIGURES OF NUMERICAL EXPERIMETNS

reward reward

Pong
20

400

350

10 0

tttt====1125eeee----7473

300 250 200 150

10 100

50

20 0

0 1 2 3 frame4 5 6 17e7

0.0

tttt====5121eeee----7347

Breakout

0.2 0.4 frame 0.6

0.8 1e8

Figure 2: Training Pong and Breakout using two timescale actor-critic algorithm, where we use 32 agents, minibatch of size 20, with a fixed actor network stepsize t  10-4, but a varied critic network stepsize t  {10-7, 2 · 10-7, 5 · 10-4, 10-3}. Left: training Pong. Right: training
Breakout.

C PROOFS OF THE MAIN RESULTS
In this section, we lay out the proofs of Theorems 4.2 and 4.4. 12

Under review as a conference paper at ICLR 2019

C.1 PROOF OF THEOREM 4.2

Proof. Our proof consists of two steps. The first step consists of the analysis under the faster timescale, where we show that in this case we could fix {t}t0 at some   X and only focus on {t}t0. In the second step, we look into the slower timescale, which yields the convergence result for {t}t0 and completes the proof.
Step 1. Faster timescale. We first consider the convergence of {t, t}t1 in the faster timescale. Using stochastic approximation, we will show that the sequence {t}t0 defined in (3.7) tracks a local minimizer of the lower level optimization.

To begin with, for notational simplicity, we define

t = Qt (st, at) - r(st, at) -  · Qt (st, at), t = Qt (st, at),
for all t  0, where at = argmaxaA Qt (st, a). Then the updating rules in Algorithms 1 reduce to

t+1  X t + t · (t - t) , t+1  t - t · t · t,

(C.1)

where {t, t}t0 are the stepsizes, and X is the projection operator onto set X , which is assumed
to be a closed Euclidean ball with radius R. In addition, let Ft(1) = ({ ,  } t) be the -algebra generated by the parameter iterates until time t. By definition, t and t are Ft(1)-measurable for any t  1. Furthermore, for any   Rd and   Rp, we define

h(, ) = E(s,a) [Q(s, a) - (T Q)(s, a)] · Q(s, a) ,

(C.2)

Thus, by the definition of the Bellman optimality operator in , for any t  0, we have
E[tt | Ft(1)] = h(t, t). This implies that {t}t0 defined by t = h(t, t) - t · t is a martingale difference sequence with respect to the filtration {Ft(1)}t0.

Moreover, for any compact set X  Rd, we denote by CX (x) the outer normal cone of X at x  X . That is, CX (x) = {u  Rd : u (y - x)  0, y  X }. In particular, if x is in the interior of X , then CX (x) is empty. Moreover, since we assume X is an Euclidean ball with radius RX , for any x in the boundary of X , denoted by X , we have CX (x) = { · x :   0}. Using the notion of the
outer normal cone, the first equation in (C.1) can be written as

t+1 = t + t · (t - t + t),

(C.3)

where t  -CX (t+1) is the correction term caused by projection onto X . Since (1 - t) · t  X and t+1 is the projection of t + t · (t - t), we have

t 2 = 1/t · inf t + t · (t - t) - y 2  t 2
yX

(C.4)

for all t  0. Then, writing the two equations in (C.1) together, we have

t+1 t+1

=

t t

+ t ·

-h(t, t) + t t/t · (t - t + t)

.

(C.5)

In the following, we apply the result on ODE approximation to (C.5). First note that under Assumption 4.1, |Q(s, a)|, Q(s, a) 2 are bounded by Qmax and Gmax, respectively. This implies that implies that t · t is bounded by 2Qmax · Gmax for all t  1. Thus, {t}t0 is a bounded martingale sequence. Then there exist an absolute constant C > 0 such that

E[

t

2 2

|

Ft]



C

for all t  1.

Moreover, let MT =

T t=0

t

·

t

for

any

T

 0.

Since

t0 t2 < , by the Martingale

convergence theorem (Proposition VII-2-3(c) on page 149 of Neveu (1975)), {MT }T 0 converges

almost surely. Additionally, MT is square integrable with E

MT

2 2



C

·

t0 t2. Thus, for any

> 0, by Doob's martingale inequality (Doob, 1953), we have

P sup MN - MT 
N T

 supNT E[

MN - MT

2 2

]



2C 2

2

tT 2

t2

,

13

Under review as a conference paper at ICLR 2019

which converges to zero as T goes to infinity. Furthermore, under Assumption 3.1 and the assump-
tion that supt0 t 2 < , it holds that t/t · (t - t + t) = 0 as t goes to infinity. Besides, since both Q(s, a) and Q(s, a) are bounded and Lipschitz continuous under Assumption 4.1,
h(, ) defined in (C.2) is Lipschitz in both  and .

Now we apply Theorem D.2 to sequence {(t, t)}t0, which implies that the asymptotic behavior of {(t, t)}t0 converges almost surely to the set of asymptotically stable equilibria of the ODE

 = -h(, ),  = 0.

(C.6)

Specifically, the set of asymptotically stable equilibria of the ODE in (C.6), denoted by K1, is

K1 = (, ) :   X , h(, ) = 0 .

(C.7)

For any   Rd, we denote L() = L(, ) = Q - T Q 2. Since h(, ) = L (), the asymptotically stable equilibrium of ODE  = -L () is the local minima of L ().

Therefore, for the analysis under the faster timescale, essentially we can fix {t}t0 at some  and
consider solely the asymptotic behavior of {t}t0. In this case, {t}t0 converges almost surely to a local minimizer  of L (·). We note that L (·) may have multiple local minimizers. In
this case, the local minimizer that {t}t0 converges to is determined by the basin of attraction that {t}t0 enters. Moreover, note that under Assumption 4.1, 2L(·) is positive definite at all its local minima, which implies that the set of all local minima of L (·) is a disjoint set.

Consider the equation L () = 0. Note that  =  is a solution. Besides, by the continuity of 2L(·), there exists a open neighborhood U of  and a positive number CL such that

2L () CL · Id

  U.

By Lipschitz implicit function theorem (Dontchev & Hager, 1994), there exist a neighborhood U of , a neighborhood V of , and a mapping  : V  U such that () = . In addition,  is
Lipschitz continuous and satisfies L(, )|=() = 0 for all   V.

Thus, we show that, in the faster timescale, {t, t}t0 converges almost surely to [(), ], where   X and  is Lipschitz continuous.

Step 2. Slower timescale.

Note that (C.7) cannot characterize the convergence of {t}t0. Now we look into the dynamics in the slower timescale for a finer characterization. Recall that the update rule of the {t}t0 is given in (C.3). For ease of presentation, we define -field Ft(2) = ({ } t) for all t  0. b In addition, we define

t = E (t - t) | Ft(2) - [((t) - t] = E t | Ft(2) - (t).

(C.8)

Then (C.3) becomes t+1 = t + t · [((t) - t] + t · (t + t), where t  -CX (t+1).

As we have shown in the first step of the proof, (t) - t converges to zero as t goes to infinity, where the mapping  : Rd  Rd is Lipschitz continuous. In addition, since supt0 t 2 is finite, t defined in (C.8) is uniformly bounded for all t  0.

Now we apply the Kushner-Clark lemma (Kushner & Clark, 1978, see also Theorem E.2 in §E for details.) to sequence {t}t0, it holds that {t}t1 converges almost surely to the set of asymptotically stable equilibria of the ODE

 = () -  + , (t)  -CX ((t)),

(C.9)

where the function  appears due to the projection in (C.11). Recall that X is a Euclidean ball with
radius RX . For any asymptotically stable equilibrium  of (C.9), if  is in the interior of X , we have CX () = 0, which implies that  = (). In this case, sequence {(t, t)}t1 converges almost surely to (, ), which satisfies that

E(s,a) [Q (s, a) - (T Q )(s, a)] · Q (s, a) = 0.

14

Under review as a conference paper at ICLR 2019

Meanwhile, if   X , we have () -   CX (), which implies that there exists  > 0 such that () =  · . Thus, {(t, t)}t1 converges almost surely to ( · , ). Therefore, we
conclude the proof of Theorem 4.2.

C.2 PROOF OF THEOREM 4.4

Proof. The proof is similar to that of Theorem 4.2 in §C.1. We prove the theorem in two steps, considering the faster and slower timescales seperately.

Step 1. Faster timescale. We first consider the convergence of {t, t}t1 in the faster timescale.
Using stochastic approximation, we will show that the critic sequence {t}t0 converges to an asymptotically stable equilibrium of ODE  = g(.) for some   .

First recall that, for any t  0, we have st  t , at  t (· | st), and st  P (· | st, at). To simplify the notation, we define

t = Vt (st) - r(st, at) -  · Vt (st), t = Vt (st), At =  log t (at | st). (C.10) Using these terms, the updating rules in Algorithm 3 become

t+1   t + t · t · At ,

t+1  t + t · t · t,

(C.11)

where {t, t}t0 are the stepsizes, and  is the projection operator onto . In addition, under Assumption 4.3, we have |V(s)|  Vmax and  log (a | s) 2  max. Thus t and At defined in (C.10) satisfy |t|  2Vmax and At  max for all t  1. Combining (C.11) and Assumption 3.1, this implies that t/t · t · At converges to zero as t goes to infinity.

Moreover, let Ft(1) = ({ ,  } t). Recall that we define function g(, ) in (3.9). To simplify the notation, let h(, ) = F (, , which is defined in (3.10). Then by (C.10), we have

E t · t | Ft(1) = g(t, t),

E t · At | Ft(1) = h(t, t),

(C.12)

which implies that {t}t0 is a martingale difference sequence, where we define t = t · t -
g(t, t) for all t  0. Moreover, under Assumption 4.3, we have t · t 2  2Vmax · Gmax, which implies that there exists an absolute constant C such that E[ t 22] < C. Therefore, similar to the proof of Theorem 4.2 in §C.1, by applying Theorem D.2, we obtain that, in the faster timescale,
sequence {t, t}t0 converges almost surely to the asymptotically stable equilibria of ODE system { = g(, ),  = 0}. Under Assumption 4.3, this implies that {t}t0 converges to some , while {t}t0 converges to a local asymptotically stable equilibrium  = (). Moreover,  : Rp  Rp is Lipschitz continuous in a open neighborhood of .

Step 2. Slower timescale. In the sequel, we characterize the convergence of {t, t}t1 under the slower timescale. To begin with, we define Ft(2) = ({ } t) for all t  0. In addition, we define

t(1) = -t · At + E t · At | Ft(2) ,

t(2) = -E[t · At | Ft(2)] + h[t, (t)],

(C.13)

where function g is defined in (3.9), At and t are defined in (C.10). Since t·At 2  2Vmax·Gmax, {t(1)}t1 is a bounded martingale difference sequence. Moreover, since Ft(2)  Ft(1), by the tower property and (C.12), we have

E[t · At | Ft(2)] = E E(t · At | Ft(1)) Ft(2) = E h(t, t) Ft(2) .

Using the notation in (C.13), the primal update in (C.11) can be written as

t+1 =  t + t · h[t, (t)] + t · t(1) + t · t(2) ,

(C.14)

where () is a local asymptotically stable attractor of ODE  = h(, ). Moreover, under Assumption 4.3, V(s) and  log (a | s) are bounded, which implies that

h((1), ) - h((2), )  2Gmax · max

15

Under review as a conference paper at ICLR 2019

for any (1), (2)  Rd and any   . Then, by Cauchy-Schwarz inequality, we have
t(2) 2  E h(t, t) - h[(t), t] 2 Ft(2)  2Gmax · max · E t - (t) 2 Ft(2) , which tends to zero as t goes to infinity. As shown in the first step of the proof, (t)-t converges to zero as t goes to infinity.

Furthermore, we define WT =

T t=1

t

·

t(1)



Rd

for

any

T



1.

Since

t1 t2 <  and t(1)

is bounded, {WT }T 1 is a square-integrable martingale sequence, which converges almost surely

by the Martingale convergence theorem (Neveu, 1975). Moreover, Doob's martingale inequality

(Doob, 1953) implies that

lim P
T 

sup
N T

WN - WT



 lim supNT E[
T 

WN
2

- WT

22]  lim 2C2
T 

tT 2

t2

= 0.

Finally, applying by the Kushner-Clark lemma (Kushner & Clark, 1978) to sequence {t}t0, it holds that {t}t0 converges almost surely to the set of asymptotically stable equilibria of the ODE

 = h[(), ] + , (t)  -C((t)),

(C.15)

where C() is the outer normal cone of  at . Recall that we assume that  is a Euclidean ball with radius R. Thus, for any asymptotically stable equilibrium  of (C.9), if  is in the interior of , i.e.,  < R, we have h[(), ] = 0. Additionally, if  2 = R, then there exists   0 such that h[(), ] =  · . Therefore, we conclude the proof of Theorem 4.4.

D BACKGROUND ON STOCHASTIC APPROXIMATION

We first present a fundamental result for stochastic approximation which is obtained from Borkar (2008). Consider a sequence of iterations in Rd

xt+1 = xt + t · [G(xt) + Mt+1], t  0, x0  Rd.

(D.1)

Here G : Rd  Rd is a deterministic mapping, Mt+1  Rd is a random vector, and t > 0 is the stepsize.

Assumption D.1. We make the following assumption on the iteration (D.1).

· Function G : Rd  Rd is Lipschitz continuous.
· The stepsizes {t}t0 satisfies t0 t =  and t0 t2 < ;
· Random vectors {Mt}t0 is a martingale difference sequence. That is, M0 = 0, E[Mt+1 | x , M ,   t] = 0. Moreover, we assume that there exists some K > 0 such that E Mt+1 2 | x , M ,   t  K · (1 + xt 2).

Then the asymptotic behavior of the {xt}t0 in (D.1) is characterized by ODE

x = G(x)

(D.2)

Suppose Eq. (D.2) has a unique globally asymptotically stable equilibrium x, we then have the following two theorems.
Theorem D.2. Under Assumption D.1, if supt xt 2 <  almost surely, we have xt  x almost surely.

Theorem D.3. Under Assumption D.1, suppose

G(cx)

lim
c

c

= G(x)

exists uniformly on compact sets for some G  C(Rn). If the ODE y = G(y) has origin as the

unique globally asymptotically stable equilibrium, then we have

sup xt <  almost surely.
t

16

Under review as a conference paper at ICLR 2019

E KUSHNER-CLARK LEMMA

We state here the well-known Kushner-Clark Lemma (Kushner & Clark, 1978; Metivier & Priouret, 1984; Prasad et al., 2014) in the sequel.
Let  be an operator that projects a vector onto a compact set X  RN . Define a vector (·) as

[x + h(x)] - x

[h(x)] = lim

,

0<0



for any x  X and with h : X  RN continuous. Consider the following recursion in N dimensions

xt+1 =  xt + t[h(xt) + t + t] .

(E.1)

The ODE associated with (E.1) is given by

x = [h(x)].

(E.2)

Assumption E.1. We make the following assumptions:

· h(·) is a continuous RN -valued function.

· The sequence {t}, t  0 is a bounded random sequence with t  0 almost surely as t  .

· The stepsizes t, t  0 satisfy t  0 as t   and t t = .

· The sequence t, t  0 satisfies for any > 0

n

lim P
t

sup
nt

  
 =t 2

= 0.

Then the Kushner-Clark Lemma says the following.
Theorem E.2. Under Assumption E.1, suppose that the ODE (E.2) has a compact set K as its set of asymptotically stable equilibria. Then xt in (E.1) converges almost surely to K as t  .

17

