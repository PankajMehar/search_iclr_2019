Under review as a conference paper at ICLR 2019
OFF-POLICY EVALUATION AND LEARNING FROM LOGGED BANDIT FEEDBACK: ERROR REDUCTION VIA SURROGATE POLICY
Anonymous authors Paper under double-blind review
ABSTRACT
When learning from a batch of logged bandit feedback, the discrepancy between the policy to be learned and the off-policy training data imposes statistical and computational challenges. Unlike classical supervised learning and online learning settings, in batch contextual bandit learning, one only has access to a collection of logged feedback from the actions taken by a historical policy, and expect to learn a policy that takes good actions in possibly unseen contexts. Such a batch learning setting is ubiquitous in online and interactive systems, such as ad platforms and recommendation systems. Existing approaches based on inverse propensity weights, such as Inverse Propensity Scoring (IPS) and Policy Optimizer for Exponential Models (POEM), enjoy unbiasedness but often suffer from large mean squared error. In this work, we introduce a new approach named Maximum Likelihood Inverse Propensity Scoring (MLIPS) for batch learning from logged bandit feedback. Instead of using the given historical policy as the proposal in inverse propensity weights, we estimate a maximum likelihood surrogate policy based on the logged action-context pairs, and then use this surrogate policy as the proposal. We prove that MLIPS is asymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared error than IPS. Such an error reduction phenomenon is somewhat surprising as the estimated surrogate policy is less accurate than the given historical policy. Results on multi-label classification problems and a large-scale ad placement dataset demonstrate the empirical effectiveness of MLIPS. Furthermore, the proposed surrogate policy technique is complementary to existing error reduction techniques, and when combined, is able to consistently boost the performance of several widely used approaches.
1 INTRODUCTION
While maintaining online and interactive systems for information retrieval, news recommendation, ad placement, and e-commerce, we collect large batches of logs during testing phases and past deployment periods (Li et al., 2010). Such logs contain rich information that can be used for many purposes. For example, the logs of an e-commerce system record the decisions (or actions) on which ads or recommendations are displayed to a user given a context, and the corresponding feedback (or reward), including whether the user clicks on any of the displayed items and whether a purchase occurs afterwards. Although the logs are collected when the online system is deployed with a historical algorithm (or policy), they are informative for future improvements and design of the system. One application of such logs is to estimate the performance of new policies, also known as off-policy evaluation (Dud´ik et al., 2014; Li et al., 2011; 2014). Furthermore, the use of logs allows for an accurate and more efficient way to test and optimize new policies without expensive trial-and-error cycles in traditional A/B tests, which often take weeks.
Since the logged feedback only provides partial information, policy evaluation and optimization using logs are often formulated as batch contextual bandit learning. Different from the online learning setting, batch contextual bandit learning is statistically more challenging because the collected logs are off-policy, that is, they are generated by a historical policy that differs from the current policy we intend to evaluate or optimize. To bridge this discrepancy, Inverse Propensity Scoring (IPS) has been introduced to construct an unbiased counterfactual estimator of policy performance using off-policy
1

Under review as a conference paper at ICLR 2019

data (Bottou et al., 2013). However, such an estimator tends to have a large mean squared error when the current policy and the historical policy are very different. To reduce the mean squared error, a number of regularization and control variate approaches have been introduced (e.g., Dud´ik et al. (2014); Hirano et al. (2003); Ionides (2008); Li et al. (2015)). Notably, Swaminathan & Joachims (2015a) proposed Policy Optimizer for Exponential Models (POEM), which introduces a mean squared error regularizer based on the counterfactual risk minimization principle. Empirical evaluations also suggested the effectiveness of these approaches on policy evaluation and optimization.
In this paper, we introduce a new yet simple parametric approach for mean squared error reduction in batch contextual bandit learning. Instead of using the given historical policy, we use linear models or neural network models to estimate a maximum likelihood surrogate policy using the logged actioncontext pairs. Then we use such a surrogate policy as the proposal distribution to obtain the inverse propensity weights in the off-policy estimator, as if the logs are generated by this surrogate policy. This idea dates back to the "estimated sampler" technique in statistics (Delyon & Portier, 2016; Henmi et al., 2007). We provide theoretical justification for this approach, which is named Maximum Likelihood Inverse Propensity Scoring (MLIPS). In particular, we show that the proposed MLIPS estimator is asymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared error than the IPS estimator. Such an error reduction effect is counterintuitive as we replace the known historical policy with a less accurate estimated surrogate policy, which is supposed to increase the mean squared error. We evaluate the MLIPS estimator on several multi-label classification problems and a large-scale ad placement dataset to demonstrate its empirical effectiveness on mean squared error reduction for policy evaluation and optimization. Furthermore, we also combine the MLIPS estimator with several existing approaches. We find that our surrogate policy technique is complementary to existing mean squared error reduction techniques and achieves consistently improved performance.

2 OFF-POLICY EVALUATION AND LEARNING FROM LOGGED BANDIT FEEDBACK
In this section, we introduce the background on off-policy evaluation and learning from logged bandit feedback. In particular, we focus on IPS and present several existing mean squared error reduction and regularization approaches.

2.1 INVERSE PROPENSITY SCORING

The value function used to evaluate a target policy  is defined as

V = E[r] = ExEa(· | x)ErD(·|a,x)[r].

(2.1)

Here x is the context drawn from the context distribution , which is independent of , a is the action taken according to  given context x, and r is the reward (or feedback) received.

In the off-policy setting, we are not able to sample different actions from the target policy  and obtain reward, which is expensive and time-consuming in practice. Instead, we collect samples from a logging policy µ, that is, a  µ(· | x), and expect to use the logs to evaluate a target policy and further perform policy optimization.

To bridge the discrepancy between  and µ, the IPS estimator, which is known as the importance

sampling estimator, reweights each logged action-context pair as follows,

VIPS

=

1 n

n i=1

(xi, ai) ·

ri

=

1 n

n i=1

(ai | xi) µ(ai | xi)

· ri.

(2.2)

In the sequel, we use V to denote VIPS for notational simplicity.

It is worth noting that the IPS estimator is an unbiased estimator of V , and thus has been
widely adopted as an objective function for optimizing the target policy . However, (xi, ai) = (ai | xi)/µ(ai | xi) in (2.2) may induce a large mean squared error, especially when the logging policy µ is very different from the target policy . As a result, the IPS estimator often leads to
unsatisfactory policy optimization and poor generalization performance.

2

Under review as a conference paper at ICLR 2019

2.2 MEAN SQUARED ERROR REDUCTION AND REGULARIZATION

To reduce the mean squared error of IPS, several thresholding approaches have been proposed and

studied in the literature (e.g., Bottou et al. (2013); Cortes et al. (2010); Ionides (2008); Strehl et al.

(2010)). One straightforward approach is Propensity Weight Capping (Ionides, 2008), which takes

the form

VI(PMS )

=

1 n

n

min

i=1

M, (ai | xi) µ(ai | xi)

· ri,

(2.3)

where M > 1 is the threshold parameter. Although thresholding reduces the mean squared error

when M is small, it introduces a large bias.

More recently, Swaminathan & Joachims (2015a) proposed the POEM algorithm for batch learning
from bandit feedback. POEM is motivated by the structural risk minimization principle and a
generalization bound that characterizes the mean squared error of VI(PMS ) based on an empirical Bernstein argument. The POEM algorithm jointly optimizes VI(PMS ) and its empirical standard deviation, which is also known as counterfactual risk minimization.

Let ui = min{M, (ai | xi)/µ(ai | xi)} · ri, u¯ =

n i=1

ui /n,

and

Var(VI(PMS ))

=

(

ni=1(ui -

u¯)2)/(n - 1), the POEM algorithm searches for a policy  that maximizes

VI(PMS ) -  Var(VI(PMS ))/n.

(2.4)

Here M > 1 and   0 are the thresholding and regularization parameters, respectively. Also note

that when  = 0, the POEM objective in (2.4) reduces to (2.3).

Motivated by the propensity overfitting problem of IPS and POEM, Swaminathan & Joachims (2015b) proposed to use control variates, which lead to the following self-normalized estimator

VSN =

1/n · 1/n ·

n i=1

(ai

|

xi)/µ(ai

|

xi

)

·

ri

n i=1

(ai

|

xi

)/µ(ai

|

xi)

.

(2.5)

Note that VSN is shifted by a constant C if each ri is shifted to ri + C. In other words, VSN is equivariant (Hesterberg, 1995) and always lies within the range of ri. In contrast, the IPS estimator does not have such properties. Self-Normalized POEM (Norm-POEM), a variant of POEM, was also
proposed by Swaminathan & Joachims (2015b). In particular, Norm-POEM searches for a policy
that maximizes the normalized estimator regularized by its empirical standard deviation

Var(VSN) =

1/n ·

n i=1

(ri

-

VSN)2

·

(ai | xi)/µ(ai | xi)

1/n ·

n i=1

(ai

|

xi

)/µ(ai

|

xi)

2

2
.

(2.6)

In addition, other techniques, such as control variates and doubly-robust estimators Dud´ik et al. (2014) have also been applied to reduce the mean squared error of the off-policy estimator.

3 ERROR REDUCTION VIA SURROGATE POLICY

Instead of directly using the logging policy propensities, we propose to refit a surrogate policy and

use it to obtain the inverse propensity weights in the IPS estimator defined in (2.2). We assume that the logging policy µ(a | x) is parameterized by   Rd and denote it by µ(a | x; ). In particular, we assume the logging policy with  =  generates the logged action-context pairs. Although we have the access to µ(a | x; ), we choose to use the maximum likelihood estimator of  based

on {(ai, xi)}ni=1 as a surrogate policy parameter, which is denoted as , to replace  and obtain a more accurate estimator of the value function V defined in (2.1). In specific, we define the MLIPS

estimator of V as

V

=

1 n

n i=1

(ai | xi) µ(ai | xi; )

· ri,

(3.1)

where {(ai, µ(a | x; ).

xi

)}ni=1

are

the

logged

action-context

pairs,

which

are

independently

sampled

from

3

Under review as a conference paper at ICLR 2019

Unlike the IPS estimator V in (2.2), the MLIPS estimator V in (3.1) is biased in general. However, V is asymptotically unbiased, and moreover, in the following subsection we show that V achieves a smaller mean squared error than V when the logging distribution µ(a | x; ) is properly chosen.

3.1 THEORETICAL ANALYSIS

In this section, for the simplicity of analysis, we assume that the reward r is determined by a and

x, that is, r = r(a, x). For notational simplicity, in the subsequent analysis we use the following

equivalent definitions of V and V based on joint distributions,

V

=

1 n

n i=1

(ai, xi) µ(ai, xi; )

· r(ai, xi),

V

=

1 n

n i=1

(ai, xi) µ(ai, xi; )

· r(ai, xi),

since we have

(a | x)

(a, x)/(x)

(a, x)

µ(a

|

x; )

=

µ(a,

x; )/(x)

=

µ(a,

. x; )

(3.2)

Before we lay out the main theory, we first introduce the following definitions.

Definition 3.1 (Score Function and Fisher Information). The score function S(a, x; )  Rd and the Fisher Information matrix I()  Rd×d are defined as

 log µ(a, x; )

S(a, x; ) =

,



I() = -E

S(a, x; ) 

.

(3.3)

It is worth noting that the reformulation from µ(a | x; ) to µ(a, x; ) does not change the score function or the Fisher information matrix, since the marginal distribution of the context x in (3.2), namely (x), does not depend on , and hence its partial derivative with respect to  is zero.

Definition 3.2 (Deviation Function). The difference between (a, x)/µ(a, x; ) · r(a, x) and the true value V of the target policy, which is defined in (2.1), is denoted by

(a, x) DV (r, a, x; ) = µ(a, x; ) · r(a, x) - V.

(3.4)

In the sequel, we introduce two assumptions and a condition on an event. For the ease of notation, we denote E(a,x)µ(·,·;)[ · ] by E[ · ]. Also, we denote by · the 2-norm of a vector or the spectral norm of a matrix. The next assumption ensures that the Fisher information matrix of the joint logging distribution µ(a, x; ) is well behaved.

Assumption 3.3 (Nonsingular Fisher Information). For the joint distribution µ(·, ·; ), we assume

that its Fisher information matrix satisfies

I -1 (  )

=

S(a, x; ) E 

-1
= O(1).

(3.5)

The next assumption ensures the score and deviation functions are sufficiently smooth.

Assumption 3.4 (Smoothness of Score and Deviation). For the deviation function DV (r, a, x; ) defined in (3.4), we assume that

DV (r, a, x; ) E 

= O(DG),

2DV (r, a, x; ) E 2

= O(DH),

(3.6)

where the second equality holds for any . Meanwhile, for the score function S(a, x; ) defined in (3.3), we assume that for any v  Rd such that v = 1, it holds that

E

2S

(a, x; 2



)

(v,

v,

·)

= O(ST),

(3.7)

where 2S(a, x; )/2 is a tensor consisting of the third-order derivative of log µ(a, x; ) with respect to . Here (2S(a, x; 0)/2)(v1, v2, ·) is the bilinear map Rd × Rd  Rd induced by the tensor, which maps (v1, v2) to a vector in Rd.

Before introducing the condition, we define several events to simplify the presentation. In the following, we first define an event on the estimation error of the maximum likelihood estimator .

4

Under review as a conference paper at ICLR 2019

Definition 3.5 (Maximum Likelihood Estimation Error). For the maximum likelihood estimator  and the true parameter , we define the event E(t) as

E(t) =  -   t .

(3.8)

Next we define several events on the concentration behavior of the score function defined in (3.3) and the deviation function defined in (3.4) as well as their derivatives. Definition 3.6 (Concentration of Score). For the score function with true parameter , we define the following events,

ESG(tSG) =

1 n

n

S(ai, xi; ) - E S(a, x; )

 tSG ,

i=1

(3.9)

ESH(tSH) =

1 n

n

S(ai, xi; 

)

-

E

S(a, x; ) 

i=1

 tSH .

(3.10)

Also, we define

EST(tST) =

sup
,vRd, v =1

1 n

n

2

S

(ai, xi; 2



)

(v,

v,

·)

-

E

i=1



2

S

(a, x; 2

)

(v,

v,

·)

 tST ,

where (2S(a, x; )/2)(v, v, ·) is defined in Assumption 3.4. Then we define the joint event

ES(tS) = ESG(tSG)  ESH(tSH)  EST(tST), where tS = (tSG, tSH, tST). Definition 3.7 (Concentration of Deviation). For deviation function with true parameter , namely DV (r, a, x; ), we define

EDG(tDG) =

1 n

n

DV

(ri, ai, 

xi;

)

-

E

DV (r, a, x; ) 

i=1

 tDG .

(3.11)

Also, we write

EDH(tDH) =

sup
Rd

1 n

n

2DV

(ri, ai, xi; 2

)

-

E

2DV (r, a, x; ) 2

i=1

 tDH .

Then we define the joint event ED(tD) = EDG(tDG)  EDH(tDH), where tD = (tDG, tDH).

We now present a condition on the overall tail behavior of the probability of the events defined above.

Condition 3.8 (Concentration Tail Behavior). Let t = (t, tS, tD) and

E(t ) = E(t)  ES(tS)  ED(tD),

(3.12)

it holds that P(E(t ))  1 - f (t), where t = t  for some decreasing tail function f (t) such that

 0

tk

df

(t)

is

finite

and

goes

to

zero

as

n





for

any

nonnegative

integer

k

and

decreases

along

k. Here f (t) also depends on the dimension d and the sample size n.

Condition 3.8 states that, in addition to the upper bound on the estimation error defined in Definition 3.5, the sample versions of the score function defined in (3.3) and the deviation function defined in (3.4) as well as their derivatives all well concentrate to their population versions. In particular, the condition on the integrability of the tail function f (t) characterizes that the tail probability of concentration decays faster than the polynomial rate. As we will illustrate using the multinomial logistic regression model in §D, such a superpolynomial tail behavior is typically guaranteed by exponential concentration inequalities such as Bernstein-type inequalities and their matrix variants.

The following theorem compares the mean squared errors of the MLIPS and IPS estimators defined in (3.1) and (2.2). Recall that DG, ST, and DH are defined in Assumption 3.4. Since DG, ST, and DH are population quantities, they do not scale with the sample size n.
Theorem 3.9 (Mean Squared Error Reduction). Under Condition 3.8 and Assumptions 3.3 and 3.4,
the MLIPS V is asymptotically unbiased and it holds that

E (V - V )2 = E (V - V )2 - 1/n · Var (r, x, a; ) -(n) ,

(3.13)

where

Reduction of MSE (a, x; ) = E DV (r, a, x; ) · S(a, x; ) · I-1() · S(a, x; ),

(3.14)

5

Under review as a conference paper at ICLR 2019

and (n) = (E[(V 1) + DG · DH) ·

- V )2])1/2 · O((DG · (ST + 1) + DH) · (

 0

t3

df

(t)).

 0

t4

df

(t))1/2)

+

O((D2 G

·

(ST

+

Proof. See §A for a detailed proof.

We interpret Theorem 3.9 as follows. For the simplicity of discussion, for now we assume that the dimension d does not scale with the sample size n. The 1/n · Var((r, x, a; )) term on

the right-hand side of (3.13) characterizes the reduction of the mean squared error. For example,

in the multinomial logistic regression model, the (n) term is roughly of the order 1/n3/2. As a

consequence, for a sufficiently large n, we have

1/n · Var (r, x, a; ) - (n)  1/(2n) · Var (r, x, a; ) .

(3.15)

In other words, the mean squared error of the MLIPS estimator is at least smaller than that of the IPS estimator by a margin of 1/(2n) · Var((r, x, a; )), where Var((r, x, a; )) is a population quantity that does not scale with n. In comparison, the mean squared error is also of the order 1/n in the multinomial logistic regression model. That is to say, the reduction effect does not vanish even when n  . See §D for more discussions and a more detailed illustration of Theorem 3.9 using the multinomial logistic regression model.

3.2 PRACTICAL CONSIDERATIONS
In practice we may not know the class of logging policies that generate the logs. However, we can use universal function approximators such as neural networks to estimate such surrogate policies. In our experiments, we found that neural network policies worked very well on both standard multi-label classification datasets and a large-scale ad placement dataset.
Due to its simplicity, MLIPS can be easily implemented without much extra effort. Furthermore, MLIPS is orthogonal to other existing approaches for mean squared error reduction in policy evaluation and optimization. Hence, it can often be combined straightforwardly with them. In our experiments, we observed that it can be used to further improve POEM for policy optimization.
Another advantage of our approach is that it still works even when we have no access to the logging policy, which often happens in real world due to various reasons. In those cases, IPS may not work at all, but our approach exhibits superior performance, as shown in our experiments.

4 EXPERIMENTS

In this section, we empirically evaluate the effectiveness of the proposed method on a number of datasets. We first check the reduction of the mean squared error enabled by the MLIPS estimator. Then we apply IPS, MLIPS, POEM and MLPOEM to policy optimization and compare their performance on several datasets.

4.1 SETTINGS

Datasets. First, we adopt the same experimental design as the one specified in Swaminathan &

Joachims (2015a) to generate batch contextual bandit learning datasets from supervised learning

datasets. Four multi-label datasets are selected from the LibSVM repository (see Swaminathan &

Joachims (2015a) for more details). To construct a batch contextual bandit learning dataset, we take

ayisupeµr(v·is|exdi)l,eaanrndincgoldleactat stehteDrew=ard{(axsi,thyei)n}enig=a1t,ivseimHualamtemuinsigngdiastlaongcgein-gp(oyliic,yyiµ)

by sampling between the

supervised label and the sampled label. We use a Conditional Random Field (CRF) model (Sutton &

McCallum, 2012) trained on 5% of the data as the logging policy µ. We repeat this procedure four times to generate the entire bandit feedback dataset D = {(xi, yi, ri = -(yi, yi), µ(yi | xi))}in=1. Both the training sets and held-out testing sets are constructed in the same way as suggested in

Swaminathan & Joachims (2015a).

In addition to these standard datasets, we also evaluate the effectiveness of our method on a large-scale ad placement dataset. The dataset is collected by Criteo, a leader in the display advertising industry (Lefortier et al., 2016). For this dataset, we consider an ad display scenario where a policy selects the

6

Under review as a conference paper at ICLR 2019
products to be displayed on a website when a user arrives. Provided the candidate set of products and impression context, we hope the policy to place ads such that the aggregated click-through-rate (CTR) or the number of clicks is maximized. We consider a subset of the dataset, which is used in the NIPS 2017 Criteo Ad Placement Challenge. Each ad is represented by a 74000-dimensional feature vector. For each context (or impression), the logged action of selecting a candidate ad, a binary reward value on whether the displayed product is clicked by the user, and the probability that the logging policy picks such a specific candidate are recorded. More details can be found at CrowdAI (2017).
Surrogate policy estimation. To fit the surrogate policy, we apply two models, a simple logistic regression model and a one-hidden-layer perceptron with ten ReLU activation units, both with 2regularization. Here five-fold cross validation is used to tune the regularization parameter. We use the L-BFGS algorithm (Liu & Nocedal, 1989) for optimization.
Policy optimization. We consider IPS, POEM (Swaminathan & Joachims, 2015a), and NormPOEM (Swaminathan & Joachims, 2015b). We compare their performance with MLIPS as well as the combination of our surrogate policy technique with POEM (MLPOEM). There are several hyperparameters in POEM, Norm-POEM, and MLPOEM, including the thresholding parameter and the regularization parameter. We use five-fold cross validation on the training set to tune them. For reward maximization, following the procedures of POEM, we initialize the model weights to zeros, use mini-batch AdaGrad (Duchi et al., 2011) with a batch size of 100 and step size  = 1. On all datasets, we repeat the experiments ten times to report the average generalization performance.
4.2 MEAN SQUARED ERROR REDUCTION IN POLICY ESTIMATION
First, we check whether the MLIPS estimator has a smaller mean squared error than the vanilla IPS estimator. Instead of comparing the estimation of the value function, we focus on comparing the performance of MLIPS and IPS on off-policy gradient estimation, whose accuracy is crucial to policy optimization. We use the LYRL dataset (Swaminathan & Joachims, 2015a) and combine the training and testing sets to obtain a large bandit feedback dataset. Given a reasonably accurate policy weight vector (after training with 10000 mini-batch updates), we compute its IPS-based gradient estimator on the entire dataset to get a rough "ground-truth" gradient. We further construct a series of data subsets to evaluate the gradient estimation with smaller sample sizes, Figure 1: Log MSE of policy gradient estimations with 2-1/2, 2-2/2, . . . , 2-k/2 of the entire dataset. by IPS, MLIPS and neural network MLIPS for
logistic regression model on the LYRL dataset. For the neural network estimate of surrogate policy (NN-surrogate), we first fit a logistic regression model and a one-hidden-layer perceptron with ten ReLU activation units to estimate the surrogate policy. After this step, we compute the Euclidean distance between the "ground-truth" gradient and the gradient estimators that correspond to smaller sample sizes. Average Euclidean distances and standard deviations are computed over twenty trials. Results are illustrated in Figure 1.
We observe that the MLIPS estimator gives much better gradient estimation than the original IPS estimator. In particular, the relative improvement is significant especially when the sample size is small. It is also interesting to see that the difference between the linear surrogate policy and the neural network surrogate policy is very small, possibly because the actual logging CRF policy is linear. On the Criteo dataset, in which the logging policy is unknown and possibly very complicated, we find that the neural network surrogate policy works better than the linear policy.
4.3 GENERALIZATION PERFORMANCE OF POLICY OPTIMIZATION
To evaluate the effectiveness of our method on policy optimization, we train logistic regression policies with IPS, POEM, Norm-POEM objective functions, and the IPS and POEM versions with maximum likelihood surrogate policies (MLIPS and MLPOEM). As mentioned before, we use both
7

Under review as a conference paper at ICLR 2019

the logistic regression model and the one-hidden-layer perceptron model to fit the surrogate policies, which are respectively marked with "-Lin" and "-NN" suffixes in the names of the training methods. Results are summarized in Table 1. The test set Hamming losses for all methods are averaged over ten runs. On each run, both MLIPS and MLPOEM consistently and significantly outperforms their corresponding original versions.

As a control, we study the scenario where

the logging policy is not available. We Table 1: Evaluations of the Hamming losses on the

train the policies using IPS while treat- test sets for policy optimizations performed by impor-

ing the logging policy to be uniform. The tance weighed policy gradient estimators and their ML-

performance of such a variant of IPS (the variants on bandit datasets. Evaluations of Norm-POEM

last row in the table with algorithm name are also included as general benchmarks.

"IPS-Uniform") is significantly worse than

our method, which also does not require Dataset/Alg. knowing the logging policy. Notably, when

Scene Yeast TMC LYRL

compared against (Norm-)POEM, the state- IPS

1.342 4.571 3.023 1.108

of-the-art algorithm on these datasets, our POEM

1.143 4.549 2.522 0.981

MLPOEM algorithm can augment the gen- Norm-POEM 1.045 3.876 2.072 0.799

eralization performance on most datasets. This indicates that our surrogate policy technique provides a complementary mean squared error reduction effect for existing methods in this field. Finally, it is worth noting that the methods with neural net-

MLIPS-Lin MLIPS-NN MLPOEM-Lin MLPOEM-NN
IPS-Uniform

1.086 1.086 1.086 1.086
1.086

3.778 3.630 3.894 3.477
5.893

2.018 2.019 2.010 2.000
6.174

1.025 0.930 0.949 0.904
1.463

work surrogate polices and linear policies

are not too much different in terms of performance on these datasets, possibly because the logging

policies used to generate these datasets are linear.

4.3.1 CRITEO AD PLACEMENT DATASET
We also evaluate the performance of our method on the preprocessed data used in the NIPS 2017 Workshop Criteo ad placement challenge. Because there is no information about the logging policy used to generate the data, it is reasonable to use a complex universal function approximator to fit the surrogate policy. We implement a neural network model with 21 tanh activation units for both the surrogate policy and the target policy. Due to the massive size of this dataset and its high-dimensional and sparse feature representation, we implement the 2-regularized IPS and MLIPS in C++ with multi-threading.
Both the maximum likelihood estimation and policy optimization are performed using the L-BFGS algorithm. We split the dataset into training (80%) and testing (20%) sets. The regularization parameter is chosen by five-fold cross-validation on the training set. Evaluation is performed by computing the inverse propensity weights as suggested by the NIPS challenge. In comparison with IPS, which obtains 0.556 on the training set and 0.551 on the testing set, our method obtains 0.586 on the training set and 0.572 on the testing set. An implementation of our method (0.576 on the training set and 0.568 on the testing set, which are evaluated on the hold out data for the competition) is also ranked among very top (prize winning teams) of the NIPS 2017 challenge leaderboard.

5 CONCLUSION
We introduce a new but simple approach for mean squared error reduction in policy evaluation and optimization. Theoretical analysis illustrates that the proposed MLIPS estimator is asymptotically unbiased, and moreover, has a smaller mean squared error than the classical IPS estimator. Experimental results on several large-scale datasets also demonstrate the empirical effectiveness of the proposed estimator. Our technique can also be combined with existing approaches and further improves the performance.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Le´on Bottou, Jonas Peters, Joaquin Quin~onero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. The Journal of Machine Learning Research, 14(1):3207­3260, 2013.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In Advances in Neural Information Processing Systems, pp. 442­450, 2010.
CrowdAI. CrowdAI NIPS 2017 Criteo Ad Placement Challenge Starter Kit. https://github. com/crowdai/crowdai-criteo-ad-placement-challenge-starter-kit, 2017.
Bernard Delyon and Franc¸ois Portier. Integral approximation by kernel smoothing. Bernoulli, 22(4): 2177­2208, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Miroslav Dud´ik, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and optimization. Statistical Science, pp. 485­511, 2014.
Masayuki Henmi, Ryo Yoshida, and Shinto Eguchi. Importance sampling via the estimated sampler. Biometrika, 94(4):985­991, 2007.
Tim Hesterberg. Weighted average importance sampling and defensive mixture distributions. Technometrics, 37(2):185­194, 1995.
Keisuke Hirano, Guido W Imbens, and Geert Ridder. Efficient estimation of average treatment effects using the estimated propensity score. Econometrica, 71(4):1161­1189, 2003.
Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical Statistics, 17(2):295­311, 2008.
Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims, and Maarten de Rijke. Largescale validation of counterfactual learning methods: A test-bed. arXiv preprint arXiv:1612.00367, 2016.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web, pp. 661­670. ACM, 2010.
Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextualbandit-based news article recommendation algorithms. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, pp. 297­306. ACM, 2011.
Lihong Li, Shunbao Chen, Jim Kleban, and Ankur Gupta. Counterfactual estimation and optimization of click metrics for search engines. arXiv preprint arXiv:1403.1891, 2014.
Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In Artificial Intelligence and Statistics, pp. 608­616, 2015.
Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1-3):503­528, 1989.
Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. Learning from logged implicit exploration data. In Advances in Neural Information Processing Systems, pp. 2217­2225, 2010.
Charles Sutton and Andrew McCallum. An Introduction to Conditional Random Fields. Nowpublishers, 2012.
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In International Conference on Machine Learning, pp. 814­823, 2015a.
Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning. In Advances in Neural Information Processing Systems, pp. 3231­3239, 2015b.
9

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 3.9

The proof of Theorem 3.9 is established based on the following two lemmas. First, we introduce a lemma that relates the errors of the MLIPS and IPS estimators. Recall that, as defined in Definitions 3.5-3.7 and Condition 3.8, we have t = (t, tS, tD) where tS = (tSG, tSH, tST) and tD = (tDG, tDH). Also, recall that DG, DH, and ST are defined in Assumption 3.4.

Lemma A.1. For the MLIPS estimator V defined in (3.1) and the IPS estimator V defined in (2.2), under Assumption 3.4 and conditioning on the event E(t ) defined in (3.12), we have

V - V = (V - V ) - E DV (r, a, x; ) · S(a, x; ) · ( - )

+ O t2 · (tDH + DH) + t · tDG .

(A.1)

Proof. See §B for a detailed proof.

The following lemma characterizes the  -  term on the right-hand side of (A.1), which is the

estimation error of .

Lemma A.2. For the maximum likelihood estimator  and the true parameter , we have

 - 

= I-1() ·

1 n

n

S(ai,

xi;

)

-

S(

-

)

+

1 2n

n



2S

(ai, xi 2

;

S

)

(

-





,



-





,

·)

,

i=1 i=1

where

1 S = n

n

S(ai, xi; 

)

-

E

S(a, x; ) 

,

S = S + (1 - S).

i=1

(A.2)

Here S  [0, 1], I-1() is the inverse of the Fisher information matrix defined in (3.3), and (2S(a, x; )/2)(v1, v2, ·) is the bilinear map defined in Assumption 3.4.

Proof. See §C for a detailed proof.

Lemma A.2 allows us to more precisely characterize the  -  term in (A.1). Now we are ready to prove Theorem 3.9.

Proof. By combining Lemmas A.1 and A.2, under Assumptions 3.3 and 3.4 and conditioning on the event E(t ) defined in (3.12), we have

V - V = (V - V ) - 1 n

n

E DV (r, a, x; ) · S(a, x; )

· I-1() · S(ai, xi; ) +(t ),

i=1 (ri, ai, xi; )

(A.3)

where

(t ) = O DG · t · (ST · t + tSH) + t2 · (t · tDG + tDH + DH) .

(A.4)

For the asymptotic expectation of the MLIPS estimator V , as V is an unbiased estimator of V , we have

lim E[V - V ] = lim E[(t )].

n

n

(A.5)

By Condition 3.8 and the definition of (t ) in (A.4), we have

E (t ) E(t ) = O DG · t · (ST · t + tSH) + t2 · (tDH + DH + t · tDG)

= O DG · (ST + 1) · t2 + t2 · (t2 + t + DH) ,

(A.6)

where t =

t

. As defined in Condition 3.8,

 0

tk

df

(t)

goes

to

zero

when

n





for

any

k.

Therefore, when n  , from (A.6) we have



E[(t )] = E E (t ) E(t ) = O DG · (ST + 1) + DH ·

t2 df (t) ,

(A.7)

0

10

Under review as a conference paper at ICLR 2019

which gives limn E[(t )] = 0. Thus, we have limn[V - V ] = 0, which means that the MLIPS estimator V is asymptotically unbiased.

For the mean squared error of the MLIPS estimator V , we have

E (V - V )2 = E =E

(V - V ) - 1 n

n

(ri, ai, xi; ) + (t )

2

i=1

(V - V ) - 1 n

n

(ri, ai, xi; )

2

+ E E 2(t )

E(t )

i=1

(i) (ii)

+2·E

(V - V ) - 1 n

n

(ri, ai, xi; )

· (t ) ,

i=1

(A.8)

(iii)
where for term (ii) we use the law of total expectation. In the sequel, we characterize terms (i)-(iii) respectively.

Term (i) in (A.8): Note that by Definition 3.2 we have

V -V = 1 n

n

DV (ri, ai, xi; ),

where

DV (r, a, x; ) =

(a, x) µ(a, x; )

· r(a, x) - V.

i=1

We reformulate the first term on the right-hand side of (A.8) as

E

(V - V ) - 1 n

n
(ri, ai, xi; )

2

= E (V - V )2

+E

1 n

n

(ri, ai, xi; )

2

i=1 i=1

nn

-2/n2 · E

DV (ri, ai, xi; ) ·

(ri, ai, xi; ) .

i=1 i=1
(A.9)

Then by the fact that E[S(a, x; )] = 0, for the second term on the right-hand side of (A.9) we have

E (r, a, x; ) = E DV (r, a, x; ) · S(a, x; ) · I-1() · E S(a, x; ) = 0. (A.10)

Meanwhile, we have

nn

E

DV (ri, ai, xi; ) ·

(ri, ai, xi; )

i=1 i=1
n
= E DV (ri, ai, xi; ) · (ri, ai, xi; ) +

E DV (ri, ai, xi; ) · (rj, aj, xj; )

i=1 i=j

= n · E DV (r, a, x; ) · (r, a, x; ) ,

(A.11)

where the second equality follows from (A.10). Hence, we obtain from (A.9) and (A.11) that

E

(V - V ) - 1 n

n

(ri, ai, xi; )

2

i=1

(A.12)

= E (V - V )2 + 1/n · Var (r, x, a; ) - 2/n2 · n · E DV (r, a, x; ) · (r, a, x; ) .

Note that we have the following equality for the Fisher information matrix I(),

E S(a, x; ) · S(a, x; ) = I() = -S(a, x; )/.

(A.13)

Then by the definition of (r, a, x; ) in (3.14), we have

E 2(r, a, x; ) = E DV (r, a, x; ) · S(a, x; ) · I-1() · E S(a, x; ) · S(a, x; )

· I-1() · E DV (r, a, x; ) · S(a, x; ) ,

11

Under review as a conference paper at ICLR 2019

which by (A.13) and the definition of (r, a, x; ) implies

E 2(r, a, x; )

= E DV (r, a, x; ) · S(a, x; ) · I-1() · E DV (r, a, x; ) · S(a, x; )

= E DV (r, a, x; ) · (r, a, x; ) .

(A.14)

Note that (A.14) implies for the third term on the right-hand side of (A.12), we have

E DV (r, a, x; ) · (r, a, x; ) = E 2(r, a, x; ) = Var (r, x, a; ) ,

(A.15)

where the second equality follows from (A.10). Plugging (A.15) into (A.12), we obtain

E

(V - V ) - 1 n

n
(ri, ai, xi; )

2

= E (V - V )2

- 1/n · Var (r, x, a; ) .

i=1

(A.16)

Term (ii) in (A.8): By Condition 3.8 and the definition of (t ) in (A.4), we have

E 2(t ) E (t ) = O DG · t · (ST · t + tSH) + t2 · (tDH + DH + t · tDG) 2

= O DG · (ST + 1) · t2 + t2 · (t2 + t + DH) 2 ,

(A.17)

where t =

t

. As defined in Condition 3.8,

 0

tk

df (t)

is

a

decreasing

function

of

k.

Therefore,

from (A.17) we have



E E 2(t ) E(t ) = O DG · (ST + 1) + DH 2 ·

t4 df (t) .

(A.18)

0

Term (iii) in (A.8): By the law of total expectation, we have

E

(V - V ) - 1 n

n
(ri, ai, xi; )

· (t )

i=1

= E (V - V ) · (t ) - E E

1 n

n

(ri, ai, xi; )

· (t ) E(t )

.

i=1

(A.19)

For the first term on the right-hand side of (A.19), by the Cauchy-Schwartz inequality for expectation,

we have

1/2
E (V - V ) · (t )  E (V - V )2 · E E 2(t ) E(t )

(A.20)

1/2

= E (V - V )2

· O DG · (ST + 1) + DH ·

 1/2

t4 df (t)

,

0

where the second equality follows from (A.18). Now we consider the second term on the right-hand

side of (A.19). Note that by the definition of (r, a, x; ) in (A.3), we have

1 n

n

(ri, ai, xi; )

=

1 n

n

E DV (r, a, x; ) · S(a, x; )

i=1 i=1

· I-1() · S(ai, xi; )

 E DV (r, a, x; ) · S(a, x; )

·

I -1 (  )

·

1 n

n
S(ai, xi; ) ,

i=1

(A.21)

where by (A.23) we have that, conditioning on the event E(t ) defined in (3.12),

1 n

n
S(ai, xi; )

 tSG.

i=1

Under Assumption 3.4, we prove in Lemma B.1 and (B.2) that

E DV (r, a, x; ) · S(a, x; )

=

E

DV (r, a, x; ) 

= O(DG).

(A.22)

12

Under review as a conference paper at ICLR 2019

Note that in the event ESG(tSG) defined in (3.9), we have E[S(a, x; )] = 0. Hence, equivalently

we have

ESG(tSG) =

1 n

n
S(ai, xi; )

 tSG

.

i=1

(A.23)

Combining (A.4), (A.21), (A.22), (A.23), and Assumption 3.3, under Condition 3.8 we have

E

1 n

n
(ri, ai, xi; )

· (t ) E(t )

i=1

= O DG · tSG · DG · t · (ST · t + tSH) + t2 · (t · tDG + tDH + DH)

= O D2 G · (ST + 1) · t3 + DG · t3 · (t2 + t + DH) ,

(A.24)

where t = t . By (A.24), using a similar argument to the one used to obtain (A.18), we have

EE

1 n

n
(ri, ai, xi; )

· (t ) E(t )

i=1



= O D2 G · (ST + 1) + DG · DH ·

t3 df (t) .

0

(A.25)

Thus, combining (A.20) and (A.25), for (A.19) we obtain

E

(V - V ) - 1 n

n
(ri, ai, xi; )

· (t )

i=1

1/2

= E (V - V )2

· O DG · (ST + 1) + DH ·

 1/2
t4 df (t)

0



+ O D2 G · (ST + 1) + DG · DH ·

t3 df (t) .

0

(A.26)

Note that term (ii) in (A.8), which is characterized by (A.18), is dominated by the last term on the

right-hand side of (A.25), since

 0

tk

df (t)

decreases

along

k

under

Condition

3.8.

Plugging

(A.16),

(A.18), and (A.26) into (A.8), we obtain

E (V - V )2 = E (V - V )2 - 1/n · Var (r, x, a; )

1/2

+ E (V - V )2

· O DG · (ST + 1) + DH ·



+ O D2 G · (ST + 1) + DG · DH ·

t3 df (t) ,

0

which concludes the proof of Theorem 3.9.

 1/2
t4 df (t)
0

B PROOF OF LEMMA A.1

Before proving the Lemma A.1, we first introduce the following lemma on the gradient of the deviation function DV (r, a, x; ).

Lemma B.1. For the score and deviation functions defined in (3.3) and (3.4), we have

DV (r, a, x; ) E 

= -E DV (r, a, x; ) · S(a, x; ) .

Proof. By Definition 3.2, we have

DV (r, a, x; ) E 

= -E

(a, x) · r(a, x) µ2(a, x; )

·

µ(a, x; 

)

.

13

(B.1)

Under review as a conference paper at ICLR 2019

Then using the fact that E S(a, x; ) = 0, we have

- E DV (r, a, x; ) · S(a, x; ) = E

V

-

(a, x) µ(a, x; )

· r(a, x)

· S(a, x; )

= -E

(a, x) · r(a, x) µ2(a, x; )

·

µ(a, x; 

)

+ V · E S(a, x; )

=E

DV (r, a, x; ) 

,

where the second equality follows from (3.3) and the last equality follows from (B.1). Hence, we conclude the proof of Lemma B.1.

Note that by Lemma B.1 and Assumption 3.4, we also have the following equality, E DV (r, a, x; ) · S(a, x; ) = O(DG).
Now we proceed to prove Lemma A.1.

(B.2)

Proof. Recall that we have

DV

(r, a, x; )

=

(a, x) µ(a, x; )

·

r(a, x) -

V,

by which we have

V -V = 1 n

n

DV (ri, ai, xi; ),

V -V = 1 n

n

DV (ri, ai, xi; ).

i=1 i=1

Then by applying the Taylor expansion to the second equality in (B.3), we have

(B.3)

V -V = 1 n

n
DV (ri, ai, xi; ) +

1 n

n

DV (ri, ai, xi; ) 

i=1 i=1

· ( - )

1 +
2n

n
( - )

·

2DV

(ri, ai, xi; 2

D)

·

(

-

)

i=1

= V - V + 1 n DV (ri, ai, xi; ) n 
i=1

· ( - ) + O t2 · (tDH + DH) ,

(B.4)

where D = D + (1 - D) for some D  [0, 1], and the last equality follows from Assumption 3.4 and the fact that we condition on the event E(t ) defined in (3.12). By Lemma B.1, for the second
term on the right-hand side of (B.4), we have

1 n DV (ri, ai, xi; )

n
i=1



· ( - )

= -E DV (r, a, x; ) · S(a, x; ) · ( - ) + D ( - ),

where

1 D = n

n

DV

(ri, ai, 

xi; )

-

E

DV (r, a, x; ) 

.

i=1

Meanwhile, conditioning on the event E(t ) defined in (3.12), we have

(B.5)

D ( - ) = O(t · tDG). Combining (B.6), (B.5), and (B.4), we have

(B.6)

V - V = (V - V ) - E DV (r, a, x; ) · S(a, x; ) · ( - )

+ O t · tDG + t2 · (tDH + DH) , which concludes the proof of Lemma A.1.

14

Under review as a conference paper at ICLR 2019

C PROOF OF LEMMA A.2

Proof. First, recall that we have

 log µ(a, x; )

S(a, x; ) =

.



Since  is the maximum likelihood estimator of the true parameter  based on {(ai, xi)}ni=1, we have

1n n S(ai, xi; ) = 0.
i=1

(C.1)

On the other hand, we expand the above sum to obtain

1 n

n

1 S(ai, xi; ) = n

n

S(a, x; ) + 1 n

n

S(a, x; ) · ( - ) 

i=1 i=1 i=1

1 +
2n

n

2S(a, x; 2

S)

(

-

,



-

,

·),

i=1

(C.2)

where S = S + (1 - S) for some S  [0, 1]. Then by the definition of S in (A.2), we combine (C.1) and (C.2) to obtain

1 0=
n

n

S(ai, xi; ) + E

S(a, x; ) 

· ( - ) - S( - )

i=1

1 +
2n

n

2S(ai, xi; 2

S)

(

-

,



-

,

·).

i=1

(C.3)

Since we have

S(a, x; ) E 

= -I(),

which is invertible by Assumption 3.3, by rearranging the terms in (C.3) we obtain

 - 

= I-1() ·

1 n

n

S(ai,

xi;

)

-

S(

-

)

+

1 2n

n



2S

(ai, xi 2

;

S

)

(

-





,



-





,

·)

,

i=1 i=1

which concludes the proof of Lemma A.2.

D APPLICATION TO MULTINOMIAL LOGISTIC REGRESSION

In this section, we consider the setting where the logging distribution µ(a | x; ) is parametrized by the multinomial logistic regression model. We assume that x  Rp and a  [m]. Let
 = (1 , . . . , m-1, 0 ) , where 1, . . . , m-1  Rp. Thus, the dimension d of the parameter  is p(m - 1). The logging distribution µ(a | x; ) is parametrized by

µ(a | x; ) =

exp(x a)

m-1 l=1

exp(x

l

)

+

1

.

(D.1)

15

Under review as a conference paper at ICLR 2019

Then the score function S(a, x; )  Rp(m-1) takes the form











S(a,

x;

)

=

 







exp(x 1)

m-1 l=1

exp(x

l)

+

1

·

x

-

1(a

=

1)

·

x

exp(x 2)

m-1 l=1

exp(x

l)

+

1

·

x

-

1(a

=

2)

·

x

...











 

,







(D.2)

 

exp(x m-1)

m-1 l=1

exp(x

l) +

1

·

x

-

1(a

=

m

-

1)

·

 x

where 1(·) is the indicator function, while the Fisher information matrix I()  Rp(m-1)×p(m-1) is

a block matrix whose (j, k)-th block, namely [I()]j,k, takes the form

[I()]j,k = E

1(j = k) -

exp(x j)

m-1 l=1

exp(x

l) + 1

·

exp(x k)

m-1 l=1

exp(x

l)

+

1

·

xx

, (D.3)

for j, k  [m - 1]. Meanwhile, the deviation function DV (r, a, x; ) takes the form

DV

(r, a, x; )

=

(a | x)

·

r(a, x) ·

m-1 l=1

exp(x

exp(x a)

l) + 1

- V.

(D.4)

The gradient of the deviation function takes the form









DV (r, a, x; )

=

(a | x)

· r(a, x) ·

  

 





exp(x exp(x

1) - 1(a = 1) ·

m-1 l=1

exp(x

exp(x a)

2) - 1(a = 2) ·

m-1 l=1

exp(x

exp(x a) ...

l) + 1 l) + 1

·x ·x











 

.







 

exp(x

m) - 1(a = m - 1) ·

m-1 l=1

exp(x

l) + 1

 · x

exp(x a)

(D.5)

The Hessian of the deviation function is a block matrix in Rp(m-1)×p(m-1), whose (j, k)-th block

takes the form

2DV (r, a, x; ) 2

= xx
j,k

· (a | x) · r(a, x)

(D.6)

1(k = j) - 1(k = a) 1(k = a = j) · ·+
exp x (a - j)

m-1 l=1

exp(x

l) + 1

- 1(a = j) · exp(x

k )

exp(x a)

for j, k  [m - 1].

Note that by (D.2)-(D.3) and (D.5) -(D.6) for the population quantities in Assumptions 3.3-3.4 are all bounded as long as x and  are bounded. Hence, in the multinomial logistic regression model defined in (D.1) with bounded x and  we have

DG = DH = ST = 1, and t  M,

(D.7)

where t is defined in Condition 3.12 and M is some constant that does not scale with n.

The subsequent lemmas establish the tail behaviors for the events defined in Definitions 3.5-3.7, which together yield a concrete form of the tail function f (t) defined in Condition 3.8.

Lemma D.1 (Maximum Likelihood Estimation Error). For the event E(t) defined in (3.8), we have
P E(t)  1 - C · exp d - nt2 ,
where C is a positive constant.

Lemma D.2 (Concentration of Score). For the logistic regression model in (D.1), with the event ESG(tSG), ESH(tSH) and EST(tST) defined in Definition 3.6, we have
P ESG(tSG)  1 - CSG · exp(d - nt2SG), P ESH(tSH)  1 - CSH · exp(d - nt2SH)

16

Under review as a conference paper at ICLR 2019

and P EST(tST)  1 - CSG · exp(d - nt2ST),
where CSG, CSH, and CST are positive constants.
Lemma D.3 (Concentration of Deviation). For the logistic regression model (D.1), with the event EDG(tDG) and EDH(tDH) defined in 3.7, we have
P EDG(tDG)  1 - CDG · exp(d - nt2DG), P EDH(tDH)  1 - CDH · exp(d - ntD2 H), where CDG and CDH are positive constants.

Recall the event E(t ) with t = t  is defined in Condition 3.8. By Lemmas D.1-D.3, the tail function f (t) takes the form

f (t)  6C · exp(d - nt2),

(D.8)

where C = max{C, CSG, CSH, CDG, CDH, CDT} > 0.

In the sequel, we use (D.7) and the tail function in (D.8) to characterize the reduction of the mean squared error induced by the multinomial logistic regression model.

First, let t0 = (T + 1) d/n with some constant T > 0, we have the following bound on the tail probability of t based on (D.8),

P(t > t0)  f (t0)  6C · e-T d.

(D.9)

Then recall that in Theorem 3.9 we have

1/2

(n) = E (V - V )2

·O

where by (D.9), we have

 1/2



t4 df (t) + O t3 df (t) ,

00

t0
t3 df (t) = E[t3 | t  t0]  (T + 1)3 · (d/n)3/2 = O (d/n)3/2 ,

(D.10)

0

and

M
t3 df (t) = E[t3 | t > t0]  P(t > t0) · M 3  6C · exp(-T d) · M 3 = O(e-T d), (D.11)

t0

which decays exponentially with d and is therefore dominated by the bound given in (D.10). Hence,

we have

 t0 M
t3 df (t) = t3 df (t) + t3 df (t) = O (d/n)3/2 .

(D.12)

0 0 t0

Using the same arguments in derivation of (D.12), we also have

Hence, we obtain

 1/2
t4 df (t) = O d/n .
0

1/2

(n) = E (V - V )2

· O d/n + O (d/n)3/2 .

Note that the mean squared error term E[(V - V )2] is of the order 1/n, which implies that (n) is of the order (d/n)3/2. Thus, (3.15) is satisfied when n is sufficiently large, which means that a reduction of the order 1/n on the mean squared error is achieved by the multinomial logistic regression model when the sample size n is sufficiently large. This result implies that the MLIPS in (3.1) is guaranteed to out perform the IPS estimator in (2.2) when the sample size n is much larger than the dimension d of the problem. Moreover, since the mean squared error term E[(V - V )2] itself is of the order 1/n, the effect of reduction does not vanish when n  .
We interpret Theorem 3.9 in a more general way. The (n) term is in general a decreasing function of the sample size n and is mostly of a higher order than 1/n in common parametric statistical models. Correspondingly, for a sufficiently large n, we obtain (3.15), which implies that our method leads to a 1/(2n) · Var((r, x, a; )) reduction on the mean squared error, which has a non-vanishing effect when n  .

17

