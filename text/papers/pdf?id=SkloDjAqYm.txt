Under review as a conference paper at ICLR 2019
LEMONADE: LEARNED MOTIF AND NEURONAL ASSEMBLY DETECTION IN CALCIUM IMAGING VIDEOS
Anonymous authors Paper under double-blind review
ABSTRACT
Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatiotemporally coordinated activation patterns, or "motifs", are thought to be building blocks of neural representations and information processing. We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology. Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction. Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used. An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance. In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations.
1 INTRODUCTION
Seventy years after being postulated by Hebb (1949), the existence and importance of reoccurring spatio-temporally coordinated neuronal activation patterns (motifs), also known as neuronal assemblies, is still fiercely debated (Marr et al., 1991; Singer, 1993; Nicolelis et al., 1997; Ikegaya et al., 2004; Cossart & Sansonetti, 2004; Buzsáki, 2004; Mokeichev et al., 2007; Pastalkova et al., 2008; Stevenson & Kording, 2011; Ahrens et al., 2013; Carrillo-Reid et al., 2015). Calcium imaging, a microscopic video technique that enables the concurrent observation of hundreds of neurons in vitro and in vivo (Denk et al., 1990; Helmchen & Denk, 2005; Flusberg et al., 2008), is best suited to witness such motifs if they indeed exist.
In recent years, a variety of methods have been developed to identify neuronal assemblies. These methods range from approaches for the detection of synchronous spiking, up to more advanced methods for the detection of arbitrary spatio-temporal firing patterns (Comon, 1994; Nicolelis et al., 1995; Grün et al., 2002a;b; Lopes-dos Santos et al., 2013; Russo & Durstewitz, 2017; Peter et al., 2017). All of these methods, however, require a spike time matrix as input. Generating such a spike time matrix from calcium imaging data requires the extraction of individual cells and discrete spike times. Again, many methods have been proposed for these tasks (Mukamel et al., 2009; Pnevmatikakis & Paninski, 2013; Pnevmatikakis et al., 2013; Diego et al., 2013; Diego & Hamprecht, 2013; Pachitariu et al., 2013; Pnevmatikakis et al., 2014; Diego & Hamprecht, 2014; Kaifosh et al., 2014; Pnevmatikakis et al., 2016; Apthorpe et al., 2016; Inan et al., 2017; Spaen et al., 2017; Klibisz et al., 2017; Speiser et al., 2017; Zhou et al., 2018). Given the low signal-to-noise ratios (SNR), large background fluctuations, non-linearities, and strong temporal smoothing due to the calcium dynamics itself as well as that of calcium indicators, it is impressive how well some of these methods perform, thanks to modern recording technologies and state-of-the-art regularization and inference (Pnevmatikakis et al., 2016; Zhou et al., 2018). Still, given the difficulty of this data, errors in segmentation and spike extraction are unavoidable, and adversely affect downstream processing steps that do not have access to the raw data. Hence, properly annotating data and correcting the output from automatic segmentation can still take up a huge amount of time.
In this paper, we propose LeMoNADe (Learned Motif and Neuronal Assembly Detection), a variational autoencoder (VAE) based framework specifically designed to identify repeating firing motifs with arbitrary temporal structure directly in calcium imaging data (see figure 1). The encoding and
1

Under review as a conference paper at ICLR 2019

Cell Identification
e.g. Pnevmatikakis et al. (2016); Zhou
et al. (2018)

Spike Time Extraction e.g. Speiser et al. (2017)

Motif Detection e.g. Russo
& Durstewitz (2017); Peter et al. (2017)

Ca Imaging Videos

Motif Detection in Ca Videos LeMoNADe

Neuronal Assemblies

Figure 1: We present LeMoNADe, a novel approach to identify neuronal assemblies directly from calcium imaging data. In contrast to previous methods, LeMoNADe does not need pre-processing steps such as cell identification and spike time extraction for unravelling assemblies.

decoding networks are set up such that motifs can be extracted directly from the decoding filters, and their activation times from the latent space (see sec. 3). Motivated by the sparse nature of neuronal activity we replace the Gaussian priors used in standard VAE. Instead we place Bernoulli priors on the latent variables to yield sparse and sharply peaked motif activations (sec. 3.1). The choice of discrete Bernoulli distributions makes it necessary to use a BinConcrete relaxation and the Gumbel-softmax reparametrization trick (Maddison et al., 2016; Jang et al., 2017) to enable gradient descent techniques with low variance (sec. 3.3). We add a -coefficient (Higgins et al., 2017) to the loss function in order to adapt the regularization to the properties of the data (sec. 3.3). Furthermore, we propose a training scheme which allows us to process videos of arbitrary length in a computationally efficient way (sec. 3.4). On synthetically generated datasets the proposed method performs as well as a state-of-the-art motif detection method that requires the extraction of individual cells (sec. 4.1). Finally, we detect possible repeating motifs in two fluorescent microscopy datasets from hippocampal slice cultures (sec. 4.2). A PyTorch implementation of the proposed method is released on GitHub1.
2 RELATED WORK
Autoencoder and variational autoencoder Variational Autoencoders (VAEs) were introduced by Kingma & Welling (2014) and have become a popular method for unsupervised generative deep learning. They consist of an encoder, mapping a data point into a latent representation, and a decoder whose task is to restore the original data and to generate samples from this latent space. However, the original VAE lacks an interpretable latent space. Recent suggestions on solving this problem have been modifications of the loss term (Higgins et al., 2017) or a more structured latent space (Johnson et al., 2016; Deng et al., 2017).
VAE have also been successfully used on video sequences. Li & Mandt (2018) learn a disentangled representation to manipulate content in cartoon video clips, while Goyal et al. (2017) combine VAEs with nested Chinese Restaurant Processes to learn a hierarchical representation of video data. Closest to our goal of detecting motifs in video data is the work described in Bascol et al. (2016). In this approach, a convolutional autoencoder is combined with a number of functions and regularization terms to enforce interpretability both in the convolutional filters and the latent space. This method was successfully used to detect patterns in data with document structure, including optical flow features of videos. However, as the cells observed in calcium imaging are spatially stationary and have varying luminosity, the extraction of optical flow features makes no sense. Hence this method is not applicable to the task of detecting neuronal assemblies in calcium imaging data.
Cell segmentation and spike time extraction from calcium imaging data Various methods have been proposed for automated segmentation and signal extraction from calcium imaging data. Most of them are based on non-negative matrix factorization (Mukamel et al., 2009; Pnevmatikakis & Paninski, 2013; Pnevmatikakis et al., 2013; 2014; Diego & Hamprecht, 2014; Pnevmatikakis et al., 2016; Inan et al., 2017; Zhou et al., 2018), clustering (Kaifosh et al., 2014; Spaen et al., 2017), and dictionary learning (Diego et al., 2013; Diego & Hamprecht, 2013; Pachitariu et al., 2013).
1link has been removed for anonymity
2

Under review as a conference paper at ICLR 2019

Recent approaches started to use deep learning for the analysis of calcium imaging data. Apthorpe et al. (2016) and Klibisz et al. (2017) use convolutional neural networks (CNNs) to identify neuron locations and Speiser et al. (2017) use a VAE combined with different models for calcium dynamics to extract spike times from the calcium transients.
Although many sophisticated methods have been proposed, the extraction of cells and spike times from calcium imaging data can still be prohibitively laborious and require manual annotation and correction, with the accuracy of these methods being limited by the quality of the calcium recordings. Furthermore, some of the mentioned methods are specially designed for two-photon microscopy, whereas only few methods are capable to deal with the low SNR and large background fluctuations in single-photon and microendoscopic imaging (Flusberg et al., 2008; Ghosh et al., 2011). Additional challenges for these methods are factors such as non-Gaussian noise, non-cell background activity and seemingly overlapping cells which are out of focus (Inan et al., 2017).
Neuronal assembly detection The identification of neuronal assemblies in spike time matrices has been studied from different perspectives. For the detection of joint (strictly synchronous) spike events across multiple neurons, rather simple methods based on PCA or ICA have been proposed (Comon, 1994; Nicolelis et al., 1995; Lopes-dos Santos et al., 2013), as well as more sophisticated statistical methods such as unitary event analysis (Grün et al., 2002a;b). Higher-order correlations among neurons and sequential spiking motifs such as synfire chains can be identified using more advanced statistical tests (Staude et al., 2010a;b; Gerstein et al., 2012). The identification of cell assemblies with arbitrary spatio-temporal structure has been addressed only quite recently. One approach recursively merges sets of units into larger groups based on their joint spike count probabilities evaluated across multiple different time lags (Russo & Durstewitz, 2017). Another method uses sparse convolutional coding (SCC) for reconstructing the spike matrix as a convolution of spatio-temporal motifs and their activations in time (Peter et al., 2017). An extension of this method uses a group sparsity regularization to identify the correct number of motifs (Mackevicius et al., 2018).
To the authors' knowledge, solely Diego & Hamprecht (2013) address the detection of neuronal assemblies directly from calcium imaging data. This method, however, only aims at identifying synchronously firing neurons, whereas the method proposed in this paper can identify also assemblies with more complex temporal firing patterns.

3 METHOD
LeMoNADe is a VAE based latent variable method, specifically designed for the unsupervised detection of repeating motifs with temporal structure in video data. The data x is reconstructed as a convolution of motifs and their activation time points as displayed in figure 2a. The VAE is set up such that the latent variables z contain the activations of the motifs, while the decoder encapsulates the firing motifs of the cells as indicated in figure 2b.

3.1 THE LEMONADE MODEL

In the proposed model the dataset consists of a single video x  RT ×P ×P with T frames of P × P pixels each. We assume this video to be an additive mixture of M repeating motifs of maximum
temporal length F . At each time frame t = 1, . . . , T , and for each motif m = 1, . . . , M , a latent random variable ztm  {0, 1} is drawn from a prior distribution pa(z). The variable ztm indicates whether motif m is activated in frame t or not. The video x is then generated from the conditional distribution p(x | z) with parameters .
In order to infer the latent activations z the posterior p(z | x) is needed. However, the true posterior p(z | x) is intractable, but it can be approximated by introducing the recognition model (or approximate posterior) q(z | x). We assume that the recognition model q(z | x) factorizes into the M motifs and T time steps of the video. In contrast to most VAE, we further assume that each latent variable ztm is Bernoulli-distributed with parameter tm(x; )

MT

MT

q(z | x) =

q(ztm | x) =

Bernoulli ztm | tm(x; )

m=1 t=1

m=1 t=1

.

(1)

3

Under review as a conference paper at ICLR 2019

calcium imaging video x

=

reconstructed video x
+
activations z (a)

motifs M

¬ENCODER

«SAMPLING

¬DECODER

x z
(b)

x

Figure 2: Schematic sketch of the proposed method. In this toy example, the input video x is an additive mixture of two motifs (highlighted in red and blue) plus noise, as shown in (a). To learn the motifs and activations, the loss between input video x and reconstructed video x is minimized. (b) shows the generation of the reconstructed video through the proposed VAE framework.

a ztm-f M
F



xt T

Generative Model
TM
z  Bernoulli ztm | a
t=1 m=1
x | z,   N x | f(z), 2-11

Recognition Model

TM

z | x,  

Bernoulli ztm | tm(x; )

t=1 m=1

Figure 3: Plate diagram and proposed generative and recognition model. We show the plate diagram of the proposed model (left), where red (solid) lines correspond to the generative/decoding process and blue (dashed) lines correspond to the recognition/encoding model. On the right the equations for the generative as well as the recognition model are given.

We sample the activations z in the latent space from the Bernoulli distributions to enforce sparse, sharply peaked activations. The parameters tm(x; ) are given by a CNN with parameters . The corresponding plate diagram and proposed generative and recognition model are shown in figure 3.
4

Under review as a conference paper at ICLR 2019

3.2 THE VAE OBJECTIVE

In order to learn the variational parameters, the KL-divergence between approximate and true
posterior KL(q (z | x) p(z | x)) is minimized. Instead of minimizing this KL-divergence, we can also maximize the variational lower bound L(, ; x) (ELBO) (see e.g. Blei et al. (2017))

L(, ; x) = Ezq(z | x) log p(x | z) - KL q(z | x) pa(z) .

(2)

In order to optimize the ELBO, the gradients w.r.t. the variational parameters  and the generative

parameters  have to be computed. The gradient w.r.t. , however, cannot be computed easily, since

the expectation in eq. (2) depends on . A reparameterization trick (Kingma et al., 2015) is used to

overcome this problem: the random variable z  q(z | x) is reparameterized using a differentiable transformation h(, x) of a noise variable  such that

z = h(, x) with   p() .

(3)

The reparameterized ELBO, for which the expectation can be computed, e.g. using Monte Carlo sampling, is then given by

L(, ; x) = Ep() log p x | z = h(, x) - KL q(z | x) pa(z) . More details on VAE as introduced by Kingma & Welling (2014) are given in appendix A.

(4)

3.3 LEMONADE REPARAMETRIZATION TRICK AND LOSS FUNCTION

In our case, however, by sampling from Bernoulli distributions we have added discrete stochastic nodes to our computational graph, and we need to find differentiable reparameterizations of these nodes. The Bernoulli distribution can be reparameterized using the Gumbel-max trick (Luce, 1959; Yellott, 1977; Papandreou & Yuille, 2011; Hazan & Jaakkola, 2012; Maddison et al., 2014). This, however, is not differentiable. For this reason we use the BinConcrete distribution (Maddison et al., 2016), which is a continuous relaxation of the Bernoulli distribution with temperature parameter . For   0 the BinConcrete distribution smoothly anneals to the Bernoulli distribution. The BinConcrete distribution can be reparameterized using the Gumbel-softmax trick (Maddison et al., 2016; Jang et al., 2017), which is differentiable.

Maddison et al. (2016) show that for a discrete random variable z  Bernoulli(), the reparameteri-

zation of the BinConcrete relaxation of this discrete distribution is

1 log(~) + log(U ) - log(1 - U )

z~ = (y) = 1 + exp(-y) with y =



(5)

where U  Uni(0, 1) and ~ = /(1 - ).

Hence the relaxed and reparameterized lower bound L~(, ~; x)  L(, ; x) can be written as

L~(, ~; x) = Eyg~,1 (y | x) log p x | (y) - KL g~,1 (y | x)||fa~,2 (y)

(6)

where g~,1 (y | x) is the reparameterized BinConcrete relaxation of the variational posterior q(z | x) and fa~,2 (y) the reparameterized relaxation of the prior pa(z). 1 and 2 are the respective temperatures and ~ and a~ the respective locations of the relaxed and reparameterized variational posterior

and prior distribution.

The first term on the RHS of eq. (6) is a negative reconstruction error, showing the connection
to traditional autoencoders, while the KL-divergence acts as a regularizer on the approximate posterior q(z | x). As shown in Higgins et al. (2017), we can add a -coefficient to this KL-term which allows to vary the strength of the constraint on the latent space.

Instead of maximizing the lower bound, we will minimize the corresponding loss function

(x, x , ~, 1, a~, 2, KL) = MSE(x, x ) + KL · KL g~,1 (y | x)||fa~,2 (y)

= MSE(x, x ) - KL · EUUni(0,1)

log fa~,2 y(U, ~, 1) g~,1 y(U, ~, 1) | x

(7)

with MSE(x, x ) being the mean-squared error between x and x , and the -coefficient KL. Datasets with low SNR and large background fluctuations will need a stronger regularization on the activations and hence a larger KL than higher quality recordings. Hence, adding the -coefficient to the loss function enables our method to adapt better to the properties of specific datasets and recording
methods.

5

Under review as a conference paper at ICLR 2019
3.4 LEMONADE NETWORK ARCHITECTURE
The encoder network starts with a few convolutional layers with small 2D filters operating on each frame of the video separately, inspired by the architecture used in Apthorpe et al. (2016) to extract cells from calcium imaging data. Afterwards the feature maps of the whole video are passed through a final convolutional layer with 3D filters. These filters have the size of the feature maps obtained from the single images times a temporal component of length F , which is the expected maximum temporal length of the motifs. We apply padding in the temporal domain to also capture motifs correctly which are cut off at the beginning or end of the analyzed image sequence. The output of the encoder are the parameters ~ which we need for the reparametrization in eq. (5). From the reparametrization we gain the activations z which are then passed to the decoder. The decoder consists of a single deconvolution layer with M filters of the original frame size times the expected motif length F , enforcing the reconstructed data x to be an additive mixture of the decoder filters. Hence, after minimizing the loss the filters of the decoder contain the detected motifs.
Performing these steps on the whole video would be computationally very costly. For this reason, we perform each training epoch only on a small subset of the video. The subset consists of a few hundred consecutive frames, where the starting point of this short sequence is randomly chosen in each epoch. We found that doing so did not negatively affect the performance of the algorithm. By using this strategy we are able to analyse videos of arbitrary length in a computationally efficient way.
More implementation details can be found in appendix B.
4 EXPERIMENTS AND RESULTS
4.1 SYNTHETIC DATA
The existence of neuronal assemblies is still fiercely debated and their detection would only be possible with automated, specifically tailored tools, like the one proposed in this paper. For this reason, no ground truth exists for the identification of spatio-temporal motifs in real neurophysiological spike data. In order to yet report quantitative accuracies, we test the algorithm on synthetically generated datasets for which ground truth is available. For the data generation we used a procedure analogous to the one used in Diego et al. (2013) and Diego & Hamprecht (2013) for testing automated pipelines for the analysis and identification of neuronal activity from calcium imaging data. In contrast to them, we include neuronal assemblies with temporal firing structure. The cells within an assembly can have multiple spikes in a randomly chosen but fixed motif of temporal length up to 30 frames. We used 3 different assemblies in each sequence. Additionally, spurious spikes of single neurons were added to simulate noise. The ratio of spurious spikes to all spikes in the dataset was varied from 0% up to 90% in ten steps. The details of the synthetic data generation can be found in appendix C.1.
To the best of our knowledge, the proposed method is the first ever to detect video motifs with temporal structure directly in calcium imaging data. As a consequence, there are no existing baselines to compare to. Hence we here propose and evaluate the SCC method presented in Peter et al. (2017) as a baseline. The SCC algorithm is able to identify motifs with temporal structure in spike trains or calcium transients. To apply it to our datasets, we first have to extract the calcium transients of the individual cells. For the synthetically generated data we know the location of each cell by construction, so this is possible with arbitrary accuracy. The output of the SCC algorithm is a matrix that contains for each cell the firing behavior over time within the motif. For a fair comparison we brought the motifs found with LeMoNADe, which are short video sequences, into the same format.
The performance of the algorithms is measured by computing the cosine similarity (Singhal, 2001) between ground truth motifs and detected motifs. The cosine similarity is one for identical and zero for orthogonal patterns. Not all ground truth motifs extend across all 30 frames, and may have almost vanishing luminosity in the last frames. Hence, the discovered motifs can be shifted by a few frames and still capture all relevant parts of the motifs. For this reason we computed the similarity for the motifs with all possible temporal shifts and took the maximum. More details on the computation of the similarity measure can be found in appendix C.2.
We ran both methods on 200 synthetically generated datasets with the parameters shown in table 3 in the appendix. We here show the results with the correct number of motifs (M = 3) used in both methods. In appendix C.3.1 we show that if the number of motifs is overestimated (here M > 3),
6

similarity

Under review as a conference paper at ICLR 2019
1.0
SCC 0.8 LeMoNADe
0.6
0.4
0.2
0.0
0 10 20 30 no4i0se level in50% 60 70 80 90
Figure 4: Similarities between found motifs and ground truth for different noise levels. We show for LeMoNADe (lime green) and SCC (blue) the average similarities between found motifs and ground truth for ten different noise levels ranging from 0% up to 90% spurious spikes. Error bars indicate the standard deviation. For each noise level 20 different datasets were analyzed. For both, LeMoNADe and SCC, the similarities between found and ground truth motifs are significantly above the 95%-tile of the corresponding bootstrap distribution (red) up to a noise level of 70% spurious spikes. Although LeMoNADe does not need the previous extraction of individual cells, it performs as well as SCC in detecting motifs and also shows a similar stability in the presence of noise.
LeMoNADe still identifies the correct motifs, but they are repeated multiple times in the surplus filters. Hence this does not reduce the performance of the algorithm. The temporal extent of the motifs was set to F = 31 to give the algorithms the chance to also capture the longer patterns. The cosine similarity of the found motifs to the set of ground truth motifs, averaged over all found motifs and all experiments for each of the ten noise levels, is shown in figure 4. The results in figure 4 show that LeMoNADe performs as well as SCC in detecting motifs and also shows a similar stability in the presence of noise as SCC. This is surprising since LeMoNADe does not need the previous extraction of individual cells and hence has to solve a much harder problem than SCC.
In order to verify that the results achieved by LeMoNADe and SCC range significantly above chance, we performed a bootstrap (BS) test. For this, multiple datasets were created with similar spike distributions as before, but with no reoccurring motif-like firing patterns. We compiled a distribution of similarities between patterns suggested by the proposed method and randomly sampled segments of same length and general statistics from that same BS dataset. The full BS distributions are shown in appendix C.4. The 95%-tile of the BS distributions for each noise level are also shown in figure 4.
Figure 5 shows an exemplary result from one of the analysed synthetic datasets with 10% noise and maximum temporal extend of the ground truth motifs of 28 frames. All three motifs were correctly identified (see figure 5a) with a small temporal shift. This shift does not reduce the performance as it is compensated by a corresponding shift in the activations of the motifs (see figure 5b). In order to show that the temporal structure of the found motifs matches the ground truth, in figure 5a for motif 1 and 2 we corrected the shift of one and two frames, respectively. We also show the results after extracting the individual cells from the motifs and the results from SCC in figure 5c. One can see that the results are almost identical, again except for small temporal shifts.
4.2 REAL DATA
We applied the proposed method on two datasets obtained from organotypic hippocampal slice cultures. The cultures were prepared from 7­9-day-old Wistar rats as described in Kann et al. (2003) and Schneider et al. (2015). The fluorescent Ca2+ sensor, GCaMP6f (Chen et al., 2013), was delivered to the neurons by an adeno-associated virus (AAV). Neurons in stratum pyramidale of CA3 were imaged for 6.5 (dataset 1) and 5 minutes (dataset 2) in the presence of the cholinergic receptor agonist carbachol. For more details on the generation of these datasets see appendix D.1.
The proposed method was run on these datasets with the parameter settings shown in table 3 in the appendix. The analysis of the datasets took less than two hours on a Ti 1080 GPU. Before running the analysis we computed F/F for the datasets. We looked for up to three motifs with a maximum extent of F = 21 frames. The results are shown in figure 6. For both datasets, one motif in figure 6a consists of multiple cells, shows repeated activation over the recording period (see figure 6b, 6c), and contains temporal structure (see figure 6d). The other two "motifs" can easily be identified as artefacts and background fluctuations. As SCC and many other motif detection methods,
7

Under review as a conference paper at ICLR 2019

frame 1

frame 3

frame 5

frame 7

frame 9

frame 11

frame 13

frame 15

frame 17

frame 19

frame 21

frame 23

frame 25

frame 27

frame 1

frame 3

frame 5

frame 7

frame 9

frame 11

frame 13

frame 15

frame 17

frame 19

frame 21

frame 23

frame 25

frame 27

(a) Superposition of the motifs in red, green and blue for the ground truth motifs (top) and found motifs (bottom)

motif 0

motif 1

motif 2

neuron

neuron

activation activation activation motif 2 motif 1 motif 0

15 15 15 200 10 10 10

100 0 0

200

400

600

800

1000

1200

1400

1600

1800

640 650 660

5 0 1 6 11 16 21 26 31

5 0 1 6 11 16 21 26 31

5 0 1 6 11 16 21 26 31

15 15 15

10 10 10 200
555

100

0 1 6 11 16 21 26 31

0 1 6 11 16 21 26 31

0 1 6 11 16 21 26 31

neuron

0

0

200

400

600

800

1000

1200

1400

1600

1800

1640 1650 1660

15

15

15

10 10 10

200 5 5 5

100

0 1 6 11 16 21 26 31
frame

0 1 6 11 16 21 26 31
frame

0 1 6 11 16 21 26 31
frame

0

0

200

400

600

800

1000

1200

1400

1600

1800

820 830 840

(c) Activity per cell in motif: ground truthframe

(b) Activation of the found motifs

(top), SCC using ground truth segmentation (middle), LeMoNADe (bottom)

Figure 5: Exemplary result from one synthetic dataset. (a) shows a single plot containing all three motifs as additive RGB values for the ground truth motifs (top) and discovered motifs (bottom). The found motifs were ordered manually and temporally aligned to match the ground truth, for better readability. The complete motif sequences can be found in figure 7 in appendix C.3.1. In (b) the activations z of the found motifs are shown in red for the complete video (left) and a small excerpt of the sequence (right). The ground truth activations are marked with blue crosses. (c) shows the firing of the extracted cells in the ground truth motifs (top), the motifs identified by SCC (middle) and the motifs found with LeMoNADe (bottom).

LeMoNADe suffers from the fact that such artefacts, especially single events with extremely high neuronal activation, potentially explain a large part of the data and hence can be falsely detected as motifs. Nevertheless, these events can be easily identified by simply looking at the motif videos or thresholding the activations as done in figure 6c. Although the found motifs also include neuropil activation, this does not imply this was indeed used by the VAE as a defining feature of the motifs, just that it was also present in the images. Dendritic/axonal structures are part of the activated neurons and therefore also visible in the motif videos. If necessary, these structures can be removed by post-processing steps. As LeMoNADe reduces the problem to the short motif videos instead of the whole calcium imaging video, the neuropil subtraction becomes much more feasible.
5 CONCLUSION
We have presented a novel approach for the detection of neuronal assemblies that directly operates on the calcium imaging data, making the cumbersome extraction of individual cells and discrete spike times from the raw data dispensable. The motifs are extracted as short, repeating image sequences. This provides them in a very intuitive way and additionally returns information about the spatial distribution of the cells within an assembly.
The proposed method's performance in identifying motifs is equivalent to that of a state-of-the-art method that requires the previous extraction of individual cells. Moreover, we were able to identify repeating firing patterns in two datasets from hippocampal slice cultures, proving that the method is capable of handling real calcium imaging conditions.
For future work, a post-processing step as used in Peter et al. (2017) or a group sparsity regularization similar to the ones used in Bascol et al. (2016) or Mackevicius et al. (2018) could be added to determine a plausible number of motifs automatically. Moreover, additional latent dimensions could be introduced to capture artefacts and background fluctuations and hence automatically separate them from the actual motifs. The method is expected to, in principle, also work on other functional imaging modalities. We will investigate the possibility of detecting motifs using LeMoNADe on recordings from human fMRI or voltage-sensitive dyes in the future.
8

motif 2 motif 1 motif 0

Under review as a conference paper at ICLR 2019 DATASET 1

frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20

activation activation activation motif 2 motif 1 motif 0

activation activation activation motif 2 motif 1 motif 0

(a) Found motifs

200 150 100
50 00

500

1000

1500

2000

2500

200 150 100
50 00

500

1000

1500

2000

2500

400

200

00

500

1000

1500

2000

2500

300

200

100

00

500

1000

1500

2000

2500

frame

400

200

00

500

1000

1500

2000

2500

300

200

100

00

500

1000

1500

2000

2500

frame

(b) Activation of the found motifs

(c) Thresholded activation of the found motifs

motif 0

frame 2

frame 5

frame 8

frame 12

(d) Highlights of motif 0

DATASET 2

frame 14

frame 16

motif 2 motif 1 motif 0

frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20
(a) Found motifs

activation motif 0

activation motif 0

activation motif 1

400
200
00 60 40 20
00

200

400

600

800

1000

1200

200

400

600

800

1000

1200

activation motif 1

400
200
00 60 40 20
00

200

400

600

800

1000

1200

200

400

600

800

1000

1200

activation motif 2

activation motif 2

4

2

00

200

400

600

800

1000

1200

frame

4

2

00

200

400

600

800

1000

1200

frame

(b) Activation of the found motifs

(c) Thresholded activation of the found motifs

motif 0

frame 9

frame 11

frame 13

frame 15

(d) Highlights of motif 0

frame 17

frame 19

Figure 6: Result from hippocampal slice culture datasets 1 (top) and 2 (bottom). The colors in (a) are inverted compared to the standard visualization of calcium imaging data for better visibility. In (c) activations are thresholded to 70% of the maximum activation for each motif. In (d) the manually selected frames of motif 0 highlight the temporal structure of the motif.

9

Under review as a conference paper at ICLR 2019
REFERENCES
Misha B Ahrens, Michael B Orger, Drew N Robson, Jennifer M Li, and Philipp J Keller. Whole-brain functional imaging at cellular resolution using light-sheet microscopy. Nature methods, 2013.
Noah J. Apthorpe, Alexander J. Riordan, Rob E. Aguilar, Jan Homann, Yi Gu, David W. Tank, and H. Sebastian Seung. Automatic neuron detection in calcium imaging data using convolutional networks. In NIPS, 2016.
Kevin Bascol, Rémi Emonet, Elisa Fromont, and Jean-Marc Odobez. Unsupervised interpretable pattern discovery in time series using autoencoders. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR). Springer, 2016.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 2017.
György Buzsáki. Large-scale recording of neuronal ensembles. Nature neuroscience, 2004. Luis Carrillo-Reid, Jae-eun Kang Miller, Jordan P. Hamm, Jesse Jackson, and Rafael Yuste. Endoge-
nous sequential cortical activity evoked by visual stimuli. Journal of Neuroscience, 2015. Tsai-Wen Chen, Trevor J. Wardill, Yi Sun, Stefan R. Pulver, Sabine L. Renninger, Amy Baohan,
Eric R. Schreiter, Rex A. Kerr, Michael B. Orger, Vivek Jayaraman, Loren L. Looger, Karel Svoboda, and Douglas S. Kim. Ultrasensitive fluorescent proteins for imaging neuronal activity. Nature, 2013. Pierre Comon. Independent component analysis, a new concept? Signal processing, 1994. Pascale Cossart and Philippe J. Sansonetti. Bacterial invasion: The paradigms of enteroinvasive pathogens. Science, 2004. Anthony Christopher Davison, David Victor Hinkley, et al. Bootstrap methods and their application. Cambridge university press, 1997. Zhiwei Deng, Rajitha Navarathna, Peter Carr, Stephan Mandt, Yisong Yue, Iain Matthews, and Greg Mori. Factorized variational autoencoders for modeling audience reactions to movies. In CVPR, 2017. Winfried Denk, James H. Strickler, and Watt W. Webb. Two-photon laser scanning fluorescence microscopy. Science, 1990. Ferran Diego and Fred A Hamprecht. Learning multi-level sparse representations. In NIPS. 2013. Ferran Diego and Fred A Hamprecht. Sparse space-time deconvolution for calcium image analysis. In NIPS. 2014. Ferran Diego, Susanne Reichinnek, Martin Both, and Fred A. Hamprecht. Automated identification of neuronal activity from calcium imaging by sparse dictionary learning. ISBI, 2013. Benjamin A. Flusberg, Axel Nimmerjahn, Eric D. Cocker, Eran A. Mukamel, Robert P. J. Barretto, Tony H. Ko, Laurie D. Burns, Juergen C. Jung, and Mark J. Schnitzer. High-speed, miniaturized fluorescence microscopy in freely moving mice. Nature Methods, 2008. George L. Gerstein, Elizabeth R. Williams, Markus Diesmann, Sonja Grün, and Chris Trengove. Detecting synfire chains in parallel spike data. Journal of Neuroscience Methods, 2012. Kunal Ghosh, Laurie Burns, Eric D. Cocker, Axel Nimmerjahn, Yaniv Ziv, Abbas El Gamal, and Mark J. Schnitzer. Miniaturized integration of a fluorescence microscope. In Nature Methods, 2011. Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, and Eric P. Xing. Nonparametric variational auto-encoders for hierarchical representation learning. In ICCV, 2017. Sonja Grün. Data-driven significance estimation for precise spike correlation. Journal of Neurophysiology, 2009. Sonja Grün, Markus Diesmann, and Ad Aertsen. Unitary events in multiple single-neuron spiking activity: I. detection and significance. Neural Computation, 2002a. Sonja Grün, Markus Diesmann, and Ad Aertsen. Unitary events in multiple single-neuron spiking activity: II. nonstationary data. Neural Computation, 2002b. Tamir Hazan and Tommi Jaakkola. On the partition function and random maximum a-posteriori perturbations. In ICML, 2012. Donald O. Hebb. The Organization of Behaviour: A Neuropsychological Theory. Wiley, 1949. Fritjof Helmchen and Winfried Denk. Deep tissue two-photon microscopy. Nature Methods, 2005. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017.
10

Under review as a conference paper at ICLR 2019
Yuji Ikegaya, Gloster Aaron, Rosa Cossart, Dmitriy Aronov, Ilan Lampl, David Ferster, and Rafael Yuste. Synfire chains and cortical songs: temporal modules of cortical activity. Science, 2004.
Hakan Inan, Murat A. Erdogdu, and Mark Schnitzer. Robust estimation of neural signals in calcium imaging. In NIPS. 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.
Matthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. Composing graphical models with neural networks for structured representations and fast inference. In NIPS, 2016.
Patrick Kaifosh, Jeffrey Zaremba, Nathan B. Danielson, and Attila Losonczy. Sima: Python software for analysis of dynamic fluorescence imaging data. In Front. Neuroinform., 2014.
O. Kann, S. Schuchmann, K. Buchheim, and U. Heinemann. Coupling of neuronal activity and mitochondrial metabolism as revealed by nad(p)h fluorescence signals in organotypic hippocampal slice cultures of the rat. Neuroscience, 2003.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. Diederik P. Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-
zation trick. In NIPS, 2015. Aleksander Klibisz, Derek Rose, Matthew Eicholtz, Jay Blundon, and Stanislav Zakharenko. Fast,
simple calcium imaging segmentation with fully convolutional networks. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support - Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC, Canada, September 14, 2017, Proceedings, 2017. Yingzhen Li and Stephan Mandt. A deep generative model for disentangled representations of sequential data. arXiv preprint arXiv:1803.02991, 2018. Vitor Lopes-dos Santos, Sidarta Ribeiro, and Adriano BL Tort. Detecting cell assemblies in large neuronal populations. Journal of neuroscience methods, 2013. R. Duncan Luce. Individual Choice Behavior: A theoretical analysis. Wiley, 1959. Emily L. Mackevicius, Andrew H. Bahle, Alex H. Williams, Shijie Gu, Natalia I. Denissenko, Mark S. Goldman, and Michale S. Fee. Unsupervised discovery of temporal sequences in high-dimensional datasets, with applications to neuroscience. bioRxiv, 2018. Chris J. Maddison, Daniel Tarlow, and Tom Minka. A* sampling. In NIPS, 2014. Christopher Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2016. David Marr, David Willshaw, and Bruce McNaughton. Simple memory: a theory for archicortex. Springer, 1991. Alik Mokeichev, Michael Okun, Omri Barak, Yonatan Katz, Ohad Ben-Shahar, and Ilan Lampl. Stochastic emergence of repeating cortical motifs in spontaneous membrane potential fluctuations in vivo. Neuron, 2007. Eran A. Mukamel, Axel Nimmerjahn, and Mark J. Schnitzer. Automated analysis of cellular signals from large-scale calcium imaging data. Neuron, 2009. W. Müller, U. Misgeld, and U. Heinemann. Carbachol effects on hippocampal neurons in vitro: dependence on the rate of rise of carbachol tissue concentration. Experimental brain research, 1988. Miguel A Nicolelis, Luiz A Baccala, RC Lin, and John K Chapin. Sensorimotor encoding by synchronous neural ensemble activity at multiple levels of the somatosensory system. Science, 1995. Miguel AL Nicolelis, Erika E Fanselow, and Asif A Ghazanfar. Hebb's dream: the resurgence of cell assemblies. Neuron, 1997. Marius Pachitariu, Adam M Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, and Maneesh Sahani. Extracting regions of interest from biological images with convolutional sparse block coding. In NIPS. 2013. George Papandreou and Alan L. Yuille. Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models. In ICCV, 2011. Eva Pastalkova, Vladimir Itskov, Asohan Amarasingham, and György Buzsáki. Internally generated cell assembly sequences in the rat hippocampus. Science, 2008. Sven Peter, Elke Kirschbaum, Martin Both, Lee Campbell, Brandon Harvey, Conor Heins, Daniel Durstewitz, Ferran Diego, and Fred A Hamprecht. Sparse convolutional coding for neuronal assembly detection. In NIPS. 2017.
11

Under review as a conference paper at ICLR 2019
Eftychios A Pnevmatikakis and Liam Paninski. Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions. In NIPS. 2013.
Eftychios A Pnevmatikakis, Timothy A Machado, Logan Grosenick, Ben Poole, Joshua T Vogelstein, and Liam Paninski. Rank-penalized nonnegative spatiotemporal deconvolution and demixing of calcium imaging data. In Computational and Systems Neuroscience (Cosyne), 2013.
Eftychios A. Pnevmatikakis, Yuanjun Gao, Daniel Soudry, David Pfau, Clay Lacefield, Kira Poskanzer, Randy Bruno, Rafael Yuste, and Liam Paninski. A structured matrix factorization framework for large scale calcium imaging data analysis. arXiv:1409.2903, 2014.
Eftychios A. Pnevmatikakis, Daniel Soudry, Yuanjun Gao, Timothy A. Machado, Josh Merel, David Pfau, Thomas Reardon, Yu Mu, Clay Lacefield, Weijian Yang, Misha Ahrens, Randy Bruno, Thomas M. Jessell, Darcy S. Peterka, Rafael Yuste, and Liam Paninski. Simultaneous denoising, deconvolution, and demixing of calcium imaging data. Neuron, 2016.
Eleonora Russo and Daniel Durstewitz. Cell assemblies at multiple time scales with arbitrary lag constellations. eLife, 2017.
Justus Schneider, Andrea Lewen, Thuy-Truc Ta, Lukas V. Galow, Raffaella Isola, Ismini E. Papageorgiou, and Oliver Kann. A reliable model for gamma oscillations in hippocampal tissue. Journal of Neuroscience Research, 2015.
Wolf Singer. Synchronization of cortical activity and its putative role in information processing and learning. Annual review of physiology, 1993.
Amit Singhal. Modern information retrieval: A brief overview. IEEE Data Eng. Bull., 2001. Paris Smaragdis. Non-negative matrix factor deconvolution; extraction of multiple sound sources
from monophonic inputs. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 2004. Quico Spaen, Dorit S Hochbaum, and Roberto Asín-Achá. Hnccorr: A novel combinatorial approach for cell identification in calcium-imaging movies. arXiv:1703.01999, 2017. Artur Speiser, Jinyao Yan, Evan W Archer, Lars Buesing, Srinivas C Turaga, and Jakob H Macke. Fast amortized inference of neural activity from calcium imaging data with variational autoencoders. In NIPS. 2017. Benjamin Staude, Sonja Grün, and Stefan Rotter. Higher-order correlations in non-stationary parallel spike trains: statistical modeling and inference. Frontiers in Computational Neuroscience, 2010a. Benjamin Staude, Stefan Rotter, and Sonja Grün. Cubic: cumulant based inference of higher-order correlations in massively parallel spike trains. Journal of Computational Neuroscience, 2010b. Ian H Stevenson and Konrad P Kording. How advances in neural recording affect data analysis. Nature neuroscience, 2011. John I. Yellott. The relationship between luce's choice axiom, thurstone's theory of comparative judgment, and the double exponential distribution. Journal of Mathematical Psychology, 1977. Pengcheng Zhou, Shanna L Resendez, Jose Rodriguez-Romaguera, Jessica C Jimenez, Shay Q Neufeld, Andrea Giovannucci, Johannes Friedrich, Eftychios A Pnevmatikakis, Garret D Stuber, Rene Hen, Mazen A Kheirbek, Bernardo L Sabatini, Robert E Kass, and Liam Paninski. Efficient and accurate extraction of in vivo calcium signals from microendoscopic video data. eLife, 2018.
12

Under review as a conference paper at ICLR 2019

APPENDIX

A VARIATIONAL AUTOENCODER

Variational autoencoder (VAE) are generative latent variable models which were first described

in Kingma & Welling (2014). The data x =

x(i)

N i=1

,

consisting

of

N

samples

of

some

random

variable x, is generated by first drawing a latent variable z(i) from a prior distribution p(z) and

then sampling from the conditional distribution p (x | z) with parameters . The distribution

p (x | z) belongs to the parametric family p(x | z) with differentiable PDFs w.r.t.  and z. Both

the true parameters  as well as the latent variables z(i) are unknown. We are interested in an

approximate posterior inference of the latent variables z given some data x. The true posterior

p(z | x), however, is usually intractable. But it can be approximated by introducing the recognition model (or approximate posterior) q(z | x). We want to learn both the recognition model parameters

 as well as the generative model parameters . The recognition model is usually referred to as the

probabilistic encoder and p(x | z) is called the probabilistic decoder.

In order to learn the variational parameters  we want to minimise the KL-divergence between approximate and true posterior KL(q(z|x) p(z|x)). Therefore we use the fact that the marginal likelihood p(x) can be written as

log p(x) = L(p, q; x) + KL q(z|x) p(z|x)

(8)

As the KL-divergence is non-negative, we can minimize KL q(z|x) p(z|x) by maximizing the (variational) lower bound L(p, q; x) with

L(p, q; x) = Ezq(z|x) [log p(x|z)] - KL q(z|x) p(z) .

(9)

In order to optimise the lower bound L(p, q; x) w.r.t. both the variational parameters  and the

generative parameters , we need to compute the gradients

,L(p, q; x) = ,Ezq(z|x) [log p(x|z)] - ,KL q(z|x) p(z) .

(10)

For the first part of the lower bound the gradient w.r.t.  can be easily computed using Monte Carlo

sampling

Ezq(z|x) [log p(x|z)]

=

Ezq(z|x) [ log p(x|z)]



1 S

S

 log p(x|zs)

s=1

(11)

with zs  q(z|x). The gradient w.r.t. , however, does not take the form of an expectation in z and can therefore not be sampled that easily:

Ezq(z|x) [log p(x|z)] =  q(z|x) log p(x|z)dz = log p(x|z)q(z|x)dz .

(12)

However, in most cases we can use the reparameterisation trick to overcome this problem: the random variable z~  q(z | x) can be reparameterised using a differentiable transformation h(, x) of a noise variable  such that

z~ = h(, x) with   p() We now can compute the gradient w.r.t.  again using Monte Carlo sampling

(13)

Ep() [log p(x|z = h(, x))] = Ep() [ log p(x|z = h(, x))]

1 S

S

 log p(x|zs = h(s, x))

s=1

(14)

with s  p(). Hence, the reparameterized lower bound L~(p, q; x)  L(p, q; x) can be written as

L~(p, q; x) = 1 S

S

log p(x|zs) - KL(q(z | x)||p(z))

s=1

(15)

with zs = h(s, x),   p(). The first term on the RHS of eq. (15) is a negative reconstruction error, showing the connection to traditional autoencoders, while the KL-divergence acts as a regularizer on

the approximate posterior q(z | x).

13

Under review as a conference paper at ICLR 2019

B LEMONADE NETWORK ARCHITECTURE AND IMPLEMENTATION DETAILS

B.1 ENCODER
The encoder network starts with a few convolutional layers with small 2D filters operating on each frame of the video separately, inspired by the architecture used in Apthorpe et al. (2016) to extract cells from calcium imaging data. The details of this network are shown in table 1. Afterwards the feature maps of the whole video are passed through a final convolutional layer with 3D filters. These filters have size of the feature maps gained from the single images times a temporal component of length F , which is the expected maximum temporal extent of a motif. We use 2 · M filters and apply padding in the temporal domain to avoid edge effects. By this also motifs that are cut off at the beginning or the end of the sequence can be captured properly. The output of the encoder are 2 · M feature maps of size (T + F - 1) × 1 × 1.

B.2 REPARAMETERIZATION

Instead of reparameterizing the Bernoulli distributions, we will reparameterize their BinConcrete
relaxations. The BinConcrete relaxation of a Bernoulli distribution with parameter  takes as input
parameter ~ = /(1 - ). Maddison et al. (2016) showed that instead of using the normalized probabilities , we can also perform the reparametrization with unnormalized parameters 1 and 2, where 1 is the probability to sample a one and 2 is the probability to sample a zero and ~ = 1/2.

The first M feature maps, which were outputted by the encoder, are assigned to contain the unnor-

malised probabilities m1 ,t for the activation of motif m in frame t to be one. The second M feature

maps contain the unnormalized probabilities m2 ,t for the activation of motif m in frame t to be

zero. The parameter ~ that is needed for the reparameterized BinConcrete distribution is obtained

by dividing the two vectors elementwise: ~tm = m1 ,t/m2 ,t. We use the reparameterization trick

to sample from BinConcrete(~tm) as follows: First we sample

{Utm}tT=+1F -1

M
from a uniform
m=1

distribution Uni(0, 1). Next, we compute y with

ytm =

~tm · Utm 1/1 1 - Utm

.

(16)

Finally, we gain z according to

ztm

=

ytm 1 + ytm

· m1 ,t

(17)

for all m = 1, . . . , M and t = 1, . . . , T + F - 1. The multiplication by m1 ,t in eq. (17) is not part of the original reparametrization trick (Maddison et al., 2016; Jang et al., 2017). But we found that the results of the algorithm improved dramatically as we scaled the activations with the 1-values
that were originally predicted from the encoder network.

B.3 DECODER
The input to the decoder are now the activations z. The decoder consists of a single deconvolution layer with M filters of the original frame size times the expected motif length F . These deconvolution filters contain the motifs we are looking for.
The details of the used networks as well as the sizes of the inputs and outputs of the different steps are shown in table 1. Algorithm 1 summarizes the reparametrization and updates.

C EXPERIMENTS AND RESULTS ON SYNTHETIC DATA
C.1 SYNTHETIC DATA GENERATION
We created 200 artificial sequences of length 60 s with a frame rate of 30 fps and 128 × 128 pixel per image. The number of cells was varied and they were located randomly in the image plane with an overlap of up to 30 %. The cell shapes were selected randomly from 36 shapes extracted from

14

Under review as a conference paper at ICLR 2019

Table 1: LeMoNADe network architecture details

Operation

Kernel

Feature maps Padding

Stride Nonlinearity

Input: T images, P × P

2D Convolution

3×3

24

0×0

1 ELU

2D Convolution

3×3

48

0×0

1 ELU

Max-Pooling

2×2

­

0×0

2­

2D Convolution

3×3

72

0×0

1 ELU

2D Convolution

3×3

96

0×0

1 ELU

Max-Pooling

2×2

­

0×0

2­

2D Convolution

3×3

120

0×0

1 ELU

2D Convolution

1×1

48

0×0

1 ELU

Output: T images, P~ × P~ , P~ = ((P - 4)/2 - 4)/2 - 2, P~ = ((P - 4)/2 - 4)/2 - 2

3D Convolution

Input: 1 video, T × P~ × P~

F × P~ × P~ 2M

(F - 1) × 0 × 0 1

Output: 2M feature maps, (T + F - 1) × 1 × 1

SoftPlus

Reparametrization

Input: 2M feature maps, (T + F - 1) × 1 × 1

­­

­

Output: M activations, (T + F - 1) × 1 × 1

­

­

Input: M activations, (T + F - 1) × 1 × 1

3D TransposedConvolution F × P × P M

(F - 1) × 0 × 0 1

Output: 1 video, T × P × P

ReLU

Algorithm 1: The LeMoNADe algorithm
Input: raw video x, normalized to zero mean and unit variance, architectures f, , hyperparameter 1, 2, a~, KL
Result: trained f, 
,   Initialize network parameters repeat
// Sample subset of video xsub  Randomly chosen sequence of consecutive frames from x // Encoding step
Encode xsub to get ~ as described in section B.1 and B.2 // Latent Step Sample noise U  Uni(0, 1) Compute y following eq. (16) Compute z following eq. (17)
// Decoding Step xsub  decode via f(z) // Update Parameters
Compute gradients of loss ,   update via , (xsub, xsub, ~, 1, a~, 2, KL) (see eq. (7) in the main paper) until until convergence of , ;

real data. The transients were modelled as two-sided exponential decay with scales of 50 ms and 400 ms, respectively. In contrast to Diego & Hamprecht (2013), we included neuronal assemblies with temporal firing structure. That means cells within an assembly can perform multiple spikes in a randomly chosen but fixed motif of temporal length up to 30 frames. We used 3 different assemblies in each sequence. The assembly activity itself was modelled as a Poisson process (Lopes-dos Santos et al., 2013) with a mean of 0.15 spikes/second and a refractory period of at least the length of the motif itself. By construction the cell locations as well as the firing motifs are known for these datasets. In order to simulate the conditions in real calcium imaging videos as good as possible, we added Gaussian background noise with a relative amplitude (max intensity - mean intensity)/noise between 10 and 20. Additionally, spurious spikes not belonging to any motif were added. The amount
15

Under review as a conference paper at ICLR 2019

Table 2: Average cosine similarity between ground truth and discovered motifs. The average similarity together with the standard deviation were computed over 20 different datasets for each noise level, both for LeMoNADe and SCC. A bootstrap distribution of similarities was computed (see section C.4). BS-95 gives the 5% significance threshold of this distribution.

NOISE LEVEL
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%

on video data

LeMoNADe
0.838 ± 0.066 0.826 ± 0.061 0.804 ± 0.080 0.770 ± 0.130 0.775 ± 0.107 0.756 ± 0.079 0.730 ± 0.098 0.639 ± 0.142 0.462 ± 0.103 0.357 ± 0.034

BS-95 0.400 0.387 0.402 0.413 0.426 0.477 0.492 0.516 0.553 0.656

after cell extraction
SCC 0.837 ± 0.088 0.826 ± 0.116 0.818 ± 0.120 0.830 ± 0.125 0.822 ± 0.093 0.791 ± 0.126 0.731 ± 0.169 0.636 ± 0.163 0.454 ± 0.135 0.351 ± 0.067

of spurious spikes was varied from 0% up to 90% of all spikes in the dataset. For each of the 10 noise levels 20 datasets were generated.

C.2 SIMILARITY MEASURE

The performance of the algorithms is measured by computing the cosine similarity (Singhal, 2001) between ground truth motifs and found motifs. The found motifs are in an arbitrary order, not necessarily corresponding to the order of the ground truth motifs. Additionally, the found motifs can be shifted in time compared to the ground truth. To account for this fact, we compute the similarity between the found motifs and each of the ground truth motifs with all possible temporal shifts and take the maximum. Hence, the similarity between the m-th found motif and the set of ground truth motifs G is defined by

 s



 Sim(Mm, G) = max

vec(Mm), vec( G )
s

 G  G, s  {-F, . . . , F }

 vec(Mm) 2 · vec( G ) 2



(18)

where Mm is the m-th found motif, ·, · is the dot product and vec(·) vectorizes the motifs with dimensions F × N into a vector of length F · N , where N is the number of cells. The shift
s
operator (·) moves a motif s frames forward in time while keeping the same size and filling missing values appropriately with zeros (Smaragdis, 2004).
The cosine similarity of the found motifs to the set of ground truth motifs was averaged over all found motifs and all experiments for each noise level. The average similarities achieved with LeMoNADe and SCC as well as the 5% significance threshold of the BS distribution for each noise level can be found in table 2.

C.3 PARAMETER SETTINGS
LeMoNADe is not more difficult to apply than other motif detection methods for neuronal spike data. In our experiments, for most of the parameters the default settings worked well on different datasets and only three parameters need to be adjusted: the maximum number of motifs M , the maximum motif length F , and one of the sparsity parameters (e.g. a~ or KL). For SCC the user also has to specify three similar parameters. In addition, SCC requires the previous extraction of a spike matrix which implies many additional parameters.
Table 3 shows the parameter settings used for the experiments shown in the paper.

16

Under review as a conference paper at ICLR 2019

Table 3: Parameters used for the shown experiments. M is the number of motifs, F the maximum
temporal extent of a motif, 1 and 2 are the temperatures for the relaxed approximate posterior and prior distributions, a~ is the location of the BinConcrete prior, b is the number of consecutive frames
analysed in each epoch, and KL is the weight of the KL-regularization term in the loss function. e is the ensemble-penalty used in SCC.

LeMoNADe on synth. datasets with noise level < 50% LeMoNADe on synth. datasets with noise level  50% LeMoNADe on real dataset 1
LeMoNADe on real dataset 2

M
3 3 3 3

F a~
31 0.05 31 0.10 21 0.05 21 0.01

1 2 #epochs learning rate b KL

0.6 0.5 5000 0.6 0.5 5000 0.4 0.3 5000 0.6 0.5 5000

10-5 10-5 10-5 10-5

500 0.10 500 0.10 150 0.10 500 0.01

SCC on synth. datasets

M F e 3 31 10-4

#epochs #inits 10 1

C.3.1 OVER- AND UNDER-ESTIMATION OF THE MAXIMUM NUMBER OF MOTIFS M
In practical settings the number of motifs in a video is unknown. We use our synthetic data with 3 true motifs and use LeMoNADe with underestimated (M = 1), correct (M = 3) and overestimated (M = 5) number of expected motifs. Figure 7 shows the complete ground truth (figure 7a) and found motifs for the exemplary synthetic dataset discussed in the paper. Besides the results for M = 3 (figure 7c) we also show the found motifs for M = 1 (figure 7b) and M = 5 (figure 7d). If the number of motifs is underestimated (M = 1) only one of the true motifs is captured. When the number of motifs is overestimated (M = 5) the correct motifs are identified and the surplus filters are filled with (shifted) copies of the true motifs and background noise. Hence, when the correct number of motifs is unknown (as expected for real datasets) one can overestimate the number of motifs. The result will capture the true motifs plus some copies of them. In future work, a post-processing step as in Peter et al. (2017) or a group sparsity regularization as in Bascol et al. (2016) and Mackevicius et al. (2018) could be introduced to eliminate these additional copies automatically. Background noise could be easily identified as no motif by either looking at the motif videos or thresholding the found activations. In future extends of the model we will study the effect of additional latent dimensions for background noise to automatically separate it from actual motifs.
C.3.2 OVER- AND UNDER-ESTIMATION OF THE MAXIMUM MOTIF LENGTH F
If the maximum motif length F is underestimated the found motifs will just contain the part of the motif that reduces the reconstruction error most. Hence in most cases the most interesting parts of the motifs will be captured but details at either end of the motifs could be lost. If the motif length is overestimated, the motifs can be captured completely but might be shifted in time. This shift, however, will be compensated by the motif activations and hence has no negative effect on the results. In our experiments we achieved good results with a generously chosen motif length. For this reason we recommend to overestimate the motif length.
C.3.3 SPARSITY PARAMETER
The parameter a~ influences the sparsity of the found activations. Smaller values of a~ will penalize activations harder and hence often result in cleaner and more meaningful motifs. However, if a~ is too small it will suppress the activations completely. For this reason we recommend to perform for each new dataset experiments with different values of a~. Changing the value of KL is another option to regulate the sparsity of the activations. However, in our experiments we found that the default value of KL = 0.1 worked well for many different datasets and varying a~ was effective enough. For the temperature parameters the default values 1 = 0.6 and 2 = 0.5 worked well in most cases and changing them is usually not necessary.
C.4 BOOTSTRAP-BASED SIGNIFICANCE TEST
Statistical methods for testing for cell assemblies (or spatio-temporal patterns more generally) have been advanced tremendously in recent years, addressing many of the issues that have plagued older approaches (Grün, 2009; Staude et al., 2010a;b; Russo & Durstewitz, 2017). Simple shuffle bootstraps are not necessarily the best methods if they destroy too much of the auto-correlative structure, and
17

motif 2 motif 1 motif 0

Under review as a conference paper at ICLR 2019 18

motif 0

motif 2 motif 1 motif 0

motif 4 motif 3 motif 2 motif 1 motif 0

frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20 frame 21 frame 22 frame 23 frame 24 frame 25 frame 26 frame 27
(a) Ground truth motifs
frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20 frame 21 frame 22 frame 23 frame 24 frame 25 frame 26 frame 27 frame 28 frame 29 frame 30
(b) Found motifs for M = 1
frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20 frame 21 frame 22 frame 23 frame 24 frame 25 frame 26 frame 27 frame 28 frame 29 frame 30
(c) Found motifs for M = 3
frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20 frame 21 frame 22 frame 23 frame 24 frame 25 frame 26 frame 27 frame 28 frame 29 frame 30
(d) Found motifs for M = 5 Figure 7: Results from the exemplary synthetic dataset discussed in the paper. (a) shows the three ground truth motifs. We also show the results of our analysis with fixed motif length (F = 31) for the different numbers of motifs M = 1 (b), M = 3 (c) and M = 5 (d).

Under review as a conference paper at ICLR 2019

0% noise 10% noise 20% noise 30% noise 40% noise 50% noise 60% noise 70% noise 80% noise 90% noise

1200 1000

1000

1200 1000

1200 1000

1200 1000

1000 800

800

800

800

800

800 800 800 800 800 600 600 600 600 600

600 400

600 400

600 400

600 400

600 400

400

400

400

400

400

200 200 200 200 200 200 200 200 200 200

0000000000 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

12 14 12

12

10 8 6 4

12 10
8 6 4

10 8 6 4

12 10
8 6 4

10 8 6 4

14 12 10
8 6 4

10 8 6 4

2222222

8 10 25 8 20
6 6 15
4 4 10
225

0000000000

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

similarity

similarity

similarity

similarity

similarity

similarity

similarity

similarity

similarity

similarity

Figure 8: Top: Bootstrap distribution for similarity between random patterns. Shown is a sample from the BS distribution (blue) and the 95% significance threshold (red). Bottom: Distribution for similarity between patterns found on data which contained repeating motifs. Shown are the similarities between motifs found with LeMoNADe (lime green) and the ground truth motifs for the synthetic datasets discussed in the paper, which contained repeating motifs. The 95% significance threshold of the corresponding BS distribution is indicated as vertical red line.

they can severely underestimate the distributional tails (Davison et al., 1997). Therefore we use sophisticated parametric, model-based bootstraps which retain the full statistical structure of the original data, except for the crucial feature of repeating motifs.
In order to provide a 'null hypothesis (H0)' reference for the motif similarities returned by LeMoNADe (or other methods), we used the following bootstrap (BS) based test procedure: We generated 20 datasets analogue to those described in section C.1, i.e. with same spiking statistics and temporal convolution with calcium transients, but without repeating motifs. These motif-less H0 datasets were then processed by LeMoNADe in the very same way as the motif-containing datasets, i.e. with the parameter settings as shown in table 3. From each of these BS datasets 150 random samples of the same temporal length as that of the 'detected' motifs were drawn. For each BS dataset, the similarities between each of the found motifs and all of the 150 random samples were computed as described in section C.2. As datasets with higher noise levels have different spiking statistics, we repeated this procedure for each of the ten noise levels.
Figure 8 shows the BS distributions (top). We also show the distribution of similarities between motifs found with LeMoNADe on the datasets which contained motifs (bottom). The 95%-tile (corresponding to a 5% alpha level) of the BS distribution is displayed as vertical red line. Up to a noise level of 70% the average of the similarities found on the datasets that contained motifs is much higher than the 95%-tile of the BS distribution.
D EXPERIMENTS AND RESULTS ON REAL DATA
D.1 DATA GENERATION
Organotypic hippocampal slice cultures were prepared from 7­9-day-old Wistar rats (Charles River Laboratories, Sulzfeld, Germany) as described by Kann et al. (2003) and Schneider et al. (2015). Animals were taken care of and handled in accordance with the European directive 2010/63/EU and with consent of the animal welfare officers.
Slices were infected with adeno-associated virus (AAV) obtained from Penn Vector Core (PA, USA) encoding GCaMP6f under the control of the CamKII promoter AAV5.CamKII.GCaMPf.WPRE.SV40, Lot # V5392MI-S). AAV transduction was achieved, under sterile conditions, by applying 0.5µl of the viral particles solution (qTiter: 1.55e13 GC/ml) on top of the slices.
Slices were maintained on Biopore membranes (Millicell standing inserts; Merck Millipore, Schwalbach, Germany) between culture medium. The medium consisted of 50% minimal essential medium, 25% Hank's balanced salt solution (Sigma-Aldrich, Taufkirchen, Germany), 25% horse serum (Life Technologies, Darmstadt, Germany), and 2mM L-glutamine (Life Technologie) at pH 7.3, stored in an incubator (Heracell; Thermoscientific, Dreieich, Germany) with humidified normal atmosphere (5% CO2, 36.5C). The culture medium (1 ml) was replaced three times per week.
19

Under review as a conference paper at ICLR 2019

motif 0

frame 0

frame 1

frame 2

frame 3

frame 4

frame 5

frame 6

frame 7

frame 8

frame 9

frame 10

frame 11

frame 12

frame 13

frame 14

frame 15

frame 16

frame 17

frame 18

frame 19

frame 20

(a) Difference between motif 0 found on real dataset 1 and the constructed synchronous firing pattern.

motif 0

frame 0

frame 1

frame 2

frame 3

frame 4

frame 5

frame 6

frame 7

frame 8

frame 9

frame 10

frame 11

frame 12

frame 13

frame 14

frame 15

frame 16

frame 17

frame 18

frame 19

frame 20

(b) Difference between motif 0 found on real dataset 2 and the constructed synchronous firing pattern.

Figure 9: Color-coded difference between discovered motifs and intensity modulated synchronous firing. Red color indicates negative differences, blue positive differences and white zero difference. The fact that for both datasets in motif 0 some cells are displayed in red over multiple frames shows that these motifs contain temporal structure beyond mere spiking synchrony.

Artificial cerebrospinal fluid used for imaging was composed of 129 mM NaCl, 3 mM KCl, 1.25 mM NaH2PO4, 1.8 mM MgSO4, 1.6 mM CaCl2, 21 mM NaHCO3, and10 mM glucose (Sigma-Aldrich, Taufkirchen, Germany). The pH of the recording solution was 7.3 when it was saturated with the gas mixture (95% O2, 5% CO2). Recording temperature was 32 ± 1C. Constant bath wash of 20µM (dataset 1) and 10µM (dataset 2) carbachol (Sigma-Aldrich) was performed to enhance neuronal activity and increase firing probability during imaging (Müller et al., 1988).
Imaging of CA3 region of the hippocampus was performed on day 29 with 20x magnification (dataset 1) and on day 30 with 10x magnification (dataset 2) in vitro (23 days post viral infection) from slices maintained in submerged chamber of Olympus BX51WI microscope. GCaMP6f was excited at 485 ± 10nm. Fluorescence images (emission at 521 ± 10nm) were recorded at 6.4Hz (dataset 1) and 4Hz (dataset 2) using a CCD camera (ORCA-ER; Hamamatsu Photonics, Hamamatsu City, Japan). Before running the analysis we computed F/F for the datasets. In order to perform the computations more efficiently, we cropped the outer parts of the images containing no interesting neuronal activity and downsampled dataset 2 by a factor of 0.4.

D.2 TEMPORAL STRUCTURE PLOTS

In order to show that the motifs 0 found in the two real datasets contain temporal structure, we
compare them to what the synchronous activity of the participating cells with modulated amplitude would look like. The synchronous firing pattern was constructed as follows: First, for the motif Mm with m = 1, . . . , M the maximum projection Pm at each pixel p = 1, . . . , P · P over time was
computed by

Ppm

=

max
f

Mpm

with f = 1, . . . , F

(19)

and normalized

P~pm

=

Ppm maxp Pm

.

(20)

Finally, the synchronous firing pattern Sm for motif m is gained by multiplying this normalized maximum projection at each time frame f with the maximum intensity of motif m at that frame:

Sfm

=

P~m

·

max
p

Mmf

for f = 1, . . . , F

.

(21)

Figures 9 shows the difference between the found motif and the constructed synchronous firing patterns for the motifs found on the two real datasets.

D.3 INFLUENCE OF MOTIF LENGTH AND NUMBER OF MOTIFS
The datasets were also analyzed with different motif lengths F and with different motif numbers M . All other parameters were fixed as shown in table 3. Figure 10 shows the found motifs on dataset 1 with M = 3 and for the different motif lengths F = 21 and F = 31. The results are highly similar. In both cases, the interesting pattern (motif 0 in figure 10a and motif 1 in figure 10b, respectively) is captured. Figure 11 shows the found motifs on dataset 1 for fixed F = 21 but the different numbers

20

Under review as a conference paper at ICLR 2019

Table 4: Attached video files and descriptions. The used parameters for the analysis are the same as given in table 3 if not mentioned differently. The three different types of video are: motif showing a single motif; parallel video showing the original video from the dataset (upper left corner) and reconstructions from the found motifs; and RGB video showing a superposition of RGB values of the reconstructed videos from the three motifs found on the dataset. Additionally to the synthetic data example discussed in the paper (with 10% noise spikes), we also provide videos from a synthetic dataset with 50% spurious spikes.

File name
real_1_e1_l21_recon.mp4 real_1_e3_l21_motif_0.tiff real_1_e3_l21_motif_1.tiff real_1_e3_l21_motif_2.tiff real_1_e3_l21_recon.mp4 real_1_e3_l21_rgb.mp4 real_1_e3_l31_recon.mp4 real_1_e5_l21_recon.mp4 real_2_e3_l21_motif_0.tiff real_2_e3_l21_motif_1.tiff real_2_e3_l21_motif_2.tiff real_2_e3_l21_recon.mp4 real_2_e3_l21_rgb.mp4 synth_example_e3_l21_motif_0.tiff synth_example_e3_l21_motif_1.tiff synth_example_e3_l21_motif_2.tiff synth_example_e3_l21_recon.mp4 synth_example_e3_l21_rgb.mp4 synth_50noise_e3_l21_recon.mp4 synth_50noise_e3_l21_rgb.mp4

dataset
real dataset 1 real dataset 1 real dataset 1 real dataset 1 real dataset 1 real dataset 1 real dataset 1 real dataset 1 real dataset 2 real dataset 2 real dataset 2 real dataset 2 real dataset 2 synth. example synth. example synth. example synth. example synth. example synth. with 50% noise synth. with 50% noise

video type
parallel video motif motif motif
parallel video RGB video parallel video parallel video
motif motif motif parallel video RGB video motif motif motif parallel video RGB video parallel video RGB video

number of motifs M
1 3 3 3 3 3 3 5 3 3 3 3 3 3 3 3 3 3 3 3

motif length F
21 21 21 21 21 21 31 21 21 21 21 21 21 21 21 21 21 21 21 21

of motifs M = 1, 2, 3, 5. When the number is limited (as for M = 1), the model is expected to learn those motifs first which best explain the data. The motif shown in figure 11a also appears if M is increased. This shows that this motif is highly present in the data. However, as long as only one filter is available the motif also contains a lot of background noise. The second filter in figure 11b contains an high luminosity artefact of the data. Although easily identifiable as no firing motif from a neuronal assembly, with its high luminosity and large spacial extent, it explains a lot of the dataset. If the number of motifs is further increased to M = 3 (see figure 11c), more background noise is captured in the additional filter and the motif becomes cleaner. When the number of motifs is further increased to M = 5, no new motifs appear and the surplus two filters seem to be filled up with parts of the structures which were already present in 11c.
E MOTIF VIDEOS
In order to give the reader a better impression of what the used data and the motifs extracted as short video sequences would look like, we provide a few video files containing extracted motifs, analyzed data and reconstructed videos at https://drive.google.com/drive/folders/ 19F76JLn490RzZ4d7GxbWZoq6RdF2nt3w?usp=sharing.
The reconstructed videos are gained by convolving the found motifs with the corresponding found activations. The videos are provided either in TIFF or MP4 format. Table 4 shows the names of the files together with short descriptions what each video shows. The videos corresponding to the synthetic dataset were generated with a frame rate of 30 fps and those corresponding to the real dataset with 10 fps.

21

Under review as a conference paper at ICLR 2019 22

motif 2 motif 1 motif 0 motif 2 motif 1 motif 0

frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20
(a) Found motifs for F = 21
frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20 frame 21 frame 22 frame 23 frame 24 frame 25 frame 26 frame 27 frame 28 frame 29 frame 30
(b) Found motifs for F = 31 Figure 10: Results from dataset 1 with fixed number of motifs (M = 3) for the different motif lengths (a) F = 21 and (b) F = 31.

motif 0

motif 1 motif 0

Under review as a conference paper at ICLR 2019 23

motif 2 motif 1 motif 0

frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20
(a) Found motif for M = 1
frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20
(b) Found motifs for M = 2
frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20
(c) Found motifs for M = 3
frame 0 frame 1 frame 2 frame 3 frame 4 frame 5 frame 6 frame 7 frame 8 frame 9 frame 10 frame 11 frame 12 frame 13 frame 14 frame 15 frame 16 frame 17 frame 18 frame 19 frame 20
(d) Found motifs for M = 5 Figure 11: Results from dataset 1 with fixed motif length (F = 21) for the different numbers of motifs (a) M = 1, (b) M = 2, (c) M = 3, and (d) M = 5.

motif 4 motif 3 motif 2 motif 1 motif 0

