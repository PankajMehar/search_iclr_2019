Under review as a conference paper at ICLR 2019
INVASE: INSTANCE-WISE VARIABLE SELECTION USING NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
The advent of big data brings with it data with more and more dimensions and thus a growing need to be able to efficiently select which features to use for a variety of problems. While global feature selection has been a well-studied problem for quite some time, only recently has the paradigm of instance-wise feature selection been developed. In this paper, we propose a new instance-wise feature selection method, which we term INVASE. INVASE consists of 3 neural networks, a selector network, a predictor network and a baseline network which are used to train the selector network using the actor-critic methodology. Using this methodology, INVASE is capable of flexibly discovering feature subsets of a different size for each instance, which is a key limitation of existing state-of-the-art methods. We demonstrate through a mixture of synthetic and real data experiments that INVASE significantly outperforms state-of-the-art benchmarks.
1 INTRODUCTION
High-dimensional data is becoming more readily available, and it brings with it a growing need to be able to efficiently select which features to use for a variety of problems. When doing predictions, it is well known that using too many variables with too few samples can lead to overfitting, which can significantly hinder the performance of predictive models. In the realm of interpretability, the large dimensionality of the data is often too much information to present to a human who may be using the machine learning model as a support system. Understanding which features are most relevant to an outcome or to a model output is an important first step in improving predictions and interpretability and many works exist that tackle feature selection on a global level. However, in the heterogeneous data we typically encounter, the prediction made by a model (and indeed the true label) may rely on a different subset of the features for different subgroups within the data [13]. In this paper we propose a novel instance-wise feature selection method, INVASE (INstance-wise VAriable SElection), which attempts to learn which subset of the features is relevant for each sample, allowing us to display the minimal information required to explain each prediction and also to reduce overfitting of predictive models.
Discovering a global subset of relevant features for a particular task is a well-studied problem and there are several existing methods for solving it such as Sequential Correlation Feature Selection [10], Mutual Information Feature Selection [19], Knockoff models [3], and more [9; 15]. However, global feature selection suffers from a key limitation - the features discovered by global feature selection are the same for all samples. In many cases, in particular when populations are highly heterogeneous, the relevant features may differ across samples. For instance, different patient subgroups have different relevant features for predicting heart failure [13]. Instance-wise feature selection methods such as [4; 24] instead try to discover the features that are relevant for each sample. When the goal is to provide an interpretable explanation of the predictions made, a key challenge is in ensuring that we do not over-explain by providing too much information (i.e. choosing too many features). Naturally, by performing feature selection on an individualized level we are able to select features that are more relevant to each sample, rather than having to choose the top k features globally, which may not explain the predictions for some samples very well, but simply perform well on average across all samples.
In this paper, we propose a novel instance-wise feature selection method which we term INVASE. We draw influence from actor-critic models [20] to solve the problem of backpropagating through
1

Under review as a conference paper at ICLR 2019

subset sampling. Our model consists of 3 neural networks: a selector network, a predictor network and a baseline network. During training, each of these are trained iteratively, with the selector network being trained to minimize a Kullback-Leibler (KL) divergence between the full conditional distribution and the selected-features-only conditional distribution of the outcome. Our model is capable of discovering a different number of relevant variables for each sample which is a key limitation in existing instance-wise approaches (such as [4]). We show significant improvements over the state-of-the-art in both synthetic data and real-word data in terms of true positive rates, false discovery rates, and show better predictive performance with respect to several prediction metrics. Our model can also be easily extended to handle both continuous and discrete outputs and time-series inputs (see the Appendix for details).
1.1 RELATED WORKS
There are many existing works on global variable selection (see [9] for a good summary paper). [19] and [10] use max-dependency min-redundancy criteria [16] with mutual information and Pearson correlation, respectively. [3] uses multiple hypothesis testing for global variable selection. As noted above, these global selection methods are not capable of learning sample-specific relevance.
Instance-wise variable selection is also closely related to model interpretation methods. Some previous works are based on backpropagation from the output of the predictive model to the input variables [25]. DeepLIFT [24] decomposes the output of the neural network on a reference input to compute the contribution of each input variable. However, both methods need white-box access to the pre-trained predictive models to compute the gradient and decomposition. [2] approximates the predictive models using a Parzen window approximator when there is only black-box access to the predictive models. Some other works are based on input perturbation such as [1], [14], [26] and [5]. [17] uses Shapley values to compute the variable importance, and [22] uses locally linear models to explain the linear dependency for each sample. [18] tries to interpret tree ensemble models using Shapley values but cannot generalize to other predictive models such as neural networks.
Our work is most closely related to L2X (Learning to Explain) [4]. However, there are 3 key differences between our work and theirs. In L2X, they try to maximize a lower bound of the mutual information between the target Y and the selected input variables XS. In contrast, we try to minimize the KL divergence between the conditional distributions Y |X and Y |XS. In order to be able to backpropagate through subset sampling, L2X use the Gumbel-softmax trick [12] to approximately discretize the continuous outputs of the neural network. In our work, we use methods from actorcritic models [20] to bypass backpropagation through the sampling and instead use the predictor network to provide a reward to the selector network. Finally, due to the Gumbel-softmax used in L2X, the number of variables to be detected must be fixed in advance and is necessarily the same for every sample. The actor-critic methodology used in our model has no such limitations and so we are able to flexibly select a different number of relevant variables for each sample and instead induce sparsity using an l0 penalty term. In fact, using the actor-critic methodology allows us to directly use the l0 penalty term (which is not differentiable and therefore not practical to use in general). A summary table highlighting the key features of all of the related works can be found in the Appendix.

2 PROBLEM FORMULATION

Let X = X1 × ... × Xd be a d-dimensional feature space and Y = {1, ..., c} be a discrete label space1. Let X = (X1, ..., Xd)  X and Y  Y be random variables with joint density (or mass) p and marginal densities (or masses) pX and pY respectively. We will refer to s  {0, 1}d as the
selection vector, where si = 1 will indicate that variable i is selected, and si = 0 will indicate that variable i is not selected. Let  be any point not in any of the spaces X1, ..., Xd and define Xi = Xi  {} and X  = X1 × ... × Xd. Given x  X we will write x(s) to denote the suppressed feature vector defined by

xi(s) =

xi if si = 1  if si = 0

so that  represents that a feature is not selected.

1In this paper we focus on classification; we discuss an extension of our model to regression in the Appendix.

2

Under review as a conference paper at ICLR 2019

In the global feature selection literature, the goal is to find the smallest s (i.e. the one with fewest 1s) such that E(Y |X(s)) = E(Y |X), or equivalently such that the conditional distribution of Y given X(s) is the same as Y given all of X. Note that this definition is given fully in terms of random
variables, rather than realizations of those random variables.

In contrast, our problem necessarily needs to be defined in terms of realizations since we are aiming
to select features for a given realization. We will write x to denote realizations of the random variable X. Then we formalize our problem as one of finding a selector function, S : X  {0, 1}d such that for almost every x  X (w.r.t. pX ) we have

(Y |X(S(x)) = x(S(x))) =d. (Y |X = x)

(1)

where =d. denotes equality in distribution and S(x) is minimal (i.e. fewest 1s) such that (1) holds.

We suppose that we have a dataset D = {(xj, yj)}nj=1 consisting of n i.i.d. realizations of the pair (X, Y ).2 Note that Y can be viewed as having either come from a dataset, in which case the problem is of selecting predictive features, or as having come from a predictive model, in which case the problem is of explaining the model's predictions.

2.1 OPTIMIZATION PROBLEM

In order to learn a suitable selector function, we transform the constraint (1) into a soft constraint

using the Kullback-Leibler (KL) divergence which, for random variables W and V with densities

pW and pV is defined as

KL(W ||V ) = E log

pW (W ) pV (W )

.

We define the following loss for our selector function S

L(S) = ExpX KL(Y |X = x||Y |X(S(x)) = x(S(x))) + ||S(x)||

(2)

where || · || simply denotes the number of non-zero entries of a vector (or equivalently in this case, the number of 1s) and  is a hyper-parameter that trades off between the constraint in (1) and the number of selected features. The KL divergence in (2) can be rewritten as

KL(Y |X = x||Y |X(S(x)) = x(S(x))) = EyY |X=x log

pY (y|x) pY (y|x(S(x)))

= EyY |X=x log(pY (y|x)) - log(pY (y|x(S(x))))

= pY (y|x) log(pY (y|x)) - log(pY (y|x(S(x)))) dy
Y
where pY (·|·) denotes the appropriate conditional densities of Y . We will write

l(x, s) = pY (y|x) log(pY (y|x)) - log(pY (y|x(s))) dy
Y
so that our final loss can be written as
L(S) = ExpX [l(x, S(x)) + ||S(x)||] where || · || denotes the l0 (pseudo-)norm.

(3) (4)

3 PROPOSED MODEL
There are two main challenges in minimizing the loss in (4). First, the output space of the selector function ({0, 1}d) is large - its size increases exponentially with the dimension of the feature space; thus a complete search is impractical in high dimensional settings (and it should be noted that it is in high dimensional settings where feature selection is most necessary). Second, we do not have access to the densities pY (·|x(S(x))) and pY (y|x) required to compute (4).
2We will occasionally abuse notation and write yi to denote the ith element of the one-hot encoding of y, though the context should make it clear when this is the case.

3

Under review as a conference paper at ICLR 2019

3.1 LOSS ESTIMATION
To approximate the densities in (3), we introduce a pair of functions f  : X  × {0, 1}d  [0, 1]c parametrized by  and f  : X  [0, 1]c parametrized by  that will estimate pY (·|x(S(x))) and pY (·|x) respectively.

3.1.1 PREDICTOR NETWORK

We refer to f  as the predictor network. This will take as input a suppressed3 feature vector x(s) and its corresponding selection vector s and will output a probability distribution (using a softmax layer) over the c-dimensional output space.
f  is trained to minimize the cross entropy loss given by

c

l1() = -E(x,y)p,s(x,·)

yi log(fi(x(s), s))

i=1

where yi is the ith component of the one-hot encoding of y and  is the distribution induced by our selector network which will be defined in the following section. f  is implemented as a fully connected neural network4.

3.1.2 BASELINE NETWORK

We refer to f  as the baseline network, which is standard in the actor-critic literature for variance reduction. f  is implemented as a fully connected neural network and is trained to minimize

c

l3() = -E(x,y)p

yi log(fi(x)) .

i=1

For fixed ,  we define our loss estimator, ^l, by

cc

^l(x, s) = -

yi log(fi(x(s), s)) - yi log(fi (x)) .

i=1 i=1

(5)

3.2 SELECTOR FUNCTION OPTIMIZATION
We approximate the selector function S : X  {0, 1}d by using a single neural network, S^ : X  [0, 1]d parameterized by weights , that outputs a probability for selecting each feature (i.e. the ith component of S^(x) will denote the probability with which we select the ith feature). The selector network induces a probability distribution over the selection space ({0, 1}d), with the probability of a given joint selection vector s  {0, 1}d being given by5
(x, s) = di=1S^i(x)si (1 - S^i(x))1-si .

Using this, we define the following loss for our selector network

l2() = E(x,y)p Es(x,·) ^l(x, s) + ||s||0



= p(x, y) 

(x, s) ^l(x, s) + ||s||0  dxdy.

X ×Y

s{0,1}d

3When implemented we set  = 0 and include the selection vector to differentiate this from the case xi = 0. 4f , f  and S^ could also be implemented as CNNs or RNNs, when appropriate. 5Note that, when d is large, this becomes vanishingly small, however,  appears in our loss only via its log
and so in practice this is not a problem.

4

Under review as a conference paper at ICLR 2019

Features    ... 

Selector Network

Selection Probability


 ... 

Selected Features

Element-wise product


   ...

Selection 1



Random Sampler

0 1 ... Features 0 





...



Predictor Network
Baseline Network

Back-propagation

Label estimation
  ... 
Label estimation
  ... 

Predictor Loss (Cross Entropy)
Label 0 1 ... 0
Baseline Loss (Cross Entropy)

Loss Difference

Back-propagation

Figure 1: Block diagram of INVASE. Instances are fed into the selector network which outputs a vector of selection probabilities. The selection vector is then sampled according to these probabilities. The predictor network then receives the selected features and makes a prediction and the baseline network is given the entire feature vector and makes a prediction. Each of these networks are trained using backpropagation using the real label. The loss of the baseline network is then subtracted from the prediction network's loss and this is used to update the selector network.

Taking the gradient of this loss with respect to  gives us



l2() =

p(x, y) 

(x, s) ^l(x, s) + ||s||0  dxdy

X ×Y

s{0,1}d



=

X ×Y

p(x,

y)


s{0,1}d

(x, s) (x, s)



(x,

s)

^l(x, s) + ||s||0

 dxdy



= p(x, y) 

 log (x, s)(x, s) ^l(x, s) + ||s||0  dxdy

X ×Y

s{0,1}d

= E(x,y)p Es(x,·) ^l(x, s) + ||s||0  log (x, s) .

We update each of S^, f  and f  iteratively using stochastic gradient descent. Pseudo-code of INVASE is given in Algorithm 1 and a block representation of INVASE can be found in Fig. 1.

4 EXPERIMENTS
In this section, we quantitatively evaluate INVASE against various state-of-the-art benchmarks on both synthetic and real-world datasets. We evaluate our performance both at identifying ground truth relevance and at enhancing predictions. We compare our model with 4 global variable selection models: Knockoffs [3], Tree Ensembles (Tree) [6], Sequential Correlation Feature Selection (SCFS) [10], and LASSO regularized linear model; and 3 instance-wise feature selection methods: L2X [4], LIME [22], and Shapley [17]. The details of benchmark implementation can be found in the appendix. Implementation of INVASE can be found at https://github.com/ iclr2018invase/INVASE/.
5

Under review as a conference paper at ICLR 2019

Algorithm 1 Pseudo-code of INVASE
1: Inputs: learning rates ,  > 0, mini-batch size nmb > 0, dataset D 2: Initialize parameters , ,  3: while Converge do 4: Sample a mini-batch from the dataset (xj, yj)jn=m1b  D 5: for j = 1, ..., nmb do 6: Calculate selection probabilities

(pj1, ..., pjd)  S^(xj)

7: Sample selection vector
8: for i = 1, ..., d do sij  Ber(pji )

9: Calculate loss

cc

^lj(xj, sj)  -

yij log(fi(xj(sj), sj )) - yij log(fi (xj ))

i=1 i=1

10: Update the selector network parameters 

1 nmb   -
nmb j=1

^lj(xj, sj) + ||sj||

 log (xj, sj)

11: Update the predictor network parameters 







-



1 nmb

nmb j=1

c i=1

yij

×



log(fi(x(jsj), sj ))

12: Update the baseline network parameters 







-

1 
nmb

nmb j=1

c i=1

yij

×



log(fi (xj))

4.1 SYNTHETIC DATA EXPERIMENTS

4.1.1 EXPERIMENTAL SETTINGS

For our first set of experiments, we use the same synthetic data generation models as in L2X [4]. The

input features are generated from an 11-dimensional67 Gaussian distribution with no correlations

across the features (X  N (0, I)). The label Y is sampled as a Bernoulli random variable with

P(Y

= 1|X) =

1 1+logit(X)

,

where

logit(X)

is

varied

to

create

3

different

synthetic

datasets:

· Syn1: exp(X1X2)

· Syn2: exp(

6 i=3

Xi2

-

4)

· Syn3: -10 × sin 2X7 + 2|X8| + X9 + exp(-X10)

In each of these datasets, the label depends on the same subset of features for every sample. To highlight the capability of INVASE to detect instance-wise dependence, we generate 3 further synthetic datasets as follows:

· Syn4: If X11 < 0, logit follows Syn1, otherwise, logit follows Syn2.
6In L2X they use a 10-dimensional Gaussian, we introduce X11 to act as a "switch" to create instance-wise relevance. Experiments where instead the "switch" variable is one of X1, ..., X10 can be found in the appendix.
7We also perform experiments using 100 features in the Appendix to demonstrate the scalability of our method.

6

Under review as a conference paper at ICLR 2019

· Syn5: If X11 < 0, logit follows Syn1, otherwise, logit follows Syn3.
· Syn6: If X11 < 0, logit follows Syn2, otherwise, logit follows Syn3.
Note that in Syn4 and Syn5, the number of relevant features is different for different samples.
For each of Syn1 to Syn6 we draw 20,000 samples from the data generation model and separate each into training (Dtrain = (xi, yi)i1=00100) and testing (Dtest = (xj , yj )j1=00100) sets. For each method we try to find the top k relevant features for each sample (we set k = 4 for Syn1, Syn2, Syn3, Syn4, Syn5 and k = 5 for Syn6), note, however, that k is not given as an input to INVASE (but is necessary for other methods). The performance metrics we use are the true positive rate (TPR) (higher is better) and false discovery rate (FDR)8 (lower is better) to measure the performance of the methods when the focus is on discovery (i.e. discovering which features are relevant) and we use Area Under the Receiver Operating Characteristic Curve (AUROC), Area Under the Precision Recall Curve (AUPRC) and accuracy when the focus is on predictions.
4.1.2 DISCOVERY

Dataset
Metrics (%)
INVASE
L2X LIME Shapley
Knockoff Tree SCFS
LASSO

Syn1

TPR FDR

100.0 0.0

100.0 0.0 13.8 86.2 60.4 39.6

10.0 100.0 23.5 19.0

70.0 0.0 76.5 81.0

Syn2

TPR FDR

100.0 0.0

100.0 100.0 93.3

0.0 0.0 6.7

8.7 100.0 39.5 39.8

36.2 0.0 60.5 60.2

Syn3

TPR FDR

92.0 0.0

69.4 30.6 98.1 1.9 90.9 9.1

81.2 100.0 78.3 78.3

17.5 0.0 22.0 21.7

Syn4
TPR FDR
99.8 10.3
79.5 21.8 40.7 49.4 65.2 31.9
38.8 35.1 54.7 39.0 48.9 52.4 49.9 50.9

Syn5
TPR FDR
84.8 1.1
74.8 26.3 41.1 50.6 62.9 33.7
41.0 51.1 56.8 37.5 42.4 51.2 45.5 48.2

Syn6
TPR FDR
90.1 7.4
83.3 16.7 50.5 49.5 71.2 28.8
56.6 42.1 60.0 40.0 56.1 43.9 56.4 43.6

Table 1: Relevant feature discovery results for Synthetic datasets with 11 features

As demonstrated by Table 1, our method is capable of detecting relevant features on a global level (Syn1, Syn2 and Syn3) as well as on an instance-wise level (Syn4, Syn5 and Syn6) outperforming all other methods in both cases (both global and instance-wise methods). The particularly poor performance of some global feature selection methods in Syn1, Syn2 and Syn3 (where there is no instance-wise relevance) is due to the non-linearity of the relationship between features and labels, further details can be found in the Appendix.
The results for Syn4, Syn5 and Syn6 demonstrate that INVASE is capable of detecting a different number of relevant features for each sample when necessary - the performance improvement over L2X is greater in Syn4 and Syn5 than Syn6. In particular, in Syn4, L2X is forced to overselect features when X11 < 0 and underselect when X11  0 thus resulting in higher FDR and lower TPR, respectively. To highlight this, in Table 2 we report the group specific FDR and TPR on Syn4 and Syn5 when setting k = 3, 4, 5, where Group 1 refers to samples with X11 < 0 and Group 2 to samples with X11  0.
For k = 3 in Syn4, we see that INVASE and L2X have comparable FDR in Group 1, since the total number of relevant features for each sample is 3 (X1, X2, X11). However, when we increase k, we see that the FDR increases for L2X as it is forced to select more than 3 features, which necessarily means that the FDR must be at least 40% even if L2X was finding the relevant features perfectly. On the other hand, for Group 2 we see that the TPR is low for k = 3 since necessarily, L2X cannot possibly select all of the 5 relevant features. INVASE, however, is able to select the correct number in both and hence enjoys low FDR and high TPR.
Syn5 reinforces the conclusions we drew for L2X in Syn4. Interestingly, though, for INVASE, we found that X11 was almost never selected for Group 1 in Syn5. We believe this is because the lack of overlap between the relevant features for each group means that the predictor network can essentially learn two separate networks - one for each group. This is because it is possible to create
8Definitions of TPR and FDR can be found in the Appendix.

7

Under review as a conference paper at ICLR 2019

two subnetworks with non-overlapping weights that each take as input the features of a given group. X11 is therefore unnecessary for prediction. Note, however, that X11 is highly relevant for the selector network in deciding which features to pass on and so it is not true that X11 isn't relevant, but simply that the selector network does not need to "pass on" its relevance to the predictor network.
To investigate this further, results for settings where the features overlap between groups (and so it
is not possible to disentangle the networks) can be found in the Appendix.

Datasets
Group
Metrics (%)
INVASE
L2X (k = 3) L2X (k = 4) L2X (k = 5)

Syn4
12
TPR FDR TPR FDR
99.5 24.6 100.0 0.4
71.1 28.9 57.2 4.6 81.0 39.2 74.9 6.3 89.9 46.0 84.6 15.4

Syn5
12
TPR FDR TPR FDR
69.2 1.6 99.8 0.6
65.5 34.5 55.4 7.7 76.2 42.9 72.4 9.4 87.5 47.5 82.1 17.9

Table 2: Detailed comparison of INVASE with L2X in Syn4 and Syn5, highlighting the capability
of INVASE to select a flexible number of features for each sample. Group 1: X11 < 0, Group 2: X11  0

4.1.3 PREDICTION
In this experiment we analyze the effect of using feature selection as a pre-processing step for prediction. We first perform feature selection (either instance-wise or global) and then train a 3-layer fully connected network with Batch Normalization [11] in every layer (to avoid overfitting) to perform predictions on top of the (feature-selected) data. In this setting we compare the two global feature selection methods (LASSO and Tree) and one instance-wise feature selection method (L2X). Furthermore, we also compare with the predictive model without any feature selections (w/o FS) and the predictive model with ground truth globally relevant features9 (with Global). In particular, this allows us to demonstrate that the improvements in prediction performance are not just because the global feature selection performed implicitly by INVASE is better than the other global feature selection methods but are also due to the fact that we select features on an instance-wise level. Experiments here are conducted on synthetic data with 100 features but the same labelling procedures as above.
As can be seen in Table 3, there is a significant performance improvement when discarding all of the irrelevant features (with Global). However, neither of the global feature selection methods (Tree and Lasso) are capable of achieving this improvement. On the other hand, INVASE is capable of achieving (and beating - in Syn4 and Syn6) this improvement, demonstrating its capability both at selecting features globally better than existing methods but also at improving on global selection with instance-wise selection (where relevant), to provide further improvements. On the other hand, L2X performs worse than the global methods in Syn1-3, demonstrating an inability to perform even global feature selection in this higher dimensional setting (this is supported by the high dimensional discovery results in the Appendix), and in Syn4-6 is performing worse than with Global (which now is not even optimal).
Furthermore, even though we include Batch Normalization to avoid overfitting, with a small number of samples and high number of dimensions, the 3-layer fully connected network still suffers from overfitting as demonstrated by the significant difference in performance between w/o FS and with Global. This demonstrates the necessity of feature selection as a pre-processing step. Lastly, in comparison to with Global, with INVASE achieves performance gains in Syn4 and Syn6. It quantitatively shows that instance-wise feature selection can further improves the predictive model from ground truth global feature selection.
9For example, in Syn1 the predictor network in the with Global setting is trained on only X1 and X2 and in Syn4 it would be trained on X1, X2, X3, X4, X5, X6, X11.
8

Under review as a conference paper at ICLR 2019

Dataset
Syn1 Syn2 Syn3 Syn4 Syn5 Syn6

w/o FS
.578±.004 .789±.003 .854±.004 .558±.021 .662±.013 .692±.015

with Global
.686±.005 .873±.003 .900±.003 .774±.006 .784±.005 .858±.004

AUROC

with INVASE with Tree

.690±.006 .877±.003 .902±.003 .787±.004 .784±.005 .877±.003

.574±.101 .872±.003 .899±.001 .684±.017 .741±.004 .771±.031

with L2X
.498±.005 .823±.029 .862±.009 .678±.024 .709±.008 .827±.017

with LASSO
.498±.006 .555±.061 .886±.003 .514±.031 .691±.024 .727±.025

Dataset
Syn1 Syn2 Syn3 Syn4 Syn5 Syn6

w/o FS
.567±.007 .799±.005 .861±.003 .572±.019 .665±.019 .709±.018

with Global
.690±.006 .878±.005 .905±.002 .794±.006 .796±.005 .870±.005

AUPRC

with INVASE with Tree

.694±.006 .886±.004 .907±.003 .804±.004 .797±.006 .886±.004

.577±.102 .878±.004 .904±.002 .681±.031 .765±.003 .779±.027

with L2X
.498±.007 .817±.031 .860±.012 .672±.025 .719±.011 .835±.017

with LASSO
.499±.008 .591±.037 .890±.002 .536±.025 .680±.040 .757±.036

Table 3: Prediction performance comparison with and without feature selection methods (L2X, LASSO, Tree, INVASE, and Global). Global is using ground-truth globally relevant features for each dataset

4.2 REAL-WORLD DATA EXPERIMENTS
4.2.1 DATA DESCRIPTION
In this section we use two real-world datasets to perform a series of further experiments. The first, the Meta-Analysis Global Group in Chronic Heart Failure (MAGGIC) dataset [21], has 40,409 patients each with 31 measured features. The label is all-cause mortality. The second, the Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer Screening Trial in the US and the European Randomized Study of Screening for Prostate Cancer (ERSPC) dataset [7; 23] contains 38,001 each with 106 measured features. The label in this dataset is mortality due to prostate cancer. We refer to this as the PLCO dataset.
The first experiment we carried out was to create semi-synthetic datasets by using the labelling procedures Syn1-6 from above but with the features now coming from real data (instead of being i.i.d. Gaussian). The results of this experiment can be found in the Appendix.

4.2.2 THE DISCOVERED FEATURE IMPORTANCE IN MAGGIC DATASET
In this next experiment, we visualize the ability of INVASE to select features on an individualized level. Fig. 2(left) shows the selection probability (given by INVASE) of each feature for 20 randomly selected patients in the MAGGIC dataset. Fig. 2(right) shows the selection probability of each feature averaged over different binary splits of the data (i.e. when split into Male and Female). In Table 4, we also report the mean and variance of the number of selected features in each subgroup.

Overall 42.5±18.4

Male 43.5±10.7
Female 40.8±15.6

Diabetes 53.2±10.8 Non-diabetes 39.3±8.0

Hypertension 46.6±9.3
Non-hypertension 40.0±9.3

Smoker 41.0±12.1 Non-smoker 43.2±7.0

Heart Failure 51.8±11.1
No Heart Failure 39.6±6.9

Table 4: Selection probability of overall and patient subgroups by INVASE in MAGGIC dataset. (Mean ± Std)

9

Under review as a conference paper at ICLR 2019

Figure 2: Left: The feature importance for each of 20 randomly selected patients in the MAGGIC dataset. Right: The average feature importance for different binary splits in the MAGGIC dataset.

As can be seen, INVASE discovers significantly different features for both individuals and for different subgroups of the dataset.

4.2.3 RESULTS: PREDICTION USING REAL DATA VARIABLES WITH REAL LABEL
Evaluating the performance of feature selection methods on real data is difficult, since ground truth relevance is often not known. We therefore cannot use TPR and FDR to evaluate the performance on real data. In our final experiment, therefore, we instead focus on prediction performance exactly as in 4.1.3 (except now both the features and label come from real data).

Datasets MAGGIC
PLCO

Metrics
Labels INVASE Without INVASE
Labels INVASE Without INVASE

AUROC AUPRC
3 year .722±.005 .655±.010 .720±.006 .639±.009
5 year .637±.007 .329±.013 .629±.008 .324±.011

AUROC AUPRC
5 year .740±.005 .867±.006 .730±.006 .855±.004
10 year .673±.007 .506±.006 .657±.006 .485±.008

Table 5: Prediction performance for MAGGIC and PLCO dataset.

As can be seen in Table 5, INVASE consistently improves prediction performance in each of the two settings (different time horizons) in each dataset.

5 FUTURE WORK
While this paper has focused on discovering relevant features in the static setting, this could also be extended to apply in the temporal setting. One such avenue of exploration for this would be to replace each of the networks with an RNN. Particular care will need to be taken in defining the problem, though; do we treat each stream as a feature or each time point of each stream? We leave this investigation to future work.
10

Under review as a conference paper at ICLR 2019
REFERENCES
[1] Sebastian Bach, Alexander Binder, Gre´goire Montavon, Frederick Klauschen, Klaus-Robert Mu¨ller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.
[2] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert MA~ zller. How to explain individual classification decisions. Journal of Machine Learning Research, 11(Jun):1803­1831, 2010.
[3] Emmanuel Cande`s, Yingying Fan, Lucas Janson, and Jinchi Lv. Panning for gold: Model-free knockoffs for high-dimensional controlled variable selection. arXiv preprint arXiv:1610.02351, 2016.
[4] Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. Learning to explain: An information-theoretic perspective on model interpretation. arXiv preprint arXiv:1802.07814, 2018.
[5] Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 598­617. IEEE, 2016.
[6] Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine learning, 63(1):3­42, 2006.
[7] John K Gohagan, Philip C Prorok, Richard B Hayes, and Barnett-S Kramer. The prostate, lung, colorectal and ovarian (plco) cancer screening trial of the national cancer institute: history, organization, and status. Controlled clinical trials, 21(6):251S­272S, 2000.
[8] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.
[9] Isabelle Guyon and Andre´ Elisseeff. An introduction to variable and feature selection. Journal of machine learning research, 3(Mar):1157­1182, 2003.
[10] Mark Andrew Hall. Correlation-based feature selection for machine learning. 1999.
[11] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[12] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
[13] David P Kao, James D Lewsey, Inder S Anand, Barry M Massie, Michael R Zile, Peter E Carson, Robert S McKelvie, Michel Komajda, John JV McMurray, and JoAnn Lindenfeld. Characterization of subgroups of heart failure patients with preserved ejection fraction with possible implications for prognosis and treatment response. European journal of heart failure, 17(9):925­935, 2015.
[14] Pieter-Jan Kindermans, Kristof Schu¨tt, Klaus-Robert Mu¨ller, and Sven Da¨hne. Investigating the influence of noise and distractors on the interpretation of neural networks. arXiv preprint arXiv:1611.07270, 2016.
[15] Kenji Kira and Larry A Rendell. A practical approach to feature selection. In Machine Learning Proceedings 1992, pp. 249­256. Elsevier, 1992.
[16] Yaojin Lin, Qinghua Hu, Jinghua Liu, and Jie Duan. Multi-label feature selection based on max-dependency and min-redundancy. Neurocomputing, 168:92­103, 2015.
[17] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pp. 4765­4774, 2017.
[18] Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for tree ensembles. arXiv preprint arXiv:1802.03888, 2018.
11

Under review as a conference paper at ICLR 2019
[19] Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on pattern analysis and machine intelligence, 27(8):1226­1238, 2005.
[20] Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180­1190, 2008. [21] Stuart J Pocock, Cono A Ariti, John JV McMurray, Aldo Maggioni, Lars Køber, Iain B Squire,
Karl Swedberg, Joanna Dobson, Katrina K Poppe, Gillian A Whalley, et al. Predicting survival in heart failure: a risk score based on 39 372 patients from 30 studies. European heart journal, 34(19):1404­1413, 2012. [22] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135­1144. ACM, 2016. [23] Fritz H Schro¨der, Jonas Hugosson, Monique J Roobol, Teuvo LJ Tammela, Stefano Ciatto, Vera Nelen, Maciej Kwiatkowski, Marcos Lujan, Hans Lilja, Marco Zappa, et al. Screening and prostate-cancer mortality in a randomized european study. New England Journal of Medicine, 360(13):1320­1328, 2009. [24] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. arXiv preprint arXiv:1704.02685, 2017. [25] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. [26] Erik S trumbelj and Igor Kononenko. Explaining prediction models and individual predictions with feature contributions. Knowledge and information systems, 41(3):647­665, 2014. [27] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pp. 267­288, 1996.
12

Under review as a conference paper at ICLR 2019

APPENDIX SUMMARY OF RELATED WORKS

SCFS [10]
MIFS [19]
LASSO [27]
Knock-off [3]
L2X [4]
LIME [22]
Shapley [17]
DeepLIFT [24]
Saliency [25]
Tree SHAP [18]
Pixel-wise [1]
INVASE (Ours)

Key ideas
Max-dependency min-redundancy criteria with Pearson correlations
Max-dependency min-redundancy criteria with Mutual Information
Linear regression with l1-norm penalty
Comparison between knock-off variables and real variables
Mutual Information maximization with Gumbel-softmax
Locally linear approximation
Shapley value estimation to quantify feature importance
Decompose the output of NN on a reference input
Backpropagation from the output of the NN to the input
Shapley value estimation only for tree-ensemble models
Measuring the effects on the output using input perturbation
Minimize KL divergence using deep NN influenced by actor-critic models

Experiments shown
Feature selection
Feature selection Feature selection
Prediction Feature selection Hypothesis test
Interpretation
Interpretation
Feature selection
Interpretation
Interpretation
Interpretation
Interpretation
Feature selection Interpretation Prediction

Global/ Instance-wise
Global Global Global Global
Instance-wise Instance-wise Instance-wise Instance-wise Instance-wise Instance-wise Instance-wise
Instance-wise

Model agnostic
Yes Yes Yes Yes
Yes Yes Yes No No No No
Yes

# of relevant features
Not needed
Not needed
Not needed
Not needed
Should be given
Should be given
Should be given
Should be given
Should be given
Should be given
Should be given
Not needed

Table 6: Summary of the related works. (NN: Neural networks, KL: Kullback-Leibler)

EXTENDING INVASE TO REGRESSION
To extend our model to the setting where Y is continuous (regression problem), we replace the estimated loss with the reconstruction error as follows.
^l(x, s) = -||y - f (x, s)||2 where f  : X  R is now the (continuous) predictor function trained to minimize the 2-norm between its outputs and the real labels. As noted in [8], when the distribution of Y given X is Gaussian, minimizing the l2-norm is equivalent to minimizing the KL divergence.
DETAILS OF INVASE
In the experiments, the depth of the selector, predictor, and baseline networks is set to 3. The number of hidden nodes in each layer is d and 2d, respectively. We use either ReLu or SeLu as the activation functions of each layer except for the output layer where we use the sigmoid activation function for the selector network and softmax activation function for the predictor and baseline networks. The number of samples in each mini-batch is 1000 for the selector, predictor, and baseline networks. We use cross-validation to select  among {0.1, 0.3, 0.5, 1, 2, 5, 10}. We use tensorflow to implement INVASE. The source-code can be found at https://github.com/iclr2018invase/ INVASE/.
13

Under review as a conference paper at ICLR 2019

DETAILS OF BENCHMARKS

We use the following links for the implementations of 7 benchmarks.

· L2X: https://github.com/Jianbo-Lab/L2X

· LIME: https://github.com/marcotcr/lime

· Shapley: https://github.com/slundberg/shap

· Knock-off:

http://web.stanford.edu/group/candes/knockoffs/

software/knockoff/

· Tree:

http://scikit-learn.org/stable/modules/generated/

sklearn.ensemble.ExtraTreesClassifier.html

· LASSO: http://scikit-learn.org/stable/modules/linear_model. html#lasso

For L2X, we use the same network settings used in INVASE for fair comparisons. For SCFS, we explicitly implement from the reference ([10]).

HIGH DIMENSIONAL DISCOVERY
To demonstrate the scalability of our method, we run an experiment in which we increase the total number of features to 100. The features are generated as a 100-dimensional Gaussian with no correlations (N (0, I)) and the relationships between features and label remains as in Table 1 in the main manucript (i.e. we are adding 89 additional noisy signals that have no effect on the label).

Dataset
Metrics (%)
INVASE
L2X LIME Shapley
Knock off Tree SCFS
LASSO

Syn1
TPR FDR
100.0 0.0
6.1 93.9 0.0 100.0 4.4 95.6
0.0 64.9 49.9 50.1 2.5 97.5 2.5 97.5

Syn2

TPR FDR

100.0 0.0

81.4 18.6 100.0 0.0 95.1 4.9

3.7 100.0
5.3 4.0

71.2 0.0 94.7 96.0

Syn3

TPR FDR

100.0 0.0

57.7 42.3 92.7 7.3 88.8 11.2

74.9 100.0 74.9 75.3

24.9 0.0 25.1 24.7

Syn4
TPR FDR
66.3 40.5
48.5 46.4 43.8 47.4 50.2 43.4
28.2 59.8 40.7 49.5 27.0 74.6 28.3 73.2

Syn5
TPR FDR
73.2 23.7
35.4 60.8 42.3 50.1 49.9 44.2
33.1 59.4 56.7 37.5 30.6 62.1 36.0 56.9

Syn6
TPR FDR
90.5 15.4
66.3 33.7 50.1 49.9 62.5 37.5
46.9 53.0 58.4 41.6 38.3 61.7 45.9 54.1

Table 7: Relevant feature discovery for synthetic datasets with 100 features

As can be seen in Table 7, INVASE also works consistently better than all other benchmarks in all 6 synthetic datasets in this setting. In fact, we see a significant reduction in performance (compared to the 11 feature setting) for L2X in Syn1, with the TPR dropping more than 90% leading to an almost complete failure of the method to detect any relevant features. In particular, we see that L2X does not scale as well as INVASE with the dimensionality of the data, which is particularly limiting for a feature selection method.
We also compare the CPU times of the algorithm for training and testing with other instance-wise feature selection benchmarks to show the scalability in terms of computational complexity. As can be seen in Table 8, INVASE is much faster (10 times) than LIME and Shapley methods and comparable with L2X; we see that INVASE takes approximately 50% longer to run than L2X, which can be accounted for by the addition of a 3rd network (the baseline network) in INVASE that is not present in L2X. Note, however, that this baseline network can be trained in parallel with the predictor network and we believe that doing so would lead to both INVASE and L2X having the same run-time.
14

Under review as a conference paper at ICLR 2019

Methods INVASE L2X Shapley LIME

Train 1327.69s 939.82s 12801.21s

-

Test 0.38s 0.78s 0.06s 18931.98s

Table 8: Comparison of CPU clock time across different instance-wise feature selection methods on average across Syn1 to Syn6 with 100 features and 10,000 samples on training/testing, respectively

ADDITIONAL RESULTS ON COMPLEX SYNTHETIC DATASETS

In the main paper, the relevant subset for a sample in each of our variable synthetic datasets (Syn46) depended on X11 only, which was unused in the rest of the model (i.e. X11 determined only the relevant subset, and was otherwise unused as a predictive variable). In this set of experiments, we investigate the effect of having the subset relevance depend on a variable that is also used in the model itself (Syn4A, Syn5A, Syn6A). We then investigate the effect of having more than one variable being used to determine subset relevance (Syn4B, Syn5B, Syn6B, Syn7). The results for these are reported in Tables 9 and 10, respectively.

The input features are generated from a 100-dimensional Gaussian distribution with no correlations

across the features (X  N (0, I)).

Y

is generated according to P(Y

= 1|X) =

1 1+logit(X)

with the

logit value for each synthetic dataset now defined as follows:

· Syn4A: If X1 < 0, logit = exp(X1X2), otherwise, logit =exp(

6 i=3

Xi2

-

4).

· Syn5A: If X1 < 0, logit = exp(X1X2), otherwise, logit =-10 × sin 2X7 + 2|X8| + X9 + exp(-X10).

· Syn6A: If X7 < 0, logit =exp(

6 i=3

Xi2

-

4),

otherwise,

logit

=-10 × sin

2X7

+ 2|X8 |

+

X9 + exp(-X10).

Dataset
Metrics (%)
INVASE+
L2X LIME Shapley
Knock off Tree SCFS
LASSO

Syn4A
TPR FDR
77.5 14.5
65.0 39.3 56.3 49.2 71.8 39.8
59.8 62.6 61.3 46.9 52.8 66.9 61.0 61.2

Syn5A
TPR FDR
85.9 8.8
48.0 57.4 58.2 48.8 71.0 41.3
55.0 49.9 75.6 39.4 55.3 50.6 55.0 50.0

Syn6A
TPR FDR
89.9 7.3
74.4 35.5 58.9 47.8 68.9 38.2
65.0 40.0 66.9 40.0 50.4 51.8 53.9 48.8

Table 9: Relevant feature discovery results for complex synthetic datasets (Syn4A, 5A, 6A) with 100 features

· Syn4B: If X1X3 < 0, logit = exp(X1X2), otherwise, logit =exp(

6 i=3

Xi2

-

4).

· Syn5B: If X1X7 < 0, logit = exp(X1X2), otherwise, logit =-10 × sin 2X7 + 2|X8| + X9 + exp(-X10).

· Syn6B: If X3X7 < 0, logit =exp(

6 i=3

Xi2

-

4),

otherwise,

logit

=-10

×

sin 2X7

+

2|X8| + X9 + exp(-X10).

· Syn7:

­ If X1 < 0, X2 < 0, logit = exp(X1X2)

­ If X1 < 0, X2  0, logit =exp(

6 i=3

Xi2

-

4).

­ If X1  0, X2 < 0, logit =-10 × sin 2X7 + 2|X8| + X9 + exp(-X10).

­ If X1  0, X2  0, logit =0.5 × exp(X1X2) + 0.5 × exp(

4 i=3

Xi2

-

2).

15

Under review as a conference paper at ICLR 2019

Dataset
Metrics (%)
INVASE+
L2X LIME Shapley
Knock off Tree SCFS
LASSO

Syn4B
TPR FDR
65.5 30.1
43.2 53.4 56.8 37.2 51.4 43.5
5.3 87.4 56.7 37.4 3.7 96.2 4.2 95.6

Syn5B
TPR FDR
85.0 15.0
50.3 50.3 71.9 27.2 77.2 24.1
73.3 25.2 73.9 25.0 72.3 26.3 73.3 25.0

Syn6B
TPR FDR
86.5 27.8
44.6 55.4 69.8 30.2 69.3 30.7
59.9 40.1 70.1 29.9 61.1 38.9 60.1 39.9

Syn7
TPR FDR
86.8 32.4
35.3 70.9 56.4 51.0 61.8 45.6
54.1 60.0 71.6 40.3 22.9 77.5 24.9 75.8

Table 10: Relevant feature discovery results for complex synthetic datasets (Syn4B, 5B, 6B, 7) with 100 features

RESULTS ON SEMI-SYNTHETIC DATASETS

In this experiment, we use real features (which have correlation across features) but generate the labels as in the synthetic experiments from the main paper, using Syn1-Syn6. This allows us to know the ground truth relevance of the features, and calculate TPR and FDR, while using unknown and correlated feature distributions (instead of the unrealistic setting of i.i.d. Gaussian used in the fully synthetic experiment). The results for the MAGGIC and PLCO datasets are given below.

Dataset
Metrics (%)
INVASE
L2X LIME Shapley
Knock off Tree SCFS
LASSO

Syn1

TPR FDR

100.0 0.0

68.8 31.2 46.9 53.1 73.9 26.1

27.5 100.0 30.0 25.0

65.0 0.0 70.0 75.0

Syn2

TPR FDR

100.0 0.0

99.9 0.1 99.9 0.1 94.5 5.5

77.5 100.0 53.0 75.0

22.5 0.0 47.0 25.0

Syn3

TPR FDR

100.0 0.0

83.0 17.0 87.2 12.8 81.0 19.0

100.0 100.0 100.0 100.0

0.0 0.0 0.0 0.0

Syn4
TPR FDR
85.9 0.0
60.0 31.3 63.6 24.4 65.3 23.9
57.0 34.4 56.3 29.7 52.0 39.9 60.7 33.1

Syn5
TPR FDR
72.9 0.1
68.3 22.3 50.2 37.6 61.2 29.0
56.1 29.8 51.6 40.2 54.0 32.4 56.1 29.8

Syn6
TPR FDR
81.0 13.2
73.5 26.5 68.7 31.3 69.9 30.1
58.0 42.0 46.7 53.3 64.5 35.5 58.2 41.8

Table 11: Relevant feature discovery for real datasets with synthetic labels using MAGGIC dataset

Dataset
Metrics (%)
INVASE
L2X LIME Shapley
Knock off Tree SCFS
LASSO

Syn1
TPR FDR
35.9 0.0
0.0 100.0 1.0 99.0 5.4 94.6
15.0 50.0 0.0 100.0 10.0 90.0 0.0 100.0

Syn2
TPR FDR
100.0 0.0
62.2 37.8 70.3 29.7 68.5 31.5
85.0 15.0 71.0 29.0 61.0 39.0 72.5 27.5

Syn3

TPR FDR

84.0 7.0

43.6 56.4 74.9 25.1 67.9 32.1

100.0 75.0 93.8 100.0

0.0 25.0 6.2 0.0

Syn4
TPR FDR
59.2 38.6
41.9 55.4 43.5 55.9 32.7 69.4
46.1 52.1 34.5 66.3 43.2 55.7 39.2 60.8

Syn5
TPR FDR
64.6 31.7
21.5 76.7 26.8 68.9 39.6 58.6
34.5 58.3 43.8 54.7 31.0 63.6 33.2 68.2

Syn6
TPR FDR
70.0 29.9
66.9 33.1 56.8 43.2 48.5 51.5
60.0 40.0 36.9 63.1 55.5 44.5 45.0 55.0

Table 12: Relevant feature discovery for real datasets with synthetic labels using PLCO dataset

As demonstrated in Tables 11 and 12, INVASE outperforms all other methods across all 6 of the synthetic-label settings using real features. This also demonstrates the capability of INVASE in settings where there are unknown correlation structures in the features.

16

Under review as a conference paper at ICLR 2019

CORRELATIONS BETWEEN FEATURES AND LABELS IN THE SYNTHETIC AND SEMI-SYNTHETIC EXPERIMENTS

Variables
X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11

Syn1
0.003 0.001 0.006 0.006 0.003 0.003 0.013 0.010 0.001 0.002 0.014

Syn2
0.008 0.005 0.011 0.003 0.015 0.004 0.009 0.008 0.003 0.003 0.012

Syn3
0.006 0.006 0.001 0.003 0.022 0.005 0.481 0.012 0.239 0.308 0.004

Syn4
0.009 0.005 0.017 0.002 0.004 0.002 0.002 0.003 0.002 0.003 0.028

Syn5
0.007 0.015 0.016 0.000 0.017 0.004 0.242 0.010 0.115 0.149 0.018

Syn6
0.006 0.005 0.010 0.002 0.028 0.005 0.235 0.022 0.121 0.144 0.002

Table 13: Correlation between features and labels in Synthetic datasets with 100 features. Ground truth (in the global sense) relevant features are given in bold. Features with correlation > 0.05 are highlighted in red.

As can be seen in Table 13, among 33 relevant features, only 9 features have more than 0.05 (linear) correlation with the label. In particular, using a linear model, it is very hard to discover the relevant features. However, Knock-off (based on LASSO and linear correlations), LASSO, and SCFS are linear models, resulting in a poor performance in our experiments. The above table results are directly reflected in the results given in the main manuscript.

Variables
X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11

Syn1
0.028 0.002 0.018 0.005 0.006 0.019 0.005 0.020 0.025 0.07 0.009

Syn2
0.030 0.011 0.079 0.113 0.034 0.114 0.010 0.030 0.022 0.043 0.006

Syn3
0.070 0.009 0.008 0.006 0.032 0.027 0.367 0.112 0.299 0.328 0.034

Syn4
0.011 0.001 0.038 0.056 0.013 0.099 0.000 0.023 0.006 0.027 0.046

Syn5
0.044 0.001 0.006 0.001 0.016 0.018 0.262 0.075 0.216 0.222 0.018

Syn6
0.026 0.012 0.046 0.055 0.036 0.004 0.272 0.082 0.200 0.206 0.058

Table 14: Correlation between features and labels in MAGGIC datasets. Ground truth relevant features are described in bold. Features with correlation > 0.05 are described in red

We do the same analysis for the MAGGIC dataset; results are given in Table 14. We see that here the linear correlation with the label is stronger and this is reflected in Tables 11 and 12, where all of the linear models performed better than in the fully-synthetic settings. However, we note that although they had a better performance, in most cases it was still not comparable with INVASE.

17

Under review as a conference paper at ICLR 2019

DEFINITION OF TPR AND FDR

Predicted Condition

Positive Negative

True Condition

Positive
True Positive
False Negative

Negative
False Positive
True Negative

True Positive Rate (TPR)=

False Discovery Rate (FDR)=

Figure 3: The definitions of True Positive Rate (TPR) and False Discovery Rate (FDR)

18

