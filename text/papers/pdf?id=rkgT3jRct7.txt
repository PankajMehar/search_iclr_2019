Under review as a conference paper at ICLR 2019
LARGE-SCALE ANSWERER IN QUESTIONER'S MIND FOR VISUAL DIALOG QUESTION GENERATION
Anonymous authors Paper under double-blind review
ABSTRACT
Answerer in Questioner's Mind (AQM) is an information-theoretic framework that has been recently proposed for task-oriented dialog systems. AQM benefits from asking a question that would maximize the information gain when it is asked. However, due to its intrinsic nature of explicitly calculating the information gain, AQM has a limitation when the solution space is very large. To address this, we propose AQM+ that can deal with a large-scale problem and ask a question that is more coherent to the current context of the dialog. We evaluate our method on GuessWhich, a challenging task-oriented visual dialog problem, where the number of candidate classes is near 10K. Our experimental results and ablation studies show that AQM+ outperforms the state-of-the-art models by a remarkable margin with a reasonable approximation. In particular, the proposed AQM+ reduces more than 60% of error as the dialog proceeds, while the comparative algorithms diminish the error by less than 6%. Based on our results, we argue that AQM+ is a general task-oriented dialog algorithm that can be applied for non-yes-or-no responses.
1 INTRODUCTION
Recent advances in deep learning have led an end-to-end neural approach to task-oriented dialog problems that can reduce a laborious labeling task on states and intents. Many researchers have applied sequence-to-sequence models (Vinyals & Le, 2015) that are trained in a supervised learning (SL) fashion. In SL approaches, given the dialog histories so far, the model predicts the distribution of the responses from the task-oriented system (Bordes & Weston, 2017; Eric & Manning, 2017; Zhao et al., 2018). However, the SL approach typically requires a lot of training data to deal with unseen scenarios and cover all trajectories of the vast action space of dialog systems (Wen et al., 2016). Furthermore, because the SL-based model does not consider the sequential characteristic of the dialog, the error may propagate over time that causes an inconsistent dialog (Li et al., 2017). To address this issue, reinforcement learning (RL) has been applied to the problem (Zhao & Eskenazi, 2016; Strub et al., 2017). By learning the intrinsic planning policy and the reward function, RL approach enables the models to generate a consistent dialog and generalize better on unseen scenarios. However, these methods struggle to find a competent RNN model that uses back-propagation, owing to the complexity of learning a series of sentences (Das et al., 2017b; Lee et al., 2018).
As an alternative, Lee et al. (2018) have recently proposed "Answerer in Questioner's Mind" (AQM) algorithm that does not depend on a limited capacity of RNN models to cover an entire dialog. AQM treats the problems as twenty question games and selects the question that gives a maximum information gain. Unlike the other approaches, AQM benefits from explicitly calculating the posterior distribution and finding a solution analytically. The authors showed promising results in the taskoriented dialog problem, such as GuessWhat (de Vries et al., 2017), where a questioner tries to find an object that is in answerer's mind via a series of Yes/No questions. The candidates are confined to the objects that is presented in the given image (less than ten on average). However, this simplified task may not be general enough to practical problems where the number of objects, questions and answers are typically unrestricted. For example, GuessWhich is a generalized version of GuessWhat that has a greater number of class candidates (9,628 images) and a dialog that consists of sentences beyond yes or no (Das et al., 2017b). Because the computational complexity vastly increases to explicitly calculate the information gain over the size of the entire search space, the
1

Under review as a conference paper at ICLR 2019
original AQM algorithm is not scalable to a large scale problem. More specifically, the number of the unit calculation for information gain in GuessWhat is 10 (number of objects) × 2 (Yes/No), while that of GuessWhich is 10, 000 (number of images) ×  (answer is a sentence) which makes the computation intractable.
To address this, we propose a more generalized version of AQM, dubbed AQM+. Compared to the original AQM, the proposed AQM+ can easily handle the increasing number of questions, answers, and candidate classes by employing an approximation based on subset sampling. Because our algorithm considers the previous history of the dialog, AQM+ can generate a more contextual question. To understand the practicality and demonstrate the superior performance of our method, we conduct extensive experiments and quantitative analysis on GuessWhich. Experimental results show that our model could successfully deal with the answers in sentence and significantly decrease 61.5% of the error while the SL and RL methods decrease less than 6% of the error. The ablation study shows that our information gain approximation is reasonable. Increasing the number of sampling by eight times brought only a marginal improvement of percentile mean rank (PMR) from 94.63% to 94.79%. This indicates that our model can effectively approximate the distribution over the large search space with a small number of sampling. Overall, our experimental results provide meaningful insights on how AQM framework can further provide an additional improvement on top of the SL and RL approaches.
Our main contributions are summarized as follows:
· We propose AQM+ that extends the AQM framework toward the more general and complicated tasks. AQM+ can handle a more complicated problem where the number of candidate classes is extremely large.
· At every turn, AQM+ generates a question considering the context of the previous dialog, which is desirable in practice.
· AQM+ outperforms the comparative deep learning models by a large margin in Guesswhich, a challenging task-oriented visual dialog task.
2 RELATED WORKS
A task-oriented visual dialog problem has recently been paid attention in the field of computer vision and natural language processing (Kim et al., 2017). GuessWhat is one of the famous task-oriented dialog tasks, where the goal is to figure out a target object in the image through a dialog that the answerer has in mind. However, GuessWhat is relatively an easy task because it only allows the answer form of yes or no. The baseline visual question answering (VQA) model achieves 78.5% (de Vries et al., 2017). In the object guessing task (i.e., GuessWhat task itself), the state-of-the-art averaged accuracy of SL, RL (Zhang et al., 2018b), and AQM (Lee et al., 2018) reached 44.6% and 60.8%, and 72.9% at the 5th round, respectively. Random guessing baseline has an accuracy of 16.0% (Han et al., 2017), thus RL algorithms achieve 53.3% error decrease, whereas AQM achieves 67.7%.
GuessWhich is a cooperative two-player game that one player tries to figure out an image out of 9,628 that another has in mind (Das et al., 2017b). GuessWhich uses Visual Dialog dataset (Das et al., 2017a) which includes human dialogs on MSCOCO images (Lin et al., 2014) as well as the captions that are generated. Although GuessWhich is similar to GuessWhat, it is more challenging in every task including asking a question, giving an answer, and guessing the target class. For example, unlike GuessWhat that can be answered in yes or no, the answer can be an arbitrary sentence in GuessWhich. Therefore, the VQA task in the Visual Dialog dataset is much studied than the GuessWhat dataset (Lu et al., 2017; Seo et al., 2017).
Similar to GuessWhat, SL and RL approaches have been applied to solve the GuessWhich task and they showed a moderate increase in performance (Das et al., 2017b; Jain et al., 2018; Zhang et al., 2018a). However, based on the authors' recent Github implementation1 of the papers in ICCV (Das et al., 2017b), SL and RL methods have shown that only 6% of error is diminished through the dialog compared to the zeroth turn baselines which only use generated caption.
1https://github.com/batra-mlp-lab/visdial-rl
2

Under review as a conference paper at ICLR 2019

Figure 1: Illustration of AQM+ applied for GuessWhich task. The goal of GuessWhich is to figure out a correct answer out of 9,628 test images by asking a sequence of questions.

3 ALGORITHM: AQM+

3.1 PROBLEM SETTING
In our experiments, a questioner bot (Qbot) and an answerer bot (Abot) cooperatively communicate to achieve the goal via natural language. Under the AQM framework, at each turn t, Qbot generates an appropriate question qt and guesses the target class c given a previous history of the dialog ht-1 = (q1:t-1, a1:t-1, h0). Here, at is the t-th answer and h0 is an initial context that can be obtained before the start of the dialog. We refer to the random variables of target class and the t-th answer as C and At, respectively. Note that the question is not a random variable in our information gain calculation. To distinguish from the random variables, we use a bold face for a set notation of target class, question, and answers; i.e. C, Q, and A.
Figure 1 explains the AQM+ algorithm applied to GuessWhich game. In Figure 1, c is the image with three elephants, q1 is "Are there many people?", a1 is "Yes it is.", a2 is "How many elephants?", and h0 is "There are elephants walking in the zoo." In GuessWhich game, C is the set of test images whose size is 9,628. The size of Q and A is theoretically infinity as questions and answers can be more than one word.

3.2 PRELIMINARY: SL, RL, AND AQM APPROACHES

In SL and RL approaches (Das et al., 2017b; Jain et al., 2018; Zhang et al., 2018a), Qbot consists of two RNN modules that generate a question and measure a score for each class (Qscore), respectively. One is "Qgen", a question generator finding the solution that maximizes its distribution p; i.e. qt = argmax p(qt|ht-1). The other is a "guesser" whose score function is f (c|ht).

On the other hand, in the previous AQM approach (Lee et al., 2018), these two RNN-based models
are substituted to the calculation that explicitly finds an analytic solution. It finds a question that maximizes information gain or mutual information I~, i.e. qt = argmaxqtQfix I~[C, At; qt, ht-1], where

I~[C,

At;

qt,

ht-1]

=

cC

at A

p^(c|ht-1)p~(at|c,

qt,

ht-1)

ln

p~(at|c, qt, ht-1) p~ (at|qt, ht-1)

.

(1)

Here, a posterior function p^ can be calculated with a following equation in a sequential way, where p^ is a prior function given h0.

t
p^(c|ht)  p^ (c|h0) p~(aj|c, qj, hj-1) = p^(c|ht-1)p~(at|c, qt, ht-1)

(2)

j=1

In AQM, Equation 1 and Equation 2 can be explicitly calculated from the model. For ease of reference, let us name every component one by one. A module that calculates an information gain I~ is referred to as "Qinfo" and a module that finds an approximated answer distribution p~(at|c, qt, ht-1) is referred to as "aprxAgen". In AQM, aprxAgen is a model distribution that Qbot has in mind where the target is the true distribution of an answer generator p¯(at|c, qt, ht-1), which is referred to as "Agen". Finally, "Qpost" denotes a posterior p^ calculation module for guessing a target class.

As AQM use full set of C and A, the complexity depends on the size of C and A. For the question
selection, AQM uses a predefined set of candidate questions (Qfix). For example, one way to obtain Qfix is to select questions from the training dataset randomly, called "randQ".

3

Under review as a conference paper at ICLR 2019

Figure 2: Modules in AQM+ and comparative models. SL and RL have their main neural modules as Qgen p and Qscore f , while AQM has aprxAgen p~ used for Qpost I~ and Qinfo p^. AQM+ con-
tains all five modules and uses these to make subsets Qt, At, and Ct, thus achieving approximated estimation on information gain for large-scale inference, along with efficient contextual question
generation.

3.3 AQM+ ALGORITHM

In this paper, we propose AQM+ algorithm for tackling the large-scale task-oriented dialog problem. The core differences of AQM+ from the previous AQM are summarized as follows:
· The candidate question set Qt,gen is sampled from p(qt|ht-1) using a beam search at every turn. Previously, Lee et al. (2018) used a predefined set of candidate questions Qfix. For example, one way to obtain Qfix is to randomly select questions from the training dataset, called "randQ".
· The answerer model (aprxAgen, p~) that Qbot has in mind is not a binary classifier (yes/no) but an RNN generator. In addition, aprxAgen does not assume p~(at|c, qt) = p~(at|c, qt, ht-1), which is not even an appropriate assumption when the previous and current questions are sequentially related. For example, p(a2 = "yes" | c, q2 = "is left?") = p(a2 = "yes" | c, q2 = "is left?", a1 = "yes", q1 = "is right?"). Regardless of the left term, the probability of the right term is almost zero.
· To approximate the information gain of each question, the subset of A and C is used. We describe an additional explanation on our information gain approximation, infogain topk as below.

Infogain topk The equation for Infogain topk is as follows:

I~topk[C, At; qt, ht-1]

=

at At,topk (qt )

cCt,topk

p^reg (c|ht-1 )p~reg (at |c,

qt,

ht-1)

ln

p~reg(at|c, qt, ht-1) p~reg(at|qt, ht-1)

,

(3)

where p^reg and p~reg is a normalized version of p^ over Ct,topk and p~ over At,topk(qt), respectively. Here, p~reg is obtained by using both p^reg and p~reg as follows:

p^reg(c|ht-1) =

p^(c|ht-1) cCt,topk p^(c|ht-1)

p~reg(at|c, qt, ht-1) =

p~(at|c, qt, ht-1) atAt,topk(qt) p~(at|c, qt, ht-1)

p~reg(at|qt, ht-1) =

p^reg(c|ht-1) · p~reg(at|c, qt, ht-1)

cCt,topk

Each set is constructed by the following procedures.

(4) (5) (6)

4

Under review as a conference paper at ICLR 2019
· Ct,topk  top-K posterior test images (from Qpost p^(c|ht-1)) · Qt,gen  top-K likelihood questions using the beam search (from Qgen p(qt|ht-1)) · At,topk(qt)  generated answers from aprxAgen for each question qt and each class in
Ct,topk (from aprxAgen p~(at|c, qt, ht-1))
In general, the AQM+ algorithm can deal with various problems where |Ct,topk|, |Qt,gen|, and |At,topk(qt)| are all different. Here, | · | denotes the cardinality of a set. We can vary the size of each set and control the complexity of the AQM+ algorithm. In our experiments, however, we mainly considered the problem when |Ct,topk| = |Qt,gen| = |At,topk(qt)|. More specifically, |Ct,topk| is equal to |At,topk(qt)| because our model finds a single best answer at given a pair (qt, c) that maximizes p~(at|c, qt, ht-1). Therefore, |At,topk| = |Qt,gen| · |Ct,topk| per information gain calculation where At,topk = {At,topk(qt)|qt  Qt,gen}. For the detailed explanation, see Algorithm 1 in Appendix A.
3.4 LEARNING
In all SL, RL, and AQM frameworks, Qbot needs to be trained to approximate the answer-generating probability of Abot. In AQM approach, aprxAgen does not share the parameters with Agen, and therefore also needs to be trained to approximate Agen. AQM can train aprxAgen by the learning strategy of the SL or RL approach. We explain two learning strategies of AQM framework below: indA and depA. In SL approach, Qgen and Qscore are trained from the training data, which have the same or similar distribution to that of the training data used in training Abot. Likewise, in indA setting of AQM approach, aprxAgen is trained from the training data. In RL approach, Qbot uses dialogs made by the conversation of Qbot and Abot and the result of the game as the objective function (i.e. reward). Likewise, in depA setting of AQM approach, aprxAgen is trained from the questions in the training data and following answers obtained in the conversation between Qbot and Abot. We also use the term trueA, referring to the setting where aprxAgen is the same as Agen, i.e. they share the same parameters. Both previous AQM algorithm and proposed AQM+ algorithm use these learning strategies.
4 EXPERIMENTS
4.1 EXPERIMENTAL SETTING
GuessWhich Task GuessWhich is a two player game played by Qbot and Abot. The goal of GuessWhich is to figure out a correct answer out of 9,628 test images by asking a sequence of questions. Abot can see the randomly assigned target image, which is unknown to Qbot. Qbot only observes a generated caption of the image as the prior knowledge. Neuraltalk2 is used for generating captions (Vinyals & Le, 2015). To achieve the goal, Qbot asks a series of questions, to which Abot responses with a sentence.
Comparative Models We compare AQM+ with three comparative models, SL-Q, RL-Q, and RLQA (Das et al., 2017b). In SL-Q, Qbot and Abot are trained separately from the training data. In RL-Q, Qbot is initialized by the Qbot trained by SL-Q and then is fine-tuned by RL. Abot is the same as the Abot trained by SL-Q, and is not fine-tuned further. In the original paper (Das et al., 2017b), it was referred to as Frozen-A. By the way, in an RL-QA setting, Abot is also concurrently trained with Qbot. In the original paper, it was referred to as RL-full-QAf.
Though RL-QA is the main setting in the work of Das et al. (2017b), there are some reports indicating that fine-tuning both Qbot and Abot is unfair (de Vries et al., 2017; Han et al., 2017). If the distribution of Abot is not fixed, Qbot and Abot can make their own language which is not compatible to natural language, and even though the generated dialog looks like human's dialog, it would eventually fail and the performance of the conversation with human would decrease when compared to SL-Q (Chattopadhyay et al., 2017; Lee et al., 2018). Obtaining a good performance by fine-tuning both Qbot and Abot is much easier than fine-tuning only Qbot. Thus, it is reasonable to compare AQM+ w/ indA and AQM+ w/ depA with SL-Q and RL-Q, respectively.
We also compare our AQM+ with "Guesser" algorithm. Guesser asks a question generated from SL-Q algorithm and calculates posterior by Qpost of AQM+.
5

Under review as a conference paper at ICLR 2019

Table 1: Test percentile mean rank (PMR) in 10th round. Baseline refers 0th round PMR of SL-Q. The results of comparative deep models in non-delta setting is from the paper of Das et al. (2017b).

Baseline SL-Q RL-QA AQM+ w/ indA AQM+ w/ depA AQM+ w/ trueA

non-delta 88.5 90.9 93.3

94.64

97.45

99.87

delta 95.45 95.72 95.69 97.17

98.25

99.22

(a) Non-Delta Hyperparameter Setting

(b) Delta Hyperparameter Setting

Figure 3: Test percentile mean ranks on GuessWhich experiments.
Non-delta vs. Delta Hyperparameter Setting The important issue in our GuessWhich experiment is delta setting. In the paper of Das et al. (2017b), SL-Q, RL-Q, and RL-QA algorithms achieve moderate increases of the performance. In SL-Q, 88.5% of percentile mean rank (PMR) is improved to 90.9%. In RL-QA, 90.6% of PMR is improved to 93.3%. Here, 90% of PMR at the zeroth turn means that the model can predict the correct image to be more likely than the other 8,665 images out of 9,628 candidates after exploiting the caption information solely. However, Das et al. (2017b) found that another hyperparameter setting, delta, makes much progress on their algorithm. Delta setting refers to different weights on loss and learning decay rate. Based on the authors' recent report on Github, SL-Q and RL-QA methods have shown that less than 6% of error is diminished through the dialog compared to the zeroth turn baseline which only uses generated caption. The PMR of the target (class) image which only uses the caption is around 95.5, but the dialog does not improve the PMR to more than 95.8. We use both non-delta setting (the setting in the original paper) and delta setting (the setting in Github) to test the performance of AQM+.
Other Experimental Setting As shown in Figure 2, our model uses five modules, Qgen, Qscore, aprxAgen, Qinfo, and Qpost. We use the same Qgen and Qscore modules as the comparative SL-Q model. In Visual Dialog, Qgen and Qscore share one RNN structure and have different output layers for each. The prior function is obtained from p^ (c|h0)  exp(- · f (c|h0)), where  is a balancing hyperparameter between prior and likelihood. We set |Ct,topk| = |Qt,gen| = |At,topk(qt)| = 20. The epoch for SL-Q is 60. The epoch for RL-Q and RL-QA is 15 for non-delta, and 20 for delta, respectively.
4.2 COMPARATIVE RESULTS
Figure 3 shows the PMR of the target image for our AQM+ and comparative models across the rounds. Figure 3a corresponds to the non-delta setting in the original paper Das et al. (2017b) and Figure 3b corresponds to the delta setting proposed in the Github code.
We see that SL-Q and RL-QA do not significantly improve the performance after a few rounds, especially for the delta setting. In delta setting, SL-Q increases their performance from 95.45% to 95.72% at 10th round, and RL-QA increases their performance from 95.44% to 95.69%. It means that error drop of SL-Q and RL-QA algorithms is 5.74% and 5.33%, respectively. On the other hand, AQM-indA increases its PMR from 95.45% to 96.53% at the fifth round and reach 97.17% at the 10th round. Likewise, AQM-depA increases its PMR from 95.45% to 97.48% at the fifth round and reach 98.25% at the 10th round, decreasing 61.5% of error.
6

Under review as a conference paper at ICLR 2019
(a) No Caption Experiment (indA, Non-delta) (b) Number of QAC Experiment (indA, Non-delta)
Figure 4: Left column shows the results on no caption experiments. Right column shows the result of ablation studies on different sizes of the subset of candidate questions, answers, and classes. In the plot, the size for three subsets are the same to K.
Figure 5: Qualitative results on image retrieval of AQM+. Left column shows true images and their corresponding caption, and right column contains selected top-k images.
4.3 ABLATION STUDY No Caption Experiments We test our AQM+ algorithm where no caption information exists. For the zeroth prediction, we simply replace the prior function from Qscore with a uniform function. Since Qgen in either SL-Q or RL-QA is trained also assuming the existence of the caption, we tried two alternative settings to approximate experiments without a caption. The first trial is the zero-caption experiment, where the caption vector is filled with zeros. In the second trial named random-caption experiment, we let the caption vector be a random caption vector, which is not related to the target image. Figure 4a shows that AQM+ performs well for both zero-vector and random-vector setting. By contrast, SL-Q and RL-QA do not work at all. It seems SL-Q and RLQA are not trained on the situation where zero-caption vector or even totally wrong caption vector comes. Though training SL-Q and RL-QA for these situations can increase their performance, it is evident that SL and RL algorithms are not robust for unexpected environments. Likewise, we also run no caption experiments for depA setting. For more ablation studies, see Figure 7 in Appendix C. Number of QAC We also change the size of subset K = |Qt,gen|=|At,topk(qt)|=|Ct,topk| to check our efficiency of information gain approximation, using non-delta setting. Figure 4b shows the experimental results. In the setting of non-delta and indA, 94.63% of PMR is achieved when K is 20, whereas 94.79% is achieved when K is 40. Note that 8 times (2 x 2 x 2) complexity increase just improves 0.16% of PMR, showing the efficiency of the setting of K in our experiments. On the other hand, this result also implies that increasing K would make further improvement on the performance. Likewise, in depA setting, changing K from 20 to 40 increases the PMR from 97.44% to 97.77%. For more ablation studies, see Figure 8 in Appendix C. Generated Questions and Selected Images Figure 5 shows the top-k images selected by AQM+'s posterior. Non-delta and indA setting is used. The figure shows that relevant images to the caption
7

Under review as a conference paper at ICLR 2019
remained after few dialog turns. The bottom number in the image denotes posterior of the image AQM+ think. We also compare selected examples of generated dialog of SL-Q, RL-QA, and AQM+ w/ indA for delta setting. See Figure 6 in Appendix B for the results.
5 DISCUSSION
5.1 DIFFICULTY OF GUESSWHICH
According to our results, we infer that PMR degradation of comparative SL and RL models during the dialog is not caused by forgetting dialog context to ask an appropriate question. Comparative results between AQM+ and Guesser show that the improvement from AQM+'s Qpost is significant, which implies that the major constraint of SL and RL is the limited capacity of RNN and its softmax score function.
Another reason for the poor performance lies in the current status of VQA models. According to Das et al. (2017a), the model structure used in our experiments can already reach 41.2% for answer retrieval accuracy from 100 candidate answers, solely using the question without exploiting image and history information. Fully exploiting these factors, however, increases the performance only slightly to 45.5%. As discrimination on different images relies on the other two information, Qbot is difficult to gain meaningful information through the dialog. Therefore, applying AQM+ to the GuessWhich problem means that we not only solve a very complicated problem, but also find that the AQM framework is applicable to the situation where the answer has high uncertainty.
5.2 TOWARD PRACTICAL APPLICATIONS
There are plenty of potential future works to improve the performance of AQM+ in real task-oriented dialog applications. For example, robust task-oriented dialog systems are required for appropriately replying to user's questions (Li et al., 2017) and responding for chit-chat style conversation (Zhao et al., 2017). The question quality can also be improved by diverse beam search approaches (Vijayakumar et al., 2016; Li et al., 2016), which prevent sampling similar questions for the candidate set. We highlight two issues described below; online learning and fast inference.
Online Learning For a novel answerer, fine-tuning on the dialog model is required (Krause et al., 2018). If the experiences of many users are available, model-agnostic meta learning (MAML) (Finn et al., 2017) can be applied for few-shot learning. Updating the hyperparameter  in an online manner, which balances the effect of the prior and the likelihood, can also be effective in practice. If the answer distribution of user is different from our aprxAgen, we can increase  to decrease the effect of the likelihood.
Fast Inference AQM+'s time complexity can be decreased further by changing the structure of aprxAgen. In specific, we can apply diverse methods such as skipping the update of hidden states in some steps (Seo et al., 2018), using convolution networks or self-attention networks (Yu et al., 2018; Vaswani et al., 2017), substituting matrix multiplication operation for hidden state update to weighted addition (Yu & Liu, 2018), and direct information gain inference from the neural networks (Belghazi et al., 2018).
6 CONCLUSION
Asking appropriate questions in practical applications has recently been paid attention (Rao & Daume´ III, 2018; Buck et al., 2018). We proposed AQM+ algorithm that is a large-scale extension of AQM framework. AQM+ can ask an appropriate question considering the context of the dialog, handle the responses in a sentence form, and efficiently estimate information gain of the target class with a given question. This improvement makes our AQM framework to step forward toward practical task-oriented applications. AQM+ not only outperforms the comparative SL and RL algorithms, but also enlarges the gap between AQM+ and the comparative algorithms comparing to the performance gaps reported in GuessWhat. AQM+ acheives more than 60% error decreases through the dialog, whereas the comparative algorithms only achieve 6% error decreases. Moreover, the performance of AQM+ can be boosted further by employing the recently proposed models in
8

Under review as a conference paper at ICLR 2019
the visual dialog field such as other question generator models (Jain et al., 2018) and the question answering models (Kottur et al., 2018).
REFERENCES
Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, and Aaron Courville. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.
Antoine Bordes and Jason Weston. Learning end-to-end goal-oriented dialog. In ICLR, 2017. Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby,
and Wei Wang. Ask the right questions: Active question reformulation with reinforcement learning. In ICLR, 2018. Prithvijit Chattopadhyay, Deshraj Yadav, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, and Devi Parikh. Evaluating visual conversational agents via cooperative human-ai games. arXiv preprint arXiv:1708.05122, 2017. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose´ MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017a. Abhishek Das, Satwik Kottur, Jose MF Moura, Stefan Lee, and Dhruv Batra. Learning cooperative visual dialog agents with deep reinforcement learning. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2970­2979. IEEE, 2017b. Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville. Guesswhat?! visual object discovery through multi-modal dialogue. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. Mihail Eric and Christopher D Manning. A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue. arXiv preprint arXiv:1701.04024, 2017. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pp. 1126­1135, 2017. Cheolho Han, Sang-Woo Lee, Yujung Heo, Wooyoung Kang, Jaehyun Jun, and Byoung-Tak Zhang. Criteria for human-compatible ai in two-player vision-language tasks. In 2017 IJCAI Workshop on Linguistic and Cognitive Approaches to Dialogue Agents, 2017. Unnat Jain, Svetlana Lazebnik, and Alexander G Schwing. Two can play this game: Visual dialog with discriminative question generation and answering. In Proc. CVPR, volume 1, 2018. Jin-Hwa Kim, Devi Parikh, Dhruv Batra, Byoung-Tak Zhang, and Yuandong Tian. Codraw: Visual dialog for collaborative drawing. arXiv preprint arXiv:1712.05558, 2017. Satwik Kottur, Jose MF Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. Visual coreference resolution in visual dialog using neural module networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 153­169, 2018. Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural sequence models. In ICML, 2018. Sang-Woo Lee, Yu-Jung Heo, and Byoung-Tak Zhang. Answerer in questioner's mind for goal-oriented visual dialogue. 2018. Jiwei Li, Will Monroe, and Dan Jurafsky. A simple, fast diverse decoding algorithm for neural generation. arXiv preprint arXiv:1611.08562, 2016. Xiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao, and Asli Celikyilmaz. End-to-end task-completion neural dialogue systems. arXiv preprint arXiv:1703.01008, 2017. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pp. 740­755. Springer, 2014. Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, and Dhruv Batra. Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model. In Advances in Neural Information Processing Systems, pp. 314­324, 2017. Sudha Rao and Hal Daume´ III. Learning to ask good questions: Ranking clarification questions using neural expected value of perfect information. arXiv preprint arXiv:1805.04655, 2018.
9

Under review as a conference paper at ICLR 2019
Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Neural speed reading via skim-rnn. In ICLR, 2018.
Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, and Leonid Sigal. Visual reference resolution using attention memory for visual dialog. In Advances in neural information processing systems, pp. 3719­3729, 2017.
Florian Strub, Harm de Vries, Jeremie Mary, Bilal Piot, Aaron Courville, and Olivier Pietquin. End-to-end optimization of goal-driven and visually grounded dialogue systems. arXiv preprint arXiv:1703.05423, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998­6008, 2017.
Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016.
Oriol Vinyals and Quoc Le. A neural conversational model. In ICML Deep Learning Workshop, 2015. Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M Rojas-Barahona, Pei-Hao Su, Stefan
Ultes, and Steve Young. A network-based end-to-end trainable task-oriented dialogue system. arXiv preprint arXiv:1604.04562, 2016. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading comprehension. In ICLR, 2018. Zeping Yu and Gongshen Liu. Sliced recurrent neural networks. arXiv preprint arXiv:1807.02291, 2018. Jiaping Zhang, Tiancheng Zhao, and Zhou Yu. Multimodal hierarchical reinforcement learning policy for taskoriented visual dialog. arXiv preprint arXiv:1805.03257, 2018a. Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, Jianfeng Lu, and Anton Van Den Hengel. Goal-oriented visual question generation via intermediate rewards. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 186­201, 2018b. Tiancheng Zhao and Maxine Eskenazi. Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning. arXiv preprint arXiv:1606.02560, 2016. Tiancheng Zhao, Allen Lu, Kyusong Lee, and Maxine Eskenazi. Generative encoder-decoder models for taskoriented spoken dialog systems with chatting capability. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pp. 27­36, 2017. Tiancheng Zhao, Kyusong Lee, and Maxine Eskenazi. Unsupervised discrete sentence representation learning for interpretable neural dialog generation. arXiv preprint arXiv:1804.08069, 2018.
10

Under review as a conference paper at ICLR 2019
APPENDIX A. AQM+ ALGORITHM
The question generating process of AQM+ used in our GuessWhich experiments are as follows. Algorithm 1 Question Generating Process of AQM+ in Our GuessWhich Experiments
p^ (c|h0)  exp(- · f (c|h0)) for t = 1:T do
Ct,topk  top-K posterior test image (from Qpost p^(c|ht-1)) Qt,gen  top-K likelihood questions using beam search (from Qgen p(qt|ht-1)) At,topk(qt)  generated answers from aprxAgen for question qt and each class in Ct,topk (from aprxAgen p~(at|c, qt, ht-1)) qt  argmaxqtQt,gen I~[C, At; qt, a1:t-1, q1:t-1] with At,topk(qt) and Ct,topk in Eq. 1 Get at from Agen p¯(at|c, qt, ht-1) Update Qpost p^(c|ht)  p~(at|c, qt, ht-1) · p^(c|ht-1) in Eq. 2 end for
11

Under review as a conference paper at ICLR 2019
APPENDIX B. GENERATING SENTENCES
Figure 6 shows selected examples of generated question in delta setting. Though delta setting boosts to increase PMR of 0th turn much, it degenerate the question quality, especially for RL-QA. Especially RL-QA prunes to concentrate 1st turn, remaining questions and answers of other turns meaningless.
Figure 6: Selected examples of generated dialog in delta setting.
12

Under review as a conference paper at ICLR 2019
APPENDIX C. ABLATION STUDY
(a) Number of QAC Experiment (depA, Non-delta) (b) Number of QAC Experiment (trueA, Non-delta)
Figure 7: Ablation studies on different size of the subset of candidate questions, answers, and classes. The size for three subsets are the same to K.
(a) No Caption Experiment (depA, Non-delta)
Figure 8: Ablation study on no caption experiments. Non-delta and depA setting is used.
13

