Under review as a conference paper at ICLR 2019
VARIATIONAL AUTOENCODERS WITH JOINTLY OPTIMIZED LATENT DEPENDENCY STRUCTURE
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a method for learning the dependency structure between latent variables in deep latent variable models. Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single variational objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values. We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10. Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.
1 INTRODUCTION
Deep latent variable models offer an effective method for automatically learning structure from data. By explicitly modeling the data distribution using latent variables, these models are capable of learning compressed representations that are then relevant for downstream tasks. Such models have been applied across a wide array of domains, such as images (Gregor et al., 2014; Kingma & Welling, 2014; Rezende et al., 2014; Gregor et al., 2015), audio (Chung et al., 2015; Fraccaro et al., 2016), video (He et al., 2018; Yingzhen & Mandt, 2018), and text (Bowman et al., 2016; Krishnan et al., 2017). However, despite their success, latent variable models are often formulated with simple (e.g. Gaussian) distributions, making independence assumptions among the latent variables. That is, each latent variable is sampled independently. Ignoring the dependencies between latent variables limits the flexibility of these models, negatively impacting the model's ability to fit the data.
Latent dependencies can be incorporated both during inference as well as in the model itself. For instance, normalizing flows (Rezende & Mohamed, 2015) accounts for latent dependencies during inference by learning a mapping from a simple distribution to a more complex distribution that contains these dependencies. On the modeling side, one can directly add dependencies by constructing a hierarchical latent representation (Dayan et al., 1995). These structures consist of conditional (empirical) priors, in which one latent variable forms a prior on another latent variable. While this conditional distribution may take a simple form, marginalizing over the parent variable can result in an arbitrarily complex distribution. Models with these more flexible latent dependency structures have been shown to result in improved performance (Sønderby et al., 2016; Burda et al., 2016; Kingma et al., 2016). However, despite the benefits of including additional structure in these models, latent dependency structures have been predefined, potentially limiting the performance of this approach.
In this work, we propose a method for learning dependency structures in latent variable models. Structure learning is a difficult task with a long history in the graphical models community (Koller & Friedman, 2009). Over the years, it has been tackled from several perspectives, including constraint-based approaches (Cheng et al., 2002; Lehmann & Romano, 2008), optimization of structure scores (Kass & Raftery, 1995; Heckerman et al., 1995; Barron et al., 1998), Bayesian model averaging (Heckerman et al., 1999; Koivisto & Sood, 2004), and many more. Unfortunately, the underlying objectives are often limited to graphs of a particular form (e.g., limited tree width),
1

Under review as a conference paper at ICLR 2019

prohibitively expensive, or difficult to integrate with the gradient-based optimization techniques of modern neural networks. Here, we discuss a variational end-to-end approach for general graph structures introducing minimal complexity overhead. In particular, we introduce a set of binary global variables to gate the latent dependencies. The whole model (including its structure) is jointly optimized with a single stochastic variational inference objective. In our experimental validation, we show that the learned dependency structures contribute to a more accurate representation of the true generative distribution, outperforming several other variants of variational autoencoders.

2 BACKGROUND

2.1 VARIATIONAL INFERENCE & VARIATIONAL AUTOENCODERS

A latent variable model, defined by the joint distribution, p(x, z) = p(x z)p(z), models each data example, x, using a local latent variable, z, and global parameters, . p(x z) denotes the conditional likelihood, and p(z) denotes the prior. Latent variable models are capable of capturing the structure present in data, with z forming a compressed representation of each data example. Unfortu-

nately, inferring the posterior, p(z x), is typically computationally intractable, prompting the use of approximate inference techniques. Variational inference (Jordan et al., 1999) introduces an approxi-

mate posterior, q(z x), and optimizes variational parameters, , to minimize the KL-divergence to the true posterior, KL(q(z x) p(z x)). As this quantity cannot be evaluated directly, the follow-
ing relation is used:

log p(x) = KL(q(z x) p(z x)) + L(x; , ),

(1)

where L(x; , ) is the evidence lower bound (ELBO), defined as

L(x; , ) = Eq(z x) [log p(x z)] - KL(q(z x) p(z)).

(2)

In Eq. (1), log p(x) is independent of , so we can minimize the KL divergence term, i.e. perform approximate inference, by maximizing L(x; , ) w.r.t. q(z x). Further, because KL divergence is non-negative, L(x; , ) is a lower bound on log p(x), meaning we can then learn the model parameters by maximizing L(x; , ) w.r.t. .

Variational autoencoders (VAEs) (Kingma & Welling, 2014; Rezende et al., 2014) amortize inference optimization across data examples by parameterizing q(z x) as a separate inference model, then jointly optimizing the model parameters  and . VAEs instantiate both the inference model
and latent variable model with deep networks, allowing them to scale to high-dimensional data.
However, VAEs are typically implemented with basic graphical structures and simple, unimodal
distributions (e.g. Gaussians). For instance, the dimensions of the prior are often assumed to be independent, p(z) = m p(zm), with a common assumption being a fixed standard Gaussian: p(z) = N (z; 0, I). Similarly, approximate posteriors often make the mean field assumption, q(z x) = m q(zm x). Independence assumptions such as these may be overly restrictive, thereby limiting modeling capabilities.

2.2 EMPIRICAL PRIORS THROUGH LATENT DEPENDENCY STRUCTURE

One technique for improving the expressive capacity of latent variable models is through incorpo-

rating dependency structure among the latent variables, forming a hierarchy (Dayan et al., 1995;

Rezende et al., 2014). These dependencies provide empirical priors, learned priors that are conditioned on other latent variables. With M latent dimensions, the full prior takes the following

auto-regressive form:

M
p(z) = p(zm zpa(m)),
m=1

(3)

where zpa(m) denotes the vector of latent variables constituting the parents of zm. Each conditional distribution can be parameterized by deep networks that output the parameters of distributions, e.g.

mean and variance of a Gaussian. While these conditional distributions may be relatively simple, the
marginal empirical prior, p(zm) =  p(zm zpa(m))p(zpa(m))dzpa(m), can be arbitrarily complex. By using more flexible priors, models with latent dependency structure are less restrictive in their

latent representations, hopefully capturing more information and enabling a better fit to the data.

2

Under review as a conference paper at ICLR 2019

fixed
<latexit sha1_base64="gYR545rgH3Kn7mTe5KJTJ+wWapM=">AAACKXicbVDLTsJAFJ36RHxVjSs3E8HEFWnZyJLEjUtM5JEAIdPpLUyYTpuZqYFUPsatfI07det3mDiULgS8yUxOzrkn9+R4MWdKO86ntbW9s7u3XzgoHh4dn5zaZ+ctFSWSQpNGPJIdjyjgTEBTM82hE0sgoceh7Y3vF3r7GaRikXjS0xj6IRkKFjBKtKEG9mXAJuDj8ksZM+FDDOYTemCXnIqTDd4Ebg5KKJ/GwP7p+RFNQuOlnCjVdZ1Y91MiNaMcZsVeoiAmdEyG0DVQkBBUP83iz/CNYXwcRNI8oXHG/nWkJFRqGnpmMyR6pNa1Bfmf1k10UOunTMSJBkGXh4KEYx3hRRfYZxKo5lMDCJXMZMV0RCSh2jS2cmUoSTxidDJbZUFkuYqmL3e9nU3QqlZcp+I+Vkv1Wt5cAV2ha3SLXHSH6ugBNVATUZSiV/SG5tbcerc+rK/l6paVey7Qyljfv8nSp04=</latexit>
p (z)
<latexit sha1_base64="1dUS6S6ffy0cF47jR+3gVKUA8lE=">AAACV3icbVDBThsxFPQuLaQptAk99mI1QUou0W4ucEFC4tIjSA0gZaOV13mbWNhey35bEVb7H3wN1/ILfA04IVIb0pEsjWbe0xtPZqRwGEXPQbjz4ePuXuNT8/P+wZevrfbhlStKy2HEC1nYm4w5kELDCAVKuDEWmMokXGe350v/+jdYJwr9CxcGJorNtMgFZ+iltDXsmjTBOSCjvUQxnGd5dV/36SlNjC2mqaJ//fu0UnW/m7Y60SBagW6TeE06ZI2LtB0cJNOClwo0csmcG8eRwUnFLAouoW4mpQPD+C2bwdhTzRS4SbX6XE2PvDKleWH900hX6r8bFVPOLVTmJ5f53XtvKf7PG5eYn0wqoU2JoPnbobyUFAu6bIpOhQWOcuEJ41b4rJTPmWUcfZ8bV2aWmbngd/WmCnqVa0PMVN30Bcbv69omV8NBHA3iy2Hn7GRdZYN8Jz9Ij8TkmJyRn+SCjAgnD+SR/CFPwXPwEu6GjbfRMFjvfCMbCNuvGtC1Ig==</latexit>

| inQdependent = m p(zm)

zm <latexit sha1_base64="FQUFB8LNxtSLjqOl+XLS2P4F8MM=">AAACKHicbVDNTgIxGGz9RRQFPXppBBNPZJeLHkm8eMTEBRLYkG7pQkPb3bRdI272GbzqK/g03gxXn8SycHDBSZpMZr4v33SCmDNtHGcBd3b39g8OS0fl45PK6Vm1dt7VUaII9UjEI9UPsKacSeoZZjjtx4piEXDaC2b3S7/3TJVmkXwy85j6Ak8kCxnBxkpe43UkGqNq3Wk6OdA2cdekDtbojGqwMhxHJBFUGsKx1gPXiY2fYmUY4TQrDxNNY0xmeEIHlkosqPbTPG2Grq0yRmGk7JMG5erfjRQLrecisJMCm6ne9Jbif94gMeGdnzIZJ4ZKsjoUJhyZCC2/jsZMUWL43BJMFLNZEZlihYmxBRWuTBSOp4y8ZEWVyjxXQQxEVrYFupt1bZNuq+k6TfexVW+31lWWwCW4AjfABbegDR5AB3iAAAbewDv4gJ/wC37DxWp0B653LkAB8OcXcdKmEA==</latexit> ... <latexit sha1_base64="GeFPqah0GabDOiVnSUd8mdAsBt0=">AAACGnicbVDLTgIxFO34RHyhLt00gokrMsNGl0Q3LjGRRwIT0ul0oKHTTto7RDLhJ9zK17gzbt34MSYWmIWAJ2lycs69uacnSAQ34Lrfztb2zu7efuGgeHh0fHJaOjtvGZVqyppUCaU7ATFMcMmawEGwTqIZiQPB2sHoYe63x0wbruQzTBLmx2QgecQpASt1Kr1xqMBU+qWyW3UXwJvEy0kZ5Wj0Sz+9UNE0ZhKoIMZ0PTcBPyMaOBVsWuylhiWEjsiAdS2VJGbGzxZ5p/jaKiGOlLZPAl6ofzcyEhsziQM7GRMYmnVvLv7ndVOI7vyMyyQFJunyUJQKDArPP49DrhkFMbGEUM1tVkyHRBMKtqKVKwNNkiGnL9NVlclFrqLty1tvZ5O0alXPrXpPtXL9Pm+ugC7RFbpBHrpFdfSIGqiJKBLoFb2hmTNz3p0P53M5uuXkOxdoBc7XLxVxodc=</latexit>

z <latexit sha1_base64="qv/I6fmAzetHscBr4SeDOeeZ+LU=">AAACF3icbVA9TwJBFHznJ+IXammzkZhYkTsarQyJjSUmHpAAIXvLO9iwt3fZ3TPihd9gK7/Gztha+mNMXA4KASfZZDLzXt7sBIng2rjut7OxubW9s1vYK+4fHB4dl05OGzpOFUOfxSJWrYBqFFyib7gR2EoU0igQ2AxGdzO/+YRK81g+mnGC3YgOJA85o8ZKficIyUuvVHYrbg6yTrwFKcMC9V7pp9OPWRqhNExQrduem5huRpXhTOCk2Ek1JpSN6ADblkoaoe5medgJubRKn4Sxsk8akqt/NzIaaT2OAjsZUTPUq95M/M9rpya86WZcJqlByeaHwlQQE5PZz0mfK2RGjC2hTHGblbAhVZQZ28/SlYGiyZCz58myijLPVbR9eavtrJNGteK5Fe+hWq7dLporwDlcwBV4cA01uIc6+MCAwyu8wdSZOu/Oh/M5H91wFjtnsATn6xfKk6Cf</latexit>

<latexit sha1_base64="H9c4lv8scahYYuN6NzGl+NKdbDc=">AAACGHicbVDLTsJAFL31ifhCXbppBBNXpGWjS6Ibl5jII4GGTIdbGJlOm5mpkTT8g1v5GnfGrTs/xsShdCHgSSY5Oefe3DPHjzlT2nG+rY3Nre2d3cJecf/g8Oi4dHLaUlEiKTZpxCPZ8YlCzgQ2NdMcO7FEEvoc2/74bu63n1EqFolHPYnRC8lQsIBRoo3UqvTiEav0S2Wn6mSw14mbkzLkaPRLP71BRJMQhaacKNV1nVh7KZGaUY7TYi9RGBM6JkPsGipIiMpLs7RT+9IoAzuIpHlC25n6dyMloVKT0DeTIdEjterNxf+8bqKDGy9lIk40Cro4FCTc1pE9/7o9YBKp5hNDCJXMZLXpiEhCtSlo6cpQEtMLfZkuqyiyXEXTl7vazjpp1aquU3UfauX6bd5cAc7hAq7AhWuowz00oAkUnuAV3mBmzax368P6XIxuWPnOGSzB+voFM8Gg1A==</latexit>
 <latexit sha1_base64="xWmXra5P90YYt2hGo0QW8XWb5is=">AAACGnicbVDLTsJAFJ3iC/GFunTTCCauSMtGl0Q3LjGRRwINmQ63dMJ02szcGgnhJ9zK17gzbt34MSYOpQsBTzLJyTn35p45fiK4Rsf5tgpb2zu7e8X90sHh0fFJ+fSsreNUMWixWMSq61MNgktoIUcB3UQBjXwBHX98v/A7z6A0j+UTThLwIjqSPOCMopG61T6GgLQ6KFecmpPB3iRuTiokR3NQ/ukPY5ZGIJEJqnXPdRL0plQhZwJmpX6qIaFsTEfQM1TSCLQ3zfLO7CujDO0gVuZJtDP178aURlpPIt9MRhRDve4txP+8XorBrTflMkkRJFseClJhY2wvPm8PuQKGYmIIZYqbrDYLqaIMTUUrV0aKJiFnL7NVFWSWq2T6ctfb2STtes11au5jvdK4y5srkgtySa6JS25IgzyQJmkRRgR5JW9kbs2td+vD+lyOFqx855yswPr6Belyob0=</latexit>

fixed
<latexit sha1_base64="hPRkNjEm+gQwq88DXy1G2kWr7vA=">AAACP3icbZDPSgMxEMaz9V+tVlu96SVYBS+W3V70KHjxWMG2Qi0lm51tg9nsksxKS13wabzqK/gYPoE38erNtN2DVT8IfPxmhpl8fiKFQdd9cwpLyyura8X10sZmeWu7Ut1pmzjVHFo8lrG+8ZkBKRS0UKCEm0QDi3wJHf/uYlrv3IM2IlbXOE6gF7GBEqHgDC3qV/ZCMYKAHj4cUj5kQp0Y1CnHVEPQr9TcujsT/Wu83NRIrma/6pRvg5inESjkkhnT9dwEexOmUXAJWek2NZAwfscG0LVWsQhMbzL7REaPLAloGGv7FNIZ/TkxYZEx48i3nRHDofldm8L/at0Uw7PeRKgkRVB8vihMJcWYThOhgdDAUY6tYVwLe+s0Cs042twWtgw0S4aCj7JFCmp21wL0o6xkA/R+x/XXtBt1z617V43a+VkeZZHskwNyTDxySs7JJWmSFuHkkTyRZ/LivDrvzofzOW8tOPnMLlmQ8/UNhH+vJA==</latexit>
p (z)
<latexit sha1_base64="UhYXzIE19f4UKdU35cK5F+u1svA=">AAACbXicbVDRShtBFJ2srU1jtdHSJ0sZjKIihN28mBch4EseFRoVsmGZndxNhszODjN3xWTdX/Jr+tKH+gv+gpMYsNEeGDhzzr3ce0+spbDo+38q3tqHj+ufqp9rG182t77Wt3eubJYbDj2eyczcxMyCFAp6KFDCjTbA0ljCdTw5n/vXt2CsyNQvnGoYpGykRCI4QydF9e6+jkIcAzJ6FKYMx3FSzMpjekZDbbJhpOirX4RxQmdlVKiS3tPX30lQHu9H9Ybf9Beg70mwJA2yxEW0XdkMhxnPU1DIJbO2H/gaBwUzKLiEshbmFjTjEzaCvqOKpWAHxeLkkh44ZUiTzLinkC7UfzsKllo7TWNXOb/KvvXm4v+8fo5Je1AIpXMExV8GJbmkmNF5fnQoDHCUU0cYN8LtSvmYGcbRpbwyZWSYHgt+V66qoBZ7rYhxWtZcgMHbuN6Tq1Yz8JvBZavRaS+jrJJdskeOSEBOSYd0yQXpEU4eyG/ylzxWnrzv3g/v50upV1n2fCMr8A6fAb9fvNA=</latexit>

| chQain-structured = n p(zn|zn+1)

zn+1
<latexit sha1_base64="rHCOl6RFYoxdcBQQIKMJMWSVD5s=">AAACNHicbVDLSsNAFJ34rNXaVpduBltBEErSjS4LblxWsA9oQ5hMJ+3QmUmYmYg15Evc6i/4L4I7ces3OE2zMK0HLhzOuZd7OH7EqNK2/WFtbe/s7u2XDsqHR5Xjaq1+0ldhLDHp4ZCFcugjRRgVpKepZmQYSYK4z8jAn98u/cEjkYqG4kEvIuJyNBU0oBhpI3m1ajMZ+wF8Tr1EXDlp06s17JadAW4SJycNkKPr1a3KeBLimBOhMUNKjRw70m6CpKaYkbQ8jhWJEJ6jKRkZKhAnyk2y5Cm8MMoEBqE0IzTM1L8XCeJKLbhvNjnSM7XuLcX/vFGsgxs3oSKKNRF49SiIGdQhXNYAJ1QSrNnCEIQlNVkhniGJsDZlFb5MJYpmFD+lRZWILFdB9HlaNgU663Vtkn675dgt577d6LTzKkvgDJyDS+CAa9ABd6ALegCDGLyAV/BmvVuf1pf1vVrdsvKbU1CA9fMLxJGqNg==</latexit>

zn
<latexit sha1_base64="C9SlwxFRYjXvyZMtIqwdEOkDqc4=">AAACLnicbVDLSsNAFJ34rNVqq0s3g63gqiTd6LLgxmUF+4A0lMl00g6dR5iZiDXkM9zqL/g1ggtx62c4TbswrQcuHM65l3s4YcyoNq776Wxt7+zu7ZcOyodHleOTau20p2WiMOliyaQahEgTRgXpGmoYGcSKIB4y0g9ntwu//0iUplI8mHlMAo4mgkYUI2Mlv5EOwwg+ZyPRGFXrbtPNATeJtyJ1sEJnVHMqw7HECSfCYIa09j03NkGKlKGYkaw8TDSJEZ6hCfEtFYgTHaR55gxeWmUMI6nsCANz9e9FirjWcx7aTY7MVK97C/E/z09MdBOkVMSJIQIvH0UJg0bCRQFwTBXBhs0tQVhRmxXiKVIIG1tT4ctEoXhK8VNWVInIcxXEkGdlW6C3Xtcm6bWantv07lv1dmtVZQmcgwtwBTxwDdrgDnRAF2AgwQt4BW/Ou/PhfDnfy9UtZ3VzBgpwfn4BS5qoiQ==</latexit>

... <latexit sha1_base64="GeFPqah0GabDOiVnSUd8mdAsBt0=">AAACGnicbVDLTgIxFO34RHyhLt00gokrMsNGl0Q3LjGRRwIT0ul0oKHTTto7RDLhJ9zK17gzbt34MSYWmIWAJ2lycs69uacnSAQ34Lrfztb2zu7efuGgeHh0fHJaOjtvGZVqyppUCaU7ATFMcMmawEGwTqIZiQPB2sHoYe63x0wbruQzTBLmx2QgecQpASt1Kr1xqMBU+qWyW3UXwJvEy0kZ5Wj0Sz+9UNE0ZhKoIMZ0PTcBPyMaOBVsWuylhiWEjsiAdS2VJGbGzxZ5p/jaKiGOlLZPAl6ofzcyEhsziQM7GRMYmnVvLv7ndVOI7vyMyyQFJunyUJQKDArPP49DrhkFMbGEUM1tVkyHRBMKtqKVKwNNkiGnL9NVlclFrqLty1tvZ5O0alXPrXpPtXL9Pm+ugC7RFbpBHrpFdfSIGqiJKBLoFb2hmTNz3p0P53M5uuXkOxdoBc7XLxVxodc=</latexit>

z <latexit sha1_base64="qv/I6fmAzetHscBr4SeDOeeZ+LU=">AAACF3icbVA9TwJBFHznJ+IXammzkZhYkTsarQyJjSUmHpAAIXvLO9iwt3fZ3TPihd9gK7/Gztha+mNMXA4KASfZZDLzXt7sBIng2rjut7OxubW9s1vYK+4fHB4dl05OGzpOFUOfxSJWrYBqFFyib7gR2EoU0igQ2AxGdzO/+YRK81g+mnGC3YgOJA85o8ZKficIyUuvVHYrbg6yTrwFKcMC9V7pp9OPWRqhNExQrduem5huRpXhTOCk2Ek1JpSN6ADblkoaoe5medgJubRKn4Sxsk8akqt/NzIaaT2OAjsZUTPUq95M/M9rpya86WZcJqlByeaHwlQQE5PZz0mfK2RGjC2hTHGblbAhVZQZ28/SlYGiyZCz58myijLPVbR9eavtrJNGteK5Fe+hWq7dLporwDlcwBV4cA01uIc6+MCAwyu8wdSZOu/Oh/M5H91wFjtnsATn6xfKk6Cf</latexit>

<latexit sha1_base64="H9c4lv8scahYYuN6NzGl+NKdbDc=">AAACGHicbVDLTsJAFL31ifhCXbppBBNXpGWjS6Ibl5jII4GGTIdbGJlOm5mpkTT8g1v5GnfGrTs/xsShdCHgSSY5Oefe3DPHjzlT2nG+rY3Nre2d3cJecf/g8Oi4dHLaUlEiKTZpxCPZ8YlCzgQ2NdMcO7FEEvoc2/74bu63n1EqFolHPYnRC8lQsIBRoo3UqvTiEav0S2Wn6mSw14mbkzLkaPRLP71BRJMQhaacKNV1nVh7KZGaUY7TYi9RGBM6JkPsGipIiMpLs7RT+9IoAzuIpHlC25n6dyMloVKT0DeTIdEjterNxf+8bqKDGy9lIk40Cro4FCTc1pE9/7o9YBKp5hNDCJXMZLXpiEhCtSlo6cpQEtMLfZkuqyiyXEXTl7vazjpp1aquU3UfauX6bd5cAc7hAq7AhWuowz00oAkUnuAV3mBmzax368P6XIxuWPnOGSzB+voFM8Gg1A==</latexit>
 <latexit sha1_base64="xWmXra5P90YYt2hGo0QW8XWb5is=">AAACGnicbVDLTsJAFJ3iC/GFunTTCCauSMtGl0Q3LjGRRwINmQ63dMJ02szcGgnhJ9zK17gzbt34MSYOpQsBTzLJyTn35p45fiK4Rsf5tgpb2zu7e8X90sHh0fFJ+fSsreNUMWixWMSq61MNgktoIUcB3UQBjXwBHX98v/A7z6A0j+UTThLwIjqSPOCMopG61T6GgLQ6KFecmpPB3iRuTiokR3NQ/ukPY5ZGIJEJqnXPdRL0plQhZwJmpX6qIaFsTEfQM1TSCLQ3zfLO7CujDO0gVuZJtDP178aURlpPIt9MRhRDve4txP+8XorBrTflMkkRJFseClJhY2wvPm8PuQKGYmIIZYqbrDYLqaIMTUUrV0aKJiFnL7NVFWSWq2T6ctfb2STtes11au5jvdK4y5srkgtySa6JS25IgzyQJmkRRgR5JW9kbs2td+vD+lyOFqx855yswPr6Belyob0=</latexit>

p

(z|c) l=earQneNnd=1|<latexitsha1_base64="OeurA8LYF6xtHZMoethndPNQXWs=">AAACP3icbVDLTgIxFO34RBQF3emmEUxckRk2siRx4xITeSQwIZ1yBxo6nUnbMZBxEr/Grf6Cn+EXuDNu3VkGFgKepMnJOffk3h4v4kxp2/6wtrZ3dvf2cwf5w6PC8UmxdNpWYSwptGjIQ9n1iALOBLQ00xy6kQQSeBw63uR27nceQSoWigc9i8ANyEgwn1GijTQonnMgUsAQV54q2OcwZSaJR5JE40GxbFftDHiTOEtSRks0ByWr0B+GNA5AaMqJUj3HjrSbEKkZ5ZDm+7GCiNAJGUHPUEECUG6SfSLFV0YZYj+U5gmNM/VvIiGBUrPAM5MB0WO17s3F/7xerP26mzARxRoEXSzyY451iOeN4CGTQDWfGUKoZOZWTMdEEqpNbytbsloYnaarKojsrhXRC9K8KdBZr2uTtGtVx64697Vyo76sMocu0CW6Rg66QQ10h5qohSh6Ri/oFb1Z79an9WV9L0a3rGXmDK3A+vkFG3iu5w==</latexit>

<latexit sha1_base64="Lf7b1QdpGvoq7C4rJFZntTGpVbXf7Py/0QtIXLt76b+8EoFDfH28yw4Hr1b4MCojrfmhRV/15bh708=">AAACsXicbVHFNbhxMxEPYUuXfyVQmSsOGIFCxiYMtUpJkQhJkRqIkVKbtQqLFBQ3uqpJFVIkLJ1Qk0hZlw+J1ZhMrtXqte9lyZz16KYHGDNix4n9W/+AWHh/+BCR1vQi8eANfv2gewCApGfkAF9KSmRp0CUo5rTkStj6SmJfTkY+p9fP88/99f/No8NMTz6WOGkZaTycEJbtQghUkFfMqY6rPLo4+3BTOvxGD0H9eu+DXwx8v/tveP/oej4Op4D8eh3b6Hr1y/H9j7PwKDB+7Zf+H8L/PbzWRPCnVi5b3X7NlbsMVjoOKbIVcDxrmOrJyexUyVpNwJbRxncIZMmsoAWyUGCpFEFgAIjiVxEcKRolGIHQtRCLpMbFDbYRAIViumikY38SCWLRHVbLPrT8/m8u00+AdSs+Ua+VJgSGbnCtG/KiBV9pQYJ9VGwJXpYGWjqFMYFtasSmLysukESRZCuBie8A7rMvQPU/jZ2WZn725flh+n04oN0ATXwRAQvAUuygZ2oY7kjs2gkvKhyYLouajsnsIp8r37v/8qo74o/+pzw/Pvo+3/Aw3p9D3KXfpMT02qOakriaTdFP7bolOL09Kl3RoV/NMYaH/zZ5e5tf6e/gup+TcOopX3yJmSbvYvGy8blsrxsgaxbQrItRkKG4qExRKzljPRDFUfrm1wZrNnm/re6qvbrPvH4rj7r2anhrq6M7L4t5jR/F5kQVr53fVp4+ppy2Tux9FsZENFgobWng4XeNQ90mDyXeIpC+kbB0XeBt1f1R5YZhb6vSoh3/f/TIWeSQ4tsNezPKQ9DoLjIOk4JD1XnNCJZulnqbwCYwIqUGJcQCwsG2mXzsKznNLcgpSRJExH3Gm8mlncqN1BDs4Aboygxu4BwBZbWJvScBOKyo6EJDkNmsFafLMahnmXJsMb4PA4rFTNQ0DszQxUeLsrBwmtdFE7K6yrvzs9udoGFoc3fpnCepnWtRZ2v0GDc91TELw+K45d5ax9RCSrudFtmeaFtR3lHsOT87UfLrabV1rFVZ0rHVy5zmLcGmZ91ucs5uYr63k8b7y1Wa5YkFQ3Pe2aNpMkYL0m8qzdNFE4+3oKnLWtpSlVCsKs6EaxQLlTBes8NDO08hykiUpvWSJLOMGplWSlzANjzIxeogbTBME2WjjaGlKAolf1QM4ex5Msw4G+xLgF2nRtfpldXbzVlKBDZF8pOmwPZwoLjC37v5X7STttOZW1n63QmYqXhu7gmhF2+y47EXFdLZZu5UTlwgOWgu1w5d3nsYm3iusHrcNzrCAr2h4X/8wOgH0uhyTM/9YX9fbodfB15E+7Z5byt2dB/HrsAKPT3iRczI36P4/FIY3dEEe0eomnT2lZcBULwMBHnek5ZAw+QX7pjOkBZSi6ccJjykwsVBnHsfy5S6IMuG7/XIJ0GmOjHRkDqTyK3TNv384gVGfVzv44+AO9k6XxcQ4fSvMd80wEjat3/a4g9OevzF2D4aSbpZrJY4jXsUDMj/IrejU3rlP2XAyIglEz6==E+Ay<38/+zlTUatZ2LNenxEiS=<t/>latexit>

flexible graph p(zn|zpa(n), cpa(n),n)

zpa(n)
<latexit sha1_base64="BxEDPI6F5q5NyMlo13JxEGjaF0s=">AAACP3icbZDLSgMxFIYzXmu9tbrTTbAKuikz3ehScOOygm2FtpRMeqYNJpkhOSPWYcCncauv4GP4BO7ErTvTy8KqPwR+vnMO5+QPEyks+v6bt7C4tLyyWlgrrm9sbm2XyjtNG6eGQ4PHMjY3IbMghYYGCpRwkxhgKpTQCm8vxvXWHRgrYn2NowS6ig20iARn6FCvtHeYdcKIPuS9rINwj0ZlCcuP9Ul+2CtV/Ko/Ef1rgpmpkJnqvbK32enHPFWgkUtmbTvwE+xmzKDgEvJiJ7WQMH7LBtB2VjMFtptNPpHTI0f6NIqNexrphP6cyJiydqRC16kYDu3v2hj+V2unGJ11M6GTFEHz6aIolRRjOk6E9oUBjnLkDONGuFspHzLDOLrc5rYMDEuGgt/n8xT05K45GKq86AIMfsf11zRr1cCvBle1ynltFmWB7JMDckwCckrOySWpkwbh5JE8kWfy4r16796H9zltXfBmM7tkTt7XNwTLr2I=</latexit>

c <latexit sha1_base64="L5vsFdfC3adaiAy6SN/mhTZlff4=">AAACGXicbVC7TgJBFL2LL8QXamkzEUysyC6NlkQbS0zkkcCGzA53YcLs7GZm1kgIH2ErX2NnbK38GBOHhULAk0xycs69uWdOkAiujet+O7mt7Z3dvfx+4eDw6PikeHrW1HGqGDZYLGLVDqhGwSU2DDcC24lCGgUCW8Hofu63nlFpHssnM07Qj+hA8pAzaqzUKneDkLByr1hyK24Gskm8JSnBEvVe8afbj1kaoTRMUK07npsYf0KV4UzgtNBNNSaUjegAO5ZKGqH2J1ncKbmySp+EsbJPGpKpfzcmNNJ6HAV2MqJmqNe9ufif10lNeOtPuExSg5ItDoWpICYm87+TPlfIjBhbQpniNithQ6ooM7ahlSsDRZMhZy/TVRVllqtg+/LW29kkzWrFcyveY7VUu1s2l4cLuIRr8OAGavAAdWgAgxG8whvMnJnz7nw4n4vRnLPcOYcVOF+/awSg6A==</latexit>

zn
<latexit sha1_base64="C9SlwxFRYjXvyZMtIqwdEOkDqc4=">AAACLnicbVDLSsNAFJ34rNVqq0s3g63gqiTd6LLgxmUF+4A0lMl00g6dR5iZiDXkM9zqL/g1ggtx62c4TbswrQcuHM65l3s4YcyoNq776Wxt7+zu7ZcOyodHleOTau20p2WiMOliyaQahEgTRgXpGmoYGcSKIB4y0g9ntwu//0iUplI8mHlMAo4mgkYUI2Mlv5EOwwg+ZyPRGFXrbtPNATeJtyJ1sEJnVHMqw7HECSfCYIa09j03NkGKlKGYkaw8TDSJEZ6hCfEtFYgTHaR55gxeWmUMI6nsCANz9e9FirjWcx7aTY7MVK97C/E/z09MdBOkVMSJIQIvH0UJg0bCRQFwTBXBhs0tQVhRmxXiKVIIG1tT4ctEoXhK8VNWVInIcxXEkGdlW6C3Xtcm6bWantv07lv1dmtVZQmcgwtwBTxwDdrgDnRAF2AgwQt4BW/Ou/PhfDnfy9UtZ3VzBgpwfn4BS5qoiQ==</latexit>

<latexit sha1_base64="H9c4lv8scahYYuN6NzGl+NKdbDc=">AAACGHicbVDLTsJAFL31ifhCXbppBBNXpGWjS6Ibl5jII4GGTIdbGJlOm5mpkTT8g1v5GnfGrTs/xsShdCHgSSY5Oefe3DPHjzlT2nG+rY3Nre2d3cJecf/g8Oi4dHLaUlEiKTZpxCPZ8YlCzgQ2NdMcO7FEEvoc2/74bu63n1EqFolHPYnRC8lQsIBRoo3UqvTiEav0S2Wn6mSw14mbkzLkaPRLP71BRJMQhaacKNV1nVh7KZGaUY7TYi9RGBM6JkPsGipIiMpLs7RT+9IoAzuIpHlC25n6dyMloVKT0DeTIdEjterNxf+8bqKDGy9lIk40Cro4FCTc1pE9/7o9YBKp5hNDCJXMZLXpiEhCtSlo6cpQEtMLfZkuqyiyXEXTl7vazjpp1aquU3UfauX6bd5cAc7hAq7AhWuowz00oAkUnuAV3mBmzax368P6XIxuWPnOGSzB+voFM8Gg1A==</latexit>
 <latexit sha1_base64="xWmXra5P90YYt2hGo0QW8XWb5is=">AAACGnicbVDLTsJAFJ3iC/GFunTTCCauSMtGl0Q3LjGRRwINmQ63dMJ02szcGgnhJ9zK17gzbt34MSYOpQsBTzLJyTn35p45fiK4Rsf5tgpb2zu7e8X90sHh0fFJ+fSsreNUMWixWMSq61MNgktoIUcB3UQBjXwBHX98v/A7z6A0j+UTThLwIjqSPOCMopG61T6GgLQ6KFecmpPB3iRuTiokR3NQ/ukPY5ZGIJEJqnXPdRL0plQhZwJmpX6qIaFsTEfQM1TSCLQ3zfLO7CujDO0gVuZJtDP178aURlpPIt9MRhRDve4txP+8XorBrTflMkkRJFseClJhY2wvPm8PuQKGYmIIZYqbrDYLqaIMTUUrV0aKJiFnL7NVFWSWq2T6ctfb2STtes11au5jvdK4y5srkgtySa6JS25IgzyQJmkRRgR5JW9kbs2td+vD+lyOFqx855yswPr6Belyob0=</latexit>

z <latexit sha1_base64="qv/I6fmAzetHscBr4SeDOeeZ+LU=">AAACF3icbVA9TwJBFHznJ+IXammzkZhYkTsarQyJjSUmHpAAIXvLO9iwt3fZ3TPihd9gK7/Gztha+mNMXA4KASfZZDLzXt7sBIng2rjut7OxubW9s1vYK+4fHB4dl05OGzpOFUOfxSJWrYBqFFyib7gR2EoU0igQ2AxGdzO/+YRK81g+mnGC3YgOJA85o8ZKficIyUuvVHYrbg6yTrwFKcMC9V7pp9OPWRqhNExQrduem5huRpXhTOCk2Ek1JpSN6ADblkoaoe5medgJubRKn4Sxsk8akqt/NzIaaT2OAjsZUTPUq95M/M9rpya86WZcJqlByeaHwlQQE5PZz0mfK2RGjC2hTHGblbAhVZQZ28/SlYGiyZCz58myijLPVbR9eavtrJNGteK5Fe+hWq7dLporwDlcwBV4cA01uIc6+MCAwyu8wdSZOu/Oh/M5H91wFjtnsATn6xfKk6Cf</latexit>

x <latexit sha1_base64="yMKvcsbQNkvFrBnVkm2K+W26W+g=">AAACGXicbVC7TgJBFL3rE/GFWtpMBBMrskujJdHGEhN5JLAhs8NdmDA7u5mZNRDCR9jK19gZWys/xsRhoRDwJJOcnHNv7pkTJIJr47rfztb2zu7efu4gf3h0fHJaODtv6DhVDOssFrFqBVSj4BLrhhuBrUQhjQKBzWD4MPebL6g0j+WzGSfoR7QvecgZNVZqljpBSEalbqHolt0MZJN4S1KEJWrdwk+nF7M0QmmYoFq3PTcx/oQqw5nAab6TakwoG9I+ti2VNELtT7K4U3JtlR4JY2WfNCRT/25MaKT1OArsZETNQK97c/E/r52a8M6fcJmkBiVbHApTQUxM5n8nPa6QGTG2hDLFbVbCBlRRZmxDK1f6iiYDzkbTVRVllitv+/LW29kkjUrZc8veU6VYvV82l4NLuIIb8OAWqvAINagDgyG8whvMnJnz7nw4n4vRLWe5cwErcL5+AY50oP0=</latexit>
VAE
<latexit sha1_base64="n8AseNkGoH4W/vGtOIIc+PD6iF8=">AAACFXicbVDLSsNAFL3xWeur6tJNsAiuStKNrqQigsuK9gFtKJPpTTt0MgkzE7GEfoJb+zXuxK1rP0ZwmmZhWw8MHM65l3vm+DFnSjvOt7W2vrG5tV3YKe7u7R8clo6OmypKJMUGjXgk2z5RyJnAhmaaYzuWSEKfY8sf3c781jNKxSLxpMcxeiEZCBYwSrSRHps3d71S2ak4GexV4uakDDnqvdJPtx/RJEShKSdKdVwn1l5KpGaU46TYTRTGhI7IADuGChKi8tIs6sQ+N0rfDiJpntB2pv7dSEmo1Dj0zWRI9FAtezPxP6+T6ODKS5mIE42Czg8FCbd1ZM/+bfeZRKr52BBCJTNZbTokklBt2lm4MpAkHjL6MllUUWS5iqYvd7mdVdKsVlyn4j5Uy7XrvLkCnMIZXIALl1CDe6hDAygM4BXeYGpNrXfrw/qcj65Z+c4JLMD6+gUBSZ+p</latexit>

N <latexit sha1_base64="P8oqS/zsceICK4985TkO8rf6tpY=">AAACFXicbVC7TsMwFL0pr1JeBUYWixaJqUq6wFjBwoSKoA+pjSrHdVKrjhPZDqKK+gms9GvYECszH4OEm2agLUeydHTOvbrHx4s5U9q2v63CxubW9k5xt7S3f3B4VD4+aasokYS2SMQj2fWwopwJ2tJMc9qNJcWhx2nHG9/O/c4zlYpF4klPYuqGOBDMZwRrIz1W76uDcsWu2RnQOnFyUoEczUH5pz+MSBJSoQnHSvUcO9ZuiqVmhNNpqZ8oGmMyxgHtGSpwSJWbZlGn6MIoQ+RH0jyhUab+3UhxqNQk9MxkiPVIrXpz8T+vl2j/2k2ZiBNNBVkc8hOOdITm/0ZDJinRfGIIJpKZrIiMsMREm3aWrgQSxyNGXqbLKhVZrpLpy1ltZ5206zXHrjkP9UrjJm+uCGdwDpfgwBU04A6a0AICAbzCG8ysmfVufVifi9GCle+cwhKsr1+MPZ9n</latexit>

x <latexit sha1_base64="yMKvcsbQNkvFrBnVkm2K+W26W+g=">AAACGXicbVC7TgJBFL3rE/GFWtpMBBMrskujJdHGEhN5JLAhs8NdmDA7u5mZNRDCR9jK19gZWys/xsRhoRDwJJOcnHNv7pkTJIJr47rfztb2zu7efu4gf3h0fHJaODtv6DhVDOssFrFqBVSj4BLrhhuBrUQhjQKBzWD4MPebL6g0j+WzGSfoR7QvecgZNVZqljpBSEalbqHolt0MZJN4S1KEJWrdwk+nF7M0QmmYoFq3PTcx/oQqw5nAab6TakwoG9I+ti2VNELtT7K4U3JtlR4JY2WfNCRT/25MaKT1OArsZETNQK97c/E/r52a8M6fcJmkBiVbHApTQUxM5n8nPa6QGTG2hDLFbVbCBlRRZmxDK1f6iiYDzkbTVRVllitv+/LW29kkjUrZc8veU6VYvV82l4NLuIIb8OAWqvAINagDgyG8whvMnJnz7nw4n4vRLWe5cwErcL5+AY50oP0=</latexit>

Ladder <latexit sha1_base64="rak78yFFnkMmPssnckSD+ST52Qo=">AAACHHicbVDLSsNAFJ3UV62vqks3g0VwVZJudCUVEVy4qGAf2IYymdy0QyeTMDMRS+hfuLVf407cCn6M4DTNwrYeGDiccy/3zPFizpS27W+rsLa+sblV3C7t7O7tH5QPj1oqSiSFJo14JDseUcCZgKZmmkMnlkBCj0PbG93M/PYzSMUi8ajHMbghGQgWMEq0kZ7uie+DxK3r2365YlftDHiVODmpoByNfvmn50c0CUFoyolSXceOtZsSqRnlMCn1EgUxoSMygK6hgoSg3DRLPMFnRvFxEEnzhMaZ+ncjJaFS49AzkyHRQ7XszcT/vG6ig0s3ZSJONAg6PxQkHOsIz76PfSaBaj42hFDJTFZMh0QSqk1JC1cGksRDRl8miyqILFfJ9OUst7NKWrWqY1edh1qlfpU3V0Qn6BSdIwddoDq6Qw3URBQJ9Ire0NSaWu/Wh/U5Hy1Y+c4xWoD19QsoAKJb</latexit>

VAE

N <latexit sha1_base64="P8oqS/zsceICK4985TkO8rf6tpY=">AAACFXicbVC7TsMwFL0pr1JeBUYWixaJqUq6wFjBwoSKoA+pjSrHdVKrjhPZDqKK+gms9GvYECszH4OEm2agLUeydHTOvbrHx4s5U9q2v63CxubW9k5xt7S3f3B4VD4+aasokYS2SMQj2fWwopwJ2tJMc9qNJcWhx2nHG9/O/c4zlYpF4klPYuqGOBDMZwRrIz1W76uDcsWu2RnQOnFyUoEczUH5pz+MSBJSoQnHSvUcO9ZuiqVmhNNpqZ8oGmMyxgHtGSpwSJWbZlGn6MIoQ+RH0jyhUab+3UhxqNQk9MxkiPVIrXpz8T+vl2j/2k2ZiBNNBVkc8hOOdITm/0ZDJinRfGIIJpKZrIiMsMREm3aWrgQSxyNGXqbLKhVZrpLpy1ltZ5206zXHrjkP9UrjJm+uCGdwDpfgwBU04A6a0AICAbzCG8ysmfVufVifi9GCle+cwhKsr1+MPZ9n</latexit>

(a) Traditional Latent Variable Models

x <latexit sha1_base64="yMKvcsbQNkvFrBnVkm2K+W26W+g=">AAACGXicbVC7TgJBFL3rE/GFWtpMBBMrskujJdHGEhN5JLAhs8NdmDA7u5mZNRDCR9jK19gZWys/xsRhoRDwJJOcnHNv7pkTJIJr47rfztb2zu7efu4gf3h0fHJaODtv6DhVDOssFrFqBVSj4BLrhhuBrUQhjQKBzWD4MPebL6g0j+WzGSfoR7QvecgZNVZqljpBSEalbqHolt0MZJN4S1KEJWrdwk+nF7M0QmmYoFq3PTcx/oQqw5nAab6TakwoG9I+ti2VNELtT7K4U3JtlR4JY2WfNCRT/25MaKT1OArsZETNQK97c/E/r52a8M6fcJmkBiVbHApTQUxM5n8nPa6QGTG2hDLFbVbCBlRRZmxDK1f6iiYDzkbTVRVllitv+/LW29kkjUrZc8veU6VYvV82l4NLuIIb8OAWqvAINagDgyG8whvMnJnz7nw4n4vRLWe5cwErcL5+AY50oP0=</latexit>

Graph <latexit sha1_base64="Rj/FAiJJYYuonId+cd7GHSaDnUc=">AAACG3icbVDLSsNAFL2pr1pfVZduBovgqiTd6EoqIrqsYB/ShjKZTtqhk0mYmYgl9Cvc2q9xJ25d+DGCkzQL23pg4HDOvdwzx4s4U9q2v63C2vrG5lZxu7Szu7d/UD48aqkwloQ2SchD2fGwopwJ2tRMc9qJJMWBx2nbG9+kfvuZSsVC8agnEXUDPBTMZwRrIz3dSRyNUOv6tl+u2FU7A1olTk4qkKPRL//0BiGJAyo04ViprmNH2k2w1IxwOi31YkUjTMZ4SLuGChxQ5SZZ4Ck6M8oA+aE0T2iUqX83EhwoNQk8MxlgPVLLXir+53Vj7V+6CRNRrKkg80N+zJEOUfp7NGCSEs0nhmAimcmKyAhLTLTpaOHKMG2GkZfpokpFlqtk+nKW21klrVrVsavOQ61Sv8qbK8IJnMI5OHABdbiHBjSBQACv8AYza2a9Wx/W53y0YOU7x7AA6+sXaC+h9w==</latexit>

VAE

N <latexit sha1_base64="P8oqS/zsceICK4985TkO8rf6tpY=">AAACFXicbVC7TsMwFL0pr1JeBUYWixaJqUq6wFjBwoSKoA+pjSrHdVKrjhPZDqKK+gms9GvYECszH4OEm2agLUeydHTOvbrHx4s5U9q2v63CxubW9k5xt7S3f3B4VD4+aasokYS2SMQj2fWwopwJ2tJMc9qNJcWhx2nHG9/O/c4zlYpF4klPYuqGOBDMZwRrIz1W76uDcsWu2RnQOnFyUoEczUH5pz+MSBJSoQnHSvUcO9ZuiqVmhNNpqZ8oGmMyxgHtGSpwSJWbZlGn6MIoQ+RH0jyhUab+3UhxqNQk9MxkiPVIrXpz8T+vl2j/2k2ZiBNNBVkc8hOOdITm/0ZDJinRfGIIJpKZrIiMsMREm3aWrgQSxyNGXqbLKhVZrpLpy1ltZ5206zXHrjkP9UrjJm+uCGdwDpfgwBU04A6a0AICAbzCG8ysmfVufVifi9GCle+cwhKsr1+MPZ9n</latexit>

(b) Proposed Model

Figure 1: Overview: Model Comparison. We show the graphical representations of (a) traditional
latent variable models (VAE, ladder VAE) and (b) the proposed graph VAE. Solid lines denote
generation, dashed lines denote inference, and the dotted area indicates the latent space governed by variational parameters  and generative parameters . Both VAE and ladder VAE use a fixed graph structure with limited expressiveness (VAE: independent; ladder VAE: chain-structured). In contrast, graph VAE jointly optimizes a distribution over latent structures c and model parameters (,), allowing test-time sampling of a flexible, data-driven latent structure.

With the added latent dependencies in the model, the independence assumption in the approximate posterior is even less valid in this setting. While normalizing flows (Rezende & Mohamed, 2015) offers one technique for overcoming the mean field assumption, a separate line of work has investigated the use of structured approximate posteriors, particularly in the context of models with empirical priors (Johnson et al., 2016). This technique introduces dependencies between the dimensions of the approximate posterior, often mirroring the dependency structure of the latent variable model. An explanation for this was provided by Marino et al. (2018): optimizing the approximate posterior requires knowledge of the prior, which is especially relevant in models with empirical priors where the prior can vary with the data. Ladder VAE (Sønderby et al., 2016) incorporates these prior dependencies by using a structured approximate posterior of the form

M
q(z x) = q(zm x, zpa(m)).
m=1

(4)

Unlike the mean field approximate posterior, which conditions each dimension only on the data example, x, the distributions in Eq. (4) account for latent dependencies by conditioning on samples from the parent variables. Ladder VAE performs this conditioning by reusing the empirical prior during inference, forming the approximate posterior by combining a "bottom-up" recognition distribution and the "top-down" prior.

While Eqs. (3) and (4) permit separate latent dependencies for each individual latent dimension, the
dimensions are typically partitioned into a set of nodes, with dimensions within each node sharing
the same parents. This improves computational efficiency by allowing priors and approximate posteriors within each node to be calculated in parallel. Using zn to denote latent node n of N and zpa(n) to denote the concatenation of its parent nodes, we can write the ELBO (Eq. (2)) as

N
L(x; , ) = Eq(z x) [log p(x z)] - Eq(z x)
n=1

log

q(zn x, zpa(n)) p(zn zpa(n))

.

(5)

Note that the KL divergence term in the ELBO can no longer be evaluated analytically, now requiring a sampling-based estimate of the expectation (Kingma & Welling, 2014). While this can lead to higher variance in ELBO estimates and the resulting gradients, models with latent dependencies still tend to empirically outperform models with independence assumptions (Burda et al., 2016; Sønderby et al., 2016). However, by increasing the number of nodes, the burden of devising a suitable dependency structure falls upon the experimental practitioner. This is non-trivial, as the structure may depend on the data and other model hyperparameters, such as the number of layers

3

Under review as a conference paper at ICLR 2019

in the deep networks, non-linearities, latent distributions, etc. Rather than relying on pre-defined fully-connected structures (Kingma et al., 2016) or chain structures (Sønderby et al., 2016), we seek to automatically learn the latent dependency structure as part of the variational optimization process. A comparison of these approaches is visualized in Fig. 1.

3 VARIATIONAL OPTIMIZATION OF LATENT STRUCTURES
A major difficulty in the context of variational optimization of latent structures is the fact that, unlike the model parameters (, ), which are optimized over a continuous domain, the latent dependency structure is discrete, without a clear ordering. While the discrete nature of the latent space's topological structure introduces discontinuities in the optimization landscape, complicating the learning process, there is (unlike the related setting of neural architecture search (Zoph & Le, 2016)) fortunately only a finite number of possible dependency structures over a fixed number of latent dimensions: In a directed graphical model, a fully-connected directed acyclic graph (DAG) models all possible dependencies. In this model, an ordering is induced over the latent nodes, and the parents of node n (of N ) are given as zpa(n) = {zn+1, . . . , zN }.1 Thus, to learn a latent dependency structure, we can maintain all dependencies in a fully-connected DAG, modifying their presence or absence during training. Naively, one might assume that a fully-connected DAG is the most general structure, and, therefore, current models should implicitly learn to ignore unnecessary dependencies. However, latent variable models are highly prone to local optima (Bowman et al., 2016; Burda et al., 2016), and as we show empirically in Section 4, modifying entire dependencies can yield improved performance.

3.1 GATED DEPENDENCIES

To control the dependency structure of the model, we introduce a set of binary global variables, c = {ci,j}i,j, which gate the latent dependencies. The element ci,j denotes the gate variable from zi to zj (i > j), specifying the presence or absence of this latent dependency. We treat each ci,j as an independent random variable, sampled from a Bernoulli distribution with mean µi,j, i.e. ci,j  B(µi,j). We denote the set of these Bernoulli means as µ. Because each element of c takes values in {0, 1}, dependencies can be preserved or removed simply through (broadcasted) element-wise
multiplication with the corresponding parent nodes. Removing a dependency entails multiplying the corresponding input parent node by 0.
With this addition, the model is now expressed as p(x, z, c) = p(x z, c)p(z c)p(c). Whereas the model parameters, , are point estimates, the dependency gates are governed by Bernoulli distributions, with each sampling from p(c) inducing a corresponding dependency structure. Similar to
Eq. (3), the prior can now be expressed as

N
p(z c) = p(zn zpa(n), cpa(n),n),
n=1

(6)

where cpa(n),n denotes the gate variables associated with the dependencies between node zn and its parents, zpa(n). Note that zpa(n) denotes the set of all possible parents of node zn in the fullyconnected DAG, i.e. zpa(n) = {zn+1, . . . , zN }. To give a concrete example of the gating procedure, consider the case in which p(zn zpa(n), cpa(n),n) is given by a Gaussian density with parameters
n = (µn,  n). We obtain these parameters recursively by multiplying samples of node zn's parent variables zpa(n) with samples of their corresponding gating variables cpa(n),n and inject a concatentation of the results of this operation into a multi-layer perceptron MLP(nT D) predicting n (see Appendix A.2 for additional details on the MLP architecture). The top-down recursion ends at the root node zN  N (0, I). An illustration of this process is shown in black in Fig. 2.
The approximate posterior, q(z x), must approximate p(z x), which is marginalized across c. In effect, q(z x) should account for the entire distribution of dependency structures, rather than any

1We follow convention (Dayan et al., 1995; Rezende et al., 2014; Sønderby et al., 2016), with parent nodes having a larger index than their children.

4

Under review as a conference paper at ICLR 2019

x
<latexit sha1_base64="g/3TbK8gLQESTZ8HxvDAlAa1LuyxaUQ27DEl/HXqtGgOgQaTCrGU6196H+K405x3GsYTh+0CQLtdcs4o=">AAACKHicbVDNLTgIxGFGOL3z9ijHERR166QEiFUPdoXOEcpmvpmjJkZCcTia4eEEIyC9jN4nsXlodPZGhcJMk4jic8RYeeMiPJSECmLwyhSdAIwAIgJdhB3vSOhL6UVY1Bao2Dau2L25mub07tBHrmQsOLgZmTZ/3OIC0qGfru+N3VrD2Tf58egDAfvN4c8fDGXtR4LeLIGsVrV5g/c9EXisneKnSwATJEQcPZEJOmIb7mTk8k53n0PvnuizyzTDen0dT9QIxcGnyZFUWZaNcGcq4e/Z/74Wd4PbaWD2od3du3bH3Jb93S9gP+nTMK2hHr+fncVXRO/UcjwGOjv5ckFt9JVsH8XS5TWaKKU0kFI9oGikEPCJFhP6VKOJDWAxGCvQPKV7mACrVaKSCeEmnYUEYUbnEq8TTGQT2aUYw4jonFbVcggGWSKn/YxWhADNF+wG2ugPsHxb4G7Yr9ez1GROp/3XF+Us4hykUVXsZN4BpLFy8PpeML/hpBoOTUYnsdp6gAUOhReobSCBKzHYFykjSgoBY2vIJQV/6LEwcU4bQ1hnopPtnTGNvLyVcWHaqfYBdmr457hIDxakboxkC3FQp2hR0L6xUwvr271XY7GfzNR0+d0/i+AzkOEggxCm9qDvV32eKIFVYKen6oX65MHEIrJEx5kMwZGlIPsIjRTlKxMSr3cqfJupsbV6c2RsfH4RZmTmJrMDSAXZwFjzjPnGKVRb52DLSTq1yEUJkpWRjVnPMtikMpInhDq2jZRjDrbtUqEo1kaCZcFKo1Dd/B1NS009n2jkbmZDwclyi1vbXgDpvGoREozbqBSF9P1kKmFshRD5r2vEur7mdfBjSzRBUFxQXvLraeSs2vocxDiPMs8J2M+mRYCmHqP6DFlBeNTb92y5bmHM+ifE5301U4MT7lH2MdeRw7FK6NKEN2R4V5UyMxohTNognyZAVfZyR0xHNOEhomKwTqDFCg8Oy4aEOZQLkpr9miHcftEWmAslZwioMZspyTNwwDiSUsTFWYUgKGkppaMVzX4IpjghGrLSHECM0GhYNtWjT0CWNV1mq5VloMUlsA9T4VxiEHOjgJOIJyRzVtqlW4Z2SBV7kKuenZed5adEuVw6sOTRAXebZ2rHalZAbOydo7L3dO12uem1bdy5KZ1pq21UVCW1UyuneFX5WjI/E6M5be1vcXUAdYKdctLCcgFA8vEE6k4FBA+lwA0SuXg4AQoMcX8IY3uIOYANGavePAIEMARPNd4+fCAEmDMA/K4HDBgdXFecb4NU+b3D5d3cY+rLFS6+czrbB2ZUkeq1A6XPG74N/41WyCrlWNj6YXtOn9O<s/VAilLDan95tfxeMexLEiPztq>P2fp6<bg/=la=<t/elxait>exit>

zpa(n)
<latexit sha1_base64="sJClZMD4xijwVmMOFhy/Y1fbQQI=">AAACP3icbZDPSgMxEMaz/q31X9WbXoKtUC9ltxc9Cl48VrAqtKVk09k2mGSXZFasy4JP41VfwcfwCbyJV2+mtQfb+kHg4zczzOQLEyks+v67t7C4tLyyWlgrrm9sbm2XdnavbZwaDk0ey9jchsyCFBqaKFDCbWKAqVDCTXh3Pqrf3IOxItZXOEygo1hfi0hwhg51S/uVrB1G9DHvZm2EBzQqS1he1cd5pVsq+zV/LDpvgokpk4ka3R1vs92LeapAI5fM2lbgJ9jJmEHBJeTFdmohYfyO9aHlrGYKbCcbfyKnR470aBQb9zTSMf07kTFl7VCFrlMxHNjZ2gj+V2ulGJ12MqGTFEHz30VRKinGdJQI7QkDHOXQGcaNcLdSPmCGcXS5TW3pG5YMBH/Ipyno8V1TMFR50QUYzMY1b67rtcCvBZf18tnpJMoCOSCHpEoCckLOyAVpkCbh5Ik8kxfy6r15H96n9/XbuuBNZvbIlLzvHwaZr2g=</latexit>

zn+1
<latexit sha1_base64="rHCOl6RFYoxdcBQQIKMJMWSVD5s=">AAACNHicbVDLSsNAFJ34rNXaVpduBltBEErSjS4LblxWsA9oQ5hMJ+3QmUmYmYg15Evc6i/4L4I7ces3OE2zMK0HLhzOuZd7OH7EqNK2/WFtbe/s7u2XDsqHR5Xjaq1+0ldhLDHp4ZCFcugjRRgVpKepZmQYSYK4z8jAn98u/cEjkYqG4kEvIuJyNBU0oBhpI3m1ajMZ+wF8Tr1EXDlp06s17JadAW4SJycNkKPr1a3KeBLimBOhMUNKjRw70m6CpKaYkbQ8jhWJEJ6jKRkZKhAnyk2y5Cm8MMoEBqE0IzTM1L8XCeJKLbhvNjnSM7XuLcX/vFGsgxs3oSKKNRF49SiIGdQhXNYAJ1QSrNnCEIQlNVkhniGJsDZlFb5MJYpmFD+lRZWILFdB9HlaNgU663Vtkn675dgt577d6LTzKkvgDJyDS+CAa9ABd6ALegCDGLyAV/BmvVuf1pf1vVrdsvKbU1CA9fMLxJGqNg==</latexit>

Encoder <latexit sha1_base64="FabAt1BdB4lmBXTXHYJUGkGfcMM=">AAACKnicbVDLSsNAFJ34rNVqq0s3wSK4Kkk3uiyI4LKCfUAbymRy0w6dmYSZiVhCPsKt/oJf46649UOcplmY1gMDh3Pu5Z45fsyo0o6ztHZ29/YPDitH1eOT2ulZvXHeV1EiCfRIxCI59LECRgX0NNUMhrEEzH0GA39+v/IHLyAVjcSzXsTgcTwVNKQEayMNHgSJApCTetNpOTnsbeIWpIkKdCcNqzYOIpJwEJowrNTIdWLtpVhqShhk1XGiIMZkjqcwMlRgDspL87yZfW2UwA4jaZ7Qdq7+3UgxV2rBfTPJsZ6pTW8l/ueNEh3eeSkVcaJBkPWhMGG2juzV5+2ASiCaLQzBRFKT1SYzLDHRpqLSlanE8YyS16ysgshzlUSfZ1VToLtZ1zbpt1uu03Kf2s1Ou6iygi7RFbpBLrpFHfSIuqiHCJqjN/SOPqxP68taWt/r0R2r2LlAJVg/v9hjp1Y=</latexit>

1 <latexit sha1_base64="sOKZWwYLP0D8eYA/vKWe/syd2xo=">AAACJnicbVC9TsMwGLT5LYVCCyOLRYvEVCVdYKzEwlgE/ZHaqHJcJ7VqO5HtIKooj8AKr8DTsCHExqPgph1Iy0mWTnffp+98fsyZNo7zDbe2d3b39ksH5cOjyvFJtXba01GiCO2SiEdq4GNNOZO0a5jhdBArioXPad+f3S78/hNVmkXy0cxj6gkcShYwgo2VHhpuY1ytO00nB9ok7orUwQqdcQ1WRpOIJIJKQzjWeug6sfFSrAwjnGblUaJpjMkMh3RoqcSCai/Ns2bo0ioTFETKPmlQrv7dSLHQei58Oymwmep1byH+5w0TE9x4KZNxYqgky0NBwpGJ0OLjaMIUJYbPLcFEMZsVkSlWmBhbT+FKqHA8ZeQ5K6pU5rkKoi+ysi3QXa9rk/RaTddpuveteru1qrIEzsEFuAIuuAZtcAc6oAsICMELeAVv8B1+wE/4tRzdgqudM1AA/PkFP/qk5w==</latexit>

n
<latexit sha1_base64="XaolyvL8VAtgkC0BkS4EduTFz6s=">AAACJnicbVC9TsMwGLT5LYVCCyOLRYvEVCVdYKzEwlgE/ZHaqHJcJ7VqO5HtIKooj8AKr8DTsCHExqPgph1Iy0mWTnffp+98fsyZNo7zDbe2d3b39ksH5cOjyvFJtXba01GiCO2SiEdq4GNNOZO0a5jhdBArioXPad+f3S78/hNVmkXy0cxj6gkcShYwgo2VHhqyMa7WnaaTA20Sd0XqYIXOuAYro0lEEkGlIRxrPXSd2HgpVoYRTrPyKNE0xmSGQzq0VGJBtZfmWTN0aZUJCiJlnzQoV/9upFhoPRe+nRTYTPW6txD/84aJCW68lMk4MVSS5aEg4chEaPFxNGGKEsPnlmCimM2KyBQrTIytp3AlVDieMvKcFVUq81wF0RdZ2Rborte1SXqtpus03ftWvd1aVVkC5+ACXAEXXIM2uAMd0AUEhOAFvII3+A4/4Cf8Wo5uwdXOGSgA/vwCqvelJA==</latexit>

N <latexit sha1_base64="tOLXbGCbkZmkCpa4aX/cGWrFPpw=">AAACJnicbVC9TsMwGLTLXykUWhhZLFokpirpAmMlFiZUBP2R2qhyXCe16jiR7SCqKI/ACq/A07AhxMaj4KYZSMtJlk5336fvfG7EmdKW9Q1LW9s7u3vl/crBYfXouFY/6aswloT2SMhDOXSxopwJ2tNMczqMJMWBy+nAnd8s/cETlYqF4lEvIuoE2BfMYwRrIz0075qTWsNqWRnQJrFz0gA5upM6rI6nIYkDKjThWKmRbUXaSbDUjHCaVsaxohEmc+zTkaECB1Q5SZY1RRdGmSIvlOYJjTL170aCA6UWgWsmA6xnat1biv95o1h7107CRBRrKsjqkBdzpEO0/DiaMkmJ5gtDMJHMZEVkhiUm2tRTuOJLHM0YeU6LKhVZroLoBmnFFGiv17VJ+u2WbbXs+3aj086rLIMzcA4ugQ2uQAfcgi7oAQJ88AJewRt8hx/wE36tRksw3zkFBcCfX3LXpQQ=</latexit>

<latexitsha1_base64="ZMThSV6AJuqyqDZRp9cO5+n+/oc=">AAACKXicbVC9TsMwGLT5LYVCCyOLRYvEVCVdYKzEwlgk+iO1UeW4TmtqO5HtIKoo78AKr8DTsAErL4KbZiAtJ1k63X2fvvP5EWfaOM4X3Nre2d3bLx2UD48qxyfV2mlPh7EitEtCHqqBjzXlTNKuYYbTQaQoFj6nfX9+u/T7T1RpFsoHs4ioJ/BUsoARbKzUa4w0E41xte40nQxok7g5qYMcnXENVkaTkMSCSkM41nroOpHxEqwMI5ym5VGsaYTJHE/p0FKJBdVeksVN0aVVJigIlX3SoEz9u5FgofVC+HZSYDPT695S/M8bxia48RImo9hQSVaHgpgjE6Ll39GEKUoMX1iCiWI2KyIzrDAxtqHClanC0YyR57SoUpnlKoi+SMu2QHe9rk3SazVdp+net+rtVl5lCZyDC3AFXHAN2uAOdEAXEPAIXsAreIPv8AN+wu/V6BbMd85AAfDnFz7cpnk=</latexit> cn+1,n
<latexit sha1_base64="qheHfKTctYBtCM1JxodXMpawjCU=">AAACNnicbVDLSsNAFJ34rNVqq0s3g60gKCXpRpcFNy4r2Ae0IUymk3bozCTMTNQS8ilu9Rf8FTfuxK2f4DTNwrQeuHA4517u4fgRo0rb9oe1sbm1vbNb2ivvH1QOj6q1454KY4lJF4cslAMfKcKoIF1NNSODSBLEfUb6/ux24fcfiVQ0FA96HhGXo4mgAcVIG8mr1hrJyA8gTr1EXDpXIm141brdtDPAdeLkpA5ydLyaVRmNQxxzIjRmSKmhY0faTZDUFDOSlkexIhHCMzQhQ0MF4kS5SZY9hedGGcMglGaEhpn69yJBXKk5980mR3qqVr2F+J83jHVw4yZURLEmAi8fBTGDOoSLIuCYSoI1mxuCsKQmK8RTJBHWpq7Cl4lE0ZTi57SoEpHlKog+T8umQGe1rnXSazUdu+nct+rtVl5lCZyCM3ABHHAN2uAOdEAXYPAEXsAreLPerU/ry/perm5Y+c0JKMD6+QX+DqrN</latexit>

. <latexit sha1_base64="4ARCFSnpLWM82MNV3ecWYZMIQ8Q=">AAACK3icbVDLSgMxFE181mq11aWbwVZwVWa60ZUU3LisYB/QDiWTybSheQxJRixlfsKt/oJf40px63+YTmfhtB4IHM65l3tygphRbVz3E25t7+zu7ZcOyodHleOTau20p2WiMOliyaQaBEgTRgXpGmoYGcSKIB4w0g9md0u//0SUplI8mnlMfI4mgkYUI2OlQWPEQml0Y1ytu003g7NJvJzUQY7OuAYro1DihBNhMENaDz03Nv4CKUMxI2l5lGgSIzxDEzK0VCBOtL/IAqfOpVVCJ5LKPmGcTP27sUBc6zkP7CRHZqrXvaX4nzdMTHTjL6iIE0MEXh2KEuYY6Sx/74RUEWzY3BKEFbVZHTxFCmFjOypcmSgUTyl+TosqEVmughjwtGwL9Nbr2iS9VtNzm95Dq96+zassgXNwAa6AB65BG9yDDugCDBh4Aa/gDb7DD/gFv1ejWzDfOQMFwJ9fFGindg==</latexit>

..

zN
<latexit sha1_base64="xvdISp2SGWA7IK+MXYZNMO/JTwY=">AAACNHicbVDLSsNAFJ3UV63Wtrp0M9gKbixJN7osuHElFewD2hAm00k7dGYSZiZiDfkSt/oL/ovgTtz6DU7TLkzrgQuHc+7lHo4fMaq0bX9Yha3tnd294n7p4LB8VKnWjnsqjCUmXRyyUA58pAijgnQ11YwMIkkQ9xnp+7Obhd9/JFLRUDzoeURcjiaCBhQjbSSvWmkkIz+Az6mX3F06acOr1u2mnQFuEmdF6mCFjlezyqNxiGNOhMYMKTV07Ei7CZKaYkbS0ihWJEJ4hiZkaKhAnCg3yZKn8NwoYxiE0ozQMFP/XiSIKzXnvtnkSE/VurcQ//OGsQ6u3YSKKNZE4OWjIGZQh3BRAxxTSbBmc0MQltRkhXiKJMLalJX7MpEomlL8lOZVIrJcOdHnackU6KzXtUl6raZjN537Vr3dWlVZBKfgDFwAB1yBNrgFHdAFGMTgBbyCN+vd+rS+rO/lasFa3ZyAHKyfX4+Xqhg=</latexit>

1

zN
<latexit sha1_base64="zgYPV75jS6Lo2oIeht0WD944b4c=">AAACMHicbVDLTgIxFG19IoqCLt00gokrMsNGlyRuXBlM5JHAhHRKBxrazth2jDiZ73Crv+DX6Mq49SsswywEPMlNTs65N/fk+BFn2jjOJ9zY3Nre2S3sFfcPSodH5cpxR4exIrRNQh6qno815UzStmGG016kKBY+p11/ej33u49UaRbKezOLqCfwWLKAEWys5NWSgR+g53SY3Ka1Ybnq1J0MaJ24OamCHK1hBZYGo5DEgkpDONa67zqR8RKsDCOcpsVBrGmEyRSPad9SiQXVXpKlTtG5VUYoCJUdaVCm/r1IsNB6Jny7KbCZ6FVvLv7n9WMTXHkJk1FsqCSLR0HMkQnRvAI0YooSw2eWYKKYzYrIBCtMjC1q6ctY4WjCyFO6rFKZ5VoSfZEWbYHual3rpNOou07dvWtUm428ygI4BWfgArjgEjTBDWiBNiDgAbyAV/AG3+EH/ILfi9UNmN+cgCXAn18YsKl1</latexit>

cN <latexit sha1_base64="bDZlai9xeNyQNdV70P7y2VFq1FM=">AAACNnicbVC7TsMwFHXKqxQKLYwsFi0SA1RJFxgrsTChItGH1EaR4zqtVduJbAeoonwKK/wCv8LChlj5BNw0A2050pWOzrlX9+j4EaNK2/aHVdjY3NreKe6W9vbLB4eV6lFXhbHEpINDFsq+jxRhVJCOppqRfiQJ4j4jPX96M/d7j0QqGooHPYuIy9FY0IBipI3kVar1ZOgHEKdecnfpXIi07lVqdsPOANeJk5MayNH2qlZ5OApxzInQmCGlBo4daTdBUlPMSFoaxopECE/RmAwMFYgT5SZZ9hSeGWUEg1CaERpm6t+LBHGlZtw3mxzpiVr15uJ/3iDWwbWbUBHFmgi8eBTEDOoQzouAIyoJ1mxmCMKSmqwQT5BEWJu6lr6MJYomFD+nyyoRWa4l0edpyRTorNa1TrrNhmM3nPtmrdXMqyyCE3AKzoEDrkAL3II26AAMnsALeAVv1rv1aX1Z34vVgpXfHIMlWD+/yNiqrw==</latexit>

<latexitsha1_base64="ZMThSV6AJuqyqDZRp9cO5+n+/oc=">AAACKXicbVC9TsMwGLT5LYVCCyOLRYvEVCVdYKzEwlgk+iO1UeW4TmtqO5HtIKoo78AKr8DTsAErL4KbZiAtJ1k63X2fvvP5EWfaOM4X3Nre2d3bLx2UD48qxyfV2mlPh7EitEtCHqqBjzXlTNKuYYbTQaQoFj6nfX9+u/T7T1RpFsoHs4ioJ/BUsoARbKzUa4w0E41xte40nQxok7g5qYMcnXENVkaTkMSCSkM41nroOpHxEqwMI5ym5VGsaYTJHE/p0FKJBdVeksVN0aVVJigIlX3SoEz9u5FgofVC+HZSYDPT695S/M8bxia48RImo9hQSVaHgpgjE6Ll39GEKUoMX1iCiWI2KyIzrDAxtqHClanC0YyR57SoUpnlKoi+SMu2QHe9rk3SazVdp+net+rtVl5lCZyDC3AFXHAN2uAOdEAXEPAIXsAreIPv8AN+wu/V6BbMd85AAfDnFz7cpnk=</latexit>

<latexitsha1_base64="ZMThSV6AJuqyqDZRp9cO5+n+/oc=">AAACKXicbVC9TsMwGLT5LYVCCyOLRYvEVCVdYKzEwlgk+iO1UeW4TmtqO5HtIKoo78AKr8DTsAErL4KbZiAtJ1k63X2fvvP5EWfaOM4X3Nre2d3bLx2UD48qxyfV2mlPh7EitEtCHqqBjzXlTNKuYYbTQaQoFj6nfX9+u/T7T1RpFsoHs4ioJ/BUsoARbKzUa4w0E41xte40nQxok7g5qYMcnXENVkaTkMSCSkM41nroOpHxEqwMI5ym5VGsaYTJHE/p0FKJBdVeksVN0aVVJigIlX3SoEz9u5FgofVC+HZSYDPT695S/M8bxia48RImo9hQSVaHgpgjE6Ll39GEKUoMX1iCiWI2KyIzrDAxtqHClanC0YyR57SoUpnlKoi+SMu2QHe9rk3SazVdp+net+rtVl5lCZyDC3AFXHAN2uAOdEAXEPAIXsAreIPv8AN+wu/V6BbMd85AAfDnFz7cpnk=</latexit>

1,n

cN,n <latexit sha1_base64="HAXZPqdENJeu1znHRsy3NStWHE0=">AAACNHicbVDLSsNAFJ34rNXaVpduBlvBhZSkG10W3LiSCvYBbQiT6aQdOjMJMxOxhHyJW/0F/0VwJ279BqdpFqb1wIXDOfdyD8ePGFXatj+sre2d3b390kH58KhyXK3VT/oqjCUmPRyyUA59pAijgvQ01YwMI0kQ9xkZ+PPbpT94IlLRUDzqRURcjqaCBhQjbSSvVm0mYz+AOPWS+yuRNr1aw27ZGeAmcXLSADm6Xt2qjCchjjkRGjOk1MixI+0mSGqKGUnL41iRCOE5mpKRoQJxotwkS57CC6NMYBBKM0LDTP17kSCu1IL7ZpMjPVPr3lL8zxvFOrhxEyqiWBOBV4+CmEEdwmUNcEIlwZotDEFYUpMV4hmSCGtTVuHLVKJoRvFzWlSJyHIVRJ+nZVOgs17XJum3W47dch7ajU47r7IEzsA5uAQOuAYdcAe6oAcwiMELeAVv1rv1aX1Z36vVLSu/OQUFWD+/0BaqPQ==</latexit>

MLPn(BU ) <latexit sha1_base64="U/UizcLQvVoX2qSMQh+/wo9LbpQ=">AAACPnicbZDPSgMxEMaz/q3Vaqsn8bLYCnopu73oUfTiQaGCrUJbSzadtqFJdklmxbIsPo1XfQVfwxfwJl49mv452OoHgY/fzDCTL4gEN+h5787C4tLyympmLbu+kdvcyhe26yaMNYMaC0Wo7wJqQHAFNeQo4C7SQGUg4DYYnI/qtw+gDQ/VDQ4jaEnaU7zLGUWL2vndUhPhEbVMri6r6X1yeFY7StuJSkvtfNEre2O5f40/NUUyVbVdcHLNTshiCQqZoMY0fC/CVkI1ciYgzTZjAxFlA9qDhrWKSjCtZPyH1D2wpON2Q22fQndMf08kVBozlIHtlBT7Zr42gv/VGjF2T1oJV1GMoNhkUTcWLobuKBC3wzUwFENrKNPc3uqyPtWUoY1tZktP06jP2WM6S0GN75qBgUyzNkB/Pq6/pl4p+17Zv64UTyvTKDNkj+yTQ+KTY3JKLkiV1AgjT+SZvJBX5835cD6dr0nrgjOd2SEzcr5/AJysrqc=</latexit>

en
<latexit sha1_base64="pDAO2t7PRs23+QuDcG76u1kW86pOdYEpABRCyZ+692Pcnhc+p8607iZ1xLNSN9mJhQtOgKdL1jaXB3bX7IxbKVkg/B+p2TEQJaqgnWs8Qk=">AAACaFY3HXicbZDCL9NSTgsNMAwxFIaZWVPnv8x+VC1bbu29hatrrthXb1ZUam73n6diLRmaoLGiJswtQogmAoKYgKpqUSEpxZBBUtIvY1zdukoRChFIsAhVYIABEgiXc0LVRFhan+Xpt0RVFDr2zqShFrQKHTlumTSiUCWxZmazO3t276okeEDY5mzkMkmOJz4SMAHgxzJqMUyq1UGgBLvWuyIYwGN8R/gjkJ6op08N7OD3IRk9RmFtfi+Xw4g8XuBmd129wgcm10enwliZn56hYkMW15634hU8GqBYCWP+jwPr93n+f5OfjvOc56ZxdEw764z5+fvMQxOcFy8VxMoqG0r8o9/T7j/z7cfULObfDaBbgLs2WtiDVf0wtWv6fWLNPNjzKamz8a3sX8V3nid07mtFsnrn76ddpxb2Zud/Zy2/9WdNou8F+bvgtL596Beb0RnKHW0kSGWwqJKYlJKgNoSZ8AapmJzPEFGYSHPth1DkHL7kPBG8hjKANYkKAGnTaEBGLMXmPQCsm1GMp2lMQpYNq4zx6BPmpiCM1ocUIkKSAiIMQAuYQ8g5+4YPgdC7oBIYXcO/P9nvmK6p23mv/H+Ww92g+9ywIk8JxqgKzS0RFsQDU163gFb8+sZm65kY5okGECMTQ0LAOUW4NG5a46SHKkxgOngYiw4UAYpSYEwJqcd1wFaoi3Ng62dvhm3vzfxnv72TKz8H7H1b5hx8XcloGEbC1oJRCcrqmyGELIEGI8F9DyGIGECv8Uz4F7AUk6SxXJbsaKZTZmOnlMrab2cLXxqJ8epT3QYFMTCYEK5JuAoFa3mhxaloReZVjwOdvYtLDRw8g4HI1KYhkd4OmX4OkNEMThvbtyN8P0l8jG5O9qprxeVflrYruQTDt6OhO/NVMqZJjYye5V0Or4T9o2OGag7ts3xy95UwiqDZ0aVKmGCxU5k3i/GAWrvn16UtShkbL25Is3A8iUMLiWRmQ3SKhTpJYgUCzlIETpkJaRocGhnwcWrqayFNhuT385X06pvsdJScRLS6dk+0SfMWq7GZDpIEUKTMlGoDxGfBzRVIzSFCULh1pP6BaEsQqmTIYAM/GJjaKkEtujRtk7I9yf04AQ3V2Nr/VVZmMZVA8gEObDoZaqGpSq5DaXO+lZomPBw+pR9z/MOZn7pdXrUpNElDCBlAkeSndxsheuJQLSl84voT4Ox3SE72StZJujCv/PFT6Wvd9Rny0I/LVqYBG5ihXpUpKhNkAYMaJDe9Emvdr0E0qS6nCbOxU9gM3VpqgVikteslV1nmosqt/2bwTGO+i8vSFL1+oLts/UG02m+GZcvrPoqOLBTZWQS2U+taphFjKIMnpGokHokgN1QNGCS5XD4jHfJosbeqRN1MRIEGVinmGZ52mrqajs2UuNJBxQ7p3m2FepKnJ47ZdIbtUMLJAY9RlDXhENuU8sJaI41skEfZxg10mbhIzECqhmpqqBm5b3d1r3wSXZVmJpejLCjfEyDKtjhHMBRCtJjBMeVul1zP4CcYlaNckpQHptRQXwE48Ckam/ZfhTiEwSHy6C1VFyxus36+oPASQTEcVYzFUGkIuO93ydypusqD1GACdn+Zo6c0XchAUTFL+Eton6kLaC9sB7/1YzH1tNq73e5vLqp5Oa0npzvdDAWidqdWvrVp+lM+Nbn71V5eHv/O29/guXeVgoWe+dCTmaH0tHzewKk<TAZRCX/fSl4gadEwotT1lewVANxDi0cugtwV>exgx30UCjoteAdr4AoUt3AuTkuVENQDtERvuxNHAC07bj8ejOk7PeQsfF6O/LKrvjOw/bqVp3hWlsCp6Q8+PNgv46tmtbJ9UVKzaqu/zfwGulcMjqGKYLSC3eIGr<O/vg/8PlFaLtz3He7x+6J+ii<tw>/=l=a<t/exlaite>xit>

= (µen, e n)

MLPn(T <latexit sha1_base64="6gZYS0e+iWJJa4JZnk7QuzMEPZQ=">AAACPnicbZDPTgIxEMa7+B9FUU/Gy0YwwQvZ5aJHEj140AQTURNA0i0DNLTdTTtrIJuNT+NVX8HX8AW8Ga8eLchBxC9p8uU3M5npF0SCG/S8NyezsLi0vLK6ll3fyG1u5bd3bkwYawZ1FopQ3wXUgOAK6shRwF2kgcpAwG0wOB3Xbx9AGx6qaxxF0JK0p3iXM4oWtfN7xSbCELVMLi9q6X1Suj47StuJSovtfMErexO588afmgKZqtbednLNTshiCQqZoMY0fC/CVkI1ciYgzTZjAxFlA9qDhrWKSjCtZPKH1D20pON2Q22fQndCf08kVBozkoHtlBT75m9tDP+rNWLsnrQSrqIYQbGfRd1YuBi640DcDtfAUIysoUxze6vL+lRThja2mS09TaM+Z8N0loKa3DUDA5lmbYD+37jmzU2l7Htl/6pSqFamUa6SfXJASsQnx6RKzkmN1Akjj+SJPJMX59V5dz6cz5/WjDOd2SUzcr6+AZ6Frqg=</latexit>

D)

<latexit sha1_base64="rmMY0VPQvCYxTerVVNICu3+w3B0=">AAACa3icbZDdSsNAEIW38b/+Vb1TL4JVUJCS9EZvBEEQLxWtCk0pk+20Xbq7CbsbtYS8kU/jnegr+A5uYhGjHlg4fDPDzJ4w5kwbz3utOFPTM7Nz8wvVxaXlldXa2vqtjhJFsUUjHqn7EDRyJrFlmOF4HysEEXK8C0dnef3uAZVmkbwx4xg7AgaS9RkFY1G3dr6bBo+sh0MwaRCKINYsy7ryZP+bFlgkOT0sw2s2EJDzg91ure41vELuX+NPTJ1MdNldqywHvYgmAqWhHLRu+15sOikowyjHrBokGmOgIxhg21oJAnUnLT6cuXuW9Nx+pOyTxi3oz4kUhNZjEdpOAWaof9dy+F+tnZj+cSdlMk4MSvq1qJ9w10Runp7bYwqp4WNrgCpmb3XpEBRQYzMubRkoiIeMPmVlirK4qwRDkVVtgP7vuP6a22bD9xr+VbN+2pxEOU+2yA7ZJz45IqfkglySFqHkmbyQN/Je+XA2nE1n+6vVqUxmNkhJzt4nkXu+9A==</latexit>
F <latexit sha1_base64="c3quR4YZHm3h9y6tXEoXw0DuW1g=">AAACJHicbVDLSsNAFJ3UV61WW126CRbBVUm60WVBEJct2Ae0oUymN+3QmUmYmYgl5Avc6i/4Ne7EhRu/xWmahWk9MHA4517umeNHjCrtON9WaWd3b/+gfFg5Oq6enNbqZ30VxpJAj4QslEMfK2BUQE9TzWAYScDcZzDwF3crf/AEUtFQPOplBB7HM0EDSrA2Uvd+Ums4TSeDvU3cnDRQjs6kblXH05DEHIQmDCs1cp1IewmWmhIGaWUcK4gwWeAZjAwVmIPykixpal8ZZWoHoTRPaDtT/24kmCu15L6Z5FjP1aa3Ev/zRrEObr2EiijWIMj6UBAzW4f26tv2lEogmi0NwURSk9Umcywx0aacwpWZxNGckue0qILIchVEn6cVU6C7Wdc26beartN0u61Gu5VXWUYX6BJdIxfdoDZ6QB3UQwQBekGv6M16tz6sT+trPVqy8p1zVID18wub8KSg</latexit>

bn

= (µbn, b n)

F <latexit sha1_base64="c3quR4YZHm3h9y6tXEoXw0DuW1g=">AAACJHicbVDLSsNAFJ3UV61WW126CRbBVUm60WVBEJct2Ae0oUymN+3QmUmYmYgl5Avc6i/4Ne7EhRu/xWmahWk9MHA4517umeNHjCrtON9WaWd3b/+gfFg5Oq6enNbqZ30VxpJAj4QslEMfK2BUQE9TzWAYScDcZzDwF3crf/AEUtFQPOplBB7HM0EDSrA2Uvd+Ums4TSeDvU3cnDRQjs6kblXH05DEHIQmDCs1cp1IewmWmhIGaWUcK4gwWeAZjAwVmIPykixpal8ZZWoHoTRPaDtT/24kmCu15L6Z5FjP1aa3Ev/zRrEObr2EiijWIMj6UBAzW4f26tv2lEogmi0NwURSk9Umcywx0aacwpWZxNGckue0qILIchVEn6cVU6C7Wdc26beartN0u61Gu5VXWUYX6BJdIxfdoDZ6QB3UQwQBekGv6M16tz6sT+trPVqy8p1zVID18wub8KSg</latexit>

precision-weighted fusion
<latexit sha1_base64="0R8zlBHJLV+S9zOdpYKeq8mzIa0=">AAACPnicbVDLSsNAFJ3UV61WW12Jm2AR3FiSbnRZcOOygn1AG8pketMMnUzCzEQtofg1bvUX/A1/wJ24dekkzcK0Hhg4nHMv585xI0alsqwPo7SxubW9U96t7O1XDw5r9aOeDGNBoEtCFoqBiyUwyqGrqGIwiATgwGXQd2c3qd9/ACFpyO/VPAInwFNOPUqw0tK4dqKnCU3ty0egU1/BxPRimXkNq2llMNeJnZMGytEZ143qaBKSOACuCMNSDm0rUk6ChaKEwaIyiiVEmMzwFIaachyAdJLsDwvzXCs6OhT6cWVm6t+NBAdSzgNXTwZY+XLVS8X/vGGsvGsnoTyKFXCyDPJiZqrQTAsxJ1Q3oNhcE0wE1beaxMcCE6VrK6RMBY58Sp4WRRV4dldBdINFRRdor9a1Tnqtpm017btWo93KqyyjU3SGLpCNrlAb3aIO6iKCntELekVvxrvxaXwZ38vRkpHvHKMCjJ9fFU2veQ==</latexit>

<latexitsha1_base64="ZMThSV6AJuqyqDZRp9cO5+n+/oc=">AAACKXicbVC9TsMwGLT5LYVCCyOLRYvEVCVdYKzEwlgk+iO1UeW4TmtqO5HtIKoo78AKr8DTsAErL4KbZiAtJ1k63X2fvvP5EWfaOM4X3Nre2d3bLx2UD48qxyfV2mlPh7EitEtCHqqBjzXlTNKuYYbTQaQoFj6nfX9+u/T7T1RpFsoHs4ioJ/BUsoARbKzUa4w0E41xte40nQxok7g5qYMcnXENVkaTkMSCSkM41nroOpHxEqwMI5ym5VGsaYTJHE/p0FKJBdVeksVN0aVVJigIlX3SoEz9u5FgofVC+HZSYDPT695S/M8bxia48RImo9hQSVaHgpgjE6Ll39GEKUoMX1iCiWI2KyIzrDAxtqHClanC0YyR57SoUpnlKoi+SMu2QHe9rk3SazVdp+net+rtVl5lCZyDC3AFXHAN2uAOdEAXEPAIXsAreIPv8AN+wu/V6BbMd85AAfDnFz7cpnk=</latexit>

sampling <latexit sha1_base64="DrhDvjcQOJv2Gu3evUdNaaiXyLA=">AAACK3icbVC9TsMwGLTLXykUWhhZLCokpirpAmMlFsYi0R+pjSrHdVKrthPZDqKK8hKs8Ao8DROIlffATTOQlk+ydLr7Pt35/JgzbRznE1Z2dvf2D6qHtaPj+slpo3k20FGiCO2TiEdq5GNNOZO0b5jhdBQrioXP6dBf3K304RNVmkXy0Sxj6gkcShYwgo2lRhoL6yLDaaPltJ180DZwC9ACxfSmTVifzCKSCCoN4VjrsevExkuxMoxwmtUmiaYxJgsc0rGFEguqvTQPnKEry8xQECn7pEE5+/cixULrpfDtpsBmrje1FfmfNk5McOulTMaJoZKsjYKEIxOh1e/RjClKDF9agIliNisic6wwMbajkkuocDxn5Dkrs1TmuUqkL7KaLdDdrGsbDDpt12m7D51Wt1NUWQUX4BJcAxfcgC64Bz3QBwRw8AJewRt8hx/wC36vVyuwuDkHpYE/vxDzp/s=</latexit>

multiplication
<latexit sha1_base64="LJDwo0WavaX6dTU+7247sb/m3GY=">AAACMXicbVDLTgIxFL3jE1EUdOmmkZi4IjNsdEnixiUm8khgJJ3SgYa2M2k7RjKZ/3Crv+DXsDNu/QnLMAsBT9Lk9Nx7c05OEHOmjesunJ3dvf2Dw9JR+fikcnpWrZ13dZQoQjsk4pHqB1hTziTtGGY47ceKYhFw2gtm98t574UqzSL5ZOYx9QWeSBYygo2VnkXCDbM+q++oWncbbg60TbyC1KFAe1RzKsNxRBJBpSEcaz3w3Nj4KVaGEU6z8jDRNMZkhid0YKnEgmo/zWNn6NoqYxRGyj5pUK7+vUix0HouArspsJnqzdlS/G82SEx456dMxomhkqyMwoQjE6FlB2jMFCWGzy3BRDGbFZEpVpgY29Say0TheMrIa7auUpnnWhMDkZVtgd5mXduk22x4bsN7bNZbzaLKElzCFdyAB7fQggdoQwcIKHiDd/hwPp2F8+V8r1Z3nOLmAtbg/PwCjLyqyg==</latexit>

bottom-up inference
<latexit sha1_base64="VqVw2dSnengxfF4U+onKgCOAyOQ=">AAACOHicbVC7SgNBFJ31GeMrmtJmMAg2ht00WgZsLBVMIsQQZid3k8F5LDN3xSXst9jqL/gndnZi6xc4iSlM4oGBwzn3cs+cOJXCYRi+Byura+sbm6Wt8vbO7t5+5eCw7UxmObS4kcbexcyBFBpaKFDCXWqBqVhCJ364nPidR7BOGH2LeQo9xYZaJIIz9FK/Uo0NolFnWUqFTsCC5tCv1MJ6OAVdJtGM1MgM1/2DYPd+YHimQCOXzLluFKbYGzOLgksoyveZg5TxBzaErqeaKXC98TR9QU+8MqCJsf5ppFP178aYKedyFftJxXDkFr2J+J/XzTC56I2FTjP03/o9lGSSoqGTKuhAWOAoc08Yt8JnpXzELOPoC5u7MrQsHQn+VMyroKe55sRYFWVfYLRY1zJpN+pRWI9uGrVmY1ZliRyRY3JKInJOmuSKXJMW4SQnz+SFvAZvwUfwGXz9jq4Es50qmUPw/QORMay1</latexit>
top-down inference
<latexit sha1_base64="lbZ0CP+3ZgKNjsmr8vh4ery5VQ4=">AAACN3icbVDLTgIxFO3gC1EUcOmmkZi4kcyw0SWJG5eYyCMBQjqdO9DQaSdtRyFkfsWt/oKf4sqdcesfWGAWAp6kyck59+aeHj/mTBvX/XByO7t7+wf5w8LRcfHktFSutLVMFIUWlVyqrk80cCagZZjh0I0VkMjn0PEndwu/8wRKMykezSyGQURGgoWMEmOlYaliZHwdyGeBmQhBgaAwLFXdmrsE3iZeRqooQ3NYdor9QNIkAmEoJ1r3PDc2gzlRhlEOaaGfaIgJnZAR9CwVJAI9mC/Dp/jSKgEOpbJPGLxU/27MSaT1LPLtZETMWG96C/E/r5eY8HYwZyJOjP3W6lCYcGwkXjSBA6aAGj6zhFDFbFZMx0QRamxfa1dGisRjRqfpugpimWtN9KO0YAv0NuvaJu16zXNr3kO92qhnVebRObpAV8hDN6iB7lETtRBFU/SCXtGb8+58Ol/O92o052Q7Z2gNzs8voW+sPA==</latexit>

zn <latexit sha1_base64="kKihJzslZNQcGREdxEzSoq/rx3Q=">AAACMHicbVDLTgIxFG19IoqCLt00gokrMsNGV4bEjUtM5JHAhHRKBxrazth2jDiZ73Crv+DX6Mq49SsswywEPMlNTs65N/fk+BFn2jjOJ9zY3Nre2S3sFfcPSodH5cpxR4exIrRNQh6qno815UzStmGG016kKBY+p11/ejP3u49UaRbKezOLqCfwWLKAEWys5NWSgR+g53SYyLQ2LFedupMBrRM3J1WQozWswNJgFJJYUGkIx1r3XScyXoKVYYTTtDiINY0wmeIx7VsqsaDaS7LUKTq3yggFobIjDcrUvxcJFlrPhG83BTYTverNxf+8fmyCKy9hMooNlWTxKIg5MiGaV4BGTFFi+MwSTBSzWRGZYIWJsUUtfRkrHE0YeUqXVSqzXEuiL9KiLdBdrWuddBp116m7d41q8zqvsgBOwRm4AC64BE1wC1qgDQh4AC/gFbzBd/gBv+D3YnUD5jcnYAnw5xdUjKmh</latexit>
Figure 2: Local Distributions. We illustrate the parametrization of a local variable zn in our structured representation. The local prior (Eq. (6)) is defined in terms of a top-down process (in black) predicting the node's parameters n from a gate-modulated sample of zn's parents zpa(n). The local approximate posterior (Eq. (7)) additionally performs a precision-weighted fusion of these parameters with the result of a bottom-up process using a node-specific MLP to predict input-conditioned parameters n from a generic encoding of x.

particular structure. Thus, we express the approximate posterior as:

N
q(z x) = q(zn x, zpa(n)),
n=1

(7)

where zpa(n) denotes the parent nodes, marginalized across the dependency gate variables, c. In practice, this involves sampling dependency gating variables to estimate the top-down input for
inference. With a single sample, corresponding to a single dependency structure, this exactly corre-
sponds to the inference procedure from Sønderby et al. (2016). Parameter prediction for the local

factors q(zn x, zpa(n)) consists of a precision-weighted fusion of the top-down prediction n described above and a bottom-up prediction n. The latter is obtained by encoding x into a generic feature that is used as an input to a node-specific multi-layer perceptron MLP(nBU) predicting n. This is shown in blue in Fig. 2.

Introducing the dependency gating variables modifies the variational objective (Eq. (2)), as we must now marginalize over these additional variables. In Appendix A.1, we derive the corresponding
lower bound, L, which can be expressed as

L = Ep(c) Eq(z x) [log p(x z, c)] - KL(q(z x) p(z c)) = Ep(c) [Lc] ,

(8)

where Lc is the ELBO for a given value of dependency gating variables, c. Thus, L can be interpreted as the expected ELBO under the distribution of dependency structures induced by p(c). We
form a Monte Carlo estimate of L by sampling c  p(c) and evaluating Lc. The training procedure is outlined in Algorithm 1.

3.2 LEARNING THE DISCRETE GATING VARIABLE DISTRIBUTIONS

For a given latent dependency structure, gradients for the parameters  and  can be estimated using
Monte Carlo samples and the reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014). To obtain gradients for the gate means, µ, we make use of recent advances in differentiating
through discrete operations (Maddison et al., 2017; Jang et al., 2017), allowing us to differentiate through the sampling of the dependency gating variables, c. Specifically, we recast the gating variables using the Gumbel-Softmax estimator from Jang et al. (2017), re-expressing ci,j as:

ci,j

=

exp((log(µi,j) + 1)  ) exp((log(µi,j) + 2)  ) + exp((log(1 -

µi,j ) +

3) ),

(9)

5

Under review as a conference paper at ICLR 2019

Algorithm 1 Optimizing VAEs with Latent Dependency Structure
Require: Data x, number of latent nodes N , number of dimensions per node N . 1: Initialize , , µ. 2: repeat 3: Sample c using Eq. (9) and determine zpa(n) for each zn based on the sampled structure. 4: For each node, compute q(zn x, zpa(n)) using Eq. (7). 5: Sample z from q(z x) using Eq. (6) and compute p(x z). 6: Update , , µ based on the gradients derived from Eq. (8). 7: until Convergence.

where 1, 2, and 3 are i.i.d samples drawn from a Gumbel(0, 1) distribution and  is a temperature parameter. The Gumbel-Softmax distribution is differentiable for  > 0, allowing us to estimate the

derivative

.ci,j
 µi,j

For

large

values

of

,

we

obtain

smoothed

versions

of

c,

essentially

interpolating

between different dependency structures. As   0, we recover binary values for c, yielding the

desired discrete sampling of dependency structures at the cost of high-variance gradient estimates.

Thus, we anneal  during training to learn the dependency gate means, µ, eventually arriving at the

discrete setting.

4 EVALUATION
We evaluate the proposed model and its learned latent dependency structure on three challenging datasets: MNIST (Lecun et al., 1998; Larochelle & Murray, 2011), Omniglot (Lake et al., 2013), and CIFAR-10 (Krizhevsky, 2009). MNIST and Omniglot are binarized and modeled with Bernoulli distributions; CIFAR-10 is modeled with Gaussian densities.
Our baselines consist of classic variational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014) as well as its popular variants, including ladder VAEs (Sønderby et al., 2016) and VAEs with fully-connected latent dependency structure (FC-VAEs; cf. (Kingma et al., 2016)). Together, these baselines cover the full spectrum of latent dependency structures, ranging from the strongest possible independence assumptions (fully independent in classic VAEs) to linear dependencies (chainstructured in ladder VAEs) to the most expressive representation (fully connected in FC-VAEs). Our experiments show that none of those predefined extremes is optimal. Instead, we demonstrate that learning a data-driven distribution over graph structures that is jointly optimized with the variational and generative model parameters leads to a superior representation of the underlying test-time distribution. An ablation study provides further insights into our learning process, hyperparameter selection, and the nature of the inferred structures.
We note that our approach is orthogonal and could be complemented by a number of other approaches attempting to overcome the mean field assumptions of classic VAEs: Similar to ladder VAEs, (Zhao et al., 2017) also introduces chain-structured latent dependencies but focuses on learning a disentangled representation. VAEs with normalizing flows (Rezende & Mohamed, 2015) gradually increase the complexity of the approximate posterior through a series of invertible transformations.
4.1 EXPERIMENTAL SETUP
To guarantee a fair comparison, the encoders of all structured methods use the same MLP architecture with batch normalization (Ioffe & Szegedy, 2015) and ReLU non-linearities (Nair & Hinton, 2010) in all experiments.2 Decoder structures are the reverse of the encoders. Likewise, the number of latent dimensions is the same in all experiments/models (M = 80). As discussed in section 3.1, all latent dependencies are modelled by non-linear MLPs as well.
For MNIST and Omniglot, we model p(x z) as a Bernoulli distribution, using a sigmoid nonlinearity on the output layer to produce the mean of this distribution. For CIFAR-10, we model
2Classic VAE uses a more complex encoder to match the number of parameters of the structured methods. All baselines use the same or more parameters than graph VAE.

6

Under review as a conference paper at ICLR 2019

(a) (b)

Figure 3: Structure Learning. In (a), we show training of the Bernoulli parameters µi,j governing the distribution over graph structures in architecture space. All edges are color-coded and can be located in (b), where we show a
random sample from the resulting steady-state distribution
with the same color scheme.

Figure 4: Ablation Study on MNIST. For a fixed latent dimension M (colorcoded), we report the log-likelihood of
all possible factorizations of node dimension N  (x-axis) and number of nodes N = M N .

p(x z) with a Gaussian density, with mean and log-variance predicted by sigmoid and linear functions, respectively. Further implementation details, including the exact model architectures and training criteria, can be found in Appendix A.2.
4.2 QUALITATIVE ANALYSIS
An example of the structure learning process on MNIST with N = 5 nodes and a total latent dimension of M = 80 (i.e., N  = M N = 16 dimensions per node) is visualized in Fig. 3. In Fig. 3a, we show the evolution of the parameters µ of the gating variables c. The network actively drops 3 10 edges during the learning process, while the remaining parameters eventually converge to 1. We observed that the resulting distribution is stable across multiple training runs with different seeds for weight initialization and minibatch sampling. A sample of the learned distribution is shown in Fig. 3b.
Fig. 4 reports an ablation study conducted on MNIST. In particular, we investigate the influence of the total latent dimension M and, for fixed latent dimension, the trade-off between number of nodes N and node dimension N  = M N . Models with the same total latent dimension M are shown in the same color. We observe two trends: (1) The performance of our model increases with latent dimension. This is an expected result and a likely explanation for this behaviour is the additional flexibility of a higher-dimensional latent space; (2) For a fixed number of latent dimensions, less node dimensions (and thus more nodes and the potential for a more complex dependency structure) typically performs better, highlighting the importance of a learned, non-trivial dependency structure.
4.3 QUANTITATIVE COMPARISON
We evaluate the performance of all models using their test-time log-likelihood log p(x) (Table 1). All values were estimated using 5, 000 importance-weighted samples. Following standard practice, we report log p(x) in nats on MNIST/Omniglot and in bits/input dimension on CIFAR-10. Our proposed model with a jointly optimized, learned dependency structure consistently outperforms both models with less expressive (VAE, ladder VAE) and more expressive (FC-VAE) predefined structures. We discuss potential reasons in section 4.4. To provide further insights into the training objective, Table 1 also reports DKL(q(z x) p(z)) and the full ELBO objective (Eq. (8)) of the trained models.
4.4 DISCUSSION
We want to briefly discuss a number of observations that we have made during training and application of the proposed model and hope they provide the starting point for further research on structure learning in deep generative models.
7

Under review as a conference paper at ICLR 2019

Dataset
VAE Ladder VAE FC VAE Graph VAE

MNIST

LL KL ELBO

-89.1 -84.8 -83.0 -82.1

29.0 24.3 28.9 27.8

-92.1 -87.8 -84.8 -84.1

Method

Omniglot

LL KL ELBO

-110.9 -106.4 -104.8 -103.4

30.5 27.9 29.9 29.1

-120.4 -112.5 -106.6 -105.2

CIFAR-10

LL KL ELBO

-6.63 -6.47 -6.44 -6.40

0.110 0.082 0.077 0.074

-6.69 -6.50 -6.46 -6.41

Table 1: Quantitative Analysis. We show test-time log-likelihoods (LL), DKL(q(z x) p(z)) (KL), and ELBO of the proposed graph VAE model (last row) and compare it to baselines on 3
popular datasets.

Performance vs. Speed/Memory. As shown in Fig. 4, the number of latent nodes has a big impact on the performance of our model. While allowing more complex dependency structures through a low M N -ratio is typically beneficial, it also has an adverse effect on the training time and memory consumption. Fortunately, the ability to freely select this ratio allows a simple adaption to the available processing power, hardware constraints, and application scenarios.
Optimization Order. It is worth noting that the learning process optimizes the model parameters (c, , ) in a clear temporal order: While the latent structure, governed by c, converges during the first  200 epochs (Fig. 3a), it takes over 10× as long until the variational and generative parameters (, ) converge. There is no external force incentivizing this behaviour, indicating that the loss can initially most easily be decreased by limiting the latent structure to the complexity prescribed by the observed training data.
Improvement over Fully-Connected VAEs. By definition, the latent spaces of FC-VAEs make no conditional independence assumptions and are therefore more expressive and more flexible than the other models in this comparison, including any latent structure resulting from the proposed learning process. The fact that graph VAE outperforms FC-VAE across all datasets is thus interesting, but not entirely surprising: It is, for instance, well-known that latent variable models are prone to local optima (Bowman et al., 2016; Burda et al., 2016) and it is reasonable to assume that complex dependencies only add to the optimization challenges. The ability to modify entire latent dependencies allows a data-driven simplification of the underlying structure and, potentially, the optimization task itself.
5 CONCLUSION
We presented a novel method for optimizing variational autoencoders jointly with their latent dependency structures. Our experiments showed that the learned latent dependency structure improves the generative performance of latent variable models. By introducing an additional set of binary structure indicator variables and optimizing a structure-dependent evidence lower bound, our model is able to learn a better representation of the underlying generative distribution than baselines with a predetermined structure.
We believe that the proposed model is useful beyond the applications shown in this work and can be extended in many interesting ways. Examples include structure learning on undirected Markov random fields, dynamic structure prediction in time-series models, and joint optimization with the neural encoder-decoder architecture.

8

Under review as a conference paper at ICLR 2019
REFERENCES
A. Barron, J. Rissanen, and B. Yu. The Minimum Description Length Principle in Coding and Modeling. In IEEE Transactions on Information Theory, 1998.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pp. 10­21, 2016.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In International Conference on Learning Representations, 2016.
Jie Cheng, Russell Greiner, Jonathan Kelly, David Bell, and Weiru Liu. Learning Bayesian Networks from Data: An Information-theoretic Approach. In Artificial Intelligence, 2002.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pp. 2980­2988, 2015.
Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine. Neural computation, 7(5):889­904, 1995.
Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. Sequential neural models with stochastic layers. In Advances in neural information processing systems, pp. 2199­2207, 2016.
Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks. In International Conference on Machine Learning, pp. 1242­1250, 2014.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. Draw: A recurrent neural network for image generation. In International Conference on Machine Learning, pp. 1462­1471, 2015.
Jiawei He, Andreas Lehrmann, Joseph Marino, Greg Mori, and Leonid Sigal. Probabilistic video generation using holistic attribute control. In European Conference on Computer Vision, 2018.
D. Heckerman, C. Meek, and G. Cooper. A Bayesian Approach to Causal Discovery. In Computation, Causation, Discovery, 1999.
David Heckerman, Dan Geiger, and David M. Chickering. Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. In Machine Learning, 1995.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumble-softmax. In International Conference on Learning Representations, 2017.
Matthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. Composing graphical models with neural networks for structured representations and fast inference. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 2946­2954. Curran Associates, Inc., 2016.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183­233, 1999.
R. Kass and A. Raftery. Bayes Factors. In Journal of the American Statistical Association, 1995.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014.
9

Under review as a conference paper at ICLR 2019
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pp. 4743­4751, 2016.
M. Koivisto and K. Sood. Exact Bayesian Structure Discovery in Bayesian Networks. In Journal of Machine Learning Research, 2004.
Daphne Koller and Nir Friedman. Probabilistic Graphical Models. MIT Press, 2009.
Rahul G Krishnan, Dawen Liang, and Matthew Hoffman. On the challenges of learning with inference networks on sparse, high-dimensional data. arXiv preprint arXiv:1710.06085, 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Brenden M Lake, Ruslan R Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting a compositional causal process. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 2526­2534. Curran Associates, Inc., 2013.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29­37, 2011.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, Nov 1998. ISSN 0018-9219. doi: 10.1109/5.726791.
Erich L. Lehmann and Joseph P. Romano. Testing Statistical Hypotheses. Springer, 2008.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. International Conference on Learning Representations, 2017.
Joseph Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. In International Conference on Machine Learning, pp. 3400­3409, 2018.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807­814, 2010.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning, pp. 1530­1538, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pp. 1278­1286, 2014.
Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder variational autoencoders. In Advances in neural information processing systems, pp. 3738­3746, 2016.
Li Yingzhen and Stephan Mandt. Disentangled sequential autoencoder. In International Conference on Machine Learning, pp. 5656­5665, 2018.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from deep generative models. In International Conference on Machine Learning, pp. 4091­4099, 2017.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.
10

Under review as a conference paper at ICLR 2019

A SUPPLEMENTARY MATERIAL

A.1 LOWER BOUND DERIVATION

Introducing the dependency gating variables, c, modifies the evidence lower bound (ELBO) from Eq. (2), as we now have an additional set of random variables over which to marginalize. To see this, we can start by re-expressing Eq. (2) as

L = Eq(z x) [log p(x, z)] - Eq(z x) [log q(z x)] .

(10)

p(x, z) can be expressed as a marginalization over the gating variables, effectively averaging over an ensemble of models with a distribution of dependency structures:

p(x, z) = p(x, z, c)dc = p(x, z c)p(c)dc = Ep(c) [p(x, z c)] .

(11)

Plugging this into Eq. (10):

L = Eq(z x) log Ep(c) [p(x, z c)] - Eq(z x) [log q(z x)] . Using Jensen's inequality, we can bring the log inside of the expectation Ep(c) [], yielding

(12)

L  L = Eq(z x) Ep(c) [log p(x, z c)] - Eq(z x) [log q(z x)] ,

(13)

where we have defined L as a lower bound on L (and therefore still a lower bound on log p(x)). Swapping the order of expectation, we can rewrite L as

L = Ep(c) Eq(z x) [log p(x, z c)] - Eq(z x) [log q(z x)] ,

(14)

and because the second term is independent of p(c), we can include both terms inside of a single outer expectation:

L = Ep(c) Eq(z x) [log p(x, z c) - log q(z x)] = Ep(c) Eq(z x) [log p(x z, c)] - KL(q(z x) p(z c)) = Ep(c) [Lc] ,

(15)

where we have defined Lc as the ELBO for a given dependency structure. Thus, during training, we can estimate L by sampling gating variables, c  p(c), and evaluating Monte Carlo estimates of Lc.

11

Under review as a conference paper at ICLR 2019
A.2 IMPLEMENTATION DETAILS A.2.1 NETWORK ARCHITECTURE We document the network architectures used in our experiments. We use the same network architectures for all datasets. Input size (pixels per image) is the only difference across datasets. The input size is 28 × 28 for MNIST and Omniglot, and 3 × 32 × 32 for CIFAR-10. Encoders: fc(input size, 512)  Batch Norm  ELU  fc(512, 512)  Batch Norm  ELU  fc(512, 512)  Batch Norm  ELU  fc(512, 256) Latent Dependencies: Latent dependencies are modelled by non-linear MLPs. Note that the topdown architecture is shared between inference model and generative model. But the MLPs are optimized independently. bottom-up: For each node with N  dimensions, the local potential is predicted by a mapping from the encoded feature: fc(256, 256)  Batch Norm  ReLU  fc(256, N ). The output feature is then mapped to µ and log var with two independent fc(N , N ) respectively. top-down: For each node (N  dimensions) with a set of parent nodes, the top-down inference/generation is implemented as: fc(sum of parent nodes' dimension, 256)  Batch Norm  ReLU  fc(256, N ) The output feature is then mapped to µ and log var with two independent fc(N , N ) respectively. Decoders: fc(512, 512)  Batch Norm  ReLU  fc(512, 512)  Batch Norm  ReLU  fc(512, 512)  Batch Norm  ReLU  fc(512, input size)  out put function() out put function for MNIST and Omniglot is sigmoid() which predicts the µ of Bernoulli observations; and sigmoid predicting µ, fc(input size, input size) predicting log var of Gaussion observations for CIFAR-10. A.2.2 TRAINING All models were implemented with PyTorch (Paszke et al., 2017) and trained using the Adam (Kingma & Ba, 2015) optimizer with a mini-batch size of 128, and learning rate of 3e-4. The Gumbel-softmax temperature were initialized as 1, and decrease to 0.999epoch at each epoch. MNIST and Omniglot took 2000 epochs to converge, and CIFAR took 3000 epochs to converge.
12

