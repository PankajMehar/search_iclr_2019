Under review as a conference paper at ICLR 2019

INFORMATION REGULARIZED NEURAL NETWORKS
Anonymous authors Paper under double-blind review

ABSTRACT
We formulate an information-based optimization problem for supervised classification. For invertible neural networks, the control of these information terms is passed down to the latent features and parameter matrix in the last fully connected layer, given that mutual information is invariant under invertible map. We propose an objective function and prove that it solves the optimization problem. Our framework allows us to learn latent features in an more interpretable form while improving the classification performance. We perform extensive quantitative and qualitative experiments in comparison with the existing state-of-the-art classification models.

1 INTRODUCTION

Quantities of information are nonlinear measures capable of describing complex relationship between unstructured data and they form the basis of the probabilistic algorithms in the literature of machine learning. Information theoretic methods are also reported to be effective on various branches of deep learning problems (see Section 3). In particular, Information Bottleneck (IB) problem (Tishby et al., 1999) is formulated as:

minimize I(X; T ) - I(Y ; T ) ,

(1)

where the solution random variable T is interpreted as a minimal sufficient representation of signal X for label Y . The mutual information term I(X; T ) has its origins in Lossy Compression and Rate-Distortion Theory (Cover & Thomas, 2006), conveying an simple idea of "keep only what is relevant".

However, Saxe et al. (2018) argued that the mutual information I(X; T ) between signal X and feature T in intermediate layer is infinite, as the transformation from X to continuous random variable T is deterministic. In addition they showed experimentally that layers equipped with ReLU actually do not compress too much information, which is supported by many recent work on the invertibility of the neural network (see Section 3). This motivates us to consider a different problem with similar principle idea: we would like to establish a theoretically valid objective that allows the neural network to extract only the relevant information for classification from the data.

We focus on the discrete prediction random variable Y inferred by the probabilistic model P(Y |X) and introduce the following information optimization problem for supervised classification:

maximize I(Y ; Y ) subject to I(X; Y ) - I(Y ; Y ) <  , for some  > 0 .

(2)

The intuition behind this objective lies in twofold:
Information perspective: A good classification model should be robust against irrelevant features of X, and prevent over-fitting in the learning process. In optimization problem (2) we maximize the relevant information I(Y ; Y ), while constraining the irrelevant information I(X; Y ) - I(Y ; Y )
that X has about Y . Although I(X; Y ) - I(Y ; Y ) converges to zero as I(Y ; Y ) approaching its maximum (see Figure 1L), in practice it's never attained due to the limited capacity of the models or over-fitting. Our proposed constrain addresses the problem of over-fitting: if two models achieve the similar classification accuracy, this constraint prefers the one that does not overfit to spurious factors of variation in X (e.g., pixel-level artifact/noise in the image that accidentally correlates to the labels in the training data).

1

Under review as a conference paper at ICLR 2019
Prediction confidence perspective: A good classification model should not be certain about its decision which is in fact wrong. However, modern neural networks are too confident in their predictions (Guo et al., 2017; Szegedy et al., 2015; Pereyra et al., 2017). To be more precise, high capacity neural networks mostly assign labels of data with prediction confidence near 0 or 1. In particular, they assign 0 probability to some correct labels and therefore do not have enough flexibility to correct themselves from making the wrong prediction. We propose to compress the irrelevant information I(X; Y ) - I(Y ; Y ), where minimizing I(X; Y ) decreases the confidence on all predictions but maximizing I(Y ; Y ) increases the confidence on the correct predictions. Therefore the overall effect reduces the certainty on the false prediction of Y (see Figure 1L). To solve this optimization problem, we first present some insights on the dynamics of deep neural network, which can be decomposed into two stages: (i) Transformation stage: samples {Xk}k=1:n of the high dimensional unstructured signal X are transformed under the deep invertible (information preserving) feature map F to become (almost) linearly separable; (ii) Classification stage: the weight matrix w in the last fully connected layer together with the Softmax function, takes structured features {F (Xk)}k=1:n as input and gives predictions. Invertibility of F allows neural networks to pass the control of I(X; Y ) = I(F (X); Y ) towards F (X) and w in the last layer, where F (X) can be interpreted as transformed signal that preserves information about the original signal X and the inference model becomes conceptually linear with classifier w (see Figure 1R). In Section 2 we derive objective function (6) and prove that it solves the optimization problem (2). We show the classification performance is improved in Section 4.1 and the features F (X) are sculpted into a form with more interpretability entry-wise in Section 4.2.
Figure 1: (L) An information Venn diagram: three disks represent the entropy of X, Y, Y respectively, the area in red is the relevant information I(Y ; Y ), the area in grey is the irrelevant information I(X; Y ) - I(Y ; Y ). The optimal solution is obtained when the smaller disks coincide, which is typically not achieved in practice. In particular, the trained model may be extremely confident in its prediction (when H(Y ) lies inside of H(X)), but predicts the wrong label (having large grey area). Our optimization problem explicitly prohibits the growth of grey area throughout the training. (R) Logic chart of our formulation: our proposed optimization problem only involves F (X) and w, allowing us to have control over the latent feature F (X) directly.
The invertibility property has been empirically demonstrated for complex non-linear deep neural networks that are widely used in practice. We will discuss the literature in Section 3. In addition, we prove in Proposition C.1 that a lower bound of classification error is minimized if neural network is invertible. Our contribution: Our contribution lies in the following: (i) we formulated a novel information optimization problem for supervised classification; (ii) we propose a simple objective function that improves supervised deep learning with better performance and interpretability; (iii) we formally justify the use of 1, 2 regularization from an information perspective. Different from the naive regularization on w, our regularization on wT F (X) is novel and effective.
2 MAIN RESULTS
Consider a classification problem where the training data D = {(xk, yk)}k=1:n are sampled from random variable pair (X, Y ) with unknown joint distribution. Each xk is fed into a deep prob-
2

Under review as a conference paper at ICLR 2019

abilistic model, which outputs probability densities and predicts yk, a realization of the prediction random variable Y . Let C denote the label class and X denote the signal space, then the

mutual information between random variables, e.g., continuous X and discrete Y , is defined as

I(X; Y ) =

yC

X p(x, y) log

p(x,y) p(x)p(y)

dx, and the entropy of Y is defined as H(Y ) =

- yC p(y) log p(y).

We first call out our assumptions used throughout our analysis. (I): we assume the marginal densities of Y, Y are uniform over C; (II): there exists a unique true label for every sample of X.

Mutual information is bounded and its gradient with respect to logits is approximately zero over a large domain. In particular if the logits are initially small for true labels, gradient updates cannot effectively correct them. Therefore it's not practical to train mutual information terms directly. In this section we introduce alternative terms and prove that they are feasible for our purpose. We show
in Proposition 2.2 that I(Y ; Y ) is maximized if the classical cross entropy objective is minimized.
On the other hand, we show in Proposition 2.1 that for invertible F , I(X; Y ) = I(F (X); Y ) is minimized if wT F (X) is minimized. We derive our objective function (6) in Section 2.2. Our experimental result in Section 4.1 verifies that the proposed objective function does compress the
irrelevant information I(X; Y ) - I(Y ; Y ).

2.1 SOLVING OPTIMIZATION PROBLEM WITH FEASIBLE TERMS

Without loss of generality, we consider the binary classification problem, i.e. the label class C =
{±1}. To tract the population quantities I(F (X); Y ) and I(Y ; Y ), we decompose each of them into an empirical part and a probabilistic bound, which is negligible if sample size n is large. In Proposition 2.1, we show that in order to compress I(F (X); Y ), we need to compress the norm of classifier w and feature F (X). In particular, smaller |wT F (X)| represents lower confidence of the
model on its predictions Y , indicating a smaller amount of mutual information I(F (X); Y ). The proof is provided in Appendix A.

Proposition 2.1. I(X; Y ) = I(F (X); Y ) is well estimated by its empirical version with high prob-

ability, which shares the same unique (global) minimum with

n k=1

|wT F (xk)|

at

wT

F (xk)

=

0,

for all k  {1, ..., n}.

Denote the sigmoid function with (a) = 1/(1 + e-a). Proposition 2.2 establishes the relation-

ship between maximization over mutual information I(Y ; Y ) and minimization over cross entropy

-

n k=1

log

(yk

wT

F

(xk

));

higher

confidence

of

the

model

on

its

correct

predictions

over

the

samples indicates a larger value of I(Y ; Y ). The proof is provided in Appendix B.

Proposition 2.2. I(Y ; Y ) is well estimated by its empirical version with high probability, which

shares the same unique (global) maximum with

n k=1

log

 (yk wT

F

(xk ))

given

that

yk wT

F

(xk )

>

1 2

,

for

all

k



{1,

...,

n}.

2.2 DERIVATION OF OBJECTIVE FUNCTION

In Lagrangian form of optimization problem (2), the constant  can be dropped and the objective becomes

maximize (1 + )I(Y ; Y ) - I(F (X); Y )  maximize I(Y ; Y ) -

 I(F (X); Y ) .

(3)

1+

Consider a single signal xk and its true label yk, we propose the following objective function for binary supervised classification problem:

Lk = R |wT F (xk)| - log (ykwT F (xk)) ,

(4)

where R is some regularizer. According to results in Section 2.1, minimizing (4) allows us to
maximize I(Y ; Y ) while constraining I(X; Y ). We typically choose the hyper-parameter  > 0 to be a reasonably small number. The intuition comes from the observation that /(1 + ) is upper

3

Under review as a conference paper at ICLR 2019

bounded by one. If we compress I(X; Y ) harshly, then neural networks may choose to minimize I(F (X); Y ) at a cost of minimizing I(Y ; Y ).

Recall from the information theoretic perspective of our proposed optimization problem, our regu-
larizer should prefer a model that does not overfit among all the ones with high training accuracy. In this case neural networks assign only large logits wyTk F (xk) to true label yk for each signal
xk, and generalization of (4) to multi-class case for I(F (X); Y ) can be simplified to constraining wyTk F (xk) - wjT F (xk), where wj is the jth column of w, assigning feature F (xk) a probability to label j. We propose to simply constrain wyTk F (xk) and does not encourage increasing wjT F (xk):

Lk = R |wyTk F (xk)| - log

ewyTk F (xk)

ewyTk F (xk) +

ewjT F (xk)
j=yk

.

(5)

In our experiment, we take the Elastic Net approach by Zou & Hastie (2005) using a combination of 2 and 1 regularizers: we use Holder's inequality to bound |wyTk F (xk)| with both sup F (X) · wyk 1 and wyk 2 · F (xk) 2. In practice we assume sup F (X) to be a constant and is absorbed
into the hyper-parameter. Our proposed objective function is of the form:

L

=

1||w||1

+

1 n

n

k=1

2 wyk 2 · F (xk) 2 - log

ewyTk F (xk)

ewyTk F (xk) +

ewjT F (xk)
j=yk

. (6)

Notice that classical training methods maximize cross entropy and therefore I(Y ; Y ), but do not compress I(X; Y ) explicitly. In Equation (6), we explicitly encourage the compression of irrelevant information I(X; Y ) - I(Y ; Y ). As we demonstrate in Section 4.3, the proposed objective function does encourage a smaller amount of I(X; Y ) - I(Y ; Y ) throughout the training process.

3 RELATED WORK
Recent experimental work reported that neural networks with invertible structure have better performance. Dosovitskiy & Brox (2015) showed that images can be resconstructed from the latent features in AlexNet through an inverting process; this reconstruction is further improved by Zhang et al. (2016), where they built an encoder-decoder structure to encourage invertibility and showed reconstructive objective is beneficial to the performance of the neural network (e.g., VGGNet). Shang et al. (2016) proposed an invertible activation scheme named CReLU to preserve information; Gilbert et al. (2017) analyzed theoretically the invertibility of CNN; Jacobsen et al. (2018) built a theoretic invertible structure whose performance is comparable to ResNet He et al. (2015). Invertibility seems to be an intriguing property or design principle that often emerges in the recent state-of-the-art deep architectures.
Information theoretic methods are reported to be effective attacking machine learning problems. In deep learning, IB was first introduced in Tishby & Zaslavsky (2015) and the follow-up experimental work Shwartz-Ziv & Tishby (2017). They argued that DNN structure forms a markov chain and information is compressed layer by layer. The theoretical breakthrough by Achille & Soatto (2017) established a connection between the IB objective and the generalization in deep learning, carrying along with the notion of sufficiency and invariance of representations. Strouse & Schwab (2016),Slonim & Tishby (1999) used IB objectives for clustering problems. Alemi et al. (2017), Gao et al. (2018),Kim & Mnih (2018) established information theoretic approaches based on VAE to encourage disentangled and informative latent representations; Grandvalet & Bengio (2004) introduced entropy regularizer in semi-supervised learning; Krause et al. (2010) took Regularized Information Maximization(RIM) approach for classification problems; Chen et al. (2016) proposed to add information ingredients to the objective of GANs, encourage to learn disentangled representations.
Our framework decomposes deep neural network into a composition of nonlinear transformation map and a linear probablistic model; this idea was originated in Bell & Sejnowski (1995) where
they considered the blind separation problem and decompose the prediction Y to be the sum of an

4

Under review as a conference paper at ICLR 2019

NaiveReg ResNet-32 2 = 0.0005 2 = 0.001 2 = 0.002 2 = 0.004 Best Accuracy % 70.06 ± 0.38 69.94 ± 0.33 69.86 ± 0.32 68.99 ± 0.52

ResNet-32

Original

2 = 0.01

2 = 0.03

2 = 0.05

Best Accuracy % 70.15 ± 0.33 70.34 ± 0.27 70.57 ± 0.20 70.25 ± 0.23

Constrain

0.296 ± 0.044 0.293 ± 0.043 0.280 ± 0.040 0.266 ± 0.038

ResNet-Wide

Original

2 = 0.01

2 = 0.05

2 = 0.09

2 = 0.15

Best Accuracy % 78.51 ± 0.27 79.37 ± 0.18 79.62 ± 0.13 79.64 ± 0.12 79.45 ± 0.14

Constrain

0.254 ± 0.056 0.240 ± 0.051 0.213 ± 0.049 0.198 ± 0.044 0.174 ± 0.038

Table 1: Performance comparison on CIFAR100, Best Accuracy (%, test set) and the average val-
ues of the constrain I(X; Y ) - I(Y ; Y ) throughout the training process are provided in the tables. The first table gives the results of naive regularization on w for ResNet-32, with 1 = 0, 2 = [0.0005, 0.001, 0.002, 0.003, 0.004], note that the choices of hyper-parameters are small because it regularize the whole matrix w but not to specific columns as proposed; the second table is for ResNet-32, comparing original ResNet and RegResNets with 1 = 0, 2 = [0.01, 0.03, 0.05]; the third table is for ResNet-28-10-Wide, comparing original ResNet and RegResNets with 1 = 5e-6, 2 = [0.01, 0.05, 0.09, 0.15]. All results are calculated from 10 samples. The constrain decreases as 2 increases; the performance can be effectively improved under proper choice of 2.

invertible deterministic part and a stochastic part. Amjad & Geiger (2018) and Kolchinsky et al. (2017) also studied the IB problem in a stochastic setting. Our idea of explicit regularization on w and F (X) is related to the margin based and stability based interpretations of generalization in deep learning respectively, studied by Arora et al. (2018), Bartlett & Mendelson (2003), Neyshabur et al. (2017), Sun et al. (2015).
4 EXPERIMENTS
In our experiments we build the feature map F with ResNet or InvNet (introduced in Section 4.2). In Appendix D we prove that ResNet by He et al. (2015) is invertible under mild assumptions. We prefix the name of the model trained under our objective with "Reg", i.e. RegResNet/RegInvNet.
4.1 PERFORMANCE
We report the accuracy of ResNet on test data of CIFAR100 in Table 1.1 We compare the performance between our proposed regularization on wyTk F (xk) and the naive regularization on w. In both form of regularization we take 1 = 0 and test over different choices of 2. We pick smaller 2 for naive regularization because it's applied to the full matrix. We observe the performance of ResNet-32 under naive regularization drops monotonically as 2 increases.
Under a suitable choice of hyperparameters, RegResNet outperforms ResNet by a noticeable margin. It implies that our proposed constrain on the irrelevant information I(X; Y ) - I(Y ; Y ) is beneficial to the classification performance. However, if the hyperparameters are too large, the performance drops, i.e. 2 = 0.05 for ResNet-32 and 2 = 0.15 for ResNet-Wide; this matches the discussion in Section 2.2 that the model may try to reduce the relevant information I(Y ; Y ). Our approach addresses the problem of over-fitting; ResNet-Wide is improved by a larger margin compared to ResNet-32 because ResNet-Wide has higher capacity and is therefore over-fits more to the training data.
4.2 ANALYSIS ON FEATURES
We introduce another invertible structured neural network on MNIST dataset and analyze the feature F (X) learned in the last layer qualitatively. The feature map F is built to be LeNet-300-100 and
1We use the open source code implemented by Xin Pan at https://github.com/tensorflow/ models/tree/master/research/resnet and build our objective function based upon it. Our results for the baseline models match the reported ones on the website.

5

Under review as a conference paper at ICLR 2019

the decoder D has the opposite structure. At each step during the training process, we update the autoencoder F + D and the InvNet F + w alternatively; our regularization is applied to w as usual. In this section we report our result with 1 = 2 = 0.002.
4.2.1 LEARNING TO OVERLOOK IRRELEVANT INFORMATION
We feed 1k testing samples of digit 9 into the neural network to get 1k samples of features F (X). Recall that the features F (X) are vectors of dimension 100 and w is a matrix of size 100 × 10. We calculate the mean and standard deviation of each entry of the features from these 1k samples. From Figure 2 we see that the trained features are roughly Gaussian distributed for InvNet and are sparse for RegInvNet. Moreover, the 10th column of w (which classifies digit 9) can capture the feature entries with large mean values, which are regarded as representative entries of digit 9. Intuitively, our regularization forces small w10 on irrelevant entries, so the logits it outputs are not sensitive to noise in these entries; on the other hand, we do encourage large w10 on the representative entries. This matches our motivation that a neural network should be robust against irrelevant information I(F (X); Y ) - I(Y ; Y ) and focus on the relevant information I(Y ; Y ).

Value Value

3.0 Digit 9 for InvNet

2.5

feature mean w_10

2.0 feature std

1.5

1.0

0.5

0.0

0.5

1.0

1.50 20 40 60 80 100 Index of Feature Entry

3.5 Digit 9 for InvNet

3.0

feature mean w_10

2.5 feature std

2.0

1.5

1.0

0.5

0.0

0.50 20 40 60 80 100 Index of Feature Entry

Figure 2: Statistics plots for feature entries of digit 9. Recall that the features F (X) are vectors of dimension 100 and w is a matrix of size 100 × 10. The horizontal coordinate represents the index of entries of F (X) ranging from 0 to 99, the vertical coordinate represents entry values of the 10th column of w, the values of mean and standard deviation for each entry of features from samples of digit 9. For RegInvNet, w10 only assigns large value to representative entries and is more robust against perturbations in other irrelevant entries.

4.2.2 LEARNING TO EXTRACT RELEVANT INFORMATION
It had been argued by Szegedy et al. (2013) that it is the space but not individual units in high level features that encodes interpretable information. Under our regularization, a meaningful basis of the informative space is found; in particular, the features of 9 are encoded into 10 entries (see Figure 2). On the other hand, features that have high values in these entries are expected to be the features of digit 9.
To validate this conjecture, for each digit, we find the entry of its feature mean with the highest value. We use the micro and macro average ROC metric on these feature entries and compare the results from InvNet and RegInvNet in Figure 3. The curve with larger area underneath indicates higher representative power of individual entries learned in the features. We conclude that under our regularization, relevant information for classification is encoded into only a few key entries of the features, and these entries are highly indicative and interpretable.
4.3 RELATIONSHIP TO LINEAR REGRESSION
We trained ResNet-32 on CIFAR10, the feature F (X) is a vector of size 64, the classifier w is a matrix of size 64 × 10. Note that the product wT F (X) is a vector of size 10 representing the probability assigned by the model for each class.
6

Under review as a conference paper at ICLR 2019

True Positive Rate True Positive Rate

InvNet
1.0 0.8 0.6 0.4
0.2 micro-average ROC curve (area = 0.81) macro-average ROC curve (area = 0.81)
0.00.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate

RegInvNet
1.0 0.8 0.6 0.4
0.2 micro-average ROC curve (area = 0.99) macro-average ROC curve (area = 0.99)
0.00.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate

Figure 3: The macro/micro average ROC curves for representative entries of features genearated by original and regularized model. The entries of features learned under our regularization strongly indicate the categories of the digits.

Under our framework, deep learning can be conceptually simplified to regularized linear regression if we regard F (X) as input data. In linear regression, w is regularized to prevent over-fitting. Under our regularization, the 2-norm of w is suppressed while the 2-norms of feature F (X) remain the same (see Figure 4). Intuitively, a 2-regularized w will be less sensitive to normalized "support features" and "outlier features". In addition, several rows of w are trained to be zero, which implies that many entries of the feature F (X) are regarded by the network as irrelevant information for classification, since any variance in the entries of F (X) where the corresponding rows of w are zero has no influence on the probability assigned to each label class by the model.

Figure 4: Compression of the RegResNet-32 (Blue) and the original ResNet-32 (Orange) on CIFAR10 over the training process: the first plot records the average 2 norm of the last layer features F (X) in a batch; the second plot records the average 2 norm of the columns of w; the third plot records the ratio of zero entries among all entries of w; the plots for trained w with/without regularization after 84000 steps are listed on the right. Best test accuracy are 92.86% and 92.64% for regularized and original ResNet-32 respectively. Under our regularization, the norm of the feature learned remains the same and the norm of classifier w is smaller. Therefore w is less sensitive to "support" and "outlier" features.
5 THE ROLE OF INVERTIBILITY
Invertibility allows us to treat F (X) as transformed data that preserves all the information from X, and therefore work on the information regularization problem under a linear scheme.
In Appendix D we prove that ResNet is fairly invertible due to the intrinsic invertibility of the operator I + L given L < 1. In this section we build a PlainNet by using only L as the operator for each building block, so the theoretical guarantee for invertibility is not present for PlainNet. In Table 2, we see that PlainNet-32 can still benefit from our regularization, however, it's performance is less stable compared to ResNet-32 if the hyper-parameters are too large. The reason is for PlainNet, the feature in the last layer F (X) does not preserve information about X very well, so it has a higher demand on the capacity of the classifer w and is therefore more sensitive to our regularization.
7

Under review as a conference paper at ICLR 2019

Performance % ResNet-32 PlainNet-32

Original 92.49 ± 0.14 90.06 ± 0.21

2 = 0.01 92.52 ± 0.30 90.33 ± 0.24

2 = 0.05 92.76 ± 0.33 90.25 ± 0.24

2 = 0.09 92.45 ± 0.37 90.06 ± 0.31

2 = 0.15 88.36 ± 3.12 85.97 ± 3.40

2 = 0.3 78.95 ± 4.33 62.76 ± 16.22

Table 2: The performance statistics for ResNet-32 and PlainNet-32 without or under various regularizations. For regularized models we hold 1 = e-5 and take 2 = [0.01, 0.05, 0.09, 0.15, 0.3]. The means and standard deviations reported are based on 5 samples. It can be observed that although
PlainNet-32 obtains marginal improvement for small 2, the performance drops dramatically and becomes unstable as 2 increases.

6 CONCLUSION
We give an interpretation of the deep learning dynamics by decomposing it into an signal transformation stage and feature classification stage, where we emphasis importance of the classifier w in the last fully connected layer given that the feature map F is invertible. Then we take the advantage of the fact that mutual information quantities are invariance under invertible mapping to attack our proposed information optimization problem for supervised classification in deep learning. Our theory justifies the use of direct regularization terms on w and gives guarantee on the performance of neural network with invertibility property. Our regularization improves the performance of neural networks by a noticeable margin and is capable of encouraging the interpretability of the entries of features learned in the last layer.
REFERENCES
Alessandro Achille and Stefano Soatto. On the emergence of invariance and disentangling in deep representations. CoRR, abs/1706.01350, 2017.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy. An information-theoretic analysis of deep latent-variable models. CoRR, abs/1711.00464, 2017.
Rana Ali Amjad and Bernhard C. Geiger. How (not) to train your neural network using the information bottleneck principle. CoRR, abs/1802.09766, 2018.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. CoRR, abs/1802.05296, 2018.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. J. Mach. Learn. Res., 3, March 2003.
Anthony J. Bell and Terrence J. Sejnowski. An information-maximization approach to blind separation and blind deconvolution. Neural Comput., 7(6), November 1995.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. CoRR, abs/1606.03657, 2016.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). 2006.
Alexey Dosovitskiy and Thomas Brox. Inverting convolutional networks with convolutional networks. CoRR, abs/1506.02753, 2015.
Shuyang Gao, Rob Brekelmans, Greg Ver Steeg, and Aram Galstyan. Auto-encoding total correlation explanation. CoRR, abs/1802.05822, 2018.
Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, and Honglak Lee. Towards understanding the invertibility of convolutional neural networks. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI'17, 2017.
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Proceedings of the 17th International Conference on Neural Information Processing Systems, NIPS'04, 2004.

8

Under review as a conference paper at ICLR 2019
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. CoRR, abs/1706.04599, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.
Marcus Hutter and Marco Zaffalon. Distribution of mutual information from complete and incomplete data. Computational Statistics & Data Analysis, 48(3):633­657, 2005.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015.
Jo¨rn-Henrik Jacobsen, Arnold W. M. Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks. CoRR, abs/1802.07088, 2018.
H. Kim and A. Mnih. Disentangling by Factorising. ArXiv e-prints, 2018.
Artemy Kolchinsky, Brendan D. Tracey, and David H. Wolpert. Nonlinear information bottleneck. CoRR, abs/1705.02436, 2017.
Andreas Krause, Pietro Perona, and Ryan G. Gomes. Discriminative clustering by regularized information maximization. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 775­783. 2010.
P.D. Lax. Functional analysis. Pure and applied mathematics. ISBN 9780471556046.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. CoRR, abs/1706.08947, 2017.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. Regularizing neural networks by penalizing confident output distributions. CoRR, abs/1701.06548, 2017.
Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning. In International Conference on Learning Representations, 2018.
Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. CoRR, abs/1603.05201, 2016.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. CoRR, abs/1703.00810, 2017.
Noam Slonim and Naftali Tishby. Agglomerative information bottleneck. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'99, 1999.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 2014.
D J Strouse and David J Schwab. The deterministic information bottleneck. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, UAI'16, 2016.
Shizhao Sun, Wei Chen, Liwei Wang, and Tie-Yan Liu. 2015.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. CoRR, abs/1503.02406, 2015.
9

Under review as a conference paper at ICLR 2019 Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. 1999. Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Sci-
ence. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. Yuting Zhang, Kibok Lee, and Honglak Lee. Augmenting supervised neural networks with unsupervised objectives for large-scale image classification. CoRR, abs/1606.06582, 2016. Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 2005.
10

Under review as a conference paper at ICLR 2019

A PROOF OF PROPOSITION 2.1

Proposition 2.1 establishes a connection between I(X, Y ) and the absolute value of the logits |wT F (X)| for the binary case. Intuitively, decreasing the confidence of the model on its predic-
tions will decrease the mutual information I(X; Y ).

Proposition 2.1. I(X; Y ) = I(F (X); Y ) is well estimated by its empirical version (Monte-

carlo approximation) with high probability, which shares the same unique (global) minimum with

n k=1

|wT

F

(xk )|

at

wT

F

(xk )

=

0,

for

all

k



{1,

...,

n}.

The mutual information I(X; Y ) is given as

I(X; Y ) =

p(x, y) log

X yC

p(x, y) p(x)p(y)

dx .

(7)

Apply the assumption (II), the marginal distribution of Y is uniformly distributed: p(x, y) = p(y|x) = 2p(y|x) . p(x)p(y) p(y)

(8)

Substituting (8) into (7) yields
I(X; Y ) = p(x) p(y|x) log
X yC

p(x, y) p(x)p(y)

dx

= p(x) p(y|x) log(2p(y|x))dx .
X yC

(9)

According to the Hoeffding's inequality for bounded random variables [Proposition 2.2.6, Vershynin (2018)], let M, m denote upper and lower bounds of the integrand of (9) correspondingly, we have


n

P



p(y|xk) log(2p(y|xk)) - I(X; Y )



 t



2e-

2t2 n(M -m)2

.

(10)

 k=1 yC



Equivalently, with probability at least 1 - ,

n k=1

yC p(y|xk) log(2p(y|xk)) - I(X; Y ) <

-

log(

 2

)(M

- m)2

.

n 2n

(11)

Here

n k=1

yC p(y|xk) log(2p(y|xk))/n is a Monte carlo estimation of RHS of I(X; Y ).

Recall that, for the binary case p(y|x) = p(y|F (x)) can expressed as p(y = 1|F (x)) = (wT F (x))

p(y = -1|F (x)) = 1 - (wT F (x)) .

(12)

Then we have p(y|F (xk)) log(2p(y|F (xk))) = (wT F (x)) log(2(wT F (x)))
yC
+ (1 - (wT F (x))) log(2 - 2(wT F (x)))) , which is bounded by [0, log(2)].

Take M = log(2), m = 0, we have

I(X; Y ) =

n k=1

 yC p(y|xk) log(2p(y|xk)) + O 

n



-

log(

 2

)

log(2)2



.

2n

(13) (14)

hold with probability at least 1 - .

The conclusion follows from the fact that

n k=1

yC p(y|xk) log(2p(y|xk))/n has a unique global

minimum at wT F (xk) = 0 for each xk.

11

Under review as a conference paper at ICLR 2019

B PROOF OF PROPOSITION 2.2

Consider the training samples {(xk, yk)}k=1:n, each xk is fed into a deep probabilistic model which
outputs probability densities and predicts yk, a realization of the prediction Y . Let C = {±1} be the binary class and ny be the counts of observed occurrences of k satisfying yk = y  C, then n = yC ny = y ny, where we omit the range over C for convenience.

We denote the true joint probability with yy = p(y, y), the marginal probabilities with y+ = y yy and +y = y yy. The mutual information I(Y ; Y ) can be expressed as

I(Y ; Y ) = I() = yy log
yy

yy y++y

.

(15)

Our empirical mutual information I() is defined as

where

yy

=

1 2ny

I() = yy log

yy

ny i=1



(ywT

F

(xi

)).

yy y++y

,

(16)

Proposition 2.2 establishes a connection between I(Y ; Y ) and the cross-entropy objective for the binary case. Intuitively, increasing the confidence of the model on its correct predictions will establish a more deterministic relationship between Y and Y and thus increase the mutual information I(Y ; Y ).

Proposition 2.2. I(Y ; Y ) is well estimated by I() with high probability, which shares the

same unique (global) maximum with

n k=1

log

(yk

wT

F

(xk

))

at

ykwT F (xk)





given

that

ykwT F (xk)

>

1 2

,

for

all

k



{1, ..., n}.

Proposition 2.2 follows from Proposition B.1 and Proposition B.2, where Proposition B.1 shows

that I(Y ; Y ) is well approximated by I() with high probability and Proposition B.2 shows the remaining claims in Proposition 2.2.

As shown in

Lemma

B.1,

yy

=

1 2ny

ny i=1

(ywT

F

(xi))

is

an

unbiased

estimate

of

yy .

Here



to represent the sigmoid function defined by (x) = ex/ (ex + 1). By leveraging the concentration

property of bounded variables, i.e., (ywT F (xi)), the estimation error can be bounded with high

probability (Lemma B.2).

Lemma B.1. The empirical joint probability, defined as

yy

=

1 2ny

ny
(ywT F (xi)) ,
i=1

(17)

is an unbiased estimate of the true joint distribution yy. Lemma B.2. With probability at least 1 - , we have

1 |yy| := |yy - yy|  2

log(

2 

)

min{

1 4

supx(wT F (x))2, 1}

.

2ny

Proposition B.1. With probability at least 1 - ,



I(Y ; Y ) = I() = I() - O 

log(

8 

)

min{

1 4

supx(wT F (x))2, 1} 

.

n

(18) (19)

Proof. To estimate the empirical mutual information given fixed samples, we use the approach by Hutter & Zaffalon (2005). In particular, taylor expansion gives

I() = I() + log
yy

yy y++y

yy + O(2) ,

(20)

12

Under review as a conference paper at ICLR 2019

where yy = yy -yy. Hence, Eq (20) together with Lemma B.2 yield, with probability exceeding 1 - |C|2,



I() = I() - O 

log(

2 

)

min{

1 4

supx(wT F (x))2, 1} n

.

(21)

Notice that, in the binary case the cardinality |C| = 2.

Next we prove the intermediate results, Lemmas B.1 and B.2.

Proof of Lemma B.1. Direct derivation on the true joint distribution yy gives

yy = p(y, y) = p(y, y, x)dx = p(y|y, x)p(y|x)p(x)dx
XX
= p(y|x)p(y|x)p(x)dx
X
= p(x|y)(ywT F (x))p(y)dx .
X

(22)

Given assumption (I) which states that the marginal density of Y is uniform over C, for every true

label

y



C

we

have

p(y)

=

1 2

.

We can therefore rewrite (22) as

1 yy = 2

(ywT F (x))p(x|y)dx .
X

(23)

According to assumption (II), p(x|y) is a probability density over space of signal x with true label y.

The Monte Carlo estimation of (23) gives the empirical joint probability which is unbiased:

yy

=

1 2ny

ny
(ywT F (xi)) .
i=1

(24)

Proof of Lemma B.2. Again, by leveraging the Hoeffding's inequality for bounded random variables [Proposition 2.2.6 of Vershynin (2018)], we have


ny
 P (ywT F (xky )) - EXy (ywT F (Xy))
 ky=1





 t



2e-

ny

2t2 (M -m)2

,



(25)

where Xy is the data random variable whose true label is y. Equivalently, with probability at least 1 - ,

ny ky =1

(ywT F (xky )) ny

-

EXy

(ywT F (Xy))

<

log(

2 

)(M

- m)2

,

2ny

(26)

where M, m are upper and lower bounds of random variable (ywT F (Xy)), respectively. Substitute (23) and (24) into (26), with probability at least 1 - ,

1 |yy - yy|  2

log(

2 

)(M

- m)2

.

2ny

(27)

13

Under review as a conference paper at ICLR 2019

To estimate the upper and lower bounds M, m for (ywT F (X)), we use the Taylor's theorem:

sup (ywT F (x)) = (0) + sup  (c)(ywT F (x))
xx
 1 + 1 sup |wT F (x)| 2 4x

(28)

and

inf (ywT F (x)) = (0) + inf  (c)(ywT F (x))
xx
 1 - 1 sup |wT F (x)| , 2 4x

given

that

the

derivative

of

sigmoid

function

is

bounded

by

1 4

.

It follows that

(29)

|M - m| = sup (ywT F (x)) - inf (ywT F (x))
xx
 1 sup |wT F (x)| . 2x

(30)

Also notice that M, m are the bounds for sigmoid function, so their difference is at most 1. From derivations above, we can rewrite (27) as

1 |yy - yy|  2

log(

2 

)

min{

1 4

supx(wT F (x))2, 1}

,

2ny

(31)

and the lemma follows.

Proposition B.2. The empirical mutual information I() shares the same unique (global) maximum

with

n k=1

log

(ykwT F (xk))

as

ykwT F (xk)



 given that ykwT F (xk)

>

1 2

,

for

all

k



{1, ..., n}.

Proof. The empirical information I() is defined by

I() = yy log
ij

yy y++y

,

where the empirical joint probability is given by

yy

=

1 2ny

ny
(ywT F (xi)) .
i=1

It then follows that for any y  C,

y+

=

1 2ny

ny
(wT F (Xi)) +
i=1

1 2ny

ny
(-wT F (Xi))
i=1

=

1 2ny

ny
((wT F (Xi)) + (-wT F (Xi)))
i=1

1 ny

1

= 1= .

2ny i=1

2

In binary case it means that 1
1(-1) = 2 - 11 ,

1 (-1)1 = 2 - (-1)(-1)

(32) (33)
(34) (35)

14

Under review as a conference paper at ICLR 2019

and the empirical mutual information can decomposed as

I() = 11 log

11

11

+

1 2

-

(-1)(-1)

+

1 2

-

11

log

1 2

-

11

1 2

-

11

+

(-1)(-1)

+

1 2

-

(-1)(-1)

log

1 2

-

(-1)(-1)

11

+

1 2

-

(-1)(-1)

+ (-1)(-1) log

(-1)(-1)

1 2

-

11

+

(-1)(-1)

+ log(2) . (36)

We differentiate (36) with respect to 11 and (-1)(-1), and calculate the critical points over the

domain

[0,

1 2

]

for

both

variables,

which

gives

1 11 = (-1)(-1) = 4 .

(37)

Observe

that

(37)

is

a

global

minimum

over

[0,

1 2

]

×

[0,

1 2

].

Since

this is the

unique

critical point

where the derivative vanishes, the global maximums can only be obtained on the boundaries. In

particular,

if

we

restrict

(11,

(-1)(-1))

on

[

1 4

,

1 2

]

×

[

1 4

,

1 2

],

(36)

is

a

strictly

increasing

function

over

11, (-1)(-1) and the unique global maximum is obtained at

1 11 = (-1)(-1) = 2 .

(38)

The proposition follows from the definition (33) of yy that (38) is only approached when ykwT F (xk)  , for all k  {1, ..., n}.

C INVERTIBILITY IS BENEFICIAL

We show in Proposition C.1 that the lower bound for the classification error is itself lower bounded by a constant, which is attained if F is invertible. Although the performance of the model also depends on the classifier w, our bound claims that an invertible feature map F could provide a better environment for the classifier w to perform well. Intuitively, invertibility preserves the information of the signal X as it flows through the neural network and reaches the classifier w; on the other hand, w potentially performs better on the input that preserves all information of the data compared to the one that doesn't.
Proposition C.1. (Fano's Inequality) The classification error is lower bounded as follows:

P(Y

=

Y)



H(Y |F (X)) - log(2) log(|C| - 1)

.

(39)

The lower bound satisfies

H(Y |F (X)) - log(2)  H(Y |X) - log(2)

log(|C| - 1)

log(|C| - 1)

(40)

for all F , and the equality is attained if F is invertible.

Let Z = F (X) and the machinery of deep learning can be decribed by the following Markov Chain:

Y XZY .

(41)

Lemma C.1 is a technical result that helps to prove Proposition C.1. The information Z = F (X) has about the true labels Y is maximized when F is invertible, which is beneficial in the sense that the key information influential for classification can be well preserved.
Lemma C.1 (Chain Rule). Given the Markov Chain assumption equation 41, we have

I(Y ; Y )  I(Y ; Z)  I(Y ; X) , and the second equality is attained if F is invertible.

(42)

15

Under review as a conference paper at ICLR 2019

Proof. We will only prove the second inequality and the first inequality follows by a similar argument. Consider the decomposition

p(x, y, z)

I(Y ; X, Z) =

p(x, y, z) log

dxdz

Xy Z

p(y)p(x, z)

=
Xy

Z

p(x|y,

z)p(y,

z)

log

p(x|y, z)p(y, z) p(y)p(x|z)p(z) dxdz

p(x|y, z)

= I(Y ; Z) +
Xy

p(x|y, z)p(y, z) log
Z

p(x|z)

dxdz

= I(Y ; Z) +

p(x, y|z)p(z) log p(x, y|z) dxdz

Xy Z

p(x|z)p(y|z)

= I(Y ; Z) + I(X; Y |Z) .

(43)

Similarly we obtain

I(Y ; X, Z) = I(X; Y ) + I(Y ; Z|X) .

(44)

equation 43 together with equation 44 yields

I(Y ; Z) + I(X; Y |Z) = I(X; Y ) + I(Y ; Z|X) .

(45)

According to the Markov Chain setting, Y and Z are conditionally independent given X, hence I(Y ; Z|X) = 0; in addition, the mutual information I(X; Y |Z) is nonnegative. It follows from (45)
that

I(Y ; Z)  I(Y ; X) .

(46)

Next we present a lower bound for the classification error. This lower bound is negatively related to the mutual information I(Y ; F (X)), and it attains its minimum if F is invertible. Although the performance also depends on the classifier w, Proposition C.1 implies that an invertible feature map F allows more chances for the classifier w to perform well.

Proof of Proposition C.1. Consider the random variable E defined as: E = 1, if Y = Y 0, otherwise .

(47)

By the Chain Rule following from similar arguments presented in Lemma C.1, we have H(E, Y |Y ) = H(Y |Y ) + H(E|Y, Y ) H(E, Y |Y ) = H(E|Y ) + H(Y |E, Y ) .

(48)

Note that H(E|Y, Y ) = 0, since the value of E is determined given the knowledge of Y, Y . It then follows that

H(Y |Y ) = H(E|Y ) + H(Y |E, Y )

 log(2) + H(Y |E = 0, Y )P(E = 0) + H(Y |E = 1, Y )P(E = 1)

= log(2) + H(Y |E = 1, Y )P(Y = Y )

 log(2) + log(|C| - 1)P(Y = Y ) .

(49)

On the other hand, Lemma C.1 shows that H(Y ) - H(Y |Y ) = I(Y ; Y )  I(Z; Y ) = H(Y ) - H(Y |Z) ,

(50)

16

Under review as a conference paper at ICLR 2019

which gives

H(Y |Z)  H(Y |Y ) .

Substitute it into (49) yields the result H(Y |Z)  log(2) + log(|C| - 1)P(Y = Y ) .

As for the second statement, Lemma C.1 shows that H(Y ) - H(Y |Z) = I(Y ; Z)  I(Y ; X) = H(Y ) - H(Y |X) .

It then follows that, H(Y |Z) = H(Y |F (X))  H(Y |X) = 0 ,
where the equality is attained if F is invertible.

(51) (52) (53) (54)

D INVERTIBILITY OF RESNET

ResNet is designed to allow the model to "learn" the identity map easily. Specifically, the input vector x and output vector y of a building block are related by

y = L(x) + x = (L + I)(x) ,

(55)

where the operator L could be a composition of activation functions, convolutions, drop-
out(Srivastava et al. (2014)) and batch normalization(Ioffe & Szegedy (2015)). It's shown in Lemma D.1 below that if the operator norm |L| < 1, then L + I is theoretically guaranteed to
have an inverse, which enables information preservation among intermediate layers. In Figure 5 we experimentally verify that |L| < 1 for all building blocks during the training process. In general,
operations such as ReLU, pooling, drop-out are not invertible(Dosovitskiy & Brox, 2015); it's chal-
lenging to build a strictly invertible network (Jacobsen et al., 2018). From this point of view, the beauty of ResNet lies in the fact that it's guaranteed to be invertible regardless how L evolves during the training process, as long as |L| < 1.

Although the usual design of ResNet does involve non-invertible components such as pooling, we argue that ResNet still has descent invertible property compared to the majority of other neural network designs. We also experimentally verify that our regularization does not improve a very deep ResNet on its performance by a clear margin; we speculate that information will lose more as it goes deeper.

Lemma D.1. Consider Holder Space C0,1(U ), where U is the closure of some bounded open set U ,

with equiped Holder norm |L| =  supxU |L(x)| + supx,yU,x=y

|L(x)-L(y)| |x-y|

, here  is some

positive scalar. If L  C0,1(U ) and |L| < 1, then there exists B such that B(I +L) = (I +L)B = I.

Proof. It's well known that C0,1(U ) is a Banach space (Lax).

Define


B = (-L)n .
n=0

(56)

Since |L| < 1, (56) is a Cauchy sequence and coverges in Banach space. Convergence sequences can be multiplied termwise, it follows that


BL = L (-L)n = - (-L)n = -(B - I) .

n=0

n=1

(57)

So B(I + L) = I. The other equality (I + L)B = I can be shown to hold in the same fashion.

17

Under review as a conference paper at ICLR 2019
Figure 5: We measure the operator norm of L in each building block of ResNet-32 over 80k training steps. It can be observed that, the operator norms are all bounded by 1 throughout the training process, which verifies the hypothesis made in Appendix D. We conlude that ResNet is invertible.
E IMPLEMENTATION DETAILS
We implement all models using Tensorflow. We modify the ResNet based on the code provided at https://github.com/tensorflow/models/tree/master/research/resnet. We follow the same learning rate scheme proposed in the original code. For the InvNet on MNIST, we train the network with initial learning rate 0.1, and decay it by 0.7 every 10 epochs. For both InvNet and ResNet, we apply the 1 norm regularization every 30 iterations.
18

