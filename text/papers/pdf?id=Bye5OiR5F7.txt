Under review as a conference paper at ICLR 2019
WASSERSTEIN PROXIMAL OF GANS
Anonymous authors Paper under double-blind review
ABSTRACT
We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. This approach is based on the gradient operator induced by optimal transport theory, which connects the geometry of the sample space and the parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wallclock time and Fre´chet Inception Distance (FID) learning curves.
1 INTRODUCTION
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are a powerful approach to learning generative models. Here, a discriminator tries to tell apart the data generated from a real source and the data generated by a generator, whereas the generator tries to fool the discriminator. This adversarial game is formulated as an optimization problem over an implicit generative model for the generator. An implicit generative model is a parametrized family of functions mapping a noise source to sample space. In trying to fool the discriminator, the generator should try to recreate the density distribution from the real source.
The problem of matching a target density can be formulated as the minimization of a discrepancy measure. The Kullback­Leibler (KL) divergence is known to be difficult when the distributions have a low dimensional support set, as is commonly the case in applications with structured data and high dimensional sample spaces. An alternative approach to define a discrepancy measure between densities is optimal transport, a.k.a. Wasserstein distance, or Earth Mover's distance. This has been used recently to define the loss function for learning generative models (Montavon et al., 2016; Frogner et al., 2015). In particular, the Wasserstein GAN (Arjovsky et al., 2017) has attracted much interest in recent years.
Besides defining the loss function, optimal transport can also be used to introduce structures serving the optimization itself, in terms of the gradient operator. In full probability space, this is known as the Wasserstein steepest descent flow (Jordan et al., 1998; Otto, 2001). In this paper we derive the Wasserstein steepest descent flow for deep generative models in GANs. We use the Wasserstein2 metric function, which allows us to obtain a Riemannian structure and a corresponding natural (i.e., Riemannian) gradient. A well known example of a natural gradient is the Fisher-Rao natural gradient, which is induced by the KL divergence. In learning problems, one often finds that the natural gradients can offer advantages compared to the Euclidean gradient (Amari, 1998; 2016). In GANs, because of the low dimensional support sets and the associated difficulties with the KL divergence, the Fisher-Rao natural gradient is problematic. Therefore, we propose to use the gradient operator induced by the Wasserstein-2 metric (Li & Montu´far, 2018a;b).
We compute the proximal operator for the generators of GANs, where the regularization is the squared constrained Wasserstein-2 distance. In practice, the constrained distance can be approximated by a simple neural network. In implicit generative models, the constrained Wasserstein-2 metric exhibits a simple structure. We generalize the metric and introduce the relaxed proximal operator for generators, which allows us to further simplify the computation. The resulting relaxed proximal operator involves only the difference of outputs, so that the proximal computation has very simple parameter updates. The method can be easily implemented and used as a drop-in regularizer for the generator updates.
1

Under review as a conference paper at ICLR 2019

This paper is organized as follows. In Section 2, we briefly introduce the Wasserstein natural gradient. A Wasserstein proximal method is introduced in Algorithm 1. In Section 3, we demonstrate the effectiveness of the proposed methods in experiments with various types of GANs. Section 4 reviews related work.

2 WASSERSTEIN PROXIMAL

In this section, we briefly present optimal transport and its proximal operator on a parameter space. We then apply them to the optimization problems of GANs.

2.1 WASSERSTEIN NATURAL GRADIENT

Optimal transportation defines a class of distance functions between probability densities. Given a pair 0, 1  Pp(Rn) of probability densities with finite p-th moment,

Wp(0, 1)p = inf

x - y p(x, y)dxdy,

Rn ×Rn

(1)

where the infimum is over all joint probability densities (x, y) with marginals 0(x), 1(y). In the literature (see Villani, 2009), Wp is referred to as the Wasserstein-p distance. In this paper, we focus on the case p = 2, and further denote W2 by W .

Following Benamou & Brenier (2000), the Wasserstein-2 distance has a dynamical formulation as a trajectory transporting the initial density 0 to the final density 1 along a trajectory of minimal
kinetic energy. The classic theory does not consider the setting where the density path is constrained
to lie within a parametrized model. In the following we extend the classic theory to cover parameterized density models. Consider a parameterized probability (, x), with parameter space   Rd. Suppose that (, x) is locally injective as a mapping from  to P2(Rn). Then the Wasserstein-2
metric function constrained to the parameter space is given as follows (see Li & Montu´far, 2018a).

Theorem 1 (Constrained Wasserstein-2 metric) The constrained Wasserstein-2 metric function dW :  ×   R+ has the following formulation:

dW (0, 1)2 = inf

1
(t, x) 2((t), x)dxdt :
0 Rn

t((t), x) +  · (((t), x)(t, x)) = 0, (0) = 0, (1) = 1 ,

where the infimum is among all feasible Borel potential functions  : [0, 1] × Rn  R and continuous parameter paths  : [0, 1]  Rd. Here · and  are the divergence and gradient operators over Rn.

We note that the constrained metric on parameter space can be different from the Wasserstein-2 distance on the full density set. The metric dW can be used to define a steepest descent optimization scheme. This can be formulated in two general ways.
One way is in terms of the corresponding Riemannian structure, i.e., an inner product between tangent vectors. A well known example is the Fisher natural gradient (Amari, 1998; 2016). The constrained Wasserstein-2 metric allows us to obtain a Riemannian metric structure, from which we obtain the following constrained Wasserstein-2 gradient. We also call it Wasserstein natural gradient.

Theorem 2 (Wasserstein natural gradient) Given a loss function F :   R, the Wasserstein
gradient operator is given by W F () = G()-1F (),
where G() = (G()ij)1i,jd  Rd×d is given by

G()ij = i(x)j(x)(, x)dx.
Rn

Here

for

each

i



{1, · · ·

, d},

i :

Rn



R

is

a

solution

(up

to

additive

constants)

of

 i

(,

x)

+

 · ((, x)i(x)) = 0.

2

Under review as a conference paper at ICLR 2019

Here W represents the natural gradient operator with respect to the constrained Wasserstein metric,  represents the ordinary Euclidean gradient operator, and G is the matrix representing the Wasserstein Riemannian metric. The steepest descent flow is given by

d dt

(t)

=

-G((t))-1  F

((t)).

(2)

The corresponding gradient descent iteration (forward Euler method) satisfies

k+1 = k - hG(k)-1F (k),

where h > 0 is the step size. Often in practice, the computation of matrix G()-1 is difficult.

The second way of obtaining a numerical scheme for equation 2 is in terms of the proximal operator.

This is the backward Euler method, also named Jordan-Kinderlehrer-Otto (JKO) scheme (Jordan

et al., 1998), which is given by

k+1

= arg

min


F ()

+

1 2h

dW

(,

k )2 .

(3)

Here, at each step, the distance of the parameter update acts as a regularization to the original loss

function.

Computing dW is also often challenging. However, we can approximate the dW distance locally by a second order Taylor expansion. This approximation is particularly tractable within the parameterized
setting that we discussed above.

This allows us to derive other first order schemes, such as the Semi-Backward Euler method:

Proposition 3 (Semi-Backward Euler method) The Semi-Backward Euler method for the gradi-

ent flow of loss function F :   R is given by

k+1 = arg min F () + 1 sup (x)((, x) - (k, x)) - 1 ((x))2(k, x)dx,



h  Rn

2

where the supremum is taken over  : Rn  R with sufficient regularity for the integral to be well

defined.

The Semi-Backward Euler method is often easier to approximate than the forward Euler method,
because it does not require computing and inverting G(), and it is often simpler than the backward Euler method (JKO), because the constrained optimization over  is more tractable than the timedependent constraint involved in computing dW .

We implement the Semi-Backward Euler method in implicit generative model as follows. For each parameter   Rd, let the generator be given by g : Rm  Rn; z  x = g(, z). This takes an input noise prior Z  p(z)  P2(Rm) to an output sample with density given by X = g(, Z)  (, x). Here Rd is the parameter space, Rm is the latent space, and Rn is the sample space.

In this case, the update in Proposition 3 forms

k+1

=

arg min sup
 

F ()

+

1 h

EZp(z)[(g(,

Z

))

-

(g(k, Z))

-

1 2

x(g(k, Z))

2].

In practice, we apply a neural network to approximate variable . See details in Appendix G.

2.2 REGULARIZATION ON GENERATORS

In fact, the constrained Wasserstein-2 metric in implicit generative models allows for yet a simpler formulation. This reformulation allows us to define the relaxed Wasserstein metric, and further introduces a simple algorithm for proximal operator on generators.

Proposition 4 (Constrained Wasserstein-2 metric in implicit generative models)

dW (0, 1)2 = inf

1
EZp(z)
0

d dt

g((t),

Z

)

2dt :

d dt

g((t),

Z

)

-

x(t, g((t), Z))

=

0,

(0)

=

0,

(1)

=

1

,

where the infimum is among all feasible Borel potential functions  : [0, 1] × Rn  R and continu-

ous parameter paths  : [0, 1]  Rd.

3

Under review as a conference paper at ICLR 2019

Here the constrained Wasserstein metric requires that the derivative of the generator g w.r.t.   Rd be a gradient vector field of  w.r.t x  Rn. In other words, if we denote x(t) = g((t), z), then

d dt

x(t)

=

x(t,

x(t)).

(Gradient constraint)

The gradient constraint is satisfied if the sample space is 1 dimensional, i.e., n = 1. In general, this is not true. Here (t, x) is the other function depending on the parameter space . Finding  involves computational difficulties. Fitting the gradient constraint is an open problem for the computations
of Wasserstein proximal operator.

For simple computations, we withdraw the gradient constraint and consider a relaxed Wassersetin metric on parameter space:

d(0, 1)2 = inf
(t)

1
EZp(z)
0

d dt

g((t),

Z)

2dt :

(0) = 0,

(1) = 1

.

We approximate the relaxed Wasserstein proximal operator based on the new metric d to obtain

k+1

=

arg

min


F ()

+

1 2h EZp(z)

g(, Z) - g(k, Z)

2,

(4)

where the infimum is among all feasible continuous parameter path  : [0, 1]  Rd.

In fact, when the sample space is high dimensional, i.e., n > 1, the above update is not exactly the Wasserstein proximal. Instead, it simply regularizes the generator by the expectation of squared difference in sample space.

Algorithm 1 Relaxed Wasserstein Proximal, where F is a parameterized function to minimize

Require: F, a parameterized function to minimize (e.g. Wasserstein-1 with a parameterized discriminator). g the generator.
Require: h proximal step-size, B batch size.

Require: OptimizerF and Optimizerg . Require: max iterations, and generator iterations

for k = 0 to max iterations do

Sample real data {xi}Bi=1 and latent data {zi}iB=1

k  OptimizerF

1 B

B i=1

F

(g

(zi

))

for = 0 to generator iterations do

Sample latent data {zi}iB=1

k  Optimizerg

1 B

B i=1

F (g (zi ))

+

1 h

g(zi) - gk-1 (zi)

2

end for

end for

2.3 ILLUSTRATION OF WASSERSTEIN PROXIMAL

We present a toy example to illustrate the effectiveness of Wasserstein proximal operator in GANs.

Consider a family of distribution with two weighted delta measures. Let  = { = (a, b) : a < 0, b > 0}, and define
(, x) = a(x) + (1 - )b(x),
where   [0, 1] is a given ratio and a(x) is the delta function supported at point a. See Figure 2.3.
In this model, for a loss function F :   R, the proximal regularization is given as follows:

k+1

=

arg

min


F ()

+

1 2h

d(,

k

)2

,

where  = (a, b) and k = (ak, bk). We check the following commonly used statistical distance (divergence) functions d between parameters  and k.

4

Under review as a conference paper at ICLR 2019

 a

a

1- 0 b b

b

a*=-3.2, b *=0.9, =0.045 5 4 3 2 1 0 -1 -2 -3 -4 -5 -5 -4 -3 -2 -1 0 1 2
a

FW1 Wasserstein-2 Euclidean

6 5

4

3

2

1

345

Figure 1: Illustration of the example from Section 2.3. The Wasserstein proximal penalizes parameter steps in proportion to the mass being transported, which results in updates pointing towards the minimum of the loss function. The Euclidean proximal penalizes all parameters equally, which results in updates naively orthogonal to the level sets of the loss function.

1. Wasserstein-2 distance:

dW (, k)2 = (a - ak)2 + (1 - )(b - bk)2;

2. Euclidean distance:

dE(, k)2 = (a - ak)2 + (b - bk)2;

3. Kullback­Leibler divergence:

dKL( k ) =

Rn

(,

x)

log

(, x) (k, x)

dx

=

;

4. L2-distance:

dL2 (, k )2 = |(, x) - (k, x)|2dx = .
Rn
Here the KL divergence and L2-distance cannot measure the difference of probability models. The Wasserstein-2 and Euclidean distances still work in these cases. In addition, the Euclidean distance dE does not depend on the structure of model (, x), while the constrained Wasserstein-2 metric dW does.

Proposition 5 Given  = (a, b)  , consider the Wasserstein-1 metric as the loss function,

i.e., FW1 () = W1(,  ) = |a - a| + (1 - )|b - b|.

Denote

Wk+1

=

arg

min

FW1 ()

+

1 2h

dW

(,

k )2 ,

and

Ek+1

=

arg

min

FW1 ()

+

1 2h

dE

(,

k )2 .

For each stepsize h > 0, then

FW1 (Ek+1)  FW1 (Wk+1).

On each step of the update, the solution obtained by Wasserstein proximal decreases the objective function further than the one by Euclidean proximal. Here the proof is based on a simple fact of the shrinkage operator, see details in Appendix B.
This example introduces a case that Wasserstein-2 proximal works better than Euclidean proximal for the Wasserstein-1 loss function.

3 EXPERIMENTS ON GANS
Here we present numerical experiments using the Relaxed Wasserstein Proximal (RWP) algorithm and the Semi-Backward Euler (SBE) method in order to perform Wasserstein gradient-descent on various GANs. We find that the Relaxed Wasserstein Proximal provides both better speed (measured by wallclock) and stability in training GANs.

5

Under review as a conference paper at ICLR 2019

3.1 RESULTS OF RELAXED WASSERSTEIN PROXIMAL

The Relaxed Wasserstein Proximal (RWP) algorithm is intended to be an easy-to-implement, drop-in replacement to improve speed and convergence of GAN training. It does this by applying regularization on the generator during training. This is novel as most GAN training focuses on regularizing the discriminator, e.g. with a gradient penalty (Gulrajani et al., 2017b; Petzka et al., 2017; Kodali et al., 2018; Adler & Lunz, 2018; Miyato et al., 2018), and there has been limited exploration in regularizing the generator (Chen et al., 2016). Specifically, we modify the update rule for the generator by:

· Update for number of iterations before updating the discriminator:

  Optimizer

Original

loss

+

1 2h

g - gk-1

2

So two hyperparameters are introduced: the proximal step-size h, and the number of iterations . In some GANs, one may update the discriminator a number of times and then update the generator a number of times, and then repeat; we will call one loop of this update an outer-iteration.
A more detailed description of the algorithm is given in Appendix D.
We test the Relaxed Wassersteing Proximal regularization on three GAN types:

· Standard GANs (Goodfellow et al., 2014), · WGAN-GP (Gulrajani et al., 2017a), and · DRAGAN (Kodali et al., 2018).

We use the CIFAR-10 dataset (Krizhevsky, 2009), and the aligned and cropped CelebA dataset (Liu et al., 2015). And we utilize the DCGAN (Radford et al., 2015) architecture for the discriminator and generator. To measure the quality of generated samples, we employ the Fre´chet Inception Distance (FID) (Heusel et al., 2017) both to measure performance and to measure convergence of GAN training (lower FID is better); we used 10,000 generated images to measure the FID. For CIFAR-10, we measure the FID every 1000 outer-iterations, and for CelebA we measure the FID every 10,000 outer-iterations.
Our particular hyperparameter choices for training are given in Appendix C. Note that since we intend RWP to be a drop-in regularization, the non-RWP hyperparameters (i.e. not h nor ) are chosen to work well before applying RWP.
To summarize our results, the Relaxed Wasserstein Proximal regularization improves both the speed (wallclock) and stability of convergence. It is a tricky to compare the result of using RWP, as it performs multiple generator iterations. We thus align the comparison according to wallclock time (this procedure was also used by Heusel et al., 2017). In Figure 2 we see that our regularization improves convergence speed (measured in wallclock time), and also obtains a lower FID for all GAN types. In particular, in DRAGAN we see a 20% improvement in sample quality according to the FID. The same results are also found for the CelebA dataset, shown in Figure 3. We note that multiple generator iterations will sometimes prevent Standard GANs on CelebA from learning initially, at which point we restart the algorithm, and once it starts learning then the run is successful. This is practically very easy to detect and provides minimal trouble, so Figure 3 focuses on successful runs. We predict this defect will be rectified with a more stable loss function, such as WGAN-GP, or with different h's and 's.
We also examine the effect of multiple generator updates compared to discriminator updates. More specifically, in RWP since we update the generator multiple times before updating the discriminator, then it is worth examining the effect of not using the regularization. We see in Figure 4 that even using the most stable GAN type out of the three ­ WGAN-GP ­ if we omit regularization then the FID has high variance and even tends to rise in the end. But with RWP, the FID converges with more stability and achieves a lower FID.
Samples from the models are provided in Appendix E. We also performed latent space walks (Radford et al., 2015) to show RWP regularization does not cause the GAN to memorize. For details see Appendix F.

6

Under review as a conference paper at ICLR 2019
Figure 2: The effect of using RWP regularization, on the CIFAR-10 dataset. The experiments are averaged over 5 runs. The bold lines are the average, and the enveloping lines are the minimum and maximum. From the three graphs, we see that using the easy-to-implement RWP regularization improves speed as measured by wallclock time, and it also is able to achieve a lower FID.
Figure 3: The effect of Relaxed Figure 4: An experiment demon- Figure 5: The effect of the Wasserstein Proximal (RWP) strating the effect of perform- Semi-Backward Euler (SBE) regularization on Standard ing 10 generator iterations per method, on the CIFAR-10 GANs, on the CelebA dataset. outer-iteration with and without dataset. As we observe, the The experiment was averaged RWP, where an outer-iteration training is comparable to the over 5 runs. The bold lines are is a single loop of: a num- standard way of training using the average, and the envelop- ber of discriminator iterations, the WGAN-GP loss. The ing lines are the minium and then a number of generator it- experiment was averaged over maximum. Here we see RWP erations. This experiment goes 5 runs. The bold lines is the regularization improves the to 1,000,000 outer-iterations to average, and the enveloping speed (via wallclock time), and show long-term behavior. With lines are the minimum and achieves a lower FID. We note RWP regularization we obtain maximum. multiple generator iterations convergence, as well as lower might cause initial learning FID. Without RWP, the training to fail, but once it starts then is highly variable and the FID is it remains successful. This is even on a rising trend in the end. practically easy to detect, so we show successful runs.
7

Under review as a conference paper at ICLR 2019
3.2 RESULTS OF SEMI-BACKWARD EULER METHOD
The training of Semi-Backward Euler (SBE) is a more complicated. Here we attempt to approximate three functions: the usual discriminator and generator, and the potential function p. The algorithm and particular hyperparameter settings are presented in the appendix in Section G. We present our attempts at optimizing over the three networks in Figure 5. Since both the standard WGAN-GP and the SBE on WGAN-GP had the same generator iterations, then we align according to this. As we see, the Semi-Backward Euler method is comparable to norm WGAN-GP. We leave deeper investigation of the Semi-Backward Euler method for future work.
4 RELATED WORKS
In the literature, many different aspects of optimal transport have been applied into machine learning and GANs.
1. Loss function. Many studies apply the Wasserstein distance as the loss function. There are mainly two reasons for using the Wasserstein loss function (Frogner et al., 2015; Montavon et al., 2016). On the one hand, the Wasserstein distance is a statistical distance depending on the metric of the sample space. So it introduces a statistical estimator, named the the minimal Wasserstein estimator (Bassetti et al., 2006), depending on the geometry of the data. On the other hand, the Wasserstein distance is useful for comparing probability distributions supported on lower dimensional sets. This is often intractable for other divergence functions. In GANs, these properties have been leveraged in Wasserstein GAN (Arjovsky et al., 2017). In this case, the loss function is chosen as the Wasserstein-1 distance function. In its computations, the discriminator, also called the Kantorovich dual variable, needs to satisfy the 1-Lipschitz condition. Many studies work on the regularization of the discriminator in order to satisfy this condition (Gulrajani et al., 2017b; Petzka et al., 2017).
2. Gradient flows in full probability set. The Wasserstein-2 metric provides a metric tensor structure (Lott, 2007; Otto, 2001; Li, 2018), under which the probability space forms an infinite dimensional Riemannian manifold, named the density manifold (Lafferty, 1988). The gradient flow in the density manifold links with many transport-related partial differential equations (Villani, 2009; Nelson, 1985). A famous example is that the Fokker-Planck equation, the probability transition equation of Langevin dynamics, is the gradient flow of the KL divergence function. In this perspective, two angles have been developed in the learning communities. Firstly, many groups try to leverage the gradient flow structure in probability space supported on the parameter space. They study the stochastic gradient descent by the transition equation in the probability over parameters (Mei et al., 2018). Secondly, many nonparametric models have been studied, such as the Stein gradient descent method (Liu, 2017). It can be viewed as the generalization of Wasserstein gradient flow. In addition, Frogner & Poggio (2018) consider an approximate inference method for computing Wasserstein gradient flow in full probability set. Here an approximation towards Kantorovich dual variables is introduced.
3. Gradient flow constrained on parameter space. The Wasserstein structure can also be constrained on parameter space. Carlen & Gangbo (2003) studied the constrained Wasserstein gradient with fixed mean and variance. Here the density subset is still infinite dimensional. Many approaches also focus on Gaussian families or elliptical distributions (Takatsu, 2011). The Wasserstein gradient flow in Gaussian family has been studied by Malago` et al. (2018).
Compared to previous works, our approach applies the Wasserstein gradient to work on general implicit generative models.
5 DISCUSSION
In this work, we apply the constrained Wasserstein gradient and its relaxations on implicit generative models. Whereas much work has focused on regularizing the discriminator, in this work we focus on regularizing the generator. For Wasserstein GAN (with gradient penalty), we compute the Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space. Experimentally, the proposed method allows us to obtain a better minimizer in the sense of FID, with faster convergence speeds in wall-clock time.
8

Under review as a conference paper at ICLR 2019
REFERENCES
J. Adler and S. Lunz. Banach Wasserstein GAN. ArXiv e-prints, June 2018.
S Amari. Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2):251­276, 1998.
S Amari. Information Geometry and Its Applications. Number volume 194 in Applied mathematical sciences. Springer, Japan, 2016.
Luigi Ambrosio, Nicola Gigli, and Savare´ Giuseppe. Gradient Flows: In Metric Spaces and in the Space of Probability Measures. Birkha¨user Basel, Basel, 2005.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein GAN. arXiv:1701.07875 [cs, stat], 2017.
Federico Bassetti, Antonella Bodini, and Eugenio Regazzini. On minimum kantorovich distance estimators. Statistics & Probability Letters, 76(12):1298 ­ 1302, 2006. ISSN 0167-7152. doi: ht tps://doi.org/10.1016/j.spl.2006.02.001. URL http://www.sciencedirect.com/scie nce/article/pii/S0167715206000381.
Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the MongeKantorovich mass transfer problem. Numerische Mathematik, 84(3):375­393, 2000.
E. A. Carlen and W. Gangbo. Constrained Steepest Descent in the 2-Wasserstein Metric. Annals of Mathematics, 157(3):807­846, 2003.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 2172­2180. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6399-infogan-interpretable-represent ation-learning-by-information-maximizing-generative-adversarial -nets.pdf.
C. Frogner and T. Poggio. Approximate inference with Wasserstein gradient flows. ArXiv e-prints, June 2018.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio. Learning with a Wasserstein Loss. arXiv:1506.05439 [cs, stat], 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672­2680. Curran Associates, Inc., 2014. URL http://papers.n ips.cc/paper/5423-generative-adversarial-nets.pdf.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5767­5777. Curran Associates, Inc., 2017a. URL http://papers.nips.cc/paper /7159-improved-training-of-wasserstein-gans.pdf.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5767­5777. Curran Associates, Inc., 2017b. URL http://papers.nips.cc/paper /7159-improved-training-of-wasserstein-gans.pdf.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6626­6637. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7240-gans-trained-by-a-two-t ime-scale-update-rule-converge-to-a-local-nash-equilibrium.pdf.
9

Under review as a conference paper at ICLR 2019
Richard Jordan, David Kinderlehrer, and Felix Otto. The Variational Formulation of the Fokker­ Planck Equation. SIAM Journal on Mathematical Analysis, 29(1):1­17, 1998.
Naveen Kodali, James Hays, Jacob Abernethy, and Zsolt Kira. On convergence and stability of GANs, 2018. URL https://openreview.net/forum?id=ryepFJbA-.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
John D. Lafferty. The density manifold and configuration space quantization. Transactions of the American Mathematical Society, 305(2):699­741, 1988.
J. Lei Ba, J. R. Kiros, and G. E. Hinton. Layer Normalization. ArXiv e-prints, July 2016.
Wuchen Li. Geometry of probability simplex via optimal transport. arXiv:1803.06360 [math], 2018.
Wuchen Li and Guido Montu´far. Natural gradient via optimal transport. arXiv:1803.07033 [cs, math], 2018a.
Wuchen Li and Guido Montu´far. Ricci curvature for parametric statistics via optimal transport. arXiv:1807.07095 [cs, math, stat], 2018b.
Wuchen Li and Stanley Osher. Constrained dynamical optimal transport and its Lagrangian formulation. arXiv:1807.00937 [math], 2018.
Qiang Liu. Stein Variational Gradient Descent as Gradient Flow. arXiv:1704.07520 [stat], 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
John Lott. Some Geometric Calculations on Wasserstein Space. Communications in Mathematical Physics, 277(2):423­437, 2007.
Luigi Malago`, Luigi Montrucchio, and Giovanni Pistone. Wasserstein Riemannian Geometry of Positive Definite Matrices. arXiv:1801.09269 [math, stat], 2018.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665­E7671, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1QRgziT-.
Gre´goire Montavon, Klaus-Robert Mu¨ller, and Marco Cuturi. Wasserstein Training of Restricted Boltzmann Machines. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3718­3726. Curran Associates, Inc., 2016.
Edward Nelson. Quantum Fluctuations. Princeton series in physics. Princeton University Press, Princeton, N.J, 1985.
Felix Otto. The geometry of dissipative evolution equations the porous medium equation. Communications in Partial Differential Equations, 26(1-2):101­174, 2001.
Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of Wasserstein GANs. arXiv:1709.08894 [cs, stat], 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015. URL http://ar xiv.org/abs/1511.06434.
Asuka Takatsu. Wasserstein geometry of Gaussian measures. Osaka Journal of Mathematics, 48 (4):1005­1026, 2011.
Ce´dric Villani. Optimal Transport: Old and New. Number 338 in Grundlehren der mathematischen Wissenschaften. Springer, Berlin, 2009.
10

Under review as a conference paper at ICLR 2019

APPENDIX

A REVIEW OF WASSERSTEIN STATISTICAL MANIFOLD

In the full probability set, we consider a metric function W2 : P2(Rn) × P2(Rn)  R+,

W2(0, 1)2 = inf
t

1
(t, x) 2(t, x)dxdt :
0 Rn

t(t, x) +  · ((t, x)(t, x)) = 0, (0, x) = 0(x), (1, x) = 1(x) ,

(5)

where the infimum is taken among all feasible Borel potential functions  : [0, 1] × Rn  R and continuous density path  : [0, 1] × Rn  R+ satisfying the continuity equation.

The variational formulation in equation 5 introduces a Riemannian structure in density space. Consider the set of smooth and strictly positive probability densities

P+ =   C(Rn) : (x) > 0,

(x)dx = 1  P2(Rn).

Rn

Denote F := C(Rn) the set of smooth real valued functions. The tangent space of P+ is given by

TP+ =   F : (x)dx = 0 .
Rn
Given   F and   P+, define
V(x) := - · ((x)(x)).
Thus V  TP+. The elliptic operator ·() identifies the function  modulo additive constants with the tangent vector V of the space of densities.
Given   P+, i  TP+, i = 1, 2, define

gW (1, 2) = (1(x), 2(x))(x)dx,
Rn
where i(x)  F /R, such that - · (i) = i.

The inner product gW endows P+ with a Riemannian metric tensor. In other words, the variational problem equation 5 is a geometric action energy in (P+, gW ).

Given a loss function F : P+  R, the Wasserstein gradient operator in (P+, gW ) is given as

follows.

gradW

F

()

=

-

·

(

 (x)

F

()).

Thus the gradient flow satisfies

 t

=

-gradW F ()

=

·

(

 (x)

F

()).

More analytical results on the Wasserstein-2 gradient flow are provided in Ambrosio et al. (2005).

We next consider Wasserstein-2 metric and gradient operator constrained on statistical models. A statistical model is defined by a triplet (, Rn, ). For simple presentation of paper, we assume   Rd and  :   P(Rn) is a parameterization function. In this case, ()  P(Rn). We
assume that the parameterization map  is locally injective and under suitable regularities. We define a Riemannian metric g on () by pulling back the Wasserstein-2 metric tensor gW .

Definition 6 (Wasserstein statistical manifold) Given    and i  T, i = 1, 2, we define

g(1, 2) = 1(x)2(x)(, x)dx,
Rn

where

- · ((, x)i(x)) = ((, x), i).

Here

 

=

(

 i

(,

x))di=1



Rd

and

(·,

·)

is

an

Euclidean

inner

product

in

Rd.

11

Under review as a conference paper at ICLR 2019

In particular, we denote g(, ) = TG(),
where G() = (G()ij)1i,jd  Rd×d is the associated metric tensor defined in Theorem 2.
Here we assume that G() is smooth and positive definite, so that (, g) forms a smooth Riemannian manifold. In this case, Theorem 2 studies the constrained Wassertein gradient operator in parameter space.

B PROOFS OF WASSERSTEIN NATURAL GRADIENT

Proof of Theorem 1 The distance dW can be written into the action function in Wasserstein statistical manifold. In other words, consider

dW (0, 1)2 = inf

1
(t)TG((t))(t) : (0) = 0, (1) = 1
0

where the infimum is taken over (t)  C1([0, 1], ). Following the definition of metric tensor in definition 6, we have

(t)TG((t))(t) = ((t, x))2((t), x)dx,
Rn
with (t, x) satisfying

t((t), x) = ((t), x)(t) = -(((t), x)(t, x)). We finish the proof.

Proof of Theorem 2 The gradient operator on a Riemannian manifold (, g) is defined as follows.

For any   T, then the Riemannian gradient W F ()  T satisfies

g(, W F ()) = (F (), ).

In other words,

TG()W F () = F ()T.

Since   Rd and G() is positive definite, then

W F () = G()-1F ().

Proof of Proposition 3. We next present the derivation of the proposed semi-backward method.

Claim: Denote  - k = h, then

(k - )TG(k)(k - ) = dW (, k)2 + O(h2),

(6)

and

1 2

(k

-

)T G(k )(k

-

)

+

O(h2)

=

sup


Rn

(x)((,

x)

-

(k, x))

-

1 2

(x) 2(k, x)dx.

(7)

Proof of Claim. We next prove the claim. Denote the geodesic path (t), t  [0, 1], with (0) = ,

(1) = k, s.t.

dW (, k)2 =

0

1
(

d dt

(t))T

G(

(t))

d dt



(t)dt.

We reparameterize the time of (t) into the time interval [0, h]. Denote  = ht and ( ) = (ht).

Thus

( )

=

k

+

-k h



+

O( 2)

and

d d

(

)

=

-k h

+

O( ),

dW (, k)2 =h

h 0

d d

(

)T

G((

))

d d

( )d

=h

h
(



0

- k h

+

O(h))TG(k

+ O(h))( 

- k h

+

O(h))d

=( - k)TG(k)( - k) + O(h2),

12

Under review as a conference paper at ICLR 2019

which proves equation 6.

We next prove equation 7. On the L.H.S. of equation 7,

(k, x)( - k) = (, x) - (k, x) + O(h). From the definition of G(),

1 2

(

- k)TG(k)(

-

k)

=

1 2

((x))2(k, x)dx,
Rn

where

- · ((k, x)(x)) = (k, x)( - k) = (k, x) + O(h).

On the R.H.S. of equation 7, the maximizer  satisfies

(, x) - (k, x) +  · ((k, x)(x)) = 0.

(8)

Applying equation 8 into the R.H.S. of equation 7, we have

Rn

(x)((, x)

-

(k ,

x))

-

1 2

(x)

2(k, x)dx

=

Rn

(x)[-

·

((k, x)(x)]

-

1 2



(x)(k

,

x)dx

=

Rn

(x)

2(k, x) -

1 2

(x) 2(k, x)dx

=

1 2

Rn

(x) 2(k, x)dx.

Comparing the L.H.S. and R.H.S. of equation 7, we prove the claim.

From the claim,

k+1

=

arg

min


F

()

+

1 h

dW

(, 2

k )2

=

arg

min


F

()

+

1 2h

(k

-

)T G(k )(k

-

)

+

O(h)

=

arg

min


F

()

+

1 h

sup


Rn

(x)((, x)

-

(k, x))

-

1 2

(x) 2(k, x)dx + O(h).

Thus we derive a consistent numerical method in time, known as the Semi-backward method:

k+1 = k - hG(k)-1F (k+1).

Proof of Proposition 4. This result is proven in Li & Osher (2018). We present it here for the
completion of paper. The implicit model is given by the following push-forward relation. Denote g#p(z) = (, x), i.e.,

f (g(, z))p(z)dz = f (x)(, x)dx, for any f  Cc(Rn).
Rm Rn

(9)

Given the gradient constraint

d dt

g((t),

z)

=

(t,

g((t),

z)),

we shall show that the probability density transition equation of g((t), z) satisfies the constrained

continuity equation

 t

((t),

x)

+



·

(((t),

x)(t,

x))

=

0,

(10)

and

EZp(z)

d dt

g((t),

Z)

2=

Rn

(t, x)) 2((t), x)dx.

(11)

13

Under review as a conference paper at ICLR 2019

On the one hand, consider f  Cc(Rn), then

d dt

EZp(z)f

(g((t),

Z ))

=

d dt

f (g((t), z))p(z)dz
Rm

=

d dt

f (x)((t), x)dx
Rn

=

Rn

f

(x)

 t

((t),

x)dx,

where the second equality holds from the push forward relation in equation 9.

(12)

On the other hand, consider

d dt

EZp(z)f

(g((t),

Z ))

=

lim
t0

EZp(z)

f

(g((t

+

t),

Z) - t

f

(g((t),

Z ))

= lim

f (g((t + t), z)) - f (g((t), z)) p(z)dz

t0 Rm

t

=

Rm

f

(g((t),

z))

d dt

g((t),

z)p(z)dz

= f (g((t), z))(t, g((t), z))p(z)dz
Rm

(13)

= f (x)(t, x)((t), x)dx
Rn

= - f (x) · ((t, x)((t), x))dx,
Rn
where , · are gradient and divergence operators w.r.t. x  Rn. The second to last equality holds
from the push forward relation equation 9, and the last equality holds using the integration by parts w.r.t. x. Since equation 12 equals equation 13 for any f  Cc(Rn), we prove equation 10.

In addition, by the definition of the push forward operator equation 9, we have

EZp(z)

d dt

g((t),

Z)

2

=

Rn

(t, g((t), z)) 2p(z)dz

Thus we prove equation 11.

= (t, x) 2((t), x)dx.
Rn

Proof of Proposition 5. This example allows us to compute the proximal operator explicitly. On the one hand, we compute the Wasserstein proximal operator explicitly:

Wk+1

=

(akW+1,

bWk+1)

=

arg

min


FW1 ()

+

1 2h

dW

(,

k )2

=

arg

min
(a,b)

|a

-

a|

+

(1

-

)|b

-

b|

+

1 2h

(|a

-

ak |2

+

(1

-

)|b

-

bk |).

I.e., Here

akW+1

=

arg

min
a

|a

-

a|

+

1 2h

|a

-

ak |2 ,

bWk+1

=

arg

min
b

|b

-

b|

+

1 2h

|b

-

bk |2 .

ak - h  akW+1 = shrinka (ak, h) = ak + h a

if ak > a + h; if ak < a - h;
otherwise.

Similarly, bWk+1 = shrinkb(bk, h).

On the other hand, we calculate the Euclidean proximal operator explicitly:

Ek+1

=

(aEk+1,

bEk+1)

=

arg

min


FW1 ()

+

1 2h

dE

(,

k )2

=

arg

min
(a,b)

|a

-

a|

+

(1

-

)|b

-

b|

+

1 2h

(|a

-

ak |2

+

|b

-

bk |2 ).

14

Under review as a conference paper at ICLR 2019

I.e.,

aEk+1

=

arg

min |a
a

-

a|

+

1 2h

|a

-

ak |2 ,

bEk+1

=

arg

min(1
b

-

)|b

-

b|

+

1 2h

|b

-

bk |2 .

Here

ak - h  aEk+1 = shrinka (ak, h) = ak + h a

if ak > a + h; if ak < a - h;
otherwise.

Similarly, bkE+1 = shrinkb(bk, (1 - )h). Here we only need to check that for all possible cases, FW1 (Ek+1) > FW1 (Wk+1). If ak > a + h and bk > b + h, then

FW1 (Wk+1)

=[(ak

-

a

-

h)

+

h 2

]

+

(1

-

)[(bk

-

b

-

h)

+

h 2

]

=(ak

-

a)

+

(1

-

)(bk

-

b)

-

h 2

,

and

FW1 (Ek+1)

=[(ak

- a

-

h)]

+

(h)2 2h

+

(1

-

)[(bk

-

b

-

h)] +

(1

- )2h2 2h

=(ak

-

a)

+

(1

-

)(bk

-

b)

-

h 2

[2

+

(1

-

)2].

Since   [0, 1], then 2 + (1 - )2  [ + (1 - )]2 = 1, then FW1 (Wk+1)  FW1 (Ek+1). In other cases, the proof follows similarly. We finish the proof.

C HYPERPARAMETERS FOR RELAXED WASSERSTEIN PROXIMAL
EXPERIMENTS
The following hyperparameter settings for the Relaxed Wasserstein Proximal experiments in Section 3.1 are:
· A batch size of 64 for all experiments. · For CIFAR-10 with WGAN-GP: The Adam optimizer with learning rate 0.0001, 1 = 0.5,
and 2 = 0.9 for both the generator and discriminator. We used a latent space dimension of 128, h = 0.1, and = 10 generator iterations. · For CIFAR-10 with Standard and DRAGAN: The Adam optimizer with learning rate 0.0002, 1 = 0.1, and 2 = 0.999 for both the generator and discriminator. We used a latent space dimension of 100, h = 0.2, and = 5 generator iterations. · For aligned and cropped CelebA with Standard: The Adam optimizer with learning rate 0.0002, 1 = 0.5, and 2 = 0.999 for both the generator and discriminator. We used a latent space dimension of 100, h = 0.2, and = 5 generator iterations.

D A PRACTICAL DESCRIPTION OF THE RELAXED WASSERSTEIN PROXIMAL
As mentioned in Section 3.1, the Relaxed Wasserstein Proximal is meant to be an easy-to-implement, drop-in regularization. For instructional purposes, we take a specific example to showcase the algorithm: Relaxed Wasserstein Proximal on Standard GANs (with non-saturating gradient for the generator):
· Given: ­ A generator g, and discriminator D, ­ The distance function F(g) = Exreal[log(D(x))] - EzN (0,1)[log(1 - D (g (z ))],

15

Under review as a conference paper at ICLR 2019

­ Choice of optimizers, Adam and Adam, ­ Proximal step-sizes h, and generator iterations , and ­ Batch size B.

Then the algorithm follows: 1. Sample real data {xi}iB=1, and latent data {zi}iB=1. 2. Update the discriminator:

k  Adam

-

1 B

B

log(D (xi ))

-

1 B

B

log(1 - D(g(zi)))

i=1 i=1

3. Sample latent data {zi}Bi=1 4. Perform Adam gradient descent number of times:

k  Adam for

-

1 B

B

log(D (g (zi )))

-

1 B

B

1 2h

g(zi) - gk-1 (zi)

2 2

i=1 i=1

number of times.

,

5. Repeat the above until a chosen stopping condition (e.g. maximum number of iterations).

As one can analyze above, the only difference between the standard way of training GANs and using

the Relaxed Wasserstein Proximal, are the

g(zi) - gk-1 (zi)

2 2

terms

and

the

number

of

generator

iterations . Note that in this paper, we call a single loop of updating a discriminator a number of

times and then updating the generator a number of a time, an outer-iteration.

E GENERATED SAMPLES FROM THE MODEL
In Figure 6, we have samples generated from a Standard GAN with RWP regularization, trained on the CelebA dataset. The FID of these images was 17.105. In Figure 7, we have samples generated from WGAN-GP with RWP , trained on the CIFAR-10 dataset. The FID for these images is 38.3.
16

Under review as a conference paper at ICLR 2019
Figure 6: A sample of images generated by RWP regularization on Standard GANs, on CelebA.
Figure 7: A sample of images generated by RWP regularization on WGAN-GP, on CIFAR-10. 17

Under review as a conference paper at ICLR 2019
F LATENT SPACE WALK
Radford et al. (2015) suggest that walking in the latent space could detect whether a generator was memorizing. We see in Figure 8 and Figure 9 that we have smooth transitions, so this is not the case for GANs with RWP regularization.
Figure 8: A latent space walk for a network with RWP regularization on Standard GANs, on CelebA. As we have smooth transitions, this shows the generator is not overfitting. The latent space walk is done by interpolating between 4 points in the latent space.
Figure 9: A latent space walk for a network with RWP regularization on WGAN-GP, on CIFAR-10. As we have smooth transitions, this shows the generator is not overfitting. The latent space walk is done by interpolating between 4 points in the latent space.
G ALGORITHM AND PARTICULAR HYPERPARAMETERS FOR THE SEMI-BACKWARD EULER METHOD
The specific hyperparameter settings used for the Semi-Backward Euler (SBE) on WGAN-GP, trained on CIFAR-10, are:
· A batch size of 64. · The DCGAN architecture for the discriminator and generator. A one-hidden-layer fully-
connected network (a.k.a. MLP) for the potential p. We also used layer-normalization (Lei Ba et al. (2016)) for each layer. · We used the Adam optimizer with learning rate 0.0002, 1 = 0.1, and 2 = 0.999 for both the generator, discriminator, and potential p. We used a latent space dimension of 100, and h = 0.2. · Every outer-iteration loop, we updated the discriminator 5 times (as suggested in WGANGP), the generator once, and the potential 5 times. Note an outer-iteration is defined as one loop of: updating the discriminator a number of times, updating the potential a number of times, and updating the generator a number of times.
18

Under review as a conference paper at ICLR 2019

Algorithm 2 Semi-backward Euler method, where F is a parameterized function to minimize.

Require: F, a parameterized function to minimize (e.g. Wasserstein-1 with a parameterized discriminator). g the generator. p the potential.
Require: h the proximal step-size, m the batch size.

Require: OptimizerF , Optimizerg , and Optimizerp Require: The number of generator iterations and p iterations to do per update.

1: for k = 0 to max iterations do 2: Sample real data {xi}Bi=1 and latent data {zi}iB=1.

3:

k  OptimizerF

1 B

B i=1

F

(g

(zi

))

4: for s = 0 to phi iterations do 5: Sample latent data {zi}Bi=1

6:

pk  Optimizerp

11 hB

B i=1

p (g (zi ))

-

p(gk-1 (zi))

-

1 2

p

(gk-1

(zi

))

7: end for

8: for = 0 to generator iterations do 9: Sample latent data {zi}Bi=1

10:

k  Optimizerg

1 B

B i=1

F (g (zi ))

+

1 h

p (g (zi ))

-

p(gk-1 (zi))

-

1 2

p

(gk-1

(zi

))

11: end for

12: end for

19

