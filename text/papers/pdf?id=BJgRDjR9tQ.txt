Under review as a conference paper at ICLR 2019

ROBUST ESTIMATION VIA GENERATIVE ADVERSARIAL NETWORKS
Anonymous authors Paper under double-blind review

ABSTRACT
Robust estimation under Huber's -contamination model has become an important topic in statistics and theoretical computer science. Rate-optimal procedures such as Tukey's median and other estimators based on statistical depth functions are impractical because of their computational intractability. In this paper, we establish an intriguing connection between f-GANs (Nowozin et al., 2016) and various depth functions through the lens of f-Learning. Similar to the derivation of f-GAN, we show that these depth functions that lead to rate-optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f-Learning. This connection opens the door of computing robust estimators using tools developed for training GANs. In particular, we show that a JS-GAN (Goodfellow et al., 2014) that uses a neural network discriminator with at least one hidden layer is able to achieve the minimax rate of robust mean estimation under Huber's -contamination model. Interestingly, the hidden layers for the neural net structure in the discriminator class is shown to be necessary for robust estimation.

1 INTRODUCTION

In the setting of Huber's -contamination model (Huber, 1964; 1965), one has i.i.d observations

X1, ..., Xn  (1 - )P + Q,

(1)

and the goal is to estimate the model parameter . Under the data generating process (1), each observation has a 1 - probability to be drawn from P and the other probability to be drawn from the contamination distribution Q. The presence of an unknown contamination distribution poses both statistical and computational challenges to the problem. For example, consider a normal mean estimation problem with P = N (, Ip). Due to the contamination of data, the sample average, which is optimal when = 0, can be arbitrarily far away from the true mean if Q charges a positive probability at infinity. Moreover, even robust estimators such as coordinatewise median and geometric median are proved to be suboptimal under the setting of (1) (Chen et al., 2018; Diakonikolas et al., 2016a; Lai et al., 2016). The search for both statistically optimal and computationally feasible procedures has become a fundamental problem in areas including statistics and computer science.

For the normal mean estimation problem, it has been shown in Chen et al. (2018) that the minimax rate with

respect to the squared

2

loss

is

p n



2 , and is achieved by Tukey's median (Tukey, 1975). Despite the statistical

optimality of Tukey's median, its computation is not tractable. In fact, even an approximate algorithm takes

O(eCp) in time (Amenta et al., 2000; Chan, 2004; Rousseeuw & Struyf, 1998).

Recent developments in robust statistics are focused on the search of computationally tractable algorithms for

estimating  under Huber's -contamination model (1). The success of the efforts started from two fundamental

papers Diakonikolas et al. (2016a); Lai et al. (2016), where two different but related computational strategies

"iterative filtering" and "dimension halving" are proposed to robustly estimate the normal mean. These algo-

rithms

can

provably

achieve

the

minimax

rate

p n



2 up to a poly-logarithmic factor within polynomial time.

1

Under review as a conference paper at ICLR 2019

The main idea behind the two methods is the fact that a good robust moment estimator can be certified efficiently by higher moments. This idea was later further extended (Diakonikolas et al., 2017; Du et al., 2017; Diakonikolas et al., 2016b; 2018a;c;b; Kothari et al., 2018) to develop robust and computable procedures for various other problems.
However, many of the computationally feasible procedures for robust mean estimation in the literature rely on the knowledge of covariance matrix and sometimes the knowledge of contamination proportion. Even though these assumptions can be relaxed, nontrivial modifications of the algorithms are required for such an extension and statistical error rates may also be affected. Compared with these computationally feasible procedures proposed in the recent literature for robust estimation, Tukey's median (9) and other depth-based estimators (Rousseeuw & Hubert, 1999; Mizera, 2002; Zhang, 2002; Mizera & Mu¨ller, 2004; Paindaveine & Van Bever, 2017) have some indispensable advantages in terms of their statistical properties. First, the depth-based estimators have clear objective functions that can be interpreted from the perspective of projection pursuit (Mizera, 2002). Second, the depth-based procedures are adaptive to unknown nuisance parameters in the models such as covariance structures, contamination proportion, and error distributions (Chen et al., 2018; Gao, 2017). Last but not least, Tukey's depth and other depth functions are mostly designed for robust quantile estimation, while the recent advancements in the literature are all focused on robust moments estimation. Although this is not an issue when it comes to normal mean estimation, the difference is fundamental for robust estimation under general settings such as elliptical distributions where moments do not necessarily exist.
Given the desirable statistical properties discussed above, this paper is focused on the development of computational strategies of depth-like procedures. Our key observation is that robust estimators that are maximizers of depth functions, including halfspace depth, regression depth and covariance matrix depth, can all be derived from the framework of f-GAN (Nowozin et al., 2016). As a result, these depth-based estimators can be viewed as minimizers of variational lower bounds of the total variation distance between the empirical measure and the model distribution. This observation allows us to leverage the recent developments in the deep learning literature to compute these variational lower bounds through neural network approximations. Our theoretical results give insights on how to choose appropriate neural network classes that lead to minimax optimal robust estimation under Huber's -contamination model.

2 ROBUST ESTIMATION AND F-GAN

We start with the definition of f-divergence (Csisza´r, 1964; Ali & Silvey, 1966). Given a strictly convex function

f that satisfies f (1) = 0, the f-divergence between two probability distributions P and Q is defined by

p

Df (P Q) =

f dQ. q

(2)

Here, we use p(·) and q(·) to stand for the density functions of P and Q with respect to some common domi-

nating measure. For a fully rigorous definition, see Polyanskiy & Wu (2017). Let f  be the convex conjugate

of f . That is, f (t) = supudomf (ut - f (u)). A variational lower bound of (2) is
Df (P Q)  sup [EP T (X) - EQf (T (X))] .
T T

(3)

Note that the inequality (3) becomes an equality whenever the class T contains the function f (p/q) (Nguyen

et al., 2010). For notational simplicity, we also use f for an arbitrary element of the subdifferential when the

derivative does not exist. With i.i.d. observations X1, ..., Xn  P , the variational lower bound (3) naturally leads to the following learning method

P = argmin sup
QQ T T

1 n

n

T (Xi) - EQf (T (X))

.

i=1

(4)

The formula (4) is a powerful and general way to learn the distribution P from its i.i.d. observations. It is

known as f-GAN (Nowozin et al., 2016), an extension of GAN (Goodfellow et al., 2014), which stands for

generative adversarial networks. The idea is to find a P so that the best discriminator T in the class T cannot

tell

the

difference

between

P

and

the

empirical

distribution

1 n

n i=1

Xi

.

2

Under review as a conference paper at ICLR 2019

2.1 F-LEARNING: A UNIFIED FRAMEWORK

Our f-Learning framework is based on a special case of the variational lower bound (3). That is,

Df (P Q)  sup EP f
QQ

q(X ) q(X )

- EQf 

f

q(X ) q(X )

,

(5)

where q(·) stands for the density function of Q. Compare (5) with (3), and it is easy to realize that (5) is a

special case of (3) with

T = f q :qQ . q

(6)

Moreover, the inequality (5) becomes an equality as long as P  Q. The sample version of (5) leads to the following learning method

1n

P = argmin sup

f

QQ

QQ

n
i=1

q(Xi) q(Xi)

- EQf 

f

q(X ) q(X )

.

(7)

The learning method (7) will be referred to as f-Learning in the sequel. It is a very general framework that
covers many important learning procedures as special cases. For example, consider the case where Q = Q and f (x) = x log x. Direct calculations give f (x) = log x + 1 and f (t) = et-1. Therefore, (7) becomes

P

=

argmin sup
QQ QQ

1 n

n i=1

log

q(Xi) q(Xi)

=

argmax
qQ

1 n

n i=1

log q(Xi),

which is the maximum likelihood estimator (MLE).

2.2 TV-LEARNING AND DEPTH-BASED ESTIMATORS

An important generator f that we will discuss here is f (x) = (x-1)+. This leads to the total variation distance

Df (P

Q)

=

1 2

|p - q|. With f (x) = I{x  1} and f (t) = tI{0  t  1}, the TV-Learning is given by

P = argmin sup
QQ QQ

1n nI
i=1

q(Xi)  1 q(Xi)

-Q

q 1 q

.

(8)

Proposition 2.1. The TV-Learning (8) includes the following special cases:

1. Take Q = {N (, Ip) :   Rp} and Q = {N (, Ip) :  -   r}. As r  0, (8) becomes

 = argmax
Rp

1 inf u =1 n

n

I{uT (Xi - )  0}.

i=1

(9)

2. Take Q = Py,X = Py|X PX : Py|X = N (XT , 1),   Rp , and Q = Py,X = Py|X PX : Py|X = N (XT , 1),  -   r . As r  0, (8) becomes

 = argmax
Rp

1 inf u =1 n

n

I{uT Xi(yi - XiT )  0}.

i=1

(10)

3. Take Q = {N (0, ) :   Ep}, where Ep is the class of all p × p covariance matrices, and Q = N (0, ) : -1 = -1 + ruuT  Ep, |r|  r, u = 1 . As r  0, (8) becomes

 = argmin sup
Ep u =1

1 n

n

I{|uT Xi|2  uT u} - P(12  1)

i=1



1 n

n

I{|uT Xi|2 > uT u} - P(12 > 1)

i=1

.

(11)

3

Under review as a conference paper at ICLR 2019

The formula (9) is recognized as Tukey's median, the maximizer of Tukey's halfspace depth. A traditional understanding of Tukey's median is that (9) maximizes the halfspace depth (Donoho et al., 1992) so that  is close to the center of all one-dimensional projections of the data. In the f-Learning framework, N (, Ip) is understood to be the minimizer of a variational lower bound of the total variation distance. The formula (10) gives the estimator that maximizes the regression depth proposed by Rousseeuw & Hubert (1999). It is worth noting that the derivation of (10) does not depend on the marginal distribution PX in the linear regression model. Finally, (11) is related to the covariance matrix depth (Zhang, 2002; Chen et al., 2018; Paindaveine & Van Bever, 2017). All of the estimators (9), (10) and (11) are proved to achieve the minimax rate for the corresponding problems under Huber's -contamination model (Chen et al., 2018; Gao, 2017).
2.3 FROM F-LEARNING TO F-GAN
The connection to various depth functions shows the importance of TV-Learning in robust estimation. However, it is well-known that depth-based estimators are very hard to compute (Amenta et al., 2000; van Kreveld et al., 1999; Rousseeuw & Struyf, 1998), which limits their applications only for very low-dimensional problems. On the other hand, the general f-GAN framework (4) has been successfully applied to learn complex distributions and images in practice (Goodfellow et al., 2014; Radford et al., 2015; Salimans et al., 2016). The major difference that gives the computational advantage to f-GAN is its flexibility in terms of designing the discriminator class T compared with the pre-specified choice (6) in f-Learning. While f-Learning provides a unified perspective in understanding various depth-based procedures in robust estimation, we can step back into the more general f-GAN for its computational advantages, and to design efficient computational strategies.

3 ROBUST MEAN ESTIMATION VIA GAN

In this section, we focus on the problem of robust mean estimation under Huber's -contamination model. Our

goal is to reveal how the choice of the class of discriminators affects robustness and statistical optimality under

the simplest possible setting. That is, we have i.i.d. observations X1, ..., Xn  (1 - )N (, Ip) + Q, and we

need to estimate the unknown location   Rp with the contaminated data. Our goal is to achieve the minimax

rate

p n



2 with respect to the squared

2 loss uniformly over all   Rp and all Q.

3.1 RESULTS FOR TV-GAN

We start with the total variation GAN (TV-GAN) with f (x) = (x - 1)+ in (4). For the Gaussian location family, (4) can be written as

1n

 = argmin max
Rp DD

n D(Xi) - EN(,Ip)D(X)
i=1

.

(12)

Now we need to specify the class of discriminators D to solve the classification problem between N (, Ip) and

the

empirical

distribution

1 n

n i=1

Xi .

One

of

the

simplest

discriminator

classes

is

the

logistic

regression,

D = D(x) = sigmoid(wT x + b) : w  Rp, b  R .

(13)

With D(x) = sigmoid(wT x + b) in (13), the procedure (12) can be viewed as a smoothed version of TVLearning (8). To be specific, the sigmoid function sigmoid(wT x + b) tends to an indicator function as w 
, which leads to a procedure very similar to (9). In fact, the class (13) is richer than the one used in (9), and

thus (12) can be understood as the minimizer of a sharper variational lower bound than that of (9).

Theorem 3.1.

Assume

p n

+

2



c for some sufficiently small constant c > 0.

With i.i.d.

observations

X1, ..., Xn  (1 - )N (, Ip) + Q, the estimator  defined by (12) satisfies

- 2 C p  2 , n

with probability at least 1 - e-C (p+n 2) uniformly over all   Rp and all Q. The constants C, C > 0 are

universal.

4

Under review as a conference paper at ICLR 2019

Figure 1: Landscape of TV-GAN objective function F (, w) = supb[EP sigmoid(wX + b) - EN(,1)sigmoid(wX + b)], where b is maximized out for visualization. Samples are drawn from P = (1 - )N (1, 1) + N (10, 1) with = 0.2. Left: a surface plot of F (, w). The solid curves are marginal
functions for fixed 's: F (1, w) (red) and F (5, w) (blue), and the dash curves are marginal functions for fixed w's: F (, -10) (orange) and F (, 10) (green). Right: a heatmap of F (, w). It is clear that F~(w) = F (, w)
has two local maxima for a given , achieved at w = + and w = -. In fact, the global maximum for F~(w) has a phase transition from w = + to w = - as  grows. For example, the maximum is achieved at
w = + when  = 1 (blue solid) and is achieved at w = - when  = 5 (red solid). Unfortunately, even if
we initialize with 0 = 1 and w0 > 0, gradient ascents on  will only increase the value of  (green dash), and thus as long as the discriminator cannot reach the global maximizer, w will be stuck in the positive half space
{w : w > 0} and further increase the value of .

Though

TV-GAN

can

achieve

the

minimax

rate

p n



2 under Huber's contamination model, it may suffer from

optimization difficulties especially when the distributions Q and N (, Ip) are far away from each other, as

shown in Figure 1.

3.2 RESULTS FOR JS-GAN

Given the intractable optimization property of TV-GAN, we next turn to Jensen-Shannon GAN (JS-GAN) with

f (x)

=

x

log

x

-

(x

+

1)

log

x+1 2

.

The

estimator

is

defined

by

1n

 = argmin max
Rp DD

n log D(Xi) + EN(,Ip) log(1 - D(Xi))
i=1

+ log 4.

(14)

This is exactly the original GAN (Goodfellow et al., 2014) specialized to the normal mean estimation problem. The advantages of JS-GAN over other forms of GAN are studied extensively in the literature (Lucic et al., 2017; Kurach et al., 2018).

Unlike TV-GAN, our experiment results show that (14) with the logistic regression discriminator class (13) is not robust to contamination. However, if we replace (13) by a neural network class with one or more hidden layers, the estimator will be robust and will also work very well numerically.

To understand why and how the class of the discriminators affects the robustness property of JS-GAN, we introduce a new concept called sigmoid Jensen-Shannon divergence. Let g : Rp  Rd be a function that maps a multivariate observation to a d-dimensional feature space. The sigmoid Jensen-Shannon divergence between
two probability distributions P and Q with respect to the feature g is defined as

JSg(P, Q) = max EP log sigmoid(wT g(X)) + EQ log(1 - sigmoid(wT g(X))) + log 4.
wW

In other words, P and Q are distinguished by a logistic regression classifier using the feature g(X). It is easy to see that JSg(P, Q) is a variational lower bound of the original Jensen-Shannon divergence. The key property of JSg(P, Q) is given by the following proposition.

5

Under review as a conference paper at ICLR 2019

Proposition 3.1. Assume W is a convex set that contains an open neighborhood of 0. Then, JSg(P, Q) = 0 if and only if EP g(X) = EQg(X).

The proposition asserts that JSg(·, ·) cannot distinguish P and Q if the feature g(X) has the same expected value under the two distributions. This generalized moment matching effect has also been studied by Liu et al. (2017) for general f-GANs. However, the linear discriminator class considered in Liu et al. (2017) is parameterized in a different way compared with the discriminator class here.

When we apply Proposition 3.1 to robust mean estimation, the JS-GAN is trying to match the values of

1 n

n i=1

g(Xi)

and

EN (,I ) g(X )

for

the

feature

g(X )

used

in

the

logistic

regression

classifier.

This

explains

what we observe in our numerical experiments. A neural net without any hidden layer is equivalent to a logistic

regression with a

JSg

1 n

n i=1

Xi

linear , N (,

feature Ip) =

g(X) = 0, which

(XT , 1)T  implies that

Rp+1. Therefore, whenever the sample mean is a global



=

1 n

n i=1

maximizer of

Xi, we have (14). On the

other hand, a neural net with at least one hidden layers involves a nonlinear feature function g(X), which is the

key that leads to the robustness of (14).

We will show rigorously that a neural net with one hidden layer is sufficient to make (14) robust and optimal. Consider the following class of discriminators,







 D = D(x) = sigmoid  wj(uTj x + bj) : |wj|  , uj  Rp, bj  R .

 j1

j1



(15)

The class (15) consists of two-layer neural network functions. While the dimension of the input layer is p, the

dimension of the hidden layer can be arbitrary, as long as the weights have a bounded 1 norm. The nonlinear

activation function (·) is allowed to take 1) indicator:

(x)

=

I{x



1}, 2) sigmoid:

(x)

=

1 1+e-x

,

3)

ramp: (x) = max(min(x + 1/2, 1), 0). Other bounded activation functions are also possible, but we do not

exclusively list them. The rectified linear unit (ReLU) will be studied in Appendix A.1.

Theorem 3.2.

Consider the estimator  defined by (14) with D specified by (15).

Assume

p n

+

2  c for

some sufficiently small constant c > 0, and set  = O

p n

+

. With i.i.d. observations X1, ..., Xn 

(1 - )N (, Ip) + Q, we have

- 2 C p  2 , n

with probability at least 1 - e-C (p+n 2) uniformly over all   Rp and all Q. The constants C, C > 0 are

universal.

4 NUMERICAL EXPERIMENTS
In this section, we give extensive numerical studies of robust mean estimation via GAN. After introducing the implementation details in Section 4.1, we verify our theoretical results on minimax estimation with both TV-GAN and JS-GAN in Section 4.2. Comparison with other methods on robust mean estimation in the literature is given in Section 4.3. The effects of various network structures are studied in Section 4.4. Finally, adaptation to unknown covariance is studied in Section 4.5. We assume i.i.d. observations are drawn from (1 - )N (0p, Ip) + Q with and Q to be specified.
4.1 IMPLEMENTATIONS
We adopt the standard algorithmic framework of f-GANs (Nowozin et al., 2016) for the implementation of JSGAN and TV-GAN for robust mean estimation. In particular, the generator for mean estimation is G(Z) = Z +  with Z  N (0p, Ip); the discriminator D is a multilayer perceptron (MLP), where each layer consisting of a linear map and a sigmoid activation function and the number of nodes will vary in different experiments to be specified below. Details related to algorithms, tuning, critical hyper-parameters, structures of discriminator networks and other training tricks for stabilization and acceleration are discussed in Appendix B.1. The source code to reproduce the results will be released upon request.

6

Under review as a conference paper at ICLR 2019
4.2 NUMERICAL SUPPORT FOR THE MINIMAX RATES
We verify the minimax rates achieved by TV-GAN (Theorem 3.1) and JS-GAN (Theorem 3.2) via numerical experiments. Two main scenarios we consider here are p/n < and p/n > , where in both cases, various types of contamination distributions Q are considered. Specifically, the choice of contamination distributions Q includes N (µ  1p, Ip) with µ ranges in {0.2, 0.5, 1, 5}, N (0.5  1p, ) and Cauchy(  1p). Details of the construction of the covariance matrix  is given in Appendix B.2. The distribution Cauchy(  1p) is obtained by combining p independent one-dimensional standard Cauchy with location parameter j = 0.5.

Figure 2: 2 error  -  against (left: p = 100, n = 50, 000 and ranges from 0.05 to 0.20), p (middle: n = 1, 000, = 0.1 and p ranges from 10 to 100) and 1/ n (right: p = 50, = 0.1 and n ranges from 50 to 1, 000), respectively. Net structure: One hidden layer with 20 hidden units (JS-GAN), zero hidden layer (TV-GAN). The vertical bars indicate ± standard deviations.
The main experimental results are summarized in Figure 2, where the 2 error we present is the maximum error among all choices of Q, and detailed numerical results can be founded in Tables 6, 7 and 8 in Appendix. We separately explore the relation between the error and one of , p and 1/ n with the other two parameters fixed. The study of the relation between the 2 error and is in the regime p/n < so that dominates the minimax rate. The scenario p/n > is considered in the study of the effects of p and 1/ n. As is shown in Figure 2, the errors are approximately linear against the corresponding parameters in all cases, which empirically verifies the conclusions of Theorem 3.1 and Theorem 3.2.
4.3 COMPARISON WITH OTHER METHODS
We perform additional experiments to compare with other methods including dimension halving (Lai et al., 2016) and iterative filtering (Diakonikolas et al., 2017) under various settings. We emphasize that our method does not require any knowledge about the nuisance parameters such as the contamination proportion . Tuning GAN is only a matter of optimization and one can tune parameters based on the objective function only.

Table 1: Comparison of various robust mean estimation methods. Net structure: One-hidden layer network

with 20 hidden units when n = 50, 000 and 2 hidden units when n = 5, 000. The number in each cell is the

average of 2 error  -  with standard deviation in parenthesis estimated from 10 repeated experiments and the smallest error among four methods is highlighted in bold.

Q

np

TV-GAN

JS-GAN

Dimension Halving Iterative Filtering

N (0.5  1p, Ip) 50,000 100 .2 0.0953 (0.0064) 0.1144 (0.0154) N (0.5  1p, Ip) 5,000 100 .2 0.1941 (0.0173) 0.2182 (0.0527) N (0.5  1p, Ip) 50,000 200 .2 0.1108 (0.0093) 0.1573 (0.0815) N (0.5  1p, Ip) 50,000 100 .05 0.0913 (0.0527) 0.1390 (0.0050) N (5  1p, Ip) 50,000 100 .2 2.7721 (0.1285) 0.0534 (0.0041) N (0.5  1p, ) 50,000 100 .2 0.1189 (0.0195) 0.1148 (0.0234) Cauchy(0.5  1p) 50,000 100 .2 0.0738 (0.0053) 0.0525 (0.0029)

0.3247 (0.0058) 0.3568 (0.0197) 0.3251 (0.0078) 0.0814 (0.0056) 0.3229 (0.0087) 0.3241 (0.0088) 0.1045 (0.0071)

0.1472 (0.0071) 0.2285 (0.0103) 0.1525 (0.0045) 0.0530 (0.0052) 0.1471 (0.0059) 0.1426 (0.0113) 0.0633 (0.0042)

7

Under review as a conference paper at ICLR 2019

Table 1 shows the performances of JS-GAN, TV-GAN, dimension halving, and iterative filtering. The network structure, for both JS-GAN and TV-GAN, has one hidden layer with 20 hidden units when the sample size is 50,000 and 2 hidden units when sample size is 5,000. The critical hyper-parameters we apply is given in Appendix and it turns out that the choice of the hyper-parameter is robust against different models when the net structures are the same. To summarize, our method outperforms other algorithms in most cases. TV-GAN is good at cases when Q and N (0p, Ip) are non-separable but fails when Q is far away from N (0p, Ip) due to optimization issues discussed in Section 3.1 (Figure 1). On the other hand, JS-GAN stably achieves the lowest error in separable cases and also shows competitive performances for non-separable ones.
4.4 NETWORK STRUCTURES
We further study the performances of TV-GAN and JS-GAN with various structures of neural networks. The main observation is tuning networks with one-hidden layer becomes tough as the dimension grows (e.g. p  200), while a deeper network can significantly refine the situation perhaps by improving the landscape. Some experiment results are given in Table 2. On the other hand, one-hidden layer performs not worse than deeper networks when dimension is not very large (e.g. p  100). More experiments are given in Appendix B.4. Additional theoretical results for deep neural nets are given in Appendix A.1.

Table 2: Experiment results for JS-GAN using networks with different structures in high dimension. Settings:

= 0.2, p  {200, 400} and n = 50, 000.

p 200-100-20-1

200-200-100-1

200-100-1

200-20-1

200 0.0910 (0.0056)

0.0790 (0.0026) 0.3064 (0.0077) 0.1573 (0.0815)

p 400-200-100-50-20-1 400-200-100-20-1 400-200-20-1

400-200-1

400 0.1477 (0.0053)

0.1732 (0.0397) 0.1393 (0.0090) 0.3604 (0.0990)

4.5 ADAPTATION TO UNKNOWN COVARIANCE

The robust mean estimator constructed through JS-GAN can be easily made adaptive to unknown covariance structure. We define

1n

(, ) = argmin max
Rp,Ep DD

n log D(Xi) + EN(,) log(1 - D(Xi))
i=1

+ log 4,

(16)

Compared with (14), (16) optimizes over both the mean and the covariance matrix. The estimator , as a result, is rate-optimal even when the true covariance matrix is not necessarily identity and is unknown (see Theorem A.2 in Appendix A.2). Below, we demonstrate some numerical evidence of the optimality of the estimator (16) in Table 3. Robust covariance matrix estimation is certainly an interesting topic. A thorough theoretical investigation and a systematic numerical study of this topic will be left for a future project.

Data generating process
0.8N (0p, 1) + 0.2N (0.5  1p, 2) 0.8N (0p, 1) + 0.2N (0.5  1p, 2)
0.8N (0p, 1) + 0.2N (1p, 2) 0.8N (0p, 1) + 0.2N (6  1p, 2) 0.8N (0p, 1) + 0.2Cauchy(0.5  1p)

Network structure
100-20-1 100-20-20-1
100-20-1 100-20-1 100-20-1

 - 0p
0.1680 (0.1540) 0.1824 (0.3034) 0.0817 (0.0213) 0.1069 (0.0357) 0.0797 (0.0257)

 - 1 op
1.9716 (0.7405) 1.4495 (0.6028) 1.2753 (0.4523) 1.1668 (0.1839) 4.0653 (0.1569)

Table 3: Numerical experiments for robust mean estimation with unknown covariance trained with 50, 000 samples. The covariance matrices 1 and 2 are generated by the same way described in Appendix B.2.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution from another. Journal of the Royal Statistical Society. Series B (Methodological), pp. 131­142, 1966.
Nina Amenta, Marshall Bern, David Eppstein, and S-H Teng. Regression depth and center points. Discrete & Computational Geometry, 23(3):305­323, 2000.
Peter L Bartlett. For valid generalization the size of the weights is more important than the size of the network. In Advances in neural information processing systems, pp. 134­140, 1997.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Timothy M Chan. An optimal randomized algorithm for maximum tukey depth. In Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms, pp. 430­436. Society for Industrial and Applied Mathematics, 2004.
Mengjie Chen, Chao Gao, and Zhao Ren. Robust covariance and scatter matrix estimation under hubers contamination model. The Annals of Statistics, 46(5):1932­1960, 2018.
Imre Csisza´r. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85­108, 1964.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pp. 655­664. IEEE, 2016a.
Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Robust learning of fixed-structure bayesian networks. arXiv preprint arXiv:1606.07384, 2016b.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Being robust (in high dimensions) can be practical. arXiv preprint arXiv:1703.00893, 2017.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815, 2018a.
Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation and learning mixtures of spherical gaussians. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1047­1060. ACM, 2018b.
Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for robust linear regression. arXiv preprint arXiv:1806.00040, 2018c.
David L Donoho, Miriam Gasko, et al. Breakdown properties of location estimates based on halfspace depth and projected outlyingness. The Annals of Statistics, 20(4):1803­1827, 1992.
Simon S Du, Sivaraman Balakrishnan, and Aarti Singh. Computationally efficient robust estimation of sparse functionals. arXiv preprint arXiv:1702.07709, 2017.
Chao Gao. Robust regression via mutivariate regression depth. arXiv preprint arXiv:1702.04656, 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­ 256, 2010.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
9

Under review as a conference paper at ICLR 2019
Peter J Huber. Robust estimation of a location parameter. The annals of mathematical statistics, 35(1):73­101, 1964.
Peter J Huber. A robust version of the probability ratio test. The Annals of Mathematical Statistics, 36(6): 1753­1758, 1965.
Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1035­1046. ACM, 2018.
Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. The gan landscape: Losses, architectures, regularization, and normalization. arXiv preprint arXiv:1807.04720, 2018.
Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pp. 665­674. IEEE, 2016.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of generative adversarial learning. In Advances in Neural Information Processing Systems, pp. 5545­5553, 2017.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148­188, 1989.
Ron Meir and Tong Zhang. Generalization error bounds for bayesian mixture algorithms. Journal of Machine Learning Research, 4(Oct):839­860, 2003.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Ivan Mizera. On depth and deep points: a calculus. The Annals of Statistics, 30(6):1681­1736, 2002.
Ivan Mizera and Christine H Mu¨ller. Location­scale depth. Journal of the American Statistical Association, 99(468):949­966, 2004.
Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature matching gan. arXiv preprint arXiv:1702.08398, 2017.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847­5861, 2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Davy Paindaveine and Germain Van Bever. Halfspace depths for scatter, concentration and shape matrices. arXiv preprint arXiv:1704.06160, 2017.
David Pollard. Convergence of stochastic processes. Springer Science & Business Media, 2012.
Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Peter J Rousseeuw and Mia Hubert. Regression depth. Journal of the American Statistical Association, 94 (446):388­402, 1999.
10

Under review as a conference paper at ICLR 2019
Peter J Rousseeuw and Anja Struyf. Computing location depth and regression depth in higher dimensions. Statistics and Computing, 8(3):193­203, 1998.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
Michel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces. Publications Mathe´matiques de l'Institut des Hautes Etudes Scientifiques, 81(1):73­205, 1995.
John W Tukey. Mathematics and the picturing of data. In Proceedings of the International Congress of Mathematicians, Vancouver, 1975, volume 2, pp. 523­531, 1975.
Aad W Van Der Vaart and Jon A Wellner. Weak convergence. In Weak convergence and empirical processes, pp. 16­28. Springer, 1996.
Marc van Kreveld, Joseph SB Mitchell, Peter Rousseeuw, Micha Sharir, Jack Snoeyink, and Bettina Speckmann. Efficient algorithms for maximum regression depth. In Proceedings of the fifteenth annual symposium on Computational geometry, pp. 31­40. ACM, 1999.
Jian Zhang. Some extensions of tukey's depth function. Journal of Multivariate Analysis, 82(1):134­165, 2002.
11

Under review as a conference paper at ICLR 2019

A ADDITIONAL THEORETICAL RESULTS

A.1 RESULTS FOR DEEP RELU NETWORKS

In this section, we investigate the performance of discriminator classes of deep neural nets with the ReLU

activation function. Since our goal is to learn a p-dimensional mean vector, a deep neural network discriminator

without any regularization will certainly lead to overfitting. Therefore, it is crucial to design a network class

with some appropriate regularizations. Inspired by the work of Bartlett (1997); Bartlett & Mendelson (2002),

we consider a network class with 1 regularizations on all layers except for the second top layer with an 2

regularization. With G1H (B) = g(x) = ReLU(vT x) : v 1  B , a neural network class with l + 1 layers is

defined as

HH

GlH+1(B) = g(x) = ReLU

vhgh(x) : |vh|  B, gh  GlH (B) .

h=1

h=1

Combining with the last sigmoid layer, we obtain the following discriminator class,

FLH (, , B) =


2p



D(x) = sigmoid  wjReLU

ujhgjh(x) + bj  :

j1

h=1

2p

|wj |  , uj2h  2, |bj |  , gjh  GLH-1(B) .

j1

h=1

We also define the following class by replacing the ReLU activation function of the second top layer with a sigmoid.

F¯LH (, , B) =


2p



D(x) = sigmoid  wjsigmoid

ujhgjh(x) + bj  :

j1

h=1

2p

|wj |  , u2jh  2, |bj |  , gjh  GLH-1(B) .

j1

h=1

Theoretical guarantees of the two classes above are given by the following theorem.

Theorem A.1. Consider i.i.d. observations X1, ..., Xn  (1 - )N (, Ip) + Q and the estimator  defined by (14) with D = FLH (, , B) or D = F¯LH (, , B) with H  2p, 2  L = O(1), 2  B = O(1), and  = p log p.



1. We set 

1/2

+ n-1/4.

Assume

p log p+ n

n

 c for some sufficiently small constant c > 0. Then,

for the estimator  defined by (14) with D = FLH (, , B), we have



 -  2  C p log p + n  , n

(17)

with probability at least 1 - p-C

uniformly over all   Rp such that

    log p and all Q.

2. We set  = O

p log p n

+

.

Assume

p log p n



2  c for some sufficiently small constant c > 0.

Then, for the estimator  defined by (14) with D = F¯LH (, , B), we have

 -  2  C p log p  2 , n
with probability at least 1 - e-C (p log p+n 2) uniformly over all   Rp such that and all Q.

(18)     log p

12

Under review as a conference paper at ICLR 2019



The

theorem

shows

that

JS-GAN

with

a

deep

ReLU

network

can

achieve

the

error

rate

p log p+ n

n

with

respect to the squared 2 loss. The dependence on is linear instead of 2 in the minimax rate, but it is still

useful given that this term is dimension-free. The sub-optimality of the rate is mainly due to the unboundedness

of the ReLU function. If we change the activation function in the second top layer from ReLU to sigmoid, we

will

achieve

the

optimal rate 

p log p n



2, even if all the other layers still use the ReLU activation.

The condition    log p for the ReLU network can be easily satisfied with a simple preprocessing step.

We split the data into two sample, whose sizes are log n and n - log n, respectively. Then, we calculate the

coordinatewise median  using the small sample. It is easy to show that  -   

log p log n



with high

probability. sample is (1

Then, - )N

for each Xi from the second

( - , Ip) + Q. Since

log log

sample, the conditional distribution

p n



  log p, the condition

-

of Xi 

-  given the first 
log p is satisfied,

and thus we can apply the estimator (14) using the shifted data Xi -  from the second sample. The theoretical guarantee of Theorem A.1 will be

  - ( - ) 2  C p log p + n 
n

,

with high probability. Hence, we can use  +  as the final estimator to achieve the same rate in Theorem A.1.
On the other hand, our experiments show that this preprocessing step is not needed. We believe that the assumption    log p is a technical artifact in the analysis of the Rademacher complexity. It can probably be dropped by a more careful analysis.

A.2 RESULTS WITH UNKNOWN COVARIANCE

The robust mean estimator constructed through JS-GAN can be easily made adaptive to unknown covariance structure. We define

1n

(, ) = argmin max
Rp,Ep(M ) DD

n log D(Xi) + EN(,) log(1 - D(Xi))
i=1

+ log 4,

(19)

where Ep(M ) = {  Ep :  op  M }. Compared with (14), (19) optimizes over both the mean and the covariance matrix. The estimator , as a result, is rate-optimal even when the true covariance matrix is not necessarily identity and is unknown.

Theorem A.2.

Consider the estimator  defined by (19) with D specified by (15).

Assume M

=

O(1),

p n

+

2  c for some sufficiently small constant c > 0, and set  = O

p n

+

. With i.i.d. observations

X1, ..., Xn  (1 - )N (, ) + Q, we have

- 2 C p  2 , n

with probability at least 1 - e-C (p+n 2) uniformly over all   Rp,   Ep(M ) and all Q. The constants C, C > 0 are universal.

Theorem A.2 enjoys the same error rate as in Theorem 3.2, even if the covariance matrix is not known. This is because the complexity of (19) is determined by the discriminator class through

max
DD

1 n

n

log D(Xi) - E log D(X) ,

i=1

regardless of the generator class. Similar results of adaptation also hold for D being deep neural network classes, and will be omitted in this paper.

13

Under review as a conference paper at ICLR 2019

B DETAILS OF EXPERIMENTS

B.1 TRAINING DETAILS

The implementation for JS-GAN is given in Algorithm 1, and a simple modification of the objective function leads to that of TV-GAN.

Algorithm

1

JS-GAN:

argmin

maxw

[

1 n

n i=1

log

Dw

(Xi)

+

E

log(1

-

Dw

(G

(Z

)))]

Input: Observation set S = {X1, . . . , Xn}  Rp, discriminator network Dw(x), generator network G(z) =

z + , learning rates d and g for the discriminator and the generator, batch size m, discriminator steps in each

iteration K, total epochs T , average epochs T0.

Initialization: Initialize  with coordinatewise median of S. Initialize w with N (0, .05) independently on each

element or Xavier (Glorot & Bengio, 2010).

1: for t = 1, . . . , T do

2: for k = 1, . . . , K do

3: Sample mini-batch {X1, . . . , Xm} from S. Sample {Z1, . . . , Zm} from N (0, Ip)

4:

gw



w

[

1 m

mi=1

log

Dw (Xi )

+

1 m

im=1

log(1

-

Dw (G (Zi )))]

5: w  w + dgw

6: end for

7: Sample {Z1, . . . , Zm} from N (0, Ip)

8:

g





[

1 m

mi=1

log(1

-

Dw (G (Zi )))]

9:    - gg

10: end for

Return: The average estimate  over the last T0 epochs.

Several important implementation details are discussed below.
· How to tune parameters? The choice of learning rates is crucial to the convergence rate, but the minimax game is hard to evaluate. We propose a simple strategy to tune hyper-parameters including the learning rates. Suppose we have estimators 1, . . . , M with corresponding discriminator networks Dw1 ,. . . , DwM . Fixing  = , we further apply gradient descent to Dw with a few more epochs (but not many in order to prevent overfitting, for example 10 epochs) and select the  with the smallest value of the objective function (14) (JS-GAN) or (12) (TV-GAN). We note that training discriminator and generator alternatively usually will not suffer from overfitting since the objective function for either the discriminator or the generator is always changing. However, we must be careful about the overfitting issue when training the discriminator alone with a fixed , and that is why we apply an early stopping strategy here. Fortunately, the experiments show if the structures of networks are same (then of course, the dimensions of the inputs are same), the choices of hyper-parameters are robust to different models and we present the critical parameters in Table 4 to reproduce the experiment results in Table 1 and Table 2.

· When to stop training? Judging convergence is a difficult task in GAN trainings, since sometimes oscillation may occur. In computer vision, people often use a task related measure and stop training once the requirement based on the measure is achieved. In our experiments below, we simply use a sufficiently large T which works well, but it is still interesting to explore an efficient early stopping rule in the future work.
· How to design the network structure? Although Theorem 3.1 and Theorem 3.2 guarantee the minimax rates of TV-GAN without hidden layer and JS-GAN with one hidden layer, one may wonder whether deeper network structures will perform better. From our preliminary experiments, TV-GAN with one hidden layer is significantly better than TV-GAN without any hidden layer. Moreover, JS-GAN

14

Under review as a conference paper at ICLR 2019

Table 4: Choices of hyper-parameters. The parameter  is the penalty factor for the regularization term (20)

and other parameters are listed in Algorithm 1. We apply Xavier initialization (Glorot & Bengio, 2010) for

both JS-GAN and TV-GAN trainings.

Structure

Net g

d K T T0 

100-20-1

JS 0.02 0.2 5 150 25 0 TV 0.0001 0.3 2 150 1 0.1

100-2-1

JS 0.01 0.2 5 150 25 0 TV 0.01 0.1 5 150 1 1

200-20-1

JS 0.02 0.2 5 200 25 0 TV 0.0001 0.1 2 200 1 0.5

200-200-100-1 JS 0.005 0.1 2 200 25 0

400-200-20-1 JS 0.02 0.05 2 250 25 0.5

with deep network structures can significantly improve over shallow networks especially when the dimension is large (e.g. p  200). For a network with one hidden layer, the choice of width may depend on the sample size. If we only have 5,000 samples of 100 dimensions, two hidden units performs better than five hidden units, which performs better than twenty hidden units. If we have 50,000 samples, networks with twenty hidden units perform the best.

· How to stabilize and accelerate TV-GAN? As we have discussed in Section 3.1, TV-GAN has a bad landscape when N (, Ip) and the contamination distribution Q are linearly separable (see Figure 1). An outlier removal step before training TV-GAN may be helpful. Besides, spectral normalization (Miyato et al., 2018) is also worth trying since it can prevent the weight from going to infinity and thus can increase the chance to escape from bad saddle points. To accelerate the optimization of TVGAN, in all the numerical experiments below, we adopt a regularized version of TV-GAN inspired by Proposition 3.1. Since a good feature extractor should match nonlinear moments of P = (1 - )N (, Ip) + Q and N (, Ip), we use an additional regularization term that can accelerate training and sometimes even leads to better performances. Specifically, let D(x) = sigmoid(wT (x)) be the discriminator network with w being the weights of the output layer and D(x) be the corresponding network after removing the output layer from D(x). The quantity D(x) is usually viewed as a feature extractor, which naturally leads to the following regularization term (Salimans et al., 2016; Mroueh et al., 2017), defined as

r(D, ) =

1 n

n2
T (D, Xi) - T (D, N (, Ip)) ,

i=1

(20)

where T (, P ) = EP (X) (moment matching) or T (, P ) = MedianXP D(X) (median matching).

B.2 SETTINGS OF CONTAMINATION Q
We introduce the contamination distributions Q used in the experiments. We first consider Q = N (µ, Ip) with µ ranges in {0.2, 0.5, 1, 5}. Note that the total variation distance between N (0p, Ip) and N (µ, Ip) is of order 0p - µ = µ . We hope to use different levels of µ to test the algorithm and verify the error rate in the worst case. Second, we consider Q = N (1.5  1p, ) to be a Gaussian distribution with a non-trivial covariance matrix . The covariance matrix is generated according to the following steps. First generate a sparse precision matrix  = (ij) with each entry ij = zij  ij, i  j, where zij and ij are independently generated from Uniform(0.4, 0.8) and Bernoulli(0.1). We then define ij = ji for all i > j and ¯ =  + (| min eig()| + 0.05)Ip to make the precision matrix symmetric and positive definite, where min eig() is the smallest eigenvalue of . The covariance matrix is  = ¯-1. Finally, we consider Q to be a Cauchy

15

Under review as a conference paper at ICLR 2019

distribution with independent component, and the jth component takes a standard Cauchy distribution with location parameter j = 0.5.
B.3 COMPARISON DETAILS
In Section 4.3, we compare GANs with the dimension halving (Lai et al., 2016) and iterative filtering (Diakonikolas et al., 2017).
· Dimension Halving. Experiments conducted are based on the code from https://github.com/ kal2000/AgnosticMeanAndCovarianceCode. The only hyper-parameter is the threshold in the outlier removal step, and we take C = 2 as suggested in the file outRemSperical.m.
· Iterative Filtering. Experiments conducted are based on the code from https://github.com/ hoonose/robust-filter. We assume is known and take other hyper-parameters as suggested in the file filterGaussianMean.m.
B.4 SUPPLEMENTARY EXPERIMENTS FOR NETWORK STRUCTURES
The experiments are conducted with i.i.d. observations drawn from (1 - )N (0p, Ip) + N (0.5  1p, Ip) with = 0.2. Table 5 summarizes results for p = 100, n  {5000, 50000} and various network structures. We
observe that TV-GAN that uses neural nets with one hidden layer improves over the performance of that without any hidden layer. This indicates that the landscape of TV-GAN might be improved by a more complicated network structure. However, adding one more layer does not improve the results. For JS-GAN, we omit the results without hidden layer because of its lack of robustness (Proposition 3.1). Deeper networks sometimes improve over shallow networks, but this is not always true. We also observe that the optimal choice of the width of the hidden layer depends on the sample size.

Table 5: Experiment results for JS-GAN and TV-GAN with various network structures.

Structure

n

JS-GAN

TV-GAN

100-1 100-20-1 100-50-1 100-20-20-1
100-1 100-2-1 100-5-1 100-20-1

50,000 50,000 50,000 50,000 5,000 5,000 5,000 5,000

0.0953 (0.0064) 0.2409 (0.0500) 0.1131 (0.0855)
0.1941 (0.0173) 0.2148 (0.0241) 0.3379 (0.0273)

0.1173 (0.0056) 0.1144 (0.0154) 0.1597 (0.0219) 0.1724 (0.0295) 0.9818 (0.0417) 0.1941 (0.0173) 0.2244 (0.0238) 0.3336 (0.0186)

B.5 TABLES FOR TESTING THE MINIMAX RATES Tables 6, 7 and 8 show numerical results corresponding to Figure 2.

16

Under review as a conference paper at ICLR 2019

Table 6: Scenario I: p/n < . Setting: p = 100, n = 50, 000, and from 0.05 to 0.20. Network structure of

JS-GAN: one hidden layer with 5 hidden units. Network structure of TV-GAN: zero-hidden layer. The number

in each cell is the average of 2 error  -  with standard deviation in parenthesis estimated from 10 repeated

experiments. The bold character marks the worst case among our choices of Q at each level. The results of

TV-GAN for Q = N (5  1p, Ip) are highlighted in slanted font. The failure of training in this case is due to the

bad landscape when N (0p, Ip) and Q are linearly separable, as discussed in Section 3.1 (see Figure 1).

Q Net = 0.05

= 0.10

= 0.15

= 0.20

N (0.2  1p, Ip)

JS 0.1025 (0.0080) 0.1813 (0.0122) 0.2632 (0.0080) TV 0.1110 (0.0204) 0.2047 (0.0112) 0.2769 (0.0315)

0.3280 (0.0069) 0.3283 (0.0745)

N (0.5  1p, Ip)

JS 0.1407 (0.0061) 0.1895 (0.0070) 0.1714 (0.0502) TV 0.2003 (0.0480) 0.2065 (0.1495) 0.2088 (0.0100)

0.1227 (0.0249) 0.3985 (0.0112)

N (1p, Ip)

JS 0.0855 (0.0054) 0.1055 (0.0322) 0.0602 (0.0133) 0.0577 (0.0029) TV 0.1084 (0.0063) 0.0842 (0.0036) 0.3228 (0.0123) 0.1329 (0.0125)

N (5  1p, Ip)

JS 0.0587 (0.0033) 0.0636 (0.0025) 0.0625 (0.0045) 0.0591 (0.0040) TV 1.2886 (0.5292) 4.4511 (0.8754) 7.3868 (0.8081) 10.5724 (1.2605)

Cauchy(0.5  1p)

JS TV

0.0625 (0.0045) 0.2280 (0.0067)

0.0652 (0.0044) 0.3842 (0.0083)

0.0648 (0.0035) 0.5740 (0.0071)

0.0687 (0.0042) 0.7768 (0.0074)

N (0.5  1p, )

JS 0.1490 (0.0061) 0.1958 (0.0074) 0.2379 (0.0076) TV 0.2597 (0.0090) 0.4621 (0.0649) 0.6344 (0.0905)

0.1973 (0.0679) 0.7444 (0.3115)

Table 7: Scenario II-a:

same as above. Q Net

N (0.2  1p, Ip)

JS TV

N (0.5  1p, Ip)

JS TV

N (1p, Ip)

JS TV

N (5  1p, Ip)

JS TV

Cauchy(0.5  1p)

JS TV

N (0.5  1p, )

JS TV

p/n > . Setting: n = 1, 000, = 0.1, and p from 10 to 100. Other details are the

p = 10
0.1078 (0.0338) 0.2828 (0.0580) 0.1587 (0.0438) 0.2864 (0.0521) 0.1644 (0.0255) 0.3733 (0.0878) 0.0938 (0.0195) 0.3707 (0.2102) 0.1188 (0.0263) 0.3198 (0.1543) 0.1805 (0.0220) 0.3036 (0.0736)

p = 25
0.1819 (0.0215) 0.4740 (0.1181) 0.2684 (0.0386) 0.5024 (0.1038) 0.2177 (0.0480) 0.5407 (0.0634) 0.2058 (0.0218) 0.7434 (0.3313) 0.1855 (0.0282) 0.5205 (0.1049) 0.2692 (0.0318) 0.5152 (0.0707)

p = 50
0.3355 (0.0470) 0.5627 (0.0894) 0.4213 (0.0356) 0.6878 (0.1146) 0.3505 (0.0552) 0.9061 (0.1029) 0.3316 (0.0462) 1.1532 (0.3488) 0.2967 (0.0284) 0.6240 (0.0652) 0.3885 (0.0339) 0.7305 (0.0966)

p = 75
0.4806 (0.0497) 0.8217 (0.0382) 0.5355 (0.0634) 0.9204 (0.0589) 0.4740 (0.0742) 1.0672 (0.0629) 0.4054 (0.0690) 1.1850 (0.3739) 0.4094 (0.0385) 0.7536 (0.0673) 0.5144 (0.0547) 0.9460 (0.0900)

p = 100
0.5310 (0.0414) 0.8090 (0.0457) 0.6825 (0.0981) 0.9418 (0.0551) 0.6662 (0.0611) 1.1150 (0.0942) 0.5553 (0.0518) 1.3257 (0.1721) 0.4826 (0.0479) 0.7612 (0.0613) 0.6833 (0.1094) 1.0888 (0.0863)

17

Under review as a conference paper at ICLR 2019

Table 8: Scenario II-b:

same as above. Q Net

N (0.2  1p, Ip)

JS TV

N (0.5  1p, Ip) N (1p, Ip)
N (5  1p, Ip) Cauchy(0.5  1p) N (0.5  1p, )

JS TV JS TV JS TV JS TV JS TV

p/n > . Setting: p = 50, = 0.1, and n from 50 to 1, 000. Other details are the

n = 50
1.3934 (0.5692) 1.9714 (0.1552) 1.6422 (0.6822) 1.9780 (0.2157) 1.8427 (0.9633) 1.9907 (0.1498) 2.6392 (1.3877) 2.1050 (0.3763) 1.6563 (0.5246) 2.1031 (0.2300) 1.2296 (0.3157) 1.9243 (0.2079)

n = 100
1.0055 (0.1040) 1.2629 (0.0882) 1.2101 (0.2826) 1.2485 (0.0668) 1.2179 (0.2782) 1.4575 (0.1270) 1.3966 (0.5370) 1.5205 (0.2221) 1.0857 (0.3613) 1.1712 (0.1493) 0.7696 (0.0786) 1.2217 (0.0681)

n = 200
0.8373 (0.1335) 0.7579 (0.0486) 0.8374 (0.1021) 0.8198 (0.0778) 1.0147 (0.2170) 0.9724 (0.0802) 0.9633 (0.1383) 1.1909 (0.2273) 0.8944 (0.1759) 0.6904 (0.0763) 0.5892 (0.0931) 0.7939 (0.0688)

n = 500
0.4781 (0.0677) 0.6640 (0.0689) 0.5832 (0.0595) 0.7597 (0.0456) 0.5586 (0.1013) 0.9050 (0.1479) 0.5360 (0.0808) 1.0957 (0.1390) 0.5363 (0.0593) 0.6300 (0.0642) 0.5015 (0.0831) 0.7033 (0.0414)

n = 1000
0.3213 (0.0401) 0.6348 (0.0547) 0.3930 (0.0485) 0.7346 (0.0750) 0.3639 (0.0464) 0.8747 (0.0757) 0.3265 (0.0336) 1.0695 (0.2639) 0.3832 (0.0408) 0.5085 (0.0662) 0.4085 (0.0209) 0.7125 (0.0490)

C PROOFS OF PROPOSITION 2.1 AND PROPOSITION 3.1

In the first example, consider

Q = {N (, Ip) :   Rp} , Q = {N (, Ip) :  -   r} .

In other words, Q is the class of Gaussian location family, and Q is taken to be a subset in a local neighborhood
of N (, Ip). Then, with Q = N (, Ip) and Q = N (, Ip), the event q(X)/q(X)  1 is equivalent to X -  2  X -  2. Since  -   r, we can write  =  + ru for some r  R and u  Rp that satisfy 0  r  r and u = 1. Then, (8) becomes

 = argmin sup
Rp u =1

1n nI
i=1

uT (Xi

-

)



r 2

-P

N (0, 1)  r 2

0rr

Letting r  0, we obtain (9), the exact formula of Tukey's median.

The next example is a linear model y|X  N (XT , 1). Consider the following classes

.

(21)

Q= Q=

Py,X = Py|X PX : Py|X = N (XT , 1),   Rp , Py,X = Py|X PX : Py|X = N (XT , 1),  -   r .

Here, Py,X stands for the joint distribution of y and X. The two classes Q and Q share the same marginal distribution PX and the conditional distributions are specified by N (XT , 1) and N (XT , 1), respectively. Follow the same derivation of Tukey's median, let r  0, and we obtain the exact formula of regression depth (10). It is worth noting that the derivation of (10) does not depend on the marginal distribution PX .
The last example is on covariance/scatter matrix estimation. For this task, we set Q = {N (0, ) :   Ep}, where Ep is the class of all p × p covariance matrices. Inspired by the derivations of Tukey depth and regression depth, it is tempting to choose Q in the neighborhood of N (0, ). However, a native choice would lead to a definition that is not even Fisher consistent. We propose a rank-one neighborhood, given by

Q = N (0, ) : -1 = -1 + ruuT  Ep, |r|  r, u = 1 .

(22)

Then, a direct calculation gives

I

dN (0, ) (X)  1 dN (0, )

= I r|uT X|2  log(1 + ruT u) .

(23)

18

Under review as a conference paper at ICLR 2019

Since

limr0

log(1+ruT u) ruT u

=

1, the limiting event of (23) is either I{|uT X|2



uT u} or I{|uT X|2



uT u}, depending on whether r tends to zero from left or from right. Therefore, with the above Q and Q, (8)

becomes (11) under the limit r  0. Even though the definition of (22) is given by a rank-one neighborhood of the inverse covariance matrix, the formula (11) can also be derived with -1 = -1 + ruuT in (22) replaced

by  =  + ruuT by applying the Sherman-Morrison formula. A similar formula to (11) in the literature is

given by

 = argmax inf
Ep u =1

1 n

n

I{|uT

Xi|2



uT

u}



1 n

n

I{|uT Xi|2  uT u}

i=1 i=1

,

(24)

which is recognized as the maximizer of what is known as the matrix depth function (Zhang, 2002; Chen

et al., 2018; Paindaveine & Van Bever, 2017). The  in (24) is a scalar defined through the equation P(N (0, 1)  ) = 3/4. It is proved in Chen et al. (2018) that  achieves the minimax rate under Hu-

ber's -contamination model. While the formula (11) can be derived from TV-Learning with discriminators

in the form of I

dN dN

(0,) (0,)

(X

)



1

, a special case of (6), the formula (24) can be derived directly from TV-

GAN with discriminators in the form of I

dN (0,) dN (0,)

(X )



1

by following a similar rank-one neighborhood

argument. This completes the derivation of Proposition 2.1.

To prove Proposition 3.1, we define F (w) = EP log sigmoid(wT g(X)) + EQ log(1 - sigmoid(wT g(X))) + log 4, so that JSg(P, Q) = maxwW F (w). The gradient and Hessian of F (w) are given by

e-wT g(X)

ewT g(X)

F (w) = EP 1 + e-wT g(X) g(X) - EQ 1 + ewT g(X) g(X),

2F (w)

=

-EP

(1

ewT g(X) + ewT g(X

)

)2

g

(X

)g

(X

)T

-

EQ (1

e-wT g(X) + e-wT g(X

)

)2

g(X

)g(X

)T

.

Therefore, F (w) is concave in w, and maxwW F (w) is a convex optimization with a convex W. Suppose JSg(P, Q) = 0. Then maxwW F (w) = 0 = F (0), which implies F (0) = 0, and thus we have EP g(X) = EQg(X). Now suppose EP g(X) = EQg(X), which is equivalent to F (0) = 0. Therefore, w = 0 is a stationary point of a concave function, and we have JSg(P, Q) = maxwW F (w) = F (0) = 0.

D PROOFS OF MAIN RESULTS
In this section, we present proofs of all main theorems in the paper. We first establish some useful lemmas in Section D.1, and the the proofs of main theorems will be given in Section D.2.

D.1 SOME AUXILIARY LEMMAS

Lemma D.1. Given i.i.d. observations X1, ..., Xn  P and the function class D defined in (13), we have for any  > 0,

sup
DD

1 n

n

D(Xi) - ED(X)

i=1

C

p log(1/) +,
nn

with probability at least 1 -  for some universal constant C > 0.

Proof.

Let f (X1, ..., Xn) = supDD

1 n

n i=1

D(Xi) - ED(X)

.

It

is

clear

that

f (X1, ..., Xn)

satisfies

the

bounded difference condition. By McDiarmid's inequality (McDiarmid, 1989), we have

f (X1, ..., Xn)  Ef (X1, ..., Xn) +

log(1/) ,
2n

19

Under review as a conference paper at ICLR 2019

with probability at least 1 - . Using a standard symmetrization technique (Pollard, 2012), we obtain the following bound that involves Rademacher complexity,

Ef (X1, ..., Xn)  2E sup
DD

1n n
i=1

iD(Xi)

,

(25)

where 1, ..., n are independent Rademacher random variables. The Rademacher complexity can be bounded by Dudley's integral entropy bound, which gives

1n

E sup
DD

n
i=1

iD(Xi)

E

1 n

2 0

log N (, D, · n)d,

where N (, D, · n) is the -covering number of D with respect to the empirical 2 distance f -

gn =

1 n

n i=1

(f

(Xi)

-

g(Xi))2.

Since the VC-dimension of D is O(p), we have N (, D, ·

n) p (16e/)O(p) (see Theorem 2.6.7 of Van Der Vaart & Wellner (1996)). This leads to the bound

1 2 n0

log N (, D, · n)d

p n

,

which

gives

the

desired

result.

Lemma D.2. Given i.i.d. observations X1, ..., Xn  P, and the function class D defined in (15), we have for

any  > 0,

sup
DD

1 n

n

log D(Xi) - E log D(X)

i=1

 C

p log(1/) +,
nn

with probability at least 1 -  for some universal constant C > 0.

Proof.

Let f (X1, ..., Xn) = supDD

1 n

n i=1

log

D(Xi)

-

E

log

D(X )

.

Since

sup sup | log(2D(x))|  ,
DD x

we have

sup |f (x1, ..., xn)
x1 ,...,xn ,xi

-

f (x1, ..., xi-1, xi, xi+1, ..., xn)|



2 .
n

Therefore, by McDiarmid's inequality (McDiarmid, 1989), we have

f (X1, ..., Xn)  Ef (X1, ..., Xn) + 

2 log(1/) ,
n

(26)

with probability at least 1 - . By the same argument of (25), it is sufficient to bound the Rademacher complex-

ity E supDD

1 n

n i=1

i log(2D(Xi)) . Since the function (x) = log(2sigmoid(x)) has Lipschitz constant

1 and satisfies (0) = 0, we have

1n

E sup
DD

n
i=1

i log(2D(Xi))

 2E

sup
j1 |wj |,uj Rp,bj R

1n n
i=1

i wj (ujT Xi + bj )
j1

,

which uses Theorem 12 of Bartlett & Mendelson (2002). By Ho¨lder's inequality, we further have

E

sup
j1 |wj |,uj Rp,bj R

1n n
i=1

i wj (uTj Xi + bj )
j1



E max sup
j1 uj Rp,bj R

1n n
i=1

i(ujT Xi + bj )

=

E sup
uRp ,bR

1n n
i=1

i(uT Xi + b)

.

20

Under review as a conference paper at ICLR 2019

Note that for a monotone function  : R  [0, 1], the VC-dimension of the class {(uT x + b) : u  R, b  R}

is O(p). Therefore, by using the same argument of Dudley's integral entropy bound in the proof Lemma D.1,

we have

E sup
uRp ,bR

1n n
i=1

i(uT Xi + b)

p ,
n

which leads to the desired result.

Lemma D.3. Given i.i.d. observations X1, .., Xn  N (, Ip) and the function class FLH (, , B). Assume    log p and set  = p log p. We have for any  > 0,

sup
DFLH (,,B)

1 n

n

log D(Xi) - E log D(X)

i=1

 C

1 + (2B)L-1

p log p ,
n

with probability at least 1 - p-C for some universal constants C, C > 0.

Proof. We use the notation Zi = Xi -   N (0, Ip) for i = 1, ..., n. Define

1n

f (Z1, ..., Zn) = sup
DFLH (,)

n log D(Zi + ) - E log D(X)
i=1

.

We first show that f (Z1, ..., Zn) is a Lipschitz function. For any D  FLH (, , B), we can write D(x) =

sigmoid

j1 wjReLU ( j(x)) with j(x) =

p h=1

ujhgjh(x)

+

bj .

Then,

|f (Z1, ..., Zn) - f (Y1, ..., Yn)|

  



1n n
i=1

sup
w 1, j

log sigmoid  wjReLU ( j(Zi + )) - log sigmoid  wjReLU ( j(Yi + ))

j1

j1



1n n
i=1

sup
w 1, j

wjReLU ( j(Zi + )) - wjReLU ( j(Yi + ))

j1

j1



1 
n

n i=1

max sup |ReLU(
j1 j

j (Zi

+

))

-

ReLU(

j (Yi

+

))|

=

1 
n

n

sup |ReLU( (Zi + )) - ReLU( (Yi + ))|

i=1

1n

2p 2p



 n
i=1

sup
u 22,ghGLH-1(B)

uhgh(Zi + ) - uhgh(Yi + )

h=1

h=1



1 4p
n

n i=1

sup |g(Zi
g GLH-1 (B )

+

) - g(Yi

+

)| .

We use induction to bound supgGLH-1(B) |g(Zi + ) - g(Yi + )|. Since
sup |g(Zi + ) - g(Yi + )|
gG1H (B)
 sup vT (Zi + ) - vT (Yi + )
v 1B
 B Zi - Yi ,

21

Under review as a conference paper at ICLR 2019

and

sup |g(Zi + ) - g(Yi + )|
g GlH+1 (B )

HH

 sup

vhgh(Zi + ) - vhgh(Yi + )

v 1B,ghGlH (B) h=1

h=1

 B sup |g(Zi + ) - g(Yi + )|,
gGlH (B)

we have

sup |g(Zi + ) - g(Yi + )|  BL-1 Zi - Yi ,
g GLH-1 (B )

which leads to the bound

|f (Z1, ..., Zn) - f (Y1, ..., Yn)|



B

L-1

 p

1

n

n

Zi - Yi 

i=1

 BL-1 p n

n

Zi - Yi

2 

i=1

 BL-1 p n

n
Zi - Yi 2.

i=1

Therefore, f (Z1, ..., Zn) is a Lipschitz function with Lipschitz constant BL-1

p n

.

By

Talagrand's

inequality

(Talagrand, 1995), we have

f (Z1, ..., Zn)  Ef (Z1, ..., Zn) + CBL-1

p log
n

2 ,


with probability at least 1 - . By the same argument of (25), it is sufficient to bound the Rademacher complexity. We have

1n

E

sup DFLH (,,B) n i=1

i log D(Zi + )

  

1n

=

E

sup
w 1,

j

n

i=1

i log sigmoid  wjReLU( j(Zi + ))
j1





1n



E

sup
w 1,

j

n

i=1

i  wjReLU( j(Zi + ))
j1

1n



E

max sup
j1 j

n
i=1

iReLU( j(Zi + ))



2E

1n sup
n

iReLU( (Zi + ))

,

i=1

22

Under review as a conference paper at ICLR 2019

where we have used Theorem 7 of Meir & Zhang (2003). We continue the bound, and we have

1n

E

sup n

iReLU( (Zi + ))

i=1

=E

1n

sup nu 22,|b|,ghGLH-1(B) i=1

iReLU

2p
uhgh(Zi + ) + b
h=1

(27)

E

1n sup nu 22,|b|,ghGLH-1(B) i=1

i

2p
uhgh(Zi + ) + b
h=1



 4 pE

1n

sup ngGLH-1(B) i=1

ig(Zi + )

1n

+ E n

i.

i=1

We bound E

supg GLH-1 (B )

1 n

n i=1

ig(Zi + )

by induction. Since

1n

E

sup gG1H (B) n i=1

ig(Zi + )

E

1n sup v 1B n i=1

ivT (Zi + )



B

1n En

iZi

1n +  E n

 i=1



i=1

 CB log p+   ,

n

i

and

1n

E

sup ngGlH+1(B) i=1

ig(Zi + )

E

1n

H

sup v n1B,ghGlH (B) i=1

i vhgh(Zi + )
h=1

1n



BE

sup
gGlH (B)

n
i=1

ig(Zi + )



2BE

1n

sup gGlH (B) n i=1

ig(Zi + )

,

we have

E

1n

sup ngGLH-1(B) i=1

ig(Zi + )

  C(2B)L-1 log p+   .
n

Combining the above inequalities, we get

1n

E

sup DFLH (,,B) n i=1

i log D(Zi + )

 C

 p(2B)L-1 log p+   + 
nn

This leads to the desired result under the conditions on  and   with  set to satisfy log(2/)

. log p.

Lemma D.4. Given i.i.d. observations X1, .., Xn  N (, Ip) and the function class F¯LH (, , B). Assume    log p and set  = p log p. We have for any  > 0,

sup
DF¯LH (,,B)

1 n

n

log D(Xi) - E log D(X)

i=1

 C

(2B)L-1

p log p +
n

log(1/) ,
n

23

Under review as a conference paper at ICLR 2019

with probability at least 1 -  for some universal constants C > 0.

Proof. The proof is largely a combination of the arguments in the proofs of Lemma D.2 and Lemma D.3. Write

f (X1, ..., Xn) = supDF¯LH (,,B)

1 n

n i=1

log

D(Xi

)

-

E

log

D(X

)

.

Then,

the

inequality

(26)

holds

with

probability at least 1 - . It is sufficient to analyze the Rademacher complexity. Using the fact that the function

log(2sigmoid(x)) is Lipschitz and Ho¨lder's inequality, we have

1n

E sup
DF¯LH (,,B)

n
i=1

i log(2D(Xi))

1n

2p



2E

sup
w 1, uj 22,|bj |,gjhGLH-1(B)

n
i=1

i wjsigmoid
j1

ujhgjh(Xi) + bj
h=1

1n

2p



2E

sup
u 2,|b|,ghGLH-1(B)

n
i=1

isigmoid

uhgh(Xi) + b
h=1

.

The bound of the term E sup u 2,|b|,ghGLH-1(B)

1 n

n i=1

isigmoid

same analysis that bounds (27), and we get

2p h=1

uhgh(Xi)

+

b

follows the

1n

E

sup
u 2,|b|,ghGLH-1(B)

n
i=1

isigmoid

2p
uhgh(Xi) + b
h=1





C

 p(2B

)L-1

log p+   .

n

This leads to the desired conclusion.

D.2 PROOFS OF MAIN THEOREMS

Proof of Theorem 3.1. We first introduce some notations. Define F (P, ) = maxw,b Fw,b(P, ), where Fw,b(P, ) = EP sigmoid(wT X + b) - EN(,Ip)sigmoid(wT X + b).

With this definition, we have  = argmin F (Pn, ), where we use Pn for the empirical distribution

1 n

n i=1

Xi .

We

shorthand

N

(,

Ip)

by

P ,

and

then

F (P, )  F ((1 - )P + Q, ) +

 F (Pn, ) + + C

p +
n

log(1/) n

(28) (29)

 F (Pn, ) + + C

p log(1/) +
nn

(30)

 F ((1 - )P + Q, ) + + 2C

p log(1/) +
nn

(31)

 F (P, ) + 2 + 2C

p log(1/) +
nn

(32)

= 2 + 2C

p log(1/) +.
nn

(33)

With probability at least 1 - , the above inequalities hold. We will explain each inequality. Since F ((1 - )P + Q, ) = max [(1 - )Fw,b(P, ) + Fw,b(Q, )] ,
w,b

24

Under review as a conference paper at ICLR 2019

we have sup |F ((1 - )P + Q, ) - F (P, )|  ,

which implies (28) and (32). The inequalities (29) and (31) are implied by Lemma D.1 and the fact that

sup |F (Pn, ) - F ((1 -


)P +

Q, )|  sup
w,b

1 n

n

sigmoid(wT Xi + b) - Esigmoid(wT X + b)

i=1

.

The inequality (30) is a direct consequence of the definition of . Finally, it is easy to see that F (P, ) = 0, which gives (33). In summary, we have derived that with probability at least 1 - ,

Fw,b(P, )  2 + 2C

p log(1/) +,
nn

for all w  Rp and b  R. For any u  Rp such that u = 1, we take w = u and b = -uT , and we have

f (0) - f (uT ( - ))  2 + 2C

p log(1/) +,
nn

where f (t) =

1 1+ez+t

(z

)dz,

with

(·)

being

the

probability

density

function

of

N (0, 1).

It

is

not

hard

to

see that as long as |f (t) - f (0)|  c for some sufficiently small constant c > 0, then |f (t) - f (0)|  c |t| for

some constant c > 0. This implies

-

= sup |uT ( - )|
u =1
 1 sup f (0) - f (uT ( - )) c u =1

p ++
n with probability at least 1 - . The proof is complete.

log(1/) ,
n

Proof of Theorem 3.2. We continue to use P to denote N (, Ip). Define

F (P, ) = max Fw,u,b(P, ),
w 1,u,b

where

Fw,u,b(P, ) = EP log D(X) + EN(,Ip) log (1 - D(X)) + log 4,

with D(x) = sigmoid j1 wj(ujT x + bj) . Then,

F (P, )  F ((1 - )P + Q, ) + 2

 F (Pn, ) + 2 + C

p +
n

log(1/) n

 F (Pn, ) + 2 + C

p log(1/) +
nn

 F ((1 - )P + Q, ) + 2 + 2C

p log(1/) +
nn

 F (P, ) + 4 + 2C

p log(1/) +
nn

= 4 + 2C

p log(1/) +.
nn

(34) (35) (36) (37) (38)

25

Under review as a conference paper at ICLR 2019

The inequalities (34)-(38) follow similar arguments for (28)-(32). To be specific, (35) and (37) are implied by Lemma D.2, and (36) is a direct consequence of the definition of . To see (34) and (38), note that for any w such that w 1  , we have

| log(2D(X))|  wj(uTj X + bj)  .
j1
A similar argument gives the same bound for | log(2(1 - D(X)))|. This leads to
sup |F ((1 - )P + Q, ) - F (P, )|  2 ,

which further implies (34) and (38). To summarize, we have derived that with probability at least 1 - ,

Fw,u,b(P, )  4 + 2C

p log(1/) +,
nn

for all w 1  , uj  1 and bj. Take w1 = , wj = 0 for all j > 1, u1 = u for some unit vector u and b1 = -uT , and we get

fuT (-)()  4 + 2C

p log(1/) +,
nn

(39)

where

22 f(t) = E log 1 + e-t(Z) + E log 1 + et(Z+) ,

with Z  N (0, 1). Direct calculations give

(40)

f (t)

=

E1

e-t(Z) + e-t(Z)

 (Z )

-

E1

et(Z+) + et(Z+)

(Z

+

),

f (t)

=

-E(Z

)2

(1

e-t(Z) + e-t(Z))2

-

E(Z

+

)2 (1

et(Z+) + et(Z+))2 .

(41)

Therefore,

f (0)

=

0,

f (0)

=

1 2

(E (Z )

-

E(Z

+

)),

and

f

(t)



-

1 2

.

By

the

inequality

f ()



f (0)

+

f (0)

-

1 2, 4

we have f(0)  f() + 2/4. In view of (39), we have

 (z)(z)dz - (z + uT ( - ))(z)dz 2

 4 + 2C

p log(1/) 2 + +.
nn4

It is easy to see that for the choices of (·), (z)(z)dz - (z + t)(z)dz is locally linear with respect to t. This implies that

  -  =  sup uT ( - ) 
u =1

Therefore, with a 

p n

+

, the proof is complete.

+

p +

log(1/) + 2.

nn

Proof of (17) in Theorem A.1. We continue to use P to denote N (, Ip). For i.i.d. observations from (1 - )P + Q, they can be written as {Xi}in=1 = {Yi}in=11  {Zi}ni=21. Marginally, we have n2  Binomial(n, )

26

Under review as a conference paper at ICLR 2019

and n1 = n - n2. Conditioning on n1 and n2, {Yi}ni=11 are i.i.d. from P and {Zi}in=21 are i.i.d. from Q. According to Lemma 7.1 of Chen et al. (2018), we have

n2  n1 1 -

25 +
12

1 log(1/),
2n

(42)

with probability at least 1 -  as long as +

log(1/) n

is

sufficiently

small.

Below,

the

analysis

will

condition

on n1, n2 that satisfy the event (42). Define F (P, ) = supDFLH(,,B) FD(P, ), with

FD(P, ) = EP log D(X) + EN(,Ip) log(1 - D(X)) + log 4.

(43)

We also use the notations F ({Xi}in=1, ) and F ({Yi}in=11, ) for P

=

1 n

n i=1

Xi

and P

=

1 n1

n1 i=1

Yi

,

respectively. It is not hard to see that

sup


n n1

F

({Xi

}in=1

,



)

-

F

({Yi

}ni=1 1

,



)

 (log 4) n2 . n1

(44)

Then,

F (P, )  F ({Yi}in=11, ) + C 1 + (2B)L-1

p log p n1



n n1

F

({Xi}ni=1

,

)

+

(log 4) n2 n1

+

C

1 + (2B)L-1

p log p n1



n n1

F

({Xi}in=1

,

)

+

(log 4) n2 n1

+

C

1 + (2B)L-1

p log p n1



F ({Yi}ni=11, )

+

2(log 4) n2 n1

+

C

1 + (2B)L-1

p log p n1



F

(P

,

)

+

2(log

4)

n2 n1

+ 2C

1 + (2B)L-1

p log p n1

= 2(log 4) n2 + 2C 1 + (2B)L-1 n1

p log p .
n1

(45) (46) (47) (48) (49)

The inequalities (45) and (49) are due to Lemma D.3. The inequalities (46) and (48) are implied by (44), and (47) uses the definition of . Together with (42), we have derived that with probability at least 1 -  - p-C ,

FD(P, )  C

+ log(1/) + C 1 + (2B)L-1 n

p log p ,
n

(50)

for any D  FLH (, , B). Choose w1 =  and wj = 0 for all wj > 1. For any unit vector u  Rp, take

u1h = -u1(h+p) = uh for h = 1, ..., p and b1 = -uT . For h = 1, ..., p, set g1h(x) = max(xh, 0). For

h=p |b1| 

+ 1, 

..., 

2p, p

set g1h(x)= max(-xh-p, 0).    p log p. We need to

It is obvious that such u show both the functions

and b satisfy max(x, 0) and

h u21h  max(-x,

2 and 0) are

elements of GLH-1(B). This can be proved by induction. It is obvious that max(xh, 0), max(-xh, 0)  G1H (B)

for any h = 1, ..., p. Suppose we have max(xh, 0), max(-xh, 0)  GlH (B) for any h = 1, ..., p. Then,

max (max(xh, 0) - max(-xh, 0), 0) = max(xh, 0), max (max(-xh, 0) - max(xh, 0), 0) = max(-xh, 0).

Therefore, max(xh, 0), max(-xh, 0)  GlH+1(B) as long as B  2. Hence, the above construction satisfies D(x) = sigmoid((uT (x - )))  FLH (, , B), and we have

fuT (-)()  C

+ log(1/) + C 1 + (2B)L-1 n

p log p ,
n

(51)

27

Under review as a conference paper at ICLR 2019

where the definition of f(t) is given by (40) with Z  N (0, 1). Suppose f()  c for a sufficiently constant
c > 0, we get || < C1 for some constant C1 > 0. Together with the formula (41), we have f (t)  -C2 for some constant C2 > 0 whenever || < C1. Apply the a similar in the proof of Theorem 3.2, we obtain

The choice 

 -

+

log(1/) +

1 + (2B)L-1

n

1/2 + n-1/4 gives the desired result.

p log p + 2. n

Proof of (18) in Theorem A.1. Follow the same argument in the proof of Theorem 3.2, use Lemma D.4, and we

have

FD(P, )  C

+ (2B)L-1

p log p +

log(1/) ,

nn

uniformly over D  F¯LH (, , B) with probability at least 1 - , where FD(P, ) is defined in (43). Then, following the same argument after (50), we obtain the desired conclusion.

Proof of Theorem A.2. We use P, to denote the distribution N (, ). max w 1,u,b Fw,u,b(P, (, )), where

Define F (P, (, ))

Fw,u,b(P, (, )) = EP log D(X) + EN(,) log (1 - D(X)) + log 4,

=

with D(x) = sigmoid get

j1 wj(ujT x + bj) . Then, following the same arguments that derive (34)-(38), we

F (P,, (, ))  F (P,, (, )) + 4 + 2C

p log(1/) +
nn

= 4 + 2C

p log(1/) +.
nn

In other words, we have with probability at least 1 - ,

Fw,u,b(P,, (, ))  4 + 2C

p log(1/) +,
nn

for all w 1  , uj  1 and bj. Take w1 = , wj = 0 for all j > 1, u1 = u for some unit vector u and b1 = -uT , and we get

f   ()  4 + 2C
uT (-), uT u, uT u

p log(1/) +,
nn

where

22

f,a,b(t)

=

E log

1+

e-t(aZ)

+ E log

, 1 + et(bZ+)

with Z  N (0, 1). Then, by similar calculations in the proof of Theorem 3.2, we have



 ( uT uz)(z)dz - ( uT uz + uT ( - ))(z)dz

2

 4 + 2C

p log(1/) 2 + +.
nn4

If we can show 
sup ( uT uz)(z)dz -
u =1

( uT uz + uT ( - ))(z)dz

sup uT ( - ),
u =1

(52)

28

Under review as a conference paper at ICLR 2019

the

proof

will

be

complete.

Since

(·)

-

1 2

is

an

odd

function,

we

have

 ( uT uz)(z)dz

=

1

+

2

 ( uT uz)

-

1

1 (z)dz = .

22

 Now consider the function h() = (bz + )(z)dz, where b = uT u  M = O(1). The fact that |h() - 1/2| is sufficiently small implies that || = O(1). Then, by the local linearity of h(), we have |h() - 1/2| = |h() - h(0)| ||, which implies (52). Thus, the proof is complete.

29

