Under review as a conference paper at ICLR 2019
ON THE TRAJECTORY OF STOCHASTIC GRADIENT DESCENT IN THE INFORMATION PLANE
Anonymous authors Paper under double-blind review
ABSTRACT
Studying the evolution of information theoretic quantities during Stochastic Gradient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained popularity in recent years. Nevertheless, this type of experiments require estimating mutual information and entropy which becomes intractable for moderately large problems. In this work we propose an experimental framework for understanding SGD based only on the output labels of ANNs. We look at SGD learning as a trajectory in the space of probability measures and define a notion of shortest learning path using a total variation metric. Using this formulation we provide a connection between learning and Markov processes that allows us characterize the trajectory of information theoretic quantities during learning. In addition, a simple Markov chain model for SGD learning, that moves along the shortest learning path is constructed and compared with SGD through empirical simulations. Experiments show that SGD moves in a similar trajectory as a Markov chain along the shortest learning path.
1 INTRODUCTION
How do information theoretic quantities behave during the training of ANNs? This question was addressed by Shwartz-Ziv & Tishby (2017) in an attempt to explain the learning through the lens of the information bottleneck method Tishby et al. (1999). In that work, the layers of an ANNs are considered random variables forming a Markov chain. The authors constructed a 2D information plane by estimating the mutual information values between hidden layers, inputs, and outputs of ANNs. Using this approach it was observed that the information bottleneck method provides an approximate explanation for Gradient Descent (GD) learning. In addition, their experiments showed the role of compression in learning. That initial paper motivated further work on this line of research (Saxe et al., 2018; Gabrie´ et al., 2018). The main practical limitation of that type of experiments is that it requires estimating mutual information between high dimensional continuous random variables. This becomes prohibitive as soon we move to moderately large problems, such as the CIFAR-100 dataset, where the large ANNs are employed. Other works dealing with information theoretic quantities tend to have this experimental limitations. For instance, Russo & Zou (2015); Xu & Raginsky (2017); Asadi et al. (2018) used generic chaining techniques to show that generalization error can be upper bounded by the mutual information between the training dataset and output of the learning algorithm. Nevertheless, estimating that mutual information to verify those results experimentally becomes intractable. Furthermore, in our previous work (Anonymous, 2018) we defined a novel 2D information plane that only requires to estimate information theoretic quantities between the correct and estimated labels. Since these random variables are discrete and one-dimensional, this framework can be used to study learning in large recognition problems as well. Moreover, that work provides a preliminary empirical study on the behavior of those information theoretic quantities during learning along with some connections between error and conditional entropy.
In this work, we extend the experiments from Anonymous (2018) to more general scenarios and aim to characterize the observed behavior of GD. To that end we provide theoretical and experimental justifications for constructing a simple Markovian model for learning, and compare it with GD through experiments. These experiments are conducted using various datasets such as MNIST (LeCun et al., 1998), CIFAR-10/ CIFAR-100, spirals (Anonymous, 2018), as well as different ANN architectures like Fully Connected Neural Networks (FCNNs), LeNet-5 (LeCun et al., 1999), and densenet (Hinton et al., 2012). Our main contributions are:
1

Under review as a conference paper at ICLR 2019
dataset
Figure 1: System model.
· We define a 2D information plane, inspired by the works of Shwartz-Ziv & Tishby (2017), and use it to study the behavior of ANNs during SGD learning.
· We propose looking at SGD as a trajectory in the space of probability measures and define the notion of shortest learning path in such space using a total variation metric.
· It is shown that the random variables involved in gradient based learning have Markov relations.
· A Markov chain than moves along the shortest learning path is constructed and compared with SGD. We consider the effect of noisy labels in order to boost the diversity of these experiments.
· Experiments show that SGD moves in a similar trajectory as the shortest learning path.
The paper is organized as follows: Section 2 introduces the notation as well as elementary notions from information theory. Section 3 formulates learning as a trajectory on the space of probability measures, defines the notion of shortest learning path, and provides a connection to Markov chains. Section 4 constructs a simple Markov chain model for gradient based learning that moves along the shortest learning path. Finally, Section 5 performs an empirical evaluation of the proposed model.
2 SYSTEM MODEL
Let x  X be a random vector belonging to some set X of possible inputs. We assume that there exists a function, known as "oracle", that maps x to one of K  N classes. Formally, there exists a deterministic mapping c : X  Y where Y = {0, . . . , K - 1} is the set of possible classes. Then, let y~  Y denote the random variable y~ = c(x). One common assumption, that is present in popular datasets such as MNIST, CIFAR-10, CIFAR-100, and Imagenet, is that ~y is uniformly distributed. We assume this to be true throughout this paper. Note that the designer of the dataset has control over the marginal distribution of y~. We model the effect of having error-prone labels, denoted by the random variable y, in the data by introducing discrete independent random noise z  Y to y~ in the form of modulo addition1, that is y = y~  z  Y. Let    be the vector, possibly random, containing of all tunable parameters in the hypothesis space . Then a classifier is a deterministic function g :  × X  Y that aims to approximate c. Further, y^ = g(, x) is defined to be the random variable of the label predicted by the classifier g(, ·). Using this notation we define the three types of error: the dataset error p = P (y = y~), the test error  = P (y^ = y), the true error ~ = P (^y = ~y). A summary of this system model is provided in Figure 1. We shortly review some elementary concepts from information theory such as entropy and mutual information. The entropy of a discrete random variable y  Y is defined as2
H(y) = - P (y = y) log P (y = y) = -E log P (y) .
yY
1We use  to denote the modulo K addition. 2In this paper we assume log to be the natural logarithm.
2

Under review as a conference paper at ICLR 2019

The entropy is bounded by 0  H(y)  log |Y| and it measures the amount uncertainty present in y. Similarly, the conditional entropy between two random variables y and ^y is

H(y|y^) = -E log P (y|^y)
and it quantifies the uncertainty about y given that ^y is known. Finally, the mutual information I(y; ^y) between y and y^ measures how much information does one random variable carry about the other. It may be defined in terms of entropies as

I(y; ^y) = H(y) - H(y|^y) = H(y^) - H(y^|y) .

Moreover, the following proposition is a well-known result from information theory, known as Fano's inequality, that relates test error  and conditional entropy.
Proposition 1. Fano's Inequality (Csiszr & Krner (2011), Lemma 3.8) The value of H(y|^y) and H(y^|y) is upper bounded by a function of the expected error as

max{H(y|y^), H(^y|y)}  (),

where the function  : [0, 1]  R is defined as (x) = x log(K - 1) + hb(x), x  [0, 1]
and hb(x) = -x log(x) - (1 - x) log(1 - x) is the binary entropy function.

This results provides an upper bound on conditional entropy in terms of , that is known to be sharp.
In the works of Feder & Merhav (1994) it has been shown that I(y; y^) gives an upper and lower bound on the minimal error 3 between y and y^. In addition, the minimal error is minimized when I(y; ^y) reaches its maximum. Therefore, learning can be modeled as finding  such that I(y; ^y) is maximized. This can be written in terms of entropies as

max H(^y) - H(y^|y) .


(1)

As in our previous work (Anonymous, 2018) we are interested on characterizing the trajectory in the 2D information plane, composed by H(y^) and H(^y|y), during the learning process of artificial neural networks (ANNs). In Figure 2 we observe that learning trajectory for the DenseNet architecture of 100 layers as it learns to classify data from the CIFAR-100 dataset, for p = 0. Intuitively, when solving equation 1, maximizing H(^y) is more related with the unsupervised component of learning since it does not depend y. On the other hand, keeping H(^y|y) low while H(y^) increases can be seen as the supervised component of equation 1. From this point of view it would be interesting to characterize the inflection point from which H(y^|y) starts decreasing, since it allows us to observe at which point GD starts paying more attention to assigning labels correctly than to learn about
the distribution of the input. One also may wander if this increasing-decreasing trajectory is an
accidental result for that occurs only on this particular experimental setup, or if it is a fundamental
property of GD. In Anonymous (2018) we showed that this behavior seems to appear regardless of
the activation function employed (see Appendix C) on the spirals and MNIST dataset. Moreover,
in further sections we provide a justification for this type of trajectory and show that it remains in other datasets. As z is independent, minimizing  amounts to g(, ·) learning c(·), regardless of the value of p (Angluin & Laird, 1988). For more information about error and entropy relations in the presence of noisy labels see Appendix A.

3 GRADIENT DESCENT ON JOINT PROBABILITY MEASURES AND ITS CONNECTION TO MARKOV CHAINS
In gradient based training of ANNs, the tunable parameters of the networks  are changed in time by the gradient updates of the loss function, in order to minimize the learning error for a particular problem at hand. Let n   denote the tunable parameters of an ANN after n  1 training steps of GD. The parameters are initialized as 0, which can be random or deterministic. The set  can be seen as a high dimensional Euclidean space with the network parameter  as its vector.
3The minimal error is the error obtained by a maximum likelihood classifier that predicts y from ^y.

3

Under review as a conference paper at ICLR 2019

Training Steps (%) Training Steps (%)

CIFAR-100
H (^y|y) H (^y|y)

p = 0.0 6 Theoretical
5
4
3
2
1
0 0123456
H (y^)
(a)

100 80 60 40 20

p = 0.0 6 Theoretical
5
4
3
2
1
0 0.0 0.2 0.4 0.6 0.8 1.0

(b)

100 80 60 40 20

Figure 2: Learning trajectory of DenseNet on the CIFAR-100 dataset. The markers in the black dashed lines represent the ideal values of H(y), H(y^|y) and  when ~ = 0. (a): The dashed lines correspond to the maximum entropy value. (b): The dashed lines are the upper bound given by
Fano's inequality.

At the training step n, the outcome of the learning algorithm is captured by the random variable ^yn which is modulated by the network function g(n, ·) applied to the random input data x, i.e., ^yn = g(n, x). Therefore the GD learning gives rise the following sequence of random variables
g(0, x), . . . , g(n, x), . . . .
As n grows large with a successful training, the sequence of random variables converges approximately to the true labels ~y which itself follows a joint distribution with x. Note that the above random variables are coupled through the common random variable x and the sequence of parameter updates n.
If the probability distribution of ^yn is denoted by pn, a first question is to see how GD methods modify pn on the space of probability measures defined on Y. As a consequence, one can determine the trajectory of H(y^n), which will be plotted later. However it is additionally important in learning that the random variable ^yn approximates the true labels. Therefore, a second question would be how GD methods change the joint distribution of (y^n, ~y). The answer could determine instead the trajectory of the conditional entropy H(y^n|~y). We first study the trajectory of H(y^n).
The random variables ^yn are defined as g(n, x). Consider the sequence of random variables {n}. Let T denote the set of training samples (x, y) that are obtained prior to training and independently. In addition, let Tn be a subset of T that is used at the step n for GD update. Tn is assumed to be independent from (0, . . . , n-1) and it is either deterministic and known all n or randomly chosen at each step. These variations correspond to the variants of gradient and stochastic GD.

In pure gradient based methods without momentum based techniques, the network parameters obey

the following recursive relation

n = f (n-1, Tn) ,

(2)

where f denotes the update rule of GD. The model assumes that the GD updates only depend on the

parameters in the last step and the training set used in the current iteration. We can assume that Tn are i.i.d. random variables if we neglect the effect of reusing training data in different batches. The

first conclusion is that the sequence of random variables {n} is a Markov chain.

Proposition 2. The sequence of random variables {n} defined as equation 2 with i.i.d. random variables Tn is a Markov Chain.

Proof. If Tn's are i.i.d. random variables, the proof follows directly from Serfozo (2009, Proposition 11) on equation 2.

The transition probability of this Markov chain can be obtained only from f and T1. In that sense, the random process {n} is a homogeneous Markov chain. Throughout this work, it is assumed that the Markov chain {n} with has a stationary distribution which corresponds to the learned ANN.

4

Under review as a conference paper at ICLR 2019
This proposition shows that GD updates induce Markov property for weights of an ANN. The sequence of random variables y^n = g(n, x) however is in general not a Markov chain, particularly because they are coupled through a common random variable x. Since we are interested in H(y^n) and the distribution pn, these random variables can be decoupled by considering the random variables g(n, xn) where xn are i.i.d. random variables with the same distribution as x. Note that the value of the entropy function remains unchanged after decoupling, namely H(y^n) = H(g(n, xn)). The new sequence is a function of a Markov chain and i.i.d. random variables. The question whether the resulting sequence is a Markov chain has been addressed in Spreij (2001); Gurvits & Ledoux (2005) showing that ^yn is not a Markov chain in general unless certain conditions are met by the function g(·). Unfortunately the function g(·) is not injective and a non-injective function of a Markov chain is not Markov chain in general. However the random variable g(n, xn) can be seen as the observation of the Markov process {n} through a noisy memoryless channel g(·, xn). Therefore the random process {g(n, xn)} is a Hidden Markov Process (HMP). See Ephraim & Merhav (2002) for an information theoretic survey.
If the learning is done perfectly, the HMP {g(n, xn)} converges to the uniform distribution of correct labels. Since the random variables are discrete, entropy is a continuous function of the distribution pn. Therefore as the correct labels are uniformly distributed, the entropy H(^yn) approaches its maximum log K. The instantaneous entropy H(^yn) would converge monotonically to the entropy of the stationary distribution log K if the sequence were to be a Markov chain. This could explain the monotonicity of H(y^n) in the experiments. Although the sequence not a Markov chain but it is indeed a HMP, the following proposition shows that the entropy is lower-bounded by an increasing function. Proposition 3. Suppose that the Markov process {n} with the probability distribution qn has a stationary distribution q. We have
H(y^n)  log |K| - D(qn q).
The proof follows from data processing inequality for Kullback-Leibler divergence and is found in the Appendix. Note that since {n} is a Markov process, D(qn q) is non-increasing.
4 GRADIENT DESCENT ON JOINT PROBABILITY MEASURES
In the previous section, the non-decreasing property of H(y^n) was investigated by modeling the network output as an HMP. In the ideal situation where the network manages to learn successfully the true labels, we can say y^n converges to y~ almost surely4. The joint distribution of (y^n, y~) specifies a point on the space of probability measures on Y × Y. The task of learning consists tuning the parameters n in a way that the joint distribution approaches the distribution of (y~, y~). Therefore the gradient descent steps corresponds to a sequence of points, that is joint distributions of (y^n, y~), on the space of probability measures on Y × Y with the end point ideally being the joint distribution of (y~, y~). In this section, we investigate the gradient descent algorithm by exploring the path it takes on the space of probability measures on Y × Y5. The trajectory of conditional entropy H(y^n|y~) is determined for the trajectory of probability distributions on the space of joint measures.
However it is in general difficult to precisely characterize this path. Instead, one might ask how the gradient descent trajectory compares with a certain natural path on the space of distributions. A relevant question is to ask what the shortest path between these probability measures is on this space and how close the gradient descent is moving to this shortest path. To be able to formally address this issue we require to define curves and lengths on the metric space of probability measures. The space of probability distributions defined on the discrete space Y × Y, denoted by P(Y × Y), with total variation metric dT V (·, ·) is a simplex in a finite dimensional Euclidean space. The total variation metric is equivalent to the L1-distance between the points in the corresponding Euclidean space. A curve in this space is defined by a continuous function  : [0, 1]  P(Y × Y). The curve is called a shortest path if it has minimal length among all curves with endpoints (0) and (1). Note that
4A weaker notion of convergence can be used although this choice is not crucial for the next results. 5The problem of finding the optimal way to change the distribution is connected to the problem of optimal transport. We would like to thank (removed for anonymity) for mentioning this connection.
5

Under review as a conference paper at ICLR 2019

Spirals
H (^y|y)

Steps (%)

Experiment
1.6 p = 0.0 p = 0.1
1.4 p = 0.2 p = 0.4
1.2 Theoretical
1.0
0.8
0.6
0.4
0.2
0.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50
H (y^)

p = 0.0

3.0

p = 0.1 p = 0.2

p = 0.4

2.5 Theoretical

2.0

1.5

1.0

0.5

0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
H (y^)

100 80 60 40 20 0
100 80 60 40 20 0

Training Steps (%)

Training Steps (%)

H (^y|y)

H (^y|y)

Markov Process
1.6 p = 0.0 p = 0.1
1.4 p = 0.2 p = 0.4 Maximum
1.2
1.0
0.8
0.6
0.4
0.2
0.0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
H (y^)

p = 0.0

3.0

p = 0.1 p = 0.2

p = 0.4

2.5 Maximum

2.0

1.5

1.0

0.5

0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
H (y^)

100 80 60 40 20 0
100 80 60 40 20 0

MNIST
H (^y|y)

Steps (%)

Figure 3: Experiments for FCNN and LeNet-5. This figures follow the same format as Figure 2(a).
The shape of the makers differentiate between different values of p. For the -SMLC model the white star shows the inflection point of H(^y|y).

the length is measured in this space using L1-norm. The following theorem guarantees that there is a shortest path on this space between probability measures and it can be traveled using a Markov chain.
Theorem 1. The shortest path between two probability measures µ and  on the space of discrete probability measures P(Y × Y) is given by tµ + (1 - t) for t  [0, 1]. Furthermore if the probability measures are represented by row vectors there is a transition matrix  with the stationary distribution  such that µn = µn is on the line segment between µ and  and limn µn = .

Proof. Not that the space of probability distributions here is a bounded compact metric space with each two points connected by a rectifiable curve. The existence of shortest path follows from (Burago et al., 2001, Corollary 2.5.20,Theorem 2.5.23) The transition matrix in Theorem 1 is given simply by:
 = (1 - )I + 1T 
where 1 = (1, 1, . . . , 1).

To see the implication of previous theorem more precisely, consider conditional distributions P (y^n  ·|y~ = l) and let p~l(n)  RK be the following vector for l  Y, that is

p~l(n) = (P (^yn = 0|y~ = l), P (^yn = 1|y~ = l), · · · , P (y^n = K - 1|~y = l)) .

We use the compact notation P~(n)  RK×K for the matrix with p~0(n), . . . , p~K-1(n) as rows.

Note that, with the assumption that ~y is uniformly distributed, the joint distribution of (y^n, ~y) is

fully

determined

by

P~ (n)

since

P (^y, y~)

=

1 K

P

(^y|~y).

Suppose

that

the

initialization

0

is

such

that

g0 (·) initially maps all inputs to the same class (the first class is assumed for simplicity).. Therefore

the initial distribution matrix P~(0) assumes no knowledge about the input and is given by

P~ (0) = 1e1T .

6

Under review as a conference paper at ICLR 2019

Ideally, in a learning algorithm, the matrix P~(0) converges to an optimal distribution P~ as n  , that is P~ = limn P~(n). If P (^y = ~y) = 1, we have:
P~  = I .

Now that we set the initial distribution and stationary distributions, the following transition matrix provides a way to pass from the initial distribution to the stationary one.
Definition 1. - Simple Markov Learning Chain (-SMLC) Given 0 <  < 1, the sequence of random pairs {(^yn, y~)} is an -SMLC if pl(n) = pl(n - 1)l, for every n  0, l  Y with
l = 1eTl + (I - 1elT) , P~ (0) = 1elT .

The above construction provides a different transition matrix for each p~l(n) depending on l. The following theorem describes how an -SMLC moves P~(n) in the space of stochastic matrices. It

actually shows this construction leads to points on the shortest path between the measures.

Theorem 2. If (^yn, y~) is the n-th random pair generated of an -SMLC, then P~(n) = (1 - t)P~(0) + tI , with t = (1 - n).

(3)

Proof. c.f. Appendix B

This shows that P~(n) belongs to the continuous curve from equation 3, regardless of the choice of . Moreover, let y^(t) be a version of ^yn parametrized by t  [0, 1] such that its conditional distribution with ~y corresponds to P~(n) = (1 - t)P~(0) + tI.

Proposition 4. If {(y^n, y~)} is an -SMLC and z follows the distribution

P (z = i) =

1 - p,

p K -1

,

i i

=0  {1, . . . , K

- 1}

,

then

 H (y^(t)|y) t

=

1 K

p

log(1

-

tp)

+

(1

-

K

+

p)

log(1

-

t

+

t

K

p -

1

)

-

(K

-

1)(1

-

p)

log(t(1

-

p))

-

(K

-

1)p

log(t

K

p -

1

)

Proof. c.f. Appendix B

Corollary 1.

In the setting of Proposition 4, if p



0 then H(y^(t)|y) has one maximum at t

=

1 2

,

H

(^y(

1 2

)|y)

=

K-1 K

log 2.

This result allows us to characterize the shape of the 2D curves (H(^y(t)), H(y^(t)|y)) for the above construction.

We now consider the implication of Markov assumption for error. Define y^ln  Y for all l  Y to be random variables distributed according to the conditional probabilities P (^yln = k) = P (g(n, x) = k|~y = l) for all k  Y. Note that, since ~y is uniformly distributed, one can compute the joint distribution of (^yn, ~y) from the marginal distributions of ^yn0 , . . . , y^Kn and vice-versa. The following propositions shows that the Markov trajectory implies the reduction in error as well.
Proposition 5. If {y^nl } is a stationary Markov Chain converging to the distribution p = el then
P (y^n = l|y~ = l)  P (^yn+1 = l|y~ = l)
Corollary 2. If {y^ln} is a stationary Markov Chain converging to the distribution p = el for all l  Y then
P (y^n = ~y)  P (^yn+1 = ~y) .

This last results shows that ~ is non-increasing with n (i.e., no over-fitting), which is a desirable property for any learning algorithm. We will show through numerical simulations how a comparable behavior is observed for gradient descent methods.

7

Under review as a conference paper at ICLR 2019

Spirals
H (^y|y)

Step (%)

Experiment

1.6 p = 0.0 p = 0.1
1.4 p = 0.2 p = 0.4
Theoretical 1.2

1.0

0.8

0.6

0.4

0.2

0.0 0.0

0.2 0.4


0.6

p = 0.0

3.0

p = 0.1 p = 0.2

p = 0.4

2.5 Theoretical

2.0

1.5

1.0

0.5

0.0 0.0

0.2

0.4 0.6


0.8

100 80 60 40 20 0
100 80 60 40 20 0

Training Steps (%)

Training Steps (%)

H (^y|y)

H (^y|y)

Markov Process
1.6 p = 0.0 p = 0.1
1.4 p = 0.2 p = 0.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7


p = 0.0 p = 0.1 3.0 p = 0.2 p = 0.4
2.5

2.0

1.5

1.0

0.5

0.0 0.0

0.2

0.4 0.6


0.8

100 80 60 40 20 0
100 80 60 40 20 0

MNIST
H (^y|y)

Step (%)

Figure 4: Experiments for FCNN and LeNet-5. This figures follow the same format as Figure 2(b). The shape of the makers differentiate between different values of p.

5 EXPERIMENTS

In this section we compare gradient based learning to the -SMLC model through empirical simula-

tions. First, we use the datasets where GD is extremely successful at the classification task (~  0),

that is the MNIST dataset and the spirals dataset (Anonymous, 2018). The spirals dataset constitutes

a 2D-spiral classification task constructed as

x=

 ( a + b) cos

2a

+

2 K

~y

,

 ( a + b) sin

2a

+

2 K

~y

,

where a  [0, 1], b  [0, 0.1] and ~y  {0, 1, . . . , K - 1} are independent uniformly distributed random variables and K = 3. This dataset is divided into a training set of 50 000 samples and a test set of 2 000. Furthermore, we train the FCNN of Anonymous (2018) for the spirals dataset and use LetNet-5 (LeCun et al., 1999) for MNIST, achieving an average accuracy above 99% in
both cases. For the sake of completeness we perform more experiments on the CIFAR-10 and CIFAR-100 datasets using the DenseNet architecture from Huang et al. (2017) with 100 layers.
FCNN and LetNet-5 are trained using Adam's optimizer, while DenseNet is trained with SGD.
More detailed explanation about the experimental setup is provided in Appendix C. We estimate use a naive estimator of entropy which consists on computing the empirical distribution of (y^, y)
and directly calculating entropy afterwards. This method is known to have an approximation error of K2/N (Miller, 1955), which is good enough for our experiments since N is much larger than K2 in our datasets. For cases with larger K one could use more sophisticated methods, such as
Schu¨rmann (2004); Archer et al. (2014). For our experiments we introduce i.i.d. noise to the dataset labels before training according to P (z = 0) = 1 - p and P (z = 0) = p/(K - 1). We keep fixed  = 0.85 for all simulations.

In Figure 3 we show the similarity between the -SMLC model and ANNs on the spirals and MNIST dataset. This figure is obtained by averaging the entropy values over 100 realizations of training. We observe ANNs move along the information plane in a similar way as the -SMLC model, and converge to the optimal distribution. In addition, we display the inflection point of H(y|y^) of the SMLC model for different values of p. Interestingly, as labels get noisier the inflection point occurs

8

Under review as a conference paper at ICLR 2019

CIFAR-10
H (^y|y) H (^y|y)

Training Steps (%) Training Steps (%)

p = 0.0

3.0

p = 0.1 p = 0.2

Theoretical

2.5

2.0

1.5

1.0

0.5

0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
H (y^)

100 80 60 40 20 0

p = 0.0

3.0

p = 0.1 p = 0.2

Theoretical

2.5

2.0

1.5

1.0

0.5

0.0 0.0

0.2

0.4 0.6


0.8

100 80 60 40 20 0

Figure 5: Experiments for DenseNet on CIFAR-10. This figures follow the same format as Figures 3 and 4.

at a larger H(y^) value. This seems to be the case for SGD learning as well. This phenomena suggests that, as labels get noisier, a learning algorithm needs to know more about the input distribution before
it can start assigning labels efficiently. Similar conclusions can be drawn form Figure 4, which is also obtained by averaging over 100 realizations of training. An interesting result is that, regardless of the value of p, a good learning algorithm should converge to a point that lies on the upper bound provided by Fano's inequality. These Figures artificially mitigate the randomness induced by SGD
on the trajectory by averaging over several realizations of training. How much SGD oscillates seems
to depend on the experimental setup, such as the learning rate, dataset, and the structure of the classifier. For instance, Figure 2 shows 1 realization of SGD training for DenseNet on CIFAR-100. In that figure we observe a rather stable trajectory, with not much oscillation. However, in Figure 5 we average over 2 realizations of SGD learning for DenseNet on CIFAR-10 and obtain highly oscillating trajectories. As expected, both trajectories follow a similar behavior as the -SMLC model.

9

Under review as a conference paper at ICLR 2019
REFERENCES
Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343­370, 1988.
Anonymous. This reference is not disclosed in order to mantain anonymity. 2018.
Evan Archer, Il Memming Park, and Jonathan W Pillow. Bayesian entropy estimation for countable discrete distributions. The Journal of Machine Learning Research, 15(1):2833­2868, 2014.
Amir R. Asadi, Emmanuel Abbe, and Sergio Verd. Chaining Mutual Information and Tightening Generalization Bounds. arXiv preprint arXiv:1806.03803, June 2018.
Dmitri Burago, IU D. Burago, and Serge Ivanov. A course in metric geometry. Number v. 33 in Graduate studies in mathematics. American Mathematical Society, Providence, RI, 2001. ISBN 978-0-8218-2129-9.
T. M. Cover and Joy A. Thomas. Elements of information theory. Wiley-Interscience, Hoboken, N.J, 2nd ed edition, 2006. ISBN 978-0-471-24195-9. OCLC: ocm59879802.
Imre Csiszr and Jnos Krner. Information theory: coding theorems for discrete memoryless systems. Cambridge University Press, Cambridge ; New York, 2nd ed edition, 2011. ISBN 978-0-52119681-9.
Y. Ephraim and N. Merhav. Hidden Markov processes. IEEE Transactions on Information Theory, 48(6):1518­1569, June 2002. ISSN 0018-9448. doi: 10.1109/TIT.2002.1003838. URL http: //ieeexplore.ieee.org/document/1003838/.
M. Feder and N. Merhav. Relations between entropy and error probability. IEEE Transactions on Information Theory, 40(1):259­266, January 1994. ISSN 0018-9448. doi: 10.1109/18.272494.
Marylou Gabrie´, Andre Manoel, Cle´ment Luneau, Jean Barbier, Nicolas Macris, Florent Krzakala, and Lenka Zdeborova´. Entropy and mutual information in models of deep neural networks. arXiv preprint arXiv:1805.09785, 2018.
Leonid Gurvits and James Ledoux. Markov property for a function of a markov chain: A linear algebra approach. Linear algebra and its applications, 404:85­117, 2005.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups. IEEE Signal Processing Magazine, 29(6): 82­97, November 2012. ISSN 1053-5888. doi: 10.1109/MSP.2012.2205597.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, volume 1, pp. 3, 2017.
Varun Jog and Venkat Anantharam. The entropy power inequality and Mrs. Gerber's lemma for groups of order 2n. IEEE transactions on information theory, 60(7):3773­3786, 2014.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Yann LeCun, Patrick Haffner, Le´on Bottou, and Yoshua Bengio. Object recognition with gradientbased learning. In Shape, contour and grouping in computer vision, pp. 319­345. Springer, 1999.
George A Miller. Note on the bias of information estimates. Information theory in psychology: Problems and methods, 2(95):100, 1955.
Daniel Russo and James Zou. How much does your data exploration overfit? Controlling bias via information usage. arXiv preprint arXiv:1511.05219, November 2015.
Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning. In International Conference on Learning Representations, 2018.
10

Under review as a conference paper at ICLR 2019 Thomas Schu¨rmann. Bias analysis in entropy estimation. Journal of Physics A: Mathematical and
General, 37(27):L295, 2004. Richard Serfozo. Basics of applied stochastic processes. Springer Science & Business Media, 2009. Ravid Shwartz-Ziv and Naftali Tishby. Opening the Black Box of Deep Neural Networks via Infor-
mation. arXiv preprint arXiv:1703.00810, March 2017. Peter Spreij. On the markov property of a finite hidden markov chain. Statistics & probability letters,
52(3):279­288, 2001. Naftali Tishby, Fernando C. Pereira, and William Bialek. The Information Bottleneck Method. In
Proc. 37th Annu. Allerton Conf. Commun., Control, Comput., pp. 368­377, 1999. Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. In Advances in Neural Information Processing Systems 30, pp. 2521­2530. 2017.
11

Under review as a conference paper at ICLR 2019

A COLLECTION OF RESULTS FOR LEARNING WITH RANDOM LABELS

For



<

1

-

1 K

,

proposition

1

can

be

written

as

a

lower

bound

on



through

the

following

Proposition

of Anonymous (2018).

Proposition 6.

(Anonymous,

2018) (H(z))



, where 

:

[0, log |K|]



[0,

1

-

1 K

]

is

the

inverse

function

of



in

the

interval

[0,

1

-

1 K

].

Proof. For independent noise z, we have

H(y|^y)  H(y|^y, y~) = H(z|y^, ~y) = H(z).

By

Proposition

1,

it

follows

that

H (z)



().

Applying

the

inverse

(x),

x



[0,

1-

1 K

],

completes

the proof.

In Anonymous (2018) it is shown that this bound is sharp when z is distributed such that (H(z)) = p, thus p  . We generalize this result in the following theorem for an arbitrary distribution of z, under some mild conditions, and show that p   is in fact a sharp lower bound for arbitrary z.
Lemma 1. Let z  Y be a random variable with P (z = 0) = 1 - p, then
H(z)  (p) .

Proof. Let us define k P (z = k) and the auxiliary random variable ~z  {1, . . . , K - 1} with P (~z = k) = k/p for all k = 1, . . . , K - 1.

K-1
H(z) = -(1 - p) log(1 - p) - k log k

k=1

= -(1 - p) log(1 - p) - p K-1 k log k p pp
k=1

=

-(1

-

p) log(1

-

p)

-

K-1
p

k p

log

p

-

K-1
p

k p

log

k p

k=1

k=1

K-1
= -(1 - p) log(1 - p) - k log p + pH(~z)

k=1
= -(1 - p) log(1 - p) - p log p + pH(~z)

= hb(p) + pH(~z)  hb(p) + p log(K - 1) = (p)

Theorem 3. If P (z = 0) = 1 - p and P (z = 0) > P (z = k) for all k = 1, . . . , K - 1, then p  ,
and equality is attained if and only if P (^y = y~) = 1.

Proof. of Theorem 3 For the sake of notation let us define

k k max

P (y^ = y~  k) P (z = k) for all k  {0, . . . , K - 1}
max k .
k=1,...,K -1

From Proposition 1 we know that   (H(z)). Since  is an increasing function in the interval

[0,

1

-

1 K

]

and

Lemma 1 (c.f.

 is its inverse in that interval, we have that  is an increasing Appendix B) we know that H(z)  (p), this leads to

function

as

well.

From

(H(z))  ((p)) = p .

12

Under review as a conference paper at ICLR 2019

Then, if the bound from Proposition 1 were to be sharp,  could reach values strictly lower than p. We will show that this is not possible.

1 -  = P (^y = y)
K -1
= P (y^ = y|z = k)P (z = k)
k=0 K -1
= P (^y = y  k)P (z = k)
k=0 K -1
= kk
k=0 K-1
= (1 - p)0 + kk
k=1 K -1
 (1 - p)0 + maxk
k=1
= (1 - p)0 + max(1 - 0)  (1 - p)0 + (1 - p)(1 - 0) = (1 - p) .

(4) (5)
(6)
(7)
(8)
(9) (10) (11) (12)

If  < p we obtain (1 - p) < 1 -   (1 - p) which is a contradiction, hence it must hold that   p.

Finally, if  = p then equation 10 yields

1 - p  (1 - p)0 + max(1 - 0) .

Since max < (1 - p), this inequality holds if and only if 0 = 1, that is P (^y = ~y) = 1.

This theorem shows that the minimum expected error  can only be attained if g(, ·) manages to denoise the labels, hence ~ = 0. We extend this result by deriving bounds for ~, given  and p, in
the following theorem.

Theorem 4. (Angluin & Laird, 1988)

Given



<

1-

1 K

and

p

<

1 2

,

if P (z

=

0)

=

1 - p,

and

P (z

=

k)

<

1 2

for

all k

=

1, . . . , K

- 1,

then ~ is bounded by

 1

- -

p p



~



-p 1 - 2p

.

Proof. of Theorem 4

1 -  = P (y^ = y)

K -1
= P (y~  k = y^) P (z = k)

k=0

k

k

K -1

= (1 - p)(1 - ~) + kk

k=1

K-1

 (1 - p)(1 - ~) + max k

k=1

= (1 - p)(1 - ~) + max~

= (1 - p) - ~((1 - p) - max) ,

(13) (14)

13

Under review as a conference paper at ICLR 2019

~ 

(1 - p) - (1 - ) (1 - p) - max



(1

- p) - (1 - (1 - p) - p

)

=

-p 1 - 2p

.

Applying

K -1 k=1

k k



0

in

equation

13

leads

to

1

-





(1

-

p)(1

-

~)

thus

(1

-

p) (1

- -

(1 p)

-

)



~



 1

- -

p p



~,

which completes the proof.

Corollary 3.

If p <

1 2

,



<

1

-

1 K

,

and

-p 1-2p

<1-

1 K

then

max{H

(y~|^y),

H

(^y|~y)}



(

-p 1 - 2p

)

.

Proof.

Since  is an increasing function in the interval [0, 1 -

1 K

],

the

proof

follows

from

applying

Theorem 4 on Proposition 1.

In information theory there is a result of this kind, known as Mrs. Gerber's Lemma (MGL), that does not require knowledge about p and . MGL provides an upper bound on H(~y|y^) given H(y|y^) for the case of K = 2. This result also states that the minimum H(y|y^) is attained when H(y~|y^) = 0. Since we assumed y~ to be uniformly distributed, this corresponds to ~ = 0, up to permutation ambiguities. Generalizing MGL for arbitrary K is still an open question in information theory. Nevertheless, Jog & Anantharam (2014) successfully proved MGL for the cases where K is a power of 2. We summarize that result in the following proposition.
Proposition 7. Generalized Mrs. Gerber's Lemma for K = 2n(Jog & Anantharam, 2014)
f2n (y~, z) = min H(~y  z|y^)
H (~y|^y)=y~,H (z)=z

with

f2n (y~, z) =

f2(y~ - k log 2, z - k log 2) + k log 2 max(y~, z)

if k log 2  y~, z  (k + 1) log 2, otherwise

where

f2(x, y) = h(h-1(x) h-1(y))

and k is an arbitrary positive integer and a b a(1 - b) + b(1 - a).

We have derived inequalities that relate entropies and error values. Then we showed that in the
presence of corrupted labels, the best g can do for minimizing  is to learn c, regardless of the value of p.

B DEFERRED PROOFS
Proof of Proposition 3: The proof follows from the following general theorem. Theorem 5. Let {yn} be an HMP defined as the observation of a Markov process {xn} through an arbitrary stationary memoryless channel with values in the state space Y. Suppose that the probability distributions on the respective state spaces of {xn} and {yn} are given by {qn} and {pn} with the stationary distribution q and p. Then
D(pn p)  log D(qn q).

Proof. Based on the assumption above, xn and yn are related according to the conditional distribution characterized by the conditional probabilities {r(·|x) : x  X}. Denote the joint distribution of xn and yn by µn defined on X × Y and given by qn(x) × r(·|x). The stationary joint distribution µ is defined by q × r. Using the chain rule (Cover & Thomas, 2006, Theorem 2.5.3), we have:
D(µn µ) = D(qn q) + D(r r) = D(qn q). On the other hand, the chain rule and the non-negativity of Kullback-Leibler divergence shows that:
D(µn µ)  D(pn p), which implies the theorem.
14

Under review as a conference paper at ICLR 2019

Training Steps (%)

Training Steps (%)

Spirals
H (~y|^y)

2.00 1.75 1.50 1.25

p = 0.0 p = 0.1 p = 0.2 p = 0.3 Gerber (UB) Data-Proc-Ineq (LB)

1.00

0.75

0.50

0.25

0.00
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
H (y|y^)

3.0 p = 0.0 p = 0.1
2.5 p = 0.2 p = 0.3 Gerber (UB)
2.0 Data-Proc-Ineq (LB)
1.5
1.0
0.5
0.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0
H (y|y^)

100 80 60 40 20 0
100 80 60 40 20 0

Training Steps (%)

Training Steps (%)

~ ~

0.8 p = 0.0
0.7 p = 0.1 p = 0.2
0.6 p = 0.3 UB
0.5 LB
0.4
0.3
0.2
0.1
0.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8


100 80 60 40 20 0

p = 0.0 0.8 p = 0.1
p = 0.2 p = 0.3 UB 0.6 LB

0.4

0.2

0.0 0.0

0.2

0.4


0.6

0.8

100 80 60 40 20 0

Figure 6: Mrs Gerber's Lemma in action.

MNIST
H (~y|^y)

Using the fact that the stationary distribution of y^n is equal to the uniform distribution, we have:
D(y^n y^) = log |K| - H(y^n).
This fact along with Theorem 5 proves Proposition 3.
After some technicalities we can prove that with a Markov chain one can move on the geodesic on the space of probability measures with total variation metric.
Theorem 6. Let P() be the space of all probability measures define on the discrete probability space (, F). Then P() is a geodesic space. Theorem 7. The geodesic segment between two measures µ and  on the space of probability measure P() is given by tµ + (1 - t) for t  [0, 1]. Furthermore there is a transition matrix  with the stationary distribution u such that µn is on the line segment between µ and .

Proof. of Theorem 2 Since 1elT1eTl = 1eTl we get
(l)2 = 1elT1elT + 1eTl (I - 1elT) + (I - 1eTl )1eTl +2(I - 1elT)

1eTl

0

= 1eTl + 2(I - 1eTl ) .

By induction we get

0

(l)n = 1eTl + n(I - 1eTl ) = (1 - n)1elT + nI . Replacing t = (1 - n)  [0, 1] yields

Finally, thus

(l)n = t1elT + (1 - t)I .
p~l(n) = p~l(0)(l)n = p~l(0)(t1eTl + (1 - t)I) = (1 - t)p~l(0) + tel ,
P~l(n) = (p~0(n)T, . . . , p~K-1(n)T)T = (1 - t)P~l(0) + t~I .

15

Under review as a conference paper at ICLR 2019

Proof. of Proposition 4 For uniform y, and z distributed according to equation 4, we get

P (y|^y = l) = P (~y  z|y^ = l) = P (~y|y^ = l) P (z)

thus

P

(t)

=

((1

-

p

-

K

p -

1 )I

+

K

p -

1 11T)

P~ (t)

,

circular convolution matrix of P (z)

Then,

Pi,j (t) = eiTP (t)ej

=

(1

-

t)eT1 ej

+

t(1

-

p

-

K

p -

1 )eTi ej

+

tK

p -

1

.

1 - t + t(1 - p),

Pi,j (t)

=

 1 - t + t
t(1 - p),

p K -1

,

 t

p K -1

,

j = 1, i = j

j j

= =

1, 1,

i i

= =

j j

.

j = 1, i = j

Pi,j (t) t

=

-e1Tej

+

(1

-

p

-

K

p -

1 )eiTej

+

K

p -

1

.

-p,

Pi,j (t) t

=

  

p K -1

-

1,

1 - p,

  

p K -1

,

j = 1, i = j

j j

= =

1, 1,

i i

= =

j j

.

j = 1, i = j

16

Under review as a conference paper at ICLR 2019



H (^y(t)|y) t

=

 t

-

K

1 K

K

Pi,j (t) log Pi,j (t)

j=1 i=1

=

-

1 K

K

K



Pi,j (t) t

(1

+

log

Pi,j

(t))

j=1 i=1

=

-

1 K

1



Pi,j (t) t

(1

+

log

Pi,j

(t))

-

1 K

1

Pi,j t

(t)

(1

+

log

Pi,j

(t))

j=1 i=j

j=1 i=j

-1 K K

Pi,j (t) t

(1

+

log

Pi,j

(t))

-

1 K

K



Pi,j (t) t

(1

+

log

Pi,j

(t))

j=2 i=j

j=2 i=j

=-1 1 K

[-p(1 + log(1 - t + t(1 - p)))] - 1 1 K

(K

p -

1

-

1)(1

+

log(1

-

t

+

tK

p -

1 ))

j=1 i=j

j=1 i=j

-

1 K

K

[(1

-

p)(1

+

log(t(1

-

p)))]

-

1 K

K

K

p -

1

(1

+

log(t

K

p -

1

))

j=2 i=j

j=2 i=j

=

1 K

p

log(1

-

tp)

+

1 K

(K

-

1)( K

p -

1

-

1)(1

+

log(1

-

t

+

tK

p -

1 ))

-

1 K

(K

-

1)(1

-

p)(1

+

log(t(1

-

p)))

-

1 K

(K

-

1)p(1

+

log(t

K

p -

1 ))

1 K

p

-

(K

-

1)(

K

p -

1

-

1)

-

(K

-

1)(1

-

p)

-

(K

-

1)p

=0

=

1 K

p

log(1

-

tp)

+

(1

-

K

+

p)

log(1

-

t

+

t

K

p -

1

)

-

(K

-

1)(1

-

p)

log(t(1

-

p))

-

(K

-

1)p

log(t

K

p -

1

)

Remark:

For

p



0

we

now

that

the

maximum

the

curve

is

at

t

=

1 2

since

lim
p0

H (y^(t)|y) t

=! 0



1 K

[(1

-

K)

log(1

-

t)

-

(1

-

K)

log

t]

=

0



log

1

- t

t

=0



t=

1 2

.

C EXPERIMENTAL DETAILS AND COMPLEMENTARY EXPERIMENTS
A fully connected ANN with four hidden layers of five neurons each, as FCNN, is trained on the spirals dataset. For the MNIST dataset, the popular convolutional network called LeNet-5 LeCun et al. (1999) is used. To train these networks we let the learning rate   R start at a given max  R and then decay by 40% per epoch until reaching some given minimum learning rate min < max, that is  = max{max0.6 epoch , min}. For the CIFAR-10 dataset we train a 100 layer DenseNet architecture as done in Huang et al. (2017), but we stop the training after 10 epochs instead of the original 300 used by the authors. The different configurations used for these experiments are summarized in Table 1.

17

Under review as a conference paper at ICLR 2019

tanh
H (^y|y)

Training Steps (%)

Table 1: Simulation Parameters for Figure 7 (Anonymous, 2018)

Dataset Activation Batch Size max min Test Accuracy

tanh

128 10-1 10-2 99.7%

Spirals sigmoid

128 10-1 10-5 99.6%

ReLU

700 10-1 10-5 97.8%

tanh

128 10-2 10-2 97.1%

MNIST sigmoid

128 10-2 10-4 96.3%

ReLU

128 10-2 10-4 99.1%

Table 2: The models with highest accuracy are used in the main paper.

Table 3: Simulation Parameters for DenseNet on CIFAR datasets

Dataset Activation Batch Size max min Test Accuracy

CIFAR-10 ReLU

64 10-1 10-1 80.2%

CIFAR-100 ReLU

64 10-1 10-1 80.2%

Spirals
1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50
H (y^)
1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50
H (y^)
1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50
H (y^)

100 80 60 40 20
100 80 60 40 20
100 80 60 40 20

Training Steps (%)

Training Steps (%)

Training Steps (%)

H (^y|y)

H (^y|y)

H (^y|y)

MNIST
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
H (y^)

100 80 60 40 20

3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
H (y^)

100 80 60 40 20

3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
H (y^)

100 80 60 40 20

Figure 7: Information plane trajectory during the learning process (Anonymous, 2018).

18

sigmoid
H (^y|y)

Training Steps (%)

ReLU
H (^y|y)

Training Steps (%)

