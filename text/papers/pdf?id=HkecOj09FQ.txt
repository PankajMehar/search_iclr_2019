Under review as a conference paper at ICLR 2019
UNIVERSAL MARGINALIZER FOR AMORTISED INFERENCE AND EMBEDDING OF GENERATIVE MODELS
Anonymous authors Paper under double-blind review
ABSTRACT
Probabilistic graphical models are powerful tools which allow us to formalise our knowledge about the world and reason about its inherent uncertainty. There exist a considerable number of methods for performing inference in probabilistic graphical models, however, they can be computational costly due to significant time burden, storage requirements or they lack theoretical guarantees of convergence and accuracy when applied to very large graphical models. We propose the Universal Marginaliser Importance Sampler (UM-IS) ­ a hybrid inference scheme that combines the flexibility of a deep neural network trained on samples from the model and it inherits the asymptotic guarantees of importance sampling. We show how combining samples drawn from the graphical model with an appropriate masking function allows us to train a single neural network to approximate any of the corresponding conditional marginal distributions, and thus amortise the cost of inference. We demonstrate that the efficiency of importance sampling is significantly improved by using as the proposal distribution samples from the neural network. We also use the embeddings obtained from the proposed neural network and utilise them for different tasks such as: clustering, classification and interpretation of relationships between the nodes. Finally, we benchmark the method on a large graph (>1000 nodes), showing that UM-IS outperforms sampling-based based methods by a large margin while being computationally efficient.
1. INTRODUCTION
Probabilistic Graphical Models (PGM) provide a natural framework for expressing the conditional independence relationships between random variables. PGMs are used to formalise our knowledge about the world and for reasoning and decision-making. A particular example of a PGM is a Bayesian Network (BN) where all variables in the graphical model are discrete. PGMs have been successfully used for problems in a wide range of real-life applications including information technology, engineering, system biology and medicine, among others. In systems biology, the structure of a PGM is learned and used to infer different biological properties from data (Friedman, 2004). For this type of application, the structure (edges) of the network is the main output. In the medical domain, a BN is used as a medical diagnosis system, where the network structure is designed by medical experts. The BN is then used to infer the conditional marginal probability of diseases given a set of evidence that contains observations for risk factors and/or symptoms (see Figure 4). In sensitive domains such as health care, the penalty for errors during inference can be potentially life-threatening. This risk can be mitigated by choosing a more complex model for the underlying process. However, exact inference is often computationally intractable for complex models so approximate inference is required. Furthermore, if we increase the complexity of the models, then, the cost of inference will increase accordingly, limiting the feasibility of available algorithms. Some approximate inference methods are: variational inference (Wainwright et al., 2008) and Monte Carlo methods such as importance sampling (Neal, 2001). Variational inference methods can be fast but do not target the true posterior, Monte Carlo inference is consistent, but can be computationally expensive.
In this paper, we propose the Universal Marginaliser Importance Sampler (UM-IS), an amortised inference-based method for graph representation and efficient computation of asymptotically exact marginals. In order to compute the marginals, the UM still relies on Importance Sampling (IS). We use a guiding framework based on amortised inference that significantly improves the perfor-
1

Under review as a conference paper at ICLR 2019
Figure 1: Universal Marginaliser: The UM performs scalable and efficient inference on graphical models. This Figure shows one pass trough the network. First, (1) a sample is drawn from the network, (2) values are then masked and (3) the masked vector is passed trough the UM, which then, (4) computes the marginal posterior.
mance of the sampling algorithm rather than computing marginals from scratch every time we run the inference algorithm, . This speed-up allows us to apply our inference scheme on large PGMs in interactive applications with small errors. Furthermore, the learned neural network can be used calculate a vectorized representation of the evidence nodes in the graphical model. This representations can be used for tasks like clustering and classification. We have implemented the UM-IS as a plugin1 to the Pyro probabilistic programming language (Uber AI Lab, 2017). The main contributions of the proposed work are as follows:
· We introduce UM-IS, a novel algorithm for amortised inference-based importance sampling. The model has the flexibility of a deep neural network to perform amortised inference. The neural network is trained purely on samples from the model prior and it has the asymptotic guarantees of importance sampling.
· We demonstrate that the efficiency of importance sampling is significantly improved, which makes the proposed method applicable for interactive applications that rely on large PGMs.
· We show on synthetic networks and on a medical knowledge graph (>1000 nodes) that the proposed UM-IS outperforms sampling-based and deep learning-based methods by a large margin, while being computational efficient.
· We also show that the networks embeddings can serve as a vectorised representation of the provided evidence for tasks like classification and clustering or interpretation of node relationships.
2. RELATED WORK
Currently, inference schemes in general PGMs use either message passing algorithms (Murphy et al., 1999), variational inference (Wainwright et al., 2008; Ng & Jordan, 2000; Jordan et al., 1999; Jaakkola & Jordan, 1999) or Markov Chain Monte Carlo (Hastings, 1970). Some exact inference algorithms are computationally expensive, within the context of the junction tree construction, because the time complexity is exponential in the size of the maximal clique in the junction tree (Jordan et al., 1999). In some cases, exact methods can be computationally efficient in a small graph or sparse regime(Heckerman, 1990). However, it has been shown that on larger graphs such methods converge to a local minimum (Heskes, 2003) that can be very different from the real marginals. Importance sampling methods (Cheng & Druzdzel, 2000; Neal, 2001) are well studied and converge asymptotically to the global optimum. The caveat is that constructing good importance sampling proposals for large PGMs is hard (Shwe & Cooper, 1991). For this reason, we focus on amortised inference, techniques which speed up sampling by allowing us to "flexibly reuse inferences so as to answer a variety of related queries" (Gershman & Goodman, 2014). Amortised inference has been popular for Sequential Monte Carlo and has been used to learn in advance either parameters (Gu et al., 2015) or a discriminative model which provides conditional density estimates (Morris, 2001; Paige & Wood, 2016). These conditional density estimates can be used as proposals for importance sampling, this approach was also explored in (Le et al., 2017). The authors use MADE, a fixed sequential density estimator (Germain et al., 2015). In contrast, our
1The code can be found in: http://www.github.com/Anonymus
2

Under review as a conference paper at ICLR 2019

method can be seen as further extension of MADE, a general density estimator, able to learn from arbitrary sets of evidence.
Feed-forward neural networks have recently been deployed to perform amortised inference (Mnih & Gregor, 2014; Rezende et al., 2014). For this application, neural networks are serving as noniterative approximate inference methods, trained by minimising the error between different sets of evidence and posteriors. They have been successfully applied to a variety for computer vision tasks, where the graphical model and its corresponding neural network for inference is trained jointly by maximising the variational evidence lower bound (Mnih & Gregor, 2014). In a similar fashion, Rezende et al. (2014) introduced stochastic back-propagation, a set of rules for gradient backpropagation through stochastic variables. The algorithm can be used to perform highly efficient inference in large scale PGMs.
Recently, probabilistic programming languages have become popular for describing and performing inference in a variety of PGMs bypassing the burden on the user of having to implement the inference method. For example, Ritchie et al. (2016) applied deep amortised inference to learn network parameters and later perform approximate inference on a PGM. Such models either follow the control flow of a predefined sequential procedure, or are restricted to use the same set of evidence.

3. UNIVERSAL MARGINALIZER (UM)

The Universal Marginaliser (UM) is a feed-forward neural network, used to perform fast, single-pass approximate inference on general PGMs at any scale. The UM can be used together with importance sampling, as the proposal distribution, to obtain asymptotically exact results when estimating marginals of interest. We refer to this hybrid model as the Universal Marginaliser Importance Sampler (UM-IS). In this section, we introduce the notation and the training algorithm for the UM (see Appendix 6.1 for a brief introduction to importance sampling).

3.1. NOTATION

A Bayesian Network (BN) encodes a distribution P over the random variables X = {X1, . . . , XN } through a Directed Acyclic Graph (DAG), the random variables are the graph nodes and the
edges dictate the conditional independence relationships between random variables. Specifically,
the conditional independence of a random variable Xi given its parents pa(Xi) is denoted as P (Xi|pa(Xi)).

The random variables can be divided into two disjoint sets, XO  X the set of observed variables within the BN, and XU  X \ XO the set of the unobserved variables.

We utilise a Neural Network (NN) as an approximation to the marginal posterior distributions

P (Xi|XO = xO) for each variable Xi  X given an instantiation xO of any set of observations.
We define x~O as the encoding of the instantiation that specifies which variables are observed, and what their values are (see Section 5.1). For a set of binary variables Xi with i  0, ..., N , the desired network maps the N -dimensional binary vector x~O  BN to a vector in [0, 1]N representing the probabilities pi := P (Xi = 1|XO = xO):

Y = UM(x~O)  (p1, . . . , pN ).

(1)

A single neural network is a function approximator, hence, it ca approximate any posterior marginal

distribution given an arbitrary set of evidence XO. For this reason, we call this discriminative
model as a Universal Marginaliser (UM). Indeed, if we consider the marginalisation operation in a Bayesian Network as a function f : BN  [0, 1]N , then, existence of a neural network which can

approximate this function is a direct consequence of the Universal Function Approximation Theorem

(UFAT) (Hornik et al., 1989). The theorem states that, under mild assumptions of smoothness, any

continuous function can be approximated to an arbitrary precision by a neural network of a finite,

but sufficiently large, number of hidden units. The hidden units serve as graph embeddings and can

be used as a compressed representation of the graph for clustering or classification.

3.2. TRAINING A UM
In this section, we describe each step of the UM's training algorithm for a given PGM. This model is typically a multi-output NN with one output per node in the PGM (i.e. each variable Xi). After

3

Under review as a conference paper at ICLR 2019

training, this modelcan handle any type of input evidence instantiation and produce approximate posterior marginals P (Xi = 1|XO = xO).
The flow chart with each step of the training algorithm is depicted in Figure 2a. For simplicity, we assume that the training data (samples for the PGM) pre-computed, and only one epoch is used to train the UM.
In practice, the following steps 1­4 are applied for each of the mini-batches separately rather than on a full training set all at once. This improves memory efficiency during training and ensures that the network receives a large variety of evidence combinations, accounting for low probability regions in P . The steps are given as follows:
1. Acquiring samples from the PGM. The UM was trained offline by generating unbiased samples (i.e., complete assignment) from the PGM using ancestral sampling (Koller & Friedman, 2009, Algorithm 12.2). The PGM described here only contains binary variables Xi, and each sample Si  BN is a binary vector. These vectors will be partially masked as input and the UM will be trained to reconstruct the complete unmasked vectors as output.
2. Masking. In order for the network to approximate the marginal posteriors at test time, and be able to do so for any input evidence, each sample Si must be partially masked. The network will then receive as input a binary vector where a subset of the nodes initially observed were hidden, or masked. This masking can be deterministic, i.e., always masking specific nodes, or probabilistic. We use a different masking distribution for every iteration during the optimization process. This is achieved in tho steps. First, we sample two random numbers from a uniform distribution i, j  U [0, N ] where N is the number of nodes in the graph. In the second step, we mask randomly select i number of nodes and mask the positive evidence and we do the same for the negative evidence of randomly selected j number of nodes. In this way, the ration between positive and negative evidence, as well as the total number of masked nodes is different with every iteration and the network (with a large enough capacity) will eventually learn all possible representations of the data.
There is some analogy here to dropout in the input layer and so this approach could work well as a regulariser, independently of this problem (Srivastava et al., 2014). However, it is not suitable for this problem because the keep probability of dropout is normally constant.
3. Encoding the masked elements. Masked elements in the input vectors Simasked artificially reproduce queries with unobserved variables, and so their encoding must be consistent with the one used at test time. Different encodings will be investigated and tested in Section 5.1.
4. Training with Cross Entropy Loss. We trained the NN by minimising the multi-label binary cross entropy between the sigmoid output layer and the unmasked samples Si.
5. Outputs: Posterior marginals. The desired posterior marginals are approximated by the output of the sigmoid output layer. We can use these values as a first estimate of the marginal posteriors (UM approach); however, combined with importance sampling, these approximated values can be further refined (UM-IS approach). This is discussed in Sections 4.1 and is empirically verified in Section 5.2.

1. Samples from PGM: Si  {0, 1}N .

2. Mask S: M : {0, 1}  {0, 1, }, M : probabilistic and/or deterministic.

3. Input: Simasked := M (Si)  {0, 1, }N , Labels: Si

4. Neural network with sigmoid output.

min. entropy loss

5. Output: Predicted Posterior P (Si|Simasked)
(a) UM Training: The process to train a Universal Marginaliser using binary data generated from a Bayesian Network

Input: X^  {0, 1, }N

NN Input: X^  {X1, ...Xi-1}  {0, 1, }N

i=i+1

UM (Trained neural network)

Output: q = Q(Xi|X^  {X1, ...Xi-1})

node sample Xi  {0, 1}

Sample node Xi with q as proposal

Output: One sample from joint Q(X^ )
(b) Inference using UM-IS: The part in the box is repeated N times, for each node i in topological order

4

Under review as a conference paper at ICLR 2019

4. HYBRID: UM-IS

4.1. SEQUENTIAL UM FOR IMPORTANCE SAMPLING

The UM is a discriminative model which, given a set of observations XO, will approximate all the posterior marginals. While useful on its own, the estimated marginals are not guaranteed to
be unbiased. To obtain a guarantee of asymptotic unbiasedness while making use of the speed of
the approximate solution, we use the estimated marginals for proposals in importance sampling. A naïve approach is to sample each Xi  XU independently from UM(x~O)i, where UM(x~O)i is the i-th element of vector UM(x~O). However, the product of the (approximate) posterior marginals may be very different to the true posterior joint, even if the marginal approximations are good (see
Appendix 6.2 for more details).

The universality of the UM makes the following scheme possible, which we call the Sequential Uni-
versal Marginaliser Importance Sampling (SUM-IS). A single proposal is sampled xS sequentially as follows. First, a new partially observed state is introduced x~SO and it is initialised to x~O. Then, we sample [xS ]1  UM(x~O)1, and update the previous sample x~SO such that X1 is now observed with this value. We repeat this process, at each step sampling [xS ]i  UM(x~SO)i, and updating x~SO to include the new sampled value. Thus, we can approximate the conditional marginal for a node i given the current sampled state XS and evidence XO to get the optimal proposal Qi as follows:

Qi = P (Xi|{X1, . . . , Xi-1}  XO)  UM(x~SO)i = Qi.

(2)

Thus, the full sample xS is drawn from an implicit encoding of the approximate posterior joint distribution given by the UM. This is because the product of sampled probabilities from Equation 3
is expected to yield low variance importance weights when used as a proposal distribution.

NN
Q = UM(x~O)1 UM(x~SO)i  P (X1|XO) P (Xi|X1, . . . , Xi-1, XO).
i=2 i=2

(3)

The process by which we sample from these proposals is illustrated in Algorithm 1 and in Figure 2b. The nodes are sampled sequentially, using the UM to provide a conditional probability estimate at

Algorithm 1 Sequential Universal Marginalizer importance sampling

1: Order the nodes topologically X1, ...XN , where N is the total number of nodes.

2: for j in [1,...,M ] (where M is the total number of samples): do

3: x~S = 

4: for i in [1,...N ]: do

5: sample node xi from Q(Xi) = UM(x~SO)i  P (Xi|XS , XO)

6: add xi to x~S

7: [xS ]j = x~S

8:

wj =

N Pi i=1 Qi

(where Pi

is the likelihood,

Pi

=

P (Xi

=

xi|xSpa(Xi)) and Qi

=

Q(Xi = xi))

9: Ep[X] =

M j=1

Xj wj

M j=1

wj

(as in standard IS)

each step. This requirement can affect computation time, depending on the parallelisation scheme used for sampling. In our experiments, we observed that some parallelisation efficiency can be recovered by increasing the number of samples per batch.

4.2. UM ARCHITECTURE
The general architecture of the UM is shown in Fig.3. It is similar to and auto-encoder but with multiple branches ­ one branch for each node of the graph. We noticed that the cross entropy loss for different nodes highly depends on the number of parents and its depth in the graph. To simplify the network and reduce the number of parameters, we share the weights of all fully connected layers that correspond to a specific type of nodes. Types are defined by the depth in the graphical model (type 1 nodes have no parents, type 2 nodes have only type 1 nodes as parents etc.). The architecture

5

Under review as a conference paper at ICLR 2019

(a) Directed Graphical Model

(b) UM architecture

Figure 3: Directed Graphical Model and the corresponding The nodes of the directed graphical model (a) are categorized regarding their depth in the network. The weights of the UM neural network (b) are shared for nodes of the same category.

of the best performing model on the large medical graph has three types of nodes and the embedding layer has 2048 hidden states. More details are in Section 5.1.

5. EXPERIMENTS
In our experiments, we chose the best performing UM in terms of Mean Absolute Error (MAE) on the test set for the subsequent experiments. We use ReLU non-linearities, apply the Dropout algorithm (Srivastava et al., 2014; Baldi & Sadowski, 2014) on the last hidden layer and use the Adam optimization method (Kingma & Ba, 2014) for parameter learning. We have also included batch normalization between all fully connected layers and selected a batch size of 2000 samples. To train the model on a large medical graphical model, we used in total 3 × 1011 samples, which required between 5 and 7 days on a single GPU.

5.1. SETUP

Graph: We carry out our experiments on a large (>1000 nodes) proprietary Bayesian Network for

medical diagnosis representing the relationships between risk factors, diseases and symptoms. A

illustration of the model structure is given in Figure 4.

Model: We tried different NN architectures with a grid search over the values of the hyperparam-

eters and on the number of hidden layers, the number of states per hidden layer and strength of

regularisation through dropout.

Test set: The quality of approximate conditional marginals was measured using a test set of poste-

rior marginals computed for 200 sets of evidence via ancestral sampling with 300 million samples.

The test evidence set for the medical graph was generated by experts from real data. The test evi-

dence set for the synthetic graphs was sampled from a uniform distribution. We used standard im-

portance sampling, which corresponds to the likelihood weighting algorithm for discrete Bayesian

networks (Koller & Friedman, 2009, Chapter 12), with 8 GPUs over the course of 5 days to compute

precise marginal posteriors of all test sets.

Metrics: Two main metrics are considered: the Mean Absolute Error (MAE), given by the absolute

difference between the true and predicted node posteriors and the Pearson Correlation Coefficient

(PCC), between the true and predicted marginal vectors. All these measures are bounded between

0 and 1. We used the Effective Sample Size (ESS) statistic for the comparison with the standard

importance sampling. This statistics measures the efficiency of the different proposal distributions

used during sampling. In this case, we do not have access to the normalizing constant of the poste-

rior distribution, the ESS is defined as (

M i=1

wi

)2

/

iM=1(wi)2, where the weights, wi, are defined

in Step 8. of Algorithm 1.

Data Representation: We consider a one hot encoding for the unobserved and observed nodes. Note that this representation only requires two binary values per node. One value represents if this node is observed and positive, while the other represents whether it is observed and negative. If the node is unobserved, then both bits in the encoding are zeros.

6

Under review as a conference paper at ICLR 2019

ESS PCC MAE ESS PCC MAE ESS PCC MAE

5.2. RESULTS
In this section, we first discuss the results of different architectures for the UM, then compare the performance of importance sampling with different proposal functions. Finally, we discuss the efficiency of the algorithm.

002:10

002:11

002:12

002:13

002:14

002:15

002:16

002:17

002:18

002:19

002:2

002:1 002:20

002:0 002:21

001:9 002:22

001:8 002:23

001:7 002:24

001:6 002:25

001:5 002:26

001:4 002:27

001:31

002:28

001:30

002:29

001:3 002:3

001:29

002:30

001:28

002:31

001:27

002:4

001:26

002:5

001:25

002:6

001:24

002:7

001:23

002:8

001:22

002:9

001:21

000:0

001:20

000:1

001:2 000:10

001:19

000:11

001:18

000:12

001:17

000:13

001:16

000:14

001:15

000:15

001:14

000:16

001:13

000:17

001:12

000:18

001:11

000:19

001:10

000:2

001:1 000:20

001:0 000:21

000:9 000:22

000:8 000:23

000:7 000:24

000:6 000:25

000:5 000:26

000:4

000:31

000:30

000:3

000:29

000:28

000:27

0000000000000000000000000000000000000000000000000000000000000111110011111001111001001100111000011001110000:::::11:::::0100::::10:0000::2221122:2::22002201::22110022:::2001122::02221:0022:1220011200111::1122100111011:20010::00102011::2000210120:1122012:0200::20112123104025096::78:120087110096225::0120400291310012::2022923111021::9204001952111::00603900211179:00933882100::1399700:2639::1100211500039::111410213900:231112300320::0083:11141900::0385012112::836000021011798::1200018848::421000970011481::2168400011:0512840000011::41:2483200100::121483211100::120044111:00127400805:::11174006::21211749700000::217881100575::2197110000057612::0000557012::1110041571201102300750:::0113257021:10:011014002:10:0501676020:1:10101760210:0918016:0:10:801961330:10:710061030:16:0106130:0015:0116030:104:20116031:0130306131::2004:006131011:105:0130100:16600:50310:170:10510391018:0051:08:1109:05401407:110:501460:01100:500541:11105:00044102:105:03401103:11500240140::15:104110500:11:0041065:01104:00470:1014:190401801041:80:0910:104517:0501:100416:50001:14015:500101:10441:502001:14003:530011:14020154001::41001:51501:100001:56401:130010:5700:139001:158001138001::9001:1367001:6001:136001:60001:13510:6110:00134:1062:1001033:0630:113102014600::13100:5611000:11063:610002:11076:100092:10086:11008211009::100727:11007:10062:110007:10052:110017:1042:1100027:1032:1100037:0022:111004710102::100517:1000:110267:10001:11077:100910:10807:11801011090::170180:11008:16010:110008:1501:0110108:1401:1102800:131:10130080:210:101140801110::0150810:010:1016108:1000:101780:91000:108080:1810010190:0:71090:101090:6100:1010090:5100:110109:0410:011209:00310:113009:200:10114901000::05191001000:061:900101:0710:90000::0809902000:9900:900200:08200:90012::0700190:02:10060090:002:10500900:002:10400901:002:10300902:02:10020093:0020:1010904:00021000:050:0902:100860:0002:19080700:00:180208080002::70018090:02:11600800:102:15008000:102:14008010:102:13008200:12:12000830:102:1108400:1021000:500:1802:100760:1002:1907700:10:180207800102::70017900:02:12600700:202:15007000:202:14007100:202:13007200:22:1200730:202:1107400:2021000:500:2702:100660:2002:1906700:20:180206800202::70016900:02:13600600:302:15006000:302:14006100:302:13006200:32:1200630:302:1106400:3021000:500:3602:100560:3002:1905700:30:180025800302::70051900:02:16400500:402:15005000:402:14005100:402:13005200:42:1200530:402:1105400:402100:500:4502:100460:4002:1904700:40:180024800402::70041900:02:16500400:502:1500400:502:14004100:502:13004200:52:1200430:502:1104400:502100:500:5402:100360:5002:1903700:50:180023800502::70031900:02:16600300:602:1500300:602:14003100:602:13003200:62:1200330:602:1103400:602100:500:6302:100260:6002:1902700:6:180022800620::70012900::0212760000::750221000500::572021004100::57201200320::75021203200::75221004100:5720:10020500::7021250060::740022107900::470210028800:4720:20017900::40212800600::842021000500::482012004100::84021200230::84022103200::48221001400:4820:10020500::8021240060::830202107900::380120028800:8302:20019700::30221900600::392021005000::392012004100::93021200230::9302021032000::392120001400:9302:10020050::9022130006::29202100097::2900012000880:29202002:0097:2202:2002006::22022000005::200222000040::02022200013::0202022000022::202220000013:0202:20000240::00222002005::10202200096::0100222000087::010220020087:1020:20000269::12022001005::110222000004::11022200013::0112022000022::112220000013:1102:00200240::2010022001::52001002200::960210022000::780100022002:87200100:2002::692000022002::50220002200::042020002200::13200202200::222200022000:31200200:2002:004202000020:::50200220210::02602900210::079002900122:2088009200:12::027900900213::02600003912::200005009312::200041093210::020320039210::200003293120:2000004193:21::020000532109::02000638120::000007983122:0200008883:12::020000798214::20000068412::200000508421::020004148210::020002348120::0020032840120000:2014:84200002::100542100020::801267400000::2217497000002::14788000002::212797000020::512756000020::2175500002::00125741020::00012570230::0020210752300002::0570140:21020:100500:026720:10600:02070:10609:202082010608:0:0902:0163070:03020:10606:03200:10600:503021:1060:40003220:106:30032003:10060023204::06:001312005:100:0032066:001005:00307:0100:520923008100:5028:00:9100:54027400:10:502004600:100:5204500:1001:0520404:102:050200403:01305200420::004:02500141:10:50020040:0100:65040400:10:2072400401090:82040:00:18:29040505000:1:72004005:1:62000000405:1:250010045:1:0200400245:120000003345:::2000024415:10:02000015050:0:10000460300:50:2210070032:51000009028:::3100002:8:9130660000020::176300002::163600000020::0016301500020:01632040200:::00600333002:00:1016034200:0:1006200050:2100:1062002:360000102602:0007:001200:0:2008:0012970:0000272:01900800:702020:100:077020002:100000720000060:10200:020107000::50002:00070200:01:2402000071003:0021:20000200030000027::1040002002:7010520000:1:7101020:620020:1:10000000:20000:711082:0:210880180::100298912::12:82::812:2220:28221:17:0:18::1:1181:6:11008:125:181:13:814:1::1:::4::0939059209169099911::799900800900091001827346560789543210

0.00200 0.00175 0.00150 0.00125 0.00100 0.00075 0.00050 0.00025
1.00000 0.99995 0.99990 0.99985 0.99980 0.99975

IS UM-IS

0.05 0.04 0.03

0.02

0.01

0.00 106

1.00

0.98

0.96

0.94

IS 0.92

UM-IS

0.90

0.88

106

IS UM-IS

0.008 0.007 0.006 0.005 0.004 0.003 0.002 0.001

106

1.00

0.99

0.98

IS UM-IS

0.97 0.96 0.95

106

IS UM-IS UM-IS_basic
106
IS UM-IS UM-IS_basic
106

IS 106 UM-IS
105
number o10f6 samples
(a) Synthetic graph, 96 nodes.

106 IS UM-IS
105 104 103
number o10f6 samples
(b) Synthetic graph, 768 nodes.

IS UM-IS 105 UM-IS_basic
104
number o10f6 samples
(c) Medical PGM, 1216 nodes.

Figure 4: Performance on three different graphical models. We applied inference trough importance sampling with and without the support of a trained UM and evaluate it in terms of Pearson Correlation Coefficient (PCC), Mean Absolute Error (MAE) and Effective Sampling Size (ESS). The medical PGM was designed by medical experts and contains 1216 nodes, comprising of 335 risk factors, 525 symptoms and 356 diseases.

UM Architecture and Performance: We ran a hyperparameter search on the different network architectures and data representations. The algorithmic performance was not greatly affected for different types of data representations. We conjecture that this is due to the fact that neural networks are flexible models capable of handling different types of inputs efficiently by capturing the representations within the hidden layers. In contrast, the network architecture of the UM highly depends on the structure of the PGM. For this reason, a specific UM needs to be trained for each PGM. This task can be computationally expensive but once the UM is trained, it can be used to compute the approximate marginals in a single forward pass on any set of evidence.
UM for Inference in PGMs: In order to evaluate the performance of sampling algorithms, we monitor the change in PCC and MAE on the test sets with respect to the total number of samples. We notice that across all experiments, a faster increase in the maximum value or the PCC is observed when the UM predictions are used as proposals for importance sampling. This effect becomes more pronounced as the size of the graphical model increases. Figure 4 indicates standard IS (blue
7

Under review as a conference paper at ICLR 2019

(a) Smoke, Obesity embeddings.

(b) Diabetes embeddings.

Figure 5: The plots show PCA over the embeddings filtered for two set of symptoms and risk factors, where each scatter point corresponds to a set of evidence. The display embedding vectors correspond to the first two components and they separate quite well unrelated medical concepts and show an overlap for concepts which are closely related.

line) reaches PCC close to 1 and an MAE close to 0 on the small network with 96 nodes. This is because the convergence of both algorithms is very fast on small networks and only a small number of samples is needed to cover most of the possible variations in the evidence. However, UM-IS (orange-line) still outperforms IS and converges faster, as it can be seen in Figure 4. In the synthetic graph with 798 nodes. In this setting, standard IS obtains a MAE of 0.012 with 106 samples, whereas the UM-IS error is 3 times lower error (0.004) for the same number of samples. The same conclusions can also be drawn for PCC. On the large medical PGM (Figure 4 (c)), the UMIS exhibits better performance than standard IS both in terms of MAE and PCC while still using 10 times less samples. In other words, the costs and time of the sampling algorithm can be reduced by a factor of 10 or more in this real world applications by using our proposed model. We expect this improvement to be even stronger on larger graphical models (see Figure 6 in the appendices for further details). We also include the results of a simple UM architecture as a baseline. The simple UM (UM-IS-Basic) has one single layer for all nodes. By using the UM-IS-Basic model, the MAE and PCC still improved over standard IS. However, UM-IS with multiple fully connected layers per group of nodes outperforms the basic UM by a large margin in terms of MAE and PCC. There are two reasons why it performs better. First, the model capacity of the UM is higher (due to the larger number of fully connected layers) which allows to learn more complex structures in the data. Secondly, the losses in the UM are spread across all group of nodes and the gradient update steps are optimized with the right order of magnitude for each group.
UM for Graph Embedding: The graph embeddings are a low-dimensional representation of the evidence set. These embeddings are extracted from the inner layer of the UM (see Fig.3) and the graphs structure is preserved and can be visualized in a lower dimensional space. For example on Figure 5b show the embeddings of evidence sets with 4 different at least 4 different active nodes. In this sets, Type 1 and Type 2 diabetes are collocated over the first two principle components. Although the two disease have different underlying cause (i.e pancreatic beta-cell atrophy and insulin-resistance respectively), they share similar symptoms and complications (e.g cardiovascular diseases, neuropathy, increased risk of infections etc.). Figure 5a shows a similar clustering of two cardiovascular risk factors: smoking and obesity, interestingly collocated with a sign seen in patient suffering from a severe heart condition (i.e unstable angina, or acute coronary syndrome): chest pain at rest. Further experiments on node classification are in the Appendix 8.
6. CONCLUSION
This paper introduces a Universal Marginaliser based on a neural network which can approximate all conditional marginal distributions of a PGM. We have shown that a UM can be used via a chain decomposition of the BN to approximate the joint posterior distribution, and thus, the optimal proposal distribution for importance sampling. While this process is computationally intensive, a first-order approximation can be used requiring only a single evaluation of a UM per evidence set. Our experiments show that the UM-IS procedure delivers significant improvements in sampling efficiency when compared against standard importance sampling. This speed-up shows how to scalable up our proposed inference scheme on large PGMs in interactive applications with a minimum of errors.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Pierre Baldi and Peter Sadowski. The dropout learning algorithm. Artificial intelligence, 210:78­ 122, 2014.
Jian Cheng and Marek J. Druzdzel. AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks. Journal of Artificial Intelligence Research, 2000.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classification. Journal of machine learning research, 9(Aug):1871­1874, 2008.
Nir Friedman. Inferring cellular networks using probabilistic graphical models. Science, 303(5659): 799­805, 2004.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: masked autoencoder for distribution estimation. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 881­889, 2015.
Samuel Gershman and Noah Goodman. Amortized inference in probabilistic reasoning. In Proceedings of the Cognitive Science Society, volume 36, 2014.
Shixiang Gu, Zoubin Ghahramani, and Richard E Turner. Neural adaptive sequential monte carlo. In Advances in Neural Information Processing Systems, pp. 2629­2637, 2015.
W Keith Hastings. Monte carlo sampling methods using markov chains and their applications. Biometrika, 57(1):97­109, 1970.
David Heckerman. A tractable inference algorithm for diagnosing multiple diseases. In Machine Intelligence and Pattern Recognition, volume 10, pp. 163­171. Elsevier, 1990.
Tom Heskes. Stable fixed points of loopy belief propagation are local minima of the bethe free energy. In Advances in neural information processing systems, pp. 359­366, 2003.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Tommi S Jaakkola and Michael I Jordan. Variational probabilistic inference and the QMR-DT network. Journal of artificial intelligence research, 10:291­322, 1999.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183­233, 1999.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
Tuan Anh Le, Atilim Gunes Baydin, Robert Zinkov, and Frank Wood. Using synthetic data to train neural networks is model-based reasoning. arXiv preprint arXiv:1703.00868, 2017.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030, 2014.
Quaid Morris. Recognition networks for approximate inference in bn20 networks. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI'01, pp. 370­377, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1-55860-800-1. URL http://dl.acm.org/citation.cfm?id=2074022.2074068.
Kevin P Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pp. 467­475. Morgan Kaufmann Publishers Inc., 1999.
9

Under review as a conference paper at ICLR 2019

Radford M Neal. Annealed importance sampling. Statistics and computing, 11(2):125­139, 2001.
Andrew Y Ng and Michael I Jordan. Approximate inference algorithms for two-layer bayesian networks. In Advances in neural information processing systems, pp. 533­539, 2000.
Brooks Paige and Frank Wood. Inference networks for sequential Monte Carlo in graphical models. In International Conference on Machine Learning, pp. 3040­3049, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Daniel Ritchie, Paul Horsfall, and Noah D Goodman. Deep amortized inference for probabilistic programs. arXiv preprint arXiv:1610.05735, 2016.
Michael Shwe and Gregory Cooper. An empirical analysis of likelihood-weighting simulation on a large, multiply connected medical belief network. Computers and Biomedical Research, 24(5): 453­475, 1991.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929­1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html.
Uber AI Lab. Pyro AI: Deep Universal Probabilistic Programming. http://pyro.ai/, 2017.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1­2):1­305, 2008.

APPENDICES
In this section, we review IS and describe how it is used for computing the marginals of a PGM given a set of evidence.

6.1. SAMPLING WITH A PROPOSAL DISTRIBUTION

In BN inference, Importance Sampling (IS) is used to provide the posterior marginal estimates P (XU |XO). To do so, we draw samples xU from a distribution Q(XU |XO), known as the proposal distribution. The proposal distribution must be defined such that we can both sample from
and evaluate it efficiently. Provided we can evaluate P (XU , XO), and that this distribution is such that XU contain the Markov boundary of XO along with all its ancestors, IS states that we can form posterior estimates:

P (XU = xU |XO = xO) =

1xU

(x)

P (x|xO Q(x|xO

) )

Q(x|xO

)dx

= Q(xO) P (xO)

1xU

(x)

P (x, Q(x,

xO xO

) )

Q(x|xO

)dx

n

= lim
n

1xU (xi)

i=1

wi
n
j=1

wj

,

(4)

where xi  Q and wi = P (xi, xO)/Q(xi, xO) are the importance sampling weights and 1xU (x) is an indicator function for xU .
The simplest proposal distribution is the prior, P (XU ). However, as the prior and the posterior may be very different (especially in large networks) this is often an inefficient approach. An alternative is to use an estimate of the posterior distribution as a proposal. In this work, we argue that the UM learns an optimal proposal distribution.

10

Under review as a conference paper at ICLR 2019

F1 Precision Recall Accuracy

Linear SVC

dense

input

0.67 ± 0.01 0.84 ± 0.03 0.58 ± 0.02 0.69 ± 0.01

0.07 ± 0.00 0.20 ± 0.04 0.05 ± 0.00 0.31 ± 0.01

RBF SVC

dense

input

0.68 ± 0.01 0.85 ± 0.02 0.58 ± 0.02 0.69 ± 0.01

0.05 ± 0.01 0.16 ± 0.09 0.04 ± 0.00 0.32 ± 0.01

Ridge

dense

input

0.66 ± 0.04 0.81 ± 0.06 0.59 ± 0.04 0.63 ± 0.02

0.17 ± 0.01 0.22 ± 0.04 0.16 ± 0.01 0.27 ± 0.01

Table 1: Classifier performances for three different classifiers. Each classifier is trained on - "dense" the dense embedding as features, and "input" - the top layer (UM input) as features. The target (output) is always the disease layer.

6.2. SAMPLING FROM THE POSTERIOR MARGINALS
Take a BN with Bernoulli nodes and of arbitrary size and shape. Consider 2 specific nodes, Xi and Xj, such that Xj is caused only and always by Xi:
P (Xj = 1|Xi = 1) = 1, P (Xj = 1|Xi = 0) = 0.
Given evidence E, we assume that P (Xi|E) = 0.001 = P (Xj|E). We will now illustrate that using the posterior distribution P (X|E) as a proposal will not necessarily yield the best result.
Say we have been given evidence, E, and the true conditional probability of P (Xi|E) = 0.001, therefore also P (Xj|E) = 0.001. We naively would expect P (X|E) to be the optimal proposal distribution. However we can illustrate the problems here by sampling with Q = P (X|E) as the proposal.
Each node k  N will have a weight wk = P (Xk)/Q(Xk) and the total weight of the sample will be
N
w = wk.
k=0
The weights should be approximately 1 if Q is close to P. However, consider the wj. There are four combinations of Xi and Xj. We will sample Xi=1, Xj=1 only, in expectation, one every million samples, however when we do the weight wj will be wj = P (Xj = 1)/Q(Xj = 1) = 1/0.001 = 1000. This is not a problem in the limit, however if it happens for example in the first 1000 samples then it will outweigh all other samples so far. As soon as we have a network with many nodes whose conditional probabilities are much greater than their marginal proposals this becomes almost inevitable. A further consequence of these high weights is that, since the entire sample is weighted by the same weight, every node probability will be effected by this high variance.
7. PERFORMANCE ON LARGE GRAPHICAL MODELS
8. NODE CLASSIFICATION WITH UM EMBEDDING
UM for Node Classification: The UM incorporates a encoding step, encoding the input layer into one shared Embedding layer (as discussed in Section 5.2). To asses the quality of this embedding step, we perform classification experiments. First, we train a classifier from the input layer S to the output layer P . Then, we compare to a classifier trained on the dense shared embedding to the output layer P . The dense shared embedding should encode all information present in the input layers separated for prediction (compare Figure 5). We compute embeddings for 853 samples with 14 diseases and use them to train an SVM classifier (Fan et al., 2008) for disease detection.
See the comparison of classifier performance in Table 1. The learnt embedding significantly increases the performance for each classifier (about one order of magnitude). Thus, the UM provides a useful encoding of the input layer for classification.

11

Under review as a conference paper at ICLR 2019

ESS PCC MAE ESS PCC MAE

0010:08104:08105:08106:08107:08180:809010:910:09100:09110:09120:09130:09140:09150:09160:90710:9081:090920:0020:0120:01200:100200:100201:100202:100203:100204:100250:100260:100270:10028:0100290:01210:101200:101210:101220:101230:101240:101250:101260:101270:10128:0101290:0120:102200:102210:102220:102230:102240:102250:10226:0102270:10320:10420:10520:10620:10720:1082:010920:0220:20020:20120:20220:20320:20420:20520:20620:20720:2082:020920:0320:30020:30120:30220:30320:30420:30520:30620:30720:3082:030920:0420:40020:40120:04220:04230:04240:04250:04260:04270:04280:409020:520:05200:05201:05202:05203:0524:55

001:83

002:56

001:82

002:57

001:81

002:58

001:80

002:59

001:8 002:6

001:79

002:60

001:78

002:61

001:77

002:62

001:76

002:63

001:75

002:64

001:74

002:65

001:73

002:66

001:72 001:71

002:67 002:68

001:70

002:69

001:7 002:7

001:69

002:70

001:68

002:71

001:67

002:72

001:66

002:73

001:65

002:74

001:64

002:75

001:63

002:76

001:62

002:77

001:61

002:78

001:60

002:79

001:6 002:8

001:59

002:80

001:58

002:81

001:57

002:82

001:56

002:83

001:55

002:84

001:54

002:85

001:53

002:86

001:52

002:87

001:51

002:88

001:50

002:89

001:5 002:9

001:49

002:90

001:48

002:91

001:47

002:92

001:46

002:93

001:45

002:94

001:44

002:95

001:43

002:96

001:42

002:97

001:41

002:98

001:40

002:99

001:4 000:0

001:39

000:1

001:38

000:10

001:37

000:100

001:36

000:101

001:35

000:102

001:34

000:103

001:33

000:104

001:32

000:105

001:31

000:106

001:30

000:107

001:3 000:108

001:29

000:109

001:28

000:11

001:27

000:110

001:26

000:111

001:25

000:112

001:24

000:113

001:23

000:114

001:22

000:115

001:21

000:116

001:20

000:117

001:2 000:118

001:19

000:119

001:18

000:12

001:17

000:120

001:16

000:121

001:15

000:122

001:14

000:123

001:13

000:124

001:127

000:125

001:126

000:126

001:125

000:127

001:124 001:123

000:13 000:14

001:122

000:15

001:121

000:16

001:120

000:17

001:12

000:18

001:119

000:19

001:118

000:2

001:117

000:20

001:116

000:21

001:115

000:22

001:114

000:23

001:113

000:24

0010:01101:021110:11001100:1101:01100:091100:081100:701100:601100:501100:401100:301100:201100:110010:010010:1010:0000:90900:90800:90700:90600:90500:90400:90300:90200:9010:090000:0900:80900:80800:80700:80600:80500:80400:80300:80200:8010:080000:0800:70900:70800:70700:70600:70500:70400:70300:70200:7010:07000:0700:60900:60800:60700:60600:60500:60400:60300:60200:6010:06000:0600:50900:50800:50700:50600:50500:50400:50300:50200:5010:050000:0500:40900:40800:40700:40600:40500:40400:40300:40200:4010:040000:0400:03090:03080:03070:03060:03050:03040:03030:03020:03010:300000:300:02009:02008:02007:0206:25

0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001111111100111100111100011111000011100110011000011001101110011001100110011001011010011000::::::::11::::0000::::111:::::00110:::11::00000::0111::00::100:::33330033::1330333333::3300113::0033033::3311:0030::33133:330000::1113333::300113300:::3311::3300033::301133001:::3011:330022221132:32331230023322300333::31113031330031::00113341:4133030411100:::41341311::14003004223::0410000042341::4011:2000::0050113328796::501450:23003223140005110516::9718832050079116:005005231430505023::11232350041::2350005113600957932:11008895:397::09610003205::069410011239600::23:963221100::004126923060511::9606003296117::23000098963::11892360007:111682300600::0086151132248::00321100933870::342011870015::2378001106700::32877100:::22897800011008932::8707117823::006010073321::15007723002411::000773332:110782400123::1187150032::60781100:3007887:1123980008700::01198327800::002711:278::006001237851::000083211423::68000023033111::862432000107::6513200001:6906320011000796::32998001196100:::39896110:10327691100:23006026911::0001520009611::24:00290033::142:0950024::11424950061500::12411504006000::42115700:0240518900::4205000981100:050::47100110005::426110005005242:11005140042::11000320:200::01132040024::0014150440042::00511042400::0016411420::007900411124:008810041142::790410011:46::00141001124500::1400002112441:41000321124::100032:211::13041000024114::3104052400001::31642000113::7912400110032::88022432001100:972423100::006:43211::00500142324111::000022403223:11000042223::11000:22214::1100302422005::1140000242260::1000024279::110238824100::31100239742000:11006324200::10523:00400::1141234200102::112332240000:230113420000::143111:202::00501311240040::603111420079::00112400088::1400112441097::41001124006:1400011425000::14:00411400::14240011232100::414211023004:241101400400::1:211005004::02411600404::00042971100::002488100500::00542791000150::0246110005:0024050110500:100:14410500::0032421150100::230041:11500:141400115200::05:1001159::0060140045911::700410911::890410009611689::00140006917::004109611160::4100009601115:14000069142:::400960011331::4100196001142:41006005111:::10680090611::4146800000711::4180008911::1478078900011::14870070001::418706000111::1487150000011:14784200001:::487330000111::1418742000011:14715000011:::17780600011::4147770000011::1479800011::4187889000011::41787000001::147806000111::1487510000011:14872400100:::4783311001::00141784211:000014815::11:0000168706::11001446807::110014609800::116941989000::11697410000::6069411000::519611411000004296:411100033:69::410000004296::14110000519:114111000060695:11100007:59:::11100098555::15110000895::51110007050::1511000060515::1000505015::110000145051:110003250:5:11000230515::100014051::110005050415:110000604:1:1000974051::11000088451:110079014:1:1100060145:1:100050045:1:1100142045:11100303045::1102040415::110010505:1:1000460305:1:1100703051:11090803:1:1108090360:1:11073060:1:1100603060:1:150103060:1:11402036:01113030306::0112403061:0:11015006:10:100630260:10:1107206010:1198020:01:181902700:10:1710270:1:016100027:01:01510207:01:011420207:1013130207::0012142071:0:01110570:10:001621070:10:1071107010:0918110:010:81911800:10:0711108:10:601011008:10:50111080:10:401121080:1301311080::021041180:1:01105108:01:000611008:01:1007100801:0918010:001:80191009:10:7011090:1:601001900:1:5011090:104:02110090:130310090::210410910:011051000:90:0060010:90:0207000990:0208002:90:09900002:90:0:80010290:10:7000290:10:0600290:10:5000029:100:40000290:110:300290:120:0200029013:100002:0014:00009080:150:0200080:16090002080::70800020180:8:0700208:019:0160208:01:05001208:10:04001208:011:031028:021:00210280301:1010:204001:0018070:510:0201070:61090100270::70801007208:10:70007209:1026:00720:105:02007200:14:02007201:103:020722:102:00207203101:200:2041000:2700605:10:02200606:10902006207::08020016280:07:0006290:1036:00620:105:03006200:14:03006210:103:030622:102:03006230101:300:240100:0360055:010:0320056:01090300527::0080301528:007:00529:0146:00502:1005:045200:1004:4005211003::4002021502:1400320:100:41051402100::40050015001::04002605100:40009702:500:040018820500::00079205105::000602150::05005201500::50004125100::50002320150:05003220:100:05051402100::500:5001400::5500260410:5900702:40:95001892040::90089204160::900702140::96000620140::690512040::690024210::96003320140:6902420:10:6900451010::60026001490:6800702:40:68001890240::80098204170::800702410::87000620140::870512040::780042210::87003320140:7802420:10:7800451010::70026001480:7700702:40:77001890240::70098204180::700702410::78000602140::780512040::87004210::78003320140:7802420:10:8700415010::8002601470:8600720:40:86001890240::6009820140::9600702410::9600602140::690152040::96004210::6900330214:69002420:10:960415001::900601460:2590072040:9500:980240::5200982040::50072040::50060240::5052040::50041200::5002304:50032020::050414020::00502450::400260420:4009702:40:400288040::4007920410::40060240::41052040::140041200::14002304:41003220:0:140140240::10050240::3100260420:13009702:40:3100288040::300792040::230060240::32052040::320041200::23003204:32003220:0:230414200::200502430::220062040:22009702:40:2200880240::2009720430::20060240::23050240::230041200::32003204:23003220:0:230414200::300502420::1300260240:31009720:40:3100288040::1009720440::10060240::410050240::140014200::410032024:14003202:0:140414200::4000504210::0400260240:04009720:40:4000288040::0009720450::00062040::500050240::050014200::500032204:50003202:0:05441200::500:050320::5400260230:9500720:30:5900298030::90098203600::90072030::69006002300::9601520300::690042230::69003302300:9604220:00:69003150200::600260023009:8600720:300:680029802300::80098023007::80072030::78006002300::8701520300::870042230::78003302300:8704220:00:78003150200::700260023008:7700720:003:770029802003::00798020038::00720703::00870260003::0782015003::007824203::00870233003:078204200::00873015002::008206000237:006820700:3:008622098003::00602980023::0069207023::009602600032::09602001523::0069202432::00960203332:0069200042:2:06930000152::00900006236:0059200073:0095:2000983::005302009833::0052007330::0050200633::00502000533::00502001433::00050203233:0050020032:3:050200031403::000000533::005240000633:00400020:973:00040002388300::4020973300::1400206330::140002053300::41002331400::01402333200:140002:3320:410020303410::1000330500::423100033600:310020:39700:130002338800::3020339700::2300203360::230002335000::00322331400::00232330032:002302:3032:003220033041::0020330005::320022033006:002220:30079:002202330088::0202330079::3002200336::002302003350::00223003341::002230033:320002320:3:230020230303::4100030033::0500220310033:600201300:3:790002310033::8800210033::97004201033::00602410033::00502140033::01402140033:003202410:3:002320140033::0041040033::000520140003300:6020400:300:79204000330::8802000330::97205003300::60250003300::5025000033::014205000033:2302050:003:0232000053003::041005:0023::200500530032:206009500:2:20700590032::0298009023::208900690032::2070960023::02600069032::21500960032::02240006923:0203300069:3:00042009623::20005100623:02000600986:2:20007008632::0200980823::200089007832::2000078723::02000607832::20000158732::02000247823:0200003378:3:00000428723::20000051723:02000006877:2:20000077732::0200089723::2000000898732::20000778::230200006087::232000005178::320200002487:2302000003387:::30000042782::322200000518:2302000006786:::2200000768::33202000896::20000023089996::002003270::00026902300600::296230000020::510693200002:2496230000000:::3396300000::24326920000022:223519000020:29506600020::::25970000002::244548900020::425890000002::4250070002::2405600000200::42055000002:2450140000020:::4052300000::42250230000022::24041000002:420450500020:::24000600002::244400079020::420048800020::42041197002::2400410006200::42001405002:0042410014002::00:41400230::00422001423022::002400141200:4200134205000::0:2003102600::240413207900::42000320880::420032220079::0002423020006::04223020050:0042002320041::00:40002323::002420200232::2300240202:00414200202203::0050:20002220::624002042200::97042020200::880042202300::3009724200320::00642200032:0005042020023::00:41400032::004202232200320::02422300003:0202414000013::2:02250000::310224006400::132024007900::10242000088::1402424000079::142000420060:410200004250::1400000:441::2041000242223::020004142023:020000424::140200004:21::020050040420004::002060402400::00279042000::2088500240000520::7900542000002:6504200000:00500:0540020::02415040002::20023415000200::00002314520::00:1145202::00000415950020::000400140295600::04120097::0002410069890::000600021496::89000004169:00702014::69000620002:400::9600051201400:69024120140::00963302:001000::602240000041::6820451000149::86200000641::20800000741::0020000788914::70000871489:002000784100::7020002780:4::00200687000:14205100780001::1420240078000:20::100330720::41002400277::00414001502080077::4100060200::741000007::87410000200:9800841780022::00089418720000:::4787200000:0410687020000::04100021875100::00:1200874200::002418033::00021400408642::0020004108600::70510000410006::06014000020:9670004102::690000928941020::0096:409802000:29601420007:6902000410::06100020961::05100200:96100::002024005190::002:003305159::00005024051::2095000600215:00510550000::51206050000000::5100000570:0::005000005089200:51000020050::98510000020::500010157020:0050:10006020000:000055120000005:0002150000000:00410000:2010045000000:0230100004:000200:000102300:0502400000:100:2145040000::02:0020005050014:0201:0000005002:4100600:01000000002005000::410971002015:00104200880:05:0201000400:00:7905001000420:2510602:00:00:120:300052:03001:6002400:410:02103200060000:::502033062::01462:63602:210:0760:0301:210002608:0130020:2611932::216:203:21:62:29:213026::2122::328:22012221::::77:2:1:72:136:7:2:1:74:251:2::1:7::5::2174007:1:6321713:17:271::27:218:71::2111:9:128:1:10:1:8::1:::21808811981812881838:187914999619::59919991562117418309020090100001000102349586177896051234450678932109876543210

IS0.025 0.12

0.020

UM-IS

0.10

IS UM-IS

0.015 0.08

0.010 0.005 0.000
106

0.06 0.04 0.02
106

1.00 1.00

0.99 0.95 0.90
0.98 0.85

0.97 0.96 0.95

IS UM-IS

0.80 0.75 0.70

IS UM-IS

106 106

106 IS UM-IS
105
104
103
number o10f6 samples

IS 103 UM-IS
102
number o10f6 samples

(a) Synthetic graph, 384 nodes.

(b) Synthetic graph, 1536 nodes.

Figure 6: Additional experimetns on very large synthetic graphs.

12

