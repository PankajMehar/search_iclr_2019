Under review as a conference paper at ICLR 2019
A CONVERGENCE ANALYSIS OF GRADIENT DESCENT FOR DEEP LINEAR NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parametrized as x  WN · · · W1x) by minimizing the 2 loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks (Bartlett et al., 2018).
1 INTRODUCTION
Deep learning builds upon the mysterious ability of gradient-based optimization methods to solve related non-convex problems. Immense efforts are underway to mathematically analyze this phenomenon. The prominent landscape approach focuses on special properties of critical points (i.e. points where the gradient of the objective function vanishes) that will imply convergence to global optimum. Several papers (e.g. Ge et al. (2015); Lee et al. (2016)) have shown that (given certain smoothness properties) it suffices for critical points to meet the following two conditions: (i) no poor local minima -- every local minimum is close in its objective value to a global minimum; and (ii) strict saddle property -- every critical point that is not a local minimum has at least one negative eigenvalue to its Hessian. While condition (i) does not always hold (cf. Safran and Shamir (2018)), it has been established for various simple settings (e.g. Choromanska et al. (2015); Kawaguchi (2016)). Condition (ii) on the other hand seems less plausible, and is in fact provably false for models with three or more layers (cf. Kawaguchi (2016)), i.e. for deep networks. It has only been established for problems involving shallow (two layer) models, e.g. matrix factorization (e.g. Ge et al. (2016); Du et al. (2018a)). The landscape approach as currently construed thus suffers from inherent limitations in proving convergence to global minimum for deep networks.
A potential path to circumvent this obstacle lies in realizing that landscape properties matter only in the vicinity of trajectories that can be taken by the optimizer, which may be a negligible portion of the overall parameter space. Several papers (e.g. Saxe et al. (2014); Arora et al. (2018)) have taken this trajectory-based approach, primarily in the context of linear neural networks -- fully-connected neural networks with linear activation. Linear networks are trivial from a representational perspective, but not so in terms of optimization -- they lead to non-convex training problems with multiple minima and saddle points. Through a mix of theory and experiments, Arora et al. (2018) argued that such non-convexities may in fact be beneficial for gradient descent, in the sense that sometimes, adding (redundant) linear layers to a classic linear prediction model can accelerate the optimization. This phenomenon challenges the holistic landscape view, by which convex problems are always preferable to non-convex ones.
Even in the linear network setting, a rigorous proof of efficient convergence to global minimum has proved elusive. One recent progress is the analysis of Bartlett et al. (2018) for linear residual networks -- a particular subclass of linear neural networks in which the input, output and all hidden dimensions are equal, and all layers are initialized to be the identity matrix (cf. Hardt and Ma (2016)).
1

Under review as a conference paper at ICLR 2019

Through a trajectory-based analysis of gradient descent minimizing 2 loss over a whitened dataset (see Section 2), Bartlett et al. (2018) show that convergence to global minimum at a linear rate -- loss is less than > 0 after O(log 1 ) iterations -- takes place if one of the following holds: (i) the objective value at initialization is sufficiently close to a global minimum; or (ii) a global minimum is attained when the product of all layers is positive definite.
The current paper carries out a trajectory-based analysis of gradient descent for general deep linear neural networks, covering the residual setting of Bartlett et al. (2018), as well as many more settings that better match practical deep learning. Our analysis draws upon the trajectory characterization of Arora et al. (2018) for gradient flow (infinitesimally small learning rate), together with significant new ideas necessitated due to discrete updates. Ultimately, we show that when minimizing 2 loss of a deep linear network over a whitened dataset, gradient descent converges to the global minimum, at a linear rate, provided that the following conditions hold: (i) the dimensions of hidden layers are greater than or equal to the minimum between those of the input and output; (ii) layers are initialized to be approximately balanced (see Definition 1) -- this is met under commonplace near-zero, as well as residual (identity) initializations; and (iii) the initial loss is smaller than any loss obtainable with rank deficiencies -- this condition will hold with probability close to 0.5 if the output dimension is 1 (scalar regression) and standard (random) near-zero initialization is employed. Our result applies to networks with arbitrary depth and input/output dimensions, as well as any configuration of hidden layer widths that does not force rank deficiency (i.e. that meets condition (i)). The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the case of scalar regression, they are met with constant probability under a random initialization scheme. We are not aware of any similarly general analysis for efficient convergence of gradient descent to global minimum in deep learning.
The remainder of the paper is organized as follows. In Section 2 we present the problem of gradient descent training a deep linear neural network by minimizing the 2 loss over a whitened dataset. Section 3 formally states our assumptions, and presents our convergence analysis. Section 4 gives a review of relevant literature, including a detailed comparison of our results against those of Bartlett et al. (2018). Finally, Section 5 concludes.

2 GRADIENT DESCENT FOR DEEP LINEAR NEURAL NETWORKS

We denote by v the Euclidean norm of a vector v, and by A F the Frobenius norm of a matrix A.

We are given a training set {(x(i), y(i))}mi=1  Rdx × Rdy , and would like to learn a hypothesis (predictor) from a parametric family H := {h : Rdx  Rdy |   } by minimizing the 2 loss:

1 min L() :=  2m

m i=1

h(x(i)) - y(i) 2 .

When the parametric family in question is the class of linear predictors, i.e. H = {x  W x | W 

Rdy×dx }, the training loss may be written as L(W )

=

1 2m

WX - Y

2 F

,

where

X

 Rdx×m

and Y  Rdy×m are matrices whose columns hold instances and labels respectively. Suppose now

that the dataset is whitened, i.e. has been transformed such that the empirical (uncentered) covariance

matrix

for

instances

--

xx

:=

1 m

X

X

 Rdx×dx -- is equal to identity. Standard calculations

(see Appendix A) show that in this case:

1 L(W ) =
2

W - yx

2 F

+c,

(1)

where yx

:=

1 m

Y

X

 Rdy×dx is the empirical (uncentered) cross-covariance matrix between

instances and labels, and c is a constant (that does not depend on W ). Denoting  := yx for brevity,

we have that for linear models, minimizing 2 loss over whitened data is equivalent to minimizing

the squared Frobenius distance from a target matrix :

minW Rdy×dx

L1(W ) :=

1 2

W

-

2 F

.

(2)

Our interest in this work lies on linear neural networks -- fully-connected neural networks with linear activation. A depth-N (N  N) linear neural network with hidden widths d1, . . . , dN-1  N

2

Under review as a conference paper at ICLR 2019

corresponds to the parametric family of hypotheses H := {x  WN WN-1 · · · W1x | Wj  Rdj×dj-1 , j = 1, . . . , N }, where d0 := dx, dN := dy. Similarly to the case of a (directly pa-
rameterized) linear predictor (Equation (2)), with a linear neural network, minimizing 2 loss over whitened data can be cast as squared Frobenius approximation of a target matrix :

minWj Rdj ×dj-1 , j=1,...,N

LN (W1, . . . , WN ) :=

1 2

WN WN-1 · · · W1 - 

2 F

.

(3)

Note that the notation LN (·) is consistent with that of Equation (2), as a network with depth N = 1 precisely reduces to a (directly parameterized) linear model.

We focus on studying the process of training a deep linear neural network by gradient descent, i.e. of tackling the optimization problem in Equation (3) by iteratively applying the following updates:

LN Wj(t + 1)  Wj(t) -  Wj W1(t), . . . , WN (t) , j = 1, . . . , N , t = 0, 1, 2, . . . , (4)

where  > 0 is a configurable learning rate. In the case of depth N = 1, the training problem in Equation (3) is smooth and strongly convex, thus it is known (cf. Boyd and Vandenberghe (2004)) that with proper choice of , gradient descent converges to global minimum at a linear rate. In contrast, for any depth greater than 1, Equation (3) comprises a fundamentally non-convex program, and the convergence properties of gradient descent are highly non-trivial. Apart from the case N = 2 (shallow network), one cannot hope to prove convergence via landscape arguments, as the strict saddle property is provably violated (see Section 1). We will see in Section 3 that a direct analysis of the trajectories taken by gradient descent can succeed in this arena, providing a guarantee for linear rate convergence to global minimum.

We close this section by introducing additional notation that will be used in our analysis. For
an arbitrary matrix A, we denote by max(A) and min(A) its largest and smallest (respectively) singular values.1 For d  N, we use Id to signify the identity matrix in Rd×d. Given weights W1, . . . , WN of a linear neural network, we let W1:N be the direct parameterization of the end-to-end linear mapping realized by the network, i.e. W1:N := WN WN-1 · · · W1. Note that LN (W1, . . . , WN ) = L1(W1:N ), meaning the loss associated with a depth-N network is equal to the loss of the corresponding end-to-end linear model. In the context of gradient descent, we will
oftentimes use (t) as shorthand for the loss at iteration t:

(t) := LN (W1(t), . . . , WN (t)) = L1(W1:N (t)) .

(5)

3 CONVERGENCE ANALYSIS
In this section we establish convergence of gradient descent for deep linear neural networks (Equations (4) and (3)) by directly analyzing the trajectories taken by the algorithm. We begin in Subsection 3.1 with a presentation of two concepts central to our analysis: approximate balancedness and deficiency margin. These facilitate our main convergence theorem, delivered in Subsection 3.2. We conclude in Subsection 3.3 by deriving a convergence guarantee that holds with constant probability over a random initialization.

3.1 APPROXIMATE BALANCEDNESS AND DEFICIENCY MARGIN

In our context, the notion of approximate balancedness is formally defined as follows: Definition 1. For   0, we say that the matrices Wj  Rdj×dj-1 , j=1, . . . , N , are -balanced if:

Wj+1Wj+1 - Wj Wj


F

, j  {1, . . . , N - 1} .

Note that in the case of 0-balancedness, i.e. Wj+1Wj+1 = WjWj , j  {1, . . . , N - 1}, all matrices Wj share the same set of non-zero singular values. Moreover, as shown in the proof of
1 If A  Rd×d , min(A) stands for the min{d, d }-th largest singular value. Recall that singular values are always non-negative.

3

Under review as a conference paper at ICLR 2019

Theorem 1 in Arora et al. (2018), this set is obtained by taking the N -th root of each non-zero
singular value in the end-to-end matrix W1:N . We will establish approximate versions of these facts for -balancedness with  > 0, and admit their usage by showing that if the weights of a linear
neural network are initialized to be approximately balanced, they will remain that way throughout
the iterations of gradient descent. The condition of approximate balancedness at initialization is trivially met in the special case of linear residual networks (d0 = · · · = dN = d and W1(0) = · · · = WN (0) = Id). Moreover, as Claim 2 in Appendix B shows, for a given  > 0, the customary initialization via random Gaussian distribution with mean zero leads to approximate balancedness
with high probability if the standard deviation is sufficiently small.

The second concept we introduce -- deficiency margin -- refers to how far a ball around the target is from containing rank-deficient (i.e. low rank) matrices.

Definition 2. Given a target matrix   RdN ×d0 and a constant c > 0, we say that a matrix W  RdN ×d0 has deficiency margin c with respect to  if:2

W -  F  min() - c .

(6)

The term "deficiency margin" alludes to the fact that if Equation (6) holds, every matrix W whose distance from  is no greater than that of W , has singular values c-bounded away from zero:
Claim 1. Suppose W has deficiency margin c with respect to . Then, any matrix W (of same size as  and W ) for which W -  F  W -  F satisfies min(W )  c.
Proof. Our proof relies on the inequality min(A+B)  min(A)-max(B) -- see Appendix D.1.

We will show that if the weights W1, . . . , WN are initialized such that the end-to-end matrix W1:N has deficiency margin c > 0 with respect to the target , convergence of gradient descent to global minimum is guaranteed. Moreover, the convergence will outpace a particular rate that gets faster when c grows larger. This suggests that from a theoretical perspective, it is advantageous to initialize a linear neural network such that the end-to-end matrix has a large deficiency margin with respect to the target. Claim 3 in Appendix B provides information on how likely deficiency margins are in the case of a single output model (scalar regression) subject to customary zero-centered Gaussian initialization. It shows in particular that if the standard deviation of the initialization is sufficiently small, the probability of a deficiency margin being met is close to 0.5; on the other hand, for this deficiency margin to have considerable magnitude, a non-negligible standard deviation is required.
Taking into account the need for both approximate balancedness and deficiency margin at initialization, we observe a delicate trade-off under the common setting of Gaussian perturbations around zero: if the standard deviation is small, it is likely that weights be highly balanced and a deficiency margin be met; however overly small standard deviation will render high magnitude for the deficiency margin, and therefore fast convergence, improbable; on the opposite end, large standard deviation jeopardizes both balancedness and deficiency margin, putting the entire convergence at risk. This trade-off is reminiscent of empirical phenomena in deep learning, by which small initialization can bring forth efficient convergence, while if exceedingly small, rate of convergence may plummet ("vanishing gradient problem"), and if made large, divergence becomes inevitable ("exploding gradient problem"). The common resolution of residual connections (He et al. (2016)) is analogous in our context to linear residual networks, which ensure perfect balancedness, and allow large deficiency margin if the target is not too far from identity.
3.2 MAIN THEOREM
Using approximate balancedness (Definition 1) and deficiency margin (Definition 2), we present our main theorem -- a guarantee for linear convergence to global minimum:
Theorem 1. Assume that gradient descent is initialized such that the end-to-end matrix W1:N (0) has deficiency margin c > 0 with respect to the target , and the weights W1(0), . . . , WN (0) are
2 Note that deficiency margin c > 0 with respect to  implies min() > 0, i.e.  has full rank.

4

Under review as a conference paper at ICLR 2019

-balanced with  = c2

256 · N 3 ·



2(N -1)/N F

. Suppose also that the learning rate  meets:



c(4N -2)/N

6144 · N 3 ·



(6N -4)/N F

.

(7)

Then, for any > 0 and:

T



1  · c2(N-1)/N

· log

(0) ,

(8)

the loss at iteration T of gradient descent -- (T ) -- is no greater than .

3.2.1 ON THE ASSUMPTIONS MADE
The assumptions made in Theorem 1 -- approximate balancedness and deficiency margin at initialization -- are both necessary, in the sense that violating any one of them may lead to convergence failure. We demonstrate this in Appendix C. In the special case of linear residual networks (uniform dimensions and identity initialization), a sufficient condition for the assumptions to be met is that the target matrix have (Frobenius) distance less than 0.5 from identity. This strengthens one of the central results in Bartlett et al. (2018) (see Section 4). For a setting of random near-zero initialization, we present in Subsection 3.3 a scheme that, when the output dimension is 1 (scalar regression), ensures assumptions are satisfied (and therefore gradient descent efficiently converges to global minimum) with constant probability. It is an open problem to fully analyze gradient descent under the common initialization scheme of zero-centered Gaussian perturbations applied to each layer independently. We treat this scenario in Appendix B, providing quantitative results concerning the likelihood of each assumption (approximate balancedness or deficiency margin) being met individually. However the question of how likely it is that both assumptions be met simultaneously, and how that depends on the standard deviation of the Gaussian, is left for future work.
An additional point to make is that Theorem 1 poses a structural limitation on the linear neural network. Namely, it requires the dimension of each hidden layer (di, i = 1, . . . , N -1) to be greater than or equal to the minimum between those of the input (d0) and output (dN ). Indeed, in order for the initial end-to-end matrix W1:N (0) to have deficiency margin c > 0, it must (by Claim 1) have full rank, and this is only possible if there is no intermediate dimension di smaller than min{d0, dN }. We make no other assumptions on network architecture (depth, input/output/hidden dimensions).

3.2.2 PROOF

The cornerstone upon which Theorem 1 rests is the following lemma, showing non-trivial descent whenever min(W1:N ) is bounded away from zero:

Lemma 1. Under the conditions of Theorem 1, we have that for every t = 0, 1, 2, . . . :3



2(N -1)

dL1

2

(t + 1)  (t) - 2 · min W1:N (t)

N

·

dW W1:N (t)

.
F

(9)

Proof of Lemma 1 (in idealized setting; for complete proof see Appendix D.2). We prove the lemma here for the idealized setting of perfect initial balancedness ( = 0):

Wj+1(0)Wj+1(0) = Wj(0)Wj (0) , j  {1, . . . , N - 1} , and infinitesimally small learning rate (  0+) -- gradient flow:

W j(

)

=

-

LN Wj

W1( ), . . . , WN ( )

, j = 1, . . . , N ,   [0, ) ,

where  is a continuous time index, and dot symbol (in W j( )) signifies derivative with respect to time. The complete proof, for the realistic case of approximate balancedness and discrete updates (,  > 0), is similar but much more involved, and appears in Appendix D.2.

3

Note

that

the

term

dL1 dW

(W1:N

(t))

below

stands

for

the

gradient

of

L1(·)

--

a

convex

loss

over

(directly

parameterized) linear models (Equation (2)) -- at the point W1:N (t) -- the end-to-end matrix of the network

at iteration t. It is therefore (see Equation (5)) non-zero anywhere but at a global minimum.

5

Under review as a conference paper at ICLR 2019

Recall that (t) -- the objective value at iteration t of gradient descent -- is equal to L1(W1:N (t)) (see Equation (5)). Accordingly, for the idealized setting in consideration, we would like to show:

d d

L1

(W1:N

(

))



-

1 2

min

W1:N ( )

2(N -1)
N·

dL1 dW W1:N ( )

2
.
F

(10)

We will see that a stronger version of Equation (10) holds, namely, one without the 1/2 factor (which only appears due to discretization).

By (Theorem 1 and Claim 1 in) Arora et al. (2018), the weights W1( ), . . . , WN ( ) remain balanced throughout the entire optimization, and that implies the end-to-end matrix W1:N ( ) moves according to the following differential equation:

vec W 1:N ( ) = -PW1:N () · vec

dL1 dW (W1:N ( ))

,

(11)

where vec(A), for an arbitrary matrix A, stands for vectorization in column-first order, and
PW1:N () is a positive semidefinite matrix whose eigenvalues are all greater than or equal to min(W1:N ( ))2(N-1)/N . Taking the derivative of L1(W1:N ( )) with respect to time, we obtain the sought-after Equation (10) (with no 1/2 factor):

d d

L1

(W1:N

(

))

=

vec

dL1 dW W1:N ( )

, vec W 1:N ( )

=

dL1 vec dW W1:N ( )

, -PW1:N ( ) · vec

dL1 dW (W1:N ( ))

2(N -1)

dL1

 -min W1:N ( ) N · vec dW W1:N ( )

2

2(N -1)

dL1

2

= -min W1:N ( )

N

·

dW W1:N ( )

.
F

The first transition here (equality) is an application of the chain rule; the second (equality) plugs
in Equation (11); the third (inequality) results from the fact that the eigenvalues of the symmetric matrix PW1:N () are no smaller than min(W1:N ( ))2(N-1)/N (recall that · stands for Euclidean norm); and the last (equality) is trivial -- A F = vec(A) for any matrix A.

With Lemma 1 established, the proof of Theorem 1 readily follows:

Proof of Theorem 1. By the definition of L1(·) (Equation (2)), for any W  RdN ×d0 :

dL1 (W )

=

W

-



=

dW

dL1 (W )

2

= 2 · L1(W ) .

dW F

Plugging this into Equation (9) while recalling that (t) = L1(W1:N (t)) (Equation (5)), we have (by Lemma 1) that for every t = 0, 1, 2, . . . :

2(N -1)
L1 W1:N (t + 1)  L1 W1:N (t) · 1 -  · min W1:N (t) N .

Since

the coefficients

1-

·

min(W1:N

(t))

2(N -1) N

are

necessarily

non-negative

(otherwise would

contradict non-negativity of L1(·)), we may unroll the inequalities, obtaining:

L1 W1:N (t + 1)  L1 W1:N (0) ·

t 2(N -1)
1 -  · min W1:N (t ) N .
t =0

(12)

Now, this in particular means that for every t = 0, 1, 2, . . . :

L1 W1:N (t )  L1 W1:N (0) = W1:N (t ) -  F  W1:N (0) -  F .

Deficiency margin c of W1:N (0) along with Claim 1 thus imply min W1:N (t )  c, which when inserted back into Equation (12) yields, for every t = 1, 2, 3, . . . :

L1 W1:N (t)

 L1 W1:N (0)

·

2(N -1)
1-·c N

t
.

(13)

6

Under review as a conference paper at ICLR 2019



· c 2(N-1) N

is

obviously

non-negative,

and

it

is

also

no

greater

than

1

(otherwise

would

contradict

non-negativity of L1(·)). We may therefore incorporate the inequality 1 -  · c2(N-1)/N  exp -

 · c2(N-1)/N into Equation (13):

L1 W1:N (t)  L1 W1:N (0) · exp -  · c2(N-1)/N · t ,

from which it follows that L1(W1:N (t))  if:

t

1  · c2(N-1)/N

· log

L1(W1:N (0))

.

Recalling again that (t) = L1(W1:N (t)) (Equation (5)), we conclude the proof.

3.3 BALANCED INITIALIZATION

We define the following procedure, balanced initialization, which assigns weights randomly while ensuring perfect balancedness:
Procedure 1 (Balanced initialization). Given d0, d1, . . . , dN  N such that min{d1, . . . , dN-1}  min{d0, dN } and a distribution D over dN × d0 matrices, a balanced initialization of Wj  Rdj×dj-1 , j=1, . . . , N , assigns these weights as follows:
(i) Sample A  RdN ×d0 according to D.
(ii) Take the singular value decomposition A = U V , where U  RdN ×min{d0,dN }, V  Rd0×min{d0,dN } have orthonormal columns, and   Rmin{d0,dN }×min{d0,dN } is diagonal and holds singular values of A.
(iii) Set WN U 1/N , WN-1 1/N , . . . , W2 1/N , W1 1/N V , where the symbol " " stands for equality up to zero-valued padding.4 5

The concept of balanced initialization, together with Theorem 1, leads to a guarantee for linear convergence (applicable to output dimension 1 -- scalar regression) that holds with constant probability over the randomness in initialization:

Theorem 2. For any constant 0 < p < 1/2, there are constants d0, a > 0 such that the following holds. Assume dN = 1, d0  d0, and that the weights W1(0), . . . , WN (0) are subject to balanced

initialization (Procedure 1), such that the entries in W1:N (0) are independent zero-centered Gaus-

sian perturbations with standard deviation s   2/ ad02. Suppose also that we run gradient

descent with learning rate   (s2d0)4-2/N

105N 3



10-6/N 2

. Then, with probability at least p

over the random initialization, we have that for every > 0 and:

T4 

ln(4)

2 s2d0

2-2/N

+



2/N -2 2

ln(



22/(8

))

,

the loss at iteration T of gradient descent -- (T ) -- is no greater than .

Proof. See Appendix D.3.

4 RELATED WORK
Theoretical study of gradient-based optimization in deep learning is a highly active area of research. As discussed in Section 1, a popular approach is to show that the objective landscape admits the properties of no poor local minima and strict saddle, which, by Ge et al. (2015); Lee et al. (2016); Panageas and Piliouras (2017), ensure convergence to global minimum. Many works, both classic (e.g. Baldi and Hornik (1989)) and recent (e.g. Choromanska et al. (2015); Kawaguchi (2016); Hardt and Ma (2016); Soudry and Carmon (2016); Haeffele and Vidal (2017); Nguyen and Hein (2017);
4These assignments can be accomplished since min{d1, . . . , dN-1}  min{d0, dN }. 5It follows from the definition that W1:N = A and Wj+1Wj+1 = WjWj , j  {1, . . . , N -1} -- these properties are actually all we need in Theorem 2, and step (iii) in Procedure 1 is just one way to ensure them.

7

Under review as a conference paper at ICLR 2019
Safran and Shamir (2018); Nguyen and Hein (2018); Laurent and Brecht (2018)), have focused on the validity of these properties in different deep learning settings. Nonetheless, to our knowledge, the success of landscape-driven analyses in formally proving convergence to global minimum for a gradient-based algorithm, has thus far been limited to shallow models (two layers) only (e.g. Ge et al. (2016); Du and Lee (2018); Du et al. (2018a)).
An alternative to the landscape approach is a direct analysis of the trajectories taken by the optimizer. Various papers (e.g. Brutzkus and Globerson (2017); Li and Yuan (2017); Zhong et al. (2017); Tian (2017); Brutzkus et al. (2018); Li et al. (2018); Du et al. (2018c;b); Liao et al. (2018)) have recently adopted this strategy, but their analyses only apply to shallow models. In the context of linear neural networks, deep models (three or more layers) have also been treated -- cf. Saxe et al. (2014) and Arora et al. (2018), from which we draw certain technical ideas for proving Lemma 1. However these treatments all apply to gradient flow (gradient descent with infinitesimally small learning rate), and thus do not formally address the question of computational efficiency.
To our knowledge, Bartlett et al. (2018) is the only existing work rigorously proving convergence to global minimum for a conventional gradient-based algorithm training a deep model. This work is similar to ours in the sense that it also treats linear neural networks trained via minimization of 2 loss over whitened data, and proves linear convergence (to global minimum) for gradient descent. It is more limited in that it only covers the subclass of linear residual networks, i.e. the specific setting of uniform width across all layers (d0 = · · · = dN ) along with identity initialization. We on the other hand allow the input, output and hidden dimensions to take on any configuration that avoids "bottlenecks" (i.e. admits min{d1, . . . , dN-1}  min{d0, dN }), and from initialization require only approximate balancedness (Definition 1), supporting many options beyond identity. In terms of the target matrix , Bartlett et al. (2018) treats two separate scenarios:6 (i)  is symmetric and positive definite; and (ii)  is within distance 1/10e from identity.7 Our analysis does not fully account for scenario (i), which seems to be somewhat of a singularity, where all layers are equal to each other throughout optimization (see proof of Theorem 2 in Bartlett et al. (2018)). We do however provide a strict generalization of scenario (ii) -- our assumption of deficiency margin (Definition 2), in the setting of linear residual networks, is met if the distance between target and identity is less than 0.5.
5 CONCLUSION
For deep linear neural networks, we have rigorously proven convergence of gradient descent to global minima, at a linear rate, provided that the initial weight matrices are approximately balanced and the initial end-to-end matrix has positive deficiency margin. The result applies to networks with arbitrary depth, and any configuration of input/output/hidden dimensions that does not include bottlenecks, i.e. in which no hidden layer has dimension smaller than both the input and output.
Our assumptions on initialization -- approximate balancedness and deficiency margin -- are both necessary, in the sense that violating any one of them may lead to convergence failure, as we demonstrated explicitly. Moreover, for networks with output dimension 1 (scalar regression), we have shown that a balanced initialization, i.e. a random choice of the end-to-end matrix followed by a balanced partition across all layers, leads assumptions to be met, and thus convergence to take place, with constant probability. Rigorously proving efficient convergence with significant probability under common layer-wise independent initialization remains an open problem. The recent work of Shamir (2018) suggests that this may not be possible, as at least in some settings, the number of iterations required for convergence is exponential in depth with overwhelming probability. This negative result, a theoretical manifestation of the "vanishing gradient problem", is circumvented by balanced initialization. We hypothesize that the latter (or similar schemes) may possess desirable properties in deep learning practice, as it does in theory.
The analysis in this paper uncovers special properties of the optimization landscape in the vicinity of gradient descent trajectories. We expect similar ideas to prove useful in further study of gradient descent on non-convex objectives, including training losses of deep non-linear neural networks.
6 There is actually an additional third scenario being treated --  is asymmetric and positive definite -- but since that requires a dedicated optimization algorithm, it is outside our scope.
7 1/10e is the optimal (largest) distance that may be obtained (via careful choice of constants) from the proof of Theorem 1 in Bartlett et al. (2018).
8

Under review as a conference paper at ICLR 2019
REFERENCES
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In International Conference on Machine Learning, pages 244­253, 2018.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53­58, 1989.
Peter Bartlett, Dave Helmbold, and Phil Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations. In International Conference on Machine Learning, pages 520­529, 2018.
Rajendra Bhatia. Matrix analysis. Springer-Verlag, 1997.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In International Conference on Machine Learning, pages 605­614, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-parameterized networks that provably generalize on linearly separable data. International Conference on Learning Representations, 2018.
A Carbery and J Wright. Distributional and lq norm inequalities for polynomials over convex bodies in Rn. Mathematics Research Letters, 8(3):233­248, 2001.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Ge´rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192­204, 2015.
Alexander Chudnov. On minimax signal generation and reception algorithms. Problems of Information Transmission, 22(4):49­54, 1986.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. In International Conference on Machine Learning, pages 1329­1338, 2018.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018a.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? International Conference on Learning Representations, 2018b.
Simon S Du, Jason D Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns onehidden-layer cnn: Dont be afraid of spurious local minima. In International Conference on Machine Learning, pages 1339­1348, 2018c.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797­842, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pages 2973­2981, 2016.
Benjamin Haeffele and Rene´ Vidal. Global optimality in neural network training. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4390­4398, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. International Conference on Learning Representations, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­778, 2016.
Roger Horn and Charles Johnson. Matrix analysis. Cambridge university press, 1990.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pages 586­594, 2016.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):1302­1338, 2000.
Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are global. In International Conference on Machine Learning, pages 2908­2913, 2018.
9

Under review as a conference paper at ICLR 2019
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pages 1246­1257, 2016.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pages 597­607, 2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Proceedings of the 31st Conference On Learning Theory, pages 2­47, 2018.
Zhenyu Liao, Yacine Chitour, and Romain Couillet. Almost global convergence to global minima for gradient descent in deep linear networks. 2018.
Shachar Lovett. An elementary proof of anti-concentration of polynomials in Gaussian variables. Electronic Colloquium on Computational Complexity, page 182, 2010.
Raghu Meka, Oanh Nguyen, and Van Vu. Anti-concentration for Polynomials of Independent Random Variables. Theory of Computing, 12:17, 2016.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In International Conference on Machine Learning, pages 2603­2612, 2017.
Quynh Nguyen and Matthias Hein. The loss surface and expressivity of deep convolutional neural networks. arXiv preprint arXiv:1710.10928, 2018.
Ioannis Panageas and Georgios Piliouras. Gradient descent only converges to minimizers: Non-isolated critical points and invariant regions. In Innovations in Theoretical Computer Science, 2017.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In International Conference on Machine Learning, 2018.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. International Conference on Learning Representations, 2014.
Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear neural networks. arXiv preprint arXiv:1809.08587, 2018.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International Conference on Machine Learning, pages 1139­1147, 2013.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for onehidden-layer neural networks. In International Conference on Machine Learning, pages 4140­4149, 2017.
10

Under review as a conference paper at ICLR 2019

A 2 LOSS OVER WHITENED DATA

Recall the 2 loss of a linear predictor W  Rdy×dx as defined in Section 2:

1 L(W ) =
2m

WX -Y

2 F

,

where X

 Rdx×m and Y

 Rdy×m.

Define xx

:=

1 m

X

X



Rdx×dx , yy

:=

1 m

Y

Y

Rdy ×dy

and

yx

:=

1 m

Y

X

 Rdy×dx . Using the relation

A

2 F

= Tr(AA

), we have:

L(W )

=

1 2m

Tr

(W X - Y )(W X - Y )



=

1 2m

Tr(W

XX

W

)

-

1 m

Tr(W XY

)

+

1 2m

Tr(Y

Y

)

=

1 2

Tr(W

xxW

)

-

Tr(W yx)

+

1 2

Tr(yy) .

By definition, when data is whitened, xx is equal to identity, yielding:

L(W )

=

1 2

Tr(W

W

)

-

Tr(W

yx)

+

1 2

Tr(yy )

=

1 2

Tr

(W - yx)(W - yx)

-

1 2

Tr(yxyx)

+

1 2

Tr(yy )

=

1 2

W

- yx

2 F

+c,

where

c

:=

-

1 2

Tr(yxyx)

+

1 2

Tr(yy )

does

not

depend

on

W

.

Hence

we

arrive

at

Equation

(1).

B APPROXIMATE BALANCEDNESS AND DEFICIENCY MARGIN AT INITIALIZATION

Two assumptions concerning initialization facilitate our main convergence result (Theorem 1): (i) the initial weights W1(0), . . . , WN (0) are approximately balanced (see Definition 1); and (ii) the initial end-to-end matrix W1:N (0) has positive deficiency margin with respect to the target  (see Definition 2). The current appendix studies the likelihood of these assumptions being met under customary initialization of random Gaussian perturbations centered at zero.
For approximate balancedness we have the following claim, which shows that it becomes more and more likely the smaller the standard deviation of initialization is:
Claim 2. Assume all entries in the matrices Wj  Rdj×dj-1 , j = 1, . . . , N , are drawn independently at random from a Gaussian distribution with mean zero and standard deviation s > 0. Then, for any  > 0, the probability of W1, . . . , WN being -balanced is at least max{0, 1 - 10-2N s4d3max}, where dmax := max{d0, . . . , dN }.
Proof. See Appendix D.4.

In terms of deficiency margin, the claim below treats the case of a single output model (scalar regression), and shows that if the standard deviation of initialization is sufficiently small, with probability close to 0.5, a deficiency margin will be met. However, for this deficiency margin to meet a chosen threshold c, the standard deviation need be sufficiently large.

Claim 3. There is a constant C1 > 0 such that the following holds. Consider the case where dN = 1, d0  20, and suppose all entries in the matrices Wj  Rdj×dj-1 , j = 1, . . . , N , are drawn inde-

pendently at random from a Gaussian distribution with mean zero, whose standard deviation s > 0

is small with respect to the target, i.e. s 



1/N F

(105d03d1 · · · dN-1C1)1/(2N). Then, for any c

with 0 < c   F 105d03C1(C1N )2N , the probability of the end-to-end matrix W1:N having

deficiency margin c with respect to  is at least 0.49 if: 8 9

s  c1/(2N) ·

C1N



1/(2N ) F

/(d1

·

·

·

dN -1 )1/(2N )

.

Proof. See Appendix D.5.
8 The probability 0.49 can be increased to any p < 1/2 by increasing the constant 105 in the upper bounds for s and c.
9 It is not difficult to see that the latter threshold is never greater than the upper bound for s, thus sought-after standard deviations always exist.

11

Under review as a conference paper at ICLR 2019

C CONVERGENCE FAILURES
In this appendix we show that the assumptions on initialization facilitating our main convergence result (Theorem 1) -- approximate balancedness and deficiency margin -- are both necessary, by demonstrating cases where violating each of them leads to convergence failure. This accords with widely observed empirical phenomena, by which successful optimization in deep learning crucially depends on careful initialization (cf. Sutskever et al. (2013)).
Claim 4 below shows10 that if one omits from Theorem 1 the assumption of approximate balancedness at initialization, no choice of learning rate can guarantee convergence:
Claim 4. Assume gradient descent with some learning rate  > 0 is a applied to a network whose depth N is even, and whose input, output and hidden dimensions d0, . . . , dN are all equal to some d  N. Then, there exist target matrices  such that the following holds. For any c with 0 < c < min(), there are initializations for which the end-to-end matrix W1:N (0) has deficiency margin c with respect to , and yet convergence will fail -- objective will never go beneath a positive constant.
Proof. See Appendix D.6.
In terms of deficiency margin, we provide (by adapting Theorem 4 in Bartlett et al. (2018)) a different, somewhat stronger result -- there exist settings where initialization violates the assumption of deficiency margin, and despite being perfectly balanced, leads to convergence failure, for any choice of learning rate:11
Claim 5. Consider a network whose depth N is even, and whose input, output and hidden dimensions d0, . . . , dN are all equal to some d  N. Then, there exist target matrices  for which there are non-stationary initializations W1(0), . . . , WN (0) that are 0-balanced, and yet lead gradient descent, under any learning rate, to fail -- objective will never go beneath a positive constant.
Proof. See Appendix D.7.

D DEFERRED PROOFS

We introduce some additional notation here in addition to the notation specified in Section 2. We

use A  to denote the spectral norm (largest singular value) of a matrix A, and sometimes v 2 as an alternative to v -- the Euclidean norm of a vector v. Recall that for a matrix A, vec(A) is its

vectorization in column-first order. We let F (·) denote the cumulative distribution function of the

standard normal distribution, i.e. F (x) =

x -

1

e-

1 2

u2

du

2

(x



R).

To simplify the presentation we will oftentimes use W as an alternative (shortened) notation

for W1:N -- the end-to-end matrix of a linear neural network. We will also use L(·) as shorthand for L1(·) -- the loss associated with a (directly parameterized) linear model, i.e. L(W ) :=

1 2

W -

2 F

.

Therefore, in the context of gradient descent training a linear neural network, the

following expressions all represent the loss at iteration t:

(t)

=

LN (W1(t), . . . , WN (t))

=

L1(W1:N (t))

=

L1(W (t))

=

L(W (t))

=

1 2

W (t) - 

2 F

.

Also, for weights Wj  Rdj×dj-1 , j = 1, . . . , N of a linear neural network, we generalize the notation W1:N , and define Wj:j := Wj Wj -1 · · · Wj for every 1  j  j  N . Note that
Wj:j = Wj Wj+1 · · · Wj . Then, by a simple gradient calculation, the gradient descent updates (4) can be written as

Wj (t

+

1)

=

Wj (t)

-

Wj+1:N (t)

·

dL dW

(W (t))

·

W1:j-1(t)

,1  j  N ,

(14)

where we define W1:0(t) := Id0 and WN+1:N (t) := IdN for completeness.

10 For simplicity of presentation, the claim treats the case of even depth and uniform dimension across all
layers. It can easily be extended to account for arbitrary depth and input/output/hidden dimensions. 11 This statement becomes trivial if one allows initialization at a suboptimal stationary point, e.g. Wj(0) =
0, j = 1, . . . , N . Claim 5 rules out such trivialities by considering only non-stationary initializations.

12

Under review as a conference paper at ICLR 2019

Finally, recall the standard definition of the tensor product of two matrices (also known as the Kronecker product): for matrices A  RmA×nA , B  RmB×nB , their tensor product A  B  RmAmB×nAnB is defined as

 a1,1B · · · a1,nA B 

AB = 

...

...

...

, 

amA,1B · · · amA,nA B

where ai,j is the element in the i-th row and j-th column of A.

D.1 PROOF OF CLAIM 1
Proof. Recall that for any matrices A and B of compatible sizes min(A + B)  min(A) - max(B), and that the Frobenius norm of a matrix is always lower bounded by its largest singular value (Horn and Johnson (1990)). Using these facts, we have:
min(W ) = min  + (W - )  min() - max(W - )  min() - W -  F  min() - W -  F  min() - (min() - c) = c .

D.2 PROOF OF LEMMA 1

To prove Lemma 1, we will in fact prove a stronger result, Lemma 2 below, which states that for each iteration t, in addition to (9) being satisfied, certain other properties are also satisfied, namely: (i) the weight matrices W1(t), . . . , WN (t) are 2-balanced, and (ii) W1(t), . . . , WN (t) have bounded spectral norms.
Lemma 2. Suppose the conditions of Theorem 1 are satisfied. Then for all t  N  {0},

(A(t)) For 1  j  N - 1, Wj+1(t)Wj+1(t) - Wj(t)Wj (t) F  2.

(A (t)) If t  1, then for 1  j  N - 1,

Wj+1(t)Wj+1(t) - Wj (t)Wj (t) F

 Wj+1(t - 1)Wj+1(t - 1) - Wj(t - 1)Wj (t - 1) F

+2

dL1 W (t - 1) · dW F

dL1 W (t - 1) dW

· 4 · (2




F )2(N-1)/N .

(B(t))

If t = 0, then

(t)



1 2



2 F

.

If

t



1,

then

 2(N-1) (t)  (t - 1) - 2 min(W (t - 1)) N

dL1 (W (t - 1)) 2 . dW F

(C(t)) For 1  j  N , Wj(t)   (4  F )1/N .

First we observe that Lemma 1 is an immediate consequence of Lemma 2.

Proof of Lemma 1. Notice that condition B(t) of Lemma 2 for each t  1 immediately establishes the conclusion of Lemma 1 at time step t - 1.

D.2.1 PRELIMINARY LEMMAS
We next prove some preliminary lemmas which will aid us in the proof of Lemma 2. The first is a matrix inequality that follows from Lidskii's theorem. For a matrix A, let Sing(A) denote the rectangular diagonal matrix of the same size, whose diagonal elements are the singular values of A arranged in non-increasing order (starting from the (1, 1) position).

13

Under review as a conference paper at ICLR 2019

Lemma 3 (Bhatia (1997), Exercise IV.3.5). For any two matrices A, B of the same size, Sing(A)- Sing(B)   A - B  and Sing(A) - Sing(B) F  A - B F .
Using Lemma 3, we get: Lemma 4. Suppose D1, D2  Rd×d are non-negative diagonal matrices with non-increasing values along the diagonal and O  Rd×d is an orthogonal matrix. Suppose that D1 - OD2O F  , for some > 0. Then:
1. D1 - OD1O F  2 .
2. D1 - D2 F  .
Proof. Since D1 and OD2OT are both symmetric positive semi-definite matrices, their singular values are equal to their eigenvalues. Moreover, the singular values of D1 are simply its diagonal elements and the singular values of OD2OT are simply the diagonal elements of D2. Thus by Lemma 3 we get that D1 - D2 F  D1 - OD2OT F  . Since the Frobenius norm is unitarily invariant, D1 - D2 F = OD1OT - OD2OT F , and by the triangle inequality it follows that
D1 - OD1OT F  OD1OT - OD2OT F + D1 - OD2OT F  2 .

Lemma 5 below states that if W1, . . . , WN are approximately balanced matrices, i.e. Wj+1Wj+1 - WjWj has small Frobenius norm for 1  j  N - 1, then we can bound the Frobenius distance between W1:j W1:j and (W1 W1)j (as well as between Wj:N Wj:N and (WN WN )N-j+1).
Lemma 5. Suppose that dN  dN-1, d0  d1, and that for some  > 0, M > 0, the matrices Wj  Rdj×dj-1 , 1  j  N satisfy, for 1  j  N - 1,

Wj+1Wj+1 - Wj Wj F  ,

(15)

and for 1  j  N , Wj   M . Then, for 1  j  N ,

W1:j W1:j - (W1 W1)j

F



3 

· M 2(j-1)j2,

2

(16)

and

Wj:N Wj:N - (WN WN )N-j+1

F



3 

·

M 2(N-j)(N

-

j

+

1)2.

2

(17)

Moreover, if min denotes the minimum singular value of W1:N , 1,min denotes the minimum singular value of W1 and N,min denotes the minimum singular value of WN , then

m2 in

-

3 M 2(N-1)N 2 2



N2N,min 12,Nmin

: :

dN  d0. dN  d0.

(18)

Proof. For 1  j  N , let us write the singular value decomposition of Wj as Wj = UjjVj , where Uj  Rdj×dj and Vj  Rdj-1×dj-1 are orthogonal matrices and j  Rdj×dj-1 is diagonal. We may assume without loss of generality that the singular values of Wj are non-increasing along the diagonal of j. Then we can write (15) as

Vj+1j+1j+1Vj+1 - Uj j j Uj F  . Since the Frobenius norm is invariant to orthogonal transformations, we get that

j+1j+1 - Vj+1Uj j j Uj Vj+1 F  .

By Lemma 4, we have that j+1j+1 - j j F   and j j - Vj+1Uj j j Uj Vj+1 F  2. We may rewrite the latter of these two inequalities as

[j j , Vj+1Uj ] F = [j j , Vj+1Uj ]Uj Vj+1 F = j j - Vj+1Uj j j Uj Vj+1 F  2.

Note that

Wj:N Wj:N = Wj+1:N Uj j j Uj Wj+1:N .

14

Under review as a conference paper at ICLR 2019

For matrices A, B, we have that AB F  A  · B F . Therefore, for j + 1  i  N , we have that

Wi:N Ui-1(i-1i-1)i-j Ui-1Wi:N - Wi+1:N Ui(ii )i-j+1Ui Wi+1:N F

= Wi+1:N Ui iVi Ui-1(i-1i-1)i-j Ui-1Vii - (ii )i-j+1 Ui Wi+1:N F



Wi+1:N Uii

2 

·

(i-1i-1)i-j + [Vi

Ui-1, (i-1i-1)i-j ]Ui-1Vi - (i i)i-j

F



Wi:N

2 

[Vi Ui-1, (i-1i-1)i-j ] F + (i-1i-1)i-j - (i i)i-j F .

Next, we have that

[Vi Ui-1, (i-1i-1)i-j ] F

i-j-1
 (i-1i-1)k[Vi Ui-1, i-1i-1](i-1i-1)i-j-1-k F
k=0

i-j-1

 (i-1i-1)i-j-1  · [Vi Ui-1, i-1i-1] F

k=0



(i - j)

Wi-1

2(i-j-1) 

·

2.

We now argue that (i-1i-1)k - (i i)k F   · kM 2(k-1). Note that i-1i-1 -
i i F  , verifying the case k = 1. To see the general case, since square diagonal matrices commute, we have that

(i-1i-1)k - (i i)k F

k-1

= (i-1i-1 - i i) ·

(i-1i-1) (i i)k-1-

=0

k-1

 ·

Wi-1

2 

·

Wi

2(k- -1) 

=0

 kM 2(k-1).

F

It then follows that

Wi:N Ui-1(i-1i-1)i-j Ui-1Wi:N - Wi+1:N Ui(ii )i-j+1Ui Wi+1:N F



Wi:N

2 

·

(i - j)M 2(i-j-1) · 2 + (i - j)M 2(i-j-1)

=

Wi:N

2 

·

3(i

-

j)M 2(i-j-1).

By the triangle inequality, we then have that

Wj:N Wj:N - UN (N N )N-j+1UN F

N



Wi:N

2 

·

3(i

-

j)M 2(i-j-1)

i=j+1

N

 3

(i - j)M 2(N-i+1)M 2(i-j-1)

i=j+1

=

3M 2(N-j)

N

(i

-

j)



3 

·

M 2(N -j)

·

(N

-

j

+

1)2.

2

i=j+1

(19)

By an identical argument (formally, by replacing Wj with WN-j+1), we get that

||W1:j W1:j - V1(1 1)j V1

F



3 

·

M 2(j-1)

·

j2.

2

(19) and (20) verify (17) and (16), respectively, so it only remains to verify (18).

(20)

Letting j = 1 in (19), we get

W1:N W1:N - UN (N N )N UN

F



3 
2

· M 2(N-1)

· N2.

(21)

15

Under review as a conference paper at ICLR 2019

Let us write the eigendecomposition of W1:N W1:N with an orthogonal eigenbasis as W1:N W1:N = U U , where  is diagonal with its (non-negative) elements arranged in non-increasing order and U is orthogonal. We can write the left hand side of (21) as U U - UN (N N )N UN F =  - U UN (N N )N UN U F .
By Lemma 4, we have that

 - (N N )N

F



3 M 2(N-1)N 2. 2

(22)

Recall that W  RdN ×d0 . Suppose first that dN  d0. Let min denote the minimum singular value of W1:N (so that m2 in is the element in the (dN , dN ) position of   RdN ×dN ), and N,min denote

the minimum singular value (i.e. diagonal element) of N , which lies in the (dN , dN ) position of N . (Note that the (dN , dN ) position of N  RdN ×dN-1 exists since dN-1  dN by assumption.)

Then

(N2N,min - m2 in)2 

3 M 2(N-1)N 2

2
,

2

so

N2N,min



m2 in

-

3 M 2(N-1)N 2. 2

By an identical argument using (20), we get that, in the case that d0  dN , if 1,min denotes the minimum singular value of 1, then

12,Nmin



m2 in

-

3 M 2(N-1)N 2. 2

(Notice that we have used the fact that the nonzero eigenvalues of W1:N W1:N are the same as the nonzero eigenvalues of W1:N W1:N .) This completes the proof of (18).

Using Lemma 5, we next show in Lemma 6 that if W1, . . . , WN are approximately balanced, then an upper bound on WN · · · W1  implies an upper bound on Wj  for 1  j  N .

Lemma 6.

Suppose , C

are real numbers satisfying C

> 0 and 0 < 



.C 2/N
30N 2

Moreover suppose

that the matrices W1, . . . , WN satisfy the following:

1. For 1  j  N - 1, Wj+1Wj+1 - WjWj F  . 2. WN · · · W1   C.

Then for 1  j  N , Wj   C1/N · 21/(2N).

Proof. For 1  j  N , let us write the singular value decomposition of Wj as Wj = UjjVj , where the singular values of Wj are decreasing along the main diagonal of j. By Lemma
4, we have that for 1  j  N - 1, j+1j+1 - jj F  , which implies that
j+1j+1  - j j   .

Write M = max1jN Wj  = max1jN j . By the above we have that jj   M 2 - N  for 1  j  N .

Let the singular value decomposition of W1:N be denoted by W1:N = U V , so that    C.

Then by (17) of Lemma 5 and Lemma 4 (see also (22), where the same argument was used), we

have that



- (N N )N

F



3 M 2(N-1)N 2. 2

Then

(N N )N   



+

3 M (2(N-1))N 2 2





3



+

 2

N N  + N N-1 N 2. (23)

16

Under review as a conference paper at ICLR 2019

Now recall that



is chosen so that 



C 2/N 30·N 2

.

Suppose

for

the

purpose

of

contradiction

that

there

is some j such that WjWj  > 21/N C2/N . Then it must be the case that

N N  > 21/N C2/N -  · N  (5/4)1/N C2/N >  · 30N 2,

(24)

where we have used that

21/N - (5/4)1/N  1 30N

for all N  2, which follows by considering the Laurent series exp(1/z) = converges in |z| > 0 for z  C.

 i=1

1 i!zi

,

which

We now rewrite inequality (24) as



N N 30N 2

.

Next, using (25) and (1 + 1/x)x  e for all x > 0,

(25)

3 
2

N N

 + N

N-1 N 2  e1/30 · 20

N N

N 

<

e 20

·

N N

N 

.

(26)

Since

(N N )N

=

N N

N 

,

we

get

by

combining

(23)

and

(26)

that

N N  < (1 - e/20)-1/N · 

1/N 



(1 - e/20)-1/N

· C2/N ,

and since 1 - e/20 > 1/(5/4), it follows that N N  < (5/4)1/N C2/N , which contradicts (24).

It follows that for all 1  j  N , WjWj   21/N C2/N . The conclusion of the lemma then

follows from the fact that WjWj

=

Wj

2 

.

D.2.2 SINGLE-STEP DESCENT
Lemma 7 below states that if certain conditions on W1(t), . . . , WN (t) are met, the sought-after descent -- Equation (9) -- will take place at iteration t. We will later show (by induction) that the required conditions indeed hold for every t, thus the descent persists throughout optimization. The proof of Lemma 7 is essentially a discrete, single-step analogue of the continuous proof for Lemma 1 (covering the case of gradient flow) given in Section 3. Lemma 7. Assume the conditions of Theorem 1. Moreover, suppose that for some t, the matrices W1(t), . . . , WN (t) and the end-to-end matrix W (t) := W1:N (t) satisfy the following properties:
1. Wj(t)   (4  F )1/N for 1  j  N .
2. W (t) -     F .
3. Wj+1(t)Wj+1(t) - Wj(t)Wj (t) F  2 for 1  j  N - 1.
4. min := min(W (t))  c.

Then, after applying a gradient descent update (4) we have that

L(W

(t

+

1))

-

L(W

(t))



-

 2

m2(iNn-1)/N

dL 2 (W (t)) .
dW F

Proof. For simplicity write M = (4  F )1/N and B =  F . We first claim that

  min

1 2M N-2BN

,

24

m2(iNn-1)/N · 2M 3N-4N 2B

,

m2(iNn-1)/N 24N 2M 4(N-1)

,

(24

·

m2(iNn-1)/(3N ) 4M 6N-8N 4B2)1/3

.

(27)

Since c  min, for (27) to hold it suffices to have







 

1

c2(N -1)/N

min 8 



(2N -2)/N F

N

,

3

·

211



4(N -1)/N F

N2

,

3

·

26



c2(N -1)/(3N )

 

.



(8N -8)/N F

1/3

N

4/3

 

17

Under review as a conference paper at ICLR 2019

As the minimum singular value of  must be at least c, we must have c   . Since then

c F



c 

 1, it holds that

c2(N -1)/N



4(N -1)/N F

 min

meaning that it suffices to have

1 c2(N -1)/(3N )



2(N -1)/N ,
F



(8N -8)/(3N ) F

,

which is guaranteed by (7). Next, we claim that

c2(N -1)/N



3 · 211N 2



4(N -1)/N ,
F

2



min

c2(N -1)/N

c2

8 · 24N 3



2(N -2)/N F

,

6

·

24N 2



2(N -1)/N F



min

m2(iNn-1)/N 8N 3M 2(N-2)

,

m2 in 6N 2M 2(N-1)

.

(28)

The second inequality above is trivial, and for the first to hold, since c   F , it suffices to take

2 

128 · N 3 ·

c2 

2(N -1)/N ,
F

which is guaranteed by the definition of  in Theorem 1. Next we continue with the rest of the proof. It follows from (14) that12

W (t + 1) - W (t)

j=N

dL

= Wj (t) - Wj+1:N (t) dW (W (t))W1:j-1(t) - W1:N (t)

1



=

N
- 

dL Wj+1:N Wj+1:N (t) dW (W (t))W1:j-1(t)W1:j-1(t) + (

),

j=1

(29)

where ( ) denotes higher order terms in . We now bound the Frobenius norm of ( ). To do this,

note

that

since

L(W )

=

1 2

W -

2 F

,

dL dW

(W (t))

=

W (t)

-

.

Then

()F



N
k · M k(N-1)+N-k ·

dL (W (t))

·

dL (W (t))

k-1
·

N

k=2 dW F dW  k



M 2N-2N

dL (W (t))

N
M N-2BN k-1

dW F k=2



 · (2M 3N-4N 2B) ·

dL (W (t))

,

dW F

(30)

12Here, for matrices A1, . . . , AK such that AK AK-1 · · · A1 is defined, we write

j=K 1

Aj

:=

AK AK-1 · · · A1.

18

Under review as a conference paper at ICLR 2019

where the last inequality uses M N-2BN  1/2, which is a consequence of (27). Next, by Lemma 5 with  = 2,

N dL Wj+1:N Wj+1:N (t) dW (W (t))W1:j-1(t)W1:j-1(t)
j=1

-

N

(WN

WN

)N -j

dL dW

(W

(t))(W1

W1 )j -1

j=1

F



N

(Wj+1:N

Wj+1:N

(t)

-

(WN

WN

)N -j

)

dL dW

(W

(t))W1:j-1(t)W1:j-1(t)

j=1

F

+

N

(WN

WN

)N -j

dL dW

(W

(t))(W1:j-1W1:j-1

-

(W1

W1)j-1)

j=1

F



dL (W (t))

·

N -1

3 2

·

M 2(N-j)(N

-

j)2M 2(j-1)

+

N

3 2 · M 2(j-2)(j - 1)2M 2(N-j)

dW

F

2
j=1

2
j=2



dL (W (t))

· 2N 3M 2(N-2).

dW F

Next, by standard properties of tensor product, we have that



vec 

N

(WN

WN

)N

-j

dL dW

(W (t))(W1

W1)j-1

j=1

=

N

(W1 W1)j-1  (WN WN )N-j vec

dL (W (t))
dW

.

j=1

Let us write eigenvalue decompositions W1 W1 = U DU , WN WN = V EV

N
(W1 W1)j-1  (WN WN )N-j
j=1

N
= U Dj-1U  V EN-j V

j=1


N



= (U  V )  Dj-1  EN-j (U  V )

j=1

. Then

= OO ,

with O = U  V , and  =

N j=1

Dj-1

 EN-j .

As

W1



Rd1×d0 ,

and

WN



RdN ×dN-1 ,

then

D  Rd0×d0 , E  RdN ×dN , so   Rd0dN ×d0dN . Moreover note that  D0  EN-1 + DN-1 

E0 = Id0  EN-1 + DN-1  IdN . If D denotes the minimum diagonal element of D and E

denotes the minimum diagonal element of E, then the minimum diagonal element of  is therefore

at least DN-1 + EN-1. But, it follows from Lemma 5 (with  = 2) that

max{ND , EN }



m2 in

-

3 2M 2(N-1)N 2 2



3m2 in/4,

where the second inequality follows from (28). Hence the minimum diagonal element of  is at

least (m2 in/(4/3))(N-1)/N  m2(iNn-1)/N /(4/3).

It follows as a result of the above inequalities that if we write E(t) = vec(W (t + 1)) - vec(W (t)) +

(OO

)vec

dL dW

(W

(t))

, then

dL

E(t) 2 =

vec(W (t + 1)) - vec(W (t)) + (OO )vec

(W (t))

dW





dL (W (t))

· (2M 3N-4N 2B + 2N 3M 2(N-2)).

dW F

2

19

Under review as a conference paper at ICLR 2019

Then we have

L(W (t + 1)) - L(W (t))

 vec

d L(W (t))

dW

vec (W (t + 1) - W (t)) + 1 2

W (t + 1) - W (t)

2 F

=  -vec

d L(W (t))

(OO )vec

d1 L(W (t)) + vec

d L(W (t))

E(t)

dW dW  dW

+1 2

W (t + 1) - W (t)

2 F

 -

d L(W (t)) 2 · m2(iNn-1)/N +

d

2
L(W (t)) ·

2M 3N-4N 2B + 2N 3M 2(N-2)

dW

F 4/3

dW

F

+1 2

W (t + 1) - W (t)

2 F

,

where the first inequality follows since L(W )

=

1 2

W -

2 F

is 1-smooth as a function of W .

Next, by (29) and (30),

 = Thus

W (t + 1) - W (t)

2 F

22 ·

N M 2(N-1) ·

dL (W (t))

2
+ 22 · (2M 3N-4N 2B)2 ·

dW F

22

dL 2 (W (t)) ·

N 2M 4(N-1) + (42M 6N-8N 4B2)

.

dW F

dL 2 (W (t))
dW F (31)

L(W (t + 1)) - L(W (t))



·

dL 2 (W (t)) ·

- m2(iNn-1)/N + 2M 3N-4N 2B + 2N 3M 2(N-2)

dW F

4/3

+ · (N 2M 4(N-1) + 42M 6N-8N 4B2) .

By (27, 28), which bound , 2, respectively, we have that

L(W (t + 1)) - L(W (t))

  · dL (W (t)) 2 · - m2(iNn-1)/N + m2(iNn-1)/N + m2(iNn-1)/N + m2(iNn-1)/N + m2(iNn-1)/N

dW F

4/3

24

8

24 24

=

-

1 2

m2(iNn-1)/N



dL 2 (W (t)) .
dW F

(32)

D.2.3 PROOF OF LEMMA 2

Proof of Lemma 2. We use induction on t, beginning with the base case t = 0. Since the weights

W1(0), . . . , WN (0) are -balanced, we get that A(0) holds automatically. To establish B(0), note

that since W1:N (0) has deficiency margin c > 0 with respect to , we must have W1:N (0)- F 

min() 



F , meaning that L1(W1:N (0)) 

1 2



2 F

.

Finally, by B(0), which gives W (0) -  F   F , we have that

W (0)   W (0) F  W (0) -  F +  F  2  F .

(33)

To show that the above implies C(0), we use condition A(0) and Lemma 6 with C = 2  F and  = 2. By the definition of  in Theorem 1 and since c   F , we have that

2 

c2

128 · N 3 · 

2(N -1)/N F

=



2/N F

128N 3

·

c2



2 F

<



2/N F

30N 2

,

(34)

20

Under review as a conference paper at ICLR 2019

as required by Lemma 6. As A(0) and (33) verify the preconditions 1. and 2., respectively, of Lemma 6, it follows that for 1  j  N , Wj(t)   (2  F )1/N · 21/(2N) < (4  F )1/N , verifying C(0) and completing the proof of the base case.
The proof of Lemma 2 follows directly from the following inductive claims.

1. A(t), B(t), C(t)  B(t + 1). To prove this, we use Lemma 7. We verify first

that the preconditions hold. First, C(t) immediately gives condition 1. of Lemma

7. By B(t), we have that W (t) -    W (t) -  F   F , giving

condition 2. of Lemma 7. A(t) immediately gives condition 3. of Lemma 7. Fi-

nally, by B(t), we have that LN (W1(t), . . . , WN (t))  LN (W1(0), . . . , WN (0)), so

min(W1:N (t))  c by Claim 1. This verifies condition 4. of Lemma 7. Then

Lemma 7 gives that LN (W1(t + 1), . . . , WN (t + 1))  LN (W1(t), . . . , WN (t)) -

1 2

min

(W

(t))2(N

-1)/N



dL dW

(W

(t))

2 F

,

establishing

B(t

+

1).

2. A(0), A (1), . . . , A (t), A(t), B(0), . . . , B(t), C(t)  A(t + 1), A (t + 1). To prove this, note that for 1  j  N - 1,

Wj+1(t + 1)Wj+1(t + 1) - Wj(t + 1)Wj (t + 1)

=

Wj+1(t)

-

W1:j

(t)

dL dW

(W

(t))

Wj+2:N (t)

·

Wj

+1

(t)

-

Wj+2:N

(t)

dL dW

(W

(t))W1:j

(t)

-

Wj (t)

-

Wj+1:N

(t)

dL dW

(W

(t))W1:j-1(t)

·

Wj

(t)

-

W1:j-1(t)

dL dW

(W

(t))

Wj+1:N (t)

.

By B(0), . . . , B(t), W1:N (t) -  F   F . By the triangle inequality it then follows that W1:N (t)   2  F . Also A(t) gives that for 1  j  N - 1, Wj(t)Wj (t) -
Wj+1(t)Wj+1(t) F  2. By Lemma 6 with C = 2  F ,  = 2 (so that (34) is satisfied),

Wj+1(t + 1)Wj+1(t + 1) - Wj(t + 1)Wj (t + 1) F



Wj+1(t)Wj+1(t) - Wj (t)Wj (t) F + 2

dL (W (t))

·

dW F

dL (W (t))
dW 

·

Wj+2:N (t)

2 

W1:j (t)

2 

+

W1:j-1

2 

Wj+1:N

2 

 Wj+1(t)Wj+1(t) - Wj (t)Wj (t) F

+42

dL (W (t))
dW F

dL (W (t))
dW

(2




F )2(N-1)/N .

(35)

In the first inequality above, we have also used the fact that for matrices A, B such that AB is defined, AB F  A  B F . (35) gives us A (t + 1).

We next establish A(t + 1). By B(i) for 0  i  t, we have that

dL dW

(W

(i))

F

=

W -  F   F . Using A (i) for 0  i  t and summing over i gives

Wj+1(t + 1)Wj+1(t + 1) - Wj(t + 1)Wj (t + 1) F

 Wj+1(0)Wj+1(0) - Wj (0)Wj (0) F

t
+4(2  F )2(N-1)/N · 2
i=0

dL 2 (W (i)) .
dW F

(36)

Next, by B(0), . . . , B(t), we have that L(W (i))  L(W (0)) for i  t. Since W (0) has deficiency margin of c and by Claim 1, it then follows that min(W (i))  c for all i  t.

21

Under review as a conference paper at ICLR 2019

Therefore, by summing B(0), . . . , B(t),

Therefore,

1 c2(N-1)/N  t

dL 2 W (i)

2

dW
i=0

F



1 
2

t

min(W (i))2(N-1)/N

i=0

dL 2 (W (i))
dW F

 L(W (0)) - L(W (t))



L(W (0))  1 2



2 F

.

t
4 (2  F )2(N-1)/N 2
i=0

dL 2 W (i)
dW F



16



2(N F

-1)/N





2 F

c2(N -1)/N



16



2(N -1)/N F

·

1 3 · 211 · N 3

·

c(4N -2)/N



(6N -4)/N F

·



2 F

c2(N -1)/N



c2

256N 3



2(N -1)/N F

= ,

(37)

where (37) follows from the definition of  in (7), and the last equality follows from definition of  in Theorem 1. By (36), it follows that

Wj+1(t + 1)Wj+1(t + 1) - Wj(t + 1)Wj (t + 1) F  2, verifying A(t + 1).

3. A(t), B(t)  C(t). We apply Lemma 6 with  = 2 and C = 2  F . First, the triangle inequality and B(t) give

W1:N (t)     +  - W1:N (t)    F + 2 · L(W1:N (t))  2  F ,
verifying precondition 2. of Lemma 6. A(t) verifies condition 1. of Lemma 6, so for 1  j  N , Wj(t)   (4  F )1/N , giving C(t).

The proof of Lemma 2 then follows by induction on t.

D.3 PROOF OF THEOREM 2

Theorem 2 is proven by combining Lemma 8 below, which implies that the balanced initialization is likely to lead to an end-to-end matrix W1:N (0) with sufficiently large deficiency margin, with Theorem 1, which establishes convergence.
Lemma 8. Let d  N, d  20; b2 > b1  1 be real numbers (possibly depending on d); and   Rd be a vector. Suppose that µ is a rotation-invariant distribution13 over Rd with a well-defined density, such that, for some 0 < < 1,

PV µ

 2  b2d

V

2



 2 b1d

1- .



Then, with probability at least (1 -

)

·

3-4F (2/ 2

b1) , V

will have deficiency margin

with respect to .

 2/(b2d)

13Recall that a distribution on vectors V



d
R

is

rotation-invariant

if

the

distribution

of

V

is the same as

the distribution of OV , for any orthogonal d × d matrix O. If V has a well-defined density, this is equivalent

to the statement that for any r > 0, the distribution of V conditioned on V 2 = r is uniform over the sphere

centered at the origin with radius r.

22

Under review as a conference paper at ICLR 2019

The proof of Lemma 8 is postponed to Appendix D.5, where Lemma 8 will be restated as Lemma 16.
One additional technique is used in the proof of Theorem 2, which leads to an improvement in the guaranteed convergence rate. Because the deficiency margin of W1:N (0) is very small, namely O(  2/d0) (which is necessary for the theorem to maintain constant probability), at the beginning of optimization, (t) will decrease very slowly. However, after a certain amount of time, the deficiency margin of W1:N (t) will increase to a constant, at which point the decrease of (t) will be much faster. To capture this acceleration, we apply Theorem 1 a second time, using the larger deficiency margin at the new "initialization." From a geometric perspective, we note that the matrices W1(0), . . . , WN (0) are very close to 0, and the point at which Wj(0) = 0 for all j is a saddle. Thus, the increase in (t) - (t + 1) over time captures the fact that the iterates (W1(t), . . . , WN (t)) escape a saddle point.

Proof of Theorem 2. Choose some a  2, to be specified later. By assumption, all entries of the
end-to-end matrix at time 0, W1:N (0), are distributed as independent Gaussians of mean 0 and standard deviation s   2/ ad02. We will apply Lemma 8 to the vector W1:N (0)  Rd0 . Since its distribution is obviously rotation-invariant, in remains to show that the distribution of the norm
W1:N (0) 2 is not too spread out. The following lemma -- a direct consequence of the Chernoff bound applied to the 2 distribution with d0 degrees of freedom -- will give us the desired result:

Lemma 9 (Laurent and Massart (2000), Lemma 1). Suppose that d  N and V  Rd is a vector whose entries are i.i.d. Gaussians with mean 0 and standard deviation s. Then, for any k > 0,



P

V

2 2



s2

d + 2k + 2

kd



P

V

2 2



s2

d-2

kd

 exp(-k)  exp(-k).

By Lemma 9 with k = d0/16, we have that

P

s2d0  2

V

2 2



2s2d0

 1 - 2 exp(-d0/16).

We next use Lemma 8, with b1 =



22/(2s2d02), b2

=

2



2 2

/(s2d20

);

note

that since

a



2,

b1  1, as required by the lemma. Lemma 8 then implies that with probability at least

3 - 4F 2/ a/2 (1 - 2 exp(-d0/16)) 2 ,

(38)

W1:N (0) will have deficiency margin s2d0/2  2 with respect to . By the definition of balanced initialization (Procedure 1) W1(0), . . . , WN (0) are 0-balanced. Since 24 · 6144 < 105, our assumption on  gives





24

(s2d0)4-2/N

· 6144N 3



10-6/N 2

,

(39)

so

that

Equation

(7)

holds

with

c

=

.s2 d0
22

The

conditions

of

Theorem

1

thus

hold

with

probability

at least that given in Equation (38). In such a constant probability event, by Theorem 1 (and the fact

that

a

positive

deficiency

margin

implies

L1(W1:N (0))



1 2



22), if we choose

t0  -1

22 s2d0

2-2/N
ln(4),

(40)

then L1(W1:N (t0)) 

1 8



22, meaning that

W1:N (t0) - 

2

1 2



2=



2-

1 2

min

().

Moreover, by condition A(t0) of Lemma 2 and the definition of  in Theorem 1, we have, for

1  j  N - 1,

WjT+1(t0)Wj+1(t0) - Wj (t0)WjT (t0)

F



(2



2s4d20 2)2 · 256N 3



2-2/N 2

=

512N

s4d02 3

4-2/N 2

.

(41)

23

Under review as a conference paper at ICLR 2019

We now apply Theorem 1 again, verifying its conditions again, this time with the initialization
(W1(t0), . . . , WN (t0)). First note that the end-to-end matrix W1:N (t0) has deficiency margin c =  2/2 as shown above. The learning rate , by Equation (39), satisfies Equation (7) with c =  2/2. Finally, since

s4d20

512N 3



4-2/N 2



 2/N (a2d02) · 512N 3



 2/N (1/2)2 256N 3

for d0  2, by Equation (41), the matrices W1(t0), . . . , WN (t0) are -balanced with  =



2/N (1/2)2 256N 3

.

Iteration

t0

thus

satisfies

the

conditions

of

Theorem

1

with

deficiency

margin

 2/2,

meaning that for

T - t0  -1 · 22-2/N ·  2/N-2 ln



2 2

,

8

(42)

we will have (T )  . Therefore, by Equations (40) and (42), to ensure that (T )  , we may take

T  4-1 ln(4)

2 s2d0

2-2/N

+



2/N -2 2

ln(



2 2

/(8

))

.

Recall that this entire analysis holds only with the probability given in Equation (38). As
limd(1 - 2 exp(-d/16)) = 1 and lima(3 - 4F (2 2/a))/2 = 1/2, for any 0 < p < 1/2, there exist a, d0 > 0 such that for d0  d0, the probability given in Equation (38) is at least p. This completes the proof.

In the context of the above proof, we remark that the expressions 1 - 2 exp(-d0/16) and (3 -
4F (2 2/a))/2 converge to their limits of 1 and 1/2, respectively, as d0, a   quite quickly. For instance, to obtain a probability of greater than 0.25 of the initialization conditions being met, we may take d0  100, a  100.

D.4 PROOF OF CLAIM 2

We first consider the probability of -balancedness holding between any two layers:
Lemma 10. Suppose a, b, d  N and A  Ra×d, B  Rd×b are matrices whose entries are distributed as i.i.d. Gaussians with mean 0 and standard deviation s. Then for k  1,

P AT A - BBT F  ks2 2d(a + b)2 + d2(a + b)  1/k2.

(43)

Proof. Note that for 1  i, j  d, let Xij be the random variable (AT A - BBT )ij, so that

Xij = (AT A - BBT )ij =

A iA j -

Bir Bj r .

1 a

1rb

If i = j, then

E[X2] =

E[A2iA2j ] +

E[Bi2rBj2r] = (a + b)s4.

1 a

1rb

We next note that for a normal random variable Y of variance s2 and mean 0, E[Y 4] = 3s4. Then if i = j,

E[X2] = s4 · (3(a + b) + a(a - 1) + b(b - 1)-ab)  s4((a + b)2 + 2(a + b)).

Thus

E[

AT A - BBT

2 F

]



s4(d((a + b)2 + 2(a + b)) + d(d - 1)(a + b))

 s4(2d(a + b)2 + d2(a + b)).

Then (43) follows from Markov's inequality.

Now the proof of Claim 2 follows from a simple union bound:

24

Under review as a conference paper at ICLR 2019

Proof of Claim 2. By (43) of Lemma 10, for each 1  j  N - 1, k  1, P WjT+1Wj+1 - Wj WjT F ks2 10dm3 ax  1/k2.
By the union bound, P 1  j  N - 1, WjT+1Wj+1 - Wj WjT F  ks2 10dm3 ax  1 - N/k2,
and the claim follows with  = ks2 10dm3 ax.

D.5 PROOF OF CLAIM 3
We begin by introducing some notation. Given d  N and r > 0, we let Bd(r) denote the open ball of radius r centered at the origin in Rd. For an open subset U  Rd, let U := U¯ \U be its boundary, where U¯ denotes the closure of U . For the special case of U = Bd(r), we will denote by Sd(r) the boundary of such a ball, i.e. the sphere of radius r centered at the origin in Rd. Let Sd := Sd(1) and Bd := Bd(1). There is a well-defined uniform (Haar) measure on Sd(r) for all d, r, which we denote by d,r; we assume d,r is normalized so that d,r(Sd(r)) = 1. Finally, since in the context of this claim we have dN = 1, we allow ourselves to regard the end-to-end matrix W1:N  R1×d0 as both a matrix and a vector.

To establish Claim 3, we will use the following low-degree anti-concentration result of Carbery and Wright (2001) (see also Lovett (2010); Meka et al. (2016)):
Lemma 11 (Carbery and Wright (2001)). There is an absolute constant C0 such that the following holds. Suppose that h is a multilinear polynomial of K variables X1, . . . , XK and of degree N . Suppose that X1, . . . , XK are i.i.d. Gaussian. Then, for any > 0:
P[|h(X1, . . . , XK )| · Var[h(X1, . . . , XK )]]  C0N 1/N .

The below lemma characterizes the norm of the end-to-end matrix W1:N following zero-centered Gaussian initialization:

Lemma 12. For any constant 0 < C2 < 1, there is an absolute constant C1 > 0 such that the following holds. Let N, d0, . . . , dN-1  N. Set dN = 1. Suppose that for 1  j  N , Wj  Rdj×dj-1 are matrices whose entries are i.i.d. Gaussians of standard deviation s and mean
0. Then

P s2N d1 · · · dN-1

1 C1N

2N



W1:N

2 2



C1d02d1

·

·

·

dN -1 s2N

 C2.

Proof. Let f (W1, . . . , WN ) = W1:N 22, so that f is a polynomial of degree 2N in the entries of W1, . . . , WN . Notice that


d0 d1

dN -1

2

f (W1, . . . , WN ) =  · · ·

(WN )1,iN-1 (WN-1)iN-1,iN-2 · · · (W1)i1,i0  .

i0=1 i1=1 iN-1=1

For 1  i0  d0, set

d1 dN-1

gi0 (W1, . . . , WN ) = · · ·

(WN )1,iN-1 (WN -1)iN-1,iN-2 · · · (W1)i1,i0 ,

i1=1 iN-1=1

so that f =

d0 i0 =1

gi20

.

Since

each gi0

is a

multilinear

polynomial

in W1, . . . , WN ,

we have

that

E[gi0 (W1, . . . , WN )] = 0 for all 1  i0  d0. Also

Var[gi(W1, . . . , WN )] = E[gi(W1, . . . , WN )2]

d1 dN-1

= ···

E (WN )12,iN-1 (WN -1)2iN-1,iN-2 · · · (W1)2i1,i0

i1=1 iN-1=1

= d1d2 · · · dN-1s2N .

25

Under review as a conference paper at ICLR 2019

It then follows by Markov's inequality that for any k  1, P[gi2  ks2N d1 · · · dN-1]  1/k. For any constant B1 (whose exact value will be specified below), it follows that

P[f (W1, . . . , WN )B1d20d1d2 · · · dN-1s2N ]

d0

=P

gi0 (W1, . . . , WN )2B1d02d1d2 · · · dN-2s2N

i0 =1

 d0 · P[g1(W1, . . . , WN )2B1d0d1 · · · dN-1s2N ]

 1/B1.

(44)

Next, by Lemma 11, there is an absolute constant C0 > 0 such that for any > 0, and any 1  i0  d0,
P |gi0 (W1, . . . , WN )|  N s2N d1 · · · dN-1  C0N .

Since f 2  gi20 for each i0, it follows that P[f (W1, . . . , WN )  2N s2N d1 · · · dN-1]  1 - C0N .

(45)

Next, given 0 < C2 < 1, choose = (1 - C2)/(2C0N ), and B1 = 2/(1 - C2). Then by (44) and (45) and a union bound, we have that

P

1 - C2 2C0N

2N

s2N d1 · · · dN-1



f (W1, . . . , WN )



1

2 - C2

s2N

d02

d1

·

·

·

dN

-1

 C2.

The result of the lemma then follows by taking C1 = max

2 1-C2

,

2C0 1-C2

.

Lemma 13. Let N, d0, . . . , dN-1  N, and set dN = 1. Suppose Wj  Rdj×dj-1 for 1  j  N , are matrices whose entries are i.i.d. Gaussians with mean 0 and standard deviation s. Then, the
distribution of W1:N is rotation-invariant.

Proof. First we remark that for any orthogonal matrix O  Rd0×d0 , the distribution of W1 is the same as that of W1O. To see this, let us denote the rows of W1 by (W1)1, . . . , (W1)d1 , and the columns of O by O1, . . . , Od0 . Then the (i1, i0) entry of W1O, for 1  i1  d1, 1  i0  d0 is (W1)i1 , Oi0 , which is a Gaussian with mean 0 and standard deviation s, since Oi0 2 = 1. Since Oi0 , Oi0 = 0 for i0 = i0, the covariance between any two distinct entries of W1O is 0. Therefore, the entries of W1O are independent Gaussians with mean 0 and standard deviation s, just as are the entries of W1.
But now for any matrix O  Rd0×d0 , the distribution of W1:N O is the distribution of WN WN-1 · · · W2(W1O), which is the same as the distribution of WN WN-1 · · · W2W1 = W1:N , since W1, W2, . . . , WN are all independent.

For a dimension d  N, radius r > 0, and 0 < h < r, a (d, r)-hyperspherical cap of height h is a subset C  Bd(r) of the form {x  Bd(r) : x, u  r - h}, where u is any d-dimensional unit
vector. We define the area of a (d, r)-hyperspherical cap of height h -- C -- to be d,r(C Sd(r)).

Lemma 14. For d  20, choose any 0  h  1. Then, the area of a (d, 1)-hyperspherical cap of

height h is at least

 3 - 4F ((1 - h) d - 1)
. 2

Proof. In Chudnov (1986), it is shown that the area of a (d, 1)-hyperspherical cap of height h is

given

by

,1-Cd-2 (h)/Cd-2 (0)
2

where

1-h

Cd(h) :=

(1 - t2)(d-1)/2dt.

0

26

Under review as a conference paper at ICLR 2019

Next, by the inequality 1 - t2  exp(-2t2) for 0  t  1/2,

1
(1 - t2)(d-3)/2dt 
0
=


1/2 -t2(d - 3)

exp 2 ·

dt

02

2F ( (d - 3)/2) - 1 /(d - 3) ·
2

/(d

-

3)

·

1

-

2

exp(-(d

-

3)/4) ,

2

(46)

where the last inequality follows from the standard estimate F (x)  1 - exp(-x2/2) for x  1. Also, since 1 - t2  exp(-t2) for all t,

1-h
(1 - t2)(d-3)/2dt 
0
=

1-h -t2(d - 3)

exp dt

0

2 

2/(d - 3) · 2F ((1 - h)

d - 3) - 1 .

2

(47)

Therefore, for d  20, by (46) and (47),



1 - Cd-2(h)/Cd-2(0)



1-

2·(2F ((1-h) d-3)-1) 1-2 exp(-(d-3)/4)

2

2



 1 - 2 · (2F ((1 - h) d - 3) - 1) · (1 + 4 exp(-(d - 3)/4))

2

3 - 4F ((1 - h) d - 3)

,

2

where the second inequality has used 1/(1 - y)  1 + 2y for all 0 < y < 1/2 (and where y = 2 exp((-(d - 3)/4)) < 2 exp(-17/4) < 1/2), and the final inequality uses 1 + 4 exp(-(d - 3)/4)  2 for d  20. The above chain of inequalities gives us the desired result.

Lemma 15. Let d  N, d  20; a  1 be a real number (possibly depending on d); and   Rd be

some vector. Set r =  2/ ad, and suppose that V  Sd(r) is drawn according to the uniform

measure.

Then,

with

probability

at

least

3-4F (2/ 2

a) , V

will have deficiency margin

 2/(ad)

with respect to .

 Proof. By rescaling, we may assume without loss of generality that  2 = 1, so that r = 1/ ad. Let D denote the intersection of Bd(r) with the open d-ball of radius 1- 1/(ad) centered at . Let C  Bd(r) denote the (d, r)-hyperspherical cap of height r · 1 - 2/( ad) = r - 2/(ad) whose

base is orthogonal to the line between 0 and  (see Figure 1). Note that d,r(D  Sd(r)), the Haar

measure of the portion of D intersecting Sd(r), gives the probability that V belongs to the boundary

of D.

1 2

·

(3

By Lemma 14 - 4F (2/ a)),

above (along and therefore

with V

rescaling arguments), since d  C with at least this probability.

20,

 d,r ( C



Sd(r))



We next claim that C  D. To see this, first let T  Rd denote the (d - 1)-sphere of radius 1 - 1/(ad) centered at  (see Figure 1). Let P be the intersection of T with the line from 0 to ,
and Q denote the intersection of this line with the unique hyperplane of codimension 1 containing T  Bd(r) -- we denote this hyperplane by H. If we can show that P - Q 2  1/(ad), then it follows that C lies entirely on the other side of H as 0, which will complete the proof that C  D.

The calculation of P - Q 2 is simply an application of the law of cosines: letting  be the angle determining the intersection of Bd(r) and T (see Figure 1), note that
 (1 - 1/(ad))2 = r2 + 12 - 2r cos  = 1/(ad) + 1 - 2/ ad · cos(),

so as desired.

d(P, Q)

=

r cos 

-

1/(ad)

=

1 (1/(ad)

-

1/(a2d2))

<

1/(ad),

2

27

Under review as a conference paper at ICLR 2019 H

T

r 0 P

Height Q of C
C



Figure 1: Figure for proof of Lemma 15. The dashed region denotes D. Not to scale.

Using that C  D, we continue with the proof. Notice the fact that C  D is equivalent to C 

Sd(r)  D  Sd(r), by the structure of C and D. Since the probability that V lands in C is at

least

3-4F (2/ 2

a) , this lower bound applies to V

landing in D as well.

Since all V

 D have

distance at most 1 - 1/(ad) from , and since min() =  2 = 1, it follows that for any

V  D,

V

-

2



min() - 1/(ad).

Therefore,

with probability of at least

3-4F (2/ 2

a) , V

has deficiency margin  2/(ad) with respect to .

Lemma 16 (Lemma 8 restated). Let d  N, d  20; b2 > b1  1 be real numbers (possibly depending on d); and   Rd be a vector. Suppose that µ is a rotation-invariant distribution over Rd with a well-defined density, such that, for some 0 < < 1,

PV µ

 2  b2d

V

2



 2 b1d

1- .



Then, with probability at least (1 -

)

·

3-4F (2/ 2

b1) , V

will have deficiency margin

with respect to .

 2/(b2d)

28

Under review as a conference paper at ICLR 2019

Proof. By rescaling we may assume that  2 = 1 without loss of generality. Then the deficiency margin of V is equal to 1 - V -  2. µ has a well-defined density, so we can set µ^ to be the probability density function of V 2. Since µ is rotation-invariant, we can integrate over spherical
coordinates, giving

P[1 - V -  2  1/(b2d)]



= P 1 - V -  2  1/(b2d) V 2 = r µ^(r)dr

0



1/( b1d) 3 - 4F (2r d)


1/( b2d)

µ^(r)dr 2



 3 - 4F (2/ b1) ·
2 

 1/( b1d)
 µ^(r)dr
1/( b2d)

 3 - 4F (2/ b1) · (1 - ),

2

where the first inequlaity used Lemma 15 and the fact that the distribution of V conditioned on V 2 = r is uniform on Sd(r).

Now we are ready to prove Claim 3:

Proof of Claim 3. We let W  R1×d0 Rd0 denote the random vector W1:N ; also let µ denote the distribution of W , so that by Lemma 13, µ is rotation-invariant. Let C1 be the constant from Lemma 12 for C2 = 999/1000. For some a  105, the standard deviation of the entries of each Wj is given by

s=



2 2

ad30d1 · · · dN-1C1

1/(2N )
.

(48)

Then by Lemma 12,

P



2 2

ad03C1

·

1 C1N

2N


W

2 2





2 2

ad0



999 .

1000

Then Lemma 16, with d = d0, b1 = a and b2 = ad02C1 · (C1N )2N , implies that with probability at

least

999 1000

·

3-4F (2/ 2

a) , W has deficiency margin

 2/(ad30C12N+1N 2N ) with respect to . But

a  105 implies that this probability is at least 0.49, and from (48),

2 ad30C12N+1N 2N

=

s2N 

d1 · · · dN-1 2(C1N )2N

.

(49)

Next recall the assumption in the hypothesis that s  C1N (c ·  2 /(d1 · · · dN-1))1/2N . Then the deficiency margin in (49) is at least

C1N (c



2 /(d1 · · · dN-1))1/(2N)  2(C1N )2N

2N d1 · · · dN-1

= c,

completing the proof.

D.6 PROOF OF CLAIM 4
Proof. The target matrices  that will be used to prove the claim satisfy min() = 1. We may assume without loss of generality that c  3/4, the reason being that if a matrix has deficiency margin c with respect to  and c < c, it certainly has deficiency margin c with respect to . We first consider the case d = 1, so that the target and all matrices are simply real numbers; we will make a slight abuse of notation in identifying 1 × 1 matrices with their unique entries. We set
29

Under review as a conference paper at ICLR 2019

 = 1. For all choices of , we will set the initializations W1(0), . . . , WN (0) so that W1:N (0) = c. Then
W1:N (0) -  F = |W1:N (0) - | = 1 - c = min() - c,
so the initial end-to-end matrix W1:N (0)  R1×1 has deficiency margin c. Now fix . Choose A  R with

A = max

2 20 · 102N-1 1/(2N-2)

N , (1 - c)c(N-1)/N , 2000, 20/,

2N

. (50)

We will set:

Wj(0) =

Ac1/N c1/N /A

: :

1  j  N/2 N/2 < j  N,

(51)

so

that

W1:N (0)

=

c.

Then

since

LN (W1, . . . , WN )

=

1 2

(1

-

WN

· · · W1)2,

the

gradient

descent

updates are given by

Wj (t + 1) = Wj (t) - (W1:N (t) - 1) · W1:j-1(t)Wj+1:N (t),

where we view W1(t), . . . , WN (t) as real numbers. This gives

c1/N A - (c - 1)c(N-1)/N /A : 1  j  N/2 Wj(1) = c1/N /A - (c - 1)c(N-1)/N A : N/2 < j  N.

Since 3/4  c < 1 and -(c - 1)c(N-1)/N A  0, we have that A/2  3A/4  Wj(1) for

1j

 N/2.

Next, since

1-c 1-c1/N

N

for 0  c < 1, we have that A2

 N



(1-c) 1-c1/N

,

which

implies

that

A2



c1/N A2 +(1-c),

or

c1/N A+

(1-c) A



A.

Thus

Wj (1)



A

for

N/2

<

j



N.

Similarly, using the same bound 3/4  c < 1 and the fact that (1 - c)c(N-1)/N A  2 we get

3 16

A



Wj (1)



A for N/2

<

j



N.

In particular, for all 1



j



N , we have that

min{,1} 10

A



Wj

(1)



max{,

1}A.

We prove the following lemma by induction:

Lemma 17. For each t  1, the real numbers W1(t), . . . , WN (t) all have the same sign and this
sign alternates for each integer t. Moreover, there are real numbers 2  B(t) < C(t) for t  1 such that for 1  j  N , B(t)  |Wj(t)|  C(t) and B(t)2N-1  20C(t).

Proof.

First we claim that we may take B(1)

=

min{,1} 10

A

and

C (1)

=

max{, 1}A.

We have

shown above that B(1)  Wj(1)  C(1) for all j. Next we establish that B(1)2N-1  20C(1).

If   1, then

B(1)2N-1 = 2N · (A/10)2N-1  20A = 20C(1),

where the inequality follows from A 

20·102N -1 2N

1/(2N -2)
by definition of A. If   1, then

B(1)2N-1 = (A/10)2N-1  20A = 20C(1),

where the inequality follows from A  2000  20 · 102N-1 1/(2N-2) by definition of A.

Now, suppose the statement of Lemma 17 holds for some t. Suppose first that Wj(t) are all positive for 1  j  N . Then for all j, as B(t)  2, and B(t)2N-1  20C(t),

Wj(t + 1)  C(t) -  · (B(t)N - 1) · B(t)N-1  C(t) -  B(t)2N-1 2
 -9C(t),

which establishes that Wj(t + 1) is negative for all j. Moreover,
Wj(t + 1)  -(C(t)N - 1) · C(t)N-1  -C(t)2N-1.

30

Under review as a conference paper at ICLR 2019

Now set B(t + 1) = 9C(t) and C(t + 1) = C(t)2N-1. Since N  2, we have that
B(t + 1)2N-1 = (9C(t))2N-1  93C(t)2N-1 > 20C(t)2N-1 = 20C(t + 1).
The case that all Wj(t) are negative for 1  j  N is nearly identical, with the same values for B(t + 1), C(t + 1) in terms of B(t), C(t), except all Wj(t + 1) will be positive. This establishes the inductive step and completes the proof of Lemma 17.

By

Lemma

17,

we

have

that

for

all

t



1,

LN (W1(t), . . . , WN (t))

=

1 2

(W1:N

(t)

-

1)2



1 2

(2N

-

1)2 > 0, thus completing the proof of Claim 4 for the case where all dimensions are equal to 1.

For the general case where d0 = d1 = · · · = dN = d for some d  1, we set  = Id, and given c, , we set Wj(0) to be the d × d diagonal matrix where all diagonal entries except the first one are equal to 1, and where the first diagonal entry is given by Equation (51), where A is given by Equation (50). It is easily verified that all entries of Wj(t), 1  j  N , except for the first diagonal element of each matrix, will remain constant for all t  0, and that the first diagonal elements evolve exactly as
in the 1-dimensional case presented above. Therefore the loss in the d-dimensional case is equal to
the loss in the 1-dimensional case, which is always greater than some positive constant.

We remark that the proof of Claim 4 establishes that the loss (t) := LN (W1(t), . . . , WN (t)) grows at least exponentially in t for the chosen initialization. Such behavior, in which gradients and weights
explode, indeed takes place in deep learning practice if initialization is not chosen with care.

D.7 PROOF OF CLAIM 5

Proof. We will show that a target matrix   Rd×d which is symmetric with at least one negative
eigenvalue, along with identity initialization (Wj(0) = Id, j  {1, . . . , N }), satisfy the conditions of the claim. First, note that non-stationarity of initialization is met, as for any 1  j  N ,

LN (W1(0), . . . , WN (0))  Wj (0)

=

Wj+1:N (0)

(W1:N (0) - )W1:j-1(0) = Id -  = 0,

where the last inequality follows since  has a negative eigenvalue. To analyze gradient descent we use the following result, which was established in Bartlett et al. (2018):

Lemma 18 (Bartlett et al. (2018), Lemma 6). If W1(0), . . . , WN (0) are all initialized to identity, 
is symmetric,  = U DU is a diagonalization of , and gradient descent is performed with any learning rate, then for each t  0 there is a diagonal matrix D^ (t) such that Wj(t) = U D^ (t)U for each 1  j  N .

By Lemma 18, for any choice of learning rate , the end-to-end matrix at time t is given by W1:N (t) = U D^ (t)N U . As long as some diagonal element of D is negative, say equal to - < 0, then

(t)

=

LN (W1(t), . . . , WN (t))

=

1 2

W1:N (t) - 

2 F

1 =
2

D^ (t)L - D

2 F

 1 2 > 0. 2

31

