Under review as a conference paper at ICLR 2019
ADVERSARIAL IMITATION VIA VARIATIONAL INVERSE REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
We consider a problem of learning a reward and policy from expert examples under unknown dynamics in high-dimensional scenarios. Our proposed method builds on the framework of generative adversarial networks and exploits reward shaping to learn near-optimal rewards and policies. Potential-based reward shaping functions are known to guide the learning agent whereas in this paper we bring forward their benefits in learning near-optimal rewards. Our method simultaneously learns a potential-based reward shaping function through variational information maximization along with the reward and policy under the adversarial learning formulation. We evaluate our method on various high-dimensional complex control tasks. We also evaluate our learned rewards in transfer learning problems where training and testing environments are made to be different from each other in terms of dynamics or structure. Our experimentation shows that our proposed method not only learns near-optimal rewards and policies matching expert behavior, but also performs significantly better than state-of-the-art inverse reinforcement learning algorithms.
1 INTRODUCTION
Reinforcement learning (RL) has emerged as a promising tool for solving complex decision-making and control tasks from predefined high-level reward functions (Silver et al., 2016; Qureshi et al., 2017). However, defining an optimizable reward function that inculcates the desired behavior can be challenging for many robotic applications, which include learning social-interaction skills (Qureshi et al., 2018), dexterous manipulation (Finn et al., 2016b), autonomous driving (Kuderer et al., 2015), and robotic surgery (Yip & Das, 2017).
Inverse reinforcement learning (IRL) (Ng et al., 2000) addresses the problem of learning reward functions from expert demonstrations, and it is often considered as a branch of imitation learning (Argall et al., 2009). The prior work in IRL includes maximum-margin (Abbeel & Ng, 2004; Ratliff et al., 2006) and maximum-entropy (Ziebart et al., 2008) formulations. Currently, maximum entropy (MaxEnt) IRL is a widely used approach towards IRL, and has been extended to use non-linear function approximators such as neural networks in scenarios with unknown dynamics by leveraging sampling-based techniques (Boularias et al., 2011; Finn et al., 2016b; Kalakrishnan et al., 2013). However, designing the IRL algorithm is usually complicated as it requires, to some extent, hand engineering such as deciding domain-specific regularizers (Finn et al., 2016b).
Rather than learning reward functions and solving the IRL problem, the imitation learning (IL) methods were proposed that learn a policy directly from expert demonstrations. Prior work addressed the IL problem through behavior cloning (BC) which learns a policy from expert trajectories using supervised learning (Pomerleau, 1991). Although BC methods are simple solutions to IL, these methods require a large amount of data because of compounding errors induced by covariate shift (Ross et al., 2011). To overcome BC limitations a generative adversarial imitation learning (GAIL) algorithm (Ho & Ermon, 2016) was proposed. GAIL uses Generative Adversarial Networks (GANs) formulation (Goodfellow et al., 2014), i.e., a generator-discriminator framework, where generator learns to generate expert-like trajectories and discriminator learns to distinguish between generated and expert trajectories. Although GAIL is highly effective and efficient framework, it does not recover transferable/portable reward functions along with the policies. Reward function learning is ultimately preferable, if possible, over direct imitation learning as rewards are portable functions that
1

Under review as a conference paper at ICLR 2019
represent the most basic and complete representation of agent intention, and can be re-optimized in new environments and new agents.
Reward learning is challenging as there can be many optimal policies explaining a set of demonstrations and many reward functions inducing an optimal policy (Ng et al., 1999). Recently, an adversarial inverse reinforcement learning (AIRL) framework (Fu et al., 2017), an extension of GAIL, was proposed that offers a solution to the former issue by exploiting the maximum entropy IRL method (Ziebart et al., 2008) whereas the latter issue is addressed through learning disentangled reward functions, i.e., the reward is a function of state only instead of both state and action. The disentangled reward prevents actions-driven reward shaping (Fu et al., 2017) and is able to recover transferable reward functions, but has two main disadvantages. First, AIRL fails to recover the ground truth reward when the ground truth reward is a function of both state and action. For example, the reward function in any locomotion or ambulation tasks contains a penalty term that discourages actions with large magnitudes. This need for action regularization is well known in optimal control literature and limits the use cases of a state-only reward function in most practical real-life applications. Second, reward shaping plays a vital role in quickly recovering invariant policies (Ng et al., 1999) and thus for AIRL, it is usually not possible to simultaneously recover optimal/near-optimal policies when learning disentangled rewards.
In this paper, we propose the empowerment-based adversarial inverse reinforcement learning (EAIRL) algorithm1. Empowerment (Salge et al., 2014) is a mutual information-based theoretic measure, like state- or action-value functions, that assigns a value to a given state to quantify an extent to which an agent can influence its environment. Our method uses variational information maximization (Mohamed & Rezende, 2015) to learn empowerment in parallel to learning the reward and policy from expert data. The empowerment acts as a potential function for shaping rewards. Our experimentation shows that the proposed method recovers not only near-optimal policies but also recovers robust, near-optimal, transferable, non-disentangled (state-action) reward functions. The results on reward learning show that EAIRL outperforms several state-of-the-art methods by recovering ground-truth reward functions. On policy learning, results demonstrate that policies learned through EAIRL perform comparably to GAIL and AIRL with non-disentangled (state-action) reward function but significantly outperform policies learned through AIRL with disentangled reward and GAN interpretation of Guided Cost Learning (GAN-GCL) (Finn et al., 2016a).
2 BACKGROUND
We consider a Markov decision process (MDP) represented as a tuple (S, A, P, R, 0, ) where S denotes the state-space, A denotes the action-space, P represents the transition probability distribution, i.e., P : S × A × S  [0, 1], R(s, a) corresponds to the reward function, 0 is the initial state distribution 0 : S  R, and   (0, 1) is the discount factor. Let q(a|s, s ) be an inverse model that maps current state s  S and next state s  S to a distribution over actions A, i.e., q : S × S × A  [0, 1]. Let  be a stochastic policy that takes a state and outputs a distribution over actions such that  : S × A  [0, 1]. Let  and E denote a set of trajectories, a sequence of state-action pairs (s0, a0, · · · sT , aT ), generated by a policy  and an expert policy E, respectively, where T denotes the terminal time. Finally, let (s) be a potential function that quantifies a utility of a given state s  S, i.e.,  : S  R. In our proposed work, we use an empowerment-based potential function (s) for reward shaping to adversarially learn both reward function and policy. Therefore, the following sections provide a brief background on potential-based reward shaping functions and their benefits to imitation learning, adversarial reward and policy learning, and variational information-maximization approach to learn the empowerment.
2.1 SHAPING REWARDS
In this section, we briefly describe a formal framework of reward-shaping and its importance to policy and reward learning (for details see (Ng et al., 1999)). We consider a general form of reward function R : S × A × S  R, i.e., the reward R(s, a, s ) is a function of current state s  S, action a  A, and next state s  S. Let F : S × A × S  R be a reward shaping function and R be a transformed reward function denoted as R = R + F . Ng et al. (1999) proved that an optimal
1Supplementary material is available at sites.google.com/view/eairl
2

Under review as a conference paper at ICLR 2019

behavior of a policy remains unchanged if the reward undergoes transformation through a shaping function F of form (s ) - (s), i.e.,
Theorem 1 (see (Ng et al., 1999)) We say F : S × A × S  R is a potential-based shaping function if there exist a real-valued function  : S  R and F = (s ) - (s). Then a potential-based shaping function F is a necessary and sufficient condition to grantee that an optimal policy  learned in MDP M = (S, A, P, R = R + F, 0, ) is also optimal in the MDP M = (S, A, P, R, 0, ), i.e., a policy  is invariant to reward transformations.
Reward shaping plays a vital role in learning both rewards and policies from expert demonstrations (Ng et al., 1999). In the former case, reward shaping determines the extent to which a true reward function can be recovered whereas in the latter case, reward shaping speeds up the learning process by supplementing an actual reward function to guide the learning process. Despite several advantages of shaping rewards, a potential-based shaping function F = (s ) - (s) which is a sufficient and necessary condition for preserving policy behavior (Ng et al., 1999) is usually not available. There exist several methods (Asmuth et al., 2008; Grzes & Kudenko, 2009) to learn potential-based reward shaping functions but they assume the availability of transition model P, and are furthermore demonstrated in small-scale maze-solving problems. In this paper, we show we are able to learn the potential-based reward shaping functions without a transition model, as well as one that also scales to higher dimensional problems by modeling a function  as Empowerment (Salge et al., 2014) which we learn efficiently online through variational information-maximization (Mohamed & Rezende, 2015).

2.2 ADVERSARIAL INVERSE REINFORCEMENT LEARNING

This section briefly describes Adversarial Inverse Reinforcement Learning (AIRL) (Fu et al., 2017) algorithm which forms a baseline of our proposed method. AIRL is state-of-the-art IRL method that builds on GAIL (Ho & Ermon, 2016), maximum entropy IRL framework (Ziebart et al., 2008) and GAN-GCL, a GAN interpretation of Guided Cost Learning (Finn et al., 2016b;a).

GAIL is a model-free adversarial learning framework, inspired from GANs (Goodfellow et al., 2014), where the policy  learns to imitate the expert policy behavior E by minimizing the JensenShannon divergence between the state-action distributions generated by  and the expert state-action distribution by E through following objective

min


max
D(0,1)S×A

E

[log

D(s,

a)]

+

EE

[log(1

-

D

(s,

a))]

-

H

(

)

(1)

where D is the discriminator that performs the binary classification to distinguish between samples generated by  and E,  is a hyper-parameter, and H() is an entropy regularization term E[log ]. Note that GAIL does not recover reward; however, Finn et al. (2016a) shows that the discriminator can be modeled as a reward function. Thus AIRL (Fu et al., 2017) presents a formal implementation of (Finn et al., 2016a) and extends GAIL to recover reward along with the policy by imposing a following structure on the discriminator:

D,(s, a, s

)

=

exp[f,(s, a, s )] exp[f,(s, a, s )] + (a|s)

(2)

where f,(s, a, s ) = r(s) + h(s ) - h(s) comprises disentangled reward term r(s) with training parameters , and shaping term F = h(s ) - h(s) with training parameters . The entire D,(s, a, s ) is trained as a binary classifier to distinguish between expert demonstrations E and policy generated demonstrations  . The policy is trained to maximize the discriminative reward
r^(s, a, s ) = log(D(s, a, s ) - log(1 - D(s, a, s ))). Note that the function F = h(s ) - h(s) consists of free-parameters as no structure is imposed on h(·), and as mentioned in (Fu et al., 2017), the reward function r(·) and function F are tied upto a constant ( - 1)c, where c  R, thus the impact of F , the shaping term, on the recovered reward r is quite limited and therefore, the benefits
of reward shaping are barely utilized.

2.3 EMPOWERMENT AS MAXIMAL MUTUAL INFORMATION
Mutual information (MI), an information-theoretic measure, quantifies the dependency between two random variables.

3

Under review as a conference paper at ICLR 2019

In intrinsically-motivated reinforcement learning, a maximal of mutual information between a sequence of K actions a and the final state s reached after the execution of a, conditioned on current state s is often used as a measure of internal reward (Mohamed & Rezende, 2015), known as Empowerment (s), i.e.,

p(a, s |s) (s) = max I(a, s |s) = max Ep(s |a,s)w(a|s) log w(a|s)p(s |s)

(3)

where p(s |a, s) is a K-step transition probability, w(a|s) is a distribution over a, and p(a, s |s) is a joint-distribution of K actions a and final state s 2.

Intuitively, the empowerment (s) of a state s quantifies an extent to which an agent can influence its future. Empowerment, like value functions, is a potential function that has been previously used in reinforcement learning but its applications were limited to small-scale cases due to computational intractability of MI maximization in higher-dimensional problems. However, recently a scalable method (Mohamed & Rezende, 2015) was proposed that learns the empowerment through the more-efficient maximization of variational lower bound, which has been shown to be equivalent to maximizing MI (Agakov, 2004). The lower bound was derived (for complete derivation see Appendix A.1) by representing MI in term of the difference in conditional entropies H(·) and utilizing the non-negativity property of KL-divergence, i.e.,
Iw(s) = H(a|s) - H(a|s , s)  H(a) + Ep(s |a,s)w(a|s)[log q(a|s , s)] = Iw,q(s) (4)
where H(a|s) = -Ew(a|s)[log w(a|s)], H(a|s , s) = -Ep(s |a,s)w(a|s)[log p(a|s , s)], q(·) is a variational distribution with parameters  and w(·) is a distribution over actions with parameters .
Finally, the lower bound in Eqn. 4 is maximized under the constraint H(a|s) <  (to avoid divergence, see (Mohamed & Rezende, 2015)) to compute empowerment as follow:

1

(s)

=

max
w,q

Ep(s

|a,s)w(a|s)[-



log

w (a|s)

+

log

q(a|s

,

s)]

(5)

where  is  dependent temperature term. Mohamed & Rezende (2015) also applied the principles

of Expectation-Maximization (EM) (Agakov, 2004) to learn empowerment, i.e., alternatively max-

imizing Eqn. 5 with respect to w(a|s) and q(a|s , s). Given a set of training trajectories  , the

maximization of Eqn. 5 w.r.t q(·) is shown to be a supervised maximum log-likelihood problem

whereas the maximization w.r.t w(·) is determined through the functional derivative I/w = 0

under the constraint a w(a|s) = 1. The optimal w that maximizes Eqn. 5 turns out to be

1 Z(s) exp(Ep(s |s,a)[log q(a|s, s )]), where Z(s) is a normalization term.

By substituting w

in

1 Eqn. 5 showed that the empowerment (s) =  log Z(s) (for full derivation, see Appendix A.2).

Since w(a|s) is an unnormalized distribution, Mohamed & Rezende (2015) introduced an approximation w(a|s)  log (a|s) + (s) where (a|s) is a normalized distribution which leaves the scalar function (s) to account for the normalization term log Z(s). Finally, the parameters of policy  and scalar function  are optimized by minimizing the discrepancy between the two approximations (log (a|s) + (s)) and  log q(a|s , s)) through the squared error as follow:

lI (s, a, s ) =  log q(a|s , s) - (log (a|s) + (s)) 2

(6)

3 EMPOWERED ADVERSARIAL INVERSE REINFORCEMENT LEARNING
We present an inverse reinforcement learning algorithm that simultaneously and adversarially learns a robust, transferable reward function and policy from expert demonstrations. Our proposed method comprises (i) an inverse model q(a|s , s) that takes the current state s and the next state s to output a distribution over actions A that resulted in s to s transition, (ii) a reward r(s, a), with parameters , that is a function of both state and action, (iii) an empowerment-based potential function (·)
2In our proposed work, we consider only immediate step transitions i.e., K = 1, hence variables s, a and s will be represented in non-bold notations.

4

Under review as a conference paper at ICLR 2019

with parameters  that determines the reward-shaping function F = (s ) - (s), and (iv) a policy model (a|s) outputs a distribution over actions given the current state s. All these models
are trained simultaneously based on the objective functions described in the following sections.

3.1 INVERSE MODEL q(a|s, s ) OPTIMIZATION

As mentioned in Section 2.3, learning the inverse model q(a|s, s ) is a maximum log-likelihood supervised learning problem. Therefore, given a set of trajectories   , where a single trajectory is a sequence states and actions, i.e., i = {s0, a0, · · · , sT , aT }i, the inverse model q(a|s , s) is trained to minimize the mean-square error between its predicted action q(a|s , s) and the action a
taken according to the generated trajectory  , i.e.,

lq(s, a, s ) = (q(·|s, s ) - a)2

(7)

3.2 EMPOWERMENT (s) OPTIMIZATION

Empowerment will be expressed in terms of normalization function Z(s) of optimal w(a|s), i.e.,

1

(s)

=

log Z(s). 

Therefore,

the estimation

of

empowerment

(s) is approximated by mini-

mizing the loss function lI (s, a, s ), presented in Eqn. 6, w.r.t parameters , and the inputs (s, a, s )

are sampled from the policy-generated trajectories  .

3.3 REWARD FUNCTION r(s, a)

To train the reward function, we first compute the discriminator as follow:

D(s, a, s

)

=

exp[r(s, a) +  (s ) - (s)] exp[r(s, a) +  (s ) - (s)] + (a|s)

(8)

where r(s, a) is the reward function to be learned with parameters . We also maintain the target  and learning  parameters of the empowerment-based potential function. The target parameters

 are a replica of  except that the target parameters  are updated to learning parameters 

after every n training epochs. Note that keeping a stationary target  stabilizes the learning as also highlighted in (Mnih et al., 2015). Finally, the discriminator/reward function parameters  are

trained via binary logistic regression to discriminate between expert E and generated  trajectories, i.e.,

E [log D(s, a, s )] + EE [(1 - log D(s, a, s ))]

(9)

3.4 POLICY OPTIMIZATION POLICY (a|s)

We train our policy (a|s) to maximize the discriminative reward r^(s, a, s ) = log(D(s, a, s ) - log(1 - D(s, a, s ))) and to minimize the loss function lI (s, a, s ) =  log q(a|s, s ) - (log (a|s) + (s)) 2 to learn the empowerment. Hence, the overall policy training objective is:

E [log (a|s)r^(s, a, s )] + I E lI (s, a, s )]

(10)

where policy parameters  are updated by taking KL-constrained natural gradient step using any policy optimization method such as TRPO (Schulman et al., 2015) or an approximated step such as PPO (Schulman et al., 2017).

Algorithm 1 outlines the overall training procedure to train all function approximators simultaneously. Note that the expert samples E are seen by the discriminator only, whereas all other models are trained using the policy generated samples  . Furthermore, as highlighted in (Fu et al., 2017), the discriminating reward r^(s, a, s ) boils down to the following expression

r^(s, a, s ) = f (s, a, s ) - log (a|s)

(11)

where f (s, a, s ) = r(s, a) +  (s ) - (s). Hence, our policy training objective maximizes the learned shaped-reward function f (s, a, s ) and minimizes the discrepancy between (log (a|s)+

5

Under review as a conference paper at ICLR 2019
Algorithm 1: Empowerment-based Adversarial Inverse Reinforcement Learning Initialize parameters of policy , and inverse model q Initialize parameters of target  and training  empowerment, and reward r functions Obtain expert demonstrations E by running expert policy E for i  0 to N do
Collect trajectories  by executing  Update i to i+1 with the gradient E [ i lq(s, a, s )] Update i to i+1 with the gradient E [ i lI (s, a, s )] Update i to i+1 with the gradient:
E [ i log Di (s, a, s )] + EE [ i (1 - log Di (s, a, s ))]
Update i to i+1 using TRPO/PPO update rule with the following objective: E i log i (a|s)r^i+1 (s, a, s ) + I E i lI (s, a, s )
After every n epochs sync  with 

(a) Ant environments

(b) Policy performance on learned rewards.

Figure 1: The policy performance in the crippled-ant environment based on the reward learned using expert demonstrations in the normal ant enviroment. It can be seen that our method performs significantly better than AIRL and exhibits expert-like performance in all five trials which implies that our method almost recovered ground-truth reward function.

(s)) and  log q(a|s , s)), with the term log (a|s) acting as a regularizer. Moreover, note that the function f (s, a, s ) can be viewed as a single-sample estimate of the advantage function i.e.,
f (s, a, s ) = r(s, a) + (s ) - (s)  r(s, a) + V (s ) - V (s) = A(s, a, s ) (12)
Hence, our method trains the policy under reward transformations which leads to learning an invariant and robust policy from expert demonstrations.
4 RESULTS
Our proposed method, EAIRL, simultaneously learns reward and policy from expert demonstrations. We evaluate our method against state-of-the-art policy and reward learning techniques on several control tasks in OpenAI Gym. In case of policy learning, we compare our method against GAIL, GAN-GCL, AIRL with state-only reward, denoted as AIRL(s), and AIRL with state-action reward, denoted as AIRL(s, a). In reward learning, we only compare our method against AIRL(s) and AIRL(s, a) as GAIL does not recover reward, and GAN-GCL is shown to exhibit inferior performance than AIRL (see (Fu et al., 2017)). Furthermore, in the comparisons, we also include the expert performances which represents a policy learned by optimizing a ground-truth reward using TRPO. The performance of different methods are evaluated in term of average total reward accu-
6

Under review as a conference paper at ICLR 2019

(a) Pointmass-maze environments

(b) Policy performance on transfered rewards.

Figure 2: The policy performance on a transfer learning task where the learned rewards are tested in a shifted maze. The task is to navigate the 2D agent (yellow) to the goal (green) and the transfer involves learning to take a completely an opposite route to the goal. It can be seen that our method recovers near-optimal reward functions and exhibit better performance than AIRL in all five trials.

mulated (denoted as score) by an agent during the trial, and for each experiment, we run five trials.

Table 1: The evaluation of reward learning on transfer learning tasks. Mean scores (higher the better) with standard deviation are presented over 5 trials.

Algorithm
Expert EAIRL(Ours) AIRL AIRL

States-Only
N/A No Yes No

Pointmass-Maze
-4.98 ± 0.29 -6.83 ± 0.54 -8.07 ± 0.50 -19.28 ± 2.03

Crippled-Ant
432.66 ± 14.38 346.53 ± 41.07 175.51 ± 27.31
46.12 ± 14.37

4.1 REWARD LEARNING PERFORMANCE (TRANSFER LEARNING EXPERIMENTS)
To evaluate the learned rewards, we consider a transfer learning problem in which the testing environments are made to be different from the training environments. More precisely, the rewards learned via IRL in the training environments are used to re-optimize a new policy in the testing environment. We consider two test cases, shown in the Fig. 1 and Fig. 2, in which the agent's dynamics and physical environment is modified, respectively.
In the first test case, as shown in Fig. 1(a), we modify the agent itself during testing. We trained a reward function to make a standard quadrupled ant to run forward. During testing, we disabled the front two legs (indicated in red) of the ant (crippled-ant), and the learned reward is used to reoptimize the policy to make a crippled-ant move forward. Note that the crippled-ant cannot move sideways (see Appendix B.1). Therefore, the agent has to change the gait to run forward. In the second test case, shown in Fig 2(a), the agent learns to navigate a 2D point-mass to the goal region in a simple maze. We re-position the maze central-wall during testing so that the agent has to take a different path, compared to the training environment, to reach the target (see Appendix B.2).
Fig. 1(b) and Fig. 2(b) compare the policy performance scores over five different trials of EAIRL, AIRL(s) and AIRL(s, a) in the aforementioned transfer learning tasks. The expert score is shown as a horizontal line to indicate the standard set by an expert policy. Table 1 summarizes the mean score of five trials with a standard deviation in above-mentioned transfer learning experiments. It can be seen that our method recovers near-optimal reward functions as the policy scores almost reach the expert scores in all five trials. Furthermore, our method performs significantly better than both AIRL(s) and AIRL(s, a) in matching an expert's performance.
7

Under review as a conference paper at ICLR 2019

(a) HalfCheetah

(b) Ant

(c) Swimmer

(d) Pendulum

Figure 3: Benchmark control tasks for imitation learning

4.2 POLICY LEARNING PERFORMANCE (IMITATION LEARNING)
Table 2 presents the means and standard deviations of policy learning performance scores, over the five different trials, in various control tasks. For each algorithm, we provided 20 expert demonstrations for imitation, generated by optimizing a policy on a ground-truth reward using TRPO. The tasks, shown in Fig. 3, include (i) making a 2D cheetah robot to run forward, (ii) making a 3D quadrupled robot (ant) to move forward, (iii) making a 2D robot to swim (swimmer), and (iv) keeping a friction less pendulum to stand vertically up. It can be seen that EAIRL, AIRL(s, a) and GAIL demonstrate similar performance and successfully learn to imitate expert policy whereas AIRL(s) and GAN-GCL fails to recover a policy.

Table 2: The evaluation of imitation learning on benchmark control tasks. Mean scores (higher the better) with standard deviation are presented over 5 trials for each method.

Methods
Expert GAIL GCL AIRL(s,a) AIRL(s) EAIRL

HalfCheetah 2139.83 ± 30.22 1880.05 ± 15.72 -189.90 ± 44.42 1826.26 ± 19.64 121.10 ± 42.31 1861.40 ± 18.49

Environments Ant Swimmer 935.12 ± 10.94 76.21 ± 1.79 738.72 ± 9.49 50.21 ± 0.26 16.74 ± 36.59 15.75 ± 7.32 645.90 ± 41.75 49.52 ± 0.48 271.31 ± 9.35 33.21 ± 2.40 635.83 ± 30.83 49.54 ± 0.32

Pendulum -100.11 ± 1.32 -116.01 ± 5.45 -578.18 ± 72.84 -118.13 ± 11.33 -134.82 ± 10.89 -116.34 ± 7.133

5 DISCUSSION
This section highlights the importance of state-action rewards and potential-based reward shaping functions on learning policies and rewards, respectively, from expert demonstrations.
Ng et al. (1999) theoretically discussed the importance of potential-based reward shaping in a structural prediction of the MDP but, to the best of our knowledge, no prior work has reported the practical approach to learn potential-based reward shaping function and its implications to IRL. Note that our method, EARIL, and AIRL with state-action reward function, i.e., AIRL(s, a), shares the same discriminator formulation except that AIRL(s, a) does not impose any structure on the rewardshaping function, while our method models the reward-shaping function through empowerment. The numerical results of reward learning, reported in the previous section, indicate that AIRL(s, a) fails to learn rewards whereas EAIRL recovers the near-optimal reward functions. This highlights the positive impact of using a potential-based reward-shaping function on reward learning. Thus our experimentation validates the theoretical propositions of (Ng et al., 1999) that the reward shaping function determines an extent to which the true reward function can be recovered from the expert demonstrations.
Our experimentation highlights the importance of modeling discriminator/reward functions in the adversarial learning framework as a function of both state and action. The notion of disentangled rewards leaves the discriminator function to depend on states only. The results show that AIRL with disentangled rewards fails to learn a policy whereas EAIRL, GAIL, and AIRL that include stateaction reward successfully recover the policies. Hence, it is crucial to model reward/discriminator
8

Under review as a conference paper at ICLR 2019
as a function of state-action as otherwise, adversarial imitation learning fails to retrieve a policy from expert data.
Our method leverages both the potential-based reward-shaping function and state-action dependent rewards, and therefore learns both reward and policy simultaneously. On the other hand, GAIL learns policy but cannot recover reward function whereas AIRL cannot learn reward and policy simultaneously.
6 CONCLUSIONS AND FUTURE WORK
We present an approach to adversarial reward and policy learning from expert demonstrations by efficiently and effectively utilizing reward shaping for inverse reinforcement learning. We learn a potential-based reward shaping function in parallel to learning the reward and policy. Our method transforms the learning reward through shaping function that leads to acquiring a rewardtransformations preserving invariant policy. The invariant policy in turn guides the reward-learning process to recover near-optimal reward. We show that our method successfully learns near-optimal rewards, policies, and performs significantly better than state-of-the-art IRL methods in both imitation learning and transfer learning. The learned rewards are shown to be transferable to environments that are dynamically or structurally different from training environments.
In our future work, we plan to extend our method to learn rewards and policies from diverse human/expert demonstrations as the proposed method assumes that a single expert generates the training data. Another exciting direction is to learn from sub-optimal demonstrations that also contains failures in addition to optimal behaviors.
REFERENCES
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.
David Barber Felix Agakov. The im algorithm: a variational approach to information maximization. Advances in Neural Information Processing Systems, 16:201, 2004.
Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469­483, 2009.
John Asmuth, Michael L Littman, and Robert Zinkov. Potential-based shaping in model-based reinforcement learning. In AAAI, pp. 604­609, 2008.
Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 182­189, 2011.
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016a.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning, pp. 49­58, 2016b.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Marek Grzes and Daniel Kudenko. Learning shaping rewards in model-based reinforcement learning. In Proc. AAMAS 2009 Workshop on Adaptive Learning Agents, volume 115, 2009.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565­4573, 2016.
9

Under review as a conference paper at ICLR 2019
Mrinal Kalakrishnan, Peter Pastor, Ludovic Righetti, and Stefan Schaal. Learning objective functions for manipulation. In Robotics and Automation (ICRA), 2013 IEEE International Conference on, pp. 1331­1336. IEEE, 2013.
Markus Kuderer, Shilpa Gulati, and Wolfram Burgard. Learning driving styles for autonomous vehicles from demonstration. In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pp. 2641­2646. IEEE, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 2125­2133, 2015.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pp. 278­287, 1999.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, pp. 663­670, 2000.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1):88­97, 1991.
Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa, and Hiroshi Ishiguro. Show, attend and interact: Perceivable human-robot social interaction through neural attention q-network. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 1639­1645. IEEE, 2017.
Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa, and Hiroshi Ishiguro. Intrinsically motivated reinforcement learning for human­robot interaction in the real-world. Neural Networks, 2018.
Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In Proceedings of the 23rd international conference on Machine learning, pp. 729­736. ACM, 2006.
Ste´phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627­635, 2011.
Christoph Salge, Cornelius Glackin, and Daniel Polani. Empowerment­an introduction. In Guided Self-Organization: Inception, pp. 67­114. Springer, 2014.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889­1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Michael Yip and Nikhil Das. Robot autonomy for surgery. arXiv preprint arXiv:1707.03080, 2017.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pp. 1433­1438. Chicago, IL, USA, 2008.
10

Under review as a conference paper at ICLR 2019

APPENDICES

A VARIATIONAL EMPOWERMENT
For completeness, we present a derivation of presenting mutual information (MI) as variational lower bound and maximization of lower bound to learn empowerment.

A.1 VARIATIONAL INFORMATION LOWER BOUND

As mentioned in section 2.3, the variational lower bound representation of MI is computed by defining MI as a difference in conditional entropies, and the derivation is formalized as follow.

Iw,q(s) = H(a|s) - H(a|s , s)

= H(a|s) + Ep(s |a,s)w(a|s)[log p(a|s , s)]

p(a|s , s)q(a|s , s)

= H(a|s) + Ep(s |a,s)w(a|s)[log

q(a|s , s) ]

p(a|s , s)

=

H (a|s)

+

Ep(s

|a,s)w(a|s)[log

q(a|s

,

s)]

+

Ep(s

|a,s)w(a|s)[log

q(a|s

,

] s)

= H(a|s) + Ep(s |a,s)w(a|s)[log q(a|s , s)] + KL[p(a|s , s)||q(a|s , s)]  H(a|s) + Ep(s |a,s)w(a|s)[log q(a|s , s)]  -Ew(a|s) log w(a|s) + Ep(s |a,s)w(a|s)[log q(a|s , s)]

A.2 VARIATIONAL INFORMATION MAXIMIZATION
The empowerment is a maximal of MI and it can be formalized as follow by exploiting the variational lower bound formulation (for details see (Mohamed & Rezende, 2015)).

1

(s)

=

max
w,q

Ep(s

|a,s)w(a|s)[-



log

w(a|s)

+

log

q(a|s

,

s)]

(13)

As mentioned in section 2.3, given a training trajectories, the maximization of Eqn. 13 w.r.t in-
verse model q(a|s , s) is a supervised maximum log-likelihood problem. The maximization of Eqn. 13 w.r.t w(a|s) is derived through a functional derivative Iw,q/w = 0 under the constraint
a w(a|s) = 1. For simplicity, we consider discrete state and action spaces, and the derivation is as follow:

I^w (s)

=

Ep(s

|a,s)w(a|s)[-

1 

log

w(a|s)

+

log

q(a|s

,

s)]

+



w(a|s) - 1

a

1

= p(s |a, s)w(a|s){- log w(a|s) + log q(a|s , s)} +  w(a|s) - 1



as

a

 I^w (s)

= w

{( - ) - log w(a|s) + Ep(s |a,s)[log q(a|s , s)]} = 0

a

w(a|s) = e- eEp(s |a,s)[log q(a|s ,s)]

By using the constraint a w(a|s) = 1, it can be shown that the optimal solution w(a|s) = 1
Z(s) exp(u(s, a)), where u(s, a) = Ep(s |a,s)[log q(a|s , s)] and Z(s) = a u(s, a). This solu-

tion maximizes the lower bound since 2Iw(s)/w2 = -

1 a w(a|s) < 0.

11

Under review as a conference paper at ICLR 2019
B TRANSFER LEARNING PROBLEMS
B.1 ANT ENVIRONMENT The following figures show the difference between the path profiles of standard and crippled Ant. It can be seen that the standard Ant can move sideways whereas the crippled ant has to rotate in order to move forward.
Figure 4: The top and bottom rows show the gait of standard and crippled ant, respectively.
B.2 MAZE ENVIRONMENT The following figures show the path profiles of a 2D point-mass agent to reach the target in training and testing environment. It can be seen that in the testing environment the agent has to take the opposite route compared to the training environment to reach the target.
Figure 5: The top and bottom rows show the path followed by a 2D point-mass agent (yellow) to reach the target (green) in training and testing environment, respectively.
C IMPLEMENTATION DETAILS
C.1 NETWORK ARCHITECTURES We use two-layer ReLU network with 32 units in each layer for the potential function h(·) and (·), reward function r(·), discriminators of GAIL and GAN-GCL. Furthermore, policy (·) of all presented models and the inverse model q(·) of EAIRL are presented by two-layer RELU network with 32 units in each layer, where the network's output parametrizes the Gaussian distribution, i.e., we assume a Gaussian policy.
12

Under review as a conference paper at ICLR 2019 C.2 HYPERPARAMETERS For all experiments, we use the temperature term  = 1. We set entropy regularization weight to 0.1 and 0.001 for reward and policy learning, respectively. The hyperparameter I was set to 1 for reward learning and 0.001 for policy learning. The target parameters of the empowerment-based potential function  (·) were updated every 5 and 2 epochs during reward and policy learning respectively. Although reward learning parameters are also applicable to policy learning, we decrease the magnitude of entropy and information regularizers during policy learning to speed up the policy convergence to optimal values. Furthermore, we set the batch size to 2000- and 20000-steps per TRPO update for the pendulum and remaining environments, respectively. For the methods (Fu et al., 2017; Ho & Ermon, 2016) presented for comparison, we use their suggested hyperparameters. We also use policy samples from previous 20 iterations as negative data to train the discriminator of all IRL methods presented in this paper to prevent the parametrized reward functions from overfitting the current policy samples.
13

