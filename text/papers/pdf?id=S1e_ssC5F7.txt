Under review as a conference paper at ICLR 2019
HYPER-REGULARIZATION: AN ADAPTIVE CHOICE FOR THE LEARNING RATE IN GRADIENT DESCENT
Anonymous authors Paper under double-blind review
ABSTRACT
We present a novel approach for adaptively selecting the learning rate in gradient descent methods. Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradientbased algorithms in terms of accuracy as well as convergence rate.
1 INTRODUCTION
The automatic choice of the learning rate remains crucial in improving the efficiency of gradient descent algorithms, especially for solving nonconvex optimization problems. It is desirable to adaptively update the learning rate during the training process with a certain strategy. The convergence guarantees of such a strategy in theories usually require the Lipschitz constant or smoothness constant of the objective function to be explicitly known (Nesterov, 2013; Bubeck et al., 2015), which is inaccessible in most cases, e.g., in deep neural networks.
Using the received gradient information to adjust the current learning rate is a natural approach. In particular, the Steepest Descent uses the received gradient direction and an exact or inexact line search to obtain proper learning rates. Another important idea is to approximate second-order methods like Newton method (Nocedal & Wright, 2006) and Quasi-Newton Methods (Liu & Nocedal, 1989). Along this idea, for example, the Barzilai-Borweinin (BB) method (Barzilai & Borwein, 1988) in classical optimization and AdaGrad (Duchi et al., 2011) in online learning have been proposed.
In this paper we propose a novel framework to learn the learning rate that we call HyperRegularization. More specifically, we regard the learning rate as a hyperparameter and cast its adaptive choice with the parameter training into a joint process. We formulate this process as a maxmin framework by imposing a regularizer on the hyperparameter. Furthermore, we demonstrate that AdaGrad and WNGrad (Wu et al., 2018) can be derived using a streamlined scheme based on Hyper-Regularization.
In addition to solving the saddle point problem exactly, we also provide an alternating strategy to solve the problem approximately. We respectively give theoretical analysis for these two updating rules in full batch setting and online learning setting. Specifically, our results of runtime bounds in full batch setting and regret bounds in online learning setting are comparable to the best known bound in corresponding settings and indicate our algorithms converge for any initial learning rate.
1.1 RELATED WORK
Steepest Descent uses the received gradient direction and an exact or inexact line search to obtain proper learning rates. Although Steepest Descent uses the direction that descends most and the best learning rate that gives the most function reduction, Steepest Descent may converge very slow for convex quadratic functions when the Hessian matrix is ill-conditioned (see, Yuan, 2008). In practice,
1

Under review as a conference paper at ICLR 2019

some line search conditions such as Goldstein conditions or Wolfe conditions (see, Fletcher, 2013) can be applied to compute the learning rate. In online or stochastic settings, one observes stochastic gradients rather than exact gradients and line search methods become less effective.
The Barzilai-Borwein method (Barzilai & Borwein, 1988) which was motivated by quasi-Newton methods presents a surprising result that it could lead to superlinear convergence in convex quadratic problem of two variables. Although numerical results often show the Barzilai-Borwein method converges superlinearly in solving nonlinear optimization problem, no superlinear convergence results have been established even for an n-dimensional strictly convex quadratic problem with the order n > 2 (Barzilai & Borwein, 1988; Dai, 2013). In minimizing the sum of cost functions and stochastic setting, SGD-BB proposed by Tan et al. (2016) takes the average of the stochastic gradients in one epoch as an estimation of the full gradient. But this approach can not directly be applied to online learning settings.
In online convex optimization (Zinkevich, 2003; Shalev-Shwartz et al., 2012; Hazan et al., 2016), AdaGrad (Duchi et al., 2011) adapts the learning rate on per parameter basis dynamically. Intuitively, AdaGrad constructs approximation to the Hessian with diagonal of accumulated outer products of gradients. This leads to many variants such as RMSProp (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), Adam (Kingma & Ba, 2015), etc.
Additionally, Cruz (2011) analyzed Adaptive Stochastic Gradient Descent (ASGD) which is a generalization of Kesten's accelerated stochastic approximation algorithm (Kesten et al., 1958) for the high-dimensional case. ASGD uses a monotone decreasing function with respect to a time variable to get learning rates. Recently, Baydin et al. (2018) proposed Hyper-Gradient Descent to learn the global learning rate in SGD, SGD with Nesterov momentum and Adam. Hyper-Gradient Descent can be viewed as an approximate line search method in the online learning setting and it uses the update rule for the previous step to optimize the leaning rate in the current step. However, HyperGradient Descent has no theoretical guarantee.
It is worth mentioning that Gupta et al. (2017) proposed a framework named Unified Adaptive Regularization from which AdaGrad and Online Newton Step (Hazan et al., 2007) can be derived. However, Unified Adaptive Regularization gives an approach for approximating the Hessian matrix in second order methods.

1.2 NOTATION
Before introducing our approach, we present the notation. We denote the set {x > 0 : x  R} by R++. For two vectors a, b  Rd, we use a/b to denote element-wise division, a  b for elementwise product (the symbol  will be omitted in the explicit context), an = (an1 , a2n, . . . , adn), and a  b if aj  bj for all j. Let 1 be the vector of ones with an appropriate size, and diag() be a diagonal matrix with the elements of the vector  on the main diagonal. In addition, we define a A = a, Aa .
Given a set X  Rd, a function f : X  R is said to satisfy f  CL1,1(X ) if f is continuously differentiable on X , and the derivative of f is Lipschitz continuous on X with constant L:
f (x) - f (y) 2  L x - y 2. More general definition can be found in Nesterov (2013).

1.3 PROBLEM STATEMENT

For an online learning problem, a learner faces a sequence of convex functions {ft} with the same domain X  Rd, receives (sub)gradient information gt  ft(xt) at each step t, and predicts a point xt+1  X .

Our theoretical analysis is based on two settings: full batch setting and online learning setting. A

full batch setting (or optimization setting) is to optimize a certain function F with exact gradient at each step, i.e., ft = F . In this setting we assume F  CL1,1 but it is not necessarily convex. We analyze the convergence of our algorithms by capping the runtime T such that the minimum value

of the norm of received gradients so far is less than a given error accuracy , that is,

min
t=0:T -1

F (xt)

2 2

 .

2

Under review as a conference paper at ICLR 2019

In online learning settings, our analysis follows from Duchi et al. (2011) and Kingma & Ba (2015). We only assume that ft's are convex and try to give an upper bound for the regret

T -1

T -1

R(T ) = ft(xt) - min ft(x).
xX t=0 t=0

(1)

2 HYPER-REGULARIZATION

Following the setting in AdaGrad (Duchi et al., 2011), we consider a generalization of the standard (sub)gradient descent as

xt+1

=

diag(t )1/2
X

xt - diag(t)-1/2gt

2

= arg min xt - diag(t)-1/2gt

. (2)

xX

diag(t )1/2

This procedure can be viewed as the minimization problem:

min
xX

gt, x - xt

1 +
2

x - xt

2 diag(t

)

.

(3)

To derive our hyper-regularization approach, we then formulate this minimization problem as a

saddle point problem by adding a hyper-regularizer about the difference between the new learning

rate  and an auxiliary vector t. Accordingly, we have

max min t(x, )
Bt xX

gt, x - xt

1 +
2

x - xt

2 diag(t )

-

D(,

t)

,

(4)

where D(, ) is defined as our hyper-regularizer and Bt is a subset in Rd. We solve the saddle point problem for new predictor and new learning rate. The following subsections will explain some
details about our framework.

2.1 THE -DIVERGENCE

It is reasonable to choose a distance function to measure the difference between  and . In this paper, we use the -divergence as our hyper-regularizer.

Definition 1 (-divergence). Let : R++  R be a differentiable strongly convex function in R++
such that (1) =  (1) = 0, where  is the derivative function of . For such a function , the function D: R+d + × Rd++  R, which is define by

D(u, v)

d
vj (uj /vj ),
j=1

is referred to as the -divergence.

Remark. Note that convex function  with (1) =  (1) = 0 satisfies (z)  0 for all z > 0, thus D(u, v)  0 for all u, v  R+d +, with equality only if u = v.
Remark. For any convex function f , (z) = f (z) - f (1)(z - 1) - f (1) is a proper function for our -divergence.

Remark. In our framework, in order to solve the problem (4) feasibly, we always assume that limz+  (z) = +.

If one only requires  to be convex and (1) = 0, the resulting distance function D is called a f -divergence (Liese & Vajda, 1987; 2006). The f -divergence has been widely applied in statistical
machine learning (e.g., Nguyen et al., 2009).

Using the -divergence as our hyper-regularizer, we can rewrite the problem (4) as

max min t(x, )
Bt xX

gt

(x

-

xt)

+

1 2

x - xt

2 diag()

-

1 2 D(, t),

or

max min t(x, )
Bt xX

d

gt,j (xj

-

xt,j )

+

1 2

j (xj - xt,j )2 - t,j (j /t,j )

.

j=1

(5) (6)

The form of problem (6) implies that we can solve the problem for each dimension separately, and only a few extra calculations are required for each step.

3

Under review as a conference paper at ICLR 2019

2.2 MAX-MIN OR MIN-MAX

Consider a problem similar to (5),

min max t(x, ).
xX Bt

(7)

The solution of problem (5) is same as (7) in unconstrained case, i.e. X = Bt = Rd. However, if we set Bt = [bt,1, Bt,1] × · · · × [bt,d, Bt,d] to constrain the range of t+1 and suppose X = Rd, the solution of (5) can be easily obtained by clipping the solution of unconstrained problem to the
required range, while the solution of (7) is more difficult to get. Thus, we choose (5) as our basic
problem.
Lemma 2. Suppose that Bt = [bt,1, Bt,1] × · · · × [bt,d, Bt,d], and X = Rd. Let  be the solution of unconstrained problem max(minx t(x, )). Then the solution of problem maxBt (minx t(x, )) is
j = min{max{j, bt,j}, Bt,j}, for j = 1, · · · , d.

2.3 SELECTION OF t
There are many ways to choose the sequence {t}:
· The sequence {t} is chosen in advance, before our methods start the job. For example, set 1) t =  1; 2) t =  t + 1 1, where  > 0 is a prespecified constant.
· t can be obtained from other adaptive learning rate methods such as AdaGrad. · Set t = t. In this case, the hyper-regularizer is the penalty for the change between t+1
and t.
The first two ways can be treated as a smoothing technique to stabilize the learning rate. We want to ensure the learning rate sequence {t} close to another learning rate sequence {t}. Although setting t = t is our main focus, we keep the sequence of {t} to maintain this flexibility.

2.4 ISOTROPIC HYPER-REGULARIZATION

We now show a special form of our framework that only adaptively maintains a single scalar learning

rate. We modify Hyper-regularization so that  is optimized over the set Bt  {1 :   R++} of all positive multiples of the vector 1  Rd. Let t = t1, and rewrite problem (5) as



max
1Bt

min t(x, )
xX

=

gt

(x

-

xt)

+

2

x - xt

2 2

-

1 2 t(/t).

(8)

We prefer to use Isotropic Hyper-Regularization as smoothing technique in online learning settings.

3 UPDATE RULES AND ALGORITHMS

In this section we present two update rules of our hyper-regularization framework. The first update rule is solving the saddle point problem (5) exactly. That is,

t(xt+1, t+1) = max min t(x, ).
Bt xX

(9)

The following lemma and Algorithm 1 give the concrete scheme of our iterations.

Lemma 3. Considering problem (6) without constraints and solving the problem exactly, we get

new predictor xt+1 and new learning rate t+1 such that

t2+1,j  (t+1,j /t,j ) = gt2,j , j = 1, . . . , d,

(10)

xt+1 = xt - gt/t+1.

Remark. Note that AdaGrad (Duchi et al., 2011) and WNGrad (Wu et al., 2018) are special cases of Algorithm 1 with a particular choice of  (detailed derivation in Appendix B).

·

If (z) = z +

1 z

- 2, then we can derive AdaGrad from Algorithm 1.

·

If (z) =

1 z

-

log(

1 z

)

-

1,

then

we

can

derive

WNGrad

from

Algorithm

1.

4

Under review as a conference paper at ICLR 2019

Algorithm 1 GD with Hyper-regularization
Input: 0 > 0, x0 1: for t = 1 to T do 2: Suffer loss ft(xt); 3: Receive subgradient gt  ft(xt) of ft at xt; 4: Update t+1,j as the solution of the equation 2 (/t,j) = gt2,j, j = 1, . . . , d; 5: Update xt+1 = xt - gt/t+1; 6: end for

3.1 ALTERNATING UPDATE RULE

However, it is sometimes difficult to solve the equation (10), especially in the case where  is a quadratic function (equation (10) will be a cubic equation in one variable for any j). In practice, using an alternating update strategy is more recommended. Under the assumption that the optimal value of  is close to t, we solve an approximate equation for t+1

t+1 = arg max t arg min t(x, t),  ,

Bt

xX

(11)

and update the new predictor xt+1 using

xt+1 = arg min t(x, t+1).
xX

Applying the alternating update rule under the same assumption in Lemma 3, we obtain the following lemma and Algorithm 2.

Lemma 4. Considering problem (6) without constraint and following from the alternating update

rule, we get new predictor xt+1 and new learning rate t+1 as

t+1,j = t,j ( )-1(gt2,j /t2,j ), j = 1, . . . , d,

(12)

xt+1 = xt - gt/t+1.

Algorithm 2 GD with Hyper-regularization using alternating update rule
Input: 0 > 0, x0 1: for t = 1 to T do 2: Suffer loss ft(xt); 3: Receive gt  ft(xt) of ft at xt; 4: Update t+1,j = t,j ( )-1(gt2,j /t2,j ), j = 1, . . . , d; 5: Update xt+1 = xt - gt/t+1; 6: end for

Computing the inverse function of  is usually easier than solving the equation (10) in practice, especially for the widely used -divergences (more details can be found in Appendix D.1). Remark. If t = t, the following simplified alternating rule can be employed:
xt+1 = arg min t(x, t),
xX
t+1 = arg max t (xt+1, ) .
Bt
We leave the corresponding algorithm 3 in Appendix C.
3.2 MONOTONICITY
Before giving more analysis, let us show monotonicity of both the two update rules. The monotonicity implies that only the property of convex function  on interval [1, +) will influence the efficient of our algorithms. Lemma 5. t+1 obtained from equation (9) or (11) satisfies t+1  t.
When setting t = t, we have that t+1  t.
5

Under review as a conference paper at ICLR 2019

4 THEORETICAL ANALYSIS
In this section we always set t = t and assume that x and  are unconstrained, i.e., X = Rd and Bt = R+d +. We first discuss the convergence rate of the two update rules in full batch setting with assumption that the objective function F is L-smooth but not necessarily convex in Section 4.1. Next we turn to online convex learning setting and establish a theorem about the regret bounds in Section 4.2. Our results for both the settings show that our algorithms are robust to the choice of initial learning rates and do not rely on the Lipschitz constant or smoothness constant.

4.1 ISOTROPIC HYPER-REGULARIZATION IN FULL BATCH SETTING

Recall that we set ft = F in the full batch setting, and assume that F  CL1,1 without convexity. In this case, two update rules can be written as

t2+1 (t+1/t) =

gt

2 2

,

xt+1

=

xt

-

1 t+1

gt.

(13)

t+1 = t( )-1(

gt

2 2

/t2),

xt+1

=

xt

-

1 t+1

gt

.

(14)

Next we show that both update rules (13) and (14) are robust to the choice of initial learning rate.

Theorem 6. Suppose that   Cl1,1 ([1, +)),  is -strongly convex, F  CL1,1(Rd), and F  = infx F (x) > -. For any   (0, 1), the sequence {xt} obtained from update rules (13) or (14) satisfies

min
j=0:T -1

F (xj)

2 2

 ,

after T = O

1 

steps.

More accurate bounds for runtime can be found in Theorems 21 and 22 in Appendix I. Theorem 6 shows that both runtime of the two update rules can be bound as O(1/) for any constant L and
initial learning rate 0. As a comparison, in classical convergence result ((1.2.13) in Nesterov (2013) or Theorem 20 in Appendix), the upper bound of runtime is O(1/) only for a certain range (related
to L) of initial learning rates.

4.2 HYPER-REGULARIZATION IN ONLINE LEARNING SETTING

We now establish the result of convergence rate for Algorithms 1 and 2 in online convex learning, i.e., the ft are convex. Especially, we try to bound regrets (1) by O( T ) for Algorithms 1 and 2.

Theorem 7. Suppose that  G, and xt - x   D.

 Cl1,1 ([1, +)), Then the sequence

and  is -strongly {xt} obtained from

convex. Assume that Algorithm 1 satisfies

gt



2R(T )  ( + D2 ) 2l02 + 4G2 d

0

j=1

g0:T -1,j

2 + 0

x0 - x

22,

and the sequence {xt} obtained from Algorithm 2 (or 3) satisfies

2R(T ) 

1 + D2 

 2G max 2l,
0

d
g0:T -1,j 2 + 0 x0 - x 22.
j=1

Note that under the assumption in Theorem 7,

d j=1

g0:T -1,j

2



 dG T , hence R(T )

=

O( T ). Our result is comparable to the best known bound for convex online learning problem

(see Hazan et al., 2016; Duchi et al., 2011; Kingma & Ba, 2015).

5 EXPERIMENTS
In this paper our principal focus has been to develop a new approach for adaptive choice of the learning rate in first-order gradient-type methods. However, this new approach also brings some insights into the resulting algorithms. Thus, it is interesting to conduct empirical analysis of the learning algorithms with different choice methods for the learning rate.

6

Under review as a conference paper at ICLR 2019

5.1 THE SET-UP

To derive a learning algorithm from the Hyper-Regularization framework, we have to first give the  divergences (Pardo, 2005). Specifically, the algorithms from our framework in the following experiments are derived from the following four  divergences (full implementations are displayed in the Appendix D.1):
· (t) = t log t - t + 1 from KL-devergence inducing KL algorithm. · (t) = - log t + t - 1 from Reverse KL-divergence inducing RKL algorithm.
 · (t) = ( t - 1)2 from Hellinger distance inducing Hellinger algorithm. · (t) = (t - 1)2 from 2 distance inducing 2 algorithm.

As mentioned in Section 3, even with a fixed  divergence, the generated algorithm still varies with different update rules. For simplicity, different update rules were compared in advance to select the specific one for any  divergence in the following experiments.

To maintain stable performance, the technique of growth clipping is applied to all algorithms in our framework. Actually, growth clipping fulfills the constraints placed on the increasing speed of t by Bt in Lemma 2. Specifically, t+1 in our experiments falls in [t, 2t]. Detailed observations on how the t of our algorithms increases are left in Appendix D.3.
Experiments involve the four algorithms generated above as well as other first-order gradient-based algorithms including SGD (with no learning rate decay), SGD-BB, and Hyper-Gradient Descent algorithms. These algorithms are evaluated on tasks of image classification with a logistic classifier on the databases of MNIST (LeCun et al., 2010) and CIFAR-10 (Krizhevsky & Hinton, 2009). Initial learning rate (in the usual sense, i.e., 1/0) varies from 10-3 to 101 for the test of convergence performance of these algorithms. Experiments are run using Tensorflow (Abadi et al., 2016), on a machine with Intel Xeon E5-2680 v4 CPU, 128 GB RAM, and NVIDIA Titan Xp GPU.

Train Loss

1.2 1.00

Test Accuracy

1.0 0.95

0.8

0.90 0.85

0.6 0.80

0.4 0.2

0.75 0.70 0.65

RRRKKKLLL132

Test Accuracy

10 3 0.10

10In2 itial Lea10rn1 ing Rate100

101 10 3 0.9850

10In2 itial Lea10rn1 ing Rate100

0.08

RKL1 (10 0.5) RKL2 (10 0.5)

0.9825 0.9800

0.06 RKL3 (10 0.5) 0.9775

0.9750

0.04 0.9725

0.02

0.9700 0.9675

0.00 10 20 Epoch30 40 50 0.9650 10 20 Epoch30 40

101 50

Train Loss

Figure 1: Convergence performances of RKL1, RKL2, and RKL3 on the database of MNIST in online learning setting, up: performances at different initial learning rates, and down: the whole
training process with the given initial learning rate in the bracket for each algorithm.

5.2 UPDATE RULE SELECTION
Taking the RKL algorithm as an example, we refer to algorithms deduced from Algorithm 1, 2, and 3 as RKL1, RKL2, and RKL3. We train a two-layer neural network with a hidden layer of 500 units on the MNIST database. Experiments are in online learning setting with a batch size of 128, and 2 regularization is applied with a coefficient of 10-4.
Figure 1 demonstrates the training loss and test accuracy of these algorithms with various initial learning rates. After fixing the learning rate with the least training loss, we compare their performances throughout the training process. Sharing comparable performances at small initial learning rate with RKL3, RKL1 and RKL2 perform better at relatively large learning rate.

7

Under review as a conference paper at ICLR 2019

Generally speaking, Algorithm 1 often suffers a higher computation complexity than Algorithm 2 for the difficulty of getting an analytical solution. Detailed observations are left in Appendix D.1. Therefore, we apply the second update rule to our algorithms without explicit notifications.

5.3 FULL BATCH SETTING AND ONLINE LEARNING SETTING

We investigate our algorithms in the full batch setting on the MNIST database where algorithms receive the exact gradients of the objective loss function each iteration. 1

Train Loss

2.5

2.0

1.5

RKL

1.0

Hellinger Phi

SGD

SGD-BB

0.5

Hyper-Gradient KL

10 3 10In2 itial Lea10rn1 ing Rate100

3.0

RKL (100.5)

2.5

Hellinger (100.75) Phi (100.25)

2.0

SGD (100) SGD-BB (100)

Hyper-Gradient (100.25)

1.5 KL (100.5)

Train Loss

1.0

0.5

101 0.0 10 20 Epoch30 40 50

Figure 2: Convergence performances of algorithms on MNIST in the full batch setting. left: the train loss of the last training epoch at different initial learning rates, right: the whole training process with the given initial learning rate in the bracket for each algorithm.

Train Loss

0.6 0.90

Test Accuracy

0.5 0.85 0.80

0.4 0.75 RKL

0.3

0.70

Hellinger
2

0.2 0.1

0.65 SGD-BB

0.60 0.55

Hyper-Gradient KL SGD

01.00 3 0.5

10In2 itial Lea10rn1 ing Rate100

101 0.5100 3 0.90

10In2 itial Lea10rn1 ing Rate100

RKL (10 1.25)

0.4

Hellinger (10 1.25)

0.85

2 (10 1.0)

0.3

SGD (10 1.25) SGD-BB (10 1.25)

0.80

0.2

Hyper-Gradient (10 0.25) KL (10 1.25)

0.75

0.1 0.70

Test Accuracy

0.0 20 40 Epoch60 80 100 0.65 20 40 Epoch60 80

101 100

Train Loss

Figure 3: Convergence performances of algorithms on CIFAR-10 in the online learning setting, left: the train loss of the last training epoch at different initial learning rates, right: the whole training process with the given initial learning rate in the bracket for each algorithm.
In terms of online learning setting, we train a VGG Net (Simonyan & Zisserman, 2014) with batch normalization on the CIFAR-10 database with a batch size of 128, and an 2 regularization coefficient of 10-4. We as well perform data augmentation as He et al. (2016) to improve the training.
Figure 2 and Figure 3 show the convergence performances on both settings, respectively. Achieving general comparable performances with other first-order gradient-based algorithms, our algorithms outperform most of other algorithms at risky large initial learning rate. Even at the fixed initial learning rate with the least training loss, the performances of our algorithms still achieve the same training performances with others.

6 DISCUSSION
Our framework generates efficient algorithms for optimization problems with regularization terms shown above. We describe another regularization term in Appendix H with a O(log(T )) regret in strongly convex conditions, to inspire new ideas of more efficient terms in the future.
1Since it is a pure optimization problem, testing performance is out of our main consideration.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Mart´in Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for largescale machine learning. In OSDI, volume 16, pp. 265­283, 2016.
Jonathan Barzilai and Jonathan M Borwein. Two-point step size gradient methods. IMA journal of numerical analysis, 8(1):141­148, 1988.
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hypergradient descent. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=BkrsAzWAb.
Se´bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning, 8(3-4):231­357, 2015.
Pedro Cruz. Almost sure convergence and asymptotical normality of a generalization of kesten's stochastic approximation algorithm for multidimensional case. arXiv preprint arXiv:1105.5231, 2011.
Yu-Hong Dai. A new analysis on the barzilai-borwein gradient method. Journal of the operations Research Society of China, 1(2):187­198, 2013.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Roger Fletcher. Practical methods of optimization. John Wiley & Sons, 2013.
Vineet Gupta, Tomer Koren, and Yoram Singer. A unified approach to adaptive regularization in online and stochastic optimization. arXiv preprint arXiv:1706.06569, 2017.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169­192, 2007.
Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends R in Optimization, 2(3-4):157­325, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Harry Kesten et al. Accelerated stochastic approximation. The Annals of Mathematical Statistics, 29(1):41­59, 1958.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2, 2010.
F Liese and I Vajda. Convex statistical distances. 1987.
Friedrich Liese and Igor Vajda. On divergences and informations in statistics and information theory. IEEE Transactions on Information Theory, 52(10):4394­4412, 2006.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1-3):503­528, 1989.
Mahesh Chandra Mukkamala and Matthias Hein. Variants of RMSProp and Adagrad with logarithmic regret bounds. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 2545­2553, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/mukkamala17a.html.
9

Under review as a conference paper at ICLR 2019
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.
XuanLong Nguyen, Martin J Wainwright, Michael I Jordan, et al. On surrogate loss functions and f-divergences. The Annals of Statistics, 37(2):876­904, 2009.
Jorge Nocedal and Stephen J Wright. Numerical optimization 2nd, 2006. Leandro Pardo. Statistical inference based on divergence measures. Chapman and Hall/CRC, 2005. Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and
Trends R in Machine Learning, 4(2):107­194, 2012. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014. Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian. Barzilai-borwein step size for stochastic
gradient descent. In Advances in Neural Information Processing Systems, pp. 685­693, 2016. Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26­ 31, 2012. X. Wu, R. Ward, and L. Bottou. WNGrad: Learn the Learning Rate in Gradient Descent. ArXiv e-prints, March 2018. Ya-xiang Yuan. Step-sizes for the gradient method. AMS IP Studies in Advanced Mathematics, 42 (2):785, 2008. M. D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. ArXiv e-prints, December 2012. Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 928­936, 2003.
10

Under review as a conference paper at ICLR 2019

A SOLUTION EXISTENCE
Note that the function h() = 2 (/t,j) is an increasing continuous function and limz+  (z) = +  (1) = 0, so [0, +) is a subset of the range of h() and the solution of (10) exists. For the same reason, the solution of (12) exists.

B SPECIAL CASES OF ALGORITHM 1

In this section, We will point out that Adagrad (Duchi et al., 2011) and WNGrad (Wu et al., 2018)

are special cases of Algorithm 1.

If

we

set

(z)

=

z

+

1 z

-

2,

then

the

new

learning

rate

t+1

can

be

obtained

by

t2+1,j

1

-

t2,j t2+1,j

= gt2,j, j = 1, · · · , d,

that implies,

t2+1 = t2 + gt2,

and we drive AdaGrad from Hyper-Regularization.

Similarly,

we

can

get

WNGrad

by

setting

(z)

=

1 z

-

log

1 z

-

1.

In

fact,

t+1

employs

update

t2+1,j

t,j (t+1,j - t,j ) t2+1,j

= gt2,j, j = 1, · · · , d,

on the other words, i.e., the update rule of WNGrad.

t+1

=

t

+

gt2 , t

C SIMPLIFIED ALTERNATING UPDATE RULE

Algorithm 3 GD with Hyper-regularization using simplified alternating update rule and t = t
Input: 0 > 0, x0 1: for t = 1 to T do
2: Suffer loss ft(xt); 3: Receive gt  ft(xt) of ft at xt; 4: Update xt+1 = xt - gt/t; 5: Update t+1,j = t,j ( )-1(gt2,j /t2,j ), j = 1, . . . , d; 6: end for

The weakness of the simplified alternating update rule is that we get the new predictor xt+1 by t which is unrelated to current gradient gt. The regret bound of Algorithm 3 has been shown in Theorem 7.
D SUPPLEMENTARY FOR NUMERICAL EXPERIMENTS
Full versions of Hellinger, KL, RKL, and 2 algorithms through Algorithm 1, 2, and 3 are listed below. Apparently, it is of great complexity to compute an analytical solution for t+1 for the equations marked red.
11

Under review as a conference paper at ICLR 2019

D.1 FULL VERSIONS OF GENERATED ALGORITHMS

K L1 K L2 K L3

t2+1 log(t+1/t) = gt2 xt+1 = xt - gt/t+1
t+1 = t exp(gt2/t2) xt+1 = xt - gt/t+1
xt+1 = xt - gt/t t+1 = t exp(gt2/t2)

H1

t2+1(1 - t/t+1) = gt2 xt+1 = xt - gt/t+1

H2

t+1 = t5/(t2 - gt2)2 xt+1 = xt - gt/t+1

H3

xt+1 = xt - gt/t t+1 = t5/(t2 - gt2)2

RK L1 RK L2 RK L3

t+1

=

1 2

t +

t2 + 4gt2

xt+1 = xt - gt/t+1

t+1 = t3/(t2 - gt2) xt+1 = xt - gt/t+1

xt+1 = xt - gt/t t+1 = t3/(t2 - gt2)

12

2t2+1(t+1/t - 1) = gt2 xt+1 = xt - gt/t+1

22

t+1 = t 1 + gt2/(2t2) xt+1 = xt - gt/t+1

23

xt+1 = xt - gt/t t+1 = t 1 + gt2/(2t2)

D.2 VERSION COMPARISON ON OTHER ALGORITHMS
Figure 4, 5 and 6 shows the results of comparison between versions of KL, Hellinger, and 2 algorithms on the base of MNIST in the online learning setting.

Train Loss

0.5 1.0

Test Accuracy

0.4 0.9

0.3 0.8 0.7

0.2 0.6

0.1

0.5 0.4

KKLL23

10 3
0.08 0.07 0.06 0.05

10In2 itial Lea10rn1 ing Rate100

101

KL2 (10 0.5) KL3 (10 0.5)

10 3
0.984 0.982 0.980

10In2 itial Lea10rn1 ing Rate100

0.04 0.978

0.03 0.976

0.02 0.974

Test Accuracy

0.01 0.972

0.00 10 20 Epoch30 40 50 0.970 10 20 Epoch30 40

101 50

Train Loss

Figure 4: Convergence performances of KL2 and KL3 on the database of MNIST in online learning setting, up:at different initial learning rates, and down:the whole training process with the given
initial learning rate in the bracket.

12

Under review as a conference paper at ICLR 2019

Train Loss

0.5 1.000

Test Accuracy

0.4

0.975 0.950

0.3 0.925 0.900
0.2 0.875

0.1

0.850 0.825

HH23

10 3
0.08 0.07 0.06 0.05

10In2 itial Lea10rn1 ing Rate100

101

H2 (10 0.5) H3 (10 0.5)

0.80100 3
0.984 0.982 0.980

10In2 itial Lea10rn1 ing Rate100

0.04 0.978

0.03 0.976

Test Accuracy

0.02 0.974

0.01 0.972

0.00 10 20 Epoch30 40 50 0.970 10 20 Epoch30 40

101 50

Train Loss

Figure 5: Convergence performances of H2 and H3 on the database of MNIST in online learning setting, up:at different initial learning rates, and down:the whole training process with the given
initial learning rate in the bracket.

Train Loss

0.5 1.0

Test Accuracy

0.4 0.9 0.8
0.3 0.7

0.2 0.6

0.1

0.5 22 0.4 32

10 3
0.08 0.07 0.06 0.05

10In2 itial Lea10rn1 ing Rate100

101

22 (10 0.25) 32 (10 0.5)

01.30 3
0.984 0.982 0.980

10In2 itial Lea10rn1 ing Rate100

0.04 0.978

Test Accuracy

0.03 0.976

0.02 0.974

0.01 0.972

0.00 10 20 Epoch30 40 50 0.970 10 20 Epoch30 40

101 50

Train Loss

Figure 6: Convergence performances of 22 and 23 on the database of MNIST in online learning setting, up:at different initial learning rates, and down:the whole training process with the given
initial learning rate in the bracket.

D.3 FIGURES ON t INCREASING PROCESS
All the following experiments are based on the MNIST database in online learning setting with a batch size of 128. For simplicity, all initial 0s are fixed to be 10-0.5. Considering the sparse feature of parameters in this task, figure 7 only observe the changing process of the maximum of t.
13

Under review as a conference paper at ICLR 2019

Maximum t

3.40

3.35

3.30

3.25 3.20

RRRKKKLLL321

0 5000 T10r0a0i0n Ste1p5000 20000
3.6

3.5

3.4

3.3 3.2

HH23

0 5000 T10r0a0i0n Ste1p5000 20000

Maximum t

Maximum t

3.40

3.35

3.30

3.25
3.20 KKLL32
3.150 5000 T10r0a0i0n Ste1p5000 20000

3.26

3.24

3.22

3.20 3.18 3.160

22 32
5000 T10r0a0i0n Ste1p5000 20000

Maximum t

Figure 7: The values of the maximum t at each training step for various algorithms.

E PROOF OF LEMMA 2

Proof. First, it is trivial to get

t,x()

minxt(x, ) = t

xt

-

gt 

,



=

-1 2

gt

2 diag()-1

-

1 2 D(, t)

= -1 d 2
j=1

gt2,j j

+ t,j 

j t,j

.

The partial derivative of t,x() with respect to j is

t,x() = 1 j 2

gt2,j j2

-

j t,j

.

Note that  is a convex function, so 

is a non-decreasing function, and

 t,x () j

is a non-increasing

function. Recall that  be the solution of unconstrained problem max(minx t(x, )), hence,

j

is

a

zero

of

function

. t,x ()
j

Moreover, if j

>

Bt,j ,

we

have

 t,x () j

 0.

Thus, t,x() with respect to j is a non-

decreasing function, and arg maxj t,x() = Bt,j. For a similar reason, if j < bt,j, then

arg maxj t,x() = bt,j. In conclusion,

arg max t,x() = min{max{j, bt,j}, Bt,j}, for j = 1, · · · , d.
j [bt,j ,Bt,j ]

F PROOF OF LEMMA 5
In this section, we denote that t,x() = minxX t(x, ). Lemma 8. t+1 obtained from equation (9) satisfies t+1  t.
14

Under review as a conference paper at ICLR 2019

Proof. Recall that (1) =  (1) = 0, so (x)  0 for all x and D(, t) = t(/t)  0. If  < t, then for all x  X

 t(x, ) = gt (x - xt) + 2

x - xt

2 2

-

1 2 D(,

t)

<

gt

(x -

xt) +

t 2

x - xt

2 2

= t(x, t).

Hence, minxX t(x, ) < minxX t(x, t), i.e., t,x() < t,x(t). It means t+1 = arg maxB t,x()  t.

Lemma 9. t+1 obtained from equation (11) satisfies t+1  t.

Proof. Let y = arg minx (x, t). If  < t, then

t(y, )

=

gt

(y

-

xt)

+

 2

y - xt

2 2

-

1 2 D(, t)

<

gt

(y

-

xt)

+

t 2

y - xt

2 2

= t(y, t).

Hence t+1 = arg maxB t(y, )  t.

G CONVERGENCE RATES IN ONLINE LEARNING SETTING

Recall the define of regret

T -1
R(T ) = (ft(xt) - ft(x)),
t=0

(15)

where x = arg minxX

T -1 t=0

ft(x).

We

show

our

hyper-regularizer

method

with

three

update

rules (Algorithm 1, 2 and 3) have O( T ) regret bounds.

Lemma 10 (Lemma 4 in Adagrad). Consider an arbitrary real-valued sequence {ai} and its vector representation a1:i = (a1, · · · , ai) . Then

T t=1

at2 a1:t

 2 a1:T
2

2

(16)

holds.

Proof. Let us use induction on T to prove inequality (10). For T = 1, the inequality trivially holds. Assume the bound (16) holds true for T - 1, in which case

T t=1

at2 a1:t

2

2

a1:T -1

2+

aT2 . a1:T 2

We denote bT =

T t=1

at2

and

have

2 a1:T -1 2 +

aT2 = 2 a1:T 2

bT

-

aT2

+

aT2 bT

2

bT

- a2T

+

a4T 4bT

+ a2T bT

= 2 bT .

15

Under review as a conference paper at ICLR 2019

Lemma 11. Suppose the sequence {xt} and sequence {t} satisfy xt+1 = xt - gt/t+1. Then the regret satisfies

T -1
2R(T ) 
t=0

T -1

gt

+2
diag(t+1 )-1

t=0

xt - x

+2
diag(t+1 -t )

x0 - x

2 diag(0 )

Proof. Note that

xt+1 = xt - diag(t+1)-1gt,

and

xt+1 - x

2 diag(t+1 )

=

xt - x - diag(t+1)-1gt

2 diag(t+1 )

=

xt - x

2 diag(t+1 )

+

gt

2 diag(t+1 )-1

-

2gt

(xt - x),

i.e.,

2gt (xt - x) =

gt

+2
diag(t+1 )-1

xt - x

2 diag(t+1 )

-

xt+1 - x

2 diag(t+1 )

.

(17)

Hence

T -1

2R(T ) = 2 (ft(xt) - ft(x))

t=0

T -1
 2 gt (xt - x)

t=0

T -1

T -1

=

gt

+2
diag(t+1 )-1

t=0 t=0

xt - x

2 diag(t+1 )

-

xt+1 - x

2 diag(t+1 )

T -1

t=0

T -1

gt

+2
diag(t+1 )-1

t=0

xt - x

+2
diag(t+1 -t )

x0 - x

2 diag(0

)

.

Lemma 12. Suppose an increasing function  satisfies (1) = 0 and (x)  l(x - 1). Consider

a real valued sequence {gt}t=0:T -1 and a positive sequence {t}t=0:T which satisfies |gt|  G,

t2+1

t+1 t

= gt2, t = 0, · · · , T - 1, 0  0. We can bound T as

where c =

t  c

02

+

2 l

t-1

gi2,

t

=

1, · · ·

,T

i=0

.02
02 +2G2 /l

Moreover,

we

have

(18)

T -1 gt2  t=0 t+1 Remark. We point out that

2l02 + 4G2 0

T -1
gt2.
t=0

(19)

· t+1  t (If t+1 < t, then t2+1(t+1/t) < 0  gt2),
· t+1 is unique with respect to t due to the fact that the function ^() = 2(/t) is strictly increasing.

16

Under review as a conference paper at ICLR 2019

Proof. Assume that t  c

02

+

2 l

t-1 i=0

gi2,

where

c

>

0

is

a

variable

coefficient.

Let us find out a specific c such that t+1  c

02

+

2 l

t i=0

gi2.

Note that

gt2 = t2+1

t+1 t

 lt2+1

t+1 - 1 . t

(20)

Define a cubic polynomial

h()

=

l 3 t

-

l2

- gt2,

and h is an increasing function when   t.

If h

c

02

+

2 l

t i=0

gi2

 0, according to h(t+1)  0, then t+1  c

02

+

2 l

t i=0

gi2.

Denote

b

=

02

+

2 l

t-1 i=0

gi2.

So

we

just

need

to

choose

c

such

that



h c

02

+

2 l

t
gi2  lc2(b + 2gt2/l)

i=0

b +2gt2/l - 1 b

- gt2  0,

where the first inequality holds for the assumption t  c

02

+

2 l

t-1 i=0

gi2,

or

c2 b

(b

+

2gt2

/l)

b

2gt2/l + 2gt2/l

 +b



gt2/l,

or

2c2 b

(b

+

2gt2/l)



 b + 2gt2/l + b.

Thus, c just need to satisfy

c2



b+

b 2gt2/l .

According to b  02, gt2  G2, hence

b b + 2gt2/l



02

02 + 2G2

/l

.

So if we choose c =

,02
02 +2G2 /l

then

1

>

0

>

c0,

hence

t  c

02

+

2 l

t-1

gi2, t

=

1,

··

·

,

T.

i=0

Moreover, following from Lemma 10, we have

T -1

gt2

T -1


t=0 t+1

t=0 c

gt2 2/l



 2l

t i=0

gi2

c

T -1
gt2.
t=0

Lemma 13. Suppose an increasing function  satisfies (1) = 0 and (x)  l(x - 1). Consider

a real valued sequence {gt}t=0:T -1 and a positive sequence {t}t=0:T which satisfies |gt|  G,

t2

t+1 t

= gt2, t = 0, · · · , T - 1, 0  0. We can bound T as

Moreover, we have

t 

02

+

2 l

t-1

gi2,

t

=

1, ·

·

·

,

T.

i=0

(21)

T -1 gt2  max

 2G 2l,

t=0 t

0

T -1
gt2.
t=0

(22)

17

Under review as a conference paper at ICLR 2019

Proof. Same as inequality (20), we have

lt2

t+1 - 1 t

 gt2,

hence

t2+1 =

t

+

gt2 lt

2



t2

+

2 l

gt2



02

+

2 l

t

gt2  min

i=0

Furthermore, following from Lemma 10, we have

1,

l02 2G2

2 l

t+1

gi2, .

i=0

T -1 gt2  t=0 t

l/2 T -1 min{1, l02/(2G2)} t=0

gt2

 max

 2G 2l,

t i=0

gi2

0

T -1
gt2.
t=0

Theorem 14. G, and xt -

Suppose x  

that   Cl1,1 ([1, +)), and  is -strongly D. Then the sequence {xt} obtained from

convex. Assume that Algorithm 1 satisfies

gt



2R(T )  ( + D2 ) 2l02 + 4G2 d

0

j=1

g0:T -1,j

2 + 0

x0 - x

22,

Proof. Following from Lemma 11,

T -1
2R(T ) 
t=0

T -1

gt

+2
diag(t+1 )-1

t=0

xt - x

+2
diag(t+1 -t )

x0 - x

2 diag(0 )

T -1

T -1



gt

+2
diag(t+1 )-1

xt - x

2 

t+1 - t

1+

x0 - x

2 diag(0 )

t=0 t=0

T -1 d


gt2,j + max

t=0 j=1 t+1,j 0t<T

T -1 d

xt - x

2 

(t+1,j - t,j ) +

t=0 j=1

x0 - x

2 diag(0

)

.

Recall  is a -strongly convex function, so,

gt2,j = t2+1,j 

t+1,j t,j

 t+1,j t,j

t+1,j - 1 , t,j

and

T -1
(t+1,j
t=0

- t,j )



1 

T -1 t=0

gt2,j . t+1,j

(23)

The function  =  satisfies (1) = 0 and (x)  l(x - 1) according to the smoothness of .

Following from Lemma 12, we have

T -1 gt2,j  t=0 t+1,j

2l02,j + 4G2 0,j

T -1
gt2,j =
i=0

2l02,j + 4G2

0,j

g0:T -1,j 2.

Combining inequality (23) and (24), we have

2R(T ) 

1 + max0t<T

xt - x

2 



d T -1 gt2,j + j=1 t=0 t+1,j

x0 - x

2 diag(0 )

(24)

 (1 + D2 ) d 
j=1

2l02,j + 4G2 0,j

g0:T -1,j

2+

x0 - x

2 diag(0 )

= ( + D2 ) 2l02 + 4G2 d

0

j=1

g0:T -1,j

2 + 0

x0 - x

2 2

.

18

Under review as a conference paper at ICLR 2019

Theorem 15. G, and xt -

Suppose x  

that   Cl1,1 ([1, +)), and  is -strongly D. Then the sequence {xt} obtained from

convex. Assume Algorithm 2 (or

that gt  3) satisfies



2R(T ) 

1 + D2 

 2G max 2l,
0

d
g0:T -1,j 2 + 0 x0 - x 22.
j=1

Proof. Similar to the proof of Theorem 14, for Algorithm 2, we have

T -1 d
2R(T ) 

gt2,j + max

t=0 j=1 t+1,j 0t<T

T -1 d

xt - x

2 

(t+1,j - t,j ) +

t=0 j=1

x0 - x

2 diag(0 )

T -1 d


gt2,j + max

t=0 j=1 t,j 0t<T

T -1 d

xt - x

2 

(t+1,j - t,j ) +

t=0 j=1

x0 - x

2 diag(0

)

.

With same reason, for Algorithm 3, we have

T -1

T -1

2R(T ) 

gt

2 diag(t )-1

+

t=0 t=0

xt - x

2 diag(t )

-

xt+1 - x

2 diag(t )

T -1

t=0

T -1

gt

2 diag(t )-1

+

t=1

xt - x

+2
diag(t -t-1 )

x0 - x

2 diag(0 )

T -1 d


gt2,j + max

t=0 j=1 t,j 0t<T

T -1 d

xt - x

2 

(t,j - t-1,j ) +

t=1 j=1

x0 - x

2 diag(0

)

,

Note that for both algorithms

holds, so

gt2,j = t2,j 

t+1,j t,j

 t2,j

t+1,j - 1 , t,j

T -1
(t,j
t=1

-

t-1,j )



T -1
(t+1,j
t=0

- t,j )



1 

T -1 t=0

gt2,j , t,j

Thus, following from Lemma 13, for both Algorithm 2 and 3, we have

2R(T )  (1 + D2 ) d T -1 gt2,j  j=1 t=0 t,j

+ 0

x0 - x

2 2

 (1 + D2 ) d max

 2G 2l,


j=1

0

g0:T -1,j 2 + 0 x0 - x 22.

H LOGARITHMIC BOUNDS

In this section, we will use a different class of `distance' function for problem 4, and establish logarithmic regret bounds under assumption ft is strongly convex. Our analysis and proof follow from Hazan et al. (2007); Mukkamala & Hein (2017).

First, we define µ-strongly convexity.

Definition 16 (Definition 2.1 in (Mukkamala & Hein, 2017)). Let X  Rd be a convex set. We say that a function f : X  R is µ-strongly convex, if there exists µ  Rd with µj > 0 for j = 1, · · · , d
such that for all x, y  X ,

f (y)  f (x) +

f (x), y - x

1 +
2

y-x

d2iag(µ).

Let  = minj=1:d µj, then this function is -strongly convex (in the usual sense), that is

f (y)  f (x) +

f (x), y - x

 +
2

y-x

22.

19

Under review as a conference paper at ICLR 2019

The modification SC-Hyper-Regularization of Hyper-Regularization which we propose in the following uses a family of distance function D : R+d + × R+d +  R formulated as

d
D(u, v) = (uj/vj),
j=1

(25)

where  is convex function with (1) =  (1) = 0 like we used in -divergence. Remark. Same as -divergence, D(u, v)  0 for any u, v  R+d +.

Different from Algorithm 1 and 2, we add a hyper-parameter  > 0 like AdaGrad to SC-HyperGradient. Rewrite problem (5) as

max min t(x, )
Bt xX

gt

(x - xt)

+

1 2

x - xt

2 diag()

-

 2

d

(j /t,j ),

j=1

and corresponding two algorithms as

(26)

Algorithm 4 GD with SC-Hyper-regularization
Input: 0 > 0, x0 1: for t = 1 to T do 2: Suffer loss ft(xt); 3: Receive gt  ft(xt) of ft at xt; 4: Update t+1,j as the solution of the equation (2/t,j) (/t,j) = gt2,j, j = 1, · · · , d; 5: Update xt+1 = xt - gt/t+1; 6: end for

Algorithm 5 GD with SC-Hyper-regularization using alternating update rule
Input: 0 > 0, x0 1: for t = 1 to T do 2: Suffer loss ft(xt); 3: Receive gt  ft(xt) of ft at xt; 4: Update t+1,j = t,j ( )-1(gt2,j /(t,j )), j = 1, . . . , d; 5: Update xt+1 = xt - gt/t+1; 6: end for

Remark. Same as Lemma 5, the monotonicity of Algorithm 4 and 5 also holds.
Theorem 17. Suppose that ft is µ-strongly convex for all t,   Cl1,1 ([1, +)), and  is strongly convex. Assume that gt   G, and   G2/( minj=1:d µj). Then the sequence {xt} obtained from Algorithm 4 satisfies

G2 2R(T )  l 1 +

2d
ln

1+

g0:T -1,j

2 2

l0 j=1

l0

+ 0 x0 - x 22,

and the sequence {xt} obtained from Algorithm 5 satisfies

d
2R(T )  l ln

1+

g0:T -1,j

2 2

j=1 l0

+ 0 x0 - x 22,

Remark. Under assumption in Theorem 17, we have

g0:T -1,j

2 2



G2T

,

so

R(T

)

=

O(ln(T

)).

To prove Theorem 17, we first prove following lemma.

Lemma 18. For an arbitrary real-valued sequence {ai} and a positive real number b,

T
t=1 b +

a2t

t i=1

a2i



ln

1+

T t=1

a2t

.

b

(27)

20

Under review as a conference paper at ICLR 2019

Proof. Let b0 = b, bt = b +

t i=1

a2i ,

t



1,

then

T
t=1 b +

a2t

t i=1

ai2

=

T t=1

bt - bt-1 bt

=

T t=1

bt 1 dx
bbt-1 t

T bt 1

bT 1



dx =

dx = ln 1 +

xt=1 bt-1

bx

T t=1

a2t

.

b

Like Lemma 12 and 13, similar lemma holds for Algorithm 4 and 5.

Lemma 19. Suppose an increasing function  satisfies (1) = 0 and (x)  l(x - 1). Consider a real valued sequence {gt}t=0:T -1 and a positive sequence {t}t=0:T which satisfies |gt|  G, 0 > 0. If (t2+1/t)(t+1/t) = gt2, t = 0, · · · , T - 1, then we have

t 

0 2 0 + G2/l

0

+

1 l

t-1

gi2

i=0

, t = 1, · · · , T

(28)

and

T -1 gt2  l t=0 t+1

0 + G2/l

2
ln

0

1+

T -1 t=0

gt2

l0

.

(29)

Meanwhile, if t(t+1/t) = gt2, t = 0, · · · , T - 1, then we have

t



0

+

1 l

t-1

gi2,

t

=

1, · · ·

,T

i=0

(30)

and

T -1 gt2  l ln 1 + t=0 t+1

T -1 t=0

gt2

l0

.

(31)

Proof. Using same methods in proof of Lemma 12 and 13, the conclusion can be deduced from Lemma 18 easily.

proof of Theorem 17. Like Lemma 11, in strongly convex case, we have

T -1
2R(T ) = 2 ft(xt) - ft(x)

t=0

T -1

T -1

 2 gt, xt - x -

xt - x

2 diag(µ)

t=0 t=0

T -1

T -1

=

gt

+2
diag(t+1 )-1

t=0 t=0

xt - x

2 diag(t+1 )

-

xt+1 - x

2 diag(t+1 )

T -1

-

xt - x

2 diag(µ)

t=0

T -1

t=0

T -1

gt

+2
diag(t+1 )-1

t=0

xt - x

2 diag(t+1 -t -µ)

+

x0 - x

2 diag(0

)

.

Note that in Algorithm 4,

t+1,j - t,j = t,j (t+1,j /t,j - 1)

 t,j  

t+1,j t,j

=

t2,j t2+1,j

gt2,j 



G2 

21

Under review as a conference paper at ICLR 2019

holds, and in Algorithm 5, same conclusion holds:

t+1,j - t,j = t,j (t+1,j /t,j - 1)

 t,j  

t+1,j t,j

=

gt2,j



G2 .

 

Hence, if   maxj=1:d

G2 µj

,

then

t+1 - t

 µ, and

T -1

xt - x

2 diag(t+1 -t -µ)



0.

t=0

On the other hand,

T -1 t=0

gt

2 diag(t+1 )-1

=

d T -1 j=1 t=0

gt2,j , t+1,j

following from Lemma 19, we have

T -1

gt

2 diag(t+1 )-1



l

t=0

G2 1+
l0

2d
ln
j=1

1+

g0:T -1,j

2 2

l0

in Algorithm 4,

T -1

d

gt

2 diag(t+1 )-1



l

ln

t=0 j=1

1+

g0:T -1,j

2 2

l0

in Algorithm 5.

I CONVERGENCE RATES IN FULL BATCH SETTING

In this section, we will discuss the convergence of our methods in full batch settings.

We first review a classical result on the convergence rate for gradient descent with fixed learning

rate.

Theorem 20. Suppose that F  CL1,1(Rd) and F  = infxF (x) > -. Consider gradient descent

with

constant

step

size,

xt+1

=

xt

-

F (xt b

)

.

If

b

>

L 2

,

then

min
0tT -1

F (xt)

2 2



after at most a number of steps

T

=

2b2(F (x0) - F ) (2b - L)

=

O

1 

Proof. Following from the fact that F is L-smooth, we have

F (xt+1) F (xt) + F (xt)

(xt+1

-

xt)

+

L 2

xt+1 - xt

2 2

=F (xt)

-

1 b

F (xt)

2 2

+

L 2b2

F (xt)

2 2

=F (xt)

-

1 b

1- L 2b

F (xt)

2 2

.

When

b

>

L 2

,

1

-

L 2b

>

0.

So

T -1

F (xt)

2 2



2b2 2b - L (F (x0)

- F (xT ))



2b2 2b -

L

(F

(x0

)

-

F ),

t=0

and

min
0tT -1

F (xt)

2 2



1 T

T -1

F (xt)

2 2



T

2b2 (2b -

L)

(F

(x0)

-

F

)



.

t=0

(32)

22

Under review as a conference paper at ICLR 2019

Remark.

If we choose b 

L 2

,

then

convergence

of

gradient

descent

with

constant

learning

rate

is

not guaranteed at all.

Like Algorithm 3, another update rule is worth considering in full batch setting:

xt+1

=

xt

-

1 t

gt

,

t+1 = t( )-1(

gt

2 2

/t2)

(33)

Next we will show that both update rules (13) and (14) are robust to the choice of initial learning rate. Our proof is followed from the proof of Theorem 2.3 in WNGrad (Wu et al., 2018). Note that in update rule (13), t+1 satisfies

t2+1 (t+1/t) =

gt

2 2

,

while in update rule (14) and (33), t+1 satisfies

t2 (t+1/t) =

gt

2 2

.

Theorem 21 (Convergence rate of update rule (13)). Suppose that   Cl1,1 ([1, +)),  is strongly convex, and F  CL1,1(Rd), F  = infx F (x) > -. For any   (0, 1), the sequence {xt} obtained from update rule (13) satisfies

min
j=0:T -1

F (xj)

2 2

 ,

after T steps, where


1 +  
T= 1 +  

2(0+2(F (x0)-F )/)(F (x0)-F ) 

if 0  L or 1  L,

log(

L 0

)

log(

 lL2

+1)

+

( )L+

1+

2 

F

(x0

)-F



+

lL(L-0 20

)



2

otherwise.

Theorem 22 (Convergence rate of update rule (14)). Suppose that   Cl1,1 ([1, +)),  is strongly convex, and F  CL1,1(Rd), F  = infx F (x) > -. For any   (0, 1), the sequence {xt} obtained from update rule (14) satisfies

min
j=0:T -1

F (xj)

2 2



after T steps, where


1 +  
T= 1 +  

2(0 +

g0

2 2

/(0

)+2(F

(x0

)-F



)/)(F

(x0

)-F



)



if 0  L or 1  L,

log(

L 0

)

log(

 lL2

+1)

+

( )L+

2l 0

L2 +

2l 

L+

1+

8 

F

(x0

)-F



+

lL(L-0 20

)



2

otherwise.

Theorem 23 (Convergence rate of update rule (33)). Suppose that   Cl1,1 ([1, +)),  is strongly convex, and F  CL1,1(Rd), F  = infx F (x) > -. For any   (0, 1), the sequence {xt} obtained from update rule (33) satisfies

min
j=0:T -1

F (xj)

2 2



after T steps, where

 1 + 

2(0+2(F (x0)-F )/)(F (x0)-F ) 

if 0  L,



 







T

=

2

+

 

( )0+

g0

2 2

0

+

1+

2 

F

(x0

)-F



+

lL g0 202

2 2



2
 if 0 < L, 1  L, 







  1 +  

log(

L 0

)

log(

 lL2

+1)

+



( ) ( )1+

2l 

L+

2lL2 0

+

1+

2 

F

(x0

)-F



+

lL 20



( )1+

2l 

L+

2lL2 0

-0

2
otherwise.

We begin our proof by following lemma.

23

Under review as a conference paper at ICLR 2019

Lemma 24. Suppose   Cl1,1(R++). Fix   (0, 1]. In both update rules (13) and (14), after

T=

log(

L 0

)

log(

 lL2

+1)

+ 1 steps, either mint=0:T -1

gt

2 2



,

or

T

 L holds.

Proof. Assume that T

< L and mint=0:T -1

gt

2 2

> .

Recall that the sequence {t} is an

increasing sequence. Hence, t < L for 0  t  T .

So, for all 0  t  T - 1,



t+1 t

=

gt

2 2

t2+1

 > L2 (in Algorithm 1),



t+1 t

=

gt

2 2

t2

>

 L2

(in Algorithm 2).

Note that  is a l-smooth convex function, and t+1/t  1. So

 t+1  l t+1 - 1 , t t

(34)

then

t+1 t

>

 lL2

+ 1.

In this case,

T L > T = 0 lL2 + 1

holds but it is impossible according to the setting of T in the lemma.

We first prove Theorem 21 using following lemma.

Lemma 25. In update rule convex function. Denote F 

(13), suppose F = infx F (x) >

 CL1,1(Rd),  -. Let t0  1

 be

Cl1,1(R++), and  is -strongly the first index such that t0  L.

Then for all t  t0,

t



t0-1

+

2  (F (xt0-1)

-

F ),

(35)

and moreover,

F (xt0-1)

-

F



F (x0)

-

F

+

Ll 20

(t0-1

-

0)

(36)

Proof. Same as equation (32),

1L

F (xt+1)  F (xt) - t+1

1- 2t+1

gt

2 2

.

For t  t0 - 1, t+1  L, so

1 F (xt+1)  F (xt) - 2t+1

gt

2 2

.

Hence, for all k  0,

F (xt0+k)



F (xt0-1)

-

1 2

k

i=0

gt0 +i-1

2
2,

t0+i

i.e.,

k i=0

gt0 +i-1 t0+i

2 2

 2(F (xt0-1) - F ).

(37) (38)

24

Under review as a conference paper at ICLR 2019

Note that  is -strongly convex and t2+1 (t+1/t) =

gt

2 2

.

So

gt

2 2

t+1

= t+1

t+1 t

 t

t+1 - 1 t

and

t+1 - t



1 

gt

2
2.

t+1

Combining equation (38) and equation (39), we have

,

(39)

t0+k



t0-1

+

1 

k

i=0

gt0+i-1

2 2

t0+i



t0-1

+

2  (F (xt0-1)

-

F ).

We remain to give an a upper bound for F (xt0-1) in the case t0 > 1. Using equation (32) again, we get

t0 -2

1

F (xt0-1) - F (x0) 

i=0

- i+1

1- L 2i+1

gi

2 2



L t0-2 2
i=0

gi

2 2

i2+1

L t0-2 =
2
i=0

i+1 i

Ll t0-2 
2
i=0

i+1 - 1 i

Ll t0-2 
2
i=0

i+1 - i 0

Ll = 20 (t0-1 - 0).

In the above, the second inequality follows from the assumed l-smoothness of , and the last inequality follows from t  0 for all t  0.

proof of Theorem 21. If t0 = 1, by equation (37), for all t  1, we have

F (xt)



F (x0)

-

1 2

t-1

i=0

gi

2 2

i+1



F (x0)

-

1 2

t-1 i=0

0

+

gi

2 2

2 

(F

(x0)

-

F ) .

Then after T = 1 +

2(0+2(F (x0)-F )/)(F (x0)-F ) 

steps,

min
t=0:T -1

gt

2 2



1 T

T -1

gt

2 2

t=0



2 T

(F (x0)

-

F )(0

+

2  (F (x0)

-

F ))



.

Otherwise, if t0 > 1, we have t0-1 < L. Then for all t  t0,

t



L

+

2 

F (x0)

-

F

+

lL(L - 20

0)

Denote the right hand of equation (40) as max. Using equation (37) again, for we have

F (xt0+M )



F (xt0-1)

-

1 2

M

i=0

gt0 +i-1

2 2

t0+i

1M  F (xt0-1) - 2max i=0

gt0+i-1

2 2

.

(40)

25

Under review as a conference paper at ICLR 2019

Hence,

min
t=0:t0+M -1

gt

2 2



min
t=t0-1:t0+M -1

gt

2 2

1M 
M +1

gt0 +i-1

2 2

i=0



1 M+

1 2max(F (xt0-1)

-

F )

 2max M +1

F

(x0)

-

F



+

lL(L - 20

0)

.

At last, with recalling the conclusion of Lemma 24, after

T=

log(

L 0

)

log(

 lL2

+ 1)

+

2max 

F (x0)

-

F

+

lL(L - 20

0)

+1

steps, we have mint=0:T -1

gt

2 2



.

Next we prove Theorem 22.

Lemma 26. In convex function.

update rule Denote F 

(14), = inf

suppose x F (x).

F Let

 t0

CL1,1 1

(Rd), be the

 first

Cl1,1(R++), and  is -strongly index such that t0  L. Then for

all t  t0,

t



t0

+

8  (F (xt0-1)

-

F ),

(41)

and moreover,

F (xt0-1)

-

F



F (x0)

-

F

+

Ll 20 (t0-1

-

0),

t0 

0 +

g0

2 2

0

if t0

=

1,

L

+

2l 0

L2

+

2l 

L

if

t0



2,

(42) (43)

Proof. Same as the proof of Lemma 25, we first get for all k  0,

k i=0

gt0 +i-1 t0+i

2 2

 2(F (xt0-1) - F ).

Note that in update rule (14), t2 (t+1/t) = gt 22. So

t0+k+1 = t0+k + t0+k

t0+k+1 - 1 t0+k



t0+k

+

t0+k 



t0+k+1 t0+k

1 = t0+k + 

gt0+k

2 2

t0+k

2  t0+k + 

gt0+k - gt0+k-1

2 2

+

t0+k

gt0+k-1

2 2



t0+k

+

2 

L2

xt0+k - xt0+k-1

2 2

+

t0+k

gt0+k-1

2 2



t0+k

+

2 

L2

gt0 +k-1 t30+k

2 2

2 +


gt0+k-1

2 2

t0+k



t0+k

+

4 

gt0+k-1 t0+k

2 2



t0

+

4 

k

i=0

gt0+i-1

2 2

t0+i



t0

+

8  (F (xt0-1)

-

F ).

26

Under review as a conference paper at ICLR 2019

If t0 = 1, then

t0  0 +

g0

2
2,

0

and if t0  2, then

t0  t0-1 +

gt0-1

2 2

t0-1

2L2 = t0-1 + 

gt0-2 t30-1

2 2

+

2 

gt0-2

2 2

t0-2



t0-1

+

2L2 

l(t0-1

- t0-2)t0-2 t30-1

+

2  l(t0-1

- t0-2)

L+

2l

L2

+

2l L.

0 

At last, for t0 > 0, we have

t0 -2

1

F (xt0-1) - F (x0) 

i=0

- i+1

1- L 2i+1

gi

2 2



L t0-2 2
i=0

gi

2 2

i2+1



L t0-2 2
i=0

gi

2 2

i2

L t0-2 =
2
i=0

i+1 i

Ll t0-2 
2
i=0

i+1 - 1 i

Ll t0-2 
2
i=0

i+1 - i 0

Ll = 20 (t0-1 - 0).

proof of Theorem 22. The proof is completely similar to the proof of Theorem 21.

Next, we prove Theorem 23.

Lemma 27. In convex function.

update rule Denote F 

(33), = inf

suppose x F (x).

F Let

 t0

CL1,1 0

(Rd), be the

 first

Cl1,1(R++), and  is -strongly index such that t0  L. Then for

all t  t0,

t



t0

+

2 (F


(xt0 )

-

F

),

(44)

and moreover,

F (xt0 )

-

F



F (x0)

-

F

+

Ll 20 (t0

-

0),

t0 

0 +

,g0

2 2

0

if

t0



1,

1

+

2l 

L+

2l 0

L2

,

if t0



2.

(45) (46)

Proof. Same as the proof of Lemma 25, we first get

1L

F (xt0+k+1)  F (xt0+k) - t0+k

1- 2t0+k

1  F (xt0+k) - 2t0+k

gt0+k

2 2

1k  F (xt0 ) - 2
i=0

gt0+i

2
2,

t0+i

gt0+k

2 2

27

Under review as a conference paper at ICLR 2019

and

k i=0

gt0+i t0+i

2 2

 2(F (xt0 ) - F ).

Note that in Algorithm 2, t2 (t+1/t) = gt 22. So

t0+k+1 = t0+k + t0+k

t0+k+1 - 1 t0+k



t0+k

+

t0+k  

t0+k+1 t0+k

1 = t0+k + 

gt0+k

2 2

t0+k



t0

+

1 

k

i=0

gt0+i

2 2

t0+i



t0

+

2  (F (xt0 )

-

F ).

At last, for t0 > 0, we have

F (xt0 ) - F (x0)



L 2

t0 -1

i=0

gi

2 2

i2

L t0-1 =
2
i=0

i+1 i

Ll  20 (t0 - 0).

Recall that

t(t+1 - t) 

gt

2 2



lt(t+1

-

t).

So if t0 = 1,

t0  0 +

g0

2
2.

0

And if t0  2, we have

t0  t0-1 +

gt0-1

2 2

t0-1

 t0-1 + 2

gt0-1 - gt0-2

2 2

+

t0-1

gt0-2

2 2

2L2  t0-1 + 

xt0-1 - xt0-2

2 2

+

2

t0-1



gt0-2

2 2

t0-2

=

t0-1

+

2L2 

gt0-2

2 2

t20-2t0-1

+

2 

gt0-2

2 2

t0-2



t0-1

+

2L2 

l(t0-1 - t0-2) t0-2t0-1

+

2  l(t0-1

-

t0-2)

L+

2l

L2

+

2l L.

0 

proof of Theorem 23. The proof is completely similar to the proof of Theorem 21.

28

