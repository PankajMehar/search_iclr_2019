Under review as a conference paper at ICLR 2019
ADAPTIVE GRADIENT METHODS WITH DYNAMIC BOUND OF LEARNING RATE
Anonymous authors Paper under double-blind review
ABSTRACT
Adaptive optimization methods such as ADAGRAD, RMSPROP and ADAM have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGRAD to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of ADAM and AMSGRAD, called ADABOUND and AMSBOUND respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks.
1 INTRODUCTION
There has been tremendous progress in first-order optimization algorithms for training deep neural networks. One of the most dominant algorithms is Stochastic gradient descent (SGD) (Robbins & Monro, 1951), which performs well across many applications in spite of its simplicity. However, there is a disadvantage of SGD that it scales the gradient uniformly in all directions. This may lead to poor performance when the training data are sparse as well as limited training speed. To address this problem, recent work has proposed a variety of adaptive methods that scale the gradient by square roots of some form of the average of the squared values of past gradients. Examples of such methods include ADAM (Kingma & Lei Ba, 2015), ADAGRAD (Duchi et al., 2011) and RMSPROP (Tieleman & Hinton, 2012). ADAM in particular has become the default algorithm leveraged across many deep learning frameworks due to its rapid training speed (Wilson et al., 2017).
Despite their popularity, the generalization ability and out-of-sample behavior of these adaptive methods are likely worse than their non-adaptive counterparts. Adaptive methods often display faster progress in the initial portion of the training, but their performance quickly plateaus on the unseen data (development/test set) (Wilson et al., 2017). Indeed, the optimizer is chosen as SGD in several recent state-of-the-art works in natural language processing and computer vision (Merity et al., 2018; Loshchilov & Hutter, 2017), wherein these instances SGD does perform better than adaptive methods. Reddi et al. (2018) have recently proposed a variant of ADAM called AMSGRAD, hoping to solve this problem. The authors provide a theoretical guarantee of convergence but only illustrate its better performance on training data. However, the generalization ability of AMSGRAD on unseen data is found to be similar to that of ADAM while a considerable performance gap still exists between AMSGRAD and SGD (Keskar & Socher, 2017; Chen et al., 2018).
In this paper, we first conduct an empirical study on ADAM and illustrate that both exceedingly large and small learning rates exist by the end of training. The results correspond with the perspective pointed out by Wilson et al. (2017) that the lack of generalization performance of adaptive methods may stem from unstable and extreme learning rates. In fact, introducing non-increasing learning rates, the key point in AMSGRAD, may help abate the impact of huge learning rates, while
1

Under review as a conference paper at ICLR 2019

it neglects possible effects of small ones. We further provide an example of a simple convex optimization problem to elucidate how tiny learning rates of adaptive methods can lead to undesirable non-convergence. In such settings, RMSPROP and ADAM provably do not converge to an optimal solution, and furthermore, however large the initial step size  is, it is impossible for ADAM to fight against the scale-down term.
Based on the above analysis, we propose new variants of ADAM and AMSGRAD, named ADABOUND and AMSBOUND, which do not suffer from the negative impact of extreme learning rates. We employ dynamic bounds on learning rates in these adaptive methods, where the lower and upper bound are initialized as zero and infinity respectively, and they both smoothly converge to a constant final step size. The new variants can be regarded as adaptive methods at the beginning of training, and they gradually and smoothly transform to SGD (or with momentum) as time step increases. In this framework, we can enjoy a rapid initial training process as well as good final generalization ability. We provide a convergence analysis for the new variants in the convex setting.
We finally turn to an empirical study of the proposed methods on various popular tasks and models in computer vision and natural language processing. Experimental results demonstrate that our methods have higher learning speed early in training and in the meantime guarantee strong generalization performance compared to several adaptive and non-adaptive methods. Moreover, they can bring considerable improvement over their prototypes especially on complex deep networks.

2 NOTATIONS AND PRELIMINARIES

Notations Given a vector   Rd we denote its i-th coordinate by i; we use k to denote elementwise power of k and  to denote its 2-norm; for a vector t in the t-th iteration, the i-th coordinate of t is denoted as t,i by adding a subscript i. Given two vectors v, w  Rd, we use v, w to denote their inner product, v w to denote element-wise product, v/w to denote element-wise division,
max(v, w) to denote element-wise maximum and min(v, w) to denote element-wise minimum. We use S+d to denote the set of all positive definite d × d matrices. For a vector a  Rd and a positive definite matrix M  Rd×d, we use a/M to denote M -1a and M to denote M 1/2. The projection operation F,M (y) for M  S+d is defined as arg minxF M 1/2(x - y) for y  Rd. We say F has bounded diameter D if x - y   D for all x, y  F .

Online convex programming A flexible framework to analyze iterative optimization methods is

the online optimization problem. It can be formulated as a repeated game between a player (the

algorithm) and an adversary. At step t, the algorithm chooses an decision xt  F, where F  Rd is

a convex feasible set. Then the adversary chooses a convex loss function ft and the algorithm incurs

loss ft(xt). The difference between the total loss

T t=1

ft(xt)

and

its

minimum

value

for

a

fixed

decision is known as the regret, which is represented by RT =

T t=1

ft(xt

)

-

minxF

T t=1

ft(x).

Throughout this paper, we assume that the feasible set F has bounded diameter and ft(x)  is

bounded for all t  [T ] and x  F. We are interested in algorithms with little regret. Formally

speaking, our aim is to devise an algorithm that ensures RT = o(T ), which implies that on average,

the model's performance converges to the optimal one. It has been pointed out that an online op-

timization algorithm with vanishing average regret yields a corresponding stochastic optimization

algorithm (Cesa-Bianchi et al., 2002). Thus, following Reddi et al. (2018), we use online gradient

descent and stochastic gradient descent synonymously.

A generic overview of optimization methods We follow Reddi et al. (2018) to provide a generic

Algorithm 1 Generic framework of optimization methods

Input: x1  F , initial step size , sequence of functions {t, t}tT=1

1: for t = 1 to T do

2: gt = ft(xt)

3: mt = t(g1, · · · , gt) and Vt = t(g1, · · · , gt)

4: t = / t



5: x^t+1 = xt - tmt/ Vt

6:

xt+1

=

F

 , Vt

(x^t+1

)

7: end for

2

Under review as a conference paper at ICLR 2019

framework of optimization methods in Algorithm 1 that encapsulates many popular adaptive and non-adaptive methods. This is useful for understanding the properties of different optimization methods. Note that the algorithm is still abstract since the functions t : F t  Rd and t : F d  S+d have not been specified. In this paper, we refer to  as initial step size and t/ Vtas learning rate of the algorithm. Note that we employ a design of decreasing step size by t = / t for it is required for theoretical proof of convergence. However such an aggressive decay of step size typically translates into poor empirical performance, while a simple constant step size t =  usually works well in practice. For the sake of clarity, we will use the decreasing scheme for theoretical analysis and the constant schemem for empirical study in the rest of the paper.
Under such a framework, we can summarize the popular optimization methods in Table 1.1 A few remarks are in order. We can see the scaling term t is I in SGD(M), while adaptive methods introduce different kinds of averaging of the squared values of past gradients. ADAM and RMSPROP can be seen as variants of ADAGRAD, where the former ones use an exponential moving average as function t instead of the simple average used in ADAGRAD. In particular, RMSPROP is essentially a special case of ADAM with 1 = 0. AMSGRAD is not listed in the table as it does not has a simple expression of t. It can be defined as t = diag(v^t) where v^t is obtained by the following recursion: vt = 2vt-1 + (1 - 2)gt2 and v^t = max(v^t-1, vt) with v^0 = v0 = 0. The definition of t is same with that of ADAM. In the rest of the paper we will mainly focus on ADAM due to its generality but our arguments also apply to other similar adaptive methods such as RMSPROP and AMSGRAD.

Table 1: An overview of popular optimization methods using the generic framework.

SGD SGDM

ADAGRAD

RMSPROP

ADAM

tt

t gt

 t-i gi

gt

gt (1 - 1) 1t-igi

i=1

i=1

tt

t

t I

I diag( gi2)/t (1 - 2)diag( 2t-igi2) (1 - 2)diag( 2t-igi2)

i=1

i=1

i=1

3 THE NON-CONVERGENCE CAUSED BY EXTREME LEARNING RATE
In this section, we elaborate the primary defect in current adaptive methods with a preliminary experiment and a rigorous proof. As mentioned above, adaptive methods like ADAM are observed to perform worse than SGD. Reddi et al. (2018) proposed AMSGRAD to solve this problem but recent researches have pointed out AMSGRAD does not show evident improvement over ADAM (Keskar & Socher, 2017; Chen et al., 2018). Since AMSGRAD is claimed to have a smaller learning rate compared with ADAM, the authors only consider large learning rates the cause for bad performance of ADAM. However, small ones might be a pitfall as well. Thus, we speculate both exceedingly large and small learning rates of ADAM are likely to account for its ordinary generalization ability.
For corroborating our speculation, we sample learning rates of several weights and biases on ResNet32 using ADAM. Specifically, we select nine 3 × 3 convolutional kernels from different layers and the biases in the last linear layer. As parameters of the same layer usually have similar properties, here we only demonstrate learning rates of 9 weights sampled from 9 kernels respectively and 1 bias from last layer by the end of training, and employ a heatmap to visualize them. As shown in Figure 1, we can find that when the model is close to convergence, learning rates are composed of too small ones less than 0.01 as well as large ones greater than 1000.
The above analysis and observation show that there are indeed learning rates which are too large or too small in the final stage of the training process. AMSGRAD may help abate the impact of huge learning rates, but it neglects the other side of the coin. Insofar, we still have the following two doubts. First, does the tiny learning rate really do harm to the convergence of ADAM? Second, as the learning rate highly depends on the initial step size, can we use a relatively larger initial step size  to get rid of too small learning rates?
1We ignore the debiasing term used in the original version of ADAM in Kingma & Lei Ba (2015) for simplicity. Our arguments apply to the debiased version as well.
3

Under review as a conference paper at ICLR 2019

-5.8 -3.7 -3.4 -3.7 4.5 -3 8.6 2 -1.6 -4 w1 w2 w3 w4 w5 w6 w7 w8 w9 b
Figure 1: Learning rates of sampled parameters. Each cell contains a value obtained by conducting a logarithmic operation on the learning rate. The lighter cell stands for the smaller learning rate.

To answer these questions, we show that undesirable convergence behavior for ADAM and RMSPROP can be caused by exceedingly small learning rates, and furthermore, in some cases no matter how large the initial step size  is, ADAM will still fail to find the right path and converge to some highly suboptimal points. Consider the following sequence of linear functions for F = [-2, 2]:

-100,  0, 
ft(x) = -x,
2x,  0,

for x < 0; for x > 1; for 0  x  1 and t mod C = 1; for 0  x  1 and t mod C = 2; otherwise

where C  N satisfies: 52C-2  (1 - 2)/(4 - 2). For this function sequence, it is easy to see any point satisfying x < 0 provides the minimum regret. Suppose 1 = 0. We show that ADAM converges to a highly suboptimal solution of x  0 for this setting. Intuitively, the reasoning is as follows. The algorithm obtains a gradient -1 once every C steps, which moves the algorithm in the wrong direction. Then, at the next step it observes a gradient 2. But the larger gradient 2 is unable to counteract the effect to wrong direction since the learning rate at this step is scaled down to a value much less than the previous one, and hence x becomes larger and larger as the time step increases. We formalize this intuition in the result below.
Theorem 1. There is an online convex optimization problem where for any initial step size , ADAM has non-zero average regret i.e., RT /T 0 as T  .

We relegate all proofs to the appendix. Note that the above example also holds for constant step size t = . Also note that vanilla SGD does not suffer from this problem. There is a wide range of valid choices of initial step size  where the average regret of SGD asymptotically goes to 0, in other words, converges to the optimal solution. This problem can be more obvious in the later stage of a training process in practice when the algorithm gets stuck in some suboptimal points. In such cases, gradients at most steps are close to 0 and the average of the second order momentum may be highly various due to the property of exponential moving average. Therefore, "correct" signals which appear with a relatively low frequency (i.e. gradient 2 every C steps in the above example) may not be able to lead the algorithm to a right path, if they come after some "wrong" signals (i.e. gradient 1 in the example), even though the correct ones have larger absolute value of gradients.
One may wonder if using large 1 helps as we usually use 1 close to 1 in practice. However, the following result shows that for any constant 1 and 2 with 1 < 2, there exists an example where ADAM has non-zero average regret asymptotically regardless of the initial step size . Theorem 2. For any constant 1, 2  [0, 1) such that 1 < 2, there is an online convex optimization problem where for any initial step size , ADAM has non-zero average regret i.e., RT /T 0 as T  .

Furthermore, a stronger result stands in the easier stochastic optimization setting. Theorem 3. For any constant 1, 2  [0, 1) such that 1 < 2, there is a stochastic convex optimization problem where for any initial step size , ADAM does not converge to the optimal solution.
Remark. The analysis of ADAM in Kingma & Lei Ba (2015) relies on decreasing 1 over time, while here we use constant 1. Indeed, since the critical parameter is 2 rather than 1 in our analysis, it is quite easy to extend our examples to the case using decreasing scheme of 1.
 As mentioned by Reddi et al. (2018), the condition 1 < 2 is benign and is typically satisfied in the parameter settings used in practice. Such condition is also assumed in convergence proof of

4

Under review as a conference paper at ICLR 2019

Kingma & Lei Ba (2015). The above results illustrate the potential bad impact of extreme learning rates and algorithms are unlikely to achieve good generalization ability without solving this problem.

4 ADAPTIVE MOMENT ESTIMATION WITH DYNAMIC BOUND

In this section we develop new variants of optimization methods and provide their convergence analysis. Our aim is to devise a strategy that combines the benefits of adaptive methods, viz. fast initial progress, and the good final generalization properties of SGD. Intuitively, we would like to construct an algorithm that behaves like adaptive methods early in training and like SGD at the end.

Algorithm 2 ADABOUND

Input: x1  F , initial step size , {1t}Tt=1, 2, lower bound function l, upper bound function u

1: Set m0 = 0, v0 = 0

2: for t = 1 to T do

3: gt = ft(xt)

4: mt = 1tmt-1 + (1 - 1t)gt

5: vt = 2vt-1 + (1 - 2)gt2 and Vt = diag(vt)



6: ^t = max{l(t), min{u(t), / Vt}} and t = ^t/ t

7: xt+1 = F,diag(t-1)(xt - t mt) 8: end for

Inspired by gradient clipping, a popular technique used in practice that clips the gradients larger than a threshold to avoid gradient explosion, we employ clipping on learning rates in ADAM to propose ADABOUND in Algorithm 2. Consider applying the following operation in ADAM

Clip(/ Vt, l, r),

which clips the learning rate element-wisely such that the output is constrained to be in [l, r].2 It follows that SGD(M) with  =  can be considered as the case where l = r = . As for ADAM, l = 0 and r = . Now we can provide the new strategy with the following steps. We employ l and r as functions of t instead of constant lower and upper bound, where l(t) is a non-decreasing function that starts from 0 as t = 0 and converges to  asymptotically; and r(t) is a non-increasing function that starts from  as t = 0 and also converges to  asymptotically.
In this setting, ADABOUND behaves just like ADAM at the beginning as the bounds have very little
impact on learning rates, and it gradually transforms to SGD(M) as the bounds become more and
more restricted. We prove the following key result for ADABOUND.

Theorem 4. Let {xt} and {vt} be the sequences for all t  [T ] and 1/ 2 < 1. Suppose l(t +

obtained from 1)  l(t) >

Algorithm 2, 0, r(t + 1)

1 = 11,  r(t), l

1t (t)

 

1 

as t  , r(t)   as t  , L = l(1) and R = r(1). Assume that x - y   D

for all x, y  F and ft(x)  G2 for all t  [T ] and x  F . For xt generated using the

ADABOUND algorithm, we have the following bound on the regret

RT



 D2 T 2(1 - 1)

d
^T-,1i +
i=1

D2 2(1 - 1)

T t=1

d 1tt-,i1 + (2 T
i=1

- 1) RG22 . 1 - 1

The following result falls as an immediate corollary of the above result.

Corollary 4.1. Suppose 1t = 1t-1 in Theorem 4, we have

RT



 D2 T 2(1 - 1)

d
^T-,1i +
i=1

1dD2 2(1 - 1)(1 - )2L

 + (2 T

-

1)

RG22 1 - 1

.

 It is easy to see that the regret of ADABOUND is upper bounded by O( T ). Similar to Reddi et al. (2018), one can use a much more modest momentum decay of 1t = 1/t and still ensure a regret of O( T ). It should be mentioned that one can also incorporate the dynamic bound in

2Here we use constant step size for simplicity. The case using a decreasing scheme of step size is similar.

5

Under review as a conference paper at ICLR 2019

 AMSGRAD. The resulting algorithm, namely AMSBOUND, also holds a regret of O( T ) and the proof of convergence is almost same to Theorem 4 (see Appendix F for details). In next section we will see that AMSBOUND has similar performance to ADABOUND in several well-known tasks.

We end this section with a comparison to the previous work. For the idea of transforming ADAM

to SGD, there is a similar work by Keskar & Socher (2017). The authors propose a measure that

uses ADAM at first and switches the algorithm to SGD at some specific step. Compared with their

approach, our methods have two advantages. First, whether there exists a fixed turning point to

distinguish ADAM and SGD is uncertain. So we address this problem with a continuous transforming

procedure rather than a "hard" switch. Second, they introduce an extra hyperparameter to decide the

switching time, which is not very easy to fine-tune. As for our methods, the flexible parts introduced

are two bound functions. We conduct an empirical study of the impact of different kinds of bound functions. The results are placed in Appendix G for we find that the convergence target  and

convergence speed are not very important to the final results. For the sake of clarity, we will use

l(t)

=

0.1

-

0.1 (1-)t+1

and

r (t)

=

0.1

+

0.1 (1-)t+1

in

the

rest

of

the

paper.

5 EXPERIMENTS

In this section, we turn to an empirical study of different models to compare new variants with popular optimization methods including SGD(M), ADAGRAD, ADAM, and AMSGRAD. We focus on three tasks: the MNIST image classification task, the CIFAR-10 image classification task, and word-level language modeling on Penn Treebank. The setup for each task is detailed in Table 2.3 We run each experiment 3 times with the specified initialization method from random starting points. A fixed budget on the number of epochs is assigned for training and the decay strategy is introduced in following parts. We choose the settings that achieve the lowest training loss at the end.
Table 2: Summaries of the models utilized for our experiments.

Model
Type Dataset

Perceptron
FNN MNIST

DenseNet
Deep-CNN CIFAR-10

ResNet-32
Deep-CNN CIFAR-10

1-layer LSTM
RNN PTB

2-layer LSTM
RNN PTB

3-layer LSTM
RNN PTB

5.1 HYPERPARAMETER TUNING
To tune the step size, we follow the method in Wilson et al. (2017). We implement a logarithmicallyspaced grid of five step sizes. If the best performing parameter is at one of the extremes of the grid, we will try new grid points so that the best performing parameters are at one of the middle points in the grid. Specifically, we tune over hyperparameters in the following way.
SGD(M) For tuning the step size of SGD(M), we first coarsely tune the step size on a logarithmic scale from {100, 10, 1, 0.1, 0.01} and then fine-tune it. ADAGRAD The initial set of step sizes used for ADAGRAD are: {5e-2, 1e-2, 5e-3, 1e-3, 5e-4}. For the initial accumulator value, we choose the recommended value as 0.
ADAM & AMSGRAD We employ the same hyperparameters for these two methods. The initial step sizes are chosen from: {1e-2, 5e-3, 1e-3, 5e-4, 1e-4}. We turn over 1 values of {0.9, 0.99} and 2 values of {0.99, 0.999}. We use for the perturbation value = 1e-8. ADABOUND & AMSBOUND After we find all the best hypyerparameters for ADAM and AMSBOUND, we directly apply them in our proposed methods.
5.2 FEEDFORWARD NETWORK
We train a simple fully connected neural network with one hidden layer for the multiclass classification problem on MNIST dataset. We run 100 epochs and omit the decay scheme for this experiment.
3Architectures can be found at the following links: (1) DenseNet&ResNet-32: https://github.com/ kuangliu/pytorch-cifar; (2) LSTM: https://github.com/salesforce/awd-lstm-lm.
6

Under review as a conference paper at ICLR 2019

Figure 2 shows the learning curve for each optimization method on both the training and test set. We find that for training, all algorithms can achieve the accuracy approaching 100%. For the test part, SGD performs slightly better than adaptive methods ADAM and AMSGRAD. Our two proposed methods, ADABOUND and AMSBOUND, display slight improvement, but compared with their prototypes there are still visible increases in test accuracy.

Train Accuracy % Test Accuracy %

100.0

99.5

99.0

98.5

98.0

SGD Adam

97.5

AMSGrad AdaBound

AMSBound

97.0 0

20

40 Epoch 60

80 100

(a) MNIST (Train)

98.0

97.5

97.0

SGD

96.5

Adam AMSGrad

AdaBound

AMSBound

96.0 0

20 40 60 80 100

Epoch

(b) MNIST (Test)

Figure 2: Training(left) and test accuracy(right) on MNIST

5.3 CONVOLUTIONAL NEURAL NETWORK
Using DenseNet and ResNet-32 under the framework of PyTorch, we then consider the task of image classification on the standard CIFAR-10 dataset. In this experiment, we employ the fixed budget of 200 epochs and reduce the learning rates by 10 after 150 epochs.

Train Accuracy % Test Accuracy % Train Accuracy % Test Accuracy %

100.0 95.0

100.0 95.0

97.5 92.5

97.5 92.5

95.0 90.0

95.0 90.0

92.5 87.5

92.5 87.5

90.0 85.0

87.5

SGDM AdaGrad

82.5

SGDM AdaGrad

85.0

Adam AMSGrad

80.0

Adam AMSGrad

82.5

AdaBound AMSBound

77.5

AdaBound AMSBound

80.0 0 25 50 75 Ep1o0c0h 125 150 175 200 75.0 0 25 50 75 Ep1o0c0h 125 150 175 200

90.0 85.0

87.5

SGDM AdaGrad

82.5

SGDM AdaGrad

85.0

Adam AMSGrad

80.0

Adam AMSGrad

82.5

AdaBound AMSBound

77.5

AdaBound AMSBound

80.0 0 25 50 75 Ep1o0c0h 125 150 175 200 75.0 0 25 50 75 Ep1o0c0h 125 150 175 200

(a) DenseNet (Train)

(b) DenseNet (Test)

(c) ResNet-32 (Train) (d) ResNet-32 (Test)

Figure 3: Performance curves on the training set (left) and the test set (right) for two experiments on the mutilclass classification problem on CIFAR-10.

DenseNet Model We first run a DenseNet model on CIFAR-10 and our results are shown in Figure 3. We can see that adaptive methods such as ADAGRAD, ADAM and AMSGRAD appear to perform better than the non-adaptive ones early in training. But by epoch 150 when the learning rates are decayed, SGDM begins to outperform those adaptive methods. As for our methods, ADABOUND and AMSBOUND, they converge as fast as adaptive ones and at the end of training achieve a bit higher accuracy than SGDM on the test set. In addition, compared with their prototypes, their performances are enhanced evidently with approximately 2% improvement in the test accuracy.
ResNet-32 Model Results for this experiment are reported in Figure 3. As is expected, the overall performance of each algorithm on ResNet-32 is similar to that on DenseNet. ADABOUND and AMSBOUND even surpass SGDM by 1%. Despite the relative bad generalization ability of adaptive methods, our proposed methods overcome this drawback by allocating bounds for their learning rates and obtain almost the best accuracy on the test set for both DenseNet and ResNet-32 on CIFAR-10.

5.4 WORD-LEVEL LANGUAGE MODELING
Finally, we conduct an experiment on the word-level language modeling with AWD-LSTM. From two experiments above, we observe that our methods show much more improvement in deep convolutional neural networks than in perceptrons. Therefore, we suppose that the enhancement is related to the complexity of the architecture and run three models with (L1) 1-layer, (L2) 2-layer and (L3) 3-layer LSTM respectively. We train them on Penn Treebank, running for a fixed budget of 200 epochs. We use perplexity as the metric to evaluate the performance and report results in Figure 4.

7

Under review as a conference paper at ICLR 2019

Perplexity Perplexity Perplexity

100 SGD Adam
95 AdaBound AMSBound
90

85

80

75 0

25 50 75 100 125 150 175 200 Epoch

(a) L1

74 72 70 68 66 64 62 0

SGD Adam AdaBound AMSBound 25 50 75 100 125 150 175 200 Epoch

(b) L2

74 72 70 68 66 64 62
0

SGD Adam AdaBound AMSBound 25 50 75 100 125 150 175 200 Epoch

(c) L3

Figure 4: Perplexity curves on the test set comparing SGD, ADAM, ADABOUND and AMSBOUND on the AWD-LSTM with different layers on Penn Treebank.

We find that in all models, ADAM has the fastest initial progress but stagnates in worse performance than SGD and our methods. Different from phenomena in previous experiments on the image classification tasks, ADABOUND and AMSBOUND does not display rapid speed at the early training stage but the curves are smoother than that of SGD.
Comparing L1, L2 and L3, we can easily notice a distinct difference of the improvement degree. In L1, the simplest model, our methods perform slightly 1.1% better than ADAM while in L3, the most complex model, they show evident improvement over 2.8% in terms of perplexity. It serves as evidence for the relationship between the model's complexity and the improvement degree.
5.5 ANALYSIS
To investigate the efficacy of our proposed algorithms, we select popular tasks from computer vision and natural language processing. Based on results shown above, it is easy to find that ADAM and AMSGRAD usually perform similarly and the latter does not show much improvement for most cases. Their variants, ADABOUND and AMSBOUND, on the other hand, demonstrate a fast speed of convergence compared with SGD while they also exceed two original methods greatly with respect to test accuracy at the end of training. This phenomenon exactly confirms our view mentioned in Section 3 that both large and small learning rates can influence the convergence.
Besides, we implement our experiments on models with different complexities, consisting of a perceptron, two deep convolutional neural networks and a recurrent neural network. The perceptron used on the MNIST is the simplest and our methods perform slightly better than others. As for DenseNet and ResNet-32, obvious increases in test accuracy can be observed. We attribute this difference to the complexity of the model. Specifically, for deep CNN models, convolutional and fully connected layers play different parts in the task. Also, different CNN layers are likely to be responsible for different roles (Lee et al., 2009), which may lead to a distinct variation of gradients of parameters. In other words, extreme learning rates (huge or tiny) may appear more frequently in complex models such as ResNet-32. As our algorithms are proposed to avoid them, the greater enhancement of performance in complex architectures can be explained intuitively. The higher improvement degree on LSTM with more layers on PTB dataset also consists with the above analysis.
6 CONCLUSION
We investigate existing adaptive algorithms and find that exceedingly large or small learning rates can result in the poor convergence behavior. A rigorous proof of non-convergence for ADAM is provided to demonstrate the above problem. We design a strategy to constrain the learning rates of ADAM and AMSGRAD to avoid a violent oscillation. Our proposed algorithms, ADABOUND and AMSBOUND, which employ dynamic bounds on their learning rates, achieve a smooth transition to SGD. They show the great efficacy on several standard benchmarks while maintaining advantageous properties of adaptive methods such as rapid initial progress and hyperparameter insensitivity.
Despite superior results of our methods, there still remain several problems to explore. For example, the improvement on simple models are not very inspiring, we can investigate how to achieve higher improvement on such models. We may also dig down deeper on the bound functions.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Nico Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. In Advances in Neural Information Processing Systems 14 (NIPS), pp. 359­ 366, 2002.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. CoRR, abs/1808.02941, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12:2121­2159, 2011.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to SGD. CoRR, abs/1712.07628, 2017.
Diederik P Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.
Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th annual international conference on machine learning, pp. 609­616, 2009.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.
H Brendan Mcmahan and Matthew Streeter. Adaptive Bound Optimization for Online Convex Optimization. In Proceedings of the 23rd Annual Conference On Learning Theory, pp. 244­256, 2010.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.
Sashank J. Reddi, Stayen Kale, and Sanjiv Kumar. On the Convergence of Adam and Beyond. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400­407, 1951.
Tijmen Tieleman and Geoffrey Hinton. Rmsprop: divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26­31, 2012.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems 30 (NIPS), pp. 4148­4158, 2017.
9

Under review as a conference paper at ICLR 2019

APPENDIX

A AUXILIARY LEMMAS
Lemma 1 (Mcmahan & Streeter (2010)). For any Q  S+d and convex feasible set F  Rd, suppose u1 = minxF Q1/2(x - z1) and u2 = minxF Q1/2(x - z2) then we have Q1/2(u1 - u2)  Q1/2(z1 - z2) .

Proof. We provide the proof here for completeness. Since u1 = minxF Q1/2(x - z1) and u2 = minxF Q1/2(x - z2) and from the property of projection operator we have the following:

z1 - u1, Q(z2 - z1)  0 and z2 - u2, Q(z1 - z2)  0.

Combining the above inequalities, we have

u2 - u1, Q(z2 - z1)  z2 - z1, Q(z2 - z1) .

(1)

Also, observe the following:

u2 - u1, Q(z2 - z1)



1 2

[

u2

- u1, Q(u2

- u1)

+

z2 - z1, Q(z2 - z1) ] .

The above inequality can be obtained from the fact that

(u2 - u1) - (z2 - z1), Q((u2 - u1) - (z2 - z1))  0 as Q  S+d

and rearranging the terms. Combining the above inequality with Equation (1), we have the required the result.

Lemma 2. Suppose mt = 1mt-1 + (1 - 1)gt with m0 = 0 and 0  1 < 1. We have

TT

mt 2 

gt 2.

t=1 t=1

Proof. If 1 = 0, the equality directly holds due to mt = gt. Otherwise, 0 < 1 < 1. For any  > 0 we have

mt 2 = 1mt-1 2 + (1 - 1)gt 2 + 2 1mt-1, (1 - 1)gt  1mt-1 2 + (1 - 1)gt 2 +  1mt-1 2 + 1/ (1 - 1)gt 2 = (1 + ) 1mt-1 2 + (1 + 1/) (1 - 1)gt 2

The inequality follows from Cauchy-Schwarz and Young's inequality. In particular, let  = 1/1-1. Then we have
mt 2  1 mt-1 2 + (1 - 1) gt 2.
Dividing both sides by 1t, we get

mt 1t

2



mt-1 1t-1

2

+

(1

-

1) 1t

gt

2
.

Note that m0 = 0. Hence,

mt 1t

2

t
 (1 - 1)
i=1

gi 21-i.

Then multiplying both sides by 1t we obtain

t

mt 2  (1 - 1)

gi 21t-i.

i=1

10

Under review as a conference paper at ICLR 2019

Take the summation of above inequality over t = 1, 2, · · · , T , we have

T Tt

mt 2  (1 - 1)

gi 21t-i

t=1 t=1 i=1

TT

= (1 - 1)

gi 21t-i

i=1 t=i

T
 gt 2.

t=1

The second inequality is due to the following fact of geometric series

We complete the proof.

N
1i
i=0




1i
i=0

=

1 ,
1 - 1

for 0

<

1

< 1.

B PROOF OF THEOREM 1

Proof. First, we rewrite the update of ADAM in Algorithm 1 in the following recursion form:

mt,i = 1mt-1,i + (1 - 1)gt,i and vt,i = 2vt-1,i + (1 - 2)gt2,i

(2)

where m0,i = 0 and v0,i = 0 for all i  [d] and t = diag(vt). We consider the setting where ft are linear functions and F = [-2, 2]. In particular, we define the following function sequence:

-100,  0, 
ft(x) = -x,
2x,  0,

for x < 0; for x > 1; for 0  x  1 and t mod C = 1; for 0  x  1 and t mod C = 2; otherwise

where C  N satisfies the following:

52C-2



1 4

- -

2 . 2

It is not hard to see that the condition hold for large constant C that depends on 2.

(3)

We use notations similar to the ones in proofs of Reddi et al. (2018). Here we provide the notations
for completeness. Since the problem is one-dimensional, we drop indices representing coordinates
from all quantities in Algorithm 1. For this function sequence, it is easy to see that any point
satisfying x < 0 provides the minimum regret. Without loss of generality, assume that the initial
point is x1 = 0. This can be assumed without any loss of generality because for any choice of initial point, we can always translate the coordinate system such that the initial point is x1 = 0 in the new coordinate system and then choose the sequence of functions as above in the new coordinate system.
Consider the execution of ADAM algorithm for this sequence of functions with 1 = 0. Note that since gradients of these functions are bounded, F has bounded D diameter and 12/2 < 1 as 1 = 0. Hence, the conditions on the parameters required for ADAM are satisfied (Kingma & Lei Ba, 2015).

Our claim is that for any initial step size , we have xt  0 for all t  N. To prove this, we resort to the principle of mathematical induction. Suppose for some t  N, we have xCt+1  0. Our aim is to prove that xi  0 for all i  N  [Ct + 2, Ct + C + 1]. It is obvious that the conditions holds if xCt+1 > 1. Now we assume xCt+1  1. We observe that the gradients have the following form:

-1, for 0  x  1 and i mod C = 1;  fi(x) = 2, for 0  x  1 and i mod C = 2;
0, otherwise.

11

Under review as a conference paper at ICLR 2019

From the (Ct + 1)-th update of ADAM in Equation (2), we obtain:

x^Ct+2

=

xCt+1

+

 Ct +

1

1  0. 2VCt + (1 - 2)

We consider the following two cases:

1. Suppose x^Ct+2 > 1, then xCt+2 = F (x^Ct+2) = min(x^Ct+2, 2) > 1 (note that in

xoni e=-dximCet+ns2ion,0foFr,alVltth=e

F is the simple Euclidean projection). It is easy to see following i as all the following gradients will equal to 0.

that

2. Suppose x^Ct+2  1, then after the (Ct + 2)-th update, we have:

x^Ct+3

=

xCt+2

-

 Ct +

2

2 2VCt+1 + 4(1 - 2)

=

xCt+1

+

 Ct +

1

1 -  2VCt + (1 - 2) Ct + 2

2 2VCt+1 + 4(1 - 2)

 Ct + 2

2 2VCt + (1 - 2)

2VCt+1 + 4(1 - 2) (VCt+1 - 4VCt)  0,

which translates to xCt+3 = x^Ct+3  0. The first inequality follows from xCt+2 = x^Ct+2 and xCt+1  0. The last inequality is due to the following lower bound:

VCt+1 - 4VCt = 2VCt + (1 - 2) - 4VCt

= (4 - 2)

1 4

- -

2 2

-

VCt

= (4 - 2)

1 - 2 4 - 2

-

(1

t
- 2)(
i=1

2Ci-1

+4

t i=1

2Ci-2)

 (4 - 2)

1 4

- -

2 2

-

(1

-

2)(

2C-1 1 - 2C

+

42C-2 1 - 2C

)

 (4 - 2)

1 4

- -

2 2

-

52C-2

 0.

The second inequality follows from 2 < 1. The last inequality follows from Equation (3). Furthermore, since gradients equal to 0 when xi  0 and i mod C = 1 or 2, we have

xCt+4 = x^Ct+3 = xCt+3  0, xCt+5 = x^Ct+4 = xCt+4  0,
··· xCt+C+1 = x^Ct+C+1 = xCt+C  0.

Therefore, given x1 = 0, it holds for all t  N by the principle of mathematical induction. Thus, we have
CC
fkC+i(xkC+i) - fkC+i(-2)  0 - (-100C) = 100C,
i=1 i=1
where k  N. Therefore, for every C steps, ADAM suffers a regret of 100C. More specifically, RT  100CT /C = 100T . Thus, RT /T 0 as T  , which completes the proof.

C PROOF OF THEOREM 2
Theorem 2 generalizes the optimization setting used in Theorem 1. We notice that the example proposed by Reddi et al. (2018) in their Appendix B already satisfies the constraints listed in Theorem 2. Here we provide the setting of the example for completeness.

12

Under review as a conference paper at ICLR 2019

Proof. Consider the setting where ft are linear functions and F = [-1, 1]. In particular, we define the following function sequence:

ft(x) =

Cx, for t mod C = 1; -x, otherwise,

where C  N, C mod 2 = 0 satisfies the following:

(1 - 1) 1C-1C  1 - 1C-1,

2(C-2)/2C2  1,

3(1 - 1)

 1 - C-1 1+

2 1 - 2

1-

+

1C/2-1

<

C ,

1 - 1 3

 where  = 1/ 2 < 1. It is not hard to see that these conditions hold for large constant C that
depends on 1 and 2. According to the proof given by Reddi et al. (2018) in their Appendix B, in such a setting RT /T 0 as T  , which completes the proof.

D PROOF OF THEOREM 3

The example proposed by Reddi et al. (2018) in their Appendix C already satisfies the constraints listed in Theorem 3. Here we provide the setting of the example for completeness.

Proof. Let  be an arbitrary small positive constant. Consider the following one dimensional

stochastic optimization setting over the domain [-1, 1]. At each time step t, the function ft(x)

is chosen as follows:

ft(x) =

C x,

with

probability

p

:=

1+ C+1

-x, with probability 1 - p,

where C is a large constant that depends on 1, 2 and . The expected function is F (x) = x. Thus the optimal point over [-1, 1] is x = -1. The step taken by ADAM is

t

=

-t (1mt-1 + (1 - 1) gt) . 2vt-1 + (1 - 2) gt2

According to the proof given by Reddi et al. (2018) in their Appendix C, there exists a large enough C such that E[t]  0, which then implies that the ADAM's step keep drifting away from the optimal solution x = -1. Note that there is no limitation of the initial step size  by now. Therefore, we
complete the proof.

E PROOF OF THEOREM 4

This theorem can be proved with similar approach of Reddi et al. (2018) with some additional work.

Proof. Let x = arg minxF

T t=1

ft(x),

which

exists

since F

is closed and convex.

We begin

with the following observation:

xt+1 = F,diag(t-1)(xt - t

mt)

=

min
xF

t-1/2

(x - (xt - t

Using Lemma 1 with u1 = xt+1 and u2 = x, we have the following:

mt)) .

t-1/2

(xt+1 - x) 2  = =

t-1/2 (xt - t mt - x) 2
t-1/2 (xt - x) 2 + t1/2 mt 2 - 2 mt, xt - x
t-1/2 (xt - x) 2 + t1/2 mt 2 - 2 1tmt-1 + (1 - 1t)gt, xt - x .

13

Under review as a conference paper at ICLR 2019

Rearranging the above inequality, we have

gt, xt - x

1 2(1 - 1t)

t-1/2

(xt - x) 2 - t-1/2

(xt+1 - x) 2

1 +
2(1 - 1t)

t1/2

mt

2 + 1t 1 - 1t

mt-1, xt - x

1 
2(1 - 1t)

t-1/2

(xt - x) 2 - t-1/2

(xt+1 - x) 2

1 +
2(1 - 1t)

t1/2

mt

2 + 1t 2(1 - 1t)

t1/2

mt-1 2

+

1t 2(1 - 1t)

t-1/2

(xt - x) 2.

(4)

The second inequality follows from simple application of Cauchy-Schwarz and Young's inequal-
ity. We now use the standard approach of bounding the regret at each step using convexity of the functions {ft}tT=1 in the following manner:

TT

ft (xt) - ft (x) 

gt, xt - x

t=1 t=1

T


1

t=1 2(1 - 1t)

t-1/2

(xt - x) 2 - t-1/2

1 +
2(1 - 1t)

t1/2

mt

2 + 1t 2(1 - 1t)

t1/2

+

1t 2(1 - 1t)

t-1/2

(xt - x) 2 .

(xt+1 - x) 2 mt-1 2

(5)

The first inequality is due to the convexity of functions {ft}tT=1. The second inequality follows from the bound in Equation (4). For further bounding this inequality, we need the following intermedia
result.

Lemma 3. For the parameter settings and conditions assumed in Theorem 4, we have

T t=1

1 2(1 - 1t)

t1/2

mt

2 + 1t 2(1 - 1t)

t1/2

mt-1 2



 (2 T

-

1) RG22

.

1 - 1

Proof. By definition of t, we have 
L  t t   R. 14

Under review as a conference paper at ICLR 2019

Hence,

T t=1

1 2(1 - 1t)

t1/2

mt

2

+

1t 2(1 - 1t)

t1/2

mt-1 2

T

t=1

R  2(1 - 1t) t

mt

2 + 1tR  2(1 - 1t) t

mt-1

2

T

t=1

R  2(1 - 1) t

mt

2 + R  2(1 - 1) t

mt-1

2

= R

T

mt

2
+

T

mt-1 2

2(1 - 1) t=1 t

t=1

t

 R

1

2(1 - 1) T

T

mt

t-1/4

21 + T

t=1

T2
mt-1 t-1/4
t=1

 R 2(1 - 1)T

TT T

T

mt 2 · t-1/2 +

mt-1 2 ·

t-1/2

t=1 t=1 t=1

t=1

 RG22

T
t-1/2

(1 - 1) t=1

  (2 T

- 1)

RG22

.

(1 - 1)

The second inequality is due to 1t  1 < 1. The third inequality follows from Jensen inequality and the fourth inequality follows from Cauchy-Schwarz inequality. The fifth inequality follows from Lemma 2 and m0 = 0. The last inequality is due to the following upper bound:

T 1  1 +

T

dt

 =2 T

- 1.

t=1 t

t=1 t

We complete the proof of this lemma.

We now return to the proof of Theorem 4. Using the above lemma in Equation (5), we have

T
ft (xt) - ft (x)

t=1

T


1

t=1 2(1 - 1t)

t-1/2

(xt - x) 2 - t-1/2

(xt+1 - x) 2

+

1t 2(1 - 1t)

t-1/2

(xt - x) 2

+

 (2 T

-

1)

RG22 1 - 1

1 2(1 - 1)

t-1/2

T
(x1 - x) 2 +
t=2

t-1/2

(xt - x) 2 - t--11/2

T
+

1t

t=1 2(1 - 1)

t-1/2

(xt - x)

 2 + (2 T

- 1) RG22 1 - 1

1 =
2(1 - 1)

d Td

1-,i1(x1,i - xi)2 +

(xt,i - xi )2 t-,i1 - t--11,i

i=1 t=2 i=1

Td
+ 1t(xt,i - xi)2t-,i1
t=1 i=1

+

 (2 T

-

1)

RG22 1 - 1

.

(xt - x) 2 (6)

15

Under review as a conference paper at ICLR 2019

The second inequality use the fact that 1t  1 < 1. In order to further simplify the bound in Equation (6), we need to use telescopic sum. We observe that, by definition of t, we have
t-,i1  t--11,i.
Using the D bound on the feasible region and making use of the above property in Equation (6), we have

T
ft (xt) - ft (x)

t=1

 D2 2(1 - 1)

d Td
1-,i1 +
i=1 t=2 i=1

t-,i1 - t--11,i

Td
+ 1tt-,i1
t=1 i=1

+

 (2 T

-

1) RG22

1 - 1

=

 D2 T 2(1 - 1)

d
^T-,1i
i=1

+

D2 2(1 - 1)

T t=1

d
1tt-,i1
i=1

 + (2 T

-

1)

R 1-

G22 1

.

The equality follows from simple telescopic sum, which yields the desired result. It is easy to see that the regret of ADABOUND is upper bounded by O( T ).

F AMSBOUND

Algorithm 3 AMSBOUND

Input: x1  F , initial step size , {1t}Tt=1, 2, lower bound function l, upper bound function u

1: Set m0 = 0, v0 = 0 and v^0 = 0

2: for t = 1 to T do

3: gt = ft(xt)

4: mt = 1tmt-1 + (1 - 1t)gt

5: vt = 2vt-1 + (1 - 2)gt2

6: v^t = max(v^t-1, vt) and Vt = diag(v^t)



7:  = max{l(t), min{u(t), / Vt}} and t = / t

8: xt+1 = F,diag(t-1)(xt - t mt) 9: end for

Theorem 5. Let {xt} and {vt} be the sequences for all t  [T ] and 1/ 2 < 1. Suppose l(t +

obtained from 1)  l(t) >

Algorithm 3, 0, r(t + 1)

1 = 11,  r(t), l

1t (t)

 

1 

as t  , r(t)   as t  , L = l(1) and R = r(1). Assume that x - y   D

for all x, y  F and ft(x)  G2 for all t  [T ] and x  F . For xt generated using the

ADABOUND algorithm, we have the following bound on the regret

RT



 D2 T 2(1 - 1)

d
T-,1i +
i=1

D2 2(1 - 1)

T t=1

d 1tt-,i1 + (2 T
i=1

-

1)

RG22 1 - 1

.

The regret of AMSBOUND has the same upper bound with that of ADABOUND.4

G EMPIRICAL STUDY OF BOUND FUNCTIONS

Here we provide an empirical study on different kinds of bound functions. We consider the following two key factors of the bound function: convergence speed and convergence target. The former one affects how "fast" our algorithms transform from adaptive methods to SGD(M), while the latter one reflects the final step size of SGD(M). In particular, we consider the following bound functions:

l(t)

=

(1

-

(1

-

1 )t

+

), 1

r (t)

=

(1

+

(1

-

1 )t

+

), 1

4One may refer to Appendix E as the process of the proof is almost the same.

16

Under review as a conference paper at ICLR 2019

where the above functions will converge to  and the larger  results in lower convergence speed.

Test Accuracy % Test Accuracy %

95

90

85

80 beta=1-1/10

beta=1-1/20

beta=1-1/30

75 0

25 50 75 100 125 150 175

Epoch

(a) ADABOUND with  = 0.1

95 90 85 80 75 70 65 60 0

25 50

75 100 Epoch

beta=1-1/10 beta=1-1/20 beta=1-1/30 125 150 175

(b) ADABOUND with  = 1

Figure 5: Test accuracy of ADABOUND with different  using ResNet-32 on CIFAR-10

We first investigate the impact of convergence speed. We conduct an experiment of ADABOUND on

CIFAR-10 dataset  is chosen from

with the {1, 0.1}.

ResNet-32 The results

model, where  is chosen are shown in Figure 5. We

in {1 - can see

1 10

,

that

1fo-r a21s0p,e1ci-fic310 },atnhde

performances with different  are almost the same . It indicates that the convergence speed of bound

functions does not affect the final result to much extent. We find a  in [1, 2] usually contributes

to a strong performance across all models.

Test Accuracy %

95 90 85 80 75 70 AdaBound,a*=0.1
AdaBound,a*=1.0 65 0 25 50 75Epoch 100 125 150 175

Figure 6: Test accuracy of ADABOUND at different  using ResNet-32 on CIFAR-10

Next, we investigate the impact of convergence target and the results are displayed in Figure 6. The algorithm with  = 0.1 (which is also the optimal step size of SGDM found by a grid search) does perform better in the early stage, while the final generalization abilities of two variants are similar.

Test Accuracy %

95 90 85 80 75 70 SGDM,a*=0.1
AdaBound,a*=1.0 65 0 25 50 75 100 125 150 175
Epoch

Figure 7: Comparison of test accuracy between SGDM with  = 0.1 and AdaBound with  = 1.0
We further compare the worse variant where  = 1 with SGDM where its step size is set to optimal value 0.1. The results are shown in Figure 7. We can see that the behaviors of the above two methods are similar in the both early and final stage.

17

Under review as a conference paper at ICLR 2019 To summarize, the convergence speed does not influence the final performance of the methods while the convergence target somehow affects the training speed in early stage but does not exert obvious impact on the final performance. Moreover, ADABOUND can achieve a similar training speed to SGDM even if it is not carefully fine-tuned. Therefore, we can expect a better performance by using ADABOUND regardless of the choice of bound functions and the two factors we discussed are not very important to the final performance.
18

