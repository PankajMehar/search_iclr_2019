Under review as a conference paper at ICLR 2019
CONTEXT-ADAPTIVE ENTROPY MODEL FOR END-TOEND OPTIMIZED IMAGE COMPRESSION
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a context-adaptive entropy model for use in end-to-end optimized image compression. Our model exploits two types of contexts, bit-consuming contexts and bit-free contexts, distinguished based upon whether additional bit allocation is required. Based on these contexts, we allow the model to more accurately estimate the distribution of each latent representation with a more generalized form of the approximation models, which accordingly leads to an enhanced compression performance. Based on the experimental results, the proposed method outperforms the traditional image codecs, such as BPG and JPEG2000, as well as other previous artificial-neural-network (ANN) based approaches, in terms of the peak signal-to-noise ratio (PSNR) and multi-scale structural similarity (MSSSIM) index.
1 INTRODUCTION
Recently, artificial neural networks (ANNs) have been applied in various areas and have achieved a number of breakthroughs resulting from their superior optimization and representation learning performance. In particular, for various problems that are sufficiently straightforward that they can be solved within a short period of time by hand, a number of ANN-based studies have been conducted and significant progress has been made. With regard to image compression, however, relatively slow progress has been made owing to its complicated target problems. A number of works, focusing on the quality enhancement of reconstructed images, were proposed. For instance, certain approaches (Dong et al., 2015; Svoboda et al., 2016; Zhang et al., 2017) have been proposed to reduce artifacts caused by image compression, relying on the superior image restoration capability of an ANN. Although it is indisputable that artifact reduction is one of the most promising areas exploiting the advantages of ANNs, such approaches can be viewed as a type of post-processing, rather than image compression itself.
Regarding ANN-based image compression, the previous methods can be divided into two types. First, as a consequence of the recent success of generative models, some image compression approaches targeting the superior perceptual quality (Agustsson et al., 2018; Santurkar et al., 2018; Rippel & Bourdev, 2017) have been proposed. The basic idea here is that learning the distribution of natural images enables a very high compression level without severe perceptual loss by allowing the generation of image components, such as textures, which do not highly affect the structure or the perceptual quality of the reconstructed images. Although the generated images are very realistic, the acceptability of the machine-created image components eventually becomes somewhat applicationdependent. Meanwhile, a few end-to-end optimized ANN-based approaches (Toderici et al., 2017; Johnston et al., 2018; Balle´ et al., 2017; Theis et al., 2017; Balle´ et al., 2018), without generative models, have been proposed. In these approaches, unlike traditional codecs comprising separate tools, such as prediction, transform, and quantization, a comprehensive solution covering all functions has been sought after using end-to-end optimization. Toderici et al. (2017)'s approach exploits a small number of latent binary representations to contain the compressed information in every step, and each step increasingly stacks the additional latent representations to achieve a progressive improvement in quality of teh reconstructed images. Johnston et al. (2018) improved the compression performance by enhancing operation methods of the networks developed by Toderici et al. (2017). Although Toderici et al. (2017); Johnston et al. (2018) provided novel frameworks suitable to quality control using a single trained network, the increasing number of iteration steps to obtain higher image quality can be a burden to certain applications. In contrast to the approaches developed by
1

Under review as a conference paper at ICLR 2019

Figure 1: Comparison of sample test results including the ground truth, our method, Balle´ et al. (2018)'s approach, BPG, and JPEG2000.

Toderici et al. (2017) and Johnston et al. (2018), which extract binary representations with as high an entropy as possible, Balle´ et al. (2017), Theis et al. (2017), and Balle´ et al. (2018) regard the image compression problem as being how to retrieve discrete latent representations having as low an entropy as possible. In other words, the target problem of the former methods can be viewed as how to include as much information as possible in a fixed number of representations, whereas the latter is simply how to reduce the expected bit-rate when a sufficient number of representations are given, assuming that the low entropy corresponds to small number of bits from the entropy coder. To solve the second target problem, Balle´ et al. (2017), Theis et al. (2017), and Balle´ et al. (2018) adopt their own entropy models to approximate the actual distributions of the discrete latent representations. More specifically, Balle´ et al. (2017) and Theis et al. (2017) proposed novel frameworks that exploit the entropy models, and proved their performance capabilities by comparing the results with those of conventional codecs such as JPEG2000. Whereas Balle´ et al. (2017) and Theis et al. (2017) assume that each representation has a fixed distribution, Balle´ et al. (2018) introduced an input-adaptive entropy model that estimates the scale of the distribution for each representation. This idea is based on the characteristics of natural images in which the scales of the representations vary together in adjacent areas. They provided test results that outperform all previous ANN-based approaches, and reach very close to those of BPG (Bellard, 2014), which is known as a subset of HEVC (ISO/IEC 23008-2, ITU-T H.265), used for image compression.
One of the principle elements in end-to-end optimized image compression is the trainable entropy model used for the latent representations. Because the actual distributions of latent representations are unknown, the entropy models provide the means to estimate the required bits for encoding the latent representations by approximating their distributions. When an input image x is transformed into a latent representation y and then uniformly quantized into y^, the simple entropy model can be represented by py^(y^), as described by Balle´ et al. (2018). When the actual marginal distribution of y^ is denoted as m(y^), the rate estimation, calculated through cross entropy using the entropy model, py^(y^), can be represented as shown in equation (1), and can be decomposed into the actual entropy of y^ and the additional bits owing to a mismatch between the actual distributions and the their approximations. Therefore, decreasing the rate term R during the training process allows the entropy model py^(y^) to approximate m(y^) as closely as possible, and let the other parameters transform x into y properly such that the actual entropy of y^ becomes small.

R = Ey^m[- log2 py^(y^)] = H(m) + DKL(m||py^).

(1)

In terms of KL-divergence, R is minimized when py^(y^) becomes perfectly matched with the actual distribution m(y^). This means that the compression performance of the methods essentially depends on the capacity of the entropy model. To enhance the capacity, we propose a new entropy model that exploits two types of contexts, bit-consuming and bit-free contexts, distinguished according to whether additional bit allocation is required. Utilizing these two contexts, we allow the model to more accurately estimate the distribution of each latent representation through the use of a more generalized form of the entropy models, and thus more effectively reduce the spatial dependencies

2

Under review as a conference paper at ICLR 2019
among the adjacent latent representations. Figure 1 demonstrates a comparison of the compression results of our method to those of other previous approaches. The contributions of our work are as follows:
· We propose a new context-adaptive entropy model framework that incorporates the two different types of contexts.
· We provide the first test results that outperform the widely used conventional image codec BPG in terms of the PSNR, within the domain of ANN-based image compression.
· We discuss the directions of improvement in the proposed methods in terms of the model capacity and the level of the contexts.
Note that we follow a number of notations given by Balle´ et al. (2018) because our approach can be viewed as an extension of their work, in that we exploit the same R-D optimization framework. The rest of this paper is organized as follows. In Section 2, we introduce the key approaches of end-to-end optimized image compression and propose the context-adaptive entropy model. Section 3 demonstrates the structure of the encoder and decoder models used, and the experimental setup and results are then given in section 4. Finally, in Section 5, we discuss the current state of our work and directions for improvement.
2 END-TO-END OPTIMIZATION BASED ON CONTEXT-ADAPTIVE ENTROPY
MODELS
2.1 PREVIOUS ENTROPY MODELS
Since they were first proposed by Balle´ et al. (2017) and Theis et al. (2017), entropy models, which approximate the distribution of discrete latent representations, have noticeably improved the image compression performance of ANN-based approaches. Balle´ et al. (2017) assumes the entropy models of the latent representations as non-parametric models, while whereas Theis et al. (2017) adopted a Gaussian scale mixture model composed of six weighted zero-mean Gaussian models per representation. Although they assume different forms of entropy models, they have a common feature in that both concentrate on learning the distributions of the representations without considering input adaptivity. In other words, once the entropy models are trained, the trained model parameters for the representations are fixed for any input during the test time. Balle´ et al. (2018), in contrast, introduced a novel entropy model that adaptively estimates the scales of the representations based on input. They assume that the scales of the latent representations from the natural images tend to move together within an adjacent area. To reduce this redundancy, they use a small amount of additional information by which the proper scale parameters (standard deviations) of the latent representations are estimated. In addition to the scale estimation, Balle´ et al. (2018) have also shown that when the prior probability density function (PDF) for each representation in a continuous domain is convolved with a standard uniform density function, it approximates the prior probability mass function (PMF) of the discrete latent representation, which is uniformly quantized by rounding, much more closely. For training, a uniform noise is added to each latent representation so as to fit the distribution of these noisy representations into the mentioned PMF-approximating functions. Using these approaches, Balle´ et al. (2018) achieved a state-of-the-art compression performance, close to that of BPG.
2.2 SPATIAL DEPENDENCIES OF THE LATENT VARIABLES
The latent representations, when transformed through a convolutional neural network, essentially contain spatial dependencies because the same convolutional filters are shared across the spatial regions, and natural images have various factors in common within adjacent regions. Balle´ et al. (2018) successfully captured these spatial dependencies and enhanced the compression performance by input-adaptively estimating standard deviations of the latent representations. Taking a step forward, we generalize the form of the estimated distribution by allowing, in addition to the standard deviation, the mean estimation utilizing the contexts. For instance, assuming that certain representations tend to have similar values within a spatially adjacent area, when all neighborhood representations have a value of 10, we can intuitively guess that, for the current representation, the chance
3

Under review as a conference paper at ICLR 2019
Figure 2: Examples of latent representations and their normalized versions for the two cases (the first two images show the results in which only the standard deviations are estimated using side information, whereas the last two images show the results in which the mu and standard deviation are estimated using our method). For a clear demonstration, the latent representations having the highest covariance between the spatially adjacent variables are extracted. Left: the latent representations of y^ from the first case. Middle left: the normalized versions of y^ from the first case, divided by the estimated standard deviation. Middle right: the latent variable of y^ from the second case. Right: the normalized versions of y^ from the second case, shifted and and divided by the estimated mu and the standard deviation.
of having a value equal or similar to 10 is relatively high. This simple estimation will consequently reduce the entropy. Likewise, our method utilizes the given contexts for estimating the mean, as well as the standard deviation, of each latent representation. Note that Toderici et al. (2017), Johnston et al. (2018), and Rippel & Bourdev (2017) also apply context-adaptive entropy coding by estimating the probability of each binary representation. However, these context-adaptive entropy-coding methods can be viewed as separate components, rather than one end-to-end optimization component because their probability estimation does not directly contribute to the rate term of the R-D optimization framework. Figure 2 visualizes the latent variables y^ and their normalized versions of the two different approaches, one estimating only the standard deviation parameters and the other estimating both the mu and standard deviation parameters with the two types of mentioned contexts. The visualization shows that the spatial dependency can be removed more effectively when the mu is estimated along with the given contexts.
2.3 CONTEXT-ADAPTIVE ENTROPY MODEL
The optimization problem described in this paper is similar with Balle´ et al. (2018), in that the input x is transformed into y having a low entropy, and the spatial dependencies of y are captured into z^. Therefore, we also use four fundamental parametric transform functions: an analysis transform ga(x; g) to transform x into a latent representation y, a synthesis transform gs(y^; g) to reconstruct image x^, an analysis transform ha(y^; h) to capture the spatial redundancies of y^ into a latent representation z, and a synthesis transform hs(z^; h) used to generate the contexts for the model estimation. Note that hs does not estimate the standard deviations of the representations directly as in Balle´ et al. (2018)'s approach. In our method, instead, hs generates the context c , one of the two types of contexts for estimating the distribution. These two types of contexts are described in this section. Balle´ et al. (2018) analyzed the optimization problem from the viewpoint of the variational autoencoder (Kingma & Welling (2014); Rezende et al. (2014)), and showed that the minimization of the KL-divergence is the same problem as the R-D optimization of image compression. Basically, we follow the same concept; however, for training, we use the discrete representations on the conditions instead of the noisy representations, and thus the noisy representations are only used as the inputs to the entropy models. Empirically, we found that using discrete representations on the conditions show better results. These results might come from removing the mismatches of the conditions between the training and testing, thereby enhancing the training capacity by limiting the affect of the uniform noise only to help the approximation to the probability mass functions. We use the gradient overriding method with the identity function, as in Theis et al. (2017), to deal with the disconti-
4

Under review as a conference paper at ICLR 2019

nuities from the uniform quantization. The resulting objective function used in this paper is given in equation (2). The total loss consists of two terms representing the rates and distortions, and the coefficient  controls the balance between the rate and distortion during the R-D optimization:

L = R+D with R = Expx Ey~,z~q - log py~|z^(y~ | z^) - log pz~(z~) , D = Expx - log px|y^(x | y^)

(2)

Here, the noisy representations of y~ and z~ follow the standard uniform distribution, the mean values of which are y and z, respectively, when y and z are the result of the transforms ga and ha, repectively. Note that the input to ha is y^, which is a uniformly quantized representation of y, rather than the noisy representation y~. Q denotes the uniform quantization function, for which we simply use a
rounding function:

q(y~, z~ | x, g, h) =

U

y~i

|

yi

-

1 2

,

yi

+

1 2

·

U

z~j

|

zj

-

1 2

,

zj

+

1 2

ij

with y = ga(x; g), y^ = Q(y), z = ha(y^; h).

(3)

The rate term represents the expected bits calculated using the entropy models of py~|z^ and pz~. Note that py~|z^ and pz~ are eventually the approximations of py^|z^ and pz^, respectively. Equation (4) represents the entropy model for approximating the required bits for y^. The model is based on the
Gaussian model, which not only has the standard deviation parameter i, but also the mu parameter
µi. The values of µi and i are estimated from the two types of given contexts based on the function
f , the distribution estimator, in a deterministic manner. The two types of contexts, bit-consuming
and bit-free contexts, for estimating the distribution of a certain representation are denoted as ci and ci . E extracts ci from c , the result of transform hs. In contrast to ci, no additional bit allocation is required for ci . Instead, we simply utilize the known (already entropy-coded or decoded) subset of y^, denoted as y^ . Here, ci is extracted from y^ by the extractor E . We assume that the entropy coder and the decoder sequentially process y^i in the same specific order, such as with raster
scanning, and thus y^ given to the encoder and decoder can always be identical when processing
the same y^i. A formal expression of this is as follows:

py~|z^(y~ | z^, h) =

N

µi, i2

U

-

1 2

,

1 2

i

with µi, i = f (ci, ci ),

ci = E (hs(z^; h), i),

ci = E ( y^ , i),

z^ = Q(z)

(y~i)

(4)

In the case of z^, a simple entropy model is used. We assumed that the model follows zero-mean Gaussian distributions which have a trainable . Note that z^ is regarded as side information and it contributes a very small amount of the total bit-rate, as described by Balle´ et al. (2018), and thus we use this simpler version of the entropy model, rather than a more complex model, for end-to-end optimization over all parameters of the proposed method:

pz~(z~) =

N

0, j2

U

-

1 2

,

1 2

(z~j )

j

(5)

Note that actual entropy coding or decoding processes are not necessarily required for training or encoding because the rate term is not the amount of real bits, but an estimation calculated from the

5

Under review as a conference paper at ICLR 2019

Encoder

entropy 

model for

EC

 z 

z 
z  U|Q  
z
ha

hs

c
E'  c
i
entropy model for
y   z 

y  y 

  

x

y
ga U|Q

 c i  E''
y 

i , i

EC

Decoder

ED  
z 

entropy model for
z 

hs

c
E'  c
i
entropy model for
y   z 

 c i  E''
y 
ED

i , i

y 
 

gs

x  
 

Figure 3: Encoder-decoder model of the proposed method. The left block represents the encoder side, whereas the right block represents the deocder side. The small icons between them represent the entropy-coded bitstreams. EC and ED represent entropy coding and entropy decoding, and U | Q represents the addition of uniform noise to y or a uniform quantization of y. Noisy representations are used only for training as inputs to the entropy models, and are illustrated with the dotted lines.

entropy models, as mentioned previously. We calculate the distortion term using the mean squared error (MSE)1, assuming that px|y^ follows a Gaussian distribution as a widely used distortion metric.
3 ENCODER-DECODER MODEL
This section describes the basic structure of the proposed encoder-decoder model. On the encoder side, an input image is transformed into latent representations, quantized, and then entropy-coded using the trained entropy models. In contrast, the decoder first applies entropy decoding with the same entropy models used for the encoder, and reconstructs the image from the latent representations, as illustrated in figure 3. It is assumed that all parameters that appear in this section were already trained. The structure of the encoder-decoder model basically includes ga and gs in charge of the transform of x into y and its inverse transform, respectively. The transformed y is uniformly quantized into y^ by rounding. Note that, in the case of approaches based on the entropy models, unlike traditional codecs, tuning the quantization steps is usually unnecessary because the scales of the representations are optimized together through training. The other components between ga and gs carry out the role of entropy coding (or decoding) with the shared entropy models and underlying context preparation processes. More specifically, the entropy model estimates the distribution of each y^i individually, in which µi and i are estimated with the two types of given contexts, ci and ci . Among these contexts, c can be viewed as side information, which requires an additional bit allocation. To reduce the required bit-rate for carrying c , the latent representation z, transformed from y^, is quantized and entropy-coded by its own entropy model, as specified in section 2.3. On the other hand, ci is extracted from y^ , without any additional bit allocation. Note that y^ varies as the entropy coding or decoding progresses, but is always identical for processing the same y^i in both the encoder and decoder, as described in 2.3. The parameters of hs and the entropy models are simply shared by both the encoder and the decoder. Note that the inputs to the entropy models during training are the noisy representations, as illustrated with the dotted line in figure 3, to allow the entropy model to approximate the probability mass functions of the discrete representations.
1We also provide supplemental experiment results in which an MS-SSIM (Wang et al. (2003)) based distortion term is used for optimization.
6

Under review as a conference paper at ICLR 2019

input image conv Nx5x5/2
GDN conv Nx5x5/2
GDN conv Nx5x5/2
GDN conv Mx5x5/2

reconstruction conv 3x5x5/2
IGDN conv Nx5x5/2
IGDN conv Nx5x5/2
IGDN conv Nx5x5/2

ga x
gs x  

ha

Qy y 

EC E''
ED

c i
 

i , i

 y  f

concat

c
i E'  

hs

 c
 

exp conv Mx3x3/1
ReLU conv Nx5x5/2
ReLU conv Nx5x5/2

abs conv Nx3x3/1
ReLU conv Nx5x5/2
ReLU conv Nx5x5/2

z QT

z  

  EC

ED

z  
 

i i MM

f

c i
  4

c i
 

4
M

M

unseen area

FCN 2*M leakyReLU conv Mx3x3/1 leakyReLU conv Mx3x3/2 leakyReLU conv Mx3x3/1

Figure 4: Implementation of the proposed method. We basically use the convolutional autoencoder structure, and the distribution estimator f is also implemented using convolutional neural networks. The notations of the convolutional layer follow Balle´ et al. (2018): the number of filters × filter height × filter width / the downscale or upscale factor, where  and  denote the up and downscaling, respectively. For up or downscaling, we use the transposed convolution.
4 EXPERIMENTS
4.1 IMPLEMENTATION
We use a convolutional neural networks to implement the analysis transform and the synthesis transform functions, ga, gs, ha, and hs. The structures of the implemented networks follow the same structures of Balle´ et al. (2018), except that we use the exponentiation operator instead of an absolute operator at the end of hs. Based on Balle´ et al. (2018)'s structure, we added the components to estimate the distribution of each y^i, as shown in figure 4. Herein, we represent a uniform quantization (round) as "Q," entropy coding as "EC," and entropy decoding as "ED." The distribution estimator is denoted as f , and is also implemented using the convolutional layers which takes channel-wise concatenated ci and ci as inputs and provides estimated µi and i as results. Note that the same ci and ci are shared for all y^is located at the same spatial position. In other words, we let E extract all spatially adjacent elements from c across the channels to retrieve ci and likewise let E extract all adjacent known elements from y^ for ci . This could have the effect of capturing the remaining correlations among the different channels. In short, when M is the total number of channels of y, we let f estimate all M distributions of y^is, which are located at the same spatial position, using only a single step, thereby allowing the total number of estimations to be reduced. Furthermore, the parameters of f are shared for all spatial positions of y^, and thus only one trained f per  is necessary to process any sized images. In the case of training, however, collecting the results from the all spatial positions to calculate the rate term becomes a significant burden, despite the simplifications mentioned above. To reduce this burden, we designate a certain number (16 in the experiments) of random spatial points as the representatives per training step, to calculate the rate term easily. Note that we let these random points contribute solely to the rate term, whereas the distortion is still calculated over all of the images.
Because y is a three-dimensional array in our implementation, index i can be represented as three indexes, k, l, and m, representing the horizontal index, the vertical index, and the channel index, respectively. When the current position is given as (k, l, m), E extracts c[k-2...k+1],[l-3...l],[1...M] as ci, and E extracts y^ [k-2...k+1],[l-3...l],[1...M] as ci , when y^ represents the known area of y^. Note that we filled in the unknown area of y^ with zeros, to maintain the dimensions of y^
7

Under review as a conference paper at ICLR 2019

identical to those of y^. Consequently, ci [3...4],4,[1...M] are always padded with zeros. To keep the dimensions of the estimation results to the inputs, the marginal areas of c and y^ are also set to zeros. Note that when training or encoding, ci can be extracted using simple 4×4×M windows and binary masks, thereby enabling parallel processing, whereas a sequential reconstruction is inevitable for decoding.
Another implementation technique used to reduce the implementation cost is combining the lightweight entropy model with the proposed model. The lightweight entropy model assumes that the representations follow a zero-mean Gaussian model with the estimated standard deviations, which is very similar with Balle´ et al. (2018)'s approach. We utilize this hybrid approach for the top four cases, in bit-rate descending order, of the nine configurations, based on the assumption that for the higher-quality compression, the number of sparse representations having a very low spatial dependency increases, and thus a direct scale estimation provides sufficient performance for these added representations. For implementation, we separate the latent representation y into two parts, y1 and y2, and two different entropy models are applied for them. Note that the parameters of ga, gs, ha, and hs are shared, and all parameters are still trained together. The detailed structure and experimental settings are described in appendix 6.1.
The number of parameters N and M are set to 128 and 192, respectively, for the five lower configurations, whereas 2-3 times more parameters, described in appendix 6.1, are used for the higher configurations. Tensorflow and Python were used to setup the overall network structures, and for the actual entropy coding and decoding using the estimated model parameters, we implemented an arithmetic coder and decoder, for which the source code of the "Reference arithmetic coding" project2 was used as the base code.

4.2 EXPERIMENTAL ENVIRONMENTS

We optimized the networks using two different types of distortion terms, one with MSE and the other with MS-SSIM. For each distortion type, the average bits per pixel (BPP) and the distortion, PSNR, and MS-SSIM over the test set are measured for each of the nine R-D configurations. Therefore, a total of 18 networks are trained and evaluated within the experimental environments, as explained below:

· For training, we used 256×256 patches extracted from 32,420 randomly selected YFCC100m (Thomee et al. (2016)) images. We extracted one patch per image, and the extracted regions were randomly chosen. Each batch consists of eight images, and 1M iterations of the training steps were conducted, applying the ADAM optimizer (Kingma & Ba (2015)). We set the initial learning rate to 5×10-5, and reduced the rate by half every 50,000 iterations for the last 200,000 iterations. Note that, in the case of the four higher configurations, in which the hybrid entropy model is used, 1M iterations of pre-training steps were conducted using the learning rate of 1×10-5. Although we previously indicated that the total loss is the sum of R and D for a simple explanation, we tuned the balancing parameter  in a similar way as Theis et al. (2017), as indicated in equation (6). Here, Wy · Hy · M and Wx · Hx · 3 represent the total number of dimensions of y and x, respectively. We used the  parameters ranging from 0.01 to 0.5.

 1-

L

=

Wy

·

Hy

·

M

R

+

1000

·

Wx

·

Hx

·

D. 3

(6)

· For the evaluation, we measured the average BPP and average quality of the reconstructed images in terms of the PSNR and MS-SSIM over 24 PNG images of the Kodak PhotoCD image dataset (Kodak, 1993). Note that we represent the MS-SSIM results in the form of decibels, as in Balle´ et al. (2018), to increase the discrimination.

4.3 EXPERIMENTAL RESULTS
We compared the test results with other previous methods, including traditional codecs such as BPG and JPEG2000, as well as previous ANN-based approaches such as Theis et al. (2017) and
2Nayuki, "Reference arithmetic coding," https://github.com/nayuki/Reference-arithmetic-coding, Copyright c 2018 Project Nayuki. (MIT License).

8

Under review as a conference paper at ICLR 2019
Figure 5: Rate­distortion curves of the proposed method and competitive methods. The top plot represents the PSNR values as a result of changes in bpp, whereas the bottom plot shows MS-SSIM values in the same manner. Note that MS-SSIM values are converted into decibels(-10 log10(1 - MS-SSIM)) for differentiating the quality levels, in the same manner as in Balle´ et al. (2018). Balle´ et al. (2018). Because two different quality metrics are used, the results are presented with two separate plots. As shown in figure 5, our methods outperform all other previous methods in both metrics. In particular, our models not only outperform Balle´ et al. (2018)'s method, which is believed to be a state-of-the-art ANN-based approach, but we also obtain better results than the widely used conventional image codec, BPG. To the best of our knowledge, these are the first test results of ANN-based image compression that outperforms BPG in terms of the PSNR.
9

Under review as a conference paper at ICLR 2019
More specifically, the compression gains in terms of the BD-rate of PSNR over JPEG2000, Balle´ et al. (2018)'s approach (MSE-optimized), and BPG are 34.08%, 11.87%, and 6.85%, respectively. In the case of MS-SSIM, we found wider gaps of 68.82%, 13.93%, and 49.68%, respectively. Note that we achieved significant gains over traditional codecs in terms of MS-SSIM, although this might be because the dominant target metric of the traditional codec developments have been the PSNR. In other words, they can be viewed as a type of MSE-optimized codec. Even when setting aside the case of MS-SSIM, our results can be viewed as the first point at which ANN-based image compression overtakes the existing traditional image codecs in terms of the compression performance. Supplemental image samples are provided in appendix 6.2.
5 DISCUSSION
Based on previous ANN-based image compression approaches utilizing entropy models (Balle´ et al., 2017; Theis et al., 2017; Balle´ et al., 2018), we extended the entropy model to exploit two different types of contexts. These contexts allow the entropy models to more accurately estimate the distribution of the representations with a more generalized form having both mean and standard deviation parameters. Based on the evaluation results, we showed the superiority of the proposed method. The contexts we utilized are divided into two types. One is a sort of free context, containing the part of the latent variables known to both the encoder and the decoder, whereas the other is the context, which requires additional bit allocation. Because the former is a generally used context in a variety of codecs, and the latter was already verified to help compression using Balle´ et al. (2018)'s approach, our contributions are not the contexts themselves, but can be viewed as providing a framework of entropy models utilizing these contexts.
Although the experiments showed the best results in the ANN-based image compression domain, and are the first results to outperform the BPG in terms of the PSNR, we still have various studies to conduct to further improve the performance. One possible way is generalizing the distribution models underlying the entropy model. Although we enhanced the performance by generalizing the previous entropy models, and have achieved quite acceptable results, the Gaussian-based entropy models apparently have a limited expression power. If more elaborate models, such as the non-parametric models of Balle´ et al. (2018) or Oord et al. (2016), are combined with the context-adaptivity proposed in this paper, they would provide better results by reducing the mismatch between the actual distributions and the approximation models. Another possible way is improving the level of the contexts. Currently, our methods only use low-level representations within very limited adjacent areas. However, if the sufficient capacity of the networks and higher-level contexts are given, a much more accurate estimation could be possible. For instance, if an entropy model understands the structures of human faces, in that they usually have two eyes, between which a symmetry exists, the entropy model could approximate the distributions more accurately when encoding the second eye of a human face by referencing the shape and position of the first given eye. As is widely known, various generative models (Goodfellow et al., 2014; Radford et al., 2016; Zhao et al., 2017) learn the distribution p(x) of the images within a specific domain, such as human faces or bedrooms. In addition, various in-painting methods (Pathak et al., 2016; Yang et al., 2017; Yeh et al., 2017) learn the conditional distribution p(x | context) when the viewed areas are given as context. Although these methods have not been developed for image compression, hopefully such high-level understandings can be utilized sooner or later. Furthermore, the contexts carried using side information can also be extended to some high-level information such as segmentation maps or any other information that helps with compression. Segmentation maps, for instance, may be able to help the entropy models estimate the distribution of a representation discriminatively according to the segment class the representation belongs to.
Traditional codecs have a long development history, and a vast number of hand-crafted heuristics have been stacked thus far, not only for enhancing compression performance, but also for compromising computational complexities. Therefore, ANN-based image compression approaches may not provide satisfactory solutions as of yet, when taking the their high complexity into account. However, considering its much shorter history, we believe that ANN-based image compression has much more potential and possibility in terms of future extension. Although we remain a long way from completion, we hope the proposed context-adaptive entropy model will provide an useful contribution to this area.
10

Under review as a conference paper at ICLR 2019
ACKNOWLEDGMENTS
This work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government (MSIT) (No. 2017-0-00072, Development of Audio/Video Coding and Light Field Media Fundamental Technologies for Ultra Realistic Teramedia).
REFERENCES
Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Generative adversarial networks for extreme learned image compression. arXiv preprint arXiv:1804.02958, 2018. URL http://arxiv.org/abs/1804.02958.
Johannes Balle´, Valero Laparra, and Eero P. Simoncelli. End-to-end optimized image compression. In the 5th Int. Conf. on Learning Representations, 2017. URL http://arxiv.org/abs/ 1611.01704.
Johannes Balle´, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. In the 6th Int. Conf. on Learning Representations, 2018. URL http://arxiv.org/abs/1802.01436.
Fabrice Bellard. Bpg image format, 2014. URL http://bellard.org/bpg/.
Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang Tang. Compression artifacts reduction by a deep convolutional network. In IEEE International Conference on Computer Vision (ICCV), 2015. URL http://arxiv.org/abs/1504.06993.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672­2680. Curran Associates, Inc., 2014. URL http://papers. nips.cc/paper/5423-generative-adversarial-nets.pdf.
ISO/IEC 23008-2, ITU-T H.265. Information technology ­ high efficiency coding and media delivery in heterogeneous environments ­ part 2: High efficiency video coding. Standard, ISO/IEC, 2013.
Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh Singh, Troy Chinen, Sung Jin Hwang, Joel Shor, and George Toderici. Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In the 3rd Int. Conf. on Learning Representations, 2015. URL http://arxiv.org/abs/1412.6980.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In the 2nd Int. Conf. on Learning Representations, 2014. URL http://arxiv.org/abs/1312.6114.
Eastman Kodak. Kodak lossless true color image suite (photocd pcd0992), 1993. URL http: //r0k.us/graphics/kodak/.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In the 33 rd International Conference on Machine Learning, 2016. URL http://arxiv. org/abs/1601.06759.
Deepak Pathak, Philipp Kra¨henbu¨hl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context encoders: Feature learning by inpainting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In the 4th Int. Conf. on Learning Representations, 2016. URL http://arxiv.org/abs/1511.06434.
11

Under review as a conference paper at ICLR 2019
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In the 31st Int. Conf. on Machine Learning, 2014. URL http://arxiv.org/abs/1401.4082.
Oren Rippel and Lubomir Bourdev. Real-time adaptive image compression. In International Conference on Machine Learning, 2017. URL http://arxiv.org/abs/1705.05823.
Shibani Santurkar, David M. Budden, and Nir Shavit. Generative compression. In The 33rd Picture Coding Symposium, 2018. URL http://arxiv.org/abs/1703.01467.
Pavel Svoboda, Michal Hradis, David Barina, and Pavel Zemc´ik. Compression artifacts removal using convolutional neural networks. In WSCG 24th International Conference on Computer Graphics, Visualization and Computer Vision, 2016. URL http://arxiv.org/abs/1605. 00366.
Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Husza´r. Lossy image compression with compressive autoencoders. In the 5th Int. Conf. on Learning Representations, 2017. URL http://arxiv.org/abs/1703.00395.
Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. The new data in multimedia research. Communications of the ACM, 59(2):64­73, 2016.
George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and Michele Covell. Full resolution image compression with recurrent neural networks. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. doi: 10.1109/CVPR.2017.577. URL http://arxiv.org/abs/1608.05148.
Zhou Wang, Eero P. Simoncelli, and Alan C. Bovik. Multiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003. doi: 10.1109/ACSSC.2003.1292216.
Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, and Hao Li. High-resolution image inpainting using multi-scale neural patch synthesis. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Raymond A. Yeh, Chen Chen, Teck-Yian Lim, Alexander G. Schwing, Mark Hasegawa-Johnson, and Minh N. Do. Semantic image inpainting with deep generative models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6882­6890. IEEE Computer Society, 2017.
Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising. IEEE Transactions on Image Processing, 26(7):3142­3155, 2017. doi: 10.1109/TIP.2017.2662206. URL http://arxiv.org/abs/ 1608.03981.
Junbo Jake Zhao, Michae¨l Mathieu, and Yann LeCun. Energy-based generative adversarial network. In the 5th Int. Conf. on Learning Representations, 2017. URL http://arxiv.org/abs/ 1609.03126.
12

Under review as a conference paper at ICLR 2019

input image conv Nx5x5/2
GDN conv Nx5x5/2
GDN conv Nx5x5/2
GDN conv Mx5x5/2

shared ga
x
shared gs
x  

y1
yS
y2

Q y^1 Q y^2

shared ha
y 

EC/ED (zero-mean 2 Gaussian)  

EC/ED (proposed
model)


c

shared hs

concat

y^2
S

y  y^1

concat abs
conv Nx3x3/1 ReLU
conv Nx5x5/2 ReLU
conv Nx5x5/2

z QT

z  

  EC

ED

z  

exp conv Mx3x3/1
ReLU conv Nx5x5/2
ReLU conv Nx5x5/2

reconstruction conv 3x5x5/2
IGDN conv Nx5x5/2
IGDN conv Nx5x5/2
IGDN conv Nx5x5/2

Figure 6: The structure of the hybrid network for higher bit-rate environments. The same notations as in the figure 4 are used. The representation y is divided into two parts and quantized. One of the resulting parts, y^1, is encoded using the proposed model, whereas the other, y^2, is encoded using a simpler model in which only the standard deviations are estimated using side information. The detailed structure of the proposed model is illustrated in figure 4. All concatenation and split operations are performed in a channel-wise manner.

6 APPENDIX
6.1 HYBRID NETWORK FOR HIGHER BIT-RATE COMPRESSIONS
We combined the lightweight entropy model with the context-adaptive entropy model to reduce the implementation costs for high-bpp configurations. The lightweight model exploits the scale (standard deviation) estimation, assuming that the PMF approximations of the quantized representations follow zero-mean Gaussian distributions convolved with a standard uniform distribution.
Figure 6 illustrates the network structure of this hybrid network. The representation y is split channel-wise into two parts, y1 and y2, which have M1 and M2 channels, respectively, and is then quantized. Here, y^1 is entropy coded using the proposed entropy model, whereas y^2 is coded with the lightweight entropy model. The standard deviations of y^2 are estimated using ha and hs. Unlike the context-adaptive entropy model, which uses the results of ha (c^ ) as the input source to the estimator f , the lightweight entropy model retrieves the estimated standard deviations from ha directly. Note that ha takes the concatenated y^1 and y^2 as input, and hs generates c^ as well as 2, at the same time.
The total loss function also consists of the rate and distortion terms, although the rate is divided into three parts, each of which is for y^1, y^2, and z^, respectively. The distortion term is the same as before, but note that y^ is the channel-wise concatenated representation of y^1 and y^2:

L = R+D with R = Expx Ey~1,y~2,z~q - log py~1|z^(y~1 | z^) - log py~2|z^(y~2 | z^) - log pz~(z~) , D = Expx - log px|y^(x | y^)]

(7)

Here, the noisy representations of y~1, y~2, and z~ follow a standard uniform distribution, the mean values of which are y1, y2, and z, respectively. In addition, y1 and y2 are channel-wise split representations from y, the results of the transform ga, and have M1 and M2 channels, respectively:
13

Under review as a conference paper at ICLR 2019

q(y~1, y~2, z~ | x, g, h) =

U

y~1i

|

y1i

-

1 2

,

y1

i

+

1 2

·

i

U

y~2j

|

y2j

-

1 2

,

y2

j

+

1 2

·

j

U

z~k

|

zk

-

1 2

,

zk

+

1 2

(8)

k

with y1, y2 = S(ga(x; g)), y^ = Q(y1)  Q(y2), z = ha(y^; h).

The rate term for y^1 is the same model as that of equation (4). Note that ^2 does not contribute here, but does contribute to the model for y^2:

py~1|z^(y~1 | z^, h) =

N

µ1i

,

1

2 i

U

-

1 2

,

1 2

i

with µ1i, 1i = f (ci, ci ),

ci = E (c , i),

ci = E ( y^1 , i),

c , 2 = S(hs(z^; h))

(y~1i)

(9)

The rate term for y^2 is almost the same as Balle´ et al. (2018), except that noisy representations are only used as the inputs to the entropy models for training, and not for the conditions of the models.

py~2|z^(y~2 | z^, h) =

N

0, 2j 2

U

-

1 2

,

1 2

(y~2j )

j

(10)

The model of z is the same as in equation (5). For implementation, we used this hybrid structure for the top-four configurations in bit-rate descending order. We set N , M1, and M2 to 400, 192, and 408, respectively, for the top-two configurations, and to 320, 192, and 228, respectively, for the next two configurations.

6.2 SAMPLES OF THE EXPERIMENTS
In this section, we provide a few more supplemental test results. Figures 7, 8, and 9 show the results of the MSE optimized version of our method, whereas figures 10 and 11 show the results of the MS-SSIM optimized version. All figures include a ground truth image and the reconstructed images for our method, BPG, and Balle´ et al. (2018)'s approach, in the clockwise direction.

14

Under review as a conference paper at ICLR 2019
Figure 7: Sample test results. Top left, ground truth; top right, our method (MSE optimized; bpp, 0.2040; PSNR, 32.2063); bottom left, BPG (bpp, 0.2078; PSNR, 32.0406); bottom right, Balle´ et al. (2018)'s approach (MSE optimized; bpp, 0.2101; PSNR, 31.7054)
15

Under review as a conference paper at ICLR 2019
Figure 8: Sample test results. Top left, ground truth; top right, our method (MSE optimized; bpp, 0.1236; PSNR, 32.4284); bottom left, BPG (bpp, 0.1285; PSNR, 32.0444); bottom right, Balle´ et al. (2018)'s approach (MSE optimized, bpp, 0.1229; PSNR, 31.0596)
16

Under review as a conference paper at ICLR 2019
Figure 9: Sample test results. Top left, ground truth; top right, our method (MSE optimized; bpp, 0.1501; PSNR, 34.7103); bottom left, BPG (bpp, 0.1477; PSNR, 33.9623); bottom right, Balle´ et al. (2018)'s approach (MSE optimized; bpp, 0.1520; PSNR, 34.0465)
17

Under review as a conference paper at ICLR 2019
Figure 10: Sample test results. Top left, ground truth; top right, our method (MS-SSIM optimized; bpp, 0.2507; MS-SSIM, 0.9740); bottom left, BPG (bpp, 0.2441; MS-SSIM, 0.9555); bottom right, Balle´ et al. (2018)'s approach (MS-SSIM optimized; bpp, 0.2101; MS-SSIM, 0.9705)
18

Under review as a conference paper at ICLR 2019
Figure 11: Sample test results. Top left, ground truth; top right, our method (MS-SSIM optimized; bpp, 0.2269; MS-SSIM, 0.9810); bottom left, BPG (bpp, 0.2316; MS-SSIM, 0.9658); bottom right, Balle´ et al. (2018)'s approach (MS-SSIM optimized; bpp, 0.2291; MS-SSIM, 0.9786)
19

