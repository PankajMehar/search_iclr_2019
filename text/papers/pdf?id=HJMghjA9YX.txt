Under review as a conference paper at ICLR 2019
MODEL COMPARISON FOR SEMANTIC GROUPING
Anonymous authors Paper under double-blind review
ABSTRACT
We introduce a probabilistic framework for quantifying the semantic similarity between two groups of embeddings. We formulate this as a model comparison task in which we contrast a generative model that encodes similarity between the two groups versus one that does not. We illustrate how this framework can be used for the Semantic Text Similarity (STS) task using clear assumptions about how the embeddings of words are generated. We apply information criteria based model comparison to overcome the shortcomings of Bayesian model comparison, whilst still penalising model complexity. We achieve competitive results by applying the proposed framework with an appropriate choice of likelihood on the STS datasets.
1 INTRODUCTION
The problem of semantic textual similarity (STS), measuring how closely the meaning of one piece of text corresponds to that of another, has been studied in the hope of improving performance across various problems in Natural Language Processing (NLP), including information retrieval (Zheng & Callan, 2015). Recent progress in the generation of word embeddings has allowed the encoding of words using distributed vector representations, which capture semantic information through their location in the learned embedding space. However, it remains a challenge to adapt these representations of individual words to express semantic similarity between words groups, like documents, sentences, and other textual formats.
Recent methods for STS rely on additive composition of word vectors (Arora et al., 2016; Blacoe & Lapata, 2012; Mitchell & Lapata, 2008; 2010) or complex deep learning architectures (Kiros et al., 2015), which summarise a sentence through a single embedding. Resulting sentence vectors are compared using cosine similarity, a choice which is well supported by empirical results, but which is not backed by a solid theoretical foundation. It has been suggested that this should be addressed by analysing the geometry of the embedding space (Almahairi et al., 2015; Hill et al., 2016; Wieting & Gimpel, 2017). However, this approach has so far not led to significant insight.
We approach the problem of STS from a novel direction. The compositional nature of distributed representations demonstrated in Mikolov et al. (2013) and Pennington et al. (2014) indicate the presence of semantic groups in the representation space of word embeddings; an idea which has been further explored in Athiwaratkun & Wilson (2017). Under this assumption, the task of semantic similarity can be formulated as the following question: "Are two arbitrary subgroups of words partitions of the same semantic group?" Figure 1 demonstrates this concept.
In this work we propose a method that brings a paradigm shift from estimating sentence embeddings to a model comparison framework in the task of semantic text similarity. We employ a generative model for sentences similar to Arora et al. (2016), but our goal differs from theirs in that our similarity is based on the aforementioned model comparison test, whilst theirs is based on the inner product of sentence embeddings derived as maximum likelihood estimators.
Assuming a probability distribution of word embeddings over the unit hypersphere, we achieve results comparable to Arora et al. (2016) on the STS dataset in O(nd) time on average ­ compared to the O(nd2) average complexity of their method (where n is the number of words in a sentence, and d is the embedding size). A further advantage of our framework is that it naturally lends itself to online and unsupervised settings. We define an online setting as one in which we have access to only a single query pair at a time.
1

Under review as a conference paper at ICLR 2019

Figure 1: This paper models the generation of embeddings of words in a sentence through a latent variable (the semantic groups). Above we visualize the semantic group space of embeddings (with Di, Dj, Dk representing different semantic groups). This enables us to perform a model comparison test to assess similarity - are the two sentences part of the same semantic group like D1i and D2i or are they from different semantic groups like D1i and D2k?
2 METHODOLOGY AND BACKGROUND
In this work we formulate the task of semantic similarity between two arbitrary groups of objects as a model comparison problem. Taking inspiration from Ghahramani & Heller (2006) and Marshall et al. (2006) we propose the generative models for groups (e.g. sentences) D1, D2 seen in Figure 2. We adopt and adapt this idea from the Bayesian model comparison setting to an Information Criterion based test.
2.1 BAYESIAN MODEL COMPARISON
The Bayes factor, traditionally used in Bayesian model comparison under the graphical models in Figure 2 is defined as

sim(D1, D2)

=

log

p(D1, D2|M1) p(D1, D2|M2)

=

log

p(D1, D2|M1) p(D1|M2)p(D2|M2

)

.

(1)

To obtain the evidence p(D|Mi) the parameters of the model must be marginalised (where  means concatenation):

Figure 2: On the left, M1 assumes that both datasets are generated i.i.d. from the same parametric distribution. On the right, M2 assumes that the datasets are generated i.i.d. from distinct parametric distributions.
2

Under review as a conference paper at ICLR 2019

p(D1, D2|M1) = p(D1, D2|)p()d =

p(wk|)p()d,

wk D1 D2

p(Di|M2) =

p(wk|)p()d.

wk Di

Computing the semantic similarity score of the two groups D1, D2 under the Bayesian framework requires selecting a reasonable model likelihood p(wk|), prior p(), and computing the marginal evidence specified above. Computing the evidence can be computationally intensive and usually requires approximation.
We have extended the method Ghahramani & Heller (2006) to compare two groups. The method by Marshall et al. (2006) applies the same score as in Equation 1 in the context of merging datasets whilst we apply it in the setting of information retrieval and semantic text similarity.
We differ from both methods in that we choose to work with information criteria based model comparison as opposed to using the Bayes factor. The Bayes factor is very sensitive to the choice of prior and can result in estimates that heavily underfit the data (especially under a vague prior), having the tendency to always select the simpler model; which is argued further by M. S. Bartlett (1957) and Akaike et al. (1981). This is handled in Ghahramani & Heller (2006) by using the empirical Bayes procedure, a shortcoming of which is the issue of double counting (Berger, 2000) and thus being prone to over-fitting. We address this issue by exploring information theoretic criteria to design a log-ratio-like test that is prior free and robust to overfitting.

2.2 INFORMATION CRITERIA
We seek to compare models M1 and M2 using information criteria (IC) to assess the goodness of fit of each model. There are multiple information criteria used for model selection, each with different settings to which they are better suited. The information criteria which we will be working with have the general form

IC(D, M) = - L^M +  (D, M) ,
where L^M = L(^|D, M) is the maximised value of the log likelihood function for model M,  is a scalar derived for each IC, and  (D, M) represents a model complexity penalty term which is model and IC specific. Using this general formulation for the involved information criteria yields the following similarity score:

sim(D1, D2) = - IC({D1, D2}, M1) + IC({D1, D2}, M2), sim(D1, D2) =  L^(^1,2|M1) - (L^(^1|M2) + L^(^2|M2))
-  ({D1, D2}, M1) +  ({D1, D2}, M2) .
Examples of these criteria can be put into two broad classes. Bayesian Information Criterion (BIC) is an example of an IC that approximates the model evidence directly, as defined in Schwarz et al. (1978). Empirically it has been shown that the BIC is likely to underfit the data, especially when the number of samples is small (Dziak et al., 2012). Sentences contain a relatively small number of words (samples), thus we focus on the second class - Information Theoretic Criteria.
2.2.1 INFORMATION THEORETIC CRITERIA
The Information Theoretic Criteria (ITC) are a family of model selection criteria. The task they address is evaluating the expected quality of an estimated model specified by L(^|w) when it is used to generate unseen data from the true distribution G(w), as defined in Konishi & Kitagawa (2008b).
3

Under review as a conference paper at ICLR 2019

These family of criteria perform this evaluation by using the Kullback-Leibler (KL) divergence between the true model G(w) and the fitted model L(^|w), with the aim of selecting the model
(from a given set of models) that minimizes the quantity

DKL

G(w) p(w|^)

= EG

G(w) ln p(w|^)

= -HG(w) - EG ln p(w|^) .

The entropy of the true model HG(w) will remain constant across different likelihoods. Thus, the quantity of interest in the definition of the information criterion under consideration is given by the expected log likelihood under the true model EG[ln p(w|^)]. The goal then, is to find a good estimator for this quantity. One such estimator is given by the normalized maximum likelihood:

EG^

ln p(w|^)

1 =
n

n

ln p(wi|^),

i=1

where G^ represents the empirical distribution. This estimator introduces a bias that varies with respect to the dimension of the model's parameter vector  and requires a correction in order to carry out a fair comparison of information criteria between models. In Takeuchi (1976) a model specific correction is derived resulting in the following information criterion, called Takeuchi Information Criterion (TIC):

TIC(D, M) = -2 L^M - tr I^J^-1 ,

where

J^ = 1 n

n

2 L(|wi) ^ ,

i=1

I^ = 1 n

n

 L(|wi ) L

(|wi) ^.

i=1

Then, under the TIC criterion we have the following similarity (derivation in Appendix A):

sim(D1, D2) = 2 L(^1,2|M1) - L(^1|M2) - L(^2|M2) - tr I^1,2J^1-,21 + tr I^1J^1-1 + tr I^2J^2-1

.

To make this entire procedure concrete, we provide Algorithm 1.
For the case where we assume our model has the same parametric form as the true model and as n  , the equality I^ = J holds resulting in a penalty of tr I^J^-1 = tr(Ik) = k where k is the number of model parameters, as shown in Konishi & Kitagawa (2008a). This results in the Akaike Information Criterion (AIC) by Akaike (1974):

AIC(D, M) = -2(L^M - k).
The AIC simplification of TIC relies on several assumptions that only hold true in the big data limit and rely on assuming our model M has the same parametric form as the true model (Konishi & Kitagawa (2008a)). In general TIC is a more robust approximation, especially in the application we study where we consider the datasets to be sentences and thus the number of samples is small.

4

Under review as a conference paper at ICLR 2019

Algorithm 1 Calculate sim(D1, D2)

1: input: Sentences D1 := {wk}mk=1, D2 = {wk}lk=1 2: Compute the maximum likelihood estimates under the 2 models :

^1,2 := arg max L(|D1  D2) ^1 := arg max L(|D1) ^2 := arg max L(|D2)

3: Calculate the grad vector and Hessian for each model:

1 L(|w) , 2 L(|w), 1,2 L(|w)

21

L(|w)

,

22

L(|w),

2
1,2

L(|w)

4: sim(D1, D2) :=  L^(^1,2|M1) - (L^(^1|M2) + L^(^2|M2))

- tr I^1,2J^1-,21 + tr I^1J^1-1 + tr I^2J^2-1

5: output: sim(D1, D2)

2.3 VON MISES-FISHER LIKELIHOOD
Word embeddings are d dimensional vectors of real numbers, that are traditionally learned by optimizing a dot product between target words and context vectors (Mikolov et al., 2013). Due to this training setup, cosine similarity is used to measure the semantic similarity of words in various information retrieval tasks. Thus, we want to use a suitable distribution induced by the cosine similarity measure. We model our embeddings as vectors lying on the surface of the d - 1 dimensional unit hyper-sphere w  Sd-1 and i.i.d according to a von Mises-Fisher (vMF) likelihood (Fisher et al. (1993)):

p(w|µ, ) = 1 exp µ w , Z ()

p(w|µ, )

=



d 2

-1

(2)

d 2

I

d 2

-1

()

exp

µ

w

,

where µ is the mean direction vector and  is the concentration parameter, with supports ||µ|| = ||w|| = 1,   0. The term I() Corresponds to a modified Bessel function of the first kind with order .
We reparametrise the random variable to polar hypersphericals w() ( = (1, ..., d-1) ) as adopted in Mabdia (1975):

p(|, ) =



d 2

-1

(2)

d 2

I

d 2

-1

()

w exp µ() w() ,


where

i-1 i-1

wi() = ((1 - id) cos i + id) sin k , µi() = ((1 - id) cos i + id) sin k,

k=1

k=1

w 

d-2
= (sin k)d-k-1.

k=1

This reparametrisation simplifies the calculation of the Hessian and the empirical Fisher information matrix. The maxima of the likelihood remains unchanged since |w/| does not depend on  thus the MLE estimate in the hyper-shperical coordinates parametrisation is just given by applying the map from the cartesian MLE to the polars.

5

Under review as a conference paper at ICLR 2019

µ^ = ||

n i=1 n i=1

wi wi

||

,

Ad()

=

Id/2 , Id/2-1

^ = µ-1(µ^),

R¯ = ||

n i=1

wi||

,

n

^

=

Ad-1(R¯)



R¯(d - R¯2) 1 - R¯2 .

where both the derivation and approximation for the MLE estimates are derived in Banerjee et al. (2005).
The log likelihood is then (where D = {i}ni=1 is the dataset)

L(, |) = w()T µ() - log Z() + log w , 
n
L(, |{i}ni=1)) = L(, |i).
i=1

The first and second order partial derivatives of the vMF log likelihood are (derived in Appendix B):

 k

L(,

|)

=



d j=k

wj

()µj

()((1

-

kj

)

cot

k

-

kj

tan

k ),

2 k2

L(,

|)

=

-

d j=k

wj ()µj

(),

 I d ()

L(, |) = w() µ() - 2 ,



I

d 2

-1

()

2 2

L(,

|)

=

I

d 2

()(I

d 2

-2()

+

I

d 2

())

-

I

d 2

-1

()(I

2I

d 2

-1

()2

d 2

-1()

+

I

d 2

+1())

.

We prove (in Appendix B) that the mixed derivatives of the vMF log likelihood are a constant (with respect to ) times L(, |)/k. Thus, evaluated at the MLE, these entries are zero.

2L(, |{i}in=1)

= ^-1 L(, |{i}ni=1)

= 0.

k

=^ ,k =^k

k

k =^k

Assuming l < k (Hessian is symmetric and diagonal has different form)

L(, |{i}ni=1)  k  l

l =^l ,k =^k

=

cot

^l

L(,

|{i}in=1) k

k =^k

= 0.

Thus, J^ is a diagonal matrix, with diagonal j = (J^11, ..., J^dd) :

J^ = diag(j),

J^-1 = diag (J^1-11, ..., J^d-d1) ,

d

tr(IJ^-1) =

J^ii-1I^ii = J^1-11

i=1

 

L(,

|{i}ni=1)

2
+

d

J^i-i 1

i=2

 i-1

L(,

|{i}ni=1

)

2
.

6

Under review as a conference paper at ICLR 2019

This quantity only requires O(nd) operations to compute and thus does not increase the asymptotic complexity of the algorithm.
The closed form of the similarity measure for two sentences D1, D2 of length m and l respectively under this model is then

sim(D1, D2) = 2 (m + l)^1,2R¯1,2 - m^1R¯1 - l^2R¯2
- (m + l) log Z(^1,2) + m log Z(^1) + l log Z(^2) - tr(I1,2J^1,2-1) - tr(I1J^1-1) - tr(I2J^2-1) ,

where the Jacobian terms (from the reparametrisation) cancel out. The subscripts indicate the sentence, with 1, 2 meaning the concatenation of the two sentences.

3 EXPERIMENTS
We assess our methods performance on the Semantic Text Similarity (STS) datasets (Agirre et al., 2012; 2013; 2014; 2015; 2016). The objective of these tasks is to predict the similarity between two given sentences, validated against human scores. In our experiments, we utilise the pre-trained, commonly used GloVe word embeddings (Pennington et al., 2014). We remove stop words and punctuation using NTLK (Bird et al., 2009) from the dataset. This is done for two reasons - firstly, these bring little semantic meaning to a sentence. Secondly, the vMF distribution is unimodal and thus highly susceptible to stop words that form their own mode. For our method, we centre the distribution of word embeddings and normalize them to be of length 1, so that they can be modelled by the vMF distribution. Additionally, after the stop word removal, some of the sentences are left with a single word, making the MLE of the  parameter ill-defined. We overcome this issue by padding each sentence with an arbitrary embedding of a stop word or punctuation symbol (i.e. '.' or '?').
We compare our method against mean word vector (MWV), word mover distance (WMD) (Kusner et al., 2015) 1, smooth inverse frequency (SIF), and SIF with principal component removal as defined in Arora et al. (2016) 2. We re-ran these models under our experimental setup, to ensure a fair comparison. The reported results in this paper are slightly higher than the ones observed in the original works, due to stop word removal. The metric used is the average Spearman correlation score over each dataset, weighted by the number of sentences. The choice of Spearman is given by its non-parametric nature (assumes no distribution over the scores), as well as measuring any monotonic relationship between the two compared quantities.
3.1 ONLINE EXPERIMENT

STS-12 STS-13 (-SMT) STS-14 STS-15 STS-16

Our method
0.5944 0.6676 0.6614 0.7244 0.6877

WMD
0.5516 0.5007 0.5811 0.6704 0.6246

MWV
0.5990 0.6371 0.6459 0.7008 0.6663

SIF
0.5980 0.6623 0.6551 0.7042 0.6612

Table 1: Spearman correlations on the STS datasets using GloVe vectors, when pre-processing of the datasets are disallowed.

In this experiment, we do not allow pre-processing of the sentence-level dataset to simulate the conditions seen during an online information-retrieval scenario. The results are presented in Table
1https://github.com/mkusner/wmd 2https://github.com/PrincetonML/SIF

7

Under review as a conference paper at ICLR 2019

3.1. We are able to out-perform the standard weighting induced by mean word vector, as well as the smooth inverse frequency weighting technique.
3.2 OFFLINE EXPERIMENT

STS-12 STS-13 (-SMT) STS-14 STS-15 STS-16

Our method
0.5944 0.6676 0.6614 0.7244 0.6877

SIF + PCA
0.5871 0.7029 0.6846 0.7288 0.6875

Table 2: Spearman correlations on the STS datasets using GloVe vectors with pre-processing allowed.

For this experiment, we compare against the SIF weightings, augmented with the additional preprocessing technique seen in Arora et al. (2016). The results are shown in Table 3.2. We remain competitive with their results on the last two datasets, acheive better performance on STS-12, but have an overall lower average.

4 DISCUSSION
We've presented a new approach to similarity measurement, that achieves competitive performance to standard methods. Our method requires a set of clear choices - model, likelihood and information criterion. From that, a comparison framework is naturally derived, which supplies us with a similarity measure.
In this study, we found that the von Mises-Fisher distribution naturally lends itself to representing word embeddings, if their magnitude is disregarded and a unimodal distribution over individual sentences is assumed. Both of these assumptions can be relaxed, but further work is needed to derive the resulting distribution's properties. Adapting the Takeuchi Information Criterion to reduce the resulting model-comparison bias, we perform a statistically justified model-comparison induced similarity score. This framework is suitable for a variety of modelling scenarios, due to the modelling freedom in specifying the generative process. The graphical model we employ is adaptable to encode structural dependencies beyond the i.i.d. data-generating process we have assumed throughout this study.
ACKNOWLEDGMENTS
REFERENCES
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pp. 385­393. Association for Computational Linguistics, 2012.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. * sem 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, volume 1, pp. 32­43, 2013.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. Semeval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014), pp. 81­91, 2014.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, et al. Semeval-2015 task 2:

8

Under review as a conference paper at ICLR 2019
Semantic textual similarity, english, spanish and pilot on interpretability. In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pp. 252­263, 2015.
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pp. 497­511, 2016.
Hirotugu Akaike. A new look at the statistical model identification. IEEE transactions on automatic control, 19(6):716­723, 1974.
Hirotugu Akaike et al. Likelihood of a model and information criteria. Journal of econometrics, 16 (1):3­14, 1981.
Amjad Almahairi, Kyle Kastner, Kyunghyun Cho, and Aaron Courville. Learning distributed representations from reviews for collaborative filtering. In Proceedings of the 9th ACM Conference on Recommender Systems, pp. 147­154. ACM, 2015.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. International Conference on Learning Representations, 2017, 2016.
Ben Athiwaratkun and Andrew Gordon Wilson. Multimodal word distributions. arXiv preprint arXiv:1704.08424, 2017.
Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, and Suvrit Sra. Clustering on the unit hypersphere using von mises-fisher distributions. Journal of Machine Learning Research, 6(Sep): 1345­1382, 2005.
James O. Berger. Bayesian analysis: A look at today and thoughts of tomorrow. Journal of the American Statistical Association, 95(452):1269­1276, 2000. ISSN 01621459. URL http: //www.jstor.org/stable/2669768.
Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. " O'Reilly Media, Inc.", 2009.
William Blacoe and Mirella Lapata. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pp. 546­556. Association for Computational Linguistics, 2012.
John J Dziak, Donna L Coffman, Stephanie T Lanza, and Runze Li. Sensitivity and specificity of information criteria. The Methodology Center and Department of Statistics, Penn State, The Pennsylvania State University, 16(30):140, 2012.
Nicholas I Fisher, Toby Lewis, and Brian JJ Embleton. Statistical analysis of spherical data. Cambridge university press, 1993.
Zoubin Ghahramani and Katherine A Heller. Bayesian sets. In Advances in neural information processing systems, pp. 435­442, 2006.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. arXiv preprint arXiv:1602.03483, 2016.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Skip-thought vectors. CoRR, abs/1506.06726, 2015. URL http: //arxiv.org/abs/1506.06726.
Sadanori Konishi and Genshiro Kitagawa. Information criteria and statistical modeling. Springer Science & Business Media, 2008a.
Sadanori Konishi and Genshiro Kitagawa. Information criteria and statistical modeling. Springer Science & Business Media, 2008b.
9

Under review as a conference paper at ICLR 2019
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 957­966, Lille, France, 07­09 Jul 2015. PMLR. URL http://proceedings.mlr. press/v37/kusnerb15.html.
Maurice S M. S. Bartlett. A comment on d. v. lindley's statistical paradox. Biometrika, 44(3/4): 533­534, 1957.
KV Mabdia. Distribution theory for the von mises-fisher distribution and ite application. Statistical Distributions for Scientific Work, 1:113­30, 1975.
Phil Marshall, Nutan Rajguru, and Anze Slosar. Bayesian evidence as a tool for comparing datasets. Physical Review D, 73(6):067302, 2006.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111­3119, 2013.
Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. proceedings of ACL-08: HLT, pp. 236­244, 2008.
Jeff Mitchell and Mirella Lapata. Composition in distributional models of semantics. Cognitive science, 34(8):1388­1429, 2010.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532­1543, 2014.
Gideon Schwarz et al. Estimating the dimension of a model. The annals of statistics, 6(2):461­464, 1978.
Kei Takeuchi. The distribution of information statistics and the criterion of goodness of fit of models. Mathematical Science, 153:12­18, 1976.
John Wieting and Kevin Gimpel. Revisiting recurrent networks for paraphrastic sentence embeddings. arXiv preprint arXiv:1705.00364, 2017.
Guoqing Zheng and Jamie Callan. Learning to reweight terms with distributed representations. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '15, pp. 575­584, New York, NY, USA, 2015. ACM. ISBN 9781-4503-3621-5. doi: 10.1145/2766462.2767700. URL http://doi.acm.org/10.1145/ 2766462.2767700.
10

Under review as a conference paper at ICLR 2019

A DERIVATION OF TIC FOR M2

The MLE for p(1,2|, M2) can be derived by simply estimating the separate MLE solutions for p(1|1, M2) and p(2|2, M2). What is not as obvious is that the penalty term follows the estimation pattern.
Gradient vectors for M2 are given by (where  is concatenation)
L(|1,2) = 1 L(1|1)  2 L(2|2),

and Hessian results in a block diagonal matrix

2 L(|1,2) =

21 L(1|1) 0

0 22 L(2|2) ,

with inverse:

2L(|1,2)-1 =

21 L(1|1)-1 0

0 22 L(2|2)-1 .

Computing tr(I^J^-1) then yields

tr(I^J^-1) = tr L(|1,2)L(|1,2) 2L(|1,2)-1

= tr

I^1121 L(1|1)-1 I^1222 L(2|2)-1 I^1221 L(1|1)-1 I^2222 L(2|2)-1

= tr I^1121 L(1|1)-1 + tr I^2222 L(2|2)-1

= tr I^1J^1-1 + tr I^2J^2-1 .

B PARTIAL DERIVATIVE CALULATIONS
We first show the following result, which is useful for the full derivation. For k  j

 k µj()

=

 k

((1 - kd) cos k

j-1
+ kd) sin i
i=1

=

(1 -

kj

)

cos sin

k k

-

kj

sin cos

k k

i-1
((1 - kd) cos k + kd) sin i
i=1

= ((1 - kj) cot k - kj tan k)µj(),

where the second line comes from the fact that sin k (or cos k) gets transformed into a cos k (or - sin k), and thus we can revert to the original definition of µj by multiplying with a cot k (or - tan k). If k > j, this derivative is 0. Thus, for a single data point w()

 L(, |) =  w() µ() -  log Z()

k k

k

µ() = w()
k

d

=  wj()µj()((1 - kj) cot k - kj tan k),

j=k

where the sum starts from k, as for j < k, the derivative is zero.

11

Under review as a conference paper at ICLR 2019

The derivative with respect to  is derived as follows:

 L(, |) =  w() µ() -  log Z()

 



I d ()

= w() µ() - 2 ,

I

d 2

-1

()

where the derivative of the second term is a known result.

We next focus on second order derivatives:

2 k2 µj ()

=

2 k2

((1 - id) cos i

j-1
+ id) sin i.
i=1

Unless

this

derivative

is

zero,

we

notice

that

we

take

the

derivative

2 cos k  k2

or

,2 sin k
 2k

both

of

which result in the negative of the original function. Thus

2 k2

L(,

|)

=

-

d j=k

wj ()µj ().

The below result is given

2 2 L(, |)

=



(-

I d () 2

)



I

d 2

-1

()

=

I

d 2

()(I

d 2

-2()

+

I

d 2

())

-

I

d 2

-1

()(I

2I

d 2

-1

()2

d 2

-1()

+

I

d 2

+1())

.

Next, we show that the second order mixed derivatives are a constant (with respect to ) times L(, |)/k, i.e.

2L(, |) =

2 w() µ() -



log Z()

k

k

k

µ() = w()
k

= -1 L(, |) , k

and assuming l < k

L(, |)

µ()

= w()

 k  l

 k  l

d

=  wj()µj()((1 - kj) cot k - kj tan k)((1 - lj) cot l - lj tan l)

j=max(k,l)

d
=  wj()µj() cot l((1 - kj) cot k - kj tan k)
j=k

d
= cot l wj()µj()((1 - kj) cot k - kj tan k)
j=l

L(, |) = cot l k .

12

