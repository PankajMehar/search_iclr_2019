Under review as a conference paper at ICLR 2019
AIM: ADVERSARIAL INFERENCE BY MATCHING PRIORS AND CONDITIONALS
Anonymous authors Paper under double-blind review
ABSTRACT
Effective inference for a generative adversarial model remains an important and challenging problem. We propose a novel approach, Adversarial Inference by Matching priors and conditionals (AIM), which explicitly matches prior and conditional distributions in both data and code spaces, and puts a direct constraint on the dependency structure of the generative model. We derive an equivalent form of the prior and conditional matching objective that can be optimized efficiently without any parametric assumption on the data. We validate the effectiveness of AIM on the MNIST, CIFAR-10, and CelebA datasets by conducting quantitative and qualitative evaluations. Results demonstrate that AIM significantly improves both reconstruction and generation as compared to other adversarial inference models.
1 INTRODUCTION
Deep directed generative models like variational autoencoder (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) and generative adversarial network (GAN) (Goodfellow et al., 2014) have been proved to be powerful for modeling complex high-dimensional distributions. While both VAE and GAN can learn to generate realistic images, their underlying mechanisms are fundamentally different. VAE maps the data into low-dimensional codes using an encoder, and then reconstructs the original data by a decoder. This allows it to perform both generation and inference. GAN, on the other hand, trains a generator and a discriminator adversarially. The generator learns to fool the discriminator by mapping low-dimensional noise vectors to the data space; at the same time, the discriminator evolves to detect the generated fake samples from the true ones. These two methods have complementary strengths and weaknesses. VAE can learn a bidirectional mapping between data and code spaces, but relies on over-simplified parametric assumptions on the complex data distribution, thereby causing it to generate blurry images (Donahue et al., 2016; Goodfellow et al., 2014; Larsen et al., 2015). GAN generates more realistic samples than VAE (Radford et al., 2015; Larsen et al., 2015) because the adversarial regime allows it to learn more complex distributions. However, note that GAN only learns a unidirectional mapping for data generation, and does not allow inferring the latent codes from given samples. This is limiting because the ability of inference is very crucial for several downstream applications, such as classification, clustering, similarity search, and interpretation. Furthermore, GAN also suffers from the model collapse problem (Che et al., 2016; Salimans et al., 2016) ­ many modes of the data distribution are not represented in the generated samples.
Therefore, one may wonder on whether it is possible to develop a generative model that enjoys the strengths of both GAN and VAE without their inherent weaknesses. Such model should be able to generate high-quality samples as good as GAN, have an inference mechanism as effective as VAE, and also avoid the mode collapse issue. Many recent efforts have been devoted to combining VAE with adversarial discriminator(s) (Brock et al., 2016; Che et al., 2016; Larsen et al., 2015; Makhzani et al., 2015). However, VAE-GAN hybrids tend to manifest a compromise of the strengths and weaknesses of both the approaches. The main reason is that all of them retain the autoencoder structure, which requires an explicit metric to measure the data reconstruction and assumes over-simplified parametric data distributions. To overcome such limitations, adversarially learned inference (ALI) (Donahue et al., 2016; Dumoulin et al., 2016) was recently proposed, wherein the discriminator is trained on the joint distribution of data and latent codes. In this way, under a perfect discriminator, one can successfully match joint distributions of the decoder and encoder, thereby, performing inference by sampling from the encoder's conditional distribution that also matches the decoder's posterior. However, in practice the equilibrium of the jointly adversarial game is hard to approach as the
1

Under review as a conference paper at ICLR 2019

dependency structure between data and codes is not explicitly specified. ALI's reconstructions of samples are thus not always faithful (Dumoulin et al., 2016; Li et al., 2017) implying that its inference mechanism is not always effective.
To overcome the aforementioned issues, in this paper, we propose a novel approach, Adversarial Inference by Matching priors and conditionals (AIM), that integrates efficient inference to GAN and overcomes the limitations of prior approaches. The approach keeps the structure simple, involving only one generator, one encoder, and one discriminator. Furthermore, AIM's objective is directly derived from our goal of matching both prior and conditional distributions of the generator and encoder, instead of a heuristic combination with lk norm regularization. Compared to regular GANs, AIM has the ability to conduct inference, and also does not suffer from the mode collapse problem. Moreover, AIM also abandons the unrealistic parametric assumption on the conditional data distribution, and does not require any reconstruction in the data space. This is fundamentally different from VAE or VAE-GAN hybrids in which the lk norm is used to measure the data reconstruction. The usage of simple data-fitting metrics on the complex data distribution leads to worse generation performance. Different from ALI, AIM decomposes the hard problem of matching the joint distributions into two sub-tasks ­ explicitly matching the priors on the latent codes and the conditionals on the data. As a consequence of more restrictive constraint, it achieves better generation and more faithful reconstruction than ALI. Last but not the least, note that GAN variations with inference mechanism usually achieve worse generation performance as compared to regular GANs. However, as demonstrated in the experiments, AIM not only performs best on inference, but also improves the generation performance over GAN.

2 RELATED WORK
The most straightforward way to learn an inference mechanism is to learn the inverse mapping of GAN's generator post-hoc (Zhu et al., 2016). However, since its training process is the same as GAN, it still suffers from mode collapse problem. AGE (Ulyanov et al., 2017) encourages encoder and generator to be reciprocal by simultaneously minimizing an l1 reconstruction error in the data space and an l2 error in the code space. This is closely related to the cycle-consistency criterion (Zhu et al., 2017; Kim et al., 2017; Yi et al., 2017; Li et al., 2017). Although the pairwise reconstruction errors help reduce mode collapse, the data reconstruction is still measured by l1 or l2 norm, which brings the same problem of VAE and VAE-GAN hybrids. InfoGAN (Chen et al., 2016) minimizes the mutual information between a subset c of the latent code and the generated samples, and hence can only do partial inference on c.
ALI (Dumoulin et al., 2016; Donahue et al., 2016) is an elegant approach to bring inference mechanism into adversarial learning without assuming parametric distribution on the data. Different from our work, it directly plays an adversarial game to match the joint distributions of the decoder and encoder. But in practice, ALI's reconstructions are not necessarily faithful because the dependency structures within the two joint distributions are not specified (Li et al., 2017). ALICE (Li et al., 2017) tries to solve this problem by regularizing ALI using an extra conditional entropy constraint on the data. The conditional entropy is either explicitly measured by lk norm, or implicitly learned by adversarial training. However, when the data distribution becomes complicated (e.g. CIFAR-10), the lk metric may lead to blurry reconstructions and the adversarial training is hard to achieve (Li et al., 2017). Compared with ALI and ALICE, our method is proven to minimize the KL-divergence between both priors and conditionals of generator and encoder, and can provide consistent effective inference even on complicated distribution (see Section 5.1).

3 BACKGROUND

We consider the generative model p(z)p(x|z), where a latent



variable z(i) is first generated from the prior distribution p(z),

and then the data x(i) is sampled from the conditional distribution p(x|z). The parameter  stands for the ground truth parameter

zx

of the distribution. The prior p(z) is always assumed to be a

simple parametric distribution (e.g. N (0, I)), but the generative Figure 1: This graphical model struc-

conditional p(x|z) is always very complex and not known to us. ture is shared with GAN and VAE.

2

Under review as a conference paper at ICLR 2019

Moreover, the posterior p(z|x) is intractable but stands for an important inference procedure: given a data x(i), it allows us to infer its latent variable z(i).

4 METHODOLOGY

In our method, we will model the generating process by a neural network called generator, and the inference process by another neural network encoder. Consider the following two distributions of the generator and encoder, and their corresponding sampling procedures:
· the generator distribution: p(z)p(x|z); z  p(z), x  p(x|z). · the encoder distribution: q(x)q(z|x); x  q(x), z  q(z|x).
The generator's conditional p(x|z) approximates the generating distribution p(x|z). The encoder's conditional q(z|x) approximates the posterior distribution p(z|x), which is what we need for inference. The marginal distribution q(x) stands for the empirical data distribution, and the other marginal p(z) is taken to be p(z), which is always a known distribution like standard Gaussian.

4.1 MATCHING PRIORS AND CONDITIONALS

The ultimate goal is to match the joint distributions, p(x, z) and q(x, z). If this is achieved, we are
guaranteed that all marginals match and all conditionals match as well. In particular, the conditional q(z|x) matches the posterior p(z|x). We propose to decompose this goal into two sub-tasks ­ matching the priors p(z) and q(z), and matching the conditionals p(x|z) and q(x|z). There are two advantages. Firstly, we explicitly define the dependency structure z  x. Secondly, the explicit
constraints on both priors and conditionals are stronger than merely one constraint on the joint
distributions.

In order to match the distributions, we will minimize the KL-divergence between the priors and between the conditionals:

Ep(z)DKL(p(x|z)||q(x|z)) + DKL(p(z)||q(z)).

(1)

By the properties of KL-divergence, when the minimum of equation 1 is attained, we have both p(z) = q(z) and p(x|z) = q(x|z), for all x and z.

4.2 OBJECTIVE FUNCTION

The objective equation 1 cannot be directly optimized because both q(z) and q(x|z) are impossible to sample from, as the flow in the encoder is from x to z. Therefore, we rewrite equation 1 as the combination of a KL-divergence term and a reconstruction term, so that both of them only contain distributions that can be sampled from or directly evaluated.

Firstly, for any fixed z, the KL-divergence between the conditionals, DKL(p(x|z)||q(x|z)), can be decomposed as:

DKL(p(x|z)||q(x|z)) = Ep(x|z) [log p(x|z) - log q(x) - log q(z|x) + log q(z)]

= DKL(p(x|z)||q(x)) - Ep(x|z)[log q(z|x)] + log q(z).

(2)

Then by adding (log p(z) - log p(z)) - log q(z) to both sides of equation 2, we have

DKL(p(x|z)||q(x|z))+(log p(z)-log q(z))-log p(z) = DKL(p(x|z)||q(x))-Ep(x|z)[log q(z|x)].

Now taking expectation with respect to p(z) on both sides, we get

Ep(z)DKL(p(x|z)||q(x|z)) + DKL(p(z)||q(z)) - Ep(z) log p(z)

= Ep(z)DKL(p(x|z)||q(x)) + Ep(z)Ep(x|z)[- log q(z|x)] .

(3)

I II
Note that the term Ep(z) log p(z) is a constant because the prior p(z) is a fixed parametric distribution. For example, when z  N (0, Id), we have Ep(z) log p(z) = -d(1 + log(2))/2. Therefore, minimizing the objective equation 1 is now transformed to minimizing the new objective equation 3. Intuitively, term (I) measures the difference between generated and real samples, and term (II) measures the reconstruction of the latent codes. We summarize the above procedure as a proposition.

3

Under review as a conference paper at ICLR 2019

Proposition 1 The objective function
Ep(z) DKL(p(x|z)||q(x)) + Ep(x|z)[- log q(z|x)]
is minimized when p(z) = q(z) and p(x|z) = q(x|z) for all z, x. And hence, at the minimum, the joint distributions p(x, z) = q(x, z).

4.3 AIM FRAMEWORK
The KL-divergence part (I) can be replaced by an adversarial game using the f -divergence theory (Nowozin et al., 2016). The reconstruction term (II) is a log-likelihood and can be simply evaluated if we assume a parametric q(z|x). Therefore, our framework only requires exactly one generator G, one discriminator D, and one encoder E. We will now discuss how to play the adversarial game and measure the reconstruction in details.

Adversarial game Because we do not want to make any parametric assumption on the distribution p(x|z), an adversarial game will be played to distinguish p(x|z) from q(x). By the theory of f -GAN

(Nowozin et al., 2016), we then construct an adversarial game with the value function

V (G, D) = Exq(x)[D(x)] + Ezp(z)[- exp(D(G(z)) - 1)].

(4)

Under the perfect discriminator, finding the optimal generator is then equivalent to minimizing the

KL-divergence. The activation function for the discriminator in equation 4 is just the identity mapping

instead of the sigmoid function in the original GAN. But just like in the original GAN, the generator

of equation 4 also suffers from the gradient vanishing problem (Goodfellow, 2016). Since the original

GAN is well studied and suffices for our purpose, we will instead use its value function:

V (G, D) = Exq(x)[log(D(x))] + Ezp(z)[log(1 - D(G(z)))].

(5)

And as suggested in Goodfellow (2016), in order to mitigate the generator's gradient vanishing problem, we minimize another value function -Ezp(z)[log D(G(z))] for the generator.

Reconstruction Because of the simplicity of the distribution on z, we make a reasonable parametric
assumption on q(z|x) so that the log-likelihood can be explicitly calculated. In this paper we will assume z|x  N (µ(x), 2(x)I), and hence

- log q(z|x) = 1 d 2
j=1

(zj

- µj(x))2 j2(x)

+

log j2(x)

+

log(2)

=: L(z, µ(x), 2(x)),

(6)

where d is the dimension of z. In this case, the encoder network only needs to output two vectors, µ(x) and 2(x), that is, E(x) = (µ(x), 2(x)). Then we can compute the approximate negative
posterior log-likelihood by plugging E(x) into equation 6, i.e. L(z, E(x)).

To summarize, our final optimization problem is

min max V (G, D) + Ep(z)[L(z, E(G(z)))] .
G,E D

(7)

Here, if we use V (G, D) in equation 4 derived from f -GAN, then  = 1. If we use V (G, D) in

equation 5, then  needs to be set so that two parts of equation 7 are in the same scale. We will

discuss this in detail in Section 5.

4.4 TRAINING AND INFERENCE PROCEDURES
The training procedure is summarized in Algorithm 1. Given random z(i)  p(z), we first generate samples x~(i)  p(x|z(i)) using the generator. Then the discriminator is updated to distinguish between generated and real samples. The encoder outputs the parameters for the distribution q(z|x), from which we calculate the log-likelihood in (II). Then the generator and encoder are updated together to minimize the reconstruction error (i.e. maximize the expected log-likelihood), while the generator has an extra goal that is to fool the discriminator. The inference procedure is in Algorithm 2. For any data x(i), its inferred latent code is set to be the conditional mean µ(x(i)) = Eq(z|x(i))[z]. Then the reconstruction of x(i) is G(µ(x(i))). Besides the reconstruction, we can also generate more samples which are close to x(i) in the sense that they have similar latent codes. This can be done by first sampling z's from the posterior q(z|x(i)), and then map them to the data space using the generator.

4

Under review as a conference paper at ICLR 2019

Algorithm 1 The AIM training procedure.

g, d, e  initialize network parameters

repeat

z(1), . . . , z(n)  p(z)

Draw n samples from the prior

x~(j)  G(z(j)), j = 1, . . . , n

Generate samples using the generator network

(µ(x~(j)), 2(x~(j)))  E(x~(j))

Calculate mean and variance of q(z|x~(j))

q(i)  D(x(i)), i = 1, . . . , n

Compute discriminator predictions

(pj)  D(x~(j)), j = 1, . . . , n

Ld



-

1 n

n i=1

log(q(i))

-

1 n

n j=1

log(1 - p(j))

Compute discriminator loss

Lg



-

1 n

n j=1

log(p(j))

Le



 n

n j=1

L(z(j)

,

µ(x~(j)

),

2(x~(j)))

Lrec  Lg + Le

Compute generator loss
Compute encoder loss Compute reconstruction loss

d  d - d Ld (g, e)  (g, e) - (g,e)Lrec until convergence

Gradient update on discriminator network Gradient update on generator and encoder networks

Algorithm 2 The AIM inference procedure.
x(i)  q(x) (µ(x(i)), 2(x(i)))  E(x(i)) x~(i)  G(µ(x(i))) z(j)  N (µ(x(i)), 2(x(i))), j = 1, . . . , m x~(i,j)  G(z(j)), j = 1, . . . , m

Data to do inference on Calculate mean and variance of q(z|x(i))
Get reconstruction of x(i) Sample z from the posterior q(z|x(i))
Get similar neighbors of x(i)

4.5 REPARAMETRIZATION
In order to sample from p(x|z) and q(z|x), we use the reparametrization trick (Kingma and Welling, 2013; Bengio et al., 2013; 2014). Instead of directly sampling from a complex distribution, we can reparametrize the random variable as a deterministic transformation of an auxiliary noise variable , that is, u = f (v, ). In our case, to sample from x~  p(x|z), we can rewrite x~ = G(z, ), where z  p(z) and is an auxiliary variable from a simple distribution. If we do not pass in any auxiliary noise , then the generator will be deterministic. For the encoder part, since q(z|x) = N (µ(x), 2(x)I), one can draw samples by computing z = µ(x) + (x) , where
 N (0, I).
5 EXPERIMENTS
We evaluate our proposed method, AIM, for both reconstruction and generation tasks, on the data sets MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky and Hinton, 2009) and CelebA (Liu et al., 2015). To show insights behind the effectiveness of AIM, we also conduct the same 2D Gaussian mixture experiment as in Dumoulin et al. (2016), and explore the 2D latent representation on MNIST. The architectures of our discriminator and generator are based on DCGAN (Radford et al., 2015) and slightly simpler, which can be easily replaced by more advanced state-of-the-art GANs, and we use a deterministic generator throughout the experiments. Our encoder network consists of convolutional layers followed by two separated fully connected networks, which are used to predict the mean and variance of the posterior q(z|x), respectively. The Adam optimizer (Kingma and Ba, 2014) is used and the learning rate decay strategy suggested by Kingma and Ba (2014) is applied. Since the input to the log-function is one-dimensional in equation 5 but d-dimensional in equation 6, we set  to be 1/d. We also observe that the discriminator shares a similar task with the encoder: both of them need to extract higher level features from raw images. Therefore, in order to reduce the number of parameters and to stabilize the training procedure, our encoder takes the intermediate hidden representation learned by the discriminator as its own input. It is worth noting that the encoder does not update the common feature extracting layers. More details about the model architecture and optimization methods are listed in the appendices.
5

Under review as a conference paper at ICLR 2019

5.1 QUANTITATIVE RESULTS ON REAL DATASETS
In this section, we use quantitative measures to compare the inference and generation performance of AIM, GAN, ALI and ALICE. And for fair comparison, GAN is implemented to have the identical generator and discriminator with AIM. We also include a reduced version of AIM, named AIM-, in which the conditional distribution q(z|x) of the encoder is assumed to be a Gaussian with identity covariance matrix. To evaluate the performance of inference, we measure it through reconstructing test images and calculating the mean squared error (MSE), which has been adopted in Li et al. (2017). As for generation, we calculate the inception score (Salimans et al., 2016) on 50, 000 randomly generated images. The inception scores on MNIST are evaluated by the pre-trained classifier from Li et al. (2017), and the inception scores on CIFAR-10 is based on the ImageNet. The results are summarized in Table 1.
Table 1: MSE (lower is better) and Inception scores (higher is better) on MNIST and CIFAR-10. ALI and ALICE results are from the experiments in Li et al. (2017).

Method
GAN ALI ALICE AIM- AIM

MNIST

MSE

Inception Score

­ 0.480 ± 0.100 0.080 ± 0.007 0.028 ± 0.018 0.026 ± 0.018

9.464 ± 0.020 8.749 ± 0.090 9.279 ± 0.070 9.331 ± 0.021 9.483 ± 0.020

CIFAR-10

MSE

Inception Score

­ 0.672 ± 0.113 0.416 ± 0.202 0.037 ± 0.017 0.019 ± 0.009

6.287 ± 0.061 5.930 ± 0.044 6.015 ± 0.028 6.324 ± 0.056 6.450 ± 0.085

Inference From Table 1, AIM achieves the best reconstruction results on both data sets. On MNIST, AIM significantly decreases the MSE by 68% and 95% compared with ALICE and ALI respectively. On the more complicated CIFAT-10 data set, AIM decreases the MSE by 95% and 97%. In order to alleviate the non-identifiable issue of ALI, ALICE adds the conditional entropy constraint by explicitly regularizing the lk norms between the reconstructed and real images. However, as the data distribution becomes more complicated like in CIFAR-10, the lk norms become inadequate to measure the reconstruction. Consequently, ALICE's reconstruction error on CIFAR-10 increases significantly compared with that on MNIST. In contrast, the reconstruction performance of AIM is consistent on both data sets. The reason is that our model explicitly specifies the dependency structure of the generative model, and matches both prior and conditional distributions without using the simple data-fitting lk metrics in the data space. This can be further justified by the performance of AIM- which follows the same structure. Compared with AIM-, AIM further decreases the MSE significantly by 49% on CIFAR-10, which shows that the inferred conditional variance is crucial for achieving the faithful reconstructions on complicated data sets.
Generation AIM outperforms all the baseline models including GAN on inception score. This suggests that AIM can bring further improvement on generation performance instead of deteriorating it like the other baselines. The reason that both ALI and ALICE perform worse than GAN on generation is that the task of matching two complicated joint distributions, p(z, x) and q(z, x), is more difficult than the task of the regular GAN, which is to match only the marginals, p(x) and q(x). The proposed model AIM explicitly defines the dependency structure between z and x, which is more effective compared with one step joint distribution matching. Comparison between AIM and AIM- shows that the learned variance is also critical for better generation performance. We also want to highlight that AIM's generation performance can be further improved by replacing the adversarial network with more advanced state-of-the-art GANs.
5.2 RECONSTRUCTION RESULTS
We compare the reconstruction produced by AIM with the results of joint distribution regimes. Since ALICE is sensitive to hyper-parameter, we show its reconstruction results with different hyperparameter settings in Appendix, from which we observe either unfaithful reconstructions or blurriness. In Figure 2, we compare reconstruction of AIM with the results reported in ALI(Dumoulin et al., 2016) (BiGAN (Donahue et al., 2016)). From the first column of Figure 2, we observe that ALI provides a certain level of reconstructions. However, it fails to capture the precise style of the original
6

Under review as a conference paper at ICLR 2019

Figure 2: Reconstruction comparison between our proposed model AIM (first row) and ALI (BiGAN) (second row) on MNIST, CIFAR-10 and CelebA. In each of the figures, the odd columns are original samples from the test set and the even columns are their reconstructions.
digits. In contrast, AIM can achieve very sharp and faithful reconstructions. On the more complicated dataset CIFAR-10, ALI's reconstructions are less faithful and oftentimes make mistakes in capturing exact object placement, color, style, and object identity. Our model produces better reconstructions in all these aspects. For the reconstructions of CelebA in column three, AIM reproduces the similar style, color and face placement, and even achieves a high level of face identity. As stated in Dumoulin et al. (2016), they believe ALI's unfaithful reconstructions is caused by underfitting. This also leads us to believe that our adversarial regime (marginal and conditional distribution matching) is more efficient for inference compared to joint distribution matching regimes.

5.3 UNSUPERVISED CONDITIONAL GENERATION

Because the explicit posterior distribution q(z|x) can be learned by the proposed model AIM, we can generate the similar samples conditioned on a given sample. This is meaningful in downstream task like data augmentation. In Figure 3, based on any image x(i) in the first column, we conditionally generate more images by sampling z from the posterior q(z|x(i)). The conditionally generated images are shown in the same row of x(i). We observe that they have the similar style, color, and face placement with the original image.

5.4 MODE COLLAPSE REDUCTION

Figure 3: Conditionally generated samples on the CelebA.

To show the effectiveness of our model on mode collapse reduction, we perform the same synthetic experiment as in Dumoulin et al. (2016). The data is a 2D Gaussian mixture of 25 components laid out on a grid as shown in Figure 4a. As reported in Dumoulin et al. (2016), on average, ALI covered 13.4 ± 5.8 modes (min: 8, max: 25), while GAN covered 10.4 ± 9.2 (min: 1, max: 22). Under the same setting, we repeat the experiment for 10 times and our model can capture 25 modes every time. This indicates that AIM can significantly help overcome the challenge of mode collapse.

7

Under review as a conference paper at ICLR 2019

(a) Test set examples

(b) Latent codes

(c) Reconstructions (d) Generator samples

Figure 4: Comparison of our proposed model AIM (first row) with ALI (second row). The ALI's result shown here is the selected best-covering result reported in their original paper. And to follow the same settings with ALI, the number of points shown in column (b,c,d) is 10,000 which is 10% of the points shown in column (a).

The inferred latent codes, reconstructions and generated samples by both models are reported in Figure 4. In Figure 4b, we observe that our latent codes exhibit a disk shape and lie in the three standard deviation area of the standard Gaussian prior. They also keep the relative spacial relationship of the test samples. From Figure 4b, we observe that AIM's reconstruction is almost identical to the original data (Figure 4a), while ALI's reconstruction is less faithful and consists of connecting points between modes. Both models have similar generating performance. Some generated samples lie in the regions between the true modes. This is because the true modes are separated by large low-probability regions, which does not align well with the assumption of continuous prior distribution.

5.5 LATENT REPRESENTATION OF MNIST
The 2D latent codes of MNIST test data inferred by AIM is shown in Figure 10a. In a totally unsupervised manner, AIM does a great job at clustering the similar digits. The latent codes still lie in the standard Gaussian range but now exhibit no "holes", compared to Figure 4b. This is because the data distribution of MNIST is much more continuous than the Gaussian mixture. In Figure 10c, we show images generated by linearly interpolating between -2 and 2 along each dimension of the latent space. The generated images are sharp and the transitions between dig- (a) The 2-D latent space (b) Data manifold its are smooth. This indicates that AIM learns Figure 5: 2D latent codes of MNIST samples and the smooth and meaningful representations which data manifold. can generalize well.

6 CONCLUSION
We proposed a novel framework, AIM, which matches both prior and conditional distributions between the generator and the encoder. Adversarial inference is incorporated into this framework and there is no parametric assumption on the conditional data distribution. Therefore, the proposed approach can not only learn an efficient inference mechanism but also improve the fidelity of generated samples. Extensive experiments on real datasets validate the effectiveness of the proposed model, and some insights are revealed by experiments on synthetic data.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Yoshua Bengio, Eric Laufer, Guillaume Alain, and Jason Yosinski. Deep generative stochastic networks trainable by backprop. In International Conference on Machine Learning, pages 226­ 234, 2014.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2172­2180, 2016.
Jeff Donahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672­2680, 2014.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. arXiv preprint arXiv:1703.05192, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun Chen, Ricardo Henao, and Lawrence Carin. Alice: Towards understanding adversarial learning for joint distribution matching. In Advances in Neural Information Processing Systems, pages 5501­5509, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages 3730­3738, 2015.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pages 271­279, 2016.
9

Under review as a conference paper at ICLR 2019
Yunchen Pu, Liqun Chen, Shuyang Dai, Weiyao Wang, Chunyuan Li, and Lawrence Carin. Symmetric variational autoencoder and connections to adversarial learning. arXiv preprint arXiv:1709.01846, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234­2242, 2016.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. It takes (only) two: Adversarial generatorencoder networks. arXiv preprint arXiv:1704.02304, 2017.
Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for image-to-image translation. arXiv preprint, 2017.
Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision, pages 597­613. Springer, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.
10

Under review as a conference paper at ICLR 2019
A MORE EXPERIMENT RESULTS
A.1 COMPARISON BETWEEN AIM'S AND ALICE'S RECONSRUCTIONS ON CELEBA
Figure 6: AIM's CelebA reconstructions. The odd columns are original samples from the CelebA test set and the even columns are their reconstructions.
Figure 7: ALICE's CelebA reconstructions with different hyper-parameter  (reported in Pu et al. (2017)). The first column is the original samples from test set, and the following columns are reconstructions corresponding to different values of . ALICE uses hyper-parameter  to control the strength of the conditional entropy (CE) regularization in the objective function. When  is 0, ALICE's objective function reduces to ALI's. As observed from Fig 7, ALICE fails to reconstruct original images when hyper-parameter  is relative small. This is related to the non-identifiability issue of ALI as described in Li et al. (2017). As  increases, more patterns are captured, but the reconstructed images becomes more like the results of VAE ­ the images are all blurred. This is because ALICE replaces the intractable CE with its upper
11

Under review as a conference paper at ICLR 2019
bound cycle-consistency loss, which becomes lk norms under parametric assumption on p(x|z). So just like VAE-GAN hybrids, ALICE can be treated as a VAE-ALI hybrids and also suffer from similar drawbacks. Therefore, adding conditional entropy into ALI's objective function heuristically manifests a compromise of the strengths and weaknesses of ALI and VAE. In contrast, Figure 6 shows that our proposed model AIM can overcome the limitations of ALI and VAE. AIM's reconstructions capture the identifiable features in the original images, and are also very sharp without blurriness. In fact, we observe that AIM is able to reconstruct the images' background (color, texture), which are always lost in ALICE's reconstructions for any choice of .
A.2 ANALYSIS OF RECONSTRUCTION ERRORS
We present the change of mean squared errors (MSE) on the first 50 training epochs in Figure 8. As can be seen, on all datasets, the MSEs decrease significantly during the first 10 epochs and then stay steadily at the similar minimum levels. Although the scales of reconstruction errors on different datasets at the beginning are different because of the various complexity of data distributions, the converged values of reconstruction errors are very similar. This observation proves that our inference mechanism can efficiently and consistently infer the latent codes of data samples, even when the data distribution is complicated.

MSE

0.18 0.16 0.14 0.12
0.1 0.08 0.06 0.04 0.02
0 0

MNIST CIFAR-10 CelebA
10 20 30 40 50 Epoch

Figure 8: The MSE changes with epochs.
A.3 UNSUPERVISED CONDITIONALLY GENERATED IMAGES In Figure 9, we present (unsupervised) conditionally generated samples of MNIST and CelebA experiments. Each element in the first column stands for an original data x(i). Then based on x(i), we conditionally generate more images by sampling z from the posterior q(z|x(i)). The conditionally generated images are shown in the same row of x(i).
A.4 RANDOMLY GENERATED IMAGES In Figure 10, we show randomly generated images by sampling z from the marginal p(z). The generation results can be easily improved by replacing DCGAN structure with more advanced GAN structures.
B HYPERPARAMETERS FOR MNIST
The details for the networks used in MNIST experiment are presented in Table 2. Here the column "feature maps" for the fully-connected (FC) layers represents the dimension of their output. FC (µ)
12

Under review as a conference paper at ICLR 2019

(a) MNIST

(b) CIFAR-10

Figure 9: Conditionally generated samples (unsupervised). The first column consists of original images.

(a) MNIST

(b) CIFAR-10 Figure 10: Randomly generated samples.

(c) CelbebA

and FC () are two fully-connected layers built on the previous common layer to separately predict mean µ(x) and variance 2(x) of the conditional distribution q(z|x).

C HYPERPARAMETERS FOR CIFAR-10 AND CELEBA
The details for the networks used in CIFAR-10 and CelebA experiments are presented in Table 3. In order to reuse the same architecture for both datasets, we scaled CIFAR-10 images and center cropped CelebA images so that they have the same image size 64 × 64.

13

Under review as a conference paper at ICLR 2019

Operation Kernel Strides Feature maps BN? Dropout

G(z) ­ 32 × 1 × 1 input FC FC
Reshape to 128 × 7 × 7 Transposed convolution Transposed convolution Dfeature(x) ­ 1 × 28 × 28 input
Convolution Convolution Dcon(Dfeature(x)) ­ 128 × 7 × 7 input Reshape to 6272 × 1 × 1
FC FC FC E(Dfeature(x)) ­ 128 × 7 × 7 input Convolution Reshape to 576 × 1 × 1 FC(µ) FC FC FC() FC FC

- -
4×4 4×4
4×4 4×4
- - -
4×4
- -
- -

- -
2×2 2×2
2×2 2×2
- - -
2×2
- -
- -

1024 6272
64 3
64 128
1024 64 1
64
32 32
32 32

  0.0
0.0 
0.0 × 0.0
× 0.0 0.0
 0.0
× 0.0 × 0.0 
0.0
 0.0
× 0.0 
0.0 × 0.0

Optimizer Adam (= 2 × 10-4, 1 = 0.5) Learning rate decay  = / t, t = epoch
Batch size 100 Epochs 400
Leaky ReLU slope 0.1 Weight, bias initialization Isotropic gaussian (µ = 0,  = 0.02), Constant(0)

Nonlinearity
ReLU ReLU
ReLU Sigmoid
Leaky ReLU Leaky ReLU
Leaky ReLU Linear Sigmoid
Leaky ReLU
Leaky ReLU Linear
Leaky ReLU Linear

Table 2: MNIST model hyperparameters.

14

Under review as a conference paper at ICLR 2019

Operation Kernel Strides Feature maps BN? Dropout

G(z) ­ 64 × 1 × 1 input Transposed convolution Transposed convolution Transposed convolution Transposed convolution Transposed convolution Dfeature(x) ­ 3 × 64 × 64 input
Convolution Convolution Convolution Convolution Dcon(Dfeature(x)) ­ 1024 × 4 × 4 input Convolution E(Dfeature(x)) ­ 1024 × 4 × 4 input Convolution Convolution Convolution Convolution Reshape to 512 × 1 × 1
FC(µ) FC FC
FC() FC FC

4×4 4×4 4×4 4×4 4×4
4×4 4×4 4×4 4×4
4×4
4×4 4×4 2×2 1×1
- -
- -

1×1 2×2 2×2 2×2 2×2
2×2 2×2 2×2 2×2
1×1
2×2 2×2 1×1 1×1
- -
- -

1024 512 256 128 3
128 256 512 1024
1
512 512 256 128
64 64
64 64

  0.0  0.0  0.0
0.0 × 0.0
× 0.0  0.0  0.0
0.0
× 0.0
× 0.0  0.0  0.0
0.0
 0.0
× 0.0 
0.0 × 0.0

Optimizer Adam (= 2 × 10-4, 1 = 0.5) Learning rate decay  = / t, t = epoch
Batch size 100 Epochs 300
Leaky ReLU slope 0.2 Weight, bias initialization Isotropic gaussian (µ = 0,  = 0.02), Constant(0)

Nonlinearity
ReLU ReLU ReLU ReLU Tanh
Leaky ReLU Leaky ReLU Leaky ReLU Leaky ReLU
Sigmoid
Leaky ReLU Leaky ReLU Leaky ReLU Leaky ReLU
Leaky ReLU Linear
Leaky ReLU Linear

Table 3: CIFAR-10 and CelebA model hyperparameters.

15

