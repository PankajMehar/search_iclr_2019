Under review as a conference paper at ICLR 2019

STOCHASTIC GRADIENT DESCENT LEARNS STATE EQUATIONS WITH NONLINEAR ACTIVATIONS
Anonymous authors Paper under double-blind review

ABSTRACT
We study discrete time dynamical systems governed by the state equation ht+1 = (Aht + But). Here A, B are weight matrices,  is an activation function, and ut is the input data. This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks. We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory {ut, ht}Nt=0. We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size. Our results apply to increasing activations whose derivatives are bounded away from zero. The analysis is based on i) an SGD convergence result with nonlinear activations and ii) careful statistical characterization of the state vector. Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.

1 INTRODUCTION

A wide range of problems involve sequential data with a natural temporal ordering. Examples include natural language processing, time series prediction, system identification, and control design, among others. State-of-the-art algorithms for sequential problems often stem from dynamical systems theory and are tailored to learn from temporally dependent data. Linear models and algorithms; such as Kalman filter, PID controller, and linear dynamical systems, have a long history and are utilized in
control theory since 1960's with great success (Brown et al. (1992); Ho & Kalman (1966); Åström & Hägglund (1995)). More recently, nonlinear models such as recurrent neural networks (RNN) found applications in complex tasks such as machine translation and speech recognition (Bahdanau et al. (2014); Graves et al. (2013); Hochreiter & Schmidhuber (1997)). Unlike feedforward neural networks, RNNs are dynamical systems that use their internal state to process inputs. The goal of this work is to shed light on the inner workings of RNNs from a theoretical point of view. In particular, we focus on the RNN state equation which is characterized by a nonlinear activation function , state weight matrix A, and input weight matrix B as follows

ht+1 = (Aht + But),

(1.1)

Here ht is the state vector and ut is the input data at timestamp t. This equation is the source of dynamic behavior of RNNs and distinguishes RNN from feedforward networks. The weight matrices A and B govern the dynamics of the state equation and are inferred from data. We will explore the statistical and computational efficiency of stochastic gradient descent (SGD) for learning these weight matrices.
Contributions: Suppose we are given a finite trajectory of input/state pairs (ut, ht)tN=0 generated from the state equation (1.1). We consider a least-squares regression obtained from N equations; with inputs (ut, ht)tN=1 and outputs (ht+1)Nt=1. For a class of activation functions including leaky ReLU and for stable systems1, we show that SGD linearly converges to the ground truth weight matrices while requiring near-optimal trajectory length N . In particular, the required sample size is O(n + p) where n and p are the dimensions of the state and input vectors respectively. Our results are extended to unstable systems when the samples are collected from multiple independent RNN trajectories rather than a single trajectory. Our results apply to increasing activation functions whose derivatives are bounded away from zero; which includes leaky ReLU. Numerical experiments on

1Throughout this work, a system is called stable if the spectral norm of the state matrix A is less than 1.

1

Under review as a conference paper at ICLR 2019

ReLU and leaky ReLU corroborate our theory and demonstrate that SGD converges faster as the activation slope increases. To obtain our results, we i) characterize the statistical properties of the state vector (e.g. well-conditioned covariance) and ii) derive a novel SGD convergence result with nonlinear activations; which may be of independent interest. As a whole, this paper provides a step towards foundational understanding of RNN training via SGD.
1.1 RELATED WORK
Our work is related to the recent optimization and statistics literature on linear dynamical systems (LDS) and neural networks.
Linear dynamical systems: The state-equation (1.1) reduces to a LDS when  is the linear activation ((x) = x). Identifying the weight matrices is a core problem in linear system identification and is related to the optimal control problem (e.g. linear quadratic regulator) with unknown system dynamics. While these problems are studied since 1950's (Ljung (1998; 1987); Åström & Eykhoff (1971)), our work is closer to the recent literature that provides data dependent bounds and characterize the non-asymptotic learning performance. Recht and coauthors have a series of papers exploring optimal control problem (Simchowitz et al. (2018); Tu et al. (2018; 2017)). In particular, Hardt et al. (2016) shows gradient descent learns single-input-single-output (SISO) LDS with polynomial guarantees. Oymak & Ozay (2018) and Faradonbeh et al. (2018) provide sample complexity bounds for learning LDS. Sanandaji et al. (2011b;a) study the identification of sparse systems.
Neural networks: There is a growing literature on the theoretical aspects of deep learning and provable algorithms for training neural networks. Most of the existing results are concerned with feedforward networks. Ge et al. (2018); Li & Yuan (2017); Mei et al. (2018); Soltanolkotabi (2017); Janzamin et al. (2015); Soltanolkotabi et al. (2017); Zhong et al. (2017b) consider learning fullyconnected shallow networks with gradient descent. Brutzkus & Globerson (2017); Zhong et al. (2017a); Du et al. (2017) address convolutional neural networks; which is an efficient weight-sharing architecture. Brutzkus et al. (2017); Wang et al. (2018) studies over-parameterized networks when data is linearly separable. Janzamin et al. (2015); Oymak & Soltanolkotabi (2018) utilize tensor decomposition techniques for learning feedforward neural nets. For recurrent networks, Sedghi & Anandkumar (2016) proposed tensor algorithms with polynomial guarantees and Khrulkov et al. (2017) studied their expressive power. More recently, Miller & Hardt (2018) showed that stable RNNs can be approximated by feed-forward networks.

2 PROBLEM SETUP

We first introduce the notation. · returns the spectral norm of a matrix and smin(·) returns the minimum singular value. The activation  : R  R applies entry-wise if its input is a vector.
Throughout,  is assumed to be a 1-Lipschitz function. With proper scaling of its parameters, the
system (1.1) with a Lipschitz activation can be transformed into a system with 1-Lipschitz activation. The functions [·] and var[·] return the covariance of a random vector and variance of a random variable respectively. In is the identity matrix of size n × n. Normal distribution with mean µ and covariance  is denoted by N (µ, ). Throughout, c, C, c0, c1, . . . denote positive absolute constants.

Setup: We consider the dynamical system parametrized by an activation function (·) and weight matrices A  Rn×n, B  Rn×p as described in (1.1). Here, ht is the n dimensional state-vector and ut is the p dimensional input to the system at time t. As mentioned previously, (1.1) corresponds to the state equation of a recurrent neural network. For most RNNs of interest, the state ht is hidden and we only get to interact with ht via an additional output equation. For Elman networks Elman (1990), this equation is characterized by some output activation y and output weights C, D as follows

yt = y(Cht + Dut).

(2.1)

In this work, our attention is restricted to the state equation (1.1); which corresponds to setting
yt = ht in the output equation. To analyze (1.1) in a non-asymptotic data-dependent setup, we assume a finite input/state trajectory of {ut, ht}Nt=0 generated by some ground truth weight matrices (A, B). Our goal is learning the unknown weights A and B in a data and computationally efficient

2

Under review as a conference paper at ICLR 2019

Algorithm 1 Learning state equations with nonlinear activations

1: Inputs: (yt, ht, ut)Nt=1 sampled from a trajectory. Scaling µ, learning rate . Initialization
A0, B0. 2: Outputs: Estimates A^, B^ of the weight matrices A, B.

3: xt  [µhTt uTt ]T for 1  t  N . 4: 0  [µ-1A0 B0]

5: for  from 1 to END do

6: Pick  from {1, 2, . . . , N } uniformly at random.

7:   -1 - L (-1) 8: end for

9: return [A^ B^]  END

µIn 0

0 Ip

.

way. In essence, we will show that, if the trajectory length satisfies N n + p, SGD can quickly and provably accomplish this goal using a constant step size.

Appoach: Our approach is described in Algorithm 1. It takes two hyperparameters; the scaling factor µ and learning rate . Using the RNN trajectory, we construct N triples of the form {ut, ht, ht+1}Nt=1. We formulate a regression problem by defining the output vector yt, input vector xt, and the target
parameter C as follows

yt = ht+1

,

xt =

µht ut

 Rn+p

,

C = [µ-1A B]  Rn×(n+p).

(2.2)

With this reparameterization, we find the input/output identity yt = (Cxt). We will consider the least-squares regression given by

L() = 1 N

N

Lt()

where

Lt()

=

1 2

yt - (xt)

2. 2

t=1

(2.3)

For learning the ground truth parameter C, we utilize SGD on the loss function (2.3) with a constant
learning rate . Starting from an initial point 0, after END SGD iterations, Algrorithm 1 returns an estimate C^ = END. Estimates of A and B are decoded from the left and right submatrices of C^ respectively.

3 MAIN RESULTS

3.1 PRELIMINARIES

The analysis of the state equation naturally depends on the choice of the activation function; which is the source of nonlinearity. We first define a class of Lipschitz and increasing activation functions.
Definition 3.1 (-increasing activation). Given 1    0, the activation function  satisfies (0) = 0 and 1   (x)   for all x  R.

Our results will apply to strictly increasing activations where  is -increasing for some  > 0. Observe that, this excludes ReLU activation which has zero derivative for negative values. However, it includes Leaky ReLU which is a generalization of ReLU. Parameterized by 1    0, Leaky ReLU is a -increasing function given by

LReLU(x) = max(x, x).

(3.1)

In general, given an increasing and 1-Lipschitz activation , a -increasing function  can be obtained by blending  with the linear activation, i.e. (x) = (1 - )(x) + x.

A critical property that enables SGD is that the state-vector covariance [ht] is well-conditioned under proper assumptions. The lemma below provides upper and lower bounds on this covariance matrix in terms of problem variables.
Lemma 3.2 (State vector covariance). Consider the state equation (1.1) where h0 = 0 and ut i.i.d. N (0, Ip). Define the upper bound term Bt as

3

Under review as a conference paper at ICLR 2019

Bt = B

1 - A 2t 1- A 2 .

(3.2)

· Suppose  is 1-Lipschitz and (0) = 0. Then, for all t  0, [ht] Bt2In. · Suppose  is a -increasing function and p  n. Then, [ht] 2smin(B)2In.

As a natural extension from linear dynamical systems, we will say the system is stable if A < 1 and unstable otherwise. For activations we consider, stability implies that if the input is set to 0, state vector ht will exponentially converge to 0 i.e. the system forgets the past states quickly. This is also the reason (Bt)t0 sequence converges for stable systems and diverges otherwise. The condition number of the covariance will play a critical role in our analysis. Using Lemma 3.2, this number can be upper bounded by  defined as

=

B

2
=

smin(B)

B2

1

smin(B) 2(1 - A 2) .

(3.3)

Observe that, the condition number of B appears inside the  term.

3.2 LEARNING FROM SINGLE TRAJECTORY

Our main result applies to stable systems ( A < 1) and provides a non-asymptotic convergence guarantee for SGD in terms of the upper bound on the state vector covariance. This result characterizes the sample complexity and the rate of convergence of SGD; and also provides insights into the role of activation function and the spectral norm of A.
Theorem 3.3 (Main result). Let {ut, ht+1}Nt=1 be a finite trajectory generated from the state equation (1.1). Suppose A < 1,  is -increasing, h0 = 0, p  n, and ut i.i.d. N (0, Ip). Let  be same as (3.3) and c, C, c0 be properly chosen absolute constants. Pick the trajectory length N to satisfy
N  CL2(n + p),

where

L

=

1

-

log(cn) log A

.

Pick

scaling

µ

=

1/B,

learning

rate



=

c0

2 n(n+p)

,

and

consider

the

loss

function

(2.3).

With

probability

1

-

4N

exp(-100n)

-

8L

exp(-O(

N L2

)),

starting

from

an

initial point 0, for all   0, the SGD iterations described in Algorithm 1 satisfies

E[

 - C

2 F

]



(1

-

c0

4 22n(n

+

p)

)

0 - C

2 F

.

(3.4)

Here the expectation is over the randomness of the SGD updates.

Sample complexity: Theorem 3.3 essentially requires N (n + p)/4 samples for learning. This can be seen by unpacking (3.3) and ignoring the logarithmic L term and the condition number of B. Observe that O(n + p) growth achieves near-optimal sample size for our problem. Each state equation (1.1) consists of n sub-equations (one for each entry of ht+1). We collect N state equations to obtain a system of N n equations. On the other hand, the total number of unknown parameters in A and B are n(n + p). This implies Theorem 3.3 is applicable as soon as the problem is mildly overdetermined i.e. N n n(n + p).

Computational complexity:

Theorem

3.3

requires

O(n(n

+

p) log

1 

)

iterations

to

reach

-

neighborhood of the ground truth. Our analysis reveals that, this rate can be accelerated if the

state vector is zero-mean. This happens for odd activation functions satisfying (-x) = -(x)

(e.g. linear activation). The result below is a corollary and requires ×n less iterations.

Theorem 3.4 (Faster learning for odd activations). Consider the same setup provided in Theorem 3.3.

Additionally,

assume

that



is

an

odd

function.

Pick

scaling

µ

=

1/B,

learning

rate



=

c0

2 (n+p)

,

and

consider

the

loss

function

(2.3).

With

probability

1

-

4N

exp(-100n)

-

8L

exp(-O(

N L2

)),

starting from an initial point 0, for all   0, the SGD iterations described in Algorithm 1 satisfies

E[

 - C

2 F

]



(1

-

c0

4 22(n +

p)

)

0 - C

2 F

,

(3.5)

where the expectation is over the randomness of the SGD updates.

4

Under review as a conference paper at ICLR 2019

Another aspect of the convergence rate is the dependence on . In terms of , the SGD error (3.4) decays as (1 - O(8)) . While it is not clear how optimal is the exponent 8, numerical experiments
in Section 6 demonstrate that larger  indeed results in drastically faster convergence.

4 MAIN IDEAS AND PROOF STRATEGY

To prove the results of the previous section, we derive a deterministic result that establishes the linear convergence of SGD for -increasing functions. For linear convergence proofs, a typical strategy is showing the strong convexity of the loss function i.e. showing that, for some  > 0 and all points v, u, the gradient satisfies

L(v) - L(u), v - u   v - u 2 . 2
The core idea of our convergence result is that the strong convexity parameter of the loss function with -increasing activations can be connected to the loss function with linear activations. In particular, recalling (2.3), set ytlin = Cxt and define the linear loss to be

Llin() = 1 N 2N

ytlin - xt

2. 2

i=1

Denoting the strong convexity parameter of the original loss by  and that of linear loss by lin, we argue that   2lin; which allows us to establish a convergence result as soon as lin is strictly positive. Next result is our SGD convergence theorem which follows from this discussion.
Theorem 4.1 (Deterministic convergence). Suppose a data set {xi, yi}Ni=1 is given; where output yi is related to input xi via yi = ( xi,  ) for some   Rn. Suppose  > 0 and  is a -increasing. Let +  - > 0 be scalars. Assume that input samples satisfy the bounds

+In

1 N

N

xixTi

i=1

-In ,

xi

2
2

 B for all i.

Let {r }=0 be a sequence of i.i.d. integers uniformly distributed between 1 to N . Then, starting

from

an

arbitrary

point

0,

setting

learning

rate



=

, 2 -
+ B

for

all





0,

the

SGD

iterations

for

quadratic loss

+1 =  - ((xrT  ) - yr ) (xTr  )xr , satisfies the error bound

(4.1)

E[



-

2 ] 2

0 - 

2 (1 - 4-2 ) , 2 +B

where the expectation is over the random selection of the SGD iterations {r }=0.

(4.2)

This theorem provides a clean convergence rate for SGD for -increasing activations and naturally generalizes standard results on linear regression which corresponds to  = 1. Its extension to proximal gradient methods might be beneficial for high-dimensional nonlinear problems (e.g. sparse/low-rank approximation and generalized linear models Cai et al. (2010); Beck & Teboulle (2009); Oymak et al. (2018); Agarwal et al. (2010)) and is left as a future work.

To derive the results from Section 3, we need to determine the conditions under which Theorem 4.1 is applicable to the data obtained from RNN state equation with high probability. Below we provide desirable characteristics of the state vector; which enables our statistical results.
Assumption 1 (Well-behaved state vector). Let L > 1 bean integer. There exists positive scalars +, -,  and an absolute constant C > 0 such that   3 n and the following holds

· Lower bound: [hL-1] -In,

· Upper bound: for all t, the state vector satisfies

[ht]

+In ,

ht - E[ht]

2

  C +

and

E[ht]

2



  +.

(4.3)

Here · 2 returns the subgaussian norm of a vector (see Def. 5.22 of Vershynin (2010)).

5

Under review as a conference paper at ICLR 2019

Assumption 1 ensures that covariance is well-conditioned, state vector is well-concentrated, and it has a reasonably small expectation. Our next theorem establishes statistical guarantees for learning the RNN state equation based on this assumption.
Theorem 4.2 (General result). Let {ut, ht+1}tN=1 be a length N trajectory of the state equation (1.1). Suppose A < 1,  is -increasing, h0 = 0, and ut i.i.d. N (0, Ip). Given scalars +  - > 0, set the condition number as  = +/-. For absolute constants C, c, c0 > 0, choose trajectory length N to satisfy

N  CL2(n + p) where L = 1 - log (cn) . log A

Suppose Assumption 1 holds with L, +, -, . Pick scaling to be µ = 1/+ and learning rate to

be



=

c0

2 (+ 2)2(n+p)

.

With

probability

1

-

4N

exp(-100n)

-

8L

exp(-O(

N L2

)),

starting

from

0, for all   0, the SGD iterations on loss (2.3) as described in Algorithm 1 satisfies

E[

 - C

2 F

]



(1

-

c0

22(

+

4 2)2(n

+

p)

)

0 - C

2 F

,

(4.4)

where the expectation is over the randomness of SGD updates.

The advantage of this theorem is that, it isolates the optimization problem from the statistical properties

of state vector. If one can prove tighter bounds on achievable (+, -, ), it will immediately imply improved performance for SGD. In particular, Theorems 3.3 and 3.4 are simple corollaries of Theorem

4.2 with proper choices.

·

Theorem 3.3 follows by setting +

= B2 , -

= 2smin(B)2, and 

 = n.

· Theorem 3.4 follows by setting + = B2 , - = 2smin(B)2, and  = 0.

5 LEARNING UNSTABLE SYSTEMS

So far, we considered learning from a single RNN trajectory for stable systems ( A < 1). For such systems, as the time goes on, the impact of the earlier states disappear. In our analysis, this allows us to split a single trajectory into multiple nearly-independent trajectories. This approach will not work for unstable systems (A is arbitrary) where the impact of older states may be amplified over time. To address this, we consider a model where the data is sampled from multiple independent trajectories.

Suppose N independent trajectories of the state-equation (1.1) are available. Pick some integer

T0  1. Denoting the ith trajectory by the triple (h(t+i)1, h(ti), u(ti))t0, we collect a single sample

from each trajectory at optimization framework

time T0 to obtain the triple (2.3); for 1  i  N , we set,

(hT(i0)+1, h(Ti0), u(Ti0)).

To utilize the existing

(yi, hi, ui) = (hT(i0)+1, h(Ti0), u(Ti0)).

(5.1)

With this setup, we can again use the SGD Algorithm 1 to learn the weights A and B. The crucial

difference compared to Section 3 is that, the samples (yi, hi, ui)Ni=1 are now independent of each other; hence, the analysis is simplified. As previously, having an upper bound on the condition

number of the state-vector covariance is critical. This upper bound can be shown to be  defined as

¯ if n > 1

=

¯

1- 2 |A|2 1-(|A|)2T0

if n = 1

where

¯ =



2

BT20 smin(B

)2

.

(5.2)

The ¯ term is similar to the earlier definition (3.3); however it involves BT0 rather than B. This modification is indeed necessary since B =  when A > 1. On the other hand, note that, BT20 grows proportional to A 2T0 ; which results in exponentially bad condition number in T0. Our  definition remedies this issue for single-output systems; where n = 1 and A is a scalar. In particular, when  = 1 (e.g.  is linear)  becomes equal to the correct value 12. The next theorem provides our
result on unstable systems in terms of this condition number and other model parameters.

2Clearly, any nonzero 1 × 1 covariance matrix has condition number 1. However, due to subtleties in the proof strategy, we don't use  = 1 for  < 1. Obtaining tighter bounds on the subgaussian norm of the state-vector would help resolve this issue.

6

Under review as a conference paper at ICLR 2019

Algorithm 2 Empirical hyperparameter selection.
1: Inputs: (ht, ut)Nt=1 sampled from a trajectory. 2: Outputs: Scaling µ. 3: Form the empirical covariance matrix h from {ht}Nt=1. 4: Form the empirical covariance matrix u from {ut}tN=1. 5: return u / h .

Normalized Error Normalized Error

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 0

||A|| = 0.2

=0 = 0.1 = 0.5 =1

10000 2I0t0e0r0a3ti0o0n00 40000 50000

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 0

||A|| = 0.8
=0 = 0.1 = 0.5 =1
10000 2I0t0e0r0a3ti0o0n00 40000 50000

(a) (b) Figure 1: SGD convergence behavior for Leaky ReLUs with varying minimum slope . Figures a) and b) repeat the same experiments. The difference is the spectral norm of the ground truth state matrix A.

Theorem 5.1 (Unstable systems). Suppose we are given N independent trajectories (h(ti), u(ti))t0 for 1  i  N . Each trajectory is sampled at time T0 to obtain N samples (yi, hi, ui)Ni=1 where the ith sample is given by (5.1). Suppose the sample size satisfies
N  C2(n + p)

where  is given by (5.2). Assume the initial states are 0,  is -increasing, p  n, and ut i.i.d.

N (0, Ip). Set scaling µ = 1/

BT0 ,

learning

rate



=

c0

2 n(n+p)

,

and

run

SGD

over

the

equations

described in (2.2) and (2.3). Starting from 0, with probability 1 - 2N exp(-100(n + p)) -

4

exp(-O(

N 2

)),

all

SGD

iterations

satisfy

E[

 - C

2 F

]



(1

-

c0

4 22n(n

+

p)

)

0 - C

2 F

,

where the expectation is over the randomness of the SGD updates.

6 NUMERICAL EXPERIMENTS

We conducted experiments on ReLU and Leaky ReLU activations. Let us first describe the experimental setup. We pick the state dimension n = 50 and the input dimension p = 100. We choose the ground truth matrix A to be a scaled random unitary matrix; which ensures that all singular values of A are equal. B is generated with i.i.d. N (0, 1) entries. Instead of using the theoretical scaling choice, we determine the scaling µ from empirical covariance matrices outlined in Algorithm 2. Similar to our proof strategy, this algorithm equalizes the spectral norms of the input and state covariances to speed up convergence. We also empirically determined the learning rate and used  = 1/100 in all experiments.

Evaluation: We consider two performance measures in the experiments. Let C^ be an estimate of

the ground truth parameter C = [µ-1A B]. The first measure is the normalized error defined as

C^ - C

2 F

/

C

2 F

.

The

second

measure

is

the

normalized

loss

defined

as

N i=1

yt - (C^ xt)

N i=1

yt

2
2

2
2.

In all experiments, we run Algorithm 1 for 50000 SGD iterations and plot these measures as a function of  ; by using the estimate available at the end of the  th SGD iteration for 0    50000.
Each curve is obtained by averaging the outcomes of 20 independent realizations.

7

Under review as a conference paper at ICLR 2019

Normalized Loss Normalized Loss

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 0

||A|| = 0.1 ||A|| = 0.3 ||A|| = 0.5 ||A|| = 0.7 ||A|| = 0.9
10000 2I0t0e0r0a3ti0o0n00 40000 50000

100 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 0

||A|| = 0.1 ||A|| = 0.3 ||A|| = 0.5 ||A|| = 0.7 ||A|| = 0.9
10000 2I0t0e0r0a3ti0o0n00 40000 50000

(a) (b) Figure 2: SGD convergence behavior for ReLU with varying spectral norm of the state matrix A. Figures a) and b) repeats the same experiments. The difference is that a) uses N = 500 trajectory length whereas b) uses N = 2500 (i.e. ×5 more data). Shaded regions highlight the one standard deviation around the mean.

Our first experiments use N = 500; which is mildly larger than the total dimension n + p = 150. In Figure 1, we plot the Leaky ReLU errors with varying slopes as described in (3.1). Here  = 0 corresponds to ReLU and  = 1 is the linear model. In consistence with our theory, SGD achieves linear convergence and as  increases, the rate of convergence drastically improves. The improvement is more visible for less stable systems driven by A with a larger spectral norm. In particular, while ReLU converges for small A , SGD gets stuck before reaching the ground truth when A = 0.8.
To understand, how well SGD fits the training data, in Figure 2a, we plotted the normalized loss for ReLU activation. For more unstable system ( A = 0.9), training loss stagnates in a similar fashion to the parameter error. We also verified that the norm of the overall gradient L( ) F continues to decay (where  is the  th SGD iterate); which implies that SGD converges before reaching a global minima. As A becomes more stable, rate of convergence improves and linear rate is visible. Finally, to better understand the population landscape of the quadratic loss with ReLU activations, Figure 2b repeats the same ReLU experiments while increasing the sample size five times to N = 2500. For this more overdetermined problem, SGD converges even for A = 0.9; indicating that
· population landscape of loss with ReLU activation is well-behaved,
· however ReLU problem requires more data compared to the Leaky ReLU for finding global minima.
Overall, as predicted by our theory, experiments verify that SGD indeed quickly finds the optimal weight matrices of the state equation (1.1) and as the activation slope  increases, the convergence rate improves.

7 CONCLUSIONS
This work showed that SGD can learn the nonlinear dynamical system (1.1); which is characterized by weight matrices and an activation function. This problem is of interest for recurrent neural networks as well as nonlinear system identification. We showed that efficient learning is possible with optimal sample complexity and good computational performance. Our results apply to strictly increasing activations such as Leaky ReLU. We empirically showed that Leaky ReLU converges faster than ReLU and requires less samples; in consistence with our theory. We list a few unanswered problems that would provide further insights into recurrent neural networks. · Covariance of the state-vector: Our results depend on the covariance of the state-vector and requires it to be positive definite. One might be able to improve the current bounds on the condition number and relax the assumptions on the activation function. Deriving similar performance bounds for ReLU is particularly interesting. · Hidden state: For RNNs, the state vector is hidden and is observed through an additional equation (2.1); which further complicates the optimization landscape. Even for linear dynamical systems, learning the (A, B, C, D) system ((1.1), (2.1)) is a non-trivial task Ho & Kalman (1966); Hardt et al. (2016). What can be said when we add the nonlinear activations? · Classification task: In this work, we used normally distributed input and least-squares regression for our theoretical guarantees. More realistic input distributions might provide better insight into contemporary problems, such as natural language processing; where the goal is closer to classification (e.g. finding the best translation from another language).

8

Under review as a conference paper at ICLR 2019
REFERENCES
Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pp. 37­45, 2010.
Karl Johan Åström and Peter Eykhoff. System identification--a survey. Automatica, 7(2):123­162, 1971.
Karl Johan Åström and Tore Hägglund. PID controllers: theory, design, and tuning, volume 2. Instrument society of America Research Triangle Park, NC, 1995.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183­202, 2009.
Robert Grover Brown, Patrick YC Hwang, et al. Introduction to random signals and applied Kalman filtering, volume 3. Wiley New York, 1992.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-parameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.
Jian-Feng Cai, Emmanuel J Candès, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956­1982, 2010.
S. Dirksen. Tail bounds via generic chaining. arXiv preprint arXiv:1309.3522, 2013.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017.
Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179­211, 1990.
Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identification in unstable linear systems. Automatica, 96:342­353, 2018.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. ICLR, 2018.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pp. 6645­6649. IEEE, 2013.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv preprint arXiv:1609.05191, 2016.
BL Ho and Rudolph E Kalman. Effective construction of linear state-variable models from input/output functions. at-Automatisierungstechnik, 14(1-12):545­548, 1966.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735­1780, 1997.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural networks. arXiv preprint arXiv:1711.00811, 2017.
Michel Ledoux. The concentration of measure phenomenon. American Mathematical Soc., 2001.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
Lennart Ljung. System identification: theory for the user. Prentice-hall, 1987.
Lennart Ljung. System identification. In Signal analysis and prediction, pp. 163­173. Springer, 1998.
9

Under review as a conference paper at ICLR 2019
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural networks. arXiv preprint arXiv:1804.06561, 2018.
John Miller and Moritz Hardt. When recurrent models don't need to be recurrent. arXiv preprint arXiv:1805.10369, 2018.
Samet Oymak and Necmiye Ozay. Non-asymptotic identification of lti systems from a single trajectory. arXiv preprint arXiv:1806.05722, 2018.
Samet Oymak and Mahdi Soltanolkotabi. End-to-end learning of a convolutional neural network via deep tensor decomposition. arXiv preprint arXiv:1805.06523, 2018.
Samet Oymak, Benjamin Recht, and Mahdi Soltanolkotabi. Sharp time­data tradeoffs for linear inverse problems. IEEE Transactions on Information Theory, 64(6):4129­4158, 2018.
Borhan M Sanandaji, Tyrone L Vincent, and Michael B Wakin. Exact topology identification of large-scale interconnected dynamical systems from compressive observations. In American Control Conference (ACC), 2011, pp. 649­656. IEEE, 2011a.
Borhan M Sanandaji, Tyrone L Vincent, Michael B Wakin, Roland Tóth, and Kameshwar Poolla. Compressive system identification of lti and ltv arx models. In Decision and Control and European Control Conference (CDC-ECC), 2011 50th IEEE Conference on, pp. 791­798. IEEE, 2011b.
Hanie Sedghi and Anima Anandkumar. Training input-output recurrent neural networks through spectral methods. arXiv preprint arXiv:1603.00954, 2016.
Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. arXiv preprint arXiv:1802.08334, 2018.
Mahdi Soltanolkotabi. Learning relus via gradient descent. arXiv preprint arXiv:1705.04591, 2017. Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape
of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017. Michel Talagrand. Gaussian processes and the generic chaining. In Upper and Lower Bounds for Stochastic
Processes, pp. 13­73. Springer, 2014. Stephen Tu, Ross Boczar, Andrew Packard, and Benjamin Recht. Non-asymptotic analysis of robust control
from coarse-grained identification. arXiv preprint arXiv:1707.04791, 2017. Stephen Tu, Ross Boczar, and Benjamin Recht. On the approximation of toeplitz operators for nonparametric
-norm estimation. In 2018 Annual American Control Conference (ACC), pp. 1867­1872. IEEE, 2018. Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010. Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable data: Algorithm,
optimality, and generalization. arXiv preprint arXiv:1808.04685, 2018. Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural networks with
multiple kernels. arXiv preprint arXiv:1711.03440, 2017a. Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-
hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b.
10

Under review as a conference paper at ICLR 2019

A DETERMINISTIC CONVERGENCE RESULT FOR SGD

Proof of Theorem 4.1.

Given two distinct scalars a, b; define  (a, b) =

(a)-(b) a-b

.

 (a, b)   since  is

-increasing. Define w to be the residual w =  - . Observing

the SGD recursion obeys

(xrT  ) - yr =  (xTr  , xTr )xrT w ,

w +1

2 2

=

w - ((xrT  ) - yr ) (xTr  )xr

2 2

.

=

w - xr  (xrT  ) (xrT  , xTr )xrT w

2 2

=

(I - Gr )w

2 2

where Gr = xr  (xTr  ) (xrT  , xTr )xTr . Since  is 1-Lipschitz and -increasing, Gr is a positivesemidefinite matrix satisfying

xr xTr Gr 2xr xrT , GTr Gr xr xTr xr xTr Bxr xrT . Consequently, we find the following bounds in expectation

+In E[Gr ] 2-In,

(A.1)

E[GTr Gr ] B+In.

Observe that (A.1) essentially lower bounds the strong convexity parameter of the problem with 2-; which is the strong convexity of the identical problem with the linear activation (i.e.  = 1). However, we only consider

strong convexity around the ground truth parameter  i.e. we restricted our attention to (,  ) pairs. With this, w+1 can be controlled as,

E[

w +1

2 2

]

=

E[

(I - Gr )w

2 2

]

=

w

2 2

-

2

E[wT Gr w ]

+

2

E[wT GTr Gr w ]



w

2 2

(1

-

22-

+

2B+).

Setting



=

, 2 -
+ B

we

find

the

advertised

bound

E[

w +1

2 2

]



E[

w

2 2

](1

-

4-2 +B

).

Applying induction over the iterations  , we find the advertised bound (4.2)

E[ w

2 ] 2

w0

2 (1 - 4-2 ) . 2 +B

Lemma A.1 (Merging L splits). Assume matrices X(i)  RNi×q are given for 1  i  L. Suppose for all 1  i  L, rows of X(i) has 2 norm at most B and each X(i) satisfies

+In

X (i)T X (i) Ni

-In.

X(1) 

Set N =

X(2) 

L i=1

Ni

and

form

the

concatenated

matrix

X

=

 



...

. 

Denote

ith

row

of

X

by

xi.

Then,

for



X (L)

each i,

xi

2 2

 B and

+In

XTX 1 =
NN

N
xixiT

i=1

-In.

Proof. The bound on the rows xi 2 directly follows by assumption. For the remaining result, first observe

that XT X =

L i=1

X (i)T

X (i) .

Next,

we

have

L
N +In = Ni+In

L
X (i)T X (i)

L
Ni-In = N -In.

i=1

i=1

i=1

Combining these two yields the desired upper/lower bounds on XT X/N .

11

Under review as a conference paper at ICLR 2019

B PROPERTIES OF THE NONLINEAR STATE EQUATIONS

This section characterizes the properties of the state vector ht when input sequence is normally distributed. These bounds will be crucial for obtaining upper and lower bounds for the singular values of the data matrix X = [x1 . . . xN ]T described in (2.2). For probabilistic arguments, we will use the properties of subgaussian random variables. Orlicz norm provides a general framework that subsumes subgaussianity.
Definition B.1 (Orlicz norms). For a scalar random variable Orlicz-a norm is defined as
X a = sup k-1/a(E[|X|k])1/k k1
Orlicz-a norm of a vector x  Rp is defined as x a = supvBp vT x a where Bp is the unit 2 ball. The subexponential norm is the Orlicz-1 norm · 1 and the subgaussian norm is the Orlicz-2 norm · 2 . Lemma B.2 (Lipschitz properties of the state vector). Consider the state equation (1.1). Suppose activation  is 1-Lipschitz. Observe that ht+1 is a deterministic function of the input sequence {u }t =0. Fixing all vectors {ui}i= (i.e. all except u ), ht+1 is A t- B Lipschitz function of u for 0    t.

Proof. Fixing {ui}i= , denote ht+1 as a function of u by ht+1(u ). Given a pair of vectors u , u using 1-Lipschitzness of , for any t >  , we have
ht+1(u ) - ht+1(u ) 2  (Aht(u ) + But) - (Aht(u ) + But) 2  A(ht(u ) - ht(u )) 2  A ht(u ) - ht(u ) 2 .
Proceeding with this recursion until t =  , we find
ht+1(u ) - ht+1(u ) 2  A t- h+1(u ) - h+1(u ) 2  A t- (Ah + Bu ) - (Ah + Bu ) 2  A t- B u - u 2 .
This bound implies ht+1(u ) is A t- B Lipschitz function of u .
Lemma B.3 (Upper bound). Consider the state equation governed by equation (1.1). Suppose ut i.i.d. N (0, Ip),  is 1-Lipschitz, (0) = 0 and h0 = 0. Recall the definition (3.2) of Bt. We have the following properties

· ht is a Bt-Lipschitz function of the vector qt = [uT0 . . . uTt-1]T  Rtp.

· There exists an absolute constant c > 0 such that ht - E[ht] 2  cBt and [ht] Bt2In.

· ht satisfies

E[

ht

2 2

]



tr(BBT

)

1- 1-

A 2t A2

 min{n, p}Bt2.

Also, there exists an absolute constant c > 0 such that for any m  n, with probability 1 - 2 exp(-100m), ht 2  c mBt.

Proof. i) Bounding Lipschitz constant: Observe that ht is a deterministic function of qt i.e. ht = f (qt) for some function f . To bound Lipschitz constant of f , for all (deterministic) vector pairs qt and q^t, we find a scalar Lf satisfying,

f (qt) - f (q^t) 2  Lf qt - q^t 2 .

(B.1)

Define the vectors, {ai}ti=0, as follows

ai = [u^T0 . . . u^iT-1 uTi . . . uTt-1]T .

Observing that a0 = qt, at = q^t, we write the telescopic sum,

t-1

f (qt) - f (q^t) 2 

f (ai+1) - f (ai) 2 .

i=0

Focusing on the individual terms f (ai+1) - f (ai), observe that the only difference is the ui, u^i terms. Viewing ht as a function of ui and applying Lemma B.2,

f (ai+1) - f (ai) 2  A t-1-i B ui - u^i 2 .

12

Under review as a conference paper at ICLR 2019

To bound the sum, we apply the Cauchy-Schwarz inequality; which yields

t-1

|f (qt) - f (q^t)| 

A t-1-i B

i=0

ui - u^i

2

t-1

t-1

 ( A 2(t-1-i) B 2)1/2 (

i=0

i=0

ui - u^i

2 )1/2 2

qt-q^t 2



B 2(1 - A 1- A 2

2t )

qt - q^t

2

= Bt qt - q^t 2 . The final line achieves the inequality (B.1) with Lf = Bt hence ht is Bt Lipschitz function of qt.

(B.2)

ii) Bounding subgaussian norm: When ut i.i.d. N (0, Ip), the vector qt is distributed as N (0, Itp). Since ht a Bt Lipschitz function of qt, for any fixed unit length vector v, v := vT ht = vT f (qt) is still Bt-Lipschitz
function of qt. Hence, using Gaussian concentration of Lipschitz functions, v satisfies

P(|v

-

E[v ]|



t)



2

exp(-

t2 2Bt2

).

This implies that for any v, v - E[v] is O(Bt) subgaussian Vershynin (2010). This is true for all unit v, hence using Definition B.1, the vector ht satisfies ht - E[ht] 2  O(Bt) as well. Secondly, Bt-Lipschitz function of a Gaussian vector obeys the variance inequality var[v]  Bt2 (page 49 of Ledoux (2001)), which
implies the covariance bound [ht] Bt2In.

iii) Bounding

2-norm: To obtain this result, we first bound E[

ht

2 ]. Since  is 1-Lipschitz and (0) = 0, 2

we have the deterministic relation

ht+1 2  Aht + But 2 .

Taking squares of both sides, expanding the right hand side, and using the independence of ht, ut and the covariance information of ut, we obtain

E[

ht+1

2 2

]



E[

Aht + But

2 2

]

=

E[

Aht

2 2

]

+

E[

But

2] 2



A

2 E[

ht

2 2

]

+

tr(B

B

T

).

(B.3) (B.4)

Now that the recursion is established, expanding ht on the right hand side until h0 = 0, we obtain

t

E[

ht+1

2 2

]



A

2itr(BBT

)



tr(BBT

)

1

- 1

A 2(t+1) - A2

.

i=0

Now using the fact that tr(BBT )  rank(B) B 2  min{n, p} B 2, we find

E[ ht+1

2 ]2  E[

ht+1

2 2

]



min{n,

p}Bt2+1.

Finally, using the fact that ht is Bt-Lipschitz function and utilizing Gaussian concentration of qt  N (0, Itp),

we find



P( ht+1

2 - E[ ht+1

2

]



t)



exp(-

t2 2Bt2

).





Setting t = (c - 1) mBt for sufficiently large c > 0, we find P( ht 2  nBt + (c - 1) mBt) 

exp(-100m).

Lemma B.4 (Odd activations). Suppose  is strictly increasing and obeys (x) = -(-x) for all x and h0 = 0. Consider the state equation (1.1) driven ut i.i.d. N (0, Ip). We have that E[ht] = 0.

Proof. We will inductively show that {ht}t0 has a symmetric distribution around 0. Suppose the vector ht satisfies this assumption. Let S  Rn be a set. We will argue that P(ht+1  S) = P(ht+1  -S). Since  is strictly increasing, it is bijective on vectors, and we can define the unique inverse set S = -1(S). Also since  is odd, (-S ) = -S. Since ht, ut are independent and symmetric, we reach the desired conclusion as follows

P(ht+1  S) = P(Aht + But  S ) = P(A(-ht) + B(-ut)  S )

(B.5)

= P(Aht + But  -S ) = P((Aht + But)  (-S )) = P(ht+1  -S). (B.6)

13

Under review as a conference paper at ICLR 2019

Theorem B.5 (State-vector lower bound). Consider the nonlinear state equation (1.1) with {ut}t0 i.i.d. N (0, Ip). Suppose  is a -increasing function for some constant  > 0. For any t  1, the state vector obeys
[ht] 2smin(BBT )In.
Proof. The proof is an application of Lemma B.7. The main idea is to write ht as sum of two independent vectors, one of which has independent entries. Consider a multivariate Gaussian vector g  N (0, ). g is statistically identical to g1 + g2 where g1  N (0, smin()Id) and g2  N (0,  - smin()Id) are independent multivariate Gaussians.
Since But  N (0, BBT ), setting  = BBT and smin = smin(), we have that But  g1 + g2 where g1, g2 are independent and g1  N (0, sminIn) and g2  N (0,  - sminIn). Consequently, we may write
But + Aht  g1 + g2 + Aht.
For lower bound, the crucial component will be the g1 term; which has i.i.d. entries. Applying Lemma B.7 by setting x = g1 and y = g2 + Aht, and using the fact that ht, g1, g2 are all independent of each other, we find the advertised bound, for all t  0, via
[ht+1] = [(g1 + g2 + Aht)] 2sminIn.

The next theorem applies to multiple-input-single-output (MISO) systems where A is a scalar and B is a row vector. The goal is refining the lower bound of Theorem B.5.

Theorem B.6 (MISO lower bound). Consider the setup of Theorem B.5 with single output i.e. n = 1. For any

t  1, the state vector obeys

var[ht]  2

B

2 2

1 - (|A|)2t 1 - 2|A|2

.

Proof. For any random variable X, applying Lemma B.7, we have var[(X)]  2var[X]. Recursively, this yields

var[ht+1] = var[(Aht + But)]  2var[Aht + But] = 2(|A|2var[ht] +

B

2 2

).

Expanding these inequalities till h0, we obtain the desired bound

t
var[ht]  (i|A|i-1 B 2 )2.
i=1

Lemma B.7 (Vector lower bound). Suppose  is a -increasing function. Let x = [x1 . . . xn]T be a vector with i.i.d. entries distributed as xi  X. Let y be a random vector independent of x. Then,
[(x + y)] 2var[X]In.

Proof. We first apply law of total covariance (e.g. Lemma B.8) to simplify the problem using the following lower bound based on the independence of x and y,

[(x + y)] Ey[[(x + y) y]] = Ey[x[(x + y)]].

(B.7) (B.8)

Now, focusing on the covariance x[(x + y)], fixing a realization of y, and using the fact that x has i.i.d. entries; (x + y) has independent entries as  applies entry-wise. This implies that x[(x + y)] is a diagonal matrix. Consequently, its lowest eigenvalue is the minimum variance over all entries,

x[(x + y)] min var[(xi + yi)]In. 1in

Fortunately, Lemma B.9 provides the lower bound var[(xi + yi)]  2var[X]. Since this lower bound holds for any fixed realization of y, it still holds after taking expectation over y; which concludes the proof.

The next two lemmas are helper results for Lemma B.7 and are provided for the sake of completeness.
Lemma B.8 (Law of total covariance). Let x, y be two random vectors and assume y has finite covariance. Then
[y] = E[[y x]] + [E[y x]].

14

Under review as a conference paper at ICLR 2019

Proof. First, write [y] = E[yyT ] - E[y] E[yT ]. Then, applying the law of total expectation to each term, [y] = E[E[yyT x]] - E[E[y x]] E[E[yT x]].
Next, we can write the conditional expectation as E[E[yyT x]] = E[[y x]] + E[E[y x] E[y x]]T . To conclude, we obtain the covariance of E[y x] via the difference,
E[E[y x] E[y x]]T - E[E[y x]] E[E[yT x]] = [E[y x]], which yields the desired bound.
Lemma B.9 (Scalar lower bound). Suppose  is a -increasing function with  > 0 as defined in Definition 3.1. Given a random variable X and a scalar y, we have
var[(X + y)]  2var[X].

Proof. Since  is -increasing, it is invertible and -1 is strictly increasing. Additionally, -1 is 1/ Lipschitz

since,

|(a) - (b)|  |a - b| = |a - b|  |-1(a) - -1(b)|.

Using this observation and the fact that E[X] minimizes E(X - )2 over , var[(X + y)] can be lower

bounded as follows

var[(X + y)] = E((X + y) - E[(X + y)])2

 2 E((X + y) - -1(E[(X + y)]))2

 2 E(X + y - E[X + y])2

= 2 E(X - E X)2 = 2var[X].

Note that, the final line is the desired conclusion.

C TRUNCATING STABLE SYSTEMS

One of the challenges in analyzing dynamical systems is the fact that samples from the same trajectory have temporal dependence. This section shows that, for stable systems, the impact of the past states decay exponentially fast and the system can be approximated by using the recent inputs only. We first define the truncation of the state vector.

Definition C.1 (Truncated state vector). Suppose (0) = 0, initial condition h0 = 0, and consider the state equation (1.1). Given a timestamp t, L-truncation of the state vector ht is denoted by h¯ t,L and is equal to qt where

q+1 = (Aq + Bu ) , q0 = 0

(C.1)

is the state vector generated by the inputs u satisfying

0 if  < t - L

u = u else

.

In words, L truncated state vector h¯ t,L is obtained by unrolling ht until time t - L and setting the contribution of the state vector ht-L to 0. This way, h¯ t,L depends only on the variables {u }t-=1t-L.

The following lemma states that impact of truncation can be made fairly small for stable systems ( A < 1). Lemma C.2 (Truncation impact ­ deterministic). Consider the state vector ht and its L-truncation h¯ t,L from Definition C.1. Suppose  is 1-Lipschitz. We have that

ht - h¯ t,L 2 

0 if t  L A L ht-L 2 else

.

Proof. When t  L, Definition C.1 implies u = u hence ht = qt = h¯ t,L. When t > L, we again use Definition C.1 and recall that u = 0 until time  = t - L - 1. For all t - L <   t, using 1-Lipschitzness of , we have that
h - q 2 = (Ah-1 + Bu-1) - (Aq-1 + Bu-1) 2  (Ah-1 + Bu-1) - (Aq-1 + Bu-1) 2  A(h-1 - q-1) 2  A h-1 - q-1 2 .
Applying this recursion between t - L <   t and using the fact that qt-L = 0 implies the advertised result ht - qt 2  A L ht-L - qt-L 2
 A L ht-L 2 .

15

Under review as a conference paper at ICLR 2019

C.1 NEAR INDEPENDENCE OF SUB-TRAJECTORIES
We will now argue that, for stable systems, a single trajectory can be split into multiple nearly independent trajectories. First, we describe how the sub-trajectories are constructed. Definition C.3 (Sub-trajectory). Let sampling rate L  1 and offset 1  ¯  L be two integers. Let N¯ = N¯¯ be the largest integer obeying (N¯ - 1)L + ¯  N . We sample the trajectory {ht, ut}Nt=0 at the points ¯, ¯ + L, . . . , ¯ + (N¯ - 1)L + ¯ and define the ¯th sub-trajectory as
(h(i), u(i)) := (h(i,¯), u(i,¯)) = (h(i-1)L+¯, u(i-1)L+¯).
Definition C.4 (Truncated sub-trajectory). Consider the state equation (1.1) and recall Definition C.1. Given offset ¯ and sampling rate L, for 1  i  N¯ , the ith truncated sub-trajectory states are {h¯ (i)}iN¯=1 where the ith state is defined as
h¯ (i) = h¯ L(i-1)+¯,L-1.
The truncated samples are independent of each other as shown in the next lemma.
Lemma C.5. Consider the truncated states of Definition C.4. If (1.1) is generated by independent vectors {ut}t0, for any offset ¯ and sampling rate L, the vectors {h¯ (i)}iN¯=1, {u(i)}Ni¯=1 are all independent of each other.

Proof. By construction h¯ (i) only depends on the vectors {u }L=(iL-(1i)-+2¯)-+1¯+1. Note that the dependence ranges
[L(i - 2) + ¯ + 1, L(i - 1) + ¯ - 1] are disjoint intervals for different i's; hence (h¯ (i))Ni¯=1 are independent of each other. To show the independence of u(i) and h¯ (i); observe that inputs u(i) = uL(i-1)+¯ have timestamp
¯ modulo L; which is not covered by the dependence range of (h¯ (i))iN¯=1.

If the input is randomly generated, Lemma C.2 can be combined with a probabilistic bound on ht, to show that truncated states h¯ (i) are fairly close to the actual states h(i).

Lemma C.6 (Truncation impact ­ random). Given offset ¯ and sampling rate L, consider the state vectors

of the sub-trajectory {h(i)}Ni¯=1 and L - 1-truncations (h¯ (i))iN¯=1. Suppose {ut}t0 i.i.d. N (0, Ip), A < 1,

h0 = 0, n, +

 >

is 0.

1-Lipschitz, and (0) = There exists an absolute

0. Also suppose upper bound constant c > 0 such that with

(4.3) of Assumption 1 holds probability at least 1 - 2N¯

for some   exp(-100n),

for all 1  i  N¯ , the following bound holds

h(i) - h¯ (i)

2

 c n

A

L-1

 +.

In particular, we can always pick + = B2 (via Lemma B.3).

Proof. Using Assumption 1, we can apply Lemma F.3 on vectors {h(i-2)L+¯+1}iN¯=1. Using a union bound, with desired probability, all vectors obey

 h(i-2)L+¯+1 - E[h(i-2)L+¯+1] 2  (c - 1) n+,

 for sufficiently large c. Since   n, triangle inequality implies

h(i-2)L+¯+1

2



 c n+.

Now,

applying

Lemma C.2, for all 1  i  N¯ , we find

h(i) - h¯ (i)

2 = h(i-1)L+¯ - h¯ (i-1)L+¯,L-1

 A L-1 h(i-2)L+¯+1 2

c

A

L-1

 n+.

2

D PROPERTIES OF THE DATA MATRIX

This section utilizes the probabilistic estimates from Section B to provide bounds on the condition number of data matrices obtained from the RNN trajectory (1.1). Following (2.2), these matrices H, U and X are defined as

H = [h1 . . . hN ]T , U = H = [u1 . . . uN ]T , X = [x1 . . . xN ]T .

(D.1)

The challenge is that, the state matrix H has dependent rows; which will be addressed by carefully splitting the trajectory {ut, ht}tN=0 into multiple sub-trajectories which are internally weakly dependent as discussed in
Section C. We first define the matrices obtained from these sub-trajectories.

16

Under review as a conference paper at ICLR 2019

Definition D.1. Given sampling rate L and offset ¯, consider the L-subsampled trajectory {h(i), u(i)}iN¯=1 as described in Definitions C.3 and C.4. Define the matrices H¯ = H¯ (¯)  RN¯×n, H~ = H~ (¯)  RN¯×n, U~ = U~ (¯)  RN¯ ×p, and X~ = X~ (¯)  RN¯ ×(n+p) as
H¯ = [h¯ (1) . . . h¯ (N¯ )]T , H~ = [h(1) . . . h(N¯ )]T , U~ = [u(1) . . . u(N¯ )]T , X~ = [µH~ U~ ].

Lemma D.2 (Handling perturbation). Consider the nonlinear state equation (1.1). Given sampling rate L > 0

and offset ¯, consider the matrices H¯ , H~ , X~ of Definition D.1 and let Q = [+-1/2H¯ U~ ]  RN¯×(n+p).

Suppose Assumption 1 holds,  is -increasing, and ut i.i.d. N (0, Ip). There exists an absolute constant

C

>

0 such that if N¯ 



C

+2 -2

(n

+

p),

with

probability

1

-

8

exp(-c

-2 +2

N¯ ),

for

all

matrices

M

obeying

M - H¯ 

- N¯ 10

,

the

perturbed

Q

matrices

given

by,

Q~ = [+-1/2M U~ ],

(D.2)

satisfy

 ( + 2)2

Q~T Q~ N¯

- . 2+

(D.3)

Proof. This result is a direct application of Theorem F.1 after determining minimum/maximum eigenvalues of

population covariance. The cross covariance obeys E[H¯ T U~ ] = 0 due to independence. Also, for i > 1, the

truncated state vector h¯ (i) is statistically identical to hL-1 hence [h¯ (i)] -In. Consequently, [u(i)] = Ip,

1 +

[h¯ (i)

]

In

for

all

i

and

- +

In

1 +

[h¯ (i)]

for

all

i

>

1.

Hence,

setting

qi

=

1+ h¯ (i) u(i)

, for all i > 1

- +

In

[qi]

In.

Set the matrix Q¯ = [q2 . . . qN¯ ]T and note that Q = [q1 Q¯T ]T . Applying Theorem F.1 on Q¯ and Corollary F.2 on Q, we find that, with the desired probability,

+

3/2  1 N¯

Q



1 N¯

smin(Q)



1 N¯

smin(Q¯ )



N -1 N

2-  0.99 × 3+

2- . 3+

Setting E = M - H¯ and observing Q~ = Q + [+-1/2E 0], the impact of the perturbation E can be bounded

naively via smin(Q) - +-1/2 E  smin(Q~)  Q~  Q + +-1/2 E . Using the assumed bound on

E , this yields



+

 2



1 N¯

Q~



1 N¯

smin(Q~ )



- . 2+

This final inequality is identical to the desired bound (D.3).

Theorem D.3 (Data matrix condition). Consider the nonlinear state-equation (1.1). Given +  - > 0,

define

the

condition

number



=

+ -

.

For

some

absolute

constants

c, C

>

0,

pick

a

trajectory

length

N

where

L = 1 - log (cn) log A

,

N0 =

N L

 C2(n + p),

and pick scaling µ = 1+ . Suppose A < 1,  is -increasing, ut i.i.d. N (0, Ip), and Assumption 1 holds with +, -, , L. Matrix X = [x1 . . . xN ]T of (D.1) satisfies the following with probability 1 - 4N exp(-100n) - 8L exp(-O(N0/2)).
 · Each row of X has 2 norm at most c0 p + n where c0 is an absolute constant.

· XT X obeys the bound

 ( + 2)2In+p

XT X N

-1In+p/2.

(D.4)

Proof. The first statement on 2-norm bound can be concluded from Lemma D.4 and holds with probability 1 - 2N exp(-100(n + p)). To show the second statement, for a fixed offset 1  ¯  L, consider Definition D.1 and the matrices H~ (¯), U~ (¯), X~ (¯). Observe that X is obtained by merging multiple sub-trajectory matrices

17

Under review as a conference paper at ICLR 2019

{X~ (¯)}L¯=1. We will first show the advertised bound for an individual X~ (¯) by applying Lemma D.2 and then apply Lemma A.1 to obtain the bound on the combined matrix X.

Recall that N¯¯ is the length of the ¯th sub-trajectory i.e. number of rows of X~ (¯). By construction 2N0 

N¯¯ N0

 N0 for all 1 is chosen to be

 ¯ large

 L. Given 1  ¯  L and triple H¯ (¯), enough, applying Theorem D.2 with µ =

H1~/(¯),+U~c(h¯)o,icsee,t

Q = [µH¯ and noting

find that, with probability 1 - 4 exp(-c1N0/2), all matrices M satisfying M - H¯ (¯) 

(¯) U~ (¯)]. Since  = +/-, we
-N0/10 and

Q~ as in (D.2) obeys

 ( + 2)2

Q~T Q~

-1/2.

N

(D.5)

Let us call this Event 1. To proceed, we will argue that with high probability H~ (¯) - H¯ (¯) is small so that

the bound above is applicable with M = H~ (¯) choice; which sets Q~ = X~ (¯) in (D.5). Applying Lemma C.6, we find that, with probability 1 - 2N¯¯ exp(-100n),

H¯ (¯) - H~ (¯)

  2N0 max{

h(i) - h¯ (i)

 2 }  c0 2N0 n+

A

L-1 .

Let us call H¯ (¯) -

this Event H~ (¯) 

2. W-eNw0i/ll1s0h.owSetht act o=urmchaoxi{ce20o0f cL20

ensures right hand side is small enough and guarantees , 1}. Desired claim follows by taking logarithms of

upper/lower bounds and cancelling out N0 terms as follows

 c0 n

A

L-1

 +



 -/10 2



(L - 1) log

A

+

log

 cn



0

(D.6)

 - log cn  L - 1 2 log A

(D.7)

= L = 1 - log (cn) . log A

(D.8)

Here we use the fact that log A < 0 since A < 1 and cn  0. Consequently, both Event 1 and Event 2 hold with probability 1-4 exp(-c1N0/2)-2N¯¯ exp(-100n), implying (D.5) holds with Q~ = X~ (¯). Union bounding this over 1  ¯  L, (D.5) uniformly holds with Q~ = X~ (¯) and all rows of X are 2-bounded with probability 1 - 4N exp(-100n) - 8L exp(-c1N0/2). Applying Lemma A.1 on (X~ (¯))L¯=1, we conclude with the bound (D.4) on the merged matrix X.

Lemma D.4 ( 2-bound on rows). Consider the setup of Theorem D.3. With probability 1 - 2N exp(-100(n + p)), each row of X has 2-norm at most c p + n for some constant c > 0.

Proof. The tth row of X is equal to xt = [ htT+ uTt ]T . Since ht - E[ht] 2  O(+) and ut 2 

O(1), we have that xt - E[xt] 2  O(1). Now, applying Lemma F.3 on all rows {xt}tN=1, andusing a

union for all

bound, with probability at t. To conclude, note that

least 1 E[xt]

-
2

2N =

Eex[hpt(]-120/0(n++p)),

we have that xt - E[xt]  3 n via Assumption 1.

2 c

n+p

E PROOFS OF MAIN RESULTS

E.1 PROOF OF LEMMA 3.2 The statement follows from upper bound Lemma B.3 and lower bound Lemma B.5.

E.2 PROOF OF THEOREM 4.2

Proof. To prove this theorem, we combine Theorem D.3 with deterministic SGD convergence result of Theorem 4.1. Applying Theorem D.3, with the desired probability, inequality (D.4) holds and for all i, input data satisfies
the bound xi 2  (n + p)/(2c0) for a sufficiently small constant c0 > 0. As the next step, we will argue that these two events imply the convergence of SGD.

Let (i), c(i)  Rn+p denote the ith rows of , C respectively. Observe that the square-loss is separable along

the rows of C via

-C

2 F

=

n i=1

(i) - c(i) 2 . Hence, SGD updates each row c(i) via its own state 2

equation

yt,i = ( c(i), xt ),

where yt,i is the ith entry of yt. Consequently, we can establish the convergence result for an individual row of C. Convergence of all individual rows will imply the convergence of the overall matrix  to the ground

18

Under review as a conference paper at ICLR 2019

truth C. Pick a row index i (1  i  n), set c = c(i) and denote ith row of  by  . Also denote the label corresponding to ith row by yt = yt,i. With this notation, SGD over (2.3) runs SGD over the ith row with equations yt = ( c, xt ) and with loss functions

L() = N -1

N

Lt(),

Lt()

=

1 2 (yt

- (

, xt

))2.

t=1

S+ub=stit(uti+ngo2u)r2h,iagnhd-pro-b=abili-ty1

bounds on xt (e.g. (D.4)) into Theorem 4.1, we /2. Consequently, using the learning rate  = c0

can set B = (n +

 2 -1 (+ 2)2(n+p)

,

for

p)/(2c0), all   0,

the  th SGD iteration  obeys

E[



-c

2 2

]



0 - c

2 2

(1

-

c0

2(

+

4-2 2)2(n

+

p)

)

,

(E.1)

where the expectation is over the random selection of SGD updates. This establishes the convergence for a particular row of C. Summing up these inequalities (E.1) over all rows (1), . . . , (n) (which converge to c(1), . . . , c(n) respectively) yields the targeted bound (4.4).

E.3 PROOFS OF MAIN RESULTS ON STABLE SYSTEMS

E.3.1 PROOF OF THEOREM 3.3

Proof. Applying Lemmas B.3 and 3.2, independent of L, Assumption 1 holds with parameters

+ = B2

,

- = 2smin(B)2

,

   = 6n - 2  n.

This

yields

(

+

 2)2

=

6n.

Hence,

we

can

apply

Theorem

4.2

with

the

learning

rate



=

c0

2 6n(n+p)

where

 = B2 = + , 2smin(B)2 -

(E.2)

and

convergence

rate

1

-

2 2

.

To

conclude

with

the

stated

result,

we

use

the

change

of

variable

c0/6



c0.

E.3.2 PROOF OF THEOREM 3.4

Proof. The proof is similar to that of Theorem 3.3. Applying Lemmas B.3, B.4, and 3.2, independent of L, Assumption 1 holds with parameters

+ = B2 , - = smin(B)2 ,  = 0.

Hence,

we

again

apply

Theorem

4.2

with

the

learning

rate



=

c0

2 2(n+p)

where



is

given

by

(E.2).

Use

the

change of variable c0/2  c0 to conclude with the stated result.

E.4 LEARNING UNSTABLE SYSTEMS

In a similar fashion to Section 4, we provide a more general result on unstable systems that makes a parametric assumption on the statistical properties of the state vector.

Assumption 2 (Well-behaved state vector ­ single timestamp). Given timestamp T0 > 0, there exists positive

scalars

+, -, 

and

an

absolute

constant

C

>

0

such

that



3 

n and the following holds 

+In [hT0 ] -In , hT0 - E[hT0 ] 2  C + and E[ht] 2   +.

(E.3)

The next theorem provides the parametrized result on unstable systems based on this assumption.

Theorem E.1 (Unstable system - general). Suppose we are given N independent trajectories (ht(i), u(ti))t0 for 1  i  N . Sample each trajectory at time T0 to obtain N samples (yi, hi, ui)iN=1 where ith sample is

(yi, hi, ui) = (hT(i0)+1, hT(i0), u(Ti0)).

Let C, c0 > 0 be absolute constants. Suppose Assumption 1 holds with T0 and sample size satisfies N 

C2(n + p) where  = +/-. Assume  is -increasing, zero initial state conditions, and ut i.i.d. N (0, Ip).

Set

scaling

to

be

µ

=

 1/ +

and

learning

rate

to

be



=

c0

2 (+ 2)2

(n+p)

.

Starting

from

0,

we

run

SGD

over

the

equations

described

in

(2.2)

and

(2.3).

With

probability

1

-

2N

exp(-100(n

+

p))

-

4

exp(-O(

N 2

)),

all iterates satisfy

E[

i - C

2 F

]



(1

-

c0

22(

+

4 2)2(n

+

p)

)

0 - C

2 F

,

where the expectation is over the randomness of the SGD updates.

19

Under review as a conference paper at ICLR 2019

Proof. Set xi = [+-1/2hiT uiT ]T and X = [x1 . . . xN ]T . Since X has i.i.d. rows, we can apply Theorem F.1 and Lemma F.3 to find with the desired probability that

 · Rows of xi satisfy xi - E[xi] 2  O(1) and E[ xi 2 ]  3 n, hence all rows of X obeys
xi 2  (n + p)/(2c0),

· X satisfies

 ( + 2)2

XT X N

-1/2.

To

proceed,

using

-

=

-1/2,

B

=

(n

+

p)/(2c0),

and

+

=

(

+

 2)2,

we

apply

Theorem

4.1

on

the

loss

function (2.3); which yields the desired result.

E.5 PROOF OF THEOREM 5.1

Proof. The proof Applying Lemma

is a B.3,

corollary of Theorem E.1. We need we can substitute + = BT20 and  =

to substitute the proper values in 6n - 2  n. Next, we need

Assumption 2. to find a lower

bound. Applying Lemma 3.2 for n > 1 and Lemma B.6 for n = 1, we can substitute - = +/ with the 

definition of (5.2). With these, the result follows as an immediate corollary of Theorem E.1.

F SUPPLEMENTARY STATISTICAL RESULTS

The following theorem bounds the empirical covariance of matrices with independent subgaussian rows. Given a random vector x, define the de-biasing operation as zm(x) = x - E[x].

Theorem F.1.

Let A



n×d
R

be

a

matrix

with

independent

subgaussian

rows

{ai }in=1

satisfying

zm(ai) 2 Suppose n 

 O(K) and [ai] O(K4d/2). Then, with

K2Id for some K probability at least

1>-04aenxdp(-Ec[aKi]-422n),.

Suppose [ai]

Id.

+

3/2K  1 n

A



1 n

smin(A)



2/3.

Proof. Let E = E[A], A¯ = A - E[A], a¯i = zm(ai). Observe that

Av

2 2

-E[

Av

2 2

]

=

A¯v

2 2

+2vT

A¯E

v+

Ev

2 2

-E[

Av

2 2

]

=

A¯v

2 2

-E[

A¯v

2 2

]+2vT

A¯T

E

v

.

Define the random process Xv =

A¯v

2 2

and

Yv

=

Xv

-

E[Xv ].

First,

we

provide

a

deviation

bound

for

the

quantity supvSd-1 |Yv|. To achieve this, we will utilize Talagrand's mixed tail bound and show that increments

of Yv are subexpoential. Pick two unit vectors v, u  Rd. Write x = u + v, y = u - v. We have that

n

Xu -Xv =

A¯u 2 - A¯v 2 = 22

A¯(x + y)/2 2 - A¯(x - y)/2 2 = xT A¯T A¯y = 22

(a¯iT x)(a¯Ti y).

i=1

Letting x^ = x/ x 2 , y^ = y/ y 2 , observe that, multiplication of subgaussians xT a¯i, yT a¯i obey (xT a¯i)(yT a¯i) 1  O( x 2 y 2 K2)  O(K2 y 2 ).

Centering this subexponential variable around zero introduces a factor of 2 when bounding subexponential norm and yields (xT a¯i)(yT a¯i) - E[(xT a¯i)(yT a¯i)] 1  O(K2 y 2 ). Now, using the fact that Yu - Yv is
sum of n independent zero-mean subexponential random variables, we have the tail bound

P(n-1|Yu

-

Yv |



t)



2

exp(-c

n

min{

K4

t2 y

2

, K2

t y

2

}).
2

Applying Talagrand's chaining bound for mixed tail processes with distance metrics 2

=

K2 ·  n

2 , 1

=

K2 · n

2 , (Theorem 3.5 of Dirksen (2013) or Theorem 2.2.23 of Talagrand (2014)) and using the fact that for

unit sphere Sd-1, Talagrand's  functionals (see Talagrand (2014)) obey 1(Sd-1), 22(Sd-1)  O(d),

n-1

sup

|Yv|  cK2(

 d/n + d/n + t/ n),

v S d-1

(F.1)

20

Under review as a conference paper at ICLR 2019

with t=

1p16rcoKba-b2ility n1,

-

2

exp(-

min{t2,

 nt}).

Since

n



with probability 1 - 2 exp(-O(K-42

C-2K4d for n)), we ensure

sufficiently large C that right hand side

> 0, picking of (F.1) is less

than /8. This leads to the following inequalities

1 n

A¯T A¯ - E[A¯T A¯]

 8

=

9K 2 8 Id

1 A¯T A¯ n

7 8 Id.

=

9 K  1 8n

A¯

 smin(A¯) 

7 . 8

(F.2)

Denote the A¯T 1n =

size n

n i=1

a¯i

all ones  Rd is

vector a vector

by 1n. Next, satisfying A¯T

we define the 
1n/ n 2 

process Zv

=

1 n

1nT

A¯v.

O(K). Hence, again using n

Observe that  CK4-2d

for sufficiently large C > 0, applying Lemma F.3 with m = c0K-42n > d by picking a sufficiently small

constant c0 > 1/C, with probability at least 1 - 2 exp(-100c0K-42n)



1 n

sup |Zv|
v 2 =1

=

1 n

A¯T 1n

2



1 KK-2 12



 .
12

Let P

=

In -

1 n

1n1nT

be the projection onto the orthogonal complement of the all ones vector.

Note that

P Ev = 0 as the rows of E are equal. With this observation, with desired probability, for any unit length v,

Av 2  P Av 2 = P A¯v 2  A¯v 2 - |Zv|   smin(A¯) - sup |Zv|  ( 7/8 - 1/12) n,
v S d-1
 which implies smin(A)/ n  2/3. For spectral norm of A, we use the naive bound

(F.3) (F.4)

1 A n

 1 ( E n

+

A¯ )  max E[ai] 1in

2 + 9K/8   +

3/2K.

The

corollary

below

is

obtained

by

slightly

modifying

the

proof

above

by

using

1 n

A¯T A¯ - E[A¯T A¯]



K2 8

in line (F.2) and only focusing on the spectral norm bound.

Corollary F.2. Let A  Rn×d be a matrix with independent {ai}in=1 subgaussian rows satisfying

zm(ai) 2 Suppose n 

 O(K) O(K 2 d).

and [ai] Then, with

K2Id for probability at

some K least 1 -

> 0 and E[ai] 4 exp(-cK-2n),

2



.

Suppose [ai]

Id.

 + 3/2K  1 A . n

The following lemma is fairly standard and is proved for the sake of completeness.
Lemma F.3 (Subgaussian vector length). Let a  Rn be a zero-mean subgaussian vector with a 2  L. Then, for any m  n, there exists C > 0 such that
 P( a 2  CL m)  1 - 2 exp(-100m).

Proof. We can implies, P(|vT

pick a| 

a 1/2 t) 

cover C 2 exp(-

of the

ct2 2L2

).

unit 2-sphere with size Setting t = CL m for

log |C|  2n. For any v  sufficiently large constant

C C

, subgaussianity > 0, and union

bounding over all v  C, we find

P(

v

2



 CL m)



1

-

2 exp(2n

-

cC 2 L2 m 2L2 )



1

-

2 exp(-100m).

vC

To conclude, let v(a)  C be a's neighbor satisfying

v-

a a2

2  1/2. Hence, we have

a

2

(a - v(a))T a

2+

vT a

2

a

 2 /2 + CL m =

 a 2  2CL m.

To conclude, use the change of variable C  C/2.

21

