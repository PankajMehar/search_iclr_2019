Under review as a conference paper at ICLR 2019
APPROXIMATION AND NON-PARAMETRIC ESTIMATION OF RESNET-TYPE CONVOLUTIONAL NEURAL NETWORKS VIA BLOCK-SPARSE FULLY-CONNECTED NEU-
RAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We develop new approximation and statistical learning theories of convolutional neural networks (CNNs) via the ResNet-type structure where the channel size, width, and filter size are fixed. It is shown that a ResNet-type CNN is a universal approximator and its expression ability is no worse than fully connected neural networks (FNNs) with a block-sparse structure even if the size of each layer in the CNN is fixed. Our result is general in the sense that we can automatically translate any approximation rate achieved by block-sparse FNNs into that by CNNs. Thanks to the general theory, it is shown that learning on CNNs satisfies optimality in approximation and estimation of several important function classes. As applications, we consider two types of function classes to be estimated: the Barron class and the Ho¨lder class. We prove the regularized empirical risk minimization (ERM) estimator can achieve the same rate as FNNs even the channel size, filter size, and width of CNNs are constant with respect to the sample size. This is minimax optimal (up to logarithmic factors) for the Ho¨lder class. Our proof is based on sophisticated evaluations of the covering number of CNNs and the non-trivial parameter rescaling technique to control the Lipschitz constant of CNNs to be constructed.
1 INTRODUCTION
Convolutional Neural Network (CNN) is one of the most popular architectures in deep learning research, with various applications such as computer vision (Krizhevsky et al. (2012)), natural language processing (Wu et al. (2016)), and sequence analysis in bioinformatics (Alipanahi et al. (2015), Zhou & Troyanskaya (2015)). Despite practical popularity, theoretical justification for the power of CNNs is still scarce from the viewpoint of statistical learning theory.
For fully-connected neural networks (FNNs), there is a lot of existing work, dating back to the 80's, for theoretical explanation regarding their approximation ability (Cybenko (1989), Barron (1993), Lu et al. (2017), Yarotsky (2017), and Petersen & Voigtlaender (2017)) and generalization power (Barron (1994), Arora et al. (2018), and Suzuki (2018)). See also Pinkus (2005) and Kainen et al. (2013) for surveys of earlier works. Although less common compared to FNNs, recently, statistical learning theory for CNNs has been studied, both about approximation ability (Zhou (2018), Yarotsky (2018), Petersen & Voigtlaender (2018)) and about generalization power (Zhou & Feng (2018)). One of the standard approaches is to relate the approximate power of CNNs with that of FNNs, either deep or shallow. For example, Zhou (2018) proved that CNNs are a universal approximator of the Barron class (Barron (1993), Klusowski & Barron (2016)), which is a historically important function class in the approximation theory. Their approach is to approximate the function using a 3-layered FNN (i.e., an FNN with a single hidden layer) with the ReLU activation function (Krizhevsky et al. (2012)) and transform the FNN into a CNN. Very recently independent of ours, Petersen & Voigtlaender (2018) showed any function realizable with an FNN can extend to an equivariant function realizable by a CNN that has the same order of parameters. However, to the best of our knowledge, no CNNs that achieves the minimax optimal rate in important function classes, including the Ho¨lder class, can keep the number of units in each layer constant with respect
1

Under review as a conference paper at ICLR 2019

to the sample size. Considering that architectures that have extremely large depth, while moderate channel size and width have become feasible, thanks to recent methods such as identity mappings (He et al. (2016), Huang et al. (2018)), sophisticated initialization schemes (He et al. (2015), Chen et al. (2018)), and normalization techniques (Ioffe & Szegedy (2015), Miyato et al. (2018)). we would argue that there are growing demands for theories which can accommodate such constantsize architectures.

In this paper, we analyze the learning ability of ResNet-type ReLU CNNs which have identity mappings and constant-width residual blocks with fixed-size filters. There are mainly two reasons that motivate us to study this type of CNNs. First, although ResNet is the de facto architecture in various practical applications, the approximation theory for ResNet has not been explored extensively, especially from the viewpoint of the relationship between FNNs and CNNs. Second, constant-width CNNs are critical building blocks not only in ResNet but also in various modern CNNs such as Inception (Szegedy et al. (2015)), DenseNet (Huang et al. (2017)), and U-Net (Ronneberger et al. (2015)), to name a few. Our strategy is to replicate the learning ability of FNNs by constructing tailored ResNet-type CNNs. To do so, we pay attention to the block-sparse structure of an FNN, which roughly means that it consists of a linear combination of multiple (possibly dense) FNNs (we define rigorously in the subsequent sections). Block-sparseness decreases the model complexity coming from the combinatorial sparsity patterns and promotes better bounds. Therefore, it is often utilized, both implicitly or explicitly, in the approximation and learning theory of FNNs (e.g., Bo¨lcskei et al. (2017), Yarotsky (2018)). We first prove that if an FNN is block-sparse, then we can realize the FNN with a ResNet-type CNN with O(M ) additional parameters, which are often negligible since the original FNN already has (M ) parameters in typical settings. Using this approximation, we give the upper bound of the estimation error of CNNs in terms of the approximation errors of block sparse FNNs and the model complexity of CNNs. Our result is general in the sense that it is not restricted to a specific function class, as long as we can approximate it using block-sparse FNNs.

To demonstrate the wide applicability of our methods, we derive the approximation and estimation

errors for two types of function classes with the same strategy: Barron class (of parameter s = 2) and

Ho¨lder class. We prove, as corollaries, that our CNNs can achieve the approximation error of order

O~(M

-

1 2

-

1 D

)

for

the

Barron

class

and

O~(M

-

 D

)

for

the

 -Ho¨ lder

class

and

the

estimation

error

of

order

O~p

(N

-

D+2 2(D+1)

)

for

the

Barron

class

and

O~p(N

-

2 2+D

),

where

M

is

the

number

of

non-zero

parameters and N is the sample size. These rates are same as the ones for FNNs ever known in

the existing literature. An important consequence of our theory is that the ResNet-type CNN can

achieve the minimax optimal estimation error (up to logarithmic factors) for -Ho¨lder class even if

it is constant-width, constant-filter-size, and constant-channel-size with respect to the sample size,

as opposed to existing works such as Yarotsky (2017) and Petersen & Voigtlaender (2018), where

optimal FNNs or CNNs could have a width or a channel size proportional to the sample size.

In summary, the contributions of our work are as follows:

· We develop the approximation theory for CNNs via ResNet-type architectures with constant-width residual blocks. We prove any M -way block-sparse FNN is realizable such CNN with O(M ) additional parameters. That means if FNNs can approximate a function with O(M ) parameters, we can approximate the function with CNNs at the same rate (Theorem 1).
· We derive the upper bound of the estimation error in terms of the approximation error of FNNs and the model complexity of CNNs (Corollary 2). This result gives the sufficient conditions on to derive the same estimation error as that of FNNs (Theorem 1).
· We apply our general theory to the Barron class and Ho¨lder class and derive the approximation (Corollary 2 and 4) and estimation error rates (Corollary 3 and 5), which are identical to those for FNNs, even if the CNNs have constant channel and filter size with respect to the sample size. In particular, this is minimax optimal for the Ho¨lder case.

2 RELATED WORK
We summarize in Table 1 the differences in the CNN architectures between our work and Zhou (2018) and Petersen & Voigtlaender (2018), which established the approximation theory of CNNs

2

Under review as a conference paper at ICLR 2019

CNN type
Function type
Channel size (Dense FNN case)
Channel size (-Ho¨lder case)
Width Filter size Norm bound Padding

Zhou (2018)
Conventional Barron (s = 2)
1
N.A. Increasing
Fixed No Yes

Petersen & Voigtlaender (2018)
Conventional Any
(FNNs)
1

O~

(-

D 

)

Fixed Full Yes No

Ours
ResNet Any
(block-sparse FNNs) 1
O(1)
Fixed Fixed Yes Yes

Table 1: Comparison of CNN architectures. "Channel size (Dense FNN case)": The number of channels needed to realize a function represented by a fixed-width dense FNN. "Channel size (Ho¨lder case)": The number of channles needed to approximate a -Ho¨lder function with accuracy  measured by the sup norm. "Increasing": The width of layer is monotonically increasing. "Full": Filter size is as large as the layer width. "Padding": Whether the theory includes convolution operations with padding.

via FNNs. First and foremost, Zhou (2018) only considered a specific function class -- the Bar-

ron class -- as a target function class, although their method is applicable to any function class

that can be realized by a 3-layered ReLU FNN. Regarding the architecture, they considered CNNs

with a single channel and whose width is "linearly increasing" (Zhou (2018)) layer by layer. For

regression or classification problems, it is rare to use such an architecture. Also, since they did not

bound the norm of parameters in the approximating CNNs, we cannot derive the estimation error

from this method. Petersen & Voigtlaender (2018) fully utilized the group invariance structure of

underlying input space to construct the approximating CNN. Such group structure makes theoretical

analysis easier, especially for investigating the equivariance properties of CNNs since it enables us

to incorporate mathematical tools such as group theory, Fourier analysis, and representation theory.

Although their results are quite strong in that it is applicable to any function that can be approxi-

mated by FNNs, their assumption on the group structure excludes the padding convolution layer, an

important and popular type of convolution operation. Another point is that if we simply apply their

construction method to derive the estimation error for (equivariant) Ho¨lder functions, combined with

the approximation result of Yarotsky (2017), the resulting CNN that achieves the minimax optimal

rate

has

O~(-

D 

)

channels

where



is

the

approximation

error

threshold.

It

is

partly

because

their

construction is not aware of the internal sparse structure of approximating FNNs. Finally, the filter

size of their CNN is as large as the input dimension. As opposed to these two works, we employ

padding-type ResNet-type CNNs which have constant width, multiple channels, and fixed-size fil-

ters. Like Petersen & Voigtlaender (2018), our result is applicable to any function, as long as the

FNNs to be approximated are block sparse, including the Barron and Ho¨lder cases. If we apply

our theorem to these classes, we can show that the optimal CNNs can achieve the same approxima-

tion and estimation rate as FNNs, while the number of channels is independent of the sample size.

Further, this is minimax optimal up to the logarithmic factors for the Ho¨lder class.

Due to its practical success, theoretical analysis for ResNet has been explored recently (e.g., Lin & Jegelka (2018), Lu et al. (2018), Nitanda & Suzuki (2018), and Huang et al. (2018)). From the viewpoint of statistical learning theory, Nitanda & Suzuki (2018) and Huang et al. (2018) investigated the generalization power of ResNet from the perspective of the boosting interpretation. However, they did not discuss the function approximation ability of ResNet. To the best of our knowledge, our theory is the first work to provide the approximation ability of the CNN class that can accommodate the ResNet-type ones.

We import the approximation theories for FNNs, especially ones for the Ho¨lder class and the Barron

class. Yarotsky (2017) proved FNNs with O(M ) parameters can approximate -Ho¨lder continuous

functions

with

the

order

of

O~(M

-

 D

).

Using this bound, Schmidt-Hieber (2017) proved that the

estimation

error

of

the

ERM

estimator

is

O~(N

-

2 2+D

),

which

is

minimax

optimal

up

to

logarithmic

3

Under review as a conference paper at ICLR 2019

factors (see, e.g., Tsybakov (2008)). The approximation theory for the Barron class has been inves-

tigated in e.g., Barron (1993), Klusowski & Barron (2016), and Lee et al. (2017). Originally Barron

(1993) considered the case s = 1 and activation functions  satisfying (z)  1 as z   and

(z)  0 as z  -. Later, Klusowski & Barron (2016) studied the approximation theory with

s = 2 and proved that 3-layered ReLU FNNs with M hidden units can approximate functions of

this

class

with

the

order

of

O~(M

-(

1 2

+

1 D

)).

3 PROBLEM SETTING

3.1 REGULARIZED EMPIRICAL RISK MINIMIZATION

We consider a regression task in this paper. Let X be a [-1, 1]D-valued random variable with unknown probability distribution PX and  be an independent random noise drawn from the Gaussian distribution with an unknown variance 2:   N (0, 2) ( > 0). Let f  be an unknown deterministic function f  : [-1, 1]D  R (we will characterize f  rigorously in the theorems later). We define a random variable Y by Y := f (X) + . We denote the joint distribution of (X, Y ) by P. Suppose we are given a dataset D = ((x1, y1), . . . , (xN , yN )) independently and identically sampled from the distribution P, we want to estimate the true function f  from the finite dataset D.

We evaluate the performance of an estimator by the squared error. For a measurable function f : [-1, 1]D  R, we define the empirical error of f by R^D(f ) := Nn=1(yn - f (xn))2 and the estimation error by R(f ) := EX,Y (f (X) - Y )2 . Given a set F consisting of measurable
functions from [-1, 1]D  R, we consider the regularized empirical risk minimization (ERM) estimator f^ of F that satisfies

f^ := clip[fmin] where fmin  arg min R^D(clip[f ]).
f F

Here, clip is the clipping operator defined by clip[f ] := (f  - f  )  f  . For a measurable function f : [-1, 1]D  R, we define the L2-norm (weighted by PX ) and the sup norm of f by

1
f L2(PX ) := [-1,1]D f 2(x)dPX (x) 2 and f  := supx[-1,1]D |f (x)|, respectively. Let

L2(PX ) be the set of measurable functions f such that f L2(PX) <  with the norm · L2(PX). The task is to estimate the approximation error minfF f - f   and the estimation error of the

regularized ERM estimator: R(f^) - R(f ). Note that the estimation error is a random variable with

respect the choice of the training dataset D. By the definition of R and the independence of X and

, the estimation error equals to

f^ - f 

2 L2

(PX

)

.

3.2 CONVOLUTIONAL NEURAL NETWORKS

In this section, we define CNNs used in this paper. For this purpose, it is convenient to introduce 0,

the set of real-valued sequences whose finitely many elements are non-zero: 0 := {w = (wn)nN | N  N s.t. wn = 0, n  N }. w = (w1, . . . , wK )  RK can be regarded as an element of 0

by setting wn = 0 for all n > K. Likewise, for C, C  N>0, which will be the input and output

channel sizes, respectively, we can think of (wk,j,i)k[K],j[C ],i[C]  RK×C ×C as an element

of

C 0

×C .

For a filter w

= (wn,j,i)nN,i[C],j[C ]



C 0

×C ,

we

define

the

one-sided

padding,

stride-one convolution by w as an order-4 tensor LDw = ((LDw ),,ji )  RD×D×C ×C defined by

(LDw ),,ji :=

w(-),j,i 0

if 0   -   D - 1 otherwise.

Here, i (resp. j) runs through 1 to C (resp. C ) and  and  runs through 1 to D. Since we fix the input dimension D throughout the paper, we will omit the subscript D and write as Lw if it is obvious from context for notational simplicity.
Remark 1. For K  K , we can embed RK into RK by inserting zeros: w = (w1, . . . , wK )  w = (w1, . . . , wK , 0, . . . , 0). It is easy to show Lw = Lw . Using this, we can expand a size-K filter to size-K .

4

Under review as a conference paper at ICLR 2019

Figure 1: ResNet-type CNN defined in Definition 1. Variables are as in Definition 1.

We can interpret Lw as a linear mapping from RD×C to RD×C . Specifically, for x = (x,i),i  RD×C , we define (y,j ),j = LwD(x)  RD×C by
yj := (Lw),,ji x,i.
i,

Next, we define the building block of CNNs: convolutional layers and fully-connected layers. Let
C, C , K  N>0 be the input channel size, output channel size, and filter size, respectively. For a
weight tensor w  RK×C ×C , a bias vector b  RC , and an activation function  : R  R, we
define the one-sided padding convolutional layer Convw ,b : RD×C  RD×C by Convw,b(x) := (LwD(x) - 1D  b) where,  is the outer product of vectors and  is applied in element-wise manner. Similarly, let W  RD×C , b  R and  : R  R, we define the fully connected layer FCW ,b : RD×C  R by FCW ,b(a) = (vec(W ) vec(a) - b). Here, vec(·) is the vectorization operator that flattens a matrix into a vector.

Finally, we define the ResNet-type CNN. We define it as a sequential concatenation of one convolu-
tion block, M residual blocks, and one fully connected layer. Figure 1 is the schematic view of the
CNN we adopt in this paper.
Definition 1. Let M  N>0 and Lm  N>0, which will be the number of residual blocks and the depth of m-th block, respectively. Let Cm(l), Km(l) be the channel size and filter size of the l-th layer of the m-th block for m = 0, . . . , M 1 and l  [Lm]. We assume C0(L0) = C1(L1) = · · · = CM(LM ). Let wm(l)  R ,Km(l)×Cm(l)×Cm(l-1) bm(l)  R be the weight tensors and biases of l-th layer of the m-th block in the convolution part, respectively. Finally, let W  RD×C0(L0) and b  R be the weight matrix and the bias for the fully-connected layer part, respectively. For  := ((wm(l))m,l, (bm(l))m,l, W, b) and an activation function  : R  R, we define CNN : RD  RD, the CNN constructed from , by

CNN := FCWid,b  (Convw M ,bM + id)  (ConvwM-1,bM-1 + id)  · · ·  (Convw 1,b1 + id)  Convw0,b0 ,

where Convw m,bm

:=

Convid
wm(Lm ) ,b(mLm )

 Conv
wm(Lm -1) ,bm(Lm -1)

 · · ·  Convw m(1),bm(1)

and

id

:

RD×C0(L0)  RD×C0(L0) is the identity function.

Although CNN in this definition has a fully-connected layer, we refer to the stack of convolutional layers both with or without the final fully-connect layer as a CNN in this paper. We say a linear
convolutional layer or a linear CNN when the activation function  is the identity function and a
ReLU convolution layer or a ReLU CNN when  is ReLU. We borrow the term from ResNet and call Convwm,bm (m > 0) and id in the above definition the m-th residual block and the m-th identity mapping, respectively. We say a 4-tuple  is compatible with (Cm(l))m,l and (Km(l))m,l when each component of  satisfies the aforementioned dimension conditions.

For architecture parameters C = (Cm(l))m,l and K = (Km(l))m,l (m = 0, . . . , M, l  [Lm]), a sparse parameter S  N>0, and norm parameters for convolution layers B(conv) > 0 and for fully-

1Note that m starts from 0. It is convenient for our purpose.

5

Under review as a conference paper at ICLR 2019

Figure 2: Schematic view of a block-sparse FNN. Variables are as in Definition 2.

connected

layers

B(fc)

>

0,

we

define

F (CNN)

=

F ,(CNN)
C ,K ,S,B (conv) ,B (fc)

the

hypothesis

class

consist-

ing of ReLU CNNs, as follows:


    F (CNN) := CNNReLU
   

 = ((wm(l))m,l, (b(ml))m,l, W, b) is compatible with (C, K),


 

M m=0

Lm l=1

(

wm(l)

0+

b(ml) 0) +

W

0+

b 0  S,

maxm=0,...,M,l[Lm] wm(l)   bm(l)   B(conv),

 
.
 

W   b   B(fc)

 

Here, the domain of CNNs is restricted to [-1, 1]D. Note that we impose norm constraints to the convolution part and fully-connected part separately. Since the notation is cluttered, we sometimes omit the subscripts as we do in the above.

Remark 2. In this paper, we adopted one-sided padding, which is not so often used practically, in

order to make proofs simple. However, with slight modifications, all statements are true for equally-

padded convolutions, widely employed padding style which adds (approximately) a same number of

zeros to both ends of input signals, with the exception that the filter size K is restricted to K 

D 2

instead of K  D. We also discuss our design choice, especially the comparison with the original

ResNet proposed in He et al. (2016) in Section G of the appendix.

3.3 BLOCK-SPARSE FULLY-CONNECTED NEURAL NETWORKS

In this section, we mathematically define FNNs we consider in this paper, in parallel with the CNN case. Our FNN, which we coin a block-sparse FNN, consists of M possibly dense FNNs (blocks) concatenated in parallel, followed by a single fully-connected layer. We sketch the architecture of a block-sparse FNN in Figure 2.

Definition 2. Let M  N>0 be the number of blocks in an FNN. Let Dm = (Dm(1), . . . , Dm(Lm))  NL>m0 be the sequence of intermediate dimensions of the m-th block, where Lm  N>0 is the depth of the m-th block for m  [M ] 2. Let Wm(l)  RDm(l)×Dm(l-1) and b(ml)  R be the weight matrix and
the bias of the l-th layer of m-th block (with the convention Dm(0) = 1). Let wm  RDm(Lm) be the weight (sub)matrix of the final fully-connected layer corresponding to the m-th block and b  R
be the bias for the last layer. For  = ((Wm(l))m,l, (bm(l))m,l, (wm)m, b) and an activation function  : R  R, we define FNN : RD  R, the block-sparse FNN constructed from , by

M

FNN :=

wmFCW m,bm (·) - b,

m=1

where

FCWm,bm

:=

FC
Wm(Lm ) ,bm(Lm )



·

·

·

FC
Wm(1) ,bm(1)

.

We say  is compatible with (Dm(l))m,l when each component of  matches the dimension conditions determined by (Dm(l))m,l, as we do in the CNN case. Note that when Lm = 1 for all m  [M ],

2Be aware that contrary to the CNN case, m starts from 1 here.

6

Under review as a conference paper at ICLR 2019

the block-sparse FNN is a 3-layered neural network with D :=

f (x) =

D d=1

bd(ad

x

-

td)

-

b

where

ad



RD ,

bd,

td,

b



R.

M m=1

Dm(1)

units

of

the

form

For an architecture D = (Dm(l))m[M],l[Lm], a sparse parameter S  N>0 and norm parameters for

the

block

part

B(bs)

>

0

and

for

the

final

layer

B(fin)

>

0,

we

define

F (FNN)

=

F ,(FNN)
D,B (bs) ,B (fin)

the

set of function realizable by FNNs:


  F (FNN) := FNNR eLU  

 = ((Wm(l))m,l, (b(ml))m,l, (wm)m, b) is compatible with D,


 

maxm[M],l[Lm]( Wm(l)   bm(l) )  B(bs),

.

maxm[M] wm   |b|  B(fin).

 

Again, the domain is restricted to [-1, 1]D. Similar to the CNN case, we sometimes remove sub-

scripts of the function class for simplicity. We denote the number of scalar parameters (some of

which are possibly zero) of m-th block by S(Dm) :=

Lm l=1

Dm(l-1)Dm(l)

and

the

total

parameter

counts of an FNN by S(D) :=

M m=1

S(Dm)

+

Dm(Lm)

+

1.

4 MAIN THEOREMS

With the preparation in the previous sections, we state our main results of this paper. We only describe statements of theorems and corollaries and key ideas in the main article. All complete proofs are deferred to the appendix.

4.1 APPROXIMATION

Our first main theorem claims that any M -way block-sparse FNN is realizable by a ResNet-type CNN with fixed-sized filters and channels by adding O(M ) parameters, if we treat the width Dm(l) of the FNN as a constant with respect to M .

Theorem 1. Let M  N>0, K  {2, . . . D} and L0 :=

D-1 K-1

.

Let Lm  N>0, Dm(l)  N>0,

and D = (Dm(l))m[M],l[Lm]. Then there exists L  N>0, C = (Cm(l))m=0,...,M,l[Lm], K = (Km(l))m=0,...,M,l[Lm], and S  N>0 satisfying the following conditions:

1. L 

M m=1

Lm

+

M L0,

2. maxl[L] C(l)  4 maxm[M],l[Lm] Dm(l),

3. maxl[L] K(l)  K, and

4. S  S(D) + mM=1((3D + 4L0)Dm(1) + Dm(Lm)) + 3,

such that, for any B(bs), B(fin) > 0, we have a

F  F ,(FNN)
D,B (bs) ,B (fin)

(CNN) C ,K ,S,B (conv) ,B (fc)

(1)

that

is,

any

FNN

in

F (FNN)
D,B (bs) ,B (fin)

can

be

realized

by

a

CNN

in

F .(CNN)
C ,K ,S,B (conv) ,B (fc)

B(conv) = B(bs) and B(fc) = B(fin)

1



1 B(bs)

.

Here,

An immediate consequence of this theorem is that if we can approximate a function f  with a blocksparse FNN, we can also approximate f  with a CNN.

4.2 ESTIMATION
Our second main theorem bounds the estimation error of the regularized ERM estimator f^. Theorem 2. Let f  : RD  R be a measurable function and B(bs), B(fin) > 0. Let M , K, L0, Lm, D, B(conv) and B(fc) as in Theorem 1. Suppose L, C, K, S satisfies the equation (1) of Theorem 1

7

Under review as a conference paper at ICLR 2019

for B(bs) and B(fin) (their existence is ensured for any B(bs) and B(fin) if they satisfy the conditions

1

­

4.

of

Theorem

1).

Suppose

that

the

covering

nubmer

of

F (CNN)

:=

F (CNN)
C ,K ,S,B (conv) ,B (fc)

is

larger

than 3. Then, the regularized ERM estimator f^ in F := {clip[f ] | f  F (CNN)} satisfies

ED

f^ - f 

2 L2(PX )



C

inf
f F (FNN)

f - f

2 

+

SF~2 N (log 2M1M2BN )

.

(2)

Here,

F (FNN)

:=

F ,(FNN)
D,B (bs) ,B (fin)

C

>

0

is

a

universal

constant,

F~

:=

f 



 1,

B

=

B(bs)



B(fin). M1 and M2 are defined by

M

M1 := (M + 2)(S  C0(L0)D)(1  B(conv))(1  B(fc))

(1 + m)

m=0

M
Lmm+
m=0

,

M Lm
M2 :=
m=1 l=1

Cm(l-1)Cm(l)Km(l) + Cm(l)

+ C0(L0)D + 1,

where m :=

Lm l=1

Cm(l-1)Km(l)B(bs)

and

m+

:=

Lm l=1

(1



Cm(l-1)Km(l)B(bs)).

The first term of (2) is the approximation error achieved by F (FNN). We note that M1 and M2 are determined by the architectural parameters of F (CNN) -- M1 corresponds to the Lipschitz constant and M2 is the number of parameters, including zeros, of a CNN. Therefore, the second term of (2) represents the model complexity of F (CNN). There is a trade-off between the two. Using appro-
priately chosen M to balance these two terms, we can evaluate the order of estimation error with
respect to the sample size N .

Corollary 1. Under the same assumptions as Theorem 2, suppose further log M1M2(B(bs) 

B(fin)) = O~(1) as a function of M . If inffF(FNN)

f - f

2 

=

O~(M -1 )

and

S

=

O~(M 2 )

for

some constant 1, 2 > 0 independent of M , then, the regularized ERM estimator f^ of F achieves

the estimation error

f

- f^

2 L2(PX )

=

O~p

N-

21 21 +2

.

5 APPLICATION OF MAIN THEOREMS

5.1 BARRON CLASS

The Barron class is an example of the function class that can be approximated by block-sparse

FNNs. We employ the definition of Barron functions used in Klusowski & Barron (2016).

Definition 3. We say a measurable function f : [-1, 1]D  R is a Barron function with

the parameter s > 0 if f admits the Fourier representation: f (x) = FF [f ] and vf :=

RD

w

s 2

|F

[f

](w)|

dw

<

.

Here, F and F are the Fourier transformation and the inverse

Fourier transformation, respectively.

Klusowski & Barron (2016) studied the approximation of the Barron function f  with the parameter s = 2 by a linear combination of M ridge functions (i.e., a 3-layered ReLU FNN). Specifically, they showed that there exists a function fM of the form

fM := f (0) + f 

1 (0)x +
M

M

bm(amx - tm)+

m=1

(3)

with |bm|  1,

am 1 and |tm|  1, such that

f  - fM  = O~

M

-(

1 2

+

1 D

)

.

Using this

approximator fM , we can derive the same approximation order using CNNs by applying Theorem 1 with M = 1 and L1 = 1.
Corollary 2. Let f  : [-1, 1]D  R be a Barron function with the parameter s = 2 such that f (0) = 0 and f (0) = 0D. Then, for any K = 2, . . . , D, there exists a CNN f (CNN) with M non-zero parameters, whose depth is O(M ), which has at most 8 channels, and whose filter size is

at most K, such that

f  - f (CNN)  = O~

M

-(

1 2

+

1 D

)

.

8

Under review as a conference paper at ICLR 2019

We have one design choice when we apply Corollary 1 to derive the estimation error: how to set

B(bs)

and

B(fin).

Looking

at

(3),

the

naive

choice

would

be

B(bs)

:=

1

and

B(fin)

:=

1 M

.

However,

this cannot satisfy the assumption on M1 of Corollary 1, due to the term

M m=0

(1

+

m)

in

M1.

We

want the logarithm of

M m=0

(1

+

m)

to

be

O~(1).

In

order

to

do

that,

we

change

the

relative

scale

between parameters in the block-sparse part and the fully-connected part using the homogeneous

property of the ReLU function: ReLU(ax) = aReLU(x) for a > 0. The rescaling operation

enables us to

choose B(bs)

:=

1 M

and

B(fin)

=

1 to

meet the

assumption of Corollary 1.

By

setting

1

=

1 2

+

1 D

and

2

=

1,

we

obtain

the

desired

estimation

error.

Corollary 3. There exist the depth L = O

D
N 2+2D

, channel size C = O(1), filter size K 

{2, . . . , D}, the number of non-zero parameters S = O

D
N 2+2D

, norm bounds for the convolution

part B(conv) = O

-D
N 2+2D

, and for the fully-connected part B(fc) = O

D
N 2+2D

such that for

sufficiently

large

N,

the

regularized

ERM

estimator

f^ of

F

:=

{clip[f ]

|

f



F }(CNN)
C ,K ,S,B (conv) ,B (fc)

achieve the estimation error

f

-

f^

2 L2(PX )

=

O~p

N-

D+2 2(D+1)

. Here, C = (C, . . . , C)  N>L0

and K = (K, . . . , K)  N>L0.

5.2 HO¨ LDER CLASS

Yarotsky (2017) showed that FNNs with O(M ) non-zero parameters can approximate any D variate

-Ho¨lder function (

>

0)

with

the

order

of

O~(M

-

 D

).

Schmidt-Hieber (2017) also proved a

similar statement using a different construction method. They only specified their width (Schmidt-

Hieber (2017) only), depth, and non-zero parameter counts of the approximating FNN explicitly in

their statements and did not write in detail how non-zero parameters are distributed (see Theorem

1 of Yarotsky (2017) and Theorem 5 of Schmidt-Hieber (2017)). However, if we carefully look

at their proofs, we find that we can transform the FNNs they constructed into the block-sparse

ones. Therefore, we can utilize these FNNs and apply Theorem 1. To meet the assumption of

Corollary 1, we again rescale the parameters of the FNNs, as we do in the Barron class case, so that

B(conv)

=

O(

1 M

).

We

can

derive

the

approximation

error

and

estimation

error

by

setting

1

=

 D

and 2 = 1.

Corollary 4. Let  > 0, and f  : [-1, 1]D  R be a -Ho¨lder function. Then, for any K =

2, . . . , D, there exists a CNN f (CNN) with M parameters, whose depth is O(M log M ), which has

O(1) channels, and whose filter size is at most K, such that

f  - f (CNN)  = O~

M

-

 D

.

D
Corollary 5. There exist the depth L = O N 2+2D log N , channel size C = O(1), filter size

K  {2, . . . , D}, the number of non-zero parameters S = O

D
N 2+2D log N

, norm bounds for

the convolution part B(conv) = O(1), and for the fully-connected part B(fc) = O(1) such that for

sufficiently

large

N,

the

regularized

ERM

estimator

f^ of

F

:=

{clip[f ]

|

f



F }(CNN)
C ,K ,S,B (conv) ,B (fc)

achieve the estimation error

f

- f^

2 L2(PX )

=

O~p

N-

2 2+D

. Here, C = (C, . . . , C)  NL>0

and K = (K, . . . , K)  NL>0.

Since the estimation error rate of the -Ho¨lder class is Op

N-

2D 2+D

(see, e.g., Tsybakov (2008)),

Corollary 5 implies that our CNN can achieve the minimax optimal rate up to logarithmic factors even the width D, the channel size C, and the filter size K are constant with respect to the sample size N .

6 CONCLUSION
In this paper, we established new approximation and statistical learning theories for CNNs by utilizing the ResNet-type architecture of CNNs and the block-sparse structure of FNNs. We proved that any M -way block-sparse FNN is realizable using CNNs with O(M ) additional parameters, when

9

Under review as a conference paper at ICLR 2019
the width of the FNN is fixed. Using this result, we derived the approximation and estimation errors for CNNs from those for block-sparse FNNs. Our theory is general because it does not depend on a specific function class, as long as we can approximate it with block-sparse FNNs. To demonstrate the wide applicability of our results, we derived the approximation and error rates for the Barron class and the Ho¨lder class in almost same manner and showed that the estimation error of CNNs is same as that of FNNs, even if the CNNs have constant width, filter size, and channel size with respect to the sample size. The key techniques were careful evaluations of the Lipschitz constant of CNNs and non-trivial weight parameter rescaling of FNNs.
One of the interesting open questions is the role of the weight rescaling. We critically use the homogeneous property of the ReLU activation function to change the relative scale between the block sparse part and the fully-connected part, if it were not for this property, the estimation error rate would be worse. The general theory for rescaling, not restricted to the Barron nor the Ho¨lder class would be beneficial for deeper understanding of the relationship between approximation and estimation abilities of FNNs and CNNs.
Another question is when the approximation and estimation error rate of CNNs can exceed that of FNNs. We can derive the same approximation and estimation rate as FNNs essentially because we can realize block-sparse FNNs using CNNs that have the same order of parameters (see Theorem 1). Therefore, if we dig into the internal structure of FNNs, like repetition, more carefully, the CNNs might need fewer parameters and can achieve better estimation error rate. Note that, there is no hope to enhance this rate for the Ho¨lder case (up to logarithmic factors) because the estimation rate using FNNs is already minimax optimal. It is left for future research which function classes and constraints of FNNs, like block-sparseness, we should choose.
REFERENCES
Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J Frey. Predicting the sequence specificities of dna-and rna-binding proteins by deep learning. Nature biotechnology, 33 (8):831, 2015.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930­945, 1993.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine learning, 14(1):115­133, 1994.
Helmut Bo¨lcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Optimal approximation with sparsely connected deep neural networks. arXiv preprint arXiv:1705.01714, 2017.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 873­882, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http://proceedings.mlr.press/ v80/chen18i.html.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303­314, 1989.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
10

Under review as a conference paper at ICLR 2019
Furong Huang, Jordan Ash, John Langford, and Robert Schapire. Learning deep ResNet blocks sequentially using boosting theory. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2058­2067, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/huang18b.html.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 448­456, Lille, France, 07­09 Jul 2015. PMLR. URL http://proceedings. mlr.press/v37/ioffe15.html.
Paul C. Kainen, Vra Krkov, and Marcello Sanguineti. Approximating Multivariable Functions by Feedforward Neural Nets., volume 49 of Handbook on Neural Information Processing, pp. 143­ 181. Springer, 2013.
Jason M Klusowski and Andrew R Barron. Approximation by combinations of relu and squared relu ridge functions with 1 and 0 controls. arXiv preprint arXiv:1607.07819, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097­1105. Curran Associates, Inc., 2012.
Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural nets to express distributions. In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pp. 1271­1296, Amsterdam, Netherlands, 07­10 Jul 2017. PMLR. URL http://proceedings. mlr.press/v65/lee17a.html.
Hongzhou Lin and Stefanie Jegelka. Resnet with one-neuron hidden layers is a universal approximator. arXiv preprint arXiv:1806.10909, 2018.
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3276­3285, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/ lu18d.html.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. In Advances in Neural Information Processing Systems, pp. 6231­6239, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1QRgziT-.
Atsushi Nitanda and Taiji Suzuki. Functional gradient boosting based on residual network perception. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3819­3828, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http: //proceedings.mlr.press/v80/nitanda18a.html.
Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. arXiv preprint arXiv:1709.05289, 2017.
Philipp Petersen and Felix Voigtlaender. Equivalence of approximation by convolutional neural networks and fully-connected networks. arXiv preprint arXiv:1809.00973, 2018.
11

Under review as a conference paper at ICLR 2019
Allan Pinkus. Density in approximation theory. Surveys in Approximation Theory (SAT)[electronic only], 1:1­45, 2005. URL http://eudml.org/doc/51470.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234­241. Springer, 2015.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation function. arXiv preprint arXiv:1708.06633, 2017.
Taiji Suzuki. Fast generalization error bound of deep learning from a kernel perspective. In Amos Storkey and Fernando Perez-Cruz (eds.), Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pp. 1397­1406, Playa Blanca, Lanzarote, Canary Islands, 09­11 Apr 2018. PMLR. URL http://proceedings.mlr.press/v84/suzuki18a.html.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1­9, 2015.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company, Incorporated, 1st edition, 2008. ISBN 0387790519, 9780387790510.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94: 103­114, 2017.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. arXiv preprint arXiv:1804.10306, 2018.
Ding-Xuan Zhou. Universality of deep convolutional neural networks. arXiv preprint arXiv:1805.10769, 2018.
Jian Zhou and Olga G Troyanskaya. Predicting effects of noncoding variants with deep learning­ based sequence model. Nature methods, 12(10):931, 2015.
Pan Zhou and Jiashi Feng. Understanding generalization and optimization performance of deep CNNs. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5960­5969, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR. URL http: //proceedings.mlr.press/v80/zhou18a.html.
A NOTATION
For tensor a, a+ := a0 where maximum operation is performed in element-wise manner. Similarly a- := -(-a  0). Note that a = a+ - a- holds for any tensor a. For normed spaces (V, ·
V ), (W, · W ) and linear operator T : V  W we denote the operator norm of T by T op := sup v V =1 T v W . For a sequence w = (w(1), . . . , w(L)) and l  l , we denote its subsequence from l-th to l -th elements by w[l : l ] := (w(l), . . . , w(l )). 1P equals to 1 if the statement P is true, equals to 0 otherwise.
12

Under review as a conference paper at ICLR 2019

B PROOF OVERVIEW

B.1 THEOREM 1

For f (FNN)  F (FNN), we realize a CNN f (CNN) using M residual blocks by "serializing" blocks in the FNN and converting each of them into convolution layers.

First, we double the channel size using the m = 0 part of CNN (i.e., D0(L0) = 2). We will use the first channel for storing the original input signal for feeding to downstream (i.e., m  1) FNNs and

the second one for accumulating the output of each blocks, that is,

m m=1

wmFCWRemLU,bm (x)

where

wm is the weight of the final fully-connected layer corresponding to m-th dense block.

For m = 1, . . . , M , we create the m-th residual block from the m-th block of f (FNN). First, we show that for any a  RD and t  R, there exists L0-layered 4-channel ReLU CNN with O(D) nonzero parameters whose first output coordinate equals to a ridge function x  (a x - t)+ (Lemma
1 and Lemma 2). Since the first layer of m-th dense FNN is concatenation of Dm(1) hinge functions,
it is realizable by a 8Dm(1)-channel ReLU CNN with L0-layers.

For the l-th layer of the m-th block (m  [M ], l = 2, . . . , Lm(l)), we prepare a Dm(l) size-1 filters made from the weight parameters of the corresponding layer of the FNN. Observing that convolution with size-1 filter is equivalent to dimension-wise affine transformation, the first coordinate of the output of l-th layer of the CNN is inductively same as that of the m-th block of the FNN. After computing the m-th block FNN using convolutions, we add its output to the accumulating channel in the identity mapping.

Finally, we pick the first coordinate of the channel for intermediate values and subtract the bias term using the final affine transformation.

B.2 THEOREM 2 AND COROLLARY 1

We relate the approximation error of Theorem 2 with the estimation error using the covering number of the hypothesis class F (CNN). Although there are several theorems of this type, we employ the one
in Schmidt-Hieber (2017) due to its convenient form (Lemma 5). We can prove that the logarithm of the covering number of is upper bounded by (S + 1) log((B(conv)  B(fc))M1M2/) (Lemma 4) using the similar techniques to the one in Schmidt-Hieber (2017). Theorem 2 is the immediate
consequence of these two lemmas.

To prove Cororraly 1, we set M = O(N ) for some   [0, 1). Then, under the assumption of the

corolarry, we have

f

- f^

2 L2 (Px )

=

O~

max N -21 , N 2-1, N -1

from Theorem 2. The

order of

the right hand

side

with respect to

N

is minimized

when



=

1 21 +2

.

By substituting ,

we can prove Corollary 1.

C PROOF OF THEOREM 1

C.1 DECOMPOSITION OF AFFINE TRANSFORMATION

The following lemma shows that any affine transformation is realizable with a

D-1 K -1

-layered linear

CNN (without the final fully-connect layer).

Lemma 1. Let a  RD, t  R, K  {2, . . . , D - 1}, and L0 :=

D-1 K-1

. Then, there exists

RK×2×1  w(l)  RK×2×2
RK×1×2

(for l = 1) (for l = 2, . . . , L0 - 1) (for l = L0)

and b  R such that

13

Under review as a conference paper at ICLR 2019

L0 L0

1.

w(l) 0 +

b(l) 0  D + L0,

l=1]

l=1

2. max wm  = a , max b(l)  = |t|, and

l[Lo ]

l[L0 ]

3. Conviwd,b : RD  RD satisfies Convwid,b(x) = a x - t for any x  [-1, 1]D.

Proof. First, observe that the convolutional layer constructed from u = [u1 . . . uK] 

RK×1×1 takes the inner product with the first K component of the input signal: Lu(x) =

K k=1

uk

xk

.

In particular, u

=

[0

...

0

1]

 RK×1×1 works as the "left-translation" by

K - 1. Therefore, we should define w so that it takes the inner product with the left-most elements

in the first channel and shift the input signal by K - 1 with the second channel. Specifically, we

define w = (w(1), . . . , w(L0)) by

 a1 

(w(1)):,1,:

=

 

...

, 

aK

0

(w(1)):,2,:

=

 

...

 

,

0

1

0

(w(l)):,1,:

=

 

...

0

a(l-1)K


+1

...

, 

alK

0

(w(l)):,2,:

=

 

...

0

1

0

...

 

,

0

0

0

 

...

(w(L0)):,1,:

=

0  0



 

...

0

a(L0-1)K+1

...

 

aD 0

 . 



...

 

0

We set b := (0, . . . , 0, t). Then w and b satisfies the condition of the lemma.

C.2 TRANSFORMATION OF A LINEAR CNN INTO A RELU CNN

The following lemma shows that we can convert any linear CNN to a ReLU CNN that has approximately 4 times larger parameters. This type of lemma is also found in Petersen & Voigtlaender (2017) (Lemma 2.3).
Lemma 2. Let C = (C(1), . . . , C(L))  N>L0 be channel sizes K = (K(1), . . . , K(L))  N>L0 be filter sizes. Let w(l)  RK(l)×Cl×C(l) and b(l)  R(l). Consider the linear convolution layers constructed from w and b: fid := Conviwd,b : RD  RD×C(L) NL>0 where w = (w(l))l and b = (b(l))l . Then, there exists a pair w~ = (w~(l))l[L], b~ = (~b(l))l[L] where w~(l)  RK(l)×2C(l)×2C(l-1) and ~b(l)  R2C(l) such that

L

LL

L

1.

w~(l) 0  4

w(l) 0,

~b(l) 0 

b(l) 0,

l=1

l=1 l=1

l=1

2. max w~(l)  = max w(l) , max ~b(l)  = max b(l) , and

l[L]

l[L]

l[L]

l[L]

3. fReLU := ConvwR~e,b~LU : RD  RD×2C(L) , satisfies fReLU(·) = (fid(·)+, fid(·)-).

14

Under review as a conference paper at ICLR 2019

Proof. We define w~ and b~ as follows:

(w~(1))k,:,: = (w~(l))k,:,: =
~b(l) =

(w(1))k,:,: -(w(1))k,:,:
(w(l))k,:,: -(w(l))k,:,:
b(l) -b(l)

for k = 1, . . . , K(1),

-(w(l))k,:,: (w(l))k,:,:

for k = 1, · · · K(l),

By definition, a pair (w~, b~) is compatible with C~ and K and satisfies the conditions (1) and (2). For any x  RD, we set y(l) := Conviwd[1:l],b[1:l](x)  RC(l)×D. We will prove

ConvwR~e[1L:Ul],b~[1:l](x) = y+(l) y-(l)

(4)

for l = 1, . . . , L by induction. Note that we obtain fReLU = (fid+, fid-) by setting l = L. For l = 1, by definition of w~(1) we have,

(w~(1)),:,:x,: =

(w(1)),:,:x,: -(w(1)),:,:x,:

holds for any ,   [D]. Summing them up and using the definition of ~b(1) yield

[Lw~(1) (x) - 1D  ~b(1)]

=

Lw(1) (x) - 1D  b(1) - Lw(1) (x) - 1D  b(1)

Suppose (4) holds up to l (l < L), by the definition of w~(l+1),

(w~(l+1)),:,:

(y+(l)),: (y-(l)),:

=

(w(l+1)),:,: -(w(l+1)),:,:

-(w(l+1)),:,: (w(l+1)),:,:

(y+(l)),: (y-(l)),:

=

 (w(l+1)),:,:
 -(w(l+1)),:,:

(y+(l)),: - (y-(l)),: (y+(l)),: - (y-(l)),:

 

=

(w(l+1)),:,:(y(l)),: -(w(l+1)),:,:(y(l)),:

for any ,  = 0, . . . , D - 1. Again, by taking summation and using the definition of b(l+1), we get

[Lw~(l+1) ([y+(l), y-(l)]) - 1D  ~b(1)]

=

Lw(l+1) (y(l)) - 1D  b(l+1) - Lw(l+1) (y(l)) - 1D  b(l+1)

.

By applying ReLU, we get

Convp(l+1) ,ReLU
w~ (l+1) ,~b(l+1)

[y+(l), y-(l)]

= ReLU

[y(l+1), -y(l+1)]

.

(5)

By using the induction hypothesis, we get

ConvRw~e[1L:U(l+1)],b~[1:(l+1)](x)

=

Convp(l+1) ,ReLU
w~ (l+1) ,~b(l+1)

[y+(l), y-(l)]

= ReLU [y(l+1), -y(l+1)]

= [y+(l+1), -y-(l+1)]
Therefore, the claim holds for l + 1. By induction, the claim holds for L, which is what we want to prove.

15

Under review as a conference paper at ICLR 2019

C.3 CONCATENATION OF CNNS

We can concatenate two CNNs with same depth and filter size in parallel. Although it is almost trivial, we state it formally as propositions. In the following proposition, C(0) and C (0) is not necessarily 1.
Proposition 1. Let C = (C(l))l[L], C = (C (l))l[L], and K = (K(l))l[L]  N>L0. Let w(l)  R ,K(l)×C(l)×C(l-1) b  RC(l) and denote w = (w(l))l and b = (b(l))l. We define w and b in the same way, with the exception that C(l) is replaced with C (l). We define w~ = (w~(1), . . . , w~(L)) and b~ = (~b(1), . . . , ~b(L)) by

(w~(l))k,:,: :=

w(l) 0

0 w (l)

 R(C(l)+C (l))×(C(l-1)+C (l-1))

~b(l) :=

b(l) b (l)

 R(C(l)+C (l))

for l  [L] and k  [K(l)]. Then, we have, Convw~,b~([x x ]) = Convw,b(x) Convw ,b (x )
for any x, x  RD×C(0) and any  : R  R.

Note that by the definition of · 0 and · , we have

LL

w~(l) 0 =

w(l) 0 + w (l) 0,

l=1 l=1

LL

~b(l) 0 =

b(l) 0 + b (l) 0,

l=1 l=1

max w~(l)  = max w(l)   w (l) ,

l[L]

l[L]

max ~b(l)  = max b(l)   b (l) .

l[L]

l[L]

and

C.4 PROOF OF THEOREM 1

By

the

definition

of

F ,(FNN)
D,B (bs) ,B (fin)

there exists a 4-tuple 

=

((Wm(l))m,l, (b(ml))m,l, (wm)m, b)

compatible with (Dm(l))m,l such that

max (
m[M ],l[Lm]

Wm(l)



bm(l) )  B(bs),

max wm   |b|  B(fin),
m[M ]

and f (FNN) = FNNR eLU. Here m  [M ] and l  [Lm]. We will construct with the desired CNN consisting of M residual blocks, whose m-th residual block is made from the ingredients of the corresponding m-th block in f (FNN) (specifically, Wm := (Wm(l))l[Lm], bm := (b(ml))l[Lm], and wm).
[The m = 0 Block]: We prepare a single convolutional layer with 2 output channels and 2 size-1 filters suth that the first filter works as the identity function and the second filter inserts zeros to the second channel. Weight parameters of this convolutional layer are all zeros except single one. We denote this block by Conv0.
[The m = 1, . . . , M Blocks]: For fixed m  [M ], we first create a CNN realizing FCWRemLU,bm . We treat the first layer (i.e. l = 1) of FCRWemLU,bm as concatenation of Dm(1) hinge functions RD x  fd(x) := ((Wm(1))dx - b(m1))+ for d  [Dm(1)]. Here, (Wm(1))d  R1×D is the d-th row of the matrix Wm(1)  RDm(1)×D. We apply Lemma 1 and Lemma 2 and obtain ReLU CNNs realizing the hinge

16

Under review as a conference paper at ICLR 2019

functions. By combining them in parallel using Proposition 1, we have a learnable parameter m(1) such that the ReLU CNN ConvRm(e1L)U : RD×2  RD×2Dm(1) constructed from m(1) satisfies

ConvRm(e1L)U([x x ] )1 = f1(x)  · · · fDm(1) (x)  .
Since we double the channel size in the m = 0 part, the identity mapping has 2 channels. Therefore we make ConvRm(e1L)U so that it has 2 input channels and neglects the input signals coming from the second one. This is possible by adding filters consisting of zeros appropriately. They do not increase the non-zero parameters of the CNN. ConvRm(e1L)U has at most 4Dm(1) channels and at most 4Dm(1)(D + L0) non-zero parameters whose norm is bounded by Wm(1)   bm(1) .

Next,

for

l-th

layer

(l

=

2, . . . , Lm),

we

prepare

size-1

filters

wm(2)



R1×Dm(2) ×2D(1)
m

for

l

=

2

and

wm(l)  R1×Dm(l)×2Dm(l-1) for l = 3, . . . , Dm(Lm) defined by

(wm(l))1,:,: :=

Wm(2)  1 Wm(l)

0

if l = 2 if l = 3, . . . , Dm(Lm),

where  is the Kronecker product of matrices. Note that ConvRm(el)LU (l  2) just rearranges parameters of FCRWemLU,bm . Intuitively, the l = 2 layer will pick all odd indices of the output of ConvRm(e1L)U and apply the fully-connected layer.

We construct CNNs from m(l) := (wm(l), bm(l)) and concatenate them along with ConvRm(e1L)U:

Convm := ConvRm(eLLmU)  · · ·  ConvRm(e2L) U  ConvRm(e1L) U.

The output dimension of Convm is either RD×2Dm(Lm) (if Lm = 1) of RD×Dm(Lm) (if Lm  2). We denote the output channel size (either 2Dm(Lm) or Dm(Lm)) by Dm(out). By the inductive calculation,
we have

Convm(x)1 =

FCRWemLU,bm (x)  1 FCRWemLU,bm (x)

0

if if

Lm Lm

= 

1 2

.

By definition, Convm has depth L0 + Lm - 1 and, at most 4Dm(1)  maxl=2,...Lm Dm(l)  4 maxl[Lm] Dm(l) channels, and 4Dm(1)(D + L0) + (S(Dm) - Dm(1)D) = S(Dm) + (3D + 4L0)Dm(1) non-zero parameters. The -norm of its parameters does not exceed that of parameters in FCWRemLU,bm .

Next, we consider the filter w~m  R1×2×Dm(out) defined by

 0 ··· 0



(w~m)1,:,: =

B(bs)

 

B(fin) 

wm  0 ···

0 0

1



 wm

if Lm = 1 ,
if Lm  2

Then, Convm := Conviw~dm,0 adds the output of m-th residual block, weighted by wm to the second channel in the identity connections, while keeping the first channel intact. Note that the final layer of each residual block does not have the ReLU activation. By definition, Convm has Dm(Lm) parameters
Given Convm and Convm for each m  [M ], we construct a CNN realizing FNNR eLU. Let f (conv) : RD  RD be the sequential interleaving concatenation of (Convm, I) and Convm, that is,
f (conv) := (ConvM  ConvM + I)  · · ·  (Conv1  Conv1 + I)  Conv0.
Then, we have

f1(conv)

=

B(bs) B(fin)

M

wmFCWRemLU,bm

m=1

17

Under review as a conference paper at ICLR 2019

(the subscript 1 represents the first coordinate).

[Final Fully-connected Layer] Finally, we set w :=

B(fin) B(bs)

0

···

0  RD and put FCiwd,b

on top of f (conv) to pick the first coordinate of f (conv) and subtract the bias term. By definition,

f (CNN) := FCiwd,b  f (conv) satisfies f (CNN) = f (FNN).

[Condition Check]: We will check f (FNN) satisfies the desired conditions. (Condition 1): Since

Convm and Convm has L0 + Lm - 1 and 1 layers, respectively, f (CNN) has mM=1(L0 + Lm) =

L0M +

M m=1

Lm

layers,

except

the

final

fully-connected

layer.

(Condition

2):

Convm

has

at

most

4 maxl[Lm] Dm(l) channels and Convm has at most 2 channels, respectively. Therefore, the channel

size of f (CNN) is at most 4 maxm[M],l[Lm] Dm(l) (Condition (2) OK). (Condition 3): Since each

filter of Conv(m) and Convm is at most K, the filter size of CNN is also at most K. (Condition 4):

Conv0 has 1 parameter. As described previously, Convm and Convm have S(Dm) + 3Dm(1)D +

4L0D and Dm(Lm) non-zero parameters, respectively. The final fully-connected layer FCw,b has

2 parameters. Summing up, f (CNN) has Mm=1(S(Dm) + (3D + 4L0)Dm(1) + Dm(Lm)) + 3 =

S(D) + mM=1((3D + 4L0)Dm(1) + Dm(Lm)) + 3. (Conditions on B(conv) and B(fin)): Parameters

of f (conv) are either 0, or parameters of FCWRemLU,Wm , whose absolute value is bounded by B(bs) or

w .B(bs)
B(fin) m

Since

we

have

wm   B(fin), the -norm of parameters in f (CNN) is bounded by

B(bs). The parameters of the final fully-connected layer FCw,b is either B(fin), 0 or b, therefore their

norm

is

bounded

by

B(fin) B(bs)



B(fin).

Remark 3. There is an obvious way to construct a CNN which is identical (as a function) to a given

FNN. First we use a "rotation" convolution with D filters each of which has size D to serialize

all input signals to D channels of a single input dimension. Then, apply size-1 convolution layers,

whose l-th layer consisting of appropriately arranged weight parameters of l-th layer of the FNN.

This is essentially what Petersen & Voigtlaender (2018) does to prove the existence of a CNN equiv-

alent to a given FNN. To restrict the size of filters to K, we should replace the the first convolution

layer with O(D/K) convolution layers with size-K filters. We note that the resulting CNN has

S(D) + O(D2/K) non-zero parameters and cannot satisfy the condition on S. So, we need more

sophisticated way to control the non-zero parameters of the CNN.

D PROOF OF THEOREM 2

D.1 COVERING NUMBER OF CNNS
The goal of this section is to prove Lemma 4, stated in Section D.1.5, that evaluates the covering number of the set of functions realized by CNNs.

D.1.1 BOUNDS FOR CONVOLUTIONAL LAYERS
We assume throughout this subsection that the activation function  is either the ReLU function or the identity function id. In this section, we assume w, w  RK×J×I , b, b  R, and x  RD×I unless specified. We consider  : R  R is the ReLU function but the following proposition holds for any 1-Lipschitz function. Remember that we can treat Lw as a linear operator from RD×I to RD×J . We endow RD×I and RD×J with the sup norm and denote the operator norm Lw by Lw op. Proposition 2. It holds that Lw op  IK w .

Proof. Write w = (wkji)k[K],j[J],i[I], Lw = ((Lw),,ji ),[D],j[J],i[I]. For any x = (xi)[D],i[I]  RD×I , the sup norm of y := (yj )[D]j[J] = Lw(x) is evaluated as follows:

y  = max |yj|
,j

 max
,j

|(Lw),,ji ||xi|

,i

18

Under review as a conference paper at ICLR 2019

 max
,j

|(Lw),,ji | x 

,i

= max |w(-),j,i| x 
,j ,i

 max

1{w(-),j,i = 0}

,j

,i

 IK w  x 

wx

Proposition 3. It holds that Convw,b(x)   Lw op x  + |b|.

Proof.

Convw,b(x)     

(Lw(x) - 1D  b) 
Lw(x) - 1D  b  Lw(x)  + 1D  b  Lw op x  + |b|.

Proposition 4. The Lipschitz constant of Convw,b is bounded by Lw op.
Proof. For any x, x  RD×I , Convw,b(x) - Convw,b(x )  =  (Lw(x) - 1D  b) -  (Lw(x ) - 1D  b)   (Lw(x) - 1D  b) - (Lw(x ) - 1D  b)   Lw(x - x )   Lw op x - x .
Note that the first inequality holds because the ReLU function is 1-Lipschitz.
Proposition 5. It holds that Convw ,b(x) - Convw ,b (x)  Lw-w op x  + |b - b |.
Proof. Convw ,b(x) - Convw ,b (x) = (Lw(x) - 1D  b) - (Lw (x) - 1D  b )  = (Lw(x) - 1D  b) - (Lw (x) - 1D  b ) = Lw(x) - Lw (x) + 1D  (b - b )   Lw-w op x  + |b - b |

D.1.2 BOUNDS FOR FULLY-CONNECTED LAYERS
In the following propositions in this subsection, we assume W, W  RD×C , b, b  R, and x  RD×C . Again, these propositions hold for any 1-Lipschitz function  : R  R. But  = ReLU or id is enough for us. Proposition 6. It holds that |FCW,b(x)|  W 0 W  x  + |b|.
Proof. |FCW,b(x)| = |vec(W ) vec(x) - b|  |vec(W ) vec(x)| + |b|  W,ix,i + |b|
,i
The number of non-zero summand in the summation is at most W 0 and each summand is bounded by W  x  Therefore, we have |FCW,b(x)|  W 0 W  x  + b .
19

Under review as a conference paper at ICLR 2019

Proposition 7. The Lipschitz constant of FCW,b is bounded by W 0 W .
Proof. For any x, x  RD×C , |FCW,b(x) - FCW,b(x )|  (vec(W ) vec(x) - b) - (vec(W ) vec(x ) - b)  vec(W ) (vec(x) - vec(x ))  W 0 W  vec(x) - vec(x ) .

Proposition 8. It holds that |FCW ,b(x) - FCW ,b (x)|  ( W 0 + W 0) W - W  x  + |b - b |.

Proof.

|FCW,b(x) - FCW ,b (x)| = |(vec(W ) vec(x) - b) - (vec(W ) vec(x) - b )| = |(vec(W - W ) vec(x) - (b - b )|  |(vec(W - W ) vec(x)| + |b - b |  W - W 0 W - W  x  + |b - b |  ( W 0 + W 0) W - W  x  + |b - b |

D.1.3 BOUNDS RESIDUAL BLOCKS

In this section, we denote the architecture of CNNs by C = (C(l))l[L]  N>L0 and K = (K(l))l[L]  NL>0 and the norm constraint on the convolution part by B(conv) (C(0) need not equal to 1 in this section). Let wl, w (l)  R ,K(l)×Cl×C(l-1) and b(l), b (l)  R. We denote w := (w(l))l[L], b := (b(l))l[L], w := (w (l))l[L], and b := (b(l))l[L].

For 1  l  l  L, we denote (l, l ) := (C (i-1) K (i) B (conv) ).

li=l(C(i-1)K(i)B(conv)), +(l, l ) :=

l i=l

1



Proposition 9. Let l  [L]. We assume maxl[L] w(l)   b(l)   B(conv). Then, for any x  [-1, 1]D×C(0) , Convw[1:l],b[1:l](x)   (1, l) x  + B(conv)l+(1, l).

Proof. We write in shorthand as C[s:t] := Convw [s:t],b[s:t]. Using Proposition 3 recursively, we get

C[1:l](x)   Lw(l) op C[1:l-1](x)  + b(l)  ...

l
 x

l
Lw(i) op +

l
b(i-1) 

Lw(j) op + b(l) .

i=1 i=2 j=i

By Proposition 2 and assumptions w(i)   B(conv) and b(i)   B(conv), it is further bounded by

l ll

x  (C(i-1)K(i)B(conv)) + B(conv)

(C(j-1)K(j)B(conv)) + B(conv)

i=1 i=2 j=i

 (1, l) x  + B(conv)l+(1, l)

Proposition 10. Let  > 0, suppose maxl[L] w(l) - w (l)    and maxl[L] b(l) - b (l)   , then C[1:L] - C[1:L](x)   (L(1, L) x  + (1  B(conv))L2+(1, l)) for any x  RD×DC(0) .

20

Under review as a conference paper at ICLR 2019

Proof. For any l  [L], we have

C[l+1:L]  (Cl - Cl )  C[1:l-1](x)
 C[l+1:L]  (Cl - Cl )  C[1:l-1](x)   (l + 1, L) (Cl - Cl )  C[1:l-1](x)  (by Proposition 2 and 4)  (l + 1, L) (l, l) C[1:l-1]  +  (by Proposition 2 and 5)
 (l + 1, L) (l, l)((1, l - 1) x  + B(conv)(l - 1)+(1, l - 1)) + 1  (by Proposition 9)

= (1, L) x  + (1  B(conv))l+(1, L) 

(6)

Therefore,

L

C[1:L](x) - C[1:L](x)  

C[l+1:L]  (Cl - Cl )  C[1:l-1](x) 

l=1

 (L(1, L) x  + (1  B(conv))L2+(1, l))

D.1.4 PUTTING THEM ALL

Let M  N>0, Lm  N>0 for m  [M ]. Let Cm(l), Km(l)  N>0 and set C := (Cm(l))m,l and

K := (Km(l))m,l. Let  = ((wm(l))m,l, (bm(l))m,l, W, b) and  be tuples compatible with (C, K) such

that

CNNReLU,

CNNReLU



F (CNN)
C ,K ,S,B (conv) ,B (fc)

for

S



N>0,

B(conv), B(fc)

>

0.

For m  0 and l  [Lm] we denote the l-th convolution layer of the m-th block by Cm(l) and the m-th residual block of by Cm:

Cm(l) :=

Conviwdm(l) ConvwRme(lL) U

(if l = Lm) (otherwise)

Cm := Cm(Lm)  · · ·  Cm(1).

Also, we denote by C[m:m ] the subnetwork of ConvReLU between m-th and m -th block. That is,

C[m:m ] :=

(Cm + I)  · · ·  (Cm + I) (Cm + I)  · · ·  Cm

(if m  1) (if m = 0)

for m, m

=

0, . . . , M .

We

define

C

(l) m

,

Cm

and

C[m:m

]

similarly

for



.

We

denote

m(l, l

)

:=

l i=l

(C

(i-1)

K

(i)B

(conv))

for

m

=

0, .

.

.,

M

and

0



l



l

.

Note

that

we

have

m

=

m(1,

Lm)

by definition where m is a constant defined in Theorem 2.

Proposition 11. For m = 0, . . . M and x  [-1, 1]D, we have C[0:m](x)  

B

M m=0

(1

+

m)

mM=0(Lm - 1)m+ . Here +m is a constant defined in Theorem 2.

Proof. By Proposition 9,

C[0:m](x)   Cm(C[0:m-1](x)) + C[0:m-1](x)   (1 + m)C[0:m-1](x) + B(Lm - 1)+m)   (1 + m) C[0:m-1](x)  + B(Lm - 1)m+ ···

m
 B (1 + i)
i=0

m
(Li - 1)+i
i=0

21

Under review as a conference paper at ICLR 2019

Lemma 3. Let  > 0. Suppose  and 

are within distance , that is, maxm,l

wm(l)

-w

(l) m

  ,

bm(l)

-

b

(l) m

  ,

W -W

  , and

b-b

  . Then,

CNNReLU - CNNR eLU  

M1 where M1 is the function defined in Theorem 2.

Proof. For any x  [-1, 1]D, we have
CNNReLU(x) - CNNR eLU(x) = FCWid,b  C[0:M](x) - FCiWd ,b  C[0:M](x)
= (FCW,b - FCW ,b )  C[0:M](x)
M
+ FCW ,b  C[m+1:M]  (Cm - Cm)  C[0:m-1](x) . (7)
m=0

We will bound each term of (7). By Proposition 8 and Proposition 11,

FCWid,b - FCWid ,b  C[0:M](x)

 ( W 0 + W 0) W - W  C[0:M](x)  + b - b

 2(S  C0(L0)D) C[0:M](x)  + 

MM

 2(S  C0(L0)D)

(1 + m) B

(Lm - 1)+m

m=0

m=0

M

 2(S  C0(L0)D)(1  B)

(1 + m)

m=0

M
Lmm+
m=0


+ 

(8)

On the other hand, for m = 0, . . . , M ,

FCW ,b  C[m+1:M]  (Cm - Cm)  C[0:m-1](x)

 W 0 W  C[m+1:M]  (Cm - Cm)  C[1:m-1](x)  (by Proposition 7)  (S  C0(L0)D)B(fc) C[m+1:M]  (Cm - Cm)  C[0:m-1](x) 

 (S  C0(L0)D)B(fc)

M
i
i=m+1

(Cm - Cm)  C[0:m-1](x)  (by Proposition 2 and 4)

 (S  C0(L0)D)B(fc)  (S  C0(L0)D)B(fc)

M
i
i=m+1
M
i
i=m+1

m C[0:m-1]  +  (by Proposition 2 and 5)

mB

m-1
(1 + i)
i=0

m-1
(Li - 1)+i + 1 
i=0

(by Proposition 9)

M

 (S  C0(L0)D)B(fc)(1  B(conv))

(1 + i)

i=0

M
Li+i
i=0



(9)

By applying (8) and (9) to (7), we have

|CNNR eLU(x) - CNNReLU(x)|

M

 2(S  C0(L0)D)(1  B(conv))

(1 + m)

m=0

M
Lmm+
m=0



M

+ M (S  C0(L0)D)B(fc)(1  B(conv))

(1 + m)

m=0

M
Lmm+
m=0



22

Under review as a conference paper at ICLR 2019

 (M + 2)(S  C0(L0)D)(1  B(fc))(1  B(conv)) = M1.

M
(1 + m)
m=0

M
Lm+m
m=0



D.1.5 BOUNDS FOR COVERING NUMBER OF CNNS

For a metric space (M0, d), and  > 0, we denote the (external) covering number of M  M0 by N (, M, d): N (, M, d) := inf{N  N | f1, . . . , fN  M0 s.t. f  M, n  [N ] s.t. d(f, fn)  }.

Lemma 4. Let B := B(conv)  B(fc). For   (0, B], we have N (, F (CNN), · ) 

2B M1 M2 

S+1.

Proof. The idea of the proof is same as that of Lemma 12 of Schmidt-Hieber (2017). We divide the

interval

of

each

parameter

range

([-B(conv), B(conv)]

or

[-B(fc), B(fc)])

into

bins

with

width

 M1

(i.e., 2B(conv)M1-1 or 2B(fc)M1-1 bins for each interval). If f, f  F (CNN) can be realized

by parameters such that the locations of non-zero parameters are same and every pair of parameters

is in a same bin, then, f - f    by Lemma 3. We make a subset F0 of F (CNN) by picking

up every choice of combination of s (s  S) non-zero parameter choices among M2 parameters

and every combination of bins for the chosen parameters. Then, for each f  F (CNN), there exists

f0  F0 such that

f - f0   . There are

M2 s

choices of non-zero parameters and at most

2BM1-1 choices of bins for each non-zero parameter. Therefore, the cardinality of F0 is at most.

S M2 s
s=0

2BM1 

sS

s=0

2BM1M2 

s



2BM1M2

S+1
.



Here, we used the inequality

S s=0

as



aS+1

if

a



2.

D.2 PROOF OF THEOREM 2 AND COROLLARY 1

We use a lemma in Schmidt-Hieber (2017) to bound the estimation error of the regularized ERM estimator f^. Since our problem setting is slightly different from one in the paper, we restate the
statement.

Lemma 5 (cf. Schmidt-Hieber (2017) Lemma 10). Let F be a family of measurable functions

from [-1, 1]D to R. Let f^ be the regularized ERM estimator of the regression problem de-

scribed in Section 3.1. Suppose the covering number of F satisfies N (, F, · )  3. Then,

ED

f  - f^

2 L2(PX )



4

inf f F

f - f

2 L2(PX )

+

56

log

N

(F

,

1 N

,

·

) + 180

F~2 N

, where

F~

:=

RF 



f 





1 2

.

Proof. Basically, we convert our problem setting so that it fits to the assumptions of Lemma 10

of Schmidt-Hieber (2017) and apply the lemma to it. For f : [-1, 1]D  [-F~, F~], we define

A[f ]

:

[0, 1]D



[0, 2F~] by A[f ](x )

:=

1 

f

(2x

- 1) + F~.

We define X

:=

1 2

(X

+ 1),

f



:=

A[f ], Y := f (X) +  , F := {A[f ] | f  F }, f^ := A[f^], and D := ((xn, yn))n[N]

where xn

:=

1 2

(xn

+

1)

and

yn

:=

f

(xn)

+

1 

(yn

-

f (xn)).

Then, the probability that D

is drawn from P N is same as the probability that D is drawn from PN where P is the joint

distribution of (X, Y ). Also, we can show that f^ is the regularized ERM estimator of the regression

problem Y = f  +  using the dataset D : f^  arg minf F R^D (f ). We apply the Lemma 10

of

Schmidt-Hieber

(2017) with

n



N,

d



D,





1,





1 N

,

n



0,

F

 F, F  2F~,

f^  f^ to conclude.

23

Under review as a conference paper at ICLR 2019

Proof of Theorem 2. By definition of · , f  - f  L2(PX)  f  - f  . By Lemma 4,

log N

:=

log

N

(

1 N

,

F

(CNN)

,

·

)  (S + 1) log(2BM1M2N ), where B = B(conv)  B(fc).

Therefore, by Lemma 5,

f

-

f^

2 L2(PX )



4

inf
f F

f - f

2 L2(PX )

+

(56

log

N

F~2 + 180)
N

C

inf
f F

f - f

2 

+

SF~2 N

log(2BM1M2N )

.

Proof of Corollay 1. In the following theorem, we only care the order with respect to N in the Onotation. Set M = N  for   [0, 1). Using the assumptions of the corollary, the estimation error
is

f

-

f^

2 L2 (Px )

=

O~

max

N -21 , N 2-1, N -1

.

by

Theorem

2.

The

order

of

the

right

hand

side

with

respect

to

N

is

minimized

when



=

1 21 +2

.

By substituting , we can show Corollary 1.

E PROOF OF COROLLARY 2 AND COROLLARY 3

By Theorem 2 of Klusowski & Barron (2016), for each M  N>0, there exists

f (FNN) := 1 M

MM

bm(amx - tm)+ =

bm

m=1

m=1

am x - tm MM

+

with |bm|  1, Cvf log M + DM

am 1

-

1 2

-

1 D

= 1, where C

and |tm|  1 such that f  - f (FNN)  is a universal constant. We set M  1, L1 

 1,

D1

 (M

), B(bs)



1 M

, B(fin)

 1 in the Theorem 1, then, we have f (FNN)

 F .(FNN)
D1 ,B (bs) ,B (fin)

Note that the number of parameter of f (FNN) is (D + 1)M . By applying Theorem 1, there ex-

ists

a

CNN

f (CNN)



F (CNN)
C ,K ,S,B (conv) ,B (fc)

such

that

f (FNN)

=

f (CNN).

Here,

L

=

(L0

+ 1)M

,

C = (4, . . . , 4)  N>L0, K = (K, . . . , K)  N>L0, S = (3D + 5L0 + 2)M

+ 3,

B(conv)

=

1 M

,

and B(fc) = M . This proves Corollary 2.

With

these

evaluations,

M1

=

O(M 3)

(note

that

since

B(conv)

=

1 M

,

we

have

M m=0

(1

+

m)

=

O(1)). In addition, M2, B(conv) are O(1) and B(fc) is O(M ). Therefore, we can use Corollary 1

with

1

=

1 2

+

1 D

,

2

=

1

and

obtain

Corollary

3.

F PROOF OF COROLLARY 4 AND COROLLARY 5

We first prove the scaling property of the FNN class.

Lemma 6. Let M  N>0, Lm  N>0, and Dm(l)  N>0 for m  [M ] and l  [Lm]. Let

B(bs), B(fin)

>

0.

Then, for any k



1,

we

have

F (FNN)
D,B (bs) ,B (fin)

=

F (FNN)
D,k-1 B (bs) ,kL B (fin)

where

L := maxm[M] Lm is the maximum depth of the blocks.

Proof. Let  = ((Wm(l))m,l, (bm(l))m,l, (wm)m, b) be the parameter of an FNN and suppose that

FNNR eLU



F .(FNN)
D,kB (bs) ,k-L B (fin)

We

define



:=

((W

(l) m

)m,l

,

(b

m(l))m,l, (wm), b

)

by

W

(l) m

:=

k-

L Lm

Wm(l)

b

(l) m

:=

k-l

L Lm

bm(l)

wm := kLwm

b := b.

Since

k



1,

we

have

FNNR eLU



F .(FNN)
D,k-1 B (bs) ,kL B (fin)

Also,

by

the

homogeneous

property

of

the

ReLU function (i.e., ReLU(ax) = aReLU(x)), we have FNNR eLU = FNNR eLU.

24

Under review as a conference paper at ICLR 2019

Proof of Corollary 4 and Corollary 5. In this proof, we only care the dependence on M in the O-
notation. By the proof of Theorem 5 of Schmidt-Hieber (2017), for any M  N>0, there exists L = (log M ), D0 = CD  N>0 (C is a universal constant independent of M ), and a blocksparse FNN f (FNN) such that, f (FNN) has M blocks, each of which has depth L and width D0 and
satisfies

f  - f (FNN)



=

O~(M

-

 D

).

We set Lm := L and Dm(l) := D0 for all m  [M ] and l  [Lm] We also define D := (Dm(l))m,l. Then, we have f (FNN)  FD(F,N1,N1). Note that f (FNN) has S(D) = O(M log M ) parameters.

Let

k

:=

16D0K

(M

1 L

 1)-1  1.

Using Lemma 6, there exists f~(FNN)  FD(F,Nk-N1),kL

such

that f~(FNN) = f (FNN). We apply Theorem 1 to FD(F,Nk-N1),kL and obtain that there exists f (CNN) 

F (CNN)
C ,K ,S,B (conv) ,B (fc)

such that L  M (L

+ L0), Cm(l)  4D0, Km(l)  K, and S

 S(D) + (3D +

4L0 + 1)D0M + 3, B(conv) = k-1, B(fc) = kL (k  1) = kL +1, and f (CNN) = f~(FNN) Here

C := (Cm(l))m=0,...,M,l[Lm] and K := (Km(l))m=0,...,M,l[Lm]. By definition of k and L , we have

B(conv)

=

k-1

=

O(M

1 log M

)

=

O(1).

Also,

we

have

B(conv)

=

O(1).

This

proves

Corollary

4.

By

the

definition

of

k

and

bound

on

Cm(l)

and

Km(l),

we

have

Cm(l-1)Km(l)k-1



M

-

1 L

/2.

There-

fore, we have m 

lL=1(Cm(l-1)Km(l)k-1)  M -1 and hence

M m=0

(1

+

m)

=

O(1).

Since

Cm(l-1)Km(l)k-1  1/2 for sufficiently large M , we have m+ = 1 for sufficiently large M . Therefore, we can bound M1 by M1 = O(M 2 log M ). Similarly, we have M2 = O(M L ) = O(M 2 log M )

using the bounds of Cm(l), Km(l) and L. Therefore, we have log M1M2(B(conv)  B(fc)) = O~(1).

Since the assumption is satisfied, we can apply Corollary 2 with 1

=

 D

,

2

=

1 and obtain

Corollary 5.

G COMPARISON OF OUR CNNS AND ORIGINAL RESNET
There are several differences between the CNN in this paper and the original ResNet, aside from the number of layers. First and foremost, our CNN does not have pooling nor Batch Normalization (Ioffe & Szegedy (2015)) layers. It is left for future research whether our result can extend to the ResNet-type CNNs with pooling or Batch Normalization layers. Second, our CNN does not have ReLU activation after the junction points and the final layer of the 0-th block, while they have in the original ResNet. We choose this design to make proofs simpler. We can easily extend our results to the architecture that adds the ReLU activations to those points with slight modifications using similar techniques appeared in Lemma 2 of the appendix.

25

