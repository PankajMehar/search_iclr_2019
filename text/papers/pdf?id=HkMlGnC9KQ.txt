Under review as a conference paper at ICLR 2019
On Regularization and Robustness of Deep Neural Networks
Anonymous authors Paper under double-blind review
Abstract
Despite their success, deep neural networks suffer from several drawbacks: they lack robustness to small changes of input data known as "adversarial examples" and training them with small amounts of annotated data is challenging. In this work, we study the connection between regularization and robustness by viewing neural networks as elements of a reproducing kernel Hilbert space (RKHS) of functions and by regularizing them using the RKHS norm. Even though this norm cannot be computed, we consider various approximations based on upper and lower bounds. These approximations lead to new strategies for regularization, but also to existing ones such as spectral norm penalties or constraints, gradient penalties, or adversarial training. Besides, the kernel framework allows us to obtain margin-based bounds on adversarial generalization. We study the obtained algorithms for learning on small datasets, learning adversarially robust models, and discuss implications for learning implicit generative models.
1 Introduction
Learning predictive models for complex tasks often requires large amounts of annotated data. Interestingly, successful models such as convolutional neural networks are also hugedimensional and typically involve more parameters than training samples. Such a setting is challenging: besides the fact that training with small datasets is difficult, these models also lack robustness to small adversarial perturbations (Szegedy et al., 2013; Biggio & Roli, 2018). In the context of perceptual tasks such as vision (Szegedy et al., 2013) or speech recognition (Carlini & Wagner, 2018), these perturbed examples are often perceived identically to the original ones by a human, but can lead to arbitrarily different model predictions.
In this paper, we present a unified perspective on regularization and robustness, by viewing convolutional neural networks as elements of a particular reproducing kernel Hilbert space (RKHS) following the work of Bietti & Mairal (2018) on deep convolutional kernels. For such kernels, the RKHS contains indeed deep convolutional networks similar to generic ones--up to smooth approximations of rectified linear units. Such a point of view is interesting to adopt for deep networks since it provides a natural regularization function, the RKHS norm, which allows us to control the variations of the predictive model according to the geometry induced by the kernel. Besides, the norm also acts as a Lipschitz constant, which provides a direct control on the stability to adversarial perturbations.
In contrast to traditional kernel methods, the RKHS norm cannot be explicitly computed in our setup. Yet, these norms admit numerous approximations--lower bounds and upper bounds--which lead to new practical strategies for regularization, but also to existing ones. We may indeed recover several approaches such as using penalties or constraints on the spectral norm of the filters (Yoshida & Miyato, 2017), various forms of robust optimization (Madry et al., 2018), variants of gradient penalties (Drucker & Le Cun, 1991), and tangent propagation Simard et al. (1998).
Moreover, regularization and robustness are tightly linked in our kernel framework. In particular, we show a close connection between adversarial training under 2 perturbations and penalizing with the RKHS norm, in a similar vein to Xu et al. (2009b); from a statistical point of view, we also obtain margin-based bounds based on the RKHS norm in the spirit
1

Under review as a conference paper at ICLR 2019

of Bartlett et al. (2017); Boucheron et al. (2005) for robust generalization, which highlight the trade-offs between accuracy and robustness (see Schmidt et al., 2018). Following these observations, we evaluate empirically such a trade-off for different regularization strategies, by considering learning tasks with small datasets or with adversarial perturbations. We also provide a new perspective on recent successful approaches to training generative adversarial networks as optimizing a kernel two-sample test (see, e.g., Gretton et al., 2012).
Related work. The construction of hierarchical kernels and the study of neural networks in the corresponding RKHS was studied by Mairal (2016); Zhang et al. (2016; 2017); Bietti & Mairal (2018). Our study of the relationship between robustness and regularization follows from Xu et al. (2009a;b), where the main focus is on linear models with quadratic or hinge losses. Some of the regularization strategies we obtain from our kernel perspective are closely related to previous approaches to adversarial robustness (Cisse et al., 2017; Madry et al., 2018; Simon-Gabriel et al., 2018; Roth et al., 2018), to improving generalization (Drucker & Le Cun, 1991; Miyato et al., 2018b; Sedghi et al., 2018; Simard et al., 1998; Yoshida & Miyato, 2017), and stable training of generative adversarial networks (Roth et al., 2017; Gulrajani et al., 2017; Arbel et al., 2018; Miyato et al., 2018a). The notion of adversarial generalization was considered by Schmidt et al. (2018), who provide lower bounds on a specific example of data distribution. Sinha et al. (2018) provide generalization guarantees from a distributional robustness perspective; however, they consider expected loss instead of classification error, and their bounds do not highlight the dependence on the model complexity, unlike ours.

2 Regularization strategies for deep neural networks
In this section, we recall the kernel perspective on deep networks introduced by Bietti & Mairal (2018), and present upper and lower bounds on the RKHS norm of a given model, leading to various regularization strategies. For simplicity, we first consider real-valued networks and binary classification, before discussing multi-class extensions.

2.1 Relation between deep neural networks and RKHSs

Kernel methods consist of mapping data living in a set X to a RKHS H associated to a positive definite kernel K through a mapping function  : X  H, and then learning simple machine learning models in H. Specifically, when considering a real-valued regression or binary classification problem, classical kernel methods find a prediction function f : X  R living in the RKHS which can be written in linear form, i.e., such that f (x) = f, (x) H for all x in X . While explicit mapping to a possibly infinite-dimensional space is of course
only an abstract mathematical operation, learning f can be done implicitly by computing
kernel evaluations and typically using convex programming (Schölkopf & Smola, 2001).

Moreover, the RKHS norm f H acts as a natural regularization function, which controls the variations of model predictions according to the geometry induced by :

|f (x) - f (x )|  f H · (x) - (x ) H.

(1)

Unfortunately, traditional kernel methods become difficult to use when the datasets are large or when evaluating the kernel is intractable. Here, we propose a different approach that considers explicit parameterized representations of functions contained in the RKHS, given by generic convolutional neural networks, and leverage properties of the RKHS and the kernel mapping in order to regularize when learning the network parameters.

Consider indeed a real-valued deep convolutional network f : X  R, where X is simply Rd, with rectified linear unit (ReLU) activations and no bias units. By constructing an appropriate multi-layer hierarchical kernel, Bietti & Mairal (2018) show that the corresponding RKHS H contains a convolutional network with the same architecture and parameters as f , but with activations that are smooth approximations of ReLU. Although the model predictions might not be strictly equal, we will abuse notation and denote this approximation with smooth ReLU by f as well, with the hope that the regularization procedures derived from the RKHS model will be effective in practice on the original CNN f .

2

Under review as a conference paper at ICLR 2019

Besides, Bietti & Mairal (2018) show that the mapping (·) is non-expansive:

(x) - (x ) H  x - x 2,

(2)

so that controlling f H provides some robustness to additive 2-perturbations, by (1). Additionally, with appropriate pooling operations, Bietti & Mairal (2018) show that the kernel mapping is also stable to deformations, meaning that the RKHS norm also controls robustness to translations and other transformations including scaling and rotations (Engstrom et al., 2017), which can be seen as deformations when they are small.

In contrast to standard kernel methods, where the RKHS norm is typically available in
closed form, this norm is difficult to compute in our setup, and requires approximations.
The following sections present upper and lower bounds on f H, with linear convolutional operations denoted by Wk for k = 1, . . . , L, where L is the number of layers. Defining  := {Wk : k = 1, . . . , L}, we then leverage these bounds to approximately solve the following penalized or constrained optimization problems on a training set (xi, yi), i = 1, . . . , n:

1n min
n

(yi, f(xi)) + 

f

2 H

i=1

or

1n

min : f HC n i=1

(yi, f(xi)).

(3)

We also note that while the construction of Bietti & Mairal (2018) considers VGG-like networks (Simonyan & Zisserman, 2014), the regularization algorithms we obtain in practice can be easily adapted to different architectures such as residual networks (He et al., 2016).

2.2 Exploiting Lower bounds of the RKHS norm
In this section, we devise regularization algorithms by leveraging lower bounds on f H, which are obtained by relying on the following variational characterization of Hilbert norms:

f H = sup f, u H.
u H1

(4)

At first sight, this definition is not useful since the set U = {u  H : u H  1} may be
infinite-dimensional and the inner products f, u H cannot be computed in general. Thus, we devise tractable lower bound approximations by considering smaller sets U¯  U .

Adversarial perturbation penalty. The non-expansiveness of  allows us to consider the subset U¯  U defined as U¯ = {(x + ) - (x) : x  X ,  2  1}, leading to the bound

f H  sup f (x + ) - f (x),
xX ,  21
which is reminiscent of adversarial perturbations. Adding a regularization parameter in front of the norm then corresponds to different sizes of perturbations:

(5) >0

f H = sup f, u H  sup f (x + ) - f (x).

u H

xX ,  2

(6)

Using this lower bound or its square as a penalty in the objective (3) when training a neural network can then provide a way to regularize. In practice, optimizing over all x in X is
difficult, and we can replace the supremum with an expectation over the training set, or
with a supremum over a subset of the training set (such as a mini-batch), yielding further
lower bounds on the penalty. In the context of a mini-batch stochastic gradient algorithm, one can obtain a subgradient of this penalty by first finding maximizers x^, ^ (where x^ is one of the examples in the mini-batch), and then simply computing gradients of f(x^+^)-f(x^) (or its square) w.r.t.  using back-propagation. We compute the perturbations ^ for each
example x by using a few steps of projected subgradient ascent with constant step-lengths,
in a similar fashion to Madry et al. (2018) in the context of robust optimization.

Relationship with robust optimization. This penalized approach turns out to be closely related to solving the robust optimization problem

1n

min sup



n
i=1

 2

(yi, f(xi + )),

(7)

3

Under review as a conference paper at ICLR 2019

which is commonly considered for training adversarially robust classifiers (Kolter & Wong, 2017; Madry et al., 2018; Raghunathan et al., 2018). In particular, Xu et al. (2009b) show that the penalized objective and the robust objective are equivalent in the case of the hinge loss with linear predictors, under some additional conditions on the data and perturbations, such as non-separability. They also show the equivalence for kernel methods when considering the (intractable) full perturbation set U around each point in the RKHS (xi), that is, predictions f, (xi) + u H with u in U . Intuitively, when a training example (xi, yi) is misclassified, we are in the "linear" part of the hinge loss, so that

sup (yi, f, (xi) + u H) = (yi, f, (xi) H) + sup f, u H = (yi, f (xi)) + f H.

u H

u H

Considering additive perturbations in the input space corresponds to reducing the RKHS perturbations to elements of the form u = (xi + ) - (xi) with  2  , giving a lower bound on the above quantity. For other losses such as the the logistic loss, the regularization effect is still present even for correctly classified examples, though it may be smaller since the loss has a reduced slope for such points, leading to a more adaptive regularization mechanism which may automatically reduce the amount of regularization on easier datasets. Either way, because both the hinge and logistic loss are 1-Lipschitz, we have that the robust objective (7) is upper bounded by the penalized objective with penalty f H.

Gradient penalties.

Taking

U¯

=

{

(x)-(y) x-y 2

:

x, y



X },

which

is

a

subset

of

U

by

Eq. (2)--it turns out that this is the same set as above, since  is homogeneous (Bietti &

Mairal, 2018) and X = Rp--we obtain a lower bound based on the Lipschitz constant of f :

f

H

f

f (x) - f (y)

L := sup
x,yX

x-y 2 .

We can further bound f L  sup f (x) 2,
xX
where the supremum is taken over points where f is differentiable, and equality holds when X

is convex. Taking the square and replacing the supremum over x by an expectation over the

training data leads to a gradient penalty that has been recently used to stabilize the training

of generative adversarial networks (Gulrajani et al., 2017; Roth et al., 2017). Leveraging

the connection to robust optimization again, when is small, we may also consider penalties

based on the gradients of the loss, which have been used for regularization (Drucker &

Le Cun, 1991) and robustness (Lyu et al., 2015; Roth et al., 2018; Simon-Gabriel et al.,

2018). This has the advantage of overcoming the difficulties of the maximization problem

over  by leveraging the closed-form expression of the gradient norm.

Other connections with lower bounds. We may also consider a set U¯ = {(x~)-(x) : x  X , x~ is a small deformation of x}, where the amount of deformation is dictated by the stability bounds of Bietti & Mairal (2018) in order to ensure that U¯  U . Then, the obtained penalty (6) and corresponding robust optimization formulation (7) resemble a form of data augmentation where transformations are optimized instead of sampled, as in Engstrom et al. (2017). By considering infinitesimal deformations from specific parameterized classes such as translations or rotations, the obtained gradient penalty then resembles the tangent propagation regularization strategy of Simard et al. (1998).
One advantage of these approaches based on lower bounds is that the obtained penalties are independent of the model parameterization, making them flexible enough to use with more complex architectures in practice. In addition, the connection with robust optimization can provide a useful mechanism for adaptive regularization. However, these lower bounds do not guarantee a control on the norm f H, since they might only provide a poor approximation. In particular, a classifier trained with such approximate penalties may simply fit a stable function on -balls around all training points, while being unstable near the decision boundary, leading to a poor control of the RKHS norm. The next section presents approaches based on upper bounds, which provide guarantees on f H, overcoming some of the aforementioned drawbacks. Nevertheless, we found that controlling the lower bounds often does help controlling the upper bounds as well in many cases (see Section 4). Moreover, we found that combining upper- and lower-bound strategies may lead to further gains.

4

Under review as a conference paper at ICLR 2019

2.3 Exploiting upper bounds: optimization with spectral norms

In contrast to lower bounds, upper bounds can provide a guaranteed control on the RKHS norm. Bietti & Mairal (2018) show the following upper bound:

f H  U ( W1 , . . . , WL ),

(8)

where U is increasing in all of its arguments, and Wk is the spectral norm of the linear convolutional operator Wk. Here, we simply consider the spectral norm on the filters, given by W := sup x 21 W x 2. Other generalization bounds relying on similar quantities have been proposed for controlling complexity (Bartlett et al., 2017; Neyshabur et al., 2018),
suggesting that using them for regularization is relevant even beyond our kernel perspective.

Penalizing the spectral norms. One way to control the upper bound (8) when learning a neural network f is to consider a regularization penalty based on spectral norms

1n min
n

L
(yi, f(xi)) + 

Wl 2,

i=1 l=1

(9)

where  is a regularization parameter. In the context of a stochastic gradient algorithm, one can obtain (sub)gradients of the penalty by computing singular vectors associated to the highest singular value of each Wl. We consider the method of Yoshida & Miyato (2017), which computes such singular vectors approximately using one or two iterations of the power method, as well as a more costly approach using the full SVD.

Constraining the spectral norms. In the constrained setting, we want to optimize:

1n

min n

(yi, f(xi)) s.t Wl   ; l  1, . . . , L ,

i=1

where  is a user-defined constraint. This objective may be optimized by projecting each Wl in the spectral norm ball of radius  after each gradient step. Such a projection is achieved by truncating the singular values to be smaller than  (see Appendix A). We found that the loss was hardly optimized with this approach, but that this issue can be avoided by using an exponentially decaying schedule for  reaching a constant 0 after a few epochs.

2.4 Extension to multiple classes

We now discuss how to extend the regularization strategies to multi-valued networks, in order to deal with multiple classes. First, the upper bound strategies of Section 2.3 are easy to extend, by simply considering spectral norms up to the last layer. This is also justified by the generalization bound of Bartlett et al. (2017), which applies to the multi-class setup.

In the context of lower bounds, we can consider a multi-class penalty

f1

2 H

+

.

.

.

+

fK

2 H

for an RK -valued function f = (f1, f2, . . . , fK ). One can then consider lower bounds on this

penalty by leveraging lower bounds on each individual fk. In particular, we use:

K

f

2 M

:=

sup (fk(x + ) - fk(x))2.

k=1 xX ,  2

Computing this quantity requires finding one perturbation for each class k and each x,

which can be costly when K is large. By constraining the perturbations to be the same

for different classes, we can obtain another lower bound that is computationally cheaper,

yielding a penalty

f

2 S

:=

supxX ,  2

f (x + ) - f (x) 22.

For robust optimization

formulations (7), the extension is straightforward, given that standard multi-class losses

such as the cross-entropy loss can be directly optimized in an adversarial training or gradient

penalty setup.

5

Under review as a conference paper at ICLR 2019

3 Theoretical guarantees and extensions
While various methods have been introduced to empirically gain robustness to perturbations, their robustness on test data, also known as adversarial generalization (Schmidt et al., 2018) is poorly understood. In this section, we study how margin bounds for kernel methods can provide theoretical guarantees. We then consider heuristic extensions to different geometries, by analogy with linear models. Finally, we discuss how our kernel approach provides novel interpretations for training generative adversarial networks.

3.1 Guarantees on adversarial generalization

Margin-based bounds have been useful to explain the generalization behavior of learning algorithms that can fit the training data well, such as kernel methods, boosting and neural networks (Koltchinskii et al., 2002; Boucheron et al., 2005; Bartlett et al., 2017). Here, we show how similar bounds can be used to obtain guarantees on adversarial generalization. For a binary classification task with labels in Y = {-1, 1} and data distribution D, we would like to bound the expected adversarial error of a classifier f , given for some > 0 by

P(x,y)D(  2  : yf (x + ) < 0).

(10)

Note that if f is f L-Lipschitz, we then have

P (  2  : yf (x + ) < 0)  P (yf (x) < f L).

(11)

We now show how to further bound this quantity using empirical margins, following the usual approach to obtaining margin bounds for kernel methods (e.g., Boucheron et al., 2005). Consider a training dataset (x1, y1), . . . , (xn, yn)  X × Y. Define

L(f ) := P (yf (x) < )

and

Ln (f )

:=

1 n

n

1{yif (xi) < }.

i=1

We then have the following bound, proved in Appendix B:
Proposition 1. With probability 1 -  over a dataset {(xi, yi)}i=1,...,n, we have, for all choices of   0,  > 0, and f  H,



L (f

)



Ln+

(f

)

+

O~



fH n

1n n K(xi, xi) +
i=1

 log 1/
n .

(12)

Taking  = 0 leads to the usual margin bound, while  = f L yields a bound on adversarial error by (11), for some neural network f learned from training data. We note that other complexity measures based on products of spectral norms may be used instead of f H, as well as multi-class extensions, following Bartlett et al. (2017); Neyshabur et al. (2018).
One can then compare different regularization algorithms by inspecting cumulative distributions of the normalized margins ¯i = yif (xi)/ f H. Given that f L  f H, one can then simply look at the part of the plot to the right of ¯ = in order to assess expected adversarial error with -bounded perturbations. When n becomes smaller, the best bound is obtained for a larger value of ¯, which also suggests that the right side of the plots is indicative of performance on small datasets. An example is given in Section 4.2.

3.2 Beyond Euclidian geometries
The kernel approach from previous sections is well-suited for input spaces X equipped with the Euclidian distance, thanks to the non-expansiveness property (2) of the kernel mapping. In the case of linear models, this kernel approach corresponds to using 2-regularization by taking a linear kernel. However, other forms of regularization and geometries can often be useful, for example to encourage sparsity with an 1 regularizer. Such a regularization approach presents tight links with robustness to  perturbations on input data, thanks to the duality relation w 1 = sup u  w, u (see Xu et al., 2009a).

6

Under review as a conference paper at ICLR 2019

In the context of deep networks, we can leverage such insights to obtain new regularizers, expressed in the same variational form as the lower bounds in Section 2.2, but with different geometries on X . For  perturbations, we obtain

f (x) - f (y) sup x,yX x - y 



sup f (x) 1.
xX

(13)

The Lipschitz regularizer (l.h.s.) can also be taken in an adversarial perturbation form, with different strenghts:
sup f (x + ) - f (x).
xX ,  

As most visible in the gradient 1-norm in (13), these penalties encourage some sparsity in the gradients of f , which is a reasonable prior for regularization on images, for instance, where we might only want predictions to change based on few salient pixel regions. This can lead to gains in interpretability, as observed by Tsipras et al. (2018). We push this principle one step further in our experiments, by considering other structured penalties on the gradients such as the total variation.
We note that in the case of linear models, our robust margin bound of Section 3.1 can be adapted to -perturbations, by leveraging Rademacher complexity bounds for 1constrained models (Kakade et al., 2009). Obtaining similar bounds for neural networks would be interesting but goes beyond the scope of this paper.

3.3 Regularization of generative adversarial networks
Generative adversarial networks attempt to learn a generator neural network G : Z  X , so that the distribution of G(z) with z  Dz a noise vector resembles a data distribution Dx. Various recent approaches have relied on regularization strategies on a discriminator network in order to improve the stability of GAN training and the quality of the produced samples. Some of these resemble our approaches from Section 2 such as gradient penalties (Gulrajani et al., 2017; Roth et al., 2017) and spectral norm regularization (Miyato et al., 2018a).
While these regularization methods had different motivations originally, we suggest that viewing them as approximations of an RKHS norm constraint provides some insight into their effectiveness in practice; specifically, we provide an interpretation as optimizing a kernel two-sample test such as MMD with the convolutional kernel introduced in Section 2:

min


f

sup
H: f H1

ExDx

[f

(x)]

-

EzDz

[f

(G(z))].

(14)

In contrast to the Wasserstein GAN interpretation of a similar objective where all 1-Lipschitz functions are considered (Arjovsky et al., 2017), the MMD interpretation yields a parametric statistical rate O(n-1/2) when learning from an empirical distribution with n samples, which is significantly better than the O(n-1/d) rate of the Wasserstein-1 distance for highdimensional data such as images (Sriperumbudur et al., 2012). While the MMD approach has been used for training generative models, it generally relies on a generic kernel function, such as a Gaussian kernel, that appears explicitly in the objective (Dziugaite et al., 2015; Li et al., 2017; Bikowski et al., 2018). Although using a learned feature extractor can improve this, the Gaussian kernel might be a poor choice when dealing with natural signals such as images, while the hierarchical kernel of Bietti & Mairal (2018) is better suited for this type of data, by providing useful invariance and stability properties. Leveraging the variational form of the MMD (14) with this kernel suggests using convolutional networks as the discriminator f , with constraints on the spectral norms in order to ensure f H  C for some C, as done by Miyato et al. (2018a) through normalization.

4 Experiments
We tested the regularization strategies presented in Section 2 in the context of improving generalization on small datasets and training adversarially robust models. Our goal is to use

7

Under review as a conference paper at ICLR 2019

Table 1: Regularization on CIFAR10 with 5 000 or 1 000 examples for VGG-11 and ResNet18. Each entry shows the test accuracy with/without data augmentation when all hyperparameters are optimized on a validation set. TV denotes the total variation norm.

Method

No weight decay Weight decay SN penalty (PI)

SN penalty (SVD)

SN projection

f

2 M

penalty

PGD- 2

PGD- 

grad- 1

grad- 2

grad-TV

PGD- 2 + SN projection

grad- 2 + SN projection

5k VGG-11
71.13/57.88 73.13/57.71 74.23/62.26 74.7/60.87 74.58/63.36 72.95/60.16 72.7/59.37 73.29/58.04 75.13/58.92 75.13/58.63 73.67/57.92 75.02/61.42 74.67/63.12

5k ResNet-18
68.14/53.01 71.59/54.71 73.25/53.19 73.49/55.46 75.87/55.31 70.61/55.47 72.72/54.99 72.81/54.13 71.11/54.46 73.83/55.39 71.82/53.13 76.06/56.99 76.17/55.53

1k VGG-11
50.68/42.82 51.16/44.11 53.38/45.29 52.32/44.52 53.52/46.0 51.71/45.0 51.52/43.74 51.92/43.74 53.86/43.65 54.79/42.73 51.4/44.18 53.74/45.93 55.98/46.68

1k ResNet-18
42.19/38.18 43.93/37.84 46.93/38.67 47.03/40.04 45.11/38.56 45.23/44.44 46.83/39.3 46.27/40.97 48.49/41.01 47.56/42.06 43.47/40.75 46.96/40.19 48.94/43.26

common convolutional architectures used for large datasets and improve their performance in different settings through regularization.

For the adversarial training strategies, the inner maximization problems are solved with

5 steps of projected gradient ascent (Madry et al., 2018) with a randomly chosen starting

point.

In the case of the multi-class lower bound adversarial penalties

f

2 M

and

f

2 S

,

which respectively optimize over one perturbation per class and a single perturbation overall

(see Section 2.4), we also maximize over examples in the mini-batch, only considering the

maximal element when computing gradients w.r.t. parameters. For the robust optimization

problem (7), we consider the PGD approach for 2 and  perturbations (Madry et al.,

2018), as well as the corresponding 2 (squared) and 1 gradient norm penalties. For the

upper bound approaches with spectral norms (SNs), we consider the SN projection strategy

with decaying  , as well as the SN penalty (9), either using power iteration (PI) or a full

SVD for computing gradients.

4.1 Improving generalization on small datasets

In this setting, we use 1 000 and 5 000 examples of the CIFAR10 dataset, with or without data augmentation. We consider a VGG network (Simonyan & Zisserman, 2014) with 11 layers, as well as a residual network (He et al., 2016) with 18 layers, which achieve 91% and 93% test accuracy respectively when trained on the full training set with data augmentation. We do not use any batch normalization layers in order to prevent any interaction with spectral norms. Each regularization strategy derived in Section 2 is trained for 500 epochs using SGD with momentum and batch size 128, halving the step-size every 40 epochs from a fixed initial step-size (0.05 for VGG-11, 0.1 for ResNet-18), a strategy we found to work relatively well for all methods. In order to study the potential effectiveness of each method, we assume that a fairly large validation set is available to select hyper-parameters; thus, we keep 10 000 annotated examples for this purpose.

Table 1 shows the test accuracies we obtain for upper and lower bound approaches, as

well as combined approaches and different geometries. Among upper bound strategies, the

constrained approach often works best, perhaps thanks to a more explicit control of the

SNs compared to the penalized approach. The SN penalty can work well nevertheless, and

provides computational benefits when using the PI variant. For lower bound approaches,

we found the gradient 1 and 2 norm penalties to often work better than the adversar-

ial penalty

f

2 M

and adversarial PGD approaches.

We note that the best regularization

parameters are often quite small on such datasets, making the gradient penalties good ap-

proximations of the robust objective (7) used by PGD. Because gradient penalties have

closed form gradients while PGD only obtains them by solving (7) approximately, they may

8

Under review as a conference paper at ICLR 2019

adversarial accuracy

2, test = 0.03

0.900 0.875 0.850 0.825 0.800

PGDPGD-

2

|f|2S

|f|M2 grad- 2 grad- 1 SN proj

clean

0.775

0.7500.800 0.90

0.825,

0.850 test

=0.807.5001.900

0.925

2, test = 0.1
0.86 0.84 0.82 0.80 0.78 0.76
0.800 0.825 ,0.8t5e0st =0.8075.002.900 0.925

0.75 0.70 0.65 0.60 0.55 0.50 0.45

0.86 0.75

0.88 0.85 0.70

0.86 0.84 0.65

0.5

2, test = 0.3 0,.6 test0=.70.005.8

0.4 0.3 0.2 0.1 0.9 0.5
0.6 0.5

2, test = 1.0 0.6, tes0t.=7 0.10.8

0.9

0.84 0.82
0.80 0.86 sta0n.8d8ard a0c.9c0uracy0.92

0.83 0.82 0.81
0.80 0.86 sta0n.8d8ard a0c.9c0uracy0.92

0.60 0.55 0.50 0.45
0.5 st0a.n6dard0.a7ccur0a.c8y 0.9

0.4
0.3
0.2
0.5 st0a.n6dard0.a7ccur0a.c8y 0.9

adversarial accuracy

Figure 1: Robustness trade-off curves of different regularization methods for VGG11 on
CIFAR10. Each plot shows test accuracy vs adversarial test accuracy for 2 (top) and  (bottom) bounded adversaries with a fixed test. Different points on a curve correspond to training with different regularization strengths.

work better in this setting thanks to optimization benefits. The adaptive nature of the reg-

ularization through robust optimization may also be beneficial on these datasets, although

the explicit penalization by

f

2 M

seems to be more helpful in the case of 1 000 examples

with no data augmentation, which is plausibly the hardest setting in our experiments. Fi-

nally, we can see that the combined approaches of robust optimization together with SN

constraints often yield the best results. Indeed, lower bound approaches do not necessarily

control the upper bounds (and this is particularly true for PGD, as discussed in Section 4.2),

which might explain why the additional constraints on SNs are helpful.

4.2 Training adversarially robust models

We consider the same VGG architecture as in Section 4.1, trained on CIFAR10 with data augmentation, with different regularization strategies. Each method is trained for 300 epochs using SGD with momentum and batch size 128, dividing the step-size in half every 30 epochs. This strategy was successful in reaching convergence for all methods.

Figure 1 shows the test performance of the different regularization approaches in the presence

of 2 and -bounded adversaries, plotted against standard accuracy (the leftmost points

correspond to stronger regularization). We can see that the robust optimization approaches

tend to work better in high-accuracy regimes, perhaps due to a more adaptive form of

regularization, while the

f

2 M

penalty can be useful in some regimes where robustness to

large perturbations is needed. Comparing the different geometries for PGD and gradient

penalties, we find that using the right metric can help, but not dramatically, which may

suggest that more appropriate optimization algorithms than SGD could be needed to better

accommodate the non-smooth case of 1/ , or perhaps that both algorithms are actually

controlling the same notion of complexity on this dataset. The plots also confirm that

gradient penalties are preferable for small regularization strengths, possibly due to better

optimization, while for stronger regularization, the gradient approximation no longer holds

and the adversarial training approaches such as PGD are preferred.

These results suggest that a robust optimization approach such as PGD can work very well on the CIFAR10 dataset, perhaps because the training data is easily separable with a large margin and the method adapts to this "easiness". However, this raises the question of whether the approach is actually controlling f H, given that it only attempts to minimize a lower bound, as discussed in Section 2.2. Figure 2(left) casts some doubt on this, showing that for PGD in contrast to other methods, the product of spectral norms (representative of an upper bound on f H) increases when the lower bound f S decreases. This suggests that a network learned with PGD with large may have large RKHS norm, possibly because the approach tries to separate -balls around the training examples, which can be more

9

Under review as a conference paper at ICLR 2019

SN product CDF

10000 8000 6000 4000 2000
0 0

norm comparison
PGD- 2 PGD|f|S2 g|fr|M2ad- 2 grad- 1 clean

2 |4f|S 6

8

1.0 0.8 0.6 0.4 0.2 0.0
0.0

|f|M2 penalty
=0 = 0.03 = 0.1 = 0.3 = 1.0
2.5 (=5.0mar7g.i5n / |1f0|S.0) 12.5 15.0

Figure 2: (left) Comparison of lower and upper bound quantities ( f S vs the product of

spectral norms). (right) CDF plot of normalized empirical margins for the

f

2 M

penalty

with different regularization strengths, normalized by f S. We consider 1000 fixed training

examples when computing f S.

difficult than simply separating the training examples (see also Madry et al., 2018). For

other methods, and in particular the adversarial penalties such as

f

2 M

,

the

upper

and

lower bounds appear more correlated, and thus we may look at the empirical distributions of

normalized margins ¯ obtained using f S for normalization, shown in Figure 2(right). The

curves suggest that for small ¯, smaller values of are preferred, while stronger regularization

helps when ¯ increases, yielding lower test error when an adversary is present according to

our bounds in Section 3.1. This qualitative behavior is indeed observed in the results of

Figure 1 on test data for the

f

2 M

penalty approach.

5 Discussion
Making generic machine learning techniques more data efficient is crucial to reduce the costs related to annotation. While other approaches may also be important to solve this grand challenge, such as incorporating more prior knowledge in the architecture (e.g., Oyallon et al., 2017), semi-supervised learning (Chapelle et al., 2006) or meta-learning (when multiple tasks or datasets are available, see, e.g., Thrun, 1998), basic regularization principles will be needed and those are crucially missing today. Such principles are also essential for obtaining robust models in applications where security is a concern, such as self driving cars. Our paper presents various algorithmic strategies for regularization on generic deep convolutional networks, by leveraging the structure of an appropriate RKHS, leading to many existing approaches to regularization, as well as new ones, in addition to providing theoretical guarantees and insights on different methods.

References
Michael Arbel, Dougal J Sutherland, Mikolaj Bikowski, and Arthur Gretton. On gradient regularizers for MMD GANs. In Advances in Neural Information Processing Systems (NIPS), 2018.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. In Proceedings of the International Conference on Machine Learning (ICML), 2017.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems (NIPS), 2017.
Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of deep convolutional representations. arXiv preprint arXiv:1706.03078, 2018.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition, 84:317­331, 2018.
Mikolaj Bikowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.

10

Under review as a conference paper at ICLR 2019
Stéphane Boucheron, Olivier Bousquet, and Gábor Lugosi. Theory of classification: A survey of some recent advances. ESAIM: probability and statistics, 9:323­375, 2005.
Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. arXiv preprint arXiv:1801.01944, 2018.
Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien (eds.). Semi-Supervised Learning. MIT Press, Cambridge, MA, 2006.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In Proceedings of the International Conference on Machine Learning (ICML), 2017.
Harris Drucker and Yann Le Cun. Double backpropagation increasing generalization performance. In International Joint Conference on Neural Networks (IJCNN), 1991.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In Conference on Uncertainty in Artificial Intelligence (UAI), 2015.
Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation suffice: Fooling cnns with simple transformations. arXiv preprint arXiv:1712.02779, 2017.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research (JMLR), 13 (Mar):723­773, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems (NIPS), 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing Systems (NIPS), 2009.
Vladimir Koltchinskii, Dmitry Panchenko, et al. Empirical margin distributions and bounding the generalization error of combined classifiers. The Annals of Statistics, 30(1):1­50, 2002.
J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. In Proceedings of the International Conference on Machine Learning (ICML), 2017.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. Mmd gan: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems (NIPS), 2017.
Chunchuan Lyu, Kaizhu Huang, and Hai-Ning Liang. A unified gradient regularization family for adversarial examples. In IEEE International Conference on Data Mining (ICDM), 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.
Julien Mairal. End-to-end kernel learning with supervised convolutional kernel networks. In Advances in Neural Information Processing Systems (NIPS), 2016.
11

Under review as a conference paper at ICLR 2019
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018a.
Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2018b.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A PACBayesian approach to spectrally-normalized margin bounds for neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.
Edouard Oyallon, Eugene Belilovsky, and Sergey Zagoruyko. Scaling the scattering transform: Deep hybrid networks. In International Conference on Computer Vision (ICCV), 2017.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. In Advances in Neural Information Processing Systems (NIPS), 2017.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Adversarially robust training through structured gradient regularization. arXiv preprint arXiv:1805.08736, 2018.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Mdry. Adversarially robust generalization requires more data. In Advances in Neural Information Processing Systems (NIPS), 2018.
Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. 2001.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. arXiv preprint arXiv:1805.10408, 2018.
Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance in pattern recognition­tangent distance and tangent propagation. In Neural networks: tricks of the trade, pp. 239­274. Springer, 1998.
Carl-Johann Simon-Gabriel, Yann Ollivier, Bernhard Schölkopf, Léon Bottou, and David Lopez-Paz. Adversarial vulnerability of neural networks increases with input dimension. arXiv preprint arXiv:1802.01421, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Proceedings of the International Conference on Learning Representations (ICLR), 2014.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, Gert RG Lanckriet, et al. On the empirical estimation of integral probability metrics. Electronic Journal of Statistics, 6:1550­1599, 2012.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181­209. Springer, 1998.
12

Under review as a conference paper at ICLR 2019

Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. There is no free lunch in adversarial robustness (but there are unexpected benefits). arXiv preprint arXiv:1805.12152, 2018.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. In Advances in Neural Information Processing Systems (NIPS), 2009a.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. Journal of Machine Learning Research (JMLR), 10(Jul):1485­ 1510, 2009b.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. 1-regularized neural networks are improperly learnable in polynomial time. In Proceedings of the International Conference on Machine Learning (ICML), 2016.
Yuchen Zhang, Percy Liang, and Martin J. Wainwright. Convexified convolutional neural networks. In Proceedings of the International Conference on Machine Learning (ICML), 2017.

A Details on optimization with spectral norms

This section details our optimization approach presented in Section 2.3 for learning with spectral norm constraints. In particular, we rely on a continuation approach, decreasing the size of the ball constraints during training, towards a final value  . The method is presented in Algorithm 1. We use an exponentially decreasing schedule for  , and found using 2 epochs for  to work well in practice. In the context of convolutional networks, we simply consider the SVD of a reshaped filter matrix, but we note that alternative approaches based on the singular values of the full convolutional operation may also be used (Sedghi et al., 2018).

Algorithm 1 Stochastic projected gradient with continuation

Input:  , , step-sizes t

for t = 1, . . . do

Sample mini-batch

t =  (1 + exp

-t 

and )

compute

gradients

of

the

loss

w.r.t.

each

W l,

denoted

Gtl

for l = 1, . . . , L do

W~ tl := Wtl - tGlt

Compute SVD: W~ tl = U diag()V T

 := proj . t ()

Wtl+1 := U diag()V T

end for

end for

B Details on generalization guarantees
This section presents the proof of Proposition 1, which relies on standard tools from statistical learning theory (e.g., Boucheron et al., 2005).

B.1 Proof of Proposition 1

Proof. Assume for now that  and  are fixed in advance, and let F := {f  H : f H  }. Consider the function

0,  (x) = 1,

if x  - -  if x  -

1 + (x + )/, otherwise.

13

Under review as a conference paper at ICLR 2019

Defining

A(f )

=

E (-yf (x))



L(f )

and

An(f )

=

1 n

n i=1

(-yif (xi))



Ln+ (f ),

and

noting that  is upper bounded by 1 and 1/ Lipschitz, we can apply similar arguments

to (Boucheron et al., 2005, Theorem 4.1) to obtain, with probability 1 - ,

L(f )  Ln+ (f ) + O

1 

Rn(F

)

+

log 1/ ,
n

where Rn(F) denotes the empirical Rademacher complexity of F on the dataset {(xi, yi)}i=1,...,n. Standard upper bounds on empirical Rademacher complexity of kernel
classes with bounded RKHS norm yield the following bound



L (f

)



Ln+ (f

)

+

O



 n

1n n K(xi, xi) +
i=1

 log 1/
n .

The final bound follows from this one by relying on a careful union bound on various choices
of ,  and , as in (Bartlett et al., 2017, Lemma A.9), which leads to additional logarithmic factors (hidden in the O~ notation).

14

