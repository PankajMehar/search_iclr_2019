Under review as a conference paper at ICLR 2019

LAPLACIAN SMOOTHING GRADIENT DESCENT
Anonymous authors Paper under double-blind review

ABSTRACT
We propose a class of very simple modifications of gradient descent and stochastic gradient descent. We show that when applied to a large variety of machine learning problems, ranging from softmax regression to deep neural nets, the proposed surrogates can dramatically reduce the variance and improve the generalization accuracy. The methods only involve multiplying the usual (stochastic) gradient by the inverse of a positive definitive matrix coming from the discrete Laplacian or its high order generalizations. The theory of Hamilton-Jacobi partial differential equations demonstrates that the new algorithm is almost the same as doing gradient descent on a new function which (i) has the same global minima as the original function and (ii) is "more convex". We show that optimization algorithms with these surrogates converge uniformly in the discrete Sobolev Hp sense and reduce the optimality gap for convex optimization problems. We implement our algorithm into both PyTorch and Tensorflow platforms which only involves changing of a few lines of code. The code will be available on Github.

1 INTRODUCTION
Stochastic gradient descent (SGD) has been the workhorse for solving large scale machine learning problems (Bottou et al., 2018). It gives rise to a family of algorithms that make the training of DNNs practical, which is believed to somehow implicitly smooth the loss function of the DNNs (Jastrzebski et al., 2018). Many efforts have been carried out to improve the training and generalization of DNNs by directly searching for flat minima (Keskar et al., 2017; Chaudhari et al., 2017; 2016). An alternative view of SGD's magic comes from the theory of uniform stability (Bousquet & Elisseeff, 2002; Duchi et al., 2011; Hardt et al., 2016; Bottou et al., 2016; Gonen & Shalev-Shwartz, 2017).
The noise in SGD, on the one hand, helps gradient based optimization algorithms circumvent spurious local minima and reach those that generalize well (Schmidhuber, 2014). On the other hand, it slows down the convergence of regular gradient descent (GD). To recover the linear convergence rate for strongly convex functions, several interesting variance reduction algorithms have been proposed, e.g., SAGA (Defazio & Bach, 2014) and SVRG (Johoson & Zhang, 2013). But none of them is suitable to train deep neural nets (DNNs). Improving the performance of SGD remains of interest, especially in nonconvex optimization.
In this work, we propose a carefully designed positive definite matrix to smooth the (stochastic) gradient on-the-fly. The resulting surrogate tends to reduce noise in SGD and improve the training of DNNs. We shall refer to this procedure as Laplacian smoothing. The gradient smoothing can be done by solving a tri-diagonal linear system with the original gradient on the right hand side. More precisely, we simply pre-multiply the gradient by the inverse of the following tri-diagonal circular convolution matrix

1 + 2 -

0 . . . 0 - 

 -

A

:=

 

0

 ...

1 + 2 - . . . 0 - 1 + 2 . . . 0 ... ... ... ...

0
0 
... 

- 0

0 . . . - 1 + 2

(1)

for some positive constant   0. In fact, we can write A = I - L, where I is the identity matrix, and L is the discrete one-dimensional Laplacian which acts on indices. Define the so-called

1

Under review as a conference paper at ICLR 2019

(periodic) forward finite difference matrix as

-1 1 0 . . . 0 0 

0 -1 1 . . . 0 0



D+

=

 

0

0 -1 . . . 0

0 . 

. . . . . . . . . . . . . . . . . .

1 0 0 . . . 0 -1

We can write A = I - D-D+, where D- = -D+ is the backward finite difference. The resulting Laplacian smoothing stochastic gradient descent (LS-SGD) requires negligible extra com-
putational cost and generalizes better than the standard SGD. When the Hessian has a poor condition
number, gradient descent performs poorly. In this case, the derivative increases rapidly in one direc-
tion, while increasing slowly in others. Gradient smoothing can avoid jitter along steep directions
and help make progress in shallow directions (Li & et al, 2018). Moreover, we show that the operator A-1 acts like a denoiser which enables better convergence in the presence of a very noisy stochastic gradient. Our proposed approach is linked to an unusual HJ-PDE whose solution makes the original
loss function more convex while retaining its flat (and global) minima, and we essentially work on
this surrogate function with a much better landscape.

2 HAMILTON-JACOBI PDES AND CONVEXIFICATION

Machine learning problems are generally formulated as finding the optimal parameters w of a para-

metric function optimal w can

y be

= h(x, w), such that for obtained by minimizing

an an

input x, the output y is empirical risk function,

afs(Xac,cYur,awte)a=s. pfos(swib)l,eg. iTvhene

the training data {X, Y }. We consider the problem of finding a global or a relatively flat minima of

f (w). To this end, we introduce the following unusual HJ-PDE with f (w) as initial condition

ut

+

1 2

wu, A- 1wu

= 0,

(w, t)   × [0, )

u(w, 0) = f (w),

w

(2)

By the Hopf-Lax formula (Evans, 2010), the unique viscosity solution to the above problem can be

represented by

1

u(w, t) = inf
v

f (v) + 2t

v - w, A(v - w)

.

This viscosity solution u(w, t) makes f (w) more convex by bringing down the local maxima while retaining the wide minima. An illustration of this is shown in Fig. 1. If we perform the smoothing GD with proper step size on the function u(w, t), it is easier to reach the global or at least a flat minima of the original nonconvex function f (w).

Figure 1: f (w) =

w

2

1+

1 2

sin(2

w

)

is made more convex by solving Eq.(2). The plot

shows the cross section of the 5D problem with  = 1 and different t values.

Proposition 1. Suppose f (w) is differentiable, the Laplacian smoothing GD update on u(w, t)

wk+1 = wk - tA-1wu(wk, t)

permits the smoothing implicit gradient descent on f (w)

wk+1 = wk - tA-1f (wk+1).

(3)

All the proofs here and below are provided in the appendix.

2

Under review as a conference paper at ICLR 2019

2.1 LAPLACIAN SMOOTHING GRADIENT DESCENT

Laplacian smoothing implicit gradient descent requires inner iterations as used in (Chaudhari et al., 2017), which is computationally expensive. We relax the implicit scheme to the explicit

wk+1 = wk - kA- 1f (wk).

Intuitively, compared to the standard GD, this scheme smooths the gradient on-the-fly by an elliptic

smoothing operator. Here we adopt an efficient fast Fourier transform (FFT) implementation, which

is available in both PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2016). Given a vector g, a smoothed vector d can be obtained by computing d = A- 1g. This is equivalent to g = d - v  d, where v = [-2, 1, 0, · · · , 0, 1] and  is the convolutional operator. Hence, we

have

fft(g)

d = ifft

,

1 -  · fft(v)

where we use component-wise division, fft and ifft are the FFT and inverse FFT, respectively. Therefore, the smoothed gradient can be obtained in quasilinear time. This additional time complexity is almost the same as performing a one step update on the weights vector w. For many machine learning models, we may need to concatenate the parameters into a vector. This reshape might lead to some ambiguity, nevertheless, based on our tests, both row and column majored reshaping work for the LS-GD algorithm. Moreover, in deep learning cases, the weights in different layers might have different physical meanings. We then perform layer-wise gradient smoothing, instead.

Remark 1. In image processing, the Sobolev gradient (Jung et al., 2009) involves a multidimensional Laplacian operator which operates on w, is different from the one-dimensional discrete Laplacian operator employed in our LS-GD scheme, which operates on indices.

To verify the improved convexity of the Laplacian smoothing gradient descent (LSGD), we show that LS-GD helps bypass sharp minima and reach the global minima for certain examples. We consider the following function, in which we `drill' narrow holes on a smooth convex function,

f (x, y, z) = -4e-((x-)2+(y-)2+(z-)2) - 4

cos(x)

cos(y)e-((x-r

sin(

i 2

)-)2

+(y-r

cos(

i 2

)-)2

)

,

(4)

i

where the summation is taken over the index set {i  N| 0  i < 4}, r and  are the parameters that

determine the location and narrowness of the local minima and are set to 1 and 1 , respectively. We do 500

GD and LS-GD starting from a random point in the neighborhoods of the narrow minima, i.e., (x0, y0, z0) 

{

i

U

(r

sin(

i 2

)

+

,

r

cos(

i 2

)

+

)|

0



i

<

4, i



N},

where

U(P )

is

a

neighborhood

of

the

point

P

with radius . Our experiments (Fig. 2) show that, if   0.2, GD will converge to narrow local minima, while

LS-GD convergences to wider global minima.

(a) (b) Figure 2: Demo of GD and LS-GD. Panel (a) depicts the slice of the function (Eq.(4)) with z = 2.34; panel (b) shows the paths of GD (red) and LS-GD (black). We take the step size to be 0.02 for both GD and LS-GD.  = 1.0 is utilized for LS-GD.
GENERALIZED SMOOTHING GRADIENT DESCENT
We can generalize A to the nth order discrete hyper-diffusion operator as follows I + (-1)nLn =. An.
Each row of the discrete Laplacian operator L consists of an appropriate arrangement of weights in central finite difference approximation to the 2nd order derivative. Similarly, each row of Ln is an arrangement of the weights of the central finite difference to approximate the 2nth order derivative. It is easy to prove that the smallest eigenvalue of An is 1 for all  and n, and the leading eigenvalue is no more than 1 + 4n.
3

Under review as a conference paper at ICLR 2019

Remark 2. The nth order smoothing operator I + (-1)nLn can only be applied to the problem with dimension at least 2n + 1. Otherwise, we need to add dummy variables to the object function.

Asmgoaiont,hewdesuarprpolgyatteh,e(AFFnT)-to1gco=.mpdu,tecathnebesmoobotathineedd

gradient vector. by solving g =

For a given gradient d + (-1)nvn  d,

vector where

g, vn

the =

(cnn+1, cnn+2, · · · , cn2n+1, 0, · · · , 0, c1n, c2n, · · · , cnn-1, cnn) is a vector of the same dimension as the gradient to

be smoothed. And the coefficient vector cn = (cn1 , cn2 , · · · , c2nn+1) can be obtained recursively by the follow-

ing formula



1

c1 = (1, -2, 1),

cni

=

 -2cn1 -1

+

c2n-1

i = 1, 2n + 1 i = 2, 2n

cin--11 - 2cni -1 + cni+-11 otherwise.

Remark 3. The computational complexities for different order smoothing schemes are the same when the FFT is utilized for computing the surrogate gradient.

3 REDUCE OPTIMALITY GAP IN SGD

We show advantages of the LS-(S)GD and generalized schemes for convex optimization. Consider finding the minima x of the quadratic function f (x) defined in Eq.(5) by different schemes.

f (x1, x2, · · · , x100) =

50

x22i-1 +

50

x22i . 102

i=1

i=1

(5)

To simulate SGD, we add Gaussian noise to the gradient vector, i.e., at a given point x, we have ~ f (x) := f (x) + N (0, I),

where the scalar controls the noise level, N (0, I) is the vector with zero mean and unit variance in each coordinate. The corresponding numerical schemes can be formulated as

xk+1 = xk - k(An )-1~ f (xk),

(6)

where  is the smoothing parameter selected to be 10.0. We take diminishing step sizes with initial values 0.1 for SGD/smoothed SGD; 0.9 and 1.8 for GD/smoothed GD, respectively. Without noise, the smoothing allows us to take larger step sizes, rounding to the first digit, 0.9 and 1.9 are the largest suitable step size for GD
and smoothed version here. We compare constant learning rate and exponentially decaying learning rate, i.e.,
after every 1000 iteration, the learning rate divided is by 10. We apply different schemes that corresponding to n = 0, 1, 2 in Eq.(6) to the problem Eq.(5), with the initial point x0 = (1, 1, · · · , 1).

Figure. 3 shows the iteration v.s. optimality gap when the constant learning rate is applied to different noise levels. In the noise free case, all three schemes converge linearly, but gradient smoothing has a smaller decay constant due to its increased condition number. When there is noise, our smoothed gradient helps to reduce the optimality gap and converges faster after a few iterations.

(a) = 0

(b) = 0.05

(c) = 0.1

(d) = 0.5

Figure 3: Iterations v.s. optimality gap for GD and smoothed GD with order 1 and 2 for the problem

in Eq.(5). Constant step size was used.

The exponentially decaying learning rate helps smoothed SGD to reach a point with a smaller optimality gap, and the higher order smoothing further reduce the optimality gap, as shown in Fig. 4. One simple reason for this in the noisy case is because of the noise removal properties of the smoothing operators. The influence of the learning rate is still under investigation. We establish the convergence of our proposed smoothing gradient descent algorithms.

3.1 SOME PROPERTIES OF LAPLACIAN SMOOTHING GRADIENT DESCENT
We say the objective function f has L-Lipschitz gradient, if for any w, u  Rm, we have f (w) - f (u)  L w - u , and f is a-strongly convex, if f (w) - f (u), w - u  a w - u 2. We define the vector norm induced by any matrix A as w A := w, Aw .

4

Under review as a conference paper at ICLR 2019

(a) = 0

(b) = 0.05

(c) = 0.1

(d) = 0.5

Figure 4: Iterations v.s. optimality gap for GD and smoothed GD with order 1 and 2 for the problem

in Eq.(5). Exponentially decaying step size is utilized here.

Proposition 2. Suppose f is convex with the global minimizer w, and f  = f (w). Consider the following iteration with constant learning rate  > 0

wk+1 = wk - (An )-1gk,

where gk is the sampled gradient in the kth iteration at wk satisfying E[gk] = f (wk). Denote GAn :=

supk

gk

2 (An )-1

and

wK

:=

K-1 k=0

wk /K

the

ergodic

average

of

iterates.

Then

the

optimality

gap

is

lim E[f (wK )] - f 
K



GAn 2

.

Note that g (An)-1 generally decreases in n unless g is constant, which indicates that a bigger n implies smaller optimality gap. This is consistent with the experimental results above.
Proposition 3. Suppose f is L-Lipschitz smooth and a-strongly convex with the global minimizer w. Consider
the generalized smoothing gradient descent algorithm

wk+1 = wk - k(An )-1gk,

where gk is the sampled gradient in the kth iteration at wk satisfying E gk = f (wk) and

E

gk 2 (An )-1

have

E



C0 + C1

f (wk)

2 for all k



N.

If we take k

=

C k+1

for some C

>

0, then we

wk - w

2 An

=E

wk - w 2 +  D+n (wk - w) 2 = O

1 k+1

,

i.e., we have Hn uniform convergence in  of {wk} in expectation. The Hn norm of w is defined by

w

n 

:=

w An = w, Anw .

Proposition 4. Consider the algorithm wk+1 = wk - k(An)-1f (wk). Suppose f is convex and L-

Lipschitz smooth. If if the Hessian 2f

the step of f is

size satisfies 0 <  continuous with w¯







¯

<

2 L

.

Then

limt

being the global minimizer of

f (wk) f , and ¯

 0. 2f

Moreover, < 1, then

wk - w An  0 as k  , and the convergence is linear and independent of .

In what follows, we present the noise reduction properties of the proposed smoothing operator A- 1.

Proposition 5. For any vector g  Rm, d = A- 1g, let jmax = arg maxi di and jmin = arg mini di. We have maxi di = djmax  gjmax  maxi gi and mini di = djmin  gjmin  mini gi.

Proposition 6.

The operator A- 1 preserves the sum of components.

For any g



m
R

and

d

=

A- 1g,

we

have j dj = j gj, or equivalently, 1 d = 1 g.

Proposition 7. Given any vector g  Rm and d = A-1g, then

d +  D+d 2  g . d

The above inequality is strict unless g = d is a constant vector. In particular, we have d  g and

D+d



1 

g.

Let g be the noise vector contained in the stochastic gradient, the above results imply that the extreme values in A-1g are smaller than those in g (in magnitude), and it also has a much smaller 2 norm.

Proposition 8.

For any g  Rm, define Var(g) :=

1 m

g

2-

1g m

2
be the variance of components in g.

Let d = A-1g, then

Var(d)  Var(g) - 2

D+d

2
- 2

D+d

4
.

m md2

The inequality is strict unless g = d is a constant vector.

5

Under review as a conference paper at ICLR 2019

Proposition 8 shows that the component-wise variance of A-1g is considerably less than that of g, unless g is a constant vector. Our last result shows that A-1g has diminishing 1 norm of finite difference of all orders. This is an excellent desnoising result.

Proposition 9. Given vectors g and d = A-1g, for any p  N, it holds that inequality is strict unless D+p g is a constant vector.

D+p d 1 

D+p g 1. The

Remark 4. The above proofs generalize for n > 1, except for Propositions 5 and 9.

3.2 SOFTMAX REGRESSION
Consider the application of the proposed optimization schemes for Softmax regression. We run 200 epochs of SGD and different order smoothing algorithms to optimize the maximum likelihood of the Softmax regression with batch size 100. Based on the results from previous section, we apply the exponentially decay learning rate with initial value 0.1 and decay 10 times after every 50 epochs. We train the model with only 10 % randomly selected MNIST training data and test the trained model on the entire testing images. We further compare the results with SVRG under the same setting. Figure. 5 shows the histograms of generalization accuracy of the Softmax regression model trained by SGD ((a)); SVRG ((b)); LS-SGD (order 1) ((c)); LS-SGD (oder 2) ((d)). It is seen that SVRG improves the generalization with higher average accuracy. But the first and second order smoothing schemes significantly improve averaged generalization accuracy by more than 1% and reduce the variance over 100 independent trials.

(a) SGD

(b) SVRG

(c) LS-GD: Order 1 (d) LS-GD Order 2

Figure 5: Testing accuracy of Softmax model trained on randomly selected 10% MNIST data.

4 APPLICATIONS TO DEEP NEURAL NETS
4.1 TRAIN NEURAL NETS WITH SMALL BATCH SIZE
Many advanced artificial intelligence tasks make high demand on training neural nets with extremely small batch size, the milestone technique for this is group normalization (Wu & He, 2018). In this section, we show that LS-SGD successfully trains DNNs with extremely small batch size. We consider LeNet-5 devised by (LeCun et al., 1998) for MNIST classification. Our network architecture is as follows
LeNet-5: input28×28  conv20,5,2  conv50,5,2  fc512  softmax.
The notation convc,k,m denotes a 2D convolutional layer with c output channels, each of which is the sum of a channel-wise convolution operation on the input using a learnable kernel of size k × k, it further adds ReLU nonlinearity and max pooling with stride size m. fc512 is an affine transformation that transforms the input to a vector of dimension 512. Finally, the tensors are activated by a softmax function. The MNIST data is first passed to the layer input28×28, and further processed by this hierarchical structure. We run 100 epochs of both SGD and LS-SGD with initial learning rate 0.01 and divide by 5 after 50 epochs, and use a weight decay of 0.0001 and momentum of 0.9. Figure. 6(a) plots the generalization accuracy on the test set with the LeNet5 trained with different batch sizes. For each batch size, LS-SGD with  = 1.0 keeps the testing accuracy more than 99.4%, SGD reduce the accuracy to 97% when batch size 4 is used. The classification become just a random guess, when the model is trained by SGD with batch size 2. Small batch size leads to large noise in the gradient, which may make the noisy gradient not along the decent direction, However, Lapacian smoothing rescues this by killing the noise.
4.2 IMPROVE GENERALIZATION ACCURACY
The skip connections in ResNet smooth the landscape of the loss function of the classical CNN (He et al., 2016; Li et al., 2017). This means that ResNet has fewer sharp minima. On Cifar10 (Krizhevsky, 2009), we compare the performance of LS-SGD and SGD on ResNet with the pre-activated ResNet56 as an illustration. We take the same training strategy as that used in (He et al., 2016), except that we run 200 epochs with the learning rate decaying by a factor of 5 after every 40 epochs. For ResNet, instead of applying LS-SGD for all epochs, we only use LS-SGD in the first 40 epochs, and the remaining training is carried out by SGD. The parameter  is set to 1.0. Figure 6(b) depicts one path of the training and generalization accuracy of the neural nets trained by
6

Under review as a conference paper at ICLR 2019

Figure 6: (a). Testing accuracy of LeNet5 trained by SGD/LS-SGD on MNIST with various batch sizes. (b). The evolution of the pre-activated ResNet56's training and generalization accuracy by SGD and LS-SGD. (Start from the 20-th epoch.)

SGD and LS-SGD, respectively. It is seen that, even though the training accuracy obtained by SGD is higher than that by LS-SGD, the generalization is however inferior to that of LS-SGD. We conjecture that this is due to the fact that SGD gets trapped into some sharp but deeper minimum, which fits better than a flat minimum but generalizes worse. We carry out 25 replicas of this experiments, the histograms of the corresponding accuracy are shown in Fig. 7.

SGD

LS-SGD with  = 1.0

Figure 7: The histogram of the generalization accuracy of the pre-activated ResNet56 on Cifar10 trained with LS-SGD over 25 independent experiments.

4.3 TRAINING WASSERSTERIN GAN
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are notoriously delicate and unstable to train (Arjovsky & Bottou, 2017). In (M. Arjovsky & Bottou, 2017), Wasserstein-GANs (WGANs) are introduced to combat the instability in the training of GANs. In addition to being more robust in training parameters and network architecture, WGANs provide a reliable estimate of the Earth Mover (EM) metric which correlates well with the quality of the generated samples. Nonetheless, WGANs training becomes unstable with a large learning rate or when used with a momentum based optimizer (M. Arjovsky & Bottou, 2017). In this section, we demonstrate that the gradient smoothing technique in this paper alleviates the instability in the training, and improves the quality of generated samples. Since WGANs with weight clipping are typically trained with RMSProp (Tieleman & Hinton, 2012), we propose replacing the gradient g by a smoothed version g = A-1g, and also update the running averages using g instead of g. We name this algorithm LS-RMSProp.
To accentuate the instability in training and demonstrate the effects of gradient smoothing, we deliberately use a large learning rate for training the generator. We compare the regular RMSProp with the proposed LSRMSProp. The learning rate for the critic is kept small and trained approximately to convergence so that the critic loss is still an effective approximation to the Wasserstein distance.To control the number of unknowns in the experiment and make a meaningful comparison using the critic loss, we use the classical RMSProp for the critic, and only apply LS-RMSProp to the generator.

RMSProp

LS-RMSProp,  = 3.0

Figure 8: Critic loss with learning rate lrD = 0.0001, lrG = 0.005 for RMSProp (Left) and LSRMSProp (Right), trained for 20K iterations. We apply a mean filter of window size 13 for better visualization. The loss from LS-RMSProp is visibly less noisy.
7

Under review as a conference paper at ICLR 2019

We train the WGANs on the MNIST dataset using the DCGAN architecture (Radford et al., 2015) for both the critic and generator. In Figure 8 (left), we observe the loss for RMSProp trained with a large learning rate has multiple sharp spikes, indicating instability in the training process. The samples generated are also lower in quality, containing noisy spots as shown in Figure 9 (a). In contrast, the curve of training loss for LS-RMSProp is smoother and exhibits fewer spikes. The generated samples as shown in Figure 9 (b) are also of better quality and visibly less noisy. A reasonable conjecture for this improved stability is that the spikes which appear in training for RMSProp correspond to sharp extrema in the approximate EM metric, and this is circumvented by smoothing the gradient. The effects are less pronounced with a small learning rate, but still result in a modest improvement in sample quality as shown in Figure 9 (c) and (d).We also apply LS-RMSProp for training the critic, but do not see a clear improvement in the quality. This may be because the critic is already trained near optimality during each iteration, and does not benefit much from gradient smoothing.

RMSProp

LS-RMSProp,  = 3.0

RMSProp

LS-RMSProp,  = 3.0

(a) (b) (c) (d)
Figure 9: Samples from WGANs trained with RMSProp (a, c) and LS-RMSProp (b, d). The learning rate is set to lrD = 0.0001, lrG = 0.005 for both RMSProp and LS-RMSProp in (a) and (b). And lrD = 0.0001, lrG = 0.0001 are used for both RMSProp and LS-RMSProp in (c) and (d). The critic is trained for 5 iterations per step of the generator, and 200 iterations per every 500 steps of the generator.

4.4 DEEP REINFORCEMENT LEARNING

Deep reinforcement learning (DRL) has been applied to playing games including Cartpole (Brockman et al., 2016), Atari (Mnih et al., 2013), Go (Silver & et al, 2016; Mnih & et al, 2015). DNNs play a vital role in approximating the Q-function or policy function. We apply the Laplacian smoothed gradient to train the policy function to play the Cartpole game. We apply the standard procedure to train the policy function by using the policy gradient (Brockman et al., 2016). We use the following network to approximate the policy function:
input4  fc20  relu  fc2  softmax.
The network is trained by RMSProp and LS-RMSProp with  = 1.0, respectively. The learning rate and other related parameters are set to be the default ones in PyTorch. The training is stopped once the average duration of 5 consecutive episodes is more than 490. In each training episode, we set the maximal steps to be 500. Left and right panels of Fig. 10 depict a training procedure by using RMSProp and LS-RMSProp, respectively. We see that Laplacian smoothed gradient takes fewer episodes to reach the stopping criterion. Moreover, we run the above experiments 5 times independently, and apply the trained model to play Cartpole. The game lasts more than 1000 steps for all the 5 models trained by LS-RMSProp, while only 3 of them lasts more than 1000 steps when the model is trained by vanilla RMSProp.

RMSProp

LS-RMSProp,  = 1.0

Figure 10: Durations of the cartpole game in the training procedure. Left and right are training procedure by RMSProp and LS-RMSProp with  = 1.0, respectively.
5 CONCLUDING REMARKS
Motivated by the theory of Hamilton-Jacobi partial differential equations, we proposed Laplacian smoothing gradient descent and its high order generalizations. This simple modification dramatically reduces the optimality gap in stochastic gradient descent and helps to find better minima. Extensive numerical examples ranging from toy cases to shallow and deep neural nets to generative adversarial networks and to deep reinforcement learning, all demonstrate the advantage of the proposed smoothed gradient. Several issues remain, in particular devising an on-the-fly adaptive method for choosing the smoothing parameter  instead of using a fixed value.
8

Under review as a conference paper at ICLR 2019
ACKNOWLEDGMENTS
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.
REFERENCES
M. Abadi, A. Agarwal, and et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.
D. P. Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
L. Bottou, E. Frank, and J. Nocedal. Optimization methods for large-scale machine learning. arXiv preprint arXiv:1606.04838, 2016.
L. Bottou, E. F. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223­311, 2018.
O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499­526, 2002.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi Carlo, C. Borgs, J. Chayes, L. Sagun, and R. Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.
P. Chaudhari, A. Oberman, S. Osher, S. Soatto, and C. Guillame. Deep relaxation: partial differential equations for optimizing deep neural networks. arXiv preprint arXiv:1704.04932, 2017.
A. Defazio and F. Bach. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, 2014.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121­2159, 2011.
L.C. Evans. Partial differential equations. 2010.
A. Gonen and S. Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle problems. arXiv preprint arXiv:1701.04271, 2017.
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672­2680, 2014.
M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Learning Representations, 2016.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
S. Jastrzebski, Z. Kenton, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. Dnn's sharpest directions along the sgd trajectory. arXiv preprint arXiv:1807.05031, 2018.
R. Johoson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, 2013.
M. Jung, G. Chung, G. Sundaramoorthi, L. Vese, and A. Yuille. Sobolev gradients and joint variational image segmentation, denoising, and deblurring. In Computational Imaging VII, volume 7246, pp. 72460I. International Society for Optics and Photonics, 2009.
N. Keskar, M. Shirish, N. Dheevatsa, S. Jorge, Mikhail, P. Tang, and P. Tak. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2017.
A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
9

Under review as a conference paper at ICLR 2019
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 81:2278­2324, 1998.
F. Li and et al. Cs231n: Convolutional neural networks for visual recognition. 2018. H. Li, Z. Xu, G. Taylor, and T. Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint
arXiv:1712.09913, 2017. S. Chintala M. Arjovsky and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017. Mnih and et al. Human-level control through deep reinforcement learning. Nature, 518:529­533, 2015. V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing Atari
with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.
Automatic differentiation in pytorch. 2017. A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative
adversarial networks. arXiv preprint arXiv:1511.06434, 2015. J Schmidhuber. Deep learning in neural networks: An overview. arXiv preprint arXiv:1404.7828, 2014. D. Silver and et al. Mastering the game of go with deep neural networks and tree search. Nature, 529:484­489,
2016. T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural networks for machine learning, 4(2):26­31, 2012. Y. Wu and K. He. Group normalization. In European Conference on Computer Vision, 2018.
10

Under review as a conference paper at ICLR 2019

APPENDIX: PROOFS

Proposition 1. Suppose f (w) is differentiable, the Laplacian smoothing GD update on u(w, t) wk+1 = wk - tA- 1wu(wk, t)
permits the smoothing implicit gradient descent on f (w) wk+1 = wk - tA-1f (wk+1).

Proof of Proposition 1. We define

z(w, v, t) := f (v) +

1 2t

v - w, A(v - w)

,

and rewrite u(w, t) = infv z(w, v, t) as z(w, v(w, t), t), where v(w, t) = arg minv z(w, v, t). Then by the Euler-Lagrange equation,

wu(w, t) = wz(w, v(w, t), t) = Jwv(w, t)vz(w, v(w, t), t) + wz(w, v(w, t), t),

where Jwv(w, t) is the Jacobian matrix of v w.r.t. w. Notice that vz(w, v(w, t), t) = 0,

wu(w, t)

=

wz(w, v(w, t), t)

=

1 t

A

(v(w,

t)

-

w).

Letting w = wk and wk+1 = v(wk, t) = arg minv z(wk, v, t) in the above equalities, we have

wu(wk, t)

=

1 t

A

(wk+1

-

wk ).

In summary, the gradient descent wk+1 = wk -tA-1wu(wk, t) is equivalent to the proximal point iteration

wk+1 = arg minv f (v) +

1 2t

v - wk, A(v - wk)

, which yields wk+1 = wk - tA- 1f (wk+1).

Proposition 2. Suppose f is convex with the global minimizer w, and f  = f (w). Consider the following iteration with constant learning rate  > 0

wk+1 = wk - (An )-1gk

where gk is the sampled gradient in the kth iteration at wk satisfying E[gk] = f (wk). Denote GAn :=

supk

gk

2 (An )-1

and

wK

:=

K-1 k=0

wk /K

the

ergodic

average

of

iterates.

Then

the

optimality

gap

is

lim E[f (wK )] - f 
K



GAn 2

.

Proof. Since f is convex, we have

f (wk), wk - w  f (wk) - f .

Furthermore,

E[

wk+1 - w

2 An

]

=

E[

wk - (An)-1gt - w

2 An

]

= E[

wk - w

2 An

]

-

2E[

gk, wk

-

w

] + 2E[

(An )-1gt

2 An

]

 E[

wk - w

2 An

]

-

2E[

f (wk), wk

-

w

] + 2

gk

2 (An )-1

 E[

wk - w

2 An

]

-

2(E[f

(wk

)]

-

f

)

+

 2 GAn

,

where the last inequality is due to (7). We rearrange the terms and arrive at

E[f (wk)]

-

f



1 2

(E[

wk - w

2 An

]

-

E[

wk+1 - w

2 An

])

+

GAn 2

.

Summing over k from 0 to K - 1 and averaging and using the convexity of f , we have

E[f (wK )] - f  

K-1 k=0

E[f (wk)]

-

f



K

1 2K E[

w0 - w

2 An

]

+

GAn 2

.

Taking the limit as K   above establishes the result.

(7)

11

Under review as a conference paper at ICLR 2019

Proposition 3. Suppose f is L-Lipschitz smooth and a-strongly convex. Consider the generalized smoothing gradient descent algorithm
wk+1 = wk - k(An )-1gk,

where gk is the sampled gradient in the kth iteration at wk satisfying E gk = f (wk) and

E

gk 2 (An )-1

have

E



C0 + C1

f (wk)

2 for all k



N.

If we take k

=

C k+1

for some C

>

0, then we

wk - w

2 An

=E

wk - w 2 +  D+n (wk - w) 2 = O

1 k+1

,

i.e., we have Hn uniform convergence in  of {wk} in expectation. The Hn norm of w is defined by

w

n 

:=

w An = w, Anw .

Proof of Proposition 3. Since f (w) = 0, by strong convexity of f , we have

f (wk), wk - w = f (wk) - f (w), wk - w  a wk - w 2.

Moreover, by L-smoothness of f and the fact that An = 1, we also have f (wk) = f (wk) - f (w)  L wk - w  L wk - w An .
Hence,

E[

wk+1 - w

2 An

]

=

E[

wk - (An )-1gk - w

2 An

]

= E[

wk - w

2 An

]

-

2k E

gk, wk - w

+ k2E[

gk

]2
(An )-1

= E[

wk - w

2 An

]

-

2k

f (wk), wk

- w

+ k2E[

gk

]2
(An )-1

 (1 - 2ka)E

wk - w

2 An

+ k2

C0 + C1E[ f (wk) 2]

 1 - 2ka + k2L2C1 E

wk - w

2 An

+ k2C0,

where in the first inequality we used

(An)-1

=

1 for all 

and n.

Taking k

=

C k+1

for some proper C

>

0

and using induction, one can show that E

wk - w

2 An

=E

wk - w 2 +  D+n (wk - w)

=

O(

1 k+1

).

Proposition 4. Consider the algorithm wk+1 = wk - k(An)-1f (wk). Suppose f is L-Lipschitz smooth

and 0 < with w

b¯eingthe m¯in<imiL2ze. rTohfefn,

limt f and ¯ 2f

(wk)  < 1, then

0. Moreover, if the Hessian 2f of f is continuous wk -w An  0 as k  , and the convergence

is linear.

Proof of Proposition 4. By the Lipschitz continuity of f and the descent lemma (Bertsekas, 1999), we have

f (wk+1) = f (wk - k(An )-1f (wk))

 f (wk) - k

f (wk), (An)-1f (wk))

+ k2L 2

(An)-1f (wk)

2

 f (wk) - k

f (wk)

2 (An )-1

+

k2 L 2

f (wk)

2 (An )-1

 f (wk) -  1 - ¯L ¯2

f (wk)

.2
(An )-1

Summing the above inequality over k, we have

 ¯

1 - ¯L 2



f (wk)

2 (An )-1

 f (w0) - lim f (wk) < . k

k=0

Therefore,

f (wk)

2 (An )-1

 0, and thus

f (wk)

 0.

12

Under review as a conference paper at ICLR 2019

For the second claim, we have

wk+1 - w = wk - w - k(An )-1(f (wk) - f (w))

= wk - w - k(An )-1

1
2f (w +  (wk+1 - w)) · (wk - w)d
0

= wk - w - k(An )-1

1
2f (w +  (wk+1 - w))d · (wk - w)
0

1

=

(An

)-

1 2

I

-

k

(An

)-

1 2

2f

(w

+



(wk+1

-

w))d

(An)-

1 2

)

(An)

1 2

(wk

-

w)

0

Therefore,

wk+1 - w An 

I

-

t

(An

)-

1 2

1

2f

(w

+



(wk+1

-

w))d

(An )-

1 2

0

wk - w An .

So if k 2f



1 (An )-1

= 1, the result follows.

Proposition 5. For any vector g  Rm, d = A-1g, let jmax = arg maxi di and jmin = arg mini di. We have maxi di = djmax  gjmax  maxi gi and mini di = djmin  gjmin  mini gi.
Proof of Proposition 5. Since g = Ad, it holds that
gjmax = djmax + (2djmax - djmax-1 - djmax+1),
where periodicity of subindex are used if necessary. Since 2djmax - djmax-1 - djmax+1  0, We have maxi di = djmax  gjmax  maxi gi. A similar argument can show that mini di = djmin  gjmin  mini gi.

Proposition 6. The operator A-1 preserves the sum of components. For any g  Rm and d = A- 1g, we have j dj = j gj, or equivalently, 1 d = 1 g.
Proof of Proposition 6. Since d = Ag,
gi = 1 g = 1 (I + D+ D+)d = 1 d = di,
ii
where we used D+1 = 0.

Proposition 7. Given any vector g  Rm and d = A-1g, then d +  D+d 2  g . d

The above inequality is strict unless g = d is a constant vector. In particular, we have d  g and

D+d



1 

g.

Proof of Proposition 7. By the definition of A,

g = Ad = (I - D-D+)d = d + D+ D+d.

(8)

Therefore, pre-multiplying by d on both sides, we have

d 2 +  D+d 2 = d g  d g .

In particular, d



g and  D+d 2 

d

g



g 2, so D+d



1 

g

. All the inequalities are

strict unless D+d = 0, and g = d is a constant vector.

13

Under review as a conference paper at ICLR 2019

Proposition 8.

For

any

g



m
R

,

define

Var(g)

:=

1 m

g

2-

1g m

2
be the variance of components in g.

Let d = A-1g, then

Var(d)  Var(g) - 2

D+d m

2
- 2

D+d md

4 2

.

The inequality is strict unless g = d is a constant vector.

Proof of Proposition 8. Since 1

g=1

d and

d

+

D+ d d

2



g,

Var(g)  1 m

d 2 + 2

D+d 2 + 2

D+d 4 d2

-

1d 2 n

= Var(d) + 2

D+d 2 + 2 m

D+d md

4 2

.

The inequality is strict unless D+d = 0, and g = d is a constant vector.

Proposition 9. Given vectors g and d = A- 1g, for any p  N, it holds that inequality is strict unless D+p g is a constant vector.

D+p d 1 

D+p g 1. The

Proof of Proposition 9. Since (1 + 2)di = gi + di+1 + di-1, for any p  N, we have (1 + 2)(D+p d)i = (D+p g)i + (D+p d)i+1 + (D+p d)i-1.

So (1 + 2)|(D+p d)i|  |(D+p g)i| + |(D+p d)i+1| + |(D+p d)i-1|.
The inequality is strict if there are sign changes among the (D+p d)i-1, (D+p d)i, (D+p d)i+1. Summing over i and using periodicity, we have

mm

m

(1 + 2) |(D+p d)i|  |(D+p g)i| + 2 |(D+p d)i|,

i=1

i=1

i=1

and the result follows. The inequality is strict unless D+p g is a constant vector.

14

