Under review as a conference paper at ICLR 2019

LEARNING TWO-LAYER NEURAL NETWORKS WITH SYMMETRIC INPUTS
Anonymous authors Paper under double-blind review

ABSTRACT
We give a new algorithm for learning a two-layer neural network under a very general class of input distributions. Assuming there is a ground-truth two-layer network
y = A(W x) + ,
where A, W are weight matrices,  represents noise, and the number of neurons in the hidden layer is no larger than the input or output, our algorithm is guaranteed to recover the parameters A, W of the ground-truth network. The only requirement on the input x is that it is symmetric, which still allows highly complicated and structured input.
Our algorithm is based on the method-of-moments framework and extends several results in tensor decompositions. We use spectral algorithms to avoid the complicated non-convex optimization in learning neural networks. Experiments show that our algorithm can robustly learn the ground-truth neural network with a small number of samples for many symmetric input distributions.

1 INTRODUCTION

Deep neural networks have been extremely successful in many tasks related to images, videos and reinforcement learning. However, the success of deep learning is still far from being understood in theory. In particular, learning a neural network is a complicated non-convex optimization problem. Why can we efficiently learn a neural network even if we assume one exists? Despite a lot of recent effort, the class of neural networks that we know how to provably learn in polynomial time is still very limited, and many results require strong assumptions on the input distribution.
In this paper we design a new algorithm that is capable of learning a two-layer1 neural network for a very general class of input distributions. Following standard models for learning neural networks, we assume there is a ground truth neural network. The input data (x, y) is generated by first sampling the input x from an input distribution D, then computing y according to the ground truth network that is unknown to the learner. The learning algorithm will try to find a neural network f such that f (x) is as close to y as possible over the input distribution D. Learning a neural network is known to be a hard problem even in some simple settings (Goel et al., 2016; Brutzkus & Globerson, 2017), so we need to make assumptions on the network structure or the input distribution D, or both. Many works have worked with simple input distribution (such as Gaussians) and try to learn more and more complex networks (Tian, 2017; Brutzkus & Globerson, 2017; Li & Yuan, 2017; Soltanolkotabi, 2017; Zhong et al., 2017). However, the input distributions in real life are distributions of very complicated objects such as texts, images or videos. These inputs are highly structured, clearly not Gaussian and do not even have a simple generative model.
We consider a type of two-layer neural network, where the output y is generated as

y = A(W x) + .

(1)

1There are different ways to count the number of layers. Here by two-layer network we refer to a fullyconnected network with two layers of edges (two weight matrices). This is considered to be a three-layer network if one counts the number of layers for nodes (e.g. in Goel & Klivans (2017)) or a one-hidden layer network if one just counts the number of hidden layers.

1

Under review as a conference paper at ICLR 2019
Here x  Rd is the input, W  Rk×d and A  Rk×k are two weight matrices2. The function  is the standard ReLU activation function (x) = max{x, 0} applied entry-wise to the vector W x, and  is a noise vector that has E[] = 0 and is independent of x. Although the network only has two layers, learning similar networks is far from trivial: even when the input distribution is Gaussian, Ge et al. (2017b) and Safran & Shamir (2018) showed that standard optimization objective can have bad local optimal solutions. Ge et al. (2017b) gave a new and more complicated objective function that does not have bad local minima.
For the input distribution D, our only requirement is that D is symmetric. That is, for any x  Rd, the probability of observing x  D is the same as the probability of observing -x  D. A symmetric distribution can still be very complicated and cannot be represented by a finite number of parameters. In practice, one can often think of the symmetry requirement as a "factor-2" approximation to an arbitrary input distribution: if we have arbitrary training samples, it is possible to augment the input data with their negations to make the input distribution symmetric, and it should take at most twice the effort in labeling both the original and augmented data. In many cases (such as images) the augmented data can be interpreted (for images it will just be negated colors) so reasonable labels can be obtained.
1.1 OUR RESULTS
When the input distribution is symmetric, we give the first algorithm that can learn a two-layer neural network. Our algorithm is based on the method-of-moments approach: first estimate some correlations between x and y, then use these information to recover the model parameters. More precisely we have
Theorem 1 (informal). If the data is generated according to Equation (1), and the input distribution x  D is symmetric. Given exact correlations between x, y of order at most 4, as long as A, W and input distribution are not degenerate, there is an algorithm that runs in poly(d) time and outputs a network A^, W^ of the same size that is effectively the same as the ground-truth network: for any input x, A^(W^ x) = A(W x).
Of course, in practice we only have samples of (x, y) and cannot get the exact correlations. However, our algorithm is robust to perturbations, and in particular can work with polynomially many samples.
Theorem 2 (informal). If the data is generated according to Equation (1), and the input distribution x  D is symmetric. As long as the weight matrices A, W and input distributions are not degenerate, there is an algorithm that uses poly(d, 1/ ) time and number of samples and outputs a network A^, W^ of the same size that computes an -approximation function to the ground-truth network: for any input x, A^(W^ x) - A(W x) 2  .
In fact, the algorithm recovers the original parameters A, W up to scaling and permutations. Here when we say weight matrices are not degenerate, we mean that the matrices A, W should be full rank, and in addition a certain distinguishing matrix that we define later in Section 2 is also full rank. We justify these assumptions using the smoothed analysis framework.
In smoothed analysis, the input is not purely controlled by an adversary. Instead, the adversary can first generate an arbitrary instance (in our case, arbitrary weight matrices W, A and symmetric input distribution D), and the parameters for this instance will be randomly perturbed to yield a perturbed instance. The algorithm only needs to work with high probability on the perturbed instance. This limits the power of the adversary and prevents it from creating highly degenerate cases (e.g. choosing the weight matrices to be much lower rank than k). Roughly speaking, we show
Theorem 3 (informal). There is a simple way to perturb the input distribution, W and A such that with high probability, the distance between the perturbed instance and original instance is at most , and our algorithm outputs an -approximation to the perturbed network with poly(d, 1/, 1/ ) time and number of samples.
In the rest of the paper, we will first review related works. Then in Section 2 we formally define the network and introduce some notations. Our algorithm is given in Section 3. Finally in Section 4
2Here we assume A  Rk×k for simplicity, our results can easily be generalized as long as the dimension of output is no smaller than the number of hidden units.
2

Under review as a conference paper at ICLR 2019

we run experiments to show that the algorithm can indeed learn the two-layer network efficiently and robustly. The experiments show that our algorithm works robustly with reasonable number of samples for different (symmetric) input distributions and weight matrices. Due to space constraints, the proof for polynomial number of samples (Theorem 2) and smoothed analysis (Theorem 3 are deferred to the appendix.
1.2 RELATED WORK
There are many works in learning neural networks, and they come in many different styles.
Non-standard Networks Some works focus on networks that does not use standard activation functions. Arora et al. (2014) gave an algorithm that learns a network with discrete variables. Livni et al. (2014) and follow-up works learn neural networks with polynomial activation functions.
ReLU network, Gaussian input When the input is Gaussian, Ge et al. (2017b) showed that for a two-layer neural network, although the standard objective does have bad local optimal solutions, one can construct a new objective whose local optima are all globally optimal. Several other works (Tian, 2017; Brutzkus & Globerson, 2017; Li & Yuan, 2017; Soltanolkotabi, 2017; Zhong et al., 2017) extend this to different settings. A closely related work Janzamin et al. (2015) does not require the input distribution to be Gaussian, but still relies on knowing the score function of the input distribution (which in general cannot be estimated efficiently from samples).
General input distributions There are several lines of work that tries to extend the learning results to more general distributions. Du et al. (2017) showed how to learn a single neuron or a single convolutional filter under some conditions for the input distribution. Daniely et al. (2016); Zhang et al. (2016; 2017); Goel & Klivans (2017); Du & Goel (2018) use kernel methods to learn neural networks when the norm of the weights and input distributions are both bounded (and in general this line of work depends exponentially on the norms). The work that is most similar to our setting is Goel et al. (2018), where they showed how to learn a single neuron (or a single convolutional filter) for any symmetric distribution. Our two-layer neural network model is much more complicated.
Our work uses method-of-moments, which has already been applied to learn many latent variable models (see Anandkumar et al. (2014) and references there). The particular algorithm that we use is inspired by a over-complete tensor decomposition algorithm FOOBI (De Lathauwer et al., 2007). Our smoothed analysis results are inspired by Bhaskara et al. (2014) and Ma et al. (2016), although our setting is more complicated and we need several new ideas.

2 PRELIMINARIES

In this section, we first describe the neural network model that we learn, and then introduce notations related to matrices and tensors. Finally we will define distinguishing matrix, which is a central object in our analysis.

2.1 NETWORK MODEL

We consider two-layer neural networks with d-dimensional input, k hidden units and k-dimensional

output, as shown in Figure 1. We assume that k  d. The input of the neural network is denoted by x  Rd. Assume that the input x is i.i.d. drawn from a symmetric distribution D3. Let the two weight matrices in the neural network be W  Rk×d and A  Rk×k. The output y  Rk is
generated as follows:

y = A(W x) + ,

(2)

where (·) is the element-wise ReLu function and   Rk is zero-mean random noise, which is independent with input x. Let the value of hidden units be h  Rk, which is equal to (W x).
Denote i-th row of matrix W as wi . Also, let i-th row of matrix A be ai . By property of ReLU activations, for any constant c > 0, scaling the i-th row of W by c while scaling the i-th column of A

3Suppose

the

density

function

of

distribution

D

is

p(·),

we

assume

p(x)

=

p(-x)

for

any

x



d
R

3

Under review as a conference paper at ICLR 2019

by 1/c does not change the function computed by the network. Therefore without loss of generality, we assume every row vector of W has unit norm in this paper.

 w1 w2

y = Ah + 

A  Rk×k

 wk

h = (W x)  Rk

W  Rk×d

xD

Figure 1: Network model.

2.2 NOTATIONS

We use [n] to denote the set {1, 2, · · · , n}. For two random variables X and Y , we say X =d Y if they come from the same distribution.

In the vector space Rn, we use ·, · to denote the inner product of two vectors, and use · to denote the Euclidean norm. We use ei to denote the i-th standard basis vector. For a matrix A  Rm×n, let A[i,:] denote its i-th row vector, and let A[:,j] denote its j-th column vector. Let A's singular values be 1(A)  2(A)  · · ·  min(m,n)(A), and denote the smallest singular value be min(A) =
min(m,n)(A). The condition number of matrix A is defined as (A) := 1(A)/min(A). We use In to denote the identity matrix with dimension n × n. The spectral norm of a matrix is denoted as
· , and the Frobenius norm as · F .
We represent a d-dimensional linear subspace S by a matrix S  Rn×d, whose columns form an orthonormal basis for subspace S. The projection matrix onto the subspace S is denoted by ProjS = SS , and the projection matrix onto the orthogonal subspace of S is denoted by ProjS = In - SS .
For matrix A  Rm1×n1 , C  Rm2×n2 , let the Kronecker product of A and C be A  C  Rm1m2×n1n2 , which is defined as (A  C)(i1,i2),(j2,j2) = Ai1,i2 Cj1,j2 . For a vector x  Rd, the Kronecker product x  x has dimension d2. We denote the p-fold Kronecker product of x as xp, which has dimension dp.

We often need to convert between vectors and matrices. For a matrix A  Rm×n, let vec(A)  Rmn

be the vector obtained by stacking all the columns of A. For a vector a  Rm2 , let mat(x) 

Rm×m denote the inverse mapping such that vec(mat(a)) = a. Let Rsky×mk be the space of all k × k

symmetric matrices, which has dimension

k 2

+ k. For convenience, we denote k2 =

k 2

.

For

a

symmetric matrix B  Rksy×mk, we denote vec(B)  Rk2+k as the vector obtained by stacking all

the upper triangular entries (including diagonal entries) of B. Note that vec(B) still has dimension

k2. For a vector b  Rk2+k, let mat(b)  Rksy×mk denote the inverse mapping of vec(·) such that vec(mat(B)) = b.

2.3 DISTINGUISHING MATRIX

A central object in our analysis is a large matrix whose columns are closely related to pairs of hidden variables. We call this the distinguishing matrix and define it below:

Definition 1. Given a weight matrix W of the first layer, and the input distribution D, the distin-

guishing

matrix

ND



d2
R

×(k2)

is

a

matrix

whose

columns

are

indexed

by

ij

where

1



i

<

j



k,

and

NiDj = ExD (wi x)(wj x)(x  x)1{wi xwj x  0} .

Another related concept is the augmented distinguishing matrix M , which is a d2 ×

k 2

+1

matrix whose first

k 2

columns are exactly the same as distinguishing matrix N , and the last column

(indexed by 0) is defined as

M0D = ExD x  x .

4

Under review as a conference paper at ICLR 2019

For both matrices, when the input distribution is clear from context we use N or M and omit the superscript.
The exact reason for these definitions will only be clear after we explain the algorithm in Section 3. Our algorithm will require that these matrices are robustly full rank, in the sense that min(M ) is lowerbounded. Intuitively, every column NiDj looks at expectation over samples that have opposite signs for weights wi, wj (wi xwj x  0, hence the name distinguishing matrix).
Requiring M and N to be full rank prevents several degenerate cases. For example, if two hidden units are perfectly correlated and always of the same sign for every input, this is very unnatural and requiring the distinguishing matrix to be full rank prevents such cases. Later in Section C we will also show that requiring a lowerbound on min(M ) is not unreasonable: in the smoothed analysis setting where the nature can make a small perturbation on the input distribution D, we show that for any input distribution D, there exists simple perturbations D that are arbitrarily close to D such that min(M D ) is lowerbounded.

3 OUR ALGORITHM
In this section, we describe our algorithm for learning the two-layer networks defined in Section 2.1. As a warm-up, we will first consider a single-layer neural network and recover the results in Goel et al. (2018) using method-of-moments. This will also be used as a crucial step in our algorithm. Due to space constraints we will only introduce algorithm and proof ideas, the detailed proof is deferred to Section A in appendix.

3.1 WARM-UP: LEARNING SINGLE-LAYER NETWORKS

We will first give a simple algorithm for learning a single-layer neural network. More precisely, suppose we are given samples (x1, y1), ..., (xn, yn) where xi  D comes from a symmetric distribution, and the output yi is computed by

yi = (w xi) + i.

(3)

Here i's are i.i.d. noises that satisfy E[i] = 0. Noise i is also assumed to be independent with input xi. The goal is to learn the weight vector w.

Algorithm 1 Learning Single-layer Neural Networks

Input: Samples (x1, y1), ..., (xn, yn) generated according to Equation (3).

Output: Estimate of weight vector w.

1:

Estimate v

=

1 n

2:

Estimate C

=

1 n

3: return 2C-1v.

n i=1

yi

xi.

n i=1

xixi

The idea of the algorithm is simple: we will estimate the correlations between x and y and the covariance of x, and then recover the hidden vector w using these two estimates. The main challenge here is that y is not a linear function on x. Goel et al. (2018) gave a crucial observation that allows us to deal with the non-linearity:
Lemma 1. Suppose x  D comes from a symmetric distribution and y is computed as in (3), then
1 E[yx] = 2 E[xx ]w.
Importantly, the right hand side of Lemma 1 does not contain the ReLU function . This is true because if x comes from a symmetric distribution, averaging between x and -x can get rid of nonlinearities like ReLU or leaky-ReLU. Later we will prove a more general version of this lemma (Lemma 6).
Using this lemma, it is immediate to get a method-of-moments algorithm for learning w: we just need to estimate E[yx] and E[xx ], then we know w = 2(E[xx ])-1E[yx]. This is summarized in Algorithm 1.

5

Under review as a conference paper at ICLR 2019

3.2 LEARNING TWO-LAYER NETWORKS
In order to learn the weights of the network defined in Section 2.1, a crucial observation is that we have k outputs as well as k hidden-units. This gives a possible way to reduce the two-layer problem to the single-layer problem. For simplicity, we will consider the noiseless case in this section, where
y = A(W x).

Let u  Rk be a vector and consider u y, it is clear that u y = (u A)(W x). Let zi be the normalized version i-th row of A-1, then we know zi has the property that zi A = iei where i > 0 is a constant and ei is a basis vector.
The key observation here is that if u = zi, then u A = iei . As a result, u y = iei (W x) = (iwi x) is the output of a single-layer neural network with weights equal to iwi. If we know all the vectors {z1, ..., zk}, the input/output pairs (x, zi y) correspond to single-layer networks with weight vector iwi. We can then apply the algorithm in Section 3.1 (or the algorithm in Goel et al. (2018)) to learn the weight vectors.
When u A = iei, we say that u y is a pure neuron. Next we will design an algorithm that can find all vectors {zi}'s that generate pure neurons, and therefore reduce the problem of learning a two-layer network to learning a single-layer network.

Pure Neuron Detector In order to find the vector u that generates a pure neuron, we will try to find some property that is true if and only if the output can be represented by a single neuron.

Intuitively, using techniques similar to Lemma 1 we can get a property that is true for all pure neurons:

Lemma 2.

Suppose y^ = (w

x), then E[y^2] =

1 2

w

E[xx

]w, and E[y^x] =

1 2

E[xx

]w.

As a

result we have

E[y^2] = 2E[y^x ]E[xx ]-1E[y^x].

As before, the ReLU activation does not appear because of the symmetric input distribution. For

y^ = u y, we can estimate all of these moments (E[y^2], E[y^x ], E[xx ]) using samples and check

whether this condition is satisfied. However, the problem with this property is that even if z = u y

is not pure, it may still satisfy the property. More precisely, if y^ =

k i=1

ci

(wi

x),

then

we

have

2E[y^x ]E[xx ]-1E[y^x] - E[y^2] =

cicjE (wi x)(wj x)1{wi xwj x  0}

1i<jk

The additional terms may accidentally cancel each other which leads to a false positive. To address this problem, we consider a higher order moment:
Lemma 3. Suppose y^ = (w x), then

E[y^2(x  x)] = 2E[y^x ]E[xx ]-1E[y^x(x  x)].

Moreover, if y^ =

k i=1

ci(wi

x)

where

c



Rk

is

a

k-dimensional

vector,

we

have

2E[y^x ]E[xx ]-1E[y^x(x  x)] - E[y^2(x  x)] =

cicj Nij .

1i<jk

Here Nij's are columns of the distinguishing matrix defined in Definition 1.

The important observation here is that there are

k 2

extra terms in 2E[y^x

]E[xx

]-1E[y^x(x 

x)] - E[y^2(x  x)] that are multiples of Nij, which are d2 (or

d+1 2

considering their symmetry)

dimensional objects. When the distinguishing matrix is full rank, we know its columns Nij are

linearly independent. In that case, if the sum of the extra terms is 0, then the coefficient in front

of each Nij must also be 0. The coefficients are cicj which will be non-zero only if both ci, cj are non-zero, therefore to make all the coefficients 0 at most one of {ci} can be non-zero. This is

summarized in the following Corollary:

6

Under review as a conference paper at ICLR 2019

Corollary 1 (Pure Neuron Detector). Define f (u) := 2E[(u y)x ]E[xx ]-1E[(u y)x(x  x)] - E[(u y)2(x  x)]. Suppose the distinguishing matrix is full rank, if f (u) = 0 for unit vector u, then u must be equal to one of ±zi.
We will call the function f (u) a pure neuron detector, as u y is a pure neuron if and only if f (u) = 0. Therefore, to finish the algorithm we just need to find all solutions for f (u) = 0.

Linearization The main obstacle in solving the system of equations f (u) = 0 is that every entry

of f (u) is a quadratic function in u. The system of equations f (u) = 0 is therefore a system

of quadratic equations. Solving a generic system of quadratic equations is NP-hard. However, in

our case this can be solved by a technique that is very similar to the FOOBI algorithm for tensor

decomposition (De Lathauwer et al., 2007). The key idea is to linearize the function by thinking of

each degree 2 monomial uiuj as a separate variable. Now the number of variables is k2+k =

k 2

+k

and f is linear in this space. In other words, there exists a matrix T  Rd2×(k2+k) such that

T vec(uu ) = f (u). Clearly, if u y is a pure neuron, then T vec(uu ) = f (u) = 0. That is,

{vec(zizi )} are all in the nullspace of T . Later in Section A we will prove that the nullspace of T

consists of exactly these vectors (and their combinations):

Lemma 4. Let T be the unique Rd2×(k2+k) matrix that satisfies T vec(uu ) = f (u) (where f (u)
is defined as in Corollary 1), suppose the distinguishing matrix is full rank, then the nullspace of T is exactly the span of {vec(zizi )}.

Based on Lemma 4, we can just estimate the tensor T from the samples we are given, and its smallest singular directions would give us the span of {vec(zizi )}.

Finding zi's from span of zizi In order to reduce the problem to a single-layer problem, the final
step is to find zi's from span of zizi . This is also a step that has appeared in FOOBI and more generally other tensor decomposition algorithms, and can be solved by a simultaneous diagonalization. Let Z be the matrix whose rows are zi's, which means Z = diag()A-1. Let X = Z DX Z and
Y = Z DY Z be two random elements in the span of zizi , where DX and DY are two random diagonal matrices. Both matrices X and Y can be diagonalized by matrix Z. In this case, if we compute XY -1 = Z DX DY-1(Z )-1, since zi is a column of Z , we know

XY -1zi = Z

DX DY-1(Z

)-1zi = Z

DX DY-1ei

=

DX (i, i) Z DY (i, i)

ei

=

DX DY

(i, (i,

i) i)

zi

.

That is, zi is an eigenvector of XY -1! The matrix XY -1 can have at most k eigenvectors and there are k zi's, therefore the zi's are the only eigenvectors of XY -1.

Lemma 5. Given the span of zizi 's, let 1 the zi's are the only eigenvectors of X

X, Y Y -1.

be

two

random

matrices

in

this

span,

with

probability

Using this procedure we can find all the zi's (up to permutations and sign flip). Without loss of generality we assume zi A = iei . The only remaining problem is that i might be negative. However, this is easily fixable by checking E[zi y] = iE[(wi x)]. Since (wi x) is always nonnegative, E[zi y] has the same sign as i, and we can flip zi if E[zi y] is negative.

3.3 DETAILED ALGORITHM AND GUARANTEES
We can now give the full algorithm, see Algorithm 2. The main steps of this algorithm is as explained in the previous section. Steps 2 - 5 constructs the pure neuron detector and finds the span of vec(zizi ) (as in Corollary 1); Steps 7 - 9 performs simultaneous diagonalization to get all the zi's; Steps 11, 12 calls Algorithm 1 to solve the single-layer problem and outputs the correct result.
We are now ready to state a formal version of Theorem 1: Theorem 4. Suppose A, W, E[xx ] and the distinguishing matrix N are all full rank, and Algorithm 2 has access to the exact moments, then the network returned by the algorithm computes exactly the same function as the original neural network.
It is easy to prove this theorem using the lemmas we have.

7

Under review as a conference paper at ICLR 2019

Algorithm 2 Learning Two-layer Neural Net

Input: Samples (x1, y1), ..., (xn, yn) generated according to Equation (2)

Output: Weight matrices W and A.

1: {Finding span of vec(zizi )'s} 2: Estimate empirical moments E^[xx ], E^[yx ], E^[y  x3] and E^[y  y  (x  x)]. 3: Compute the vector f (u) = 2E^ (u y)x E^ xx -1E^ (u y)x(x  x) - E^ (u y)2(x  x)

where each entry is expressed as a degree-2 polynomial over u.

4: Construct matrix T  Rd2×(k2+k) such that, T vec(uu ) = f (u). 5: Compute the k-least right singular vectors of T , denoted by vec(U1), vec(U2), · · · , vec(Uk).
Let S be a k2 + k by k matrix, where the i-th column of S is vector vec(Ui). {Here span S is equal to span of {vec(zizi )}.} 6: {Finding zi's from span} 7: Let X = mat(S1), Y = mat(S2), where 1 and 2 are two independently sampled k-

dimensional standard Gaussian vectors. 8: Let z1, ..., zk be eigenvectors of XY -1. 9: For each zi, if E^[zi y] < 0 let zi  -zi. {Here zi's are normalized rows of A-1.} 10: {Reduce to 1-Layer Problem}

11: For each zi, let vi be the output of Algorithm 1 with input (x1, zi y1), ..., (xn, zi yn).

12: 13:

Let Z be the matrix return V , Z-1.

whose

rows

are

zi

's, V

be the matrix whose rows are vi

's.

Proof. By Corollary 1, we know that after Step 5 of Algorithm 2, the span of columns of R is

exactly equal to the span of {vec Step 7 are exactly the normalized

(vzeirzsiio)n}o. fBryowLsemofmAa-51,.

we know Without

the loss

eigenvectors of X of generality, we

Y -1 at will fix

the permutation and assume zi A = iei . In Step 8, we use the fact that E[zi y] = iE[(wi x)]

where E[(wi x)] is always positive because  is ReLU function. Therefore, after Step 8 we can assume all the i's are positive.

Now the output zi y = the design of Algorithm

i(wi x) 1 we know

= vi

(iwi = iwi.

x) (again by property We also know that Z

of =

ReLU function ), by diag()A-1, therefore

Z-1 = Adiag()-1. Notice that Z-1(V x) = Adiag()-1(diag()W x) = A(W x). These

two scaling factors cancel each other, so the two networks compute the same function.

4 EXPERIMENTS
In this section, we provide some experimental results to validate the robustness of our algorithm for both Gaussian input distributions as well as more general symmetric distributions such as symmetric mixtures of Gaussians.
There are two important ways in which our implementation differs from our description in Section 3.3. First, our description of the simultaneous diagonalization step in our algorithm is mostly for simplicity of both stating and proving the algorithm. In practice we find it is more robust to draw 10k random samples from the subspace spanned by the last k right-singular vectors of T and compute the CP decomposition of all the samples (reshaped as matrices and stacked together as a tensor) via alternating least squares(Comon et al., 2009). As alternating least squares can also be unstable we repeat this step 10 times and select the best one. Second, once we have recovered and fixed A we use gradient descent to learn W , which compared to Algorithm 1 does a better job of ensuring the overall error will not explode even if there is significant error in recovering A. Crucially, these modifications are not necessary when the number of samples is large enough. For example, given 10,000 input samples drawn from a spherical Gaussian and A and W drawn as random 10 × 10 orthogonal matrices, our implementation of the original formulation of the algorithm was still able to recover both A and W with an average error of approximately 0.15 and achieve close to zero mean square error across 10 random trials.
8

Under review as a conference paper at ICLR 2019
Figure 2: Error in recovering W , A and outputs ("MSE") for different numbers of training samples. Each point is the result of averaging across five trials, where on the left W and A are both drawn as random 10 × 10 orthonormal matrices and on the right as 32 × 32 orthonormal matrices. The input distribution is a spherical Gaussian.
Figure 3: Error in recovering W , A and outputs ("MSE") for different amounts of label noise. Each point is the result of averaging across five trials with 10,000 training samples, where for each trial W and A are both drawn as 10 × 10 orthonormal matrices. The input distribution on the left is a spherical Gaussian and on the right a mixture of two Gaussians with one component based at the all-ones vector and the other component at its reflection.
4.1 SAMPLE EFFICIENCY First we show that our algorithm does not require a large number of samples when the matrices are not degenerate. In particular, we generate random orthonormal matrices A and W as the ground truth, and use our algorithm to try to learn the neural network. As illustrated by Figure 2, regardless of the size of W and A our algorithm is able to recover both weight matrices with negligible error so long as the number of samples is around 5x the number of parameters. To measure the error in recovering A and W , we first normalize the columns of A and rows of W for both our learned parameters and the ground truth, pair corresponding columns and rows together and then compute the squared distance between learned and ground truth parameters. In Figure 2 we also show the overall mean square error--averaged over all output units--achieved by our learned parameters.
4.2 ROBUSTNESS TO NOISE Figure 3 demonstrates the robustness of our algorithm to label noise  for Gaussian and symmetric mixture of Gaussians input distributions. In this experiment, we fix the size of both A and W to be 10 × 10 and again generate both parameters as random orthonormal matrices. The overall mean square error achieved by our algorithm grows almost perfectly in step with the amount of label noise, indicating that our algorithm recovers the globally optimal solution regardless of the choice of input distribution.
9

Under review as a conference paper at ICLR 2019
Figure 4: Error in recovering W , A and outputs ("MSE"), on the left for different levels of conditioning of W and on the right for A. Each point is the result of averaging across five trials with 20,000 training samples, where for each trial one parameter is drawn as a random orthonormal matrix while the other as described in Section 4.3. The input distribution is a mixture of Gaussians with two components, one based at the all-ones vector and the other at its reflection.
4.3 ROBUSTNESS TO CONDITION NUMBER We've already shown that our algorithm continues to perform well across a range of input distributions and even when A and W are high-dimensional. In all previous experiments however, we sampled A and W as random orthonormal matrices so as to control for their conditioning. In this experiment, we take the input distribution to be a random symmetric mixture of two Gaussians and vary the condition number of either A or W by sampling singular value decompositions U V such that U and V are random orthonormal matrices and ii = -i, where  is chosen based on the desired condition number. Figure 4 respectively demonstrate that the performance of our algorithm remains steady so long as A and W are reasonably well-conditioned before eventually fluctuating. Moreover, even with these fluctuations the algorithm still recovers A and W with sufficient accuracy to keep the overall mean square error low.
5 CONCLUSION
Optimizing the parameters of a neural network is a difficult problem, especially since the objective function depends on the input distribution which is often unknown and can be very complicated. In this paper, we design a new algorithm using method-of-moments and spectral techniques to avoid the complicated non-convex optimization for neural networks. Our algorithm can learn a network that is of similar complexity as the previous works, while allowing more general input distributions. There are still many open problems. Besides the obvious ones of extending our results to more general distributions and more complicated networks, we are also interested in the relations to optimization landscape for neural networks. In particular, our algorithm shows there is a way to find the global optimal network in polynomial time, does that imply anything about the optimization landscape of the standard objective functions for learning such a neural network, or does it imply there exists an alternative objective function that does not have any local minima? We hope this work can lead to new insights for optimizing a neural network.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773­2832, 2014.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pp. 584­592, 2014.
Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analysis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pp. 594­603. ACM, 2014.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
A Carbery and J Wright. Distributional and l^ q norm inequalities for polynomials over convex bodies in r^ n. Mathematical research letters, 8(3):233­248, 2001.
Pierre Comon, Xavier Luciani, and Andre´ LF De Almeida. Tensor decompositions, alternating least squares and other tales. Journal of Chemometrics: A Journal of the Chemometrics Society, 23 (7-8):393­405, 2009.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In Advances In Neural Information Processing Systems, pp. 2253­2261, 2016.
Lieven De Lathauwer, Josphine Castaing, and Jean-Franois Cardoso. Fourth-order cumulant-based blind identification of underdetermined mixtures. IEEE Transactions on Signal Processing, 55 (6):2965­2973, 2007.
Simon S Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks with overlaps. arXiv preprint arXiv:1805.07798, 2018.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017.
Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimensions. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp. 761­770. ACM, 2015.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017a.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017b.
Surbhi Goel and Adam Klivans. Learning depth-three neural networks in polynomial time. arXiv preprint arXiv:1709.06010, 2017.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. arXiv preprint arXiv:1611.10258, 2016.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches. arXiv preprint arXiv:1802.02547, 2018.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pp. 855­863, 2014.
11

Under review as a conference paper at ICLR 2019
Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-ofsquares. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pp. 438­446. IEEE, 2016.
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 62(12):1707­1739, 2009.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In International Conference on Machine Learning, 2018.
Mahdi Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information Processing Systems, pp. 2007­2017, 2017.
Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares problems. SIAM review, 19(4):634­662, 1977.
GW Stewart and JG Sun. Computer science and scientific computing. matrix perturbation theory, 1990.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389­434, 2012.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly learnable in polynomial time. In International Conference on Machine Learning, pp. 993­1001, 2016.
Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of fullyconnected neural networks. In Artificial Intelligence and Statistics, pp. 83­91, 2017.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
12

Under review as a conference paper at ICLR 2019

A DETAILS OF EXACT ANALYSIS
In this section, we first provide the missing proofs for the lemmas appeared in Section 3. Then we discuss how to handle the noise case (i.e. y = (W x) + ) and give the corresponding algorithm (Algorithm 3). At the end we also briefly discuss how to handle the case when the matrix A has more rows than columns (more outputs than hidden units).

A.1 MISSING PROOFS FOR SECTION 3

Single-layer: To get rid of the non-linearities like ReLU, we use the property of the symmetric distribution (similar to (Goel et al., 2018)). Here we provide a more general version (Lemma 6) instead of proving the specific Lemma 1. Note that Lemma 1 is the special case when a = w and p = q = 1 (here  does not affect the result since it has zero mean and is independent with x, thus E[yx] = E[(w x)x]).
Lemma 6. Suppose input x comes from a symmetric distribution, for any vector a  Rd and any non-negative integers p and q satisfying that p + q is an even number, we have

E

(a

x) pxq

1 = 2 E[(a

x)p xq ],

where the expectation is taken over the input distribution.

Proof. Since input x comes from a symmetric distribution, we know that E (a x) pxq E (-a x) p(-x)q . Thus, we have

E

(a x) pxq

1 =
2

E

(-a x) p(-x)q

+E

(a x) pxq

.

There are two cases to consider: p and q are both even numbers or both odd numbers.

=

1. For the case where p and q are even numbers, we have

1 2E

(-a x) p(-x)q

+E

(a x) pxq

1

= 2

E

(-a x) pxq

+E

(a x) pxq

1 =2E

(-a x) p + (a x) p xq .

If (a x)  0, we know (-a x) p + (a x) p = (a x)p + 0 = (a x)p. Otherwise, we have (-a x) p + (a x) p = 0 + (a x)p = (a x)p. Thus,

E

(a x) pxq

1 = 2E

(-a x) p + (a x) p xq

1 = 2 E[(a

x)p xq ].

2. For the other case where p and q are odd numbers, we have

1 2E

(-a x) p(-x)q

+E

(a x) pxq

1 =2E

- (-a x) p + (a x) p xq .

Similarly, if (a x)  0, we know - (-a x) p + (a x) p = -(-a x)p + 0 =

(a x)p. Otherwise, we have - (-a x) p + (a x) p = 0 + (a x)p = (a x)p.

Thus,

E

(a x) pxq

1 = 2E

- (-a x) p + (a x) p xq

1 = 2 E[(a

x)p xq ].

13

Under review as a conference paper at ICLR 2019

Pure neuron detector: The first step in our algorithm is to construct a pure neuron detector based on Lemma 2 and Lemma 3. We will provide proofs for these two lemmas here.

Proof of Lemma 2. This proof easily follows from Lemma 6. Setting a = w, p = 2 and q = 0 in

Lemma 6, we have

E[y^2]

=

1 2

w

E[xx

]w. Similarly, we also know E[y^x

]

=

1 2

w

E[xx

] and

E[y^x]

=

1 2

E[xx

]w. Thus, we have E[y^2] = 2E[y^x

]E[xx

]-1E[y^x].

Proof of Lemma 3. Here, we only prove the second equation, since the first equation is just a special

case of the second equation. First, we rewrite y^ =

k i=1

ci(wi

x)

=

u

y by letting u

A=c

.

Then we transform these two terms in the LHS as follows:

2E (u y)x

E xx

-1
E

(u

y)x(x  x)

=2E (u A(W x))x

E xx

-1
E

(u

A(W x))x(x  x)

1

=u 2

AW E xx

E xx

-1
E

x(u

AW x)(x  x)

1 = 2 E (u

AW x)2(x  x)

1 =2E

A

u, W x 2(x  x)

1 =
2

(A u)2i E (wi x)2(x  x) +

(A u)i(A u)jE (wi x)(wj x)(x  x)

1ik

1i<jk

1 =
2

(A uu A)iiE (wi x)2(x  x) +

(A uu A)ijE (wi x)(wj x)(x  x) .

1ik

1i<jk

(4)

where the second equality holds due to Lemma 6.

Now, let's look at the second term E (u y)2(x  x) .

E (u y)2(x  x) =E (u A(W x))2(x  x) =E A u, (W x) 2(x  x)

= (A u)i2E (wi x)2(x  x) + 2

(A u)i(A u)jE (wi x)(wj x)(x  x)

1ik

1i<jk

= (A uu A)iiE (wi x)2(x  x) + 2

(A uu A)ijE (wi x)(wj x)(x  x)

1ik

1i<jk

1 =
2

(A uu A)iiE (wi x)2(x  x) + 2

(A uu A)ijE (wi x)(wj x)(x  x) .

1ik

1i<jk

(5)

Now, we subtract (4) by (5) to obtain

2E (u y)x

E xx

-1
E

(u

y)x(x  x)

- E (u

y)2(x  x)

= (A uu A)ijE (wi x)(wj x)(x  x) - 2 (A uu A)ijE (wi x)(wj x)(x  x)

1i<jk

1i<jk

= (A uu A)ijE[ (wi x)(wj x)(x  x)1{wi xwj x  0}
1i<jk

(6)

= (A uu A)ijNij,

(7)

1i<jk

where (6) uses (8) of the following Lemma 7, and (7) uses the definition of distinguishing matrix N (Definition 1).

14

Under review as a conference paper at ICLR 2019

Lemma 7. Given input x coming from a symmetric distribution, for any vector a, b  Rd, we have

1 2 E (a

x)(b

x)

- E (a

x)(b

x)

1 = 2 E (a

x)(b

x)1{a

xb

x  0}

and

1 2 E (a

x)(b

x)(xx) -E (a

x)(b

x)(xx)

1 = 2 E (a

x)(b

x)(xx)1{a

xb

x  0} ,

(8)

where the expectation is taken over the input distribution.

Proof. Here we just prove the first identity, because the proof of the second one is almost identical.

First,

we

rewrite

1 2

E

(a

x)(b

x)

as follows

1 2 E (a

x)(b

x)

1 = 2 E (a

x)(b

x)1{a

xb

x  0}

1 + 2 E (a

x)(b

x)1{a

xb

x > 0} .

Thus,

we

only

need

to

show

that

1 2

E

(a

x)(b

x)1{a

xb

x > 0}

= E (a

x)(b x) . It's

clear that

E (a x)(b x) = E (a x)(b x)1{a xb x > 0} .

Since input x comes from a symmetric distribution, we have

E (a x)(b x)1{a xb x > 0} =E (-a x)(-b x)1{a xb x > 0}

1 =
2

E (a

x)(b

x)1{a

xb

x > 0}

+ E (-a x)(-b x)1{a xb x > 0}

1 =2E

(a x)(b

x) + (-a

x)(-b

x) 1{a xb

x > 0}

1 = 2 E (a

x)(b

x)1{a

xb

x > 0} .

When a xb x > 0, we know a x and b x are both positive or both negative. In either case, we know that (a x)(b x) + (-a x)(-b x) = (a x)(b x). Thus, we have

E (a x)(b x) =E (a x)(b x)1{a xb x > 0}

1 = 2 E (a

x)(b

x)1{a

xb

x > 0} ,

which finished our proof.

Finding span: Now, we find the span of {vec(zizi )}. First, we recall that f (u) =

2E (u y)x

E xx

-1
E

(u

y)x(x  x)

- E (u

y)2(x  x) . Then, according to (7), we have

f (u) = 2E (u y)x

E xx

-1
E

(u

y)x(xx) -E (u

y)2(xx)

=

(A uu A)ij Nij .

1i<jk

It is not hard to verify that u y is a pure neuron if and only if f (u) = 0. Note that f (u) = 0 is a

system of quadratic equations. So we linearize it by increasing the dimension (i.e., consider uiuj as

a single variable) similar to the FOOBI algorithm. Thus the number of variable is

k 2

+ k = k2 + k,

i.e.,

T



d2
R

×(k2

+k)

such

that

T vec(U )

=

f (U )

=

(A U A)ij Nij .

(9)

1i<jk

Now, we prove the Lemma 4 which shows the null space of T is exactly the span of {vec(zizi )}.

Proof of Lemma 4. We divide the proof to the following two cases:

1. For any vector vec(U ) belongs to the null space of T , we have T vec(U ) = 0. Note that the RHS of (9) equals to 0 if and only if A U A is a diagonal matrix since the distinguishing matrix N is full column rank and A U A is symmetric. Thus vec(U ) belongs to the span of {vec(zizi )} since U = Z DZ for some diagonal matrix D.

15

Under review as a conference paper at ICLR 2019
2. For any vector vec(U ) belonging to the span of {vec(zizi )}, U is a linear combination of zizi 's. Furthermore, T vec(U ) is a linear combination of T vec(zizi ). Note that A zi only has one non-zero entry due to the definition of zi, for any i  [k]. Thus all coefficients in the RHS of (9) are 0. We get T vec(U ) = 0.
Finding zi's: Now, we prove the final Lemma 5 which finds all zi's from the span of {vec(zizi )} by using simultaneous diagonalization. Given all zi's, this two-layer network can be reduced to a single-layer one. Then one can use Algorithm 1 to recover the first layer parameters wi's.
Proof of Lemma 5. As we discussed before this lemma, we have XY -1 = Z DX DY-1(Z )-1. According to the following Lemma 8 (i.e., all diagonal elements of DxDy-1 are non-zero and distinct), the matrix XY -1 have k eigenvectors and there are k zi's, therefore zi's are the only eigenvectors of XY -1. Lemma 8. With probability 1, all diagonal elements of DX and DY are non-zero and all diagonal elements of DX DY-1 are distinct, where X = Z DX Z and Y = Z DY Z are defined in Line 7 of Algorithm 2.
Proof. First, we know there exist diagonal matrices {Di : i  [k]} such that mat(vec(Ui)) = Z DiZ for all i  [k], where {vec(Ui) : i  [k]} are the k-least right singular vectors of T (see Line 5 of Algorithm 2). Then, let the vector di  Rk be the diagonal elements of Di, for all i  k. Let matrix Q  Rk×k be a matrix where its i-th column is di. Then DX = diag(Q1) and DY = diag(Q2), where 1 and 2 are two random k-dimensional standard Gaussian vectors (see Line 7 of Algorithm 2). Since {vec(Ui) : i  [k]} are singular vectors of T , we know d1, d2, · · · , dk are independent. Thus, Q has full rank and none of its rows are zero vectors. Let i-th row of Q be qi . Let's consider DX first. In order for i-th diagonal element of DX to be zero, we need qi 1 = 0. Since qi is not a zero vector, we know the solution space of qi1 = 0 is a lower-dimension manifold in Rk, which has zero measure. Since finite union of zero-measure sets still has measure zero, the event that zero valued elements exist in the diagonal of DX or DY happens with probability zero. If i-th and j-th diagonal elements of DX DY-1 have same value, we have qi 1(qi 2)-1 = qj 1(qj 2)-1. Again, we know the solution space is a lower-dimensional manifold in R2k space, with measure zero. Since finite union of zero-measure sets still has measure zero, the event that duplicated diagonal elements exist in DX DY-1 happens with probability zero.
A.2 NOISY CASE
Now, we discuss how to handle the noisy case (i.e. y = (W x) + ). The corresponding algorithm is described in Algorithm 3. Note that the noise  only affects the first two steps, i.e., pure neuron detector (Lemma 3) and finding span of vec(zizi ) (Lemma 4). It does not affect the last two steps, i.e., finding zi's from the span (Lemma 5) and learning the reduced single-layer network. Because Lemma 5 is independent of the model and Lemma 1 is linear wrt. noise , which has zero mean and is independent of input x. Many of the steps in Algorithm 3 are designed with the robustness of the algorithm in mind. For example, in step 5 for the exact case we just need to compute the null space of T . However if we use the empirical moments the null space might be perturbed so that it has small singular values. The separation of the input samples into two halves is also to avoid correlations between the steps, and is not necessary if we have the exact moments.
Modification for pure neuron detector: Recall that in the noiseless case, our pure neuron detector contains a term E[(u y)2(x  x)], which causes a noise square term in the noisy case. Here, we modify our pure neuron detector to cancel the extra noise square term. In the following lemma, we state our modified pure neuron detector in Equation 10, and give it a characterization.
16

Under review as a conference paper at ICLR 2019

Algorithm 3 Learning Two-layer Neural Net with Noise

Input: Samples (x1, y1), ..., (xn, yn) generated according to Equation (2)
Output: Weight matrices W and A. 1: {Finding span of vec(zizi )} 2: Use first half of samples (i.e. {(xi, yi)}ni=/12) to estimate empirical moments E^[xx ], E^[yx ],
E^[yy ], E^[y  x3] and E^[y  y  (x  x)]. 3: Compute the vector f (u) = 2E^ (u y)x E^ xx -1E^ (u y)x(xx) -E^ (u y)2(xx) +
E^ (u y)2 - 2E^ (u y)x E^ xx -1E^ (u y)x E^[x  x] where each entry is expressed as

a degree-2 polynomial over u.

4: Construct matrix T  Rd2×(k2+k) such that, T vec(uu ) = f (u). 5: Compute the k-least right singular vectors of T , denoted by vec(U1), vec(U2), · · · , vec(Uk).
Let S be a k2 + k by k matrix, where the i-th column of S is vector vec(Ui). {Here span S is equal to span of {vec(zizi )}.} 6: {Finding zi's from span} 7: Let X = mat(S1), Y = mat(S2), where 1 and 2 are two independently sampled k-

dimensional standard Gaussian vectors. 8: Let z1, ..., zk be eigenvectors of XY -1. 9: For each zi, use the second half of samples {(xi, yi)}ni=n/2+1 to estimate E^[zi y]. If E^[zi y] < 0
let zi  -zi. {Here zi's are normalized rows of A-1.} 10: {Reduce to 1-Layer Problem} 11: For each zi, let vi be the output of Algorithm 1 with input {(xj, zi yj)}nj=n/2+1.

12: 13:

Let Z be the matrix return V , Z-1.

whose

rows

are

zi

's, V

be the matrix whose rows are vi

's.

Lemma 9. Suppose y = A(W x) + , for any u  Rk, we have

f (u) := 2E (u y)x

E xx

-1
E

(u

y)x(x  x)

- E (u

y)2(x  x)

+ E (u y)2 - 2E (u y)x

E xx

-1
E

(u

y)x

E[x  x]

(10)

= (A uu A)ijNij -

(A uu A)ijmij E[x  x],

1i<jk

1i<jk

(11)

where Nij's are columns of the distinguishing matrix (Definition 1), and mij :=
E (wi x)(wj x)1{wi xwj x  0} .

We defer the proof of this lemma to the end of this section. Recall that the augmented distinguishing matrix M consists of the distinguishing matrix N plus column E[x  x]. Now, we need to assume the augmented distinguishing matrix M is full rank.

Modification for finding span: For Lemma 4, as we discussed above, here we assume the augmented distinguishing matrix M is full rank. The corresponding lemma is stated as follows (the proof is exactly the same as previous Lemma 4):
Lemma 10. Let T be the unique Rd2×(k2+k) matrix that satisfies T vec(uu ) = f (u) (where f (u) is defined as in Equation 10, suppose the augmented distinguishing matrix is full rank, then the nullspace of T is exactly the span of {vec(zizi )}.
Similar to Theorem 4, we provide the following theorem for the noise case. The proof is almost the same as Theorem 4 by using the noisy version lemmas (Lemmas 9 and 10) instead of previous Lemmas 3 and 4.
Theorem 5. Suppose E[xx ], A, W and the augmented distinguishing matrix M are all full rank, and Algorithm 3 has access to the exact moments, then the network returned by the algorithm computes exactly the same function as the original neural network.

Now, we only need to prove Lemma 9 to finish this noise case.

17

Under review as a conference paper at ICLR 2019

Proof of Lemma 9. Similar to (4) and (5), we deduce these three terms in RHS of (10) one by one as follows. For the first term, it is exactly the same as (4) since the expectation is linear wrt. . Thus, we have

2E (u y)x

E xx

-1
E

(u

y)x(x  x)

1 =
2

(A uu A)iiE (wi x)2(x  x) +

(A uu A)ijE (wi x)(wj x)(x  x) .

1ik

1i<jk

(12)

Now, let's look at the second term E (u y)2(x  x) which is slightly different from (5) due to the noise . Particularly, we add the third term to cancel this extra noise square term later.

E (u y)2(x  x)

=E (u (A(W x) + ))2(x  x)

=E (u A(W x))2(x  x) + E (u )2(x  x)

1 =
2

(A uu A)iiE (wi x)2(x  x) + 2

(A uu A)ijE (wi x)(wj x)(x  x)

1ik

1i<jk

(13)

+ E (u )2(x  x) ,

(14)

where (14) uses (5).

For the third term, we have

E (u y)2 - 2E (u y)x

E xx

-1
E

(u

y)x

=E (u

A(W x))2

+ E (u

)2

-

1 2

E

A

u, W x 2

=

(A

uu

A)iiE (wi x)2

-1 2

(A uu A)iiE (wi x)2

1ik

1ik

+ 2 (A uu A)ijE (wi x)(wj x) - (A uu A)ijE (wi x)(wj x)

1i<jk

1i<jk

+ E (u )2

= - (A uu A)ijE (wi x)(wj x)1{wi xwj x  0} + E (u )2
1i<jk

= - (A uu A)ijmij + E (u )2 ,
1i<jk

(15)

where the third equality holds due to Lemma 6 and Lemma 7, and (15) uses the definition of mij.

Finally, we combine these three terms (12­15) as follows:

f (u) = 2E (u y)x

E xx

-1
E

(u

y)x(x  x)

- E (u

y)2(x  x)

+ E (u y)2 - 2E (u y)x

E xx

-1
E

(u

y)x

E[x  x]

= (A uu A)ijE (wi x)(wj x)(x  x)
1i<jk
- 2 (A uu A)ijE (wi x)(wj x)(x  x) - E (u )2 E x  x
1i<jk

+-

(A uu A)ijmij + E (u )2 E x  x

1i<jk

= (A uu A)ijNij -
1i<jk
where (16) uses (8) (same as (6)).

(A uu A)ijmij E[x  x],
1i<jk

(16)

18

Under review as a conference paper at ICLR 2019

Algorithm 4 Learning Two-layer Neural Net with Non-square A
Input: Samples (x1, y1), ..., (xn, yn) generated according to Equation (2). Output: Weight matrices W  Rk×d and A l×k. 1: Using half samples (i.e. {(xi, yi)}in=/12) to estimate empirical moments E^[yx ]. 2: Let P be a l × k matrix, which columns are left singular vectors of E^[yx ]. 3: Run Algorithm 3 on samples {(xi, P yi)}ni=n/2. Let the output of Algorithm 3 be V, Z-1. 4: return V , P Z-1.

A.3 EXTENSION TO NON-SQUARE A
In this paper, for simplicity, we have assumed that the dimension of output equals the number of hidden units and thus A is a k × k square matrix. Actually, our algorithm can be easily extended to the case where the dimension of output is at least the number of hidden units. In this section, we give an algorithm for this general case, by reducing it to the case where A is square. The pseudo-code is given in Algorithm 4.
Theorem 6. Suppose E[xx ], W, A and the augmented distinguishing matrix M are all full rank, and Algorithm 4 has access to the exact moments, then the network returned by the algorithm computes exactly the same function as the original neural network.

Proof. Let the ground truth parameters be A  Rl×k and W  Rk×d. The samples are generated by

y = A(W x)+, where the noise  is independent with input x. We have E[yx

]

=

1 2

AW

E[xx

].

Since both W and E[xx ] are full-rank, we know the column span of E[yx ] are exactly the column

span of A. Furthermore, we know the columns of P is a set of orthonormal basis for the column

span of A.

For a ground truth neural network with weight matrices W and P A, the generated sample will just be (x, P y). According to Theorem 5, we know for any input x, we have Z-1(V x) = P A(W x). Thus, we have

P Z-1(V x) = P P A(W x) = A(W x),

where the second equality holds since P P is just the projection matrix to the column span of A.

B ROBUSTNESS OF MAIN ALGORITHM
In this section we will show that even if we do not have access to the exact moments, as long as the empirical moments are estimated with enough (polynomially many) samples, Algorithm 2 and Algorithm 3 can still learn the parameters robustly. We will focus on Algorithm 3 as it is more general, the result for Algorithm 2 can be viewed as a corollary when the noise  = 0. Throughout this section, we will use V^ , Z^-1 to denote the results of Algorithm 3 with empirical moments, and use V, Z-1 for the results when the algorithm has access to exact moments, similarly for other intermediate results. For the robustness of Algorithm 3, we prove the following theorem.
Theorem 7. Assume that the norms of x, , A are bounded by x  ,   P2 almost surely, A  P1, the covariance matrix and the weight matrix are robustly full rank: min(E[xx ])  , min(A)  . Further assume that the augmented distinguishing matrix has smallest singular values min(M )  . For any small enough , for any  < 1, given poly , P1, P2, d, 1/ , 1/, 1/, 1/, 1/ number of i.i.d. samples, let the output of Algorithm 3 be V^ , Z^-1, we know with probability at least 1 - ,
A(W x) - Z^-1(V^ x)  ,
for any input x with x  .
In order to prove the above Theorem, we need to show that each step of Algorithm 3 is robust. We can divide Algorithm 3 into three steps: finding the span of vec(zizi )'s; finding zi's from the span

19

Under review as a conference paper at ICLR 2019

of vec(zizi )'s; recovering first layer using Algorithm 1. We will first state the key lemmas that prove every step is robust to noise, and finally combine them to show our main theorem.

First, we show that with polynomial number of samples, we can approximate the span of
vec(zizi )'s in arbitrary accuracy. Let T^ be the empirical estimate of T , which is the pure neuron detector matrix as defined in Algorithm 3. As shown in Lemma 10, the null space of T is exactly the span of vec(zizi )'s. We use standard matrix perturbation theory (see Section D.2) to show that the null space of T is robust to small perturbations. More precisely, in Lemma 11, we
show that with polynomial number of samples, the span of k least singular vectors of T^ is close to

the null space of T .

Lemma 11. Under the same assumptions as in Theorem 7, let S  R(k2+k)×k be the matrix whose

k columns are the k least right singular vectors of T . Similarly define S^  R(k2+k)×k for empirical

estimate T^. Then for any



/2,

for

any



<

1,

given

O( d314(P1

k+P2 )6 4 2

log(

d 

)

)

number

of

i.i.d.

samples, we know with probability at least 1 - ,



SS - S^S^



2 2 .

The proof of the above lemma is in Section B.1. Basically, we need to lowerbound the spectral gap (k2-th singular value of T ) and to upperbound the Frobenius norm of T - T^. Standard matrix perturbation bound shows that if the perturbation is much smaller than the spectral gap, then the null
space is preserved.

Next, we show that we can robustly find zi's from the span of vec(zizi )'s. Since this step of the algorithm is the same as the simultaneous diagonalization algorithm for tensor decompositions, we

use the robustness of simultaneous diagonalization (Bhaskara et al., 2014) to show that we can find

zi's robustly. The detailed proof is in Section B.2.

Lemma 12. Suppose that SS - S^S^ F  , A  P1,   P2, min(E[xx ]) 

, min(A)  . Let X^ = mat(S^1), Y^ = mat(S^2), where 1 and 2 are two inde-

pendent standard Gaussian vectors. Let z1, · · · zk be the normalized row vectors of A-1. Let

z^1, ..., z^k be the eigenvectors of X^ Y^ -1 (after sign flip). For any  > 0 and small enough , with

O( (P1

k+P2 )2
2

log(d/)

)

number

of

i.i.d.

samples in E^[z^i

y], with probability at least 1 - 

over the

randomness of 1, 2 and i.i.d. samples, there exists a permutation (i)  [k] such that

z^i - z[i]  poly(d, P1, 1/, , 1/),

for any 1  i  k.

Finally, given z^i's, the problem reduces to a one-layer problem. We will first give an analysis for Algorithm 1 as a warm-up. When we call Algorithm 1 from Algorithm 3, the situation is slightly

different. Note we reserve fresh samples for this step, so that the samples used by Algorithm 1 are
still independent with the estimate z^i (learned using the other set of samples). However, since z^i is not equal to zi, this introduces an additional error term (z^i - zi) y which is not independent of x and cannot be captured by . We modify the proof for Algorithm 1 to show that the algorithm is still robust as long as z^i - zi is small enough.

Lemma 13. Assume that x  , A  P1,   P2 and min(E[xx ])  . Suppose that

for each 1  i  k, z^i - zi

  . Then for any



/2

and



<

1,

given

O(

(2

+P2 )4 4 2

log(

d 

)

)

number of samples for Algorithm 1, we know with probability at least 1 - ,



vi - v^i

 2 (P1 k + P2) + 2 , 

for each 1  i  k.

Combining the above three lemmas, we prove Theorem 7 in Section B.4.

B.1 ROBUST ANALYSIS FOR FINDING THE SPAN OF {VEC(zizj )}'S
We first prove that the step of finding the span of {vec(zizj )} is robust. The main idea is based on standard matrix perturbation bounds (see Section D.2). We first give a lowerbound on the k2-th

20

Under review as a conference paper at ICLR 2019

singular value of T , giving a spectral gap between the smallest non-zero singular value and the null space. See the lemma below. The proof is given in Section B.1.1.
Lemma 14. Suppose min(M )  , min(A)  , we know that matrix T has rank k2 and the k2-th singular value of T is lower bounded by 2.

Then we show that with enough samples the estimate T^ is close enough to T , so Wedin's Theorem (Lemma 25) implies the subspace found is also close to the true nullspace of T . The proof is deferred to Section B.1.2.

Lemma 15. Assume that x  , A  P1,   P2 and min(E[xx ])   > 0, then for any



/2,

for

any

1

>



>

0,

given

O( d314(P1

k+P2 )6 4 2

log(

d 

)

)

number

of

i.i.d.

samples,

we

know

T^ - T F  ,

with probability at least 1 - .

Finally we combine the above two lemmas and show that the span of the least k right singular vectors of T^ is close to the null space of T .

Lemma 11. Under the same assumptions as in Theorem 7, let S  R(k2+k)×k be the matrix whose

k columns are the k least right singular vectors of T . Similarly define S^  R(k2+k)×k for empirical

estimate T^. Then for any



/2,

for

any



<

1,

given

O( d314(P1

k+P2 )6 4 2

log(

d 

)

)

number

of

i.i.d.

samples, we know with probability at least 1 - ,

SS - S^S^





2 2 .

Proof.

According

to

Lemma

15,

given

O( )d3

14

(P1

 k+P2

)6

log(

d 

)

4 2

number

of

i.i.d.

samples, we

know with probability at least 1 - ,

T^ - T F  .

According to Lemma 14, we know k2 (T )  2. Then, due to Lemma 27, we have

SS - S^S^

 2

T

- T^

F

 k2 (T )

2  2 .

B.1.1 LOWERBOUNDING k2-TH SINGULAR VALUE OF T

T = M BCF  Rd2×(k2+k)

k2 + 1

k2

k2 k2 + k

d2 B  R(k2+1)×k2 C  Rk2×k2

F  Rk2×(k2+k)

M  Rd2×(k2+1)

recall that k2 :=

k 2

Figure 5: Characterize T as the product of four matrices.

In order to lowerbound the k2-th singular value of T , we first express T as the product of four simpler matrices, T = M BCF , as illustrated in Figure 5. The definitions of these four matrices

21

Under review as a conference paper at ICLR 2019

M  Rd2×(k2+1), B  R(k2+1)×k2 , C  R(k2×k2), F  Rk2×(k2+k) will be introduced later as we explain their effects. From Lemma 9, we know that

T vec(U ) =

(A U A)ij Mij -

(A U A)ijmij E[x  x],

1i<jk

1i<jk

for any symmetric k × k matrix U .
Note that vec(U ) is a (k2 + k)-dimensional vector. For convenience, we first use matrix F to transform vec(U ) to vec(U ), which has k2 dimensions. Matrix F is defined such that F vec(U ) = vec(U ), for any k × k symmetric matrix U . Note that this is very easy as we just need to duplicate all the non-diagonal entries.

Second, we hope to get the coefficients (A U A)ij's. Notice that vec(A U A) = A  A vec(U ) = A  A F vec(U ).

Since we only care about the elements of A U A at the ij-th position for 1  i < j  k, we just pick corresponding rows of A  A to construct our matrix C, which has dimension k2 × k2.
The first matrix M is the augmented distinguishing matrix (see Definition 1). In order to better understand the reason that we need matrix B, let's first re-write T vec(U ) in the following way:

T vec(U ) =

(A U A)ij Mij - mijE[x  x] .

1i<jk

Thus, T vec(U ) is just a linear combination of (Mij - mijE[x  x])'s with coefficients equal to (A U A)ij's. We have already expressed coefficients (A U A)ij's using CF vec(U ). Now, we just need to use matrix B to transform the augmented distinguishing matrix M to a d2 × k2 matrix, with each column equal to (Mij - mijE[x  x]). In order to achieve this, the first k2 rows of B is just
the identity matrix Ik2 , and the last row of B is [-m12, -m13, · · · , -m1k, -m23, -m24, · · · ] .

With above characterization of T , we are ready to show that the k2-th singular value of T is lower bounded.

Lemma 14. Suppose min(M )  , min(A)  , we know that matrix T has rank k2 and the k2-th singular value of T is lower bounded by 2.

Proof. Since matrix C has dimension k2 × k2, it's clear that the rank of T is at most k2. We first prove that the rank of T is exactly k2.

Since the first k2 rows of B constitute the identity matrix Ik2 , we know B is a full-column rank matrix with rank equal to k2. We also know that matrix M is a full column rank matrix with rank k2 + 1. Thus, the product matrix M B is still a full-column rank matrix with rank k2. If we can prove that the product matrix CF has full-row rank equal to k2. It's clear that T = M BCF also has rank k2. Next, we prove that CF has full-row rank.

Since min(A)  , we know A  A is full rank, and a subset of its rows C has full row rank. For the sake of contradiction, suppose that there exists non-zero vector a  Rk2 , such that

k2 l=1

al(C

F

)[l,:]

=

0.

Note that for any 1



l



k2, (CF )[l,ii]

=

C[l,ii] for 1



i



k and

(CF )[l,ij] = C[l,ij] + C[l,ji] for 1  i < j  k. Since C consists of a subset of rows of A  A ,

we know C[l,ij] = C[l,ji] for any l and any i < j. Thus,

k2 l=1

al

(C

F

)[l,:]

=

0

simply

implies

k2 l=1

alC[l,:]

=

0,

which

breaks

the

fact

that

C

is

full-row

rank.

Thus,

the

assumption

is

false

and

CF has full-row rank.

Now, let's prove that the k2-th singular value of T is lower bounded. We first show that in the product characterization of T , the smallest singular value of each individual matrix is lower bounded.
According to the assumption, we know the smallest singular value of M is lower bounded by . Since the first k2 rows of matrix B constitute a k2 × k2 identity matrix, we know

min(B) := min
u: u 1

Bu

 min
u: u 1

Ik2×k2 u

=: min(Ik2×k2 ) = 1,

where u is any k2-dimensional vector.

22

Under review as a conference paper at ICLR 2019

Since min(A)  , we know min(A  A )  2. According to the construction of C, we know C consists a subset of rows of A  A . Denote the indices of the row not picked as S. We have

min(C) := min u C
u: u 1

= min

v (A  A )

v: v 1  (vi=0,iS)

 min v (A  A )
v: v 1

=: min(A  A ) 2,

where u has dimension k2 and v has dimension k2.
We lowerbound the smallest singular value of CF by showing that min(CF )  min(C). For any unit vector u  Rk2 , we know [u CF ]ii = [u C]ii for any i and [u CF ]ij = [u C]ij + [u C]ji for any i < j. We also know [u C]ij = [u C]ji for i < j. Thus, we know for any unit vector u, u CF  u C , which implies min(CF )  min(C).
Finally, since in the beginning we have proved that matrix T has rank k2, the k2-th singular value is exactly the smallest non-zero singular value of T . Denote the smallest non-zero singular of T as m+in(T ), we have
k2 (T ) =m+in(T ) min(M )min(B)min(CF ) 2,

where the first inequality holds because both M and B has full column rank.

B.1.2 UPPERBOUNDING T^ - T F

In this section, we prove that given polynomial number of samples, T^ - T F is small with high probability. We do this by standard matrix concentration inequalities. Note that our requirements on the norm of x is just for convenience, and the same proof works as long as x has reasonable tail-behavior (e.g. sub-Gaussian).

Lemma 15. Assume that x  , A  P1,   P2 and min(E[xx ])   > 0, then for any



/2,

for

any

1

>



>

0,

given

O( d314(P1

k+P2 )6 4 2

log(

d 

)

)

number

of

i.i.d.

samples,

we

know

T^ - T F  ,

with probability at least 1 - .

Proof. In order to get an upper bound for T^ - T F , we first show that T^ - T 2 is upper bounded. We know

T - T^

= max

(T - T^)v

vRk2+k: v 1



max
U Rsky×mk : U

 F 2

(T - T^)vec(U ) .

23

Under review as a conference paper at ICLR 2019

For any k × k symmetric matrix U with eigenvalue decomposition U =

k i=1

i

u(i)(u(i))

, ac-

cording to the definition of T , we know

max
U Rksy×mk : U

 F 2

(T - T^)vec(U )

=

max
U Rksy×mk : U

 F 2

T vec(U ) - T^vec(U )



u(i) :

max
u(i) 

4 2

k
i(f (u(i)) - f^(u(i)))
i=1

k



u(i) :

max
u(i) 

4 2

i=1

|i|

f (u(i)) - f^(u(i)

k



|i|
i=1

u: mu ax4 2 f (u) - f^(u)

k  k i=1 2i u: mu ax4 2 f (u) - f^(u)
 = k U F u: mu ax4 2 f (u) - f^(u)
  2k max f (u) - f^(u)
u: u 2

where f^(u) = T^vec(uu ) and the fourth inequality uses the Cauchy-Schwarz inequality. Next, we only need to upper bound maxu: u 2 f (u) - f^(u) . Recall that

f (u) =2E (u y)x

E xx

-1
E

(u

y)x(x  x)

- E (u

y)2(x  x)

+ E (u y)2 - 2E (u y)x

E xx

-1
E

(u

y)x

E[x  x],

and f^(u) =2E^ (u y)x E^ xx -1E^ (u y)x(x  x) - E^ (u y)2(x  x) + E^ (u y)2 - 2E^ (u y)x E^ xx -1E^ (u y)x E^[x  x].

We first show that given polynomial number of samples,

2E^ (u y)x

E^ xx

-1E^ (u y)x(x  x) - 2E (u y)x

E xx

-1
E

(u

y)x(x  x)

 is upper bounded with high probability. Since each row of W has unit norm, we have W  k. Due to the assumption that x  , A  P1,   P2, we have

(u y)x  u A(W x) +  x

u(A W x + )x 
2(P1 k + P2).



According to Lemma 24, we know given O( 2(P1

k+P2 )2
2

log(

d 

)

)

number

of

samples,

E^ (u y)x - E (u y)x  ,

with probability at least 1 - .

Similarly,

we

can

show

that

given

O(

6

(P1

 k+P2

)2

2

log(

d 

)

)

number

of

samples,

E^ (u y)x(x  x) - E (u y)x(x  x)  ,

with probability at least 1 - .

24

Under review as a conference paper at ICLR 2019

Since xx



2,

we

know

that

given

O(

4

log(
2

d 

)

)

number

of

samples,

E^ xx - E xx  ,

with probability at least 1 - . Suppose that  /2  min(E[xx ])/2, we know E^[xx ] has full rank. According to Lemma 29, we have

E^ xx

-1 - E xx

-1

 2 2

E^ xx - E xx m2 in(E[xx ])

  2 2 /2,

with probability at least 1 - .



By union bound, we know for any

< /2, given O( 6(P1

k+P2 )2
2

log(

d 

)

)

number

of

samples,

with probability at least 1 - , we have

E^ (u y)x - E (u y)x  ,

E^ xx

-1 - E xx

-1

  2 2 /2,

E^ (u y)x(x  x) - E (u y)x(x  x)  .

Define

E1 := E^ (u y)x - E (u y)x , E2 := E^ xx -1 - E xx -1, E3 := E^ (u y)x(x  x) - E (u y)x(x  x) .

Then, we have

2E^ (u y)x

E^ xx

-1E^ (u y)x(x  x) - 2E (u y)x

E xx

-1
E

(u

y)x(x  x)

2 E1 E2 E3 + 2 E1 E xx -1 E (u y)x(x  x)

+ 2 E (u y)x E2 E (u y)x(x  x) + 2 E (u y)x E xx -1 E3

+ 2E1

E2

E

(u y)x(x 



x)

+2 

E

(u

y)x 

E2 E3 + 2E1 E xx -1 E3

2

22 2 

3

+

23(P1 

k

+

P2) + 

8

24(P1 k + P2)2  2

+ 2(P1 k + P2) 

4 +

23(P1  2

k

+ P2)

2

+

4

2(P1 k + P2) 2

2

+

2


=O( 4(P1

k + P2)2 2

).

Thus,

given

O(

14

(P1

 k+P2

)6

4 2

log(

d 

)

)

number

of

samples,

we

know

2E^ (u y)x

E^ xx

-1E^ (u y)x(x  x) - 2E (u y)x

E xx

-1
E

(u

y)x(x  x)

,

with probability at least 1 - .

Now, let's consider the second term

E^ (u y)2(x  x) - E (u y)2(x  x) .



Since (u y)2(x  x)


 42(P1 k + P2)2, according to Lemma 24, we know given

O( 4(P1

k+P2 )4
2

log(

d 

)

),

E^ (u y)2(x  x) - E (u y)2(x  x)  ,

25

Under review as a conference paper at ICLR 2019

with probability at least 1 - . Next, let's look at the third term

E^ (u y)2 E^[x  x] - E (u y)2 E[x  x] .



Again, using Lemma 24 and union bound, we know given O( 2(P1

)k+P2 )2

log(

d 

)

2

number

of

samples, we have

E^ (u y)2 - E (u y)2  , E^[x  x] - E[x  x]  .

Thus, Define Then, we have

E4 := E^ (u y)2 - E (u y)2 E5 := E^[x  x] - E[x  x].

E^ (u

y)2 E^[x  x] - E (u

y)2 E[x  x]

 E4 E5 + E4 E[x  x]





2

+

2 + 

4(P1

k + P2)2

=O((P1 k + P2)2 ).

+

Thus,

we

know

that

given

O(

2

(P1

 k+P2

)6

2

log(

d 

)

)

number

of

samples,

we

know

E (u

y)2

E^ (u y)2 E^[x  x] - E (u y)2 E[x  x]  ,

E5

with probability at least 1 - .

Now, let's bound the last term,

2 E^ (u y)x

E^ xx

-1E^ (u y)x

E^[xx]-2 E (u y)x

E xx

-1
E

(u

y)x

E[xx] .



Similar as the first term, we can show that given O( 10(P1

k+P2 )6 4 2

log(

d 

)

)

number

of

samples,

we

have

2 E^ (u y)x

E^ xx

-1E^ (u y)x

E^[xx]-2 E (u y)x

E xx

-1
E

(u

y)x

E[xx] 

with probability at least 1 - .

Now, we are ready to combine our bound for each of four terms. By union bound, we know given

O( 14(P1

k+P2 )6 4 2

log(

d 

)

)

number

of

samples,

2E^ (u y)x

E^ xx

-1E^ (u y)x(x  x) - 2E (u y)x

E xx

-1
E

(u

y)x(x  x)



E^ (u y)2(x  x) - E (u y)2(x  x)  ,

E^ (u y)2 E^[x  x] - E (u y)2 E[x  x]  ,

2 E^ (u y)x

E^ xx

-1E^ (u y)x

E^[x  x] - 2 E (u y)x

E xx

-1
E

(u

y)x

E[x  x]  ,

hold with probability at least 1 - . Thus, we know max f (u) - f^(u)  + + + = 4
u: u 2
with probability at least 1 - .

26

Under review as a conference paper at ICLR 2019

Recall that

T^ - T F  k2 + k T^ - T k 2k max f (u) - f^(u) ,
u: u 2

where the second inequality holds since

T^ - T

  2k maxu: u 2

f (u) - f^(u) .



Thus, we know given O( 14(P1

k+P2 )6 4 2

log(

d 

)

)

number

of

samples,

 T^ - T F  k 2k  d 2d

with

probability

at

least

1

-

.

Thus,

given

O( d3

14

(P1

 k+P2

)6

4 2

log(

d 

)

)

number

of

samples,

T^ - T F 

with probability at least 1 - .

B.2 ROBUST ANALYSIS FOR SIMULTANEOUS DIAGONALIZATION

In this section, we will show that the simultaneous diagonalization step in our algorithm is robust. Let S and S^ be two (k2 + k) by k matrices, whose columns consist of the least k right singular vectors of T and T^ respectively.
According to Lemma 11, we know with polynomial number of samples, the Frobenius norm of SS - S^S^ is well bounded. However, due to the rotation issue of subspace basis, we cannot conclude that S - S^ F is small. Only after appropriate alignment, the difference between S and S^ becomes small.
Lemma 16. Let S and S^ be two (k2 + k) by k matrices, whose columns consist of the least k right singular vectors of T and T^ respectively. If SS - S^S^ F  , there exists an rotation matrix R  Rk×k satisfying RR = R R = Ik, such that
S^ - SR F  2 .

Proof. Since S has orthonormal columns, we have k(SS ) = 1. Then, according to Lemma 35, we know there exists rotation matrix R such that

S^ - SR F 

SS - S^S^ F 

2 .

2( 2 - 1) k(SS )

Let the k columns of S be vec(U1), vec(U2), · · · , vec(Uk). Note each Ui can be expressed as A- DiA-1, where Di is a diagonal matrix. Let Q be a k × k matrix, whose i-th column consists of the diagonal elements of Di, such that Qij equals the j-th diagonal element of Di. Let vec(X) = SR1, vec(Y ) = SR2, where R is the rotation matrix in Lemma 16 and 1, 2 are two independent standard Gaussian vectors. Let DX = diag(QR1) and DY = diag(QR2). It's not hard to check that X = A- DX A-1 and Y = A- DY A-1. Furthermore, we have XY -1 = A- DX DY-1A . Next, we show that the diagonal elements of DX DY-1 are well separated. Lemma 17. Assume that A  P1, min(A)  . Then for any  > 0 we know with probability at least 1 - , we have
sep(DX DY-1)  poly(1/d, 1/P1, , ), where sep(DX DY-1) := mini=j |(DX DY-1)ii - (DX DY-1)jj |.
Proof. We first show that matrix Q is well-conditioned. Since Ui = A- DiA-1, we have vec(Ui) = A- A- vec(Di). Let U be a k2 ×k matrix whose columns consist of vec(Ui)'s. Also define Q¯ as a k2 × k matrix whose columns are vec(Di)'s. Note that matrix Q¯ only has k non-zero
27

Under review as a conference paper at ICLR 2019

rows, which are exactly matrix Q. With the above definition, we have U = A-  A- Q¯. Since min(U )  A-  A- min(Q¯), we have

min(Q¯) 

min(U ) A-  A-

.

Notice that a subset of rows of U constitute matrix S, which is an orthonormal matrix. Thus, we have min(U )  min(S) = 1. Since we assume min(A)  , we have

A-  A-

= A-

2

=

1 min(A)2



1 2 .

Thus, we have min(Q¯)  2, which implies min(Q)  2.

We also know U  min(A-  A- ) Q¯ , thus

Q¯

U  min(A-  A-

. )

 Since S = 1, we know U  2. For the smallest singular value of A-  A- , we have

min(A-

 A- ) = m2 in(A-1) =

1 A

2



1 P12 .

Thus, we have

Q¯

  2P12, which implies

Q

  2P12.

Now, let's prove that the diagonal elements of DX DY-1 are well-separated.

vector of Q. Then we know the i-th diagonal element of DX DY-1 is

qi ,R1 qi ,R2

Let qi . Since

be the i-th row Q  2P12,

we have qi  2P12 for every row vector.

It's not hard to show that with probability at least 1 - exp(-d(1)), we have | qi, R2 | 

poly(d, P1) for each i.

Now given 2 for which this happens, we have

qi ,R1 qi ,R2

-

qj ,R1 qj ,R2

=

ci qi, R1 - cj qj, R1 , where ci, cj have magnitude as least poly(1/d, 1/P1). Since min(Q) 

2, we know Projqj qi  2 (because otherwise there exists  such that (ei + ej) min(Q) 

(ei + ej) Q = Projqj qi < 2, which is a contradiction). Let qi,j = Projqj qi = qi - i,jqj,

we can rewrite this as

qi, R1 qi, R2

-

qj , R1 qj , R2

= ci qi, R1

- cj qj , R1

= ci qi,j , R1

- (cj + i,j ci) qj , R1 .

By properties of Gaussians, we know qi,j, R1 is independent of qj, R1 , so we can first fix qj, R1 and apply anti-concentration of Gaussians (see Lemma 38) to qi,j, R1 . As a result we know with probability at least 1 - /k2:

qi, R1 - qj, R1

qi, R2

qj , R2

 poly(1/d, 1/P1, , ).

By union bound, we know with probability at least 1 - , sep(DX DY-1)  poly(1/d, 1/P1, , ).

Let X^ = S^1 and Y^ = S^2. Next, we prove that the eigenvectors of X^ Y^ -1 are close to the eigenvectors of XY -1. Lemma 12. Suppose that SS - S^S^ F  , A  P1,   P2, min(E[xx ])  , min(A)  . Let X^ = mat(S^1), Y^ = mat(S^2), where 1 and 2 are two independent standard Gaussian vectors. Let z1, · · · zk be the normalized row vectors of A-1. Let z^1, ..., z^k be the eigenvectors of X^ Y^ -1 (after sign flip). For any  > 0 and small enough , with
28

Under review as a conference paper at ICLR 2019



O( (P1

k+P2 )2
2

log(d/)

)

number

of

i.i.d.

samples in E^[z^i

y], with probability at least 1 - 

over the

randomness of 1, 2 and i.i.d. samples, there exists a permutation (i)  [k] such that

z^i - z[i]  poly(d, P1, 1/, , 1/),

for any 1  i  k.

Proof. Let z1, · · · , zk be the eigenvectors of XY -1 (before sign flip step). Similarly define

z^1, · · · of XY

-, z^1k.

for

X^ Y^ -1.

We

first

prove

that

the

eigenvectors

of

X^ Y^ -1

are

close

to

the

eigenvectors

Let X^ = X + EX and Y^ = Y + EY . Then we have

X^ Y^ -1 = XY -1(I + F ) + G,

where F = -EY (I + Y -1EY )-1Y -1 and G = EX Y^ -1. According to Lemma 34, we have

F

 EY
min(Y )- EY

and G



.EX
min(Y^ )

In order

to bound the perturbation matrices

F

and

G , we need to first bound EX , EY and min(Y ), min(Y^ ).

As we know, EX = X^ - X = (S^ - SR)1. According to Lemma 16, we have S^ - SR  S^ - SR F  2 . We also know with probability at least 1 - exp(-d(1)), 1  poly(d). Thus,
we have

EX = (S^ - SR)1  S^ - SR 1  poly( , d).

Similarly, with probability at least 1 - exp(-d(1)), we also know EY  poly( , d).

Now, we lower bound the smallest singular value of X and Y . Since X = A- DX A-1, we have

min(X) m2 in(A-1)min(DX ) 1
= A 2 min(DX )



1 P12

min(DX

).

Since DX is a diagonal matrix, its smallest singular value equals the smallest absolute value of
its diagonal element. Recall each diagonal element of DX is qi, R1 , which follows a Gaussian distribution whose standard deviation is at least qi  2. By anti-concentration property of Gaussian (see Lemma 38), we know | qi, R2 |  (2/k) for all i with probability 1 - /4. Thus, we have min(X)  poly(1/d, 1/P1, , ). Similarly we have the same conclusion for Y . For small enough EY , we have min(Y^ )  min(Y ) - EY .

Thus, for small enough , we have F  poly(d, P1, 1/, , 1/) and G 

poly(d, P1, 1/, , 1/). In order to apply Lemma 33, we also need to bound (A- ) and XY -1 .

Since min(A-

)



1 P1

and

A-

 1/, we have (A- )  P1/. For the norm of XY -1,

we have

XY -1

X  min(Y )

X poly(d, P1, 1/, 1/),

where the second inequality holds because min(Y )  poly(1/d, 1/P1, , ). Recall that X = mat(SR1). It's not hard to verify that with probability at least 1 - exp(-d(1)), we have X  poly(d). Thus, we know XY -1  poly(d, P1, 1/, 1/). Similarly, we can also prove that DX DY-1  poly(d, P1, 1/, 1/)

Overall, we have

(A- )( XY -1F + G ) (A- )( XY -1 F + G )

 P1 

poly(d, P1, 1/, 1/)poly(d, P1, 1/, , 1/) + poly(d, P1, 1/, , 1/)

poly(d, P1, 1/, , 1/).

29

Under review as a conference paper at ICLR 2019

According to Lemma 17, we know with probability at least 1 - /4, sep(DX DY-1)  poly(1/d, 1/P1, , ). Thus, by union bound, we know for small enough , with probability at least 1 - ,
(A- )( XY -1F + G ) < sep(DX DY-1)/(2k).
According to Lemma 33, we know there exists a permutation [i]  [k], such that

z^i - zi

3

F DX min(A-

DY-1 + )sep(DX

G DY-1

)

3 poly(d, P1, 1/, , 1/)poly(d, P1, 1/, 1/) + poly(d, P1, 1/, , 1/) 1/P1poly(1/d, 1/P1, , 1/)

poly(d, P1, 1/, , 1/).

with probability at least 1 - .

According to Lemma 5, the eigenvectors of XY -1 (after sign flip) are exactly the normalized rows

of A-1 (up to permutation). Now the only issue is the sign of z^i. By the robustness of sign flip

step (see Lemma 18), we know for small enough

, with O( (P1

k+P2 )2
2

log(d/) )

number

of

i.i.d.

samples in E^[z^i y], with probability at least 1 - , the sign flip of z^i is consistent with the sign flip

of zi.

In the following lemma, we show that the sign flip step of z^i is robust.

Lemma 18. Suppose that x  , A  P1,   P2. Let z1, · · · , zk be the eigenvectors of

XY -1 (before sign flip step). Similarly define z^1, · · ·

, where 

 . We know, for any 

4(1+P1 k+P2)

,<z^k1f,owr iXt^hY^O-(1(.SPu1ppko+sPe22f)o2rloega(dc/hi),)

zi -z^i number

 of

i.i.d. samples,

Pr sign(E[ zi, y ]) = sign(E^[ z^i, y ])  .

Prorwoosfa. rWe {ezfii}rs.tWshiothwoutht alot sEs[ozfig, yen]eriaslibtoy,unasdseudmaewtahyatfrZom=zedrioa.gL(±etZ)A-be1

ak×k . Since

matrix, whose A-1  1/,

we know i  , for each i. Thus, we have

|E[ zi, y ]| = iE[(wi x)]  E[(wi x)].

In order to lowerbound E[(wi x)], let's first look at E[(wi x)x ].

1

E[(wi x)x

]

= 2

E[wi xx

]

1 2

wi

min(E[xx

])

.

2

Now, let's connect E[(wi x)x ] with E[(wi x)].

E[(wi x)x ] E[ (wi x)x ] =E[(wi x) x ] E[(wi x)].

Thus,

we

have

E[(wi

x)]



 2

,

and

further

|E[

zi, y

]|



 2

.

Now, let's bound E[ zi, y ] - E^[ z^i, y ] .

E[ zi, y ] - E^[ z^i, y ] = E[ zi, y ] - E[ z^i, y ] + E[ z^i, y ] - E^[ z^i, y ]  E[ zi - z^i, y ] + E[ z^i, y ] - E^[ z^i, y ] (P1 k + P2) + E[ z^i, y ] - E^[ z^i, y ] .

30

Under review as a conference paper at ICLR 2019

Note that we Since z^i, y

reserve fresh  P1 k

samples for this step, thus z^i isindependent with samples in E^[ z^i, y

+ P2, we know with O( (P1

k+P2 )2
2

log(d/) )

number

of

samples,

]]. we

have

E[ z^i, y ] - E^[ z^i, y ]]  .

Overall, we have

E[ zi, y ] - E^[ z^i, y ]]

  (1 + P1 k + P2) . Combined with the fact that

|E[

zi, y

]|



 2

,

we

know

as

long

as

  , with O( (P1
4(1+P1 k+P2)

k+P2 )2
2

log(d/)

)

number

of samples, we have

Pr[sign(E[ zi, y ]) = sign(E^[ z^i, y ])]  .

B.3 ROBUST ANALYSIS FOR RECOVERING FIRST LAYER WEIGHTS

We will first show that Algorithm 1 is robust.

Theorem 8. Assume that x  , w  1, ||  P2 and min(E[xx ])  . Then for any



/2,

for

any

1

>



>

0,

given

O(

(2

+P2 )4 4 2

log(

d 

)

)

number

of

i.i.d.

samples,

we

know

with

probability at least 1 - ,

w^ - w  ,

where w^ is the learned weight vector.

Proof. We first show that given polynomial number of i.i.d. samples, E^[yx] - E[yx] is upper bounded with high probability, where E^[yx] is the empirical estimate of E[yx]. Due to the assumption that x  , w  1,   P2, we have

yx = ((w x) + )x
 w xx + x  w x 2 + || x 2 + P2

According

to

Lemma

24,

we

know

given

O(

(2 +P2 )2
2

log(

d 

)

)

number

of

samples,

with

probability

at least 1 - , we have E^[yx] - E[yx]  .

Since xx



2,

we

know

that

given

O(

4

log(
2

d 

)

)

number

of

samples,

E^ [xx

] - E[xx

]



with probability at least 1 - . Suppose that  /2  min(E[xx ])/2, we know E^[xx ] has full

rank. According to Lemma 29, we have

E^ xx

-1 - E xx

-1

 2 2

E^ xx - E xx m2 in(E[xx ])

  2 2 /2,

with probability at least 1 - .

By union bound, we know for any



/2,

given

O(

(2

+P2

)2
2

log(

d 

)

)

number

of

samples,

with

probability at least 1 - , we have

E^ [xx

]-1 - E[xx

]-1

  2 2 /2

E^[yx] - E[yx] 

Denote

E1 :=E^[xx ]-1 - E[xx ]-1 E2 :=E^[yx] - E[yx].

31

Under review as a conference paper at ICLR 2019

Then, we have

w^ - w

=2 E^[xx ]-1E^[yx] - E[xx ]-1E[yx]

2 E1

E2

+2 

E1

E[yx] + 2 E[xx

4

2 2

2

+

4

2(2 + P2) 2

2 +


O(

(2

+ P2) 2

).

]-1

E2

Thus,

given

O(

(2 +P2 )4 4 2

log(

d 

)

)

number

of

samples,

with

probability

at

least

1

-

,

we

have

w^ - w  .

Now let's go back to the call to Algorithm 1 in Algorithm 3. Let zi's be the normalized rows of A-1, and let z^i's be the eigenvectors of X^ Y^ -1 (with correct sign). From Lemma 12, we know {z^i}
are close to {zi} with permutation. Without loss of generality, we assume the permutation here is
just an identity mapping, which means zi - z^i is small for each i.

For each zi, let vi be the output of Algorithm 1 given infinite number of inputs (x, zi y). For each z^i, let v^i be the output of Algorithm 1 given only finite number of samples (x, z^i y). In this section, we show that suppose zi - z^i is bounded, with polynomial number of samples, vi - v^i is also bounded.

The input for Algorithm 1 is (x, z^i y). We view z^i y as the summation of zi y and a noise term (z^i - zi) y. Here, the issue is that the noise term (z^i - zi) y is not independent with the sample (x, z^i y), which makes the robust analysis in Theorem 8 not applicable. On the other hand, since we reserve a separate set of samples for Algorithm 1, the estimate z^i is independent with the samples
(x, y)'s used by Algorithm 1. Thus, the samples (x, z^i y)'s here are still i.i.d., which enables us to use matrix concentration bounds to show the robustness here.

Lemma 13. Assume that x  , A  P1,   P2 and min(E[xx ])  . Suppose that

for each 1  i  k, z^i - zi

  . Then for any



/2

and



<

1,

given

O(

(2

+P2 )4 4 2

log(

d 

)

)

number of samples for Algorithm 1, we know with probability at least 1 - ,



vi - v^i

 2 (P1 k + P2) + 2 , 

for each 1  i  k.

Proof. For any i, let's bound vi - v^i . As we know, vi = 2E[xx ]-1E[zi yx] and v^i = 2E^[xx ]-1E^[z^i yx]. Thus, in order to bound vi - v^i , we only need to show
E[xx ]-1E[z^i yx] - E[xx ]-1E[zi yx] and E[xx ]-1E[z^i yx] - E^[xx ]-1E^[z^i yx] are both bounded.

The first term can be bounded as follows.

E[xx

]-1E[z^i yx] - E[xx

]-1E[zi yx]

= E[xx ]-1E[(z^i - zi) yx]

 E[xx ]-1 

z^i - zi

  (P1 k + P2)



A(W x) + 

x

We can use standard matrix concentration bounds to upper bound the second term. By similar anal-

ysis

of

Theorem

1,

we

know

given

O(

(2

+P2 )4 4 2

log(

d 

)

)

number

of

i.i.d.

samples,

with

probability

at least 1 - ,

E[xx ]-1E[z^i yx] - E^[xx ]-1E^[z^i yx]  .

32

Under review as a conference paper at ICLR 2019

Overall, we have vi - v^i = 2E[xx ]-1E[zi yx] - 2E^[xx ]-1E^[z^i yx]
2 E[xx ]-1E[z^i yx] - E[xx ]-1E[zi yx] + 2 E[xx ]-1E[z^i yx] - E^[xx ]-1E^[z^i yx]  2 (P1 k + P2) + 2 .


By

union

bound,

we

know

given

O(

(2

+P2 )4 4 2

log(

d 

)

)

number

of

i.i.d.

samples,

with

probability

at

least 1 - ,



vi - v^i

 2 (P1 k + P2) + 2 . 

for any 1  i  k.

B.4 PROOF OF THEOREM 7

Proof of Theorem 7. Combining Lemma 11, Lemma 12 and Lemma 13, we know given poly , P1, P2, d, 1/ , 1/, 1/, 1/, 1/ number of i.i.d. samples, with probability at least 1 - ,

for each 1  i  k.

zi - z^i  , vi - v^i 

Let for

V be any i,

a k × d matrix whose rows we know every row vector

are vi's. Similarly define matrix V^ for v^i's. Since vi of V - V^ has norm at most , which implies V - V^

-

v^i 

 k

.

Let Z be Z -Z^

a 

k× k

k .

matrix whose rows are zi's. In order to show Z-1 - Z^-1

Similarly is small

define matrix Z^ for z^i's. Again, we have using standard matrix perturbation bounds

(Lemma 29), we need to lower bound min(Z). Notice that Z is just matrix A-1 with normalized

row vectors. As we know, min(A-1)  1/P1, and A-1  1/, which implies that every row

vector of A-1 has norm at most 1/. Let Dz be the diagonal matrix whose i, i-th entry is the norm

of i-th row of A-1, then Z = Dz-1A-1, and we know min(Z)  min(Dz-1)min(A-1)  /P1.

Then, according to Lemma 29, as long as



 2P1

,

we

have

Z-1 - Z^-1



 22

Z - Z^ m2 in(Z)

  2 2P12 k

/2.

Define

E1 :=Z-1 - Z^-1



E2

:=V

- V^ . 

We know E1  2 2P12 k /2 and E2  k . In order to bound V , we can bound the

norm of its row vectors. We have,

vi = 2E[xx ]-1E[zi yx]  2(P1 k + P2) . 

Now we can bound Z-1(V x) - Z^-1(V^ x) as follows.

Z-1(V x) - Z^-1(V^ x)

 E1

E2x

+

E1

Vx

+

Z -1 

E2x

2

2P12k 2

2

+

4

2P122(P1 k + P2) 2

k



+ P1

k ,



where the first inequality holds since (V x)  V x and (V^ x) - (V x)  V^ x - V x .

Thus, we know given poly , P1, P2, d, 1/ , 1/, 1/, 1/, 1/ number of i.i.d. samples, with probability at least 1 - ,
A(W x) - Z^-1(V^ x) = Z-1(V x) - Z^-1(V^ x)  ,
where the first equality holds because A(W x) = Z-1(V x), as shown in Theorem 5.

33

Under review as a conference paper at ICLR 2019

C SMOOTHED ANALYSIS FOR DISTINGUISHING MATRICES

In smoothed analysis, it's clear that after adding small Gaussian perturbations, matrix A and W
will become robustly full rank with reasonable probability (Lemma 36). In this section, we will
focus on the tricky part, using smoothed analysis framework to show that it is natural to assume
the distinguishing matrix is robustly full rank. We will consider two settings. In the first case, the input distribution is the Gaussian distribution N (0, Id), and the weights for the first layer matrix W is perturbed by a small Gaussian noise. In this case we show that the augmented distinguishing
matrix M has smallest singular value min(M ) that depends polynomially on the dimension and the amount of perturbation. This shows that for the Gaussian input distribution, min(M ) is lower bounded as long as W is in general position. In the second case, we will fix a full rank weight matrix W , and consider an arbitrary symmetric input distribution D. There is no standard way of perturbing a symmetric distribution, we give a simple perturbation D that can be arbitrarily close to D, and prove that min(M D ) is lowerbounded.

Perturbing W for Gaussian Input We first consider the case when the input follows standard Gaussian distribution N (0, Id). The weight matrix W is perturbed to W where

W = W + E.

(17)

Here E  Rk×d is a random matrix whose entries are i.i.d. standard Gaussians. We will use M to denote the perturbed version of the augmented distinguishing matrix M . Recall that the columns of

M has the form:

Mij = E (wi x)(wj x)(x  x)1{wi xwj x  0} ,

where wi is the i-th row of W . Also, since M is the augmented distinguishing matrix it has a final column M0 = vec(Id). We show that the smallest singular value of M is lower bounded with high probability.
Theorem 9. Suppose that k  d/5, and the input follows standard Gaussian distribution N (0, Id). Given any weight matrix W with wi   for each row vector, let W be a perturbed version
of W according to Equation (17) and M be the perturbed augmented distinguishing matrix. With probability at least 1 - exp(-d(1)), we have

min(M )  poly(1/, 1/d, ).

We will prove this Theorem in Section C.1.

Perturbing the Input Distribution Our algorithm works for a general symmetric input distribution D. However, we cannot hope to get a result like Theorem 9 for every symmetric input distribution D. As a simple example, if D is just concentrated on 0, then we do not get any information
about weights and the problem is highly degenerate. Therefore, we must specify a way to perturb
the input distribution.

We define a perturbation that is parametrized by a random Gaussian matrix Q and a parameter
  (0, 1). The random matrix Q is used to generate a Gaussian distribution DQ with a random covariance matrix. To sample a point in DQ, first sample n  N (0, Id), and then output Qn. The (Q, ) perturbation of a distribution D, which we denote by DQ, is a mixture between the distribution D and the distribution DQ. More precisely, to sample x from DQ,, pick z as a Bernoulli random variable where Pr[z = 1] =  and Pr[z = 0] = 1 - ; pick x according to D and pick
x = Qn according to distribution DQ, then let

x=

x x

z=0 z=1

Intuitively, the (Q, ) perturbation of a distribution D mixes the distribution D with a Gaussian
distribution DQ with covariance matrix QQ . Since both D and DQ are symmetric, their mixture is also symmetric. Also, the TV-distance between D and DQ, is bounded by . Throughout this section we will use D to denote the perturbed distribution DQ,

34

Under review as a conference paper at ICLR 2019

We show that given any input distribution, after applying (Q, )-perturbation with a random Gaussian matrix Q, the smallest singular value of the augmented distinguishing matrix M D is lower bounded. Recall that M D is defined as

MiDj = ExD (wi x)(wj x)(x  x)1{wi xwj x  0} ,

as the first

k 2

columns and has ExD [x  x] as the last column.

Theorem 10. Given weight matrix W with wi   for each row vector and symmetric input distribution D. Suppose that k  d/7 and min(W )  , after applying (Q, )-perturbations to yield perturbed input distribution D , where Q is a d × d matrix whose entries are i.i.d. Gaussians,

we have with probability at least 1 - exp(-d(1)) over the randomness of Q,

min(M D )  poly(1/, 1/d, , ).

We will prove this later in Section C.3.

C.1 SMOOTHED ANALYSIS FOR GAUSSIAN INPUTS

In this section, we will prove Theorem 9, as restated below: Theorem 9. Suppose that k  d/5, and the input follows standard Gaussian distribution N (0, Id). Given any weight matrix W with wi   for each row vector, let W be a perturbed version of W according to Equation (17) and M be the perturbed augmented distinguishing matrix. With probability at least 1 - exp(-d(1)), we have
min(M )  poly(1/, 1/d, ).

To prove this theorem, recall the definition of Mij:
mat(Mij) = E (wi x)(wj x)(xx )1{wi xwj x  0} .

Since Gaussian distribution is highly symmetric, for every direction u that is orthogonal to both wi and wj, we have u mat(Mij)u be a constant. We can compute this constant as
mij := E (wi x)(wj x)1{wi xwj x  0} .

This implies that if we consider mat(Mij) - mijId, it is going to be a matrix whose rows and columns are in span of wi and wj. In fact we can compute the matrix explicitly as the following lemma:

Lemma 19. Suppose input x follows standard Gaussian distribution N (0, Id), and suppose weight matrix W has full-row rank, then for any 1  i < j  k, we have

1 mat(Mij) = 

ij cos(ij) - sin(ij)

wi

wj

Id

+

ij 

(wiwj

+ wj wi

)

- sin(ij) ( wi  wj

wj wj +

wj wi

wiwi ),

where 0 < ij <  is the angle between weight vectors wi and wj.

Of course, the same lemma would be applicable to W , so we have an explicit formula for Mij. We will bound the smallest singular value using the idea of leave-one-out distance (as previously used
in Rudelson & Vershynin (2009)).

Leave-one-out Distance Leave-one-out distance is a metric that is closely related to the smallest singular value but often much easier to estimate.

Definition 2. For a matrix A  Rd×n(d  n), the leave-one-out distance d(A) is defined to be the smallest distance between a column of A to the span of other columns. More precisely, let Ai be the i-th column of A and S-i be the span of all the columns except for Ai, then

d(A) := min
i[n]

(Id - ProjS-i )Ai

.

35

Under review as a conference paper at ICLR 2019

Rudelson & Vershynin (2009) showed that one can lowerbound the smallest singular value of a matrix by its leave-one-out distance.

Lemma 20 (Rudelson & Vershynin (2009)). For matrix A  Rd×n(d  n), we always have

d(A)



min(A)



1 n

d(A).

Therefore, to bound min(M ) we just need to lowerbound d(M ). We use the ideas similar to
Bhaskara et al. (2014) and Ma et al. (2016). Since every column of M (except for M0) is random, we will try to show that even if we condition on all the other columns, because of the randomness
in Mij, the distance between Mij to the span of other columns is large. However, there are several obstacles in this approach:

1. The augmented distinguishing matrix M has a special column M0 = vec(Id) that does not have any randomness.
2. The closed form expression for M (as in Lemma 19) has complicated coefficients that are not linear in the vectors wi and wj.
3. The columns of Mij are not independent with each other, so if we condition on all the other columns, Mij is no longer random.

To address the first obstacle, we will prove a stronger version of Lemma 20 that allows a special column.
Lemma 21. Let A  Rd×(n+1) (d  n + 1) be an arbitrary matrix whose columns are A0, A1, ..., An. For any i = 1, 2, ..., n, let S-i be the subspace spanned by all the other columns (including A0) except for Ai, and let d (A) := mini=1,...,n (Id -ProjS-i )Ai . Suppose the column A0 has norm d and A1, ..., An has norm at most C, then

min(A)  min

nC 2 d

d

4nC2 + d , 4n2C2 + nd d (A) .

This lemma shows that if we can bound the leave-one-out distance for all but one column, then the smallest singular value of the matrix is still lowerbounded as long as the columns do not have very different norms. We defer the proof to Section C.2.

For the second obstacle, we show that these coefficients are lowerbounded with high probability. Therefore we can condition on the event that all the coefficients are large enough.
Lemma 22. Given weight vectors wi and wj with norm wi , wj   , let wi = wi + i, wj = wj +j where i, j are i.i.d. Gaussian random vectors. With probability at least 1-exp(-d(1)), we know wi   + 32d/2, wj   + 32d/2 and

2(d - 2)

ij   2 +

, 32d

where ij is the angle between wi and wj. In particular, if W = W + E where E is an i.i.d.

Gaussian random matrix, with probability at least 1-exp(-d(1)), for all i, wi   + 32d/2,

and for all i < j, the coefficient ij/ in front of the term wiwj

+ wj wi

is at least  2(d-2) .
( 2 + 32d)

This lemma intuitively says that after the perturbation wi and wj cannot be close to co-linear. We defer the detailed proof to Section C.2.
For the final obstacle, we use ideas very similar to Ma et al. (2016) which decouples the randomness of the columns.

Proof of Theorem 9. Let E1 be the event that Lemma 22 does not hold. Event E1 will be one of the bad events (but note that we do not condition on E1 not happening, we use a union bound at the end).

36

Under review as a conference paper at ICLR 2019

We partition [d] into two disjoint subsets L1, L2 of size d/2. Let M be the set of rows of M indexed by L1 × L2. That is, the columns of M are

Mij

=

ij 

(wi,L1



wj,L2

+

wj,L1



wi,L2

)

-

sin(ij) ( 

wi wj

wj,L1  wj,L2 +

wj wi

wi,L1  wi,L2 ),

for i < j, where wi,L denotes the restriction of vector wi to the subset L. Note that the restriction of vec(Id) to the rows indexed by L1 × L2 is just an all zero vector.
We will focus on a column Mij with i < j and try to prove it has a large distance to the span of all the other columns. Let Vij be the span of all other columns, which is equal to Vij = span{Mkl : k < l  (k, l) = (i, j)} (note that we do not need to consider M0 because that column is 0 when restricted to L1 × L2.
It's clear that Vij is correlated with Mij, which is bad for the proof. To get around this problem, we follow the idea of Ma et al. (2016) and define the following subspace that contains Vij,

V^ij = span wk,L1  x, x  wk,L2 , wj,L1  x, x  wi,L2 k / {i, j}, x  Rd/2 .

By definition Vij  V^ij, and thus V^ij  Vij , where Vij denotes the orthogonal subspace of Vij. Observe that wj,L1  wi,L2 , wj,L1  wj,L2 , wi,L1  wi,L2  V^ij , thus

ProjV^ij Mij =

ij 

ProjV^ij

wi,L1  wj,L2

.

Note that wi,L1  wj,L2 is independent with V^ij. Moreover, subspace V^ij has dimension at most

(k - 2)d/2 + (k - 2)d/2 + d/2 + d/2

=

(k - 1)d

<

4 5

·

d2 4

.

Then

by

Lemma

31,

we

know

that

with

probability at least 1 - exp(-d(1)),

ProjV^ij wi,L1  wj,L2  poly(1/d, ).
Let E2 be the event that this inequality does not hold for some i, j. Let Sij = span{M0, Mkl : k < l  (k, l) = (i, j)}. Now we know when neither bad events E1 or E2 happens, for every pair i < j,

ProjSij Mij ProjVij Mij

ProjV^ij Mij

=

ij 

ProjV^ij

wi,L1  wj,L2

poly(1/, 1/d, ).

Currently, we have proved that for any i < j, the distance between column Mij and the span of other columns is at least inverse polynomial. To use Lemma 21 we just need to give a bound on the norms of these columns. By Lemma 22, we know when E1 does not happen

i, wi   +

32d ,
2

where  is the uniform upper bound of the norm of every row vector of W . Let  =  +

32 d 2

,

we

know  = poly(, d, ).

37

Under review as a conference paper at ICLR 2019

Thus, we have

Mij

1 

ij| cos(ij)| + sin(ij)

wi

 wj d

+

ij ( 

wi

wj + wj

wi )

+

sin(ij) ( 

wi

wj + wj

2 

(

+

 1) d

+

2 2

+

2 2

.



wi )

Thus, there exists C = poly(, d, ), such that Mij  C for every i < j. Now applying Lemma 21 immediately gives the result.

C.2 PROOF OF AUXILIARY LEMMAS FOR SECTION C.1
We will first prove the characterization for columns in the augmented distinguishing matrix.
Proof of Lemma 19. For simplicity, we start by assuming that every weight vector wi has unit norm. At the end of the proof we will discuss how to incorporate the norms of wi, wj. Also throughout the proof we will abuse notation to use Mij as its matrix form mat(Mij).
Let Sij be the subspace spanned by wi and wj. Let Sij be the orthogonal subspace of Sij. Let {e(1i,j), e2(i,j)} be a set of orthonormal basis for Sij such that e(1i,j) = wi and e(2i,j), wj > 0. We use matrix Sij  Rd×2 to represent subspace Sij, which matrix has e(1i,j) and e2(i,j) as two columns. Also, let Sij be a d × (d - 2) matrix, whose columns constitute an orthonormal basis of Sij .
Let ProjSij = Sij Sij , and ProjSij = Id - Sij Sij . Then, we have
Mij = ProjSij Mij + ProjSij Mij = (ProjSij Mij + mij ProjSij Id) + (ProjSij Mij - mij ProjSij Id) = (ProjSij Mij + mij ProjSij Id) + (ProjSij Mij - mij Sij Sij )
First, we show that ProjSij Mij + mij ProjSij Id = mij Id,
which is equivalent to proving that ProjSij Mij = mijProjSij Id. It's obvious that the column span of ProjSij Mij belongs to the subspace Sij . Actually, the row span of ProjSij Mij also belongs to the subspace Sij . To show this, let's consider u (ProjSij Mij)v, where u  Sij and v  Sij.
u (ProjSij Mij )v = u (Id - Sij Sij )Mij v = (u - u Sij Sij )Mij v = u Mijv,
where the last equality holds since u  Sij is orthogonal to e(1i,j) and e(2i,j). We also know that
u Mijv = u E (wi x)(wj x)(xx )1{wi xwj x  0} v = E (wi x)(wj x)(u x)(v x)1{wi xwj x  0} = E (wi x)(wj x)(v x)1{wi xwj x  0} E[(u x)]
= 0,
where the third equality holds because u x is independent with wi x, wj x and v . Note since u is orthogonal with wi, wj, v, we know for standard Gaussian vector x, random variable u x is independent with wi x, wj x, v x.

38

Under review as a conference paper at ICLR 2019

Since the column span and row span of ProjSij Mij both belong to the subspace Sij , there must exist a (d - 2) × (d - 2) matrix C, such that ProjSij Mij = Sij C(Sij ) . We only need to show this matrix C must be mijId-2. In order to show this, we prove for any u, v  Sij , u (ProjSij Mij)v = mij u v.
u (ProjSij Mij )v = u Mij v
= u E (wi x)(wj x)(xx )1{wi xwj x  0} v = E (wi x)(wj x)(u x)(v x)1{wi xwj x  0} = E (wi x)(wj x)1{wi xwj x  0} E[u xv x]
= miju E[xx ]v
= miju v,
where the fourth equality holds because u x, v x are independent with wi x, wj x.
Thus, we know Mij =(ProjSij Mij + mij ProjSij Id) + (ProjSij Mij - mij Sij Sij )
=mij Id + (ProjSij Mij - mij Sij Sij ).

Let's now compute the closed form for mij. Recall that

mij := E (wi x)(wj x)1{wi xwj x  0} .

Note, we only need to consider input x within subspace Sij, which subspace has dimension two. Using the polar representation of two-dimensional Gaussian random variables (r is the radius and 
is the angle), we have

1 mij = 2

 r3 exp(- r2 )dr 02



21

 2

-ij

2 cos() cos(

+

ij )d

=

 (ij

cos(ij )

-

sin(ij )).

Next, we compute the closed form of ProjSij Mij. Note ProjSij Mij is symmetric, because
ProjSij Mij = Mij -mijId +mijSijSij , and Mij, Id and SijSij are all symmetric. It's obvious that the column span of ProjSij Mij belongs to subspace Sij. Combined with the fact that ProjSij Mij is symmetric, we know the row span of ProjSij Mij also belongs to the subspace Sij. Thus, matrix
ProjSij Mij can be represented as a linear combination of (e(1i,j))(e1(i,j)) ,
(e(1i,j))(e(2i,j)) , (e(2i,j))(e(1i,j)) and (e2(i,j))(e2(i,j)) , which means

ProjSij Mij = c(1i1,j)(e(1i,j))(e(1i,j)) +c1(i2,j)(e(1i,j))(e2(i,j)) +c2(i1,j)(e(2i,j))(e1(i,j)) +c2(i2,j)(e(2i,j))(e(2i,j)) ,

where c1(i1,j), c1(i2,j), c2(i1,j) and c(2i2,j) are four coefficients. Now, we only need to figure out the four coefficients of this linear combination. Similar as the computation for mij, we use polar integration
to show that,

c(1i1,j) = P rojSij Mij , (e(1i,j))(e(1i,j))T

=1 2

 r5 exp(- r2 )dr 02

 2

 2

-ij

cos3() cos( + ij) + cos() cos3( + ij) d

=

1 4

(12ij

cos(ij

)

-

9

sin(ij

)

-

sin(3ij

)),

where the first equality holds because e1(i,j) is orthogonal with e(2i,j). Similarly, we can show that

c2(i2,j) = P rojSij Mij , (e2(i,j))(e2(i,j))T

1 =
2

 r5 exp(- r2 )dr 02

 2

 2

-ij

cos() cos( + ij) sin2() + cos() cos( + ij) sin2( + ij) d

=

1 4

(4ij

cos(ij

)

-

7

sin(ij

)

+

sin(3ij

)),

39

Under review as a conference paper at ICLR 2019

and

c2(i1,j) = P rojSij Mij , (e(2i,j))(e(1i,j))T

1 =
2

 r5 exp(- r2 )dr 02

 2

 2

-ij

- cos2() cos( + ij) sin() + cos() cos2( + ij) sin( + ij) d

=

1 

(ij

sin(ij

)

-

cos(ij

)

sin2(ij

)).

It's easy to check that c(1i2,j) = c(2i1,j). Let Mij be ProjSij Mij - mij Sij Sij . Then, according to above computation, we know

Mij =ProjSij Mij - mij Sij Sij

=(c(1i1,j) - mij )(e(1i,j))(e1(i,j)) + c(1i2,j)(e1(i,j))(e(2i,j)) + c2(i1,j)(e(2i,j))(e(1i,j))

1 =
4

8ij cos(ij) - 5 sin(ij) - sin(3ij)

(e(1i,j))(e1(i,j))

1 +
4

- 3 sin(ij ) + sin(3ij ) (e(2i,j))(e(2i,j))

1 +


ij sin(ij) - cos(ij) sin2(ij)

(e1(i,j))(e(2i,j))

+ (e(2i,j))(e1(i,j))

+ (c(2i2,j) - mij )(e2(i,j))(e(2i,j)) .

Since

e(1i,j)

=

wi

and

e(2i,j)

=

1 sin(ij

)

wj

-

cot(ij )wi ,

we

can

also

express

Mij

as

a

linear

combi-

nation of wiwi , wjwj , wiwj and wjwi .

1

Mij

= 4

8ij cos(ij) - 5 sin(ij) - sin(3ij)

wiwi

1 +
4

- 3 sin(ij) + sin(3ij)

( wj sin(ij )

-

cot(ij

)wi

)(

wj sin(ij

)

- cot(ij)wi)

1 +


ij sin(ij) - cos(ij) sin2(ij)

wi

(

wj sin(ij

)

-

cot(ij )wi )

+

( wj sin(ij )

-

cot(ij )wi )wi

=

ij 

(wiwj

+ wj wi

)

-

sin(ij 

)

(wj

wj

+ wiwi

).

Thus,

Mij =mij Id + Mij

1 =  (ij

cos(ij )

-

sin(ij ))Id

+

ij 

(wiwj

+ wj wi

)

-

sin(ij 

)

(wj

wj

+ wiwi

).

Finally, if the rows wi, wj do not have unit norm, let w¯i = wi/ wi , w¯j = wj/ wj , we know

mij = E (wi x)(wj x)1{wi xwj x  0}

= wi wj E (w¯i x)(w¯j x)1{wi xwj x  0}

=

1 

(ij

cos(ij )

-

sin(ij

))

wi

wj .

Here we used the fact that the indicator variable does not change whether we use wi, wj or w¯i, w¯j. Similarly,

Mij = E (wi x)(wj x)xx 1{wi xwj x  0}

= wi wj E (w¯i x)(w¯j x)xx 1{wi xwj x  0}

=

1 

(ij

cos(ij

)

-

sin(ij

))

wi

wj

Id

+

ij 

(wiwj

+ wj wi

)

-

sin(ij) ( 

wi wj

wj wj +

wj wi

wiwi ).

Now we can prove the lemmas used to handle the two obstacles. First we give the stronger leaveone-out distance bound.

40

Under review as a conference paper at ICLR 2019

Proof of Lemma 21. The smallest singular value of A can be defined as follows:

min(A) := min Au .
u: u =1

Suppose u  argminu: u =1 Au . Let ui be the coordinate corresponding to the column Ai, for

0  i  n. We consider two cases here. If |u0| 

4nC 2 4nC 2 +d

,

then

we

have

min(A) = Au

= u0A0 +

ui Ai

1in

 u0A0 -

uiAi

1in



4nC 2 4nC2 +

d

 d

-

(

|ui |)C

1in



4nC 2 d 4nC2 + d

-

 n

d 4nC2 + d C

nC 2 d = 4nC2 + d ,

where the third inequality uses Cauchy-Schwarz inequality.

If |u0| <

4nC 2 4nC 2 +d

,

we

know

|uk| 

4n2

d C2

+nd

.

Thus,

1in |ui |2



d 4nC 2

+d

.

Let k



argmax1in|ui|.

We know that

min(A)  ukAk +

ui Ai

i:i=k

=|uk |

Ak

+

i:i=k

ui uk

Ai

|uk| (Id - ProjS-k )Ak

|uk|d (A)



d 4n2C2 + nd d (A).

Above all, we know that the smallest singular value of A is lower bounded as follows,

min(A)  min

nC 2 d

d

4nC2 + d , 4n2C2 + nd d (A) .

Next we give the bound on the angle between two perturbed vectors wi and wj.

Proof of Lemma 22. According to the definition of -perturbation, we know wi = wi + i, wj = wj + j, where i, j are i.i.d. standard Gaussian vectors. First, we show that with high probability, the projection of wi on the orthogonal subspace of wj is lower bounded. Denote the subspace spanned by wj as Swj , and denote the subspace spanned by {wj, wi} as Swjwi . Thus, we have

ProjSwj wi

 ProjSwjwi (wi + i) = ProjSwj wi i

where Swj is the orthogonal subspace of Swj .

41

Under review as a conference paper at ICLR 2019

Fix j, then Swjwi is a fixed subspace of Rd with dimension d - 2. Let U be a d × (d - 2) matrix, whose columns constitute a set of orthonormal basis for the subspace Swjwi . Thus, it's
not hard to check that ProjSwjwi i =d U , where   Rd-2 is a standard Gaussian vector. Denote
Y := ProjSwjwi i 2 =d U  2 =  2, which is a chi-squared random variable with (d - 2) degrees of freedom. According to the tail bound for chi-squared random variable, we have

1 -(d-2)t2 Pr[| Y - 1|  t]  2e 8 , t  (0, 1).
d-2

Let

t

=

1 2

,

we

know

that

with

probability

at

least

1

-

2

exp(

-(d-2) 32

),

Y



d-2 .

2

Thus, we have ProjSwj wi   ProjSwjwi i 

2 (d-2) 2

.

Recall

that

We also know

ProjSwj wi = sin(ij ) wi .

wi = wi + i  wi +  i

= +  i ,

where the last equality holds since wi   . Note i 2 is another chi-squared random vari-

able with d degrees of freedom. Similar as above, we can show that with probability at least

1

-

2

exp(

-d 32

),

i

2

3d .
2

By

union

bound,

we

know

with

probability

at

least

1

-

2

exp(

-d 32

)

-

2

exp(

-(d-2) 32

),

2(d - 2)

sin(ij) wi 

, 2

32d

wi  +

. 2

Combined with the fact that ij  sin(ij) when ij  [0, ], we know with probability at least

1

-

2

exp(

-d 32

)

-

2

exp(

-(d-2) 32

),

ij  sin(ij )

= sin(ij) wi wi

2 (d-2)

2

+

32 d 2

2(d - 2) =
2 + 32d

Given W = W + E, where E is an i.i.d. Gaussian matrix, by union bound, we know with probability at least 1 - exp(-d(1)),
i, wi   + 32d/2
i < j, ij   2(d - 2)  ( 2 + 32d)

42

Under review as a conference paper at ICLR 2019
C.3 SMOOTHED ANALYSIS FOR GENERAL INPUTS
In this section, we show that starting from any well-conditioned weight matrix W , and any symmetric input distribution D, how to perturb the distribution locally to D so that the smallest singular value of M D is at least inverse polynomial. Recall the definition of (Q, )-perturbation: we mix the original distribution D with a distribution DQ which is just a Gaussian N (0, QQ ). To create a sample x in D , with probability 1 -  we draw a sample from D; otherwise we draw a standard Gaussian n  N (0, Id) and let x = Qn. We will prove Theorem 10 which we restate below: Theorem 10. Given weight matrix W with wi   for each row vector and symmetric input distribution D. Suppose that k  d/7 and min(W )  , after applying (Q, )-perturbations to yield perturbed input distribution D , where Q is a d × d matrix whose entries are i.i.d. Gaussians, we have with probability at least 1 - exp(-d(1)) over the randomness of Q,
min(M D )  poly(1/, 1/d, , ).
To prove this, let us first take a look at the structure of augmented distinguishing matrix for these distributions. Let M D, M DQ , M D be the augmented distinguishing matrices for distributions D, DQ and D respectively. Since D is a mixture of D and DQ, and the augmented distinguishing matrix is defined as expectations over samples, we immediately have
M D = (1 - )M D + M DQ .
Our proof will go in two steps. First we will show that min(M DQ ) is large. Then we will show that even mixing with M D will not significantly reduce the smallest singular value, so min(M D ) is also large. In addition to the techniques that we developed in Section C.1, we need two ideas that we call noise domination and subspace decoupling to solve the new challenges here.
Noise Domination First let us focus on min(M DQ ). This instance has weight W and input distribution N (0, QQ ). Let M W Q be the augmented distinguishing matrix for an instance with weight W Q and input distribution N (0, Id). Our first observation shows that M DQ and M W Q are closely related, and we only need to analyze the smallest singular value of M W Q. The problem now is very similar to what we did in Theorem 9, except that the weight W Q is not an i.i.d. Gaussian matrix. However, we will still be able to use Theorem 9 as a black-box because the amount of noise in W Q is in some sense dominating the noise in a standard Gaussian. More precisely, we use the following simple claim: Claim 1. Suppose property P holds for N (µ, Id) for any µ, and the property P is convex (in the sense that if P holds for two distributions it also holds for their mixture), then for any covariance matrix  Id, we know P also holds for N (µ, ).
Intuitively the claim says that if the property holds for a Gaussian distribution with smaller variance regardless of the mean, then it will also hold for a Gaussian distribution with larger variance. The proof is quite simple:
Proof. Let  =  - Id, by assumption we know  is still a positive semidefinite matrix. Let x  N (µ, ), x  N (µ,  ) and   N (0, Id), by property of Gaussians it is easy to see that x =d x + . Let dx, dx and d be the density function for x, x ,  respectively, then we know for any point u
dx(u) = Ex N (µ, )[d(u - x )].
That is, N (µ, ) is a mixture of N (x , I). Since property P is true for all N (x , I), it is also true for N (µ, ).
With this claim we can immediately use the result of Theorem 9 to show min(M DQ ) is large.
43

Under review as a conference paper at ICLR 2019

Subspace Decoupling Next we need to consider the mixture M D . The worry here is that although min(M DQ ) is large, mixing with D might introduce some cancellations and make min(M D ) much smaller. To prove that this cannot happen with high probability, the key observation is that in the first step, to prove min(M W Q) is large we have only used the property of W Q. If we let Q¯ be the projection of Q to the orthogonal space of row span of W , then Q¯ is still a Gaussian random matrix even if we condition on the value of W Q! Therefore in the second step we will use the additional randomness in Q¯ to show that the cancellation cannot happen. The idea of partitioning the randomness of Gaussian matrices has been widely used in analysis of approximate message passing algorithms. The actual proof is more involved and we will need to partition the Gaussian matrix Q into more parts in order to handle the special column in the augmented distinguishing matrix .
Now we are ready to give the full proof of Theorem 10

Proof of Theorem 10. Let us first recall the definition of augmented distinguishing matrix: M D is a d2 by (k2 + 1) matrix, where the first k2 columns consist of
MiDj := ExD (wi x)(wj x)(x  x)1{wi xwj x  0} ,
and the last column is ExD [x  x]. According to the definition of (Q, )-perturbation, if we let DQ be N (0, QQ ), then we have
M D = (1 - )M D + M DQ .

In the first step, we will try to analyze M DQ . The first k2 columns of this matrix M DQ can be written as:
MiDj Q = ExDQ (wi x)(wj x)(x  x)1{wi xwj x  0} = EnN (0,Id) (wi Qn)(wj Qn)(Qn  Qn)1{wi Qnwj Qn  0} = Q  QEnN (0,Id) (wi Qn)(wj Qn)(n  n)1{wi Qnwj Qn  0}
for any i < j, and the last column is
ExDQ [x  x] = EnN (0,Id)[Qn  Qn] = Q  QEnN (0,Id)[n  n].

Except for the factor Q  Q, the remainder of these columns are exactly the same as the augmented

distinguishing matrix of a network whose first layer weight matrix is W Q and input distribution is

N (0, Id). We use M W Q to denote the augmented distinguishing matrix of such a network, then we

have

M DQ = Q  QM W Q.

Therefore we can first analyze the smallest singular value of M W Q. Let W = W Q. Note that Q is a Gaussian matrix, and W is fixed, so W Q is also a Gaussian random matrix except its entries are not i.i.d. More precisely, there are only correlations within columns of W Q, and for any column of W Q, the covariance matrix is W W . Since the smallest singular value of W is at least , we know min(W W )  2. Let the covariance matrix of W Q be W Q  Rkd×kd, which has smallest singular value at least 2. Therefore we know W Q 2Ikd. It's not hard to verify that with probability at least 1 - exp(-d(1)), the norm of every row of W Q is upper bounded by poly(, d). By Claim 1, any convex property that holds for any N (0, 2Ikd) perturbation must also hold for W Q 4. Thus, we know with probability at least 1 - exp(-d(1)),
min(M W Q)  poly(1/, 1/d, ).
To prepare for the next step, we will rewrite M W Q as the product of two matrices. According to the closed form of MiWj Q in Lemma 19, we know each column of M W Q can be expressed as a linear combination of wi  wj's and vec(Id). Therefore:
M W Q = W  W , vec(Id) R,

4The conclusion of Theorem 9 is clearly convex because it is a probability, and probabilities are linear in terms of mixture of distributions.

44

Under review as a conference paper at ICLR 2019

where matrix R has dimension (k2 + 1) × (k2 + 1). It's not hard to verify that min(M W Q)  W  W , vec(Id) min(R).

Thus,

min(R) 

min(M W Q)

.

W  W , vec(Id)

Note that W is a k × d matrix with wi   for every row vector, and W = W Q, where Q is an standard Gaussian matrix. Thus, similar as the proof in Lemma 22, we can show that with probability at least 1 - exp(-d(1)),

W Thus, we know

 W , vec(Id)  W  W , vec(Id) min(R)  poly(1/, 1/d, ).

 poly(, d).
F

Now we will try to perform the the projection matrix to the row

second span of

step using W , and let

the idea ProjW 

of =

subspace decoupling. Id - ProjW . Let Q¯ =

Let ProjW ProjW  Q.

be Let

the columns of U  Rd×(d-k) be a set of orthonormal basis for the orthogonal subspace W . By

symmetry of Gaussian, ProjW Q is independent with ProjW  Q. Thus, from now on we will condition on ProjW Q, and still treat ProjW  Q as a Gaussian random matrix. More precisely, ProjW  Q has the same distribution as U P , where P  R(d-k)×d is a standard Gaussian matrix.

We further decouple the P part into two subspaces (this is done mostly to handle the special column in augmented distinguishing matrix). Let the rows of V  Rk×d be a set of orthonormal basis for the row span of W = W Q. And let the rows of V   R(d-k)×d be a a set of orthonormal basis for the orthogonal subspace W . We can then decompose U P into the row span of V and V  as follows,
U P =d U P1V  + U P2V,
where P1  R(d-k)×(d-k) and P2  R(d-k)×k are two independent standard Gaussian matrices. After this decomposition, we have

Q¯  Q¯ W  W , vec(Id) R =d U P  U P W  W , vec(Id) R =d (U P1V  + U P2V )  (U P1V  + U P2V ) W  W , vec(Id) R

=U  U (P1V  + P2V )  (P1V  + P2V ) W  W , vec(Id) R

=U  U (P1V  + P2V )W  (P1V  + P2V )W , vec((P1V  + P2V )(P1V  + P2V ) ) R

=U  U P2V W  P2V W , vec(P1P1 + P2P2 ) R,

where the last equality holds because the row span of V  is orthogonal to the column span of W . Now, we go back to matrix M D . Let ProjW W  be ProjW   ProjW  . We have, min(M D ) =min (1 - )M D + M DQ
=min (1 - )M D + Q  Q W  W , vec(Id) R min (1 - )ProjW W  M D + ProjW W  Q  Q W  W , vec(Id) R =min (1 - )ProjW W  M D + Q¯  Q¯ W  W , vec(Id) R =min (1 - )ProjW W  M D + U  U P2V W  P2V W , vec(P1P1 + P2P2 ) R

45

Under review as a conference paper at ICLR 2019

Since R has full row rank, we know that the row span of ProjW W  M D belongs to the row span of R. According to the definition of U , it's also clear that the column span of ProjW W  M D
belongs to the column span of U  U . Thus, there exists matrix C  R(d-k)2×(k2+1) such that

ProjW W  M D = U  U CR.

Thus,

min(M D ) =min (1 - )M D + M DQ

min (1 - )ProjW W  M D + U  U P2V W  P2V W , vec(P1P1 + P2P2 ) R

=min U  U

1-  C + P2V W

 P2V W , vec(P1P1 + P2P2 )

R.

Note that C only depends on U and R, U only depends on W , and R only depends on W Q. With W Q fixed, C is also fixed. Clearly, C is independent with P1 and P2. For convenience, denote

1- H :=  C + P2V W  P2V W , vec(P1P1 + P2P2 ) .

Now, let's prove that the smallest singular value of matrix H  R(d-k)2×(k2+1) is lower bounded using leave-one-out distance. Let's first consider its submatrix H^ which consists of the first k2
columns of H. Note that within random matrix P2V W , every row are independent with each
other. Within each row, the covariance matrix is W W . Recall that W is a random matrix whose covariance W Q 2Ikd, we can again apply Claim 1 with the property proved in Lemma 37. As a result, with probability at least 1 - exp(-d(1)),

min(W )  poly(1/d, ).
Thus the covariance matrix of each row of P2V W has smallest singular value at least  := poly(1/d, ).

We can view P2V W as the summation of two independent Gaussian matrix, one of which has covariance matrix I(d-k)k. For this matrix, we will do something very similar to Theorem 9 in order to lowerbound its smallest singular value.
Claim 2. For a random matrix K  R(d-k)×k that is equal to Ko + E where E is a Gaussian random matrix whose entries have variance . If d  7k, for any subspace SC that is independent of K and has dimension at most k2 + 1, the leave-one-out distance d(ProjSC K  K) is at least poly(, 1/d).

The proof idea is similar as Theorem 9, and we try to apply Lemma 31 to K  K. In the proof we
should think of K := P2V W , and denote i-th column of K as Ki. We also think of the space SC as the column span of C.
As we did in Theorem 9, we partition [d - k] into 2 disjoint subsets L1 and L2 of size (d - k)/2. Let H^ be the set of rows of H^ indexed by L1 × L2.
We fix a column H^ij, i = j  [k]. Let S = span{H^kl : (k, l) = (i, j)}. It's clear that S is correlated with H^ij. Let C be the set of rows of C indexed by L1 × L2. Let SC be the column span of C , which has dimension at most k2 + 1. We define the following subspace that contains S,

S^ = SC  span{Kj,L1  x, x  Ki,L2 , Kl,L1  x, x  Kl,L2 x  R(d-k)/2, l / {i, j}}

Therefor by definition S  S^, and thus S^  S, where S denotes the orthogonal subspace of S.

Notice

that

H^ ij

=

Ki,L1



Kj,L2

+

 1-

Cij

is

independent

with

S^,

assuming

C

is

fixed.

Moreover,

S^ has dimension at most

(d

-

k)/2

+

(d

-

k)/2

+

(k

-

2)(d

-

k)/2

+

(k

-

2)(d

-

k)/2

+

k2

+

1



4

(d

-

k)2 ,

54

46

Under review as a conference paper at ICLR 2019

if k  d/7. Then, according to Lemma 31, we know with probability at least 1 - exp(-d(1)), ProjS^ H^ij  poly(1/d, ).

For the column H^ii, i  [k], we define subspace S^ slightly different, S^ = SC  span{Kl,L1  x, x  Kl,L2 x  R(d-k)/2, l = i}.
Here the dimension of S^ is also smaller than (d - k)2/5, assuming that k  d/7. We can similarly show that with probability at least 1 - exp(-d(1)),
ProjS^ H^ii  poly(1/d, ).

Thus, by union bound, we know that the leave-one-out distance of matrix H^ is lower bounded by poly(1/d, ).

Now, let's add the additional column vec(P1P1 + P2P2 ) into consideration. For convenience we denote this column by b. We will first prove that the vector b has large norm when projected to the
orthogonal subspace of columns in H^ , then we will combine this with the fact that min(H^ ) is large
to show that min(H) is also large (this last step is very similar to Lemma 21).

We know matrix H^ only depends on the randomness of P2. Thus, with P2 fixed, all columns in H are fixed except for b. Now, we rely on the randomness in P1 to show that the distance between b and the span of other columns in H is lower bounded. In order to get ride of the correlation within
column b, we also need to consider a subset of its rows indexed by L1 × L2, denoted by b . Let the first column of P1 be p  Rd-k, and the submatrix consisting of other columns be P^1. Let SH^ be the column span of H^ . Let

S^H^ = SH^  SC  span(vec(P^1P^1 ) , vec(P2P2 ) ), where vec(P^1P^1 ) is the restriction of vec(P^1P^1 ) to the rows indexed by L1 × L2. The dimension of S^H^ is at most k2 + k2 + 1 + 2  (d - k)2/12, assuming that k  d/7. Clearly,

ProjSH^ b ProjS^H^ b =ProjS^H^ pL1  pL2 ,

where pL is the restriction of p to rows indexed by L. Note that pL1 and pL2 are two independent standard Gaussian vectors. Thus, according to Lemma 31, we know with probability at least 1 -
exp(-d(1)), the distance between b and the column span of H^ is at least poly(1/d).

Claim 3.

For

any

matrix

A



(d-k)2
R4

×k2

and a vector v



R

(d-k)2 4

,

if

the

leave-one-out

distance

d(A)  ,

ProjA b

 , and

v



C1,

let B



R (d-k)2 4

×(k2 +1)

be

the matrix that

is the

concatenation of A and v, then the leave-one-out distance d(B)  poly(, , 1/C1).

The proof idea is similar as the proof in Lemma 21. In the proof, we should think of A := H^ , v := b
and B := H , where H is the subset of rows of H indexed by L1 × L2. We know that d(H^ )   = poly(1/d, ), ProjA b   = poly(1/d). It's not hard to show that with probability at least 1 - exp(-d(1)),

b poly(d).

Thus, there exists C1 = poly(d), such that b  C1. We already proved that the leave-one-out distance of b in H is lower bounded. We only need to show the leave-one-out distance for the first k2 columns, which are H^ij, i, j  [k].

For any i, j  [k], the leave-one-out distance for H^ij within matrix H can be expressed as follows

min
ckl ,cb

H^ij +

cklH^kl + cbb

(k,l)=(i,j)

47

Under review as a conference paper at ICLR 2019

Let {ckl, cb} be one set of the optimal solutions to minckl,cb H^ij + cb = 0, we immediately have

H^ij +

cklH^kl + cb b

(k,l)=(i,j)

= H^ij +

ckl H^ kl

(k,l)=(i,j)

 min
ckl

H^ij +

ckl H^ kl

(k,l)=(i,j)

,

(k,l)=(i,j) H^kl + cbb . If

where the last inequality holds because the leave-one-out distance of matrix H^ is lower bounded by .

If cb = 0, we need to be more careful. In this case, we have,

H^ij +

cklH^kl + cb b

(k,l)=(i,j)

=|cb|

1 cb

H^ ij

+

(k,l)=(i,j)

ckl cb

H^ kl

+

b

|cb |,

where the last inequality holds because the distance of b to the column span of H^ is lower bounded.

If

|cb|



 2C1

,

we

have

H^ij +

(k,l)=(i,j) cklH^kl + cbb



 2C1

.

If

|cb|

<

 2C1

,

we

have

H^ij +

cklH^kl + cb b

(k,l)=(i,j)

 H^ij +

cklH^kl - cb b

(k,l)=(i,j)

 min
ckl

H^ij +

cklH^kl - |cb| b

(k,l)=(i,j)



-

 2C1

C1

=/2.

Thus, the leave-one-out distance of H is lower bounded by poly(, , 1/C1). Recall that with probability at least 1 - exp(-d(1)), we have  = poly(1/d, ),  = poly(1/d), C1 = poly(d). Thus, we have
d(H )  poly(1/d, ).

According to Lemma 20, we have

min(H

)

 1 k2 +

d(H 1

)

poly(1/d, ).

Finally, we put everything together. Since H is a full column rank matrix, we know that min(H)  min(H )  poly(1/d, ). By union bound, we know with probability at least 1 - exp(-d(1)),
min(H) poly(1/d, ) min(R) poly(1/, 1/d, ).

48

Under review as a conference paper at ICLR 2019

Since U is an orthonormal matrix, we know min(U  U ) = 1. According to Eq. 18, we know with probability at least 1 - exp(-d(1)),
min(M D ) min(U  U HR) min(U  U )min(H)min(R) poly(1/, 1/d, , )
where the second inequality holds since all of U  U , H and R have full column rank.

D TOOLS

In this section, we collect some known results on matrix perturbations and concentration bounds. Basically, we used matrix concentration bounds to do the robust analysis and used matrix perturbation bounds to do the smoothed analysis. We also proved several corollaries that are useful in our setting.

D.1 MATRIX CONCENTRATION BOUNDS

Matrix concentration bounds tell us that with enough number of independent samples, the empirical mean of a random matrix can converge to the mean of this matrix.

Lemma 23 (Matrix Bernstein; Theorem 1.6 in Tropp (2012)). Consider a finite sequence {Zk} of independent, random matrices with dimension d1 × d2. Assume that each random matrix satisfies

E[Zk] = 0 and Zk  R almost surely.

Define

Then, for all t  0,

2 := max

E[ZkZk] ,

E[ZkZk] .

kk

Pr

Zk

t

 (d1 + d2) exp

-t2/2 2 + Rt/3 .

k

As a corollary, we have:

Lemma 24. Consider a finite sequence {Z1, Z2 · · · Zm} of independent, random matrices with dimension d1 × d2. Assume that each random matrix satisfies

Zk  R, 1  k  m.

Then, for all t  0,

Pr

m
(Zk - E[Zk])

t

 (d1 + d2) exp

-t2/2 4mR2 + (2Rt)/3 .

k=1

Proof. For each k, let Zk = Zk - E[Zk] be the new random matrices. It's clear that E[Zk] = 0 and Zk  2R. For the variance,

m

2 

E[Zk Zk ]

(18)

k=1

m

 4R2

(19)

k=1
=4mR2.

(20)

(21)

Thus, according to Lemma 23, we have
m
Pr (Zk - E[Zk])  t = Pr
k=1

m
(Zk)  t
k=1

 (d1 + d2) exp

-t2/2 4mR2 + (2Rt)/3

.

49

Under review as a conference paper at ICLR 2019

D.2 MATRIX PERTURBATION BOUNDS

Perturbation Bound for Singular Vectors For singular vectors, the perturbation is bounded by Wedin's Theorem.
Lemma 25 (Wedin's theorem; Theorem 4.1, p.260 in Stewart & Sun (1990).). Given matrices A, E  Rm×n with m  n. Let A have the singular value decomposition

1 0 A = [U1, U2, U3] 0 2 [V1, V2]
00

Let A^ = A + E, with analogous singular value decomposition. Let  be the matrix of canonical angles between the column span of U1 and that of U^1, and  be the matrix of canonical angles between the column span of V1 and that of V^1. Suppose that there exists a  such that
min |[1]i,i - [2]j,j| >  and min |[1]i,i| > ,
i,j i

then

sin() 2 +

sin()

22

E2 2 .

In order to show the robustness of least k right singular vectors of T , we combine Wedin's theorem with the following Lemma.
Lemma 26 (Theorem 4.5, p.92 in Stewart & Sun (1990).). Let  be the matrix of canonical angles between the column span of U and that of U^ , then

ProjU^ - ProjU = sin() .

The exact lemma used in our proof is the following corollary in Ge et al. (2015).

Lemma 27 (Lemma G.5 in Ge et al. (2015)). Given matrix A, E  Rm×n with m  n. Suppose

that the A has rank k. Let S and S^ be the subspaces spanned by the first k right singular vectors of

A and A^ = A + E, respectively. Then, we have:



ProjS^ - ProjS



2 E F. k (A)

Perturbation Bound for pseudo-inverse With a lowerbound on min(A), we can get bounds for the perturbation of pseudo-inverse.
Lemma 28 (Theorem 3.4 in Stewart (1977)). Consider the perturbation of a matrix A  Rm×n : B = A + E. Assume that rank(A) = rank(B) = n, then
 B - A  2 A B E .

The following corollary is particularly useful for us.
Lemma 29 (Lemma G.8 in Ge et al. (2015)). Consider the perturbation of a matrix A  Rm×n : B = A + E where E  min(A)/2. Assume that rank(A) = rank(B) = n, then
 B - A  2 2 E /min(A)2.

Perturbation Bound for Tensor To lowerbound the leave-one-out distance in augmented distinguishing matrix , we use the following Lemma as the main tool.
Lemma 30 (Theorem 3.6 in Bhaskara et al. (2014)). For any constant   (0, 1), given any subspace V of dimension dl in Rdl , there exist vectors v1, v2, · · · , vr in V with unit norm, such that for random (/ d)-perturbations x(1), x(2), · · · , x(l)  Rd of any vector x(1), x(2), · · · , x(l)  Rd, we know with probability at least 1 - exp(-d1/(2l)l ),

j  [r],

vj , x(1)  x(2)  · · ·  x(l)

 l( 1 )3l . d

50

Under review as a conference paper at ICLR 2019

For second-order tensor, we have the following corollary.

Lemma 31. For any constant   (0, 1), given any subspace V of dimension d2 in Rd2 , there exist vectors v1, v2, · · · , vr in V with unit norm, such that for random (/ d)-perturbations x(1), x(2)  Rd of any vector x(1), x(2)  Rd, we know with probability at least 1 - exp(-d1/16),

j  [r],

vj , x(1)  x(2)

 2( 1 )9. d

Perturbation Bound for Eigendecomposition Here, We restate some generic results from Bhaskara et al. (2014) on the stability of a matrix's eigendecomposition under perturbation. Let M and M^ be two n × n mtrices such that M = U DU -1 and M^ = M (I + E) + F .
Definition 3 (Definition A.1 in Bhaskara et al. (2014)). Let sep(D) = mini=j |Dii - Djj|.

The following Lemma guarantees that the eigenvalues of M^ are distinct if the perturbation are not too large.
Lemma 32 (Lemma A.2 in Bhaskara et al. (2014)). If (U )( M E + F ) < sep(D)/2n, then the eigenvalues of M^ are distinct and diagonalizable.

The following Lemma further upperbound the difference between corresponding eigenvectors.

Lemma 33 (Lemma A.3 in Bhaskara et al. (2014)). Let u1, ..., un and u^1, ..., u^n respectively be the eigenvectors of M and M^ , ordered by their corresponding eigenvalues. If (U )( M E + F ) <

sep(D)/2n, then for all i we have

u^i - ui

 3 .max(E)max(D)+max(F )
min(U )sep(D)

In the setting of simultaneous diagonalization, let Na = Ta + Ea and Nb = Tb + Eb, we have NaNb-1 = TaTb-1(I + F ) + G,
where F = -Eb(I + Tb-1Eb)-1Tb-1 and G = EaNb-1. The following lemma bound the maximum singular value of perturbation matrix F and G.

Lemma 34 (Claim A.5 in Bhaskara et al. (2014)).

max(F ) 

max (Eb ) min (Tb )-max (Eb )

and max(G) 

max (Ea )

min (Nb )

Alignment of Subspace Basis. Due to the rotation issue, we cannot conclude that S - S^ is small even we know SS - S^S^ is bounded. The following Lemma shows that after appropriate alignment, S is indeed close to S^.

Lemma 35 (Lemma 6 in Ge et al. (2017a)). Given matrices S, S^  Rd×r, we have

Z

min
Z =Z Z

=Ir

S^ - SZ

2 F



SS - S^S^

2 F

2( 2 - 1)r(SS

)

D.3 SMALLEST SINGULAR VALUE OF RANDOM MATRICES

For a random rectangular matrix where each element is an inpdependent Gaussian variable, Rudel-

son & Vershynin (2009) gives the following result:

Lemma 36 (Theorem 1.1 in Rudelson & Vershynin (2009)). Let A  Rm×n and suppose that

m  n. Assume that the entries of A are independent standard Gaussian variable, then for every

> 0, with probability at least 1 - (C )m-n+1 + e-C n, where C, C are two absolute constants,

we have:

 n(A)  ( m - n - 1).

However, in our setting, we are more interested in fixed matrices perturbed by Gaussian variables. The smallest singular value of these "perturbed rectangular matrices" can be bounded as follows. Lemma 37 (Lemma G.16 in Ge et al. (2015)). Let A  Rm×n and suppose that m  3n. If all the
entries of A are independently -perturbed to yield A, then for any > 0, with probability at least 1 - (C )0.25m, for some absolute constant C, the smallest singular value of A is bounded below by:
 n(A)   m.

51

Under review as a conference paper at ICLR 2019 D.4 ANTI-CONCENTRATION We use the anti-concentration property for Gaussian random variables in our proof of Lemma 17. Lemma 38 (Anti-concentration in Carbery & Wright (2001)). Let x  Rn be a Gaussian variable x  N (0, I), for any polynomial p(x) of degree d, there exists a constant  such that
Pr |p(x)|  V ar[p(x)]   1/d.
52

