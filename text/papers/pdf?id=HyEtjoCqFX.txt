Under review as a conference paper at ICLR 2019
SOFT Q-LEARNING WITH MUTUAL-INFORMATION REGULARIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize the prior action distribution for better performance and exploration. Entropy-based regularization has previously been shown to improve both exploration and robustness in challenging sequential decision-making tasks. It does so by encouraging policies to put probability mass on all actions. However, entropy regularization might be undesirable when actions have significantly different importance. In this paper, we propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutualinformation. In particular, we express the RL problem as an inference problem where the prior probability distribution over actions is subject to optimization. We show that the prior optimization introduces a mutual-information regularizer in the RL objective. This regularizer encourages the policy to be close to a nonuniform distribution that assigns higher probability mass to more important actions. We empirically demonstrate that our method significantly improves over entropy regularization methods, attaining state-of-the-art performance.
1 INTRODUCTION
Reinforcement Learning (RL) (Sutton & Barto, 1998) is a framework for solving sequential decision-making problems under uncertainty. Contemporary state-of-the-art RL methods often use an objective that includes an entropy regularization term (Haarnoja et al., 2018; 2017; Teh et al., 2017). Entropy regularized RL has been shown to capture multi-modal behaviour, as well as exhibiting superior exploration (Haarnoja et al., 2017). Additionally, the learned policies are more robust, as the entropy bonus accounts for future action stochasticity (Grau-Moya et al., 2016) and reduces value overestimation (Fox et al., 2016).
While encouraging high-entropy policies can provide several benefits, it is possible to devise examples where entropy regularization actually impedes exploration. Since high-entropy policies tend to spread the probability mass across all actions equally, they can perform poorly when the RL problem contains actions that are rarely useful. In this paper, we design a reinforcement learning algorithm that dynamically adjusts the importance of actions while learning. We motivate our algorithm by phrasing RL as an inference problem with an adaptive prior action distribution. Where previous work assumes a uniform prior distribution over the actions (Rawlik et al., 2012; Levine, 2018), we generalize the formulation by optimizing the prior. We show that this optimization process leads to an RL objective function with a regularizer based on the mutual-information between states and actions.
We develop a novel algorithm that uses mutual-information regularization to obtain an optimal action prior for better performance and exploration. This novel regularizer for RL encourages policies to be close to the marginal distribution over actions. This results in assigning higher probability to actions frequently used by the optimal policy, while actions that are used infrequently have lower probability under the prior. We demonstrate significant improvement on 19 Atari games over a Deep Q-Network (Mnih et al., 2015) (DQN) baseline without regularization and over Soft QLearning (Schulman et al., 2017) (SQL) that employs standard entropy regularization.
1

Under review as a conference paper at ICLR 2019

2 BACKGROUND

2.1 REINFORCEMENT LEARNING

We consider the standard Markov decision process (MDP) setting. Formally, an MDP is defined as

the tuple S, A, P, R,  where S is the state space, A the action space, and P : S × A × S 

[0, 1] denotes the state transition distribution. Upon taking action at  A in state st  S, the agent transitions to st+1 with probability P (st+1|st, at). The reward function R : S × A  R
quantifies the agent's performance. The goal is to find a policy that maximizes the value function,

i.e. (a|s) = arg max V (s), where V (s) = E

T t=0

tr(st

,

at

)|s0

=s

. Here  is a discount

factor (0 <  < 1) that allows to account for the future in different ways.

The policy-dependent state transition probabilities are defined as P(s |s) := a P (s |a, s)(a|s) which can be written in matrix notation as P  R|S| ×R|S| where the rows are indexed by s and the columns by s . This allows to us conveniently define the agent's stationary distribution over states
µ(s) and actions (a) as follows:

Definition 1 (Stationary distribution over states). The stationary distribution over states is defined in vector form as µ := limt 0 Pt with 0 being an arbitrary vector of probabilities over states at time t = 0. The stationary distribution satisfies µ(s ) = s P(s |s)µ(s) and therefore is a fixed point under the state transition probabilities µ = µ P.
Definition 2 (Stationary distribution over actions). Let µ(s) be the stationary distribution over states induced by the policy . Then the stationary distribution over actions under the policy  is defined as (a) := sS µ(s)(a|s).

2.2 MAXIMUM ENTROPY REINFORCEMENT LEARNING

Maximum Entropy Reinforcement Learning augments the standard RL reward objective with an additional policy entropy term. The optimal value function under entropy regularization (Haarnoja et al., 2017) is defined as:

T
V(s) = max E


r(st,

at)

-

1 

log

(at|st)

,

t=0

(1)

where

s0

=

s,

1 

trades

off

between

reward

and

entropy

maximization,

and

the

expectation

operation

is over state-action trajectories. Here we focus on the undiscounted case for ease of exposition. We

will introduce the discounted case below. The optimal policy that solves (1) can be written in closed

form as:

(a|s) =

e Q (s,a) aA eQ(s,a) ,

where Q(s, a) := r(s, a) + s S P (s |s, a)V(s ). Note that the above represents a generalization of standard RL settings, where    corresponds to a standard RL valuation (lim V(s) = max V (s)), while for   0 we recover the valuation under a random uniform policy. For intermediate values of  we can trade off between reward maximization and
entropy maximization.

Interestingly, one can formulate the maximum entropy RL optimization as an inference problem (Levine, 2018) by specifying a prior distribution over trajectories that assumes a fixed uniform distribution over actions. Precisely this assumption is what encourages the policies to maximize entropy. As outlined in the introduction, encouraging policies to be close to a uniform distribution might be undesirable when some actions are simply non-useful or not frequently used. In the following section, we show that relaxing this assumption by allowing for prior optimization we obtain a novel variational inference formulation of the RL problem that constrains the policies' mutualinformation between states and actions. We show that such policies must be close to the marginal distribution over actions which automatically assign high probability mass to overall useful actions and low probability to non-important or infrequently used actions.

2

Under review as a conference paper at ICLR 2019

3 MUTUAL-INFORMATION REINFORCEMENT LEARNING

The mutual information, core to our approach, is a basic quantity in information theory to mea-

sure the statistical dependence between two random variables. Machine learning applications that

use the mutual information are numerous and include the information-bottleneck method (Tishby

et al., 1999), rate-distortion theory (Cover & Thomas, 2006), clustering (Still & Bialek, 2004)

and curiosity driven exploration (Still & Precup, 2012). Typically, the mutual information is de-

fined as as I(X; Y ) :=

x,y

p(x,

y)

log

pY

p(x,y) (y)pX (x)

=

x pX (x)KL(pY |X (·|x)||pY (·)) where

KL(p(·)||q(·)) =

x

p(x)

log

p(x) q(x)

is the Kullback-Leibler (KL) divergence,

pX , pY

are the

marginal distributions, and pY |X the conditional distribution with respect to the joint p(x, y). However, in this paper, we use an alternative formulation of the mutual information, that will be useful

later on in order to phrase our optimization objective for RL.

Proposition 1 (Mutual Information). Let If be a functional, in particular:

If (pX , pY |X , qY ) := pX (x)KL(pY |X (·|x)||qY (·)),
x
where pX (x) is the distribution of the input, pY |X (y|x) is the channel, and qY (y) a variational distribution. Then, the mutual information can be recovered with

I [X,

Y

]

=

min
qY

If

(pX ,

pY

|X ,

qY

),

where the optimal variational distribution is qY (y) = x pX (x)pY |X (y|x). See e.g. (Cover & Thomas, 2006, Lemma 10.8.1) for details.

As expected, the original definition of the mutual information is recovered since the optimal variational distribution q that minimizes the information functional is the marginal distribution pY .
In the following section, we propose an inference based formulation where the mutual information arises as a consequence of allowing for optimizing the action-prior distributions.

3.1 VARIATIONAL INFERENCE RL FORMULATION WITH OPTIMAL ACTION PRIORS

The RL problem can be expressed as an inference problem by introducing a binary random variable

R that denotes whether the trajectory  := (s0, a0, . . . sT , aT ) is optimal (R = 1) or not (R = 0).

The likelihood of an optimal trajectory can then be expressed as p(R = 1| )  exp(

T t=0

r(st,

at))

(Levine, 2018). We additionally introduce a scaling factor  > 0 into the exponential, i.e. p(R =

1| )  exp(

T t=0

r(st,

at)).

This will allow us to trade off reward and entropy maximization

1. Next, we can define the posterior trajectory probability assuming optimality, i.e. p( |R = 1).

Here we treat  as a latent variable with prior probability p( ), and we specify the log-evidence

as log p(R = 1) = log p(R = 1| )p( )d . We now introduce a variational distribution q( ) to

approximate the posterior p( |R = 1). This leads to an Evidence Lower BOund (ELBO) of the

previous

expression

(scaled

by

1 

)

2:

11 log p(R = 1) = log

p(R = 1| )p( )d





1  E q( )

p(R = 1| )p( ) log
q( )

(2)

The generative model is written as p( ) = p(s0)

T -1 t=0

p(at)P (st+1|st, at)

and

the

variational

distribution as q( ) = p(s0)

T -1 t=0

(at|st)P

(st+1|st,

at).

The

RL

problem

can

now

be

stated

as

a

maximization of the ELBO w.r.t . The maximum entropy RL objective is recovered when assuming

a

fixed

uniform

prior

distribution

over

actions,

i.e.

p(at)

=

1 |A|

for

all

t.

1Other authors absorb this scaling factor into the reward, but we keep it as an explicit hyperparameter. 2Obtained by multiplying and dividing by q( ) inside the integral and applying Jensen's inequality f (Exq[x])  Exq[f (x)] (assuming concave f ).

3

Under review as a conference paper at ICLR 2019

We obtain a novel variational RL formulation by introducing an adaptive prior over actions . Con-

trary to maximum entropy RL, where the priors of the generative model are fixed and uniform,

here the prior over actions is subject to optimization. Therefore, the generative model is written

as p( ) = p(s0)

T -1 t=0

(at)P (st+1|st, at)

whereas

the

variational

distribution

q( )

remains

un-

changed. Starting from Equation (2) and substituting p( ) and q( ) we obtain the following ELBO:

max Eq
,

T t=0

r(st,

at)

-

1 

log

(at|st) (at)

.

Since we are interested in infinite horizon problems, we introduce a discount factor and take the limit limT  (see (Haarnoja et al., 2017)). This leads to the optimization objective that we use in our experiments:

max
,

Eq


t
t=0

r(st, at)

-

1 

log

(at|st) (at)

,

(3)

where 0 <  < 1 is the discount factor. In the following, we show that the the solution for the prior and the policy can be expressed in closed form giving rise to a novel RL regularization scheme.

3.2 RECURSION, OPTIMAL POLICIES AND PRIORS
Crucial for the construction of a practical algorithm are closed form expressions for the optimal policy and prior. More concretely, the optimal policy takes the form of a Boltzmann distribution weighted by the prior . When fixing the policy, the optimal prior is the marginal distribution over actions under the discounted stationary distribution over states. This finding is important to obtain a practical algorithm for learning an optimal prior.

Optimal policy for a fixed prior : We start by defining the value function with the information

cost as V,(s) := E

 t=0



t

r(st, at)

-

1 

log

 (at |st ) (at )

|s0 = s

where one can show that V,

satisfies a recursion similar to the Bellman equations where:

1 (a|s)

V,(s) = E

r(s, a) - log 

(a)

+ Es [V,(s )]

.

(4)

Additionally, when considering a fixed , the problem of maximizing Equation (4) over the policy can be solved analytically by standard variational calculus (Rubin et al., 2012; Genewein et al., 2015). The optimal policy then is

with Z =

(a|s)

:=

1 Z (a) exp(Q,(s, a))

a (a) exp(Q,(s, a)) and the following definition of the soft Q-function as

Q,(s, a) := r(s, a) + Es [V,(s )].

(5) (6)

Being able to write the optimal policy in this way for a given Q-function is needed in order to estimate the optimal prior as we show next.

Optimal prior for a fixed policy: In order to solve for the optimal prior, we rewrite the problem in Equation (3) as


max
,

tt(s)

(a|s)

r(st,

at)

-

1 

log

(a|s) (a)

,

t=0 s

a

where we have defined the marginal distribution over states at time t as

t-1

t(st) :=

p(s0) (ai|si)T (si+1|si, ai).

s0 ,a0 ,...,st-1 ,at-1

i=0

(7)

For a fixed , we eliminate the max operator for  and all components that do not depend on :

max - 1


t



t(s)KL((·|s)||(·)).

t=0 s

4

Under review as a conference paper at ICLR 2019

Swapping the sums and letting p(s) :=

 t=0

tt(s)

be

the

unnormalized

discounted

marginal

distribution

over

states,

we

obtain

max

-

1 

s p(s)KL((·|s)||(·)). The solution to the latter,

 (a) =

s p(s)(a|s) a,s p(s)(a|s)

,

can

easily

obtained

by

adding

the

constraint

that

the

action

prior

is

a

valid

distribution (i.e., a (a) = 1 and (a) > 0, a) and using the method of Lagrange multipliers

and standard variational calculus.

With the form of the optimal prior for a fixed policy at hand, one can easily devise a stochastic approximation method (e.g. k+1(a) = (1 - )k(a) + (a|s) with   [0, 1] and s  p(·) ) to estimate the optimal  using the current estimate of the optimal policy from Equation (5).

Before proposing a practical algorithm (see Section 4), we note that up to this stage there is still no clear connection between the proposed approach and the mutual information. To provide further insight, it will be instructive to show next that, under some limit conditions, the optimization problem proposed by the ELBO can be mapped to an optimization problem with a mutual-information constraint between the stationary distributions over states and actions.

3.3 EQUIVALENCE BETWEEN OPTIMIZING PRIORS AND MUTUAL INFORMATION
The goal of this section is to show that when   1, Equation (3) can be expressed as the following average-reward formulation (Puterman, 1994) with a constraint on the stationary mutual information

max


Esµ

(a|s)r(s, a) s.t. If (µ, , )  C,

a

(8)

where µ and  are the stationary distributions induced by policy  over states and actions, respectively, and thus If (µ, , ) is defined as the stationary mutual-information. Note that for a fixed stationary distribution over states, this problem coincides exactly with the well-known rate-distortion
problem (Cover & Thomas, 2006).

We start by expressing (3) as a constrained problem



max


Eq

tr(st, at)

s.t. min


tIf (t, , )  K(),

t=0 t=0

(9)

where K()

:=

C 1-

(although we have set K(·) as a function of , it is without loss of gener-

ality since we can always obtain a desired K(·) by choosing an appropriate C for a given ) and

the marginal probability of state st at time t following the Markovian dynamics is written as in

Equation (7).

A standard result in the MDP literature (Bertsekas, 1995) is that



arg

max


lim (1


-

)Eq

tr(st, at)



arg

max


Esµ

t=0

(a|s)r(s, a)
a

which basically says that the optimal policy for the limit   1 of an infinite horizon problem is equivalent to the average reward formulation. Now it only remains to make explicit a similar equivalence on the constraint.

We rewrite the constraint by multiplying on both sides by (1 - ) assuming   (0, 1)


min tIf (t, , )  K()  min(1 - ) tIf (t, , )  C.
 t=0 t=0

Taking the limit   1 in the last inequality and interchanging the limit and the min operators3, we

obtain the constraint min lim(1 - )

 t=0

tIf (t,

,

)



C.

Then,

we

see

the

connection

between the last inequality and the constraint on Equation (8) using the following Proposition 2.

3We can interchange the operators because the l.h.s. of the inequality is finite for any 0 <  < 1.

5

Under review as a conference paper at ICLR 2019

Algorithm 1 MIRL

1: Input: the learning rates , Q and , a Q-network Q(s, a), a target network Q¯(s, a), a behavioural policy b, an initial prior 0 and parameters 0 at t = 0.
2: for i = 1 to N iterations do

3: Get environment state si and apply action ai  b(·|si) 4: Get ri, si+1 and store (si, ai, ri, si+1) in replay memory M 5: Update prior i+1(·) = i(·)(1 - ) + i(·|si)
6: if i mod update frequency then

7: Update Q-function i+1 = i - QL(, i+1) (Equation (12))

8:

Update parameter i+1 = (1 - )i + 

1 L(i ,i+1 )

9: end if

10: end for

Proposition 2. Let µ(s) and (a) be the stationary distribution over states and actions under policy  according to Definitions (1) and (2). Then the stationary mutual information defined as
If (µ, , ) := min If (µ, ) can also be written as



If

(µ

,

,



)

=

min


lim (1
1

-

)

tIf (t, , )

t=0

(10)

where t(s) is defined as in (7).

The proof can be found in the Appendix. Since in practice we use a discount factor  1, our original problem formulation in (3) can be seen as an approximation to the problem with stationary mutual-information constraints in (8).

The conclusion of this section is that we have established a clear link between the ELBO with optimizable priors and the average reward formulation with stationary mutual-information constraints. Having the basic theory in place, in the next section we propose a practical algorithm to optimize the problem in (3) using the analytical solutions for the policy and the priors from Section 3.2.

4 MIRL: A MUTUAL INFORMATION RL ALGORITHM

In this section, we present the MIRL agent, our proposed algorithm (see Algorithm 1) for mutual-

information RL. MIRL optimizes Equation (3) using a parametric function Q-function approximator

Q with parameters , and approximates the optimal prior over actions  during learning. The data

used to learn those functions is collected with a behavioural policy that leverages the existence of the

adjusted prior over actions. More concretely, at iteration i, MIRL holds an estimate of the Q-function

denoted by Qi and an estimate of the optimal prior i. The optimal policy for a given estimate of

Qi

and

i

at

iteration

i

is

written

as

i(a|s)

=

1 Z

i

(a)

exp(i

Qi

(s,

a))

(see

Equation

(5)).

Note

that we have introduced an iteration-dependent i which will be useful to slowly shrink the effect

of the constraint over time for improved performance. Now it only remains to show the form of the

behavioural policy and how MIRL updates i, i and i at each iteration.

Prior Updates: We approximate the optimal prior by employing the following update equation,

i+1(a) = (1 - )i(a) + i(a|si)

(11)

where si  i(·). Note that assuming a fixed policy it is easy to show that this iterative update converges to (a) = s µ(s)(a|s), thus estimating correctly the optimal prior.

Q-function Updates: Concurrently to learning the prior, MIRL updates the parameters of the Qfunction by minimizing the following loss,

L(, ) := Es,a,r,s M Q(s, a) - (TsoftQ¯)(s, a) 2

(12)

where M is a replay memory (Mnih et number of training iterations,  is the

easlt.,im20a1te5)o,fQth¯eisoapttiamrgaeltpnreiotwr oarnkdtThasotfitQs uipsdtahteedemafpteirriacaclerstoafitn-

operator

defined

as

(TsoftQ)(s,

a)

:=

r(s,

a)

+



1 

log

a (a ) exp (Q(s , a )). Importantly, this

6

Under review as a conference paper at ICLR 2019

Normalized Score (%)

200

Normalized Score (Median across games) DQN

SQL

150 MIRL

100

50

0

0

1234 Environment Steps

15e7

Prior Probabilities

Reward

50000 40000 30000 20000 10000
0 0

RoadRunnerNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

RoadRunnerNoFrameskip-v4

0.4 0.3 0.2 0.1 0.0
0

1 Envir2onment3Steps 4 15e7

Prior Probabilities

Reward

SeaquestNoFrameskipv4

8000 6000 4000 2000
0 0

DQN SQL MIRL

1234 Environment Steps

15e7

0.125 0.100 0.075 0.050 0.025 0.000
0

SeaquestNoFrameskip-v4 1 Envir2onment3Steps 4 15e7

Figure 1: Left panel: Median Normalized score across 19 Atari games. Comparison between mutual information RL (MIR)L, Soft Q-learning (SQL) and DQN. Right panels: Top figures show the raw score for exemplary games i.e., RoadRunner and Seaquest. Bottom shows the evolution of the estimated prior over actions. We observe that for RoadRunner the prior converges to stable values, whereas for Seaquest it does not. See Appendix for details and plots for all environments. The curves are smoothed with an exponential moving average with effective window size of 106 environment steps.

operator differs from other soft operators arising when employing entropy regularization. The main difference lies in that previous approaches consider a fixed uniform prior, whereas in our case the optimal prior is estimated in the course of learning.

Behavioural policy: Since the Q-function can be learned off-policy, the samples in the replay

memory can conveniently be sampled from a behavioural policy b different from the current estimate of the optimal policy. As such, the behavioural policy used in our experiments is similar in

spirit to an epsilon greedy policy but it better exploits the existence of the estimated optimal prior

when both exploring and exploiting. When exploring, MIRL's behavioural policy samples from the

current estimate of the optimal prior i which has adjusted probabilities, in contrast to vanilla greedy that samples all actions with equal frequency. Additionally, when exploiting, MIRL selects

the maximum probability action that depends not only on the Q-values but also the current estimate

of the optimal action prior, instead of selecting the action with highest Q-value as in traditional -

greedy. More formally, given a random sample u  Uniform[0,1] and epsilon , the action at is obtained by

at =

arg maxa i(a|st(i)), a where a  i(·)

if u > if u  .

Parameter  Updates: The parameter  can be seen as the Lagrange multiplier of Equation (9)

that is directly controlled by the constant K in the constraint. As such, a small fixed value of 

would restrict the class of available policies and evidently constrain the asymptotic performance of

MIRL. In order to remedy this problem and obtain better asymptotic performance, we use the same

adaptive -scheduling from (Leibfried et al., 2017) in which i is updated according to the inverse

of

the

empirical

loss

of

the

Q-function

i.e,

i+1

=

(1 - )i

+



(

L(i

1 ,i+1

)

).

This

update

favours

small values of  at the beginning of training and large values towards the end of training when

the error over Q-values is small. Therefore, towards the end of training when  is large, MIRL is

solving the problem in (9) but without the constraint. This ensures that the asymptotic performance

of MIRL is not hindered.

5 EXPERIMENTS
We conducted experiments on 19 Atari games (Brockman et al., 2016) with Algorithm 1 (MIRL) and two baselines, Deep Q-Networks (DQN) (Mnih et al., 2015) that does not use any regularization and soft Q-learning (SQL) (Leibfried et al., 2017; Haarnoja et al., 2018) which employs entropy regularization. The three algorithms use a neural network for the estimation of Q-values as in (Mnih

7

Under review as a conference paper at ICLR 2019

et al., 2015). The network receives as an input the state s which is composed of the last four frames
of the game with some extra pre-processing (see Appendix), and it outputs a vector of Q-values, i.e. one value for each valid action. We train the network for 5 · 107 environment steps, where a training iteration is performed every four steps. The target network Q¯ is updated every 104 training iterations. Both SQL and MIRL update the Lagrange multiplier  over time by using an exponential moving average of the inverse loss (Leibfried et al., 2017) with  = 3 · 105. MIRL additionally updates the estimate of the optimal prior by using a learning rate  = 5 · 10-5.
Additional experimental details can be found in the Appendix.

During the training of each algorithm,

snapshots of the network parameters (and
the estimate of prior in the case of MIRL) are stored every 105 environment steps for
evaluation. The evaluation for a single

Game
Alien Assault Asterix

DQN (%)
101.58 250.61 166.32

SQL (%)
51.02 283.62 242.73

MIRL (%)
40.23 357.40 330.19

snapshot is conducted by running the pol-

Asteroids

9.74 8.57

7.80

icy for 30 episodes that last a maximum

BankHeist

97.12

94.62

166.26

of 4.5 · 103 environment steps. The epsilon value when in evaluation mode is set to = 0.05.

BeamRider Boxing
ChopperCommand DemonAttack

99.16 2178.57 72.71 350.95

113.64 2283.33
26.37 451.78

117.21 2338.89 65.03 469.30

To summarize the results across all games,

Gopher

474.18 538.87 429.44

we normalized the episodic reward ob-

tained in the evaluation. The normalized

episodic rewards are computed as follows

znormalized

=

z-zrandom zhuman -zrandom

· 100%,

where

z stands for the score obtained from our

agent at test time, zrandom stands for the score that a random agent would obtain

Kangaroo Krull
KungFuMaster Riverraid
RoadRunner Seaquest
SpaceInvaders StarGunner

351.48 843.16 122.14 77.21 548.90 21.95 166.62 653.44

393.16 886.68 142.04 109.37 613.62 36.00 200.38 681.12

405.9 1036.04 121.41 76.02 695.88 64.86 164.79 574.89

and zhuman for the score a human would obtain. Random and human scores are taken

UpNDown Mean

183.19 356.26

230.82 388.83

394.21 413.46

from (Mnih et al., 2015; Van Hasselt et al.,

2016). As seen in Figure 1, our algorithm Table 1: Median Normalized score in 19 Atari games

significantly outperforms the baselines in for Deep Q-Networks (DQN), Soft Q-Learning (SQL)

terms of the median normalized score. In and our approach (MIRL).

particular, after 50M interactions we ob-

tain about 30% higher median normalized

score compared to Soft-Q-Learning and 50% higher compared to DQN. MIRL reaches the SQL final

performance in about half the amount of interactions with the environment and, similarly, it reaches

DQN final performance in about five times less interactions.

In Table 1 we show the comparison between best-performing agents for all the environments, where a best-performing agent is the agent that achieves the best score in evaluation mode considering all evaluation snapshots. Although this measure is not very robust, we include it since it is a commonly reported measure of performance in the field. MIRL outperforms the other baselines in 11 out of 19 games compared to SQL and DQN that are best on 5 and 3 games respectively.

6 CONCLUSION
Using a variational inference perspective, we derived a novel RL objective that allows optimization of the prior over actions. This generalizes previous methods in the literature that assume fixed uniform priors. We show that our formulation is equivalent to applying a mutual information regularization and derive a novel algorithm (MIRL) that learns the prior over actions. We demonstrate that MIRL significantly improves performance over Soft Q-Learning and Deep Q-Networks.
An interesting direction for future work would be to investigate the convergence properties of the alternating optimization problem presented here. We believe that, at least in the tabular case, the framework of stochastic approximation for two timescales (Borkar, 2009) is sufficient to prove convergence. On the experimental side, one could also investigate how our approach can be combined with the Rainbow framework (Hessel et al., 2017) which is the current state of the art in performance.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Dimitri P Bertsekas. Dynamic Programming and Optimal Control: Volume 2. Athena Scientific, 1995.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer, 2009.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, second edition, 2006.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, pp. 202­211. AUAI Press, 2016.
Tim Genewein, Felix Leibfried, Jordi Grau-Moya, and Daniel Alexander Braun. Bounded rationality, abstraction, and hierarchical decision-making: An information-theoretic optimality principle. Frontiers in Robotics and AI, 2:27, 2015.
Jordi Grau-Moya, Felix Leibfried, Tim Genewein, and Daniel A Braun. Planning with informationprocessing constraints and model uncertainty in markov decision processes. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 475­491. Springer, 2016.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning, pp. 1352­1361, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.
Felix Leibfried, Jordi Grau-Moya, and Haitham B Ammar. An information-theoretic optimality principle for deep reinforcement learning. arXiv preprint arXiv:1708.01867, 2017.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Martin Puterman. Markov decision processes: Discrete stochastic dynamic programming. 1994.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement learning by approximate inference. In Robotics: science and systems, volume 13, pp. 3052­3056, 2012.
Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading value and information in mdps. In Decision Making with Imperfect Decision Makers, pp. 57­74. Springer, 2012.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft qlearning. arXiv preprint arXiv:1704.06440, 2017.
Susanne Still and William Bialek. How many clusters? an information-theoretic perspective. Neural computation, 16(12):2483­2506, 2004.
9

Under review as a conference paper at ICLR 2019

Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 131(3):139­148, 2012.
R Sutton and A Barto. Reinforcement learning. MIT Press, Cambridge, 1998.
Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4496­4506, 2017.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. The 37th Annual Allerton Conference on Communication, Control, and Computing., 1999.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. 2016.

A APPENDIX

Proposition 2. Let µ(s) and (a) be the stationary distribution over states and actions under policy  according to Definitions (1) and (2). Then the stationary mutual information defined as
If (µ, , ) := min If (µ, ) can also be written as



If

(µ

,

,



)

=

min


lim (1
1

-

)

tIf (t, , )

t=0

(13)

where t(s) is defined as in (7).

Proof. Following similar steps as in (Bertsekas, 1995, p.186) we have

1 N-1

If

(µ ,

,

 )

=

min


lim
N 

N

If (t, , )

t=0

= min lim lim
 N  1

N -1 t=0



t

If

(t,



,

)

N t=0

t

= min lim lim
 1 N 

N t=0



t

If

(t

,

,

)

N t=0

t



= min lim (1 - )
 1

tIf (t, , )

t=0

where we used Proposition 3 (shown next) in the first equality and where the limits in the third equality can be interchanged due to the Monotone Convergence Theorem.

Proposition 3. Let µ(s) and (a) be the stationary distribution over states and actions under policy  according to Definitions (1) and (2). Then the stationary mutual information defined as
If (µ, , ) can also be written as

1 N-1

If

(µ ,

,

 )

=

min


lim
N 

N

t(st)KL((·|st)||(·))

t=0 st

(14)

where t(s) is defined as in (7).

10

Under review as a conference paper at ICLR 2019

Proof. Let I(s) := KL((·|s)||(·)) and I (s) := KL((·|s)||(·)). Note that both previous quantities are bounded for all t. Then

1 N-1 min lim
 N N

t(s)I(s) =

t=0 s

1 = min lim
 N N

K N -1

t(s)I(s) +

t(s)I(s)

t=0 s

t=K+1 s

1 N-1 = min lim
 N N

t(s)I(s)

t=K+1 s

1 N-1 = min lim
 N N

µ (s)I (s)

t=K+1 s

= min


µ (s)I (s)

lim
N 

1 N

N -1
1

s t=K+1

=

µ (s)Iµ (s)

lim
N 

N

- N

K

s

= µ(s)Iµ(s)
s
= If (µ, , )

where we assumed that t(s) = µ(s) for all t > K and finite large enough K.

Parameter
Frame size Frame skip History frames (in s) Reward clipping Max environment steps Target update frequency (train. steps) Training update frequency (env. steps) Batch size
Memory capacity 
 Q  0

Value
[84, 84]
4
4 [-1., 1.]
27000
10000
4
32 106
0.99 2 · 10-6 2 · 10-5 3.3 · 10-6
0.01

Table 2: Hyperparameters for MIRL. DQN and SQL have the same parameter values as in this table for the used parameters.

11

Under review as a conference paper at ICLR 2019

Prior Probabilities

Prior Probabilities

Prior Probabilities

Prior Probabilities

Reward

AlienNoFrameskip-v4

0.20 0.15 0.10 0.05

0.00 0

1 Envir2onment3Steps 4 15e7

BeamRiderNoFrameskip-v4

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

KangarooNoFrameskip-v4 0.4

0.3

0.2

0.1

0.0 0
0.125 0.100 0.075 0.050 0.025 0.000
0

1 Envir2onment3Steps 4 15e7 SeaquestNoFrameskip-v4
1 Envir2onment3Steps 4 15e7

Prior Probabilities

Prior Probabilities

Prior Probabilities

Prior Probabilities

AssaultNoFrameskip-v4 0.6

0.4

0.2

0.0 0
0.4 0.3 0.2 0.1 0.0
0

1 Envir2onment3Steps 4 BoxingNoFrameskip-v4

15e7

1 Envir2onment3Steps 4 KrullNoFrameskip-v4

15e7

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

SpaceInvadersNoFrameskip-v4

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

Prior Probabilities

Prior Probabilities

Prior Probabilities

Prior Probabilities

AsterixNoFrameskip-v4 0.4

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

ChopperCommandNoFrameskip-v4 0.4

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

KungFuMasterNoFrameskip-v4

0.6

0.4

0.2

0.0 0

1 Envir2onment3Steps 4 15e7

StarGunnerNoFrameskip-v4

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

Prior Probabilities

Prior Probabilities

Prior Probabilities

Prior Probabilities

0.150 0.125 0.100 0.075 0.050 0.025 0.000 0

AsteroidsNoFrameskip-v4 1 Envir2onment3Steps 4 15e7

0.6 DemonAttackNoFrameskip-v4

0.5

0.4

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

RiverraidNoFrameskip-v4 0.4

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

0.6 0.5 0.4 0.3 0.2 0.1
0

UpNDownNoFrameskip-v4 1 Envir2onment3Steps 4 15e7

Prior Probabilities

Prior Probabilities

Prior Probabilities

BankHeistNoFrameskip-v4 0.4

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

GopherNoFrameskip-v4

0.4

0.3

0.2

0.1

0.0 0

1 Envir2onment3Steps 4 15e7

RoadRunnerNoFrameskip-v4

0.4 0.3 0.2 0.1 0.0
0

1 Envir2onment3Steps 4 15e7

Figure 2: Prior Evolution for all games.

4000 3000 2000 1000
00
1000 800 600 400 200
0 0
12000 10000 8000 6000 4000 2000
0 0
25000 20000 15000 10000 5000
0 0
2500 2000 1500 1000 500
0 0

AlienNoFrameskipv4

DQN SQL MIRL

1 Envi2ronment S3teps 4

15e7

BankHeistNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

DemonAttackNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

KungFuMasterNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

SpaceInvadersNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

Reward

Reward

Reward

Reward

Reward

3000
2000
1000
0 0
5000 4000 3000 2000 1000
0 0
10000 8000 6000 4000 2000
0 0
12000 10000 8000 6000 4000 2000
00
60000 50000 40000 30000 20000 10000
0 0

AssaultNoFrameskipv4

DQN SQL MIRL

1 Envi2ronment S3teps 4

15e7

BeamRiderNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

GopherNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

RiverraidNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

StarGunnerNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

Reward

Reward

Reward

Reward

Reward

20000 15000 10000 5000
0 0
100 80 60 40 20 0 20
0
10000 8000 6000 4000 2000
0 0
50000 40000 30000 20000 10000
0 0
25000 20000 15000 10000 5000
0 0

AsterixNoFrameskipv4

DQN SQL MIRL

1 Envi2ronment S3teps 4

15e7

BoxingNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

KangarooNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

RoadRunnerNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

UpNDownNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

Reward

Reward

Reward

Reward

1200 1000 800 600 400 200
0

AsteroidsNoFrameskipv4

DQN SQL MIRL

1 Envi2ronment S3teps 4

15e7

5000 4000 3000 2000 1000
0 0

ChopperCommandNoFrameskipv4

DQN SQL MIRL

1234 Environment Steps

15e7

KrullNoFrameskipv4 8000

6000

4000 2000
0 0

DQN SQL MIRL

1234 Environment Steps

15e7

SeaquestNoFrameskipv4

8000 6000 4000 2000
0 0

DQN SQL MIRL

1234 Environment Steps

15e7

Figure 3: Scores for all games on the evaluation snapshots.

12

Reward

Reward

Reward

Reward

