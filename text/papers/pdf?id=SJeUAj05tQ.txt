Under review as a conference paper at ICLR 2019

DADAM: A CONSENSUS-BASED DISTRIBUTED ADAPTIVE GRADIENT METHOD FOR ONLINE OPTIMIZATION
Anonymous authors Paper under double-blind review

ABSTRACT
Online and stochastic optimization methods such as SGD, ADAGRAD and ADAM are key algorithms in solving large-scale machine learning problems including deep learning. A number of schemes that are based on communications of nodes with a central server have been recently proposed in the literature to parallelize them. A bottleneck of such centralized algorithms lies on the high communication cost incurred by the central node. In this paper, we present a new consensus-based distributed adaptive moment estimation method (DADAM) for online optimization over a decentralized network that enables data parallelization, as well as decentralized computation. Such a framework note only can be extremely useful for learning agents with access to only local data in a communication constrained environment, but as shown in this work also outperform centralized adaptive algorithms such as ADAM for certain realistic classes of loss functions. We analyze the convergence properties of the proposed algorithm and provide a dynamic regret bound on the convergence rate of adaptive moment estimation methods in both stochastic and deterministic settings. Empirical results demonstrate that DADAM works well in practice and compares favorably to competing online optimization methods.

1 INTRODUCTION

Online optimization or learning is a fundamental procedure for solving a wide range of machine learning problems Shalev-Shwartz et al. (2012); Hazan et al. (2016). It can be formulated as a repeated game between a learner (algorithm) and an adversary. The learner receives a streaming data sequence, sequentially selects actions, and the adversary reveals the convex or nonconvex losses to the learner. A standard performance metric for an online algorithm is regret, which measures the performance of the algorithm versus a static benchmark Zinkevich (2003); Hazan et al. (2016). For example, the benchmark could be an optimal point of the online average of loss ( local cost) functions, had the learner known all the losses in advance. In a broad sense, if the benchmark is a fixed sequence, the regret is called static. Recent work on online optimization has investigated the notion of dynamic regret Zinkevich (2003); Hall & Willett (2015); Besbes et al. (2015). The dynamic regret can take the form of the cumulative difference between the instantaneous loss and the minimum loss. For convex functions, previous studies have shown that the dynamic regret of online gradient-based methods can be upper bounded by O( T DT ), where DT is a measure of regularity of the comparator sequence or the function sequence Zinkevich (2003); Hall & Willett (2015). This bound can be improved to O(DT ) Mokhtari et al. (2016); Zhang et al. (2017) when all the cost functions are strongly convex and smooth.

Decentralized nonlinear programming has received a great deal of interest in diverse scientific and

engineering fields Tsitsiklis et al. (1986); Li et al. (2002); Rabbat & Nowak (2004); Lesser et al.

(2012).

The key problem involves optimizing a cost function

f (x)

=

1 n

in=1

fi(x),

where

x



Rp

and each fi is only known to the individual agent i in a connected network of n agents. The agents

collaborate by successively sharing information with other agents located in their neighborhood with

the goal of jointly converging to the network-wide optimal argument Nedic & Ozdaglar (2009).

Compared to optimization procedures involving a fusion center that collects data and performs the

computation, decentralized nonlinear programming enjoys the advantage of scalability to network

sizes, robustness to dynamic topologies, and privacy preservation in data-sensitive applications Yuan

et al. (2016); Shi et al. (2015); Jiang et al. (2017); Lian et al. (2017).

1

Under review as a conference paper at ICLR 2019
Appropriately choosing the learning rate that scale coordinates of the gradient and the way of updating them are crucial issues driving the performance of first Duchi et al. (2011); Kingma & Ba (2014) and second order optimization procedures Tarzanagh et al. (2015); Peyghami & Tarzanagh (2015). Indeed, the understanding that adaptation of the learning rate is advantageous, particularly on a per parameter basis dynamically, led to the development of a family of widely-used adaptive gradient methods including ADAGRAD Duchi et al. (2011), RMSPROP Tieleman & Hinton, and ADAM Kingma & Ba (2014). The ADAM optimizer computes adaptive learning rates for different parameters from estimates of first and second moments of the gradients and performs a local optimization. Numerical results show that ADAM can achieve significantly better performance compared to ADAGRAD, RMSPROP and other gradient descent procedures when the gradients are sparse, or in general small in magnitude. However, its performance has been observed to deteriorate in settings where the loss functions are non-convex and gradients are dense. Further, there is currently a gap in the theoretical understanding of these methods, especially in the non-convex and stochastic setting Reddi et al. (2018); Ward et al. (2018); Zeng & Yin (2018).
In this paper, we develop and analyze a new consensus-based distributed adaptive moment estimation (DADAM) method that incorporates decentalized optimization and uses a variant of the adaptive moment estimation method Duchi et al. (2011); Kingma & Ba (2014); Tieleman & Hinton; McMahan & Streeter (2010). The proposed method is suitable for large scale machine learning problems such as deep learning, since it enables both data parallelization and decentralized computation. Numerical results show the efficiency and effectiveness of the method in comparison with existing methods in the literature.
Next, we briefly summarize the main technical contributions of the work.
- Our first main result (Theorem 4) provides guarantees of DADAM for constrained convex minimization problems defined over a closed convex set X . We provide the convergence bound in terms of dynamic regret and show that when the data features are sparse and have bounded gradients, our algorithm's regret bound can be considerably better than the ones provided by standard mirror descent and gradient descent methods Nedic & Ozdaglar (2009); Hall & Willett (2015); Besbes et al. (2015). It is worth mentioning that the regret bounds provided for adaptive gradient methods Duchi et al. (2011) are static and our results generalize them to dynamic settings.
- In Theorem 5, we give a new static regret analysis for distributed gradient-based algorithms for nonconvex minimization problems computed over a network of agents. Specifically, we prove that under certain regularity conditions, DADAM can achieve a regret bound of order O(1/T ) for nonconvex distributed optimization. To the best of our knowledge, rigorous extensions of existing adaptive gradient methods Duchi et al. (2011); Kingma & Ba (2014); Tieleman & Hinton to the (distributed) nonconvex setting considered in this work do not seem to be available.
- In this paper, we also present regret analysis for distributed stochastic optimization problems computed over a network of agents. Theorems 7 and 9 provide regret bounds of DADAM for minimization problem equation 3 with stochastic gradients and indicate that the result of Theorem 4 and 5 hold true in expectation. Hence, DADAM can be used in decentralized environments where the agents have access to noisy data.
- DADAM uses the projection map X [x]. If X (x) is the identity operator for all x and t, then DADAM corresponds to a decentralized version of adaptive gradient methods Duchi et al. (2011); Kingma & Ba (2014); Tieleman & Hinton. Further, if the mixing matrix W is the identity, Theorems 5, 7 and 9 provide novel regret bounds for adaptive gradient methods for nonconvex and stochastic settings.
The remainder of the paper is organized as follows. Section 2 gives a detailed description of DADAM, while Section 3 establishes its theoretical results. Section 4 explains a network correction technique for our proposed algorithm. Section 5 illustrates the proposed framework on a number of synthetic and real data sets. Finally, Section 6 concludes the paper.
The detailed proofs of the main results established are delegated to the Appendix.
Notation. Throughout the paper, Rp denotes the real coordinate space of p dimensions. For any pair of vectors x, y  Rp, x, y indicates the standard Euclidean inner product. We denote the 1
2

Under review as a conference paper at ICLR 2019

norm by X 1 = i j |xi j|, the infinity norm by X  = maxi j |xi j|, and the Euclidean norm by X =
i j |xi j|2. The above norms reduce to the vector norms if X is a vector. The diameter of the set X with respect to these norms is given respectively by

 = sup x - y ,
x,yX

 = sup x - y .
x,yX

(1)

Let S+p be the set of all positive definite p × p matrices. X ,A [x] denotes the Euclidean projection of a vector x onto X for A  S+p:

X ,A

x

= arg min

A

1 2

(x

-

y)

.

yX

The subscript t is often used to denote the time step while yi,t,d stands for the d-th element of yi,t . Further, yi,1:t,d  Rt is given by

yi,1:t,d = [yi,1,d , yi,2,d , . . . , yi,t,d ] .

We let gi,t to denote the gradient of f at xi,t . The i-th largest singular of matrix X is denoted by i(X). We denote the element in the ith row and jth column of matrix X by [X]i j. In several theorems, we consider a connected undirected graph G = (V , E ) with nodes V = {1, 2, . . . , n} and edges E . The matrix W  Rp×p is often used to denote a symmetric mixing matrix Shi et al. (2015) of graph G .
Indeed, for any agent i, we assign a positive weight [W ]i j for the information received from agent j = i so that [W ]i j > 0 if and only if (i, j)  E and

nn
[W ]i j =  [W ]i j = 1.
i=1 j=1

(2)

The Hadamard (entrywise) and Kronecker product are denoted by and , respectively. Finally, the expectation operator is denoted by E.

2 PROBLEM FORMULATION AND ALGORITHM

In this section, we propose a new optimization method for online optimization that employs data

parallelization and decentralized computation over a network of agents. In our new structure, given

a connected undirected graph G = (V , E ), we let each node i  V at time t  [T ] hold its own

measurement

and

training

data

mi,

and

set

fi,t (x) =

1 mi

mj=i 1

fi,jt (x).

We

also

let

each

agent

i

hold

a

local copy of the global variable x at time t  [T ], which is denoted by xi,t  Rp. With this setup, we

present the following decentralized adaptive gradient method for solving the minimization problem

1T n

 minimize xX

F(x) = n t=1 i=1 fi,t (x),

(3)

where fi,t : X  R is a continuously differentiable mapping on the convex set X .

It is known that it is impossible to achieve a sub-linear dynamic regret bound Zhang et al. (2017),

because of the arbitrary fluctuations in problem equation 3. We want to characterize the hardness of

the problem via a complexly where xt = arg minxX ft (x).

measure that captures the pattern of the Subsequently, we would like to provide

minimizer sequence {xt}tT=1, a regret bound in terms of

T -1
DT = |xt+1,d - xt,d |, t=1

(4)

which represents the variations in {xt}tT=1.

DADAM uses a new distributed adaptive gradient method in which a group of n agents seeking to
solve a sequential version of problem equation 3. Here, we assume that each component function fi,t : X  R becomes only available to agent i  V , after having made its decision at time t  {1, . . . , T }. In the t-th, the i-th agent chooses a point xi,t corresponding to what it considers the network as a whole should have selected. After committing to this choice, the agent has access to

3

Under review as a conference paper at ICLR 2019

Algorithm 1: A decentralized adaptive moment estimation method (DADAM)

input : x0  X , a positive sequence {t }tT=1, exponential decay rates for the moment estimate 1, 2, 3  [0, 1), and a mixing matrix W satisfying equation 2;

1 for all i  V , initialize moment vectors mi,0 = i,0 = ^i,0 = 0; 2 for i  V do

3 for t  1 to T do

4 gi,t =  fi,t (xi,t );

5 mi,t = 1mi,t-1 + (1 - 1)gi,t ;

6 i,t = 2i,t-1 + (1 - 2)gi,t gi,t ;

7 ^i,t = 3i,t + (1 - 3) max(^i,t-1, i,t );

8 9

xi,t xi,t

+

1 2

+1

= =

n
 j=1

[W

]i

j

x

j,t

;

X


, diag(^i,t

)

xi,t

+

1 2

-

t

mi,t
^i,t

;

output:

resulting

parameter

x¯

=

1 n

ni=1

xi,T +1

a

cost

function

fi,t

:X

R

and the

network cost is

then given

by

ft (x) =

1 n

in=1

fi,t (x).

Note that

this function is not known to any of the agents and is not available at any single location.

The procedure of our proposed method is outlined in Algorithm 1.

It is worth mentioning that DADAM computes adaptive learning rates from estimates of both first and second moments of the gradients similar to Duchi et al. (2011); Tieleman & Hinton; Kingma & Ba (2014). However, DADAM uses a smaller learning rate in comparison to ADAM Kingma & Ba (2014) and yet incorporates the intuition of slowly decaying the effect of previous gradients on the learning rate. The key difference of DADAM with ADAM is that it maintains the maximum of all second moment estimates of the gradient vectors until time step t and uses

^i,t = 3i,t + (1 - 3) max(^i,t-1, i,t ),

for normalizing the running average of the gradient instead of i,t in ADAM. The learning rate ^i,t is an important component of the DADAM framework, since it enables us to develop a convergent adaptive method for solving equation 3. Note that in general, ADAM is not necessarily convergent even without operator X ,. Reddi et al. (2018). Note also that DADAM is initialized at xi,t = 0 to keep the presentation of the convergence analysis clear. In general, any initialization can be selected for implementation purposes.
Next, we provide the notion of regret which measures the performance of DADAM against a sequence of local minimizers.
In the framework of online convex optimization, the performance of algorithms is assdessed by regret that measures how competitive the algorithm is with respect to the best fixed solution MateosNu´nez & Corte´s (2014); Hazan et al. (2016); Shahrampour et al. (2016). However, the notion of regret fails to illustrate the performance of online algorithms in a dynamic setting. To overcome this issue, we consider a more stringent metric--dynamic regret Hall & Willett (2015); Besbes et al. (2015); Zinkevich (2003), in which the cumulative loss of the learner is compared against the minimizer sequence {xt}tT=1, i.e.,

  RegCT

:=

1 n

nT i=1 t=1

T
ft (xi,t ) -
t=1

ft (xt),

where xt = arg minxX ft (x).
On the other hand, in the framework of nonconvex optimization, it is usual to state convergence guarantees of an algorithm towards an -approximate stationary point- that is, there exist some iterates xt for which  ft (xt )  . Inspired by Hazan et al. (2017), we next provide the definition of projected gradient and introduce local static regret, a new notion of regret which quantifies the moving average of gradients over network.

4

Under review as a conference paper at ICLR 2019

Definition 1. Let fi : X  R be a differentiable function on a closed convex set X  Rn. Let

 > 0. We define GX (x, fi, ) : X  Rn, the projected gradient of fi at x, by



GX (x, fi, ) =

^i 

(x

-

xi+),

i  V ,

(5)

where

xi+

= arg min{
yX

y, mi ^i

1 +
2

n
y - [W ]i jx j
j=1

2}.

(6)

Now,

let

f¯i,t (xi,t ) =

1 t

ts=1

fi,s(xi,t )

be

the

aggregate

loss

with

gradient

 f¯i,t (xi,t ) =

1 t

ts=1  fi,s(xi,t ).

We analyze the convergence of DADAM as applied to the nonconvex minimization problem equa-

tion 3 using the following local dynamic regret:

RegNT

:=

1 n

n
min
i=1 t{1,...,T }

GX (xi,t , f¯i,t , t )

2,

where GX is given by equation 5.

3 CONVERGENCE ANALYSIS

In this section, our aim is to establish convergence properties of DADAM under the following assumptions:
Assumption 2. For all i  V and t  {1, . . . , T }, the function fi,t (x) is continuously differentiable over X , and has Lipschitz continuous gradient on this set, i.e. there exists a constant   0 so that

 fi,t (x) -  fi,t (y)   x - y , x, y  X ,

which implies

| fi,t (x) - fi,t (y) -



fi,t (y), x - y

|



 2

x-y

2,

x, y  X .

(7a)

Further, fi,t (.) is Lipschitz continuous on X with a uniform constant G  0, i.e.,

| fi,t (x) - fi,t (y)|  G x - y , x, y  X .

(7b)

There are many cost functions fi,t that satisfy this type of Lipschitz condition. For example, it holds for any convex function on a compact set X , or for any polyhedral function on an arbitrary domain Duchi et al. (2011).
Assumption 3. For all i  V and t  {1, . . . , T }, the stochastic gradient g =  fi,t (xi,t ) satisfies

E  fi,t (xi,t ) Ft-1 =  fi,t (xi,t ),

E  fi,t (xi,t ) 2 Ft-1   2,

where Ft is the  -field containing all information prior to the outset of round t + 1.

3.1 CONVEX CASE

Next, we focus on the case where for all i  V and t  {1, . . . , T }, the agent i at time t has access to

the exact gradient gi,t =  fi,t (xi,t ). The following results apply to convex problems, as well as their stochastic variants in a dynamic environment.

Theorem 4.

Suppose that the parameters 1, 2  [0, 1) satisfy 

=

1 2

< 1.

Let 1,t

= 1 t-1, 



(0, 1) and

 fi,t (x)



 G

for all t  {1, . . . , T }.

Then, using a step-size t

=

 t

for the sequence

xi,t generated by Algorithm 1, we have

  RegCT

 +

 (1 -1) 1 + log
2 n (1 - 2) ( + DT ) p
n(1 - 1) d=1

T T

p d=1
^T,d

g1:T,d +
(1

+ -

p G

d=1

(1 - 

1)(1

4 1 + log



-)

T

p
d=1

g1:T,d

2(W )) 4 n (1 - 1)(1 - )

. (1 - 2)

5

Under review as a conference paper at ICLR 2019

Next, we analyze the stochastic convex setting and extend the result of Theorem 4 to the noisy case where agents have access to stochastic gradients of the objective function equation 3.

Theorem 5. Suppose that Assumption 3 holds. Further, the parameters 1, 2  [0, 1) satisfy  =

1 2

<

1.

Let

1,t

= 1 t-1, 



(0, 1).

Then,

using

a

step-size

t

=

 t

for

the

sequence

xi,t

generated

by Algorithm 1, we have

E RegCT

  (1 -1) 1 + log T

p
E

2 n (1 - 2) d=1

g1:T,d

+

( + DT ) n(1 - 1)

p d=1

 T

E

^T,d +

+

p  

d=1

(1 - 

1

)(1

-

4 1 + log T



)
p
d=1

E

g1:T,d

(1 - 2(W )) 4 n (1 - 1)(1 - )

. (1 - 2)

Remark 6. The dynamic regret bounds provided in Theorems 4 and 5 are worse-case optimal, and for many classes of loss functions they are better than existing bounds for both centralized and decentralized settings Zinkevich (2003); Hall & Willett (2015); Besbes et al. (2015); Shahrampour & Jadbabaie (2018) since

pp
 g1:T,d =
d=1 d=1

p

g21,d + g22,d + · · · + g2T,d 

G2 + G2 + · · · + G2

d=1

p 
=  T G = pG T , d=1

and p p 
 T ^T,d  T G  pG T .
d=1 d=1

3.2 NONCONVEX CASE

In this section, we provide convergence guarantees for DADAM for the nonconvex minimization

problem equation To do so, we use

3 defined over the projection

a closed convex set X and show

map X

instead

of

X


, diag(v^i,t

that its regret bound is O( ) for updating parameters

1 T

).

xi,t

(see, Algorithm 1 for details). Also, we assume that there exist two positive numbers  and ¯

such that mind{1,...,p} gi,t,d   and maxd{1,...,p} gi,t,d  ¯ . Now, using the update of i,t as i,t =

(1 - 2) tl=1 2t-l gi,l and since mind{1,...,p} gi,t,d  , we have


i,t



min

d {1,..., p}

i,t,d = min
d {1,..., p}

t
(1 - 2) 2t-l g2i,l,d
l=1

t

= min  d {1,..., p}

(1 - 2) 2t-l = 
l=1

(1 - 2t ),

and

(8)


i,t  max
d {1,..., p}

i,t,d = max
d {1,..., p}

t
(1 - 2) 2t-l g2i,l,d
l=1

t

= max ¯ d {1,..., p}

(1 - 2) 2t-l = ¯
l=1

The update rules ~i,t and ^i,t along with equation 8 and equation 9 lead to

2  ~i,t  ¯ 2,

(1 - 2t ).

(9)

and
2  32 + (1 - 3)2  ^i,t = 3~i,t + (1 - 3) max(^i,t-1, ~i,t )  3¯ 2 + (1 - 3)¯ 2  ¯ 2.

6

Under review as a conference paper at ICLR 2019

Therefore, the estimates of second moments of the gradients satisfy

  ^i,t  ¯ , i,t,

(10)

where  and ¯ are two positive constants.

Theorem 7. Suppose that Assumption 2 holds. Further, the parameters 1, 2  [0, 1) satisfy  =

1 2

< 1.

Choose a positive sequence

{t }tT=1

such that 0 < t



(2-1 ) 2  ¯

with t

<

(2-1 ) 2  ¯

for at

least one t. Then, for the sequence xi,t generated by Algorithm 1, we have

RegNT



1 t

[(2 + log T )2G max
t{2,...,T }

 2n (1 - ) (1 - 2)

t-1
s2t-s-1(W )
s=0

+

T t=1

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

],

(11)

where

t

=

tT=1

[

(2-1 )t 2¯

-

 t2 2 2

].

The following corollary shows that DADAM using a certain step-size leads to a near optimal regret

bound for nonconvex functions.

Corollary 8.

Suppose that in the DADAM algorithm the step-size is given by t =

(2-1 ) 2 2 ¯

for all

t  {1, . . . , T }. Then, we have

RegNT



2¯ 2

T

(2

-

1)(1

- 

1

)(1

-



)2(1

-

2)(1

-



)

+ 16 n¯ (2 + log T )G .

T (2 - 1)(1 - ) (1 - 2)(1 - 2(W ))

(12)

To complete the analysis of our algorithm in the nonconvex setting, we provide the regret bound for DADAM, when stochastic gradients are accessible to the learner.

Theorem 9. Suppose that Assumption 2 holds. Further, the parameters 1, 2  [0, 1) satisfy  =

1 2

< 1.

Choose a positive sequence

{t }tT=1

such that 0 < t



(2-1 ) 2  ¯

with t

<

(2-1 ) 2  ¯

for at

least one t. Then, for the sequence xi,t generated by Algorithm 1, we have

E RegNT



1 t

[(2 + log T )2G max
t{2,...,T }

 2n (1 - ) (1 - 2)

t-1
s2t-s-1(W )
s=0

 +

T t=1

2(1

-

t 1,t ¯ 1,t )(1 - )2(1

-

2)

+

¯  2 2(1 - 1)

T t=1

t

],

(13)

where

t

=

tT=1

[

(2-1 )t 2¯

-

 t2 2 2

].

3.2.1 WHEN DOES DADAM OUTPERFORM ADAM?

We next theoretically justify the potential advantage of the proposed decentralized algorithm

DADAM over centralized adaptive moment estimation methods such as ADAM . More specifically,

the following

corollary shows

that

when

T

is sufficiently

large,

the

1 T

term

will be

dominated by

the

1 term which leads to a 1 convergence rate. That is, DADAM achieves linear speed-up with
nT nT

respect to the number of agents.

Corollary 10. Suppose that Assumptions 2 and 3 hold. Moreover, the parameters 1, 2  [0, 1)

satisfy 

=

1 2

< 1 and

1,t

= 1 t-1, 

 (0, 1).

Choose

the

step-size sequence

as

t

=

 nT

with



=

(2-1)  ¯

2

.

Then,

for

the

sequence

xi,t

generated

by

Algorithm

1,

we

have



E RegNT T



6¯  2 2(1 - 1)

1 + nT

 2G n

x1 - x1

+ (1 - )

8 2G

1,

(1 - 2)(1 - 2(W )) T

(14)

7

Under review as a conference paper at ICLR 2019

if the total number of time steps T is sufficiently large; in particular,

T



(I1

+

I2)



2(1 - 2

1) ,

T



max{

n

4 2 ¯ 4(2 -

2
1)2

,

4¯ 2n (2 - 1)2

},

where 1
I1 = 2(1 - )2(1 - 2)(1 - 1)(1 -  ) ,



2 nG

I2 = (1 - )

. (1 - 2)(1 - 2(W ))¯

(15a) (15b)

4 AN EXTENSION OF DADAM WITH A CORRECTED UPDATE RULE
Compared to classical centralized algorithms, decentralized algorithms encounter more restrictive assumptions and typically worse convergence rates. In order to improve the convergence rate, Shi et al. (2015) introduced a corrected decentralized gradient method called EXTRA, which has a linear rate of convergence if the objective function is strongly convex. Similar to EXTRA, we provide next a corrected update rule for i = 1, . . . , n, given by

t-1 n

 xiC,t-+D1ADAM = xi,t+1 +

[W -W^ ]i jx j,s, t = 0, 1, · · · .

s=0 j=1

(16)

correction

where W^

=

I+W 2

and

xi,t+1

is

generated

by

Algorithm

1.

Our numerical results on some test problems show the efficiency and effectiveness of the corrected DADAM (C-DADAM) in practice. Note that a C-DADAM update is a DADAM update with a cumulative correction term. The summation in equation 16 is necessary, since each individual term nj=1[W -W^ ]i jx j,s is asymptotically vanishing and the terms must work cumulatively.

5 NUMERICAL RESULTS
In this section, we present numerical results for Algorithm 1 (DADAM), on both synthetic and real data sets. The employed data sets and the code will be made available on github upon acceptance of this manuscript. The results are organized in the following sub-sections. In Section 5.1, we investigate the efficency of adaptive decentralized updating rule in static and deterministic settings. We present numerical results on a sparse binary classification problem, comparing the performance of DADAM to that of standard decentralized gradient descent (DGD) Nedic & Ozdaglar (2009); Yuan et al. (2016) and its corrected version EXTRA Shi et al. (2015). In Section 5.2, we demonstrate the efficacy of DADAM in comparison with a decentralized ADAM (Algorithm 1 with 3 = 1) and the recently proposed distributed federated averaging SGD (FedAvg) algorithm on benchmark datasets such as MNIST and CIFAR-10.
All algorithms have been implemented in a Mac machine equipped with a 1.8 GHz Intel Core i5 processor and 8 GB 1600 MHz DDR3 of memory.

5.1 BINARY CLASSIFICATION BASED ON SYNTHETIC DATA IN THE STATIC ENVIRONMENT

Consider the following online distributed learning setting: at each time t, mi randomly generated data points are given to every agent i in the form of (yt,i, j, xt,i, j), where yt,i, j  {±1} is a binary label and xt,i, j  Rp is a feature vector for j = 1, . . . , mi. Our goal is to train a classifier   Rp such that
for an arbitrary new feature vector x^ it assigns y^ = sign(  , x^ ). We design decentralized convex

logistic and nonconvex sigmoid loss functions as follows:

fi1,t ( )

=

1 mi mi j=1

1 + exp(10 xt,i, j, 

yt,i, j)

-1,

fi2,t ( )

=

1 mi

mi
log(1 + exp(-
j=1

xt,i, j, 

yt,i, j)),

(17)

8

Under review as a conference paper at ICLR 2019

(a)

(b)

Figure 1: The residual

t -  0- 

on iteration (time step t  [T ]) for binary classification problem using 

convex and nonconvex (with / t) loss functions. Constant  is the theoretical critical step size

given for DGD Yuan et al. (2016). (a) classification with p = 10. (b) classification with p = 20.

where mi is the size of the training data on node i. Note that fi1,t ( ) is smooth, Lipschitz and nonconvex, while fi2,t ( ) is convex. For X , we consider the 1 ball X 1 = {  Rp :  1  r}, when a sparse classifier is preferred.

We show results for the decentralized problem solved by DADAM, C-DADAM, DGD and EXTRA

over a medium-scale network. We have implemented EXTRA and DGD with their default settings 1. The connected network is randomly generated with n = 50 agents and connectivity ratio r = 0.2.

Each agent holds 10 samples, i.e., mi = 10, i. The agents shall collaboratively obtain p coefficients

via loss functions equation 17. All samples are randomly generated, and the reference classifier

  is pre-computed using a centralized method. As it is easy to implement in practice, we use the

Metropolis

constant

edge

weight

matrix

W

Shi

et

al.

(2015)

and

we

use

W^

=

I+W 2

.

We

have

tested

the

DADAM and C-DADAM with 1 = 2 = 3 = 0.9. The numerical results are illustrated in Figure 1.

It can be seen that DADAM and C-DADAM outperform DGD and EXTRA, showing almost linear convergence in the convex setting to the reference logistic classifier x.

5.2 CIFAR AND MNIST EXPERIMENTS
Next, we present the experimental results using the CIFAR-10 image recognition dataset and the MNIST digit recognition task. The model architecture for training CNN to classify CIFAR10 dataset was taken from the TensorFlow tutorial 2. Also, the model for training a simple multilayer percepton (MLP) on the MNIST dataset was taken from https://github.com/keras-team/keras/blob/master.
We compare the accuracy of DADAM with that of the decentralized ADAM (Algorithm 1 with 3 = 1 ) and the Federated averaging SGD (FedAvg) algorithm McMahan et al. (2016) which also performs data parallelization without decentralized computation. The connected network is gener-
1 http://www.math.ucla.edu/ wotaoyin/software.html 2 http://www.tensorflow.org/tutorials

9

Under review as a conference paper at ICLR 2019
ated with n = 5 agents and connectivity ratio r = 0.9. The parameters for DADAM and ADAM are selected in a way similar to the previous experiments. In our implementation, we use same number of agents and choose E = C = 1 as the parameters in the FedAvg algorithm since it is close to a connected topology scenario as considered in the DADAM and ADAM. It can be easily seen from Figure 2 that DADAM and the decentralized ADAM (3 = 1) outperform FedAvg. Further, DADAM can achieve high accuracy in comparison with the decentralized ADAM and FedAvg.
(a)
(b) Figure 2: Training loss and accuracy over 15 epochs. (a) MNIST digit recognition task (b) CIFAR10 image recognition dataset.
6 CONCLUSION
A decentralized adaptive moment estimation method DADAM was proposed for the distributed learning of deep networks based on adaptive moment of first and second moment of estimations. Convergence properties of the proposed algorithm were established for convex (and dynamic) and nonconvex functions in both stochastic and deterministic settings. Numerical results on some datasets show the efficiency and effectiveness of the new proposed method in practice. ACKNOWLEDGMENTS Use unnumbered third level headings for the acknowledgments.
REFERENCES
Hedy Attouch, Je´ro^me Bolte, and Benar Fux Svaiter. Convergence of descent methods for semialgebraic and tame problems: proximal algorithms, forward­backward splitting, and regularized gauss­seidel methods. Mathematical Programming, 137(1-2):91­129, 2013.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167­175, 2003. 10

Under review as a conference paper at ICLR 2019
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations Research, 63(5):1227­1244, 2015.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Eric C Hall and Rebecca M Willett. Online convex optimization in dynamic environments. IEEE Journal of Selected Topics in Signal Processing, 9(4):647­662, 2015.
Elad Hazan, Karan Singh, and Cyril Zhang. Efficient regret minimization in non-convex games. arXiv preprint arXiv:1708.00075, 2017.
Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends R in Optimization, 2(3-4):157­325, 2016.
Roger A Horn, Roger A Horn, and Charles R Johnson. Matrix analysis. Cambridge university press, 1990.
Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar. Collaborative deep learning in fixed topology networks. In Advances in Neural Information Processing Systems, pp. 5904­5914, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Victor Lesser, Charles L Ortiz Jr, and Milind Tambe. Distributed sensor networks: A multiagent perspective, volume 9. Springer Science & Business Media, 2012.
Dan Li, Kerry D Wong, Yu Hen Hu, and Akbar M Sayeed. Detection, classification, and tracking of targets. IEEE signal processing magazine, 19(2):17­29, 2002.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems, pp. 5330­5340, 2017.
David Mateos-Nu´nez and Jorge Corte´s. Distributed online convex optimization over jointly connected digraphs. IEEE Transactions on Network Science and Engineering, 1(1):23­37, 2014.
H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. arXiv preprint arXiv:1002.4908, 2010.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimization in dynamic environments: Improved regret rates for strongly convex problems. In Decision and Control (CDC), 2016 IEEE 55th Conference on, pp. 7195­7201. IEEE, 2016.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48­61, 2009.
M Reza Peyghami and D Ataee Tarzanagh. A relaxed nonmonotone adaptive trust region method for solving unconstrained optimization problems. Computational Optimization and Applications, 61(2):321­341, 2015.
Michael Rabbat and Robert Nowak. Distributed optimization in sensor networks. In Proceedings of the 3rd international symposium on Information processing in sensor networks, pp. 20­27. ACM, 2004.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.
Shahin Shahrampour and Ali Jadbabaie. An online optimization approach for multi-agent tracking of dynamic parameters in the presence of adversarial noise. In American Control Conference (ACC), 2017, pp. 3306­3311. IEEE, 2017.
11

Under review as a conference paper at ICLR 2019

Shahin Shahrampour and Ali Jadbabaie. Distributed online optimization in dynamic environments using mirror descent. IEEE Transactions on Automatic Control, 63(3):714­725, 2018.
Shahin Shahrampour, Alexander Rakhlin, and Ali Jadbabaie. Distributed detection: Finite-time analysis and impact of network topology. IEEE Transactions on Automatic Control, 61(11): 3256­3268, 2016.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends R in Machine Learning, 4(2):107­194, 2012.
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944­966, 2015.
D Ataee Tarzanagh, M Reza Peyghami, and Fabian Bastin. A new nonmonotone adaptive retrospective trust region method for unconstrained optimization problems. Journal of Optimization Theory and Applications, 167(2):676­692, 2015.
T Tieleman and G Hinton. Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning. Technical report, Technical Report. Available online: https://zh. coursera. org/learn/neuralnetworks/lecture/YQHki/rmsprop-divide-the-gradientby-a-running-average-of-its-recent-magnitude (accessed on 21 April 2017).
John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic and stochastic gradient optimization algorithms. IEEE transactions on automatic control, 31(9): 803­812, 1986.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835­1854, 2016.
Jinshan Zeng and Wotao Yin. On nonconvex decentralized gradient descent. IEEE Transactions on Signal Processing, 66(11):2834­2848, 2018.
Lijun Zhang, Tianbao Yang, Jinfeng Yi, Jing Rong, and Zhi-Hua Zhou. Improved dynamic regret for non-degenerate functions. In Advances in Neural Information Processing Systems, pp. 732­741, 2017.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. 2003.
Next, we establish a series of lemmas used in the proof of main theorems.

6.1 PROPERTIES OF DADAM

Lemma 11. Beck & Teboulle (2003) Let X be a nonempty closed convex set in Rp. Then, for all

d  X , we have

x - d, a  1 d - c 2 - 1 d - x 2 - 1 x - c 2,

22

2

where

x = arg min{ a, x + 1 x - c 2}. xX 2

Lemma 12. McMahan & Streeter (2010) For any A  S+p and convex feasible set C  Rp, suppose a1 = C,A[b1], a2 = C,A[b2], we have

A

1 2

(a1

-

a2

)



A

1 2

(b1

-

b2)

.

Lemma 13.

For all i  V

if

1, 2



[0, 1)

satisfy



=

1 2

< 1, then we have

  T p t m2i,t,d 

  1 + log T

p

t=1 d=1 ^i,t,d

(1 - 1)(1 - )2 (1 - 2) d=1

gi,1:T,d

,

where

t

=

 t

for

all

t



{1, . . . , T }.

12

Under review as a conference paper at ICLR 2019

Proof. Using the update rules in Algorithm 1, we have

 T

 mi2,t,d

T -1
=

 m2i,t,d

+

t=1 t^i,t,d t=1 t^i,t,d

T -1


 mi2,t,d

+

t=1 t^i,t,d

T mi2,T,d (1 - 3) max{^i,T -1,d , i,T,d } + 3i,T,d
T m2i,T,d (1 - 3)i,T,d + 3i,T,d

(=i) T -1 m2i,t,d + (Tl=1(1 - 1)1T -l gi,l,d )2

t=1 t^i,t,d

T Tl=1(1 - 2)2T -l gi2,l,d

(ii) T -1


 m2i,t,d

+

t=1 t^i,t,d

 (Tl=1 1T -l )(Tl=1 1T -l gi2,l,d )

T (1 - 2)

lT=1 2T -l g2i,l,d

 (iii) T -1


 m2i,t,d

+



T 1T -l gi2,l,d

t=1 t^i,t,d (1 - 1) T (1 - 2) l=1 2T -l g2i,l,d

 T-1

t=1

 m2i,t,d t ^i,t ,d

+ (1 - 1)

 T

(1

-

2)

T l=1



T

-l

|gi,l

,d

|,

where (i) follows from the fact that the update rules of mT and T can be written as mT = (1 -

1) Tl=1 1T -lgl and T = (1 - 2) lT=1 2T -lgl2, respectively. (ii) follows from Cauchy-Schwarz

inequality

and

the

fact

that

0



1

<

1.

Inequality

(iii)

follows

since

Tl=1

1T -l



1 1-1

.

Hence,

we

have

  T mi2,t,d
t=1 (1 - 1) t^i,t,d

T

t=1

(1 - 1)

 t

(1

-

2

)

t l=1



t-l

|gi,l,d

|

 =

 (1 - 1)

(1 - 2)

T t=1

1 t

t
t-l |gi,l,d |
l=1

 =

 (1 - 1)

(1 - 2)

TT
|gi,l,d |
t=1 l=t

l-t l

 

(1 -

1)



(1

-

2)

TT
|gi,l,d |
t=1 l=t

l-t t

(i)


 (1 - 1)

(1

-

2)

T t=1

|gi,l,d

|

(1

1 -

 )t

(ii)




(1 - 1)(1 - )2



(1 - 2) gi,1:T,d

T1
t=1 t

(iii)


 1 + log T

(1 - 1)(1 - )2 (1 - 2)

gi,1:T,d

,

where

inequality

(i)

follows

since

lT=t

 l-t



1 (1-

)

.

Inequality

(ii)

follows

from

Cauchy-Schwarz

inequality. The inequality (iii) follows since

T 1  1 +
t=1 t

T t=1

1 dt
t

=

1

+

log t |1T

=

1

+ log T.

(18)

Next, we provide an upper bound on the deviation of the local estimates at each iteration from their consensual value. A similar result has been proven in Shahrampour & Jadbabaie (2017) for online decentralized mirror descent; however, the following lemma extends that of Shahrampour & Jadbabaie (2017) to the online adaptive setting and takes into account the sparsity of gradient vector.
13

Under review as a conference paper at ICLR 2019

Lemma 14.

( Network Error with Sparse Data) If 1, 2  [0, 1) satisfy  =

1 2

< 1, then the

sequence xi,t generated by Algorithm 1 satisfies

 T n 1
t=1 i=1 t

1
V^i,4t (x¯t - xi,t )



n

 1

+

log

T

p
d=1

g1:T,d

,

(1 - 2(W ))(1 - 1)(1 - )2 (1 - 2)

where

V^i,t

=

diag(^i,t )

and

x¯t

=

1 n

n
i=1

xi,t

.

Proof. Let ei,t := xi,t+1 - nj=1[W ]i jx j,t , where W satisfies equation 2. Using the update rule of xi,t+1 in Algorithm 1, we have

T 1
t=1 t

1
V^i,4t ei,t

 T 1
= t=1 t

1n
V^i,4t (xi,t+1 - [W ]i jx j,t )
j=1

  T 1
= t=1 t

1
V^i,4t

(X


, V^i,t

n
[W
j=1

]i

jx

j,t

-

t

mi,t ^i,t

n
- [W ]i jx j,t )
j=1

  T


1

t=1 t

1n

-1 n

V^i,4t ( [W ]i jx j,t - tV^i,t2 mi,t - [W ]i jx j,t )

j=1 j=1

 T
=

p t m2i,t,d ,

t=1 d=1 ^i,t,d

where the first inequality follows from Lemma 12.

Further, from the definition of ei,t , we have

n
xi,t+1 = [W ]i jx j,t + ei,t . j=1

(19) (20)

Now, from equation 2,equation 20 and we have:

1n 1n n

1n

   x¯t+1

=

n

xi,t+1
i=1

=

n

i=1

[W ]i jx j,t
j=1

+

n

ei,t
i=1

1n n

1n

  =

n

( [W ]i j)x j,t
j=1 i=1

+

n

ei,t
i=1

= x¯t + e¯t ,

where

e¯t

=

1 n

n
i=1

ei,t

.

Hence,

It follows from equation 20 that

t
x¯t+1 = e¯s. s=0

tn

 xi,t+1 =

[W t-s]i jei,s.

s=0 j=1

Now, using equation 22 and equation 21, we have

 xi,t+1

-

x¯t+1

=

t s=0

n
([W
j=1

t-s]i

j

-

1 n

)ei,s.

14

(21) (22) (23)

Under review as a conference paper at ICLR 2019

Now, taking the Euclidean norm of equation 23 and summing over t  {1, . . . , T }, one has:

1

   T 1
t=1 t

1
V^i,4t (xi,t+1 - x¯t+1)

=

T t=1

t s=0

n j=1

|[W t-s]i

j

-

1 n

|

V^i,4s ei,s s

1

 (i) T

s=0

V^i,4s ei,s s

T n2t-s(W )
t=1


n

1
T V^i,4t ei,t

(1 - 2(W )) t=0 t

 

(ii)


n

p T t m2i,t,d

(1

-

2 

(W )) 

d=1

t

=1

(iii)


n 1 + log T

^i,t,d
p
d=1

gi,1:T,d

,

(1 - 2(W ))(1 - 1)(1 - )2 (1 - 2)

(24)

where inequality (i) is obtained from the following property of mixing matrix W Horn et al. (1990),

n
j=1

Wt

i

j

-

1 n

 n2t (W ),

(25)

inequality (ii) follows from equation 19. Finally, the inequality (iii) follows from Lemma 13.

Now, summing the inequality equation 24 over i  V and using

 n

n gi,1:T,d  n(

gi,1:T,d

2

)

1 2

 =n

g1:T,d

,

i=1 i=1

we complete the proof.

(26)

Lemma 15. For the sequence xi,t generated by Algorithm 1, we have

  1 n T
( n i=1 t=1

^i,t t

n
xt - [W ]i jx j,t 2 -
j=1

^i,t t

xt - xi,t+1

2)

   22 p
n d=1

^T,d + 2 p T n d=1

^T,d T

T -1
|xt+1,d
t=1

- xt,d |.

(27)

Proof. From the left side of equation 27, we have

^i,t
t

n
xt - [W ]i jx j,t
j=1

2-

^i,t t

xt - xi,t+1

2

 =

^i,t t

n
xt - [W ]i jx j,t
j=1

2-

^i,t+1 t+1

n
xt+1 - [W ]i jx j,t+1
j=1

2

 +

^i,t+1 t+1

n
xt+1 - [W ]i jx j,t+1
j=1

2-

^i,t+1 t+1

n
xt - [W ]i jx j,t+1
j=1

2

+

^i,t+1 t+1

xt - xi,t+1

2-

^i,t t

xt - xi,t+1

2.

By construction of equation 28, we have

nn
 ^i,t+1 xt+1 - [W ]i jx j,t+1 2 - ^i,t+1 xt - [W ]i jx j,t+1 2 j=1 j=1

pn
 = ^i,t+1,d xt+1,d - xt,d , xt+1,d + xt,d - 2 [W ]i jx j,t+1,d d=1 j=1

p

 2

^i,t+1,d |xt+1,d - xt,d |,

d=1

(28) (29)
(30)

15

Under review as a conference paper at ICLR 2019

where the last inequality holds due to equation 1.

Summing equation 29 over t  {1, . . . , T }, the first term telescopes, while the second is handled with equation 30. Hence,

 T
(
t=1

^i,t t

n
xt - [W ]i jx j,t
j=1

2-

^i,t t

xt - xi,t+1

2)

     p 2 d=1

^i,1,d

T -1
+

p

2

1 t=1 d=1

^i,t+1,d t+1

|xt+1,d

-

xt,d

|

+



2

T -1 t=1

p
(
d=1

   p 22

^i,T,d

T -1
+

p

2

d=1 T

t=1 d=1

^i,t+1,d t+1

|xt+1,d

-

xt,d

|,

where the last inequality follows since from definition of ^i,t , we have

^i,t+1,d - t+1

^i,t,d ) t
(31)

^i,t+1,d  t+1

^i,t,d . t

Now, summing equation 31 over i  V and using the inequality ni=1 equation 27 follows.

(32)  ^i,t  n ^t , the claim in

Lemma 16.

Suppose that the parameters 1, 2  [0, 1) satisfy  =

1 2

< 1.

Let 1,t

= 1 t-1, 



(0, 1) and

 fi,t (x)



 G

for all t  {1, . . . , T }.

Then, using a step-size t

=

 t

for the sequence

xi,t generated by Algorithm 1, we have

1n T
 n i=1 t=1

fi,t (xi,t ) - fi,t (xt)

 
+

(1

 - 1) 1 + log T
2 (1 -2) 2 1 + log 

pp

g1:T,d +

d=1 d=1

T

p
d=1

g1:T,d

G (1 - 1)(1 -  )

(1 - 2(W )) 4 n (1 - 1)(1 - ) (1 - 2)

  +

 2 n

p d=1

T ^T,d (1 - 1)

+

 n

p d=1

T ^T,d (1 - 1)

T -1
|xt+1,d
t=1

- xt,d |.

Proof. From convexity of fi,t (·), we have

   1 n T
n i=1 t=1

fi,t (xi,t ) -

fi,t (xt) 

1n T n i=1 t=1

 fi,t (xi,t ), xi,t - xt

    =

1n T n i=1 t=1

 fi,t (xi,t ), xi,t+1 - xt

1n T +
n i=1 t=1

n
 fi,t (xi,t ), xi,t - [W ]i jx j,t
j=1

I1

I2

  1 n T
+ n i=1 t=1

n
 fi,t (xi,t ), [W ]i jx j,t - xi,t+1
j=1

.

I3

(33)

Individual terms in equation 33 can be bounded in the following way. From the Young's inequality for products3, we have

n
I3 =  fi,t (xi,t ), [W ]i jx j,t - xi,t+1 j=1

 ^i,t
2t (1 - 1)

n
[W ]i jx j,t - xi,t+1
j=1

2 + (1 - 1)t 2 ^i,t

 fi,t (xi,t ) 2.

(34)

3An

elementary

case

of

Young's

inequality

is

ab



a2 2

+

b2 2

.

16

Under review as a conference paper at ICLR 2019

In addition, we have

  I2

=

1n T n i=1 t=1

gi,t , xi,t -

n
[W ]i jx j,t
j=1

  =

1n T n i=1 t=1

gi,t , xi,t - x¯t + x¯t

n
- [W ]i jx j,t
j=1

    =

1n T n i=1 t=1

gi,t , xi,t - x¯t

1n T n

+

n

i=1 t=1

[W
j=1

]i

j

gi,t , x¯t - x j,t

1

 =

1n T n i=1 t=1

tV^i,-t41 gi,t ,

V^i,4t (xi,t t

- x¯t )

1

  1 n T n

+

n

i=1 t=1

[W
j=1

]i

j

 t

-1
V^i,t4

gi,t

,

V^i,4t t

(x¯t

-

x

j,t

)

 1 n
n i=1

T -1
 tV^i,t2 g2i,t
t=1

1
T V^i,2t (xi,t - x¯t )2
t=1 t

1n n

 +

n

i=1

[W
j=1

]i

j

T -1
 tV^i,t2 gi2,t
t=1

1
T V^i,2t (x j,t - x¯t )2
t=1 t

(35)

1

      1
n

nT p

-1

t ^i,t2,d g2i,t,d

i=1 t=1 d=1

n T p ^i2,t,d (xi,t,d - x¯t,d )2 i=1 t=1 d=1 t

1

      1 n

+

n

[W ]i j
j=1

nT p

-1

t ^i,t2,d gi2,t,d

i=1 t=1 d=1

n T p ^i2,t,d (x j,t,d - x¯t,d )2 i=1 t=1 d=1 t



2

 1+ 

log

T

p
d=1

g1:T,d

,

(1 - 2(W )) 4 n (1 - 1)(1 - ) (1 - 2)

(36) (37)

where equation 35 and equation 36 utilize Cauchy-Schwarz inequality. Inequality equation 37 follows from equation 2, Lemma 14 and the following inequality:

 T t g2i,t,d = T -1 t gi2,t,d + T gi2,T,d

t=1 ^i,t,d t=1 ^i,t,d

^i,T,d

T -1
=

t g2i,t,d

+

t=1 ^i,t,d

 gi2,T,d T (1 - 2) Tl=1 2T -l g2i,l,d

T -1


t g2i,t,d

+

|gi,T,d |

,

t=1 ^i,t,d

T (1 - 2)

hence, from Cauchy-Schwarz inequality, equation 18 and equation 26 we get

 n T t g2i,t,d 
i=1 t=1 ^i,t,d

  n T |gi,t,d|
(1 - 2) i=1 t=1 t


 

 (1(11-+-lo2 g)2 )Ti=n1i=n1gi,g1:iT,1,d:T,d

T1 t=1 t



n 1 + log T (1 - 2)

g1:T,d

.

(38)

17

Under review as a conference paper at ICLR 2019

To bound I1, using the update rule of mi,t in Algorithm 1, we have

t

mi,t ^i,t

,

xi,t

+1

-

xt

=

t (

1,t ^i,t

mi,t-1

+

(1

- 1,t ) ^i,t



fi,t (xi,t

)),

xi,t+1

-

xt

=

t

1,t ^i,t

mi,t

-1,

xi,t+1

-

xt

+

t

(1

- 1,t ^i,t

)



fi,t

(xi,t

),

xi,t

+1

-

xt

.

Now, by rearranging the above equality, one has:

p
d=1

(1

- 1,t ^i,t,d

)



fi,t,d

(xi,t,d

),

xi,t

+1,d

-

xt,d

 p
=
d=1

mi,t,d ^i,t,d

,

xi,t+1,d

-

xt,d

p
+
d=1

1,t ^i,t,d

mi,t-1,d , xt,d - xi,t+1,d



mi,t ^i,t

,

xi,t+1

-

xt

+

||xt

-

xi,t+1||

p d=1

1,t

mi,t-1,d ^i,t,d

 

1 2t

||xt -

n
[W ]i jx j,t ||2 -
j=1

1 2t

||xt - xi,t+1||2 -

1 2t

||xi,t+1 -

n
[W ]i jx j,t ||2
j=1

p
+ ||xt - xi,t+1||G
d=1

1,t , ^i,t,d

(I I1 )

(39)

(I I2 )

where (II1) follows from Lemma 11. The term (II2) is obtained by induction as follows: using the assumption, gi,t   G; now, using the update rule of mi,t in Algorithm 1, we have

mi,t   (1 + (1 - 1)) max( gi,t , mi,t-1 ) = max( gi,t , mi,t-1 )  G,

(40)

where mi,t-1   G by induction hypothesis.

Note that from our assumption, we have 1,t = 1 t-1,   (0, 1) and 1,t  1. Substituting this into equation 39, we get

I1 =  fi,t (xi,t ), xi,t+1 - xt



(1

^i,t - 1,t

)

[

1 2t

||xt -

n
[W ]i jx j,t ||2 -
j=1

1 2t

||xt - xi,t+1||2

 -

1 2t

||xi,t+1

-

n
[W
j=1

]i

jx

j,t

||2]

+

||xt

-

xi,t+1||G

p d=1

(1

1,t - 1,t

)



(1

^i,t - 1

)

[

1 2t

n
xt - [W ]i jx j,t
j=1

2- 1 2t

xt - xi,t+1

2]

 -

(1

-

^i,t 1)2t

||xi,t+1

-

n
[W
j=1

]i

jx

j,t ||2

+

G

p d=1

(1

1,t - 1,t

)

,

(41)

where the last line comes from equation 1. Plugging equation 34, equation 37, equation 38 and

equation 41 into equation 33, we obtain

1n T
 n i=1 t=1

fi,t (xi,t ) - fi,t (xt)

  
+

(1



-1) 1 + log T

2

n

(1 - 

2

)

2 1 + log



p

g1:T,d + G

d=1

T

p
d=1

g1:T,d

t

T =1

p d=1

1,t (1 - 1,t )

(1 - 2(W )) 4 n (1 - 1)(1 - ) (1 - 2)

  +

1 n

nT i=1 t=1

2(1

^i,t - 1)

[

1 t

||xt

-

n
[W
j=1

]i

jx

j,t ||2

-

1 t

||xt

-

xi,t+1||2].

18

Under review as a conference paper at ICLR 2019

Now, since 1,t = 1 t-1,   (0, 1) and 1,t  1, we have

 T

1,t

T


 t-1 

1.

t=1 (1 - 1,t ) t=1 (1 - 1) (1 - 1)(1 -  )

(42)

Finally, using equation 42 and inequality in=1

 ^i,T,d  n

^t,d, we obtain the desired result.

6.1.1 PROOF OF THEOREM 4

Proof. From convexity of fi,t (·) it follows that

ft (xi,t ) - ft (xt) = ft (xi,t ) - ft (x¯t ) + ft (x¯t ) - ft (xt)  gi,t , xi,t - x¯t + ft (x¯t ) - ft (xt)

 =

1 n

n i=1

fi,t (x¯t ) -

1 n

n i=1

fi,t (xt) +

gi,t , xi,t - x¯t

,

and

  ft(xi,t) -

ft (xt)



1 n

n i=1

fi,t (xi,t ) -

1 n

n i=1

fi,t (xt) +

gi,t , xi,t - x¯t

+

1n n i=1

gi,t , xi,t - x¯t

.

(43)

Summing over t  {1, . . . , T } and i  V , and applying Lemma 16 and equation 37 gives the desired result.

6.1.2 PROOF OF THEOREM 5

Proof. We just need to rework the proof of Lemma 16 and Theorem 4 using stochastic gradients by tracking the changes. Indeed, using stochastic gradient at the beginning of Lemma 16, we have

TT
 fi,t (xi,t ) - fi,t (xt)   fi,t (xi,t ), xi,t - xt
t=1 t=1

TT
 =  fi,t (xi,t ), xi,t - xt +  fi,t (xi,t ) -  fi,t (xi,t ), xi,t - xt t=1 t=1

T Tn
  =  fi,t (xi,t ), xi,t+1 - xt +  fi,t (xi,t ), xi,t - [W ]i jx j,t t=1 t=1 j=1

Tn

T

  +  fi,t (xi,t ), [W ]i jx j,t - xi,t+1 +  fi,t (xi,t ) -  fi,t (xi,t ), xi,t - xt .

t=1 j=1

t=1

Further, if we replace any bound involving G which is an upper bound on the exact gradient with the norm of stochastic gradient, we obtain

1n T
 n i=1 t=1

fi,t (xi,t ) - fi,t (xt)

  
+

(1



-1) 1 + log T

2

n

(1 - 

2

)

2 1 + log



p

g1:T,d +

d=1

T

p
d=1

g1:T,d



mi,t-1

Tp

1,t

 t=1 d=1 (1 - 1,t )

(1 - 2(W )) 4 n (1 - 1)(1 - ) (1 - 2)

  +

1 n

nT i=1 t=1

2(1

^i,t - 1)

[

1 t

||xt

-

n
[W
j=1

]i

jx

j,t ||2

-

1 t

||xt

-

xi,t+1||2]

1 n
+ n i=1

 fi,t (xi,t ) -  fi,t (xi,t ), xi,t - xt

.

(44)

Note that using Assumption 3, we have

E [  fi,t (xi,t ) -  fi,t (xi,t ), xi,t - xt ] = 0,

19

Under review as a conference paper at ICLR 2019

and 1 E [  fi,t (xi,t ) ]  E  fi,t (xi,t ) 2 2   .

Hence taking expectation from equation 44, using the above inequality and equation 40, we have

1n T
 n i=1 t=1

E [ fi,t (xi,t )] -
+

fi,t (xt)

  (1 -1) 1 + log T

p
E

2

 1

2n + log 

(1 - 2)

T

p
d=1

E

d=1
g1:T,d

g1:T,d

(1 - 2(W )) 4 n (1 - 1)(1 - ) (1 - 2)

 +



T t=1

p d=1

(1

1,t - 1,t )

1n 1
+ n i=1 2(1 - 1) E

^i,t
t

n
xt - [W ]i jx j,t
j=1

2-

^i,t t

xt - xi,t+1 2

.

According to the result in Lemma 15 and equation 42, we have

  1 n T
n i=1 t=1

E [ fi,t (xi,t )] - +

fi,t (xt)

  (1 -1) 1 + log T

p
E

2

 1

2n + log 

(1 - 2)

T

p
d=1

E

d=1
g1:T,d

g1:T,d

p
+

 

d=1 (1 - 1)(1 -  )

(1 - 2(W )) 4 n

(1

-

1)(1

-) 

(1 - 2)

  + 2 p
n d=1

T E ^T,d +  p

(1 - 1)

n d=1

T E ^T,d (1 - 1)

T -1
|xt+1,d - xt,d |.
t=1

Finally, using equation 43 we complete the proof.

Next, we establish a series of lemmas used in the proof of Theorem 7 and 9.
Lemma 17. Attouch et al. (2013) Let GX be the projected gradient defied in equation 5. Then, GX (, fi, ) = 0 if and only if  is a critical point of equation 3.

Next, we show that the projection map GX in Definition 1 is Lipschitz continuous.

Lemma 18. Suppose the second moment ^ satisfies equation 10. Let x1+,i and x2+,i be given in

Definition

1

with

mi
^i

replaced

by

m1i
^i1

and

mi2
^i2

,

respectively.

Then,

GX1 (xi, f¯i, ) - GX2 (xi, f¯i, )

 ¯ 

m1i - mi2 ,

i  V ,

where G1X and GX2 are the projection maps corresponding to x1+,i and x2+,i, respectively.

Proof. Consider the optimality condition of equation 6, for any u  X . For each i  V , observe

that

m1i
^i1

+

1 

(x1+,i

-

n
[W ]i jx j), u
j=1

- x1+,i

 0,

(45)

mi2
^i2

+

1 

(x2+,i

-

n
[W ]i jx j), u
j=1

- x2+,i

 0.

(46)

Taking u = x2+,i in equation 45, we have

mi1, x2+,i - x1+,i 

^i1 

n
[W ]i jx j - x1+,i, x2+,i - x1+,i .
j=1

Likewise, setting u = x1+,i in equation 46, we get

(47)

m2i , x1+,i - x2+,i 

^i2 

n
[W ]i jx j - x2+,i, x1+,i - x2+,i .
j=1

(48)

20

Under review as a conference paper at ICLR 2019

Now, using equation 47, equation 48 and equation 10, we obtain

m1i - m2i

 

x2+,i - x1+,i .

Without loss of generality, assumes that ^i1  ^i2. Then, using equation 5, we have

(49)

GX1 (xi, f¯i, ) - G2X (xi, f¯i, ) =

^i1 

(x

-

x1+,i)

-

^i2 

(x

-

x2+,i)



^i1 

(x

-

x1+,i)

-

^i1 

(x

-

x2+,i)

(i)


¯



x2+,i - x1+,i

(ii)


¯



mi1 - m2i ,

where (i) follows from equation 10 and (ii) follows from equation 49.

Lemma 19.

Let 1, 2  [0, 1) satisfy 

=

1 2

< 1.

Then, for the sequence xi,t

generated by Algo-

rithm 1, we have

 1 n
n i=1

xi,t - xt



 2n (1 - ) (1 - 2)

t-1
s2t-s-1(W )
s=0

=

Bt .

Proof. In the proof of Lemma 14 we showed that

 xi,t+1

-

x¯t+1

=

t s=0

n
([W
j=1

t-s]i

j

-

1 n

)ei,s.

Now, using the above equality, we have

 xi,t+1 - x¯t+1

=

t s=0

n j=1

|[W

t-s]i

j

-

1 n

|

ei,s

.

(50)

Also, using Lemma 13, we get

mi,t 

1.

^i,t (1 - ) (1 - 2)

(51)

The above inequality together with the update rule of xi,t+1, imply that

  ei,t

=

n
xi,t+1 - [W ]i jx j,t
j=1

=

n
X [ [W ]i jx j,t - t
j=1

mi,t ^i,t

]

-

n
[W
j=1

]i

j

x

j,t

 

n
[W ]i jx j,t - t
j=1

mi,t ^i,t

-

n
[W ]i jx j,t
j=1

= t

mi,t ^i,t

 t , (1 - ) (1 - 2)

(52)

where the first inequality follows from the nonexpansiveness property of the Euclidean projection4.

Substituting equation 25 and equation 52 into equation 50, we get

xi,t - x¯t



 n
(1 - ) (1 - 2)

t-1
s2t-s-1(W ).
s=0

Summing the above inequality over i  V , we conclude that

   n
i=1

xi,t - xt

n
 xi,t - x¯t
i=1

n
+ x¯t - xt
i=1



 2n n (1 - ) (1 - 2)

t-1
s2t-s-1(W ).
s=0

4 X [x1] - X [x2]  x1 - x2 , x1, x2  Rp.

21

Under review as a conference paper at ICLR 2019

Lemma 20. For the sequence xi,t generated by Algorithm 1, we have

 f¯i,t , GX (xi,t , f¯i,t , t )

 (2 - 1,t ) 2(1 - 1,t )

GX (xi,t , f¯i,t , t )

2

-

2(1

-

1,t ^i,t 1,t )(1 - )2(1

-

2)

.

Proof. A quick look at optimality condition of equation 6 verifies that

mi,t
^i,t

+

1 t

(xi,t+1 -

n
[W ]i jx j,t ), z - xi,t+1
j=1

 0,

z  X .

Substituting z = xi,t into the above inequality, we get

mi,t
^i,t

,

xi,t

-

xi,t

+1

1 t

n
xi,t+1 - [W ]i jx j,t , xi,t+1 - xi,t .
j=1

Now using equation 2, we have

^i,t xi,t - xi,t+1 2  mi,t , xi,t - xi,t+1 = 1,t mi,t-1 + (1 - 1,t ) f¯i,t , xi,t - xi,t+1 t

= 1,t

^i,t-1 t

t mi,t-1 ^i,t-1

,

xi,t

-

xi,t+1

+ (1 - 1,t )  f¯i,t , xi,t - xi,t+1



1,t ^i,t 2t

(

(1

-



t2 )2(1

-

2

)

+

xi,t - xi,t+1

2) + (1 - 1,t )  f¯i,t , xi,t - xi,t+1 ,

where the first equality follows from the update rule of mi,t . The last inequality is valid since Cauchy-
Schwarz inequality and inequality ^i,t-11,t  ^i,t 1. The claim then follows after using Definition 1.

6.1.3 PROOF OF THEOREM 7

Proof.

Let

f¯i,t (xi,t ) =

1 t

ts=1

fi,s(xi,t ).

Using Assumption 2, Taylor's expansion and Definition 1, we

have

f¯i,t (xi,t+1)  f¯i,t (xi,t ) +

 f¯i,t (xi,t ), xi,t+1 - xi,t

+ 2

xi,t+1 - xi,t

2

 f¯i,t (xi,t ) -

t ^i,t

 f¯i,t (xi,t ), GX (xi,t , f¯i,t , t )

+ t2 2^i,t

GX (xi,t , f¯i,t , t ) 2.

The above inequality together with Lemma 20, equation 10 and 1,t  1, imply that

f¯i,t (xi,t+1)



f¯i,t (xi,t ) -

t (2 - 1) 2¯

GX (xi,t , f¯i,t , t )

2

+

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

+

 t2 2 2

GX (xi,t , f¯i,t , t ) 2.

(53)

Let i,t = f¯i,t (xi,t ) - f¯i,t (xt) denotes the instantaneous loss at round t. We have

i,t+1

=

t

t +1

( f¯i,t (xi,t+1) -

f¯i,t (xt+1)) + t

1 +

1

(

fi,t+1(xi,t+1)

-

fi,t+1(xt+1)).

(I)

(54)

Note that (I) can be bounded as follows:

f¯i,t (xi,t+1) - f¯i,t (xt+1) 

f¯i,t (xi,t+1)

-

f¯i,t (xt)



i,t

- [ (2

- 1)t 2¯

-

 t2 2 2

]

GX (xi,t , f¯i,t , t )

2

+

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

,

(55)

22

Under review as a conference paper at ICLR 2019

where the first inequality is due to the optimality of xt and the second inequality follows from equation 53.

Now, combining equation 55 and equation 54 gives

i,t+1



t

t + 1 (i,t

- [ (2 - 1)t 2¯

-

 t2 2 2

]

GX (xi,t , f¯i,t , t )

2

+

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

)

+

t

1 +

1

(

fi,t+1(xi,t+1)

-

fi,t+1(xt+1)).

By rearranging the above inequality, we have

t

t +

(2 [ 1

- 1)t 2¯

-

 t2 2 2

]

GX (xi,t , f¯i,t , t )

2



t

t + 1 (i,t

+

2(1

-

1,t

t 1,t ¯ )(1 - )2

(1

-

2

)

)

+

t

1 +

1

(

fi,t+1(xi,t+1)

-

fi,t+1(xt+1))

-

i,t+1.

(56)

Using

the

definition

of

i,t+1,

we

get

1 t+1

(

fi,t+1(xi,t+1) -

fi,t+1(xt+1)) - i,t+1

=

-(

t t+1

)(

f¯i,t (xi,t+1)-

f¯i,t (xt+1)). Hence, using equation 56 and simplifying terms, we obtain

[

(2

- 1 2¯

)t

-

 t2 2 2

]

GX (xi,t , f¯i,t , t )

2



i,t

-

(

f¯i,t (xi,t+1)

-

f¯i,t (xt+1))

+

2(1 -

t 1,t ¯ 1,t )(1 - )2(1

- 2)

.

(57)

Observe that:

 1 n T
n i=1 t=1

i,t - ( f¯i,t (xi,t+1) - f¯i,t (xt+1))

 1 n T
= n i=1 t=1

( f¯i,t (xi,t ) - f¯i,t (xi,t+1)) - ( f¯i,t (xt) - f¯i,t (xt+1))

=

1 n

n
[-
i=1

f¯i,T (xi,T +1) +

f¯i,1(xi,1) -

f¯i,1(x1) +

f¯i,T (xT +1)]

 1 n
+

T
t -1

n i=1 t=2

fi,t (xi,t ) - f¯i,t-1(xi,t ) - ( fi,t (xt) - f¯i,t-1(xt))

  1 n L
n i=1

T
xi,T +1 - xT +1 + xi,1 - x1 + 2t-1 xi,t - xt
t=2

(58)

T

 2L max Bt (1 + t-1)  2L max Bt (2 + log T ),

t{2,...,T }

t=2

t{2,...,T }

(59)

where the first equality uses

f¯i,t (xi,t ) - f¯i,t-1(xi,t ) = t-1( fi,t (xi,t ) - f¯i,t-1(xi,t )).

The first inequality follows from Assumption 2. The second inequality follows from Lemma 19 and equation 18.

Summing equation 57 over i  V , t  {1, . . . , T } and using equation 59, we have

 1 n
min n i=1 t{1,...,T }

GX (xi,t , f¯i,t , t )

2

T
[
t=1

(2

- 1 2¯

)t

-

 t2 2 2

]

  1 n T [ (2 - 1)t
n i=1 t=1 2¯

-

 t2 2 2

]

GX (xi,t , f¯i,t , t )

2



(2

+

log

T

)2L

max
t{2,...,T }

(1

-



 2n ) (1

-

2)

t-1 s=0

s2t-s-1(W

)

+

t

T =1

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

.

(60)

23

Under review as a conference paper at ICLR 2019

Note

that

T
t=1

[

(2-1 )t 2¯

-

 t2 2 2

]

>

0.

Therefore, dividing both sides of the equation 60 by

tT=1

[

(2-1 2¯

)t

-

 t2 2 2

],

we

obtain

equation

13.

6.1.4 PROOF OF COROLLARY 8

Proof.

With the constant step-sizes t

=

(2-1 ) 2 2 ¯

for all t

 {1, . . . , T }, we have

t

= T [ (2 - 1)t t=1 2¯

-

 t2 2 2

]

=

T

(2 - 1)2 8¯ 2



2

.

Therefore,

using

the

above

equality

and

equation

11

together

with

t =

(2-1 ) 2 2 ¯

for

all t 

{1, . . . , T }, we obtain

 1
t

T t=1

t 1,t ¯ 2(1 - 1,t )(1 - )2(1 - 2)

=

4¯ T (2 - 1)

T t=1

1,t ¯ 2(1 - 1,t )(1 - )2(1 - 2)



4¯ T (2 -

1)

T t=1

2(1

 t-1¯ - 1)(1 - )2(1

-

2)



T (2

-

1)(1

2¯ 2 - 1)(1 - )2(1

-

2)(1 -

, )

and

1
t

(2

+

log

T

)2L

max
t{2,...,T }

(1

-

 2n ) (1

-

2)

t-1 s=0

s2t-s-1(W

)



T

(2

16¯ (2 + log - 1)(1 - 

T )

 )L n
(1 -

max 2) t{2,...,T

}

t-1 s=0

2t-s-1(W

)



16¯ (2 + log T )L n

.

T (2 - 1)(1 - ) (1 - 2)(1 - 2(W ))

Combine the results in equation 61 and equation 62, we have equation 12.

(61) (62)

6.1.5 PROOF OF THEOREM 9

Proof. Let

i,t =  f¯i,t (xi,t ) -  f¯i,t (xi,t ), t  1,

where



f¯i,t (xi,t )

=

1 t

ts=1



fi,s(xi,t )

denotes

the

minibatch

stochastic

gradient

on

node

i.

Since fi,t is -smooth, it follows that f¯i,t is also -smooth. Hence for every i  V and t  {1, . . . , T }, we have

f¯i,t (xi,t+1)  f¯i,t (xi,t ) +

 f¯i,t (xi,t ), xi,t+1 - xi,t

+ 2

xi,t+1 - xi,t

2

 f¯i,t (xi,t ) -

t ^i,t

 f¯i,t (xi,t ), GX (xi,t , f¯i,t , t )

+

 t2 2^i,t

GX (xi,t , f¯i,t , t ) 2

= f¯i,t (xi,t ) -

t ^i,t

 f¯i,t (xi,t ), GX (xi,t , f¯i,t , t )

+ t2 2^i,t

GX (xi,t , f¯i,t , t )

2

+

t ^i,t

i,t , GX (xi,t , f¯i,t , t ) ,

where GX (xi,t , f¯i,t , t ) denotes the projected stochastic gradient on node i.

24

Under review as a conference paper at ICLR 2019

The above inequality together with Lemma 20, equation 10 and 1,t  1, imply that

f¯i,t (xi,t+1)



f¯i,t (xi,t ) -

t (2 - 1) 2¯

GX (xi,t , f¯i,t , t )

2

+

2(1

-

t 1,t ¯ 1,t )(1 - )2(1

-

2)

+

 t2 2 2

GX (xi,t , f¯i,t , t )

2

+

t ^i,t

i,t , GX (xi,t , f¯i,t , t )

+

t ^i,t

i,t , GX (xi,t , f¯i,t , t ) - GX (xi,t , f¯i,t , t )



f¯i,t (xi,t ) -

t (2 - 1) 2¯

GX (xi,t , f¯i,t , t )

2

+

2(1

-

t 1,t ¯ 1,t )(1 - )2(1

-

2)

+

 t2 2 2

GX (xi,t , f¯i,t , t )

2

+

t ^i,t

i,t , GX (xi,t , f¯i,t , t )

+

t ^i,t i,t

GX (xi,t , f¯i,t , t ) - GX (xi,t , f¯i,t , t )



f¯i,t (xi,t ) -

t (2 - 1) 2¯

GX (xi,t , f¯i,t , t )

2

+

2(1

-

t 1,t ¯ 1,t )(1 - )2(1

-

2)

+

 t2 2 2

GX (xi,t , f¯i,t , t )

2

+

t ^i,t

i,t , GX (xi,t , f¯i,t , t )

+

t ^i,t

i,t

¯ 

t
(1
r=1

-

1,r )1t -r

i,r

,

(63)

where the second inequality follows from the Cauchy-Schwartz inequality. The last inequality follows from the fact that 1,t = 1 t-1,   (0, 1) and Lemma 18 if we set GX1 (xi, f¯i, ) = GX (xi,t , f¯i,t , t ), and G2X (xi, f¯i, ) = GX (xi,t , f¯i,t , t ).

Let i,t = f¯i,t (xi,t ) - f¯i,t (xt) denotes the instantaneous stochastic loss at time t. We have

i,t+1

=

t

t +1

( f¯i,t (xi,t+1) -

f¯i,t (xt+1)) + t

1 +

1

(

fi,t+1(xi,t+1)

-

fi,t+1(xt+1)).

(64)

(I)

Observe that (I) can be bounded as follows:

f¯i,t (xi,t+1) - f¯i,t (xt+1) 

f¯i,t (xi,t+1)

-

f¯i,t (xt)



i,t

- [ (2

- 1)t 2¯

-

 t2 2 2

]

GX (xi,t , f¯i,t , t )

2

+

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

+

t ^i,t

i,t , GX (xi,t , f¯i,t , t )

+

¯ t 2

t
1t-r
r=1

i,r

i,t ,

(65)

where the first inequality is due to xt+1  X and the optimality of xt and the second inequality is due to equation 63 and the fact that 0  1,r < 1. Thus, using equation 64 and equation 65, we get

i,t+1



t

t +1

i,t

-

[

(2

- 1)t 2¯

-

 t2 2 2

]

GX (xi,t , f¯i,t , t )

2

+

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

+

t ^i,t

i,t , GX (xi,t , f¯i,t , t )

+

¯ t 2

t
1t-r
r=1

i,r

i,t

+

t

1 +

1

(

fi,t+1(xi,t+1)

-

fi,t+1(xt+1)).

By rearranging the above inequality, we have

t [ (2 - 1)t t + 1 2¯

-

 t2 2 2

]

GX (xi,t , f¯i,t ,t )

2 t t+1

i,t

+

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

+

t ^i,t

i,t , GX (xi,t , f¯i,t , t )

+

¯ t 2

t
1t-r
r=1

i,r

i,t

+

t

1 +

1

(

fi,t+1(xi,t+1)

-

fi,t+1(xt+1))

-

i,t+1.

(66)

Now,

using

definition

of

i,t+1,

1 t+1

(

fi,t+1(xi,t+1

)

-

fi,t+1(xt+1)) -

i,t+1

=

-(

t t+1

)(

f¯i,t

(xi,t+1

)

-

f¯i,t (xt+1)), together with equation 66, we obtain

(2 [

- 1)t 2¯

-

 t2 2 2

]

GX (xi,t , f¯i,t , t )

2  i,t - ( f¯i,t (xi,t+1) - f¯i,t (xt+1))

+

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

+

t ^i,t

i,t , GX (xi,t , f¯i,t , t )

+

¯ t 2

t
1t-r
r=1

i,r

i,t .

(67)

25

Under review as a conference paper at ICLR 2019

Note that from Assumption 3, we have E i,t , GX (xi,t , f¯i,t , t ) = 0 and

E  fi,t (xi,t ) -  fi,t (xi,t ) 2 = E  fi,t (xi,t ) 2 - E [ fi,t (xi,t )] 2  E  fi,t (xi,t ) 2   2,

which implies that

E i,t 2 = E  f¯i,t (xi,t ) -  f¯i,t (xi,t ) 2

= E

1 t

t
(
s=1

fi,s

(xi,t

)

-



fi,s

(xi,t

))

2



1 t

t
E
s=1

 fi,s(xi,t ) -  fi,s(xi,t ) 2   2.

The above inequality together with Cauchy-Schwarz for expextations, imply that

E [ i,t i,r ]  E [ i,r 2] E [ i,t 2]   2.

(68)

Now, using equation 68 and equation 59, summing equation 67 over i  V and t  {1, . . . , T }, and taking expectation, we obtain

 1 n

n

min
i=1 t{1,...,T

}

E

GX (xi,t , f¯i,t , t )

2

T
[
t=1

(2

- 1 2¯

)t

-

 t2 2 2

]

  T [ (2 - 1)t
t=1 2¯

-

 t2 2 2

]

1 n

n
E
i=1

GX (xi,t , f¯i,t , t ) 2



(2

+

log

T

)2L

max
t{2,...,T }

(1

-



 2n ) (1

-

2)

t-1 s=0

s2t-s-1(W

)

  +

T t=1

2(1

-

1,t

t 1,t ¯ )(1 - )2(1

-

2)

+

¯  2 2

T t=1

t

t r=1

1t -r .

Note

that

tT=1

[

(2-31 )t 2¯

-

 t2 2 2

]

>

0,

and

(69)

    T
t
t=1

t
1t-r
r=1

=

T
t
t=1

T
1r-t
r=t



T t=1

t . (1 - 1)

(70)

Therefore,

dividing

both

sides

of

the

equation

71

by

tT=1

[

(2-31 )t 2¯

-

 t2 2 2

],

using

equation

70,

we

complete the proof.

6.1.6 PROOF OF COROLLARY 10

Proof. Recall equation 58, equation 67 and equation 68. These expressions lead to the following inequality:

 1 n

T

n

min
i=1 t{1,...,T

}

E

GX (xi,t , f¯i,t , t )

2

T
[
t=1

(2

- 1 2¯

)t

-

 t2 2 2

]

 

1 T

T [ (2 - 1)t t=1 2¯

-

 t2 2 2

]

1 n

n
E
i=1

GX (xi,t , f¯i,t , t ) 2

 

1n L(
T n i=1

xi,T +1 - xT +1

+

xi,1 - x1

T
+ 2t-1
t=2

xi,t - xt

)

 +

1 T

T t=1

t 1,t ¯ 2(1 - 1,t )(1 - )2(1 - 2)

+

¯  2 2(1 - 1)T

T
t .
t=1

(71)

We

first

show

t

=

tT=1[

(2-1 2¯

)t

-

 t2 2 2

]

is

approximately

constant

when

equation

15b

is

satisfied.

Observe that

(2 - 1)t 2¯

-

 t2 2 2



1 2T

 t



1 ,
2

26

Under review as a conference paper at ICLR 2019

which mean that

-T t2¯ + T 2(2 - 1)t - ¯ 2  0,

equivalently

1



t ¯ 2(2 - 1)

+

T (2

¯ - 1)t

.

The

requirements

on

t

are

given

by

making

each

of

the

last

two

terms

smaller

than

1 2

:

t ¯ 2(2 - 1)



1 2



t





2

(2 - 1 2 ¯

)

,

¯  1 T (2 - 1)t 2



t



2¯ . T (2 - 1)

By

selecting

t

=

1 nT

,

we

have

T



max{

n

4 2 ¯ 4(2 -

2
1)2

,

4¯ 2n (2 - 1)2

}.

t



1 2

will

be

satisfied.

We

proceed

by

substituting

t

=

 nT

,

1,t

=

1 t-1, 



(0, 1)

and

t



1 2

into equation 71 and using Lemma 19, yields

2¯  2
2(1 - 1)T

T
t
t=1

=

2

¯



2


2(1 - 1) nT

,

(72)

and

 1
T

T t=1

t 1,t ¯ (1 - 1,t )(1 - )2(1 - 2)

=

 ¯ T nT (1 - )2(1 - 2)

T t=1

1,t (1 - 1,t )

=

 ¯

,

T nT (1 - )2(1 - 2)(1 - 1)(1 -  )

(73)

also

 2L n
T n i=1

xi,T +1 - xT +1



T (1

-

 4 nL  ) (1 -2) nT

T
2T -s(W )
s=0



4 nL

.

T (1 - ) (1 - 2) nT (1 - 2(W ))

(74)

Given equation 15a, the equation 73 and equation 74 are bounded by equation 72. Moreover

 4L

n

T
t -1

T n i=1 t=2

xi,t - xt

 

  8 nL
nT (1 - ) (1 - 2)T

T t-1
t-1 2t-s-1(W )
t=2 s=0

   8 nL
nT (1 - ) (1 - 2)T 

T
 t-2
t=2 

T t-1
 ( 2t-s-1(W ))2
t=2 s=0



 8 nL nT (1 - ) (1 - 2)T

2T (1 - 2(W )) ,

where the first inequality follows from Lemma 19. The second inequality is due to Cauchy-Schwarz

inequality.

The

last

inequality

holds

because

tT=1

1 t2



2-

1 T



2.

This

completes

the

proof.

27

