Under review as a conference paper at ICLR 2019
HIERARCHICAL GENERATIVE MODELING FOR CONTROLLABLE SPEECH SYNTHESIS
Anonymous authors Paper under double-blind review
ABSTRACT
This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.
1 INTRODUCTION
Recent development of neural end-to-end TTS models has shown promising results in generating high fidelity speech without the need of handcrafted linguistic features (Sotelo et al., 2017; Wang et al., 2017; Arik et al., 2017; Shen et al., 2018). These models rely heavily on the encoder-decoder neural network structure (Sutskever et al., 2014) that maps a text sequence to a sequence of speech frames. Extensions to these models have shown that attributes such as speaker identity can be controlled by conditioning the decoder on additional attribute labels (Arik et al., 2017; 2018; Jia et al., 2018).
There are many speech attributes aside from speaker identity that are difficult to annotate, such as speaking style, prosody, recording channel, and noise levels. Skerry-Ryan et al. (2018); Wang et al. (2018) model such latent attributes through conditional auto-encoding: in addition to text and a speaker label, a vector inferred from the target speech is passed to the decoder as input, which aims to capture the residual attributes that are not specified by other input streams. Such models have shown convincing results in synthesizing speech that resembles the prosody or the noise conditions of the reference speech, which may not have the same text or speaker identity as the target speech.
Nevertheless, the presence of multiple latent attributes is common in crowdsourced data (Panayotov et al., 2015), in which prosody, speaker, and noise conditions can vary simultaneously. In such scenarios, simply copying the latent attributes from a reference is insufficient if one desires to synthesizing speech that mimics the prosody of one reference, but is in the same noise condition as another. Learning a disentangled latent representation would enable control of independent generating factors. It is also desirable to construct a systematic method for synthesizing speech with random latent attributes, which can facilitate data augmentation (Tjandra et al., 2017; 2018; Hsu et al., 2017b; 2018; Hayashi et al., 2018) by providing more diverse examples. Neither of these properties were explicitly addressed in the previous studies, which model variation of a single latent attribute.
The objectives of this work are as follows: (1) constructing a continuous latent space of disentangled attribute representations, where each dimension controls a different generating factor; (2) discovering a set of interpretable clusters, each of which is a representative mode (e.g., one cluster for clean speech and another for noisy speech); (3) providing a systematic sampling mechanism for attribute representations. All three are achieved by introducing a Gaussian Mixture Variational Auto-Encoder (GMVAE) to Tacotron 2 (Shen et al., 2018). The proposed model is extensively evaluated on four datasets with subjective and objective quantitative metrics, as well as comprehensive qualitative
1

Under review as a conference paper at ICLR 2019

text observed class
speech

latent class
latent repr

text speech

observed class
observed repr

latent class
latent repr

Figure 1: Graphical model representation of the proposed models. Observed class often corresponds to the speaker label. The left illustrates equation 1, and the right illustrates the extension from Section 2.3. The grey and white nodes correspond to observed and latent variables.

studies. Experiments confirm that GMVAE-Tacotron is capable of controlling speaker, noise, and style independently, even when variation of all attributes is present but unannotated in the train set.

2 MODEL

Tacotron-like TTS systems take a text sequence yt and optional observed conditioning information yo as input (e.g. speaker identity), and predict a sequence of acoustic feature frames X. Training such a system can be regarded as fitting a probabilistic model p(X | yt, yo) that maximizes the likelihood of generating the training data. If there are other unlabeled attributes such as prosody, such a model is
effectively integrating out those latent attributes and producing a conditional distribution with higher
variance. As a result, the model would opaquely produce speech with unpredictable latent attributes.
To enable control of those attributes, we adopt a graphical model with hierarchical latent variables,
which captures such attributes. Below we explain how the formulation can achieve interpretability,
disentanglement, and sampling capability, and propose efficient inference and training methods.

2.1 CONDITIONAL GENERATIVE MODEL WITH HIERARCHICAL LATENT VARIABLES

Two latent variables yl and zl are introduced in addition to the observed variables, X, yt, and yo, as shown in the graphical model in the left of Figure 1. yl is a K-way categorical discrete variable, named latent attribute class, and zl is a D-dimensional continuous variable, named latent attribute representation. To generate speech X conditioned on the text yt and observed attribute yo, yl is first sampled from its prior, p(yl), then a latent attribute representation zl is sampled from the conditional distribution p(zl | yl). Finally, a sequence of speech frames is drawn from p(X | yt, yo, zl), parameterized by the decoder neural network. The joint probability can be written as:

p(X, yl, zl | yt, yo) = p(X | yt, yo, zl) p(zl | yl) p(yl)

(1)

Specifically, it is assumed that p(yl) = K-1 to be a non-informative prior to encourage every component to be used, and p(zl | yl) = N (µyl , diag(yl )) to be diagonal-covariance Gaussian with learnable means and variances. As a result, the marginal prior of zl becomes a GMM with diagonal covariances and equal mixture weights. We hope this GMM latent model can better capture the
complexity of unseen attributes. Furthermore, in the presence of natural clusters of unseen attributes,
the proposed model can achieve interpretability by learning to assign instances from different clusters
to different mixture components. The covariance matrix of each mixture component is constrained to
be diagonal to encourage each dimension to capture a statistically uncorrelated factor.

2.2 VARIATIONAL INFERENCE AND TRAINING
The observation model, p(X | yt, yo, zl), is parameterized with a neural network. Following the framework of VAE (Kingma & Welling, 2014), a variational distribution q(yl | X)q(zl | X) is used to approximate the posterior p(yl, zl | X, yt, yo), which assumes that the posterior of unseen attributes is independent of the text and observed attributes. The approximated posterior for zl, q(zl | X), is modeled as a Gaussian distribution with diagonal covariance matrix, whose mean and variance are parameterized by a neural network. For q(yl | X), instead of introducing another neural network, we configure it to be an approximation of p(yl | X) that reuses q(zl | X) as follows:
p(yl|X) = p(yl | zl) p(zl|X) dzl = Ep(zl|X) [p(yl | zl)]  Eq(zl|X) [p(yl | zl)] := q(yl|X) (2)
zl

2

Under review as a conference paper at ICLR 2019
which enjoys the closed-form solution of Gaussian mixture posteriors, p(yl | zl). Similar to VAE, the model is trained by maximizing its evidence lower bound (ELBO), as follows:
L(p, q; X, yt, yo) = Eq(zl|X)[log p(X | yt, yo, zl)] - Eq(yl|X)[DKL(q(zl | X) || p(zl | yl))] - DKL(q(yl | X) || p(yl)) (3)
where q(zl | X) is estimated via Monte Carlo sampling, and all components are differentiable thanks to reparameterization. Details can be found in Appendix A.
2.3 A CONTINUOUS ATTRIBUTE SPACE FOR CATEGORICAL OBSERVED LABELS
Categorical observed labels, such as speaker identity, can often be seen as a categorization from a continuous attribute space, which for example could model a speaker's characteristic F0 range and vocal tract shape. Given an observed label, there may still be some variation of these attributes. We are interested in learning this continuous attribute space for modeling within-class variation and inferring a representation from an instance of an unseen class for one-shot learning.
To achieve this, a continuous latent variable, zo, named the observed attribute representation, is introduced between the observed label yo and speech X, as shown on the right of Figure 1. Each observed class forms a mixture component in this continuous space, whose conditional distribution is a diagonal-covariance Gaussian p(zo | yo) = N (µyo , diag(yo )). With this formulation, speech from an observed class yo is now generated by conditioning on yt, zl, and a sample zo drawn from p(zo | yo). As before, a variational distribution q(zo | X), parameterized by a neural network, is used to approximate the true posterior, where the ELBO becomes:
Lo(p, q; X, yt, yo) = Eq(zo|X)q(zl|X)[log p(X | yt, zo, zl)] - DKL(q(zo | X) || p(zo | yo)) - Eq(yl|X)[DKL(q(zl | X) || p(zl | yl))] - DKL(q(yl | X) || p(yl)). (4)
To encourage zo to disentangle observed attributes from latent attributes, the variances of p(zo | yo) are initialized to be smaller than those of p(zl | yl). The intuition is that this space should capture variation of attributes that are highly correlated with the observed labels, so the conditional distribution of all dimensions should have relatively small variance for each mixture component. Experimental results verify the effectiveness, and similar design is used in Hsu et al. (2017a). In the extreme case where the variance is fixed and approaches zero, this formulation converges to using an lookup table.
2.4 NEURAL NETWORK ARCHITECTURE
The observation model, p(X | yt, yo, zl) or p(X | yt, zo, zl), is adopted from a sequence-to-sequence Tacotron 2 architecture (Shen et al., 2018), with extra input zl and yo (or zo) concatenated and passed to the decoder at each step. Text yt and speech X are represented as a sequence of phonemes and a sequence of mel-scale filterbank coefficients, respectively. For fast inference, we use a WaveRNNbased neural vocoder (Kalchbrenner et al., 2018) instead of WaveNet (van den Oord et al., 2016).
The two posteriors, q(zl | X) and q(zo | X), are both parameterized by a recurrent encoder that maps a variable-length mel-spectrogram to two fixed-dimensional vectors, corresponding to the posterior mean and log variance, respectively. Full architecture details can be found in Appendix B.
3 RELATED WORK
The proposed GMVAE-Tacotron is most related to Skerry-Ryan et al. (2018), Wang et al. (2018), Henter et al. (2018), and Akuzawa et al. (2018), which introduce a reference embedding to model prosody or noise. The first uses an autoencoder to extract a prosody embedding from a reference speech spectrogram. The second Global Style Token (GST) model constrains a reference embedding to be a weighted combination of a fixed set of learned vectors, while the third further restrict the weights to be one-hot, and is built on a conventional parametric speech synthesizer (Zen et al., 2009). The main focus of these approaches was style transfer from a reference audio. They provide neither a systematic sampling mechanism nor disentangled representations as we show in Section 4.3.1. The last model on the other hand adopts a Gaussian prior, which enables sampling, but does not provide interpretability, nor does it evaluate disentangled control. In contrast, the proposed model can
3

Under review as a conference paper at ICLR 2019

achieve interpretability by modeling different mixture components, and promotes disentanglement by encouraging statistical independence between each dimension.
The proposed formulation for learning an observed attribute embedding zo for speaker modeling is also related to Arik et al. (2018), which controls the speaker identity with speaker embeddings, and trains a separate regression model to predict them from the audio. This can be regarded as a special case of the proposed model where the variance of zo is set to be almost zero, such that a speaker always generates a fixed representation; meanwhile, the posterior model q(zo | X) corresponds to their embedding predictor, because it now aims to predict a fixed embedding for each speaker.
Using a mixture distribution for latent variables in a VAE was explored in Dilokthanakul et al. (2016); Nalisnick et al. (2016), and Jiang et al. (2017) for unconditional image generation and text topic modeling. These models correspond to the sub-graph yl  zl  X in Figure 1. The proposed model provides extra flexibility to model both latent and observed attributes in a conditional generation scenario. Hsu et al. (2017a) similarly learned disentangled representations at the variable level (i.e. disentangling zl and zo) by defining different priors for different latent variables. Higgins et al. (2017) also used a prior with diagonal covariance matrix to disentangle different embedding dimensions. Our model provides additional flexibility by learning a different variance in each mixture component.

4 EXPERIMENTS

The proposed GMVAE-Tacotron was evaluated on four datasets, spanning a wide degree of variations in speaker, recording channel conditions, background noise, prosody, and speaking styles. For all experiments, yl was configured to be a 10-way categorical variable (K = 10), and zl and zo (if used) were configured to be 16-dimensional variables (D = 16). Tacotron 2 (Shen et al., 2018) with a speaker embedding table was used as the baseline for all experiments. For all other variants (e.g., GST), the reference encoder follows Wang et al. (2018). Each model was trained for at least 200k steps to maximize the ELBO in equation 3 or equation 4 using the Adam optimizer. A list of detailed hyperparameter settings can be found in Appendix C. Quantitative subjective evaluations relied on crowd-sourced mean opinion scores (MOS) rating the naturalness of the synthesized speech by native speakers using headphones, with scores ranging from 1 to 5 in increments of 0.5. For single speaker datasets each sample was rated by 6 raters, while for other datasets each sample was rated by a single rater. We strongly encourage readers to listen to the samples on the demo page.1

4.1 MULTI-SPEAKER ENGLISH CORPUS

To evaluate the ability of GMVAE-Tacotron to model speaker variation and discover meaningful speaker clusters, we used a proprietary dataset of 385 hours of high-quality English speech from 84 professional voice talents with accents from the United States (US), Great Britain (GB), Australia (AU), and Singapore (SG). Speaker labels were not seen during training (yo and zo were unused), and were only used for evaluation.

To probe the interpretability of the model, we computed the distribution of mixture components yl for utterances of a particular accent or gender. Specifically, we collected at most 100 utterances from each of the 44 speakers with at least 20 test utterances (2,332 in total), and assigned each utterance to the component with the highest posterior probability: arg maxyl q(yl|X).
Figure 2 plots the assignment distributions for each gender and accent in this set. Most components were only used to model speakers from one gender. Each component which modeled both genders (0, 2, and 9) only represented a subset of accents (US, US, and AU/GB, respectively). We also found that the several components which modeled US female speakers (3, 5, and 6) actually modeled groups of speakers with distinct characteristics, e.g. different F0 ranges. To quantify the association between speaker and mixture components, we computed the

Figure 2: Assignment distribution over yl for each gender (upper) and for each accent (lower).

1https://x10q92sh3d.github.io/gmvae_tacotron

4

Under review as a conference paper at ICLR 2019

assignment

consistency

w.r.t.

speaker:

1 M

N i=1

Ni j=1

1yij =y^i

where

M

is

the

number

of

utterances,

yij is the component assignment of utterance j from speaker i, and y^i is the mode of {yij}jN=i1 . The

resulting consistency was 92.9%, suggesting that the mixture components learned to group utterances

by speaker and group speakers by gender or accent attributes.

We also explored what each dimension of zl controlled by decoding with different values of the target dimension, keeping all other factors fixed. We discovered that there were individual dimensions which controlled F0, speaking rate, accent, length of starting silence, etc., demonstrating the disentangled nature of the learned latent attribute representation. Appendix D contains visualization of attribute control and additional quantitative evaluation of using zl for gender/accent/speaker classification.

4.2 NOISY MULTI-SPEAKER ENGLISH CORPUS
High quality data can be both expensive and time consuming to record. Vast amounts of rich real-life expressive speech are often noisy and difficult to label. In this section we demonstrate that our model can synthesize clean speech directly from noisy data by disentangling the background noise level from other attributes, allowing it to be controlled independently. As a first experiment, we artificially generated training sets using a room simulator (Kim et al., 2017) to add background noise and reverberation to clean speech from the multi-speaker English corpus used in the previous section. We used music and ambient noise sampled from YouTube and recordings of "daily life" environments as noise signals, mixed at signal-to-noise ratios (SNRs) ranging from 5­25dB. The reverberation time varied between 100 and 900ms. Noise was added to a random selection of 50% of utterances by each speaker, holding out two speakers (one male and one female) for whom noise was added to all of their utterances. This construction was used to evaluate the ability of the model to synthesize clean speech for speakers whose training utterances were all corrupted by noise. In this experiment, we provided speaker labels yo as input to the decoder, and only expect the latent attribute representations zl to capture the acoustic condition of each utterance.

4.2.1 IDENTIFYING MIXTURE COMPONENTS THAT GENERATE CLEAN/NOISY SPEECH
Unlike clustering speakers, we expected that latent attributes would naturally divide into two categories: clean and noisy. To verify this hypothesis, we plotted the Euclidean distance between means of each pair of components on the left of Figure 3, which clearly form two distinct clusters. The right two plots in Figure 3 show the mel-spectrograms of two synthesized utterances of the same text and speaker, conditioned on the means of two different components, one from each group. It clearly presents the samples (in fact, all the samples) drawn from components in group one were noisy, while the samples drawn from the other components were clean. See Appendix E for more examples.

Figure 3: Left: Euclidean distance between the means of each mixture component pair. Right: Decoding the same text conditioned on the mean of a noisy (center) and a clean component (right).

4.2.2 CONTROL OF THE BACKGROUND NOISE LEVEL

We next explored if the level of noise was dominated by a single latent dimension, and

whether we could determine such a dimension automatically. For this purpose, we adopted

a per-dimension LDA, which computed a between and within-mixture scattering ratio: rd =

K yl =1

p(yl)(µyl,d

-

µ¯ d )2

/

K yl

=1

p(yl

)y2l

,d

,

where

µyl,d

and

yl,d

are

the

d-th

dimension

mean and variance of mixture component yl, and µ¯d is the d-th dimension marginal mean. This is a

scale-invariant metric of the degree of separation between components in each latent dimension.

We discovered that the most discriminative dimension had a scattering ratio r13 = 21.54, far larger than the second largest, which had r11 = 0.64. Drawing samples and traversing different values along

5

Under review as a conference paper at ICLR 2019

Table 1: MOS and SNR comparison among baseline, GST, VAE, and GMVAE models.

Figure 4: Mel-spectrograms of Figure 5: Estimated traversing the noise level dimension SNR with respect to with three values: -1, -0.5 and 0. different values for the
noise level dimension.

Model
Baseline GST VAE GMVAE

MOS
2.87 ± 0.25 3.32 ± 0.13 3.55 ± 0.17 4.25 ± 0.13

SNR
11.56 14.43 12.91 17.20

the 13th dimension, while keeping other dimensions fixed demonstrated the effect of this dimension. Figure 4 shows that the noise-level was clearly controlled by manipulating this dimension while the underlying speech content was unchanged. To quantify the noise control results, we used waveform amplitude distribution analysis (WADA) (Kim & Stern, 2008) to estimate an SNR without a reference clean signal. Figure 5 plots the average estimated SNR over 200 utterances from two speakers as the noise level dimension value was varied, which matched the qualitative observations.
4.2.3 SYNTHESIZING CLEAN SPEECH FOR NOISY SPEAKERS
In this section, we evaluated synthesis quality for the two held out noisy speakers. Evaluation metrics included subjective naturalness MOS ratings and an objective SNR metric. Table 1 compares the proposed model with a baseline, a 16-token GST, and a VAE variant which replaces the GMM prior with an isotropic Gaussian. To encourage synthesis of clean audio under each model we manually selected the cleanest token (weight=0.15) for GST, used the Gaussian prior mean (i.e. a zero vector) for VAE, and the mean of a clean component for GMVAE. For the VAE model, the mean captured the average condition, which still exhibited a moderate level of noise, resulting in a lower SNR and MOS. The generated speech from the GST was cleaner, however raters sometimes found its prosody to be unnatural. Note that it is possible that another token would obtain a different trade-off between prosody and SNR, and using multiple tokens could improve both. Finally, the proposed model synthesized both natural and high-quality speech, with the highest MOS and SNR.
4.3 SINGLE-SPEAKER AUDIOBOOK CORPUS
Prosody and speaking style is another important factor for human speech other than speaker and noise. Control of these aspects of the synthesize speech is essential to building an expressive TTS system. In this section, we evaluated the ability of the proposed model to sample and control speaking styles. A single speaker US English audiobook dataset of 147 hours, recorded by professional speaker, Catherine Byers, from the 2013 Blizzard Challenge (King & Karaiskos, 2013) is used for training. The data incorporated a wide range of prosody variation. We used an evaluation set of 150 audiobook sentences, including many long phrases. Table 2 shows the naturalness MOS between baseline and proposed model conditioning on the same zl, set to the mean of a selected yl, for all utterances. The results show that the prior already captured a common prosody, which could be used to synthesize more naturally sounding speech with a lower variance compared to the baseline.
4.3.1 STYLE SAMPLING AND DISENTANGLED CONTROL
Compared to GST, one primary advantage of the proposed model is that it supports random sampling of natural speech from the prior. Figure 6 illustrates such samples, where the same text is synthesized with wide variation in speaking rate, rhythm, and F0. In contrast, the GST model does not define a prior for normalized token weights, requiring weights to be chosen heuristically or by fitting a distribution after training. Empirically we found that the GST weight simplex was not fully exploited during training and that careful tuning was required to find a stable sampling region.
An additional advantage of GMVAE-Tacotron is that it learns a representation which disentangles these attributes, enabling them to be controlled independently. Specifically, latent dimensions in the proposed model are conditionally independent, while token weights of GST are in fact correlated. Figure 7(b) contains an example of the proposed model traversing the "speed" dimension with three
6

Under review as a conference paper at ICLR 2019

Table 2: MOS comparison of the baseline and GMVAE.

Model

MOS

Baseline 4.29 ± 0.11 Proposed 4.67 ± 0.07

Figure 6: Mel-spectrograms of three samples with the same text, "We must burn the house down! said the Rabbit's voice." drawn from the proposed model, showing variation in speed, F0, and pause duration.

(a) (b)
Figure 7: (a) Mel-spectrograms of two unnatural GST samples when setting the weight for one token -0.1: first with tremolo at the end, and second with abnormally long duration for the first syllable. (b) F0 tracks and spectrograms from GMVAE-Tacotron using different values for the "speed" dimension.
values: µ - 2, µ, µ + 2, plotted accordingly from left to right. Their F0 tracks, obtained using the YIN (De Cheveigne´ & Kawahara, 2002) F0 tracker, are shown on the left. From these we can observe that the shape of the F0 contours did not change. They were simply stretched horizontally, indicating that only the speed was manipulated. In contrast, the style control of GST is more entangled, as shown in Figure 3(a) of Wang et al. (2018), where the F0 also changed while controlling speed.
Additional evaluation of style transfer can be found in Appendix F, demonstrating the ability of the proposed the model to synthesize speech that resembles the prosody of a given reference utterance.
4.4 CROWD-SOURCED AUDIOBOOK CORPUS
We used an audiobook dataset2 derived from the same subset of LibriVox audiobooks used for the LibriSpeech corpus (Panayotov et al., 2015), but sampled at 24kHz and segmented differently, making it appropriate for TTS instead of speech recognition. The corpus contains recordings from thousands of speakers, with wide variation in recording conditions and speaking style. Speaker identity is often highly correlated with the recording channel and background noise level, since many speakers tended to use the same microphone in a consistent recording environment. The ability to disentangle and control these attributes independently is essential to synthesizing high-quality speech for all speakers.
We augmented the model with the zo layer described in Section 2.3 to learn a continuous speaker representation and an inference model for it. The train-clean-{100,360} partitions were used for training, which spans 1,172 unique speakers and, despite the name, includes many noisy recordings. As in previous experiments, by traversing each dimension of zl we found that different latent dimensions independently control different attributes of the generated speech. Moreover, this representation was disentangled from speaker identity, i.e. modifying zl did not affect the generated speaker identity if zo was fixed. In addition, we discovered that the mean of one mixture component corresponded to a narrative speaking style in a clean recording condition. Speaker similarity tests and demonstrations of latent attribute control are shown in Appendix G and the demo page.
We demonstrate the ability of GMVAE-Tacotron to consistently generate high-quality speech by conditioning on a value of zl associated with clean output. We considered two approaches: (1) using the mean of the identified clean component, which can be seen as a preset configuration with a fixed channel and style; (2) inferring a latent attribute representation zl from reference speech and denoising it by modifying dimensions3 associated with the noise level to predetermined values.
2This dataset will be open-sourced soon. 3We found two relevant dimensions, controlling 1) low frequency, narrowband, and 2) wideband noise levels.
7

Under review as a conference paper at ICLR 2019

Table 3: SNR of original audio, baseline, and the proposed models with different conditioned zl, on different speakers.

Set

Original

Baseline

mean

Proposed latent latent-dn

SC 18.61 14.33 15.90 16.28

SN 11.80

9.69 15.82 6.78

UC 20.39

N/A 15.70 16.40

UN 10.92

N/A 15.27 4.81

17.94 18.94 18.83 16.89

Table 4: Subjective preference (%) between baseline and proposed model with denoised zl.

Set Baseline Neutral Proposed

SN

4.0 10.5

85.5

Table 5: MOS of baseline and the proposed model with the clean component mean.

Set Model

MOS

SC

Baseline Proposed

4.171 ± 0.071 4.179 ± 0.062

Baseline 3.635 ± 0.099 SN + denoise 3.842 ± 0.102
Proposed 4.093 ± 0.084

UC

d-vector Proposed

4.099 ± 0.055 4.261 ± 0.048

UN

d-vector Proposed

3.764 ± 0.120 4.197 ± 0.084

We evaluated a set of eight "seen clean" (SC) speakers and a set of nine "seen noisy" (SN) speakers from the training set, a set of ten "unseen noisy" (UN) speakers from a held-out set with no overlapping speakers, and the set of ten unseen speakers used in Jia et al. (2018), denoted as "unseen clean" (UC). For consistency, we always used an inferred zo from an utterance from the target speaker, regardless of whether that speaker was seen or unseen. As a baseline we used a Tacotron model conditioned on a 128-dimensional speaker embedding learned for each speaker seen during training.
Table 3 shows the SNR of the original audio, audio synthesized by the baseline, and by the GMVAETacotron using the two proposed approaches, denoted as mean and latent-dn, respectively, on all speaker sets whenever possible. In addition, to see the effectiveness of the denoising operation, the table also includes the results of using inferred zl directly, denoted as latent. The results show that the inferred zl followed the same SNR trend as the original audio, indicating that zl captured the variation in acoustic condition. The high SNR values of mean and latent-dn verifies the effectiveness of using a preset and denoising arbitrary inferred latent features, both of which outperformed the baseline by a large margin, and produced better quality than the original noisy audio.
Table 4 compares the proposed model using denoised zl to the baseline in a subjective side-byside preference test. Table 5 further compares subjective naturalness MOS of the proposed model using the mean of the clean component to the baseline on the two seen speaker sets, and to the d-vector model (Jia et al., 2018) on the two unseen speaker sets. Specifically, we consider another stronger baseline model to compare on the SN set, which is trained on denoised data using spectral subtraction (Boll, 1979), denoted as "+ denoise." Both results indicate that raters preferred the proposed model to the baselines. Moreover, the MOS evaluation shows that the proposed model delivered similar level of naturalness under all conditions, seen or unseen, clean or noisy.
5 CONCLUSION
We describe GMVAE-Tacotron, a TTS model which learns an interpretable and disentangled latent representation to enable fine-grained control of latent attributes and provides a systematic sampling scheme for them. If speaker labels are available, we demonstrate an extension of the model that learns a continuous space that captures speaker attributes, along with an inference model which enables one-shot learning of speaker attributes from unseen reference utterances.
The proposed model was extensively evaluated on tasks spanning a wide range of signal variation. We demonstrated that it can independently control many latent attributes, and is able to cluster them without supervision. In particular, we verified using both subjective and objective tests that the model could synthesize high-quality clean speech for a target speaker even if the quality of data for that speaker does not meet high standard. These experimental results demonstrated the effectiveness of the model for training high-quality controllable TTS systems on large scale training data with rich styles by learning to factorize and independently control latent attributes underlying the speech signal.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Expressive speech synthesis via modeling expressions with variational autoencoder. In Interspeech, pp. 3067­3071, 2018.
Sercan Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep Voice: Real-time neural text-to-speech. In International Conference on Machine Learning (ICML), pp. 195­204, 2017.
Sercan Arik, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In Advances in Neural Information Processing Systems (NIPS), 2017.
Sercan Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning with a few samples. arXiv preprint arXiv:1802.06006, 2018.
Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transactions on Acoustics, Speech, and Signal Processing, 27(2):113­120, 1979.
Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-based models for speech recognition. In Advances in Neural Information Processing Systems (NIPS), 2015.
Alain De Cheveigne´ and Hideki Kawahara. YIN, a fundamental frequency estimator for speech and music. The Journal of the Acoustical Society of America, 111(4):1917­1930, 2002.
Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with Gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.
Tomoki Hayashi, Shinji Watanabe, Yu Zhang, Tomoki Toda, Takaaki Hori, Ramon Astudillo, and Kazuya Takeda. Back-translation-style data augmentation for end-to-end ASR. arXiv preprint arXiv:1807.10893, 2018.
Gustav Eje Henter, Xin Wang, and Junichi Yamagishi. Deep encoder-decoder models for unsupervised learning of controllable speech synthesis. arXiv preprint arXiv:1807.11470, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations (ICLR), 2017.
Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised learning of disentangled and interpretable representations from sequential data. In Advances in Neural Information Processing Systems (NIPS), 2017a.
Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation. In Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 16­23, 2017b.
Wei-Ning Hsu, Hao Tang, and James Glass. Unsupervised adaptation with interpretable disentangled representations for distant conversational speech recognition. In Interspeech, pp. 1576­1580, 2018.
Ye Jia, Yu Zhang, Ron J Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, et al. Transfer learning from speaker verification to multispeaker text-to-speech synthesis. arXiv preprint arXiv:1806.04558, 2018.
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep embedding: an unsupervised and generative approach to clustering. In International Joint Conference on Artificial Intelligence (IJCAI), 2017.
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aa¨ron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In International Conference on Machine Learning (ICML), 2018.
9

Under review as a conference paper at ICLR 2019
Chanwoo Kim and Richard M Stern. Robust signal-to-noise ratio estimation based on waveform amplitude distribution analysis. In Interspeech, pp. 2598­2601, 2008.
Chanwoo Kim, Ananya Misra, Kean Chin, Thad Hughes, Arun Narayanan, Tara Sainath, and Michiel Bacchiani. Generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in Google Home. In Interspeech, pp. 379­383, 2017.
Simon King and Vasilis Karaiskos. The Blizzard Challenge 2013. In Blizzard Challenge Workshop, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations (ICLR), 2014.
LibriVox. https://librivox.org, 2005.
Eric Nalisnick, Lars Hertel, and Padhraic Smyth. Approximate inference for deep latent Gaussian mixtures. In NIPS Workshop on Bayesian Deep Learning, 2016.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: An ASR corpus based on public domain audio books. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5206­5210, 2015.
Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, et al. Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4779­4783, 2018.
RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J Weiss, Rob Clark, and Rif A Saurous. Towards end-to-end prosody transfer for expressive speech synthesis with Tacotron. In International Conference on Machine Learning (ICML), 2018.
J. Sotelo, S. Mehri, K. Kumar, J. Santos, K. Kastner, A. Courville, and Y. Bengio. Char2Wav: End-to-End speech synthesis. In International Conference on Learning Representations (ICLR), 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (NIPS), 2014.
Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. Listening while speaking: Speech chain by deep learning. In Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 301­308, 2017.
Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. Machine speech chain with one-shot speaker adaptation. In Interspeech, pp. 887­891, 2018.
Aa¨ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NIPS), 2017.
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, and Rif A Saurous. Tacotron: Towards end-to-end speech synthesis. In Interspeech, pp. 4006­4010, 2017.
Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Fei Ren, Ye Jia, and Rif A Saurous. Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. In International Conference on Machine Learning (ICML), pp. 5180­5189, 2018.
10

Under review as a conference paper at ICLR 2019 Heiga Zen, Keiichi Tokuda, and Alan W Black. Statistical parametric speech synthesis. Speech
Communication, 51(11):1039­1064, 2009.
11

Under review as a conference paper at ICLR 2019

A DERIVATION OF REPARAMETERIZED TRAINING OBJECTIVES
This section gives detailed derivation of the evidence lower bound (ELBO) estimation used for training. We first present a differentiable Monte Carlo estimation of the posterior q(yl | X), and then derive an ELBO for each of the graphical models in Figure 1, which differ in whether an additional observed attribute representation zo is used.

A.1 MONTE CARLO ESTIMATION OF THE REPARAMETERZIED CATEGORICAL POSTERIOR

As shown in equation 2, we approximate the posterior over latent attribute class yl with

q(yl | X) = Eq(zl|X)[p(yl | zl)],

(5)

where q(zl | X) is a diagonal-covariance Gaussian, and p(yl | zl) is the probability of zl being
drawn from the yl-th Gaussian mixture component. We first denote the mean vector and the diagonal elements of the covariance matrix of the yl-th component as µl,yl and l2,yl , and write the posterior over mixture components given a latent attribute representation, p(yl | zl):

p(yl | zl) = =

p(zl | yl)p(yl)

K y^ l =1

p(zl

|

y^l)p(y^l)

f (zl; µl,yl , l2,yl )K-1

K y^l

=1

f

(zl

;

µl,yl

,

l2,yl

)K

-1

,

(6) (7)

with

exp f (zl; µl,yl , l2,yl ) =

-

1 2

(zl

-

µl,yl )

diag(l2,yl )-1(zl - µl,yl )

(2)D diag(l2,yl )

,

(8)

where D is the dimensionality of zl, and K is the number of classes for yl.
Finally, we denote the posterior mean and variance of q(zl | X) by µ^l and ^l2, and compute a Monte Carlo estimate of the expectation in equation 5 after reparameterization:

q(yl | X) = Eq(zl|X)[p(yl | zl)] = EN ( ;0,I)[p(yl | µ^l + ^l

)]

1 N

N

p(yl | µ^l + ^l

n=1

(n));

(n)  N (0, I)

1 N exp =
N
n=1

-

1 2

(z~l(n)

-

µl,yl )

diag(l2,yl )-1(z~(ln) - µl,yl )

(2)D diag(l2,yl )

:= q~(yl | X),

(9) (10) (11)
(12)
(13)

where z~l(n) = µ^l + ^l (n) is a random sample, drawn from a standard Gaussian distribution using (n)  N (0, I), and N is the number of samples used for the Monte Carlo estimation, which is set to
1. The resulting estimate q~(yl | X) is differentiable w.r.t. the parameters of p(zl | yl) and q(zl | X).

12

Under review as a conference paper at ICLR 2019

A.2 DIFFERENTIABLE TRAINING OBJECTIVE

We next derive the ELBO L(p, q; X, yt, yo) and rewrite it as a Monte Carlo estimate used for training:

log p(X | yt, yo)  Eq(zl|X)q(yl|X)

log p(X | yt, yo, zl)p(zl | yl)p(yl) q(zl | X)q(yl | X)

= Eq(zl|X) [log p(X | yt, yo, zl)]

- Eq(yl|X) [DKL (q(zl | X) || p(zl | yl))]

- DKL (q(yl | X) || p(yl))

:= L(p, q; X, yt, yo),

(14)
(15) (16)

1 N

N

log p(X | yt, yo, z~l(n ))

n =1

K

- q~(yl | X)DKL (q(zl | X) || p(zl | yl))

yl =1

- DKL (q~(yl | X) || p(yl))

:= L~(p, q; X, yt, yo),

(17) (18)

where z~(ln ) = µ^l + ^l (n ), (n )  N (0, I), and L~(p, q; X, yt, yo) is the estimator used for training. Similarly, N is the number of samples used for the Monte Carlo estimate, which is set to 1.

A.3 DIFFERENTIABLE TRAINING OBJECTIVE WITH OBSERVED ATTRIBUTE REPRESENTATION

In this section, we derive the ELBO Lo(p, q; X, yt, yo) when using an additional observed attribute representation, zo, as described in Section 2.3, and rewrite it with a Monte Carlo estimation used for training. As before, we denote the posterior mean and variance of q(zo | X) by µ^o and ^o2.

log p(X | yt, yo)  Eq(zo|X)q(zl|X)q(yl|X)

log p(X | yt, zo, zl)p(zo | yo)p(zl | yl)p(yl) q(zo | X)q(zl | X)q(yl | X)

(19)

= Eq(zo|X)q(zl|X)[log p(X | yt, zo, zl)]

- DKL(q(zo | X) || p(zo | yo))

- Eq(yl|X)[DKL(q(zl | X) || p(zl | yl))]

- DKL(q(yl | X) || p(yl))

(20)

:= Lo(p, q; X, yy, yo)

(21)

1 NN

N

N
log p(X | yt, z~(on ), z~(ln ))

n =1 n =1

- DKL(q(zo | X) || p(zo | yo))

K
- q~(yl | X)DKL(q(zl | X) || p(zl | yl))
yl =1
- DKL(q(yl | X) || p(yl)) := L~o(p, q; X, yy, yo),

(22) (23)

where the continuous latent variables are reparameterized as z~(on ) = µ^o +^o

(n o

)

and

z~(l n

)

= µ^l +

^ l

(n l

), with auxiliary noise variables

(n o

),

(n l

)

 N (0, I).

The estimator L~o(p, q; X, yt, yo)

is used for training. Both N and N are set to 1 for the Monte Carlo estimation.

B NEURAL NETWORK ARCHITECTURE DETAILS
We parameterize three distributions: p(X|yt, zo, zl), q(zo|X), and q(zl|X) with neural networks, referred to in Figure 8 as the synthesizer, observed encoder, and latent encoder, respectively.

13

Under review as a conference paper at ICLR 2019

audio sequence

latent attribute posterior observed attribute posterior

latent encoder observed encoder

text sequence

text encodings

decoder

mel spectrogram
synthesizer

Figure 8: Training configuration of the GMVAE-Tacotron model. Dashed lines denotes sampling. The model is comprised of three modules: a synthesizer, a latent encoder, and an observed encoder.

B.1 SYNTHESIZER
The synthesizer is an attention-based sequence-to-sequence network which generates a mel spectrogram as a function of an input text sequence and conditioning signal generated by the auxiliary encoder networks. It closely follows the network architecture of Tacotron 2 (Shen et al., 2018). The input text sequence is encoded by three convolutional layers, which contains 512 filters with shape 5 × 1, followed by a bidirectional long short-term memory (LSTM) of 256 units for each direction. The resulting text encodings are accessed by the decoder through a location sensitive attention mechanism (Chorowski et al., 2015), which takes attention history into account when computing a normalized weight vector for aggregation.
The base Tacotron 2 autoregressive decoder network takes as input the aggregated text encoding, and the bottlenecked previous frame (processed by a pre-net comprised of two fully-connected layers of 256 units). To condition the output on additional attribute representations, the decoder is extended to consume zl and zo (or yo) after passing them through a stack of two uni-directional LSTM layers with 1024 units. The output from the stacked LSTM is concatenated with the decoder input, and linearly projected to predict the mel spectrum of the current frame, as well as an end-of-sentence token. Finally, the predicted spectrogram frames are passed to a post-net, which predicts a residual that is added to the initial decoded sequence of spectrogram frames, to better model detail in the spectrogram and reduce the overall mean squared error.
Similar to Tacotron 2, we separately train a neural vocoder to invert a mel spectrograms to a timedomain waveform. In contrast to that work, we replace the WaveNet (van den Oord et al., 2016) vocoder with one based on the recently proposed WaveRNN (Kalchbrenner et al., 2018) architecture, which is more efficient during inference.
B.2 LATENT ENCODER AND OBSERVED ENCODER
Both the latent encoder and the observed encoder map a mel spectrogram from a reference speech utterance to two vectors of the same dimension, representing the posterior mean and log variance of the corresponding latent variable. We design both encoders to have exactly the same architecture, whose outputs are conditioned by the decoder in a symmetric way. Disentangling of latent attributes and observed attributes is therefore achieved by optimizing different KL-divergence objectives.
For each encoder, a mel spectrogram is first passed through two convolutional layers, which contains 512 filters with shape 3 × 1. The output of these convolutional layers is then fed to a stack of two bidirectional LSTM layers with 256 cells at each direction. A mean pooling layer is used to summarize the LSTM outputs across time, followed by a linear projection layer to predict the posterior mean and log variance.
C DETAILED EXPERIMENTAL SETUP
The network is trained using the Adam optimizer (Kingma & Ba, 2015), configured with an initial learning rate 10-3, and an exponential decay that halved the learning rate every 12.5k steps, beginning after 50k steps.
14

Under review as a conference paper at ICLR 2019

Table 6 details the list of prior hyperparameters used for each of the four datasets described in Section 4: multi-speaker English data (multi-spk), noisified multi-speaker English data (noisy-multispk), single-speaker story-telling data (audiobooks), and crowd-sourced audiobook data (crowdsourced). To ensure numerical stability we set a minimum value allowed for the variance. We initially set the lower bound to e-0.5; however, with the exception of the multi-speaker English data, the trained variance reached the lower bound for all mixture components for all dimensions. We therefore lowered the minimum variance to e-2, and found that it left sufficient range to capture the amount of variation.
Table 6: Prior hyperparameters for each dataset used in Section 4.

dim(yl) dim(zl) initial l minimum l dim(yo) dim(zo) initial o minimum o

multi-spk (Section 4.1)
10 16 e0 e-0.5 N/A N/A N/A N/A

noisy-multi-spk (Section 4.2)
10 16 e-1 e-2 84 N/A N/A N/A

audiobooks (Section 4.3)
10 16 e-1 e-2 N/A N/A N/A N/A

crowd-sourced (Section 4.4)
10 16 e-1 e-2 1,172 16 e-2 e-4

D ADDITIONAL RESULTS ON THE MULTI-SPEAKER ENGLISH CORPUS
D.1 RANDOM SAMPLES BY MIXTURE COMPONENT

Component 1: male

Component 3: US female (low-pitched)

Component 4: GB/AU female

Component 5: US female (high-pitched)

Component 8: US/SG male

Component 8: US/SG female

Figure 9: Mel-spectrograms and F0 tracks of three random samples drawn from each of six selected mixture components. Each component represents certain gender and accent group. The input text is "The fake lawyer from New Orleans is caught again." which emphasizes the difference between British and US accents. As mentioned in the paper, although samples from component 3 and 5 both capture US female voices, each component captures specific speakers with different F0 ranges. The former ranges from 100 to 250 Hz, and the latter ranges from 200 to 350 Hz. Audio samples can be found at
https://x10q92sh3d.github.io/gmvae_tacotron/#multispk_en.sample

15

Under review as a conference paper at ICLR 2019
D.2 CONTROL OF LATENT ATTRIBUTES
Dimension 0: start offset
Dimension 2: speed
Dimension 3: accent
Dimension 9: pitch
Figure 10: Mel-spectrograms and F0 tracks of the synthesized samples demonstratoing independent control of several latent attributes. Each row traverses one dimension with three different values, keeping all other dimensions fixed.. All examples use the same input text: "The fake lawyer from New Orleans is caught again." The plots for dimension 0 (top row) and dimension 2 (second row) mainly show variation along the time axis. The underlying F0 contour values do not change, however dimension 0 controls the duration of the initial pause before the speech begins, and dimension 2 controls the overall speaking rate, with the F0 track stretching in time (i.e. slowing down) when moving from the left column to the right. Dimension nine (bottom row) mainly controls the degree of F0 variation while maintaining the speed and starting offset. Finally, we note that differences in accent controlled by dimension 3 (third row) are easier to recognize by listening to audio samples, which can be found at https://x10q92sh3d.github.io/gmvae_tacotron/#multispk_en.control.
16

Under review as a conference paper at ICLR 2019

D.3 CLASSIFICATION OF LATENT ATTRIBUTE REPRESENTATIONS

Table 7: Accuracy (%) of linear classifiers trained on zl.

Gender Accent Speaker Identity

Train 100.00 98.76 Eval 98.72 98.72

97.66 95.39

To quantify how well the learned representation captures useful speaker information, we experimented with training classifiers for speaker attributes on the latent features. The test utterances were partitioned in a 9:1 ratio for training and evaluation, which contain 2,098 and 234 utterances, respectively. Three linear discriminant analysis (LDA) classifiers were trained on the latent attribute representations zl to predict speaker identity, gender and accent. Table 7 shows the classification results. The high accuracies in the table demonstrate the potential of applying the learned low-dimensional representations to tasks of predicting other unseen attributes.
E ADDITIONAL RESULTS ON THE NOISY MULTI-SPEAKER ENGLISH CORPUS
E.1 RANDOM SAMPLES FROM NOISY AND CLEAN COMPONENTS

Speaker 1

Noisy Speaker A

Noisy Speaker B

sample 1

sample 2 A noisy component

sample 3

sample 1

sample 2 A clean component

sample 3

Figure 11: Mel-spectrograms of random samples drawn from a noisy (left) and a clean (right) mixture component. Samples within each row are conditioned on the same speaker. Likewise, samples within each column are conditioned on the same latent attribute representation zl. For all samples, the input text is "This model is trained on multi-speaker English data." Samples drawn from the clean component are all clean, while samples drawn from the noisy component all contain obvious background noise. Finally, note that samples within each column contain similar types of noise since they are conditioned on the same zl. Audio samples can be found at https: //x10q92sh3d.github.io/gmvae_tacotron/#noisy_multispk_en.sample

17

Under review as a conference paper at ICLR 2019

E.2 CONTROL OF BACKGROUND NOISE LEVEL
Speaker 1

Speaker 1

Noisy Speaker A

Noisy Speaker A

Noise-level dim =

-0.8 -0.6 -0.4 -0.2

0 0.2

Figure 12: Mel-spectrograms of the synthesized samples demonstrating control of the background noise level by varying the value of dimension 13. Each row conditions on a seed zl drawn from a mixture component, where all values except for dimension 13 are fixed. The embedding used in row 1 and row 3 are drawn from a noisy component, and used in row 2 and row 4 are drawn from a clean component. In addition, we condition the decoding on the same speaker for the first two rows, and the same held-out speaker for the last two rows. The value of dimension 13 used in each column is shown at the bottom, and the input text is "Traversing the noise level dimension." In all rows, samples on the right are cleaner than those on the left, with the background noise gradually fading away as the value for dimension 13 increases. Audio samples can be found at https:
//x10q92sh3d.github.io/gmvae_tacotron/#noisy_multispk_en.control

E.3 PRIOR DISTRIBUTION OF THE NOISE LEVEL DIMENSION

Figure 13: Prior distributions of each component for dimension 13, which controls background noise level. The first four components (0­3) model noisy speech, and the other six (4­9) model clean speech. The two groups of mixture components are clearly separated in this dimension. Furthermore, the clean components have lower variances than the noisy components, indicated a narrower range of noise levels in clean components compared to noisy ones.
18

Under review as a conference paper at ICLR 2019
F ADDITIONAL RESULTS ON THE SINGLE-SPEAKER AUDIOBOOK CORPUS
F.1 PARALLEL STYLE TRANSFER
We evaluated the ability of the proposed model to synthesize speech that resembled the prosody or style of a given reference utterance, by conditioning on a latent attribute representation inferred from the reference. We adopted two metrics from Skerry-Ryan et al. (2018) to quantify style transfer performance: the mel-cepstral distortion(MCD13), measuring the phonetic and timbral distortion, and F0 frame error (FFE), which combines voicing decision error and F0 error metrics to capture how well F0 information, which encompasses much of the prosodic content, is retained. Both metrics assume that the generated speech and the reference speech are frame aligned. We therefore synthesized the same text content as the reference for this evaluation.

Table 8: Quantitative evaluation for parallel style transfer. Lower is better for both metrics.

Model
Baseline GST Proposed (16) Proposed (32)

MCD13
17.91 14.34 15.78 14.42

FFE
64.1% 41.0% 51.4% 42.5%

Table 8 compares the proposed model against the baseline and a 16-token GST model. The proposed model with a 16-dimensional zl (D = 16) was better than the baseline but inferior to the GST model. Because the GST model uses a four-head attention (Vaswani et al., 2017), it effectively has 60 degrees of freedom, which might explain why it performs better in replicating the reference style. By increasing the dimension of zl to 32 (D = 32), the gap to the GST model is greatly reduced. Note that the total number of parameters is still smaller than in the GST model.
19

Under review as a conference paper at ICLR 2019 F.2 NON-PARALLEL STYLE TRANSFER

Reference style 1

Reference style 2

Reference style 3

"By water in the midst of water!" "And she began fancying the sort of thing that would happen: Miss Alice!" "She tasted a bite, and she read a word or two, and she sipped the amber wine and wiggled her toes in the silk stockings."

Figure 14: Mel-spectrograms of reference and synthesized style transfer utterances. The four reference utterances are shown on the top, and the four synthesized style transfer samples are shown below, where each row uses the same input text (shown above the spectrograms), and each column is conditioned on the zl inferred from the reference in the top row. From left to right, the voices of the three reference utterances can be described as (1) tremulous and high-pitched, (2) rough, low-pitched, and terrifying, and (3) deep and masculine. In all cases, the synthesized samples resemble the prosody and the speaking style of the reference. For example, samples in the first column have the highest F0 (positively correlated to the spacing between horizontal stripes) and more tremulous (vertical fluctuations), and spectrograms in the middle column are more blurred, related to roughness of a voice. Audio samples can be found at https://x10q92sh3d.github.io/
gmvae_tacotron/#singlespk_audiobook.transfer

Figure 14 demonstrates that the GMVAE-Tacotron can also be applied in a non-parallel style transfer scenario to generate speech whose text content differs significantly from the reference.
20

Under review as a conference paper at ICLR 2019 F.3 RANDOM STYLE SAMPLES

Sample 1

Text 1

Text 2

Text 3

Sample 2

Sample 3

Sample 4

Sample 5
Figure 15: Mel-spectrograms and F0 tracks of different input text with five random samples of zl drawn from the prior. The three input text sequences from left to right are: (1) "We must burn the house down! said the Rabbit's voice.", (2) "And she began fancying the sort of thing that would happen: Miss Alice!", and (3) "She tasted a bite, and she read a word or two, and she sipped the amber wine and wiggled her toes in the silk stockings." The five samples of zl encode different styles: the first sample has the fastest speaking rate, the third sample has the slowest speaking rate, and the fourth sample has the highest F0. Audio samples can be found at https: //x10q92sh3d.github.io/gmvae_tacotron/#singlespk_audiobook.sample
21

Under review as a conference paper at ICLR 2019
F.4 CONTROL OF STYLE ATTRIBUTES
Dimension 8: pitch
Dimension 10: pause length
Dimension 14: roughness
Figure 16: Synthesized mel-spectrograms demonstrating independent control of speaking style and prosody. The same input text is used for all samples: "He waited a little, in the vain hope that she would relent: she turned away from him." In the top row, F0 is controlled by setting different values for dimension eight. F0 tracks show that the F0 range increases from left to right, while other attributes such as speed and rhythm do not change. In the second row, the duration of pause before the phrase "she turned away from him." (red boxes) is varied. The three spectrograms are very similar, except for the width of the red boxes, indicating that only the pause duration changed. In the bottom row, the "roughness" of the voice is varied. The same region of spectrograms is zoomed-in for clarity, where the spectrograms became less blurry and the harmonics becomes better defined from left to right. Audio samples can be found at https://x10q92sh3d.github.io/gmvae_ tacotron/#singlespk_audiobook.control.
22

Under review as a conference paper at ICLR 2019
G ADDITIONAL RESULTS ON THE CROWD-SOURCED AUDIOBOOK CORPUS
G.1 CONTROL OF STYLE, CHANNEL, AND NOISE ATTRIBUTES
Dimension 0: pitch
Dimension 1: filter
Dimension 4: noise
Dimension 13: speed
Figure 17: Synthesized mel-spectrograms and F0 tracks demonstrating independent control of attributes related to style, recording channel, and noise-condition. The same text input was used for all the samples: '"Are you Italian?" asked Uncle John, regarding the young man critically.' In each row we varied the value for a single dimension while holding other dimensions fixed. In the top row, we controlled the F0 by traversing dimension zero. Note that the speaker identity did not change while traversing this dimension. In the second row, the F0 contours did change while traversing this dimension; however, it can be seen from the spectrograms that the leftmost one attenuated the energy in low-frequency bands, and the rightmost one attenuated energy in high-frequency bands. This dimension appears to control the shape of a linear filter applied to the signal, perhaps corresponding to variation in microphone frequency response in the training data. In the third row, the F0 contours did not change, either. However, the background noise level does vary while traversing this dimension, which can be heard on the demo page. In the bottom row, variation in the speaking rate can be seen while other attributes remain constant. Audio samples can be found at https://x10q92sh3d. github.io/gmvae_tacotron/#crowdsourced_audiobook.control.
23

Under review as a conference paper at ICLR 2019

G.2 SPEAKER SIMILARITY TEST
In this section we test whether the synthesized speech resembles the identity of the speaker of the reference utterance. For evaluation, we paired each synthesized utterance with the reference utterance for subjective MOS on speaker similarity.
Table 9: Speaker similarity MOS.

Set Model

MOS

SC

Baseline Proposed

3.54 ± 0.09 3.60 ± 0.09

Ground truth (w/ channel variation) 3.30 ± 0.27

SN

Baseline Baseline + denoise

3.83 ± 0.08 3.23 ± 0.20

Proposed

3.11 ± 0.08

d-vector UC d-vector (large)
Proposed

2.23 ± 0.08 3.03 ± 0.09 2.79 ± 0.08

Table 9 compares the proposed model using denoised latent attribute representations to baseline systems on the two seen speaker sets, and to d-vector systems on the unseen clean speaker set. The d-vector systems used a separately trained speaker encoder model to extract speaker representations for TTS conditioning. Here we considered two speaker encoder models, one trained on the same train-clean partition as the proposed model, and another trained on a larger scale dataset containing 18K speakers. We denote these two systems as d-vector and d-vector (large), respectively.
On the seen clean speaker set, the proposed model achieved similar speaker similarity scores to the baseline. However, on the seen noisy speaker set, both the proposed model and the baseline model trained on denoised data were significantly worse than the baseline. We hypothesize that similarity of the acoustic conditions between the paired utterances biased the speaker similarity ratings. To confirm this hypothesis, we evaluated the speaker similarity of the ground truth utterances from a speaker whose recordings contained significant variation in acoustic conditions. As shown in Table 9, these ground truth utterances are also rated with a significantly lower MOS than the baseline, but were close to the proposed model and the denoised baseline. This result implies that this subjective speaker similarity test may not be reliable in the presence of noise and channel variation, requiring additional work to design a speaker similarity test that is unbiased to nuisance factors.
Finally, on the unseen clean speaker set, the proposed model achieved better speaker similarity scores than the d-vector-based system whose speaker representation extractor was trained on the same set as the TTS system, but worse than the d-vector-based system with the extractor trained on the larger dataset.

24

