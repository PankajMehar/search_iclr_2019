Under review as a conference paper at ICLR 2019
EXPLORING THE INTERPRETABILITY OF LSTM NEURAL NETWORKS OVER MULTI-VARIABLE DATA
Anonymous authors Paper under double-blind review
ABSTRACT
In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data. To this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation. In particular, IMVLSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. On top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. Extensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. It also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data.
1 INTRODUCTION
Our daily life is now surrounded by various types of sensors, ranging from smart phones, video cameras, Internet of things, to robots. The observations yield by such devices over time are naturally organized in time series data (Qin et al., 2017; Yang et al., 2015). In this paper, we focus on multivariable time series consisting of target and exogenous variables. Each variable corresponds to a monitoring over physical world. A predictive model over such multi-variable data aims to predict the future values of the target series using historical values of target and exogenous series.
In addition to forecasting, the interpretability of prediction models is essential for deployment and knowledge extraction as well (Hu et al., 2018; Foerster et al., 2017; Lipton, 2016). For multi-variable time series in this paper, we focus on two types of importance interpretation. (1) Variable-wise temporal importance: exogenous variables present different temporal influence on the target one (Kirchgässner et al., 2012). For instance, for the exogenous variable having instant effect on the target one, its historical data at short time lags is expected to high importance values. (2) Overall variable importance: exogenous variables and the auto-regressive part of the target variable differ in predictive power, which reflects different variable importance w.r.t. the prediction of the target (Feng et al., 2018; Riemer et al., 2016). The ability to unveil such knowledge through predictive models enables to fundamentally understand the effect of exogenous variables on the target one.
Recently, recurrent neural networks (RNNs), especially long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) and the gated recurrent unit (GRU) (Cho et al., 2014), have been proven to be powerful sequence modeling tools in a variety of tasks such as language modelling, machine translation, health informatics, time series, and speech (Ke et al., 2018; Lin et al., 2017; Lipton et al., 2015; Sutskever et al., 2014; Bahdanau et al., 2014).
However, current RNNs fall short of aforementioned interpretability for multi-variable data due to their opaque internal states. Specifically, when fed with the multi-variable observations of the target and exogenous variables, RNNs blindly blend the information of all variables into memory cells and hidden states which are used for prediction. It is intractable to distinguish the contribution of individual variables into the prediction through hidden states (Zhang et al., 2017). Recently, attention-based neural networks have been proposed to enhance the ability of RNN in selectively using long-term memory and the interpretability (Vaswani et al., 2017; Qin et al., 2017; Choi et al., 2016; Vinyals et al., 2015; Chorowski et al., 2015; Bahdanau et al., 2014). Nevertheless, current
1

Under review as a conference paper at ICLR 2019
attention mechanism is mostly applied to hidden states across time steps and capture globally temporal information, thereby failing to uncover fine-grained variable level importance.
To this end, in this paper we build interpretable LSTM models with the aim to achieve a unified framework of forecasting and knowledge extraction. In particular, the contribution is fourfold. First, we propose the interpretable multi-variable LSTM, referred to as IMV-LSTM, with tensorized hidden states and updating scheme, such that each element of the hidden state tensor encodes information for a certain input variable. Second, based on these variable-wise hidden states we develop a novel mixture temporal and variable attention mechanism. Third, attention values are further summarized to quantify variable-wise temporal importance and overall variable importance. Lastly, we perform extensive experimental evaluation of IMV-LSTM against statistical, machine learning and neural network baselines to demonstrate the superior prediction performance and interpretability of IMVLSTM. The idea of IMV-LSTM easily applies to other RNN structures, e.g. GRU and stacked recurrent layers. This will be the future work.
2 RELATED WORK
In time series analysis, prediction with exogenous variables is formulated as an auto-regressive exogenous model. Vanilla RNNs have been used to study it in Zemouri et al. (2010); Diaconescu (2008), where interpretability was not investigated. To the best of our knowledge, this is the first work to explore the capability of recurrent neural networks for accurate forecasting, variable and temporal level importance interpreting simultaneously.
Recent research on the interpretability of RNNs is categorized into two groups: attention methods and post-analyzing over trained models. Attention mechanism has gained tremendous popularity (Xu et al., 2018; Choi et al., 2018; Guo et al., 2018; Lai et al., 2017; Qin et al., 2017; Cinar et al., 2017; Choi et al., 2016; Vinyals et al., 2015; Bahdanau et al., 2014). However, current attention mechanism is mainly applied to hidden states across time steps. Qin et al. (2017); Choi et al. (2016) use weighted input data learned by encoder networks to do forecasting. Weighting input data by attention overlooks the direction of correlation with the target as well as lacking the ability to interpret variable-wise temporal and overall variable importance.
As for post-analyzing based interpretation, Murdoch et al. (2018); Murdoch & Szlam (2017); Arras et al. (2017) extracted temporal importance scores over words or phrases of individual language sequences by decomposing the memory cells of trained LSTM. Chu et al. (2018) proposed interpretation solutions for piece-wise linear neural networks. In Wang et al. (2018), it quantified the importance of each middle layer to the output. Foerster et al. (2017) introduced input-switched affine transformations into RNNs, which analyzed the contribution of input steps via linear methods. Above work focuses on global temporal importance and does not support variable specific temporal interpretation. Our following proposed IMV-LSTM is a combination of attention and post-analyzing for multi-variable time series, where novel variable-wise hidden states and mixture attention enable fine-grained temporal and variable importance interpretation during the training.
Another line of related research is about tensorization and selectively updating of hidden states in RNNs. Do et al. (2017); Novikov et al. (2015) proposed to represent hidden states as a matrix. He et al. (2017) developed tensorized LSTM to enhance the capacity of networks without additional parameters. Kuchaiev & Ginsburg (2017); Neil et al. (2016); Koutnik et al. (2014) put forward to partition the hidden layer into separated modules with different updates. The hidden state tensors and update processes in existing works do not maintain variable-wise correspondence, thereby lacking the desirable interpretability.
3 INTERPRETABLE MULTI-VARIABLE LSTM
Assume we have N -1 exogenous time series and a target series y of length T , where y = [y1, · · · , yT ] and y  RT .1 By stacking exogenous time series and target series, we define a multi-variable input series as XT = {x1, · · · , xT }, where xt = [xt1, · · · , xNt -1, yt]. Both of xnt and yt can be multidimensional vector. xt  RN is the multi-variable input at time step t. Given XT , we aim to
1Vectors are assumed to be in column form throughout this paper.
2

Under review as a conference paper at ICLR 2019
learn a non-linear mapping to predict the next values of the target series, namely y^T +1 = F (XT ). Meanwhile, through trained model F(·), we aim to extract variable-wise temporal importance and overall variable importance w.r.t. the prediction from the data. The following described IMVLSTM can be easily extended to multi-step ahead prediction via iterative methods as well as vector regression (Fox et al., 2018; Cheng et al., 2006).
3.1 NETWORK ARCHITECTURE
In IMV-LSTM we develop tensorized hidden states and update scheme, which ensure that each element of the hidden state tensor encapsulates information exclusively from a certain variable of the input. It gives rise to mixture attention on both variable and temporal aspects and fine-grained interpretation described below.

Figure 1: A toy example of a IMV-LSTM with a two-variable input sequence and the hidden matrix
of 4-dimensions per variable. Circles represent one dimensional elements. Purple and blue colors
correspond to two variables. Blocks containing rectangles with circles inside represent input data and hidden matrix. Panel (a) exhibits the derivation of hidden update ~jt. Grey areas represent transition weights. Panel (b) demonstrates the mixture attention process. (best viewed in color)

To distinguish from the hidden state and gate vectors in a standard LSTM, tensorized hidden state and gates in IMV-LSTM are denoted with tildes. Specifically, we define the hidden state tensor (matrix) at time step t as h~t = [h1t , · · · , hNt ] , where h~t  RN×d, hnt  Rd. The overall size of the layer is derived as D = N · d. The element htn of h~t is the hidden state vector specific to n-th input variable.
Then, we define the input-to-hidden transition tensor as U j = [U1j , · · · , UjN ] , where U j  RN×d×d0 , Ujn  Rd×d0 and d0 is the dimension of individual variables at each time step. The hidden-to-hidden transition tensor is defined as: Wh = [Wh1 , · · · , WhN ], where Wh  RN×d×d and Whn  Rd×d.
As standard LSTM neural networks (Hochreiter & Schmidhuber, 1997), IMV-LSTM has the input it, forget ft, output gates ot and the memory cells ct in the update process. Given the newly incoming input xt at time t and the hidden state matrix h~t-1, the hidden state update is defined as:

~jt = tanh Wj h~t-1 + U j xt + bj ,

(1)

where ~jt = [ j1t , · · · , jNt ] has the same shape as hidden state matrix (i.e. RN×d). Each element jnt  Rd corresponds to the update of the hidden state w.r.t. input variable n. Term Wj h~t-1 and U j xt respectively capture the update from the hidden states at the previous step and the new input. The tensor-dot operation is defined as the product of two tensors along the N axis, e.g., Wj h~t-1 = [Wj1ht1-1 , · · · , WjN htN-1] where Wjnhtn-1  Rd.
Depending on different update schemes of gates and memory cells, we proposed two realizations of IMV-LSTM, i.e. IMV-Full in Equation set 1 and IMV-Tensor in Equation set 2. In these two sets of equations, vec(·) refers to the vectorization operation, which concatenates columns of a matrix into a vector.  is the concatenation operation. denotes element-wise multiplication. In this paper, matricization(·) reshapes a vector of RD into a matrix of RN×d.

3

Under review as a conference paper at ICLR 2019

it ft =  W [xt  vec( h~t-1)] + b ot ct = ft ct-1 + it vec( ~jt) h~t = matricization(ot tanh(ct))
Equation set 1: IMV-Full

(2)
(3) (4)

~it  ~ft  =  W
o~t

h~t-1 + U

~ct = ~ft ~ct-1 + ~it ~jt

h~t = ~ot tanh(~ct)

xt + b

Equation set 2: IMV-Tensor

(5)
(6) (7)

IMV-Full: With vectorization in Eq. (2) and (3), IMV-Full updates gates and memories using full h~t-1 and ~jt regardless of the variable-wise data in them. By simple replacement of the hidden update in standard LSTM by ~jt, IMV-Full behaves identically to standard LSTM while enjoying the
interpretability shown below.

IMV-Tensor: By applying tensor-dot operations in Eq. (5), gates and memory cells are matrices as well, elements of which have the correspondence to input variables as hidden state matrix h~t does.
In IMV-Full and IMV-Tensor, gates only scale ~jt and ~ct-1 and thus retain the variable-wise data organization in h~t. Meanwhile, based on tensorized hidden state Eq. (1) and gate update Eq. (5), IMV-Tensor can also be considered as a set of parallel LSTMs, each of which processes one variable series. The derived hidden states specific to individual variables are aggregated on both temporal and variable level through the attention described below.

3.2 MIXTURE ATTENTION

After feeding a sequence of {x1, · · · , xT } into either IMV-Full or IMV-Tensor, we obtain a sequence of hidden state matrices {h~1, · · · , h~T }, where the sequence of hidden states specific to variable n is extracted as {hn1 , · · · , hTn }.
In this part, we present the novel mixture attention mechanism in IMV-LSTM based on the following idea. Temporal attention is first applied to the sequence of hidden states corresponding to each variable, so as to obtain the summarized history of each variable. Then by using the history enriched hidden state of each variable, global variable attention is derived. These two steps are assembled into a probabilistic mixture model (Zong et al., 2018; Graves, 2013; Bishop, 1994), which facilitates the subsequent training, inference, and interpretation process.
In particular, the mixture attention is formulated as:

N

p(yT +1 |XT ) = p(yT +1|zT +1 = n, XT ) · p(zT +1 = n|XT )

n=1

N
= p(yT +1 | zT +1 = n, h1n, · · · , hTn ) · p(zT +1 = n | h~1, · · · , h~T )

n=1

N
= p(yT +1 | zT +1 = n, hTn  gn ) · p(zT +1 = n | hT1  g1, · · · , hTN  gN )

n=1

variable-wise temporal attention

overall variable attention

(8)

In Eq. (8), we introduce a latent random variable zT +1 into the the density function of yT +1 to govern the generation process. zT +1 is a discrete variable over the set of values {1, · · · , N } corresponding to N input variables. Mathematically, p(yT +1 | zT +1 = n, hnT  gn) characterizes the density of yT +1 conditioned on historical data of variable n, while the prior of zT +1, i.e. p(zT +1 = n | h1T  g1, · · · , hNT  gN ) controls to what extent yT +1 is driven by variable n.

The context vector gn is computed as the temporal attention weighted sum of hidden states corre-

sponding to variable n, i.e., gn =

t tnhnt where attention weight tn =

.exp ( fn(htn) )
k exp ( fn(hkn) )

fn(·)

can be a flexible function specific to variable n, e.g., neural networks (Bahdanau et al., 2014). The

p(yT +1 | zT +1 = n, hTn  gn) is a Gaussian distribution parameterized by [ µn, n] = n( hnT  gn), where n(·) can be a feedforward neural network.

4

Under review as a conference paper at ICLR 2019

The overall variable attention p(zT +1 = n | h1T  g1, · · · , hTN  gN ) is derived by a softmax function over {f( hnT  gn)}N , where f(·) can be a feedforward neural network shared by all variables.

3.3 LEARNING, INFERENCE, AND INTERPRETATION

In the learning phase, the set of parameters in IMV-Full or IMV-Tensor as well as the attention functions is denoted by . Given a set of M training sequences {XT }M and {yT +1}M , the loss function to optimize is defined based on the negative log likelihood of the mixture attention model in
Eq. (8) plus regularization terms.

In the inference phase, the prediction of yT +1 is obtained by the weighted sum of means as : y^T +1 = n µn · p(zT +1 = n | hT1  g1, · · · , hTN  gN ).
Regarding interpretation, we first illustrate the burden of decipher variable and temporal importance from the raw attentions mentioned above. For instance, during the training on PLANT dataset used in the experiment section, we collect variable-wise temporal attention and overall variable attention values w.r.t. each training instance at each epoch. In Fig 2, Panel (a) plots the histograms of overall variable attention of three variables in PLANT at two different epochs. Ideally, attention weights of different variables are expected to distribute distinctly. However, through histograms in Panel (a) it is nontrivial to fully discriminate variable importance. Likewise, in Panel (b) plotting the histogram of temporal attention at some time lags of variable "P-temperature" at two different epochs does not ease the importance interpretation. Similar phenomena are observed in other variables and datasets during the experiments.

Density

100 80 60 40 20 00.00
150 125 100
75 50 25 00.00

Epoch 10
temperature p-temperature irradiance

0.05 0.10 0.15 0.20 0.25
Epoch 20
temperature p-temperature irradiance

0.0V5ariabl0e.1a0ttenti0o.1n5 value0s.20

0.25

Density

Density

Variable p-temperature, Epoch 10

2000

Time lag 18 Time lag 9

1500 Time lag 1

1000

500

00.00 1000

Va0r.i0a2ble p0.0-t4emp0e.0r6atur0e.0,8Epoc0.h1020 0.12 Time lag 18

800 Time lag 9

600 Time lag 1

400

200

00.00 0.02Temp0.o04ral at0t.e06ntion0.v08alues0.10 0.12

Density

30 25 20 15 10
5 0 0.0 30 25 20 15 10 5 0 0.0

Variable Temperature Prior Posterior

0.1

0.2

0.3

0.4

0.5

0.6

Variable P-temperature

Prior Posterior

0.1

0.2

0.3

0.4

0.5

0.6

Attention values

Density

(a) (b) (c)

Figure 2: (a) Histograms of overall variable attention at different epochs. (b) Histograms of temporal attention of variable "P-temperature". (c) Prior and posterior attention histograms of two example variables.

Therefore, we proposed the following summarization method over temporal and variable atten-

tions of data instances to quantify temporal and variable importance. First, we define the function fagg : RA×B  RB, which maps an input Z  RA×B into an aggregated vector ¯z  RB. In the present paper, we choose the simple normalized summation function as:

¯z = fagg(Z) :=

a

a

za,1 b za,b

,

·

·

·

,

p za,B a b za,b

.

It is flexible to choose alternative functions for fagg, e.g., robust statistics based methods.

(9)

For temporal importance, we collect the temporal attention w.r.t. n-th variable of each instance m as mn = [mn ,1, · · · , mn ,T -1]  RT -1. Then, the unified temporal importance of variable n is obtained as ¯n = fagg([n1 , · · · , nM ] ), t ¯tn = 1, ¯tn  [0, 1].

The overall variable importance is formulated by using a novel posterior variable attention. Concretely,

the posterior variable attention is derived as:

qn := p(zT +1 = n| XT , yT +1)  N (yT +1 | n( hnT  gn ))

,exp (f( hTn gn))
k exp (f( hTk gk))

(10)

where qn provides more distinguishable attention distribution of different variables by taking the

predictive likelihood into account. For instance, Panel (c) in Fig. (2) demonstrates the histograms of

posterior and prior attention (i.e. p(zT +1 = n|XT )) of two example variables in PLANT.

5

Under review as a conference paper at ICLR 2019
The unified variable importance can then be defined as q¯ = fagg([q1, · · · , qM ] ), n q¯n = 1, q¯n  [0, 1], where qm = [qm1 , · · · qmN ]  RN is the overall variable attention of instance m.
4 EXPERIMENTS
4.1 DATASETS
NASDAQ is the dataset from Qin et al. (2017). It contains 81 major corporations under NASDAQ 100, as exogenous time series. The index value of the NASDAQ 100 is the target series. The frequency of the data collection is minute-by-minute. The first 35,100, the following 2,730 and the last 2,730 data points are respectively used as the training, validation and test sets. PLANT: This dataset records the time series of energy production of a photo-voltaic power plant in Italy (Ceci et al., 2017). Exogenous data consists of 9 weather conditions variables (such as temperature, cloud coverage, etc.). The power production is the target. It provides 20842 sequences split into training (70%), validation (10%) and testing sets (20%). SML is a public dataset used for indoor temperature forecasting. Same as Qin et al. (2017), the room temperature is taken as the target series and another 16 time series are exogenous series. The data were sampled every minute. The first 3200, the following 400 and the last 537 data points are respectively used for training, validation, and test. The window size for NASDAQ and SML, namely T in Sec. 3, is set to 10 according to Qin et al. (2017), while for PLANT it is 20 to test long temporal dependency.
4.2 BASELINES AND EVALUATION SETUP
The first category of statistics baselines includes: STRX is the structural time series model with exogenous variables (Scott & Varian, 2014; Radinsky et al., 2012). It is formulated in terms of unobserved components via the state space method. ARIMAX is the auto-regressive integrated moving average with regression terms on exogenous variables (Hyndman & Athanasopoulos, 2014). It is a special case of vector auto-regression in this scenario. The second category of machine learning baselines includes: RF refers to random forests. It is an ensemble learning method consisting of several decision trees (Liaw et al., 2002; Meek et al., 2002) and was used in time series prediction (Patel et al., 2015). XGT refers to the extreme gradient boosting (Chen & Guestrin, 2016). It is the application of boosting methods to regression trees (Friedman, 2001). ENET represents Elastic-Net, which is a regularized regression method combining both L1 and L2 penalties of the lasso and ridge methods (Zou & Hastie, 2005) and has been used in time series analysis (Liu et al., 2010; Bai & Ng, 2008). The third category of neural network baselines includes: RETAIN uses RNNs to respectively learn weights on input data, which are then used to perform prediction (Choi et al., 2016). DUAL is an encoder-decoder architecture, which uses an encoder LSTM to learn weights and then feeds pre-weighted input data into a decoder LSTM for forecasting (Qin et al., 2017). In ARIMAX, the orders of auto-regression and moving-average terms are set via the autocorrelation and partial autocorrelation. For RF and XGT, the hyper-parameter tree depth and the number of iterations are chosen from range [3, 10] and [2, 200] via grid search. For XGT, L2 regularization is added by searching within {0.0001, 0.001, 0.01, 0.1, 1, 10}. As for ENET, the coefficients for L2 and L1 penalties are selected from {0, 0.1, 0.3, 0.5, 0.7, 0.9, 1, 1.5, 2}. For machine learning baselines, multi-variable input sequences are flattened into feature vectors.
6

Under review as a conference paper at ICLR 2019

We implemented IMV-LSTM and neural network baselines with Tensorflow2. We used Adam with the mini-batch of 64 instances (Kingma & Ba, 2014). For the size of recurrent and dense layers in the baselines, we conduct grid search over {16, 32, 64, 128, 256, 512}. The size of the IMV-LSTM layer is set by the number of neurons per variable selected from {10, 15, 20, 25}. Dropout is selected in {0.8, 0.5}. Learning rate is searched in {0.0005, 0.001, 0.005, 0.01, 0.05}. L2 regularization is added with the coefficient chosen from {0.0001, 0.001, 0.01, 0.1, 1.0}. We train each approach 10 times and report average performance. For baseline DUAL on NASDAQ and SML datasets, we use the hyper-parameters achieving the best performance in Qin et al. (2017). On PLANT dataset, hyper-parameters are searched in above sets of values.
We consider two metrics to measure the prediction performance. Specifically, RMSE is defined as RMSE = m(ym - y^m)2/M . MAE is defined as MAE = m |ym - y^m|/M .

4.3 PREDICTION PERFORMANCE
We report the prediction errors in Table 1, each cell of which presents the average RMSE and MAE with standard errors. Note that IMV-Full and IMV-Tensor are single network structures. Their good prediction performance below verifies the idea that instead of complex network architecture, simple and proper mixture of well-maintained variable-wise hidden states also improves the prediction performance as well as empowering the interpretability shown below. In particular, IMV-LSTM family outperforms baselines by around 80% at most. IMV-Full performs mostly better than baselines, while IMV-Tensor surpasses IMV-Full on NASDAQ and SML datasets.

Table 1: RMSE and MAE with std. errors

Dataset STRX ARIMAX
RF XGT ENET DUAL RETAIN IMV-Full IMV-Tensor

NASDAQ 0.41 ± 0.01, 0.35 ± 0.02 0.34 ± 0.02, 0.23 ± 0.03 0.31 ± 0.02, 0.27 ± 0.03 0.28 ± 0.01, 0.23 ± 0.02 0.31 ± 0.03, 0.21 ± 0.01 0.31 ± 0.003, 0.21 ± 0.002 0.12 ± 0.07 , 0.11 ± 0.06 0.27 ± 0.01, 0.23 ± 0.01 0.09 ± 0.01, 0.07 ± 0.01

PLANT 231.43 ± 0.19, 193.23 ± 0.43 225.54 ± 0.23, 193.42 ± 0.41 164.23 ± 0.65, 130.90 ± 0.15 164.10 ± 0.54, 131.47 ± 0.21 168.22 ± 0.49, 137.04 ± 0.38 163.29 ± 0.54, 130.87 ± 0.12 250.69 ± 0.36, 190.11 ± 0.15 157.23 ± 0.16,128.13 ± 0.14 159.90 ± 0.22, 129.43 ± 0.10

SML 0.039 ± 0.001, 0.033 ± 0.001 0.060 ± 0.002, 0.053 ± 0.002 0.045 ± 0.001, 0.032 ± 0.001 0.017 ± 0.001, 0.013 ± 0.001 0.018 ± 0.001, 0.015 ± 0.001 0.019 ± 0.001, 0.015 ± 0.001 0.048 ± 0.001, 0.037 ± 0.001 0.015 ± 0.002, 0.012 ± 0.001 0.009 ± 0.0009, 0.006 ± 0.0005

4.4 INTERPRETATION
In this part, we investigate the interpretability of IMV-Full and IMV-Tensor by collecting the variable and temporal importance values during the training under the best hyper-parameters. As far as we know, experiments in previous work using attention in RNNs do not unveil such fine-grained interpretation over both variable and temporal level. Due to the page limitation, the interpretability results on other datasets are in the appendix section.
In Fig. 3, Panel (a) shows the overall variable importance values w.r.t. training epochs on PLANT dataset. Specifically, as variable importance converges, the ranking of variables is clearly identified at the end of the training, i.e. variables with high importance values are top ranked. Meanwhile, Panel (b) demonstrates the temporal importance values of each variable at the beginning and ending of the training (i.e. epoch 0 and 75). The lighter the color, the higher temporal importance value of the corresponding lag. At epoch 0, the randomly initialized network gives rises to similar temporal importance pattern for most of variables. At epoch 75, diverse patterns w.r.t different variables are learned. For instance, the importance value at around time lag 1 to 4 of variable "P-temperature" are obviously higher, which indicates that this variable has instant relation to the target. For variable "wind-speed", this heat map tells that its historical observations after time lag 14 could be negligibly correlated to the target.
Panel (c) shows the convergence of the variable importance of IMV-Tensor on PLANT dataset. The ranking of variable importance is slightly different to that in Panel (a). It is because in IMV-Tensor the tensorized gate and memory update scheme makes hidden states w.r.t. different variables evolve independently, thereby leading to different hidden states and attention values to IMV-Full. However,
2 Code will be released upon requested.

7

Under review as a conference paper at ICLR 2019

we can still decipher something in common. For instance, variable "wind-speed" stays relatively important in both Panel (a) and (c), i.e. rank 1st and 4th respectively. As for temporal importance, in Panel (d) variable "P-temperature" presents temporal importance pattern similar to that in Panel (b). It is also worth exploring whether we can obtain more consistent variable and temporal importance from IMV-Full and IMV-Tensor in the future work.

Variable importance

0.225 0.200 0.175 0.150 0.125 0.100 0.075 0.050 0.025
0

Epoch 0

Epoch 75

Irradiance P-temperature Cloud-cover Dew-point Humidity Pressure Temperature Wind-bearing Wind-speed Auto-regressive

Irradiance P-temperature
Cloud-cover Dew-point Humidity Pressure
Temperature

0.075 0.060 0.045

0.075 0.060 0.045

Wind-bearing Wind-speed

0.030

0.030

Auto-regressive

20 40 60 80 100 120 Epoch

19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1
Time step lag

19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1
Time step lag

0.015

(a) IMV-Full: variable importance.

Irradiance

P-temperature

0.20 Cloud-cover

Dew-point

Humidity

0.15

Pressure Temperature

Wind-bearing

Wind-speed 0.10 Auto-regressive

0.05

0 20 40 60 80 100 120 Epoch

(b) IMV-Full: variable-wise temporal importance at different epochs.

Irradiance P-temperature
Cloud-cover Dew-point Humidity Pressure
Temperature Wind-bearing
Wind-speed

Epoch 0

0.064 0.056 0.048 0.040 0.032

Epoch 75

0.075 0.060 0.045 0.030

Auto-regressive 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1
Time step lag

0.024

19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1
Time step lag

0.015

(c) IMV-Tensor: variable importance. (d) IMV-Tensor: variable-wise temporal importance at different epochs.

Variable importance

Figure 3: Variable and temporal importance interpretation during the training of IMV-Full and IMV-Tensor on PLANT dataset. (Best viewed in color)
4.5 VARIABLE IMPORTANCE FOR PREDICTION

In this group of experiments, we evaluate the efficacy of variable importance through the lens of prediction tasks. We focus on IMV-LSTM family and RNN baselines, i.e. DUAL and RETAIN. Specifically, for each approach we first rank variables respectively according to the variable importance in IMV-LSTM and variable attention in DUAL and RETAIN. Then we rebuild datasets only consisting of top 50% ranked variables for each approach (i.e. high importance or attention values) to retrain each model and obtain the prediction errors in Table 2. (The full ranking of variables is in the appendix.)
Ideally, effective variable importance leads to top variables highly related to the target and thus retrained models have comparable errors in comparison to their counterparts in Table 1. In particular, IMV-Full and IMV-Tensor present comparable and even lower errors in Table 2, while DUAL and RETAIN have higher errors mostly. Additional advantage of using top variables is the training efficiency, e.g. the training time of each epoch in IMV-Tensor is reduced from 16 sec to 11 sec.
Table 2: RMSE and MAE with std. errors under top 50% important variables

Dataset DUAL RETAIN IMV-Full IMV-Tensor

NASDAQ 0.16 ± 0.08, 0.16 ± 0.05 0.17 ± 0.03, 0.15 ± 0.02 0.26 ± 0.01, 0.23 ± 0.02 0.12 ± 0.007, 0.10 ± 0.01

PLANT 171.30 ± 0.17, 154.15 ± 0.20 226.38 ± 0.72, 167.90 ± 0.81 162.14 ± 0.10, 128.51 ± 0.12 157.64 ± 0.14, 128.86 ± 0.13

SML 0.026 ± 0.002, 0.018 ± 0.002 0.060 ± 0.001, 0.044 ± 0.004 0.015 ± 0.001, 0.011 ± 0.002 0.007 ± 0.001, 0.006 ± 0.001

5 CONCLUSION
In this paper, we propose an interpretable multi-variable LSTM (IMV-LSTM) for time series with exogenous variables. Based on the tensorized hidden states and update scheme, we present two realizations i.e. IMV-Full and IMV-Tensor as well as developing mixture temporal and variable attention mechanism. It enables to infer and quantify fine-grained variable-wise temporal importance and overall variable importance w.r.t. the target. Extensive experiments exhibit the superior prediction performance, interpretability and efficacy of variable importance of IMV-LSTM.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Leila Arras, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. Explaining recurrent neural network predictions in sentiment analysis. arXiv preprint arXiv:1706.07206, 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2014.
Jushan Bai and Serena Ng. Forecasting economic time series using targeted predictors. Journal of Econometrics, 146(2):304­317, 2008.
Christopher M Bishop. Mixture density networks. 1994.
Michelangelo Ceci, Roberto Corizzo, Fabio Fumarola, Donato Malerba, and Aleksandra Rashkovska. Predictive modeling of pv energy production: How to set up the learning task for a better prediction? IEEE Transactions on Industrial Informatics, 13(3):956­966, 2017.
Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In SIGKDD, pp. 785­794. ACM, 2016.
Haibin Cheng, Pang-Ning Tan, Jing Gao, and Jerry Scripps. Multistep-ahead time series prediction. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 765­774. Springer, 2006.
Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.
Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and Walter Stewart. Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. In Advances in Neural Information Processing Systems, pp. 3504­3512, 2016.
Heeyoul Choi, Kyunghyun Cho, and Yoshua Bengio. Fine-grained attention mechanism for neural machine translation. Neurocomputing, 284:171­176, 2018.
Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-based models for speech recognition. In Advances in neural information processing systems, pp. 577­585, 2015.
Lingyang Chu, Xia Hu, Juhua Hu, Lanjun Wang, and Jian Pei. Exact and consistent interpretation for piecewise linear neural networks: A closed form solution. In Proceedings of the 24th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1244­1253, New York, NY, USA, 2018. ACM.
Yagmur Gizem Cinar, Hamid Mirisaee, Parantapa Goswami, Eric Gaussier, Ali Aït-Bachir, and Vadim Strijov. Position-based content attention for time series forecasting with sequence-to-sequence rnns. In International Conference on Neural Information Processing, pp. 533­544. Springer, 2017.
Eugen Diaconescu. The use of narx neural networks to predict chaotic time series. Wseas Transactions on computer research, 3(3):182­191, 2008.
Kien Do, Truyen Tran, and Svetha Venkatesh. Matrix-centric neural networks. arXiv preprint arXiv:1703.01454, 2017.
Jean Feng, Brian D Williamson, Marco Carone, and Noah Simon. Nonparametric variable importance using an augmented neural network with multi-task learning. In International Conference on Machine Learning, pp. 1495­1504, 2018.
Jakob N Foerster, Justin Gilmer, Jascha Sohl-Dickstein, Jan Chorowski, and David Sussillo. Input switched affine networks: An rnn architecture designed for interpretability. In International Conference on Machine Learning, pp. 1136­1145, 2017.
Ian Fox, Lynn Ang, Mamta Jaiswal, Rodica Pop-Busui, and Jenna Wiens. Deep multi-output forecasting: Learning to accurately predict blood glucose trajectories. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#38; Data Mining, KDD '18, pp. 1387­1395. ACM, 2018.
9

Under review as a conference paper at ICLR 2019
Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pp. 1189­1232, 2001.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
Tian Guo, Tao Lin, and Yao Lu. An interpretable lstm neural network for autoregressive exogenous model. In workshop track at International Conference on Learning Representations, 2018.
Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, and David Barber. Wider and deeper, cheaper and faster: Tensorized lstms for sequence learning. In Advances in Neural Information Processing Systems, pp. 1­11, 2017.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Ziniu Hu, Weiqing Liu, Jiang Bian, Xuanzhe Liu, and Tie-Yan Liu. Listening to chaotic whispers: A deep learning framework for news-oriented stock trend prediction. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pp. 261­269. ACM, 2018.
Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2014.
Nan Rosemary Ke, Konrad Zolna, Alessandro Sordoni, Zhouhan Lin, Adam Trischler, Yoshua Bengio, Joelle Pineau, Laurent Charlin, and Chris Pal. Focused hierarchical rnns for conditional sequence processing. arXiv preprint arXiv:1806.04342, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Gebhard Kirchgässner, Jürgen Wolters, and Uwe Hassler. Introduction to modern time series analysis. Springer Science & Business Media, 2012.
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork rnn. In International Conference on Machine Learning, pp. 1863­1871, 2014.
Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for lstm networks. arXiv preprint arXiv:1703.10722, 2017.
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. arXiv preprint arXiv:1703.07015, 2017.
Andy Liaw, Matthew Wiener, et al. Classification and regression by randomforest. R news, 2(3): 18­22, 2002.
Tao Lin, Tian Guo, and Karl Aberer. Hybrid neural networks for learning the trend in time series. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pp. 2273­2279, 2017.
Zachary C Lipton. The mythos of model interpretability. arXiv preprint arXiv:1606.03490, 2016.
Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzell. Learning to diagnose with lstm recurrent neural networks. arXiv preprint arXiv:1511.03677, 2015.
Yan Liu, Alexandru Niculescu-Mizil, Aurelie C Lozano, and Yong Lu. Learning temporal causal graphs for relational time-series analysis. In ICML, pp. 687­694, 2010.
Christopher Meek, David Maxwell Chickering, and David Heckerman. Autoregressive tree models for time-series analysis. In SDM, pp. 229­244. SIAM, 2002.
W James Murdoch and Arthur Szlam. Automatic rule extraction from long short term memory networks. International Conference on Learning Representations, 2017.
W James Murdoch, Peter J Liu, and Bin Yu. Beyond word importance: Contextual decomposition to extract interactions from lstms. arXiv preprint arXiv:1801.05453, 2018.
10

Under review as a conference paper at ICLR 2019
Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased lstm: Accelerating recurrent network training for long or event-based sequences. In Advances in Neural Information Processing Systems, pp. 3882­3890, 2016.
Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. In Advances in Neural Information Processing Systems, pp. 442­450, 2015.
Jigar Patel, Sahil Shah, Priyank Thakkar, and K Kotecha. Predicting stock and stock price index movement using trend deterministic data preparation and machine learning techniques. Expert Systems with Applications, 42(1):259­268, 2015.
Yao Qin, Dongjin Song, Haifeng Cheng, Wei Cheng, Guofei Jiang, and Garrison W. Cottrell. A dual-stage attention-based recurrent neural network for time series prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI'17, pp. 2627­2633. AAAI Press, 2017.
Kira Radinsky, Krysta Svore, Susan Dumais, Jaime Teevan, Alex Bocharov, and Eric Horvitz. Modeling and predicting behavioral dynamics on the web. In WWW, pp. 599­608. ACM, 2012.
Matthew Riemer, Aditya Vempaty, Flavio Calmon, Fenno Heath, Richard Hull, and Elham Khabiri. Correcting forecasts with multifactor neural attention. In International Conference on Machine Learning, pp. 3010­3019, 2016.
Steven L Scott and Hal R Varian. Predicting the present with bayesian structural time series. International Journal of Mathematical Modelling and Numerical Optimisation, 5(1-2):4­23, 2014.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104­3112, 2014.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000­6010, 2017.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692­2700, 2015.
Jingyuan Wang, Ze Wang, Jianfeng Li, and Junjie Wu. Multilevel wavelet decomposition network for interpretable time series analysis. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#38; Data Mining, pp. 2437­2446, New York, NY, USA, 2018. ACM.
Yanbo Xu, Siddharth Biswal, Shriprasad R Deshpande, Kevin O Maher, and Jimeng Sun. Raim: Recurrent attentive and intensive model of multimodal patient monitoring data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2565­2573. ACM, 2018.
Jian Bo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiao Li Li, and Shonali Krishnaswamy. Deep convolutional neural networks on multichannel time series for human activity recognition. In IJCAI, pp. 25­31, 2015.
Ryad Zemouri, Rafael Gouriveau, and Noureddine Zerhouni. Defining and applying prediction performance metrics on a recurrent narx time series model. Neurocomputing, 73(13-15):2506­ 2521, 2010.
Liheng Zhang, Charu Aggarwal, and Guo-Jun Qi. Stock price prediction via discovering multifrequency trading patterns. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 2141­2149. ACM, 2017.
Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International Conference on Learning Representations, 2018.
Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301­320, 2005.
11

Under review as a conference paper at ICLR 2019

6 APPENDIX

6.1 INTERPRETABLE MULTI-VARIABLE LSTM

In IMV-Full and IMV-Tensor, the learning phase aims to minimize the loss function as follows:

MN
L() = - log N (yT +1,m | n( hTn,m  gmn ))
m=1 n=1

exp (f( hTn,m  gmn )) k exp (f( hTk ,m  gmk ))

+



2

(11)

6.2 MODEL INTERPRETATION

Variable importance

0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00
0

Epoch 0

Epoch 70

Temp. dinning Forecast temp. CO2 dinning CO2 room Humid. dinning Humid. room Lighting dinning Lighting room Sun dusk Wind Sunlight in west Sunlight in east Sunlight in south

Temp. dinning Forecast temp.
CO2 dinning CO2 room
Humid. dinning Humid. room
Lighting dinning Lighting room Sun dusk Wind
Sunlight in west Sunlight in east

0.1140 0.1125 0.1110

Sun irradiance

Sunlight in south

Outdoor temp. Outdoor humidity Auto-regressive

Sun irradiance Outdoor temp. Outdoor humidity

0.1095

Auto-regressive

20 40 60 80 100 120 Epoch

987654321
Time step lag

987654321
Time step lag

(a) Variable importance w.r.t. epochs.

(b) Variable-wise temporal importance at different epochs.

0.125 0.120 0.115 0.110 0.105

Figure 4: IMV-Full on SML dataset. (Best viewed in color)

Variable importance

0.6 0.5 0.4 0.3 0.2 0.1 0.0
0

Epoch 0

Epoch 70

Temp. dinning Forecast temp. CO2 dinning CO2 room Humid. dinning Humid. room Lighting dinning Lighting room Sun dusk

Temp. dinning Forecast temp.
CO2 dinning CO2 room
Humid. dinning Humid. room
Lighting dinning Lighting room

0.113 0.112

Wind Sun dusk

Sunlight in west Sunlight in east

Wind Sunlight in west

0.111

Sunlight in south

Sunlight in east

Sun irradiance Outdoor temp. Outdoor humidity Auto-regressive

Sunlight in south Sun irradiance Outdoor temp.
Outdoor humidity Auto-regressive

0.110 0.109

20 40 60 80 100 120 Epoch

987654321
Time step lag

987654321
Time step lag

(a) Variable importance w.r.t. epochs.

(b) Variable-wise temporal importance at different epochs.

0.114 0.112 0.110 0.108

Figure 5: IMV-Tensor on SML dataset. (Best viewed in color)

In the following table Table 3, 4, and 5, we list the full rank of variables of each approach in NASDAQ and SML datasets. Variables associated with the important or attention values are ranked in decreasing order.

12

Under review as a conference paper at ICLR 2019

Variable importance

0.200 0.175 0.150 0.125 0.100 0.075 0.050 0.025 0.000
0

AAL AAPL ADBE ADI ADP ADSK AKAM ALXN AMAT AMGN AMZN ATVI AVGO BBBY BIDU BIIB CA

CELG CERN CMCSA COST CSCO CSX CTRP CTSH DISCA DISH DLTR EA EBAY ESRX EXPE FAST FB

FOX FOXA GILD GOOGL INTC JD KHC LBTYA LBTYK LRCX MAR MAT MCHP MDLZ MSFT MU

MXIM MYL NCLH NFLX NTAP NVDA NXPI PAYX PCAR PYPL QCOM QVCA ROST SBUX SIRI STX

SWKS SYMC TMUS TRIP TSCO TSLA TXN VIAB VOD VRTX WBA WDC WFM XLNX YHOO Auto-regressive

20 40 60 80 100 120 140 Epoch

(a) Variable importance w.r.t. epochs.

Epoch 0 AAL AAPL ADBE ADI ADP ADSK AKAM ALXN AMAT AMGN AMZN ATVI AVGO BBBY BIDU BIIB CA CELG CERN CMCSA COST CSCO CSX CTRP CTSH DISCA DISH DLTR EA EBAY ESRX EXPE FAST FB FOX FOXA GILD GOOGL INTC JD KHC LBTYA LBTYK LRCX MAR MAT MCHP MDLZ MSFT MU MXIM MYL NCLH NFLX NTAP NVDA NXPI PAYX PCAR PYPL QCOM QVCA ROST SBUX SIRI STX SWKS SYMC TMUS TRIP TSCO TSLA TXN VIAB VOD VRTX WBA WDC WFM XLNX YHOO Auto-regressive
987654321 Time step lag

0.120

Epoch 75

0.116

0.112

0.108
0.104 987654321
Time step lag

(b) Variable-wise temporal importance at different epochs.

0.132 0.126 0.120 0.114 0.108 0.102

Figure 6: IMV-Full on NASDAQ dataset. (Best viewed in color)

13

Under review as a conference paper at ICLR 2019

Variable importance

0.25 0.20 0.15 0.10 0.05 0.00
0

AAL AAPL ADBE ADI ADP ADSK AKAM ALXN AMAT AMGN AMZN ATVI AVGO BBBY BIDU BIIB CA

CELG CERN CMCSA COST CSCO CSX CTRP CTSH DISCA DISH DLTR EA EBAY ESRX EXPE FAST FB

FOX FOXA GILD GOOGL INTC JD KHC LBTYA LBTYK LRCX MAR MAT MCHP MDLZ MSFT MU

MXIM MYL NCLH NFLX NTAP NVDA NXPI PAYX PCAR PYPL QCOM QVCA ROST SBUX SIRI STX

SWKS SYMC TMUS TRIP TSCO TSLA TXN VIAB VOD VRTX WBA WDC WFM XLNX YHOO Auto-regressive

20 40 60 80 100 120 140 Epoch

(a) Variable importance w.r.t. epochs.

Epoch 0 AAL AAPL ADBE ADI ADP ADSK AKAM ALXN AMAT AMGN AMZN ATVI AVGO BBBY BIDU BIIB CA CELG CERN CMCSA COST CSCO CSX CTRP CTSH DISCA DISH DLTR EA EBAY ESRX EXPE FAST FB FOX FOXA GILD GOOGL INTC JD KHC LBTYA LBTYK LRCX MAR MAT MCHP MDLZ MSFT MU MXIM MYL NCLH NFLX NTAP NVDA NXPI PAYX PCAR PYPL QCOM QVCA ROST SBUX SIRI STX SWKS SYMC TMUS TRIP TSCO TSLA TXN VIAB VOD VRTX WBA WDC WFM XLNX YHOO Auto-regressive
987654321 Time step lag

0.125

Epoch 75

0.120

0.115

0.110
0.105 987654321
Time step lag

(b) Variable-wise temporal importance at different epochs.

0.24 0.20 0.16 0.12 0.08

Figure 7: IMV-Tensor on NASDAQ dataset. (Best viewed in color)

14

Under review as a conference paper at ICLR 2019

Table 3: Variable importance ranking by IMV-Full and IMV-Tensor on NASDAQ dataset.

Dataset NASDAQ

Method IMV-Full
IMV-Tensor

Rank of variables according to importance
'ADSK', 0.00023858716, 'PAYX', 0.00023869322, 'AAL', 0.00023993119, 'MYL', 0.00024015515, 'CA', 0.00024144033], 'FOX', 0.00024341498, 'EA', 0.00024963205], 'BIDU', 0.00025009923, 'MCHP', 0.00025015706, 'QVCA', 0.00025018162, 'NVDA', 0.00025088928, 'WBA', 0.00025147066, 'LRCX', 0.00025165512, 'TSCO', 0.00025247637, 'CTSH', 0.00025284023, 'CSX', 0.00025417344, 'COST', 0.00025498777, 'BIIB', 0.00025547648, 'LBTYA', 0.00025680827, 'SIRI', 0.00025686354, 'ADBE', 0.00025687047, 'MDLZ', 0.00025788756, 'LBTYK', 0.00025885308, 'INTC', 0.00025894548, 'TSLA', 0.0002592771, 'WFM', 0.00025941888, 'SBUX', 0.00025953245, 'AVGO', 0.00026012328], 'CTRP', 0.00026024296, 'AMZN', 0.00026168497, 'ALXN', 0.00026173133, 'AMGN', 0.0002617908, 'GILD', 0.0002619058, 'VOD', 0.00026195042, 'ROST', 0.00026237246, 'NXPI', 0.0002624988, 'KHC', 0.0002625609, 'ADP', 0.0002626155, 'WDC', 0.00026269013, 'QCOM', 0.00026288, 'TMUS', 0.00026333777, 'AMAT', 0.00026334616, 'AKAM', 0.00026453246, 'PCAR', 0.00026510606, 'CERN', 0.00026535543, 'VRTX', 0.00026579297, 'MU', 0.00026719182, 'MAR', 0.00026789604, 'TXN', 0.00026821258, 'GOOGL', 0.0002684545, 'ESRX', 0.00026995668, 'ATVI', 0.0002703378, 'STX', 0.0002708045, 'FAST', 0.00027182887, 'EXPE', 0.0002747627, 'CELG', 0.00027897576, 'PYPL', 0.00027971127, 'MXIM', 0.0002802631, 'NFLX', 0.00028330996, 'BBBY', 0.00028975168, 'SYMC', 0.0002932911, 'CMCSA', 0.00031882498, 'SWKS', 0.00034903747, 'DLTR', 0.0004099159, 'YHOO', 0.0004359138, 'VIAB', 0.00046212596, 'Autoregressive', 0.0004718905, 'MAT', 0.0008193875, 'MSFT', 0.002350653, 'ADI', 0.0035426863, 'DISH', 0.0056709386, 'AAPL', 0.007597621, 'EBAY', 0.008922806, 'JD', 0.03449823, 'FB', 0.056254942, 'XLNX', 0.09711476, 'CSCO', 0.09782402, 'DISCA', 0.108503476, 'NCLH', 0.11029968, u'TRIP', 0.12302372, 'FOXA', 0.14510903, 'NTAP', 0.18010232 'ATVI', 0.00012247293, 'ADSK', 0.00012340973, 'FAST', 0.0001275845, 'WFM', 0.00013183481, 'ALXN', 0.00014380908, 'NFLX', 0.00014429294, 'QVCA', 0.00014494512, 'MSFT', 0.00014505234, 'BIDU', 0.00014950531, 'ESRX', 0.00015155961, 'DISCA', 0.00015276023, 'GILD', 0.00015325642], 'KHC', 0.00015800942, 'EBAY', 0.00015860642, 'NTAP', 0.00015893515, 'INTC', 0.0001592579, 'LBTYK', 0.00015955475], 'SWKS', 0.00015960005, 'SBUX', 0.0001602487, 'AMGN', 0.00016195989, 'AVGO', 0.00016398374, 'AMAT', 0.00016628107, 'FB', 0.0001681524], 'MYL', 0.00016860824, 'CELG', 0.00016944246, 'BIIB', 0.00016954532, 'CTRP', 0.00016966274, 'DLTR', 0.00017032732, 'ROST', 0.00017111507, 'MXIM', 0.00017283233, 'CTSH', 0.00017294307, 'TMUS', 0.00017294812], 'CERN', 0.00017299024, 'MDLZ', 0.0001731659, 'EA', 0.00017319905, 'CA', 0.00017323176, 'NVDA', 0.0001732874], 'COST', 0.00017329257, 'FOX', 0.00017335685, 'EXPE', 0.00017337044, 'CMCSA', 0.00017339012, 'QCOM', 0.00017341232, 'PCAR', 0.00017345154, 'ADI', 0.00017346471, 'TXN', 0.00017350669, 'PAYX', 0.00017363002, 'SYMC', 0.00017364287, 'TSCO', 0.0001738104, 'CSCO', 0.000173811, 'GOOGL', 0.0001748285, 'AMZN', 0.00018132143, 'STX', 0.00018217335, 'VRTX', 0.0001833355, 'MAT', 0.00018341257, 'AAL', 0.00019685448, 'YHOO', 0.00019987396, 'JD', 0.00020849655, 'XLNX', 0.0002687659, 'FOXA', 0.0004502886, 'WBA', 0.0004633159, 'Autoregressive', 0.000513615, 'WDC', 0.00055753137, 'ADBE', 0.000716344, 'TSLA', 0.001061617, [u'MCHP', 0.0031253153, 'AAPL', 0.004319694, 'SIRI', 0.0043806615, 'VOD', 0.007254429, 'ADP', 0.012473345, 'AKAM', 0.012664658, 'TRIP', 0.016278176, 'MAR', 0.022737814, 'CSX', 0.028656403, 'MU', 0.05227967, 'BBBY', 0.07014921, 'DISH', 0.07253124, 'NXPI', 0.09813042, 'PYPL', 0.10145339, 'VIAB', 0.10335469, 'LBTYA', 0.114357226, 'LRCX', 0.11704111, 'NCLH', 0.14532024

15

Under review as a conference paper at ICLR 2019

Table 4: Variable importance ranking by DUAL and RETAIN methods on NASDAQ dataset.

Dataset NASDAQ

Method DUAL
RETAIN

Rank of variables according to importance
'NXPI', 0.003557, 'QCOM', 0.003564, 'FOX', 0.003566, 'NTAP', 0.003566, 'CELG', 0.003566, 'FOXA', 0.003567, 'PAYX', 0.003567, 'AAPL', 0.003567, 'WFM', 0.003567, 'ADSK', 0.003567, 'SBUX', 0.003567, 'STX', 0.003567, 'AKAM', 0.003567, 'DISH', 0.003567, 'AVGO', 0.003567, 'XLNX', 0.003567, 'AAL', 0.003567, 'FAST', 0.003567, 'TMUS', 0.003567, 'LRCX', 0.003567, 'NCLH', 0.003567, 'MCHP', 0.003567, 'MSFT', 0.003567, 'MU', 0.003567, 'NFLX', 0.003567, 'NVDA', 0.003567, 'PCAR', 0.003567, 'SIRI', 0.003567, 'MAR', 0.003567, 'TXN', 0.003567, 'ROST', 0.003567, 'CMCSA', 0.003567, 'ADI', 0.003567, 'ADP', 0.003567, 'DISCA', 0.003567, 'AMAT', 0.003567, 'WDC', 0.003567, 'CSX', 0.003567, 'WBA', 0.003567, 'GOOGL', 0.003622, 'COST', 0.003678, 'INTC', 0.003712, 'CTSH', 0.003908, 'BBBY', 0.004027, 'TRIP', 0.004881, 'MAT', 0.004956, 'ATVI', 0.005121, 'LBTYK', 0.00523, 'CERN', 0.00524, 'CTRP', 0.005283, 'ALXN', 0.00536, 'VOD', 0.005369, 'VRTX', 0.005433, 'LBTYA', 0.005445, 'MXIM', 0.00554, 'BIIB', 0.005554, 'EBAY', 0.005555, 'BIDU', 0.005605, 'FB', 0.005654, 'VIAB', 0.005685, 'GILD', 0.005695, 'AMGN', 0.005716, 'MYL', 0.005737, 'YHOO', 0.006166, 'KHC', 0.006555, 'AMZN', 0.006605, 'CSCO', 0.007836, 'ESRX', 0.010614, 'SWKS', 0.012777, 'MDLZ', 0.017898, 'CA', 0.02198, 'EXPE', 0.024373, 'QVCA', 0.026462, 'EA', 0.027808, 'TSLA', 0.043082, 'ADBE', 0.043829, 'JD', 0.071079, 'SYMC', 0.081596, 'PYPL', 0.087612, 'DLTR', 0.119737, 'TSCO', 0.122887 'DLTR', 0.000866, 'QVCA', 0.001128, 'TSLA', 0.00119, 'PYPL', 0.00128, 'EA', 0.001439, 'EXPE', 0.001502, 'CA', 0.001713, 'TSCO', 0.001737, 'SYMC', 0.002334, 'ADBE', 0.00252, 'JD', 0.002607, 'AMZN', 0.003367, 'CSCO', 0.003543, 'KHC', 0.003996, 'CTSH', 0.004695, 'NXPI', 0.004865, 'EBAY', 0.004963, 'SWKS', 0.005011, 'MXIM', 0.005135, 'MYL', 0.005541, 'COST', 0.006052, 'BIDU', 0.006534, 'GOOGL', 0.006906, 'INTC', 0.007153, 'GILD', 0.007212, 'ESRX', 0.007512, 'NTAP', 0.007695, 'QCOM', 0.008037, 'CELG', 0.008168, 'MDLZ', 0.008829, 'AMGN', 0.008998, 'FOX', 0.009943, 'VIAB', 0.010123, 'AAPL', 0.010157, 'FB', 0.010359, 'YHOO', 0.010744, 'PAYX', 0.010899, 'BBBY', 0.01117, 'AKAM', 0.012054, 'BIIB', 0.012069, 'NFLX', 0.012266, 'ADSK', 0.012319, 'DISH', 0.012338, 'LBTYA', 0.012697, 'FOXA', 0.01282, 'MCHP', 0.012833, 'WFM', 0.012869, 'STX', 0.012887, 'VRTX', 0.013318, 'SBUX', 0.013458, 'VOD', 0.013798, 'ALXN', 0.013878, 'CTRP', 0.013963, 'SIRI', 0.01475, 'CERN', 0.014777, 'LBTYK', 0.014799, 'ATVI', 0.015651, 'AVGO', 0.016382, 'CMCSA', 0.016531, 'TXN', 0.016977, 'LRCX', 0.017131, 'AMAT', 0.017378, 'ROST', 0.017399, 'MU', 0.018045, 'TRIP', 0.018236, 'MAT', 0.018297, 'NDX', 0.018626, 'WDC', 0.019083, 'DISCA', 0.019233, 'FAST', 0.019392, 'CSX', 0.019734, 'WBA', 0.019984, 'AAL', 0.021188, 'ADI', 0.021215, 'NCLH', 0.022932, 'NVDA', 0.022994, 'TMUS', 0.024187, 'MSFT', 0.026354, 'ADP', 0.028515, 'MAR', 0.028783, 'PCAR', 0.029459, 'XLNX', 0.03248

16

Under review as a conference paper at ICLR 2019

Dataset PLANT
SML

Table 5: Variable importance ranking on PLANT and SML datasets.

Method IMV-Full IMV-Tensor DUAL RETAIN IMV-Full
IMV-Tensor
DUAL
RETAIN

Rank of variables according to importance
'Dew-point', 0.040899094, 'Wind-bearing', 0.04476319, 'Pressure', 0.06180005, 'P-temperature', 0.07244386, 'Auto-regressive', 0.1083069, 'Temperature', 0.11868146, 'Irradiance', 0.12043289, 'Humidity', 0.13192631, 'Cloud-cover', 0.14283147, 'Wind-speed', 0.15791483 'Dew-point', 0.034108493, 'Temperature', 0.041016363, 'Cloud-cover', 0.07639352, 'Irradiance', 0.08453229, 'Humidity', 0.09652364, 'Pressure', 0.10533282, 'Wind-speed', 0.115569875, 'P-temperature', 0.12627581, 'Autoregressive', 0.15163974, 'Wind-bearing', 0.16860741 'Irradiance', 0.06128826, 'Dew-point', 0.066655099, 'Temperature', 0.071131147, 'Wind-speed', 0.094427079, 'Wind-bearing', 0.106529392, 'P-temperature', 0.115000054, 'Pressure', 0.115962856, 'Cloud cover', 0.144996881, 'Humidity', 0.224009201 'Dewpoint', 0.031317, 'Temperature', 0.037989, 'Wind-bearing', 0.044226, 'Wind-speed', 0.052027, 'P-temperature', 0.053034, 'Cloud cover', 0.138427, 'Irradiance', 0.142899, 'Auto-regressive', 0.143269, 'Humidity', 0.172893, 'Pressure', 0.183919
'Outdoor temp.', 0.008530081, 'Outdoor humidity', 0.0120737655, 'Sun irradiance', 0.012943255, 'CO2 dinning', 0.01563413, 'Sunlight in south', 0.01569774, 'Sun dusk', 0.015769556, 'Wind', 0.015868865], 'Forecast temp.', 0.015990425, 'Sunlight in west', 0.01609429, 'Lighting dinning', 0.016338758, 'Humid. dinning', 0.016379833, 'Sunlight in east', 0.016386982, 'Autoregressive', 0.016530316, 'Temp. dinning', 0.01663947, 'Lighting room', 0.18322693, 'CO2 room', 0.26715645, 'Humid. room', 0.33873916 'Temp. dinning', 0.00178118, 'Auto-regressive', 0.0019085788, 'Lighting room', 0.0019707098, 'Outdoor humidity', 0.0019725773, 'CO2 room', 0.0019752455, 'Wind', 0.0019779552, 'Sunlight in south', 0.0019810565, 'Sunlight in east', 0.0019821296, 'Sun dusk', 0.001982501, 'Humid. room', 0.0020072663, 'Sunlight in west', 0.0028204536, 'Outdoor temp.', 0.0031943072, 'CO2 dinning', 0.0056894314, 'Forecast temp.', 0.07802243, 'Humid. dinning', 0.1383277, 'Lighting dinning', 0.26305506, 'Sun irradiance', 0.48935142 'Humid. room', 0.059424, 'Humid. dinning', 0.059656, 'Outdoor humidity', 0.059803, 'Temp. dinning', 0.059878, 'Sun dusk', 0.060408, 'Sunlight in south', 0.061626, 'Wind', 0.061629, 'Sunlight in east', 0.062792, 'Lighting room', 0.063381, 'Forecast temp.', 0.063503, 'Sunlight in west', 0.063832, 'CO2 room', 0.064149, 'CO2 dinning', 0.064383, 'Sun irradiance', 0.064703, 'Lighting dinning', 0.0651, 'Outdoor temp.', 0.065733 'Humid. dinning', 0.012169, 'Humid. room', 0.014563, 'Sunlight in south', 0.018446, 'Lighting room', 0.018732, 'Outdoor humidity', 0.019388, 'Sunlight in west', 0.02219, 'Sunlight in east', 0.036744, 'CO2 room', 0.036864, 'CO2 dinning', 0.037174, 'Sun dusk', 0.040011, 'Sun irradiance', 0.04075, 'Wind', 0.041191, 'Lighting dinning', 0.054166, 'Forecast temp.', 0.133079, 'Outdoor temp.', 0.144314, 'Temp. room', 0.164673, 'Temp. dinning', 0.165543

17

