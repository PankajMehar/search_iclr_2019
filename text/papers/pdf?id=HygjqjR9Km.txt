Under review as a conference paper at ICLR 2019
IMPROVING MMD-GAN TRAINING WITH REPULSIVE LOSS FUNCTION
Anonymous authors Paper under double-blind review
ABSTRACT
Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance heavily depends on the loss functions used in training. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of data structures as it attempts to attract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieved an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization.
1 INTRODUCTION
Generative adversarial nets (GANs) (Goodfellow et al. (2014)) are a branch of generative models that learns to mimic the real data generating process. GANs have been intensively studied in recent years, with a variety of successful applications (Karras et al. (2018); Li et al. (2017b); Lai et al. (2017); Zhu et al. (2017); Ho & Ermon (2016)). The idea of GANs is to jointly train a generator network that attempts to produce artificial samples, and a discriminator network or critic that distinguishes the generated samples from the real ones. Compared to maximum likelihood based methods, GANs tend to produce samples with sharper and more vivid details but they are much harder to train.
Recent studies on improving GAN training have mainly focused on designing loss functions, network architectures and training procedures. A loss function, or simply loss, defines quantitatively the difference of discriminator outputs or scores between real and generated samples. Its gradients are used to train the generator and discriminator. This study focuses on a loss function called maximum mean discrepancy (MMD), which is well known as the distance metric between two probability distributions and widely applied in kernel two-sample test (Gretton et al. (2012)). Theoretically, MMD reaches its global minimum of zero if and only if the tested two distributions are equal. Thus, MMD has been applied to assess the quality of generated samples directly (Li et al. (2015)) and extended to the GAN framework recently (Unterthiner et al. (2018); Li et al. (2017a); Bin´kowski et al. (2018)).
In this paper, we interpret the optimization of MMD loss by the discriminator as a combination of attraction and repulsion processes, similar to that of linear discriminant analysis. We argue that the existing MMD loss may discourage the learning of data structures, as the discriminator attempts to minimize the within-group variance of its outputs for the real data. To address this issue, we propose a repulsive loss for the discriminator that explicitly explores the differences among real data. The proposed loss achieved significant improvements over the MMD loss on image generation tasks of four benchmark datasets, without incurring any additional computational cost. Furthermore, a bounded Gaussian kernel is proposed such that using a single kernel in MMD is sufficient, in contrast
1

Under review as a conference paper at ICLR 2019

to a linear combination of kernels used in Li et al. (2017a) and Bin´kowski et al. (2018). This may further bring down the computational cost of the MMD loss in a variety of applications.
The paper is organized as follows. Section 2 reviews the GAN trained using the MMD loss (MMDGAN). We propose the repulsive loss in Section 3, two practical techniques to improve the model stability in Section 4, and the results of extensive experiments in Section 5. At last, we discuss the connections of our model to existing work in Section 6.

2 MMD-GAN

This section introduces GAN and the MMD loss. Consider a random variable X  X with an empirical data distribution PX to be learned. A typical GAN model consists of two neural networks: a generator G and a discriminator D. The G maps a latent code z with a fixed distribution PZ, e.g., Gaussian, to the data space X : y = G(z)  X , where y is called the generated samples with distribution PG. The D maps a sample a to some score D(a)  Rd, where a may be y or a real sample x. We assume linear activation is used at the last layer of D.
Several loss functions have been proposed to calculate the difference between the scores of real and generated samples: {D(x)} and {D(y)}, including

· The Minimax loss (Goodfellow et al. (2014)): LD = EPX [Softplus(-D(x))] + EPZ [Softplus(D(G(z)))] and LG = -LD, which can be derived from the Jensen­Shannon (JS) divergence between PX and the model distribution PG.
· The non-saturating loss (Goodfellow et al. (2014)) is a variant of the minimax loss with the same LD and LG = EPZ [Softplus(-D(G(z)))].
· The Hinge loss (Tran et al. (2017)): LD = EPX [ReLU(1 - D(x))] + EPZ [ReLU(1 + D(G(z)))], LG = EPZ [-D(G(z))], which is notably known for usage in support vector machines and is related to the total variation (TV) distance (Nguyen et al. (2009)).
· The Wasserstein loss (Arjovsky et al. (2017); Gulrajani et al. (2017)) is derived from the Wasserstein distance between PX and PG: LG = - EPZ [D(G(z))], LD = EPZ [D(G(z))] - EPX [D(x)], where D is subject to some Lipschitz constraint.
· The maximum mean discrepancy (MMD) (Li et al. (2017a); Bin´kowski et al. (2018)).

Among them, MMD uses kernel embedding (a) = k(·, a) associated with a characteristic kernel
k such that  is infinite-dimensional and (a), (b) H = k(a, b). The squared MMD distance between two distributions P and Q is then

Mk2(P, Q) =

µP

- µQ

2 H

=

Ea,a

P

[k(a,

a

)]

+

Eb,b

Q[k(b,

b

)]

-

2EaP,bQ[k(a,

b)]

(1)

The kernel k(a, b) measures the similarity between two samples a and b. Gretton et al. (2012) proved that, using a characteristic kernel k, Mk2(P, Q)  0 with equality applies if and only if P = Q.

In MMD-GAN, the discriminator D can be interpreted as forming a new kernel with k: kD(a, b) = k(D(a), D(b)) = kD(a, b). If D is injective, k  D is characteristic and Mk2D(PX, PG) reaches its minimum only when PX = PG (Li et al. (2017a)). Thus, the objective functions for D and G
could be (Li et al. (2017a); Bin´kowski et al. (2018)):

min
D

LaDtt

=

-Mk2D (PX ,

PG)

=

2EPX,PG [kD(x,

y)]

-

EPX [kD(x,

x

)]

-

EPG [kD(y,

y

)]

(2)

min
G

LGmmd

=

Mk2D (PX ,

PG)

=

EPG [kD(y,

y

)]

-

2EPX,PG [kD(x,

y)]

+

C

(3)

where C = EPX [kD(x, x )] is independent of G. This approach has been shown to improve over directly using MMD as the loss function for the generator G (Li et al. (2017a)).

Liu et al. (2017) showed that Wasserstein metric and MMD are weaker objective functions for GAN than the JS-divergence and TV distance, in the sense that convergence of PG to PX in JS-divergence and TV distance also implies convergence in Wasserstein metric and MMD. Weak metrics are desirable as they provide more information on adjusting the model to fit the data distribution (Liu et al. (2017)). Nagarajan & Kolter (2017) proved that the GAN trained using the minimax loss and

2

Under review as a conference paper at ICLR 2019

gg gg
gg

rr rr
rr

rr
gg r gg r
gg
rr

rr
gg r gg r
gg
rr

(a) LDatt (Eq. 2)

(b) LrDep,1 (Eq. 5)

(c) LGmmd paired with LDrep,1

Figure 1: Illustration of the gradient directions of each loss on the real sample scores {D(x)} ("r"
nodes) and generated sample scores {D(y)} ("g" nodes). The blue arrows stand for attraction and the orange arrows for repulsion. When LmGmd is paired with LDatt, the gradient directions of LmGmd on {D(y)} can be obtained by reversing the arrows in (a), thus are omitted.

gradient updates on model parameters is locally exponentially stable near equilibrium, while the GAN using Wasserstein loss is not. In Appendix A, we demonstrate that the MMD-GAN trained using gradient methods is locally exponentially stable near equilibrium.

3 REPULSIVE LOSS FUNCTION

In this section, we interpret the MMD-GAN training using LDatt and LmGmd, both as a combination of attraction and repulsion processes, and propose a novel loss function for the discriminator by rearranging these components.
First, consider a linear discriminant analysis (LDA) model as the discriminator. The task is to find a projection w to maximize the between-group variance wT µx - wT µy and minimize the withingroup variance wT (x + y)w, where µ and  are group mean and covariance.
We then consider the neural network discriminator in MMD-GAN. By minimizing LDatt, the discriminator D tackles two tasks: 1) D reduces EPX,PG [kD(x, y)], i.e., repulses the two groups {D(x)} and {D(y)} from each other (see Fig 1a orange arrows); and 2) D increases EPX [kD(x, x )] and EPG [k(y, y )], i.e. attracts {D(x)} and {D(y)} within each group (see Fig 1a blue arrows). Thus, similar to the LDA method, D may be interpreted as finding nonlinear projections to maximize the between-group variance and minimize the within-group variance of projected scores. We refer to loss functions that attract the real data scores {D(x)} as attractive losses.
We argue that the attractive loss in Eq. 2 has two issues that may slow down the GAN training:

1. D may focus more on the similarities among real samples (in order to attract {D(x)}) than the fine details that separate them. Initially, G produces low-quality samples and it may be adequate for D to learn the common features of {x} in order to distinguish between {x} and {y}. Only when {D(y)} is sufficiently close to {D(x)} will D learn the fine details of {x} to be able to separate {D(x)} from {D(y)}. Consequently, D may leave out the data information for G to access during training.
2. As shown in Fig 1a, the gradients on D(y) from the attraction (blue arrows) and repulsion (orange arrows) terms in LDatt (and thus LGmmd) may have opposite directions during training. Their summation may be small in magnitude even when D(y) is far away from D(x), which may cause G to get stagnated locally.

Therefore, we propose a different tactic called repulsive loss for the discriminator D: LrDep, = EPX [kD(x, x )] - ( - 1)EPX,PG [kD(x, y)] - EPG [kD(y, y )]

(4)

3

Under review as a conference paper at ICLR 2019

10 1

10 0.2 0.5

0.8 2

0.8 1 2

0.6 0.4

2 22
4
Mean

-0.2 -0.4

1 2
2 22

0.6 0.4

5 Mean

-0.2 -0.4

0.2 0.5 1 2

0.2 4 0.2 5

Mean

Mean

0 -0.6 0 -0.6

0 2 4 6 8 10

0 2 4 6 8 10 0 2 4 6 8 10

0 2 4 6 8 10

(a) (b) (c) (d)

Figure 2: (a) Gaussian kernels {krbif(a, b)} and their mean as a function of e = a - b , where

i  {1, 2, 2, 2 2, 4} rational quadratic kernel of {krqi (a, b)} in (c).

were used in our experiments; (b) derivatives of {krqi (a, b)} and their mean, where i  {0.2, 0.5,

{krbif(a, 1, 2, 5};

b)} (d)

in (a); (c) derivatives

where the weight   0 is a hyper-parameter1. In its simplest form ( = 1),

LDrep,1 = EPX [kD(x, x )] - EPG [kD(y, y )]

(5)

The generator still uses the same MMD loss as before (see Eq. 3). Thus, the adversary lies in
D attracting {D(y)} via maximizing EPG [kD(y, y )] (see Fig 1b) and G repulsing {D(y)} (see Fig 1c). On the other hand, D also learns to separate the real data by minimizing EPX [kD(x, x )], which actively explores the fine details in data and may result in more meaningful gradients for G.

Note that in Eq. 5, D does not explicitly push the average score of {D(y)} away from that of

{D(x)} because it may have no effect on the pair-wise sample distances. But G explicitly matches

the average scores. Thus, with LrDep,1 than LDatt (see

we Fig

believe it is less likely 1c). At last, we note

for LmGmd that the

to yield opposite gradients when paired proposed LDrep, (Eq. 4) has exactly the

same computational cost as LaDtt, (Eq. 2), given that we only rearranged the terms in LaDtt,.

4 REGULARIZATION ON MMD AND DISCRIMINATOR

This section presents two approaches as well as the proposed repulsive loss

to improve LDrep, (Eq.

the 4).

training of MMD-GAN using We focus on image generation

LDatt (Eq. 2) tasks using

convolutional neural networks (CNN) for both G and D. First, we propose a bounded kernel for

MMD that improve the model stability; then a generalized power iteration method to estimate

the spectral norm of a convolutional kernel, which was used in spectral normalization on the

discriminator in all experiments.

4.1 KERNEL IN MMD

For MMD-GAN, the following two kernels have been used:

· Gaussian radial basis function (RBF), or Gaussian kernel (Li et al. (2017a)), krbf(a, b) =

exp(-

1 22

a - b 2) where  > 0 is the kernel scale or bandwidth.

·

Rational

quadratic

kernel

(Bin´ kowski

et

al.

(2018)),

krq(a,

b)

=

(1

+

1 2

a - b 2)-, where the

kernel scale  > 0 corresponds to a mixture of Gaussian kernels with a Gamma(, 1) prior on

the inverse kernel scales -1.

It is interesting that both studies used a linear combination of kernels with five different kernel scales,

i.e., krbf =

5 i=1

krbif

and

krq

=

5 i=1

krqi ,

where

i



{1, 2, 4, 8, 16},

i



{0.2, 0.5, 1, 2, 5}

(see

Fig 2a and 2c for illustration). We suspect the reason is that a single kernel k(a, b) is saturated when

the distance a - b is either too large or too small compared to the kernel scale (see Fig 2b and

2d), which may cause diminishing gradients during training. Both Li et al. (2017a) and Bin´kowski

et al. (2018) applied penalties on the discriminator parameters but not to the MMD loss. Thus the

1The weights for the three terms in LrDep, sum up to zero. This is to make sure the LDrep,/D is zero at equilibrium PX = PG, where D is the parameters of D.

4

Under review as a conference paper at ICLR 2019

saturation issue may still exist. Using a linear combination of kernels with different kernel scales may alleviate this issue but not eradicate it.

Inspired by the hinge loss, we propose a bounded RBF (RBF-B) kernel for the discriminator. The
idea is to prevent D from pushing {D(x)} too far away from {D(y)} and causing saturation. For LaDtt in Eq. 2, the RBF-B kernel is:

krbf-b(a, b) =

exp(-

1 22

max(

a-b

2 , bl))

exp(-

1 22

min(

a-b

2 , bu))

if a, b  {D(x)} or a, b  {D(y)} if a  {D(x)} and b  {D(y)}

For LDrep, in Eq. 4 and LDrep,1 Eq. 5, the RBF-B kernel is:

(6)

krbf-b(a, b) =

exp(-

1 22

max(

a-b

2 , bl))

exp(-

1 22

min(

a-b

2 , bu))

if a, b  {D(y)} if a, b  {D(x)}

(7)

where bl and bu are the lower and upper bounds. As such, a single kernel is sufficient and we set  = 1, bl = 0.25 and bu = 4 in all experiments for simplicity and leave their tuning for future work.
It should be noted that RBF-B kernel is among many techniques to address the saturation issue. We found other methods, e.g., random sampling kernel scale, instance noise (Sønderby et al. (2017)) and label smoothing Szegedy et al. (2016); Salimans et al. (2016), may also improve the model performance and stability. However, the computational cost of RBF-B kernel is relatively low.

4.2 SPECTRAL NORMALIZATION IN DISCRIMINATOR
Arjovsky et al. (2017) showed that, if the supports of PX and PG do not overlap, there exists a perfect discriminator to separate {x} and {y}. Without any Lipschitz constraints on D, it may simply increase the magnitude of its outputs to minimize the discriminator loss, causing unstable training. Spectral normalization divides the weight matrix of each layer by its spectral norm, which imposes an upper bound on the magnitudes of outputs and gradients at each layer of D (Miyato et al. (2018)). However, to estimate the spectral norm of a convolution kernel, Miyato et al. (2018) reshaped the kernel into a matrix. We propose a generalized power iteration method to directly estimate the spectral norm of a convolution kernel (see Appendix B.1 for details) and applied spectral normalization to the discriminator in all experiments.

5 EXPERIMENTS
In this section, we empirically evaluate the proposed 1) repulsive loss LrDep, (Section 3) on unsupervised training of GAN for image generation tasks; and 2) RBF-B kernel to stabilize MMD-GAN training. To show the efficacy of LrDep,, we compared the loss functions (LDrep,, LGmmd) using Gaussian kernel (MMD-rep) with (LaDtt,, LmGmd) using Gaussian kernel (MMD-rbf) (Li et al. (2017a)) and rational quadratic kernel (MMD-rq) (Bin´kowski et al. (2018)), as well as non-saturating loss (Goodfellow et al. (2014)) and hinge loss (Tran et al. (2017)). To show the efficacy of RBF-B kernel, we applied it to both LDatt, and LrDep,, resulting in two methods MMD-rbf-b and MMD-rep-b. The Wasserstein loss was excluded for comparison because it cannot be directly used with spectral normalization ( Miyato et al. (2018)).
5.1 EXPERIMENT SETUP
Dataset: The loss functions were evaluated on four datasets: 1) CIFAR-10 (50K images, 32 × 32 pixels) (Krizhevsky & Hinton (2009)); 2) STL-10 (100K images, 48 × 48 pixels) (Coates et al. (2011)); 3) CelebA (about 203K images, 64 × 64 pixels) (Liu et al. (2015)); and 4) LSUN bedrooms (around 3 million images, 64×64 pixels) (Yu et al. (2015)). The images were scaled to range [-1, 1] to avoid numeric issues.
Network architecture: The DCGAN (Radford et al. (2016)) architecture from Miyato et al. (2018) was used (see Appendix B.2 for details). In all experiments, batch normalization (BN) (Ioffe & Szegedy (2015)) was used in the generator and spectral normalization with the generalized power

5

Under review as a conference paper at ICLR 2019

Table 1: Inception score (IS), Fre´chet Inception distance (FID) and multi-scale structural similarity (MS-SSIM) on image generation tasks

Methods
Real data
Non-saturating Hinge MMD-rbf2 MMD-rq2

CIFAR-101 IS FID

STL-101 IS FID

CelebA1

LSUN-bedrom1

FID MS-SSIM FID MS-SSIM

11.31 2.09 26.37 2.10 1.09 0.2678 1.24 0.0915

7.39 23.23 8.25 48.53 10.64 7.33 23.46 8.24 49.44 8.60 7.05 28.38 8.13 57.52 13.03 7.22 27.00 8.11 54.05 12.74

0.2895 0.2894 0.2937 0.2935

23.66 16.73

0.1027 0.0946

MMD-rbf-b MMD-rep MMD-rep-b

7.18 25.25 8.07 51.86 10.09 7.99 16.65 9.36 36.67 7.20 8.29 16.21 9.34 37.63 6.79

0.3090 0.2761 0.2659

32.29 16.91 12.52

0.1001 0.0901 0.0908

1 MS-SSIM is not compatible with CIFAR-10 and STL-10 which have data from many classes; for CelebA
and LSUN-bedroom, IS is not meaningful (Bin´kowski et al. (2018)); thus they are not shown. 2 On LSUN-bedroom, MMD-rbf and MMD-rq did not achieve reasonable results and thus are omitted.

iteration (see Appendix B.1) in the discriminator. For MMD related losses, the dimension of discriminator output layer is set to 16; for non-saturating loss and hinge loss, it is set to 1.
Hyper-parameters: We used Adam optimizer (Kingma & Ba (2015)) with momentum parameters 1 = 0.5, 2 = 0.999; two-timescale update rule (TTUR) (Heusel et al. (2017)) with two learning rates (D, G) chosen from {1e-4, 2e-4, 5e-4, 1e-3} (16 combinations in total); and batch size 64. Fine-tuning on learning rates may improve the model performance, but constant learning rates were used for fair comparison. All models were trained for 100K iterations on CIFAR-10, STL-10, CelebA and LSUN bedrooms dataset, with ndis =1, i.e.,one discriminator update per generator update2. For MMD-rbf, the kernel scales i  {1, 2, 2, 2 2, 4} were used due to better performance than the original values in Li et al. (2017a). For MMD-rq, i  {0.2, 0.5, 1, 2, 5}. For MMD-rbf-b, MMD-rep, MMD-rep-b, a single Gaussian kernel with  = 1 was used.
Evaluation metrics: Inception score (IS) (Salimans et al. (2016)), Fre´chet Inception distance (FID) (Heusel et al. (2017)) and multi-scale structural similarity (MS-SSIM) (Wang et al. (2003)) were used for quantitative evaluation. Both IS and FID are calculated using a pre-trained Inception model (Szegedy et al. (2016)). Higher IS and lower FID scores indicate better image quality. MS-SSIM calculates the pair-wise image similarity and is used to detect mode collapses among images of the same class (Odena et al. (2017)). Lower MS-SSIM values indicate perceptually more diverse images. For each model, 50K randomly generated samples and 50K real samples were used to calculate IS, FID and MS-SSIM.
5.2 QUANTITATIVE ANALYSIS
Table 1 shows the IS, FID and MS-SSIM scores of applying different loss functions on the benchmark datasets with the optimal learning rate settings tested experimentally. The proposed MMD-rep and MMD-rep-b methods (both using LDre,p1 in Eq. 5) achieved the best scores on almost all tasks. We observed that: 1) MMD-rep-b performed significantly better than MMD-rbf-b by changing only the loss function for discriminator, showing the proposed repulsive loss can greatly improve MMD-GAN training; 2) Using a single kernel, MMD-rbf-b still performed better than MMD-rbf and MMD-rq which used a linear combination of five kernels, indicating that the kernel saturation may be an issue that slows down MMD-GAN training; 3) MMD-rep-b performed better than MMD-rep on the CelebA and LSUN-bedroom datasets where we found MMD-rep was not stable on many learning rate combinations.
2Increasing or decreasing ndis may improve the model performance in some cases, but it has significant impact on the computation cost. For simple and fair comparison, we set ndis = 1 in all cases.
6

Under review as a conference paper at ICLR 2019



FID FID

26 24 22 20 18 16
 = -1

 = -0.5



=0

 = 0.5

(a) MMD-rep

=1

=2

26 24 22 20 18 16
 = -1

 = -0.5

=0

 = 0.5

(b) MMD-rep-b

=1

=2

Figure 3: FID scores of (a) MMD-rep and (b) MMD-rep-b on CIFAR-10 dataset for 16 learning rate combinations and   {-1, -0.5, 0, 0.5, 1, 2}. For each , the order of learning rate combination (D, G) is (1e-4, 1e-4), (1e-4, 2e-4),...,(1e-3, 1e-3). We use the infinite () FID score to indicate that the model diverged or produced poor results (FID> 30).

Fig 3 shows the influence of learning rate (D, G) and  (in LrDep,, see Eq. 4) on the performance of MMD-rep and MMD-rep-b3 on the CIFAR-10 dataset. Note that when  = -1, MMD-rep is essentially the MMD-rbf with a single Gaussian kernel and MMD-rep-b is the same as MMD-rbf-b. We observed that: 1) both MMD-rep and MMD-rep-b performed good for   0, with  = 0.5, 1 slightly better than the rest; 2) the MMD-rbf model can be significantly improved by simply increasing  from -1 to -0.5, which reduces the attraction of discriminator on real sample scores; 3) large  may lead to unstable models when learning rate for generator G is large; 4) the RBF-B kernel managed to stabilize MMD-rep for these unstable cases but may occasionally cause the FID score to rise up. At last, it is interesting that when  > 1, the discriminator explicitly attracts {D(x)} and {D(y)} via maximizing EPX,PG [kD(x, y)] (see Eq. 4). In these cases, EPX,PG [kD(x, y)] may work as a penalty on the repulsion on real samples scores that prevents the pairwise distances of {D(x)} from increasing too fast.
5.3 QUALITATIVE ANALYSIS
The discriminator scores may be interpreted as a learned representation of the input samples. Fig 4 visualizes the discriminator scores learned by the MMD-rbf and proposed MMD-rep methods on CIFAR-10 dataset using t-SNE (van der Maaten (2014)). MMD-rbf ignored the class structure in data (see Fig 4a) while MMD-rep learned to concentrate the data from the same class and separate different classes to some extent (Fig 4b). This is because the discriminator D has to actively learn the data structure in order to repulse the real sample scores {D(x)}. Thus, we speculate that techniques reinforcing the learning of cluster structures in data may further improve the training of MMD-GAN.
On the other hand, the performance gain of proposed MMD-rep over the MMD-rbf and MMD-rq methods comes at no additional computational cost. In fact, by using a single kernel rather than a linear combination of kernels, MMD-rep and MMD-rep-b are simpler than MMD-rbf and MMD-rq. Besides, given a typically small batch size and a small number of discriminator output neurons (64 and 16 in our experiments), the cost of MMD over the non-saturating and hinge loss is marginal compared to the convolution operations. At last, we provide random generated samples for different methods in Appendix C.1.
3For  < 0, the RBF-B kernel in Eq. 6 was used.

7

Under review as a conference paper at ICLR 2019

(a) MMD-rbf

(b) MMD-rep

Figure 4: t-SNE visualization of real sample scores {D(x)} learned by (a) MMD-rbf and (b) MMDrep on the CIFAR-10 dataset. The samples were colored based on their class labels.

6 DISCUSSION
This study is an extension to the previous work on MMD-GAN (Li et al. (2017a)) with two contributions. First, we interpreted the optimization of MMD loss as a combination of attraction and repulsion processes, and proposed a repulsive loss for the discriminator that actively learns the difference among real data. Second, we proposed a bounded Gaussian RBF (RBF-B) kernel that can stabilize the MMD-GAN training in many cases. We expect tuning the hyper-parameters in RBF-B or using other regularization methods may further improve our results.
The theoretical advantages of MMD-GAN require that the discriminator is injective. The proposed repulsive loss (Eq. 4) attempts to satisfy this condition by explicitly maximizing the pair-wise distances among the real samples. Li et al. (2017a) achieved the injection property by using the discriminator as the encoder and an auxiliary network as the decoder to reconstruct the real and generated samples, which is more computationally extensive than our proposed approach.
The idea of repulsion on real sample scores is in line with existing studies. It has been widely accepted that the quality of generated samples can be significantly improved by integrating labels (Odena et al. (2017); Miyato & Koyama (2018); Zhou et al. (2018)) or even pseudo-labels generated by k-means method (Grinblat et al. (2017)) in the training of discriminator. The reason may be that the labels help concentrate the data from the same class and repulse those from different classes. Using a pre-trained classifier may also help produce vivid image samples (Huang et al. (2017)) as the learned representations of the real samples in the hidden layers of the classifier tend to be well separated/organized and may produce more meaningful gradients to the generator.
At last, we note that the proposed repulsive loss is orthogonal to the GAN studies on designing network structures and training procedures, and thus may be combined with a variety of novel techniques. For example, the ResNet architecture (He et al. (2016)) has been reported to outperform the plain DCGAN used in our experiments on image generation tasks (Miyato et al. (2018); Gulrajani et al. (2017)) and self-attention module may further improve the results (Zhang et al. (2018)). On the other hand, Karras et al. (2018) proposed to progressively grows the size of both discriminator and generator and achieved the state-of-the-art performance on unsupervised training of GANs on the CIFAR-10 dataset. Future work may explore these directions.
REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein generative adversarial networks. In ICML, volume 70 of PMLR, pp. 214­223, 2017.
Mikolaj Bin´kowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In ICLR, 2018.
8

Under review as a conference paper at ICLR 2019
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In AISTATS, volume 15 of PMLR, pp. 215­223, 2011.
Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning, 2016. arxiv:1603.07285.
Marc G. Genton. Classes of kernels for machine learning: A statistics perspective. J. Mach. Learn. Res., 2:299­312, 2002.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672­2680, 2014.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scho¨lkopf, and Alexander Smola. A kernel two-sample test. J. Mach. Learn. Res., 13:723­773, 2012.
Guillermo L. Grinblat, Lucas C. Uzal, and Pablo M. Granitto. Class-splitting generative adversarial networks, 2017. arXiv:1709.07359.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In NIPS, pp. 5767­5777, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pp. 770­778, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In NIPS, pp. 6626­6637, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In NIPS, pp. 4565­4573, 2016.
Xun Huang, Yixuan Li, Omid Poursaeed, John E. Hopcroft, and Serge J. Belongie. Stacked generative adversarial networks. CVPR, pp. 1866­1875, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pp. 448­456, 2015.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master's thesis, Department of Computer Science, University of Toronto, 2009.
Wei-Sheng Lai, Jia-Bin Huang, and Ming-Hsuan Yang. Semi-supervised learning for optical flow with generative adversarial networks. In NIPS, pp. 354­364, 2017.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. MMD GAN: Towards deeper understanding of moment matching network. In NIPS, pp. 2203­2213, 2017a.
Jiwei Li, Will Monroe, Tianlin Shi, Se´bastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural dialogue generation. In EMNLP, pp. 2157­2169, 2017b.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In ICML, volume 37, pp. 1718­1727, 2015.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of generative adversarial learning. In NIPS, pp. 5545­5553, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, pp. 3730­3738, 2015.
Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In ICLR, 2018.
9

Under review as a conference paper at ICLR 2019
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018.
Vaishnavh Nagarajan and J. Zico Kolter. Gradient descent GAN optimization is locally stable. In NIPS, pp. 5585­5595, 2017.
XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. On surrogate loss functions and f-divergences. Ann. Stat., 37(2):876­904, 04 2009.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier GANs. In ICML, pp. 2642­2651, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. In ICLR, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NIPS, pp. 2234­2242, 2016.
C. Sønderby, J. Caballero, L. Theis, W. Shi, and F. Husza´r. Amortised map inference for image super-resolution. In ICLR, 2017.
C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In CVPR, pp. 2818­2826, 2016.
Dustin Tran, Rajesh Ranganath, and David M. Blei. Hierarchical implicit models and likelihood-free variational inference, 2017. arXiv:1702.08896.
Thomas Unterthiner, Bernhard Nessler, Calvin Seward, Gu¨nter Klambauer, Martin Heusel, Hubert Ramsauer, and Sepp Hochreiter. Coulomb GANs: Provably optimal Nash equilibria via potential fields. In ICLR, 2018.
Laurens van der Maaten. Accelerating t-SNE using tree-based algorithms. Journal of Machine Learning Research, 15:3221­3245, 2014.
Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale structural similarity for image quality assessment. In Asilomar Conference on Signals, Systems Computers, volume 2, pp. 1398­1402, 2003.
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. 2015. arXiv:1506.03365.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks, 2018. arXiv:1805.08318.
Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, and Yong Yu. Activation maximization generative adversarial nets. In ICLR, 2018.
J. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, pp. 2242­2251, 2017.
Appendices
A STABILITY ANALYSIS OF MMD-GAN
This section demonstrates that, under mild assumptions, MMD-GAN trained by a gradient method is locally exponentially stable at equilibrium. It is organized as follows. The main assumption and proposition are presented in Section A.1, followed by simulation study in Section A.2 and proof in Section A.3. At last, we discuss the indications of assumptions on the discriminator of GAN in Section A.4.
10

Under review as a conference paper at ICLR 2019

A.1 MAIN PROPOSITION

Let S(P ) be the support of distribution P ; let G  G, D  D be the parameters of the generator G and discriminator D respectively. To prove that GANs trained using the minimax loss and gradient updates is locally stable at the equilibrium point (D , G ), Nagarajan & Kolter (2017) made the following assumption:
Assumption 1 (Nagarajan & Kolter (2017)). PG = PX and x  S(PX), DD (x) = 0.

For loss functions like minimax and Wasserstein, DD (x) may be interpreted as how plausible a sample is real. Thus at equilibrium, it may be reasonable to assume all real and generated samples
are equally plausible. However, DD (x) = 0 also indicates that DD may have no discrimination power (see Appendix A.4 for discussion). For MMD loss in Eq. 2, DD (x)|xP may be interpreted as a learned representation of the distribution P . As long as two distributions P and Q match, Mk2DD (P, Q) = 0. On the other hand, DD (x) = 0 is a minima solution for D but D is trained to find local maxima. Thus in contrast to Assumption 1, we assume
Assumption 2. For GANs using MMD loss in Eq. 2 and Eq. 3, and random initialization on parameters, at equilibrium, DD (x) is injective on S(PX) S(PG ).

Assumption 2 indicates that DD (x) is not constant almost everywhere. We use a simulation in Section A.2 to show that DD (x) = 0 does not hold in general for MMD loss. Based on Assumption 2, we propose the following proposition and prove it in Appendix A.3:

PadnersdocpeEonqst.imt3ioehnthao1s d.esqIfiusitlhlioebcrreaialelyx(iesGxtsp, onGDe)ntfioarllGaynssytuacbhDlethaatt(PDGG.,

= PX, then GANs with MMD loss
Moreover, the model trained using D) for any D  D.

in Eq. 2 gradient

There may exist non-realizable cases where PX cannot be represented by any generator GG with G  G. On the other hand, it is interesting to investigate the stability of the proposed repulsive loss in Eq. 4. In Section A.2, we use a simulation study to show that both the original MMD loss
and the proposed repulsive loss may be locally stable and leave the proof for future work.

A.2 SIMULATION STUDY
In this section, we reused the example from Nagarajan & Kolter (2017) to show that GAN using the MMD loss is locally stable. Consider a two-parameter MMD-GAN with generator G(z) = w1z, discriminator D(x) = w2x2 and Gaussian kernel k0rb.f5. The MMD-rbf model (LGmmd and LDatt from Eq. 2) and the MMD-rep model (LmGmd and LDrep,1 from Eq. 5) were tested. Each model was applied to two cases: (a) both the latent and true distributions are uniform over [-1, 1] thus PX is realizable; (b) the latent distribution is uniform over [-1, 1] and the true distribution is standard Gaussian, thus PX is non-realizable for any w1  R. Fig. S1 shows that MMD-GAN are locally stable in all cases and DD (x) = 0 does not hold in general for MMD loss. However, MMD-rep may not be globally stable for the tested cases: initialization of (w1, w2) in some regions may lead to the trivial solution w2 = 0 (see Fig S1b and S1d). We note that by decreasing the learning rate for G, the area of such regions decreased. At last, it is interesting to note that both MMD-rbf and MMD-rep had the same nontrivial solution w1  1.55 for generator in the non-realizable cases (see Fig S1c and S1d).

A.3 PROOF OF PROPOSITION 1

This section provides the proof for Proposition 1 into two parts. First, we show that GAN with the MMD loss in Eq. 2 and Eq. 3 has equilibria for any parameter configuration of discriminator D; second, we prove the model is locally exponentially stable.

Consider real data Xr  PX, latent variable xr, z, xg be their instantiations. Denote ab

Z =

 PZ. and

a b

;

D

=

-geneLrDDat,ed. Gva=ria-ble LXGGg;

= GG (Z). Let dg = D(G(z)),

dr = D(xr) where LD and LG are the losses for D and G respectively. Assume an isotropic

stationary kernel k(a, b) = kI ( a - b ) (Genton (2002)) is used in MMD. We first show:

Proposition in Eq. 2 and

1 (Part 1). If there exists Eq. 3 has equilibria (G ,

G D

 G such that PG ) for any D  D.

=

PX,

the

GAN

with

the

MMD

loss

11

Under review as a conference paper at ICLR 2019

(a) MMD-rbf, PX = U (-1, 1)

(b) MMD-rep, PX = U (-1, 1)

(c) MMD-rbf, PX = N (0, 1)

(d) MMD-rep, PX = N (0, 1)

Figure S1: Streamline plots of MMD-GAN using the MMD-rbf and the MMD-rep model on distributions: PZ = U (-1, 1), PX = U (-1, 1) or PX = N (0, 1). In (a) and (b), the equilibria satisfying PG = PX lie on the line w1 = 1. In (c), the equilibrium lies around point (1.55, 0.74); in (d), it is around (1.55, 0.32).

Proof.

Denote ei,j

= ai - bj

and

k
ei,j

=

k(ai,bj ) e

where k is the kernel of MMD. The gradients

.of MMD loss is

D

=

E [  ]k er1,r2
PX er1,r2 D

+

EPG

[k
eg1,g2

eg1,g2
D

]

-

2EPX ,PG

[ekr,g

er,g
D

]

(S1a)

.G

=

-EPG

[k
eg1,g2

(dxgg11

xg1
G

-

dg2
xg2

xg2
G

)]

+

2EPG ,PX [ekg,r xdgg xGg ]

(S1b)

We note that, given i.i.d. drawn samples X = {xi}in=1  PX and Y = {yi}ni=1  PG, an unbiased estimator of the squared MMD is (Gretton et al. (2012))

M^ k2(PX, PG)

=

1 n(n -

1)

n

1 k(xi, xj)+ n(n - 1)

n

2 k(yi, yj)- n(n - 1)

n

k(xi, yj) (S2)

i=j i=j i=j

At equilibrium, consider a sequence of N samples dri = dgi = di with N  , we have

.
D 

  +k ei,j
ei,j D

 k ei,j
ei,j D

-

2

 k ei,j
ei,j D

=

0

i=j i=j

i=j

. G  -

k
ei,j

(xdii

xGi

-

dxjj xGj )

+

2

k
ei,j

xdii

xGi

i=j i=j

=

k
ei,j

(dxii

xGi

+

dxjj xGj )

=

0

i=j
.
where for G we have used the fact that for each term in the summation, there exists an term with

i, j

reve.rsed

and

k
ei,j

=

-ekj,i

thus

the

summation

is

zero.

Since

we

have

not

assumed

the

status

of D, D = 0 for any D  D.

12

Under review as a conference paper at ICLR 2019

We proceed to prove the model stability. First, following Theorem 5 in Gretton et al. (2012) and Theorem 4 in Li et al. (2017a), it is straightforward to see:
Lemma A.1. Under Assumption 2, Mk2DD (PX, PG )  0 with the equality if and only if PX = PG .

Lemma A.1 and Proposition 1 (Part 1) state that at equilibrium PG = PX, every discriminator DD and kernel k will give Mk2DD (PG , PX) = 0, thus no discriminator can distinguish the two distributions. On the other hand, we cite Theorem A.4 from Nagarajan & Kolter (2017):

Lemma A.2 (Nagarajan & Kolter (2017)). Consider a non-linear system of parameters (, ):

 = h1(, ),  = h2(, ) with an equilibrium point at (0, 0). Let there exist such that  

B

(0),

(0, ) is an

equilibrium.

If J

=

 h1 (, ) 

(0,0)

is

a Hurwitz

matrix,

the non-linear system is

exponentially stable.

Now we can prove:

Proposition 1 (Part 2). At descent methods is locally

equilibrium PG = exponentially stable

PX, the GAN trained using MMD at (G , D) for any D  D.

loss

and

gradient

Proof. Inspired by Nagarajan & Kolter (2017), we first derive the Jacobian of the system

J=

JDD JGD

JDG JGG

.. .DT /D .DT /G
GT /D GT /G

Denote

ab

=

2a b2

and

abc

=

2a bc

.

Based

on

Eq.

S1,

we

have

JDD

=

EPX [(erD1,r2 )T



(k
er1,r2

)T

+ ( )   ]er1,r2 T
D

k er1,r2

er1,r2 D

+ EPG [(egD1,g2 )T



(k
eg1,g2

)T

+ ( )   ]eg1,g2 T
D

k eg1,g2

eg1,g2 D

- 2EPX,PG [(erD,g )T

 (ekr,g )T

+

(erD,g

)T

k
er,g

er,g
D

]

(S3a)

JDG

=

EPG

[(eg1,g2
D G

)T



(k
eg1,g2

)T

+ ( )   ]eg1,g2 T
D

k eg1,g2

eg1,g2 G

- 2EPX,PG [(erD,gG )T

 (ekr,g )T

+

(erD,g

)T

k
er,g

dGg

]

(S3b)

JGG

= - EPG [(egG1,g2 )T



(k
eg1,g2

)T

+ ( )   ]eg1,g2 T
G

k eg1,g2

eg1,g2 G

+ 2EPX,PG [(dGg )T

 (ker,g )T

+

(dGg

)T

k
er,g

dGg

]

(S3c)

where  is the kronecker product and JGD = -JDT G. At equilibrium, consider a sequence of N samples dri = dgi = di with N  , we have JDD = 0, JDG = 0 and

JGG

=

EPG

[(dGg1

)T

k
eg1,g2

dg1
G

+

(dGg2

)T

k
eg1,g2

dg2
G

- ( )   ]eg1,g2 T
G

k eg1,g2

eg1,g2 G

=

EPG

[(dGg1

)T

k
eg1,g2

dg2
G

+

(dGg2

)T

k
eg1,g2

dg1
G

]

At first sight, its seems difficult to show the definite property of JGG. Given Lemma A.1 and fact that JGG is the Hessian matrix of Mk2DD (PX, PG ), JGG is negative semidefinite. The eigenvectors of JGG corresponding to zero eigenvalues form null(JGG). There may exist small distortion G  null(JGG) such that PG +G = PG , that is, PG is locally constant along some parameters in G. Following Lemma C.3 of Nagarajan & Kolter (2017), we consider eigenvalue decomposition JGG = UGGUGT and UG = [TGT , TGT ] such that Col(TGT ) = null(JGG). Thus, the projections G = TGG are orthogonal to null(JGG). Then, the Jacobian corresponding to the projected system has the form J = [0, 0; 0, JGG] with negative definite block JGG = TGJGGTGT and zeros elsewhere. Moreover, on all directions exclude those described by JGG, the system is surrounded by a neighborhood of equilibia at least locally. According to Lemma A.2, the system is
exponentially stable.

13

Under review as a conference paper at ICLR 2019

A.4 DISCUSSION ON ASSUMPTION 1
This section shows that constant discriminator output DD (x) = c for x  S(PX) S(PG ) indicates that DD may have no discrimination power. First, we make the following assumptions: Assumption 3. 1. D is a multilayer perceptron where each layer l can be factorized into an affine transform and an element-wise activation function fl. 2. Each activation function fl  C0; furthermore, fl has a finite number of discontinuities and fl  C04. 3. Input data to D is continuous and its support S is compact in Rd with non-zero measure in each dimension and d > 15.
Based on Assumption 3, we have the following proposition: Proposition 2. If x  S, D(x) = c, where c is constant, then there always exists distortion x such that x + x  S and D(x + x) = c.

Proof. Without loss of generality, we consider D(x) = W2h(x) + b2 and h(x) = f (W1x + b1), where W1  Rdh×d, W2, b1, b2 are model weights and biases, f is some activation function. For x  S, since D(x) = c, we have h(x)  null(W2). Furthermore:

(a) If rank(W1) < d, for any x  null(W1), h(x + x)  null(W2).

(b) If rank(W1) = d = dh, the problem h(x + x) = k · h(x) has unique solution for any k  R as long as k · h(x) is within the output range of f .

(c) If rank(W1) = d < dh, let U and V be two basis matrices of Rdh such that W1x =

U x^T 0T T and any vector in null(W2) can be represented as V zT 0T T , where

x^  Rdh×d, z  Rdh×n and n is the nullity of W2. Let the projected support be S^. Thus,

x^  S^, there exists z such that f (U x^T 0T T + b1) = V zT zcT T with zc = 0.

Consider the Jacobian:

 zT J=
 x^T

zcT 0T

T T

= V -1U

(S4)

where



=

diag(

df d ai

)

and

a

=

[ai ]id=h 1

is

the

input

to

activation,

or

pre-activations.

Since

S^

is continuous and compact, it has infinite number of boundary points {x^b} for d > 1. Consider

one boundary point x^b and its normal line x^b. Let > 0 be a small scalar such that x^b- x^b 

S^ and x^b + x^b  S^.

· For linear activation,  = I and J is constant. Then zc remains 0 for x^b + x^b, i.e., there exists z such that h(x^ + x^)  null(W2).
· For nonlinear activations, assume f has N discontinuities. Since U x^T 0T T + b1 = c
has unique solution for any vector c, the boundary points {x^b} cannot yield pre-activations {ab} that all lie on the discontinuities in any of the dh directions. Though we might need to sample dNh +1 points in the worst case to find an exception, there are infinite number of exceptions. Let x^b be a sample where {ab} does not lie on the discontinuities in any direction. Because f is continuous, zc remains 0 for x^b + x^b, i.e., there exists z such that h(x^ + x^)  null(W2).

In conclusion, we can always find x such that x + x / S and D(x + x) = c.

Proposition 2 indicates that if DD (x) = 0 for x  S(PX) S(PG ), DD cannot discriminate against fake samples with distortions to the original data. In contrast, Assumption 2 and Lemma A.1 guarantee that, at equilibrium, the discriminator trained using MMD loss function is effective against such fake samples given a large number of i.i.d. test samples (Gretton et al. (2012)).
4This include many commonly used activations like linear, sigmoid, tanh, ReLU and ELU. 5For distributions with semi-infinite or infinite support, we consider the effective or truncated support S (P ) = {x  X |P (x)  }, where > 0 is a small scalar. This is practical, e.g., univariate Gaussian has support in (-, +) yet a sample five standard deviations away from the mean is unlikely to be valid.

14

Under review as a conference paper at ICLR 2019

B SUPPLEMENTARY ON METHODOLOGY

B.1 POWER ITERATION FOR CONVOLUTION OPERATION

This section proposes the power iteration for convolution operation (PICO) method to estimate the spectral norm of a convolution kernel.
For a weight matrix W , the spectral norm is defined as (W ) = max v 21 W v 2. Power iteration on matrix (PIM) is often used to estimate (W ), which iterates between the following two steps:

1. Update u = W v/ W v 2; 2. Update v = W T u/ W T u 2.

The convolutional kernel Wc is a tensor of shape h × w × cin × cout with h, w the receptive field
size and cin, cout the number of input/output channels. To estimate (Wc), Miyato et al. (2018) reshaped it into a matrix Wrs of shape (hwcin) × cout and estimated (Wrs).

We propose a simple method to calculate Wc directly based on the fact that convolution operation is linear. For any linear map T : Rm  Rn, there exists matrix WL  Rn×m such that y = T (x)

can

be

represented

as

y

=

WLx.

Thus,

we

may

simply

substitute

WL

=

y x

in

the

PIM

method

to

estimate the spectral norm of any linear operation. In the case of convolution operation , there exist

doubly block circulant matrix Wdbc such that u = Wc  v = Wdbcv. Consider v = WdTbcu =

[

u v

]T

u

which

is

essentially

the

transpose

convolution

of

Wc

on

u

(Dumoulin

&

Visin

(2016)).

Thus, similar to PIM, PICO iterates between the following two steps:

1. Update u = Wc  v/ Wc  v 2; 2. Do transpose convolution of Wc on u to get v^; update v = v^/ v^ 2.
Empirically we found that for convolution kernel Wc, PICO(Wc) > PIM(Wc). That is, spectral normalization using PICO imposes a stronger constraint than PIM. However, we did not find significant and consistent difference in performance between PICO and PIM on tested datasets. Nevertheless, we proposed a general framework to estimate the spectral norm for any linear operation (and possibly combination of linear operations), which may be helpful in more general applications.

B.2 NETWORK ARCHITECTURE
For unsupervised image generation tasks on CIFAR-10 and STL-10 datasets, the DCGAN (Radford et al. (2016)) architecture from Miyato et al. (2018) was used. For CelebA and LSUN bedroom datasets, we added more layers to the generator and discriminator accordingly. See Table S1 and S2 for details.

C SUPPLEMENTARY ON EXPERIMENTS
C.1 UNSUPERVISED IMAGE GENERATION Generated samples on CelebA dataset are given in Fig. S2 and LSUN bedrooms in Fig. S3.

15

Under review as a conference paper at ICLR 2019

Table S1: DCGAN models for image generation on CIFAR-10 (h = w = 4, H = W = 32) and STL-10 (h = w = 6, H = W = 48) datasets. For non-saturating loss and hinge loss, s = 1; for MMD-rand, MMD-rbf, MMD-rq, s = 16.

(a) Generator
z  R128  N (0, I) 128  h × w × 512, dense, linear 4 × 4, stride 2 deconv, 256, BN, ReLU 4 × 4, stride 2 deconv, 128, BN, ReLU 4 × 4, stride 2 deconv, 64, BN, ReLU
3 × 3, stride 1 conv, 3, Tanh

(b) Discriminator
RGB image x  [-1, 1]H×W ×3
3 × 3, stride 1 conv, 64, LReLU
4 × 4, stride 2 conv, 128, LReLU 3 × 3, stride 1 conv, 128, LReLU
4 × 4, stride 2 conv, 256, LReLU 3 × 3, stride 1 conv, 256, LReLU
4 × 4, stride 2 conv, 512, LReLU 3 × 3, stride 1 conv, 512, LReLU
h × w × 512  s, dense, linear

Table S2: DCGAN models for image generation on CelebA and LSUN-bedroom datasets. For non-saturating loss and hinge loss, s = 1; for MMD-rand, MMD-rbf, MMD-rq, s = 16.

(b) Discriminator

(a) Generator
z  R128  N (0, I) 128  4 × 4 × 1024, dense, linear 4 × 4, stride 2 deconv, 512, BN, ReLU 4 × 4, stride 2 deconv, 256, BN, ReLU 4 × 4, stride 2 deconv, 128, BN, ReLU 4 × 4, stride 2 deconv, 64, BN, ReLU
3 × 3, stride 1 conv, 3, Tanh

RGB image x  [-1, 1]64×64×3
3 × 3, stride 1 conv, 64, LReLU
4 × 4, stride 2 conv, 128, LReLU 3 × 3, stride 1 conv, 128, LReLU
4 × 4, stride 2 conv, 256, LReLU 3 × 3, stride 1 conv, 256, LReLU
4 × 4, stride 2 conv, 512, LReLU 3 × 3, stride 1 conv, 512, LReLU
4 × 4, stride 2 conv, 1024, LReLU 3 × 3, stride 1 conv, 1024, LReLU
4 × 4 × 512  s, dense, linear

16

Under review as a conference paper at ICLR 2019

(a) Real samples

(b) Hinge

(c) MMD-rbf

(d) MMD-rbf-b

(e) MMD-rep

(f) MMD-rep-b

Figure S2: Image generation using different loss functions on 64 × 64 CelebA dataset.

17

Under review as a conference paper at ICLR 2019

(a) Real samples

(b) Non-saturating

(c) Hinge

(d) MMD-rbf-b

(e) MMD-rep

(f) MMD-rep-b

Figure S3: Image generation using different loss functions on 64 × 64 LSUN bedroom dataset.

18

