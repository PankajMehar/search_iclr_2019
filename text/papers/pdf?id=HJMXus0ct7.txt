Under review as a conference paper at ICLR 2019
IRDA METHOD FOR SPARSE CONVOLUTIONAL NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a new approach, known as the iterative regularized dual averaging (iRDA), to improve the efficiency of convolutional neural networks (CNN) by significantly reducing the redundancy of the model without reducing its accuracy. The method has been tested for various data sets, and proven to be significantly more efficient than most existing compressing techniques in the deep learning literature. For many popular data sets such as MNIST and CIFAR-10, more than 95% of the weights can be zeroed out without losing accuracy. In particular, we are able to make ResNet18 with 95% sparsity to have an accuracy that is comparable to that of a much larger model ResNet50 with the best 60% sparsity as reported in the literature.
1 INTRODUCTION
In recent decades, deep neural network models have achieved unprecedented success and state-ofthe-art performance in various tasks of machine learning or artificial intelligence, such as computer vision, natural language processing and reinforcement learning Lecun et al. (2015). Deep learning models usually involve a huge number of parameters to fit variant kinds of datasets, and the number of data may be much less than the amount of parameters He et al. (2016). This may implicate that deep learning models have too much redundancy. This can be validated by the literatures from the general pruning methods Pratt (1988) to the compressing models Han et al. (2015a).
While compressed sensing techniques have been successfully applied in many other problems, few reports could be found in the literature for their application in deep learning. The idea of sparsifying machine learning models has attracted much attention in the last ten years in machine learning Donoho (2006); Xiao (2010). When considering the memory and computing cost for some certain applications such as Apps in mobile, the sparsity of parameters plays a very important role in model compression Han et al. (2015a); Cheng et al. (2017). The topic of computing sparse neural networks can be included in the bigger topic on the compression of neural networks, which usually further involves the speedup of computing the compressed models.
There are many sparse methods in machine learning models such as FOBOS method Duchi and Singer (2009), also known as proximal stochastic gradient descent (prox-SGD) methods Mine and Fukushima (1981), proposed for general regularized convex optimization problem, where 1 is a common regularization term. One drawback of prox-SGD is that the thresholding parameters will decay in the training process, which results in unsatisfactory sparsity Xiao (2010). Apart from that, the regularized dual averaging (RDA) method Xiao (2010), proposed to obtain better sparsity, has been proven to be convergent with specific parameters in convex optimization problem, but has not been applied in deep learning fields.
In this paper, we analyze the relation between simple dual averaging (SDA) method Nesterov (2009) and the stochastic gradient descent (SGD) method Robbins and Monro (1951), as well as the relation between SDA and RDA. It is well-known that SGD and its variants work quite well in deep learning problems. However, there are few literatures in applying pure training algorithms to deep CNNs for model sparsification. We propose an iterative RDA (iRDA) method for training sparse CNN models, and prove the convergence under convex conditions. Numerically, we compare prox-SGD with iRDA, where the latter can achieve better sparsity results while keeping satisfactory accuracy on MNIST, CIFAR-10 and CIFAR-100. We also show iRDA works for different CNN models such
1

Under review as a conference paper at ICLR 2019

as VGG Simonyan and Zisserman (2014) and He et al. (2016). Finally, we compare the performance of iRDA with some other state-of-the-art compression methods.

2 RELATED WORKS
Cheng et al. (2017) reviews the work on compressing neural network models, and categorizes the related methods into four schemes: parameter pruning and sharing, low-rank factorization, transfered/compact convolutional filters and knowledge distillation. Among them, Liu et al. (2015) uses sparse decomposition on the convolutional filters to get sparse neural networks, which could be classified to the second scheme. Apart from that, Han et al. (2015b) prunes redundant connections by learning only the important parts. Louizos et al. (2017) starts from a Bayesian point of view, and removes large parts of the network through sparsity inducing priors. Yin et al. (2018) He et al. (2018) combines reinforcement learning methods to compression. Li and Hao (2018) considers deep learning as a discrete-time optimal control problem, and obtains sparse weights on ternary networks. Recently, Feng (2018) applies RDA to fully-connected neural network models on MNIST.

3 ALGORITHMS

Let z = (x, y) be an input-output pair of data, such as a picture and its corresponding label in a classification problem, and f (w, z) be the loss function of neural networks, i.e. a scalar function that is differentiable w.r.t. weights w. We are interested in the expected risk minimization problem

The empirical risk minimization

min
w

{Ezf (w, z)} .

(1)

1T

min wT

f (w, zt)

t=1

(2)

is an approximation of (1) based on some finite given samples {z1, z2, . . . , zT } , where T is the size of the sample set.

Regularization is a useful technique in deep learning. In general, the regularized expected risk minimization has the form

min
w

{(w) = Ezf (w, z) + (w)} ,

(3)

where (w) is a regularization term with certain effect. For example, (w) =

w

2 2

may

im-

prove the generalization ability, and an 1-norm of w can give sparse solutions. The corresponding

regularized empirical risk minimization we concern takes the form

1T

min (w) = wT

f (w, zt) + (w) .

t=1

(4)

SDA method is a special case of primal-dual subgradient method first proposed in Nesterov (2009). Xiao (2010) proposes RDA for online convex and stochastic optimization. RDA not only keeps the same convergence rate as Prox-SGD, but also achieves more sparsity in practice.
In next sections, we will discuss the connections between SDA and SGD, as well as RDA and Prox-SGD. We then propose iRDA for 1 regularized problem of deep neural networks.

3.1 SIMPLE DUAL AVERAGING METHOD

As a solution of (2), SDA takes the form

wt+1 = arg min
w

1t t

g (w ), w

+ t h(w) t

 =1

.

(5)

The first term

t  =1

g , w

is a linear function obtained by averaging all previous stochastic gradi-

ent. gt is chosen randomly from the set of subfunctions. The second term h(w) is a strongly convex

2

Under review as a conference paper at ICLR 2019

function, and {t} is a nonnegative and nondecreasing sequence which determines the convergence rate. As g (w ),  = 1, . . . , t - 1 is constant in current iteration, we use g instead for simplicity in the following. Since subproblem equation 5 is strongly convex, it has a unique optimal solution
wt+1.

Let

w0

be

the

initial point,

and

h(w)

=

1 2

w - w0

22, the iteration scheme of SDA can be written as

wt+1

=

w0

-

1 t

t
g
 =1

=

w0 -

t t g¯t,

(6)

where

g¯t

=

1 t

t  =1

g , w

. Let t = t, SDA can be rewritten recursively as

wt+1

=

w0

-

1 t

t
g

 =1

=

w0

-

1 t

(t - 1) t-1 (t - 1) g + gt
 =1

=

1

-

(t

- 1) t

(t - 1) w0 + t

w0

-

(t

1 -

1)

t

g

 =1

=

1-

1- 1 t



w0 +

1- 1 t



wt

-

1 t

gt,

-

1 t

gt

(7)

where

1-

1

-

1 t



 0 and

1

-

1 t



 1 as t



.

Thus, SDA can be viewed as a

perturbation of SGD.

3.2 PROXIMAL STOCHASTIC GRADIENT DESCENT AND REGULARIZED DUAL AVERAGING METHODS

For the regularized problem (4), we recall the well-known Prox-SGD and RDA method first. At each iteration, Prox-SGD solves the subproblem

wt+1 = arg min
w

gt, w

1 +
2t

w - wt

2 2

+

(w)

.

(8)

Specifically, t

=

1 t

obtains the best convergence rate.

The first two terms are an approxima-

tion of the original objective function. Note that without the regularization term , equation 8 is

equivalent to SGD. It can be written in forward-backward splitting (FOBOS) scheme

wt+

1 2

= wt - tgt,

wt+1 = arg min
w

1 2

w

-

wt+

1 2

2 2

+

t(w)

,

(9) (10)

where the forward step is equivalent to SGD, and the backward step is a soft-thresholding operator

working

on

wt+

1 2

with

the

soft-thresholding

parameter

t.

Different from Prox-SGD, each iteration of RDA takes the form

wt+1 = arg min
w

1t t

g , w

+ (w) + t h(w) t

 =1

.

Similarly,

taking

h(w)

=

1 2

w - w0

2 2

,

RDA

can

be

written

as

wt+1 = arg min
w
= arg min
w

g¯t, w

+ t 2t

w - w0

2 2

+

(w)

1 2

w

-

(w0

+

t t

g¯t)

2 2

+

t t

(w)

,

(11)
(12) (13)

3

Under review as a conference paper at ICLR 2019

or equivalently,

t

wt+

1 2

= w0 -

t g¯t,

(14)

wt+1 = arg min
w

1 2

w

-

wt+

1 2

2 2

+

t t

(w)

,

(15)



where t =  t to obtain the best convergence rate. From equation 14, one can see that the forward

step is actually SDA and the backward step is the soft-thresholding operator, with the parameter t/t.

3.3 1 REGULARIZATION AND THE SPARSITY

Set (w) =  w 1. The problem (4) then becomes

T

min
w

ft(w) +  w 1,

t=1

where  is a hyper-parameter that determines sparsity.

(16)

In this case, from Xiao's analysis of RDA Xiao (2010), the expected cost E(w¯t) -  associated

with

the

random

variable

w¯t

converges

with

rate

O( 1 )
t

when

t

=



t. This convergence rate is

consistent with FOBOS Duchi and Singer (2009). However, both results assume f to be a convex

function, which can not be guaranteed in deep learning. Nevertheless, we can still verify that RDA

is a powerful sparse optimization method for deep neural networks.

We conclude the closed form solutions of Prox-SGD and RDA for equation 16 as follows.

1. The subproblem of Prox-SGD

wt+1 = arg min
w

gtT w

+

1 2t

w - wt

2 2

+



w

1

has the closed form solution

wt(+i)1

=

wt(i) 0,

-

t(gt(i)

+

),

wt(i) - t(gt(i) - ),

wt(i) - tgt(i) > t, |wt(i) - tgt(i)|  t, wt(i) - tgt(i) < -t.

(17) (18)

2. The subproblem of RDA

wt+1 = arg min
w

g¯tT w

+

t 2t

w - w0

2 2

+



w

1

(19)

has the closed form solution

wt(+i)1

=

w0(i) 
0,

-

t t

(g¯t(i)

+

),

w0(i)

-

t t

g¯t(i)

>

t t

,

|w0(i)

-

t t

g¯t(i)

|



t t

,

w0(i)

-

t t

(g¯t(i)

-

),

w0(i)

-

t t

g¯t(i)

<

-

t t

.



3. The t-proximal stochastic gradient method has the form

(20)

wt+

1 2

= wt - tgt,

wt+1 = arg min
w

1 2

w

-

wt+

1 2

2 2

+

t t

(w)

.

(21)

 Thedifference between t-Prox-SGD and Prox-SGD is the soft-thresholding parameter chosen to be t. It has the closed form solution

wt(+i)1

=

wt(i) 
0,

-

tgt(i)

-

t t

,

wt(i)

-

tgt(i)

+

t t

,

wt(i)

- tgt(i)

>

t t

,

|wt(i)

- tgt(i)|



t t

,

wt(i)

-

tgt(i)

<

-

t t

.

(22)

4

Under review as a conference paper at ICLR 2019

It is equivalent to

wt+1 = arg min
w

gtT w

+

1 2t

w - wt

2 2

+

t tt

w

1

,

and is actually an approximation of

(23)

T t i=1 fi(w) + tt w 1.



We

can

easily

conclude

that

this

iteration

will

converge

to

w

=

0

if

t

=

1 t

and

t

=



t.

(24)

Now compare the threshold P G

=

t of PG and the threshold RDA

=

t t



of

RDA.

With

t

=

1 t

and

t

=



t, we have P G  0 and RDA   as t  0. It is clear that RDA uses a

much more aggressive threshold, which guarantees to generate significantly more sparse solutions.

3.4 ITERATIVE RDA METHOD FOR DEEP NEURAL NETWORKS

Note that when  =  w 1, RDA requires w1 = w0 = 0. However, this will make deep neural

network a constant function, with which the parameters can be very hard to update. Thus, in Algo-

rithm 1, we modify the RDA method as Step 1, where w1 can be chosen not equal to 0, and add an

extra Step 2 to improve the performance. We also prove the convergence rate of Step 1 for convex

problem

is

O(

1 t

)

when

t

=

O(

t).

Theorem 3.1 Assume there exists an optimal solution w to the problem (3) with (w) =  w 1

that satisfies h(w )  D2 for some D > 0, and let  = (w ). Let the sequences {wt}t1 be

generated by Step 1 in iRDA, and assume gt   G for some constant G. Then the expected cost

E(w¯t) converges to 

with rate O( 1 )
t

E(w¯t) - 

= O( 1 ). t

See Appendix A for the proof.

3.5 INITIALIZATION

To apply iRDA, the weights of a neural network should be initialized differently from that in a normal optimization method such as SGD or its variants. Our initialization is based on LeCun et al. (2012), Glorot and Bengio (2010) and He et al. (2015), with an additional re-scaling. Let s be a scalar, the mean and the standard deviation of the uniform distribution for iRDA is zero and

iRDA =

s2 ,

n = k2c

n

(25)

respectively.

Choosing a suitable s is important when applying iRDA. As shown in Table 5 and Table 6 in Appendix B, if s is too small or too large, the training process could be slowed down and the generalization ability may be affected. Moreover, a small s usually requires much better initial weights, which results in too many samplings in initialization process. In our experiments, a good s for iRDA is usually much larger than 2, and unsuitable for SGD algorithms.

3.6 ITERATIVE RETRAINING
Iterative retraining is a method that only updates the non-zero parameters at each iteration. A trained model can be further updated with retraining, thus both the accuracies and sparsity can be improved. See Table 4 for comparisons on CIFAR-10.

5

Under review as a conference paper at ICLR 2019

Algorithm 1 The iterative RDA method for 1 regularized DNNs

Input:

· A strongly convex function h(w) =

w

2 2

.



· A nonnegative and nondescreasing sequence t =  t.

Step 1: RDA with proper initialization

Initialize: set w0 = 0, g¯0 = 0 and randomly choose w1 with methods explained in section 3.5. for t=1,2, ..., T do

Given the sample zit and corresponding loss function fit . Compute the stochastic gradient

gt = fit (wt).

(26)

Update the average gradient:

t-1

1

g¯t = t g¯t-1 + t gt.

Compute the next weight vector:

wt+1 = arg min
w

g¯t, w

+

w

1

+

t 2t

w

2 2

.

Step 2: iterative retraining
for t=T+1,T+2,T+3, ... do Given the sample zit and corresponding loss function fit . Compute the stochastic gradient gt = fit (wt).

(27) (28) (29)

Set (gt)j = 0 if (wt)j = 0 for every j.

Update the average gradient:

t-1

1

g¯t = t g¯t-1 + t gt.

Compute the next weight vector:

wt+1 = arg min
w

g¯t, w

+

w

1

+

t 2t

w

2 2

.

(30) (31)

4 EXPERIMENTS

In this section,  denotes the sparsity of a model, i.e. quantity of zero parameters
= . quantity of all parameters

(32)

4.1 PARAMETERS TEST
We provide a test on different hyper-parameters, so as to give an overview of their effects on performance, as shown in Table 7. We also show that the sparsity and the accuracy can be balanced with iRDA by adjusting the parameters  and , as shown in Table 8. Both tables are put in Appendix C.

4.2 MAIN RESULTS
 We compare iRDA with several methods including prox-SGD, t-SGD and normal SGD, on different datasets including MNIST, CIFAR-10, CIFAR-100 and ImageNet(ILSVRC2012). The main results are shown in Table 1. Table 2 shows the performance of iRDA on different architectures including ResNet18, VGG16 and VGG19. Table 3 shows the performance of iRDA on different datasets including MNIST, CIFAR-10, CIFAR-100 and ImageNet(ILSVRC2012). In all tables, SGD denotes stochastic gradient methods with momentum Ruder (2016).

6

Under review as a conference paper at ICLR 2019
Figure 1: The first 120 epochs of loss curves corresponding to Table 1, and the sparsity curve for another result, where the top-1 validation accuracy is 91.34%, and  = 0.87. 4.3 COMPARISON Currently, many compression methods include human experts involvement. Some methods try to combine other structures in training process to automatize the compression process. For example, He et al. (2018) combines reinforcement learning. iRDA, as an algorithm, requires no extra structure. As shown above, iRDA can achieve good sparsity while keeping accuracy automatically, with carefully chosen parameters. For CIFAR-10, we compare the performance of iRDA with some other state-of-art compression methods in Table 4. Due to different standards,  is referred to directly or computed from the original papers approximately.
5 CONCLUSION
In comparison with many existing rule-based heuristic approaches, the new approach is based on a careful and iterative combination of 1 regularization and some specialized training algorithms. We find that the commonly used training algorithms such as SGD methods are not effective. We thus develop iRDA method that can be used to achieve much better sparsity. iRDA is a variant of RDA methods that have been used for some special types of online convex optimization problems in the literature. New elements in the iRDA mainly consist of judicious initialization and iterative retraining. In addition, iRDA method is carefully analyzed on its convergence for convex objective functions. Many deep neural networks trained by iRDA can achieve good sparsity while keeping the same validation accuracy as those trained by SGD with momentum on many popular datasets. This result shows iRDA is a powerful sparse optimization method for image classification problems in deep learning fields.
7

Under review as a conference paper at ICLR 2019

Table 1: The main results of different methods. The architecture is ResNet18, and the dataset is CIFAR-10. This table shows the top-1 and top-5 accuracies on the validation dataset. iRDA achieves the highest top-1 accuracy and sparsity. See figure 1 for the corresponding loss curves.

Method

TOP 1 Acc. TOP 5 Acc. 



SGD prox-SGD
t-prox-SGD iRDA

92.69 89.80
82.47 93.47

99.72 0.00 N/A N/A 99.40 0.03 10-5 0.8
99.07 0.72 10-8 1.0 99.69 0.95 10-6 1.0

Table 2: iRDA on different Architectures. This table shows the top-1 and top-5 accuracies on the validation dataset. The results from SGD with momentum are in the brackets. iRDA works well on different CNN architectures.

ARCHITECTURE TOP 1 Acc. TOP 5 Acc. 



ResNet18 VGG16 VGG19

93.47 (92.69) 99.69 (99.72) 0.95 10-6 1.0 93.24 (93.42) 99.52 (99.79) 0.94 10-6 1.0 91.87 (91.70) 99.37 (99.40) 0.98 10-5 1.0

Table 3: iRDA on different datasets. The architecture is ResNet18. This table shows the top-1 and top-5 accuracies on the validation dataset. The results from SGD with momentum are in the brackets. iRDA works well on different datasets.

DATASET

TOP 1 Acc. TOP 5 Acc. 



MNIST

99.63 (99.65)

100.00 0.95 10-6 0.1

CIFAR-10

93.47 (92.69) 99.69 (99.72) 0.95 10-6 1.0

CIFAR-100 72.29 (73.69) 89.94 (92.43) 0.56 10-8 0.09

ILSVRC2012 64.93 (70.58) 84.92 (89.64) 0.36 10-8 0.1

Table 4: iRDA and different state-of-the-art compression methods on CIFAR-10. This table shows the top-1 accuracies on the validation dataset. Due to different standards,  is referred to directly or computed approximately. iRDA achieves almost the same accuracy and sparsity on VGG16, while the sparsity on ResNet18 is much better.

ARCHITECTURE METHOD

TOP 1 Acc. 



ResNet-50 ResNet-18 VGG-16 VGG-16

AMC(RParam) He et al. (2018) iRDA VIBNet Dai et al. (2018)
iRDA

93.64 0.60 N/A N/A 93.47 0.95 10-6 1.0
93.80 0.94 N/A N/A 93.24 0.94 10-6 1.0

8

Under review as a conference paper at ICLR 2019
REFERENCES
Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.
Bin Dai, Chen Zhu, Baining Guo, and David Wipf. Compressing neural networks using the variational information bottleneck. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018), 2018.
D. L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289­1306, 2006.
John Duchi and Yoram Singer. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10(Dec):2899­2934, 2009.
Jing Feng. Sparsification methods in convolutional neural networks. Master's thesis, Beijing University of Technology, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249­256, 2010.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pages 1135­ 1143, 2015b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026­1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770­778, 2016.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV), pages 784­800, 2018.
Y Lecun, Y Bengio, and G Hinton. Deep learning. Nature, 521(7553):436, 2015.
Yann A LeCun, Le´on Bottou, Genevieve B Orr, and Klaus-Robert Mu¨ller. Efficient backprop. In Neural networks: Tricks of the trade, pages 9­48. Springer, 2012.
Qianxiao Li and Shuji Hao. An optimal control approach to deep learning and applications to discrete-weight neural networks. arXiv preprint arXiv:1803.01299, 2018.
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 806­814, 2015.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In Advances in Neural Information Processing Systems, pages 3288­3298, 2017.
Hisashi Mine and Masao Fukushima. A minimization method for the sum of a convex function and a continuously differentiable function. Journal of Optimization Theory and Applications, 33(1): 9­23, 1981.
Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming, 120(1):221­259, 2009.
Lorien Y. Pratt. Comparing biases for minimal network construction with back-propagation. In International Conference on Neural Information Processing Systems, pages 177­185, 1988.
9

Under review as a conference paper at ICLR 2019 Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, 22(3):400­407, 1951. Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2016. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. Computer Science, 2014. Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11(Oct):2543­2596, 2010. Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Bina-
ryrelax: A relaxation approach for training deep neural networks with quantized weights. arXiv preprint arXiv:1801.06313, 2018.
10

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 3.1

One of the differences between RDA Xiao (2010) and iRDA is that the former one takes w1 = arg min h(w) whereas the latter one chooses w1 randomly. In the following, we will prove the
w
convergence of iRDA Step 1 for convex problem. The proofs use Lemma 9, Lemma 10, Lemma 11
directly and modify Theorem 1 and Theorem 2 in Xiao (2010). For clarity, we have some general
assumptions:

· The regularization term (w) is a closed convex function with convexity parameter  and dom is closed.
· For each t  1, ft(w) is convex and subdifferentiable on dom.
· h(w) is strongly convex on dom and subdifferentiable on rint(dom) and also satisfies

w0 = arg min h(w)  Arg min (w).
ww

(33)

Without loss of generality, assume h(w) has convexity parameter 1 and minw h(w) = 0. · There exist a constant G such that

gt   G, t  1.

(34)

· Require {}t1 be a nonnegative and nondecreasing sequence and 0 = max{, 1} > 0.
Moreover, we could always choose 1   such that 0 = 1. · For a random choosing w1, we assume
(w1)  Q.

(35) (36)

First of all, we define two functions:

Ut(s) = max { s, w - w0 - t(w)},
wFD

Vt(s)

=

max{
w

s, w

-

w0

- t(w) - th(w)}.

(37) (38)

The maximum in (37) is always achieved because FD = {w  dom|h(w)  D2} is a nonempty compact set. Because of (35), we have t+t  0 > 0 for all t  0, which means t(w)+th(w) are all strongly convex, therefore the maximum in (38) is always achieved and unique. As a result, we have domUt = domVt = E for all t  0. Moreover, by the assumption (33), both of the functions are nonnegative.
Let st denote the sum of the subgradients obtained up to time t in iRDA Step 1, that is

t
st = g = tg¯t,
 =1

(39)

and t(s) denotes the unique maximizer in the definition of Vt(s)

t(s)

=

arg max{
w

s, w

-

w0

- t(w) - th(w)}

= arg min{ -s, w + t(w) + th(w)},
w

which then gives

wt+1 = t(-st).

(40) (41)

Lemma A.1 For any s  E and t  0, we have Ut(s) < Vt(s) + tD2.

(42)

For a proof, see Lemma 9 in Xiao (2010).

11

Under review as a conference paper at ICLR 2019

Lemma A.2 The function Vt is convex and differentiable. Its gradient is given by

Vt(s) = t(s) - w0

and the gradient Lipschitz continuous with constant 1/(t + t), that is

Vt(s1) - Vt(s2)

1 t + t

s1 - s2 ,

s1, s2  E.

Moreover, the following inequality holds:

Vt(s + g)  Vt(s) +

g, Vt(s)

1 +
2(t + t)

g

2 ,

s, g  E.

(43) (44) (45)

The results are from Lemma 10 in Xiao (2010).

Lemma A.3 For each t  1, we have Vt(-st) + (wt+1)  Vt-1(-st) + (t-1 - t)h(wt+1).

(46)

Since h(wt+1)  0 and the sequence {t}t1 is nondecreasing, we have

Vt(-st) + (wt+1)  Vt-1(-st), V1(-s1) + (w2)  V0(-s1) + (0 - 1)h(w2),

t  2, t = 1.

(47) (48)

To prove this lemma, we refer to the Lemma 11 in Xiao (2010). What's more, from the assumption 35, we could always choose 1   such that 1 = 0 and

V1(-s1) + (w2)  V0(-s1), t = 1.

(49)

The learner's regret of online learning is the difference between his cumulative loss and the cumulative loss of the optimal fixed hypothesis, which is defined by

tt

Rt(w) = (f (w ) + (w )) - (f (w) + (w)),

 =1

 =1

(50)

and bounded by

t

=

Q

+

tD2

+

G2 2

t-1  =0



1 +



.

(51)

Lemma A.4 Let the sequence {wt}t1 and {gt}t1 be generated by iRDA Step 1, and assume (34)

and (35) hold. Then for any t  1 and any w  FD = {w  dom|h(w)  D2}, the regret defined

in (50) is bounded by t

Rt(w)  t

(52)

Proof First, we define the following gap sequence which measures the quality of the solutions w1, .., wt:

t

t = max

g , w - w + (w ) - t(w) , t = 1, 2, 3, ....

wFD  =1

(53)

and t is an upper bound on the regret Rt(w) for all w  FD, to see this, we use the convexity of ft(w) in the following:

t
t  (f (w ) - f (w) + (w )) - t(w) = Rt(w).
 =1
Then, We are going to derive an upper bound on t. For this purpose, we subtract (53), which leads to

(54)

t  =1

g , w0

in

t

t = ( g , w - w0 + (w )) + max { st, w0 - w - t(w)} ,

 =1

wFD

(55)

12

Under review as a conference paper at ICLR 2019

the maximization term in (55) is in fact Ut(-st), therefore, by applying Lemma A.1, we have

t

t 

g , w - w0 + (w ) + Vt(-st) + tD2.

 =1

(56)

Next, we show that t is an upper bound for the right-hand side of inequality (56). We consider   2 and  = 1 respectively.
For any   2, we have

V (-s ) + (w+1)  V-1(-s-1) +

-g , w - w0

+

g

2 

,

2(( - 1) + -1)

where (47),(39),(45) and (43) are used. Therefore, we have

g , w - w0

+

(w +1 )



V -1 (-s -1 )

-

V

(-s

)

+

2((

g - 1)

2 
+

 -1 )

,

  2.

For  = 1, we have a similar inequality by using (49)

g1, w1 - w0

+ (w2)  V0(-s0) - V1(-s1) +

g1

2
.

20

Summing the above inequalities for  = 1, ..., t and noting that V0(-s0) = V0 = 0, we arrive at

t  =1

g , w - w0

+ (w+1)

+

Vt(-st)



t  =1

2((

g - 1)

2 
+

.  -1 )

Since (wt+1)  0, we subtract it from the left hand side and add (w1) to both sides of the above inequality yields

t  =1

g , w - w0

+ (w )

+

Vt(-st)



(w1)

+

1 2

t  =1

2((

g - 1)

2 
+

.  -1 )

(57)

Combing (54), (56), (57) and using assumption (34) and (36)we conclude

Rt(w)



t



t

=

Q

+

tD2

+

G2 2

t-1  =0



1 +



.

Lemma A.5 Assume there exists an optimal solution w to the problem (3) that satisfies h(w )  D2 for some D > 0, and let  = (w ). Let the sequences {wt}t1 be generated by iRDA Step 1, and assume gt   G for some constant G. Then for any t  1, the expected cost associated with
the random variable w¯t is bounded as

E(w¯t) - 



1 t t.

Proof First, from the definition (50), we have the regret at w

tt

Rt(w ) = (f (w , z ) + (w )) - (f (w , z ) + (w )),

 =1

 =1

Let z[t] denote the collection of i.i.d. random variables (z,..., zt). We note that the random variable w , where 1  w  t, is a function of (z1, ..., z-1) and is independent of (z , ..., zt). Therefore

Ez[t] (f (w , z ) + (w )) = Ez[-1] (E f (w , z ) + (w )) = Ez[-1](w ) = Ez[t](w ),

and Ez[t] (f (w , z ) + (w )) = E f (w , z ) + (w ) = (w ) =  .
Since  = (w ) = min (w), we have the expected regret
w

t
Ez[t]Rt(w ) = Ez[t](w ) - t  0.
 =1

(58)

13

Under review as a conference paper at ICLR 2019

Then, by convexity of , we have

(w¯t) = 

1t t w
 =1

1 t

t

 (w ) .

 =1

(59)

Finally, from (59) and (58), we have

1t

Ez[t](w¯t) - 

 t

Ez[t](w ) - t

 =1

1 = t Ez[t]Rt(w ).

Then the desired follows from that of Lemma A.4. Proof of Theorem 3.1 From Lemma A.5, the expected cost associated with the random variable w¯t is bounded as

E(w¯t) - 

1 t

Q

+

tD2

+

G2 2

t-1  =0



1 +



,

(60)

Here, we consider 1 regularization function (w) =  w 1 and it is a convex but not strongly

convex function, which means  = 0. Now, we consider how to choose t for t  1 and 0 = 1.

First if t = t, we have assume t = t,  > 0

1 t

·

tD2

and  =

= D2, which means the expected cost does not converge. 1, the right hand side of the inequality (60) becomes

Then

1 t

Q+

D2t

+

G2 2

t-1

1 

1 t

Q + D2t + G2 2

t-1 1 2 + 

 =0

 =2

1 t

Q + D2t + G2 2 + 2

t-1 1 1 

 O(t-1 + t-).

From above, we see that if 0 <  < 1, the expected cost converges and the optimal convergence

rate

O(t-

1 2

)

achieves

when



=

1 2

.

Then

we

proved

the

Theorem

3.1.

B INITIALIZATION

Table 5: Different initialization scalars on CIFAR-10 with iRDA. The architecture is ResNet18.  = 10-6 and  = 1.0. This table shows the top-1 accuracies on the validation dataset. All models are trained for 120 epochs.

s TOP 1 Acc. TOP 5 Acc. 

1, 2 3 4 5 10 100 1000 10000 20000

10.00 85.52 86.72 90.03 90.67 91.41 90.36 71.80 68.06

50.00 99.24 99.45 99.44 89.50 99.58 99.62 97.94 97.39

N/A 0.98 0.97 0.95 0.94 0.84 0.63 0.34 0.99

Table 6: Different initialization scalars on CIFAR-100 with iRDA. The architecture is ResNet18.  = 10-8 and  = 0.1. This table shows the top-1 accuracies on the validation dataset. All models are trained for 120 epochs.

s TOP 1 Acc. TOP 5 Acc. 

1

63.67

87.85 0.91

2

66.90

88.53 0.60

5

65.47

88.09 0.60

10

65.54

88.21 0.42

15

64.22

87.53 0.43

25

63.06

88.10 0.50

30

62.75

86.80 0.42

50

64.48

87.14 0.38

100 60.00 86.14 0.36

14

Under review as a conference paper at ICLR 2019

C PARAMETERS TEST

Table 7: Fix  = 1.0 and test different  with different methods on CIFAR-10. The architecture is ResNet18. All models are trained for 120 epochs. This table shows the top-1 accuracies on the validation dataset. We have shown why prox-SGD will give poor sparsity, and although t-proxSGD may introduce greater sparsity, it is not convergent. Finally, iRDA gives the best result, on both the top-1 accuracy and the sparsity.
 10-2 10-3 10-4 10-5 10-6 10-7 10-8

iRDA(without Retraining) 
t-prox-SGD  prox-SGD 

10.00 1.00
10.00 1.00 17.90 1.00

38.73 1.00
10.00 1.00 75.11 0.97

74.04 1.00
10.00 1.00 88.74 0.52

86.39 0.99
10.00 1.00 88.20 0.01

90.67 0.94
8.43 1.00 89.02 0.00

90.14 0.61
49.20 0.99 86.92 0.00

88.38 0.16
82.47 0.72 87.75 0.00

Table 8: Different  and  with iRDA on CIFAR-10. The architecture is ResNet18. All models are trained for 120 epochs. This table shows the top-1 accuracies on the validation dataset. The highest accuracy is 92.07% with sparsity 0.84, and with a lower accuracy 90.67% we get a sparsity of 0.94. The result shows that we can balance the accuracy and the sparsity with iRDA, by adjusting the parameters  and .

 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

 = 10-6
  = 10-7
  = 10-8


90.73 0.78
90.87 0.41
91.42 0.09

91.68 0.81
91.39 0.45
91.00 0.12

91.88 0.82
91.88 0.49
91.62 0.14

92.07 0.84 90.77 0.55 92.25 0.12

91.68 0.89 91.80 0.53 91.06 0.13

90.33 0.83 90.46 0.55 89.71 0.14

90.88 0.92 90.04 0.60 87.11 0.13

89.40 0.94 N/A N/A
90.45 0.15

89.72 0.94 90.19 0.61 86.31 0.16

90.67 0.94
90.14 0.61
88.38 0.16

15

