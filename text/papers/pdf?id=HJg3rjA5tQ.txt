Under review as a conference paper at ICLR 2019

SPREAD DIVERGENCES
Anonymous authors Paper under double-blind review

ABSTRACT
For distributions p and q with different support, the divergence D(p||q) generally will not exist. We define a spread divergence D~ (p||q) on modified p and q and describe sufficient conditions for the existence of such a divergence. We give examples of using a spread divergence to train implicit generative models, including linear models (Principal Components Analysis and Independent Components Analysis) and non-linear models (Deep Generative Networks).

1 INTRODUCTION

A divergence D(p||q) (see, for example Dragomir (2005)) is a measure of the difference between two distributions p and q with the property

D(p||q)  0, and D(p||q) = 0  p = q

(1)

Whilst our general discussion will hold for all divergences, some results are specific to the f divergence, defined as

p(x) Df (p||q) = Eq(x) f q(x)

(2)

where f (x) is a convex function with f (1) = 0. An important special case of an f -divergence is the

well-known Kullback-Leibler divergence KL(q||p) = Eq(x)

log

p(x) q(x)

which is widely used to train

models using maximum likelihood. We are interested in situations in which the supports of the two

distributions are different, supp (p) = supp (q). In this case the divergence may not be defined. For

example, for p(x) being an empirical data distribution on continuous dataset x1, . . . , xN , p(x) =

1 N

N n=1



(x

-

xn)

where



(·)

is

the

Dirac

Delta

function.

For

a

model

q(x)

with

support

R,

then

KL(q||p) is not formally defined. This is a challenge since implicit generative models of the form

q(x) =  (x - g(z)) p(z)dz only have limited support; in this case maximum likelihood to learn the model parameter  is not available and alternative approaches are required ­ see Mohamed &

Lakshminarayanan (2016) for a recent survey.

2 SPREAD DIVERGENCES

The aim is, from q(x) and p(x) to define new distributions q~(y) and p~(y) that have the same support1. Using the notation x to denote integration (·) dx for continuous x, and xX for discrete x with domain X , we define

p~(y) = p(y|x)p(x),
x

q~(y) = p(y|x)q(x)
x

(3)

where p(y|x) is a `noise' process designed to `spread' the mass of p and q such that p~(y) and q~(y) have the same support. For example, if we use a Gaussian p(y|x) = N y x, 2 , then p~ and q~ both
have support R. We therefore use noise with the property that, despite D(p||q) not existing, D(p~||q~) does exist and we define the Spread Divergence

D~ (p||q)  D(p~||q~)

(4)

1For simplicity, we use univariate x, with the extension to the multivariate setting being straightforward.

1

Under review as a conference paper at ICLR 2019

Note that this satisfies the divergence requirement D~ (p||q)  0. The second requirement, D~ (p||q) = 0  p = q, is guaranteed for certain `noise' processes, as described in section(2.2).
Spread divergences have many potential applications. For example, for a model q(x|) with parameter  and empirical data distribution p(x), maximum likelihood training corresponds to minimising KL(p||q) with respect to . However, for implicit models, the divergence KL(p||q) does not exist. However, if a spread divergence exists, provided that the data is distributed according the model p(x) = q(x|0) for some unknown parameter 0, the spread divergence D(p(x)||q(x|)) has a minimum at  = 0. That is (for identifiable models) we can correctly learn the underlying data generating process, even when the original divergence is not defined.

2.1 SPREAD NOISE MAKES DISTRIBUTIONS MORE SIMILAR

The general effect of spread noise is that it makes the distributions p and q more similar. To show this formally for the case of f -divergences, we consider the following joint distributions

q(y, x) = p(y|x)q(x),

p(y, x) = p(y|x)p(x)

(5)

whose marginals are the spreaded distributions

p~(y) = p(y|x)p(x),

q~(y) = p(y|x)q(x)

xx

The divergence between the two joint distributions is

(6)

Df (q(y, x)||p(y, x)) = p(y, x)f
x

p(y|x)q(x) p(y|x)p(x)

= Df (q(x)||p(x))

(7)

Recently, Zhang et al. (2018) showed that the f -divergence between two marginal distributions is no larger than the f -divergence between the joint. Hence,

Df (q~(y)||p~(y))  Df (q(y, x)||p(y, x)) = Df (q(x)||p(x))

(8)

Intuitively, spreading two distributions increases their overlap, reducing the divergence. When p and
q do not have the same support, Df (q(x)||p(x)) can be infinite or not well-defined. However, as we show below, provided we use noise appropriately, Df (q~(y)||p~(y)) is always well-defined.

2.2 NOISE REQUIREMENTS FOR A SPREAD DIVERGENCE

Whilst adding spread noise reduces the f -divergence, our main interest is in using noise to define a new divergence in situations in which the original divergence D(p||q) is itself not defined. For discrete variables, the noise Pij = p(y = i|x = j) must be a distribution i Pij = 1, Pij  0 and

Pij pj = Pij qj i  pj = qj j
jj

(9)

which is equivalent to the requirement that the matrix P is invertible. There is an additional requirement that the spread divergence exists. In the case of f -divergences, the spread divergence exists provided that p~ and q~ have the same support. This is guaranteed if

Pij pj > 0,
j

Pij qj > 0 i
j

(10)

which is satisfied if Pij > 0. In general, therefore, there is a space of noise distributions p(y|x) that define a valid spread divergence. The `antifreeze' method of Furmston & Barber (2009) is a special
form of spread noise to define a valid Kullback-Leibler divergence (see also Barber (2012)).

For continuous variables, in order that D~ (p||q) = 0  p = q, the noise p(y|x) must be a probability density and satisfy

p(y|x)p(x)dx = p(y|x)q(x)dx y  p(x) = q(x) x

(11)

This is satisfied if there exists an invertible transform p-1 such that p-1(x |y)p(y|x)dy =  (x - x)

(12)

2

Under review as a conference paper at ICLR 2019

where  (·) is the Dirac delta function. As for the discrete case, the spread divergence exists provided that p~ and q~ have the same support, which is guaranteed if p(y|x) > 0. A well known example of such an invertible integral transform is the Weierstrass Transform p(y|x) = N y x, 2 , which has an explicit representation for p-1. In general, however, we can demonstrate the existence of a spread divergence without the need for an explicit representation of p-1. As we will see below, the noise requirements for defining a valid spread divergence such that D(p~||q~) = 0  p = q are analogous to the requirements on kernels such that the Maximum Mean Discrepancy MMD(p, q) = 0  p = q,
see Sriperumbudur et al. (2011) and Sriperumbudur et al. (2012).

3 STATIONARY SPREAD DIVERGENCES

Consider stationary noise p(y|x) = K(y - x) where K(x) is a probability density function with K(x) > 0, x  R. In this case p~ and q~ are defined as a convolution

p~(y) = K(y - x)p(x)dx = (K  p) (y), q~(y) = K(y - x)q(x)dx = (K  q) (y) (13)

Since K > 0, p~ and q~ are guaranteed to have the same support R. A sufficient condition for the existence of the Fourier Transform F {f } of a function f (x) for real x is that f is absolutely
integrable. By definition, all distributions p(x) are absolutely integrable and therefore have a Fourier Transform, so that both F {p} and F {q} are guaranteed to exist. Assuming F {K} exists, we can
use the convolution theorem to write

F {p~} = F {K} F {p} , F {q~} = F {K} F {q}

(14)

Hence, we can write

D(p~||q~) = 0  p~ = q~  F {K} F {p} = F {K} F {q}  F {p} = F {q}  p = q (15)

where we used the invertibility of the Fourier transform and assumed that F {K} = 0, or equivalently2, F {K} > 0. Hence, provided that K(x) > 0 and F {K} > 0 then K(x) defines a valid spread divergence. Note that other transforms have a corresponding convolution theorem3 and the
above derivation holds, with the requirement that the corresponding transform of K(x) is non-zero.

As an example of such a noise process, consider Gaussian noise,

K(y - x) =  1

e-

1 22

(y-x)2

22

leading to a positive Fourier Transform:

F {K} () =  1



eix

e-

1 22

x2 dx

=

e-

22 2

>0

22 -

Similarly,

for

Laplace

noise

K (x)

=

1 2b

e-

1 b

|x|

p(y|x) = K(y - x) =

1

e-

1 b

|y-x|

,

2b

F {K} () =

2 b-1  b-2 + 2 > 0

Since K > 0 and F {K} > 0, this also defines a valid spread divergence over R.

(16) (17) (18)

3.1 INVERTIBLE MAPPINGS

Consider p(y|x) = K(y - f (x)) for strictly monotonic f . Then, using the change of variables

p~(y) = K(y - z)pz(z)dz, pz(z) = px(f -1(z)) |J x = f -1(z) | -1

(19)

where J is the Jacobian of f . For distributions p(x) with bounded domain, for example x  [0, 1], we can use a logit function, f (x) = - log x-1 - 1 , which maps the interval [0, 1] to R. Using then, for example, Gaussian spread noise p(y|x) = N y f (x), 2 , both p~(y) and q~(y) have support R. If D(p~||q~) is zero then p = q on the domain [0, 1].
2If F {K} can change sign, by continuity, there must exist a point at which F {K} = 0. 3This includes the Laplace, Mellin and Hartley transforms.

3

Under review as a conference paper at ICLR 2019

bb

a A

u
a A

Figure 1: Left: The lower dotted line denotes Gaussian distributed data p(x) with support only along the linear subspace defined by the origin a and direction A. The upper dotted line denotes Gaussian distributed data q(x) with support different from p(x). Optimally, to maximise the spread divergence between the two distributions, for fixed noise entropy, we should add noise that preferentially spreads out along the directions defined by p and q, as denoted by the ellipses.

3.2 MAXIMISING THE SPREAD

To gain intuition, we define p and q to generate data in separated linear subspaces, p(x) =  (x - a - Az) p(z)dz, q(x) =  (x - b - Bz) p(z)dz, p(z) = N (z 0, Iz). Using Gaussian
spread, p(y|x) = N (y µ, ), for fixed entropy of p(y|x), what is the optimal µ,  that maximises
the divergence? In this case the spreaded distributions are given by

p~(y) = N y µ + a, AAT +  ,

q~(y) = N y µ + b, BBT + 

(20)

We define a simple Factor Analysis noise model with  = 2I + uuT, where 2 is fixed and uTu = 1. The entropy of p(y|x) is then fixed and independent of u. Also, for simplicity, we assume A = B. It is straightforward to show that the spread divergence KL(p~||q~) is maximised for u pointing orthogonal to the vector AAT + 2I -1 (b - a). Then u optimally points along the
direction in which the support lies. The support of p(y|x) must be the whole space but to maximise
the divergence the noise preferentially spreads along directions defined by p and q, see figure(1).

4 MERCER SPREAD DIVERGENCE

Let x  [a, b], y  [a, b] and K(x, y) = K(y, x) be square integrable, K(x, y)  L2. We define Mercer noise p(y|x) = K(x, y)/K(x), where K(x) = K(x, y)dy. For strictly positive definite K, by Mercer's Theorem, it admits an expansion

K(x, y) = nn(x)n(y)
n

(21)

where the eigenfunctions n form a complete orthogonal set of L2[a, b] and all n > 0, see for example Sriperumbudur et al. (2011). Then

p~(y) =

nn(x)n(y)p(x)dx,

n

q~(y) =

nn(x)n(y)q(x)dx

n

and p~(y) = q~(y) is equivalent to the requirement

p(x) q(x)

nn(x)n(y) K(x) dx =

nn(x)n(y) K(x) dx

nn

Multiplying both sides by m(y) and integrating over y we obtain

p(x) q(x) m(x) K(x) dx = m(x) K(x) dx

If p(x)/K(x) and q(x)/K(x) are in L2[a, b] then we may write

p(x) =
K (x)

np n (x),

n

q(x) =
K (x)

nq n(x)

n

(22) (23) (24) (25)

4

Under review as a conference paper at ICLR 2019

Then, equation(24) is

m(x) npn(x)dx = m(x) nq n(x)dx
nn

(26)

which reduces to (using orthonormality), mp = mq  p = q. For example, for strictly monotonic f (x) with domain X and codomain [a, b], K(x, y) = exp - (f (x) - f (y))2 enables one to define a spread divergence for distributions with domain X .

5 APPLICATIONS

We demonstrate using a spread divergence to train implicit models

q(x) =  (x - g(z)) q(z)dz

(27)

where  are the parameters of the generating function g. We start with some simple linear models and show that, despite the likelihood not being defined, we can nevertheless successfully train the models using an EM (see for example Barber (2012)) style algorithm. We then show how to train a deterministic non-linear generative model using a variational approximation.

5.1 DETERMINISTIC LINEAR LATENT MODEL

For observation noise , the Probabilistic PCA model Tipping & Bishop (1999) is

x = F z +  , z  N (0, Iz),  N (0, Ix), p(x) = N y 0, F F T + 2Ix (28)

To fit the model to iid data {x1, . . . , xN } using maximum likelihood, the only information required

from the dataset is the data covariance ^ . PPCA has closed form solution, F = Uz

z - 2Iz

1
2 R,

where z, Uz are the z largest eigenvalues, eigenvectors of ^ ; R is an arbitrary orthogonal matrix. Using spread noise p(y|x) = N y x, 2I , the spreaded distribution p~(y) is a Gaussian

p~(y) = N y 0, F F T + (2 + 2)Ix

(29)

Thus, p~(y) is of the same form as PPCA, albeit with an inflated covariance matrix. Adding Gaussian spread noise to the data also simply inflates the sample covariance to ^ = ^ + 2Ix. Since the eigenvalues of ^  ^ + 2Ix are simply  =  + 2I, with unchanged eigenvectors, the
11
optimal deterministic ( = 0) latent linear model has solution F = Uz z - 2Iz 2 R = Uzz2 R. Unsurprisingly, this is exactly the standard PCA solution. Whilst this result is to be expected, the
route to its derivation is non-standard since the likelihood of the deterministic latent linear model
is not defined and maximum-likelihood cannot be directly used to train this model. Nevertheless,
using the spread divergence, we learn a sensible model and recover the true data generating process
if the data were exactly generated according to the deterministic model.

5.2 DETERMINISTIC INDEPENDENT COMPONENTS ANALYSIS

ICA corresponds to the model, Bermond & Cardoso (1999),

p(x, z) = p(x|z) p(zi)
i

(30)

where the independent components zi follow a non-Gaussian distribution. For Gaussian noise ICA an observation x is assumed to be generated by the process

p(x|z) = N xj gj(z), 2
j

(31)

where gi(z) mixes the independent latent process z. In standard linear ICA, gj(z) = aTj z where aj is the jth column on the mixing matrix A. A well known problem with ICA is that, for small

5

Under review as a conference paper at ICLR 2019

(a) True A (left) versus estimated A (right).

(b) Data zn vs posterior mean of p(zn|xn).

Figure 2: For X = 20 observations and Z = 10 latent variables, we generate N = 5000 datapoints from the model x = Az, for independent zero mean unit variance Laplace components on z. We use Sy = 1, Sz = 1000 samples and 2000 EM iterations to estimate the mixing matrix.

observation noise 2, the EM algorithm becomes ineffective. To see this, consider X = Z and
invertible mixing matrix A, x = Az. At iteration k the EM algorithm has an estimate Ak of the mixing matrix. The M-step of the EM algorithm updates A using

-1
Ak+1 = E xzT E zzT

(32)

where, for noiseless data ( = 0),

E

xzT

1 =
N

xn Ak-1xnT = S^A-k T,

n

E zzT = A-k 1S^Ak-T

(33)

where S^



1 N

n xnxnT is the moment matrix of the data. Thus, Ak+1 = S^Ak-T

Ak-1S^Ak-T

-1
=

Ak. and the algorithm `freezes'. Similarly, for low noise  1 progress critically slows down.

Whilst over-relaxation methods, see for example Winther & Petersen (2007) can help in the case of

small noise, for zero noise  = 0, over-relaxation is of no benefit.

5.2.1 HEALING CRITICAL SLOWING DOWN
To deal with small noise and the limiting case of a deterministic model ( = 0), we consider Gaussian spread noise p(y|x) = N y x, 2Ix to give

p(y, z) = p(y|x)p(x, z)dx = N y gi(z), 2 + 2 Ix p(zi)
ii

The empirical distribution is replaced by the spreaded empirical distribution

1 p^(y) =
N

N y xn, 2Ix

n

The M-step has the same form as equation(32) but with modified statistics

E

yzT

1 =
N

n

N y xn, 2 p(z|y)yzTdzdy,

(34) (35) (36)

E

zzT

1 =
N

n

N y xn, 2 p(z|y)zzTdzdy

The E-step optimally sets

p(z|y) = 1 N (z µ(y), ) Zq (y)

i

p(zi),

Zq(y) =

(37)
N (z µ(y), ) p(zi)dz (38)
i

6

Under review as a conference paper at ICLR 2019

(a) VAE samples

(b) Noisy VAE means

(c) Noisy VAE samples

Figure 3: Comparison of deep generative models trained on the MNIST digits after 300K iterations. (a) Samples from the trained deterministic model. (b) Means from a standard (noisy) VAE with fixed observation noise. (c) Samples from the standard noisy VAE model.

where Zq(y) is a normaliser and
-1
 = (2 + 2) ATA ,

-1
µ(y) = ATA Ay

(39)

We can rewrite the expectations required for the E-step of the EM algorithm as

E

yzT

1 =
N

n

N y xn, 2 N (z µ(y), ) i p(zi) yzTdzdy Zq (y)

(40)

E

zzT

1 =
N

n

N y xn, 2 N (z µ(y), ) i p(zi) zzTdzdy Zq (y)

(41)

Generally the posterior p(z|y) will be peaked around N (z µ(y), ) and writing the expectations with respect to N (z µ(y), ) allows for an effective sampling approximation focussed on regions
of high probability. We can then implement this update by simply sampling Sy samples from N y xn, 2I and, for each y sample, we sample Sz samples from N (z µ(y), ). This sampling scheme has the advantage over more standard variational approaches, see for example Winther & Petersen (2007), in that we obtain a consistent estimator of the M-step update for A4. We show
results for a toy experiment in figure(2), learning the underlying mixing matrix in deterministic non-
square setting. Note that standard algorithms such as FastICA Hyvarinen (1999) fail in this setting. The noise value is set to  = max(0.001, 2.5  sqrt(mean(AAT))), for estimated mixing matrix A
of the underlying deterministic model xn = Azn, n = 1, . . . , N . The EM algorithm learns a good approximation of the unknown mixing matrix and latent components zn, with no critical slowing down. Note that, if we were to set the spread noise to a very low value, similar to standard training,
learning would also slow down ­ however, for the spread divergence we can successfully train the
model for non-zero noise, meaning that critical slowing down is no longer an issue.

5.3 TRAINING IMPLICIT NON-LINEAR MODELS
For a deterministic non-linear implicit model, we set q(z) = N (z 0, I) and parameterise g(x) by a deep neural network. The likelihood equation(27) is in general intractable and it is natural to consider a variational approximation, Kingma & Welling (2013),

log q(x)  - q(z|x) (log q(z|x) + log (q(x|z)q(z))) dz

(42)

4We focus on demonstrating how the spread divergences heals critical slowing down, rather than deriving a state-of-the-art approximation of p(z|y). The importance sampling approach has fast run time and works well, even for large latent dimensions, Z = 50. We also implemented a variational factorised approximation of p(z|y) but found this to be relatively slow and ineffective. A variational Gaussian approximation of p(z|y)
improves on the factorised approximation, but is still slow compared to the importance sampling scheme.

7

Under review as a conference paper at ICLR 2019

(a) VAE samples

(b) Noisy VAE means

(c) Noisy VAE samples

Figure 4: Comparison of training approaches for the CelebA dataset. All models had the same structure and were trained using the same Adam settings, as in the MNIST experiment.

However, since q(x|z) =  (x - g(z)) this bound is not well defined. Instead, we minimise the spread divergence KL(p~(y)||q~(y)). The approach is a straightforward extension of the standard variational autoencoder and in section(A) we provide details of how to do this, along with higher resolution images of samples from the generative model. We dub this model and associated spread divergence training the `VAE'. As a demonstration, we trained a generative network on the MNIST dataset, figure(3) and section(B). We used Gaussian spread noise  = 1 for the VAE and observation noise  = 0.5 for the standard noisy VAE. The network g(x) contains 8 layers, each layer with 400 units and relu activation function and latent dimension Z = 64.
We trained a deep convolutional generative model on the CelebA dataset Liu et al. (2015), see figure(4) and section(C). We pre-process CelebA images by first taking 140x140 centre crops and then resizing to 64x64. Pixel values were then rescaled to lie in [0, 1]. We use Gaussian spread noise  = 0.5 for the VAE and observation noise  = 0.5 for the standard noisy VAE.
6 SUMMARY
We described an approach to defining a divergence, even when two distributions to not have the same support. The method works by introducing a new `noise' variable that allows one to `spread' mass from each distribution to form cover the same domain. Previous approaches Furmston & Barber (2009), Sønderby et al. (2016) can be seen as special cases, which we note is not restricted to convolutional forms of noise alone. We showed that defining divergences this way enables us to train deterministic generative models (both classical linear and non-linear) using standard `likelihood' based approaches. Indeed, for simple models such as Independent Components Analysis, we showed how we can implement a principled learning method based on classical EM training, without the standard difficulty of critical slowing down in the case of small (or zero) observation noise.
Introducing noise means that an additional expectation is required. This can be carried out, in part, exactly, although additional approximations using perturbation theory are possible, similar to Roth et al. (2017). Spread divergences have deep connections to other approaches to define measures of disagreement between distributions. In particular, one can view the spread divergence as the probabilistic analogue of MMD, with conditions required for the existence of the spread divergence closely related to the universality requirement on MMD kernels, Micchelli et al. (2006).
Theoretically, we can learn the underlying true data generating process by the use of any valid spread divergence -- for example for fixed Gaussian spread noise. In practice, however, the quality of the learned model can depend on the choice of spread noise. In this work we fixed the spread noise, but showed that if we were to learn the spread noise, it would preferentially spread mass across the manifolds defining the two distributions. In future work, we will investigate learning spread noise to maximally discriminate two distributions, which would involve a minimax model training objective, with an inner maximisation over the spread noise and an outer maximisation over the model parameters. This would bring our work much closer to adversarial training methods, Goodfellow (2017).

8

Under review as a conference paper at ICLR 2019
REFERENCES
D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, New York, NY, USA, 2012. ISBN 0521518148, 9780521518147.
O. Bermond and J. F. Cardoso. Approximate likelihood for noisy mixtures. In Proc. ICA 99, pp. 325­330, 1999.
S. S. Dragomir. Some general divergence measures for probability distributions. Acta Mathematica Hungarica, 109(4):331­345, Nov 2005. ISSN 1588-2632. doi: 10.1007/s10474-005-0251-6.
T. Furmston and D. Barber. Solving deterministic policy (PO)MPDs using ExpectationMaximisation and Antifreeze. In First international workshop on learning and data mining for robotics (LEMIR), pp. 56­70, 2009. In conjunction with ECML/PKDD-2009.
Ian J. Goodfellow. NIPS 2016 Tutorial: Generative Adversarial Networks. CoRR, abs/1701.00160, 2017.
A. Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE Transactions on Neural Networks, 10(3):626­634, May 1999. ISSN 1045-9227. doi: 10.1109/ 72.761722.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. arXiv:1312.6114 [stat.ML], 2013. Z. Liu, P. Luo, X. Wang, and X. Tang. Deep Learning Face Attributes in the Wild. In Proceedings
of International Conference on Computer Vision (ICCV), 2015. C. A. Micchelli, Y. Xu, and H. Zhang. Universal Kernels. Journal of Machine Learning Research,
6:2651­2667, 2006. S. Mohamed and B. Lakshminarayanan. Learning in implicit generative models. arXiv preprint,
2016. doi: arXiv:1610.03483. K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. Stabilizing Training of Generative Adversarial
Networks through Regularization . arXiv:1705.09367, 2017. C. K. Sønderby, J. Caballero, L. Theis, W. Shi, and F. Husza´r. Amortised map inference for image
super-resolution. arXiv preprint arXiv:1610.04490, 2016. B. Sriperumbudur, K. Fukumizu, A. Gretton, B. Scho¨lkopf, and G. Lanckriet. On the Empirical
Estimation of Integral Probability Metrics. Electronic Journal of Statistics, 6:1550­1599, 2012. B. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet. Universality, Characteristic Kernels
and RKHS Embedding of Measures. J. Mach. Learn. Res., 12:2389­2410, July 2011. ISSN 1532-4435. M. E. Tipping and C. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, Series B, 21/3:611622, January 1999. O. Winther and K. B. Petersen. Bayesian independent component analysis: Variational methods and non-negative decompositions. Digital Signal Processing, 17(5):858 ­ 872, 2007. ISSN 10512004. Special Issue on Bayesian Source Separation. M. Zhang, T. Bird, R. Habib, T. Xu, and D. Barber. Training Generative Latent Models by Variational f -Divergence Minimization. arXiv preprint, 2018.
9

Under review as a conference paper at ICLR 2019

A SPREAD DIVERGENCE FOR DETERMINISTIC DEEP GENERATIVE MODELS

Instead of minimising the likelihood, we train an implicit generative model by minimising the spread divergence

min KL(p~(y)||q~(y))


(43)

where

1 p~(y) =
N

N

N

y xn, 2Ix

n=1

(44)

and

q~(y) = p(y|x)q(x)dx = N y g(z), 2Ix q(z)dz = q(y|z)q(z)dz

(45)

According to our general theory,

min KL(p~(y)||q~(y)) = 0  p(x) = q(x)


(46)

Here

1N KL(p~(y)||q~(y)) = N

N y xn, 2Ix log q~(y)dy + const.

n=1

(47)

Typically, the integral over y will be intractable and we resort to an unbiased sampled estimate (though see below for Gaussian q). Neglecting constants, the KL divergence estimator is

1N NS

s
log q~(ysn)

n=1 s=1

(48)

where ysn is a noisy sample of xn, namely ysn  N ysn xn, 2Ix . In most cases of interest, with non-linear g, the distribution q~(y) is intractable. We therefore use the variational lower bound

log q~(y)  q(z|y) (- log q(z|y) log (q(y|z)q(z))) dz

(49)

Parameterising the variational distribution as a Gaussian,

q(z|y) = N (z µ(y), (y))

(50)

then we can reparameterise and write

log q~(y)  H() + EN ( 0,I) [log (q(y|z = µ + C )qz(z = µ + C ))]

(51)

where H is the entropy of a Gaussian with covariance . For Gaussian spread noise in D dimensions, this is (ignoring constants)

log q~(y)  H()+EN (

0,I )

-1 (22)D/2

(y - g (µ(y) + C

))2

+ log qz(z

=

µ(y) + C

)

(52)

where C is the Cholesky decomposition of .

The overall procedure is therefore a straightforward modification of the standard VAE method Kingma & Welling (2013) in which both the model and data are corrupted by noise:

1. Choose a noise corruption variance 2.
2. Choose a tractable family for the variational distribution, for example q(z|y) N (z µ(y), (y)) and initialise .
3. We then sample a noisy version yn for each datapoint (if we're using S = 1 samples)
4. Draw samples to estimate log q~(yn), equation(52)

=

10

Under review as a conference paper at ICLR 2019

5. Do a gradient ascent step in (, ). 6. Go to 3 and repeat until convergence.

We note that for  independent of y, we can partially integrate equation(52) over y to give the bound

N y x, 2Ix log q~(y)  H() + EN ( 0,I) [log qz(z = µ(y) + C )]

(53)

1 - (22)D/2 EN (

0,I )

EN (y x,2Ix)

(y - g (µ(y) + C ))2

(54)

where

EN (y x,2Ix) (y - g (µ(y) + C ))2
= 2 - 2EN ( x 0,Ix) [ xg(µ(x +  x))] + EN ( x 0,Ix) (x - g(µ (x +  x)))2 (55)

Similar to Roth et al. (2017), in principle, one can form a perturbation approximation of the above to second order in x and express the integral over the spread noise as a form of regularisation; however, in our experiments we found that the above works well ­ we therefore leave such analysis for future work.

B MNIST EXPERIMENT
We first scaled the MNIST data to lie in [0.05, 0.95] and then transformed using the logit (inverse logistic sigmoid) of the pixel value in order to use Gaussian spread noise. We use Gaussian spread noise  = 1 for the VAE and observation noise  = 0.5 for the standard noisy VAE. The network g(x) contains 8 layers, each layer with 400 units and relu activation function and latent dimension Z = 64. The variational inference network q(z|y) = N z µ(y), 2Iz has a similar structure for the mean network µ(y). Learning was done using the Adam optimiser with learning rate 10-4 and exponential decay rate of 0.96 every 10000 iterations.

C CELEBA EXPERIMENT
Both encoder and decoder used fully convolutional architectures with 5x5 convolutional filters and used vertical and horizontal strides 2 except the last deconvolution layer we used stride 1. Here Convk stands for a convolution with k filters, DeConvk for a deconvolution with k filters, BN for the batch normalization Ioffe & Szegedy (2015), ReLU for the rectified linear units, and FCk for the fully connected layer mapping to Rk.
x  R64×64×3  Conv128  BN  Relu  Conv256  BN  Relu  Conv512  BN  Relu  Conv1024  BN  Relu  FC64

z  R64  FC8×8×1024  DeConv512  BN  Relu  DeConv256  BN  Relu  DeConv128  BN  Relu  DeConv64  BN  Relu  DeConv3

11

Under review as a conference paper at ICLR 2019
Figure 5: VAE samples
Figure 6: Noisy VAE means 12

Under review as a conference paper at ICLR 2019
Figure 7: Noisy VAE samples
Figure 8: VAE samples 13

Under review as a conference paper at ICLR 2019
Figure 9: Noisy VAE means
Figure 10: Noisy VAE samples 14

