Under review as a conference paper at ICLR 2019

D2KE: F D

K

RF

FS

Anonymous authors Paper under double-blind review

E I

A
We present a new methodology that constructs a family of positive definite kernels from any given dissimilarity measure on structured inputs whose elements are either real-valued time series or discrete structures such as strings, histograms, and graphs. Our approach, which we call D2KE (from Distance to Kernel and Embedding), draws from the literature of Random Features. However, instead of deriving random feature maps from a user-defined kernel to approximate kernel machines, we build a kernel from a random feature map, that we specify given the distance measure. We further propose use of a finite number of random objects to produce a random feature embedding of each instance. We provide a theoretical analysis showing that D2KE enjoys better generalizability than universal Nearest-Neighbor estimates. On one hand, D2KE subsumes the widely-used representative-set method as a special case, and relates to the well-known distance substitution kernel in a limiting case. On the other hand, D2KE generalizes existing Random Features methods applicable only to vector input representations to complex structured inputs of variable sizes. We conduct classification experiments over such disparate domains as time series, strings, and histograms (for texts and images), for which our proposed framework compares favorably to existing distance-based learning methods in terms of both testing accuracy and computational time.
1I
In many problem domains, it is easier to specify a reasonable dissimilarity (or similarity) function between instances than to construct a feature representation. This is particularly the case with structured inputs whose elements are either real-valued time series or discrete structures such as strings, histograms, and graphs, where it is typically less than clear how to construct the representation of entire structured inputs with potentially widely varying sizes, even when given a good feature representation of each individual component. Moreover, even for complex structured inputs, there are many well-developed dissimilarity measures, such as the Dynamic Time Warping measure between time series, Edit Distance between strings, Hausdorff distance between sets, and Wasserstein distance between distributions.
However, standard machine learning methods are designed for vector representations, and classically there has been far less work on distance-based methods for either classification or regression on structured inputs. The most common distance-based method is Nearest-Neighbor Estimation (NNE), which predicts the outcome for an instance using an average of its nearest neighbors in the input space, with nearness measured by the given dissimilarity measure. Estimation from nearest neighbors, however, is unreliable, specifically having high variance when the neighbors are far apart, which is typically the case when the intrinsic dimension implied by the distance is large.
To address this issue, a line of research has focused on developing global distance-based (or similaritybased) machine learning methods (Pkkalska & Duin, 2005; Duin & Pekalska, 2012; Balcan et al., 2008a; Cortes et al., 2012), in large part by drawing upon connections to kernel methods (Scholkopf et al., 1999) or directly learning with similarity functions (Balcan et al., 2008a; Cortes et al., 2012; Balcan et al., 2008b; Loosli et al., 2016); we refer the reader in particular to the survey in (Chen et al., 2009a). Among these, the most direct approach treats the data similarity matrix (or transformed dissimilarity matrix) as a kernel Gram matrix, and then uses standard kernel-based methods such as
1

Under review as a conference paper at ICLR 2019

Support Vector Machines (SVM) or kernel ridge regression with this Gram matrix. A key caveat with this approach however is that most similarity (or dissimilarity) measures do not provide a positive-definite (PD) kernel, so that the empirical risk minimization problem is not well-defined, and moreover becomes non-convex (Ong et al., 2004; Lin & Lin, 2003).
A line of work has therefore focused on estimating a positive-definite (PD) Gram matrix that merely approximates the similarity matrix. This could be achieved for instance by clipping, or flipping, or shifting eigenvalues of the similarity matrix (Pekalska et al., 2001), or explicitly learning a PD approximation of the similarity matrix (Chen & Ye, 2008; Chen et al., 2009b). Such modifications of the similarity matrix however often leads to a loss of information; moreover, the enforced PD property is typically guaranteed to hold only on the training data, resulting in an inconsistency between the set of testing and training samples (Chen et al., 2009a) 1.
Another common approach is to select a subset of training samples as a held-out representative set, and use distances or similarities to structured inputs in the set as the feature function (Graepel et al., 1999; Pekalska et al., 2001). As we will show, with proper scaling, this approach can be interpreted as a special instance of our framework. Furthermore, our framework provides a more general and richer family of kernels, many of which significantly outperform the representative-set method in a variety of application domains.
To address the aforementioned issues, in this paper, we propose a novel general framework that constructs a family of PD kernels from a dissimilarity measure on structured inputs. Our approach, which we call D2KE (from Distance to Kernel and Embedding), draws from the literature of Random Features (Rahimi & Recht, 2008), but instead of deriving feature maps from an existing kernel for approximating kernel machines, we build novel kernels from a random feature map specifically designed for a given distance measure. The kernel satisfies the property that functions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are Lipschitz-continuous w.r.t. the given distance measure. We also provide a tractable estimator for a function from this RKHS which enjoys much better generalization properties than nearest-neighbor estimation. Our framework produces a feature embedding and consequently a vector representation of each instance that can be employed by any classification and regression models. In classification experiments in such disparate domains as strings, time series, and histograms (for texts and images), our proposed framework compares favorably to existing distance-based learning methods in terms of both testing accuracy and computational time, especially when the number of data samples is large and/or the size of structured inputs is large.
We highlight our main contributions as follows:
· From the perspective of distance kernel learning, we propose for the first time a methodology that constructs a family of PD kernels via Random Features from a given distance measure for structured inputs, and provide theoretical and empirical justifications for this framework.
· From the perspective of Random Features (RF) methods, we generalize existing Random Features methods applied only to vector input representations to complex structured inputs of variable sizes. To the best of our knowledge, this is the first time that a generic RF method has been used to accelerate kernel machines on structured inputs across a broad range of domains such as time-series, strings, and the histograms.

2R

W

Distance-Based Kernel Learning. Existing approaches either require strict conditions on the distance function (e.g. that the distance be isometric to the square of the Euclidean distance) (Haasdonk & Bahlmann, 2004; Schölkopf, 2001), or construct empirical PD Gram matrices that do not necessarily generalize to the test samples (Pekalska et al., 2001; Pkkalska & Duin, 2005; Pekalska & Duin, 2006; 2008; Duin & Pekalska, 2012). Haasdonk & Bahlmann (2004) and Schölkopf (2001) provide conditions under which one can obtain a PD kernel through simple transformations of the distance measure, but which are not satisfied for many commonly used dissimilarity measures such as Dynamic Time Warping, Hausdorff distance, and Earth Mover's distance (Haasdonk & Bahlmann,
1A generalization error bound was provided for the similarity-as-kernel approach in (Chen et al., 2009a), but only for a positive-definite similarity function.

2

Under review as a conference paper at ICLR 2019

Table 1: Comparison between D2KE and different random features methods.

Methods D2KE Rahimi's RF and its variants

Inputs format Time-series, strings, sets
Vector-form

Distance Metric DTW, EditDist, HD
Euclidean

Random Feature From any distribution
User defined

Build new kernel Yes No

2004). Equivalently, one could also find a Euclidean embedding (also known as dissimilarity representation) approximating the dissimilarity matrix as in Multidimensional Scaling (Pekalska et al., 2001; Pkkalska & Duin, 2005; Pekalska & Duin, 2006; 2008; Duin & Pekalska, 2012) 2. Differently, Loosli et al. (2016) presented a theoretical foundation for an SVM solver in Krein spaces and directly evaluated a solution that uses the original (indefinite) similarity measure.
There are also some specific approaches dedicated to building a PD kernel on some structured inputs such as text and time-series (Collins & Duffy, 2002; Cuturi, 2011), that modify a distance function over sequences to a kernel by replacing the minimization over possible alignments into a summation over all possible alignments. This type of kernel, however, results in a diagonal-dominance problem, where the diagonal entries of the kernel Gram matrix are orders of magnitude larger than the off-diagonal entries, due to the summation over a huge number of alignments with a sample itself.
Random Features Methods. Interest in approximating non-linear kernel machines using randomized feature maps has surged in recent years due to a significant reduction in training and testing times for kernel based learning algorithms (Dai et al., 2014). There are numerous explicit nonlinear random feature maps that have been constructed for various types of kernels, including Gaussian and Laplacian Kernels (Rahimi & Recht, 2008; Wu et al., 2016), intersection kernels (Maji & Berg, 2009), additive kernels Vedaldi & Zisserman (2012), dot product kernels (Kar & Karnick, 2012; Pennington et al., 2015), and semigroup kernels (Mukuta et al., 2018). Among them, the Random Fourier Features (RFF) method, which approximates a Gaussian Kernel function by means of multiplying the input with a Gaussian random matrix, and its fruitful variants have been extensively studied both theoretically and empirically (Sriperumbudur & Szabó, 2015; Felix et al., 2016; Rudi & Rosasco, 2017; Bach, 2017; Choromanski et al., 2018). To accelerate the RFF on input data matrix with high dimensions, a number of methods have been proposed to leverage structured matrices to allow faster matrix computation and less memory consumption (Le et al., 2013; Hamid et al., 2014; Choromanski & Sindhwani, 2016).
However, all the aforementioned RF methods merely consider inputs with vector representations, and compute the RF by a linear transformation that is either a matrix multiplication or an inner product under Euclidean distance metric. In contrast, D2KE takes structured inputs of potentially different sizes and computes the RF with a structured distance metric (typically with dynamic programming or optimal transportation). Another important difference between D2KE and existing RF methods lies in the fact that existing RF work assumes a user-defined kernel and then derives a randomfeature map, while D2KE constructs a new PD kernel through a random feature map and makes it computationally feasible via RF. The table 1 lists the differences between D2KE and existing RF methods.
A very recent piece of work (Wu et al., 2018) has developed a kernel and a specific algorithm for computing embeddings of single-variable real-valued time-series. However, despite promising results, this method cannot be applied on discrete structured inputs such as strings, histograms, and graphs. In contrast, we have an unified framework for various structured inputs beyond the limits of (Wu et al., 2018) and provide a general theoretical analysis w.r.t KNN and other generic distance-based kernel methods.

3P

S

We consider the estimation of a target function f : X  R from a collection of samples {(xi, yi)}in=1, where xi  X is the structured input object, and yi  Y is the output observation associated with the target function f (xi). For instance, in a regression problem, yi  f (xi) + i  R for some random noise i, and in binary classification, we have yi  {0, 1} with P(yi = 1|xi) = f (xi). We are given a
dissimilarity measure d : X × X  R between input objects instead of a feature representation of x.

2A proof of the equivalence between PD of similarity matrix and Euclidean of dissimilarity matrix can be found in (Borg & Groenen, 1997).

3

Under review as a conference paper at ICLR 2019

Note that the size structured inputs xi may vary widely, e.g. strings with variable lengths or graphs with different sizes. For some of the analyses, we require the dissimilarity measure to be a metric as follows.
Assumption 1 (Distance Metric). d : X × X  R is a distance metric, that is, it satisfies (i) d(x1, x2)  0, (ii) d(x1, x2) = 0  x1 = x2, (iii) d(x1, x2) = d(x2, x1), and (iv) d(x1, x2)  d(x1, x3) + d(x3, x2).

3.1 F

C

SC

An ideal feature representation for the learning task is (i) compact and (ii) such that the target function
f (x) is a simple (e.g. linear) function of the resulting representation. Similarly, an ideal dissimilarity measure d(x1, x2) for learning a target function f (x) should satisfy certain properties. On one hand, a small dissimilarity d(x1, x2) between two objects should imply small difference in the function values | f (x1) - f (x2)|. On the other hand, we want a small expected distance among samples, so that the data lies in a compact space of small intrinsic dimension. We next build up some definitions
to formalize these properties.

Assumption 2 (Lipschitz Continuity). For any x1, x2  X, there exists some constant L > 0 such

that

| f (x1) - f (x2)|  L d(x1, x2),

(1)

We would prefer the target function to have a small Lipschitz-continuity constant L with respect to the dissimilarity measure d(., .). Such Lipschitz-continuity alone however might not suffice. For example, one can simply set d(x1, x2) =  for any x1 x2 to satisfy Eq. equation 1. We thus need the following quantity that measures the size of the space implied by a given dissimilarity measure.
Definition 1 (Covering Number). Assuming d is a metric. A -cover of X w.r.t. d(., .) is a set E s.t.
x  X, xi  E, d(x, xi)  .
Then the covering number N(; X, d) is the size of the smallest -cover for Xwith respect to d.

Assuming the input domain X is compact, the covering number N(; X, d) measures its size w.r.t. the distance measure d. We show how the two quantities defined above affect the estimation error of a Nearest-Neighbor Estimator.

3.2 E

D

NN

E

We extend the standard analysis of the estimation error of k-nearest-neighbor from finite-dimensional vector spaces to any structured input space X, with an associated distance measure d, and a finite covering number N(; X, d), by defining the effective dimension as follows.

Assumption 3 (Effective Dimension). Let the effective dimension pX,d > 0 be the minimum p

satisfying

c > 0,  : 0 <  < 1,

N(; X, d)  c

1 

p
.

Here we provide an example of effective dimension in case of measuring the space of Multiset.

Multiset with Hausdorff Distance. A multiset is a set that allows duplicate elements. Consider two multisets x1 = {ui }iM=1, x2 = {vj }jN=1. Let (ui, vj ) be a ground distance that measures the distance between two elements ui, vj  V in a set. The (modified) Hausdorff Distance (Dubuisson
& Jain, 1994) can be defined as d(x1, x2) :=

1 max{
N

N i=1

min (ui, vj ),
j [M]

1 M

M
min (vj, ui)}
j=1 i [N ]

(2)

Let N(; V, ) be the covering number of V under the ground distance . Let X denote the set of

all sets of size bounded by L. By constructing a covering of X containing any set of size less or

equal than L with its elements taken from the covering of V, we have N(; X, d)  N(; V; )L.

Therefore, pX,d  L log N(; V, ). For example, if V := {v  Rp | v 2  1} and  is Euclidean

distance,

we

have

N(; V, )

=

(1

+

2 

)p

and

p X, d



L p.

4

Under review as a conference paper at ICLR 2019

Equipped with the concept of effective dimension, we can obtain the following bound on the estimation error of the k-Nearest-Neighbor estimate of f (x).
Theorem 1. Let V ar(y| f (x))  2, and f^n be the k-Nearest Neighbor estimate of the target function f constructed from a training set of size n. Denote p := pX,d. We have

Ex

f^n(x) - f (x) 2

 2 + cL2 k

k 2/p n

for some constant c > 0. For  > 0, minimizing RHS w.r.t. the parameter k, we have

2

Ex

f^n(x) - f (x) 2

4 2p
 c2 p+2 L 2+p

1 2+p n

(3)

for some constant c2 > 0.

Proof. The proof is almost the same to a standard analysis of k-NN's estimation error in, for example, (Györfi et al., 2006), with the space partition number replaced by the covering number, and dimension replaced by the effective dimension in Assumption 3.

When pX,d is reasonably large, the estimation error of k-NN decreases quite slowly with n. Thus, for the estimation error to be bounded by , requires the number of samples to scale exponentially in pX,d. In the following sections, we develop an estimator f^ based on a RKHS derived from the distance measure, with a considerably better sample complexity for problems with higher effective
dimension.

4F D

KS

I

We aim to address the long-standing problem of how to convert a distance measure into a positivedefinite kernel. Here we introduce a simple but effective approach D2KE that constructs a family of positive-definite kernels from a given distance measure. Given an structured input domain X and a distance measure d(., .), we construct a family of kernels as

 k(x, y) := p()(x)(y)d, where (x) := exp(-d(x, )),

(4)

where    is a random structured object whose elements could be real-valued time-series, strings, and histograms, p() is a distribution over , and (x) is a feature map derived from the distance of x to all random objects   . The kernel is parameterized by both p() and .
Relationship to Distance Substitution Kernel. An insightful interpretation of the kernel in Equation (4) can be obtained by expressing the kernel in Equation (4) as

exp -softminp(){d(x, ) + d(, y)}

(5)

where the soft minimum function, parameterized by p() and , is defined as

softminp()

f

()

:=

-

1 

log



p()e- f ()d.

(6)

Therefore, the kernel k(x, y) can be interpreted as a soft version of the distance substitution kernel

(Haasdonk & Bahlmann, 2004), where instead of substituting d(x, y) into the exponent, it substitutes

a soft version of the form

softminp(){d(x, ) + d(, y)}.

(7)

Note when   , the value of Equation (7) is determined by min d(x, ) + d(, y), which equals d(x, y) if X  , since it cannot be smaller than d(x, y) by the triangle inequality. In other

words, when X  ,

k(x, y)  exp(-d(x, y)) as   .

On the other hand, unlike the distance-substituion kernel, our kernel in Equation (5) is always PD by construction.

5

Under review as a conference paper at ICLR 2019

Algorithm 1 Random Feature Approximation of function in RKHS with the kernel in Equation 4

1: Draw R samples from p() to get {j }Rj=1. 2: Set the R-dimensional feature embedding as

^ j (x)

=

1 

exp(-d(x, j)),  j  [R]

R

3: Solve the following problem for some µ > 0:

w^ := argmin 1 n w RR n i=1

(wT ^(xi),

yi )

+

µ 2

w2

4: Output the estimated function f~R(x) := w^ T ^(x).

Random Feature Approximation. The reader might have noticed that the kernel in Equation (4) cannot be evaluated analytically in general. However, this does not prohibit its use in practice, so long as we can approximate it via Random Features (RF) (Rahimi & Recht, 2008), which in our case is particularly natural as the kernel itself is defined via a random feature map. Thus, our kernel with the RF approximation can not only be used in small problems but also in large-scale settings with a large number of samples, where standard kernel methods with O(n2) complexity are no longer efficient enough and approximation methods, such as Random Features, must be employed (Rahimi & Recht, 2008). Given the RF approximation, one can then directly learn a target function as a linear function of the RF feature map, by minimizing a domain-specific empirical risk. It is worth noting that a recent work (Sinha & Duchi, 2016) that learns to select a set of random features by solving an optimization problem in an supervised setting is orthogonal to our D2KE approach and could be extended to develop a supervised D2KE method. We outline this overall RF based empirical risk minimization for our class of D2KE kernels in Algorithm 1. It is worth pointing out that in line 2 of Algorithm 1 the random feature embeddings are computed by a structured distance measure between the original structured inputs and the generated random structured inputs, followed by the application of the exponent function parameterized by . This is in contrast with traditional RF methods that translate the input data matrix into the embedding matrix via a matrix multiplication with random Gaussian matrix followed by a non-linearity. We will provide a detailed analysis of our estimator in Algorithm 1 in Section 5, and contrast its statistical performance to that of K-nearest-neighbor.

Relationship to Representative-Set Method. A naive choice of p() relates our approach to the

representative-set method (RSM): setting  = X, with p() = p(x). This gives us a kernel Equation

(4) that depends on the data distribution. One can then obtain a Random-Feature approximation to

the

kernel

in

Equation

(4)

by

holding

out

a

part

of

the

training

data

{x^ j

}

R j=1

as

samples

from

p(),

and creating an R-dimensional feature embedding of the form:

^j (x)

:=

1 

exp

-d(x, x^ j) ,

j  [R],

R

(8)

 as in Algorithm 1. This is equivalent to a 1/ R-scaled version of the embedding function in

the representative-set method (or similarity-as-features method) (Graepel et al., 1999; Pekalska

et al., 2001; Pkkalska & Duin, 2005; Pekalska & Duin, 2006; 2008; Chen et al., 2009a; Duin &

Pekalska, 2012), where one computes each sample's similarity to a set of representatives as its feature

representation. However, here by interpreting Equation (8) as a random-feature approximation to the

kernel in Equation (4), we obtain a much nicer generalization error bound even in the case R  .

This is in contrast to the analysis of RSM in (Chen et al., 2009a), where one has to keep the size of the

representative set small (of the order O(n)) in order to have reasonable generalization performance.

Effect of p(). The choice of p() plays an important role in our kernel. Surprisingly, we found that many "close to uniform" choices of p() in a variety of domains give better performance than for instance the choice of the data distribution p() = p(x) (as in the representative-set method). Here are some examples from our experiments: i) In the time-series domain with dissimilarity computed via Dynamic Time Warping (DTW), a distribution p() corresponding to random time series of length uniform in  [2, 10], and with Gaussian-distributed elements, yields much better performance than

6

Under review as a conference paper at ICLR 2019

the Representative-Set Method (RSM); ii) In string classification, with edit distance, a distribution p() corresponding to random strings with elements uniformly drawn from the alphabet  yields much better performance than RSM; iii) When classifying sets of vectors with the Hausdorff distance in Equation (2), a distribution p() corresponding to random sets of size uniform in  [3, 15] with elements drawn uniformly from a unit sphere yields significantly better performance than RSM.
We conjecture two potential reasons for the better performance of the chosen distributions p() in these cases, though a formal theoretical treatment is an interesting subject we defer to future work. Firstly, as p() is synthetic, one can generate unlimited number of random features, which results in a much better approximation to the exact kernel in Equation (4). In contrast, RSM requires held-out samples from the data, which could be quite limited for a small data set. Second, in some cases, even with a small or similar number of random features to RSM, the performance of the selected distribution still leads to significantly better results. For those cases we conjecture that the selected p() generates objects that capture semantic information more relevant to the estimation of f (x), when coupled with our feature map under the dissimilarity measure d(x, ).

5A

In this section, we analyze the proposed framework from the perspectives of error decomposition. Let H be the RKHS corresponding to the kernel in Equation (4). Let

fC := argminE[ ( f (x), y)] s.t. f H  C
f H

(9)

be the population risk minimizer subject to the RKHS norm constraint f H  C. And let

f^n

:=

1 ar gmi n
f H n

n i=1

( f (xi), yi)

s.t.

f

H C

(10)

be the corresponding empirical risk minimizer. In addition, let f~R be the estimated function from our random feature approximation (Algorithm 1). Then denote the population and empirical risks as L( f ) and L^ ( f ) respectively. We have the following risk decomposition L( f~R) - L( f ) =

(L( f~R) - L( f^n)) + (L( f^n) - L( fC )) + (L( fC ) - L( f ))

randomf eature

estimation

a p pr oximat ion

In the following, we will discuss the three terms from the rightmost to the leftmost.

Function Approximation Error. The RKHS implied by the kernel in Equation (4) is

m
 H :=  f f (x) = j k(xj, x), xj  X,  j  [m], m  N ,

 j=1 

 

which is a smaller function space than the space of Lipschitz-continuous function w.r.t. the distance d(x1, x2). As we show, any function f  H is Lipschitz-continous w.r.t. the distance d(., .).

Proposition 1. Let H be the RKHS corresponding to the kernel in Equation (4) derived from some metric d(., .). For any f  H ,

| f (x1) - f (x2)|  Lf d(x1, x2)

where Lf = C.

We refer readers to the detailed proof in Appendix A.1. While any f in the RKHS is Lipschitzcontinuous w.r.t. the given distance d(., .), we are interested in imposing additional smoothness via the RKHS norm constraint f H  C, and by the kernel parameter . The hope is that the best
function fC within this class approximates the true function f well in terms of the approximation error L( fC) - L( f ). The stronger assumption made by the RKHS gives us a qualitatively better estimation error, as discussed below.

Estimation Error. Define D as

D

:=

 j=1

1 1 + /µj

7

Under review as a conference paper at ICLR 2019

where {µj }j=1 is the eigenvalues of the kernel in Equation (5) and  is a tuning parameter. It holds

that

for

any





D/n,

with

probability

at

least

1

-

,

L( f^n)

-

L( fC )



c(log

1 

)2C2



for

some

universal constant c (Zhang, 2005). Here we would like to set  as small as possible (as a function

of n). By using the following kernel-independent bound: D  1/, we have  = 1/ n and thus a

bound on the estimation error

L(

f^n)

-

L(

fC )



c(log

1 

)2C2

1. n

(11)

The estimation error is quite standard for a RKHS estimator. It has a much better dependency w.r.t. n (i.e. n-1/2) compared to that of k-nearest-neighbor method (i.e. n-2/(2+pX,d)) especially for higher
effective dimension. A more careful analysis might lead to tighter bound on D and also a better rate w.r.t. n. However, the analysis of D for our kernel in Equation (4) is much more difficult than that of typical cases as we do not have an analytic form of the kernel.

Random Feature Approximation. Denote L^ (.) as the empirical risk function. The error from RF approximation L( f~R) - L( f^n) can be further decomposed as

(L( f~R) - L^ ( f~R)) + (L^ ( f~R) - L^ ( f^n)) + (L^ ( f^n) - L( f^n))

where the first and third terms can be bounded via the same estimation error bound in Equation (11), as both f~R and f^n have RKHS norm bounded by C. Therefore, in the following, we focus only on the second term of empirical risk. We start by analyzing the approximation error of the kernel R(x1, x2) = k~R(x1, x2) - k(x1, x2) where

k~R(x1, x2) :=

1 R

R j=1

j (x1)j (x2).

(12)

Proposition 2. Let R(x1, x2) = k(x1, x2) - k~(x1, x2), we have uniform convergence of the form

P

max
x1,x2 X

| R (x1,

x2)|

>

2t

2

12 2pX, d e-Rt2/2, t

where pX,d is the effective dimension of X under metric d(., .). In other words, to guarantee |R(x1, x2)|  with probability at least 1 - , it suffices to have

R=

p X, d
2

 log( )

+

11 2 log(  )

.

We refer readers to the detailed proof in Appendix A.2. Proposition 2 gives an approximation

error in terms of kernel evaluation. To get a bound on the empirical risk L^ ( f~R) - L^ ( f^n), consider the optimal solution of the empirical risk minimization. By the Representer theorem we have

f^n(x)

=

1 n

i

i k(xi, x) and

f~R (x)

=

1 n

i ~i k~(xi, x). Therefore, we have the following corollary.

Corollary 1. To guarantee L^ ( f~R) - L^ ( f^n)  , with probability 1 - , it suffices to have

R=

pX,d M2 A2
2

 log( )

+

M2 A2
2

1 log(  )

.

where M is the Lipschitz-continuous constant of the loss function (., y), and A is a bound on  1/n.

We refer readers to the detailed proof in Appendix A.3. For most of loss functions, A and M are typically small constants. Therefore, Corollary 1 states that it suffices to have number of Random Features proportional to the effective dimension O(pX,d/ 2) to achieve an approximation error.
Combining the three error terms, we can show that the proposed framework can achieve -suboptimal performance.
Claim 1. Let f~R be the estimated function from our random feature approximation based ERM estimator in Algorithm 1, and let f  denote the desired target function. Suppose further that for some absolute constants c1, c2 > 0 (up to some logarithmic factor of 1/ and 1/):

8

Under review as a conference paper at ICLR 2019

1. The target function f  lies close to the population risk minimizer fC lying in the RKHS spanned by the D2KE kernel: L( fC) - L( f )  /2.
2. The number of training samples n  c1 C4/ 2. 3. The number of random features R  c2pX,d/ 2.
We then have that: L( f~R) - L( f )  with probability 1 - .

6E

We evaluate the proposed method in four different domains involving time-series, strings, texts, and images. First, we discuss the dissimilarity measures and data characteristics for each set of experiments. Then we introduce comparison among different distance-based methods and report corresponding results.
Distance Measures. We have chosen three well-known dissimilarity measures: 1) Dynamic Time Warping (DTW), for time-series (Berndt & Clifford, 1994); 2) Edit Distance (Levenshtein distance), for strings (Navarro, 2001); 3) Earth Mover's distance (Rubner et al., 2000) for measuring the semantic distance between two Bags of Words (using pretrained word vectors), for representing documents. 4) (Modified) Hausdorff distance (Huttenlocher et al., 1993; Dubuisson & Jain, 1994) for measuring the semantic closeness of two Bags of Visual Words (using SIFT vectors), for representing images. Note that Bag of (Visual) Words in 3) and 4) can also be regarded as a histogram. Since most distance measures are computationally demanding, having quadratic complexity, we adapted or implemented C-MEX programs for them; other codes were written in Matlab.
Datasets. For each domain, we selected 4 datasets for our experiments. For time-series data, all are multivariate time-series and the length of each time-series varies from 2 to 205 observations; three are from the UCI Machine Learning repository (Frank & Asuncion, 2010), the other is generated from the IQ (In-phase and Quadrature components) samples from a wireless line-of-sight communication system from GMU. For string data, the size of alphabet is between 4 and 8 and the length of each string ranges from 34 to 198; two of them are from the UCI Machine Learning repository and the other two from the LibSVM Data Collection (Chang & Lin, 2011). For text data, all are chosen partially overlapped with these in (Kusner et al., 2015). The length of each document varies from 9.9 to 117. For image data, all of datasets were derived from Kaggle; we computed a set of SIFTdescriptors to represent each image and the size of SIFT feature vectors of each image varies from 1 to 914. We divided each dataset into 70/30 train and test subsets (if there was no predefined train/test split). Properties of these datasets are summarized in Table 6 in Appendix B.

Table 2: Classification performance comparison on multi-variate Time-series with variable lengths

Methods Datasets Auslan pentip ActRecog IQ_radio

D2KE Accu Time 92.60 42.4s 99.88 4.5s 64.72 43.4s 86.87 469.3s

KNN Accu Time 70.26 52.1s 98.37 27.3s 53.43 85.5s 60.25 3734s

DSK_RBF Accu Time 92.47 43.4s 98.02 125.4s 55.58 64.9s 77.41 13381s

DSK_ND Accu Time 89.74 44.6s 70.40 126.6s 45.31 68.0s 47.31 12251s

KSVM Accu Time 85.58 95.6s 98.37 125.3s 51.65 75.2s 80.52 10084s

RSM Accu Time 88.96 68.6s 98.48 13.6s 62.44 64.5s 70.84 1275.9s

Table 3: Classification performance comparison on Strings with variable lengths.

Methods Datasets bit-str4
splice dna3 mnist-str8

D2KE Accu Time 90.00 2.3s 90.03 46.9s 89.65 125.1s 98.49 2196s

KNN Accu Time 80.00 3.2s 79.41 164.2s 85.79 859.6s 96.58 13207s

DSK_RBF Accu Time 88.33 3.9s 87.88 204.9s 86.75 1005.2s 97.5 18666s

DSK_ND Accu Time 86.67 3.5s 85.89 208.2s 87.15 1025.2s 92.66 18604s

KSVM Accu Time 83.33 2.8s 67.29 169.9s 46.10 991.8s 96.80 84684s

RSM Accu Time 86.67 2.6s 86.10 87.3s 87.04 213.5s 97.31 4608.6s

Table 4: Classification performance comparison on Bag of Words Vectors for Documents.

Methods Datasets Bbcsport Twitter Recipe Ohsumed

D2KE Accu Time 98.11 90.0s 74.2 15.1s 61.5 257s 64.4 532.3s

KNN Accu Time 95.4 157.2s 71.3 65.4s 57.4 478.1s 55.5 3650.5s

DSK_RBF Accu Time 97.15 514.5s 72.05 73.5s 58.51 1508.5s 59.89 6236.1s

DSK_ND Accu Time 96.52 512.5s 71.93 73.0s 57.53 1502.2s 58.51 6229.2s

KSVM Accu Time 96.27 511.0s 70.60 73.0s 52.63 2364.1s 46.50 6108.1s

RSM Accu Time 97.0 263.2s 71.84 17.9s 58.55 480.2s 59.91 841.5s

9

Under review as a conference paper at ICLR 2019

Table 5: Classification performance comparison on Bag of Visual Words for images.

Methods Datasets flower letters2
decor style

D2KE Accu Time 46.03 22.0s 55.45 64.5s 70.06 150.3s 40.29 20.5s

KNN Accu Time 33.33 96.4s 42.52 90.9s 61.81 1017.3s 36.57 348.0s

DSK_RBF Accu Time 36.51 103.5s 54.55 101.9s 70.83 1225.1s 38.06 450.3s

DSK_ND Accu Time 36.51 102.4s 53.27 99.7s 70.14 1221.9s 30.59 449.2s

KSVM Accu Time 38.10 85.1s 42.45 205.2s 52.78 987.1s 25.74 443.1s

RSM Accu Time 33.33 38.6s 53.34 89.8s 67.25 425.2s 37.68 352.6s

Baselines. We compare D2KE against 5 state-of-the-art baselines, including 1) KNN: a simple yet universal method to apply any distance measure to classification tasks; 2) DSK_RBF (Haasdonk & Bahlmann, 2004): distance substitution kernels, a general framework for kernel construction by substituting a problem specific distance measure in ordinary kernel functions. We use a Gaussian RBF kernel; 3) DSK_ND (Haasdonk & Bahlmann, 2004): another class of distance substitution kernels with negative distance; 4) KSVM (Loosli et al., 2016): learning directly from the similarity (indefinite) matrix followed in the original Krein Space; 5) RSM (Pekalska et al., 2001): building an embedding by computing distances from randomly selected representative samples.
Among these baselines, KNN, DSK_RBF, DSK_ND, and KSVM have quadratic complexity O(N2L2) in both the number of data samples and the length of the sequences, while RSM has computational complexity O(N RL2), linear in the number of data samples but still quadratic in the length of the sequence. These compare to our method, D2KE, which has complexity O(N RL), linear in both the number of data samples and the length of the sequence. For each method, we search for the best parameters on the training set by performing 10-fold cross validation. For our new method D2KE, since we generate random samples from the distribution, we can use as many as needed to achieve performance close to an exact kernel. We report the best number in the range R = [4, 4096] (typically the larger R is, the better the accuracy). We employ a linear SVM implemented using LIBLINEAR (Fan et al., 2008) for all embedding-based methods (RSM and D2KE) and use LIBSVM (Chang & Lin, 2011) for precomputed dissimilairty kernels (DSK_RBF, DSK_ND, and KSVM). More details of experimental setup are provided in Appendix B.
Results. As shown in Tables 2, 3, 4, and 5, D2KE can consistently outperform or match the baseline methods in terms of classification accuracy while requiring far less computation time. There are several observations worth making here. First, D2KE performs much better than KNN, supporting our claim that D2KE can be a strong alternative to KNN across applications. Second, compared to the two distance substitution kernels DSK_RBF and DSK_ND and the KSVM method operating directly on indefinite similarity matrix, our method can achieve much better performance, suggesting that a representation induced from a truly PD kernel makes significantly better use of the data than indefinite kernels. Among all methods, RSM is closest to our method in terms of practical construction of the feature matrix. However, the random objects (time-series, strings, or sets) sampled by D2KE perform significantly better, as we discussed in section 4. More detailed discussions of the experimental results for each domain are given in Appendix C.

7C

FW

In this work, we have proposed a general framework for deriving a positive-definite kernel and a feature embedding function from a given dissimilarity measure between input objects. The framework is especially useful for structured input domains such as sequences, time-series, and sets, where many well-established dissimilarity measures have been developed. Our framework subsumes at least two existing approaches as special or limiting cases, and opens up what we believe will be a useful new direction for creating embeddings of structured objects based on distance to random objects. A promising direction for extension is to develop such distance-based embeddings within a deep architecture to support use of structured inputs in an end-to-end learning system.

10

Under review as a conference paper at ICLR 2019
R
Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions. Journal of Machine Learning Research, 18(21):1­38, 2017.
Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. A theory of learning with similarity functions. Machine Learning, 72(1-2):89­112, 2008a.
Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. A discriminative framework for clustering via similarity functions. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pp. 671­680. ACM, 2008b.
Donald J Berndt and James Clifford. Using dynamic time warping to find patterns in time series. KDD workshop, 10(16):359­370, 1994.
I Borg and P Groenen. Modern multidimensional scaling. series in statistics, 1997.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):27, 2011.
Jianhui Chen and Jieping Ye. Training svm with indefinite kernels. In Proceedings of the 25th international conference on Machine learning, pp. 136­143. ACM, 2008.
Yihua Chen, Eric K Garcia, Maya R Gupta, Ali Rahimi, and Luca Cazzanti. Similarity-based classification: Concepts and algorithms. Journal of Machine Learning Research, 10(Mar):747­ 776, 2009a.
Yihua Chen, Maya R Gupta, and Benjamin Recht. Learning kernels from indefinite similarities. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 145­152. ACM, 2009b.
Krzysztof Choromanski and Vikas Sindhwani. Recycling randomness with structure for sublinear time kernel expansions. arXiv preprint arXiv:1605.09049, 2016.
Krzysztof Choromanski, Mark Rowland, Tamas Sarlos, Vikas Sindhwani, Richard Turner, and Adrian Weller. The geometry of random features. In International Conference on Artificial Intelligence and Statistics, pp. 1­9, 2018.
Michael Collins and Nigel Duffy. Convolution kernels for natural language. In Advances in neural information processing systems, pp. 625­632, 2002.
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based on centered alignment. Journal of Machine Learning Research, 13(Mar):795­828, 2012.
Marco Cuturi. Fast global alignment kernels. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 929­936, 2011.
Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scalable kernel methods via doubly stochastic gradients. In Advances in Neural Information Processing Systems, pp. 3041­3049, 2014.
M-P Dubuisson and Anil K Jain. A modified hausdorff distance for object matching. In Pattern Recognition, 1994. Vol. 1-Conference A: Computer Vision & Image Processing., Proceedings of the 12th IAPR International Conference on, volume 1, pp. 566­568. IEEE, 1994.
Robert PW Duin and Elbieta Pekalska. The dissimilarity space: Bridging structural and statistical pattern recognition. Pattern Recognition Letters, 33(7):826­832, 2012.
X Yu Felix, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice, and Sanjiv Kumar. Orthogonal random features. In Advances in Neural Information Processing Systems, pp. 1975­1983, 2016.
Andrew Frank and Arthur Asuncion. Uci machine learning repository [http://archive. ics. uci. edu/ml]. irvine, ca: University of california. School of information and computer science, 213, 2010.
11

Under review as a conference paper at ICLR 2019
Yue Gao, Meng Wang, Dacheng Tao, Rongrong Ji, and Qionghai Dai. 3-d object retrieval and recognition with hypergraph analysis. IEEE Transactions on Image Processing, 21(9):4290­4303, 2012.
Thore Graepel, Ralf Herbrich, Peter Bollmann-Sdorra, and Klaus Obermayer. Classification on pairwise proximity data. In Advances in neural information processing systems, pp. 438­444, 1999.
László Györfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A distribution-free theory of nonparametric regression. Springer Science & Business Media, 2006.
Bernard Haasdonk and Claus Bahlmann. Learning with distance substitution kernels. In Joint Pattern Recognition Symposium, pp. 220­227. Springer, 2004.
Raffay Hamid, Ying Xiao, Alex Gittens, and Dennis DeCoste. Compact random feature maps. In International Conference on Machine Learning, pp. 19­27, 2014.
Daniel P. Huttenlocher, Gregory A. Klanderman, and William J Rucklidge. Comparing images using the hausdorff distance. IEEE Transactions on pattern analysis and machine intelligence, 15(9): 850­863, 1993.
Purushottam Kar and Harish Karnick. Random feature maps for dot product kernels. In Artificial Intelligence and Statistics, pp. 583­591, 2012.
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In International Conference on Machine Learning, pp. 957­966, 2015.
Quoc Le, Tamás Sarlós, and Alex Smola. Fastfood-approximating kernel expansions in loglinear time. In Proceedings of the international conference on machine learning, volume 85, 2013.
Hsuan-Tien Lin and Chih-Jen Lin. A study on sigmoid kernels for svm and the training of non-psd kernels by smo-type methods. submitted to Neural Computation, 3:1­32, 2003.
Gaelle Loosli, Stephane Canu, and Cheng Soon Ong. Learning svm in krein spaces. IEEE transactions on pattern analysis and machine intelligence, 38(6):1204­1216, 2016.
Subhransu Maji and Alexander C Berg. Max-margin additive classifiers for detection. In Computer Vision, 2009 IEEE 12th International Conference on, pp. 40­47. IEEE, 2009.
Yusuke Mukuta, Yoshitaka Ushiku, and Tatsuya Harada. Alternating circulant random features for semigroup kernels. In AAAI, 2018.
Gonzalo Navarro. A guided tour to approximate string matching. ACM computing surveys (CSUR), 33(1):31­88, 2001.
Cheng Soon Ong, Xavier Mary, Stéphane Canu, and Alexander J Smola. Learning with non-positive kernels. In Proceedings of the twenty-first international conference on Machine learning, pp. 81. ACM, 2004.
Elzbieta Pekalska and Robert PW Duin. Dissimilarity-based classification for vectorial representations. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, volume 3, pp. 137­140. IEEE, 2006.
Elzbieta Pekalska and Robert PW Duin. Beyond traditional kernels: Classification in two dissimilarity-based representation spaces. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(6):729­744, 2008.
Elzbieta Pekalska, Pavel Paclik, and Robert PW Duin. A generalized kernel approach to dissimilaritybased classification. Journal of machine learning research, 2(Dec):175­211, 2001.
Jeffrey Pennington, X Yu Felix, and Sanjiv Kumar. Spherical random features for polynomial kernels. In Advances in neural information processing systems, pp. 1846­1854, 2015.
E Pkkalska and R Duin. The dissimilarity representation for pattern recognition. World Scientific, 2005.
12

Under review as a conference paper at ICLR 2019
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pp. 1177­1184, 2008.
Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover's distance as a metric for image retrieval. International journal of computer vision, 40(2):99­121, 2000.
Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems, pp. 3215­3225, 2017.
Bernhard Schölkopf. The kernel trick for distances. In Advances in neural information processing systems, pp. 301­307, 2001.
Bernhard Scholkopf, Sebastian Mika, Chris JC Burges, Philipp Knirsch, K-R Muller, Gunnar Ratsch, and Alexander J Smola. Input space versus feature space in kernel-based methods. IEEE transactions on neural networks, 10(5):1000­1017, 1999.
Mehmet Sezgin and Bülent Sankur. Survey over image thresholding techniques and quantitative performance evaluation. Journal of Electronic imaging, 13(1):146­166, 2004.
Aman Sinha and John C Duchi. Learning kernels with random features. In Advances in Neural Information Processing Systems, pp. 1298­1306, 2016.
Bharath Sriperumbudur and Zoltán Szabó. Optimal rates for random fourier features. In Advances in Neural Information Processing Systems, pp. 1144­1152, 2015.
Andrea Vedaldi and Andrew Zisserman. Efficient additive kernels via explicit feature maps. IEEE transactions on pattern analysis and machine intelligence, 34(3):480­492, 2012.
Lingfei Wu, Ian EH Yen, Jie Chen, and Rui Yan. Revisiting random binning features: Fast convergence and strong parallelizability. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1265­1274. ACM, 2016.
Lingfei Wu, Ian En-Hsu Yen, Jinfeng Yi, Fangli Xu, Qi Lei, and Michael Witbrock. Random warping series: A random features method for time-series embedding. In International Conference on Artificial Intelligence and Statistics, pp. 793­802, 2018.
Tong Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural Computation, 17(9):2077­2098, 2005.
13

Under review as a conference paper at ICLR 2019

AP T 1 T 2

A.1 P T 1

Proof. Note the function g(t) = exp(-t) is Lipschitz-continuous with Lipschitz constant . Therefore,

| f (x1) - f (x2)| = | f , (x1) - (x2) |  f H (x1) - (x2) H



= fH

p()((x1) - (x2))2d





 fH

p()2|d(x1, ) - d(x2, )|2d





 f H

p()d(x1, x2)2d



  f H d(x1, x2)  Cd(x1, x2)

A.2 P T 2

Proof. Our goal is to bound the magnitude of R(x1, x2) = k~R(x1, x2) - k(x1, x2). Since E[R(x1, x2)] = 0 and |R(x1, x2)|  1, from Hoefding's inequality, we have
P {|R(x1, x2)|  t}  2 exp(-Rt2/2)

a given input pair (x1, x2). To get a unim bound that holds (x1, x2)  X × X, we find an -covering E of X w.r.t. d(., .) of size N( , X, d). Applying union bound over the -covering E for x1 and x2,
we have

P

max
x1  E,x2  E

| R (x1,

x2)|

>

t

 2|E |2 exp(-Rt2/2).

(13)

Then by the definition of E we have |d(x1, ) - d(x1, )|  d(x1, x1)  . Together with the fact that exp(-t) is Lipschitz-continuous with parameter  for t  0, we have

and thus

|(x1) - (x1)|   |k~R(x1, x2) - k~R(x1, x2)|  3 ,

|k(x1, x2) - k(x1, x2)|  3 for  chosen to be  1. This gives us

|R(x1, x2) - R(x1, x2)|  6 Combining equation 13 and equation 14, we have

(14)

P

max
x1  E,x2  E

| R (x1,

x2)|

>

t

+

6

2

2

2pX, d
exp(-Rt2/2).

(15)

Choosing = t/6 yields the result.

14

Under review as a conference paper at ICLR 2019

A.3 P

C

1

Proof. First of all, we have

1n n i=1

(1 n

n
~ j k~(xj, xi), yi)
j=1

1n 
n i=1

(1 n

n j=1

j k~(xj, xi), yi)

by the optimality of {~ j }nj=1 w.r.t. the objective using the approximate kernel. Then we have

L^ ( f~R) - L^ ( f^n)

1n 
n i=1

(1 n

n
j k~(xj, xi), yi) -
j=1

(1 n

n
j k(xj, xi), yi)
j=1

M

1 n

max
x1,x2 X

|

k~

(x1,

x2)

-

k

(x1,

x2)|

 M A max |k~(x1, x2) - k(x1, x2)|
x1,x2 X
where A is a bound on  1/n. Therefore to guarantee L^ ( f~R) - L^ ( f^n) 
we would need maxi, j [n] |R(x1, x2)|  ^ := /M A. Then applying Theorem 2 leads to the result.

BG

E

S

Table 6: Properties of the datasets. TS, Str, Text, and Img stand for Time-Series, String, text, and Image respectively. Var stands for the number of variables for time-series, word embeddings, and image SIFT-descriptors while Alpb stands for the size of the alphabet for strings. Note that all data samples have quite different range of lengths in different datasets.

Domain TS TS TS TS Str Str Str Str Text Text Text Text Img Img Img Img

Name Auslan pentip ActRecog IQ_radio bit-str4 splice dna3 mnist-str8 Bbcsport Twitter Recipe Ohsumed flower decor style letters2

Var/Alpb 22 3 3 4 4 4 4 8 300 300 300 300 128 128 128 128

Classes 95 20 7 5 10 3 2 10 5 3 15 10 10 7 7 33

Train 1795 2000 1837 6715 140 2233 3620 60000 517 2176 3059 3999 147 340 625 3277

Test 770 858 788 6715 60 957 1555 10000 220 932 1311 5153 63 144 268 1404

length 45-136 109-205 2-151
512 44/158
60 147 17/99 43-469 1-26 1-340 11-166 66/429 35/914 6/530 1/22

General Setup. For each method, we search for the best parameters on the training set by performing 10-fold cross validation. Following (Haasdonk & Bahlmann, 2004), we use an exact RBF kernel for DSK_RBF while choosing squared distance for DSK_ND. We use the Matlab implementation provided by Loosli et al. (2016) to run experiments for KSVM. Similarly, we adopted a simple method ­ random selection ­ to obtain R = [4, 512] data samples as the representative set for RSM (Pekalska et al., 2001). For our new method D2KE, since we generate random samples from the distribution, we can use as many as needed to achieve performance close to an exact kernel. We

15

Under review as a conference paper at ICLR 2019

report the best number in the range R = [4, 4096] (typically the larger R is, the better the accuracy). We employ a linear SVM implemented using LIBLINEAR (Fan et al., 2008) for all embedding-based methods (RSM, and D2KE) and use LIBSVM (Chang & Lin, 2011) for precomputed dissimilairty kernels (DSK_RBF, DSK_ND, and KSVM).
All datasets are collected from popular public websites for Machine Learning and Data Science research, including the UCI Machine Learning repository (Frank & Asuncion, 2010), the LibSVM Data Collection (Chang & Lin, 2011), and the Kaggle Datasets, except one time-series dataset IQ that is shared from researchers from George Mason University. Table 6 lists the detailed properties of the datasets from four different domains. All computations were carried out on a DELL dual-socket system with Intel Xeon processors at 2.93GHz for a total of 16 cores and 250 GB of memory, running the SUSE Linux operating system. To accelerate the computation of all methods, we used multithreading with 12 threads total for various distance computations in all experiments.

CD

E

R

T -S , S

,I

C.1 R

-

Setup. For time-series data, we employed the most successful distance measure - DTW - for all methods. For all datasets, a Gaussian distribution was found to be applicable, parameterized by its bandwidth . The best values for  and for the length of random time series were searched in the ranges [1e-3 1e3] and [2 50], respectively.
Results. As shown in Table 2, D2KE can consistently outperform or match all other baselines in terms of classification accuracy while requiring far less computation time for multivariate time-series. The first interesting observation is that our method performs substantially better than KNN, often by a large margin, i.e., D2KE achieves 26.62% higher performance than KNN on IQ_radio. This is because KNN is sensitive to the data noise common in real-world applications like IQ_radio, and has notoriously poor performance for high-dimensional data sets like Auslan. Moreover, compared to the two distance substitution kernels DSK_RBF and DSK_ND, and KSVM operating directly on indefinite similarity matrix, our method can achieve much better performance, suggesting that a representation induced from a truly p.d. kernel makes significantly better use of the data than indefinite kernels. Among all methods, RSM is closest to our method in terms of practical construction of the feature matrix. However, the random time series sampled by D2KE performs significantly better, as we discussed in section 4. First, RSM simply chooses a subset of the original data points and computes the distances between the whole dataset and this representative set; this may suffer significantly from noise or redundant information in the time-series. In contrast, our method samples a short random sequence that could both denoise and find the patterns in the data. Second, the number of data points that can be sampled is limited by the total size of the data while the number of possible random sequences drawn from the distribution is unlimited, making the feature space much more abundant. Third, RSM may incur significant computational cost for long time-series, due to its quadratic complexity in length.

C.2 R
Setup. For string data, there are various well-known edit distances. Here, we choose Levenshtein distance as our distance measure since it can capture global alignments of the underlying strings. We first compute the alphabet from the original data and then uniformly sample characters from this alphabet to generate random strings. We search for the best parameters for  in the range [1e-5 1], and for the length of random strings in the range [2 50], respectively.
Results. As shown in Table 3, D2KE consistently performs better than or similarly to other distancebased baselines. Unlike the previous experiments where DTW is not a distance metric, Levenshtein distance is indeed a distance metric; this helps improve the performance of our baselines. However, D2KE still offers a clear advantage over baseline. It is interesting to note that the performance of DSK_RBF is quite close to our method's, which may be due to DKS_RBF with Levenshtein distance producing a c.p.d. kernel which can essentially be converted into a p.d. kernel. Notice that on relatively large datasets, our method, D2KE, can achieve better performance, and often with far less computation than other baselines with quadratic complexity in both number and length of data samples. For instance, on mnist-str8 D2KE obtains higher accuracy with an order of magnitude less

16

Under review as a conference paper at ICLR 2019

runtime compared to DSK_RBF and DSK_ND, and two orders of magnitude less than KSVM, due to higher computational costs both for kernel matrix construction and for eigendecomposition.

C.3 R

WV

Setup. For text data, following (Kusner et al., 2015) we use the earth mover's distance as our distance measure between two documents, since this distance has recently demonstrated a strong performance when combining with KNN for document classifications. We first compute the Bag of Words for each document and represent each document as a histogram of word vectors, where google pretrained word vectors with dimension size 300 is used. We generate random documents consisting of each random word vectors uniformly sampled from the unit sphere of the embedding vector space R300. We search for the best parameters for  in the range [1e-2 1e1], and for length of random document in the range [3 21].
Results. As shown in Table 4, D2KE outperforms other baselines on all four datasets. First of all, all distance based kernel methods perform better than KNN, illustrating the effectiveness of SVM over KNN on text data. Interestingly, D2KE also performs significantly better than other baselines by a notiably margin, in large part because document classification mainly associates with "topic" learning where our random documents of short length may fit this task particularly well. For the datasets with large number of documents and longer length of document, D2KE achieves about one order of magnitude speedup compared with other exact kernel/similarity methods, thanks to the use of random features in D2KE.

C.4 R

SIFT-

Setup. For image data, following (Pekalska et al., 2001; Haasdonk & Bahlmann, 2004) we use the modified Hausdorff distance (MHD) (Dubuisson & Jain, 1994) as our distance measure between images, since this distance has shown excellent performance in the literature (Sezgin & Sankur, 2004; Gao et al., 2012). We first applied the open-source OpenCV library to generate a sequence of SIFT-descriptors with dimension 128, then MHD to compute the distance between sets of SIFTdescriptors. We generate random images of each SIFT-descriptor uniformly sampled from the unit sphere of the embedding vector space R128. We search for the best parameters for  in the range [1e-3 1e1], and for length of random SIFT-descriptor sequence in the range [3 15].
Results. As shown in Table 5, D2KE performance outperforms or matches other baselines in all cases. First, D2KE performs best in three cases while DSK_RBF is the best on dataset decor. This may be because the underlying SIFT features are not good enough and thus random features is not effective to find the good patterns quickly in images. Nevertheless, the quadratic complexity of DSK_RBF, DSK_ND, and KSVM in terms of both the number of images and the length of SIFT descriptor sequences makes it hard to scale to large data. Interestingly, D2KE still performs much better than KNN and RSM, which again supports our claim that D2KE can be a strong alternative to KNN and RSM across applications.

17

