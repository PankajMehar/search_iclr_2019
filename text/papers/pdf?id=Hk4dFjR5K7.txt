Under review as a conference paper at ICLR 2019
ADEF: AN ITERATIVE ALGORITHM TO CONSTRUCT ADVERSARIAL DEFORMATIONS
Anonymous authors Paper under double-blind review
ABSTRACT
While deep neural networks have proven to be a powerful tool for many recognition and classification tasks, their stability properties are still not well understood. In the past, image classifiers have been shown to be vulnerable to so-called adversarial attacks, which are created by additively perturbing the correctly classified image. In this paper, we propose the ADef algorithm to construct a different kind of adversarial attack created by iteratively applying small deformations to the image, found through a gradient descent step. We demonstrate our results on MNIST with convolutional neural networks and on ImageNet with Inception-v3 and ResNet-101.
1 INTRODUCTION
In a first observation in Szegedy et al. (2013) it was found that deep neural networks exhibit unstable behavior to small perturbations in the input. For the task of image classification this means that two visually indistinguishable images may have very different outputs, resulting in one of them being misclassified even if the other one is correctly classified with high confidence. Since then, a lot of research has been done to investigate this issue through the construction of adversarial examples: given a correctly classified image x, we look for an image y which is visually indistinguishable from x but is misclassified by the network. Typically, the image y is constructed as y = x + r, where r is an adversarial perturbation that is supposed to be small in a suitable sense (normally, with respect to an p norm). Several algorithms have been developed to construct adversarial perturbations, see Goodfellow et al. (2014); Moosavi Dezfooli et al. (2016); Kurakin et al. (2017b); Madry et al. (2018); Carlini & Wagner (2017b) and the review paper Akhtar & Mian (2018).
Even though such pathological cases are very unlikely to occur in practice, their existence is relevant since malicious attackers may exploit this drawback to fool classifiers or other automatic systems. Further, adversarial perturbations may be constructed in a black-box setting (i.e., without knowing the architecture of the DNN but only its outputs) (Papernot et al., 2017; Moosavi-Dezfooli et al., 2017) and also in the physical world (Kurakin et al., 2017b; Athalye & Sutskever, 2017; Brown et al., 2017; Sharif et al., 2016). This has motivated the investigation of defenses, i.e., how to make the network invulnerable to such attacks, see Kurakin et al. (2017a); Carlini & Wagner (2017a); Madry et al. (2018); Trame`r et al. (2018); Wong & Kolter (2018); Raghunathan et al. (2018); Athalye et al. (2018); Kannan et al. (2018). In most cases, adversarial examples are artificially created and then used to retrain the network, which becomes more stable under these types of perturbations.
Most of the work on the construction of adversarial examples and on the design of defense strategies has been conducted in the context of small perturbations r measured in the  norm. However, this is not necessarily a good measure of image similarity: e.g., for two translated images x and y, the norm of x-y is not small in general, even though x and y will look indistinguishable if the translation is small. Several papers have investigated the construction of adversarial perturbations not designed for norm proximity (Rozsa et al., 2016; Sharif et al., 2016; Brown et al., 2017; Engstrom et al., 2017; Xiao et al., 2018).
In this work, we build up on these ideas and investigate the construction of adversarial deformations. In other words, the misclassified image y is not constructed as an additive perturbation y = x + r, but as a deformation y = x  (id +  ), where  is a vector field defining the transformation. In this case, the similarity is not measured through a norm of y - x, but instead through a norm of  , which quantifies the deformation between y and x.
1

Under review as a conference paper at ICLR 2019

We develop an efficient algorithm for the construction of adversarial deformations, which we call ADef. It is based on the main ideas of DeepFool (Moosavi Dezfooli et al., 2016), and iteratively constructs the smallest deformation to misclassify the image. We test the procedure on MNIST (LeCun) (with convolutional neural networks) and on ImageNet (Russakovsky et al., 2015) (with Inception-v3 (Szegedy et al., 2016) and ResNet-101 (He et al., 2016)). The results show that ADef can succesfully fool the classifiers in the vast majority of cases (around 99%) by using very small and imperceptible deformations. We also test our adversarial attacks on adversarially trained networks for MNIST.
The results of this work have initially appeared in the master's thesis Anonymous (2017), to which we refer for additional details on the mathematical aspects of this construction1. While writing this paper, we have come across Xiao et al. (2018), in which a similar problem is considered and solved with a different algorithm. Whereas in Xiao et al. (2018) the authors use a second order solver to find a deforming vector field, we show how a first order method can be formulated efficiently and justify a smoothing operation, independent of the optimization step. We report, for the first time, success rates for adversarial attacks with deformations on ImageNet. The topic of deformations has also come up in Jaderberg et al. (2015), in which the authors introduce a class of learnable modules that deform inputs in order to increase the performance of existing DNNs.

2 ADVERSARIAL DEFORMATIONS

2.1 ADVERSARIAL PERTURBATIONS

Let K be a classifier of images consisting of P pixels into L  2 categories, i.e. a function from the

space of images X = RcP , where c = 1 (for grayscale images) or c = 3 (for color images), and into the set of labels L = {1, . . . , L}. Suppose x  X is an image that is correctly classified by K and

suppose y  X is another image that is imperceptible from x and such that K(y) = K(x), then y is

said to be an adversarial example. The meaning of imperceptibility varies, but generally, proximity

in p-norm (with 1  p  ) is considered to be a sufficient substitute. Thus, an adversarial

perturbation for an image x  X is a vector r  X such that K(x + r) = K(x) and r p is small,

where


cP

1/p

r p =  |rj|p
j=1

if 1  p < , and

r



=

max
j=1,...,cP

|rj| .

(1)

Given such a classifier K and an image x, an adversary may attempt to find an adversarial example
y by minimizing x - y p subject to K(y) = K(x), or even subject to K(y) = k for some target
label k = K(x). Different methods for finding minimal adversarial perturbations have been proposed, most notably FGSM (Goodfellow et al., 2014) and PGD (Madry et al., 2018) for , and the DeepFool algorithm (Moosavi Dezfooli et al., 2016) for general p-norms.

2.2 DEFORMATIONS
Instead of constructing adversarial perturbations, we intend to fool the classifier by small deformations of correctly classified images. Our procedure is in the spirit of the DeepFool algorithm. Before we explain it, let us first clarify what we mean by a deformation of an image. The discussion is at first more intuitive if we model images as functions  : [0, 1]2  Rc (with c = 1 or c = 3) instead of discrete vectors x in RcP . In this setting, perturbing an image  : [0, 1]2  Rc corresponds to adding to it another function  : [0, 1]2  Rc with a small Lp-norm.
While any transformation of an image  can be written as a perturbation  + , we shall restrict ourselves to a particular class of transformations. A deformation with respect to a vector field  : [0, 1]2  R2 is a transformation of the form    , where for any image  : [0, 1]2  Rc, the image  : [0, 1]2  Rc is defined by
 (u) =  (u +  (u)) for all u  [0, 1]2,
extending  by zero outside of [0, 1]2. Deformations capture many natural image transformations. For example, a translation of the image  by a vector v  R2 is a deformation with respect to the
1Appendix C contains an anonymized version of the relevant part of Anonymous (2017).

2

Under review as a conference paper at ICLR 2019

Original

Translation by (-2, 1)

Rotation by 10

Deformation w.r.t. 

: 1.00

: 0.98

: 1.00

T : 2.24

T : 3.33

T : 1.63

Figure 1: First row: The original 28 × 28 pixel image from the MNIST database, and the same image translated by (-2, 1), rotated by an angle of 10, and deformed w.r.t. an arbitrary smooth vector field  . The -norm of the corresponding perturbation is shown under each deformed image. The pixel values range from 0 (white) to 1 (black), so the deformed images all lie far from the original image in the -norm. Second row: The vector fields corresponding to the above deformations and their T -norms (cf. equation (3)).

constant vector field  = v. If v is small, the images  and v may look similar, but the corresponding perturbation  = v -  may be arbitrarily large in the aforementioned Lp-norms. Figure 1 shows three minor deformations, all of which yield large L-norms.
In the discrete setting, deformations are implemented as follows. We consider square images of W × W pixels and define the space of images to be X = RW ×W c. A discrete vector field is a function  : {1, . . . , W }2  R2. In what follows we will only consider the set T of vector fields that do not move points on the grid {1, . . . , W }2 outside of [1, W ]2. More precisely,
T :=  : {1, . . . , W }2  R2 |  (s, t) + (s, t)  [1, W ]2 for all s, t  {1, . . . , W } .

An image x  X can be viewed as the collection of values of a function  : [0, 1]2  Rc on a regular
grid {1/(W +1), . . . , W/(W +1)}2  [0, 1]2, i.e. xs,t = (s/(W +1), t/(W +1)) for s, t = 1, . . . , W . Such
a function  can be computed by interpolating from x. Thus, the deformation of an image x with respect to the discrete vector field  can be defined as the discrete deformed image x in X by

xs,t = 

(s, t) +  (s, t) W +1

,

s, t  {1, . . . , W }.

(2)

It is not straightforward to measure the size of a deformation such that it captures the visual difference between the original image x and its deformed counterpart x . We will use the size of the
corresponding vector field,  , in the norm defined by



T

=

max
s,t=1,...,W

 (s, t) 2

(3)

as a proxy. The p-norms defined in (1), adapted to vector fields, can be used as well. (We remark,
however, that none of these norms define a distance between x and x , since two vector fields ,   T with  T =  T may produce the same deformed image x = x.)

3

Under review as a conference paper at ICLR 2019

2.3 THE ALGORITHM ADEF

We will now describe our procedure for finding deformations that will lead a classifier to yield an output different from the original label.
Let F = (F1, . . . , FL) : X  RL be the underlying model for the classifier K, such that

K(x) = arg max Fk(x).
k=1,...,L

Let x  X be the image of interest and fix  : [0, 1]2  Rc obtained by interpolation from x. Let l = K(x) denote the true label of x, let k  L be a target label and set f = Fk - Fl. We assume that x does not lie on a decision boundary, so that we have f (x) < 0.
We define the function g : T  R,   f (x ) and note that g(0) = f (x0) = f (x) < 0. Our goal is to find a small vector field   T such that g( ) = f (x )  0. We can use a linear approximation of g around the zero vector field as a guide:

g( )  g(0) + (D0g) 

(4)

for small enough   T and D0g : T  R the derivative of g at  = 0. Hence, if  is a vector field

such that

(D0g)  = -g(0)

(5)

xandtohaTveisesimthaelrl,lathbeenl

the classifier l or k. This

K is

has approximately equal confidence for the a scalar equation with unknown in T , and

deformed image so has infinitely

many solutions. In order to select  with small norm, we solve it in the least-squares sense.

In view of (2), we have

x 

|

=0(s,

t)

=

1 W +1



(s,t) W +1

 Rc×2. Thus, by applying the chain

rule to g( ) = f (x ), we obtain that its derivative at  = 0 can, with a slight abuse of notation, be

identified with the vector field

D0g(s, t)

=

W

1 +

1

f (x)

s,t

(s, t) W +1

,

(6)

where f (x) s,t  R1×c is the derivative of f in x calculated at (s, t). With this, (D0g)  stands

for

W s,t=1

D0g(s,

t)

·



(s,

t),

and

the

solution

to

(5)

in

the

least-square

sense

is

given

by

 =-

f (x)

W s,t=1

|D0

g(s,

t)|2

D0g.

Finally, we define the deformed image x  X according to (2).

(7)

One might like to impose some degree of smoothness on the deforming vector field. In fact, it suffices to search in the range of a smoothing operator S : T  T . However, this essentially amounts to applying S to the solution from the larger search space T . Let  = S(D0g) = (D0g), where S denotes the componentwise application of a two-dimensional Gaussian filter  (of any
standard deviation). Then the vector field

~ = -

f (x)

W s,t=1

|(s,

t)|2

S

=

-

f (x)

W s,t=1

|(s,

t)|2

S 2 (D0 g)

also satisfies (5), since S is self-adjoint. We can hence replace  by ~ to obtain a smooth deformation of the image x.

We iterate the deformation process until the deformed image is misclassified. More explicitly, let x(0) = x and for n  1 let  (n) be given by (7) for x(n-1). Then we can define the iteration as x(n) = x(n-1)  (id +  (n)). The algorithm terminates and outputs an adversarial example y = x(n)
if K(x(n)) = l. The iteration also terminates if x(n) lies on a decision boundary of K, in which case we propose to introduce an overshoot factor 1 +  on the total deforming vector field. Provided that the number of iterations is moderate, the total vector field can be well approximated by   =  (1) +· · ·+ (n) and the process can be altered to output the deformed image y = x(id+(1+) )
instead.

4

Under review as a conference paper at ICLR 2019

The target label k may be chosen in each iteration to minimize the vector field to obtain a better approximation in the linearization (4). More precisely, for a candidate set of labels k1, . . . , km, we compute the corresponding vectors fields 1, . . . , m and select
k = arg min j T .
j=1,...,m
The candidate set consists of the labels corresponding to the indices of the m smallest entries of F - Fl, in absolute value.

Algorithm ADef

Input: Classification model F , image x, correct label l, candidate labels k1, . . . , km

Output: Deformed image y

Initialize y  x

while K(y) = l do

for j = 1, . . . , m do

j  S

c i=1

(Fkj )i - (Fl)i

j



-

Fkj

(y)-Fl

j

2 2

(y)

S

j

· yi

end for

i  arg minj=1,...,m j T

y  y  (id + i)

end while

return y

By equation (6), given that f is moderate, the deforming vector field takes small values wherever  has a small derivative. This means that the vector field will be concentrated on the edges in the image x (see e.g. the first row of figure 2). Further, note that the result of a deformation is always a valid image in the sense that it does not violate the pixel value bounds. This is not guaranteed for
the perturbations computed with DeepFool.

3 EXPERIMENTS
3.1 SETUP
We evaluate the performance of ADef by applying the algorithm to classifiers trained on the MNIST (LeCun) and ImageNet (Russakovsky et al., 2015) datasets. Below, we briefly describe the setup of the experiments and in tables 1 and 2 we summarize their results.
MNIST: We train two convolutional neural networks based on architectures that appear in Madry et al. (2018) and Trame`r et al. (2018) respectively. The network MNIST-A consists of two convolutional layers of sizes 32 × 5 × 5 and 64 × 5 × 5, each followed by 2 × 2 max-pooling and a rectifier activation function, a fully connected layer into dimension 1024 with a rectifier activation function, and a final linear layer with output dimension 10. The network MNIST-B consists of two convolutional layers of sizes 128 × 3 × 3 and 64 × 3 × 3 with a rectifier activation function, a fully connected layer into dimension 128 with a rectifier activation function, and a final linear layer with output dimension 10. During training, the latter convolutional layer and the former fully connected layer of MNIST-B are subject to dropout of drop probabilities 1/4 and 1/2. We use ADef to produce adversarial deformations of the images in the test set. The algorithm is configured to pursue any label different from the correct label (all incorrect labels are candidate labels). It performs smoothing by a Gaussian filter of standard deviation 1/2, and it overshoots by  = 2/10 whenever it converges to a decision boundary.
ImageNet: We apply ADef to pretrained Inception-v3 (Szegedy et al., 2016) and ResNet-101 (He et al., 2016) models to generate adversarial deformations for the images in the ILSVRC2012 validation set. The images are preprocessed by first scaling so that the smaller axis has 299 pixels for the Inception model and 224 pixels for ResNet, and then they are center-cropped to a square image. The algorithm is set to focus only on the label of second highest probability. It employs a Gaussian filter of standard deviation 1 and an overshoot factor  = 1/10.

5

Under review as a conference paper at ICLR 2019

Table 1: The results of applying ADef to the images in the MNIST test set and the ILSVRC2012
validation set. The accuracy of the Inception and ResNet models is defined as the top-1 accuracy on
the center-cropped and resized images. The success rate of ADef is shown as a percentage of the correctly classified inputs. The pixel range is scaled to [0, 1], so the perturbation r = y - x, where x is the input and y the output of ADef, has values in [-1, 1]. The averages in the three last columns are computed over the set of images on which ADef is successful.

Model
MNIST-A MNIST-B Inception-v3 ResNet-101

Accuracy
98.99% 98.91% 77.56% 76.97%

ADef success
99.85% 99.51% 98.94% 99.78%

Avg.   T
1.1950 1.0841 0.5984 0.5561

Avg. r 
0.7455 0.7654 0.2039 0.1882

Avg. # iterations
7.002 4.422 4.050 4.176

Original: orangutan

Deformed: chimpanzee

Vector field

Perturbation

Close-up

Original: orangutan

Deformed: chimpanzee

T : 0.395
Vector field

: 0.121
Perturbation

Close-up

T : 1.273

: 0.276

Figure 2: Sample deformations for the Inception-v3 model. The vector fields and perturbations have been amplified for visualization. First row: An image from the ILSVRC2012 validation set, the output of ADef with a Gaussian filter of standard deviation 1, the corresponding vector field and perturbation. The rightmost image is a close-up of the vector field around the nose of the ape. Second row: A larger deformation of the same image, obtained by using a wider Gaussian filter (standard deviation 6) for smoothing.

We only consider inputs that are correctly classified by the model in question, and, since   =  (1)+· · ·+ (n) approximates the total deforming vector field, we declare ADef to be successful if its output is misclassified and   T  , where we choose  = 3. Observe that, by (3), a deformation with respect to a vector field  does not displace any pixel further away from its original position than  T . Hence, for high resolution images, the choice  = 3 indeed produces small deformations if the vector fields are smooth. In appendix A, we illustrate how the success rate of ADef depends on the choice of .
When searching for an adversarial example, one usually searches for a perturbation with -norm smaller than some small number  > 0. Common choices of  range from 1/10 to 3/10 for MNIST classifiers (Goodfellow et al., 2014; Madry et al., 2018; Wong & Kolter, 2018; Trame`r et al., 2018; Kannan et al., 2018) and 2/255 to 16/255 for ImageNet classifiers (Goodfellow et al., 2014; Kurakin et al., 2017a; Trame`r et al., 2018; Kannan et al., 2018). Table 1 shows that on average, the perturbations obtained by ADef are quite large compared to those constraints. However, as can be seen in figure 2, the relatively high resolution images of the ImageNet dataset can be deformed into adversarial examples that, while corresponding to large perturbations, are not visibly different from the original images. In appendix B, we give more examples of adversarially deformed images.
6

Under review as a conference paper at ICLR 2019

Table 2: Success rates for PGD and ADef attacks on adversarially trained networks.

Model MNIST-A MNIST-B

Adv. training
PGD ADef PGD ADef

Accuracy
98.36% 98.95% 98.74% 98.79%

PGD success
5.81% 100.00%
5.84% 100.00%

ADef success
6.67% 54.16% 20.35% 45.07%

Original Target: 0 Target: 1 Target: 2 Target: 3 Target: 4 Target: 5 Target: 6 Target: 7 Target: 8

: 0.96

: 0.96

: 0.98

: 0.90

: 0.55

: 0.88

: 1.00

: 0.74

: 0.96

T : 2.37

T : 2.53

T : 1.39

T : 1.34

T : 0.57

T : 1.43

T : 2.30

T : 1.15

T : 2.22

Figure 3: Targeted ADef against MNIST-A. First row: The original image and deformed images produced by restricting ADef to the target labels 0 to 8. The -norms of the corresponding perturbations are shown under the deformed images. Second row: The vector fields corresponding to the deformations and their T -norms.
3.2 ADVERSARIAL TRAINING
In addition to training MNIST-A and MNIST-B on the original MNIST data, we train independent copies of the networks using the adversarial training procedure described by Madry et al. (2018). That is, before each step of the training process, the input images are adversarially perturbed using the PGD algorithm. This manner of training provides increased robustness against adversarial perturbations of low -norm. Moreover, we train networks using ADef instead of PGD as an adversary. In table 2 we show the results of attacking these adversarially trained networks, using ADef on the one hand, and PGD on the other. We use the same configuration for ADef as above, and for PGD we use 40 iterations, step size 1/100 and 3/10 as the maximum -norm of the perturbation. Interestingly, using these configurations, the networks trained against PGD attacks are more resistant to adversarial deformations than those trained against ADef.
3.3 TARGETED ATTACKS
ADef can also be used for targeted adversarial attacks, by restricting the deformed image to have a particular target label instead of any label which yields the optimal deformation. Figure 3 demonstrates the effect of choosing different target labels for a given MNIST image, and figure 4 shows the result of targeting the label of lowest probability for an image from the ImageNet dataset.
4 CONCLUSION
In this work, we proposed a new efficient algorithm, ADef, to construct a new type of adversarial attacks for DNN image classifiers. The procedure is iterative and in each iteration takes a gradient descent step to deform the previous iterate in order to push to a decision boundary.
We demonstrated that with almost imperceptible deformations, state-of-the art classifiers can be fooled to misclassify with a high success rate of ADef. This suggests that networks are vulnerable to different types of attacks and that simply training the network on a specific class of adversarial examples might not form a sufficient defense strategy. Given this vulnerability of neural networks to deformations, we wish to study in future work how ADef can help for designing possible defense strategies. Furthermore, we also showed initial results on fooling adversarially trained networks.
7

Under review as a conference paper at ICLR 2019

Original: fig

Deformed: grocery store

Vector field

Perturbation

Original: fig

Deformed: gazelle

T : 0.897
Vector field

: 0.301
Perturbation

T : 2.599

: 0.595

Figure 4: Untargeted vs. targeted attack on the ResNet-101 model. An image from the ILSVRC2012 validation set deformed to the labels of second highest (first row) and lowest (second row) probabilities (out of 1,000) for the original image. The vector fields and perturbations have been amplified for visualization.
Remarkably, PGD trained networks on MNIST are more resistant to adversarial deformations than ADef trained networks. However, for this result to be more conclusive, similar tests on ImageNet will have to be conducted. We wish to study this in future work.
REFERENCES
N. Akhtar and A. Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. IEEE Access, 6:14410­14430, 2018. doi: 10.1109/ACCESS.2018.2807385.
Author Anonymous. Adversarial perturbations and deformations for convolutional neural networks, 2017.
Anish Athalye and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Tom B Brown, Dandelion Mane´, Aurko Roy, Mart´in Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 3­14. ACM, 2017a.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (SP), pp. 39­57. IEEE, 2017b.
Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. arXiv preprint arXiv:1712.02779, 2017.
8

Under review as a conference paper at ICLR 2019
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pp. 770­778, 2016.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu. Spatial transformer networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2017­2025. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/ 5854-spatial-transformer-networks.pdf.
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In International Conference on Learning Representations, 2017a.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In International Conference on Learning Representations, 2017b.
Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/ mnist/.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
S. M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 86­94, July 2017. doi: 10.1109/CVPR.2017.17.
Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a simple and accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506­519. ACM, 2017.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=Bys4ob-Rb.
Andras Rozsa, Ethan M Rudd, and Terrance E Boult. Adversarial diversity and hard positive generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 25­32, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211­252, 2015. doi: 10.1007/s11263-015-0816-y.
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 1528­1540. ACM, 2016.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818­2826, 2016.
9

Under review as a conference paper at ICLR 2019 Florian Trame`r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick
McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= rkZvSe-RZ. Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, 2018. Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed adversarial examples. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HyydRMZC-.
10

Under review as a conference paper at ICLR 2019

MNIST-A 1.2
0.8
0.4

MNIST-A ADef 0.3 0.2 0.1

0.015 0.010 0.005

MNIST-A PGD

0.0 0
1.2
0.8
0.4

12 T -norm
MNIST-B

3

0.0 40
0.3 0.2 0.1

5 10 T -norm
MNIST-B ADef

0.000 15 0 100 200 300 400 500
T -norm MNIST-B PGD
0.15
0.10
0.05

0.0 0

123 T -norm

0.0 40

5 10 T -norm

0.00 15 0 5 10 15 20 25 30
T -norm

Figure 5: The (normalized) distribution of   T from the MNIST experiments. Deformations that fall to the left of the vertical line at  = 3 are considered successful. The networks in the first column were trained using the original MNIST data, and the networks in the second and third columns were adversarially trained using ADef and PGD, respectively.

Inception-v3
1.2

ResNet-101
1.2

0.8 0.8

0.4 0.4

0.0 0

123 T -norm

0.0 40

123 T -norm

4

Figure 6: The (normalized) distribution of   T from the ImageNet experiments. Deformations that fall to the left of the vertical line at  = 3 are considered successful.

A DISTRIBUTION OF VECTOR FIELD NORMS
Figures 5 and 6 show the distribution of the norms of the total deforming vector fields,  , from the experiments in section 3. For networks that have not been adversarially trained, most deformations fall well below the threshold of = 3. Out of the adversarially trained networks, only MNIST-A trained against PGD is truly robust against ADef. Further, a comparison between the first column of figure 5 and figure 6 indicates that ImageNet is much more vulnerable to adversarial deformations than MNIST, also considering the much higher resolution of the images in ImageNet. Thus, it would be very interesting to study the performance of ADef with adversarially trained network for ImageNet, as mentioned in the Conclusion.
11

Under review as a conference paper at ICLR 2019

7210414959

Original

Deformed

9176978867

T-norm: 1.621 0

T-norm: 1.905 6

T-norm: 1.531 9

T-norm: 1.405 0

T-norm: 0.615 1

T-norm: 1.133 5

T-norm: 0.650 9

T-norm: 0.842 7

T-norm: 0.469 3

T-norm: 1.082 4

Original

Deformed

5075537387
T-norm: 1.256 T-norm: 1.027 T-norm: 0.846 T-norm: 1.630 T-norm: 1.104 T-norm: 1.011 T-norm: 1.223 T-norm: 1.324 T-norm: 0.348 T-norm: 2.483
Figure 7: Adversarial deformations for MNIST-A. First and third rows: Original images from the MNIST test set. Second and fourth rows: The deformed images and the norms of the corresponding deforming vector fields.
7210414959

Original

Deformed

3147988467

T-norm: 1.042 0

T-norm: 1.290 6

T-norm: 1.130 9

T-norm: 1.101 0

T-norm: 0.952 1

T-norm: 1.380 5

T-norm: 0.488 9

T-norm: 0.531 7

T-norm: 0.245 5

T-norm: 1.025 4

Original

Deformed

2847534339
T-norm: 1.200 T-norm: 1.044 T-norm: 0.905 T-norm: 1.229 T-norm: 1.203 T-norm: 1.746 T-norm: 1.068 T-norm: 1.845 T-norm: 0.043 T-norm: 1.371
Figure 8: Adversarial deformations for MNIST-B. Note that image 9 in row 3 is misclassified, and is then deformed to its correct label.
B ADDITIONAL DEFORMED IMAGES
B.1 MNIST Figures 7 and 8 show adversarial deformations for the models MNIST-A and MNIST-B, respectively. The attacks are performed using the same configuration as in the experiments in section 3.
B.2 IMAGENET Figures 9 ­ 13 show additional deformed images resulting from attacking the Inception-v3 model using the same configuration as in the experiments in section 3. Similarly, figures 14 ­ 18 show deformed images resulting from attacking the ResNet-10 model. However, in order to increase variability in the output labels, we perform a targeted attack, targeting the label of 50th highest probability.
12

Under review as a conference paper at ICLR 2019

Original: water bottle

Deformed: oil filter

Original: sea cucumber

T : 0.028
Deformed: chiton

Original: reflex camera

T : 0.584
Deformed: projector

T : 0.548 Figure 9: ADef attacks on the Inception-v3 model using the same configuration as in the experiments in section 3.
13

Under review as a conference paper at ICLR 2019

Original: bison

Deformed: ox

Original: minibus

T : 0.919
Deformed: police van

Original: red wolf

T : 0.005
Deformed: dam

T : 1.874 Figure 10: ADef attacks on the Inception-v3 model using the same configuration as in the experiments in section 3.
14

Under review as a conference paper at ICLR 2019

Original: water tower

Deformed: chime

Original: pelican

T : 2.791
Deformed: bath towel

T : 0.906
Original: reflex camera Deformed: Polaroid camera

T : 0.061 Figure 11: ADef attacks on the Inception-v3 model using the same configuration as in the experiments in section 3.
15

Under review as a conference paper at ICLR 2019

Original: pretzel

Deformed: chain

Original: garbage truck

T : 0.774
Deformed: trailer truck

Original: ice lolly

T : 0.183
Deformed: spatula

T : 1.156 Figure 12: ADef attacks on the Inception-v3 model using the same configuration as in the experiments in section 3.
16

Under review as a conference paper at ICLR 2019

Original: tobacco shop

Deformed: bookshop

Original: zebra

T : 0.027
Deformed: hartebeest

T : 2.002
Figure 13: ADef attacks on the Inception-v3 model using the same configuration as in the experiments in section 3.

17

Under review as a conference paper at ICLR 2019
Original: sewing machine Deformed: paper towel

Original: starfish

T : 1.139
Deformed: electric ray

Original: freight car

T : 1.505
Deformed: triceratops

T : 1.270 Figure 14: ADef attacks on the ResNet-101 model targeting the 50th most likely label.
18

Under review as a conference paper at ICLR 2019

Original: parachute

Deformed: golfcart

Original: Christmas stocking

T : 1.060
Deformed: crane

Original: sock

T : 0.995
Deformed: ice cream

T : 1.233 Figure 15: ADef attacks on the ResNet-101 model targeting the 50th most likely label.
19

Under review as a conference paper at ICLR 2019

Original: black stork

Deformed: hippopotamus

Original: worm fence

T : 1.396
Deformed: viaduct

Original: chain mail

T : 1.550
Deformed: ski mask

T : 0.851 Figure 16: ADef attacks on the ResNet-101 model targeting the 50th most likely label.
20

Under review as a conference paper at ICLR 2019

Original: mousetrap

Deformed: perfume

Original: pillow

T : 0.904
Deformed: electric guitar

Original: pickup

T : 0.588
Deformed: projector

T : 0.922 Figure 17: ADef attacks on the ResNet-101 model targeting the 50th most likely label.
21

Under review as a conference paper at ICLR 2019

Original: forklift

Deformed: minibus

Original: dingo

T : 1.233
Deformed: lynx

T : 0.391 Figure 18: ADef attacks on the ResNet-101 model targeting the 50th most likely label.

22

Under review as a conference paper at ICLR 2019

C MATHEMATICAL DERIVATION

To maintain anonymity during the reviewing process, we wish to include the section of the master's thesis Anonymous (2017) which includes the mathematical derivation of the ADef algorithm. This section can be removed altogether when the anonymity of this reference has been lifted. The remainder of the text is an anonymized version of the relevant part of Anonymous (2017). Keep in mind that the mathematical notation used in the following differs from the notation used in the main body of this paper.

C.1 ADVERSARIAL DEFORMATIONS

In this section, we model images (or signals in general) as functions in a L2-Bochner space. Let D := (0, 1)d be the domain on which our signals are defined. In the case of images we have
d = 2, but we allow any dimension  1. Let c be the number of channels in the signal (c = 1 for
grayscale images and c = 3 for color images). Define the space of signals to be the Bochner space X := L2(D, Rc). It is a Hilbert space with the inner product

c

x, y X = x(p), y(p) Rc dµ(p) =

xi(p)yi(p) dµ(p)

D i=1 D

where µ is the Lebesgue measure on D.

For all x  X, define

x

X,

:=

max
i=1,...,c

xi

L(D) .

Note that this quantity is infinite for some x  X and thus does not define a norm on X. In the space X, the box of valid signals is

B = {x  X : x X,  1}.

In the literature, adversarial perturbations are usually regarded as "imperceptible" if they are small in some p-norm. These norms, however, do not sufficiently describe what is imperceptable to the human eye. For example, if x  B and u  Rd is small, then the translated image xu, defined by

xu(p) =

x(p + u) x(p)

if p + u  D otherwise,

is likely to be indistinguishable from x to the human eye. The corresponding perturbation xu - x may however have a relatively large Lp-norm. Translation by u can be seen as a deformation of x by a constant vector field  = u. In this section we generalize this example to deformations by
arbitrary vector fields.

We search for adversarial deformations using the same strategy as in the search for adversarial perturbations: We assume that there exists a small deformation of x that changes its label and use a first order approximation of the classifier to find the corresponding vector field.

We consider deformations to be small when the corresponding vector field is small in the supremum
norm. This allows us to bypass some technical difficulties that appear in the analysis with other Lp-norms. It may be of interest to limit the search for adversarial deformations to smooth vector
fields. This can be achieved by limiting the search to the range of a Gaussian smoothing operator.

C.1.1 DEFORMATIONS

We now make precise the notion of deformations through vector fields on D. First, we specify a set U of admissible vector fields, and then for any bounded x  X we define a family of deformed signals (x )U .

Let 0

<



<

1 2

,

K

:=

[, 1 - ]d



D and denote by T

:=

L(K, Rd) the Bochner space

of vector fields  : p  p supported on K and bounded with respect to the norm  T =

ess suppK p Rd . For a vector field  , define the deformation

 : D  Rd, p  p + p

23

Under review as a conference paper at ICLR 2019

(taking p = 0 for p / K) and define the set of admissible vector fields as U = {  T :  (D)  D}.

Note that since we have

inf p - q Rd : p  K, q  Rd \ D = , B(0) := {  T :  T < }  U.

For any x  X with x X, <  we define the deformation operator of x:

x : U  X,   x

where x := x   . Observe that x X,  x X, <  for any   U . Hence, x X <  for all   U and x is well defined.
Lemma C.1. If x is twice continuously differentiable in a neighborhood of K and x X < , then x : U  X is Fre´chet differentiable at 0 and its Fre´chet derivative at 0, D0x, is equal to the operator
A : T  X,    ,

where

 : D  Rc, p  (Dpx) p.

Proof. First we note that any   T is bounded and supported on K and since x has continuous derivatives on K, we get



2 X

=

(Dpx) p

2 Rc

dµ(p)

D



Dpx

2 op

p

2 Rd

dµ(p)

K

 µ(K)

sup
pK

Dpx

2 op



2 T

< ,

where

Dpx op = sup
uRd \{0}

(Dpx) u Rc u Rd

is the operator norm of Dpx. Hence, the mapping

A : T  X,   

is a well defined bounded linear operator.

To understand x, we consider local linear approximations of x. For any p  K and u  Rd small enough we have

x(p + u) = x(p) + (Dpx) u + p(u)

where

p(u) Rc

u

-2 Rd

 0 as u  0.

Moreover, by continuity of the second derivatives of x on

the compact set K, we have the uniform bound u Rd <  for some constant C > 0.

p(u) Rc  C

u

2 Rd

for all p  K and all u with

Note that 0 = idD, so x0 = x. Hence for   U we have for all p  K

x (p) - x0(p) = x(p + p) - x(p) = (Dpx) (p) + p(p) =  (p) + p(p)

24

Under review as a conference paper at ICLR 2019

and for p / K we have x (p) - x0(p) =  (p) = 0. Recall that U , the set of admissible vector fields, contains an open ball around 0  T , so the following limit is well defined.

lim
 0

x( ) - x(0) - A( ) T

X

= lim
 0

1 T

 lim
 0
 lim
 0

1 T 1 T

1/2

p(p)

2 Rc

dµ(p)

K

1/2

C2

p

4 Rd

dµ(p)

K

µ(K )C 2



4 T

1/2

 lim C
 0

µ(K)  T

= 0.

This proves that x is Fre´chet differentiable and that D0x = A.

Remark C.2. Alternatively, we could have considered vector fields in the space T := L2(K, Rd) instead of T . Note that while T  T , the set of admissible vector fields

U := {  T :  (D)  D}2

does not contain an open ball around 0 in T and the Fre´chet derivative of x at 0 is not well defined. To see this, let  > 0 and consider the vector field   : K  Rd, defined for all p  K by

p =

0 e1

if p - q Rd   otherwise,

where

q

=

(

1 2

,

.

.

.

,

1 2

)



Rd

is

the

center

of

the

hypercube

K

and

e1

=

(1, 0, . . . , 0)

is

the

first

standard basis vector in Rd. The vector and B(q + e1)   (D). Although

field   is not admissible   T = 1 for all  > 0,

since q + we have

e1

=

(

3 2

,

1 2

,

.

.

.

,

1 2

)

/

D

 T =

K

p

2 Rd

dµ(p)

=

B (q)

e1

2 Rd

dµ(p)

=

µ(B (q))

=

dVd,

where Vd stands for the volume of the d-dimensional unit ball. Hence, for any  > 0, the nonadmissible vector field   lies in B(0) as long as  < (/Vd)1/d.

The fact that x is not defined on a whole neighborhood of 0 in T would lead us to consider the Ga^teaux derivative of x at 0  T in any direction  in the subspace T :

DG0x(

)

=

lim
0

x(

)

- 

x(0)

For the sake of simplicity, we stick to our choice T = L(K, Rd).

C.1.2 FINDING ADVERSARIAL DEFORMATIONS
Suppose we have a classifier on the signal space into L categories,
F : X  RL,
and an interpretable signal x  B classified as
l := arg max Fk(x).
k=1,...,L
Further assume that x is twice continuously differentiable, and that F is Fre´chet differentiable at x. Our goal is to find a small deformation   U such that
l = arg max Fk(x ).
k=1,...,L

We proceed as with the DeepFool algorithm. First we target a different label k = l and define the binary classifier fk := Fk - Fl. The function fk : X  R returns positive values for signals for
2Since any vector field in U is bounded, we have U = U .

25

Under review as a conference paper at ICLR 2019

which the classifier F prefers the label k over the label l and it returns negative values for signals
for which F prefers l over k. Composing this new classifier with the deformation operator of x we get another binary classifier gk := fk  x on U . We have gk( ) > 0 for vector fields  such that x is considered to belong to class k rather than class l. Likewise gk( ) < 0 if  is unsuccessful in changing the preference from l to k.

By assumption, fk is Fre´chet differentiable at x. Its Fre´chet derivative, Dxfk, is a continuous linear functional on the Hilbert space X, and by the Riesz representation theorem there exists a unique function   X such that

(Dxfk)y = , y X , for all y  X.

By the chain rule and lemma C.1, gk is differentiable at 0. Let us calculate the Fre´chet derivative of gk in the direction of   T . The first equality is due to x(0) = x.

(D0gk) = (Dxfk)(D0x) = ,  X
c
= i, ( )i L2(D)
i=1 c
= i, xi,  Rd L2(D)
i=1 cd
= i, j j xi L2(D)
i=1 j=1
cd
= i, j j xi L2(D)
i=1 j=1
cd
= ij xi, j L2(K).
i=1 j=1
dc
= ij xi, j L2(K).
j=1 i=1

(8)

The second to last step is due to the partial derivatives jxi : R  R being continuous and hence bounded on the compact set K and j being zero outside of K:

i, j j xi L2(D) = ij j xi = ij xij = ij xi, j L2(K).
DK
Note that j  L2(K) since it is bounded on account of   T = L(K, Rd) and µ(K) < .
If there exists a vector field   U such that

(D0gk)  = -gk(0)

(9)

and if it is small enough, then by first order approximation

gk( )  gk(0) + (D0gk)  = 0.

(10)

In this case x lies approximately on the decision boundary of fk. For j = 1, . . . , d define j  L2(K) by setting

c

j = , j x Rc = ij xi.

(11)

i=1

Then by the calculations above, the vector field

 (x, k) :=

-gk (0)

d j=1

j

2 L2 (K )

1

 

...

 

d

26

Under review as a conference paper at ICLR 2019

satisfies condition (9), provided that it is bounded. Even if  (x, k) is bounded, it may not be admissible and small enough for (10) to be a good approximation. However, we still have the freedom to choose the targeted label k to minimize  (x, k) T .
We define an algorithm for finding adversarial deformations for x: Initialize by x0 := x. Then for n  0 let
kn := arg min  (xn, k) T
k=l
and  n :=  (xn, kn) and define xn+1 := xn ( n)
where xn is the deformation operator of xn. Subtleties of this update step are addressed in section C.1.3.

Once l = arg max Fk(xn), the iteration stops and the algorithm outputs the deformed image
k=1,...,L

n-1

x^ := xn = 0x

m .

m=0

One may also limit the number of iterations to some nmax  1 and the magnitude of the total

deformation

n-1 m=0

m

(in

some

norm)

to

a

number

tmax

>

0.

C.1.3 SMOOTH DEFORMATIONS

In order to iterate the deformation process, we need to assume that F is also Fre´chet differentiable at x . Moreover, the function x = x( ) : D  Rc does not in general satisfy the conditions of lemma C.1. However, if x is twice continuously differentiable on an open set E  D containing K, and  is smooth on the interior of K (denoted by K°), then x is twice continuously differentiable
on the open set
K°  -1(E) = p  K° : p + p  E .

If  T is sufficiently small, then there exists a number ~ with 0 <  < ~ < 1/2 such that
K~ := [~, 1 - ~]d  K°  -1(E).
Thus x satisfies the conditions of lemma C.1 with K replaced by the smaller domain K~ , and we can repeat the deformation step on this shrunk set. In this section, we give a simple method for obtaining a smooth vector field  that satisfies condition (9). This way, one can iterate the deformation without worry, given that F is everywhere Fre´chet differentiable and the domain K is allowed to shrink with each iteration.

Let  : Rd  R be a smooth, integrable function with the property (p) = (-p) for all p  Rd. For example,  may be chosen to be the d-dimensional Gaussian function of width  > 0,

(p)

=

1 (22)d/2

exp

-

1 22

p

2 2

for all p  Rd.

Define the linear operator S : T  T by

(  1) K 

S :=  

...

 

(  d) K

for all   T,

where (  j) K denotes the restriction of   j to K and the convolution is defined by extending j by zero outside of K. The operator is well defined since for each j = 1, . . . , d, the function   j is smooth, K is compact, and thus (S )j = (  j) K is bounded. Any vector field in the range of S is smooth on the interior of K, and hence we shall search for a vector field   T such that
~ := S satisfies condition (9).

Note that for any u, v  L2(K), we have

u, (  v)
K

L2(K) =

(  u) , v
K

L2 (K )

27

Under review as a conference paper at ICLR 2019

since  = (-·). By calculation (8), we have for all   T :

dd

(D0gk) (S ) =

j , (  j ) K L2(K) =

(  j ) K , j L2(K),

j=1

j=1

where 1, . . . , d are defined by formula (11). Set ~j := (  j) K for j = 1, . . . , d and

 :=

-gk (0)

d j=1

~j

2 L2 (K )

~1

 

...

. 

~d

Then ~ := S is a smooth vector field on K that satisfies condition (9),

(D0gk)~ = -gk(0). Hence we may substitute ~ for  (x, k) in the algorithm described in section C.1.2.

C.1.4 ADVERSARIAL DEFORMATIONS IN PRACTICE

We now make explicit the discrete version of the deformation algorithm. Specifically, we consider square images x = (x1, x2, x3)  X = R3×w×w of P = w2 pixels and c = 3 color channels.
Here it is important to maintain the structure of the image x as a triplet of square matrices instead of representing it simply as a column vector in R3P . Let F : X  RL be a differentiable classifier and let x0  X be an image with label l = arg maxk Fk(x0).

Now let n  0 be an integer and write x = xn for simplicity. The image x  X can be thought of as a sampling of a twice differentiable function  : (0, 1)2  R3 through

xi,p,q = i

w

p +

1,

w

q +

1

,

for p, q = 1, . . . , w, and i = 1, 2, 3.

We approximate the partial derivatives of  by finite differences. For i = 1, 2, 3, define the matrices D1xi, D2xi  Rw×w by central differences where possible

[D1xi]p,q

=

w

+1 2

(xi,p+1,q

- xi,p-1,q)

for p = 2, . . . , (w - 1) and all q,

[D2xi]p,q

=

w

+1 2

(xi,p,q+1

- xi,p,q-1)

for q = 2, . . . , (w - 1) and all p,

and define the remaining entries on the boundary by forward and backward differences accordingly:

[D1xi]1,q = (w + 1) (xi,2,q - xi,1,q) and [D1xi]w,q = (w + 1) (xi,w,q - xi,w-1,q) for all q, [D2xi]p,1 = (w + 1) (xi,p,2 - xi,p,1) and [D2xi]p,w = (w + 1) (xi,p,w - xi,p,w-1) for all p.

The entries of D1xi and D2xi then approximate the partial derivatives of i:

1i

w

p +

1

,

w

q +

1

 [D1xi]p,q

and

2i

w

p +

1

,

w

q +

1

 [D2xi]p,q

for p, q = 1, . . . , w and i = 1, 2, 3.

Let k = l be the target label.

By assumption, the gradient xFk =

((xFk)1, (xFk)2, (xFk)3)  R3×w×w is well defined. Define the matrices 1, 2  Rw×w

by inner product over the color dimension as in (11),

3
1 = (xFk - xFl)i
i=1
3
2 = (xFk - xFl)i
i=1

D1xi, D2xi.

(12)

28

Under review as a conference paper at ICLR 2019

where denotes the Hadamard (entry-wise) product of matrices. The matrices 1 and 2 are the (scaled) components of the discrete vector field

 (x, k) :=

Fl(x) - Fk(x)

1

2 F

+

2

2 F

(1,

2)



T

:=

R2×w×w

where · F is the Frobenius norm. Remark C.3. The matrices 1 and 2 can be replaced by convolutions ~j =   j with a smooth kernel  in order to obtain a smooth vector field  .

Define the norm3 of   T = R2×w×w by



T

= max
p,q=1,...,w

12,p,q + 22,p,q

and let

kn := arg min  (xn, k) T
k=l

 n :=  (xn, kn).

Finally, we define the deformed image xn+1  X by

xin,+p,1q = i

w

p +

1

+

1n,p,q ,

w

q +

1

+

2n,p,q

for p, q = 1, . . . , w and i = 1, 2, 3. The values of  can be estimated by interpolation from xn.

If l = arg maxk Fk(xn+1), then return x^ := xn+1. Otherwise, repeat the procedure above for n+1.

3 We choose this norm to be consistent with the theory in section C.1.2. One might want to minimize the Frobenius norm of  instead.
29

