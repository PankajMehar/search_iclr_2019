Under review as a conference paper at ICLR 2019
ROTDCF: DECOMPOSITION OF CONVOLUTIONAL FILTERS FOR ROTATION-EQUIVARIANT DEEP NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Explicit encoding of group actions in deep features makes it possible for convolutional neural networks (CNNs) to handle global deformations of images, which is critical to success in many vision tasks. This paper proposes to decompose the convolutional filters over joint steerable bases across the space and the group geometry simultaneously, namely a rotation-equivariant CNN with decomposed convolutional filters (RotDCF). This decomposition facilitates computing the joint convolution, which is proved to be necessary for the group equivariance. It significantly reduces the model size and computational complexity while preserving performance, and truncation of the bases expansion serves implicitly to regularize the filters. On datasets involving in-plane and out-of-plane object rotations, RotDCF deep features demonstrate greater robustness and interpretability than regular CNNs. The stability of the equivariant representation to input variations is also proved theoretically. The RotDCF framework can be extended to groups other than rotations, providing a general approach which achieves both group equivariance and representation stability at a reduced model size.
1 INTRODUCTION
While deep convolutional neural networks (CNN) have been widely used in computer vision and image processing applications, they are not designed to handle large group actions like rotations, which degrade the performance of CNN in many tasks Cheng et al. (2016); Hallman & Fowlkes (2015); Jaderberg et al. (2015b); Laptev et al. (2016); Maninis et al. (2016). The regular convolutional layer is equivariant to input translations, but not other group actions. An indirect way to encode group information into the deep representation is to conduct generalized convolutions across the group as well, as in Cohen & Welling (2016a). In theory, this approach can guarantee the group equivariance of the learned representations, which provides better interpretability and regularity as well as the capability of estimating the group action in localization, boundary detection, etc. For the important case of 2D rotations, group-equivariant CNNs have been constructed in several recent works, e.g., Weiler et al. (2017), Harmonic Net Worrall et al. (2017) and Oriented Response Net Zhou et al. (2017). In such networks, the layer-wise output has an extra index representing the group element (c.f. Table 1), and consequently, the convolution must be across the space and the group jointly (proved in Section 3.1). This typically incurs a significant increase in the number of parameters and computational load, even with the adoption of steerable filters Freeman et al. (1991); Weiler et al. (2017); Worrall et al. (2017). In parallel, low-rank factorized filters have been proposed for sparse coding as well as the compression and regularization of deep networks. In particular, Qiu et al. (2018) showed that decomposing filters under non-adaptive bases can be an effective way to reduce the model size of CNNs without sacrificing performance. However, these approaches do not directly apply to be group-equivariant. We review these connections in more detail in Section 1.1. This paper proposes a truncated bases decomposition of the filters in group-equivariant CNNs, which we call the rotation-equivariant CNN with decomposed convolutional filters (RotDCF). Since we need a joint convolution over R2 and SO(2), the bases are also joint across the two geometrical domains, c.f. Figure 1. The benefits of bases decomposition are three-fold: (1) Reduction of the number of parameters and computational complexity of rotation-equivariant CNNs, c.f. Section 2.3; (2) Implicit regularization of the convolutional filters, leading to improved robustness of the learned deep representation shown experimentally in Section 4; (3) Theoretical guarantees on stability of
1

Under review as a conference paper at ICLR 2019
Figure 1: Decomposition of the convolutional filter across the 2D space (variable u) and the SO(2) rotation group geometry (variable ) simultaneously. The filter is represented as a truncated expansion under the prefixed bases k(u)m() with adaptive coefficients ak,m learned from data, where k are Fourier-Bessel bases and m are Fourier bases. The filter has N group-indexed channels (indexed by ) and only one input and output unstructured channel (indexed by  and  respectively) for simplicity, c.f. Section 2.1 and Table 1. the equivariant representation to input deformations, which follow from a generic condition on the filters in the decomposed form, c.f. Section 3.2. 1.1 RELATED WORK Learning with factorized filters. In the sparse coding literature, Rubinstein et al. (2010) proposed the factorization of learned dictionaries under another prefixed dictionary. Separable filters were used in Rigamonti et al. (2013) to learn the coding of images. Papyan et al. (2017) interpreted CNN as an iterated convolutional sparse coding machine, and in this view, the factorized filters should correspond to a "dictionary of the dictionary" as in Rubinstein et al. (2010). In the deep learning literature, low-rank factorization of convolutional filters has been previously used to remove redundancy in trained CNNs Denton et al. (2014); Jaderberg et al. (2014). The compression of deep networks has also been studied in Chen et al. (2015); Han et al. (2016; 2015), SqueezeNet Iandola et al. (2016), etc., where the low-rank factorization of filters can be utilized. MobileNets Howard et al. (2017) used depth-wise separable convolutions to obtain significant compression. Tensor decomposition of convolutional layers was used in Lebedev et al. (2014) for CPU speedup. Tai et al. (2015) proposed low-rank-regularized filters and obtained improved classification accuracy with reduced computation. Qiu et al. (2018) studied decomposed-filter CNN with prefixed bases and trainable expansion coefficients, showing that the truncated Fourier-Bessel bases decomposition incurs almost no decrease in classification accuracy while significantly reducing the model size and improving the robustness of the deep features. None of the above networks are group equivariant. Group-equivariant deep networks. The encoding of group information into network representations has been studied extensively. Among earlier works, transforming auto-encoders Hinton et al. (2011) used a non-convolutional network to learn group-invariant features and compared with handcrafted ones. Rotation-invariant descriptors were studied in Schmidt & Roth (2012b) with product models, and in Jaderberg et al. (2015a); Kivinen & Williams (2011); Schmidt & Roth (2012a) by estimating the specific image transformation. Gonzalez et al. (2016); Wu et al. (2015) proposed rotating conventional filters to perform rotation-invariant texture and image classification. The joint convolution across space and rotation has been studied in the scattering transform Oyallon & Mallat (2015); Sifre & Mallat (2013). Group-equivariant CNN was considered by Cohen & Welling (2016a), which handled several finite small-order discrete groups on the input image. Rotationequivariant CNN was later developed in Weiler et al. (2017); Worrall et al. (2017); Zhou et al. (2017) and elsewhere. In particular, steerable filters were used in Cohen & Welling (2016b); Weiler et al. (2017); Worrall et al. (2017). SO(3)-equivariant CNN for signals on spheres was studied in Cohen et al. (2018) in a different setting. Overall, the efficiency of equivariant CNNs remains to be improved since the model is typically several times larger than that of a regular CNN.
2 ROTATION-EQUIVARIANT DCF NET
2.1 ROTATION-EQUIVARIANT CNN A rotation-equivariant CNN indexes the channels by the SO(2) group Weiler et al. (2017); Zhou et al. (2017): The l-th layer output is written as x(l)(u, , ), the position u  R2, the rotation
2

Under review as a conference paper at ICLR 2019

  S1, and   [Ml], Ml being the number of unstructured channel indices. Throughout the paper, [m] stands for the set {1, · · · , m}. We denote the group SO(2) also by the circle S1 since the former is parametrized by the rotation angle. The convolutional filter at the l-th layer is represented

as W(l),(v, ),   [Ml-1],   [Ml], v  R2,   S1, except for the 1st layer where there is no

indexing of . In practice, S1 is discretized into N points on (0, 2). We denote the summation

over u and  by continuous integration, and the notation

S1 (· · · )d

means

1 2

2 0

(·

·

·

)d.

Let the 2D rotation by angle t be denoted by t, in the 1st layer of the group-invariant CNN,

M0

x(1)(u, , ) = 

x(0)(u + v ,  )W(1,)(v )dv + b(1)() .

 =1 S1 R2

(1)

Note that the 1st layer output has N orientations indexed by , forming a "channel geometry" (Table 1). For l > 1, the convolution is jointly over R2 and SO(2), which takes the form as

Ml-1



x(l)(u, , ) =  

x(l-1)(u + v ,  ,  )W(l),(v ,  - )dv d + b(l)() .

 =1 S1 R2

(2)

The joint convolution over R2 and SO(2) is both sufficient and necessary to guarantee group-

equivariance (Theorem 3.1). While group equivariance is a desirable property, the model size and

computation can be increased significantly due to the extra index   [N].

2.2 DECOMPOSED FILTERS UNDER STEERABLE BASES

We decompose the filters with respect to u and  simultaneously: Let {k}k be a set of bases on the unit 2D disk, and {m}m be bases on S1. At the l-th layer, let jl be the scale of the filter in u, and j,k = 2-2jk(2-ju) (the filter is supported on the disk of radius 2jl ). Since we use continuous convolutions, the down-sampling by "pooling" is modeled by the rescaling of the filters in space. The decomposed filters are of the form

W(1,)(v) =

a(1),(k)j1,k(v), W(l),(v, ) =

a(l),(k, m)jl,k(v)m(), l > 1, (3)

k km

which is illustrated in Figure 1 (for l > 1). We use Fourier-Bessel (FB) bases for {k}k which are steerable, and Fourier bases for {m}m, so that the operation of rotation is a diagonalized linear transform under both bases. Specifically, in the complex-valued version,

k(tv) = e-im(k)tk(v), k, m( - t) = e-imtm(), m.

(4)

This means that after the convolutions on R2 × S1 with the bases k(v)l() are computed for all k and l, both up to certain truncation, the joint convolution (1), (2) with all rotated filters can be calculated by the algebraic manipulation of the expansion coefficients a(l),(k, m), and without any re-computation of the spatial-rotation joint convolution. Standard real-valued versions of the bases k and m in sin's and cos's are used in practice. During training, only the expansion coefficients a's are updated, and the bases are fixed.

Apart from the saving of parameters and computation, the bases truncation also regularizes the convolutional filters by discarding the high frequency components. As a result, DCF Net reduces response to those components in the input at all layers, which improves the robustness of the learned feature without affecting recognition performance. The visibly smoother trained filters in RotDCF are shown in Supplementary Material (S.M.) Figure A.1, demonstrating the same regularization effects as in Qiu et al. (2018), the latter being without the rotation-equivariant setting. The theoretical properties of RotDCF Net, particularly the representation stability, will be analyzed in Section 3.

fully-connected layer

regular convolutional layer

CNN with group-indexed channels

x(l-1)( )  x(l)() x(l-1)(u ,  )  x(l)(u, ) x(l-1)(u ,  ,  )  x(l)(u, , )

  : dense

u  u: spatial convolution   : dense

u  u,   : joint convolution   : dense

Table 1: Comparison of a fully-connected layer, a regular convolutional layer, and a rotation-equivariant convolutional layer with group-indexed channels.
3

Under review as a conference paper at ICLR 2019

2.3 NUMBERS OF PARAMETERS AND COMPUTATION FLOPS

hMNausmL×b2MeMr0,oMsfot0rtahpiaantraatbmhleeetnpeuramsr.abImenraeonteferpqsa:uriIavnmaareirtaeengrtsuCliasNrLNC2,NNaNjoM, ianctMocno.vnoIvnloulatuiotRinooantlDallaCfiyFletreNroeifsts,oiKzfesLbiza×eseLLs××arLMe ×u0s×NedM ×in0 space and K bases across the angle , so that the number of parameters is KKM M . This gives

a

reduction

of

K L2

·

K N

compared

to

non-bases

equivariant

CNN.

In

practice,

after

switching

from

a

regular

CNN

to

a

RotDCF

Net,

typically

M



1 2

M0

or

more

due

to

the

adoption

of

filters

in

all

orientations. The Qiu et al. (2018).

factor In all

LtKh2eiesxupseuraimllyenbtestwineeSnec81tioannd4,31

depending on the K is typically 5,

network and N

and =8

the or

problem 16. This

means that RotDCF Net achieves a significant parameter reduction from the non-bases equivariant

CNN,

and

even

reduces

parameters

from

a

regular

CNN

by

a

factor

of

1 2

or

more.

Computation in a forward pass: When the input and output are both W × W in space, the regular

CNN about

l2aMyerMneWeds2L2M2N02M. 0InWc2oLnt2ramsta,ntyheflocopms, pauntdataionnonin-baasReosteDqCuFivalariyaenrt

convolutional layer needs is dominated by a term of

2M M W 2KKN, calculations in S.M..

which

is

reduced

from

the

non-bases

network

by

a

factor

of

K L2

·

K N

.

Detailed

In summary, terms of both

RotDCF Net achieves a reduction model size and computation. With

toyfpiLKc2al·nKNetwofrrokmarcnhointe-bcatusreess,eqRuoitvDarCiaFnNt CetNmNasy,

in be

of a smaller model size than regular CNNs. Numbers for specific networks are shown in Section 4.

3 THEORETICAL ANALYSIS OF DEEP FEATURES

This section presents two analytical results: (1) Joint convolution (1), (2) is sufficient and actually necessary to obtain rotation equivariance; (2) Stability of the equivariant representation with respect to input variations is proved under generic conditions, which is important in practice since rotations are never perfect.

3.1 GROUP-EQUIVARIANT PROPERTY

We consider the change of the l-th layer output when the input image undergoes some arbitrary rotation. Let rotation around point u0 by angle t be denoted by  = u0,t, i.e. u0,tu = u0 + t(u - u0), for any u  R2, and the transformed image by Dx(0)(u, ) = x(0)(u0,tu, ), for any   [M0]. We also define the action T on the l-th layer output x(l), l > 0, as

Tx(l)(u, , ) = x(l)(u0,tu,  - t, ),   [Ml].

(5)

The following theorem, proved in S.M., shows that the joint convolution scheme (1), (2) not only produces group-equivariant features at all layers in the sense of

x(l)[Dx(0)] = Tx(l)[x(0)],

(6)

but is necessary for a CNN with SO(2)-indexed channels to achieve (6). The sufficiency part is previously shown in Weiler et al. (2017). The necessity of the joint convolution motivates the design and the efforts of reducing the complexity of such models. Note that RotDCF is a type of the channel-SO(2)-indexed CNNs considered in the theorem, so it follows that RotDCF is groupequivariant.

Theorem 3.1. In a CNN with SO(2)-indexed channels, let x(l)[x(0)] be the output at the l-th layer from input x(0)(u, ). The relation (6) holds for all l if and only if the convolutional layers are given by (1), (2).

3.2 REPRESENTATION STABILITY UNDER INPUT VARIATIONS Assumptions on the RotDCF layers. Following Qiu et al. (2018), we make the following generic assumptions on the convolutional layers: First,
(A1) Non-expansive sigmoid:  : R  R is non-expansive.

4

Under review as a conference paper at ICLR 2019

Second, we also need a boundedness assumption on the convolutional filters W (l) for all l:

(A2) Boundedness of filters: In all layers, Al  1,

where Al is defined as

Ml-1
Al :=  max{sup
  =1

a(l),

FB,

sup


Ml-1 Ml

Ml =1

a(l), FB},

(7)

a(1),

2 FB

=

µk (a(1), (k))2 ,

a(l),

2 FB

=

µk(a(l),(k, m))2, l > 1,

(8)

k km

µk being the Dirichlet Laplacian eigenvalues of the unit disk in R2. Note that (A2) bounds the expansion coefficients, which implies a sequence of boundedness conditions on the convolutional filters in all layers (Proposition B.1), based upon which the stability results below are derived. Since µk typically increases in order, (A2) suggests truncating the series to only include low-frequency k and m's, which is implemented in Section 4. The validity of the boundedness assumption can be qualitatively fulfilled by normalization layers which is standard in practice.

Non-expansiveness of the network mapping. Let the L2 norm of x(l) be defined as

x(l) 2 = 1 Ml 1

x(l)(u, , )2dud,

Ml =1 || R2 S1

l1

and

x(0)

2=

1 M0

1  ||

R2 x(0)(u, )2du.  is the domain on which x(0) is supported, usually

 = [-1, 1] × [-1, 1]  R2. The following result is proved in S.M.:

Proposition 3.2. In a RotDCF Net, under (A1), (A2), for all l,

(a) The mapping of the l-th convolutional layer (including ), denoted as x(l)[x(l-1)], is nonexpansive, i.e., x(l)[x1] - x(l)[x2]  x1 - x2 for arbitrary x1 and x2.

(b) xc(l)  xc(l-1) for all l, where x(cl)(u, , ) = x(l)(u, , ) - x0(l)() (without index  when l=1) is the centered version of x(l) by removing x0(l), defined to be the output at the l-th layer from a zero bottom-layer input. As a result, x(cl)  x(c0) = x(0) .

Insensitivity to input deformation. We consider the deformation of the input "module" to a global

rotation. Specifically, let the deformed input be of the form D  D x(0), where D is as in Section 3.1,  = u0,t being a rigid 2D rotation, and D is a small deformation in space defined by

D x(0)(u, ) = x(0)(u -  (u), ), u  R2,   [M0],

(9)

with  : R2  R2 is C2. Following Qiu et al. (2018), we assume the small distortion condition:

(A3) Small distortion: | | = supu

 (u)

<

1 5

,

with

·

being the operator norm.

The mapping T as defined

u  u -  (u) is locally invertible, and the constant in (5), the stability result is summarized as

1 5

is

chosen

for

convenience.

With

Theorem 3.3. Let  = u0,t be an arbitrary rotation in R2, around u0 by angle t, and let D be a small deformation. In a RotDCF Net, under (A1), (A2), (A3), c1 = 4, c2 = 2, for any L,

x(L)[D  D x(0)] - Tx(L)[x(0)]  (2c1L| | + c22-jL | |) x(0) .

To prove Theorem 3.3, we firstly establish an approximate equivariant relation for all layers l (Proposition B.2), which can be of independent interest for estimating the image transformations. All the proofs are left to S.M. Unlike previous stability results for regular CNNs, the above result allows an arbitrary global rotation  with respect to which the RotDCF representation is equivariant, apart from a small "residual" distortion  whose influence can be bounded. This is also an important result in practice, because most often in recognition tasks the image rotation is not a rigid in-plane one, but is induced by the rotation of the object in 3D space. Thus the actual transformation of the image may be close to a 2D rotation but is not exact. The above result guarantees that in such cases the RotDCF representation undergoes approximately an equivariant action of T, which implies consistency of the learned deep features up to a rotation. The improved stability of RotDCF Net over regular CNNs in this situation is observed experimentally in Section 4.

5

Under review as a conference paper at ICLR 2019

Figure 2: Representative class activation maps (CAM) on testing images in the rotMNIST transfer learning experiment. The heatmap indicates the importance of image regions used in recognizing a digit class. The CNN and RotDCF networks are trained on up-right samples, with no retrain (left) and retraining the fully connected layers respectively (right) before testing. Testing samples are randomly rotated up to 60 degrees. (c.f. Table 2).

MNIST to rotMNIST MaxRot=30 Degrees

no-retrain

fc-retrain

CNN

92.61

94.71

RotDCF

96.90

98.48

MNIST to rotMNIST MaxRot=60 Degrees

no-retrain

fc-retrain

CNN

69.61

85.90

RotDCF

82.36

97.68

Table 2: Test accuracy in the rotMNIST transfer learning experiment. The network is trained on 10K up-right MNIST samples and tested on 50K randomly rotated samples up to the MaxRot degrees.

4 EXPERIMENTAL RESULTS

In this section, we experimentally test the performance of RotDCF Nets on object classification and face recognition tasks. The advantage of RotDCF Net is demonstrated via improved recognition accuracy and robustness to rotations of the object, not only with in-plain rotations but with 3D rotations as well. To illustrate the rotation equivariance of the RotDCF deep features, we show that a trained auto-encoder with RotDCF encoder layers is able to reconstruct rotated digit images from "circulated" codes. All codes will be publicly available.

4.1 OBJECT CLASSIFICATION

Non-transfer learning setting. The rotMNIST dataset contains 28 × 28 grayscale images of digits

from 0 to 9, randomly rotated by an angle uniformly distributed from 0 to 2 Cohen & Welling

(2016a). We use 10,000 and 5,000 training samples, and 50,000 testing samples. A CNN consisting

of 3 convolutional layers (Conv-3, Table A.1) is trained as a performance baseline, and the RotDCF

counterpart is made by replacing the regular convolutional layers with RotDCF layers, with reduced

number of (unstructured) channels M , and N many rotation-indexed channels (N = 8). K bases are used for k and K for m. The classification accuracy is shown in Table 3, for various choices of M , K and K. We see that RotDCF net obtains improved classification accuracy with significantly reduced number of parameters, e.g., with 10K training, the smallest RotDCF Net (M = 8,

K

=

3,

K

=

5)

improves

the

test

accuracy

from

95.67

to

97.59

with

less

than

1 20

many

parameters

of the CNN training size

model, and (5K). Note

t13haotfcthhoeoDsiCngF

model Qiu et al. (2018). The maximum K and K applies

trend continues with reduced no bases truncation to the fil-

ters in the linear algebra sense, which provides a baseline of group-equivariant CNNs without bases

decomposition. The CIFAR10 dataset consists of 32 × 32 colored images from 10 object classes

Krizhevsky (2009), and we use 10,000 training and 50,000 testing samples. The network architec-

rotMNIST Conv-3, Ntr = 10K Test Acc. # param.

CNN M =32

95.67

2.570×105

DCF M =32, K=5

95.58

5.158×104

DCF M =32, K=3

95.69

3.104×104

RotDCF N = 8 M =16, K=14, K=8 M =16, K=5, K=8 M =16, K=3, K=8 M =16, K=5, K=5 M =16, K=3, K=5 M =8, K=5, K=5 M =8, K=3, K=5

97.86 97.81 97.77 97.96 97.95 97.81 97.59

2.871×105 1.026×105 6.160×104 6.419×104 3.856×104 1.610×104 9.680×103

Ratio 1.00 0.20 0.12
1.12 0.40 0.24 0.25 0.15 0.06 0.04

rotMNIST Conv-3, Ntr = 5K Test Acc. # param.

Ratio

CNN M =32 DCF M =32, K=3 RotDCF N=8 M =16, K=3, K=5 M =8, K=3, K=5

94.04 94.08 96.79 96.53

(same as left)

CIFAR10 VGG-16, Ntr = 10K

CNN M = 64 RotDCF, N= 8 M =32, K=3, K=7 M =32, K=3, K=5

78.40
79.44 79.53

2.732×106
1.593×106 1.138×106

1.00
0.58 0.42

Table 3: Classification accuracy using regular CNN, DCF and RotDCF Nets on rotMNIST and CIFAR10. "#

param." is number of parameters in all convolutional layers, and "Ratio" indicates the proportion to the # param.

of the regular CNN. Notice that the reduction from non-bases rotation-equivariant CNNs (the fair comparison

c(Kas=e)14ca, nKbe=e8viennrosmtMalNleIrS,Tw)hisicmh aisthtehmeaftaicctaollryoefquLKi2vKNale,nct .tfo.

Section 2.3. The using "full" filters

case of without

maximum K and K bases decomposition.

6

Under review as a conference paper at ICLR 2019

Figure 3: Codes and reconstructions of rotMNIST digits. (Top) A test image is encoded into a 16 × 32 array in the red box (the intermediate representation), and the code generates 16 copies by circulating the rows. (Bottom) Images reconstructed from the row-circulated codes above by the decoder. ture is modified from VGG-16 net Simonyan & Zisserman (2014) (Table A.2). As shown in Table 3, RotDCF Net obtains better testing accuracy with reduced model size from the regular CNN baseline model. Throughout these experiments, the higher accuracy is due to the group equivariance and the lower model complexity is due to the bases decomposition. Transfer learning setting. We train a regular CNN and a RotDCF Net on 10,000 up-right MNIST data samples, and directly test on 50,000 randomly rotated MNIST samples where the maximum rotation angle MaxRot=30 or 60 degrees (the "no-retrain" case). We also test after retraining the last two non-convolutional layers (the " fc-retrain" case). To visualize the importance of image regions which contribute to the classification accuracy, we adopt Class Activation Maps (CAM) Zhou et al. (2016), and the network is modified accordingly by removing the last pooling layer in the net in Table A.1 and inserting a "gap" global averaging layer. The test accuracy are listed in Table 2, where the superiority of RotDCF Net is clearly shown in both the "no-retrain" and " fc-retrain" cases . The improved robustness of RotDCF Net is furtherly revealed by the CAM maps (Figure 2): the red-colored region is more stable for RotDCF Net even in the case with retraining.
4.2 IMAGE RECONSTRUCTION To illustrate the explicit encoding of group actions in the RotDCF Net features, we train a convolutional auto-encoder on the rotated MNIST dataset, where encoder consists of stacked RotDCF layers, and the decoder consists of stacked transposed-convolutional layers (Table A.3). The encoder maps a 28×28 image into an array of 16 × 32, where the first dimension is the discretization of the rotation angles in [0, 2], and the second dimension is the unstructured channels. Due to the rotation equivariant relation, the "circulation" of the rows of the code array should correspond to the rotation of the image. This is verified in Figure 3: The top panel shows the code array produced from a testing image, and the 16 row-circulated copies of it. The bottom panel shows the output of the decoder fed with the codes in the top panel.
4.3 FACE RECOGNITION As a real-world example, we test RotDCF on the Facescrub dataset Ng & Winkler (2014) containing over 100,000 face images of 530 people. A CNN and a RotDCF Net (Table A.4) are trained respectively using the gallery images from the 500 known subjects, which are preprocessed to be near-frontal and upright-only by aligning facial landmarks Kazemi & Sullivan (2014). See S.M. for data preparation and training details. For the trained deep networks, we remove the last softmax layer, and then use the network outputs as deep features for faces, which is the typical way of using deep models for face verification and recognition to support both seen and unseen subjects Parkhi et al. (2015). Using deep features generated by the trained networks, a probe image is then compared with the gallery faces whose identities are known and classified as the identity of the top match.

CNN

CNN

RotDCF

RotDCF

Figure 4: Example CAM maps for recognizing faces with in-plane rotations. The heatmap indicates the importance of different image regions used by respective models in defining a face, and a good CNN model is expected to select consistent face regions across the same-person images to determine the identity. Across different in-plane rotated copies, RotDCF chooses significantly more consistent discriminative regions than CNN, indicating more stable representations. In this experiment, we obtain 0.54% recognition accuracy using CNN (nearly random guess), and 97.04% accuracy using RotDCF with feature alignment, on known subjects.
7

Under review as a conference paper at ICLR 2019

Figure 5: Synthesized faces from a testing image with -40o to 40o yaw, and -20o to 20o pitch, at a 10o interval.

CNN

CNN

RotDCF

RotDCF

Figure 6: Example CAM maps for recognizing faces with out-of-plane rotations. Across out-of-plane rotated copies, the discriminative regions chosen by RotDCF in describing a subject are more consistent, showing better representation stability than CNN. In this experiment, we obtain 80.79% recognition accuracy using CNN, and 89.66% using RotDCF, on known subjects.

Under this gallery-probe face recognition setup, we obtain 94.10% and 96.92% accuracy for known and unknown subjects respectively using the CNN model; using RotDCF, the accuracies are 93.42% and 96.92%. Testing on unknown subjects are critical for validating the model representation power over unseen identities, and the reason for higher accuracy is simply due to the smaller number of classes. For both cases, RotDCF reports comparable performance as CNN, while the number of parameters in the RotDCF model is about one-fourth of the CNN model (see S.M.).

In-plane rotation. This experiment demonstrates the rotation-equivariance of the RotDCF features.

We set

apply in-plane rotations be the new gallery, the

at intervals of rotated copies

4betoththeenperwobperiombaegseest.(FIinguthreis4s),eattnindgl,etutshinegortihgeinRaloptDroCbFe

model we obtain 97.04% and 97.58% recognition accuracy for known and unknown subjects re-

spectively, after aligning the deep features by circular shifts (using the largest-magnitude  channel

as reference). Notice that the model only sees upright faces. This is due to the rotation-equivariant

property of the RotDCF Net, which means that the face representation is consistent regardless of

its orientations after the group alignment. Lacking such properties, CNN obtains 0.54% and 5.05%

recognition accuracies, which is close to random guess. We further compare CNN and RotDCF

models via the CAM maps, which indicate the image regions relied by the CNN to make a classifi-

cation prediction. In Figure 4, with regular CNN face regions used for prediction varies dramatically

from image to image, and with RotDCF a significantly more consistent region mostly covering eyes

and nose are selected across images, which explains its superior accuracy.

Out-of-plane rotation. To validate our theoretical result on representation stability under input deformations, we introduce out-of-plane rotations to the probe. Each probe image is fitted to a 3D face mesh, and rotated copies are rendered at the 10o intervals with -40o to 40o yaw, and -20o to 20o pitch, generating 45 synthesized faces in total (Figure 5). The synthesis faces at two poses (highlighted in red) are used as the new gallery, and all remaining synthesis faces form the new probe. The out-of-plane rotations here can be viewed as mild in-plane rotations plus additional variations, a situation frequently encountered in the real world. With this gallery-probe setup, the RotDCF model obtains 89.66% and 97.01% recognition accuracy for known and unknown subjects, and the accuracies are 80.79% and 89.97% with CNN. The CAM plots in Figure 6 also indicate that RotDCF Net chooses more consistent regions over CNN in describing a subject across different poses. Since the out-of-plane rotations as in Figure 5 can be considered as in-plane rotations with additional variations, the superior performance of RotDCF is consistent with the theory in Section 3.

5 CONCLUSION AND DISCUSSION
This work introduces a decomposition of the filters in rotation-equivariant CNNs under joint steerable bases over space and rotations simultaneously, obtaining equivariant deep representations with significantly reduced model size and an implicit filter regularization. The group equivariant property and representation stability are proved theoretically. In experiments, RotDCF demonstrates improved recognition accuracy and better feature interpretability and stability on synthetic and realworld datasets involving object rotations, particularly in the transfer learning setting. To extend the work, implementation issues like parallelism efficiency and memory usage should be considered before the computational savings can be fully achieved. The framework should also extend to other groups and joint geometrical domains.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. In International Conference on Machine Learning, pp. 2285­2294, 2015.
Gong Cheng, Peicheng Zhou, and Junwei Han. Rifd-cnn: Rotation-invariant and fisher discriminative convolutional neural networks for object detection. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pp. 2884­2893. IEEE, 2016.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In ICML, pp. 2990­2999, 2016a. URL http://jmlr.org/proceedings/papers/v48/cohenc16.html.
Taco S. Cohen and Max Welling. Steerable cnns. arXiv preprint arXiv:1612.08498, 2016b. Taco S Cohen, Mario Geiger, Jonas Koehler, and Max Welling. Spherical cnns. arXiv preprint
arXiv:1801.10130, 2018. Emily L. Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure
within convolutional networks for efficient evaluation. In NIPS, pp. 1269­1277, 2014. William T Freeman, Edward H Adelson, et al. The design and use of filters. IEEE Transactions on Pattern
analysis and machine intelligence, 13(9):891­906, 1991. Diego Marcos Gonzalez, Michele Volpi, and Devis Tuia. Learning rotation invariant convolutional filters for
texture classification. CoRR, abs/1604.06720, 2016. URL http://arxiv.org/abs/1604.06720. Sam Hallman and Charless C Fowlkes. Oriented edge forests for boundary detection. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1732­1740, 2015. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural
network. In Advances in Neural Information Processing Systems, pp. 1135­1143, 2015. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with
pruning, trained quantization and huffman coding. International Conference on Learning Representations (ICLR), 2016. Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In International Conference on Artificial Neural Networks, pp. 44­51. Springer, 2011. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5mb model size. arXiv:1602.07360, 2016. Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks. In NIPS, pp. 2017­2025, 2015a. URL http://papers.nips.cc/paper/ 5854-spatial-transformer-networks. Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural information processing systems, pp. 2017­2025, 2015b. Vahid Kazemi and Josephine Sullivan. One millisecond face alignment with an ensemble of regression trees. In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2014. Jyri J. Kivinen and Christopher K. I. Williams. Transformation equivariant boltzmann machines. In ICANN, pp. 1­9, 2011. doi: 10.1007/978-3-642-21735-7 1. URL http://dx.doi.org/10.1007/ 978-3-642-21735-7_1. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. Dmitry Laptev, Nikolay Savinov, Joachim M Buhmann, and Marc Pollefeys. Ti-pooling: transformationinvariant pooling for feature learning in convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 289­297, 2016.
9

Under review as a conference paper at ICLR 2019 Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. Speeding-up con-
volutional neural networks using fine-tuned cp-decomposition. arXiv preprint arXiv:1412.6553, 2014. Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbela´ez, and Luc Van Gool. Convolutional oriented bound-
aries. In European Conference on Computer Vision, pp. 580­596. Springer, 2016. Hong-Wei Ng and Stefan Winkler. A data-driven approach to cleaning large face datasets. In Image Processing
(ICIP), 2014 IEEE International Conference on, pp. 343­347. IEEE, 2014. Edouard Oyallon and Ste´phane Mallat. Deep roto-translation scattering for object classification. In CVPR,
volume 3, pp. 6, 2015. Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via convolutional
sparse coding. The Journal of Machine Learning Research, 18(1):2887­2938, 2017. O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In British Machine Vision Conference,
2015. Qiang Qiu, Xiuyuan Cheng, Robert Calderbank, and Guillermo Sapiro. Dcfnet: Deep neural network with
decomposed convolutional filters. arXiv preprint arXiv:1802.04145, 2018. Roberto Rigamonti, Amos Sironi, Vincent Lepetit, and Pascal Fua. Learning separable filters. In Computer
Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 2754­2761. IEEE, 2013. Ron Rubinstein, Michael Zibulevsky, and Michael Elad. Double sparsity: Learning sparse dictionaries for
sparse signal approximation. IEEE Transactions on signal processing, 58(3):1553­1564, 2010. Uwe Schmidt and Stefan Roth. Learning rotation-aware features: From invariant priors to equivariant descrip-
tors. In CVPR, pp. 2050­2057, 2012a. doi: 10.1109/CVPR.2012.6247909. URL http://dx.doi.org/ 10.1109/CVPR.2012.6247909. Uwe Schmidt and Stefan Roth. Learning rotation-aware features: From invariant priors to equivariant descriptors. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 2050­2057. IEEE, 2012b. Laurent Sifre and Ste´phane Mallat. Rotation, scaling and deformation invariant scattering for texture discrimination. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 1233­1240. IEEE, 2013. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015. Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation equivariant cnns. arXiv preprint arXiv:1711.07289, 2017. Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic networks: Deep translation and rotation equivariance. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), volume 2, 2017. Fa Wu, Peijun Hu, and Dexing Kong. Flip-rotate-pooling convolution and split dropout on convolution neural networks for image classification. CoRR, abs/1507.08754, 2015. URL http://arxiv.org/abs/ 1507.08754. Bolei Zhou, Aditya Khosla, A` gata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2016. Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4961­4970. IEEE, 2017.
10

Under review as a conference paper at ICLR 2019
SUPPLEMENTARY MATERIAL

A COMPUTATION FLOPS CALCULATION IN SECTION 2

When the input and output are both W × W in space, the forward pass in a regular convolutional

Wttlaaakky2eeeMsrn20eLM2eM2d0WsflMM2o,pW0asMn.)2d0LIWtnh2Nae2rr(e2o1atfla+roteipo2MsnL. 0e2Mq)u0ivca2oriLnav2noMtlCu0tNMioNn0

W 2 flops. (Each convolution with a L × L filter operations, plus that the summation over  takes without using bases, an convolutional layer would

In a RotDCF layer, the computation consists of three parts: (1) The inner-product with m bases takes W 2M · 2NK flops. (2) The spatial convolution with the k bases takes KM K · 2L2W 2

flops. (3) The multiplication with a ,(k, m)e-im(k)-im and summation over  , k, m takes

M N(4KKM + 2W 2KKM ) flops (real-valued version). Putting together, the total is 2M W 2K(N + L2K + M NK), and when M is large, the third term dominates and it gives 2M M W 2KKN. Thus the reduction by using bases-decomposed filters is again a factor of

K L2

·

K N

,

and

the

relative

ratio

with

a

regular

CNN

is

about

MM M0 M0

·

.K K N
L2

B PROOFS IN SECTION 3

B.1 THEOREM 3.1

Proof of Theorem 3.1. Note: convolutional layers, but only

The bases expansion impose regularity of

tuhnedfielrtersj,l

,k and m does not affect thus the group-equivariant

the form property

of of

RotDCF follows from the theorem.

Observe that the equivariant relation (6) is equivalent to that Tx(l)[x(l-1)] = x(l)[Tx(l-1)]
for all l, where Tx(0) means Dx(0).
The sufficiency part: When l = 1, by (1),

(A.1)

Tx(1)[x(0)](u, , ) = (

x(0)(u0,tu + v,  )W(1,)(-tv)dv + b(1)()),

 R2

x(1)[Dx(0)](u, , ) = (

x(0)(u0,t(u + v),  )W(1,)(v)dv + b(1)()).

 R2

Sofincetvu0,tv(u. + v) = u0,t + tv, we have that Tx(1)[x(0)] = x(1)[Dx(0)] by a change of variable

When l > 1, by (2),

Tx(l)[x(l-1)](u, , ) = (

x(l-1)(u0,tu + v,  ,  )W(l),(-tv,  -  + t)dvd + b(l)()),

 R2 S1

x(l)[Tx(l-1)](u, , ) = (

x(l-1)(u0,t(u + v),  - t,  )W(l),(v,  - )dvd + b(l)()).

 R2 S1

Again, inserting  -t.

u0,t(u

+

v)

=

u0,t

+

tv,

the

claim

follows

by

changing

variables

tv



v

and

The necessity part: When l = 1, denote the general convolutional filter as w(1)(v;  , , ), and

x(1)(u, , ) = (

Recall that



Tx(1)[x(0)](u, , ) = (


x(1)[Dx(0)](u, , ) = (


x(0)(u + v,  )w(1)(v;  , , )dv + b(1)()).
R2
x(0)(u0,tu + v,  )w(1)(v;  , ,  - t)dv + b(1)()),
R2
x(0)(u0,t(u + v),  )w(1)(v;  , , )dv + b(1)())
R2

11

Under review as a conference paper at ICLR 2019

and then (A.1) with l = 1 holding for any x(0) implies that

x(0)(u0,tu+v,  )w(1)(v;  , , -t)dv =

x(0)(u0,t(u+v),  )w(1)(v;  , , )dv.

 R2

 R2

By that u0,t(u + v) = u0,t + tv, the above equality gives that

w(1)(v;  , ,  - t) = w(1)(t-1v;  , , ), , t  S1.

Let t = , and F ,(v) = w(1)(v;  , , 0), we have that

w(1)(-1v;  , , ) = F ,(v), and this gives that w(1)(v;  , , ) = F ,(v). This proves that (1) is necessary.

When l > 1, consider the general convolutional filter as w(l)(v;  , ,  , ). Using a similar argument, (A.1) implies that

w(l)(v;  , ,  ,  - t) = w(l)(-t 1v;  , ,  + t, ), , t  S1. Let t = , and F ,(v,  ) = w(l)(v;  , ,  , 0), then

F ,(v,  ) = w(l)(- 1v;  , ,  + , ), which gives that w(l)(v;  , ,  , ) = F ,(v,  - ), which proves that (2) is necessary.

B.2 PROPOSITION 3.2

Proposition B.1. For all l,

where

B(l),, C(l),, 2jl D(l),   a(l), FB,

B(l), :=

|W(l),(v, )|dvd, l > 1, B(1,) := |W(1,)(v)|dv

R2 S1

R2

C(l), :=

|v||vW(l),(v, )|dvd, l > 1, C(1,) := |v||vW(1,)(v)|dv

R2 S1

R2

D(l), :=

|vW(l),(v, )|dvd, l > 1, D(1), :=

|v W(1,) (v)|dv

R2 S1

R2

As a result,

Bl, Cl, 2jl Dl  Al,

where

Bl

Ml-1
:= max{sup
  =1

B(l),,

sup


Ml-1 Ml

Ml
B(l),},
=1

Cl

:= max{sup


Ml-1  =1

C(l),,

sup


Ml-1 Ml

Ml
C(l),},
=1

Dl

Ml-1
:= max{sup
  =1

D(l),,

sup


Ml-1 Ml

Ml
D(l),},
=1

and thus (A2) implies that Bl, Cl, 2jl Dl  1 for all l.

(A.2) (A.3)

Proof of Proposition B.1. The proof for the case of l = 1 is the same as Lemma 3.5 and Proposition 3.6 of Qiu et al. (2018). We reproduce it for completeness. When l = 1, it suffices to show that for F (v) = k akk(v),

|F (v)|dv, |v||F (v)|dv, |F (v)|dv  ( µka2k)1/2.
k

(A.4)

12

Under review as a conference paper at ICLR 2019

Rescaling to jl,k in v leads to the desired inequality with the factor of 2jl for D(l),. To prove

(A.4), oFbs2e,rvwehtehraet

F is F

supported on the

2 2

=



k µkak2

unit disk, and then F 1, due to the orthogonality of

|v||F (v)|dv k .



F

1

For l > 1, similarly, we only consider the rescaled filters supported on the unit disk in v. Let F (v, ) = k,m ak,mk(v)m(),   S1, similarly as above, we have that

|F (v, )|dvd,

|v||vF (v, )|dvd 

|vF (v, )|dvd

 (

|vF (v, )|2dvd)1/2

recalling that

d

on

S1

has

the

normalization

of

1 2

.

Again,

|vF (v, )|2dvd =

 k,m µka2k,m due to the orthogonality of k and m. This proves that

|vF (v, )|dvd  ( µka2k,m)1/2,
k,m
which leads to the claim after a rescaling of v.
Remark 1 (Remark to Proposition 3.2). The proposition only needs Bl, defined in (A.3), to be less than 1 for all l, in a rotation-equivariant CNN, which is implied by (A2) by Proposition B.1.

Proof of Proposition 3.2. The proof is similar to that of Proposition 3.1(a) of Qiu et al. (2018). Specifically, in (a), the argument is the same for l = 1, making use of the fact that

|w(v)|dv = |w(v)|dv,   S1

and

S1 d

=

1

due

to

the

normalization

of

1 2

.

For l > 1, the same technique proceeds with

the new definition of B(l), as in (A.2) which involves the integration of omitted.

S1 (· · · )d. The detail is

To prove (b), Suppose that

we firstly verify it holds for (l -

1th)a, tcoxn(0ls)idoenrlyl

depends > 1,

on

.

When

l

=

1,

x(01)(u,

,

)

=

(b(1)()).

x0(l)(u, , ) = (

x(0l-1)(u + v,  + ,  )W(l),(v, )dvd + b(l)())

 S1 R2

= ( x0(l-1)( )

W(l),(v, )dvd + b(l)())

 S1 R2

= ( x(0l-1)( ) ·

W(l),(v , )dv d + b(l)())

 S1 R2

= x(0l)().

Thus x0(l)(u, , ) = x(0l)() for all l (without index  for l = 1). The rest of the argument follows

from that where the

xc(l) = inequality

xis(lb)y-(ax)0(.l)

=

x(l)[x(l-1)] - x(l)[x(0l-1)]



x(l-1) - x0(l-1)

=

xc(l-1) ,

B.3 THEOREM 3.3 Proposition B.2. In a RotDCF Net, under (A1), (A2), (A3), c1 = 4, for any l,
x(l)[D  D x(0)] - T  D x(l)[x(0)]  2c1l| | x(0) , where D only acts on the space variable u of x(l) similar to (9).

13

Under review as a conference paper at ICLR 2019

Proof of Proposition B.2. We firstly establish that for all l,

x(l)[T  D x(l-1)] - T  D x(l)[x(l-1)]  2c1| | xc(l-1) ,

(A.5)

where T is replaced by D if applies to x(0) which does not have index . This is because that

x(l)[T  D x(l-1)] = Tx(l)[D x(l-1)]

by Theorem 3.1, and that

Tx(l)[D x(l-1)] - T  D x(l)[x(l-1)] = x(l)[D x(l-1)] - D x(l)[x(l-1)]
by the definition of T (a rigid rotation in u, and a translation in ). This term can be upper bounded by c1(Bl + Cl)| | x(cl-1) (Lemma B.3), which leads to the desired bound under (A2) by Proposition B.1.
The rest of the proof is similar to that of Proposition 3.3 of Qiu et al. (2018): Write x(l)[D  D x(0)] - T  D x(l)[x(0)] as the sum of the differences x(l)[x(j)[D  D x(j-1)]] - x(l)[T  D x(j)[x(j-1)]] for j = 1, · · · , l. The norm of the j-th term is bounded by x(j)[D  D x(j-1)] - T  D x(j)[x(j-1)] due to Proposition 3.2 (a), which, by applying (A.5) together with Proposition 3.2 (b), can be bounded by 2c1| | x(0) . Summing over j gives the claim.

Proof of Theorem 3.3. The proof is similar to that of Theorem 3.8 of Qiu et al. (2018). With the bound in Proposition B.2, it suffices to show that
T  D x(L)[x(0)] - Tx(L)[x(0)]  c22-jL | | x(0) .
By the definition of T, the l.h.s. equals D x(L)[x(0)] - x(L)[x(0)] , which can be shown to be less than c2| |DL xc(l-1) by extending the proof of Proposition 3.4 of Qiu et al. (2018), similar to the argument in proving Lemma B.3. The desired bound then follows by that DL  2-jL AL  2-jL (Proposition B.1 and (A2)) and that x(cl-1)  x(0) (Proposition 3.2 (b)).
Lemma B.3. In a rotation-equivariant CNN, Bl, Cl defined as in (A.3), under (A1), (A3), for all l > 0, with c1 = 4, xc(l) as in Proposition 3.2,
x(l)[D x(l-1)] - D x(l)[x(l-1)]  c1(Bl + Cl)| | x(cl-1) .

Proof of Lemma B.3. The proof is similar to that of Lemma 3.2 of Qiu et al. (2018). Specif-

ically, when l = 1, the argument is the same, making use of the fact that |w(v)|dv =

|w(v)|dv,

  S1 and

S1 d

=

1 due to the normalization of

1 2

.

When l

>

1, the same

technique applies by considering the joint integration of R2 S1 (· · · )dvd instead of just dv. The

only difference is in using the new definitions of B(l), and C(l), for l > 1 as in (A.2), both of which

involve the integration of S1 (· · · )d. The detail is omitted.

Conv-3 CNN-M c5x5x1xM ReLu ap2x2 c5x5xM x2M ReLu ap2x2 c5x5x2M x4M ReLu ap2x2 fc64 ReLu fc10 softmax-loss

Conv-3 RotDCF-M rc5x5x1xM ReLu ap2x2 rc5x5xNxM x2M ReLu ap2x2 rc5x5xNx2M x4M ReLu ap2x2 fc64 ReLu fc10 softmax-loss

Table A.1: Conv-3 network architectures used in rotMNIST, M = 32, 16 or 8. cLxLxM xM stands for a convolutional layer of patch size LxL and input (output) channel M (M ). apLxL stands for LxL averagepooling. In the RotDCF Net, rcLxLxNxM xM stands for a rotation-indexed convolutional layer, which includes N-times many number of filters except for the 1st rc layer (see Section 2). Batch-normalization layers (not shown) are used during training.

14

Under review as a conference paper at ICLR 2019

VGG-16 CNN-M c3x3x3xM ReLu c3x3xM xM ReLu c3x3xM xM ReLu c3x3xM xM ReLu c3x3xM xM ReLu mp2x2 c3x3xM x2M ReLu c3x3x2M x2M ReLu c3x3x2M x2M ReLu c3x3x2M x2M ReLu mp2x2 c3x3x2M x4M ReLu c3x3x4M x4M ReLu c3x3x4M x4M ReLu c3x3x4M x4M ReLu mp2x2 fc128 ReLu fc10 softmax-loss

VGG-16 RotDCF-M rc3x3x3xM ReLu rc3x3xNxM xM ReLu rc3x3xNxM xM ReLu rc3x3xNxM xM ReLu rc3x3xNxM xM ReLu mp2x2 rc3x3xNxM x2M ReLu rc3x3xNx2M x2M ReLu rc3x3xNx2M x2M ReLu rc3x3xNx2M x2M ReLu mp2x2 rc3x3xNx2M x4M ReLu rc3x3xNx4M x4M ReLu rc3x3xNx4M x4M ReLu rc3x3xNx4M x4M ReLu mp2x2 fc128 ReLu fc10 softmax-loss

Table A.2: VGG-16-like network architectures used in CIFAR10, M = 64 or 32. mpLxL stands for LxL max-pooling, and other notations similar to Table A.1.

RotDCF ConvAE

rc5x5x1x8 ReLu ap2x2

rc5x5xNx8x16 ReLu ap2x2

rc5x5xNx16x32 ReLu ap2x2

rc5x5xNx32x32 ReLu

 Encoded representation

fc128 ReLu ct5x5x128x16N ReLu

ct5x5x16Nx8N (upsample 2x2) ReLu

ct5x5x8Nx1 (upsample 2x2) Eucledian-loss

Table A.3: Convolutional Auto-encoder network used in the image reconstruction experiment. RotDCF layers are used in the encoder network, with N = 16, K = 5, K = 5 (K = 8, K = 15 in the last RotDCF layer), and transposed-convolutional layers with upsampling are used in the decoder net. apLxL stands for LxL average-pooling, and other notations similar to Table A.1.

C EXPERIMENTAL DETAILS IN SECTION 4
C.1 OBJECT RECOGNITION WITH ROTMNIST AND CIFAR10 In the experiments on rotMNIST dataset, the network architecture is shown in Table A.1. Stochastic gradient descent (SGD) with momentum is used to train 100 epochs with decreasing learning rate from 10-2 to 10-4. In the experiments on CIFAR10 dataset, the VGG16-like network architecture is shown in Table A.2. SGD with momentum is used to train 100 epochs with decreasing learning rate from 10-2 to 10-4.
C.2 CONVOLUTIONAL AUTO-ENCODER FOR IMAGE RECONSTRUCTION The network architecture is shown in Table A.3. The network is trained on 50,000 training samples, the training set is augmented by rotating each sample at 8 random angles, producing 400k training set. The network is trained for 10 epochs, where the learning rate decreases from 10-3 to 10-6.
C.3 FACE RECOGNITION ON FACESCRUB To facilitate the evaluation on both known and unknown subjects, we select the first 500 of the 530 identities as our training subjects. The remaining 30 subjects are used for validating out of sample performance, namely the unknown subjects. The experiment on unknown subjects is critical for face models to generate over unseen people. For both known and unknown subjects, we hold 10 images from each person as the probe images, and the remaining as the gallery images. The images are

CNN c5x5x3x32 ReLu mp2x2 c5x5x32x64 ReLu mp2x2 c5x5x64x128 ReLu c5x5x128x128 ReLu mp2x2 c5x5x128x256 ReLu c5x5x256x256 ReLu mp2x2 c5x5x256x256 ReLu c5x5x256x256 ReLu gap13x13 fc softmax

RotDCF rc5x5x3x16 ReLu mp2x2 rc5x5xNx16x32 ReLu mp2x2 rc5x5xNx32x64 ReLu c5x5x64x64 ReLu mp2x2 rc5x5xNx64x128 ReLu c5x5x128x128 ReLu mp2x2 rc5x5xNx128x128 ReLu c5x5x128x128 ReLu gap13x13 fc softmax

Table A.4: Network architectures used in face experiments, notations as in Table A.1. In the RotDCF Net, N = 8, K = 5, K = 5.

15

Under review as a conference paper at ICLR 2019

Figure A.1: Trained filters in RotDCF (Left), and in ordinary rotation-equivariant CNNs (Middle) without truncated decomposition. (Right) Reconstructed digits from row-rotated codes where the AE net is trained with larger K and K, showing more overfitting than Figure 3 in the paper.

preprocessed by aligning facial landmarks using Kazemi & Sullivan (2014). and crop the aligned face images to 112 × 112 with color. Thus, both our CNN and RotDCF models are trained with near-frontal and upright-only face images.

The network architecture is shown in Table A.4. According to the formula in Section 2, the number

of

trainable

parameters

in

the

RotDCF

Net

is

about

(

1 2

)2

·

K L2

·

K

=

1 4

of

that

of

the

CNN.

C.4 EFFECTS OF REGULARIZATION BY FILTER DECOMPOSITION

Sample trained filters in the reconstruction experiment are shown in Figure A.1, which shows that RotDCF obtains smoother trained filters as a result of the regularization applied by the bases truncation. In the reconstruction experiment, increasing the number of bases degrades the synthesis (Right in Figure A.1). Such improved stability due to the truncated decomposition supports the analysis in Section 3.2.

16

