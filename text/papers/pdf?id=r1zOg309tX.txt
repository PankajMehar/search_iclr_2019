Under review as a conference paper at ICLR 2019
UNDERSTANDING THE EFFECTIVENESS OF LIPSCHITZCONTINUITY IN GENERATIVE ADVERSARIAL NETS
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we investigate the underlying factor that leads to the failure and success in training of GANs. Specifically, we study the property of the optimal discriminative function f (x) and show that f (x) in most GANs can only reflect the local densities at x, which means the value of f (x) for points in the fake distribution (Pg) does not contain any information useful about the location of other points in the real distribution (Pr). Given that the supports of the real and fake distributions are usually disjoint, we argue that such a f (x) and its gradient tell nothing about "how to pull Pg to Pr", which turns out to be the fundamental cause of failure in training of GANs. We further demonstrate that a well-defined distance metric (including Wasserstein distance) does not necessarily ensure the convergence of GANs. Finally, we propose Lipschitz-continuity condition as a general solution and show that in a large family of GAN objectives, Lipschitz condition is capable of connecting Pg and Pr through f (x) such that the gradient xf (x) at each sample x  Pg points towards some real sample y  Pr.
1 INTRODUCTION
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), as a new way of learning generative models, have recently shown promising results in various challenging tasks. Although GANs are popular and widely-used (Isola et al., 2016; Brock et al., 2016; Nguyen et al., 2016; Zhu et al., 2017; Karras et al., 2017), they are notoriously hard to train (Goodfellow, 2016). The underlying obstacles, though have been widely studied (Arjovsky & Bottou, 2017; Lucic et al., 2017; Heusel et al., 2017a; Mescheder et al., 2017; 2018), are still not fully understood. In this paper, we study the convergence of GANs from the perspective of the optimal discriminative function f (x). We show that in original GAN and its most variants, f (x) is a function of densities at the current point x but does not reflect any information about the densities/locations of other points in the real and fake distributions. Moreover, Arjovsky & Bottou (2017) states that the supports of real and fake distributions are usually disjoint. In this paper, we argue that the fundamental cause of failure in training of GANs (Section 2.1) stems from the combination of the above two facts. The generator uses xf (x) as the guidance for updating the generated samples, but xf (x) actually tells nothing about where Pr is. Therefore, the generator is not guaranteed to converge to the case Pg = Pr. Accordingly, Arjovsky et al. (2017) proposed the Wasserstein distance (in its dual form) as an alternative objective, which can properly measure the distance between two distributions no matter whether their supports are disjoint. However, as shown in Section 2.3, when the supports of the Pg and Pr are disjoint, the gradient of f (x) in Wasserstein distance also does not reflect any useful information about other points in Pr. Thus it is also unable to guarantee the convergence. Based on this observation, we provide further investigation in Section 2.4 and argue that measuring the distance properly does not necessarily imply that the gradient is well-defined in terms of xf (x). In Section 3, we propose incorporating Lipschitz-continuity condition in the objectives of GANs as a general solution, and prove that in a broad family of discriminator objectives, Lipschitz-continuity condition can build strong connections between Pg and Pr through f (x) such that when the supports of Pr and Pr are disjoint, xf (x) at each sample x  Pg will point towards some real sample y  Pr. This guarantees that Pg is moving towards Pr at every step.
1

Under review as a conference paper at ICLR 2019

We extend our discussion on f (x) and xf (x) to the case where the supports of Pg and Pr are overlapped, and present the general arguments in Section 4.1. Subsequently, in Section 4.2, we show that the locality of f (x) and xf (x) in traditional GANs turns out to be an intrinsic cause to mode collapse. Finally, in Section 4.3, we explain the reason of empirical success of traditional GANs
under the circumstance that they have no convergence guarantee.

Table 1: Comparison of different objectives in GAN models.

JS-Divergence
Least Square Wasserstein-1 with Lip1
µ-Fisher IPM

 - log((-x))
(x - )2 x x

 - log((x))
(x - )2 -x -x

F {f : Rn  R}
{f : Rn  R} {f : Rn  R, f lip  1} {f : Rn  R, Exµ f (x) 2  1}

f (x)

log

Pr (x) Pg (x)

·Pg (x)+·Pr (x)

Pg (x)+Pr (x)

N/A

1 Pr (x)-Pg (x)

Fµ(Pr ,Pg )

µ(x)

2 THE FUNDAMENTAL CAUSE OF FAILURE IN TRAINING OF GANS

Typically, the objectives of GANs can be formulated as follows:

min JD
f F
min JG
gG

EzPz [(f (g(z)))] + ExPr [(f (x))], EzPz [(f (g(z)))],

(1)

where Pz is the source distribution of the generator (usually a Gaussian distribution) in Rm and Pr is the target (real) distribution in Rn. The generative function g : Rm  Rn learns to output samples that shares the same dimension as Pr, while the discriminative function f : Rn  R learns to output a score indicating the authenticity of a given sample. We denote the implicit distribution

of the generated samples as Pg, i.e., Pg = g(Pz).

F and G denote discriminative and generative function spaces parameterized by neural networks, respectively; functions , , : R  R are loss metrics. We list the choices of F,  and  in some representative GAN models in Table 1, where we denote f  = arg minfF JD.

In these GANs, the gradient that the generator receives from the discriminator with respect to a

generated sample x  Pg is

xJG(x) = f(x)(f (x)) · xf (x).

(2)

In Eq. (2), the first term f(x)(f (x)) is a step-related scalar that is out of the scope of our discussion in this paper; the second term xf (x) is a vector indicating the direction that the generator should follow for optimizing on sample x.

2.1 xf (x) ON Pg DOES NOT REFLECT USEFUL INFORMATION ABOUT Pr
In this section, we will show that when the supports of Pg and Pr are disjoint, xf (x) in traditional GANs does not reflect any useful information about Pr, and Pg is not guaranteed to converge to Pr. We argue that this is the fundamental cause of non-convergence and instability in traditional GANs.1

2.1.1 THE ORIGINAL GAN AND LEAST-SQUARES GAN

In the simplest case of Eq. (1), e.g., the original GAN (Goodfellow et al., 2014) and Least-Squares GAN (Mao et al., 2016), there is no restriction on F. Therefore, f (x) for each point x is indepen-

dent of other points, and we have

f (x) = arg min Pg(x) · (f (x)) + Pr(x) · (f (x)), x.
f (x)R

(3)

Since we assume supports of Pg and Pr are disjoint, we further have

f (x) = arg minf(x)R Pg(x) · (f (x)), x  Pg, arg minf(x)R Pr(x) · (f (x)), x  Pr.

(4)

1In this paper, traditional GANs mainly refers to the original GAN and Least-Squares GAN, where f (x) depends only on the densities Pg(x) and Pr(x). Broadly, it refers to all GANs where f (x) does not reflect information about the locations of the other points in Pg and Pr, such as the Fisher GAN.

2

Under review as a conference paper at ICLR 2019

(a) Original GAN

(b) Wasserstein distance

(c) Lipschitz constraint

Figure 1: In traditional GANs, f (x) is only defined on the supports of Pg and Pr and its values do not reflect any information about the locations of other points in Pg and Pr. Therefore, they have no guarantee on the convergence. Wasserstein distance suffers from the same problem. GANs under Lipschitz constraint builds connection between Pg and Pr, where xf (x) pulls Pg towards Pr.

For x  Pg, the value of f (x) is irrelevant to Pr. Since Pg and Pr are disjoint2, xf (x) for x  Pg also tells nothing about Pr. In consequence, the generator can hardly learn useful information and is not guaranteed to converge to the case where Pg = Pr.

2.1.2 THE FISHER GAN

Mroueh et al. (2017) prove that the optimal f  of µ-Fisher IPM Fµ(Pr, Pg), the objective used in Fisher GAN (Mroueh & Sercu, 2017), has the following form

f (x) = 1 Pr(x) - Pg(x) .

Fµ(Pr, Pg)

µ(x)

(5)

where µ is a distribution whose support covers Pr and Pg. Given Pr and Pg are disjoint, we have

1

 

Fµ

(Pr

,Pg

)

-Pg (x) µ(x)

,



f (x) =

1 Fµ(Pr ,Pg )

Pr (x) µ(x)

,



0,

x  Pg; x  Pr; otherwise.

(6)

Note

that

the

scalar

1 Fµ(Pr ,Pg )

is

a

constant.

Eq.

(6)

also

defines

f (x)

on

Pg

and

Pr

independently.

Therefore, for x  Pg, f (x) and xf (x) tell nothing about Pr.

2.2 CONNECTION TO GRADIENT VANISHING
The non-convergence problem of the original GAN has once been considered as the gradient vanishing problem. In (Goodfellow et al., 2014), it is addressed by using an alternative objective for the generator. However, it actually only changes the scalar f(x)(f (x)) while the aforementioned problem in xf (x) still exists. The least-squares GAN (Mao et al., 2016) is proposed to address the gradient vanishing problem, but it also focuses on f(x)(f (x)) basically. As we have discussed, the least-squares GAN also belongs to traditional GANs, which is not guaranteed to converge when Pg and Pr are disjoint.
Arjovsky et al. (2017) provided a new perspective on understanding the gradient vanishing problem. They argued that gradient vanishing stems from the ill-behaving of traditional metrics, i.e., the distance between Pg and Pr remains constant when they are disjoint. Wasserstein distance is thus proposed as an alternative metric, which can properly measure the distance between two distributions no matter they are disjoint or not. However, as we will show in the next sections, Wasserstein distance still suffers from the same problem on xf (x) as traditional GANs.
In summary, gradient vanishing is about the scalar term f(x)(f (x)) in xJG(x) or the overall scale of xJG(x), and in this paper we investigate its direction xf (x), where the problem is so fundamental and challenging that even the Wasserstein distance, which can properly measure the distance for disjoint distributions, also suffers from the same issue.
2Here and later, "two distributions are disjoint" means that their supports are disjoint.

3

Under review as a conference paper at ICLR 2019

2.3 WASSERSTEIN DISTANCE SUFFERS FROM THE SAME PROBLEM

The (1st-)Wasserstein distance is a distance function defined between two probability distributions:

W1(Pg, Pr)

=

inf
(Pg ,Pr )

E(x,y)(Pg ,Pr )

[d(x, y)],

(7)

where (Pr, Pg) denotes the collection of all probability measures with Pr and Pg being the marginal distributions of the first and second factors respectively. Since solving it in the primal
form (Eq. (7)) is burdensome, Wasserstein distance is usually solved in its dual form (Villani, 2008):

W1(Pg, Pr) = supf ExPg [f (x)] - ExPr [f (x)], s.t. f (x) - f (y)  d(x, y), x  Pg, y  Pr.

(8)

We leave the detailed discussion on the relationship between Lipschitz-continuity and Wasserstein distance in Section 4.4. Eq. (8) is the exact form of Wasserstein distance in duality, in which we remove the unnecessarily stronger Lipschitz constraint and keep a pure form of Wasserstein distance. This is to demonstrate that a well-defined distance metric may also suffer from the same problem in xf (x) and does not necessarily ensure the convergence of GANs.
We now study the optimal discriminative function f (x) of Wasserstein distance. Since there is generally no closed-form solution for f (x) in Wasserstein distance, we use an illustrative example for demonstration here, but the conclusion is general. Let Z  U [0, 1] be a uniform variable on interval [0, 1], Pg be the distribution of (1, Z)  R2, and Pr be the distribution of (0, Z)  R2, as shown in Figure 1. According to Eq. (8), one of the optimal f  is as follows

f (x) = 0 1

x  Pg, x  Pr.

(9)

Though having the constraint "f (x) - f (y)  d(x, y), x  Pg, y  Pr", Wasserstein distance also only defines the value of f (x) on the supports of Pg and Pr, and the values of f (x) on Pg contain no useful information about the location of Pr. Therefore, if Pg and Pr are disjoint, xf (x) hardly provides useful information to the generator about "how to change Pg into Pr" and the generator is not guaranteed to converge to the case Pg = Pr. It is worth noticing that in Wasserstein distance, the value of f (x) on the region other than the supports of Pg and Pd is still undefined (see Figure 1b).

2.4 "Pg = Pr IS THE OPTIMUM" NOT NECESSARILY GUARANTEES THE CONVERGENCE
The objectives of GANs are usually defined as (or proved equivalent to) minimizing a distance metric between Pg and Pr, which implies that "Pg = Pr is the unique global optimum," and is in accordance with the final goal of the generative model, i.e., estimating the distribution of real samples. In this section, however, we emphasize that "Pg = Pr is the optimum" is not enough for guaranteeing the convergence of GANs.
Given an objective is convex with respect to Pg and holds the property that "Pg = Pr is the unique optimum," the convergence of GANs is guaranteed if only they are directly optimizing Pg, i.e., "dragging" Pg to Pr. However, what they actually do is using xf (x) as the guidance to optimize the generated samples. As shown in previous sections, when Pg and Pr are disjoint, xf (x), i.e., the direction that the generator follows for updating the generated samples, tells nothing about how to pull Pg to Pr. In this way, the convergence of GANs are not necessarily guaranteed. xf (x) indeed indicates the direction of decreasing the objective in terms of the current f (x), but updating x to make the value of f (x) increase / decrease does not necessarily imply that Pg is getting closer to Pr. Recall that in the failure case of Wasserstein distance in the above section, the values of f (x) on Pg is 0, while the values of f (x) around Pg is undefined.
In conclusion, "Pg = Pr is the optimum" does not guarantee the convergence and sample updating according to xf (x) does not necessarily decrease the distance between Pg and Pr. Therefore, if we use xf (x) for updating the generator, it is necessary to make xf (x) aware of how to pull Pg to Pr. Alternative strategies actually exist, for example, Seguy et al. (2017) use the optimal transport plan between Pg and Pr to update the generator; however, their results tend to be blurry. In the next section, we will introduce the Lipschitz constraint as a general solution for making xf (x) well-behaving and guaranteeing the convergence of xf (x)-based GANs.

4

Under review as a conference paper at ICLR 2019

3 A GENERAL SOLUTION: LIPSCHITZ CONDITION

Lipschitz-continuity constraint becomes popular in GANs recently. The first attempt of introducing Lipschitz constraint in GANs is the Wasserstein GAN (Arjovsky et al., 2017), and later Lipschitz constraint is extended to other objectives such as (Kodali et al., 2017; Fedus et al., 2017; Miyato et al., 2018), achieving great success. In this section, we explain the significance of Lipschitz constraint when introduced into the objective of the discriminator. In a nutshell, under a board family of GAN objectives, Lipschitz constraint is able to connect Pg and Pr through f (x) such that when Pr and Pg are disjoint, xf (x) for each generated sample x  Pg will point towards some real sample y  Pr, which guarantees the trend that "Pg is getting closer to Pr at every step".

3.1 THE MAIN RESULT

A function f : X  Y is k-Lipschitz continuous if it satisfies the following property:

dY (f (x), f (y))  k · dX (x, y),  x, y  X,

(10)

where dX and dY are distances metrics in domains X and Y , respectively. The smallest constant k is called the Lipschitz constant. In this paper (and most GAN papers), dX and dY are defined as Euclidean distance.3 We let y-x denote Euclidean distance.

As proved by Gulrajani et al. (2017), when the Lipschitz condition is combined with Wasserstein distance, we have the following property if f (x) is differentiable, then

Pr

xf (xt) =

y-x y-x

= 1, for (x, y)  ,

(11)

where xt = tx + (1 - t)y, 0  t  1, and  is the optimal  in Eq. (7). The meaning of this

proposition is two-fold: (i) for each x  Pg, there exists a y  Pr such that xf (xt) =

y-x y-x

, for

all linear interpolations xt between x and y; (ii) these (x, y) pairs match the optimal coupling .

Next we introduce our theorem on the Lipschitz condition. It turns out when combining the Lipschitz condition with generalized objectives, Property-(i) still holds and Property-(ii) is naturally dismissed as it is now not restricted to Wasserstein distance.

Theorem 1. Let P¯g and P¯r denote the supports of Pg and Pr, respectively. Assume f  =

arg minf [JD +  · k(f )2], where k(f ) is the Lipschitz constant of f . If (x) and (x) in JD

satisfy

 (x) > 0,  (x)  0, 

 (x) < 0,  (x)  0,

(12)

  a,  (a) +  (a) = 0,

then we have that

(a) x  P¯g  P¯r, y=x  P¯g  P¯r such that |f (y)-f (x)| = k(f ) · x-y or f(x)JD(x) = 0;

(b) x  P¯g  P¯r - P¯g  P¯r, y=x  P¯g  P¯r such that |f (y)-f (x)| = k(f ) · x-y ;

(c) if P¯g = P¯r and Pg = Pr, then x, y=x such that |f (y)-f (x)| = k(f ) · x-y ;

(d) the only Nash Equilibrium of JD +  · k(f )2 is reached when Pg = Pr, where k(f ) = 0.

The above theorem states that when the Lipschitz condition is combined with an objective that satisfies Eq. (12), then: (a) for the optimal discriminative function f (x) at any point x  P¯g  P¯r, it either is bounded by the Lipschitz constant or JD(x) holds a zero-gradient with respect to f (x); (b) for any point that only appears in P¯g or P¯r, there must exist a point that bounds this point in terms of |f (y)-f (x)| = k(f ) · x-y , because for these points, JD(x) will never get zero gradient with respect to f (x) as we prove in the Appendix G; (c) when Pg and Pr are totally overlapped, as long as Pg still not converges to Pr, there exists at least one pair (x, y) that bounds each other; (d) the only Nash Equilibrium among Pg and f (x) under this objective is "Pg = Pr with k(f ) = 0." The formal proof is in Appendix G.
3Actually, we argue that the distance metrics must be Euclidean distance in GANs. See Appendix D.

5

Under review as a conference paper at ICLR 2019

The Wasserstein distance, i.e., (x) = (-x) = x is one instance that satisfies Eq. (12); and it is a very special case, which holds  (x) = 0 and  (x) = 0. Eq. (12) is actually quite generaland there exists many other settings, e.g., (x) = (-x) = - log((-x)), (x) = (-x) = x + x2 + 1, (x) = (-x) = exp(x), etc. Generally, it is feasible to set (x) = (-x). As such, to build a new objective, one only needs to find a function that is increasing and has non-decreasing derivative. In addition, all linear combinations of feasible (, ) pairs also lie in the family.
It is worth noting that k(f ) is also optimized here and it is actually necessary for Property-(c) and Property-(d). This is the key difference when the Lipschitz condition is extended to general objectives. The underlying reason for the need of also minimizing k(f ) comes from the existence of the case "f(x)JD(x) = 0 for Pg(x) = Pr(x)", which does not hold when the objective is Wasserstein distance. Minimizing k(f ) guarantees that the only Nash Equilibrium is "Pg = Pr with k(f ) = 0." On the other hand, if k(f ) is not minimized towards zero, Wasserstein distance based GANs are not guaranteed to have zero gradient f(x) at the convergence state Pg = Pr. It indicates that minimizing k(f ) is also beneficial to the Wasserstein GAN (Arjovsky et al., 2017).

3.2 LIPSCHITZ CONDITION CONNECTS Pg AND Pr VIA f (x)
From Theorem 1, we know that for any point x, as long as JD(x) does not hold a zero gradient with respect to f (x), f (x) must be bounded by another point y such that |f (y) - f (x)| = k(f ) · x-y . We here further clarify that, when there is a bounding relationship, it must involve both real sample(s) and fake sample(s). More formally, we have Theorem 2. If f  = arg minf [JD +  · k(f )2], then
· x  P¯g, if z=x  P¯g  P¯r such that |f (x)-f (z)| = k(f ) · x-z , then y=x  P¯r such that f (y)-f (x) = k(f ) · x-y ,
· y  P¯r, if z=y  P¯g  P¯r such that |f (z)-f (y)| = k(f ) · z-y , then x=y  P¯g such that f (y)-f (x) = k(f ) · x-y .
The intuition behind the above theorem is that samples from the same distribution, e.g., the fake samples, will not bound each other. It is worth noticing that there might exist a chain of bounding relationships that involves a dozen of fake samples and real samples, and these points all lie in the same line and bounds each other. Under the Lipschitz condition, the bounded line in the value surface of f  is the basic building block that connects Pg and Pr, and each fake sample lies in one of the bounded lines. Next we will further interpret the implication of bounding relationship and show that it guarantees meaningful xf (x) for all involved points.

3.3 LIPSCHITZ CONDITION ENSURES THE CONVERGENCE OF xf (x)-BASED GANS

Recall that the proposition in Eq. (11) states that xf (xt) =

y-x y-x

.

This is actually a direct

consequence of bounding relationship between x and y. We formally state it as follows:

Theorem 3. Assume f (x) is differentiable and k-Lipschitz continuous. If f (y)-f (x) = k· x-y

x = y, then xf (xt) = k ·

y-x x-y

, where xt

= tx + (1 - t)y for 0  t  1.

In other words, if two points x and y bound each other in terms of f (y)-f (x) = k · x-y , there is a straight line between x and y in the value surface of f . Any point in this line holds the maximum gradient slope k, and the direction of these gradient all point towards the x  y direction. The proof is provided in Appendix D. Combining Theorem 1 and Theorem 2, we can conclude that when Pg and Pr are disjoint, xf (x) for each sample x  Pg points to a sample y  Pr, which guarantees that Pg is moving towards Pr.

In fact, Theorem 1 provides further guarantee on the convergence. Property-(b) implies that for any x  Pg that does not lies in Pr, xf (x) points to some real sample y  Pr. In the fully overlapped case, according to Property-(c), unless Pg = Pr, there exists a pair (x, y) in bounding relationship and xf (x) pulls x towards y. Property-(d) guarantees that the only Nash Equilibrium is "Pg = Pr."

6

Under review as a conference paper at ICLR 2019

p(x) f *(x) p(x) f *(x) p(x) f *(x)

0.4 real 0.3 fake 0.2 0.1 0.0
10 5

x0

80 60 40 20 0 5 10

0.4 0.3 0.2 0.1 0.0
10 5 x0

1.0 0.5 0.0
0.5 1.0 5 10

0.4 0.3 0.2 0.1 0.0
10 5 x0

0.2 0.1 0.0
0.1 0.2 0.3 5 10 0.4

(a) Original GAN

(b) Least Square GAN

(c) Fisher GAN with uniform µ

Figure 2: The source of Mode Collapse. In traditional GANs, f (x) is a function of the local densities Pg(x) and Pr(x). Given f (x) is an increasing function of Pr(x) and decreasing function of Pg(x), when fake samples get close to a mode of the Pr, xf (x) move them towards the mode.

4 EXTENSIONS AND DISCUSSIONS
4.1 FROM DISJOINT CASE TO OVERLAPPING CASE
In Section 2, we discuss the problem of f (x) and xf (x) in the case where Pg and Pr are disjoint. In this section, we extend our discussion to the overlapping case. In the disjoint case, we argue that "f (x) on Pg does not reflect any information about the location of other points in Pr" will lead to an unfeasible xf (x) and thus non-convergence. In the overlapping and continuous case, things are actually different, f (x) around each point is also defined, and its gradient xf (x) now reflects the local variation of f (x).
For most traditional GANs, f (x) mainly reflects the local information about the density Pg(x) and Pr(x). However, it is worth noting that f (x) is usually an increasing function with respect to Pr(x) while a decreasing function with respect to Pg(x). For instance, f (x) in the original GAN is log Pr(x)/Pg(x). Optimizing the generator according xf  (x) will move sample x towards the direction of increasing f (x). Because f (x) positively correlates with Pr(x) and negatively correlated with Pg(x), it in sense means x is becoming more real. However, such a local greedy strategy turns out to be a fundamental cause of mode collapse.
4.2 THE CAUSE OF MODE COLLAPSE: THE LOCALITY OF f (x) AND 
Mode collapse is a notorious problem in GANs' training, which refers to the phenomenon that the generator only learns to produce part of Pr. Many literatures try to study the source of mode collapse (Che et al., 2016; Metz et al., 2016; Kodali et al., 2017; Arora et al., 2017) and measure the degree of mode collapse (Odena et al., 2016; Arora & Zhang, 2017).
The most recognized cause of mode collapse is that, if the generator is much stronger than the discriminator, it may learn to only produce the sample(s) in the local or global maximum of f (x) for the current discriminator. This argument is true for most of GAN models. However, from our perspective on f (x) and its gradient, there actually exists a much more fundamental cause of mode collapse, i.e., the locality of f (x) in traditional GANs and the locality of gradient operator .
In traditional GANs, f (x) is a function of local densities Pg(x) and Pr(x), which is local, and the gradient operator  is also a local operator. As the result, xf (x) only reflects its local variations and cannot capture the statistic of Pr and Pg that is far from itself. If f (x) in the surrounding area of x is well-defined, xf (x) will move x towards the nearby location where the value of f (x) is higher. It does not take the global status into account.
The typical result is that when fake samples get close to a mode of the Pr, they move towards the mode and get stuck there (due to the locality). Assume Pr consists of two Gaussian distributions (A and B) that are distant from each other, while the current Pg is uniformly distributed over its support and close to real Gaussian A. In this case, xf (x) of all fake samples will point towards the center of Gaussian A. If Pg is a Gaussian with the same standard deviation as Gaussian A, xf (x) in original GAN and Least-Square GAN shows almost identical behaviors, which is illustrated in Figure 2. In Fisher GAN, if µ(x) is uniform, the case is even worse: a large amount of points that are relatively far from Gaussian A will move away from A (but the direction is not necessarily towards B, though in our 1-D case it is).

7

Under review as a conference paper at ICLR 2019

This observation again supports our argument: "Pg = Pr is optimum" is not enough and the validity of xf (x) is also necessary (even if Pg and Pr is continuous and overlapped). As far as we know, this is the first work that provides a clear explanation on this cause of mode collapse.

4.3 EXPLANATION ON THE EMPIRICAL SUCCESS OF TRADITIONAL GANS
Though traditional GANs does not have any guarantee on its convergence, it has already achieved its great success. The reason is that having no guarantee does not mean it cannot converge. It turns out extensive parameter-tuning actually increases the probability of the convergence.
As shown in Appendix A, hyper-parameters are important in influencing the value surface of f (x). Some typical settings (e.g., simplified neural network architecture, relu or leaky relu activation, relatively high learning rate, Adam optimizer, etc.) tend to form a relatively smooth value surface (e.g., monotonically increasing from Pg to Pr), making xf (x) much more meaningful. That is, one can find these settings, where xf (x) or xf (x) is more favourable, to enable traditional GAN to work. In opposite, we have tried highly-nonlinear activation such as swish (Ramachandran et al., 2018) in the discriminator. It turns out traditional GANs is very likely to fail. In contrast, our proposed Lipschitz constraint based GAN is compatible with highly-nonlinear activation. Another important empirical technique is to delicately balance the generator and the discriminator or limit the capacity of the discriminator. This is to avoid the fatal optimal f (x). All these could possibly make traditional GANs work. However, the consequence is that these GANs are very sensitive to hyper-parameters and hard to use.

4.4 LIPSCHITZ CONDITION IS STRONGER THAN THE ONE IN WASSERSTEIN DISTANCE

Most literature directly writes the dual form of Wasserstein distance with the Lipschitz condition. However, it is worth noticing that the Lipschitz condition is actually stronger than the necessary one in the dual form of Wasserstein distance. Recall that the necessary constraint in dual form of Wasserstein distance (Section 2.3) is

f (x) - f (y)  d(x, y), x  Pg, y  Pr.

(13)

However, the 1-Lipschitz constraint is

f (x) - f (y)  d(x, y), x, y.

(14)

The key difference is that the necessary constraint (Eq. 13) restricts the range of x and y, but Lipschitz constraint (Eq. 14) does not have the restriction on the range and thus is the sufficient condition of the former one.

If the supports of Pg and Pr are the entire space, Eq. (13) and Eq. (14) are actually identical, and Wasserstein distance should also work. However, Pg and Pr are usually disjoint in GANs. Therefore, introducing a stronger Lipschitz constraint is necessary to ensure the validity of Wasserstein distance in xf (x)-based updating.

4.5 CONTRIBUTION CLARIFICATION
This work is substantially different from Wasserstein GAN (Arjovsky et al., 2017). The main argument in (Arjovsky et al., 2017) is "we should switch to Wasserstein distance", which does not hold in general, as we have argued. Though the final solution in Wasserstein GAN is sound as we prove.
We have shown that Lipschitz constraint is able to ensure the convergence of GANs in a family of GAN objectives, which is not restricted to Wasserstein distance. For example, Lipschitz constraint is also introduced to original GAN in (Miyato et al., 2018) and (Kodali et al., 2017) and shows improvements on the quality of generated samples. As a matter of fact, the original GAN objective (x) = (-x) = - log((-x)) is another instance in our proposed family and thus our analysis explains why and how it works.
It is also worth noticing that JD in our formulation is not derived from any well-established distance metric; it is derived based on Lipschitz constraint. As we have shown that a well-established distance or divergence does not necessarily ensure the convergence, we believe our trial could shed light on the new direction of GANs.

8

Under review as a conference paper at ICLR 2019
Last but no least, though we do not discuss the generator's objective, our analysis indicates that the minimax in terms of  in Eq. (1) is not essential, because it only influences the scale of the gradient. Nevertheless, the function  does influence the updating of the generator, and we leave the detailed investigation as future work. Another example that has a theoretically meaningful xf (x) is Coulomb GAN (Unterthiner et al., 2017), which is also derived neither from minimax game nor from well-defined distance metric.
4.6 RELATED WORK
Fedus et al. (2017) also argued that divergence is not the primary guide of the training of GANs and pointed out that the gradient does not necessarily related to the divergence. However, they tended to believe that original GAN with non-saturating generator objective can somehow work. As we have proved before, given the optimal f , the original GAN has no guarantee on its convergence. And we argue that practical work scenarios benefit from parameter-tuning. Some work study the suboptimal f (x) (Mescheder et al., 2017; 2018; Arora et al., 2017), which is another important direction for understanding GANs theoretically. While the behaviors of suboptimal can be slightly different, we think the optimal f (x) should well-behave in the first place. Researchers also found that applying Lipschitz constraint to the generator also benefits the quality of generated samples (Zhang et al., 2018; Odena et al., 2018). In addition, researchers also investigated implementation of Lipschitz constraint in GANs (Gulrajani et al., 2017; Petzka et al., 2017; Miyato et al., 2018). However, this branch of related work is out of scope of the discussion in this paper.
5 EXPERIMENTS
In this section, we present the experiment results on our proposed objectives for GANs. The anonymous code is provided at http://bit.ly/2Kvbkje.
5.1 VERIFYING THE OBJECTIVE FAMILY AND ITS GRADIENT xf (x)

(a) x (b) - log((-x))

 (c) x + x2 + 1

(d) exp(x)

Figure 3: Verifying the objective family

Figure 4: xf (x) gradation with CIFAR-10

We verify a set of  and  satisfying Eq. (12): (a) (x) = (-x) = x; (b) (x) = (-x) = - log((-x)); (c) (x) = (-x) = x + x2 + 1; (d) (x) = (-x) = exp(x). As shown in Figure 3, the gradient of each generated sample is towards a real sample.
We further verify xf (x) with the real-world data, using ten CIFAR-10 images as Pr and ten noise images as Pg to make the solving of f (x) feasible. The result is shown in Figure 4, where The leftmost in each row are the x  Pg and the second are their gradient xf (x). The interior are x + · xf (x) with increasing , which will pass through a real sample, and the rightmost are the nearest y  Pr. This result visually demonstrates that the gradient of a generated sample is towards the direction of one real sample. Note that the final results of this experiment keep almost identical when varying the loss metric (x) and (x) in the family.

9

Under review as a conference paper at ICLR 2019

(a) x (b) - log((-x)) Figure 5: f (x) in new objective is more stable.

FID

45 x

40 min(0, x 1)

35

exp(x) log( ( x)) + 0.01x

30

log( ( x)) x+ x2 +1

25

20

15 0 100000 200It0e0r0at3io0n0s000 400000 500000

Figure 6: Training curves on CIFAR-10.

5.2 STABILIZING f (x) WITH NEW OBJECTIVES
Wasserstein distance is a special case in our proposed family of objectives where  (x) =  (x) = 0. As a result, f (x) under the Wasserstein distance objective where (x) = (-x) = x has a free offset, which means given a f (x), f (x) + b with any b  R is also an optimal. In practice, this behaves as an oscillatory f (x) during training. Any other instance in our new proposed objectives does not have this problem. We illustrate this practical difference in Figure 5.

5.3 BENCHMARK ON UNSUPERVISED IMAGE GENERATION TASKS

Table 2: Quantitative comparisons on unsupervised image generation tasks.

Objective
- min(0, -x - 1) x
- log((-x)) x + x2 + 1
exp(x) - log((-x)) + 0.01x

CIFAR-10 FID IS 21.58 ± 0.21 7.43 ± 0.04 19.64 ± 0.23 7.66 ± 0.03 16.36 ± 0.09 8.49 ± 0.11
15.76 ± 0.13 8.04 ± 0.04 19.82 ± 0.13 7.79 ± 0.03 18.32 ± 0.15 7.75 ± 0.04

Tiny ImageNet FID IS 16.22 ± 0.33 8.58 ± 0.08 18.81 ± 0.58 8.20 ± 0.05 15.94 ± 0.33 8.42 ± 0.04
16.83 ± 0.41 8.35 ± 0.09 20.45 ± 0.15 8.06 ± 0.05 16.09 ± 0.23 8.47 ± 0.10

Oxford 102

FID*

IS*

9.72 ± 0.51 21.91 ± 0.18

9.74 ± 0.63 21.66 ± 0.22

9.40 ± 0.49 21.82 ± 0.11

9.16 ± 0.52 21.96 ± 0.19

9.90 ± 0.72 21.91 ± 0.22

9.50 ± 0.39 21.91 ± 0.20

Finally, we fix (x) = -x in the generator's objective and compare various objectives on unsupervised image generation tasks. The results of Inception Score (Salimans et al., 2016) and Frechet Inception Distance (Heusel et al., 2017b) are presented in Table 2. We also include the hinge loss (x) = (-x) = - min(0, -x - 1) which used in (Miyato et al., 2018).
The gradient of exp(x) varies significantly and we find it requires a small learning rate to avoid explosion. The objectives x + x2 + 1, - log((-x)) and - log((-x)) + 0.1x achieve the best performances. This is probably because they have bounded gradient and reduce the gradient of well-identified points towards zero, which enables the discriminator to pay more attention to these ill-identified. Hinge loss - min(0, -x - 1) does not lie in our proposed objective family and turns out to be unstable and performs unsatisfactory in same cases. We also plot the training curve in terms of FID in Figure 6.
Due to page limitation, we leave the details, visual results and more experiments in the Appendix.

6 CONCLUSION
In this paper we have shown that the fundamental cause of failure in training of GANs stems from the unreliable xf (x). Specifically, when Pg and Pr are disjoint, xf (x) for fake sample x  Pg tells nothing about Pr, making it impossible for Pg to converge to Pr. We have further demonstrated that even Wasserstein distance, which can properly measure the distance for two disjoint distributions, also suffers from the same problem when Pg and Pr are disjoint. This implies that "what distance metric should be used" does not touch the key of non-convergence of GANs. We have highlighted in the paper that a well-defined distance metric, or more generally, "Pg = Pr is the optimum", is not enough for guaranteeing the convergence of GANs. Therefore, if we update the generator based on xf (x), we need to make much attention on the design of f (x). Furthermore, to address the aforementioned problem, we have proposed the Lipschitz-continuity condition as a general solution to make xf (x) reliable and ensure the convergence of GANs, which works well in a large family of GAN objectives. In addition, we have shown that in the overlapping case, xf  (x) is also problematic which turns out to be an intrinsic cause of mode collapse in traditional GANs.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Martin Arjovsky and Le´on Bottou. Towards principled methods for training generative adversarial networks. In ICLR, 2017.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
Andrew Brock, Theodore Lim, JM Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: Gans do not need to decrease adivergence at every step. arXiv preprint arXiv:1710.08446, 2017.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6626­6637, 2017a.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gu¨nter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. arXiv preprint arXiv:1706.08500, 2017b.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint arXiv:1611.07004, 2016.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans. arXiv preprint arXiv:1705.07215, 2017.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. arXiv preprint ArXiv:1611.04076, 2016.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in Neural Information Processing Systems, pp. 1825­1835, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In International Conference on Machine Learning, pp. 3478­3487, 2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.
11

Under review as a conference paper at ICLR 2019
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Youssef Mroueh and Tom Sercu. Fisher gan. In Advances in Neural Information Processing Systems, pp. 2510­2520, 2017.
Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, and Yu Cheng. Sobolev gan. arXiv preprint arXiv:1711.04894, 2017.
Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune. Plug & play generative networks: Conditional iterative generation of images in latent space. arXiv preprint arXiv:1612.00005, 2016.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B Brown, Christopher Olah, Colin Raffel, and Ian Goodfellow. Is generator conditioning causally related to gan performance? arXiv preprint arXiv:1802.08768, 2018.
Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of wasserstein gans. arXiv preprint arXiv:1709.08894, 2017.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. 2018. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2226­2234, 2016. Vivien Seguy, Bharath Bhushan Damodaran, Re´mi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel. Large-scale optimal transport and mapping estimation. arXiv preprint arXiv:1711.02283, 2017. Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends R in Machine Learning, 4(2):107­194, 2012. Thomas Unterthiner, Bernhard Nessler, Gu¨nter Klambauer, Martin Heusel, Hubert Ramsauer, and Sepp Hochreiter. Coulomb gans: Provably optimal nash equilibria via potential fields. arXiv preprint arXiv:1708.08819, 2017. Ce´dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.
12

Under review as a conference paper at ICLR 2019

A EXPERIMENTS: THE INFLUENCE OF HYPER-PARAMETERS

The value surface of traditional GANs is highly depends on the network and training hyperparameters. It also suggests why traditional GANs are highly unstable and sensitive to hyperparameters.
We here plot the Least-Square GAN value surface with various hyper-parameter settings, to give directly impression on how these parameters influence GANs training. Not very strictly, but our empirical code is: (i) a low-capacity network tends to learn a simple surface; (ii) SGD tends to learn a more complex surface than ADAM; (iii) large learning rate tends to learns a simpler surface than small learning rate; (iv) highly nonlinear activation function tends to result into more complex value surface.

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 1

1.5 1.0 0.5 0.0 0.5

| f(xx) |=1.33E-01

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 2

1.5 1.0 0.5 0.0 0.5

| f(xx) |=1.80E-02

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 3

1.5 1.0 0.5 0.0 0.5

| f(xx) |=2.30E+00

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

Figure 7: ADAM optimizer with lr=1e-2, beta1=0.0, beta2=0.9. MLP with RELU activations, #hidden units=1024, #layers=1.

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 1

1.5 1.0 0.5 0.0 0.5

| f(xx) |=0.00E+00

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 2

1.5 1.0 0.5 0.0 0.5

| f(xx) |=0.00E+00

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 3

1.5 1.0 0.5 0.0 0.5

| f(xx) |=2.24E+00

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

Figure 8: ADAM optimizer with lr=1e-2, beta1=0.0, beta2=0.9. MLP with RELU activations, #hidden units=1024, #layers=4.

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 1

1.5 1.0 0.5 0.0 0.5

| f(xx) |=6.93E-01

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 2

1.5 1.0 0.5 0.0 0.5

| f(xx) |=4.01E-01

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 3

1.5 1.0 0.5 0.0 0.5

| f(xx) |=2.48E+00

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

Figure 9: ADAM optimizer with lr=1e-5, beta1=0.0, beta2=0.9. MLP with RELU activations, #hidden units=1024, #layers=4.

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 1

1.5 1.0 0.5 0.0 0.5

| f(xx) |=1.56E-02

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 2

1.5 1.0 0.5 0.0 0.5

| f(xx) |=2.85E-03

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 3

1.5 1.0 0.5 0.0 0.5

| f(xx) |=2.35E+00

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

Figure 10: SGD optimizer with lr=1e-3. MLP with SELU activations, #hidden units=1024, #layers=64.

13

Under review as a conference paper at ICLR 2019

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 1

1.5 1.0 0.5 0.0 0.5

| f(xx) |=3.85E+00

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 2

1.5 1.0 0.5 0.0 0.5

| f(xx) |=4.09E-01

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

1.5 1.0 0.5 0.0 0.5 1.0 1.5
2.0

real samples fake samples

Case 3

1.5 1.0 0.5 0.0 0.5

| f(xx) |=2.81E+00

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0 1.0 1.5 2.0

Figure 11: SGD optimizer with lr=1e-4. MLP with SELU activations, #hidden units=128, #layers=64.

B VARIOUS (x) AND (x) THAT SATISFIES EQUATION 12

For Lipschitz constraint based GANs, (x) and (x) are required to satisfy Equation 12. Eq. (12) is actually quite general and there exists many other instances, e.g., (x) = (-x) = x, (x) = (-x) = - log((-x)), (x) = (-x) = x + x2 + 1, (x) = (-x) = exp(x), etc. We plot these instances of (x) and (x) in Figure 12.
Generally, it is feasible to set (x) = (-x). Note that rescaling and offsetting along the axises are trivial operation to found more (x) and (x) within a function classes, and linear combination of two or more (x) or (x) from different function classes also keep satisfying Equation 12.

4.5 (x), (x)

6 (x), (x)

14 (x), (x)

10 (x), (x)

4.0 3.5

(x) = - log((x)) (x) = - log(( - x))

4

3.0 2

(x) = - x (x) = x

12 10

(x) = - x + x2 + 1 (x) = x + x2 + 1

8

(x) = e-x (x) = ex

2.5 2.0

0

8 6

6 4

1.5 2 4

1.0 0.5

4

2

2

0.0 6 4 2 0 2 4 6 6 6 4 2 0 2 4 6 0 6 4 2 0 2 4 6 02.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0
xxxx

Figure 12: Various (x) and (x) that satisfies Equation 12

FID y
y y
Icp
y

C GENERATED IMAGES AND TRAINING CURVES

Extra training curves on Tiny ImageNet is proved in Figure 13. And comparisons on the visual results among different objectives are aslo provided in Figure 14, Figure 15 and Figure 16.

45 x

40 min(0, x 1)

35

log( ( x)) + 0.01x log( ( x))

30 x + x2 + 1

25

20

15 0 100000 200It0e0r0at3io0n0s000 400000 500000

9.0

8.5

8.0

7.5

7.0 x

6.5 6.0

min(0, x 1) log( ( x)) + 0.01x log( ( x))

5.5 x + x2 + 1

5.0 0 100000 200It0e0r0at3io0n0s000 400000 500000

(a) FID training curve

(b) Inception Score training curve

Figure 13: FID and Icp(Inception Score) training curves towards different objective functions on Tiny Imagenet Dataset.

14

Under review as a conference paper at ICLR 2019

(a) - log((-x))

(b) - log((-x)) + 0.01x

(c) x

 (d) x + x2 + 1

(e) exp(x)

(f) - min(0, -x - 1)

Figure 14: Random Samples of Lipschitz GAN trained towards different objectives on Oxford 102.

15

Under review as a conference paper at ICLR 2019

(a) - log((-x))

(b) - log((-x)) + 0.01x

(c) x

 (d) x + x2 + 1

(e) exp(x)

(f) - min(0, -x - 1)

Figure 15: Random Samples of Lipschitz GAN trained towards different objectives on Cifar-10.

16

Under review as a conference paper at ICLR 2019

(a) - log((-x))

(b) - log((-x)) + 0.01x

(c) x

 (d) x + x2 + 1

(e) exp(x)

(f) - min(0, -x - 1)

Figure 16: Random Samples of Lipschitz GAN trained towards different objectives on Tiny Imagenet.

17

Under review as a conference paper at ICLR 2019

Figure 17: The gradient of Lipschitz constraint based GANs with real world data, where Pr consists

of ten images and Pg is Gaussian noise. Up: Each odd column are x  pG and the nearby column

are

their

gradient

f (x) x

.

Down:

the

leftmost

in

each

row

are

x



pG,

the

second

are

their

gradients

f (x) x

,

the

interior

are

x

+

·

f (x) x

with

increasing

, and the rightmost are the nearest y  pdata.

18

Under review as a conference paper at ICLR 2019

D EUCLIDEAN DISTANCE IS ESSENTIAL

In this section, we delve deeply into the relationship between gradient properties and different norm in Lipschitz continuity. We will prove the Theorem 3, i.e. Lipschitz continuity with l2-norm (Euclidean Distance) can guarantee the gradient direction of f (x), and at the same time, demonstrate that the other norm does not have this property.
A brief review on Lipschitz continuity: a function f is k-Lipschitz over a set S with respect to a norm . p, means that, for all a, b  S there is f (a) - f (b)  k a - b p. To start with, we give the proof of Theorem 3 in the following.

Proof.

Let (x, y) be such that x = y, and we define xt = x + t · (y - x) with t  [0, 1]. We claim that: if f (x) is k-Lipschitz with respect to . p and f (y) - f (x) = k x - y p, then f (xt) = f (x) + t · k x - y p.

As we know f (x) is k-Lipschitz, with the property of norms, we have

f (y) - f (x) = f (y) - f (xt) + f (xt) - f (x)  f (y) - f (xt) + k xt - x p = f (y) - f (xt) + t · k x - y p  k y - xt p + t · k x - y p = k · (1 - t) x - y p + t · k x - y p = k x - y p.

(15)

Given f (x) - f (y) = k x - y p, it implies all the inequalities need to be equalities. Therefore, f (xt) = f (x) + t · k x - y p.

It is clear that: given f (x) is k-Lipschitz with respect to . 2, if f (x) is differentiable at xt, then

f (xt) 2  k. With f (xt) = f (x) + t · k x - y 2, the directional derivative of f (x) on the

direction v =

y-x y-x

at xt is equal to k,

2

f (xt + h y-x ) - f (xt)

f (xt) = lim f (xt + hv) - f (xt) = lim

y-x 2

v h0

h

h0

h

f (xt+ h ) - f (xt)

= lim h0

y-x 2
h

= lim h0

h ·k y-x

y-x

2

2 = k.

h

(16)

Note that v 2 =

y-x y-x

2 = 1, i.e. v is a unit vector. Now,

2

k2 = k f (xt) = k v

v, f (xt)

=

kv, f (xt)



kv 2

f (xt) 2 = k2.

(17)

As the equality holds only when f (xt) = kv = k y-x , we prove that f (xt) = k y-x .

y-x

y-x

22

Above proof utilizes the property that f (xt) 2  k, which is derived from that f (x) is k-

Lipschitz with respect to . 2. However, other norms do not hold this property. Specifically, according to the theory in Shalev-Shwartz et al. (2012): if a convex and differentiable function f is

k-Lipschitz over S with respect to norm . p, then the Lipschitz continuity actually implies a bound

on the dual norm of gradients, i.e. f q  k. Here . q is the dual norm of . p, which satisfies

the

equation

that

1 p

+

1 q

=

1.

As we could notice, a norm is equal to its dual norm if and only if p = 2. Switching to lp-norm with p = 2, it is actually bounding the lq-norm of the gradients. However, bounding the lq-norm of the

19

Under review as a conference paper at ICLR 2019

gradients does not guarantee the gradient direction at fake samples point towards real samples. A counter-example is provided as follows.

Consider a function g(x, y) = x + y on R2.  p1 = (x1, y1), p2 = (x2, y2), there is g(p1) - g(p2) =

g(x1, y1) - g(x2, y2) = (x1 - x2) + (y1 is a 1-Lipschitz function with respect to

- y2)  |x1 - x2| + l1-norm. According

|y1 - y2| to above

= p1 - p2 analysis, the

1d,uwalhnicohrmmeoafns gg

is bounded, i.e. g   1. Actually g is equal to (1, 1) at every point in R2 with g  = 1.

Selecting two points A = (0, 0) and B = (2, 1), we have g(A)-g(B) = A-B 1, however, g(A)

= (1, 1) is not pointing towards B.

E ON THE IMPLEMENTATION OF K-LIPSCHITZ FOR GANS

Typical techniques for enforcing k-Lipschitz includes: spectral normalization Miyato et al. (2018), gradient penalty Gulrajani et al. (2017), and Lipschitz penalty Petzka et al. (2017). Before moving into the detailed discussion of these methods, we would provide several important notes in the first place.

Firstly, enforcing k-Lipschitz in the blending-region of pG and pdata is actually sufficient. Define B(µ, ) = {x^ = x · t + y · (1 - t) | x  µy   t  [0, 1]}. It is clear that f (x) is 1-Lipschitz in B(µ, ) implies f (x) - f (y)  d(x, y), x  µ, y  . Thus, it is a sufficient constraint for
Wasserstein distance in Equation 8. In fact, f (x) is k-Lipschitz in B(pG, pdata) is also a sufficient condition for all properties described in Lipschitz constraint based GANs (Section 3).

Secondly, enforcing k-Lipschitz with regularization would provide a dynamic Lipschitz constant k.

Theorem 4. With Wasserstein GAN objective, we have minfFk-Lip JD(f ) = k ·minfF1-Lip JD(f ).

Assuming we know and can control the Lipschitz constant k of f (x), by introducing a loss, saying

square loss, on k respecting to a constant k0, the total loss of the discriminator (critic) becomes

J (k) minfFk-Lip JD(f ) +  · (k - k0)2. With Lemma 4, let  = - minfF1-Lip JD(f ), then

J(k) = -k ·  +  · (k - k0)2,

and J(k) achieves

its minimum when k =

 2

+

k0.

When 

goes

to zero, i.e. pG converges to pdata, the optimal k decreases. And when pG = pdata, we have  = 0

and optimal k = k0. We choose k0 = 0 in our experiments. The similar analysis applies to Lipschitz

constraint based GANs and we use  · k2 to enforcing k-Lipschitz for general Lipschitz constraint

based GANs.

For practical methods, though spectral normalization Miyato et al. (2018) recently demonstrates their excellent results in training GANs, spectral normalization is an absolute constraint for Lipschitz over the entire space, i.e., constricting the maximum gradient of the entire space, which is unnecessary. On the other side, we also notice both penalty methods proposed in Gulrajani et al. (2017) and Petzka et al. (2017) are not the exactly implementing the Lipschitz continuity condition, because it not simply penalties the maximum gradient, but penalties all gradients towards 1 or penalties all these greater than one towards 1.

We found in our experiments that the existing methods including spectral normalization Miyato et al. (2018), gradient penalty Gulrajani et al. (2017), and Lipschitz penalty Petzka et al. (2017) all fail to converge to the optimal f (x) in many of our synthetic experiments. We developed a new method for enforcing k-Lipschitz and we found in our experiments that the new method stably converges to the optimal f (x).

The new method. Note that the practical methods of imposing k-Lipschitz is not the key contribution of this work, and it is far from well-validated. We plan a further work on this topic for a more rigorous study. But for the necessity for understanding our paper and reproducing of experiments, we introduce it as follows.
Combining the idea of spectral normalization and gradient penalty, we developed an new regularization for Lipschitz continuity in our experiments. Spectral normalization is actually constraining the maximum gradient over the entire space. And as we argued previously, enforcing Lipschitz continuity in the blending region is sufficient. Therefore, we propose to restricting the maximum gradient over the blending region:

20

Under review as a conference paper at ICLR 2019

(a) Gradient Penalty

(b) Maximum Gradient Penalty

Figure 18: Comparison between gradient penalty and maximum gradient penalty, with Pr and Pg

consist of ten real and noise images, respectively. The leftmost in each row is a x  pG and the

second

is

its

gradient



f (x) x

.

The

interior

are

x+

·

f (x) x

with

increasing

, which will pass through

a real sample, and the rightmost is the corresponding y  pdata.

Jmaxgp = max [
x^B(µ,)

f (x)

22]

(18)

In practice, we sample x^ from training batch as in Gulrajani et al. (2017); Petzka et al. (2017). To
improve the stability and reduce the biased introduced via batch sampling, we propose the keep track x^ with the maximum f (x) 2. A practical and light weight method is to maintain a list Smax that has the currently highest (top-k) f (x) 2 (initialized with random x^ samples), using the Smax as part of the batch estimation of Jmaxgp, and update the Smax after each batch updating of the discriminator. In our experiment, Smax takes 1/2 batch, and the remaining 1/2 batch are random sampled. Smax always keeps track of the maximal 1/2 samples in the batch.

We comparing the practical result of gradient penalty Ex^B[

f (x)

2 2

]

and

the

proposed

maximum

gradient penalty in Figure 18. Before switching to maximum gradient penalty, we struggled for a

long time and cannot achieve a high quality result as showed in Figure 18b. The other forms of

gradient penalty Gulrajani et al. (2017); Petzka et al. (2017) perform similar as Ex^B[

f (x)

2 2

].

F NO-DIFFERENTIABLE f (x)

If f (x) is k-Lipschitz and f (y) - f (x) = k · d(x, y), we say that (x, y) are coupled. When a sample x is coupled with more than one y and these y lie in different directions of x, f (x) is non-differentiable at x and it will has sub-gradient along each direction.
When the f (x) non-differentiable, due to the smoothness of practically-used neural network, as we noticed in the experiments, it usually behaviors as that the gradient direction is pointing in the middle of these sub-gradient (more strictly, a linear combination of these sub-gradients).
It seems that when the pG is discrete (simulating discrete token generation, such as language and music), it is easy to become non-differentiable: in the optimal transport perspective, once it is required to move to more than one targets, f (x) is non-differentiable at this point.
One way to alleviate this above problem is adding noise (e.g. Gaussian) to each discrete token from pG. The discrete token with different noises now disperse to different targets. In the practical generator for continuous token, such as images, this kind of non-differentiable problem naturally get solved.
The more serious non-differentiable problem requires trace back to the Monge problem Villani (2008), which theoretically discussed under which condition the optimal transport is a one-one mapping, which by nature solve the non-differentiable problem, as each sample now has a single target.

21

Under review as a conference paper at ICLR 2019

However, for the Monge problem is solvable, i.e. the mapping from pG and pdata is one-one, it requires the d(x, y) to be a strictly convex and super-linear Villani (2008). Unfortunately, the Euclidean distance, which is necessary to ensure the gradient direction from fake sample directly points toward real sample, does not fit this condition. So we currently does not figure out a practical solution to take advantage of the Monge problem related theories.
Nonetheless, even if f  (x) is non-differentiable, the gradient is also usually somehow pointing towards the real samples. And the empirical founding is that: when the pG get close to pdata, the non-differentiable problem diminishes.

G PROOF OF THE THEOREM

Define JD = ExpG [(f (x))] + Expdata [(f (x))] = pG(x)(f (x)) + pdata(x)(f (x))dx.

Note that xJD

JD x

=

pG(x)(f (x)) + pdata(x)(f (x)).

Define J

= JD +  · k(f )2.

Let

JD (k) = arg minfFk-Lip JD. Let f (x) = arg minf [JD +  · k(f )2], where k(f ) is Lipschitz

constant of f (x).

Lemma 1.

x,

[xJD ] f (x)

=

0

if

and

only

if

k(f )

=

0.

Proof.

(i)

x,

[xJD ] f (x)

=

0 implies k(f )

=

0.

For

the

optimal

f (x),

it

holds

that

J k(f )

=

 JD k(f )

+

2

·

k(f )

=

0.

x,

[xJD ] f (x)

= 0 implies

 JD k(f )

=

0.

We

thus

conclude

that

k(f )

=

0.

(ii) k(f )

=

0 implies

x,

[xJD ] f (x)

=

0.

For the optimal f (x), it holds that

J k(f )

=

 JD k(f )

+ 2 · k(f )

=

0.

So k(f )

=

0 implies

 JD k(f )

=

0.

k(f )

=

0

also

implies

x, y, f (x)

=

f (y).

If

there

exists

some point x

such

that

[xJD ] f (x)

=

0,

then,

given

x, y, f (x)

=

f (y),

it

is

obviously

that

 JD k(f )

=

0.

It

is

contradictory

to

 JD k(f )

=

0.

Thus

we

has

x,

[xJD ] f (x)

=

0.

Lemma 2. If x, y, f (x) = f (y), then pG = pdata.

Proof.

x, y, f (x)

=

f (y) implies k(f )

=

0.

According

to

Lemma

1,

x,

[xJD ] f (x)

=

pG

(x)

(f (x)) f (x)

+pdata

(x)



(f (x)) f (x)

=

0.

So

pG(x)
pdata (x)

=

(f  (x))

-

f  (x) (f  (x))

,

and

thus

f  (x)

pG(x)
pdata (x)

has a constant value, which

straightforwardly implies pG = pdata.

Proof of Theorem 1.

(i) Considering the f  (x), x  p¯G  p¯data, if there does not exist a y such that

|f (y)

-

f (x)|

=

k(f )

·

d(x, y),

because

f (x)

is

the

optimal,

it

must

hold

that

[xJD ] f (x)

=

0.

4

4Otherwise, as f (x) is not constrained by the Lipschitz constraint, we can construct a better f  by adjusting the value of f (x) at x according to the non-zero gradient.

22

Under review as a conference paper at ICLR 2019

(ii) For x



p¯G  p¯data - p¯G  p¯data, assuming pG(x)

=

0 and pdata(x)

=

0, we have

[xJD ] f (x)

=

pG

(x)

(f (x)) f (x)

+

pdata(x)

(f (x)) f (x)

=

pG

(x)

(f (x)) f (x)

>

0,

because

pG(x)

>

0 and

(f (x)) f (x)

>

0.

Then, according to (i), there must exist a y such that |f (y) - f (x)| = k(f ) · d(x, y). The other

situation can be proved in the same way.

(iii) According to Lemma 2, in this situation that pG = pdata, for the optimal f (x), there must exist at least one pair of points x and y such that x = y and f (x) = f (y). If there are no x and y satisfying that |f (y) - f (x)| = k(f ) · d(x, y), it will be contradictory to that f (x) is optimal, because we can construct a better f  by decreasing the value of k(f ) until there are two points, e.g. x and y, constrained by Lipschitz constraint, i.e. |f (y) - f (x)| = k(f ) · d(x, y).

(iv)

In

Nash

Equilibrium

state,

it

holds that,

for

any

x



p¯G  p¯data,

J k(f )

=

 JD k(f )

+ 2 · k(f )

=

0 and

[xJD ] f (x) f (x) x

=

0.

We

claim

that

in

the

Nash

Equilibrium state,

the

Lipschitz

constant k(f )

must

be

0.

If

k(f )

=

0,

according

to

Lemma

1,

there

must exist

a

point

x^

such

that

[x^JD ] f (x^)

=

0.

And

according to (i), it must hold that y^ fitting |f (y^) - f (x^)| = k(f ) · d(x^, y^). According to Theorem

3, we have

f (x) x

x=x^

2 = k(f ) = 0.

This

is

contradictory

to

that

[xJD ] f (x) f (x) x

x=x^ = 0. Thus

k(f )

=

0, that is, x



p¯G  p¯data,

f (x) x

=

0, which means

x, y, f (x)

=

f (y).

According to

Lemma 2, x, y, f (x) = f (y) implies pG = pdata. Thus pG = pdata is the only Nash Equilibrium

of our system.

Remark:

For the Wasserstein distance,

[xJD ] f (x)

=

0 if and only if pG(x)

=

pdata(x).

For the

Wasserstein distance, enforcing a dynamic k-Lipschitz also benefits: at the convergence state, it

holds

f (x) x

=

0.

The proof of Theorem 3 can be found at Section D.

H THE IMPORTANCE OF EQUATION 12
Requiring (x) and (x) to satisfy Equation 12 is important, because it is the non-trivial condition that makes sure ExpG [(f (x))] + Expdata [(f (x))] +  · k(f )2 has global minimum with respect to f .
Lemma 3. If (x) and (x) satisfies Equation 12, then for any fixed pG and pdata, ExpG [(f (x))] + Expdata [(f (x))] has global minimum with respect to f , for any fixed k(f ) = k^.

Proof.

(i) Assuming pG and pdata are two delta distributions.

Given pG and pdata are two delta distributions, according to Theorem 1 and Theorem 2, for x  pG and y  pdata, f (y) - f (x) = k^ · d(x, y). Let f (x) =  and  = k^ · d(x, y), then f (y) =  + .
Define JD() = ExpG [(f (x))] + Expdata [(f (x))] = () + ( + ).

Given

a

such

that



(a) + 

(a)

=

0,

[ 2

](x)x



0

and

[ 2

](x)x



0,

we

have,

when



is

small

enough (such that,  < a and  +  < a), JD() =  () +  ( + )   (a) +  (a) = 0.

Similarly, when  is large enough (such that,  > a and  +  > a), JD() =  () +  ( + ) 

 (a) +  (a) = 0.

Therefore, JD() is convex with respect to  and there exists an 0 such that JD(0) = 0, where

JD

achieves

its

the

global

minimum.

When

[ 2

](x)x

>

0

and

[ 2

](x)x

>

0,

it

is

the

unique

global

minimum.

(ii) The general case.

23

Under review as a conference paper at ICLR 2019

For any pG and pdata, given any initial f (x), we construct a f (x) in the following manner.

For any point x  p¯G  p¯data, we define the initial bounding set Sx = {x}. We will use Sx to

keep track of all points that have direct or indirect bounding relationship with x. When we say

x and y has bounding relationship, we mean |f (y) - f (x)| = k(f ) · d(x, y). Define x =

tSx

pG

(t)



(f (t)) f (t)

+

pdata(t)



(f (t)) f (t)

dt.

We apply the following process separately and sequentially to all x  p¯data. For every point x  p¯data,

we define the initial temporal bounding set Mx = Sx. We will gradually adjust the f (x) for all points

in Mx with the same amount according to x, and iteratively collect all sets that have bounding

relationship with any point in Mx into Mx, i.e., Mx := Mx + Sy, y that holds x  Mx such that

|f (y) - x  Sx

f (x)| = k(f ) · such that f (x) -

d(x, y), f (y) =

at the k(f )

same · d(x,

time, we update y). For any x 

Sx p¯data

:= Sx + , we will

Sy, y  p¯G continue the

that holds increasing

of the f (x) for all x in Mx until x = 0.

After that, we apply a similar process for each x  p¯G. For every x  p¯G, we would gradually adjust the f (x) for all points in Mx with the same amount according to x if x = 0, until x = 0.

During the process, we iteratively collect all sets that have bounding relationship with any point

in Mx into Mx, i.e., Mx := Mx + Sy, y that holds x  Mx such that |f  (y) - f  (x)| =

k(f ) f (y)

· d(x, y), at the same time, we - f (x) = k(f ) · d(x, y).

update

Sx

:=

Sx

+

Sy ,

y



p¯data

that

holds

x



Sx

such

that

It can be proved that the "continue..until..." process would finally end for any x  p¯G  p¯data. If not,

then

Mx

will

collect

all

x



p¯G

 p¯data

at

the

ending.

Given

 a,

(x) x

x=a

+

(x) x

x=a

=

0

and

[ 2

](x)x



0

and

[ 2

](x)x



0,

and

Mx

=

p¯G

 p¯data:

if

x



Mx, f (x)

<

a,

it

would

holds

that x  0; if x  Mx, f (x) < a, it would holds that x  0, and there exists a f (x) such that

x = 0.

When all the processes ended, it can be proved that for any point x  p¯G  p¯data, it hold s = 0,

and f (x) achieves its minimum. As showed in the above paragraph, the tSx pG(t)(f (t)) +

pdata(t)(f (t))dt in construction process is convex all along, so the minimum is the global min-

imum.

And

when

[ 2

](x)x

>

0

and

[ 2

](x)x

>

0,

it

is

strictly

convex

and

thus

the

global

minimum is the unique global minimum.

Lemma 4. If (x) and (x) satisfies Equation 12, then for any fixed pG and pdata, ExpG [(f (x))] + Expdata [(f (x))] monotonically increases as k(f ) decreases.

Proof.

(i) Assuming pG and pdata are two delta distributions

Given pG and pdata are two delta distributions, according to Theorem 1 and Theorem 2, for x  pG and y  pdata, f (y) - f (x) = k · d(x, y). Let f (x) =  and  = k · d(x, y) > 0, then f (y) =
+. Define JD() = minfFk-Lip ExpG [(f (x))]+Expdata [(f (x))] = min ()+(+). We need to prove JD() is monotonically decreasing, for   0.

Let 0  1 < 2, let 1 = min () + ( + 1) and 2 = min () + ( + 2). Given

(x) x

<

0 and 1

<

2, we have (1) + (1 + 1)

>

(1) + (1 + 2).

Given 2

=

min () + ( + 2), we further have (1) + (1 + 1) > (1) + (1 + 2)  (2) +

(2 + 2). Done.

Additionally, with

(x) x

x=1

+

(x) x

x=1 +1

=

0 and

[ 2

](x)x



0, we have

(x) x

x=1

+

(x) x

x=1 +2

 0.

Providing

(x) x

x=2

+

(x) x

x=2 +2

= 0,

[ 2

](x)x



0

and

[ 2

](x)x



0,

we

get

2



1.

That

is,

2



1

<

1

<

2.

When

[ 2

](x)x

>

0

and

[ 2

](x)x

>

0,

we

have

2 < 1 < 1 < 2.

24

Under review as a conference paper at ICLR 2019

I HYPER-PARAMETER & NETWORK ARCHITECTURE

We follow the network architecture proposed in Gulrajani et al. (2017) to conduct our experiments on Cifar-10, Tiny Imagenet, Oxford 102. The details of network architecture are in Table 3.

Generator: Operation
Noise Linear Residual block Residual block Residual block Conv,tanh

Kernel
N/A N/A 3×3 3×3 3×3 3×3

Resample N/A N/A UP UP UP N/A

Output Dims
128 128×4×4 128×8×8 128×16×16 128×32×32 3×32×32

Critic: Operation
Residual Block Residual Block Residual Block Residual Block ReLU,mean pool
Linear

Kernel 3×3×2 3×3×2 3×3×2 3×3×2
N/A N/A

Resample Down Down N/A N/A N/A N/A

Output Dims 128×16×16
128×8×8 128×8×8 128×8×8
128 1

Optimizer: Adam with beta1=0.0, beta2=0.9; For more details please refer to our published codes.
Table 3: Hyper-parameter and Network Architectures

25

