Under review as a conference paper at ICLR 2019
ON THE SPECTRAL BIAS OF NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis to study neural networks, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects this spectral bias by showing evidence that the learning gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.
1 INTRODUCTION
While the universal function approximation property of neural networks is known since the early 90s (Hornik et al., 1989; Cybenko, 1989; Leshno et al., 1993), recent research has shed light on the underlying mechanisms which endow neural networks with such expressivity (Montufar et al., 2014; Raghu et al., 2016; Poole et al., 2016). At the same time, deep neural networks, despite being massively over-parameterized, have been remarkably successful at generalizing to natural data. This fact is at odds with the traditional notions of model complexity and their empirically demonstrated ability to fit arbitrary random data to perfect accuracy (Zhang et al., 2017a; Arpit et al., 2017). Therefore, it highlights the need of asking not just if a neural network can express a given function, but also how it comes to expressing the said function.
In this work, we connect the if to the how by utilizing a potent mathematical toolbox that has seen wide application in the natural sciences and beyond ­ Fourier analysis1. The Fourier transform affords a natural way of measuring how fast a function can change within a small neighborhood in its input domain - and as such, it is a strong candidate for quantifying and analyzing the sensitivity of a model. Moreover, its mathematical structure makes it intimately compatible with the Jacobian, which in turn has been empirically found to be a good measure of complexity (Novak et al., 2018). The conclusions we draw from our analysis are two-fold: while neural networks can approximate arbitrary continuous functions, they exhibit a certain bias towards those that are smooth. We find that this bias manifests not just in the process of learning, but also in the parameterization of the model itself. At the same time, we discover that the effect of the said bias is rather intricate when the data lies on a lower dimensional manifold embedded in a higher dimensional space, as it tends to be the case for natural data.
Contributions. In section 2, we exploit the well known piecewise-linear structure of a deep ReLU network to evaluate and bound it's Fourier spectrum. In the sections that follow, we demonstrate the peculiar behaviour of neural networks in illustrative and minimal settings. In section 3, we demonstrate that lower frequencies are always regressed first, even when the higher frequencies dominate in the power spectrum of the target function. We call this phenomenon the the spectral bias and place it in the context of the analysis in section 2. In section 4, we find that the manifold hypothesis adds a layer of subtlety, and demonstrate how the shape of the data-manifold interacts with the spectral bias. We then go on to formalize this interaction and derive conditions on the shape
1See appendix A for a brief recap of Fourier analysis.
1

Under review as a conference paper at ICLR 2019

of the manifold that facilitate learning. Finally in section 5, we investigate yet another implication of our analysis, but now on the parameterization of a trained network. We find that parameters that contribute towards expressing high frequencies are less robust than their low frequency counterparts.

2 FOURIER ANALYSIS OF RELU NETWORKS
In this section, we express the family of ReLU networks in the Fourier domain. Throughout the paper, we will denote a vector with a, its magnitude with a, its direction with ^a, and matrices as A.

2.1 PRELIMINARIES

The continous piecewise linear (CPWL) structure of ReLU networks has been extensively studied (Raghu et al., 2016; Montufar et al., 2014; Zhang et al., 2018; Arora et al., 2018a). Not only are ReLU networks CPWL, but remarkably, every CPWL function can be represented by a ReLU network (Arora et al., 2018a). In equation 1, we express a ReLU network in a way that exposes the piecewise linearity.

Consider a ReLU network f with d inputs (i.e. x  Rd) and one output. It can be expressed as:

f (x) = 1P (x) (W x + b )

(1)

where is an index over the linear regions of the network, which are known to be convex polytopes P . 1P is the indicator function on P , and W a 1×d matrix characterizing the slope of the network
in the corresponding linear region (its exact form is explicitly presented in appendix B). We let the
network be L + 1 layers deep, and denote by W (k) the weights and b(k) the biases of the k-th layer.
W (k) is a dk-1 × dk matrix and b(k) is a dk vector. We will assume that the input data lies in a bounded domain of Rd, say X = [-A, A]d for some A > 0. We shall thus restrict ourselves to ReLU networks with bounded support2.

2.2 FOURIER TRANSFORM

We want to study the structure of ReLU networks in the Fourier domain, f (x) = (2)d/2 f~(k) eik·xdk, f~(k) := f (x) e-ik·xdx

(2)

where dx, dk are the uniform Lebesgue measure on Rd and f~ denotes the Fourier transform of f (a short recap of the Fourier transform is included in appendix A). Lemmas 1 and 2 (proved in appendix C) yield the explicit form of the Fourier components.

Lemma 1. The Fourier transform of ReLU networks decomposes as,

f~(k) = i

Wk k2

1~P

(k)

(3)

where ~1P (k) = P e-ik·xdx is the Fourier transform of the indicator function of P  Rd.

The fourier transform of a polytope (appearing in equation 1) is a fairly intricate mathematical object; Diaz et al. (2016) develop an elegant procedure for evaluating it in arbitrary number of dimensions via a recursive application of Stokes theorem (briefly described in appendix C.2). One obtains:
Lemma 2. The Fourier transform of a polytope P is a rational function of k of degree k(^P ) - d, where (k^P ) is the maximum integer 0  j < d such that k^ is orthogonal to (the tangent space of) a j-dimensional face of P .

2Note that there is no loss of generality here: given any ReLU network f , its restriction f|X to X can always be extended to a continuous piece-wise linear function on Rd with bounded support; this function, in turn, is representable by a ReLU network. This argument also shows that ReLU networks with bounded support are universal approximators on X.

2

Under review as a conference paper at ICLR 2019

Note that, given a face of P that is not a point, the set of vectors orthogonal to its tangent face is a strict subspace of Rd with zero Lebesgue measure. Hence k(^P ) = 0 for almost every k in Rd. Finally, we have the following bound for the Lipschitz constant of a deep ReLU network:

Lemma 3. The Lipschitz constant Lf of the ReLU network f is bound as follows (for all ):

L+1

L

W  Lf 

W (k)





L+1 

d

dk

(4)

k=1

k=1

where · is the spectral norm,  is the ravelled parameter vector of the network and ·  is the max-norm.

Lemmas 1, 2 and 3 taken together yield the main result of this section. Given a ReLU network f and the polytopes P defined by its linear regions, we denote by F the set of faces of all such polytopes. For k  Rd, let k^ be the maximum integer 0  j < d such that k^ is orthogonal to the tangent space of some j-dimensional face Fj  F :

k^ = max{0  j < d | k^Fj, Fj  F , dim Fj = j} Theorem 1. The Fourier components of the ReLU network f admit the following bound:

(5)

|f~(k)|



C

Nf Lf k d-k^ +1

(6)

where Nf is the number of linear regions (polytopes Pf, ), Lf = max W 2 is the Lipschitz constant, and C is a universal constant.

Several remarks are in order:

(a) The spectral decay of ReLU networks is highly anisotropic in large dimension. We have k = 0, hence a k-d-1 decay, in almost all directions k^ of Rd. It can decay as slow as k-2 in specific directions orthogonal to the facets bounding linear regions3.
(b) Montufar et al. (2014) and Raghu et al. (2016) obtain tight bounds for the number Nf of linear regions, which also scale as wdL for a network with L layers of width w (cf. theorem 1 of Raghu et al. (2016)). This makes the overall bound ­ and with it the ability to express larger frequencies ­ scale exponentially in depth and polynomial in width (see Lemma 3). This result complements the well known universal approximation property4 by explicitly incorporating a control on the capacity5 of the network, namely the width, depth and the norm of parameters.
(c) For a given architecture (i.e. fixed width and depth), the high frequency contributions of the network can be increased by increasing the norm of the parameters. Assuming the weight norm increases with iteration time in the learning process, that suggests that the training of ReLu networks is biased towards low frequencies. We investigate this fact empirically in the next section.

3 LOWER FREQUENCIES ARE LEARNED FIRST

In this section and the following ones, we present experiments that illustrate the peculiar behaviour of deep ReLU networks in the Fourier domain. With the following first experiment, the goal is to demonstrate that networks tend to fit lower frequencies first during training, a phenomenon we refer to as the spectral bias.

Experiment 1. The setup is as follows6 : Given frequencies  = (k1, k2, ...) with corresponding amplitudes  = (A1, A2, ...), and phases  = (1, 2, ...), we consider the mapping  : [0, 1]  R
given by

(z) = Ai sin(2kiz + i).

(7)

i

3Note that such a rate is not guaranteed by piecewise smoothness alone. For instance, the function |x| is continuous and smooth everywhere except at x = 0, yet it decays as k-1.5 in the Fourier domain.
4The universal approximation property of MLPs with non-polynomial activations is known since the 90s,
see e.g. Leshno et al. (1993). 5Note that the bound is relaxed with increasing capacity. In the limit of capacity to infinity, any constraint
on universal approximation is dissolved. 6More experimental details and additional plots are provided in Appendix D.1.

3

Under review as a conference paper at ICLR 2019

(a) Equal Amplitudes

(b) Increasing Amplitudes

Figure 1: Left (a, b): Plot showing the evolution of the spectrum (x-axis for frequency) during training (y-axis). The colors show the measured amplitude of the network spectrum at the corresponding frequency, normalized by the target amplitude at the same frequency (i.e. |f~ki |/Ai) and the colorbar is clipped between 0 and 1. Right (a, b): Evolution of the spectral norm (y-axis) of each layer during training (x-axis). Figure-set (a) shows the
setting where all frequency components in the target function have the same amplitude, and (b) where higher frequencies have larger amplitudes. Gist: We find that even when higher frequencies have larger amplitudes,
the model prioritizes learning lower frequencies first. We also find that the spectral norm of weights increases
as the model fits higher frequency, which is what we expect from theorem 1.

A 6-layer deep 256-unit wide ReLU network f is trained to regress  with  = (5, 10, ..., 45, 50) and N = 200 input samples spaced equally over [0, 1]; its spectrum f~(k) in expectation over i  U (0, 2) is monitored as training progresses. In the first setting, we set equal amplitude Ai = 1 for all frequencies and in the second setting, the amplitude increases from A1 = 0.1 to A10 = 1. Fig 1 shows the normalized magnitudes |f~(ki)|/Ai at various frequencies, as training progresses. The result is that lower frequencies (i.e. smaller ki's) are regressed first, regardless of their amplitudes.
The interpretation is as follows: for a fixed architecture, the bound in Theorem 1 allows for larger fourier coefficients at higher frequencies if the parameter norm is large. However, the parameter norm can increase only gradually during training by gradient descent, which leads to the higher frequencies being learned late in the optimization process. To confirm that the bound indeed increases as the model fits higher frequencies, we plot in Fig 1 the spectral norm of weights of each layer during training for both cases of constant and increasing amplitudes.
We stress that this bias does not originate from the Mean Squared Error. This can be seen by writing the mean squared training error MSE[f, ] in terms of the Fourier components: letting zi = i/N be the training sample points,

MSE[f, ]

=

1 N

N -1
|f(zi) -

(zi)|2

=

1 N

N -1
|f~(k) - ~(k)|2

=

MSE[f~, ~]

i=0 k=0

(8)

where the second equality follows from Plancherel theorem. This shows that square error in input space translates into square error in Fourier domain, with no bias towards any particular frequency component (note that the finite range in frequencies is due to sampling).

4 NOT ALL MANIFOLDS ARE LEARNED EQUAL
In this section, we investigate the subtleties that arise when the data lies on a lower dimensional manifold embedded in the higher dimensional input space of the model (Goodfellow et al., 2016). To systematically investigate how the shape of such data-manifolds interacts with the spectral bias, we first demonstrate results in an illustrative minimal setting7. Finally, we present a mathematical exposition coupling the Fourier spectrum of the network with that of the target function defined on the manifold and geometry of the manifold itself, and in the process justify our choice of the experimental setting.
The experimental setting is designed to afford control over both the shape of the data manifold and the target function defined on it. We will consider the family of curves in R2 generated by mappings
7We include an experiments on MNIST and CIFAR-10 in appendices D.3 and D.4.

4

Under review as a conference paper at ICLR 2019

Figure 2: Functions learned by two identical networks (identical upto initialization) to classify the binarized value of a sine wave of frequency k = 200 defined on a L=20 manifold. Both functions yield close to perfect accuracy for the samples defined on the manifold (scatter plot), yet they differ significantly elsewhere. The shaded regions show the predicted class (Red or Blue) whereas contours show the confidence (absolute value of logits).

Figure 3: Heatmap of training accuracies of a network trained to predict the binarized value of a sine wave of given frequency (x-axis) defined on L for various L (y-axis).

L : [0, 1]  R2 given by8

1 L(z) = RL(z)(cos(2z), sin(2z)) where RL(z) = 1 + 2 sin(2Lz)

(9)

Here, L([0, 1]) defines the data-manifold and corresponds to a flower-shaped curve with L petals,
or a unit circle when L = 0 (see e.g. Fig 2). Given a signal  : [0, 1]  R defined on the latent space [0, 1], the task entails learning a network f : R2  R such that f  L matches the signal .

Experiment 2. The set-up is similar to that of Experiment 1, and  is as defined in Eqn. 7 with frequencies  = (20, 40, ..., 180, 200), and amplitudes Ai = 1  i. The model f is trained on the dataset {L(zi), (zi)}iN=1 with N = 1000 uniformly spaced samples zi between 0 and 1. The spectrum of f  L in expectation over i  U (0, 2) is monitored as training progresses, and the result shown in Fig 4 for L = 0, 4, 10, 16. Fig 5 shows the corresponding mean squared error curves.

More experimental details in appendix D.2.

The results demonstrate a clear attenuation of the spectral bias as L grows. Moreover, Fig 5 suggests that the larger the L, the easier the learning task.

Experiment 3. Here we adapt the setting of Experiment 2 to binary classification by simply thresholding the function  at 0.5. To simplify visualization, we only use signals with a single frequency mode k, such that (z) = sin(2kz + ). We train the same network on the resulting classification task for k  {50, 100, ..., 350, 400 and L  {0, 2, ..., 18, 20}. The heatmap in Fig 3 shows the classification accuracy for each (k, L) pair. Fig 2 shows visualizations of the functions learned by the same network, trained on (k, L) = (200, 20) under identical conditions up to random initialization.

The results confirm the behaviour observed in Experiment 2, but in the case of classification tasks with categorical cross-entropy loss. Increasing L effectively reduces the spectral bias of the network.

8The interval [0, 1] can be thought of as the latent space of a generative process characterized by L.

5

Under review as a conference paper at ICLR 2019

(a) L = 0

(b) L = 4

(c) L = 10

(d) L = 16

Figure 4: Plots showing the evolution of the network spectrum (x-axis for frequency, colorbar for magnitude) during training (y-axis) for the same target functions defined on manifolds L for various L. Since the target function has amplitudes Ai = 1 for all frequencies ki plotted, the colorbar is clipped between 0 and 1. See more details in experiment 2. Gist: Some manifolds (here with larger L) make it easier for the network to learn higher frequencies than others.

Figure 5: Loss curves for the model trained on target defined on L for various L's.

Discussion. These experiments hint at a rich interaction between the shape of the manifold and the effective difficulty of the learning task. The key technical reason underlying this phenomenon (formalized later in the section) is that the relationship between frequency spectrum of the network f that of the fit f  L is mediated by the embedding map L. In particular, we will argue that a given signal defined on the manifold is easier to fit when the coordinate functions of the manifold embedding itself has high frequency components. Thus, in our experimental setting, the same signal embedded in a flower with more petals can be captured with lower frequencies of the network.

In our experimental setting where we assumed explicit control over the embedding , we had defined the target function  directly on the latent space [0, 1]. In doing so we modeled the manifold
hypothesis, which states that the data lies on a lower dimensional manifold in a higher dimensional
space, and that only a small number of latent factors contain all the relevant information about the data. This can be translated to a degeneracy of the data probability distribution µ in the input space induced under the mapping  by (say) a uniform distribution in the latent space. In this case, the standard regression problem minf Eµ|f -  |2 becomes minimizing the expected square error for some  : Rd  R defined on the input space such that    = . But the fact that the support of µ lies on a lower dimensional data manifold (i.e. the image of [0, 1] under ) leads to a vast space  of degenerate functions    which can be identified with  when restricted to the data-manifold (see
Fig 2). This connects with the spectral bias we found in the previous section, in the sense that neural networks are biased towards expressing a particular subset of solutions in , namely those that are low frequency. It is also worth noting that there exist methods that restrict the space of functions :
notably adversarial training (Goodfellow et al., 2014) and Mixup (Zhang et al., 2017b).

In the following, we formally relate the frequency spectrum of f with that of f   to find that the
lowest frequency a solution in  can have depends intimately with the shape of the manifold (i.e. the support of µ). We start by formulating the expected loss for an arbitrary   . Recall that µ is induced by a uniform distribution in the latent space [0, 1]m:

Eµ|f -  |2 =

dz|f ((z)) - (z))|2 = dl|(f  )(l) - ~(l))|2

[0,1]m

(10)

6

Under review as a conference paper at ICLR 2019

Figure 6: Plot showing the normalized spectrum of the model (x-axis for frequency, colorbar for magnitude) with perturbed parameters as a function of parameter perturbation (y-axis). The colormap is clipped between 0 and 1. Observe that the lower frequencies are more robust to parameter perturbations than the higher frequencies.

where the second equality arises from Parceval's theorem. Now the Fourier transform of the function composition can be expressed as:

(f  )(l) = dkf~(k)P(l, k) where P(l, k) =

dz ei(k·(z)-l·z)

[0.1]m

(11)

The kernel P depends on only  and elegantly encodes the correspondence between frequencies k  Rd in input space and frequencies l  Rm in the latent space [0, 1]m. Following the procedure
from Bergner et al., we can further investigate the behaviour of the kernel in the regime where the stationary phase approximation is applicable, i.e. when l2 + k2   (cf. section 3.2. of Bergner

et al.). In this regime, the integral P is dominated by critical points z¯ of its phase, which satisfy

l = J(z¯) k

(12)

where J(z)ij = ij(z) is the m × d Jacobian matrix of . Non-zero values of the kernel corre-
spond to pairs (l, k) such that Eqn 12 has a solution. Further, given that the components of  (i.e. its coordinate functions) are defined on an interval (namely [0, 1]m), one can use their Fourier series

representation together with Eqn 12 to obtain a condition on their frequencies (shown in appendix

E). More precisely, we find that the i-th component of the RHS in Eqn 12 is proportional to p~i[p]ki where p  Zm is the frequency of the coordinate function i. This yields that we can get arbitrarily large frequencies li if ~i[p] is large9 enough for large p, even when ki is fixed.

This is precisely what experiments 2 and 3 demonstrate in a minimal setting. From Eqn 9, observe that the coordinate functions have a frequency mode at L. For increasing L, it is apparent that the frequency magnitudes l (in the latent space) that can be expressed with the same frequency k (in the input space) increases with increasing L. This allows the remarkable interpretation that the neural network function can express large frequencies on a manifold (l) with smaller frequencies w.r.t its input domain (k), provided that the coordinate functions of the data manifold embedding itself has high-frequency components10.

5 LOWER FREQUENCIES ARE MORE ROBUST
The goal of this section is to show that lower frequencies of trained networks are more robust than larger ones, with respect to perturbations in parameter space. More precisely, we observe that in the neighbourhood of a solution in parameter space, the large frequency components decay faster than the low frequency ones. This property does not directly depend on the training process, but rather on the parametrization of the trained model. We present empirical evidence and a theoretical explanation of the phenomenon.
Experiment 4. The set up is the same as in Experiment 1, where  is given by Eqn. 7. Training is performed for the frequencies  = (10, 15, 20, ..., 45, 50) and amplitudes Ai = 1  i. After
9Consider that the data-domain is bounded, implying that ~ cannot be arbitrarily scaled. 10Informally, we allow ourselves the intuition that it's easy for neural networks to fit wiggly functions if they are defined on wiggly manifolds.

7

Under review as a conference paper at ICLR 2019

convergence to , we consider random (isotropic) perturbations  =  + ^ of given magnitude , where ^  U (Sdim()) is a unit vector. We evaluate the network function f at the perturbed parameters, and compute the magnitude of its discrete Fourier transform at frequencies ki, |f~(ki)|. We also average over 100 samples of ^ to obtain |f~E(ki)|, which we normalize by |f~(ki)|. The result, shown in Figure 6, demonstrate that higher frequencies are significantly less robust than the
lower ones.

The interpretation is as follows: parameters that contribute towards expressing high-frequency components occupy a small volume in the parameter space. To formalize this intuition, given a bounded domain  of parameter space, let us define,
 (k) = {  |k , k > k, |f~(k )| > }
to be the set of parameters such that f has Fourier components larger than for some k with larger norm than k. Then the following Proposition holds (proved in appendix F).

Proposition 1. The volume ratio,

Vol( (k)) R(k) =
Vol()

(13)

inherits the spectral decay rate of |f~(k)|, given by Theorem 1.

Intuitively, expressing larger frequencies requires the parameters to be finely-tuned to work together.

6 RELATED WORK
Recently a number of papers derive modern generalization bounds (Dziugaite & Roy, 2017; Neyshabur et al., 2017; Achille & Soatto, 2017; Smith & Le, 2017; Arora et al., 2018b) for deep networks that are non-vacuous in certain cases compared with the traditional ones (Eg. VC-dimensions Vapnik (2013), Rademacher complexity Bartlett et al. (2005)). A main novel contribution of our work that complements the aforementioned line of work is that we establish the implicit bias of ReLU networks towards learning functions that suppress local irregularities over the input space, thus representing globally smooth functions. While on the other hand generalization bounds provide conditions under which the generalization gap will be small, but do not reveal such a bias. Interestingly, a commonality between our work and most of these modern generalization bounds is that of the role of parameter norm. For many of these bounds, a larger parameter norm makes the generalization gap loose, and in a similar spirit, our results show that a larger norm allows the network to represent more complex functions. On the analysis side, Sonoda & Murata (2017) reprove Leshno et al.'s with ridgelet transforms, and Cande`s (1999) use harmonic analysis to express neural networks with oscillatory activation as a superposition of ridge functions.
In light of our findings, we also contrast between kernel machines (KM) and Knearest neighbor classifiers in appendix G. In summary, our discussion suggests that 1. DNNs strike a good balance between function smoothness and expressivity/parameter-efficiency compared with KM; 2. DNNs learn a smoother function compared with KNNs since the spectrum of the DNN decays faster compared with KNNs in the experiments shown there.

7 CONCLUSION
We expressed deep ReLU networks in the Fourier domain, and used our analysis to explain a peculiar behaviour of such networks ­ that they tend to learn lower frequencies first. We called this behaviour the spectral bias and exposed the subtleties that arise when the training data lies on a manifold. Finally, we found that the parameters contributing towards expressing lower frequencies are more robust to random perturbations than their higher frequency counterparts.

REFERENCES
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentangling in deep representations. arXiv preprint arXiv:1706.01350, 2017.

8

Under review as a conference paper at ICLR 2019
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks with rectified linear units. In International Conference on Learning Representations, 2018a. URL https://openreview.net/forum?id=B1J_rgWRW.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018b.
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. arXiv preprint arXiv:1706.05394, 2017.
Peter L Bartlett, Olivier Bousquet, Shahar Mendelson, et al. Local rademacher complexities. The Annals of Statistics, 33(4):1497­1537, 2005.
Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R in Machine Learning, 2(1):1­127, 2009.
Steven Bergner, Torsten Mo¨ller, Daniel Weiskopf, and David J Muraki. A spectral analysis of function concatenations and its implications for sampling in direct volume visualization.
Emmanuel J Cande`s. Harmonic analysis of neural networks. Applied and Computational Harmonic Analysis, 6(2):197­218, 1999.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2(4):303­314, 1989.
Luc Devroye, La´szlo´ Gyo¨rfi, and Ga´bor Lugosi. Consistency of the k-nearest neighbor rule. In A Probabilistic Theory of Pattern Recognition, pp. 169­185. Springer, 1996.
Ricardo Diaz, Quang-Nhat Le, and Sinai Robins. Fourier transforms of polytopes, solid angle sums, and discrete volume. arXiv preprint arXiv:1602.08593, 2016.
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred A Hamprecht. Essentially no barriers in neural network energy landscape. arXiv preprint arXiv:1803.00885, 2018.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
Barbara Hammer and Kai Gersmann. A note on the universal approximation capability of support vector machines. Neural Processing Letters, 17(1):43­53, 2003.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Esben L Kolsbjerg, Michael N Groves, and Bjørk Hammer. An automated nudged elastic band method. The Journal of chemical physics, 145(9):094107, 2016.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural networks, 6(6):861­867, 1993.
Siyuan Ma and Mikhail Belkin. Diving into the shallows: a computational perspective on large-scale shallow learning. In Advances in Neural Information Processing Systems, pp. 3781­3790, 2017.
9

Under review as a conference paper at ICLR 2019

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1QRgziT-.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in neural information processing systems, pp. 2924­2932, 2014.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 5949­5958, 2017.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= HJC2SzZCW.
Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3360­3368. Curran Associates, Inc., 2016.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. arXiv preprint arXiv:1606.05336, 2016.
Samuel L Smith and Quoc V Le. Understanding generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451, 2017.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approximator. Applied and Computational Harmonic Analysis, 43(2):233­268, 2017.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. International Conference on Learning Representations (ICLR), 2017a.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017b.
Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. arXiv preprint arXiv:1805.07091, 2018.

A BRIEF RECAPITULATION OF FOURIER ANALYSIS

The Fourier transform is a powerful mathematical tool used to represent functions as a weighted sum
of oscillating functions, given that the function satisfies certain conditions. In the realm of signal
processing and beyond, it is used to represent a time (space) domain signal f as a sum of sinusoids of various (spatial) frequencies k, where the weights are referred to as the Fourier coefficients f~(k).

Let f : Rn  R be a squared-integrable function11, i.e.

xRn |f (x)|2dnx

is finite, or f  L2(Rn).

With the Fourier inversion theorem, it holds:

1 f (x) =

f~(k)eik·xdk

2 kRn

(14)

f~(k) =

f (x)e-ik·xdx

xRn

(15)

11On a formal note, the squared integrability is only required for the inverse Fourier transform to exist; for the forward transform, integrability is enough. Moreover, the Fourier transform can be generalized to tempered distributions, which allow for evaluating the fourier coefficients of non-integrable e.g. non-zero constant functions.

10

Under review as a conference paper at ICLR 2019

Informally, equation 14 expresses the function f (x) as a weighted sum (the integral) of plane waves

e±ik·x of the angular wavenumber k, where the unit vector k^ gives the direction of the corresponding

wave in n-D space and the magnitude k is inversely proportional to the wavelength. Equation 15

gives the expression for f~(k), which is called the Fourier transform or the Fourier spectrum or

simply

the

spectrum

of

f.

The

1 2

coefficient

and

sign

in

the

exponential functions

are

matters

of

convention.

The asymptotic behaviour of f~ for k   is a measure of smoothness of f . In Bachmann-Landau

or asymptotic notation12, we say f~ = O(k-1) if for k  , the function f~ decays at least as fast

as

1 k

.

A function whose spectrum is O(k-2) is in a sense smoother than one whose spectrum is

O(k-1), while the spectrum of an infinitely differentiable (or smooth) function must decay faster

than any rational function of k, assuming the function is integrable, i.e. the integral of its absolute

value over its domain is finite (or the function is L1). Intuitively, the higher-frequency oscillations

in a smoother function must vanish faster. Formally, this is a straightforward consequence of the

Riemann-Lebesgue lemma, stating that spectrum of any L1 function must vanish at infinity (poten-

tially arbitrarily slowly), taken together with the well known property of the Fourier transform that

it diagonalizes the differential operator i.e. [xf ](k) = kf~(k).

B THE CONTINUOUS PIECEWISE LINEAR STRUCTURE OF DEEP RELU NETWORKS

We consider the class of scalar functions f : Rd  R defined by a neural network with L hidden layers of widths d1, · · · dL and a single output neuron, defined by:

f (x) = T (L+1)    T (L)  · · ·    T (1) (x)

(16)

where each Tk : Rdk-1  Rdk is an affine function (d0 = d and dL+1 = 1) and  = max(0, ui) denotes the ReLU activation function acting elementwise on a vector u = (u1, · · · un).
ReLU networks are known to be continuous piece-wise linear (CPWL) functions, where the linear regions are convex polytopes (Raghu et al., 2016; Zhang et al., 2018). To expose the piecewise linear structure more explicitly, we associate a binary activation variable  {-1, 1} to each hidden neuron, conditioned on whether its input is positive or negative. Each linear region of the network then corresponds to a unique activation pattern13, wherein each neuron is assigned an activation variable. With that, the ReLU network from Eqn. 16 can be explictly expressed as a sum over all possible activation patterns, as in the following lemma.

Lemma 4.

Given L binary vectors

1, · · ·

L with

k



{-1, 1}dk ,

let

T

(k)
k

:

Rdk-1



Rdk

the

affine

function

defined

by

T

(k)
k

(u)i

=

(T (k)(u))i if ( k)i

=

1, and 0 otherwise.

ReLU network

functions, defined in Eqn. 16, can be expressed as

f (x) =

1Pf, (x)

T (L+1)  T (L)  · · ·  T (1) L1

(x)

1,··· L

(17)

where 1P denotes the indicator function of the subset P  Rd, and Pf, is the polytope defined as the set of solutions of the following linear inequalities (for all k = 1, · · · , L):

(

k)i (T (k)

 T (k-1) k-1



·

·

·



T

(1)
1

)(x)i



0,

i = 1, · · · dk

(18)

f is therefore affine on the polytopes Pf, , which finitely partition the input space Rd to convex polytopes. But remarkably, the correspondence between ReLU networks and CPWL functions goes both ways: Arora et al. (2018a) show that every CPWL function is be represented by a ReLU network, which in turn endows ReLU networks with the universal approximation property.
Finally, in the standard basis, each affine map T (k) : Rdk-1  Rdk is specified by a weight matrix W (k)  Rdk-1 × Rdk and a bias vector b(k)  Rdk . In the linear region Pf, , f can be expressed as

12Formally, f = O(g) = lim supx

f (x) g(x)

< .

13We adopt the terminology of Raghu et al. (2016); Montufar et al. (2014).

11

Under review as a conference paper at ICLR 2019

f (x) = W x + b , where in particular

W

=

W (L+1)W (L) L

·

·

· W (1) 1



R1×d,

(19)

where W (k) is obtained from W (k) by setting its jth column to zero whenever ( k)j = -1. In what

follows, we will assume that the input data lies in a bounded domain of Rd, say X = [-A, A]d for

some A > 0. We will thus restrict to ReLU networks with bounded support. Note that there is no

loss of generality here: given any ReLU network f , its restriction f|X to X can always be extended

to a continuous piece-wise linear function on Rd with bounded support; this function, in turn, is

representable by a ReLU network. This argument also shows that ReLU networks with bounded

support are universal approximators on X.

C FOURIER ANALYSIS OF RELU NETWORKS

C.1 PROOF OF LEMMA 1

Proof. The vector-valued function kf (x)eik·x is continuous everywhere and has well-defined and continuous gradients almost everywhere. So by Stokes' theorem [cite], the integral of its divergence is a pure boundary term. Since f vanishes outside its bounded support, the theorem yields

x · kf (x)e-ik·x dx = 0

(20)

The integrand is (k · (xf )(x) - ik2f (x))e-ik·x, so we deduce,

f^(k)

=

1 -ik2

k

·

(xf )(x) e-ik·x

(21)

Now, within each polytope of the decomposition (17), f is affine so its gradient is a constant vector, xf = W T , which gives the desired result (1).

C.2 FOURIER TRANSFORM OF POLYTOPES

C.2.1 THEOREM 1 OF DIAZ ET AL. (2016)

Let F be a m dimensional polytope in Rn, such that 1  m  n. Denote by k  Rn a vector in the fourier space, by k(x) = -k·x the linear phase function, by F~ the fourier transform of the indicator function on F , by F the boundary of F and by volm the m-dimensional (Hausdorff) measure. Let ProjF (k) be the orthogonal projection of k on to F (obtained by removing all components of k orthogonal to F ). Given a m - 1 dimensional facet G of F , let NF (G) be the unit normal vector to G that points out of F . It then holds:

1. If ProjF (k) = 0, then k(x) = k is constant on F , and we have: F~ = volF (F )eik

2. But if ProjF (k) = 0, then:

F~

=

i
GF

ProjF (k) · NF ProjF (k)

(G)
2

G~(k)

C.2.2 DISCUSSION
The above theorem provides a recursive relation for computing the fourier transform of an arbitrary polytope. More precisely, the Fourier transform of a m-dimensional polytope is expressed as a sum of fourier transforms over the m - 1 dimensional boundaries of the said polytope (which are themselves polytopes) times a O(k-1) weight term (with k = k ). The recursion terminates if ProjF (k) = 0, which then yields a constant.
To structure this computation, Diaz et al. (2016) introduce a book-keeping device called the face poset of the polytope. It can be understood as a weighted tree diagram with polytopes of various dimensions as its nodes. We start at the root node which is the full dimensional polytope P (i.e. we initially set m = n). For all of the codimension-one boundary faces F of P , we then draw an edge from the root P to node F and weight it with a term given by:

12

Under review as a conference paper at ICLR 2019

WF,G

=

i

ProjF (k) · NF ProjF (k)

(G)
2

G~(k)

and repeat the process iteratively for each F . Note that the weight term is O(k-1) where ProjF (k) = 0. This process yields tree paths P  F1  ...  Fq where each Fi+1 has one dimension less than Fi. For a given path and k, the terminal node for this path, Fq, is the first polytope for which ProjFq (k) = 0. The final fourier transform is obtained by multiplying the weights along each path and summing over all paths.
It is then shown that for a generic vector k, all paths terminate at the zero-dimensional vertices of the original polytope, i.e. dim(Fq) = 0, implying the length of the path q equals the number of dimensions n, yielding a O(k-n) spectrum. The exceptions occur if a path terminates prematurely, because k happens to lie orthogonal to some n - r-dimensional face Fr in the path, in which case we are left with a O(k-r) term (with r < n) which dominates asymptotically. Note that all vectors orthogonal to the n - r dimensional face Fr lie on a r-dimensional subspace of Rn. Since a polytope has a finite number of faces (of any dimension), the k's for which the Fourier transform is O(k-r) (instead of O(k-n)) lies on a finite union of closed subspaces of dimension r (with r < n). The Lebesgue measure of all such lower dimensional subspaces for all such r is 0, leading us to the conclusion that the spectrum decays as O(k-n) for almost all k's. We formalize this in the following corollary.
Corollary 1. Let P be a full dimensional polytope in Rn. The Fourier spectrum of its indicator function ~1P satisfies the following:

|1~P (k)| = O

1 kk

where 1  k  n, and k = j for k on a finite union of j-dimensional subspaces of Rn.

C.3 PROOF OF LEMMA 3

The first equality is simply the fact that Lf = max W , and the second inequality follows trivially from the parameterization of a ReLU network as a chain of function compositions14, together with
the fact that the Lipschitz constant of the ReLU function is 1 (cf. Miyato et al. (2018), equation 7). To see the third inequality, consider the definition of the spectral norm of a I × J matrix W :

W = max W h
h =1

(22)

Now, W h =

i |wi · h|, where wi is the i-th row of the weight matrix W and i = 1, ..., I.

Further, if h

= 1, we have |wi · h| 

wi

h

= 

wi

.

Since

wi

=

j = 1, ..., J) and |wij|   , we find that wi  J  . Consequently,

IJ   and we obtain:

j |wij| (with i |wi · h| 

 W  IJ  

(23)

Now for W = W (k), we have I = dk-1 and J = dk. In the product over k, every dk except the first and the last occur in pairs, which cancels the square root. For k = 1, dk-1 = d (for the d input neurons) and for k = L + 1, dk = 1 (for a single output neuron). The final inequality now follows.

14Recall that the Lipschitz constant of a composition of two or more functions is the product of their respective Lipschtiz constants.

13

Under review as a conference paper at ICLR 2019

(a) Iteration 100.

(b) Iteration 1000.

(c) Iteration 10000.

(d) Iteration 80000.

Figure 7: The learnt function (green) overlayed on the target function (blue) as the training progresses. The target function is a superposition of sinusoids of frequencies  = (5, 10, ..., 45, 50), equal amplitudes and randomly sampled phases.

D EXPERIMENTAL DETAILS

D.1 EXPERIMENT 1

We fit a 6 layer ReLU network with 256 units per layer f to the target function , which is a superposition of sine waves with increasing frequencies:

 : [0, 1]  R, (z) = Ai sin(2kiz + i)

i

where ki = (5, 10, 15, ..., 50), and i is sampled from the uniform distribution U (0, 2). In the first setting, we set equal amplitude for all frequencies, i.e. Ai = 1  i, while in the second setting we assign larger amplitudes to the higher frequencies, i.e. Ai = (0.1, 0.2, ..., 1). We sample  on 200 uniformly spaced points in [0, 1] and train the network for 80000 steps of full-batch gradient descent

with Adam (Kingma & Ba, 2014). Note that we do not use stochastic gradient descent to avoid the

stochasticity in parameter updates as a confounding factor. We evaluate the network on the same 200

point grid every 100 training steps and compute the magnitude of its (single-sided) discrete fourier

transform at frequencies ki which we denote with |f~ki |. Finally, we plot in figure 1 the normalized

magnitudes

|f~ki | Ai

averaged

over

10

runs

(with

different

sets

of

sampled

phases

i).

We

also

record

the spectral norms of the weights at each layer as the training progresses, which we plot in figure 1

for both settings (the spectral norm is evaluated with 10 power iterations). In figure 7, we show an

example target function and the predictions of the network trained on it (over the iterations).

D.2 EXPERIMENT 2
We use the same 6-layer deep 256-unit wide network and define the target function
 : D  R, z  (z) = Ai sin(2kiz + i)
i
where ki = (20, 40, ..., 180, 200), Ai = 1  i and   U (0, 2). We sample  on a grid with 1000 uniformly spaced points between 0 and 1 and map it to the input domain via L to obtain a dataset {(L(zj), (zj))}9j=990, on which we train the network with 50000 full-batch gradient descent steps of Adam. On the same 1000-point grid, we evaluate the magnitude of the (single-sided) discrete Fourier transform of f  L every 100 training steps at frequencies ki and average over 10 runs (each with a different set of sampled zi's). Fig 4 shows the evolution of the spectrum as training progresses for L = 0, 4, 10, 16, and Fig 5 shows the corresponding loss curves.

D.3 MNIST: A PROOF OF CONCEPT
In the following experiment, we show that given two manifolds of the same dimension ­ one flat and the other not ­ the task of learning random labels is harder to solve if the input samples lie on the same manifold. We demonstrate on MNIST under the assumption that the manifold hypothesis is true, and use the fact that the spectrum of the target function we use (white noise) is constant in expectation, and therefore independent of the underlying coordinate system when defined on the manifold.
Experiment 5. In this experiment, we investigate if it is easier to learn a signal on a more realistic data-manifold like that of MNIST (assuming the manifold hypothesis is true), and compare with

14

Under review as a conference paper at ICLR 2019
a flat manifold of the same dimension. To that end, we use the 64-dimensional feature-space E of a denoising15 autoencoder as a proxy for the real data-manifold of unknown number of dimensions. The decoder functions as an embedding of E in the input space X = R784, which effectively amounts to training a network on the reconstructions of the autoencoder. For comparision, we use an injective embedding16 of a 64-dimensional hyperplane in X. The latter is equivalent to sampling 784-dimensional vectors from U ([0, 1]) and setting all but the first 64 components to zero. The target function is white-noise, sampled as scalars from the uniform distribution U ([0, 1]). Two identical networks are trained under identical conditions, and Fig 8 shows the resulting loss curves, each averaged over 10 runs.
This result complements the findings of Arpit et al. (2017) and Zhang et al. (2017a), which show that it's easier to fit random labels to random inputs if the latter is defined on the full dimensional input space (i.e. the dimension of the flat manifold is the same as that of the input space, and not that of the underlying data-manifold being used for comparison).

Figure 8: Loss curves of two identical networks trained to regress white-noise under identical conditions, one on MNIST reconstructions from a DAE with 64 encoder features (blue), and the other on 64-dimensional random vectors (green).

D.4 CIFAR-10: IT'S ALL CONNECTED

We have seen that deep neural networks are biased towards learning low frequency functions. This should have as a consequence that isolated bubbles of constant prediction are rare. This in turn implies that given any two points in the input space and a network function that predicts the same class for the said points, there should be a path connecting them such that the network prediction does not change along the path. In the following, we present an experiment where we use a path finding method to find such a path between all Cifar-10 input samples indeed exist.
Experiment 6. Using AutoNEB Kolsbjerg et al. (2016), we construct paths between (adversarial) Cifar-10 images that are classified by a ResNet20 to be all of the same target class. AutoNEB bends a linear path between points in some space Rm so that some maximum energy along the path is minimal. Here, the space is the input space of the neural network, i.e. the space of 32 × 32 × 3 images and the logit output of the ResNet20 for a given class is minimized. We construct paths between the following points in image space:

· From one training image to another, · from a training image to an adversarial, · from one adversarial to another.

We only consider pairs of images that belong to the same class c (or, for adversarials, that originate from another class = c, but that the model classifies to be of the specified class c). For each class, we randomly select 50 training images and select a total of 50 random images from all other classes

15This experiment yields the same result if variational autoencoders are used instead.

16The

xy-plane

is

3
R

an

injective

embedding

of

a

subset

of

2
R

in

3
R

.

15

Under review as a conference paper at ICLR 2019

(frog) (truck) (truck) (deer) (horse)
(cat) (horse) (airplane) (deer) (horse)

(automobile) -> airplane (airplane) -> automobile (airplane) -> bird (automobile) -> cat (frog) -> deer (bird) -> dog (bird) -> frog (truck) -> horse (bird) -> ship (deer) -> truck

Figure 9: Path between CIFAR-10 adversarial examples (e.g. "frog" and "automobile", such that all images are classified as "airplane").

and generate adversarial samples from the latter. Then, paths between all pairs from the whole set of images are computed.
The AutoNEB parameters are chosen as follows: We run four NEB iterations with 10 steps of SGD with learning rate 0.001 and momentum 0.9. This computational budget is similar to that required to compute the adversarial samples. The gradient for each NEB step is computed to maximize the logit output of the ResNet-20 for the specified target class c. We use the formulation of NEB without springs Draxler et al. (2018).
The result is very clear: We can find paths between all pairs of images for all CIFAR10 labels that do not cross a single decision boundary. This means that all paths belong to the same connected component regarding the output of the DNN. This holds for all possible combinations of images in the above list. Figure 10 shows connecting training to adversarial images and Figure 9 paths between pairs of adversarial images. Paths between training images are not shown, they provide no further insight. Note that the paths are strikingly simple: Visually, they are hard to distinguish from the linear interpolation. Quantitatively, they are essentially (but not exactly) linear, with an average length (3.0 ± 0.3)% longer than the linear connection.

E THE FOURIER TRANSFORM OF A FUNCTION COMPOSITION

Consider equation 11. The general idea is to investigate the behaviour of P(l, k) for large frequen-
cies l on manifold but smaller frequencies k in the input domain. In particular, we are interested in the regime where the stationary phase approximation is applicable to P, i.e. when l2 +k2   (cf.
section 3.2. of Bergner et al.). In this regime, the integrand in P(k, l) oscillates fast enough such that the only constructive contribution originates from where the phase term u(z) = k · (z) - l · z

does not change with changing z. This yields the condition that zu(z) = 0, which translates to the condition (with Einstein summation convention implied and  = /x):

l = kµ µ(z)

(24)

Now, let the components of  be periodic, and without loss of generality we let the pe-

riod be 2. Further, we require that the manifold be contained in a box of some size in Rd. The µ-th component µ can now be expressed as a Fourier series:

µ(z) =

~µ[p]e-ipz

(25)

 µ(z) =

-ip ~µ[p]e-ipz

pZm

pZm

Equation 26 can be substituted in equation 24 to obtain:

l^l = -ik

p k^µ~µ[p]e-ipz

pZm

(26) (27)

16

Under review as a conference paper at ICLR 2019

airplane automobile
bird cat deer dog frog horse ship truck

(cat) (truck) (horse) (airplane) (bird) (frog) (deer) (dog) (automobile) (ship)

Figure 10: Each row is a path through the image space from an adversarial sample (right) to a true training image (left). All images are classified by a ResNet-20 to be of the class of the training sample on the right with at least 95% softmax certainty. This experiment shows we can find a path from adversarial examples (right, Eg. "(cat)") that are classified as a particular class ("airplane") are connected to actual training samples from that class (left, "airplane") such that all samples along that path are also predicted by the network to be of the same class.

where we have split kµ and l in to their magnitudes k and l and directions k^ and ^lµ (respectively). We are now interested in the conditions on  under which the RHS can be large in magnitude, even
when k is fixed. Recall that  is constrained to a box ­ consequently, we can not arbitrarily scale
up ~µ. However, if ~µ[p] decays slowly enough with increasing p, the RHS can be made arbitrarily large (for certain conditions on z, ^lµ and k^).

F VOLUME IN PARAMETER SPACE AND PROOF OF PROPOSITION 1

For a given neural network, we now show that the volume of the parameter space containing parameters that contribute -non-negligibly to frequency components of magnitude k above a certain cut-off k decays with increasing k. For notational simplicity and without loss of generality, we absorb the direction k^ of k in the respective mappings and only deal with the magnitude k.
Definition 1. Given a ReLU network f of fixed depth, width and weight clip K with parameter vector , an > 0 and  = BK(0) a L ball around 0, we define:
 (k) = {  |k > k, |f~(k )| > } as the set of all parameters vectors    (k) that contribute more than an in expressing one or more frequencies k above a cut-off frequency k.
Remark 1. If k2  k1, we have  (k2)   (k1) and consequently vol( (k2))  vol( (k1)), where vol is the Lebesgue measure.
Lemma 5. Let 1k() be the indicator function on  (k). Then:   > 0 : k  , 1k() = 0

Proof. From theorem 1, we know that17 |f~(k)| = O(k-d+-1) for an integer   d - 1. In

the worse case where 

=

d - 1, we have that M

<



:

|f~ (k)|

<

M k2

.

Now, simply select a

>

M

such that

M 2

<

.

This yields that |f~()|

<

M 2

<

, and given that

M 2



M k2

k



, we

find |f~(k)| <  k  . Now by definition 1,    (), and since  (k)   () (see remark 1),

we have    (k), implying 1k() = 0  k  .

Remark 2. We have 1k()  |f~(k)| for large enough k (i.e. for k  ), since |f~(k)|  0. 17Note that in theorem 1, k^ depends only on the direction of k, which we absorb in the definition of .

17

Under review as a conference paper at ICLR 2019

Theorem 2. The relative volume of  (k) w.r.t.  is O(k-d+-1) where 0    d - 1.

Proof. The volume is given by the integral over the indicator function, i.e.

vol( (k)) =

1k()d



For a large enough k, we have from remark 2, the monotonicity of the Lebesgue integral and theorem 1 that:

vol( (k)) =

1k()d 

|f~(k)|d = O(k-d+-1)vol()





= vol( (k)) = O(k-d+-1) vol()

G KERNEL MACHINES AND KNNS

In this section we contrast DNNs with K-nearest neighbor (k-NN) classifier and kernel machines which are also popular learning algorithms, but in contrast to DNNs are better understood theoretically.

G.1 KERNEL MACHINES VS DNNS
Given we study why DNNs are biased towards learning smooth functions, we note that kernel machines (KM) are also highly Lipschitz smooth (Eg. for Gaussian kernels all derivatives are bounded). However there are crutial differences between the two. While kernel machines can approximate any target function in principal (Hammer & Gersmann, 2003), the number of Gaussian kernels needed scales linearly with the number of sign changes in the target function (Bengio et al., 2009). Ma & Belkin (2017) have further shown that for smooth kernels, a target function cannot be approximated within precision in any polynomial of 1/ steps by gradient descent.
Deep networks on the other hand are also capable of approximating any target function (as shown by the universal approximation theorems Hornik et al. (1989); Cybenko (1989)), but they are also parameter efficient in contrast to KM. For instance, we have seen that deep ReLU networks separate the input space into number of linear regions that grow polynomially in width of layers and exponentially in the depth of the network (Montufar et al., 2014; Raghu et al., 2016). A similar result on the exponentially growing expressive power of networks in terms of their depth is also shown in (Poole et al., 2016). In this paper we have further shown that DNNs are inherently biased towards lower frequency (smooth) functions over a finite parameter space. This suggests that DNNs strike a good balance between function smoothness and expressibility/parameter-efficiency compared with KM.

G.2 K-NN CLASSIFIER VS. DNN CLASSIFIER

K-nearest neighbor (KNN) also has a historical importance as a classification algorithm due to its simplicity. It has been shown to be a consistent approximator Devroye et al. (1996), i.e., asymptotically its empirical risk goes to zero as K   and K/N  0, where N is the number of training samples. However, because it is a memory based algorithm, it is prohibitively slow for large datasets. Since the smoothness of a KNN prediction function is not well studied, we compare the smoothness between KNN and DNN. For various values of K, we train a KNN classifier on a k = 150 frequency signal (which is binarized) defined on the L = 20 manifold (see section 4), and extract probability predictions on a box interval in R2. On
18

Figure 11: The frequency spectrum of KNNs with different values of K, and a DNN. The DNN learns a smoother function compared with the KNNs considered since the spectrum of the DNN decays faster compared with KNNs.

Under review as a conference paper at ICLR 2019

(a) K = 5.

(b) K = 10.

(c) K = 15.

(d) K = 20.

Figure 12: Heatmaps of training accuracies (L-vs-k) of KNNs for various K. When comparing with figure 3, note that the y-axis is flipped.

this interval, we evaluate the 2D FFT and integrate out the angular components to obtain (k):

d (k) =

k
dk k

2
d|f~(k , )|

dk 0

0

(28)

Finally, we plot (k) for various K in figure 11. Furthermore, we train a DNN on the very same

dataset and overlay the radial spectrum of the resulting probability map on the same plot. We

find that while DNN's are as expressive as a K = 1 KNN classifier at lower (radial) frequencies,

the frequency spectrum of DNNs decay faster than KNN classifier for all values of K considered,

indicating that the DNN is smoother than the KNNs considered. We also repeat the experiment

corresponding to figure 3 with KNNs (figure 12) for various K's, to find that unlike DNNs, KNNs

do not necessarily perform better for larger L's, suggesting that KNNs do not exploit the geometry

of the manifold like DNNs do.

19

